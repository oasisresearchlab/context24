O R I G I N A L A R T I C L E Reinforcement Learning with Analogical Similarity to Guide Schema Induction and Attention James M . Foster | Matt Jones DepartmentofPsychologyandNeuroscience InstituteofCognitiveScience UniversityofColorado , Boulder 1 DepartmentofPsychologyand Neuroscience , InstituteofCognitive Science , UniversityofColorado , Boulder Correspondence JamesM . Foster DepartmentofPsychologyand Neuroscience UniversityofColorado 345UCB Boulder , CO , 80309 Email : james . m . foster @ colorado . edu Fundinginformation ThisworkwassupportedbyAFOSRGrant FA - 9550 - 10 - 1 - 0177toMattJones . Research in analogical reasoning suggests that higher - order cognitive functions such as abstract reasoning , far transfer , and creativity are founded on recognizing structural similar - ities among relational systems . Here we integrate theories of analogy with the computational framework of reinforce - ment learning ( RL ) . We propose a psychology theory that is a computational synergy between analogy and RL , in which analogical comparison provides the RL learning algorithm with a measure of relational similarity , and RL provides feed - back signals that can drive analogical learning . Simulation results support the power of this approach . KEYWORDS Reinforcement Learning , Analogy , Schema Induction , Attention , Representation Learning 1 | INTRODUCTION How do people learn new abstract concepts ? Everyone has a repertoire of concepts they use in making sense of the world and ﬁltering the inﬁnite complexity of experience into manageable chunks . Abstract representations that capture reoccurring patterns in the environment are used for generalization ( i . e . , making predictions or inferences about states of the environment that haven’t been experienced before ) . How are these representations constructed , and how is their usefulness evaluated ? For example , how do people discover and apply concepts such as a ’fork’ in the game of chess ? How do scientists create theories such as natural selection ? The goal of the present work is to develop a computational understanding of how people learn abstract concepts . Previous research in analogical reasoning suggests that higher - order cognitive functions such as abstract reasoning , 1 a r X i v : 1712 . 10070v1 [ c s . A I ] 28 D ec 2017 2 far transfer , and creativity are founded on recognizing structural similarities among relational systems ( Doumas et al . , 2008 ; Gentner , 1983 ; Hummel and Holyoak , 2003 ) . However , we argue a critical element is missing from these theories , in that their operation is essentially unsupervised , merely seeking patterns that recur in the environment , rather than focusing on the ones that are predictive of reward or other important outcomes . Here we integrate theories of analogy with the computational framework of reinforcement learning ( RL ) . RL offers a family of learning algorithms that have been highly successful in machine learning applications ( e . g . , Bagnell and Schneider , 2001 ; Tesauro , 1995 ) and that have neurophysiological support in the brain ( e . g . , Schultz et al . , 1997 ) . A shortcoming of RL is that it only learns efﬁciently in complex tasks if it starts with a representation ( i . e . , a means for encoding stimuli or states of the environment ) that somehow captures the critical structure inherent in the task . We formalize this notion below in terms of similarity - based generalization ( Shepard , 1987 ) and kernel methods from statistical machine learning ( Shawe - Taylor and Cristianini , 2004 ) . In other words , RL requires a sophisticated sense of similarity to succeed in realistically complex tasks . Psychologically , the question of how such a similarity function is learned can be cast as a question of learning sophisticated , abstract representations . This paper proposes a computational model of analogical RL , in which analogical comparison provides the RL learning algorithm with a measure of relational similarity , and RL provides feedback signals that can drive analogical learning . Relational similarity enables RL to generalize knowledge from past to current situations more efﬁciently , leading to faster learning . Conversely , the prediction - error signals from RL can be used to guide induction of new higher - order relational concepts . Thus we propose there exists a computationally powerful synergy between analogy and RL . The simulation experiment reported here supports this claim . Because of the strong empirical evidence for each of these mechanisms taken separately , we conjecture that the brain exploits this synergy as well . We want to emphasize that this isn’t a theory of speciﬁc psychological phenomena . We’re coming from the perspective that human conceptual learning and invention is far beyond current scientiﬁc explanation . There are no existing models that can compare to what the brain achieves . Our goal is to explore where this power might come from . The computational framework we propose is grounded in well - established psychological principles that themselves are supported by large bodies of experimental evidence , but the aim of our model is not to explain speciﬁc data . It’s to demonstrate the potential power that comes from combining these principles in the way we propose . Thus the scope of this paper is to understand how the proposed mechanism might work in principle . Future work will derive testable predictions and empirical means for testing them . In the following sections , we will review analogy and RL , then lay out our computational proposal and present a formal model , then present simulation results and discuss implications and limitations of the model . 2 | ANALOGY Research in human conceptual knowledge representation has shown that concepts are represented not just as distribu - tions of features ( cf . Nosofsky , 1986 ; Rosch and Mervis , 1975 ) but as relational structures . This relational knowledge includes both internal structure , such as the fact that a robin’s wings allow it to ﬂy ( Sloman et al . , 1998 ) , as well as externalstructure , suchasthefactthatadoglikestochasecats ( JonesandLove , 2007 ) . Theoriesofanalogicalreasoning represent relational knowledge of this type in a predicate calculus that binds objects to the roles of relations , for example CHASE ( DOG , CAT ) . According to these theories , an analogy between two complex episodes ( each a network of relations and objects ) amounts to recognition that they share a common relational structure ( Gentner , 1983 ; Hummel and Holyoak , 2003 ) . At a more mechanistic level , the dominant theory of analogy is structural alignment ( Gentner , 1983 ) . This process 3 involves building a mapping between two episodes , mapping objects to objects and relations to relations . The best mapping is one that maps objects to similar objects , maps relations to similar relations , and most importantly , satisﬁes parallel connectivity . Parallel connectivity means that , whenever two relations are mapped to each other , the objects ﬁlling their respective role - ﬁllers are also mapped together . An example is shown in Figure 1 . Parallel connectivity is satisﬁed here because , for each mapped pair of ATTACK relations ( red arrows ) , the objects ﬁlling the ATTACKER role are mapped together ( knight is mapped to queen ) , and the objects ﬁlling the ATTACKED role are also mapped together ( rook to rook and king to king ) . Thus structural alignment constitutes a ( potentially partial or imperfect ) isomorphism betweentwoepisodes , whichrespectstherelationalstructurethattheyhaveincommon . Importantly , ifthesearchfora mapping gives little emphasis to object - level similarity ( as opposed to relation - level similarity and parallel connectivity ) , then structural alignment can ﬁnd abstract commonalities between episodes having little or no surface similarity ( i . e . , in terms of perceptual features ) . FIGURE 1 An example of structural alignment between two chess positions . Both positions contain instances of the abstract concept of a FORK : black’s piece is simultaneously attacking both of white’s pieces . These attacking relations are represented by the red arrows . Cyan lines indicate the mapping between the two episodes . The mapping satisﬁes parallel connectivity because it respects the bindings between relations and their role - ﬁllers . We propose structural alignment is critical to learning of abstract concepts for three reasons . First , perceived similarity of relational stimuli depends on structural alignability ( Markman and Gentner , 1993 ) . Second , structural alignment is important for analogical transfer , which is the ability to apply knowledge from one situation to another , superﬁciallydifferentsituation ( GickandHolyoak , 1980 ) . Forexample , awinningmoveinonechesspositioncanbeused todiscoverawinningmoveinadifferent ( butaligned ) position , bytranslatingthatactionthroughtheanalogicalmapping . Third , a successful analogy can lead to schema induction , which involves extraction of the shared relational structure identiﬁed by the analogy ( Doumas et al . , 2008 ; Gentner , 1983 ; Hummel and Holyoak , 2003 ) . In the example of Figure 1 , this schema would be a system of relational knowledge on abstract ( token ) objects , including ATTACK ( PIECE 1 , PIECE 2 ) , ATTACK ( PIECE 1 , PIECE 3 ) , and potentially other shared information such as NOT _ ATTACKED ( PIECE 1 ) and KING ( PIECE 2 ) . These three observations suggest that analogy plays an important role in learning and use of abstract relational concepts . The ﬁrst two observations suggest that analogical transfer is like similarity - based generalization , but it’s also more sophisticated because it takes structure into account , as we elaborate in the next two sections . In brief , structural alignmentoffersasophisticatedformofsimilaritythatcanbeusedtogeneralizeknowledgebetweensituationsthatare superﬁcially very different . The third observation suggests that analogy can discover new relational concepts ( e . g . , the concept of a chess fork , from Figure 1 ) , which can in turn lead to perception of even more abstract similarities among future experiences . One potential shortcoming of the basic theory of analogy reviewed here is that is it essentially unsupervised . In this 4 framework , the quality of an analogy depends only on how well the two systems can be structurally aligned , and not on howusefulorpredictivethesharedstructuremightbe . Forexample , onecouldlistmanyrelationalpatternsthatarisein chess games but that are not especially useful for choosing a move or for predicting the course of the game . In previous work , we have found that implementing structural alignment and schema induction in a rich and structured artiﬁcial environment results in discovery of many frequent but mostly useless schemas ( Foster et al . , 2012 ) . An alternative , potentially more powerful model of analogical learning would involve feedback from the environment , so that the value of an analogy or schema is judged partially by how well it improves predictions of reward or other important environmental variables . For example , the concept of a fork in chess is an important schema not ( only ) because it is a recurring pattern in chess environments , but because it carries information about signiﬁcant outcomes ( i . e . , about sudden changes in each player’s chances of winning ) . A natural framework for introducing this sort of reward sensitivity into theories of analogy is that of RL , which we review next . 3 | REINFORCEMENT LEARNING RL is a mathematical and computational theory of learning from reward in dynamic environments . An RL task is characterized by an agent embedded in an environment that exists in some state at any given moment in time . At each timestep , theagentsensesthestateofitsenvironment , takesanactionthataffectswhatstateoccursnext , andreceives a continuous - valued reward that depends on the state and its action ( Sutton and Barto , 1998 ) . This framework is very general and can encompass nearly any psychological task in which the subject has full knowledge of the state of the world at all times ( i . e . , there are no relevant hidden variables ) . MostRLmodelsworkbylearning values fordifferentstatesoractions , whichrepresentthetotalfuturerewardthat can be expected from any given starting point ( i . e . , from any state or from any action within a state ) . These values can be learned incrementally , from temporal - difference ( TD ) error signals calculated from the reward and state following each action ( see Model section ) . There is strong evidence that the brain computes something close to TD error , and thus that RL captures a core principle of biological learning ( Schultz et al . , 1997 ) . In principle , this type of simple algorithm could be used to perfectly learn a complex task such as chess , by expe - riencing enough games to learn the true state values ( i . e . , probability of winning from every board position ) and then playing according to those values . However , a serious shortcoming of this naive approach is that it learns the value of each state independently , which can be hopelessly inefﬁcient for realistic tasks that typically have very large state spaces . Instead , some form of generalization is needed , to allow value estimates for one state to draw on experience in other , similar states . Many variants of RL have been proposed for implementing generalization among states ( e . g . , Albus , 1981 ; Sutton , 1988 ) . Here we pursue a direct and psychologically motivated form of generalization , based on similarity ( Jones and Cañas , 2010 ; Ormoneit and Sen , 2002 ) . We assume the model has a stored collection of exemplar states , each associated with a learned value . This exemplar representation is particularly suited for the analogy model we present below because it allows us to treat schemas as exemplars . The value estimate for any state is obtained by a similarity weighted average over the exemplars’ values ; that is , knowledge from each exemplar is used in proportion to how similar it is to the current state . This approach is closely related to exemplar - generalization models in more traditional psychologicaltaskssuchascategorylearning ( Nosofsky , 1986 ) . Itcanalsobeviewedasasubsetofkernelmethodsfrom machinelearning ( Shawe - TaylorandCristianini , 2004 ) , undertheidentiﬁcationofthekernelfunctionwithpsychological similarity ( Jäkel et al . , 2008 ) . A critical consideration for all learning models ( including RL models ) is how well their pattern of generalization 5 matches the inherent structure of the task . If generalization is strong only between stimuli or states that have similar values or outcomes , then learning will be efﬁcient . On the other hand , if the model generalizes signiﬁcantly between stimuli or states with very different outcomes , its estimates or predictions will be biased and learning and performance will be poor . The kernel or exemplar - similarity approach makes this connection explicit , because generalization be - tween two states is directly determined by their similarity . As we propose next , analogy and schema induction offer a sophisticated form of similarity that is potentially quite powerful for learning complex tasks with structured stimuli . 4 | ANALOGICAL RL The previous two sections suggest a complementary relationship between analogy and RL , which hint at the potential for a computationally powerful , synergistic interaction between these two cognitive processes . We outline here a formaltheoryofthisinteraction . Thenexttwosectionsprovideamathematicalspeciﬁcationofapartialimplementation of this theory , and then present simulation results offering a proof - in - principle of the computational power of this approach . The ﬁrst proposed connection between analogy and RL is that structural alignment yields an abstract form of psychological similarity that can support sophisticated generalization ( Gick and Holyoak , 1980 ; Markman and Gentner , 1993 ) . IncorporatinganalogicalsimilarityintotheRLframeworkcouldthusleadtorapidlearningincomplex , structured environments . For example , an RL model of chess equipped with analogical similarity and a notion of an attack relation shouldrecognizethesimilaritybetweenthetwopositionsinFigure1andhencegeneralizebetweenthem . Consequently the model should learn to create forks and to avoid forks by the opponent much more rapidly than if it had to learn about each possible fork instance individually . The second proposed connection is that the TD error computed by RL models , for updating value estimates , can potentially drive analogical learning by guiding schema induction and attention . Instead of forming schemas for whatever relational structures are frequently encountered ( or are discovered by analogical comparison of any two states ) , an analogical RL model can be more selective , only inducing schemas from analogies that signiﬁcantly improve reward prediction . Such analogies indicate that the structure common to the two analogue states may have particular predictive value in the current task , and hence that it might be worth extracting as a standalone concept . For example , if the model found a winning fork move by analogical comparison to a previously seen state involving a fork , the large boost in reward could trigger induction of a schema embodying the abstract concept of a fork . Furthermore , the model can use prediction error to increase attention to concepts that are more useful for predicting reward , and decrease attention to concepts that are less useful . Such an attention - learning mechanism is proposed to bias the model to rely more on concepts that have been consistently useful in the past . The proposed model thus works as follows ( see the next section for technical details ) . The model maintains a set of exemplars E , eachwithalearnedvalue , v ( E ) . Toestimatethevalueofanystate S , itcomparesthatstatetoallexemplars by structural alignment , which yields a measure of analogical similarity for each exemplar ( Forbus and Gentner , 1989 ) . The estimated value of the state , ˜ V ( s ) , is then obtained as a similarity - weighted average of v ( E ) . After any action is taken and the immediate reward and next state are observed , a TD error is computed as in standard RL . The exemplar values are then updated in proportion to the TD error and in proportion to how much each contributed to the model’s prediction , that is , in proportion to sim ( s , E ) . The attentions u ( E ) implement exemplar - speciﬁc attentions weights or learning rates . Attention learning is an additional mechanism of the model whose purpose is to increase the model’s reliance on useful exemplars . Although the model makes sense without this attention learning mechanism , including it improves performance and integrates 6 analogy , RL , and attention , and has been demonstrated in experiments with humans ( Foster and Jones , 2013b ; Foster , 2015 ) . A model that increases its repertoire of concepts needs some pruning mechanism to sort through what’s been discovered . This attention learning approach is a reasonable way to handle the pruning problem in an exemplar setting : the model needs to learn which exemplars or schemas to retain , so we attach an attention value to each one . The u ( E ) values can also be thought of as voting weights in the computation of ˜ V ( s ) . Exemplars with higher u values have greater inﬂuence on the similarity - weighted average of exemplar values . The attentions are updated in proportion to the TD error , in proportion to how much each contributed to the model’s prediction , and in proportion to how much that exemplar’s prediction differed from the overall prediction ˜ V . Attention is increased to exemplars that individually made a more accurate prediction that the overall prediction , and attention is decreased to exemplars that individually made a less accurate prediction than the overall prediction . AnalogyandRLalsomutuallyfacilitateeachotherthroughanadditionalmechanismofschemainduction . Whenever thestructuralalignmentbetweenastateandanexemplarproducesasufﬁcientreductioninpredictionerror ( relativeto whatwouldbeexpectedifthatexemplarwereabsent ) , aschemaisinducedfromthatanalogy . Theschemaisanabstract representation , deﬁnedontoken ( placeholder ) objects , anditcontainsonlythesharedinformationthatwassuccessfully mapped by the analogy . The schema is added to the pool of exemplars , where it can acquire value associations directly ( just like the exemplars do ) . Theoretically , we take the position that there is no real psychological difference between schemasandconcretestateexemplars , it’sjustacontinuumofspeciﬁcity . Theadvantageconferredbythenewschemais thatitallowsforevenfasterlearningaboutallstatesitappliesto ( i . e . , thatcontainthatsubstructure ) . Forexample , rather than learning by generalization among different instances of forks , the model would learn a direct value for the fork concept , which it could immediately apply to any future instances . A consequence of the schema induction mechanism is that the pool of exemplars comes to contain more and more abstract schemas . Thus the model’s representation transitions from initially episodic to more abstract and conceptual . The facilitation between analogy and RL here is that analogy provides new representations for RL to use for its value learning , and RL provides a TD error signal to help analogy decide which schemas to build . Analogical RL thus integrates three principles from prior research : RL , exemplar generalization , and structural alignmentofrelationalrepresentations . Becauseeachoftheseprincipleshasstrongempiricalsupportasapsychological mechanism , it is plausible that they all interact in a manner similar to what we propose here . Thus it seems fruitful to explore computationally what these mechanisms can achieve when combined . 5 | MODEL The proposed model applies to Markov Decision Process tasks , where an agent makes decisions based on the current state of the environment . At each time step , the agent chooses from the available actions in the current state and then the environment gives the agent an immediate reward and moves into the next state . The simulation study presented below uses a variant of RL known as afterstate learning , in which the agent learns values for the possible states it can move into ( Sutton and Barto , 1998 ) . This is a reasonable and efﬁcient method for the task we use here—tic - tac - toe , or noughts & crosses—because the agent’s opponent can be treated as part of the environment and is the only source of randomness . Our main proposal regarding the interaction between RL and analogical learning is not limited to this approach . The operation of the model is illustrated in Figure 2 . On each time step , the model identiﬁes all possible actions and their associated afterstates . For each afterstate S , it computes an analogical similarity , sim ( S , E ) , to each exemplar , E , by structural alignment . At the theoretical level , the model does not commit to a particular mapping algorithm . Instead , 7 X " O " O " X " X " O " O " X " O " X " O " X " O " X " O " X " Candidate state ( S ) Exemplars ( E )  V ( S ) v ( E ) a ( E ) = u ( E ) ⋅ sim ( S , E ) u ( E ) ⋅ sim ( S , E ) ∑ = v ( E ) ⋅ a ( E ) ∑ Δ v ( E ) TD … X O O X O X O X X – – – X Analogy provides similarity measure RL guides schema induction TD w / exemplar vs . without Δ u ( E ) FIGURE 2 Schema Induction Model operation . Each candidate afterstate is evaluated by analogical comparison to stored exemplars , followed by similarity - weighted averaging among the learned exemplar values . Learning is by TD errorappliedtotheexemplarvalues . Onsometrials , especiallyusefulanalogiesproducenewschemasthatareaddedto the exemplar pool . In the example here , S and E both have guaranteed wins for X by threatening a win in two ways . The induced schema embodies this abstract structure . Dots with red arrows indicate ternary “same - rank” relations . we assume the mapping could be done by any of the extant mapping models ( e . g . , Falkenhainer et al . , 1989 ; Larkey and Love , 2003 ; Hummel and Holyoak , 2003 ; Hofstadter et al . , 1994 ) . Each possible mapping M : S → E is evaluated according to Φ ( M ) = β · (cid:213) o ∈ s sim ( o , M ( o ) ) + (cid:213) r ∈ s sim ( r , M ( r ) ) · (cid:34) 1 + n r (cid:213) i = 1 I { M ( child i ( r ) ) = child i ( M ( r ) ) } (cid:35) . ( 1 ) Thisexpressiontakesintoaccountobjectsimilarity , bycomparingeachobject o in S toitsimagein E ; relationalsimilarity , by comparing each relation r in S to its image in E ; and parallel connectivity , by having similarity between mutually mapped relations “trickle down” to add to the similarity of any mutually mapped role - ﬁllers ( Forbus and Gentner , 1989 ) . The sim function is a primitive ( object - and relation - level ) similarity function , β determines the relative contribution of object similarity , n r is the number of roles in relation r , child i ( r ) is the object ﬁlling the i th role of r , and I { P } is an indicator function equal to 1 when proposition P is true . A more intuitive understanding of the Φ ( M ) equation may come from a description of its implementation , which works as follows . Given a mapping , a schema is created for that mapping ( one node for each pair of mapped nodes , with linksbetweenschemanodeswhenthere’sparallellinksin S and E . Theschemanodesaresortedbyconcepttype , sothat higher - order concepts are processed ﬁrst . Then , each node is processed by incrementing its score by a constant , and incrementing its child nodes’ scores by a trickle down factor times its score . Finally , each node’s ﬁnal score is summed to get a total score for the mapping . This trickle down scoring gives a bonus to systematic mappings ( those that share 8 systems of relations governed by common higher - order relations ) . Analogical similarity is then deﬁned as the value of the best mapping ( here the θ parameter determines speciﬁcity of generalization ) : sim ( S , E ) = exp (cid:18) θ · max M Φ ( M ) (cid:19) . ( 2 ) The activation a ( E ) of each exemplar is determined by weighting the analogical similarity between that exemplar E and the candidate state S by its attention u and normalizing by the attention - weighted analogical similarity across all exemplars : a ( E ) = u ( E ) · sim ( S , E ) (cid:205) E (cid:48) ∈ Exemplars u ( E (cid:48) ) · sim ( S , E (cid:48) ) . ( 3 ) The estimated value of S , ˜ V ( S ) , is computed as a similarity - weighted average of the exemplar values v ( E ) : ˜ V ( S ) = (cid:213) v ( E ) · a ( E ) . ( 4 ) Thus the estimate is based on the learned values of the exemplars most similar to the candidate state and the exemplars with the highest attention weights . There is a separate pass through the whole network for each candidate state ( i . e . , an outer loop over candidates ) . Although our model evaluates all possible afterstates , a more realistic model would be selective . How an agent decides which options to even consider is an important question but is outside the scope of this paper . Once values ˜ V ( S ) have been estimated for all candidate afterstates , the model uses a softmax ( Luce - choice or Gibbs - sampling rule ) to select what state to move into ( here τ is an exploration parameter ) : Pr [ S t = S ] ∝ e ˜ V ( S ) / τ . ( 5 ) Learning based on the chosen afterstate S t follows the SARSA rule ( Rummery and Niranjan , 1994 ) , after the model chooses its action on the next time step . This produces a TD error , which is then used to update the exemplar values and exemplar attention or voting weights by gradient descent . Exemplar values are updated in proportion to the TD error and their activations : (cid:52) v ( E ) = (cid:15) · T D · a ( E ) ( 6 ) where (cid:15) is a learning rate . Exemplar attentions are updated in proportion to the learning rate (cid:15) , the TD error , the analogical similarity between the exemplar and candidate state sim ( S , E ) , and the difference between the exemplar value v ( E ) and the estimated value of the candidate state ˜ V ( S ) , normalized by the attention - weighted analogical similarity across all exemplars : ∆ u ( E ) = (cid:15) · T D · sim ( S , E ) · ( v ( E ) − ˜ V ( S ) ) (cid:205) E (cid:48) ∈ Exemplars u ( E (cid:48) ) · sim ( S , E (cid:48) ) . ( 7 ) This exemplar - speciﬁc attention weight learning mechanism ( u parameter in the equations ) allows the model to 9 learn which exemplars are most useful for reducing prediction error . Over time , the model learns to increase attention to exemplars which are predictive of reward outcome , and decrease attention to exemplars which are not predictive of outcome . Following learning after each trial , the schema induction mechanism determines how much each exemplar con - tributed to reducing prediction error , by comparing TD to what it would have been without that exemplar . If the reduction is above some threshold , the analogical mapping found for that exemplar ( lower right of Figure 2 ) produces a schema that is added to the exemplar pool ( far right ) . The schema is given a value of v initialized at ˜ V ( S t ) . This schema value is updated on future trials just as are the exemplar values . Acquisition of new schemas in this way is predicted to improve the model’s pattern of generalization , tuning it to the most useful relational structures in a task . 1 6 | SIMULATION The goal of the simulation was to test whether the model could ( 1 ) learn more quickly by generalizing between states sharing structure but not surface similarity and ( 2 ) bootstrap its learning by discovering composite structures ( schemas ) that are particularly predictive . If the model succeeded at these two measures , then that would suggest it has potential to exhibit humanlike behavior in more complex settings . For example , the model might offer insight into how humans become experts in complex games like chess and Go , or how they acquire and apply relationally complex sentence structures ( see Goldwater et al . , 2011 ) . The analogical RL model was tested on its ability to learn tic - tac - toe ( Foster and Jones , 2013a ) . Tic - tac - toe was chosen as a test domain because it is a simple game with relational structure and a clear task goal . However , we want to be clear that this is not a theory of how people play tic - tac - toe . People are typically given explicit instruction on the rules , goals , and winning states of the game . The model isn’t provided any rules of the game , other than implicitly knowing what moves are legal at any point ( i . e . , you can only move in an empty square ) and the model doesn’t know what constitutes a win . Instead , the model learns entirely from trial and error . Whereas people are able to look ahead and mentally simulate moves several steps into the future , the model as currently implemented does not look ahead beyond the current move . In the simulation , each board position was represented by treating the nine squares as objects of types 0 ( blank ) , 1 ( focal agent’s ) , and 2 ( opponent’s ) , and deﬁning 8 ternary “same - rank” relations for the rows , columns , and diagonals . Thus a player wins by ﬁlling all squares in any one of these relations ( see Figure 3 ) . Object similarity was deﬁned as 1 for matching object types and 0 otherwise . Similarity between relations was always 1 because there was only one type TABLE 1 Model Variations . Model Variation Additional Mechanism Tested Featural Relational Relational similarity Unguided Schema Induction , Fixed Attentions Schema induction Guided Schema Induction , Fixed Attentions Schema induction guided by RL Guided Schema Induction , Learned Attentions State and Schema attentions guided by RL 1 Schemas can be spawned from the mapping between a candidate state and an exemplar state ( schema induction ) as well as from the mapping between a candidatestateandanexemplarschema ( schemareﬁnement ; Doumasetal . , 2008 ) 10 of relation . Reward was given only at the end of a game , as + 1 for the winner , - 1 for the loser , or 0 for a draw . After the game ended , it moved to a special terminal state with ﬁxed value of 0 . For simplicity , all free parameters of the model ( β , θ , α , γ , τ ) were set to a default value of 1 , except the schema induction threshold which was set to 6 standard deviations above the mean TD error reduction on each turn . FIGURE 3 Relations Deﬁned on the Tic - Tac - Toe board . The ﬁgure illustrates the 8 ternary relations that the Relational Model uses to represent a state of the game . There are three relations for the 3 vertical columns ( in red ) , 3 relations for the 3 horizontal rows ( in orange ) , and 2 relations for the diagonals ( in yellow ) . Each relation has 3 roles , which are ﬁlled by the objects in their corresponding board locations . State exemplars ( i . e . , non - schema exemplars ) were added to the model probabilistically , with probability of recruit - ment inversely proportional to the number of state exemplars already recruited . Recruitment of duplicate exemplars was not allowed . Five variations of the model were implemented to verify its mechanisms . Each model variation builds on the previous variation by adding an additional mechanism ( see Table 1 ) . The Featural model was restricted to literal mappings between states ( upper - left square to upper - left square , etc . ) . This model still included generalization , but its similarity was restricted to the concrete similarity of standard feature - based models . Featural similarity was deﬁned as the proportion of matching objects in the same absolute board locations . The Relational model considered all 8 mappings deﬁned by rigid rotation and reﬂection of the board . This scheme was used in place of searching all 9 ! possible mappings for every comparison , to reduce computation time ( see Figure 4 ) . The Unguided Schema Schema Induction , Fixed Attentions model extended the Relational model by inducing schemas that capture the relational structure critical to the task . This model variation included schema induction and relational similarity , but the schema induction process was not guided by RL feedback . Instead , this baseline model randomly induced schemas between exemplars and the current state ( or between schemas and the current state ) . The number of schemas induced was determined by yoking to the feedback - guided schema induction model . Thus this yoked baseline model induced the same number of schemas as the feedback - guided schema induction model , but the 11 X X O FeaturalDifferences X X O = Model = Opponent = Blank = Unspecified ( not part of schema ) X O * State State RelationalDifferences X X O X X O * * * X X X * * * O O X X X X O 0 0 4 0 0 0 Schema State O * X X X O * O O X X X X O 1 1 ( a ) ( b ) ( c ) ( d ) FIGURE 4 Featural vs . Relational Similarity in the Tic - Tac - Toe implementation . X indicates the model’s token , O indicates the opponent’s token , blank indicates an open space on the grid . Similarity is inversely related to the number of differences between states and states , or between states and schemas . Asterisks indicate board locations that are not part of the schema , and so do not contribute to the count of differences . ( a ) The featural and relational model similarity would be the same because all tokens match based on their absolute board locations . ( b ) The relational model would score these two states as having higher similarity because it allows for the reﬂection of the board , whereas the featural model would count four differences . ( c ) The schema on the right contains three X’s in a row , as well as two O’s , two blanks , and two locations that are not part of the schema . ( d ) The schema on the right has been further reﬁned , and is " clean " in that it only represents the information relevant to a winning board conﬁguration , 3 X’s in a row particular schemas learned were induced from comparisons between the current state and randomly chosen exemplars . The hypothesis was that the feedback - guided schema induction model would learn faster than the unguided model , and would also discover more useful representations . The Guided Schema Induction , Fixed Attentions model extended the Unguided Schema Induction , Fixed Attentions model by using RL to guide schema induction . Whenever an exemplar was particularly useful ( meaning it reduced TD error by a thresholded amount ) , that exemplar was used to induce a schema by comparing it to the candidate state . The threshold used was in terms of standard deviations in reduction of TD error . The reduction in TD error was computed by leaving out each exemplar and computing TD without it . The difference between the TD error with and without an exemplaristhereductioninTDforthatexemplar . Exemplarswhosereductionsweregreaterthan6standarddeviations above the mean reduction were used to induce schemas . Thus in this model , RL is used both to learn the values of the exemplars and to guide schema induction . The Guided Schema Induction , Learned Attentions model extended the Guided Schema Induction , Fixed Attentions model by using RL to learn exemplar - speciﬁc attention weights ( the u values ) . Each exemplar’s attention weight u was 12 initialized to 1 . Over time the model uses the TD error signal to update the attention weights so that particularly useful exemplars have their attentions increased and misleading exemplars have their attentions decreased ( see Equation 7 ) . Anytime an exemplar’s attention went below 0 , it was pruned from the exemplar pool . Each model variant was trained in blocks of 10 games of self - play . In self play , at the start of each game the model variant was cloned and played a game against itself . Learning ( updates to v and u ) was cached during the game and then applied after the game ﬁnished . Learning occurred only during training . Following each block of 10 self - play games , the model was tested in a pair of games against an ideal player ( playing ﬁrst in one game and second in the other ) . The ideal player was given the correct values for every state and always moved into the highest - valued next state . In testing games , the model was given one point for each non - losing move it made ( i . e . , moves from which it could still guarantee a draw ) , for a maximum of 9 points per pair of testing games . In other words , points were awarded for how long the model could play before the ideal opponent could guarantee a win . More rigorously , the ideal player is deﬁned by backward induction , partitioning the state space into states from which ideal play on both sides will lead to a draw , an X win , or an O win . Tic - tac - toe has the property that the initial ( blank ) state lies in the ﬁrst of these three subsets . The model’s score is determined by how long it keeps the game in that regime . FIGURE 5 Simulation Results . Performance of the four model variations over 50 , 000 training games . The featural model ( black ) is extremely slow to learn . The relational model ( red ) uses analogical generalization and performs better than the featural model . Adding schema induction ( light blue ) to the relational model does not improve performance , unless the schema induction process is guided by reinforcement learning ( dark blue ) . Adding the exemplar - speciﬁc attention learning mechanism ( green ) to the guided schema induction model further improves performance . Performance is measured by points , which is the number of moves made before the game is a sure loss against an ideal player , and is averaged across 64 independent copies of the model . Shading around each line indicates standard error bars . Training games are labeled in units of 5000 games . 13 7 | RESULTS Averaged learning curves are shown in Figure 5 for 64 independent copies of each model over 5000 blocks ( 50 , 000 training games ) . These results show that the featural model ( black ) is extremely slow to learn . The relational model ( red ) uses analogical generalization and performs better than the featural model . Adding schema induction ( light blue ) to the relational model does not improve performance , unless the schema induction process is itself guided by the prediction error signal ( dark blue ) . There is a further improvement in performance when the model can also learn to adapt its attentions based on prediction error with the exemplar - speciﬁc attention learning mechanism ( green ) . Asthemodellearnsfromtraininggames , itupdatesitsestimateofthevalueandattentionforeachexemplar . Figure 6 shows timelines of the model’s values ( v ) and attentions ( u ) as the model learns from more training games . Values and attentions are plotted separately for states recruited as exemplars vs . schemas induced by the model . The values for schemas tend to be more extreme ( either highly positive or highly negative ) than the values for states , and so are more diagnosticofwinningorlosingboardpositions . Additionally , theattentionsforschemastendtobelargerthanattentions for states . An exemplar with a higher attention value tends to make especially accurate reward predictions . The overall larger attentions for schemas ( as compared to states ) indicates that schemas are , overall , more useful exemplars for reward prediction . The important conclusion is that the model learns to shift its representation of the environment from states that it has directly experienced to more abstract schematic representations . Examples of some of the most useful representations learned by the SchemaInduction model are shown in Figure 7 . The schemas in ( a ) represent winning states of tic - tac - toe . The top left schema perfectly matches ( with a similarity of 1 ) anytic - tac - toestatewith3X’sinthebottomrow , theleftrow , therightrow , orthetoprow - regardlessofwhichobjects ( X , O , or blank ) occupy the other grid locations . Similarly , the left schema in ( b ) perfectly matches any tic - tac - toe state with an O in the center , an O in a corner , and a blank in the opposite corner . This schema represents the state in which the model is about to lose the game by an opponent making 3 O’s along the diagonal . These two schemas are “pure” or “clean” in that they contain no information that’s irrelevant to the winning or about - to - win states . However , the right schema in ( a ) does contain an irrelevant piece of information - the O in the top center grid location . This schema is 0 1 2 3 4 5 6 7 8 9 10 Training Games 0 . 2 0 . 4 0 . 6 0 . 8 1 1 . 2 1 . 4 1 . 6 1 . 8 2 A v e r age V a l ue ( V ) Value Learning for States vs . Schemas States Schemas ( a ) ValueLearning 0 1 2 3 4 5 6 7 8 9 10 Training Games 1 1 . 1 1 . 2 1 . 3 1 . 4 1 . 5 1 . 6 1 . 7 A v e r age A tt en t i on ( U ) Attention Learning for States vs . Schemas StatesSchemas ( b ) AttentionLearning FIGURE 6 Value and Attention Learning for States vs . Schemas over 50 , 000 training games . Averages for states ( red dotted lines ) vs . schemas ( solid blue lines ) for 64 independent copies of the Guided Schema Induction , Learned Attentions model . ( a ) Average absolute value of V for states vs . schemas as the model learns . Schemas quickly increase in absolute value compared to states , indicating that they are more diagnostic of winning and losing . ( b ) Once schemas begin to be induced , they rapidly rise in attention U and competitively reduce attention to states , which indicates that the model is learning that schemas are more useful representations than states . Training games are labeled in units of 5000 games . 14 * * * * * * X X X * * O * O * * * X O * * X * * * X X O * * O * * * O X X O O X O X X O X X X O O O O X X O X X ( a ) Model Win ( b ) Opponent Win ( c ) Model Fork ( d ) Opponent Fork = Model = Opponent = Blank = Unspecified ( not part of schema ) X O * FIGURE 7 Examples of some of the representations with the highest attentions , as averaged over 64 independent copies of the Guided Schema Induction , Learned Attentions model after 50 , 000 training games . Because the model is implemented with afterstates , in each of these examples is valued from the perspective of the model ( X ) just having placed a token , and it being the opponent’s ( O ) turn . The model associates ( a ) and ( c ) with positive reward value , and ( b ) and ( d ) with negative reward value , but all have high attentions because they are useful for predicting reward . ( a ) Examples of schemas where model wins the game . ( b ) Examples of schemas where the model is about to lose . ( c ) Examples of states where the model has two ways to win , and the opponent can only block one of them . ( d ) Examples of states where the opponent may have two ways to win . In the left board , a play by O in the top right corner will create two ways for the opponent to win . In the right board , the opponent is about to win , perhaps because it had produced a fork before X played in the left - center position . “dirty” in that it contains irrelevant details from the states from which it was induced . With further training , the model would likely reﬁne this schema and learn that abstracting out the O provides a more general , more useful schema . The right schema in ( b ) is also " dirty " in that it contains the extraneous X in the top left corner . The states in ( c ) represent something like " forks " in the tic - tac - toe domain , where the model has two ways to win and the opponent can only block one of them . The left state in ( d ) represents a state the model should avoid , because it provides opportunity for the opponent to create a fork by playing in the top right corner . In the right board in ( d ) , the opponent is about to win , possibly because it had created a fork before X played in the left - center position . 8 | GENERAL DISCUSSION The results presented here constitute a proof - of - principle that analogy and schema induction can be productively integrated with a learning framework founded on RL and similarity - based generalization . This integration leads to a model exhibiting sophisticated , abstract generalization derived from analogical similarity , as well as discovery of new relational structures driven by their ability to predict reward . 15 The basic modeling frameworkused here applies not just to analogical similarity and schema induction , but to other forms of representational learning as well . Kernel - based RL offers a powerful and general theory of representation learning , because it can be integrated with any form of representation that yields a pairwise similarity function . Its TD error signal can drive changes in representation via the objective of improving generalization . This idea has been applied to learning of selective attention among continuous stimulus dimensions ( Jones and Cañas , 2010 ) . The current model offers a richer form of representation learning , in that it acquires new concepts rather than reweighting existing features . 8 . 1 | Related Models The analogical RL model also builds on other models of relational learning . Tomlinson and Love ( 2006 ) propose a model of analogical category learning ( BRIDGES ) , with essentially the same similarity and exemplar generalization mechanisms adopted in the present model . Our model adds to theirs in that it applies to dynamic tasks and in that it grows its representation through schema induction . In BRIDGES , analogy contributes to RL by providing relational generalization . Our model also has the reverse , in that RL guides schema induction and hence acquisition of abstract concepts . Van Otterlo ( 2012 ) has developed methods for applying RL to relational representations of the same sort used here , although the approach to learning is quite different . His models are not psychologically motivated and hence learn in batches and form massive conjunctive rules , with elaborate updating schemes to keep track of the possible combinations of predicates . In contrast , the present approach learns iteratively , behaves probabilistically , and grows its representation more gradually and conservatively . This approach is likely to provide a better accountof human learning , but a more interesting question may be whether it offers any performance advantages from a pure machine - learning perspective . In the present model , the activation of each exemplar elicited by a candidate state can be thought of as a feature of that state . The exemplar effectively has a “receptive ﬁeld” within the state space , deﬁned by the similarity function . This dualitybetweenexemplar - andfeature - basedrepresentationsisfoundedinthekernelframework ( seeShawe - Taylorand Cristianini , 2004 ) . The present model takes advantage of this duality , producing a smooth transition from an episodic , similarity - based representation to a more semantic , feature - based representation deﬁned by learned schemas . The value and attention learning mechanisms have roots in traditional associational learning models . The classic Rescorla - Wagner model introduced joint learning of cue - outcome associations , but had no mechanism for attention learning ( Rescorla et al . , 1972 ) . Although the Rescorla - Wagner model allowed for different input cues to have different associabilities , there was no mechanism for learning these attentions . Mackintosh ( 1975 ) introduced an attention learning mechanism which modiﬁes attention to cues to reduce prediction error and reduce interference between cues . Support for this attention learning mechanism comes from demonstrations of learned inattention in experiments with rats ( Mackintosh and Turner , 1971 ) and humans ( Kruschke and Blair , 2000 ) . Although the analogical RL attention - learning mechanism in the present work is being implemented in an exemplar model , it is compatible with Mackintosh’s theory and with extensions by Kruschke ( 2001 ) . Each of these models would predict that attention would increase to lower - variance cues based on the idea that attention increases to cues that are more predictive ( contribute less error ) , compared to the average individual cue predictiveness ( as in Mackintosh ( 1975 ) and Kruschke ( 2001 ) ’s mixture of experts model ) or the overall combined cue predictiveness ( as in Kruschke ( 2001 ) ’s EXIT model and the present ARL model ) . In the exemplar setting , the model needs to learn which exemplars to retain in order to interpolate the reward value of each new stimulus based on learned exemplars . Thus the present ARL model also includes a normalization in the deﬁnition of exemplar activation , which makes attention more like a voting weight and less like salience . 16 The present work is complementary to hierarchical Bayesian models that discover relational structure through probabilistic inference ( Tenenbaum et al . , 2011 ) . Whereas our model builds up schemas from simpler representations , theBayesianapproachtakesatop - downapproach , deﬁningthecompletespaceofpossibilitiesaprioriandthenselecting among them . The top - down approach applies to any learning model , because any well - deﬁned algorithm can always be circumscribed in terms of its set of reachable knowledge states . This is a useful exercise for identifying inductive biases and absolute limits of learning , but it offers little insight into the constructive processes that actually produce the learning . The present model offers proposals about the mechanisms underlying how the human mind discovers new , abstract concepts . Extensions to this model are discussed in the following sections . 8 . 2 | Afterstate vs . Forestate Learning Although the tic - tac - toe domain is well captured with an afterstate representation , most tasks can’t use this simpliﬁca - tion . In the forestate version of the model , each state would have a set of available actions , and the model would learn values for state - action pairings rather than learning values just for ( after ) states . Furthermore , theafterstateformulationmissesanimportantwayinwhichanalogicaltransferismorecomplexthan similarity - based generalization ( SBG ) , which doesn’t accommodate the translation provided by the analogical mapping . In the extended forestate version of the present model , generalization is not just via blind similarity , because it takes the mapping ( and hence the structure of the stimuli ) into account . In a sense , the action is in the schema . A forestate model wherein structure mapping informed only the similarity computation and not the mapping of actions would fail in cases where the extended model succeeds . Consider the fork example from chess in Figure 1 . SBG would infer that a capturing action involving the knight in the game on the left would apply to a knight in the game on the right . If instead the action is in the schema , then a capturing action by the knight on the left would be correctly translated through the mapping into a capturing action by the analogous queen on the right . 8 . 3 | Two - Stage Memory Retrieval A challenge for the present model is tractability , because it’s not feasible to compute analogical mapping to all stored exemplars in memory . For computational efﬁciency and simplicity , the current implementation in the tic - tac - toe domain exploits knowledge of the game’s invariance under a predetermined set of simple symmetries . A solution to the tractability problem for the full model that uses structural mapping is to incorporate two - stage memory retrieval , following the MAC / FAC model ( Forbus et al . , 1995 ) . The ﬁrst stage uses fast feature - vector similarity to efﬁciently retrieve a set of candidate exemplars , and the second stage uses structural alignment to determine the best analogical matches . Such two - stage retrieval enables a more computationally tractable and psychologically plausible form of analogical inference from stored exemplars to novel situations . The ﬁrst stage of retrieval ( Many Are Called ) computes a MAC score which is used to select the set of candidate exemplars that pass on to the second ( Few Are Called ) stage . The candidate exemplar set is deﬁned by the top N exemplars sorted by descending MAC score . To compute each exemplar’s MAC score , ﬁrst the model computes the cosine similarity between each exemplar’s feature vector and the candidate state’s feature vector . The choice of cosine similarityisanimplementationaldetail , andthemodelisnottheoreticallycommittedtothisparticularchoiceoffeatural similarity . The present model’s attention - learning mechanism can be incorporated into the MAC score by multiplying the featural similarity measure by the exemplar - speciﬁc attention weights u to compute each exemplar’s MAC score : MAC ( S , E ) = u ( E ) p f ( S ) · f ( E ) (cid:107) f ( S ) (cid:107) (cid:107) f ( E ) (cid:107) . ( 8 ) 17 These two components ( featural similarity and attention ) in the MAC score make exemplars with higher featural similarity and higher attentions more likely to be retrieved from memory and included in the set of candidate exemplars . Thus more useful exemplars are more likely to be retrieved and relied upon for generalization . 2 The relative weighting of the featural similarity and the exemplar attentions is set by an exponent p on the attention weight . The second stage of retrieval ( Few Are Called ) computes a FAC score through a more computationally complex structure - mapping process on relational representations . In the present model , the FAC score is the analogical similarity of Equation 2 . Although the mapping and similarity scoring process itself is not currently learned , it may be productive to enable the prediction error signal to inﬂuence mapping or scoring itself , as in Liang and Forbus ( 2015 ) . The MAC / FAC extension also lays groundwork that will be useful for a later model with relational consolidation , which is discussed in detail in the next section . In brief , the exemplars that are learned to be most useful ( via the attention - learning mechanism described above ) would become new perceptual features which could be leveraged by the MAC retrieval stage to improve the candidate exemplars retrieved for the subsequent structural alignment processing . 8 . 4 | Relational Consolidation Although the present integration of analogy , schema induction , and reinforcement learning proves powerful , there’s a ceiling to what it can learn because all it can do is build conﬁgurations of the primitive elements it’s endowed with . It lacks a mechanism to create rich compositional hierarchies of relational concepts . Examples of such compositional hierarchies include computer architecture , mathematical functions , and natural languages , which all exemplify multiple levels of abstraction by chunking systems of relations at one level into building blocks at the next level . In computer architecture , digital logic gates are composed to form adders , which are composed with other digital circuits to form an arithmetic logic unit ( ALU ) , which is a building block in a computer’s CPU . Software design manages complexity by continuing this hierarchy , composing primitive functions into more complex functions , and from there to objects and design patterns . The conceptual progression in mathematics proceeds similarly , composing the counting operation to deﬁneadding , whichisfurthercomposedtoformmultiplying , andthenexponentiation . Intraditionalviewsoflinguistics , phonemes , morphemes , words , and sentences form another example of a relational hierarchy . 3 Althoughweagreewiththeoriesofschemainduction , weargueitisinsufﬁcienttoexplainhumanrelationallearning . Schemas are explicit relational structures , and thus they cannot be bound to roles of yet - higher - order relations in the way unitary objects and relations can . Experiments with chimpanzees suggest that newly learned relations can only ﬁll roles of other relations if they can be represented as atomic entities ( Thompson et al . , 1997 ) . Therefore , to explain acquisition of relational hierarchies , we put forward the hypothesis that useful schemas are eventually replaced ( or supplemented ) with unitary representations ( Foster et al . , 2012 ) . Thus , a concept that was represented as a system of relations ( via the schema ) can now be represented as an atomic entity , capable of entering into relations itself . We label this process relational consolidation , in a deliberate parallel to theories of episodic memory consolidation ( e . g . , Squire and Alvarez , 1995 ) . As summarized in Table 2 , consolidation is hypothesized to confer properties to a concept that are not true of ( unconsolidated ) schemas , because consolidated concepts are recognizable perceptually , without explicit ( working - memory dependent ) structure mapping ( Corral and Jones , 2014 , 2017 ; Foster et al . , 2012 ) . A consolidated concept can 2 Intheexistingmodel , exemplarattention u isstandinginfortheexpectedvalueofretrieval , whereasintheextendedmodelwithtwo - stageretrieval , retrieval probability is linearly related to u . Our theoretical commitment is that an exemplar with a higher u has higher probability of being retrieved ( monotonic relationship ) . 3 Relationalhierarchiesarenottaxonomichierarchies . Inataxonomichierarchy , eachconceptorcategoryisaunionoflower - levelcategories . Inarelational hierarchy , eachinstanceofaconceptisaconﬁgurationofinstancesoflower - orderconcepts . 18 TABLE 2 Predicted consequences of consolidation . Not Consolidated Consolidated More affected by WM demands Less affected by WM demands Quicker at analogical inference , because structure mapping is active Easier to learn higher - order structure , because in - stances can be represented by tokens Serial retrieval Parallel retrieval be recognized automatically , retrieved from memory in parallel , and represented as an element of yet - higher - order relations . It is important to note that consolidation is not a change in the declarative knowledge embodied by a concept . Rather , it is a proceduralization of the concept that enables future changes in knowledge – similar to the interaction between declarative and procedural knowledge in production systems ( Anderson and Lebiere , 1998 ) . The MAC / FAC model discussed above embodies the assumption that verifying the lower - level elements of an episode ( i . e . , predeﬁned objects and relations ) is fast and automatic , whereas verifying relational structure is slower and requires working - memory resources ( Forbus et al . , 1995 ) . From this perspective , relational consolidation enables higher - order relational structure to be chunked and treated as a dimension of the feature vector used for memory probing . Prior to consolidation , retrieval of instances of a higher - order relational structure requires something like the FAC stage , in which agents explicitly map between those instances and the schema . Following consolidation , retrieval can rely solely on the MAC stage , thus operating much more rapidly and without requiring working memory . We also propose a similar difference for perceptual recognition of instances of the concept . Before consolidation , episodes must be structurally aligned to a schema . After consolidation , an instance of the concept is explicitly represented and bound to the lower - order relations . We further propose that analogy , schema induction , and relational consolidation form a cycle that , when iterated , can producerelational hierarchies ofarbitrary depth ( height ) . Thisform of learning leadsto a dualistview of objects and relations , in which ( nearly ) every concept is both a relational structure among its components and an object capable of participating in relations . The conceptual systems built from this hierarchical relational chunking are potentially quite powerful and ﬂexible . 8 . 5 | Conclusions This paper has proposed a psychological theory that integrates reinforcement learning with relational representations and analogy . The integration produces a computational synergy in which analogy enables abstract generalization , and reinforcement learning drives discovery of useful relational concepts without relying on hand - coded representations . Analogy contributes mechanisms to generalize based on relational similarity , translate rewarding actions between mapped scenarios via analogical transfer , and produce progressively more abstract representations of the environment via schema induction . In return , reinforcement learning contributes mechanisms to learn the long - term value of exemplars , guide schema induction , and learn exemplar - speciﬁc attentions based on reward predictiveness . These mutually supportive mechanisms combine in a way that begins to explain how people and machines can learn abstract concepts based on experience with the world . 19 REFERENCES Albus , J . S . ( 1981 ) Brains , BehaviorandRobotics . ByteBooks . Anderson , J . J . R . andLebiere , C . J . ( 1998 ) TheAtomicComponentsofThought . Mahwah , NJ : Erlbaum . Bagnell , J . A . and Schneider , J . C . ( 2001 ) Autonomous helicopter control using reinforcement learning policy search methods . IEEEIntConfRobo , 1615 – 1620 . Corral , D . andJones , M . ( 2014 ) Theeffectsofrelationalstructureonanalogicallearning . Cognition , 132 , 280 – 300 . — ( 2017 ) Learning relational concepts through unitary versus compositional representations . Proceedings of the 39th Annual MeetingoftheCognitiveScienceSociety . Doumas , L . A . A . , Hummel , J . E . and Sandhofer , C . M . ( 2008 ) A theory of the discovery and predication of relational concepts . PsychologicalReview , 115 , 1 – 43 . Falkenhainer , B . , Forbus , K . D . andGentner , D . ( 1989 ) Thestructure - mappingengine : Algorithmandexamples . ArtiﬁcialIntelli - gence , 41 , 1 – 63 . Forbus , K . , Gentner , D . andLaw , K . ( 1995 ) MAC / FAC : Amodelofsimilarity - basedretrieval . CognitiveScience , 19 , 141 – 205 . Forbus , K . D . andGentner , D . ( 1989 ) Structuralevaluationofanalogies : Whatcounts . Proceedingsofthe11thAnnualConference oftheCognitiveScienceSociety , 341 – 348 . Foster , J . M . ( 2015 ) Analogicalreinforcementlearning . Foster , J . M . , Cañas , F . and Jones , M . ( 2012 ) Learning conceptual hierarchies by iterated relational consolidation . Proceedings ofthe34thAnnualConferenceoftheCognitiveScienceSociety , 324 – 329 . Foster , J . M . andJones , M . ( 2013a ) Analogicalreinforcementlearning . Proceedingsofthe35thAnnualConferenceoftheCognitive ScienceSociety . — ( 2013b ) Aresomeschemasstrongerthanothers ? thereinforcementofrelationalconcepts . Gentner , D . ( 1983 ) Structure - mapping : Atheoreticalframeworkforanalogy . CognitiveScience , 7 , 155 – 170 . Gick , M . L . andHolyoak , K . J . ( 1980 ) Analogicalproblemsolving . CognitivePsychol , 12 , 306 – 355 . Goldwater , M . B . , Tomlinson , M . T . , Echols , C . H . and Love , B . C . ( 2011 ) Structural priming as structure - mapping : Children use analogiesfrompreviousutterancestoguidesentenceproduction . CognitiveScience , 35 , 156 – 170 . Hofstadter , D . R . , Mitchell , M . et al . ( 1994 ) The copycat project : A model of mental ﬂuidity and analogy - making . Advances in connectionistandneuralcomputationtheory , 2 , 29 – 30 . Hummel , J . E . andHolyoak , K . J . ( 2003 ) Asymbolic - connectionisttheoryofrelationalinferenceandgeneralization . Psychologi - calReview , 110 , 220 – 264 . Jäkel , F . , Schölkopf , B . andWichmann , F . A . ( 2008 ) Generalizationandsimilarityinexemplarmodelsofcategorization : Insights frommachinelearning . PsychonBRev , 15 , 256 – 271 . Jones , M . and Cañas , F . ( 2010 ) Integrating reinforcement learning with models of representation learning . Proceedings of the 32ndAnnualConferenceoftheCognitiveScienceSociety , 1258 – 1263 . Jones , M . andLove , B . C . ( 2007 ) Beyondcommonfeatures : Theroleofrolesindeterminingsimilarity . CognitivePsychology , 55 , 196 – 231 . 20 Kruschke , J . K . ( 2001 ) Towardauniﬁedmodelofattentioninassociativelearning . JournalofMathematicalPsychology , 45 , 812 – 863 . Kruschke , J . K . and Blair , N . J . ( 2000 ) Blocking and backward blocking involve learned inattention . Psychonomic Bulletin & Review , 7 , 636 – 645 . Larkey , L . B . andLove , B . C . ( 2003 ) CAB : ConnectionistAnalogyBuilder . CognitiveScience , 27 , 781 – 794 . Liang , C . andForbus , K . D . ( 2015 ) Learningplausibleinferencesfromsemanticwebknowledgebycombininganalogicalgener - alizationwithstructuredlogisticregression . In AAAI , 551 – 557 . Mackintosh , N . and Turner , C . ( 1971 ) Blocking as a function of novelty of cs and predictability of ucs . The Quarterly Journal of ExperimentalPsychology , 23 , 359 – 366 . Mackintosh , N . J . ( 1975 ) A theory of attention : Variations in the associability of stimuli with reinforcement . Psychological Review , 82 , 276 . Markman , A . B . andGentner , D . ( 1993 ) Structuralalignmentduringsimilaritycomparisons . CognitivePsychol , 25 , 431 – 431 . Nosofsky , R . M . ( 1986 ) Attention , similarity , andtheidentiﬁcation - categorizationrelationship . JExpPsycholGen , 115 , 39 – 57 . Ormoneit , D . andSen , S . ( 2002 ) Kernel - basedreinforcementlearning . MachLearn , 49 , 161 – 178 . Rescorla , R . A . , Wagner , A . R . etal . ( 1972 ) Atheoryofpavlovianconditioning : Variationsintheeffectivenessofreinforcement andnonreinforcement . ClassicalconditioningII : Currentresearchandtheory , 2 , 64 – 99 . Rosch , E . and Mervis , C . B . ( 1975 ) Family resemblances : Studies in the internal structure of categories . Cognitive Psychol , 7 , 573 – 605 . Rummery , G . A . and Niranjan , M . ( 1994 ) On - line q - learning using connectionist systems . Tech . Rep . CUED / F - INFENG / TR 166 , CambridgeUniversity . Schultz , W . , Dayan , P . andMontague , P . R . ( 1997 ) Aneuralsubstrateofpredictionandreward . Science , 275 , 1593 – 1599 . Shawe - Taylor , J . andCristianini , N . ( 2004 ) KernelMethodsforPatternAnalysis . CambridgeUniversityPress . Shepard , R . N . ( 1987 ) Towardauniversallawofgeneralizationforpsychologicalscience . Science , 237 , 1317 – 1323 . Sloman , S . A . , Love , B . C . andAhn , W . K . ( 1998 ) Featurecentralityandconceptualcoherence . CognitiveSci , 22 , 189 – 228 . Squire , L . R . and Alvarez , P . ( 1995 ) Retrograde amnesia and memory consolidation : a neurobiological perspective . Current OpinioninNeurobiology , 5 , 169 – 177 . Sutton , R . S . ( 1988 ) Learningtopredictbythemethodsoftemporaldifferences . MachLearn , 3 , 9 – 44 . Sutton , R . S . andBarto , A . G . ( 1998 ) ReinforcementLearning : AnIntroduction . TheMITPress . Tenenbaum , J . B . , Kemp , C . , Grifﬁths , T . L . andGoodman , N . D . ( 2011 ) Howtogrowamind : Statistics , structure , andabstraction . Science , 331 , 1279 – 1285 . Tesauro , G . ( 1995 ) Temporaldifferencelearningandtd - gammon . CommunACM , 38 , 58 – 68 . Thompson , R . K . , Oden , D . L . and Boysen , S . T . ( 1997 ) Language - naive chimpanzees ( Pan troglodytes ) judge relations between relationsinaconceptualmatching - to - sampletask . JournalofExperimentalPsychology . AnimalBehaviorProcesses , 23 , 31 – 43 . Tomlinson , M . T . andLove , B . C . ( 2006 ) Frompigeonstohumans : Groundingrelationallearninginconcreteexamples . Proceed - ingsofthe21stNationalConferenceonArtiﬁcialIntelligence ( AAAI - 06 ) , 199 – 204 . Van Otterlo , M . ( 2012 ) Solving relational and ﬁrst - order logical markov decision processes : A survey . Reinforcement Learning , 12 , 253 – 292 .