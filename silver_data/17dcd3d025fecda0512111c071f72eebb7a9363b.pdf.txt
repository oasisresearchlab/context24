208 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 Online Public Shaming on Twitter : Detection , Analysis , and Mitigation Rajesh Basak , Shamik Sural , Senior Member , IEEE , Niloy Ganguly , and Soumya K . Ghosh , Member , IEEE Abstract —Public shaming in online social networks and related online public forums like Twitter has been increasing in recent years . These events are known to have a devastating impact on the victim’s social , political , and ﬁnancial life . Notwithstanding its known ill effects , little has been done in popular online social media to remedy this , often by the excuse of large volume and diversity of such comments and , therefore , unfeasible number of human moderators required to achieve the task . In this paper , we automate the task of public shaming detection in Twitter from the perspective of victims and explore primarily two aspects , namely , events and shamers . Shaming tweets are categorized into six types : abusive , comparison , passing judg - ment , religious / ethnic , sarcasm / joke , and whataboutery , and each tweet is classiﬁed into one of these types or as nonshaming . It is observed that out of all the participating users who post comments in a particular shaming event , majority of them are likely to shame the victim . Interestingly , it is also the shamers whose follower counts increase faster than that of the nonshamers in Twitter . Finally , based on categorization and classiﬁcation of shaming tweets , a web application called BlockShame has been designed and deployed for on - the - ﬂy muting / blocking of shamers attacking a victim on the Twitter . Index Terms —BlockShame , online user behavior , public sham - ing , tweet classiﬁcation . I . I NTRODUCTION O NLINE social networks ( OSNs ) are frequently ﬂooded with scathing remarks against individuals or organiza - tions on their perceived wrongdoing . When some of these remarks pertain to objective fact about the event , a sizable proportion attempts to malign the subject by passing quick judgments based on false or partially true facts . Limited scope of fact checkability coupled with the virulent nature of OSNs often translates into ignominy or ﬁnancial loss or both for the victim . Negative discourse in the form of hate speech , bullying , profanity , ﬂaming , trolling , etc . , in OSNs is well studied in the literature . On the other hand , public shaming , which is condemnation of someone who is in violation of accepted social norms to arouse feeling of guilt in him or her , has not attracted much attention from a computational perspective . Nevertheless , these events are constantly being on the rise for some years . Public shaming events have far - reaching impact Manuscript received March 2 , 2018 ; revised October 25 , 2018 ; accepted January 21 , 2019 . Date of publication February 20 , 2019 ; date of current version April 1 , 2019 . ( Corresponding author : Shamik Sural . ) The authors are with the Department of Computer Science and Engineering , IIT Kharagpur , Kharagpur 721302 , India ( e - mail : rajesh @ sit . iitkgp . ac . in ; shamik @ cse . iitkgp . ac . in ; niloy @ cse . iitkgp . ac . in ; skg @ cse . iitkgp . ac . in ) . Digital Object Identiﬁer 10 . 1109 / TCSS . 2019 . 2895734 on virtually every aspect of victim’s life . Such events have certain distinctive characteristics that set them apart from other similar phenomena : 1 ) a deﬁnite single target or victim ; 2 ) an action committed by the victim perceived to be wrong ; and 3 ) a cascade of condemnation from the society . In public shaming , a shamer is seldom repetitive as opposed to bullying . Hate speech and profanity are sometimes part of a shaming event but there are nuanced forms of shaming such as sarcasm and jokes , comparison of the victim with some other persons , etc . , which may not contain censored content explicitly . The enormous volume of comments which is often used to shame an almost unknown victim speaks of the viral nature of such events . For example , when Justine Sacco , a public relations person for American Internet Company tweeted “Going to Africa . Hope I don’t get AIDS . Just kidding . I’m white ! ” she had just 170 followers . Soon , a barrage of criticisms started pouring in , and the incident became one of the most talked about topics on Twitter and the Internet , in general , within hours . She lost her job even before her plane landed in South Africa . Jon Ronson’s “So You’ve Been Publicly Shamed” [ 1 ] presents an account of several online public shaming victims . What is common for a diverse set of shaming events we have studied is that the victims are subjected to punishments disproportionate to the level of crime they have apparently committed . In Table I , we have listed the victim , year in which the event took place , action that triggered public shaming along with the triggering medium , and its immediate consequences for each studied event . “Trig - ger” is the action or words spoken by the “Victim” which initiated public shaming . “Medium of triggering” is the ﬁrst communication media through which general public became aware of the “Trigger . ” The consequences for the victim , during or shortly after the event , are listed in “Immediate consequences . ” Henceforth , the two - letter abbreviations of the victim’s name will be used to refer to the respective shaming event . In the past , work ( see [ 2 ] – [ 5 ] ) on this topic has been done from the perspective of administrators who want to ﬁlter out any content perceived as malicious according to their website policy . However , none of these considers any speciﬁc victim . On the contrary , we look at the problem from the victim’s perspective . We consider a comment to be shaming only when it criticizes the target of the shaming event . For example , while “Justine Sacco gonna get off that international ﬂight and cry mountain stream fresh white whine tears b” is an instance of shaming , a comment like “Just read the Justine Sacco story lol 2329 - 924X © 2019 IEEE . Personal use is permitted , but republication / redistribution requires IEEE permission . See http : / / www . ieee . org / publications _ standards / publications / rights / index . html for more information . BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 209 smh sucks that she got ﬁred for a funny tweet . People so fuckin sensitive . ” is not an example of shaming from the perspective of Justine Sacco ( although it contains censored words ) as it rebukes other people and not her . In this paper , we propose a methodology for the detection and mitigation of the ill effects of online public shaming . We make three main contributions in this paper : 1 ) categorization and automatic classiﬁcation of shaming tweets ; 2 ) provide insights into shaming events and shamers ; 3 ) design and develop a novel application named Block - Shame that can be used by a Twitter user for blocking shamers . The rest of this paper is organized as follows . Section II discusses the related work . We introduce a categorization of shaming comments based on an in - depth study of a variety of tweets in Section III . A methodology for the identiﬁcation and prevention of such incidents is proposed in Section IV . Section V presents details of experiments and important results . The functionality and effectiveness of BlockShame are discussed in Section VI . Finally , we conclude this paper and provide directions for future research in Section VII . II . R ELATED W ORK Efforts to moderate user - generated content in the Internet started very early . Smokey [ 2 ] is one of the earliest compu - tational works in this direction which builds a decision tree classiﬁer for insulting posts trained on labeled comments from two web forums . Although academic research in this area started that early , it used different nomenclatures including abusive , ﬂame , personal attack , bullying , hate speech , etc . , often grouping more than a single category under a single name [ 6 ] . Based on the content ( and not the speciﬁc term used ) , we divide the related work into ﬁve categories : profan - ity , hate speech , cyberbullying , trolling , and personal attacks . Sood et al . [ 3 ] examine the effectiveness of list - based profanity detection for Yahoo ! Buzz comments . Relatively low F1 score ( harmonic mean of precision and recall ) of this approach is attributed to distortion of profane words with special characters ( e . g . , @ ss ) or spelling mistakes and low coverage of list words . The ﬁrst caveat was partly overcome by considering words as abusive whose edit distance from a known abusive word equals the number of “punctuation marks” present in the word . Rojas - Galeano [ 4 ] solves the problem of intentional distortion of abusive words in order to avoid censorship by allowing homoglyph ( characters that are similar in appearance , e . g . , “a” and “ α ” ) substitution to bear zero penalty in calculating edit distance between an abusive word and a distorted word , thereby increasing recall rate substantially . Hate speech , though well deﬁned as : “Abusive or threaten - ing speech or writing that expresses prejudice against a partic - ular group , especially on the basis of race , religion , or sexual orientation” [ 7 ] , is often used in several other connotations ( see [ 6 ] ) . Warner and Hirschberg [ 8 ] attempt to identify hate 1 Public relations . 2 Major Indian e - commerce company www . snapdeal . com . 3 An Australian television channel . speech targeting Jews from a data set consisting of Yahoo ! comments and known antisemitic web page contents . A similar type of work has been done on antiblack hate speech on Twit - ter [ 9 ] . Burnap and Williams [ 10 ] collected tweets for 2 weeks after the Lee Rigby incident [ 11 ] and trained a classiﬁer on typed dependence and hateful terms as features . Waseem and Hovy [ 12 ] released a public data set of 16000 tweets labeled in one of the three categories : racist , sexist , or none . They achieved an F1 score of 0 . 73 using character n - grams with logistic regression . Recently , Badjatiya et al . [ 13 ] reported F1 score of 0 . 93 using deep neural networks on the same data set . Academic research on bullying was started by social scientists and psychologists with a special focus on adoles - cents [ 14 ] – [ 16 ] . Similarly , social studies on cyberbullying pre - date computational endeavors . Cyberbullying has three deﬁnite characteristics [ 14 ] borrowed from traditional bullying [ 17 ] : intentional harm , repetitiveness , and power imbalance ( e . g . , anonymity in the Internet ) which differentiates it from other forms of online attacks . Vandebosch and Cleemput [ 18 ] give a detailed analysis of cyberbullies , their victims , and bystanders based on self - reported experience of bullying , cyberbully - ing , and Information and Communication Technology used by school children . Dinakar et al . [ 19 ] employ Open Mind Common Sense ( OMCS ) [ 20 ] , a common sense knowledge database , with custom built assertions related to the spe - ciﬁc domain of interests , e . g . , Lesbian , Gay , Bisexual , and Transgender ( LGBT ) , cyberbullying , to detect comments that deviate from real - world beliefs and is a good indicator of subtler forms of bullying . For instance , asking a male which beauty salon he visits can be a case of bullying as OMCS tells that beauty salons are more likely to be associated with females . In addition , the authors propose several techniques to counter these incidents ranging from delaying posts , issuing explicit warnings , etc . , to educating users about cyberbully - ing . Stressing the difference between cyberbullying and other forms of cyber - aggression , Hosseinmardi et al . [ 21 ] consider Instagram pictures with a minimum of 15 comments of which more than 40 % contain at least one profane word , to account for repetitiveness of bullying . Their best performing classiﬁer uses unigram and trigram text features with image category ( e . g . , person , car , nature , etc . ) and its metadata to achieve an F1 score of 0 . 87 . Trolls disrupt meaningful discussions in online com - munities by posting irrelevant and provocative comments . Cheng et al . [ 22 ] contrast traits of users banned by moderators to users who are not banned in news websites . They observe differences in the quality of comments , number of replies received , and use of positive words for the two groups . A classiﬁer trained on such features in one community is also able to perform well in another . Cheng et al . [ 23 ] equate ﬂagging of comments by community as instances of trolling and discover that a signiﬁcant portion of users has very low ﬂagged content earlier . They suggest that an ordinary user can behave like a troll depending on the mood of the user and the context of the discussion . Tsantarliotis et al . [ 24 ] introduce troll vulnerability metrics to predict likelihood of a post being trolled . 210 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 TABLE I E VENTS W ITH T RIGGER AND C ONSEQUENCES C ONSIDERED IN T HIS P APER Personal attack is less rigorously deﬁned and often holds all of the above - mentioned categories in it . Such attacks can be directed toward the author of a previous comment or a third party . Sood et al . [ 25 ] show that using two classiﬁers : one for object of insult ( previous author or third party ) identiﬁcation and another for insulting comment identiﬁcation , which boosts the overall accuracy of the system . A recent work [ 5 ] reports the classiﬁcation of personal attacks on Wikipedia author pages with accuracy comparable to annotation by a group of three human annotators . Compared with all of the above - mentioned work , in this paper , we study shaming comments on Twitter , which are part of a particular shaming event , and hence , they are related . Furthermore , when we consider a shaming event , the focus lies on a single victim . All the comments that are of interest should invariably be about that particular victim . Other comments are ignored . Most of the previous works mentioned above do not make a distinction between acceptability and nonacceptability of a comment based on the presence or absence of a predeﬁned victim . III . C ATEGORIZATION OF S HAMING T WEETS After studying more than 1000 shaming tweets from eight shaming events on Twitter , we have come up with six cat - egories of shaming tweets as shown in Table II . A brief description of these categories along with their most common attributes is given in the following . 1 ) Abusive : A comment falls in this category when the victim is abused by the shamer . It may be noted that mere presence of a list of abusive words is not enough to detect abusive shaming , because a comment may contain abusive utterances but it can still be in support of the victim . However , abusive words associated with the victim as found from dependency parsing of the comment are a strong marker of this type of shaming . 2 ) Comparison : In this form of shaming , the intended victim’s action or behavior is compared and contrasted with another entity . The main challenge here is to auto - matically detect perception of the entity mentioned in the comment so as to determine whether the comparison is an instance of shaming . The text itself may not contain enough hints , e . g . , adjectives with polarity associated with the entity . In such cases , the author of the comment relies on the collective memory of the social network users to provide for the necessary context . This is true more often when the said entity appeared recently in other events , e . g . , “ # AamirKhan you have forgotten that acting is being appreciated only in cinema ! Learn something from Mahadik’s 1 wife . ” This comment would be understood as shaming ( Aamir Khan is the target ) with little effort by anyone who has the knowledge that Mahadik is a positive mention . For someone who thinks Mahadik is a negative mention , the intent of the comment becomes ambiguous . Automatically predicting polarity of a mentioned entity in a comment in real time is a difﬁcult task . An approx - imation would be average perception ( sentiment score ) about the entity in most recent comments , recent news 1 Colonel Santosh Mahadik of the Indian army was killed in a terrorist encounter . BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 211 TABLE II D IFFERENT F ORMS OF S HAMING T WEETS sources , and so on . A static database would be of little use as public perception about an entity can change frequently . 3 ) Passing Judgment : Shamers can pass quick judgments vilifying the victim . Passing judgment often overlaps with other categories . A comment is PJ shaming only when it does not fall in any of the other categories . Passing judgment often starts with a verb and contains modal auxiliary verbs . 4 ) Religious / Ethnic : Often , there are multiple groups which a person identiﬁes with . We consider three types of identities of a victim - nationality like Indian , Chinese , ethnicity / race like black , white , and religious like Chris - tian and Jewish . Maligning any one of these group identities in reference to the victim constitutes a reli - gious / ethnic shaming . In this paper , we assume that we know the group identities to which a victim associates . For example , Justine Sacco is a U . S . citizen , white , and Christian . In actual scenario , this information can be inferred from the user’s proﬁle information on Twitter like name and location . In their absence , the display picture can potentially be used to predict a user’s demo - graphic information ( see [ 26 ] uses a third party service called Face + + [ 27 ] ) . 5 ) Sarcasm / Joke : Sarcasm is deﬁned as “a way of using words that are the opposite of what one means in order to be unpleasant to somebody or to make fun of them” in Oxford learner’s dictionary . This deﬁnition is also used by some recent work on sarcasm detection in Twitter like that in [ 28 ] . We have tagged joke and sarcasm in the same category due to an inherent overlap between the two . A sarcasm / joke tweet is not shaming unless the subject of fun is the victim , for example , “Wow I remember last night seeing the Justine Sacco thing start , never thought it would get this big ! Well played guys ! ” This tweet sarcastically criticizes Twitter users . Hence , it is not shaming . Presence of emojis and sudden change of sentiment are important attributes of this category . 6 ) Whataboutery : In whataboutery , the shamer highlights the victim’s purported duplicity by pointing out earlier action / in - action in a past situation similar to the present one . Important indicators for these categories of com - ments are the use of Wh - adverbs ( such as What , Why , How , etc . ) and past form of verbs . It is worthwhile mentioning that in a work - in - progress version of this paper published as a poster paper [ 29 ] , we categorized shaming into ten broad categories includ - ing the six described above . However , after more detailed scrutiny , in this paper , we have merged and omitted certain categories due to several reasons includ - ing sharing of features between two categories , low occurrences of comments in a category , and so on . IV . A UTOMATED C LASSIFICATION OF S HAMING T WEETS Our goal is to automatically classify tweets in the afore - mentioned six categories . In Fig . 1 , the main functional units involving automated classiﬁcation of shaming tweets are shown . Both labeled training set and test set of tweets for each of the categories go through the preprocessing and feature extraction steps . The training set is used to train six support vector machine ( SVM ) classiﬁers . The precision scores of the trained SVMs are next evaluated on the test set . Based on these scores , the classiﬁers are arranged hierarchically . A new tweet , after preprocessing and feature extraction , is fed to the trained classiﬁers and is labeled with the class of the ﬁrst classiﬁer that detects it to be positive . A tweet is deemed nonshame if all the classiﬁers label it as negative . We discuss the three steps of preprocessing , feature extrac - tion , and classiﬁcation in detail in the following . A . Preprocessing We perform a series of preprocessing steps before feature extraction and classiﬁcation is done . Named entity ( NE ) recognition , coreference resolution , and dependence parsing are performed using the Stanford CoreNLP library [ 30 ] . All references to victims including names or surnames preceded by salutations , mentions , and so on , are replaced with a uniform victim marker after the dependency parsing step . We also remove user mentions , retweet marker , hashtags , and URLs from the tweet text after dependency parsing and before parts of speech ( POS ) tagging with Stanford CoreNLP . If the event considered is a past event , current news source or search engine results would not be good indicators of a mentioned entity’s polarity in that period . For those , a list is constructed based on historical news related to the mentioned entities . For recent events , search engine results can be relied upon . 212 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 Fig . 1 . Block diagram for shaming detection . Fig . 2 . Structure of the feature vector . a1 and a2 : negative and positive words , b1 – b3 : abusive , negative and positive association , c1 – c6 : named entity associations , d1 and d2 : authority , e1 : group identities , f1 – f25 : POS and others , g1 – g4 : emojis , h1 – h6 : sentiment features , and i1 – i800 : Brown cluster unigrams . B . Feature Extraction We take into account a variety of syntactic , semantic , and contextual features derived from the text of a tweet . The overall structure of the feature vector is shown in Fig . 2 . In this ﬁgure , a feature is represented by an index containing a letter followed by a number . Similar features are grouped together and they share a common letter in their indexes . The original features ( with their respective indexes in parentheses ) are described next with the help of the following example tweet from the event TH . “Boris Johnson is an embarrassing Roderick Spode wannabe , and his comments on Tim Hunt are even stupider than Hunt’s original remarks . ” This tweet belongs to the comparison shaming category . Hereafter , by the presence of a feature , we mean the feature value is in binary . Similarly , count of a feature is in integer while proportions are in ﬂoating point numbers . 1 ) Negative and Positive Words ( a1 – a2 ) : Shaming com - ments tend to contain more negative words than non - shaming ( NS ) ones do . Proportion of negative ( a1 ) and positive words ( a2 ) to all words in a tweet are taken as features . We use negative and positive words lexicon provided by Hu and Liu [ 31 ] . In the example tweet mentioned above , negative word count is 2 ( “embarrass - ing” and “stupider” ) which is divided by 21 ( number of tokens separated by space ) to give a value of 0 . 095 for a1 . As there are no positive words in the tweet , the value of a2 is 0 . 2 ) Abusive , Negative , and Positive Association ( b1 – b3 ) : We consider the presence of negative ( b1 ) , positive ( b2 ) , and abusive ( b3 ) words directly associated with the victim found from dependency relation as features . This additional information helps to reduce the number of false negative decisions by the classiﬁers . In the example tweet mentioned above , there are no associations of the victim with abusive , negative , or positive words . Thus , b1 , b2 , and b3 are set to false . 3 ) Association With Named Entities ( c1 – c6 ) : Mention of NEs other than the victim in a tweet is a good indicator of comparison shaming . To handle this , a list of NEs with their polarities ( negative , neutral , or positive ) is used . Any NE that is not present in the list is also considered to be neutral . Count of mentions of these three polarities , i . e . , number of positive mentions ( c1 ) , neutral mentions ( c2 ) , and negative mentions ( c3 ) are used as features . In addition , we use direct association of negative / positive words with NEs to get the number of implied positive and negative mentions ( c4 and c5 ) in a comment . Presence of direct association of an NE with the victim ( by “and , ” “or , ” etc . ) ( c6 ) , which is a stronger indicator of comparison as opposed to a mere presence of the NE , is taken as a feature . For the example tweet , the NE recognizer correctly identiﬁes “Boris Johnson” and “Roderick Spode” as persons other than the victim . The ﬁrst one is included in the NE list as a negative mention setting c3 to 1 . c2 is also set to 1 as the second one is not present in the list . Values of c4 and c5 are both 0 , as there are no dependency relationships between the mentioned entities and positive / negative words . “Tim Hunt” is not directly associated with any of the NEs . Therefore , c6 is set to false . 4 ) Authority ( d1 – d2 ) : Presence of a dependency relation - ship between the victim and certain auxiliary verbs , BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 213 TABLE III ( A ) F EATURE V ALUES FOR THE C OMPARISON S HAMING E XAMPLE T WEET . ( B ) F EATURE V ALUES FOR AN A BUSIVE S HAMING T WEET . ( C ) F EATURE V ALUES FOR A S ARCASM / J OKE S HAMING T WEET such as “should , ” “must , ” and “ought” ( d1 ) , and tweet starting with a verb ( d2 ) usually indicate authority , which is a feature of shaming utterances . d1 and d2 are set to false as these features are not present in the above - mentioned tweet . 5 ) Group Identities ( e1 ) : The victim’s collective identities like religion , race , color , and so on , are used to determine the count of negative words associations with these iden - tities ( e1 ) , which is a strong indicator of religious / ethnic shaming . There are no negative word associations with Tim Hunt’s collective identities . Therefore , the value of e1 is set to 0 for the example tweet . 6 ) Parts of Speech and Others ( f1 – f25 ) : Proportion of POS tags in a tweet varies depending on the nature of the utterance , e . g . , use of the ﬁrst and second person pronouns is more probable for subjective comments than objective ones . Shaming comments are primarily subjective in nature . The proportion of the number of occurrences of a POS tag to all tokens is taken as a feature . We use the following tags from the Penn treebank [ 32 ] tagset : JJ , JJR , JJS , NN , NNS , NNP , NNPS , POS , PRP , PRP $ , RB , RBR , RBS , UH , VB , VBD , VBG , VBN , VBP , WDT , WP , WP $ , and WRB ( f1 to f23 ) . In addition , we consider the number of sentences ( f24 ) and the number of capital words ( f25 ) in a tweet , which implies emphasis , as features . The values of features from f1 to f23 are the number of each POS tag count divided by 21 . The example tweet has a single sentence and there are no capital words . Hence , the value of f24 is 1 and f25 is 0 . 7 ) Emojis ( g1 – g4 ) : Emojis constitute a popular means for expressing emotions . We divide common human face emojis into two groups , namely , happy and sad . Use of emojis from both the groups is often an indicator of sarcasm / joke . Presence of happy ( g1 ) and sad emojis ( g2 ) along with count of those ( g3 and g4 ) are used as features . These features are absent in the example tweet . 8 ) Sentiment Features ( h1 – h6 ) : It is intuitive to assume shaming utterances to be in a negative side of sentiment scale except in case of sarcasm / joke . We take the whole tweet sentiment ( h1 ) , which is an integer from 0 to 4 , for ﬁve sentiment classes of very negative to very positive as a feature . For sarcasm / joke , the change of sentiment in a single tweet is also an important marker . Therefore , 214 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 TABLE IV D ETAILED B REAKUP OF T WEETS U SED FOR E XPERIMENT we consider the proportion of nonleaf nodes belonging to each of the ﬁve sentiment categories ( h2 to h6 ) in the parse tree as features [ 33 ] . Sentiment of the example tweet is negative giving h1 a value of 1 . Most of the nonleaf nodes in the parse tree of the example tweet are of neutral sentiment followed by negative sentiment . 9 ) Brown Cluster Unigrams ( i1 – i800 ) : A typical tweet con - tains too few tokens from a huge vocabulary ( comprised of dictionary words , hashtags , URLs , mentions , etc . ) to create direct unigram features from it . As the resulting feature vector would be of very large dimension and sparse . To compensate for that , we use Brown cluster ( a hierarchical clustering of words ) unigrams as fea - tures [ 34 ] . We consider a Brown cluster unigram list having 800 clusters ( i1 to i800 ) produced from a corpus of about 6million tweets [ 35 ] . It may be noted that after tokenization , the given tweet produces 24 tokens includ - ing 2 punctuation marks ( a comma and a period ) and a special “s” ( from the word “Hunt’s” ) . However , “s” is missing from the clusters and some tokens are from common clusters . For example , “Borris , ” “Roderick , ” and “Tim” are from cluster index 12 while “comments” and “remarks” are from cluster index 650 . The token “Hunt” appears twice . Thus , only 19 cluster indexes out of the 800 have true values set for this particular tweet . Considering all the above - mentioned feature types , there are a total of 849 features ( i . e . , 800 unigrams plus 49 other features described earlier ) , all derived from the texts of the tweets . The values of the features for the example tweet are shown in Table III ( a ) . For Brown cluster unigrams ( i1 to i800 ) , only the cluster indexes that have true value are shown . “T” and “F” in the table denote True and False values ( 1 and 0 in the feature vector ) , respectively . Table III ( b ) and ( c ) shows feature values ( rounded off to two places of decimal ) for another two shaming tweets belonging to abusive and sarcasm / joke categories , respectively . C . Classiﬁcation Using Support Vector Machine Shaming classes are often found to be inherently overlap - ping , e . g . , a comment is both RE and AB when it abuses a victim’s ethnicity . For the sake of simplicity , we categorize each comment in only one class . Six one - versus - all SVM classiﬁers [ 36 ] for each shaming category are constructed . While training a classiﬁer , shaming comments from all other categories along with nonshame comments are treated as negative examples . Based on test set precision , the classiﬁers are arranged hierarchically placing one with higher precision above one with lower precision . The abusive classiﬁer that has the highest precision ( as shown in Section V ) is placed on top . For classiﬁcation , we use SVM with linear kernel from the java - ml library [ 37 ] . Linear kernel is chosen since it is known TABLE V P ERFORMANCE OF I NDIVIDUAL C LASSIFIERS to perform well in classifying text data and is faster than nonlinear kernels . Equal number of tweets is sampled from all the shaming categories and the NS category for each of the six classiﬁers to get balanced positive and negative examples in the training data set . V . E XPERIMENTAL R ESULTS A large number of tweets belonging to a diverse set of shaming events occurring over years were collected using the Twitter 1 % stream , Twitter search application programming interface ( API ) , and Topsy API ( defunct at present ) . These were annotated by a group of annotators , who were instructed to label a tweet in one of the six shaming categories or label it as NS . Details of the collected shaming events are given in Table IV . In the table , “ # Annotated” is the number of tweets manually labeled for each event . Note , for events LD , MT , and PC , we do not have any annotated data . “ # Unique tweets” is the number of collected unique tweets for an event . We do not include retweets explicitly in the data set since a retweet is given the label of the original tweet . A . Classiﬁcation Performance Performance scores for the six classiﬁers are shown in Table V . True positive rate ( TPR ) , true negative rate ( TNR ) , false positive rate ( FPR ) , false negative rate ( FNR ) , and precision in percentage are reported in the table . Fivefold cross validation was performed for reporting this performance result . From the table , it is observed that the abusive shaming clas - siﬁer has the highest precision and sarcasm / joke classiﬁer has the lowest precision , which is consistent with our expectations . As mentioned earlier , shaming categories are overlapping . It is , therefore , interesting to know which proportion of comments from a particular category is likely to get classiﬁed in other categories , i . e . , labeled positive by a wrong classiﬁer . This is illustrated in Table VI . In the ﬁrst row of the table , out of the 319 manually annotated AB shaming category tweets , when each one is presented to all the trained classiﬁers one after another , the AB - classiﬁer correctly outputs positive for 274 tweets , CO - classiﬁer wrongly labels 12 tweets as positive , BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 215 TABLE VI I NTERCATEGORY M ISCLASSIFICATION FOR I NDIVIDUAL C LASSIFIERS TABLE VII H IERARCHICAL C LASSIFICATION P ERFORMANCE and so on . Finally , 20 tweets get negative labels from all the six classiﬁers , thus wrongly deciding these to be NS . We observe that for all categories , a signiﬁcant number of false negative decisions would end up in passing judgment category ( these can also go to nonshame but only after the PJ - classiﬁer outputs a negative label ) . This validates our decision to instruct annotators to label a tweet as PJ only when it does not fall in any other category but it is an instance of shaming . AB tweets have almost uniform tendency to get classiﬁed positive by other classiﬁers , thus indicating that abusive words are used uniformly across all other categories . Sarcasm / joke and whataboutery comments are most often confused with NS . This reﬂects the inherent difﬁculty in distinguishing these two categories from NS when contextual information is limited or worse , absent . After hierarchical arrangement , the precision and recall scores for the classiﬁers are given in Table VII . The ﬁnal system has overall precision and recall scores of 72 . 69 and 88 . 08 , respectively . From the classiﬁed tweets , we have access to a large set of shamer and nonshamer users . The question we ask at this point is whether these two categories of users are inherently different from one another . Also , there are two types of shamers : active , those who write an original shaming tweet , and passive , those who only retweet a shaming tweet ( similar to bullies and bystanders in [ 18 ] ) . The major ﬁndings of our work are given in the following . B . Popularity and Shaming Follower count is an important indicator of a user’s pop - ularity ( there can be others , e . g . , number of retweets , likes his / her tweets get , etc . ) . Our event data set contains a diverse set of users with respect to popularity having follower count ranging from zero to a few million . To compare the tendency of shaming among these users , we divide them into equal size quartiles based on follower count—from very low popular Fig . 3 . Number of shamers and nonshamers in quartiles . Fig . 4 . Distribution of shaming comments with time . ( VLP ) to very high popular ( VHP ) . The intuition behind this is that there are different classes of users in every OSN as also in real society in terms of popularity . For example , a celebrity or politician’s Twitter attributes ( like follower count , status count , etc . ) are very unlikely to match that of a commoner . We observe in Fig . 3 that the number of shamers to that of nonshamers is almost double for each quartile increasing marginally with popularity . However , this small increase is due to the fact that in many cases , users have multiple comments and we mark them as shamers if any one of those is a shaming comment . Popular users are likely to comment more and they comment on multiple events increasing their chance of being labeled as shamers . C . Rewards for Shamers Negative discourse like public shaming also signiﬁes emo - tional attachment and engagement of the users with the Twitter ecosystem . Hence , it is relevant to ask whether shamers get rewarded or not by such behavior . In this context , we deﬁne followers per month ( FPM ) to be the number of followers divided by the number of months spent in Twitter by a user . 216 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 TABLE VIII A VERAGE FPM IN P OPULARITY C ATEGORIES The intuition behind this is that a user who has acquired more followers than another user in the same period of time posts more engaging and interesting comments . Are shaming comments one of those ? Comparing shamers with nonshamers , we ﬁnd that the average FPM is 204 for shamers while it is only 119 for the latter . In Table VIII , we list FPMs for shamers and nonshamers of the four classes separately . In all the popularity classes , shamers acquire more FPM than the nonshamers do . Note that “ # followers range” in parenthesis is the range of follower count for each quartile . While our analysis covers a fairly large number of users , yet further experiments are needed to claim that shaming increases follower count as a causal relationship . This is due to the fact that there could be other subtle characteristics of tweets ( e . g . , sense of humor ) or even attributes of the user ( e . g . , education level ) that can also potentially contribute to change in popularity . Study of political polarization in Twitter has recently been gaining momentum [ 38 ] – [ 40 ] . Hong and Kim [ 40 ] show that among the members of the United States House of Represen - tatives , all else being equal , those with more extreme views are likely to gather more Twitter followers than their moderate peers . Shaming can be construed as a form of extreme negative opinion as opposed to NS , which includes objective statements about the event as well as statements even supporting the victim . Thus , our results comparing average FPM for shamers and nonshamers point in a similar direction . D . Dynamics of Shaming Events In a bid to study the dynamics of shaming events , it was noted that their durations vary over a wide range . For ensuring uniformity in representation , the entire duration for each event is divided into 100 - time slots and the percentage of shaming comments ( i . e . , tweets and retweets ) posted in each of these time slots for that particular event is plotted in Fig . 4 . It may be observed from the ﬁgure that each of the six events has one major peak and several minor peaks . This indicates that the rate of shaming in Twitter is not uniform and usually occurs in bursts . Interestingly , only the events AK and LD , wherein both of the victims are popular television actors , have at least one prominent minor peak on the left of its major peak , i . e . , smaller but signiﬁcant bursts of shaming comments precede the major burst with respect to time . Our chosen events are very diverse in terms of when these occurred , victim’s proﬁle , and the nature of apparent violation Fig . 5 . Relative distribution of tweets and retweets in categories . Fig . 6 . Change in distribution of shaming categories across all events . of social norms . Despite these , in Fig . 5 , we observe similarity in the distribution of tweets and retweets across all events . In the ﬁgure , proportions of tweet and retweet categories for the eight events are shown . For every two consecutive bars , the ﬁrst bar denotes tweets and the second bar stands for retweets of an event . Although nonshame constitutes a major part of the bar , these are less likely to get retweeted . Sarcasm / jokes and passing judgments are popular means of shaming . Also , SJ tweets are very likely to get retweeted . It was also observed that the distribution of the six cate - gories of shaming tweets is not static and it changes over time as the shaming event progresses . Fig . 6 shows the proportion of posted shaming tweets in a category with respect to the total number of tweets in that category across all the shaming events . It is seen from the ﬁgure that all of the six categories peak on the third day and then goes down . However , the rise is not uniform . While the AB category rises moderately on the third day , the remaining ﬁve categories make big leaps from being very low on the ﬁrst and second days . This implies that the abusive form of shaming of the victim starts early and its volume remains relatively steady as compared to the other types . Fig . 7 shows this trend for six individual events over 4 days starting from the ﬁrst shaming tweet’s postdate in our corpus . BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 217 Fig . 7 . Change in distribution of shaming categories for individual events . The remaining two events have too few number of shaming tweets to be divided into 4 days . As an event progresses , the share of SJ comments increases in most of the cases . We also note that the share of RE comments for events JS and AK remains relatively larger for all days in comparison with other events . It may be concluded that the victim’s original comment or action coupled with his or her social background have some inﬂuence on the type of shaming received . If the proportion of abusive comments are any approximation for the degree of outrage caused among Twitter users , then , in this respect , events JS and TH rank higher than the others . VI . M ITIGATION OF P UBLIC S HAMING IN T WITTER There are two broad sets of controls available for users to counter inappropriate behavior in Twitter . The ﬁrst consists of several tools for reporting tweets as well as accounts directly to Twitter for spam , harassment , abuse , and so on . These measures are very effective in the sense that global actions can be taken by Twitter like deleting the offending tweet or even suspending the account of the offender altogether . However , the main problem with this approach is that action against a reported shaming tweet or account may take time . Twitter speciﬁes the time to conﬁrm the receipt of a report to be within 24 h [ 41 ] . However , there is no commitment on the actual time needed to take action against the offender . As shaming events are viral in nature , delayed action would defeat any attempt aimed at protecting the victim . The second set consists of three local controls provided by Twitter API , namely , “mute” that prevents tweets originating from the muted account from appearing in the user’s feed , Fig . 8 . BlockShame : home page . “block” that is similar to mute but it also unfollows / unfriends the blocked account , and “delete” that deletes a direct message ( DM ) received by the user . Although limited in scope , these actions remove any tweet immediately from the victim’s feed , thus , shielding him / her from direct shaming attacks . However , these tweets remain in the Twitter ecosystem to be viewed by users other than the victim . Making use of the above - mentioned handles , we have designed an application named BlockShame [ 42 ] which proac - tively takes user - deﬁned actions ( i . e . , any one of the “block , ” “mute , ” “delete , ” or none ) for three kinds of interactions in Twitter , i . e . , tweets , mentions , and DMs . In addition , users have the freedom to choose certain shaming categories to be out of the purview of it . The workﬂow of BlackShame includes the following steps . 1 ) User authorizes BlockShame in Twitter from the appli - cation’s website ( see Fig . 8 ) . 2 ) User sets choice of actions along with ( optionally ) his / her group identities ( see Fig . 9 ) for detecting and taking appropriate action on religious / ethnic type of shaming . 3 ) User’s recent tweets , mentions , and DMs are accessed from Twitter . 4 ) The obtained tweets are classiﬁed using pretrained SVMs . 5 ) Actions are taken according to the choices set by the user in step 2 ) . 6 ) Steps 3 ) – 5 ) are repeated periodically at ﬁxed short intervals until user revokes permission for BlockShame in Twitter . One of the ways to measure the effectiveness of a system like BlockShame is to count the average number of shaming tweets a shamer can post before he gets detected . To this end , we attempted to recreate a shaming event by directing a set of withheld labeled shaming tweets to a Twitter account speciﬁcally created for this purpose . The account was made 218 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 Fig . 9 . BlockShame : setting preferred actions by users . to subscribe to BlockShame . For the sake of this experiment , no action is actually taken on the shamer except for the fact that the sequence of labels predicted by BlockShame is stored . It may be noted that when a tweet is correctly classiﬁed as shaming , the shamer can be muted or blocked immediately . However , if a shaming tweet is missclassiﬁed into nonshame , the victim can be potentially shamed by the same shamer again until he gets detected in one of his later attempts . Keeping these facts in mind , we deﬁne a detection block to be a sequence of consecutive undetected shaming tweets followed by a single detected shaming tweet . Detection length is the number of tweets in a detection block . A detected shaming tweet that has no preceding undetected shaming tweet is of detection length one . For the exceptional case of one or more undetected shaming tweets appearing without a detected one , the detection length is taken to be the number of such tweets . From this perspective , the sequence of predictions by BlockShame for any shaming event can be viewed as a series of detection blocks , where each of the blocks corre - sponds to a shamer being detected . Fig . 10 shows the relative frequencies of detection lengths in percentage . It is observed that more than 80 % of the detections blocks are of length1 and about 13 % are of length 2 . This implies that a large majority of the shamers can be detected and action taken by BlockShame after their ﬁrst two shaming posts . A negligible number of shamers remain undetected after their third shaming tweet . Once a tweet has been detected as shaming and the attacker is subsequently blocked / muted by BlockShame , further tweets from him / her , unless unblocked / unmuted by the victim , do not show up in the victim’s feed even if those are NS . However , tweets of the same shamer to other users who have not been shamed are not blocked . In case of “delete , ” where a DM received gets deleted from the victim’s account , future NS communications ( DMs , Mentions , and Tweets ) from the shamer appear normally in the victim’s feed . This behavior is changed when the “DM Account Action” in BlockShame preferences is set to “block” or “mute” instead of “none” as shown in Fig . 9 . Fig . 10 . BlockShame : number of tweets by shamers before detection . After offending accounts have been muted or blocked by BlockShame , the victim may choose to report the accounts to Twitter for permanent action , if desired . The approach mentioned here can also be potentially deployed by Twitter itself for automating the process of taking appropriate action against repeated offenders . VII . C ONCLUSION In this paper , we proposed a potential solution for counter - ing the menace of online public shaming in Twitter by catego - rizing shaming comments in six types , choosing appropriate features , and designing a set of classiﬁers to detect it . Instead of treating tweets as standalone utterances , we studied them to be part of certain shaming events . In doing so , we observe that seemingly dissimilar events share a lot of interesting properties , such as a Twitter user’s propensity to participate in shaming , retweet probabilities of the shaming types , and how these events unfold in time . With the growth of online social networks and proportional rise in public shaming events , voices against callousness on part of the site owners are growing stronger . Categorization of shaming comments as presented in this paper has the potential for a user to choose to allow certain types of shaming comments ( e . g . , comments that are sarcastic in nature ) giving his / her an opportunity for rebuttal and block others ( e . g . , comments that attack her ethnicity ) according to individual choices . Freedom to choose what type of utterances one would not like to see in his / her feed beforehand is way better than ﬂagging a deluge of comments on the event of shaming . This also liberates moderators from the moral dilemma of deciding a threshold that separates acceptable online behavior from unacceptable ones , thus relieving themselves to a certain extent from the responsibility of ﬁxing what is best for another person . To ascertain whether shaming a victim in itself causes an increase in popularity in Twitter as mentioned in BASAK et al . : ONLINE PUBLIC SHAMING ON TWITTER 219 Section V - C , the change in FPM of shamers and that of nonshamers before and after a shaming event need to be compared at an individual level . The caveat is that Twitter only provides the present follower count as public domain information and not the follower count history . One possibility to overcome this shortcoming is to track a large set of shamers and nonshamers for a long time before and after the respective events have taken place . We intend to collect and curate even longer term Twitter data to achieve this . Shaming is subjective in reference to shamers . For example , the same comment made by two different persons coming from different social , cultural , or political backgrounds may have different connotations to the victim . We would like to include the attributes of the author of the comment as contextual information when deciding if the comment is sham - ing or not . This algorithm , once developed , will be included in BlockShame to enrich its functionality . Moreover , in every event , we note that after the initial outrage , the volume of apologetic or reconciliatory comments gradually increases . A considerable proportion of users made multiple comments in a single event that contains both shaming and nonshame categories . We plan to investigate these behaviors further in the future . The performance of individual classiﬁers is promising though there are scopes for improvement . We would like to repeat our experiments with an even larger annotated data set to improve the performance further . R EFERENCES [ 1 ] J . Ronson , So You’ve Been Publicly Shamed . London , U . K . : Picador , 2015 . [ 2 ] E . Spertus , “Smokey : Automatic recognition of hostile messages , ” in Proc . AAAI / IAAI , 1997 , pp . 1058 – 1065 . [ 3 ] S . Sood , J . Antin , and E . Churchill , “Profanity use in online com - munities , ” in Proc . SIGCHI Conf . Hum . Factors Comput . Syst . , 2012 , pp . 1481 – 1490 . [ 4 ] S . Rojas - Galeano , “On obstructing obscenity obfuscation , ” ACM Trans . Web , vol . 11 , no . 2 , p . 12 , 2017 . [ 5 ] E . Wulczyn , N . Thain , and L . Dixon , “Ex machina : Personal attacks seen at scale , ” in Proc . 26th Int . Conf . World Wide Web , 2017 , pp . 1391 – 1399 . [ 6 ] A . Schmidt and M . Wiegand , “A survey on hate speech detection using natural language processing , ” in Proc . 5th Int . Workshop Natural Lang . Process . Social Media Assoc . Comput . Linguistics , Valencia , Spain , 2017 , pp . 1 – 10 . [ 7 ] Hate - Speech . Oxford Dictionaries . Accessed : Aug . 30 , 2017 . [ Online ] . Available : https : / / en . oxforddictionaries . com / deﬁnition / hate _ speech [ 8 ] W . Warner and J . Hirschberg , “Detecting hate speech on the world wide Web , ” in Proc . 2nd Workshop Lang . Social Media , 2012 , pp . 19 – 26 . [ 9 ] I . Kwok and Y . Wang , “Locate the hate : Detecting tweets against blacks , ” in Proc . AAAI , 2013 , pp . 1621 – 1622 . [ 10 ] P . Burnap and M . L . Williams , “Cyber hate speech on Twitter : An appli - cation of machine classiﬁcation and statistical modeling for policy and decision making , ” Policy Internet , vol . 7 , no . 2 , pp . 223 – 242 , 2015 . [ 11 ] Lee - Rigby . Lee Rigby Murder : Map and Timeline . Accessed : Dec . 7 , 2017 . [ Online ] . Available : https : / / http : / / www . bbc . com / news / uk - 25298580 [ 12 ] Z . Waseem and D . Hovy , “Hateful symbols or hateful people ? Predictive features for hate speech detection on Twitter , ” in Proc . SRW HLT - NAACL , 2016 , pp . 88 – 93 . [ 13 ] P . Badjatiya , S . Gupta , M . Gupta , and V . Varma , “Deep learning for hate speech detection in tweets , ” in Proc . 26th Int . Conf . World Wide Web Companion , 2017 , pp . 759 – 760 . [ 14 ] D . Olweus , S . Limber , and S . Mihalic , Blueprints for Violence Pre - vention , Book Nine : Bullying Prevention Program . Boulder , CO , USA : Center for the Study and Prevention of Violence , 1999 . [ 15 ] P . K . Smith , H . Cowie , R . F . Olafsson , and A . P . D . Liefooghe , “Deﬁnitions of bullying : A comparison of terms used , and age and gender differences , in a fourteen – country international comparison , ” Child Develop . , vol . 73 , no . 4 , pp . 1119 – 1133 , 2002 . [ 16 ] R . S . Grifﬁn and A . M . Gross , “Childhood bullying : Current empirical ﬁndings and future directions for research , ” Aggression Violent Behav . , vol . 9 , no . 4 , pp . 379 – 400 , 2004 . [ 17 ] H . Vandebosch and K . Van Cleemput , “Deﬁning cyberbullying : A qual - itative research into the perceptions of youngsters , ” CyberPsychol . Behav . , vol . 11 , no . 4 , pp . 499 – 503 , 2008 . [ 18 ] H . Vandebosch and K . Van Cleemput , “Cyberbullying among young - sters : Proﬁles of bullies and victims , ” New Media Soc . , vol . 11 , no . 8 , pp . 1349 – 1371 , 2009 . [ 19 ] K . Dinakar , B . Jones , C . Havasi , H . Lieberman , and R . Picard , “Common sense reasoning for detection , prevention , and mitigation of cyberbully - ing , ” ACM Trans . Interact . Intell . Syst . , vol . 2 , no . 3 , p . 18 , 2012 . [ 20 ] P . Singh , T . Lin , E . T . Mueller , G . Lim , T . Perkins , and W . L . Zhu , “Open mind common sense : Knowledge acquisition from the general public , ” in Proc . OTM Confederated Int . Conf . Move Meaningful Internet Syst . Berlin , Germany : Springer , 2002 , pp . 1223 – 1237 . [ 21 ] H . Hosseinmardi , S . A . Mattson , R . I . Raﬁq , R . Han , Q . Lv , and S . Mishra . ( 2015 ) . “Detection of cyberbullying incidents on the instagram social network . ” [ Online ] . Available : https : / / arxiv . org / abs / 1503 . 03909 [ 22 ] J . Cheng , C . Danescu - Niculescu - Mizil , and J . Leskovec , “Antisocial behavior in online discussion communities , ” in Proc . ICWSM , 2015 , pp . 61 – 70 . [ 23 ] J . Cheng , C . Danescu - Niculescu - Mizil , J . Leskovec , and M . Bernstein , “Anyone can become a troll , ” Amer . Sci . , vol . 105 , no . 3 , p . 152 , 2017 . [ 24 ] P . Tsantarliotis , E . Pitoura , and P . Tsaparas , “Deﬁning and predicting troll vulnerability in online social media , ” Social Netw . Anal . Mining , vol . 7 , no . 1 , p . 26 , 2017 . [ 25 ] S . O . Sood , E . F . Churchill , and J . Antin , “Automatic identiﬁcation of personal insults on social news sites , ” J . Assoc . Inf . Sci . Technol . , vol . 63 , no . 2 , pp . 270 – 285 , 2012 . [ 26 ] A . Chakraborty , J . Messias , F . Benevenuto , S . Ghosh , N . Ganguly , and K . Gummadi , “Who makes trends ? Understanding demographic biases in crowdsourced recommendations , ” in Proc . 11th Int . AAAI Conf . Web Social Media , 2017 , pp . 22 – 31 . [ 27 ] Face + + Cognitive Services . Accessed : Feb . 20 , 2018 . [ Online ] . Avail - able : https : / / www . faceplusplus . com / [ 28 ] A . Rajadesingan , R . Zafarani , and H . Liu , “Sarcasm detection on Twitter : A behavioral modeling approach , ” in Proc . 8th ACM Int . Conf . Web Search Data Mining , 2015 , pp . 97 – 106 . [ 29 ] R . Basak , N . Ganguly , S . Sural , and S . K . Ghosh , “Look before you shame : A study on shaming activities on Twitter , ” in Proc . 25th Int . Conf . Companion World Wide Web , 2016 , pp . 11 – 12 . [ 30 ] C . D . Manning , M . Surdeanu , J . Bauer , J . Finkel , S . J . Bethard , and D . McClosky , “The Stanford CoreNLP natural language processing toolkit , ” in Proc . Assoc . Comput . Linguistics ( ACL ) Syst . Demonstrations , 2014 , pp . 55 – 60 . [ Online ] . Available : http : / / www . aclweb . org / anthology / P / P14 / P14 - 5010 [ 31 ] M . Hu and B . Liu , “Mining and summarizing customer reviews , ” in Proc . 10th ACM SIGKDD Int . Conf . Knowl . Discovery Data Mining , 2004 , pp . 168 – 177 . [ 32 ] M . P . Marcus and M . A . Marcinkiewicz , and B . Santorini , “Building a large annotated corpus of English : The Penn treebank , ” Comput . Linguistics , vol . 19 , no . 2 , pp . 313 – 330 , 1993 . [ 33 ] D . Bamman and N . A . Smith , “Contextualized sarcasm detection on Twitter , ” in Proc . ICWSM , 2015 , pp . 574 – 577 . [ 34 ] O . Owoputi , B . O’Connor , C . Dyer , K . Gimpel , and N . Schneider , “Part - of - speech tagging for Twitter : Word clusters and other advances , ” Ph . D . dissertation , School Comput . Sci . , Carnegie Mellon Univ . , Pittsburgh , PA , USA , 2012 . [ 35 ] Brown - Clusters . Twitter Word Clusters . Accessed : Jul . 2 , 2017 . [ Online ] . Available : http : / / www . cs . cmu . edu / ~ ark / TweetNLP / [ 36 ] C . Cortes and V . Vapnik , “Support - vector networks , ” Mach . Learn . , vol . 20 , no . 3 , pp . 273 – 297 , 1995 . [ 37 ] C . - C . Chang and C . - J . Lin , “LIBSVM : A library for support vec - tor machines , ” ACM Trans . Intell . Syst . Technol . , vol . 2 , no . 3 , pp . 27 : 1 – 27 : 27 , 2011 . [ 38 ] E . Colleoni , A . Rozza , and A . Arvidsson , “Echo chamber or pub - lic sphere ? Predicting political orientation and measuring political homophily in Twitter using big data , ” J . Commun . , vol . 64 , no . 2 , pp . 317 – 332 , 2014 . 220 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS , VOL . 6 , NO . 2 , APRIL 2019 [ 39 ] M . Conover , J . Ratkiewicz , M . R . Francisco , B . Gonçalves , F . Menczer , and A . Flammini , “Political polarization on Twitter , ” in Proc . ICWSM , vol . 133 , 2011 , pp . 89 – 96 . [ 40 ] S . Hong and S . H . Kim , “Political polarization on Twitter : Implications for the use of social media in digital governments , ” Government Inf . Quart . , vol . 33 , no . 4 , pp . 777 – 782 , 2016 . [ 41 ] Twitter . Report Abusing Behavior . Accessed : Feb . 7 , 2018 . [ Online ] . Available : https : / / help . twitter . com / en / safety - and - security / report - abusive - behavior [ 42 ] Blockshame Shields you from the Online Mob Just in Case ! Accessed : Feb . 7 , 2018 . [ Online ] . Available : http : / / cse . iitkgp . ac . in / ~ rajesh . basak / blockshame / Rajesh Basak received the M . Tech . degree in net - work and internet engineering from Pondicherry University , Pondicherry , India , in 2014 . He is currently a Research Scholar with the Department of Computer Science and Engineer - ing , IIT Kharagpur , Kharagpur , India . His current research interests include online social networks and machine learning . Shamik Sural ( SM’06 ) received the Ph . D . degree from Jadavpur University , Kolkata , India , in 2000 . He is currently a Professor with the Department of Computer Science and Engineering , IIT Kharagpur , Kharagpur , India . He has authored or co - authored more than 200 papers in reputed international jour - nals and conferences . His current research interests include computer security and data science . Dr . Sural was a recipient of the Alexander von Humboldt Fellowship for experienced researchers . He served as the Chairman for the IEEE Kharagpur Section . He is an Associate Editor of the IEEE T RANSACTIONSON S ERVICES C OMPUTING . Niloy Ganguly received the B . Tech . degree from IIT Kharagpur , Kharagpur , India , in 1992 , and the Ph . D . degree from Bengal Engineering and Science University , Shibpur , India , in 2004 . He was a Post - doctoral Fellow with the Dresden University of Technology , Dresden , Germany . He is currently a Professor with the Department of Computer Science and Engineering , IIT Kharagpur , where he leads the Complex Networks Research Group . His current research interests include complex networks , social networks , peer - to - peer networks , and information retrieval . Soumya K . Ghosh ( M’05 ) received the M . Tech . and Ph . D . degrees from the Department of Com - puter Science and Engineering , IIT Kharagpur , Kharagpur , India , in 1996 and 2002 , respec - tively . He was with the Indian Space Research Organi - zation , Bengaluru , India . He is currently a Profes - sor with the Department of Computer Science and Engineering , IIT Kharagpur . He has authored or co - authored more than 200 research papers in reputed journals and conference proceedings . His cur - rent research interests include spatial data science , spatial web services , and cloud computing .