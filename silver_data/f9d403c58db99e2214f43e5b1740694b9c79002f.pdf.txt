The Art and Practice of Data Science Pipelines A Comprehensive Study of Data Science Pipelines In Theory , In - The - Small , and In - The - Large Sumon Biswas Iowa State University Ames , IA , USA sumon @ iastate . edu Mohammad Wardat Iowa State University Ames , IA , USA wardat @ iastate . edu Hridesh Rajan Iowa State University Ames , IA , USA hridesh @ iastate . edu ABSTRACT Increasingly larger number of software systems today are including data science components for descriptive , predictive , and prescriptive analytics . The collection of data science stages from acquisition , to cleaning / curation , to modeling , and so on are referred to as data science pipelines . To facilitate research and practice on data science pipelines , it is essential to understand their nature . What are the typical stages of a data science pipeline ? How are they connected ? Do the pipelines differ in the theoretical representations and that in the practice ? Today we do not fully understand these architectural characteristics of data science pipelines . In this work , we present a three - pronged comprehensive study to answer this for the state - of - the - art , data science in - the - small , and data science in - the - large . Our study analyzes three datasets : a collection of 71 proposals for data science pipelines and related concepts in theory , a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in - the - small , and a collection of 21 mature data science projects from GitHub to understand data science in - the - large . Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory , in - the - small , and in - the - large . CCS CONCEPTS ‚Ä¢ Software and its engineering ‚Üí Software creation and man - agement ; ‚Ä¢ Computing methodologies ‚Üí Machine learning . KEYWORDS data science pipelines , data science processes , descriptive , predictive ACM Reference Format : Sumon Biswas , Mohammad Wardat , and Hridesh Rajan . 2022 . The Art and Practice of Data Science Pipelines : A Comprehensive Study of Data Science Pipelines In Theory , In - The - Small , and In - The - Large . In 44th Inter - national Conference on Software Engineering ( ICSE ‚Äô22 ) , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA . ACM , New York , NY , USA , 16 pages . https : / / doi . org / 10 . 1145 / 3510003 . 3510057 1 INTRODUCTION Data science processes , also called data science stages as in stages of a pipeline , for descriptive , predictive , and prescriptive analytics are becoming integral components of many software systems today . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA ¬© 2022 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9221 - 1 / 22 / 05 . https : / / doi . org / 10 . 1145 / 3510003 . 3510057 The data science stages are organized into a data science pipeline , where data might flow from one stage in the pipeline to the next . These data science stages generally perform different tasks such as data acquisition , data preparation , storage , feature engineering , modeling , training , evaluation of the machine learning model , etc . In order to design and build software systems with data science stages effectively , we must understand the structure of the data science pipelines . Previous work has shown that understanding the structure and patterns used in existing systems and literature can help build better systems [ 34 , 107 ] . In this work , we have taken the first step to understand the structure and patterns of DS pipelines . Fortunately , we have a number of instances in both the state - of - the - art and practice to draw observations . In the literature , there have been a number of proposals to organize data science pipelines . We call such proposals DS Pipelines in theory . Another source of information is Kaggle , a widely known platform for data scientists to host and participate in DS competitions , share datasets , machine learning models , and code . Kaggle contains a large number of data science pipelines , but these pipelines are typically developed by a single data scientist as small standalone programs . We call such instances DS Pipelines in - the - small . The third source of DS pipelines are mature data science projects on GitHub developed by teams , suitable for reuse . We call such instances DS Pipelines in - the - large . This work presents a study of DS pipelines in theory , in - the - small , and in - the - large . We studied 71 different proposals for DS pipelines and related concepts from the literature . We also studied 105 instances of DS pipelines from Kaggle . Finally , we studied 21 matured open - source data science projects from GitHub . For both Kaggle and GitHub , we selected projects that make use of Python to ease comparative analysis . In each setting , we answer the following overarching questions . ( 1 ) Representative pipeline : What are the stages in DS pipeline and how frequently they appear ? ( 2 ) Organization : How are the pipeline stages organized ? ( 3 ) Characteristics : What are the characteristics of the pipelines in a setting and how does that compare with the others ? This work attempts to inform the terminology and practice for designing DS pipeline . We found that DS pipelines differ signifi - cantly in terms of detailed structures and patterns among theory , in - the - small , and in - the - large . Specifically , a number of stages are absent in - the - small , and the pipelines have a more linear structure with an emphasis on data exploration . Out of the eleven stages seen in theory , only six stages are present in pipeline in - the - small , namely data collection , data preparation , modeling , training , eval - uation , and prediction . In addition , pipelines in - the - small do not have clear separation between stages which makes the maintenance harder . On the other hand , the DS pipelines in - the - large have a a r X i v : 2112 . 01590v3 [ c s . S E ] 14 F e b 2022 ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan more complex structure with feedback loops and sub - pipelines . We identified different pipeline patterns followed in specific phase ( development / post - development ) of the large DS projects . The ab - straction of stages are stricter in - the - large having both loosely - and tightly - coupled structure . Our investigation also suggest that DS pipeline is a well used software architecture but often built in ad hoc manner . We demon - strated the importance of standardization and analysis framework for DS pipeline following the traditional software engineering re - search on software architecture and design patterns [ 70 , 84 , 107 ] . We contributed three representations of DS pipelines that capture the essence of our subjects in theory , in - the - small , and in - the - large that would facilitate building new DS systems . We anticipate our results to inform design decisions made by the pipeline architects , practitioners , and software engineering teams . Our results will also help the DS researchers and developers to identify whether the pipeline is missing any important stage or feedback loops ( e . g . , storage and evaluation are missed in many pipelines ) . The rest of this paper is organized as follows : in section ¬ß 2 , we present our study of DS pipelines in theory . Section ¬ß 3 describes our study of DS pipelines in - the - small . In section ¬ß 4 , we describe our study of DS pipelines in - the - large . Section ¬ß 5 discusses the implications , section ¬ß 6 describes the threats to the validity , section ¬ß 7 describes related work , and section ¬ß 8 concludes . 2 DS PIPELINE IN THEORY Data Science . Data Science ( DS ) is a broad area that brings together computational understanding , inferential thinking , and the knowl - edge of the application area . Wing [ 129 ] argues that DS studies how to extract value out of data . However , the value of data and extrac - tion process depends on the application and context . DS includes a broad set of traditional disciplines such as data management , data infrastructure building , data - intensive algorithm development , AI ( machine learning and deep learning ) , etc . , that covers both the fundamental and practical perspectives from computer science , mathematics , statistics , and domain - specific knowledge [ 13 , 116 ] . DS also incorporates the business , organization , policy and privacy issues of data and data - related processes . Any DS project involves three main stages : data collection and preparation , analysis and modeling , and finally deployment [ 128 ] . DS is also more than statis - tics or data mining since it incorporates understanding of data and its pattern , developing important questions and answering them , and communicating results [ 116 ] . Data Science Pipeline . The term pipeline was introduced by Garlan with box - and - line diagrams and explanatory prose that as - sist software developers to design and describe complex systems so that the software becomes intelligible [ 37 ] . Shaw and Garlan have provided the pipes - and - filter design pattern that involves stages with processing units ( filters ) and ordered connections ( pipes ) [ 107 ] . They also argued that pipeline gives proper semantics and vo - cabulary which helps to describe the concerns , constraints , re - lationship between the sub - systems , and overall computational paradigm [ 37 , 107 ] . By data science pipeline ( DS pipeline ) , we are referring to a series of processing stage s that interact with data , usually acquisition , management , analysis , and reasoning [ 77 , 79 ] . The sequential DS stages from acquisition , to cleaning / curation , to modeling , and so on are referred to as data science pipeline . A DS pipeline may consist of several stages and connections between them . The stages are defined to perform particular tasks and con - nected to other stage ( s ) with input - output relations [ 6 ] . However , the definitions of the stages are not consistent across studies in the literature . The terminology vary depending on the application context and focus . Different study in the literature presented DS pipeline based on their context and desiderata . No study has been conducted to unify the notions DS pipeline and collect the concepts [ 103 ] . While designing a new DS pipeline [ 130 ] , dividing roles in DS teams [ 65 ] , defining software process in data - intensive setting [ 123 ] , identi - fying best practices in AI and modularizing DS components [ 6 ] , it is important to understand the current state of the DS pipeline , its variations and different stages . To understand the DS pipelines and compare them , we collected the available pipelines from the literature and conducted an empirical study to unify the stages with their subtasks . Then we created a representative DS pipeline with the definitions of the stages . Next , we present the methodology and results of our analysis of DS pipelines in theory . 2 . 1 Methodology 2 . 1 . 1 Collecting Data Science Pipelines . We searched for the stud - ies published in the literature and popular press that describes DS pipelines . We considered the studies that described both end - to - end DS pipeline or a partial DS pipeline specific to a context . First , we searched for peer - reviewed papers published in the last decade i . e . , from 2010 to 2020 . We searched the terms ‚Äú data science pipeline ‚Äù , ‚Äú machine learning pipeline ‚Äù , ‚Äú big data lifecycle ‚Äù , ‚Äú deep learning work - flow ‚Äù , and the permutation of these keywords in IEEE Xplore , ACM Digital Library and Google Scholar . From a large pool , we selected 1 , 566 papers that fall broadly in the area of computer science , soft - ware engineering and data science . Then we analyzed each article in this pool to select the ones that propose or describe a DS pipeline . We found many papers in this collection use the terms ( e . g . , ML lifecycle ) , but do not contain a DS pipeline . We selected the ones that contain DS pipeline and extracted the pipelines ( screenshot / de - scription ) as evidence from the article . The extracted raw pipelines are available in the artifact accompanied by this paper [ 7 ] . Thus , we found 46 DS pipelines that were published in the last decade . Besides peer - reviewed papers , by searching the keywords on web , we collected the DS pipelines from US patent , industry blogs ( e . g . , Microsoft , GoogleCloud , IBM blogs ) , and popular press pub - lished between 2010 and 2020 . After manual inspection , we found 25 DS pipelines from this grey literature . Thus , we collected 71 subjects ( 46 from peer reviewed articles and 25 from grey litera - ture ) that contain DS pipeline . We used an open - coding method to analyze these DS pipelines in theory [ 7 ] . 2 . 1 . 2 Labeling Data Science Pipelines . In the collected references , DS pipeline is defined with a set of stages ( data acquisition , data preparation , modeling , etc . ) and connections among them . Each stage in the pipeline is defined for performing a specific task and connected to other stages . However , not all the studies depict DS pipelines with the same set of stages and connections . The studies use different terminologies for defining the stages depending on the context . To be able to compare the pipelines , we had to understand The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA End Training Independent Labeling Train the raters Label one subject Discussion meeting Reconcilingdisagreements Finishedtraining Label next pipeline Perfect agreement Independentre - labeling No Yes Finished Yes No Reconcile all disagreements No Yes Figure 1 : Labeling method for DS pipelines in theory the definitions and transform them into a canonical form . For a given DS pipeline , identifying their stages and mapping them to a canonical form is often challenging . The sub - tasks , overall goal of the project , utilities affect the understanding of the pipeline stages . To counter these challenges , we used an open - coding method to label the stages of the pipelines . Two authors labeled the collected DS pipelines into different criteria . Each author read the article , understood the pipeline , iden - tified the stages , and labeled them . In each iteration , the raters labeled 10 % of the subjects ( 7 - 8 pipelines ) . The first 8 subjects were used for training and forming the initial labels . After each iteration , we calculated the Cohen‚Äôs Kappa coefficient [ 121 ] , identified the mismatches , and resolved them in the presence of a moderator , who is another author . Thus , we found the representative DS pipeline after rigorous discussions among the raters and the moderator . The methodology of this open - coding procedure is shown in Figure 1 . The entire labeling process was divided into two phases : 1 ) training , and 2 ) independent labeling . Training : The two raters were trained on the goal of this project and their roles . We randomly selected eight subjects for training . First , the raters and the moderator had discussions on three sub - jects and identified the stages in their DS pipeline . Thus , we formed the commonly occurred stages and their definitions , which were updated through the entire labeling and reconciliation process later . After the initial discussion and training , the raters were given the already created definitions of the stages and one pipeline from the remaining five for training . The raters labeled this pipeline indepen - dently . After labeling the pipeline , we calculated the agreement and conducted a discussion session among the raters and the moderator . In this session , we reconciled the disagreements and updated the labels with the definitions . We continued the training session until we got perfect agreement independently . The inter - rater agreement was calculated using Cohen‚Äôs Kappa coefficient [ 121 ] . A higher ùúÖ ( [ 0 , 1 ] ) indicates a better agreement . The interpretation of of ùúÖ is shown in Figure 2a . In the discussion meetings , the raters discussed each label ( both agreed and disagreed ones ) with the other rater and moderator , argued for the disagreed ones and reconciled them . In this way , we came up with most of the stages and a representative terminology for each stage including the sub - tasks . Independent labeling : After completing the training session , the rest of the subjects were labeled independently by the raters . The raters labeled the remaining 63 labels : 7 subjects ( 10 % ) in each Range ( ùúÖ ) Agreement level 0 . 00 - 0 . 20 Slight agreement 0 . 21 - 0 . 40 Fair agreement 0 . 41 - 0 . 60 Moderate agreement 0 . 61 - 0 . 80 Substantial agreement 0 . 81 - 1 . 00 Perfect agreement ( a ) Interpretation of Kappa ( ùúÖ ) Iteration # ùúÖ Iteration # ùúÖ 1 0 . 67 6 0 . 91 2 0 . 74 7 0 . 87 3 0 . 82 8 0 . 90 4 0 . 84 9 0 . 94 5 0 . 84 10 0 . 91 ( b ) Agreement in different stages Figure 2 : Labeling agreement calculation of the 9 iterations . The distribution of ùúÖ after each independent labeling iteration is shown in Figure 2b . In each iteration , first , the raters had the labeling session , and then the raters and moderator had the reconciliation session . Labeling . The raters labeled separately so that their labels were private , and they did not discuss while labeling . The raters identi - fied the stages and connections between them , and finally labeled whether the DS pipeline involves processes related to cyber , physi - cal or human component in it . In independent labeling , we found almost perfect agreement ( ùúÖ = 0 . 83 ) on average . Even after high agreement , there were very few disagreements in the labels , which were reconciled after each iteration . Reconciling . Reconciliation happened for each label for the sub - ject studies in the training session , and the disagreed labels for the studies in independent labeling session . In training session , the reconciliation was done in discussion meetings among the raters and the moderator , whereas for the independent labels , reconcilia - tion was done by the moderator after separate meetings with the two raters . For reconciliation , the raters described their arguments for the mislabeled stages . For a few cases , we had straightforward solution to go for one label . For others , both the raters had good arguments for their labels , and we had to decide on that label by updating the stages in the definition of the pipeline . All the labeled pipelines from the subjects are shared in our paper artifact [ 7 ] . Furthermore , after finishing labeling the pipelines stages , we also classified the subject references into four classes based on the overall purpose of the article . First , after a few discussions , the raters and moderator came up with the classes . Then , the raters classified each pipeline into one class . We found disagreements in 6 out of 71 references , which the moderator reconciled with separate meetings with the two raters . Based on our labeling , the literature that we collected are divided into four classes : describe or propose DS pipeline , survey or review , DS optimization , and introduce new method or application . Next , we are going to discuss the result of analyzing the DS pipelines in theory . 2 . 2 Representative Pipeline in Theory The labeled pipelines with their stages are visually illustrated in the artifact Table 3 . We found that pipelines in theory can be both software architecture and team processes unlike pipelines in - the - small and in - the - large . Through the labeling process , we separated those team processes ( 25 out of 71 ) , which are discussed in ¬ß 2 . 4 . RQ1a : What is a representative definition of the DS pipe - line in theory ? From the empirical study , we created a represen - tative rendition of DS pipeline with 3 layers , 11 stages and possible connections between stages as shown in Figure 3 . Each shaded box represents a DS stage that performs certain sub - tasks ( listed under the box ) . In the preprocessing layer , the stages are data acquisition , preparation , and storage . The preprocessing stage study design only ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan Data preparation Feature engineering Communication Interpretation Post - processing Layer Model Building Layer Data acquisition Storage Modeling Training Evaluation Prediction LoadCollectObtainCaptureSurvey ExploreWrangleCleanFilterOrganize PreserveArchiveWarehouseLogRecycle Feature - select - construct LabelAnnotate ClassifyClusterMineAnalyzeProcess TuneOptimize ValidateTestVerifyReview DiscoverDeriveDetermine TransformVisualizeRenderTranslateExplain TransferShareDistributeTransmitPublish InstallExploitServeMonitor Deployment Study design Pre - processing Layer Figure 3 : Concepts in a data science pipeline . The sub - tasks are listed below each stage . The stages are connected with feedback loops denoted with arrows . Solid arrows are always present in the lifecycle , while the dashed arrows are optional . Distant feedback loops ( e . g . , from deployment to data acquisition ) are also possible through intermediate stage ( s ) . Stages of Data Science Pipeline Data Acquisition ( ACQ ) : In the beginning of DS pipeline , data are collected from appropriate sources . Data can be acquired manually or automatically . Data acquisition also involves understanding the nature of the data , collecting relevant data , and integrating available datasets . Data Preparation ( PRP ) : Data are generally acquired in a raw format that needs certain preprocessing steps . This involves exploration and filtering , which helps identify the correct data for further processing . Well prepared data reduces the time required for data analysis and contributes to the success of the DS pipeline . Storage ( STR ) : It is important to find an appropriate hardware - software combina - tion to preserve data so that it can be processed efficiently . For example , Miao et al . used graph database system Neo4j [ 75 ] to build a collaborative analysis pipeline [ 72 ] , since Neo4J supports querying graph data properties . Feature Engineering ( FTR ) : The entire dataset might not contribute equally to decision making . In this stage , appropriate features that are useful to build the model are identified or constructed . Features that are not readily available in the dataset , require engineering to create them from raw data . Modeling ( MDL ) : When data are preprocessed and features are extracted , a model isbuilttoanalyzethedata . Modelbuildingincludesmodelplanning , modelselection , mining and deriving important properties of data . Appropriate data processing strategies and algorithms are selected to create a good model . Training ( TRN ) : For a specific model , we need to train the model with available labeled data . By each training iteration , we optimize the model and try to make it better . The quality of the training dataset contributes to the training accuracy of the model . Evaluation ( EVL ) : After training the model , it is tested with a new dataset which has not been used as training data . Also , the model can be evaluated in real - life scenarios and compared with other competing models . Existing metrics are used or new metrics are created to evaluate the model . Prediction ( PRD ) : The success of the model depends on how good a model can predict in an unknown setup . After a satisfactory evaluation , we employ the model to solve the problem and see how it works . There are many prediction metrics such as classification accuracy , log - loss , F1 - score , to measure the success of the model . Interpretation ( INT ) : The prediction result might not be enough to make a deci - sion . We often need a transformation of the prediction result and post - processing to translate predictions into knowledge . For example , only numerical results do not help much but a good visualization can help to make a decision . Communication ( CMN ) : Different components of the DS system might reside in a distributed environment . So , we might need to communicate with the in - volved parties ( e . g . , devices , persons , systems ) to share and accumulate information . Communication might take place in different geographical locations or the same . Deployment ( DPL ) : The built DS solution is installed in its problem domain to serve the application . Over time , the performance of the model is monitored so that the model can be improved to handle new situations . Deployment also includes model maintenance and sending feedback to the model building layer . Table 1 : Description of the stages in DS pipeline appeared in team process pipelines that comprise requirement for - mulation , specification , and planning , which are often challenging in data science . The algorithmic steps and data processing are done in the model building layer . Modeling does not necessarily imply the existence of an ML component , since DS can involve custom data processing or statistical modeling . Post - processing layer includes the tasks that take place after the results have been generated . The DS pipeline stages are described in Table 1 . RQ1b : What are the frequent and rare stages of the DS pipeline in theory ? The frequency of stage can depend on the Figure 4 : Frequency of pipeline stages in theory focus of the pipeline or its importance in certain context ( ML , big - data management ) . Among 46 DS pipelines ( which are not team processes ) , Figure 4 shows the number of times each stage appears . A few pipelines present stages with broad terminology that fit mul - tiple stage - definitions . In those cases , the pipelines were labeled with the fitted stages and counted multiple times . Modeling , data preparation , and feature engineering appear most frequently in the literature . While modeling is present in 93 % of the pipelines , other model related stages ( feature engineering , training , evaluation , pre - diction ) are not used consistently . Often training is not considered as a separate stage and included inside the modeling stage . Similarly , we found that evaluation and prediction are often not depicted as separate stages . However , by separating the stages and modulariz - ing the tasks , the DS process can be maintained better [ 6 , 103 ] . The pipeline created with the most number of stages ( 11 ) is provided by Ashmore et al . [ 9 ] . On the other hand , about 15 % of the pipelines from the literature are created with a minimal number ( 3 ) of stages . Among them , 80 % are ML processes and falls in the category of DS optimizations . We found that these pipelines are very specific to particular applications , which include context - specific stages like data sampling , querying , visualization , etc . , but do not cover most of the representative stages . A pipeline in theory may not require all representative stages , since it can have novelty in certain stages and exclude the others . However , the representative pipeline provides common terminology and facilitate comparative analysis . Finding 1 : Post - processing layers are included infrequently ( 52 % ) compared to pre - processing ( 96 % ) and model building ( 96 % ) layers of pipelines in theory . Clearly , preprocessing and model building layers are considered in almost all of the studies . In most of the cases , the pipelines do not consider the post - processing activities ( interpretation , com - munication , deployment ) . These pipelines often end with the pre - dictive process and thus do not follow up with the later stages which entails how the result is interpreted , communicated and de - ployed to the external environment . Miao et al . argued that overall lifecycle management tasks ( e . g . , model versioning , sharing ) are largely ignored for deep learning systems [ 73 ] . Previous studies The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA also showed that significant amount of cost and effort is spent in the post - development phases in traditional software lifecycle [ 70 , 90 ] . In data - intensive software , the maintenance cost can go even higher with the high - interest technical debt in the pipeline [ 102 ] . Therefore , post - processing stages should be incorporated for a better under - standing of the impact of the proposed approach on maintenance of the DS pipeline . 2 . 3 Organization of Pipeline Stages in Theory RQ2 : How are pipeline stages connected to each other ? In Fig - ure 3 , for simplicity , we depicted the DS pipeline as a mostly linear chain . However , our subject DS pipelines often have non - linear behavior . In any stage , the system might have to return to the pre - vious stage for refinement and upgrade , e . g . , if a system faces a real - world challenge in modeling , it has to update the algorithm which might affect the data pre - processing and feature engineering as well . Furthermore , the stages do not have strict boundaries in the DS lifecycle . In Figure 3 , two backward arrows , from feature engineering and evaluation , indicate feedback to any of the previous stages . Although in traditional software engineering processes ( e . g . , waterfall model , agile development , etc . ) , feedback loop is not un - common , in DS lifecycle , there are multiple stakeholders and models in a distributed environment which makes the feedback loops more frequent and complex . Sculley et al . pointed that DS tasks such as sampling , learning , hyperparameter choice , etc . are entangled together so that Changing Anything Changes Everything ( CACE principle ) [ 103 ] , which in turn creates implicit feedback loops that are not depicted in the pipelines [ 20 , 35 , 91 , 120 ] . The feedback loops inside any specific layer are more frequent than the feedback loops from one layer to another . Also , a feedback loop to a distant previous stage is expensive . For example , if we do data preparation after evaluation then the intermediate stages also require updates . 2 . 4 Characteristics of the Pipelines in Theory RQ3 : What are the different types of pipelines available in theory ? The context and requirements of the project can influence pipeline design and architecture [ 36 ] . Here , we present the types of pipelines with different characteristics that are available in theory . We classified each subject in our study into four classes based on the overall goal of the article . The most of the pipelines in theory ( 39 % ) are describing or proposing new pipelines to solve a new or existing problem . About 31 % of the pipelines are on reviewing or comparing the existing pipelines . The third group of DS pipelines ( 14 % ) are intended to optimize a certain part of the pipeline . For example , Van Der Weide et al . proposes a pipeline for managing multiple versions of pipelines and optimize performance [ 120 ] . Most of the pipelines in this category are application specific and include very few stages that are necessary for the optimization . Fourth , some research introduce new application or method and present within the pipeline . We observed that there is no standard methodology to develop comparable and inter - operable DS pipelines . Using the labeling methodology shown in Figure 1 , we labeled each pipeline and found three types of DS pipelines in the literature : 1 ) ML process , 2 ) big data management process , and 3 ) team process . ML process : 46 % of all the pipelines we found in the literature are describing machine learning processes . The recent advent of artificial intelligence , supervised learning and deep learning has led to more DS systems that involve ML components . The pipelines in this category emphasize the algorithmic process , learning pat - terns , and building predictive models . However , the post - processing stages are rare in these type of pipelines . The ML pipelines are often thought of as algorithmic process in the laboratory scenario . But as mentioned in [ 9 ] , incorporating the post - processing stages would be desired to ensure safe real - world deployment of such pipelines . Big data management : The references in this category present DS pipelines that manage a large amount of data or describes a framework ( software - hardware infrastructure ) for data processing but do not contain machine learning components in the pipeline . Processing large amount data often requires specific algorithms and engineering methods for efficiency and further processing . We found that 18 % of all the subject studies fall in this category . Team process : We also found some DS pipelines that are not describing DS software architecture . These pipelines describe work - flow of human activities that needs to be followed in a DS pipeline . These studies present a high - level view for building DS component in a team environment . The data science teams require specific expertise and management to build successful DS pipelines [ 6 , 65 ] . In this paper , in ¬ß 3 and ¬ß 4 , we are only focusing on DS pipeline as software architecture , and therefore , we did not compare the team process pipelines in the rest of this section . Finding 2 : Most of the pipelines in - theory involve cyber and phys - ical components , only a few with human processes in the loop . We identified whether the pipelines involve cyber , physical or hu - man process , using our labeling process described in section ¬ß 2 . 1 . 2 . Cyber processes refer to activities that involve automated systems and machinery computations . Since modern DS systems involves large amount of data and requires extensive computation , all of the pipelines include cyber component in it . Physical processes include the activities which require real - world connections with the system . For example , collecting data using mobile sensors or cam - eras is a physical process . Although 23 % of the big data pipelines include physical processes , only 9 % of the ML pipelines include that in the pipeline . In many DS systems , developers or researchers participate in the pipelines actively to make decisions that need human interventions [ 116 , 120 ] . For example , in many DS systems , analytical model validation , troubleshooting , data interpretation is necessary which requires human involvement . However , only 13 % of the pipelines acknowledged human involvement in the pipeline . 3 DS PIPELINE IN - THE - SMALL Similar to the DS pipelines in large systems and frameworks , for a very specific data science task ( e . g . , object recognition , weather forecasting , etc . ) , programmers build pipeline . Different stages of the program perform a specific sub - task and connect with the other stages using data - flow or control - flow relations . In this section , we described such DS pipelines in - the - small . 3 . 1 Methodology We collected 105 DS programs from Kaggle competition notebooks [ 53 ] . Kaggle is one of the most popular crowd - sourced platforms for DS competitions , owned by Google . Besides participating in ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan 1 . Featured 2 . Research 3 . Recruitment 4 . Masters 5 . Analytics 6 . Playground 7 . Getting started 105 top - rated DS programs API : Stage Dictionary Parse and extract APIs CreatePipeline of Stages Competitions PRP PRD MDL TRN MDL PRP ACQ For each competition , select most voted solution Filter out solutions - votes < 10 - not end - to - end pipeline Figure 5 : The pipeline creation process for Kaggle programs competitions , data scientists , researchers , developers collaborate to learn and share DS knowledge in variety of domains . The users and organizations can host a DS competition in Kaggle to solve real - world problems . A competition is accompanied by a dataset and prize money . Many Kaggle solutions have resulted in impactful DS algorithms and research such as neural networks used by Hinton and Dahl [ 23 ] , improving the search for the Higgs Boson at CERN [ 51 ] , etc . We chose Kaggle solutions to analyze DS pipeline for three reasons : 1 ) all programs perform a DS task and provide solution to a well specified problem associated with a dataset , 2 ) solutions with the highest number of votes are well accepted solutions for a specific problem , and 3 ) the problems cover a wide range of domains . There are 331 completed competitions in Kaggle to date . They categorized the competitions into Featured , Research , Recruitment , Masters , Analytics , Playground and Getting started . We collected solutions of all the competitions from each category except Getting started and Playground ( these two categories are intended to serve as DS tutorials and toy projects ) . First , we filtered the competitions for which there are solutions available ( many old competitions do not contain any public solution ) . We found 138 such competitions . For a given competition problem , we selected the most voted so - lution which has at least 10 votes . Thus , we got 105 top - rated DS solutions for analyzing pipelines in - the - small . This selection and pipeline creation process is shown in Figure 5 . All of the DS programs are written in Python using ML libraries like Keras , Scikit - learn , Tensorflow , etc . These packages provide high - level Application Programming Interfaces ( APIs ) for perform - ing a specific task on data or model . We parsed the programs into Abstract Syntax Tree ( AST ) and collected all the API calls from the programs . Then the functionality of an API is used to identify the stage of the pipeline . We extracted the temporal order of API calls to identify the stages . Standard static analysis of the Python pro - grams facilitate the extraction process . Our analysis suggests that the DS programs follow a linear structure with less than 4 % AST nodes being conditional or loops . Wang et al . proposed a similar ap - proach for extracting external dependencies in Jupyter Notebooks by creating an API database and analyzing AST [ 125 ] . We created a dictionary by mapping each API collected from the programs , to one of the 11 stages of the DS pipeline described in section ¬ß 2 . During the mapping , we excluded the generic APIs from the dictionary . For example , model . summary ( ) is used to print the model parameters and does not represent any stage of the pipeline . For creating the dictionary , we taken a two - fold approach . First , we understand the context of the program and API usage . Second , Data Acquisition Data Preparation Modeling Training Evaluation Prediction Figure 6 : Pipeline in - the - small extracted from API usages 98 102 73 76 38 74 ACQ PRP MDL TRN EVL PRD Figure 7 : Frequency of pipeline stages in - the - small we look at the API documentation to confirm the corresponding pipeline stage . We found that DS APIs are definitive in their oper - ations and well - categorized by the library . For example , the APIs in Keras [ 60 ] and Scikit - learn [ 61 ] are grouped into preprocess - ing , models , etc . Our API - dictionary was manually validated by a second - rater and moderator who labeled DS pipelines in section ¬ß 2 . Then , we built a tool which takes the API dictionary and DS program , and automatically creates the DS pipeline . For a sequence of APIs with the same stage , we abstracted them into a single stage . As an example , Figure 5 shows a DS pipeline created from a Kaggle solution [ 54 ] . Each stage in the pipeline ( e . g . , ACQ , PRP ) represents one or more API usages . The arrows in the pipeline denote the tem - poral sequence of stages . Note that , one stage can appear multiple times in a pipeline . The API dictionary , Kaggle programs , and tool to generate the pipelines is shared in the paper artifact [ 7 ] . 3 . 2 Representative Pipeline in - the - Small RQ4 : What are the stages of DS pipeline in - the - small ? Among the 11 pipeline stages described in Figure 3 , we found only 6 stages in the DS programs that are depicted in Figure 6 . Other stages ( e . g . , storage , feature engineering , interpretation , communication , deploy - ment ) are not found in these programs because these stages occur while building a production - scale large DS system and often not present in the DS notebooks . Therefore , the pipeline in DS programs consists of the subset of pipeline stages in theory . We summarized the frequency of each stage of the DS programs in Figure 7 . Among 105 programs , data acquisition and data prepa - ration are present in almost all of them . Surprisingly , modeling is present in only 70 % of the programs . We found that , in many pro - grams , no modeling APIs had been used because developers did not use any built - in ML algorithm from libraries , e . g . , LogisticRegres - sion , LSTM , etc . In these cases , the developers use data - processing APIs on the training data to build custom model , e . g . , this note - book [ 55 ] uses data preparation APIs to produce results . To enable more abstraction of the stages in these pipelines , further modular - ization is necessary , which has been investigated in RQ8 . Finding 3 : Evaluation stage is infrequent , appearing only in 36 % of the pipelines in - the - small . Evaluation is a tricky stage of the DS pipeline . Developers have to choose the appropriate metric and methodology to evaluate their model . Based on the evaluation result , the model is updated over multiple iterations . We found that , besides using metrics , in many The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA 0 % 20 % 40 % 60 % 80 % 100 % Before After Before After Before After Before After Before After Before After ACQ PRP MDL TRN EVL PRD ACQ PRP MDL TRN EVL PRD Figure 8 : Stages occurring before and after each stage cases , evaluation requires human understanding and comparison of the result produced by the model . The reason for having less number of evaluation stage in the pipeline is that often the develop - ers evaluate the performance by plotting and visualizing the result . Since the visualization APIs are not considered as evaluation stage , we found this stage less frequently in pipelines . Also , many pro - grams directly go to the prediction stage without going to evaluation stage at all . Furthermore , notebooks are often used for experimen - tation purposes so that many computations are performed during development but eliminated when the notebooks are shared [ 62 ] . For example , one developer might try a number of classifiers and evaluate their accuracy . After finding the best performing classi - fier , it can be the only one shared in the notebook . Therefore , we experienced many missing stages in the pipeline in - the - small . The complex DS tasks require several computations which might not be used in producing the final prediction , but definitely should be considered as part of the pipeline . 3 . 3 Pipeline Organization in the Small RQ5 : How are the stages connected with each other in pipe - line in - the - small ? To answer RQ5 , we considered each occurrence of the stages in a DS program and looked at its previous and next stage . In Figure 8 , we showed which stages are followed or preceded by each stage . We found that data preparation can occur before or after all other stages . Apart from that , data acquisition is followed by data preparation most of the time , which in turn is followed by modeling . Modeling is followed mostly by training , which in turn is followed by prediction . Evaluation is mostly surrounded by prediction and data preparation . From Figure 8 , we can also find some most occurring feedback loop : evaluation to preparation , evaluation to modeling and prediction to modeling . Data preparation tasks ( e . g . , formatting , reshaping , sorting ) are not limited to just before the modeling stage , rather it is done on a whenever - needed basis . For example , in the following code snippet from a Kaggle competition [ 56 ] , while creating model - layers , data preprocessing API has been called in line 2 . 1 x = Conv2D ( mid , ( 4 , 1 ) , activation = ' relu ' , padding = ' valid ' ) ( x ) 2 x = Reshape ( ( branch _ model . output _ shape [ 1 ] , mid , 1 ) ) ( x ) 3 x = Conv2D ( 1 , ( 1 , mid ) , activation = ' linear ' , padding = ' valid ' ) ( x ) 4 x = Flatten ( name = ' flatten ' ) ( x ) 5 head _ model = Model ( [ xa _ inp , xb _ inp ] , x , name = ' head ' The modeling stage is always surrounded by other stages of the pipeline . However , there is often a loop around modeling , training , evaluation , and prediction . Modeling often repeats many times to improve the model over multiple iterations . For example , in the following Kaggle code snippet [ 57 ] , the model is created and trained multiple times to find the best one . 1 random _ forest = RandomForestClassifier ( n _ estimators = 100 , random _ state = 50 , verbose = 1 , n _ jobs = ‚àí1 ) # Modeling 2 random _ forest . fit ( train , train _ labels ) # Train 3 . . . 4 poly _ features = scaler . fit _ transform ( poly _ features ) 5 poly _ features _ test = scaler . transform ( poly _ features _ test ) 6 random _ forest _ poly = RandomForestClassifier ( n _ estimators = 100 , random _ state = 50 , verbose = 1 , n _ jobs = ‚àí1 ) # Modeling 7 random _ forest _ poly . fit ( poly _ features , train _ labels ) # Training 8 pred = random _ forest _ poly . predict _ proba ( poly _ features _ test ) [ : , 1 ] Finding 4 : Stages of pipelines in - the - small are often tangled with each other . All of the DS programs fail to maintain a good separation of con - cerns [ 28 ] between stages . Strong abstraction boundaries help to make the program modular and easy - to - maintain [ 81 , 82 , 84 ] . In addition , a good DS solution should not only compute better pre - dictive result , but also facilitate software engineering activities e . g . , debugging , testing , monitoring [ 42 ] . However , we found that stages are often tangled with other stages [ 18 , 64 , 88 ] across the pipelines . The code for one stage is interspersed with the code for other stages . For example , while building the deep learning network ( modeling ) , the developers often switch to different data preparation tasks , e . g . , reshaping , resizing [ 48 , 49 ] , which tangles data preparation concern with the modeling concern . We observed some early attempts to adopt modular design practices . For instance , this notebook [ 58 ] separated code into different high - level stages , namely , prepara - tion , feature extraction , exploratory data analysis ( EDA ) , topic model , etc . These high - level pipelines can improve the abstraction , which further enable the maintainability , and reusability [ 97 ] . In some scenarios , reuse or maintenance might not be desired for pipelines in - the - small . However , to enhance readability [ 62 ] and repeatability [ 42 ] and ease of testing , debugging or repairing [ 126 , 127 ] , more attention on modular design practices is needed for DS pipelines . Finding 5 : Data preparation stage is occurring significant number of times between any two stages of pipelines in - the - small , which is causing pipeline jungles . We found that new data sources are added , new features are identi - fied , and new values are calculated incrementally in the pipeline which evolves organically . This results in a large number of data preprocessing tasks like sampling , joining , resizing along with ran - dom file input - output . This is called pipeline jungles [ 103 ] , which causes technical debt for DS systems in the long run . Pipeline jun - gles are hard to test and any small change in the pipeline will take a lot of effort to integrate . The situation gets worse in case of larger DS pipelines , where several data management activities ( e . g . , clean , serve , validate ) are necessary through the pipeline in differ - ent stages [ 85 , 86 ] . The recommended way is to think about the pipeline holistically and scrape the pipeline jungle by redesigning it , which in turn takes further engineering effort [ 103 ] . We found that the large DS projects , which are discussed in ¬ß 4 , isolate the data preparation tasks into separate files and modules [ 17 , 100 , 118 , 134 ] , which alleviates the pipeline jungles problem . So , DS pipeline in - the ‚Äì small needs further IDE ( e . g . , Jupyter Notebook , etc . ) support and methodologies for code isolation and modularization . ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan Load Data and Library Feature Engineering Modeling Training Evaluation & Visualization Prediction EDA & Visualization Data Preparation Figure 9 : Representative data science pipeline in - the - small 3 . 4 Characteristics of Pipelines in - the - Small RQ6 : What are the patterns in pipeline in - the - small and how it compares to pipeline in theory ? We have not found many stages from Figure 3 , e . g . , feature engineering , interpretation , com - munication , in pipeline in - the - small . One reason is that the low - level pipeline extracted from the API usages cannot capture some stages . For example , even if a developer is conducting feature engi - neering , the used APIs might be from the data preparation stage . Fortunately , we found many Kaggle notebooks that are organized by the pipeline stages . We visited all the 105 Kaggle notebooks in our collection and extracted these high - level pipelines manually . Unlike the low - level pipelines ( extracted using API usages ) , a high - level pipeline consists of the stages abstracted by the developers . The Kaggle notebooks follow literate programming paradigm [ 97 , 122 ] , which allows the developers to describe code using rich text and separate them into sections . We found that 34 out of 105 note - books divided the code into stages . We collected those stages from the Kaggle notebooks . Furthermore , we labeled these notebooks into the 11 stages from DS pipeline in theory by two raters , and extracted the stages that are not present in theory . The extracted high - level pipelines and labels are available in the paper artifact [ 7 ] . We observed that no notebooks specify these stages : storage , interpretation , communication , and deployment . These DS programs are not production - scale projects . Therefore , they do not include the post - processing stages in the pipeline . The most common stages are modeling ( 79 % ) , data preparation ( 62 % ) , data acquisition ( 53 % ) , and feature engineering ( 35 % ) , which is aligned with the finding of DS pipeline in theory . In addition , we found these stages which are not present in theory : library loading , exploratory data analysis ( EDA ) , visualization . Among them EDA has been used most of the times ( 43 % ) and covered the most part of those pipeline . Before going to the modeling and successive stages , a lot of effort is given on understanding the data , compute feature importance , and visualize the patterns , which help to build models quickly in later stages [ 9 ] . Furthermore , some notebooks present library loading as separate stage . We observed that choosing appropriate library / framework and setting up the environment is an important step while devel - oping pipeline in - the - small . We also found that data visualization is an recurring stage mentioned by the developers . Visualization can be done for EDA or feature engineering ( before modeling ) , or for evaluation ( after modeling ) . Based on these observations we updated the representative pipeline in - the - small in Figure 9 . The high - level pipeline provides an overall representation of the sys - tem , which can be leveraged to design software process . It would be beneficial for the developers to close the gap between the low - level and the high - level pipeline by identifying the tangled stages . 4 DS PIPELINE IN - THE - LARGE The DS solutions described in the previous section are specific to a given dataset and a well - defined problem . However , there are many Table 2 : GitHub projects for analyzing pipeline in - the - large ProjectName Purpose # Files # AST LOC Autopilot [ 5 ] Pilotacarusingcomputervision 36 11185 348 CNN - Text - Classification [ 17 ] Sentenceclassification 69 47797 11 . 4K Darkflow [ 118 ] Real - timeobjectdetectionandclassification 1025 655670 8 . 6K DeepANPR [ 31 ] Automaticnumberplaterecognition 64 70464 10 . 8K DeepTextCorrector [ 80 ] Correctinputerrorsinshorttext 47 50770 3 . 0K FaceClassification [ 8 ] Real - timefaceandemotion / genderdetection 292 117901 35 . 3K FaceNet [ 100 ] Facerecognition 1352 1889529 18 . 2K KittiSeg [ 115 ] Roadsegmentation 276 187143 4 . 8K LSTM - Neural - Network [ 10 ] Predicttimeseriesstepsandsequences 24 11434 1 . 2K MaskR - CNN [ 2 ] Objectdetectionandinstancesegmentation 256 1567786 15 . 6K MobileNetSSD [ 89 ] Objectdetectionnetwork 28 21272 25 . 6K MTCNN [ 21 ] Jointfacedetectionandalignment 153 121138 219 . 7K Object - Detector - App [ 24 ] Real - timeobjectrecognition 215 318534 47 . 9K Password - Analysis [ 98 ] Analyzealargecorpusoftextpasswords 148 67870 3 . 6K PersonBlocker [ 132 ] Blockpeopleinimages 12 44517 977 QANet [ 134 ] Machinereadingcomprehension 83 107669 2K Speech - to - Text - WaveNet Sentencelevelenglishspeechrecognition 32 18626 5 . 1K Tacotron [ 83 ] Text - to - speechsynthesis 114 58845 1 . 4K Text - Detection - CTPN [ 95 ] Textdetection 640 257083 18 . 4K TF - Recomm [ 112 ] Recommendationsystems 17 7789 535 XLNet [ 137 ] Languageunderstanding 36 143172 11 . 5K DS projects which are large , not limited to a single source file , and contains multiple modules . These solutions are intended to solve more general problems which might not be specific to a dataset . For example , the objective of the Face Classification project in GitHub [ 8 ] is to detect face from images or videos and classify them based on gender and emotion . This problem is not specific to a particular dataset and the scope is broader compared to the Kaggle solutions . We collected such top - rated DS projects from GitHub to analyze DS pipeline in - the - large . 4 . 1 Methodology Biswas et al . published a dataset containing top rated DS projects from GitHub [ 14 ] . From the list of projects in this dataset , we filtered mature DS projects having more than 1000 stars . Thus , we found 269 mature GitHub projects . However , there are many projects in this list which are DS libraries , frameworks or utilities . Since we want to analyze the pipeline of data science software , we removed those projects . Finally , we also removed the repositories which serve educational purposes . Thus , we found a list of 21 mature open - source DS projects . The list of projects , and their purpose are shown in Table 2 . For each project , we created two pipelines : high - level pipeline and low - level pipeline . For creating the high - level pipeline , we manually checked the project architecture , module structure and execution process . This gave us a good understanding of the source file organization and linkage between modules . After identifying the high - level pipeline and execution sequences of the source files , we used the same API based method used to analyze Kaggle programs in the previous section , to create low - level pipeline of these GitHub projects . The methodology of selecting and extracting pipelines from the GitHub projects is shown in Figure 10 . For example , the project QANet [ 134 ] is intended to do machine reading comprehension . Here , Python has been used as the primary The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA projects for data science High level pipeline 21 MatureGithubProjects Low level pipeline Filter out projects - stars < 1000 - libraries - utility - tutorials Analyzepipeline architecture Source code Figure 10 : Pipeline creation process for GitHub projects language , and shell script has been used for data downloading and project setup . The high - level pipeline for QANet includes the stages : data acquisition , data preparation , modeling , training , evaluation and prediction . In the beginning , config . py file integrates the modules ( preparation , modeling , and training ) and provides an interface to configure a model by specifying dataset and other parameters . Then , the file evaluate . py is executed to perform the evaluation and prediction . For the low - level pipeline , for a specific file , we used the API based analysis to generate the pipeline , which was used to analyze pipeline in - the - small . For instance , in the project QANet , although model . py serves modeling at a high level , it also does data preparation , training , and evaluation , when APIs are considered . In addition to the pipeline stages , we also identified a few other properties of each project : 1 ) number of contributors , 2 ) AST count , 2 ) technology / language used , 3 ) entry points and 4 ) execution sequence . We leveraged the Boa infrastructure [ 29 , 30 ] to analyze the different properties of the projects . These properties helped us to categorize and analyze the pipeline in - the - large . The details of the projects are available in the paper artifact [ 7 ] . The projects are from various domains : object detection , face classification , automated driving , speech synthesis , number plate recognition , predict time series sequence , etc . The number of devel - opers in each project ranges between 1 and 40 with an average of 8 . Among 21 projects , 16 of them are developed by teams and 5 of them are developed by individuals . The primary language used to develop these projects is Python . 4 . 2 Representative Pipeline in - the - Large Compared to the Kaggle programs , we found a significant differ - ence in the pipeline of large DS projects . Because of the larger size of the projects , the pipeline architecture is different . All the projects contain multiple source files for handling different tasks ( e . g . , modeling , training ) and about 50 % of the projects organize the source files into modules ( e . g . , utils , preprocessing , model , etc . ) . RQ7 : What is the representative DS pipeline in - the - large ? Each of the projects contains six stages described in Figure 6 : ac - quisition , preparation , modeling , training , evaluation , and prediction . However , since the projects are not coupled to a specific dataset and they solve a more general problem , the projects are not lim - ited to one single pipeline . We found that the pipeline of each project is divided into two phases : 1 ) development phase and 2 ) post - development phase , which is depicted in Figure 11 . In development phase , the main goal is to build a model that solves the problem in general . A base dataset is used to build the model that would be used for other future datasets . After completing a modeling , training , evaluation loop , the final model is created and saved as an artifact . Afterwards , the projects also create model interfaces , which lets the user modify and exploit the model in the post - development phase . Finally , the model artifact is saved as a source file or some model archiving formats . For example , the Modify model A c qu i s i t i on Training Modeling Model artifact Training P r epa r a t i on P r ed i c t i on Evaluation Evaluation Training Evaluation Trained model Development phase Post - development phase Figure 11 : DS pipeline in - the - large . Development phase ( top ) runs during model building and post - development phase ( bottom ) runs for making prediction . project Person - Blocker [ 132 ] and Speech - to - Text - WaveNet [ 66 ] saved the model in the source file ( model . py ) and lets the users train the model in the next phase . On the other hand , the project KittiSeg [ 115 ] and Autopilot [ 5 ] saved the built model artifact in JSON format ( . json ) and checkpoint format ( . ckpt ) respectively . We observed that the evaluation and prediction is often not the main goal in this phase ; rather , building an appropriate model and making it available for further usage is the central activity . In post - development phase , the users access the pre - built model and use that for prediction . After acquiring data , a few preprocessing steps are needed to feed the model . In all of the projects under this study , we found that the development phase is similar . However , we identified three different patterns in the post - development phase which are shown in Figure 11 . First , the users can modify the model by setting its hyperparameters and use that to make prediction on a new dataset . Second , the users can use the model as - it - is and train the model on the new dataset to make prediction . Third , the users can also download the pre - trained model and directly leverage that for prediction . Finally , at the end of this phase , the prediction result is obtained . The post - development phase in the pipeline enabled software reusability of the models . All of these projects have instructions in their readme or documentation explaining the usage and customiza - tion . For example , the project Deep ANPR [ 31 ] provides instructions for obtaining large training data , retraining the models , and build it for prediction . However , not all the projects enable reusability in the development pipelines . Only a few of them provides access to the modules by importing in new development scenario . For instance , Darkflow [ 118 ] let users access the darkflow . net . build module and use it in new application development . To increase the reusability of DS programs , it would be desired to consider similar access to the development pipeline of these large projects . 4 . 3 Organization of DS Pipeline in - the - Large RQ8 : How are the stages connected in pipeline in - the - large ? The abstraction in DS projects is stricter than the DS programs described in ¬ß 3 . The projects are built in a modular fashion , i . e . , one source file for a broad task ( e . g . , train . py , model . py ) . How - ever , inside one specific file , there are many other possible stages , especially data preprocessing appears inside all the source files . In addition , the module connectivity is not linear . All of the modules use external libraries for performing different tasks . As a result , ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan there are a lot of interdependencies ( both internal and external ) in the DS pipeline . One immediate difference of these pipelines with traditional software is DS pipelines are heavily dependant on the data . For example , the project Speech - to - Text - WaveNet [ 66 ] requires a certain format of data . When we want to use that in a new situa - tion , the data properties might be different . So , the usage pipelines would have a few additional stages . In some cases , the original pipeline is modified . Here , there are many sub - pipelines work to - gether to build a large pipeline . However , we have not found any framework or common methodology these software are using . The different patterns of DS pipelines seek more advanced methodology or framework to build DS pipeline and release for production . 4 . 4 Characteristics of Pipelines in - the - Large RQ9 : What are the patterns found in the pipelines ? The pipe - lines found in this setting can be categorized into 1 ) loosely coupled and 2 ) tightly coupled , based on their modularity . A high number of contributors in the project resulted in loosely coupled pipelines . We found the loosely coupled ones are designed in a modular fashion and one module ( e . g . , data cleaning , modeling ) is designed to be used by other modules . Usually , there are multiple entry - points in a loosely coupled pipeline and user has more flexibility . On the other hand , in a tightly coupled pipeline , the modules are stricter and integrated tightly with other modules . There is only one or two entry - points to the pipeline , which automatically calls the other modules . We found that the projects with 6 or more contributors ( ‚àº 75 % ) followed a loosely coupled architecture and projects with 1 to 5 contributors followed a tightly coupled architecture . Finding 6 : There is need for integration and deployment tools for pipelines in - the - large but no common framework is used in practice . Although all the project under this study are written using Python , no project is using any common tool that integrates the DS modules and provides interface to the pipeline . Today , contin - uous integration and deployment ( CI / CD ) tools are widely used in traditional software lifecycle to automate compilation , build - ing , and testing [ 43 , 59 ] . Additionally , from our subject studies of pipelines in theory , we found some CI / CD tools designed for ML pipelines available [ 44 , 74 , 76 ] . Surprisingly , here we found no projects in pipeline in - the - large are using any CI / CD tools . How - ever , the projects demonstrate the need of CI / CD in the repositories . In most of the projects , the environment setup and access to func - tionalities are configured through command lines scripts [ 5 , 95 ] . Some projects used docker container [ 8 , 66 , 100 , 132 ] to set up the environment and run the pipeline . A few others used Python note - books that call different modules to integrate the pipeline stages [ 2 , 24 , 80 ] . 7 out of 21 projects used shell script for integration ( e . g . , sending HTTP request to download data , model reuse , etc . ) [ 89 , 134 ] . Although CI / CD frameworks e . g . , TravisCI , GitHub Ac - tions , Microsoft Azure DevOps are well established for traditional software such as web applications , several challenges remain for DS pipelines . Karla≈° et al . outlined the probabilistic nature of ML testing as a major CI / CD challenge and pointed out the gap between recent theoretical development of CI / CD in DS and their usage in practice [ 59 ] . Hence , further research is needed to investigate the practical challenges of using CI / CD in data science projects . 5 DISCUSSION Through our survey , empirical study , and analysis , we presented the state of data science pipeline that describes its semantics , design concerns , and the overall computational paradigm . Furthermore , the findings show the importance of studying the pipeline structure reminiscing the traditional software engineering works on design patterns and architecture . In Theory : We presented all the representative stages and sub - tasks that inform the terminology of DS pipelines to be used in future works . By comparing with the available pipeline categories e . g . , ML process , big data , and team processes , similarities and diver - gences can be directly identified . The presence of implicit feedback loops and lack of post - processing stages suggest ad hoc pipeline construction at the present time . This paper takes the first step towards comparable and reusable pipeline construction . In - The - Small : The novel API - based analysis can be utilized for mining , extracting , and statically analyzing pipelines . We also elicited the notion of high - level and low - level pipelines , where the high - level abstraction has more similarity with that in theory . However , low - level pipelines exhibit many differences such as miss - ing some stages , sparse data preparation , lack of modularization . The gap between low - level pipeline and its presentation in high - level can be reduced by making pipeline specific features available in development environment e . g . , pipeline template in Jupyter Note - book . Additionally , the low - level pipelines often had an important stage exploratory data analysis missing which incurs much time and effort . Pipeline versioning techniques that consider data , model , and source code will facilitate storing such intermediate stages . In - The - Large : Different pipeline patterns emerged in develop - ment and post - development phase of the large projects , which suggest creating separate developer - centric and user - centric pipeline structure . In tightly - coupled projects , the abstraction of stages are contingent upon the project - specific requirements and internal / ex - ternal dependencies , whereas , in loosely - coupled projects , opportu - nities remain to build reusable sub - pipelines that span over project boundaries . Finally , there is a need for building automated CI / CD tools for data science specific testing , deployment , and maintenance . To researchers and tool builders . ( 1 ) Modularization of DS pipeline into stages is challenging over all three representations . Further works are needed for standardization of pipeline architec - ture e . g . , defining the interfaces of stages , enumerating externally visible properties , identifying domain - specific constraints , to de - velop reusable and interoperable DS pipelines . ( 2 ) We showed po - tentials for automatic pipeline analysis framework based on static analysis and API specifications . A few future directions would be mining ( sub - ) pipelines patterns , build AutoML pipelines [ 78 ] , and analyzing evolution . ( 3 ) We confirmed several antipatterns of pipelines that call for actions e . g . , CACE principle , pipeline jungles , scarce post - processing , implicit feedback loop , CI / CD challenges . ( 4 ) Pipeline specific tool support is needed such as version control for data and models , storing intermediate results between stages . To data scientists and engineers . ( 1 ) Pipelines are often built for a prototype in - the - small , which might not scale to a production level system . A well - designed pipeline in the early stage will help to identify key components , estimate cost , optimize , and manage risks better in the lifecycle . ( 2 ) The representative views of pipelines The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA will serve as a checklist of stages and their connections . ( 3 ) Data and algorithms being the focus of DS pipeline , preprocessing and modeling activities are well understood and practiced by data scien - tists . However , they should emphasize more on including rigorous evaluation beyond accuracy such as robustness and fairness [ 15 , 16 ] . ( 4 ) Many people with diverse backgrounds are involved in a DS pipeline . A pipeline with human - in - the - loop approach will benefit identifying collaboration points , decomposing tasks , and manage transdisciplinary teams . For example , a pipeline can encourage data scientists to choose a modeling technique that is maintain - able . ( 5 ) Future work is necessary to identify the interactions of DS pipeline with the real world i . e . , which stages receive inputs , when a checkpoint is saved , how results are disseminated , etc . 6 THREAT TO VALIDITY For building the pipelines from DS programs , we relied on the APIs . One threat might be , what happens if the developer does not use any API for completing a stage in the program . We examined this possibility and found that DS programs are heavily dependent on libraries and external APIs and ML tasks are always performed using library APIs . Additionally , we validated the API - to - stage dictionary with the API documentation and manual verification . Another possible threat is that the Kaggle solutions might not be representative . We adopted a two - fold strategy to mitigate that threat . First , we selected the solutions with the most number of votes and at least 10 votes . Second , we manually verified each program whether it is an end - to - end DS solution . Since some most voted solutions are only for introduction and exploratory analysis of the dataset , by manual verification , we excluded those programs . The GitHub projects are also taken from a previously published dataset containing DS repositories . We further filtered them based on the number of stars and whether they perform a DS task . Moreover , sincethechosenDSprogramsfromKaggle andGitHub are using Python as the primary language , another question might be on the generalization of them as DS programs . According to GitHub and Stack Overflow , Python has become the most growing language in recent times [ 47 , 93 ] . In data science , Python is the most used language because of the availability of numerous ML , DL and data analysis packages such as Pandas , NumPy , TensorFlow , Keras , Caffe , Theano , Scikit - Learn and many more . 7 RELATED WORK Many studies presented ML pipeline in their own context , which can not be generalized for all DS systems . Garcia et al . focused on building an iterative process with three main phases : develop - ment , training and inference . They described the interpretation of data and code while integrating the whole lifecycle [ 36 ] . Polyzotis et al . presented the challenges of data management in building production - level ML pipeline in Google around three broad themes : data understanding , data validation and cleaning , and data prepa - ration [ 85 , 86 ] . They also provided an overview of an end - to - end large - scale ML pipeline with a data point of view . Carlton E . Sapp defined ML concepts , business challenges , stages in the lifecycle , roles of DS teams with comprehensive end - to - end ML architec - ture [ 101 ] . This gives us a holistic understanding of the business processes ( e . g . , acquire , organize , analyze , deliver ) of a DS project . A few other studies try to capture the DS process by surveying and interviewing developers . Roh et al . surveyed the data collection techniques in the field of big data . They presented the workflow of data collection answering how to improve data or models in an ML system [ 94 ] . Another study identified the software engineering practices and challenges in building AI applications inside Microsoft development teams [ 6 ] . They found some key differences in AI software process compared to other domains . They considered a 9 - stage workflow for DS software development . Hill et al . interviewed experienced AI developers and identified problems they face in each stage [ 42 ] . They also tried to compare the traditional software process and the AI process . Zhou presented her own view to build a better ML pipeline [ 139 ] . They presented three challenges in building ML pipelines : data quality , reliability and accessibility . Some articles described ML applications and frameworks which present DS pipelines from industry . For example , Databricks pro - vides high - level APIs for programming languages [ 44 ] . Team Data Science Process ( TDSP ) is an agile and iterative process to build intelligent applications inside Microsoft corporation [ 104 ] . In a US patent , the authors compared two data analytic lifecycles [ 116 ] , and presented the difference in the set of parameters with respect to time and cost . CRoss Industry Standard Process for Data Mining ( CRISP - DM ) is a 6 - stage comprehensive process model for data mining projects across any industry [ 130 ] . Google Cloud Blog described the workflow of an AI platform [ 40 ] . They explained tasks completed in each stage with respect to Google Cloud and TensorFlow [ 1 ] . Although there are many papers in the literature presenting DS pipeline , there is no comprehensive study that tries to understand and compare DS pipelines in theory and practice . 8 CONCLUSION Many software systems today are incorporating a data science pipeline as their integral part . In this work , we argued that to facili - tate research and practice on data science pipelines , it is essential to understand their nature . To that end , we presented a three - pronged comprehensive study of data science pipelines in theory , data sci - ence pipelines in - the - small , and data science pipelines in - the - large . Our study analyzed three datasets : a collection of 71 proposals for data science pipelines and related concepts in theory , a collection of 105 implementations of data science pipelines from Kaggle com - petitions to understand data science in - the - small , and a collection of 21 mature data science projects from GitHub to understand data science in - the - large . We have found that DS pipelines differ signifi - cantly between these settings . Specifically , a number of stages are absent in - the - small , and the DS pipelines have a more linear struc - ture . The DS pipelines in - the - large have a more complex structure and feedback loops compared to the theoretical representations . We also contribute three representations of DS pipelines that capture the essence of our subjects in theory , in - the - small , and in - the - large . ACKNOWLEDGMENTS This work was supported in part by US NSF grants CNS - 21 - 20448 and CCF - 19 - 34884 . We also thank the reviewers for their insightful comments . All opinions are of the authors and do not reflect the view of sponsors . ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan REFERENCES [ 1 ] Mart√≠n Abadi , Paul Barham , Jianmin Chen , Zhifeng Chen , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Geoffrey Irving , Michael Isard , et al . 2016 . Tensorflow : A system for large - scale machine learning . In 12th USENIX SymposiumonOperatingSystemsDesignandImplementation ( OSDI16 ) . 265 ‚Äì 283 . [ 2 ] Waleed Abdulla . 2017 . Mask R - CNN for object detection and instance segmen - tation on Keras and TensorFlow . https : / / github . com / matterport / Mask _ RCNN . [ 3 ] Sudeep Agarwal . 2018 . Understanding the Data Science Lifecycle . http : / / sudeep . co / data - science / Understanding - the - Data - Science - Lifecycle . [ 4 ] Charu Aggarwal , Djallel Bouneffouf , Horst Samulowitz , Beat Buesser , Thanh Hoang , Udayan Khurana , Sijia Liu , Tejaswini Pedapati , Parikshit Ram , Ambrish Rawat , et al . 2019 . How can ai automate end - to - end data science ? arXiv preprint arXiv : 1910 . 14436 ( 2019 ) . [ 5 ] Jesse Hu Alexis Chan , Octavio Arriaga . 2017 . Autopilot - TensorFlow . https : / / github . com / SullyChen / Autopilot - TensorFlow . [ 6 ] Saleema Amershi , Andrew Begel , Christian Bird , Robert DeLine , Harald Gall , Ece Kamar , Nachiappan Nagappan , Besmira Nushi , and Thomas Zimmermann . 2019 . Software Engineering for Machine Learning : A Case Study . In Proceedings of the 41st International Conference on Software Engineering . ACM . [ 7 ] Anonymous . 2021 . Data Science Pipline Artifact . https : / / github . com / anonymous - authorss / DS - Pipeline . [ 8 ] Octavio Arriaga . 2018 . Face classification and detectionn . https : / / github . com / oarriaga / face _ classification . [ 9 ] Rob Ashmore , Radu Calinescu , and Colin Paterson . 2021 . Assuring the Machine Learning Lifecycle : Desiderata , Methods , and Challenges . ACM Comput . Surv . 54 , 5 , Article 111 ( may 2021 ) . https : / / doi . org / 10 . 1145 / 3453444 [ 10 ] Jakob Aungiers . 2019 . LSTM Neural Network for Time Series Prediction . https : / / github . com / jaungiers / LSTM - Neural - Network - for - Time - Series - Prediction . [ 11 ] Alex Ball . 2012 . Review of data management lifecycle models . University of Bath , IDMRC . [ 12 ] Denis Baylor , Eric Breck , Heng - Tze Cheng , Noah Fiedel , Chuan Yu Foo , Zakaria Haque , Salem Haykal , Mustafa Ispir , Vihan Jain , Levent Koc , et al . 2017 . TFX : A tensorflow - based production - scale machine learning platform . In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 1387 ‚Äì 1395 . [ 13 ] Francine Berman , Rob Rutenbar , Brent Hailpern , Henrik Christensen , Susan Davidson , DeborahEstrin , MichaelFranklin , MargaretMartonosi , PadmaRagha - van , Victoria Stodden , et al . 2018 . Realizing the potential of data science . Com - mun . ACM 61 , 4 ( 2018 ) , 67 ‚Äì 72 . [ 14 ] Sumon Biswas , Md Johirul Islam , Yijia Huang , and Hridesh Rajan . 2019 . Boa meets Python : a Boa dataset of data science software in Python language . In Proceedings of the 16th International Conference on Mining Software Repositories . IEEE Press , 577 ‚Äì 581 . [ 15 ] Sumon Biswas and Hridesh Rajan . 2020 . Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias ? An Empirical Study on Model Fairness . In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering ( Virtual Event , USA ) . 642 ‚Äì 653 . https : / / doi . org / 10 . 1145 / 3368089 . 3409704 [ 16 ] Sumon Biswas and Hridesh Rajan . 2021 . Fair Preprocessing : Towards Under - standing Compositional Fairness of Data Transformers in Machine Learning Pipeline . In ESEC / FSE‚Äô2021 : The 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering ( Athens , Greece ) . [ 17 ] Denny Britz . 2018 . Convolutional Neural Network for Text Classification in Tensorflow . https : / / github . com / dennybritz / cnn - text - classification - tf . [ 18 ] Muffy Calder , Mario Kolberg , Evan H . Magill , and Stephan Reiff - Marganiec . 2003 . Feature Interaction : A Critical Review and Considered Forecast . Comput . Netw . 41 , 1 ( Jan . 2003 ) , 115 ‚Äì 141 . https : / / doi . org / 10 . 1016 / S1389 - 1286 ( 02 ) 00352 - 3 [ 19 ] Maurice Chang . 2017 . 4 Stages of the Machine Learning ( ML ) Modeling Cy - cle . https : / / www . linkedin . com / pulse / 4 - stages - machine - learning - ml - modeling - cycle - maurice - chang . [ 20 ] CL Philip Chen and Chun - Yang Zhang . 2014 . Data - intensive applications , challenges , techniques and technologies : A survey on Big Data . Information sciences 275 ( 2014 ) , 314 ‚Äì 347 . [ 21 ] Z Ming Chen Mengda . 2018 . reproduce MTCNN , a Joint Face Detection and Alignment using Multi - task Cascaded Convolutional Networks . https : / / github . com / AITTSMD / MTCNN - Tensorflow . [ 22 ] Trishul Chilimbi , Yutaka Suzue , Johnson Apacible , and Karthik Kalyanaraman . 2014 . Project Adam : Building an efficient and scalable deep learning train - ing system . In 11th { USENIX } Symposium on Operating Systems Design and Implementation ( ùëÇùëÜùê∑ùêº 14 ) . 571 ‚Äì 582 . [ 23 ] George E Dahl , Navdeep Jaitly , and Ruslan Salakhutdinov . 2014 . Multi - task neural networks for QSAR predictions . arXiv preprint arXiv : 1406 . 1231 ( 2014 ) . [ 24 ] Sam Crane Dat Tran . 2018 . Real - Time Object Recognition App with Tensorflow and OpenCV . https : / / github . com / datitran / object _ detector _ app . [ 25 ] Hal Daum√© III . 2016 . What Is a Machine Learning Pipeline ? https : / / nlpers . blogspot . com / 2016 / 08 / debugging - machine - learning . html . [ 26 ] YuriDemchenko , FatihTurkmen , CeesdeLaat , ChristopheBlanchet , andCharles Loomis . 2016 . Cloud based big data infrastructure : Architectural components and automated provisioning . In 2016 International Conference on High Perfor - mance Computing & Simulation ( HPCS ) . IEEE , 628 ‚Äì 636 . [ 27 ] Yuri Demchenko , Zhiming Zhao , Paola Grosso , Adianto Wibisono , and Cees De Laat . 2012 . Addressing big data challenges for scientific data infrastructure . In 4th IEEE International Conference on Cloud Computing Technology and Science Proceedings . IEEE , 614 ‚Äì 617 . [ 28 ] Edsger W Dijkstra . 1982 . On the role of scientific thought . In Selected writings on computing : a personal perspective . Springer , 60 ‚Äì 66 . [ 29 ] Robert Dyer , Hoan Anh Nguyen , Hridesh Rajan , and Tien N . Nguyen . 2013 . Boa : A Language and Infrastructure for Analyzing Ultra - Large - Scale Software Repositories . In Proceedings of the 35th International Conference on Software Engineering ( San Francisco , CA ) ( ICSE‚Äô13 ) . 422 ‚Äì 431 . [ 30 ] Robert Dyer , Hoan Anh Nguyen , Hridesh Rajan , and Tien N . Nguyen . 2015 . Boa : Ultra - Large - Scale Software Repository and Source - Code Mining . ACM Trans . Softw . Eng . Methodol . 25 , 1 , Article 7 ( Dec . 2015 ) , 34 pages . https : / / doi . org / 10 . 1145 / 2803171 [ 31 ] Matthew Earl . 2016 . Using neural networks to build an automatic number plate recognition system . https : / / github . com / matthewearl / deep - anpr . [ 32 ] Mohammed El Arass and Nissrine Souissi . 2018 . Data lifecycle : from big data to SmartData . In 2018 IEEE 5th international congress on information science and technology ( CiSt ) . IEEE , 80 ‚Äì 87 . [ 33 ] DouglasFisher . 2017 . AselectedsummaryofAIforcomputationalsustainability . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol . 31 . [ 34 ] Erich Gamma , Richard Helm , Ralph E . Johnson , and John M . Vlissides . 1993 . De - sign Patterns : Abstraction and Reuse of Object - Oriented Design . In Proceedings of the 7th European Conference on Object - Oriented Programming ( ECOOP ‚Äô93 ) . Springer - Verlag , Berlin , Heidelberg , 406 ‚Äì 431 . [ 35 ] Amir Gandomi and Murtaza Haider . 2015 . Beyond the hype : Big data concepts , methods , and analytics . International journal of information management 35 , 2 ( 2015 ) , 137 ‚Äì 144 . [ 36 ] Rolando Garcia , Vikram Sreekanti , Neeraja Yadwadkar , Daniel Crankshaw , Joseph E Gonzalez , and Joseph M Hellerstein . 2018 . Context : The missing piece in the machine learning lifecycle . In KDD CMI Workshop , Vol . 114 . [ 37 ] David Garlan . 2000 . Software architecture : a roadmap . In Proceedings of the Conference on the Future of Software Engineering . 91 ‚Äì 101 . [ 38 ] Yolanda Gil , Ke - Thia Yao , Varun Ratnakar , Daniel Garijo , Greg Ver Steeg , Pedro Szekely , Rob Brekelmans , Mayank Kejriwal , Fanghao Luo , and I - Hui Huang . 2018 . P4ML : A phased performance - based pipeline planner for automated machine learning . In AutoML Workshop at ICML . [ 39 ] Stephanie Glen . 2019 . The Lifecycle of Data . https : / / www . datasciencecentral . com / profiles / blogs / the - lifecycle - of - data . [ 40 ] Google Cloud Blog . 2019 . Machine Learning Workflow . https : / / cloud . google . com / ml - engine / docs / tensorflow / ml - solutions - overview . [ 41 ] Yufeng Guo . 2017 . The 7 Steps of Machine Learning . https : / / towardsdatascience . com / the - 7 - steps - of - machine - learning - 2877d7e5548e . [ 42 ] Charles Hill , Rachel Bellamy , Thomas Erickson , and Margaret Burnett . 2016 . Trials and tribulations of developers of intelligent systems : A field study . In 2016 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 162 ‚Äì 170 . [ 43 ] Michael Hilton , Nicholas Nelson , Timothy Tunnell , Darko Marinov , and Danny Dig . 2017 . Trade - Offs in Continuous Integration : Assurance , Security , and Flexibility . In Proceedingsofthe201711thJointMeetingonFoundationsofSoftware Engineering ( Paderborn , Germany ) ( ESEC / FSE 2017 ) . Association for Computing Machinery , New York , NY , USA , 197 ‚Äì 207 . https : / / doi . org / 10 . 1145 / 3106237 . 3106270 [ 44 ] Sue Ann Hong and Tim Hunter . 2017 . Build , Scale , and Deploy Deep Learning PipelineswithEase . https : / / databricks . com / blog / 2017 / 09 / 06 / build - scale - deploy - deep - learning - pipelines - ease . html . [ 45 ] Han Hu , Yonggang Wen , Tat - Seng Chua , and Xuelong Li . 2014 . Toward scalable systems for big data analytics : A technology tutorial . IEEE access 2 ( 2014 ) , 652 ‚Äì 687 . [ 46 ] Waldemar Hummer , Vinod Muthusamy , Thomas Rausch , Parijat Dube , Kaoutar El Maghraoui , Anupama Murthi , and Punleuk Oum . 2019 . Modelops : Cloud - basedlifecyclemanagementforreliableandtrustedai . In 2019IEEEInternational Conference on Cloud Engineering ( IC2E ) . IEEE , 113 ‚Äì 120 . [ 47 ] GitHub Inc . 2019 . Octoverse 2018 . https : / / octoverse . github . com / projects . [ 48 ] Md Johirul Islam , Giang Nguyen , Rangeet Pan , and Hridesh Rajan . 2019 . A Comprehensive Study on Deep Learning Bug Characteristics . In ESEC / FSE‚Äô19 : The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering ( ESEC / FSE ) . [ 49 ] Md Johirul Islam , Rangeet Pan , Giang Nguyen , and Hridesh Rajan . 2020 . Repair - ing Deep Neural Networks : Fix Patterns and Challenges . In ICSE‚Äô20 : The 42nd International Conference on Software Engineering ( Seoul , South Korea ) . [ 50 ] HV Jagadish . 2015 . Big data and science : Myths and reality . Big Data Research 2 , 2 ( 2015 ) , 49 ‚Äì 52 . [ 51 ] Kathryn Jepsen . 2014 . The machine learning community takes on the Higgs . https : / / www . symmetrymagazine . org / article / july - 2014 / the - machine - The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA learning - community - takes - on - the - higgs . [ 52 ] M . Tim Jones . 2018 . Data , structure , and the data science pipeline . https : / / developer . ibm . com / technologies / data - science / articles / ba - intro - data - science - 1 / . [ 53 ] Kaggle . 2021 . Kaggle Notebook . www . kaggle . com / competitions . [ 54 ] Kaggle . 2021 . Kaggle Notebook . www . kaggle . com / thousandvoices / simple - lstm . [ 55 ] Kaggle . 2021 . Kaggle Notebook . https : / / www . kaggle . com / zfturbo / simple - ru - baseline - lb - 0 - 9627 . [ 56 ] Kaggle . 2021 . Kaggle Notebook . www . kaggle . com / seesee / siamese - pretrained - 0 - 822 . [ 57 ] Kaggle . 2021 . Kaggle Notebook . www . kaggle . com / willkoehrsen / start - here - a - gentle - introduction . [ 58 ] Kaggle . 2021 . Kaggle Notebook . https : / / www . kaggle . com / danielbecker / careervillage - org - recommendation - engine . [ 59 ] Bojan Karla≈° , Matteo Interlandi , Cedric Renggli , Wentao Wu , Ce Zhang , Deepak Mukunthu Iyappan Babu , Jordan Edwards , Chris Lauren , Andy Xu , and Markus Weimer . 2020 . Building continuous integration services for machine learning . In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2407 ‚Äì 2415 . https : / / doi . org / 10 . 1145 / 3394486 . 3403290 [ 60 ] Keras . 2021 . Keras API Reference . https : / / keras . io / api / . [ 61 ] Keras . 2021 . Scikit - LearnAPIReference . https : / / scikit - learn . org / stable / modules / classes . html . [ 62 ] Mary Beth Kery , Marissa Radensky , Mahima Arya , Bonnie E John , and Brad A Myers . 2018 . Thestoryinthenotebook : Exploratorydatascienceusingaliterate programming tool . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 11 . [ 63 ] Samiya Khan , Xiufeng Liu , Kashish A Shakil , and Mansaf Alam . 2017 . A sur - vey on scholarly data : From big data perspective . Information Processing & Management 53 , 4 ( 2017 ) , 923 ‚Äì 944 . [ 64 ] Gregor Kiczales , John Lamping , Anurag Mendhekar , Chris Maeda , Cristina Lopes , Jean - Marc Loingtier , and John Irwin . 1997 . Aspect - oriented program - ming . In ECOOP‚Äô97 ‚Äî Object - Oriented Programming , Mehmet Ak≈üit and Satoshi Matsuoka ( Eds . ) . Springer Berlin Heidelberg , Berlin , Heidelberg , 220 ‚Äì 242 . [ 65 ] Miryung Kim , Thomas Zimmermann , Robert DeLine , and Andrew Begel . 2016 . The emerging role of data scientists on software development teams . In Pro - ceedings of the 38th International Conference on Software Engineering . ACM , 96 ‚Äì 107 . [ 66 ] Namju Kim . 2018 . Speech - to - Text - WaveNet : End - to - end sentence level English speech recognition based on DeepMind‚Äôs WaveNet and tensorflow . https : / / github . com / buriburisuri / speech - to - text - wavenet . [ 67 ] Tim Kraska , Ameet Talwalkar , John C Duchi , Rean Griffith , Michael J Franklin , and Michael I Jordan . 2013 . MLbase : A Distributed Machine - learning System . . In Cidr , Vol . 1 . 2 ‚Äì 1 . [ 68 ] Sara Landset , Taghi M Khoshgoftaar , Aaron N Richter , and Tawfiq Hasanin . 2015 . A survey of open source tools for machine learning with big data in the Hadoop ecosystem . Journal of Big Data 2 , 1 ( 2015 ) , 24 . [ 69 ] Deanne Larson and Victor Chang . 2016 . A review and future direction of agile , business intelligence , analytics and data science . International Journal of Information Management 36 , 5 ( 2016 ) , 700 ‚Äì 710 . [ 70 ] Bennet P Lientz , E . Burton Swanson , and Gail E Tompkins . 1978 . Characteristics of application software maintenance . Commun . ACM 21 , 6 ( 1978 ) , 466 ‚Äì 471 . [ 71 ] Sin Kit Lo , Qinghua Lu , Chen Wang , Helen Paik , and Liming Zhu . 2020 . A systematic literature review on federated machine learning : From a software engineering perspective . arXiv preprint arXiv : 2007 . 11354 ( 2020 ) . [ 72 ] Hui Miao , Amit Chavan , and Amol Deshpande . 2017 . Provdb : Lifecycle manage - ment of collaborative analysis workflows . In Proceedings of the 2nd Workshop on Human - In - the - Loop Data Analytics . ACM , 7 . [ 73 ] Hui Miao , Ang Li , Larry S Davis , and Amol Deshpande . 2017 . Towards unified data and lifecycle management for deep learning . In 2017 IEEE 33rd International Conference on Data Engineering ( ICDE ) . IEEE , 571 ‚Äì 582 . [ 74 ] Microsoft Blog . 2019 . What are ML pipelines in Azure Machine Learning service ? https : / / docs . microsoft . com / en - us / azure / machine - learning / service / concept - ml - pipelines . [ 75 ] Justin J Miller . 2013 . Graph database applications and concepts with Neo4j . In Proceedings of the Southern Association for Information Systems Conference , Atlanta , GA , USA , Vol . 2324 . [ 76 ] Valohai MLOps . 2020 . What Is a Machine Learning Pipeline ? https : / / valohai . com / machine - learning - pipeline / . [ 77 ] Giang Nguyen , Stefan Dlugolinsky , Martin Bob√°k , Viet Tran , √Ålvaro L√≥pez Garc√≠a , Ignacio Heredia , Peter Mal√≠k , and Ladislav Hluch ` y . 2019 . Machine Learning and Deep Learning frameworks and libraries for large - scale data mining : a survey . Artificial Intelligence Review ( 2019 ) , 1 ‚Äì 48 . [ 78 ] Giang Nguyen , Johir Islam , Rangeet Pan , and Hridesh Rajan . 2022 . Manas : Min - ing Software Repositories to Assist AutoML . In ICSE‚Äô22 : The 44th International Conference on Software Engineering ( Pittsburgh , PA , USA ) . [ 79 ] Randal S Olson , Nathan Bartley , Ryan J Urbanowicz , and Jason H Moore . 2016 . Evaluationofatree - basedpipelineoptimizationtoolforautomatingdatascience . In Proceedings of the Genetic and Evolutionary Computation Conference 2016 . ACM , 485 ‚Äì 492 . [ 80 ] Alex Paino . 2017 . Deep learning models trained to correct input errors in short , message - like text . https : / / github . com / atpaino / deep - text - corrector . [ 81 ] RangeetPanandHrideshRajan . 2020 . OnDecomposingaDeepNeuralNetwork into Modules . In ESEC / FSE‚Äô2020 : The 28th ACM Joint European Software Engi - neering Conference and Symposium on the Foundations of Software Engineering ( Sacramento , California , United States ) . [ 82 ] Rangeet Pan and Hridesh Rajan . 2022 . Decomposing Convolutional Neural Net - works into Reusable and Replaceable Modules . In ICSE‚Äô22 : The 44th International Conference on Software Engineering ( Pittsburgh , PA , USA ) . [ 83 ] Kyubyong Park . 2018 . A TensorFlow Implementation of Tacotron : A Fully End - to - End Text - To - Speech Synthesis Model . https : / / github . com / Kyubyong / tacotron . [ 84 ] David Lorge Parnas , Paul C Clements , and David M Weiss . 1985 . The modular structure of complex systems . IEEE Transactions on software Engineering 3 ( 1985 ) , 259 ‚Äì 266 . [ 85 ] Neoklis Polyzotis , Sudip Roy , Steven Euijong Whang , and Martin Zinkevich . 2017 . Data management challenges in production machine learning . In Proceed - ings of the 2017 ACM International Conference on Management of Data . ACM , 1723 ‚Äì 1726 . [ 86 ] Neoklis Polyzotis , Sudip Roy , Steven Euijong Whang , and Martin Zinkevich . 2018 . Data Lifecycle Challenges in Production Machine Learning : A Survey . ACM SIGMOD Record 47 , 2 ( 2018 ) , 17 ‚Äì 28 . [ 87 ] Line Pouchard . 2016 . Revisiting the data lifecycle with big data curation . Inter - national Journal of Digital Curation 10 , 2 ( 2016 ) , 176 ‚Äì 192 . [ 88 ] Christian Prehofer . 1997 . Feature - oriented programming : A fresh look at ob - jects . In ECOOP‚Äô97 ‚Äî Object - Oriented Programming , Mehmet Ak≈üit and Satoshi Matsuoka ( Eds . ) . Springer Berlin Heidelberg , Berlin , Heidelberg , 419 ‚Äì 443 . [ 89 ] Chuan Qi . 2019 . Caffe implementation of Google MobileNet SSD detection network . https : / / github . com / chuanqi305 / MobileNet - SSD . [ 90 ] V√°clav Rajlich . 2014 . Software evolution and maintenance . In Future of Software Engineering Proceedings . 133 ‚Äì 144 . [ 91 ] Muhammad Habib Rehman , Victor Chang , Aisha Batool , and Teh Ying Wah . 2016 . Bigdatareductionframeworkforvaluecreationinsustainableenterprises . International Journal of Information Management 36 , 6 ( 2016 ) , 917 ‚Äì 928 . [ 92 ] Syed Ali Asad Rizvi , Elmarie Van Heerden , Arnold Salas , Favour Nyikosa , Stephen J Roberts , Michael A Osborne , and Elmer Rodriguez . 2017 . Identi - fying Sources of Discrimination Risk in the Life Cycle of Machine Intelligence Applications under New European Union Regulations . In 2017 AAAI Spring Symposium Series . [ 93 ] David Robinson . 2017 . The Incredible Growth of Python . https : / / stackoverflow . blog / 2017 / 09 / 06 / incredible - growth - python / . [ 94 ] Yuji Roh , Geon Heo , and Steven Euijong Whang . 2019 . A Survey on Data Collection for Machine Learning : a Big Data - AI Integration Perspective . IEEE Transactions on Knowledge and Data Engineering ( 2019 ) . [ 95 ] Eragon Ruan . 2019 . Scene text detection based on ctpn ( connectionist text proposal network ) . https : / / github . com / eragonruan / text - detection - ctpn . [ 96 ] Janine R√ºegg , Corinna Gries , Ben Bond - Lamberty , Gabriel J Bowen , Benjamin S Felzer , Nancy E McIntyre , Patricia A Soranno , Kristin L Vanderbilt , and Kath - leen C Weathers . 2014 . Completing the data life cycle : using information management in macrosystems ecology research . Frontiers in Ecology and the Environment 12 , 1 ( 2014 ) , 24 ‚Äì 30 . [ 97 ] Adam Rule , Aur√©lien Tabard , and James D Hollan . 2018 . Exploration and expla - nation in computational notebooks . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 12 . [ 98 ] PhilippeR√©my . 2018 . DeepLearningmodeltoanalyzealargecorpusofcleartext passwords . https : / / github . com / philipperemy / tensorflow - 1 . 4 - billion - password - analysis . [ 99 ] Shazia Sadiq , Tamraparni Dasu , Xin Luna Dong , Juliana Freire , Ihab F Ilyas , Sebastian Link , Miller J Miller , Felix Naumann , Xiaofang Zhou , and Divesh Srivastava . 2018 . Data quality : The role of empiricism . ACM SIGMOD Record 46 , 4 ( 2018 ) , 35 ‚Äì 43 . [ 100 ] David Sandberg . 2018 . Face Recognition using Tensorflow . https : / / github . com / davidsandberg / facenet . [ 101 ] Carlton E Sapp . 2017 . Preparing and architecting machine learning . Gartner Technical Professional Advice ( 2017 ) , 1 ‚Äì 37 . [ 102 ] David Sculley , Gary Holt , Daniel Golovin , Eugene Davydov , Todd Phillips , Dietmar Ebner , Vinay Chaudhary , and Michael Young . 2014 . Machine learning : The high interest credit card of technical debt . ( 2014 ) . [ 103 ] David Sculley , Gary Holt , Daniel Golovin , Eugene Davydov , Todd Phillips , Dietmar Ebner , Vinay Chaudhary , Michael Young , Jean - Francois Crespo , and Dan Dennison . 2015 . Hidden technical debt in machine learning systems . In Advances in neural information processing systems . 2503 ‚Äì 2511 . [ 104 ] Roald Bradley Severtson . 2017 . What is the Team Data Science Pro - cess ? https : / / docs . microsoft . com / en - us / azure / machine - learning / team - data - science - process / overview . [ 105 ] Zeyuan Shang , Emanuel Zgraggen , Benedetto Buratti , Ferdinand Kossmann , Philipp Eichmann , Yeounoh Chung , Carsten Binnig , Eli Upfal , and Tim Kraska . ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan 2019 . Democratizing data science through interactive curation of ML pipelines . In Proceedingsofthe2019InternationalConferenceonManagementofData . ACM , 1171 ‚Äì 1188 . [ 106 ] M Shashanka . 2019 . What is a Pipeline in Machine Learning ? How to create one ? https : / / medium . com / analytics - vidhya / what - is - a - pipeline - in - machine - learning - how - to - create - one - bda91d0ceaca . [ 107 ] Mary Shaw and David Garlan . 1996 . Software Architecture : Perspectives on an Emerging Discipline . Prentice - Hall , Inc . , USA . [ 108 ] Naoki Shibuya . 2017 . Pipelines , Mind Maps and Convolutional Neu - ral Networks . https : / / towardsdatascience . com / pipelines - mind - maps - and - convolutional - neural - networks - 34bfc94db10c . [ 109 ] Amir Sinaeepourfard , Jordi Garcia , Xavier Masip - Bruin , and Eva Mar√≠n - Torder . 2016 . Towards a comprehensive data lifecycle model for big data environ - ments . In Proceedings of the 3rd IEEE / ACM International Conference on Big Data Computing , Applications and Technologies . ACM , 100 ‚Äì 106 . [ 110 ] Sivakar Siva . 2020 . The ‚ÄúGeneric‚Äù Data Science Life - Cycle . https : / / towardsdatascience . com / stoend - to - end - data - science - life - cycle - 6387523b5afc . [ 111 ] Micah J Smith , Roy Wedge , and Kalyan Veeramachaneni . 2017 . FeatureHub : Towards collaborative data science . In 2017 IEEE International Conference on Data Science and Advanced Analytics ( DSAA ) . IEEE , 590 ‚Äì 600 . [ 112 ] Guocong Song . 2017 . Tensorflow - based Recommendation systems . https : / / github . com / songgc / TF - recomm . [ 113 ] Evan R Sparks , Shivaram Venkataraman , Tomer Kaftan , Michael J Franklin , and Benjamin Recht . 2017 . Keystoneml : Optimizing pipelines for large - scale advancedanalytics . In 2017IEEE33rdinternationalconferenceondataengineering ( ICDE ) . IEEE , 535 ‚Äì 546 . [ 114 ] Victoria Stodden . 2020 . The data science life cycle : a disciplined approach to advancing data science as a science . Commun . ACM 63 , 7 ( 2020 ) , 58 ‚Äì 66 . [ 115 ] Marvin Teichmann . 2018 . A Kitti Road Segmentation Model Implemented in Tensorflow . https : / / github . com / MarvinTeichmann / KittiSeg . [ 116 ] Stephen Todd and David Dietrich . 2017 . Computing resource re - provisioning during data analytic lifecycle . US Patent 9 , 619 , 550 . [ 117 ] Ehsan Toreini , Mhairi Aitken , Kovila Coopamootoo , Karen Elliott , Carlos Gon - zalez Zelaya , and Aad van Moorsel . 2020 . The relationship between trust in AI and trustworthy machine learning technologies . In Proceedings of the 2020 Conference on Fairness , Accountability , and Transparency . 272 ‚Äì 283 . [ 118 ] Andrew Bagshaw Trieu . 2018 . Real - time object detection and classification . https : / / github . com / thtrieu / darkflow . [ 119 ] Cagatay Turkay , Nicola Pezzotti , Carsten Binnig , Hendrik Strobelt , Barbara Hammer , Daniel A Keim , Jean - Daniel Fekete , Themis Palpanas , Yunhai Wang , and Florin Rusu . 2018 . Progressive data science : Potential and challenges . arXiv preprint arXiv : 1812 . 08032 ( 2018 ) . [ 120 ] TomVanDerWeide , DimitrisPapadopoulos , OlegSmirnov , MichalZielinski , and Tim Van Kasteren . 2017 . Versioning for end - to - end machine learning pipelines . In Proceedings of the 1st Workshop on Data Management for End - to - End Machine Learning . ACM , 2 . [ 121 ] Anthony J Viera , Joanne M Garrett , et al . 2005 . Understanding interobserver agreement : the kappa statistic . Fam med 37 , 5 ( 2005 ) , 360 ‚Äì 363 . [ 122 ] Ben Wagner . 2020 . Accountability by design in technology research . Computer Law & Security Review 37 ( 2020 ) , 105398 . [ 123 ] Zhiyuan Wan , Xin Xia , David Lo , and Gail C Murphy . 2019 . How does machine learning change software development practices ? IEEE Transactions on Software Engineering ( 2019 ) . [ 124 ] Dakuo Wang , Justin D Weisz , Michael Muller , Parikshit Ram , Werner Geyer , Casey Dugan , Yla Tausczik , Horst Samulowitz , and Alexander Gray . 2019 . Human - AI collaboration in data science : Exploring data scientists‚Äô perceptions of automated AI . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 ‚Äì 24 . [ 125 ] Jiawei Wang , Li Li , and Andreas Zeller . 2021 . Restoring Execution Environ - ments of Jupyter Notebooks . In 2021 IEEE / ACM 43rd International Conference on Software Engineering ( ICSE ) . IEEE , 1622 ‚Äì 1633 . [ 126 ] MohammadWardat , BrenoDantasCruz , WeiLe , andHrideshRajan . 2022 . Deep - Diagnosis : Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs . In ICSE‚Äô22 : The 44th International Conference on Software Engineering ( Pittsburgh , PA , USA ) . [ 127 ] Mohammad Wardat , Wei Le , and Hridesh Rajan . 2021 . DeepLocalize : Fault Localization for Deep Neural Networks . In ICSE‚Äô21 : The 43nd International Conference on Software Engineering ( Virtual Conference ) . [ 128 ] Hadley Wickham . 2019 . Data science : how is it different to statistics ? IMS Bulletin 48 ( 2019 ) . [ 129 ] Jeannette M Wing . 2019 . The Data Life Cycle . Harvard Data Science Review ( 2019 ) . [ 130 ] R√ºdiger Wirth and Jochen Hipp . 2000 . CRISP - DM : Towards a standard process model for data mining . In Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining . Citeseer , 29 ‚Äì 39 . [ 131 ] Christof Wolf , Dominique Joye , Tom W Smith , and Yang - chih Fu . 2016 . The SAGE handbook of survey methodology . Sage . [ 132 ] Max Woolf . 2018 . Automatically " block " people in images ( like Black Mirror ) us - ingapretrainedneuralnetwork . https : / / github . com / minimaxir / person - blocker . [ 133 ] Mehmet Yildiz . 2020 . Big Data Lifecycle Management . https : / / medium . com / technology - hits / big - data - lifecycle - management - 629dfe16b78d . [ 134 ] Adams Wei Yu , David Dohan , Minh - Thang Luong , Rui Zhao , Kai Chen , Moham - mad Norouzi , and Quoc V Le . 2018 . A Tensorflow implementation of QANet for machine reading comprehension . https : / / github . com / NLPLearn / QANet . [ 135 ] YuyuZhang , MohammadTahaBahadori , HangSu , andJimengSun . 2016 . FLASH : fast Bayesian optimization for data analytic pipelines . In Proceedings of the 22nd ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining . ACM , 2065 ‚Äì 2074 . [ 136 ] Yingfeng Zhang , Shan Ren , Yang Liu , Tomohiko Sakao , and Donald Huisingh . 2017 . A framework for Big Data driven product lifecycle management . Journal of Cleaner Production 159 ( 2017 ) , 229 ‚Äì 240 . [ 137 ] Charlie Bickerton Zhilin Yang , Zihang Dai . 2019 . XLNet : Generalized Autore - gressivePretrainingforLanguageUnderstanding . https : / / github . com / zihangdai / xlnet . [ 138 ] Baifan Zhou , Yulia Svetashova , Tim Pychynski , Ildar Baimuratov , Ahmet Soylu , and Evgeny Kharlamov . 2020 . SemFE : facilitating ML pipeline development with semantics . In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 3489 ‚Äì 3492 . [ 139 ] Linda Zhou . 2019 . How to Build a Better Machine Learning Pipeline . https : / / www . datanami . com / 2018 / 09 / 05 / how - to - build - a - better - machine - learning - pipeline . The Art and Practice of Data Science Pipelines ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Table 3 : Labeled data science pipelines from the subject studies . ACQ : Data acquisition , PRP : Data preparation , STR : Data storage , FTR : Feature engineering , MDL : Modeling , TRN : Training , EVL : Evaluation , PRD : Prediction , INT : Interpretation , CMN : Communication , DPL : Deployment . Overall goal : Describe / propose pipeline , Survey / compare / review , DS optimization , Introduce new method / application Preprocessing Modeling Post - processing Involves Type References ACQ PRP STR FTR MDL TRN EVL PRD INT CMN DPL Cyber Physical Human Olson et al . , 2016 [ 79 ] - - - - - - Miao et al . , 2017b [ 73 ] - - - - - - - Garcia et al . , 2018 [ 36 ] - - - - - - - Hong and Hunter , 2017 [ 44 ] - - - - - - - - - Microsoft Blog , 2019 [ 74 ] - - - - - Zhou , 2019 [ 139 ] - - - - - - - - Shibuya , 2017 [ 108 ] - - - - - - - - - - Polyzotis et al . , 2018 [ 86 ] - - - - - - - Roh et al . , 2019 [ 94 ] - - - - - - - Miao et al . , 2017a [ 72 ] - - - - - - - - - - Sparks et al . , 2017 [ 113 ] - - - - - - - - - Guo , 2017 [ 41 ] - - - - - - Baylor et al . , 2017 [ 12 ] - - - - - - Abadi et al . , 2016 [ 1 ] - - - - - - - - - Chilimbi et al . , 2014 [ 22 ] - - - - - - - - - Kraska et al . , 2013 [ 67 ] - - - - - - - Sculley et al . , 2015 [ 103 ] - - - - - - - - - Chang , 2017 [ 19 ] - - - - - Google Cloud Blog , 2019 [ 40 ] - - - - - - - Amershi et al . , 2019 [ 6 ] - - - - - Van Der Weide et al . , 2017 [ 120 ] - - - - - - - - Hill et al . , 2016 [ 42 ] - - - - - - - - - Shang et al . , 2019 [ 105 ] - - - - - - - - - - Zhang et al . , 2016 [ 135 ] - - - - - - - - - - Gil et al . , 2018 [ 38 ] - - - - - - - - Sadiq et al . , 2018 [ 99 ] - - - - - - Zhou et al . , 2020 [ 138 ] - - - - - - Aggarwal et al . , 2019 [ 4 ] - - - - - - - - Toreini et al . , 2020 [ 117 ] - - - - - - - Ashmore et al . , 2021 [ 9 ] Shashanka , 2019 [ 106 ] - ‚Äì ‚Äì ‚Äì ‚Äì MLOps , 2020 [ 76 ] - - ‚Äì ‚Äì ‚Äì ‚Äì M a c h i n e l e a r n i n g p r o c e ss Daum√© III , 2016 [ 25 ] - - - - - - - Todd and Dietrich , 2017 [ 116 ] - - - - - Zhang et al . , 2017 [ 136 ] - - - - - - Sapp , 2017 [ 101 ] - Landset et al . , 2015 [ 68 ] - - - - - - - Polyzotis et al . , 2017 [ 85 ] - - - - - - - - - Hu et al . , 2014 [ 45 ] - - - - - - - - Demchenko et al . , 2012 [ 27 ] - - - - - - - - - Khan et al . , 2017 [ 63 ] - - - - - - El Arass and Souissi , 2018 [ 32 ] - - - ‚Äì ‚Äì - - Hummer et al . , 2019 [ 46 ] - - - - - Yildiz , 2020 [ 133 ] - - - - Glen , 2019 [ 39 ] - - - - - - - B i g d a t a m a n a g e m e n t Jones , 2018 [ 52 ] - - - ‚Äì - - ICSE ‚Äô22 , May 21 ‚Äì 29 , 2022 , Pittsburgh , PA , USA Sumon Biswas , Mohammad Wardat , and Hridesh Rajan Preprocessing Modeling Post - processing Involves Type References ACQ PRP STR FTR MDL TRN EVL PRD INT CMN DPL Cyber Physical Human Pouchard , 2016 [ 87 ] - - - - - - - - Severtson , 2017 [ 104 ] - - Berman et al . , 2018 [ 13 ] - - - - - Agarwal , 2018 [ 3 ] - - - Nguyen et al . , 2019 [ 77 ] - - - - - - R√ºegg et al . , 2014 [ 96 ] - - - - - - - - - Gandomi and Haider , 2015 [ 35 ] - - - - - - Ball , 2012 [ 11 ] - - - - - - - - - Wing , 2019 [ 129 ] - - - - - - Rehman et al . , 2016 [ 91 ] - - - - - - - - Chen and Zhang , 2014 [ 20 ] - - - - - - - - - Jagadish , 2015 [ 50 ] - - - - - - Larson and Chang , 2016 [ 69 ] - - - - - - Rizvi et al . , 2017 [ 92 ] - - - - - - - Demchenko et al . , 2016 [ 26 ] - - - - - Wolf et al . , 2016 [ 131 ] - - - - - - - - Sinaeepourfard et al . , 2016 [ 109 ] - - - - - - - Kim et al . , 2016 [ 65 ] - - - - Fisher , 2017 [ 33 ] - - - - - Turkay et al . , 2018 [ 119 ] - - - - - - Smith et al . , 2017 [ 111 ] - - - - - - - - Wang et al . , 2019 [ 124 ] - - - - - - Lo et al . , 2020 [ 71 ] - - - - - Siva , 2020 [ 110 ] - - - T e a m p r o c e ss Stodden , 2020 [ 114 ] -