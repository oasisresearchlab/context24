Preprint T HOUGHT P ROPAGATION : A N A NALOGICAL A PPROACH TO C OMPLEX R EASON - ING WITH L ARGE L ANGUAGE M ODELS Junchi Yu & Ran He ∗ Institute of Automation Chinese Academy of Sciences Beijing , China yujunchi2019 @ ia . ac . cn , rhe @ nlpr . ia . ac . cn Rex Ying Department of Computer Sciences Yale University New Haven , USA rex . ying @ yale . edu A BSTRACT Large Language Models ( LLMs ) have achieved remarkable success in reasoning tasks with the development of prompting methods . However , existing prompt - ing approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi - step reasoning , since they prompt LLMs to reason from scratch . To address these issues , we propose Thought Propagation ( TP ) , which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs . These analogous problems are related to the input one , with reusable solutions and problem - solving strategies . Thus , it is promising to propagate insights of solving previous analogous problems to inspire new problem - solving . To achieve this , TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one . Then , TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge - intensive plan for execution to amend the initial solution obtained from scratch . TP is compatible with existing prompting approaches , allowing plug - and - play generalization and enhancement in a wide range of tasks without much labor in task - specific prompt engineering . Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an av - erage of 12 % absolute increase in finding the optimal solutions in Shortest - path Reasoning , 13 % improvement of human preference in Creative Writing , and 15 % enhancement in the task completion rate of LLM - Agent Planning . You are in the middle of a room . Looking quickly around you , you see a bed 1 and …… Your task is to : examine the book with the desk lamp . Task Descriptions : You are in the middle of a room . Looking quickly around you , you see a bed 1 , and … Your task is to : examine the alarmclock with the desklamp . Agent : take alarmclock1 from desk 2 Feedback : You pick up the alarmclock1 from the desk 2 . … … Plan : To solve the task , I need to find and take a book , then find and use a desk lamp . First I …… Agent : take book 1 from bed 1 Feedback : You pick up the book 1 from the bed 1 . Agent : use desk lamp 1 Feedback : You turn on the desk lamp 1 . STATUS : SUCCESS Similar task Input task You are in the middle of a room . Looking quickly around you , you see a bed 1 and …… Your task is to : examine the book with the desk lamp . Input problem You are in the middle of a room . Looking quickly around you , you see a bed 1 and …… Your task is to : examine the alarm clock with the desk lamp . Analogous problem Agent : take alarm clock 1 from desk 2 Feedback : You pick up the alarm clock 1 from the desk 2 . Agent : use desk lamp 1 Feedback : You turn on the desk lamp 1 . STATUS : SUCCESS Reason from Scratch Thought Propagation Input task Output Output Agent : take desk lamp 1 from desk 1 Feedback : Nothing happens . Agent : take desk lamp 1 from desk 1 Feedback : Nothing happens . STATUS : FAIL Output … …… … … … Input problem Figure 1 : Reasoning from scratch cannot reuse insight of solving similar problems and suffers from accumulated errors in intermediate reasoning stages . Thought Propagation explores analogous prob - lems that are related to the input one and derives insights from solutions to analogous problems . ∗ Corresponding Author . 1 a r X i v : 2310 . 03965v2 [ c s . A I ] 9 O c t 2023 Preprint 1 I NTRODUCTION The scaling - up Large Language Models ( LLMs ) ( OpenAI , 2023 ) have achieved notable success in logical ( Khot et al . , 2022 ) , arithmetic ( Wei et al . , 2022 ) , and commonsense ( Liu et al . , 2021 ) reasoning with the development of prompting methods ( Qiao et al . , 2022 ) . The early works employ few - shot input and output exemplars to prompt the LLM to perform simple reasoning ( Brown et al . , 2020 ) . Recent methods decompose the complex reasoning process into intermediate reasoning steps to enable LLMs with multi - step reasoning abilities ( Wei et al . , 2022 ; Wang et al . , 2022a ) . Although many efforts are made to improve complex reasoning with LLMs by crafted prompt design ( Zhang et al . , 2022b ) , delicate decomposition ( Khot et al . , 2022 ; Zhou et al . , 2022 ) , and advanced searching scheme ( Yao et al . , 2023 ) , these methods prompt the LLM to reason from scratch . This scheme is problematic to complex reasoning for two reasons . First , reasoning from scratch cannot reuse the insights of solving similar problems ( Hall , 1989 ) . Using such insights as prior knowledge can ease the difficulty of solving complex problems and develop new solutions ( Carbonell , 1985 ) . One can further assess new solutions with rough ones obtained from scratch and yield refined so - lutions . Second , reasoning from scratch is sensitive to the errors made in intermediate stages when facing tasks involving multi - step reasoning . As shown in Figure 1 , these errors will accumulate as they misguide the searching and planning afterward , which eventually leads to invalid reason - ing outcome ( Dziri et al . , 2023 ; Yao et al . , 2022 ) . These two challenges motivate us to develop an alternative framework for LLM reasoning to amend the limitations of reasoning from scratch . The study of human cognition presents a promising way to amend the limitations of reasoning from scratch , primarily through the application of analogy ( Bartha , 2013 ) . The analogy highlights the occurrence of entities’ relationship in the form of ”A is to B what C is to D” . By discerning and applying such analogical reasoning , humans can stand on the shoulder of existing knowledge to pio - neer new concepts in novel domains . A compelling historical example is the discovery of Coulomb’s Law , which can be traced back to the analogy drawn between gravitational forces governing celestial bodies and electrical forces acting upon charged particles ( Priestley , 1775 ) . Such a framework has been recently proven to be efficient in relational reasoning between entities on knowledge graphs ( Zhang et al . , 2022a ; Yuan et al . , 2023 ) . However , a general framework of harnessing analogies among problems to facilitate LLM reasoning , to the best of our knowledge , is yet to be explored . Hence , we advance the traditional analogical reasoning and propose a novel Thought Propaga - tion ( TP ) framework to amend existing reasoning - from - scratch methods and enhance the complex reasoning ability of LLMs . Given an input problem , TP first prompts LLMs to propose a set of analogous problems that are related to the input one . Then , the input problem with its analogous counterpart is solved by existing prompting approaches such as Chain - of - Thought ( CoT ) Wei et al . ( 2022 ) . An aggregation module further aggregates the solutions from these analogous problems , fa - cilitating input problem - solving through two distinct avenues . First , this module reuses the solutions derived from analogous problems to generate a new solution to the input problem . The aggregation module assesses the new solution produced by the analogical approach with the initial one obtained from scratch to output a refined result for the input problem . Second , this module compares the input problem with its analogous counterparts and devises high - level plans based on the results of analogous problems . These plans are then executed by the LLM to rectify its intermediate reason - ing steps when addressing the input problem . TP is compatible with existing approaches , allowing plug - and - play generalization and enhancement to various tasks ranging from optimization problems to autonomous agent planning . Thus , it reduces intensive labor in task - specific prompt engineering . We test the proposed method on three tasks , including Shortest - path Reasoning , Creative Writing , and LLM - Agent Planning . These tasks require searching over graph - structure data , open - ended writing , and long - trial planning , which challenge existing methods for LLM reasoning . Experimen - tal results show that Thought Propagation can generalize to a wide range of different reasoning tasks and enjoys superior performances on all of them . 2 R ELATED W ORK Graph Neural Network . Graph neural networks ( GNNs ) are expressive network backbones of deep graph learning due to their inductive bias on graph - structured data ( Hamilton et al . , 2017 ; Kipf & Welling , 2017 ) . GNNs aggregate the neighborhood node messages to obtain node embeddings 2 Preprint Idx = 4 Shortest path 0 1 3 4 2 1 2 2 4 5 Starting from node 0 , we arrive at node 1 . The distance between these two nodes is 1 . Starting from node 1 , we arrive at node 3 . The distance between these two nodes is 4 . Starting from node 3 , we arrive at node 4 . The distance between these two nodes is 2 . Thus , the shortest path from the source node to the target node is [ 0 , 1 , 3 , 4 ] . The shortest path from the source node to the target node is [ 0 , 1 , 3 , 4 ] . ( a ) . An input undirected weighted graph . The task for LLM is to find the shortest path from Node 0 to Node 4 . ( b ) . The path found by IO Prompting . ( c ) . The path found by CoTwith intermediate reasoning steps . ( d ) . The path found by ToT bysearching . ToT evaluates the green nodes as the best one that forms a shortest path from Node 0 to Node 4 . The shortest path from the source node to the target node is [ 0 , 3 , 1 , 3 , 4 ] . 0 1 3 1 4 3 1 4 Figure 2 : An example of existing methods on shortest path reasoning task . Although the graph in ( a ) is quite simple , these methods only prompt the LLM to find the sub - optimal solutions ( b , c ) , and even repeatedly visit intermediate nodes ( d ) , due to reasoning from scratch . ( Gilmer et al . , 2017 ) . Recent works incorporate parameterized GNNs with Large Language Models ( LLMs ) for graph - related tasks , such as graph explainability ( He et al . , 2023 ) , classification ( Chen et al . , 2023b ; Qian et al . , 2023 ) , and question answering ( Jiang et al . , 2023 ) . Differently , our work aims to improve the general reasoning ability of LLMs using problem analogy without fine - tuning . Analogical Reasoning . The analogical reasoning has been applied to visual reasoning ( Małki´nski & Ma ´ ndziuk , 2022 ) , natural language reasoning ( Chen et al . , 2022 ; Sultan & Shahaf , 2022 ) , and knowledge graph reasoning ( Zhang et al . , 2022a ) . These methods train parameterized neural net - works to perform relational reasoning between entities . Early attempts have shown LLMs can do analogical reasoning just like humans by case study ( Webb et al . , 2023 ) . Recent works explore anal - ogy generation and analogy reasoning with knowledge graphs on LLMs ( Yuan et al . , 2023 ; Bhavya et al . , 2022 ; 2023 ) . However , they focus on different applications instead of general reasoning prob - lems . Moreover , they rely on large - scale external knowledge bases to store entity relationships to perform analogical reasoning , which is expensive for general reasoning tasks . Thus , a general ana - logical approach for LLM complex reasoning on general tasks is still in its vacuum . Prompt - based Large Language Model Reasoning . Originally designed for text generation , the Large Language Models ( LLMs ) have succeeded in many applications with prompting methods ( Liu et al . , 2023b ; Zhao et al . , 2023 ) . Early methods employ input and output ( IO ) prompting that appends pairs of problems and solutions exemplars on top of the input problem ( Brown et al . , 2020 ) . Recent methods decompose the complex reasoning process into intermediate reasoning steps . They use multi - step prompting ( Wei et al . , 2022 ; Wang et al . , 2022a ; Zhang et al . , 2022b ) or recursive problem decomposition ( Zhou et al . , 2022 ; Khot et al . , 2022 ) to enable multi - step LLMs reasoning . Others design searching schemes for LLMs ( Yao et al . , 2023 ; Besta et al . , 2023 ) . However , they solve each problem from scratch . Thus , they cannot reuse the insights of solving similar problems . Moreover , they suffer from accumulated errors in intermediate reasoning steps . LLM as Autonomous Agents . LLM - Agents can interface with tools ( Cai et al . , 2023 ; Schick et al . , 2023 ; Chen et al . , 2023a ; Liu et al . , 2022 ) , other LLMs ( Wu et al . , 2023 ; Li et al . , 2023 ; Chan et al . , 2023 ) , and humans ( Wang et al . , 2023c ; Liu et al . , 2023a ) to autonomously make decisions and formulate planning to solve tasks with feedback . The key component of LLM - Agents is the LLM - based planning module to process the environmental feedback and make planning ( Wang et al . , 2023b ; Zhu et al . , 2023a ) . When deployed in long - trial decisions and planning scenarios , LLM - Agents should make multiple rounds of action and planning . As LLMs are likely to hallucinate , LLM - Agents will accumulate errors and finally fail to complete planning tasks ( Yao et al . , 2022 ) . Recent work amends this issue by reflecting on its previous failures in completing the same task when LLM - Agents start one more try for task completion ( Shinn et al . , 2023 ) . Differently , we apply the proposed method to help LLM - Agents summarize their successful experience in completing other similar tasks and formulate a plan to improve task completion . More Discussion with Retrieval - augmented LLMs . The retrieval - augmented LLM is proposed to alleviate the hallucination phenomenon and improve the output quality of LLM ( Asai et al . , 2023 ; Mialon et al . , 2023 ; Shi et al . , 2023 ) . For an input question , the retrieval - augmented LLM first queries an external database with billion - level tokens ( Borgeaud et al . , 2022 ; Lan et al . , 2023 ; Zhu et al . , 2023b ) to retrieve a subset of the text corpus to construct the output answer . The retrieval - augmented LLM achieves improved question - answering quality with fewer parameters than standard LLM ( Mialon et al . , 2023 ) and has been applied to many downstream tasks such as text / multi - modal generation ( Lan et al . , 2023 ; Yasunaga et al . , 2023 ) , question answering ( Borgeaud et al . , 2022 ; Izacard et al . , 2022 ) and biomedical applications ( Wang et al . , 2022b ) . The retrieval - augmented 3 Preprint s p ( d ) Thought Propagation ( TP ) ( c ) Tree - of - Thought ( ToT ) s p . . . ( a ) IO Prompting ( b ) Chain - of - Thought ( CoT ) Flowcharts K = 1 K = 0 p ! p p " p # s s ! s " s # s ! s # s " s ! z " z # z $ s p z " z # z % z & z ’ Propose Analogous Problems for the Input Problem Initialize Solutions to Each Problem Aggregate Solutions to Analogous Problems to Update Solution to Input Problem K = 0 K = 0 K = 1 K = 1 Figure 3 : The illustrative comparison between Thought Propagation ( TP ) and other representative methods . For an input problem p , IO , CoT , and ToT reason from scratch to yield the solution s . Differently , TP explores analogous problems to improve solving the input problem . LLM has been widely applied to many tasks ( Lan et al . , 2023 ; Yasunaga et al . , 2023 ; Izacard et al . , 2022 ; Wang et al . , 2022b ) . Differently , the proposed method requires no external database to query from but aggregates the knowledge of solving analogous problems to hint at reasoning . 3 P RELIMINARIES Denote the reasoning problem and the solution as p ∈ P and s ∈ S . P and S are the problem and solution space . Let the LLM be f θ where θ denotes model weights . IO Prompting . IO prompting ( Brown et al . , 2020 ) uses task descriptions and few - shot pairs of Input and Output ( IO ) prompting demonstrations to assist LLMs in reasoning to solve the input problem p by s = f θ ( p ) . Thus , we denote the reasoning path of IO prompting as p → s . As shown in Figure 3 ( a ) , the reasoning path of IO prompting is one - step . One - step reasoning is insufficient to solve complex problems which involve multi - step reasoning . Chain - of - Thought Prompting . Chain - of - Thought ( CoT ) Prompting ( Wei et al . , 2022 ) enables complex reasoning ability with LLMs by decomposing the reasoning path of these problems into multi - step : p → z 1 · · · z k → s . Here z 1 · · · z k are sub - solutions in intermediate reasoning steps , a . k . a ’thoughts’ . CoT uses few - shot prompts to prompt LLM to output reasoning results together with its intermediate reasoning steps : { z 1 , · · · z k , s } = f θ ( p ) . Notice this framework can be done sequentially by z i = f θ ( p ; { z j | j < i } ) until reaches the final solution s ( Zhou et al . , 2022 ) . Tree - of - Thought Prompting . Tree - of - Thought ( ToT ) Prompting formulates LLM reasoning as searching in the solution space with heuristics , such as breadth - first searching ( BFS ) and depth - first searching ( DFS ) Yao et al . ( 2023 ) . When it reaches the sub - solution z i at i - th step , ToT employ an LLM to propose sub - solution candidates { z ni + 1 | n = 1 · · · N } = f θ ( p ; { z j | j ≤ i } ) . Then , it leverages an LLM to evaluate { z ni + 1 | n = 1 · · · N } for the best one and choose it as the next sub - solution z i + 1 . The above searching process is repeated until ToT obtains a satisfying solution . Although these methods improve the complex reasoning ability of LLMs with different prompting , they all prompt the LLM to reason from scratch . This reasoning scheme cannot reuse the prior knowledge in problem - solving and suffers from accumulated errors during multi - step reasoning . Thus , they fall short in tasks involving optimization and multi - step searching . As shown in Figure 2 , IO , CoT , and ToT prompting all fail to find the optimal shortest path 0 → 3 → 4 from Node 0 to Node 4 in a very simple graph , which can be easily solved by humans . When using multi - step searching for this task with ToT , it even falsely searches backward and visits Node 3 two times . The result of ToT on a more complex graph is shown in Figure 4 ( b . 2 ) . 4 Preprint Shortest path 8 0 2 3 5 7 4 6 1 2 1 5 3 4 3 2 5 2 2 3 3 2 1 5 5 2 ( a ) . An input undirected weighted graph . The task for LLM is to find the shortest path from Node 0 to Node 8 . Init Path = [ 0 ] While not reach Node 8 and not exceed max steps : Current _ node = Path [ - 1 ] Next _ node _ set = LLM _ Neighbor _ search ( Current _ node ) Best _ next _ node = LLM _ Evaluate ( Next _ node _ set ) Path . append ( Best _ next _ node ) print ( Path ) Find the shortest path between Node 0 and Node 8 . Find the shortest path between Node 0 and Node 1 . Find the shortest path between Node 0 and Node 2 . Find the shortest path between Node 0 and Node 4 . Find the shortest path between Node 0 and Node 6 . Find the shortest path between Node 0 and Node 7 . The shortest path from Node 0 to Node 1 is [ 0 , 2 , 1 ] . The shortest path from Node 0 to Node 2 is [ 0 , 2 ] . The shortest path from Node 0 to Node 6 is [ 0 , 2 , 6 ] . The shortest path from Node 0 to Node 7 is [ 0 , 2 , 7 ] . The shortest path from Node 0 to Node 4 is [ 0 , 2 , 8 , 4 ] . The shortest path from Node 0 to Node 8 is [ 0 , 3 , 7 , 8 ] . The shortest path from Node 0 to Node 8 is [ 0 , 2 , 8 ] . LLM Propose LLM Solve LLM Aggregate LLM Solve Sub - optimal ✅ Optimal ❌ Hallucinated ( c ) . Although Thought Propagation hallucinates on one analogous problem , it succeed in finding the optimal shortest path . LLM _ Neighbor _ search : “Find the neighborhood set of the given node . ” LLM _ Evaluate : “Given several input nodes , evaluate these input nodes and find the most promising one that forms the shortest path to the target node . ” 0 2 3 1 5 6 7 1 2 3 6 8 5 1 2 3 7 8 5 1 3 6 7 1 2 3 7 8 5 ( b . 1 ) . Implementation of Tree - of - Thought ( ToT ) . ( b . 2 ) . ToT fails to find a valid path in a highly dense graph shown in ( a ) due to a large searching space and hallucination in intermediate steps . ❌ Hallucinated Figure 4 : An example of TP and ToT for the Shortest - path Task . ToT ( b ) fails to solve the problem in ( a ) due to the accumulated errors in intermediate reasoning steps . Building upon solutions from analogous problems , TP ( c ) refines the initial sub - optimal solution and finally finds the optimal one . 4 M ETHODOLOGY When humans encounter a new problem , they often compare it to familiar ones with similar charac - teristics , a process known as analogical reasoning Carbonell ( 1985 ) . Thus , we aim to leverage in - sights in exploring some problems that are analogous to the input problem , i . e . analogous problems , to facilitate input problem - solving . We introduce Thought Propagation ( TP ) , a versatile analogical framework for LLM reasoning . As shown in Figure 3 ( d ) , TP actively generates analogous problems related to the input problem during the reasoning process , all without relying on external knowledge bases . It then combines the solutions from these proposed analogous problems to facilitate solving the input problem by creating an updated solution or formulating a high - level plan . We introduce the three modules of TP : LLM Propose , LLM Solve , and LLM Aggregate . Then , we give a general setup of the proposed framework and leave the implementation for each task in Section 5 . LLM Propose . Given an input problem , LLM Propose generates a set of analogous problems . Solving these analogous problems should provide distinctive insights to help solve the input one . Thus , LLM Propose generates analogous problems in two perspectives . First , the solutions from analogous problems can be transferred to yield new solutions to the input problem . In this manner , TP can reuse the solutions from analogous problems to develop new solutions to the input problem in an analogical approach instead of reasoning from scratch . Second , solving analogous problems can produce high - level plans for the input problem - solving . In this way , TP can rectify the errors during planning and improve the multi - step reasoning of the input problem . Both ways to generate analogous problems are instantiated using few - shot problem exemplars or zero - shot prompting . LLM Solve . LLM Solve serves a dual purpose : solving the input problem to produce an initial solution and solving the analogous problems proposed by LLM Propose . This module can be instantiated using existing prompting approaches such as CoT ( Wei et al . , 2022 ) . Although the solutions to analogous problems are not expert - level , the following aggregation module can assess these solutions and use the most promising one to instantiate analogical reasoning . Moreover , we introduce a multi - layer implementation of TP to improve solutions to analogous problems further . LLM Aggregate . LLM Aggregate aggregates solutions from analogous problems to enhance solving the input problem . This module is conjugated to the LLM Propose module , since it de - pends on the relationship between the input problem and its analogous counterparts generated by LLM Propose . Thus , LLM Aggregate utilizes the solutions from analogous problems in two ways . First , it prompts the LLM to develop new solutions to the input problems based on the results of analogous problems . An example of this manner is shown in Figure 4 . ( c ) . If we already obtain the shortest paths to the neighborhood nodes of the target node , only one - step reasoning is required to yield a new path to the target node . Notice this manner is different from recursive problem de - composition Khot et al . ( 2022 ) since it only requires one - step reasoning to develop a new solution to the input problem . Second , this module prompts the LLM to derive high - level plans to solve the input problem using the solutions from analogous problems . The plan is knowledge - intensive , thus the LLM can carry this plan in every round of decision - making when solving long - trial planning 5 Preprint Table 1 : The performance of TP and other baselines on Shortest - path Reasoning Task . LLM - Backend Method 0 - shot 1 - shot 5 - shot OR ↑ FR ↑ OLR ↓ OR ↑ FR ↑ OLR ↓ OR ↑ FR ↑ OLR ↓ PaLM - 2 IO 0 . 14 0 . 37 0 . 62 0 . 28 0 . 48 0 . 43 0 . 26 0 . 41 0 . 35 CoT 0 . 24 0 . 52 0 . 40 0 . 33 0 . 45 0 . 41 0 . 29 0 . 56 0 . 39 BaG 0 . 23 0 . 47 0 . 44 0 . 28 0 . 52 0 . 45 0 . 26 0 . 52 0 . 51 ToT - - - - - - - - - TP 0 . 36 0 . 57 0 . 37 0 . 38 0 . 59 0 . 36 0 . 37 0 . 62 0 . 36 GPT - 3 . 5 IO 0 . 33 0 . 50 0 . 17 0 . 62 0 . 86 0 . 15 0 . 61 0 . 9 0 . 27 CoT 0 . 26 0 . 35 0 . 13 0 . 58 0 . 85 0 . 16 0 . 52 0 . 85 0 . 32 BaG 0 . 25 0 . 32 0 . 13 0 . 61 0 . 87 0 . 14 0 . 64 0 . 86 0 . 13 ToT 0 . 22 0 . 42 0 . 82 0 . 38 0 . 79 0 . 72 0 . 58 0 . 93 0 . 32 TP 0 . 65 0 . 89 0 . 12 0 . 74 0 . 89 0 . 07 0 . 73 0 . 91 0 . 10 GPT - 4 IO 0 . 78 1 . 00 0 . 10 0 . 80 0 . 99 0 . 08 0 . 81 1 . 00 0 . 08 CoT 0 . 76 1 . 00 0 . 10 0 . 75 1 . 00 0 . 11 0 . 78 1 . 00 0 . 08 BaG 0 . 77 0 . 98 0 . 09 0 . 80 0 . 99 0 . 09 0 . 78 1 . 00 0 . 11 ToT 0 . 46 0 . 84 0 . 52 0 . 46 0 . 73 0 . 40 0 . 77 1 . 00 0 . 07 TP 0 . 88 1 . 00 0 . 05 0 . 88 1 . 00 0 . 04 0 . 86 1 . 00 0 . 05 tasks . After generating new solutions or plans using the results of analogous problems , the LLM evaluates these outputs and chooses the best one to improve input problem - solving . Multi - layer Implementation . As shown in Figure 3 ( d ) , we can stack K layers of TP to leverage the solutions from K - hop analogous problems to improve the solution to the input problem . Thus , existing methods , such as IO , CoT , ToT , etc . , can be viewed as the special cases of TP by setting K = 0 since they solve each problem from scratch and do not instantiate analogical reasoning . By setting K = 1 , TP aggregates the solutions from 1 - hop analogous problems to refine the solution to the input one . TP further allows hierarchical refinement by setting K ≥ 2 . In this case , the problems in i - th layer are the analogous problems in ( i − 1 ) - th layer ( i ≥ 1 ) . Thus , we can use the solutions from ( i ) - th - layer analogous problems to refine ( i − 1 ) - th - layer analogous problems’ solutions . This hierarchical refinement finishes until the solution to the input problem is refined . General Setup and Recipe . TP allows plug - and - play generalization and enhancement to different tasks , since we can use existing prompting methods for LLM reasoning to instantiate LLM Solve . Using IO prompting and CoT for most tasks is sufficient in our experiments . For more complex prob - lems involving autonomous planning and exploration , prompting methods that synergize thinking and action such as ReAct ( Yao et al . , 2022 ) is needed . Although TP builds upon existing prompting methods , it aids their reasoning - from - scratch manner with the hint of solving analogous problems and leads to significant performance gain . Complexity Analysis . The complexity of Thought Propagation mainly comes from two perspec - tives . Firstly , the complexity exponentially increases as the layer number k increases . However , the K - hop analogous problems are intuitively less related to the input problem , and considering such long - range analogous problems only leads to marginal performance gain . Thus , we only consider implementing up to 2 - layers of TP to trade off performance between complexity . We find the perfor - mance gain in 2 - layer TP is marginal when compared with 1 - layer TP , but 2 - layer TP leads to more token expenses . 1 - layer TP achieves very competitive performances against the baselines with no significant increase in token expenses . For example , 1 - layer TP outperforms ToT by a large margin in different LLM backends but shares similar token expenses . Secondly , instantiating LLM Solve under 5 - shot setting is more expensive than 0 - shot setting due to increasing prompting exemplars . We provide a detailed quantitative complexity analysis in Section 5 . 1 . 5 E XPERIMENTS We employ three challenging tasks , such as Shortest - Path Reasoning , Creative Writing , and LLM - Agent Planning , to evaluate the proposed method ( TP ) . We generate 100 shortest - path problems with non - trivial solutions for the Shortest - Path Reasoning task . We employ the dataset proposed by Yao et . al . ( Yao et al . , 2023 ) with 100 writing problems for the Creative Writing task . And we use ALFWorld ( Shridhar et al . , 2021 ) game suite to instantiate the LLM - Agent Planning task with 134 6 Preprint O R ↑ Layers Layers Layers ( a ) . Complexity ( b ) . 0 - shot ( c ) . 1 - shot ( d ) . 5 - shot Figure 5 : Study on the complexity and performance of TP under different configurations . environments . TP finds the most optimal shortest paths , generates the most coherent messages , and achieves the highest task completion rate in three tasks . 5 . 1 S HORTEST - PATH R EASONING The Shortest - path Reasoning task is to find the shortest path from the source node to the target node in a weighted undirected graph . This task is challenging for LLMs since 1 . the graph structure does not conform to the sequential corpus for training LLMs , and 2 . this discrete optimization problem requires searching in an explosively large space . Task Setup . For an input graph , LLM is required to find the shortest path from the source node to the target node using the baselines and TP . For a graph with N nodes , the source node is set to Node 0 , and the target node is set to Node ( N − 1 ) . We filter out the trivial cases where the shortest path only contains one edge . Detailed task setup is in Appendix . Baselines and LLM Backends . We use standard ( IO ) prompting ( Brown et al . , 2020 ) , Chain - of - Thought ( CoT ) ( Wei et al . , 2022 ) , Build - a - Graph ( BaG ) ( Wang et al . , 2023a ) , and Tree - of - Thought ( ToT ) ( Yao et al . , 2023 ) as the baseline methods . The implementation and prompting exemplars of all the baselines are shown in Appendix . We evaluate all the methods under 0 - shot , 1 - shot , and 5 - shot prompting settings . We conduct experiments on three LLM backends such as PaLM 2 ( Bison ) ( Anil et al . , 2023 ) , GPT - 3 . 5 ( OpenAI , 2022 ) , and GPT - 4 ( OpenAI , 2023 ) . Thought Propagation Setup . Suppose the input problem is finding the shortest path from Node 0 to Node ( N − 1 ) in the graph G i with N nodes . LLM Propose prompts the LLM to propose analogous problems of finding the shortest path from Node 0 to the neighborhood nodes of Node ( N − 1 ) . LLM Solve is implemented with IO prompting with 0 - shot / 1 - shot / 5 - shot prompting . This module outputs the initial solutions to the input problem and analogous problems . Afterward , LLM Aggregate uses the results of analogous problems to develop a new path to the input problem . Then , it compares the new path with the initial path and outputs a better one . The implementation and prompts are shown in Appendix . Evaluation Metrics . Denote the length of shortest path of graph G i as L ∗ i . Let the length of the valid path output by LLM be L i . N is the total number of graphs . N optimal and N feasible are the number of optimal paths and valid paths output by LLMs . We propose to use three metrics to evaluate the performance of different methods in Shortest - path Reasoning . Optimal Rate ( OR ) = N optimal / N measures the percentage of paths generated by LLMs being the optimal paths . The higher the better . Feasible Rate ( FR ) = N feasible / N measures the percentage of paths generated by LLMs being the valid paths . The higher the better . Over - Length Rate ( OLR ) = (cid:80) N feasible i = 1 ( L i − L ∗ i ) / L ∗ i measures the over - length of valid paths generated by LLMs over the optimal ones . The lower the better . These metrics take the following forms . Results . The quantitative results of TP and the baselines are shown in Table 1 . TP achieves a sig - nificant performance gain over the baselines by generating the most optimal and valid shortest paths when testing on three LLM backends with different model capacities . Moreover , the valid paths generated by TP are the closest to the optimal paths when compared with the baselines due to the lowest Over - Length Rate ( OLR ) . On the PaLM - 2 backend , ToT fails to find valid paths from source nodes to target nodes . For GPT - 3 . 5 and GPT - 4 backends , ToT underperforms IO prompting . We find ToT sometimes searches backward or even fails to find the valid path due to the accumulated error shown in Figure 4 . CoT only outperforms IO on PaLM - 2 where IO performs badly . Nevertheless , we observe no significant preference gain of CoT over IO on the other LLM backends . 7 Preprint Table 2 : The performance of Thought Propagation ( TP ) and baselines on Creative Writing Task . Metric Coherent Score User Study LLM - Backend GPT - 3 . 5 GPT - 4 GPT - 3 . 5 GPT - 4 IO 6 . 087 ± 2 . 229 6 . 193 ± 1 . 953 14 % 7 % CoT 6 . 654 ± 2 . 201 6 . 927 ± 1 . 508 21 % 15 % ToT 6 . 856 ± 1 . 975 7 . 684 ± 1 . 141 26 % 33 % TP 7 . 000 ± 1 . 783 7 . 989 ± 1 . 453 39 % 45 % Although the 1 - shot setting leads to performance gains over 0 - shot on most prompting methods , the performance gains of 5 - shot over 1 - shot setting are unexpectedly marginal , or sometimes worse than 1 - shot setting . There are two reasons for this phenomenon . Firstly , the 5 - shot setting feeds long prompting exemplars to LLM , which potentially contains more redundant information . Secondly , the 5 - shot setting sometimes leads to output cutoff due to the maximal token limit of LLMs . We leave the in - depth exploration of this phenomenon in our future work . Impact of Layers on Performance . We further study the influence of layer numbers of TP on the complexity and performance in the Shortest - path Task . As shown in Figure 5 , 1 - layer TP has similar token costs as ToT in different settings . However , 1 - layer TP already achieves very competitive performance in finding the optimal shortest path . Also , the performance gain of 1 - layer TP over 0 - layer TP ( IO ) is significant . Although 2 - layer TP achieves the best performance as shown in Table 1 , the performance gain of 2 - layer TP over 1 - layer TP is less significant . And Figure 5 ( a ) . indicates the increase in the token cost of TP with 2 layers . Thus we aim to harness multi - hop analogous problems with decreased expenses in our future work . More results are shown in Appendix . 5 . 2 C REATIVE W RITING We proceed to evaluate Thought Propagation on the Creative Writing task ( Yao et al . , 2023 ) . Given 4 randomly sampled sentences , the goal of this task is to generate 4 paragraphs ending with these sentences respectively to construct a coherent message . Such task challenges LLM reasoning by highly creative thinking and planning . Task Setup . We follow the task setup proposed by Yao et . al . ( Yao et al . , 2023 ) that consists of 100 test instances . We use the coherent score ( 1 - 10 scalar score generated by GPT - 4 ) and user study to evaluate the coherence of generated messages . The details of the evaluation are in Appendix . Baselines and LLM Backends . We consider three baselines : IO prompting ( Brown et al . , 2020 ) , CoT ( Wei et al . , 2022 ) and ToT ( Yao et al . , 2023 ) . All these methods use zero - shot prompts due to the creative nature of writing ( Yao et al . , 2023 ) . The baseline setup and prompting exemplars are shown in Appendix . We instantiate each method using GPT - 3 . 5 and GPT - 4 backends . Thought Propagation Setup . We build Thought Propagation with one layer for this task to maintain a fair comparison with the baselines . Every module of Thought Propagation is implemented with zero - shot prompts . LLM Propose rephrases the four input sentences using the simple prompt : ”Rephrase the input sentences but do not change their meanings or orders . ” , and produces the ana - logical problem which is to generate a writing plan to write a message with the rephrased sentences . This module generates 5 analogous problems to ensure a fair comparison with baselines . LLM Solve uses CoT prompting to generate writing plans to write four paragraphs that end with four given sentences . This module is employed to solve the input and proposed analogous problems , leading to 6 plans . Since the rephrased sentences share similar contextual information with the input sentences , their writing plans potentially apply to the input ones . Thus LLM Aggregate evalu - ates all 6 plans output by LLM Solve and outputs the most promising plan for the input problem . Finally , the LLM is asked to write the whole message in four paragraphs using the most promising plan . The prompting exemplars of TP in the Creative Writing task are shown in Appendix . Results . Table 2 shows the performance of TP and baselines with GPT - 3 . 5 and GPT - 4 . Thought Propagation outperforms the baselines with the highest coherent scores on both GPT - 3 . 5 and GPT - 4 backends . Moreover , TP achieves the highest human preference in user study . Additional findings are all the methods achieve better performance on GPT - 4 due to the improved model capability . 8 Preprint Table 3 : The performance of different variant models of Thought Propagation ( TP ) and baselines on LLM - Agent planning in the ALFWORLD dataset ( Shridhar et al . , 2021 ) . We reproduce the result of Reflexion ( Shinn et al . , 2023 ) . Other baseline results are quoted from ReACT ( Yao et al . , 2022 ) . Method Pick Clean Heat Cool Look Pick 2 All BULTER 33 26 70 76 17 12 22 BULTER G 46 39 74 100 22 24 37 Act ( best of 6 ) 88 42 74 67 72 41 45 ReAct ( avg ) 65 39 83 76 55 24 57 ReAct ( best of 6 ) 92 58 96 86 78 41 71 Reflexion 100 . 00 74 . 19 73 . 91 85 . 71 66 . 67 70 . 59 79 . 1 TP - SR - SE 100 . 00 77 . 42 65 . 22 95 . 24 94 . 44 82 . 35 85 . 82 TP - SE 91 . 67 83 . 87 69 . 56 100 . 00 83 . 3 70 . 59 83 . 68 TP - SR - SM 95 . 83 96 . 77 78 . 26 100 . 00 94 . 44 88 . 24 92 . 54 TP - SM 100 . 00 93 . 55 86 . 96 100 . 00 94 . 44 88 . 24 94 . 78 5 . 3 LLM - A GENT P LANNING LLM - Agents use LLMs as the core component to interact with environments and autonomously make plans and decisions . We study the capability of TP to formulate high - level , knowledge - intensive plans for LLM - Agents in an analogical way to improve the task completion rate . Task Setup . ALFWorld ( Shridhar et al . , 2021 ) is a text - based game suite with various interactive housework environments aligned with ALFRED and TextWorld ( C ˆ ot ´ e et al . , 2019 ; Shridhar et al . , 2020 ) . It contains six types of tasks with 134 unseen environments for evaluation ( Yao et al . , 2022 ; Shinn et al . , 2023 ) . Baselines and LLM Backends . BULTER is a trainable parameterized method based on reinforce - ment learning ( Shridhar et al . , 2021 ) . ReAct ( Yao et al . , 2022 ) builds LLM - Agents with synergy between reasoning traces and action trials . Act ( Yao et al . , 2022 ) removes the reasoning trace of Re - Act . Reflexion improves ReAct with verbal reflections on previous failures in the same task to refine the planning of new trials ( Shinn et al . , 2023 ) . We run Reflexion for 6 trials since its performance is stable after 4 trials . We use GPT - 3 for LLM - Agents following Shinn et . al . Thought Propagation Setup . Unlike Reflexion which reflects upon previous failure in the same task to help task completion in the next planning trial , Thought Propagation aims to aggregate useful information from successful trials in similar but different tasks to improve task completion . Trials S u cc e ss R a t e ( % ) Figure 6 : The success rate of task completion in different trials . Thus , LLM Propose uses a zero - shot prompt to assess the similarity score between the original task and the rest with successful planning trials . The rest tasks with the top two similarity scores are treated as two analogical problems . LLM Solve employs ReAct to instantiate LLM - Agent planning in the original task following Re - flexion ( Shinn et al . , 2023 ) . LLM Aggregate uses a zero - shot prompt to formulate two plans to help complete the original problem based on the success - ful trials of analogical problems and the planning trial of the original problem . Then , it evaluates the two plans and outputs the better one to guide the LLM - Agent to complete the task . We run Thought Propagation for 6 trials to maintain consistency with Reflexion . The prompt exemplars are shown in Appendix . ( Shinn et al . , 2023 ) . Variant Models . We introduce two strategies for LLM Aggregate for plan evaluation : 1 . Self - Evaluation ( SE ) : The LLM evaluates two plans by zero - shot prompt and outputs the better one ; 2 . Simulation ( SM ) : The LLM - Agent executes new planning trials in the task environment using two plans and outputs a better one . We additionally add Self - Reflection ( SR ) modules to reflect LLM - Agent on its own failures just like Reflexion . These implementations lead to four variant models of Thought Propagation : 1 ) . TP - SR - SE : Thought Propagation with Self - Reflection and Self - Evaluation ; 2 ) . TP - SE : Thought Propagation with Self - Evaluation ; 3 ) . TP - SR - SM : Thought Propagation with Self - Reflection and Simulation ; 4 ) . TP - SM : Thought Propagation with Simulation . 9 Preprint Results . Table 3 shows good performance of Thought Propagation over the learnable parameterized method and other LLM - Agent baselines . Thought Propagation achieves large performance gains even without a memory module to store its previous failures ( TP - SE / TP - SM ) . This shows the supe - riority of the reflection upon successful planning in completing similar tasks . Moreover , Thought Propagation also works well with the additional memory module to store previous failures ( TP - SR - SE / TP - SR - SM ) . Figure 6 shows that different variant models of Thought Propagation achieve consistent performance improvement by iterative reflecting on successful planning in similar tasks . We show how TP formulates a constructive plan from solving ”examine the alarmclock with the desklamp” task to successfully complete ”examine the book with the desklamp” task , where ReAct and Reflexion are trapped in a loop , in Appendix . 6 C ONCLUSIONS Existing prompting approaches for LLM reasoning cannot leverage the insights of solving similar problems and suffer from accumulated errors in multi - step reasoning , due to reasoning from scratch . To address these issues , we propose Thought Propagation ( TP ) , which explores analogous problems to yield a refined solution or a knowledge - intensive plan in an analogical approach to facilitate new problem - solving . TP is compatible with existing prompting methods , showing plug - and - play generalization and enhancement to a wide range of tasks such as Shortest - path Planning , Creative Writing , and LLM - Agent Planning . Future directions would further enhance the performance and efficiency of the proposed framework . 10 Preprint R EFERENCES Rohan Anil , Andrew M Dai , Orhan Firat , Melvin Johnson , Dmitry Lepikhin , Alexandre Passos , Siamak Shakeri , Emanuel Taropa , Paige Bailey , Zhifeng Chen , et al . Palm 2 technical report . arXiv preprint arXiv : 2305 . 10403 , 2023 . Akari Asai , Sewon Min , Zexuan Zhong , and Danqi Chen . Retrieval - based language models and applications . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Volume 6 : Tutorial Abstracts ) , pp . 41 – 46 , 2023 . Paul Bartha . Analogy and analogical reasoning . 2013 . Maciej Besta , Nils Blach , Ales Kubicek , Robert Gerstenberger , Lukas Gianinazzi , Joanna Gajda , Tomasz Lehmann , Michal Podstawski , Hubert Niewiadomski , Piotr Nyczyk , et al . Graph of thoughts : Solving elaborate problems with large language models . arXiv preprint arXiv : 2308 . 09687 , 2023 . Bhavya Bhavya , Jinjun Xiong , and Chengxiang Zhai . Analogy generation by prompting large lan - guage models : A case study of instructgpt . arXiv preprint arXiv : 2210 . 04186 , 2022 . Bhavya Bhavya , Jinjun Xiong , and Chengxiang Zhai . Cam : A large language model - based creative analogy mining framework . In Proceedings of the ACM Web Conference 2023 , pp . 3903 – 3914 , 2023 . Sebastian Borgeaud , Arthur Mensch , Jordan Hoffmann , Trevor Cai , Eliza Rutherford , Katie Milli - can , George Bm Van Den Driessche , Jean - Baptiste Lespiau , Bogdan Damoc , Aidan Clark , et al . Improving language models by retrieving from trillions of tokens . In International conference on machine learning , pp . 2206 – 2240 . PMLR , 2022 . Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . Language models are few - shot learners . Advances in neural information processing systems , 33 : 1877 – 1901 , 2020 . Tianle Cai , Xuezhi Wang , Tengyu Ma , Xinyun Chen , and Denny Zhou . Large language models as tool makers . arXiv preprint arXiv : 2305 . 17126 , 2023 . Jaime Guillermo Carbonell . Derivational analogy : A theory of reconstructive problem solving and expertise acquisition . Carnegie - Mellon University , Department of Computer Science , 1985 . Chi - Min Chan , Weize Chen , Yusheng Su , Jianxuan Yu , Wei Xue , Shanghang Zhang , Jie Fu , and Zhiyuan Liu . Chateval : Towards better llm - based evaluators through multi - agent debate . arXiv preprint arXiv : 2308 . 07201 , 2023 . Jiangjie Chen , Rui Xu , Ziquan Fu , Wei Shi , Zhongqiao Li , Xinbo Zhang , Changzhi Sun , Lei Li , Yanghua Xiao , and Hao Zhou . E - kar : A benchmark for rationalizing natural language analogical reasoning . arXiv preprint arXiv : 2203 . 08480 , 2022 . Xinyun Chen , Maxwell Lin , Nathanael Sch¨arli , and Denny Zhou . Teaching large language models to self - debug . arXiv preprint arXiv : 2304 . 05128 , 2023a . Zhikai Chen , Haitao Mao , Hang Li , Wei Jin , Hongzhi Wen , Xiaochi Wei , Shuaiqiang Wang , Dawei Yin , Wenqi Fan , Hui Liu , et al . Exploring the potential of large language models ( llms ) in learning on graphs . arXiv preprint arXiv : 2307 . 03393 , 2023b . Marc - Alexandre Cˆot´e , Akos K´ad´ar , Xingdi Yuan , Ben Kybartas , Tavian Barnes , Emery Fine , James Moore , Matthew Hausknecht , Layla El Asri , Mahmoud Adada , et al . Textworld : A learning environment for text - based games . In Computer Games : 7th Workshop , CGW 2018 , Held in Con - junction with the 27th International Conference on Artificial Intelligence , IJCAI 2018 , Stockholm , Sweden , July 13 , 2018 , Revised Selected Papers 7 , pp . 41 – 75 . Springer , 2019 . Nouha Dziri , Ximing Lu , Melanie Sclar , Xiang Lorraine Li , Liwei Jian , Bill Yuchen Lin , Peter West , Chandra Bhagavatula , Ronan Le Bras , Jena D Hwang , et al . Faith and fate : Limits of transformers on compositionality . arXiv preprint arXiv : 2305 . 18654 , 2023 . 11 Preprint Justin Gilmer , Samuel S Schoenholz , Patrick F Riley , Oriol Vinyals , and George E Dahl . Neural message passing for quantum chemistry . In International conference on machine learning , pp . 1263 – 1272 . PMLR , 2017 . Rogers P Hall . Computational approaches to analogical reasoning : A comparative analysis . Artifi - cial intelligence , 39 ( 1 ) : 39 – 120 , 1989 . Will Hamilton , Zhitao Ying , and Jure Leskovec . Inductive representation learning on large graphs . Advances in neural information processing systems , 30 , 2017 . Xiaoxin He , Xavier Bresson , Thomas Laurent , and Bryan Hooi . Explanations as features : Llm - based features for text - attributed graphs . arXiv preprint arXiv : 2305 . 19523 , 2023 . Gautier Izacard , Patrick Lewis , Maria Lomeli , Lucas Hosseini , Fabio Petroni , Timo Schick , Jane Dwivedi - Yu , Armand Joulin , Sebastian Riedel , and Edouard Grave . Few - shot learning with re - trieval augmented language models . arXiv preprint arXiv : 2208 . 03299 , 2022 . Jinhao Jiang , Kun Zhou , Zican Dong , Keming Ye , Wayne Xin Zhao , and Ji - Rong Wen . Structgpt : A general framework for large language model to reason over structured data . arXiv preprint arXiv : 2305 . 09645 , 2023 . Tushar Khot , Harsh Trivedi , Matthew Finlayson , Yao Fu , Kyle Richardson , Peter Clark , and Ashish Sabharwal . Decomposed prompting : A modular approach for solving complex tasks . arXiv preprint arXiv : 2210 . 02406 , 2022 . Thomas N . Kipf and Max Welling . Semi - supervised classification with graph convolutional net - works . In The International Conference on Representation Learning , 2017 . Tian Lan , Deng Cai , Yan Wang , Heyan Huang , and Xian - Ling Mao . Copy is all you need . In The Eleventh International Conference on Learning Representations , 2023 . URL https : / / openreview . net / forum ? id = CROlOA9Nd8C . Guohao Li , Hasan Abed Al Kader Hammoud , Hani Itani , Dmitrii Khizbullin , and Bernard Ghanem . Camel : Communicative agents for” mind” exploration of large scale language model society . arXiv preprint arXiv : 2303 . 17760 , 2023 . Hao Liu , Carmelo Sferrazza , and Pieter Abbeel . Chain of hindsight aligns language models with feedback . arXiv preprint arXiv : 2302 . 02676 , 3 , 2023a . Jiacheng Liu , Alisa Liu , Ximing Lu , Sean Welleck , Peter West , Ronan Le Bras , Yejin Choi , and Han - naneh Hajishirzi . Generated knowledge prompting for commonsense reasoning . arXiv preprint arXiv : 2110 . 08387 , 2021 . Pengfei Liu , Weizhe Yuan , Jinlan Fu , Zhengbao Jiang , Hiroaki Hayashi , and Graham Neubig . Pre - train , prompt , and predict : A systematic survey of prompting methods in natural language pro - cessing . ACM Computing Surveys , 55 ( 9 ) : 1 – 35 , 2023b . Ruibo Liu , Jason Wei , Shixiang Shane Gu , Te - Yen Wu , Soroush Vosoughi , Claire Cui , Denny Zhou , and Andrew M Dai . Mind’s eye : Grounded language model reasoning through simulation . arXiv preprint arXiv : 2210 . 05359 , 2022 . Mikołaj Małki´nski and Jacek Ma´ndziuk . Deep learning methods for abstract visual reasoning : A survey on raven’s progressive matrices . arXiv preprint arXiv : 2201 . 12382 , 2022 . Gr´egoire Mialon , Roberto Dess ` ı , Maria Lomeli , Christoforos Nalmpantis , Ram Pasunuru , Roberta Raileanu , Baptiste Rozi ` ere , Timo Schick , Jane Dwivedi - Yu , Asli Celikyilmaz , et al . Augmented language models : a survey . arXiv preprint arXiv : 2302 . 07842 , 2023 . OpenAI . https : / / platform . openai . com / docs / models / gpt - 3 - 5 . 2022 . OpenAI . Gpt - 4 technical report , 2023 . Joseph Priestley . “The” History and Present State of Electricity : With Original Experiments . C . Bathurst and T . Lowndes , in Fleet - Street , J . Rivington and J . Johnson , in . . . , 1775 . 12 Preprint Chen Qian , Huayi Tang , Zhirui Yang , Hong Liang , and Yong Liu . Can large language models empower molecular property prediction ? arXiv preprint arXiv : 2307 . 07443 , 2023 . Shuofei Qiao , Yixin Ou , Ningyu Zhang , Xiang Chen , Yunzhi Yao , Shumin Deng , Chuanqi Tan , Fei Huang , and Huajun Chen . Reasoning with language model prompting : A survey . arXiv preprint arXiv : 2212 . 09597 , 2022 . Timo Schick , Jane Dwivedi - Yu , Roberto Dess ` ı , Roberta Raileanu , Maria Lomeli , Luke Zettlemoyer , Nicola Cancedda , and Thomas Scialom . Toolformer : Language models can teach themselves to use tools . arXiv preprint arXiv : 2302 . 04761 , 2023 . Weijia Shi , Sewon Min , Michihiro Yasunaga , Minjoon Seo , Rich James , Mike Lewis , Luke Zettle - moyer , and Wen - tau Yih . Replug : Retrieval - augmented black - box language models . arXiv preprint arXiv : 2301 . 12652 , 2023 . Noah Shinn , Beck Labash , and Ashwin Gopinath . Reflexion : an autonomous agent with dynamic memory and self - reflection . arXiv preprint arXiv : 2303 . 11366 , 2023 . Mohit Shridhar , Jesse Thomason , Daniel Gordon , Yonatan Bisk , Winson Han , Roozbeh Mottaghi , Luke Zettlemoyer , and Dieter Fox . Alfred : A benchmark for interpreting grounded instructions for everyday tasks . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pp . 10740 – 10749 , 2020 . Mohit Shridhar , Xingdi Yuan , Marc - Alexandre Cote , Yonatan Bisk , Adam Trischler , and Matthew Hausknecht . Alfworld : Aligning text and embodied environments for interactive learning . In International Conference on Learning Representations , 2021 . Oren Sultan and Dafna Shahaf . Life is a circus and we are the clowns : Automatically finding analogies between situations and processes . arXiv preprint arXiv : 2210 . 12197 , 2022 . Heng Wang , Shangbin Feng , Tianxing He , Zhaoxuan Tan , Xiaochuang Han , and Yulia Tsvetkov . Can language models solve graph problems in natural language ? arXiv preprint arXiv : 2305 . 10037 , 2023a . Lei Wang , Chen Ma , Xueyang Feng , Zeyu Zhang , Hao Yang , Jingsen Zhang , Zhiyuan Chen , Jiakai Tang , Xu Chen , Yankai Lin , et al . A survey on large language model based autonomous agents . arXiv preprint arXiv : 2308 . 11432 , 2023b . Xuezhi Wang , Jason Wei , Dale Schuurmans , Quoc Le , Ed Chi , Sharan Narang , Aakanksha Chowdh - ery , and Denny Zhou . Self - consistency improves chain of thought reasoning in language models . arXiv preprint arXiv : 2203 . 11171 , 2022a . Yufei Wang , Wanjun Zhong , Liangyou Li , Fei Mi , Xingshan Zeng , Wenyong Huang , Lifeng Shang , Xin Jiang , and Qun Liu . Aligning large language models with human : A survey . arXiv preprint arXiv : 2307 . 12966 , 2023c . Zichao Wang , Weili Nie , Zhuoran Qiao , Chaowei Xiao , Richard Baraniuk , and Anima Anandkumar . Retrieval - based controllable molecule generation . arXiv preprint arXiv : 2208 . 11126 , 2022b . Taylor Webb , Keith J Holyoak , and Hongjing Lu . Emergent analogical reasoning in large language models . Nature Human Behaviour , pp . 1 – 16 , 2023 . Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Fei Xia , Ed Chi , Quoc V Le , Denny Zhou , et al . Chain - of - thought prompting elicits reasoning in large language models . Advances in Neural Information Processing Systems , 35 : 24824 – 24837 , 2022 . Qingyun Wu , Gagan Bansal , Jieyu Zhang , Yiran Wu , Shaokun Zhang , Erkang Zhu , Beibin Li , Li Jiang , Xiaoyun Zhang , and Chi Wang . Autogen : Enabling next - gen llm applications via multi - agent conversation framework . arXiv preprint arXiv : 2308 . 08155 , 2023 . Shunyu Yao , Jeffrey Zhao , Dian Yu , Nan Du , Izhak Shafran , Karthik Narasimhan , and Yuan Cao . React : Synergizing reasoning and acting in language models . arXiv preprint arXiv : 2210 . 03629 , 2022 . 13 Preprint Shunyu Yao , Dian Yu , Jeffrey Zhao , Izhak Shafran , Thomas L Griffiths , Yuan Cao , and Karthik Narasimhan . Tree of thoughts : Deliberate problem solving with large language models . arXiv preprint arXiv : 2305 . 10601 , 2023 . Michihiro Yasunaga , Armen Aghajanyan , Weijia Shi , Richard James , Jure Leskovec , Percy Liang , Mike Lewis , Luke Zettlemoyer , and Wen - tau Yih . Retrieval - augmented multimodal language modeling . 2023 . Siyu Yuan , Jiangjie Chen , Changzhi Sun , Jiaqing Liang , Yanghua Xiao , and Deqing Yang . Analo - gykb : Unlocking analogical reasoning of language models with a million - scale knowledge base . arXiv preprint arXiv : 2305 . 05994 , 2023 . Ningyu Zhang , Lei Li , Xiang Chen , Xiaozhuan Liang , Shumin Deng , and Huajun Chen . Multimodal analogical reasoning over knowledge graphs . arXiv preprint arXiv : 2210 . 00312 , 2022a . Zhuosheng Zhang , Aston Zhang , Mu Li , and Alex Smola . Automatic chain of thought prompting in large language models . arXiv preprint arXiv : 2210 . 03493 , 2022b . Wayne Xin Zhao , Kun Zhou , Junyi Li , Tianyi Tang , Xiaolei Wang , Yupeng Hou , Yingqian Min , Beichen Zhang , Junjie Zhang , Zican Dong , et al . A survey of large language models . arXiv preprint arXiv : 2303 . 18223 , 2023 . Denny Zhou , Nathanael Sch ¨ arli , Le Hou , Jason Wei , Nathan Scales , Xuezhi Wang , Dale Schuur - mans , Claire Cui , Olivier Bousquet , Quoc Le , et al . Least - to - most prompting enables complex reasoning in large language models . arXiv preprint arXiv : 2205 . 10625 , 2022 . Xizhou Zhu , Yuntao Chen , Hao Tian , Chenxin Tao , Weijie Su , Chenyu Yang , Gao Huang , Bin Li , Lewei Lu , Xiaogang Wang , et al . Ghost in the minecraft : Generally capable agents for open - world enviroments via large language models with text - based knowledge and memory . arXiv preprint arXiv : 2305 . 17144 , 2023a . Yutao Zhu , Huaying Yuan , Shuting Wang , Jiongnan Liu , Wenhan Liu , Chenlong Deng , Zhicheng Dou , and Ji - Rong Wen . Large language models for information retrieval : A survey . arXiv preprint arXiv : 2308 . 07107 , 2023b . 14