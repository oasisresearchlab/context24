Filtering Offensive Language in Online Communities using Grammatical Relations Zhi Xu Department of Computer Science and Engineering The Pennsylvania State University University Park , PA 16802 zux103 @ cse . psu . edu Sencun Zhu Department of Computer Science and Engineering The Pennsylvania State University University Park , PA 16802 szhu @ cse . psu . edu ABSTRACT Oﬀensive language has arisen to be a big issue to the health of both online communities and their users . To the online community , the spread of oﬀensive language undermines its reputation , drives users away , and even directly aﬀects its growth . To users , viewing oﬀensive language brings negative inﬂuence to their mental health , especially for children and youth . When oﬀensive language is detected in a user message , a problem arises about how the oﬀensive language should be removed , i . e . the oﬀensive language ﬁltering problem . To solve this problem , manual ﬁltering approach is known to produce the best ﬁltering result . However , manual ﬁltering is costly in time and labor thus can not be widely applied . In this paper , we analyze the oﬀensive language in text messages posted in online communities , and propose a new automatic sentence - level ﬁltering approach that is able to semantically remove the oﬀensive language by utilizing the grammatical relations among words . Comparing with ex - isting automatic ﬁltering approaches , the proposed ﬁltering approach provides ﬁltering results much closer to manual ﬁltering . To demonstrate our work , we created a dataset by manu - ally ﬁltering over 11 , 000 text comments from the YouTube website . Experiments on this dataset show over 90 % agree - ment in ﬁltered results between the proposed approach and manual ﬁltering approach . Moreover , we show the overhead of applying proposed approach to user comments ﬁltering is reasonable , making it practical to be adopted in real life applications . 1 . INTRODUCTION Online social networking ( OSN ) websites have enjoyed a great success in recent years . People in OSN websites form social aggregations , called online communities [ 8 ] . These on - line communities have become the new frontier in today’s so - cial relationships and provide great places for self - expression and the exchange of ideas . Many of them , such as Facebook , have grown to huge communities with millions of registered members 1 . 1 Facebook statistics , at http : / / www . facebook . com / press / info . php ? statistics CEAS 2010 - Seventh annual Collaboration , Electronic messaging , Anti - Abuse and Spam Conference July 13 - 14 , 2010 , Redmond , Washington , US Online communities , being virtual , however , have also en - couraged the use of oﬀensive language . The deﬁnition of oﬀensive language can be subjective because diﬀerent view - ers have diﬀerent feelings about the same content . In this paper , we accept the deﬁnition of oﬀensive language as text content including gutter language , sexually explicit mate - rial , racist , graphic violence , or any other content that may be considered oﬀensive on social , religious , cultural or moral grounds 2 . Unfortunately , oﬀensive language has spread into almost every corner of online communities . A study , done by ScanSafe , shows that up to 80 % of blogs contain oﬀensive language [ 5 ] . Posting messages with oﬀensive language intentionally has become a major way of cyber - bullying in online commu - nities . To users , oﬀensive language can be very harmful to their mental health , especially for children and youth . To the online community , the deluge of oﬀensive language undermines the community’s reputation , drives users away , and even directly aﬀects its growth . For example , an iPhone application , Tweetie , was once rejected by Apple company in March 2009 , for bringing oﬀensive language posted in the Twitter community to iPhone users . People have realized the problems brought by oﬀensive language in online communities . And many eﬀorts have been made on detecting the existence of oﬀensive language within user messages , such as [ 10 ] and [ 3 ] . However , detection alone is not enough to eliminate the hazard caused by oﬀensive language . When oﬀensive con - tents ( e . g . , oﬀensive words ) are detected within a user mes - sage , a question arises naturally about how the detected oﬀensive content should be removed from message . The process of removing oﬀensive content from user messages is called oﬀensive language ﬁltering . Consider a sentence consisting of a sequence of words . The problem of identi - fying words that should be removed in oﬀensive language ﬁltering is called oﬀensive language ﬁltering problem . In this paper , we propose a sentence - level semantic ﬁlter - ing approach , which utilizes grammatical relations among words to semantically remove oﬀensive content in a sen - tence . Speciﬁcally , for each sentence within a user message , we ﬁrst identify oﬀensive words , and then extract semantic relations and syntactic relations among words in the sen - tence . Based on the extracted relations , we estimate which words should be removed with those oﬀensive words using two heuristic rules , i . e . “Modiﬁcation Relation Rule” and 2 The Internet Content Rating Association ( ICRA ) , at http : / / www . icra . org / sitelabel / “Pattern Integrity Rule” . To avoid confusion , we use the term “ removable ” to describe the result of our estimation . Words estimated as removable will be deleted from sentence at the end of ﬁltering . Compared with existing automatic ﬁltering approaches , such as keyword censoring approach applied on YouTube website and content control approach applied in Microsoft Parental Controls , the proposed semantic ﬁltering approach is able to remove oﬀensive content in a text message thor - oughly while keeping inoﬀensive content untouched as much as possible . Further , the readability of ﬁltered content is guaranteed so as to make our ﬁltering transparent to reader . Compared with manual ﬁltering , which outputs optimal ﬁl - tering results , the proposed semantic ﬁltering approach is fully automatic with close ﬁltering results . To demonstrate the performance , we have created a dataset containing 11670 user comments collected from YouTube website . For each user comment , we perform both man - ual ﬁltering and semantic ﬁltering and then compare their outputs . According to experimental results , the semantic ﬁltering achieves as high as 90 . 94 % agreement with man - ual ﬁltering . Meanwhile , the processing speed of semantic ﬁltering is about 48 . 8 𝑚𝑠𝑒𝑐 per comment , making it prac - tical to be deployed at the server side of online websites . Furthermore , for user side application , we present an imple - mentation of semantic ﬁlter as a Firefox extension , which is able to ﬁlter the user comments on webpages when a user is browsing on OSN websites . We show the advantage of our proposed ﬁltering extension by comparing it with the “ Hide objectionable words ” function provided by YouTube . 2 . RELATED WORK In this section , we review related work and list some major approaches and methodologies for oﬀensive language ﬁlter - ing in online communities . To demonstrate our observations , we present examples of applying diﬀerent approaches in Ta - ble 1 . Sentences in the table are cited from real user com - ments in YouTube dataset . For mental health of readers , we replaced the oﬀensive words by special words . 2 . 1 Keyword Censoring Approach Keyword censoring approaches match words appearing in text messages with oﬀensive words stored in the blacklist . Once found , these oﬀensive words will be removed , partially replaced ( e . g . , “a * * * ” ) , completely replaced ( e . g . , “ * * * * ” ) , or substituted by family friendly words ( e . g . , “nice” ) . Be - cause of its simplicity , keyword based censoring approach has been widely applied in OSN websites , such as YouTube 3 and World of Warcraft 4 . However , the ﬁltering result is not as desired . Brutally removing words from text message breaks the readability of text messages . Replacing oﬀensive words with symbols usually makes it easy to guess the original oﬀensive words . The idea of substitution seems tempting , but accurate sub - stitution is usually impractical . Inaccurate substitution will introduce additional issues . For example , in 2001 , Yahoo ! deployed an Email ﬁlter which may automatically alter cer - tain words in emails by family friendly words . This ﬁlter was criticized as a “foolish ﬁlter” by BBC news 5 because of 3 YouTube , at http : / / www . youtube . com 4 World of Warcraft , at www . worldofwarcraft . com / 5 http : / / news . bbc . co . uk / 2 / hi / sci / tech / 2138014 . stm its inaccurate substitution . To demonstrate the shortcoming of keyword censoring ap - proaches , we present examples in Table 1 . According to pre - sented ﬁltering results , readers can still easily understand what the oﬀender wants to say and even be able to infer the removed words . This indicates the failure ﬁltering because oﬀensive opinion has been successfully delivered to victims . Also , removing words from a sentence without considering their context breaks the readability of rest of the sentence . Compared with keyword censoring approaches , our pro - posed semantic ﬁltering approach is much more sophisti - cated and can achieve thorough ﬁltering eﬀort by utiliz - ing the grammatical relations among words in the sentence . Given a sentence containing both oﬀensive and inoﬀensive words , not only oﬀensive words but also inoﬀensive words assisting to express oﬀensive opinions will be removed dur - ing our ﬁltering . In this way , we essentially stop the delivery of oﬀensive opinion . And , there will be no way to infer the oﬀensive content in original messages after ﬁltering . 2 . 2 Content Control Approach Content control approaches are usually deployed at user side or ISP side to prevent user from seeing inappropriate content on the Internet . Its ﬁltering is usually done based on certain criteria , such as URL address , the occurrence of oﬀensive words , and topic classiﬁcation . Here our focus is text based criteria . For example , in Table 1 , we present a sentence based content control approach with threshold set as the number of oﬀensive words in the sentence . If at least two oﬀensive words are detected within a sentence , the ﬁlter will remove the sentence from user message . However , content control approaches are too coarse - grained to be applied in online communities . First of all , oﬀender can easily bypass the ﬁltering as long as knowing the estima - tion criteria . More important , a sentence in user comment may contain both oﬀensive and inoﬀensive content . Inoﬀen - sive part may be removed falsely because of oﬀensive part , e . g . , the partial oﬀensive case shown in Table 1 . Not allow - ing user to post inoﬀensive content would easily drive users away and thus aﬀect the growth of community . Compared with content control approaches , we provide a ﬁne - grained ﬁltering by removing only the smallest syntactic part in the sentence containing oﬀensive language . The inof - fensive content in the original message will remain ; thereby , user still has the freedom of speech for posting inoﬀensive content . We believe such delicate ﬁltering will be more ac - ceptable to online communities . 2 . 3 Manual Filtering Approach Manual ﬁltering is believed to produce the best ﬁltering result . Basically , user messages are reviewed by community administrator before being posted on the website . As shown in Table 1 , the administrator is able to easily understand what the author wants to express and precisely remove only the oﬀensive content within the text . However , manual ﬁltering is very time and labor consum - ing , making it impossible to be widely applied . For example , in the Sina blog community 6 , the blog administrator will manually review and ﬁlter user comments on some celebri - ties’ public blogs . Obviously , users would expect a delay between posting a comment on a blog and displaying this 6 Sina Blog Community , at http : / / blog . sina . com . cn Table 1 : Filtering results with diﬀerent approaches Sentence ( we use 𝑐𝑟𝑦𝑖𝑛𝑔 and 𝑝𝑖𝑔 to denote two oﬀensive words ) Partial Oﬀensive Absolute Oﬀensive Original Comment “this video is 𝑐𝑟𝑦𝑖𝑛𝑔 good” “it is aston martin and you are a 𝑐𝑟𝑦𝑖𝑛𝑔 𝑝𝑖𝑔 ” “you’re a 𝑝𝑖𝑔 ” Keyword Censoring “this video is 𝑐 ∗ ∗∗ good” “it’s aston martin and you are a 𝑐 ∗ ∗∗ 𝑝 ∗ ∗ ” “you’re a 𝑝 ∗ ∗∗ ” Content Control ( thld = 2 ) “this video is 𝑐𝑟𝑦𝑖𝑛𝑔 good” “ ” “you’re a 𝑝𝑖𝑔 ” Manual Filtering “this video is good” “it’s aston martin” “ ” comment on the blog’s webpage . Further , the ﬁltering to - tally relies on the judgment of the community administrator . Our proposed semantic ﬁltering approach mimics the pro - cedure of manual ﬁltering by trying to understand the re - lations among words in order to remove the oﬀensive con - tent semantically . In our experiments , we show that the results between our proposed approach and manual ﬁlter - ing are very close ( more than 90 % agreement ) . Moreover , the proposed semantic ﬁltering approach is fully automatic requiring no interference of administrator . 3 . PROPOSED FILTERING PHILOSOPHY The goal of our semantic ﬁltering is to achieve ﬁltering results close to that of manual ﬁltering . To reach this goal , the foremost thing is to answer the question about how the ﬁltering should be performed in order to get the desired ﬁltering results . In this section , we present our answer in three steps . First , we analyze the characteristics of oﬀensive text content in user messages . Then , we introduce our ﬁltering philosophy according to the summarized characteristics . Finally , we show how this philosophy is transformed into heuristic rules applicable in the ﬁltering process . 3 . 1 Offensive Language Text Content Based on the observation on user comments collected from YouTube website , a sentence in a user message may contain both oﬀensive and inoﬀensive text content . Oﬀensive text content is exposed intentionally with purpose of bringing negative inﬂuence to victims ( e . g . , the readers of text mes - sage ) . The victim receives the negative inﬂuence by reading the oﬀensive part of sentence and understanding the carried oﬀensive information . Hence , the information carried by original sentence can be represented as 𝐼 = 𝐼 𝑜𝑓𝑓 + 𝐼 𝑖𝑛𝑜𝑓𝑓 . The oﬀender reaches his goal when the oﬀensive information 𝐼 𝑜𝑓𝑓 is delivered to readers . Therefore , to achieve a thorough ﬁltering , all words used to deliver 𝐼 𝑜𝑓𝑓 should be removed . Meanwhile , with respect to free speech , the part with 𝐼 𝑖𝑛𝑜𝑓𝑓 should be saved . 3 . 2 Filtering Philosophy According to the analysis , we propose the philosophy that should be followed in sentence - level oﬀensive language ﬁlter - ing : ∙ Precisely identify all oﬀensive contents and remove them semantically , so that viewers will not notice the existence of oﬀensive language in the original sen - tence ; ∙ Keep the readability and inoﬀensive content in the sentence , so that the author will still be allowed to express his opinion freely as long as it is not oﬀensive ; We call this the philosophy of “ ﬁltering instead of block - ing ” . To the ﬁlter , the philosophy states that : if removing one word will make another word meaningless or confusing to readers , we should consider removing both words to keep the readability of a ﬁltered sentence ; meanwhile , we only remove words that are aﬀected by oﬀensive words . For example , in the sentence “it is aston martin and you are a crying pig” , suppose “crying” and “pig” are two of - fensive words , the sentence can be separated into two parts . The ﬁrst part , “it is aston martin” , is inoﬀensive ; but the second part , “you are a crying pig” is oﬀensive . Therefore , we should remove the second part completely while keep - ing the ﬁrst part . The word “and” should also be removed in order to keep the transparency of ﬁltering as well as the readability of ﬁltered text content . 3 . 3 Filtering Rules Speciﬁcally , the proposed philosophy is transformed into two heuristic rules to estimate the impact of removing words in a sentence . Rule 1 . ( Modiﬁcation Relation ) in a modiﬁcation rela - tion , if the modiﬁer is determined to be oﬀensive , removing modiﬁer solely is enough ; if the head is determined to be of - fensive , both the head and the modiﬁer should be removed . The modiﬁcation relation is a binary semantic relation - ship between two syntactic elements , such as word , phrase , etc . One element is named head and the other is named modiﬁer . The modiﬁer is used to describe the head ( i . e . the modiﬁed component ) . Semantically , modiﬁers describe and provide more accurate deﬁnitional meaning for head . As the modiﬁer acts as a complement , the removal of the modiﬁer typically will not aﬀect the grammaticality of the construc - tion . For example , in the sentence “she likes red apples . ” , the adjective “red” is used to modify the noun “apples” . Re - moving “red” will keep the readability of rest of sentence . We admit that , removing modiﬁers will lose some informa - tion carried by modiﬁers . However , if the modiﬁer is deter - mined removable but the head is not , removing modiﬁer will remove only the oﬀensive information . Rule 2 . ( Pattern Integrity ) if removing the oﬀensive word breaks the integrity of sentence’s basic pattern , the whole sentence should be removed in order to keep the readability . English sentences and clauses are organized in basic pat - terns , such as “Subject - Verb” , “Subject - Verb - Object” , “Subject - Verb - Adjective” , “Subject - Verb - Adverb” , and “Subject - Verb - Noun” . Every sentence or clause can be categorized into one pattern . The integrity of basic pattern is essential to the readability of content . For example , the sentence “she sleeps on the sofa . ” follows “Subject - Verb” pattern . If we only remove “sleeps” , the rest of the sentence , “she on the sofa . ” will become nonsense . In the next section we will show details about applying these two rules during the ﬁltering . 4 . IDENTIFY REMOVABLE CONTENT BY GRAMMATICAL RELATIONS A text message can be decomposed into a sequence of sentences . Each sentence is considered as a unit in ﬁlter - ing . Given a sentence containing both oﬀensive words and inoﬀensive words , the goal of ﬁltering is to identify inoﬀen - sive words which should be removed together with oﬀensive words . We deﬁne the words that should be removed by the ﬁltering as “ removable ” words . We noticed that manual ﬁltering can easily achieve this goal because human can easily understand the context of words in a sentence and precisely identify which words should be removed with known oﬀensive words . So , we mimic the manual ﬁltering in that , we extract the grammatical rela - tions among words from sentences and use the proposed ﬁl - tering rules to estimate the impact of removing oﬀensive words on other inoﬀensive words based on extracted gram - matical relations . Speciﬁcally , the proposed approach includes two steps ( de - tails to be elaborated in the next two subsections ) . In the ﬁrst step , we scan the sentence and see if oﬀensive words ex - ist . If exist , we continue to retrieve grammatical information ( i . e . Part - of - Speech tags and typed dependency relations ) among words in the sentence . Using retrieved grammatical information , we create a tree data structure , named RelTree , for the second step estimation . In this second step , we pro - pose a set of estimation functions following the ﬁltering rules we proposed . Using the RelTree structure and the proposed rules , we then estimate if there are inoﬀensive words that should be removed together with those identiﬁed oﬀensive words . The overview idea of our semantic ﬁltering approach is shown in Algorithm 1 . Within the algorithm , the functions 𝑃𝑂𝑆𝑡𝑎𝑔𝑔𝑖𝑛𝑔 and 𝑇𝐷𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑜𝑟 generate Part - of - Speech tags and typed dependency relations , respectively . We use exist - ing NLP ( Natural Language Processing ) tools [ 2 ] to imple - ment these two functions . In the rest of this section , we will focus on the design of two other functions 𝐶𝑟𝑒𝑎𝑡𝑒𝑅𝑒𝑙𝑇𝑟𝑒𝑒 and 𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒𝑅𝑒𝑙𝑇𝑟𝑒𝑒 . Note that , in this section , we assume that the ﬁltering is based on a comprehensive oﬀensive lexicon containing all oﬀensive words . Words do not appear in the lexicon are considered inoﬀensive . We discuss the case of incomplete lexicon separately in Section 6 . 4 . 1 First Step : Grammatical Analysis In the ﬁrst step , we extract two types of grammatical in - formation from a given sentence . One is the Part - of - Speech information associated with every word . The other is the dependency relation among words . Part - of - Speech informa - tion helps us to understand the organization of a sentence , which is essential for keeping the readability when we try to remove words from a sentence . Dependency relations will be used directly to estimate the impact of removing one word on other semantically related words , making the ﬁltering more “meaningful” . Combining these two types of informa - tion , we can create a new data structure , called RelTree , for input : a text comment 𝑇 , a blacklist of oﬀensive words 𝐵𝑙𝑎𝑐𝑘𝑙𝑖𝑠𝑡 output : a ﬁltered text comment 𝑇 ′ 𝑇 ′ ← “” ; 1 𝑠𝑒𝑛𝐿𝑖𝑠𝑡 ← chunk 𝑇 into a list of sentences ; 2 foreach sentence 𝑠 ∈ 𝑠𝑒𝑛𝐿𝑖𝑠𝑡 do 3 scan 𝑠 for oﬀensive words using 𝐵𝑙𝑎𝑐𝑘𝑙𝑖𝑠𝑡 ; 4 if no oﬀensive word found then 5 𝑇 ′ ← 𝑇 ′ + 𝑠 ; 6 end 7 else 8 𝑃𝑇𝑟𝑒𝑒 ← 𝑃𝑂𝑆𝑡𝑎𝑔𝑔𝑖𝑛𝑔 ( 𝑠 ) ; / * get parse tree * / 9 𝑇𝐷𝑠𝑒𝑡 ← 𝑇𝐷𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑜𝑟 ( 𝑠 ) ; / * get typed 10 dependency relations * / 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ← 𝐶𝑟𝑒𝑎𝑡𝑒𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ( 𝑃𝑇𝑟𝑒𝑒 , 𝑇𝐷𝑠𝑒𝑡 ) ; 11 / * create RelTree * / 𝐿𝑎𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ← 12 𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ( 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 , 𝐵𝑙𝑎𝑐𝑘𝑙𝑖𝑠𝑡 ) ; / * estimate using RelTree * / 𝑠 ′ ← remove all words in 𝐿𝑎𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 those 13 are labeled as “removable” ; 𝑇 ′ ← 𝑇 ′ + 𝑠 ′ ; 14 end 15 end 16 Return 𝑇 ′ ; 17 Algorithm 1 : Procedure of Semantic Filtering Figure 1 : A parse tree of a sentence basing on Part - of - Speech tags the next - step estimation . 4 . 1 . 1 Part - of - Speech Tagging Part - of - Speech tagging has been widely used in NLP ap - plications to identify the syntactic properties of lexical items in a sentence , such as word or phrase . Through Part - of - Speech tagging , the sentence can be represented in a tree structure basing on Part - of - Speech tags . We adopt the Penn Treebank tag set [ 4 ] for our Part - of - Speech tagging . An example of Penn Treebank style parse tree is shown in Figure 1 . Here , the leaf nodes are words appearing in the sentence . The non - leaf nodes represent syntactic ele - ments , such as phrases or clauses . Each element consists of the words within its subtree . For example , in Figure 1 , the words “is” , “aston” , and “martin” constitute a Verb Phrase ( i . e . VP ) node . To avoid showing oﬀensive language in this paper , we use two terms , “crying” and “pig” , to replace original oﬀensive words in this example . Figure 2 : An example of typed dependency graph 4 . 1 . 2 Typed Dependency Relations Typed Dependency is a kind of general relations describ - ing the grammatical dependencies within a sentence , pro - posed by Stanford Natural Language Processing Group [ 6 ] . According to [ 6 ] , each typed dependency includes a depen - dency type and a ( 𝑔𝑜𝑣𝑒𝑟𝑛𝑜𝑟 , 𝑑𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡 ) word pair . For ex - ample , in the sentence “you are a crying pig . ” , the typed dependency amod ( pig , crying ) means that “crying” is an adjectival modiﬁer of an noun phrase containing “pig” . A typed dependency may represent the dependent relations between two syntactic elements , not limited to words only . The typed dependencies in a sentence can be represented as a graph . For example , Figure 2 shows the typed depen - dency relations for the same sentence shown in Figure 1 . We explain the relations appeared in Figure 2 from left to right : the nominal subject relation , nsubj ( martin , it ) , means that “it” is the syntactic subject of the clause ( same is nsubj ( pig , you ) ) ; the copula relation , cop ( martin , is ) , means that “martin” is the complement of verb “is” ( same is cop ( pig , are ) ) ; the noun compound modiﬁer , nn ( martin , aston ) , means that the noun “aston” serves to modify the head noun “mar - tin” ; the determiner , det ( pig , a ) , means that “the” is a de - terminer of “pig” ; the adjectival modiﬁer , amod ( pig , crying ) , means that “crying” serves as adjectival modiﬁer of “pig” ; and the conjunct , conj and ( martin , pig ) , means that the coordinating conjunction “and” is used to connect two ele - ments with head “martin” and “pig” , respectively . 4 . 1 . 3 Relation Tree ( RelTree ) Both Part - of - Speech and typed dependency relations are utilized in the second step estimation . The parse tree shows the sentence syntactic organization and typed dependency relations provide semantic information among words . To combine both information , we propose a new data structure called RelTree . In a RelTree , as shown in Figure 3 , the leaf nodes are words in the sentence . And the non - leaf node represents either a phrase or a clause inside the sentence . In each non - leaf node , we associate the set of typed dependency relations on the words within its subtree . Each node only contains the typed dependency relations which have not appeared in its subtree nodes . The RelTree data structure is proposed only for the con - venience of oﬀensiveness estimation in the next step . Algo - rithm 2 shows the algorithm for RelTree construction . With the parse tree 𝑃𝑇𝑟𝑒𝑒 given , the computational complexity of algorithm 𝐶𝑟𝑒𝑎𝑡𝑒𝑅𝑒𝑙𝑇𝑟𝑒𝑒 relies on the post - order traversal and the search in TDset . As the number of relations never exceeds 𝑁 ( 𝑁 − 1 ) / 2 , where 𝑁 is the number of words in the sentence , the computational complexity is 𝑂 ( 𝑁 3 ) . The com - putational complexity itself is acceptable . Indeed , there are a lot of ways to improve the eﬃciency in the implementation of this algorithm . 4 . 2 Step Two : Bottom - up Estimation Figure 3 : A RelTree combining the parse tree and typed dependency relations input : a parse tree 𝑃𝑇𝑟𝑒𝑒 , a set of typed dependency relations 𝑇𝐷𝑠𝑒𝑡 output : a RelTree 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ← 𝑃𝑇𝑟𝑒𝑒 ; 1 Remove all word nodes in 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ; 2 Traverse 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 in 𝑝𝑜𝑠𝑡𝑜𝑟𝑑𝑒𝑟 foreach node 𝑛 3 visited do if 𝑛 is a leaf node then 4 n . wordset ← { n } ; / * create word nodes * / 5 end 6 if 𝑛 is not a leaf node then 7 n . wordset ← ∅ ; 8 foreach direct child node 𝑐 𝑖 do 9 𝑛 . 𝑤𝑜𝑟𝑑𝑠𝑒𝑡 ← 𝑛 . 𝑤𝑜𝑟𝑑𝑠𝑒𝑡 ∪ 𝑐 𝑖 . 𝑤𝑜𝑟𝑑𝑠𝑒𝑡 ; 10 𝑛 . 𝑟𝑒𝑙 ← ∅ ; 11 foreach relation 𝑇 𝑖 ( 𝐺 𝑖 , 𝐷 𝑖 ) in 𝑇𝐷𝑠𝑒𝑡 do 12 if 𝐺 𝑖 ∈ 𝑛 . 𝑤𝑜𝑟𝑑𝑠𝑒𝑡 and 13 𝐷 𝑖 ∈ 𝑛 . 𝑤𝑜𝑟𝑑𝑠𝑒𝑡 then 𝑛 . 𝑟𝑒𝑙 ← 𝑛 . 𝑟𝑒𝑙 ∪ 𝑇 𝑖 ( 𝐺 𝑖 , 𝐷 𝑖 ) ; 14 𝑇𝐷𝑠𝑒𝑡 ← 𝑇𝐷𝑠𝑒𝑡 − 𝑇 𝑖 ( 𝐺 𝑖 , 𝐷 𝑖 ) ; 15 end 16 end 17 end 18 end 19 end 20 Return 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ; 21 Algorithm 2 : create a RelTree using the parse tree and typed dependency relations In the second step , we ﬁrst use the oﬀensive lexicon to identify oﬀensive words in the sentence . The leaf node with an oﬀensive word will be labeled as “removable” . Starting from leaf nodes , we perform bottom - up estimation through a postorder traversal on the RelTree . For each non - leaf node in the RelTree , we estimate whether it should be removed based on ( 1 ) the associated typed de - pendency relations and ( 2 ) its child nodes within its subtree . If a non - leaf node is estimated to be “removable” , all its de - scendants , including words , within its subtree will also be labeled as “removable” . The meaning of “removable” to a non - leaf node is that all words , phrases , or even clauses within its subtree have been determined to be removed at the end of ﬁltering . The estimation process includes two steps . We ﬁrst estimate based on typed dependency rela - tions , and then apply a set of heuristic rules as complements . 4 . 2 . 1 Estimation with Typed Dependency Relations Consider a non - leaf node 𝑛 in a RelTree with a set 𝑛 . 𝑟𝑒𝑙 of typed dependency relations . Each relation describes a se - mantic connection between a 𝑔𝑜𝑣𝑒𝑟𝑛𝑜𝑟 word and a 𝑑𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡 word . Both words are leaf nodes in the subtree rooted at 𝑛 . 𝑛 . 𝑟𝑒𝑙 could be empty when 𝑛 only has one child node . For each typed dependency relation in 𝑛 . 𝑟𝑒𝑙 , we study its semantic information and map it to an estimation function we deﬁned . There are totally 55 typed dependency relations deﬁned in [ 6 ] , and we map each of them to one estimation function . We show all the mappings in Table 2 . These estimation functions and mapping are created fol - lowing the Modiﬁcation Relation and Pattern Integrity rules . Take the Direct Object ( dobj ) relation for example . In [ 6 ] , the dobj ( G , D ) relation is deﬁned as : the direct object of a verb phrase , containing governor word 𝐺 , is the noun phrase , containing dependent word 𝐷 . For example , in a relation 𝑑𝑜𝑏𝑗 ( 𝑤𝑖𝑛 , 𝑚𝑎𝑡𝑐ℎ ) , “win” is the governor word and “match” is the dependent word . According to Pat - tern Integrity rule , we know that “Subject - Verb - Object” is a basic pattern . Therefore , if either the phrase with 𝐺 or phrase with 𝐷 will be removed because of oﬀensiveness , both phrases should be removed together . To formalize , we deﬁne an estimation function H ( T ) = H ( P ( G ) ) OR H ( P ( D ) ) and map relation 𝑑𝑜𝑏𝑗 ( 𝐺 , 𝐷 ) to it . We use symbol 𝐶 ( 𝐺 ) and 𝑃 ( 𝐺 ) to denote the clause and phrase containing word G as head , respectively . In this esti - mation function , 𝐻 ( 𝑇 ) is the label to be assigned to relation 𝑇 and 𝐻 ( 𝑃 ( 𝐺 ) ) is the label with phrase node containing 𝐺 in the RelTree . For example , in Figure 3 , 𝑃 ( 𝑎𝑠𝑡𝑜𝑛 ) is the 𝑁𝑃 node of “aston martin” , and 𝐶 ( 𝑎𝑠𝑡𝑜𝑛 ) is the clause ( i . e . 𝑆 ) node of “it is aston martin . ” . The relation 𝑑𝑜𝑏𝑗 will be labeled as “removable” if at least one of these two phrases are labeled as “removable” . Using the estimation function , we generate a label for ev - ery relation associated with node 𝑛 and then for the node itself . If a relation 𝑇 ( 𝐺 , 𝐷 ) of node 𝑛 is estimated and la - beled as “removable” , the two child nodes of 𝑛 , containing word 𝐺 and word 𝐷 , will be labeled as “removable” . If all relations in 𝑛 . 𝑟𝑒𝑙 are labeled as “removable” , the node 𝑛 as well as all its descendants , will be labeled as “removable” . 4 . 2 . 2 Estimation with Heuristic Rules Through experiments on YouTube dataset , we realize that ﬁltering with typed dependency relations may not be enough . Heuristic rules must be applied as complement after typed dependency relation estimation . Applying heuristic rules is necessary mainly because of two reasons . First of all , the typed dependency relation contains some syntactic informa - tion but limited . For example , the possessive ending ( i . e . 𝑃𝑂𝑆 ) tag , which is a quite popular Part - of - Speech tag , is ignored during the typed dependency tagging . Secondly , not all relations between syntactic elements in a sentence can be classiﬁed into one of typed dependency relations deﬁned in [ 6 ] . For those uncertain relations , [ 6 ] de - ﬁnes a generic grammatical relation , named 𝑑𝑒𝑝 . To prevent confusion to ﬁlter , in the Table 2 , we include 𝑑𝑒𝑝 into the Rule H ( T ) = H ( G ) AND H ( D ) which means either 𝐺 or 𝐷 is labeled removable will not aﬀect each other and the label of 𝑇 . Because 𝑑𝑒𝑝 relation stands for uncertain relation , we have to rely on Part - of - Speech tags in the RelTree for our ﬁltering . We present several examples of heuristic rules applied in Figure 4 : Estimate a RelTree in a bottom - up man - ner semantic ﬁltering in Table 3 . Take the 𝑐𝑜𝑛𝑗 tag node rule as an example . The conjunct relation ( 𝑐𝑜𝑛𝑗 ) is a type of relation between two syntactic elements connected by a co - ordinating conjunction , such as “and” . The parameters of 𝑐𝑜𝑛𝑗 do not include the coordinating conjunction . However , explicitly , the coordinating conjunction sits between the two parameters of 𝑐𝑜𝑛𝑗 . If one side is determined removable , the coordinating conjunction should be removed as well . For ex - ample , in the sentence “I like A and B” , if either 𝐴 or 𝐵 is removed , the coordinating conjunction “ 𝑎𝑛𝑑 ” should also be removed . 4 . 2 . 3 Estimation Algorithm To estimate and assign labels for all nodes in a RelTree , we perform the estimation also in a bottom - up manner . Fig - ure 4 shows an example estimation process . The number in the circle represents the order of estimation for each node in the RelTree . The dashed nodes are estimated as “remov - able” . For example , the clause node with nsubj ( pig , you ) is estimated as “removable” according to the estimation . Therefore , its two child nodes containing “pig” and “you” respectively are both labeled as “removable” . Moreover , the word “and” is removable according to the heuristic rule ( i . e . conj tag node rule ) , in order to keep the ﬁltering transparent to readers . Finally , inoﬀensive words , “you” , “are” , and “a” , are removed with two oﬀensive words , “crying” and “pig” , in the ﬁltering . According to Algorithm 2 , each typed dependency rela - tion will appear exactly once in the RelTree . No relation will be checked repeatedly in the estimation . The cleaned sentence after ﬁltering in this example will be “it is aston martin . ” . As we can see , the result satisﬁes the requirement of our proposed ﬁltering philosophy . Only the oﬀensive part , “you are a crying pig” , is removed . The reader can still get the inoﬀensive information . The detailed algorithm for esti - mation process is presented in Algorithm 3 . 5 . APPLICATIONS The proposed semantic ﬁltering approach can be applied in real - world applications . In this section , we present two types of applications for demonstration . One application is at administrator side where the semantic ﬁltering approach can help administrators to automatically remove oﬀensive language in text messages submitted by users . The other application is at browser side where we implement the se - mantic ﬁlter as a Firefox extension for parental control . Table 2 : The mapping between typed dependency relations and estimation functions . ( please see Section 4 . 2 . 1 for the deﬁnition of notations ) Index Estimation Function Typed Dependency Relation 1 H ( T ) = H ( P ( G ) ) cop , expl , measure , partmod , poss , possessive , preconj , prep / prepc , purpcl , quantmod , rcmod , ref , tmod ; 2 H ( T ) = H ( P ( D ) ) pcomp , pobj , predet ; 3 H ( T ) = H ( C ( G ) ) complm , mark , rel ; 4 H ( T ) = H ( P ( G ) ) OR H ( C ( D ) ) xcomp ; 5 H ( T ) = H ( C ( G ) ) OR H ( P ( D ) ) xsubj ; 6 H ( T ) = H ( G ) OR H ( P ( D ) ) nsubj , nsubjpass ; 7 H ( T ) = H ( G ) AND H ( D ) conj , nn , number , dep ; 8 H ( T ) = H ( G ) aomp , advcl , advmod , agent , amod , appos , attr , aux , auxpass , cc , ccomp , det , neg , num , parataxis , punct ; 9 H ( T ) = H ( G ) OR H ( C ( D ) ) csubj , csubjpass ; 10 H ( T ) = H ( P ( G ) ) OR H ( P ( D ) ) abbrev , dobj , infmod , iobj , prt ; Table 3 : Examples of heuristic rules applied in the proposed semantic ﬁltering Index Name Rules 1 POS tag node rule IF { its previous noun is removable } THEN { POS tag node will be removable } 2 conj tag node rule IF { both neighbor nodes are removable } THEN { conjunction node will be removable } 3 ADVP - VP rule IF { the current non - leaf node has only two child nodes ; one is ADVP node and the other is VP node } THEN { IF { ADVP node is removable } THEN { VP node will not be aﬀected } ; IF { VP node is removable } THEN { ADVP node will also be removable } ; } input : a RelTree RelTree , a blacklist of oﬀensive words 𝐵𝑙𝑎𝑐𝑘𝑙𝑖𝑠𝑡 , output : a labeled RelTree LebelRelTree 𝐿𝑒𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ← 𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ; 1 Label all leaf nodes with oﬀensive words by 2 “removable” in 𝐿𝑎𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ; Traverse 𝐿𝑎𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 in 𝑝𝑜𝑠𝑡𝑜𝑟𝑑𝑒𝑟 foreach node 3 𝑛 visited do if 𝑛 is a leaf node then 4 ignore ; / * already labeled * / 5 end 6 if 𝑛 is not a leaf node then 7 if 𝑛 only has one child node then 8 𝑛 . 𝑙𝑎𝑏𝑒𝑙 ← 𝑛 . 𝑐ℎ𝑖𝑙𝑑 . 𝑙𝑎𝑏𝑒𝑙 ; 9 end 10 if 𝑛 has more than one child node then 11 Estimate the label for 𝑛 by its associated 12 labels , using proposed estimation function and heuristic rules ; end 13 end 14 end 15 Return 𝐿𝑎𝑏𝑒𝑙𝑅𝑒𝑙𝑇𝑟𝑒𝑒 ; 16 Algorithm 3 : estimate nodes in RelTree 5 . 1 Semantic Filter for Administrators in On - line Communities When a user wants to post a text message , he has to ﬁrst send the message to the server and let the server post this message so that other users can see it . Therefore , one im - portant place to eliminate oﬀensive language in online com - munities is at the server of online communities . Many online communities have deployed sensor tools , such as swear ﬁl - ter in the World of Warcraft , to detect and remove sensitive words , including oﬀensive words . However , as we shown in Section 2 , ﬁltering without considering semantics of text message turns out to be ineﬀective to ﬁght against oﬀensive language . To a server side ﬁltering tool , three metrics are most im - portant to measure its performance : eﬀectiveness , accuracy , and speed . For eﬀectiveness , we already show the advantage of applying semantic ﬁltering in Section 2 . For accuracy and speed , we have implemented an oﬀensive language ﬁl - ter in Java using the proposed semantic ﬁltering approach . Stanford parser [ 2 ] is adapted to perform the Part - of - Speech tagging and generate the typed dependency relations . In the following subsections , we ﬁrst present the details about our collected YouTube dataset , and then present the results of our experiments . 5 . 1 . 1 YouTube Dataset YouTube is a leading video sharing website in the world . Videos on YouTube websites are classiﬁed into 15 categories , such as Comedy , Entertainment , and Music . For each video webpage , contents contributed by users include the video for viewing , video title , video author ID , video description , and a list of text comments . Each text comment is associated with a user ID and a piece of text content . User ID identiﬁes the author who generates this comment , and text content contains the body of user’s opinion . Our ﬁltering focuses on the text content of text comments . To build the dataset , we collected text comments from video webpages on YouTube website in the days between September 27 and September 29 , 2009 . For each of 15 cate - gories , we collected the top 20 “most discussed” video web - pages ranked in “this week” . For each webpage , we collected the ﬁrst 40 text comments . The dataset contains 11670 text comments in total . For each text comment , we did manual ﬁltering for each sentence in the comment . Specif - ically , the manual ﬁltering process includes three steps . In the ﬁrst step , we read the comment and chunk it into sen - tences . Then , we identiﬁed the oﬀensive words appearing in the sentence . Finally , for each oﬀensive word , we marked the least set of words that should be removed together with it in order to eliminate the oﬀensive information . We as - sign the collected YouTube dataset to ﬁve students for sep - arate manual ﬁltering . According to the results , 1739 text comments contain oﬀensive words . Within these comments , 2063 sentences contain oﬀensive words and the total number of unique oﬀensive words appeared is 368 . During the manual ﬁltering , one key question is how to identify oﬀensive words . The same as in [ 3 ] , the knowl - edge about oﬀensive language in our case is represented by a lexicon of oﬀensive words . All words in this lexicon have been determined to be oﬀensive and should be prevented from being seen by readers . To estimate a word , our judg - ment is based on a list of oﬀensive words provided by [ 1 ] and the word’s meaning listed in the Urban Dictionary website 7 . All oﬀensive words we detected are determined as oﬀensive words by these two . 5 . 1 . 2 Accuracy The results of both semantic ﬁltering and manual ﬁltering on a sentence will be in one of three types . ∙ “Clean” : if there is no oﬀensive word in the sentence ; ∙ “Semantic Removing” : there exist oﬀensive words in the sentence and some inoﬀensive words have to be removed together with those oﬀensive words ; ∙ “Keyword Removing only” : there exist oﬀensive words in the sentence and removing those oﬀensive words would be enough for the ﬁltering . To compare and measure the accuracy , we apply our pro - posed semantic ﬁltering approach to process all comments collected in the YouTube dataset , and compare the result with the manual ﬁltering result on the dataset . We assume that the manual ﬁltering result represents the optimal ﬁlter - ing result we want to achieve . To understand the diﬀerence between the ﬁltering result by human and by our ﬁlter , we manually compare the results of both ﬁltering , sentence by sentence . The comparison outputs three types of results : ∙ if words removed by semantic ﬁltering are exactly the same as those chosen by manual ﬁltering , we call it “ Correct Filtering ” ; ∙ if more words are removed by semantic ﬁltering than manual ﬁltering , we call it “ Excessive Filtering ” ; ∙ if fewer words are removed by semantic ﬁltering than manual ﬁltering , we call it “ Insuﬃcient Filtering ” ; Excessive ﬁltering means that there are unnecessary words , carrying inoﬀensive information , being falsely removed . In - suﬃcient ﬁltering means that ﬁltering is not thorough . According to our comparison , with 2063 sentences con - taining oﬀensive words , the number of insuﬃcient ﬁltering is 58 ( i . e . 2 . 81 % ) , and the number of excessive ﬁltering is 129 ( i . e . 6 . 25 % ) . To sum up , the overall ratios of accuracy of our implemented semantic ﬁlter is 90 . 94 % . Through reviewing the results , three reasons are responsi - ble for the inaccuracy , which are Informal English Writing , Incorrect Part - of - Speech tagging , and Incorrect Typed De - pendency Analysis . 7 Urban Dictionary , at http : / / www . urbandictionary . com / Informal English Writing is a character of language in the online community . People would like to write text comments on a casual or conversational tone , even with a lot of spelling errors . Luckily , our goal is not to understand the meaning of sentences but the relations among words . In the experiments , we apply the unlexicalized PCFG parser [ 7 ] proposed by Stanford NLP group and our results show some resistance to certain informal writings . For example , for word misuse case , in the Part - of - Speech tags generated from sentence “I like to apple you . ” , “apple” will be correctly tagged with as a verb . Another example , for misspelling case , the sentence of “I like to eat apple . ” and “I like to ea app . ” outputs the same Part - of - Speech tagging during our experiments . This is an important reason why we can still achieve such a high overall accuracy mentioned above . However , piling up several sentences casually without us - ing any delimiters , such as “ . ” , “ ! ” , “ ? ” , will make it diﬃcult for the parser to generate correct Part - of - Speech tags and typed dependency relations . Moreover , misspelling of cer - tain sensitive words will confuse the parser ( even human ) . For example , in some text comment , “your” is used to stand for “you’re” . There is no way for us to tell that . Incorrect Part - of - Speech tagging contributes to 69 . 5 % of incorrect ﬁltering results . The major reason of incorrect tagging is caused by false sentence segmentation . In our experiments , we use the default sentence splitter provided by Stanford parser . Because typed dependency relations are generated based on the result of Part - of - Speech tagging , the incorrect tagging will certainly lead to incorrect ﬁlter - ing result . For example , many incorrect tagging cases are caused by arbitrarily inserting phrases in the pattern of “you ABCD ” in a sentence . “ ABCD ” usually is a noun phrase or adjective phrase . Because “you” is a sensitive words in tagging , this can easily confuse the Stanford Parser . Typed dependency analysis could be incorrect , even based on correct Part - of - Speech tagging . Especially , when the relation is uncertain , Stanford parser tends to assign a “dep” relation which basically means every thing could be possible . In some cases , we can apply heuristic rules mentioned in Section 4 . 2 . 2 as complement . 5 . 1 . 3 Speed The processing speed on masses of text messages is im - portant for ﬁltering oﬀensive language in real online commu - nities . For our proposed ﬁlter , the time consuming part of ﬁltering is the Part - of - Speech tagging and typed dependency relation generation parts . To measure the time eﬃciency of the implemented ﬁlter , we measure the speed in two cases : one is a normal case and the other is a high - overload case . In the normal case , we use the ﬁlter to process all text com - ments in the collected YouTube dataset . These comments contain both oﬀensive comments and inoﬀensive comments . In the overload case , we apply the ﬁlter on oﬀensive com - ments only . As the dataset is stored in ﬁles on local disk and each webpage is stored in a separate ﬁle , the measured processing time includes that for ﬁle I / O operations . The time cost in each case for processing the same number of comments is listed in Figure 5 . In both cases , the time cost increases almost linearly . In the high - overload case , the time cost is about 231 . 3 𝑚𝑠𝑒𝑐 per comment . In the normal case , the time cost increases much slower because a lot of comments processed are inoﬀensive comments . For those inoﬀensive comments , no grammatical analysis is needed . 0 300 600 900 1200 1500 1739 0 0 . 5 1 1 . 5 2 2 . 5 3 3 . 5 4x 10 5 Number of Comment Processed T i m e C o s t ( i n m illi s ec o nd ) High OverLoad Case Normal Case Figure 5 : Measurement of Filtering Time Cost in Two Cases Therefore , the average speed is much faster , about 38 . 6 𝑚𝑠𝑒𝑐 per comment . 5 . 2 Firefox Extension for Parental Control We also implemented the semantic ﬁlter as a Firefox ex - tension which can be embedded into Firefox browser to ﬁlter oﬀensive language in OSN websites . Brieﬂy , our extension consists of three modules , which are Pre - processor , Text An - alyzer , and Post - processor . The Pre - processor module is de - signed for content extraction . After receiving original web - page ( e . g . , a HTML ﬁle ) from a website , it decomposes the content of a webpage and extract its text content posted by community members . The Text Analyzer module applies the proposed semantic ﬁltering approach to ﬁlter the oﬀen - sive language in the text content ( Here we are interested in user messages ) . And the Post - processor module modiﬁes the original webpage based on the results of ﬁltering before showing the webpage in the browser . The biggest challenge to implement the extension is to understand the structure of a webpage , because we are only interested in text contents that are contributed by members of online communities . Admittedly , to segment and label the text content inside HTML elements of a random webpage on Internet is still an open problem . Luckily , OSN websites usually have neat and ﬁxed template for their webpages , such as YouTube , MySpace , and Facebook . All webpages in an OSN website share a limited number of templates . Users have no control over the structure of webpages . For example , if a user wants to post a comment on his friend’s blog , he has to ﬁrst send the text to the website and the website will update his friend’s blog with submitted texts . Based on this observation , we employ the Document Ob - ject Model ( DOM ) to extract the text content we are in - terested in . Each HTML webpage maps to a DOM tree where tags are internal nodes and the actual text , images or hyperlinks are the leaf nodes . For example , Figure 6 shows a comment with its corresponding fragments in the DOM tree . As we can see from Figure 6 , in all webpages on YouTube website , comment body is within the subtree with the “watch - comment - body” tag . Situation is the same in other OSN websites . So far , we have considered extracting templates from websites , such as YouTube , Google Search , MySpace blog , Facebook , and Twitter . For example , every user status update on Twitter’s post wall is within a node tagged by “entry - content” . Figure 6 : A YouTube comment and its correspond - ing DOM tree fragment Figure 7 : Comparison between YouTube ﬁlter and our semantic ﬁlter with partial oﬀensive sentences To demonstrate the eﬀectiveness of semantic ﬁltering with Firefox extension , we show a comparison of ﬁltered results between our Firefox extension and YouTube’s default ﬁlter ( i . e . the “Hide objectionable words” function ) , in Figures 7 and 8 . For better viewing , we put red lines at places with diﬀerences . As we can see from this comparison , the reader can easily guess the word ﬁltered by YouTube’s default ﬁl - ter . With proposed semantic ﬁltering approach , we provide a “transparent” ﬁltering . In these two cases , we are able to thoroughly remove the oﬀensive parts while keeping the inoﬀensive part as much as possible . We have also evaluated the speed of ﬁltering ordinary video webpages on YouTube websites . During the evalua - tion , we select top 50 most “popular” videos in “This Week” and records the time cost for processing each webpage . Each webpage contains 10 user comments . According to the ex - periments , the minimum time cost for processing one web - page is 43 𝑚𝑠𝑒𝑐 and the maximum is 2839 𝑚𝑠𝑒𝑐 . The dif - ference of time cost depends on the amount of text as well as the oﬀensive words in the webpage . The mean of pro - cessing time is about 752 𝑚𝑠𝑒𝑐 , 50 % webpages requires less than 431 𝑚𝑠𝑒𝑐 ﬁltering time , and 75 % webpages requires Figure 8 : Comparison between YouTube ﬁlter and our semantic ﬁlter with absolute oﬀensive sentences less than 1211 𝑚𝑠𝑒𝑐 . 6 . LIMITATIONS AND COUNTERMEASURES We now discuss ( 1 ) several limitations associated with our semantic ﬁltering approach as well as a few possible ways oﬀender may evade it , and ( 2 ) possible countermea - sures against those evasions . 6 . 1 Offensive Language Detection In this paper , we made an assumption that all oﬀen - sive opinions are expressed by oﬀensive words and we have a comprehensive oﬀensive lexicon containing all oﬀensive words . Based on this assumption , we adopt a simple word matching approach to identify oﬀensive words in the sen - tence to be ﬁltered . This assumption is made because the fo - cus of our paper is about oﬀensive language ﬁltering instead of detection . Since our ﬁltering approach depends on detec - tion of oﬀensive language , the ﬁltering might fail if oﬀensive language cannot be detected before the ﬁltering process . To avoid ﬁltering , oﬀender may try to evade the oﬀensive lan - guage detection mechanisms . For detection , there are many literatures discussing about detecting oﬀensive language in sentence level [ 3 ] or message level [ 10 ] . For oﬀensive lexi - con generation , [ 9 ] presents a study . We believe oﬀensive language detection a very challenging problem worthy of separate treatment . 6 . 2 Nature Language Process The language used in online communities has its own char - acteristics , compared to the language used in Journal and newspaper . When applying NLP techniques , people would worry about the accuracy because of such characteristics as casual and informal English writing style . As we mentioned in Section 5 . 1 . 2 , we did notice the inaccuracy brought by inaccurate text analysis . On the other hand , we discovered that language in online communities also has many charac - ters suitable for text analysis . First of all , compared with articles or journals , most text messages posted are usually very short . A typical example is the 𝑚𝑖𝑐𝑟𝑜 − 𝑏𝑙𝑜𝑔𝑔𝑖𝑛𝑔 ser - vice provided by Twitter 8 which allows users to send brief text updates ( less than 140 characters ) . Secondly , most text messages use spoken English which has very simple and neat grammatical structures , making it easy to achieve high accuracy in text analysis . This phe - nomenon is also called self - identity in social psychology . It states that people actually like to make comments as simple and clear as possible so that readers can understand his / her opinion easily . In our case , self - identity makes the oﬀensive comment easier to ﬁlter . If the comment is hard to analyze by a parser , it will probably cause diﬃculty to its human readers as well . 6 . 3 Changes to User Message One concern about the proposed semantic ﬁltering is the change made to the original text message . Admittedly , re - moving words from original message may cause information loss . Consider the information carried by original user mes - sage as 𝐼 = 𝐼 𝑖𝑛𝑜𝑓𝑓 + 𝐼 𝑜𝑓𝑓 . During the ﬁltering , oﬀensive information 𝐼 𝑜𝑓𝑓 is okay to be removed , but the inoﬀensive information 𝐼 𝑖𝑛𝑜𝑓𝑓 should not be deleted or altered . 8 Twitter , at http : / / twitter . com In semantic ﬁltering , we are fully aware of the impact of ﬁltering . As stated in the proposed “ﬁltering instead of blocking” philosophy , the semantic ﬁltering only removes the smallest semantic part which containing oﬀensive words in the sentence . Meanwhile , the readability of ﬁltered sentence will be kept , making ﬁltering transparent to user . Taking manual ﬁltering as the standard , we demonstrate the ability to achieving close results by comparison in experiments . 7 . CONCLUSION Oﬀensive language is a serious problem facing the online community . In this paper , we proposed a semantic ﬁltering technique based on the grammatical relations of words in a sentence so that the rest of the ﬁltered sentence is readable and the existence of oﬀensive words in the original sentence is hard to notice . We tested the eﬀectiveness of our approach with a large dataset and the results show that our techniques are very eﬀective and accurate with little process overhead . Our future work includes looking at the issues described in the discussion section . Moreover , as the most time - consuming part of semantic ﬁltering is the sentence parsing process , we will examine other light - weighted NLP techniques to speed up sentence parsing . Last but not the lease , we also plan to extend our ﬁltering approach to support other languages such as Chinese and French . 8 . REFERENCES [ 1 ] Bad word list and swear ﬁlter . Available : http : / / www . noswearing . com / list . php . [ 2 ] The stanford parser : A statistical parser . Available : http : / / nlp . stanford . edu / software / lex - parser . shtml . [ 3 ] M . K . Altaf Mahmud , Kazi Zubair Ahmed . Detecting ﬂames and insults in text . In Proceedings of 6th International Conference on Natural Language Processing , 2008 . [ 4 ] A . Bies , M . Ferguson , K . Katz , R . Macintyre , M . Contributors , V . Tredinnick , G . Kim , M . A . Marcinkiewicz , and B . Schasberger . Bracketing Guidelines for Treebank II Style Penn Treebank Project , 1995 . [ 5 ] J . Cheng . Report : 80 percent of blogs contain ”oﬀensive” content . ars technica , 2007 . [ 6 ] M . - C . de Marneﬀe and C . D . Manning . Stanford typed dependencies manual , 2008 . [ 7 ] D . Klein and C . D . Manning . Accurate unlexicalized parsing . In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics , 2003 . [ 8 ] H . Rheingold . The virtual community : Homesteading on the electronic frontier . Reading , Massachusetts : Addison - Wesley Publishing Company , 1993 . [ 9 ] J . Sjbergh and K . Araki . A multi - lingual dictionary of dirty words . In Proceedings of the 6th International Conference on Language Resources and Evaluation , 2008 . [ 10 ] E . Spertus . Smokey : Automatic recognition of hostile messages . In Proceedings of the 9th Conference on Innovative Application of AI , 1997 .