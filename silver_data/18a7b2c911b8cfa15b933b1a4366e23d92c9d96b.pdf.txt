JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 1 TOPIQ : A Top - down Approach from Semantics to Distortions for Image Quality Assessment Chaofeng Chen , Jiadi Mo , Jingwen Hou , Student Member , IEEE , Haoning Wu , Liang Liao , Member , IEEE , Wenxiu Sun , Qiong Yan , Weisi Lin , Fellow , IEEE Abstract —Image Quality Assessment ( IQA ) is a fundamental task in computer vision that has witnessed remarkable progress with deep neural networks . Inspired by the characteristics of the human visual system , existing methods typically use a combina - tion of global and local representations ( i . e . , multi - scale features ) to achieve superior performance . However , most of them adopt simple linear fusion of multi - scale features , and neglect their possibly complex relationship and interaction . In contrast , hu - mans typically first form a global impression to locate important regions and then focus on local details in those regions . We there - fore propose a top - down approach that uses high - level semantics to guide the IQA network to focus on semantically important local distortion regions , named as TOPIQ . Our approach to IQA involves the design of a heuristic coarse - to - fine network ( CFANet ) that leverages multi - scale features and progressively propagates multi - level semantic information to low - level representations in a top - down manner . A key component of our approach is the pro - posed cross - scale attention mechanism , which calculates attention maps for lower level features guided by higher level features . This mechanism emphasizes active semantic regions for low - level distortions , thereby improving performance . CFANet can be used for both Full - Reference ( FR ) and No - Reference ( NR ) IQA . We use ResNet50 as its backbone and demonstrate that CFANet achieves better or competitive performance on most public FR and NR benchmarks compared with state - of - the - art methods based on vision transformers , while being much more efficient ( with only ∼ 13 % FLOPS of the current best FR method ) . Codes are released at https : / / github . com / chaofengc / IQA - PyTorch . Index Terms —Image Quality Assessment , Top - down Approach , Multi - scale Features , Cross - scale Attention I . I NTRODUCTION I MAGE Quality Assessment ( IQA ) aims to estimate per - ceptual image quality similar to the human visual system ( HVS ) . It can be useful in enhancing the visual experience of humans in various applications such as image acquisition , compression , restoration , editing , and generation . The rapid advancement of image processing algorithms based on deep learning has created an urgent need for better IQA metrics . According to the requirement for pristine reference images , most IQA techniques can be categorized as Full - Reference ( FR ) IQA or No - Reference ( NR ) IQA . In both cases , multi - scale feature extraction is a crucial method to enhance the per - formance and is commonly utilized in both hand - crafted and C . Chen , J . Mo , J . Hou , H . Wu , L . Liao and W . Lin are with School of Computer Science and Engineering , Nanyang Technological University , Singapore . ( Email : [ chaofeng . chen , liang . liao , wslin ] @ ntu . edu . sg , [ JMO004 , jingwen003 , haoning001 ] @ e . ntu . edu . sg ) W . Sun and Q . Yan are with Tetras . AI and Sensetime Research . ( Email : [ irene . wenxiu . sun , sophie . yanqiong ] @ gmail . com ) Corresponding author : Weisi Lin . Distorted image A Distorted image B A is better B is better Humans ( MOS ) ✓ PSNR , SSIM , MS - SSIM ✓ LPIPS , DISTS ✓ Ours ✓ Fig . 1 : An example from the TID2013 dataset [ 1 ] ( the refer - ence image is omitted for easier comparison ) . It is noticeable that , although the large background region is noisy in image A , humans assign a higher quality score ( Mean Opinion Score , a . k . a . , MOS ) to A than to B , because the birds’ region in A is much clearer . This indicates that humans tend to focus on more semantically important regions . Simple multi - scale approaches such as LPIPS and DISTS ignore the correlation between high - level semantics and low - level distortions , and therefore , produce inconsistent judgments compared to humans . deep learning features . These multi - scale techniques can be roughly classified into three categories based on how they ex - tract and use multi - scale features : the parallel , bottom - up , and top - down methods ( as depicted in Fig . 2 for a brief overview ) . Traditional approaches , such as MS - SSIM [ 2 ] and NIQE [ 3 ] , typically use the parallel paradigm ( Fig . 2a ) . They resize the original image to create multi - scale inputs , and then extract features and calculate quality scores in parallel on these re - sized images . However , directly extracting features from multi - scale RGB images is often less effective because it is diffi - cult to obtain meaningful quality representations from a low - resolution RGB image . Bottom - up approaches extract feature pyramids from original images in a bottom - up manner , such as the traditional steerable pyramid used in CW - SSIM [ 4 ] . Deep learning - based approaches , such as LPIPS [ 5 ] and DISTS [ 6 ] , naturally follow the bottom - up approach ( Fig . 2b ) . They use features from different levels as individual components and estimate quality scores for them separately , and the final scores are obtained through a weighted sum . Although bottom - up ap - proaches are more effective than parallel methods in extracting multi - scale features , they have similar drawbacks : 1 ) they do a r X i v : 2308 . 03060v1 [ c s . C V ] 6 A ug 2023 JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 2 F e a t u r e E x t r a c t i o n & S c o r e R e g r e ss i o n S c o r e A v e r a g e Semantic Propagation Score High - level semantics Semantic Propagation Semantic Propagation S c o r e R e g r e ss i o n S c o r e A v e r a g e ( a ) Image pyramid : parallel approach , such as MS - SSIM , NIQE etc . F e a t u r e E x t r a c t i o n & S c o r e R e g r e ss i o n S c o r e A v e r a g e Semantic Propagation Score High - level semantics Semantic Propagation Semantic Propagation S c o r e R e g r e ss i o n S c o r e A v e r a g e ( b ) Feature pyramid : bottom - up ap - proach , such as LPIPS , DISTS etc . F e a t u r e E x t r a c t i o n & S c o r e R e g r e ss i o n S c o r e A v e r a g e Semantic Propagation Score High - level semantics Semantic Propagation Semantic Propagation S c o r e R e g r e ss i o n S c o r e A v e r a g e ( c ) Feature pyramid : our top - down approach . Fig . 2 : Three types of IQA framework based on how they extract and employ multi - scale features : the parallel , bottom - up and top - down methods . not consider the fact that high - level semantic information can guide the network to focus on more semantically active low - level features ; 2 ) two images with different distortions may have similar high - level semantic features , making it difficult to use these features to regress quality scores directly . For example , in Fig . 1 , image A has clearer bird heads but a much noisier background than image B . Humans are more sensitive to the quality of bird regions and tend to prefer image A , while MS - SSIM , LPIPS , and DISTS give better quality scores to image B due to the distraction from the large background region . This observation suggests that a top - down approach to exploiting multi - scale features , where high - level semantic features guide the level of distortion perception , may be ben - eficial ( see Fig . 2c for an example ) . However , to the best of our knowledge , most CNN - based approaches , including the latest works in the NTIRE IQA challenge [ 7 ] , still follow the bottom - up paradigm , and the top - down approach for multi - scale features remains largely under - explored . In this paper , we propose a top - down approach for IQA that utilizes deep multi - scale features . Our approach involves a heuristic coarse - to - fine attention network , referred to as CFANet . It emulates the process of the human visual system ( HVS ) by propagating semantic information from the highest level to the lowest level in a progressive manner . This heuris - tic design avoids the complexity of selecting among multiple features from different scales and has proven to be effective . Our key innovation is a novel cross - scale attention ( CSA ) mechanism that allows information propagation between dif - ferent levels . The CSA takes high - level features as guidance to select important low - level distortion features . Inspired by the widely used attention mechanism in transformers [ 8 ] , the proposed CSA is formulated as a query problem based on fea - ture similarities where high - level features serve as queries and low - level features make ( key , value ) pairs . Intuitively , the high level semantic features can be regarded as clustering centers , thereby aggregating low - level features that are more semanti - cally active . We apply multiple CSA blocks to multi - scale fea - tures from pretrained CNN backbones , such as ResNet50 [ 9 ] . A practical challenge is that the spatial size of feature maps , increases quadratically from coarse to fine level , which makes it expensive to directly calculate cross - scale attention in the original multi - scale features . To address this , we introduce a gated local pooling ( GLP ) block to reduce the size of low - level features . The GLP block consists of a gated convolution followed by average pooling with a predefined window size . It helps filter out redundant information and significantly reduces the computational cost . We conduct comprehensive experi - mental comparisons on both FR and NR ( including aesthetic ) IQA datasets . Our CFANet demonstrates better or competitive performance with lower computational complexity . Our contributions can be summarized as follows : • We introduce a top - down approach that leverages deep multi - scale features for IQA . Unlike previous parallel and bottom - up methods , our proposed CFANet can effectively propagate high - level semantic information from coarse to fine scales , enabling the network to focus on distortion regions that are more semantically important . • We propose a novel cross - scale attention ( CSA ) mecha - nism to transfer high - level semantics to low - level distor - tion representations . Additionally , we introduce a gated local pooling ( GLP ) block that reduces the computational cost by filtering redundant information . • Our proposed CFANet is significantly more efficient than state - of - the - art approaches . With a simple ResNet50 [ 9 ] backbone , it achieves competitive performance while only requiring approximately 13 % of the floating point oper - ations ( FLOPS ) of the best existing FR method . II . R ELATED W ORKS A . Full - Reference Image Quality Assessment FR - IQA methods compare a reference image and a distorted image to measure the dissimilarities between them . The most commonly used traditional metric is peak signal - to - noise ratio ( PSNR ) , which is simple to calculate and represents the pixel - wise fidelity of the images . However , the HVS is highly non - linear , and the pixel - wise comparison of PSNR does not align with human perception . To address this , Wang et al . [ 10 ] intro - duced the structural similarity ( SSIM ) index to compare struc - tural similarity in local patches , which inspired a lot of follow - up works [ 4 ] , [ 11 ] – [ 16 ] . These works introduce more compli - cated hand - crafted features to measure image dissimilarities . Learning - based approaches have been proposed recently to overcome the limitations of hand - crafted features . However , early end - to - end works [ 17 ] , [ 18 ] suffer from over - fitting . Zhang et al . [ 5 ] proposed a large - scale dataset and found that pretrained deep features are effective for measuring perceptual similarity . Similarly , Prashnani et al . [ 19 ] created a comparable dataset . Gu et al . [ 20 ] proposed the PIPAL dataset and initiated the NTIRE2021 [ 21 ] and NTIRE2022 [ 7 ] IQA challenges . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 3 This greatly advanced deep learning - based IQA , leading to the emergence of many new approaches . Among these , methods based on vision transformers , such as IQT [ 22 ] and AHIQ [ 23 ] , perform the best . B . No - Reference Image Quality Assessment NR - IQA is a more challenging task due to a lack of ref - erence images . There are two subtasks in NR - IQA : technical quality assessment [ 24 ] and aesthetic quality assessment [ 25 ] . The former focuses on technical aspects of the image such as sharpness , brightness , and noise , and is commonly used to measure the fidelity of an image to the original scene and the accuracy of image acquisition , transmission , and re - production . The latter , on the other hand , is concerned with the subjective perceptions of viewers towards the visual ap - peal of an image , taking into account aesthetic aspects such as composition , lighting , color harmony , and overall artis - tic impression . As such , image aesthetic evaluation is more subjective than image quality evaluation , as it is largely de - pendent on individual viewer’s personal preferences and cul - tural background . Although they have different focus , both of them involve subjective or objective assessment of visual images , and are influenced by factors such as lighting , color accuracy , and sharpness . Traditional approaches for NR - IQA rely on natural scene statistics ( NSS ) [ 3 ] , [ 26 ] – [ 30 ] . While NSS - based methods perform well in distinguishing synthetic technical distortions , they struggle with modeling authentic technical distortions and aesthetic quality assessment . As a result , many works have turned to deep learning for NR - IQA . They are generally improved with more advanced network architecture , from deep belief net [ 31 ] to CNN [ 32 ] , then to deeper CNN [ 33 ] – [ 35 ] , later to ResNet [ 36 ] – [ 38 ] , and now vision transformers [ 39 ] – [ 41 ] . In additional to these works , there have been several notable works in NR - IQA . Liu et al . [ 42 ] introduced a ranking loss for pretraining networks with synthetic data . Talebi et al . [ 43 ] proposed a new distribution loss to replace simple score regression . Zheng et al . [ 44 ] pro - posed generating the degraded - reference representation from the distorted image via knowledge distillation . Ke et al . [ 45 ] employed multi - scale inputs and a vision transformer back - bone to process images with varying sizes and aspect ratios . Hu et al . [ 46 ] focus on the quality evaluation of image restora - tion algorithms . They proposed a pairwise - comparison - based rank learning framework [ 47 ] and a hierarchical discrepancy learning model [ 48 ] for performance benchmarking of image restoration algorithms . Despite achieving promising performance , the latest ap - proaches based on transformers are typically more computa - tionally expensive than ResNet models to achieve the same level of performance with the same input size . Furthermore , the computational cost of transformers increases quadratically with larger image sizes , which can be a significant drawback . This work shows that by imitating the global - to - local process of the HVS , our model can achieve better or comparable per - formance in both FR and NR tasks using a simple ResNet50 as the backbone . I m a g e A I m a g e B I m a g e A I m a g e B 384 × 512 192 × 256 96 × 128 48 × 64 24 × 32 Fig . 3 : Multi - scale spatial quality maps ( H × W ) of MS - SSIM ( top two rows ) and LPIPS ( bottom two rows ) with example images ( Image A and Image B ) from Fig . 1 . Please zoom in for best view . Note : since LPIPS is lower better , we use ( 1 - LPIPS ) here . III . T HE T OP - D OWN A PPROACH FOR IQA A . Observations and Motivation To illustrate our motivation , we conducted a detailed anal - ysis of two seminal multi - scale approaches : the MS - SSIM and LPIPS 1 . We used example images from Fig . 1 and the TID2013 dataset for our analysis . Figure 3 shows the spatial quality maps of MS - SSIM and LPIPS before pooling for example images from Fig . 1 . We have the following observations : • Both MS - SSIM and LPIPS appear to be distracted by the large background region in Image B , leading them to assign higher final scores to Image B . However , humans tend to focus more on the birds region and tend to prefer Image A . • For these two cases , the high - level differences between Image A and Image B are small . MS - SSIM appears to have difficulties in extracting semantic features , and the pixel - level differences after downsampling are also small . On the other hand , the backbone network of LPIPS is capable of extracting high - level semantics , but it tends to lose distortion differences . Therefore , it can be challeng - ing to determine which image is better based on high - level feature differences alone . Based on these observations , we hypothesize that neither parallel nor bottom - up approaches can fully utilize multi - scale features . The parallel methods , such as MS - SSIM , have diffi - culties in extracting semantic representations . Conversely , for bottom - up approaches like LPIPS , although they can extract better semantic representations , they typically regress scores with different scale features independently , and therefore , are unable to focus on semantic regions as humans do . The LPIPS + metric . To verify our hypothesis , we explore a simple extension of LPIPS by replacing the average pooling 1 LPIPS has many different versions . We use the VGG backbone of the latest 0 . 1 version here . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 4 ( a ) Example of multi - scale semantic activation maps in LPIPS from low - level to high - level . Please zoom in for best view . LPIPS 1 2 3 4 5 0 . 748 0 . 753 0 . 758 0 . 763 0 . 768 P L CC on T I D 2013 ( b ) LPIPS + using different layers as semantic weights . Fig . 4 : Empirical study of the LPIPS + metric . ( a ) The fea - ture activation maps can be roughly taken as semantic weight maps ; ( b ) The third layer semantic features bring the most improvement compared with original LPIPS . with weighted average pooling , denoted as LPIPS + . We take the feature maps of reference images as rough estimations of semantic weights . As is known , features with higher activa - tion values in neural networks usually correspond to semantic regions , as shown in Fig . 4a for an example . Take reference features from i - th layer as F ri ∈ R C i × H i × W i , and the spatial quality map of m - th layer as S rm ∈ R 1 × H m × W m , LPIPS + can be briefly formulated as follow : LPIPS + = (cid:88) m (cid:80) Resize ( F ri ) ⊙ S rm (cid:80) Resize ( F ri ) , ( 1 ) where ⊙ is element - wise multiplication , F ri is resized to the same shape as S rm using bilinear interpolation , and the sum - mary dimension is omitted here for simplicity . From the exam - ples in in Fig . 4a , we can see that F ri in different layers display varying scales of semantic structures . As a result , we con - ducted an empirical study on TID2013 to evaluate the selection of semantic weight maps F ri . The results , depicted in Fig . 4b , show that all layers of semantic weight maps contribute to per - formance improvement , highlighting the importance of seman - tic information for multi - scale features . It is worth noting that each layer encompasses different scales of semantic structures , resulting in differing levels of performance enhancement . For LPIPS + , we selected i = 3 based on our empirical findings . It is worth mentioning that LPIPS + is an improved version of LPIPS that does not require additional training . The performance enhancements resulting from this simple extension have motivated us to develop a more robust frame - work that leverages the full potential of multi - scale features for IQA . To avoid the tedious and non - generalizable manual selection of multi - scale features across various datasets , we propose a heuristic top - down approach . This paradigm has proven to be effective in many different tasks , including object detection [ 49 ] and semantic segmentation [ 50 ] . In the follow - ing section , we provide details on our top - down framework . B . Architecture of Coarse - to - Fine Attention Network We have employed the top - down paradigm to develop the Coarse - to - Fine Attention Network ( CFANet ) to improve the utilization of multi - scale features for IQA , which can be ap - plied to both FR and NR tasks . In this section , we focus on introducing the FR framework , as the NR framework is a simplified version . The pipeline of CFANet - FR is presented in Fig . 5 . Given distortion - reference image pairs as input , we first extract their multi - scale features using a backbone network . Next , we employ gated local pooling ( GLP ) to reduce the multi - scale features to the same spatial size , which are then enhanced using self - attention ( SA ) blocks . Subsequently , we progressively apply cross - scale attention ( CSA ) blocks from high - level to low - level features . Finally , we pool the semantic - aware distortion features and regress them to the quality score through a multilayer perceptron ( MLP ) . We provide a detailed explanation of each component below . 1 ) Gated Local Pooling : Denote input image pairs as ( I d , I r ) ∈ R 3 × H × W , the backbone features from block i as ( F di , F ri ) ∈ R C i × H i × W i , where H i , W i are height and width , C i is the channel dimension , i ∈ { 1 , 2 , . . . , n } and n = 5 for ResNet50 . In general , low - level features are twice larger than their adjacent high - level features , and we have H i = H / 2 i . Therefore , directly compute correlation between large matrix like F 1 and F 2 is too expensive . For simplicity and efficiency , we reduce F i to the same shape as the highest level features F n . A na ¨ ıve solution is simple window average pooling . How - ever , this would fuse features inside local window and make the distortion feature less distinguishable . Instead , we propose to select the distortion related features before pooling through a gated convolution [ 51 ] , which has been proven to be useful in image inpainting . The problem here is how to calculate the gating mask . Notice that for FR task , the difference between ( F di , F ri ) is a strong clue for feature selection , we therefore formulate the gated convolution as F maski = σ (cid:0) ϕ i ( | F di − F ri | ) (cid:1) · ( F di ⊕ F ri ⊕ | F di − F ri | ) , ( 2 ) where σ is the sigmoid activation function that constrains the mask value to the range of [ 0 , 1 ] , ϕ i represents a bottleneck convolution block , and ⊕ denotes the concatenation operation . Please refer to Fig . 6 for further details . For efficiency , we use a single - channel mask , i . e . , ϕ i ( · ) ∈ R 1 × H i × W i . For the NR task , we use the same gated convolution formu - lation as follows : F maski = σ ( ϕ i ( F i ) ) · ReLU ( W f F i ) . ( 3 ) Subsequently , the masked feature F maski undergoes window average pooling and a linear dimension reduction layer , pro - ducing features G i ∈ R D × H n × W n for the following blocks , where D denotes the reduced feature dimension . Our experi - ments show that our model can learn quality - aware masks and filter redundant features , as illustrated by the visualization of the gated mask . 2 ) Attention Modules : To help with the IQA task , we uti - lize the scaled dot - product attention [ 8 ] as the basis for our attention modules . Given triplets of feature vectors ( query , key , value ) , the attention function first calculates similarities between the query ( Q ) and key ( K ) vectors and then outputs JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 5 Feature Extraction CSA CSA CSA S A - P OO L M L P  SA SA SA SA GLP GLP GLP GLP          −    Position encoding     G 1′′ G 1′ G 4′ G 3′ G 2′ GLP SA CSA G 5′ Fig . 5 : Architecture overview of the proposed CFANet - FR . We use 5 - scale features here same as previous works such as MS - SSIM and LPIPS . 1x1 , 64 1x 1 , 1 3x3 , 64 |    −    |    ⊕    ⊕ |    −    | 1x1 , D B l o c k A vg P oo l   Fig . 6 : The GLP block comprises a mask branch and a feature branch . The mask branch is a bottleneck convolution block with an internal channel dimension of 64 . For FR datasets , we set the output dimension D to 256 , and for NR datasets , we set it to 512 . All convolution layers are followed by the GELU activation function . the weighted sum of values ( V ) . Suppose Q ∈ R N q × d k , K ∈ R N v × d k , V ∈ R N v × d v , the attention output is computed as Attn ( Q , K , V ) = softmax ( QK T √ d k ) V , ( 4 ) where N q and N v represent the number of feature vectors , and d k and d v indicate the feature dimension . We employ Eq . ( 4 ) in various ways to aid the IQA task . a ) Self - attention : After GLP , we obtain a set of fea - tures from different scales , denoted by { G 1 , . . . , G n } ∈ R ( H n × W n ) × D . As the receptive field of low - level features is limited , we first enhance G i with a self - attention block as follows : G ′ i = SA ( G i ) = Attn ( G i W q , G i W k , G i W v ) + G i , ( 5 ) where G i is projected onto Q , K , V through simple linear projection . Through the SA block , G ′ i aggregates features from other positions to enhance G i . In [ 40 ] , they concatenate the multi - scale features and use several transformer layers to regress the score , without considering the fact that different semantic regions hold different importance to humans . This approach does not allow for interaction between high - level se - mantic features and low - level distortion features , and thus can - not model such relationships . Our proposed cross - scale atten - tion method addresses this issue in a straightforward manner . b ) Cross - scale Attention : Since the query feature Q in Eq . ( 4 ) naturally serves as a guide when computing the out - put , our cross - attention is designed by simply generating the Q , K , V with features from different scales , i . e . , G ′′ i = CSA ( G ′ i , G ′′ i + 1 ) = Attn ( W q G ′′ i + 1 , W k G ′ i , W v G ′ i ) + G ′′ i + 1 , ( 6 ) where i ∈ { 1 , . . . , n − 1 } , and G ′′ n = G ′ n . Intuitively speak - ing , the CSA block selects the most semantically relevant distortions in G ′ i with high - level features G ′′ i + 1 . The residual connection here serves as a simple fusion between features from different levels . The final output can be obtained by progressively applying CSA as G ′′ 1 = CSA (cid:0) . . . CSA (cid:0) G ′ n − 2 , CSA ( G ′ n − 1 , G ′ n ) (cid:1)(cid:1) . ( 7 ) 3 ) Unified position encoding : In transformers , position en - coding is crucial to inject awareness of feature positions in Eq . ( 4 ) . In our CSA blocks , position information is also impor - tant as another clue for cross - scale feature query . In [ 45 ] , Ke et al . designed a hash - based 2D spatial embedding for multi - scale inputs . In our framework , since the multi - scale features G i have the same shape after GLP , we simply add the same learnable position encoding to all G i , as shown in Fig . 5 . This unified position encoding enables CSA to better match features from different scales . 4 ) Score Regression : The final scores are obtained using the final features G ′′ 1 as follows : ˆ y = MLP (cid:0) SA - Pool ( G ′′ 1 ) (cid:1) , ( 8 ) where SA - Pool is a self - attention block followed by average pooling . The SA block is added to better assemble features from all positions . When predicting score distributions , we have ˆ p = softmax ( ˆ y ) . C . Loss Functions Since different datasets have different kinds of labels , we need different losses for them , which are detailed below : 1 ) MOS labeled datasets : For these datasets , we first nor - malize the MOS scores to [ 0 , 1 ] and then use the MSE loss . 2 ) MOS distribution labels : For datasets that are labeled with score distributions , such as the AVA dataset [ 25 ] , we predict the distribution and use the Earth Mover’s Distance ( EMD ) loss proposed by [ 43 ] . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 6 TABLE I : FR and NR IQA Datasets used for training and evaluation . Type Dataset # Ref # Dist Dist Type . # Rating Split Original size W × H Resize ( shorter side ) Train size ( cropped patch ) FR LIVE 29 779 Synthetic 25k 6 : 2 : 2 768 × 512 ( typical ) — 384 × 384 CSIQ 30 866 Synthetic 5k 6 : 2 : 2 512 × 512 — 384 × 384 TID2013 25 3 , 000 Synthetic 524k 6 : 2 : 2 512 × 384 — 384 × 384 KADID - 10k 81 10 . 1k Synthetic 30 . 4k 6 : 2 : 2 512 × 384 — 384 × 384 PieAPP 200 20k Synthetic 2 . 3M Official 256 × 256 — 224 × 224 BAPPS – 187 . 7k Syth . + alg . 484k Official 500 × 500 — 384 × 384 PIPAL 250 29k Syth . + alg . 1 . 13M Official 288 × 288 — 224 × 224 NR CLIVE – 1 . 2k Authentic 350k 8 : 2 500 × 500 — 384 × 384 KonIQ - 10k 10k Authentic 1 . 2M 8 : 2 512 × 384 — 384 × 384 SPAQ 11k Authentic – 8 : 2 4K ( typical ) 448 384 × 384 AVA 250k Aesthetic 53M Official < 800 384 ∼ 416 384 × 384 FLIVE 160k Auth . + Aest . 3 . 9M Official Train < 640 | Test > 640 384 ∼ 416 384 × 384 3 ) 2AFC datasets : Some recent large scale datasets , such as PieAPP [ 19 ] and BAPPS [ 5 ] are labeled with preference through 2AFC ( two - alternative force choice 2 ) rather than sin - gle MOS label . Given triplet pairs , a reference image with two distorted images denoted as ( I r , I A , I B ) , the datasets provide the probability of subject preference to one of I A and I B . Fol - lowing the same practice of [ 19 ] , we first learn the perceptual error scores for I A and I B with the network separately , i . e . , ˆ y A = CFANet ( I r , I A ) , ˆ y B = CFANet ( I r , I B ) . ( 9 ) Then , ˆ y A and ˆ y B are used to compute the preference prob - ability of I A over I B with the Bradley - Terry ( BT ) sigmoid model [ 52 ] as follows , ˆ p AB = 1 1 + e ˆ y A − ˆ y B . ( 10 ) The common MSE is finally used as the loss function : L 2 AFC ( ˆ y A , ˆ y B , p AB ) = 1 N N (cid:88) i = 1 ∥ ˆ p AB − p AB ∥ 2 . ( 11 ) IV . E XPERIMENTS A . Implementation Details 1 ) Datasets : As shown in Tab . I , we conduct experi - ments on several public benchmarks . For FR datasets , we have LIVE [ 53 ] , CSIQ [ 54 ] , TID2013 [ 1 ] , KADID - 10k [ 55 ] , PieAPP [ 19 ] , BAPPS [ 5 ] and PIPAL [ 20 ] . For NR datasets , we have got CLIVE [ 56 ] , KonIQ - 10k [ 24 ] , SPAQ [ 57 ] , FLIVE [ 58 ] and AVA [ 25 ] . We use the official train / val / test splits if available , otherwise , we randomly split it 10 times and report the mean and variance . For FR datasets , the split is based on reference images to avoid content overlapping . 2 ) Performance Evaluation : We applied two commonly used metrics : the Pearson linear correlation coefficient ( PLCC ) and the Spearman’s rank - order correlation coefficient ( SRCC ) . PLCC measures the linear correlation between predicted scores ( ˆ y ) and ground truth labels ( y ) , while SRCC assesses rank correlation . The same as [ 6 ] , [ 35 ] , we fitted a 4 - parameter 2 The subjects need to choose a better one given two candidates . logistic function to the predicted scores before calculating PLCC : ˆ y ′ = β 1 − β 2 1 + exp ( − ( ˆ y − β 3 ) / | β 4 | ) + β 2 , ( 12 ) where { β i | i = 1 , 2 , 3 , 4 } are fitted with least square losses between ˆ y ′ and GT labels y , and are initialized with β 1 = max ( y ) , β 2 = min ( y ) , β 3 = µ ( ˆ y ) , β 4 = σ ( ˆ y ) / 4 . Here , σ ( · ) is the standard variation . 3 ) Training Details : We use ResNet50 , pretrained on Ima - geNet [ 59 ] , as the backbone for most of our experiments . As is common in domain transfer , we fix the batch normalization layers and finetune the other parameters . We use data aug - mentation operators that do not affect image quality , such as random crop and horizontal / vertical flip . We use the AdamW optimizer with a weight decay of 10 − 5 for all experiments . The initial learning rate ( lr ) is set to 10 − 4 for FR datasets and 3 × 10 − 5 for NR datasets . We use a cosine annealing scheduler with T max = 50 , η min = 0 , η max = lr , following previous works [ 22 ] , [ 23 ] . The total number of training epochs is 200 , and we use early stopping based on validation performance to reduce training time . Our model is implemented using PyTorch and trained on an NVIDIA V100 GPU . We keep the training settings , including network hyperpa - rameters and optimizer settings , consistent across different FR and NR benchmarks . However , due to differences in image sizes across datasets , we have to resize the images to an appropriate size for training the network . As shown in Tab . I , images from three datasets , SPAQ , AVA , and FLIVE , need to be resized . To preserve image quality , we maintain aspect ratio during resizing and Tab . I shows the size of the shorter side after resize . For AVA and FLIVE , we randomly set the shorter side between 384 and 416 as a data augmentation strategy . B . Visualization of Attention Maps In this part , we visualize attention maps to show how CFANet works in a top - down manner . CFANet has two types of attention maps : i ) the distortion attention masks learned in GLP and ii ) the cross - scale attention maps learned in CSA blocks . The former filters redundant information and reduces JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 7 Distorted Image ϕ 4 ( · ) ϕ 3 ( · ) ϕ 2 ( · ) ϕ 1 ( · ) Reference Image CSA ( F 5 → F 4 ) CSA ( F 4 → F 3 ) CSA ( F 3 → F 2 ) CSA ( F 2 → F 1 ) ( a ) Example with “gaussian blur” distortion . Distorted Image ϕ 4 ( · ) ϕ 3 ( · ) ϕ 2 ( · ) ϕ 1 ( · ) Reference Image CSA ( F 5 → F 4 ) CSA ( F 4 → F 3 ) CSA ( F 3 → F 2 ) CSA ( F 2 → F 1 ) ( b ) Example with “high frequency noise” distortion . Distorted Image ϕ 4 ( · ) ϕ 3 ( · ) ϕ 2 ( · ) ϕ 1 ( · ) Reference Image CSA ( F 5 → F 4 ) CSA ( F 4 → F 3 ) CSA ( F 3 → F 2 ) CSA ( F 2 → F 1 ) ( c ) Example with “change of color saturation” . Fig . 7 : Attention visualization with different distortion types from TID2013 dataset . First row : GLP mask , ϕ i ( · ) in Eqs . ( 2 ) and ( 3 ) ; Second row : CSA attention weights . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 8 TABLE II : Quantitative comparison with related works on public FR benchmarks , including the traditional LIVE , CSIQ , TID2013 with MOS labels , and recent large scale datasets PieAPP , PIPAL with 2AFC labels . The best and second results are colored in red and blue , and “ - ” indicates the score is not available or not applicable . LIVE [ 53 ] CSIQ [ 54 ] TID2013 [ 1 ] PieAPP [ 19 ] PIPAL [ 20 ] Method PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PSNR 0 . 865 0 . 873 0 . 819 0 . 810 0 . 677 0 . 687 0 . 135 0 . 219 0 . 277 0 . 249 SSIM [ 10 ] 0 . 937 0 . 948 0 . 852 0 . 865 0 . 777 0 . 727 0 . 245 0 . 316 0 . 391 0 . 361 MS - SSIM [ 2 ] 0 . 940 0 . 951 0 . 889 0 . 906 0 . 830 0 . 786 0 . 051 0 . 321 0 . 163 0 . 369 VIF [ 11 ] 0 . 960 0 . 964 0 . 913 0 . 911 0 . 771 0 . 677 0 . 250 0 . 212 0 . 479 0 . 397 FSIMc [ 13 ] 0 . 961 0 . 965 0 . 919 0 . 931 0 . 877 0 . 851 0 . 481 0 . 378 0 . 571 0 . 504 MAD [ 12 ] 0 . 968 0 . 967 0 . 950 0 . 947 0 . 827 0 . 781 0 . 231 0 . 304 0 . 580 0 . 543 GMSD [ 14 ] 0 . 957 0 . 960 0 . 945 0 . 950 0 . 855 0 . 804 0 . 242 0 . 297 0 . 608 0 . 537 VSI [ 15 ] 0 . 948 0 . 952 0 . 928 0 . 942 0 . 900 0 . 897 0 . 364 0 . 361 0 . 517 0 . 458 NLPD [ 16 ] 0 . 932 0 . 937 0 . 923 0 . 932 0 . 839 0 . 800 0 . 360 0 . 245 0 . 401 0 . 355 DeepQA [ 17 ] 0 . 982 0 . 981 0 . 965 0 . 961 0 . 947 0 . 939 0 . 172 0 . 252 - - WaDIQaM - FR [ 18 ] 0 . 980 0 . 970 - - 0 . 946 0 . 940 0 . 439 0 . 352 0 . 548 0 . 553 PieAPP [ 19 ] 0 . 986 0 . 977 0 . 975 0 . 973 0 . 946 0 . 945 0 . 842 0 . 831 0 . 597 0 . 607 LPIPS - VGG [ 5 ] 0 . 978 0 . 972 0 . 970 0 . 967 0 . 944 0 . 936 0 . 654 0 . 641 0 . 633 0 . 595 DISTS [ 6 ] 0 . 980 0 . 975 0 . 973 0 . 965 0 . 947 0 . 943 0 . 725 0 . 693 0 . 687 0 . 655 JND - SalCAR [ 60 ] 0 . 987 0 . 984 0 . 977 0 . 976 0 . 956 0 . 949 - - - - IQT [ 22 ] - - - - - - 0 . 829 0 . 822 0 . 790 0 . 799 AHIQ [ 23 ] 0 . 989 0 . 984 0 . 978 0 . 975 0 . 968 0 . 962 0 . 840 0 . 838 0 . 823 0 . 813 TOPIQ ( CFANet - ResNet50 ) 0 . 984 0 . 984 0 . 980 0 . 978 0 . 958 0 . 954 0 . 849 0 . 841 0 . 830 0 . 813 std ± 0 . 003 ± 0 . 003 ± 0 . 003 ± 0 . 002 ± 0 . 011 ± 0 . 012 - - - - the spatial size of feature maps , while the latter enables se - mantic propagation from coarse to fine . Figure 7 shows the visualization of the learned masks in GLP blocks for multi - scale features F 1 , · · · , F 4 and the cross - scale attention weights from F i + 1 to F i in CSA blocks . Examples of three different distortions , i . e . , “gaussian blur” , “high frequency noise” , and “change of color saturation” , are presented . We can observe that GLP blocks can selectively identify distortion - related features at different scales for different types of distortions , especially in F 1 . The CSA attention maps show that the model gradually focuses on semantic regions in a coarse - to - fine manner . For example , in Fig . 7a ( the Image B in Fig . 1 ) , the network is not distracted by the large background regions and is able to focus on the birds . This explains why CFANet makes consistent judgements with humans in the case in Fig . 1 . Similar observations can be found in Fig . 7b and Fig . 7c , which prove that CFANet is robust to different types of distortions . These observations demonstrate that CFANet effectively extracts semantically important distortion features . C . Comparison with FR Methods To demonstrate the superiority of the top - down approach , we compare our proposed CFANet to various traditional and deep learning methods using FR benchmarks ( see Tab . I ) . Our evaluations include both intra - dataset and cross - dataset exper - iments . Additionally , we compare our results to those of the widely recognized LPIPS using the same experimental setup . 1 ) Intra - dataset results of public benchmarks : We con - ducted intra - dataset experiments on five benchmarks , namely LIVE , CSIQ , TID2013 , PieAPP , and PIPAL . The first three datasets are small synthetic datasets labeled with MOS scores , while the latter two are much larger datasets labeled through 20 40 60 80 100 120 140 160 # FLOPS ( G ) 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 P L CC o n N T I R E 2022 C h a ll e n g e F R T r a c k WaDIQaM PieAPP LPIPS DISTS IQT ( CVPRW 2021 Champion ) AHIQ ( CVPRW 2022 Champion ) Ours Params ( M ) 10 20 50 200 Fig . 8 : Computational cost ( FLOPS ) vs . Performance ( PLCC ) on NTIRE IQA Challenge 2022 FR Track . Our model achieves the best performance with only ∼ 13 % FLOPS as previous state - of - the - art AHIQ . Note : The input image size is 3 × 224 × 224 . The number of parameters is indicated by the circle radius . For AHIQ , the backbone is fixed and the number of trainable parameters is indicated by the orange circle . 2AFC and contain a wider variety of distortion types . The results are presented in Tab . II . As we can see , both traditional and deep learning methods perform well on the easier conven - tional benchmarks , LIVE , CSIQ , and TID2013 , which only contain a few types of synthetic distortions . In particular , the proposed CFANet performs as well as AHIQ and demonstrates remarkable performance . It’s important to note that perfor - mance on these three datasets can vary significantly due to dif - ferent splits , especially for TID2013 according to the variance . Regarding the larger - scale datasets , PieAPP and PIPAL , our CFANet outperforms all previous methods , including JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 9 TABLE III : Comparison of cross - dataset performance on public benchmarks . Train dataset KADID - 10k PIPAL Test dataset LIVE CSIQ TID2013 LIVE CSIQ TID2013 Method PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC WaDIQaM - FR [ 18 ] 0 . 940 0 . 947 0 . 901 0 . 909 0 . 834 0 . 831 0 . 895 0 . 899 0 . 834 0 . 822 786 0 . 739 PieAPP [ 19 ] 0 . 908 0 . 919 0 . 877 0 . 892 0 . 859 0 . 876 - - - - - - LPIPS - VGG [ 5 ] 0 . 934 0 . 932 0 . 896 0 . 876 0 . 749 0 . 670 0 . 901 0 . 893 0 . 857 0 . 858 0 . 790 0 . 760 DISTS [ 6 ] 0 . 954 0 . 954 0 . 928 0 . 929 0 . 855 0 . 830 0 . 906 0 . 915 0 . 862 0 . 859 0 . 803 0 . 765 AHIQ [ 23 ] 0 . 952 0 . 970 0 . 955 0 . 951 0 . 899 0 . 901 0 . 911 0 . 920 0 . 861 0 . 865 0 . 804 0 . 763 TOPIQ ( Resnet50 ) 0 . 957 0 . 974 0 . 963 0 . 969 0 . 916 0 . 915 0 . 913 0 . 939 0 . 908 0 . 908 0 . 846 0 . 816 the AHIQ with a heavy transformer backbone . Notably , our CFANet achieves this with a simple ResNet50 backbone , demonstrating the remarkable effectiveness of the proposed top - down framework for IQA . 2 ) Cross dataset experiments : Furthermore , CFANet ex - hibits significantly better generalization abilities with fewer parameters , as reported in Tab . III . With the current largest dataset , PIPAL , containing only 29k pairs 3 , larger models also face the issue of overfitting . Comparing the results in Tab . II and Tab . III , we can observe that the performance gaps of AHIQ on LIVE , CSIQ , and TID2013 are much larger than those of CFANet , demonstrating that the simpler CFANet is more robust across different datasets . 3 ) Comparison of computation complexity : Figure 8 presents an intuitive comparison of the computational ex - penses of recent deep learning - based FR methods . It is ev - ident that CFANet exhibits the best performance with only approximately 13 % FLOPS and around 1 / 7 of AHIQ’s param - eters . While earlier works with simpler architectures , such as WaDIQaM , are more efficient , their performance is notably inferior . With the aid of the efficient ResNet50 backbone , CFANet is also more efficient than LPIPS . In terms of infer - ence time , methods with CNN backbones , including CFANet , are comparable and nearly twice as fast as transformer - based approaches like AHIQ . In summary , CFANet strikes the best balance between performance and computational complexity . 4 ) Comparison on BAPPS dataset : BAPPS [ 5 ] is a 2AFC FR dataset proposed by the widely recognized LPIPS . Because its evaluation protocol differs from other mainstream datasets , we provide a separate comparison experiment on BAPPS in this section . The validation set of BAPPS only has binary preference labels , so we cannot calculate PLCC and SRCC scores . Instead , LPIPS uses the consistency between model preference and human judgment to calculate the final score , which is defined as follows : Score = 1 ( ˆ y A < ˆ y B ) 1 ( p A < p B ) + 1 ( ˆ y A > ˆ y B ) 1 ( p A > p B ) + 0 . 5 1 ( ˆ y A = ˆ y B ) . ( 13 ) This score only measures the binary preference judgements rather than exact probability values . The comparison of CFANet and other methods on the 2AFC test set of BAPPS is shown in Tab . IV . We can observe that 3 Due to ambiguities in human perception , one image pair usually requires dozens of annotations to obtain the final MOS , making it expensive to build large - scale datasets for IQA . the proposed CFANet achieves the best performance on both synthetic and real algorithmic distortions , outperforming pre - vious approaches by a large margin . Our results are very close to human judgments , especially on synthetic distortions . In ad - dition , we also tested the proposed LPIPS + . The results show that LPIPS + outperforms LPIPS in almost all sub - tasks , further proving the effectiveness of semantic guidance for IQA . D . Comparison with NR Methods NR - IQA is more challenging than FR - IQA due to the lack of references and the complexity of criteria . As discussed in related works , we split the NR datasets into two types : tech - nical quality assessment and aesthetic quality assessment , as shown in Tab . I . We compare the proposed CFANet on both of these types in the following sections . 1 ) Results on technical distortion benchmarks : There are mainly three NR datasets with authentic distortion , namely CLIVE ( also known as the LIVE Challenge dataset ) , KonIQ - 10k , and SPAQ , with the latter two being much larger than the first one . According to the results in Tab . V and Tab . VII , we can see that traditional approaches based on hand - crafted NSS features cannot handle natural images with complicated authentic distortions , while deep learning methods perform much better . In all three of these datasets , our model with a ResNet50 backbone outperforms existing CNN - based methods in both PLCC and SRCC . Our results are also better than MUSIQ , which is a purely vision transformer architecture . This indicates that the proposed CFANet is effective for au - thentic distortions even without reference images . 2 ) More results on KonIQ - 10k : Following previous works [ 37 ] , [ 45 ] , we report the results of 10 random splits on KonIQ - 10k in Tab . V . However , [ 24 ] provides a fixed split in their official codes 4 , and reports their results on it . We also report our results with the same setting in Tab . VIII . We can observe that with a simple ResNet50 backbone , CFANet outperforms both KonCepth512 with inception - resnet - v2 [ 64 ] and MUSIQ with a vision transformer [ 8 ] . This further proves the effec - tiveness and efficiency of the proposed CFANet . 3 ) Results for aesthetic quality estimation : The AVA dataset is the primary benchmark for aesthetic evaluation . Since FLIVE has approximately 23 % overlap with images in the AVA dataset , we combine them for comparison . Unlike technical distortion , the assessment of image aesthetic quality 4 https : / / github . com / subpic / koniq JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 10 TABLE IV : Performance comparison on the 2AFC test set of BAPPS ( 2AFC score , higher is better ) . Note : the results “All” are simply calculated as the mean value of corresponding sub - terms same as [ 5 ] . Method Synthetic distortions Distortions by real algorithms All Traditional CNN - based All Super resolution Video deblurring Colorization Frame interpolation All Human 0 . 808 0 . 844 0 . 826 0 . 734 0 . 671 0 . 688 0 . 686 0 . 695 0 . 739 PSNR 0 . 573 0 . 801 0 . 687 0 . 642 0 . 590 0 . 624 0 . 543 0 . 600 0 . 629 SSIM 0 . 605 0 . 806 0 . 705 0 . 647 0 . 589 0 . 624 0 . 573 0 . 608 0 . 641 MS - SSIM 0 . 585 0 . 768 0 . 676 0 . 638 0 . 589 0 . 524 0 . 572 0 . 581 0 . 613 VSI 0 . 630 0 . 818 0 . 724 0 . 668 0 . 592 0 . 597 0 . 568 0 . 606 0 . 646 MAD 0 . 598 0 . 770 0 . 684 0 . 655 0 . 593 0 . 490 0 . 581 0 . 580 0 . 615 VIF 0 . 556 0 . 744 0 . 650 0 . 651 0 . 594 0 . 515 0 . 597 0 . 589 0 . 610 FSIMc 0 . 627 0 . 794 0 . 710 0 . 660 0 . 590 0 . 573 0 . 581 0 . 601 0 . 638 NLPD 0 . 550 0 . 764 0 . 657 0 . 655 0 . 584 0 . 528 0 . 552 0 . 580 0 . 606 GMSD 0 . 609 0 . 772 0 . 690 0 . 677 0 . 594 0 . 517 0 . 575 0 . 591 0 . 624 DeepIQA 0 . 703 0 . 794 0 . 748 0 . 660 0 . 582 0 . 585 0 . 598 0 . 606 0 . 654 PieAPP 0 . 725 0 . 769 0 . 747 0 . 685 0 . 582 0 . 594 0 . 598 0 . 615 0 . 659 LPIPS 0 . 760 0 . 828 0 . 794 0 . 705 0 . 605 0 . 625 0 . 630 0 . 641 0 . 692 DISTS 0 . 772 0 . 822 0 . 797 0 . 710 0 . 600 0 . 627 0 . 625 0 . 641 0 . 693 LPIPS + 0 . 756 0 . 833 0 . 795 0 . 706 0 . 606 0 . 630 0 . 631 0 . 643 0 . 694 TOPIQ ( ResNet50 ) 0 . 805 0 . 843 0 . 824 0 . 724 0 . 616 0 . 662 0 . 634 0 . 659 0 . 714 TABLE V : Quantitative comparison on NR benchmarks : CLIVE , KonIQ - 10k and FLIVE . CLIVE KonIQ - 10k FLIVE Methods PLCC SRCC PLCC SRCC PLCC SRCC DIIVINE [ 26 ] 0 . 591 0 . 588 0 . 558 0 . 546 0 . 186 0 . 092 BRISQUE [ 27 ] 0 . 629 0 . 629 0 . 685 0 . 681 0 . 341 0 . 303 NIQE [ 3 ] 0 . 493 0 . 451 0 . 389 0 . 377 0 . 211 0 . 288 ILNIQE [ 28 ] 0 . 508 0 . 508 0 . 537 0 . 523 0 . 332 0 . 294 PI [ 30 ] 0 . 521 0 . 462 0 . 488 0 . 457 0 . 334 0 . 170 PQR [ 36 ] 0 . 836 0 . 808 - - - - MEON [ 33 ] 0 . 710 0 . 697 0 . 628 0 . 611 0 . 394 0 . 365 WaDIQaM [ 18 ] 0 . 671 0 . 682 0 . 807 0 . 804 0 . 467 0 . 455 DBCNN [ 35 ] 0 . 869 0 . 869 0 . 884 0 . 875 0 . 551 0 . 545 HyperIQA [ 38 ] 0 . 882 0 . 859 0 . 917 0 . 906 0 . 602 0 . 544 MetaIQA [ 37 ] 0 . 802 0 . 835 0 . 856 0 . 887 0 . 507 0 . 540 TIQA [ 39 ] 0 . 861 0 . 845 0 . 903 0 . 892 0 . 581 0 . 541 TReS [ 40 ] 0 . 877 0 . 846 0 . 928 0 . 915 0 . 625 0 . 554 MUSIQ [ 45 ] - - 0 . 928 0 . 916 0 . 739 0 . 646 Ours ( ResNet50 ) 0 . 884 0 . 870 0 . 939 0 . 926 0 . 722 0 . 633 std ± 0 . 012 ± 0 . 014 ± 0 . 003 ± 0 . 003 - - TOPIQ ( Swin ) - - - - 0 . 745 0 . 652 pays more attention to the global feeling , where global seman - tics are more important than local textures . From the results in Tab . IX , we can observe that ThemeAware significantly improves the results by introducing extra theme labels , and KD achieves better results by distilling semantic knowledge from multiple classification backbones . Since the proposed CFANet is mainly designed to better extract local distortions , its perfor - mance is expected to be worse than methods with more power - ful classification backbones . However , CFANet with ResNet50 still achieves competitive results in both Tab . V and Tab . IX , indicating that CFANet still preserves global semantic infor - mation well . We suspect that the residual connections in SA and CSA blocks enable CFANet to adaptively fuse global and local information . Next , we replace the ResNet50 backbone in CFANet with a relatively cheaper transformer backbone , namely the Swin transformer [ 65 ] . From Tab . V and Tab . IX , we can observe that CFANet - Swin outperforms the previous state - of - the - art methods on both FLIVE and AVA . 4 ) Cross dataset experiments . : We also conducted cross - dataset experiments on NR benchmarks to establish the ro - bustness of our proposed method . Experiment setting . We used three NR datasets ( KonIQ - 10k , FLIVE , and SPAQ ) from Tab . I for training . The CLIVE dataset is only used for testing , as it is relatively small , and the AVA dataset is an aesthetic dataset , thus not applicable in this context . Regarding KonIQ - 10k and FLIVE , we utilized the official test split that contains approximately 2k and 7 . 3k images , respectively . Since SPAQ does not have an official split , we employed the entire dataset for testing , which con - tains approximately 11k images . Results . As demonstrated in Tab . VI , the proposed CFANet significantly outperforms other approaches . These results are consistent with the cross - dataset experiments on FR datasets in Tab . III , both of which highlight the advanced robustness and generalization capabilities of the proposed CFANet . V . A BLATION S TUDY AND B ACKBONE A NALYSIS In this section , we first present ablation experiments on the proposed components in CFANet , and then analyze the effects of different backbones on FR and NR tasks , respectively . 1 ) Ablation of the proposed components : In Tab . X , we evaluate the proposed components in CFANet with a cross - dataset experiment , similar to Tab . III , as it does not require random splits and leads to a more fair comparison . The base - line model is a simple linear regression network with multi - scale features after global average pooling , and each pro - posed component is added sequentially . All model variants are JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 11 TABLE VI : PLCC / SRCC scores of cross - dataset experiments with NR benchmarks . Train on KonIQ - 10k FLIVE SPAQ Test on CLIVE FLIVE SPAQ CLIVE KonIQ - 10k SPAQ CLIVE KonIQ - 10k FLIVE TReS 0 . 8118 / 0 . 7771 0 . 513 / 0 . 4919 0 . 8624 / 0 . 8619 0 . 7213 / 0 . 7336 0 . 7507 / 0 . 7068 0 . 6137 / 0 . 7269 – – – MUSIQ 0 . 8295 / 0 . 7889 0 . 5128 / 0 . 4978 0 . 8626 / 0 . 8676 0 . 8014 / 0 . 7672 0 . 7655 / 0 . 7084 0 . 8112 / 0 . 8436 0 . 8134 / 0 . 789 0 . 7528 / 0 . 6799 0 . 6039 / 0 . 5627 TOPIQ 0 . 8389 / 0 . 8206 0 . 6272 / 0 . 5796 0 . 8791 / 0 . 8758 0 . 8140 / 0 . 7868 0 . 8008 / 0 . 7622 0 . 812 / 0 . 8479 0 . 8327 / 0 . 8128 0 . 8112 / 0 . 7632 0 . 6154 / 0 . 5653 TABLE VII : Results on SPAQ dataset . Method PLCC SRCC DIIVINE [ 26 ] 0 . 600 0 . 599 BRISQUE [ 27 ] 0 . 817 0 . 809 ILNIQE [ 28 ] 0 . 721 0 . 713 PI [ 30 ] 0 . 724 0 . 709 Fang et al . [ 57 ] 0 . 909 0 . 908 DBCNN [ 35 ] 0 . 915 0 . 911 MUSIQ [ 45 ] 0 . 920 0 . 917 TOPIQ ( ResNet50 ) 0 . 924 0 . 921 std ± 0 . 002 ± 0 . 003 TABLE VIII : Results of KonIQ - 10k using official split . Method PLCC SRCC DIIVINE 0 . 612 0 . 589 BRISQUE 0 . 707 0 . 705 KonCept512 [ 24 ] 0 . 937 0 . 921 MUSIQ [ 45 ] 0 . 937 0 . 924 TOPIQ ( ResNet50 ) 0 . 941 0 . 928 trained on KADID - 10k and tested on CSIQ and TID2013 . We evaluate four components of CFANet : 1 ) Gated Local Pooling ( GLP ) ; 2 ) Self - Attention ( SA ) ; 3 ) Cross - scale Attention ( CSA ) and 4 ) Position embedding ( Pos . ) . We can observe that all four components are beneficial to the results . Specifically , the GLP and SA blocks slightly improve the baseline performance . The CSA block brings the most significant improvement , which proves the effectiveness of top - down semantic propagation . The Pos . also contributes slightly to the final performance . The full CFANet makes significant improvements to the baseline . 2 ) Ablation with different variants : To further validate the effectiveness of our architecture design , we conduct experi - ments of the following three variants of CFANet : • a ⃝ Replacing GLP with resize . • b ⃝ Replacing CSA with convolution fusion . • c ⃝ Directly using top - layer feature to guide lowest - layer . According to the results presented in Table X , we can make the following observations about the overall performance : a ⃝ > b ⃝ > c ⃝ . From this , we can draw the following conclusions : 1 ) the proposed GLP is slightly superior to resize since GLP can more accurately and selectively capture local distortion information ; 2 ) the proposed CSA outperforms convolution fusion , likely because the attention mechanism is more ef - fective in aggregating features from the entire image ; and 3 ) leveraging multi - scale semantic information is crucial for achieving optimal performance . These findings lend support to the effectiveness of the proposed modules . TABLE IX : Results on AVA dataset . ThemeAware † uses extra theme labels . Method Backbone PLCC SRCC NIMA [ 43 ] Inception - v2 0 . 636 0 . 612 PQR [ 36 ] ResNet101 0 . 720 0 . 719 Hosu et al . [ 61 ] Inception - v2 0 . 757 0 . 756 ThemeAware † [ 62 ] Inception - v2 0 . 775 0 . 774 MUSIQ [ 45 ] ViT - B / 32 0 . 726 0 . 738 KD [ 63 ] ResNeXt101 0 . 770 0 . 770 TOPIQ ResNet50 0 . 733 0 . 733 Swin 0 . 790 0 . 791 VGG19ResNet50 Swin 0 . 960 0 . 970 VGG19ResNet50 Swin 0 . 962 0 . 965 0 . 967 VGG19ResNet50 Swin 0 . 912 0 . 914 0 . 916 0 . 918 PLCC SRCC ( a ) LIVE ( b ) CSIQ ( c ) TID2013 Fig . 9 : Results of different backbones on FR benchmarks . VGG19ResNet50 Swin 0 . 920 0 . 940 0 . 960 VGG19ResNet50 Swin 0 . 650 0 . 700 0 . 750 VGG19ResNet50 Swin 0 . 700 0 . 750 PLCC SRCC ( a ) KonIQ - 10k ( b ) FLIVE ( c ) AVA Fig . 10 : Results of different backbones on NR benchmarks . 3 ) Performances with different backbones : In the previous experiments , we found that the backbone has a significant impact on the performance of aesthetic quality estimation . Therefore , we further evaluate how different backbones affect the performance on FR and NR benchmarks , respectively . We choose three representative backbones in our experiments , i . e . , VGG19 [ 66 ] , ResNet50 , and Swin transformer , and the results are shown in Fig . 9 and Fig . 10 . We can observe that stronger backbones generally give better performance in both FR and NR benchmarks . However , the improvement between CFANet - Swin and CFANet - ResNet50 is much larger on NR bench - marks ( + 0 . 02 ) than on FR benchmarks ( + 0 . 003 ) . We hypoth - esize that there are two main reasons : 1 ) the FR task relies more on the difference between distorted images and reference images , which is much easier to model , and simple ResNet50 is sufficient ; 2 ) without reference images , the NR task needs to evaluate the global aesthetic quality , and transformers are good at learning global representation . Despite the differences , we are surprised to find that CFANet - VGG already outperforms most previous approaches on several FR and NR benchmarks . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 12 TABLE X : Ablation study through cross dataset experiments for different components in CFANet . Experiments are done for both FR and NR datasets . ( PLCC SRCC ) scores are reported . Model Index ResNet50 GLP SA CSA Pos . KADID - 10k ( FR ) KonIQ - 10k ( NR ) CSIQ TID2013 CLIVE SPAQ 1 ⃝ ✓ 0 . 946 0 . 945 0 . 891 0 . 886 0 . 792 0 . 775 0 . 853 0 . 851 2 ⃝ ✓ ✓ 0 . 952 0 . 952 0 . 894 0 . 885 0 . 808 0 . 801 0 . 861 0 . 860 3 ⃝ ✓ ✓ ✓ 0 . 952 0 . 954 0 . 896 0 . 894 0 . 824 0 . 809 0 . 866 0 . 863 4 ⃝ ✓ ✓ ✓ ✓ 0 . 963 0 . 965 0 . 912 0 . 908 0 . 836 0 . 817 0 . 874 0 . 872 5 ⃝ ✓ ✓ ✓ ✓ ✓ 0 . 963 0 . 969 0 . 916 0 . 915 0 . 839 0 . 821 0 . 879 0 . 876 a ⃝ ✓ Resize ✓ ✓ ✓ 0 . 961 0 . 961 0 . 913 0 . 910 0 . 834 0 . 814 0 . 868 0 . 865 b ⃝ ✓ ✓ ✓ Convolution fusion ✓ 0 . 958 0 . 960 0 . 910 0 . 908 0 . 830 0 . 813 0 . 865 0 . 862 c ⃝ ✓ ✓ ✓ Top layer guidance ✓ 0 . 956 0 . 957 0 . 905 0 . 903 0 . 821 0 . 806 0 . 864 0 . 860 It proves the superiority of the proposed top - down framework to combine semantics with distortions in IQA . VI . C ONCLUSION In this work , we have proposed a top - down method , named as TOPIQ for image quality assessment . Drawing inspiration from our understanding of the global - to - local processes of HVS , we hypothesize that semantic information is critical in guiding the perception of local distortions . By extending the widely used LPIPS method with feature re - weighting , we have discovered that current bottom - up techniques fail to ex - ploit multi - scale features to their full potential as they neglect the importance of semantic guidance . To address this issue , we propose a heuristic top - down network , i . e . , the coarse - to - fine attention network ( CFANet ) , which effectively propagates multi - scale semantic information to low - level distortion fea - tures . The key element of CFANet is a novel cross - scale atten - tion ( CSA ) mechanism that utilizes high - level features to guide the selection of semantically significant low - level features . We have also devised a gated local pooling ( GLP ) block to im - prove the efficiency of CSA . Lastly , we have conducted com - prehensive experimental comparisons on various public bench - marks for both Full - Reference ( FR ) and No - Reference ( NR ) scenarios . Our proposed CFANet , with ResNet50 backbone , exhibits the best or highly competitive performance across all relevant benchmarks and is substantially more efficient than state - of - the - art approaches . R EFERENCES [ 1 ] N . Ponomarenko , O . Ieremeiev , V . Lukin , K . Egiazarian , L . Jin , J . Astola , B . Vozel , K . Chehdi , M . Carli , F . Battisti et al . , “Color image database tid2013 : Peculiarities and preliminary results , ” in European Workshop on Visional Information Processing ( EUVIP ) . IEEE , 2013 , pp . 106 – 111 . 1 , 6 , 8 [ 2 ] Z . Wang , E . P . Simoncelli , and A . C . Bovik , “Multiscale structural similarity for image quality assessment , ” in The Thrity - Seventh Asilomar Conferencerence on Signals , Systems & Computers , 2003 , vol . 2 . Ieee , 2003 , pp . 1398 – 1402 . 1 , 8 [ 3 ] A . Mittal , R . Soundararajan , and A . C . Bovik , “Making a “completely blind” image quality analyzer , ” IEEE Signal Processing Letters , vol . 20 , no . 3 , pp . 209 – 212 , 2012 . 1 , 3 , 10 [ 4 ] M . P . Sampat , Z . Wang , S . Gupta , A . C . Bovik , and M . K . Markey , “Complex wavelet structural similarity : A new image similarity index , ” IEEE Transactions on Image Processing , vol . 18 , no . 11 , pp . 2385 – 2401 , 2009 . 1 , 2 [ 5 ] R . Zhang , P . Isola , A . A . Efros , E . Shechtman , and O . Wang , “The unreasonable effectiveness of deep features as a perceptual metric , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2018 . 1 , 2 , 6 , 8 , 9 , 10 [ 6 ] K . Ding , K . Ma , S . Wang , and E . P . Simoncelli , “Image quality assess - ment : Unifying structure and texture similarity , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020 . 1 , 6 , 8 , 9 [ 7 ] J . Gu , H . Cai , C . Dong , J . S . Ren , R . Timofte , Y . Gong , S . Lao , S . Shi , J . Wang , S . Yang et al . , “Ntire 2022 challenge on perceptual image quality assessment , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2022 , pp . 951 – 967 . 2 [ 8 ] A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , Ł . Kaiser , and I . Polosukhin , “Attention is all you need , ” Advanced Neural Information Processing System , vol . 30 , 2017 . 2 , 4 , 9 [ 9 ] K . He , X . Zhang , S . Ren , and J . Sun , “Deep residual learning for image recognition , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2016 , pp . 770 – 778 . 2 [ 10 ] Z . Wang , A . C . Bovik , H . R . Sheikh , and E . P . Simoncelli , “Image quality assessment : from error visionbility to structural similarity , ” IEEE Transactions on Image Processing , vol . 13 , no . 4 , pp . 600 – 612 , 2004 . 2 , 8 [ 11 ] H . R . Sheikh and A . C . Bovik , “Image information and visional quality , ” IEEE Transactions on Image Processing , vol . 15 , no . 2 , pp . 430 – 444 , 2006 . 2 , 8 [ 12 ] E . C . Larson and D . M . Chandler , “Most apparent distortion : full - reference image quality assessment and the role of strategy , ” Journal of Electronic Imaging , vol . 19 , no . 1 , p . 011006 , 2010 . 2 , 8 [ 13 ] L . Zhang , L . Zhang , X . Mou , and D . Zhang , “Fsim : A feature similar - ity index for image quality assessment , ” IEEE Transactions on Image Processing , vol . 20 , no . 8 , pp . 2378 – 2386 , 2011 . 2 , 8 [ 14 ] W . Xue , L . Zhang , X . Mou , and A . C . Bovik , “Gradient magnitude similarity deviation : A highly efficient perceptual image quality index , ” IEEE Transactions on Image Processing , vol . 23 , no . 2 , pp . 684 – 695 , 2013 . 2 , 8 [ 15 ] L . Zhang , Y . Shen , and H . Li , “Vsi : A visional saliency - induced index for perceptual image quality assessment , ” IEEE Transactions on Image Processing , vol . 23 , no . 10 , pp . 4270 – 4281 , 2014 . 2 , 8 [ 16 ] V . Laparra , J . Ball´e , A . Berardino , and E . P . Simoncelli , “Perceptual image quality assessment using a normalized laplacian pyramid , ” Human Visionon and Electronic Imaging ( HVEI ) , pp . 43 – 48 , 2016 . 2 , 8 [ 17 ] J . Kim and S . Lee , “Deep learning of human visional sensitivity in image quality assessment framework , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2017 , pp . 1676 – 1684 . 2 , 8 [ 18 ] S . Bosse , D . Maniry , K . - R . M¨uller , T . Wiegand , and W . Samek , “Deep neural networks for no - reference and full - reference image quality as - sessment , ” IEEE Transactions on Image Processing , vol . 27 , no . 1 , pp . 206 – 219 , 2017 . 2 , 8 , 9 , 10 [ 19 ] E . Prashnani , H . Cai , Y . Mostofi , and P . Sen , “Pieapp : Perceptual image - error assessment through pairwise preference , ” in IEEE Conference on Computer Vision and Pattern Recognition , June 2018 . 2 , 6 , 8 , 9 [ 20 ] J . Gu , C . Haoming , C . Haoyu , Y . Xiaoxing , R . Jimmy , and D . Chao , “Pi - pal : a large - scale image quality assessment dataset for perceptual image restoration , ” in European Conference on Computer Vision . Springer International Publishing , 2020 , pp . 633 – 651 . 2 , 6 , 8 [ 21 ] J . Gu , H . Cai , C . Dong , J . S . Ren , Y . Qiao , S . Gu , and R . Timofte , “Ntire 2021 challenge on perceptual image quality assessment , ” in IEEE Conference on Computer Vision and Pattern Recognition Workshop , 2021 , pp . 677 – 690 . 2 [ 22 ] M . Cheon , S . - J . Yoon , B . Kang , and J . Lee , “Perceptual image quality assessment with transformers , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2021 , pp . 433 – 442 . 3 , 6 , 8 [ 23 ] S . Lao , Y . Gong , S . Shi , S . Yang , T . Wu , J . Wang , W . Xia , and Y . Yang , “Attentions help cnns see better : Attention - based hybrid image quality JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2021 13 assessment network , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2022 , pp . 1140 – 1149 . 3 , 6 , 8 , 9 [ 24 ] V . Hosu , H . Lin , T . Sziranyi , and D . Saupe , “Koniq - 10k : An ecologically valid database for deep learning of blind image quality assessment , ” IEEE Transactions on Image Processing , vol . 29 , pp . 4041 – 4056 , 2020 . 3 , 6 , 9 , 11 [ 25 ] N . Murray , L . Marchesotti , and F . Perronnin , “Ava : A large - scale database for aesthetic visional analysis , ” in IEEE Conference on Com - puter Vision and Pattern Recognition , 2012 , pp . 2408 – 2415 . 3 , 5 , 6 [ 26 ] A . K . Moorthy and A . C . Bovik , “Blind image quality assessment : From natural scene statistics to perceptual quality , ” IEEE Transactions on Image Processing , vol . 20 , no . 12 , pp . 3350 – 3364 , 2011 . 3 , 10 , 11 [ 27 ] A . Mittal , A . K . Moorthy , and A . C . Bovik , “No - reference image quality assessment in the spatial domain , ” IEEE Transactions on Image Pro - cessing , vol . 21 , no . 12 , pp . 4695 – 4708 , 2012 . 3 , 10 , 11 [ 28 ] L . Zhang , L . Zhang , and A . C . Bovik , “A feature - enriched completely blind image quality evaluator , ” IEEE Transactions on Image Processing , vol . 24 , no . 8 , pp . 2579 – 2591 , 2015 . 3 , 10 , 11 [ 29 ] C . Ma , C . - Y . Yang , X . Yang , and M . - H . Yang , “Learning a no - reference quality metric for single - image super - resolution , ” Computer Vision and Image Understanding , vol . 158 , pp . 1 – 16 , 2017 . 3 [ 30 ] Y . Blau , R . Mechrez , R . Timofte , T . Michaeli , and L . Zelnik - Manor , “The 2018 pirm challenge on perceptual image super - resolution , ” in European Conference on Computer Vision Workshop , 2018 , pp . 0 – 0 . 3 , 10 , 11 [ 31 ] W . Hou , X . Gao , D . Tao , and X . Li , “Blind image quality assessment via deep learning , ” IEEE Transactions on Neural Networks and Learning Systems , vol . 26 , no . 6 , pp . 1275 – 1286 , 2014 . 3 [ 32 ] L . Kang , P . Ye , Y . Li , and D . Doermann , “Convolutional neural networks for no - reference image quality assessment , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2014 , pp . 1733 – 1740 . 3 [ 33 ] K . Ma , W . Liu , K . Zhang , Z . Duanmu , Z . Wang , and W . Zuo , “End - to - end blind image quality assessment using deep neural networks , ” IEEE Transactions on Image Processing , vol . 27 , no . 3 , pp . 1202 – 1213 , 2017 . 3 , 10 [ 34 ] K . - Y . Lin and G . Wang , “Hallucinated - iqa : No - reference image quality assessment via adversarial learning , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2018 . 3 [ 35 ] W . Zhang , K . Ma , J . Yan , D . Deng , and Z . Wang , “Blind image quality assessment using a deep bilinear convolutional neural network , ” IEEE Transactions on Circuits and Systems for Video Technology , vol . 30 , no . 1 , pp . 36 – 47 , 2020 . 3 , 6 , 10 , 11 [ 36 ] H . Zeng , L . Zhang , and A . C . Bovik , “Blind image quality assessment with a probabilistic quality representation , ” in IEEE International Con - ference on Image Processing . IEEE , 2018 , pp . 609 – 613 . 3 , 10 , 11 [ 37 ] H . Zhu , L . Li , J . Wu , W . Dong , and G . Shi , “Metaiqa : Deep meta - learning for no - reference image quality assessment , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2020 , pp . 14143 – 14152 . 3 , 9 , 10 [ 38 ] S . Su , Q . Yan , Y . Zhu , C . Zhang , X . Ge , J . Sun , and Y . Zhang , “Blindly assess image quality in the wild guided by a self - adaptive hyper network , ” in IEEE Conference on Computer Vision and Pattern Recognition , June 2020 . 3 , 10 [ 39 ] J . You and J . Korhonen , “Transformer for image quality assessment , ” in IEEE International Conference on Image Processing . IEEE , 2021 , pp . 1389 – 1393 . 3 , 10 [ 40 ] S . A . Golestaneh , S . Dadsetan , and K . M . Kitani , “No - reference im - age quality assessment via transformers , relative ranking , and self - consistency , ” in IEEE / CVF Winter Conference on Applications of Com - puter Vision , 2022 , pp . 1220 – 1230 . 3 , 5 , 10 [ 41 ] S . Yang , T . Wu , S . Shi , S . Lao , Y . Gong , M . Cao , J . Wang , and Y . Yang , “Maniqa : Multi - dimension attention network for no - reference image quality assessment , ” in IEEE Conference on Computer Vision and Pattern Recognition Workshop , 2022 , pp . 1191 – 1200 . 3 [ 42 ] X . Liu , J . Van De Weijer , and A . D . Bagdanov , “Rankiqa : Learning from rankings for no - reference image quality assessment , ” in International Conference on Computer Vision , 2017 , pp . 1040 – 1049 . 3 [ 43 ] H . Talebi and P . Milanfar , “Nima : Neural image assessment , ” IEEE Transactions on Image Processing , vol . 27 , no . 8 , pp . 3998 – 4011 , 2018 . 3 , 5 , 11 [ 44 ] H . Zheng , J . Fu , Y . Zeng , Z . - J . Zha , and J . Luo , “Learning conditional knowledge distillation for degraded - reference image quality assessment , ” International Conference on Computer Vision , 2021 . 3 [ 45 ] J . Ke , Q . Wang , Y . Wang , P . Milanfar , and F . Yang , “Musiq : Multi - scale image quality transformer , ” in International Conference on Computer Vision , 2021 , pp . 5148 – 5157 . 3 , 5 , 9 , 10 , 11 [ 46 ] B . Hu , L . Li , J . Wu , and J . Qian , “Subjective and objective quality assessment for image restoration : A critical survey , ” Signal Processing : Image Communication , vol . 85 , p . 115839 , 2020 . 3 [ 47 ] B . Hu , L . Li , H . Liu , W . Lin , and J . Qian , “Pairwise - comparison - based rank learning for benchmarking image restoration algorithms , ” IEEE Transactions on Multimedia , vol . 21 , no . 8 , pp . 2042 – 2056 , 2019 . 3 [ 48 ] B . Hu , S . Wang , L . Li , J . Leng , Y . Yang , and X . Gao , “Hierarchical discrepancy learning for image restoration quality assessment , ” Signal Processing , vol . 198 , p . 108595 , 2022 . 3 [ 49 ] Y . Zheng , D . Huang , S . Liu , and Y . Wang , “Cross - domain object detec - tion through coarse - to - fine feature adaptation , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2020 , pp . 13766 – 13775 . 4 [ 50 ] L . Jing , Y . Chen , and Y . Tian , “Coarse - to - fine semantic segmentation from image - level labels , ” IEEE Transactions on Image Processing , vol . 29 , pp . 225 – 236 , 2019 . 4 [ 51 ] J . Yu , Z . Lin , J . Yang , X . Shen , X . Lu , and T . S . Huang , “Free - form im - age inpainting with gated convolution , ” IEEE Conference on Computer Vision and Pattern Recognition , pp . 4471 – 4480 , 2019 . 4 [ 52 ] R . A . Bradley and M . E . Terry , “Rank analysis of incomplete block designs : I . the method of paired comparisons , ” Biometrika , vol . 39 , no . 3 / 4 , pp . 324 – 345 , 1952 . 6 [ 53 ] H . R . Sheikh , M . F . Sabir , and A . C . Bovik , “A statistical evaluation of recent full reference image quality assessment algorithms , ” IEEE Trans - actions on Image Processing , vol . 15 , no . 11 , pp . 3440 – 3451 , 2006 . 6 , 8 [ 54 ] E . C . Larson and D . M . Chandler , “Most apparent distortion : full - reference image quality assessment and the role of strategy , ” Journal of Electronic Imaging , vol . 19 , no . 1 , p . 011006 , 2010 . 6 , 8 [ 55 ] H . Lin , V . Hosu , and D . Saupe , “Kadid - 10k : A large - scale artificially distorted iqa database , ” in 2019 Tenth International Conferencerence on Quality of Multimedia Experience ( QoMEX ) . IEEE , 2019 , pp . 1 – 3 . 6 [ 56 ] D . Ghadiyaram and A . C . Bovik , “Massive online crowdsourced study of subjective and objective picture quality , ” IEEE Transactions on Image Processing , vol . 25 , no . 1 , pp . 372 – 387 , 2015 . 6 [ 57 ] Y . Fang , H . Zhu , Y . Zeng , K . Ma , and Z . Wang , “Perceptual quality assessment of smartphone photography , ” in IEEE Conference on Com - puter Vision and Pattern Recognition , 2020 , pp . 3677 – 3686 . 6 , 11 [ 58 ] Z . Ying , H . Niu , P . Gupta , D . Mahajan , D . Ghadiyaram , and A . Bovik , “From patches to pictures ( paq - 2 - piq ) : Mapping the perceptual space of picture quality , ” IEEE Conference on Computer Vision and Pattern Recognition , pp . 3575 – 3585 , 2020 . 6 [ 59 ] J . Deng , W . Dong , R . Socher , L . - J . Li , K . Li , and L . Fei - Fei , “Imagenet : A large - scale hierarchical image database , ” in IEEE Conference on Computer Vision and Pattern Recognition , 2009 , pp . 248 – 255 . 6 [ 60 ] S . Seo , S . Ki , and M . Kim , “A novel just - noticeable - difference - based saliency - channel attention residual network for full - reference image quality predictions , ” IEEE Transactions on Circuits and Systems for Video Technology , vol . 31 , no . 7 , pp . 2602 – 2616 , 2020 . 8 [ 61 ] V . Hosu , B . Goldlucke , and D . Saupe , “Effective aesthetics prediction with multi - level spatially pooled features , ” in IEEE Conference on Com - puter Vision and Pattern Recognition , 2019 , pp . 9375 – 9383 . 11 [ 62 ] G . Jia , P . Li , and R . He , “Theme - aware aesthetic distribution prediction with full - resolution photographs , ” IEEE Transactions on Neural Net - works and Learning Systems , 2022 . 11 [ 63 ] J . Hou , H . Ding , W . Lin , W . Liu , and Y . Fang , “Distilling knowledge from object classification to aesthetics assessment , ” IEEE Transactions on Circuits and Systems for Video Technology , vol . 32 , no . 11 , pp . 7386 – 7402 , 2022 . 11 [ 64 ] C . Szegedy , S . Ioffe , V . Vanhoucke , and A . A . Alemi , “Inception - v4 , inception - resnet and the impact of residual connections on learning , ” in Association for the Advancement of Artificial Intelligence , 2017 . 9 [ 65 ] Z . Liu , Y . Lin , Y . Cao , H . Hu , Y . Wei , Z . Zhang , S . Lin , and B . Guo , “Swin transformer : Hierarchical visionon transformer using shifted win - dows , ” in International Conference on Computer Vision , 2021 , pp . 10012 – 10022 . 10 [ 66 ] K . Simonyan and A . Zisserman , “Very deep convolutional networks for large - scale image recognition , ” in International Conference on Learning Representations , 2015 . 11