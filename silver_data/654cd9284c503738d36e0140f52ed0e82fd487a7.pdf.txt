Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality Doris Aschenbrenner Jonas S . I . Rieder Dani¨elle van Tol Joris van Dam and Zoltan Rusak Industrial Design Engineering TU Delft , Delft , Netherlands Email : d . aschenbrenner @ tudelft . nl Jan Olaf Blech Mohammad Azangoo Salo Panu Electrical Engineering and Automation Aalto University , Espoo , Finland Email : jan . blech @ aalto . ﬁ Karl Kruusam¨ae Houman Masnavi Igor Rybalskii and Alvo Aabloo Intelligent Materials and Systems Lab University Tartu , Tartu , Estonia Email : karl . kruusamae @ ut . ee Marcelo Petry and Gustavo Teixeira CRIIS , INESC TEC - Institute for Systems and Computer Engineering , Technology and Science Porto , Portugal Email : marcelo . petry @ inesctec . pt Bastian Thiede TU Braunschweig Braunschweig , Germany Email : b . thiede @ tu - braunschweig . de Paolo Pedrazzoli Andrea Ferrario Michele Foletti Matteo Confalonieri Daniele Bertaggia SUPSI , Manno , Switzerland Email : paolo . pedrazzoli @ supsi . ch Thodoris Togias and Sotiris Makris LMS , University of Patras , Patras , Greece Email : togias @ lms . mech . upatras . gr Abstract —How to visualize recorded production data in Virtual Reality ? How to use state of the art Augmented Reality displays that can show robot data ? This paper introduces an open - source ICT framework approach for combining Unity - based Mixed Reality applications with robotic production equipment using ROS Industrial . This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientiﬁc exchange within the area of Mixed Reality enabled human - robot co - production . 1 . Introduction There are many studies [ 1 ] [ 2 ] , that show that the future productivity within the manufacturing industry ( as envisioned by the Industry 4 . 0 paradigm ) will depend on humans working alongside intelligent machines and robots in the factories . There is a lot of new technology , which is introduced to the production ﬂoor : collaborative robots ( cobots ) for close human - robot interaction , new interface technologies like Augmented and Virtual Reality , and new applications based on data and artiﬁcial intelligence . If we want to achieve sustainable future production workplaces , the need for designing an ”augmentation layer” for the worker arises , which helps the human worker to deal with the increasing complexity and self - organization of the cyber - physical production system ( CPPS ) . This could be done by visually facilitating the vast amount of sensor data and information . Emerging human - robot interaction techniques like Aug - mented and Virtual Reality will play an increasing role , among others in plant planning , plant supervision , close col - laboration with robots , and plant optimization . Nevertheless , combining AR / VR with existing robotics infrastructure is quite challenging , because it requires knowledge from both domains . The development of a bridging ICT framework is the precondition for being able to conduct studies on how the displayed information could support human situation awareness , sense - and decision making , and the sharing of mental models between humans and machine . To solve this problem , research needs to be carried out on the entire work socio - technical system , both sides : the AR / VR research and the manufacturing robotics researchers need to team up to provide sustainable solutions . Existing work covers mainly one part of this , which is certainly due to the general problem of real cross - disciplinary research , but also because the tools are not easily available . There is a lot of work on applying AR / VR to manufacturing , for example , AR - based repair instructions (cid:21)(cid:20) (cid:19)(cid:17)(cid:19)(cid:17)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:42)(cid:79)(cid:85)(cid:70)(cid:83)(cid:79)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79)(cid:66)(cid:77)(cid:1)(cid:36)(cid:80)(cid:79)(cid:71)(cid:70)(cid:83)(cid:70)(cid:79)(cid:68)(cid:70)(cid:1)(cid:80)(cid:79)(cid:1)(cid:34)(cid:83)(cid:85)(cid:74)(cid:71)(cid:74)(cid:68)(cid:74)(cid:66)(cid:77)(cid:1)(cid:42)(cid:79)(cid:85)(cid:70)(cid:77)(cid:77)(cid:74)(cid:72)(cid:70)(cid:79)(cid:68)(cid:70)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:55)(cid:74)(cid:83)(cid:85)(cid:86)(cid:66)(cid:77)(cid:1)(cid:51)(cid:70)(cid:66)(cid:77)(cid:74)(cid:85)(cid:90)(cid:1)(cid:9)(cid:34)(cid:42)(cid:55)(cid:51)(cid:10) (cid:26)(cid:24)(cid:25)(cid:14)(cid:18)(cid:14)(cid:24)(cid:19)(cid:25)(cid:18)(cid:14)(cid:24)(cid:21)(cid:23)(cid:20)(cid:14)(cid:18)(cid:16)(cid:19)(cid:17)(cid:16)(cid:5)(cid:20)(cid:18)(cid:15)(cid:17)(cid:17)(cid:1)(cid:165)(cid:19)(cid:17)(cid:19)(cid:17)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:34)(cid:42)(cid:55)(cid:51)(cid:22)(cid:17)(cid:23)(cid:18)(cid:25)(cid:15)(cid:19)(cid:17)(cid:19)(cid:17)(cid:15)(cid:17)(cid:17)(cid:17)(cid:18)(cid:24) 2020 I EEE I n t e r n a ti on a l C on f e r e n c e on A r ti f i c i a l I n t e lli g e n ce a nd V i r t u a l R ea lit y ( A I V R ) | 978 - 1 - 7281 - 7463 - 1 / 20 / $ 31 . 00 © 2020 I EEE | DO I : 10 . 1109 / A I V R 50618 . 2020 . 00017 Figure 1 . VR application case for data analysis Figure 2 . ICT architecture for data display (cid:21)(cid:21) [ 3 ] , AR - based picking instructions [ 4 ] , displaying additional information within a factory [ 5 ] , or virtual product in - spection stations based on VR - goggles [ 6 ] . Additionally , there is work on VR - based collaboration [ 7 ] , the use of exoskeletons within manufacturing tasks [ 8 ] , and factory planning [ 9 ] . On the side of the robotic research , there are tools for developing immersion - based situational aware - ness ( including virtual reality user experience ) for robots supported by the robot operation system ( ROS ) using the native RViz visualization [ 10 ] , this technology does not enable communication with the main frameworks used for AR and VR development : Unity and Unreal . Although such bridging solutions exist already – for example , a middleware approach for teleoperation [ 11 ] or over - the - Internet interface for teleportation [ 12 ] – their usage within the state of the art Mixed Reality devices is not entirely comprehensible and easy to set up . This concerns mainly head - mounted displays , especially the Microsoft Hololens which is based on the Universal Windows Platform . The aim of the EIT Manufacturing project “Mirrorlabs” is the development of an integrated , easy - to - use ICT in - frastructure for the off - the - shelf Mixed Reality hardware in combination with ROS - driven ( Robot Operation System ) production systems and to make it available for education and research . 2 . Use cases The possible use cases of this framework range from a VR - enabled remote analysis of robot - operated production line datasets ( as displayed in Figure 1 ) to the use of Aug - mented Reality for close collaboration between humans and robots on the production ﬂoor . In this section the different use cases of the Framework are discussed and the existing use cases are displayed . 2 . 1 . User task description 2 . 1 . 1 . Inspection use case . In Figure 1 the use case for data visualisation is displayed . The application context is a visual abrasion check for industrial robot axes . As industrial robots can run for 24 hours a day and 7 days a week , the inspection targets at identifying the amount of abrasion compared to the initial setup of the robot . In order to realize this scenario , the robot needs to record process data directly after the initial setup ( without abrasion effects ) . This dataset is stored . After a longer time of active robot work , another dataset is recorded . With the help of the framework , it can be displayed on top of the initially gathered dataset . The visual inspection can be either realized within Virtual Reality or as an overlay of the real robot with the help of an Augmented Reality head mounted display . 2 . 1 . 2 . Teaching use case . The usage of a Virtual Machine robot for a VR application ( Figure 5 ) is necessary for each recorded or simulated VR environment , in which a robot is used . The robot simulator runs in a ROS VM and the movement is displayed in Unity . A very interesting application case is also to use Virtual reality for a dual arm robot position teaching ( Figure 6 and 7 ) : The user can move the 6DOF visualization marker and record the positions for a simulated program . 2 . 1 . 3 . Direct interaction use case . Possible application cases encompass the usage of a head mounted display for showing the movement of the robot ( in Figure 4 and 9 ) . Here either measured data could be displayed at the corresponding space ( for example temperature at a work piece near the end effector ) . Or the virtual representation can be used to show past and future movement of the robot or display the work envelope as a safety feature . 2 . 1 . 4 . Factory planning use case . Additional use cases can be explored with the creation of a “mini factory” visual - ization ( Figure 8 ) , for example for remote supervision and support or factory planning . 2 . 2 . Interaction design requirements Figure 3 . Abstract tasks within human - robot co - production following [ 13 ] In general , the tasks conducted by a human operator within the the ”human supervisory control concept” [ 13 ] are displayed in Figure 3 : Plan , Teach , Monitor , Intervene , Learn . The interface between the robot and the human ( in this context realized with the help of Augmented and Virtual Reality ) serves the following purposes : • Ensuring that the user knows ”what is going on” . This is described in the Situation Awareness concept [ 14 ] in three different Levels Perception , Compre - hension , and Projection . • Helping the user with sense - making [ 15 ] and decision - making [ 16 ] . • Helping to share the mental model between the human and the robot [ 17 ] . 3 . Implementation overview In Figure 2 the concept of the Mirrorlabs architecture is displayed : A real robot is running in normal operation within a smart factory ( testbed ) . The movement and sensor (cid:21)(cid:22) data are made accessible through the ROS platform and stored into a database . The so - called “snapshot” can be loaded into a ROS virtual machine . MirrorLabs has extended existing ROS with Unity bridge software and tested it with different robots and Mixed Reality hardware . After a VR or AR application has been built using Unity , a user can watch the recorded snapshot visualised using a virtual robot within Virtual Reality . The software and tutorials are available under / url - http : / / mirrorlabs . eu / and will be further maintained and ex - tended . Currently , the framework has been tested for the following supported hardware : Robots : Universal Robots , Franka Emika Panda , Doosan , MiR100 . Mixed Reality De - vices : Hololens 1 and 2 , HTC Vive , Oculus Quest 1 and Rift . 3 . 1 . Unity3D Unity is a game engine freely available for non - commercial usage ( paid for commercial ) . It uses a combina - tion of scripts ( written in C # ) and since the ﬁrst release of Microsoft HoloLens 1 , it is the ofﬁcially recommended and until the release of HoloLens 2 the only supported develop - ment platform . Furthermore , it is the primary development platform used for most available VR applications . Leading to a broad variety of open - source projects , tutorials and a vibrant online community of developers . The platform used for AR / VR is Unity version 2019 . 4 . 3 . 2 . Robot Operating System ROS With the variety of industrial robots commercially avail - able , the transition between robotic installations executing a similar task may require unnecessary time for engineers to rewrite and adapt existing programs . To simplify this transition process , robots can operated using ROS . ROS itself enables engineers to write protocols in Python or C + + once and execute them on a broad variety of robots with sig - niﬁcantly reduced transition times . To enable these reduced transition times , ROS uses standardized communication be - tween its core algorithm and secondary algorithms operating the physical robots which only need to be developed once . For many robots these secondary algorithms are available online . Currently Ubuntu with Melodic v18 . 04 is supported . 3 . 3 . AR and VR Devices To utilize the vision of MirrorLabs , a variety of Aug - mented and Virtual Reality devices will be deployed , which are either standalone , wireless or cable bound . Both types of devices alter the vision of a user by either replacing the perceived image with an artiﬁcial one or altering the a users perception with additional augmented content . Some commonly used devices are HTC Vive , Oculus Rift and Oculus Quest for VR and Microsoft HoloLens 1 and 2 for AR . 3 . 4 . ROS2Unity bridge The framework consists of a Unity - Ros - Bridge ( using ROS # and Rviz2AR ) and aims to establish a generic data - communication bridge between ROS robot hardware and virtual robots in Unity3D ( e . g . UR5 robot - arm or COMAU dual - arm manipulator ) , enabling visualization in Unity of ROS controlled robots , as well as controlling robots from Unity3d by virtual interactions . 3 . 5 . Unity - ROS communication workﬂow example In general , the MirrorLabs framework follows the sys - tematic presented in above ﬁgure . Commands are initi - ated within a Unity - made application ( HoloLens 2 , Oculus Quest or Desktop ) and forwarded using ROS # . The ROS environment then processes the command accordingly and publishes the resultant information ( joint states , odom , etc . ) which is received by the unity application . Hence , within the unity application no robot speciﬁc calculations are made . Furthermore , from ROS the commands can be forwarded to a physical robot . As a result , limited computational resources available on most Head - Mounted - Devices [ HMD ] is sufﬁcient since no “heavy” calculations are executed within Unity . To simplify the setup and usage of the ROS interaction within the unity - application a Graphical - User - Interface [ GUI ] is implemented . Currently , these have 2 dis - tinct functions : First , the setup of the ROS # bridge ( remote IP address and port of the ROS environment ) and how to use it ( ROS Mode ) . Second , the “RobotControl” panel , allowing a user publish control signals including predeﬁned postures for the robot . 4 . Summary This is a preliminar introduction into the possibilities of the discussed ICT framework . There had been work speciﬁc usability requirements , remote collaboration and teleoper - ation among the different labs , which will be published separately . We hope to encourage the reader to try our framework and might even join the MirrorLabs team with deploying the software in their own labs . Especially the use of Virtual and Augmented Reality within a production environment is an opportunity for stu - dent projects – it is at the same time intuitive , motivating , and creates learning beneﬁt . The generation of multiple Mixed Reality visualizations of CPPS with student projects can be both an educational but also a research beneﬁt if we regard the possibility to conduct user studies with the developed applications to deduct more general principles for human - machine interaction within industry 4 . 0 . As nu - merous research institutions currently have a similar set of hardware ( for example UR5 , Hololens , HTC Vive , . . . ) there is an opportunity to create comparable results and to share algorithms – both for education and research . Furthermore , there is a beneﬁt of being able to share snapshots of existing smart factory constellations and to “visit each other” remotely ( especially useful for pandemic (cid:21)(cid:23) times ) . The achievement of this goal is only possible with the creation of a “digital twin mirror lab” . In the long run , the platform should enable to share algorithms and data recordings , so that also remote maintenance scenarios can be explored between different research institutions . As an additional beneﬁt , the framework could help to initiate a smart factory research environment . Small and medium - sized enterprises or research institutes that start with this ﬁeld of research could use the tutorials to set up supported hardware and deploy the software . This enables them to build an “analog twin mirror lab” of an already existing research lab constellation . Acknowledgments The authors would like to thank the funding of EIT Manufacturing for the project ”MirrorLabs” that made this research possible . The developed software is released open - source so that it can be used by other partners within and outside EIT Manufacturing . The project develops a software platform and tutorials on how to set up the platform and how to get started to work with it : http : / / mirrorlabs . eu / References [ 1 ] M . Lorenz , M . R¨ußmann , R . Strack , K . L . Lueth , and M . Bolle , “Man and machine in industry 4 . 0 : How will technology transform the industrial workforce through 2025 , ” The Boston Consulting Group , vol . 2 , 2015 . [ 2 ] T . Ludwig , C . Kotthaus , M . Stein , V . Pipek , and V . Wulf , “Revive old discussions ! socio - technical challenges for small and medium enterprises within industry 4 . 0 , ” in Proceedings of 16th European Conference on Computer - Supported Cooperative Work - Exploratory Papers . European Society for Socially Embedded Technologies ( EUSSET ) , 2018 . [ 3 ] D . Aschenbrenner , F . Leutert , A . C¸enc¸en , J . Verlinden , K . Schilling , M . Latoschik , and S . Lukosch , “Comparing human factors for aug - mented reality supported single - user and collaborative repair opera - tions of industrial robots , ” Frontiers in Robotics and AI , vol . 6 , p . 37 , 2019 . [ 4 ] V . Paelke , “Augmented reality in the smart factory : Supporting work - ers in an industry 4 . 0 . environment , ” in Proceedings of the 2014 IEEE emerging technology and factory automation ( ETFA ) . IEEE , 2014 , pp . 1 – 4 . [ 5 ] I . D . Peake , J . O . Blech , and M . Schembri , “A software framework for augmented reality - based support of industrial operations , ” in 2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation ( ETFA ) , 2016 , pp . 1 – 4 . [ 6 ] M . Azangoo , J . O . Blech , U . D . Atmojo , V . Vyatkin , K . Dhakal , M . Eriksson , M . Lehtim¨aki , J . Leinola , and P . Pietarila , “Towards a 3d scanning / vr - based product inspection station , ” in the 25th IEEE International Conference on Emerging Technologies and Factory Automation ( ETFA 2020 ) , in press . [ 7 ] I . D . Peake , J . O . Blech , E . Watkins , S . Greuter , and H . W . Schmidt , “The virtual experiences portals — a reconﬁgurable platform for immersive visualization , ” in Augmented Reality , Virtual Reality , and Computer Graphics , L . T . De Paolis and A . Mongelli , Eds . Cham : Springer International Publishing , 2016 , pp . 186 – 197 . [ 8 ] A . Karvouniari , G . Michalos , N . Dimitropoulos , and S . Makris , “An approach for exoskeleton integration in manufacturing lines using virtual reality techniques , ” Procedia CIRP , vol . 78 , pp . 103 – 108 , 2018 . [ 9 ] D . Aschenbrenner , M . Li , R . Dukalski , J . Verlinden , and S . Lukosch , “Collaborative production line planning with augmented fabrication , ” in 2018 IEEE Conference on Virtual Reality and 3D User Interfaces ( VR ) . IEEE , 2018 , pp . 509 – 510 . [ 10 ] V . Vunder , R . Valner , C . McMahon , K . Kruusam¨ae , and M . Pryor , “Improved situational awareness in ros using panospheric vision and virtual reality , ” in 2018 11th International Conference on Human System Interaction ( HSI ) , 2018 , pp . 471 – 477 . [ 11 ] R . Codd - Downey , P . M . Forooshani , A . Speers , H . Wang , and M . Jenkin , “From ros to unity : Leveraging robot and virtual en - vironment middleware for immersive teleoperation , ” in 2014 IEEE International Conference on Information and Automation ( ICIA ) , 2014 , pp . 932 – 936 . [ 12 ] D . Whitney , E . Rosen , D . Ullman , E . Phillips , and S . Tellex , “Ros reality : A virtual reality framework using consumer - grade hardware for ros - enabled robots , ” in 2018 IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) , 2018 , pp . 1 – 9 . [ 13 ] T . B . Sheridan , Telerobotics , automation , and human supervisory control . MIT press , 1992 . [ 14 ] M . R . Endsley , “Measurement of situation awareness in dynamic systems , ” Human factors , vol . 37 , no . 1 , pp . 65 – 84 , 1995 . [ 15 ] D . M . Russell , M . J . Steﬁk , P . Pirolli , and S . K . Card , “The cost structure of sensemaking , ” in Proceedings of the INTERACT’93 and CHI’93 conference on Human factors in computing systems , 1993 , pp . 269 – 276 . [ 16 ] P . Zhang and D . Soergel , “Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking , ” Journal of the Association for Information Science and Technology , vol . 65 , no . 9 , pp . 1733 – 1756 , 2014 . [ 17 ] C . Borst , “Shared mental models in human - machine systems , ” IFAC - PapersOnLine , vol . 49 , no . 19 , pp . 195 – 200 , 2016 . (cid:21)(cid:24) Figure 4 . INESC - TEC application with UR10 Figure 5 . Virtual Machine Robot Figure 6 . LMS dual arm robot Figure 7 . LMS virtual teach - in Figure 8 . TU Delft application for Mini - robot desktop overview Figure 9 . TU Delft SAM XL application with Doosan Robot (cid:21)(cid:25)