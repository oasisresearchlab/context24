A Taxonomy of Metrics for Software Fault Prediction Maria Caulo maria . caulo @ unibas . it University of Basilicata Potenza , Italy ABSTRACT In the field of Software Fault Prediction ( SFP ) , researchers exploit software metrics to build predictive models using machine learning and / or statistical techniques . SFP has existed for several decades and the number of metrics used has increased dramatically . Thus , the need for a taxonomy of metrics for SFP arises firstly to stan - dardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systemat - ically classify the used metrics . In this doctoral symposium paper , I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible , but also to provide a global understanding of the metrics for SFP in terms of detailed informa - tion : acronym ( s ) , extended name , univocal description , granularity of the fault prediction ( e . g . , method and class ) , category , and re - search papers in which they were used . CCS CONCEPTS • General and reference → Surveys and overviews ; Metrics ; • Applied computing → Enterprise ontologies , taxonomies and vocabularies . KEYWORDS software metrics , software fault prediction , taxonomy ACM Reference Format : Maria Caulo . 2019 . A Taxonomy of Metrics for Software Fault Prediction . In Proceedings of the 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering ( ESEC / FSE ’19 ) , August 26 – 30 , 2019 , Tallinn , Estonia . ACM , New York , NY , USA , 4 pages . https : / / doi . org / 10 . 1145 / 3338906 . 3341462 1 RESEARCH PROBLEM AND HYPOTHESIS Software Fault Prediction ( SFP ) techniques aim at identifying fault - prone code components to help developers to better distribute their ( i ) testing efforts , since components predicted as fault - prone are more likely to require testing attention and ( ii ) refactoring efforts , with the goal of improving the design of such components , to mini - mize the possibilities of introducing new bugs while working on them ( e . g . , it is well - known that components affected by code smells exhibit a higher fault - proneness [ 11 ] ) . The research community has Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . ESEC / FSE ’19 , August 26 – 30 , 2019 , Tallinn , Estonia © 2019 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 5572 - 8 / 19 / 08 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3338906 . 3341462 invested substantial effort in the last decades to develop SFP tech - niques . The basic idea behind these techniques is to exploit a set of metrics describing different aspects of code components ( to be used as independent variables ) to predict the presence or the number of faults ( dependent variables ) in such components . Examples of used independent variables are product metrics ( i . e . , those captur - ing intrinsic characteristics of the code components , like their size and complexity ) and process metrics ( i . e . , metrics capturing specific aspects of the development process , such as the number of develop - ers modifying a component in a given time ) . Clearly , the selection of these metrics represents one of the main factors impacting the accuracy of the prediction . Given the popularity of SFP , the number of different metrics used to build SFP models is increasing every year and a complete overview of all the existing metrics lacks . A taxonomy typically allows advancing knowledge by : ( i ) providing a set of unifying constructs , thus defining a common terminology for communication purposes ; ( ii ) identifying relationship schemes , thus understanding interrelationships among objects and their cat - egories ; and ( iii ) identifying knowledge gaps and weaknesses in the prior research , that can be addressed in future research ( e . g . , to discover why a metric has been used in the past and not in recent re - search ) . A taxonomy provides an organized structure of knowledge items , hence it might represent a point of reference for researchers and practitioners . During my Ph . D . course , my advisor and I have defined a long - term plan to systematically build a taxonomy of metrics for SFP so that it might represent the starting point for future research in SFP . Details about that taxonomy and its building process will be also shared with the community so that external researchers can increase their awareness with that taxonomy and possibly propose changes . That is , we plan to support a mechanism pull - requests - like to let the software engineering community modify / evolve the taxonomy throughout the years . 2 RESEARCH METHOD In Figure 1 , the process followed to create the taxonomy is de - picted . It is defined in terms of an activity diagram with object flow . Rounded rectangles represent the phases of the process , while rectangles the intermediate artifacts the phases produce / consume . The phases of the process are detailed in the following subsections . 2 . 1 Searching for a Taxonomy In this phase , a systematic approach guided the search for tax - onomies in SFP . In particular , the following search string has been used : software AND metric AND fault AND prediction AND taxonomy . Such string was executed on Scopus , IEEE Xplore , and ACM Digital Library . Choosing such digital libraries is accustomed when conducting studies in Software Engineering . By searching 1144 ESEC / FSE ’19 , August 26 – 30 , 2019 , Tallinn , Estonia Caulo M . 2 . 1 2 . 2 2 . 3 2 . 4 2 . 5 2 . 6 Figure 1 : Adopted process on Title , Abstract , and Keywords , the following papers were found : [ 2 , 13 , 14 ] . These papers were read and it was observed that the authors did not classify software metrics applied to SFP in a tax - onomy . This outcome further justifies the need for a taxonomy of software metrics for SFP . 2 . 2 Defining RQs The following main research question ( RQ ) was defined : RQ1 How could the metrics used for SFP be classified in a taxonomy ? This RQ was defined by following the guidelines by Easterbrook et al . [ 3 ] . It is an exploratory / descriptive RQ because it aims at advanc - ing the knowledge in metrics for SFP . Once a clear and complete taxonomy is attained , several base - rate questions about the normal patterns of occurrence in the taxonomy will be answered in future works , such as : RQ2 Which categories of metrics more often occur in SFP research papers ? RQ3 What is the longevity of software metrics in SFP ? RQ3 . 1 How long ( on average ) a software metric survives ? RQ3 . 2 Which metrics are the most long - living ? RQ3 . 3 Which metrics are the least long - lived ? I defined a metric as “most long - living " if it was collected by the authors of previous SLRs but it is still used in literature ( i . e . , it can be found in research papers published in 2017 ) , while a “least long - lived " one is a metric that has fallen into disuse . I have planned to provide answers to these RQs before the end of my Ph . D . course . 2 . 3 Searching for SLRs in SFP We followed a systematic approach to identify the SLRs of interest . Such secondary studies are the recommended EBSE ( Evidence - Based Software Engineering ) method to aggregate evidence . The following search string was executed : software AND fault AND prediction AND ( Systematic Literature Review OR SLR ) on Scopus , IEEE Xplore , and ACM Digital Library . This search ( on Title , Abstract , and Keywords ) allowed to identify nine research papers [ 1 , 4 – 7 , 9 , 10 , 12 , 15 ] . These papers were read to understand which research works presented SLRs whose results included details about metrics for SFP . Radjenović et al . [ 12 ] selected 106 papers published between 1991 and 2011 , by classifying them according to metrics and con - text properties . They found that object - oriented metrics were used nearly twice more often than traditional source code metrics ( 49 % vs 27 % ) and process metrics ( 24 % ) , with the Chidamber and Ke - merer’s metrics being the most frequently used . They also found that object - oriented and process metrics have been reported to be more successful in SFP , compared to size and complexity metrics ; moreover , process ones seemed to be better at predicting post - release faults , compared to any static code metric . Malhotra [ 9 ] synthesized and assessed the empirical evidence re - garding Machine Learning ( ML ) techniques for SFP . She identified 64 primary studies published between 1991 and 2013 and high - lighted seven categories of ML techniques . She also grouped many studies according to the used metrics : ( i ) procedural ; ( ii ) object - oriented and ( iii ) miscellaneous . Research papers by Hall et al . [ 5 ] , Syeed et al . [ 15 ] , and Isong and Obeten [ 6 ] presented the results of SLRs not specifically focusing on metrics for SFP . For example , the SLR by Hall et al . [ 5 ] shows how the context of models , the independent variables used , and the modeling techniques applied , influence the performance of fault prediction models . As for the independent variables , the authors focused on categories of software metrics rather than on single metrics , thus making their results too large - grained for our goal . The remaining four papers were out of topic . Thus , the output of this step led to the selection of the SLRs by Malhotra [ 9 ] and Radjenović et al . [ 12 ] since the type of analysis they made was more suited to the goals of my research . Moreover , these SLRs considered the longest time window in the literature . These SLRs together cover the time window between 1991 and 2013 ( i . e . , the union of the time periods between the two SLRs ) . Hence , such SLRs were exploited in order to collect the metrics ( : SoftwareMetrics ) to be included in the taxonomy . Then , search strings by Malhotra and Radjenović et al . ( : SearchStrings ) were run . The former string was run on papers published between January 2014 and December 2017 and the latter on papers published between January 2012 and December 2017 . Since the research for papers was conducted in 2018 , this year has been excluded from the analysis to avoid incompleteness . Please note also that it was preferred to re - analyze the papers published in 2012 and 2013 despite they were already “covered” in the work by Malhotra [ 9 ] , in order to double check them ( something not needed for 1991 - 2011 , in which the work by Radjenović et al . [ 12 ] acts as a double check ) . To be consistent , inclusion and exclusion criteria that these authors used in their SLRs ( : InclusionCriteria ) were adopted as well . 1145 A Taxonomy of Metrics for Software Fault Prediction ESEC / FSE ’19 , August 26 – 30 , 2019 , Tallinn , Estonia 2 . 4 Conducting Reviews The SLRs deriving from the previous step were selected to be ex - tended , by analyzing the most recent research papers on SFP , that such SLRs did not cover . This is crucial to provide an updated overview of SFP research papers . Conducting a review is a critical step and requires the definition of a review protocol ; both the SLRs of reference were based on the one by Kitchenham and Charters [ 8 ] . Such protocol represents a de - facto standard in the software en - gineering field , hence it was adopted to extend these SLRs . It is described in the following . 2 . 4 . 1 Search Strategy and Study Selection . To be included , a study must have been written in English and published in either a journal or conference proceedings . In order to search among the primary studies , both the search strings previously used by Radjenović et al . [ 12 ] and Malhotra [ 9 ] were run in : Scopus , IEEE Xplore , and ACM Digital Library , as well as when searching for a taxonomy . The used search strings are : • “software AND ( metric * OR measurement * ) AND ( fault * OR defect * OR quality OR error - prone ) AND ( predict * OR prone * OR probability OR assess * OR detect * OR estimat * OR classi - ficat * ) " [ 12 ] ; • “software AND ( fault OR defect OR error ) AND ( proneness OR prone OR prediction OR probability ) AND ( regression OR machine learning OR soft computing OR data mining OR classification OR Bayesian network OR neural network OR decision tree OR support vector machine OR genetic algorithms OR random forest ) " [ 9 ] . The initial searches elicited 7078 papers . The author and two other researchers evaluated the title and abstract of each paper and 6635 of them were rejected as not relevant to SFP . This process was validated by randomly assigning 2360 papers to each researcher . Pairwise inter - rater reliability was measured across the three sets of decisions to get a fair / good agreement on the first iteration of this process . A second iteration resulted in a 100 % agreement among the researchers . Moreover , twenty - two papers were in common between the results of the two search strings , hence they were analyzed just once . Please note that if a paper was not available online for the download , its authors were invited to share it . A paper was not included if its authors did not positively answer to such a request . The remaining 421 papers were read in full . The inclusion and exclusion criteria shown in Table 1 were applied and through this phase , only 201 papers passed the selection . 2 . 4 . 2 Study QualityAssessment . Inadditiontogeneralinclusion / ex - clusion criteria , the quality assessment of primary studies needs to be carefully considered . To this end , Kitchenham and Charter [ 8 ] suggest exploiting a quality checklist . In this study , the checklists by Malhotra [ 9 ] and Radjenović et al . [ 12 ] were merged . The resulting quality checklist is reported in Table 2 . Each item in this checklist admitted the following three possible answers : Yes , Partially , or No . These answers were marked by 1 , 0 . 5 , and 0 , respectively . There - fore , the final score ranged from 0 to 20 . A paper was excluded if it obtained a score lower than 10 , i . e . , the cut - off value proposed in both [ 12 ] and [ 9 ] . No papers obtained a score of less than 10 . Thus , all the papers were considered in the creation of the taxonomy . 2 . 4 . 3 Data Extraction . Google spreadsheets were used to record information about the primary studies . In particular , one sheet concerns the selected research papers ; while another focuses on software metrics . The first contains standard information such as : ( i ) authors , ( ii ) title , ( iii ) abstract , ( iv ) publication details ( v ) additional notes ; but also specific data about prediction techniques such as : ( i ) the used predictor ( s ) or classifier ( s ) , ( ii ) the performance evalu - ation metrics , ( iii ) whether the authors performed a correlation between metrics and bugs . Concerning the experimental objects , the following information was tracked : ( i ) the number of mined software systems or datasets , ( ii ) the license of the analyzed soft - ware and ( iii ) the programming language . In this first spreadsheet , research papers were grouped according to the two used search strings . Please note that such gathered information was not used in the study presented in this paper since it will be subject to further work . Concerning metrics for SFP , the following data were collected in another spreadsheet : ( i ) extended name , ( ii ) acronym ( s ) , ( iii ) a detailed description , ( iv ) the category of the metrics , ( v ) the level of granularity ( e . g . , class , method , file ) and ( vi ) which research papers exploited such metrics . Each information about the metrics was manually annotated by the author , and the first draft of such a collection has been created . The three researchers are currently assigning a unique description to each metric , by searching among the papers in which it was used . Thus , the whole collection of met - rics has been divided by three , in a way that each researcher must check about 180 metrics descriptions . In subsequent rounds , each author is going to check the work performed by one of the other two . A big effort is still being employed to combine in a single entry those metrics that represented the same concept but had a slightly different name or acronym . 2 . 5 Creating the Taxonomy The creation of the taxonomy ( and the description of its metrics ) is based on the following steps : ( i ) a first draft of the taxonomy is built on the basis of the SLRs by Radjenović et al . [ 12 ] and Malhotra et al . [ 9 ] and ( ii ) this taxonomy is enriched with the metrics from the previous phase of the process . Each of the steps above is going through a validation process conducted by the three researchers ( the author , her advisor and an external researcher ) . In the first step , a first draft of the taxonomy has been built and then one of the other two researchers individually validated this taxonomy . The third one double - checked the work that the first and the second people did . A similar process is being applied to validate the final version of the taxonomy obtained in the second step . The identified metrics are being placed in a specific category . Each category may descend from a more general category . In this way , the taxonomy is being created . This process is still in progress since all the three researchers have to completely agree about the identified structure of the taxonomy before let two external experts validate it . 2 . 6 Validating the Taxonomy Some experts in the field of software metrics will be asked to vali - date this taxonomy . To this end , at least two researchers experienced in software metrics are going to be invited . They will be asked to apply a two steps validation approach . In the first step , they will be asked to validate the taxonomy on the basis of a checklist . In the 1146 ESEC / FSE ’19 , August 26 – 30 , 2019 , Tallinn , Estonia Caulo M . Table 1 : Inclusion and Exclusion Criteria by Malhotra and Radjenović Inclusion criteria ( a paper must be one of . . . ) Exclusion criteria ( a paper must NOT be one of . . . ) − Empirical studies using the ML techniques for SFP − Studies without empirical analysis or results of use of the ML techniques for SFP − Empirical studies combining the ML and non ML techniques − Studies based on dependent variable other than fault proneness − Empirical studies comparing the ML and statistical techniques − Similar studies i . e . , studies by same author in conference as well extended version in journal − Prediction is made on the basis of software metrics ( independent variables ) − Review Studies − Empirical studies ( academic and industry ) using large and small scale datasets − Studies without an empirical validation or including experimental results of software metrics in fault prediction − Studies comparing software metrics performance in the area of SFP − Studies discussing software metrics in a context other than software fault prediction ( e . g . , maintainability ) − Studies discussing modeling techniques ( e . g . , Naive Bayes , Random Forest ) and not software metrics Table 2 : Quality criteria Q # Quality questions Q1 Are the aims of the research clearly stated ? Q2 Was the sample size justified ? Q3 If the study involves assessment of prediction model or technique , is it clearly defined ? Q4 Are the metrics used in the study adequately measured ? Q5 Are the metrics used in the study fully defined ? Q6 Are the data collection methods adequately described ? Q7 Were the data sets adequately described ? Q8 Are the statistical methods described ? Q9 Are the statistical methods justified ? Q10 Is the purpose of the analysis clear ? Q11 Are scoring systems ( performance evaluation metrics or techniques ) described ? Q12 Are the results and findings clearly stated ? Q13 Are all study questions answered ? Q14 Are the limitations of the study specified ? Q15 Are negative findings presented ? Q16 Are null findings interpreted ? Q17 Does the study discuss how the results add to the literature ? Q18 Does the report have implications for practice ? Q19 Do the researchers explain the consequences of any problems with the reliability of their measures ? Q20 Is the study repeatable ? Are public data sets used ? second step , experts will be asked to freely provide indications and suggestions to improve the taxonomy . 3 PRELIMINARY RESULTS Considering the 1991 - 2013 time window , the first set of 218 soft - ware metrics was collected . Keeping in mind that the time window between 1991 and 2013 has been covered independently by the two SLRs of reference ( i . e . , [ 9 ] and [ 12 ] ) , it is reasonable to be confident about the comprehensiveness of the identified metrics set . Then , for the 2012 - 2017 time window ( both extremes included ) , the extension of the two SLRs of reference led to the identification of a set of new 308 metrics . The current taxonomy is available at the following link : https : / / bit . ly / 2xkuJ1L and details on the metrics collection are avail - able at the following Google Spreadsheet : https : / / bit . ly / 2Gzf7NC . The taxonomy is divided into two sets : one for the process and the other for the product metrics . If it is read from left to right , the categories become more specific , otherwise more general . As for software metrics , each of them has an acronym , a description , a goal ( i . e . , the granularity of the prediction ) and the category under which it was included in the taxonomy . References of the selected papers are indicated in the columns with the header “Used in” ( columns F and G ) and not this paper because of space reasons . A glossary to ease the comprehension of some metrics is being created as well . 4 CONCLUSIONS AND FUTURE WORK In this paper , I introduced a taxonomy of metrics for SFP . It currently includes 526 metrics used in research papers published from 1991 to 2017 . As future work , I have planned to : ( i ) study the defined RQs not yet answered ; ( ii ) provide a standard way to measure metrics used in SFP and formalize their definitions ; ( iii ) provide information on the effectiveness of each of these metrics ; ( iv ) group those metrics by the co - linearity that characterizes them ; and ( v ) foresee a pull - request - like mechanism too keep my taxonomy updated throughout the years . ACKNOWLEDGMENTS I would like to thank my Ph . D . advisor Professor Giuseppe Scan - niello , for guiding me in this research work and Professor Gabriele Bavota for his precious help and availability . REFERENCES [ 1 ] WasifAfzalandRichardTorkar . 2011 . Ontheapplicationofgeneticprogramming for software engineering predictive modeling : A systematic review . Expert Syst . Appl . 38 ( 09 2011 ) , 11984 – 11997 . [ 2 ] Djuradj Babich , Peter J . Clarke , James F . Power , and B . M . Golam Kibria . 2011 . UsingaClassAbstractionTechniquetoPredictFaultsinOOClasses : ACaseStudy Through Six Releases of the Eclipse JDT . In Proceedings of the ACM Symposium on Applied Computing . ACM , 1419 – 1424 . [ 3 ] Steve Easterbrook , Janice Singer , Margaret - Anne Storey , and Daniela Damian . 2008 . Selecting Empirical Methods for Software Engineering Research . Springer London , London , 285 – 311 . [ 4 ] Tracy Hall , Sarah Beecham , David Bowes , David Gray , and Steve Counsell . 2011 . Developing Fault - Prediction Models : What the Research Can Show Industry . IEEE Software 28 , 6 ( Nov 2011 ) , 96 – 99 . [ 5 ] Tracy Hall , Sarah Beecham , David Bowes , David Gray , and Steve Counsell . 2012 . A Systematic Literature Review on Fault Prediction Performance in Software Engineering . IEEE Trans . Softw . Eng . 38 , 6 ( Nov . 2012 ) , 1276 – 1304 . [ 6 ] B . Isong and E . Obeten . 2013 . A systematic review of the empirical validation of object - oriented metrics towards fault - proneness prediction . International Journal of Software Engineering and Knowledge Engineering 23 , 10 ( 2013 ) , 1513 – 1540 . [ 7 ] Upulee Kanewala and James M . Bieman . 2014 . Testing Scientific Software : A Systematic Literature Review . IST 56 ( 10 2014 ) . [ 8 ] Barbara Kitchenham and Stuart Charters . 2007 . Guidelines for performing System - atic Literature Reviews in Software Engineering . Technical Report EBSE 2007 - 001 . Keele University and Durham University Joint Report . [ 9 ] Ruchika Malhotra . 2015 . A Systematic Review of Machine Learning Techniques for Software Fault Prediction . Appl . Soft Comput . 27 , C ( Feb . 2015 ) , 504 – 518 . [ 10 ] Sonia Montagud , Silvia Abrahão , and Emilio Insfran . 2012 . A Systematic Review of Quality Attributes and Measures for Software Product Lines . Software Quality Journal 20 , 3 - 4 ( Sept . 2012 ) , 425 – 486 . [ 11 ] Fabio Palomba , Gabriele Bavota , Massimiliano Di Penta , Fausto Fasano , Rocco Oliveto , and Andrea De Lucia . 2018 . On the diffuseness and the impact on maintainability of code smells : a large scale empirical investigation . Empir Softw Eng 23 , 3 ( 2018 ) , 1188 – 1221 . [ 12 ] Danijel Radjenovic , Marjan Hericko , Richard Torkar , and Ales Zivkovic . 2013 . Software fault prediction metrics : A systematic literature review . IST 55 , 8 ( 2013 ) , 1397 – 1418 . [ 13 ] Felix Salfner , Maren Lenk , and Miroslaw Malek . 2010 . A Survey of Online Failure Prediction Methods . ACM Comput . Surv . 42 , 3 ( March 2010 ) , 10 : 1 – 10 : 42 . [ 14 ] Ajmer Singh , Rajesh Bhatia , and Anita Singhrova . 2018 . Taxonomy of machine learning algorithms in software fault prediction using object oriented metrics . Procedia Comput . Sci . 132 ( 2018 ) , 993 – 1001 . [ 15 ] M . M . M . Syeed , I . Hammouda , and T . Systä . 2014 . Prediction models and tech - niques for Open Source Software projects : A systematic literature review . Inter - national Journal of Open Source Software and Processes 5 , 2 ( 2014 ) , 1 – 39 . 1147