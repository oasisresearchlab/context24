Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web Raquel Justo a , Thomas Corcoran b , Stephanie M . Lukin b , Marilyn Walker b , M . Inés Torres a , ⇑ a Speech Interactive Research Group , Universidad del País Vasco UPV / EHU , Depto . Electricidad y Electrónica , Fac . Ciencia y Tecnología , Sarriena s / n , 48940 Bilbao , Spain b Natural Language and Dialogues Systems Lab , University of California , Santa Cruz , Department of Computer Science , 1156 N . High , SOE - 3 , Santa Cruz , CA 95064 , USA a r t i c l e i n f o Article history : Available online 17 June 2014 Keywords : Emotional language Social web Feature extraction Sarcasm Nastiness a b s t r a c t Automatic detection of emotions like sarcasm or nastiness in online written conversation is a difﬁcult task . It requires a system that can manage some kind of knowledge to interpret that emotional language is being used . In this work , we try to provide this knowledge to the system by considering alternative sets of features obtained according to different criteria . We test a range of different feature sets using two dif - ferent classiﬁers . Our results show that the sarcasm detection task beneﬁts from the inclusion of linguis - tic and semantic information sources , while nasty language is more easily detected using only a set of surface patterns or indicators . (cid:2) 2014 Elsevier B . V . All rights reserved . 1 . Introduction Dialogic language on the web in interactive forms of media such as social networks and online forums is very different than the newspaper articles or task - oriented dialogs typically studied in work on natural language processing [ 33 , 23 , 10 , 36 , 37 , 43 ] . Online conversation is both more informal and more subjective : users tend to express their opinions with highly subjective and often emotional language . Moreover , in many cases context is needed in order to understand what people are saying . Only recently have large corpora of this type of subjective dialog language become available , including labeled corpora of tweets , user reviews , online conversations and chats [ 40 , 12 , 39 , 32 , 22 ] . However because the number of studies using these corpora are limited , in many cases there are few baselines establishing how difﬁcult it is to understand user utterances in these contexts . In particular , our work draws on the recently released the Internet Argument Corpus ( IAC ) , a publicly available corpus of online forum conversations on a range of social and political topics [ 40 ] . The IAC includes a large set of conversations from 4forums . com , a website for political debate and discourse . This site is a fairly typical inter - net forum where people post a discussion topic , other people post responses , and a treelike conversation structure is created . The cor - pus comes with annotations of different types of social language categories including sarcastic vs . not sarcastic , nasty vs . not nasty , rational vs . emotional and respectful vs . insulting . Figs . 1 and 2 provide examples of posts and post pairs from the IAC . We focus on two types of social dialogic language for which annotations are provided in the IAC distribution , namely SARCASM and NASTINESS . Our primary goal is simply to test how difﬁcult it is to automatically classify sarcastic and nasty utterances using supervised learning techniques , independently of topic . Both sar - casm and nastiness are highly subjective utterance types , but pre - vious work suggests that they are likely to differ in detection difﬁculty [ 24 ] . We hypothesize that nastiness is presented more overtly , using less ﬁgurative language , and requiring less world knowledge to recognize . See Figs . 1 and 2 . We present a set of supervised learning experiments on detec - tion of sarcasm and nastiness in online dialog . We compare a range of feature sets developed using different criteria . One of our foci is to test whether it is possible to automatically obtain a set of fea - tures valuable for identifying different forms of social language in online conversations regardless of the topic , style , speaker , or affordances of the online forum . To do so , we integrate statistic , linguistic , semantic and emotional information into our features , as a richer alternative to purely statistical or syntactically moti - vated surface patterns . These feature sets are then used to establish a baseline for a rule - based classiﬁer and for a Naive Bayes classiﬁer . The unique contributions of this paper include : (cid:2) Methods for discovering an appropriate set of features for dif - ferent types of social language . http : / / dx . doi . org / 10 . 1016 / j . knosys . 2014 . 05 . 021 0950 - 7051 / (cid:2) 2014 Elsevier B . V . All rights reserved . ⇑ Corresponding author . Address : Depto . Electricidad y Electrónica , Fac . Ciencia y Tecnología , Universidad del País Vasco , Sarriena s / n , 48940 Leioa , Spain . Tel . : + 34 94601 2715 . E - mail addresses : raquel . justo @ ehu . es ( R . Justo ) , tcorcora @ soe . ucsc . edu ( T . Corcoran ) , slukin @ soe . ucsc . edu ( S . M . Lukin ) , mwalker @ soe . ucsc . edu ( M . Walker ) , manes . torres @ ehu . es , manes @ ehu . es ( M . I . Torres ) . Knowledge - Based Systems 69 ( 2014 ) 124 – 133 Contents lists available at ScienceDirect Knowledge - Based Systems journal homepage : www . elsevier . com / locate / knosys (cid:2) Classiﬁcation methods that explicitly consider the possibility of different forms of sarcasm such as hyperbole , understatement and irony [ 14 , 4 ] . (cid:2) Feature sets which explicitly considers the semantic meaning and the problem of long utterances that include both the target category of sentences , e . g . , sarcastic , as well as sentences not in the target category ( e . g . , not sarcastic ) . (cid:2) Comparison of ease of detection of two types of social language ( sarcasm and nastiness ) in an identical context . 2 . Related work Social networks , blogs , forums and many other websites allow people to share information . This social use of the web can provide valuable information to companies , which are therefore interested in opinion mining and sentiment analysis . Developing tools to select and analyze opinions is now a challenging goal for many companies . However , this information is informal and unstruc - tured so that it is also a challenging topic for research in natural language processing . Truly understanding natural language requires computational models that can decoding the semantic meaning of utterances as well as the sentics , requiring methods that go beyond words to deal with concepts [ 8 ] . Cambria et al . [ 7 ] present a review of the evolution of research on these topics . According to their work , sentiment analysis has typically been per - formed over on - topic documents [ 7 ] . The review includes a discus - sion of the most relevant text features for sentiment classiﬁcation , such as term and n - gram frequencies and presence , certain adjec - tives as indicators , as well as phrases chosen by POS patterns and sentiment lexicons . Revised methods included keyword spotting , lexical afﬁnity , and concept - based approaches using web ontolo - gies , as well as Bayesian inference and support vector machines as statistical classiﬁers . Recent techniques also include concept - level analysis for both knowledge - based [ 9 , 3 ] and statistical approaches [ 13 ] . However , concept - based approaches to date have mainly dealt with polarity detection [ 9 , 13 ] . Only irony , sometimes theorized as an utterance that assumes the opposite of the actual situation , has been addressed as playing the role of a polarity reverser [ 3 , 32 ] . In this situation , detection of irony is assumed to require a representation of the dialog context . Fig . 1 . Sarcastic Posts and Post Pairs from 4forums . com . Sarcastic examples were all reliably rated sarcastic : 4 or more turkers voted sarcastic , greater than 80 % sarcastic yes count . Fig . 2 . Nasty Post Pairs from 4forums . com . Nastiness was annotated on a scale of (cid:3) 5 . . . 5 , with (cid:3) 5 being very nasty . The selected examples all had average nastiness ratings less than (cid:3) 2 . 5 . R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 125 Previous theoretical work on sarcasm deﬁnes it as mocking , contemptuous , or ironic language intended to convey scorn or insult [ 14 , 4 ] . However while it may employ ambivalence , it is not necessarily ironic . For example , if a person requires a lot of time to obtain the solution of a simple mathematical problem , one might ask ‘‘How many days does he need ? ’’ . This is sarcastic but not ironic . However , ‘‘He is like Einstein’’ is a sarcastic sentence that uses irony . More importantly , almost any utterance can be sar - castic in the correct context , e . g . , ‘‘you are early’’ will be interpreted as sarcastic in a context where the addressee is clearly late . Some sarcastic forms also require sociocultural knowledge to detect , e . g . , ‘‘The Nazis really brought good to the world’’ . In spoken language , sarcasm is manifested partly in vocal inﬂec - tions [ 19 ] ; however , in written language there are no standard cues for signaling the speaker’s intention . Several form of punctuation have been proposed , like percontation point , but are not usually employed . Twitter users sometimes employ hashtags like ‘ # sar - casm’ as an explicit marker of sarcasm , and [ 22 ] show that extra - linguistic elements such as hashtags can be seen as the social media equivalent of non - verbal expressions that people employ in live interaction . Unfortunately , all sarcastic tweets are not marked by a hashtag . Previous work suggests that hashtags : ( 1 ) are not used reliably ; and ( 2 ) may be used only for the hardest forms of sarcasm that would not be recognized without the hash - tag [ 15 , 32 ] . In written texts , scare quotes may be used to denote skepticism or disagreement with the quoted terminology , but this cue is not reliable either . Clearly , new methods are needed to detect the different forms in which sarcasm is expressed in social media . Previous work on sarcasm detection has primarily focused on product reviews and tweets rather than online dialogs . Ref . [ 38 ] proposes a semi - supervised algorithm based on k - nearest neigh - bors to detect sarcasm in online product reviews using pattern and punctuation based features . Subsequently the same algorithm applied to millions of tweets collected from Twitter [ 15 ] produced signiﬁcantly better results . They suggest that this is due to the con - textless nature of Twitter , which forces tweeters to express sar - casm in a way that is easy to understand from individual sentences . In contrast , sarcastic utterances in Amazon reviews co - appear with other sentences ( in the same review ) and sarcastic meaning may emerge from the context . Other work on sarcasm detection in tweets explored the contribution of linguistic and pragmatic features of tweets to the automatic separation of posi - tive and negative polarity messages from sarcastic messages [ 15 ] . This work found that the three pragmatic features ( ToUser , smiley and frown ) were among the ten most discriminating features in the classiﬁcation task . Ref . [ 22 ] uses n - gram features to build a Winnow classiﬁer to identify sarcastic tweets in Dutch , based on the hypothesis that hashtags can , and do , replace linguistic mark - ers typically needed to mark sarcasm . They also conclude that peo - ple tend to be more sarcastic towards speciﬁc topics such as school , homework , and family life . Ref . [ 32 ] presents a method to detect a common form of sarcasm consisting of contrasting a positive sen - timent with a negative situation in tweets . They use a bootstrap - ping method to recognize the contrasting sentiments in syntactic structures related to sarcasm . Lukin and Walker ( 2013 ) is the only other work we are aware of that focuses on sarcasm detection in online dialog , using a boot - strapping method to automatically ﬁnd sarcastic utterances in unannotated dialog [ 24 ] . Their work explores using a weakly supervised bootstrapping approach , rather than establishing accu - racies for a range of supervised classiﬁers based on different fea - ture sets as we do here . As above , this task is more difﬁcult than sarcasm detection in Twitter due to the contextless nature of the tweets and their limited length . In contrast , in product reviews and online dialog , a sarcastic utterance may also include sentences that are not sarcastic at all , introducing noise to the sarcasm clas - siﬁcation task . Moreover , other challenges of online dialog include the diversity of topics and the dialogic nature of the utterances . These factors lead to more colloquial vocabulary and language style , and the appearance of many different forms of sarcasm [ 14 , 4 ] . Some examples of these situations are provided in Fig . 1 . For example , while P1 uses the word ‘‘Bravo’’ that could be identiﬁed as a sarcastic cue , P2 does not seem to contain any surface sarcastic indicators : recognizing it as sarcastic depends on semantic inter - pretation and sociocultural knowledge . We also see long sarcastic posts ( P3 ) where some sentences are not sarcastic . Finally , exam - ple P5 – 2 shows a form of sarcasm that might be difﬁcult to identify because there appear to be no explicit cues : rather recognizing P5 – 2 as sarcasm appears to rely on world knowledge to recognize that it makes a ridiculous analogy , equating a kidney to a human being . Turning to our other category of social language , we also aim to automatically identify utterances from the IAC that are categorized as nasty . Fig . 2 provides examples of nasty posts and post pairs from IAC . Post P7 – 2 contains potentially identiﬁable nasty patterns or keywords such as ‘‘you’re an’’ along with the presence of a curse word [ 35 ] . P9 – 2 also contains keywords like ‘‘silly’’ and ‘‘clueless’’ but context may be necessary to recognize that these words are used as an insult with respect to the third party evolutionists men - tioned in P9 – 1 . More interestingly , identifying P10 – 2 as a nasty utterance requires understanding that when the poster of P10 – 2 is ‘‘laughing’’ , they are actually amused at the expense of the poster of P10 – 1 . Early work in this area in [ 35 ] focuses on identifying abusive messages , known as ﬂames , in emails . This approach builds fea - tures from the syntax and semantics of each sentence and uses a decision tree for rule - based classiﬁcation . These features were hypothesized to be useful and then automatically extracted . Ref . [ 31 ] examine ﬂame detection in text messages based on statistical models and automatically create rule - based patterns based on word sequences and inserting wild - cards . They found that previous work was unable to determine agreement between human expert annotators on what a ﬂame is . They deﬁne a tolerance margin for ﬂames based on certain conditions or different contexts , thus cre - ating a dictionary of words and phrases with different degrees of ﬂame intensity . Ref . [ 44 ] use a semi - supervised technique for detecting insults in Tweets based on statistical topical modeling techniques for feature induction rather than keyword or pattern matching as in the two techniques previously described . By boot - strapping this process , they are able to classify many Tweets using only a small set of seed words . Ref . [ 34 ] use crowdsourcing to label comments from an online community form that contain profani - ties and insults . They then use Support Vector Machines to identify negative comments , which are often coupled with insults , and fur - thermore to whom the comment is directed : to another user in the forum or to a third party . Their best performing system for insult detection is a multistep classiﬁer that utilizes both valence and rel - evance analyses . Other work that is closely related is the detection of cyberbully - ing or trolling ( one of the multiple forms of cyberbullying ) . Cyber - bullying is generally deﬁned as the repeated injurious use of harassing language to insult or attack another individual in an online community and it is widely recognized as a serious social problem , especially for adolescents . In [ 5 ] a troll detector is trained by ﬁrst identifying the concepts most commonly used by trolls and then expanding the resulting knowledge base with semantically related concepts . This work uses Sentic Computing ( a new opinion mining and sentiment analysis paradigm ) to analyse the texts . Ref . [ 11 ] provides a new concept - level approach for bullying detec - tion that outperforms state - of - the - art natural language processing and statistical approaches . Ref . [ 18 ] determined the terms most 126 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 commonly used in cyberbullying and developed queries that can be used to detect cyberbullying content using a combination of bag - of - words language models and a supervised machine learning approach based on Latent Semantic Indexing . More generally , all types of semantic features of user generated content in social media can be useful . This includes trustworthi - ness of information and deception detection , as well as language that indicates social or political afﬁliation between conversants [ 1 , 25 , 41 , 16 , 2 ] . For example previous work focuses on using super - vised techniques to identify web , email and opinion spam [ 28 , 27 , 20 , 21 , 17 ] . However , because deceptive opinion spam is not easily identiﬁed by humans , it is difﬁcult to construct a gold - standard corpus . Several lines of previous work therefore crowd - source deceptive reviews using Amazon Mechanical Turk [ 28 , 20 , 21 ] . Other work [ 26 ] proposed a method to exploit observed reviewing behaviors to detect opinion spammers ( fake reviewers ) in an unsupervised Bayesian inference framework . Other approaches are based on graph models [ 42 ] , or analysis of the gen - erality or speciﬁcity of the reviews , as well as some aspects of the content [ 20 , 21 ] . 3 . Empirical approach In order to identify sarcasm or nastiness in online written con - versation , humans make use of different types of knowledge that is related to the type of language that is used . Thus , in this work , we would like to provide the system with this kind of knowledge by using different information sources . Speciﬁcally , we were inter - ested in exploring the ability of statistic , linguistic , semantic , length and emotional information to heuristically approximate the ability of humans to detect sarcasm and nastiness . 3 . 1 . Feature extraction In this section we present the way in which the required infor - mation is given to the system . Speciﬁcally , different sets of features were obtained according to the following criteria : (cid:2) Mechanical Turk Cues : We ﬁrst consider a set of features made up of cues that were identiﬁed as indicators of sarcasm or nas - tiness in our previous work [ 24 ] , e . g . , ‘‘oh really’’ , ‘‘I get it’’ , and ‘‘no way’’ associated with sarcasm , and others associated with nastiness like ‘‘idiot’’ , ‘‘you’re an’’ , ‘‘your ignorance is’’ . These were identiﬁed in a Mechanical Turk procedure applied to an independent set of 617 posts for sarcasm and a different set of 510 posts for nastiness . The sarcastic set of cues consists of 1815 n - grams ( 1629 unigrams , 147 bigrams , 39 trigrams ) and the nasty set of cues consists of 3256 n - grams ( 289 unigrams , 966 bigrams and 2001 trigrams ) . Lukin and Walker ( 2013 ) dem - onstrated that these cues for sarcasm and nastiness were reli - able selected across a number of different annotators [ 24 ] . Table 1 provides detailed examples of some of the top unigram , bigram and trigram cues for both sarcasm and nastiness , along with the interannotor agreement scores for each cue and the frequency of the cue in the annotated dataset ( see column MT for the cues and column IA for interannotator agreement scores ) . Note the very high agreement among the human anno - tators , even though the frequency of each cue is relatively low . Thus , although these cues might also appear in other types of sentences , they appear to be recurrent resources in written sar - castic / nasty utterances that appear in forum dialogs , regardless of the speciﬁc context and situation . Table 1 also indicates some of the features that would be selected from the same dataset if a feature selection procedure based on v 2 was used . Examination of the selected features shows that v 2 feature selection appears to overﬁt on this small dataset , and indeed Lukin and Walker show that the cues selected by v 2 do not perform as well in their bootstrapping approach as the MT cues selected by the human annotators . See [ 24 ] for more details about how the annotations were collected . (cid:2) Statistical Cues : Since manually identifying the cues associated with each emotion is costly , the set of Mechanical Turk Cues might not be general enough to correctly identify the required emotions in different corpora . Thus , a second set of features was automatically extracted from the training set . This set of Table 1 Mechanical Turk ( MT ) and v 2 indicators for Sarcasm and Nastiness , shown with Interannotator Agreement ( IA ) and Frequency of Occurrence ( N ) . From Lukin and Walker ( 2013 ) , and used in the experiments below . SARCASM CUES NASTINESS CUES v 2 MT IA N v 2 MT IA N unigram unigram right ah 0 . 95 2 like idiot 0 . 9 3 oh relevant 0 . 85 2 them unfounded 0 . 85 2 we amazing 0 . 8 2 too babbling 0 . 8 2 same haha 0 . 75 2 oh lie 0 . 72 11 all yea 0 . 73 3 mean selﬁsh 0 . 7 2 them thanks 0 . 68 6 just nonsense 0 . 69 9 mean oh 0 . 56 56 make hurt 0 . 67 3 bigram bigram the same oh really 0 . 83 2 of the don’t expect 0 . 95 2 mean like oh yeah 0 . 79 2 you mean get your 0 . 9 2 trying to so sure 0 . 75 2 yes , you’re an 0 . 85 2 that you no way 0 . 72 3 oh , what’s your 0 . 77 4 oh yeah get real 0 . 7 2 you are prove it 0 . 77 3 I think oh no 0 . 66 4 like a get real 0 . 75 2 we should you claim 0 . 65 2 I think what else 0 . 7 2 trigram trigram you mean to I get it 0 . 97 3 to tell me get your sick 0 . 75 2 mean to tell I’m so sure 0 . 65 2 would deny a your ignorance is 0 . 7 2 have to worry then of course 0 . 65 2 like that ? make up your 0 . 7 2 sounds like a are you saying 0 . 6 2 mean to tell do you really 0 . 7 2 to deal with well if you 0 . 55 2 sounds like a do you actually 0 . 65 2 I know I go for it 0 . 52 2 you mean to doesn’t make it 0 . 63 3 you mean to oh , sorry 0 . 5 2 to deal with what’s your point 0 . 60 2 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 127 cues consists of the unigrams , bigrams and trigrams extracted from each utterance in the training set . Then a feature selection procedure is used to reduce the number of features removing those that are not relevant . (cid:2) Linguistic information : Even when considering Statistical Cues , we need a large training set to automatically extract a set of cues general enough to be used with different test data . How - ever , large annotated sets of training data are not always avail - able . Therefore we also consider what types of generalizations of features are possible through the use of linguistic categories . Speciﬁcally , in this feature set , we generalize using the part - of - speech ( POS ) labels associated with each word in an utterance . These labels correspond to the classical 8 lexical categories in English : Noun , Pronoun , Adjective , Verb , Adverb , Preposition , Conjunction and Interjection . The inclusion of this kind of lin - guistic information provides a more general way of deﬁning patterns . For example , ‘‘Really ? well’’ is a sarcastic feature appearing in the training set along with the sarcastic POS pat - tern ‘‘ADV ADV’’ . Then , if ‘‘Really ? then’’ , which could also be a sarcastic marker , does not appear in the training set , its corre - sponding POS pattern ‘‘ADV ADV’’ could still be used to identify this phrase as a useful feature . Thus , in this work , a set POS n - grams was considered where n = 1 , 2 , 3 . (cid:2) Semantic information : Although some patterns may be good cues to different emotions , there is additional information in online conversation that is more related to semantic categories than to speciﬁc words or POS types . Here , we explore whether LIWC classescanprovidegeneralizationsofpatternsbasedonsemantic information [ 30 ] . The LIWC dictionary includes 64 different lexi - cal categories including sets of words related to anger , affect , feel - ings , social , number , and health but also more general categories suchas pronoun , verb or future . Inordertomakemoremeaningful feature patterns , we associate each word in an utterance with its corresponding LIWC semantic category or categories . For exam - ple , the word ‘‘address’’ is associated with the LIWC category ‘‘home’’ while‘‘adult’’ is associatedwith LIWC categories‘‘social’’ and ‘‘humans’’ . Then , a vector is built with the number of times each category appears in the utterance . (cid:2) Length information : Given the speciﬁc corpus we are dealing with , in which there are very different length posts , we noticed that length information can be a very useful factor for the cor - rectly interpretation of previous sets of features . Thus , in this work , a vector with the following information is also considered for each utterance : number of words , number of characters , number of sentences , average words per sentence , average character per sentence . (cid:2) Concept and Polarity Information : Considering that both , sarcasm and nastiness , are closely tied to speciﬁc emotional states , new feature sets including emotional information were also consid - ered . Speciﬁcally , SenticNet - 3 . 0 [ 6 ] was used . This tool aims to infer cognitive and affective information from natural language text . Morespeciﬁcally , itprovidesthesemanticsandsenticsasso - ciated with more than 14 , 000 common sense concepts . In this work , we wrote the sentences of the posts in terms of those con - cepts and obtained the semantics and polarity associated with each one . If we consider the sentence ‘‘I love chocolate but ice cream is better’’ the corresponding concept sentence would be ‘‘love better’’ . The semantic information associated with this con - ceptsentencewouldbe : love : lust , love _ another _ person , sexuality , beloved , show _ empathy and better : tall _ play _ basketball , reliable , good _ quality , all _ right , best . Thus , the feature set is made up of vectors with the number of times each semantic item appears intheutterance . Ontheotherhand , polarityinformationwasalso obtained for the concept sentence : love : 0 . 667 , better : 0 . 132 . A feature set considering this kind of information was also built . Concretely , a vector of two features , Positive Polarity ( the sum of positive polarities in the utterance ) and Negative Polarity ( the sum of the negative polarities ) was built for each utterance . In our example that vector would be ( 0 . 799 , 0 ) . Note that instead of using only one score with the total polarity of the sentence , both positive and negative polarities are considered . This allows us to differentiate between sentences with neutral words and thosethathave a similarnumberof words withpositiveandneg - ative polarity . 3 . 2 . Sarcasm detection In order to tackle the problem of classifying a post as sarcastic or not sarcastic , different combinations of the features above were deﬁned . First , the set of Mechanical Turk cues ( MT _ C ) , as illustrated in Table 1 , and the set of Statistical cues ( ST _ C ) were considered . The idea is to see whether a set of cues of a speciﬁc emotion can be automatically extracted from the corpus . In a second stage , Semantic Information was considered . The use of this kind of information can help to detect some sarcastic forms that are related to the correct interpretation of the post’s meaning . For example , in the following sarcastic utterance : ‘‘is there any rea - son to respond to this guy at all ? if we ignore him , may be he’ll go again’’ , there are no speciﬁc terms that indicate sarcasm . Addition - ally , there are some uses of irony , like ‘‘Oh how I love being ignored ’’ , where a positive sentiment is contrasted with a negative situation [ 32 ] . In this kind of sarcasm , we hypothesize that a bal - anced number of categories associated with positive and negative emotions will be a cue . Thus , we ﬁrst considered a set of features with the information coming from LIWC semantic classes ( Liwc ) . Then , we also used a set of semantic features extracted from Sen - ticNet 3 . 0 tool , SEM , so that emotional semantic information can be compared to more general semantic information obtained from LIWC classes . The feature sets were built as explained in Sec - tion 3 . 1 . Then , and in order to see whether these two sources are complementary , a new set , Liwc _ SEM , with the combination of the two semantic information sources was built . Then , in a third stage , the automatically obtained ST _ C set was combined with other types of information . Our goal is to enrich the statistical set in order to detect additional forms of sarcasm that are directly related to linguistic and semantic information and that are not only expressed by using speciﬁc patterns like ‘‘oh really’’ : First of all , Statistical cues and Linguistic information were gath - ered in a new set ST _ Li . The inclusion of Linguistic information is useful to detect more general linguistic patterns found in the train - ing data as described above . Alternatively , ST _ Liwc and ST _ Liwc _ SEM sets were built includ - ing Statistical cues + Semantic information extracted from LIWC clas - ses and enriched with emotional semantic information . On the other hand , the set of utterances we are attempting to classify has a great deal of variation , including long sarcastic posts where sarcasm is only used in part of the post , as well as short sar - castic posts , where the whole post is sarcastic . The posts that are not sarcastic also vary a great deal in length . Thus , it is possible for the features for an utterance to include a balanced number of positive and negative emotions if the post is long , but not sarcastic . In shorter posts , such as those in Twitter [ 32 ] , the probability of being sarcastic in that situation is higher . In the same way , there could be a speciﬁc cue in a short post than can be a very good indi - cator of sarcasm , whereas it could not indicate sarcasm at all in a longer post where it is not relevant for the whole utterance . Thus , Length information was also considered in ST _ Lg set that combines Statistical cues + Length information . Then , we deﬁned a feature set that combines Statistical cues with Mechanical Turk cues which we label ST _ MT , in order to test whether the human provided annotations from Mechanical Turk 128 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 can signiﬁcantly improve the results . That is , the idea is to see to what extent there are speciﬁc patterns in MT _ C not included in ST _ C that might be discriminant in the classiﬁcation process . In a fourth stage of our work , we included Polarity Information , extracted from SenticNet 3 . 0 tool , together with semantic and sta - tistical information leading to Liwc _ SEM _ Pol and ST _ Liwc _ Pol sets . The inclusion of polarity in the feature set could help detect the emotional attitude of the poster . It could also capture the uses of irony where positive situations are contrasted with negative ones as explained above . Finally , two new different sets were also considered : Comb that included Statistical cues , Linguistic information , Semantic information and Length information and All set including Comb + MT _ C . These new sets could be helpful to see if different information sources can cooperate to provide better results . 3 . 3 . Nastiness detection Since some sarcastic forms can express nastiness in an utter - ance , there should be similarities between the previous task and this one ; so the same sets of features suggested for sarcasm detec - tion were also considered here . However , there are some expres - sions or cue words that indicate that an utterance is insulting or nasty without any ambiguity , i . e . as mentioned above we hypoth - esized that nastiness is often expressed very overtly . See Fig . 2 . For example , when a post contains the ‘‘you idiot’’ bigram it is unam - biguously nasty , while it is possible for the ‘‘really ? well’’ bigram to occur in a not sarcastic sentence , even though it appears more fre - quently in sarcastic posts . Thus , we expect that the features derived from patterns extracted from the training data ( automati - cally or manually ) should be more robust for nastiness . Summing up , the different feature sets employed here were also MT _ C , ST _ C , Liwc , SEM , Liwc _ SEM on the one hand and ST _ Li , ST _ Liwc , ST _ Liwc _ SEM , ST _ Lg , ST _ MT on the other hand . Finally , Liwc _ SEM _ Pol , ST _ Liwc _ Pol , Comb and All sets were also considered . 4 . Classiﬁcation methods We evaluate the utility of the cues and features deﬁned above for the task of detecting either sarcastic or nasty posts in online conversation through classiﬁcation experiments . We utilize two different classiﬁers : a rule - based classiﬁer as a baseline and a clas - sical Naive Bayes classiﬁer . 4 . 1 . Baseline classiﬁer The rule - based classiﬁer is designed to allow our results to be comparable to previous work aimed at developing a high - precision classiﬁer for bootstrapping [ 24 , 33 ] . However in contrast to the bootstrapping approach , we use a much larger training set to learn cues and we apply different methods to learn more general cues as described in Section 3 . 1 . The classiﬁer is based on computing two metrics for each fea - ture : the number of times each feature appears in the training set ( freq ) and the percentage of cases that the feature occurs in a sarcastic utterance ( % sarc ) for sarcasm detection , or the percent - age of cases that the feature occurs in a nasty utterance ( % nast ) for nastiness detection . Then two different thresholds were deﬁned for these metrics as follows : h 1 6 freq and h 2 6 % sarc or h 2 6 % nasty . A test post is then classiﬁed as sarcastic if it contains at least two features x 1 and x 2 such that ð freq ð x 1 Þ > h 1 ^ freq ð x 2 Þ > h 1 Þ and ð % sarc ð x 1 Þ > h 2 ^ % sarc ð x 2 Þ > h 2 Þ . Inthesame way a test post is classiﬁed as nasty if it contains at least two fea - tures x 1 and x 2 such that ð freq ð x 1 Þ > h 1 ^ freq ð x 2 Þ > h 1 Þ and ð % nast ð x 1 Þ > h 2 ^ % nast ð x 2 Þ > h 2 Þ . In this work , h 1 and h 2 parame - ters were chosen in a tuning step where the classiﬁer was run over the training set . 4 . 2 . Naive Bayes classiﬁer A Naive Bayes classiﬁer implements Bayes’ theorem under the assumption of independence between pairs of features . Given a class x and a feature vector x ¼ x 1 ; . . . ; x d the Bayes’ formula states : P ð x j ; x 1 ; . . . ; x d Þ ¼ p ð x 1 ; . . . ; x d j x Þ P ð x Þ P cj ¼ 1 p ð x 1 ; . . . ; x d j x j Þ P ð x j Þ ð 1 Þ Using the naive independence assumption p ð x 1 ; . . . ; x d j x Þ ¼ Q di ¼ 1 p ð x i j x Þ the Naive Bayes classiﬁcation rule is set as : ^ x ¼ argmax x P ð x j x 1 ; . . . ; x d Þ (cid:4) argmax x ð P ð x Þ Y d i ¼ 1 p ð x i j x ÞÞ ð 2 Þ Usually Naive Bayes classiﬁers assume classic forms for the dis - tribution p ð x i j x Þ such as Gaussian , multinomial or Bernouilli . In this work , we assume our data to be multinomially distributed , Naive Bayes Multinomial ( NBM ) . The distribution is parametrized by vectors h x ¼ ð h x 1 ; . . . ; h x d Þ for each class x , where d is the num - ber of features and h x i ¼ p ð x i j x Þ is the probability of feature i appearing in a sample in class x . Parameters h x are estimated as : ^ h x i ¼ N x i þ a N x þ a d ð 3 Þ where N x i ¼ P x 2 H x i is the number of times feature i appears in a sample of class x in the training set H ; N x ¼ P j H j i ¼ 1 N x i is the total count of all features for class x , and a P 0 implements a smoothed maximum likelihood estimation that prevents zero probabilities . When the number of features is high , a previous selection pro - cedure is required in order to extract the most signiﬁcant features . Here , the feature selection procedure is based on the v 2 distribu - tion . The v 2 test determines whether there is a signiﬁcant associa - tion between two variables . In this way v 2 statistics is also used to select a set of signiﬁcant features for classiﬁcation . The set of the n features with the highest values for the v 2 statistic is selected from the data , which are assumed to be multinomially distributed among classes . This selection was carried out in a tuning step where the classiﬁer was run over the training data set . 5 . Experimental evaluation The Cues and Features deﬁned in Sections 3 . 2 and 3 . 3 for sar - casm and nastiness detection were evaluated in two main sets of classiﬁcation experiments . The ﬁrst one aims to evaluate the basic sets of cues , MT _ C and ST _ C , using a simple baseline classiﬁer . The second one , presented in Section 5 . 3 , evaluates all the features with a NBM classiﬁer . Classiﬁers results are reported in terms of Preci - sion ( P ) , Recall ( R ) , F - measure ( F ) and Accuracy ( Ac ) . 5 . 1 . Corpus and annotations For our experiments we used the IAC [ 40 ] , along with the anno - tations it provides for different types of social language categories including sarcastic vs . not sarcastic , nasty vs . not nasty , rational vs . emotional and respectful vs . insulting . We focus specially on sar - castic and nasty utterances labeled by Mechanical Turkers as described in [ 24 ] . From this corpus we selected the sarcasm and nastiness data sets used in our experiments : (cid:2) Sarcasm Data Set : a subset of IAC consisting of 6 , 461 instances balanced between sarcastic and not sarcastic posts . (cid:2) Nastiness Data Set : a subset of the IAC consisting of 2765 instances balanced between nasty and not nasty posts . R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 129 A cross - validation procedure was employed in all experiments . Both the Sarcasm and the Nastiness Data Sets were previously split into 10 disjoint subsets . At each iteration of the cross validation procedure 9 subsets were used as training and the last subset is used as an independent test - set . 5 . 2 . Experiments with the baseline classiﬁer The values of h 1 and h 2 that maximized the F - measure over the training set ( consisting of 9 subsets of the cross - validation parti - tion ) were chosen as thresholds for the ﬁnal classiﬁcation of a com - pletely independent test set . This procedure was carried out at each iteration of the cross - validation procedure for both sarcasm and nastiness detection . In this ﬁrst series of experiments , MT _ C and ST _ C cues were employed and compared . The results are given in Table 2 and Table 3 respectively . These results show that the ST _ C feature set provides better results than the MT _ C features in terms of all the evaluation metrics , for both sarcasm and nastiness detection . How - ever , the most important difference can be observed in Recall val - ues . This could be explained by looking at the number of sarcastic features that are above the mentioned threshold ( h 1 ; h 2 ) in the training set for each case . When considering MT _ C this value was 345 for sarcasm and 696 for nastiness , while it was much higher ( 10 , 824 and 3432 ) for ST _ C case . As discussed above , and illus - trated by Table 1 , the MT _ C features are better when the training set is small , but for larger data sets feature selection based on v 2 reveals more useful cues . If we directly compare the performance of sarcasm and nasti - ness detection , we note that the nastiness detector works better with both MT _ C and ST _ C feature sets . This supports our initial hypothesis that nasty language is easier to identify than sarcastic language . Further support for this hypothesis is that the training set for sarcastic detection was much larger than the training set for nastiness ( 6461 vs . 2765 ) , but better results were still obtained for nastiness . This difference is more evident for ST _ C set and it could be related to the speciﬁc sarcastic and nasty MT _ C sets that were employed , as illustrated in Table 1 . While there are many more sar - castic unigrams than sarcastic bigrams or trigrams , the set of nasty trigrams is much higher than the sets of unigrams and bigrams . Since it is more difﬁcult to statistically select predeﬁned bigrams and trigrams than unigrams in a corpus , the sarcastic MT _ C set is more efﬁcient than the nasty MT _ C set . 5 . 3 . Experiments with the Naive Bayes classiﬁer The second series of experiments aims to evaluate the ability of the features deﬁned in Section 3 to detect sarcasm and nastiness through a NBM classiﬁer . We ﬁrst selected the most relevant fea - tures and for this purpose , different sets with increasing number of features were obtained according to the v 2 statistic . The number of features added to each step was the total number divided by 25 . Then the training set was classiﬁed , and the F measure computed , using each of the 25 sets . The feature set that maximized F was selected , and the test set , which was not seen during feature selec - tion and tuning , is evaluated using the selected features . This pro - cedure was repeated in each iteration of the cross - validation procedure for both sarcasm and nastiness detection . For these experiments we employed the implementation provided in the Sci - kit - learn python package for v 2 statistics and NBM classiﬁer [ 29 ] . 5 . 3 . 1 . Sarcasm detection Table 4 shows the obtained P , R , F and Ac results for the different feature sets described in Section 3 . 2 . The average number of fea - tures selected for each of the 10 cross - validation iterations is also given ( 1st column ) . Table 4 shows that the NBM classiﬁer provides better results than the Baseline classiﬁer . The improvements in terms of F are higher than 20 % for both the MT _ C and ST _ C feature sets . However , focusing on the comparison between MT _ C and ST _ C the same ten - dency is observed for both classiﬁers . That is , ST _ C provides better results in terms of all the measures , but the most signiﬁcant improvements are associated with R . Note that if we directly compare the different types of Semantic Information used here ( SEM vs . Liwc ) , that we get slightly better results with more general semantic classes ( Liwc ) . However , the combination of the two types of features ( Liwc _ SEM ) provided a signiﬁcant improvement in terms of F . Thus , it seems that the emo - tional semantic information complements the general semantic information provided by Liwc , resulting in better performance . However , note that the ST _ C features still perform better . In the third stage of our work , different information sources were added to ST _ C , namely linguistic and semantic information . Table 4 shows that the four new sets ( ST _ Li , ST _ Liwc , ST _ Liwc _ Table 2 Precision , recall , F - measure and accuracy results for the baseline classiﬁer for sarcasm detection and MT _ C and ST _ C sets . MT _ C ST _ C Sarcasm h 1 ; h 2 2 , . 55 2 , . 75 # sarc . feat . 345 10 , 824 P ( % ) 53 . 5 54 . 4 R ( % ) 46 . 7 55 . 3 F ( % ) 49 . 8 54 . 8 Ac ( % ) 53 . 0 54 . 5 Table 3 Precision , recall , F - measure and accuracy results for the baseline classiﬁer for nastiness detection and MT _ C and ST _ C sets . MT _ C ST _ C Nastiness h 1 ; h 2 2 , . 55 2 , . 80 # nasty feat . 696 3 , 432 P ( % ) 56 . 6 61 . 6 R ( % ) 47 . 1 57 . 4 F ( % ) 51 . 3 59 . 3 Ac ( % ) 55 . 4 60 . 7 Table 4 F - measure ( F ) , accuracy ( Ac ) , precision ( P ) and recall ( R ) results obtained with NBM classiﬁer for sarcasm detection and different sets of features . # Feat . P ( % ) R ( % ) F ( % ) Ac ( % ) Sarcasm MT _ C 1089 65 . 1 67 . 3 66 . 2 65 . 6 ST _ C 239 K 67 . 8 72 . 3 69 . 7 68 . 7 Liwc 35 60 . 2 64 . 5 62 . 2 60 . 7 SEM 5093 59 . 8 63 . 8 61 . 6 60 . 1 Liwc _ SEM 5492 59 . 3 73 . 6 65 . 6 61 . 5 ST _ Li 374 K 65 . 6 75 . 9 70 . 0 67 . 5 ST _ Liwc 334 K 65 . 3 77 . 4 70 . 7 67 . 9 ST _ Liwc _ SEM 370 K 66 . 1 75 . 1 70 . 1 68 . 0 ST _ Lg 157 K 65 . 2 77 . 4 70 . 5 67 . 8 ST _ MT 353 K 66 . 1 76 . 2 70 . 6 68 . 3 Liwc _ SEM _ Pol 5454 59 . 3 73 . 7 65 . 7 61 . 5 ST _ Liwc _ Pol 348 K 65 . 5 76 . 9 70 . 7 68 . 1 Comb 303 K 65 . 3 76 . 7 70 . 1 67 . 3 All 280 K 65 . 3 76 . 7 70 . 2 67 . 5 130 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 SEM , ST _ Lg ) provide signiﬁcant improvements in terms of R when compared to ST _ C set , i . e . although P values are lower F increases in all cases . Therefore it is clear that the addition of linguistic , seman - tic and length information improves our ability to identify different forms of sarcasm . The best results are obtained with ST _ Liwc and ST _ Lg , the feature sets including Semantic and Length Information respectively . Although Liwc _ SEM provided better results than the Liwc feature set , the combined feature set ST _ Liwc _ SEM does not result in an F value as high as that obtained with the ST _ Liwc feature set . This might be due to the fact that feature selection that has more inﬂuence on the results when the number of features is high . More precise ways of carrying out feature selection should be considered in future work , in order to obtain more balanced sets of statistical and other types of features . However , the results obtained with these semantic feature sets highlights the inﬂuence of semantic meaning on sarcasm detection in online dialog , when combined with the effect of the differences in post lengths . Finally , when considering the ST _ MT feature set , which combines both sta - tistical and manually obtained cues , the R is lower than that obtained with ST _ Liwc but the P does not drop as much : this set has a F value similar to those obtained with ST _ Lg and ST _ Liwc . These results suggest that adding human knowledge to automati - cally obtained cues leads to higher performance . The integration of polarity information in the feature set does not signiﬁcantly improve the results , as can be seen when compar - ing Liwc _ SEM to Liwc _ SEM _ Pol and ST _ Liwc to ST _ Liwc _ Pol . However , given that polarity information typically helps with other sentiment classiﬁcation tasks , we posit that its effect might be hidden by feature sets with more features , and plan to explore different ways of integrating polarity features in future work . Table 4 also shows that combining different information sources together in Comb . yields performance improvements over the ST _ C feature set in terms of R and F , but performance lower than ST _ Liwc . Therefore it is clear that additional features do not always improve performance . The MT _ C feature set also achieves very similar results ( see the column of the All set ) . Over all the different feature sets , the best P and Ac values were obtained with ST _ C , while ST _ Liwc provides the best R and F values . 5 . 3 . 2 . Nastiness detection The same experiments were carried out for the nastiness detec - tion task with results shown in Table 5 . Again , a comparison of MT _ C and ST _ C shows that better results are achieved with ST _ C for all measures , but for nastiness the improvement is even greater . Note that in this case a fewer number of cues were obtained after the feature selection procedure ( 922 vs . 1089 ) while the number of statistical cues is higher ( 246 K vs . 239 K ) . For nastiness detection , the effect of semantic information from different sources is similar to that for sarcasm detection . A compar - ison of emotional semantic information ( SEM ) to more general semantic information ( Liwc ) , shows that SEM provides slightly better results : a higher value of P was achieved for a similar R value . However , the combination of the two sources also results in better performance ( see Liwc _ SEM set ) . Table 5 shows that combining different information sources from ST _ Li , ST _ Liwc , ST _ Liwc _ SEM and ST _ Lg , does not result in better performance than ST _ C . This suggests that speciﬁc patterns are good features for predicting nastiness and that the integration of additional information adds little value . ST _ Liwc provides con - siderably better results than Set _ Lg in this case , suggesting that length information does not contribute to nastiness detection but that semantic information does . Indeed , the fact that new nasty expressions were detected using this set is demonstrated by the high R value ( 84 . 7 ) that was achieved . Interestingly , combining the manually obtained cues from ST _ MT with statistical features shows a small improvement over ST _ C alone , providing the best results overall . However , again the polarity features fail to improve performance . When all types of features are combined into the Comb and All feature sets however , the results are worse . In this case these results are also worse than ST _ C . 6 . Discussion Our experimental results conﬁrm our hypothesis that while sar - casm and nastiness share some properties , there are also key dif - ferences in the way they are expressed . While sarcastic language is often subtle and requires world knowledge to recognize , nasti - ness is often expressed more overtly and can be unambiguously detected . However , in both cases we saw that the cost involved with obtaining manual patterns meant that the MT _ C set is much smal - ler and provides worse results than ST _ C . Although a bigger set of manually chosen patterns could possibly improve performance , it is difﬁcult to predict how many posts would have to be hand - anno - tated to obtain a better set of manually chosen cues . Since the MT cues were extracted from an independent set , as explained in Sec - tion 3 . 1 , some of these cues are not retrieved by the statistical method , and they signiﬁcantly enrich the ST _ C set . The results with the ST _ MT feature set include a high F - measure that is very near to the best F - measure for sarcasm and is the best F - measure for nas - tiness . For sarcasm detection , there were 1815 manually obtained cues of which 252 are not in the statistical set of features . Some of them correspond to spelling issues like ‘‘thankyou’’ , ‘‘diagree’’ . . . , plurals ( ‘‘absolutes’’ vs . ‘‘absolute’’ ) or third person derivations ( ‘‘enhance’’ vs . enhances ) . However , there are also some cues , not included in ST _ C set , that are good sarcasm indicators , like ‘‘an idiot’’ , ‘‘don’t you think’’ , ‘‘then of course’’ . Alternatively , there were 3256 nasty cues of which about 1900 were not in the statistical set of features . This difference in the overlapping percentage might result from higher variability in the n - grams that express nastiness , which means they can be unambiguously identiﬁed more easily , leading to a bigger feature set . However , data sparsity is also more noticeable in this case , so the performance of the ST _ MT set is lower than that for sarcasm . On the other hand , the inclusion of additional information ( lin - guistic , semantic and length ) improves our ability to detect differ - ent forms of sarcastic forms , with the semantic and length information providing the most discriminant information sources . Table 5 F - measure ( F ) , accuracy ( Ac ) , precision ( P ) and recall ( R ) results obtained with NBM classiﬁer for nastiness detection and different sets of features . # Feat . P ( % ) R ( % ) F ( % ) Ac ( % ) Nastiness MT _ C 922 73 . 2 63 . 8 67 . 9 70 . 0 ST _ C 246 K 77 . 4 81 . 2 79 . 1 78 . 5 Liwc 41 66 . 3 66 . 9 66 . 4 66 . 2 SEM 4913 67 . 9 67 . 1 67 . 2 67 . 4 Liwc _ SEM 5056 64 . 8 77 . 9 70 . 6 67 . 7 ST _ Li 229 K 76 . 9 76 . 8 76 . 7 76 . 7 ST _ Liwc 269 K 74 . 3 84 . 7 79 . 0 77 . 4 ST _ Liwc _ SEM 262 K 75 . 9 80 . 2 77 . 9 77 . 2 ST _ Lg 149 K 75 . 4 79 . 7 77 . 2 76 . 4 ST _ MT 235 K 76 . 9 82 . 3 79 . 4 78 . 6 Liwc _ SEM _ Pol 5056 64 . 8 77 . 7 70 . 5 67 . 6 ST _ Liwc _ Pol 269 K 75 . 7 83 . 1 78 . 8 77 . 6 Comb . 129 K 74 . 1 77 . 8 75 . 6 75 . 1 All 194 K 74 . 3 78 . 3 76 . 0 75 . 3 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 131 As mentioned above , there are some forms of sarcasm that are not associated with speciﬁc cues : they can only be detected by consid - ering the meaning of utterance . Thus , the ST _ C feature set is able to successfully detect sarcastic posts such as ‘‘oh they are ? that’s strange . . . i could not ﬁnd where exactly they stand on any of their numerous websites , just more restrictions . . . and the old hci was not moderate’’ where n - grams like ‘‘they are ? ’’ or ‘‘that’s strange’’ are fre - quently found in sarcastic posts . However , other posts such as ‘‘because . . . ( 1 ) that texas has a higher rate of violent crime ? ( 2 ) that texas fails to prevent such crimes ? ( 3 ) that texas sees a huge ﬁscal def - icit from such cases ? strange sense of honor you got there’’ was wrongly labeled as not sarcastic when ST _ C was used but correctly detected as sarcastic with ST _ Liwc . Furthermore , speciﬁc patterns can be used to identify sarcastic posts only when considered in combination with information about the length of the post . For instance , an isolated pattern , in a very long post , with a lot of dif - ferent sentences , might not indicate sarcasm , while the same pat - tern would indicate sarcasm in a very short post . For example , the following post was correctly labeled as not sarcastic by ST _ Lg whereas ST _ C labeled it as sarcastic ‘‘well , there are some groups that say only some gun control , but they have a hidden agenda . the brady campaign and the violence policy center , both favor complete prohibition of guns , even though they do not advocate it openly . yes i believe in some gun control , but only that which is logical . if you make it illegal , it will just create a big black market , and we all know how that is . la in california has a big black market thanks to gun con - trol’’ . In this case there are some n - grams like ‘‘well , ’’ that can signal sarcasm in some cases , but do not function as a strong indicator when the whole post is considered . Thus , the inclusion of length information in the feature set improves performance in these cases . An informal error analysis using the best performing classiﬁers suggests that misclassiﬁed posts are often those where context is needed . For example , ‘‘your proof ? ’’ was wrongly labeled as not sar - castic because the feature sets currently do not capture any con - text . There are other misclassiﬁed examples where world knowledge is essential , like ‘‘oh it applies , but if we were to look at the bible that way , god wouldn’t allow one single bad thing to hap - pen’’ . Thus , in future work it will be important to develop methods for representing context and some types of world knowledge . Our results clearly indicate that the speciﬁc properties of the data have to be taken into account when developing feature sets for different sentiment detection tasks . Our results for the same type of online conversation suggests that different sets of features should be used to detect sarcasm in tweets , where all the utter - ances have a limited length , vs . in online forums which vary much more in length . For nastiness detection , additional features representing lin - guistic information do not lead to better results than those obtained from statistical cues based on n - grams . This conﬁrms the hypothesis of Section 3 . 3 and shows that nastiness is expressed more overtly , and indicated with speciﬁc cues . Thus ﬁnding such cues is sufﬁcient for nastiness detection , regardless of the length of the utterance . Furthermore , adding linguistic and semantic information introduces noise and reduces performance . An error analysis of the misclassiﬁed examples suggests that nasty posts that are wrongly classiﬁed as nice using the ST _ C feature set result from cases where sarcasm or ﬁgurative language are used to express nastiness , e . g . , ‘‘if the world was a ﬁgment of your imagina - tion it would be a lot more pornographic’’ . This post was properly classiﬁed as nasty when the ST _ Liwc feature set was used . Indeed , ST _ Liwc provide the best R value , although ST was better in terms of F . The remaining misclassiﬁed posts , when ST _ C was used , result from the lack of contextual information . Additionally , while it seems plausible that combining all infor - mation sources should yield better performance than pairwise combinations , this is not borne out by our experiments . This could possibly be due to the heterogeneous origin of the different sets , or possibly that the feature selection procedure signiﬁcantly prunes the number of features . 7 . Concluding remarks and future work In this work , we report on the performance of different feature sets and different classiﬁers for the automatic detection of sarcasm and nastiness in online forums dialog . The results show that the best features are different for each emotion , although the two tasks appear to be similar a priori . Sarcasm can occur in many different forms , so the results were better when multiple sources of infor - mation were combined . Speciﬁcally , semantic information pro - vides the best results although length information also results in signiﬁcant improvement . Two semantic feature sets , one repre - senting emotion and the other general lexical categories were shown to be complementary to each other , with their combination yielding a signiﬁcant improvement . In future work , alternative ways of combining relevant knowledge should be explored , e . g . , balancing features from different sources when combining them might improve our results . Acknowledgements Partial funding for this work was provided by NSF CISE Grant # 1302668 , by the Basque Government Grant IT685 - 13 and by CICYT Grant TIN2011 - 28169 - C05 - 04 . References [ 1 ] R . Abbott , M . Walker , P . Anand , J . E . Fox Tree , R . Bowmani , J . King , How can you say such things ? ! ? : recognizing disagreement in informal political argument , in : Proceedings of the Workshop on Languages in Social Media . Association for Computational Linguistics , 2011 , pp . 2 – 11 . [ 2 ] A . Abu - Jbara , M . Diab , B . King , D . Radev , Identifying opinion subgroups in arabic online discussions , in : Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics , 2013 , pp . 829 – 835 . [ 3 ] C . Bosco , V . Patti , A . Bolioli , Developing corpora for sentimentanalysis : the case of irony and senti - tut , IEEE Intell . Syst . 28 ( 2 ) ( 2013 ) 55 – 63 . [ 4 ] G . Bryant , J . Fox Tree , Recognizing verbal irony in spontaneous speech , Meta . Symb . 17 ( 2 ) ( 2002 ) 99 – 119 . [ 5 ] E . Cambria , P . Chandra , A . Sharma , A . Hussain , Do not feel the trolls , in : Proceedings of the Third International Workshop on Social Data on the Web ( SDoW2010 ) , vol . I , 2010 . . [ 6 ] E . Cambria , D . Olsher , D . Rajagopal , Senticnet 3 : a common and common - sense knowledge base for cognition - driven sentiment analysis , in : Proceedings of the Twenty - Eighth AAAI Conference on Artiﬁcial Intelligence , AAAI Press , 2014 . [ 7 ] E . Cambria , B . Schuller , Y . Xia , C . Havasi , New avenues in opinion mining and sentiment analysis , IEEE Intell . Syst . 28 ( 2 ) ( 2013 ) 15 – 21 . [ 8 ] E . Cambria , B . White , Jumping NLP curves : a review of natural language processing research , IEEE Comput . Intell . Magaz . 9 ( 2 ) ( 2014 ) 48 – 57 . [ 9 ] A . Charng - Rurng Tsai , C . E . Wu , R . Tzong - Han Tsai , J . Yung - jen Hsu , Building a concept - level sentiment dictionary based on commonsense knowledge , IEEE Intell . Syst . 28 ( 2 ) ( 2013 ) 22 – 30 . [ 10 ] B . Di Eugenio , J . D . Moore , M . Paolucci , Learning features that predict cue usage , in : Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics , vol . 97 , Association for Computational Linguistics , ACL / EACL , 1997 , pp . 80 – 87 . [ 11 ] K . Dinakar , B . Jones , C . Havasi , H . Lieberman , R . Picard , Common sense reasoning for detection , prevention , and mitigation of cyberbullying , ACM Trans . Interact . Intell . Syst . 2 ( 3 ) ( 2012 ) 18 : 1 – 18 : 30 . [ 12 ] E . Forsyth , C . Martell , Lexical and discourse analysis of online chat dialog , in : Proceedings of the First IEEE International Conference on Semantic Computing ( ICSC 2007 ) , IEEE Computer Society , 2007 , pp . 19 – 26 . [ 13 ] L . García - Moya , H . Anaya - Sánchez , R . Berlanga - Llavori , Retrieving product features and opinions from customer reviews , IEEE Intell . Syst . 28 ( 3 ) ( 2013 ) 19 – 27 . [ 14 ] R . Gibbs , Irony in talk among friends , Meta . Symb . 15 ( 1 ) ( 2000 ) 5 – 27 . [ 15 ] R . González - Ibáñez , S . Muresan , N . Wacholder , Identifying sarcasm in twitter : a closer look , in : Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , HLT’ 11 , vol . 2 , Association for Computational Linguistics , 2011 , pp . 581 – 586 . [ 16 ] A . Hassan , V . Qazvinian , D . Radev , What’s with the attitude ? : identifying sentences with attitude in online discussions , in : Proceedings of the 2010 132 R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 Conference on Empirical Methods in Natural Language Processing , Association for Computational Linguistics ; 2010 , pp . 1245 – 1255 . [ 17 ] N . Jindal , B . Liu , Opinion spam and analysis , in : Proceedings of the 2008 International Conference on Web Search and Data Mining . Association for Computing Machinery , WSDM ’08 , 2008 , pp . 219 – 230 . [ 18 ] A . Kontostathis , K . Reynolds , A . Garron , L . Edwards , Detecting cyberbullying : query terms and techniques , in : H . C . Davis , H . Halpin , A . Pentland , M . Bernstein , L . A . Adamic ( Eds . ) , ACM Web Science 2013 , Association for Computing Machinery , 2013 , pp . 195 – 204 . [ 19 ] C . M . Lee , S . S . Narayanan , Toward detecting emotions in spoken dialogs , IEEE Transactions on Speech and Audio Processing 13 ( 2 ) ( 2005 ) 293 – 303 ( IEEE Signal Processing Society Best Paper Award 2009 ) . [ 20 ] J . M . Li , C . Cardie , Identifying manipulated offerings on review portals , in : Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , Association for Computational Linguistics , Seattle , Wash . , 2013 , pp . 1933 – 1942 . [ 21 ] J . Li , C . Cardie , S . Li , Topicspam : a topic - model - based approach for spam detection , in : Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics , Association for Computational Linguistics , 2013 , pp . 217 – 221 . [ 22 ] C . Liebrecht , F . Kunneman , A . Van den Bosch , The perfect solution for detecting sarcasm in tweets # not , in : Proceedings of the 4th Workshop on Computational Approaches to Subjectivity , Sentiment and Social Media Analysis , Association for Computational Linguistics , 2013 , pp . 29 – 37 . [ 23 ] D . Litman , J . Allen , Recognizing and relating discourse intentions and task - oriented plans , in : P . Cohen , J . Morgan , M . Pollack ( Eds . ) , Intentions in Communication , MIT Press , 1990 . [ 24 ] S . Lukin , M . Walker , Really ? Well . Apparently bootstrapping improves the performance of sarcasm and nastiness classiﬁers for online dialogue , in : Proceedings of the Workshop on Language Analysis in Social Media , Association for Computational Linguistics , 2013 , pp . 30 – 40 . [ 25 ] A . Misra , M . Walker , Topic independent identiﬁcation of agreement and disagreement in social media dialogue , in : Proceedings of the SIGDIAL 2013 Conference , Association for Computational Linguistics , 2013 , pp . 41 – 50 . [ 26 ] A . Mukherjee , A . Kumar , B . Liu , J . Wang , M . Hsu , M . Castellanos , R . Ghosh , Spotting opinion spammers using behavioral footprints , in : The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’13 , 2013 , pp . 632 – 640 . [ 27 ] M . Ott , C . Cardie , J . Hancock , Estimating the prevalence of deception in online review communities , in : Proceedings of the 21st International Conference on World Wide Web , Association for Computing Machinery , 2012 , pp . 201 – 210 . [ 28 ] M . Ott , Y . Choi , C . Cardie , J . T . Hancock , Finding deceptive opinion spam by any stretch of the imagination , in : Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , vol . 1 , Association for Computational Linguistics , 2011 , pp . 309 – 319 . [ 29 ] F . Pedregosa , G . Varoquaux , A . Gramfort , V . Michel , B . Thirion , O . Grisel , M . Blondel , P . Prettenhofer , R . Weiss , V . Dubourg , J . Vanderplas , A . Passos , D . Cournapeau , M . Brucher , M . Perrot , E . Duchesnay , Scikit - learn : machine learning in Python , J . Mach . Learn . Res . 12 ( 2011 ) 2825 – 2830 . [ 30 ] J . W . Pennebaker , M . E . Francis , R . J . Booth , Linguistic inquiry and word count : Liwc [ computer software ] , Austin , TX , liwc net 2007 . [ 31 ] A . Razavi , D . Inkpen , S . Uritsky , S . Matwin , Offensive language detection using multi - level classiﬁcation , Advan . Artif . Intell . ( 2010 ) 16 – 27 . [ 32 ] E . Riloff , A . Qadir , P . Surve , L . De Silva , N . Gilbert , R . Huang , Sarcasm as contrast between a positive sentiment and negative situation , in : Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , Association for Computational Linguistics , 2013 , pp . 704 – 714 . [ 33 ] E . Riloff , J . Wiebe , Learning extraction patterns for subjective expressions , in : Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing , EMNLP ’03 , Association for Computational Linguistics , 2003 , pp . 105 – 112 . [ 34 ] S . Sood , E . Churchill , J . Antin , Automatic identiﬁcation of personal insults on social news sites , J . Am . Soc . Inform . Sci . Technol . 63 ( 2 ) ( 2011 ) 270 – 285 . [ 35 ] E . Spertus , Smokey : automatic recognition of hostile messages , in : Proceedings of the National Conference on Artiﬁcial Intelligence , John Wiley & Sons Ltd . , 1997 , pp . 1058 – 1065 . [ 36 ] A . Stent , A conversation acts model for generating spoken dialogue contributions , Comp . Speech Lang . : Spec . Iss . Spok . Lang . Gener . 16 ( 3 – 4 ) ( 2002 ) 313 – 352 . [ 37 ] R . Subba , B . Di Eugenio , An effective discourse parser that uses rich linguistic information , in : Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , NAACL ’09 , Association for Computational Linguistics , 2009 , pp . 566 – 574 . [ 38 ] O . Tsur , D . Davidov , A . Rappoport , Icwsm – a great catchy name : semi - supervised recognition of sarcastic sentences in online product reviews , in : Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media , The AAAI Press , 2010 , pp . 162 – 169 . [ 39 ] P . Turney , Thumbs up or thumbs down ? Semantic orientation applied to unsupervised classiﬁcation of reviews , in : Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , ACL ’02 , Association for Computational Linguistics , 2002 , pp . 417 – 424 . [ 40 ] M . Walker , J . F . Tree , P . Anand , R . Abbott , J . King , A corpus for research on deliberation and debate , in : Proceedings of the Eight International Conference on Language Resources and Evaluation ( LREC’12 ) , European Language Resources Association ( ELRA ) , 2012 , pp . 23 – 25 . [ 41 ] M . A . Walker , P . Anand , R . Abbott , J . E . F . Tree , C . Martell , J . King , That is your evidence ? : Classifying stance in online political debate , Dec . Supp . Syst . 53 ( 4 ) ( 2012 ) 719 – 729 . [ 42 ] G . Wang , S . Xie , B . Liu , P . S . Yu , Identify online store review spammers via social review graph , ACM Trans . Intell . Syst . Technol . ( TIST ) 3 ( 4 ) ( 2012 ) 61 . [ 43 ] T . Wilson , P . Hoffmann , S . Somasundaran , J . Kessler , J . Wiebe , Y . Choi , C . Cardie , E . Riloff , S . Patwardhan , Opinionﬁnder : a system for subjectivity analysis , in : Proceedings of HLT / EMNLP on Interactive Demonstrations , HLT - Demo ’05 , Association for Computational Linguistics , 2005 , pp . 34 – 35 . [ 44 ] G . Xiang , B . Fan , L . Wang , J . Hong , C . Rose , Detecting offensive tweets via topical feature discovery over a large scale twitter corpus , in : Proceedings of the 21st ACM International Conference on Information and Knowledge Management , ACM , 2012 , pp . 1980 – 1984 . R . Justo et al . / Knowledge - Based Systems 69 ( 2014 ) 124 – 133 133