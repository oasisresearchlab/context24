Noise2Void - Learning Denoising from Single Noisy Images Alexander Krull 1 , 2 , Tim - Oliver Buchholz 2 , Florian Jug 1 krull @ mpi - cbg . de 2 Authors contributed equally MPI - CBG / PKS ( CSBD ) , Dresden , Germany Abstract The ﬁeld of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images . Re - cently it has been shown that such methods can also be trained without clean targets . Instead , independent pairs of noisy images can be used , in an approach known as N OISE 2N OISE ( N2N ) . Here , we introduce N OISE 2V OID ( N2V ) , a training scheme that takes this idea one step further . It does not require noisy image pairs , nor clean target images . Consequently , N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot . Especially inter - esting is the application to biomedical image data , where the acquisition of training targets , clean or noisy , is fre - quently not possible . We compare the performance of N2V to approaches that have either clean target images and / or noisy image pairs available . Intuitively , N2V cannot be ex - pected to outperform methods that have more information available during training . Still , we observe that the denois - ing performance of N OISE 2V OID drops in moderation and compares favorably to training - free denoising methods . 1 . Introduction Image denoising is the task of inspecting a noisy image x = s + n in order to separate it into two components : its signal s and the signal degrading noise n we would like to remove . Denoising methods typically rely on the assump - tion that pixel values in s are not statistically independent . In other words , observing the image context of an unob - served pixel might very well allow us to make sensible pre - dictions on the pixel intensity . A large body of work ( e . g . [ 16 , 19 ] ) explicitly mod - eled these interdependencies via Markov Random Fields ( MRFs ) . In recent years , convolutional neural networks ( CNNs ) have been trained in various ways to predict pixel values from surrounding image patches , i . e . from the recep - noisy clean TraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditionalTraditional InputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput noisy noisy N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE N OISE 2N OISE noisy void N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID N OISE 2V OID Figure 1 : Training schemes for CNN - based denoising . Tra - ditionally , training networks for denoising requires pairs of noisy and clean images . For many practical appli - cations , however , clean target images are not available . N OISE 2N OISE ( N2N ) [ 12 ] enables the training of CNNs from independent pairs of noisy images . Still , also noisy image pairs are not usually available . This motivated us to propose N OISE 2V OID ( N2V ) , a novel training procedure that does not require noisy image pairs , nor clean target im - ages . By enabling CNNs to be trained directly on a body of noisy images , we open the door to a plethora of new appli - cations , e . g . on biomedical data . tive ﬁeld of that pixel [ 24 , 11 , 26 , 6 , 23 , 25 , 18 , 14 ] . Typically , such systems require training pairs ( x j , s j ) of noisy input images x j and their respective clean target im - ages s j ( ground truth ) . Network parameters are then tuned to minimize an adequately formulated error metric ( loss ) between network predictions and known ground truth . Whenever ground truth images are not available , these methods cannot be trained and are therefore rendered use - less for the denoising task at hand . Recent work by Lehti - nen et al . [ 12 ] offers an elegant solution for this problem . Instead of training a CNN to map noisy inputs to clean ground truth images , their N OISE 2N OISE ( N2N ) train - 2129 ing attempts to learn a mapping between pairs of inde - pendently degraded versions of the same training image , i . e . ( s + n , s + n ′ ) , that incorporate the same signal s , but independently drawn noise n and n ′ . Naturally , a neural network cannot learn to perfectly predict one noisy image from another one . However , networks trained on this im - possible training task can produce results that converge to the same predictions as traditionally trained networks that do have access to ground truth images [ 12 ] . In cases where ground truth data is physically unobtainable , N2N can still enable the training of denoising networks . However , this re - quires that two images capturing the same content ( s ) with independent noises ( n , n ′ ) can be acquired [ 3 ] . Despite these advantages of N2N training , there are at least two shortcomings to this approach : ( i ) N2N train - ing requires the availability of pairs of noisy images , and ( ii ) the acquisition of such pairs with ( quasi ) constant s is only possible for ( quasi ) static scenes . Here we present N OISE 2V OID ( N2V ) , a novel training scheme that overcomes both limitations . Just as N2N , also N2V leverages on the observation that high quality denois - ing models can be trained without the availability of clean ground truth data . However , unlike N2N or traditional training , N2V can also be applied to data for which nei - ther noisy image pairs nor clean target images are available , i . e . N2V is a self - supervised training method . In this work we make two simple statistical assumptions : ( i ) the signal s is not pixel - wise independent , ( ii ) the noise n is condi - tionally pixel - wise independent given the signal s . We evaluate the performance of N2V on the BSD68 dataset [ 17 ] and simulated microscopy data 1 . We then compare our results to the ones obtained by a tradition - ally trained network [ 24 ] , a N2N trained network and several self - supervised methods like BM3D [ 5 ] , non - local means [ 2 ] , and to mean - and median - ﬁlters . While it can - not be expected that our approach outperforms methods that have additional information available during training , we observe that the denoising performance of our results only drops moderately and is still outperforming BM3D . Additionally , we apply N2V training and prediction to three biomedical datasets : cryo - TEM images from [ 3 ] , and two datasets from the Cell Tracking Challenge 2 [ 20 ] . For all these examples , the traditional training scheme cannot be applied due to the lack of ground truth data and N2N train - ing is only applicable on the cryo - TEM data . This demon - strates the tremendous practical utility of our method . In summary , our main contributions are : • Introduction of N OISE 2V OID , a novel approach for training denoising CNNs that requires only a body of single , noisy images . • Comparison of our N2V trained denoising results 1 For simulated microscopy data we know the perfect ground truth . 2 http : / / celltrackingchallenge . net / to results obtained with existing CNN training schemes [ 24 , 12 , 25 ] and non - trained methods [ 18 , 2 ] . • A sound theoretical motivation for our approach as well as a detailed description of an efﬁcient implemen - tation . The remaining manuscript is structured as follows : Sec - tion 2 contains a brief overview of related work . In Sec - tion 3 , we introduce the baseline methods we later compare our own results to . This is followed by a detailed description of our proposed method and its efﬁcient implementation . All experiments and their results are described in Section 4 , and our ﬁndings are ﬁnally discussed in Section 5 . 2 . Related Work Below , we will discuss other methods that consider not the denoising task as mentioned above , but instead the more general task of image restoration . This includes the removal of perturbations such as JPEG artifacts or blur . With N2V we have to stick to the more narrow task of denoising , as we rely on the fact that multiple noisy observations can help us to retrieve the true signal [ 12 ] . This is not the case for general perturbations such as blur . We see N2V at the intersection of multiple methodolog - ical categories . We will brieﬂy discuss the most relevant works in each of them . Note that N2N is omitted here , as it has been discussed above . In concurrent work [ 1 ] , Batson et al . also introduce a method for self - supervised training of neural networks and other systems that is based on the idea of removing parts of the input . They show that this scheme can not only be applied by removing pixels , but also groups of variables in general . 2 . 1 . Discriminative Deep Learning Methods Discriminative deep learning methods are trained ofﬂine , extracting information from ground truth annotated training sets before they are applied to test data . In [ 9 ] , Jain et al . ﬁrst apply CNNs for the denoising task . They introduce the basic setup that is still used by success - ful methods today : Denoising is seen as a regression task and the CNN learns to minimize a loss calculated between its prediction and clean ground truth data . In [ 25 ] , Zhang et al . achieve state - of - the - art results , by introducing a very deep CNN architecture for denoising . The approach is based on the idea of residual learning [ 7 ] . Their CNN attempts to predict not the clean signal , but in - stead the noise at every pixel , allowing for the computation of the signal in a subsequent step . This structure allows them to train a single CNN for denoising of images cor - rupted by a wide range of noise levels . Their architecture completely dispenses with pooling layers . At about the same time Mao et al . introduce a com - plementary very deep encoder - decoder - architecture [ 14 ] for 2130 the denoising task . They too make use of residual learning , but do so by introducing symmetric skip connections be - tween the corresponding encoding and decoding modules . Just as [ 25 ] , they are able to use a single network for vari - ous levels of noise . In [ 18 ] Tai et al . use recurrent persistent memory units as part of their architecture , and further improve on previous methods . Recently Weigert et al . presented the CARE software framework for image restoration in the context of ﬂuores - cence microscopy data [ 24 ] . They acquire their training data by recording pairs of low - and high - exposure - images . This can be a difﬁcult procedure since the biological sample must not move between exposures . We use their implemen - tation as starting point for our experiments , including their speciﬁc U - Net [ 15 ] architecture . Note that N2V could in principle be applied with any of the mentioned architectures . However , [ 18 ] and [ 25 ] present an interesting peculiarity in this respect , as their residual architecture requires knowledge of the noisy input at each pixel . In N2V , this input is masked when the gradient is calculated ( see Section 3 ) . 2 . 2 . Internal Statistics Methods Internal Statistics Methods do not have to be trained on ground truth data beforehand . Instead , they can be directly applied to a test image where they extract all required infor - mation [ 27 ] . N2V can be seen as member of this category , as it enables training directly on a test image . In [ 2 ] , Buades et al . introduced non - local means , a clas - sic denoising approach . Like N2V , this method predicts pixel values based on their noisy surroundings . BM3D , introduced by Dabov et al . [ 5 ] , is a classic in - ternal statistics based method . It is based on the idea , that natural images usually contain repeated patterns . BM3D performs denoising of an image by grouping similar pat - terns together and jointly ﬁltering them . The downside of this approach is the computational cost during test time . In contrast , N2V requires extensive computation only during training . Once a CNN is trained for a particular kind of data , it can be applied efﬁciently to any number of additional im - ages . In [ 21 ] , Ulyanov et al . show that the structure of CNNs , inherently resonates with the distribution of natural images and can be utilized for image restoration without requiring additional training data . They feed a random but constant input into a CNN and train it to approximate a single noisy image as output . Ulyanov et al . ﬁnd that when they inter - rupt the training process at the right moment before conver - gence , the network produces a regularized denoised image as output . 2 . 3 . Generative Models In [ 4 ] , Chen et al . present an image restoration approach based on generative adversarial networks ( GANs ) . The au - thors use unpaired training samples consisting of noisy and clean images . The GAN - generator learns to generate noise and create pairs of corresponding clean and noisy images , which are in turn used as training data in a traditional su - pervised setup . Unlike N2V , this approach requires clean images during training . Finally , we want to mention the work by Van Den Oord et al . [ 22 ] . They present a generative model that is not used for denoising , but in spirit similar to N2V . Like N2V , Van Den Oord et al . train a neural network to predict an unseen pixel value based on its surroundings . The network is then used to generate synthetic images . However , while we train our network for a regression task , they predict a probability distribution for each pixel . Another difference lies in the structure of the receptive ﬁelds . While Van Den Oord et al . use an asymmetric structure that is shifted over the image , we always mask the central pixel in a square receptive ﬁeld . 3 . Methods Here , we will begin by discussing our image formation model . Then , we will give a short recap of the traditional CNN training and of the N2N method . Finally , we will introduce N2V and its implementation . 3 . 1 . Image Formation We see the generation of an image x = s + n as a draw from the joint distribution p ( s , n ) = p ( s ) p ( n | s ) . ( 1 ) We assume p ( s ) to be an arbitrary distribution satisfying p ( s i | s j ) 6 = p ( s i ) , ( 2 ) for two pixels i and j within a certain radius of each other . That is , the pixels s i of the signal are not statistically inde - pendent . With respect to the noise n , we assume a condi - tional distribution of the form p ( n | s ) = Y i p ( n i | s i ) . ( 3 ) That is , pixels values n i of the noise are conditionally inde - pendent given the signal . We furthermore assume the noise to be zero - mean E [ n i ] = 0 , ( 4 ) which leads to E [ x i ] = s i . ( 5 ) In other words , if we were to acquire multiple images with the same signal , but different realizations of noise and av - erage them , the result would approach the true signal . An 2131 example of this would be recording multiple photographs of a static scene using a ﬁxed tripod - mounted camera . 3 . 2 . Traditional Supervised Training We are now interested in training a CNN to implement a mapping from x to s . We will assume a fully convolu - tional network ( FCN ) [ 13 ] , taking one image as input and predicting another one as output . Here we want to take a slightly different but equivalent view on such a network . Every pixel prediction ˆ s i in the output of the CNN is has a certain receptive ﬁeld x RF ( i ) of input pixels , i . e . the set of pixels that inﬂuence the pixel prediction . A pixel’s receptive ﬁeld is usually a square patch around that pixel . Based on this consideration , we can also see our CNN as a function that takes a patch x RF ( i ) as input and outputs a prediction ˆ s i for the single pixel i located at the patch center . Following this view , the denoising of an entire im - age can be achieved by extracting overlapping patches and feeding them to the network one by one . Consequently , we can deﬁne the CNN as the function f ( x RF ( i ) ; θ ) = ˆ s i , ( 6 ) where θ denotes the vector of CNN parameters we would like to train . In traditional supervised training we are presented with a set of training pairs ( x j , s j ) , each consisting of a noisy input image x j and a clean ground truth target s j . By again applying our patch - based view of the CNN , we can see our training data as pairs ( x j RF ( i ) , s ji ) . Where x j RF ( i ) is a patch around pixel i , extracted from training input image x j , and s ji is the corresponding target pixel value , extracted from the ground truth image s j at same position . We now use these pairs to tune the parameters θ to minimize pixel - wise loss arg min θ X j X i L (cid:16) f ( x j RF ( i ) ; θ ) = ˆ s ji , s ji (cid:17) . ( 7 ) Here we consider the standard MSE loss L (cid:16) ˆ s ji , s ji (cid:17) = ( ˆ s ji − s ji ) 2 . ( 8 ) 3 . 3 . Noise2Noise Training Now let us consider the training procedure according to [ 12 ] . N2N allows us to cope without clean ground truth training data . Instead we start out with noisy image pairs ( x j , x ′ j ) , where x j = s j + n j and x ′ j = s j + n ′ j , ( 9 ) that is the two training images are identical up to their noise components n j and n ′ j , which are , in our image generation Target Prediction Input ( a ) ( b ) Figure 2 : A conventional network versus our proposed blind - spot network . ( a ) In the conventional network the pre - diction for an individual pixel depends an a square patch of input pixels , known as a pixel’s receptive ﬁeld ( pixels under blue cone ) . If we train such a network using the same noisy image as input and as target , the network will degenerate and simply learn the identity . ( b ) In a blind - spot network , as we propose it , the receptive ﬁeld of each pixel excludes the pixel itself , preventing it from learning the identity . We show that blind - spot networks can learn to remove pixel wise independent noise when they are trained on the same noisy images as input and target . model , just two independent samples from the same distri - bution ( see Eq . 3 ) . We can now again apply our patch - based perspective and view our training data as pairs ( x j RF ( i ) , x ′ ji ) consisting of a noisy input patch x j RF ( i ) , extracted from x j , and a noisy target x ′ ji , taken from x ′ j at the position i . As in traditional training , we tune our parameters to minimize a loss , sim - ilar to Eq . 7 , this time however using our noisy target x ′ ji instead of the ground truth signal s ji . Even though we are attempting to learn a mapping from a noisy input to a noisy target , the training will still converge to the correct solution . The key to this phenomenon lies in the fact that the expected value of the noisy input is equal to the clean signal [ 12 ] ( see Eq . 5 ) . 3 . 4 . Noise2Void Training Here , we go a step further . We propose to derive both parts of our training sample , the input and the target , from a single noisy training image x j . If we were to simply ex - tract a patch as input and use its center pixel as target , our network would just learn the identity , by directly mapping the value at the center of the input patch to the output ( see Figure 2 a ) . To understand how training from single noisy images is possible nonetheless , let us assume that we use a network architecture with a special receptive ﬁeld . We assume the receptive ﬁeld ˜ x RF ( i ) of this network to have a blind - spot in its center . The CNN prediction ˆ s i for a pixel is affected 2132 by all input pixels in a square neighborhood except for the input pixel x i at its very location . We term this type of network blind - spot network ( see Figure 2 b ) . A blind - spot network can be trained using any of the training schemes described above . Like with a normal net - work , we can apply the traditional training or N2N , using a clean target , or a noisy target respectively . The blind - spot network has a little bit less information available for its predictions , and we can expect its accuracy to be slightly impaired compared to a normal network . Considering how - ever that only one pixel out of the entire receptive ﬁeld is removed , we can assume it to still perform reasonably well . The essential advantage of the blind - spot architecture is its inability to learn the identity . Let us consider why this is the case . Since we assume the noise to be pixel - wise independent given the signal ( see Eq . 3 ) , the neighboring pixels carry no information about the value of n i . It is thus impossible for the network to produce an estimate that is better than its a priori expected value ( see Eq . 4 ) . The signal however is assumed to contain statistical de - pendencies ( see Eq . 2 ) . As a result , the network can still estimate the signal s i of a pixel by looking at its surround - ings . Consequently , a blind - spot network allows us to extract the input patch and target value from the same noisy training image . We can train it by minimizing the empirical risk arg min θ X j X i L (cid:16) f ( ˜ x j RF ( i ) ; θ ) , x ji (cid:17) . ( 10 ) Note that the target x ji , is just as good as the N2N target x ′ ji , which has to be extracted from a second noisy image . This becomes clear when we consider Eqs . 9 and 3 : The two target values x ji and x ′ ji have an equal signal s ji and their noise components are just two independent samples from the same distribution p ( n i | s ji ) . We have seen that a blind - spot network can in princi - ple be trained using only individual noisy training images . However , implementing such a network that can still oper - ate efﬁciently is not trivial . We propose a masking scheme to avoid this problem and achieve the same properties with any standard CNN : We replace the value in the center of each input patch with a randomly selected value form the surrounding area ( see supplementary material for details ) . This effectively erases the pixel’s information and prevents the network from learning the identity . 3 . 5 . Implementation Details If we implement the above training scheme naively , it is unfortunately still not very efﬁcient : We have to process an entire patch to calculate the gradients for a single out - put pixel . To mitigate this issue , we use the following ap - proximation technique : Given a noisy training image x i , we ( a ) ( b ) ( c ) Figure 3 : Blind - spot masking scheme used during N OISE 2V OID training . ( a ) A noisy training image . ( b ) A magniﬁed image patch from ( a ) . During N2V training , a randomly selected pixel is chosen ( blue rectangle ) and its intensity copied over to create a blind - spot ( red and striped square ) . This modiﬁed image is then used as input image during training . ( c ) The target patch corresponding to ( b ) . We use the original input with unmodiﬁed values also as target . The loss is only calculated for the blind - spot pixels we masked in ( b ) . randomly extract patches of size 64 × 64 pixels , which are bigger than our networks receptive ﬁeld ( see supplementary material for details ) . Within each patch we randomly select N pixels , using stratiﬁed sampling to avoid clustering . We then mask these pixels and use the original noisy input val - ues as targets at their position ( see Figure 3 ) . Further details on the masking scheme can be found in the supplementary note . We can now simultaneously calculate the gradients for all of them , while ignoring the rest of the predicted im - age . This is achieved using the standard Keras pipeline with a specialized loss function that is zero for all but the se - lected pixels . We use the CSBDeep framework [ 23 ] as basis for our implementation . Following the standard CSBDeep setup , we use a U - Net [ 15 ] architecture , to which we added batch normalization [ 8 ] before each activation function . 4 . Experiments We evaluate N OISE 2V OID on natural images , simulated biological image data , and acquired microscopy images . N2V results are then compared to results of traditional and N OISE 2N OISE training , as well as results of training - free denoising methods like BM3D , non - local means , and mean - and median ﬁlters . Please refer to the supplementary mate - rial for more details on all experiments . 4 . 1 . Denoising of BSD68 Data For the evaluation on natural image data we follow the example of [ 25 ] and take 400 gray scale images with 180 × 180 pixels as our training dataset . For testing we use the gray scale version of the BSD68 dataset . Noisy versions of all images are generated by adding zero mean Gaussian noise with standard deviation σ = 25 . Furthermore , we used data augmentation on the training dataset . More pre - cisely , we rotated each image three times by 90 ◦ and also 2133 Ground Truth B S D 68 Input BM3D PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 PSNR : 28 . 59 Traditional PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 PSNR : 29 . 06 N OISE 2N OISE PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 PSNR : 28 . 86 N OISE 2V OID PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 PSNR : 27 . 71 S i m u l a t e d D a t a PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 29 . 96 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 56 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 43 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 PSNR : 32 . 28 ? Does not exist . cr yo - TE M ➤ ➤➤ ➤ Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s Runtime : ~ 33 . 2s ➤ ➤➤ ➤ ∅ Clean target not available . Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s ➤ ➤➤ ➤ Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s Runtime : ~ 1 . 3s ➤ ➤➤ ➤ ? Does not exist . C T C - M S C Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s Runtime : ~ 4 . 6s ∅ Clean target not available . ∅ Noisy target not available . Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s ? Does not exist . C T C - N 2 D H Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s Runtime : ~ 5 . 2s ∅ Clean target not available . ∅ Noisy target not available . Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Runtime : ~ 0 . 1s Figure 4 : Results and average PSNR values obtained by BM3D , traditionally trained , N2N trained , and N2V trained de - noising networks . For BSD68 data and simulated data all methods are applicable . For cryo - TEM data ground truth images are unobtainable . Since pairs of noisy images are available , we can still perform N OISE 2N OISE training . Red , yellow , and blue arrowheads indicate an ice artifact , two tubulin protoﬁlaments that are known to be 4 nm apart , and a 10 nm gold bead , respectively . For the CTC - MSC and CTC - N2DH data only single noisy images exist . Hence , neither traditional nor N2N training is applicable , while our proposed training scheme can still be applied . added all mirrored versions . During training we draw ran - dom 64 × 64 pixel patches from this augmented training dataset . The network architecture we use for all BSD68 exper - iments is a U - Net [ 15 ] with depth 2 , kernel size 3 , batch normalization , and a linear activation function in the last layer . The network has 96 feature maps on the initial level , which get doubled while the network gets deeper . We use a learning rate of 0 . 0004 and the default CSBDeep learning rate schedule , halving the learning rate when a plateau on the validation loss is detected . We used batch size 128 for traditional training and batch size 16 for N OISE 2N OISE , where we found that a larger batch leads to slightly diminished results . For N OISE 2V OID training we use a batch size of 128 and simultaneously ma - nipulate N = 64 pixels per input patch ( see Section 3 . 5 ) , as before with an initial learning rate of 0 . 0004 . In the ﬁrst row of Figure 4 , we compare our results to the ones obtained by BM3D , traditional training , and N OISE 2N OISE training . We report the average PSNR num - bers on each dataset . As mentioned earlier , N2V is not ex - pected to outperform other training methods , as it can utilize less information for its prediction . Still , here we observe that the denoising performance of N2V drops moderately below the performance of BM3D ( which is not the case for other data ) . 4 . 2 . Denoising of Simulated Microscopy Data The acquisition of close to ground truth quality mi - croscopy data is either impossible or at the very least , difﬁ - cult and expensive . Since we need ground truth data to com - pute desired PSNR values , we decided to use a simulated dataset for our second set of experiments . To this end , we simulated membrane labeled cells epithelia and mimicked 2134 the typical image degradation of ﬂuorescence microscopy by ﬁrst applying Poisson noise and then adding zero mean Gaussian noise . We used this simulation scheme to gener - ate high - SNR ground truth images and two corresponding low - SNR input images . This data enables us to perform tra - ditional , N2N , as well as N2V training . We used the same data augmentation scheme as described in Section 4 . 1 . The network architecture we use for all experiments on simulated data is a U - Net [ 15 ] of depth 2 , kernel size 5 , batch norm , 32 initial feature maps , and a linear activation function in the last layer . Traditional and N OISE 2N OISE training was performed with batch size 16 and an initial learning rate of 0 . 0004 . The N OISE 2V OID training was performed with a batch size of 128 . We simultaneously ma - nipulate N = 64 pixels per input patch ( see Section 3 . 5 ) . We again use the standard CSBDeep learning rate schedule for all three training methods . In the second row of Figure 4 one can appreciate the denoising quality of N OISE 2V OID training , which reaches virtually the same quality as traditional and N OISE 2N OISE training . All trained networks clearly outperform the results obtained by BM3D . 4 . 3 . Denoising of Real Microscopy Data As mentioned in the previous section , ground truth qual - ity microscopy data is typically not available . Hence , we can no longer compute PSNR values . The network architecture we use for all experiments on real microscopy data is a U - Net [ 15 ] of depth 2 , kernel size 3 , batch norm , 32 initial feature maps , and a linear activa - tion function in the last layer . For an efﬁcient training of N OISE 2V OID we simultaneously manipulate N = 64 pix - els per input patch ( see Section 3 . 5 ) . We use a batch size of 128 and a initial learning rate of 0 . 0004 . For all three tasks we extracted random patches of 64 × 64 pixels and augmented them as described in previous sections . 4 . 3 . 1 Cryo - TEM Data In cryo - TEM , the acquisition of high - SNR images is not possible due to beam induced damage [ 10 ] . Buchholz et al . show in [ 3 ] how N OISE 2N OISE training can be applied to data acquired with a direct electron detector . To enable a qualitative assessment , we applied N2V to the same data as in [ 3 ] . In the third row of Figure 4 , we show the raw image data , results obtained by BM3D , N OISE 2N OISE results of [ 3 ] , and our N OISE 2V OID results . The runtime of both trained methods is roughly equal and about 25 times faster then the one of BM3D . For better orientation we marked some known structures in the shown cryo - TEM image ( see ﬁgure caption for details ) . Unlike BM3D , the N2V trained net - work is able to preserve these as good as the N2N baseline . 4 . 3 . 2 Fluorescence Microscopy Data Finally , we tested N OISE 2V OID on ﬂuorescence mi - croscopy data from the Cell Tracking Challenge . More speciﬁcally , we used the datasets Fluo - C2DL - MSC ( CTC - MSC ) and Fluo - N2DH - GOWT1 ( CTC - N2DH ) . As before , no ground truth images or second noisy images are avail - able . Hence , only BM3D and N2V training can be applied to this data . In the last two rows of Figure 4 , we compare our results to BM3D . In the absence of ground truth data , we can only judge the results visually . We ﬁnd that the N2V trained net - work gives subjectively smooth and appealing result , while requiring only a fraction of the BM3D runtime . ( a ) ➤ ( b ) ➤ ( c ) ➤ ( d ) ( e ) ( f ) Figure 5 : Failure cases of N2V trained networks . ( a ) A crop from the ground truth test image with the largest indi - vidual pixel error ( indicated by red arrow ) . ( b ) Result of a traditionally trained network on the same image . ( c ) Result of our N2V trained network . The network fails to predict this bright and isolated pixel . ( d ) A crop from the ground truth test image with the largest total error . ( e ) Result of a traditionally trained network on the same image . ( f ) Result of our N2V trained network . Both networks are not able to preserve the grainy structure of the image , but the N2V trained network loses more high - frequency detail . 4 . 4 . Errors and Limitations We want to start this section by showing extreme error cases of N2V trained network predictions on real images ( for which our training method performs least convincing ) . Figure 5 shows the ground truth image , and prediction re - sults of traditionally trained and N2V trained networks . While the upper row contains the image with the largest squared single pixel error , the lower row shows the image with the largest sum of squared pixel errors . We see these errors as an excellent illustration , showing a limitation of the N2V method . One of the underlying as - sumptions of N2V is the predictability of the signal s ( see 2135 ( a ) ( b ) ( c ) ( d ) ∅ Clean target not available . ( e ) Figure 6 : Effect of structured noise on N2V trained net - work predictions . Structured noise violates our assumption that noise is pixel - independent ( see also Eq . 3 ) . ( a ) A pho - tograph corrupted by structured noise . The hidden checker - board pattern is barely visible . ( b ) The denoised result of a traditionally trained CNN . ( c ) The denoised result of an N2V trained CNN . The independent components of the noise are removed , but the structured components remain . ( d ) Structured noise in real microscopy data . ( e ) The de - noised result of an N2V trained CNN . A hidden pattern in the noise is revealed . Note that due to the lacking training data , it is not possible to use N2N or the traditional training scheme in this case . Eq . 2 ) . Both test images shown in Figure 5 include high ir - regularities , that are difﬁcult to predict . The more difﬁcult it is to predict a pixel’s signal from its surroundings the more errors are expected to appear in N2V predictions . This is of course true for traditional training and N2N as well . How - ever , while these methods can utilize the value in the center pixel of the receptive ﬁeld , this value is blocked for N2V . In Figure 6 , we illustrate another limitation of our method . N2V cannot distinguish between the signal and structured noise that violates the assumption of pixel - wise independence ( see Eq . 3 ) . We demonstrate this behaviour using artiﬁcially generated structured noise applied to an image . The N2V trained CNN removes the unpredictable components of the noise , but reveals the hidden pattern . Interestingly , we ﬁnd the same phenomenon in real mi - croscopy data from the Fluo - C2DL - MSC dataset . Denois - ing with a N2V trained CNN reveals a systematic error of the imaging system , visible as a striped pattern . 4 . 5 . Performance over Various Noise Levels We additionally ran our method and multiple baselines , including mean and median ﬁlters , as well as the classical non - local means [ 2 ] , on the BSD68 dataset using various levels of noise . To ﬁnd the optimal parameter h for non - local means we performed a grid search . We also include a 20 30 40 50 60 70 Noise std . 22 24 26 28 30 32 34 A v g . P S N R dnCNN * Traditional * N2N BM3D N2V Non - local means Best mean filter Best median filter Mean filter ( 5x5 ) Median filter ( 5x5 ) N2V Figure 7 : Performance of N2V on the BSD68 dataset com - pared to various baselines . Left : Average PSNR values as a function of the amount of added Gaussian noise . We con - sider square mean and median ﬁlters of 3 , 5 , and 7 pixels width / height , and show the best avg . PSNR for each noise level . ∗ : Method uses ground truth for training ; † : uses noisy image pairs ; ‡ : uses only single noisy images . Right : Qualitative results of the best performing mean ﬁler , median ﬁlter , and N2V on an image with Gaussian noise ( std . 40 ) . comparison to DnCNN using the numbers reported in [ 25 ] . All results can be found in Figure 7 . 5 . Conclusion We have introduced N OISE 2V OID , a novel training scheme that only requires single noisy acquisitions to train denoising CNNs . We have demonstrated the applicability of N2V on a variety of imaging modalities i . e . photography , ﬂuorescence microscopy , and cryo - Transmission Electron Microscopy . As long as our initial assumptions of a pre - dictable signal and pixel - wise independent noise are met , N2V trained networks can compete with traditionally and N2N trained networks . Additionally , we have analyzed the behaviour of N2V training when these assumptions are vi - olated . We believe that the N OISE 2V OID training scheme , as we propose it here , will allow us to train powerful denoising networks . We have shown multiple examples how denois - ing networks can be trained on the same body of data which is to be processed in the ﬁrst place . Hence , N2V train - ing will open the doors to a plethora of applications , i . e . on biomedical image data . Acknowledgements We thank Uwe Schmidt , Martin Weigert , Alexander Di - brov , and Vladimir Ulman for the helpful discussions and for their assistance in data preparation . We thank Tobias Pietzsch for proof reading . 2136 References [ 1 ] J . Batson and L . Royer . Noise2self : Blind denoising by self - supervision . arXiv preprint arXiv : 1901 . 11365 , 2019 . 2 [ 2 ] A . Buades , B . Coll , and J . - M . Morel . A non - local algorithm for image denoising . In CVPR , 2005 . 2 , 3 , 8 [ 3 ] T . - O . Buchholz , M . Jordan , G . Pigino , and F . Jug . Cryo - care : Content - aware image restoration for cryo - transmission electron microscopy data . arXiv preprint arXiv : 1810 . 05420 , 2018 . 2 , 7 [ 4 ] J . Chen , J . Chen , H . Chao , and M . Yang . Image blind denois - ing with generative adversarial network based noise model - ing . In CVPR , pages 3155 – 3164 , 2018 . 3 [ 5 ] K . Dabov , A . Foi , V . Katkovnik , and K . Egiazarian . Image denoising by sparse 3 - d transform - domain collaborative ﬁl - tering . IEEE Transactions on image processing , 16 ( 8 ) : 2080 – 2095 , 2007 . 2 , 3 [ 6 ] S . Guo , Z . Yan , K . Zhang , W . Zuo , and L . Zhang . Toward convolutional blind denoising of real photographs . arXiv preprint arXiv : 1807 . 04686 , 2018 . 1 [ 7 ] K . He , X . Zhang , S . Ren , and J . Sun . Deep residual learning for image recognition . In CVPR , pages 770 – 778 , 2016 . 2 [ 8 ] S . Ioffe and C . Szegedy . Batch normalization : Accelerating deep network training by reducing internal covariate shift . arXiv preprint arXiv : 1502 . 03167 , 2015 . 5 [ 9 ] V . Jain and S . Seung . Natural image denoising with convo - lutional networks . In Advances in Neural Information Pro - cessing Systems , pages 769 – 776 , 2009 . 2 [ 10 ] E . Knapek and J . Dubochet . Beam damage to organic ma - terial is considerably reduced in cryo - electron microscopy . Journal of molecular biology , 141 ( 2 ) : 147 – 161 , 1980 . 7 [ 11 ] S . Lefkimmiatis . Universal denoising networks : A novel cnn architecture for image denoising . In CVPR , pages 3204 – 3213 , 2018 . 1 [ 12 ] J . Lehtinen , J . Munkberg , J . Hasselgren , S . Laine , T . Kar - ras , M . Aittala , and T . Aila . Noise2Noise : Learning image restoration without clean data . In ICML , pages 2965 – 2974 , 2018 . 1 , 2 , 4 [ 13 ] J . Long , E . Shelhamer , and T . Darrell . Fully convolutional networks for semantic segmentation . In CVPR , pages 3431 – 3440 , 2015 . 4 [ 14 ] X . Mao , C . Shen , and Y . - B . Yang . Image restoration us - ing very deep convolutional encoder - decoder networks with symmetric skip connections . In Advances in neural informa - tion processing systems , pages 2802 – 2810 , 2016 . 1 , 2 [ 15 ] O . Ronneberger , P . Fischer , and T . Brox . U - net : Convolu - tional networks for biomedical image segmentation . In MIC - CAI , pages 234 – 241 . Springer , 2015 . 3 , 5 , 6 , 7 [ 16 ] S . Roth and M . J . Black . Fields of experts : A framework for learning image priors . In CVPR , volume 2 , pages 860 – 867 . IEEE , 2005 . 1 [ 17 ] S . Roth and M . J . Black . Fields of experts . International Journal of Computer Vision , 82 ( 2 ) : 205 , 2009 . 2 [ 18 ] Y . Tai , J . Yang , X . Liu , and C . Xu . Memnet : A persis - tent memory network for image restoration . In CVPR , pages 4539 – 4547 , 2017 . 1 , 2 , 3 [ 19 ] M . F . Tappen , C . Liu , E . H . Adelson , and W . T . Freeman . Learning gaussian conditional random ﬁelds for low - level vi - sion . In CVPR , pages 1 – 8 . IEEE , 2007 . 1 [ 20 ] V . Ulman , M . Maˇska , K . E . Magnusson , O . Ronneberger , C . Haubold , N . Harder , P . Matula , P . Matula , D . Svoboda , M . Radojevic , et al . An objective comparison of cell - tracking algorithms . Nature methods , 14 ( 12 ) : 1141 , 2017 . 2 [ 21 ] D . Ulyanov , A . Vedaldi , and V . S . Lempitsky . Deep image prior . CoRR , abs / 1711 . 10925 , 2017 . 3 [ 22 ] A . Van Den Oord , N . Kalchbrenner , and K . Kavukcuoglu . Pixel recurrent neural networks . In ICML , pages 1747 – 1756 . JMLR . org , 2016 . 3 [ 23 ] M . Weigert , L . Royer , F . Jug , and G . Myers . Isotropic recon - struction of 3d ﬂuorescence microscopy images using con - volutional neural networks . In M . Descoteaux , L . Maier - Hein , A . Franz , P . Jannin , D . L . Collins , and S . Duchesne , editors , MICCAI , pages 126 – 134 , Cham , 2017 . Springer In - ternational Publishing . 1 , 5 [ 24 ] M . Weigert , U . Schmidt , T . Boothe , A . M¨uller , A . Dibrov , A . Jain , B . Wilhelm , D . Schmidt , C . Broaddus , S . Cul - ley , M . Rocha - Martins , F . Segovia - Miranda , C . Norden , R . Henriques , M . Zerial , M . Solimena , J . Rink , P . Tomancak , L . Royer , F . Jug , and E . W . Myers . Content - aware image restoration : Pushing the limits of ﬂuorescence microscopy . Nature Methods , 2018 . 1 , 2 , 3 [ 25 ] K . Zhang , W . Zuo , Y . Chen , D . Meng , and L . Zhang . Be - yond a gaussian denoiser : Residual learning of deep cnn for image denoising . IEEE Transactions on Image Processing , 26 ( 7 ) : 3142 – 3155 , 2017 . 1 , 2 , 3 , 5 , 8 [ 26 ] K . Zhang , W . Zuo , and L . Zhang . Ffdnet : Toward a fast and ﬂexible solution for cnn based image denoising . IEEE Transactions on Image Processing , 2018 . 1 [ 27 ] M . Zontak and M . Irani . Internal statistics of a single natural image . In CVPR , pages 977 – 984 . IEEE , 2011 . 3 2137