Multidimensional Fairness in Paper Recommendation Reem Alsaffar 1 , Susan Gauch 1 and Hiba Al - Kawaz 2 1 University of Arkansas , Fayetteville , AR , USA 2 Baghdad University , Baghdad , Iraq 1 { rbalsaff , sgauch } @ uark . edu 2 hiba . m @ csw . uobaghdad . edu . iq Abstract : To prevent potential bias in the paper review and selection process for conferences and journals , most include double blind review . Despite this , studies show that bias still exists . Recommendation algorithms for paper review also may have implicit bias . We offer three fair methods that specifically take into account author diversity in paper recommendation to address this . Our methods provide fair outcomes across many protected variables concurrently , in contrast to typical fair algorithms that only use one protected variable . Five demographic characteristics‚Äîgender , ethnicity , career stage , university rank , and geolocation‚Äîare included in our multidimensional author profiles . The Overall Diversity approach uses a score for overall diversity to rank publications . The Round Robin Diversity technique chooses papers from authors who are members of each protected group in turn , whereas the Multifaceted Diversity method chooses papers that initially fill the demographic feature with the highest importance . We compare the effectiveness of author diversity profiles based on Boolean and continuous - valued features . By selecting papers from a pool of SIGCHI 2017 , DIS 2017 , and IUI 2017 papers , we recommend papers for SIGCHI 2017 and evaluate these algorithms using the user profiles . We contrast the papers that were recommended with those that were selected by the conference . We find that utilizing profiles with either Boolean or continuous feature values , all three techniques boost diversity while just slightly decreasing utility or not decreasing . By choosing authors who are 42 . 50 % more diverse and with a 2 . 45 % boost in utility , our best technique , Multifaceted Diversity , suggests a set of papers that match demographic parity . The selection of grant proposals , conference papers , journal articles , and other academic duties might all use this strategy . Keywords : User Profiling , Paper Recommendation , Diversity and Fairness 1 Introduction Our modern world is very diverse and individuals and institutions strive for inclusion . Despite gain in legislation and attitudes , there is still discrimination against people because of their race , color , gender , religion , national origin , disability or and age [ 1 ] . These groups have legal protection within the United States , but discrimination , conscious or unconscious , still exists throughout society [ 38 , 13 ] . Unfortunately , academia is no exception as evidenced by the fact that only 38 % of tenure - track positions were awarded to women despite women receiving more than 50 % of the Ph . D . ‚Äôs awarded [ 15 ] . Computer Science is a long way from achieving diversity . [ 45 ] and [ 10 ] document the fact that only 18 % of graduates in Computer Science are women and also only 18 % are minorities . These statistics are reflected in the lack of diverse speakers at Computer Science conferences and in the demographics of conference attendees where minorities are underrepresented [ 28 ] . Racial , gender , and other types of discrimination among reviewers , editors and program committee might lead to bias in choosing papers for publishing [ 33 ] which has led to SIGCHI , one of the highest impact ACM conferences , announcing in 2020 an explicit goal of increasing the diversity of its Program Committee [ 40 ] . Merely using a double - blind review process fails to solve the problem of discrimination [ 9 , 30 ] . Reviewers can often infer the authors of papers from previous publications or readily available electronic preprints even when using double - blind review , so the review process is not actually double - blind [ 1 , 36 ] . Our approach is based on building a profile for each paper that reflects the paper‚Äôs overall quality and also models the diversity of the paper authors . The demographic features most frequently identified as a source of bias are Gender , Ethnicity [ 7 ] , Career Stage [ 31 ] , University Rank [ 16 ] and geolocation [ 26 ] , thus these are the features we use in our demographic user profiles . Our fair recommender system then uses this multi - dimensional profile to recommend papers for inclusion in the conference balancing the goals of increasing the diversity of the authors whose work is selected for presentation while minimizing any decrease in the quality of papers presented . In our previous paper , we proposed two methods that use multiple attributes when picking a subset of authors from the pool to achieve demographic parity . We incorporated the diversity and the quality of the authors during the selection process to minimize the utility loss and maximize the diversity . We applied two different feature weighting schemes , Boolean and continuous , in demographic profiles used to increase fairness in paper recommendation . In this paper , we present three fair recommendation algorithms that balance two aspects of a paper , its quality and the authors‚Äô demographic features , when recommending papers to be selected by the conference . Because information about the review process is generally confidential , we simulate the results of the review process by creating pools of papers from related conferences within a specific field that have different impact factors . The highest impact factor conference papers will play the role of the papers that are rated most highly by the reviewers , the middle impact factor conference papers those with the second best reviews , and papers published at the conference with the lowest of the three impact factors will be treated as papers with lower reviews . Our main contributions in this work are : ‚Ä¢ Modelling author demographics using profiles that contain multiple demographic features . ‚Ä¢ Developing and evaluating fair recommendation algorithms for paper selections that balance quality and diversity . ‚Ä¢ Achieving demographic parity between the accepted authors with the pool of all authors . 2 Related Work We begin by discussing aspects of bias in academia , then we review previous work on the construction of demographic profiles for users . Finally , we summarize recent approaches to incorporate fairness in algorithmic processes . 2 . 1 Bias Bias in Academia : Bias in academic research can be seen when one outcome or result is preferred over others during the testing or sampling phase , and also during any research stage , i . e . , design , data collection , analysis , testing and publication [ 37 ] . Bornmann and Daniel discuss the evidence that gender , major field of study , and institutional affiliation caused bias in the committee decisions when awarding doctoral and post - doctoral research fellowships [ 5 ] . Flaherty investigates discrimination in the US college faculty focusing on ethnicity . The results showed that only 6 % of professors are black versus 76 % of white professors [ 17 ] . More recently , an article published by researchers from Stanford Graduate School of Education in 2021 showed that , in the United States , more doctoral degrees have been earned by women than men . Despite this , women are still less likely than men to receive tenured positions , have their research published , or obtain leadership roles in academia . After analyzing one million doctoral dissertations from US universities , they found that the authors whose topics are related to women or who used methodologies that refer to women have decreased career prospects versus those related to men [ 51 ] . Bias in Peer Review : Several studies have that the lack of fairness in the peer review process has a major impact on which papers are accepted to conferences and journals [ 41 ] . Reviewers tend to accept the papers whose authors have the same gender and are from the same region [ 33 ] . Double - blind reviews do not entirely solve this issue and some researchers demonstrate that bias still exists in the reviewing process . For example , Cox et al . concludes that the double - blind review did not increase the proportion of females significantly compared with the single - blind review [ 10 ] . 2 . 2 Fairness Demographic Profiling : User profiling can be used to understand the users‚Äô intentions and develop personalized services to better assist users [ 18 ] . Recently , researchers are incorporating demographic user profiles in recommender systems hoping to limit unfairness and discrimination within the recommendation process [ 29 , 14 ] . Within academia , the demographic attributes of age , gender , race , and education are widely used and researchers often infer these features from the user‚Äôs name [ 8 , 39 ] . Demographic Parity : To achieve fairness , many approaches aim for demographic parity , which is when members of the protected groups and non - protected groups are equally likely to receive positive outcomes . However , this requirement generally causes a decrease in utility . Yang and Stoyanovich focus on developing new metrics to measure the lack of demographic parity in ranked outputs [ 46 ] . [ 47 ] and [ 48 ] address the problem of improving fairness in the ranking problem over a single binary type attribute when selecting a subset of candidates from a large pool while we work with multiple features at the same time . It maximizes utility subject to a group fairness criteria and ensuring demographic parity at the same time . We extended these works by using multiple attributes when picking a subset of authors from the pool to achieve demographic parity . We also incorporated the diversity and the quality of the authors during the selection process to minimize the utility loss and maximize the diversity . A study by [ 52 ] extended these works by using multiple attributes when picking a subset of authors from the pool to achieve demographic parity . Using Boolean profiles , they also incorporated the diversity and the quality of the authors during the selection process to minimize the utility loss and maximize the diversity . Further extensions [ 53 ] compared two different feature weighting schemes , Boolean and continuous , in demographic profiles used to increase fairness in paper recommendation [ 52 ] . Recently , some authors have been working on Generalized Demographic Parity ( GDP ) which is a group fairness metric for continuous and discrete features , to make fairness metrics more accurate . [ 54 ] proposed their method by displaying the relationship between joint and product margin distributions distance . They demonstrate two methods named histogram and kernel with linear computation complexity . Their experiment showed that GDP regularizer can reduce bias more accurately . Fairness in Machine Learning : As more and more algorithms make financial , scholastic , and career decisions , it is very important that the algorithms to not perpetuate bias towards demographic subgroups . Many investigations show that machine learning approaches can lead to biased decisions [ 12 ] . Thus , researchers are working to improve classifiers so they can achieve good utility in classification for some purpose while decreasing discrimination that can happen against the protected groups by designing a predictor with providing suitable data representation [ 22 , 49 ] . Other researchers attempt to improve fairness by training machine learning models without knowing the protected group memberships . In particular , [ 55 ] proposed an Adversarially Reweighted Learning ( ARL ) approach to improve the utility for the least represented protected groups when they train the model . During the training stage , they relied more on the non - protected features and task labels to identify unfair biases and train their model to improve fairness . Their solution outperformed state - of - the - art alternatives across a variety of datasets . Paper Assignment Fairness : Some researchers have explored and measured fairness when choosing a suitable reviewer to review a paper . [ 32 ] and [ 42 ] focus on fairness and statistical accuracy in assigning papers to reviewers in conferences during the peer review process . Most of these studies propose methods to improve the quality of the reviewer assignment process . We contribute to this area by creating author profiles with multiple demographic features and using them in new fair recommendation algorithms to achieve demographic parity when selecting papers for inclusion in a conference . 3 Demographic Profile Construction We first build a demographic profile for each paper by modeling the demographic features for the paper‚Äôs authors so that this information is available during paper selection . Some demographic features are protected attributes , e . g . , gender , race , that qualify for special protection from discrimination by law [ 25 ] . In this section , we will describe how we collect the demographic features for each author in our papers pool and then how we build the paper profile . 3 . 1 Data Extraction For a given paper , our goal is to extract five demographic features that are Gender , Race , University Rank , Career Stage , and Geolocation for its author ( s ) then combine them to create a profile for the paper . Each feature is mapped to a Boolean value , either 1 ( true ) or 0 ( false ) based on that paper‚Äôs author ( s ) membership in the protected group . We then extend our approach beyond current approaches by modeling demographics with continuous - valued features ( each feature is mapped to a value between 0 and 1 ) to represent the complement of the proportion of each feature among computer science professionals . Table 1 outlines the protected and non - protected categories for each of our demographic features . Table 1 . Demographic Features and Categories Feature Protected / Non - Protected Category Gender Female / Male Ethnicity Non - White / White Geo - Location Developing / Developed ( by country ) EPSCoR / Non - EPSCoR ( by state in USA ) Career Stage Junior / Senior UniversityRank Less than or equal mean / more than mean Gender : To gather information about an author‚Äôs gender , we use the NamSor API v2 , a data mining tool that uses a person‚Äôs first and last names from different languages , alphabets , countries , and regions to infer their gender . The software processed more than 4 billion names with high precision and recall which are 98 . 41 % and 99 . 28 % respectively . The tool returns a value between - 1 and + 1 indicates that the name is male if it is close to + 1 and female if it is close to - 1 . The accuracy of gender prediction using this tool is close to 99 % [ 3 ] . After collecting each author‚Äôs gender , we map females to 1 since they are the protected group and males to 0 . To calculate the continuous value for gender , we map females and males to the complement of their participation in computer science . Women are considered a protected group since they make up only 27 % of professionals in the computer science field [ 4 ] . Ethnicity : To predict ethnicity , we again use the NamSor tool , a web API that is used to predict ethnicity from the first and last names with the limitation of 500 names / month . It returns ethnicity as one of five values : { White , Black , Hispanic , Asian , other } [ 3 ] . Non - whites are considered a protected group since they make up less than 40 % of professionals in the computer science field [ 4 ] . The continuous values for Ethnicity were calculated by mapping each category to the complement of its proportion in the population of Computer Science professionals from [ 51 ] ( Computer , engineering , & science occupations , 2020 ) . Whites comprise 70 . 46 % of computer science professionals , so they are mapped to 0 . 2954 . Similarly , Black , Asian , Hispanic and others are assigned to 0 . 9295 , 0 . 8237 , 0 . 9281 , and 0 . 7400 respectively . Career Stage : In order to extract the academic position for each author , we utilize the researcher‚Äôs Google Scholar pages [ 19 ] or their homepages . Researchers whose primary appointment is within industry are omitted from our data set . The results are then mapped to Boolean values , 0 if they are a senior researcher ( Distinguished Professor , Professor , Associate Professor ) and 1 if they are a junior researcher ( Assistant Professor , Postdoc , Student ) . To calculate the continuous values for this feature , we map to six values equally distributed between [ 0 , . . , 1 . 0 ] in increasing order by rank , i . e . , Distinguished Professor : 0 / 5 = 0 . 0 ; Professor : 1 / 5 = 0 . 2 ; . . . ; Student : 5 / 5 = 1 . 0 . Table 2 shows the values for each category . Table 2 . Career Stage Weight Allocation Position Weight Distinguished Professor 0 . 17 Professor 0 . 33 Associated Professor 0 . 50 Assistant Professor or Lecturer 0 . 67 Post - Doctoral or Research Fellow 0 . 83 Graduate Student 1 . 0 University Rank : Collecting this feature is done by extracting the institution‚Äôs name from the Google Scholar page for the author [ 19 ] or their home pages and mapping it to the World University Rankings obtained from [ 44 ] . We partition the authors into low - rank ( 1 ) or high - rank institutions ( 0 ) using the median value . Then , we normalize the raw value to a continuous value by dividing the university rank ( U r ) by the lowest university rank ( L r ) : ùëÖ ùê∂ = ùëà ùëü ùêø ùëü ( 1 ) Geolocation : We set the researcher‚Äôs geolocation ( country and state if inside the US ) based on information extracted from their institution‚Äôs home page using the university name that was extracted . If the author is working inside the United States , we extract the state name as well . We find the category of the country ( developed or developing ) by mapping the country to the tables of the developed and developing economies that offered by the UN [ 35 ] . Thus , the Geolocation Boolean value is assigned to 0 if the researcher is working in a developed country ( non - protected group ) and 1 if a developing country ( protected group ) . For those who live in the US , we use the EPSCOR ( Established Program to Simulate Competitive Research ) [ 34 ] to map the Geolocation to Boolean values . EPSCoR states which obtain less federal grant funding are the protected group with the value 1 and non - EPSCoR states values are 0 . To calculate the continuous value for the Geolocation , we use the complement values of Human Development Index ( HDI ) ranking [ 24 ] . The values are ranging from 0 . 957 to 0 . 394 and Table 3 shows a sample of these values . Table 3 . Countries HDI sample Country HDI Norway 0 . 957 Ireland 0 . 955 Iceland 0 . 949 Germany 0 . 947 Sweden 0 . 945 Australia 0 . 944 H - index : We extract the h - index for each author from their Google Scholar page so we can measure the conference utility in our evaluation . If the author doesn‚Äôt have a scholar page , we obtain their h - index using Harzing ' s Publish or Perish tool . This software calculates the h - index for the scholar using some impact metrics [ 23 ] . To conclude , each researcher has a demographic profile consists of five features ( gender , ethnicity , career stage , university rank , and geolocation ) . Each feature has a Boolean weight that represents whether or not the candidate is a member of the protected group for that feature and a continuous value to represent the complement of the proportion of each feature among computer science professionals . In addition , we collect the h - index for each researcher using either their Google Scholar profile or is calculated and we use it to evaluate the utility of each accepted papers list in our evaluation . 3 . 2 Paper Profile Formation We construct the demographic profile for each paper by combining the demographic profiles for all of the paper authors . Recall that each author has a Boolean value profile and a continuous value profile . Boolean : The paper profile is created by doing a bit - wise OR on the paper‚Äôs author profiles . Thus , the paper profile is 1 for a given demographic feature when any author is a member of that feature‚Äôs protected group . We considered summing the author profiles , but this would give preferential treatment to papers with more authors and normalizing the summed profile would penalize papers with many authors . Continuous : The paper‚Äôs demographic profile is created by selecting the maximum value for each feature among the paper authors‚Äô profiles . 3 . 3 Paper Quality Profiler There are several ways to measure a paper‚Äôs quality such as the number of citations of the paper , the reputation of the editorial committee for the publication venue , or the publication venue‚Äôs quality itself , often measured by Impact Factor ( IF ) [ 6 ] . Although the IF is not accurate for new venues that contain high quality papers with few citations , we use it as the basis of the quality profile for the papers in our research since the conferences in our dataset are all well - established [ 50 ] . We extract the Impact Factor ( IF ) for each paper‚Äôs conference from Guide2Research website published in 2019 [ 21 ] . The IF was calculated by using Google Scholar Metrics to find the highest h - index for the published papers in the last 5 years [ 20 ] . 3 . 4 Pool Distribution When applying our proposed methods as described below , we rely on reaching demographic parity during accomplishing our goal . This means that we select the papers such that the demographics of the accepted authors match those of the pool of candidates . To achieve this , we measure the proportion of participants for each feature in the pool and store them in a vector ( PoolParity ) . PoolParity = < GenderWt , EthnicityWt , CareerWt , UniversityWt , GeoWt > where each weight is the number of authors from that protected group normalized by the number of authors in the pool . 4 Approaches The next goal is maximizing the diversity of the conference by applying three different methods to select papers with respect to each features‚Äô distribution in the pool and achieving demographic parity . The reason is to get a list of papers that have more diverse people in the high rank conferences while keeping the level of quality the same or with a little drop . 4 . 1 Overall Diversity Method After creating paper demographic profiles as described in section ( 3 ) , paper diversity scores ( PDScore ) are calculated using formula ( 2 ) on the feature values : PDScore = ‚àë ùëì ùëñ 5ùëñ = 1 ( 2 ) where ùëì ùëñ is the value for each paper‚Äôs demographic feature ( i . e . , five features for each paper ) . Our first method to choose a diverse list of papers considers two different queues . The quality queue ( ùëÑùëû uality ) which contains the papers ranked by the Impact Factor ( IF ) as described in Section 3 . This gives preference to the papers ranked highest by the reviewers , in our case represented by papers that appeared in the most selective conference . The demographic queue ( ùëÑùëë emog ) which contains the ranked papers by PDScore . Next , we pick papers from the top of ( ùëÑùëë emog ) until satisfying the pool demographic parity for each feature then the remaining papers are added from the quality queue in order to meet the number of papers desired by the conference . Thus , as long as there are sufficient candidates in the pool , we are guaranteed to meet or exceed demographic parity for each protected group . Algorithm 1 : Overall Diversity 1 ùëÑùëû uality , ùëÑùëë emog ‚Üê Initialize two empty priority queues 2 PoolParity ‚Üê Initialize an empty vector 3 ùëÑùëû ‚Üê insert the papers and sort them based on Quality - Scores 4 for each feature : 5 PoolParity [ feature ] ‚Üê compute Demographic Parity 6 for each paper : 7 PDScore ‚Üê compute paper diversity score 8 add paper to ùëÑùëë emog and order them using PDScore 9 If 2 or more papers have same PDScore : 10 Sort papers using Quality - Score 11 while PoolParity Not satisfied : 12 Papers ‚Üê select a paper from top of ùëÑùëë emog 13 delete selected paper from ùëÑùëû uality 14 while # of conference papers not satisfied : 15 Papers ‚Üê select a paper from top of ùëÑùëû uality 4 . 2 Multi - Faceted Diversity Method The previous method selects papers based on the total diversity score for each paper . However , it does not guarantee that the selected authors from the protected groups are actually diverse . It might end up selecting papers that have high diversity scores but are all females from developing countries , for example , with no minority authors at all . To correct for this possibility , we extend the previous approach by creating five ranked queues ( one per feature ) and sorting the papers using one demographic feature at a time . Including the quality - ranked queue , we now have six queues total . Based on the pool demographics , we give the highest priority to the rarest features in the pool first , so we create the accepted papers list by selecting papers from the queues whose features have the fewest candidates in the pool until the demographic parity goal for those features is achieved . After satisfying demographic parity for all protected groups , the remaining papers are added in order from the quality queue . Algorithm 2 : Multi - Faceted Diversity 1 FeatureName ‚Üê List of five queue names , one per feature 2 for each feature in FeatureName : 3 DivQueue [ feature ] ‚Üê Initialize empty priority queue 4 ùëÑualityQueue ‚Üê Initialize an empty priority queue 5 PoolParity ‚Üê Initialize an empty vector 6 ùëÑ ualityQue ue ‚Üê insert papers and sort by Quality - Score 7 for each feature in FeatureName : 8 PoolParity [ feature ] ‚Üê compute Demographic Parity 9 for each paper : 10 PDScore ‚Üê compute paper diversity score 11 for each feature in FeatureName : 12 DivQueue [ feature ] ‚Üê add paper if this feature is 1 13 Sort papers based on Quality - Score 14 If 2 or more papers has the same Quality - Score : 15 Sort papers using PDScore 16 while PoolParity NOT empty : 17 LowFeature ‚Üê min ( PoolParity ) 18 while LowFeature Not reached demographic parity 19 Papers ‚Üê select top DivQueue [ LowFeature ] 20 delete selected paper from ùëÑ ualityQueue 21 delete LowFeature from DParity 22 while # of conference papers not satisfied : 23 Papers ‚Üê select a paper from top of ùëÑ ualityQueue 4 . 3 Round Robin Method Our third method employs the same demographic queues as the Multi - Faceted Diversity method , but visits the queues using a round robin algorithm . It specifically creates one priority queue per demographic feature and selects from each in a round - robin fashion until the list of papers is formed . Since the proposed demographic profiles model five features , we create five priority queues , each of which orders all the papers by one specific feature based on PDScore . For example , one queue orders the papers based on gender , whereas another would order the papers based on academic position . Once all queues have been sorted , we apply round - robin selection by picking the highest sorted profile from each queue . Accordingly , the selected profile is then eliminated from all queues to endure that the same profile will not be selected again . This process continues iteratively until the list of papers reaches the desired size . We use this approach with both Boolean and continuous weight diversity profiles . Algorithm 3 : Round Robin 1 FeatureName ‚Üê List of five queue names , one per feature 2 for each feature in FeatureName : 3 DivQueue [ feature ] ‚Üê Initialize empty priority queue 4 ùëÑualityQueue ‚Üê Initialize an empty priority queue 5 ùëÑualityQueue ‚Üê insert papers and sort by Quality - Score 6 for each paper : 7 PDScore ‚Üê compute paper diversity score 8 for each feature in FeatureName : 9 add profile to [ feature ] using PDScore [ feature ] as priority order 10 Papers ‚Üê empty list 11 While number of Papers < N : 12 feature ‚Üê FeatureName [ 0 ] 13 repeat : 14 paper ‚Üê get and remove profile from DivQueue [ feature ] 15 until paper is not in Papers 16 add paper to Papers 17 delete feature from FeatureName . 18 we now have N papers selected . 5 Experiment and Result We now introduce our dataset and describe the process of evaluating our algorithms . 5 . 1 Datasets For our driving problem , we focus on selecting papers for a high impact computer science conference from a pool of papers that vary in quality and demographics . To create pools of candidate papers that simulate the papers submitted to a conference , we select a trio of conferences based on several criteria : 1 ) the conferences should publish papers on related topics ; 2 ) the conferences should have varying levels of impact { very high , high and medium } mimicking submitted papers reviewed as high accept , accept , borderline accept ; 3 ) the conferences should have a reasonably large number of accepted papers and authors . Based on these criteria , we selected SIGCHI ( The ACM Conference on Human Factors in Computing Systems ) , DIS ( The ACM conference on Designing Interactive Systems ) , and IUI ( The ACM Conference where the Human - Computer Interaction ( HCI ) community meets the Artificial Intelligence community ) . The papers published in SIGCHI represent papers rated highly acceptable by SIGCHI reviewers , DIS papers represent papers rated acceptable by SIGCHI reviewers , and IUI papers represent papers rated borderline acceptable . Excluding authors from industry , we create a dataset for each conference that contains the accepted papers and their authors ( see Table 4 ) . This dataset contains 592 papers with 813 authors for which we demographic profiles . We will expand this work to other conferences in the future . Table 4 . Composition of Our Dataset [ 56 ] . Dataset Accepted Papers Authors Impact Factor SIGCHI17 351 435 87 DIS17 114 231 33 IUI17 64 147 27 The demographic distribution of the authors in each conference is summarized in Fig . 1 . These clearly illustrate each of the conferences had few authors from most of the protected groups with the lowest participation in the highest impact conference , SIGCHI , with gender being an exception . As an example , SIGCHI 2017 had only 8 . 28 % non - white authors , DIS 2017‚Äôs authors were only 16 . 45 % non - white , and IUI 2017 had 27 . 21 % non - white . Fig . 1 . Protected Group Membership of Authors for Three Current Conferences [ 56 ] . We define demographic parity as the participation rate for each of our demographic features in the pool created by combining the authors of all three conferences . Based on the 813 authors in our dataset , Table 5 presents the average participation in the pool for each feature and thus the demographic parity that is our goal . Table 5 . Demographic Participation from protected groups in Three Current Conferences [ 56 ] . Gender Ethnicity CStage URank Geoloc SIGCHI 45 . 01 % 7 . 69 % 52 . 14 % 25 . 64 % 8 . 26 % DIS 57 . 89 % 31 . 58 % 72 . 81 % 55 . 26 % 11 . 40 % IUI 39 . 06 % 56 . 25 % 76 . 56 % 28 . 13 % 26 . 56 % Average 47 . 07 % 18 . 71 % 59 . 55 % 32 . 33 % 11 . 15 % 5 . 2 Baseline and Metrics Baseline . Our baseline is the original list of papers that were chosen by the program committee for SIGCHI 2017 and were represented in the venue . As shown in Table 2 , the distribution of the protected groups in our baseline is : 45 . 01 % female , 7 . 69 % non - white , 52 . 14 % junior professors , 25 . 64 % authors from low ranked universities and 8 . 26 authors from developing countries . Metrics . We evaluate our algorithms‚Äô effectiveness by calculating Diversity Gain ( ùê∑ ùê∫ ) of our proposed set of papers versus the baseline : ùê∑ ùê∫ = ‚àë ùëÄùêºùëÅ ( 100 , ùúå ùê∫ùëñ ) ùëõùëñ = 1 ùëõ ( 3 ) 0 20 40 60 80 100 F Non - White Junior Low RankedU Developing EPSCoR nu m b e r o f a u t h o r s b y p e r c e n t SIGCH17 DIS17 IUI17 Pool where ùúå ùê∫ ùëñ is the relative percentage gain for each feature versus the baseline , divided by the total number of features ùëõ . Each feature‚Äôs diversity gain is capped at a maximum value of 100 to prevent a large gain in a single feature dominating the value . By choosing to maximize diversity , it is likely that the quality of the resulting papers will be slightly lower . To measure this drop in quality , we use the average h - index of the paper authors and compute the utility loss ( ùëàùêø ùëñ ) for each proposed list of papers using the following formula : ùëàùêø ùëñ = ùëà ùëè ‚Äì ùëà ùëÉùëó ùëà ùëè * 100 ( 4 ) where ùëà ùëÉ ùëñ is the utility of the proposed papers for conference i and ùëà ùëè is the utility of the baseline . We then compute the utility savings ( ùëå ùëñ ) of papers for conference i relative to the baseline as follows : ùëå ùëñ = 100 ‚àí ùëàùêø ùëñ ( 5 ) We compute the F measure [ 27 ] to examine the ability of our algorithms to balance diversity gain and utility savings : ùêπ = 2 ‚àó ùê∑ ùê∫ ‚àó ùëå ùëñ ùê∑ ùê∫ + ùëå ùëñ ( 6 ) In order to measure how far away from demographic parity our results are , we calculate the Euclidean Distance [ 11 ] between our selected papers and the pool : DemographicDistance = ‚àö‚àë ( ùêπ1 ùëñ ‚àí ùêπ2 ùëñ ) 5ùëñ = 1 2 ( 7 ) where F1 is the participation of each feature in the proposed list of papers to select and F2 is the feature‚Äôs participation in the pool . Finally , we normalized the distance values to obtain the similarity percentages between our results and the pool as shown in the formula below : DemographicSimilarity = 1 - DemographicDistance MaxD ( 8 ) where MaxD is the largest possible distance between two vectors in our feature space . To summarize the ability of the methods to balance the competing demands of increasing demographic parity and saving utility , we again apply the F measure using formula 6 calculated using DemographicSimilarity and ùëå ùëñ . 5 . 3 Results Our recommender system produces ranked list ( s ) from which we select to form the accepted papers list with the overarching goal of increasing the diversity in the papers . Both methods reported here select papers from a quality sorted queue and one or more demographic queue ( s ) . Whenever there are ties in a demographic queue , those papers are sorted by their quality score . 5 . 4 Comparison with the Baseline We report the differences between the accepted papers in SIGCHI 2017 and the accepted papers produced by the recommender system described in Section 4 using Boolean and Continuous profiles . Looking at Fig . 2 . , we can see that all algorithms succeeded in increasing the diversity in the recommended papers for acceptance across all demographic groups when using the Boolean profiles . However , it is obvious that Round Robin method produced the highest diversity in all the protected groups except gender . Fig . 2 . Improvement in Protected Group Participation between the SIGCHI2017 and our Paper Recommendation Algorithms when using Boolean Profiles . Fig . 3 represents the protected groups participation with the Continuous profiles when applying our proposed recommendation algorithms . We can see that all methods succeeded in increasing the diversity in the recommended papers for acceptance across all demographic groups . Fig . 3 . Improvement in Protected Group Participation between the SIGCHI2017 and our Paper Recommendation Algorithms when using Continuous Profiles . Table 6 compares the participation of the protected groups between the actual accepted papers for SIGCHI with the accepted papers proposed by our three methods , and demographic parity based on the participation of the protected groups in the pool of authors in our dataset . We can see that all algorithms increase the diversity of authors across all protected groups . With the exception of female researchers for the Boolean profile , the Round Robin algorithm increases participation among the protected groups more than the Multifaceted Diversity algorithm across all demographics . As expected , 0 20 40 60 80 100 F Non - white Junior Low Ranked U Developing A u t h o r s P e r c e n t SIG - Actual Overall Diversity Multi - Faceted Diversity RoundRobin 0 20 40 60 80 100 F Non - white Junior Low Ranked U Developing A u t h o r P e r c e n t SIG - Actual Overall Diversity Multi - Faceted Diversity Round _ Robin these diversity - based recommendation methods overcorrected by including more authors from the protected groups proportionally than in the pool as a whole . Feature SIGCHI Overall Diversity ( B ) Overall Diversity ( C ) Multi - Faceted ( B ) Multi - Faceted ( C ) Round Robin ( B ) Round Robin ( C ) Pool Female 45 . 01 % 62 . 96 % 50 . 71 % 56 . 13 % 48 . 15 % 61 . 82 % 62 . 39 % 47 . 07 % Non - White 7 . 69 % 23 . 08 % 25 . 36 % 18 . 80 % 24 . 50 % 28 . 21 % 28 . 77 % 18 . 71 % Junior 52 . 14 % 73 . 79 % 65 . 24 % 64 . 96 % 67 . 24 % 75 . 50 % 73 . 50 % 59 . 55 % Low Ranked University 25 . 64 % 42 . 45 % 39 . 03 % 35 . 90 % 37 . 32 % 46 . 44 % 48 . 43 % 32 . 33 % Develop Country 8 . 26 % 14 . 53 % 11 . 11 % 11 . 68 % 10 . 83 % 16 . 81 % 13 . 68 % 11 . 15 % The recommended papers are a mix of papers from the three conferences in our datasets in different proportions as described in Table 7 . The Multi - Faceted Diversity method selects the highest proportion of the recommended papers , 85 . 8 % ( Bool ) and 78 . 06 % ( Cont . ) , from the actual SIGCHI papers , but Overall Diversity and Round Robin also selects the majority of its papers , 75 . 5 % ( Bool ) , 62 . 11 % ( Cont . ) and 61 . 53 % ( Bool ) , 60 . 11 % ( Cont . ) respectively , from the original SIGCHI selected papers . We further observe that Overall Diversity and Multi - Faceted algorithms selected the majority of papers from the demographic queue ( s ) with only a few from the quality - sorted queue . The Overall Diversity method selected 67 . 24 % ( Bool ) and 66 . 67 % ( Cont . ) of its accepted papers from the demographic queue and only 32 . 76 % ( Bool ) and 33 . 33 % ( Cont . ) from the quality queue . In contrast , the Multi - Faceted Diversity method selected nearly all of its accepted papers , 92 . 88 % , from one of the five demographic queues , and only 7 . 12 % from the quality queue . Table 7 . Proportion of Recommended Papers from each Conference . We also compare the performance of our algorithms with respect to the quality of the resulting accepted papers . Table 8 summarizes the diversity gain ( ùê∑ ùê∫ ) , Utility Savings ( ùëå ùëñ ) , and F scores for the accepted papers proposed by each algorithm when using the Boolean and Continuous profiles . All methods obtained Diversity Gains of over 40 % for the proposed set of accepted papers , with the biggest gain occurring with the Round Overall Diversity ( B ) Overall Diversity ( C ) Multi - Faceted ( B ) Multi - Faceted ( C ) Round Robin ( B ) Round Robin ( C ) SIGCHI 265 ( 75 . 5 % ) 218 ( 62 . 11 % ) 301 ( 85 . 8 % ) 274 ( 78 . 06 % ) 216 ( 61 . 53 % ) 211 ( 60 . 11 % ) DIS 59 ( 16 . 8 % ) 87 ( 24 . 79 % ) 47 ( 13 . 4 % ) 61 ( 17 . 38 % ) 84 ( 23 . 93 % ) 90 ( 25 . 64 % ) IUI 27 ( 7 . 7 % ) 46 ( 13 . 11 % ) 3 ( 0 . 9 % ) 16 ( 4 . 56 % ) 51 ( 14 . 52 % ) 50 ( 14 . 25 % ) Papers # 351 351 351 351 351 351 Table 6 . Protected Group Participation for the recommender algorithms using Boolean and Continuous profiles . Robin algorithm . The gains in diversity occur with Utility Savings of 95 . 14 % ( B ) and 94 . 06 ( C ) for the Round Robin algorithm versus 93 . 47 % ( B ) , 97 . 52 % ( B ) and 97 . 52 % ( C ) , 102 . 45 % ( C ) for the Overall Diversity and Multi - Faceted Diversity algorithms respectively . Based on these results , we conclude that the Round Robbin algorithm outperforms the other two algorithms and when considering author demographics and aiming for demographic parity , the quality of the selected papers actually increased . Table 8 . diversity gain and utility savings for our algorithms versus the Baseline for Boolean and Continuous profiles . Overall Diversity Multi - Faceted Diversity Round Robin ùê∑ ùê∫ ( Bool ) 64 . 58 % 46 . 00 % 72 . 65 % Y i ( Bool ) 93 . 47 % 97 . 52 % 95 . 14 % F - score ( Bool ) 76 . 39 62 . 51 82 . 39 ùê∑ ùê∫ ( Cont . ) 44 . 90 % 42 . 50 % 66 . 80 % Y i ( Cont . ) 102 . 49 % 102 . 45 % 94 . 06 % F - score ( Cont ) 62 . 44 60 . 08 78 . 12 Diversity - based algorithms may overcorrect and result in reverse discrimination , or the diversity gains may all be in one subgroup while other underrepresented populations are ignored . Tables 9 and 10 show the results when evaluating our algorithms‚Äô ability to achieve demographic parity with Boolean and Continuous features , respectively . We observe that , based on this criteria , the Multi - faceted Diversity algorithm produces results closest to Demographic Parity , with 95 . 01 % similarity to the pool and a utility loss of just 2 . 48 % when using Boolean profiles . We further observe that the Multi - faceted method produces even better Demographic Parity of 95 . 12 % when using continuous - valued features and actually results in a 2 . 45 % increase in utility . This means that , by considering author diversity and aiming for demographic parity when selecting papers , the quality of the papers accepted to the conference could actually be improved . Table 9 . demographic parity similarity and utility savings for our algorithms versus the baseline ( Boolean ) . Method Demographic Similarity ùëå ùëñ F - score Overall Diversity 89 . 15 % 93 . 47 % 91 . 26 Multi - Faceted 95 . 01 % 97 . 52 % 96 . 24 Round Robin 87 . 40 % 95 . 14 % 91 . 11 Table 10 . demographic parity similarity and utility savings for our algorithms versus the baseline ( Continuous ) . Method Demographic Similarity ùëå ùëñ F - score Overall Diversity 94 . 80 % 102 . 49 % 98 . 27 Multi - Faceted 95 . 12 % 102 . 45 % 98 . 44 Round Robin 87 . 38 % 94 . 06 % 90 . 60 5 . 5 Discussion In the previous sections , we discuss the evaluation for our approaches by comparing them to the baseline using Diversity Gain ( ùê∑ ùê∫ ) , Utility Saving ( ùëå ùëñ ) , F - score metrics , and DemographicSimilarity . We compared three algorithms for our recommender system using Boolean and continuous weight features in the demographic profiles . 5 . 5 . 1 Diversity Gain Comparison Table 11 summarizes the results for all experiment with each algorithm on both profile weights , evaluated using Diversity Gain , Utility loss , and the F - measure . From this we can see that the Overall Diversity method with Boolean profiles maximized diversity gain and the Overall Diversity method with continuous weights minimizes the utility loss . However , the Overall Diversity method with Boolean weights produces the highest F - score that balances these . Table 11 . Diversity Gain and Utility Saving with Boolean and Continuous weights profiles Profile ùê∑ ùê∫ ùëå ùëñ F - score Overall Diversity Boolean 64 . 58 93 . 47 76 . 39 Continuous 44 . 90 102 . 49 62 . 44 Average 54 . 74 97 . 98 69 . 41 Multi - Faceted Diversity Boolean 46 . 00 97 . 52 62 . 51 Continuous 42 . 50 102 . 45 60 . 08 Average 44 . 25 99 . 98 61 . 29 Round Robin Boolean 72 . 65 95 . 14 82 . 39 Continuous 66 . 80 94 . 06 78 . 12 Average 69 . 73 94 . 60 80 . 26 Average Boolean 61 . 08 95 . 38 73 . 76 Continuous 51 . 40 99 . 67 66 . 88 All the methods Comparison Averaged over both feature weights , the Round Robin measure produces higher diversity gain , 69 . 73 % versus 44 . 25 % and 54 . 74 % . The Multi - Faceted Diversity method produces a smaller drop in utility on average , 0 . 02 % versus 2 . 02 % . However , when balancing diversity gain with utility drop , the Round Robin method still produces better results with an average F - score of 80 . 26 versus 69 . 41 and 61 . 29 for the Overall and Multi - Faceted approach . Boolean versus Continuous feature weights Averaged over the three methods , the Boolean profiles produces higher diversity gain , 61 . 08 % versus 51 . 40 % for Continuous . The Continuous profiles produces a little drop in utility on average , 0 . 33 % versus 4 . 62 % drop in utility for Boolean profiles . However , when balancing diversity gain with utility drop , the Boolean profiles still produces better results with an average F - score of 73 . 76 versus 66 . 88 for Continuous profiles . 5 . 5 . 2 Demographic Parity Comparison Table 12 summarizes the results for all experiment with each algorithm on both profile weights , evaluated using Demographic Similarity , Utility loss , and the F - measure . From this we can see that the Multi - Faceted method with Continuous profiles maximized Demographic Similarity and the Overall Diversity method with continuous weights minimized the loss in utility . In fact , it actually produced a gain in utility ! The Multi - Faceted method with Continuous weights also produces the highest F - score that balances these . Table 12 . Demographic Similarity with Boolean and Continuous weights profiles Profile Demographic Similarity ùëå ùëñ F - score Overall Diversity Boolean 89 . 15 93 . 47 91 . 26 Continuous 94 . 80 102 . 49 98 . 27 Average 91 . 97 97 . 98 94 . 76 Multi - Faceted Diversity Boolean 95 . 01 97 . 52 96 . 24 Continuous 95 . 12 102 . 45 98 . 44 Average 95 . 06 99 . 98 97 . 34 Round Robin Boolean 87 . 40 95 . 14 91 . 11 Continuous 87 . 38 94 . 06 90 . 60 Average 87 . 39 94 . 60 90 . 86 Average Boolean 90 . 53 95 . 38 92 . 87 Continuous 92 . 43 99 . 67 95 . 77 All the methods comparison Averaged over both feature weights , the Multi - Faceted Diversity measure produces higher Demographic Similarity , 95 . 06 % versus 91 . 97 % and 87 . 39 % for the other two methods . The Multi - Faceted Diversity method produces a smaller drop in utility on average , 0 . 02 % versus 2 . 02 % and 5 . 4 % . Also , when balancing diversity gain with utility drop , the Multi - Faceted Diversity method still produces better results with an average F - score of 97 . 34 % versus 94 . 76 % and 90 . 86 % for the Overall Diversity approach and Round Robin . Boolean versus Continuous feature weights Averaged over both methods , the Continuous profiles produces higher Demographic Similarity , 92 . 43 % versus 90 . 53 % for Boolean profiles . The Continuous profiles produces a little drop in utility on average , 0 . 33 % versus 4 . 62 % drop in utility for Boolean profiles . However , when balancing diversity gain with utility drop , the Continuous profiles still produces better results with an average F - score of 95 . 77 versus 92 . 87 for Boolean profiles . 6 Conclusion Our goal is maximizing diversity while reducing quality loss when recommending papers for inclusion in conferences . Thus , we provide new recommendation algorithms to select papers for publication in a conference . Our methods promote diversity by taking into account multiple demographic characteristics of authors in addition to paper quality . The majority of earlier research has concentrated on algorithms that ensure fairness based on a single Boolean attribute such as ethnicity , gender , or disability . In contrast , we profile the authors based on five attributes , i . e . , gender , ethnicity , career stage , university rank , and geographic region . Furthermore , we extend previous work by comparing Boolean with Continuous - valued feature weights . To compare our algorithms for a real - world task , we created a dataset of authors whose papers were chosen for publishing at computer science conferences with varying impact factors to simulate papers judged by reviewers at various levels of acceptability in order to show our methodology . We evaluated three algorithms : Overall Diversity , Multi - faceted Diversity , and Round Robin . The Overall Diversity method rates the papers based on an overall diversity score ; the Multi - Faceted Diversity method chooses papers that satisfy the highest - priority demographic attribute participation goal first ; and the Round Robin approach selects a single paper for each demographic attribute at a time . The resultant recommended papers were compared to the actual papers accepted in SIGCHI 2017 in terms of diversity gain and utility savings , where utility savings was measured based on how much , if at all , the h - index of the selected papers‚Äô authors declined . Our results showed that , based on the F - measure , the Round Robin approach with Boolean feature weights was the most effective strategy for maximizing diversity . It increased diversity by 72 . 6 percent with only a 4 . 8 percent reduction in utility . Also based on the F - measure , the Multifaceted Diversity algorithm with continuous features weights in the demographic profile was the most effective strategy for reaching demographic parity . With a 42 . 50 % gain in diversity and a 2 . 45 % boost in utility , it managed to achieve 95 . 12 % similarity to the demographics of the pool of authors of all submitted papers . This finding indicates that , contrary to expectations , improving diversity actually raises the quality of publications that are accepted . In the future , we will investigate dynamic hill - climbing algorithms that modify the recommendation goals following each paper selection . Additionally , we will research the efficacy of fair deep learning approaches for paper recommendation . References 1 . Barak , B . ( 2018 ) . On double blind reviews in theory conferences . Windows on Theory . 2 . blog , NamSor . ( 2018 ) . Understanding NamSor API precision for Gender inference . Inferring The World ' s Gender and Ethnic Diversity using Personal Names . 3 . blog , NamSor . ( 2019 ) . NamSor US ‚ÄòRace‚Äô / Ethnicity model helps estimate diversity in Chicago Police . Inferring The World ' s Gender and Ethnic Diversity using Personal Names . 4 . Khan , B . , Robbins , C . , and Okrent , A . ( 2020 ) . Science and Engineering Indicator . NSF . https : / / ncses . nsf . gov / pubs / nsb20198 / demographic - trends - of - the - s - e - workforce 5 . Bornmann , L . , & Daniel , H . D . ( 2005 ) . Selection of research fellowship recipients by committee peer review . Reliability , fairness and predictive validity of Board of Trustees ' decisions . Scientometrics , 63 ( 2 ) , 297 - 320 . 6 . Bornmann , L . , & Daniel , H . D . ( 2009 ) . The state of h index research : is the h index the ideal way to measure research performance ? . EMBO reports , 10 ( 1 ) , 2 - 6 . 7 . Cannon , S . , Reid , D . A . , McFarlane , K . , King , L . , MacKenzie , L . , Tadaki , M . , and Koppes , M . ( 2018 ) . Race and gender still an issue at academic conferences . The Conversation . 8 . Chandrasekaran , K . , Gauch , S . , Lakkaraju , P . , & Luong , H . P . ( 2008 , July ) . Concept - based document recommendations for citeseer authors . In International conference on adaptive hypermedia and adaptive web - based systems ( pp . 83 - 92 ) . Springer , Berlin , Heidelberg . 9 . Code . org . ( 2020 ) . , Women computer science graduates finally surpass record set 17 years ago , but percentages lag behind . Medium . 10 . Cox , A . R . , & Montgomerie , R . ( 2019 ) . The cases for and against double - blind reviews . PeerJ , 7 , e6702 . Data USA , Computer , engineering , & science occupations . . ( 2020 ) . 11 . Draisma , J . , Horobe≈£ , E . , Ottaviani , G . , Sturmfels , B . , & Thomas , R . ( 2014 ) . The Euclidean distance degree . In proceedings of the 2014 Symposium of Symbolic - Numeric Computation ( SNC‚Äô14 ) . ACM . ( pp . 9 - 16 ) . 12 . Dwork , C . , Hardt , M . , Pitassi , T . , Reingold , O . , & Zemel , R . ( 2012 , January ) . Fairness through awareness . In Proceedings of the 3rd innovations in theoretical computer science conference ( pp . 214 - 226 ) . 13 . eeoc . gov . ( n . d . ) . US Equal Employment Opportunity Commission . 14 . Farnadi , G . , Kouki , P . , Thompson , S . K . , Srinivasan , S . , & Getoor , L . ( 2018 ) . A fairness - aware hybrid recommender system . arXiv preprint arXiv : 1809 . 09030 . 15 . Flaherty , C . ( 2016 ) . More Faculty Diversity , Not on Tenure Track . Inside Higher ED . 16 . Flaherty , C . ( 2018 ) . When Journals Play Favorites . Inside Higher ED . 17 . Flaherty , C . ( 2019 ) . Professors Still More Likely Than Students to Be White . Inside Higher Ed . 18 . Gauch , S . , Speretta , M . , Chandramouli , A . , & Micarelli , A . ( 2007 ) . User profiles for personalized information access . The adaptive web , 54 - 89 . 19 . Google Scholar . 2020 . Google . https : / / scholar . google . com / 20 . Google Scholar ( n . d . ) . https : / / scholar . google . com / citations ? view _ op = top _ venues & hl = en & vq = eng 21 . Guide2Research . ( 2020 ) . Top Computer Science Conferences . Guide2Research . http : / / www . guide2research . com / topconf / 22 . Hardt , M . , Price , E . , & Srebro , N . ( 2016 ) . Equality of opportunity in supervised learning . arXiv preprint arXiv : 1610 . 02413 . 23 . Harzing , A . ( 2016 ) . Publish or Perish . https : / / harzing . com / resources / publish - or - perish 24 . Human Development Reports . ( n . d . ) . Human Development Data Center . Human Development Reports . 25 . Inc . US Legal . ( n . d . ) . Protected Group Member Law and Legal Definition . https : / / definitions . uslegal . com / p / protected - group - member / 26 . Jacob , B . A . , & Lefgren , L . ( 2011 ) . The impact of research grant funding on scientific productivity . Journal of public economics , 95 ( 9 - 10 ) , 1168 - 1177 . 27 . Jardine , N . & . ( 1971 ) . The use of hierarchic clustering in information retrieval . . Information storage and retrieval , ( pp . 7 ( 5 ) , 217 - 240 . ) . 28 . Jones , T . M . , Fanson , K . V . , Lanfear , R . , Symonds , M . R . , & Higgie , M . ( 2014 ) . Gender differences in conference presentations : a consequence of self - selection ? . PeerJ , 2 , e627 . 29 . Labille , K . , Gauch , S . , Joseph , A . S . , Bogers , T . , & Koolen , M . ( 2015 ) . Conceptual Impact - Based Recommender System for CiteSeerx . In CBRecSys @ RecSys ( pp . 50 - 53 ) . 30 . Lemire , A . D . ( 2020 , December 3 ) . Double - blind peer review is a bad idea . Daniel Lemire ' s blog . 31 . Lerback , J . , & Hanson , B . ( 2017 ) . Journals invite too few women to referee . Nature News , 541 ( 7638 ) , 455 . 32 . Long , C . , Wong , R . C . W . , Peng , Y . , & Ye , L . ( 2013 , December ) . On good and fair paper - reviewer assignment . In 2013 IEEE 13th international conference on data mining ( pp . 1145 - 1150 ) . IEEE . 33 . Murray , D . , Siler , K . , Larivi√®re , V . , Chan , W . M . , Collings , A . M . , Raymond , J . , & Sugimoto , C . R . ( 2019 ) . Gender and international diversity improves equity in peer review . BioRxiv , 400515 . 34 . Naional Sciencef Foundation . ( 2019 ) . Established Program to Stimulate Competitive Research ( EPSCoR ) . NSF website . 35 . Nations , U . ( 2020 ) . The World Economic Situation and Prospects . 2020 . Acessado em , 20 . 36 . Palus , S . ( 2015 ) . Is Double - Blind Review Better ? . American Physical Society . 37 . Pannucci , C . J . , & Wilkins , E . G . ( 2010 ) . Identifying and avoiding bias in research . Plastic and reconstructive surgery , 126 ( 2 ) , 619 . 38 . Protected group . ( 2020 ) . Wikipedia . https : / / en . wikipedia . org / wiki / Protected _ group 39 . Santamar√≠a , L . , & Mihaljeviƒá , H . ( 2018 ) . Comparison and benchmark of name - to - gender inference services . PeerJ Computer Science , 4 , e156 . 40 . SIGCHI . ( 2019 ) . Diversity of the Program Committee for CHI 2020 . 41 . Sikdar , S . , Marsili , M . , Ganguly , N . , & Mukherjee , A . ( 2016 , October ) . Anomalies in the peer - review system : A case study of the journal of High Energy Physics . In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management ( pp . 2245 - 2250 ) . 42 . Stelmakh , I . , Shah , N . B . , & Singh , A . ( 2019 , March ) . PeerReview4All : Fair and accurate reviewer assignment in peer review . In Algorithmic Learning Theory ( pp . 828 - 856 ) . PMLR . 43 . Sugarman , D . B . , Nation , M . , Yuan , N . P . , Kuperminc , G . P . , Hassoun Ayoub , L . , & Hamby , S . ( 2018 ) . Hate and violence : Addressing discrimination based on race , ethnicity , religion , sexual orientation , and gender identity . Psychology of violence , 8 ( 6 ) , 649 . 44 . Times Higher Education ( THE ) . ( 2020 ) . World University Rankings . 45 . ComputerScience . Women in Computer Science : Getting Involved in STEM . ComputerScience . org . ( 2021 , May 5 ) . 46 . Yang , K . , & Stoyanovich , J . ( 2017 , June ) . Measuring fairness in ranked outputs . In Proceedings of the 29th International Conference on Scientific and Statistical Database Management ( pp . 1 - 6 ) . 47 . Zehlike , M . , Bonchi , F . , Castillo , C . , Hajian , S . , Megahed , M . , & Baeza - Yates , R . ( 2017 , November ) . Fa * ir : A fair top - k ranking algorithm . In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management ( pp . 1569 - 1578 ) . 48 . Zehlike , M . , & Castillo , C . ( 2020 , April ) . Reducing disparate exposure in ranking : A learning to rank approach . In Proceedings of The Web Conference 2020 ( pp . 2849 - 2855 ) . 49 . Zhong , Z . ( 2018 ) . A Tutorial on Fairness in Machine Learning . Towards Data Science . 50 . Zhuang , Z . , Elmacioglu , E . , Lee , D . , & Giles , C . L . ( 2007 , June ) . Measuring conference quality by mining program committee characteristics . In Proceedings of the 7th ACM / IEEE - CS joint conference on Digital libraries ( pp . 225 - 234 ) . 51 . Andrews , L . Edmund . ( 2021 , December ) . Stanford research reveals a hidden obstacle for women in Academia . Stanford News Service . Retrieved from https : / / news . stanford . edu / press - releases / 2021 / 12 / 16 / hidden - obstacle - women - academia / 51 . Zweben , S . , & Bizot , B . ( 2018 ) . 2018 CRA Taulbee survey . Computing Research News , 30 ( 5 ) , 1 - 47 . 52 . Alsaffar , R . , Gauch , S . , Alqahtani , M . , & Salman , O . ( 2021 , September ) . Incorporating Fairness in Paper Recommendation . In 2021 ACM / IEEE Joint Conference on Digital Libraries ( JCDL ) ( pp . 318 - 319 ) . IEEE . 53 . Al - Saffar , R . , & Gauch , S . ( 2021 ) . Multidimensional Demographic Profiles for Fair Paper Recommendation . In KDIR ( pp . 199 - 208 ) . 54 . Jiang , Z . , Han , X . , Fan , C . , Yang , F . , Mostafavi , A . , & Hu , X . ( 2022 , September ) . Generalized demographic parity for group fairness . In International Conference on Learning Representations . 55 . Lahoti , P . , Beutel , A . , Chen , J . , Lee , K . , Prost , F . , Thain , N . , Wang , X . , & Chi , E . ( 2020 ) . Fairness without Demographics through Adversarially Reweighted Learning . NeurIPS 2020 . http : / / alexbeutel . com / papers / NeurIPS - 2020 - fairness - without - demographics . pdf 56 . Al - Saffar , R . , & Gauch , S . ( 2021 ) . Multidimensional Demographic Profiles for Fair Paper Recommendation . In KDIR ( pp . 199 - 208 ) .