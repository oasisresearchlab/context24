1 Agility in Software 2 . 0 – Notebook Interfaces and MLOps with Buttresses and Rebars Markus Borg RISE Research Institutes of Sweden , Lund , Sweden , Dept . of Computer Science , Lund University , Lund , Sweden markus . borg @ ri . se Summary . Artiﬁcial intelligence through machine learning is increasingly used in the digital society . Solutions based on machine learning bring both great oppor - tunities , thus coined “Software 2 . 0 , ” but also great challenges for the engineering community to tackle . Due to the experimental approach used by data scientists when developing machine learning models , agility is an essential characteristic . In this keynote address , we discuss two contemporary development phenomena that are fundamental in machine learning development , i . e . , notebook interfaces and MLOps . First , we present a solution that can remedy some of the intrinsic weaknesses of working in notebooks by supporting easy transitions to integrated development en - vironments . Second , we propose reinforced engineering of AI systems by introducing metaphorical buttresses and rebars in the MLOps context . Machine learning - based solutions are dynamic in nature , and we argue that reinforced continuous engineering is required to quality assure the trustworthy AI systems of tomorrow . 1 . 1 Introduction No one has missed the AI surge in the last decade . There is an ever - increasing number of AI applications available as enterprises across domains seek to har - ness the promises of AI technology . Enabled by the growing availability of data , most of the AI success stories in recent years originate in solutions dom - inated by Machine Learning ( ML ) [ 1 ] . Where human programmers previously had to express all logic in source code , ML models can now be trained on huge sets of annotated data – for certain tasks , this works tremendously well . An - drej Karpathy , AI Director at Tesla , somewhat cheekily refers to development according to the ML paradigm as “Software 2 . 0” 1 . For many applications seeking mapping from input to output , it is easier to collect and annotate high - quality data than to program a mapping function in code explicitly . Agile software development has become the norm in the software engineer - ing industry . Flexibly adapting to change has proven to be a recipe to ripe 1 bit . ly / 3dKeUEH a r X i v : 2111 . 14142v1 [ c s . S E ] 28 N ov 2021 2 Markus Borg some of the beneﬁts of software – signiﬁcant changes can often occur at any time , both during a development project and post - release . Quickly adapting to shifting customer needs and technology changes is often vital to survival in a competitive market . In this light , the concept of DevOps has emerged as an approach to minimize time to market while maintaining quality [ 2 ] . While ag - ile development is particularly suitable for customer - oriented development in the Internet era , it is also increasingly used in embedded systems development of more critical nature [ 3 ] with adaptations such as SafeScrum [ 4 ] . Moreover , while agile software development is ﬂexible , we argue that ML development iterates even faster – and thus necessitates “agility on steroids . ” Data scientists often conduct the highly iterative development of ML mod - els . Data scientists , representing a new type of software professionals , often do not have the software engineering training of conventional software de - velopers [ 5 ] . This observation is analogous to what has been reported for developers of scientiﬁc computing in the past , e . g . , regarding their familiarity with agile practices [ 6 ] . Instead of prioritizing the crafts of software engineer - ing and computer science , many data scientists focus on mastering the art of taming data into shapes that are suitable for model training – typically using domain knowledge to hunt quantitative accuracy targets for a speciﬁc application . The ML development process is experimental in nature and in - volves iterating between several intertwined activities , e . g . , data collection , data preprocessing , feature engineering , model selection , model evaluation , and hyperparameter tuning . An unfortunate characteristic of ML develop - ment is that nothing can be considered in isolation . A foundational ML paper by Google researchers described this as the CACE principle “Changing Any - thing Changes Everything” [ 7 ] . When developing ML models in Software 2 . 0 , no data science activities are ever independent . In this keynote address , we will discuss two phenomena that have emerged to meet the characteristics of ML development . First , Notebook interfaces to meet the data scientists’ needs to move swiftly . Unfortunately , the step from prototyping in Notebook interfaces to a mature ML solution is often considerable – and cumbersome for many data scientists . In Section 1 . 2 , we will present a solution by Jakobsson and Henriksson that bridges the gap between the data scientists’ preferred notebook interfaces and standard development in Integrated Development Environments ( IDE ) . Second , analogous to DevOps in conventional agile software development , in Section 1 . 3 , we will look at how MLOps has emerged to close the gap between ML development and ML operations . More than just an agility concept , we claim that it is required to meet the expectations on the trustworthy AI of the future – illustrated in the light of the recently proposed Artiﬁcial Intelligence Act in the European Union . We refer to our concept of reinforcing the development and operations of AI systems , aﬄicted by the CACE principle , using two metaphors from construction engineering : buttresses and rebars . 1 Agility in Software 2 . 0 3 1 . 2 Connecting Notebook Interfaces and IDEs Many data scientists are not trained software engineers and thus might not be fully aware of available best practices related to various software engi - neering activities [ 5 ] . Moreover , even with awareness of software engineering best practices , data science introduces new challenges throughout the engi - neering lifecycle [ 8 , 9 ] – from requirements engineering [ 10 ] to operations [ 7 ] . Due to the intrinsically experimental nature of data science , practitioners seek development environments that allow maximum agility , i . e . , high - speed development iterations . The go - to solution for many data scientists is to work iteratively in cloud - based notebook interfaces . While this allows rapid experimentation , it does not easily allow the application of the various tools available in a modern IDE [ 11 ] . The ﬁrst part of this keynote address presents a solution developed as part of a MSc thesis project by Jakobsson and Henriksson at Backtick Tech - nologies [ 12 ] that enables data scientists to easily move between notebook in - terfaces and an IDE thanks to a networked ﬁle system . The idea is to let data scientists work in their favorite editor and use all the tools available for local development while still being able to use the cloud - based notebook interface for data exploration – and reaping its beneﬁts of easy access to distributed cloud computing . Jakobsson and Henriksson integrated and evaluated the so - lution as part of Cowait Notebooks , an experimental cloud notebook solution developed by Backtick Technologies . Cowait 2 is an open - source framework for creating containerized distributed applications with asynchronous Python . 1 . 2 . 1 Agility Supported by Notebook Interfaces A substantial part of today’s data science revolves around notebook interfaces , also known as computational notebooks . Notebook interfaces are typically cloud - based and consist of environments with interactive code interpreters accessible from web browsers that allow ra¨oid , iterative development . The notebooks themselves usually run on a remote machine or a computer cluster , allowing the user easy access to compute resources available in data centers . While the notebook interfaces gradually mature , i . e . , more features become available , the environments are still far from as capable as the IDEs software developers run locally . Consequently , the support for version control software , static analysis , linting , and other widely used development tools is limited in notebook interfaces [ 11 ] . The implementation of a notebook interface diﬀers from a conventional IDE . A notebook runs an interpreter in the background that preserves the state for the duration of a programming session . A user observes a notebook as a sequence of cells that are either textual ( allowing data scientists to doc - ument the process ) or containing code . These two diﬀerent types of cells are 2 https : / / cowait . io 4 Markus Borg interwoven in the notebook . Notebook interfaces usually excel at presenting plots and tables that support data exploration . A code cell contains one or more statements and can be executed independently from any other code cell . Users can execute code cells in any order , but the cells all mutate the shared state of the background interpreter . This freedom of execution order greatly supports the agility of data science as users can re - run portions of a program while keeping other parts of the previously generated state . While this en - ables fast iterations toward a useful solution , it also makes it diﬃcult to trace the path of execution that led to a speciﬁc result . Even worse , subsequent executions of the notebook may yield diﬀerent results . The concept of computational notebooks was envisioned by Knuth already in 1984 [ 13 ] . Knuth proposed the literate programming paradigm and showed how the idea could support program comprehension by mixing snippets of source code and natural language explanations of its embedded logic . As elab - orated in Knuth’s seminal book on the topic [ 14 ] , the key point is that literate programming explicitly shifts who is the most important reader of the pro - gramming artifact . In literate programming , source code is primarily written for humans instead of computers – and the artifact can be seen as a piece of literature . Many developers of scientiﬁc computing follow this paradigm to develop maintainable software artifacts [ 15 ] . A more general version of literate programming is literate computing , where the source code cells and natural language explanations are accompanied by visual content such as tables , graphs , and images . Today’s widely used notebook interfaces , such as the popular Jupyter Notebook 3 and Databrick’s Collaborative Notebook 4 , are examples of literate computing . For a recent overview of the notebook landscape , we refer the curious reader to an article by Vognstrup Fog and Nylandsted Klokmose [ 16 ] . Their summary presents both a historical perspective and a discussion of design decisions for future notebook interfaces . Notebook interfaces have certainly evolved substantially since Knuth ﬁrst envisioned them . However , there are still certain impediments for data sci - entists working in notebooks . Chattopadhyay et al . analyzed contemporary issues with notebook interfaces and reported nine pain points [ 11 ] . Accord - ing to the authors , the most pressing pain points for developers of notebook interfaces to tackle are 1 ) code refactoring , 2 ) deployment to production , 3 ) exploring notebook history , and 4 ) managing long - running tasks . Notebook interfaces constitute a highly active research topic , and researchers have pro - posed several solutions to address their limitations [ 17 , 18 , 19 ] . However , while notebook interfaces are a prominent medium for software development , there is still a substantial need for research and development [ 20 ] . This talk will introduce a solution proposal by Jakobsson and Henriks - son that bridges the beneﬁts of notebook interfaces and local IDEs . Lau et 3 https : / / jupyter . org 4 https : / / databricks . com / product / collaborative - notebooks 1 Agility in Software 2 . 0 5 al . examined 60 diﬀerent notebook interfaces and categorized them according to 10 dimensions of analysis : 1 ) data sources , 2 ) editor style , 3 ) programming language , 4 ) versioning , 5 ) collaboration , 6 ) execution order , 7 ) execution live - ness , 8 ) execution environment , 9 ) cell outputs , and 10 ) notebook outputs . In the MSc thesis project by Jakobsson and Henriksson , the authors focused on the dimensions of execution environment and data sources for Cowait Note - books . Their solution allows Cowait Notebooks to execute code in a remote multi - process execution environment using local ﬁles as data sources . This solution contrasts with Jupyter Notebook for which both code execution and data is local . The solution is also diﬀerent from Databrick’s Collaborative Notebook , where code is executed in a remote multi - process execution envi - ronment , but the data sources cannot be local . In the next section , we present the open - source Cowait framework . 1 . 2 . 2 Cowait – A Framework for Simpliﬁed Container Orchestration Cowait is a framework that simpliﬁes the execution of Python code on the con - tainer orchestration system Kubernetes . The two main constituents of Cow - ait are 1 ) a workﬂow engine built on top of Docker and Kubernetes and 2 ) a build system to easily package source code into containers . Together , the workﬂow engine and the build system form an abstraction of containers and container hosts that helps developers leverage the power of containerization through Docker and cluster deployment using Kubernetes without knowing all technical details . Backtick Technologies designed Cowait to hide the in - trinsic complexity of Docker and Kubernetes behind simple concepts that are familiar to general software developers . Cowait is developed under an Apache License and the source code is available on GitHub 5 . Cowait provides four key features with a focus on user - friendliness , i . e . , Cowait . . . 1 . . . . helps the development of distributed workﬂows on your local machine with minimal setup . 2 . . . . simpliﬁes dependency management for Python projects . 3 . . . . allows developers to unit test their workﬂow tasks . 4 . . . . lowers the bar for users to deploy solutions on Kubernetes clusters . In line with other workﬂow engines , Cowait organizes code into tasks . A task is essentially a function that can accept input arguments and return val - ues . As for functions in general , a task can invoke other tasks — with one key diﬀerence : a call to invoke another task will be intercepted by the Cow - ait runtime environment and subsequently executed in a separate container . Cowait can also direct the execution of this separate container to a particu - lar machine . The fundamental diﬀerentiator oﬀered by Cowait is that tasks 5 https : / / github . com / backtick - se / cowait 6 Markus Borg can interface directly with the underlying cluster orchestrator . In practice , this means that tasks can start other tasks without going through a cen - tral scheduler service . Instead , tasks create other tasks on demand , and they communicate with their parent tasks using web sockets . Further details are available in the Cowait Documentation 6 . The task management system in Cowait relies on containers and thus sup - ports the execution of arbitrary software . Thanks to this ﬂexibility , Cowait can execute notebook interfaces . In their MSc thesis project , Jakobsson and Henriksson demonstrate the execution of the open - source JupyterLab note - book interface in a Cowait solution – we refer to this as running a Cowait Notebook . JupyterLab is a popular notebook interface that is particularly suitable for this demonstration since it is implemented in Python . Once the JupyterLab task is started in a cluster , it automatically gets a public URL that the users can connect to . Cowait Notebooks allow data scientists to host notebook interfaces in any Kubernetes cluster with minimal setup . Executing Cowait Notebooks within a Cowait task lets the notebook access Cowait’s underlying task scheduler and allow sub - tasks to be launched directly from the notebook cells – data scientists can thus easily execute background tasks on the cluster . In the next section , we present Jakobsson and Henriksson’s solution to allow access to local ﬁles – and thus enabling work with local IDEs . 1 . 2 . 3 Local Files and Cowait Notebooks Executing on Clusters Jakobsson and Henriksson developed a proof - of - concept implementation of a general solution to ﬁle sharing between a data scientist’s local computer and software running on a remote cluster . The key enabler is a custom networked ﬁle system implemented using File System in Userspace ( FUSE ) 7 . FUSE is an interface for userspace programs to export a ﬁle system to the Linux ker - nel . To make the solution compatible with as many diﬀerent data science applications as possible , the network ﬁle system was implemented as a cus - tom storage driver for Kubernetes . Kubernetes is the most popular cluster orchestration solution , available as a managed service from all major cloud providers . Furthermore , Kubernetes is an open - source solution that users can also deploy on - premise . Practically , Jakobsson and Henriksson ensured com - patibility with Kubernetes by implementing the Container Storage Interface , an open standard for developing new Kubernetes storage options 8 . The goal of the MSc thesis project was to design a user - friendly , reliable , and widely compatible solution to ﬁle sharing for data scientists . The aim was to provide seamless access to ﬁles residing on a data scientist’s local computer for other data scientists accessing the local ﬁles through cloud - based notebook 6 https : / / cowait . io / docs / 7 File System in Userspace , https : / / github . com / libfuse / libfuse 8 https : / / kubernetes - csi . github . io / docs / 1 Agility in Software 2 . 0 7 interfaces executing on Kubernetes clusters . With such a solution in place , data scientists could collaborate online using the notebook interfaces they prefer while allowing state - of - the - art software engineering tools to operate in IDEs on local machines . To evaluate the proof - of - concept , Jakobsson and Henriksson conducted two separate studies . First , a quantitative study was carried out to verify the solution’s performance in light of requirements set by prior user experience research on human response times [ 21 , p . 135 ] . The authors studied the performance as diﬀerent numbers of ﬁles , of diﬀerent sizes , where accessed under diﬀerent network conditions . While details are available in the MSc thesis [ 12 ] , the general ﬁnding is that the solution satisﬁed the requirement of ﬁle access within 1 second for reasonable ﬁle sizes and realistic network latency . We consider this a necessary but not suﬃcient requirement for the novel solution . Second , Jakobsson and Henriksson conducted a qualitative study to collect deep insights into the solution’s utility . The authors recruited a mix of data scientists and software developers ( with substantial ML experience ) to per - form a carefully designed programming task under a think - aloud protocol [ 22 ] . The purpose was to collect feedback on whether the novel ﬁle sharing solution could improve the overall experience of working with cloud - based notebook in - terfaces . The feedback displayed mixed impressions . Data scientists who were comfortable using managed cloud solutions expressed hesitation to use such a system due to reduced ease - of - use and potential collaboration issues . The group that was the most positive were developers with a software engineering background , who were excited to be able to use familiar tooling for local ﬁles . Despite the mixed opinions , we still perceive the proof - of - concept as promising – but more work is needed to bridge notebook interfaces and local IDEs . 1 . 3 MLOps – A Key Enabler for Agility in Software 2 . 0 Many organizations report challenges in turning an ML proof - of - concept into a production - quality AI system [ 23 ] . The experimental nature of ML devel - opment limits qualities such as reproducibility , testability , traceability , and explainability — which are needed when putting a trustworthy product or service on the market . On top of this , an AI system must be maintained until the product or service reaches its end - of - life . This holistic lifecycle per - spective , i . e . , what follows post - release , is often missing when novice data science teams develop AI proofs - of - concept in the sandbox . An organization must continuously monitor the ML models in operation and , in many cases , evolve the models according to feedback from the production environment – where phenomena such as distributional shifts can be game - changers [ 7 ] . Without designing for the operations phase and ensuring that ML model changes easily can be pushed to production , it will be tough to reach sustain - ably value - creating AI solutions . This attractive state is sometimes referred 8 Markus Borg to as Operational AI [ 24 ] . In the next section , we will share our view on how the concept of MLOps can help organizations reach this state . 1 . 3 . 1 Continuous Engineering in the AI Era In software development , continuous software engineering and DevOps emerged to reduce the lead time and remove the barriers between development , test - ing , and operations [ 2 ] . Workﬂow automation in pipelines is fundamental , as it enables approaches such as 1 ) continuous integration ( integration of code changes followed by test automation ) , 2 ) continuous delivery ( building soft - ware for an internal test environment ) , and 3 ) continuous deployment ( delivery of software to actual users ) [ 25 ] . Depending on the application , organizations can also add staging processes when human validation is needed . Thanks to the automation , development qualities such as traceability come at a substan - tially lower cost compared to a manual workﬂow [ 26 ] . DevOps has inspired a similar mindset within ML development in the form of MLOps , i . e . , the stan - dardization and streamlining of ML lifecycle management [ 27 ] – which is a recommended approach to tackle continuous engineering in Software 2 . 0 [ 28 ] . Just like DevOps is more than a set of tools , MLOps can be seen as a mindset on the highest level . As an engineering discipline , MLOps is a set of practices that combines ML , DevOps , and Data Engineering . Organizations adopting MLOps hope to deploy and maintain ML systems in production re - liably and eﬃciently . Going beyond technology , MLOps involves embracing a culture with corresponding processes that an organization must adapt for the speciﬁc application domain . MLOps has emerged from the Big Tech In - ternet companies ; thus , customization is required to ﬁt smaller development organizations . Extrapolating from DevOps in conventional software engineer - ing [ 2 , 26 ] , MLOps relies on pipeline automation to remove the barriers be - tween data processing , model training , model testing , and model deployment . MLOps is not yet well - researched from an academic perspective . The pri - mary reason is that MLOps is a production concept , i . e . , the phenomenon must be studied in the ﬁeld rather than in university labs . However , this does not mean that MLOps should not be targeted by academic research . On the contrary , it is critically important that software and systems engineering re - searchers initiate industrial collaborations to allow empirical studies of what works and what does not when developing and evolving AI systems . As always in software engineering research , we have to identify the most important vari - ation points needed to provide accurate guidance given speciﬁc application contexts . Just like there are uncountably many ways to implement pipeline automation – the ML tools market is booming – there is not a one - size - ﬁts - all way to adopt MLOps in an organization . 1 . 3 . 2 Reinforced AI Systems using Buttresses and Rebars Just as agile development enters regulated domains [ 3 ] , Software 2 . 0 is grad - ually entering critical applications [ 29 ] . Examples include automotive soft - 1 Agility in Software 2 . 0 9 ware [ 30 ] and software in healthcare [ 31 ] . From a quality assurance perspective , AI systems using ML constitute a paradigm shift compared to conventional software systems . A contemporary deep neural network might be composed of hundreds of millions of parameter weights – such an artifact is neither applica - ble to code reviews nor standard code coverage testing . Development organi - zations have learned how to develop trustworthy code - based software systems through decades of software engineering experience . This collected experience has successfully been captured in diﬀerent industry standards . Unfortunately , many best practices are less eﬀective when developing AI systems . Bosch et al . and others argue that software and systems engineering must evolve to enable eﬃcient and eﬀective development of trustworthy AI systems [ 23 ] . One response to this call is that new standards are under development in various domains to complement existing alternatives for high - assurance systems [ 32 ] . Due to the growing reliance on AI systems , the European Union ( EU ) AI strategy stresses the importance of Trustworthy AI . EU deﬁnes such sys - tems as lawful , ethical , and robust [ 33 ] . Unfortunately , we know that existing software engineering approaches such as requirements traceability [ 34 ] and veriﬁcation & validation [ 29 ] are less eﬀective at demonstrating system trust - worthiness when functionality depends on ML models . Due to its experimental nature , data science makes it hard to trace design decisions after - the - fact and the resulting ML models become less reproducible [ 11 ] . Moreover , the inter - nals of ML models are notoriously diﬃcult to interpret [ 35 ] , and AI systems are diﬃcult to test [ ? , ? ] . Not only must developers of critical AI systems comply with emerging industry standards , but novel AI regulations are also expected in the EU . In April 2021 , the European Commission proposed an ambitious Artiﬁcial Intelligence Act ( AIA ) [ 36 ] . AIA is a new legal framework with dual ambitions for turning Europe into the global hub for trustworthy AI . First , AIA aims to guarantee the safety and fundamental rights of EU citizens when interacting with high - risk AI systems . Second , AIA seeks to strengthen AI innovation by providing legal stability and instilling public trust in the technology . Many voices have been raised about the proposed legislation , in which especially the broad deﬁnition of AI has been criticized . However , all signs point to increased regulation of AI in the EU , in line with the now established General Data Protection Regulation [ 37 ] – including substantial ﬁnes deﬁned in relation to annual global turnover . ML is an increasingly important AI technology in the digitalization of society that receives substantial attention in the AIA . According to the pro - posal , any providers of high - risk solutions using ML must demonstrate AIA conformance to an independent national authority prior to deployment on the EU internal market . Demonstrating this compliance will be very costly – and how to eﬀectively ( and eﬃciently ! ) do it remains an important open research question . We are currently exploring the topic of built - in trustworthiness through a metaphor of reinforced engineering : buttresses and rebars . Our fundamental 10 Markus Borg position is that organizations must tackler quality assurance from two direc - tions . Requirements engineering and veriﬁcation & validation shall work to - gether like two bookends supporting the AI system , including its development and operations , from either end . Figure 1 . 3 . 2 illustrates how the primary re - inforcement originates in buttressing the development of the AI system with requirements engineering ( to the left ) and veriﬁcation & validation ( to the right ) . The metaphor , inspired by construction engineering , further borrows the concept of rebars , i . e . , internal structures to strengthen and aid the AI system . In our metaphor , the rebars are realized in the form of so - called au - tomation pipelines for data , training , and deployment , respectively . Pipeline automation allows continuous engineering throughout the lifecycle , i . e . , data management , training , deployment , and monitoring in an MLOps context . Pipeline automation enables ﬂexibly adding automated quality assurance ap - proaches as pipe segments , e . g . , GradCAM heatmaps for explainability [ 38 ] , originating in the requirements engineering and veriﬁcation & validation but - tresses . The envisioned reinforcement allows organizations to continuously steer the development and operations toward a trustworthy AI system — in the context of highly agile data science , the CACE principle , and the ever - present risks of distributional shifts . Fig . 1 . 1 . Metaphorical buttresses and rebars . Robust requirements engineering and veriﬁcation & validation support the engineering of an ever - changing AI system . Pipeline automation in an MLOps context constitutes the rebars that sustain trust - worthiness by strengthening the AI system despite the dynamics involved . Numerous studies report that requirements engineering is the foundation of high - quality software systems . However , the academic community has only recently fully embraced the idea of tailored requirements engineering for AI systems . We argue that the particular characteristics of ML development in data science necessitate an evolution of requirements engineering processes 1 Agility in Software 2 . 0 11 and practices [ 10 ] . New methods are needed when development transitions to the generation of rules based on training data and speciﬁc ﬁtness functions . Based on a 2020 Dagstuhl seminar , K¨astner stressed requirements engineering as a particular ML challenge , Google researchers express it as underspeciﬁca - tion [ 39 ] , and several papers have been recently published by the requirements engineering research community [ 40 , 41 , 42 ] . Academic research on veriﬁcation & validation tailored for AI systems has received a head start compared to requirements engineering for AI . New pa - pers continuously appear , and secondary studies on AI testing [ 43 , 44 ] and AI veriﬁcation [ 45 ] reveal hundreds of publications . As automation is close at hand for veriﬁcation & validation solutions , the primary purpose of the pipelines in the metaphor is to stress that they shall reach all the way to the re - quirements engineering buttress . Aligning requirements engineering with veri - ﬁcation & validation can have numerous beneﬁts in software engineering [ 46 ] – and even more so , we argue , in AI engineering . Our planned next steps include exploring AIA conformant high - risk computer vision systems with industry partners reinforced by buttresses and rebars . Our ambition is to combine au - tomated veriﬁcation & validation with an integrated requirements engineering approach [ 47 ] in the continuous engineering of MLOps . Finally , we are con - sidering introducing yet another metaphor from construction engineering , i . e . , virtual plumblines as proposed by Cleland - Huang et al . to maintain critical system quantities [ 48 ] . We posit that reinforcement and alignment will be two key essential concepts in future AI engineering , supported by a high level of automation to allow agile development of Software 2 . 0 . 1 . 4 Conclusion Whether we endorse the term Software 2 . 0 or not , AI engineering inevitably brings novel challenges . The experimental nature of how data scientists per - form ML development means that the work must be agile . However , this agility can be supported in various ways . In this keynote address , we discussed two contemporary phenomena in data science and ML . First , we presented note - book interfaces , weaknesses , and a solution proposal to lower the bar for them to co - exist with modern IDEs . Second , we shared our perspective on MLOps and our ongoing work on providing reinforced engineering of AI systems in this context . Agility and continuous engineering are needed in AI engineer - ing , as AI systems are ever - changing and often operate in dynamic environ - ments . Finally , the EU AI Act further exacerbates the need for reinforced engineering and alignment between requirements engineering and veriﬁcation & validation . As a guiding light toward this goal , we introduced our vision of metaphorical buttresses and rebars . 12 Markus Borg Acknowledgements Martin Jakobsson and Johan Henriksson are the co - creators of the solution presented in Section 1 . 2 and deserve all credit for this work . Our thanks go to Backtick Technologies for hosting the MSc thesis project and Dr . Niklas Fors , Dept . of Computer Science , Lund University for acting as the exam - iner . This initiative received ﬁnancial support through the AIQ Meta - Testbed project funded by Kompetensfonden at Campus Helsingborg , Lund Univer - sity , Sweden and two internal RISE initiatives , i . e . , “SODA - Software & Data Intensive Applications” and “MLOps by RISE . ” References 1 . Giray , G . : A software engineering perspective on engineering machine learning systems : State of the art and challenges . Journal of Systems and Software 180 ( 2021 ) 111031 2 . Ebert , C . , Gallardo , G . , Hernantes , J . , Serrano , N . : DevOps . IEEE Software 33 ( 3 ) ( 2016 ) 94 – 100 3 . Diebold , P . , Theobald , S . : How is agile development currently being used in regulated embedded domains ? Journal of Software : Evolution and Process 30 ( 8 ) e1935 4 . Hanssen , G . K . , St˚alhane , T . , Myklebust , T . : SafeScrum ® - Agile Development of Safety - Critical Software . Springer Nature , Cham , Switzerland ( 2018 ) 5 . Kim , M . , Zimmermann , T . , DeLine , R . , Begel , A . : The emerging role of data scientists on software development teams . In : Proc . of the 38th International Conference on Software Engineering . ( 2016 ) 96 – 107 6 . Sletholt , M . T . , Hannay , J . E . , Pfahl , D . , Langtangen , H . P . : What do we know about scientiﬁc software development’s agile practices ? Computing in Science & Engineering 14 ( 2 ) ( 2011 ) 24 – 37 7 . Sculley , D . , et al . : Hidden Technical Debt in Machine Learning Systems . In : Proc . of the 28th International Conference on Neural Information Processing Systems . ( 2015 ) 2503 – 2511 8 . Amershi , S . , Begel , A . , Bird , C . , DeLine , R . , Gall , H . , Kamar , E . , Nagappan , N . , Nushi , B . , Zimmermann , T . : Software engineering for machine learning : A case study . In : Proc . of the 41st International Conference on Software Engineering . ( 2019 ) 291 – 300 9 . Wan , Z . , Xia , X . , Lo , D . , Murphy , G . C . : How does machine learning change software development practices ? IEEE Transactions on Software Engineering 47 ( 9 ) ( 2021 ) 1857 – 1871 10 . Vogelsang , A . , Borg , M . : Requirements Engineering for Machine Learning : Per - spectives from Data Scientists . In : Proc . of the 27th International Requirements Engineering Conference Workshops . ( 2019 ) 245 – 251 11 . Chattopadhyay , S . , Prasad , I . , Henley , A . Z . , Sarma , A . , Barik , T . : What’s wrong with computational notebooks ? Pain points , needs , and design opportunities . In : Human Factors in Computing Systems . ( 2020 ) 1 – 12 12 . Jakobsson , M . , Henriksson , J . : Sharing local ﬁles with Kubernetes clusters ( 2021 ) MSc Thesis , Lund University , http : / / lup . lub . lu . se / student - papers / record / 9066685 / ﬁle / 9066686 . pdf . 1 Agility in Software 2 . 0 13 13 . Knuth , D . E . : Literate programming . The Computer Journal 27 ( 2 ) ( 1984 ) 97 – 111 14 . Knuth , D . E . : Literate Programming . Center for the Study of Language and Information , Stanford , US ( 1992 ) 15 . Hannay , J . E . , MacLeod , C . , Singer , J . , Langtangen , H . P . , Pfahl , D . , Wilson , G . : How do scientists develop and use scientiﬁc software ? In : Proc . of the ICSE Workshop on Software Engineering for Computational Science and Engineering , Ieee ( 2009 ) 1 – 8 16 . Vognstrup Fog , B . , Nylandsted Klokmose , C . : Mapping the landscape of liter - ate computing . In : Proc . of the 30th Annual Workshop of the Psychology of Programming Interest Group . ( 2019 ) 17 . Kery , M . B . , John , B . E . , O’Flaherty , P . , Horvath , A . , Myers , B . A . : Towards eﬀective foraging by data scientists to ﬁnd past analysis choices . In : Human Factors in Computing Systems . ( 2019 ) 1 – 13 18 . Kery , M . B . , Myers , B . A . : Interactions for untangling messy history in a compu - tational notebook . In : Proc . of the IEEE Symposium on Visual Languages and Human - Centric Computing . ( 2018 ) 147 – 155 19 . Head , A . , Hohman , F . , Barik , T . , Drucker , S . M . , DeLine , R . : Managing messes in computational notebooks . In : Human Factors in Computing Systems . ( 2019 ) 1 – 12 20 . Singer , J . : Notes on notebooks : Is Jupyter the bringer of jollity ? Proc . of the ACM SIGPLAN International Symposium on New Ideas , New Paradigms , and Reﬂections on Programming and Software ( 2020 ) 180 – 186 21 . Nielsen , J . : Usability Engineering . Morgan Kaufmann Publishers , Burlington , MA , USA ( 1993 ) 22 . Kuusela , H . , Paul , P . : A comparison of concurrent and retrospective verbal protocol analysis . The American Journal of Psychology 113 ( 3 ) ( 2000 ) 387 – 404 23 . Bosch , J . , Holmstr¨om Olsson , H . , Crnkovic , I . : Engineering AI systems : A re - search agenda . In : Artiﬁcial Intelligence Paradigms for Smart Cyber - Physical Systems . IGI Global ( 2021 ) 1 – 19 24 . Tapia , P . , Palacios , E . , No¨el , L . , et al . : Implementing Operational AI in telecom environments . Tupl White Paper 7 ( 2018 ) 25 . Fitzgerald , B . , Stol , K . J . : Continuous software engineering : A roadmap and agenda . Journal of Systems and Software 123 ( 2017 ) 176 – 189 26 . Jabbari , R . , Ali , N . , Petersen , K . , Tanveer , B . : Towards a beneﬁts dependency network for DevOps based on a systematic literature review . Journal of Software : Evolution and Process 30 ( 11 ) ( 2018 ) e1957 27 . Treveil , M . , Omont , N . , Stenac , C . , Lefevre , K . , Phan , D . , Zentici , J . , Lavoillotte , A . , Miyazaki , M . , Heidmann , L . : Introducing MLOps . O’Reilly Media , Inc . , Sebastopol , CA , USA ( 2020 ) 28 . Hummer , W . , Muthusamy , V . , Rausch , T . , Dube , P . , Maghraoui , K . E . , Murthi , A . , Oum , P . : ModelOps : Cloud - Based Lifecycle Management for Reliable and Trusted AI . In : Proc . of the International Conference on Cloud Engineering . ( 2019 ) 113 – 120 29 . Borg , M . , Englund , C . , Wnuk , K . , Duran , B . , Levandowski , C . , Gao , S . , Tan , Y . , Kaijser , H . , L¨onn , H . , T¨ornqvist , J . : Safely Entering the Deep : A Review of Veriﬁcation and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry . Journal of Automotive Software Engineering 1 ( 1 ) ( 2019 ) 1 – 19 14 Markus Borg 30 . Falcini , F . , Lami , G . , Costanza , A . M . : Deep learning in automotive software . IEEE Software 34 ( 3 ) ( 2017 ) 56 – 63 31 . Jiang , F . , et al . : Artiﬁcial Intelligence in Healthcare : Past , Present and Future . Stroke and Vascular Neurology 2 ( 4 ) ( 2017 ) 230 – 243 32 . Vidot , G . , Gabreau , C . , Ober , I . , Ober , I . : Certiﬁcation of embedded systems based on machine learning : A survey . arXiv preprint arXiv : 2106 . 07221 ( 2021 ) 33 . High - Level Expert Group on Artiﬁcial Intelligence : Ethics Guidelines for Trust - worthy Artiﬁcial Intelligence . Technical report , European Commission , Brussels , Belgium ( 2019 ) 34 . Borg , M . , Englund , C . , Duran , B . : Traceability and deep learning - safety - critical systems with traces ending in deep neural networks . Proc . of the Grand Chal - lenges of Traceability : The Next Ten Years ( 2017 ) 48 – 49 35 . Gilpin , L . H . , Bau , D . , Yuan , B . Z . , Bajwa , A . , Specter , M . , Kagal , L . : Explaining explanations : An overview of interpretability of machine learning . In : Proc . of the 5th International Conference on Data Science and Advanced Analytics . ( 2018 ) 80 – 89 36 . European Commission : Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artiﬁcial intelligence ( Arti - ﬁcial Intelligence Act ) and amending certain union legislative acts ( 2021 - 04 - 21 ) https : / / eur - lex . europa . eu / legal - content / EN / TXT / ? uri = CELEX % 3A52021PC0206 . 37 . European Commission : Regulation ( EU ) 2016 / 679 of the European Parliament and of the Council on the protection of natural persons with regard to the processing of personal data and on the free movement of such data , and repealing Directive 95 / 46 / ec ( General Data Protection Regulation ) . Oﬃcial Journal of the European Union 119 ( 2016 - 05 - 04 ) 1 – 88 38 . Borg , M . , Jabangwe , R . , ˚Aberg , S . , Ekblom , A . , Hedlund , L . , Lidfeldt , A . : Test automation with Grad - CAM heatmaps - A future pipe segment in MLOps for vision AI ? In : Proc . of the 14th International Conference on Software Testing , Veriﬁcation and Validation Workshops . ( 2021 ) 175 – 181 39 . D’Amour , A . , Heller , K . , Moldovan , D . , Adlam , B . , Alipanahi , B . , Beutel , A . , Chen , C . , Deaton , J . , Eisenstein , J . , Hoﬀman , M . D . , et al . : Underspeciﬁcation presents challenges for credibility in modern machine learning . arXiv preprint arXiv : 2011 . 03395 ( 2020 ) 40 . Ahmad , K . , Bano , M . , Abdelrazek , M . , Arora , C . , Grundy , J . : What’s up with requirements engineering for artiﬁcial intelligence systems ? In : Proc . of the 29th International Requirements Engineering Conference . ( 2021 ) 1 – 12 41 . Habibullah , K . M . , Horkoﬀ , J . : Non - functional requirements for machine learn - ing : Understanding current use and challenges in industry . In : Proc . of the 29th International Requirements Engineering Conference . ( 2021 ) 13 – 23 42 . Siebert , J . , Joeckel , L . , Heidrich , J . , Trendowicz , A . , Nakamichi , K . , Ohashi , K . , Namba , I . , Yamamoto , R . , Aoyama , M . : Construction of a quality model for machine learning systems . Software Quality Journal ( 2021 ) 1 – 29 43 . Zhang , J . M . , Harman , M . , Ma , L . , Liu , Y . : Machine learning testing : Survey , landscapes and horizons . IEEE Transactions on Software Engineering ( 2020 ) 44 . Riccio , V . , Jahangirova , G . , Stocco , A . , Humbatova , N . , Weiss , M . , Tonella , P . : Testing machine learning based systems : A systematic mapping . Empirical Software Engineering 25 ( 6 ) ( 2020 ) 5193 – 5254 1 Agility in Software 2 . 0 15 45 . Xiang , W . , Musau , P . , Wild , A . A . , Lopez , D . M . , Hamilton , N . , Yang , X . , Rosen - feld , J . , Johnson , T . T . : Veriﬁcation for machine learning , autonomy , and neural networks survey . arXiv preprint arXiv : 1810 . 01989 ( 2018 ) 46 . Bjarnason , E . , Runeson , P . , Borg , M . , Unterkalmsteiner , M . , Engstr¨om , E . , Reg - nell , B . , Sabaliauskaite , G . , Loconsole , A . , Gorschek , T . , Feldt , R . : Challenges and practices in aligning requirements with veriﬁcation and validation : A case study of six companies . Empirical software engineering 19 ( 6 ) ( 2014 ) 1809 – 1855 47 . Bjarnason , E . : Integrated Requirements Engineering - Understanding and Bridging Gaps in Software Development . Lund University , Sweden , https : / / lucris . lub . lu . se / ws / portalﬁles / portal / 3427902 / 4117182 . pdf ( 2013 ) 48 . Cleland - Huang , J . , Marrero , W . , Berenbach , B . : Goal - centric traceability : Using virtual plumblines to maintain critical systemic qualities . IEEE Transactions on Software Engineering 34 ( 5 ) ( 2008 ) 685 – 699