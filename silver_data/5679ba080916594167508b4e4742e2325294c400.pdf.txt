ii ~ II ii ill if if ill ~ : iiiiiiii i iii I : i il ii / i i i ~ i ~ i ~ ~ ii ~ i ~ : ~ ~ iiiiiiiii ~ i ~ Barry G . Silverman < ii iiii Q iiiiiiiii ~ i ii ~ i / ii ~ ~ . . . . . 106 April 1992 / Vo1 . 35 , NO . 4 / COMMUNICATIONS OF THE ACM he earliest use of the term ' critic ' to describe a program that critiques human experts is attributable to Perry Miller at Yale University . He pioneered a number of important concepts and approaches during his years of research with a family of critics called ATTENDING ( one was called ICON ) see 17 , 18 . Miller ' s use of the term refers to a computer program that critiques human - generated solutions , and that is the definition that will be used here . Another definition of a critic is Karl Popper ' s concept of creating statements in such a way as to maxi - mize their falsifiability see 19 . Ambiguous statements which have more than one meaning cannot be clearly confirmed logically , nor can they be completely disproven em - pirically . They may be true accord - ing to some interpretations . A clear and unambiguous statement has one meaning . In this sense it is fal - sifiable and can be tested for error . Our definition of critics will include programs that first cause their user to maximize the falsifiability of his statements and then proceed to check to see if errors exist . A good critic program doubts and traps its user into revealing his or her er - rors . It then attempts to help the user to make the necessary repairs . The purpose of this article is to survey the developments to date in the critiquing systems field . There is much to be learned from the re - sults of the numerous critics that have already been implemented in applications such as decisionmak - ing , engineering design , word pro - cessing , knowledge base acquisi - tion , and software engineering . The history of these implementa - tions is a story of a successful set of applications that were built out of necessity , and that are practical in their approach . Many of these are in daily use , improving the perfor - mance of large numbers of users . This experience is leading many others to build critics , and the field is expanding at a rapid rate . As a result , the survey provided here should be considered illustrative rather than exhaustive . By now , there are far too many critic pro - grams to cite them all in any mean - ingful way . Many developers of critic pro - grams are unaware of the extent of the field or of the latest findings at the state of the art . Also , many critic programs have been created , par - ticularly in knowledge base acquisi - tion and software engineering , by individuals who did not realize they were building critics . These are suc - cessful applications of the critiqu - ing approach . Yet , their developers failed to capitalize on even more successful techniques they could have implemented . Helping these and other developers benefit from what is known about critiquing the - ory today is another goal of this survey . A final purpose of this survey is to highlight some of the promising critic research challenges . In vari - ous situations critics still appear unintelligent , inflexible , frustrat - ing , and ill - conceived . There are numerous opportunities to over - come these barriers without sacri - ficing the simplicity and implemen - tability of the critic approach . This article addresses these opportuni - ties as well . Proposed Model of the Critiquing Process Before presenting the survey , it is useful to adopt an illustrative model of the critiquing process that can help in comparing critics . Cri - tiquing in electronic work environ - ments occurs when one is attempt - ing to complete specific tasks . As Figure 1 shows , the user initiates a task using some task - support soft - ware ( e . g . , an expert system , a com - puter - aided design package , a deci - sion support system - - or even a word processor ) . For a given task , the human ' s input to this environ - ment consists of : ( 1 ) the problem description ( e . g . , patient ' s symp - toms , design requirements , or type of document to be created ) . The problem may also be one the com - puter is displaying to the user , as perhaps in a process control appli - cation . ( 2 ) The second input is the proposed solution to the problem , such as a completed diagnosis , a final design , or a finished docu - ment . The quality of the human ' s solution is a sign of the level of task performance he or she is capable of . This second input is what distin - guishes an expert critic from either an expert system or an expert advi - sory system . Both the expert system and the expert advisory system compute their own solutions and offer those to the user as output . For a critic , the solution is part of the input , and the output is criti - cism . Unlike the critic , neither the expert advisory system nor the ex - pert system specifically examines the user ' s solution , nor do they pro - vide feedback specific to what the user did wrong . The critic system resides in this same electronic work environment , independent of , though closely in - teractive with , the task support soft - ware and user . Depending on whether the critic is designed as incremental or batch , it will analyze the user ' s task result before , dur - ing , or after the task , using the problem description . It will then provide feedback , criticism , and explanation to the user , so the user may improve his or her solution or performance . Some critics also offer preventive advice ( before - task COMMUNICATIONS OF THE ACM / April 1992 / Vol . 35 , No . 4 ~ O1 USER ELECTRONIC WORK ENVIRONMENT Initiate Task : Description ~ ~ " ^ . . . . " ~ ' ~ . , ~ ~ " ~ " ' | • Proposed k ° ~ ' ~ ' ~ . , ' , u ; ' o , ~ uc ~ . uVu / I I • ( Possibly Also : : i : ~ : : : : : : : i ~ i : . : : : Influencing + Rep I If : iii : : : i : . iii ; i •Errors I l : iiiii : i iiiiiii ii ! ! • Problems : : ~ : " I I : ii ! : : : : : i iii : . : > Untaken : i I Model I lii : i iiiii : ii ; ii : Opportunities : i i ~ : I i : ' i / : : Figure 1 . Overview of the critiquing process ~ ! ! iiiiI ~ iiii ! iii ~ iiii ~ i ~ ! ~ ! ~ i ! i ~ iiiiiii ~ iiiiiii ~ ! ~ i ! i ! ii ~ iii ! iiiii ~ iiiiiiii ~ ii ~ CORRESPONDENC ~ Is it externally unrealistic ? I Does it fail to correspond , with I } CLARITY Is it vague , ambiguous , or poorly understood ? Does it fail to include statements that are testable ? Figure 2 . Framework for critiquing a body of knowledge " influencers " ) , while others also include suggestions about how to fix the problem identified by the criticism . The critic normally functions by using a " different analyzer , " and a " dialog generator . " The differen - tial analyzer infers the user ' s goal and compares the user ' s task result to that produced by an expert mod - ule which is often a knowledge base of rules that an expert would run to perform the task . Differences be - yond an acceptance threshold are passed to a file of errors , This file might contain biases , or missed opportunities . Some critics reduce the set of criticisms to maximize what the user has already done . The dialog generator receives a file of errors from the differential analyzer , parses it into a user - presentable form , and displays it on the screen . Often critics use canned text strings to converse with the users . In some cases , however , crit - ics include varying degrees of lan - guage generation capability to im - prove the dialog with the user . Also , some critics incorporate mod - els of the user , that alter what is shown to the user , based on insights into that user ' s personal character - istics . Not many user - model critics are currently used in working ap - plications , however . In general , critics are easy to implement be - cause they leave the user - model out of their architecture . How Critiquing Works In all the environments where crit - ics exist , the user ' s solution or task result is in electronic knowledge . The user applies both judgment and knowledge to produce this re - sultant knowledge . Whether it is a document , a design , a plan , a knowledge base , or a finished les - son , the task result is some form of knowledge . It is this knowledge that the critic scrutinizes . For example , a business letter on a word processor may not seem to be knowledge . Yet , a style and grammar critic can ex - amine the judgement and knowl - edge used in forming the sentences . At this point , it is worth develop - ing a framework for classifying the possible types of knowledge cor - rectness checking . The reader should first note that the word cor - rect is too strong . It implies proving . The goal is to critique the credibility of the knowledge ( so it avoids dis - provability ) , rather than to prove the correctness of the user ' s task result . The framework flows from the idea that observers will reject the credibility of a body of knowledge if it fails any of the four tests that fol - low . These also appear in Figure 2 . ( 1 ) Clarity Tests - - A clear statement is unambiguous . It has one mean - ing . For instance , a generalization and compression of three almost identical rules into one rule is clearer than the three rules apart . Clear statements are also easier to falsify than ambiguous ones . This makes them more testable . One cannot clearly confirm or falsify 108 April I992 / Vol . 35 , No . 4 / COMMUNICA ' rlONS OF THE ACM ambiguous statements that have more than one meaning . None of the subsequent tests will be mean - ingful . So clarity ( falsifiability ) is an important first testing stage . That is why Figure 2 places it on the bot - tom . ( 2 ) Coherence Testing - - The next test is coherence . This deals with abstract truth , or the logical structure of sen - tences and statements ( including rules and equations ) . Coherence tests whether the result omits knowledge about the problem at hand . The common question to ask is : Is it incoherent ( i . e . , incomplete , inconsistent ) ? Verification equates to coherence testing . The term ver - ification means the passing of the tests of coherence or logic . ( 3 ) Correspondence Testing - - At this point the body of knowledge is rela - tively free of clarity and coherence concerns . Only now is it possible to find out if it also satisfies tests of correspondence . Correspondence concerns the agreement of state - ments with reality . It is a test to find out if a body of knowledge omits situations , experience , and / or em - pirical information ( either descrip - tive or normative ) that one knows to be relevant for practical prob - lems of the real world . Is it noncor - respondent with reality ? Validation equates to correspondence . Valida - tion means that knowledge passes the tests of correspondence or ex - perience . ( 4 ) Workability Testing - - Perfect de - scriptive knowledge is infinitely costly . The body of knowledge must stop short of that goal . Still , critiquing must explore whether the body of knowledge leads to de - scriptions or prescriptions that are unworkable ( errors of omission or commission ) . Does it fail the work - ability tests ? In this sense , effective critiquing must be a two - way com - munication process . It should not be a one - directional sermon ( from the machine to the user ) . Truth appears while solving a problem . One verifies and validates a body of knowledge pragmatically . It re - quires a mutual exchange of view - points and a two - way communica - tion between the expert human and the expert critic . This also implies one must vest power in the hands of the users . They need to have a say in what descriptive , normative , and / or prescriptive knowledge remains . These four credibility conditions are the tests of clarity , coherence , correspondence , and workability shown in Figure 2 . The discussion of these tests explains that credibil - ity does not require proof . It re - quires , instead , adequate testing for the problem or problems at hand . This suggests : PRINCIPLE : One can criticize knowledge in terms of its clarity , coherence , correspondence , and workability . If one test fails to discredit knowledge , the others are still free to do so . This principle permits a high degree of flexibility . It allows one to acquire knowledge from the ( active ) process of iterative interac - tions required by criticism dialogs . It also gives the flexibility to set specifications for the amount of testing required . These limits may be set by reasonable dimensions of practical problems . Specifically , the principle is one of critiquing the credibility of the user ' s task result ( so it avoids disprovability ) , rather than proving the correctness of it . What is most important is the realization that effective critiquing requires a process , not an event . There must be a mutual exchange of viewpoints , not a one - sided test . The critic needs interactive skills , not a sermon . It requires a mutual exchange , a search for truth , and a dialogue in which both parties can benefit , not just the recipient of the criticism . The discussion will now return to the survey of machine critics . From time to time , it will be important to recall that machine critics can im - plement the four types of credibil - ity tests . Also important is the inter - active , two - way exchange process . Today ' s machine critics face many challenges in implementing these COMPUTING PRACTICES features . Still , progress to date in these directions is encouraging . Survey of Five Categories of Critiquing System Usage This section will discuss five sets of critics : ( 1 ) the early influential prototypes , commercialized critics in ( 2 ) word processing and ( 3 ) engineering design , ( 4 ) " unacknowledged " use of critics in knowledge base acquisition and software engineering , and ( 5 ) other market places that seem primed to exploit the critiquing approach . As mentioned earlier , some critics are left out of this survey , not be - cause they are insignificant , but rather because this survey exists primarily to illustrate the range of what has been done . Also , some of the decisions made about what be - longs under early prototypes or under mature systems were made for the convenience of discussion - - some of the critics could be handled under both headings . An overview of the first three sets of critics of this survey may be found in Table 1 . It plots the sys - tems surveyed as the rows of the table and the alternative ways one might implement the architecture of the critic as the columns . The body of the table indicates which algorithmic alternatives each critic has pursued . In most critics the expert module is oriented toward problem solving ( ranging from the simple to the deep ) , while in a few it addresses how to prevent known problems from arising . The differ - ential analyzer often includes two stages - - criticism identification and criticism refinement - - each of which may be handled with varying degrees of sophistication . Similarly , there is a range of ways in which dialog or feedback may be sent to the user , to convey criticism the sys - tem has decided is worth showing . The last column is the user model . Details of each individual column is discussed as it arises in the survey . The last two groups of critics are COMMUNICATIONS OF THE ACM / April 1992 / Vol . 35 , No . 4 IO9 omitted from this table since they are either unacknowledged or as yet undeveloped . The Formative Years : Early and Influential Prototypes This section traces the evolution of the critiquing approach to date , by describing five sets of early , forma - tive critics . The ATTENDING Family of Critics . This family of critics includes five separate programs for five separate clinical medicine applications deal - ing with medical management , di - agnostic workup , and differential diagnosis . The details of the spe - cific application domains are less important than the principal out - comes of Miller ' s research 17 , 18 . Essentially , he learned that differ - ences in the kinds of problems to be solved leads to the importance of different critic architectures and algorithms in the critic ' s differential analyzer . These differences and the resulting lessons learned will be highlighted below . E - ATTENDING is the empty shell Miller evolved for program - ming critic applications . He argues that it is more efficient for critic applications than general purpose expert system languages such as OPS - 5 , since it exclusively offers the features one requires in the cri - tiquing process . As delineated in Table l , E - ATTENDING includes a differential analyzer and a dialog generator but it does not have a user model . It also includes a sim - ple but useful knowledge exerciser that helps in testing the correctness of the critic ' s knowledge . Miller purposely avoided going beyond the prototype stage with any of his critics so he could con - centrate on research issues across a span of different application do - mains . His principal finding was that simpler differential analyzer algorithms apply to domains where the possible number of correct an - swers on any given task were few . Alternatively , research into more sophisticated algorithms is required where numerous correct answers might be possible and the search space of possible user errors is am - biguous or uncertain . Hence the need for risk assessment , or con - flicting goal management . More specifically , the lessons learned include : when to use cri - tiquing by reacting , when to cri - tique by local risk analysis , and when to critique by global plan analysis . Critiquing by reacting occurs where : ( 1 ) specific rules can be written for each type of wrong answer , ( 2 ) the rules for assessing user so - lution optimality are objective and few , ( 3 ) only one or two possible correct outcomes exist for that task , and ( 4 ) each subtask can be critiqued independently of the others . In this case , the differential advisor can critique by simply reacting , by Table 1 . Overview of HOW the Critics of This SUrvey Implement the Critiquing Process LIJ ' m = ~ i , _ c l # " O O ~ . _ = e - IJJ Critics In Survey E - ATI ' ENDING ONCOCIN Colorado Critics COPE Shell Spell , ' Grammar Checkers Style Replicators Document Critics ( COPE - TIME ) CRITTER VLSI Critics Mechanical Parts Design Shipboard Antennas Expert Module Differential Analyzer Dialog Generator Problem Solution Criticism Criticism Theory Problem Identification Refinement Direct Graphic Lookup Infer - Deep Preven - Error Sophis - Degree Manipu - Tables ence Model tion Isola - ticated lation / and Engine Man ~ pu - Theory tion / Dec . Feed - Files + KB lation Diag - Theories : back nosis • • ® O • • • • O @ @ • • O O • O • • O • ® @ • 0 0 ® 0 0 O 0 • • @ • @ • • 0 © @ 0 • • • 0 ® 0 Simple Exhaus - tive Compar - ison @ @ © ® User of Error Solution Cutoff Adoption Canned Hierar - Dynamic User Static chical Prose Model Text Text / Gener - Appli - Expla - ation cator ation • 0 @ 0 0 @ 0 @ • ® 0 ® • • ® @ 0 @ • © @ 0 • ® 0 © • • • @ 0 @ 0 • 0 • 0 • • 0 0 • • 0 0 • • 0 © • @ • 0 • 0 © @ @ • • 0 @ ® • © 0 0 0 • 0 0 0 @ • ® ® 0 0 • 0 0 0 @ • ® © 0 • 0 0 0 0 0 0 • • • • 0 • LEGEND : O Not Used O Used to a Degree • Used 110 April 1992 / Vol . 35 , No . 4 / OOMMUNICATIONS OF THE ACM looking up the feedback in a table or knowledge base of rules . This is how most grammar critics currently work . If conditions ( 1 ) through ( 3 ) de - teriorate and the domain has nu - merous feasible solutions and pri - marily subjective standards for evaluation , then critiquing requires evaluation of the risks and benefits of a number of alternative solu - tions . These are dynamically instantiated with the values of the current problem to determine if the user - proposed solution falls within the envelope of acceptable solution sets . This , Miller calls critiquing by local risk analysis . Finally , if condi - tion ( 4 ) does not hold for a given domain , then critiquing cannot simply address an isolated portion of the proposed solution ( e . g . , each subdecision in isolation ) , but most adopt a global view to examine the totality of what the user is suggest - ing . This is called critiquing by global plan analysis , a form of critiquing neither Miller nor anyone else in the literature has yet attempted . This kind of critiquing can become important , however , particularly in some domains like engineering de - sign , discussed later . ONCOCIN : Reconfiguring an Expert System Into a Critic . Unlike AT - TENDING , which was designed specifically with critiquing in mind , ONCOCIN started as an expert system for use in diagnosing cancer problems in a clinical setting . The principal contribution of ON - COCIN is to illustrate how a tradi - tional expert system can be con - verted to a critiquing process . Langlotz and Shortliffe built a pro - totype to test this process , because they found user acceptance prob - lems with the expert system ' s human - computer interface . In the original system , the human expert was largely reduced to the role of data gatherer and recipient of the machine ' s solution . In the con - verted system , however , Langlotz and Shortliffe report , " Instead of acting as a mechanized consultant that methodically asks for findings and renders a treatment decision which the physician must override if a disagreement exists , the expert clinical consultant becomes a silent partner in the decision making pro - cess and only makes its opinions known when a suboptimal therapy is proposed by the physician . " 13 , p . 495 . The conversion from an expert system to a critiquing process pri - marily involved adding a differen - tial analyzer that would : suppress the expert system ' s diagnosis until after the user had also input his or her own diagnosis ( the machine would request that input ) , compare its diagnosis to that of the human user , and determine if the human deviated significantly enough from the machine ' s ( " optimal " ) diagnosis and plan , to warrant interrupting the human to explain the problem it had uncovered . The comparison process uses a hierarchical plan analysis approach that is based on domain knowledge and on pre - programmed rules that account for allowable discrepancies on a diagnosis - by - diagnosis and drug - by - drug basis . This is an exhaustive approach which is not reusable in other applications ( i . e . , the domain rules must be recoded for each new application ) , but if the new critic is to be used often enough the invest - ment may be justified . In terms of the dialog generator , the ONCOCIN critic prototype added some capability that might be reusable . When feedback must be given to the user , it is unlikely the user needs the entire chain of reasoning with all its branches and detailed information . ONCOCIN , like ATTENDING however , has no user model ( and no updated store of contextual information ) to tailor the explanations to the needs of the current user . Instead , ONCOCIN uses a user - selectable " agenda . " That is , the system extracts a list of the parameters contained in each rule that appears in the explana - tion . ( These are the rules that exe - cuted to pinpoint suboptimalities in the user ' s therapy plan . ) These pa - rameters are terms the user can re - COMPUTING PRACTICES late to , and they are displayed as a menu or agenda the user can select from in order to obtain a line of explanation . In this way the expla - nation tree can be traversed in an arbitrary order according to the needs of the user . CRITTER : The Beginnings of the Engineering Uses . In engineering design , great care is usually taken to analyze a design for a wide variety of weaknesses . Some of them are : failing to satisfy a specification or requirement ; neglecting to analyze over the life of the system , the pro - jected flaws in reliability , maintain - ability , or others ( interoperability , susceptibility , usability ) , and ignor - ing manufacturing constraints . There are also cost faults ( areas of needless goldplating ) that can al - ways be found . A host of tools exist for these purposes which could be incorporated into an engineering critic . That the field of engineering , and particularly engineering de - sign , is ripe for the critiquing ap - proach may explain the increase in design critic applications discussed later . Possibly the earliest instance of such an approach was Kelly ' s CRITTER , a system that evaluates digital circuit designs for issues in - volving functional correctness , op - erating speed , timing , robustness , and circuit sensitivity to changes in device parameters 12 . Like most engineering critic applications to follow , CRITTER had to pioneer a nonprocedural ( e . g . , rule or frame - based ) representation which was able to accommodate mathematical functions and interface to dynamic numerical analysis capabilities . CRITTER is thus an example of how to embody varied and useful kinds of reasoning within a single critic program . The input to CRITTER is a cir - cuit schematic or solved design , plus a set of specifications of the desired behavior ( the problem to be solved ) , along with characteristic signals it should be able to accom - modate . CRITTER ' s expert mod - ule assumes the task of emulating COMMUNICATIONS OF THE ACM / April 1992 / Vo1 . 35 , No . 4 ~ the circuit performance ( simula - tions , path delay analyses , proofs of specifications ) and in this regard , CRITTER is an example of the use of stochastic simulation of a de - sign ' s performance to assess its weaknesses ( other later critics intro - duced additional analytical tech - niques ) . The differential analyzer compares the user ' s results to the machine ' s and isolates weaknesses . The dialog generator reduces data complexity and presents only the findings it judges to be unusual or unexpected . It also retains the abil - ity to justify its conclusions with an orderly presentation of supporting data when required . While a num - ber of simplifications , limitations , and purposeful restrictions exist in CRITTER , it successfully ran on several circuit designs . Increasing the Usability . The term " Colorado critics " is used here as a catchall for the work of Gerhard Fischer and his colleagues at the University of Colorado ( some are now graduated , and building critics elsewhere ) , who have generated a series of critics over the past several years . What is interesting about the Colorado critics is their contribu - tion to our understanding of how critics can and should be embedded in electronic work environments . In each of the ATTENDING , ON - COCIN , and CRITTER applica - tions , the critic process is the pri - mary attraction to the user for using that environment . Fischer is more concerned with the role critics play when embedded in other , high - function , complex environ - ments . Fischer has been researching a comprehension - centered theory of human - computer interaction , where the goal is to develop design principles for systems that are radi - cally easier to understand and use . In more concrete terms , he and his colleagues are concerned with mak - ing difficult work situations , elec - tronic or otherwise , comprehensi - ble to humans . He uses a host of knowledge - based techniques ( such as hypertext , user models , visual representations , and alternative control strategies , to mention a few ) to do this . Examples of complex , or high functionality computer envi - ronments might include strictly electronic ones such as CAD or pro - gramming environments , as well as physical environments such as air - plane cockpits or appliances around the home . One comes to these environ - ments to perform a task such as designing a kitchen , as in 7 , writ - ing a Lisp program as in 5 , or in designing windows for user inter - faces such as in 6 . The Colorado critics in each of these environ - ments serve to improve the user ' s performance by interrupting when feedback must be provided . Thus for example , a Lisp programmer would be interrupted during his or her programming task if he or she committed a clarity error ( will other programmers be able to read this code ? ) or a workability error ( is this the most efficient way to write that function ? ) . If no errors are commit - ted , the user is unaware that a critic even exists in the environment . In this fashion , users can approach high functionality environments with greater confidence that they can use them without having to spend days or weeks in a trading mode . Fischer ' s philosophy of combin - ing the best ideas for increasing the usability of the environment has led him and his researchers to other contributions useful to the field of critiquing . One contribution his researchers incorporated was hy - pertext into the critic ' s feedback loop and the creation of what they call " minimalist explanations " . Thus , for example , in the Lisp critic , the programmer ( criticism recipient ) is initially provided with a minimal amount of text in the ex - planation of the criticism and via hyper - jumps may proceed to obtain more indepth explanations , even progressing into the relevant sec - tions of an on - line Lisp reference manual if desired . Another contri - bution is in exploring possibilities and alternatives for implementing a user model inside of the Lisp critic as previously mentioned 16 . COPE ' s Contributions : A Theory of Expert Errors and Critiquing . While the purpose of Section 3 . 1 is to de - scribe five early and influential pro - totypes , COPE as it exists today is both a prototype and a full - strength , production system . The reason for this is that COPE is a generic critic shell capable of gen - erating fielded , practical critic sys - tems ( full - strength ) , but at the same time it is a research tool for investi - gating the nature of critics ( proto - type shell ) . It is also early because it began in 1986 ( see 27 ) , and it is influential because it is the first at - tempt to develop and apply a the - ory of bugs and repairs for criticiz - ing expert error . The goal of this research is to explore whether cog - nitive bias ( i . e . , systematic error ) in expert practitioners can be detected and mitigated by the critiquing pro - cess . One of the problems this class of applications attempts to solve is that a human judge of a practition - er ' s work is often subject to the same types of biases as the practi - tioner . Hence , the judge cannot detect and eliminate such errors when they arise . What follow now are three contributions that have come from the COPE research . The first goal of the COPE re - lated research is to derive , test , and validate a theory of expert error . The research objective is to exploit a rich theoretical base of principles about general frailties of human cognition . The results to date have been promising . For example , in preparing curriculum plans , grad - uate students are seen to regularly overlook both their stated educa - tional objective and written rules of guidance in the face of more con - crete fears about current semester grades 22 . Qualified statisticians fail to recognize everyday situations as classical probability and statistics problems , due to a human ten - dency to disregard abstract data , as described in experiments reported in 26 . Mental myopia arises in trained professionals of a variety of 112 April 1992 / % 1 . 35 , No . 4 / COMMUNICATIONS OF 1rile ACM ~ , O , ' Q ' a a a a a a a a s a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a s a a a a a a a a a a s s a a a a a a a a a s a a s a a a a disciplines due to force of habit , se - lective perception , and availability biases , as reported in 24 . This approach shows the use of domain - independent theories of systematic errors in human judgment for de - signing critics that help reasonable experts cope with weaknesses in intuition they tend to overlook . A second goal of the COPE re - search is to develop a theory of crit - icism ( rather than error ) with a pre - dictive value that can be used to make critic design decisions . Miller started such an endeavor with his study of how the differential ana - lyzer algorithm must change as the domain varies from constrained to subjective and so forth . Still , data on improvements in user perfor - mance as a result of critics are nota - bly absent from the literature . For example , ATTENDING , ON - COCIN , and CRITTER all make use of after - task , batch criticism , yet the research offers no results to support their intuitive argument that this strategy improves user acceptability and / or performance . There are numerous surprises in terms of both strengths and points of breakdown in the critiquing approach that can only be deter - mined through careful study of performance results under varying conditions and factors . The empiri - cal results of COPE to date show greater improvement from before - task influencing ; incremental , dur - ing task interruption ; and various user - sensitive items about the criti - cism itself . Perhaps as important as these experimental findings are a set of methodological approaches that had to be developed to engineer , program , evaluate , and refine the critics created by the author and his colleagues . These engineering , programming , and evaluation methodologies have received a sig - nificant amount of use , and have led to the creation of one of the largest critics yet built ( TIME , in 23 is a fielded critic generated in COPE with over 1 , 500 rules , 1 , 500 objects , and 2 , 000 textual note - cards ) . These methodologies were designed to improve the speed and precision with which critics can be programmed . While selected as - pects of these methodologies will be of interest to the reader designing a small critic ( a few dozen rules or less ) , the full methodology will be relevant for anyone attempting siz - able applications , or for anyone in - terested in precisely measuring the impact of their critic on user per - formance . Critics Permeate the Desktop Publishing Industry The critiquing approach has pro - gressed quite rapidly in several fields to become a widely utilized technique , primarily because of its utility and cognitive ' naturalness ' . Some of the more noticeable work - ing applications are reviewed in this and the next two sections , although again , this is not an exhaustive sur - vey . By far the largest number of critic applications to date , possibly as many as a million users , exist in the field of desktop publishing and word processing . These , however , are some of the more trivial appli - cations from an architectural or in - telligence standpoint . In particular , these include the spelling - , grammar - , and most recently the style - checkers available in numer - ous word processing environments . These are passive critics that are invoked by the user during or after the task of authoring a piece of text . The input from the user is the text , the problem statement is " critique this text for spelling or grammar errors . " The output is either auto - matic correction or , in interactive mode , reverse video showing the trouble spots , sometimes accompa - nied by a line or two of textual ex - planation . Spelling checkers in particular , are the crudest example of what Miller refers to as table - driven lookup , and they arguably are not knowlege - based critics . The first generation of grammar - and style - checkers , which were marketed in the early 1980s , were almost as sim - plistic as spelling checkers . They COMPUTING PRACTICES used a brute - force differential ana - lyzer , scanning the text or docu - ment one letter at a time , looking for strings of text that matched those in the phrase dictionary . This allowed them to find clarity and consistency errors ( e . g . , incomplete parentheses , tense changes ) , but their understanding of grammar and style was minimal . These first - generation checkers had no idea of context or of the relationship of words to each other . Thus if a pro - gram were told to criticize the use of the word " type " as a verb in its latest computer science meaning , " to classify a piece knowledge , " they would also criticize all other occur - rences of the word in all its other verb , noun , meanings - - even in its role as part of other words such as " typewriter . " By the end of the decade , how - ever , true grammar - and phrase - critic programs became commer - cially available and are now being widely distributed . These critics have been shown to find about 25 % of the grammar errors and 80 % of the objectionable phrases ( stylistic errors ) in typical office documents . This is because the differential ana - lyzers now do the following : ( 1 ) They compare each word in a sentence of the document to a sen - tence file listing the parts of speech ( verb , adverb , noun ) that approxi - mately 100 , 000 of the most com - mon words in the English language can assume . The dictionary also identifies the range of potential functions , conjugations , tenses , and so on , that each word can legally assume . ( 2 ) The analyzers next parse the sentence into its main grammatical components and examine the order in which the words appear as well as the other legal ranges the diction - ary ( expert module ) says the words should assume . Dozens of tests may be applied to each sentence , includ - ing comparing it to a file of objec - tionable words . The file is sensitive to context ( e . g . , " type " as a verb for " classifying " ) . Finally , the dialog generator takes the list of errors COMMUNICATIONSOFTHE ACM / Apr ; 1992 / Vol . 35 , No . 4 ~ i ! 3 and matches them against a table of messages to send to the users . Spellers and grammar / style checkers operate at the string level . There is an additional class of crit - ics , called style replicators ( e . g . , 20 ) , that analyze three levels of the document : ( 1 ) at the word level they do sylla - ble counts , ( 2 ) at the sentence level they do counts of runs of long and short words , and ( 3 ) at the document level they use formulas for sentence readability based on word length and run scores to predict the difficulty of the sentences and sections of the document . The criticism from such programs can be used to encourage the user to adopt a consistent style throughout a given document . These programs can also be given a preferred styleIthey generate the style model themselves from scan - ning a preferred document ( e . g . , a winning proposal , a popular novel , or a highly readable section of a government regulation ) . The user can then interact with the style rep - licator and receive criticism about the way in which he or she is deviat - ing from the preferred or expert model . These programs have some ob - stacles yet to overcome . Only the spelling checkers are embedded in the word processor . The grammar and style checkers are either too large to fit or proprietary obstacles exist . To overcome this problem the grammar and style critics place your document in their editor win - dow . The critic runs in the back - ground of this editor . Unfortu - nately , this editor is crude and unreliable . In my use of a critic in editing this article , it crashed fre - quently . It takes 20 minutes to save the document and restart the critic . To be safe , this had to be done every hour and a half . Also , the critic - provided editor frequently loses the word being typed . One sometimes has to enter a correction two or three times before it appears correctly . Another category of obstacle in the grammar and style critics con - cerns their inflexibility . Too many of their rules still apply to all users . These are difficult to turn off just for situations where they are un - necessary . For example , the unbal - anced parentheses rule could not distinguish between the use of a single right hand parentheses in a section header and a legitimate pa - rentheses error . The commonly confused words rule lacks a way to detect if one has an error . It inter - rupts the user for every occurrence of a long list of commonly confused words . As a third example , the pas - sive voice critic states it is fine to use passive voice in less than 25 % of the document or in about four situa - tions . Yet , it labels every such oc - currence as an error . The net effect of these concerns is that the critics intrude . A full 35 % of the " errors " in this document were like those just mentioned . That is , the critics have an intrusion ratio of 35 % . In these cases they are irritating , time consuming , and unnecessary . Still , the grammar and style critic is a partial success . It got me started on the road to correcting my com - mon errors . Although I learned ( thanks to the critic ) how to make the corrections outside the critic ' s editor , the critic is still useful for small error criticisms . That is , I make my corrections for editor - intensive operations such as passive voice , long sentences , and long - winded passages . I then run the critic in interactive mode . The re - maining errors are many but they do not strain the critic ' s editor . Also , the critic tells me my readabil - ity score improvements . In sum - mary , these types of critics are use - ful . Still , they have several challenges yet to overcome . There are other challenges that grammar and style critics ultimately will conquer as well . These pro - grams still cannot catch correspon - dence errors such as " the green cheese is made of moon , " but the field is moving in that direction as well . For one thing , researchers are beginning to program large scale knowledge bases that provide con - textual and background knowledge about the world , and which should be available by the mid - 1990s ( see 14 ) . A second trend is the appear - ance of document generators , most of which are intelligent templates or fill - out - the - forms types of pro - grams , yet a few are active expert systems . Merging these two types of technology and adding the critics already discussed could lead to a powerful authoring environment able to turn out standard reports such as term papers , legal docu - ments ( wills , deeds , mortgages ) and other common types of documents . This is likely to be available by the end of century . Intelligent authoring critics are one step away from successfully cri - tiquing at the correspondence level . While this and generalized critics may be further down the road , sev - eral existing and implemented pro - grams currently offer correspon - dence checking and critiquing for the specific domain models they have incorporated into their expert module . If one of these approaches could be extended and connected with a large scale common sense knowledge base , the capability to recognize the flaw in sentences like " The green cheese is made of moon , " may very well be feasible . Penetration Into Computer Aided Design ( CAD ) Environments Engineering design was already mentioned as an area that is ripe for widespread use of the critiquing approach because there are already so many error analysis tools that critiquing can make use of ( as Kelly did with simulation ) and also , in a Fischerian sense , because of the complexity of the high functionality CAD environments . CRITTER was the sole engineering application in 1985 . Now there are many . This section describes in brief four critics in various stages of being fielded . Some are already commercially available . The next several years should see a virtual explosion of critics , leading to increasingly intel - ligent CAD environments . In addi - 114 April 1992 / Vol . 35 , No . 4 / COMMUNICATIONS OF THE ACM tion , these environments will grow more versatile , expanding the breadth of what each critic does as they mature . By 1988 three CAD , system - embedded critics that were fielded were reported in the literature . One , developed by Steele at NCR 29 , was a commercially - available system that provided batch , after - task feedback on user - specified , application - specific integrated cir - cuit designs . Called the Design Advisor ( TM ) it had a truth mainte - nance system , and its differential analyzer used rules with a frame representation to reason about the performance , testability , manufac - turability , and overall quality of CMOS semicustom VLSI designs . The expert module held a library of common design problems culled from the study of several years ' worth of major design problems in commercial VLSI designs . A second , closely related com - mercial application is a critic - - called Critic - - that is integrated with the Berkeley CAD System for CMOS , NMOS , and general types of circuits 28 . The focus of this system is to look for errors so the user will not have to track them down with traditional error analysis tools . Often users do not find er - rors because of the breadth of the search space , and their failure to incorporate just the right ranges of test parameter values . The auto - mated critic can catch many such errors heuristically and inform the user about the need to make design modifications . It also can find er - rors for novices and examine de - sign - style compliance . This system has been around for several years and a number of articles and tech - nical reports exist on its results 28 . The third CAD critic is that of Jakiela 10 , which encoded an ex - pert module for the design of parts in a machining domain and that would factor manufacturing con - straints into the designer ' s feedback loop . Thus the differential analyzer would note when an otherwise use - ful design would be likely to incur manufacturing difficulties or unac - ceptable machining costs . Feedback is provided to the user in terms of both text and graphical portrayal ( redrawing of the offending por - tion of the part ) . Experiments on the value of during - versus after - task critiquing have so far proved inconclusive . " Unacknowledged " Critics Critics are also currently in wide - spread use in the fields of knowl - edge - base acquisition and software engineering . In these applications , the critic techniques are more primitive than most of those we have just surveyed . Also , the users and developers of these program - mers do not call them critics and they are unaware that they are using the critiquing approach . For example , source code debuggers and language sensitive editors criti - cize the syntactic properties of the user ' s task result . Code skeleton graphers and dataflow analyzers critique completeness and clarity and code optimizers critique the workability of the program . Other examples follow . It is useful to view these catego - ries of programs as critics to see how they can be improved and ex - tended . That is , since the software engineering and knowledge acqui - sition fields are unaware of the the - ory of the critic approach , they tend to use an impoverished set of critic capabilities , incorrect theories of user error and interaction , and out - dated techniques . These fields could benefit from recognizing they are using the critiquing ap - proach . Specifically , they could use the results described in this survey to implement critics with appropri - ate theories of user error , better coverage of the coherence , clarity , range of capabilities , modern archi - tectures / algorithms , and latest re - sults on effectiveness of alternative modes and styles of interaction . These benefits promise direct and significant payoff in reducing both the number of errors in the devel - opment process and the high cost of maintaining a poor program / knowledge base even if it has no COMPUTING PRACTICES obvious errors during the develop - ment step . In the use of software engineer - ing here , the concern is not just with the programmer positions , but also with the entire team of project managers , requirement analysts , designers , maintainers , and so on . Each has a Computer Aided Soft - ware Engineering ( CASE ) environ - ment in which he or she performs tasks . These are the proper places to embed critics . Consider a matrix of software engineering life cycle steps in one dimension and the other dimension consisting of the four error categories . Only the pro - grammers and maintainers cur - rently have error checking pro - grams that could be called critics , although all need such assistance . Further , the critics for program - mers / maintainers are overly con - cerned with syntax , after task batch debiasing , and passive mode of use . Also , these tools are almost exclu - sively within step critics . Yet , one could populate this entire matrix with a variety of critics that look for cross step errors ( code that is hard to maintain ) , other position errors ( software metrics and quality checkers for managers ) , errors be - fore they arise ( the sooner you re - move an error the cheaper the soft - ware ) , and so on . Many other possibilities also come to mind . A correlate to this situation exists in the field of knowledge base ac - quisition . The field of expert sys - tems is currently evolving auto - mated knowledge acquisition systems that permit nonprogram - mers to author their own knowl - edge bases . This is an important class of systems that is trying to break the bottleneck of creating knowledge bases by eliminating or reducing the need for the knowl - edge engineer . These acquisition systems directly interview the nonprogramming domain expert for his or her rules and translate them into the syntax of the rule or object language of interest . To help the user create a debugged knowl - edge base , the knowledge acquisi - tion systems include a number of COMMUNICATIONS OF THE AOM / April 1992 / Vo1 . 35 , No . 4 11s jiijjilmttmjajlilltiitmljljAljjitmtmmiliAiJmttiimmJlimiJilitmm | imttmiiitttJm checking routines that qualify as critics . Like software engineering , the knowledge acquisition system field ' s use of the critiquing process evolved out of a practice - driven approach , and not out of an explicit attempt to embed the critiquing process into their programs . Their concern was to build whatever was needed to assure the correctness of the knowledge base created by the domain expert . Nevertheless , the result from the point of view of this survey , is the same as if they had specifically set out to embed the cri - tiquing process in their knowledge acquisition environment . For knowledge acquisition system de - velopers , it is hoped that this survey will enrich and make explicit the set of criticism - based options available to this rapidly evolving practice . Also , the discussion that follows is relevant to the designers of any critic since most are concerned with the acquisition of knowledge . That is , engineering critics help acquire designs , medical critics help acquire diagnoses , grammar critics help acquire documents , and so on . To facilitate the presentation of a critiquing example ( discussed in the next section ) , we will now examine the types of KB debugging support that knowledge acquisition systems offer to their users . In particular , the classification of systems for knowledge acquisition via inter - viewing is shown in Figure 3 to in - clude the four principal types of critiquing we will describe . The examples cited in the leaf nodes of Figure 3 may straddle more than one of the four principal types but are so placed because they were early or outstanding examples of that type of critiquing support . Ar - ticles on most of these systems may be found in 2 and will not be indi - vidually cited here . Structural coherence checking systems , the first of these four types , as concerned with the inter - nal logical correctness of the KB being developed . AQUINAS and KITTEN are examples of the sim - plest way of supporting bug identi - fication under this type of ap - proach . Consistency ratios are shown to the user as metrics of the correctness of his or her relative importance weightings thus far . This is informative , but limited in debugging assistance . Other inter - viewing tools , a notable example of which is KREME , are intelligent editors at the knowledge object , rule , or method level such as type checking , cross - reference checking , and subsumption correctness checking . Each of these services includes support for fault identifi - cation and repair . At a more global KB - wide perspective , several tools examine the full KB for consistency and completeness . MORE is an early example of an interviewing system that compares the knowl - edge it has collected to date with an internal model of the knowledge required for a complete KB , and uses the differences to fuel its que - rying . Similarly , CHECK uses graph detection algorithms to de - tect a wide array of inconsistencies such as dead - end chains , circular rule paths , and unreachable goals . Even though it is not part of an au - tomated interviewing system , when manually applied to the KB , it per - forms these functions . In addition to the logical or structural coherence of what is transferred to the KB it is impor - tant that the content be clear , partic - ularly if others most understand , read , reuse , and / or maintain the KB . Clarity checking and repairing aids , the second major type of de - bugging aid shown in Figure 3 , have received less attention than logical coherence techniques , possi - bly since they seem to require the use of software engineering tools rather than strict logic - oriented approaches or possibly since the clarity problems are noticed less often as a serious KB problem . Three widely different examples of clarity - enhancing interviewing aids are XCON ' s RIME which attempts to impose common and good rule writing practices across different KB authors ; EES which utilizes au - tomatic program translation tech - niques to instantiate high level KB principles into numerous lower level rule instantiations regardless of their author , thereby facilitating explainability and maintainability ; and Conceptual Comparison which clarifies consensus and conflict in the terminology and concepts across numerous KB authors . Clearly , with such a range of ap - proaches there is room for more research in this type of criticism aid set . A more heavily studied set of cri - tiquing techniques involves real case performance checking or workability checking . This is an old and popular approach , going back to TEIRESlUS . Workability check - ing is used in several more recent systems . TIMM attempts to solve real cases with only a partial KB , while the interviewing system helps the use to add corrective rules as needed . A variant of this approach is popular in the machine learning field , where the tutor provides the machine a correct trace of rule fir - ings needed to solve the real case . The machine then attempts to diag - nose and correct any KB deficien - cies as best it can , and resorts to in - terviewing only when it cannot reproduce the correct trace : . ( e . g . , see MOLE in 2 ) . The final set of critiquing aids check for correspondence to the real world . Similar to clarity check - ing , this approach is understudied , and few examples exist . The inter - viewing systems mentioned thus far view the human as a tutor who know the correct knowledge to transfer to the KB and critiquing serves to help in that endeavor . The human , however , may at times bet - ter be viewed as a consultant pro - viding information rather than as a tutor transferring lessons . In this fashion the machine ' s cognitive model can doubt the information , assess if it is relevant for another problem rather than for the prob - lem at hand , and wonder about the correspondence of the human ' s in - formation relative to reality . This approach requires the machine to have a model of the real world 116 April 1992 / Vol . 35 , No . 4 / COMMUNICATIONS OF THE ACM COMPUTING PRACTICES Debugging Support in Knowledge Interviewing Systems Knowledge Knowledge Structural Clarity Coherence Critiquing Critiquing ( e . g . , RIME , EES , CC ) Correctness Type and KB Consistence Metrics Cross Reference & Completeness Approaches Critics Critics ( e . g . , AQUINAS ( e . g . , KREME ) ( e . g . , MORE , KITTEN ) CHECK ) Real Case Real World Performance Correspondence Critiquing Critiquing User Correct World Judgement Interactive Solution Model Bias Critics Trace Critics Critics Critics ( e . g . , TEIRESlUS ( e . g . , MOLE , ( e . g . , KNACK ) ( COPE ) TIMM ) LEAP , AMBER ) Figure 3 . A debugging support classi - fication of knowledge acquisition sys - tems against which it can check a " les - son ' s " content . In one approach to correspondence checking , KNACK checks human knowledge relative to a model of the physical laws of nature and of electrical devices . COPE - TIME ' s checking for gen - eral human cognition errors is an - other approach to correspondence checking . To summarize the state of the art in critiquing support during knowl - edge interviewing , Table 2 portrays the type of critiquing technique down the left side , and attempts to show when each technique would be useful for a given user situation ( middle column ) and what kind of help it can be expected to offer ( right hand column ) . Coherence critiquing is useful to help human authors ( that is , experts who pro - vide knowledge to machines ) grap - ple with the complexity of the pro - gram artifacts . Clarity critiquing influences style - biased human au - thors either to cause them to con - form to a standard , or to flush out differences for discussion . Work - ability critiquing helps the situa - tionally - triggered human expert recall heuristics in the context of Table 2 . Scope and Purpose of Various Types of Critic TeStS when Embedded in Knowledge Acquisition SyStems Use When Human ' s Purpose / Strength Critic Type ROle IS : of Critic Coherence Knowledge Helps User With Internal Base Consistency and Syntactic Author or Superficial Completeness Clarity Style - Biased Helps in Multi - User , Cross - Author Terminology Settings Workability Situationally - Helps User Remember Triggered Heuristics , Helps with Author Slips / Lapses Correspondence Highly - Specialized Helps Influence and Debias Author or Content - user ' s Judgments AbOut Biased Consultant the Domain the problem . Finally , correspon - dence critiquing helps either the highly specialized author , or con - tent - biased consultant debias his or her judgments and assertions . The field of knowledge acquisition sys - tems has thus developed a wide array of tools to help overcome many categories of interviewing obstacles when the author is a valid tutor , but few techniques yet exist for interviewing potentially suscep - tible and / or biased authors . Achiev - ing correspondence checking capa - bilities is as yet an unachieved and unstated goal of most knowledge acquisition systems , although this goal along with that of adding clar - ity critics is an attainable one - - as can be seen from this survey . Other Classes of Applications In addition to the desktop pub - lishing / document generating , CAD environments , knowledge acqui - sition and software engineering applications that are already ex - hibiting the effects of the industrialization of the critic ap - proach , several other domains ap - pear ripe for exploitation of critics as well . For example , numerous business and decision making envi - ronments have decision support COMMUNICATIONS OF THE ACM / April 1992 / Vol . 35 , No . ÷ ! ! 7 i i i i l l i i i i l i l l i l i l l i i l i l i i l i l l l l i l i i l i l l l l i i l i l l l i l i l l i l i l l i l l l l l l l l i l i l i l l l systems where critics could be em - bedded . As a test of the value of heuristic critics in decision making settings , several experiments are reported in 26 on a critic that debiases judgment under uncer - tainty . Similarly , the critic of the case study presented in 23 , while currently instantiated for a real world military forecasting problem , is generic and should be applicable to other forecasting applications in business , meteorology , and the like . Command and control settings are yet another domain where au - tomation levels are high and expert critics rather than expert advisory systems should prove to be useful . For example , in the pilot ' s cockpit or in control room consoles of power stations , air traffic towers , satellite control centers , and the like , the architecture of the critiqu - ing process is eminently well - suited to silently observing the human process , and only interrupting when a problem arises : ( e . g . , 9 ) . One research question for this class of domains that does not arise in others is the difficulty of having the computer discern what stream of inputs from the user are associated with which task and / or problem . That is , in all the other critic appli - cations discussed in this survey there is a discrete task and a dis - crete proposed solution . Command and control environments , how - ever , are continuous and real time . There is no point at which the user says , now I am done with that anomalous condition and let me move on to the next one . The critic would need to infer this . A final class of environments exhibiting significant promise for future critic activity is training envi - ronments . Unlike the other envi - ronments , training is a theory - rich environment . It contains many pro - posals , prototypes , and ideas of potential value to the critiquing process as the section on tutoring systems will explain . In a sense , in - telligent tutoring systems ( hereafter called tutors ) do for novices what critic systems do for experts . Both try to communicate to the user and to correct their errors . Yet , most tutors are geared to correct errors in the use of new knowledge and skills , while most critics are geared to correct errors in the judgment or use of skills presumably already mastered . Still , both types of pro - grams use an expert module , some form of differential analyzer , and a dialog generator . The principal source of differ - ence between critics and tutors lies in the openendedness of the do - mains that critics must work in . That is , tutors are usually built in narrow domains where all of the errors that students could possibly make can be identified and added to the expert module . This makes it possible to do a more thorough job of knowledge communication , and much of the technology of tutoring hinges on the completeness of this enumeration of possible user er - rors . Critics , by contrast , have adopted less complicated technolo - gies because they have to relax the completeness constraint - - also , be - cause they do not need all the appa - ratus of tutoring to remind an ex - pert of a skill he or she neglected to apply . To use a metaphor , a tutor is a power tool useful to solve large problems while a critic is more of a surgical instrument appropriate for smaller repairs . This greater sim - plicity has allowed critics to be more successful than intelligent tutoring systems in terms of the large num - ber of fielded systems , but it means that critics do not say the right thing at the right time as often as tutors do . In terms of what is technologi - cally similar between critics and tu - tors , it may be argued that critics acquired what they needed from the older field of tutors . On the other hand it could be said that both fields arrived at these similari - ties because they are the natural way to solve such problems , or per - haps some combination of the two occurred . Precisely how the paral - lels arose is less important than the fact that such parallels make it rela - tively straightforward , at least con - ceptually , to do two things . ( 1 ) Merge the two types of pro - grams into high function environ - ments . An example of an attempt to create an integrated environment for learning and problem solving . And , ( 2 ) it is desirable to advance the sophistication and usefulness of critics by adopting more of the user - oriented approaches that seem to help tutors . The first point is relevant to this section on new applications , while the second point is explored at some length in the section on tutor - ing system . Illustrative Example of the Use of Critics Some of the points made thus far can be clarified with an example of the kinds of dialogs currently possi - ble between critics and their human users . In this example , a knowledge base will be elicited from a hypo - thetical human by an illustrative knowledge acquisition system . The example uses a domain most read - ers should find fairly simple , that of purchasing a truck or vehicle . To keep the example short , only a few rules will be elicited . Also , the type of vehicles to be included in the KB shall be restricted to personnel car - riers for the U . S . Army . The first few questions of the knowledge acquisition system have already been asked at the point this example begins and the result is that the user has specified two logi - cally inconsistent rules . These are shown in tree format in Figure 4 . a which may be interpreted as : IF : The Vehicle Is Not Relia - ble , AND The Vehicle Is Not Main - tainable , THEN : The Vehicle Is Operation - ally UnSuitable . IF : The Vehicle Is Reliable , THEN : The Vehicle Is Operation - ally UnSuitable . The knowledge acquisition system has also acquired from the user two problem traits about passenger and speed thresholds , which as of yet ~ April 1992 / Vu1 . 35 , No . 4 / COMMUNICATIONS OFTHE ACM are unattached to any parent classes . At this point there are a number of items to criticize and the critiqu - ing process model begins by run - ning its coherence critic which parses the knowledge base into a logic graph , and as shown in Figure 4 . b initiates both a completeness and consistency critic . Recall there is a template that follows the struc - tural model format and which re - quires the KB to have all of its rules be children ( or linked to grandchil - dren or subsequent ancestors ) of a goal node . The completeness critic is " conscious " only of the template which requires a root goal and that all traits be given parents ( it also knows all parents require one or more children ) . The two parentless traits pose potential dead ends in the logic graph and must be con - nected to one or more parents be - fore the KB can be considered fin - ished or before it can be the completeness critic . Similarly , the consistency critic catches the fla - grant contradiction between the two existing rules , another logic violation that cannot be allowed to persist . Also , at the level it is ap - plied , coherence critiquing makes no distinction between kind of knowledge and works equally well on descriptive , normative , prescrip - tive , pragmatic , or heuristic knowl - edge as long as it can be parsed into graph form . KB clarity should also be criti - cized at this point . Clarity criticisms were defined earlier to include items such as ambiguity , vagueness , or disorganization . In the single example shown in Figure 4 . c a Readability Critic which has been preloaded with insight about nu - merical thresholds in the problem traits attempts to help the user find and eliminate ambiguity and vagueness in thresholds with im - properly clarified booleans ( e . g . , < , > , = ) . This Readability Critic works with simple string match and detection algorithms that notice whether any of the com - mon booleans in its dictionary exist in the phrase or not , and if so it then proceeds to check the ambigu - ity and / or vagueness of the boole - an ' s arguments . Interestingly , had the template originally defined booleans as a part of the problem trait that had to be elicited , the fail - ure to properly specify the booleans would have been a criticism of the completeness critic instead of the readability critic . This is not to say that all clarity criticisms can be sub - sumed by other forms of critiquing but , as mentioned earlier , there can be some arbitrariness in the defini - tions . The clarity critiquing version is , however , a more general solu - tion since it applies to booleans of thresholds wherever they might appear while completeness de - pends on the template ' s specificity . Once again , although this example deals exclusively with prescriptive knowledge , clarity criticisms are possible across all kinds of knowl - edge and generally should be con - strained only by the imagination and budget of the knowledge ac - quisition system building team . Once alerted to the coherence and clarity criticisms the user inter - acts with the knowledge acquisition system to specify the modifications he or she wants . Figure 4 . d illus - trates the corrections the user of - fered in this hypothetical case . Here the root goal is now present , the contradiction is removed , a par - ent is added that creates a new rule encompassing the previously unat - tached problem traits , and the bool - eans of those traits are clarified . This version of the KB has no co - herence problems . It does still have some clarity problems but the read - ability critic of the prior paragraph is unprepared for such criticism tasks . For example , " The Vehicle Is Not Reliable " is somewhat vague , or at the very least it leaves the judg - ment call of what level of perfor - mance is " Not Reliable " in the hands of the KB ( not knowledge acquisition system ) user . This is a difficult problem for the readability critic of the prior paragraph which works on prescriptive rather than normative knowledge : the phrase in question is normative , however . COMPUTING PRACTICES Next , the workability critic of the knowledge acquisition system asks the human to input a real case to be solved ( see Figure 4 . e ) . In this ex - ample , the KB developed thus far is attached to an expert system shell that must attempt to solve the real case provided by the human . The real case performance or workabil - ity critic then attempts to criticize the expert system ' s effort as shown in Figure 4 . e by : ( 1 ) noting its failure to reach a con - clusion , ( 2 ) commenting on the rules that were used to attempt to satisfy root goal conditions , ( 3 ) listing root goal conditions that were never satisfied and attempting to explain why not , ( 4 ) delineating facts that the user provided and which could not be used along with a rationale , and ( 5 ) returning the user to the knowledge acquisition system to help him or her further edit the KB should that be desired . Figure 4 . f portrays the KB after the user has accepted the offer to work with the knowledge acquisi - tion system to perform further ed - iting . In the example shown the principal change is that the user has decided the old IF clause ( " The Ve - hicle Is Not Reliable " ) and the real case expression ( " but breaks after one hour " ) were both impractical . Instead the user has substituted the more pragmatic string , " The Mean Time Between Failure < 40 hours , " probably with the feeling that a more universally accepted measure ( MTBF ) and threshold would be workable for future KB users . Sev - eral points should be made here . Namely , this type of dynamics cap - tures the benefits of real case work - ability testing which both raises the problem requiring attention and leaves the knowledge acquisition system user free to decide how to creatively and pragmatically alter the impasse or bias in the KB . It also illustrates the weakness of real case testing which is its dependence on fixing problems only when the case surfaces them . Thus , for ex - COMMUNICATIONSOFTHE ACM / Apri 1992 / Vo1 , 35 , No . 4 ! ! ~ Figure 4 . How a knowledge acquisition system utilizes critics and critiques knowledge a ) The User Specifies Part of a Knowledoe Base When Promoted By the Knowledee AcoulsltlonSystem : Tree Reoresentation of the Resultino KB Then Clauses ( Seecifled in response tg auedg ~ about hvDotheses the traits SUbOOrt ~ KB If Clauses ( Soecifled In resoonse to ouedmi about Qroblem | raits ~ The Vehicle Won ' t Hold 10 Passengers The Vehicle Can ' t Do 30 mph The Vehicle is Not Reliable The Vehicle is Operationally UnSuitable , - = = = : : : : : : ~ L and The Vehicle Is Not Maintainable The Vehicle Is Operationally UnSuitable The Vehicle is Reliable b ) Coherence Critics ( Loalc Verifiers ~ Comoare the KB to a ( Task Structure ~ Model of What It Should Contain Comeletenes ~ Critic : Two IF Clauses Support No Goal ( Hypothesis ) . Which Hypothesis do they support ? There is No Root Goal . Are you ready to add one ? Consistency Critic : The last two rules have a logical contradiction . Which of these would you like to change ? ¢ ) The KAS Runs a Clarity Checker To Determine If Rule Phrases Are Well Formed Readablfitv Critic : Do you mean exactly 10 , no less than 10 , no more than 10 passengers , or some other qualification ? Do you mean at least 30 mph , no more than 30 . . . ? d ) Revised KB After Steos " b ~ " ~ rtd " c ! " Then Clauses ISoecified In resoonse to auedes about hvootheses the traits ~ uneort ) i Root Goal : Is the Vehicle InEffective AND Operationally UnSuitable ? The Vehicle Is InEffective The Vehicle is Operationally UnSuitable If Clauses ( Soecitgd in resoonse to auedes about Droblem traits ~ ~ The Vehicle Holds < 10 Passangers The Vehicle Moves < 30 mph The Vehicle Is Not Reliable ~ T h e Vehicle Is Not Maintainable KB e ) Praomatics Critic Reacts to Real Case Performance Results Please specify a real case you would like me to solve : > A vehicle holds 11 passengers and moves 35 mph but breaks after 1 hour . No conclusion could be reached . First R1 was used which says : IF : The Vehicle Holds < 10 passengers , AND The Vehicle Moves < 30 mph THEN : The Vehicle is InEffective . Clearly , this vehicle is not " InEffective " . However , the goal also requires consideration of whether the vehicle is " Operationally UnSuitable " , an hypothesis for which there Is no information . Also , the string " but breaks after 1 hour . " does not match any existing KB phrases . Would you like to edit the KB ? 1 . Yes 2 . No > 120 April 1992 / Vol . 35 , No . 4 / COMMUNICATIONS OF THE ACM COMPUTING PRACTICES Figure 4 . cont . f ) The KB After the User and KAS Reoalr the Flaw Discovered Durlna Real Case Performance Testino Then Clauses l ~ : 3oecified in resgopse If Clauses ISoecofoed in response to oueries about hYPotheses th @ traits su012ort ! to auedes about oroblem traits1 Root Goal : IIs the Vehicle InEffective AND Operationally UnSuitable ? The Vehicle Is InEffective The Vehicle Is Operationally UnSuitable KB The Vehicle Holds < 10 Passangers The Vehicle Moves < 30 mph The Mean Time Between Failure < 40 hours The Vehicle Is Not Maintainable g ) Three Illustrative Corresoondence Critics ( Realitv Validatorsl Check the KB Aoainst Domain Descriotive . Normative . and Prescriotive Knowledae World Model Checker : It seems you should reject a vehicle if It Is either Ineffective OR unsuitable . Shoudn ' t you replace the AND In the Root Goal with an OR ? Likewise , wouldn ' t you prefer an OR as the Boolean for the Operationally Unsuitable rule ? Global Critiquing : Organizational Policy Memo 345 states all thresholds should be achievable in 90 % of trials with a significance level of . 05 . None of your thresholds Indicate such constraints . Would you like to add them ? If not , a warning message will be placed in your KB . Local Critiquing : Your domain is armored personnel carriers . Did you know that upon perpendicular Impact by a shoulder fired missile , the current generation of vehicles has been found in Armor Lab tests to spray its passengers with hot molten shards generated by the Inner layer of the armor ? It is strongly recommended that Inner armor protection be procured to guarantee Operational Suitability . If you don ' t add this to your KB , please Indicate why not . h ) Final Version of Ithe KB After Prescriotive Knowledae Has Been Incorporated Then Clauses ( Soecified in resoonse to ouerles about hvoothe ~ es the | mits suogort ~ I ~ tauses ( S ~ 2ecified In resoonse to ouedes about problem traits1 Root Goal : Is the Vehicle InEffective OR Operationally UnSuitable ? The Vehicle Is InEffective The Vehicle Holds < 10 Passangers In 90 % of trials , alpha = . 05 The Vehicle Moves < 30 mph In 90 % of trials , alpha = . 05 KB The Veh icle is Operationally UnSuitable The Mean Time Between Failure < 40 hours In 90 % of trials , alpha = . 05 The Vehicle Is Not Maintainable The Vehicle Has No Inner Armor Protection ample , the phrase " The Vehicle Is Not Maintainable " was not refined into a more pragmatic form since the knowledge acquisition system was not focusing on it and the user did not notice it . At this juncture the KB is consid - ered coherent ( the coherence critic and its subcritics are not triggered into criticism by the changes just added ) , clear in - so - far as the read - ability critic is concerned , and workable with respect to the cases run to date . The correspondence of the KB , however , with respect to reality is still inadequate and corre - spondence critics are triggered as shown in Figure 4 . g . Correspon - dence criticism as shown here helps the user tune the KB to aspects of his or her domain that may not be familiar either because of orienta - tion ( e . g . , specialization , skill level , background ) , because the knowl - edge is new , or because of specific cognitive biases on the part of the user ( e . g . , memory availability bi - ases , weak heuristics replacing proper probability computations , capacity overload ) . COMMUNICATIONSOFTHE ACM / Apfi 1992 / Vo . 35 , No . 4 121 In particular , three illustrative levels of correspondence criticism are offered . The first critic notices that the KB logic does not corre - spond with an expertise module . This internal module holds expert knowledge that indicates the types of tradeoffs people normally do and do not consider acceptable . In this example , if the vehicle were EITHER ineffective OR operation - ally unsuitable most buyers would reject it . Thus the user has made a logical error none of the earlier critics were able to discern . Simi - larly , the boolean connector be - tween " mean time between failure " and " maintainable " is more likely to be OR than AND . Detecting these two types of errors requires that the critic have some sort of world model which is able to discern the types of tradeoffs that are normal in the real world . The latter two correspondence critics are more straightforward . Here , the user is presumed to be uninformed about certain latest information , quite possibly the user is suffering from an availability bias in that he or she knew about the information contained in these two critics but tends to overlook it in the light of all the other , more concrete information in his or her experi - ence base . Global correspondence criticism affects large portions of the KB , in this case all phrases with thresholds must be modified by the user . Local correspondence criti - cism tends to pinpoint content weaknesses in the user ' s thoughts such as in the case of the Opera - tionally UnSuitable rule . Both of these criticisms were triggered by string detection and matching algo - rithms in much the same fashion as in the case of the readability critic , however , the correspondence criti - cism is usually accomplished by the knowledge acquisition system rea - soning from a model containing deep knowledge of the principles of the domain as was done by the first of the three correspondence critics . The impact of the correspon - dence critics surfaces when the user further alters the KB to take that criticism into account ( see Figure 4 . h ) . Readers may be tempted to argue one could achieve the global KB refinements with other types of critics . For example , the global change could be accommodated ei - ther by a clarity or a completeness critic . The goal here , however , was not necessarily greater clarity , al - though that is a side - effect , but to assure already disseminated corpo - rate policy is actually integrated into the field operatives ' behavior patterns . In the same vein , the glo - bal correspondence critic is a con - venient way to disseminate policy information on a timely basis with - out having to wait to update the KB template so the completeness critic can check for those types of errors . As a second example , the local change pertaining to armor vulner - ability could conceivably be accom - plished during a real case workabil - ity test , provided that case was thought about . The utility of the local correspondence critic , unlike that of the workability critic , is that it is dependent on a model rather than the vagaries of which case is run . Real world correspondence can be more completely guaranteed and in a shorter amount of KB de - velopment time relative to work - ability testing . There are numerous dimensions to correspondence critiquing and the ones shown here cover only a few of the numerous possibilities . One example showed a hypotheti - cal proclivity on the part of experts toward habit and selective percep - tion . In the other examples , and no conclusion is offered on this point , it is possible that the knowledge acquisition system user had heard both the company policy and armor vulnerability findings before , but had omitted them due to being mentally preoccuppied with other , more concrete aspects of KB devel - opment . In this case the user is said to be judgmentally biased and the correspondence critics , and its subcritics , are attempting to influ - ence and debias the user ' s judg - ment . An important area for further formalization of types of correspondence criticism lies in rec - ognizing and helping to correct common human judgment biases in the presence of uncertainty ( e . g . , humans replace proper analytical approaches with inappropriate and weak heuristics with no loss of con - fidence in the results ) : this is the role for human judgment theory as discussed elsewhere in this survey . In closing the discussion of this example it should be noted that the KB can be both refined still further ( nothing has yet been done about the maintainability phrase ) and ex - tended in depth and breadth . Cer - tainly there are many more rules one can author about ineffective - ness and unsuitability ( e . g . , under what conditio , ns will a speed of less than 30 mph still be acceptable ? ) . Also , the second half of the KB has yet to be started - - rules suggesting easy fixes that should be attempted before eliminating a given vehicle from consideration such as , among others : IF the vehicle has no inner armor , THEN determine whether it is desirable to install inner armor . As new rules are added by the knowledge a ~ cquisition system and the user laboring jointly to flesh out the structure of the KB , criticism - based knowledge acquisition will continue . That is , the critics de - scribed above plus similar ones with other specific concerns to convey will continue to be triggered as the user injects bias into the rules he or she inserts into the KB . Acquiring a KB is not just a matter of filling out structure , it is also a two - way , inter - active , communicating , view - shar - ing , criticism process . Conclusions and Future Work This artic ~ has presented the reader with a compendium of critic - based approaches in numer - ous phases of human endeavor ( e . g . , problem solving , knowledge acquisition , and learning ) , and in a multitude , of applications . These approaches have ranged from the practical , simple , and immediately usable to the highly theoretical and frontier - defining ones , that offer greater degrees of problem solving 122 April 1992 / Vol . 35 , No . 4 / COMMUNICATIONS OF THiS ACM iiliiiiiiiill ~ iillllilliilllillllailliIilillaililllllillliillilillllliillill ability and of credibility checking . The survey also serves as a way of organizing the type : s of critics one might deploy , and a : s such provides a point of departure for critic engi - neering . This article also surveyed the evolution of the critiquing systems approach and the benefits numer - ous users are drawing from fielded applications in document creation , medical , engineering , knowledge base acquisition , software engineer - ing , and training domains . Critics are an important class of program that will be used in an ever - rapidly widening circle of applications in the coming years . At the same time , researchers will be improving the usability of critics b ) y tapping tech - niques now in use in the intelligent tutoring field . To see what some of these improvements might lead to , what follows is a short discussion of promising ideas for future critics . Intelligent Tutoring Systems as a Source of New Technology for Critics The goal here is to raise the pros - pect that some of the intelligent tu - toring apparatus night be worth further investigation to help the critic " say the right thing at the right time " ior , to offer more of a " user ' s perspective ' " of things . Use - ful surveys of the relevant aspects of tutoring system architecture and technology may be found in 21 , 31 , among others . As Figure 5 shows , critics currently offer criti - cism with very little accounting for how the individual user will receive it . The remainder ' of this section will clarify the ne , ed and role for user - oriented approaches in critics . Even so , adding new technology to critics should be done with caution . While intelligent tutoring systems are rich in theory , they are not as successful as critics in terms of widespread usage . One would not wish to jeopardize what already works in the critiquing process by adding overly complex techniques that do not work . well in real set - tings . The results ; reported in this article should be viewed as the ini - tial exploration into why , how , and when to adopt next generation critic technology so as to avoid los - ing any of the benefits of the cur - rent generation . In this respect , three ideas from the intelligent tutoring systems field seem worth exploring for their utility to critic programs ( see Figure 5 ) . ( 1 ) " overlay models " that might help identify what critiquing strat - egy will work based on the errors a given user has committed thus far ; ( 2 ) " user models , " also called " stu - dent models , " that can adapt the dialog and advice to the needs of the individual user ; and ( 3 ) elements of the " theory of bugs " that can possibly be appro - priated to improve the handling of cognitive bias . All three of these techniques are used in tutoring to put the program more into the user ' s perspective , and thereby improve the program ' s knowledge communication process 30 . It is hoped they might accomplish a similar purpose for critics . We will now explore each of these three ideas . Next Generation Differential Analyzers : The Overlay Model . The differential analyzer , universal to critic pro - grams , was first conceived under the name of " differential model - ing " in a coaching ( tutoring ) pro - gram embedded in a small gaming environment 3 . The coach pro - vides context - sensitive advice when the students get stuck at a given skill level . Its success as a coach and its inspiration to the early critic re - searchers came from its differential model , which compared the stu - COMPUTING PRACTICES dent ' s solutions with an internal expert module for each move of the game . It became obvious to critic researchers that any development in the differential modeling ap - proach would be potentially inter - esting to the field of critics , as well . In intelligent tutoring , differen - tial modeling ' s principal drawback was not so much the idea of analyz - ing the differences between the stu - dent and the expert module , but that it relied upon an expert mod - ule containing prescriptions from the expert ' s point of view . Since students may not understand that viewpoint they may fail to benefit from advice generated to explain what the expert would do . The al - ternative is a learner - oriented ap - proach that gives feedback relevant to the current skill level of the user , that tracks and understands the student ' s frontier of knowledge as it evolves , and that only gives the ex - pert ' s point of view after the stu - dent has mastered the intermediate skill levels . Such a learner - oriented ap - proach has now become the stan - dard in the intelligent tutoring field . It is based on the overlay the - ory of 4 , which presumes that a student will make different moves at different skill levels . Each skill level can be imagined as a frontier across the net of skills to be mas - tered . Models of these subnets are overlaid atop the expert module ' s layer of expert knowledge . The stu - dent or user model plots the stu - dent ' s current skill level and con - veys this to the differential analyzer Figure 5 . A research objective to improve the user - orientation of the critic I User I I I Oriented I I Expert I I Criticism I I Oriented I I Criticism I : : ~ . Overlay Theory ~ : . . ' ~ User Modeling ~ ~ Theories of Bugs ~ Jritiquing Process COMMUNICATIONS OF THE ACM / April 1992 / Vol . 35 , No . 4 ! 2 3 • n • • l a • a l • • • • a o . • a l a a a a i l a a a a a A a a l O • O l o • a a a a a l O a • a a i m a a a l a a a a a J a l a a a l a a o • that determines which of the skill - overlays to utilize . There are numerous concerns and challenges associated with adopting such an approach for crit - ics . For example , more must be known about the users , the overlays must be fleshed out , a user model must be added , and some of the overlay theory is unfinished . The advantages , however , could be po - tentially substantial . Criticism would be more oriented toward the specific needs , current areas of skill , and specific cognitive biases of the user . The dialog generator could better tailor its text to say the right thing at the right time . A rig - orous basis would exist for deciding which critiquing strategies to in - voke - - ( e . g . , influencer vs . debiaser , default vs . analog reasoning , or dis - playing the proper level of depth of knowledge ) . These are just a few of the advantages of using overlay theory . The Role of the User Model . In the survey of critics , the user model was included in the architecture of critic programs ( Figure 1 ) , but no critics were cited in the section on the five categories that actually used it prior to 1990 . That is , critic programs do not tailor their dialogs to the needs of an individual user , to the degree of sensitivity accomplished by a user model . If two users committed the same error , a critic program would give the same feedback to both of them . The same is not true of intelligent tutor programs , as was just explained in the preceding section . There is a need for critics to adopt user - sensitive dialogs . In the presence of identical criticism , seemingly minor skill and experi - ence differences between qualified subject matter experts can lead to major task performance differ - ences . The intelligent tutoring field is acutely aware of this problem and has adopted the user model , called a student model , as a response ( e . g . , see 11 ) and others . This does not mean that user modeling is either a panacea or a simple fix . It is not . The differences in the open vs . closed nature of the domains tack - led by critics and tutors make the transfer of the user model technol - ogy a nontrivial research challenge that should not be entered into lightly . Silverman expands upon these concerns further in 26 . At the present time it will only be men - tioned that user modeling is a wor - thy , long - term research objective of the critiquing paradigm . As one example , when using a grammar critic ( see 20 ) to help write a book , the author misunder - stood what leads to passive voice errors . The critic shows that the error is due to a form of the word " to be " followed by a past participle . The author initially obtained the added belief that any sentence that refers to the subject passively is also a passive voice error . For example , the author thought " as shown in Figure 5 " is just as passive as " the item is shown in Figure 5 . " The au - thor used the grammar critic and successfully corrected many passive voice sentences that it identified . After about four hours , a sentence arose that satisfied the author ' s be - lief about errors in the passive voice , yet , it did not include a form of the verb " to be . " When the critic did not flag this error in the sen - tence , the author ' s confidence in the understanding of the passive voice problem fell apart . The gram - mar critic could not help the author and the author could not resolve the dilemma unaided . Many technical writers , like the author , learn to use the passive voice since it appears to distance the writer from the subject . This makes the writer appear more detached and scientific . Correcting technical writers of this class of deeply in - grained errors probably requires use of an overlay theory that offers multiple strategies of criticism . Yet , the critic ' s correction used a single explaining strategy only . In the ab - sence of an overlay , the critic never recovered the situation when the author became confused . It never even knew about the author ' s con - fusion . A user model can help re - duce such difficulties . The Theory of Bugs in Tutors and Crit - ics . The term , bug arises from the computer programming jargon to refer to errors that programmers introduce into their programs . In the field of intelligent tutoring the term is widely used to refer to er - rors internalized by the student that are explicitly represented in the student model . Bugs are usually procedural or localized errors rather than deep , pervasive mis - conceptions . Also , they are errors the student would repeat , rather than slips or accidental errors . The study of the bugs commonly en - countered by a student population in a domain is called a theory of bugs . An understanding of the origin or theory of bugs amongst the stu - dents ( users ) in a given domain has critical implications for corrective action as well as for the design of preventive or influencer strategies . In the tutoring field , the bugs are usually used as triggers for pre - stored preventive and corrective actions . According to Wenger 30 theories about bugs can be grouped into three categories : enumerative , reconstructive , and generative . Enumerative theo ~ ries of bugs involve the generation of exhaustive cata - logs or libraries , ; of all the bugs - - internalized procedural errors - - commonly encountered in that domain . Reconsitructive theories of bugs attempt to , escape the limita - tions of a fixed catalog of bugs by adopting a model which can recon - struct a relevant : portion of the list of student - internalized bugs based on observing the external manifes - tations of student errors . Both enumerative and reconstructive theories are basically concerned with the detection of errors as they are exhibited in the student ' s be - havior and with corrections that effect the surface or local cause of the bug . Generative theories are like reconstructive ones , except they also attempt to account for psycho - logically plausible mechanisms that explain why these ' , bugs were gener - ated in the first place . This opens 124 April 1992 / Vo1 . 35 , No . 4 / I ~ OM & IUNIr . . ATION $ OF THE ACM new possibilities for remediation ; however , unlike the other two theo - ries , generative inferences are not computed on - line - - they are de - rived off - line and incorporated into the instructional system before it runs . Various combinations of these theories of bugs have been adopted , with a fair degree of success in helping the program perceive the student ' s knowledge exactly . Im - plementations of enumerative the - ory are desirable because of their exhaustive coverage , but they suf - fer from a fixed size , and in some domains that is not practical . Re - constructive approaches by them - selves have not yet proven their practicality in terms of coverage and generality , but when used in tandem with enumerative ap - proaches , they can escape some of the limitations of the fixed catalog . Finally , generative theories open new possibilities for supporting in - depth remediation , preventing the occurrence of bugs , and diagnosing generative patterns using pre - stored recognition schemes 30 . Generative theories are the newest group and the least exploited thus far , but new contributions to any of the theories , or to their implemen - tation , alone or in combination , are still needed . These theories of bugs have a clear counterpart in critiquing sys - tems , and further study of them can contribute to the advancement of the critiquing process . These contributions will not only make critic programs more effective , but they will also model humans , in terms of how we criticize one an - other , and how our internal cogni - tive processes of self - criticism work . Looking for counterparts in critics for these theories of bugs , one finds most of the critics built to date are enumerative and contain large , fixed catalogs of bugs that serve as triggers that lead the differential analyzer to fire criticisms . Miller ' s unconstrained domain critics that use risk analysis could possibly be thought of as hybrid reconstruc - tive - enumerative . Finally , the framework of expert intuition and cognitive bias pursued Silverman ' s COPE research clearly combines an enumerative with a generative theory . Readily Implemented Lessons Learned Since the history of critics to date is highly pragmatic and use - oriented , it is fitting to close this survey with a discussion of what lessons are implementable . No one critiquing program will ever implement all the features discussed in this article . Critic developers must choose those features that seem most reasonable for their needs . What follows is a recipe for configuring the critic features relevant to a specific situa - tion . The Proper Types of Tests To Include in a Critic . Four types of critic tests exist . These are the clarity , coher - ence , correspondence , and work - ability tests defined in an earlier section . As the survey illustrates , no critic yet built incorporates all four types of tests . To implement one or two test types thoroughly is a major undertaking . The reader , like critic engineers of the past , must decide the types of tests that are most im - portant for his or her application . For example , the grammar checkers will not notice any error with the correspondence of " the green cheese is made of moon . " That is because grammar critics use clarity and coherence tests . A prop - erly tuned correspondence critic ( e . g . , TIME reapplied to a new domain ) could catch the green cheese error . Yet , TIME would miss the erroneous use of the pas - sive voice in " is made . " Using Figure 2 and the defini - tions of the four types of tests , the reader should decide what is ap - propriate for his or her application . The technology to build each type of critic exists . Still , unless the do - main is tiny , no one is likely to be given a budget or deadline great enough to fill out all the expertise and canned text modules for such an endeavor . One usually needs to COMPUTING PRACTICES establish priorities . How Critics Should Handle Domain Knowledge . Expert critics are a knowledge - rich approach . The expertise and canned text modules hold rules , heuristics , and other knowledge chunks . One develops these knowledge bases in many of the same ways as an expert system ' s knowledge base . Here again , there are few shortcuts . That is , one must interview domain super experts for rules about the normative cues and about the specific errors experts and nonexperts frequently commit . Knowledge engineering is ex - pensive and time consuming whether it is for an expert critic or for other types of knowledge - based systems . To save money and effort , one should attempt to adapt a knowledge acquisition tool to the process of interviewing for error knowledge . No one yet attempted such an effort . In part , this is be - cause critic engineering is different from traditional knowledge engi - neering . One difference is that critics often find use in broad , semistruc - tured domains . In these domains there are many directions that cri - tiquing can pursue . Incorporating them all is a long - term objective . In the short term , one must select the most cost - beneficial directions and focus narrowly upon those . After achieving success in one area , one can deploy further critics . For example , today ' s grammar critics try to appeal to all word pro - cessing system users . In the future , one can imagine grammar critics tuned to specific kinds of writing only . Thus there could be a com - mercial critic for legal and govern - ment documents , another for novel authoring , and a third for advertis - ing . These would extend the sim - plistic style replicator approach to include other types of tests . The discussion in this survey presented some details appropriate to such an approach . As a second example , in engi - neering design there are many dis - ciplines and perspectives that need COMMUNICATIONS OF THE ACM / April 1992 / Vol . 35 , No . 4 ! 2 5 critics . In the naval ship design domain 32 , the electromagnetic interference problem is only one of many that critics could help with . Yet , electromagnetic interference is probably the most important . Lives can he lost in battle if it is neglected . Once the electromagnetic interfer - ence critics are complete , one can start work on critics for human fac - tors of the design , and so on . In the long term , it is likely that whole di - visions of various design firms will exist only to build and run a broad spectrum of critics for ship design . The same future is likely for VLSI design firms , mechanical parts de - sign firms , and so on . What Level of User Model to Add to the Critic . The user modeling topic in - volves a delicate balancing act . Crit - ics with no user adaptivity run sig - nificant risk of saying the wrong thing to their users . If a critic says the wrong thing too often , the users become frustrated . They begin to ignore even the correct advice of the critic . At the other end of the scale , user modeling can turn all too easily into a budget breaking activ - ity . Once started , the user model will never end . No simple user model is ever entirely satisfying . Critic engineers always wish to re - lease new and improved versions of the user model . As one example , the grammar critic the author used to help write a book has a simplistic user model ( see 20 ) . It has " learn word " and " ignore class " buttons that let the user adapt it . This is a good work around for getting the critic to stop bothering the user for some classes of errors . Thus , the grammar critic can be set to stop bothering the user who does not confuse the words " principle " and " principal . " Yet , for other types of nonerrors there is no way to turn the critic off . Thus , for every heading and subheading of this article ( and for three sets of parentheses after that ) the gram - mar critic flags the unbalanced pa - rentheses it feels are in error . Another difficulty is that the grammar critic appears unable to interrupt its goal stack for a given sentence . Thus it flags all errors with a sentence and shows them to the user one at a time . Yet , on in - specting the first error of the sen - tence many users will notice and fix the other errors of the same sen - tence . The critic does not recognize the changes and the user must wade through the original stack of sentence errors telling the critic to ignore them . The critic lacks a way to detect when the user has had the ' aha ' experience and seen the error of his or her previous ways . The user is trapped in the middle of a corrective process . He or she must work through it if he or she wants to receive subsequent criticism that will be of value . The simple fix for this type of problem is to add a button to the user screen that says " flush stack . " This will reduce some frustration and avoid the felling of having a dumb machine for a critic . Alterna - tively , it runs the risk of users who have not truly found all the errors on the stack prematurely flushing it . To compensate for that problem , one could devise yet another fix to the user model . One could add an inferencing device that somehow infers what portion of the stack is still relevant . This will slow the critic down and more research will be needed to solve that problem . There is more to say but the point should be clear . The user model is never complete . The critic engineer must decide at what point to curtail the effort . In closing , a specific grammar critic ' s difficulties was mentioned in several places in this survey 20 . The point is not to single that piece of software out for negative comment . To the contrary , that software was cited to illustrate further advancements that technol - ogy might offer to an already useful critic . That grammar critic is a state - of - the - practice defining co - herence and clarity critic in the desktop publishing domain . The Best Architecture and Language for a Critic . The architecture of many critics is a major focus of this survey . Figure 1 and Table 1 illus - trate the major modules of a critic ' s architecture . These include differ - ential analyzer , expertise module , dialogue generator , text files , user model , and interface . Table 1 also shows how differently various crit - ics implement the capabilities and sub - components of these modules . One would like to have a critiquing language that offers all the alterna - tives . In this fashion , one could program any new critic application just by changing module configura - tions and by filling in the domain knowledge . At present only two critic lan - guages exist , E - ATTENDING and COPE . Readers might use one of these to get a start on building the architecture needed for their appli - cation . If the reader ' s application fits the features that one of these languages supports , then critic con - struction will be relatively easy . Rather than constructing a new critic shell , the engineer only will need to learn the one that already exists . One also may attempt exten - sions to the architectures of these languages . Alone or combined these two languages still do not offer the full gamut of architectural features cov - ered in this survey . To build all the critiquing features one may ever need into a single shell or language is a large undertaking . Few , if any , readers will ever attempt such an effort . Readers who construct their critic environments will need to choose features that are the most important ones for their particular needs . The discussion of this survey should help them make those deci - sions . References 1 . Atwood , M . E . , et al . Cooperative learning and cooperative problem solving : The case of grace . Knowl - edge - Based Human - Computer Commu - nication Workshop Proceedings , AAAI , Menlo Park , ( 1990 ) , pp . 6 - 10 . 2 . Boose , J . , Gaines , B . , ed . , Interna - tional Journal Man - Machine Studies , v . 26 / 27 ( Five special issues on knowledge acquisition , 1987 and 1988 ) . 126 April 1992 / Vo1 , 35 , NO . 4 / COMMUNICATIONS OF THE ACM jjlljllllijiliiiijiilliliilllllililllillillJJllAllllllllliiJlJllllJlillilJl 3 . Burton , R . R . and Brown , J . S . An investigation of computer coaching for informal learning activities . In Intelligent Tutoring Systems , Aca - demic Press , New York : 1982 , pp . 79 - 98 . 4 . Carr , B . , Goldstein , I . P . Overlays : A theory of modeling for computer aided instruction . AI Lab Memo 406 , Mass . Institute of Technology , Cambridge , 1977 . 5 . Fischer , G . , A Critic for Lisp . In Proceedings of 10th International Joint Conf . on Artificial Intelligence , Los Altos , Morgan Kaufman , ( 1987 ) , pp . 177 - 84 . 6 . Fischer , G . , Lemke , A . C . , Mastaglio , T . Critics : An emerging approach to knowledge - based human com - puter interaction . Conference on Human Factors in Computing Systems ( CHI ' 90 ) , preprint . 7 . Fischer , G . , McCall , R . , Morch , A . Janus : Integrating hypertext with a knowledge - based design environ - ment " . In Hypertext " 89 Proceedings , ACM , New York : 1989 , ( reprint ) . 8 . Frankel , S . Hello , mr . chips : PCs learn english . Washington Post , Wash . D . C . , April 29 , 1990 , p , D3 . 9 . Hammer , J . M . , Geddes , N . D . De - sign of an intelligent monitor for human error in a complex system . In Proceedings of the Computers in Aerospace Conference , AIAA , Wake - field Mass . , 1987 , ( preprint ) . 10 , Jakiela , M . J . Intelligent suggestive CAD systems . Ph . D . Diss . Univ . Mi - crofilms , U . of Mich . , Ann Arbor , ( 1988 ) . 11 . Kass , R . Student modeling in intelli - gent tutoring systems - - implications for user modeling . In UserModels in Dialog Systems , A . Kobsa , W . Washister , eds . , Berlin , Springer - Verlag , ( 1987 ) . 12 . Kelly , V . E . The CRITTER system : Automated critiquing of digital cir - cuit designs . In Proceedings of the 21st Design Automation Conference , ACM / IEEE , ( 1984 ) , pp . 419 - 425 . 13 . Langlotz , C . P . , Shortliffe , E . H . Adapting a consultation system to critique user plans . Int . J . Man - Machine Studies , ( 1983 ) , 479 - 96 . 14 . Lenat , D . , Guha , A . Building Large Knowledge Based Systems . Addison - Wesley , Reading , 1990 , 15 . Lewis , C . I . The Ground and Nature of the Right . Columbia University Press , New York , 1955 . 16 . Mastaglio , T . User modeling in computer - based critics . In Proceed - ings of 23rd Hawaii Conf . on Systems Sciences , IEEE Computer Soc . , ( Jan . 1990 ) . 17 . Miller , P . Expert Critiquing Systems : Practice - Based Medical Consultation By Computer . Springer - Verlag , New York , 1986 . 18 . Miller , P . L . ATTENDING : Critiqu - ing a physician ' s management plan . IEEE Trans . On PAMI , PAMI - 5 , ( Sept . 1983 ) , pp . 449 - 61 . 19 . Popper , K . The Logic of Scientific Dis - covery . Harper and Row , New York , 1959 . 20 . Reference Software International , Grammatik Mac : The easiest way to improve your writing style - - Version 2 . 0 User ' s Guide . RSI , San Francisco : 1990 . 21 . Richardson , J . J . Directions for Re - search and Applications . In Funda - mentals of lntell . Tutoring Systems , J . J . Richardson , M . C . Poison Ed . , L . Erlbaum , Hillsdale , 1988 . 22 . Silverman , B . G . Influencing human bias with knowledge acquisition sys - tems . AI Magazine , 11 , 3 ( 1990a ) , 60 - 79 . 23 . Silverman , B . G . Criticism based knowledge acquisition in document generation . Innovative Applications of Artificial Intelligence , AAAI Press , Menlo Park ( July 1991 ) . 24 . Silverman , B . G . Expert critics : Operationalizing the judgment / decisionmaking literature as a the - ory of bugs and repair strategies . Knowledge Acquisition , 3 , 2 ( June 1991 ) ( tent . ) . 25 . Silverman , B . G . Evaluating and re - fining expert critiquing systems . De - cision Sciences Journal , ( Jan . / Feb . , 1992 ) . 26 . Silverman , B . G . Critiquing Human Error : A Knowledge Based Human - Computer Collaboration Approach , Academic Press , London , 1992 . 27 . Silverman , B . G . , et al . COPE : A Case Oriented Processing Environ - ment , European Computing Confer - ence Proc . , Avignon , 1987 . 28 . Spickehnier , R . L . , Newton , A . R . Critic : A knowledge - based program for critiquing circuit designs . In Proceedings of the 1988 1EEE Interna - tional Conf . on Computer Design ; VLSI in Computers and Processors , IEEE Computer Soc . Press , ( 1988 ) , pp . 324 - 7 . COMPUTING PRACTICES 29 . Steele , R . Cell - based VLSI design advice using default reasoning . In Proceedings of 3rd Annual Rocky Mountain Conf . on A1 , RMSAI , Den - ver : ( 1988 ) , pp . 66 - 74 . 30 . Wenger , E . Artificial Intelligence and Tutoring Systems , Morgan Kaufman , Los Altos , 1987 . 31 . Woolf , B . , Solloway , E . , Clancey , W . , et al . Knowledge based environ - ments for learning and teaching . AAAI Spring Symposium Series , Workshop Notes , Stanford , Mar . 1990 . 32 . Zhou , H . , Simkol , J . , Silverman , B . C . Configuration assessment lo - gics for electromagnetic effects re - duction ( CLEER ) of equipment on naval ships . Naval Engineers Journ . 101 , 3 ( May 1989 ) , 127 - 37 . CR Categories and Subject Descrip - tors : A . 1 Introductory and Survey : D . 2 . 5 Software Engineering : Testing and Debugging - - debugging aids , error handling and recovery ; H . 4 . 2 Informa - tion Systems Applications : Types of Systems - - decision support ; 1 . 2 . 5 Artifi - cial Intelligence : Programming Lan - guages and Software - - expert systems tools and techniques ; 1 . 2 . 6 Artificial Intelli - gence : Learning - - knowledge acquisition , J . 0 General General Terms : Algorithms , Human Factors , Languages , Verification Additional Key Words and Phrases : Critics , expert critiquing systems BARRY G . SILVERMAN is Director of the Institute for Artificial Intelligence and Professor of Engineering Manage - ment at George Washington University in Washington D . C . His research inter - ests include expert critics , judgment bias , human - computer interaction , and decision support . Author ' s Present Address : Institute for Artificial Intelli - gence , George Washington University , Washington , D . C . 20052 . emaih barry @ gwusun . gwu . edu Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage , the ACM copyright notice and the title of the publication and its date appear , and notice is given that copying is by permission of the Association for Computing Machinery . To copy otherwise , or to republish , requires a fee and / or specific permission . © ACM 0002 - 0782 / 92 / 0400 - 106 $ 1 . 50 COMMUNICATIONS OF THE ACM / Apri11992 / Vo1 . 35 , No . 4 ! 27