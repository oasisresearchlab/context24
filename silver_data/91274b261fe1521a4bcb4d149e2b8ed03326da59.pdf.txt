Improving Energy Conserving Descent for Machine Learning : Theory and Practice G . Bruno De Luca ∗ 1 gbdeluca @ stanford . edu Alice Gatti ∗ agatti0 @ proton . me Eva Silverstein ∗ 1 evas @ stanford . edu 1 Stanford Institute for Theoretical Physics , Stanford University , Stanford , CA 94306 , USA Abstract We develop the theory of Energy Conserving Descent ( ECD ) and introduce ECDSep , a gradient - based optimization algorithm able to tackle convex and non - convex optimization problems . The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy - conserving dynamical system , enabling analytic control of the distribution of results – dominated at low loss – even for generic high - dimensional problems with no symmetries . Compared to previous realizations of this idea , we exploit the theoretical control to improve both the dynamics and chaos - inducing elements , enhancing performance while simplifying the hyper - parameter tuning of the optimization algorithm targeted to different classes of problems . We empirically compare with popular optimization methods such as SGD , Adam and AdamW on a wide range of machine learning problems , finding competitive or improved performance compared to the best among them on each task . We identify limitations in our analysis pointing to possibilities for additional improvements . 1 Introduction The novel Energy Conserving Descent ( ECD ) framework for optimization introduced in [ 6 ] was shown in small experiments to be competitive with standard algorithms such as stochastic gradient descent with momentum ( SGDM ) while offering a theoretically predictable distribution of optimization results . Our contribution is to leverage ∗ Equal contribution . 1 a r X i v : 2306 . 00352v1 [ c s . L G ] 1 J un 2023 and improve this theoretical understanding to systematically enhance the performance and demonstrate an overall advantage compared to standard methods ( Adam [ 18 ] , AdamW [ 25 ] and SGDM [ 29 ] ) on a diverse suite of small / medium - scale machine learning benchmarks . This succeeds via a new parameter enabling stronger concentration of the results at small loss , combined with the use of a separable Hamiltonian as in previously developed sampling versions of the algorithm [ 27 , 28 ] , which allow a more effective integration . Overall in our experiments , ECD is competitive with the better of Adam and SGDM , without need for learning rate ( lr ) scheduling required by SGDM for competitive performance . Along the way we describe limitations and derive theoretical predictions for further improvement . 2 ECD Theory and formulaic improvements ECD is formulated , in analogy to classical mechanics in physics , as a discretization of chaotic energy - conserving Hamiltonian evolution on a 2 n - dimensional phase space of positions Θ ( e . g . an n - dimensional space of neural network weights and biases ) and momenta Π . The distribution of results in Θ – either along a given trajectory , or among multiple trajectories – is given by µ ( Θ ) = (cid:90) d n Π δ ( H ( Π , Θ ) − E ) = Ω n − 1 | Π | n − 1 | ∂ Π H | = Ω n − 1 | Π | n − 1 | d Θ / dt | , ( 1 ) where E is a constant , the conserved energy , and H ( Π , Θ ) is a time - independent Hamil - tonian function which depends on the objective function F ( Θ ) and on the magnitude | Π | of the momentum . The continuum evolution equations are d Π / dt = − ∂H / ∂ Θ , d Θ / dt = ∂H / ∂ Π . An example we will focus on is a Hamiltonian describing the kinetic energy of a particle with Θ - dependent mass ∼ 1 / V [ F ( Θ ) ] [ 6 , 27 , 28 ] H ( Π , Θ ) = V [ F ( Θ ) ] Π 2 = 1 V ˙ Θ 2 ( 2 ) V [ F ( Θ ) ] ∝ ( F ( Θ ) − F 0 ) η , η ⩾ 1 ( 3 ) along with energy - conserving momentum rotations to enhance chaotic behavior [ 6 ] ( see § 3 . 2 ) . The continuum - equivalent dynamics obtained from the logarithm of ( 2 ) separates the position and momentum dependencies allowing more robust numerical integration [ 27 , 28 ] . For ( 2 ) , the measure ( 1 ) gives µ ( Θ ) = E ( n − 2 ) / 2 π n / 2 Γ ( n / 2 ) V [ F ( Θ ) ] − n / 2 . ( 4 ) We would like to concentrate this measure in desirable regions of the objective . From ( 4 ) and ( 3 ) we see that increasing η above 1 ( the value taken in [ 6 ] ) accentuates the 2 concentration of measure as close as possible to F ≃ F 0 , suggesting improved performance with increasing η . However , we also must ensure that the enhanced measure – which corresponds to smaller velocity – does not come at the cost of excessive slowdown of the motion on the way to the objective . It is useful to analyze this in concert with the energy conservation equation which determines the speed in terms of the objective ( using ( 2 ) - ( 3 ) ) ˙ Θ 2 = EV = E ( F ( Θ ) − F 0 ) η . ( 5 ) To avoid exponential suppression of the speed at large η , this motivates considering also a regime of F 0 values for which | F − F 0 | ≳ 1 at large η . This depends on the minimal value of the objective , F min . This is a priori unknown , but in simple cases is well approximated by 0 ; more generally it can be found via an adaptive procedure [ 6 ] . Studying the quantity ( 4 ) in the minimal quadratic basin of the loss F ∼ F ( 2 ) ij Θ i Θ j + F min yields (cid:90) d n Θ µ ( Θ ) ∼ (cid:90) | Θ | n − 1 d | Θ | ( F ( 2 ) ij Θ i Θ j + F min − F 0 ) − ηn / 2 . ( 6 ) To get a sense of the dominant contributions to the measure , taking into account the geometrical | Θ | n − 1 factor , let us consider for simplicity an isotropic basin with F ij = F 2 δ ij . The integral in ( 6 ) has a saddle point ( a peak of the integrand ) at Θ ∗ 2 = ( F min − F 0 ) ( n − 1 ) F 2 ( 1 + n ( η − 1 ) ) . ( 7 ) This is where the measure is concentrated ( somewhat analogous to the ‘typical set’ in Hamiltonian sampling [ 3 , 27 ] ) , and it indeed approaches the bottom of the basin for large η . Consistently with this , the speed ( 5 ) remains finite and nonzero generically | ˙ Θ | Θ ∗ = √ E (cid:18) ( F min − F 0 ) nη 1 + n ( η − 1 ) (cid:19) η / 2 . ( 8 ) The two results ( 7 ) - ( 8 ) can be used in concert to improve optimization with reasonable speed , taking into account their explicit dependence on the parameters . Note that the speed increases exponentially with F min − F 0 as we increase η while Θ 2 ∗ grows only linearly . This strongly motivates including F 0 to generate speedier evolution without paying a significant price in proximity to Θ = 0 . In the special case with F min − F 0 strictly zero , the peak in measure is at Θ 2 ∗ = 0 for all η ; in that case , the η - dependence enters into how strongly peaked the measure is there ( 4 ) . In that case a clear limiting factor is that the speed near the minimum ( specifically the region Θ 2 F 2 < 1 ) dies exponentially with η . This last effect is avoided in the generic case with nonzero F min − F 0 . In this generic case with F min − F 0 , at large dimension n we observe a strong dependence on η − 1 in ( 7 ) - ( 8 ) . The special choice η = 1 leads to strong growth with 3 n of Θ ∗ and | ˙ Θ | Θ ∗ . Conversely , choosing η > 1 removes this behavior and increasing η pushes the peak Θ ∗ toward the optimum while maintaining speed | ˙ Θ | Θ ∗ . Thus we predict that η > 1 should improve optimization performance per se , and we will test and confirm this below . Of course , in ML problems the optimization performance ( training loss ) need not reflect the test accuracy , particularly in overparameterized problems , and we will analyze the effect of our hyperparameters ( HP ) s , including η , separately on training and test performance . In less over - parameterized ML settings , with F min > 0 , this analysis indicates that η > 1 helps directly with test accuracy as this reflects the optimization itself . We will verify aspects of these predictions in our experiments below and use it to improve ECD optimization in practice . A limitation will be that we will set F 0 = 0 , a simple way to keep the number of tested HPs comparable among the different optimizers . That is we will not experimentally exploit the role of F 0 in combination with η just derived , in concentrating the measure while maintaining speed ( 8 ) , but we will observe a window where η improves optimization performance in itself . The joint behavior of η , F 0 along with the learning rate and chaos elements presents a promising avenue for further improvements . Relatedly , we can improve the theoretical analysis by doing this section’s calculations directly in the discrete case , e . g . summing rather than integrating over momenta in ( 1 ) . 3 The ECDSep Optimization Algorithm Here we construct our algorithm from a simple symplectic 1st order integration of the evolution equations from the Hamiltonian ( 2 ) , adding chaos - inducing elements and addressing mini batches . 3 . 1 The update rules We can separate the Hamiltonian ( 2 ) by taking a logarithm [ 28 ] , obtaining H sep ( Π , Θ ) = log ( Π 2 ) + log ( V [ F ( Θ ) ] ) . ( 9 ) Here V is any function of the target F , and in this work we focus on a power law ( 3 ) . The dynamics induced in parameter space can explore any value of the objective F for any E , by virtue of Π ’s full range , with | Π | → 0 for F → ∞ . If it is known that the relevant part of the problem does not require exploring F → ∞ , the Hamiltonian can be regularized such that for a given E only an upper - limited range of F is explored . This is achieved by a simple change of the kinetic term H reg ( Π , Θ ) = log ( Π 2 + 1 ) + log ( V [ F ( Θ ) ] ) . ( 10 ) 4 Calling E ≡ e E , where E is the energy associated to H sep , the dynamics will now only explore regions in Θ with F ( Θ ) − F 0 ⩽ E 1 / η . The advantage is that the algorithm now has a smaller region to explore , but it will not be able to overcome arbitrarily high barriers in the target . This regularization does not change the dynamics in the accessed regions where V ≪ E , since Π 2 ≫ 1 . We will consider both algorithms , by adding a discrete switch s = { 0 , 1 } that for s = 1 selects the regularized Hamiltonian . The continuum evolution equations are ˙Π i = − η ∂ i F F − F 0 , ˙Θ i = 2Π i s + Π 2 , ( 11 ) where η is the hyperparameter that concentrates the measure towards F = F 0 as in ( 6 ) . We consider a simple first order discretization that leads to the iterative update rules Π t + 1 = Π t − ∆ t η F ( Θ t ) − F 0 ∇ F ( Θ t ) , Θ t + 1 = Θ t + 2∆ t Π t + 1 s + Π 2 t + 1 , ( 12 ) where ∆ t is the step - size hyperaparameter . ( Being the Hamiltonian separable , an interesting future direction would be to explore the use of 2nd order schemes that do not require extra gradient evaluations . ) It is instructive to compare this to Gradient Descent with Momentum ( GDM ) , see e . g . [ 9 ] , which reads Π t + 1 = Π t − ( 1 − β ) Π t − ∇ F ( Θ t ) , Θ t + 1 = Θ t + α Π t + 1 , ( 13 ) where β ∈ ( 0 , 1 ) is usually called the momentum parameter ( not to be confused with Π ) and α the learning rate . We write Π ’s update in this way to stress that for GDM ( 1 − β ) acts as friction ( a negative additive term in the update for the velocity , proportional to the velocity ) . Friction is how GDM converges : setting β = 1 removes the friction and ruins convergence . For ECD instead convergence arises via non - linearities in the update rules : in the example ( 2 ) the evolution slows down as the mass increases . Without the need for friction to converge , energy is conserved ; E = ( F − F 0 ) η ( Π 2 + s ) is constant . More precisely E is exactly constant for the continuum dynamics ( 11 ) but in the discrete case it can oscillate about E by an amount controlled by ∆ t ( see § 3 . 3 ) . Energy conservation implies ( 12 ) Θ t + 1 = Θ t + 2∆ t ( F − F 0 ) η E Π t + 1 . Comparing with with GDM , we see that the dynamics behaves effectively as having a non - constant learning rate that adaptively changes according to the local value of the objective function F . This is true also for Π ( 12 ) : the magnitude of the gradient update is informed by the local value of the function . In § 4 thanks to this property the algorithm generically performs well without needing any HP scheduling . This is a general feature of ECD : it provides gradient based optimization with updates informed by F ( Θ ) . This leads to the HP F 0 introduced above in ( 3 ) , which can be 5 adaptively tuned as in [ 6 ] and in the code associated to this work . One way to view this parameter is as the expected value of the target at the global minimum . More precisely , its effect on the distribution of results and the ( continuum ) speed of propagation was derived above in ( 4 ) - ( 8 ) . In our experiments below , F 0 = 0 , but a full HP analysis based on the formulae in § 2 is an important future direction . Finally , to follow the updated rules ( 12 ) we need to specify an initialization for the momenta Π 0 . Generically , we can initialize them along the direction of minus the initial gradient , with magnitude | Π 0 | 2 ≡ δE . With this choice , the energy is E = F ηinit ( s + δE ) . For the regularized algorithm s = 1 we can set δE = 0 ( default ) and it explores regions with F ⩽ F init . A higher value of δE > 0 allows the regularized algorithm to explore regions in parameter space where F > F init . For s = 0 , we default to δE = 1 . We initialize the momenta as Π 0 = √ δE ∇ F ( Θ 0 ) | ∇ F ( Θ 0 ) | . 3 . 2 Chaos - inducing elements and the volume formula The discrete dynamics in ( 12 ) is symplectic , meaning that volumes in phase space are exactly conserved by the discrete dynamics ( modulo numerical errors ) . See [ 23 , § 2 ] for an introduction . Thanks to this , the continuum prediction ( 1 ) for the distribution of results holds to a very good approximation also for the discrete dynamics , provided sufficient chaos . Although chaos is generic , it can fail or take too long to set in . We encourage chaotic exploration by a tunable modification ensuring trajectories disperse . In [ 6 ] , energy - conserving billiard bounces were introduced with this purpose , implemented by randomly rotating the momentum after a certain number of steps ( either fixed or dynamically tuned ) . In this work , we simplify the chaos - inducing prescription ( reducing the number of HPs ) by adapting a method introduced in [ 27 ] : we replace the full random momentum rotation after multiple steps with a random rotation by some angle at every step . More precisely , after each step the momentum is further updated as Π ← | Π | | Π | Π | + ν z | (cid:18) Π | Π | + ν z (cid:19) ( 14 ) where ν is the chaos hyperparameter and each component of z is drawn from a standard gaussian . The prefactor ensures that the the norm of Π is conserved , thus conserving E . As a consequence of this , the momentum will rotate by an amount controlled by ν √ n , where n is the dimensionality of the problem [ 27 ] . When ν √ n ≪ 1 the angle between the original and rotated Π is small , and for ν √ n > 1 the angle is π / 2 . See § A . 1 for more details . 6 3 . 3 Energy conservation with batches Though phase space volumes are exactly conserved by symplectic integration ( 12 ) , energy violations ∼ ∆ t 2 arise , just oscillating about the continuum value ( until much later timescales with E drift ) . This applies to problems in which the target F is fixed ( i . e . full batch ) . When minibatches are present , they introduce explicit time dependence in the target , F ( Θ ( t ) ; t ) coming from the fact that at given time intervals the target explicitly changes since it is being evaluated on different batches of data . As a result , E will not be automatically conserved with minibatches . To enforce this , we rescale the Π at each step projecting back to the original energy surface , if possible . Explicitly after the update of Π we compute Π 2 , and compare with the value it should have in the original energy surface , from the relation Π 2 = ε ( F − F 0 ) η − s , where we recall that E is the fixed energy determined at initialization . If the right hand side is positive we rescale the Π homogeneously so that Π 2 agrees with it . We observe good performance from this on ML problems with minibatches . It can be skipped for optimization problems in which the target does not change in time , including full - batch training of neural networks . 3 . 4 Weight decay ( WD ) For a non - linear optimizer , WD and L 2 regularization act differently . WD is defined as exponential decay of the weights during training [ 12 ] , while the latter is the addition of a Θ 2 term to the loss . While for SGD there is no difference between the two ( with an appropriate rescaling ) the difference appears already for Adam , as stressed in [ 24 ] with regard to AdamW . More precisely , the first way of implementing WD changes the update rule of the parameter as Θ t + 1 = · · · − ∆ tw d 0 Θ t where the dots denote terms that would be there in the update rule if weight decay was zero . This modification of the Hamiltonian dynamics introduces an E violation . The second one , akin to an L 2 term , is just a modification of the target function , not a change of the form of the Hamiltonian . To implement it , we define a constant w d and modify the update rules by the shift F ( Θ ) → F ( Θ ) + w d 2 Θ 2 , ∇ F ( Θ ) → ∇ F ( Θ ) + w d Θ . ( 15 ) While the first choice can also be made compatible with the ECD framework when E conservation is explicitly enforced as in § 3 . 3 , at this stage we have only implemented the E - conserving option in ( 15 ) . It would be interesting to compare the two for ECD algorithms to see if it yields an advantage . 3 . 5 The full algorithm Algorithm 1 summarizes ECDSep , aside from the also - included adaptive tuning of F 0 . We note here that Theorem 2 . 1 of [ 6 ] – the impossibility of stopping at a local minimum 7 due to the energy conservation – holds also in our current algorithm ( see § A in the Appendix for our proof ) . The s = 1 algorithm explores the region in which the objective function F satisfies F − F 0 ⩽ ( F init − F 0 ) ( 1 + δE ) 1 / η , while the s = 0 algorithm explores the whole space . Algorithm 1 ECDSep . s = 1 ( default ) is the regularized version of the algorithm . Defaults are F 0 = δE = w d = 0 , ∆ t = 0 . 4 , ν = 10 − 5 and η is required . For s = 0 , δE = 1 . ε 1 = 10 − 10 and ε 2 = 10 − 40 are numerical constants ensuring stability . The block ensuring energy conservation can be removed for optimization problems without minibatches . Require : F ( Θ ) : Function to minimize . Require : Θ : Initial parameter vector . E ← (cid:0) F ( Θ ) − F 0 + 12 w d Θ 2 (cid:1) η ( δE + s ) ( Initialize energy ) Π ← − ∇ F ( Θ ) | ∇ F ( Θ ) | √ δE ( Initialize momenta ) repeat V ← (cid:0) F ( Θ ) − F 0 + 12 w d Θ 2 (cid:1) η if energy conservation = True then π 2 C ← E V − s if | Π 2 − π 2 C | > ε 1 and π 2 C > 0 then Π ← (cid:113) π 2 C Π 2 Π end if end if Π ← Π − ∆ tη V 1 / η ( ∇ F ( Θ ) + w d Θ ) Θ ← Θ + 2∆ t ΠΠ 2 + s Π N ← Π | Π | + ν z ( z is a normal random vector ) Π ← | Π | | Π N | Π N until V < ε 2 3 . 6 A guide to hyperparameter tuning Important HPs for the algorithm 1 include the step - size ∆ t , the concentration exponent η , and the chaos HP ν ; we expect also the loss offset F 0 to play an important role ultimately ( cf . § 2 ) . Along with the continuuum formulas in § 2 which relate the concentration of measure to some of HPs , their intuitive behavior explained earlier in this section simplifies the task of their tuning . In our experiments we will focus on ∆ t , η , and ν , leaving a detailed experimental treatment of F 0 for future work ( while including an adaptive tuning option for F 0 in our current algorithm ) . As described in ( 6 ) , increasing η from 1 accentuates the region of smallest loss . We expect this to be important for problems in the undeparameterized regime , where the loss is a good indicator of the test accuracy , and thus where we want to get to the 8 bottom of the basin as quickly as possible . We see indications of this in the problems in § 4 . 3 and on the comparison in Fig . 2 . The formulas in § 2 suggest opportunities for significant principled improvements bringing in HP relations including F 0 . On the other hand , a choice η = 1 can work for situations where instead of directly targeting the loss we want the optimizer to explore the low - loss basin region , with the goal of visiting points that might have a higher test accuracy ( either individually or after averaging ) even if they have a higher loss . We check this on image classification problems in § 4 . 2 , where we show that weight averaging [ 16 ] with η = 1 efficiently explores the low - lying basin finding solutions that generalize well , without needing any learning rate scheduling . More generally one could use our understanding of the HPs’ effects on the measure to sample at the radius | Θ | ∗ in ( 7 ) to see how that affects performance . Increasing ν accentuates chaos , generically reducing the mixing time . In a given problem it may need to to exceed a certain threshold so that ( 1 ) applies . As discussed above in § 3 . 2 , the bounce angle per step is determined by ν 2 n , with small - angle bounces for ν 2 n ≪ 1 and large ( π / 2 angle ) bounces otherwise . We find examples of each regime in our HP - tuned experiments below ( see § B . 6 ) , with the latter somewhat akin to a random walk . We set 10 − 5 as default , and did a logarithmic scan to understand it for different problems . This again is an opportunity for improvement by relating it to the timescale related to | Θ | ∗ in ( 7 ) , similarly to the typical set timescale giving a tuning - free algorithm in [ 27 ] . We derive the corresonding relations in § A . 1 . We tune the learning rate ∆ t , empirically finding good performance with a high value , setting 0 . 4 as default . This effect might be related to the more stable symplectic ( and approximately energy - conserving ) dynamics . It will be an important future direction to derive a principled scaling of the hyperparameters with the scale of the problem , c . f . [ 26 , 32 ] . 4 Experimental results Here we compare ECDSep ( Algorithm 1 ) to SGD with momentum , Adam and AdamW , often used to achieve state of the art performance different ML tasks . We treat synthetic benchmarks in § 4 . 1 , image classification in § 4 . 2 , node classification problems on graphs in 4 . 3 and natural language processing ( NLP ) in § 4 . 4 . The problems in 4 . 1 ( and Fig . 2 ) test ECDSep as a pure optimizer ( targeting directly the loss ) , while the problems in the other sections analyze its ability to find regions in the loss landscape that lead to better accuracy in ML setups ( e . g . overparameterized ) where the two don’t agree . Along the way , we test ( for just F 0 = 0 ) the predicted improvement of the training loss as a function of η in § 2 . For Adam , AdamW and SGD we scanned over a fixed number of parameters : the learning rate α , WD and the momentum β ( only for SGD ) . Depending on the experiment , 9 references can supply a good range of HPs for some optimizers ; we then allow a larger scan for the others . We briefly discuss the setups and HP selection ; more details appear in § B , and the full code to reproduce them can be found at https : / / github . com / gbdl / ECDSep . Given the modest statistics , we only quote the average in the tables and present the full distribution of the corresponding experimental results in § B in the Appendix . The experiments in § 4 . 1 were performed on a single laptop CPU , the experiments in § 4 . 2 and § 4 . 4 on a single NVIDIA - GEFORCE - RTX2080Ti with 11GB of GPU memory and 16GB RAM , and the experiments in § 4 . 3 on a single NVIDIA Tesla T4 with 15GB of GPU memory and 12 . 7GB of RAM . Overall we find that ECDSep , while not always best on individual experiments , is more reliable across a heterogeneous set of problems . Depending on the problem Adam ( W ) performs better than SGDM , or viceversa , with ECDSep competitive with the leader , as summarized in Table 1 . Table 1 : Average accuracy over the experiments in this paper ( excluding synthetics ) ECDSep SGD Adam AdamW Average accuracy 74 . 83 73 . 10 74 . 06 73 . 74 4 . 1 Synthetic benchmarks We start by testing Algorithm 1 on synthetic benchmark optimization problems and compare with GDM and Adam ( = AdamW since WD = 0 ) . To test on a problem with a shallow valley we considered the n = 10 Zakharov function [ 17 , Function 173 ] , where we also compared to BBI [ 6 ] , and to test escape from local minima we use a regularized Ackley function [ 1 ] . We select HPs using Optuna [ 2 ] by performing 500 trials starting form a fixed point , and compared algorithms starting from new random initial points for more iterations ( Fig . 1 ) . More details can be found in § B . 1 . 0 500 1000 1500 2000 2500 3000 1e7 1 1e - 7 1e - 14 1e - 21 1e - 28 0 500 1000 1500 2000 2500 1e2 1 1e - 2 1e - 4 GDM Adam ECDSep BBI Figure 1 : Zakharov and Ackley : a typical run from a random starting point . 10 4 . 2 Image classification CIFAR - 100 and Tiny Imagenet training with weight averaging We trained residual networks from scratch on the popular medium - scale image classification tasks CIFAR - 100 [ 20 ] and Tiny - Imagenet [ 22 ] . State of the art on these problems with residual networks and without data augmentation [ 16 ] uses Stochastic Weight Averaging ( SWA ) , averaging the network visited during the last phase of the training , giving a network with a better test accuracy than the individual elements of the average . Our goal is to understand how ECDSep compares with SGD and Adam ( W ) in this task . Since in this problem we are not interested in concentrating the volume at small loss but on exploring the low - loss region to find points with better accuracy , for this comparison we set η = 1 ( default ) as explained in § 3 . 6 ; later we will analyze the training loss , to test the behavior of η in optimization per se . For CIFAR - 100 we reproduced and improved the results in [ 16 , § 4 . 4 ] by train - ing a WideResnet28x10 [ 33 ] with SGDM on CIFAR - 100 and scanning on their same learning rates α = { 0 . 1 , 0 . 05 , 0 . 01 , 0 . 001 } to which we added a momentum scan β = { 0 . 9 , 0 . 95 , 0 . 99 } . From the procedure in that paper , we also borrowed the value of weight decay ( w d = 5 × 10 − 4 ) , training epochs ( 300 ) and the fixed epoch at which to start the averaging , swa - start = 161 ( budget 1 . 5 ) . More details on the SWA proce - dure are collected in § B . 2 . For Adam and AdamW , we scanned over a broader set of learning rates α = { 0 . 1 , 0 . 01 , 0 . 001 , 0 . 00001 } and allowed a possibly different weight decay w d = { 10 − 4 , 5 × 10 − 4 , 10 − 3 } . For ECDSep instead , we scanned over learning rates ∆ t = { 0 . 4 , 0 . 6 } and chaos parameter ν = { 10 − 4 , 5 × 10 − 5 , 10 − 5 } . We repeated each experiment 4 times with different seeds , with best results in Table 2 . To confirm these results , we performed a more extensive study on a different data set and with a different architecture : ResNet - 18 [ 13 ] on Tiny Imagenet . We trained for 100 epochs and , since we were using a constant learning rate thorough , we gave more freedom to each optimizer by allowing it to choose the best epoch at which the averaging starts . This is done by saving the networks found during training and averaging over them at the end , going back up to half of the training . The best result found is then kept . For SGD we performed an extensive scan on α = { 0 . 1 , 0 . 05 , 0 . 01 , 0 . 001 } and β = { 0 . 9 , 0 . 99 } and fixed w d = 10 − 4 . For ECDSep we fixed the same value for the weight decay and fixed again η = 1 and scanned over ∆ t = { 0 . 4 , 0 . 6 } and chaos parameter ν = { 10 − 4 , 5 × 10 − 5 , 10 − 5 } . For Adam and AdamW we scanned α = { 0 . 1 , 0 . 01 , 0 . 001 , 0 . 0001 } and also allowed different weight decays values w d = { 10 − 4 , 5 × 10 − 4 } . We repeated all the experiments three times with different seeds and collected the results in Table 2 . Finally , we analyzed the behavior of ECDSep as a function of η . In Fig . 2 we show different training runs with the WideResnet on CIFAR 100 for different values of η , keeping fixed the other hyperparameters ( ∆ t = 0 . 5 , ν = 10 − 4 ) . We see that increasing 11 Table 2 : Full training on CIFAR - 100 and TinyImagenet and fine - tuning on IN - 1K : mean accuracy . ECDSep SGD Adam AdamW CIFAR 100 82 . 57 82 . 50 79 . 01 78 . 71 Tiny Imagenet 66 . 44 64 . 83 61 . 67 59 . 84 IN - 1K ( fine tuning ) 70 . 49 70 . 49 70 . 48 70 . 48 η > 1 monotonically decreases the loss as predicted in § 2 , in this case tested up to η = 4 . In this overparametrizied regime this translates into an improvement on accuracy only up to a certain point η = 2 . 5 , with extra improvements on the loss being detrimental for accuracy . However , the η = 1 case , which has a lower accuracy before averaging , makes a much higher jump in accuracy after SWA ( starting here at epoch 200 ) , resulting in the best averaged accuracy . This suggests that the η = 1 dynamics visits a region of the landscape more relevant for generalization in this class of problems . 0 150 300 1 0 . 1 0 . 01 200 250 300 74 78 82 200 250 300 68 72 76 = 1 = 1 . 5 = 2 = 2 . 5 = 3 = 3 . 5 = 4 Figure 2 : Left : training loss , center : SWA test accuracy , right : test accuracy , over 300 epochs . Fine tuning on Imagenet - 1K We tested ECDSep on the task of fine - tuning on a large image dataset , Imagenet - 1K [ 7 ] , which is freely available for non - commercial research and educational purposes . Due to computational constraints , we did not perform the whole tuning , but we start with a pre - trained ResNet 18 in PyTorch , which we fine - tuned for 10 extra epochs and perform SWA as in § 4 . 2 . We fix w d = 10 − 4 and again η = 1 and we scan ∆ t = { 0 . 4 , 0 . 1 , 0 . 05 , 0 . 01 } and ν = { 10 − 3 , 10 − 4 , 10 − 5 } . For SGD we scanned α = { 5 × 10 − 2 , 10 − 2 , 10 − 3 , 10 − 5 } and momentum β = { 0 . 9 , 0 . 95 , 0 . 99 } For Adam and AdamW we allowed the weight decay to change , and we scanned over α = { 10 − 3 , 10 − 4 , 5 × 10 − 5 , 10 − 5 } and w d = { 10 − 3 , 10 − 4 , 10 − 5 } . Table 2 averages 2 runs per optimizer , which are essentially tied . 12 4 . 3 Graphs We tested our optimizer on two graph problems from the Open Graph Benchmark ( OGB ) [ 15 ] , a rich graph benchmark licensed under the MIT license . The datasets are ogbn - arxiv and ogbn - proteins , and in both cases a deep neural network is trained for node classification . In particular , we focus on certain Graph Neural Networks ( GNNs ) [ 4 , 19 ] . The details about the datasets and the GNNs are described in § B . 4 . For SGD and Adam ( W ) we performed a scan over α = { 10 − 1 , 5 · 10 − 2 , 10 − 2 , 5 · 10 − 3 , 10 − 3 , 5 · 10 − 4 } and momentum for SGD over β = { 0 . 9 , 0 . 95 , 0 . 99 , 0 . 999 } . Regarding ECDSep , first we did a few runs to understand the scale of the HPs , then we performed a finer scan over ∆ t = { 1 . 5 , 1 . 8 , 2 , 2 . 5 , 2 . 8 , 3 } , η = { 4 . 5 , 5 , 5 . 5 , 6 } , with ν = 10 − 5 ; we scanned WDs { 0 , 10 − 3 , 10 − 4 , 10 − 5 } for all 4 . We performed 10 and 5 runs of the best combination of HPs with different seeds for ogbn - arxiv and ogbn - proteins respectively . See Table 3 . AdamW performed best , with ECDSep very close . For ogbn - proteins ECDSep outperforms SGD by many points of ROC - AUC score . Table 3 : Full training ogbn - arxiv , ogbn - proteins : best accuracy , ROC - AUC score . ECDSep SGD Adam AdamW ogbn - arxiv 71 . 55 71 . 81 72 . 37 72 . 41 ogbn - proteins 74 . 67 65 . 79 77 . 42 77 . 44 4 . 4 Language on the BERT transformer Next we finetune BERT [ 8 ] on the GLUE benchmark [ 30 ] , one of the standard benchmarks for NLP ( licensed under the CC BY 4 . 0 license ) . The GLUE benchmark comprises 8 different NLP tasks : CoLA ( acceptability prediction ) , MNLI ( natural language inference ) , MRPC ( semantic - similarity scoring ) , QNLI ( sentence pair classification ) , QQP ( semantic - similarity scoring ) , RTE ( natural language inference ) , SST - 2 ( sentiment classification ) , STS - B ( text scoring ) . The scan over different optimizers was performed as follows . For Adam and AdamW we scanned the learning rate over the values identified in [ 8 , App . A . 3 ] { 2 × 10 − 5 , 3 × 10 − 5 , 5 × 10 − 5 } , and the weight decay over { 0 , 10 − 2 , 10 − 3 } , giving 9 combinations in total . For SGD , since we had no prior information of performance on this problem , we allowed a more extensive scan on learning rates { 10 − 2 , 10 − 3 , 10 − 4 , 10 − 5 } , momentum { 0 . 9 , 0 . 99 } and weight decay { 10 − 2 , 10 − 3 } , for 18 combinations in total . For ECDSep first , similarly as in the graph setting , we briefly determined that a smaller learning rate was needed being this a fine tuning problem , fixing it at ∆ t = 0 . 04 , then we scanned 13 over η = { 1 , 1 . 4 , 2 . 0 } , ν = { 10 − 4 , 10 − 5 } and w d = { 0 , 10 − 2 } , for 12 combinations in total . For SGD , Adam and AdamW we used a linear learning rate schedule as in [ 8 , App . A . 3 ] . We repeated each experiment 3 times with results in Table 4 , and in full in § B . 5 . Table 4 : BERT fine tuning . First 8 columns : the average over 3 runs of the metrics test Matthews correlation for CoLA , test Spearmans’s correlation for STS - B , test F1 score for MRPC and QQP , and test accuracy for the remaining datasets . Last column : average over all datasets . MNLI QQP QNLI SST - 2 CoLA STS - B MRPC RTE avg . ECDSep 84 . 24 86 . 70 91 . 19 92 . 66 57 . 91 89 . 26 90 . 96 73 . 16 83 . 26 SGD 83 . 31 86 . 36 91 . 03 92 . 17 60 . 54 89 . 26 90 . 88 71 . 96 83 . 19 Adam 84 . 31 88 . 14 91 . 39 92 . 81 59 . 34 89 . 02 91 . 09 71 . 36 83 . 43 AdamW 84 . 41 88 . 21 91 . 49 93 . 03 59 . 68 89 . 15 91 . 13 71 . 24 83 . 54 5 Limitations and Future Directions We described important limitations in the text , and here summarize . We have a broad class of experiments , but low statistics and dominantly over - parameterized . We have not exploited the full power of the predictions in § 2 for the distribution of results and speed of propagation , particularly as regards the dependence on and relationships among η , F 0 , ν and other HPs , and their dependence on the dimension n ( including width scalings ) . All this leaves room for substantial improvements in the future . Those calculations could be generalized to the discrete case , though our symplectic integration controls some discretization errors . Finally , we mention that adaptive methods to integrate the equations of motion ( 11 ) can provide further improvements to performance , via the wealth of literature about numerical Hamiltonian integration . It would also be interesting to explore the possibility of algorithmically discover Hamiltonian ECD optimizers , along the lines of [ 5 ] . 6 CO 2 emission Overall , the experiments in § 4 run for a cumulative of approximatively 11658 hours ( of which 7672 hours for the experiments presented in the paper ) . Total emissions are estimated to be 683 . 34 kgCO 2 eq , of which 17 . 14 kgCO 2 eq directly offset . More details can be found in § B . 7 . 14 7 Acknowledgments We are grateful to Daniel Kunin for discussions and collaboration during the early stages of this project . We also thank Guy Gur - Ari , Ethan Dyer , Aitor Lewkowycz , Dan Roberts , Jakob Robnik , Uros Seljak , Sho Yaida and participants of the 2023 Aspen Winter conference on “Theoretical Physics for Machine Learning” for fruitful discussions and suggestions . Our research is supported in part by the Simons Foundation Investigator and Modern Inflationary Cosmology programs , and the National Science Foundation under grant number PHY - 1720397 . Some of the computing for this project was performed on the Sherlock cluster . We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results . A More details on the theory and proofs Here we derive ( 4 ) from the first equality in ( 1 ) , plugging in the Hamiltonian ( 2 ) . This gives µ = (cid:90) d n Θ d n Π δ ( E − V Π 2 ) ( 16 ) = (cid:90) d Ω n − 1 (cid:90) d n Θ (cid:90) d | Π | | Π | n − 1 δ ( E − V | Π | 2 ) ( 17 ) = (cid:90) d Ω n − 1 (cid:90) d n Θ | Π | n − 1 2 V | Π | | | Π | = √ E / V ( 18 ) = E ( n − 2 ) / 2 π n / 2 Γ ( n / 2 ) (cid:90) d n Θ V − n / 2 . ( 19 ) The first equality is the definition of the measure . The second rewrites the momentum integral in terms of its angular and radial directions ( the Hamiltonian depends only on the radial direction , its magnitude ) . In the third equality we used the delta function to do the integral over the magnitude | Π | of the momentum , via the general formula (cid:82) dxδ ( f ( x ) ) = 1 / f ′ ( x ) | x ∗ where f ( x ∗ ) = 0 . The final line implements the substitution indicated in the previous line and also evaluates the angular integral ( the area of the ( n − 1 ) - sphere ) . Theorem 1 . If V ̸ = 0 ( and V ̸ = E / s ) then ˙ Θ ̸ = 0 in the continuum evolution , and Θ ( t + ∆ t ) ̸ = Θ ( t ) for the discrete algorithm . The theorem and proof is a direct generalization of Thm 2 . 1 in [ 6 ] , with the replace - ment of the Born - Infeld Hamiltonian with ( 2 ) and use of the resulting update rules for our case ( 12 ) . In particular in comparison to equation ( 4 ) of [ 6 ] , we have constant energy E = ˙Θ 2 V + sV = V Π 2 + sV ( 20 ) 15 where s = 0 for the dynamics that can reach everywhere , and s = 1 for the regularized version , as described in the main text . A similar result holds for a wide range of ECD Hamiltonians . A . 1 Theory of the chaos parameter ν and additional hyperpa - rameter relations In § 2 we used the energy - conserving feature of ECD to derive key relations such as ( 7 ) and ( 8 ) . Following [ 27 ] we can further relate the parameter dimensionality n and various hyperparameters to the chaos hyperparameter ν defined in § 3 . 2 . First it is useful to note that the bounce angle is determined by the combination ν √ n , as in [ 27 ] . This is because ν appears in combination with z in the chaos update in § 3 . 2 , and in its component - wise Gaussian distribution the expectation value of z i z j is nδ ij . In more detail , the bounce angle is determined by Π · Π ′ = | Π | | Π ′ | cos ( α ν ) , ( 21 ) with Π ′ the bounce - updated momentum Π ′ = | Π | | Π | Π | + ν z | (cid:18) Π | Π | + ν z (cid:19) . ( 22 ) This gives cos ( α ν ) = 1 + ν z · ˆ Π | ˆ Π + ν z | = 1 + ν z · ˆ Π (cid:112) 1 + 2 ν ˆ Π · z + ν 2 z 2 ( 23 ) where ˆ Π denotes the unit vector in the direction of Π . To estimate the angle we take an expectation value of this in the standard Gaussian ensemble for each component of z . For large ν 2 n this gives an expected cos ( α ν ) near 0 , meaning a large per - step bounce angle ∼ π / 2 . Below we will see that this regime arises in some of our fine - tuning experiments ( ImageNet 1K and the BERT examples with ν = 10 − 4 ) . For ν 2 n ≪ 1 , the angle is small , α ν ∼ ν √ n . This corresponds to a small bounce per step , a regime which arises in our other experiments . In the use of energy - conserving chaotic Hamiltonian dynamics for sampling , a tuning - free prescription for ν was derived in [ 27 ] . This followed from the observation that the needed chaos should amount to one bounce per orbit on the typical set , in that context at a distance θ typ ∝ √ n . If we adopt this idea in our case , replacing the typical set by our scale | Θ ∗ | , we have a timescale between full ( 2 π ) bounces ∆ t sep = E ∆ t = E 2 π | Θ ∗ | | ˙ Θ ∗ | . ( 24 ) 16 In the small - angle case , this means an angle per step α ν ∼ 2 π ∆ t ∆ t sep ( 25 ) with ∆ t our step size . Plugging in the explicit forms for these quantities given in § 2 , for small angle and for F 0 = 0 , s = 1 we find for n ≫ 1 ν ∗ √ n ≈ ∆ t (cid:114) F 2 F min (cid:18) F min F init (cid:19) η / 2 (cid:18) η η − 1 (cid:19) η / 2 ( η − 1 ) 1 / 2 ( 26 ) with F init the value of the objective at initialization . For η = 1 and F 0 ̸ = 0 , this suggests instead for n ≫ 1 : ν ∗ √ n ≈ ∆ t (cid:114) F 2 F init − F 0 . ( 27 ) This analysis unveils strong dependencies among our hyperparameters . For example , given the small ratio F min / F init we note an exponential sensitivity of ν ∗ to η in ( 26 ) . As with the other general relations derived in § 2 , it will be very interesting to test and exploit them experimentally . A . 2 Self - tuning of F 0 Here we describe a version of Algorithm 1 where F 0 is automatically tuned , if the initial value is higher than the true global minimum . The idea is that in this situation there will be a step at which , due to discreteness , one iteration will try to jump to a negative V . This works only for the case in which η is an odd integer , to which we now restrict ourselves . Here we present a linear shift of F 0 , but another option can be an exponential backoff with some cutoff . The algorithm 2 as presented here is preliminary . B More details on the experimental setup and results In this section , we expand on the results and setups for the experiments presented in the main text in § 4 . In particular we discuss more details about the full specification of the problems together with the full distributions of the results whose average appear in the Tables in the main text . We conclude in § B . 7 with more details on the estimation of CO2 emissions . The full code needed to reproduce the experimental results , together with licensing specification , can be found at https : / / github . com / gbdl / ECDSep . 17 Algorithm 2 ECDSep - self tuning . s = 1 ( default ) is the regularized version of the algorithm . Defaults are F 0 = δE = w d = 0 , ∆ t = 0 . 4 , ν = 10 − 5 . For s = 0 , δE = 1 . ε 1 = 10 − 10 and ε 2 = 10 − 40 are numerical constants ensuring stability . The block ensuring energy conservation can be removed for optimization problems without minibatches . η ( required ) has to be an odd integer . Require : F ( Θ ) : Function to minimize . Require : Θ : Initial parameter vector . ∆ F 0 ← 0 ( Initialize F 0 shift ) E ← (cid:0) F ( Θ ) − F 0 + 12 w d Θ 2 (cid:1) η ( δE + s ) ( Initialize energy ) Π ← − ∇ F ( Θ ) | ∇ F ( Θ ) | √ δE ( Initialize momenta ) while True do V ← (cid:0) F ( Θ ) − ( F 0 + ∆ F 0 ) + 12 w d Θ 2 (cid:1) η if V < ε 2 then ∆ F 0 ← ∆ F 0 + 5 V else if energy conservation = True then π 2 C ← E V − s if | Π 2 − π 2 C | > ε 1 and π 2 C > 0 then Π ← (cid:113) π 2 C Π 2 Π end if end if Π ← Π − ∆ tη V 1 / η ( ∇ F ( Θ ) + w d Θ ) Θ ← Θ + 2∆ t ΠΠ 2 + s Π N ← Π | Π | + ν z ( z is a normal random vector ) Π ← | Π | | Π N | Π N end if end while B . 1 Synthetic Experiments The Zakharov function [ 17 , Function 173 ] is a standard benchmark for optimization on shallow valleys , given by F ( Θ ) ≡ n (cid:88) i = 1 θ 2 i + (cid:32) 1 2 n (cid:88) i = 1 iθ i (cid:33) 2 + (cid:32) 1 2 n (cid:88) i = 1 iθ i (cid:33) 4 , ( 28 ) and we study it for n = 10 . It has no local minima , but the global minimum at Θ = ( 0 , . . . , 0 ) lies in a nearly flat valley , slowing optimization . Even though we are studying it in 10 dimensions , to guide the eye we depict it for n = 2 in Fig . 3 . To compare the various optimizers on such synthetic problems , we use Optuna [ 2 ] to find 18 the best hyperparameters . Specifically we fix a common starting initial point ( 1 , . . . , 1 ) and we perform 500 trials for each algorithm , each one corresponding to evolution for 250 iterations . With this approach , optimizers with more hyperparameters to tune are indirectly penalized since within the fixed amount of trials a much smaller portion of the search space is explored . For Adam , we searched in α ∈ ( 10 − 2 , 10 4 ) , β 1 ∈ ( 0 . 7 , 1 ) , β 2 ∈ ( 0 . 7 , 1 ) , ϵ ∈ ( 10 − 12 , 10 − 6 ) . For ECDSep we searched in ∆ t ∈ ( 10 − 2 , 10 4 ) , η ∈ ( 1 , 4 ) , ν ∈ ( 10 − 8 , 1 ) , δE ∈ ( 0 , 5 ) and consEn ∈ { True , False } . For SGD we first searched with learning rate in the same range of the other optimizer , but this always resulted in a divergent evolution . Thus we lowered the learning rate search space by searching over α ∈ ( 10 − 8 , 10 − 3 ) . The fact that SGD requires a smaller learning rate is confirmed by the fact that the optimal value found by Optuna for SGD is α ∼ 10 − 6 . Simultaneously we also searched over momentum β ∈ ( 0 . 8 , 1 ) . Finally , for BBI we used the same search space as in [ 6 ] . The results of a typical run from a randomly selected point using the HPs thus es - timates are shown in Fig . 1 , left panel . More details can be found in the notebook ECDSep _ zakharov . ipynb . A 2 - dimensional depiction of the n - dimensional Zakharov function can be found in Fig . 3 ( left ) . An important property of ECD algorithms is that thanks to energy conservation the evolution does not stop at local minima , as proved in Thm . 1 . To check this and compare with common algorithms we tested evolution on a regularized two - dimensional Ackley function [ 1 ] , which reads F ( θ 1 , θ 2 ) ≡ − 20 exp (cid:20) − 0 . 2 (cid:113) 0 . 5 ( θ 21 + θ 22 ) (cid:21) − exp [ 0 . 5 ( cos 2 πθ 1 + cos 2 πθ 2 ) ] + + e + 20 + 10 − 8 ( θ 21 + θ 22 ) 4 . ( 29 ) We depict ( 29 ) in Fig . ( 3 ) ( right ) . For this test we followed a protocol similar to the one used for the Zakharov function , this time starting from the initial point ( − 4 , 3 ) . For Adam , we searched over α ∈ ( 10 − 4 , 1 ) , β 1 ∈ ( 0 . 7 , 1 ) , β 2 ∈ ( 0 . 7 , 1 ) , ϵ ∈ ( 10 − 12 , 10 − 6 ) . For ECDSep we searched in ∆ t ∈ ( 10 − 4 , 1 ) , η ∈ ( 1 , 10 ) , ν ∈ ( 10 − 5 , 1 ) , δE = 0 . For SGD α ∈ ( 10 − 8 , 10 − 3 ) and momentum β ∈ ( 0 . 8 , 1 ) . We find that SGD and Adam with a small learning rate immediately get stuck in local minima . With a higher learning rate they erratically explore the landscape , sometimes getting to smaller values of the target , but they do not converge there . ECDSep instead explores the landscape and eventually converges to the global minimum . The results of a typical run starting from a randomly selected point using the HPs thus estimates are shown in Fig . 1 , right panel . More details can be found in the notebook ECDSep _ ackley . ipynb . 19 6 4 2 0 2 4 6 6 4 20 2 4 6 1000 2000 3000 4000 5000 6000 4 2 0 2 4 4 2 0 2 4 1 2 3 4 Figure 3 : Left : a 2 - dimensional depiction of the n - dimensional Zakharov function ( 28 ) ; Right : regularized Ackley function ( 29 ) . B . 2 CIFAR - 100 and Tiny Imagenet In the experiments in § 4 . 2 we studied image classification using residual networks . In particular , we used the procedure of averaging the weights of the networks explored during the final phase of the training ( SWA ) , which was introduced in [ 16 ] , where it was demonstrated the the averaging results in a better test accuracy than the individual networks visited during training . See also [ 10 ] for a recent analysis and extension . More in detail , [ 16 ] discusses a procedure in which a first learning rate scheduling is performed for the pre - averaging phase , followed by the averaging phase that starts at a pre - determined epoch swa - start in which the learning rate is raised again and annealed . In § 4 . 2 , we worked with a simplified protocol for SWA that requires much less tuning while still achieving very good performance , where the learning rate is kept constant during the whole training including the averaging phase . This has been studied already in [ 16 , § 4 . 4 ] by training a WideResnet28x10 [ 33 ] with SGD on CIFAR - 100 , where was shown that even though using a fixed high learning rate results in a much higher test error than with a scheduling , it makes a larger jump after averaging improving over SGD with fixed learning rate . We find the same phenomenon to be true for ECDSep where it is accentuated for small values of η , as shown in Fig . 2 . The result we obtain for SGD is higher than the reported one ( 81 . 5 ) in [ 16 , § 4 . 4 ] , see Table 2 . As discussed in the main text , we also tested Adam and AdamW on the same dataset and with the same network . For Tiny Imagenet , instead , we used a different network achitecture , ResNet 18 [ 14 ] , and we did not fix beforehand the epoch at which the averaging starts , but we collected all the visited networks and at the end of training we computed running averaging start 20 from the last epoch , for each experiment and for each optimizer . In each case we kept the best result found in this way . The motivation behind this protocol was to not to rely on the choice of swa - start as determined in [ 16 ] where only SGD has been tested . The results are collected in Table 2 in the main text . Below we present the best hyperparameters found with the scan discussed in § 4 . 2 , together with the full distributions of results obtained by running the experiments multiple times with different seeds with the best HPs . • On CIFAR100 : – ECDSep : ∆ t = 0 . 4 , ν = 5 × 10 − 5 , with results { 82 . 74 , 82 . 6 , 82 . 48 , 82 . 45 } – SGD : α = 0 . 05 , β = 0 . 9 , with results { 82 . 59 , 82 . 59 , 82 . 41 , 82 . 39 } – AdamW : α = 10 − 4 , w d = 10 − 4 , with results { 79 . 04 , 78 . 37 , 78 . 66 , 78 . 78 } – Adam : α = 10 − 4 , w d = 10 − 4 , with results { 79 . 31 , 79 . 1 , 79 . 02 , 78 . 58 } • On Tiny Imagenet : – ECDSep : ∆ t = 0 . 6 , ν = 5 × 10 − 5 , with results { 66 . 42 , 66 . 58 , 66 . 33 } – SGD : α = 0 . 1 , β = 0 . 9 , with results { 64 . 41 , 64 . 64 , 64 . 45 } – AdamW : α = 10 − 3 , w d = 10 − 4 , with results { 59 . 84 , 60 . 05 , 60 . 12 } – Adam : α = 10 − 3 , w d = 10 − 4 , with results { 61 . 59 , 61 . 81 , 61 . 6 } These experiments were conducted on a single NVIDIA - GEFORCE - RTX2080Ti with 11GB of GPU memory and 16GB RAM , with batch sizes 128 . B . 3 Imagenet - 1K For the experiments in this section , we performed a 10 epochs fine - tuning on the IN - 1K dataset [ 7 ] , starting from a pre - trained ResNet - 18 included with PyTorch . We then performed the same SWA procedure described for the Tiny - Imagenet experiments in § B . 2 . Due to resource constraints we only performed a short fine - tuning and with little statistics , which was insufficient to reveal appreciable differences among the optimizers . It would be interesting to perform a more extensive study of ECD on the IN - 1K dataset . For the scan detailed in § 4 . 2 , we found the following best HPs and corresponding distribution of results . • ECDSep : ∆ t = 0 . 1 , ν = 10 − 3 , w d = 10 − 4 , with results { 70 . 515 , 70 . 456 } • SGD : α = 5 × 10 − 5 , β = 0 . 99 , w d = 10 − 4 , with results { 70 . 494 , 70 . 483 } 21 • AdamW : α = 10 − 5 , w d = 10 − 4 with results , { 70 . 481 , 70 . 487 } • Adam : α = 10 − 5 , w d = 10 − 4 with results , { 70 . 482 , 70 . 479 } These experiments were conducted on a single NVIDIA - GEFORCE - RTX2080Ti with 11GB of GPU memory and 16GB RAM , with batch sizes 128 . B . 4 Graphs The details about the datasets ogbn - arxiv and ogbn - proteins are given below . • ogbn - arxiv is a dataset made of a directed graph that represents the citation network between all Computer Science ( CS ) arXiv papers indexed by MAG [ 31 ] . Each vertex is an arXiv paper , and there is a directed edge between two vertices if one paper cites the other one . The goal is to predict the primary categories of arXiv CS papers , formulated as a 40 - class classification problem . The metric used to measure the performance is the usual accuracy . The graph has 169 , 343 vertices and 1 , 166 , 243 edges . • ogbn - proteins is a dataset made of an undirected , weighted and typed graph . Each vertex is a protein and edges represent different types of biological association between proteins , like physical interaction , homology etc . The task is to predict the presence of a protein function among 112 possibilities , so to label correctly each vertex . The metric used to mesure the performance is the ROC - AUC score . The graph has 132 , 534 vertices and 39 , 561 , 252 edges . For each of the above datasets the benchmark [ 15 ] provides different types of deep neural networks . We focus on Graph Neural Networks ( GNNs ) having GraphSage [ 11 ] as graph convolutional layers . Each GNN has 3 graph convolutional layers with 256 hidden channels and ReLU activation function . The GNN for ogbn - arxiv includes also batch normalization and 0 . 5 droput . The number of training epochs is 500 for ogbn - arxiv and 1000 for ogbn - proteins , and the evaluation on the testing set is performed every 1 and 5 epochs respectively . The best performance is • for ogbn - arxiv : – ECDSep : ∆ t = 2 . 8 , η = 4 . 5 , ν = 10 − 5 , w d = 0 , with results { 71 . 57 , 71 . 61 , 71 . 9 , 71 . 62 , 71 . 4 , 71 . 43 , 71 . 51 , 71 . 53 , 71 . 33 , 71 . 48 } . – SGD : α = 0 . 1 , β = 0 . 95 , w d = 10 − 3 with results { 71 . 64 , 71 . 74 , 71 . 51 , 71 . 55 , 71 . 37 , 71 . 58 , 71 . 84 , 71 . 49 , 71 . 8 , 71 . 39 } . 22 – AdamW : α = 5 · 10 − 3 , w d = 0 , with results { 72 . 1 , 72 . 65 , 72 . 11 , 72 . 42 , 72 . 33 , 72 . 34 , 72 . 38 , 72 . 28 , 72 . 42 , 72 . 33 } . – Adam : α = 5 · 10 − 3 , w d = 0 , with results { 72 . 26 , 72 . 11 , 72 . 6 , 71 . 95 , 72 . 46 , 72 . 41 , 72 . 42 , 72 . 58 , 72 . 54 , 72 . 36 } . • for ogbn - proteins : – ECDSep : ∆ t = 1 . 8 , η = 5 , ν = 10 − 5 , w d = 0 , with results { 76 . 6 , 74 . 61 , 74 . 32 , 73 . 92 , 73 . 88 } . – SGD : α = 0 . 1 , β = 0 . 999 , w d = 10 − 5 , with results { 68 . 32 , 68 . 24 , 66 . 1 , 65 . 37 , 60 . 92 } . – AdamW : α = 0 . 01 , w d = 10 − 5 , with results { 78 . 09 , 77 . 86 , 77 . 81 , 77 . 66 , 77 . 53 } . – Adam : α = 0 . 01 , w d = 0 , with results { 77 . 9 , 77 . 87 , 77 . 7 , 77 . 52 , 77 . 44 } . These experiments were performed on a single NVIDIA Tesla T4 with 15GB of GPU memory and 12 . 7GB of RAM . More details can be found in the notebooks ECDSep _ graphs _ arxiv . ipynb and ECDSep _ graphs _ proteins . ipynb . B . 5 Language Here we report the HPs found with the scan discussed in § 4 . 4 that resulted in the best performance . The metric used to determine the performance is different for different task , as discussed in Table 4 . We also report here the full distribution of results over 3 initializations with different seeds . Recall that the learning rate ∆ t = 0 . 04 for all experiments with ECDSep . • For MNLI : – ECDSep : w d = 0 , η = 2 , ν = 10 − 4 , { 83 . 95 , 84 . 38 , 84 . 38 } – SGD : w d = 10 − 3 , α = 10 − 5 , β = 0 . 99 , { 83 . 53 , 83 . 52 , 83 . 28 } – AdamW : w d = 10 − 2 , α = 2 × 10 − 5 , { 84 . 56 , 84 . 3 , 84 . 36 } – Adam : w d = 0 , α = 2 × 10 − 5 , { 84 . 16 , 84 . 41 , 84 . 36 } • For QQP : – ECDSep : w d = 0 , η = 2 , ν = 10 − 4 , { 87 . 1 , 86 . 53 , 86 . 48 } – SGD : w d = 10 − 3 , α = 10 − 5 , β = 0 . 99 , { 86 . 11 , 86 . 02 , 85 . 63 } – AdamW : w d = 10 − 2 , α = 2 × 10 − 5 , { 88 . 31 , 88 . 19 , 88 . 12 } – Adam : w d = 0 , α = 2 × 10 − 5 , { 88 . 38 , 88 . 21 , 87 . 84 } 23 • For QNLI : – ECDSep : w d = 0 , η = 1 . 4 , ν = 10 − 5 , { 91 . 16 , 91 . 38 , 91 . 03 } – SGD : w d = 10 − 3 , α = 10 − 5 , β = 0 . 99 , { 91 . 25 , 91 . 16 , 90 . 77 } – AdamW : w d = 10 − 2 , α = 2 × 10 − 5 , { 91 . 62 , 91 . 43 , 91 . 43 } – Adam : w d = 0 , α = 2 × 10 − 5 , { 91 . 49 , 91 . 4 , 91 . 29 } • For SST - 2 : – ECDSep : w d = 0 , η = 1 , ν = 10 − 4 , { 93 , 92 . 78 , 92 . 2 } – SGD : w d = 10 − 3 , α = 10 − 5 , β = 0 . 99 , { 92 . 43 , 91 . 86 , 91 . 86 } – AdamW : w d = 0 , α = 2 × 10 − 5 , { 93 . 23 , 93 , 92 . 89 } – Adam : w d = 0 , α = 2 × 10 − 5 , { 93 . 00 , 92 . 78 , 92 . 66 } • For CoLA : – ECDSep : w d = 10 − 2 , η = 2 , ν = 10 − 5 , { 60 . 07 , 57 . 85 , 55 . 82 } – SGD : w d = 10 − 3 , α = 10 − 4 , β = 0 . 9 , { 58 . 8 , 58 . 11 , 54 . 96 } – AdamW : w d = 10 − 3 , α = 3 × 10 − 5 , { 59 . 87 , 59 . 78 , 59 . 38 } – Adam : w d = 10 − 5 , α = 2 × 10 − 5 , { 61 . 09 , 58 . 84 , 58 . 08 } • For STS - B : – ECDSep : w d = 10 − 2 , η = 2 , ν = 10 − 5 , { 89 . 3 , 89 . 27 , 89 . 21 } – SGD : w d = 10 − 2 , α = 10 − 5 , β = 0 . 99 , { 88 . 72 , 88 . 71 , 88 . 68 } – AdamW : w d = 0 , α = 3 × 10 − 5 , { 89 . 37 , 89 . 1 , 88 . 98 } – Adam : w d = 0 , α = 2 × 10 − 5 . { 89 . 33 , 89 . 09 , 88 . 66 } • For MRPC : – ECDSep : w d = 10 − 3 , η = 1 . 4 , ν = 10 − 5 , { 91 . 58 , 90 . 18 , 91 . 12 } – SGD : w d = 10 − 3 , α = 10 − 4 , β = 0 . 99 , { 89 . 66 , 88 . 19 , 88 . 07 } – AdamW : w d = 0 , α = 2 × 10 − 5 , { 91 . 87 , 91 . 07 , 90 . 46 } – Adam : w d = 10 − 2 , α = 2 × 10 − 5 , { 91 . 78 , 90 . 78 , 90 . 69 } • For RTE : – ECDSep : w d = 10 − 3 , η = 1 , ν = 10 − 5 , { 75 . 09 , 72 . 56 , 71 . 84 } – SGD : w d = 10 − 3 , α = 10 − 4 , β = 0 . 99 , { 71 . 12 , 69 . 68 , 69 . 31 } 24 – AdamW : w d = 10 − 2 , α = 3 × 10 − 5 , { 71 . 84 , 71 . 12 , 70 . 76 } – Adam : w d = 10 − 2 , α = 2 × 10 − 5 , { 72 . 2 , 71 . 84 , 70 . 04 } These experiments were conducted on a single NVIDIA - GEFORCE - RTX2080Ti with 11GB of GPU memory and 16GB RAM . More details can be found in the notebook ECDSep _ language _ bert . B . 6 Hyperparameter scaling An important goal for the new designed optimizer ECDSep is to understand how to scale the hyperparameters , in particular ∆ t , η and ν , with the problem size . In Table 5 , for each performed experiment we collect the values of the ECDSep HPs ∆ t , η and ν together with the number of parameters of the associated neural network . We stress that these are very different kind of problems , which include fine - tuning , weight averaging and small Graph Neural Networks . For these results we targeted test accuracy ( which as discussed in the main text is not the same as optimization performance per se , which also favors larger η e . g . in the image problems ) Table 5 : ECDSep HP values , number of parameters of the neural network and problem size for each experiment . For bert the HPs are averaged over all the GLUE datasets . ( FT ) after a dataset means that the goal was fine - tuning instead of training from scratch . Experiment Num . parameters Hyperparameters n ∆ t η ν ogbn - arxiv 200 K 2 . 8 4 . 5 10 − 5 ogbn - proteins 200 K 1 . 8 5 10 − 5 Tiny Imagenet 12 M 0 . 6 1 5 × 10 − 5 Imagenet - 1K ( FT ) 12 M 0 . 1 1 10 − 3 CIFAR100 56 M 0 . 4 1 5 × 10 − 5 bert ( FT ) 110 M 0 . 04 1 . 6 4 × 10 − 5 We note that ∆ t decreases with bigger neural networks . It is also important to note the ν √ n figure of merit determining the stepwise bounce angle as discussed in § A . 1 . Among the experiments here , we find some with small angle ν √ n ≪ 1 and others among the fine - tuning ones ( Imagenet - 1K and some of the BERT examples , those with ν ∼ 10 − 4 ) with ν √ n ≥ 1 , α ν ≃ π / 2 ( a large bounce each step ) . In these cases , such near - random - walk behavior proves competitive with traditional optimizers . This pattern deserves further study . 25 B . 7 CO2 Emissions Experiments in § 4 . 2 and § 4 . 4 used a private infrastructure with carbon efficiency ∼ 0 . 25 kgCO 2 eq / kWh with approximatively 10638 hrs computation ( of which 7092 for the experiments in the paper ) was performed on a GPU RTX 2080 Ti ( TDP of 250W ) . Total emissions for these are estimated to be 666 . 2 kgCO 2 eq . Experiments in § 4 . 3 used Google Cloud Platform in region us - west2 , which has a carbon efficiency of 0 . 24 kgCO 2 eq / kWh with approximatively 1020 hrs of computation ( of which 680 for the experiments in the paper ) performed on a GPU T4 ( TDP of 70W ) . Total emissions for these are estimated to be 17 . 14 kgCO 2 eq of which 100 % were directly offset by the provider . Estimations conducted using the impact calculator presented in [ 21 ] . References [ 1 ] D . Ackley . A Connectionist Machine for Genetic Hillclimbing . The Springer International Series in Engineering and Computer Science . Springer US , 2012 . [ 2 ] Takuya Akiba , Shotaro Sano , Toshihiko Yanase , Takeru Ohta , and Masanori Koyama . Optuna : A next - generation hyperparameter optimization framework . In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2019 . [ 3 ] Michael Betancourt . A Conceptual Introduction to Hamiltonian Monte Carlo . arXiv e - prints , page arXiv : 1701 . 02434 , January 2017 . [ 4 ] Michael M . Bronstein , Joan Bruna , Taco Cohen , and Petar Veličković . Geometric deep learning : Grids , groups , graphs , geodesics , and gauges , 2021 . [ 5 ] Xiangning Chen , Chen Liang , Da Huang , Esteban Real , Kaiyuan Wang , Yao Liu , Hieu Pham , Xuanyi Dong , Thang Luong , Cho - Jui Hsieh , Yifeng Lu , and Quoc V . Le . Symbolic discovery of optimization algorithms . ArXiv , abs / 2302 . 06675 , 2023 . [ 6 ] Giuseppe Bruno De Luca and Eva Silverstein . Born - infeld ( BI ) for AI : Energy - conserving descent ( ECD ) for optimization . In Kamalika Chaudhuri , Stefanie Jegelka , Le Song , Csaba Szepesvari , Gang Niu , and Sivan Sabato , editors , Pro - ceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 4918 – 4936 . PMLR , 17 – 23 Jul 2022 . [ 7 ] Jia Deng , Wei Dong , Richard Socher , Li - Jia Li , Kai Li , and Li Fei - Fei . Imagenet : A large - scale hierarchical image database . In 2009 IEEE conference on computer vision and pattern recognition , pages 248 – 255 . Ieee , 2009 . 26 [ 8 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . Bert : Pre - training of deep bidirectional transformers for language understanding . arXiv preprint arXiv : 1810 . 04805 , 2018 . [ 9 ] Gabriel Goh . Why momentum really works . Distill , 2017 . [ 10 ] Hao Guo , Jiyong Jin , and Bin Liu . Stochastic weight averaging revisited . arXiv e - prints , pages arXiv – 2201 , 2022 . [ 11 ] William L . Hamilton , Rex Ying , and Jure Leskovec . Inductive representation learning on large graphs . NIPS’17 , page 1025 – 1035 , Red Hook , NY , USA , 2017 . Curran Associates Inc . [ 12 ] Stephen Jose Hanson and Lorien Y . Pratt . Comparing biases for minimal network construction with back - propagation . In NIPS , 1988 . [ 13 ] Kaiming He , X . Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 770 – 778 , 2015 . [ 14 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . [ 15 ] Weihua Hu , Matthias Fey , Marinka Zitnik , Yuxiao Dong , Hongyu Ren , Bowen Liu , Michele Catasta , and Jure Leskovec . Open graph benchmark : Datasets for machine learning on graphs , 2020 . [ 16 ] Pavel Izmailov , Dmitrii Podoprikhin , Timur Garipov , Dmitry Vetrov , and Andrew Gordon Wilson . Averaging weights leads to wider optima and better generalization . In Ricardo Silva , Amir Globerson , and Amir Globerson , editors , 34th Conference on Uncertainty in Artificial Intelligence 2018 , UAI 2018 , 34th Conference on Uncertainty in Artificial Intelligence 2018 , UAI 2018 , pages 876 – 885 . Association For Uncertainty in Artificial Intelligence ( AUAI ) , 2018 . [ 17 ] Momin Jamil and Xin - She Yang . A literature survey of benchmark functions for global optimisation problems . International Journal of Mathematical Modelling and Numerical Optimisation , 4 ( 2 ) : 150 – 194 , 2013 . [ 18 ] Diederik P . Kingma and Jimmy Ba . Adam : A method for stochastic optimization . CoRR , abs / 1412 . 6980 , 2015 . [ 19 ] Thomas N . Kipf and Max Welling . Semi - supervised classification with graph convolutional networks , 2017 . 27 [ 20 ] Alex Krizhevsky . Learning multiple layers of features from tiny images . 2009 . [ 21 ] Alexandre Lacoste , Alexandra Luccioni , Victor Schmidt , and Thomas Dan - dres . Quantifying the carbon emissions of machine learning . arXiv preprint arXiv : 1910 . 09700 , 2019 . [ 22 ] Ya Le and Xuan S . Yang . Tiny imagenet visual recognition challenge . 2015 . [ 23 ] Ben Leimkuhler and Charles Matthews . Molecular dynamics . Interdisciplinary applied mathematics , 39 : 443 , 2015 . [ 24 ] Ilya Loshchilov and Frank Hutter . Decoupled weight decay regularization . In International Conference on Learning Representations , 2018 . [ 25 ] Ilya Loshchilov and Frank Hutter . Decoupled weight decay regularization , 2019 . [ 26 ] Daniel A . Roberts , Sho Yaida , and Boris Hanin . The Principles of Deep Learning Theory . 6 2021 . [ 27 ] Jakob Robnik , G . Bruno De Luca , Eva Silverstein , and Uroš Seljak . Microcanonical Hamiltonian Monte Carlo . 12 2022 . [ 28 ] Greg Ver Steeg and Aram Galstyan . Hamiltonian dynamics with non - newtonian momentum for rapid sampling . CoRR , abs / 2111 . 02434 , 2021 . [ 29 ] Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the im - portance of initialization and momentum in deep learning . In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 , ICML’13 , page III – 1139 – III – 1147 . JMLR . org , 2013 . [ 30 ] Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R Bowman . Glue : A multi - task benchmark and analysis platform for natural language understanding . arXiv preprint arXiv : 1804 . 07461 , 2018 . [ 31 ] Kuansan Wang , Zhihong Shen , Chiyuan Huang , Chieh - Han Wu , Yuxiao Dong , and Anshul Kanakia . Microsoft Academic Graph : When experts are not enough . Quantitative Science Studies , 1 ( 1 ) : 396 – 413 , 02 2020 . [ 32 ] Greg Yang , Edward J . Hu , Igor Babuschkin , Szymon Sidor , Xiaodong Liu , David Farhi , Nick Ryder , Jakub Pachocki , Weizhu Chen , and Jianfeng Gao . Tensor Programs V : Tuning Large Neural Networks via Zero - Shot Hyperparameter Transfer . arXiv e - prints , page arXiv : 2203 . 03466 , March 2022 . [ 33 ] Sergey Zagoruyko and Nikos Komodakis . Wide residual networks . ArXiv , abs / 1605 . 07146 , 2016 . 28