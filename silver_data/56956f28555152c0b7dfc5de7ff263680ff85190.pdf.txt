Challenges for Cognition in Intelligence Analysis Stoney A . Trent Department of Behavioral Sciences and Leadership United States Military Academy Emily S . Patterson , David D . Woods Cognitive Systems Engineering Laboratory Institute for Ergonomics The Ohio State University A BSTRACT : Intelligence analysis is a high - stakes domain that poses challenges to effective individual and collaborative cognition . The design of support tools and analytical ped - agogy could benefit from an understanding of how challenges that are reported in other decision - making literature generalize and are manifested in more naturalistic settings . The objective of this research was to elicit challenges for cognition in collaborative intel - ligence analysis . Two complementary research methods were used : unstructured inter - views with 46 analysts and supervisors , and observations of eight teams of military intelligence analysts conducting a training scenario . Interviews with designers , educa - tors , and practitioners in the intelligence community revealed trends in unsupported cognitive work and cultural challenges , whereas observations from a training exercise for army intelligence analysts instantiated other cognitive challenges of collaborative analysis . This study indicates that analytical style ( part tradition and part due to indivi - dual reasoning tendencies ) can result in premature narrowing , difficulty in reframing , and getting lost in the details . The study also illustrates the effects of friction within and across federated teams , how variable tempo can produce inexpert behavior , and con - siderations for the design of analytical support tools . This work suggests the value of complementary research methods in the study of other domains involving collabora - tive work . It is likely that these cognitive challenges affect other domains involving collaborative analysis . Finally , this study suggests that the effects of individual cogni - tive challenges are difficult to isolate in naturalistic settings and should most likely be considered collectively rather than independently . Know the enemy and know yourself ; in a hundred battles you will never be in peril . – Sun Tzu I NTELLIGENCE ANALYSIS IS A HIGH - STAKES DOMAIN THAT POSES CHALLENGES TO INDIVIDUAL ( Heuer , 1999 ) and collaborative ( Johnston , 2005 ) cognition . Analysts must find and assemble data on an intelligent adversary in order to present a coherent reflection of reality for a decision maker . Intelligence data are cluttered with nondiagnostic information , and the cognitive work is environmentally constrained by factors such as time , weather , data overload , and the need to work in distributed teams . In col - lecting and analyzing information , analysts are hampered by brittle collection assets 75 ADDRESS CORRESPONDENCE TO : Stoney A . Trent , Department of Behavioral Sciences and Leadership , United States Military Academy , 276C Thayer Hall , West Point , NY 10996 , stoney . trent @ us . army . mil . Visit the JCEDM Online Companion at http : / / cedm . webexone . com . Journal of Cognitive Engineering and Decision Making , Volume 1 , Number 1 , Winter 2007 , pp . 75 – 97 ©2007 Human Factors and Ergonomics Society . All rights reserved . Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 75 and analytical support tools of questionable usability and usefulness . These tools , including some new ones that are being developed , often actually add to the work of the analyst because they were not designed to adequately support these experts’ work in the first place . The Army’s All - Source Analysis System ( ASAS ) required a team of four enlisted analysts to perform database management and plot data for analysis by a senior ana - lyst . Ultimately , both versions of ASAS went unused during the 1991 and 2003 U . S . invasions of Iraq . The requirements for its replacement system were generated not through a task analysis of users but , rather , through a survey of functions resident in ASAS and seven other intelligence support tools ( Operational Requirements Docu - ment for Distributed Common Ground System - Army , 2004 ) . In order to facilitate effec - tive analysis , stakeholders in the intelligence community and other similar domains must understand its cognitive challenges . Furthermore , the challenges should be ad - dressed through a comprehensive program of education , continual learning , and tool development based on the empirical study of usefulness and usability issues . The cognitive work of intelligence analysts has not been studied as much as one might suppose . In part this is due to the complex nature of intelligence analysis and the wide variability in tasks throughout the intelligence community . Indeed , though reference is made to jobs such as “imagery analyst” or “all - source analyst , ” there are dozens of distinct analytical roles within various intelligence agencies . Additional fieldwork and targeted empirical investigations are needed to assess the critical needs and vulnerabilities prior to implementation of solutions . To this end , the present study relied on two complementary investigative strategies : open - ended interviews with practitioners from several intelligence organizations , and observations of ana - lysts in a military intelligence training exercise . Our work benefited from the domain experience of the first author , who is an Army Military Intelligence officer with six years of experience in military intelligence and experience in two hostile fire theaters . The purpose of this study is to review challenges noted in other studies on ana - lytical work and to determine if those same or other challenges also appear in the domain of military intelligence analysis . We begin our investigation by describing the work of information analysis in general and then relate it to the work of mili - tary intelligence analysis in particular . Description of the Domain Information Analysis Information analysis involves making inferences from available data . The ana - lyst determines the best explanation for uncertain , contradictory , and / or incomplete data ( Heuer , 1999 ; Patterson , Roth , & Woods , 2001 ) . Analysts must cope with the complexity of cluttered data sets and often work on only pieces of a larger problem . This necessitates synthesis of both hypotheses and assessments in order to provide a single coherent product for a decision maker or policy maker . Analytical problems are often sensitive and associated with high stakes for suc - cess or failure . In many analytical subdomains , the objectives of the analysis can be 76 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 76 open and shifting , and analysts must sometimes determine for themselves the goals of their collection or research . Furthermore , the analyst’s work extends beyond sim - ple problem solving . In addition to Elm’s ( Elm et al . , 2005 ) three analytical cognitive tasks of information search ( Down Collect ) , verifying information accuracy ( Con - flict and Corroboration ) , and considering multiple explanations for data ( Hypothe - sis Exploration ) , most analytical domains require additional cognitive work with regard to organizational needs , policies , and constraints , including framing assess - ments for communication to a decision maker or other analysts . Many decisions and assessments are collaborative efforts . Analysts rely on the input of those around them as well as of distant teams of other specialists who have their own sets of potentially unobservable goals and perspectives . Military intelligence in particular is a unique analytical subdomain that exhibits these characteristics . “The most important roles of intelligence are assisting [ commanders ] and their staffs in visualizing the battlespace , assessing adversary capabilities and will , identifying the adversary’s [ strengths and weaknesses ] , and discerning the adversary’s probable intent” ( Doctrine for Intelligence Support to Joint Operations , 2000 , p . I - 1 ) . The military intelligence community consists of multiple echelons of staff and agencies that are responsible for providing intelligence support to their command - ers . Each echelon has its own collection assets and analysts who are responsible for the identification of possible threat courses of action , relative strengths and weakness - es of the adversary , and areas of uncertainty within these assessments . Thus , mil - itary intelligence analysis is a federated process . Just as a federation is a group of independent entities aligned to achieve a common goal , teams of analysts who are collaborating at a distance are federated . They may have centralized direction and a common goal , but they also maintain their own sets of goals and are capable of only periodic physical interaction . Intelligence analysis has been characterized as a supervisory control situation ( Patterson , Rogers , & Render , 2004 ) . Each echelon receives orders from the higher organization , sends requests for information ( RFIs ) or collection , reports to high - er headquarters , communicates with adjacent units , and issues its own orders to sub - ordinates . In effect , each team of analysts serves as a supervisory agent or as an agent that must also supervise other distant agents . Because of the distance ( which can be geographic or organizational ) among these multiple levels of supervisory agents , each echelon relies on the feedback of others to accurately understand its own area of responsibility in context . Intelligence analysis is an action - driven feedback loop . Actions of both friendly and potentially deceptive threat elements cause changes that are assessed by feedback ( i . e . , reports ) . Joint intelligence doctrine states that the intelligence process is a non - sequential iterative process that includes planning , preparation , collection , processing , and production ( Doctrine for Intelligence Support to Joint Operations , 2000 ) . The ana - lyst is never responsible for assessing a static situation . The pace of both the threat and the feedback are in constant flux . Challenges for Cognition in Intelligence Analysis 77 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 77 Knowledge Elicitation Method In order to identify the constraints and perceived challenges facing practitioners , instructors , and designers of intelligence support tools , we conducted unstructured interviews with analysts and supervisors in a number of organizations . Each inter - view lasted from 30 to 60 minutes , depending on the time the participant had to offer . Table 1 provides the breakdown of the 46 volunteers by rank and organization . In order to comply with security regulations and facilitate rapport with the partici - pants , all notes were taken by hand , and no recording devices were used . Findings Four trends were identified from the interviews : environmental pressure , learn - ing , sustained attention , and experience viewed as expertise all influence intelligence analysis . Environmental Pressure . All 12 analysts with experience in hostile fire theaters re - lated stories of data overload , weather , and austere field environments hampering 78 Journal of Cognitive Engineering and Decision Making / Spring 2007 TABLE 1 . Demographics of interviewees System Development Instructors and Operations / Analysis and Reorganization Training Development Participants : 22 Participants : 7 Participants : 17 Experience in Tactical or Strategic Responsible for developing Current and past instructors Intelligence from either deployed requirements documents and and training developers or sanctuary locations supervising development of systems and personnel manning Organizations Organizations Organizations Analysis and Control Elements Advanced Information U . S . Army Intelligence Center – for U . S . Army Europe , V Corps , Processing Office at National Ft . Huachuca , AZ 1st Armored Division Ground Intelligence Center , Charlottesville , VA DIA , NSA Program Manager Office for Sensor Processing Other tactical Army units Program Manager Office for Ground Sensors Army G - 2 , Pentagon Ranks Ranks Ranks Lieutenant Colonel ( 2 ) , Major ( 5 ) , Lieutenant Colonel ( 1 ) , Lieutenant Colonel ( 2 ) , Captain ( 4 ) , Warrant Officer ( 4 ) , Captain ( 1 ) , Warrant Major ( 4 ) , Captain ( 3 ) , Civilian ( 4 ) , Non - Commissioned Officer ( 1 ) , Civilian ( 4 ) Warrant Officer ( 2 ) , Officer ( 3 ) Civilian ( 6 ) Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 78 their ability to focus on analytical tasks . In a typical deployed Tactical Operations Center , analytical work groups are often crowded into confined trailers or tents , with the potential that products or tools will be destroyed by sand , mud , or precipitation . The most frequently reported environmental pressure was time . Two analysts’ stories indicated that these environmental pressures aggravated other vulnerabilities , such as trust in their systems or adjacent units . Three analysts reported that they were able to adapt relatively quickly to environmental pressures , however , and seemed to notice the effects of these pressures only when a change in the environment occurred . Several analysts’ reports seemed to support Huey and Wickens’s ( 1993 ) findings that this adaptation generally takes the form of teams shedding , delaying , or performing tasks in a suboptimal manner . Learning . Interviews revealed broad consensus that few practicing analysts were well skilled in their particular assigned task , which likely influenced performance and analytical outcomes . Much of the work that an analyst must perform is learning about the environment , teammates , and area of responsibility . Instructors at the U . S . Army Intelligence Center reported that the average education of entry - level Army analysts is one to two years of college education . In the military setting , an addition - al challenge is that , according to Army authorizations , nearly 50 % of all noncivilian analysts have fewer than three years of experience in a particular duty assignment . This finding represents a significant challenge , given that intelligence analysis has long been recognized as a complex domain in which it takes years to acquire expert - ise ( Feltovich , Ford , & Hoffman , 1997 ; Johnston , 2005 ) . Sustained Attention . Experienced analysts reported that a large proportion of junior analysts demonstrated poor motivation for conducting research and spent extended periods focused on preparing reports . Practitioners described undermotivated and underemployed analysts on missions in Kosovo and Iraq . They reported that the mo - tivation level or willingness of these junior analysts to sustain attention on the tasks at hand varied with threat activity throughout the mission . Units nearing redeploy - ment were judged as having stopped improving their intelligence products . Similar - ly , experienced analysts reported that the ability of junior analysts to focus was hampered by competing goals or tasks , particularly when working on multiple , ex - tended problems at the same time . These findings resonate with findings in the literature on sustained attention ( vigilance ) , learning , and motivation . Sustained attention refers to one’s ability to focus on a given task for prolonged periods ( Warm , Dember , & Hancock , 1996 ) . Re - search has shown significant effects of sustained attention ( Davies & Parasuraman , 1982 ) and motivation ( Frederick - Recascino & Hall , 2003 ) on job performance . Similarly , “experts in the work practice , by nature of their involvement with their work , have an intrinsic motivation that learners lack” ( Quintana , Krajick , & Soloway , 2003 , p . 826 ) . Similar observations have been made by others ( Hoffman , 1998 ) . Motivation was studied in two intifadas on the Israeli Defense Force and found to be a product of several elements : esprit de corps or unit cohesion , training , institutional Challenges for Cognition in Intelligence Analysis 79 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 79 value system , trust , leader competency , communication , personal uncertainty , and sense of achievement ( Catignani , 2004 ) . Experience Viewed as Expertise . A final trend obtained from analyst interviews was that commanders and supervisors were perceived to occasionally overestimate the scope of expertise of an experienced analyst . Four experienced analysts reported be - ing regarded as subject matter experts in areas with which they had little familiarity . One analyst indicated that she felt unable to offer an alternative analysis in the pres - ence of higher - ranking analysts , regardless of her expertise . In the Army , the rapid turnover of personnel reduces supervisors’ ability to gain insight into the scope of ex - pertise in recently formed analyst teams that make high - risk recommendations , which could potentially address this challenge . Interviewees were encouraged to identify the greatest challenges to conducting successful intelligence analysis in the field . In order to avoid missing key challenges , we encouraged discussion that was free - flowing and completely unstructured . It is not surprising that this methodology tended to elicit strategic or global challenges rather than specific examples of deficient analysis at a detailed level ( although it did elicit some of those ) , particularly given that detecting erroneous assessments would be difficult due to a lack of feedback on a regular basis . Consequently , a direct obser - vational study of analysis in a training exercise was conducted to further investigate challenges in collaborative analysis . Observational Study Method Based upon the interviews and a literature review of decision making and prob - lem solving , we hypothesized a total of seven challenges in conducting collaborative intelligence analysis prior to the observational study . These challenges are described in Table 2 . A four - day military intelligence training exercise that was conducted at Fort Huachuca in Arizona provided the opportunity to observe eight analytical teams . The teams were concurrently working on a simulated complex problem representative of the type of work conducted in operational Army units . Instructors provided real - time judgments about erroneous assumptions and assessments . One benefit of observing teams at work is that the work is distributed among many people . Individuals must interact and express their thought processes to each other . This allows the observer to infer cognitive processes with minimal interaction ( Woods & Hollnagel , 2006 ) . Participants . The participants in the observational study included a class of 40 recent - ly commissioned lieutenants and 16 sergeants first class . All the lieutenants possessed a bachelor’s degree and had nearly completed their 18 - week initial analyst training or Military Intelligence Officers Basic Course . As such , they would still be classified as trainees or junior journeymen . All the sergeants were senior intelligence sergeants 80 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 80 with approximately 10 years of experience in various military intelligence specialties ( e . g . , signal or imagery intelligence ) who were participating in the Advanced Non - Commissioned Officers Course . Although these sergeants had experience in their specialty areas , they did not necessarily have experience working on the type of prob - lems posed by this scenario . Therefore , the sources of difficulty in this exercise dif - fered from those in their usual tasks . The lieutenants and sergeants were divided into eight teams ( five lieutenants and two sergeants per team ) by the instructors and assigned to work in one of three Battalion Tactical Operations Centers ( TOCs ) , or the Brigade TOC . In each unit TOC , two teams worked a 12 - hour day or night shift throughout the exercise . Study Setting . Each team was assigned to a simulated TOC . Each team was provid - ed with an instructor to coach them through their training exercise . Instructors were retired military civilian contractors with at least 10 years of experience in intelligence fields . Because these retired contractors’ primary job was to conduct this training , they had intimate knowledge of the scenario and possible points of difficulty for the teams . Instructors interacted with the teams to facilitate learning objectives and pro - vided prompts when the teams appeared to be deviating too far from the objectives of the exercise . All observations for the eight teams were conducted by the first author . Having Challenges for Cognition in Intelligence Analysis 81 TABLE 2 . Challenges to military intelligence analysts Challenges found in the literature • Mental Set – use of problem - solving strategies based on past experience • Fixation – difficulty in changing , revising , or replanning in the face of new disconfirming evidence • Recognition of Relevant Data – difficulty recognizing data relevant to their assessment from a vast array of possible data that includes confusing and non - diagnostic information • Environmental Pressure ( Time ) – pressure to meet production deadlines • Trust – attitude that an agent will help achieve an individual’s goals in a situation characterized by uncertainty and vulnerability Challenges revealed in the interviews • Environmental pressure a . Time pressure to meet production deadlines b . Other factors of the operating environment such as weather • Sustained attention a . Motivational issues related to simulation fidelity b . Motivational issues related to under - skilled or disinterested junior analysts c . Motivational issues related to inactivity or extension of mission • Learninga . Understanding of collection systems , automated support tools , and manual analytic tools such as event maps b . Learning other than tool understanding • Experience viewed as expertise – Mistaking practitioner with domain experience as having expertise relevant to the particular problem at hand Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 81 a single observer divide his attention among all teams was both good and bad . The observer was able to establish a common rapport and observational standard with all the teams . The observer was also experienced in the domain , which reduced the interactions necessary to understand the mindset and processes of the participants . However , it was not possible to make concurrent observations for specific processes for all teams . The instructors were sensitized to the objectives of the study and thus helped to direct the observer’s attention to relevant observations . Prior to the exercise , all analysts were given a 20 - minute “Road - to - War” brief - ing , which provided a general orientation for the scenario’s geography , history , and demographics . Each team was then provided an operations order and threat assess - ment on its area of operations , friendly and enemy forces in the area , and other demographic details of varying importance . Teams had three hours to review these products , and then they conducted verbal “back - briefs” to their instructors to ensure accurate baseline understanding of the general situation . These back - briefs were in the same format as the teams’ required daily briefings and were similar to back - briefs conducted in other training and military operations . Teams were provided a personality database as a starting point for identifying any associations between identified insurgents . This database contained biographi - cal data such as names , nicknames , associations , and other details as they were known for significant or suspicious individuals . Similar to real personality databases , it con - tained errors that included misspelled names and irrelevant information . During the exercise , teams received simulated reports ( approximately 40 mes - sages per 12 - hour shift ) via All - Source Analysis System - Light ( ASAS - L ) , a Microsoft Windows – based analytical support tool that teams would find in their follow - on ( real ) assignments . To operate this workstation , each team was aided by an enlisted soldier who was trained on the system . These operators had supported this partic - ular training exercise many times before . They were instructed to offer no analytical assistance and to perform only those functions on the automation that were request - ed by the teams . Other support tools available for use included Crimelink ( a link analysis software application ) , standard Windows applications , dry - erase boards , and butcher paper on an easel . Reporting consisted of scripted spot reports and human - intelligence reporting that replicated reports from friendly units . Instructors used a general storyboard to retain continuity across all teams . ( This storyboard will be discussed in the next sec - tion . ) Similar activity occurred in each battalion’s area of operations , but the timing for the reports was controlled by the instructor within the bounds of the storyboard and based on the actions of each of the teams . Each team was responsible for analyz - ing its messages and producing a daily intelligence assessment and briefing for the instructor who played the commander role . Many reports were based on actual events that had happened on past deploy - ments and were included to teach specific lessons . These messages served as pre - scripted event probes that would potentially illuminate various vulnerabilities . Event probes that allow for unambiguous judgments of performance provide a useful struc - ture for the analysis of behavior in complex settings ( Patterson et al . , 2004 ) . 82 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 82 Exercise Scenario . In the hypothetical scenario , U . S . forces had recently completed a high - intensity conflict to secure a country that had been invaded by a neighboring regional power . The U . S . forces were now transitioning to a stability and support operation with the objective of identifying and eliminating threats to safety and secu - rity . Throughout this scenario , action cells of a larger insurgency movement were conducting a wide range of activities ( e . g . , attacks on a strategic fuel pipeline , assas - sination of local leaders , recruiting and logistics , and attacks on U . S . forces ) that were designed to destabilize the region . Concurrently , groups of local civilians and organ - ized criminals were also attempting to advance their own causes , and a failing infra - structure and fledgling government further complicated the operation . Table 3 provides a timeline of the most notable events . To support friendly oper - ations , each team had the implied task of solving three simultaneous analytical prob - lems : identify insurgent activity patterns , identify group structures , and identify operational methods , such as communication , logistics , and planning . Findings The occurrence of many of the hypothesized challenges ( see Table 2 ) was inferred by noting the misinterpretations of available data . Aided by the instructors’ knowl - edge of the scenario , normative interpretations for individual reports were compared with the actual interpretations made by the teams in their briefs to the instructors . Because the study was concerned with collaborative analysis , the following heuristic was used to distinguish between overlooking and discounting of hypotheses or infor - mation : If the team did not discuss a hypothesis or piece of information , it was clas - sified as being overlooked . If the team discussed it but subsequently ruled it out , it was classified as being discounted . Each of the observed challenges will be discussed . Mental Set Mental set is the use of ( sometimes inappropriate ) problem - solving strategies based on past experiences . This vulnerability can present problems when an alter - nate , better , and sometimes simpler strategy would suffice ( Heuer , 1999 ; Luchins , 1942 ) . In complex situations , this vulnerability could also cause an analyst or team of analysts to impose unnecessary constraints on the subject of their analysis . The following types of observations were attributed to mental set : • Team uses analytical assumptions or processes that are not relevant under the current conditions , • Team discounts hypotheses because it views them as impossible despite having no credible information to support this assumption . Three examples of this vulnerability were observed during this study and are described in Table 4 . On Day Two , the instructors judged that one team made two premature and incorrect assessments . The first was that the insurgent groups were being funded primarily through the sale of drugs within and outside the team’s area . This was based on assumptions that illegal black market activity is an integral part Challenges for Cognition in Intelligence Analysis 83 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 83 of the insurgent threat – a situation that has been true in other regional insurgencies . In this scenario , however , no reporting had supported this hypothesis . Additionally , because no electronic communications had been intercepted between insurgent groups , this same team assessed that the primary means of com - munication was physical meetings . The team prematurely discounted the possibil - ity of poor collection planning and execution because of its preexisting belief that the orientation of its collection systems was adequate for detecting all possible insur - gent electronic communications . Furthermore , this team overlooked the possibility 84 Journal of Cognitive Engineering and Decision Making / Spring 2007 TABLE 3 . Storyboard for the exercise scenario Day 1 – 2 Collection assets are initially misplaced or misoriented to collect on relevant targets Rally planned at a local university by an anti - U . S . professor Threat groups use Improvised Explosive Devices ( IEDs ) in certain areas of each Battalion Area of Operation ( AO ) Patrol finds grave site for 10 people that were executed Some terrorists are identified and captured Local political party HQ destroyed Insurgents attack U . S . patrol bases with mortars Day 3 Reports of logistics coordinator being spotted in two different areas New threat organization appears in the Brigade AO , and reports of possible terrorist training camps in each Battalion AO Two bomb makers are captured Security patrol for local official is ambushed Cooperative local mayor is killed Insurgents attack local militia unit Foreign fighters discovered A helicopter crashes in the AO , requiring security and evacuation Insurgents attack U . S . patrol base for second day with mortars Day 4 Reports of planning for bombing attacks after prominent cleric issues fatwah , or religious edict Reports indicate large weapons shipment from outside of the AO 22 civilians die as a result of water pollution incident Report of boat being stolen and report of planned boat attack on port Throughout the week Personality database lists misleading biographical data Strategic fuel pipeline attacked five times Significant black market activity and major fuel shortage Teams must adhere to an established daily production schedule : 0500 / 1700 – Battlefield update assessment 0700 / 1900 – Shift change 1000 / 2200 – Commanders update brief Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 84 that runners could have been delivering hand - carried messages as a means of com - munication . On Day Three , a discussion within one of the teams revealed this vulnerability . While considering the possibility that the same insurgents had been reported in mul - tiple areas , one person asserted , “These guys can’t travel that far [ approximately 25 km ] ” and then followed with “ [ They ] wouldn’t be operating in two areas . ” In addi - tion , the team discounted the accuracy of subordinate unit reports , stating , “Battalions sometimes assume that anyone that is captured is a terrorist . ” Thus , the team dis - counted the hypothesis that some of the reported insurgents were operating in mul - tiple areas and possibly coordinating actions between groups . Fixation Fixation refers to the difficulty in changing , revising , or replanning in the face of new disconfirming evidence ( De Keyser & Woods , 1990 ; Woods , 1994 ) . Earlier research in decision making has suggested that this results from a confirmation bias ( Einhorn & Hogarth , 1978 ; Lord , Lepper , & Ross , 1979 ) . Klayman and Ha ( 1987 ) suggested that fixation can result from a “positive test strategy , ” which is usually effec - tive in coping with data overload challenges but sometimes can lead to inaccurate conclusions . Feltovich ( Feltovich , Coulsen , Spiro , & Adami , 1993 ) referred to var - ious mental maneuvers to resist reframing as knowledge shields . We refer to the pro - cessing strategy as a mental set , whereas the inability to reframe is a Fixation . Any instance in which teams did not consider revising their hypotheses given discon - firming evidence was attributed to Fixation . Four observations were attributed to Fixation in this study . These are present - ed in Table 5 . One team assessed an insurgent to be a group leader because he had claimed to be a former lieutenant . This assessment was made based on a preexisting perception of the role of military rank . Subsequent reports showed the “lieutenant” taking overt action to denounce and inhibit U . S . operations in the area . This behavior Challenges for Cognition in Intelligence Analysis 85 TABLE 4 . Examples of mental set Mental Set Day Reports Normative Interpretation Misinterpretation 1 – 4 Significant black market Possible link between criminal Illegal activity = funding activity and major fuel groups and insurgent groups for insurgent activity shortage 2 – 3 No electronic Consider other possible Assume only possible communications methods ( physical meetings , method is physical intercepted between runners ) or poor collection meetings insurgent groups 3 Reports of logistics Possible coordination between “No way these guys can coordinator being groups via logistics officer travel that far” ; assume spotted in two insurgent groups are different areas independent Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 85 is atypical of an insurgent leader because it draws undue attention and encourages detention . Despite this conflicting reporting and prompting from an instructor , the team stuck to its assessment that the individual was an insurgent leader . During this same time frame , this team also misclassified an individual as an operations officer in the team’s area based on his education level . In doing so , they overlooked biographical data that indicated that he was a direct action member in another battalion’s area . This particular misclassification also persisted for more than 12 hours , even after the instructor asked the team to review the available informa - tion concerning this individual . The third indication of this challenge occurred when two reports indicated sig - nificant intergroup logistical coordination . A known enemy logistics officer was spot - ted in two different insurgent group areas , and subsequently , a report indicated that a large shipment of weapons from outside the country was arriving for the insurgents . Other reports later corroborated the coordination hypothesis . A religious edict , or fat - wah , was issued by a regional cleric calling for attacks on U . S . forces . The report indi - cated that a boat was stolen in one area for use in an attack in another . Some teams disregarded these reports , choosing to retain the hypothesis given to them at the beginning of the exercise that claimed the groups were small and independent of each other . Recognition of Relevant Data Given a vast array of data that contains some confusing and nondiagnostic information , analysts can experience difficulty recognizing data that are relevant to their assessments . Other research has indicated that this situation can lead individ - uals to use irrelevant information to solve problems ( Dominowski & Bourne , 1994 ) . The following two types of observations were attributed to difficulty recog - nizing relevant data : • Team uses nondiagnostic data to support a hypothesis . 86 Journal of Cognitive Engineering and Decision Making / Spring 2007 TABLE 5 . Examples of fixation Fixation Day Reports Normative Interpretation Misinterpretation 2 – 4 Personality database Use data and reports to Continue to miscategorize lists misleading generate , confirm , or deny personalities despite new biographical data hypotheses contradictory reports 3 Reports of logistics Possible coordination between Continue to assume coordinator being groups via logistics officer insurgent groups are spotted in two independent different areas 4 Reports of planning for Possible outside influence on Continue to assess that bombing attacks after insurgent groups ; assess possible insurgent groups are prominent cleric issues increase in insurgent activity acting without outside fatwah , or religious edict resources Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 86 • Team does not use diagnostic data to support or reject a hypothesis . Four instances of this vulnerability were observed in this study ( Table 6 ) . Two teams overlooked available biographical data in favor of other information , which led to erroneous personality assessments . For instance , an anti - U . S . professor organized a rally that was advertised as pro - U . S . Overlooking the organizer’s biographical data in favor of the advertisement flyer , the team assessed the rally as a nonthreatening , pro - U . S . demonstration . Conversely , two teams prematurely selected a favored hypothesis as a result of irrelevant information in the personality database . One team assessed a man to be a leader because he referred to himself as a lieutenant . Another team assessed a man to be an operations planner based solely on his college education . These two observa - tions could also be attributed to a mental set . Therefore , it is possible that some chal - lenges could be multiply classified , or else that there might be interactions between the hypothesized challenges , possibly indicating a need to address challenges col - lectively rather than independently . On Days Two and Three , insurgents fired mortars at a U . S . patrol base . Critical information , such as size of mortar and direction of fire , was missing from the report and could have been requested by the teams but was not . This information could have been used to determine possible firing positions for the attacker in order to prevent future attacks . Analysis of both attacks could have determined whether it was the same group ( or even the same weapon ) . This would have been helpful in establish - ing the capabilities of the insurgents and even the number of teams that were active in the area . However , these two attacks were used only as data points similar to all other attacks during the four days , and no further analysis was done . Challenges for Cognition in Intelligence Analysis 87 TABLE 6 . Examples of the difficulty of recognizing relevant data Recognition of Relevant Data Day Reports Normative Interpretation Misinterpretation 1 – 2 Flyer advertises pro - U . S . Use personality database to Overlook anti - U . S . rally at a local university identify organizer as anti - U . S . professor and assess rally by an anti - U . S . professor as pro - U . S . 2 – 4 Personality database lists Use data and reports to Categorize insurgents misleading biographical generate , confirm , or deny solely based on database data hypotheses while ignoring other sources 2 – 3 Insurgents attack U . S . Request further information Do not request further patrol bases with about size of mortar and information and assess mortars possible firing positions capabilities on incomplete data 4 22 civilians die as a Broaden assessment of possible Overlook possibility of result of water pollution methods of attack insurgent bio - attack incident Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 87 Finally , on Day Four , 22 civilians died in a local village because of water pollu - tion . This could have indicated a change in tactics or capabilities of the insurgents if they had intentionally contaminated the water supply . Two teams overlooked this report completely , regarding it as insignificant local news . Environmental Pressure The only environmental pressure with which participants in this study had to contend was time . The effects of time pressure appeared for all teams during the first two days . As teams approached production deadlines , they resorted to last - minute cutting and pasting of reports into their assessments and provided little analysis . One team coped with time pressure by having all but one person prepare the slides for the daily brief , ignoring all other products or analysis . By Day Three , most teams had adapted to the production schedule , but one team experienced a spike in enemy activity , which caused the team to exclaim that it was “getting too much [ informa - tion ] to handle . ” One team member openly complained that there was “no time and not enough computers for everyone to work on . ” Observations and interviews sug - gest that the timeline of events and activities can move beyond a guideline and plan - ning tool . In effect , the product can drive the assessment , rather than vice versa . Trust “Trust can be defined as the attitude that an agent will help achieve an individ - ual’s goals in a situation characterized by uncertainty and vulnerability” ( Lee & See , 2004 , p . 50 ) . In this study , friction arose between and within teams from unrealistic taskings , miscommunications , and poor feedback on requests for information . Be - cause of unplanned technical difficulties , electronic mail was unreliable at the begin - ning of the exercise and contributed to the general frustration between the teams when reporting deadlines were missed . Although this friction was reduced as teams found various coping strategies , poor communication capability among teams con - tinued to cause inefficiencies and confusion . On Day Four , members of one Battalion TOC finally complained , “Brigade is withholding information from us ! ” This friction eroded trust and ultimately affected analytical assessments . Tool Understanding There can be a direct relationship between errant mental models and decreased human - machine performance ( Dekker , 2002 ; Woods , 2005 ) . Researchers have noted that the accuracy of a worker’s mental model of his / her tools correlates to the effica - cy of the total human - machine system ( Hollnagel & Woods , 2005 ; Norman , 1990 ) . Intelligence analysts have three classes of tools on which they rely : collection systems , automated support tools , and manual analytical tools such as event maps , time - wheels , and link diagrams . For the present study , any instances in which tools were misused or were judged by the instructors to be a primary contributor to a deficient analysis were attributed to a poor understanding of the tool . Two examples of poor tool understanding were observed during this study and are summarized in Table 7 . When the scenario began , the collection assets for the 88 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 88 brigade had already been tasked to collect data in certain areas . A review of this col - lection plan should have identified gaps in collection on key targets . Initially , teams were too busy reviewing reports to make adjustments to the plan . When they began to relocate their assets , all teams exhibited poor understanding of the capabilities and limitations of these collection systems . Three teams were tasked with assets to collect in areas for which they were unsuited , and two teams did not acknowledge the need to move any of their assets . It is noteworthy that none of these collection systems had any feedback mechanism to inform the analysts about improper use . On Day Three , the instructors intervened with corrective instruction on the appro - priate use of these systems . Poor understanding of an automated intelligence support tool was inferred from the interaction between the supervisors and the computer operators . For all teams , the supervisory role for this system was reduced to two typical questions of the operator : “Are you getting your stuff ? ” and “Is everything working ? ” It was clear that in the event of a system error , such as corrupt or missing data , the supervisors would be underskilled for the task of problem identification or intervention . Discussion To elicit a broad set of challenges for cognition in collaborative intelligence analy - sis , we used two complementary methods : unstructured interviews with 46 ana - lysts and supervisors , and observations of eight teams of military intelligence analysts conducting a training scenario . Interviews revealed four types of challenges : environmental pressure , learning , sustained attention , and experience viewed as ex - pertise . Observations provided a “proof of concept” for all the hypothesized chal - lenges : mental set ( three examples ) , fixation ( four examples ) , recognition of relevant data ( four examples ) , environmental ( time ) pressure ( observed and self - reported influence throughout the simulation ; six examples ) , trust ( observed and self - report - ed influence throughout the simulation ; three examples ) , and tool understanding ( two examples ) . As mentioned earlier , some challenges , such as mental set and re - cognition of relevant data , could be multiply classified . This suggests interactions Challenges for Cognition in Intelligence Analysis 89 TABLE 7 . Examples of poor tool understanding Poor Tool Understanding Day Reports Normative Interpretation Misinterpretation 1 – 2 Collection assets are Identify existing gaps in Produce no new initially misplaced or collection plan ; adjust assets collection plan ; produce misoriented to collect to focus on new targets flawed collection plan on relevant targets 2 – 3 No electronic Consider other possible Assume collection plan is communications methods ( physical meetings , good intercepted between runners ) or poor collection insurgent groups Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 89 between these hypothesized challenges and indicates a need to address challenges collectively rather than independently . The exercise , and thus the participants , for the observational study were select - ed partially for convenience because the classification level of many facilities prohibits observations that could be reported in unclassified publications . Although this train - ing exercise met many of the training objectives set by the Military Intelligence School , it did have some limiting artificialities . The analysts were trainees , not experts . It is possible , if not likely , that the challenges for analytical work would differ depending on level of experience or expertise . It is possible , for instance , that phenomena attrib - uted to “bias” ( e . g . , mental set ; see Heuer , 1999 ) might be less pronounced for experts . In this case , recommendations stemming from this study would pertain primarily to training and mentorship of junior analysts ( apprentices and junior journeymen ) . Analysts had less ability to interact with subordinate units or other staff sections than they would have had in real missions . This may have reduced their ability to ask for clarification or further information on most reports . As mentioned previously , the teams were composed of lieutenants and senior noncommissioned officers , and duty positions were changed daily to facilitate training objectives . Actual teams of analysts would have a permanent command structure , and a variety of experience and edu - cation levels would be represented among the members . Although the instructors served as their commanders , the participants were not actually working for a senior officer , who ordinarily would exercise full authority and influence their careers . Scenario - based simulations have become an important part of training and the design process . They allow many deficiencies to be corrected prior to major collisions with reality . However , all simulations , such as the training scenario in our study , have inherent artificialities , and thus can mask some aspects of the real - world work . Com - bining multiple methodologies , such as the unstructured interviews and observations in a simulated setting in this paper , might help to illuminate the limits of fidelity in the simulated exercise and setting . The findings have a number of implications for the design of both procedures and software tools to aid collaborative intelligence analysis under data overload con - ditions and environmental pressures : • There is a need for backups ( both electronic and manual ) to mitigate the risk of the destruction of working products by sand , mud , or precipitation . • The production of briefings might be more efficient if resources needed to focus on the generation of analytic insight were provided . • Tools that help to bring new analysts and teams up to speed on historical information and responsibilities may be particularly important in hostile fire theaters where there is rapid turnover of personnel . • Providing objective feedback on performance ( Smith , Klopfenstein , Jezerinac , & Spencer , 2005 ) may enhance the ability of junior analysts to sustain attention . • Workspaces that are observable to all team members crystallize knowledge and process ( Hutchins , 1995 ) , facilitate ad hoc and scheduled collaborative 90 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 90 cross - checking , and may enable early detection of erroneous assessments and conclusions ( Patterson , Woods , Cook , & Render , in press 2006 ; Sawyer , Farber , & Spillers , 1997 ) . • Training and system design should explicate the information about capabili - ties and limitations of collection systems , support tools , and analytical tools ( e . g . , event maps , time - wheels , and link diagrams ) . This would likely increase the skill of analysts and improve feedback , thus reducing the likelihood of misplaced trust in tools . The present study confirms several important aspects about military intelligence analysis that should be considered in design : • The majority of analysts , military or otherwise , are underskilled junior analysts . • Analysts arrive at assessments in a variety of ways . ( This variety actually strengthens the analysis and is necessary in changing environments . ) • Analysts are susceptible to a variety of extrinsic motivators that affect their sustained attention . • To facilitate learning and workload distribution , senior analysts require the ability to teach and interact with junior analysts . These findings echo those of Soloway ( Soloway , Guzdial , & Hay , 1994 ; Soloway et al . , 1996 ) , who points out several inaccurate assumptions about users . Ideally , mentors would be able to monitor the work of their apprentices and provide feed - back to aid in learning ( Quintana , Krajcik , & Soloway , 2003 ) . Ultimately , as Hoffman , Lintern , and Eitelman ( 2004 ) suggested , tools that are good for conducting cognitive work should also work well to support continual learning and the achievement of expertise . Overall , the present study indicates that collaborative military intelligence ana - lyst teams in both hostile fire theaters and a training simulation contend with vulner - abilities that can result in premature narrowing , difficulty in reframing , and getting lost in the details . Although it is difficult to predict the extent to which the recom - mended interventions will address the identified challenges , this study is a stepping stone for future research in intelligence and information analysis . Acknowledgments This research was supported by the Department of Defense ( BAA - 001 - 04 ) and the Advanced Decision Architectures Consortium sponsored by the U . S . Army Re - search Laboratory under the Collaborative Technology Alliance Program , Coopera - tive Agreement DAAD19 - 01 - 2 - 0009 . The views expressed in this article are those of the authors and do not necessarily represent the views of the Department of De - fense . Furthermore , this study would not have been possible without the exception - al flexibility and assistance of LTC Brian Clark , MAJ Rhett Cox , CW5 Mark Ulatowski , and Don Eveland with the 309th MI Battalion in Fort Huachuca , Arizona . Challenges for Cognition in Intelligence Analysis 91 continued on page 95 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 91 A PP E N D I X 1 . S u mm a r y o f A n a l y t i c a l O u t c o m e s . T h i s t a b l e c a t e g o r i z e s o b s e r v e d m i s i n t e r p r e t a t i o n s o f a v a i l a b l e d a t a . R e p o r t i n g a n d a n a l y s i s a r e g r o u p e d a cc o r d i n g t o t h e t h r ee i m p l i c i t a n a l y t i c a l p r o b l e m s t h a t t e a m s h a d t o a n s w e r . I d e n t i f y P a tt e r n o f A c t i v i t y D a y R e l a t e d R e p o r t s N o r m a t i v e I n t e r p r e t a t i o n M i s i n t e r p r e t a t i o n I n f e rr e d C h a ll e n g e 1 – 4 S t r a t e g i c f u e l p i p e l i n e a tt a c k e d D e v e l o p t r e n d s i n t i m e , l o c a t i o n , f i v e t i m e s t y p e o f t a r g e t , a n d m e t h o d o f a tt a c k 1 – 2 T h r e a t g r o u p s u s e I m p r o v i s e d I d e n t i f y s e v e r a l p o ss i b l e g o a l s f o r E x p l o s i v e D e v i c e s ( I E D s ) i n c e r t a i n i n s u r g e n t a tt a c k s a r e a s o f e a c h B a tt a l i o n A O 1 – 2 P a t r o l f i n d s g r a v e s i t e f o r 10 p e o p l e t h a t w e r e e x e c u t e d 1 – 2 L o c a l p o l i t i c a l p a r t y H Q d e s t r o y e d 1 – 2 F l y e r a d v e r t i s e s p r o - U . S . r a ll y a t U s e p e r s o n a l i t y d a t a b a s e t o i d e n t i f y O v e r l oo k a n t i - U . S . p r o f e ss o r a n d R e c o g n i t i o n o f R e l e v a n t D a t a a l o c a l u n i v e r s i t y b y a n a n t i - U . S . o r g a n i z e r a s a n t i - U . S . a ss e ss r a ll y a s p r o - U . S . p r o f e ss o r 2 – 3 I n s u r g e n t s a tt a c k U . S . p a t r o l R e q u e s t f u r t h e r i n f o r m a t i o n a b o u t D o n o t r e q u e s t f u r t h e r i n f o r m a - R e c o g n i z e R e l e v a n t D a t a b a s e s w i t h m o r t a r s s i z e o f m o r t a r a n d p o ss i b l e f i r i n g t i o n a n d a ss e ss c a p a b i l i t i e s o n p o s i t i o n s i n c o m p l e t e d a t a 3 S e c u r i t y p a t r o l f o r l o c a l o ff i c i a l I n s u r g e n t s a r e f o c u s i n g o n l o c a l i s a m b u s h e d o ff i c i a l s i n a tt e m p t t o d e s t a b i l i z e p r o v i s i o n a l g o v e r n m e n t 3 C oo p e r a t i v e l o c a l m a y o r i s k i ll e d 3 I n s u r g e n t s a tt a c k l o c a l m i l i t i a u n i t 92 Journal of Cognitive Engineering and Decision Making / Spring 2007 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 92 3 I n s u r g e n t s a tt a c k U . S . p a t r o l U s e n e w a tt a c k i n f o r m a t i o n t o c o n - T r e a t i n c i d e n t s a s i n d e p e n d e n t M e n t a l S e t b a s e f o r s e c o n d d a y w i t h m o r t a r s f i r m o r d e n y p r e v i o u s d a y ’ s a n a l y s i s 4 22 c i v i l i a n s d i e a s a r e s u l t o f B r o a d e n a ss e ss m e n t o f p o ss i b l e O v e r l oo k p o ss i b i l i t y o f i n s u r g e n t R e c o g n i t i o n o f R e l e v a n t D a t a w a t e r p o ll u t i o n i n c i d e n t m e t h o d s o f a tt a c k b i o - a tt a c k I d e n t i f y G r o u p S t r u c t u r e D a y R e l a t e d R e p o r t s N o r m a t i v e I n t e r p r e t a t i o n M i s i n t e r p r e t a t i o n I n f e rr e d C h a ll e n g e 2 – 4 P e r s o n a l i t y d a t a b a s e l i s t s U s e d a t a a n d r e p o r t s t o g e n e r a t e , C a t e g o r i z e i n s u r g e n t s s o l e l y R e c o g n i t i o n o f R e l e v a n t D a t a / m i s l e a d i n g b i o g r a p h i c a l d a t a c o n f i r m , o r d e n y h y p o t h e s e s b a s e d o n d a t a b a s e w h i l e F i x a t i o n i g n o r i n g o t h e r s o u r c e s 1 – 2 S o m e t e rr o r i s t s a r e i d e n t i f i e d D e v e l o p l i n k a n a l y s i s t h a t d e p i c t s a n d c a p t u r e d r e s p o n s i b i l i t i e s f o r i n s u r g e n t g r o u p m e m b e r s 3 T w o b o m b - m a k e r s a r e c a p t u r e d 3 F o r e i g n f i g h t e r s d i s c o v e r e d I d e n t i f y C o mm u n i c a t i o n / C oo r d i n a t i o n / L o g i s t i c s M e t h o d s D a y R e l a t e d R e p o r t s N o r m a t i v e I n t e r p r e t a t i o n M i s i n t e r p r e t a t i o n I n f e rr e d C h a ll e n g e 1 – 4 S i g n i f i c a n t b l a c k m a r k e t a c t i v i t y P o ss i b l e l i n k b e t w ee n c r i m i n a l I ll e g a l a c t i v i t y = i n s u r g e n t a c t i v i t y M e n t a l S e t a n d m a j o r f u e l s h o r t a g e g r o u p s a n d i n s u r g e n t g r o u p s 1 – 2 C o ll e c t i o n a ss e t s a r e i n i t i a ll y I d e n t i f y e x i s t i n g g a p s i n c o ll e c t i o n P r o d u c e n o n e w c o ll e c t i o n p l a n ; P oo r T oo l U n d e r s t a n d i n g m i s p l a c e d o r m i s o r i e n t e d t o p l a n ; a d j u s t a ss e t s t o f o c u s o n p r o d u c e f l a w e d c o ll e c t i o n p l a n c o ll e c t o n r e l e v a n t t a r g e t s n e w t a r g e t s 2 – 3 N o e l e c t r o n i c c o mm u n i c a t i o n s C o n s i d e r o t h e r p o ss i b l e m e t h o d s A ss u m e c o ll e c t i o n p l a n i s g oo d M e n t a l S e t / P oo r T oo l i n t e r c e p t e d b e t w ee n i n s u r g e n t ( p h y s i c a l m ee t i n g s , r u nn e r s ) o r a n d o n l y p o ss i b l e m e t h o d i s U n d e r s t a n d i n g g r o u p s p oo r c o ll e c t i o n p h y s i c a l m ee t i n g s Challenges for Cognition in Intelligence Analysis 93 C o n t i n u e d n e x t p a g e Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 93 94 Journal of Cognitive Engineering and Decision Making / Spring 2007 A PP E N D I X 1 . ( c o n t i n u e d ) I d e n t i f y C o mm u n i c a t i o n / C oo r d i n a t i o n / L o g i s t i c s M e t h o d s D a y R e l a t e d R e p o r t s N o r m a t i v e I n t e r p r e t a t i o n M i s i n t e r p r e t a t i o n I n f e rr e d C h a ll e n g e 3 R e p o r t s o f l o g i s t i c s c oo r d i n a t o r P o ss i b l e c oo r d i n a t i o n b e t w ee n “ N o w a y t h e s e g u y s c a n t r a v e l F i x a t i o n b e i n g s p o tt e d i n t w o d i ff e r e n t g r o u p s v i a l o g i s t i c s o ff i c e r t h a t f a r ” ; a ss u m e i n s u r g e n t g r o u p s a r e a s a r e i n d e p e n d e n t 4 R e p o r t s i n d i c a t e l a r g e w e a p o n s C o n f i r m l o g i s t i c a l s u pp o r t f r o m s h i p m e n t f r o m o u t s i d e o f t h e A O o u t s i d e o f A O 4 R e p o r t s o f p l a nn i n g f o r b o m b i n g P o ss i b l e o u t s i d e i n f l u e n c e o n I n s u r g e n t g r o u p s a c t i n g w i t h o u t F i x a t i o n a tt a c k s a f t e r p r o m i n e n t c l e r i c i n s u r g e n t g r o u p s ; a ss e ss p o ss i b l e o u t s i d e r e s o u r c e s i ss u e s f a t w a h , o r r e l i g i o u s e d i c t i n c r e a s e i n i n s u r g e n t a c t i v i t y 4 R e p o r t o f b o a t b e i n g s t o l e n a n d C o n f i r m c oo r d i n a t i o n b e t w ee n r e p o r t o f p l a nn e d b o a t a tt a c k o n g r o u p s p o r t Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 94 Challenges for Cognition in Intelligence Analysis 95 References Catignani , S . ( 2004 ) . Motivating soldiers : The example of the Israeli Defense Forces . Parameters , Autumn , 108 – 121 . Davies , D . , & Parasurman , R . ( 1982 ) . The psychology of vigilance . London : Academic Press . De Keyser , V . , & Woods , D . ( 1990 ) . Fixation errors : Failures to revise situation assessment in dynamic and risky systems . In A . G . Colombo & A . Saiz de Bustamante ( Eds . ) , Systems Reli - ability Assessment ( pp . 231 – 252 ) . Dordrecht , Netherlands : Kluwer Academic Publishers . Dekker , S . W . A . ( 2002 ) . The field guide to human error investigations . Bedford , UK : Cranfield Uni - versity Press / Aldershot , UK : Ashgate . Doctrine for Intelligence Support to Joint Operations . ( 2000 ) . Washington , DC : Department of Defense . Dominowski , R . , & Bourne , L . ( 1994 ) . History of research on thinking and problem solving . In R . Sternberg ( Ed . ) , Thinking and problem solving ( pp . 1 – 35 ) . San Diego , CA : Academic Press . Einhorn , H . , & Hogarth , R . ( 1978 ) . Confidence in judgment : Persistence in the illusion of valid - ity . Psychological Review , 85 , 395 – 416 . Elm , W . , Potter , S . , Tittle , J . , Woods , D . , Grossman , J . , & Patterson , E . ( 2005 ) . Finding decision support requirements for effective intelligence analysis tools . In Proceedings of the Human Fac - tors and Ergonomics Society 49th Annual Meeting ( pp . 297 – 301 ) . Santa Monica , CA : Human Factors and Ergonomics Society . Feltovich , P . J . , Coulsen , R . , Spiro , R . , & Adami , J . ( 1993 ) . Conceptual understanding and stability and knowledge shield for fending off conceptual change . Springfield : Southern Illinois University School of Medicine . Feltovich , P . J . , Ford , K . M . , & Hoffman , R . R . ( 1997 ) . Expertise in context : Human and machine . Cambridge : MIT Press . Frederick - Recascino , C . , & Hall , S . ( 2003 ) . Pilot motivation and performance : Theoretical and empirical relationships . International Journal of Aviation Psychology , 13 , 401 – 414 . Hoffman , R . R . ( 1998 ) . How can expertise be defined ? : Implications of research from cognitive psychology . In R . Williams , W . Faulkner , & J . Fleck ( Eds . ) , Exploring expertise ( pp . 81 – 100 ) . New York : Macmillan . Hoffman , R . R . , Lintern , G . , & Eitelman , S . ( 2004 , March / April ) . The Janus principle . IEEE : Intel - ligent Systems , pp . 78 – 80 . Hollnagel , E . , & Woods , D . ( 2005 ) . Joint cognitive systems : Foundations of cognitive systems engi - neering . Boca Raton , FL : CRC Press . Huey , B . M . , & Wickens , C . D . ( 1993 ) . Workload transition : Implications for individual and team performance . Washington , DC : National Academy Press . Heuer , R . ( 1999 ) . Psychology of intelligence analysis . Washington , DC : Central Intelligence Agency , Center for the Study of Intelligence . Hutchins , E . ( 1995 ) . Cognition in the wild . Cambridge : MIT Press . Johnston , R . ( 2005 ) . Analytic culture in the U . S . intelligence community : An ethnographic study . Wash - ington , DC : Central Intelligence Agency , Center for the Study of Intelligence . Klayman , J . , & Ha , Y . ( 1987 ) . Confirmation , disconfirmation , and information in hypothesis test - ing . Psychological Review , 94 , 211 – 228 . Lee , J . D . , & See , A . ( 2004 ) . Trust in automation : Designing for appropriate reliance . Human Fac - tors , 46 , 50 – 80 . Lord , C . , Lepper , M . , & Ross , L . ( 1979 ) . Biased assimilation and attitude polarization : The effects of prior theories on subsequently considered evidence . Journal of Personality and Social Psychology , 37 , 2098 – 2110 . Luchins , A . S . ( 1942 ) . Mechanization in problem solving . Psychological Monographs , 54 . 1 – 95 . Norman , D . A . ( 1990 ) . The design of everyday things . New York : Doubleday . Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 95 96 Journal of Cognitive Engineering and Decision Making / Spring 2007 Norman , D . A . ( 1990 ) . The “problem” with automation : Inappropriate feedback and interaction , not “over - automation . ” Philosophical Transactions of the Royal Society of London , 327 , 585 – 593 . Operational Requirements Document for Distributed Common Ground System - Army ( DCGS - A ) . ( 2004 ) . Washington , DC : Headquarters , Department of the Army . Patterson , E . S . , Roth , E . M . , & Woods , D . D . ( 2001 ) . Predicting vulnerabilities in computer - supported inferential analysis under data overload . Cognition , Technology and Work , 3 , 224 – 237 . Patterson , E . S . , Woods , D . D . , Cook , R . I . , & Render , M . L . ( in press , 2006 ) . Collaborative cross - checking to enhance resilience . Cognition , Technology and Work , Special Issue on Large - Scale Coordination . Patterson , E . S . , Rogers , M . L . , & Render , M . L . ( 2004 ) . Simulation - based embedded probe tech - nique for human - computer interaction evaluation . Cognition , Technology , and Work , 6 , 197 – 205 . Quintana , C . , Krajcik J . , & Soloway , E . ( 2003 ) . A framework for understanding the development of educational software . In J . Jacko & A . Sears ( Eds . ) , The human - computer interaction hand - book ( pp . 823 – 834 ) . Mahwah , NJ : Erlbaum . Sawyer , S . , Farber , J . , & Spillers , R . ( 1997 ) . Supporting the social processes of software develop - ment teams . Information Technology & People , 10 , 46 – 62 . Smith , P . J . , Klopfenstein , M . , Jezerinac , J . , & Spencer , A . ( 2005 ) . Distributed work in the National Airspace System : Providing feedback loops using the Post - Operations Evaluation Tool ( POET ) . In B . Kirwan , M . Rodgers , & D . Schaefer ( Eds . ) , Human factors impacts in air traffic manage - ment ( pp . 127 – 152 ) . Hampshire , UK : Ashgate . Soloway , E . , Guzdial , M . , & Hay , K . ( 1994 ) . Learner - centered design : The challenge for HCI in the 21st century . Interactions , 1 , 36 – 48 . Soloway , E . , Jackson , S . , Klein , J . , Quintana , C . , Reed , J . , Spitulnik , J . , Stratford , S . , Struder , S . , Eng , J . , & Scala , N . ( 1996 ) . Learning theory in practice : Case studies in learner - centered design . Human Factors in Computing Systems : CHI 1996 Conference Proceedings ( pp . 189 – 196 ) . New York : ACM Press . Warm , J . , Dember , W . , & Hancock , P . ( 1996 ) . Automation and human performance : Theory and applications ( pp . 183 – 200 ) . Mahwah , NJ : Erlbaum . Woods , D . ( 1994 ) . Cognitive demands and activities in dynamic fault management : Abduction and disturbance management . In N . Stanton ( Ed . ) , Human factors of alarm design ( pp . 63 – 92 ) . London : Taylor & Francis . Woods , D . ( 2005 ) . Supporting cognitive work : How to achieve high levels of coordination and resilience in joint cognitive systems . To appear in Naturalistic Decision Making 7 . Amsterdam , The Netherlands . Woods , D . , & Hollnagel , E . ( 2006 ) . Joint cognitive systems : Patterns in cognitive systems engineering . Boca Raton , FL : CRC Press . Major Stoney Trent ( ABD ) teaches Psychology for Leaders and Biomechanics at the United States Military Academy , 276C Thayer Hall , West Point , NY , 10966 , stoney . trent @ us . army . mil . His research includes military applications for human factors engineering including com - mand and control and intelligence analysis . Stoney served in the army for 11 years in various tactical positions . He has supported operations in Kosovo as well as Iraq . Stoney Trent’s views do not represent those of the U . S . Army or the United States Military Academy . Emily S . Patterson is a research scientist at The Ohio State University’s Institute for Ergonom - ics and the VA Getting at Patient Safety ( GAPS ) Center . She serves as associate director of the Converging Perspectives on Data ( CPoD ) Consortium . Her research interests include the appli - cation of human factors , human - computer interaction ( HCI ) , and computer - supported co - operative work ( CSCW ) concepts to improve human performance in complex sociotechnical settings , including military command and control , intelligence analysis , health care , space shuttle mission control , and emergency call centers . Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 96 David D . Woods has studied and designed decision support systems for 25 years beginning with nuclear power emergencies following the Three Mile Island accident . He has studied how coordination breakdowns between airline flight crews and automated systems occur , how space mission control achieves high levels of coordination in anomaly response , how operating room teams handle crises , and how military C2 responds to handle disruptions in ongoing plans , using results from these studies to design new support systems . See http : / / csel . eng . ohio - state . edu / woods for multimedia productions on data overload , human error , laws that govern cognitive work , as well as other guides to his research . Challenges for Cognition in Intelligence Analysis 97 Trent , r3 . qxd 3 / 22 / 07 4 : 53 PM Page 97