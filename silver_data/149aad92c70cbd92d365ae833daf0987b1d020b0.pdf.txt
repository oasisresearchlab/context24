Parallel and I / O - eﬃcient Randomisation of Massive Networks using Global Curveball Trades Corrie Jacobien Carstens University of Amsterdam , Netherlands c . j . carstens @ uva . nl Michael Hamann Karlsruhe Institute of Technology , Germany michael . hamann @ kit . edu Ulrich Meyer Goethe University , Frankfurt , Germany umeyer @ ae . cs . uni - frankfurt . de Manuel Penschuck Goethe University , Frankfurt , Germany mpenschuck @ ae . cs . uni - frankfurt . de Hung Tran Goethe University , Frankfurt , Germany htran @ ae . cs . uni - frankfurt . de Dorothea Wagner Karlsruhe Institute of Technology , Germany dorothea . wagner @ kit . edu Abstract Graph randomisation is a crucial task in the analysis and synthesis of networks . It is typically implemented as an edge switching process ( ESMC ) repeatedly swapping the nodes of random edge pairs while maintaining the degrees involved [ 24 ] . Curveball is a novel approach that instead considers the whole neighbourhoods of randomly drawn node pairs . Its Markov chain converges to a uniform distribution , and experiments suggest that it requires less steps than the established ESMC [ 6 ] . Since trades however are more expensive , we study Curveball’s practical runtime by introducing the ﬁrst eﬃcient Curveball algorithms : the I / O - eﬃcient EM - CB for simple undir - ected graphs and its internal memory pendant IM - CB . Further , we investigate global trades [ 6 ] processing every node in a graph during a single super step , and show that undirected global trades converge to a uniform distribution and perform superior in practice . We then discuss EM - GCB and EM - PGCB for global trades and give experimental evidence that EM - PGCB achieves the quality of the state - of - the - art ESMC algorithm EM - ES [ 15 ] nearly one order of magnitude faster . 2012 ACM Subject Classiﬁcation Mathematics of computing → Random graphs Keywords and phrases Graph randomisation , Curveball , I / O - eﬃciency , Parallelism Supplement Material Stable versions of IM - CB and EM - GCB are released as part of NetworKit ( http : / / network - analysis . info ) . Funding This work was partially supported by Deutsche Forschungsgemeinschaft ( DFG ) under grants ME 2088 / 3 - 2 , ME 2088 / 4 - 2 , and WA 654 / 22 - 2 . a r X i v : 1804 . 08487v2 [ c s . D S ] 17 A ug 2018 11 : 2 Randomisation of Massive Networks using Global Curveball Trades 1 Introduction In the analysis of complex networks , such as social networks , the underlying graphs are commonly compared to random graph models to understand their structure [ 17 , 28 , 35 ] . While simple models like Erdős - Rényi graphs [ 11 ] are easy to generate and analyse , they are too diﬀerent from commonly observed powerlaw degree sequences [ 28 , 27 , 35 ] . Thus , random graphs with the same degree sequence as the given graph are frequently used [ 8 , 17 , 33 ] . In practice , many of these graphs are simple graphs , i . e . graphs without self - loops and multiple edges . In order to obtain reliable results in these cases , the graphs sampled need to be simple since non - simple models can lead to signiﬁcantly diﬀerent results [ 32 , 33 ] . The randomisation of a given graph is commonly implemented as an edge switching Markov chain ESMC [ 8 , 25 ] . Nowadays , massive graphs that cannot be processed in the RAM of a single computer , require new analysis algorithms to handle these huge datasets . In turn , large benchmark graphs are required to evaluate the algorithms’ scalability — in terms of speed and quality . LFR is a standard benchmark for evaluating clustering algorithms which repeatedly generates highly biased graphs that are then randomised [ 18 , 19 ] . [ 15 ] presents the external memory LFR generator EM - LFR and its I / O - eﬃcient edge switching EM - ES . Although EM - ES is faster than previous results even for graphs ﬁtting into RAM , it dominates EM - LFR ’s running time . Alternative sampling via the Conﬁguration Model [ 26 ] was studied to reduce the initial bias and the number of ESMC steps necessary [ 14 ] . Still , graph randomisation remains a major bottleneck during the generation of these huge graphs . The Curveball algorithm has been originally proposed for randomising binary matrices while preserving row and column sums [ 36 , 37 ] and has been adopted for graphs [ 5 , 6 ] : instead of switching a pair of edges as in ESMC , Curveball trades the neighbours of two nodes in each step . Carstens et al . further propose the concept of a global trade , a super step composed of single trades targetting every node 1 in a graph once [ 6 ] . The authors show that global trades in bipartite or directed graphs converge to a uniform distribution , and give experimental evidence that global trades require fewer Markov - chain steps than single trades . However , while fewer steps are needed , the trades themselves are computationally more expensive . Since we are not aware of previous eﬃcient Curveball algorithms and implementations , we investigate this trade - oﬀ here . Our contributions . We present the ﬁrst eﬃcient algorithms for Curveball : the ( sequen - tial ) internal memory and external memory algorithms IM - CB 2 and EM - CB for the Simple Undirected Curveball algorithm ( see section 4 ) . Experiments in section 5 , indicate that they are faster than the established edge switching approaches in practice . In section 3 , we show that random global trades lead to uniform samples of simple , undirected graphs and demonstrate experimentally in section 5 that they converge even faster than the corresponding number of uniform single trades . Exploiting structural properties of global trades , we simplify EM - CB yielding EM - GCB and the parallel I / O - eﬃcient EM - PGCB which achieves EM - ES ’s quality nearly one order of magnitude faster in practice ( see section 5 ) . 1 For an odd number n of nodes , a single node is left out 2 We preﬁx internal memory algorithms with IM and I / O - eﬃcient algorithms with EM . The suﬃces CB , GCB , and PGCB denote Curveball , CB . with global trades , and parallel CB . with global trades respectively . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 3 2 Preliminaries and Notation We deﬁne the short - hand [ k ] : = { 1 , . . . , k } for k ∈ N > 0 , and write [ x i ] bi = a for an ordered sequence [ x a , x a + 1 , . . . , x b ] . Graphs and degree sequences . A graph G = ( V , E ) has n = | V | sequentially numbered nodes V = { v 1 , . . . , v n } and m = | E | edges . Unless stated diﬀerently , graphs are assumed to be undirected and unweighted . To obtain a unique representation of an undirected edge { u , v } ∈ E , we use ordered edges [ u , v ] ∈ E implying u ≤ v ; in contrast to a directed edge , the ordering is used algorithmically but does not carry any meaning . A graph is called simple if it contains neither multi - edges nor self - loops , i . e . E ⊆ { { u , v } | u , v ∈ V with u 6 = v } . For node u ∈ V deﬁne the neighbourhood A u : = { v : { u , v } ∈ E } and degree deg ( u ) : = | A u | . Let d max : = max v { deg ( v ) } be the maximal degree of a graph . A vector D = [ d i ] ni = 1 is a degree sequence of graph G iﬀ ∀ v i ∈ V : deg ( v i ) = d i . Randomisation and Distributions . Pld ( [ a , b ) , γ ) refers to an integer Powerlaw Distribution with exponent − γ ∈ R for γ ≥ 1 and values from the interval [ a , b ) ; let X be an integer random variable drawn from Pld ( [ a , b ) , γ ) then P [ X = k ] ∝ k − γ ( proportional to ) if a ≤ k < b and P [ X = k ] = 0 otherwise . A statement depending on some number x > 0 is said to hold with high probability if it is satisﬁed with probability at least 1 − 1 / x c for some constant c ≥ 1 . Let S be a ﬁnite set , x ∈ S and let σ be permutation on S , we deﬁne rank σ ( x ) as the number of elements positioned in front of x by σ . 2 . 1 External - Memory Model In contrast to classic models of computation , such as the unit - cost random - access machine , modern computers contain deep memory hierarchies ranging from fast registers , over caches and main memory to solid state drives ( SSDs ) and hard disks . Algorithms unaware of these properties may face performance penalties of several orders of magnitude . We use the commonly accepted external memory ( EM ) model by Aggarwal and Vitter [ 1 ] to reason about the inﬂuence of data locality in memory hierarchies . It features two memory types , namely fast internal memory ( IM or RAM ) holding up to M data items , and a slow disk of unbounded size . The input and output of an algorithm are stored in EM while computation is only possible on values in IM . An algorithm’s performance is measured in the number of I / Os required . Each I / O transfers a block of B = Ω ( √ M ) consecutive items between memory levels . Reading or writing n contiguous items is referred to as scanning and requires scan ( n ) : = Θ ( n / B ) I / Os . Sorting n consecutive items triggers sort ( n ) : = Θ ( ( n / B ) · log M / B ( n / B ) ) I / Os . For all realistic values of n , B and M , scan ( n ) < sort ( n ) (cid:28) n . Sorting complexity constitutes a lower bound for most intuitively non - trivial EM tasks [ 23 ] . EM queues use amortised O ( 1 / B ) I / Os per operation and require O ( B ) main memory [ 29 ] . An external priority queue ( PQ ) requires O ( sort ( n ) ) I / Os to push and pop n items [ 2 ] . 2 . 2 TFP : Time Forward Processing Time Forward Processing ( TFP ) is a generic technique to manage data dependencies of external memory algorithms [ 21 ] . Consider an algorithm computing values x 1 , . . . , x n in which the calculation of x i requires previously computed values . One typically models these dependencies using a directed acyclic graph G = ( V , E ) . Every node v i ∈ V corresponds to the computation of x i and an edge ( v i , v j ) ∈ E indicates that the value x i is necessary to compute x j . For instance consider the Fibonacci sequence x 0 = 0 , x 1 = 1 , x i = x i − 1 + x i − 2 ∀ i ≥ 2 in 11 : 4 Randomisation of Massive Networks using Global Curveball Trades v 2 x 0 + x 1 x 2 = 1 v 3 x 1 + x 2 x 3 = 2 v 4 x 2 + x 3 x 4 = 3 v 5 x 3 + x 4 x 5 = 5 v 6 x 4 + x 5 x 6 = 8 v 7 x 5 + x 6 x 7 = 13 1 PQ . push ( < key = 2 , value = 0 > ) ; PQ . push ( < key = 2 , value = 1 > ) 2 foreach i ← 2 , . . . , n do 3 sum ← 0 4 while PQ . min . key = = i do / / Two iterations 5 sum ← sum + PQ . remove - min ( ) . value 6 print ( “ x i = ” , sum ) 7 PQ . push ( < key = i + 1 , sum > ) ; PQ . push ( < key = i + 2 , sum > ) Figure 1 Left : Dependency graph of the Fibonacci sequence ( ignoring base case ) . Right : Time Forward Processing to compute sequence . which each node v i with i ≥ 2 depends on exactly its two predecessors ( see Fig . 1 ) . Here , a linear scan for increasing i suﬃces to solve the dependencies . In general , an algorithm needs to traverse G according to some topological order ≺ T of nodes V and also has to ensure that each v j can access values from all v i with ( v i , v j ) ∈ E . The TFP technique achieves this as follows : as soon as x i has been calculated , messages of the form h v j , x i i are sent to all successors ( v i , v j ) ∈ E . These messages are kept in a minimum priority queue sorting the items by their recipients according to ≺ T . By construction , the algorithm only starts the computation v i once all predecessors v j ≺ T v i are completed . Since these predecessors already removed their messages from the PQ , items addressed to v i ( if any ) are currently the smallest elements in the data structure and can be dequeued . Using a suited EM PQ [ 2 ] , TFP incurs O ( sort ( k ) ) I / Os , where k is the number of messages sent . 3 Randomisation schemes Here , we summarise the randomisation schemes ESMC [ 25 ] and Curveball for simple undirec - ted graphs [ 5 ] , and then discuss the notion of global trades . Since these algorithms iteratively modify random parts of a graph , they can be analysed as ﬁnite Markov chains . It is well known that any ﬁnite , irreducible , aperiodic , and symmetric Markov chain converges to the uniform distribution on its state space ( e . g . [ 20 ] ) . Its mixing time indicates the number of steps necessary to reach the stationary distribution . 3 . 1 Edge - Switching ESMC is a state - of - the - art randomisation method with a wide range of applications , e . g . the generation of graphs [ 15 , 19 ] , or the randomisation of biological datasets [ 16 ] . In each step , ESMC chooses two edges e 1 = [ u 1 , v 1 ] , e 2 = [ u 2 , v 2 ] and a direction d ∈ { 0 , 1 } uniformly at random and rewires them into { u 1 , u 2 } , { v 1 , v 2 } if d = 0 and { u 1 , v 2 } , { v 1 , u 2 } otherwise . If a step yields a non - simple graph , it is skipped . ESMC ’s Markov chain is irreducible [ 10 ] , aperiodic and symmetric [ 24 ] and hence converges to the uniform distribution on the space of simple graphs with ﬁxed degree sequence . While analytic bounds on the mixing time [ 12 , 13 ] are impractical , usually a number of steps linear in the number of edges is used in practice [ 30 ] . 3 . 2 Simple Undirected Curveball algorithm Curveball is a novel randomisation method . In each step , two nodes trade their neighbour - hoods , possibly yielding faster mixing times [ 5 , 36 , 37 ] . (cid:73) Deﬁnition 1 ( Simple Undirected Trade ) . Let G = ( V , E ) be a simple graph , A be its adjacency list representation , and A u be the set of neighbours of node u . A trade t = ( i , j , σ ) from A to adjacency list B is deﬁned by two nodes i and j , and a permutation σ : D ij → D ij where A i − j : = A i \ ( A j ∪ { j } ) and D ij : = A i − j ∪ A j − i . As shown in Fig . 2 , performing t on C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 5 i j 1 2 3 4 5 6 i j 1 2 2 3 4 5 6 A i = { 1 , 2 , 6 , j } A j = { 3 , 4 , 5 , 6 , i } B i = { 3 , 4 , 6 , j } B j = { 1 , 2 , 5 , 6 , i } B i − j = { 3 , 4 } B j − i = { 1 , 2 , 5 } σ ( 1 , 2 | { z } A i − j , 3 , 4 , 5 | { z } A j − i ) 7→ ( 4 , 3 | { z } B i − j , 5 , 1 , 2 | { z } B j − i ) Figure 2 The trade ( i , j , σ ) between nodes i and j only considers edges to the disjoint neigh - bours { 1 , . . . , 5 } . For the reassigned disjoint neighbours we use the short - hand B i − j : = { x | x ∈ D ij , rank σ ( x ) ≤ | A i − j | } and B j − i : = { x | x ∈ D ij , rank σ ( x ) > | A i − j | } . The triangle ( i , j , 6 ) is omitted as trading any of its edges would either introduce parallel edges , self loops , or result in no change at all . Then , the given σ exchanges four edges . G results in B i = ( A i \ A i − j ) ∪ { x | x ∈ D ij , rank σ ( x ) ≤ | A i − j | } and B j = ( A j \ A j − i ) ∪ { x | x ∈ D ij , rank σ ( x ) > | A i − j | } . Since edges are undirected , symmetry has to be preserved : for all u ∈ A i \ B i the label j in adjacency list B u is changed to i and analogously for A j \ B j . Simple Undirected Curveball randomises a graph by repeatedly selecting a node pair { i , j } and permutation σ on the disjoint neighbours uniformly at random . Its Markov chain is irreducible , aperiodic and symmetric and hence converges to the uniform distribution [ 6 ] . 3 . 3 Undirected Global Trades Trade sequences typically consist of pairs in which each constituent is drawn uniformly at random . While it is a well - known fact 3 that Θ ( n log n ) trades are required in expectation until each node is included at least once , there is no apparent reason why this should be beneﬁcial ; in fact , experiments in section 5 suggest the contrary . Carstens et al . propose the notion of global trades for directed or bipartite graphs as a 2 - partition of all nodes implicitly forming n / 2 node pairs to be traded in a single step [ 6 ] . This concept fails for undirected graphs where in general the two directions ( u , v ) and ( v , u ) of an edge { u , v } cannot be processed independently in a single step . We hence extend global trades to undirected graphs by interpreting them as a sequence of n / 2 simple trades which together target each node exactly once ( we assume n to be even ; if this is not the case we add an isolated node 4 ) . Dependencies are then resolved by the order of this sequence . (cid:73) Deﬁnition 2 ( Undirected Global Trade ) . Let G = ( V , E ) be a simple undirected graph and π : V → V be a permutation on the set of nodes . A global trade T = ( t 1 , . . . , t ‘ ) for ‘ = b n / 2 c is a sequence of trades t i = { π ( v 2 i − 1 ) , π ( v 2 i ) , σ i } . By applying T to G we mean that the trades t 1 , . . . , t ‘ are applied successively starting with G . Theorem 3 allows us to use global trades as a substitute for a sequence of single trades , as global trades preserve the stationary distribution of Curveball’s Markov chain . The proof extends [ 6 ] , which shows convergence of global trades in bipartite or directed graphs , to undirected graphs and uses similar techniques . (cid:73) Theorem 3 . Let G = ( V , E ) be an arbitrary simple undirected graph , and let Ω G be the set of all simple directed graphs that have the same degree sequence as G . The Curveball algorithm with global trades and started at G converges to the uniform distribution on Ω G . 3 For instance studied as the coupon collector problem . 4 This is equivalent to randomly excluding a single node from a global trade 11 : 6 Randomisation of Massive Networks using Global Curveball Trades Proof . In order to prove the claim , we have to show irreducibility and aperiodicity of the Markov chain as well as symmetry of the transition probabilities . For the ﬁrst two properties it suﬃces to show that whenever there exists a single trade from state A to B , there also exists a global trade from A to B ( see [ 4 ] for a similar argument ) . 5 Observe that there is a non - zero probability that a single trade does not change the graph , e . g . by selecting σ i as the identity . Hence there is a non - zero probability that . . . a global trade does not alter the graph at all . This corresponds to a self - loop at each state of the Markov chain and hence guarantees aperiodicity . all but one single trade of a global trade do not alter the graph . In this case , a global trade degenerates to a single trade and the irreducibility shown in [ 4 ] carries over . It remains to show that the transition probabilities are symmetric . Let T gAB be the set of global trades that transform state A to state B . Then the transition probability between A and B equals the sum of probabilities of selecting a trade sequence from T gAB . That is P AB = P T ∈T gAB P A ( T ) where P A ( T ) denotes the probability of selecting global trade T in state A . The probability P A ( t ) of selecting a single trade t = ( i , j , σ ) from state A to state B equals the probability P B ( ˜ t ) of selecting the reverse trade ˜ t = ( i , j , σ − 1 ) from state B to A [ 6 ] . We now deﬁne the reverse global trade of T = ( t 1 , . . . , t ‘ ) as ˜ T = ( ˜ t ‘ , . . . , ˜ t 1 ) . It is straight - forward to check that this gives a bijection between the sets T gAB and T gBA . It remains to show that the middle equality holds in P AB = X T ∈T gAB P A ( T ) ! = X ˜ T ∈T gBA P B ( ˜ T ) = P BA . Let T = ( t 1 , . . . , t ‘ ) be a global trade from state A to state B as implied by π and A = A 1 , . . . , A ‘ + 1 = B be the intermediate states . We denote the reversal of T and π as ˜ T and ˜ π respectively and obtain P A ( T ) = P ( π ) P A 1 ( t 1 ) . . . P A ‘ ( t ‘ ) = P ( ˜ π ) P B ( ˜ t ‘ ) . . . P A 2 ( ˜ t 1 ) = P B ( ˜ T ) . Clearly P ( π ) = P ( ˜ π ) as we are picking permutations uniformly at random . The second equality follows from P A ( t ) = P B ( ˜ t ) for a single trade between A and B . (cid:74) 4 Novel Curveball algorithms for undirected graphs In this section we present the related algorithms EM - CB , IM - CB , EM - GCB and EM - PGCB . The algorithms receive a simple graph G and a trade sequence T = [ { u i , v i } ] ‘i = 1 as input and compute the result of carrying out the trade sequence T ( see section 3 . 2 ) in order . EM - CB and IM - CB are sequential solutions suited to process arbitrary trade sequences T . For our analysis , we assume T ’s constituents to be drawn uniformly at random ( as expected in typical applications ) . Both algorithms share a common design , but diﬀer in the data structures used . EM - CB is an I / O - eﬃcient algorithm while IM - CB is optimised for small graphs allowing for unstructured accesses to main memory . In contrast , EM - GCB and EM - PGCB process global trades only . This restricted input model allows us to represent the trade sequence T implicitly by hash functions which further accelerates trading . 5 Since each global trade can be emulated by its n / 2 decomposed single trades , the reverse is true for a hop of n / 2 single trade steps . Due to dependencies however the transition probabilities generally do not match , see V = { 1 , 2 , 3 , 4 } and E = { [ 1 , 2 ] , [ 3 , 4 ] } for a simple counterexample . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 7 Algorithm 1 : EM - CB Data : Trade sequence T , simple graph G = ( V , E ) by edge list E / / Preprocessing : Compute Dependencies 1 foreach trade t i = ( u , v ) ∈ T for increasing i do 2 Send messages h u , t i i and h v , t i i to Sorter SorterTtoV 3 Sort SorterTtoV lexicographically / / All trades of a node are next to each other 4 foreach node u ∈ V do 5 Receive S ( u ) = [ t 1 , . . . , t k ] from k messages addressed to u in SorterTtoV 6 Set t k + 1 ← ∞ / / t 1 = ∞ iff u is never active 7 Send h t i , u , t i + 1 i to SorterDepChain for i ∈ [ k ] 8 foreach directed edge ( u , v ) ∈ E do 9 if u < v then 10 Send message h v , u , t 1 i via PqVtoV 11 else 12 Receive t v 1 from unique message received via PqVtoV 13 if t 1 ≤ t v 1 then Send message h t 1 , u , v , t v 1 i via PqTtoT else Send message h t v 1 , v , u , t 1 i via PqTtoT 14 Sort SorterDepChain / / Main phase – Currently at least the first trade has all information it needs 15 foreach trade t i = ( u , v ) ∈ T for increasing i do 16 Receive successors τ ( u ) and τ ( v ) via SorterDepChain 17 Receive neighbours A G ( u ) , A G ( v ) and their successors τ ( · ) from PqTtoT 18 Randomly reassign disjoint neighbours , yielding new neighbours A 0 G ( u ) and A 0 G ( v ) . 19 foreach ( a , b ) ∈ ( { u } × A 0 G ( u ) ) ∪ ( { v } × A 0 G ( v ) ) do 20 if τ a = ∞ and τ b = ∞ then Output ﬁnal edge { a , b } else if τ a ≤ τ b then Send message h τ a , a , b , τ b i via PqTtoT else Send message h τ b , b , a , τ a i via PqTtoT At core , all algorithms perform trades in a similar fashion : In order to carry out the i - th trade { u i , v i } , they retrieve the neighbourhoods A u i and A v i , shuﬄe 6 them , and then update the graph . Once the neighbourhoods are known , trading itself is straight - forward . We compute the set of disjoint neighbours D = ( A u i ∪ A v i ) \ ( A u i ∩ A v i ) and then draw | A u i ∩ D | nodes from D for u i uniformly at random while the remaining nodes go to v i . If A u i and A v i are sorted this requires only O ( | A u i | + | A v i | ) work and scan ( | A u i | + | A v i | ) I / Os ( see also proof of Lemma 6 if the neighbourhoods ﬁt into RAM ) . Hence we focus on the harder task of obtaining and updating the adjacency information . 4 . 1 EM - CB : A sequential I / O - eﬃcient Curveball algorithm EM - CB is an I / O - eﬃcient Curveball algorithm to randomise undirected graphs as detailed in Alg . 1 . This basic algorithm already contains crucial design principles which we further explore with IM - CB , EM - GCB and EM - PGCB in sections 4 . 2 and 4 . 4 respectively . The algorithm encounters the following challenges . After an undirected trade { u , v } is carried out , it does not suﬃce to only update the neighbourhoods A u and A v : consider the case that edge { u , x } changes into { v , x } . Then this switch also has to be reﬂected in the neighbourhood of A x . Here , we call u and v active nodes while x is a passive neighbour . 6 In contrast to Deﬁnition 2 , we do not consider the permutation σ of disjoint neighbours as part of the input , but let the algorithm choose one randomly for each trade . We consider this design decision plausible as the set of disjoint neighbours only emerges over the course of the execution . 11 : 8 Randomisation of Massive Networks using Global Curveball Trades In the EM setting another challenge arises for graphs exceeding main memory ; it is prohibitively expensive to directly access the edge list since this unstructured pattern triggers Ω ( 1 ) I / Os for each edge processed with high probability . EM - CB approaches these issues by abandoning a classical static graph data structure containing two redundant copies of each edge . Following the TFP principle , we rather interpret all trades as a sequence of points over time that are able to receive messages . Initially , we send each edge to the earliest trade one of its endpoints is active in . 7 This way , the ﬁrst trade receives one message from each neighbour of the active nodes and hence can reconstruct A u 1 and A v 1 . After shuﬄing and reassigning the disjoint neighbours , EM - CB sends each resulting edge to the trade which requires it next . If no such trade exists , the edge can be ﬁnalised by committing it to the output . The algorithm hence requires for each ( actively or passively ) traded node u , the index of the next trade in which u is actively processed . We call this the successor of u and deﬁne it to be ∞ if no such trade exists . The dependency information is obtained in a preprocessing step ; given T = [ { u i , v i } ] ‘i = 1 , we ﬁrst compute for each node u the monotonically increasing index list S ( u ) of trades in which u is actively processed , i . e . S ( u ) : = (cid:2) i | u ∈ t i for i ∈ [ ‘ ] (cid:3) ◦ [ ∞ ] . (cid:73) Example 4 . Let G = ( V , E ) be a simple graph with V = { v 1 , v 2 , v 3 , v 4 } and trade sequence T = [ t 1 : { v 1 , v 2 } , t 2 : { v 3 , v 4 } , t 3 : { v 1 , v 3 } , t 4 : { v 2 , v 4 } , t 5 : { v 1 , v 4 } ] . Then , the successors S follow as S ( v 1 ) = [ 1 , 3 , 5 , ∞ ] , S ( v 2 ) = [ 1 , 4 , ∞ ] , S ( v 3 ) = [ 2 , 3 , ∞ ] , S ( v 4 ) = [ 2 , 4 , 5 , ∞ ] . This information is then spread via two channels : After preprocessing , EM - CB scans S and T conjointly and sends h t i , u i , t ui i and h t i , v i , t vi i to each trade t i . The messages carry the successors t ui and t vi of the trade’s active nodes . When sending an edge as described before , we augment it with the successor of the passive node . Initially , this information is obtained by scanning the edge list E and S conjointly . Later , it can be inductively computed since each trade receives the successors of all nodes involved . (cid:73) Lemma 5 . For an arbitrary trade sequence T of length ‘ , EM - CB has a worst - case I / O complexity of O [ sort ( ‘ ) + sort ( n ) + scan ( m ) + ‘d max / B log M / B ( m / B ) ] . For r global trades , the worst case I / O complexity is O ( r [ sort ( n ) + sort ( m ) ] ) . Proof . Refer to Appendix A for the proof . (cid:74) 4 . 2 IM - CB : An internal memory version of EM - CB While EM - CB is well - suited if memory access is a bottleneck , we also consider the modiﬁed version IM - CB . As shown in section 5 , IM - CB is typically faster for small graph instances . IM - CB uses the same algorithmic ideas as EM - CB but replaces its priority queues and sorters 8 by unstructured I / O into main memory ( see Alg . 2 ( Appendix ) for details ) : Instead of sending neighbourhood information in a TFP - fashion , we now rely on a classical adjacency vector data structure A G ( an array of arrays ) . Similarly to EM - CB , we only 7 If an edge connects two nodes that are both actively traded we implicitly perform an arbitrary tie - break . 8 The term sorter refers to a container with two modes of operation : in the ﬁrst phase , items are pushed into the write - only sorter in an arbitrary order by some algorithm . After an explicit switch , the ﬁlled data structure becomes read - only and the elements are provided as a lexicographically non - decreasing stream which can be rewound at any time . While a sorter is functionally equivalent to ﬁlling , sorting and reading back an EM vector , the restricted access model reduces constant factors in the implementation’s runtime and I / O - complexity [ 3 ] . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 9 v 3 π 1 ( 1 ) v 1 π 1 ( 2 ) v 2 π 1 ( 3 ) v 5 π 1 ( 4 ) v 4 π 1 ( 5 ) v 6 π 1 ( 6 ) v 6 π 2 ( 1 ) v 3 π 2 ( 2 ) v 5 π 2 ( 3 ) v 1 π 2 ( 4 ) v 2 π 2 ( 5 ) v 4 π 2 ( 6 ) current trade , v 1 v 2 new edge produced : { } h round : 2 , slot : 4 , neighbour : v 2 i Figure 3 During the trade j = 1 , i 1 = 3 , i 2 = 4 the edge { v 1 , v 2 } is produced ; the arrows indicate positions considered as successors . Since v 1 and v 2 are already processed in round j = 1 , π 2 is used to compute the successor . Then , the message is sent to v 1 in round 2 as v 1 is processed before v 2 . keep one directed representation of an undirected edge . As an invariant , an edge is always placed in the neighbourhood of the incident node traded before the other . To speed - up these insertions , IM - CB maintains unordered neighbourhood buﬀers . IM - CB does not forward successor information , but rather stores S in a contiguous block of memory . The algorithm additionally maintains the vector S idx [ 1 . . . n ] where the i - th entry points to the current successor of node v i . Once this trade is reached , the pointer is incremented giving the next successor . (cid:73) Lemma 6 . For a random trade sequence T of length ‘ , IM - CB has an expected running time of O ( n + ‘ + m + ‘m / n ) . In the case of r many global trades ( each consisting of n / 2 normal trades ) the running time is given by O ( n + rm ) . Proof . Refer to Appendix B for the proof . (cid:74) 4 . 3 EM - GCB : An I / O - eﬃcient Global Curveball algorithm EM - GCB builds on EM - CB and exploits the regular structure of global trades to simplify and accelerate the dependency tracking . As discussed in section 3 . 3 , a global trade can be encoded as a permutation π : [ n ] → [ n ] by interpreting adjacent ranks as trade pairs , i . e . T π = [ { v π ( 2 i − 1 ) , v π ( 2 i ) } ] n / 2 i = 1 . In this setting , a sequence of global trades is given by r permutations [ π j ] rj = 1 . The model simpliﬁes dependencies as it is not necessary to explicitly gather S and communicate successors . As illustrated in Fig . 3 , we also change the addressing scheme of messages . While EM - CB sends messages to speciﬁc nodes in speciﬁc trades , EM - GCB exploits that each node v i is actively traded only once in each round j and hence can be addressed by its position π j ( i ) . Successors can then be computed in an ad hoc fashion ; let a trade of adjacent positions i 1 < i 2 of the j - th global trade produce ( amongst others ) the edge { v x , v y } . The successor of v x ( and analogously the one of v y ) is S j , i 2 [ v x ] = ( j , π j ( x ) ) if v x is processed later in round j ( i . e . π j ( x ) / 2 > i 2 ) and otherwise S j , i 2 [ v x ] = ( j + 1 , π j + 1 ( x ) ) . Here we imply an untraded additional function π r + 1 ( x ) = x which avoids corner cases and generates an ordered edge list as a result of the r - th global trade . To reduce the computational cost of the successor computation , EM - GCB supports fast injective functions f : X → Y where [ n ] ⊆ X and [ n ] ⊆ Y . In contrast to the original permutations , their relevant image { f ( x ) | x ∈ [ n ] } may contain gaps which are simply skipped by EM - GCB . This requires minor changes in the addressing scheme ( see Appendix C ) . In practice , we use functions from the family of linear congruential maps H p where p is the smallest prime number p ≥ n : H p : = { h a , b | 1 ≤ a < p and 0 ≤ b < p } ( 1 ) h a , b ( x ) ≡ ( ax + b ) mod p , ( 2 ) 11 : 10 Randomisation of Massive Networks using Global Curveball Trades 1 k 1 k In EM In IM ( front block ) current round next round 1 z macrochunk 1 p batch the p microchunks in a batch are processed in parallel Figure 4 EM - PGCB splits each global trade into k macrochunks and maintains an external memory queue for each . Before processing a macrochunk , the buﬀer is loaded into IM and sorted , and further subdivided into z batches each consisting of p microchunks . A type ( ii ) message is visualised by the red intra - batch arrow . As detailled in Appendix D random choices from H p are well suited for EM - GCB since they are 2 - universal 9 and contain only O ( log ( n ) ) gaps . They are also bijections with an easily computable inverse h − 1 a , b that allows EM - GCB to determine the active node h − 1 a , b ( i ) traded at position i ; this operation is only performed once for each traded position . EM - GCB also supports non - invertible functions . This can be implemented with messages h h ( i ) , i i that are generated for 1 ≤ i ≤ n and delivered using TFP . 4 . 4 EM - PGCB : An I / O - eﬃcient parallel Global Curveball algorithm EM - PGCB adds parallelism to EM - GCB by concurrently executing multiple sequential trades . As in Fig . 4 , we split a global trade into microchunks each containing a similar number of node pairs and then execute a batch of p such subdivisions in parallel . The batch’s size is a compromise between intra - batch dependencies ( messages are awaited from another processor ) and overhead caused by synchronising threads at the batch’s end ( see Appendix E ) . EM - PGCB processes each microchunk similarly as in EM - CB but diﬀerentiates between messages that are sent ( i ) within a microchunk , ( ii ) between microchunks of the same batch ( iii ) and microchunks processed later . Each class is transported using an optimised data structure ( see below ) and only type ( ii ) messages introduce dependencies between parallel executions and are resolved as follows : each processor retrieves the messages that are sent to its next trade and checks whether all information required is available by comparing the number of messages to the active nodes’ degrees . If data is missing the trade is skipped and later executed by the processor that adds the last missing neighbour . For graphs with m = O ( M 2 / B ) edges 10 , we optimise the communication structure for type ( iii ) messages . Observe that EM - PGCB sends messages only to the current and the subsequent round . We partition a round into k macrochunks each consisting of Θ ( n / k ) contiguous trades . An external memory queue is used for each macrochunk to buﬀer messages sent to it ; in total , this requires Θ ( kB ) internal memory . Before processing a macrochunk , all its messages are loaded into IM , subsequently sorted and arranged such that missing messages can be directly placed to the position they are required in . This can also be overlapped with the processing of the previous macrochunk . As thoroughly discussed in Appendix E , the number k of macrochunks should be as small as possible to reduce overheads , but suﬃciently large such that all messages of a macrochunk ﬁt into main memory ( see Appendix F ) . (cid:73) Theorem 7 . EM - PGCB requires O ( r · [ sort ( n ) + sort ( m ) ] ) I / Os to perform r global trades . Proof . Observe that we can analyse each of the r rounds individually . A constant amount of auxiliary data is needed per node to provision gaps for missing data , to detect whether a 9 i . e . given one node in a single trade , the other is uniformly chosen among the remaining nodes . 10 Even with as little as 1 GiB of internal memory , several billion edges are supported . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 11 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 25 ] Maximum dependent edges ES CBU CBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 [ MaxDeg 750 ] Maximum dependent edges ES CBU CBG Figure 5 Fraction of edges still correlated as a function of the thinning parameter k for graphs with n = 2 · 10 3 nodes and degree distribution Pld ( [ a , b ) , γ ) with γ = 2 , a = 5 , and b ∈ { 25 , 750 } . The ( not thinned ) long Markov chains of edge switching ( ES ) , Curveball with uniform trades ( CBU ) and Curveball with global trades ( CBG ) contain 6000 super steps each . trade can be executed and ( if required ) to invert the permutation . This accounts for Θ ( n ) messages requiring sort ( n ) I / Os to be delivered . Using an ordinary PQ , the analysis of EM - CB ( see Lemma 5 ) carries over , requiring sort ( m ) I / Os for a global trade . (cid:74) 5 Experimental Evaluation In this section we evaluate the quality of the proposed algorithms and analyse the runtime of our C + + implementations . 11 EM - CB , IM - CB , EM - GCB are designed as modules of NetworKit [ 34 ] ; due to their superior performance , only the latter two were added to the library and are available since release 4 . 6 . EM - PGCB ’s implementation is developed separately and facilitates external memory data structures and algorithms of STXXL [ 9 ] . Intuitively , graphs with skewed degree distributions are hard instances for Curveball since it shuﬄes and reassigns the disjoint neighbours of two trading nodes . Hence , limited progress is achieved if a high - degree node trades with a low - degree node . Since our experiments support this hypothesis , we focus on graphs with powerlaw degree distributions as diﬃcult but highly relevant graph instances . Our experiments use two parameter sets : ( lin ) − The maximal possible degree scales linearly as a function of the number n of nodes . The degree distribution Pld ( [ a , b ) , γ ) is chosen as a = 10 , b = n / 20 and γ = 2 . ( const ) − The extremal degrees are kept constant . In this case the parameters are chosen as a = 50 , b = 10000 and γ = 2 . We select these conﬁgurations to be comparable with [ 15 ] where both parameter sets are used to evaluate EM - ES . The ﬁrst setting ( lin ) considers the increasing average degree of real - world networks as they grow . The second setting ( const ) approximates the degree distribution of the Facebook network in May 2011 ( refer to [ 14 ] for details ) . Runtimes are measured on the following oﬀ - the - shelf machine : Intel Xeon E5 - 2630 v3 ( 8 cores at 2 . 40GHz ) , 64GB RAM , 2 × Samsung 850 PRO SATA SSD ( 1 TB ) , Ubuntu Linux 16 . 04 , GCC 7 . 2 . 5 . 1 Mixing of Edge - Switching , Curveball and Global Curveball We are not aware of any practical theoretical bounds on the mixing time of Markov chains of Curveball , Global Curveball or edge switching ( see section 3 ) . Hence , we quantitatively study the progress made by Curveball trades compared to edge switching and approximate the 11 Code used for the presented benchmarks can be found at our fork https : / / github . com / hthetran / networkit ( IM - CB and EM - CB ) and https : / / github . com / massive - graphs / extmem - lfr ( EM - PGCB ) . 11 : 12 Randomisation of Massive Networks using Global Curveball Trades mixing time of the underlying Markov chains by a method developed in [ 31 ] . This criterion is a more sensitive proxy to the mixing time than previously used alternatives , such as the local clustering coeﬃcient , triangle count and degree assortativity [ 14 ] . Intuitively , one determines the number of Markov chain steps required until the correlation to the initial state decays . Starting from an initial graph G 0 , the Markov chain is executed for a large number of steps , yielding a sequence ( G t ) t ≥ 0 of graphs evolving over time . For each occurring edge e , we compute a boolean vector ( Z e , t ) t ≥ 0 where a 1 at position t indicates that e exists in graph G t . We then derive the k - thinned series ( Z ke , t ) t ≥ 0 only containing every k - th entry of the original vector ( Z e , t ) t ≥ 0 and use k as a proxy for the mixing time . To determine if k Markov chain steps suﬃce for edge e to lose the correlation to the initial graph , the empirical transition probabilities of the k - thinned series ( Z ke , t ) t ≥ 0 are ﬁtted to both an independent and a Markov model respectively . If the independent model is a better ﬁt , we deem edge e to be independent . The results presented here consider only small graphs due to the high computational cost involved . However , additional experiments suggest that the results hold for graphs at least one order of magnitude larger which is expected as powerlaw distributions are scale - free . We compare a sequence of uniform ( single ) trades , global trades and edge switching and visually align the results of these schemes by deﬁning a super step . Depending on the algorithm a super step corresponds to either a single global trade , n / 2 uniform trades or m edge - swaps . Comparing n / 2 uniform trades with a global trade seems sensible since a global trade consists of exactly n / 2 single trades , furthermore randomising with n / 2 single trades considers the state of 2 m edges which is also true for m edge - swaps . The alignment accounts for the fact that a single Curveball Markov chain step may execute multiple neighbour switches , thus easily outperforming ESMC in a step - by - step comparison . Fig . 5 contains a selection of results obtained for small powerlaw graph instances using this method ( see Appendix G . 1 for the complete dataset ) . Progress is measured by the fraction of edges that are still classiﬁed as correlated , i . e . the faster a method approaches zero the better the randomisation . We omit an in - depth discussion of uniform trades and rather focus on global trades which consistently outperform the former ( cf . section 3 . 2 ) . In all settings ESMC shows the fastest decay . The gap towards global trades growths temporarily as the maximal degree is increased which is consistent with our initial claim that skewed degree distributions are challenging for Curveball . The eﬀect is however limited and in all cases performing 4 global trades for each edge switching super step gives better results . This is a pessimistic interpretation since typically 10 m to 100 m edge switches are used to randomise graphs in practice ; in this domain global trades perform similarly well and 20 global trades consistently give at least the quality of 10 m edge switches . 5 . 2 Runtime performance benchmarks We measure the runtime of the algorithms proposed in section 4 and compare them to two state - of - the - art edge switching schemes ( using the authors’ C + + implementations ) : VL - ES is a sequential IM algorithm with a hashing - based data structure optimised for eﬃcient neighbourhood queries and updates [ 38 ] . To achieve comparability , we removed connectivity tests , ﬁxed memory management issues , and adopted the number of swaps . EM - ES is an EM edge switching algorithm and part of EM - LFR ’s toolchain [ 15 ] . We carry out experiments using the ( const ) and ( lin ) parameter sets , and limit the problem sizes for internal memory algorithms to avoid exhaustion of the main memory . For C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 13 10 6 10 7 10 8 10 9 10 10 Number m of edges 10 2 10 3 T i m e / e d g e / s up e r s t e p [ n s ] Parameter set : ( const ) EM - CB EMG - CB IM - CB VL - ES EM - ES EM - PGCB 10 5 10 6 10 7 10 8 10 9 10 10 Number m of edges 10 2 10 3 Parameter set : ( lin ) Figure 6 Runtime per edge and super step ( global trade or m edge swaps ) of the proposed algorithms IM - CB , EM - CB and EM - PGCB compared to state - of - the - art IM edge switching VL - ES and EM edge switching EM - ES . Each data point is the median of S ≥ 5 runs over 10 super steps each . The left plot contains the ( const ) - parameter set , the right one ( lin ) . Observe that the super steps of diﬀerent algorithms advance the randomisation process at diﬀerent speeds ( see discussion ) . each data point we carry out 10 super steps ( i . e . 10 global trades or 10 m edge swaps ) on a graph generated with Havel - Hakimi from a random powerlaw degree distribution . Figure 6 presents the walltime per edge and super step including pre - computation 12 required by the algorithms but excluding the initial graph generation process . The plots include ( mostly small ) errorbars corresponding to the unbiased estimation of the standard deviation of S repetitions per data point ( with diﬀerent random seeds ) . The number k of macrochunks does not signiﬁcantly aﬀect EM - PGCB ’s performance for small graphs due to comparably high synchronisation cost . In contrast , adjusting k for larger graphs can noticeably increase the performance of EM - PGCB . We thus experimentally determined the value k = 32 for both ( const ) and ( lin ) with n = 10 7 nodes and use that value for all other instances . All Curveball algorithms outperform their direct competitors signiﬁcantly — even if we pessimistically executed two global trades for each edge switching super step ( see section 5 . 1 ) . For large instances of ( const ) EM - PGCB carries out one super step 14 . 3 times faster than EM - ES and 5 . 8 times faster for ( lin ) . EM - PGCB also shows a superior scaling behaviour with an increasing speed - up for larger graphs . Similarly , IM - CB processes super steps up to 6 . 3 times faster than VL - ES on ( const ) and 5 . 1 times on ( lin ) . On our test machine , the implementation of IM - CB outperforms EM - CB in the internal memory regime ; EM - GCB is faster for large graphs . As indicated in Fig . 10 ( Appendix G . 2 ) , this changes qualitatively for machines with slower main memory and smaller cache ; on such systems the unstructured I / O of IM - CB and VL - ES is more signiﬁcant rendering EM - CB and EM - GCB the better choice with a speed - up factor exceeding 8 compared to VL - ES . 6 Conclusion and outlook We applied global Curveball trades to undirected graphs simplifying the algorithmic treatment of dependencies and showed that the underlying Markov chain converges to a uniform distribution . Experimental results show that global trades yield an improved quality compared to a sequence of uniform trades of the same size . We presented IM - CB and EM - CB , the ﬁrst eﬃcient algorithms for Simple Undirected 12 For VL - ES we report only the swapping process and the generation of the internal data structures . 11 : 14 Randomisation of Massive Networks using Global Curveball Trades Curveball algorithms ; they are optimised for internal and external memory respectively . Our I / O - eﬃcient parallel algorithm EM - PGCB exploits the properties of global trades and executes a super step 14 . 3 times faster than the state - of - the - art edge switching algorithm EM - ES ; for IM - CB we demonstrate speed - ups of up to 6 . 3 ( in a conservative comparison the speed - ups should be halved to account for the diﬀerences in mixing times of the underlying Markov chains ) . The implementations of all three algorithms are freely available and are in the process of being incorporated into EM - LFR and considered for NetworKit . Acknowledgments We thank the anonymous reviewers for their many insightful comments and suggestions . References 1 A . Aggarwal , J . Vitter , et al . The input / output complexity of sorting and related problems . Communications of the ACM , 31 ( 9 ) : 1116 – 1127 , 1988 . doi : 10 . 1145 / 48529 . 48535 . 2 L . Arge . The buﬀer tree : A new technique for optimal I / O - algorithms , pages 334 – 345 . Springer Berlin Heidelberg , 1995 . doi : 10 . 1007 / 3 - 540 - 60220 - 8 _ 74 . 3 A . Beckmann , R . Dementiev , and J . Singler . Building a parallel pipelined external memory algorithm library . In IPDPS’09 , 2009 . doi : 10 . 1109 / IPDPS . 2009 . 5161001 . 4 C . J . Carstens . Proof of uniform sampling of binary matrices with ﬁxed row sums and column sums for the fast curveball algorithm . Physical Review E , 91 : 042812 , 2015 . 5 C . J . Carstens . Topology of Complex Networks : Models and Analysis . PhD thesis , RMIT University , January 2016 . 6 C . J . Carstens , A . Berger , and G . Strona . Curveball : a new generation of sampling al - gorithms for graphs with ﬁxed degree sequence . 2016 . arXiv : 1609 . 05137 . 7 J . L . Carter and M . N . Wegman . Universal classes of hash functions . Journal of computer and system sciences , 18 ( 2 ) : 143 – 154 , 1979 . 8 G . W . Cobb and Y . - P . Chen . An application of markov chain monte carlo to community ecology . The American Mathematical Monthly , 110 ( 4 ) : 265 – 288 , 2003 . 9 R . Dementiev , L . Kettner , and P . Sanders . STXXL : standard template library for XXL data sets . Software : Practice and Experience , 38 ( 6 ) : 589 – 637 , 2008 . doi : 10 . 1002 / spe . 844 . 10 R . B . Eggleton and D . A . Holton . Simple and multigraphic realizations of degree sequences , pages 155 – 172 . Springer Berlin Heidelberg , 1981 . doi : 10 . 1007 / BFb0091817 . 11 P . Erdős and A . Rényi . On random graphs I . Publicationes Mathematicae Debrecen , 1959 . 12 C . Greenhill . A polynomial bound on the mixing time of a markov chain for sampling regular directed graphs . The Electronic Journal of Combinatorics , 18 ( 1 ) : P234 , 2011 . 13 C . Greenhill . The switch markov chain for sampling irregular graphs : Extended abstract . In Proceedings of SODA ’15 , pages 1564 – 1572 , 2015 . 14 M . Hamann , U . Meyer , M . Penschuck , H . Tran , and D . Wagner . I / O - eﬃcient generation of massive graphs following the LFR benchmark . 2017 . arXiv : 1604 . 08738 . 15 M . Hamann , U . Meyer , M . Penschuck , and D . Wagner . I / O - eﬃcient generation of massive graphs following the LFR benchmark . In ALENEX , 2017 . doi : 10 . 1137 / 1 . 9781611974768 . 16 F . Iorio , M . Bernardo - Faura , A . Gobbi , T . Cokelaer , G . Jurman , and J . Saez - Rodriguez . Eﬃcient randomization of biological networks while preserving functional characterization of individual nodes . BMC bioinformatics , 17 ( 1 ) : 542 , 2016 . 17 S . Itzkovitz , R . Milo , N . Kashtan , G . Ziv , and U . Alon . Subgraphs in random networks . Physical review E , 68 : 026127 , Aug 2003 . doi : 10 . 1103 / PhysRevE . 68 . 026127 . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 15 18 A . Lancichinetti and S . Fortunato . Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities . Phys . Rev . E , 80 : 016118 , Jul 2009 . doi : 10 . 1103 / PhysRevE . 80 . 016118 . 19 A . Lancichinetti , S . Fortunato , and F . Radicchi . Benchmark graphs for testing community detection algorithms . Phys . Rev . E , 78 : 046110 , 2008 . doi : 10 . 1103 / PhysRevE . 78 . 046110 . 20 D . A . Levin , Y . Peres , and E . L . Wilmer . Markov chains and mixing times . American Mathematical Society , Providence , Rhode Island , 2009 . 21 A . Maheshwari and N . Zeh . A Survey of Techniques for Designing I / O - Eﬃcient Algorithms , pages 36 – 61 . Springer Berlin Heidelberg , 2003 . doi : 10 . 1007 / 3 - 540 - 36574 - 5 _ 3 . 22 P . Massart . Concentration inequalities and model selection , volume 6 . Springer , 2007 . 23 U . Meyer , P . Sanders , and J . Sibeyn . Algorithms for Memory Hierarchies : Advanced Lec - tures . Springer Berlin Heidelberg , 2003 . doi : 10 . 1007 / 3 - 540 - 36574 - 5 . 24 C . G . M . Mihail and E . Zegura . The markov chain simulation method for generating connected power law random graphs . In Proceedings of ALENEX ’03 . SIAM , 2003 . 25 R . Milo , N . Kashtan , S . Itzkovitz , M . E . J . Newman , and U . Alon . On the uniform generation of random graphs with prescribed degree sequences . 2003 . arXiv : cond - mat / 0312028 . 26 M . Molloy and B . Reed . A critical point for random graphs with a given degree sequence . Random Struct . Algorithms , 6 ( 2 / 3 ) : 161 – 179 , 1995 . 27 M . E . J . Newman . The Structure and Function of Complex Networks . SIAM Review , 45 ( 2 ) : 167 – 256 , 2003 . doi : 10 . 1137 / S003614450342480 . 28 M . E . J . Newman , S . H . Strogatz , and D . J . Watts . Random graphs with arbitrary degree distributions and their applications . Phys . Rev . E , 64 : 026118 , Jul 2001 . doi : 10 . 1103 / PhysRevE . 64 . 026118 . 29 R . Pagh . Basic external memory data structures , pages 36 – 61 . Springer Berlin Heidelberg , 2003 . doi : 10 . 1007 / 3 - 540 - 36574 - 5 _ 3 . 30 J . Ray , A . Pinar , and C . Seshadhri . Are We There Yet ? When to Stop a Markov Chain while Generating Random Graphs , pages 153 – 164 . Springer Berlin Heidelberg , 2012 . doi : 10 . 1007 / 978 - 3 - 642 - 30541 - 2 _ 12 . 31 J . Ray , A . Pinar , and C . Seshadhri . A stopping criterion for markov chains when generating independent random graphs . J . of Compl . Net . , 3 ( 2 ) , 2015 . doi : 10 . 1093 / comnet / cnu041 . 32 W . E . Schlauch , E . Á . Horvát , and K . A . Zweig . Diﬀerent ﬂavors of randomness : comparing random graph models with ﬁxed degree sequences . Social Network Analysis and Mining , 5 ( 1 ) : 1 – 14 , 2015 . doi : 10 . 1007 / s13278 - 015 - 0267 - z . 33 W . E . Schlauch and K . A . Zweig . Inﬂuence of the null - model on motif detection . In ASONAM’15 , NY , USA , 2015 . ACM . doi : 10 . 1145 / 2808797 . 2809400 . 34 C . L . Staudt , A . Sazonovs , and H . Meyerhenke . NetworKit : A tool suite for large - scale complex network analysis . Network Science , 4 ( 04 ) , 2016 . doi : 10 . 1017 / nws . 2016 . 20 . 35 S . H . Strogatz . Exploring complex networks . Nature , 410 ( 6825 ) : 268 , 2001 . 36 G . Strona , D . Nappo , F . Boccacci , S . Fattorini , and J . San - Miguel - Ayanz . A fast and unbiased procedure to randomize ecological binary matrices with ﬁxed row and column totals . Nature Communications , 5 : 4114 – , June 2014 . doi : 10 . 1038 / ncomms5114 . 37 N . D . Verhelst . An eﬃcient MCMC algorithm to sample binary matrices with ﬁxed mar - ginals . Psychometrika , 73 ( 4 ) : 705 – 728 , 2008 . 38 F . Viger and M . Latapy . Fast generation of random connected graphs with prescribed degrees . February 2005 . Source code available at https : / / www - complexnetworks . lip6 . fr / ~ latapy / FV / generation . html . arXiv : cs / 0502085 . 11 : 16 Randomisation of Massive Networks using Global Curveball Trades A Appendix : EM - CB Proof of Lemma 5 . As in Alg . 1 , EM - CB scans T and E during preprocessing thereby trig - gering O ( scan ( ‘ ) + scan ( m ) ) I / Os . It also involves sorters SorterTtoV and SorterDepChain as well as priority queues PqVtoV and PqTtoT transporting O ( ‘ ) , O ( ‘ ) , O ( n ) and O ( n ) messages respectively . Hence preprocessing incurs O ( sort ( ‘ ) + sort ( n ) + scan ( m ) ) I / Os . During the i - th trade O ( deg ( u i ) + deg ( v i ) ) messages are retrieved shuﬄed and redistributed causing O [ sort ( deg ( u i ) + deg ( v i ) ) ] I / Os . The bound can be improved to O ( ( deg ( u i ) + deg ( v i ) ) / B log M / B ( m / B ) ) by observing that O ( m ) items are stored in the PQ at any time . For a worst - case analysis we set deg ( u i ) = deg ( v i ) = d max yielding the ﬁrst claim . In case of r global trades , preprocessing can be performed in r chunks of n / 2 trades each . By arguments similar to the previous analysis , this yields an I / O complexity of O ( r sort ( n ) + r scan ( m ) ) . For the main phase , the above analysis tightens to O ( r sort ( m ) ) using the fact that a single global trade targets each edge at most twice . (cid:74) B Appendix : IM - CB Algorithm 2 : IM - CB as detailled in section 4 . 2 . Data : Trade sequence T , simple graph G / / Compute S : First count how often a node is active , then store when 1 S idx [ 1 . . . n + 1 ] ← 0 2 foreach { u , v } ∈ T do 3 S idx [ u ] ← S idx [ u ] + 1 ; S idx [ v ] ← S idx [ v ] + 1 4 S begin [ i ] ← 1 + P i − 1 j = 1 S idx [ j ] ∀ 1 ≤ i ≤ n + 1 / / Exclusive prefix sum with stop marker 5 copy S idx ← S begin 6 Allocate S [ 1 . . . 2 ‘ ] 7 foreach t i = { u i , v i } ∈ T for increasing i do 8 S [ S idx [ u i ] ] ← i ; S idx [ u i ] ← S idx [ u i ] + 1 9 S [ S idx [ v i ] ] ← i ; S idx [ v i ] ← S idx [ v i ] + 1 10 reset S idx ← S begin 11 τ v i : = if ( S idx [ i ] = = S begin [ i + 1 ] ) then ∞ else S [ S idx [ i ] ] / / Short - hand to read successor / / Fill A G 12 A begin [ i ] ← 1 + P i − 1 j = 1 deg ( v j ) ∀ 1 ≤ i ≤ n + 1 / / Exclusive prefix sum with stop marker 13 copy A idx ← A begin 14 Allocate A G [ 1 . . . 2 m ] 15 foreach { a , b } ∈ E do 16 if τ a ≤ τ b then push b into A G ( a ) : A G [ A idx [ a ] ] ← b ; A idx [ a ] ← A idx [ a ] + 1 else push a into A G ( b ) : A G [ A idx [ b ] ] ← a ; A idx [ b ] ← A idx [ b ] + 1 / / Trade 17 foreach trade t i = ( u , v ) ∈ T for increasing i do 18 Gather neighbours A G ( u ) , A G ( v ) from A G using A begin 19 Reset A idx [ u ] ← A begin [ u ] , A idx [ v ] ← A begin [ v ] 20 Advance S idx [ u ] and S idx [ v ] , s . t . τ u and τ v gets next trades 21 Randomly reassign disjoint neighbours , yielding new neighbours A u and A v . 22 foreach ( a , b ) ∈ ( { u } × A 0 G ( u ) ) ∪ ( { v } × A 0 G ( v ) ) do / / Push node edge into A G ; same as line 15 23 if τ a < τ b then Push b in A G ( a ) else Push a in A G ( b ) C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 17 Proof of Lemma 6 . As detailled in Alg . 2 , the computation of S [ · ] and its auxiliary structures involves scanning over T and V resulting in O ( n + ‘ ) operations . Inserting all edges into A G requires another O ( n + m ) steps . The i - th trade takes O ( deg ( v i ) + deg ( u i ) ) time to retrieve the input edges and distribute the new states . To compute the disjoint neighbours , we insert A u i into a hash set and subsequently issue one existence query for each neighbour in A v i ; this takes expected time O ( deg ( v i ) + deg ( u i ) ) . Since T ’s constituents are drawn uniformly at random , we estimate the neighbourhood sizes as E [ deg ( u i ) ] = E [ deg ( v i ) ] = m / n yielding the ﬁrst claim . In case of r global trades , T consists of r groups with n / 2 trades targeting all nodes each . Hence , trading requires time r P i ( deg ( u i ) + deg ( v i ) ) = r P v ∈ V deg ( v ) = O ( rm ) . (cid:74) C Appendix : EM - GCB Recall that a global trade can be encoded by a permutation π : V → V on the nodes or equivalently on the node indices ( see section 3 . 2 ) . Consequently , generating a uniform random permutation on [ n ] yields a uniform random global trade . Injective hash - functions have several computational advantages and can substitute the random permutation : (cid:73) Deﬁnition 8 ( Relaxed global trade ) . Let h : [ n ] → N be an injective hash - function and [ a i ] ni = 1 be the image [ h ( i ) ] ni = 1 in sorted order . Further let T h = [ t i ] n / 2 i = 1 where t i trades the nodes with indices h − 1 ( a 2 i − 1 ) and h − 1 ( a 2 i ) . Hence h implies the global trade T h analogously to a permutation . In this setting , similar to using permutations , a sequence T of global trades is given by r hash - functions T = [ h i ] ri = 1 . Again , EM - GCB uses the fact that each node v i is actively traded only once in each round j and can then be addressed by h j ( i ) ( instead of previously π j ( i ) ) . D Linear congruential maps We use linear congruential maps as fast injective hash - functions to model global trades for EM - PGCB . In this section , some of their useful properties are shown . We use the notation Z p = { 0 , 1 , . . . , p − 1 } and Z ∗ p = { 1 , . . . , p − 1 } for p prime and implicitly use 0 ≡ p mod p . Additionally for a map h : X → Y we denote the image of h as im ( h ) = { h ( x ) : x ∈ X } . (cid:73) Deﬁnition 9 ( 2 - universal hashing ) . Let H be an ensemble of maps from X to Y and h be uniformly drawn from H . For ﬁnite X and Y we call the ensemble H 2 - universal if for any two distinct x 1 , x 2 ∈ X and any two y 1 , y 2 ∈ Y and uniform random h ∈ H P ( h ( x 1 ) = y 1 ∧ h ( x 2 ) = y 2 ) = | Y | − 2 . (cid:73) Proposition 10 . A linear congruential map h a , b : Z p → Z p , x 7→ ax + b mod p for a 6 = 0 and p prime is a bijection . Proof . The translation τ b ( x ) = x + b mod p and multiplication χ a ( x ) = ax mod p is injective for all a ∈ Z ∗ p and b ∈ Z p . Then , the composition h a , b = ( χ a ◦ τ b ) is also injective and the inverse is given by h − 1 a , b ( y ) = a − 1 ( y − b ) mod p . (cid:74) (cid:73) Lemma 11 . The ensemble H = { h a , b : a ∈ Z ∗ p , b ∈ Z p } is 2 - universal . Proof . see Proposition 7 of [ 7 ] . (cid:74) 11 : 18 Randomisation of Massive Networks using Global Curveball Trades h π h π − 1 0 1 2 3 4 5 1 5 2 6 3 0 0 1 2 3 5 6 introduces gap at 4xdd 0 1 2 3 4 5 0 1 2 3 4 5 6 Figure 7 The sorted rank - map for n = 6 and h : [ n ] → Z 7 , x 7→ 4 x + 1 . For the set { 0 , 1 , 2 , 3 } the sorted rank - map π is just the identity . In contrast for x ∈ { 4 , 5 } the value x is mapped to π ( x ) = x + 1 . The input size will most likely not be prime but linear congruential maps can still be used as injective maps since by the prime number theorem the next larger prime to a number n is on average O ( ln ( n ) ) larger . Additionally , since [ n ] is a subset of Z p the 2 - universality also already applies to distinct keys x 1 , x 2 ∈ [ n ] . The small diﬀerence in n and p brings an additional feature we exploit while sending type ( ii ) messages ( see Proposition 16 ) : given a lower and upper bound on a hashed value with their respective ranks , one can estimate the rank of an element lying between those bounds . (cid:73) Deﬁnition 12 ( Sorted rank - map ) . Let n ∈ N . Further , let h : [ n ] → N be an injective map restricted to [ n ] and π h be the permutation that sorts [ h ( i ) ] ni = 1 ascendingly . Denote with π = ( h ◦ π h ) : [ n ] → im ( h ) the sorted rank - map . It is clear that π is bijective , and π − 1 remaps a mapped value to its rank in im ( h ) , see Fig . 7 . (cid:73) Remark 13 . The sorted rank - map π can only shift the original values and is thus mono - tonically increasing , see Fig . 7 . The shift in value is given by π ( x ) − x and is monotonically increasing , too . By applying π we introduce gaps in the set Z p from [ n ] , refer to Fig . 7 . (cid:73) Proposition 14 . Let n ∈ N and p ≥ n be a prime number . Further , let h : [ n ] → Z p be a linear congruential map and π be its sorted rank - map . If we want to compute the rank of y ∈ im ( h ) and know x , x 0 ∈ [ n ] where h ( x ) ≤ y ≤ h ( x 0 ) then we can bound the rank π − 1 ( y ) of y by using the shifts of x and x 0 : y − ( π ( x 0 ) − x 0 ) ≤ π − 1 ( y ) ≤ y − ( π ( x ) − x ) . Proof . The sorted rank - map π is by deﬁnition monotone increasing , see also Fig . 7 . It follows that π ( x ) = x + k , π ( x 0 ) = x 0 + k 0 and k ≤ k 0 for some k , k 0 ∈ N . By monotonicity π ( π − 1 ( y ) ) = π − 1 ( y ) + s for s ∈ { k , . . . , k 0 } , resulting in inequalities π − 1 ( y ) + k ≤ y , y ≤ π − 1 ( y ) + k 0 . By subtracting k and k 0 on both sides , the claim follows . (cid:74) With Proposition 14 we can reduce the number of candidates to search in . This is especially useful , when working on a smaller contiguous part of the data ( see EM - PGCB , section 4 . 4 ) . (cid:73) Example 15 . Let n and h be given from Fig . 7 . It is clear that the hashed - values are given by im ( h ) = { 0 , 1 , 2 , 3 , 5 , 6 } . Suppose the rank of 2 in im ( h ) has to be computed given the outer values e . g . that π ( 0 ) = 0 and π ( 5 ) = 6 . Then by Propositon 14 2 − ( π ( 5 ) − 5 ) ≤ π − 1 ( 2 ) ≤ 2 − ( π ( 0 ) − 0 ) , 1 ≤ π − 1 ( 2 ) ≤ 2 . Thus , the rank of 2 in im ( h ) is either 1 or 2 . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 19 E Appendix : EM - PGCB EM - PGCB achieves parallelism by performing multiple trades concurrently . In contrast to EM - GCB , rather than only retrieving the ﬁrst two necessary adjacency rows for the single next trade , a whole chunk of data is loaded and maintained in IM - CB ’s adjacency list to store neighbours for a subset of nodes . The adjacency list is further used as a way to transport messages within a loaded macrochunk . Observe that at most 2 m many messages are sent in a global trade round since only neighbourhood information is forwarded . The idea is to split the messages into chunks of size M = cM where c ∈ ( 0 , 1 ) which can be processed in IM . For this , EM - PGCB loads and proceesses all messages targetted to the next n / k nodes for a constant k and performs the corresponding trades concurrently . This subdivides the messages and its processing into k macrochunks . If a macrochunk is too large , it cannot be fully kept in IM resulting in unstructured I / O in the trading process . The choice of k should therefore additionally consider the variance . An analysis on the size of the macrochunks is given in section F . E . 1 Data structure for message transportation Recall in subsection 4 . 4 that each macrochunk is subdivided into many microchunks and processed in batches . During the trading process EM - PGCB has to diﬀerentiate between messages that are sent ( i ) within a microchunk , ( ii ) between microchunks of the same batch ( iii ) and microchunks processed later . To support both type ( i ) and type ( ii ) messages we organise the messages of the current macrochunk in an adjacency vector data structure similar to IM - CB . Instead of forwarding these messages in a TFP - fashion , EM - PGCB inserts them directly into the adjacency data structure . We rebuild the data structure for each macrochunk requiring the degrees of the n / k loaded nodes to leave gaps if messages are missing . In a preprocessing step we provide EM - PGCB with this information by inserting messages h h r ( v ) , deg ( v ) , v i into a separate priority queue . Initialising the adjacency vector can now be done by loading the degrees for the next n / k targets and reserving for each target h r ( v ) the necessary deg ( v ) slots . Messages h r , h r ( v ) , x i targetted to the node v can then be inserted in an unstructured fashion in IM . This can be done in parallel for all targets in the macrochunk : ﬁrst the retrieved messages are sorted in parallel and then accessed concurrently after determining delimiters by a parallel preﬁx sum over the message counts . For a trade t = { u i , v i } of targets h r ( u i ) and h r ( v i ) the assigned processor can determine if the t is tradable by checking whether deg ( u i ) and deg ( v i ) match the number of available messages . After performing the trade , we forward the updated adjacency information . Assume that the edge { u i , x } has to be send to a later trade in the same global trade . ( i ) If x is traded within the processed microchunk there is no synchronisation required and u i can be inserted into the row corresponding to target h r ( x ) . ( ii ) If x is traded within the currently processed batch the processor has to insert u i into the row corresponding to target h r ( x ) with synchronisation . This yields a data dependency in the parallel execution . Inferring if the trade for x belongs to the current batch can be done by comparing h r ( x ) to the maximum target of the batch . ( iii ) If x is traded in a later microchunk , it either belongs to the same macrochunk or a later one ( of the same global trade ) . For the former EM - PGCB proceeds similar to type ( ii ) without processing foreign trades . In the latter case EM - PGCB inserts a message h r , h r ( x ) , u i i into the priority queue . Addressing the adjacency row of a target h r ( u ) can be done by computing the rank of h r ( u ) in the retrieved n / k targets . Since the separate priority queue provides all loaded targets 11 : 20 Randomisation of Massive Networks using Global Curveball Trades by messages h h r ( u ) , deg ( u ) , u i , we can perform a binary search and obtain the rank in time O ( log ( n / k ) ) . For linear congruential maps ( section D ) we can do better : (cid:73) Proposition 16 . Let h be a linear congruential map . Then , heuristically computing the row ( rank ) corresponding to h ( u ) requires O ( log log n ) time . Proof . The next larger prime p to n is heuristically ln ( n ) larger than n . After loading all messages h h ( u ) , deg ( u ) , u i for the current macrochunk the smallest and largest hashed value of the current macrochunk are known . By subtracting both values by the already processed number of targets and using Proposition 14 the search space can be reduced to O ( log n ) elements . Application of a binary search on the remaining elements yields the claim . (cid:74) As already mentioned , if a trade has not received all its required messages , the assigned processor cannot perform the trade yet and therefore skips it . This can only happen within a batch when type ( ii ) messages occur . In section F we argue that this happens rarely . The processor that inserts the last message for that particular trade will perform it instead . E . 2 Improvements for type ( iii ) messages Messages inserted into the priority queue need to contain the round - id to process global trades separately . Observe however that in a sequence of global trades , messages are only send to the current and subsequent round . We therefore modify our data structure , omitting the round from every message reducing the memory footprint signiﬁcantly . Recall that , as an optimisation for m = O ( M 2 / B ) edges , EM - PGCB uses external memory queues for each of the k macrochunks of both global trade rounds . A previously generated message h r , h r ( u ) , x i is now inserted into the corresponding queue containing messages for h r ( u ) . Again , in a preprocessing step EM - PGCB determines for each queue its target range . For this , the separate priority queue containing messages h h r ( u ) , deg ( u ) , u i is read while extracting every ( n / k ) - th target ( retrieving every element results in a sequence of sorted messages ) . This enables the computation of the correct queue for h r ( u ) with a binary search in time O ( log ( k ) ) . Naturally since both the current and subsequent round are relevant , EM - PGCB employs k external memory queues for each . If a global trade is ﬁnished , the k EM queues of the currently processed and ﬁnished round can be reused for the next global trade . EM - PGCB ’s pseudo code can be found in Algorithm 3 . F Analysis of EM - PGCB F . 1 Macrochunk size As already mentioned , the number of incoming messages may exceed the size of the internal memory M , since we partition the nodes into chunks which then may receive a diﬀerent number of messages . Therefore some analysis on the size of the maximum macrochunk is necessary . Denote with N ( µ , σ 2 ) the distribution of a Gaussian r . v . with mean µ and variance σ 2 . A macrochunk holds the sum of n / k many iid degrees and is thus approximately Gaussian with mean 2 m / k and variance n / k · Var ( D ) where D is distributed to the underlying degree distribution . This approximation gets better for larger values of n / k and is thus a suitable approximation for large graphs . Denote with S 1 , . . . , S k the sizes of all k macrochunks . When determining a suitable choice of k , it is necessary to consider both the mean and the variance of the maximum macrochunk max 1 ≤ i ≤ k S i . The largest macrochunk may receive C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 21 Algorithm 3 : EM - PGCB as detailled in section 4 . 4 and section E . Data : Trade sequence T = [ h i ] ri = 1 , simple graph G = ( V , E ) as edge list E Result : Randomised graph G 0 / / Initialisation : provide auxiliary info and initialise with edges 1 foreach node u ∈ V do 2 Send h h 1 ( u ) , deg ( u ) , u i via AuxInfoToTarget / / Send node and degree to target 3 Sort AuxInfoToTarget lexicographically 4 Scan AuxInfoToTarget and determine bounds for the k queues 5 foreach edge e = [ u , v ] in E do 6 Insert e according to h 1 into one of the corresponding queues / / Execution : Process rounds and macrochunks 7 for round R = 1 , . . . , r do 8 for macrochunk K = 1 , . . . , k do 9 Retrieve auxiliary data h h R ( u ) , deg ( u ) , u i from AuxInfoToTarget 10 Load and sort messages of the K - th queue 11 Insert the messages into the adjacency list A G in parallel 12 for batch B = 1 , . . . , z do 13 pardo the i - th processor works on the i - th microchunk of batch B 14 for a trade t = { u , v } do 15 Retrieve A u and A v from A G 16 With deg ( u ) and deg ( v ) determine whether tradable 17 if tradable then 18 Compute A 0 u and A 0 v 19 Forward each resulting edge worksteal if inserted message ﬁlls all necessary data 20 else Skip 21 if R < r then 22 Clear AuxInfoToTarget and reﬁll for h R + 1 ( repeat steps 3 to 5 ) many high - degree nodes exceeding the size of the internal memory M . We thus bound its number in Corollary 18 and Corollary 20 . (cid:73) Lemma 17 . Let Y = max 1 ≤ i ≤ k X i , where the X i are iid r . v . distributed as N ( 0 , σ 2 ) . Then , E [ Y ] ≤ σ p 2 log ( k ) . Proof . The following chain of inequalities holds e t E [ Y ] ≤ E [ e tY ] = E [ max 1 ≤ i ≤ k e tX i ] ≤ P ki = 1 E [ e tX i ] = ke t 2 σ 2 / 2 , where in order Jensen’s inequality 13 monotonicity and non - negativity of the exponential function as well as the deﬁnition of the moment generating function of a Gaussian r . v . have been applied . Taking the natural logarithm and dividing by t on both sides ( ruling out t 6 = 0 ) yields E [ Y ] ≤ log ( k ) t + tσ 2 2 , which is minimized by t = p 2 log ( k ) / σ . The above proof is a special case in a proof of [ 22 ] . (cid:74) (cid:73) Corollary 18 . Let Y = max 1 ≤ i ≤ k S i . By approximating S i with a Gaussian r . v . N i with µ = E [ S i ] and σ 2 = Var ( S i ) , one gets an approximate upper bound on Y : E [ Y ] ≈ E (cid:20) max 1 ≤ i ≤ k N i (cid:21) ≤ E [ S 1 ] + p 2 log ( k ) Var ( S 1 ) = E [ S 1 ] + r n log ( k ) 2 k Var ( D ) . 13 For a convex function f and non - negative λ i with P n i = 1 λ i = 1 follows f ( P n i = 1 λ i x i ) ≤ P n i = 1 λ i f ( x i ) . 11 : 22 Randomisation of Massive Networks using Global Curveball Trades Proof . Since max 1 ≤ i ≤ k N i is centred around µ , it is identically distributed to µ + max 1 ≤ i ≤ k N 0 i where N 0 i has the same variance but is centred around 0 . By applying Lemma 17 to max 1 ≤ i ≤ k N 0 i the claim follows , since E [ max 1 ≤ i ≤ k N i ] = µ + E [ max 1 ≤ i ≤ k N 0 i ] . (cid:74) (cid:73) Lemma 19 . Let X 1 , . . . , X k be iid and Y = max 1 ≤ i ≤ k X i . Then , Var ( Y ) ≤ k Var ( X 1 ) . Proof . For Z , Z 0 iid . E [ ( Z − Z 0 ) 2 ] = 2 Var ( Z ) holds , since E [ Z 2 − 2 ZZ 0 + Z 0 2 ] = 2 E [ Z 2 ] − 2 E [ Z ] 2 . Now , let Y 0 = max 1 ≤ i ≤ k X 0 i be an independent copy of Y and r > 0 . First , the inequality P ( | Y − Y 0 | 2 > r ) ≤ P ki = 1 P ( | X i − X 0 i | 2 > r ) is shown . We show the implication that when | Y − Y 0 | 2 > r then there exists an index i s . t . | X i − X 0 i | 2 > r . If | Y − Y 0 | 2 > r holds , then w . l . o . g . let Y = X i and Y 0 = X 0 j and Y > Y 0 s . t . | X i − X 0 j | 2 > r . By maximality the following chain of inequalities holds X i > X 0 j ≥ X 0 i . Which already implies | X i > X 0 i | > r and consequently P ( | Y − Y 0 | 2 > r ) ≤ P ( exists index i s . t . | X i − X 0 i | > r ) . Now by bounding the union , one gets P ( | Y − Y 0 | 2 > r ) ≤ P ki = 1 P ( | X i − X 0 i | 2 > r ) . At last , integrating r from 0 to ∞ yields 2 Var ( Y ) = E [ ( Y − Y 0 ) 2 ] ≤ k E [ ( X 1 − X 0 1 ) 2 ] = 2 k Var ( X 1 ) , which concludes the proof . (cid:74) (cid:73) Corollary 20 . Let Y = max 1 ≤ i ≤ k S i . Then , Var ( Y ) ≤ k Var ( S 1 ) = n Var ( D ) . Proof . This is a special case of Lemma 19 . (cid:74) The probability mass of a Gaussian r . v . is concentrated around its mean , e . g . the tails vanish very quickly , see Proposition 21 . This heuristically additionally holds true for the maximum macrochunk size ( Lemma 22 ) . (cid:73) Proposition 21 . Let X be a standard Gaussian r . v . and f ( x ) = 1 √ 2 π e − x 2 / 2 be its probability density function . Let t > 0 then it holds P ( X > t ) ≤ exp ( − t 2 / 2 ) / √ 2 π / t = O (cid:16) e − t 2 / 2 t (cid:17) . Proof . The value of P ( X > t ) equals R ∞ t 1 √ 2 π e − x 2 / 2 dx . Since the integrating variable ranges from [ t , ∞ ) then xt ≥ 1 s . t . P ( X > t ) ≤ R ∞ t xt 1 √ 2 π e − x 2 / 2 dx = 1 t e − t 2 / 2 √ 2 π . (cid:74) (cid:73) Lemma 22 . Let Y = max 1 ≤ i ≤ k N i where N i are iid standard Gaussian random variables . Then P ( Y > t ) = O (cid:0) k exp ( − t 2 / 2 ) / t (cid:1) . Proof . The claim follows by the following calculation : P ( Y > t ) = P (cid:18) max 1 ≤ i ≤ k N i > t (cid:19) = P ( ∃ i s . t . N i > t ) ≤ k X i = 1 P ( N i > t ) = O k · e − t 2 / 2 t ! . If for any random variable N i > t , then already max 1 ≤ i ≤ k N i > t , inversely if max 1 ≤ i ≤ k N i > t then there exists a N i s . t . N i > t , which shows the ﬁrst equality . After applying the union bound and Proposition 21 the claim follows . (cid:74) F . 2 Heuristic on intra - batch dependencies In EM - PGCB , if information on an edge { u , w } has to be inserted into the same batch a dependency arises . We will now argue that this happens not too often when the number of batches z is chosen suﬃciently large . (cid:73) Lemma 23 . Let B be the set of targets for a batch . Assuming uniform neighbours , the number of dependencies from B to B heuristically is (cid:0) p 2 (cid:1) 2 m k 2 z 2 p 2 . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 23 performs performs performs performs PU 1 PU 2 PU 3 PU 4 Figure 8 The arrows represent the long chain of trades that are getting work - stolen from the ﬁrst PU where p = 4 . The red marked area represents still untouched trades of the ﬁrst microchunk that will get processed after the long chain by the ﬁrst PU . Proof . By construction | B | = nkz since B is part of an equal subdivision of a macrochunk . Each individual microchunk consists of nkzp many targets for the same reason . The i - th microchunk therefore has ( p − i ) nkzp many critical targets . On average each microchunk generates deg nkzp = 2 mkzp many messages that need to be forwarded . For an edge produced by the i - th microchunk assume uniformity on the neighbours A , then V i is the number of critical messages where V i = P ni = 1 1 i ∈ A 1 i ∈ h − 1 ( B ) . Its expectation is given by E [ V i ] = n X i = 1 P ( i ∈ A ) P ( i ∈ h − 1 ( B ) ) = n deg avg n n ( p − i ) kzp n = deg avg p − i kzp . Now let the total number of messages from the i - th microchunk to B be H i . Since each microchunk holds nkzp many nodes , H i is given by E [ H i ] = n kzp E [ V i ] = 2 m ( p − i ) k 2 z 2 p 2 . By summing over all p microchunks , e . g . P pi = 1 E [ H i ] the claim follows . (cid:74) (cid:73) Example 24 . Consider Lemma 23 where m = 12 × 10 9 , k = 32 , z = 2 11 and p = 16 . The average number of messages in the batch is given by m / kz ≥ 1 . 8 × 10 5 . And Lemma 23 predicts a count of less than 4 critical messages on average in a batch . Theoretically by Lemma 23 the number of critical messages is very small if z is set to be suﬃciently large . Therefore waiting and stalling for missing messages is ineﬃcient and should be avoided . EM - PGCB thus skips a trade when it cannot be performed and is later executed by the processor that adds the last missing neighbour . However , since a work - stealing processor spends time on a trade that is possibly assigned to another microchunk , it is not working on its own . Therefore messages coming from that particular microchunk are generated later down the line . This may be especially bad when a PU performs a chain of trades that it was not originally assigned to as illustrated in Fig . 8 . Since work - stealing can only be done in a time - forward fashion , the chain length therefore is geometrically distributed ( in fact , the probability declines in each step since less targets are critical ) and is thus whp of order O ( 1 ) by Proposition 25 . (cid:73) Proposition 25 . Let X be geometrically distributed with parameter ( 1 − 1 / z 2 ) for z > 1 . Then , P ( X > t ) = 1 z 2 t = e − 2ln ( z ) t . Proof . The claim follows by P ( X > t ) = 1 / z 2 t and setting t = O ( 1 ) . (cid:74) 11 : 24 Randomisation of Massive Networks using Global Curveball Trades 0 2000 4000 6000 8000 10000 n 0 . 000 0 . 002 D i ff . f r a c t i on Diff 0 2000 4000 6000 8000 10000 n 0 . 48 0 . 49 0 . 50 F r a c t i on CB _ g CB _ u 2000 4000 6000 8000 10000 n 0 . 001 0 . 000 0 . 001 D i ff . f r a c t i on Diff 2000 4000 6000 8000 10000 n 0 . 30 0 . 35 0 . 40 F r a c t i on CB _ g CB _ u Figure 9 The average fraction of performed neighbourhood swaps of n / 2 uniform trades and a single global trade . Left : 10 - regular graphs for increasing n . Right : powerlaw graphs realised from Pld ( [ 10 , n / 20 ) , 2 ) for increasing n by the Havel - Hakimi algorithm . G Additional experimental results G . 1 Swaps performed by Curveball and Global Curveball In Fig . 9 we counted the number of neighbourhood swaps in n / 2 uniform trades and a single global trade and obtain the fraction of performed swaps to all possible swaps . These experiments are performed on a series of 10 - regular graphs and powerlaw graphs with increasing maximum degree . Both algorithms perform a similar count of swaps and suggest no systematic diﬀerence . As expected , for regular graphs the fraction of performed swaps goes to 1 / 2 for an increasing number of nodes , since with increasing n the number of common neighbours goes to zero . On the other hand the fraction of performed swaps decreases for powerlaw graphs with a higher maximum degree . G . 2 Autocorrelation time of Curveball and Edge Switching 10 5 10 6 10 7 10 8 Number m ofedges 10 2 10 3 10 4 T i m e / e d g e / s up e r s t e p [ n s ] Parameter set : ( const ) EM - CB IM - CB VL - ES 10 4 10 5 10 6 10 7 Number m ofedges 10 2 10 3 10 4 Parameter set : ( lin ) Figure 10 Runtime per edge and super step of IM - CB and EM - CB compared to state - of - the - art IM edge switching VL - ES . Each data point is the median of S ≥ 5 runs over 10 super steps each . The left plot contains the ( const ) - parameter set , the right one ( linear ) . Machine : Intel i7 - 6700HQ CPU ( 4 cores ) , 64 GB RAM , Ubuntu Linux 17 . 10 with kernel 4 . 13 . 0 - 38 . C . J . Carstens , M . Hamann , U . Meyer , M . Penschuck , H . Tran and D . Wagner 11 : 25 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 25 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 50 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 100 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 150 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 200 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 250 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 300 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 350 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 400 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 450 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 500 ] Maximum dependent edges ESCBUCBG 2 0 2 1 2 2 2 3 Thinning 10 − 2 10 − 1 1 - m i n ( i nd e p e nd e n ce r a t e ) [ MaxDeg 750 ] Maximum dependent edges ESCBUCBG Figure 11 Fraction of edges still correlated as function of the thinning parameter k for graphs with n = 2 · 10 3 nodes and degree distribution Pld ( [ a , b ) , γ ) with γ = 2 , a = 5 , and several diﬀerent values for b . The ( not thinned ) long Markov chains contain 6000 super steps each .