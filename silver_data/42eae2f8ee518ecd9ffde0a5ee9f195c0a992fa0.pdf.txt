Wisdom of Micro - Crowds in Evaluating Solutions to Esoteric Engineering Problems Nurcan Gecer Ulu , Michael Messersmith , Kosa Goucher - Lambert , Jonathan Cagan , Levent Burak Kara ∗ Department of Mechanical Engineering Carnegie Mellon University Pittsburgh , PA 15213 Email : lkara @ cmu . edu A multitude of studies in economics , psychology , political and social sciences have demonstrated the wisdom of crowds ( WoC ) phenomenon , where the collective estimate of a group can be more accurate than estimates of individuals . While WoC is observable in such domains where the participating individuals have experience or familiarity with the question at hand , it remains unclear how effective WoC is for do - mains that traditionally require deep expertise or sophisti - cated computational models to estimate objective answers . This work explores how effective WoC is for engineering de - sign problems that are esoteric in nature , that is , problems ( 1 ) whose solutions traditionally require expertise and spe - cialized knowledge , ( 2 ) where access to experts can be costly or infeasible , and ( 3 ) in which previous WoC studies with the general population have been shown to be highly ineffective . The main hypothesis in this work is that in the absence of experts , WoC can be observed in groups that consist of prac - titioners who are deﬁned to have a base familiarity with the problems in question but not necessarily domain experts . As a way to emulate commonly encountered engineering prob - lem solving scenarios , this work studies WoC with practition - ers that form micro - crowds consisting of 5 to 15 individuals , thereby giving rise to the term the wisdom of micro - crowds ( WoMC ) . Our studies on design evaluations show that WoMC produces results whose mean is in the 80th percentile or bet - ter across varying crowd sizes , even for problems that are highly non - intuitive in nature . 1 Introduction Crowdsourcing is emerging as a cost - effective , rapid ap - proach to problem solving in a variety of disciplines where the collective estimate of a group can outperform the indi - viduals , even in the presence of domain experts . This phe - nomenon is known as the wisdom of crowds ( WoC ) and has been demonstrated across a range of problem domains [ 1 – 3 ] . Traditional crowdsourcing naturally focuses on tasks that are human easy and computer hard , such as vision problems where crowds are asked to identify and label objects in large ∗ Address all correspondence to this author . sets of images [ 4 ] . In such problems , the task is typically very intuitive for humans , and thus the correct answer can be inferred from a crowd consensus . In engineering prob - lems requiring domain expertise , however , crowdsourcing has proven to be signiﬁcantly less effective , in part due to the limited number of experts in the sampled crowd [ 5 ] . This suggests that extending traditional crowdsourcing to tasks requiring expertise is non - trivial , especially if experts are scarce . As an alternative to crowdsourcing , expert collab - oration has been extensively studied [ 6 – 10 ] . However , inter - actions among group members have been shown to lead to similarity of experts [ 11 ] , which may result in experts being outperformed by diverse groups [ 12 ] . As such , it remains un - clear how conventional crowdsourcing can be made truly ef - fective for engineering design problems , especially for tasks that require expertise . As one step toward addressing this gap , this work ex - plores the effectiveness of WoC for engineering design prob - lems that are esoteric in nature . Esoteric problems are de - ﬁned as those ( 1 ) that traditionally require expertise and spe - cialized knowledge , ( 2 ) where access to experts can be costly or infeasible , and ( 3 ) in which previous WoC studies with the general population have been shown to be highly inef - fective [ 5 ] . The main hypothesis in this work is that in the absence of experts , WoC can be observed in groups that con - sist of practitioners who are deﬁned to have a base famil - iarity with the domain and the problems in question , even though no single individual may have the expertise to cor - rectly solve the problem . With this deﬁnition , experts are a subset of practitioners . However , in this work , in contrast to purely expert crowds , practitioner crowds are characterized by individual responses that exhibit both signiﬁcant accuracy ( deviation from the ground truth ) and precision errors ( vari - ation among the responses ) . This new deﬁnition and focus on practitioners stands in contrast to previous studies that explore WoC in design that rely either on the general pop - ulation crowds where experts are extremely scarce and un - known [ 5 ] , or on teams of experts [ 7 ] as the basis of crowds . Additionally , as a way to emulate commonly encountered en - gineering problem solving scenarios , this work studies WoC with practitioners that form micro - crowds consisting of 5 to 15 individuals ( rather than tens or hundreds of individuals ) , thereby giving rise to the term the wisdom of micro - crowds ( WoMC ) which is central to the presented work . As part of this study , four design assessment questions with varying levels of difﬁculty and intuitiveness were de - ployed where the participants were asked to assess the qual - ity of the candidate design solutions . Several data aggre - gation methods were developed and tested on the acquired data . The results suggest that WoMC with practitioners can indeed be observed , where the crowd estimate outperforms the individuals in the vast majority of instances . To facili - tate benchmarking , these results have been obtained for prob - lems in which there already exists an objectively true solu - tion ( i . e . , benchmark results obtained through optimization ) . As such , it could be argued that crowdsourcing is remark - ably unnecessary for such problems where solution methods already exist . However , the most signiﬁcant conclusion of the presented work is that for current or future engineering design problems where algorithmic solutions may currently not exist , small groups of practitioners may in fact provide very effective solutions . Note that , in this context the current lack of solution methods implies a lack of experts , which re - inforces the importance of practitioners . An interesting limitation of the presented work , how - ever , is that when applied to open - ended , conceptual design problems where no objectively true solution exists , the per - formance of WoMC declines signiﬁcantly . The results in - dicate that in such cases , the individuals in the crowd tend to make signiﬁcant estimation errors when benchmarked against expert ratings . Nevertheless , it remains unclear whether these estimation errors are due to the practitioners’ inability to accurately assess candidate solutions , or whether there exists issues even with expert ratings of such open - ended problems . Crowdsourcing scenarios in esoteric domains . In en - gineering , crowdsourcing is often used in the form of grand challenges to gather candidate solutions where the crowd - sourced solutions are assessed by experts juries . However , use of experts to assess these solutions may not be ideal since these grand challenges are usually created for very complex problems where experts can not solve optimally . The idea of wisdom of micro - crowds with practitioners that is pre - sented here can be an alternative to experts juries in such problems . This way , both the generation and assessment of the candidate solutions can be crowdsourced through practi - tioner crowds that are exposed to the esoteric domain . Note that , micro - crowds are composed of anonymous and non - interacting people with individually unknown expertise lev - els unlike traditional design teams . Another esoteric crowdsourcing scenario includes on - line communities in advancing ﬁelds . Thingiverse , an on - line community for 3D printing designs or GrabCAD , an on - line community for sharing CAD designs can be examples of communities in advancing esoteric ﬁelds . These communi - ties already include large groups of people that are familiar with their respective domains , practitioners . Such communi - ties can beneﬁt from the utility of our work to asses candidate designs that may in fact produce successful outcomes , which would be critically important in cases where no appropriate computational evaluation techniques exist . One such prob - lem might be planning for hybrid manufacturing [ 13 ] . While hybrid manufacturing pushes the boundaries of production processes , its use is limited to manually created suboptimal plans since there are no established computational solutions to handle such complex planning . Our studies in this paper could help selecting the best possible plan among these man - ually created candidate solutions . 2 Background Amazon Mechanical Turk ( AMT ) [ 14 ] , CrowdFlower ( CF ) [ 15 ] and Proliﬁc Academic ( ProA ) [ 16 ] are among the most prominent crowdsourcing platforms . These platforms have a wide reach and are designed to be representative of the general public consisting of diverse crowds [ 17 ] . While AMT allows the surveys to be targeted toward speciﬁc de - mographics , it is difﬁcult to identify crowds that share a pre - scribed technical background . By contrast , our work focuses on solving esoteric problems via micro - crowds that consist of practitioners . Previous studies have developed task design and re - sponse quality detection methods as a way to maximize the useful information content in crowdsourcing [ 18 , 19 ] . Ex - ample methods include the use of explicitly veriﬁable ques - tions to identify malicious users and to encourage honest responses , and task ﬁngerprinting to monitor completion time , mouse movements , key presses , and scroll movements , which can all be used as indicator attributes for detecting suspect responses [ 20 ] . Effect of incentives and competiton in crowdsourcing data quality has been investigated in [ 21 ] . The presented work uses response speed as one such indica - tor to vet data quality and monetary compensation as incen - tive . Consensus through collaboration is a widely used ap - proach in engineering [ 6 , 9 ] . However , driven by the pre - vious observations that there is a danger of expert collabo - ration to result in a singular thought pattern that could be outperformed by diverse groups [ 11 ] , this work explores WoMC with individuals who remain independent and form crowds that are more diverse than collaborating experts . Surowiecki [ 3 ] argues that one requirement for a good crowd judgement is that people’s decisions remain independent of one another . This was further validated by Lorenz et al . [ 12 ] where individuals were observed to produce collectively more accurate crowd estimations over cases where the same individuals were informed by others’ estimates . Indepen - dence of opinion ( no contact between the individuals ) con - stitutes one of the major differences between the traditional collaborative design teams and micro - crowds . Team network structure in mass collaboration design projects and effect of individual’s characteristics have been studied in [ 22 ] . In con - trast to our work , aforementioned study assumes each indi - vidual in the group has known levels of expertise and abil - ity . Yet , in the context of our work , a practitioner’s expertise level is unknown albeit the group is assumed to have an ex - posure to the problem domain . Thus , unlike traditional de - sign teams , micro - crowds are composed of anonymous and non - interacting people with individually unknown expertise levels . Burnap et al . [ 5 , 23 ] explored the use of crowdsourc - ing in engineering design assessment as well as techniques for identifying the experts in a crowd . These studies do not assume an apriori knowledge of the individuals’ background and are thus greatly suited for studies involving large crowds . Our work builds on and complements these studies by focus - ing on a small group of practitioners , none of whom may be an expert but whose technical familiarity with the problem domain is signiﬁcantly higher and more homogeneous com - pared to crowds extracted from the general population . Crowdsourcing has also been used in design for iden - tifying customer preferences to balance style with brand recognition [ 24 ] or to study the relationship between prod - uct geometry and consumer judgment of style [ 25 ] . Ghosh et al . [ 26 ] modeled user preferences by considering percep - tions estimated by user - product interaction data . While these works primarily focus on eliciting subjective judgments of preference and perception , the main focus of the presented work is to crowdsource solutions to engineering problems where an objectively true solution must exist ( albeit un - known ) . Another popular use of crowdsourcing involves the dis - covery of diverse solutions to complex technical problems involving very high - dimensional design spaces , such as the GE bracket design challenge [ 27 ] . While the generation of solutions is typically the core challenge ( hence crowd - sourced ) , candidate solutions can be rather easily assessed using computational analysis tools . However , the main hy - pothesis and the utility of our work is that further crowd - sourcing to assess candidate designs may in fact produce successful outcomes , which would be critically important in cases where no appropriate computational evaluation tech - nologies exist . Another open problem within the engineering design re - search community where crowdsourcing could provide value relates to the consistent evaluation of conceptual designs . In contrast to engineering problems with known solutions ( i . e . , structural mechanics ) , conceptual design problems have no true solution . When studying the conceptual design pro - cess , researchers often utilize cognitive studies to explore speciﬁc process characteristics , such as the impact of ana - logical stimuli on solution output [ 28 – 30 ] . Typically , design output from such studies is evaluated qualitatively ; trained experts rate deﬁned metrics , such as the novelty or qual - ity , across a wide design space [ 31 ] . Unsurprisingly , the process of both training and rating design solutions can be incredibly time consuming and costly . This is particularly true for cognitive studies requiring hundreds of design con - cepts to be evaluated at a given time [ 32 ] . Another chal - lenge with the current approach to evaluating conceptual de - sign solutions is that when multiple experts are used , they do not always agree upon the particular merits of a given de - sign concept . This can lead to low inter - rater reliability met - rics , and require researchers to retrain experts prior to hav - ing them re - evaluate designs . With this in mind , a combined human - computational framework that removes the necessity of training experts could greatly improve and expedite the conceptual design evaluation process . Recently , evaluation of creativity in conceptual designs by crowdsourcing and im - pact of expertise on creative concept selection has been stud - ied [ 33 , 34 ] . Toh et al . [ 35 ] developed a machine learning ap - proach for computing design creativity of large sets of design ideas . In this work , we also explore WoMC for evaluation of conceptual designs . 3 Experimental Design In order to study the wisdom of crowd in esoteric en - gineering applications , it is necessary to understand the re - lationship between crowds and problem types . This section explains the characteristics of the crowd participants and the design problems used in this work . 3 . 1 Crowd Population Two key factors in the WoC are diversity of opinion and independence . Therefore , a crowd should include people with a variety of opinions rather than a group of elites or experts that may create bubbles and conform to each other’s opinions [ 3 ] . To support independence , we collected sur - vey results through a web - based survey providing anonymity and independence across participants . To support diversity of opinion , we collected crowds through AMT or students specializing various topics in mechanical engineering . This work considers two types of crowds : AMT workers and practitioners . AMT crowds consist of individuals from the population at large , with no explicit control over an in - dividual’s level of expertise . On the other hand , the practi - tioner group represents individuals who have familiarity and knowledge within the target domain , however are not neces - sarily domain experts for the given task . For example , a prac - titioner would be an individual who has studied or currently practices mechanical engineering , but does not necessarily specialize in the ﬁeld of a given task such as heat transfer , structural mechanics , or manufacturing . For a practitioner group , performance of individuals may have signiﬁcant vari - ation yet the base domain knowledge pushes the estimation method to accurate levels . Note that with this deﬁnition , ex - perts are a subset of practitioners . For the practitioner group , 15 mechanical engineering graduate students at Carnegie Mellon University were re - cruited to participate . Each participant was compensated monetarily for their time . The 15 practitioners were recruited from an available pool of over 300 graduate students . It is important to note that these students have different skill lev - els . As later will be shown , this can be observed by large in - dividual estimation errors and signiﬁcant performance vari - ation among the group members . Students in our study are graduate students who already have engineering degrees as well as engineering experience through internships and pos - sible full - time jobs . In that sense , they also represent engi - neers not just students . For the AMT surveys , groups of 100 Fig . 1 . 3D printing - 1 : support material question . Numbers indicate the amount of support material required to print the object at the given orientation on a scale from 1 ( very little ) to 10 ( a lot ) . Fig . 2 . 3D printing - 2 : surface ﬁnish question . Numbers indicate the surface quality rating between 1 ( poor ) and 10 ( excellent ) . Fig . 3 . 3D printing - 3 : surface ﬁnish question . Numbers indicate the surface quality rating between 1 ( poor ) and 10 ( excellent ) . people were gathered through Amazon Mechanical Turk , re - ceiving monetary compensation . In order to remain true to the notion of general public as closely as possible , no spe - ciﬁc demographic groups were targeted . For the structural mechanics questions ( discussed in detail below ) , the study used the data provided by Burnap et al . [ 5 ] . Fig . 4 . The structural mechanics problem [ 5 ] . Numbers indicate the strength of each bracket between 1 ( weak ) and 5 ( strong ) . 3 . 2 Survey Design and Questions This study investigates the WoC with four different sur - veys that range in the challenge they present to a human . All surveys require the respondents to be knowledgeable about the terminology used in the questions . 3D printing questions ( Fig . 1 , 2 , 3 ) aim to probe broadly intuitive perception skills involving visual estimations of areas and volumes . However , they are designed to be increasingly more challenging . Con - versely , the structural mechanics problem that involves esti - mating shape deformations ( Fig . 4 ) presents a much greater challenge to humans , even for experts . Although engineering problems are often computer easy , human hard , they are solved using expert intuition when no computational tools are available . A series of sur - veys for problems with known solutions such that the crowd evaluation accuracy could be determined , assessed whether such situations could beneﬁt from WoC . The structural me - chanics problem ( Fig . 4 ) provides a good example , as such structural design problems had been solved primarily by experts’ knowledge and intuition until the introduction of topology optimization techniques in the 1990s [ 36 ] . There - fore , there now exists the tools to computationally evaluate the aggregated crowd evaluations and benchmark the perfor - mance against true values . As such , practitioners’ perfor - mance on such problems ( which can now be objectively as - sessed ) may provide insights into whether crowd - evaluations of design proposals may yield successful outcomes espe - cially for engineering challenges for which computational modeling and analysis tools may not yet exist . A rating - assignment approach within a predeﬁned scale is utilized . Each survey consists of multiple questions ( e . g . , rating the amount of support material for six differ - ent orientations ) to facilitate expertise inference later in the crowd aggregation stage . In all surveys , participants are pre - sented with the problem statement and the candidate solu - tions to be rated . Figure 1 shows 3D printing - 1 survey where partici - pants are asked to rate the amount of support material re - quired to print an object at various orientations using a fused - depositon printer . For each of the given orientations , partic - ipants are required to evaluate the amount of support mate - rial needed on a scale from 1 ( very little ) to 10 ( a lot ) . The benchmark analysis computes the required support material as the volume that is created by the projection of overhangs to the base with zero overhang angle [ 37 ] . Then , the scores are scaled linearly between 1 and 10 to create the benchmark values . 3D printing - 2 survey is about evaluating the surface ﬁn - ish quality of an object in various orientations ( Fig 2 ) . The participants are asked to rate the quality of the printed ob - ject considering the amount of surfaces in contact with sup - port material for each presented orientation . Surface quality rating is between 1 ( poor ) and 10 ( excellent ) . To compute the true surface ﬁnish , the overhang areas are computed with zero overhang angle . Then , the overhang areas are scaled inversely between 1 and 10 such that 1 represents large sup - port material contact with poor ﬁnish and 10 is very good ﬁnish with the least amount of support material contact . 3D printing - 3 survey ( Fig . 3 ) asks the same question on an object with more features that increase the difﬁculty of evaluation . In the structural design survey , participants are presented with eight different bracket designs intended to support a downward force at the end of the bracket ( Figure 4 ) . Then , they are asked to rate the strength of each bracket on a scale from 1 ( weak ) to 5 ( strong ) , where strength is deﬁned to be the amount of deformation under the given load [ 5 ] . The main reason we use this problem is that estimating the strength of arbitrary shapes is signiﬁcantly more demand - ing compared to volume / area evaluations . While humans are exposed to volume / area computations in daily life , rating the strength of an arbitrary design requires a speciﬁc expe - rience [ 38 ] , which is highly unlikely to be prevalent in the general population . 4 Crowd Estimate Aggregation Techniques The choice of aggregation method affects the collective estimate of the group . For instance , previous studies show that the median or geometric mean can result in estimates that are more accurate over the arithmetic mean [ 1 , 12 ] . This section explains the different aggregation methods used in this work . The following metrics are used : Arithmetic mean , ge - ometric mean , median , majority voting and Bayesian net - works . In a crowd of n participants with a set of estimates Y : y 1 , . . . , y n where y i ∈ Z : 1 (cid:54) y i (cid:54) 10 for all i , the arith - metic mean is y agg = 1 n ∑ nj = 1 y i . The geometric mean is exp ( 1 n ∑ nj = 1 ln ( y i ) ) . The median is the median value in Y . The majority vote is the mode of Y . Bayesian networks have been widely used in crowd - sourcing to mitigate the noise from biased responses . Rel - evant studies model the sources of bias using models that consider problem difﬁculty and the competence of partici - pants [ 4 , 5 , 39 – 43 ] . Similar to these approaches , this work adopts a Bayesian model as shown in Fig . 5 . The evalua - tion process is modeled such that for participant i working on problem j , participant expertise , α i , and problem difﬁ - culty , β j , result in variance , δ ij . Thus , the evaluation of par - ticipant i on problem j , y ij , is obtained when the true score of the problem , x j is combined with the variance , δ ij . Note that the Bayesian model does not require prior knowledge of the true answers , participant expertise or problem difﬁculty . The only observed variable is the participant answer for each question . The variance is obtained using participant expertise and problem difﬁculty . This work assumes that a participant may Fig . 5 . The Bayesian network model . be malicious , inexperienced or experienced . Also , a problem can be easy , difﬁcult or unintuitive . Deﬁning both parameters on a continuous range , the variance is modeled as follows : δ ij = exp ( − α i / β j ) 1 + exp ( − α i / β j ) ( 1 ) where the participant expertise is modeled by the param - eter α i ∈ ( − inf , + inf ) and the problem difﬁculty is β j ∈ ( 0 , + inf ) . The resulting variance becomes δ ij ∈ [ 0 , 1 ] . The evaluation process is modeled as a random variable with a truncated Gaussian distribution around the true score ( µ = x j ) with a variance δ ij . To bring everything into the same scale , evaluations , y ij , are scaled to [ 0 , 1 ] from the original survey scale . The true scores are also represented as x j ∈ [ 0 , 1 ] . The relationship between the evaluation variance with participant expertise and problem difﬁculty is further ex - plained in Figure 6 . The variance indicates how far the eval - uations may be spread apart from the true score . Therefore , a high variance implies probability of sampling far away from the true score , resulting in high evaluation error . From per - spective of precision i . e . , reciprocal of variance , small vari - ance means high precision , meaning higher chance of get - ting the correct evaluation . As anticipated , smaller variance is observed as expertise increases as shown in Figure 6 for three problem difﬁculty levels that correspond to easy , dif - ﬁcult and unintuitive . On the other hand , non - experts can give answers with large variance . Yet there is a potential for malicious participants who intentionally give the wrong an - swers . Since the answers are maliciously wrong , the amount of variance ( thus the evaluation error ) is even more than that of a non - expert that randomly guesses the answers . On the other hand , for a very easy question , even unskilled partic - ipants can give answers with a small variance and anyone malicious can make the most damage ( Figure 6 - Top ) . As the questions get more difﬁcult , expertise affects the varaince of answers more ( Figure 6 - Mid ) . Yet , an unintuitive question can not be evaluated with small variance ( high precision ) by participants at any skill level and evaluated with similar vari - ance since all participants evaluate the problem with random guesses ( Figure 6 - Bottom ) . The structure explained above leads to the graphical model shown in Figure 5 . In the model , participant expertise , Fig . 6 . The evaluation variance with participant expertise shown at three different problem difﬁculty levels as easy , difﬁcult and unintu - itive . α i , problem difﬁculty , β j , and true scores , x j , are sampled from a known prior distribution and these determine the ob - served evaluations , y ij . Given a set of observed evaluations , the task is to infer the most likely values of true scores , x j , together with the participant expertise , α i , and problem dif - ﬁculty , β j , parameters . Assuming a Bayesian treatment with priors on all parameters , the joint probability distribution can be written as p ( y , x , δ , α , β ) = ∏ i p ( α i ) ∏ j p ( β j ) p ( x j ) ∏ ij p ( y ij | δ ij , x j ) p ( δ ij | α i , β j ) ( 2 ) Equation ( 2 ) excludes hyper - parameters for brevity . In our implementation , we use Gaussian priors for α with mean , µ α = 1 , and precision , τ alpha = 1 . Since the value of β needs to be positive , the implementation imposes a truncated Gaus - sian prior with mean , µ β = 1 , and precision , τ beta = 1 , with a lower bound as + ε . For the true scores , x j , we use a trun - cated Gaussian with bounds [ 0 , 1 ] , mean µ x = 0 . 5 and preci - sion τ x = 0 . 1 . Markov Chain Monte Carlo ( MCMC ) simulations are employed to infer the results utilizing Metropolis - Hastings method . Empirically , we observe that using thinning inter - val of 3 and burn - in length of 10 5 works well with 5 × 10 5 iterations . 5 Results To demonstrate the WoMC in esoteric engineering prob - lems , we conducted four surveys on two sets of crowds ( prac - titioners and AMT workers ) having different skill levels as explained in the previous sections . This section presents the results of the surveys and compares the performance of the aggregation methods . Survey results . The results of the surveys with dif - ferent crowds and aggregation methods are summarized in Table 1 . All scores are scaled between 0 and 1 for direct comparison across surveys . In addition to the overall survey results , Figure 7 includes estimation errors for each question in the surveys . While the collective error can be deﬁned as the difference between the true answer and the aggregated answer ( y t − y agg ) for a single question , this work uses root mean square ( RMS ) error for multi - question surveys since it provides a performance measure in the same scale as the in - dividual questions . For a survey containing m questions , the collective error can be computed as (cid:113) 1 m ∑ mj = 1 ( y tj − y aggj ) 2 . Note that the participant responses are discrete scores rather than continuous variables . While arithmetic mean , geomet - ric mean , and Bayesian networks produce a real number from discrete inputs , median and majority voting remain discrete values . For consistency , we compare continuous and discrete aggregates with true continuous answers and their rounded values , respectively . Crowd expertise and aggregation methods . As shown in Table 1 , with the AMT groups , there is no accu - rate estimations with any of the aggregation methods , with RMS errors around 40 % and as high as 60 % . Moreover , the Bayesian network method is outperformed by the other methods in all of the AMT studies . This outcome is consis - tent with previous ﬁndings that argue crowdsourcing AMT populations for engineering design evaluations may produce unreliable results [ 5 ] . On the other hand , the results of the practitioner studies suggest that crowdsourcing can indeed be useful for the same kinds of problems , where consistently more accurate estimations are obtained relative to the AMT groups . When the aggregation methods are compared , no single method appears to be best in the AMT studies . On the other hand , for the practitioner groups , the results indicate that the Bayesian network consistently produces accurate crowd es - timations . Of note , for both the practitioner and the AMT groups , the geometric mean method never emerges as the best approach . This can be explained by the fact that the responses are constrained within particular upper and lower bounds ( 1 - 10 for the 3D printing and 1 - 5 for the structural design problems ) where the range spans only one order of magnitude , whereas the geometric mean is most useful when input data varies in orders of magnitude [ 12 ] . WoMC and individuals . To analyze the WoC ef - fect , the performance of the aggregated crowd estimation is compared against the individuals ( Fig . 8 ) . Only practitioner crowds are included in this analysis as we do not observe a reasonable accuracy in AMT surveys . The collective an - swers aggregated with Bayesian networks are employed as they consistently perform well in practitioner group studies . Figure 8 shows that the collective estimation of the crowd is more accurate than most of the individuals 1 . Note that the practitioner group is composed of individuals with 1 WoC is not expected to outperform all individuals . Rather , its effective - ness is proportional to the fraction of individuals it is able to outperform . In actual use , which individuals have the best answer in unknown . Table 1 . In practitioner groups , the WoC effect is observable as evidenced by the low RMS errors ( over the scale 0 - 1 ) . The Bayesian model gives the best estimate in most cases for practitioners . For the AMT groups , however , the high RMS errors suggest poor estimation accuracy hence much weaker WoC . Note that for the AMT groups , no single aggregation method consistently performs better . RMS error in crowd estimation Question Arithmetic mean Geometric mean Median Majority voting Bayesian model 3D printing - 1 , practitioner 0 . 111 0 . 091 0 . 136 0 . 079 0 . 055 3D printing - 1 , AMT 0 . 403 0 . 378 0 . 430 0 . 336 0 . 363 3D printing - 2 , practitioner 0 . 202 0 . 236 0 . 197 0 . 163 0 . 113 3D printing - 2 , AMT 0 . 438 0 . 462 0 . 473 0 . 540 0 . 600 3D printing - 3 , practitioner 0 . 196 0 . 198 0 . 136 0 . 111 0 . 116 3D printing - 3 , AMT 0 . 402 0 . 431 0 . 363 0 . 453 0 . 561 Structural Mech . , practitioner 0 . 197 0 . 217 0 . 198 0 . 342 0 . 173 Structural Mech . , AMT 0 . 339 0 . 352 0 . 385 0 . 395 0 . 392 Fig . 7 . Error of crowd estimation for each question in the four survey sets . Each bar - group represents the RMS of the crowd aggregated through arithmetic mean , geometric mean , median , majority voting and Bayesian model , respectively . Note that for each of the 3D printing surveys , there are six questions . For the structural design survey , there are eight questions . different skill levels and estimation errors signiﬁcantly vary in the group . This conﬁrms that Bayesian networks can pro - duce an accurate measure of the WoC for the problems that are of esoteric nature . This can be explained by the partic - ipant expertise and problem difﬁculty based inference that considers all answers of an individual to multiple questions collectively rather than a single one . Moreover , these results suggest that the Bayesian networks approach does not un - dermine the WoC effect by erroneously honing in on only an elite group of experts in the group , and instead allows diverse perspectives to be incorporated . This can be explained by the fact that the level of expertise is not prescribed but rather in - Fig . 8 . Estimation error signiﬁcantly varies in the practitioner group . Collective estimate of the practitioner crowd is more accurate than the vast majority of individual practitioners . Collective error of the crowd and errors of individual practitioners in the crowd are given in the center node and the surrounding nodes , respectively . The color of the circles represents the error with dark green representing high error and light yellow low error . Individuals who perform better than the collective answer are marked with a dashed circle . Table 2 . Percentile rank of crowd estimation in individual estima - tions for the practitioner crowd . Percentile rank of crowd estimation Question Continuous Discrete 3D printing - 1 87 % 100 % 3D printing - 2 87 % 93 % 3D printing - 3 93 % 93 % Structural Mech . 93 % 100 % ferred as a latent variable in the Markov Chain Monte Carlo simulations . Table 2 further quantiﬁes the WoC effect by revealing the fraction of people that are outperformed by the collective answer . A higher percentile suggests that a higher fraction of individuals are outperformed , hence a stronger WoC ef - fect is achieved . The percentile rank of the crowd is com - puted using two error metrics as continuous and discrete : the continuous percentile rank computed as the distance between the true answers and participant ratings ; the discrete measure rounding the true answers to the nearest integer while com - puting the individual estimate errors . Note that the discrete measure can be signiﬁcantly affected by these round off er - rors . The difference between continuous and discrete per - centile ranks can be explained by this fact . Of note is the distinction between the percentile rank and the accuracy of the collective estimate . The percentile rank reveals the rela - tive performance of the collective estimate compared to the individual estimates , while the accuracy refers to the RMS error between the estimate and ground truth benchmark . Effect of crowd size . Platforms such as AMT enable access to large and diverse groups . However , in most practi - cal problem - solving settings , only a limited number of prac - titioners are likely to be accessible for the solution of the en - gineering challenge . To gain insight into the impact of small - sized practitioner groups , we analyze the WoC effect across even smaller group sizes , leading to the term micro - crowds ( WoMC ) . Fig . 9 . The effect of crowd size on the performance of the crowd es - timate represented as the percentile and population bias . A slightly upward trend in the percentiles and a signiﬁcant decrease in the stan - dard deviation ( yellow shaded ) as the crowd size increases suggest that higher percentile ranks can be achieved with stronger certainty in larger crowds . For population bias , both mean and standard devi - ation slightly decrease as crowd size increases . Figure 9 shows that WoMC can still be observed in smaller groups . The crowd size is analyzed with the 3D printing - 1 survey and crowd estimation computed using Bayesian networks ( Table1 ) . Initially , practitioner studies are conducted with 15 participants . To simulate micro - crowds with smaller number of participants , a subset of 500 randomly generated combinations of 5 to 14 individuals were generated from the original 15 participant set . The results suggest that the WoC effect can still be observed in diminish - ing group sizes . The probability of obtaining crowd estima - tions with higher success ( percentile ) increases with larger crowds . An approximately 6 % increase in percentile rank with 10 % decrease in standard deviation is observed as the crowd size is increased from 5 to 14 . Figure 9 also shows the effect of crowd size on population bias , deﬁned as the error of aggregated estimate across the crowd [ 44 ] . Both the mean and standard deviation slightly decrease with the increasing crowd size . Evaliuation Metric . One might argue that internal scales may play an important role in people’s ratings . In other words , different individuals may use different internal scales and their deﬁntion of very weak , very strong or very little may differ . For example , a strict grader may score the surface quality between 0 and 5 ( instead of 0 to 10 ) while another rater gives scores between 5 and 10 . For this reason , we compare our RMS error metric with Kendall’s tau coef - ﬁcient [ 45 ] which is a correlation metric that measures the similarity of the orderings . Figure 10 shows the correlation of the collective esti - mate and each individual participant’s answers to ground truth similar to Figure 8 . Note that RMS error and Kendall’s tau correlation have an inverse relationship ( i . e . , better per - formance is indicated by a smaller error or a larger corre - lation value ) . When two metrics are compared , in general , we observe similar wisdom of crowds in terms of the num - ber of people that the aggregated result has outperformed . 3D Printing - 1 and Structural Mechanics surveys result in the same performance with both evaluation metrics . When the Kendall’s tau coefﬁcient is used as the evaluation metric , we still observe that the collective estimate of the practitioner crowd is more accurate than the vast majority of individual practitioners which is a key observation in this study . We believe the reason for this similar behavior may be the pre - conditioning in our questions where we ask users to scale their rating between predeﬁned boundaries as well as having access to all candidate questions before rating each question . This way , the participants are preconditioned to use the given scale rather than their internal scales . Conceptual design evaluations . As an extension of the methods presented in this paper , the feasibility of using a practitioner - sourced Bayesian network model within the context of conceptual designs was explored . To accomplish this , a practitioner evaluation study was run in which each in - dividual practitioner evaluated a pre - existing set of concep - tual design solutions that had also previously been evaluated by two trained experts . Fifteen practitioners were recruited from Carnegie Mellon University , each specializing in Me - chanical Engineering ( Design focus ) , or Product Develop - ment . Participants were allowed a maximum of 120 minutes to complete the ratings , and were monetarily compensated for their time . Each practitioner evaluated 114 conceptual designs , cor - responding to one of four design problems . These prob - lems are as follows : a device that disperses a light coating of a powdered substance over a surface [ 46 ] , a way to min - imize accidents from people walking and texting on a cell phone [ 47 ] , a device to immobilize a human joint [ 48 ] and a device to remove the shell from a peanut in areas with no electricity [ 49 ] . This set of conceptual design solutions was taken from a solution set collected for prior work by Table 3 . RMS error in crowd estimation for the conceptual design evaluations . Aggregation method RMS error Arithmetic mean 0 . 2388 Geometric mean 0 . 6028 Median 0 . 3256 Majority voting 0 . 3652 Bayesian model 0 . 3268 Goucher - Lambert and Cagan [ 32 ] . Each design was eval - uated across four metrics : usefulness , feasibility , novelty , and quality . In the previous study , consistency of the two trained experts was assessed using the intraclass correlation coefﬁcient ( ICC ) . ICC correlations have been reported as ICC > 0 . 65 , ICC > 0 . 77 , ICC > 0 . 71 , ICC > 0 . 50 for usefulness , feasibility , novelty and quality , respectively . While three of the four metrics demonstrate strong corelation and the other metric ( quality ) was fair , all inter - reliability levels are within the range of values typically found in behavioural studies with human raters [ 50 ] . During our experiments , practition - ers were provided with one - sentence criteria for each metric ( including scoring ) , and did not see any example solutions prior to rating designs . Example concepts for two of the problems are shown in Figure 11 . The goal here is to de - termine the accuracy of the Bayesian network model for a class of problems with extremely low structural and func - tional similarity . Table 3 summarizes the collective estimation errors ag - gregated with different methods . Here , the Bayesian model does not perform well and is outperformed by arithmetic mean . In addition to the large collective estimation errors , Figure 12 illustrates that individual estimation errors of prac - titioners are also signiﬁcantly large . 6 Discussions The analyses conducted identiﬁed some key insights on how WoMC can be achieved in esoteric engineering prob - lems , highlighted as follows . Problem intuitiveness and difﬁculty . All of the surveys require speciﬁc knowledge about the engineering problem at hand but they range in intuitiveness and difﬁ - culty levels . 3D printing questions are based on qualitative area / volume estimations in 3D scenes , which humans are ex - pected to be relatively comfortable with . On the other hand , the structural design problem is signiﬁcantly more demand - ing since estimating the strength of complex geometries re - quires a deeper familiarity and experience within the do - main [ 38 ] . While individuals are able to make more accurate estimations in the 3D printing questions than they can in the structural design question , an interesting observation is that no signiﬁcant difference in the wisdom of crowds ( i . e . , per - centile rank ) is observed implying that crowdsourcing works Fig . 10 . Kendall’s tau coefﬁcient as evaluation metric : Correlation of collective estimation and individual practitioner ratings to ground truth are given in the center node and the surrounding nodes , respectively . The color of the circles represents the correlation with dark green representing low correlation and light yellow representing high correlation . Individuals who perform better than the collective answer are marked with a dashed circle . Note that higher values indicate better correlation meaning better performance . Fig . 11 . Example conceptual designs . Fig . 12 . The conceptual design survey illustrates signiﬁcant estima - tion errors for each individual practitioner . Individual estimation errors of practitioners are given at the surrounding nodes and the collective estimation error is the center node . Left : arithmetic mean , Right : Bayesian model . equally effective in both cases . Moreover , no signiﬁcant dif - ference between the results of 3D printing - 2 and 3D printing - 3 surveys occur , even though the latter is more demanding with a larger number of geometrical features . These results suggest that even for problems that are demanding , the WoC is attainable at levels comparable to those attained in less de - manding problems . Level of expertise . Populations of ordinary people ( e . g . , AMT crowds ) perform poorly on esoteric engineering problems . Results indicate that the wisdom of crowds can be achieved in practitioner micro - crowds of the domain of such problems . This suggests that people who are still gaining ex - perience in the domain may prove to be a valuable asset as problem solvers . This is especially important as practitioner crowds may be more accessible than experts . Aggregation methods . In the context of practitioner populations , the most effective aggregation method found in this work is the Bayesian network . For practitioner groups , the exposure to the domain of the esoteric problem builds true consistency in the data and allows the Bayesian network to mitigate the mistakes made by individual practitioners . In the AMT groups , however , we observe consistently wrong answers due to lack of expertise . For that reason , Bayesian network method performs worse than arithmetic mean here for AMT populations as also discussed in [ 5 ] . This work in - dicates that Bayesian network method is more effective given a minimum level of expertise in the group . Crowd size . As shown in Figure 9 , as the crowd size increases , the mean percentile performance increases ( albeit modestly ) while the standard deviation of the percentile rank of the group estimates decreases over sets of different micro - crowds . This indicates larger practitioner crowds will likely lead to better and more consistent outcomes . On the perfor - mance of WoMC on an absolute scale , our results indicate group estimates in the 90 th percentile can be achieved with as few as 5 to14 practitioners . This suggests that in cases where computational tools are not readily available , high quality assessments on engineering problems can be gleaned from small groups of practitioners . Conceptual design evaluations . When assessing so - lutions to a set of open - ended , conceptual problems , practi - tioner crowds struggle to give answers at a level that experts do . For these problems , estimation error in crowd estima - tion aggregated with the Bayesian model is signiﬁcant and it is outperformed by arithmetic mean . Looking into individ - ual estimation errors gives an insight into why the Bayesian model is not performing well for these conceptual designs that lack the structural and functional similarity . Figure 12 demonstrates that every individual in the practitioner group makes a signiﬁcant estimation error . Even though the esti - mation aggregated through the Bayesian model is better than all individuals , it is still very high due to large estimation er - rors of each practitioner . In contrast to the previous esoteric engineering problems , conceptual design problems have no true solution . We believe the open ended nature of concep - tual design problems creates a challenge for consistent eval - uation in crowd sourced environments and requires further exploration . 7 Conclusion This work explored the ability of crowdsourced popu - lations to estimate accurate values for a variety of esoteric problems within the domain of engineering design . Results demonstrate that the wisdom of crowd is most effective in practitioner groups , or groups of individuals who possess some level of domain knowledge , but are not necessarily experts . Aggregated crowd results of practitioners achieve high accuracy across a range of problems . By simulating small groupings of 5 to15 practitioners , called micro - crowds , it is found that crowd estimates perform more accurately than individual estimates across the majority of the studies . These results suggest that the WoMC can provide a power - ful tool for answering difﬁcult problems in which computa - tional methods have not been established . In addition , these results argue for the establishment of online communities of practitioners , which could facilitate the solution of future en - gineering challenges . However , the results also suggest that the practitioner crowds struggle to evaluate open - ended con - ceptual design problems at a level that experts do . An open research questions is thus the utility of crowdsourcing for problems involving open - ended synthesis . Acknowledgements We would like to thank authors of [ 5 ] for making their data publicly available . References [ 1 ] Galton , F . , 1907 . “The ballot - box” . Nature , 75 ( 1952 ) , p . 509 . [ 2 ] Hooker , R . H . , 1907 . “Mean or median” . Nature , 75 , pp . 487 – 488 . [ 3 ] Surowiecki , J . , 2005 . The wisdom of crowds . Anchor . [ 4 ] Wah , C . , 2006 . “Crowdsourcing and its applications in computer vision” . University of California , San Diego . [ 5 ] Burnap , A . , Ren , Y . , Gerth , R . , Papazoglou , G . , Gon - zalez , R . , and Papalambros , P . Y . , 2015 . “When crowd - sourcing fails : A study of expertise on crowdsourced design evaluation” . Journal of Mechanical Design , 137 ( 3 ) , p . 031101 . [ 6 ] Summers , J . D . , and Shah , J . J . , 2010 . “Mechani - cal engineering design complexity metrics : size , cou - pling , and solvability” . Journal of Mechanical Design , 132 ( 2 ) , p . 021004 . [ 7 ] Yang , M . C . , 2010 . “Consensus and single leader decision - making in teams using structured design methods” . Design Studies , 31 ( 4 ) , pp . 345 – 362 . [ 8 ] Gurnani , A . , and Lewis , K . , 2008 . “Collaborative , decentralized engineering design at the edge of ra - tionality” . Journal of Mechanical Design , 130 ( 12 ) , p . 121101 . [ 9 ] Takai , S . , 2010 . “A game - theoretic model of collabo - ration in engineering design” . Journal of Mechanical Design , 132 ( 5 ) , p . 051005 . [ 10 ] Cabrerizo , F . J . , Ure˜na , R . , Pedrycz , W . , and Herrera - Viedma , E . , 2014 . “Building consensus in group deci - sion making with an allocation of information granu - larity” . Fuzzy Sets and Systems , 255 , pp . 115 – 127 . [ 11 ] Hong , L . , and Page , S . E . , 2004 . “Groups of di - verse problem solvers can outperform groups of high - ability problem solvers” . Proceedings of the National Academy of Sciences of the United States of America , 101 ( 46 ) , pp . 16385 – 16389 . [ 12 ] Lorenz , J . , Rauhut , H . , Schweitzer , F . , and Helbing , D . , 2011 . “How social inﬂuence can undermine the wisdom of crowd effect” . Proceedings of the National Academy of Sciences , 108 ( 22 ) , pp . 9020 – 9025 . [ 13 ] Lorenz , K . , Jones , J . , Wimpenny , D . , and Jackson , M . , 2015 . “A review of hybrid manufacturing” . In Solid Freeform Fabrication Symposium ( SFF ) , Austin , TX , Aug , pp . 10 – 12 . [ 14 ] Amazon mechanical turk . https : / / www . mturk . com . Accessed : 2017 - 01 - 30 . [ 15 ] CrowdFlower . https : / / www . crowdflower . com . Accessed : 2017 - 01 - 30 . [ 16 ] Proliﬁc academic . https : / / www . prolific . ac / . Accessed : 2017 - 01 - 30 . [ 17 ] Berinsky , A . J . , Huber , G . A . , and Lenz , G . S . , 2012 . “Evaluating online labor markets for experimental re - search : Amazon . com’s mechanical turk” . Political Analysis , 20 ( 3 ) , pp . 351 – 368 . [ 18 ] Kittur , A . , Chi , E . H . , and Suh , B . , 2008 . “Crowdsourc - ing user studies with mechanical turk” . In Proceedings of the SIGCHI conference on human factors in comput - ing systems , ACM , pp . 453 – 456 . [ 19 ] Kittur , A . , Nickerson , J . V . , Bernstein , M . , Gerber , E . , Shaw , A . , Zimmerman , J . , Lease , M . , and Horton , J . , 2013 . “The future of crowd work” . In Proceedings of the 2013 conference on Computer supported coopera - tive work , ACM , pp . 1301 – 1318 . [ 20 ] Rzeszotarski , J . M . , and Kittur , A . , 2011 . “Instrument - ing the crowd : using implicit behavioral measures to predict task performance” . In Proceedings of the 24th annual ACM symposium on User interface software and technology , ACM , pp . 13 – 22 . [ 21 ] Panchal , J . H . , Sha , Z . , and Kannan , K . N . , 2017 . “Un - derstanding design decisions under competition using games with information acquisition and a behavioral experiment” . Journal of Mechanical Design , 139 ( 9 ) , p . 091402 . [ 22 ] Ball , Z . , and Lewis , K . , 2018 . “Observing network characteristics in mass collaboration design projects” . Design Science , 4 . [ 23 ] Burnap , A . , Gerth , R . , Gonzalez , R . , and Papalambros , P . Y . , 2017 . “Identifying experts in the crowd for eval - uation of engineering designs” . Journal of Engineering Design , 28 ( 5 ) , pp . 317 – 337 . [ 24 ] Burnap , A . , Hartley , J . , Pan , Y . , Gonzalez , R . , and Pa - palambros , P . Y . , 2015 . “Balancing design freedom and brand recognition in the evolution of automotive brand styling” . In ASME 2015 International Design Engi - neering Technical Conferences and Computers and In - formation in Engineering Conference , American Soci - ety of Mechanical Engineers . [ 25 ] Orbay , G . , Fu , L . , and Kara , L . B . , 2015 . “Deciphering the inﬂuence of product shape on consumer judgments through geometric abstraction” . Journal of Mechanical Design , 137 ( 8 ) , p . 081103 . [ 26 ] Ghosh , D . D . , Olewnik , A . , and Lewis , K . E . , 2017 . “An integrated framework for predicting consumer choice through modeling of preference and product use data” . In ASME 2017 International Design Engineer - ing Technical Conferences and Computers and Infor - mation in Engineering Conference , American Society of Mechanical Engineers . [ 27 ] Morgan , H . , Levatti , H . , Sienz , J . , Gil , A . , and Bould , D . , 2014 . “Ge jet engine bracket challenge : A case study in sustainable design” . Sustainable Design and Manufacturing 2014 Part 1 , p . 95 . [ 28 ] Fu , K . , Chan , J . , Cagan , J . , Kotovsky , K . , Schunn , C . , and Wood , K . , 2013 . “The meaning of near and far ? : the impact of structuring design databases and the ef - fect of distance of analogy on design output” . Journal of Mechanical Design , 135 ( 2 ) , p . 021007 . [ 29 ] Murphy , J . , Fu , K . , Otto , K . , Yang , M . , Jensen , D . , and Wood , K . , 2014 . “Function based design - by - analogy : a functional vector approach to analogical search” . Jour - nal of Mechanical Design , 136 ( 10 ) , p . 101102 . [ 30 ] Moreno , D . P . , Hernandez , A . A . , Yang , M . C . , Otto , K . N . , H ¨ oltt ¨ a - Otto , K . , Linsey , J . S . , Wood , K . L . , and Linden , A . , 2014 . “Fundamental studies in design - by - analogy : A focus on domain - knowledge experts and applications to transactional design problems” . Design Studies , 35 ( 3 ) , pp . 232 – 272 . [ 31 ] Shah , J . J . , Kulkarni , S . V . , and Vargas - Hernandez , N . , 2000 . “Evaluation of idea generation methods for conceptual design : effectiveness metrics and design of experiments” . Journal of mechanical design , 122 ( 4 ) , pp . 377 – 384 . [ 32 ] Goucher - Lambert , K . , and Cagan , J . , 2018 . “Crowd - sourcing inspiration : Using crowd generated inspira - tional stimuli to support designer ideation” . Design Studies ( Accepted ) . [ 33 ] Green , M . , Seepersad , C . C . , and H¨oltt¨a - Otto , K . , 2014 . “Crowd - sourcing the evaluation of creativity in con - ceptual design : A pilot study” . In ASME 2014 In - ternational Design Engineering Technical Conferences and Computers and Information in Engineering Con - ference , American Society of Mechanical Engineers , pp . V007T07A016 – V007T07A016 . [ 34 ] Gosnell , C . A . , and Miller , S . R . , 2016 . “But is it cre - ative ? delineating the impact of expertise and concept ratings on creative concept selection” . Journal of Me - chanical Design , 138 ( 2 ) , p . 021101 . [ 35 ] Toh , C . A . , Starkey , E . M . , Tucker , C . S . , and Miller , S . R . , 2017 . “Mining for creativity : Determining the creativity of ideas through data mining techniques” . In ASME 2017 International Design Engineering Techni - cal Conferences and Computers and Information in En - gineering Conference , American Society of Mechani - cal Engineers , pp . V007T06A010 – V007T06A010 . [ 36 ] Bendsøe , M . P . , 1989 . “Optimal shape design as a ma - terial distribution problem” . Structural optimization , 1 ( 4 ) , pp . 193 – 202 . [ 37 ] Alexander , P . , Allen , S . , and Dutta , D . , 1998 . “Part ori - entation and build cost determination in layered man - ufacturing” . Computer - Aided Design , 30 ( 5 ) , pp . 343 – 356 . [ 38 ] Nobel - Jørgensen , M . , Malmgren - Hansen , D . , Bærentzen , J . A . , Sigmund , O . , and Aage , N . , 2016 . “Improving topology optimization intuition through games” . Structural and Multidisciplinary Optimization , 54 ( 4 ) , pp . 775 – 781 . [ 39 ] Whitehill , J . , Wu , T . - f . , Bergsma , J . , Movellan , J . R . , and Ruvolo , P . L . , 2009 . “Whose vote should count more : Optimal integration of labels from labelers of unknown expertise” . In Advances in neural information processing systems , pp . 2035 – 2043 . [ 40 ] Bachrach , Y . , Graepel , T . , Minka , T . , and Guiver , J . , 2012 . “How to grade a test without knowing the answers—a bayesian graphical model for adaptive crowdsourcing and aptitude testing” . arXiv preprint arXiv : 1206 . 6386 . [ 41 ] Welinder , P . , Branson , S . , Belongie , S . J . , and Perona , P . , 2010 . “The multidimensional wisdom of crowds . ” . In NIPS , Vol . 23 , pp . 2424 – 2432 . [ 42 ] Lakshminarayanan , B . , and Teh , Y . W . , 2013 . “In - ferring ground truth from multi - annotator ordinal data : a probabilistic approach” . arXiv preprint arXiv : 1305 . 0015 . [ 43 ] Wauthier , F . L . , and Jordan , M . I . , 2011 . “Bayesian bias mitigation for crowdsourcing” . In Advances in neural information processing systems , pp . 1800 – 1808 . [ 44 ] Vul , E . , and Pashler , H . , 2008 . “Measuring the crowd within probabilistic representations within individu - als” . Psychological Science , 19 ( 7 ) , pp . 645 – 647 . [ 45 ] Kendall , M . G . , 1955 . “Rank correlation methods” . [ 46 ] Linsey , J . s . , Wood , K . l . , and Markman , A . b . , 2008 . “Modality and representation in analogy” . Artif . Intell . Eng . Des . Anal . Manuf . , 22 ( 2 ) , Jan . , pp . 85 – 100 . [ 47 ] Miller , S . R . , Bailey , B . P . , and Kirlik , A . , 2014 . “Ex - ploring the utility of bayesian truth serum for assessing design knowledge” . Hum . - Comput . Interact . , 29 ( 5 - 6 ) , Aug . , pp . 487 – 515 . [ 48 ] Wilson , J . O . , Rosen , D . , Nelson , B . A . , and Yen , J . , 2010 . “The effects of biological examples in idea gen - eration” . Design Studies , 31 ( 2 ) , pp . 169 – 186 . [ 49 ] Viswanathan , V . K . , and Linsey , J . S . , 2013 . “De - sign ﬁxation and its mitigation : a study on the role of expertise” . Journal of Mechanical Design , 135 ( 5 ) , p . 051008 . [ 50 ] Cicchetti , D . V . , 1994 . “Guidelines , criteria , and rules of thumb for evaluating normed and standardized as - sessment instruments in psychology . ” . Psychological assessment , 6 ( 4 ) , p . 284 .