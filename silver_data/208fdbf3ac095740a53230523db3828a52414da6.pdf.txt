PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning Simon Holk ∗ sholk @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Daniel Marta ∗ dlmarta @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Iolanda Leite iolanda @ kth . se KTH Royal Institute of Technology Stockholm , Sweden ABSTRACT Preference - based reinforcement learning ( RL ) has emerged as a new field in robot learning , where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state - action pairs . However , formulating realistic policies for robots demands responses from humans to an extensive array of queries . In this work , we approach the sample - efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting . To accomplish this , we leverage the zero - shot capabilities of a large language model ( LLM ) to reason from the text provided by humans . To accommodate the additional query information , we reformulate the reward learning objectives to contain flexible highlights – state - action pairs that contain relatively high information and are related to the features processed in a zero - shot fashion from a pretrained LLM . In both a simulated scenario and a user study , we reveal the effectiveness of our work by analyzing the feedback and its implications . Addi - tionally , the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape . We provide video examples of the trained policies at https : / / sites . google . com / view / rl - predilect CCS CONCEPTS • Computing methodologies → Inverse reinforcement learn - ing ; Learning to rank . KEYWORDS Preference learning , Reinforcement learning , Interactive learning , Human - in - the - loop Learning ACM Reference Format : Simon Holk , Daniel Marta , and Iolanda Leite . 2024 . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning . In Proceedings of the 2024 ACM / IEEE International Conference on Human - Robot Interaction ( HRI ’24 ) , March 11 – 14 , 2024 , Boulder , CO , USA . ACM , NewYork , NY , USA , 11pages . https : / / doi . org / 10 . 1145 / 3610977 . 3634970 1 INTRODUCTION A key ingredient for the success of preference - based RL is that modern methods place minimal constraints on the modality of the ∗ These authors contributed equally ThisworkislicensedunderaCreativeCommonsAttribution International 4 . 0 License . HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA © 2024 Copyright held by the owner / author ( s ) . ACM ISBN 979 - 8 - 4007 - 0322 - 5 / 24 / 03 . https : / / doi . org / 10 . 1145 / 3610977 . 3634970 reward function [ 16 , 26 , 31 , 70 ] , facilitating the formulation of com - plex objectives for robotic applications . While preference - driven teaching incorporates the essential element of structural alignment , vital for designing intricate objectives [ 7 , 9 , 33 ] , its substantial reliance on extensive human feedback limits its applicability in real - world robotics [ 26 , 30 , 41 , 49 ] . Moreover , while preferences do show a strong correlation with causality [ 21 , 65 ] , there is evidence indicating that preferences may not be sufficient as a standalone modality to thoroughly delineate the causal relationship among states , actions , and rewards . This challenge is identified as causal confusion [ 19 , 68 ] . In a comprehensive empirical study on causal confusion in preference - based RL , Tien et al . [ 68 ] highlighted that the introduction of spurious features and a rise in model capacity can induce causal confusion regarding the true reward function , even when learning from thousands of pairwise preferences . Over - looking this aspect can result in spurious correlations , which may ultimately lead to either reward exploitation [ 2 , 25 ] or the creation of a distributional shift [ 19 ] . Both scenarios necessitate additional interactions and can result in diminished performance , representing a failure in reward inference and leading to suboptimal behaviors . We believe that a balance can be struck between the easiness of providing preferences [ 16 , 26 ] and offering an optional natural language interface for humans , in an effort to uncover the true causal relationship , thus greatly reducing the entropy of credit assignment , as natural language presents a more natural way for humans to interact [ 34 , 40 , 51 ] . While the integration of natural language can pose a significant challenge on its own , we leverage recent advancements in large pretrained foundational models [ 8 ] , such as BERT [ 20 ] , CLIP with GPT - 2 [ 46 ] , and GPT - 3 [ 12 ] . These models excel in various tasks , including text completion , image - text similarity , image captioning , and robot planning [ 78 ] . Additional evidence indicates that adequately large language models possess the capability to execute complex reasoning [ 29 , 78 ] , potentially revealing causal reasoning from human prompts and thereby mit - igating aspects of causal confusion . For instance , Wei et al . [ 71 ] demonstrated that the generation of a thought chain—a sequence of intermediate reasoning steps— enables the emergence of advanced reasoning abilities in sufficiently expansive language models . To address the aforementioned limitations we leverage the zero - shot capabilities of pretrained models , we introduce PREDILECT : PRE ferences D el I neated with Zero - Shot L anguag E - based Reason - ing in Reinfor C emen T Learning . Figure 1 provides a macroscopic depiction of our approach . Consider the two trajectories on the left side of the figure . While the queried person preferred trajectory B , when asked to justify their preference they mentioned that the robot being too close to the group of humans was unnecessary . In typical preference learning methods , where only the preferred a r X i v : 2402 . 15420v1 [ c s . R O ] 23 F e b 2024 HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite I prefer B because it reached the goal but the robot was too close to the group of humans A B LLM FeatureSentimentMagnitude Preference Timestep R e w a r d Intrinsic features Goal Humansafety Speed Trajectory B Wall avoidance Figure 1 : An overview of PREDILECT in a social navigation scenario : Initially , a human is shown two trajectories , A and B . They signal their preference for one of the trajectories and provide an additional text prompt to elaborate on their insights . Subsequently , an LLM can be employed for extracting feature sentiment , revealing the causal reasoning embedded in their text prompt , which is processed and mapped to a set of intrinsic values . Finally , both the preferences and the highlighted insights are utilized to more accurately define a reward function . trajectory is used as feedback , the result of this query could lead to causal confusion . PREDILECT addresses the limitations inherent to preference - based RL by delineating preferences with highlights ( sequences of state - action pairs ) from the sentiment analysis of an LLM . The goal is to enhance the granularity of the reward model by partially uncovering the causal relationships between state - action and rewards . We achieve this by modifying the learning objective of the reward model with said highlights . We provide empirical evidence for the efficacy of PREDILECT , in the form of ablations on synthetic benchmarks . We also collect actual human prefer - ences in a simulated social robot navigation scenario , to verify its applicability and further reinforce its merits and performance . 2 RELATED WORK 2 . 1 Learning from evaluative feedback Utilizing human knowledge for robot learning serves as an effective and interactive method [ 43 , 80 ] , particularly through the medium of evaluative feedback [ 47 ] . By deducing reward functions from human input , we can facilitate the swift adaptation of robot policies [ 74 ] , craft policies that are tailored to individual users [ 61 ] , and achieve alignment with instructions and descriptions [ 66 ] . Humans can convey this form of feedback in a variety of manners , including scalar form [ 35 ] , verbal directives [ 62 ] , trajectory segmentation [ 17 ] , or by employing buttons to signal preferred behaviors [ 35 , 36 , 60 ] , such as for expressing preferences [ 16 , 73 ] . Prior studies have also focused on refining policies through preferences , either by pre - defining features [ 13 ] , augmenting features [ 3 ] , or utilizing Bayesian methods [ 6 , 56 ] . Leveraging human preferences for learning has received sub - stantial focus in recent literature [ 73 ] , showcasing potential as an effective RL method applicable even in high - dimensional robotic settings [ 26 ] . Contemporary methods in preference learning impose few restrictions on reward function modality and can be interpreted as an iteration of repeated inverse reinforcement learning [ 1 ] . A reward function can either be inferred from a tabula rasa approach [ 16 ] or be bootstrapped via imitation learning [ 31 ] . The underlying principle is the perpetual refinement of a reward function by solic - iting preferences on subsequent iterations of a policy . Strategies that incrementally incorporate human participation in the learning loop to infer reward functions exhibit enhanced robustness and diminish the risk of obtaining contradictory feedback from humans [ 4 , 16 , 57 ] . Recent approaches have tackled the issue of feedback inefficiency utilizing techniques like pre - training [ 26 , 39 , 49 ] and bi - level optimization [ 42 ] . In contrast , PREDILECT delves into the promising and relatively uncharted challenge associated with pref - erence explanation [ 30 ] . 2 . 2 Zero - shot multimodal prompting In this work , our aim is to integrate multimodal [ 48 ] language in - formation with preferences in a zero - shot fashion , leveraging large language models ( LLMs ) [ 12 , 14 , 20 , 53 , 67 ] . LLMs are renowned for executing linguistic tasks , such as textual interpretation [ 55 ] and thus suitable for feature interpretation and extraction . Our interest lies in a variant of zero - shot transfer learning [ 24 , 59 , 64 , 76 ] . Specifically , we seek to utilize the underlying reasoning found in human text prompts acquired with an LLM from a source domain ( Internet - scale text prompts ) with preferences , to both enhance performance and reduce the number of labeled queries needed in a target domain . During multimodal training [ 78 ] , it is common to retain specific subcomponents of models—particularly those related to one modality but not others—in a frozen state for downstream tasks [ 22 , 37 , 69 , 77 , 79 ] . The fusion of weights from extensive pretrained models with multimodal joint training has PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA led to notable accomplishments in diverse downstream multimodal applications , including image captioning [ 46 ] . In alignment with our research , Zeng et al . [ 78 ] presents So - cratic Models ( SMs ) , which are portrayed as modular frameworks . In these frameworks , new tasks emerge from a language - based interaction between pretrained models and additional modules , such as a reward model . PREDILECT can be conceptualized as an SM , where the large pretrained LLM is predetermined , while a reward model undergoes training based on joint text inferences between the LLM and preferences . Alternatively , our approach can be interpreted as a form of distillation [ 15 , 54 , 75 ] , where the LLM serves as the teacher and effectively functions as a regularizer , aid - ing in the training of the student reward function . Inspired by the aforementioned works , PREDILECT contributes by utilizing the simplicity of preference - based RL and delving into the contextual text information associated with the respective queries . 3 BACKGROUND We present the fundamentals to understand PREDILECT ( see Sec . 4 ) . In this work , our goal is to develop a model that accurately pre - dicts the reward function by efficiently leveraging human feedback , represented by preferences and prompts . The scenario we consider is one where a robot , in a given state 𝑠 𝑡 , initiates an action 𝑎 𝑡 ac - cording to a policy 𝜋 𝜔 ( 𝑎 𝑡 , 𝑠 𝑡 ) , parameterized by 𝜔 . Upon executing this action , the robot receives a reward 𝑟 ( 𝑠 𝑡 , 𝑎 𝑡 ) and transitions to a new state 𝑠 𝑡 + 1 , all within the framework of a Markov Decision Process ( MDP ) . The final objective for the robot is to discover an optimal policy 𝜋 ∗ 𝜔 ( 𝑎 𝑡 , 𝑠 𝑡 ) that maximizes the expected discounted sum of rewards . 3 . 1 Preference Learning Following Christiano et al . [ 16 ] , we define the task of inferring a reward function , ˆ 𝑟 𝜓 , characterized by 𝜓 , from preferences as an issue in supervised learning . The core ambition of preference - based RL , as explored in [ 16 , 73 ] , revolves around deducing rewards from sequences of state - action pairs . Defining trajectory segments [ 72 ] as sequences constituted by state - action pairs , they are represented as 𝜎 𝑗 = ( ( 𝑠 𝑗𝑡 , 𝑎 𝑗𝑡 ) , . . . , ( 𝑠 𝑗𝑡 + 𝑚 − 1 , 𝑎 𝑗𝑡 + 𝑚 − 1 ) ) , with 𝑗 signifying the seg - ment index , encompassing state - action pairs from 𝑡 to 𝑡 + 𝑚 , where 𝑚 symbolizes the segment length . Humans are presented with pairs of these trajectory segments , designated as ( 𝜎 0 , 𝜎 1 ) , and are tasked with allocating a preference 𝑤 ∈ { 0 , 0 . 5 , 1 } . A preference of 𝑤 = 0 signifies favoring 𝜎 0 over 𝜎 1 , depicted as 𝜎 0 ≻ 𝜎 1 , while 𝑤 = 1 is interpreted as 𝜎 1 ≻ 𝜎 0 , and 𝑤 = 0 . 5 indicates an equivalent prefer - ence for both segments . Adhering to the Bradley - Terry model [ 10 ] , the likelihood of a human exhibiting a preference for 𝜎 0 ≻ 𝜎 1 , con - tingent upon it being exponentially reliant on the reward sum over the segments’ length , is expressed as : 𝑃 𝜓 [ 𝜎 0 ≻ 𝜎 1 ] = exp ( (cid:205) 𝑡 ˆ 𝑟 𝜓 ( 𝑠 0 𝑡 , 𝑎 0 𝑡 ) ) exp ( (cid:205) 𝑡 ˆ 𝑟 𝜓 ( 𝑠 0 𝑡 , 𝑎 0 𝑡 ) ) + exp ( (cid:205) 𝑡 ˆ 𝑟 𝜓 ( 𝑠 1 𝑡 , 𝑎 1 𝑡 ) ) ( 1 ) In this framework , the reward model , ˆ 𝑟 𝜓 , is amenable to train - ing as a binary classifier to anticipate human preferences on new segments , serving as a surrogate for the reward function . The pref - erences elicited from humans are store alongside the respective segments in a labeled dataset D 𝑙 , composed of triples ( 𝜎 0 , 𝜎 1 , 𝑤 ) . During the optimization of ˆ 𝑟 𝜓 , we draw samples from D 𝑙 and aim to minimize the binary cross - entropy loss : L 𝐶𝐸 ( ˆ 𝑟 𝜓 , D 𝑙 ) = − E ( 𝜎 1 , 𝜎 2 , 𝑤 ) ∼D 𝑙 [ ( 1 − 𝑤 ) log 𝑃 𝜓 ( 𝜎 1 ≻ 𝜎 2 ) + 𝑤 log 𝑃 𝜓 ( 𝜎 2 ≻ 𝜎 1 ) ] ( 2 ) 4 PREDILECT 4 . 1 Prompt - Response formulation In PREDILECT a human can optionally offer a prompt to comple - ment their preference , such that prompt 𝑖 ∈ P , where 𝑖 indexes the 𝑖 - th prompt , and P the set of all prompts provided by the humans . To analyse human prompts , we require a set of intrinsic features F = { 𝑓 1 , 𝑓 2 , . . . , 𝑓 𝑛 } , where 𝑛 ∈ N + represents the size of the fea - ture set . It is important to note we do not want to bias humans on which features they should consider . Rather , we can denote intrin - sic features we might find important for particular tasks , such as speed and distance to humans on social navigation . We leverage an LLM , to not only detect features from human prompts but provide sentiment analysis linked to these features . Thus , we input to an LLM both the prompt provided by the human and our feature set , such that : LLM : ( prompt 𝑖 ∈ P , F ) → r 𝑖 ∈ R ( 3 ) where R corresponds to the set of all possible responses . Each r 𝑖 consists of a set of triplets of the size 𝑛 = | F | = | R | , where r 𝑖 = { ( 𝑓 1 , y 1 , v 1 ) , . . . , ( 𝑓 𝑛 , y 𝑛 , v 𝑛 ) } , where 𝑓 1 ∈ F is a feature , y 1 ∈ { positive , negative } is the sentiment associated with feature 𝑓 1 , and v 1 ∈ { low , high } is the magnitude associated with the sentiment y 1 . To clarify our formulation we provide a concrete example of a prompt that was evaluated for our experiments ( see Sec . 5 ) in the social navigation environment . For visualisation purposes , the feature set F is blue , the human prompt prompt 𝑖 is purple and the output response r 𝑖 is green . : Input : You are a robot navigating a corridor with humans walking around trying to reach the goal / star . The user had to pick between two alternatives and picked their preferred alternative and they are now giving an explanation for their pick . Which feature ( s ) was most important of [ distance to goal , distance to human , speed ] ? The text given by the user is : ’was less close to hitting a human / wall and moved at a slower pace . ’Please respond in the following format for each feature that is relevant to the text given by the user : [ feature : insert feature , sentiment : insert positive or negative , value : insert high or low ] . Sentiment explains if the user thought the robot was behaving well in regards to the feature , if the robot behaved well it should be positive , else negative . Value indicates if the value of the feature was high or low . Only mention the features that are relevant , disregard the others . Output : [ feature : distance to human , sentiment : positive , value : high ] [ feature : speed , sentiment : positive , value : low ] 4 . 2 Mapping responses to highlights We use each response r 𝑖 to obtain highlighted subsequences from a trajectory segment 𝜎 , which we define as highlights . A highlight , denoted as ℎ , is characterized as a subsequence residing within a preferred segment . Highlights are constructed as subsequences of segments , rep - resented by ℎ = 𝜎 𝑖 , 𝑗 = ( ( 𝑠 𝑖 , 𝑎 𝑖 ) , . . . , ( 𝑠 𝑗 , 𝑎 𝑗 ) ) ∈ ( 𝑠 , 𝑎 ) 𝑗 − 𝑖 , 𝑖 , 𝑗 ∈ N + with 0 ≤ 𝑖 ≤ 𝑗 ≤ 𝑚 . The highlight’s length is given by 𝑗 − 𝑖 = 𝐿 , wherein 𝐿 signifies the maximum length contemplated for a high - light . To map each r 𝑖 to highlights , we first semantically map each trajectory segment 𝜎 to a tensor of feature metrics T . HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite We define a mapping function such as M : 𝜎 → T , that maps the corresponding state sequence to a tensor of feature metrics T . Each T 𝑖𝑗 is the tensor element corresponding to the 𝑖 𝑡ℎ state and 𝑗 𝑡ℎ feature . Moreoever , each tensor element can be defined as T 𝑖𝑗 = { 𝑓 𝑗 , m 𝑖𝑗 } which contains a feature 𝑓 𝑗 and its corresponding metric value m 𝑖𝑗 ∈ R such that the tensor of feature metrics can be defined as : T =  { 𝑓 1 , m 11 } { 𝑓 2 , m 12 } . . . { 𝑓 𝑛 , m 1 𝑛 } { 𝑓 1 , m 21 } { 𝑓 2 , m 22 } . . . { 𝑓 𝑛 , m 2 𝑛 } . . . . . . . . . . . . { 𝑓 1 , m 𝑚 1 } { 𝑓 2 , m 𝑚 2 } . . . { 𝑓 𝑛 , m 𝑚𝑛 }  ( 4 ) In this context , each m 𝑖𝑗 is a scalar derived through a heuristic , which is intrinsically associated with the relevant feature . For in - stance , " distance to humans " is expected to represent a scalar value , measured in meters . We require a search function 𝑔 that navigates through the tensor of metrics T to identify a highlight ℎ for each r 𝑖 . Given a response r 𝑖 , our task is to pinpoint , for every feature 𝑓 ∈ 𝐹 referenced in 𝑟 𝑖 , a singular subsequence 𝑆 ′ 𝑓 ⊆ 𝑆 that aligns with the specified criteria . The set of all possible subsequences of fixed length 𝐿 is represented as 𝑆 ′ ⊆ 𝑆 : | 𝑆 ′ | = 𝐿 , and from this set , for each feature 𝑓 , we extract one subsequence to form the set H F ( 𝑆 ) = { 𝑆 ′ 𝑓 ⊆ 𝑆 : | 𝑆 ′ 𝑓 | = 𝐿 | 𝑓 ∈ F } . The search function 𝑔 is defined as follows : 𝑔 ( T , 𝑅 ) = H F ( 𝑆 ) ( 5 ) Here , H F ( 𝑆 ) denotes the set containing a unique subsequence for each feature in F . The function 𝑔 maps the tensor of metrics T and the response set 𝑅 to H F ( 𝑆 ) , aligning each subsequence with the user - specified prompt for the corresponding feature . For clarity , we will refer to the unique subsequence for each feature 𝑓 , denoted as 𝑆 ′ 𝑓 , as the highlight ℎ 𝑖 ( as aforementioned defined ) for feature 𝑓 𝑖 . In PREDILECT , we elucidate further on the formulation of 𝑔 . For each triplet ( 𝑓 𝑘 , y 𝑘 , v 𝑘 ) in r 𝑖 , v 𝑘 is utilized to probe for either low or high metric values in T , with y 𝑘 delineating whether the identi - fied highlight is of a positive or negative nature . Consequently , 𝑔 is articulated to search for highlights corresponding to either the maximum or minimum metric values , contingent on v 𝑘 , and sub - sequently categorizes them based on the sentiment y 𝑘 as positive or negative highlights . Subsequently , we bifurcate H F ( 𝑆 ) into two distinct subsets , denoted as H + F and H −F , based on the sentiment of each feature . A query , as delineated in Sec . 3 . 1 , encapsulates a human pref - erence , symbolized as 𝑤 , and is paired with two trajectory seg - ments , represented as 𝑞 = ( 𝜎 1 , 𝜎 2 , 𝑤 ) . This definition is augmented by incorporating sets of positive and negative highlights , H + F = { ℎ + 𝑓 1 , . . . , ℎ + 𝑓 𝑛 } and H −F = { ℎ − 𝑓 1 , . . . , ℎ − 𝑓 𝑛 } , respectively , for each fea - ture . The enhanced quintuple , termed as sentiment highlighted query , is denoted as 𝑠ℎ𝑞 = ( 𝜎 1 , 𝜎 2 , 𝑤 , H + F , H −F ) and we provide a visualization of these highlights in Fig . 2 . Within PREDILECT , the sentiment highlighted queries are compiled in a dataset represented as D 𝑠ℎ𝑞 . segment Figure 2 : Representation of highlights within a segment . The segment 𝜎 outlined by a curve contains multiple highlights , two negative in H −F depicted in red and one positive H + F depicted in green ℎ + . All highlights are of the same length 𝐿 . 4 . 3 Reward Model Regularization Empirical studies illustrate the importance of strategic regular - ization in shaping state representations by enhancing the initial learning objective [ 18 ] . Auxiliary tasks , secondary yet semi - related to the primary task , offer valuable training signals for learning shared representations , thereby enhancing learning and data effi - ciency [ 32 , 44 , 45 , 52 , 63 ] . Regularization limits the search for solu - tions by adding bias . Using human natural language for shaping representations develops human - like biases and behaviors [ 5 , 38 ] . In PREDILECT , we devise a state representation task incorporat - ing causal reasoning from human teachers to reduce the entropy of the credit assignment when we only use preferences , refining the distinction between high and low - value sequences . This task is added as additional regularization terms to Eq . 2 to shape the reward function , aiming to maximize positive highlights ℎ + and minimize negative ones ℎ − . A discount is applied to preceding states , as in similar works predicting future rewards . L + = E ℎ + ∼D 𝑠ℎ𝑞 (cid:34) 𝐿 ∑︁ 𝑙 = 0 𝜆 𝑙 ˆ 𝑟 𝜓 ( 𝑠 𝑗 − 𝑙 , 𝑎 𝑗 − 𝑙 ) (cid:35) ( 6 ) L − = − E ℎ − ∼D 𝑠ℎ𝑞 (cid:34) 𝐿 ∑︁ 𝑙 = 0 𝜆 𝑙 ˆ 𝑟 𝜓 ( 𝑠 𝑗 − 𝑙 , 𝑎 𝑗 − 𝑙 ) (cid:35) ( 7 ) The final objective optimizes both L + ( Eq . 6 ) and L − ( Eq . 7 ) while sustaining the baseline preference learning loss , denoted as L 𝐶𝐸 ( see Eq . 2 ) . To optimize ˆ 𝑟 𝜓 , we utilize sentiment highlighted query samples 𝑠ℎ𝑞 drawn from D 𝑠ℎ𝑞 . The hyperparameters 𝛼 + and 𝛼 − are employed to assign weights to the two regularization terms . Consequently , the resulting learning objective takes the form : L PREDILECT = L 𝐶𝐸 + 𝛼 + L + + 𝛼 − L − ( 8 ) 4 . 4 Preference learning with PREDILECT Similar to other preference - based RL methods [ 16 , 39 ] , PREDILECT , as delineated in Alg . 1 , integrates policy with reward learning . In step A ( see Fig . 4 ) , policy 𝜋 𝜔 engages with the environment , yielding ( 𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 + 1 ) and estimates of ˆ 𝑟 𝜓 ( 𝑠 𝑡 , 𝑎 𝑡 ) . The resulting transitions PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA I prefer B because it reached the goal but the robot was too close to the group of humans and walls Which feature ( s ) was most important of [ Set of features ] ? The text given by the user is : [ user description ] Respond in the format for each relevant feature : [ feature : feature , sentiment : positive / negative , value : high / low ] . Prompt Intrinsic Features Goal Humansafety Speed Wall avoidance [ feature : Wall distance , sentiment : negative , value : low ] [ feature : Goal , sentiment : positive , value : low ] GPT - 4 Response [ feature : Human distance , sentiment : negative , value : low ] Figure 3 : An overview of how PREDILECT processes prompts from humans is as follows : Initially , a human provides a prompt , depicted in green , along with a set of intrinsic features F in purple which is environment dependant . Both are input into the LLM ( ChatGPT - 4 in the case of PREDILECT ) to generate a response r 𝑖 . Subsequently , after mapping a segment 𝜎 to a tensor of metrics T using the mapping function 𝑀 , we apply a searching function 𝑔 to obtain the set H F of highlights for each feature . These highlights are then utilized to train our reward model ˆ 𝑟 𝜓 as per Eq . 8 . ( 𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 + 1 , ˆ 𝑟 𝜓 ( 𝑠 𝑡 , 𝑎 𝑡 ) ) , organized into trajectories , are gathered in a temporary buffer . This buffer is utilized for gradient descent on 𝜋 𝜔 concerning 𝜔 , following the PPO algorithm [ 58 ] . Post training of 𝜋 𝜔 , a substantial number of trajectory segments are collected and stored in D 𝜎 . Subsequently , in step B , a feedback session takes place . We randomly select 𝑁 ∈ N + trajectory segments 𝜎 to obtain query preferences from humans . In addition , they can provide optional auxiliary language feedback . Given the human prompt and the set of intrinsic features , we utilize the LLM to derive a response r . After transforming the segment 𝜎 into a tensor of metrics T using 𝑀 , we employ the search function 𝑔 to extract both positive H + F and negative highlights H −F from segments and prompts , as described in Sec . 4 . 2 . All highlights are subsequently stored in the sentiment highlighted queries dataset D 𝑠ℎ𝑞 . In step C , gradient descent is executed on the parameters 𝜓 to refine our reward model ˆ 𝑟 𝜓 , with L PREDILECT serving as the learning objective . Upon acquiring an updated version of ˆ 𝑟 𝜓 , the process reverts to step A , and the algorithm iteratively progresses until convergence . Algorithm 1 : PREDILECT 1 D 𝜎 ← ∅ ; D 𝑠ℎ𝑞 ← ∅ ; F ← getFeatureSet ( ) ; 2 𝜋 𝜔 ← train ( 𝜋 𝜔 , ˆ 𝑟 𝜓 , 𝑒𝑛𝑣 ) ⊲ / / Step A 3 D 𝜎 ← sampleSegments ( 𝜋 𝜔 ) 4 while | D 𝑠ℎ𝑞 | ≤ 𝑁 do 5 ( 𝜎 1 , 𝜎 2 ) ← samplePairs ( D 𝜎 ) ⊲ / / Step B 6 𝑤 ← collectPreference ( 𝜎 1 , 𝜎 2 ) 7 prompt ← collectPrompt ( 𝜎 1 , 𝜎 2 , w ) 8 r ← LM ( prompt , F ) 9 T ← M ( 𝜎 ∗ ) 10 ( H + F , H −F ) ← g ( T , r ) 11 D 𝑠ℎ𝑞 ← D 𝑠ℎ𝑞 ∪ ( 𝜎 1 , 𝜎 2 , 𝑤 , H + F , H −F ) 12 for each gradient step do 13 Sample minibatch from D 𝑠ℎ𝑞 ⊲ / / Step C 14 Optimize ˆ 𝑟 𝜓 L PREDILECT with respect to 𝜓 in Eq . ( 8 ) 15 return ˆ 𝑟 𝜓 ⊲ / / return ˆ 𝑟 𝜓 + A B C LLM Figure 4 : Framework representation of PREDILECT . Step A : We train policy 𝜋 𝜔 and sample rollouts which are stored in D 𝜎 . Step B : We sample trajectory segments 𝜎 to query hu - mans and collect both preferences and prompts . The prompts are processed through an LLM to obtain responses . Those re - sponses are used to obtain highlights ( H + F , H −F ) from the pre - ferred segment 𝜎 ∗ Step C : The sentiment highlighted queries are collected to form dataset D 𝑠ℎ𝑞 and update the current reward model ˆ 𝑟 𝜓 . 5 EXPERIMENTS This section aims to examine the efficiency of PREDILECT and how incorporating human language to express preferences impacts the preference learning framework . To begin , we conduct experiments in simulated settings . We create an oracle that offers feedback to demonstrate how merging preference learning with textual expla - nations can enhance outcomes . We also carry out an experiment in a social robot navigation context , where a robot is trained using real human feedback sourced from Amazon Mechanical Turk ( MTurk ) participants . Social robot navigation is a complex task , requiring a balance between objectives such as reaching the destination , effi - ciency , and ( perceived human ) safety , making it a compelling case study [ 23 ] . We assess the effectiveness of PREDILECT with human feedback and how well the LLM can capture essential information from these textual descriptions to produce highlights . We used GPT - 4 for all experiments . We also demonstrate our ability to develop HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite PREDILECT 200 LLM 200 LLM 400 2378 635 968 Table 1 : Ablation comparing the final cumulative reward when using only L + , L − and PREDILECT for Cheetah . varied policies , such as those prioritizing safety , by only consid - ering features related to safety when training the human reward function . Our main hypotheses are summarized below : • H1 : PREDILECT will learn a human reward function more efficiently compared to the baseline . • H2 : PREDILECT can learn policies that put more focus on specific objectives as described by the user rather than more generalized policies learned by regular preference - based learning . • H3 : The LLM will accurately extract the information needed to create highlights . 5 . 1 Simulation experiment setup We aim to demonstrate the effectiveness of PREDILECT by con - ducting simulated experiments using the Reacher and Cheetah environments from OpenAI Gym [ 11 ] . In these simulations , an oracle compares two segments and provides preferences based on which segment achieves a higher cumulative reward , as determined by the true environment reward function . Recognizing that human instructors are not infallible , we introduce a 10 % error rate in the oracle’s feedback to mimic potential human inaccuracies . We’ve also extended the oracle to work with PREDILECT . Besides just indicating a preference , the oracle also offers explanations for its choices . It does this by monitoring the values of certain features within a segment . If a feature’s value surpasses or falls below a set threshold , the oracle will add this feature for highlighting . This can be seen as PREDILECT after the first LLM processing step . The specific features that the Oracle monitors vary by environment . For Cheetah , we use the x - axis velocity ; for Reacher , we use the distance between the fingers and the goal . Further details can be found in the Appendix . 5 . 2 Simulation experiment results Upon initial observation , it’s evident that utilizing highlights based on features results in quicker convergence compared to relying solely on preference - based learning ( see Fig 5 ) . The results derive from highlights pinpointing more specific and non - sequential areas of interest that align with the provided description . This advan - tage is evident in the reward curves for both Cheetah and Reacher . Notably , this enhanced performance with PREDILECT is achieved using only half the number of queries typically required by tradi - tional preference - based learning . Tab . 1 further demonstrates that the high convergence stems from the multi - modal feedback . The simulated results offer support for H1 due to the higher convergence and reduction of queries . 5 . 3 Real human feedback experiment setup To understand how textual descriptions affect the learned reward model , we perform experimentation using real human feedback in a social navigation scenario . The purpose of this experiment is threefold . 1 ) validate that PREDILECT performs better than baseline using real human feedback ; 2 ) show that the policies learned can be more aligned with the participant’s preferences ; 3 ) show that the LLM can accurately deduce the information needed from the textual descriptions . The social navigation scenario is built using Unity and involves a Pepper robot navigating between three humans in order to collect a goal shaped like a star ( see Fig 6 ) that acts as a guide for the robot . In order to sense its environment the robot is equipped with lidar rays that can detect humans , walls , and the end goal . To ensure safety , the robot follows the social force model which treats the human and robot as repelling forces [ 27 ] . One of the actions the robot can take is to lower and increase the social force which will compel the robot to keep its distance from the humans . For PREDILECT we add the features for goal distance , human distance , and speed to the prompt . The agent runs for 500 , 000 timesteps and updates the reward function once at the start as a single batch . 5 . 3 . 1 Participants . In total , 43 individuals were recruited from Amazon Mechanical Turk to participate in the experiment : 21 were assigned to the PREDILECT group and 22 to the baseline group . However , three participants were excluded due to not passing an attention check , resulting in an even 20 participants for each group . The participants’ ages ranged from 22 to 59 years . Of them , 64 . 9 % identified as male and 35 . 1 % as female . The experiment lasted about 25 minutes and participant were compensated at a rate of approxi - mately $ 10 per hour for their time . 5 . 3 . 2 Procedure . After agreeing to participate in the experiment , participants went through a tutorial explaining the goal and the UI . They were instructed to embody the humans in the environment and rate the queries based on their comfort and how well the robot achieved its goal . Participants were then provided with pairs of videos that represent two segments collected by the robot following a pre - trained policy 𝜋 𝜔 forming a query , and had to indicate which video they preferred . If they couldn’t decide which video they pre - ferred , they had the option to say " none " . After each preference , they were moved to the next query pair until they had labeled 20 queries in total . At the end of the experiment , the participants are asked to fill in a demographic survey . In the PREDILECT trial , before proceeding to the next query , participants were prompted with a question : " Please describe why you preferred video A / B . " They were provided with a text box to record their response . If they were uncertain about their preference reasons , they were instructed to leave the text box blank . To assist them in recalling the video’s content , the selected video remained centered on the screen , allowing participants to rewatch it instead of relying solely on memory . Table 3 includes some examples of textual descriptions given by participants and how the LLM module in PREDILECT mapped these into features . We conducted separate trials for both PREDILECT and the base - line , gathering 400 preferences for each . Both trials began with the same initial set of segments and preference pairs , all generated from the same policy 𝜋 𝜔 . The distinctions between the two arise from how they evaluate preference pairs and the inclusion of textual descriptions in PREDILECT . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA 0 100 200 300 400 500 Steps ( 1k ) 80 70 60 50 40 30 20 E n v R e w a r d Reacher 200 - PREDILECT 400 - Baseline 0 250 500 750 1000 1250 1500 1750 2000 Steps ( 1k ) 0 500 1000 1500 2000 2500 3000 E n v R e w a r d Cheetah 200 - PREDILECT 400 - Baseline ( a ) ( b ) Figure 5 : Learning curves : for Reacher ( a ) , PREDILECT used 200 queries , Baseline used 400 queries ; for Cheetah ( b ) , PREDILECT used 200 queries , Baseline used 400 queries . Figure 6 : Social robot navigation environment used for the human experiments . 5 . 4 Real human feedback experiment results We further assess the algorithm’s efficiency by using actual human feedback . When comparing PREDILECT to baseline preference learning , PREDILECT converges to a higher reward and stays rela - tively stable at just above 0 . 8 reward while the baseline converges around 0 . 6 , as shown in Figure 7 . b . These results add further sup - port for Hypothesis H1 on top of the simulation results . Given that humans can further articulate their preferences using text , we be - lieve it’s beneficial to investigate how we can tailor policies based on these textual descriptions in order to validate our hypothesis H2 . With this in mind , we use PREDILECT while focusing solely on safety - related features , such as the proximity to humans and walls . Figure 7 . a illustrates a distinct difference in the robot’s appli - cation of social force when trained using only these safety - centric features compared to baseline . Initially , both the baseline and our method exhibit similar values . However , over time , the baseline value diminishes , which might be attributed to a trade - off between maintaining a safe distance and efficiently reaching the destination . The ability to have the robot use a higher social force by training using safety - based description offers support for H2 . To ensure that the LLM effectively and accurately extracts information from human descriptions and to validate hypothesis H3 , we compared its predictions with those of a human labeler , as detailed in Table 2 . The data reveals that the LLM correctly identifies features from the LLM metrics Feature Sentiment Magnitude Acc 85 . 71 % 77 . 14 % 80 % F . pos 11 . 54 % 13 . 04 % 8 % Table 2 : Accuracy and false positives of the LLM is at predict - ing the feature , sentiment , and magnitude based on human description when compared to a human labeler . descriptions 85 % of the time . It also has similar accuracy rates for determining sentiment ( ∼ 77 % ) and magnitude ( ∼ 80 % ) . While these accuracy rates are noteworthy , they also encompass instances where the LLM might overlook certain features that the human labeler identified . In such cases , we lose some information and revert to standard preferences . However , a more important concern arises when the LLM identifies or " hallucinates " features that the human never mentioned , as this could adversely impact performance . Our data shows that the LLM makes this error 11 . 54 % of the time when identifying features . Furthermore , in situations where both the human labeler and the LLM agree on a particular feature , there’s a discrepancy in sentiment interpretation in 13 % of the instances and in magnitude interpretation in 8 % of the in - stances . This error rate is based solely on the human descriptions . Interestingly , when considering our prompt we have a general goal description of the robot along with the human language description . When considering the goal description , most of the cases where there is an error , they are still aligned with the overarching goal description . With these accuracies and the fact that any error can mostly be explained with the prompts structure , we find support for H3 . Overall the participants chose the " none " alternative 13 . 75 % of the time , indicating no preference . For PREDILECT when there was a preference , the participants gave additional textual feedback in 61 . 44 % of the cases . 6 DISCUSSION Based on the results from both the simulated oracle experiments and the social robot navigation evaluations using real human feed - back , PREDILECT demonstrates a faster convergence compared to traditional preference learning . In the simulated environments , this HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite 0 100 200 300 400 500 Steps ( 1k ) 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 S o c i a l F o r c e Social Force PREDILECT Baseline 0 100 200 300 400 500 Steps ( 1k ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 E n v R e w a r d Reward PREDILECT Baseline ( a ) ( b ) Figure 7 : For the social robot navigation environment : a ) Social force when applying only safety features on PREDILECT compared to baseline ; b ) Reward curves for PREDILECT and baseline . Text Description Feat . Sent . Magn . got both stars and avoided hitting humans H . Dist , G . Dist Positive , Positive High , Low I prefer video A because of the fast speed . Speed Positive High Did not hit a human H . Dist Positive High Stayed a longer distance away from hitting any - thing and moved at a slower pace . H . Dist , Speed Positive , Negative High , High Table 3 : Human descriptions justifying their preferences and the feature , sentiment , and magnitude predicted by the LLM . higher efficiency is obtained using 50 % of the queries compared to the baseline . This superior performance can be attributed to the feedback’s capacity to mitigate causal confusion , as it enables humans to provide a clear rationale behind their preferences . Our results further illustrate by emphasizing particular features , such as those related to safety , our system can tailor policies based on user explanations rather than merely adopting a generic policy . For instance , within the social navigation context , the robot consis - tently maintained a higher level of social force during training . In contrast , the baseline reduced the social force over time . We contend that the integration of textual explanations for preferences allows the system to more rapidly align with human preferences , elimi - nating the need for over - querying making this approach relevant to several robotics applications . We evaluated the LLM’s capability to extract relevant informa - tion from text descriptions provided by humans . The LLM demon - strated high accuracy , closely aligned with the feature identification by a human labeler . While there were occasional discrepancies , a de - tailed analysis revealed that the LLM’s interpretations , even when diverging from participants’ descriptions , frequently aligned with the robot’s overarching objective . This may be attributed to the context provided within our prompt , which briefly described the robot’s task . It is noteworthy that the LLM considers the entirety of the context presented in the prompt unless explicitly directed otherwise . This observation is insightful for two reasons : it under - scores the importance of crafting prompts , and it highlights the potential benefits of combining a short summary of the general goal with human descriptions , enabling the robot to make implicit inferences when possible . While such inferences can be advanta - geous in certain scenarios , they might not be suitable in others . We believe that refining our prompts could further reduce the error rate . For example , if the goal was personalization , we could empha - size the importance of the individual human description and ask the LLM to not draw implicit inferences based on the overarching objective . When accounting for both the global goal and the human descriptions , the error rate was further reduced to lower single digits . Interestingly , the LLM most frequently misinterpreted the ’speed’ feature . This could be attributed to the inherent challenge of balancing two objectives : reaching the destination ( faster speed ) and ensuring perceived safety ( slower speed ) . Following this , one natural limitation is that successful parsing of language descrip - tions depends on how well the prompt is formulated . Finally , our comparison uses ’preferences only’ as the baseline . 7 CONCLUSION In this paper , we introduced PREDILECT , a framework that lever - ages the zero - shot capabilities of LLMs to expand the amount of in - formation available , when inferring a reward function from human preferences , in an effort to mitigate causal confusion . By combining preferences with language we can extract more information per query while offering a natural way of interacting with robots . We believe that the combination of these modalities is a promising approach to improve learning from human feedback . ACKNOWLEDGMENTS This research has been carried out as part of the Vinnova Com - petence Center for Trustworthy Edge Computing Systems and Applications at KTH , and partially supported by the Swedish Foun - dation for Strategic Research ( SSF FFL18 - 0199 ) and the Wallenberg AI , Autonomous Systems and Software Program ( WASP ) funded by the Knut and Alice Wallenberg Foundation . All of the authors are with the Division of Robotics , Perception and Learning , School of Electrical Engineering and Computer Science , KTH Royal Institute of Technology , 114 28 Stockholm , Sweden . The authors are also affiliated with Digital Futures . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA REFERENCES [ 1 ] Kareem Amin , Nan Jiang , and Satinder Singh . 2017 . Repeated inverse reinforce - ment learning . Advances in neural information processing systems 30 ( 2017 ) . [ 2 ] DarioAmodei , ChrisOlah , JacobSteinhardt , PaulChristiano , JohnSchulman , and Dan Mané . 2016 . Concrete problems in AI safety . arXiv preprint arXiv : 1606 . 06565 ( 2016 ) . [ 3 ] Chandrayee Basu , Mukesh Singhal , and Anca D Dragan . 2018 . Learning from richer human guidance : Augmenting comparison - based learning with feature queries . In Proceedings of the 2018 ACM / IEEE International Conference on Human - Robot Interaction . 132 – 140 . [ 4 ] Erik Båvenstrand and Jakob Berggren . 2019 . Performance evaluation of imitation learning algorithms with human experts . [ 5 ] Kush Bhatia , Ashwin Pananjady , Peter Bartlett , Anca Dragan , and Martin J Wainwright . 2020 . Preference learning along multiple criteria : A game - theoretic perspective . Advances in neural information processing systems 33 ( 2020 ) , 7413 – 7424 . [ 6 ] Erdem Bıyık , Nicolas Huynh , Mykel J Kochenderfer , and Dorsa Sadigh . 2020 . Active preference - based gaussian process regression for reward learning . In Robotics : Science and Systems ( RSS ) . [ 7 ] Andreea Bobu , Dexter RR Scobee , Jaime F Fisac , S Shankar Sastry , and Anca D Dragan . 2020 . Less is more : Rethinking probabilistic models of human behav - ior . In Proceedings of the 2020 acm / ieee international conference on human - robot interaction . 429 – 437 . [ 8 ] Rishi Bommasani , Drew A Hudson , Ehsan Adeli , Russ Altman , Simran Arora , Sydney von Arx , Michael S Bernstein , Jeannette Bohg , Antoine Bosselut , Emma Brunskill , et al . 2021 . On the opportunities and risks of foundation models . arXiv preprint arXiv : 2108 . 07258 ( 2021 ) . [ 9 ] Serena Booth , Sanjana Sharma , Sarah Chung , Julie Shah , and Elena L Glassman . 2022 . Revisiting Human - Robot Teaching and Learning Through the Lens of Human Concept Learning . In Proceedings of the 2022 ACM / IEEE International Conference on Human - Robot Interaction . 147 – 156 . [ 10 ] Ralph Allan Bradley and Milton E Terry . 1952 . Rank analysis of incomplete block designs : I . Themethodofpairedcomparisons . Biometrika 39 , 3 / 4 ( 1952 ) , 324 – 345 . [ 11 ] Greg Brockman , Vicki Cheung , Ludwig Pettersson , Jonas Schneider , John Schul - man , Jie Tang , and Wojciech Zaremba . 2016 . Openai gym . arXiv preprint arXiv : 1606 . 01540 ( 2016 ) . [ 12 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . Language models are few - shot learners . Advances in neural information processing systems 33 ( 2020 ) , 1877 – 1901 . [ 13 ] Maya Cakmak , Siddhartha S Srinivasa , Min Kyung Lee , Jodi Forlizzi , and Sara Kiesler . 2011 . Human preferences for robot - human hand - over configurations . In 2011 IEEE / RSJ International Conference on Intelligent Robots and Systems . IEEE , 1986 – 1993 . [ 14 ] MarkChen , JerryTworek , HeewooJun , QimingYuan , HenriquePondedeOliveira Pinto , Jared Kaplan , Harri Edwards , Yuri Burda , Nicholas Joseph , Greg Brockman , et al . 2021 . Evaluating large language models trained on code . arXiv preprint arXiv : 2107 . 03374 ( 2021 ) . [ 15 ] Yanbei Chen , Xiatian Zhu , Wei Li , and Shaogang Gong . 2020 . Semi - supervised learningunderclassdistributionmismatch . In ProceedingsoftheAAAIConference on Artificial Intelligence , Vol . 34 . 3569 – 3576 . [ 16 ] Paul F Christiano , Jan Leike , Tom Brown , Miljan Martic , Shane Legg , and Dario Amodei . 2017 . Deep reinforcement learning from human preferences . Advances in neural information processing systems 30 ( 2017 ) . [ 17 ] Yuchen Cui and Scott Niekum . 2018 . Active reward learning from critiques . In 2018 IEEE international conference on robotics and automation ( ICRA ) . IEEE , 6907 – 6914 . [ 18 ] Tim De Bruin , Jens Kober , Karl Tuyls , and Robert Babuška . 2018 . Integrating state representation learning into deep reinforcement learning . IEEE Robotics and Automation Letters 3 , 3 ( 2018 ) , 1394 – 1401 . [ 19 ] Pim De Haan , Dinesh Jayaraman , and Sergey Levine . 2019 . Causal confusion in imitation learning . Advances in Neural Information Processing Systems 32 ( 2019 ) . [ 20 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . Bert : Pre - trainingofdeepbidirectionaltransformersforlanguageunderstanding . arXiv preprint arXiv : 1810 . 04805 ( 2018 ) . [ 21 ] Frederick Eberhardt . 2017 . Introduction to the foundations of causal discovery . International Journal of Data Science and Analytics 3 , 2 ( 2017 ) , 81 – 91 . [ 22 ] Peter Florence , Lucas Manuelli , and Russ Tedrake . 2019 . Self - supervised corre - spondence in visuomotor policy learning . IEEE Robotics and Automation Letters 5 , 2 ( 2019 ) , 492 – 499 . [ 23 ] Anthony Francis , Claudia Pérez - d’Arpino , Chengshu Li , Fei Xia , Alexandre Alahi , RachidAlami , AniketBera , AbhijatBiswas , JoydeepBiswas , RohanChandra , etal . 2023 . Principles and guidelines for evaluating social robot navigation algorithms . arXiv preprint arXiv : 2306 . 16740 ( 2023 ) . [ 24 ] Efstratios Gavves , Thomas Mensink , Tatiana Tommasi , Cees GM Snoek , and Tinne Tuytelaars . 2015 . Active transfer learning with zero - shot priors : Reusing past datasets for future tasks . In Proceedings of the IEEE International Conference on Computer Vision . 2731 – 2739 . [ 25 ] Dylan Hadfield - Menell , Smitha Milli , Pieter Abbeel , Stuart J Russell , and Anca Dragan . 2017 . Inverse reward design . Advances in neural information processing systems 30 ( 2017 ) . [ 26 ] Donald Joseph Hejna III and Dorsa Sadigh . 2023 . Few - shot preference learning for human - in - the - loop rl . In Conference on Robot Learning . PMLR , 2014 – 2025 . [ 27 ] Dirk Helbing and Péter Molnár . 1995 . Social force model for pedestrian dynamics . Physical Review E 51 , 5 ( 1995 ) , 4282 – 4286 . [ 28 ] AshleyHill , AntoninRaffin , MaximilianErnestus , AdamGleave , AnssiKanervisto , Rene Traore , Prafulla Dhariwal , Christopher Hesse , Oleg Klimov , Alex Nichol , Matthias Plappert , Alec Radford , John Schulman , Szymon Sidor , and Yuhuai Wu . 2018 . Stable Baselines . https : / / github . com / hill - a / stable - baselines . [ 29 ] Namgyu Ho , Laura Schmid , and Se - Young Yun . 2022 . Large language models are reasoning teachers . arXiv preprint arXiv : 2212 . 10071 ( 2022 ) . [ 30 ] Robert Hu , Siu Lun Chau , Jaime Ferrando Huertas , and Dino Sejdinovic . 2022 . Explaining Preferences with Shapley Values . In Advances in Neural Informa - tion Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = - me36V0os8P [ 31 ] Borja Ibarz , Jan Leike , Tobias Pohlen , Geoffrey Irving , Shane Legg , and Dario Amodei . 2018 . Reward learning from human preferences and demonstrations in atari . Advances in neural information processing systems 31 ( 2018 ) . [ 32 ] MaxJaderberg , VolodymyrMnih , WojciechMarianCzarnecki , TomSchaul , JoelZ Leibo , David Silver , and Koray Kavukcuoglu . 2016 . Reinforcement learning with unsupervised auxiliary tasks . arXiv preprint arXiv : 1611 . 05397 ( 2016 ) . [ 33 ] Hong Jun Jeon , Smitha Milli , and Anca Dragan . 2020 . Reward - rational ( implicit ) choice : Aunifyingformalismforrewardlearning . AdvancesinNeuralInformation Processing Systems 33 ( 2020 ) , 4415 – 4426 . [ 34 ] Huda Khayrallah , Sean Trott , and Jerome Feldman . 2015 . Natural language for human robot interaction . In International Conference on Human - Robot Interaction ( HRI ) . [ 35 ] W Bradley Knox and Peter Stone . 2009 . Interactively shaping agents via human reinforcement : The TAMER framework . In Proceedings of the fifth international conference on Knowledge capture . 9 – 16 . [ 36 ] W Bradley Knox , Peter Stone , and Cynthia Breazeal . 2013 . Training a robot via human feedback : A case study . In International Conference on Social Robotics . Springer , 460 – 470 . [ 37 ] Tejas D Kulkarni , Ankush Gupta , Catalin Ionescu , Sebastian Borgeaud , Malcolm Reynolds , AndrewZisserman , andVolodymyrMnih . 2019 . Unsupervisedlearning of object keypoints for perception and control . Advances in neural information processing systems 32 ( 2019 ) . [ 38 ] Sreejan Kumar , Carlos G Correa , Ishita Dasgupta , Raja Marjieh , Michael Y Hu , Robert Hawkins , Jonathan D Cohen , Karthik Narasimhan , Tom Griffiths , et al . 2022 . Usingnaturallanguageandprogramabstractionstoinstillhumaninductive biases in machines . Advances in Neural Information Processing Systems 35 ( 2022 ) , 167 – 180 . [ 39 ] Kimin Lee , Laura Smith , and Pieter Abbeel . 2021 . Pebble : Feedback - efficient interactive reinforcement learning via relabeling experience and unsupervised pre - training . arXiv preprint arXiv : 2106 . 05091 ( 2021 ) . [ 40 ] Zhihao Li , Yishan Mu , Zhenglong Sun , Sifan Song , Jionglong Su , and Jiaming Zhang . 2021 . Intention understanding in human – robot interaction based on visual - NLP semantics . Frontiers in Neurorobotics 14 ( 2021 ) , 610139 . [ 41 ] Xinran Liang , Katherine Shu , Kimin Lee , and Pieter Abbeel . 2022 . Reward Uncer - tainty for Exploration in Preference - based Reinforcement Learning . In Interna - tional Conference on Learning Representations . https : / / openreview . net / forum ? id = OWZVD - l - ZrC [ 42 ] Runze Liu , Fengshuo Bai , Yali Du , and Yaodong Yang . 2022 . Meta - Reward - Net : Implicitly Differentiable Reward Learning for Preference - based Reinforce - ment Learning . In Advances in Neural Information Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = OZKBReUF - wX [ 43 ] James MacGlashan , Mark K Ho , Robert Loftin , Bei Peng , Guan Wang , David L Roberts , Matthew E Taylor , and Michael L Littman . 2017 . Interactive learning frompolicy - dependenthumanfeedback . In Int . Conf . onMachineLearning . PMLR , 2285 – 2294 . [ 44 ] JanMatas , StephenJames , andAndrewJDavison . 2018 . Sim - to - realreinforcement learning for deformable object manipulation . In Conference on Robot Learning . PMLR , 734 – 743 . [ 45 ] Piotr Mirowski , Razvan Pascanu , Fabio Viola , Hubert Soyer , Andrew J Ballard , Andrea Banino , Misha Denil , Ross Goroshin , Laurent Sifre , Koray Kavukcuoglu , et al . 2016 . Learning to navigate in complex environments . arXiv preprint arXiv : 1611 . 03673 ( 2016 ) . [ 46 ] Ron Mokady , Amir Hertz , and Amit H Bermano . 2021 . Clipcap : Clip prefix for image captioning . arXiv preprint arXiv : 2111 . 09734 ( 2021 ) . [ 47 ] Anis Najar and Mohamed Chetouani . 2021 . Reinforcement learning with human advice : a survey . Frontiers in Robotics and AI 8 ( 2021 ) , 584075 . [ 48 ] Jiquan Ngiam , Aditya Khosla , Mingyu Kim , Juhan Nam , Honglak Lee , and An - drew Y Ng . 2011 . Multimodal deep learning . In ICML . [ 49 ] Jongjin Park , Younggyo Seo , Jinwoo Shin , Honglak Lee , Pieter Abbeel , and Kimin Lee . 2022 . SURF : Semi - supervised Reward Learning with Data Augmentation HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite for Feedback - efficient Preference - based Reinforcement Learning . In Interna - tional Conference on Learning Representations . https : / / openreview . net / forum ? id = TfhfZLQ2EJO [ 50 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gre - gory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Kopf , Edward Yang , Zachary DeVito , Martin Rai - son , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . PyTorch : An Imperative Style , High - Performance Deep Learning Library . In Advances in Neural Information Processing Systems 32 , H . Wallach , H . Larochelle , A . Beygelzimer , F . d ' Alché - Buc , E . Fox , and R . Garnett ( Eds . ) . Curran Associates , Inc . , 8024 – 8035 . [ 51 ] Seth Pate , Wei Xu , Ziyi Yang , Maxwell Love , Siddarth Ganguri , and Lawson LS Wong . 2021 . Natural language for human - robot collaboration : Problems beyond language grounding . arXiv preprint arXiv : 2110 . 04441 ( 2021 ) . [ 52 ] Lerrel Pinto and Abhinav Gupta . 2017 . Learning to push by grasping : Using multiple tasks for effective learning . In 2017 IEEE international conference on robotics and automation ( ICRA ) . IEEE , 2161 – 2168 . [ 53 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , etal . 2021 . Learningtransferablevisualmodelsfromnaturallanguagesupervision . In International Conference on Machine Learning . PMLR , 8748 – 8763 . [ 54 ] Ilija Radosavovic , Piotr Dollár , Ross Girshick , Georgia Gkioxari , and Kaiming He . 2018 . Data distillation : Towards omni - supervised learning . In Proceedings of the IEEE conference on computer vision and pattern recognition . 4119 – 4128 . [ 55 ] Pranav Rajpurkar , Robin Jia , and Percy Liang . 2018 . Know what you don’t know : Unanswerable questions for SQuAD . arXiv preprint arXiv : 1806 . 03822 ( 2018 ) . [ 56 ] Dorsa Sadigh , Anca D Dragan , Shankar Sastry , and Sanjit A Seshia . 2017 . Active preference - based learning of reward functions . [ 57 ] MariahLSchrum , ErinHedlund - Botti , NinaMoorman , andMatthewCGombolay . 2022 . MIND MELD : Personalized Meta - Learning for Robot - Centric Imitation Learning . . In HRI . 157 – 165 . [ 58 ] John Schulman , Filip Wolski , Prafulla Dhariwal , Alec Radford , and Oleg Klimov . 2017 . Proximal policy optimization algorithms . arXiv preprint arXiv : 1707 . 06347 ( 2017 ) . [ 59 ] DevinSchwab , YifengZhu , andManuelaVeloso . 2018 . Zeroshottransferlearning forrobotsoccer . In Proceedingsofthe17thInternationalConferenceonAutonomous Agents and MultiAgent Systems . 2070 – 2072 . [ 60 ] Emmanuel Senft , Paul Baxter , James Kennedy , Séverin Lemaignan , and Tony Belpaeme . 2017 . Supervised autonomy for online learning in human - robot interaction . Pattern Recognition Letters 99 ( 2017 ) , 77 – 86 . [ 61 ] Dhruv Shah , Arjun Bhorkar , Hrishit Leen , Ilya Kostrikov , Nicholas Rhinehart , and Sergey Levine . 2022 . Offline Reinforcement Learning for Visual Navigation . In 6th Annual Conference on Robot Learning . https : / / openreview . net / forum ? id = uhIfIEIiWm _ [ 62 ] PratyushaSharma , BalakumarSundaralingam , ValtsBlukis , ChrisPaxton , Tucker Hermans , Antonio Torralba , Jacob Andreas , and Dieter Fox . 2022 . Correcting robot plans with natural language feedback . In Robotics : Science and Systems ( RSS ) . [ 63 ] Evan Shelhamer , Parsa Mahmoudieh , Max Argus , and Trevor Darrell . 2016 . Loss is its own reward : Self - supervision for reinforcement learning . arXiv preprint arXiv : 1612 . 07307 ( 2016 ) . [ 64 ] Jae Woong Soh , Sunwoo Cho , and Nam Ik Cho . 2020 . Meta - transfer learning for zero - shot super - resolution . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 3516 – 3525 . [ 65 ] Peter Spirtes , Clark N Glymour , Richard Scheines , and David Heckerman . 2000 . Causation , prediction , and search . MIT press . [ 66 ] TheodoreSumers , RobertD . Hawkins , MarkKHo , ThomasL . Griffiths , andDylan Hadfield - Menell . 2022 . How to talk so AI will learn : Instructions , descriptions , and autonomy . In Advances in Neural Information Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = ZLsZmNe1RDb [ 67 ] Romal Thoppilan , Daniel De Freitas , Jamie Hall , Noam Shazeer , Apoorv Kul - shreshtha , Heng - TzeCheng , AliciaJin , TaylorBos , LeslieBaker , YuDu , etal . 2022 . Lamda : Language models for dialog applications . arXiv preprint arXiv : 2201 . 08239 ( 2022 ) . [ 68 ] Jeremy Tien , Jerry Zhi - Yang He , Zackory Erickson , Anca D Dragan , and Daniel Brown . 2022 . A Study of Causal Confusion in Preference - Based Reward Learning . arXiv preprint arXiv : 2204 . 06601 ( 2022 ) . [ 69 ] Maria Tsimpoukelli , Jacob L Menick , Serkan Cabi , SM Eslami , Oriol Vinyals , and Felix Hill . 2021 . Multimodal few - shot learning with frozen language models . Advances in Neural Information Processing Systems 34 ( 2021 ) , 200 – 212 . [ 70 ] Xiaofei Wang , Kimin Lee , Kourosh Hakhamaneshi , Pieter Abbeel , and Michael Laskin . 2022 . Skill preferences : Learning to extract and execute robotic skills from human feedback . In Conference on Robot Learning . PMLR , 1259 – 1268 . [ 71 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Fei Xia , Ed Chi , QuocVLe , DennyZhou , etal . 2022 . Chain - of - thoughtpromptingelicitsreasoning in large language models . Advances in Neural Information Processing Systems 35 ( 2022 ) , 24824 – 24837 . [ 72 ] Aaron Wilson , Alan Fern , and Prasad Tadepalli . 2012 . A bayesian approach for policylearningfromtrajectorypreferencequeries . Advancesinneuralinformation processing systems 25 ( 2012 ) . [ 73 ] Christian Wirth , Riad Akrour , Gerhard Neumann , Johannes Fürnkranz , et al . 2017 . A survey of preference - based reinforcement learning methods . Journal of Machine Learning Research 18 , 136 ( 2017 ) , 1 – 46 . [ 74 ] Annie Xie , Avi Singh , Sergey Levine , and Chelsea Finn . 2018 . Few - shot goal inference for visuomotor learning and planning . In Conference on Robot Learning . PMLR , 40 – 52 . [ 75 ] Qizhe Xie , Minh - Thang Luong , Eduard Hovy , and Quoc V Le . 2020 . Self - training with noisy student improves imagenet classification . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 10687 – 10698 . [ 76 ] Wei Ying , Yu Zhang , Junzhou Huang , and Qiang Yang . 2018 . Transfer learning via learning to transfer . In International Conference on Machine Learning . PMLR , 5085 – 5094 . [ 77 ] Kevin Zakka , Andy Zeng , Pete Florence , Jonathan Tompson , Jeannette Bohg , and Debidatta Dwibedi . 2022 . Xirl : Cross - embodiment inverse reinforcement learning . In Conference on Robot Learning . PMLR , 537 – 546 . [ 78 ] AndyZeng , MariaAttarian , KrzysztofMarcinChoromanski , AdrianWong , Stefan Welker , Federico Tombari , Aveek Purohit , Michael S Ryoo , Vikas Sindhwani , Johnny Lee , et al . 2022 . Socratic Models : Composing Zero - Shot Multimodal Reasoning with Language . In The Eleventh International Conference on Learning Representations . [ 79 ] XiaohuaZhai , XiaoWang , BasilMustafa , AndreasSteiner , DanielKeysers , Alexan - der Kolesnikov , and Lucas Beyer . 2021 . LiT : Zero - Shot Transfer with Locked - image Text Tuning . arXiv preprint arXiv : 2111 . 07991 ( 2021 ) . [ 80 ] Ruohan Zhang , Dhruva Bansal , Yilun Hao , Ayano Hiranaka , Jialu Gao , Chen Wang , Roberto Martín - Martín , Li Fei - Fei , and Jiajun Wu . 2022 . A Dual Repre - sentation Framework for Robot Learning with Human Guidance . In 6th Annual Conference on Robot Learning . https : / / openreview . net / forum ? id = H6rr _ CGzV9y PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI ’24 , March 11 – 14 , 2024 , Boulder , CO , USA Table 5 : PPO hyperparameters Value Walker2D Cheetah Social Nav . Network architecture { hidden , output } act . ( Actor ) { tanh , tanh } { relu , tanh } { relu , tanh } { hidden , output } act . ( Critic ) { tanh , tanh } { relu , tanh } { relu , tanh } Hidden layers ( Actor ) { 64 , 64 } { 256 , 256 } { 128 , 128 } Hidden layers ( Critic ) { 64 , 64 } { 256 , 256 } { 128 , 128 } RL parameters Critic learning rate 5e - 05 2e - 05 3e - 04 Actor learning rate 5e - 05 2e - 05 3e - 04 Batch size 32 64 128 Discount factor 0 . 99 0 . 98 0 . 99 n steps 512 512 1024 n epochs 20 20 10 GAE lambda ( 𝜏 ) 0 . 95 0 . 92 0 . 99 Clip range 0 . 1 0 . 2 0 . 2 Normalized advantage True True True Entropy coefficient 6e - 04 4e - 04 5e - 04 VF coefficient 0 . 87 0 . 5 0 . 5 8 APPENDIX 8 . 1 Implementation details For our implementation , we use a PPO [ 58 ] algorithm implementa - tion from Stable Baselines [ 28 ] . We modified the normal RL loop to make it conform to our preference learning implementation as de - scribed in the main paper . We implemented the reward model from scratch based on details from Cristiano et al . [ 16 ] in PyTorch [ 50 ] . In addition , we extend the reward model to enable the usage of highlights as an additional learning objective . To collect segments , we temporarily stop training and execute the current policy using undirected homogenous exploration [ 73 ] . The segments are collected sequentially with a randomly sized interval to increase the diversity . We handle terminal states by restarting the environment while continuing the same segment . Another considered method was to end the segment and fill the rest with some value without meaning . After consideration , we chose the restart method to ensure that no information about the terminal state was leaked into the segments . We used uniform random sampling to generate the queries . The reward function’s architecture includes hidden layers with dimensions of ( 256 , 256 , 256 ) . These layers primarily use ReLU activation functions with the output layer using a tanh activation . Both the agent’s policy and value function have a structure of ( 128 , 128 ) and use ReLU activations . The reward function’s network architecture was designed as a neural network . This network had hidden layers with dimensions of ( 255 , 255 , 255 ) and primarily employed ReLU activation functions . Only the final layer used a tanh activation , ensuring output values remained between - 1 and 1 . The agent’s policy and value network was of size ( 128 , 128 ) for Reacher and ( 256 , 256 ) for Cheetah . Both were using ReLU activations for the hidden layers . In both Cheetah and Reacher , every 20K timesteps , the agent updates its reward function . With each update , we use 10 % of the total available queries until it used up the query quota . 8 . 2 Hyperparameters Table 4 : Reward hyperparameters Value Network architecture { hidden , output } activation { relu , tanh } Hidden sizes { 256 , 256 , 256 } Preference Learning Segment size 50 Initial amount of queries 1 / 10 of queries Initial training epochs 200 Queries per update 1 / 10 of queries Training epochs per update 50 Batch size 128 Timesteps between updates 20K Learning rate 0 . 0003