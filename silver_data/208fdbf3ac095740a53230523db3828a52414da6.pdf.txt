PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning Simon Holk âˆ— sholk @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Daniel Marta âˆ— dlmarta @ kth . se KTH Royal Institute of Technology Stockholm , Sweden Iolanda Leite iolanda @ kth . se KTH Royal Institute of Technology Stockholm , Sweden ABSTRACT Preference - based reinforcement learning ( RL ) has emerged as a new field in robot learning , where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state - action pairs . However , formulating realistic policies for robots demands responses from humans to an extensive array of queries . In this work , we approach the sample - efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting . To accomplish this , we leverage the zero - shot capabilities of a large language model ( LLM ) to reason from the text provided by humans . To accommodate the additional query information , we reformulate the reward learning objectives to contain flexible highlights â€“ state - action pairs that contain relatively high information and are related to the features processed in a zero - shot fashion from a pretrained LLM . In both a simulated scenario and a user study , we reveal the effectiveness of our work by analyzing the feedback and its implications . Addi - tionally , the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape . We provide video examples of the trained policies at https : / / sites . google . com / view / rl - predilect CCS CONCEPTS â€¢ Computing methodologies â†’ Inverse reinforcement learn - ing ; Learning to rank . KEYWORDS Preference learning , Reinforcement learning , Interactive learning , Human - in - the - loop Learning ACM Reference Format : Simon Holk , Daniel Marta , and Iolanda Leite . 2024 . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning . In Proceedings of the 2024 ACM / IEEE International Conference on Human - Robot Interaction ( HRI â€™24 ) , March 11 â€“ 14 , 2024 , Boulder , CO , USA . ACM , NewYork , NY , USA , 11pages . https : / / doi . org / 10 . 1145 / 3610977 . 3634970 1 INTRODUCTION A key ingredient for the success of preference - based RL is that modern methods place minimal constraints on the modality of the âˆ— These authors contributed equally ThisworkislicensedunderaCreativeCommonsAttribution International 4 . 0 License . HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Â© 2024 Copyright held by the owner / author ( s ) . ACM ISBN 979 - 8 - 4007 - 0322 - 5 / 24 / 03 . https : / / doi . org / 10 . 1145 / 3610977 . 3634970 reward function [ 16 , 26 , 31 , 70 ] , facilitating the formulation of com - plex objectives for robotic applications . While preference - driven teaching incorporates the essential element of structural alignment , vital for designing intricate objectives [ 7 , 9 , 33 ] , its substantial reliance on extensive human feedback limits its applicability in real - world robotics [ 26 , 30 , 41 , 49 ] . Moreover , while preferences do show a strong correlation with causality [ 21 , 65 ] , there is evidence indicating that preferences may not be sufficient as a standalone modality to thoroughly delineate the causal relationship among states , actions , and rewards . This challenge is identified as causal confusion [ 19 , 68 ] . In a comprehensive empirical study on causal confusion in preference - based RL , Tien et al . [ 68 ] highlighted that the introduction of spurious features and a rise in model capacity can induce causal confusion regarding the true reward function , even when learning from thousands of pairwise preferences . Over - looking this aspect can result in spurious correlations , which may ultimately lead to either reward exploitation [ 2 , 25 ] or the creation of a distributional shift [ 19 ] . Both scenarios necessitate additional interactions and can result in diminished performance , representing a failure in reward inference and leading to suboptimal behaviors . We believe that a balance can be struck between the easiness of providing preferences [ 16 , 26 ] and offering an optional natural language interface for humans , in an effort to uncover the true causal relationship , thus greatly reducing the entropy of credit assignment , as natural language presents a more natural way for humans to interact [ 34 , 40 , 51 ] . While the integration of natural language can pose a significant challenge on its own , we leverage recent advancements in large pretrained foundational models [ 8 ] , such as BERT [ 20 ] , CLIP with GPT - 2 [ 46 ] , and GPT - 3 [ 12 ] . These models excel in various tasks , including text completion , image - text similarity , image captioning , and robot planning [ 78 ] . Additional evidence indicates that adequately large language models possess the capability to execute complex reasoning [ 29 , 78 ] , potentially revealing causal reasoning from human prompts and thereby mit - igating aspects of causal confusion . For instance , Wei et al . [ 71 ] demonstrated that the generation of a thought chainâ€”a sequence of intermediate reasoning stepsâ€” enables the emergence of advanced reasoning abilities in sufficiently expansive language models . To address the aforementioned limitations we leverage the zero - shot capabilities of pretrained models , we introduce PREDILECT : PRE ferences D el I neated with Zero - Shot L anguag E - based Reason - ing in Reinfor C emen T Learning . Figure 1 provides a macroscopic depiction of our approach . Consider the two trajectories on the left side of the figure . While the queried person preferred trajectory B , when asked to justify their preference they mentioned that the robot being too close to the group of humans was unnecessary . In typical preference learning methods , where only the preferred a r X i v : 2402 . 15420v1 [ c s . R O ] 23 F e b 2024 HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite I prefer B because it reached the goal but the robot was too close to the group of humans A B LLM FeatureSentimentMagnitude Preference Timestep R e w a r d Intrinsic features Goal Humansafety Speed Trajectory B Wall avoidance Figure 1 : An overview of PREDILECT in a social navigation scenario : Initially , a human is shown two trajectories , A and B . They signal their preference for one of the trajectories and provide an additional text prompt to elaborate on their insights . Subsequently , an LLM can be employed for extracting feature sentiment , revealing the causal reasoning embedded in their text prompt , which is processed and mapped to a set of intrinsic values . Finally , both the preferences and the highlighted insights are utilized to more accurately define a reward function . trajectory is used as feedback , the result of this query could lead to causal confusion . PREDILECT addresses the limitations inherent to preference - based RL by delineating preferences with highlights ( sequences of state - action pairs ) from the sentiment analysis of an LLM . The goal is to enhance the granularity of the reward model by partially uncovering the causal relationships between state - action and rewards . We achieve this by modifying the learning objective of the reward model with said highlights . We provide empirical evidence for the efficacy of PREDILECT , in the form of ablations on synthetic benchmarks . We also collect actual human prefer - ences in a simulated social robot navigation scenario , to verify its applicability and further reinforce its merits and performance . 2 RELATED WORK 2 . 1 Learning from evaluative feedback Utilizing human knowledge for robot learning serves as an effective and interactive method [ 43 , 80 ] , particularly through the medium of evaluative feedback [ 47 ] . By deducing reward functions from human input , we can facilitate the swift adaptation of robot policies [ 74 ] , craft policies that are tailored to individual users [ 61 ] , and achieve alignment with instructions and descriptions [ 66 ] . Humans can convey this form of feedback in a variety of manners , including scalar form [ 35 ] , verbal directives [ 62 ] , trajectory segmentation [ 17 ] , or by employing buttons to signal preferred behaviors [ 35 , 36 , 60 ] , such as for expressing preferences [ 16 , 73 ] . Prior studies have also focused on refining policies through preferences , either by pre - defining features [ 13 ] , augmenting features [ 3 ] , or utilizing Bayesian methods [ 6 , 56 ] . Leveraging human preferences for learning has received sub - stantial focus in recent literature [ 73 ] , showcasing potential as an effective RL method applicable even in high - dimensional robotic settings [ 26 ] . Contemporary methods in preference learning impose few restrictions on reward function modality and can be interpreted as an iteration of repeated inverse reinforcement learning [ 1 ] . A reward function can either be inferred from a tabula rasa approach [ 16 ] or be bootstrapped via imitation learning [ 31 ] . The underlying principle is the perpetual refinement of a reward function by solic - iting preferences on subsequent iterations of a policy . Strategies that incrementally incorporate human participation in the learning loop to infer reward functions exhibit enhanced robustness and diminish the risk of obtaining contradictory feedback from humans [ 4 , 16 , 57 ] . Recent approaches have tackled the issue of feedback inefficiency utilizing techniques like pre - training [ 26 , 39 , 49 ] and bi - level optimization [ 42 ] . In contrast , PREDILECT delves into the promising and relatively uncharted challenge associated with pref - erence explanation [ 30 ] . 2 . 2 Zero - shot multimodal prompting In this work , our aim is to integrate multimodal [ 48 ] language in - formation with preferences in a zero - shot fashion , leveraging large language models ( LLMs ) [ 12 , 14 , 20 , 53 , 67 ] . LLMs are renowned for executing linguistic tasks , such as textual interpretation [ 55 ] and thus suitable for feature interpretation and extraction . Our interest lies in a variant of zero - shot transfer learning [ 24 , 59 , 64 , 76 ] . Specifically , we seek to utilize the underlying reasoning found in human text prompts acquired with an LLM from a source domain ( Internet - scale text prompts ) with preferences , to both enhance performance and reduce the number of labeled queries needed in a target domain . During multimodal training [ 78 ] , it is common to retain specific subcomponents of modelsâ€”particularly those related to one modality but not othersâ€”in a frozen state for downstream tasks [ 22 , 37 , 69 , 77 , 79 ] . The fusion of weights from extensive pretrained models with multimodal joint training has PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA led to notable accomplishments in diverse downstream multimodal applications , including image captioning [ 46 ] . In alignment with our research , Zeng et al . [ 78 ] presents So - cratic Models ( SMs ) , which are portrayed as modular frameworks . In these frameworks , new tasks emerge from a language - based interaction between pretrained models and additional modules , such as a reward model . PREDILECT can be conceptualized as an SM , where the large pretrained LLM is predetermined , while a reward model undergoes training based on joint text inferences between the LLM and preferences . Alternatively , our approach can be interpreted as a form of distillation [ 15 , 54 , 75 ] , where the LLM serves as the teacher and effectively functions as a regularizer , aid - ing in the training of the student reward function . Inspired by the aforementioned works , PREDILECT contributes by utilizing the simplicity of preference - based RL and delving into the contextual text information associated with the respective queries . 3 BACKGROUND We present the fundamentals to understand PREDILECT ( see Sec . 4 ) . In this work , our goal is to develop a model that accurately pre - dicts the reward function by efficiently leveraging human feedback , represented by preferences and prompts . The scenario we consider is one where a robot , in a given state ğ‘  ğ‘¡ , initiates an action ğ‘ ğ‘¡ ac - cording to a policy ğœ‹ ğœ” ( ğ‘ ğ‘¡ , ğ‘  ğ‘¡ ) , parameterized by ğœ” . Upon executing this action , the robot receives a reward ğ‘Ÿ ( ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) and transitions to a new state ğ‘  ğ‘¡ + 1 , all within the framework of a Markov Decision Process ( MDP ) . The final objective for the robot is to discover an optimal policy ğœ‹ âˆ— ğœ” ( ğ‘ ğ‘¡ , ğ‘  ğ‘¡ ) that maximizes the expected discounted sum of rewards . 3 . 1 Preference Learning Following Christiano et al . [ 16 ] , we define the task of inferring a reward function , Ë† ğ‘Ÿ ğœ“ , characterized by ğœ“ , from preferences as an issue in supervised learning . The core ambition of preference - based RL , as explored in [ 16 , 73 ] , revolves around deducing rewards from sequences of state - action pairs . Defining trajectory segments [ 72 ] as sequences constituted by state - action pairs , they are represented as ğœ ğ‘— = ( ( ğ‘  ğ‘—ğ‘¡ , ğ‘ ğ‘—ğ‘¡ ) , . . . , ( ğ‘  ğ‘—ğ‘¡ + ğ‘š âˆ’ 1 , ğ‘ ğ‘—ğ‘¡ + ğ‘š âˆ’ 1 ) ) , with ğ‘— signifying the seg - ment index , encompassing state - action pairs from ğ‘¡ to ğ‘¡ + ğ‘š , where ğ‘š symbolizes the segment length . Humans are presented with pairs of these trajectory segments , designated as ( ğœ 0 , ğœ 1 ) , and are tasked with allocating a preference ğ‘¤ âˆˆ { 0 , 0 . 5 , 1 } . A preference of ğ‘¤ = 0 signifies favoring ğœ 0 over ğœ 1 , depicted as ğœ 0 â‰» ğœ 1 , while ğ‘¤ = 1 is interpreted as ğœ 1 â‰» ğœ 0 , and ğ‘¤ = 0 . 5 indicates an equivalent prefer - ence for both segments . Adhering to the Bradley - Terry model [ 10 ] , the likelihood of a human exhibiting a preference for ğœ 0 â‰» ğœ 1 , con - tingent upon it being exponentially reliant on the reward sum over the segmentsâ€™ length , is expressed as : ğ‘ƒ ğœ“ [ ğœ 0 â‰» ğœ 1 ] = exp ( (cid:205) ğ‘¡ Ë† ğ‘Ÿ ğœ“ ( ğ‘  0 ğ‘¡ , ğ‘ 0 ğ‘¡ ) ) exp ( (cid:205) ğ‘¡ Ë† ğ‘Ÿ ğœ“ ( ğ‘  0 ğ‘¡ , ğ‘ 0 ğ‘¡ ) ) + exp ( (cid:205) ğ‘¡ Ë† ğ‘Ÿ ğœ“ ( ğ‘  1 ğ‘¡ , ğ‘ 1 ğ‘¡ ) ) ( 1 ) In this framework , the reward model , Ë† ğ‘Ÿ ğœ“ , is amenable to train - ing as a binary classifier to anticipate human preferences on new segments , serving as a surrogate for the reward function . The pref - erences elicited from humans are store alongside the respective segments in a labeled dataset D ğ‘™ , composed of triples ( ğœ 0 , ğœ 1 , ğ‘¤ ) . During the optimization of Ë† ğ‘Ÿ ğœ“ , we draw samples from D ğ‘™ and aim to minimize the binary cross - entropy loss : L ğ¶ğ¸ ( Ë† ğ‘Ÿ ğœ“ , D ğ‘™ ) = âˆ’ E ( ğœ 1 , ğœ 2 , ğ‘¤ ) âˆ¼D ğ‘™ [ ( 1 âˆ’ ğ‘¤ ) log ğ‘ƒ ğœ“ ( ğœ 1 â‰» ğœ 2 ) + ğ‘¤ log ğ‘ƒ ğœ“ ( ğœ 2 â‰» ğœ 1 ) ] ( 2 ) 4 PREDILECT 4 . 1 Prompt - Response formulation In PREDILECT a human can optionally offer a prompt to comple - ment their preference , such that prompt ğ‘– âˆˆ P , where ğ‘– indexes the ğ‘– - th prompt , and P the set of all prompts provided by the humans . To analyse human prompts , we require a set of intrinsic features F = { ğ‘“ 1 , ğ‘“ 2 , . . . , ğ‘“ ğ‘› } , where ğ‘› âˆˆ N + represents the size of the fea - ture set . It is important to note we do not want to bias humans on which features they should consider . Rather , we can denote intrin - sic features we might find important for particular tasks , such as speed and distance to humans on social navigation . We leverage an LLM , to not only detect features from human prompts but provide sentiment analysis linked to these features . Thus , we input to an LLM both the prompt provided by the human and our feature set , such that : LLM : ( prompt ğ‘– âˆˆ P , F ) â†’ r ğ‘– âˆˆ R ( 3 ) where R corresponds to the set of all possible responses . Each r ğ‘– consists of a set of triplets of the size ğ‘› = | F | = | R | , where r ğ‘– = { ( ğ‘“ 1 , y 1 , v 1 ) , . . . , ( ğ‘“ ğ‘› , y ğ‘› , v ğ‘› ) } , where ğ‘“ 1 âˆˆ F is a feature , y 1 âˆˆ { positive , negative } is the sentiment associated with feature ğ‘“ 1 , and v 1 âˆˆ { low , high } is the magnitude associated with the sentiment y 1 . To clarify our formulation we provide a concrete example of a prompt that was evaluated for our experiments ( see Sec . 5 ) in the social navigation environment . For visualisation purposes , the feature set F is blue , the human prompt prompt ğ‘– is purple and the output response r ğ‘– is green . : Input : You are a robot navigating a corridor with humans walking around trying to reach the goal / star . The user had to pick between two alternatives and picked their preferred alternative and they are now giving an explanation for their pick . Which feature ( s ) was most important of [ distance to goal , distance to human , speed ] ? The text given by the user is : â€™was less close to hitting a human / wall and moved at a slower pace . â€™Please respond in the following format for each feature that is relevant to the text given by the user : [ feature : insert feature , sentiment : insert positive or negative , value : insert high or low ] . Sentiment explains if the user thought the robot was behaving well in regards to the feature , if the robot behaved well it should be positive , else negative . Value indicates if the value of the feature was high or low . Only mention the features that are relevant , disregard the others . Output : [ feature : distance to human , sentiment : positive , value : high ] [ feature : speed , sentiment : positive , value : low ] 4 . 2 Mapping responses to highlights We use each response r ğ‘– to obtain highlighted subsequences from a trajectory segment ğœ , which we define as highlights . A highlight , denoted as â„ , is characterized as a subsequence residing within a preferred segment . Highlights are constructed as subsequences of segments , rep - resented by â„ = ğœ ğ‘– , ğ‘— = ( ( ğ‘  ğ‘– , ğ‘ ğ‘– ) , . . . , ( ğ‘  ğ‘— , ğ‘ ğ‘— ) ) âˆˆ ( ğ‘  , ğ‘ ) ğ‘— âˆ’ ğ‘– , ğ‘– , ğ‘— âˆˆ N + with 0 â‰¤ ğ‘– â‰¤ ğ‘— â‰¤ ğ‘š . The highlightâ€™s length is given by ğ‘— âˆ’ ğ‘– = ğ¿ , wherein ğ¿ signifies the maximum length contemplated for a high - light . To map each r ğ‘– to highlights , we first semantically map each trajectory segment ğœ to a tensor of feature metrics T . HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite We define a mapping function such as M : ğœ â†’ T , that maps the corresponding state sequence to a tensor of feature metrics T . Each T ğ‘–ğ‘— is the tensor element corresponding to the ğ‘– ğ‘¡â„ state and ğ‘— ğ‘¡â„ feature . Moreoever , each tensor element can be defined as T ğ‘–ğ‘— = { ğ‘“ ğ‘— , m ğ‘–ğ‘— } which contains a feature ğ‘“ ğ‘— and its corresponding metric value m ğ‘–ğ‘— âˆˆ R such that the tensor of feature metrics can be defined as : T = ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£° { ğ‘“ 1 , m 11 } { ğ‘“ 2 , m 12 } . . . { ğ‘“ ğ‘› , m 1 ğ‘› } { ğ‘“ 1 , m 21 } { ğ‘“ 2 , m 22 } . . . { ğ‘“ ğ‘› , m 2 ğ‘› } . . . . . . . . . . . . { ğ‘“ 1 , m ğ‘š 1 } { ğ‘“ 2 , m ğ‘š 2 } . . . { ğ‘“ ğ‘› , m ğ‘šğ‘› } ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£» ( 4 ) In this context , each m ğ‘–ğ‘— is a scalar derived through a heuristic , which is intrinsically associated with the relevant feature . For in - stance , " distance to humans " is expected to represent a scalar value , measured in meters . We require a search function ğ‘” that navigates through the tensor of metrics T to identify a highlight â„ for each r ğ‘– . Given a response r ğ‘– , our task is to pinpoint , for every feature ğ‘“ âˆˆ ğ¹ referenced in ğ‘Ÿ ğ‘– , a singular subsequence ğ‘† â€² ğ‘“ âŠ† ğ‘† that aligns with the specified criteria . The set of all possible subsequences of fixed length ğ¿ is represented as ğ‘† â€² âŠ† ğ‘† : | ğ‘† â€² | = ğ¿ , and from this set , for each feature ğ‘“ , we extract one subsequence to form the set H F ( ğ‘† ) = { ğ‘† â€² ğ‘“ âŠ† ğ‘† : | ğ‘† â€² ğ‘“ | = ğ¿ | ğ‘“ âˆˆ F } . The search function ğ‘” is defined as follows : ğ‘” ( T , ğ‘… ) = H F ( ğ‘† ) ( 5 ) Here , H F ( ğ‘† ) denotes the set containing a unique subsequence for each feature in F . The function ğ‘” maps the tensor of metrics T and the response set ğ‘… to H F ( ğ‘† ) , aligning each subsequence with the user - specified prompt for the corresponding feature . For clarity , we will refer to the unique subsequence for each feature ğ‘“ , denoted as ğ‘† â€² ğ‘“ , as the highlight â„ ğ‘– ( as aforementioned defined ) for feature ğ‘“ ğ‘– . In PREDILECT , we elucidate further on the formulation of ğ‘” . For each triplet ( ğ‘“ ğ‘˜ , y ğ‘˜ , v ğ‘˜ ) in r ğ‘– , v ğ‘˜ is utilized to probe for either low or high metric values in T , with y ğ‘˜ delineating whether the identi - fied highlight is of a positive or negative nature . Consequently , ğ‘” is articulated to search for highlights corresponding to either the maximum or minimum metric values , contingent on v ğ‘˜ , and sub - sequently categorizes them based on the sentiment y ğ‘˜ as positive or negative highlights . Subsequently , we bifurcate H F ( ğ‘† ) into two distinct subsets , denoted as H + F and H âˆ’F , based on the sentiment of each feature . A query , as delineated in Sec . 3 . 1 , encapsulates a human pref - erence , symbolized as ğ‘¤ , and is paired with two trajectory seg - ments , represented as ğ‘ = ( ğœ 1 , ğœ 2 , ğ‘¤ ) . This definition is augmented by incorporating sets of positive and negative highlights , H + F = { â„ + ğ‘“ 1 , . . . , â„ + ğ‘“ ğ‘› } and H âˆ’F = { â„ âˆ’ ğ‘“ 1 , . . . , â„ âˆ’ ğ‘“ ğ‘› } , respectively , for each fea - ture . The enhanced quintuple , termed as sentiment highlighted query , is denoted as ğ‘ â„ğ‘ = ( ğœ 1 , ğœ 2 , ğ‘¤ , H + F , H âˆ’F ) and we provide a visualization of these highlights in Fig . 2 . Within PREDILECT , the sentiment highlighted queries are compiled in a dataset represented as D ğ‘ â„ğ‘ . segment Figure 2 : Representation of highlights within a segment . The segment ğœ outlined by a curve contains multiple highlights , two negative in H âˆ’F depicted in red and one positive H + F depicted in green â„ + . All highlights are of the same length ğ¿ . 4 . 3 Reward Model Regularization Empirical studies illustrate the importance of strategic regular - ization in shaping state representations by enhancing the initial learning objective [ 18 ] . Auxiliary tasks , secondary yet semi - related to the primary task , offer valuable training signals for learning shared representations , thereby enhancing learning and data effi - ciency [ 32 , 44 , 45 , 52 , 63 ] . Regularization limits the search for solu - tions by adding bias . Using human natural language for shaping representations develops human - like biases and behaviors [ 5 , 38 ] . In PREDILECT , we devise a state representation task incorporat - ing causal reasoning from human teachers to reduce the entropy of the credit assignment when we only use preferences , refining the distinction between high and low - value sequences . This task is added as additional regularization terms to Eq . 2 to shape the reward function , aiming to maximize positive highlights â„ + and minimize negative ones â„ âˆ’ . A discount is applied to preceding states , as in similar works predicting future rewards . L + = E â„ + âˆ¼D ğ‘ â„ğ‘ (cid:34) ğ¿ âˆ‘ï¸ ğ‘™ = 0 ğœ† ğ‘™ Ë† ğ‘Ÿ ğœ“ ( ğ‘  ğ‘— âˆ’ ğ‘™ , ğ‘ ğ‘— âˆ’ ğ‘™ ) (cid:35) ( 6 ) L âˆ’ = âˆ’ E â„ âˆ’ âˆ¼D ğ‘ â„ğ‘ (cid:34) ğ¿ âˆ‘ï¸ ğ‘™ = 0 ğœ† ğ‘™ Ë† ğ‘Ÿ ğœ“ ( ğ‘  ğ‘— âˆ’ ğ‘™ , ğ‘ ğ‘— âˆ’ ğ‘™ ) (cid:35) ( 7 ) The final objective optimizes both L + ( Eq . 6 ) and L âˆ’ ( Eq . 7 ) while sustaining the baseline preference learning loss , denoted as L ğ¶ğ¸ ( see Eq . 2 ) . To optimize Ë† ğ‘Ÿ ğœ“ , we utilize sentiment highlighted query samples ğ‘ â„ğ‘ drawn from D ğ‘ â„ğ‘ . The hyperparameters ğ›¼ + and ğ›¼ âˆ’ are employed to assign weights to the two regularization terms . Consequently , the resulting learning objective takes the form : L PREDILECT = L ğ¶ğ¸ + ğ›¼ + L + + ğ›¼ âˆ’ L âˆ’ ( 8 ) 4 . 4 Preference learning with PREDILECT Similar to other preference - based RL methods [ 16 , 39 ] , PREDILECT , as delineated in Alg . 1 , integrates policy with reward learning . In step A ( see Fig . 4 ) , policy ğœ‹ ğœ” engages with the environment , yielding ( ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘  ğ‘¡ + 1 ) and estimates of Ë† ğ‘Ÿ ğœ“ ( ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) . The resulting transitions PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA I prefer B because it reached the goal but the robot was too close to the group of humans and walls Which feature ( s ) was most important of [ Set of features ] ? The text given by the user is : [ user description ] Respond in the format for each relevant feature : [ feature : feature , sentiment : positive / negative , value : high / low ] . Prompt Intrinsic Features Goal Humansafety Speed Wall avoidance [ feature : Wall distance , sentiment : negative , value : low ] [ feature : Goal , sentiment : positive , value : low ] GPT - 4 Response [ feature : Human distance , sentiment : negative , value : low ] Figure 3 : An overview of how PREDILECT processes prompts from humans is as follows : Initially , a human provides a prompt , depicted in green , along with a set of intrinsic features F in purple which is environment dependant . Both are input into the LLM ( ChatGPT - 4 in the case of PREDILECT ) to generate a response r ğ‘– . Subsequently , after mapping a segment ğœ to a tensor of metrics T using the mapping function ğ‘€ , we apply a searching function ğ‘” to obtain the set H F of highlights for each feature . These highlights are then utilized to train our reward model Ë† ğ‘Ÿ ğœ“ as per Eq . 8 . ( ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘  ğ‘¡ + 1 , Ë† ğ‘Ÿ ğœ“ ( ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) ) , organized into trajectories , are gathered in a temporary buffer . This buffer is utilized for gradient descent on ğœ‹ ğœ” concerning ğœ” , following the PPO algorithm [ 58 ] . Post training of ğœ‹ ğœ” , a substantial number of trajectory segments are collected and stored in D ğœ . Subsequently , in step B , a feedback session takes place . We randomly select ğ‘ âˆˆ N + trajectory segments ğœ to obtain query preferences from humans . In addition , they can provide optional auxiliary language feedback . Given the human prompt and the set of intrinsic features , we utilize the LLM to derive a response r . After transforming the segment ğœ into a tensor of metrics T using ğ‘€ , we employ the search function ğ‘” to extract both positive H + F and negative highlights H âˆ’F from segments and prompts , as described in Sec . 4 . 2 . All highlights are subsequently stored in the sentiment highlighted queries dataset D ğ‘ â„ğ‘ . In step C , gradient descent is executed on the parameters ğœ“ to refine our reward model Ë† ğ‘Ÿ ğœ“ , with L PREDILECT serving as the learning objective . Upon acquiring an updated version of Ë† ğ‘Ÿ ğœ“ , the process reverts to step A , and the algorithm iteratively progresses until convergence . Algorithm 1 : PREDILECT 1 D ğœ â† âˆ… ; D ğ‘ â„ğ‘ â† âˆ… ; F â† getFeatureSet ( ) ; 2 ğœ‹ ğœ” â† train ( ğœ‹ ğœ” , Ë† ğ‘Ÿ ğœ“ , ğ‘’ğ‘›ğ‘£ ) âŠ² / / Step A 3 D ğœ â† sampleSegments ( ğœ‹ ğœ” ) 4 while | D ğ‘ â„ğ‘ | â‰¤ ğ‘ do 5 ( ğœ 1 , ğœ 2 ) â† samplePairs ( D ğœ ) âŠ² / / Step B 6 ğ‘¤ â† collectPreference ( ğœ 1 , ğœ 2 ) 7 prompt â† collectPrompt ( ğœ 1 , ğœ 2 , w ) 8 r â† LM ( prompt , F ) 9 T â† M ( ğœ âˆ— ) 10 ( H + F , H âˆ’F ) â† g ( T , r ) 11 D ğ‘ â„ğ‘ â† D ğ‘ â„ğ‘ âˆª ( ğœ 1 , ğœ 2 , ğ‘¤ , H + F , H âˆ’F ) 12 for each gradient step do 13 Sample minibatch from D ğ‘ â„ğ‘ âŠ² / / Step C 14 Optimize Ë† ğ‘Ÿ ğœ“ L PREDILECT with respect to ğœ“ in Eq . ( 8 ) 15 return Ë† ğ‘Ÿ ğœ“ âŠ² / / return Ë† ğ‘Ÿ ğœ“ + A B C LLM Figure 4 : Framework representation of PREDILECT . Step A : We train policy ğœ‹ ğœ” and sample rollouts which are stored in D ğœ . Step B : We sample trajectory segments ğœ to query hu - mans and collect both preferences and prompts . The prompts are processed through an LLM to obtain responses . Those re - sponses are used to obtain highlights ( H + F , H âˆ’F ) from the pre - ferred segment ğœ âˆ— Step C : The sentiment highlighted queries are collected to form dataset D ğ‘ â„ğ‘ and update the current reward model Ë† ğ‘Ÿ ğœ“ . 5 EXPERIMENTS This section aims to examine the efficiency of PREDILECT and how incorporating human language to express preferences impacts the preference learning framework . To begin , we conduct experiments in simulated settings . We create an oracle that offers feedback to demonstrate how merging preference learning with textual expla - nations can enhance outcomes . We also carry out an experiment in a social robot navigation context , where a robot is trained using real human feedback sourced from Amazon Mechanical Turk ( MTurk ) participants . Social robot navigation is a complex task , requiring a balance between objectives such as reaching the destination , effi - ciency , and ( perceived human ) safety , making it a compelling case study [ 23 ] . We assess the effectiveness of PREDILECT with human feedback and how well the LLM can capture essential information from these textual descriptions to produce highlights . We used GPT - 4 for all experiments . We also demonstrate our ability to develop HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite PREDILECT 200 LLM 200 LLM 400 2378 635 968 Table 1 : Ablation comparing the final cumulative reward when using only L + , L âˆ’ and PREDILECT for Cheetah . varied policies , such as those prioritizing safety , by only consid - ering features related to safety when training the human reward function . Our main hypotheses are summarized below : â€¢ H1 : PREDILECT will learn a human reward function more efficiently compared to the baseline . â€¢ H2 : PREDILECT can learn policies that put more focus on specific objectives as described by the user rather than more generalized policies learned by regular preference - based learning . â€¢ H3 : The LLM will accurately extract the information needed to create highlights . 5 . 1 Simulation experiment setup We aim to demonstrate the effectiveness of PREDILECT by con - ducting simulated experiments using the Reacher and Cheetah environments from OpenAI Gym [ 11 ] . In these simulations , an oracle compares two segments and provides preferences based on which segment achieves a higher cumulative reward , as determined by the true environment reward function . Recognizing that human instructors are not infallible , we introduce a 10 % error rate in the oracleâ€™s feedback to mimic potential human inaccuracies . Weâ€™ve also extended the oracle to work with PREDILECT . Besides just indicating a preference , the oracle also offers explanations for its choices . It does this by monitoring the values of certain features within a segment . If a featureâ€™s value surpasses or falls below a set threshold , the oracle will add this feature for highlighting . This can be seen as PREDILECT after the first LLM processing step . The specific features that the Oracle monitors vary by environment . For Cheetah , we use the x - axis velocity ; for Reacher , we use the distance between the fingers and the goal . Further details can be found in the Appendix . 5 . 2 Simulation experiment results Upon initial observation , itâ€™s evident that utilizing highlights based on features results in quicker convergence compared to relying solely on preference - based learning ( see Fig 5 ) . The results derive from highlights pinpointing more specific and non - sequential areas of interest that align with the provided description . This advan - tage is evident in the reward curves for both Cheetah and Reacher . Notably , this enhanced performance with PREDILECT is achieved using only half the number of queries typically required by tradi - tional preference - based learning . Tab . 1 further demonstrates that the high convergence stems from the multi - modal feedback . The simulated results offer support for H1 due to the higher convergence and reduction of queries . 5 . 3 Real human feedback experiment setup To understand how textual descriptions affect the learned reward model , we perform experimentation using real human feedback in a social navigation scenario . The purpose of this experiment is threefold . 1 ) validate that PREDILECT performs better than baseline using real human feedback ; 2 ) show that the policies learned can be more aligned with the participantâ€™s preferences ; 3 ) show that the LLM can accurately deduce the information needed from the textual descriptions . The social navigation scenario is built using Unity and involves a Pepper robot navigating between three humans in order to collect a goal shaped like a star ( see Fig 6 ) that acts as a guide for the robot . In order to sense its environment the robot is equipped with lidar rays that can detect humans , walls , and the end goal . To ensure safety , the robot follows the social force model which treats the human and robot as repelling forces [ 27 ] . One of the actions the robot can take is to lower and increase the social force which will compel the robot to keep its distance from the humans . For PREDILECT we add the features for goal distance , human distance , and speed to the prompt . The agent runs for 500 , 000 timesteps and updates the reward function once at the start as a single batch . 5 . 3 . 1 Participants . In total , 43 individuals were recruited from Amazon Mechanical Turk to participate in the experiment : 21 were assigned to the PREDILECT group and 22 to the baseline group . However , three participants were excluded due to not passing an attention check , resulting in an even 20 participants for each group . The participantsâ€™ ages ranged from 22 to 59 years . Of them , 64 . 9 % identified as male and 35 . 1 % as female . The experiment lasted about 25 minutes and participant were compensated at a rate of approxi - mately $ 10 per hour for their time . 5 . 3 . 2 Procedure . After agreeing to participate in the experiment , participants went through a tutorial explaining the goal and the UI . They were instructed to embody the humans in the environment and rate the queries based on their comfort and how well the robot achieved its goal . Participants were then provided with pairs of videos that represent two segments collected by the robot following a pre - trained policy ğœ‹ ğœ” forming a query , and had to indicate which video they preferred . If they couldnâ€™t decide which video they pre - ferred , they had the option to say " none " . After each preference , they were moved to the next query pair until they had labeled 20 queries in total . At the end of the experiment , the participants are asked to fill in a demographic survey . In the PREDILECT trial , before proceeding to the next query , participants were prompted with a question : " Please describe why you preferred video A / B . " They were provided with a text box to record their response . If they were uncertain about their preference reasons , they were instructed to leave the text box blank . To assist them in recalling the videoâ€™s content , the selected video remained centered on the screen , allowing participants to rewatch it instead of relying solely on memory . Table 3 includes some examples of textual descriptions given by participants and how the LLM module in PREDILECT mapped these into features . We conducted separate trials for both PREDILECT and the base - line , gathering 400 preferences for each . Both trials began with the same initial set of segments and preference pairs , all generated from the same policy ğœ‹ ğœ” . The distinctions between the two arise from how they evaluate preference pairs and the inclusion of textual descriptions in PREDILECT . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA 0 100 200 300 400 500 Steps ( 1k ) 80 70 60 50 40 30 20 E n v R e w a r d Reacher 200 - PREDILECT 400 - Baseline 0 250 500 750 1000 1250 1500 1750 2000 Steps ( 1k ) 0 500 1000 1500 2000 2500 3000 E n v R e w a r d Cheetah 200 - PREDILECT 400 - Baseline ( a ) ( b ) Figure 5 : Learning curves : for Reacher ( a ) , PREDILECT used 200 queries , Baseline used 400 queries ; for Cheetah ( b ) , PREDILECT used 200 queries , Baseline used 400 queries . Figure 6 : Social robot navigation environment used for the human experiments . 5 . 4 Real human feedback experiment results We further assess the algorithmâ€™s efficiency by using actual human feedback . When comparing PREDILECT to baseline preference learning , PREDILECT converges to a higher reward and stays rela - tively stable at just above 0 . 8 reward while the baseline converges around 0 . 6 , as shown in Figure 7 . b . These results add further sup - port for Hypothesis H1 on top of the simulation results . Given that humans can further articulate their preferences using text , we be - lieve itâ€™s beneficial to investigate how we can tailor policies based on these textual descriptions in order to validate our hypothesis H2 . With this in mind , we use PREDILECT while focusing solely on safety - related features , such as the proximity to humans and walls . Figure 7 . a illustrates a distinct difference in the robotâ€™s appli - cation of social force when trained using only these safety - centric features compared to baseline . Initially , both the baseline and our method exhibit similar values . However , over time , the baseline value diminishes , which might be attributed to a trade - off between maintaining a safe distance and efficiently reaching the destination . The ability to have the robot use a higher social force by training using safety - based description offers support for H2 . To ensure that the LLM effectively and accurately extracts information from human descriptions and to validate hypothesis H3 , we compared its predictions with those of a human labeler , as detailed in Table 2 . The data reveals that the LLM correctly identifies features from the LLM metrics Feature Sentiment Magnitude Acc 85 . 71 % 77 . 14 % 80 % F . pos 11 . 54 % 13 . 04 % 8 % Table 2 : Accuracy and false positives of the LLM is at predict - ing the feature , sentiment , and magnitude based on human description when compared to a human labeler . descriptions 85 % of the time . It also has similar accuracy rates for determining sentiment ( âˆ¼ 77 % ) and magnitude ( âˆ¼ 80 % ) . While these accuracy rates are noteworthy , they also encompass instances where the LLM might overlook certain features that the human labeler identified . In such cases , we lose some information and revert to standard preferences . However , a more important concern arises when the LLM identifies or " hallucinates " features that the human never mentioned , as this could adversely impact performance . Our data shows that the LLM makes this error 11 . 54 % of the time when identifying features . Furthermore , in situations where both the human labeler and the LLM agree on a particular feature , thereâ€™s a discrepancy in sentiment interpretation in 13 % of the instances and in magnitude interpretation in 8 % of the in - stances . This error rate is based solely on the human descriptions . Interestingly , when considering our prompt we have a general goal description of the robot along with the human language description . When considering the goal description , most of the cases where there is an error , they are still aligned with the overarching goal description . With these accuracies and the fact that any error can mostly be explained with the prompts structure , we find support for H3 . Overall the participants chose the " none " alternative 13 . 75 % of the time , indicating no preference . For PREDILECT when there was a preference , the participants gave additional textual feedback in 61 . 44 % of the cases . 6 DISCUSSION Based on the results from both the simulated oracle experiments and the social robot navigation evaluations using real human feed - back , PREDILECT demonstrates a faster convergence compared to traditional preference learning . In the simulated environments , this HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite 0 100 200 300 400 500 Steps ( 1k ) 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 S o c i a l F o r c e Social Force PREDILECT Baseline 0 100 200 300 400 500 Steps ( 1k ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 E n v R e w a r d Reward PREDILECT Baseline ( a ) ( b ) Figure 7 : For the social robot navigation environment : a ) Social force when applying only safety features on PREDILECT compared to baseline ; b ) Reward curves for PREDILECT and baseline . Text Description Feat . Sent . Magn . got both stars and avoided hitting humans H . Dist , G . Dist Positive , Positive High , Low I prefer video A because of the fast speed . Speed Positive High Did not hit a human H . Dist Positive High Stayed a longer distance away from hitting any - thing and moved at a slower pace . H . Dist , Speed Positive , Negative High , High Table 3 : Human descriptions justifying their preferences and the feature , sentiment , and magnitude predicted by the LLM . higher efficiency is obtained using 50 % of the queries compared to the baseline . This superior performance can be attributed to the feedbackâ€™s capacity to mitigate causal confusion , as it enables humans to provide a clear rationale behind their preferences . Our results further illustrate by emphasizing particular features , such as those related to safety , our system can tailor policies based on user explanations rather than merely adopting a generic policy . For instance , within the social navigation context , the robot consis - tently maintained a higher level of social force during training . In contrast , the baseline reduced the social force over time . We contend that the integration of textual explanations for preferences allows the system to more rapidly align with human preferences , elimi - nating the need for over - querying making this approach relevant to several robotics applications . We evaluated the LLMâ€™s capability to extract relevant informa - tion from text descriptions provided by humans . The LLM demon - strated high accuracy , closely aligned with the feature identification by a human labeler . While there were occasional discrepancies , a de - tailed analysis revealed that the LLMâ€™s interpretations , even when diverging from participantsâ€™ descriptions , frequently aligned with the robotâ€™s overarching objective . This may be attributed to the context provided within our prompt , which briefly described the robotâ€™s task . It is noteworthy that the LLM considers the entirety of the context presented in the prompt unless explicitly directed otherwise . This observation is insightful for two reasons : it under - scores the importance of crafting prompts , and it highlights the potential benefits of combining a short summary of the general goal with human descriptions , enabling the robot to make implicit inferences when possible . While such inferences can be advanta - geous in certain scenarios , they might not be suitable in others . We believe that refining our prompts could further reduce the error rate . For example , if the goal was personalization , we could empha - size the importance of the individual human description and ask the LLM to not draw implicit inferences based on the overarching objective . When accounting for both the global goal and the human descriptions , the error rate was further reduced to lower single digits . Interestingly , the LLM most frequently misinterpreted the â€™speedâ€™ feature . This could be attributed to the inherent challenge of balancing two objectives : reaching the destination ( faster speed ) and ensuring perceived safety ( slower speed ) . Following this , one natural limitation is that successful parsing of language descrip - tions depends on how well the prompt is formulated . Finally , our comparison uses â€™preferences onlyâ€™ as the baseline . 7 CONCLUSION In this paper , we introduced PREDILECT , a framework that lever - ages the zero - shot capabilities of LLMs to expand the amount of in - formation available , when inferring a reward function from human preferences , in an effort to mitigate causal confusion . By combining preferences with language we can extract more information per query while offering a natural way of interacting with robots . We believe that the combination of these modalities is a promising approach to improve learning from human feedback . ACKNOWLEDGMENTS This research has been carried out as part of the Vinnova Com - petence Center for Trustworthy Edge Computing Systems and Applications at KTH , and partially supported by the Swedish Foun - dation for Strategic Research ( SSF FFL18 - 0199 ) and the Wallenberg AI , Autonomous Systems and Software Program ( WASP ) funded by the Knut and Alice Wallenberg Foundation . All of the authors are with the Division of Robotics , Perception and Learning , School of Electrical Engineering and Computer Science , KTH Royal Institute of Technology , 114 28 Stockholm , Sweden . The authors are also affiliated with Digital Futures . PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA REFERENCES [ 1 ] Kareem Amin , Nan Jiang , and Satinder Singh . 2017 . Repeated inverse reinforce - ment learning . Advances in neural information processing systems 30 ( 2017 ) . [ 2 ] DarioAmodei , ChrisOlah , JacobSteinhardt , PaulChristiano , JohnSchulman , and Dan ManÃ© . 2016 . Concrete problems in AI safety . arXiv preprint arXiv : 1606 . 06565 ( 2016 ) . [ 3 ] Chandrayee Basu , Mukesh Singhal , and Anca D Dragan . 2018 . Learning from richer human guidance : Augmenting comparison - based learning with feature queries . In Proceedings of the 2018 ACM / IEEE International Conference on Human - Robot Interaction . 132 â€“ 140 . [ 4 ] Erik BÃ¥venstrand and Jakob Berggren . 2019 . Performance evaluation of imitation learning algorithms with human experts . [ 5 ] Kush Bhatia , Ashwin Pananjady , Peter Bartlett , Anca Dragan , and Martin J Wainwright . 2020 . Preference learning along multiple criteria : A game - theoretic perspective . Advances in neural information processing systems 33 ( 2020 ) , 7413 â€“ 7424 . [ 6 ] Erdem BÄ±yÄ±k , Nicolas Huynh , Mykel J Kochenderfer , and Dorsa Sadigh . 2020 . Active preference - based gaussian process regression for reward learning . In Robotics : Science and Systems ( RSS ) . [ 7 ] Andreea Bobu , Dexter RR Scobee , Jaime F Fisac , S Shankar Sastry , and Anca D Dragan . 2020 . Less is more : Rethinking probabilistic models of human behav - ior . In Proceedings of the 2020 acm / ieee international conference on human - robot interaction . 429 â€“ 437 . [ 8 ] Rishi Bommasani , Drew A Hudson , Ehsan Adeli , Russ Altman , Simran Arora , Sydney von Arx , Michael S Bernstein , Jeannette Bohg , Antoine Bosselut , Emma Brunskill , et al . 2021 . On the opportunities and risks of foundation models . arXiv preprint arXiv : 2108 . 07258 ( 2021 ) . [ 9 ] Serena Booth , Sanjana Sharma , Sarah Chung , Julie Shah , and Elena L Glassman . 2022 . Revisiting Human - Robot Teaching and Learning Through the Lens of Human Concept Learning . In Proceedings of the 2022 ACM / IEEE International Conference on Human - Robot Interaction . 147 â€“ 156 . [ 10 ] Ralph Allan Bradley and Milton E Terry . 1952 . Rank analysis of incomplete block designs : I . Themethodofpairedcomparisons . Biometrika 39 , 3 / 4 ( 1952 ) , 324 â€“ 345 . [ 11 ] Greg Brockman , Vicki Cheung , Ludwig Pettersson , Jonas Schneider , John Schul - man , Jie Tang , and Wojciech Zaremba . 2016 . Openai gym . arXiv preprint arXiv : 1606 . 01540 ( 2016 ) . [ 12 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . Language models are few - shot learners . Advances in neural information processing systems 33 ( 2020 ) , 1877 â€“ 1901 . [ 13 ] Maya Cakmak , Siddhartha S Srinivasa , Min Kyung Lee , Jodi Forlizzi , and Sara Kiesler . 2011 . Human preferences for robot - human hand - over configurations . In 2011 IEEE / RSJ International Conference on Intelligent Robots and Systems . IEEE , 1986 â€“ 1993 . [ 14 ] MarkChen , JerryTworek , HeewooJun , QimingYuan , HenriquePondedeOliveira Pinto , Jared Kaplan , Harri Edwards , Yuri Burda , Nicholas Joseph , Greg Brockman , et al . 2021 . Evaluating large language models trained on code . arXiv preprint arXiv : 2107 . 03374 ( 2021 ) . [ 15 ] Yanbei Chen , Xiatian Zhu , Wei Li , and Shaogang Gong . 2020 . Semi - supervised learningunderclassdistributionmismatch . In ProceedingsoftheAAAIConference on Artificial Intelligence , Vol . 34 . 3569 â€“ 3576 . [ 16 ] Paul F Christiano , Jan Leike , Tom Brown , Miljan Martic , Shane Legg , and Dario Amodei . 2017 . Deep reinforcement learning from human preferences . Advances in neural information processing systems 30 ( 2017 ) . [ 17 ] Yuchen Cui and Scott Niekum . 2018 . Active reward learning from critiques . In 2018 IEEE international conference on robotics and automation ( ICRA ) . IEEE , 6907 â€“ 6914 . [ 18 ] Tim De Bruin , Jens Kober , Karl Tuyls , and Robert BabuÅ¡ka . 2018 . Integrating state representation learning into deep reinforcement learning . IEEE Robotics and Automation Letters 3 , 3 ( 2018 ) , 1394 â€“ 1401 . [ 19 ] Pim De Haan , Dinesh Jayaraman , and Sergey Levine . 2019 . Causal confusion in imitation learning . Advances in Neural Information Processing Systems 32 ( 2019 ) . [ 20 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . Bert : Pre - trainingofdeepbidirectionaltransformersforlanguageunderstanding . arXiv preprint arXiv : 1810 . 04805 ( 2018 ) . [ 21 ] Frederick Eberhardt . 2017 . Introduction to the foundations of causal discovery . International Journal of Data Science and Analytics 3 , 2 ( 2017 ) , 81 â€“ 91 . [ 22 ] Peter Florence , Lucas Manuelli , and Russ Tedrake . 2019 . Self - supervised corre - spondence in visuomotor policy learning . IEEE Robotics and Automation Letters 5 , 2 ( 2019 ) , 492 â€“ 499 . [ 23 ] Anthony Francis , Claudia PÃ©rez - dâ€™Arpino , Chengshu Li , Fei Xia , Alexandre Alahi , RachidAlami , AniketBera , AbhijatBiswas , JoydeepBiswas , RohanChandra , etal . 2023 . Principles and guidelines for evaluating social robot navigation algorithms . arXiv preprint arXiv : 2306 . 16740 ( 2023 ) . [ 24 ] Efstratios Gavves , Thomas Mensink , Tatiana Tommasi , Cees GM Snoek , and Tinne Tuytelaars . 2015 . Active transfer learning with zero - shot priors : Reusing past datasets for future tasks . In Proceedings of the IEEE International Conference on Computer Vision . 2731 â€“ 2739 . [ 25 ] Dylan Hadfield - Menell , Smitha Milli , Pieter Abbeel , Stuart J Russell , and Anca Dragan . 2017 . Inverse reward design . Advances in neural information processing systems 30 ( 2017 ) . [ 26 ] Donald Joseph Hejna III and Dorsa Sadigh . 2023 . Few - shot preference learning for human - in - the - loop rl . In Conference on Robot Learning . PMLR , 2014 â€“ 2025 . [ 27 ] Dirk Helbing and PÃ©ter MolnÃ¡r . 1995 . Social force model for pedestrian dynamics . Physical Review E 51 , 5 ( 1995 ) , 4282 â€“ 4286 . [ 28 ] AshleyHill , AntoninRaffin , MaximilianErnestus , AdamGleave , AnssiKanervisto , Rene Traore , Prafulla Dhariwal , Christopher Hesse , Oleg Klimov , Alex Nichol , Matthias Plappert , Alec Radford , John Schulman , Szymon Sidor , and Yuhuai Wu . 2018 . Stable Baselines . https : / / github . com / hill - a / stable - baselines . [ 29 ] Namgyu Ho , Laura Schmid , and Se - Young Yun . 2022 . Large language models are reasoning teachers . arXiv preprint arXiv : 2212 . 10071 ( 2022 ) . [ 30 ] Robert Hu , Siu Lun Chau , Jaime Ferrando Huertas , and Dino Sejdinovic . 2022 . Explaining Preferences with Shapley Values . In Advances in Neural Informa - tion Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = - me36V0os8P [ 31 ] Borja Ibarz , Jan Leike , Tobias Pohlen , Geoffrey Irving , Shane Legg , and Dario Amodei . 2018 . Reward learning from human preferences and demonstrations in atari . Advances in neural information processing systems 31 ( 2018 ) . [ 32 ] MaxJaderberg , VolodymyrMnih , WojciechMarianCzarnecki , TomSchaul , JoelZ Leibo , David Silver , and Koray Kavukcuoglu . 2016 . Reinforcement learning with unsupervised auxiliary tasks . arXiv preprint arXiv : 1611 . 05397 ( 2016 ) . [ 33 ] Hong Jun Jeon , Smitha Milli , and Anca Dragan . 2020 . Reward - rational ( implicit ) choice : Aunifyingformalismforrewardlearning . AdvancesinNeuralInformation Processing Systems 33 ( 2020 ) , 4415 â€“ 4426 . [ 34 ] Huda Khayrallah , Sean Trott , and Jerome Feldman . 2015 . Natural language for human robot interaction . In International Conference on Human - Robot Interaction ( HRI ) . [ 35 ] W Bradley Knox and Peter Stone . 2009 . Interactively shaping agents via human reinforcement : The TAMER framework . In Proceedings of the fifth international conference on Knowledge capture . 9 â€“ 16 . [ 36 ] W Bradley Knox , Peter Stone , and Cynthia Breazeal . 2013 . Training a robot via human feedback : A case study . In International Conference on Social Robotics . Springer , 460 â€“ 470 . [ 37 ] Tejas D Kulkarni , Ankush Gupta , Catalin Ionescu , Sebastian Borgeaud , Malcolm Reynolds , AndrewZisserman , andVolodymyrMnih . 2019 . Unsupervisedlearning of object keypoints for perception and control . Advances in neural information processing systems 32 ( 2019 ) . [ 38 ] Sreejan Kumar , Carlos G Correa , Ishita Dasgupta , Raja Marjieh , Michael Y Hu , Robert Hawkins , Jonathan D Cohen , Karthik Narasimhan , Tom Griffiths , et al . 2022 . Usingnaturallanguageandprogramabstractionstoinstillhumaninductive biases in machines . Advances in Neural Information Processing Systems 35 ( 2022 ) , 167 â€“ 180 . [ 39 ] Kimin Lee , Laura Smith , and Pieter Abbeel . 2021 . Pebble : Feedback - efficient interactive reinforcement learning via relabeling experience and unsupervised pre - training . arXiv preprint arXiv : 2106 . 05091 ( 2021 ) . [ 40 ] Zhihao Li , Yishan Mu , Zhenglong Sun , Sifan Song , Jionglong Su , and Jiaming Zhang . 2021 . Intention understanding in human â€“ robot interaction based on visual - NLP semantics . Frontiers in Neurorobotics 14 ( 2021 ) , 610139 . [ 41 ] Xinran Liang , Katherine Shu , Kimin Lee , and Pieter Abbeel . 2022 . Reward Uncer - tainty for Exploration in Preference - based Reinforcement Learning . In Interna - tional Conference on Learning Representations . https : / / openreview . net / forum ? id = OWZVD - l - ZrC [ 42 ] Runze Liu , Fengshuo Bai , Yali Du , and Yaodong Yang . 2022 . Meta - Reward - Net : Implicitly Differentiable Reward Learning for Preference - based Reinforce - ment Learning . In Advances in Neural Information Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = OZKBReUF - wX [ 43 ] James MacGlashan , Mark K Ho , Robert Loftin , Bei Peng , Guan Wang , David L Roberts , Matthew E Taylor , and Michael L Littman . 2017 . Interactive learning frompolicy - dependenthumanfeedback . In Int . Conf . onMachineLearning . PMLR , 2285 â€“ 2294 . [ 44 ] JanMatas , StephenJames , andAndrewJDavison . 2018 . Sim - to - realreinforcement learning for deformable object manipulation . In Conference on Robot Learning . PMLR , 734 â€“ 743 . [ 45 ] Piotr Mirowski , Razvan Pascanu , Fabio Viola , Hubert Soyer , Andrew J Ballard , Andrea Banino , Misha Denil , Ross Goroshin , Laurent Sifre , Koray Kavukcuoglu , et al . 2016 . Learning to navigate in complex environments . arXiv preprint arXiv : 1611 . 03673 ( 2016 ) . [ 46 ] Ron Mokady , Amir Hertz , and Amit H Bermano . 2021 . Clipcap : Clip prefix for image captioning . arXiv preprint arXiv : 2111 . 09734 ( 2021 ) . [ 47 ] Anis Najar and Mohamed Chetouani . 2021 . Reinforcement learning with human advice : a survey . Frontiers in Robotics and AI 8 ( 2021 ) , 584075 . [ 48 ] Jiquan Ngiam , Aditya Khosla , Mingyu Kim , Juhan Nam , Honglak Lee , and An - drew Y Ng . 2011 . Multimodal deep learning . In ICML . [ 49 ] Jongjin Park , Younggyo Seo , Jinwoo Shin , Honglak Lee , Pieter Abbeel , and Kimin Lee . 2022 . SURF : Semi - supervised Reward Learning with Data Augmentation HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Simon Holk , Daniel Marta , & Iolanda Leite for Feedback - efficient Preference - based Reinforcement Learning . In Interna - tional Conference on Learning Representations . https : / / openreview . net / forum ? id = TfhfZLQ2EJO [ 50 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gre - gory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Kopf , Edward Yang , Zachary DeVito , Martin Rai - son , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . PyTorch : An Imperative Style , High - Performance Deep Learning Library . In Advances in Neural Information Processing Systems 32 , H . Wallach , H . Larochelle , A . Beygelzimer , F . d ' AlchÃ© - Buc , E . Fox , and R . Garnett ( Eds . ) . Curran Associates , Inc . , 8024 â€“ 8035 . [ 51 ] Seth Pate , Wei Xu , Ziyi Yang , Maxwell Love , Siddarth Ganguri , and Lawson LS Wong . 2021 . Natural language for human - robot collaboration : Problems beyond language grounding . arXiv preprint arXiv : 2110 . 04441 ( 2021 ) . [ 52 ] Lerrel Pinto and Abhinav Gupta . 2017 . Learning to push by grasping : Using multiple tasks for effective learning . In 2017 IEEE international conference on robotics and automation ( ICRA ) . IEEE , 2161 â€“ 2168 . [ 53 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , etal . 2021 . Learningtransferablevisualmodelsfromnaturallanguagesupervision . In International Conference on Machine Learning . PMLR , 8748 â€“ 8763 . [ 54 ] Ilija Radosavovic , Piotr DollÃ¡r , Ross Girshick , Georgia Gkioxari , and Kaiming He . 2018 . Data distillation : Towards omni - supervised learning . In Proceedings of the IEEE conference on computer vision and pattern recognition . 4119 â€“ 4128 . [ 55 ] Pranav Rajpurkar , Robin Jia , and Percy Liang . 2018 . Know what you donâ€™t know : Unanswerable questions for SQuAD . arXiv preprint arXiv : 1806 . 03822 ( 2018 ) . [ 56 ] Dorsa Sadigh , Anca D Dragan , Shankar Sastry , and Sanjit A Seshia . 2017 . Active preference - based learning of reward functions . [ 57 ] MariahLSchrum , ErinHedlund - Botti , NinaMoorman , andMatthewCGombolay . 2022 . MIND MELD : Personalized Meta - Learning for Robot - Centric Imitation Learning . . In HRI . 157 â€“ 165 . [ 58 ] John Schulman , Filip Wolski , Prafulla Dhariwal , Alec Radford , and Oleg Klimov . 2017 . Proximal policy optimization algorithms . arXiv preprint arXiv : 1707 . 06347 ( 2017 ) . [ 59 ] DevinSchwab , YifengZhu , andManuelaVeloso . 2018 . Zeroshottransferlearning forrobotsoccer . In Proceedingsofthe17thInternationalConferenceonAutonomous Agents and MultiAgent Systems . 2070 â€“ 2072 . [ 60 ] Emmanuel Senft , Paul Baxter , James Kennedy , SÃ©verin Lemaignan , and Tony Belpaeme . 2017 . Supervised autonomy for online learning in human - robot interaction . Pattern Recognition Letters 99 ( 2017 ) , 77 â€“ 86 . [ 61 ] Dhruv Shah , Arjun Bhorkar , Hrishit Leen , Ilya Kostrikov , Nicholas Rhinehart , and Sergey Levine . 2022 . Offline Reinforcement Learning for Visual Navigation . In 6th Annual Conference on Robot Learning . https : / / openreview . net / forum ? id = uhIfIEIiWm _ [ 62 ] PratyushaSharma , BalakumarSundaralingam , ValtsBlukis , ChrisPaxton , Tucker Hermans , Antonio Torralba , Jacob Andreas , and Dieter Fox . 2022 . Correcting robot plans with natural language feedback . In Robotics : Science and Systems ( RSS ) . [ 63 ] Evan Shelhamer , Parsa Mahmoudieh , Max Argus , and Trevor Darrell . 2016 . Loss is its own reward : Self - supervision for reinforcement learning . arXiv preprint arXiv : 1612 . 07307 ( 2016 ) . [ 64 ] Jae Woong Soh , Sunwoo Cho , and Nam Ik Cho . 2020 . Meta - transfer learning for zero - shot super - resolution . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 3516 â€“ 3525 . [ 65 ] Peter Spirtes , Clark N Glymour , Richard Scheines , and David Heckerman . 2000 . Causation , prediction , and search . MIT press . [ 66 ] TheodoreSumers , RobertD . Hawkins , MarkKHo , ThomasL . Griffiths , andDylan Hadfield - Menell . 2022 . How to talk so AI will learn : Instructions , descriptions , and autonomy . In Advances in Neural Information Processing Systems , Alice H . Oh , Alekh Agarwal , Danielle Belgrave , and Kyunghyun Cho ( Eds . ) . https : / / openreview . net / forum ? id = ZLsZmNe1RDb [ 67 ] Romal Thoppilan , Daniel De Freitas , Jamie Hall , Noam Shazeer , Apoorv Kul - shreshtha , Heng - TzeCheng , AliciaJin , TaylorBos , LeslieBaker , YuDu , etal . 2022 . Lamda : Language models for dialog applications . arXiv preprint arXiv : 2201 . 08239 ( 2022 ) . [ 68 ] Jeremy Tien , Jerry Zhi - Yang He , Zackory Erickson , Anca D Dragan , and Daniel Brown . 2022 . A Study of Causal Confusion in Preference - Based Reward Learning . arXiv preprint arXiv : 2204 . 06601 ( 2022 ) . [ 69 ] Maria Tsimpoukelli , Jacob L Menick , Serkan Cabi , SM Eslami , Oriol Vinyals , and Felix Hill . 2021 . Multimodal few - shot learning with frozen language models . Advances in Neural Information Processing Systems 34 ( 2021 ) , 200 â€“ 212 . [ 70 ] Xiaofei Wang , Kimin Lee , Kourosh Hakhamaneshi , Pieter Abbeel , and Michael Laskin . 2022 . Skill preferences : Learning to extract and execute robotic skills from human feedback . In Conference on Robot Learning . PMLR , 1259 â€“ 1268 . [ 71 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Fei Xia , Ed Chi , QuocVLe , DennyZhou , etal . 2022 . Chain - of - thoughtpromptingelicitsreasoning in large language models . Advances in Neural Information Processing Systems 35 ( 2022 ) , 24824 â€“ 24837 . [ 72 ] Aaron Wilson , Alan Fern , and Prasad Tadepalli . 2012 . A bayesian approach for policylearningfromtrajectorypreferencequeries . Advancesinneuralinformation processing systems 25 ( 2012 ) . [ 73 ] Christian Wirth , Riad Akrour , Gerhard Neumann , Johannes FÃ¼rnkranz , et al . 2017 . A survey of preference - based reinforcement learning methods . Journal of Machine Learning Research 18 , 136 ( 2017 ) , 1 â€“ 46 . [ 74 ] Annie Xie , Avi Singh , Sergey Levine , and Chelsea Finn . 2018 . Few - shot goal inference for visuomotor learning and planning . In Conference on Robot Learning . PMLR , 40 â€“ 52 . [ 75 ] Qizhe Xie , Minh - Thang Luong , Eduard Hovy , and Quoc V Le . 2020 . Self - training with noisy student improves imagenet classification . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 10687 â€“ 10698 . [ 76 ] Wei Ying , Yu Zhang , Junzhou Huang , and Qiang Yang . 2018 . Transfer learning via learning to transfer . In International Conference on Machine Learning . PMLR , 5085 â€“ 5094 . [ 77 ] Kevin Zakka , Andy Zeng , Pete Florence , Jonathan Tompson , Jeannette Bohg , and Debidatta Dwibedi . 2022 . Xirl : Cross - embodiment inverse reinforcement learning . In Conference on Robot Learning . PMLR , 537 â€“ 546 . [ 78 ] AndyZeng , MariaAttarian , KrzysztofMarcinChoromanski , AdrianWong , Stefan Welker , Federico Tombari , Aveek Purohit , Michael S Ryoo , Vikas Sindhwani , Johnny Lee , et al . 2022 . Socratic Models : Composing Zero - Shot Multimodal Reasoning with Language . In The Eleventh International Conference on Learning Representations . [ 79 ] XiaohuaZhai , XiaoWang , BasilMustafa , AndreasSteiner , DanielKeysers , Alexan - der Kolesnikov , and Lucas Beyer . 2021 . LiT : Zero - Shot Transfer with Locked - image Text Tuning . arXiv preprint arXiv : 2111 . 07991 ( 2021 ) . [ 80 ] Ruohan Zhang , Dhruva Bansal , Yilun Hao , Ayano Hiranaka , Jialu Gao , Chen Wang , Roberto MartÃ­n - MartÃ­n , Li Fei - Fei , and Jiajun Wu . 2022 . A Dual Repre - sentation Framework for Robot Learning with Human Guidance . In 6th Annual Conference on Robot Learning . https : / / openreview . net / forum ? id = H6rr _ CGzV9y PREDILECT : Preferences Delineated with Zero - Shot Language - based Reasoning in Reinforcement Learning HRI â€™24 , March 11 â€“ 14 , 2024 , Boulder , CO , USA Table 5 : PPO hyperparameters Value Walker2D Cheetah Social Nav . Network architecture { hidden , output } act . ( Actor ) { tanh , tanh } { relu , tanh } { relu , tanh } { hidden , output } act . ( Critic ) { tanh , tanh } { relu , tanh } { relu , tanh } Hidden layers ( Actor ) { 64 , 64 } { 256 , 256 } { 128 , 128 } Hidden layers ( Critic ) { 64 , 64 } { 256 , 256 } { 128 , 128 } RL parameters Critic learning rate 5e - 05 2e - 05 3e - 04 Actor learning rate 5e - 05 2e - 05 3e - 04 Batch size 32 64 128 Discount factor 0 . 99 0 . 98 0 . 99 n steps 512 512 1024 n epochs 20 20 10 GAE lambda ( ğœ ) 0 . 95 0 . 92 0 . 99 Clip range 0 . 1 0 . 2 0 . 2 Normalized advantage True True True Entropy coefficient 6e - 04 4e - 04 5e - 04 VF coefficient 0 . 87 0 . 5 0 . 5 8 APPENDIX 8 . 1 Implementation details For our implementation , we use a PPO [ 58 ] algorithm implementa - tion from Stable Baselines [ 28 ] . We modified the normal RL loop to make it conform to our preference learning implementation as de - scribed in the main paper . We implemented the reward model from scratch based on details from Cristiano et al . [ 16 ] in PyTorch [ 50 ] . In addition , we extend the reward model to enable the usage of highlights as an additional learning objective . To collect segments , we temporarily stop training and execute the current policy using undirected homogenous exploration [ 73 ] . The segments are collected sequentially with a randomly sized interval to increase the diversity . We handle terminal states by restarting the environment while continuing the same segment . Another considered method was to end the segment and fill the rest with some value without meaning . After consideration , we chose the restart method to ensure that no information about the terminal state was leaked into the segments . We used uniform random sampling to generate the queries . The reward functionâ€™s architecture includes hidden layers with dimensions of ( 256 , 256 , 256 ) . These layers primarily use ReLU activation functions with the output layer using a tanh activation . Both the agentâ€™s policy and value function have a structure of ( 128 , 128 ) and use ReLU activations . The reward functionâ€™s network architecture was designed as a neural network . This network had hidden layers with dimensions of ( 255 , 255 , 255 ) and primarily employed ReLU activation functions . Only the final layer used a tanh activation , ensuring output values remained between - 1 and 1 . The agentâ€™s policy and value network was of size ( 128 , 128 ) for Reacher and ( 256 , 256 ) for Cheetah . Both were using ReLU activations for the hidden layers . In both Cheetah and Reacher , every 20K timesteps , the agent updates its reward function . With each update , we use 10 % of the total available queries until it used up the query quota . 8 . 2 Hyperparameters Table 4 : Reward hyperparameters Value Network architecture { hidden , output } activation { relu , tanh } Hidden sizes { 256 , 256 , 256 } Preference Learning Segment size 50 Initial amount of queries 1 / 10 of queries Initial training epochs 200 Queries per update 1 / 10 of queries Training epochs per update 50 Batch size 128 Timesteps between updates 20K Learning rate 0 . 0003