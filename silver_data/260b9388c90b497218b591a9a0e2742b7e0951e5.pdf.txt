Proceedings of Machine Learning Research vol 145 : 1 – 22 , 2022 3rd Annual Conference on Mathematical and Scientiﬁc Machine Learning Momentum Transformer : Closing the Performance Gap Between Self - attention and Its Linearization Tan Nguyen MN 15 @ RICE . EDU Department of Mathematics , UCLA , Los Angeles , CA , 90095 Richard G . Baraniuk RICHB @ RICE . EDU Department of ECE , Rice University , Houston , TX , 77005 Robert M . Kirby KIRBY @ SCI . UTAH . EDU School of Computing and Scientiﬁc Computing and Imaging Institute , University of Utah , Salt Lake City , UT , 84112 Stanley J . Osher SJO @ MATH . UCLA . EDU Department of Mathematics , UCLA , Los Angeles , CA , 90095 Bao Wang WANGBAONJ @ GMAIL . COM Department of Mathematics and Scientiﬁc Computing and Imaging Institute , University of Utah , Salt Lake City , UT , 84112 Editors : Bin Dong , Qianxiao Li , Lei Wang , Zhi - Qin John Xu Abstract Transformers have achieved remarkable success in sequence modeling and beyond but suffer from quadratic computational and memory complexities with respect to the length of the input sequence . Leveraging techniques include sparse and linear attention and hashing tricks ; efﬁcient transformers have been proposed to reduce the quadratic complexity of transformers but signiﬁcantly degrade the accuracy . In response , we ﬁrst interpret the linear attention and residual connections in computing the attention map as gradient descent steps . We then introduce momentum into these components and propose the momentum transformer , which utilizes momentum to improve the accuracy of lin - ear transformers while maintaining linear memory and computational complexities . Furthermore , we develop an adaptive strategy to compute the momentum value for our model based on the opti - mal momentum for quadratic optimization . This adaptive momentum eliminates the need to search for the optimal momentum value and further enhances the performance of the momentum trans - former . A range of experiments on both autoregressive and non - autoregressive tasks , including im - age generation and machine translation , demonstrate that the momentum transformer outperforms popular linear transformers in training efﬁciency and accuracy . Keywords : transformer , adaptive momentum , efﬁcient attention 1 . Introduction The self - attention mechanism is the backbone of building transformers ( Vaswani et al . , 2017 ; Kim et al . , 2017 ) . Given an input sequence X = [ x 1 , . . . , x N ] (cid:62) ∈ R N × D x of N feature vectors , the self - attention transforms it into another sequence ˆ V = [ ˆ v 1 , . . . , ˆ v N ] (cid:62) ∈ R N × D v as follows ˆ v i = N (cid:88) j = 1 softmax (cid:16) q (cid:62) i k j √ D (cid:17) v j , for i = 1 , . . . , N , ( 1 ) © 2022 T . Nguyen , R . G . Baraniuk , R . M . Kirby , S . J . Osher & B . Wang . a r X i v : 2208 . 00579v1 [ c s . L G ] 1 A ug 2022 N GUYEN B ARANIUK K IRBY O SHER W ANG where the scalar softmax ( ( q (cid:62) i k j ) / √ D ) can be understood as the attention ˆ v i pays to the input feature x j . The vectors q i , k j , and v j are the query , key , and value vectors , respectively , and are computed as follows [ q 1 , q 2 , . . . , q N ] (cid:62) : = Q = XW (cid:62) Q ∈ R N × D , [ k 1 , k 2 , . . . , k N ] (cid:62) : = K = XW (cid:62) K ∈ R N × D , [ v 1 , v 2 , . . . , v N ] (cid:62) : = V = XW (cid:62) V ∈ R N × D v , ( 2 ) where W Q , W K ∈ R D × D x , and W V ∈ R D v × D x are the weight matrices . ( 1 ) can be written as ˆ V = softmax (cid:16) QK (cid:62) √ D (cid:17) V , ( 3 ) where the softmax function is applied to each row of the matrix ( QK (cid:62) ) / √ D . ( 3 ) is also called the “softmax attention” . Each transformer layer T (cid:96) ( · ) is deﬁned via the following residual connection , T (cid:96) ( X ) = f (cid:96) ( ˆ V + X ) , ( 4 ) where f (cid:96) ( · ) is a function that transforms each feature vector independently and usually chosen to be a feedforward network . In this paper , we call a transformer built with softmax attention standard transformer or transformer . It is easy to see that both memory and computational complexity of ( 3 ) are O ( N 2 ) with N being the length of the input sequence . We can further introduce causal masking into ( 3 ) for autoregressive applications ( Vaswani et al . , 2017 ) . Transformers have become the state - of - the - art model for solving many challenging problems in natural language processing ( Vaswani et al . , 2017 ; Al - Rfou et al . , 2019 ; Dai et al . , 2019 ; Williams et al . , 2018 ; Devlin et al . , 2018 ; Brown and et al . , 2020 ; Howard and Ruder , 2018 ; Rajpurkar et al . , 2016 ) and computer vision ( Dehghani et al . , 2018 ; So et al . , 2019 ; Dosovitskiy et al . , 2020 ; Touvron et al . , 2020 ) . Nevertheless , the quadratic memory and computational cost of computing the softmax attention ( 3 ) is a major bottleneck for applying transformers to large - scale applications that involve very long sequences , such as those in ( Liu et al . , 2018 ; Huang et al . , 2018 ; Parmar et al . , 2018 ) . Thus , much recent research on transformers has been focusing on developing efﬁcient transformers , aiming to reduce the memory and computational complexities of the model ( Qiu et al . , 2019 ; Child et al . , 2019 ; Ho et al . , 2019 ; Parmar et al . , 2018 ; Beltagy et al . , 2020 ; Ainslie et al . , 2020 ; Wang et al . , 2020 ; Tay et al . , 2020a , b ; Kitaev et al . , 2020 ; Roy et al . , 2021 ; Vyas et al . , 2020 ; Zaheer et al . , 2021 ; Wang et al . , 2020 ; Katharopoulos et al . , 2020 ; Choromanski and et al . , 2021 ; Shen et al . , 2021 ; Schlag et al . , 2021 ; Blanc and Rendle , 2018 ; Rawat et al . , 2019 ; Song et al . , 2021 ; Peng et al . , 2021 ; Xiong et al . , 2021 ; Nguyen et al . , 2021 ) . A thorough survey of recent advances in efﬁcient transformers is available at ( Tay et al . , 2020c ) . These efﬁcient transformers have better memory and / or computational efﬁciency at the cost of a signiﬁcant reduction in accuracy . 1 . 1 . Motivation Katharopoulos et al . ( 2020 ) have established a connection between transformers and recurrent neu - ral networks ( RNNs ) through the kernel trick . They propose the linear transformer , which can be considered a rank - one approximation of the softmax transformer . Linear transformers have compu - tational advantages in training , test , and inference : the RNN formulation ( see Equation ( 8 ) below ) enjoys fast inference , especially for autoregressive tasks , and the unrolled RNN formulation ( see 2 M OMENTUM T RANSFORMER Equation ( 6 ) below ) is efﬁcient for fast training . See section 2 for a detailed review of the linear transformer and its advantages . Nguyen et al . ( 2020 ) proposes integrating momentum into RNNs to accelerate training RNNs and improve learning long - term dependencies . We notice that Momen - tumRNN also enjoys a closed unrolling form , which is quite unique among existing techniques for improving RNNs , enabling fast training , test , and inference ; see section 3 for details . As such , in this paper we study how does momentum improves linear transformers ? 1 . 2 . Contribution We propose momentum transformers by integrating two new momentum - related ingredients into the recently proposed linear transformers ( Katharopoulos et al . , 2020 ) and its RNN formulation to improve the model’s accuracy and efﬁciency . Our contributions include : 1 ) Similar to ( Nguyen et al . , 2020 ) , we make an analogy between the RNN formulation of causal linear attention , which is the linear attention with causal masking for auto - regressive applications ( Katharopoulos et al . , 2020 ) , and a gradient descent step . We then integrate the heavy ball - style momentum into this RNN formulation and result in the causal momentum attention . We extend this causal momentum attention into momentum attention for both autoregressive and non - autoregressive applications . We name the transformer with the new momentum attention the momentum transformer . 2 ) We further introduce another momentum into the residual connection between the attention ˆ V and the input X in ( 4 ) to enhance the model’s performance . 3 ) We develop a new adaptive strategy to compute the momentum value and eliminate the burden of tuning momentum hyperparameters in our model . We name the momentum transformer with adaptive momentum the adaptive momentum transformer . The major advantages of momentum - based transformers include : • Momentum and adaptive momentum transformers inherit memory and computational efﬁ - ciency from the linear transformers while achieving better accuracy . • The training of momentum - based transformers converges remarkably faster than the training of linear transformers . • The design principle of momentum transformers is rooted in momentum - based optimization algorithms , enabling us to design more general and advanced momentum transformers for a wide range of applications . 1 . 3 . Related Work In this part , we brieﬂy review three lines of recent research that are most related to our work : 1 ) momentum in optimization and sampling , 2 ) momentum in deep neural network ( DNN ) design and 3 ) algorithms for efﬁcient transformers . Momentum in optimization and sampling Momentum has been a popular technique for accel - erating ( stochastic ) gradient - based optimization ( Polyak , 1964 ; Goh , 2017 ; Sutskever et al . , 2013 ; Kingma and Ba , 2014 ; Paszke et al . , 2019 ; Sun et al . , 2021 ) and sampling algorithms ( Duane et al . , 1987 ; Neal et al . ; Chen et al . , 2014 ; Betancourt , 2017 ) . A particularly interesting momentum is the iteration - dependent one in NAG ( Nesterov , 1983 ; Nemirovskii and Nesterov , 1985 ; Beck and Teboulle , 2009 ) , which has a signiﬁcantly better convergence rate than constant momentum for con - vex optimization . The stochastic gradient NAG that employs a scheduled restart can also be used to accelerate DNN training with better accuracy and faster convergence ( Wang et al . , 2022 ) . 3 N GUYEN B ARANIUK K IRBY O SHER W ANG DNNs with momentum . Momentum has been used for designing DNN architectures . He et al . ( 2019 ) employ momentum to build large and consistent dictionaries for unsupervised learning with a contrastive loss leveraging momentum - based moving average of the queue encoder . Many DNN - based methods for sparse coding are designed by unfolding the classical optimization algorithms with momentum , e . g . , FISTA ( Beck and Teboulle , 2009 ) . MomentumRNNs ( Nguyen et al . , 2020 ; Wang et al . , 2021 ) are a class of RNNs that are designed based on momentum accelerated ﬁrst - order optimization algorithms . MomentumRNNs can effectively resolve the vanishing gradient issue in training RNNs and obtain faster training and better performance over traditional RNNs . Momentum has also been integrated into ResNets ( Li et al . , 2018 ) and neural ODEs Xia et al . ( 2021 ) . Efﬁcient transformers . Existing efﬁcient transformers can be roughly classiﬁed into several cate - gories , as summarized in ( Roy et al . , 2021 ) . Among these categories are models with ﬁxed patterns , which sparsify the attention matrix ( Parmar et al . , 2018 ; Liu et al . , 2018 ; Qiu et al . , 2019 ; Child et al . , 2019 ; Beltagy et al . , 2020 ) . Another category includes models that integrate two or more dis - tinct access patterns to improve the coverage ( Child et al . , 2019 ; Ho et al . , 2019 ) . Learnable patterns are also leveraged to learn the access pattern in a data - driven fashion ( Kitaev et al . , 2020 ; Roy et al . , 2021 ; Tay et al . , 2020b ) . Some other efﬁcient transformers take advantage of a side memory module to access multiple tokens at once ( Lee et al . , 2019 ; Sukhbaatar et al . , 2019 ; Asai and Choi , 2020 ; Beltagy et al . , 2020 ) . Finally , low - rank and kernelization approximation are employed to improve the memory and computational efﬁciency of computing self - attention , see e . g . , ( Tsai et al . , 2019 ; Wang et al . , 2020 ; Katharopoulos et al . , 2020 ; Choromanski and et al . , 2021 ; Shen et al . , 2021 ) . 1 . 4 . Notations We denote scalars by lower - or upper - case letters . We also denote vectors and matrices by lower - and upper - case boldface letters , respectively . For a vector x = ( x 1 , . . . , x d ) (cid:62) ∈ R d , where ( x 1 , . . . , x d ) (cid:62) denotes the transpose of the vector ( x 1 , . . . , x d ) , we use (cid:107) x (cid:107) = ( (cid:80) di = 1 | x i | 2 ) 1 / 2 to denote its (cid:96) 2 norm . We denote the vector whose entries are all 0s as 0 . For a matrix A , we use A (cid:62) , A − 1 , and (cid:107) A (cid:107) to denote its transpose , inverse , and spectral norm , respectively . We use I to denote the identity matrix , whose dimension can be determined in its context . For a function f ( x ) : R d → R , we denote its gradient as ∇ f ( x ) . Given two sequences { a n } and { b n } , we write a n = O ( b n ) if there exists a positive constant 0 < C < + ∞ such that a n ≤ Cb n . 1 . 5 . Organization We organize this paper as follows : In section 2 , we review the kernelization trick used to linearize the softmax attention and the RNN formulation of the linear transformer with causal masking . In section 3 , we present the momentum transformer and adaptive momentum transformer , providing the motivation and detailed derivation . We verify the efﬁciency of our momentum - based transform - ers on various applications , including both autoregressive and non - autoregressive tasks in section 4 . The paper ends up with concluding remarks . 4 M OMENTUM T RANSFORMER 2 . Linear Transformer Transformers learn long - term dependencies in sequences effectively and concurrently through the self - attention mechanism . By denoting k ( q i , k j ) : = exp ( q (cid:62) i k j / √ D ) , we can rewrite ( 1 ) as ˆ v i = ( N (cid:88) j = 1 k ( q i , k j ) v j ) / ( N (cid:88) j = 1 k ( q i , k j ) ) . In linear transformers ( Wang et al . , 2020 ; Katharopoulos et al . , 2020 ; Choromanski and et al . , 2021 ; Shen et al . , 2021 ) , the feature map k ( q i , k j ) is linearized as the product of feature maps φ ( · ) on the vectors q i and k j , i . e . , k ( q i , k j ) = φ ( q i ) (cid:62) φ ( k j ) . The associative property of matrix multiplication is then utilized to derive the following efﬁcient computation of the attention map ˆ v i = (cid:80) Nj = 1 k ( q i , k j ) v j (cid:80) N j = 1 k ( q i , k j ) = (cid:80) Nj = 1 φ ( q i ) (cid:62) φ ( k j ) v j (cid:80) N j = 1 φ ( q i ) (cid:62) φ ( k j ) = φ ( q i ) (cid:62) (cid:80) Nj = 1 φ ( k j ) v (cid:62) j φ ( q i ) (cid:62) (cid:80) N j = 1 φ ( k j ) . ( 5 ) In the matrix - product form , we can further write ( 5 ) as follows ˆ V = φ ( Q ) ( φ ( K ) (cid:62) V ) φ ( Q ) φ ( K ) (cid:62) . ( 6 ) Replacing ( φ ( Q ) φ ( K (cid:62) ) ) V with φ ( Q ) ( φ ( K (cid:62) ) V ) reduces the memory and computational cost of computing the attention map from O ( N 2 ) to O ( N ) , making linear transformers scalable to very long sequences . Furthermore , causal masking can be easily implemented in the linearized attention by truncating the summation term in the last equation of ( 5 ) , resulting in ˆ v i = φ ( q i ) (cid:62) (cid:80) ij = 1 φ ( k j ) v (cid:62) j φ ( q i ) (cid:62) (cid:80) ij = 1 φ ( k j ) : = φ ( q i ) (cid:62) s i φ ( q i ) (cid:62) z i , ( 7 ) where s i = (cid:80) ij = 1 φ ( k j ) v (cid:62) j and z i = (cid:80) ij = 1 φ ( k j ) . The states s i and z i can be computed in a recurrent fashion . Efﬁcient inference via the RNN formulation . Self - attention processes tokens of a sequence con - currently , enabling fast training of transformers via layerwise parallelism . However , during infer - ence , the output for timestep i is the input for timestep i + 1 . As a result , the inference in standard transformers cannot be parallelized and is thus computationally inefﬁcient . Linear transformers pro - vide an elegant approach to ﬁxing this issue by leveraging their RNN formulation . In particular , we can further write the linear attention with causal masking in ( 7 ) into the following RNN form 1 s i = s i − 1 + φ ( k i ) v (cid:62) i ; z i = z i − 1 + φ ( k i ) ; ˆ v i = φ ( q i ) (cid:62) s i φ ( q i ) (cid:62) z i , ( 8 ) where s 0 = 0 and z 0 = 0 . Note that this RNN formulation of linear transformers with causal masking contains two memory states s i and z i . 1 . For simplicity , we omit the nonlinearity ( a two - layer feedforward network ) compared to ( Katharopoulos et al . , 2020 ) . 5 N GUYEN B ARANIUK K IRBY O SHER W ANG 3 . Momentum Transformer In this section , we present the momentum transformer . We start by integrating the heavy ball mo - mentum into the RNN formulation of causal linear attention in ( 8 ) , resulting in the causal momen - tum attention . Next , we generalize the causal momentum attention to momentum attention that can efﬁciently train the model . Moreover , we propose the momentum connection to replace residual connections between the attention ˆ V and the input X in ( 4 ) to boost the model’s performance . Fi - nally , we derive the adaptive momentum attention from the theory of optimal choice of momentum for the heavy ball method . 3 . 1 . Momentum Transformer Heavy ball momentum . Let us recall the heavy ball momentum for accelerating gradient descent in solving min x ∈ R d f ( x ) ( Polyak , 1964 ) . Starting from x 0 and x 1 , the heavy ball method iterates as follows x k + 1 = x k − γ ∇ f ( x k ) + β ( x k − x k − 1 ) , ( 9 ) where γ > 0 is the step size and 0 ≤ β < 1 is the momentum parameter . By introducing the momentum state m , we can rewrite the HB method as m k + 1 = β m k + ∇ f ( x k ) ; x k + 1 = x k − γ m k + 1 . ( 10 ) In contrast , gradient descent updates at each step as follows x k + 1 = x k − γ ∇ f ( x k ) . ( 11 ) Integrating momentum into causal linear attention . Now we consider integrating the heavy ball momentum into causal linear attention . We integrate momentum into the state s i in ( 8 ) only since the denominator in causal linear attention is simply a normalizing scalar . If we regard − φ ( k i ) v (cid:62) i as the gradient vector in ( 11 ) , then we can add momentum into the state s i by following the heavy ball method in ( 10 ) , resulting in the following RNN formulation of causal momentum attention , m i = β m i − 1 − φ ( k i ) v (cid:62) i ; s i = s i − 1 − γ m i ; z i = z i − 1 + φ ( k i ) ; ˆ v i = φ ( q i ) (cid:62) s i φ ( q i ) (cid:62) z i , ( 12 ) where m 0 = 0 , and γ > 0 and 0 ≤ β < 1 are two hyperparameters . The RNN formulation of causal momentum attention in ( 12 ) is efﬁcient for autoregressive inference . For efﬁcient training , we need to rewrite ( 12 ) into a form that is similar to the linear attention in ( 7 ) . To this end , we need to eliminate the states m i , s i , and z i from ( 12 ) . Notice that s i = s i − 1 − γ m i (cid:124)(cid:123)(cid:122)(cid:125) : = p i = s 0 (cid:124)(cid:123)(cid:122)(cid:125) = 0 − (cid:16) p i + p i − 1 + . . . + p 1 (cid:17) , 6 M OMENTUM T RANSFORMER since m i = β m i − 1 − φ ( k i ) v (cid:62) i , we have p i = β p i − 1 − γφ ( k i ) v (cid:62) i . Therefore , s i = − ( p i + p i − 1 + . . . + p 1 ) = γφ ( k i ) v (cid:62) i − (cid:16) ( 1 + β ) p i − 1 + p i − 2 + . . . + p 1 (cid:17) = γφ ( k i ) v (cid:62) i + γ ( 1 + β ) φ ( k i ) v (cid:62) i − (cid:16) ( 1 + β ) 2 p i − 2 + . . . + p 1 (cid:17) = . . . = γ i (cid:88) j = 1 1 − β i − j + 1 1 − β φ ( k j ) v (cid:62) j for i ≥ 1 . We can then formulate the causal momentum attention as follows ˆ v i = γφ ( q i ) (cid:62) (cid:80) ij = 1 (cid:16) 1 − β i − j + 1 1 − β φ ( k j ) v (cid:62) j (cid:17) φ ( q i ) (cid:62) z i . ( 13 ) Note that ( 13 ) is mathematically equivalent to ( 12 ) , but it can be trained much more efﬁciently in a concurrent fashion via layerwise parallelism . Remark 1 Comparing ( 13 ) with ( 7 ) , we see that momentum plays a role in reweighting the terms { φ ( k j ) v (cid:62) j } ij = 1 . It is interesting to note that this reweighting is opposite to that used for reweighting the local attention ( Dai et al . , 2019 ) . It has also been noticed that low - rank attention can comple - ment local attention , resulting in improved performance ( Nguyen et al . , 2021 ) . Often local attention behaves quite differently from low - rank attention , and different reweighting can be beneﬁcial . One particular reweighting strategy is decomposing softmax attention into long and short - range compo - nents and using different weighting schemes for each part . We leave the study of reweighting local attention and low - rank attention differently as future work . Integrating momentum into linear attention . To obtain momentum attention without causal masking , we can simply take the sum from 1 to N instead of summing from 1 to i . Therefore , we obtain the following momentum attention ˆ v i = γφ ( q i ) (cid:62) (cid:80) N j = 1 (cid:16) 1 − β N − j + 1 1 − β φ ( k j ) v (cid:62) j (cid:17) φ ( q i ) (cid:62) (cid:80) Nj = 1 φ ( k j ) . ( 14 ) Memory and computational complexity . It is clear that training momentum transformers have the same memory and computational complexities of O ( N ) as the training of linear transformers . For test and inference , momentum transformers also have the same memory and computational complexities as linear transformers . However , in the RNN form , momentum transformers require slightly more memory than linear transformers to store the extra momentum state m i . 3 . 2 . Momentum Connection On top of the self - attention unit , each transformer layer has a residual connection between the self - attention output and the input as shown in ( 4 ) . We further integrate momentum into ( 4 ) and derive the momentum connection as follows 7 N GUYEN B ARANIUK K IRBY O SHER W ANG T (cid:96) ( X ) = f (cid:96) (cid:0) ˆ V + X + ˜ β ( X − T (cid:96) − 1 ( X ) ) (cid:1) , ( 15 ) where 0 ≤ ˜ β < 1 is a hyperparameter . 3 . 2 . 1 . A DAPTIVE M OMENTUM Our momentum transformer introduces additional hyperparameters γ and β , as well as ˜ β , compared to the linear transformer . In section 4 , we show that γ can be simply set to 1 in many experiments . However , tuning the momentum - related hyperparameters β and ˜ β can introduce extra computational cost for training transformers . Moreover , using a constant momentum may not give us optimal performance . In this section , we will introduce an adaptive momentum formula for computing the momentum hyperparameter in momentum connection and thus eliminating the computational overhead for tuning ˜ β . Here , the adaptive momentum does not apply to β since it will break the closed unrolling form in ( 13 ) . Optimal choice of heavy ball momentum . We motivate our adaptive momentum from the opti - mal choice of the heavy ball momentum for solving the following quadratic minimization problem min x f ( x ) : = 1 2 x (cid:62) Ax + x (cid:62) b . ( 16 ) The following theorem gives the optimal choice of β for ( 10 ) to solve ( 16 ) . Theorem 2 ( Sun et al . ( 2021 ) ) Let f ( x ) be the quadratic function ( 16 ) with ∇ f ( x ) = Ax + b , where A is positive deﬁnite . Moreover , we denote the smallest ( λ min ( A ) ) and the largest eigenval - ues ( λ max ( A ) ) of A as ν and L , respectively . Given any ﬁxed step size γ ≤ 1 / L , the optimal choice for β is ˜ β = ( 1 − √ γν ) 2 . In this case , the heavy ball method achieves a convergence rate of (cid:107) x k + 1 − x ∗ (cid:107) ≤ ( 1 − √ γν ) (cid:107) x k − x ∗ (cid:107) , where x ∗ is the minimum of the quadratic function ( 16 ) . Theorem 2 shows that the optimal momentum for the heavy ball method should be ( 1 − √ γν ) 2 if γ ≤ 1 / L . However , the smallest eigenvalue ν is usually unknown . Therefore , we consider constructing the sequence { (cid:107)∇ f ( x k ) − ∇ f ( x k − 1 ) (cid:107) / (cid:107) x k − x k − 1 (cid:107) } k ≥ 1 to approximate ν . We have the following theoretical result to guarantee that (cid:107)∇ f ( x k ) − ∇ f ( x k − 1 ) (cid:107) / (cid:107) x k − x k − 1 (cid:107) → ν as k → ∞ . Proposition 3 ( Sun et al . ( 2021 ) ) Assume that conditions in Theorem 2 hold and { x k } k ≥ 0 is gen - erated by the heavy ball method ( 10 ) . If γ ≤ 1 / L , for any ﬁxed 0 ≤ β < 1 , we have lim k →∞ (cid:107)∇ f ( x k ) − ∇ f ( x k − 1 ) (cid:107) (cid:107) x k − x k − 1 (cid:107) = ν . In practice , for a given step size γ , we restrict the adaptive momentum to be in the range [ 0 , 1 − δ ] with δ being the threshold parameter , and we choose it to be 10 − 3 in this work . Hence , we have the following adaptive momentum proj [ 0 , 1 − δ ] (cid:32) 1 − (cid:115) γ (cid:107)∇ f ( x k ) − ∇ f ( x k − 1 ) (cid:107) (cid:107) x k − x k − 1 (cid:107) (cid:33) 2 , ( 17 ) 8 M OMENTUM T RANSFORMER Copy Task T r a i n L o ss Iteration T e s t A cc u r a c y Epoch Adaptive Momentum Transformer Momentum Transformer Linear Transformer Softmax Transformer Momentum Transformer + Momentum Connection Reformer T r a i n L o ss Time ( seconds ) T e s t A cc u r a c y Time ( seconds ) Figure 1 : Convergence comparison of adaptive momentum , momentum , reformer , linear , and soft - max transformer on the sequence copy task . Momentum and adaptive momentum trans - formers converge faster and achieve better training loss than both linear transformer and reformer . Softmax transformer converges the fastest but suffers from quadratic memory and computational complexities . Adaptive momentum transformer performs as well as momentum transformer without intensively searching for momentum values . The train - ing time per epoch for softmax transformer , reformer , linear transformer , our momentum transformer without and with momentum connection , and our adaptive momentum trans - former are 7 . 1 , 6 . 9 , 6 . 4 , 6 . 5 , 6 . 6 , and 6 . 8 seconds , respectively . Here , the computational time per epoch does not reveal a signiﬁcant advantage of efﬁcient transformers over the softmax transformer because the sequence length is only 128 . where proj [ 0 , 1 − δ ] ( · ) : = max ( 0 , min ( 1 − δ , · ) ) . To simplify our computation , we apply the gradient descent update to approximate x k − x k − 1 , i . e . , we approximate x k − x k − 1 by γ ∇ f ( x k − 1 ) , and we end up with ˜ β k : = proj [ 0 , 1 − δ ] (cid:32) 1 − (cid:115) (cid:107)∇ f ( x k ) − ∇ f ( x k − 1 ) (cid:107) (cid:107)∇ f ( x k − 1 ) (cid:107) (cid:33) 2 . ( 18 ) 4 . Experimental Results In this section , we evaluate the beneﬁts of our momentum transformers in terms of convergence speed , efﬁciency , and accuracy . We compare the performance of momentum and adaptive mo - mentum transformers 2 with the baseline standard softmax transformer and several other efﬁcient transformers in the following tasks : 1 ) the synthetic copy task , 2 ) the MNIST and CIFAR image generation task , 3 ) Long - Range Arena ( Tay et al . , 2021 ) , and 4 ) the non - autoregressive machine translation task . These tasks are among standard benchmarks for measuring the performance of transformers and their computational and memory efﬁciency . The tasks we choose also cover dif - ferent data modalities — text and image — and a variety of model sizes . Our experimental results conﬁrm that momentum and adaptive momentum transformers outperform many existing efﬁcient transformers , including linear transformers and reformers , in accuracy and convergence . Further - more , adaptive momentum transformer improves over momentum transformer without the need of 2 . Here , the momentum and adaptive momentum transformers are built on the same baseline architecture of the linear transformer in terms of the number of layers and number of heads . 9 N GUYEN B ARANIUK K IRBY O SHER W ANG Table 1 : Momentum and Stepsize Hyperparameteres for Momentum - based Transformers . Model Momentum Stepsize Momentum in Stepsize in Momentum Connection Momentum Connection Copy Task Momentum transformer 0 . 1 0 . 6 Momentum transformer 0 . 1 0 . 6 0 . 99 0 . 99 + Momentum connection Adatptive momentum transformer 0 . 1 0 . 6 0 . 99 MNIST Generation Momentum transformer 0 . 6 0 . 9 Momentum transformer 0 . 6 0 . 9 0 . 1 0 . 99 + Momentum connection Adatptive momentum transformer 0 . 6 0 . 9 0 . 99 CIFAR Generation Momentum transformer 0 . 1 0 . 9 Momentum transformer 0 . 1 0 . 9 0 . 1 0 . 9 + Momentum connection Adatptive momentum transformer 0 . 1 0 . 9 0 . 9 Non - Autoregressive Machine Translation Momentum transformer 0 . 6 0 . 6 Momentum transformer 0 . 6 0 . 6 0 . 3 0 . 9 + Momentum connection Adatptive momentum transformer 0 . 6 0 . 6 0 . 9 ListOps Momentum transformer 0 . 1 0 . 6 Adatptive momentum transformer 0 . 1 0 . 6 0 . 4 Text Momentum transformer 0 . 6 2 . 0 Adatptive momentum transformer 0 . 6 2 . 0 0 . 001 Retrieval Momentum transformer 0 . 6 1 . 0 Adatptive momentum transformer 0 . 6 1 . 0 0 . 5 Image Momentum transformer 0 . 9 0 . 9 Adatptive momentum transformer 0 . 9 0 . 9 0 . 001 Pathﬁnder Momentum transformer 0 . 3 0 . 1 Adatptive momentum transformer 0 . 3 0 . 1 0 . 8 searching for momentum hyperparameter for the momentum connection . Values of momentum - related hyperparameters for experiments in our experiments are provided in Table 1 below . 10 M OMENTUM T RANSFORMER Method Bits / dim Images / sec Standard softmax transformer 0 . 84 0 . 45 ( 1 × ) Linear transformer 0 . 85 142 . 8 ( 317 × ) Momentum transformer 0 . 84 139 . 7 ( 310 × ) Momentum transformer + momentum connection 0 . 82 135 . 5 ( 301 × ) Adaptive momentum transformer 0 . 80 134 . 9 ( 300 × ) Table 2 : Momentum transformers achieve better test bits / dim than both softmax and linear trans - formers on MNIST generation . 4 . 1 . Copy Task We train momentum transformers and baseline models on a synthetic copy task to analyze their convergence speed . In this task , the model has to duplicate a sequence of symbols . Each training and test sample has the form 0 w 0 w where w is a sequence of symbols collected from the set { 1 , . . . , N } . An example with the word w of length 3 is given below . Example : 0 15 124 71 0 15 124 71 In our experiments , we follow the same experimental setting as that used by Katharopoulos et al . ( 2020 ) . In particular , we use a sequence of maximum length 128 with 10 different symbols separated by a separator symbol . The baseline architecture for all methods is a 4 - layer transformer with 8 attention heads and D = 32 . The models are trained with the RAdam optimizer using a batch size of 64 and a learning rate of 10 − 3 which is reduced to 10 − 4 after 3000 iterations . Figure 1 shows the training loss and the test accuracy over epochs and over GPU time . Both the momentum and the adaptive momentum transformers converge much faster and achieve better training loss than the linear transformer . Notice that while the standard softmax transformer converges the fastest , it has quadratic complexity . 4 . 2 . Image Generation Transformers have shown great promise in autoregressive generation applications ( Radford et al . , 2019 ; Child et al . , 2019 ) , such as autoregressive image generation ( Ramesh et al . , 2020 ) . How - ever , the training and sampling procedure using transformers are quite slow for these tasks due to the quadratic computational time complexity and the memory scaling with respect to the sequence length . In this section , we train our momentum - based transformers and the baselines with causal masking to predict images pixel by pixel and compare their performance . In particular , we demon - strate that , like linear transformers , both momentum and adaptive momentum transformers are able to generate images much faster than the standard softmax transformer . Furthermore , we show that momentum - based transformers converge much faster than linear transformers while achieving bet - ter bits per dimension ( bits / dim ) . We compare the generated images by different models in the Appendix . Note that momentum and adaptive momentum transformers also generate images with constant memory per image like linear transformers . 11 N GUYEN B ARANIUK K IRBY O SHER W ANG MNIST Task T e s t B i t s / d i m Epoch Adaptive Momentum Transformer Momentum Transformer Linear Transformer Softmax Transformer Momentum Transformer + Momentum Connection Figure 2 : Momentum transformers outperform linear transformers on the MNIST image generation task . Adaptive momentum transformer achieves the best test bits / dim . MNIST . We ﬁrst examine our momentum - based transformers on the MNIST image generation task . MNIST is a popular benchmark dataset used for image recognition and generation . For all methods , we train a 8 - layer transformer with 8 attention heads and the embedding size of 256 , which corresponds to 32 dimensions per head . The feedforward dimensions are 4 times larger than the embedding size . A mixture of 10 logistics is used to model the output as in ( Salimans et al . , 2017 ) . For training , we use the RAdam optimizer with a learning rate of 10 − 4 and train all models for 250 epochs except for the adaptive momentum transformer . We report the bits / dim and image generation throughput in Table 2 . Compared to the linear transformer , all momentum - based transformers not only attain better bits / dim but also have com - parable image generation throughput , justifying the linear complexity of our models . In addition , we demonstrate that the adaptive momentum transformer converges much faster than the baseline models in Figure 2 . Momentum - based transformers even outperform softmax transformers in this task . We also compare our adaptive momentum transformer with the standard softmax and linear transformer qualitatively . In particular , we analyze the models trained for the MNIST image gener - ation task and show the generated images from each model in Figure 3 . We observe that the quality of images generated from the adaptive momentum transformer and linear transformer is as high as the quality of images generated from the softmax transformer while the ﬁrst two models are much more computational and memory efﬁcient . CIFAR10 . Next , we investigate the advantages of our momentum - based transformers when the sequence length and the number of layers in the model increase . We consider the CIFAR - 10 image generation task , in which we train 16 - layer transformers to generate CIFAR - 10 images . The conﬁg - uration for each layer is the same as in the MNIST experiment . For the linear transformer and our momentum - based transformer , we use a batch size of 4 while using a batch size of 1 for the stan - dard softmax transformer due to the memory limit of the largest GPU available to us , i . e . , NVIDIA 12 M OMENTUM T RANSFORMER Adaptive Momentum Transformer Linear Transformer Softmax Transformer Figure 3 : MNIST samples generated by the standard softmax transformer ( left ) ( Vaswani et al . , 2017 ) , the linear transformer ( middle ) ( Katharopoulos et al . , 2020 ) , and the adaptive mo - mentum transformer ( right ) . Method Bits / dim Images / sec Standard softmax transformer 3 . 20 0 . 004 ( 1 × ) Linear transformer 3 . 44 17 . 85 ( 4462 × ) Momentum transformer 3 . 43 17 . 52 ( 4380 × ) Momentum transformer + momentum connection 3 . 41 17 . 11 ( 4277 × ) Adaptive momentum transformer 3 . 38 17 . 07 ( 4267 × ) Table 3 : Momentum - based transformers achieve better test bits / dim than linear transformer on CI - FAR10 image generation task . V100 . This is similar to the setting in ( Katharopoulos et al . , 2020 ) . Like in the MNIST image generation task , our momentum - based transformers outperform the linear transformer in terms of bits / dim while maintaining comparable image generation throughput . This is a very expensive task , limiting us to perform a thorough hyperparameter search ; we believe better results can be obtained with a more thorough hyperparameter search . 4 . 3 . Long - Range Arena In this experiment , we evaluate our model on tasks that involve longer sequence lengths in the Long Range Arena ( LRA ) benchmark ( Tay et al . , 2021 ) . We show that the momentum - based transformer outperforms the baseline linear transformer and other popular efﬁcient transformers , including per - former ( Choromanski and et al . , 2021 ) , reformer ( Kitaev et al . , 2020 ) , and linformer ( Wang et al . , 2020 ) . We also demonstrate that our momentum - based transformer yields better accuracy than the standard softmax transformer ( Vaswani et al . , 2017 ) in most tasks except the ListOps . These results justify the advantage of our momentum - based transformers in capturing long - term dependency . Datasets and metrics We consider all ﬁve tasks in the LRA benchmark ( Tay et al . , 2021 ) , includ - ing Listops , byte - level IMDb reviews text classiﬁcation , byte - level document retrieval , CIFAR - 10 13 N GUYEN B ARANIUK K IRBY O SHER W ANG Model ListOps ( 2K ) Text ( 4K ) Retrieval ( 4K ) Image ( 1K ) Pathﬁnder ( 1K ) Avg Softmax 37 . 10 ( 37 . 10 ) 64 . 17 ( 65 . 02 ) 80 . 71 ( 79 . 35 ) 39 . 06 ( 38 . 20 ) 72 . 48 ( 74 . 16 ) 58 . 70 ( 58 . 77 ) Linear 18 . 30 64 . 22 81 . 37 38 . 29 71 . 17 54 . 67 Performer 18 . 80 63 . 81 78 . 62 37 . 07 69 . 87 53 . 63 Reformer 19 . 05 64 . 88 78 . 64 43 . 29 69 . 36 55 . 04 Linformer 37 . 25 55 . 91 79 . 37 37 . 84 67 . 60 55 . 59 Momentum transformer 19 . 56 64 . 35 81 . 95 39 . 40 73 . 12 55 . 68 Adaptive momentum 20 . 16 64 . 45 82 . 07 39 . 53 74 . 00 56 . 04 transformer Table 4 : Results on the LRA tasks . We report the test classiﬁcation accuracy for each task and aver - age accuracy across all tasks . The momentum - based transformers , in particular , the adap - tive momentum transformer , outperforms all other transformers except on the ListOps . The numbers in the parenthesis are from the paper ( Xiong et al . , 2021 ) . Unit : % . Method BLEU Score Speed ( tokens / s ) Standard softmax transformer 24 . 34 5104 Linear transformer 21 . 37 1382 Momentum transformer 22 . 11 1398 Momentum transformer + momentum connection 22 . 14 1403 Adaptive momentum transformer 22 . 20 1410 Table 5 : BLEU scores and tokens per second from machine translation models trained on IWSLT show the advantages of our momentum - based transformers . The number of trainable pa - rameters is almost the same for all models , up to the small difference introduced by the momentum mechanism in our models . Momentum - based transformers outperform the linear transformer in generation quality in terms of BLEU score and obtain comparable generation efﬁciency in terms of tokens per second . image classiﬁcation on sequences of pixels , and Pathﬁnder . These tasks involve long sequences of length 2 K , 4 K , 4 K , 1 K , and 1 K , respectively . We follow the setup / evaluation protocol in ( Tay et al . , 2021 ) and report the test accuracy for each task and the average result across all tasks . Models and training All models have 2 layers , 64 embedding dimension , 128 hidden dimension , 2 attention heads . Mean pooling is applied in all models . Also , we use the nonlinear activation elu ( x ) + 1 for the linear transformer . Our implementation uses the public code by Xiong et al . ( 2021 ) as a starting point , and we follow their training procedures . The training setting and ad - ditional baseline model details are provided in the conﬁguration ﬁle used in ( Xiong et al . , 2021 ) and can be found at https : / / github . com / mlpen / Nystromformer / blob / main / LRA / code / lra _ config . py . Results We summarize our results in Table 4 . Both momentum - based transformers outperform linear transformers in all tasks and yield better accuracy than the standard softmax transformer in most tasks except the Listops . The adaptive momentum transformer performs the best on every task except the LipsOps , far behind the softmax transformer and Linformer . 14 M OMENTUM T RANSFORMER T r a i n l o ss T e s t a cc u r a c y Iteration Epoch Figure 4 : Ablation study of the effects of ˜ β on the performance of momentum transformer with momentum connection in the synthetic copy task . We report the train loss ( Left ) and test accuracy ( Right ) . Adaptive ˜ β ( the blue curve ) yields similar train loss and test accuracy as the best constant ˜ β found by a careful search . 4 . 4 . Non - Autoregressive Machine Translation The previous experiments are for auto - regressive tasks . In this experiment , we demonstrate that the beneﬁts of our momentum - based transformers also hold for a non - autoregressive task . We consider a machine translation task on the popular IWSLT’ 16 En - De dataset . We follow the setting in ( Lee et al . , 2018 ) . In particular , we tokenize each sentence using a script from Moses ( Koehn and et al . , 2007 ) and segment each word into subword units using BPE ( Sennrich et al . , 2016 ) . We also use 40 K tokens from both source and target . Our baseline model is the small transformer - based network in ( Lee et al . , 2018 ) with d model = 278 , d hidden = 507 , p dropout = 0 . 1 , n layer = 5 , and n head = 2 . This model has 5 layers , and each layer has 2 attention heads . A depiction of this architecture is given in Figure 2 in ( Lee et al . , 2018 ) . The block “Encoder” encodes the input X , the block “Decoder 1” computes the conditional log p ( Y 0 | X ) , and the block “Decoder 2” is shared across iterative reﬁnement steps , calculating log p ( Y (cid:96) | ˆ Y (cid:96) − 1 , X ) . For the baseline standard softmax transformer model , we use the same architecture as in ( Lee et al . , 2018 ) with an additional positional attention and using the highway layer in the decoders . For the linear and our momentum - based transformer models , we replace the softmax attention with the linear attention and momentum - based attention , respectively . During training , we use linear annealing learning rate scheduling ( from 3 × 10 − 4 to 10 − 5 ) . We do not use label smoothing nor average multiple check - pointed models . Table 5 reports the results in terms of generation quality , measured by the BLEU score ( Papineni et al . , 2002 ) , and generation efﬁciency , measured by the number of generated tokens per second . Consistent with other experiments above , our momentum - based transformers obtain better BLEU scores than the linear transformer in this non - autoregressive setting . Furthermore , in terms of gen - eration efﬁciency , momentum - based models are comparable with the linear transformer and much more efﬁcient than the standard softmax transformer . 4 . 5 . Ablation Studies Effects of ˜ β on the performance of momentum transformer with momentum connection . We have conducted an ablation study to analyze how values of ˜ β in momentum connection inﬂuence the the performance of momentum transformer with momentum connection in the synthetic copy 15 N GUYEN B ARANIUK K IRBY O SHER W ANG T r a i n l o ss T e s t a cc u r a c y Iteration Epoch Figure 5 : Ablation study of the effects of momentum β in the momentum attention on the perfor - mance of adaptive momentum transformer in the synthetic copy task . We report the train loss ( Left ) and test accuracy ( Right ) . Smaller β yields better results . task . Figure 4 shows that the transformer with adaptive ˜ β ( the blue curve ) achieves as good train loss and test accuracy as the transformer with the best constant ˜ β that are carefully ﬁne - tuned , i . e . ˜ β = 0 . 01 . Note that our adaptive approach to computing ˜ β eliminates the need of expensive search for the good values of ˜ β . Effects β in the momentum attention on the performance adaptive momentum transformer . We have conducted another ablation study on how the values of β in the momentum attention affect the performance of the adaptive momentum transformer in the synthetic copy task . Figure 5 demonstrates that smaller β yields better results in terms of train loss and test accuracy than large ones . We also notice that when β ≥ 1 , the training is unstable and does not converge . 5 . Concluding Remarks In this paper , we developed a new class of efﬁcient transformers , i . e . , momentum transformers , which have the same memory and computational complexity as the recently developed linear trans - former . We developed momentum transformers based on an analogy between the RNN formulation of causal linear attention and gradient descent . Then we integrate the momentum into causal linear attention following the heavy ball method . Furthermore , we introduce an additional momentum into the residual connection between the attention ˆ V and the input X in ( 4 ) to further improve the per - formance of the model . To eliminate the computational overhead for tuning the momentum - related hyperparameters and enhancing momentum transformers’ performance , we developed the adaptive momentum transformer that can adaptively compute the momentum values based on the optimal momentum choice for the heavy ball method for quadratic optimization . An interesting observation is that the momentum attention can be understood as a reweighting between the product of the “key” and “value” in the standard attention model . There are numerous avenues for future work : 1 ) can we develop momentum transformers based on other popular optimization algorithms beyond the heavy ball method , e . g . , Adam ? And 2 ) can we design better weighting schemes to improve the performance of transformers ? 16 M OMENTUM T RANSFORMER References Joshua Ainslie , Santiago Ontanon , Chris Alberti , Vaclav Cvicek , Zachary Fisher , Philip Pham , Anirudh Ravula , Sumit Sanghai , Qifan Wang , and Li Yang . ETC : Encoding long and structured inputs in transformers . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 268 – 284 , November 2020 . doi : 10 . 18653 / v1 / 2020 . emnlp - main . 19 . URL https : / / www . aclweb . org / anthology / 2020 . emnlp - main . 19 . Rami Al - Rfou , DK Choe , Noah Constant , Mandy Guo , and Llion Jones . Character - level language modeling with deeper self - attention . In Thirty - Third AAAI Conference on Artiﬁcial Intelligence , 2019 . URL https : / / arxiv . org / abs / 1808 . 04444 . Akari Asai and Eunsol Choi . Challenges in information seeking qa : Unanswerable questions and paragraph retrieval . arXiv preprint arXiv : 2010 . 11915 , 2020 . Amir Beck and Marc Teboulle . A fast iterative shrinkage - thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2 ( 1 ) : 183 – 202 , 2009 . Iz Beltagy , Matthew E Peters , and Arman Cohan . Longformer : The long - document transformer . arXiv preprint arXiv : 2004 . 05150 , 2020 . Michael Betancourt . A conceptual introduction to hamiltonian monte carlo . arXiv preprint arXiv : 1701 . 02434 , 2017 . Guy Blanc and Steffen Rendle . Adaptive sampled softmax with kernel based sampling . In Jennifer Dy and Andreas Krause , editors , Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 590 – 599 . PMLR , 10 – 15 Jul 2018 . URL http : / / proceedings . mlr . press / v80 / blanc18a . html . Tom Brown and et al . Language models are few - shot learners . In H . Larochelle , M . Ranzato , R . Hadsell , M . F . Balcan , and H . Lin , editors , Advances in Neural Information Processing Sys - tems , volume 33 , pages 1877 – 1901 , 2020 . URL https : / / proceedings . neurips . cc / paper / 2020 / file / 1457c0d6bfcb4967418bfb8ac142f64a - Paper . pdf . Tianqi Chen , Emily Fox , and Carlos Guestrin . Stochastic gradient hamiltonian monte carlo . In International conference on machine learning , pages 1683 – 1691 , 2014 . Rewon Child , Scott Gray , Alec Radford , and Ilya Sutskever . Generating long sequences with sparse transformers . arXiv preprint arXiv : 1904 . 10509 , 2019 . Krzysztof Marcin Choromanski and et al . Rethinking attention with performers . In Interna - tional Conference on Learning Representations , 2021 . URL https : / / openreview . net / forum ? id = Ua6zuk0WRH . Zihang Dai , Zhilin Yang , Yiming Yang , Jaime Carbonell , Quoc V Le , and Ruslan Salakhutdi - nov . Transformer - xl : Attentive language models beyond a ﬁxed - length context . arXiv preprint arXiv : 1901 . 02860 , 2019 . Mostafa Dehghani , Stephan Gouws , Oriol Vinyals , Jakob Uszkoreit , and Lukasz Kaiser . Universal transformers . arXiv preprint arXiv : 1807 . 03819 , 2018 . 17 N GUYEN B ARANIUK K IRBY O SHER W ANG Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . Bert : Pre - training of deep bidirectional transformers for language understanding . arXiv preprint arXiv : 1810 . 04805 , 2018 . Alexey Dosovitskiy , Lucas Beyer , Alexander Kolesnikov , Dirk Weissenborn , Xiaohua Zhai , Thomas Unterthiner , Mostafa Dehghani , Matthias Minderer , Georg Heigold , Sylvain Gelly , et al . An image is worth 16x16 words : Transformers for image recognition at scale . arXiv preprint arXiv : 2010 . 11929 , 2020 . Simon Duane , Anthony D Kennedy , Brian J Pendleton , and Duncan Roweth . Hybrid monte carlo . Physics Letters B , 195 ( 2 ) : 216 – 222 , 1987 . Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . Kaiming He , Haoqi Fan , Yuxin Wu , Saining Xie , and Ross Girshick . Momentum contrast for unsupervised visual representation learning . arXiv preprint arXiv : 1911 . 05722 , 2019 . Jonathan Ho , Nal Kalchbrenner , Dirk Weissenborn , and Tim Salimans . Axial attention in multidi - mensional transformers . arXiv preprint arXiv : 1912 . 12180 , 2019 . Jeremy Howard and Sebastian Ruder . Universal language model ﬁne - tuning for text classiﬁca - tion . In Proceedings of the 56th Annual Meeting of the Association for Computational Linguis - tics ( Volume 1 : Long Papers ) , pages 328 – 339 , Melbourne , Australia , July 2018 . Association for Computational Linguistics . doi : 10 . 18653 / v1 / P18 - 1031 . URL https : / / www . aclweb . org / anthology / P18 - 1031 . Cheng - Zhi Anna Huang , Ashish Vaswani , Jakob Uszkoreit , Ian Simon , Curtis Hawthorne , Noam Shazeer , Andrew M Dai , Matthew D Hoffman , Monica Dinculescu , and Douglas Eck . Music transformer : Generating music with long - term structure . In International Conference on Learn - ing Representations , 2018 . Angelos Katharopoulos , Apoorv Vyas , Nikolaos Pappas , and Franc¸ois Fleuret . Transformers are rnns : Fast autoregressive transformers with linear attention . In International Conference on Ma - chine Learning , pages 5156 – 5165 . PMLR , 2020 . Yoon Kim , Carl Denton , Luong Hoang , and Alexander M Rush . Structured attention networks . arXiv preprint arXiv : 1702 . 00887 , 2017 . Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . Nikita Kitaev , Lukasz Kaiser , and Anselm Levskaya . Reformer : The efﬁcient transformer . arXiv preprint arXiv : 2001 . 04451 , 2020 . Philipp Koehn and et al . Moses : Open source toolkit for statistical machine translation . In Pro - ceedings of the 45th Annual Meeting of the Association for Computational Linguistics Compan - ion Volume Proceedings of the Demo and Poster Sessions , pages 177 – 180 , June 2007 . URL https : / / www . aclweb . org / anthology / P07 - 2045 . 18 M OMENTUM T RANSFORMER Jason Lee , Elman Mansimov , and Kyunghyun Cho . Deterministic non - autoregressive neural se - quence modeling by iterative reﬁnement . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1173 – 1182 , October - November 2018 . doi : 10 . 18653 / v1 / D18 - 1149 . URL https : / / www . aclweb . org / anthology / D18 - 1149 . Juho Lee , Yoonho Lee , Jungtaek Kim , Adam Kosiorek , Seungjin Choi , and Yee Whye Teh . Set transformer : A framework for attention - based permutation - invariant neural networks . In Inter - national Conference on Machine Learning , pages 3744 – 3753 . PMLR , 2019 . Huan Li , Yibo Yang , Dongmin Chen , and Zhouchen Lin . Optimization algorithm inspired deep neural network structure design . In Asian Conference on Machine Learning , pages 614 – 629 . PMLR , 2018 . Peter J Liu , Mohammad Saleh , Etienne Pot , Ben Goodrich , Ryan Sepassi , Lukasz Kaiser , and Noam Shazeer . Generating wikipedia by summarizing long sequences . arXiv preprint arXiv : 1801 . 10198 , 2018 . Radford M Neal et al . MCMC using Hamiltonian dynamics . Arkaddii S Nemirovskii and Yu E Nesterov . Optimal methods of smooth convex minimization . USSR Computational Mathematics and Mathematical Physics , 25 ( 2 ) : 21 – 30 , 1985 . Yurii E Nesterov . A method for solving the convex programming problem with convergence rate o ( 1 / kˆ 2 ) . In Dokl . Akad . Nauk Sssr , volume 269 , pages 543 – 547 , 1983 . Tan Nguyen , Richard Baraniuk , Andrea Bertozzi , Stanley Osher , and Bao Wang . MomentumRNN : Integrating Momentum into Recurrent Neural Networks . In Advances in Neural Information Processing Systems ( NeurIPS 2020 ) , 2020 . Tan Minh Nguyen , Vai Suliafu , Stanley Osher , Long Chen , and Bao Wang . FMMformer : Efﬁcient and ﬂexible transformer via decomposed near - ﬁeld and far - ﬁeld attention . In A . Beygelzimer , Y . Dauphin , P . Liang , and J . Wortman Vaughan , editors , Advances in Neural Information Pro - cessing Systems , 2021 . URL https : / / openreview . net / forum ? id = YTKwvw7XI1 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei jing Zhu . Bleu : a method for automatic evaluation of machine translation . pages 311 – 318 , 2002 . Niki Parmar , Ashish Vaswani , Jakob Uszkoreit , Lukasz Kaiser , Noam Shazeer , Alexander Ku , and Dustin Tran . Image transformer . In Jennifer Dy and Andreas Krause , editors , Proceed - ings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 4055 – 4064 . PMLR , 10 – 15 Jul 2018 . URL http : / / proceedings . mlr . press / v80 / parmar18a . html . Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , et al . Pytorch : An imperative style , high - performance deep learning library . In Advances in Neural Information Processing Systems , pages 8024 – 8035 , 2019 . 19 N GUYEN B ARANIUK K IRBY O SHER W ANG Hao Peng , Nikolaos Pappas , Dani Yogatama , Roy Schwartz , Noah Smith , and Lingpeng Kong . Random feature attention . In International Conference on Learning Representations , 2021 . URL https : / / openreview . net / forum ? id = QtTKTdVrFBB . Boris T Polyak . Some methods of speeding up the convergence of iteration methods . USSR Com - putational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . Jiezhong Qiu , Hao Ma , Omer Levy , Scott Wen - tau Yih , Sinong Wang , and Jie Tang . Blockwise self - attention for long document understanding . arXiv preprint arXiv : 1911 . 02972 , 2019 . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . Language models are unsupervised multitask learners . OpenAI blog , 1 ( 8 ) : 9 , 2019 . Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang . SQuAD : 100 , 000 + questions for machine comprehension of text . In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383 – 2392 , Austin , Texas , November 2016 . Association for Computational Linguistics . doi : 10 . 18653 / v1 / D16 - 1264 . URL https : / / www . aclweb . org / anthology / D16 - 1264 . Aditya Ramesh , Mikhail Pavlov , Gabriel Goh , Scott Gray , Mark Chen , Rewon Child , Vedant Misra , Pamela Mishkin , Gretchen Krueger , Sandhini Agarwal , and Ilya Sutskever . Dall·e : Creating images from text . OpenAI blog , 2020 . Ankit Singh Rawat , Jiecao Chen , Felix Xinnan X Yu , Ananda Theertha Suresh , and Sanjiv Kumar . Sampled softmax with random fourier features . In H . Wallach , H . Larochelle , A . Beygelzimer , F . d ' Alch´e - Buc , E . Fox , and R . Garnett , editors , Advances in Neural Information Processing Sys - tems , volume 32 . Curran Associates , Inc . , 2019 . URL https : / / proceedings . neurips . cc / paper / 2019 / file / e43739bba7cdb577e9e3e4e42447f5a5 - Paper . pdf . Aurko Roy , Mohammad Saffar , Ashish Vaswani , and David Grangier . Efﬁcient content - based sparse attention with routing transformers . Transactions of the Association for Computational Linguistics , 9 : 53 – 68 , 2021 . doi : 10 . 1162 / tacl a 00353 . URL https : / / www . aclweb . org / anthology / 2021 . tacl - 1 . 4 . Tim Salimans , Andrej Karpathy , Xi Chen , and Diederik Kingma . Pixelcnn + + : Improving the pix - elcnn with discretized logistic mixture likelihood and other modiﬁcations . In International Con - ference on Learning Representations , 2017 . Imanol Schlag , Kazuki Irie , and J¨urgen Schmidhuber . Linear transformers are secretly fast weight memory systems . CoRR , abs / 2102 . 11174 , 2021 . URL https : / / arxiv . org / abs / 2102 . 11174 . Rico Sennrich , Barry Haddow , and Alexandra Birch . Neural machine translation of rare words with subword units . In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 – 1725 , August 2016 . doi : 10 . 18653 / v1 / P16 - 1162 . URL https : / / www . aclweb . org / anthology / P16 - 1162 . Zhuoran Shen , Mingyuan Zhang , Haiyu Zhao , Shuai Yi , and Hongsheng Li . Efﬁcient attention : Attention with linear complexities . In Proceedings of the IEEE / CVF Winter Conference on Ap - plications of Computer Vision , pages 3531 – 3539 , 2021 . 20 M OMENTUM T RANSFORMER David R So , Chen Liang , and Quoc V Le . The evolved transformer . arXiv preprint arXiv : 1901 . 11117 , 2019 . Kyungwoo Song , Yohan Jung , Dongjun Kim , and Il - Chul Moon . Implicit kernel attention . arXiv preprint arXiv : 2006 . 06147 , 2021 . Sainbayar Sukhbaatar , Edouard Grave , Guillaume Lample , Herve Jegou , and Armand Joulin . Aug - menting self - attention with persistent memory . arXiv preprint arXiv : 1907 . 01470 , 2019 . Tao Sun , Huaming Ling , Zuoqiang Shi , Dongsheng Li , and Bao Wang . Training deep neu - ral networks with adaptive momentum inspired by the quadratic optimization . arXiv preprint arXiv : 2110 . 09057 , 2021 . Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initializa - tion and momentum in deep learning . In International Conference on Machine Learning , pages 1139 – 1147 , 2013 . Yi Tay , Dara Bahri , Donald Metzler , Da - Cheng Juan , Zhe Zhao , and Che Zheng . Synthesizer : Rethinking self - attention in transformer models . arXiv preprint arXiv : 2005 . 00743 , 2020a . Yi Tay , Dara Bahri , Liu Yang , Donald Metzler , and Da - Cheng Juan . Sparse Sinkhorn attention . In Hal Daum´e III and Aarti Singh , editors , Proceedings of the 37th International Conference on Ma - chine Learning , volume 119 of Proceedings of Machine Learning Research , pages 9438 – 9447 . PMLR , 13 – 18 Jul 2020b . URL http : / / proceedings . mlr . press / v119 / tay20a . html . Yi Tay , Mostafa Dehghani , Dara Bahri , and Donald Metzler . Efﬁcient transformers : A survey . arXiv preprint arXiv : 2009 . 06732 , 2020c . Yi Tay , Mostafa Dehghani , Samira Abnar , Yikang Shen , Dara Bahri , Philip Pham , Jinfeng Rao , Liu Yang , Sebastian Ruder , and Donald Metzler . Long range arena : A benchmark for efﬁcient transformers . In International Conference on Learning Representations , 2021 . URL https : / / openreview . net / forum ? id = qVyeW - grC2k . Hugo Touvron , Matthieu Cord , Matthijs Douze , Francisco Massa , Alexandre Sablayrolles , and Herv´e J´egou . Training data - efﬁcient image transformers & distillation through attention . arXiv preprint arXiv : 2012 . 12877 , 2020 . Yao - Hung Hubert Tsai , Shaojie Bai , Makoto Yamada , Louis - Philippe Morency , and Ruslan Salakhutdinov . Transformer dissection : An uniﬁed understanding for transformer’s attention via the lens of kernel . arXiv preprint arXiv : 1908 . 11775 , 2019 . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . Attention is all you need . In Advances in neural infor - mation processing systems , pages 5998 – 6008 , 2017 . Apoorv Vyas , Angelos Katharopoulos , and Franc¸ois Fleuret . Fast transformers with clustered at - tention . Advances in Neural Information Processing Systems , 33 , 2020 . 21 N GUYEN B ARANIUK K IRBY O SHER W ANG Bao Wang , Hedi Xia , Tan Nguyen , and Stanley Osher . How does momentum beneﬁt deep neural networks architecture design ? a few case studies . arXiv preprint arXiv : 2110 . 07034 , 2021 . Bao Wang , Tan Nguyen , Tao Sun , Andrea L . Bertozzi , Richard G . Baraniuk , and Stanley J . Os - her . Scheduled restart momentum for accelerated stochastic gradient descent . SIAM Jour - nal on Imaging Sciences , 15 ( 2 ) : 738 – 761 , 2022 . doi : 10 . 1137 / 21M1453311 . URL https : / / doi . org / 10 . 1137 / 21M1453311 . Sinong Wang , Belinda Li , Madian Khabsa , Han Fang , and Hao Ma . Linformer : Self - attention with linear complexity . arXiv preprint arXiv : 2006 . 04768 , 2020 . Adina Williams , Nikita Nangia , and Samuel Bowman . A broad - coverage challenge corpus for sentence understanding through inference . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , Volume 1 ( Long Papers ) , pages 1112 – 1122 , June 2018 . doi : 10 . 18653 / v1 / N18 - 1101 . URL https : / / www . aclweb . org / anthology / N18 - 1101 . Hedi Xia , Vai Suliafu , Hangjie Ji , Tan Nguyen , Andrea Bertozzi , Stanley Osher , and Bao Wang . Heavy ball neural ordinary differential equations . In M . Ranzato , A . Beygelzimer , Y . Dauphin , P . S . Liang , and J . Wortman Vaughan , editors , Advances in Neural Information Processing Systems , volume 34 , pages 18646 – 18659 . Curran Asso - ciates , Inc . , 2021 . URL https : / / proceedings . neurips . cc / paper / 2021 / file / 9a86d531e19ec6f5937ad1373bb118bd - Paper . pdf . Yunyang Xiong , Zhanpeng Zeng , Rudrasis Chakraborty , Mingxing Tan , Glenn Fung , Yin Li , and Vikas Singh . Nystr¨omformer : A Nystr¨om - based Algorithm for Approximating Self - Attention . 2021 . Manzil Zaheer , Guru Guruganesh , Avinava Dubey , Joshua Ainslie , Chris Alberti , Santiago Ontanon , Philip Pham , Anirudh Ravula , Qifan Wang , Li Yang , and Amr Ahmed . Big bird : Transformers for longer sequences . arXiv preprint arXiv : 2007 . 14062 , 2021 . 22