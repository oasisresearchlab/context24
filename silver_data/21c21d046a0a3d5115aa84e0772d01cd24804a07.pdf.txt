L EVERAGING O PTIMIZATION FOR A DAPTIVE A TTACKS ON I MAGE W ATERMARKS Nils Lukas , Abdulrahman Diaa , Lucas Fenaux , Florian Kerschbaum University of Waterloo , Canada { nlukas , abdulrahman . diaa , lucas . fenaux , florian . kerschbaum } @ uwaterloo . ca A BSTRACT Untrustworthy users can misuse image generators to synthesize high - quality deepfakes and engage in online spam or disinformation campaigns . Watermarking deters misuse by marking generated content with a hidden message , enabling its detection using a secret watermarking key . A core security property of watermarking is robustness , which states that an attacker can only evade detection by substantially degrading image quality . Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm . A challenge when evaluating watermarking algorithms and their ( adaptive ) attacks is to determine whether an adaptive attack is optimal , i . e . , it is the best possible attack . We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem . The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack‚Äôs parameters . We demonstrate for Stable Diffusion models that such an attacker can break all five surveyed watermarking methods at negligible degradation in image quality . These findings emphasize the need for more rigorous robustness testing against adaptive , learnable attackers . K eywords Watermarking , Stable Diffusion , Robustness , Adaptive Attacks 1 Introduction Deepfakes are images synthesized using deep image generators that can be difficult to distinguish from real images . While deepfakes can serve many beneficial purposes if used ethically , for example , in medical imaging [ Akrout et al . , 2023 ] or education [ Peres et al . , 2023 ] they also have the potential to be misused and erode trust in digital media . Deepfakes have already been used in disinformation campaigns [ Boneh et al . , 2019 ] and social engineering attacks [ Mirsky and Lee , 2021 ] , highlighting the need for methods that control the misuse of deep image generators . Watermarking offers a solution to controlling misuse by embedding hidden messages into all generated images that are later detectable using a secret watermarking key . Images that are detected as deepfakes can be flagged by social media platforms or news agencies , which can mitigate potential harm [ Grinbaum and Adomaitis , 2022 ] . Providers of large image generators such as Google have announced the deployment of their own watermarking methods [ Gowal and Kohli , 2023 ] to enable the detection of deepfakes and promote the ethical use of their models . A core security property of watermarking is robustness , which states that an attacker can evade detection only by substantially degrading the image‚Äôs quality . While several watermarking methods have been proposed for image generators [ Wen et al . , 2023 , Zhao et al . , 2023 , Fernandez et al . , 2023 ] , none of them are certifiably robust [ Bansal et al . , 2022 ] and instead , robustness is tested empirically using a limited set of known attacks . Claimed security properties of previous watermarking methods have been broken by novel attacks [ Lukas et al . , 2022 ] , and no comprehensive method exists to validate robustness , which causes difficulty in trusting the deployment of watermarking in practice . We propose testing the robustness of watermarking by defining robustness using objective function and approaching adaptive attacks as an optimization problem . Adaptive attacks are specific to the watermarking algorithm used by the defender but have no access to the secret watermarking key . Knowledge of the watermarking algorithm enables the attacker to consider a range of surrogate keys similar to the defender‚Äôs key . This is also a challenge for optimization since the attacker only has imperfect information about the optimization problem . Adaptive attackers had previously a r X i v : 2309 . 16952v1 [ c s . CR ] 29 S e p 2023 Leveraging Optimization for Adaptive Attacks on Image Watermarks 1 . ) KeyGen 2 . ) Iteratively Optimize Attack Parameters Surrogate Key œÑ ‚Ä≤ Œ∏ ùíú ‚Ñ∞ ùíü 3 . ) KeyGen Secret Key œÑ 4 . ) Embed 5 . ) Generate Generation Embedded Message Attack Preparation Verify Loss Quality Loss 6 . ) Evasion with Optimized Parameters VeriÔ¨Åcation 7 . ) Extract Message 9 . ) Compute p - value Œ∏ ùíú ‚Ñ∞ ùíü ‚àá Œ∏ ùíú Evasion Extract from Image Extracted Message ‚ÄúA teddy bear in Washington‚Äù Generator Figure 1 : An overview of our adaptive attack pipeline . The attacker prepares their attack by generating a surrogate key and leveraging optimization to find optimal attack parameters Œ∏ A ( illustrated here as an encoder E and decoder D ) for any message . Then , the attacker generates watermarked images and applies a modification using their optimized attack to evade detection . The attacker succeeds if the verification procedure cannot detect the watermark in high - quality images . been shown to break the robustness of watermarking for image classifiers [ Lukas et al . , 2022 ] , but attacks had to be handcrafted against each watermarking method . Finding attack parameters through an optimization process can be challenging when the watermarking method is not easily optimizable , for instance , when it is not differentiable . Our attacks leverage optimization by approximating watermark verification through a differentiable process . We show that adaptive , learnable attackers , whose parameters can be optimized efficiently , can evade watermark detection for 1 billion parameter Stable Diffusion models at a negligible degradation in image quality . 2 Background Latent Diffusion Models ( LDMs ) are state - of - the - art generative models for image synthesis [ Rombach et al . , 2022 ] . Compared to Diffusion Models [ Sohl - Dickstein et al . , 2015 ] , LDMs operate in a latent space using fixed , pre - trained autoencoder consisting of an image encoder E and a decoder D . LDMs use a forward and reverse diffusion process across T steps . In the forward pass , real data point x 0 is encoded into a latent point z 0 = E ( x 0 ) and is progressively corrupted into noise via Gaussian perturbations . Specifically , q ( z t | z t ‚àí 1 ) = N (cid:16) z t ; (cid:112) 1 ‚àí Œ≤ t z t ‚àí 1 , Œ≤ t I (cid:17) , t ‚àà { 0 , 1 , . . . , T ‚àí 1 } , where Œ≤ t is the scheduled variance . In the reverse process , a neural network f Œ∏ guides the denoising , taking z t and time - step t as inputs to predict z t ‚àí 1 as f Œ∏ ( x t , t ) . The model is trained to minimize the mean squared error between the predicted and actual z t ‚àí 1 . The outcome is a latent ÀÜ z 0 resembling z 0 that can be decoded into ÀÜ x 0 = D ( z 0 ) . Synthesis in LDMs can be conditioned with textual prompts . 2 . 1 Watermarking Watermarking embeds a hidden signal into a medium , such as images , using a secret watermarking key that is later extractable using the same secret key . In the context of deep learning , watermarking can be characterized by the medium used by the defender to verify the presence of the hidden signal . White - box and black - box watermarking methods assume access to the model‚Äôs parameters or query access via an API respectively , and have been used primarily for Intellectual Property protection [ Uchida et al . , 2017 ] . No - box watermarking [ Lukas and Kerschbaum , 2023 ] assumes a more restrictive setting where the defender only knows the generated content but does not know the query used to generate the image . This type of watermarking has been used to control misuse by having the ability to detect any image generated by the provided image generator [ Gowal and Kohli , 2023 ] . Given a generator‚Äôs parameters Œ∏ G , a no - box watermarking method defines the following three procedures . 2 Leveraging Optimization for Adaptive Attacks on Image Watermarks ‚Ä¢ œÑ ‚Üê K EY G EN ( Œ∏ G ) : A randomized function to generate a watermarking key œÑ . ‚Ä¢ Œ∏ ‚àó G ‚Üê E MBED ( Œ∏ G , œÑ , m ) : Given a generator Œ∏ G , a watermarking key œÑ and a message m , return parameters Œ∏ ‚àó G of a watermarked generator that only generates watermarked images . ‚Ä¢ p ‚Üê V ERIFY ( x , œÑ , m ) : This function ( i ) extracts a message m ‚Ä≤ from x using œÑ and ( ii ) returns the p - value to reject the null hypothesis that m and m ‚Ä≤ match by random chance . A watermarking method is a set of algorithms that specify ( K EY G EN , E MBED , V ERIFY ) . A watermark is a hidden signal in an image that can be mapped to a message m using a secret key œÑ . In this paper , we denote the similarity between two messages by their L 1 - norm difference . We use more meaningful similarity measures when M allows it , such as the Bit - Error - Rate ( BER ) when the messages consist of bits . A watermark is retained in an image if the verification procedure returns p < 0 . 01 , following Wen et al . [ 2023 ] . Adi et al . [ 2018 ] specify the requirements for trustworthy watermarking , and we focus on two properties : Effectiveness and robustness . Effectiveness states that a watermarked generator has a high image quality while retaining the watermark , and robustness means that a watermark is retained in an image unless the image‚Äôs quality is substantially degraded . We refer to Lukas and Kerschbaum [ 2023 ] for security games encoding effectiveness and robustness . 2 . 2 Watermarking for Image Generators Several works propose no - box watermarking methods to prevent misuse for two types of image generators : Generative Adversarial Networks ( GANs ) [ Goodfellow et al . , 2020 ] and Latent Diffusion Models ( LDMs ) [ Rombach et al . , 2022 ] . We distinguish between post - hoc watermarking methods that apply an imperceptible modification to an image and semantic watermarks that modify the output distribution of an image generator and are truly ‚Äúinvisible " [ Wen et al . , 2023 ] . For post - hoc watermarking , traditional methods hide messages using the Discrete Wavelet Transform ( DWT ) and Discrete Wavelet Transform with Singular Value Decomposition ( DWT - SVD ) [ Cox et al . , 2007 ] and are currently used for Stable Diffusion 1 . RivaGAN [ Zhang et al . , 2019 ] watermarks by training a deep neural network adversarially to stamp a pattern on an image . Yu et al . [ 2020 , 2021 ] propose two methods that modify the generator‚Äôs training procedure but require expensive re - training from scratch . Lukas and Kerschbaum [ 2023 ] propose a watermarking method for GANs that can be embedded into a pre - trained generator . Zhao et al . [ 2023 ] propose a general method to watermark diffusion models ( WDM ) that uses a method similar to Yu et al . [ 2020 ] , which trains an autoencoder to stamp a watermark on all training data before also re - training the generator from scratch . Fernandez et al . [ 2023 ] pre - train an autoencoder to encode hidden messages into the training data and embed the watermark by fine - tuning the decoder D component of the LDM . Wen et al . [ 2023 ] are the first to propose a semantic watermarking method for LDMs they call Tree - Rings Watermarks ( TRW ) . The idea is to mark the initial noise x T with a detectable , tree - ring - like pattern m in the frequency domain before generating an image . During detection , they leverage the property of LDM‚Äôs that the diffusion process is invertible , which allows mapping an image back to its original noise . The verification extracts a message m ‚Ä≤ by spectral analysis and tests whether the same tree - ring patterns m are retained in the frequency domain of the reconstructed noise . Surveyed Watermarking Methods . In this paper , we evaluate the robustness of five watermarking methods : TRW , WDM , DWT , DWT - SVD and RivaGAN . DWT , DWT - SVD and RivaGAN are default choices when using StabilityAI‚Äôs Stable Diffusion repository and WDM and TRW are two recently proposed methods for Stable Diffusion models . However , WDM requires re - training a Stable Diffusion model from scratch , which can require 150 - 1000 GPU days [ Dhariwal and Nichol , 2021 ] and is not replicable with limited resources . For this reason , instead of using the autoencoder on the input data , we apply their autoencoder as a post - processor after generating images . 3 Threat Model We consider a provider capable of training large image generators who make their generators accessible to many users via a black - box API , such as OpenAI with DALL¬∑E . Users can query the generator by including a textual prompt that controls the content of the generated image . We consider an attack by an untrustworthy user who wants to misuse the provided generator without detection . Provider‚Äôs Capabilities and Goals ( Model Capabilities ) The provider fully controls image generation , including the ability to post - process generated images . ( Watermark Verification ) In a no - box setting , the defender must verify their 1 https : / / github . com / Stability - AI / stablediffusion 3 Leveraging Optimization for Adaptive Attacks on Image Watermarks watermark using a single generated image . The defender aims for an effective watermark that preserves generator quality while preventing the attacker from evading detection without significant image quality degradation . Attacker‚Äôs Capabilities . ( Model Capabilities ) The user has black - box query access to the provider‚Äôs watermarked model and also has white - box access to less capable , open - source surrogate generators , such as Stable Diffusion on Huggingface . We assume the surrogate model‚Äôs image quality is inferior to the provided model ; otherwise , there would be no need to use the watermarked model . Our attacker does not require access to image generators from other providers , but , of course , such access may imply access to surrogate models as our attack does require . ( Data Access ) The attacker has unrestricted access to real - world image and caption data available online , such as LAION - 5B [ Schuhmann et al . , 2022 ] . ( Resources ) Computational resources are limited , preventing the attacker from training their own image generator from scratch . ( Queries ) The provider charges the attacker per image query , limiting the number of queries they can make . The attacker can generate images either unconditionally or with textual prompts . ( Adaptive ) The attacker knows the watermarking method but lacks access to the secret watermarking key œÑ and chosen message m . Attacker‚Äôs Goal . The attacker wants to use the provided , watermarked generator to synthesize images ( i ) without a watermark that ( ii ) have a high quality . We measure quality using a perceptual similarity function Q : X √ó X ‚Üí R between the generated , watermarked image and a perturbed image after the attacker evades watermark detection . We require that the defender verifies the presence of a watermark correctly with a p - value of at least p < 0 . 01 , same as Wen et al . [ 2023 ] . 4 Conceptual Approach In this Section , we define robustness as an objective function and show that we can optimize this function efficiently using stochastic gradient descent when the watermark algorithm is known ( i . e . , in adaptive attacks ) for all five surveyed watermarking methods . Then , we propose learnable attacks and find optimal parameters through empirical loss minimization . 4 . 1 Robustness as an Objective Function As described in Section 2 , a watermarking method defines three procedures ( KeyGen , Embed , Verify ) . The provider generates a secret watermarking key œÑ ‚Üê K EY G EN ( Œ∏ G ) that allows them to watermark their generator so that all its generated images retain the watermark . To embed a watermark , the provider chooses a message ( we sample a message m ‚àº M uniformly at random ) and modifies their generator‚Äôs parameters Œ∏ ‚àó G ‚Üê E MBED ( Œ∏ G , œÑ , m ) . Any image generated by Œ∏ ‚àó G should retain the watermark . For any x ‚Üê G ENERATE ( Œ∏ ‚àó G ) we call a watermark effective if ( i ) the watermark is retained , i . e . , V ERIFY ( x , œÑ , m ) < 0 . 01 and ( ii ) the watermarked images have a high perceptual quality . The attacker generates images x ‚Üê G ENERATE ( Œ∏ ‚àó G ) and applies an image - to - image transformation , A : X ‚Üí X with parameters Œ∏ A to evade watermark detection by perturbing ÀÜ x ‚Üê A ( x ) . Finally , the defender verifies the presence of their watermark in ÀÜ x , as shown in Figure 1 . Let W ( Œ∏ G , œÑ ‚Ä≤ , m ) = E MBED ( Œ∏ G , œÑ ‚Ä≤ , m ) be the watermarked generator after embedding with key œÑ ‚Ä≤ and message m and G W = G ENERATE ( W ( Œ∏ G , œÑ ‚Ä≤ , m ) ) denotes the generation of an image using the watermarked generator parameters . For any high - quality G W , the attacker‚Äôs objective becomes : max Œ∏ A E œÑ ‚Ä≤ ‚Üê K EYGEN ( Œ∏ G ) m ‚ààM [ V ERIFY ( A ( G W ) , œÑ ‚Ä≤ , m ) + Q ( A ( G W ) , G W ) ] ( 1 ) This objective seeks to maximize ( i ) the expectation of successful watermark evasion over all potential watermarking keys œÑ ‚Ä≤ and messages m ( since the attacker does not know which key - message pair was chosen ) and ( ii ) the perceptual similarity of the images before and after the attack . Note that in this paper , we define image quality as the perceptual similarity to the watermarked image before the attack . There are two obstacles for an attacker to optimize this objective : ( 1 ) The attacker has imperfect information about the optimization problem and must substitute the defender‚Äôs image generator with a less capable , open - source surrogate generator . When K EY G EN depends on Œ∏ G , then the distribution of keys differs , and the attack‚Äôs effectiveness must transfer to keys generated using Œ∏ G . ( 2 ) The optimization problem might be hard to approximate , even when perfect information is available , e . g . , when the watermark verification procedure is not differentiable . 4 . 2 Making Watermarking Keys Differentiable We overcome the two aforementioned limitations by ( 1 ) giving the attacker access to a similar ( but less capable ) surrogate generator ÀÜ Œ∏ G , enabling them to generate surrogate watermarking keys , and ( 2 ) by creating a method GK EY G EN ( ÀÜ Œ∏ G ) that 4 Leveraging Optimization for Adaptive Attacks on Image Watermarks Algorithm 2 Adversarial Noising Require : surrogate ÀÜ Œ∏ G , budget œµ , image x 1 : Œ∏ A ‚Üê 0 ‚ñ∑ adversarial perturbation 2 : Œ∏ D ‚Üê GK EY G EN ( ÀÜ Œ∏ G ) 3 : m ‚Üê E XTRACT ( x ; Œ∏ D ) 4 : for j ‚Üê 1 to N do 5 : m ‚Ä≤ ‚Üê E XTRACT ( x + Œ∏ A , Œ∏ D ) 6 : g Œ∏ A ‚Üê ‚àí‚àá Œ∏ A | | m ‚àí m ‚Ä≤ | | 1 7 : Œ∏ A ‚Üê P œµ ( Œ∏ A ‚àí Adam ( Œ∏ A , g Œ∏ A ) ) return x + Œ∏ A Algorithm 3 Adversarial Compression Require : surrogate ÀÜ Œ∏ G , strength Œ± , image x 1 : Œ∏ A ‚Üê [ Œ∏ E , Œ∏ D ] ‚ñ∑ Compressor parameters 2 : Œ∏ D ‚Üê GK EY G EN ( ÀÜ Œ∏ G ) ‚ñ∑ surrogate key 3 : for j ‚Üê 1 to N do 4 : m ‚àº M 5 : ÀÜ Œ∏ ‚àó G ‚Üê E MBED ( ÀÜ Œ∏ G , Œ∏ D , m ) 6 : x ‚Üê G ENERATE ( ÀÜ Œ∏ ‚àó G ) 7 : x ‚Ä≤ ‚Üê D ( E ( x ; Œ∏ A ) ) ‚ñ∑ compression 8 : m ‚Ä≤ ‚Üê E XTRACT ( x ‚Ä≤ , Œ∏ D ) 9 : g Œ∏ A ‚Üê ‚àá Œ¥ ( L LPIPS ( x ‚Ä≤ , x ) ‚àí Œ± | | m ‚àí m ‚Ä≤ | | 1 ) 10 : Œ∏ A ‚Üê Œ∏ A ‚àí Adam ( Œ∏ A , g Œ∏ A ) return D ( E ( x ; Œ∏ A ) ) creates a surrogate watermarking key Œ∏ K through which we can backpropagate gradients . A simple but computationally expensive method of creating differentiable keys Œ∏ D is using Algorithm 1 to train a watermark extraction neural network with parameters Œ∏ D to predict the message m from an image . Algorithm 1 GK EY G EN : A Simple Method to Generate Differentiable Keys Require : Surrogate generator ÀÜ Œ∏ G , Watermarking method ( KeyGen , Embed , Verify ) , N steps 1 : œÑ ‚Üê K EYGEN ( ÀÜ Œ∏ G ) ‚ñ∑ The non - differentiable surrogate key 2 : for j ‚Üê 1 to N do 3 : m ‚àº M ‚ñ∑ Sample a random message 4 : ÀÜ Œ∏ ‚àó G ‚Üê E MBED ( ÀÜ Œ∏ G , œÑ , m ) ‚ñ∑ Embed the watermark 5 : x ‚Üê G ENERATE ( ÀÜ Œ∏ ‚àó G ) 6 : m ‚Ä≤ ‚Üê E XTRACT ( x ; Œ∏ D ) 7 : g Œ∏ D ‚Üê ‚àá Œ∏ D | | m ‚àí m ‚Ä≤ | | 1 ‚ñ∑ Compute gradients using distance between messages 8 : Œ∏ D ‚Üê Œ∏ D ‚àí Adam ( Œ∏ D , g Œ∏ D ) 9 : return Œ∏ D ‚ñ∑ The differentiable surrogate key Our adaptive attacker must invoke Algorithm 1 for the non - differentiable watermarks DCT and DCT - SVD [ Cox et al . , 2007 ] . Fortunately for the attacker , the remaining three watermarking methods TRW [ Wen et al . , 2023 ] , WDM [ Zhao et al . , 2023 ] and RivaGAN Zhang et al . [ 2019 ] do not require invoking GK EY G EN because their watermarking keys are already differentiable , reducing computational resources required for the attacker . 4 . 3 Leveraging Optimization against Watermarks Equation ( 1 ) requires finding attack parameters Œ∏ A against any watermarking key œÑ ‚Ä≤ ‚Üê K EY G EN ( Œ∏ G ) , which can be computationally expensive if the attacker has to invocate GK EY G EN many times . We find empirically that generating many keys is unnecessary , and the attacker can find effective attacks using only a single surrogate watermarking key Œ∏ D ‚Üê GK EY G EN ( ÀÜ Œ∏ G ) . We propose two learnable attacks A 1 , A 2 whose parameters Œ∏ A 1 , Œ∏ A 2 can be optimized efficiently . The first attack , called Adversarial Noising finds adversarial examples given an image x using the surrogate key as a reward model . The second attack , called Adversarial Compression , first fine - tunes the parameters of a pre - trained autoencoder in a preparation stage and uses the optimized parameters during an attack . The availability of a pre - trained autoencoder is a realistic assumption if the attacker has access to a surrogate Stable Diffusion generator , as the autoencoder is a detachable component of any Stable Diffusion generator . Access to a surrogate generator implies the availability of a pre - trained autoencoder at no additional cost in computational resources for the attacker . Adversarial Noising . Algorithm 2 shows the pseudocode of our adversarial noising attack . Given a surrogate generator ÀÜ Œ∏ G , a budget œµ ‚àà R + for the maximum allowed noise perturbation , and a watermarked image x generated using the provider‚Äôs watermarked model , the attacker wants to compute a perturbation within an œµ - ball of the L ‚àû norm that evades watermark detection . The attacker generates a local surrogate watermarking key ( line 2 ) and extracts a message 5 Leveraging Optimization for Adaptive Attacks on Image Watermarks m from x ( line 3 ) . Then , the attacker computes the adversarial perturbation by maximizing the distance to the initially extracted message m while clipping the perturbation into an œµ - ball using P œµ ( line 7 ) . Adversarial Compression . Algorithm 3 shows the pseudocode of our adversarial compression attack . After generating a surrogate watermarking key ( line 2 ) , the attacker generates images containing a random message ( lines 4 - 6 ) and uses their encoder - decoder pair to compress the images ( line 7 ) . The attacker iteratively updates their model‚Äôs parameters by ( i ) minimizing a quality loss , which we set to the LPIPS metric [ Zhang et al . , 2018 ] , and ( ii ) maximizing the distance between the extracted and embedded messages ( line 9 ) . The output Œ∏ A of the optimization loop between lines 3 and 10 only needs to be run once , and the weights Œ∏ A can be re - used in subsequent attacks . We highlight that the attacker optimizes an approximation of Equation ( 1 ) since they only have access to a surrogate generator ÀÜ Œ∏ G , but not the provider‚Äôs generator Œ∏ G . This may lead to a generalization gap of the attack at inference time . Even if an attacker can find optimal attack parameters Œ∏ A that optimizes Equation ( 1 ) using ÀÜ Œ∏ G , the attacker cannot test whether their attack remains effective when the defender uses a different model Œ∏ G to generate watermarking keys . 5 Experiments Image Generators . We experiment with Stable Diffusion , an open - source , state - of - the - art latent diffusion model . The defender deploys a Stable Diffusion - v2 . 0 model 2 trained for 1 . 4m steps in total on a subset of LAION - 5B [ Schuhmann et al . , 2022 ] . The attacker uses a less capable Stable Diffusion - v1 . 1 3 checkpoint , trained for 431k steps in total on LAION - 2B and LAION - HD . All experiments were conducted on NVIDIA A100 GPUs . Images are generated using a DPM solver [ Lu et al . , 2022 ] with 20 inference steps and a default guidance scale of 7 . 5 . We create three different watermarked generators for each surveyed watermarking method by randomly sampling a watermarking key œÑ ‚Üê K EY G EN ( Œ∏ G ) and a message m ‚àº M , used to embed a watermark . Appendix A . 1 contains descriptions of the watermarking keys . All reported values represent the mean value over three independently generated secret keys . Quantitative Analysis . Similar to Wen et al . [ 2023 ] , we report the True Positive Rate when the False Positive Rate is fixed to 1 % , called the TPR @ 1 % FPR . Appendix A . 2 describes statistical tests used in the verification procedure of each watermarking method to derive p - values . We report the Fr√©chet Inception Distance ( FID ) [ Heusel et al . , 2017 ] , which measures the similarity between real and generated images . Additionally , we report the CLIP score [ Radford et al . , 2021 ] that measures the similarity of a prompt to an image . We generate 1k images to evaluate TPR @ 1 % FPR and 5k images to evaluate FID and CLIP score on the training dataset of MS - COCO - 2017 [ Lin et al . , 2014 ] . 5 . 1 Evaluating Robustness Figure 2 shows a scatter plot of the effectiveness of our attacks against all surveyed watermarking methods . We evaluate adaptive and non - adaptive attacks . Similar to Wen et al . [ 2023 ] , for the non - adpative attacks we use Blurring , JPEG Compression , Cropping , Gaussian noise , Jittering , Quantization , and Rotation , but find these attacks to be ineffective at removing the watermark . Figure 2 highlights Pareto optimal attacks for pairs of ( i ) watermark detection accuracies and ( ii ) perceptual distances . We find that only adaptive attacks evade watermark detection and preserve image quality . Table 1 summarizes the best attacks from Figure 2 with a lowest acceptable detection accuracy of 10 % . When multiple attacks achieve a detection accuracy lower than 10 % , we pick the attack with the lowest perceptual distance to the watermarked image . We observe that adversarial compression is an effective attack against all watermarking methods . TRW is also evaded by adversarial compression , but adversarial noising at œµ = 2 / 255 preserves a higher image quality . TRW WDM DWT DWT - SVD RivaGAN Best Attack Adv . Noising Compression Compression Compression Compression Parameters œµ = 2 / 255 r = 1 r = 1 r = 1 r = 1 LPIPS ‚áì 3 . 2e - 2 5 . 6e - 2 7 . 7e - 2 7 . 5e - 2 7 . 3e - 2 Accuracy ‚áì 5 . 2 % 2 . 0 % 0 . 8 % 1 . 9 % 6 . 3 % Table 1 : A summary of Pareto optimal attacks with detection accuracies less than 10 % . We list the attack‚Äôs name and parameters , the perceptual distance before and after evasion , and the accuracy ( TPR @ 1 % FPR ) . œµ is the maximal perturbation in the L ‚àû norm and r is the number of compressions . 2 https : / / huggingface . co / stabilityai / stable - diffusion - 2 - base 3 https : / / huggingface . co / CompVis / stable - diffusion - v1 - 1 6 Leveraging Optimization for Adaptive Attacks on Image Watermarks 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 Perceptual Distance ( LPIPS ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 A cc u r a c y ( T P R @ 1 % F P R ) Watermark Accuracy versus Perceptual Distance No Watermark Adaptive Attacks Non - Adaptive No Attack WDM TRW RivaGAN DWT DWT - SVD Figure 2 : The effectiveness of our attacks against all watermarks . We highlight the Pareto front for each watermarking method by dashed lines and indicate adaptive / non - adaptive attacks by colors . TRW Adversarial Compression Adversarial Noise WDM œµ = 8 / 255 , p = 0 . 13 125 œµ = 2 / 255 , p = 0 . 09 r = 1 , p = 0 . 69 r = 1 , p = 0 . 79 DWT r = 1 , p = 0 . 30 œµ = 6 / 255 , p = 0 . 18 DWT - SVD RivaGAN œµ = 4 / 255 , p = 0 . 29 r = 1 , p = 1 . 00 N o W a t e r m a r k œµ = 8 / 255 , p = 0 . 05 r = 3 , p = 0 . 89 Figure 3 : A visual analysis of two adaptive attacks . The left image shows the unwatermarked output , including a high - contrast cutout of the top left corner of the image to visualize noise artifacts . On the right are images after evasion with adversarial noising ( top ) and adversarial compression ( bottom ) . 5 . 2 Image Quality after an Attack Figure 3 shows the perceptual quality after using our adaptive attacks . We show a cutout of the top left image patch with high contrasts on the bottom right to visualize noise artifacts potentially introduced by our attacks . We observe that , unlike adversarial noising , the compression attack introduces no new visible artifacts . Appendix A . 3 displays more visualizations on the perceptual impact of our attacks on the image quality . TRW WDM DWT DWT - SVD RivaGAN FID CLIP FID CLIP FID CLIP FID CLIP FID CLIP No WM 23 . 32 31 . 76 23 . 48 31 . 77 23 . 48 31 . 77 23 . 48 31 . 77 23 . 48 31 . 77 WM 24 . 19 31 . 78 23 . 43 31 . 72 23 . 16 32 . 11 23 . 10 32 . 15 22 . 96 31 . 84 A - Noise 23 . 67 32 . 15 N / A N / A 23 . 55 32 . 46 22 . 89 32 . 50 N / A N / A A - Comp 24 . 36 31 . 87 23 . 27 32 . 01 23 . 16 32 . 17 23 . 06 31 . 92 23 . 25 31 . 86 Table 2 : Quality metrics before and after watermark evasion . FID ‚áì represents the Fr√©chet Inception Distance , and CLIP ‚áë represents the CLIP score , computed on 5k images from MS - COCO - 2017 . N / A means the attack could not evade watermark detection , and we do not report quality measures . Table 2 shows the FID and CLIP score of the watermarked images and the images after using adversarial noising and adversarial compression . We calculate the quality using the best attack configuration from Figure 2 when the detection 7 Leveraging Optimization for Adaptive Attacks on Image Watermarks 1 / 255 2 / 255 6 / 255 10 / 255 Epsilon Budget ( L ) 0 20 40 60 80 100 A cc u r a c y ( T P R @ 1 % F P R ) Ablation over the Epsilon Budget for Adversarial Noising TRW WDM DWT DWT - SVD RivaGAN No Optimization 1 3 Number of Repetitions ( r ) 0 20 40 60 80 100 A cc u r a c y ( T P R @ 1 % F P R ) Ablation over the Repetitions for Adversarial Compression TRW WDM DWT DWT - SVD RivaGAN Figure 4 : Ablation studies over ( left ) the maximum perturbation budget œµ in L ‚àû for adversarial noising and ( right ) the number of adversarial compressions against each watermarking method . ‚ÄúNo Optimizations " means we did not optimize the parameters Œ∏ A of the attack . accuracy is less than 10 % . Adversarial Noising is ineffective at removing WDM and RivaGAN for œµ ‚â§ 10 / 255 , which we denote by N / A in Table 2 . 5 . 3 Ablation Study Figure 4 shows ablation studies for our adaptive attacks over the ( i ) maximum perturbation budget and ( ii ) the number of compressions applied during the attack . TRW and DWT - SVD are highly vulnerable to adversarial noising , whereas RivaGAN and WDM are substantially more robust to these types of attacks . We believe this is because keys generated by RivaGAN and WDM are sufficiently randomized , which makes our attack ( that uses only a single surrogate key ) less effective unless the surrogate key uses similar channels as the secret key to hide the watermark . Figure 4 shows that adversarial compression without optimization of the parameters Œ∏ A is ineffective at evading watermark detection against all methods except DWT . After optimization , adversarial compression evades detection from all watermarking methods with only a single compression . 6 Discussion & Related Work Attack Scalability . The presented findings clearly indicate that even with a less capable surrogate generator , an adaptive attacker can remove all surveyed watermarks with minimal quality degradation . Our attackers generate a single surrogate key and are able to evade watermark verification , which indicates a design flaw since the key seems to have little impact . An interesting question emerging from our study relates to the maximum difference between the watermarked and surrogate generators for the attacks to remain effective . We used two public checkpoints with the largest reported quality difference : Stable Diffusion v1 . 1 and v2 for the attacker and defender respectively . Types of Learnable Attacks . Measuring the robustness against different types of learnable attacks is crucial in assessing the trustworthiness of watermarking . We explored ( i ) Adversarial Examples , which rely solely on the surrogate key , and ( ii ) Adversarial Compression , which additionally requires the availability of a pre - trained autoencoder . We believe this requirement is satisfied in practice , given that ( i ) training autoencoders is computationally less demanding than training Stable Diffusion , and many pre - trained autoencoders have already been made publicly available [ Podell et al . , 2023 ] . Although autoencoders enhance an attacker‚Äôs ability to modify images , our study did not extend to other learnable attacks such as inpainting [ Rombach et al . , 2022 ] or more potent image editing methods [ Brooks et al . , 2023 ] which could further enhance an attack‚Äôs effectiveness . Enhancing Robustness using Adaptive and Learnable Attacks . Relying on non - adaptive attacks for evaluating a watermark‚Äôs robustness is inadequate as it underestimates the attacker‚Äôs capabilities . To claim robustness , the defender could ( i ) provide a certification of robustness [ Bansal et al . , 2022 ] , or ( ii ) showcase empirically that their watermark withstands strong attacks . The issue is that we lack strong attackers . Although Lukas et al . [ 2022 ] demonstrated that adaptive attackers can break watermarks for image classifiers , their attacks were handcrafted and did not scale . Instead , we propose a better method of empirically testing robustness by proposing adaptive learnable attackers that require only the specification of a type of learnable attack , followed by an optimization procedure to find parameters that minimize an objective function . We believe that any watermarking method proposed in the future should evaluate robustness using our attacks and expect that future watermarking methods can enhance their robustness by incorporating our attacks . Limitations . Our attacks are based on the availability of the watermarking algorithm and an open - source surrogate generator to replicate keys . While providers like Stable Diffusion openly share their models , and replicas of OpenAI‚Äôs 8 Leveraging Optimization for Adaptive Attacks on Image Watermarks DALL¬∑E models are publicly available [ Dayma et al . , 2021 ] , not all providers release information about their models . To the best of our knowledge , Google has not released their generators [ Saharia et al . , 2022 ] , but efforts to replicate are ongoing 4 . Providers like Midjourney 5 , who keep their image generation algorithms undisclosed , prevent adaptive attackers altogether but may be vulnerable to these attacks by anyone to whom this information is released . Ethical Considerations . The attacks we provide target academic systems and the engineering efforts to attack real systems are substantial . We make it harder by not releasing our code publicly . We will , however , release our code including pre - trained checkpoints upon carefully considering each request . Currently , there are no known security impacts of our attacks since users cannot yet rely on the provider‚Äôs use of watermarking . The use of watermarking is experimental and occurs at the provider‚Äôs own risk , and our research aims to improve the trustworthiness of image watermarking by evaluating it more comprehensively . 6 . 1 Related Work Jiang et al . [ 2023 ] use adversarial noise to evade watermark detection , but they require access to the secret watermarking key . Our attacks require no access to the secret watermarking key . Peng et al . [ 2023 ] , Cui et al . [ 2023 ] propose black - box watermarking methods that protect the Intellectual Property of Diffusion Models . We focus on no - box verifiable watermarking methods that control misuse . Lukas and Kerschbaum [ 2023 ] , Yu et al . [ 2020 , 2021 ] propose watermarking methods but only evaluate GANs . We focus on watermarking methods for pre - trained Stable Diffusion models with much higher output diversity and image quality [ Dhariwal and Nichol , 2021 ] . 7 Conclusion We propose testing the robustness of watermarking through adaptive , learnable attacks . Our empirical analysis shows that such attackers can evade watermark detection at negligible degradation in image quality against all five surveyed image watermarks . We encourage using these adaptive attacks to more comprehensively test the robustness of watermarking methods in the future . References Yossi Adi , Carsten Baum , Moustapha Cisse , Benny Pinkas , and Joseph Keshet . Turning your weakness into a strength : Watermarking deep neural networks by backdooring . In 27th USENIX Security Symposium ( USENIX Security 18 ) , pages 1615 ‚Äì 1631 , 2018 . Mohamed Akrout , B√°lint Gyepesi , P√©ter Holl√≥ , Adrienn Po√≥r , Bl√°ga KincsÀùo , Stephen Solis , Katrina Cirone , Jeremy Kawahara , Dekker Slade , Latif Abid , et al . Diffusion - based data augmentation for skin disease classification : Impact across original medical datasets to fully synthetic images . arXiv preprint arXiv : 2301 . 04802 , 2023 . Arpit Bansal , Ping - yeh Chiang , Michael J Curry , Rajiv Jain , Curtis Wigington , Varun Manjunatha , John P Dickerson , and Tom Goldstein . Certified neural network watermarks with randomized smoothing . In International Conference on Machine Learning , pages 1450 ‚Äì 1465 . PMLR , 2022 . Dan Boneh , Andrew J Grotto , Patrick McDaniel , and Nicolas Papernot . How relevant is the turing test in the age of sophisbots ? IEEE Security & Privacy , 17 ( 6 ) : 64 ‚Äì 71 , 2019 . Tim Brooks , Aleksander Holynski , and Alexei A Efros . Instructpix2pix : Learning to follow image editing instructions . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 18392 ‚Äì 18402 , 2023 . Ingemar Cox , Matthew Miller , Jeffrey Bloom , Jessica Fridrich , and Ton Kalker . Digital watermarking and steganogra - phy . Morgan kaufmann , 2007 . Yingqian Cui , Jie Ren , Han Xu , Pengfei He , Hui Liu , Lichao Sun , and Jiliang Tang . Diffusionshield : A watermark for copyright protection against generative diffusion models . arXiv preprint arXiv : 2306 . 04642 , 2023 . Boris Dayma , Suraj Patil , Pedro Cuenca , Khalid Saifullah , Tanishq Abraham , Ph√∫c Le Khac , Luke Melas , and Ritobrata Ghosh . Dall e mini , 7 2021 . URL https : / / github . com / borisdayma / dalle - mini . Prafulla Dhariwal and Alexander Nichol . Diffusion models beat gans on image synthesis . Advances in neural information processing systems , 34 : 8780 ‚Äì 8794 , 2021 . 4 https : / / github . com / lucidrains / imagen - pytorch 5 www . midjourney . com / 9 Leveraging Optimization for Adaptive Attacks on Image Watermarks Pierre Fernandez , Guillaume Couairon , Herv√© J√©gou , Matthijs Douze , and Teddy Furon . The stable signature : Rooting watermarks in latent diffusion models . arXiv preprint arXiv : 2303 . 15435 , 2023 . Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . Generative adversarial networks . Communications of the ACM , 63 ( 11 ) : 139 ‚Äì 144 , 2020 . Sven Gowal and Pushmeet Kohli . Identifying ai - generated images with synthid , 2023 . URL https : / / www . deepmind . com / blog / identifying - ai - generated - images - with - synthid . Accessed : 2023 - 09 - 23 . Alexei Grinbaum and Laurynas Adomaitis . The ethical need for watermarks in machine - generated language . arXiv preprint arXiv : 2209 . 03118 , 2022 . Martin Heusel , Hubert Ramsauer , Thomas Unterthiner , Bernhard Nessler , and Sepp Hochreiter . Gans trained by a two time - scale update rule converge to a local nash equilibrium . Advances in neural information processing systems , 30 , 2017 . Zhengyuan Jiang , Jinghuai Zhang , and Neil Zhenqiang Gong . Evading watermark based detection of ai - generated content . arXiv preprint arXiv : 2305 . 03807 , 2023 . Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Doll√°r , and C Lawrence Zitnick . Microsoft coco : Common objects in context . In Computer Vision ‚Äì ECCV 2014 : 13th European Conference , Zurich , Switzerland , September 6 - 12 , 2014 , Proceedings , Part V 13 , pages 740 ‚Äì 755 . Springer , 2014 . Cheng Lu , Yuhao Zhou , Fan Bao , Jianfei Chen , Chongxuan Li , and Jun Zhu . Dpm - solver + + : Fast solver for guided sampling of diffusion probabilistic models . arXiv preprint arXiv : 2211 . 01095 , 2022 . Nils Lukas and Florian Kerschbaum . Ptw : Pivotal tuning watermarking for pre - trained image generators . 32nd USENIX Security Symposium , 2023 . Nils Lukas , Edward Jiang , Xinda Li , and Florian Kerschbaum . Sok : How robust is image classification deep neural network watermarking ? In 2022 IEEE Symposium on Security and Privacy ( SP ) , pages 787 ‚Äì 804 . IEEE , 2022 . Yisroel Mirsky and Wenke Lee . The creation and detection of deepfakes : A survey . ACM Computing Surveys ( CSUR ) , 54 ( 1 ) : 1 ‚Äì 41 , 2021 . Sen Peng , Yufei Chen , Cong Wang , and Xiaohua Jia . Protecting the intellectual property of diffusion models by the watermark diffusion process . arXiv preprint arXiv : 2306 . 03436 , 2023 . Renana Peres , Martin Schreier , David Schweidel , and Alina Sorescu . On chatgpt and beyond : How generative artificial intelligence may affect research , teaching , and practice . International Journal of Research in Marketing , 2023 . Dustin Podell , Zion English , Kyle Lacey , Andreas Blattmann , Tim Dockhorn , Jonas M√ºller , Joe Penna , and Robin Rom - bach . Sdxl : improving latent diffusion models for high - resolution image synthesis . arXiv preprint arXiv : 2307 . 01952 , 2023 . Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , et al . Learning transferable visual models from natural language supervision . In International conference on machine learning , pages 8748 ‚Äì 8763 . PMLR , 2021 . Robin Rombach , Andreas Blattmann , Dominik Lorenz , Patrick Esser , and Bj√∂rn Ommer . High - resolution image synthesis with latent diffusion models . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pages 10684 ‚Äì 10695 , 2022 . Chitwan Saharia , William Chan , Saurabh Saxena , Lala Li , Jay Whang , Emily L Denton , Kamyar Ghasemipour , Raphael Gontijo Lopes , Burcu Karagol Ayan , Tim Salimans , et al . Photorealistic text - to - image diffusion models with deep language understanding . Advances in Neural Information Processing Systems , 35 : 36479 ‚Äì 36494 , 2022 . Christoph Schuhmann , Romain Beaumont , Richard Vencu , Cade Gordon , Ross Wightman , Mehdi Cherti , Theo Coombes , Aarush Katta , Clayton Mullis , Mitchell Wortsman , et al . Laion - 5b : An open large - scale dataset for training next generation image - text models . Advances in Neural Information Processing Systems , 35 : 25278 ‚Äì 25294 , 2022 . Jascha Sohl - Dickstein , Eric Weiss , Niru Maheswaranathan , and Surya Ganguli . Deep unsupervised learning using nonequilibrium thermodynamics . In International conference on machine learning , pages 2256 ‚Äì 2265 . PMLR , 2015 . Yusuke Uchida , Yuki Nagai , Shigeyuki Sakazawa , and Shin‚Äôichi Satoh . Embedding watermarks into deep neural networks . In Proceedings of the 2017 ACM on international conference on multimedia retrieval , pages 269 ‚Äì 277 , 2017 . Yuxin Wen , John Kirchenbauer , Jonas Geiping , and Tom Goldstein . Tree - ring watermarks : Fingerprints for diffusion images that are invisible and robust . arXiv preprint arXiv : 2305 . 20030 , 2023 . Ning Yu , Vladislav Skripniuk , Dingfan Chen , Larry Davis , and Mario Fritz . Responsible disclosure of generative models using scalable fingerprinting . arXiv preprint arXiv : 2012 . 08726 , 2020 . 10 Leveraging Optimization for Adaptive Attacks on Image Watermarks Ning Yu , Vladislav Skripniuk , Sahar Abdelnabi , and Mario Fritz . Artificial fingerprinting for generative models : Rooting deepfake attribution in training data . In Proceedings of the IEEE / CVF International conference on computer vision , pages 14448 ‚Äì 14457 , 2021 . Kevin Alex Zhang , Lei Xu , Alfredo Cuesta - Infante , and Kalyan Veeramachaneni . Robust invisible video watermarking with attention . arXiv preprint arXiv : 1909 . 01285 , 2019 . Richard Zhang , Phillip Isola , Alexei A Efros , Eli Shechtman , and Oliver Wang . The unreasonable effectiveness of deep features as a perceptual metric . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 586 ‚Äì 595 , 2018 . Yunqing Zhao , Tianyu Pang , Chao Du , Xiao Yang , Ngai - Man Cheung , and Min Lin . A recipe for watermarking diffusion models . arXiv preprint arXiv : 2303 . 10137 , 2023 . A Appendix A . 1 Parameters for Watermarking Methods Tree Ring Watermark ( TRW ) [ Wen et al . , 2023 ] : We evaluate the Tree - Ring Rings method , which the authors state ‚Äúdelivers the best average performance while offering the model owner the flexibility of multiple different random keys " . Using the author‚Äôs implementation 6 , we generate and verify watermarks using 20 inference steps , where we use no knowledge of the prompt during verification and keep the remaining default parameters chosen by the authors . Watermark Diffusion Model ( WDM ) [ Zhao et al . , 2023 ] : As stated in the paper , instead of stamping the model‚Äôs training data to embed a watermark , we apply the pre - trained encoder to a generated image as a post - processing step . We choose messages with n = 40 bits and use the encoder architecture proposed by [ Yu et al . , 2021 ] , followed by a ResNet - 50 decoder . Each call to K EY G EN ( ÀÜ Œ∏ G ) , where ÀÜ Œ∏ G is the surrogate generator , trains a new autoencoder from scratch . DWT , DWT - SVD [ Cox et al . , 2007 ] and RivaGAN [ Zhang et al . , 2019 ] . We use 32 - bit messages and keep the default parameters set in the implementation used by the Stable Diffusion models 7 . A . 2 Statistical Tests Matching Bits . WDM [ Zhao et al . , 2023 ] , DWT , DWT - SVD [ Cox et al . , 2007 ] and RivaGAN [ Zhang et al . , 2019 ] encode messages m ‚àà M by bits and our goal is to verify whether message m ‚àà M is present in x ‚àà X using key œÑ . We extract m ‚Ä≤ from x and want to reject the following null hypothesis . H 0 : m and m ‚Ä≤ match by random chance . For a given pair of bit - strings of length n , if we denote the number of matching bits as k , the expected number of matches by random chance follows a binomial distribution with parameters n and expected value 0 . 5 . The p - value for observing at least k matches is given by : p = 1 ‚àí CDF ( k ‚àí 1 ; n , 0 . 5 ) Where CDF represents the cumulative distribution function of the binomial distribution . Matching Latents . TRW [ Wen et al . , 2023 ] leverages the forward diffusion process of the diffusion model to reverse an image x to its initial noise representation x T . This transformation is represented by m ‚Ä≤ = F ( x T ) , where F denotes a Fourier transform . The authors find that reversed real images and their representations in the Fourier domain are expected to follow a Gaussian distribution . The watermark verification process aims to reject the following null hypothesis : H 0 : y originates from a Gaussian distribution N ( 0 , œÉ 2 IC ) Here , y is a subset of m ‚Ä≤ based on a watermarking mask chosen by the provider , which determines the relevant coefficients . The test statistic , Œ∑ , denotes the normalized sum - of - squares difference between the original embedded message m and the extracted message m ‚Ä≤ , which can be complex - valued due to the Fourier transform . Specifically , Œ∑ = 1 œÉ 2 (cid:88) i | m i ‚àí m ‚Ä≤ i | 2 6 https : / / github . com / YuxinWenRick / tree - ring - watermark 7 https : / / github . com / ShieldMnt / invisible - watermark 11 Leveraging Optimization for Adaptive Attacks on Image Watermarks P - value = 0 . 28 Watermarked No Watermark P - value = 1 . 77e - 09 Attacked P - value = 0 . 52 TRW : ‚ÄúCars are parked on the street near an old building‚Äù WDM : ‚ÄúA bench at the beach next to the sea‚Äù P - value = 3 . 73 - 11 P - value = 0 . 08 P - value = 0 . 13 DWT : ‚ÄúA blue train on some train tracks about to go under a bridge‚Äù P - value = 2 . 33 - 10 P - value = 0 . 57 P - value = 0 . 30 DWT - SVD : ‚ÄúA white horse standing on top of a dirt Ô¨Åeld . ‚Äù P - value = 2 . 33 - 10 P - value = 0 . 05 P - value = 0 . 30 RivaGAN : ‚ÄúDonuts with frosting and glazed toppings sit on table next to co Ô¨Ä ee maker‚Äù P - value = 2 . 33 - 10 P - value = 0 . 43 P - value = 0 . 30 P - value = 0 . 28 Watermarked No Watermark P - value = 1 . 77e - 09 Attacked P - value = 0 . 52 TRW : ‚ÄúCars are parked on the street near an old building‚Äù WDM : ‚ÄúA bench at the beach next to the sea‚Äù P - value = 3 . 73 - 11 P - value = 0 . 08 P - value = 0 . 13 DWT : ‚ÄúA blue train on some train tracks about to go under a bridge‚Äù P - value = 2 . 33 - 10 P - value = 0 . 57 P - value = 0 . 30 DWT - SVD : ‚ÄúA white horse standing on top of a dirt Ô¨Åeld . ‚Äù P - value = 2 . 33 - 10 P - value = 0 . 05 P - value = 0 . 30 RivaGAN : ‚ÄúDonuts with frosting and glazed toppings sit on table next to co Ô¨Ä ee maker‚Äù P - value = 2 . 33 - 10 P - value = 0 . 43 P - value = 0 . 30 P - value = 0 . 28 Watermarked No Watermark P - value = 1 . 77e - 09 Attacked P - value = 0 . 52 TRW : ‚ÄúCars are parked on the street near an old building‚Äù WDM : ‚ÄúA bench at the beach next to the sea‚Äù P - value = 3 . 73 - 11 P - value = 0 . 08 P - value = 0 . 13 DWT : ‚ÄúA blue train on some train tracks about to go under a bridge‚Äù P - value = 2 . 33 - 10 P - value = 0 . 57 P - value = 0 . 30 DWT - SVD : ‚ÄúA white horse standing on top of a dirt Ô¨Åeld . ‚Äù P - value = 2 . 33 - 10 P - value = 0 . 05 P - value = 0 . 30 RivaGAN : ‚ÄúDonuts with frosting and glazed toppings sit on table next to co Ô¨Ä ee maker‚Äù P - value = 2 . 33 - 10 P - value = 0 . 43 P - value = 0 . 30 P - value = 0 . 28 Watermarked No Watermark P - value = 1 . 77e - 09 Attacked P - value = 0 . 52 TRW : ‚ÄúCars are parked on the street near an old building‚Äù WDM : ‚ÄúA bench at the beach next to the sea‚Äù P - value = 3 . 73 - 11 P - value = 0 . 08 P - value = 0 . 13 DWT : ‚ÄúA blue train on some train tracks about to go under a bridge‚Äù P - value = 2 . 33 - 10 P - value = 0 . 57 P - value = 0 . 30 DWT - SVD : ‚ÄúA white horse standing on top of a dirt Ô¨Åeld . ‚Äù P - value = 2 . 33 - 10 P - value = 0 . 05 P - value = 0 . 30 RivaGAN : ‚ÄúDonuts with frosting and glazed toppings sit on table next to co Ô¨Ä ee maker‚Äù P - value = 2 . 33 - 10 P - value = 0 . 43 P - value = 0 . 30 Figure 5 : Qualitative showcase of three kinds of images : non - watermarked , watermarked with mentioned technique , and attacked images with the strongest attack from Table 1 . The p - values and text prompts are also provided . And , p = Pr (cid:16) œá 2 | M | , Œª ‚â§ Œ∑ | H 0 (cid:17) = Œ¶ œá 2 ( Œ∑ ) Where Œ¶ œá 2 represents the cumulative distribution function of the noncentral œá 2 distribution . We refer to Wen et al . [ 2023 ] for more detailed descriptions of these statistical tests . A . 3 Qualitative of Watermarking Techniques We refer to Figure 5 for examples of non - watermarked , watermarked , and attacked images using the attacks summarized in Table 1 . 12