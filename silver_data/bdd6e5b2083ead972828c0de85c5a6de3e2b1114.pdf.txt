MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs THOMAS LANGERAK , ETH Z√ºrich , Switzerland SAMMY CHRISTEN , ETH Z√ºrich , Switzerland MERT ALBABA , ETH Z√ºrich , Switzerland CHRISTOPH GEBHARDT , ETH Z√ºrich , Switzerland OTMAR HILLIGES , ETH Z√ºrich , Switzerland Filter Text Sticker Size Orri . SmallMediumLarge User Agent User Interface / Shared Environement Reinforcement Learning Agents Interface Agent Filter Text Sticker Size Orri . ColorSepiaGray Filter Text Sticker Size Orri . ColorSepiaGray Suggests Filter Selects Small Selects Gray Rewards both Agents Fig . 1 . We formulate the task of online user interface adaptation as a multi - agent reinforcement learning problem . The approach comprises a user - and interface agent . The user agent interacts with an application in order to reach a goal and the interface agent learns to assist in reaching this goal without requiring a - priori knowledge about the task . In the depicted example the user agent interacts with a GUI , while the interface agent opens relevant sub - menus for the user agent . The interface agent does not know the goal of the user agent but has to infer it from observing the user‚Äôs actions . Our method does not rely on labeled offline data or application - specific handcrafted cost functions , instead it relies on learning the underlying structure of the task through exploration . The goal of Adaptive UIs is to automatically change an interface so that the UI better supports users in their tasks . A core challenge is to infer user intent from user input and chose adaptations accordingly . Designing effective online UI adaptations is challenging because it relies on tediously hand - crafted rules or carefully collected , high - quality user data . To overcome these challenges , we formulate UI adaptation as a multi - agent reinforcement learning problem . In our formulation , a user agent learns to interact with a UI to complete a task . Simultaneously , an interface agent learns UI adaptations to maximize the user agent ‚Äôs performance . The interface agent is agnostic to the goal . It learns the task structure from the behavior of the user agent and based on that can support the user agent in completing its task . We show that our approach leads to a significant reduction in necessary number of actions on a photo editing task in silico . Furthermore , our user studies demonstrate the generalization capabilities of our interface agent from a simulated user agent to real users . CCS Concepts : ‚Ä¢ Human - centered computing ‚Üí Graphical user interfaces ; User models ; HCI theory , concepts and models . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . ¬© 2022 Association for Computing Machinery . Manuscript submitted to ACM 1 a r X i v : 2209 . 12660v1 [ c s . H C ] 26 S e p 2022 , , Langerak , et al . Additional Key Words and Phrases : Multi - Agent Reinforcement Learning , Adaptive User Interfaces , Intelligent User Interfaces ACM Reference Format : Thomas Langerak , Sammy Christen , Mert Albaba , Christoph Gebhardt , and Otmar Hilliges . 2022 . MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs . In Proceedings of . ACM , New York , NY , USA , 24 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION Many tasks require powerful User Interfaces ( UIs ) . However , the more powerful the interface , the more complex it becomes to navigate . For instance , consider the cornucopia of different options in sophisticated photo editing software . One solution to the complexity problem is to dynamically adapt the UI to the user and their task by showing relevant information in a timely manner . Such Adaptive UIs ( AUIs ) have been demonstrated to improve the usability over standard UIs in various use cases , including menus [ 2 , 16 , 23 , 72 ] , cooperative interfaces [ 63 ] , and virtual reality interfaces [ 52 ] . Any complex interface might benefit from appropriate adaptations that help users complete their work . However , designing useful adaptive UIs is difficult . AUIs typically require ( 1 ) access to or must infer the users‚Äô intention from observing their behavior and ( 2 ) must choose suitable adaptations based on the inferred intent [ 62 ] . Modern AUIs [ 27 , 75 , 76 , 82 ] leverage machine learning ( ML ) methods that find correlations between user input , user intention , and adaptation . Such approaches significantly lower the development effort and improve the usability of AUIs over their rule - based predecessors [ 28 ] . However , their reliance on user data introduces three significant limitations . First , data is usually not available for the UIs of emerging technologies ( data availability ) . Second , when collecting user data it is non - trivial to ensure that the data is representative of the users‚Äô actual intentions ( data quality ) . Most importantly , user behavior will change in response to UI adaptations . Hence , the underlying user data of such an AUI might become invalidated by its own interventions ( user behavior adaptation ) . In this paper , we provide a new perspective on learning adaptive UIs that addresses these challenges . Our insight is that the interaction between interface and a user in an adaptive setting is inherently bilateral : user actions trigger adaptations which in turn trigger behavior changes . Where previous work generally either learns i ) a user model for a static interface or ii ) an adaptive interface for an existing , pre - defined user model , we take a more holistic perspective and learn models of both the user and the adaptation strategy . To this end , we formulate the task of learning adaptive UIs as a cooperative multi - agent reinforcement learning problem ( MARL ) . In this novel formulation , a simulated user agent learns to interact with a UI . At the same time , we train an interface agent that learns to adapt the same UI to help the user agent achieve its task more efficiently ( see Fig . 1 ) . Therefore , the interface agent learns to empower the user agent . By working in a simulated environment , our approach does not rely on carefully collected user data ( data availability ) , since the user agent learns to use the interface and thus generates unambiguous data online ( data quality ) . Finally , both agents co - evolve during training and thus inherently learn to deal with user behavior adaptation . We model the user agent as a Hierarchical Reinforcement Learning ( HRL ) agent that learns to navigate and interact with the UI to solve its task . To achieve realistic simulated behavior , we decompose high - level decision - making ( e . g . , the decision to select a menu item ) , from motor control ( e . g . , moving the cursor to the corresponding menu slot ) . This resembles cognitive behavior in human motion tasks [ 40 ] as well as in supervisory control [ 5 , 25 ] . The interface agent is a flat reinforcement learning ( RL ) policy . Its goal is to assist the user agent in solving the task more efficiently , for instance , by expanding the optimal submenu for a photo editing task just in time for the user to find the appropriate entry ( cf . Fig . 1 ) . Crucially , the interface agent does not require access to the intent of the user agent , thereby making it 2 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , goal - agnostic and generally applicable . That is , the interface agent learns to infer the goal of the user agent solely by observing its interactions with the environment . In our setting , the user agent is rewarded for completing its task as quickly as possible , while the interface agent shares the same reward . To demonstrate the feasibility of the proposed method , we introduce three proof - of - concept use cases . ( 1 ) In our first application , the user edits a photo . We show an intelligent menu that automatically opens the optimal submenu . ( 2 ) In our second application we present an intelligent toolbar that automatically assigns the optimal tools to the toolbar . ( 3 ) In our third application we demonstrate that our method also works in a 3D virtual reality setting . The interface agent learns which item the user intends to grab and brings it to the vicinity of the user agent . We perform both in silico studies and an evaluation with real users . In the simulation study , we evaluate all three applications against different baselines . Our method reduces the mean number of clicks by up to 5 . 67 compared to 10 . 15 in a standard interface , which is a 44 % improvement . Furthermore , this effect increases for prolonged , more complex interactions . We also observe that the user agent and interface agent learn to cooperate . Crucially , we find that training the interface agent with our simulated user agent generalizes well to humans , which validates the fidelity of our user agent model and highlights the advantages of not having to rely on real user data . In summary , this paper makes four key contributions : ( 1 ) a novel MARL - based framework to adapt user interfaces online without relying on real user data ; ( 2 ) an HRL - based , cognitively plausible user agent that can learn to operate an UI and enables an interface agent to learn adaptations that are useful to real end - users ; ( 3 ) a goal - agnostic interface agent that learns the underlying structure of the task purely by observing the changes in the interface by the user agent ; and ( 4 ) empirical results showing the effectiveness of our approach under three different usage scenarios . 2 RELATED WORK In this paper , we propose multi - agent RL as a framework for adaptive UIs . Our method features a user agent that models human interaction behavior and an interface agent that adapts the UI to support the user agent . Most related to our work is research on computational user modelling and on methods for adaptive UIs . 2 . 1 Computational User Modelling Computational user modeling has a long tradition in Human - Computer Interaction . These models predict user per - formance and are essential for both online and offline UI optimization [ 62 ] . Early work relies on heuristics or rules [ 1 , 8 ‚Äì 10 , 43 ] as well as on simple mathematical models [ 24 , 34 ] . More recent work extends these models and , for instance , predicts operating time for a linear menu [ 17 ] , gaze patterns [ 70 ] , or cognitive load [ 21 ] . Recently , reinforcement learning gained popularity within the research area of computational user models . This is due to its neurological plausibility [ 5 , 25 ] , as it can serve as model of human cognitive function . The underlying assumption of RL in HCI is that users behave rational within their bounded resources [ 30 , 61 ] . There is evidence that humans use such strategy across domains , such as in causal reasoning [ 20 ] or perception [ 31 ] . In human - computer interaction , researchers have leveraged RL to automate the sequence of user actions in a KLM framework [ 49 ] or to predict fatigue in volumetric movements [ 11 ] . It was also used to explain search behavior in user interfaces [ 83 ] or menus [ 13 ] and as a model for multitasking [ 41 ] . Most similar to our work is research on hierarchical reinforcement learning for user modelling . Jokinen et al . [ 40 ] show that , with the help of Fitts‚Äô Law and a gaze model , human - like typing can emerge . Other works show that HRL can elicit human - like behavior in task interleaving [ 29 ] or touch interactions [ 40 ] . Inspired by this work , we design our user agent by decomposing high - level decision - making from 3 , , Langerak , et al . motor control . Using two separate hierarchical levels yields realistic simulated behavior , unlike prodividing enough model capacity to learn reasonably complex tasks . 4 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , 2 . 2 Methods for Adaptive UIs UI adaptation can either be offline , to computationally design an interface , or online , to adapt the UI according to users‚Äô goals . We will focus on online adaptive UIs and refer readers to [ 59 , 60 ] for an overview of computational UI design . 2 . 2 . 1 Heuristics , Bayesian Networks & Combinatorial Optimization . In early works , heuristic - or knowledge - based approaches ( e . g . , semantic networks ) are used to adapt the UI [ 7 , 77 , 79 ] . Similarly , so - called multi - agent systems employ rule - based and message - passing approaches [ 67 , 68 , 84 ] . Another popular technique for AUIs is domain - expert - designed Bayesian networks [ 4 , 35 ] . More recently , combinatorial optimization was used to dynamically adapt interfaces [ 52 , 63 ] . The downside of these approaches is that experts need to specify user goals using complex rule - based systems or mathematical formulations . Creating them comprehensively and accurately requires developers to foresee all possible states of the user , which is very tedious and requires expert knowledge . Commonly , these approaches also get into a conflict when multiple rules or objectives apply . This conflict often results in unintuitive adaptations . In contrast , our method only requires the layout of the UI . From its representation as an RL environment , we learn policies that meaningfully adapt the UI and realistically reproduce user behavior . 2 . 2 . 2 Machine Learning . Leveraging machine learning can overcome the limitations of heuristic - , network - , and optimization - based systems by learning appropriate UI adaptions from user data . Traditional machine learning ap - proaches commonly learn a mapping from user input to UI adaptation . Algorithms like nearest neighbor [ 48 , 56 ] , Na√Øve Bayes [ 22 , 57 ] , perceptron [ 75 , 76 ] , support vector machines [ 3 ] , or random forests [ 58 , 64 ] are used and models are learned offline [ 3 ] and online [ 75 ] . Due to the problem setting , these approaches require users‚Äô input to be highly predictive of the most appropriate adaptation . Furthermore , it restricts the methods to work in use cases where myopic planning is sufficient , i . e . , a single UI adaptation leads users to their goal . In contrast , our method considers multiple goals when selecting an adaptation and can lead users to their goal using sequences of adaptations . More recent work overcomes the limitations stemming from simple input - to - adaptation mapping by following a two - step approach . They ( 1 ) infer users‚Äô intention based on observations and ( 2 ) choose an appropriate adaptation based on the inferred intent [ 62 ] . Such work uses neural networks , and user intention is modeled either explicitly [ 45 , 78 ] or as a low - dimensional latent representation [ 69 ] . However , these approaches suffer are highly dependent on the quality of the training data , which may not even be available for emerging technologies . In contrast , our method does not depend on pre - collected user data and can learn supportive policies just by observing simulated user behavior . 2 . 2 . 3 Bandits & Bayesian Optimization . Bandit systems are a probabilistic approach often used in recommender systems [ 32 ] . In a Multi - armed bandit setting , each adaptation is modeled as an arm with a probability distribution describing the expected reward . The Bayes theorem updates the expectation , given a new observation and prior data . Related work leverages this approach for AUIs [ 42 , 44 , 54 ] . Bayesian optimization is a sample - efficient global optimization method that finds optimal solutions in multi - dimensional spaces by probing a black box function [ 73 ] . In the case of AUIs , it is used to find optimal UI adaptations by sampling users‚Äô preferences [ 46 , 47 ] . Both approaches trade off exploration and exploitation when searching for appropriate adaptations ( i . e . , exploration finds entirely new solutions , exploit is improves existing solutions ) , rendering them suitable approaches to the AUI problem . However , such methods are not able to plan adaptations over a sequence of interaction steps , i . e . , they plan myopic strategies . In addition , these approaches need to sample user feedback to learn or optimize for meaningful adaptations and , hence , also rely on high fidelity user data . Furthermore , as users themselves learn during training or optimization , 5 , , Langerak , et al . solutions can converge to sub - optimal user behavior ( as methods reduce exploration with convergence ) . In contrast , our method can plan adaptations by considering sequences of interactions learned from realistic , simulated user data . 2 . 2 . 4 Reinforcement Learning . Reinforcement learning is a natural approach to solve the AUI problem as its underlying decision - making formalism implicitly captures the closed - loop iterative nature of HCI [ 37 ] . It is a generalization of bandits and learns policies for longer horizons where current actions can influence future states . This generalization enables selecting UI adaptations according to user goals that require multiple interaction steps . Its capability makes RL a powerful approach for AUIs with applications in dialog systems [ 26 , 80 ] , crowd sourcing [ 19 , 38 ] , sequential recommendations [ 12 , 51 , 53 ] , and mixed reality [ 27 ] . Most similar to our work is a model - based RL method that optimizes menu adaptations [ 82 ] . Current RL methods sample predictive models [ 26 , 38 , 82 ] or logged user traces [ 27 ] . However , these predictive models and offline traces represent user interactions with non - adaptive interfaces . Introducing an adaptive interface will change user behavior ; so - called co - adaptation [ 55 ] . Hence , it is unclear if the learned model can choose meaningful adaptations when user behavior changed significantly due to the model‚Äôs introduction . In contrast , our user agent learns to interact with the adapted UI ; hence , our interface agent learns on behavioral traces from the adapted setting . 3 MULTI - AGENT REINFORCEMENT LEARNING FOR ADAPTIVE UIS The goal of our work is to automatically and intelligently adapt a UI such that it guides the user toward completing their task . Thus , it needs to be able to infer users‚Äô intentions based on observations and choose an appropriate adaptation based on the inferred intent [ 62 ] . Our work aims at performing both steps , inference and adaptation , without requiring logged user data or online user feedback . To facilitate and model the bidirectional interaction , we introduce an RL - based user agent and interface agent , and a user interface ( see Fig . 2 ) as a shared environment . We additionally specify a goal state that the user agent should achieve via the UI . The goal state is randomly sampled and unkown to the interface agent . During training , the user agent tries to reach this state through trial and error . For instance , in the case of the hierarchical menu in Figure 1 , the user agent clicks through random sequences of menu items and then observes if the resulting photo matches the goal state . The user agent will explore all paths in the UI with which this goal can be reached and adapt its behavioral policy based on whether a path leads to success . At the same time and in the same environment ( i . e . , the same UI ) , we train an interface agent . While the goal of the user agent is to transition the task into a specific goal state , the interface agent aims to change this UI such that the user agent can achieve its goal faster . Thus , it employs the explained procedure to find meaningful sequences of UI adaptations . In the example of the hierarchical menu , it would randomly open up menus and observe if this helps the user agent to make the desired changes on the photo faster . The interface agent will observe all possible states of the user agent and learn how to guide it towards completing its task . This joint exploration of both agents avoids the need of the interface agent to have direct access to the goal state , i . e . , it makes it goal - agnostic . Following this approach , multi - agent reinforcement learning can discover the underlying task structure through repeated observation of usage patterns . By exploring UI paths , this task structure generalizes to real users . In the following , we briefly introduce reinforcement learning formally and then describe the details of our method . We then show how our approach helps to adapt UIs for real users . 6 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , Shared Environment Hierarchical User Agent Interface Agent Interact : Decide : Observe : Adapt : Observe : Performance - based Reward : Learned : Decision Making Policy Learned : Interface Policy Ruled - based : Motor Control Filter Text Sticker Size Orri . Color Sepia Gray Filter Text Sticker Size Orri . Small Medium Large Filter Text Sticker Size Orri . Small Medium Large Fig . 2 . Our interface agent and user agent both act in the same environment , however , they do not share the same observations . The user agent is modelled as a two - level hierarchy with a high - level decision making policy ùúã ùê∑ and a low - level motor control policy ùúã ùëÄ . It interacts with the interface , while the interface agent policy ùúã ùêº adapts the interface to assist the user agent in achieving its task more efficiently . 4 BACKGROUND We briefly introduce ( multi - agent ) reinforcement learning and its underlying decision processes . Specifically , we assume that users behave according Computational Rationality ( CR ) [ 61 ] . This allows us to frame user behavior as a Markov Decision Process ( MDP ) , which are used to describe Reinforcement Learning problems . For a more detailed overview on RL , we refer the reader to [ 81 ] and for a review on user modeling with CR to [ 61 ] . 4 . 1 Markov Decision Processes Markov Decision Processes ( MDP ) are a mathematical framework for single agent decision - making in stochastic environments [ 36 ] . An MDP is a five - tuple ( ùëÜ , ùê¥ , ùëá , ùëÖ , ùõæ ) , where ùëÜ is a set of states and ùê¥ a set of actions . ùëá : ùëÜ √ó ùê¥ √ó ùëÜ ‚Üí [ 0 , 1 ] is a transition probability function , where ùëá ( s ‚Ä≤ , a , s ) is the probability of the transition from state s ‚Ä≤ to s after taking action a . ùëÖ : ùëÜ √ó ùê¥ ‚Üí R is the reward function , discounted with factor ùõæ . 4 . 2 Reinforcement Learning Reinforcement Learning is a machine learning paradigm based on rewarding desired and penalizing undesired behavior . In general , an agent observes an environment state and tries to take optimal actions in order to maximize a numerical reward signal . A key difference with supervised learning is that RL learns through exploration and exploitation rather than from an annotated dataset . We follow the standard formulation of RL as an MDP [ 81 ] . The goal is to find an optimal policy ùúã : ùëÜ ‚Üí ùê¥ , a mapping from states to actions that maximizes the expected return : E (cid:2)(cid:205) ùëáùë° = 0 ùõæ ùë° ùëÖ ùëñ ( s ùë° , a ùë° ) (cid:3) . Since both state - and action spaces can be high - dimensional , neural networks are used for policy learning ( i . e . , we approximate the policy as ùúã ùúÉ where ùúÉ are the learned parameters ) . We use model - free RL ( for a comparison to model - based RL , see [ 65 ] ) . This set of algorithms is used in an environment where the underlying dynamics ùëá ( s ‚Ä≤ , a , s ) are unknown , but it can be explored via trial - and - error . 7 , , Langerak , et al . 4 . 3 Multi - Agent Reinforcement Learning Standard MDPs assume a single policy . Stochastic games are a generalization of MDPs for multiple policies [ 74 ] . A stochastic game is defined as a six - tuple ( ùëÅ , S , A , ùëá , R , ùõæ ) , where ùëÅ is the number of policies . S = ùëÜ 1 √ó . . . √ó ùëÜ ùëÅ is a finite set of state sets , with subscripts indicating different policies . A = ùê¥ 1 √ó . . . √ó ùê¥ ùëÅ is a finite set of action sets . ùëá is the transition probability function . A set of reward functions is defined as R = ùëÖ 1 , . . . ùëÖ ùëÅ . Furthermore , we define a set of policies as Œ† = ùúã 1 , . . . ùúã ùëÅ . Finally , ùõæ is the discount factor . All policies have their individual actions , states , and rewards . However , their observations are bound by the actions of the other policies . In this paper , we optimize each policy individually , while the observations are influenced by each others actions . Note that the terms policy and agent are not interchangeable , a single agent can have multiple policies . 5 METHOD We present a general task description and outline of our user agent and interface agent ( Fig . 2 ) . Note that within the context of this paper , we will use the term observation and state interchangeably , since the agents in our environment never get access to the full underlying environment state . 5 . 1 General Task Description The tasks we study share the concept that a user is trying achieve an intrinsic goal . For instance , a goal can be the desired configuration of a photo in a graphics editing software with size ( for instance , with the items small and large ) and orientation ( original or flipped ) as discrete attributes . We encode the goal in a vector g , which is a one - hot encoding of these attributes . For instance , with two attributes ( size and orientation ) and two items each , g would be in Z 42 . The term Z ùëñ is the set of integers { 0 , . . . , ùëñ ‚àí 1 } , where the subscript denotes discrete attributes of the goal , while the superscript denotes the total length of options , i . e . , the total length of the goal vector . We discretize the environment so that each item is located in a predefined slot . Furthermore , the user has access to an input state , which we denote by x . For example , this can correspond to the current photo configuration . The current input state , x , and the goal state g are identical in their dimension and type . The user agent interacts with the interface and attempts to make the input state and the goal state identical as fast as possible , such that x = g . Each interaction updates x accordingly and a trial terminates once they are identical , e . g . , if the size and orientation of the edited photo are identical to the goal . The interface agent makes online adaptations to the interface . Similar to a real - world designer , it does not know the specific goal of a user . Instead , it needs to observe user interactions with the interface to learn the underlying task structure that will yield the optimal adaptations , e . g . , that the user likely wants to configure the size of a photo , after configuring the orientation . 5 . 2 User Agent First , we introduce the user agent , which interacts with an environment to achieve a certain goal ( e . g . , select the intended attributes of a photo ) . The agent tries to accomplish this as fast and accurately as possible . To do so , the user agent has to first compare the goal and input state , and then plan movements to reach the target . Therefore , we model the user as a hierarchical agent with separate policies . 8 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , Specifically , we introduce a two - level hierarchy : a high - level decision - making policy ùúã ùê∑ that computes a target for the agent ( high - Level decision making ) , and a Fitts‚Äô - Law - based low - level motor policy ùúã ùëÄ that decides on a strategy to reach this target . We now explain both policies in more detail . 5 . 2 . 1 High - Level Decision - Making Policy . The high - level decision - making policy of the hierarchy is responsible for high - level decisions , i . e . , what is the next target . The overall goal of the policy is to complete a given task while being as fast as possible . Its actions are based on the current observation of the interface , the goal state , and the current state of the agent . The goal state is a task - dependent encoding of what the user attempts to achieve . More specifically , the high - level state space ùëÜ ùê∑ is defined as : ùëÜ ùê∑ = ( p , m , x , g ) , ( 1 ) which comprises : i ) the current position of the user agent ‚Äôs end - effector , p ‚àà ùêº ùëõ ( where ùëõ denotes the dimensions e . g . , 2D vs 3D ) , ii ) an encoding of the location each item m ‚àà Z ùëõ ùëñ √ó ùëõ ùë† 2 , with ùëõ ùëñ and ùëõ ùë† being the number of menu items and environment locations , respectively , and iii ) the current input state x ‚àà Z ùëõ ùëñ 2 , and iv ) the goal state g ‚àà Z ùëõ ùëñ 2 . Here , ùêº denotes the unit interval [ 0 , 1 ] , and Z ùëñ is the set of integers { 0 , . . . , ùëñ ‚àí 1 } , where the subscript denotes the range of that space , while the superscript denotes the size of the vector g . The action space ùê¥ ùê∑ is defined as : ùê¥ ùê∑ = ùë° , ( 2 ) which indicates the next target slot , ùë° ‚àà N ùëõ ùë† . The reward for the high - level decision making policy consists of two weighted terms to trade - off between task completion accuracy and task completion time : i ) how different the current input state x is from the goal g , and ii ) the time it takes to execute an action . Therefore , the high - level policy needs to learn how items correlate with the task goal as well as how to interact with any given interface . With this , we define the reward as follows : ùëÖ ùê∑ = ùõº E ùëîùëë (cid:124)(cid:123)(cid:122)(cid:125) ùëñ ) ‚àí ( 1 ‚àí ùõº ) ( ùëá ùê∑ + ùëá ùëÄ ) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) ùëñùëñ ) + 1 success , ( 3 ) where E ùëîùëë is the difference between the input state and the goal state , ùõº is a weight term , and the movement time ( ùëá ùëÄ ) is an output of the low - level policy , ùëá ùê∑ is the search - and - decision time , and 1 success is an indicator function that is 1 if the task has been successfully completed and 0 otherwise . In addition to movement time , we also need to determine the search - and - decision time ùëá ùê∑ . To this end , we utilize the Search - Decision - Pointing model [ 17 ] . This model interpolates between a linear visual search - time component ( ùëá ùë† ) and the Hick - Hyman decision time [ 34 ] ( ùëá ‚Ñé‚Ñé ) . We refer to [ 17 ] for more details . We define the difference E ùëîùëë between the input state x and the goal state g as the number of mismatched attributes : E ùëîùëë = ‚àí ‚àëÔ∏Å ùë• ‚àà g , ùë¶ ‚àà x 1 ùë• ‚â† ùë¶ ùëõ ùëéùë°ùë°ùëü , ( 4 ) where 1 is an indicator function that is 1 if ùë• ‚â† ùë¶ and else 0 , ùë• and ùë¶ are individual entries in the vectors g and x respectively , and ùëõ ùëéùë°ùë°ùëü is the number of attributes ( e . g . , color , size , and orientation ) . 5 . 2 . 2 Low - Level Motor Control Policy . The low - level motor control policy is a non - learned controller for the end - effector movement . In particular , given a target , it selects the parameters of an end point distribution ( mean ùúá p and standard 9 , , Langerak , et al . deviation ùúé p ) . We set ùúá p to the center of the target . The target , ùë° , is the action of the higher level decision making policy ( ùê¥ ùê∑ ) . We set ùúé p to 1 / 6th of the button width , to reach a hitrate of 96 % [ 24 ] . Given the current position and the end point parameters ( mean and standard deviation ) , we compute the predicted movement time using the WHo Model [ 33 ] . ùëá ùëÄ = (cid:18) ùëò ( ùúé p / ùëë p ‚àí ùë¶ 0 ) 1 ‚àí ùõº (cid:19) 1 / ùõº + ùëá ( 0 ) ùëÄ , ( 5 ) where ùëò and ùõº are parameters that describe a group of users , ùëá ( 0 ) ùëÄ is the minimal movement time , and ùë¶ 0 is equal to the minimum standard deviation . The term ùëë p indicates the traveled distance from the current position to the new desired position ùúá p . We closely follow the literature for the other parameters [ 33 , 40 ] . We sample a new end - effector position from a normal distribution : p ‚àº N (cid:0) ùúá p , ùúé p (cid:1) . 5 . 3 Interface Agent The interface agent makes changes to the UI in order to maximize the performance of the user agent . We specifically focus on discrete changes to the UI ( e . g . , item - to - slot assignment , which page to show , or making certain elements available to the user ) . Unlike the user agent , we model the interface agent as a flat RL policy . The interface agent needs to learn i ) the objective of the user agent based on observing the changes in the interface as the result of the user‚Äôs action , ii ) the underlying task structure , and iii ) how to empower the user agent to increase their performance . The state space , ùëÜ ùêº of the agent is defined as : ùëÜ ùêº = ( p , x , m ) , ( 6 ) which includes : i ) the position of the user p ‚àà ùêº 2 , ii ) the current state of the tool x ‚àà Z ùëõ ùëñ 2 , and iii ) the current state of the menu m ‚àà Z ùëõ ùëñ √ó ùëõ ùë† 2 . Note that the interface agent does not have access to the goal ( g ) or the user agent ‚Äôs target ( ùë° ) . This forces the agent to reason about the intention of the user based on its observable actions and changes to the interface rather than the user‚Äôs internal state . This makes the problem more challenging , but also more realistic . The action space ùê¥ ùêº is in Z , however its dimension differs per application . The goal of the interface agent is to support the user agent . Therefore , the reward of the user agent is directly coupled to the performance of the user agent . We define the reward of the interface agent to be equal to the reward of the user agent ‚Äôs high - level policy ( see Eq . 10 ) : ùëÖ ùêº = ùëÖ ùê∑ ( 7 ) 6 TRAINING & IMPLEMENTATION We train the user and interface agents‚Äô policies simultaneously . All policies receive an independent reward , and the actions of the policies influence a shared environment . We execute actions in the following order : ( 1 ) the interface agent‚Äôs action , ( 2 ) the user agent‚Äôs high - level action , followed by ( 3 ) the user agent‚Äôs low - level motor action . The reward for all three policies is computed after the low - level motor action has been executed . The episode is terminated when the user agent has either completed the task or exceeded a time limit . We implement our method in Python 3 . 8 using RLLIB [ 50 ] and Gym [ 6 ] . We use PPO [ 71 ] to train our policies . We use 3 cores on a Intel ( R ) Xeon ( R ) CPU @ 2 . 60GHz during training . Training takes ‚àº 20 hours . We do not utilize a GPU for training . The user agent‚Äôs high - level decision - making policy ùúã ùê∑ is a 3 - layer MLP with 512 neurons per layer and 10 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , Filter Text Sticker Size Orri . Filter Text Sticker Size Orri . SmallMediumLarge Filter Text Sticker Size Orri . SmallMediumLarge Filter Text Sticker Size Orri . ColorSepiaGray Filter Text Sticker Size Orri . ColorSepiaGray 1 ) Initialization 2 ) User Presses Size 3 ) User Presses Small 4 ) Interface Opens Filter 5 ) User Presses Gray Fig . 3 . We introduce a photo editing task . In this task , a user matches a photo to a target photo by operating a hierarchical menu ( 1 ) . The user selects the submenu ‚Äòsize‚Äò ( 2 ) . The user then selects the attribute ‚Äòsmall‚Äò , which alters the image ( 3 ) . After the user has changed an attribute , the interface observes the new state of the photo and finds the most likely submenu for the next user action ( 4 ) . The user uses this to copmlete the task ( 5 ) . ReLU activation functions . The interface agent‚Äôs policy ùúã ùêº is a two - layer network with 256 neurons per layer and ReLU activation functions . We use curriculum learning to increase the task difficulty and improve learnability , for more information see Appendix A . A full implementation can be found at [ redacted ] . 7 EVALUATION To assess the validity and efficacy of our method , we evaluate it in simulation and with real users . To this end , we first introduce a photo editing task with accompanying interface ( see Sec . 7 . 1 ) . We first compare our method against baselines with hand - designed , random , or supervised learning interface agents in simulation ( see Sec . 7 . 2 ) . We then evaluate results in silico ( see Sec . 7 . 3 ) and in a user study ( see Sec . 7 . 4 ) . In the user study , we compare the performance of our trained interface agent and baselines when real users interact with the UI . For clarity , the interface agent is always trained purely in simulation and does not use any offline or real user data . The independent variables in the evaluation are the method ( ours and baselines ) and the number of Initial Attribute Differences ( IAD ) . The initial number of attribute differences is how many attributes differ between the goal g and initial selection x at the start of an episode . For example , an IAD of one could be that the only mismatch is the color attribute , whereas for an IAD of three the color , size and orientation could differ . Our dependent variable is the number of actions for task completion . In our experiments , we instruct the users to execute the task as accurately as possible , rather than focusing on speed . Thus , we do not use task completion time as a metric . However , previous work indicates that task completion time is proportional to the number of actions [ 10 ] . 7 . 1 Task & Environment In the experimental task , a user edits a photo by changing its attributes ( see Fig . 3 ) . A photo has five distinct attributes with three states per attribute : i ) filter ( color , sepia , gray ) , ii ) text ( none , Lorem , Ipsum ) , iii ) sticker ( none , unicorn , cactus ) , iv ) size ( small , medium large ) , and v ) orientation ( original , flipped horizontal , and vertical ) . The photo‚Äôs attribute states are limited to one per attribute , i . e . , the photo cannot be in grayscale and color simultaneously . This leads to a total of 15 attribute states and 243 photo configurations . The graphical interface is a hierarchical menu , where each attribute is a top - level menu entry , and each attribute state is in the corresponding submenu . By clicking a top - level menu , the submenu expands and thus becomes visible and selectable . Only one menu can be expanded at any given time . The photo attribute states correspond to the current input state x and the target state g , where g is only known to the user agent . The interface agent selects an attribute menu to open . Its goal is to reduce the number of clicks necessary to change an attribute , e . g . , from two user interactions ( filter - > color ) to one ( color ) , by inferring what the user most 11 , , Langerak , et al . Table 1 . Overview of the state and action dimensionality of the photo editing task . ùê¥ ùêº ùê¥ ùê∑ m x g p Z 15 Z 115 Z 20 √ó 20 2 Z 152 Z 152 I 2 likely intends to do next . For the user agent , the higher level selects a target slot , and the lower level moves to the corresponding location . If the target slot is unavailable ( e . g . , color when ‚Äòsize‚Äô is expanded ) , we terminate the episode early . The final dimensionalities of state and action spaces can be found in Tab . 1 . 7 . 2 Baselines To evaluate our method , we compare it to three different baseline variants for the interface : ( 1 ) hand designed , ( 2 ) random , and ( 3 ) supervised learning . ( 1 ) Hand - Designed : The hand - designed baseline is a traditional interface that can be regarded as the current status quo . The layout is identical to the intelligent settings ( five top - level attribute menus , each with three items when expanded ) . However , none of the submenus open automatically . ( 2 ) Random : In the random baseline , the interface agent randomly opens a submenu . This baseline , standard in RL literature , will allow us to judge how much our agent learns . ( 3 ) SVM : To compare to a supervised learning baseline , we follow [ 27 ] and use a Support Vector Machine ( SVM ) with an RBF kernel as the interface agent . We train the SVM on a synthetic dataset of 10000 samples generated with a user agent pre - trained on the random baseline . More samples did not result in a better performing SVM . The input is identical to the state representation of the interface agent . The labels indicate which item was selected by the user agent . The task is to predict the most likely item to be used for the next user interaction , given the observed user state . 7 . 3 In Silico Evaluation In order to characterize the performance our method we first perform an in silico evaluation . 7 . 3 . 1 Procedure . In our in silico evaluation , we compare our method against the three baselines ( Sec . 7 . 2 ) . To this end , we train the user agent model on each baseline interface . Note that the baselines stay fixed during training of the user agent ( i . e . , the SVM is trained a priori and not updated ) , whereas our method allows for simultaneous training . We run each experiment twelve times across random seeds . To compare the methods quantitatively , we vary the number of initial attribute differences from 1 to 5 . For each IAD , we generate 200 rollouts with randomly sampled initial and goal states , yielding 1000 evaluation samples in total . 7 . 3 . 2 Results . Figure 4 summarizes the training results . We use two metrics for this evaluation : ( a ) task completion rate , which indicates the fraction of successfully completed episodes ( an episode is terminated when it lasts too long or an agent takes an impossible action ) , and ( b ) the number of actions needed by the user agent to complete a task , normalized by the number of IADs . In Figure 4 a ) , we see that all methods reach a task completion rate close to 100 % within 100 training epochs . Thus , the user agent learns to successfully interact with all of the baseline interfaces . However , when looking at Figure 4 b ) , we observe that in terms of actions required per attribute difference , the baselines converge to higher values than 12 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , a ) b ) Fig . 4 . a ) The fraction of successfully completed tasks . For visibility reasons we only plot the first 100 training epochs . b ) The mean number of steps needed to solve a task , normalized by the number of initial attribute difference . We see that , although with all methods the user agent learns to complete the task ( a ) , the user agent uses requires the least actions when trained with our interface agent ( b ) .            , Q L W L D O  $ W W U L E X W H  ' L I I H U H Q F H V                  1  X  P  E H  U    R  I   $  F  W  L  R Q  V  + D Q G  ' H V L J Q H G  5 D Q G R P  6 9 0  2 X U V Fig . 5 . We evaluate our method in silico . We report the mean number of actions our user agent takes for different IADs . Our method outperforms the three baselines . Furthermore , we see that the improvement with regard to other baseline increases with more IADs . Hence , our method‚Äôs advantages become more evident with increasing task complexity . our method ( lower is better ) . The Hand - Designed baseline converges to a stable number of actions , which is expected because the actions per IAD is lower - bounded ( Figure 5 ) . More specifically , it requires the user to open a submenu and select an attribute for each desired change . Therefore , we see that the Random baseline out - performs the Hand - Designed variant . This likely occurs because occasionally opening the correct submenu is more beneficial than opening no submenu at all ( Hand - Designed ) . These findings are also reflected in Tab . 2 . A Friedman test revealed a significant effect for the method ( ùúí 2 ( 3 ) = 18 . 04 , ùëù < . 001 ) and IAD ( ùúí 2 ( 4 ) = 166 . 10 , ùëù < . 001 ) . We analyzed our results further with a Bonferonni - corrected post - hoc test . We find significant differences across the board for the method ( all ùëù < . 001 , averaged across IADs ) . Our 13 , , Langerak , et al . Table 2 . In Silico Results : mean ( ¬± SD ) number of actions per method across different number of initial attribute differences . Lower values mean better performance . Bold indicates a significant difference with regard to all baselines . All experiments are averaged over twelve random seeds . Number of Initial Attribute Differences 1 2 3 4 5 Mean Hand - Designed 2 . 12 ¬± 0 . 03 4 . 10 ¬± 0 . 02 6 . 13 ¬± 0 . 03 8 . 18 ¬± 0 . 05 10 . 21 ¬± 0 . 05 6 . 15 ¬± 0 . 04 Random 1 . 86 ¬± 0 . 03 3 . 51 ¬± 0 . 03 5 . 01 ¬± 0 . 05 6 . 37 ¬± 0 . 08 7 . 50 ¬± 0 . 08 4 . 85 ¬± 0 . 06 SVM 1 . 85 ¬± 0 . 02 3 . 3 ¬± 0 . 04 4 . 50 ¬± 0 . 05 5 . 45 ¬± 0 . 05 6 . 18 ¬± 0 . 05 4 . 26 ¬± 0 . 04 Ours 1 . 92 ¬± 0 . 07 3 . 25 ¬± 0 . 06 4 . 23 ¬± 0 . 09 5 . 05 ¬± 0 . 04 5 . 37 ¬± 0 . 04 3 . 97 ¬± 0 . 06 method , with a mean of 3 . 97 actions , uses on average 2 . 18 actions less than the Hand - Designed baseline , 0 . 88 actions less than Random , and 0 . 29 actions less than the SVM baseline . This statistically significant result indicates that our method outperforms all baselines , and reduces the number of user agent actions , increasing the user agent ‚Äôs efficiency . When looking at the performance across different number of initial attribute differences in Tab . 2 , we see that the Hand - Designed baselines always uses significantly more actions than all three other methods ( all ùëù < . 001 ) . We find no significant difference between Ours - Random ( ùëù = 0 . 23 ) and Ours - SVM ( ùëù = 0 . 17 ) for a single initial attribute difference . With two or more IADs , our method uses significantly less actions than Random ( all ùëù < . 001 ) . For low IADs , there is no significant difference between our method and the SVM baseline ( ùëù = 1 . 00 ) . However , for three or more IADs , our method uses significant less actions than the SVM baseline ( all ùëù < . 001 ) . These results indicate that our method improves the further we go beyond a myopic setting compared to baselines . Real world tasks often feature long action sequences ; where this effect may be even more pronounced . 7 . 4 User Study To evaluate whether the interface agent improves real user performance , we conduct a user study . We do this by replacing the user agent with real end - users . 7 . 4 . 1 Procedure . The participants interact with our interface agent and the baseline interfaces . Due to potential participant fatigue , we evaluate on two baselines ( Hand - Designed and SVM , thereby omitting Random ) , as well as four and five IADs . The three settings are counter balanced with a Latin square design and the participants complete 26 trials per setting ; we discarcd the first six trials for training . The user study lasts 20 to 25 minutes per participant and they can rest between trials . We ensure that the number of initial attribute differences is uniformly distributed among the participant‚Äôs trials . The participants use a Logitech MX Master 3 Mouse and a 27 " 4K display . Participants . We conduct our study on 12 users ( 9 male , 3 female , aged between 24 and 38 ) . All participants are right handed and have normal or correct - to - normal vision . 7 . 4 . 2 Results . As shown in Tab . 3 , real users require less actions on average to complete the tasks with our method compared to both the Hand - Designed and SVM baselines . More specifically , users on average need 3 . 82 actions less with our method compared to the Hand - Designed baseline , and 0 . 650 actions less than the SVM baseline . A Friedman test reveals a significant effect for method ( ùúí 2 ( 2 ) = 37 . 28 , ùëù < . 001 ) , but not for IAD ( ùúí 2 ( 1 ) = 2 . 95 , ùëù = 0 . 09 ) . 14 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , ,      , Q L W L D O  $ W W U L E X W H  ' L I I H U H Q F H V               1  X  P  E H  U    R  I   $  F  W  L  R Q  V  + D Q G  ' H V L J Q H G  6 9 0  2 X U V Fig . 6 . The number of actions real users need to solve a task in the photo editing setting for four and five initial attribute differences . We see that both the SVM and our method outperform the standard Hand - Designed interface . Our method outperforms the SVM . Table 3 . User Study Results : Mean ( ¬± SD ) number of actions per method across different number of initial attribute differences . We show a subset of the in silico results for convenience and direct comparison . Bold indicates a significant difference with regard to both baselines . We see that our method improves significantly upon both baselines . Number of IAD 4 5 Mean Hand - Designed 8 . 59 ¬± 0 . 37 10 . 15 ¬± 0 . 21 9 . 37 ¬± 0 . 30 SVM 5 . 91 ¬± 0 . 44 6 . 49 ¬± 0 . 45 6 . 20 ¬± 0 . 45 Ours 5 . 43 ¬± 0 . 43 5 . 67 ¬± 0 . 38 5 . 43 ¬± 0 . 31 To evaluate the difference in method , we conduct a Bonferroni - corrected post - hoc test . The real users require significantly less actions with our method compared to both baselines ( ùëù < . 001 ) . Averaged over both IADs , users need 5 . 55 actions on average with our method , compared to 9 . 37 and 6 . 20 for the Hand - Designed and SVM baseline . We find that real users use less number of actions with our method than the Hand - Designed baseline for both initial attribute differences ( both ùëù < . 001 ) . In the case of Ours vs SVM , we also find a significant decrease in actions with our method for both attribute differences ( ùëù = 0 . 006 and ùëù < . 001 for four and five IADs , respectively ) . Therefore , we observe a significant decrease in the number of actions between our method and the two baselines for both simulated users and real user . This shows that our method increases the efficiency of real users that interact with our method . Furthermore , the results indicate that our interface agent learns an adaptation strategy that transfers well to real users despite being trained only with our simulated user model . We find that users need to get used to our interface agent . For instance , P7 remarked during trials in the beginning " I don‚Äôt know which tab will open next " about our method . While the same participant in later in the block remarked " now that I understand what this setting does , I am liking it more than the standard interface . " This could be an indication how co - adaption is indeed happening when we introduce an AUI and that users take a few trials to get used to this . 15 , , Langerak , et al . > > > > 2 ) Interface shows Green , Triangle , Blue 4 ) Interface shows Square , Triangle , Circle 3 ) User selects Green 5 ) User selects Circle > > 1 ) Initializes to Blue , Square and Small S > > > > Current Selection Target Fig . 7 . We introduce an intelligent toolbar . In this task , the user is trying to match their current tool selection to a target configuration ( 1 ) . The toolbar has more items than slots , therefore the interface agent needs to assign the most relevant items to the slots ( 2 ) . The user agent ‚Äôs high - level policy updates the target based on the available items , e . g . , it selects ‚ÄôGreen‚Äô . The low - level motor control policy then executes a motion to select this attribute ( 3 ) . Thereafter , the interface agent again updates the selection of available attributes ( 4 ) . Finally , the user agent selects another action , e . g . , ‚ÄôCircle‚Äô , after which the task is finished ( 5 ) . The task is completed when the shape and goal match . Human - Agent Comparison . To evaluate the validity of our user agent , we compare the real user data against user agent data ( collected from interactions with our method and the Hand - Designed and SVM baseline interfaces ) . We conduct a Friedman test and find a significant effect for method on number of actions ( ùúí 2 ( 2 ) = 78 . 17 ) . We conduct a Bonferroni - corrected post - hoc test . We find no significant difference between the agent and human on the Hand - Designed baseline ( ùëù = 0 . 79 ) . Since the interface has no automatic component , this reduces the likelihood for non - forced errors by real users . However , we find a difference with SVM ( ùëù < . 001 ) and Ours ( ùëù = 0 . 003 ) between real user and user agent . 8 USAGE SCENARIOS To showcase the generalizability of our approach across different settings , we introduce two additional use cases , which we evaluate in simulation . 8 . 1 Toolbar The first additional scenario is an abstract scenario for operating a toolbar in a graphic design software . In general , toolbars are features to make complex interfaces easier to navigate . However , they suffer from limited space to show items . Our toolbar is used for configuring a brush . The goal is to match a shape input state to a target shape state ( Fig . 7 ) . We represent the brush with four attributes : i ) shape , ii ) color , iii ) size , and iv ) outline . The shape and color each have three possibles states , while the size and outline each have two . Our menu has three visible slots that can be used to assign items to , which is significantly lower than the ten available items . Furthermore , there is a permanent button ( " next " in Fig . 7 ) that triggers an interface agent action without altering the current shape input state . This button is also used to update the visible slots when interacting with the Hand - Designed baseline . To show the modularity and versatility of our hierarchical user model , we replaced our user agent ‚Äôs rule - based low - level motor control policy ( see Section 5 . 2 . 2 ) with an RL - based policy , similar to [ 40 ] . The advantage of a Fitts‚Äô - Law based policy ( as in Sec . 5 . 2 . 2 ) is that it avoids the complexity of learning , thereby enabling more realistic and complicated interfaces . On the other hand , Jokinen et al . have shown that human - like behavior can emerge from RL - based control methods . Furthermore , a learned low - level motor control policy could pave the way to more complex models , such as biomechanical , muscle - based skeletons [ 39 ] . 16 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , Table 4 . Mean ( ¬± SD ) number of clicks per method across different number of initial attribute differences for the toolbar task with real users . Bold indicates a significant difference with regard to both baselines . Number of Initial Attribute Differences 1 2 3 4 mean Hand - Designed 2 . 47 ¬± 0 . 32 4 . 38 ¬± 0 . 24 5 . 89 ¬± 0 . 29 7 . 08 ¬± 0 . 14 4 . 96 ¬± 0 . 26 Random 2 . 10 ¬± 0 . 29 4 . 01 ¬± 0 . 27 5 . 23 ¬± 0 . 26 6 . 64 ¬± 0 . 25 4 . 50 ¬± 0 . 27 Ours 2 . 12 ¬± 0 . 13 2 . 99 ¬± 0 . 11 4 . 11 ¬± 0 . 25 5 . 20 ¬± 0 . 29 3 . 61 ¬± 0 . 21 To compare the validity of our method for this new usage scenario we conduct an additional user study . The setup of the study was similar to our main user study ( see Sec . 7 . 4 ) . We compare with the Hand - Designed and Random baselines . As shown in Tab . 4 , real users require less actions to solve the task with our method than with the baselines . We find significant effects both for the method and attribute types . We conduct a mixed - factor two - way repeated - measures ANOVA with a Greenhouse - Geisser correction . We will refer to classification of real or simulated user as " type " . Overall we find no significance between subjects with regard to type ( ùêπ ( 1 , 11 ) = 1 . 20 , ùëù = 0 . 286 ) . However , we find a significant interplay between method and type ( ùêπ ( 1 . 01 , 11 ) = 18 . 116 , ùëù < . 001 ) , as well as a significant interplay between initial goal attributes and type ( ùêπ ( 2 . 26 , 11 ) = 4 . 73 , ùëù = 0 . 011 ) . We carry out further analysis via a Bonferroni - corrected post - hoc test . There is no significant difference in the number of actions between a real user and the user agent in the case of our method ( ùëù = 0 . 15 ) and the Hand - Designed baseline ( ùëù = 0 . 98 ) . In terms of the initial attribute difference , averaged over the different methods , there is no significant difference between our agent and human participants ( all ùëù > 0 . 05 ) . We also find no significant difference between the user agent and the real users when averaging over all baselines and initial attribute differences . The real user and the user agent are closest , in terms of average number of actions , in the Hand - Designed setting . This means that we a lower baseline our user agent and user do not use a distinctly different number of actions to complete a task . The most interesting qualitative insight about our method is reported by P10 : " The order in which I click [ the relevant items ] matters . If I do it right I don‚Äôt have to click next . " This observation could indicate two things : i ) our user agent and interface agent adapt to each other during training and develop implicit communication , ii ) our participant is able to detect and adapt to this behavior . Investigating how explicit communication between human - in - the - loop and interface agents can be leveraged is an interesting and active research direction [ 66 ] . 8 . 2 Virtual Reality Out - of - Reach Item Grabbing In the second additional scenario , the user is asked to select and grab elements that are initially out of reach . The interface agent needs to move the intended item closer for the user to reach it and complete the task . For example , the task can include a series of objects that are out of reach in a virtual reality environment , as shown in Figure 8 . The need for the user agent to express their intention to the interface agent through its actions makes this task conceptually interesting . Furthermore , the task becomes more challenging compared to photo editing ( see Sec . 7 . 1 ) , because the user agent cannot solve the task on its own . Therefore , the user agent will not be able to learn the task at hand without the cooperation of the intelligent interface . We find that our method successfully learns the task and thus generalizes to this setting . For qualitative results , we refer the reader to the supplementary video . 17 , , Langerak , et al . 1 ) Initialization 2 ) User moves hand 4 ) User grabs object 3 ) Interface moves Object Fig . 8 . We introduce an out - of reach object grabbing task in VR . In this task a user attempts to grab a specific object , that is initially out of reach , in a Virtual Reality space containing multiple objects ( 1 ) . First the user agent needs to learn to move towards that object ( 2 ) . The interface agent needs to learn to correlate that movement to the intended object ( 3 ) . The user then grabs the intended object to finish the task ( 4 ) . 9 DISCUSSION Our work models online UI adaptation via multi - agent reinforcement learning . In the following , we discuss our findings , limitations and elaborate on exciting opportunities for future work . The results of our in silico evaluation ( see Sec . 7 . 3 ) have shown that our multi - agent reinforcement learning approach is capable of learning UI adaptations that increase user efficiency . The user agent learns to interact with the interface , while the interface agent learns suitable UI adaptions . We empirically show that our method supports the user agent better than other approaches . The SVM baseline achieves comparably good results , since it was trained on the high - quality data of the user agent , which had learned to behave optimally . However , our method still outperforms this baseline . We argue that this is due to our interface agent seeing non - optimal interactions of the user agent at the beginning of training . As a result , it knows how to recover from the errors of real users , rendering it more robust than the SVM . We have provided empirical evidence that the interface agent helps real users in a user study ( see Sec . 7 . 4 ) . This shows that the user agent is a good proxy to learn useful interface agent policies . We hypothesize that the difference in behavior between the user agent and participants comes from cognitive mistakes real users make ( e . g . , the user mistakenly selecting an irrelevant attribute in the photo editing task ) . The user agent trained with reinforcement learning , as proposed in our method , is less prone to these types of errors . In the toolbar scenario ( see Sec . 8 . 1 ) , we have demonstrated that the high - level decision making policy of our user agent can be combined with a learning based low - level motor control policy . We have found no significant difference with the behavior of real users , which shows that our method can be used in use cases where no user data is available . An interesting direction for future work is to learn motor control from human - like models , such as a biomechanical upper extremity model [ 39 ] . This would allow to further remove any assumptions on user behavior form the model . We also show how our method can be easily adapted to 3D Virtual Reality interfaces ( see Sec . 8 . 2 ) . In the presented scenario , it is not possible for users to complete the task without the help of the interface agent . This indicates the potential our approach for adaptive interfaces where the UI needs to work in conjuncture with the user . This is especially important for emerging interface technologies where AUIs need to continuously adapt system input and output for users to be able to fully leverage them [ 28 ] . In addition to its practical use , our method has theoretical appeal as it provides an intuitive , computational and plausible model of the bilateral nature of AUIs . The user and the computer need to form an implicit theory of mind to infer the other‚Äôs intention and goal without explicit communication . While it extends the scope of the paper , we believe that future work can leverage our method to gain a better theoretical understanding of how users interact with a UI 18 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , and how successful AUIs need to be adapted . A first step towards this would be to increase the cognitive capabilities of our user agent . For instance , human gaze patterns could be modeled [ 70 ] to elicit more complex and realistic user behavior . Moreover , other cognitive processes , such as task comprehension and memory , could be considered . Our work is the first to demonstrate the feasibility of modeling both user and interface as agents in an RL setting . This will open up exciting directions for future work . However , community effort is required to scale to real - world applicability . The main technical difficulty lies in 1 ) the exponential growth in complexity with the increased number of items and 2 ) longer horizon tasks . While we have demonstrated that our formulation can be used to solve problems with up to 5 billion possible states ( toolbar application ) , further research is necessary to overcome the complexity hurdle . This will require both , algorithmic advancements , including pre - training [ 18 ] and leveraging human demonstrations to guide exploration [ 15 ] , as well as more efficient formulations of the UI adaptation problem . Our setup lends itself to scale to i ) multiple users with different skills and intentions and ii ) more complex UI adaptation strategies ( e . g . , changing the items and the size of a toolbar ) that work in conjuncture . Furthermore , increasing the number of interface agents could lead to more intricate user behavior and open the door for more elaborate interface adaptations . For instance bespoke assistive UIs for users with specific needs , UIs for users with specific expertise levels , or UIs that adapt to the physical environment . 10 CONCLUSION In this work , we have taken a first step towards a general learning - based framework for adaptive UIs . We achieve this via a multi - agent reinforcement learning formulation that includes a user agent and an interface agent . The user agent tries to achieve a task - dependent goal as fast as possible . The interface agent cannot observe the intention of the user agent . Hence , it is forced to learn the underlying task structure by observing the interactions between the user agent and the UI . Since both agents are trained jointly , we find that the agents learn to collaborate and become more efficient at solving the task . More fundamentally , it also eliminates the need for offline user data . We have evaluated our approach in simulation and with humans . Our method reduces the mean number of actions to complete the task when compared to baselines . Furthermore , it generalizes well to real users . This illustrates that we can learn useful adaptation strategies without pre - collected user data or extensive task knowledge . ACKNOWLEDGMENTS Left empty for review . REFERENCES [ 1 ] John R Anderson , Michael Matessa , and Christian Lebiere . 1997 . ACT - R : A theory of higher level cognition and its relation to visual attention . Human ‚Äì Computer Interaction 12 , 4 ( 1997 ) , 439 ‚Äì 462 . [ 2 ] Gilles Bailly , Eric Lecolinet , and Laurence Nigay . 2016 . Visual Menu Techniques . ACM Comput . Surv . 49 , 4 , Article 60 ( dec 2016 ) , 41 pages . https : / / doi . org / 10 . 1145 / 3002171 [ 3 ] Pauline M Berry , Melinda Gervasio , Bart Peintner , and Neil Yorke - Smith . 2011 . PTIME : Personalized assistance for calendaring . ACM Transactions on Intelligent Systems and Technology ( TIST ) 2 , 4 ( 2011 ) , 1 ‚Äì 22 . [ 4 ] Wauter Bosma and Elisabeth Andr√© . 2004 . Exploiting Emotions to Disambiguate Dialogue Acts . In Proceedings of the 9th International Conference on Intelligent User Interfaces ( Funchal , Madeira , Portugal ) ( IUI ‚Äô04 ) . Association for Computing Machinery , New York , NY , USA , 85 ‚Äì 92 . https : / / doi . org / 10 . 1145 / 964442 . 964459 [ 5 ] Matthew Michael Botvinick . 2012 . Hierarchical reinforcement learning and decision making . Current opinion in neurobiology 22 , 6 ( 2012 ) , 956 ‚Äì 962 . [ 6 ] Greg Brockman , Vicki Cheung , Ludwig Pettersson , Jonas Schneider , John Schulman , Jie Tang , and Wojciech Zaremba . 2016 . OpenAI Gym . arXiv : 1606 . 01540 [ cs . LG ] [ 7 ] Dermot Browne , Peter Totterdell , and Mike Norman . 2016 . Adaptive user interfaces . Elsevier . 19 , , Langerak , et al . [ 8 ] S Card , T Moran , and A Newell . 1983 . T he Psychology of Human Computer Interaction . [ 9 ] Stuartk Card , THOMASP MORAN , and Allen Newell . 1986 . The model human processor - An engineering model of human performance . Handbook of perception and human performance . 2 , 45 ‚Äì 1 ( 1986 ) . [ 10 ] Stuart K . Card , Thomas P . Moran , and Allen Newell . 1980 . The Keystroke - Level Model for User Performance Time with Interactive Systems . Commun . ACM 23 , 7 ( jul 1980 ) , 396 ‚Äì 410 . https : / / doi . org / 10 . 1145 / 358886 . 358895 [ 11 ] NoshabaCheema , LauraAFrey - Law , KouroshNaderi , JaakkoLehtinen , PhilippSlusallek , andPerttuH√§m√§l√§inen . 2020 . Predictingmid - airinteraction movements and fatigue using deep reinforcement learning . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 13 . [ 12 ] Minmin Chen , Alex Beutel , Paul Covington , Sagar Jain , Francois Belletti , and Ed H Chi . 2019 . Top - K Off - Policy Correction for a REINFORCE Recommender System . In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining ( WSDM ‚Äô19 ) . ACM , 456 ‚Äì 464 . https : / / doi . org / 10 . 1145 / 3289600 . 3290999 [ 13 ] Xiuli Chen , Gilles Bailly , Duncan P Brumby , Antti Oulasvirta , and Andrew Howes . 2015 . The emergence of interactive behavior : A model of rational menu search . In Proceedings of the 33rd annual ACM conference on human factors in computing systems . 4217 ‚Äì 4226 . [ 14 ] Sammy Christen , Lukas Jendele , Emre Aksan , and Otmar Hilliges . 2021 . Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning . IEEE Robotics and Automation Letters 6 , 2 ( 2021 ) , 3623 ‚Äì 3630 . https : / / doi . org / 10 . 1109 / LRA . 2021 . 3060403 [ 15 ] Sammy Christen , Stefan Stevsic , and Otmar Hilliges . 2019 . Demonstration - Guided Deep Reinforcement Learning of Control Policies for Dexterous Human - Robot Interaction . In International Conference on Robotics and Automation ( ICRA ) . [ 16 ] Andy Cockburn , Carl Gutwin , and Saul Greenberg . 2007 . A Predictive Model of Menu Performance . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI ‚Äô07 ) . Association for Computing Machinery , New York , NY , USA , 627 ‚Äì 636 . https : / / doi . org / 10 . 1145 / 1240624 . 1240723 [ 17 ] Andy Cockburn , Carl Gutwin , and Saul Greenberg . 2007 . A Predictive Model of Menu Performance . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI ‚Äô07 ) . Association for Computing Machinery , New York , NY , USA , 627 ‚Äì 636 . https : / / doi . org / 10 . 1145 / 1240624 . 1240723 [ 18 ] Gabriel V Cruz Jr , Yunshu Du , and Matthew E Taylor . 2017 . Pre - training neural networks with human demonstrations for deep reinforcement learning . arXiv preprint arXiv : 1709 . 04083 ( 2017 ) . [ 19 ] Peng Dai , Christopher Lin , Mausam Mausam , and Daniel Weld . 2013 . POMDP - based control of workflows for crowdsourcing . Artificial Intelligence 202 ( 09 2013 ) , 52 ‚Äì 85 . https : / / doi . org / 10 . 1016 / j . artint . 2013 . 06 . 002 [ 20 ] Stephanie Denison , Elizabeth Bonawitz , Alison Gopnik , and Thomas L Griffiths . 2013 . Rational variability in children‚Äôs causal inferences : The sampling hypothesis . Cognition 126 , 2 ( 2013 ) , 285 ‚Äì 300 . [ 21 ] Andrew T Duchowski , Krzysztof Krejtz , Izabela Krejtz , Cezary Biele , Anna Niedzielska , Peter Kiefer , Martin Raubal , and Ioannis Giannopoulos . 2018 . The index of pupillary activity : Measuring cognitive load vis - √† - vis task difficulty with pupil oscillation . In Proceedings of the 2018 CHI conference on human factors in computing systems . 1 ‚Äì 13 . [ 22 ] Andrew Faulring , Brad Myers , Ken Mohnkern , Bradley Schmerl , Aaron Steinfeld , John Zimmerman , Asim Smailagic , Jeffery Hansen , and Daniel Siewiorek . 2010 . Agent - AssistedTaskManagementThatReducesEmailOverload . In Proceedingsofthe15thInternationalConferenceonIntelligentUser Interfaces ( Hong Kong , China ) ( IUI ‚Äô10 ) . Association for Computing Machinery , New York , NY , USA , 61 ‚Äì 70 . https : / / doi . org / 10 . 1145 / 1719970 . 1719980 [ 23 ] Leah Findlater , Karyn Moffatt , Joanna McGrenere , and Jessica Dawson . 2009 . Ephemeral Adaptation : The Use of Gradual Onset to Improve Menu Selection Performance . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Boston , MA , USA ) ( CHI ‚Äô09 ) . Association for Computing Machinery , New York , NY , USA , 1655 ‚Äì 1664 . https : / / doi . org / 10 . 1145 / 1518701 . 1518956 [ 24 ] Paul M Fitts . 1954 . The information capacity of the human motor system in controlling the amplitude of movement . Journal of experimental psychology 47 , 6 ( 1954 ) , 381 . [ 25 ] Michael J Frank and David Badre . 2012 . Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 : computational analysis . Cerebral cortex 22 , 3 ( 2012 ) , 509 ‚Äì 526 . [ 26 ] Milica Ga≈°iƒá and Steve Young . 2014 . Gaussian processes for POMDP - based dialogue manager optimization . IEEE Transactions on Audio , Speech and Language Processing 22 , 1 ( 2014 ) , 28 ‚Äì 40 . https : / / doi . org / 10 . 1109 / TASL . 2013 . 2282190 [ 27 ] Christoph Gebhardt , Brian Hecox , Bas van Opheusden , Daniel Wigdor , James Hillis , Otmar Hilliges , and Hrvoje Benko . 2019 . Learning Cooperative Personalized Policies from Gaze Data . In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology ( New Orleans , LA , USA ) ( UIST ‚Äô19 ) . ACM , New York , NY , USA , 10 . https : / / doi . org / 10 . 1145 / 3332165 . 3347933 [ 28 ] Christoph Gebhardt and Otmar Hilliges . 2021 . Optimal Control to Support High - Level User Goals in Human - Computer Interaction . In Artificial Intelligence for Human Computer Interaction : A Modern Approach . Springer , 33 ‚Äì 72 . [ 29 ] Christoph Gebhardt , Antti Oulasvirta , and Otmar Hilliges . 2021 . Hierarchical Reinforcement Learning as a Model of Human Task Interleaving . Computational Brain and Behavior ( 2021 ) . https : / / arxiv . org / pdf / 2001 . 02122 . pdf [ 30 ] Samuel J Gershman , Eric J Horvitz , and Joshua B Tenenbaum . 2015 . Computational rationality : A converging paradigm for intelligence in brains , minds , and machines . Science 349 , 6245 ( 2015 ) , 273 ‚Äì 278 . [ 31 ] Samuel J Gershman , Edward Vul , and Joshua B Tenenbaum . 2012 . Multistability and perceptual inference . Neural computation 24 , 1 ( 2012 ) , 1 ‚Äì 24 . [ 32 ] Dorota Glowacka . 2019 . Bandit algorithms in recommender systems . In Proceedings of the 13th ACM Conference on Recommender Systems . 574 ‚Äì 575 . [ 33 ] Yves Guiard and Olivier Rioul . 2015 . A mathematical description of the speed / accuracy trade - off of aimed movement . In Proceedings of the 2015 British HCI Conference . 91 ‚Äì 100 . 20 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , [ 34 ] William E Hick . 1952 . On the rate of gain of information . Quarterly Journal of experimental psychology 4 , 1 ( 1952 ) , 11 ‚Äì 26 . [ 35 ] Eric Horvitz , Jack Breese , David Heckerman , David Hovel , and Koos Rommelse . 1998 . The Lumi√®Re Project : Bayesian User Modeling for Inferring the Goals and Needs of Software Users . In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence ( Madison , Wisconsin ) ( UAI‚Äô98 ) . Morgan Kaufmann Publishers Inc . , San Francisco , CA , USA , 256 ‚Äì 265 . [ 36 ] Ronald A Howard . 1960 . Dynamic programming and markov processes . ( 1960 ) . [ 37 ] Andrew Howes , Xiuli Chen , Aditya Acharya , and Richard L Lewis . 2018 . Interaction as an emergent property of a Partially Observable Markov Decision Process . Computational interaction ( 2018 ) , 287 ‚Äì 310 . [ 38 ] Zehong Hu , Yitao Liang , Jie Zhang , Zhao Li , and Yang Liu . 2018 . Inference aided reinforcement learning for incentive mechanism design in crowdsourcing . In Advances in Neural Information Processing Systems ( NIPS ‚Äô18 ) . 5508 ‚Äì 5518 . https : / / arxiv . org / abs / 1806 . 00206 [ 39 ] AleksiIkkala , FlorianFischer , MarkusKlar , MiroslavBachinski , ArthurFleig , AndrewHowes , PerttuH√§m√§l√§inen , J√∂rgM√ºller , RoderickMurray - Smith , and Antti Oulasvirta . 2022 . Breathing Life Into Biomechanical User Models . ( 2022 ) . [ 40 ] Jussi Jokinen , Aditya Acharya , Mohammad Uzair , Xinhui Jiang , and Antti Oulasvirta . 2021 . Touchscreen Typing as Optimal Supervisory Control . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems ( CHI ‚Äô21 ) . ACM . https : / / userinterfaces . aalto . fi / touchscreen - typing / [ 41 ] Jussi PP Jokinen , Tuomo Kujala , and Antti Oulasvirta . 2021 . Multitasking in driving as optimal adaptation under uncertainty . Human factors 63 , 8 ( 2021 ) , 1324 ‚Äì 1341 . [ 42 ] Ioannis Kangas , Maud Schwoerer , and Lucas Bernardi . 2022 . Scalable User Interface Optimization Using Combinatorial Bandits . In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 3375 ‚Äì 3379 . [ 43 ] Davis E Kieras and Davis E Meyer . 1997 . An overview of the EPIC architecture for cognition and performance with application to human - computer interaction . Human ‚Äì Computer Interaction 12 , 4 ( 1997 ) , 391 ‚Äì 438 . [ 44 ] JaninKoch , Andr√©sLucero , LenaHegemann , andAnttiOulasvirta . 2019 . MayAI ? Designideationwithcooperativecontextualbandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 12 . [ 45 ] Sucheta V Kolekar , Sriram G Sanjeevi , and DS Bormane . 2010 . Learning style recognition using artificial neural network for adaptive user interface in e - learning . In 2010 IEEE International conference on computational intelligence and computing research . IEEE , 1 ‚Äì 5 . [ 46 ] Yuki Koyama , Daisuke Sakamoto , and Takeo Igarashi . 2014 . Crowd - powered parameter analysis for visual design exploration . Proceedings of the 27th annual ACM symposium on User interface software and technology - UIST ‚Äô14 ( 2014 ) , 65 ‚Äì 74 . https : / / doi . org / 10 . 1145 / 2642918 . 2647386 [ 47 ] Yuki Koyama , Daisuke Sakamoto , and Takeo Igarashi . 2016 . SelPh : Progressive Learning and Support of Manual Photo Color Enhancement . Proc . of CHI ‚Äô16 ( 2016 ) . https : / / doi . org / 10 . 1145 / 2858036 . 2858111 [ 48 ] Yezdi Lashkari , Max Metral , and Pattie Maes . 1997 . Collaborative interface agents . Readings in agents ( 1997 ) , 111 ‚Äì 116 . [ 49 ] Katri Leino , Kashyap Todi , Antti Oulasvirta , and Mikko Kurimo . 2019 . Computer - Supported Form Design Using Keystroke - Level Modeling with Reinforcement Learning . In Proceedings of the 24th International Conference on Intelligent User Interfaces : Companion ( Marina del Ray , California ) ( IUI ‚Äô19 ) . Association for Computing Machinery , New York , NY , USA , 85 ‚Äì 86 . https : / / doi . org / 10 . 1145 / 3308557 . 3308704 [ 50 ] Eric Liang , Richard Liaw , Philipp Moritz , Robert Nishihara , Roy Fox , Ken Goldberg , Joseph E . Gonzalez , Michael I . Jordan , and Ion Stoica . 2018 . RLlib : Abstractions for Distributed Reinforcement Learning . arXiv : 1712 . 09381 [ cs . AI ] [ 51 ] Elad Liebman , Maytal Saar - Tsechansky , and Peter Stone . 2015 . DJ - MC : A Reinforcement - Learning Agent for Music Playlist Recommendation . In Proceedingsofthe2015InternationalConferenceonAutonomousAgentsandMultiagentSystems ( AAMAS‚Äô15 ) . 591 ‚Äì 599 . https : / / arxiv . org / abs / 1401 . 1880 [ 52 ] David Lindlbauer , Anna Maria Feit , and Otmar Hilliges . 2019 . Context - aware online adaptation of mixed reality interfaces . In Proceedings of the 32nd annual ACM symposium on user interface software and technology . 147 ‚Äì 160 . [ 53 ] Feng Liu , Ruiming Tang , Xutao Li , Weinan Zhang , Yunming Ye , Haokun Chen , Huifeng Guo , and Yuzhou Zhang . 2018 . Deep reinforcement learning based recommendation with explicit user - item interactions modeling . arXiv preprint arXiv : 1810 . 12027 ( 2018 ) . https : / / arxiv . org / abs / 1810 . 12027 [ 54 ] J Derek Lomas , Jodi Forlizzi , Nikhil Poonwala , Nirmal Patel , Sharan Shodhan , Kishan Patel , Ken Koedinger , and Emma Brunskill . 2016 . Interface design optimization as a multi - armed bandit problem . In Proceedings of the 2016 CHI conference on human factors in computing systems . 4142 ‚Äì 4153 . [ 55 ] Wendy Mackay . 2000 . Responding to cognitive overload : Co - adaptation between users and technology . Intellectica 30 , 1 ( 2000 ) , 177 ‚Äì 193 . [ 56 ] Pattie Maes . 1995 . Agents that reduce work and information overload . In Readings in human ‚Äì computer interaction . Elsevier , 811 ‚Äì 821 . [ 57 ] Eric McCreath , Judy Kay , and Elisabeth Crawford . 2006 . IEMS - an approach that combines handcrafted rules with learnt instance based rules . Aust . J . Intell . Inf . Process . Syst . 9 , 1 ( 2006 ) , 40 ‚Äì 53 . [ 58 ] Abhinav Mehrotra and Robert Hendley . 2015 . Designing Content - driven Intelligent Notification Mechanisms for Mobile Applications . ( 2015 ) , 813 ‚Äì 824 . [ 59 ] Antti Oulasvirta , Niraj Ramesh Dayama , Morteza Shiripour , Maximilian John , and Andreas Karrenbauer . 2020 . Combinatorial Optimization of Graphical User Interface Designs . Proc . IEEE 108 , 3 ( 2020 ) , 434 ‚Äì 464 . https : / / doi . org / 10 . 1109 / JPROC . 2020 . 2969687 [ 60 ] Antti Oulasvirta , Anna Feit , Perttu L√§hteenlahti , and Andreas Karrenbauer . 2017 . Computational Support for Functionality Selection in Interaction Design . 24 , 5 , Article 34 ( oct 2017 ) , 30 pages . https : / / doi . org / 10 . 1145 / 3131608 [ 61 ] Antti Oulasvirta , Jussi PP Jokinen , and Andrew Howes . 2022 . Computational Rationality as a Theory of Interaction . In CHI Conference on Human Factors in Computing Systems . 1 ‚Äì 14 . [ 62 ] Antti Oulasvirta , Per Ola Kristensson , Xiaojun Bi , and Andrew Howes . 2018 . Computational interaction . Oxford University Press . [ 63 ] Seonwook Park , Christoph Gebhardt , Roman R√§dle , Anna Maria Feit , Hana Vrzakova , Niraj Ramesh Dayama , Hui - Shyong Yeo , Clemens N Klokmose , Aaron Quigley , Antti Oulasvirta , et al . 2018 . Adam : Adapting multi - user interfaces for collaborative environments in real - time . In Proceedings of the 21 , , Langerak , et al . 2018 CHI conference on human factors in computing systems . 1 ‚Äì 14 . [ 64 ] Veljko Pejovic and Mirco Musolesi . 2014 . InterruptMe : Designing Intelligent Prompting Mechanisms for Pervasive Applications . Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing ( 2014 ) , 897 ‚Äì 908 . https : / / doi . org / 10 . 1145 / 2632048 . 2632062 [ 65 ] Athanasios S Polydoros and Lazaros Nalpantidis . 2017 . Survey of model - based reinforcement learning : Applications on robotics . Journal of Intelligent & Robotic Systems 86 , 2 ( 2017 ) , 153 ‚Äì 173 . [ 66 ] Siddharth Reddy , Anca D Dragan , and Sergey Levine . 2018 . Shared autonomy via deep reinforcement learning . arXiv preprint arXiv : 1802 . 01744 ( 2018 ) . [ 67 ] Charles Rich , Candy Sidner , Neal Lesh , Andrew Garland , Shane Booth , and Markus Chimani . 2005 . DiamondHelp : A collaborative interface framework for networked home appliances . In 25th IEEE International Conference on Distributed Computing Systems Workshops . IEEE , 514 ‚Äì 519 . [ 68 ] Charles Rich and Candace L Sidner . 1998 . COLLAGEN : A collaboration manager for software interface agents . In Computational Models of Mixed - Initiative Interaction . Springer , 149 ‚Äì 184 . [ 69 ] Fabio Rizzoglio , Maura Casadio , Dalia De Santis , and Ferdinando A . Mussa - Ivaldi . 2021 . Building an adaptive interface via unsupervised tracking of latent manifolds . Neural Networks 137 ( 2021 ) , 174 ‚Äì 187 . https : / / doi . org / 10 . 1016 / j . neunet . 2021 . 01 . 009 [ 70 ] Dario D Salvucci . 2001 . An integrated model of eye movements and visual encoding . Cognitive Systems Research 1 , 4 ( 2001 ) , 201 ‚Äì 220 . [ 71 ] John Schulman , Filip Wolski , Prafulla Dhariwal , Alec Radford , and Oleg Klimov . 2017 . Proximal Policy Optimization Algorithms . arXiv : 1707 . 06347 [ cs . LG ] [ 72 ] Andrew Sears and Ben Shneiderman . 1994 . Split Menus : Effectively Using Selection Frequency to Organize Menus . ACM Trans . Comput . - Hum . Interact . 1 , 1 ( mar 1994 ) , 27 ‚Äì 51 . https : / / doi . org / 10 . 1145 / 174630 . 174632 [ 73 ] Bobak Shahriari , Kevin Swersky , Ziyu Wang , Ryan P Adams , and Nando De Freitas . 2015 . Taking the human out of the loop : A review of Bayesian optimization . Proc . IEEE 104 , 1 ( 2015 ) , 148 ‚Äì 175 . [ 74 ] Lloyd S Shapley . 1953 . Stochastic games . Proceedings of the national academy of sciences 39 , 10 ( 1953 ) , 1095 ‚Äì 1100 . [ 75 ] Jianqiang Shen , Erin Fitzhenry , and Thomas G Dietterich . 2009 . Discovering frequent work procedures from resource connections . In Proceedings of the 14th international conference on Intelligent user interfaces . 277 ‚Äì 286 . [ 76 ] Jianqiang Shen , Jed Irvine , Xinlong Bao , Michael Goodman , Stephen Kolibaba , Anh Tran , Fredric Carl , Brenton Kirschner , Simone Stumpf , and Thomas G Dietterich . 2009 . Detecting and correcting user activity switches : algorithms and interfaces . In Proceedings of the 14th international conference on Intelligent user interfaces . 117 ‚Äì 126 . [ 77 ] Dustin A Smith and Henry Lieberman . 2010 . The why UI : using goal networks to improve user interfaces . In Proceedings of the 15th international conference on Intelligent user interfaces . 377 ‚Äì 380 . [ 78 ] Harold Soh , Scott Sanner , Madeleine White , and Greg Jamieson . 2017 . Deep sequential recommendation for personalized adaptive user interfaces . In Proceedings of the 22nd international conference on intelligent user interfaces . 589 ‚Äì 593 . [ 79 ] Constantine Stephanidis , Charalampos Karagiannidis , and Adamantios Koumpis . 1997 . Decision making in intelligent user interfaces . In Proceedings of the 2nd international conference on Intelligent user interfaces . 195 ‚Äì 202 . [ 80 ] Pei - Hao Su , Pawel Budzianowski , Stefan Ultes , Milica Gasic , and Steve Young . 2017 . Sample - efficient actor - critic reinforcement learning with supervised data for dialogue management . arXiv preprint arXiv : 1707 . 00130 ( 2017 ) . https : / / arxiv . org / abs / 1707 . 00130 [ 81 ] Richard S Sutton , Andrew G Barto , et al . 1998 . Introduction to reinforcement learning . ( 1998 ) . [ 82 ] Kashyap Todi , Gilles Bailly , Luis Leiva , and Antti Oulasvirta . 2021 . Adapting User Interfaces with Model - based Reinforcement Learning . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems ( CHI ‚Äô21 ) . ACM . https : / / userinterfaces . aalto . fi / adaptive / [ 83 ] ZhiboYang , LihanHuang , YupeiChen , ZijunWei , SeoyoungAhn , GregoryZelinsky , DimitrisSamaras , andMinhHoai . 2020 . Predictinggoal - directed human attention using inverse reinforcement learning . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 193 ‚Äì 202 . [ 84 ] Neil Yorke - Smith , Shahin Saadati , Karen L Myers , and David N Morley . 2012 . The design of a proactive personal agent for task management . International Journal on Artificial Intelligence Tools 21 , 01 ( 2012 ) , 1250004 . 22 MARLUI : Multi - Agent Reinforcement Learning for Goal - Agnostic Adaptive UIs , , Fig . 9 . An overview of positional variables used to model our user agent . p is the current user position , ùúá p and ùúé p describe the desired end point distribution , ùëë p is the Euclidean distance between the current position and ùúá p . ùë° is the selected target of the high - level policy and ùúá ùë° is the center of that target . We use ùëë p and ùúé p to compute a movement time ùëá ùëÄ . We update the environment after a low - level step with a new position p ‚Ä≤ sampled from N ( ùúá p , ùúé p ) . A CURRICULUM LEARNING We use curriculum learning for the hierarchical menu setting ( see Sec . 7 . 1 ) . Specifically , we adjust the difficulty level every time a criteria has been met by increasing the mean number of initial attribute differences . More initial attribute differences result in longer action sequences , and is therefore more complex to learn . We increase the mean by 0 . 01 every time the successful completion rate is above 90 % and the last level up was at least 10 epochs away . We randomly sample the number of attribute differences from a normal distribution and round it to the nearest integer . The mean of this distribution is the current task level divided by 100 , and the standard deviation is 1 . we clip sampled number of attribute difference to the range [ 1 , ùëõ ùëé ] , where ùëõ ùëé is the number of attributes in the setting ( in the case of photo editing 5 ) . B LEARNED USER AGENT B . 0 . 1 High - Level Decision Making . To enable the learning - based low - level motor control policy , we need to make slight changes to the decision making policy . More specifically , the high - level state space ùëÜ ùê∑ includes ùúÜ and ùë° and is defined as : ùëÜ ùê∑ = ( p , m , x , g , ùë° , ùúÜ ) , ( 8 ) which comprises : i ) the current position of the agent p ‚àà ùêº 2 , ii ) a one - hot encoding of the item - to - slot assignment m ‚àà Z ùëõ ùëñ √ó ùëõ ùë† 2 , with ùëõ ùëñ and ùëõ ùë† being the number of menu items and slots , respectively , iii ) the current input state x ‚àà Z ùëõ ùëñ 2 , iv ) the goal state g ‚àà Z ùëõ ùëñ 2 , v ) the target slot of the previous time - step ùë° ‚àà Z ùëõ ùë† 2 , and vi ) the normalized last used speed - accuracy trade - off weight ùúÜ ‚àà [ 0 , 1 ] . Here , ùêº denotes the unit interval [ 0 , 1 ] . The action space ùê¥ ùê∑ now contaings the ùúÜ , and is defined as : ùê¥ ùê∑ = (cid:16) ùë° , (cid:98) ùúÜ (cid:17) , ( 9 ) which contains the next target slot , ùë° ‚àà Z ùëõ ùë† 2 , for the low - level and the speed - accuracy weight (cid:98) ùúÜ ‚àà Z 20 , where 20 is an empirically found discretezation of the speed - accuracy trade - off space . The weight (cid:98) ùúÜ is then scaled to the range [ 0 , 1 ] . The speed - accuracy trade - off action is discretized in the action space , since mixed action spaces ( i . e . discrete and continuous ) are not readily implementable in PPO . 23 , , Langerak , et al . The reward for the high - level decision making policy consists of two weighted terms to trade - off between task com - pletion accuracy and task completion time . Therefore , the high - level policy needs to learn an optimal risk - aversenesss , we append a regularizer on ùúÜ to the cost function . ùëÖ ùê∑ = ùõº E ùëîùëë (cid:124)(cid:123)(cid:122)(cid:125) ùëñ ) ‚àí ( 1 ‚àí ùõº ) ( ùëá ùê∑ + ùëá ùëÄ ) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) ùëñùëñ ) ‚àí ùúÖ | | (cid:164) ùúÜ | | 22 + 1 success , ( 10 ) E ùëîùëë is the difference between the input state and the goal state , ùõº is a weight term , and the movement time is an output of the low - level policy . ùúÖ | | (cid:164) ùúÜ | | 22 is a weighted regularizer that penalizes a change in lambda . 1 success is an indicator function that is 1 if the task has been successfully completed and 0 otherwise . B . 0 . 2 Low - Level Motor Control . The low - level motor control policy controls the end - effector movement . In particular , given a target slot and a speed - accuracy trade - off weight , the policy selects the parameters of an end point distribution . Given the current position and the end point parameters ( mean and standard deviation ) , we compute the predicted movement time using the WHo Model [ 33 ] . The low - level policy needs to learn i ) the coordinates and dimensions of menu slots , ii ) an optimal speed - accuracy trade - off given a weight ùúÜ , a target slot , and its current position ( see Fig . 9 ) . To prevent the low - level motor control policy from correcting wrong high - level decisions , and to increase general performance , we limit the state space ùëÜ ùëÄ to strictly necessary elements with respect to the motor control task [ 14 ] : ùëÜ ùëÄ = ( p , ùë° , ùúÜ ) , ( 11 ) with the current position p ‚àà ùêº 2 , the target slot ùë° ‚àà Z ùëõ ùë† 2 , and the speed - accuracy weight ùúÜ ‚àà [ 0 , 1 ] . The action space ùê¥ ùëÄ is defined as follows : ùê¥ ùëÄ = (cid:0) ùúá p , ùúé p (cid:1) . ( 12 ) It consists of ùúá p ‚àà ùêº 2 and ùúé p ‚àà ùêº , i . e . , the mean and standard - deviation which describes the end point distribution in the unit interval . We scale the standard deviation linearly between a min and max value where the minimum value is the size of a normalized pixel width and the max value is empirically chosen to be 15 % of the screen width . Once an action is taken , we sample a new end - effector position from a normal distribution : p ‚àº N (cid:0) ùúá p , ùúé p (cid:1) . Given the predicted actions , we compute the expected movement time via the WHo model [ 33 ] , similar as in our non - learned low - level motor control policy in the main paper . The reward for the low - level motor control policy is based on the motoric speed - accuracy trade - off . Specifically , we penalize : i ) missing the target supplied by the high - level ( ¬¨ ‚Ñé ) , and ii ) the movement time ( ùëá ùëÄ ) . Furthermore , we add a penalty iii ) which amounts to the squared Euclidean distance between the center of the target ùë° and ùúá p . This incentivizes the policy to hit the desired target in the correct location . Since the penalty only considers the desired point ùúá p , it will not impact the speed - accuracy trade - off ( which is a function of ùúé p ) . The total reward is defined as follows : ùëÖ ùëÄ = ùúÜ ( ¬¨ ‚Ñé ) (cid:124)(cid:123)(cid:122)(cid:125) ùëñ ) ‚àí ( 1 ‚àí ùúÜ ) ùëá ùëÄ (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) ùëñùëñ ) ‚àí ùõΩ | | ùúá p ‚àí ùúá ùë° | | 22 (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) ùëñùëñùëñ ) , ( 13 ) where ¬¨ ‚Ñé equals 0 when the target button is hit and ‚àí 1 on a miss . A hit occurs when the newly sampled user position p is within the target ùë° , while a miss happens if the user position is outside of the target . ùõΩ is a small scalar weight . 24