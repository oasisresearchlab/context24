Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Yineng Chen † 1 Zuchao Li † 1 2 Lefei Zhang 1 2 Bo Du 1 2 Hai Zhao 3 Abstract Optimizer is an essential component for the suc - cess of deep learning , which guides the neural network to update the parameters according to the loss on the training set . SGD and Adam are two classical and effective optimizers on which researchers have proposed many variants , such as SGDM and RAdam . In this paper , we in - novatively combine the backward - looking and forward - looking aspects of the optimizer algo - rithm and propose a novel A DMETA ( A D ouble exponential M oving averag E T o A daptive and non - adaptive momentum ) optimizer framework . For backward - looking part , we propose a DEMA variant scheme , which is motivated by a metric in the stock market , to replace the common ex - ponential moving average scheme . While in the forward - looking part , we present a dynamic looka - head strategy which asymptotically approaches a set value , maintaining its speed at early stage and high convergence performance at ﬁnal stage . Based on this idea , we provide two optimizer im - plementations , A DMETA R and A DMETA S , the former based on RAdam and the latter based on SGDM . Through extensive experiments on di - verse tasks , we ﬁnd that the proposed A DMETA optimizer outperforms our base optimizers and shows advantages over recently proposed com - † Equal contribution . This work was supported by the Fundamental Research Funds for the Central Universities ( No . 2042023kf1033 ) , the Special Fund of Hubei Luojia Laboratory under Grant 220100014 , and the National Science Fund for Dis - tinguished Young Scholars under Grant 62225113 . Hai Zhao was funded by the Key Projects of National Natural Science Founda - tion of China ( U1836222 and 61733011 ) . 1 National Engineering Research Center for Multimedia Software , School of Computer Science , Wuhan University , Wuhan , 430072 , P . R . China 2 Hubei Luojia Laboratory , Wuhan 430072 , P . R . China 3 Department of Computer Science and Engineering , Shanghai Jiao Tong Univer - sity , Shanghai , 200240 , P . R . China . Correspondence to : Zuchao Li < zcli - charlie @ whu . edu . cn > . Proceedings of the 40 th International Conference on Machine Learning , Honolulu , Hawaii , USA . PMLR 202 , 2023 . Copyright 2023 by the author ( s ) . petitive optimizers . We also provide theoretical proof of these two algorithms , which veriﬁes the convergence of our proposed A DMETA . 1 . Introduction The ﬁeld of training neural network is dominated by gradi - ent decent optimizers for a long time , which use ﬁrst order method . Typical ones include SGD ( Robbins & Monro , 1951 ) and SGD with momentum ( SGDM ) ( Sutskever et al . , 2013 ) , which are simple yet efﬁcient algorithms and enjoy even better resulting convergence than many recently pro - posed optimizers . However , it suffers the disadvantage of low speed in initial stage and poor performance in sparse training datasets . This shortcoming can not be ignored since with the development of deep learning , the amount of data becomes much larger , and the model becomes much more complex . The time to train a network is also considered an important metric when evaluating an optimizer . To address this issue , optimizers with adaptive learning rate have been proposed which use nonuniform stepsizes to scale the gradi - ent while training , and the usual implementation is scaling the gradient by square roots of some kind of combination of the squared values of historical gradients . By far the most used are Adam ( Kingma & Ba , 2014 ) and AdamW ( Loshchilov & Hutter , 2017 ) due to their simplicity and high training speed in early stage . Despite their popularity , Adam and many variants like of it ( such as RAdam ( Liu et al . , 2019 ) ) is likely to achieve worse generalization ability than non - adaptive optimizers , observing that their perfor - mance quickly plateaus on validation sets . To achieve a better tradeoff , researchers have made many improvements based on SGD and Adam family optimiz - ers . One attempt is switching from adaptive learning rate methods to SGD , based on the idea of complementing each other’s advantages . However , a sudden change from one optimizer to another in a set epoch or step is not ap - plicable because different algorithms make characteristic choices at saddle points and tend to converge to ﬁnal points whose loss functions nearby have different geometry ( Im et al . , 2016 ) . Therefore , many optimizers based on this idea seek for a smooth switch . The representative ones 1 a r X i v : 2307 . 00631v1 [ c s . L G ] 2 J u l 2023 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers are AdaBound ( Luo et al . , 2019 ) and SWATS ( Keskar & Socher , 2017 ) . The second attempt is proposing new method to further accelerate SGDM , including introducing power exponent ( pbSGD ( Zhou et al . , 2020a ) ) , aggregated mo - mentum ( AggMo ( Lucas et al . , 2018 ) ) and warm restarts ( SGDR ( Loshchilov & Hutter , 2016 ) ) . The third attempt is modifying the process of optimizers with adaptive learning rate to achieve better local optimum , which is the most pop - ular ﬁeld in recent researches ( Zhuang et al . , 2020 ; Li et al . , 2020a ) . Due to space constraints , please see more related work in Appendix A . We focus in this paper on the use of historical and future information about the optimization process of the model , both of which we argue are important for models to reach their optimal points . To this end , we introduce a bidirec - tional view , backward - looking and forward - looking . In the backward - looking view , EMA is an exponentially decreas - ing weighted moving average , which is used as a trend - type indicator in terms of the optimization process . And since the training uses a mini - batch strategy , each batch is likely to have deviations from the whole , so it may mislead the model to the local optimal point . Inspired by stock market indicators , DEMA ( Mulloy , 1994 ) is an exponential average calculated on the traditional EMA and current input , which can effectively maintain the trend while reducing the impact caused by short - term bias . We thus replace the traditional exponential moving average ( EMA ) with double exponen - tial moving average ( DEMA ) . It is worth noting that our usage is not equivalent to the original DEMA , but rather a variant of it . In the forward - looking part , since we observe that a constant weight adopted by the original Lookahead optimizer ( Zhang et al . , 2019 ) to control the scale of fast weights and slow weights in each synchronization period makes the early stage training slow and lossy , we propose a new dynamic strategy which adopts an asymptotic weight for improvement . By applying these two ideas , we pro - pose A DMETA optimizer with A DMETA R and A DMETA S implementations based on RAdam and SGDM respectively . Extensive experiments have been conducted on computer vision ( CV ) , natural language processing ( NLP ) and au - dio processing tasks , which demonstrate that our method achieves better convergence results compared to other re - cently proposed optimizers . Further analysis shows that A DMETA S achieves higher performance than SGDM and A DMETA R achieves better convergence results and main - tains high speed in the initial stage compared to other adaptive learning rate methods . We further ﬁnd that the DEMA and dynamic looking strategy can improve perfor - mance compared to EMA and constant strategy , respec - tively . In addition , we provide convergence proof of our proposed A DMETA in convex and non - convex optimiza - tions . The code is available at https : / / github . com / Chernyn / Admeta - Optimizer . 2 . Admeta 2 . 1 . Background The role of the optimizer in model training is to minimize the loss on the training set and thus drive the learning of model parameters . Formally , consider a loss function f : R d → R that is bounded below greater than zero , where R represents the ﬁeld of real numbers , d denotes the dimension of the parameter and thus R d denotes d - dimensional Euclidean space . The optimization problem can be formulated as : min θ ∈F f ( θ ) , where θ indicates a parameter whose domain is F and F ⊂ R d . If we deﬁne the optimum parameter of the above loss function as θ ∗ , then the optimization objective can be written as : θ ∗ = arg min θ ∈F f ( θ ) . ( 1 ) Optimizers iteratively update parameters to make them close to the optimum as training step t increases , that is to make : lim t →∞ (cid:107) θ t − θ ∗ (cid:107) = 0 . The stochastic gradient algorithm SGD ( Robbins & Monro , 1951 ) optimizes f by iteratively updating parameter θ t at step t in the opposite direction of the stochastic gradient g ( θ t − 1 ; ξ t ) where ξ t is the input variables of the t - th mini - batch in training datasets . For the sake of clarity , we ab - breviate g ( θ t − 1 ; ξ t ) as g t for the rest of the paper unless speciﬁed . SGD optimization aims to calculate the updated model parameters based on the previous model parameters , the current gradient and the learning rate . Deﬁne the learn - ing rate as α t , the update process is summarized as follows : θ t = θ t − 1 − α t g t . ( 2 ) Original SGD tends to vibrate along the process due to the mini - batch strategy and not using of past gradients . What’s more , this disadvantage also results in its long - time plateaus in valleys and saddle points , thus slowing the speed . To smooth the oscillation and speed up convergence rate , mo - mentum , also known as Polyak’s Heavy Ball ( Polyak , 1964 ) , is introduced to modify SGD . Momentum at step t is often denoted as m t and obtained by iterative calculation with a dampening coefﬁcient β . Thus , the update process of SGD with momentum ( SGDM ) ( Sutskever et al . , 2013 ) becomes as follows : m t = βm t − 1 + ( 1 − β ) g t , ( 3 ) θ t = θ t − 1 − α t m t , ( 4 ) Although momentum works well , the uniform stepsize on every parameter is also another factor to limit the speed , especially in large datasets and sparse datasets . To further accelerate the update , adaptive learning rate optimizer is introduced which adopts an individual stepsize for each parameter based on their unique update process . Since a 2 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers smoothing mechanism is employed in the calculation of stepsize , two dampening coefﬁcients , β 1 and β 2 , are intro - duced for balancing the current and historical information . Adam ( Kingma & Ba , 2014 ) , a typical adaptive learning rate optimizer , is implemented as follows : m t = β 1 m t − 1 + ( 1 − β 1 ) g t , ( 5 ) v t = β 2 v t − 1 + ( 1 − β 2 ) g 2 t , ( 6 ) θ t = θ t − 1 − α t m t / √ v t , ( 7 ) where m t indicates the ﬁrst momentum , corresponding to the momentum in SGDM ; v t indicates the second momen - tum . To emphasize the functionality of v t , we call it adaptive item for the rest of the paper . Adam may sometimes converge to bad local optimum , partly due to its large variance in the early stage . To ﬁx this issue , RAdam ( Liu et al . , 2019 ) introduces a further rectiﬁed item r t and splits the update process into two sub - processes sequentially connected : ρ ∞ = 2 / ( 1 − β 2 ) − 1 , ( 8 ) ρ t = ρ ∞ − 2 tβ t 2 / ( 1 − β t 2 ) , ( 9 ) r t ← (cid:115) ( ρ t − 4 ) ( ρ t − 2 ) ρ ∞ ( ρ ∞ − 4 ) ( ρ ∞ − 2 ) ρ t , ( 10 ) θ t = (cid:26) θ t − 1 − α t m t , ρ t ≤ 4 θ t − 1 − α t r t m t / √ v t , ρ t > 4 . ( 11 ) 2 . 2 . Backward - looking In fact , the calculation of momentum m t in Eq . ( 3 ) and Eq . ( 5 ) is an exponential moving average ( EMA ) on gradient g t . EMA , also known as exponential weighted moving average , can be used to estimate the local mean value of variables , so that the update of variables is related to historical values over a period of time . Formally , EMA is expressed as : S t = βS t − 1 + ( 1 − β ) p t , ( 12 ) where the variable S is denoted as S t at time t and p t are the newly assigned values . Particularly , S t = p t without using EMA . In Eq . ( 3 ) , SGDM employs EMA to take a moving average of the past gradients . While in Eq . ( 5 ) , Adam and RAdam further apply EMA on the square of past gradients to construct the adaptive item . In the EMA , the moving average of the variable S at time t is roughly equal to the average of the values p over the past 1 / ( 1 − β ) steps . This makes the moving average vary more at the beginning , so a bias correction is proposed and used in Adam ( Eq . ( 7 ) ) and in RAdam ( Eq . ( 11 ) ) when ρ > 4 . EMA can be regarded as obtaining the average values of the variables over time . Compared with the direct assignment of values to variables , the change curve of the values obtained by moving average is smoother and less jittery , and the moving average does not ﬂuctuate greatly when inputting outliers , which is very important for the optimization using sampled mini - batch . Although efﬁcient , EMA is not nec - essarily the best strategy for using historical information when it comes to the backward - looking part . Although it can effectively suppress the vibration caused by mini - batch training by performing the moving average on g t , it also brings a lag time that affects the convergence speed and increases with the length of the moving average . What’s more , it can result in the overshoot problem ( An et al . , 2018 ) , one possible reason is that EMA might make the wrong use of historical gradients in the ﬁnal stage and thus have a “burden” to converge to optimum . Double Exponential Moving Average ( DEMA ) , ﬁrst pro - posed by ( Mulloy , 1994 ) , is a faster moving average strategy and was invented to reduce the lag time of EMA . Thus , mo - tivated by the advantage of DEMA , we developed a DEMA variant for the model optimization . It is worth noting DEMA is not simply taking a moving average of historical gradients twice , instead , it takes the moving average of the linear com - bination of the current gradient the moving average of past gradients . The form of our DEMA variant can be written as : DEMA = EMA out ( µ EMA in + κg t ) , ( 13 ) where µ and κ are coefﬁcients that control the scale of current gradient and only depend on β . From the formula EMA = Σ ni = 1 β n − i g i , past gradients follow a ﬁxed proportionality , that is , the ratio of gradient weight at one time to gradient weight at the previous time is β . Due to the use of minibatch training strategy , the input is randomly sampled . The effect of each minibatch towards optimization is varied . Therefore , applying a ﬁxed propor - tionality to past gradients is not a reasonable approach since it does not take into account the changeable situation . The disadvantage of overshoot that EMA usually has may also be caused by the above reasons ( An et al . , 2018 ) . Thus , we deal with the relationship between the historical gradients and the current gradient more ﬂexibly by further controlling the proportion of past gradients . Our design of coefﬁcients in DEMA is also for this purpose . Based on Eq . ( 13 ) , our actual implementation on algorithm is : I t = λI t − 1 + g t , ( 14 ) h t = κg t + µI t + ν , ( 15 ) m t = βm t − 1 + ( 1 − β ) h t , ( 16 ) where I t is the output of EMA in with a 0 initial value and m t is the output of EMA out also initiated with 0 . λ and β are dampening coefﬁcients of inner EMA and outer EMA respectively , ν is a bias item , which is set to a small amount 3 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers that decreases exponentially to 0 and chosen as λ t g 1 . The bias item does not affect the convergence proof , so for the sake of brevity , it is omitted for the rest of this paper and the details can be seen in the code . Please refer to Appendix B for more comparison and discussion between EMA and DEMA . 2 . 3 . Forward - looking Focusing on gradient history , that is , backward - looking , the optimizer is conducive to alleviating the vibration problem in the optimization process and preventing it from being misled by local noise information . However , since the opti - mization problem of the deep neural network is very com - plex , the optimizer can make the optimization process more robust by pre - exploration , so as to obtain better optimization results , which is called forward - looking . Based on Reptile algorithm and advances in understanding the loss surface , ( Zhang et al . , 2019 ) proposed Lookahead optimizer , which introduces two update processes and aver - ages fast and slow weights periodically . The algorithm can be expressed as the cycle of the following process : Pre - exploration : θ t = O PTIM ( θ t − 1 ) Synchronization : ( every k steps ) φ t = φ t − k + η ( θ t − 1 − φ t − k ) θ t = φ t where O PTIM ( · ) denotes a chosen optimizer , k denotes the synchronization period , or in other words , the period of forward - looking , φ t denotes the slow weights , θ t denotes the fast weight updated with a chosen optimizer , and η is a constant coefﬁcient controlling the proportion of slow weights and fast weights in each synchronization . Generally , the chosen optimizer can be arbitrary . We can get an intuitive explanation of Lookahead optimizer from the pseudo code above : Guided by fast weight θ t , the slow weight φ t updates by taking linear interpolation between itself and the fast weight . Every time the fast weight updates k steps , the slow weight updates 1 step . The update direction of slow weight can be regarded as θ t − φ t from the equation . Therefore , η can also be interpreted as the stepsize of slow weight in each synchronization . In order not to be confused with the stepsize of fast weight , we rename the stepsize of slow weight as stepsize s . The recommended value of η in ( Zhang et al . , 2019 ) is 0 . 5 and 0 . 8 . In the original Lookahead optimizer implementation , the fast and slow optimization processes were synchronized according to a given period , and parameters are fused at a ﬁxed ratio during synchronization . However , optimization is a continuous process . In different optimization stages , fast optimization steps have different guiding effects on parameters . We argue that using ﬁxed stepsize s in each synchronization is not an optimal strategy , and may even lead to negative effects . For this consideration , we turn the constant η into a η t that changes over step monotonously and asymptotically . Generally , η t is a function that starts from 1 and converges to a set value and depends only on the step t . In this setting , the proportion of slow weights increases and this part gradually turns into the original Lookahead method . In other words , the slow weights in our method adopt a faster stepsize s at the beginning , and it asymptotically slows down as processing . Speciﬁcally , we deﬁne two asymptotic functions for η t : η t = 0 . 5 ∗ (cid:18) 1 + 1 0 . 01 √ t + 1 (cid:19) , ( 17 ) η t = 0 . 8 ∗ (cid:18) 1 + 1 0 . 1 √ t + 3 . 8 (cid:19) , ( 18 ) thus we call this as dynamic asymptotic lookahead . The two functions are designed to turn η t from 1 to 0 . 5 and 0 . 8 respectively . Notably , these asymptotic functions may not be the best . We just ﬁnd that it works well and maybe future work can be done to investigate a more suitable one . For the sake of clarity , we will use the latter one in the rest of the paper and the results of experiments trained from scratch are based on this function unless speciﬁed . To illustrate the advantages of our dynamic lookahead strat - egy over no lookahead and the original constant lookahead , we give an optimization example in Figure 1 . In region 1 , which is around the early stage , the direction of the update is relatively stable and a large stepsize s is needed . θ 1 → θ 4 denotes the update of fast weights . A constant lookahead method will slow the update process in each synchroniza - tion period , as can be seen in θ 1 → θ 2 . In our method , fast weights share more proportion in each synchronization period in early stage , thus updating faster , as can be seen in θ 1 → θ 3 . In region 2 , which is around the ﬁnal stage , the direction of the update is relatively oscillated , and a small stepsize s is needed . Fast weights tend to overshoot the optimum , as can be seen in θ 5 → θ 8 . Lookahead optimizer can achieve bet - ter convergence result than general algorithm as it averages the weights to make them more close to the optimum point , as can be seen in θ 5 → θ 6 . In our method , the proportion of fast weights has already been reduced asymptotically to a set value , thus can achieve similar efﬁcacy as Lookahead optimizer as can be seen in θ 5 → θ 7 . From these analyses , we demonstrate that our dynamic lookahead strategy method improves the robustness of train - ing . 4 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers ① ② θ 1 x f ( x ) θ 2 θ 3 θ 4 θ 5 θ 6 θ 7 θ * θ 8 SGD SGD with Lookahead SGD with dynamic Lookahead ( Ours ) | θ 1 - θ * | is large , a large η scan maintain the convergence speed | θ 5 - θ * | is small , a relative small η can achieve better convergence Figure 1 . Comparison between no lookahead , constant lookahead and dynamic lookahead . 2 . 4 . Implementations of AdmetaR and AdmetaS Since optimizers of the Adam family and SGD family have their own advantages and disadvantages , and the bidirec - tional looking optimizer framework and improvement we propose do not have too many restrictions on the basic opti - mizer , we have implemented improved versions A DMETA R and A DMETA S based on RAdam and SGDM optimizer . The ﬁnal algorithm forms are shown in Algorithm 1 and 2 . De - tailed proof of convergence and convergence rate for our A DMETA R and A DMETA S is put in Appendix C and D . Notations : • α t : learning rate at step t • λ , β , β 1 , β 2 : the momentum coefﬁcients • (cid:15) : a small value used to avoid a zero denominator • k : synchronization period • (cid:81) F , M ( y ) = argmin x ∈F | | M 1 / 2 ( x − y ) | | • µ = 25 − 10 (cid:0) λ + 1 λ (cid:1) , κ = 10 λ − 9 3 . Experiments In this section , we demonstrate the effectiveness of our optimizer by turning to an empirical exploration of different datasets and different models to compare some popular optimizers . Speciﬁcally , we conduct experiments on typical CV , NLP , and audio processing tasks . Inﬂuenced by the Transformer structure , models are becoming deeper and Algorithm 1 A DMETA R Optimizer . All operations are element - wise . Initialize θ 1 ∈ F , φ 0 ← 0 , m 0 ← 0 , v 0 ← 0 , I 0 ← 0 , t ← 0 for t = 1 , 2 , . . . do t ← t + 1 g t ← ∇ t f t ( θ t ) I t ← λI t − 1 + g t h t ← κg t + µI t m t ← β 1 m t − 1 + ( 1 − β 1 ) h t ρ t ← ρ ∞ − 2 t β t 2 1 − β t 2 if the variance is tractable , i . e . , ρ t > 4 , then v t ← β 2 v t − 1 + ( 1 − β 2 ) h 2 t r t ← (cid:113) ( ρ t − 4 ) ( ρ t − 2 ) ρ ∞ ( ρ ∞ − 4 ) ( ρ ∞ − 2 ) ρ t (cid:99) m t ← m t 1 − β t 1 , (cid:98) v t ← v t 1 − β t 2 θ t + 1 ← Π F , √ (cid:98) v t ( θ t − α t r t (cid:99) m t √ (cid:98) v t + (cid:15) ) else θ t + 1 ← Π F , √ (cid:98) v t ( θ t − α t (cid:99) m t ) if t + 1 % k = = 0 : φ t ← η t θ t + ( 1 − η t ) φ t − k θ t ← φ t end for return x larger , and therefore training is becoming more difﬁcult . The current paradigm of pre - training - ﬁne - tuning is mainly used for large models . Therefore , we compare optimizers not only in the training - from - scratch setup , but also in the ﬁne - tuning setup . In this section , we compare our proposed optimizer with several typical optimizers , including classic SGD ( Robbins & Monro , 1951 ) and Adam ( Kingma & Ba , 2014 ) , our base , SGDM ( Sutskever et al . , 2013 ) 1 and RAdam ( Liu et al . , 2019 ) , the current state - of - the - art AdaBelief ( Zhuang et al . , 2020 ) , and the optimizer combined of many modules , Ranger ( Wright , 2019 ) . Since we should compare these optimizers under the same condition , the model used may be different from the original paper of them , which may lead to different convergence results compared to the results reported in the original paper . Please refer to Appendix E for more experimental details . 3 . 1 . Image Classiﬁcation Consistent with general optimizer researches ( Zhuang et al . , 2020 ) , we conduct experiments on two image classiﬁcation tasks , CIFAR - 10 and CIFAR - 100 ( Krizhevsky et al . , 2009 ) in CV ﬁeld , and the results are presented in Table 1 . For 1 Notably , we employed nesternov momentum ( Nesterov , 1983 ) in the SGDM for a stronger comparison baseline . 5 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers               ( S R F K                          7  U  D  L  Q  L  Q J    /  R  V V  & , ) $ 5     6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K                                                           7  H  V  W    $  F F  & , ) $ 5     6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K               7  U  D  L  Q  L  Q J    /  R  V V  & , ) $ 5      6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K                                                           7  H  V  W    $  F F  & , ) $ 5      6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6 Figure 2 . Training loss and test accuracy comparison on CIFAR - 10 and CIFAR - 100 datasets . Algorithm 2 A DMETA S Optimizer . All operations are element - wise . Initialize θ 1 ∈ F , φ 0 ← 0 , m 0 ← 0 , I 0 ← 0 , t ← 0 for t = 1 , 2 , . . . do t ← t + 1 g t ← ∇ f t ( θ t ) I t ← λI t − 1 + g t h t ← κg t + µI t m t ← βm t − 1 + ( 1 − β ) h t θ t + 1 ← θ t − α t m t if t + 1 % k = = 0 : φ t ← η t θ t + ( 1 − η t ) φ t − k θ t ← φ t end for return x Model CIFAR - 10 CIFAR - 100 ResNet - 110 PyramidNet ResNet - 110 PyramidNet Adam 91 . 89 ± 0 . 23 94 . 55 ± 0 . 24 68 . 45 ± 0 . 43 76 . 72 ± 0 . 32 RAdam 93 . 09 ± 0 . 05 94 . 58 ± 0 . 14 70 . 39 ± 0 . 08 76 . 02 ± 0 . 53 Ranger 92 . 85 ± 0 . 34 94 . 76 ± 0 . 03 68 . 96 ± 0 . 68 76 . 35 ± 0 . 08 AdaBelief 92 . 81 ± 0 . 26 94 . 70 ± 0 . 03 70 . 88 ± 0 . 07 76 . 57 ± 0 . 04 A DMETA R 93 . 63 ± 0 . 22 94 . 81 ± 0 . 19 71 . 00 ± 0 . 05 76 . 82 ± 0 . 07 SGD 90 . 27 ± 0 . 15 91 . 52 ± 0 . 03 65 . 70 ± 0 . 25 76 . 51 ± 0 . 06 SGDM 93 . 68 ± 0 . 20 95 . 08 ± 0 . 13 72 . 07 ± 0 . 28 79 . 49 ± 0 . 11 A DMETA S 94 . 12 ± 0 . 17 95 . 30 ± 0 . 08 73 . 74 ± 0 . 26 79 . 61 ± 0 . 34 Table 1 . Results on CIFAR - 10 and CIFAR - 100 test sets . model baselines , we choose the popular and leading perfor - mance ResNet - 110 ( He et al . , 2016 ) and PyramidNet ( Han et al . , 2017 ) , respectively . From the experimental results , whether in CIFAR - 10 or CIFAR - 100 dataset , and based on the ResNet - 110 or PyramidNet model , SGDM achieves better results than SGD , indicating that backward - looking improves the optimization effect . EMA with rectiﬁed item in RAdam performs better than EMA in Adam , suggesting that a better backward - looking process can lead to perfor - mance gains . Comparing SGDM and RAdam , we ﬁnd that SGDM has a performance advantage , showing that though Adam uses an adaptive learning rate to improve the speed of convergence , it is lossy for performance . Among optimizers with adaptive learning rate , AdaBelief achieves better results than Adam and RAdam in CIFAR - 10 with PyramidNet and CIFAR - 100 with ResNet - 110 and PyramidNet . Ranger , which combines forward and backward looking , achieves better performance than the backward - looking - only RAdam in CIFAR - 10 and CIFAR - 100 with PyramidNet . Our A DMETA R achieves consistent improvement over the optimizer baseline RAdam , which also conﬁrms the gain of bidirectional looking for optimiza - tion . And A DMETA R has better results than Ranger , indi - cating that our bidirectional looking is better than Ranger’s simple combination of multiple optimization features . Our A DMETA S also performs better than SGDM , further demon - strating the adaptability of our approach , which not only performs well in Adam family , but also works in SGD fam - ily . Following the previous practice ( Liu et al . , 2019 ) , we visual - ize the optimization process of the ResNet - 110 model with Adam , RAdam , SGDM , and our A DMETA S , A DMETA R optimizers on the CIFAR - 10 and CIFAR - 100 datasets in Figure 2 . As can be seen from the training loss ﬁgure , the above optimizers can successfully train the model to con - verge to a stable state , but A DMETA S obtains the lowest training loss on CIFAR - 10 , while AdaBelief obtains the training loss on CIFAR - 100 . In terms of performance on the test set , A DMETA S has obtained the best convergence result , which shows that the lower the loss of the training set may not necessarily lead to the better performance on the model . In addition , from the accuracy of the test set , the convergence speed of the SGD family including SGDM and A DMETA S is generally slower than that of the Adam family ( Adam , RAdam , Ranger , AdaBelief and A DMETA R ) , but the ﬁnal convergence result of the SGD family is better than the Adam family . However , our A DMETA R achieves more comparable performance to the SGD family , while maintaining the advantage of the fast convergence of the 6 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Model Optim MNLI QQP QNLI SST - 2 CoLA STS - B MRPC RTE Average m / mm ( Acc ) ( F 1 ) ( Acc ) ( Acc ) ( MCC ) ( SCC ) ( F 1 ) ( Acc ) BERT base AdamW 83 . 85 / 84 . 08 87 . 72 90 . 74 93 . 23 60 . 32 89 . 11 90 . 85 67 . 51 82 . 92 Ranger 83 . 80 / 84 . 24 87 . 83 90 . 76 92 . 32 58 . 87 89 . 19 90 . 05 68 . 59 82 . 68 AdaBelief 83 . 91 / 84 . 42 86 . 76 90 . 92 92 . 55 58 . 05 88 . 94 90 . 38 67 . 87 82 . 42 RAdam 83 . 91 / 84 . 24 87 . 66 90 . 88 92 . 20 59 . 31 89 . 07 90 . 91 70 . 04 83 . 00 A DMETA R 83 . 90 / 84 . 53 87 . 91 91 . 14 93 . 35 62 . 07 89 . 62 91 . 47 71 . 48 83 . 87 BERT large AdamW 86 . 05 / 86 . 55 88 . 58 92 . 40 93 . 00 59 . 58 89 . 21 91 . 67 71 . 12 83 . 95 Ranger 86 . 53 / 86 . 58 88 . 58 92 . 39 93 . 46 63 . 81 89 . 73 92 . 04 72 . 56 84 . 89 AdaBelief 85 . 59 / 86 . 25 86 . 99 92 . 42 93 . 00 61 . 11 90 . 17 91 . 28 72 . 92 84 . 19 RAdam 86 . 40 / 86 . 72 88 . 36 92 . 35 93 . 69 62 . 61 89 . 64 91 . 29 71 . 48 84 . 48 A DMETA R 86 . 21 / 86 . 54 88 . 54 92 . 63 93 . 69 64 . 12 89 . 92 92 . 10 73 . 65 85 . 11 Table 2 . Development results on GLUE benchmark . Optim SQuAD v1 . 1 SQuAD v2 . 0 NER - CoNLL03 EM F 1 EM F 1 P R F 1 BERT base : AdamW 80 . 87 88 . 39 72 . 63 75 . 99 94 . 65 95 . 24 94 . 94 Ranger 81 . 30 88 . 58 73 . 32 76 . 73 94 . 47 95 . 17 94 . 82 AdaBelief 80 . 63 88 . 10 72 . 97 76 . 25 93 . 79 94 . 60 94 . 19 RAdam 80 . 68 88 . 19 73 . 21 76 . 49 94 . 61 95 . 42 95 . 01 A DMETA R 81 . 55 88 . 69 73 . 81 77 . 19 94 . 96 95 . 41 95 . 13 BERT large : AdamW 83 . 31 90 . 39 76 . 67 80 . 02 94 . 77 95 . 73 95 . 24 Ranger 84 . 21 90 . 97 77 . 22 80 . 35 95 . 24 95 . 89 95 . 56 AdaBelief 83 . 53 90 . 42 77 . 48 80 . 57 94 . 28 95 . 17 94 . 72 RAdam 84 . 17 90 . 90 77 . 39 80 . 72 94 . 80 95 . 64 95 . 22 A DMETA R 84 . 25 90 . 92 77 . 08 80 . 36 95 . 38 95 . 93 95 . 65 Table 3 . Results on SQuAD v1 . 1 and v2 . 0 development sets and NER - CoNLL03 test sets . Adam family . A DMETA R has the highest results on the test set in the early stage of optimization ( < 80 epoch ) , which demonstrates that bidirectional looking improves both accu - racy and speed , making A DMETA R an efﬁcient and effective optimizer implementation . Compared to ResNet - 110 , PyramidNet has a more compli - cated structure and can achieve better results in these tasks . In cases where the model is strong enough , the selection of optimizer will not be the main factor for the ﬁnal perfor - mance . As shown in Table 1 , compared to Adam , RAdam and AdaBelief achieve just a bit of improvement on CIFAR - 10 task and even achieve worse results on CIFAR - 100 task , which also veriﬁes our above claims . It also shows that some recently proposed methods are not always suitable when the structure is complex enough . 3 . 2 . Natural Language Understanding As a general AI component , the general capability for var - ious tasks and various models is a basic requirement for optimizers . We evaluate the adaptability of our A DMETA optimizer on the ﬁnetune training scenario with current popular pre - trained language models . Speciﬁcally , we con - Optim SUPERB Common Language Acc Training Acc Training AdamW 98 . 26 10m44s 79 . 45 8h27m33s AdaBelief 98 . 41 11m20s 80 . 29 8h28m25s Ranger 98 . 35 11m50s 81 . 18 8h29m55s RAdam 98 . 37 11m30s 80 . 35 8h28m38s A DMETA R 98 . 50 11m54s 81 . 57 8h30m15s Table 4 . Results on speech keyword spotting and language identiﬁ - cation tasks . duct experiments based on the pre - trained language model BERT ( Devlin et al . , 2018 ) on three natural language un - derstanding tasks , GLUE benchmark ( Wang et al . , 2018 ) , machine reading comprehension ( SQuAD v1 . 1 and v2 . 0 ( Ra - jpurkar et al . , 2016 ) ) and named entity recognition ( NER - CoNLL03 ( Sang & De Meulder , 2003 ) ) . We report results for two model sizes , BERT base and BERT large to explore whether model size has an effect on the optimizer . In Table 2 , we report the results on the development set of 8 datasets of the GLUE benchmark , where Acc , MCC , SCC are abbreviations of accuracy , Matthews Correlation and Spearman Correlation Coefﬁcient , respectively . First , under the BERT - base model , compared with the basic opti - mizer RAdam , A DMETA R achieves consistent improvement . The most signiﬁcant improvement is obtained on RTE and CoLA , which indicates that our A DMETA R optimizer ex - hibits greater stability for low - resource optimization . On the other seven datasets , some of them are slightly improved . This is because most of the parameters of the model in the pre - training - ﬁne - tuning paradigm have converged to a cer - tain extent in the pre - training stage , so the further advantage of the optimizer in ﬁnetune is not apparent . And when the model is switched to a larger BERT - large , most tasks re - ceive performance gains , except for CoLA and RTE using AdamW optimizer . Due to the further increase in model pa - rameters , the low - resource dataset is not enough to ﬁne - tune the large model , it will even reduce the model performance . 7 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optim CIFAR - 10 ∆ Optim CIFAR - 10 ∆ Adam 91 . 89 \ SGD 90 . 22 \ RAdam 93 . 09 \ SGDM 93 . 68 \ A DMETA R 93 . 63 A DMETA S 94 . 12 - DEMA 93 . 24 - 0 . 39 - DEMA 89 . 13 - 4 . 99 - LB 92 . 29 - 0 . 95 - LB 89 . 88 - 4 . 24 - LF 93 . 14 - 0 . 10 - LF 93 . 51 - 0 . 61 - LB - LF 92 . 36 - 0 . 88 - LB - LF 89 . 80 - 4 . 32 A DMETA R w / constant LF 93 . 03 - 0 . 60 A DMETA S w / constant LF 93 . 75 - 0 . 37 Table 5 . Ablation study on A DMETA optimizer . But RAdam with rectiﬁed item , Ranger with bidirectional looking , and our A DMETA R handle the low - resource chal - lenge well , continue to improve performance , and take ad - vantage of large models . Our A DMETA R achieves the best results on these two low - resource datasets , demonstrating the effectiveness of our bidirectional looking approach . In Table 3 , we further report the results of machine reading comprehension and named entity recognition . A DMETA R achieved improvements at both model sizes in SQuAD v1 . 1 dataset , while similar improvements were achieved in SQuAD v2 . 0 with more complex models , illustrating that our optimizer is model - independent . Named entity recog - nition has reached a very accurate level with the help of pre - trained language models , and our A DMETA R optimizer also brings performance improvements over such a strong baseline , showing that optimization is also a bottleneck that restricts further performance improvement in addition to model structure and data . 3 . 3 . Audio Classiﬁcation Like images and natural language , speech is one of the mainstream ﬁelds of deep learning research . In speech pro - cessing , there are also a large number of pre - trained large models , such as Wav2vec ( Schneider et al . , 2019 ) . To high - light the input - independent nature of the optimizer , we also conduct experiments on two typical tasks of audio classi - ﬁcation , keyword spotting ( SUPERB ) ( Yang et al . , 2021 ) and language identiﬁcation ( Common Language ) ( Sinisetty et al . , 2021 ) . We employ Wav2vec 2 . 0 base as the baseline model and report the results of each optimizer in Table 4 . In addition , we also list the training time of each optimizer to evaluate the impact of the bidirectional looking mechanism on the optimizer time overhead 2 . A DMETA R shows better classiﬁcation accuracy than AdamW , RAdam , Ranger and AdaBelief , which is con - sistent with the experimental conclusions in the image and natural language tasks . Consistent results across image , 2 Notably , the reported training time is only for rough compari - son due to the inﬂuence of environments . natural language , and speech modalities verify the task - independence of our optimizer . Comparing the training time of A DMETA R with AdamW , RAdam , Ranger , and Ad - aBelief , our A DMETA R has different degrees of increase due to the additional computation and storage in the opti - mization process . Ranger and our A DMETA R increased the time most , but it can still be regarded as slight compared to the overall training time . Therefore , it can be concluded that the bidirectional looking mechanism adopted by A DMETA optimizer will bring additional computational overhead and increase the training time , but compared with the overall training cost , it is very small . A DMETA achieves better per - formance without increasing model parameters and training data , and does not have any impact on the inference time of the model , which achieves a better tradeoff . 4 . Ablation Study We perform an ablation study on various designs of A D - META in bidirectional looking in this section . - DEMA means removing the DEMA mechanism in backward - looking and using the original EMA . - LB means complete removal of backward - looking , - LF means complete removal of forward - looking . - LB - LF means to remove bidirectional looking at the same time . w / constant LF means use the original Lookahead mechanism in the forward - looking . The results are evaluated using the ResNet - 110 model on the test set of CIFAR - 10 . According to the results shown in Table 5 , it can be found that the improvement of SGDM com - pared with SGD initially shows the advantage of backward - looking . And compared with Adam , RAdam reveals that the EMA with the rectiﬁed item in backward - looking is more suitable for the training of the model than the original EMA . Our A DMETA ( including A DMETA R and A DMETA S ) achieved the best results . After removing DEMA and re - placing dynamic lookahead with constant lookahead , respec - tively , the performance drops , indicating that both DEMA and dynamic asymptotic lookahead play an important role in stable optimization . After further removing the backward - looking , the forward - looking , and the bidirectional looking , the results drop further , validating our argument that bidi - 8 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers rectional looking is beneﬁcial for optimization . Another observation is that backward - looking and DEMA make a more signiﬁcant contribution to the performance of SGDM compared to RAdam . This may show that our methods have a better complementarity for SGD - family optimizers . 5 . Conclusion In this paper , we introduce a bidirectional looking opti - mizer framework , exploring the use of historical and future information for optimization . For backward - looking , we introduce a DEMA scheme to replace the traditional EMA strategy , while for forward - looking , we propose a dynamic asymptotic lookahead strategy to replace the constant looka - head scheme . In this way , we propose the A DMETA opti - mizer , and provide two implement versions , A DMETA R and A DMETA S , which are based on adaptive and non - adaptive momentum optimizers , RAdam and SGDM respectively . We verify the beneﬁts of A DMETA with intuitive examina - tions and various experiments , showing the effectiveness of our proposed optimizer . Please refer to Appendix F for future work discussion . 6 . Limitation Although improving the performance on different tasks , our method introduces additional computational complexity and requires more hyperparameters than some existing ap - proaches . However , the selection range of hyperparameters can be preliminarily determined through the visual tool we proposed ( Figure 3 ) , which can slightly reduce the workload of tuning parameters . References Alacaoglu , A . , Malitsky , Y . , Mertikopoulos , P . , and Cevher , V . A new regret analysis for adam - type algorithms . In International conference on machine learning , pp . 202 – 210 . PMLR , 2020 . An , W . , Wang , H . , Sun , Q . , Xu , J . , Dai , Q . , and Zhang , L . A pid controller approach for stochastic optimization of deep networks . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 8522 – 8531 , 2018 . Anil , R . , Gupta , V . , Koren , T . , Regan , K . , and Singer , Y . Scalable second order optimization for deep learning . arXiv preprint arXiv : 2002 . 09018 , 2020 . Baevski , A . , Zhou , Y . , Mohamed , A . , and Auli , M . wav2vec 2 . 0 : A framework for self - supervised learning of speech representations . Advances in Neural Information Process - ing Systems , 33 : 12449 – 12460 , 2020 . Beckenbach , E . F . and Bellman , R . Inequalities , volume 30 . Springer Science & Business Media , 2012 . Chen , X . , Liu , S . , Sun , R . , and Hong , M . On the conver - gence of a class of adam - type algorithms for non - convex optimization . arXiv preprint arXiv : 1808 . 02941 , 2018 . Devlin , J . , Chang , M . - W . , Lee , K . , and Toutanova , K . Bert : Pre - training of deep bidirectional transformers for lan - guage understanding . arXiv preprint arXiv : 1810 . 04805 , 2018 . Duchi , J . , Hazan , E . , and Singer , Y . Adaptive subgradient methods for online learning and stochastic optimization . Journal of machine learning research , 12 ( 7 ) , 2011 . Fassold , H . Adafamily : A family of adam - like adaptive gra - dient methods . arXiv preprint arXiv : 2203 . 01603 , 2022 . Goh , G . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . Han , D . , Kim , J . , and Kim , J . Deep pyramidal residual networks . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 5927 – 5935 , 2017 . He , K . , Zhang , X . , Ren , S . , and Sun , J . Deep residual learn - ing for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 770 – 778 , 2016 . Huang , G . , Li , Y . , Pleiss , G . , Liu , Z . , Hopcroft , J . E . , and Weinberger , K . Q . Snapshot ensembles : Train 1 , get m for free . arXiv preprint arXiv : 1704 . 00109 , 2017 . Im , D . J . , Tao , M . , and Branson , K . An empirical analysis of the optimization of deep network loss surfaces . arXiv preprint arXiv : 1612 . 04010 , 2016 . Keskar , N . S . and Socher , R . Improving generalization per - formance by switching from adam to sgd . arXiv preprint arXiv : 1712 . 07628 , 2017 . Kingma , D . P . and Ba , J . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . Krizhevsky , A . , Hinton , G . , et al . Learning multiple layers of features from tiny images . 2009 . Li , W . , Zhang , Z . , Wang , X . , and Luo , P . Adax : Adap - tive gradient descent with exponential long term memory . arXiv preprint arXiv : 2004 . 09740 , 2020a . Li , Z . , Wang , R . , Chen , K . , Utiyama , M . , Sumita , E . , Zhang , Z . , and Zhao , H . Data - dependent gaussian prior objective for language generation . In 8th International Confer - ence on Learning Representations , ICLR 2020 , Addis 9 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Ababa , Ethiopia , April 26 - 30 , 2020 . OpenReview . net , 2020b . URL https : / / openreview . net / forum ? id = S1efxTVYDr . Li , Z . , Zhang , Z . , Zhao , H . , Wang , R . , Chen , K . , Utiyama , M . , and Sumita , E . Text compression - aided transformer encoding . IEEE Trans . Pattern Anal . Mach . Intell . , 44 ( 7 ) : 3840 – 3857 , 2022 . doi : 10 . 1109 / TPAMI . 2021 . 3058341 . URL https : / / doi . org / 10 . 1109 / TPAMI . 2021 . 3058341 . Liu , L . , Jiang , H . , He , P . , Chen , W . , Liu , X . , Gao , J . , and Han , J . On the variance of the adaptive learning rate and beyond . arXiv preprint arXiv : 1908 . 03265 , 2019 . Loshchilov , I . and Hutter , F . Sgdr : Stochastic gra - dient descent with warm restarts . arXiv preprint arXiv : 1608 . 03983 , 2016 . Loshchilov , I . and Hutter , F . Decoupled weight decay regu - larization . arXiv preprint arXiv : 1711 . 05101 , 2017 . Lucas , J . , Sun , S . , Zemel , R . , and Grosse , R . Aggregated momentum : Stability through passive damping . arXiv preprint arXiv : 1804 . 00325 , 2018 . Luo , L . , Xiong , Y . , Liu , Y . , and Sun , X . Adaptive gradient methods with dynamic bound of learning rate . arXiv preprint arXiv : 1902 . 09843 , 2019 . Ma , X . Apollo : An adaptive parameter - wise diagonal quasi - newton method for nonconvex stochastic optimization . arXiv preprint arXiv : 2009 . 13586 , 2020 . McMahan , H . B . and Streeter , M . Adaptive bound opti - mization for online convex optimization . arXiv preprint arXiv : 1002 . 4908 , 2010 . Mulloy , P . G . Smoothing data with faster moving averages . Stocks & Commodities , 12 ( 1 ) : 11 – 19 , 1994 . Nesterov , Y . E . A method for solving the convex program - ming problem with convergence rate o ( 1 / kˆ 2 ) . In Dokl . akad . nauk Sssr , volume 269 , pp . 543 – 547 , 1983 . Pascanu , R . and Bengio , Y . Revisiting natural gradient for deep networks . arXiv preprint arXiv : 1301 . 3584 , 2013 . Polyak , B . T . Some methods of speeding up the convergence of iteration methods . Ussr computational mathematics and mathematical physics , 4 ( 5 ) : 1 – 17 , 1964 . Popel , M . and Bojar , O . Training tips for the transformer model . arXiv preprint arXiv : 1804 . 00247 , 2018 . Rajpurkar , P . , Zhang , J . , Lopyrev , K . , and Liang , P . Squad : 100 , 000 + questions for machine comprehension of text . arXiv preprint arXiv : 1606 . 05250 , 2016 . Reddi , S . J . , Kale , S . , and Kumar , S . On the convergence of adam and beyond . arXiv preprint arXiv : 1904 . 09237 , 2019 . Robbins , H . and Monro , S . A stochastic approximation method . The annals of mathematical statistics , pp . 400 – 407 , 1951 . Sang , E . F . and De Meulder , F . Introduction to the conll - 2003 shared task : Language - independent named entity recognition . arXiv preprint cs / 0306050 , 2003 . Schneider , S . , Baevski , A . , Collobert , R . , and Auli , M . wav2vec : Unsupervised pre - training for speech recog - nition . arXiv preprint arXiv : 1904 . 05862 , 2019 . Sinisetty , G . , Ruban , P . , Dymov , O . , and Ravanelli , M . Com - monlanguage , June 2021 . URL https : / / doi . org / 10 . 5281 / zenodo . 5036977 . Sutskever , I . , Martens , J . , Dahl , G . , and Hinton , G . On the importance of initialization and momentum in deep learn - ing . In International conference on machine learning , pp . 1139 – 1147 . PMLR , 2013 . Tieleman , T . and Hinton , G . Lecture 6 . 5 - rmsprop , coursera : Neural networks for machine learning . University of Toronto , Technical Report , 6 , 2012 . Vaswani , A . , Shazeer , N . , Parmar , N . , Uszkoreit , J . , Jones , L . , Gomez , A . N . , Kaiser , Ł . , and Polosukhin , I . At - tention is all you need . Advances in neural information processing systems , 30 , 2017 . Wang , A . , Singh , A . , Michael , J . , Hill , F . , Levy , O . , and Bowman , S . R . Glue : A multi - task benchmark and anal - ysis platform for natural language understanding . arXiv preprint arXiv : 1804 . 07461 , 2018 . Wang , J . , Tantia , V . , Ballas , N . , and Rabbat , M . Looka - head converges to stationary points of smooth non - convex functions . In ICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pp . 8604 – 8608 . IEEE , 2020 . Wang , Y . , Kang , Y . , Qin , C . , Wang , H . , Xu , Y . , Zhang , Y . , and Fu , Y . Adapting stepsizes by momentumized gra - dients improves optimization and generalization . arXiv preprint arXiv : 2106 . 11514 , 2021 . Weng , B . , Sun , J . , Sadeghi , A . , and Wang , G . Adapid : An adaptive pid optimizer for training deep neural networks . In ICASSP 2022 - 2022 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pp . 3943 – 3947 . IEEE , 2022 . Wright , L . Ranger - a synergistic optimizer . https : / / github . com / lessw2020 / Ranger - Deep - Learning - Optimizer , 2019 . 10 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Yang , S . - w . , Chi , P . - H . , Chuang , Y . - S . , Lai , C . - I . J . , Lakho - tia , K . , Lin , Y . Y . , Liu , A . T . , Shi , J . , Chang , X . , Lin , G . - T . , et al . Superb : Speech processing universal performance benchmark . arXiv preprint arXiv : 2105 . 01051 , 2021 . Yao , Z . , Gholami , A . , Shen , S . , Mustafa , M . , Keutzer , K . , and Mahoney , M . Adahessian : An adaptive second order optimizer for machine learning . In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 35 , pp . 10665 – 10673 , 2021 . Zeiler , M . D . Adadelta : an adaptive learning rate method . arXiv preprint arXiv : 1212 . 5701 , 2012 . Zhang , M . , Lucas , J . , Ba , J . , and Hinton , G . E . Lookahead optimizer : k steps forward , 1 step back . Advances in neural information processing systems , 32 , 2019 . Zhang , Z . , Wu , Y . , Zhao , H . , Li , Z . , Zhang , S . , Zhou , X . , and Zhou , X . Semantics - aware BERT for language un - derstanding . In The Thirty - Fourth AAAI Conference on Artiﬁcial Intelligence , AAAI 2020 , The Thirty - Second Innovative Applications of Artiﬁcial Intelligence Confer - ence , IAAI 2020 , The Tenth AAAI Symposium on Edu - cational Advances in Artiﬁcial Intelligence , EAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 , pp . 9628 – 9635 . AAAI Press , 2020 . URL https : / / ojs . aaai . org / index . php / AAAI / article / view / 6510 . Zhou , B . , Liu , J . , Sun , W . , Chen , R . , Tomlin , C . J . , and Yuan , Y . pbsgd : Powered stochastic gradient descent methods for accelerated non - convex optimization . In IJCAI , pp . 3258 – 3266 , 2020a . Zhou , J . , Li , Z . , and Zhao , H . Parsing all : Syntax and semantics , dependencies and spans . In Cohn , T . , He , Y . , and Liu , Y . ( eds . ) , Findings of the Association for Computational Linguistics : EMNLP 2020 , Online Event , 16 - 20 November 2020 , volume EMNLP 2020 of Find - ings of ACL , pp . 4438 – 4449 . Association for Compu - tational Linguistics , 2020b . doi : 10 . 18653 / v1 / 2020 . ﬁndings - emnlp . 398 . URL https : / / doi . org / 10 . 18653 / v1 / 2020 . findings - emnlp . 398 . Zhuang , J . , Tang , T . , Ding , Y . , Tatikonda , S . C . , Dvornek , N . , Papademetris , X . , and Duncan , J . Adabelief optimizer : Adapting stepsizes by the belief in observed gradients . Advances in neural information processing systems , 33 : 18795 – 18806 , 2020 . Zinkevich , M . Online convex programming and generalized inﬁnitesimal gradient ascent . In Proceedings of the 20th international conference on machine learning ( icml - 03 ) , pp . 928 – 936 , 2003 . 11 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Appendix A . Related Work As an important part of machine learning and deep learning , optimizers have received much attention in recent years . The optimizer plays a prominent role in the convergence speed and the convergence effect of the model . To seek good properties like fast convergence , good generalization and robustness , many algorithms have been put forward recently , and they can be divided into four families according to their characteristics and motivation . SGD Family In this family , the optimizers adopt the method of update like θ t = θ t − 1 − α t m t , where θ t denotes the parameter to be optimized at iteration step t and m t refers to some combination of past gradients ( such as EMA ) , which can be represented as f 1 ( g 1 , g 2 , . . . , g t ) . Original SGD ( Robbins & Monro , 1951 ) directly minus the product of global learning rate and the gradient at each step . Despite of its simplicity , it is still widely used in many datasets . However , SGD is blamed for its low convergence rate and high ﬂuctuation , thus many methods have been proposed to accelerate the speed and smooth the update process . One efﬁcient optimizer to tackle this issue is SGDM ( Sutskever et al . , 2013 ) , which uses a exponential moving average ( EMA , also known as momentum ) to replace the gradient with an exponential weight decay of past gradients . SGDM - Nesterov ( Nesterov , 1983 ) is a variant of SGDM which modiﬁes the momentum by computing gradient based on the approximation of the next position and thus changing the descent direction . Experiments have shown that Nesterov momentum tends to achieve a higher speed and performance . Adam Family The Adam family optimizers usually update parameters by θ t = θ t − 1 − α t m t / √ v t , where v t is the adaptive item and can be represented as f 2 ( g 21 , g 22 , . . . , g 2 t ) . Compared to SGD family , instead of using a uniform learning rate , this kind of optimizer computes an individual learning rate for each parameter due to the effect of the denominator √ v t in the equation . v t is usually a dimension - reduction approximation to the matrix which contains the information of second order curvature , such as Fisher matrix ( Pascanu & Bengio , 2013 ) . Adadelta ( Zeiler , 2012 ) , Adagrad ( Duchi et al . , 2011 ) and RMSprop ( Tieleman & Hinton , 2012 ) are early optimizers in this family . A stand out generation is Adam ( Kingma & Ba , 2014 ) which combines the RMSprop with Adagrad . It has been widely used in a wide range of datasets and works well even with sparse gradients . However , there are problems with Adam with respect to convergence and generalization , thus many methods have been proposed to make improvements Based on the large variance in the early stage that may lead to a bad optimum , heuristic warmup ( Vaswani et al . , 2017 ; Popel & Bojar , 2018 ) and RAdam ( Liu et al . , 2019 ) are proposed , of which the former starts with a small initial learning rate and the latter introduces a rectiﬁed item . To ﬁx the convergence error , ( Reddi et al . , 2019 ) proposed AMSGrad which requires the non - decreasing property of the second momentum . In fact , this method can be interpolated into other Adam family algorithms to guarantee the convergence in convex situations . Considering the curvature of the loss function , AdaBelief ( Zhuang et al . , 2020 ) and AdaMomentum ( Wang et al . , 2021 ) are proposed . More recently , there are still numerous studies devoted to improving Adam , such as AdaX ( Li et al . , 2020a ) and AdaFamily ( Fassold , 2022 ) . However , we notice that most researchers put a solid emphasis on modifying the second momentum term , i . e . , the adaptive item and ignore the possibility to make a relative overall change to the algorithms . Stochastic Second - Order Family In the stochastic second - order optimizers , parameters are updated using second - order information related to Hessian matrix . The update process is typically written as θ t = θ t − 1 − α t H − 1 m t , where H is the Hessian matrix or approximation matrix to it . Ideally , they can achieve better results than the ﬁrst order optimizers ( like Adam family and SGD family ) , but their practicality is limited due to the large computational cost of the second order information , like the Fisher / Hessian matrix . Some methods have been proposed using low - rank decomposition and approximating to hessian diagonal to reduce the cost , like Apollo ( Ma , 2020 ) , AdHessian ( Yao et al . , 2021 ) and Shampoo ( Anil et al . , 2020 ) . 12 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Starting Point Optimum Solution SGD with EMA ( SGDM : vanilla momentum ) Momentum β = 0 . 90 SGD with DEMA ( Ours ) Momentum λ = 0 . 90 Momentum β = 0 . 60 Learning rate α = 0 . 0015 Figure 3 . EMA vs . DEMA in SGD optimizer . Please refer to our online demo https : / / sites . google . com / view / optimizer - admeta for more comparison . Other Optimizers There are some algorithms that are not convenient to be categorized into the above families and we list some examples here . Motivated by PID controller , SGD - PID ( An et al . , 2018 ) takes an analogy between the gradient and the input error in an automatic control system . Analysis shows that it can reduce the overshoot problem in SGD and SGD variants . Furthermore , ( Weng et al . , 2022 ) applied PID to Adam and proposed the AdaPID optimizer . Lookahead ( Zhang et al . , 2019 ) optimizer updates two sets of weight wherein ”fast weights” function as a guide to search for the direction and ”slow weights” follow the guide to achieve better optimization . Ranger ( Wright , 2019 ) optimizer further combines RAdam and Lookahead to get a compound algorithm and shows a better convergence performance . Discussion To show the advantage of bidirectional looking , we propose A DMETA optimizer . Speciﬁcally , it is based on the idea of considering backward - looking and forward - looking , wherein DEMA plays a important role in the former aspect and dynamic asymptotic forward - looking strategy serves for the latter aspect . In practical use , we provide two versions , A DMETA S and A DMETA R , using the framework of A DMETA and based on SGDM and RAdam respectively . Speciﬁcally , A DMETA S replaces the traditionally used EMA in backward - looking part of SGDM with DEMA and adds the forward - looking part which is derived from Lookahead optimizer . A DMETA R is based on RAdam in the same way . The second order family is also introduced above because the framework of A DMETA can also be applied in this family , and it is remained as the future work . B . EMA vs . DEMA To corroborate our analysis of EMA and DEMA , we compared the optimization process of EMA and DEMA on the SGD optimizer according to the practice of ( Goh , 2017 ) . Using the same learning rate α and starting from the same starting point , the convergence process is shown in Figure 3 . The decent surface in the ﬁgure is the convex quadratic , which is a useful model despite its simplicity , for it comprises an important structure , the “valleys” , which is often studied as an example in momentum - based optimizers . As demonstrated in Figure 3 , on the one hand , DEMA achieves faster speed than EMA , which can be easily seen by comparing the distance to the optimal point at the same time ; on the other hand , DEMA achieves better convergence results than EMA as can be seen in the distance between the point of convergence and optimum . C . Proof of Convergence In this section , following ( Chen et al . , 2018 ) , ( Alacaoglu et al . , 2020 ) and ( Reddi et al . , 2019 ) , we provide detailed proofs of convergence for A DMETA R and A DMETA S optimizers in convex and non - convex situations . 13 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers C . 1 . Convergence Analysis in Convex and Non - convex Optimization Optimization problem For deterministic problems , the problem to be optimized is min θ ∈F f ( θ ) , where f denotes the loss function . For online optimization , the problem is min θ ∈F (cid:80) Tt = 1 f t ( θ ) , where f t is the loss function of the model with the given parameters at the t - th step . The criteria for judging convergence in convex and non - convex cases are different . For convex optimization , following ( Reddi et al . , 2019 ) , the goal is to ensure R ( T ) = o ( T ) , i . e . , lim T →∞ R ( T ) / T = 0 . For non - convex optimization , following ( Chen et al . , 2018 ) , the goal is to ensure min t ∈ [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 = o ( T ) . Theorem C . 1 . ( Convergence of A DMETA R for convex optimization ) Let { θ t } be the sequence obtained from A DMETA R , 0 ≤ λ , β 1 , β 2 < 1 , γ = β 21 β 2 < 1 , α t = α √ t and v t ≤ v t + 1 , ∀ t ∈ [ T ] . Suppose x ∈ F , where F ⊂ R d and has bounded diameter D ∞ , i . e . | | θ t − θ | | ∞ ≤ D ∞ , ∀ t ∈ [ T ] . Assume f ( θ ) is a convex function and | | g t | | ∞ is bounded . Denote the optimal point as θ . For θ t generated , A DMETA R achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( θ t ) − f t ( θ ) ] = O ( √ T ) Theorem C . 2 . ( Convergence of A DMETA R for non - convex optimization ) Under the assumptions : • ∇ f exits and is Lipschitz - continuous , i . e , | | ∇ f ( x ) − ∇ f ( y ) | | ≤ L | | x − y | | , ∀ x , y ; f is also lower bounded . • At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ∇ f is also bounded . • The noisy gradient is unbiased , and has independent noise , i . e . g t = ∇ f ( θ t ) + δ t , E [ δ t ] = 0 and δ i ⊥ δ j , ∀ i (cid:54) = j . Assume min j ∈ [ d ] ( v 1 ) j ≥ c > 0 and α t = α / √ t , then for any T we have : min t ∈ [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ≤ 1 √ T ( Q 1 + Q 2 log T ) where Q 1 and Q 2 are constants independent of T . Theorem C . 3 . ( Convergence of A DMETA S for convex optimization ) Let { θ t } be the sequence obtained by A DMETA S , 0 ≤ λ , β < 1 , α t = α √ t , ∀ t ∈ [ T ] . Suppose x ∈ F , where F ⊂ R d and has bounded diameter D ∞ , i . e . | | θ t − θ | | ∞ ≤ D ∞ , ∀ t ∈ [ T ] . Assume f ( θ ) is a convex func - tion and | | g t | | ∞ is bounded . Denote the optimal point as θ . For θ t generated , A DMETA S achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( θ t ) − f t ( θ ) ] = O ( √ T ) Theorem C . 4 . ( Convergence of A DMETA S for non - convex optimization ) Under the assumptions : • ∇ f exits and is Lipschitz - continuous , i . e , | | ∇ f ( x ) − ∇ f ( y ) | | ≤ L | | x − y | | , ∀ x , y ; f is also lower bounded . • At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ∇ f is also bounded . • The noisy gradient is unbiased , and has independent noise , i . e . g t = ∇ f ( θ t ) + δ t , E [ δ t ] = 0 and δ i ⊥ δ j , ∀ i (cid:54) = j . Assume α t = α / √ t , then for any T we have : min t ∈ [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ≤ 1 √ T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where Q (cid:48) 1 and Q (cid:48) 2 are constants independent of T . 14 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Before formally proving the theorems , here list some remarks and preparations . Remark 1 . For brevity , we omit the rectiﬁed item of A DMETA R in the proof . However , it does not inﬂuence the proof since it can be integrated into the learning rate . Remark 2 . Following ( Luo et al . , 2019 ) , the bias correction 1 / ( 1 − β t 1 ) of the ﬁrst momentum m t is omitted in the convergence of A DMETA R . Since 1 / ( 1 − β t 1 ) is bounded above 1 and below 10 , the order of the terms used is not affected , thus hardly affecting the proof . Remark 3 . The forward - looking part is not considered in the proof . On the one hand , explanations and proofs of constant Lookahead have been given in ( Zhang et al . , 2019 ) and ( Wang et al . , 2020 ) , which can be imitated by our dynamic method . On the other hand , forward - looking part is exactly the interpolation of fast weights and slow weights at each synchronization period , and the fast weights are updated by the given optimizer . Therefore , the convergence proof is equivalent to only proving the convergence of fast weights . Remark 4 . The condition in the theorem that v t ≤ v t + 1 , ∀ t ∈ [ T ] does not necessarily hold in the practice of our method . Dropping this condition may lead to a non - convergence result , which can be seen in ( Reddi et al . , 2019 ) . However , the counterexample given by this article is a very artiﬁcial design , which may not represent the case in practice . Many optimizers that do not meet this condition can eventually converge in the training process and further exploration may show that this condition is not necessary . Remark 5 . If we ﬁx the number of steps , the training is a ﬁnite process over ﬁnite data , thus the iteration is bounded . Lemma C . 5 . if (cid:107) g t (cid:107) ∞ is bounded , i . e . (cid:107) g t (cid:107) ∞ ≤ G ∞ , ∀ t ∈ [ T ] , where G ∞ is a constant independent of T , then I t , h t and m t are also bounded . Proof . First of all , we prove (cid:107) I t (cid:107) ∞ ≤ ( 1 + λ ) G ∞ by induction : when t = 1 (cid:107) I 1 (cid:107) ∞ = (cid:107) g 1 (cid:107) ∞ ≤ G ∞ Suppose t = k satisﬁes , then for t = k + 1 (cid:107) I k + 1 (cid:107) ∞ = (cid:107) λI k + g k + 1 (cid:107) ∞ ≤ λ (cid:107) I k (cid:107) ∞ + (cid:107) g k + 1 (cid:107) ∞ ≤ ( λ + 1 ) max { (cid:107) I k (cid:107) ∞ , (cid:107) g k + 1 (cid:107) ∞ } ≤ ( 1 + λ ) G ∞ Next , for (cid:107) h k (cid:107) ∞ (cid:107) h t (cid:107) ∞ = (cid:107) κg t + µI t (cid:107) ∞ ≤ κ (cid:107) g t (cid:107) ∞ + µ (cid:107) I t (cid:107) ∞ ≤ [ κ + ( 1 − λ ) µ ) ] G ∞ Since m t is the moving average of h i where i = 1 , . . . , t , we can get that it is also bounded following the proof of I t . In this way , we can redeﬁne G ∞ by enlarging it and the bounded stochastic gradient assumption in the theorem is equivalent to assuming (cid:107) g t (cid:107) ∞ , (cid:107) I t (cid:107) ∞ , (cid:107) h t (cid:107) ∞ , (cid:107) m t (cid:107) ∞ ≤ G ∞ . Remark 6 . As for non - convex optimization , in the same way , the bounded noisy gradient assumption is equivalent to (cid:107) g t (cid:107) , (cid:107) I t (cid:107) , (cid:107) h t (cid:107) , (cid:107) m t (cid:107) ≤ H where H is a constant independent of T . This remark will be used in several places in the following proof . Lemma C . 6 ( Generalized H¨older inequality , ( Beckenbach & Bellman , 2012 ) ) . For x , y , z ∈ R n + and positive p , q , r such that 1 p + 1 q + 1 r = 1 , we have n (cid:88) j = 1 θ j y j z j ≤ (cid:107) x (cid:107) p (cid:107) y (cid:107) q (cid:107) z (cid:107) r . This is a common mathematical inequality , so the proof is omitted here . Lemma C . 7 ( nonexpansiveness property of arg min x ∈F (cid:107) . (cid:107) , ( McMahan & Streeter , 2010 ) ) . For any Q ∈ S d + , i . e . Q is a Positive deﬁnite matrice and convex feasible set F ⊂ R d , suppose u 1 = arg min x ∈F (cid:107) Q 1 / 2 ( x − z 1 ) (cid:107) and u 2 = arg min x ∈F (cid:107) Q 1 / 2 ( x − z 2 ) (cid:107) then we have (cid:107) Q 1 / 2 ( u 1 − u 2 ) (cid:107) ≤ (cid:107) Q 1 / 2 ( z 1 − z 2 ) (cid:107) . 15 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Proof . First , we claim that (cid:104) u 1 − z 1 , Q ( u 2 − u 1 ) (cid:105) ≥ 0 and (cid:104) u 2 − z 2 , Q ( u 1 − u 2 ) (cid:105) ≥ 0 ( We only prove the former as the proofs are exactly the same ) . Otherwise , consider a small δ , we have u 1 + δ ( u 2 − u 1 ) ∈ F 1 2 (cid:104) u 1 + δ ( u 2 − u 1 ) − z 1 , Q ( u 1 + δ ( u 2 − u 1 ) − z 1 ) (cid:105) = 1 2 (cid:104) u 1 − z 1 , Q ( u 1 − z 1 ) (cid:105) + 1 2 δ 2 (cid:104) u 2 − u 1 , Q ( u 2 − u 1 ) (cid:105) + δ (cid:104) u 1 − z 1 , Q ( u 2 − u 1 ) (cid:105) If there exists (cid:104) u 1 − z 1 , Q ( u 2 − u 1 ) (cid:105) < 0 , δ can be chosen so small that it satisﬁes 12 δ 2 (cid:104) u 2 − u 1 , Q ( u 2 − u 1 ) (cid:105) + δ (cid:104) u 1 − z 1 , Q ( u 2 − u 1 ) (cid:105) < 0 , which contradicts the deﬁnition of u 1 . Using the above claim , we further have (cid:104) u 1 − z 1 , Q ( u 2 − u 1 ) (cid:105) − (cid:104) u 2 − z 2 , Q ( u 2 − u 1 ) (cid:105) ≥ 0 ⇒(cid:104) z 2 − z 1 , Q ( u 2 − u 1 ) (cid:105) ≥ (cid:104) u 2 − u 1 , Q ( u 2 − u 1 ) (cid:105) ( 19 ) Also , observing the following (cid:104) ( u 2 − u 1 ) − ( z 2 − z 1 ) , Q ( ( u 2 − u 1 ) − ( z 2 − z 1 ) ) (cid:105) ≥ 0 ⇒(cid:104) u 2 − u 1 , Q ( z 2 − z 1 ) (cid:105) ≤ 1 2 [ (cid:104) u 2 − u 1 , Q ( u 2 − u 1 ) (cid:105) + (cid:104) z 2 − z 1 , Q ( z 2 − z 1 ) (cid:105) ] ( 20 ) Combining ( 19 ) and ( 20 ) , we have the required result . C . 2 . Convergence Analysis of AdmetaR for Convex Optimization Lemma C . 8 . Consider m t = β 1 m t − 1 + ( 1 − β 1 ) h t , ∀ t ≥ 1 . it follows that (cid:104) h t , θ t − θ (cid:105) = (cid:104) m t − 1 , θ t − 1 − θ (cid:105) − β 1 1 − β 1 (cid:104) m t − 1 , θ t − θ t − 1 (cid:105) + 1 1 − β 1 ( (cid:104) m t , θ t − θ (cid:105) − (cid:104) m t − 1 , θ t − 1 − θ (cid:105) ) . Proof . By deﬁnition of m t , h t = 1 1 − β 1 m t − β 1 1 − β 1 m t − 1 . Thus , we have (cid:104) h t , θ t − θ (cid:105) = 1 1 − β 1 (cid:104) m t , θ t − θ (cid:105) − β 1 1 − β 1 (cid:104) m t − 1 , θ t − θ (cid:105) = 1 1 − β 1 (cid:104) m t , θ t − θ (cid:105) − β 1 1 − β 1 (cid:104) m t − 1 , θ t − 1 − θ (cid:105) − β 1 1 − β 1 (cid:104) m t − 1 , θ t − θ t − 1 (cid:105) = 1 1 − β 1 (cid:0) (cid:104) m t , θ t − θ (cid:105) − (cid:104) m t − 1 , θ t − 1 − θ (cid:105) (cid:1) + (cid:104) m t − 1 , θ t − 1 − θ (cid:105) − β 1 1 − β 1 (cid:104) m t − 1 , θ t − θ t − 1 (cid:105) . Lemma C . 9 ( Bound for (cid:80) Tt = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ) . Under Assumption in Theorem 1 , we have T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ≤ ( 1 − β 1 ) α √ 1 + log T (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 2 16 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Proof . First , we bound (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 . From the deﬁnition of m t and v t , it follows that m t = ( 1 − β 1 ) t (cid:88) j = 1 β t − j 1 h j , v t = ( 1 − β 2 ) t (cid:88) j = 1 β t − j 2 h 2 j Then we have (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ≤ (cid:107) v − 1 / 4 t m t (cid:107) 2 = d (cid:88) i = 1 m 2 t , i v 1 / 2 t , i = d (cid:88) i = 1 (cid:16)(cid:80) tj = 1 ( 1 − β 1 ) β t − j 1 h j , i (cid:17) 2 (cid:113)(cid:80) tj = 1 ( 1 − β 2 ) β t − j 2 h 2 j , i = ( 1 − β 1 ) 2 √ 1 − β 2 d (cid:88) i = 1 (cid:16)(cid:80) tj = 1 β t − j 1 h j , i (cid:17) 2 (cid:113)(cid:80) tj = 1 β t − j 2 h 2 j , i ≤ ( 1 − β 1 ) 2 √ 1 − β 2 (cid:32) d (cid:88) i = 1 (cid:20) (cid:16) (cid:80) tj = 1 ( β t − j 4 2 | h j , i | 12 ) 4 (cid:17) 14 (cid:16) (cid:80) tj = 1 ( β 1 / 2 1 β − 1 / 4 2 ) 4 ( t − j ) (cid:17) 14 (cid:16) (cid:80) tj = 1 ( β t − j 1 | h j , i | ) 12 · 2 (cid:17) 12 (cid:21) 2 (cid:113)(cid:80) tj = 1 β t − j 2 h 2 j , i (cid:33) = ( 1 − β 1 ) 2 √ 1 − β 2 d (cid:88) i = 1   t (cid:88) j = 1 γ t − j   12 t (cid:88) j = 1 β t − j 1 | h j , i | ≤ ( 1 − β 1 ) 2 (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 t (cid:88) j = 1 β t − j 1 | h j , i | , ( 21 ) where the ﬁrst inequality follows from the fact that ˆ v 1 / 2 t , i ≥ v 1 / 2 t , i , the second one follows from the generalized H ¨ older inequality for θ j = β t − j 4 2 | h j , i | 12 , y j = ( β 1 β − 1 / 2 2 ) t − j 2 , z j = ( β t − j 1 | h j , i | ) 12 and p = q = 4 , r = 2 , and the third one follows from the sum of geometric series and the assumption γ = β 21 β 2 < 1 . In this way , we can bound (cid:80) Tt = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 . T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ≤ ( 1 − β 1 ) 2 (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 T (cid:88) t = 1 α t t (cid:88) j = 1 β t − j 1 | h j , i | = ( 1 − β 1 ) 2 (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 T (cid:88) j = 1 T (cid:88) t = j α t β t − j 1 | h j , i | ≤ ( 1 − β 1 ) (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 T (cid:88) j = 1 α j | h j , i | ≤ 1 − β 1 (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 (cid:118)(cid:117)(cid:117)(cid:116) T (cid:88) j = 1 α 2 j (cid:118)(cid:117)(cid:117)(cid:116) T (cid:88) j = 1 h 2 j , i ≤ ( 1 − β 1 ) α √ 1 + log T (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 (cid:118)(cid:117)(cid:117) (cid:116) T (cid:88) t = 1 h 2 t , i = ( 1 − β 1 ) α √ 1 + log T (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 17 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where the ﬁrst inequality follows from ( 21 ) . The ﬁrst equality is by changing order of summation . The second inequality follows from the fact that (cid:80) Tt = j α t β t − j 1 ≤ α j 1 − β 1 . The third inequality is by Cauthy - Schwartz . The last inequality is by using (cid:80) Tj = 1 1 j ≤ 1 + log T Theorem C . 10 . ( Convergence of A DMETA R for convex optimization ) Let { θ t } be the sequence obtained from A DMETA R , 0 ≤ λ , β 1 , β 2 < 1 , γ = β 21 β 2 < 1 , α t = α √ t and v t ≤ v t + 1 , ∀ t ∈ [ T ] . Suppose x ∈ F , where F ⊂ R d and has bounded diameter D ∞ , i . e . | | θ t − θ | | ∞ ≤ D ∞ , ∀ t ∈ [ T ] . Assume f ( θ ) is a convex func - tion and | | g t | | ∞ is bounded . Denote the optimal point as θ . For θ t generated , A DMETA R achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( θ t ) − f t ( θ ) ] = O ( √ T ) Proof . • Bound for (cid:80) T t = 1 (cid:104) m t , θ t − θ (cid:105) . As x ∈ F , we get θ t + 1 = Π F , √ ˆ v t ( θ t − α t ˆ v − 1 / 2 t m t ) = min x ∈F (cid:107) ˆ v 1 / 4 t ( x − ( θ t − α t ˆ v − 1 / 2 t m t ) ) (cid:107) . Furthermore , Π F , √ ˆ v t ( x ) = x for all x ∈ F . Using Lemma C . 7 with u 1 = θ t + 1 and u 2 = θ , we have the following : (cid:107) ˆ v 1 / 4 t ( θ t + 1 − θ ) (cid:107) 2 ≤ (cid:107) ˆ v 1 / 4 t ( θ t − α t ˆ v − 1 / 2 t m t − θ ) (cid:107) 2 = (cid:107) ˆ v 1 / 4 t ( θ t − θ ) (cid:107) 2 + α 2 t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 − 2 α t (cid:104) m t , θ t − θ (cid:105) ( 22 ) we rearrange and divide both sides of ( 22 ) by 2 α t to get (cid:104) m t , θ t − θ (cid:105) ≤ 1 2 α t (cid:107) ˆ v 1 / 4 t ( θ t − θ ) (cid:107) 2 − 1 2 α t (cid:107) ˆ v 1 / 4 t ( θ t + 1 − θ ) (cid:107) 2 + α t 2 (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 = 1 2 α t − 1 (cid:107) ˆ v 1 / 4 t − 1 ( θ t − θ ) (cid:107) 2 − 1 2 α t (cid:107) ˆ v 1 / 4 t ( θ t + 1 − θ ) (cid:107) 2 + 1 2 d (cid:88) i = 1 (cid:32) ˆ v 1 / 2 t , i α t − ˆ v 1 / 2 t − 1 , i α t − 1 (cid:33) ( θ t , i − θ i ) 2 + α t 2 (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ≤ 1 2 α t − 1 (cid:107) ˆ v 1 / 4 t − 1 ( θ t − θ ) (cid:107) 2 − 1 2 α t (cid:107) ˆ v 1 / 4 t ( θ t + 1 − θ ) (cid:107) 2 + D 2 ∞ 2 d (cid:88) i = 1 (cid:32) ˆ v 1 / 2 t , i α t − ˆ v 1 / 2 t − 1 , i α t − 1 (cid:33) + α t 2 (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ( 23 ) where the last inequality is due to the fact that ˆ v t , i ≥ ˆ v t − 1 , i , 1 α t ≥ 1 α t − 1 , and the deﬁnition of D ∞ . Summing ( 23 ) over t = 1 , . . . T and using that ˆ v 0 = 0 yields T (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) ≤ D 2 ∞ 2 α T d (cid:88) i = 1 ˆ v 1 / 2 T , i + 1 2 T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 . 18 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers • Bound for (cid:80) Tt = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) . T (cid:88) t = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) = T (cid:88) t = 2 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) = T − 1 (cid:88) t = 1 (cid:104) m t , θ t − θ t + 1 (cid:105) ≤ T − 1 (cid:88) t = 1 (cid:107) ˆ v − 1 / 4 t m t (cid:107)(cid:107) ˆ v 1 / 4 t ( θ t + 1 − θ ) (cid:107) = T − 1 (cid:88) t = 1 (cid:107) ˆ v − 1 / 4 t m t (cid:107) (cid:13)(cid:13)(cid:13) ˆ v 1 / 4 t [ Π F , ˆ v 1 / 2 t (cid:16) θ t − α t ˆ v − 1 / 2 t m t (cid:17) − Π F , ˆ v 1 / 2 t ( θ t ) ] (cid:13)(cid:13)(cid:13) ≤ T − 1 (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107)(cid:107) ˆ v − 1 / 4 t m t (cid:107) = T − 1 (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 where the ﬁrst inequality follows from H¨older inequality and the second inequality is due to lemma C . 7 • Bound for (cid:104) m T , θ T − θ (cid:105) . (cid:104) m T , θ T − θ (cid:105) ≤ (cid:107) ˆ v − 1 / 4 t m T (cid:107)(cid:107) ˆ v 1 / 4 t ( θ T − θ ) (cid:107) ≤ α T (cid:107) ˆ v − 1 / 4 t m T (cid:107) 2 + 1 4 α T (cid:107) ˆ v 1 / 4 t ( θ T − θ ) (cid:107) 2 ≤ α T (cid:107) ˆ v − 1 / 4 t m T (cid:107) 2 + D 2 ∞ 4 α T d (cid:88) i = 1 ˆ v 1 / 2 T , i where the ﬁrst inequality follows from H ¨ older inequality and the second inequality follows from Young’s inequality . The last inequality is due to the deﬁnition of D ∞ . After all these preparations , we obtain : T (cid:88) t = 1 (cid:104) h t , θ t − θ (cid:105) = β 1 1 − β 1 (cid:32) (cid:104) m T , θ T − θ (cid:105) + T (cid:88) t = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) (cid:33) + T (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) ≤ β 1 1 − β 1 (cid:32) D 2 ∞ 4 α T d (cid:88) i = 1 ˆ v 1 / 2 T , i + T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 (cid:33) + D 2 ∞ 2 α T d (cid:88) i = 1 ˆ v 1 / 2 T , i + 1 2 T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 = ( 2 − β 1 ) D 2 ∞ 4 α T ( 1 − β 1 ) d (cid:88) i = 1 ˆ v 1 / 2 T , i + 2 + β 1 2 ( 1 − β 1 ) T (cid:88) t = 1 α t (cid:107) ˆ v − 1 / 4 t m t (cid:107) 2 ≤ ( 2 − β 1 ) D 2 ∞ √ T 4 α ( 1 − β 1 ) d (cid:88) i = 1 ˆ v 1 / 2 T , i + ( 2 + β 1 ) α √ 1 + log T 2 (cid:112) ( 1 − β 2 ) ( 1 − γ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 2 This proves that (cid:80) Tt = 1 (cid:104) h t , θ t − θ (cid:105) = O ( √ T ) . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Since h t = κg t + µI t , h t is the same order as g t when the time is long enough , thus we have T (cid:88) t = 1 (cid:104) g t , θ t − θ (cid:105) = O ( √ T ) ( 24 ) 19 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers In addition , due to the convexity of f ( . ) , we have R ( T ) = T (cid:88) t = 1 ( f t ( θ t ) − f t ( x ) ) ≤ T (cid:88) t = 1 (cid:104) g t , θ t − θ (cid:105) Combined with ( 24 ) , we complete the proof . C . 3 . Convergence Analysis of AdmetaR for Non - convex Optimization Lemma C . 11 . Set θ 0 (cid:44) x 1 in Algorithm ( 1 ) , and deﬁne z t as z t = θ t + β 1 1 − β 1 ( θ t − θ t − 1 ) , ∀ t ≥ 1 . ( 25 ) Then the following holds true z t + 1 − z t = − β 1 1 − β 1 (cid:32) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:33) m t − 1 − α t h t / (cid:112) ˆ v t Proof . By the update of A DMETA R , we have θ t + 1 − θ t = − α t m t / (cid:112) ˆ v t = − α t ( β 1 m t − 1 + ( 1 − β 1 ) h t ) / (cid:112) ˆ v t = β 1 α t α t − 1 (cid:112) ˆ v t − 1 √ ˆ v t ( θ t − θ t − 1 ) − α t ( 1 − β 1 ) h t / (cid:112) ˆ v t = β 1 ( θ t − θ t − 1 ) + β 1 (cid:32) α t α t − 1 (cid:112) ˆ v t − 1 √ ˆ v t − 1 (cid:33) ( θ t − θ t − 1 ) − α t ( 1 − β 1 ) h t / (cid:112) ˆ v t = β 1 ( θ t − θ t − 1 ) − β 1 (cid:32) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:33) m t − 1 − α t ( 1 − β 1 ) h t / (cid:112) ˆ v t ( 26 ) Since we also have θ t + 1 − θ t = ( 1 − β 1 ) θ t + 1 + β 1 ( θ t + 1 − θ t ) − ( 1 − β 1 ) θ t Combined with ( 26 ) , we have ( 1 − β 1 ) θ t + 1 + β 1 ( θ t + 1 − θ t ) = ( 1 − β 1 ) θ t + β 1 ( θ t − θ t − 1 ) − β 1 (cid:32) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:33) m t − 1 − α t ( 1 − β 1 ) h t / (cid:112) ˆ v t . Divide both sides by 1 − β 1 , we have θ t + 1 + β 1 1 − β 1 ( θ t + 1 − θ t ) = θ t + β 1 1 − β 1 ( θ t − θ t − 1 ) − β 1 1 − β 1 (cid:32) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:33) m t − 1 − α t h t / (cid:112) ˆ v t . Lemma C . 12 . Suppose that the conditions in Theorem C . 2 hold , then E [ f ( z t + 1 ) − f ( z 1 ) ] ≤ 4 (cid:88) i = 1 T i , ( 27 ) 20 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where T 1 = − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , β 1 1 − β 1 (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) m i − 1 (cid:105) (cid:35) T 2 = − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) T 3 = E   t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) β 1 1 − β 1 (cid:32) α t √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) m i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   T 4 = E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13) α i h i / (cid:112) ˆ v i (cid:13)(cid:13)(cid:13) 2 (cid:35) Proof . By the Lipschitz smoothness of ∇ f , f ( z t + 1 ) ≤ f ( z t ) + (cid:104)∇ f ( z t ) , z t + 1 − z t (cid:105) + L 2 (cid:107) z t + 1 − z t (cid:107) 2 , Based on ( C . 18 ) , we have E [ f ( z t + 1 ) − f ( z 1 ) ] = E (cid:34) t (cid:88) i = 1 f ( z i + 1 ) − f ( z i ) (cid:35) ≤ E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , z i + 1 − z i (cid:105) + L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:35) = − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , β 1 1 − β 1 (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) m i − 1 (cid:105) (cid:35) − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) + E (cid:20) t (cid:88) i = 1 L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:21) = T 1 + T 2 + E (cid:20) t (cid:88) i = 1 L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:21) , Then , using inequality (cid:107) a + b (cid:107) 2 ≤ 2 (cid:107) a (cid:107) 2 + 2 (cid:107) b (cid:107) 2 and combined with lemma C . 11 , E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:35) ≤ T 3 + T 4 Lemma C . 13 . In this part , we bound T 1 , T 2 , T 3 Proof . • Bound for T 1 T 1 = − E (cid:34) t (cid:88) i = 2 (cid:104)∇ f ( z i ) , β 1 1 − β 1 (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) m i − 1 (cid:105) (cid:35) ≤ E   t (cid:88) i = 1 (cid:107)∇ f ( z i ) (cid:107) (cid:107) m i − 1 (cid:107) (cid:18) 1 1 − β 1 − 1 (cid:19) d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) (cid:12) (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) j (cid:12)(cid:12)(cid:12) (cid:12)   ≤ H 2 β 1 1 − β 1 E   t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12) 21 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers • Bound for T 3 T 3 ≤ LE   t (cid:88) i = 2 (cid:18) β 1 1 − β 1 (cid:19) 2 d (cid:88) j = 1   (cid:32) α t √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) 2 j ( m i − 1 ) 2 j     ≤ (cid:18) β 1 1 − β 1 (cid:19) 2 LH 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) α t √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) 2 j   • Bound for T 2 T 2 = − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) = − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) − ∇ f ( θ i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) . ( 28 ) The second term of ( 28 ) can be bounded as − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) − ∇ f ( θ i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) ≤ E (cid:34) t (cid:88) i = 2 1 2 (cid:107)∇ f ( z i ) − ∇ f ( θ i ) (cid:107) 2 + 1 2 (cid:107) α i h i / (cid:112) ˆ v i (cid:107) 2 (cid:35) ≤ L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) β 1 1 − β 1 α i − 1 m i − 1 / (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) α i h i / (cid:112) ˆ v i (cid:107) 2 (cid:35) = L 2 2 (cid:18) β 1 1 − β 1 (cid:19) 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13) α i − 1 m i − 1 / (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) α i h i / (cid:112) ˆ v i (cid:107) 2 (cid:35) where the second inequality is due to (cid:107)∇ f ( z i ) − ∇ f ( θ i ) (cid:107) ≤ L (cid:107) z i − θ i (cid:107) . Then consider the ﬁrst term of ( 28 ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , h i / (cid:112) ˆ v i (cid:105) (cid:35) = κE (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , g i / (cid:112) ˆ v i (cid:105) (cid:35) + µE (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , I i / (cid:112) ˆ v i (cid:105) (cid:35) Consider the term with κ E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , g i / (cid:112) ˆ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ( ∇ f ( θ i ) + δ i ) / (cid:112) ˆ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , δ i / (cid:112) ˆ v i (cid:105) (cid:35) . ( 29 ) 22 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers For the second term in RHS of ( 29 ) , we have E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , δ i / (cid:112) ˆ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 2 (cid:104)∇ f ( θ i ) , δ i ( α i / (cid:112) ˆ v i − α i − 1 / (cid:112) ˆ v i − 1 ) (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 2 α i − 1 (cid:104)∇ f ( θ i ) , δ i ( 1 / (cid:112) ˆ v i − 1 ) (cid:105) (cid:35) + E (cid:104) α 1 (cid:104)∇ f ( x 1 ) , δ 1 / (cid:112) ˆ v 1 (cid:105) (cid:105) ≥ E (cid:34) t (cid:88) i = 2 (cid:104)∇ f ( θ i ) , δ i ( α i / (cid:112) ˆ v i − α i − 1 / (cid:112) ˆ v i − 1 ) (cid:105) (cid:35) − 2 H 2 E   d (cid:88) j = 1 ( α 1 / (cid:112) ˆ v 1 ) j   ( 30 ) where the last equation is because given θ i , ˆ v i − 1 , E (cid:2) δ i ( 1 / (cid:112) ˆ v i − 1 ) | θ i , ˆ v i − 1 (cid:3) = 0 and (cid:107) δ i (cid:107) ≤ 2 H . Further , we have E (cid:34) t (cid:88) i = 2 (cid:104)∇ f ( θ i ) , δ t ( α i / (cid:112) ˆ v i − α i − 1 / (cid:112) ˆ v i − 1 ) (cid:105) (cid:35) = E   t (cid:88) i = 2 d (cid:88) j = 1 ( ∇ f ( θ i ) ) j ( δ t ) j ( α i / ( (cid:112) ˆ v i ) j − α i − 1 / ( (cid:112) ˆ v i − 1 ) j )   ≥ − E   t (cid:88) i = 2 d (cid:88) j = 1 | ( ∇ f ( θ i ) ) j | | ( δ t ) j | (cid:12)(cid:12)(cid:12) ( α i / ( (cid:112) ˆ v i ) j − α i − 1 / ( (cid:112) ˆ v i − 1 ) j ) (cid:12)(cid:12)(cid:12) ≥ − 2 H 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( α i / ( (cid:112) ˆ v i ) j − α i − 1 / ( (cid:112) ˆ v i − 1 ) j ) (cid:12)(cid:12)(cid:12) ( 31 ) Substitute ( 30 ) and ( 31 ) into ( 29 ) , we then get − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , g i / (cid:112) ˆ v i (cid:105) (cid:35) ≤ 2 H 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( α i / ( (cid:112) ˆ v i ) j − α i − 1 / ( (cid:112) ˆ v i − 1 ) j ) (cid:12)(cid:12)(cid:12) + 2 H 2 E   d (cid:88) j = 1 ( α 1 / (cid:112) ˆ v 1 ) j   − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) ( 32 ) Then , consider the term with µ . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . In other words , we can bound it the same way as the term with κ . After all these bounds , we ﬁnally get T 2 ≤ L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) β 1 1 − β 1 α i − 1 m i − 1 / (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) α i h i / (cid:112) ˆ v i (cid:107) 2 (cid:35) + 2 ( κ + µ ) H 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( α i / ( (cid:112) ˆ v i ) j − α i − 1 / ( (cid:112) ˆ v i − 1 ) j ) (cid:12)(cid:12)(cid:12) + 2 ( κ + µ ) H 2 E   d (cid:88) j = 1 ( α 1 / (cid:112) ˆ v 1 ) j   − ( κ + µ ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) 23 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Lemma C . 14 . Suppose the conditions in theorem C . 2 hold . Then we have E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) ≤ E (cid:34) C 1 t (cid:88) i = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t √ ˆ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 t − 1 (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 where C 1 , C 2 , C 3 , C 4 and C 5 are independent of the step . Proof . Combining lemma C . 12 and lemma C . 13 , we get E [ f ( z t + 1 ) − f ( z 1 ) ] ≤ H 2 β 1 1 − β 1 E   t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12) + (cid:18) β 1 1 − β 1 (cid:19) 2 LH 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) α t √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) 2 j   + E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13) α i h i / (cid:112) ˆ v i (cid:13)(cid:13)(cid:13) 2 (cid:35) + L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) β 1 1 − β 1 α i − 1 m i − 1 / (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) α i h i / (cid:112) ˆ v i (cid:107) 2 (cid:35) + 2 ( κ + µ ) H 2 E   t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)   + 2 ( κ + µ ) H 2 E   d (cid:88) j = 1 ( α 1 / (cid:112) ˆ v 1 ) j   − ( κ + µ ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) 24 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers By merging similar terms in above inequality and noticing that κ + µ > 0 , we get E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) / (cid:112) ˆ v i (cid:105) (cid:35) ≤ (cid:18) 2 H 2 + β 1 H 2 ( 1 − β 1 ) ( κ + µ ) (cid:19) E   t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) α i √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12) + (cid:18) β 1 1 − β 1 (cid:19) 2 LH 2 κ + µE   t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) α t √ ˆ v i − α i − 1 (cid:112) ˆ v i − 1 (cid:33) 2 j   + (cid:18) 2 L + 1 2 ( κ + µ ) (cid:19) E (cid:34) t (cid:88) i = 2 (cid:107) α i h i √ ˆ v i (cid:107) 2 (cid:35) + L 2 2 ( κ + µ ) (cid:18) β 1 1 − β 1 (cid:19) 2 E   t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   + 2 H 2 E   d (cid:88) j = 1 ( α 1 / (cid:112) ˆ v 1 ) j   + 1 κ + µE [ f ( z 1 ) − f ( z t + 1 ) ] = E (cid:34) C 1 t (cid:88) i = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t √ ˆ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 t − 1 (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 ( 33 ) Theorem C . 15 . ( Convergence of A DMETA R for non - convex optimization ) Under the assumptions : • ∇ f exits and is Lipschitz - continuous , i . e , | | ∇ f ( x ) − ∇ f ( y ) | | ≤ L | | x − y | | , ∀ x , y ; f is also lower bounded . • At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ∇ f is also bounded . • The noisy gradient is unbiased , and has independent noise , i . e . g t = ∇ f ( θ t ) + δ t , E [ δ t ] = 0 and δ i ⊥ δ j , ∀ i (cid:54) = j . Assume min j ∈ [ d ] ( v 1 ) j ≥ c > 0 and α t = α / √ t , then for any T we have : min t ∈ [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) 2 ≤ 1 √ T ( Q 1 + Q 2 log T ) where Q 1 and Q 2 are constants independent of T . Proof . We bound non - constant terms in RHS of ( 33 ) , which is given by E (cid:34) C 1 T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t √ ˆ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 T − 1 (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 • Bound the term with C 1 . 25 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Note that min j ∈ [ d ] ( √ ˆ v 1 ) j ≥ min j ∈ [ d ] | ( h 1 ) j | ≥ c > 0 , thus we have E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t √ ˆ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) ≤ E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t c (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) = E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) αh t c √ t (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) = E (cid:34) T (cid:88) t = 1 (cid:18) α c √ t (cid:19) 2 (cid:107) h t (cid:107) 2 (cid:35) ≤ H 2 α 2 c 2 T (cid:88) t = 1 1 t ≤ H 2 α 2 c 2 ( 1 + log T ) where the ﬁrst inequality is due to ( ˆ v t ) j ≥ ( ˆ v t − 1 ) j , and the last inequality is due to (cid:80) Tt = 1 1 t ≤ 1 + log T . • Bound the term with C 2 . Apply the same proof as above , we get t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 ≤ H 2 α 2 c 2 ( 1 + log T ) • Bound the term with C 3 . E (cid:34) T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 (cid:35) = E   d (cid:88) j = 1 T (cid:88) t = 2 (cid:32) α t − 1 ( (cid:112) ˆ v t − 1 ) j − α t ( √ ˆ v t ) j (cid:33)  = E   d (cid:88) j = 1 (cid:18) α 1 ( √ ˆ v 1 ) j − α T ( √ ˆ v T ) j (cid:19) ≤ E   d (cid:88) j = 1 α 1 ( √ ˆ v 1 ) j   ≤ dα c ( 34 ) where the ﬁrst equality is due to ( ˆ v t ) j ≥ ( ˆ v t − 1 ) j and α t ≤ α t − 1 , and the second equality is due to telescope sum . • Bound the term with C 4 . E   T − 1 (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   = E   T − 1 (cid:88) t = 2 d (cid:88) j = 1 (cid:32) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:33) 2 i   ≤ E   T − 1 (cid:88) t = 2 d (cid:88) j = 1 α c (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) i   ≤ dα 2 c 2 where the ﬁrst inequality is due to | ( α t / √ ˆ v t − α t − 1 / (cid:112) ˆ v t − 1 ) j | ≤ 1 / c . Then we have for A DMETA R , E (cid:34) C 1 T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) α t h t √ ˆ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α i − 1 m i − 1 (cid:112) ˆ v i − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 ( 35 ) + C 4 T − 1 (cid:88) t = 2 (cid:13) (cid:13) (cid:13)(cid:13)(cid:13) α t √ ˆ v t − α t − 1 (cid:112) ˆ v t − 1 (cid:13) (cid:13) (cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 ( 36 ) ≤ C 1 H 2 α 2 c 2 ( 1 + log T ) + C 2 H 2 α 2 c 2 ( 1 + log T ) + C 3 dα c + C 4 dα 2 c 2 + C 5 ( 37 ) 26 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Furthermore , due to (cid:107) g t (cid:107) ≤ H , we have ( ˆ v t ) j ≤ H 2 , then we get α / ( (cid:112) ˆ v t ) j ≥ 1 H √ t Thus we have E (cid:34) T (cid:88) t = 1 α i (cid:104)∇ f ( θ t ) , ∇ f ( θ t ) / (cid:112) ˆ v t (cid:105) (cid:35) ≥ E (cid:34) T (cid:88) t = 1 1 H √ t (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:35) ≥ √ T H min t ∈ [ T ] E (cid:2) (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:3) ( 38 ) Combining ( 37 ) and ( 38 ) , we have min t ∈ [ T ] E (cid:2) (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:3) ≤ H √ T (cid:18) ( C 1 + C 2 ) H 2 α 2 c 2 ( 1 + log T ) + C 3 dα c + C 4 dα 2 c 2 + C 5 (cid:19) = 1 √ T ( Q 1 + Q 2 log T ) This completes the proof . C . 4 . Convergence Analysis of AdmetaS for Convex Optimization Lemma C . 16 ( Bound for (cid:80) Tt = 1 α t (cid:107) m t (cid:107) 2 ) . Under Assumption in Theorem 3 , we have T (cid:88) t = 1 α t (cid:107) m t (cid:107) 2 ≤ 2 αdG 2 ∞ √ T Proof . First , we bound (cid:107) m t (cid:107) . (cid:107) m t (cid:107) 2 ≤ d (cid:107) m t (cid:107) 2 ∞ ≤ dG 2 ∞ ( 39 ) Now we can bound (cid:80) Tt = 1 α t (cid:107) m t (cid:107) 2 T (cid:88) t = 1 α t (cid:107) m t (cid:107) 2 ≤ dG 2 ∞ T (cid:88) t = 1 α t = αdG 2 ∞ T (cid:88) t = 1 1 √ t ≤ 2 αdG 2 ∞ √ T Theorem C . 17 . ( Convergence of A DMETA S for convex optimization ) Let { θ t } be the sequence obtained by A DMETA S , 0 ≤ λ , β < 1 , α t = α √ t , ∀ t ∈ [ T ] . Suppose x ∈ F , where F ⊂ R d and has bounded diameter D ∞ , i . e . | | θ t − θ | | ∞ ≤ D ∞ , ∀ t ∈ [ T ] . . Assume f ( θ ) is a convex func - tion and | | g t | | ∞ is bounded . Denote the optimal point as θ . For θ t generated , A DMETA S achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( θ t ) − f t ( θ ) ] = O ( √ T ) Proof . • Bound for (cid:80) T t = 1 (cid:104) m t , θ t − θ (cid:105) . From the update process , we get (cid:107) θ t + 1 − θ (cid:107) 2 = (cid:107) θ t − θ − α t m t (cid:107) 2 = (cid:107) θ t − θ (cid:107) 2 − 2 α t (cid:104) m t , θ t − θ (cid:105) + α 2 t (cid:107) m t (cid:107) 2 27 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers thus we have T (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) = T (cid:88) t = 1 1 2 α t (cid:0) (cid:107) θ t − θ (cid:107) 2 − (cid:107) θ t + 1 − θ (cid:107) 2 (cid:1) + T (cid:88) i = 1 α t 2 (cid:107) m t (cid:107) 2 Consider the left - hand side T (cid:88) t = 1 1 2 α t (cid:0) (cid:107) θ t − θ (cid:107) 2 − (cid:107) θ t + 1 − θ (cid:107) 2 (cid:1) = 1 2 α 1 (cid:107) θ 1 − θ (cid:107) 2 + T (cid:88) t = 2 (cid:18) 1 2 α t − 1 2 α t − 1 (cid:19) (cid:107) θ t − θ (cid:107) 2 − 1 2 α T (cid:107) θ T + 1 − θ (cid:107) 2 ≤ dD 2 ∞ 2 α 1 + dD 2 ∞ T (cid:88) t = 2 (cid:18) 1 2 α t − 1 2 α t − 1 (cid:19) + 0 = dD 2 ∞ 2 α T Finally , we get T (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) ≤ dD 2 ∞ 2 α T + T (cid:88) i = 1 α t 2 (cid:107) m t (cid:107) 2 • Bound for (cid:80) Tt = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) . T (cid:88) t = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) = T − 1 (cid:88) t = 1 (cid:104) m t , θ t − θ t + 1 (cid:105) = T − 1 (cid:88) t = 1 (cid:104) m t , α t m t (cid:105) = T − 1 (cid:88) t = 1 α t (cid:107) m t (cid:107) 2 • Bound for (cid:104) m T , θ T − θ (cid:105) . (cid:104) m T , θ T − θ (cid:105) ≤ α T (cid:107) m T (cid:107) 2 + 1 4 α T (cid:107) θ T − θ (cid:107) 2 ≤ α T (cid:107) m T (cid:107) 2 + dD 2 ∞ 4 α T where the ﬁrst inequality follows from Young’s inequality . Combining all these preparations , we obtain T (cid:88) t = 1 (cid:104) h t , θ t − θ (cid:105) = 1 1 − β (cid:0) (cid:104) m T , θ T − θ (cid:105) − (cid:104) m 0 , θ 0 − θ (cid:105) (cid:1) + (cid:104) m 0 , θ 0 − θ (cid:105) + T − 1 (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) + β 1 − β T (cid:88) t = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) = β 1 − β (cid:104) m T , θ T − θ (cid:105) + β 1 − β T (cid:88) t = 1 (cid:104) m t − 1 , θ t − 1 − θ t (cid:105) + T (cid:88) t = 1 (cid:104) m t , θ t − θ (cid:105) ≤ β 1 − β (cid:32) dD ∞ 4 α T + T (cid:88) t = 1 α t (cid:107) m t (cid:107) 2 (cid:33) + dD 2 ∞ 2 α T + T (cid:88) i = 1 α t 2 (cid:107) m t (cid:107) 2 ≤ (cid:18) β 1 − β + 2 (cid:19) dD ∞ 4 α T + (cid:18) 2 αβ 1 − β + α (cid:19) dG 2 ∞ √ T 28 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers This proves that (cid:80) Tt = 1 (cid:104) h t , θ t − θ (cid:105) = O ( √ T ) . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Since h t = κg t + µI t , h t is the same order as g t when the time is long enough , thus we have T (cid:88) t = 1 (cid:104) g t , θ t − θ (cid:105) = O ( √ T ) ( 40 ) In addition , due to the convexity of f ( . ) , we have R ( T ) = T (cid:88) t = 1 ( f t ( θ t ) − f t ( x ) ) ≤ T (cid:88) t = 1 (cid:104) g t , θ t − θ (cid:105) Combined with ( 40 ) , we complete the proof . C . 5 . Convergence Analysis of AdmetaS for Non - convex Optimization Lemma C . 18 . Set θ 0 (cid:44) θ 1 in Algorithm ( 2 ) , and deﬁne z t as z t = θ t + β 1 − β ( θ t − θ t − 1 ) , ∀ t ≥ 1 . ( 41 ) Then the following holds z t + 1 − z t = − β 1 − β ( α t − α t − 1 ) m t − 1 − α t h t Proof . By the update rule of A DMETA S , we have θ t + 1 − θ t = − α t m t = − α t [ βm t − 1 + ( 1 − β ) h t ] = β α t α t − 1 ( θ t − θ t − 1 ) − α t ( 1 − β ) h t = β ( θ t − θ t − 1 ) + β (cid:18) α t α t − 1 − 1 (cid:19) ( θ t − θ t − 1 ) − α t ( 1 − β ) h t = β ( θ t − θ t − 1 ) − β ( α t − α t − 1 ) m t − 1 − α t ( 1 − β ) h t ( 42 ) Since we also have θ t + 1 − θ t = ( 1 − β ) θ t + 1 + β ( θ t + 1 − θ t ) − ( 1 − β ) θ t Combined with ( 42 ) , we have ( 1 − β ) θ t + 1 + β ( θ t + 1 − θ t ) = ( 1 − β ) θ t + β ( θ t − θ t − 1 ) − β ( α t − α t − 1 ) m t − 1 − α t ( 1 − β ) h t Divide both sides by 1 − β θ t + 1 + β 1 − β ( θ t + 1 − θ t ) = θ t + β 1 − β ( θ t − θ t − 1 ) − β 1 − β ( α t − α t − 1 ) m t − 1 − α t h t Lemma C . 19 . Suppose that the conditions in Theorem C . 4 hold , then E [ f ( z t + 1 ) − f ( z 1 ) ] ≤ 4 (cid:88) i = 1 T i , 29 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where T 1 = − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , β 1 1 − β 1 ( α i − α i − 1 ) m i − 1 (cid:105) (cid:35) T 2 = − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) , h i (cid:105) (cid:35) T 3 = E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13)(cid:13) β 1 − β ( α i − α i − 1 ) m i − 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) T 4 = E (cid:34) t (cid:88) i = 1 L (cid:107) α i h i (cid:107) 2 (cid:35) Proof . By the Lipschitz smoothness of ∇ f , f ( z t + 1 ) ≤ f ( z t ) + (cid:104)∇ f ( z t ) , z t + 1 − z t (cid:105) + L 2 (cid:107) z t + 1 − z t (cid:107) 2 , Based on ( C . 18 ) , we have E [ f ( z t + 1 ) − f ( z 1 ) ] = E (cid:34) t (cid:88) i = 1 f ( z i + 1 ) − f ( z i ) (cid:35) ≤ E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , z i + 1 − z i (cid:105) + L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:35) = − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) , β 1 − β ( α i − α i − 1 ) m i − 1 (cid:105) (cid:35) − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( z i ) , h i (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:35) Then , using inequality (cid:107) a + b (cid:107) 2 ≤ 2 (cid:107) a (cid:107) 2 + 2 (cid:107) b (cid:107) 2 and combined with lemma C . 18 , E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 − z i (cid:107) 2 (cid:35) ≤ T 3 + T 4 Lemma C . 20 . In this part , we bound T 1 , T 2 , T 3 , T 4 . We claim that the order of them is O ( log T ) . Proof . • Bound for T 1 T 1 ≤ E (cid:34) t (cid:88) i = 1 (cid:107)∇ f ( z i ) (cid:107)(cid:107) m i − 1 (cid:107) β 1 − β | α i − α i − 1 | (cid:35) ≤ H 2 β 1 − β E (cid:34) t (cid:88) i = 1 | α i − α i − 1 | (cid:35) ≤ H 2 β 1 − β α 30 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where the second and last inequality is due to the monotone decreasing property of α i • Bound for T 3 T 3 ≤ (cid:18) β 1 − β (cid:19) 2 LH 2 E (cid:34) t (cid:88) i = 1 ( α i − α i − 1 ) 2 (cid:35) ≤ 2 α (cid:18) β 1 − β (cid:19) 2 LH 2 E (cid:34) t (cid:88) i = 1 | α i − α i − 1 | (cid:35) ≤ 2 α 2 (cid:18) β 1 − β (cid:19) 2 LH 2 where the monotone decreasing property of α i is also used • Bound for T 4 T 4 ≤ H 2 Lα 2 E (cid:34) t (cid:88) i = 1 1 t (cid:35) ≤ H 2 Lα 2 ( 1 + log T ) where the second inequality is due to (cid:80) ti = 1 1 t ≤ 1 + log T • Bound for T 2 T 2 = − E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , h i (cid:105) (cid:35) − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) − ∇ f ( θ i ) , h i (cid:105) (cid:35) ( 43 ) The second term of ( 43 ) can be bounded as − E (cid:34) t (cid:88) i = 1 (cid:104)∇ f ( z i ) − ∇ f ( θ i ) , h i (cid:105) (cid:35) ≤ E (cid:34) t (cid:88) i = 1 1 2 (cid:107)∇ f ( z i ) − ∇ f ( θ i ) (cid:107) 2 + 1 2 (cid:107) α i h i (cid:107) 2 (cid:35) ≤ L 2 2 E (cid:34) t (cid:88) i = 1 (cid:107) β 1 − β α i − 1 m i − 1 (cid:107) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 1 (cid:107) α i h i (cid:107) 2 (cid:35) ≤ α 2 H 2 L 2 2 (cid:18) β 1 − β (cid:19) 2 t (cid:88) i = 1 1 t + α 2 H 2 2 t (cid:88) i = 1 1 t ≤ α 2 H 2 2 (cid:34) L 2 (cid:18) β 1 − β (cid:19) 2 + 1 (cid:35) ( 1 + log T ) where the second inequality is due to (cid:107)∇ f ( z i ) − ∇ f ( θ i ) (cid:107) ≤ L (cid:107) z i − θ i (cid:107) . 31 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Then , consider the ﬁrst term of ( 43 ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , h i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , κg i + µI i (cid:105) (cid:35) ≈ κE (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) + δ i (cid:105) (cid:35) + µE (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) + δ i (cid:105) (cid:35) = ( κ + µ ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) (cid:105) (cid:35) The second and third equality holds for the follow reasons : on the one hand , g t = ∇ f ( θ t ) + δ t in which E [ δ t ] = 0 , so according to ( Chen et al . , 2018 ) , given θ i , E [ δ i | θ i ] = 0 ; On the other hand , suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Finally , we can ﬁnally bound T 2 T 2 ≤ α 2 H 2 2 (cid:34) L 2 (cid:18) β 1 − β (cid:19) 2 + 1 (cid:35) ( 1 + log T ) + ( κ + µ ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) (cid:105) (cid:35) Theorem C . 21 . ( Convergence of A DMETA S in non - convex stochastic optimization ) Under the assumptions : • ∇ f exits and is Lipschitz - continuous , i . e , | | ∇ f ( x ) − ∇ f ( y ) | | ≤ L | | x − y | | , ∀ x , y ; f is also lower bounded . • At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ∇ f is also bounded . • The noisy gradient is unbiased , and has independent noise , i . e . g t = ∇ f ( θ t ) + δ t , E [ δ t ] = 0 and δ i ⊥ δ j , ∀ i (cid:54) = j . And α t = α / √ t , then for any T we have : min t ∈ [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ≤ 1 √ T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where Q (cid:48) 1 and Q (cid:48) 2 are constants independent of T . Proof . We combine lemma C . 18 , lemma C . 19 and lemma C . 20 to bound the overall expected descent of the objective . First , we have E [ f ( z t + 1 ) − f ( z 1 ) ] ≤ T 1 + T 2 + T 3 + T 4 ( 44 ) ≤ H 2 β 1 − β α + α 2 H 2 2 (cid:34) L 2 (cid:18) β 1 − β (cid:19) 2 + 1 (cid:35) ( 1 + log T ) ( 45 ) − ( κ + µ ) E (cid:34) t (cid:88) i = 1 α i (cid:104)∇ f ( θ i ) , ∇ f ( θ i ) (cid:105) (cid:35) ( 46 ) + 2 α 2 (cid:18) β 1 − β (cid:19) 2 LH 2 + H 2 Lα 2 ( 1 + log T ) ( 47 ) Notice that E (cid:34) T (cid:88) t = 1 α i (cid:104)∇ f ( θ t ) , ∇ f ( θ t ) (cid:105) (cid:35) ≥ E (cid:34) T (cid:88) t = 1 1 √ t (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:35) ≥ √ T min t ∈ [ T ] E (cid:2) (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:3) ( 48 ) 32 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Rearrange ( 44 ) , combined with ( 48 ) and notice that κ + µ > 0 , we have min t ∈ [ T ] E (cid:2) (cid:107)∇ f ( θ t ) (cid:107) 2 (cid:3) ≤ 1 √ T E (cid:34) T (cid:88) t = 1 α i (cid:104)∇ f ( θ t ) , ∇ f ( θ t ) (cid:105) (cid:35) ≤ 1 √ T (cid:34) 1 κ + µ (cid:32) α 2 H 2 L 2 2 (cid:18) β 1 − β (cid:19) 2 + α 2 H 2 2 + H 2 Lα 2 (cid:33) ( 1 + log T ) + 1 κ + µ (cid:32) H 2 β 1 − β α + 2 α 2 (cid:18) β 1 − β (cid:19) 2 LH 2 + E [ f ( z 1 ) − f ( z ∗ ) ] (cid:33) (cid:35) = 1 √ T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where z ∗ is the optimal of f , i . e . z ∗ = arg min z f ( z ) This completes the proof . C . 6 . Convergence Analysis of Forward - looking In this section , based on ( Wang et al . , 2020 ) , we further analysis forward - looking part to complete the convergence proof of A DMETA optimizer . According to ( Zhang et al . , 2019 ) , Lookahead is an algorithm that can be combined with any standard optimization method . The same is true for dynamic lookahead method in forward - looking part . What’s more , optimizers with forward - looking is essentially processing with two loops as discussed in the main text . The fast weight is updated by optimizers , while the slow weight is updated by interpolating with fast weight every given period . In other words , the slow weight is updated passively . Therefore , though the slow weight is relevant to optimizers , it is almost irrelevant to the selection of optimizers . For this reason , we only prove the convergence of forward - looking of A DMETA S , which can be easily extended to the A DMETA R . Remarks : ( some preliminaries ) Based on the design of the asymptotic dynamic weight η t of the forward - looking part , it can be concluded that when it runs for a long time , η t is highly close to the set point , at which we can safely assume that η t is a constant and thus we denote it as η . In this way , the analysis of a dynamic lookahead is the same as the case of static lookahead . According to algorithm of A DMETA , the slow weight φ t updates every k steps . We can assume that the slow weight is trained in sync with fast weight . For this purpose , all we should do is to stipulate φ τk + l = φ τk , where k denotes the synchronization period , τ ∈ N ∗ and 0 ≤ l < k . Deﬁne y t = ηθ t + ( 1 − η ) θ t , then according to the update of θ t and φ t , we have y t + 1 = y t − ηα t m t and on each period of synchronization , we have y τk − θ τk = ( 1 − η ) ( φ τk − θ τk ) = 0 y τk − φ τk = η ( θ τk − φ τk ) = 0 Theorem C . 22 . ( convergence of forward - looking part ) Suppose f ( . ) is L - smooth , i . e , | | ∇ f ( x ) − ∇ f ( y ) | | ≤ L | | x − y | | , ∀ x , y . The bias of noisy gradient is bounded , i . e . , | δ t | ≤ σ , where δ t = ∇ f ( θ t ) − g t . Then we have that : 1 T T (cid:88) t = 0 E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∇ f ( θ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ≤ O ( 1 √ T ) Proof . Following the L - smooth property , we have f ( y t + 1 ) − f ( y t ) ≤ − ηα t (cid:104)∇ f ( y t ) , m t (cid:105) + η 2 α 2 t L 2 (cid:107) m t (cid:107) 2 ( 49 ) 33 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Taking the expectation of both sides , E [ (cid:104)∇ f ( y t ) , m t (cid:105) ] = E [ (cid:104)∇ f ( y t ) , κg t + µI t (cid:105) ] = κ E [ (cid:104)∇ f ( y t ) , g t (cid:105) ] + µ E [ (cid:104)∇ f ( y t ) , I t (cid:105) ] ( 50 ) Consider the term with κ , E [ (cid:104)∇ f ( y t ) , g t (cid:105) ] = (cid:104)∇ f ( y t ) , ∇ f ( θ t ) (cid:105) = 1 2 [ (cid:107)∇ f ( y t ) (cid:107) 2 + (cid:107)∇ f ( θ t ) (cid:107) 2 − (cid:107)∇ f ( y t ) − ∇ f ( θ t ) (cid:107) 2 ] ≥ 1 2 [ (cid:107)∇ f ( y t ) (cid:107) 2 + (cid:107)∇ f ( θ t ) (cid:107) 2 − L 2 (cid:107) y t − θ t (cid:107) 2 ] = 1 2 [ (cid:107)∇ f ( y t ) (cid:107) 2 + (cid:107)∇ f ( θ t ) (cid:107) 2 − ( 1 − η ) 2 L 2 (cid:107) φ t − θ t (cid:107) 2 ] ( 51 ) Suppose the optimizer runs for a long time , the bias of EMA is small enough , thus E ( I t ) approaches E ( g t ) . For this reason , we can estimate the term with µ in ( 50 ) the same way as ( 51 ) . Based on the bounded bias gradient assumption and inequality that ( a + b ) 2 ≤ 2 a 2 + 2 b 2 , we have : E [ (cid:107) m t (cid:107) 2 ] ≤ 2 µ 2 E [ (cid:107) I t (cid:107) 2 ] (cid:107) + 2 κ 2 E [ (cid:107) g t (cid:107) 2 ] (cid:107) ≤ 4 ( µ 2 + κ 2 ) E (cid:107)∇ f ( θ t ) (cid:107) 2 + 4 ( µ 2 + κ 2 ) σ 2 ( 52 ) Combined with ( 49 ) , ( 50 ) , ( 51 ) and ( 52 ) , rearrange the inequality and take the expectation E [ f ( y t + 1 ) ] ≤ E [ f ( y t ) ] − ηα t ( µ + κ ) 2 E [ (cid:107)∇ f ( y t ) (cid:107) 2 ] − ηα t ( µ + κ ) 2 E [ (cid:107)∇ f ( θ t ) (cid:107) 2 ] + ηα t ( 1 − η ) 2 L 2 ( µ + κ ) 2 E [ (cid:107) φ t − θ t (cid:107) 2 ] + 2 ( µ 2 + κ 2 ) η 2 α 2 t L E [ (cid:107)∇ f ( θ t ) (cid:107) 2 ] + 2 ( µ 2 + κ 2 ) η 2 α 2 t Lσ 2 Since the learning rate is decreasing to zero , we can safely assume that after several iterations , 1 − ηα t L > 0 . Then , summing over one outer loop E [ f ( y ( τ + 1 ) k ) ] − E [ f ( y τk ) ] ≤ − ηα ( τ + 1 ) k ( µ + κ ) 2 k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( y τk + l ) (cid:107) 2 ] + 2 ( µ 2 + κ 2 ) kη 2 α 2 τk Lσ 2 − ηα ( τ + 1 ) k ( µ + κ − 4 ( µ 2 + κ 2 ) ηα ( τ + 1 ) k L ) 2 k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( θ τk + l ) (cid:107) 2 ] + ηα τk ( 1 − η ) 2 L 2 ( µ + κ ) 2 k − 1 (cid:88) l = 0 E [ (cid:107) φ τk + l − θ τk + l (cid:107) 2 ] ( 53 ) 34 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Consider the last term of ( 53 ) , we have E [ (cid:107) φ τk + l − θ τk + l (cid:107) 2 ] = E [ (cid:107) θ τk − θ τk + l (cid:107) 2 ] ≤ α 2 τk E   (cid:107) l − 1 (cid:88) j = 0 m τk + j (cid:107) 2   = 2 κ 2 α 2 τk E   (cid:107) l − 1 (cid:88) j = 0 g τk + j (cid:107) 2   + 2 µ 2 α 2 τk E   (cid:107) l − 1 (cid:88) j = 0 I τk + j (cid:107) 2   ≤ 4 κ 2 α 2 τk E   (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l − 1 (cid:88) j = 0 ( g τk + j − ∇ f ( θ τk + j ) ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   + 4 κ 2 α 2 τk E   (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l − 1 (cid:88) j = 0 ∇ f ( θ τk + j ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   + 4 µ 2 α 2 τk E   (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l − 1 (cid:88) j = 0 ( I τk + j − ∇ f ( θ τk + j ) ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   + 4 µ 2 α 2 τk E   (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l − 1 (cid:88) j = 0 ∇ f ( θ τk + j ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2   ≤ 4 ( κ 2 + µ 2 ) σ 2 lα 2 τk + 4 ( µ 2 + κ 2 ) α 2 τk E   (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) l − 1 (cid:88) j = 0 ∇ f ( θ τk + j ) (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) 2   ≤ 4 ( κ 2 + µ 2 ) σ 2 lα 2 τk + 4 ( µ 2 + κ 2 ) lα 2 τk l − 1 (cid:88) j = 0 E [ (cid:107)∇ f ( θ τk + j ) (cid:107) 2 ] where the ﬁrst equality using the property that θ τk = φ τk = φ τk + l . Summing from l = 0 to l = k − 1 , we get , k − 1 (cid:88) l = 0 E [ (cid:107) φ τk + l − θ τk + l (cid:107) 2 ] ≤ 2 ( κ 2 + µ 2 ) σ 2 α 2 τk k ( k − 1 ) + 4 ( µ 2 + κ 2 ) α 2 τk k − 1 (cid:88) l = 0 l l − 1 (cid:88) j = 0 E [ (cid:107)∇ f ( θ τk + j ) (cid:107) 2 ] = 2 ( κ 2 + µ 2 ) σ 2 α 2 τk k ( k − 1 ) + 4 ( µ 2 + κ 2 ) α 2 τk k − 2 (cid:88) j = 0 E [ (cid:107)∇ f ( θ τk + j ) (cid:107) 2 ] k − 1 (cid:88) l = j + 1 l = 2 ( κ 2 + µ 2 ) σ 2 α 2 τk k ( k − 1 ) + 2 ( µ 2 + κ 2 ) α 2 τk k − 2 (cid:88) j = 0 E [ (cid:107)∇ f ( θ τk + j ) (cid:107) 2 ] ( j + k ) ( k − j − 1 ) ( j + k ) ( k − j − 1 ) achieves its maximal value when j = 0 . Therefore , we have k − 1 (cid:88) l = 0 E [ (cid:107) φ τk + l − θ τk + l (cid:107) 2 ] ≤ 2 ( κ 2 + µ 2 ) σ 2 α 2 τk k ( k − 1 ) + 2 ( µ 2 + κ 2 ) α 2 τk k ( k − 1 ) k − 2 (cid:88) j = 0 E [ (cid:107)∇ f ( θ τk + j ) (cid:107) 2 ] Here , we can ﬁnally bound the the last term of ( 53 ) E [ f ( y ( τ + 1 ) k ) ] − E [ f ( y τk ) ] ≤ − ηα ( τ + 1 ) k ( µ + κ ) 2 k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( y τk + l ) (cid:107) 2 ] + G + M k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( θ τk + l ) (cid:107) 2 ] ≤ − ηα ( τ + 1 ) k ( µ + κ ) 2 k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( y τk + l ) (cid:107) 2 ] + G ( 54 ) 35 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where G = 2 ( µ 2 + κ 2 ) kη 2 α 2 τk Lσ 2 + ( κ 2 + µ 2 ) ( κ + µ ) η ( 1 − η ) 2 L 2 σ 2 k ( k − 1 ) α 3 τk and M = − ηα ( τ + 1 ) k ( µ + κ − 4 ( µ 2 + κ 2 ) ηα ( τ + 1 ) k L ) 2 + ( κ 2 + µ 2 ) ( κ + µ ) η ( 1 − η ) 2 L 2 σ 2 k ( k − 1 ) α 3 τk When α is small enough , M is below zero , for which the second inequality of ( 54 ) holds . Summing from τ = 0 to τ = Υ − 1 , we get E [ f ( y Υ k ) ] − E [ f ( y 0 ) ] ≤ − η ( µ + κ ) 2 Υ − 1 (cid:88) τ = 0 α ( τ + 1 ) k k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( y τk + l ) (cid:107) 2 ] + 2 ( µ 2 + κ 2 ) kη 2 Lσ 2 Υ − 1 (cid:88) τ = 0 α 2 τk + ( κ 2 + µ 2 ) ( κ + µ ) η ( 1 − η ) 2 L 2 σ 2 k ( k − 1 ) Υ − 1 (cid:88) τ = 0 α 3 τk Following ( Wang et al . , 2020 ) , we ﬁrst assume the learning rate α as a ﬁxed constant , then rearrange the inequality above , we get 1 Υ k Υ − 1 (cid:88) τ = 0 k − 1 (cid:88) l = 0 E [ (cid:107)∇ f ( y τk + l ) (cid:107) 2 ] ≤ 2 [ f ( y 0 ) − f inf ] ηα Υ k ( µ + κ ) + 4 ( µ 2 + κ 2 ) ηαLσ 2 µ + κ + 2 ( κ 2 + µ 2 ) ( 1 − η ) 2 α 2 L 2 σ 2 ( k − 1 ) Deﬁne T as Υ k and set the learning rate α to 1 / √ T 1 T T − 1 (cid:88) t = 0 E [ (cid:107)∇ f ( y t ) (cid:107) 2 ] ≤ 2 [ f ( y 0 ) − f inf ] η √ T ( µ + κ ) + 4 ( µ 2 + κ 2 ) ηLσ 2 ( µ + κ ) √ T + 2 ( κ 2 + µ 2 ) ( 1 − η ) 2 L 2 σ 2 ( k − 1 ) T = O ( 1 √ T ) D . Analysis of Convergence Rate For convex situation , we adopt the regret function to estimate the convergence rate . And for non - convex situation , we adopt the minimum of the expectation of the squared gradient to estimate the convergence , which are corresponding to the proof of convergence since the process of the convergence proof is actually the process of ﬁnding the convergence rate . From Table 6 , we notice that the convergence rates for all optimizers for convex case are of magnitude of O ( 1 / √ T ) and for non - convex are of O ( log T / √ T ) , which means in essence , algorithms based on gradient decent follow a similar rate constraint . However , the convergence speed of different optimizers may attribute to many other factors , such as on the implementation . Therefore additional statistical experiments are needed for analysis , as we did in Table 4 . E . Experimental Details E . 1 . Hyperparameter Tuning For A DMETA optimizer , we ﬁrst determined a rough value range for learning rate and lambda with the toy model according to the visualization as in Figure B . While for other baseline optimizers , we refer to the recommended / default hyperparameter settings in the original paper . In this way , we get the rouge range of the hyperparameter in optimizers . Then , we search the hyperparameters in the adjacent interval , which is listed in the following three subsections . 36 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Case Optim Source Convergence rate ( a rough estimation ) Convex SGD ( Zinkevich , 2003 ) D 2 ∞ 2 α T + G 2 ∞ 2 (cid:80) Tt = 1 α t AMSGrad ( Reddi et al . , 2019 ) D 2 ∞ √ T α ( 1 − β 1 ) (cid:80) di = 1 ˆ v 1 T , i / 2 + D ∞ 2 ( 1 − β 1 ) (cid:80) Tt = 1 (cid:80) di = 1 β ˆ v 1 t , i / 2 α t + α √ 1 + log T ( 1 − β 1 ) 2 ( 1 − γ ) √ 1 − β 2 (cid:80) di = 1 (cid:107) g 1 : T , i (cid:107) A DMETA S - (cid:16) β 1 − β + 2 (cid:17) dD ∞ 4 α T + (cid:16) 2 αβ 1 − β + α (cid:17) dG 2 ∞ √ T A DMETA R - ( 2 − β 1 ) D 2 ∞ √ T 4 α ( 1 − β 1 ) (cid:80) di = 1 ˆ v 1 / 2 T , i + ( 2 + β 1 ) α √ 1 + log T 2 √ ( 1 − β 2 ) ( 1 − γ ) (cid:80) di = 1 (cid:107) h 1 : T , i (cid:107) 2 Non - convex SGD - - AMSGrad ( Chen et al . , 2018 ) H √ T (cid:16) C 1 H 2 c 2 ( 1 + log T ) + C 2 dc + C 3 dc 2 + C 4 (cid:17) A DMETA S - 1 √ T (cid:34) 1 κ + µ (cid:18) α 2 H 2 L 2 2 (cid:16) β 1 − β (cid:17) 2 + α 2 H 2 2 + H 2 Lα 2 (cid:19) ( 1 + log T ) + 1 κ + µ (cid:18) H 2 β 1 − β α + 2 α 2 (cid:16) β 1 − β (cid:17) 2 LH 2 + E [ f ( z 1 ) − f ( z ∗ ) ] (cid:19) (cid:35) A DMETA R - H √ T (cid:16) ( K 1 + K 2 ) H 2 α 2 c 2 ( 1 + log T ) + K 3 dαc + K 4 dα 2 c 2 + K 5 (cid:17) Table 6 . The comparison of convergence rate of several optimizers . Model task SGD SGDM Adam RAdam Ranger AdaBelief A DMETA R A DMETA S LR LR LR LR LR LR LR λ LR β ResNet - 110 CIFAR - 10 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 001 0 . 05 0 . 1 0 . 05 0 . 2 CIFAR - 100 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 01 0 . 05 0 . 05 0 . 05 0 . 1 PyramidNet CIFAR - 10 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 001 0 . 01 0 . 1 0 . 05 0 . 4 CIFAR - 100 0 . 5 0 . 5 0 . 001 0 . 01 0 . 01 0 . 001 0 . 01 0 . 1 0 . 05 0 . 1 Table 7 . Optimizer hyperparameter settings on the CIFAR task . E . 2 . Image Classiﬁcation We conduct image classiﬁcation experiments on CIFAR - 10 and CIFAR - 100 datasets , which are trained on a single NVIDIA RTX - 3090 GPU . Typical architectures like ResNet - 110 and PyramidNet are employed as the baseline models . In the ResNet - 110 architecture , there are 54 stacked identical 3 × 3 convolutional layers with 54 two - layer Residual Units ( He et al . , 2016 ) . While in the PyramidNet architecture , there are 110 layers with a widening factor of 48 ( Han et al . , 2017 ) . We set the training batch size to 128 and the validation batch size to 256 . Both models are trained with 160 epochs . Milestone schedule is adopted as the learning rate decay strategy , with learning rate decaying at the end of 80 - th and 120 - th epochs by 0 . 1 . We report the hyperparameters tuning for our proposed A DMETA and other optimizers for reproduction of our experiments . For all optimizers , the weight decay is ﬁxed as 1 e − 4 . The searching scheme of hyperparameter settings for each optimizer is concluded as follows : • For SGD and SGDM , the momentum is ﬁxed as 0 . 9 , and the best - performing learning rate is searched from { 0 . 01 , 0 . 05 , 0 . 1 } and recommended values in original paper . For our A DMETA S , the λ is set to ﬁxed 0 . 9 and we search the best - performing β from { 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 } and learning rate from { 0 . 01 , 0 . 05 , 0 . 1 } . • For all adaptive learning rate optimizers , hyperparameters β 1 , β 2 and (cid:15) are set to β 1 = 0 . 9 , β 2 = 0 . 999 and (cid:15) = 1e - 9 respectively . For Adam , RAdam and AdaBelief optimizer , the learning rate is searched from { 0 . 1 , 0 . 01 , 0 . 001 } . For Ranger , η and k are set to η = 0 . 5 and k = 6 according to ( Wright , 2019 ) . The learning rate is searched from { 0 . 1 , 0 . 01 , 0 . 001 } . And for our A DMETA R , the setting of k is the same as Ranger , and we search λ from { 0 . 05 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 } and learning rate from { 0 . 1 , 0 . 05 , 0 . 01 } . The resulting hyperparameters reported in the paper are shown in Table 7 , where LR is the abbreviation of learning rate . 37 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Model Optim MNLI QQP QNLI SST - 2 CoLA STS - B MRPC RTE LR λ LR λ LR λ LR λ LR λ LR λ LR λ LR λ BERT base AdamW 2e - 5 − 3e - 5 − 3e - 5 − 2e - 5 − 5e - 5 − 5e - 5 − 4e - 5 − 6e - 5 − RAdam 2e - 5 − 2e - 5 − 6e - 5 − 4e - 5 − 1e - 4 − 4e - 4 − 1 . 5e - 4 − 5e - 4 − Ranger 5e - 5 − 5e - 5 − 1e - 4 − 8e - 5 − 2e - 4 − 5e - 4 − 4e - 4 − 1e - 3 − AdaBelief 5e - 4 − 5e - 4 − 5e - 4 − 8e - 4 − 4e - 4 − 6e - 4 − 5e - 4 − 6e - 4 − A DMETA R 1 . 5e - 4 0 . 08 1e - 4 0 . 36 2e - 4 0 . 03 1e - 4 0 . 03 7e - 4 0 . 02 1e - 3 0 . 08 1 . 2e - 3 0 . 3 1 . 8e - 3 0 . 36 BERT large AdamW 2e - 5 − 2e - 5 − 2e - 5 − 2e - 5 − 6e - 5 − 5e - 5 − 4e - 5 − 2e - 5 − RAdam 2e - 5 − 2e - 5 − 5e - 5 − 4e - 5 − 1e - 4 − 2e - 4 − 8e - 5 − 5e - 4 − Ranger 5e - 5 − 5e - 5 − 5e - 5 − 6e - 5 − 6e - 5 − 5e - 4 − 5e - 4 − 5e - 4 − AdaBelief 2e - 4 − 4e - 4 − 5e - 4 − 2e - 4 − 6e - 4 − 2e - 4 − 4e - 4 − 8e - 4 − A DMETA R 1 . 5e - 4 0 . 08 8e - 5 0 . 2 8e - 5 0 . 03 9e - 5 0 . 3 7e - 4 0 . 02 1e - 3 0 . 03 6e - 4 0 . 08 8e - 4 0 . 1 Table 8 . Optimizer hyperparameter settings on the GLUE benchmark . Model Optim SQuAD v1 . 1 SQuAD v2 . 0 NER - CoNLL03 LR λ LR λ LR λ BERT base AdamW 5e - 5 − 5e - 5 − 6e - 5 − RAdam 1e - 4 − 5e - 5 − 5e - 5 − Ranger 1e - 4 − 8e - 5 − 1e - 4 − AdaBelief 1e - 3 − 8e - 4 − 5e - 4 − A DMETA R 4e - 4 0 . 05 3e - 4 0 . 2 2e - 4 0 . 3 BERT large AdamW 2e - 5 − 5e - 5 − 2e - 5 − RAdam 6e - 5 − 5e - 5 − 3e - 5 − Ranger 1e - 4 − 8e - 5 − 5e - 5 − AdaBelief 8e - 4 − 8e - 4 − 4e - 4 − A DMETA R 4e - 4 0 . 05 3e - 4 0 . 2 1 . 5e - 4 0 . 2 Table 9 . Hyperparameter settings of SQuAD v1 . 1 and v2 . 0 development sets . E . 3 . Natural Language Understanding In the NLU experiments , we employ a pre - trained language model BERT ( Devlin et al . , 2018 ; Zhang et al . , 2020 ) as our backbone . There are two model sizes for BERT : BERT base and BERT large , where the base model size has 12 Transformer layers with 768 hidden size , 12 self - attention heads and 110M model parameters and the large model size has 24 Transformer layers with 1024 hidden size , 16 self - attention heads and 340M parameters ( Li et al . , 2022 ) . In natural language understanding , we perform experiments on three modeling types of tasks : text classiﬁcation , machine reading comprehension and token classiﬁcation . The text classiﬁcation uses the GLUE benchmark as the evaluation data set , the machine reading comprehension uses SQuAD v1 . 1 and v2 . 0 , and the token classiﬁcation uses the NER - CoNLL03 named entity recognition data set ( Zhou et al . , 2020b ) . We train the eight tasks in GLUE benchmark for 3 epochs on a single NVIDIA RTX - 3090 GPU , except for MRPC , which is trained for 5 epochs due to its relatively small data size ( Li et al . , 2020b ) . The maximum sequence length is set to 128 and the training batch size is set to 32 . SQuAD v1 . 1 and SQuAD v2 . 0 are trained for 2 epochs with two GPUs . The maximum sequence length is set to 384 and the training batch size per device is set to 12 . And NER - CoNLL03 is trained for 3 epochs on a single GPU . The training batch size per device is set to 8 . Because of the pre - training - ﬁne - tuning paradigm , we only employ the adaptive learning rate optimizer . We set β 1 , β 2 , (cid:15) and weight decay of these optimizers to 0 . 9 , 0 . 999 , 1e - 8 and 0 . 0 respectively . η and k are set to 0 . 5 and 6 in the Ranger optimizer and A DMETA uses the same value of k as Ranger . We perform hyperparameter tuning on the learning rate and λ , and the resulting hyperparameters reported in the paper are shown in Table 8 and 9 . E . 4 . Audio Classiﬁcation Based on Wav2vec ( Schneider et al . , 2019 ) , the Wav2vec 2 . 0 ( Baevski et al . , 2020 ) is a framework for self - supervised learning of speech representations which is composed of 3 modules : feature encoder , contextualized representations and quantization module . In the feature encoder , there are 7 blocks with temporal convolutions that have 512 channels for each block and the relative positional embeddings of the convolutional layer modeling has kernel size of 128 and 16 groups . 38 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optim SUPERB Common Language LR λ LR λ AdamW 3e - 5 − 3e - 4 − AdaBelief 8e - 4 − 2e - 3 − Ranger 3e - 4 − 5e - 4 − RAdam 8e - 5 − 5e - 4 − A DMETA R 5e - 4 0 . 05 2e - 3 0 . 2 Table 10 . Hyperparameter settings of SUPERB and Common Language . Among the conﬁgurations of Wav2vec 2 . 0 , we choose Wav2vec 2 . 0 base model , which has 12 Transformer blocks , 95M parameters and 8 attention heads , with model dimension of 768 and inner dimension ( FFN ) of 3072 . We ﬁnetune Wav2vec 2 . 0 base for keyword spotting and language identiﬁcation on SUPERB dataset ( Yang et al . , 2021 ) and Common Language ( Sinisetty et al . , 2021 ) dataset respectively . The dataset size of keyword spotting is smaller than that of language identiﬁcation , so we use a single NVIDIA RTX - 3090 GPU for training on the SUPERB dataset , and use four GPUs for parallel training on the Common Language dataset . The keyword spotting model is trained for 5 epochs with training batch size 32 and language identiﬁcation model for 10 epochs with training batch size 8 per device . Due to the same reason as in NLU experiments , i . e . the pre - training - ﬁne - tuning paradigm , we only employ adaptive learning rate optimizers here . For all optimizers chosen , we ﬁx β 1 = 0 . 9 , β 2 = 0 . 999 , (cid:15) = 1 e − 8 and set weight decay to 0 . 0 . The learning rate is searched from { 5e - 5 , 8e - 5 , 1e - 4 , 3e - 4 , 5e - 4 , 8e - 4 } , and for A DMETA R , λ is searched from { 0 . 05 , 0 . 1 0 . 2 } . The resulting hyperparameters reported in the paper are shown in Table 10 . F . Future Work In the future work , for backward - looking part , though DEMA provides a more ﬂexible way to deal with past gradients , it is still unable to intelligently judge the value of certain historical gradient information , such as discarding some obviously unreasonable gradients caused by noise . A better optimizer may have the ability to forget these wrong information and take advantage of what works , just working like human brains . For forward - looking part , our method takes the constant coefﬁcient into a dynamic one . It is kind of like milestone scheme of learning rate decay strategies to some extent . However , several experiments ( Huang et al . , 2017 ; Ma , 2020 ) have shown that cosine strategy ( Loshchilov & Hutter , 2016 ) works better . Therefore , we will follow the cosine scheme and propose a new forward - looking strategy that may work even better . G . Performance of SGDM and AdmetaS on Finetune Setting In this section , we test the performance of SGDM and A DMETA S on ﬁntune setting and the results are shown in Table 11 . For keyword spotting ( SUPERB ) ( Yang et al . , 2021 ) task , we train the models for 5 epochs and use Wav2vec base ( Schneider et al . , 2019 ) as the baseline model . And for CIFAR - 10 ( Krizhevsky et al . , 2009 ) task , we train the model for 40 epochs from the checkpoint already trained with Adam using learning rate of 0 . 001 for 160 epochs . The baseline model of CIFAR - 10 is ResNet - 110 ( He et al . , 2016 ) with deep CNN architecture . We report the results of best hyperparameter settings for SGD and A DMETA S via grid searching . From Table 11 , we notice that in SUPERB task , compared to adaptive learning rate methods , SGDM achieves worse results in SUPERB task , but not by much , which shows that SGDM can also be used in ﬁnetune setting . While A DMETA S can achieve better result than any other learning rate methods used in our experiment , demonstrating the advantage of our approach . This phenomenon contradicts the mainstream view that SGD family is not suitable for ﬁnetune task . While for CIFAR - 10 task , SGDM and A DMETA S both improve the performance compared to the start point . However , they are both obviously worse than the performance of training the task from scratch using SGDM and A DMETA S respectively , which shows that pre - training is a very strong approach that makes the model achieve a good state . The reason why A DMETA S performs better than SGDM in ﬁnetune setting may lie in two aspects . On the one hand , DEMA scheme in the backward - looking part reduces the overshoot problem that may do harm especially near convergence . On the other hand , the forward - looking part improves the stability of the training process . 39 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optimizer SUPERB CIFAR - 10 SGDM 98 . 25 91 . 71 A DMETA S 98 . 54 91 . 87 Table 11 . Performance of SGDM and A DMETA S on ﬁnetune setting . Optim CIFAR - 10 CIFAR - 100 A DMETA S 94 . 12 73 . 74 SGDM 93 . 68 72 . 07 SGDM ( lr = 0 . 5 ) 93 . 65 73 . 48 Table 12 . Performance of SGD family optimizers in CIFAR task . H . Inﬂuence of Different Learning Rates in SGD Family Optimizers Since the learning rate of 0 . 5 for SGDM is a recommended value in ( Han et al . , 2017 ) but not in ( He et al . , 2016 ) , to alleviate the inﬂuence of different learning rates , we also try the performance of SGDM with a learning rate of 0 . 5 in the ResNet - 110 network and the results are listed in Table 12 . The results show that choosing a large learning rate for SGDM may increase the performance , as shown that when setting the learning rate to 0 . 5 instead of 0 . 1 , the recommended value in ResNet - 110 . However , this is not always true since the performance on CIFAR - 10 when using the learning rate of 0 . 5 does not get prompted . 40