Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Yineng Chen ‚Ä† 1 Zuchao Li ‚Ä† 1 2 Lefei Zhang 1 2 Bo Du 1 2 Hai Zhao 3 Abstract Optimizer is an essential component for the suc - cess of deep learning , which guides the neural network to update the parameters according to the loss on the training set . SGD and Adam are two classical and effective optimizers on which researchers have proposed many variants , such as SGDM and RAdam . In this paper , we in - novatively combine the backward - looking and forward - looking aspects of the optimizer algo - rithm and propose a novel A DMETA ( A D ouble exponential M oving averag E T o A daptive and non - adaptive momentum ) optimizer framework . For backward - looking part , we propose a DEMA variant scheme , which is motivated by a metric in the stock market , to replace the common ex - ponential moving average scheme . While in the forward - looking part , we present a dynamic looka - head strategy which asymptotically approaches a set value , maintaining its speed at early stage and high convergence performance at Ô¨Ånal stage . Based on this idea , we provide two optimizer im - plementations , A DMETA R and A DMETA S , the former based on RAdam and the latter based on SGDM . Through extensive experiments on di - verse tasks , we Ô¨Ånd that the proposed A DMETA optimizer outperforms our base optimizers and shows advantages over recently proposed com - ‚Ä† Equal contribution . This work was supported by the Fundamental Research Funds for the Central Universities ( No . 2042023kf1033 ) , the Special Fund of Hubei Luojia Laboratory under Grant 220100014 , and the National Science Fund for Dis - tinguished Young Scholars under Grant 62225113 . Hai Zhao was funded by the Key Projects of National Natural Science Founda - tion of China ( U1836222 and 61733011 ) . 1 National Engineering Research Center for Multimedia Software , School of Computer Science , Wuhan University , Wuhan , 430072 , P . R . China 2 Hubei Luojia Laboratory , Wuhan 430072 , P . R . China 3 Department of Computer Science and Engineering , Shanghai Jiao Tong Univer - sity , Shanghai , 200240 , P . R . China . Correspondence to : Zuchao Li < zcli - charlie @ whu . edu . cn > . Proceedings of the 40 th International Conference on Machine Learning , Honolulu , Hawaii , USA . PMLR 202 , 2023 . Copyright 2023 by the author ( s ) . petitive optimizers . We also provide theoretical proof of these two algorithms , which veriÔ¨Åes the convergence of our proposed A DMETA . 1 . Introduction The Ô¨Åeld of training neural network is dominated by gradi - ent decent optimizers for a long time , which use Ô¨Årst order method . Typical ones include SGD ( Robbins & Monro , 1951 ) and SGD with momentum ( SGDM ) ( Sutskever et al . , 2013 ) , which are simple yet efÔ¨Åcient algorithms and enjoy even better resulting convergence than many recently pro - posed optimizers . However , it suffers the disadvantage of low speed in initial stage and poor performance in sparse training datasets . This shortcoming can not be ignored since with the development of deep learning , the amount of data becomes much larger , and the model becomes much more complex . The time to train a network is also considered an important metric when evaluating an optimizer . To address this issue , optimizers with adaptive learning rate have been proposed which use nonuniform stepsizes to scale the gradi - ent while training , and the usual implementation is scaling the gradient by square roots of some kind of combination of the squared values of historical gradients . By far the most used are Adam ( Kingma & Ba , 2014 ) and AdamW ( Loshchilov & Hutter , 2017 ) due to their simplicity and high training speed in early stage . Despite their popularity , Adam and many variants like of it ( such as RAdam ( Liu et al . , 2019 ) ) is likely to achieve worse generalization ability than non - adaptive optimizers , observing that their perfor - mance quickly plateaus on validation sets . To achieve a better tradeoff , researchers have made many improvements based on SGD and Adam family optimiz - ers . One attempt is switching from adaptive learning rate methods to SGD , based on the idea of complementing each other‚Äôs advantages . However , a sudden change from one optimizer to another in a set epoch or step is not ap - plicable because different algorithms make characteristic choices at saddle points and tend to converge to Ô¨Ånal points whose loss functions nearby have different geometry ( Im et al . , 2016 ) . Therefore , many optimizers based on this idea seek for a smooth switch . The representative ones 1 a r X i v : 2307 . 00631v1 [ c s . L G ] 2 J u l 2023 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers are AdaBound ( Luo et al . , 2019 ) and SWATS ( Keskar & Socher , 2017 ) . The second attempt is proposing new method to further accelerate SGDM , including introducing power exponent ( pbSGD ( Zhou et al . , 2020a ) ) , aggregated mo - mentum ( AggMo ( Lucas et al . , 2018 ) ) and warm restarts ( SGDR ( Loshchilov & Hutter , 2016 ) ) . The third attempt is modifying the process of optimizers with adaptive learning rate to achieve better local optimum , which is the most pop - ular Ô¨Åeld in recent researches ( Zhuang et al . , 2020 ; Li et al . , 2020a ) . Due to space constraints , please see more related work in Appendix A . We focus in this paper on the use of historical and future information about the optimization process of the model , both of which we argue are important for models to reach their optimal points . To this end , we introduce a bidirec - tional view , backward - looking and forward - looking . In the backward - looking view , EMA is an exponentially decreas - ing weighted moving average , which is used as a trend - type indicator in terms of the optimization process . And since the training uses a mini - batch strategy , each batch is likely to have deviations from the whole , so it may mislead the model to the local optimal point . Inspired by stock market indicators , DEMA ( Mulloy , 1994 ) is an exponential average calculated on the traditional EMA and current input , which can effectively maintain the trend while reducing the impact caused by short - term bias . We thus replace the traditional exponential moving average ( EMA ) with double exponen - tial moving average ( DEMA ) . It is worth noting that our usage is not equivalent to the original DEMA , but rather a variant of it . In the forward - looking part , since we observe that a constant weight adopted by the original Lookahead optimizer ( Zhang et al . , 2019 ) to control the scale of fast weights and slow weights in each synchronization period makes the early stage training slow and lossy , we propose a new dynamic strategy which adopts an asymptotic weight for improvement . By applying these two ideas , we pro - pose A DMETA optimizer with A DMETA R and A DMETA S implementations based on RAdam and SGDM respectively . Extensive experiments have been conducted on computer vision ( CV ) , natural language processing ( NLP ) and au - dio processing tasks , which demonstrate that our method achieves better convergence results compared to other re - cently proposed optimizers . Further analysis shows that A DMETA S achieves higher performance than SGDM and A DMETA R achieves better convergence results and main - tains high speed in the initial stage compared to other adaptive learning rate methods . We further Ô¨Ånd that the DEMA and dynamic looking strategy can improve perfor - mance compared to EMA and constant strategy , respec - tively . In addition , we provide convergence proof of our proposed A DMETA in convex and non - convex optimiza - tions . The code is available at https : / / github . com / Chernyn / Admeta - Optimizer . 2 . Admeta 2 . 1 . Background The role of the optimizer in model training is to minimize the loss on the training set and thus drive the learning of model parameters . Formally , consider a loss function f : R d ‚Üí R that is bounded below greater than zero , where R represents the Ô¨Åeld of real numbers , d denotes the dimension of the parameter and thus R d denotes d - dimensional Euclidean space . The optimization problem can be formulated as : min Œ∏ ‚ààF f ( Œ∏ ) , where Œ∏ indicates a parameter whose domain is F and F ‚äÇ R d . If we deÔ¨Åne the optimum parameter of the above loss function as Œ∏ ‚àó , then the optimization objective can be written as : Œ∏ ‚àó = arg min Œ∏ ‚ààF f ( Œ∏ ) . ( 1 ) Optimizers iteratively update parameters to make them close to the optimum as training step t increases , that is to make : lim t ‚Üí‚àû (cid:107) Œ∏ t ‚àí Œ∏ ‚àó (cid:107) = 0 . The stochastic gradient algorithm SGD ( Robbins & Monro , 1951 ) optimizes f by iteratively updating parameter Œ∏ t at step t in the opposite direction of the stochastic gradient g ( Œ∏ t ‚àí 1 ; Œæ t ) where Œæ t is the input variables of the t - th mini - batch in training datasets . For the sake of clarity , we ab - breviate g ( Œ∏ t ‚àí 1 ; Œæ t ) as g t for the rest of the paper unless speciÔ¨Åed . SGD optimization aims to calculate the updated model parameters based on the previous model parameters , the current gradient and the learning rate . DeÔ¨Åne the learn - ing rate as Œ± t , the update process is summarized as follows : Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t g t . ( 2 ) Original SGD tends to vibrate along the process due to the mini - batch strategy and not using of past gradients . What‚Äôs more , this disadvantage also results in its long - time plateaus in valleys and saddle points , thus slowing the speed . To smooth the oscillation and speed up convergence rate , mo - mentum , also known as Polyak‚Äôs Heavy Ball ( Polyak , 1964 ) , is introduced to modify SGD . Momentum at step t is often denoted as m t and obtained by iterative calculation with a dampening coefÔ¨Åcient Œ≤ . Thus , the update process of SGD with momentum ( SGDM ) ( Sutskever et al . , 2013 ) becomes as follows : m t = Œ≤m t ‚àí 1 + ( 1 ‚àí Œ≤ ) g t , ( 3 ) Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t m t , ( 4 ) Although momentum works well , the uniform stepsize on every parameter is also another factor to limit the speed , especially in large datasets and sparse datasets . To further accelerate the update , adaptive learning rate optimizer is introduced which adopts an individual stepsize for each parameter based on their unique update process . Since a 2 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers smoothing mechanism is employed in the calculation of stepsize , two dampening coefÔ¨Åcients , Œ≤ 1 and Œ≤ 2 , are intro - duced for balancing the current and historical information . Adam ( Kingma & Ba , 2014 ) , a typical adaptive learning rate optimizer , is implemented as follows : m t = Œ≤ 1 m t ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) g t , ( 5 ) v t = Œ≤ 2 v t ‚àí 1 + ( 1 ‚àí Œ≤ 2 ) g 2 t , ( 6 ) Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t m t / ‚àö v t , ( 7 ) where m t indicates the Ô¨Årst momentum , corresponding to the momentum in SGDM ; v t indicates the second momen - tum . To emphasize the functionality of v t , we call it adaptive item for the rest of the paper . Adam may sometimes converge to bad local optimum , partly due to its large variance in the early stage . To Ô¨Åx this issue , RAdam ( Liu et al . , 2019 ) introduces a further rectiÔ¨Åed item r t and splits the update process into two sub - processes sequentially connected : œÅ ‚àû = 2 / ( 1 ‚àí Œ≤ 2 ) ‚àí 1 , ( 8 ) œÅ t = œÅ ‚àû ‚àí 2 tŒ≤ t 2 / ( 1 ‚àí Œ≤ t 2 ) , ( 9 ) r t ‚Üê (cid:115) ( œÅ t ‚àí 4 ) ( œÅ t ‚àí 2 ) œÅ ‚àû ( œÅ ‚àû ‚àí 4 ) ( œÅ ‚àû ‚àí 2 ) œÅ t , ( 10 ) Œ∏ t = (cid:26) Œ∏ t ‚àí 1 ‚àí Œ± t m t , œÅ t ‚â§ 4 Œ∏ t ‚àí 1 ‚àí Œ± t r t m t / ‚àö v t , œÅ t > 4 . ( 11 ) 2 . 2 . Backward - looking In fact , the calculation of momentum m t in Eq . ( 3 ) and Eq . ( 5 ) is an exponential moving average ( EMA ) on gradient g t . EMA , also known as exponential weighted moving average , can be used to estimate the local mean value of variables , so that the update of variables is related to historical values over a period of time . Formally , EMA is expressed as : S t = Œ≤S t ‚àí 1 + ( 1 ‚àí Œ≤ ) p t , ( 12 ) where the variable S is denoted as S t at time t and p t are the newly assigned values . Particularly , S t = p t without using EMA . In Eq . ( 3 ) , SGDM employs EMA to take a moving average of the past gradients . While in Eq . ( 5 ) , Adam and RAdam further apply EMA on the square of past gradients to construct the adaptive item . In the EMA , the moving average of the variable S at time t is roughly equal to the average of the values p over the past 1 / ( 1 ‚àí Œ≤ ) steps . This makes the moving average vary more at the beginning , so a bias correction is proposed and used in Adam ( Eq . ( 7 ) ) and in RAdam ( Eq . ( 11 ) ) when œÅ > 4 . EMA can be regarded as obtaining the average values of the variables over time . Compared with the direct assignment of values to variables , the change curve of the values obtained by moving average is smoother and less jittery , and the moving average does not Ô¨Çuctuate greatly when inputting outliers , which is very important for the optimization using sampled mini - batch . Although efÔ¨Åcient , EMA is not nec - essarily the best strategy for using historical information when it comes to the backward - looking part . Although it can effectively suppress the vibration caused by mini - batch training by performing the moving average on g t , it also brings a lag time that affects the convergence speed and increases with the length of the moving average . What‚Äôs more , it can result in the overshoot problem ( An et al . , 2018 ) , one possible reason is that EMA might make the wrong use of historical gradients in the Ô¨Ånal stage and thus have a ‚Äúburden‚Äù to converge to optimum . Double Exponential Moving Average ( DEMA ) , Ô¨Årst pro - posed by ( Mulloy , 1994 ) , is a faster moving average strategy and was invented to reduce the lag time of EMA . Thus , mo - tivated by the advantage of DEMA , we developed a DEMA variant for the model optimization . It is worth noting DEMA is not simply taking a moving average of historical gradients twice , instead , it takes the moving average of the linear com - bination of the current gradient the moving average of past gradients . The form of our DEMA variant can be written as : DEMA = EMA out ( ¬µ EMA in + Œ∫g t ) , ( 13 ) where ¬µ and Œ∫ are coefÔ¨Åcients that control the scale of current gradient and only depend on Œ≤ . From the formula EMA = Œ£ ni = 1 Œ≤ n ‚àí i g i , past gradients follow a Ô¨Åxed proportionality , that is , the ratio of gradient weight at one time to gradient weight at the previous time is Œ≤ . Due to the use of minibatch training strategy , the input is randomly sampled . The effect of each minibatch towards optimization is varied . Therefore , applying a Ô¨Åxed propor - tionality to past gradients is not a reasonable approach since it does not take into account the changeable situation . The disadvantage of overshoot that EMA usually has may also be caused by the above reasons ( An et al . , 2018 ) . Thus , we deal with the relationship between the historical gradients and the current gradient more Ô¨Çexibly by further controlling the proportion of past gradients . Our design of coefÔ¨Åcients in DEMA is also for this purpose . Based on Eq . ( 13 ) , our actual implementation on algorithm is : I t = ŒªI t ‚àí 1 + g t , ( 14 ) h t = Œ∫g t + ¬µI t + ŒΩ , ( 15 ) m t = Œ≤m t ‚àí 1 + ( 1 ‚àí Œ≤ ) h t , ( 16 ) where I t is the output of EMA in with a 0 initial value and m t is the output of EMA out also initiated with 0 . Œª and Œ≤ are dampening coefÔ¨Åcients of inner EMA and outer EMA respectively , ŒΩ is a bias item , which is set to a small amount 3 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers that decreases exponentially to 0 and chosen as Œª t g 1 . The bias item does not affect the convergence proof , so for the sake of brevity , it is omitted for the rest of this paper and the details can be seen in the code . Please refer to Appendix B for more comparison and discussion between EMA and DEMA . 2 . 3 . Forward - looking Focusing on gradient history , that is , backward - looking , the optimizer is conducive to alleviating the vibration problem in the optimization process and preventing it from being misled by local noise information . However , since the opti - mization problem of the deep neural network is very com - plex , the optimizer can make the optimization process more robust by pre - exploration , so as to obtain better optimization results , which is called forward - looking . Based on Reptile algorithm and advances in understanding the loss surface , ( Zhang et al . , 2019 ) proposed Lookahead optimizer , which introduces two update processes and aver - ages fast and slow weights periodically . The algorithm can be expressed as the cycle of the following process : Pre - exploration : Œ∏ t = O PTIM ( Œ∏ t ‚àí 1 ) Synchronization : ( every k steps ) œÜ t = œÜ t ‚àí k + Œ∑ ( Œ∏ t ‚àí 1 ‚àí œÜ t ‚àí k ) Œ∏ t = œÜ t where O PTIM ( ¬∑ ) denotes a chosen optimizer , k denotes the synchronization period , or in other words , the period of forward - looking , œÜ t denotes the slow weights , Œ∏ t denotes the fast weight updated with a chosen optimizer , and Œ∑ is a constant coefÔ¨Åcient controlling the proportion of slow weights and fast weights in each synchronization . Generally , the chosen optimizer can be arbitrary . We can get an intuitive explanation of Lookahead optimizer from the pseudo code above : Guided by fast weight Œ∏ t , the slow weight œÜ t updates by taking linear interpolation between itself and the fast weight . Every time the fast weight updates k steps , the slow weight updates 1 step . The update direction of slow weight can be regarded as Œ∏ t ‚àí œÜ t from the equation . Therefore , Œ∑ can also be interpreted as the stepsize of slow weight in each synchronization . In order not to be confused with the stepsize of fast weight , we rename the stepsize of slow weight as stepsize s . The recommended value of Œ∑ in ( Zhang et al . , 2019 ) is 0 . 5 and 0 . 8 . In the original Lookahead optimizer implementation , the fast and slow optimization processes were synchronized according to a given period , and parameters are fused at a Ô¨Åxed ratio during synchronization . However , optimization is a continuous process . In different optimization stages , fast optimization steps have different guiding effects on parameters . We argue that using Ô¨Åxed stepsize s in each synchronization is not an optimal strategy , and may even lead to negative effects . For this consideration , we turn the constant Œ∑ into a Œ∑ t that changes over step monotonously and asymptotically . Generally , Œ∑ t is a function that starts from 1 and converges to a set value and depends only on the step t . In this setting , the proportion of slow weights increases and this part gradually turns into the original Lookahead method . In other words , the slow weights in our method adopt a faster stepsize s at the beginning , and it asymptotically slows down as processing . SpeciÔ¨Åcally , we deÔ¨Åne two asymptotic functions for Œ∑ t : Œ∑ t = 0 . 5 ‚àó (cid:18) 1 + 1 0 . 01 ‚àö t + 1 (cid:19) , ( 17 ) Œ∑ t = 0 . 8 ‚àó (cid:18) 1 + 1 0 . 1 ‚àö t + 3 . 8 (cid:19) , ( 18 ) thus we call this as dynamic asymptotic lookahead . The two functions are designed to turn Œ∑ t from 1 to 0 . 5 and 0 . 8 respectively . Notably , these asymptotic functions may not be the best . We just Ô¨Ånd that it works well and maybe future work can be done to investigate a more suitable one . For the sake of clarity , we will use the latter one in the rest of the paper and the results of experiments trained from scratch are based on this function unless speciÔ¨Åed . To illustrate the advantages of our dynamic lookahead strat - egy over no lookahead and the original constant lookahead , we give an optimization example in Figure 1 . In region 1 , which is around the early stage , the direction of the update is relatively stable and a large stepsize s is needed . Œ∏ 1 ‚Üí Œ∏ 4 denotes the update of fast weights . A constant lookahead method will slow the update process in each synchroniza - tion period , as can be seen in Œ∏ 1 ‚Üí Œ∏ 2 . In our method , fast weights share more proportion in each synchronization period in early stage , thus updating faster , as can be seen in Œ∏ 1 ‚Üí Œ∏ 3 . In region 2 , which is around the Ô¨Ånal stage , the direction of the update is relatively oscillated , and a small stepsize s is needed . Fast weights tend to overshoot the optimum , as can be seen in Œ∏ 5 ‚Üí Œ∏ 8 . Lookahead optimizer can achieve bet - ter convergence result than general algorithm as it averages the weights to make them more close to the optimum point , as can be seen in Œ∏ 5 ‚Üí Œ∏ 6 . In our method , the proportion of fast weights has already been reduced asymptotically to a set value , thus can achieve similar efÔ¨Åcacy as Lookahead optimizer as can be seen in Œ∏ 5 ‚Üí Œ∏ 7 . From these analyses , we demonstrate that our dynamic lookahead strategy method improves the robustness of train - ing . 4 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers ‚ë† ‚ë° Œ∏ 1 x f ( x ) Œ∏ 2 Œ∏ 3 Œ∏ 4 Œ∏ 5 Œ∏ 6 Œ∏ 7 Œ∏ * Œ∏ 8 SGD SGD with Lookahead SGD with dynamic Lookahead ( Ours ) | Œ∏ 1 - Œ∏ * | is large , a large Œ∑ scan maintain the convergence speed | Œ∏ 5 - Œ∏ * | is small , a relative small Œ∑ can achieve better convergence Figure 1 . Comparison between no lookahead , constant lookahead and dynamic lookahead . 2 . 4 . Implementations of AdmetaR and AdmetaS Since optimizers of the Adam family and SGD family have their own advantages and disadvantages , and the bidirec - tional looking optimizer framework and improvement we propose do not have too many restrictions on the basic opti - mizer , we have implemented improved versions A DMETA R and A DMETA S based on RAdam and SGDM optimizer . The Ô¨Ånal algorithm forms are shown in Algorithm 1 and 2 . De - tailed proof of convergence and convergence rate for our A DMETA R and A DMETA S is put in Appendix C and D . Notations : ‚Ä¢ Œ± t : learning rate at step t ‚Ä¢ Œª , Œ≤ , Œ≤ 1 , Œ≤ 2 : the momentum coefÔ¨Åcients ‚Ä¢ (cid:15) : a small value used to avoid a zero denominator ‚Ä¢ k : synchronization period ‚Ä¢ (cid:81) F , M ( y ) = argmin x ‚ààF | | M 1 / 2 ( x ‚àí y ) | | ‚Ä¢ ¬µ = 25 ‚àí 10 (cid:0) Œª + 1 Œª (cid:1) , Œ∫ = 10 Œª ‚àí 9 3 . Experiments In this section , we demonstrate the effectiveness of our optimizer by turning to an empirical exploration of different datasets and different models to compare some popular optimizers . SpeciÔ¨Åcally , we conduct experiments on typical CV , NLP , and audio processing tasks . InÔ¨Çuenced by the Transformer structure , models are becoming deeper and Algorithm 1 A DMETA R Optimizer . All operations are element - wise . Initialize Œ∏ 1 ‚àà F , œÜ 0 ‚Üê 0 , m 0 ‚Üê 0 , v 0 ‚Üê 0 , I 0 ‚Üê 0 , t ‚Üê 0 for t = 1 , 2 , . . . do t ‚Üê t + 1 g t ‚Üê ‚àá t f t ( Œ∏ t ) I t ‚Üê ŒªI t ‚àí 1 + g t h t ‚Üê Œ∫g t + ¬µI t m t ‚Üê Œ≤ 1 m t ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) h t œÅ t ‚Üê œÅ ‚àû ‚àí 2 t Œ≤ t 2 1 ‚àí Œ≤ t 2 if the variance is tractable , i . e . , œÅ t > 4 , then v t ‚Üê Œ≤ 2 v t ‚àí 1 + ( 1 ‚àí Œ≤ 2 ) h 2 t r t ‚Üê (cid:113) ( œÅ t ‚àí 4 ) ( œÅ t ‚àí 2 ) œÅ ‚àû ( œÅ ‚àû ‚àí 4 ) ( œÅ ‚àû ‚àí 2 ) œÅ t (cid:99) m t ‚Üê m t 1 ‚àí Œ≤ t 1 , (cid:98) v t ‚Üê v t 1 ‚àí Œ≤ t 2 Œ∏ t + 1 ‚Üê Œ† F , ‚àö (cid:98) v t ( Œ∏ t ‚àí Œ± t r t (cid:99) m t ‚àö (cid:98) v t + (cid:15) ) else Œ∏ t + 1 ‚Üê Œ† F , ‚àö (cid:98) v t ( Œ∏ t ‚àí Œ± t (cid:99) m t ) if t + 1 % k = = 0 : œÜ t ‚Üê Œ∑ t Œ∏ t + ( 1 ‚àí Œ∑ t ) œÜ t ‚àí k Œ∏ t ‚Üê œÜ t end for return x larger , and therefore training is becoming more difÔ¨Åcult . The current paradigm of pre - training - Ô¨Åne - tuning is mainly used for large models . Therefore , we compare optimizers not only in the training - from - scratch setup , but also in the Ô¨Åne - tuning setup . In this section , we compare our proposed optimizer with several typical optimizers , including classic SGD ( Robbins & Monro , 1951 ) and Adam ( Kingma & Ba , 2014 ) , our base , SGDM ( Sutskever et al . , 2013 ) 1 and RAdam ( Liu et al . , 2019 ) , the current state - of - the - art AdaBelief ( Zhuang et al . , 2020 ) , and the optimizer combined of many modules , Ranger ( Wright , 2019 ) . Since we should compare these optimizers under the same condition , the model used may be different from the original paper of them , which may lead to different convergence results compared to the results reported in the original paper . Please refer to Appendix E for more experimental details . 3 . 1 . Image ClassiÔ¨Åcation Consistent with general optimizer researches ( Zhuang et al . , 2020 ) , we conduct experiments on two image classiÔ¨Åcation tasks , CIFAR - 10 and CIFAR - 100 ( Krizhevsky et al . , 2009 ) in CV Ô¨Åeld , and the results are presented in Table 1 . For 1 Notably , we employed nesternov momentum ( Nesterov , 1983 ) in the SGDM for a stronger comparison baseline . 5 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers               ( S R F K                          7  U  D  L  Q  L  Q J    /  R  V V  & , ) $ 5     6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K                                                           7  H  V  W    $  F F  & , ) $ 5     6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K               7  U  D  L  Q  L  Q J    /  R  V V  & , ) $ 5      6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6               ( S R F K                                                           7  H  V  W    $  F F  & , ) $ 5      6 * ' 6 * ' 0 $ G D P 5 $ G D P 5 D Q J H U $ G D % H O L H I $ G P H W D 5 $ G P H W D 6 Figure 2 . Training loss and test accuracy comparison on CIFAR - 10 and CIFAR - 100 datasets . Algorithm 2 A DMETA S Optimizer . All operations are element - wise . Initialize Œ∏ 1 ‚àà F , œÜ 0 ‚Üê 0 , m 0 ‚Üê 0 , I 0 ‚Üê 0 , t ‚Üê 0 for t = 1 , 2 , . . . do t ‚Üê t + 1 g t ‚Üê ‚àá f t ( Œ∏ t ) I t ‚Üê ŒªI t ‚àí 1 + g t h t ‚Üê Œ∫g t + ¬µI t m t ‚Üê Œ≤m t ‚àí 1 + ( 1 ‚àí Œ≤ ) h t Œ∏ t + 1 ‚Üê Œ∏ t ‚àí Œ± t m t if t + 1 % k = = 0 : œÜ t ‚Üê Œ∑ t Œ∏ t + ( 1 ‚àí Œ∑ t ) œÜ t ‚àí k Œ∏ t ‚Üê œÜ t end for return x Model CIFAR - 10 CIFAR - 100 ResNet - 110 PyramidNet ResNet - 110 PyramidNet Adam 91 . 89 ¬± 0 . 23 94 . 55 ¬± 0 . 24 68 . 45 ¬± 0 . 43 76 . 72 ¬± 0 . 32 RAdam 93 . 09 ¬± 0 . 05 94 . 58 ¬± 0 . 14 70 . 39 ¬± 0 . 08 76 . 02 ¬± 0 . 53 Ranger 92 . 85 ¬± 0 . 34 94 . 76 ¬± 0 . 03 68 . 96 ¬± 0 . 68 76 . 35 ¬± 0 . 08 AdaBelief 92 . 81 ¬± 0 . 26 94 . 70 ¬± 0 . 03 70 . 88 ¬± 0 . 07 76 . 57 ¬± 0 . 04 A DMETA R 93 . 63 ¬± 0 . 22 94 . 81 ¬± 0 . 19 71 . 00 ¬± 0 . 05 76 . 82 ¬± 0 . 07 SGD 90 . 27 ¬± 0 . 15 91 . 52 ¬± 0 . 03 65 . 70 ¬± 0 . 25 76 . 51 ¬± 0 . 06 SGDM 93 . 68 ¬± 0 . 20 95 . 08 ¬± 0 . 13 72 . 07 ¬± 0 . 28 79 . 49 ¬± 0 . 11 A DMETA S 94 . 12 ¬± 0 . 17 95 . 30 ¬± 0 . 08 73 . 74 ¬± 0 . 26 79 . 61 ¬± 0 . 34 Table 1 . Results on CIFAR - 10 and CIFAR - 100 test sets . model baselines , we choose the popular and leading perfor - mance ResNet - 110 ( He et al . , 2016 ) and PyramidNet ( Han et al . , 2017 ) , respectively . From the experimental results , whether in CIFAR - 10 or CIFAR - 100 dataset , and based on the ResNet - 110 or PyramidNet model , SGDM achieves better results than SGD , indicating that backward - looking improves the optimization effect . EMA with rectiÔ¨Åed item in RAdam performs better than EMA in Adam , suggesting that a better backward - looking process can lead to perfor - mance gains . Comparing SGDM and RAdam , we Ô¨Ånd that SGDM has a performance advantage , showing that though Adam uses an adaptive learning rate to improve the speed of convergence , it is lossy for performance . Among optimizers with adaptive learning rate , AdaBelief achieves better results than Adam and RAdam in CIFAR - 10 with PyramidNet and CIFAR - 100 with ResNet - 110 and PyramidNet . Ranger , which combines forward and backward looking , achieves better performance than the backward - looking - only RAdam in CIFAR - 10 and CIFAR - 100 with PyramidNet . Our A DMETA R achieves consistent improvement over the optimizer baseline RAdam , which also conÔ¨Årms the gain of bidirectional looking for optimiza - tion . And A DMETA R has better results than Ranger , indi - cating that our bidirectional looking is better than Ranger‚Äôs simple combination of multiple optimization features . Our A DMETA S also performs better than SGDM , further demon - strating the adaptability of our approach , which not only performs well in Adam family , but also works in SGD fam - ily . Following the previous practice ( Liu et al . , 2019 ) , we visual - ize the optimization process of the ResNet - 110 model with Adam , RAdam , SGDM , and our A DMETA S , A DMETA R optimizers on the CIFAR - 10 and CIFAR - 100 datasets in Figure 2 . As can be seen from the training loss Ô¨Ågure , the above optimizers can successfully train the model to con - verge to a stable state , but A DMETA S obtains the lowest training loss on CIFAR - 10 , while AdaBelief obtains the training loss on CIFAR - 100 . In terms of performance on the test set , A DMETA S has obtained the best convergence result , which shows that the lower the loss of the training set may not necessarily lead to the better performance on the model . In addition , from the accuracy of the test set , the convergence speed of the SGD family including SGDM and A DMETA S is generally slower than that of the Adam family ( Adam , RAdam , Ranger , AdaBelief and A DMETA R ) , but the Ô¨Ånal convergence result of the SGD family is better than the Adam family . However , our A DMETA R achieves more comparable performance to the SGD family , while maintaining the advantage of the fast convergence of the 6 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Model Optim MNLI QQP QNLI SST - 2 CoLA STS - B MRPC RTE Average m / mm ( Acc ) ( F 1 ) ( Acc ) ( Acc ) ( MCC ) ( SCC ) ( F 1 ) ( Acc ) BERT base AdamW 83 . 85 / 84 . 08 87 . 72 90 . 74 93 . 23 60 . 32 89 . 11 90 . 85 67 . 51 82 . 92 Ranger 83 . 80 / 84 . 24 87 . 83 90 . 76 92 . 32 58 . 87 89 . 19 90 . 05 68 . 59 82 . 68 AdaBelief 83 . 91 / 84 . 42 86 . 76 90 . 92 92 . 55 58 . 05 88 . 94 90 . 38 67 . 87 82 . 42 RAdam 83 . 91 / 84 . 24 87 . 66 90 . 88 92 . 20 59 . 31 89 . 07 90 . 91 70 . 04 83 . 00 A DMETA R 83 . 90 / 84 . 53 87 . 91 91 . 14 93 . 35 62 . 07 89 . 62 91 . 47 71 . 48 83 . 87 BERT large AdamW 86 . 05 / 86 . 55 88 . 58 92 . 40 93 . 00 59 . 58 89 . 21 91 . 67 71 . 12 83 . 95 Ranger 86 . 53 / 86 . 58 88 . 58 92 . 39 93 . 46 63 . 81 89 . 73 92 . 04 72 . 56 84 . 89 AdaBelief 85 . 59 / 86 . 25 86 . 99 92 . 42 93 . 00 61 . 11 90 . 17 91 . 28 72 . 92 84 . 19 RAdam 86 . 40 / 86 . 72 88 . 36 92 . 35 93 . 69 62 . 61 89 . 64 91 . 29 71 . 48 84 . 48 A DMETA R 86 . 21 / 86 . 54 88 . 54 92 . 63 93 . 69 64 . 12 89 . 92 92 . 10 73 . 65 85 . 11 Table 2 . Development results on GLUE benchmark . Optim SQuAD v1 . 1 SQuAD v2 . 0 NER - CoNLL03 EM F 1 EM F 1 P R F 1 BERT base : AdamW 80 . 87 88 . 39 72 . 63 75 . 99 94 . 65 95 . 24 94 . 94 Ranger 81 . 30 88 . 58 73 . 32 76 . 73 94 . 47 95 . 17 94 . 82 AdaBelief 80 . 63 88 . 10 72 . 97 76 . 25 93 . 79 94 . 60 94 . 19 RAdam 80 . 68 88 . 19 73 . 21 76 . 49 94 . 61 95 . 42 95 . 01 A DMETA R 81 . 55 88 . 69 73 . 81 77 . 19 94 . 96 95 . 41 95 . 13 BERT large : AdamW 83 . 31 90 . 39 76 . 67 80 . 02 94 . 77 95 . 73 95 . 24 Ranger 84 . 21 90 . 97 77 . 22 80 . 35 95 . 24 95 . 89 95 . 56 AdaBelief 83 . 53 90 . 42 77 . 48 80 . 57 94 . 28 95 . 17 94 . 72 RAdam 84 . 17 90 . 90 77 . 39 80 . 72 94 . 80 95 . 64 95 . 22 A DMETA R 84 . 25 90 . 92 77 . 08 80 . 36 95 . 38 95 . 93 95 . 65 Table 3 . Results on SQuAD v1 . 1 and v2 . 0 development sets and NER - CoNLL03 test sets . Adam family . A DMETA R has the highest results on the test set in the early stage of optimization ( < 80 epoch ) , which demonstrates that bidirectional looking improves both accu - racy and speed , making A DMETA R an efÔ¨Åcient and effective optimizer implementation . Compared to ResNet - 110 , PyramidNet has a more compli - cated structure and can achieve better results in these tasks . In cases where the model is strong enough , the selection of optimizer will not be the main factor for the Ô¨Ånal perfor - mance . As shown in Table 1 , compared to Adam , RAdam and AdaBelief achieve just a bit of improvement on CIFAR - 10 task and even achieve worse results on CIFAR - 100 task , which also veriÔ¨Åes our above claims . It also shows that some recently proposed methods are not always suitable when the structure is complex enough . 3 . 2 . Natural Language Understanding As a general AI component , the general capability for var - ious tasks and various models is a basic requirement for optimizers . We evaluate the adaptability of our A DMETA optimizer on the Ô¨Ånetune training scenario with current popular pre - trained language models . SpeciÔ¨Åcally , we con - Optim SUPERB Common Language Acc Training Acc Training AdamW 98 . 26 10m44s 79 . 45 8h27m33s AdaBelief 98 . 41 11m20s 80 . 29 8h28m25s Ranger 98 . 35 11m50s 81 . 18 8h29m55s RAdam 98 . 37 11m30s 80 . 35 8h28m38s A DMETA R 98 . 50 11m54s 81 . 57 8h30m15s Table 4 . Results on speech keyword spotting and language identiÔ¨Å - cation tasks . duct experiments based on the pre - trained language model BERT ( Devlin et al . , 2018 ) on three natural language un - derstanding tasks , GLUE benchmark ( Wang et al . , 2018 ) , machine reading comprehension ( SQuAD v1 . 1 and v2 . 0 ( Ra - jpurkar et al . , 2016 ) ) and named entity recognition ( NER - CoNLL03 ( Sang & De Meulder , 2003 ) ) . We report results for two model sizes , BERT base and BERT large to explore whether model size has an effect on the optimizer . In Table 2 , we report the results on the development set of 8 datasets of the GLUE benchmark , where Acc , MCC , SCC are abbreviations of accuracy , Matthews Correlation and Spearman Correlation CoefÔ¨Åcient , respectively . First , under the BERT - base model , compared with the basic opti - mizer RAdam , A DMETA R achieves consistent improvement . The most signiÔ¨Åcant improvement is obtained on RTE and CoLA , which indicates that our A DMETA R optimizer ex - hibits greater stability for low - resource optimization . On the other seven datasets , some of them are slightly improved . This is because most of the parameters of the model in the pre - training - Ô¨Åne - tuning paradigm have converged to a cer - tain extent in the pre - training stage , so the further advantage of the optimizer in Ô¨Ånetune is not apparent . And when the model is switched to a larger BERT - large , most tasks re - ceive performance gains , except for CoLA and RTE using AdamW optimizer . Due to the further increase in model pa - rameters , the low - resource dataset is not enough to Ô¨Åne - tune the large model , it will even reduce the model performance . 7 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optim CIFAR - 10 ‚àÜ Optim CIFAR - 10 ‚àÜ Adam 91 . 89 \ SGD 90 . 22 \ RAdam 93 . 09 \ SGDM 93 . 68 \ A DMETA R 93 . 63 A DMETA S 94 . 12 - DEMA 93 . 24 - 0 . 39 - DEMA 89 . 13 - 4 . 99 - LB 92 . 29 - 0 . 95 - LB 89 . 88 - 4 . 24 - LF 93 . 14 - 0 . 10 - LF 93 . 51 - 0 . 61 - LB - LF 92 . 36 - 0 . 88 - LB - LF 89 . 80 - 4 . 32 A DMETA R w / constant LF 93 . 03 - 0 . 60 A DMETA S w / constant LF 93 . 75 - 0 . 37 Table 5 . Ablation study on A DMETA optimizer . But RAdam with rectiÔ¨Åed item , Ranger with bidirectional looking , and our A DMETA R handle the low - resource chal - lenge well , continue to improve performance , and take ad - vantage of large models . Our A DMETA R achieves the best results on these two low - resource datasets , demonstrating the effectiveness of our bidirectional looking approach . In Table 3 , we further report the results of machine reading comprehension and named entity recognition . A DMETA R achieved improvements at both model sizes in SQuAD v1 . 1 dataset , while similar improvements were achieved in SQuAD v2 . 0 with more complex models , illustrating that our optimizer is model - independent . Named entity recog - nition has reached a very accurate level with the help of pre - trained language models , and our A DMETA R optimizer also brings performance improvements over such a strong baseline , showing that optimization is also a bottleneck that restricts further performance improvement in addition to model structure and data . 3 . 3 . Audio ClassiÔ¨Åcation Like images and natural language , speech is one of the mainstream Ô¨Åelds of deep learning research . In speech pro - cessing , there are also a large number of pre - trained large models , such as Wav2vec ( Schneider et al . , 2019 ) . To high - light the input - independent nature of the optimizer , we also conduct experiments on two typical tasks of audio classi - Ô¨Åcation , keyword spotting ( SUPERB ) ( Yang et al . , 2021 ) and language identiÔ¨Åcation ( Common Language ) ( Sinisetty et al . , 2021 ) . We employ Wav2vec 2 . 0 base as the baseline model and report the results of each optimizer in Table 4 . In addition , we also list the training time of each optimizer to evaluate the impact of the bidirectional looking mechanism on the optimizer time overhead 2 . A DMETA R shows better classiÔ¨Åcation accuracy than AdamW , RAdam , Ranger and AdaBelief , which is con - sistent with the experimental conclusions in the image and natural language tasks . Consistent results across image , 2 Notably , the reported training time is only for rough compari - son due to the inÔ¨Çuence of environments . natural language , and speech modalities verify the task - independence of our optimizer . Comparing the training time of A DMETA R with AdamW , RAdam , Ranger , and Ad - aBelief , our A DMETA R has different degrees of increase due to the additional computation and storage in the opti - mization process . Ranger and our A DMETA R increased the time most , but it can still be regarded as slight compared to the overall training time . Therefore , it can be concluded that the bidirectional looking mechanism adopted by A DMETA optimizer will bring additional computational overhead and increase the training time , but compared with the overall training cost , it is very small . A DMETA achieves better per - formance without increasing model parameters and training data , and does not have any impact on the inference time of the model , which achieves a better tradeoff . 4 . Ablation Study We perform an ablation study on various designs of A D - META in bidirectional looking in this section . - DEMA means removing the DEMA mechanism in backward - looking and using the original EMA . - LB means complete removal of backward - looking , - LF means complete removal of forward - looking . - LB - LF means to remove bidirectional looking at the same time . w / constant LF means use the original Lookahead mechanism in the forward - looking . The results are evaluated using the ResNet - 110 model on the test set of CIFAR - 10 . According to the results shown in Table 5 , it can be found that the improvement of SGDM com - pared with SGD initially shows the advantage of backward - looking . And compared with Adam , RAdam reveals that the EMA with the rectiÔ¨Åed item in backward - looking is more suitable for the training of the model than the original EMA . Our A DMETA ( including A DMETA R and A DMETA S ) achieved the best results . After removing DEMA and re - placing dynamic lookahead with constant lookahead , respec - tively , the performance drops , indicating that both DEMA and dynamic asymptotic lookahead play an important role in stable optimization . After further removing the backward - looking , the forward - looking , and the bidirectional looking , the results drop further , validating our argument that bidi - 8 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers rectional looking is beneÔ¨Åcial for optimization . Another observation is that backward - looking and DEMA make a more signiÔ¨Åcant contribution to the performance of SGDM compared to RAdam . This may show that our methods have a better complementarity for SGD - family optimizers . 5 . Conclusion In this paper , we introduce a bidirectional looking opti - mizer framework , exploring the use of historical and future information for optimization . For backward - looking , we introduce a DEMA scheme to replace the traditional EMA strategy , while for forward - looking , we propose a dynamic asymptotic lookahead strategy to replace the constant looka - head scheme . In this way , we propose the A DMETA opti - mizer , and provide two implement versions , A DMETA R and A DMETA S , which are based on adaptive and non - adaptive momentum optimizers , RAdam and SGDM respectively . We verify the beneÔ¨Åts of A DMETA with intuitive examina - tions and various experiments , showing the effectiveness of our proposed optimizer . Please refer to Appendix F for future work discussion . 6 . Limitation Although improving the performance on different tasks , our method introduces additional computational complexity and requires more hyperparameters than some existing ap - proaches . However , the selection range of hyperparameters can be preliminarily determined through the visual tool we proposed ( Figure 3 ) , which can slightly reduce the workload of tuning parameters . References Alacaoglu , A . , Malitsky , Y . , Mertikopoulos , P . , and Cevher , V . A new regret analysis for adam - type algorithms . In International conference on machine learning , pp . 202 ‚Äì 210 . PMLR , 2020 . An , W . , Wang , H . , Sun , Q . , Xu , J . , Dai , Q . , and Zhang , L . A pid controller approach for stochastic optimization of deep networks . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 8522 ‚Äì 8531 , 2018 . Anil , R . , Gupta , V . , Koren , T . , Regan , K . , and Singer , Y . Scalable second order optimization for deep learning . arXiv preprint arXiv : 2002 . 09018 , 2020 . Baevski , A . , Zhou , Y . , Mohamed , A . , and Auli , M . wav2vec 2 . 0 : A framework for self - supervised learning of speech representations . Advances in Neural Information Process - ing Systems , 33 : 12449 ‚Äì 12460 , 2020 . Beckenbach , E . F . and Bellman , R . Inequalities , volume 30 . Springer Science & Business Media , 2012 . Chen , X . , Liu , S . , Sun , R . , and Hong , M . On the conver - gence of a class of adam - type algorithms for non - convex optimization . arXiv preprint arXiv : 1808 . 02941 , 2018 . Devlin , J . , Chang , M . - W . , Lee , K . , and Toutanova , K . Bert : Pre - training of deep bidirectional transformers for lan - guage understanding . arXiv preprint arXiv : 1810 . 04805 , 2018 . Duchi , J . , Hazan , E . , and Singer , Y . Adaptive subgradient methods for online learning and stochastic optimization . Journal of machine learning research , 12 ( 7 ) , 2011 . Fassold , H . Adafamily : A family of adam - like adaptive gra - dient methods . arXiv preprint arXiv : 2203 . 01603 , 2022 . Goh , G . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . Han , D . , Kim , J . , and Kim , J . Deep pyramidal residual networks . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 5927 ‚Äì 5935 , 2017 . He , K . , Zhang , X . , Ren , S . , and Sun , J . Deep residual learn - ing for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 770 ‚Äì 778 , 2016 . Huang , G . , Li , Y . , Pleiss , G . , Liu , Z . , Hopcroft , J . E . , and Weinberger , K . Q . Snapshot ensembles : Train 1 , get m for free . arXiv preprint arXiv : 1704 . 00109 , 2017 . Im , D . J . , Tao , M . , and Branson , K . An empirical analysis of the optimization of deep network loss surfaces . arXiv preprint arXiv : 1612 . 04010 , 2016 . Keskar , N . S . and Socher , R . Improving generalization per - formance by switching from adam to sgd . arXiv preprint arXiv : 1712 . 07628 , 2017 . Kingma , D . P . and Ba , J . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . Krizhevsky , A . , Hinton , G . , et al . Learning multiple layers of features from tiny images . 2009 . Li , W . , Zhang , Z . , Wang , X . , and Luo , P . Adax : Adap - tive gradient descent with exponential long term memory . arXiv preprint arXiv : 2004 . 09740 , 2020a . Li , Z . , Wang , R . , Chen , K . , Utiyama , M . , Sumita , E . , Zhang , Z . , and Zhao , H . Data - dependent gaussian prior objective for language generation . In 8th International Confer - ence on Learning Representations , ICLR 2020 , Addis 9 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Ababa , Ethiopia , April 26 - 30 , 2020 . OpenReview . net , 2020b . URL https : / / openreview . net / forum ? id = S1efxTVYDr . Li , Z . , Zhang , Z . , Zhao , H . , Wang , R . , Chen , K . , Utiyama , M . , and Sumita , E . Text compression - aided transformer encoding . IEEE Trans . Pattern Anal . Mach . Intell . , 44 ( 7 ) : 3840 ‚Äì 3857 , 2022 . doi : 10 . 1109 / TPAMI . 2021 . 3058341 . URL https : / / doi . org / 10 . 1109 / TPAMI . 2021 . 3058341 . Liu , L . , Jiang , H . , He , P . , Chen , W . , Liu , X . , Gao , J . , and Han , J . On the variance of the adaptive learning rate and beyond . arXiv preprint arXiv : 1908 . 03265 , 2019 . Loshchilov , I . and Hutter , F . Sgdr : Stochastic gra - dient descent with warm restarts . arXiv preprint arXiv : 1608 . 03983 , 2016 . Loshchilov , I . and Hutter , F . Decoupled weight decay regu - larization . arXiv preprint arXiv : 1711 . 05101 , 2017 . Lucas , J . , Sun , S . , Zemel , R . , and Grosse , R . Aggregated momentum : Stability through passive damping . arXiv preprint arXiv : 1804 . 00325 , 2018 . Luo , L . , Xiong , Y . , Liu , Y . , and Sun , X . Adaptive gradient methods with dynamic bound of learning rate . arXiv preprint arXiv : 1902 . 09843 , 2019 . Ma , X . Apollo : An adaptive parameter - wise diagonal quasi - newton method for nonconvex stochastic optimization . arXiv preprint arXiv : 2009 . 13586 , 2020 . McMahan , H . B . and Streeter , M . Adaptive bound opti - mization for online convex optimization . arXiv preprint arXiv : 1002 . 4908 , 2010 . Mulloy , P . G . Smoothing data with faster moving averages . Stocks & Commodities , 12 ( 1 ) : 11 ‚Äì 19 , 1994 . Nesterov , Y . E . A method for solving the convex program - ming problem with convergence rate o ( 1 / kÀÜ 2 ) . In Dokl . akad . nauk Sssr , volume 269 , pp . 543 ‚Äì 547 , 1983 . Pascanu , R . and Bengio , Y . Revisiting natural gradient for deep networks . arXiv preprint arXiv : 1301 . 3584 , 2013 . Polyak , B . T . Some methods of speeding up the convergence of iteration methods . Ussr computational mathematics and mathematical physics , 4 ( 5 ) : 1 ‚Äì 17 , 1964 . Popel , M . and Bojar , O . Training tips for the transformer model . arXiv preprint arXiv : 1804 . 00247 , 2018 . Rajpurkar , P . , Zhang , J . , Lopyrev , K . , and Liang , P . Squad : 100 , 000 + questions for machine comprehension of text . arXiv preprint arXiv : 1606 . 05250 , 2016 . Reddi , S . J . , Kale , S . , and Kumar , S . On the convergence of adam and beyond . arXiv preprint arXiv : 1904 . 09237 , 2019 . Robbins , H . and Monro , S . A stochastic approximation method . The annals of mathematical statistics , pp . 400 ‚Äì 407 , 1951 . Sang , E . F . and De Meulder , F . Introduction to the conll - 2003 shared task : Language - independent named entity recognition . arXiv preprint cs / 0306050 , 2003 . Schneider , S . , Baevski , A . , Collobert , R . , and Auli , M . wav2vec : Unsupervised pre - training for speech recog - nition . arXiv preprint arXiv : 1904 . 05862 , 2019 . Sinisetty , G . , Ruban , P . , Dymov , O . , and Ravanelli , M . Com - monlanguage , June 2021 . URL https : / / doi . org / 10 . 5281 / zenodo . 5036977 . Sutskever , I . , Martens , J . , Dahl , G . , and Hinton , G . On the importance of initialization and momentum in deep learn - ing . In International conference on machine learning , pp . 1139 ‚Äì 1147 . PMLR , 2013 . Tieleman , T . and Hinton , G . Lecture 6 . 5 - rmsprop , coursera : Neural networks for machine learning . University of Toronto , Technical Report , 6 , 2012 . Vaswani , A . , Shazeer , N . , Parmar , N . , Uszkoreit , J . , Jones , L . , Gomez , A . N . , Kaiser , ≈Å . , and Polosukhin , I . At - tention is all you need . Advances in neural information processing systems , 30 , 2017 . Wang , A . , Singh , A . , Michael , J . , Hill , F . , Levy , O . , and Bowman , S . R . Glue : A multi - task benchmark and anal - ysis platform for natural language understanding . arXiv preprint arXiv : 1804 . 07461 , 2018 . Wang , J . , Tantia , V . , Ballas , N . , and Rabbat , M . Looka - head converges to stationary points of smooth non - convex functions . In ICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pp . 8604 ‚Äì 8608 . IEEE , 2020 . Wang , Y . , Kang , Y . , Qin , C . , Wang , H . , Xu , Y . , Zhang , Y . , and Fu , Y . Adapting stepsizes by momentumized gra - dients improves optimization and generalization . arXiv preprint arXiv : 2106 . 11514 , 2021 . Weng , B . , Sun , J . , Sadeghi , A . , and Wang , G . Adapid : An adaptive pid optimizer for training deep neural networks . In ICASSP 2022 - 2022 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pp . 3943 ‚Äì 3947 . IEEE , 2022 . Wright , L . Ranger - a synergistic optimizer . https : / / github . com / lessw2020 / Ranger - Deep - Learning - Optimizer , 2019 . 10 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Yang , S . - w . , Chi , P . - H . , Chuang , Y . - S . , Lai , C . - I . J . , Lakho - tia , K . , Lin , Y . Y . , Liu , A . T . , Shi , J . , Chang , X . , Lin , G . - T . , et al . Superb : Speech processing universal performance benchmark . arXiv preprint arXiv : 2105 . 01051 , 2021 . Yao , Z . , Gholami , A . , Shen , S . , Mustafa , M . , Keutzer , K . , and Mahoney , M . Adahessian : An adaptive second order optimizer for machine learning . In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 35 , pp . 10665 ‚Äì 10673 , 2021 . Zeiler , M . D . Adadelta : an adaptive learning rate method . arXiv preprint arXiv : 1212 . 5701 , 2012 . Zhang , M . , Lucas , J . , Ba , J . , and Hinton , G . E . Lookahead optimizer : k steps forward , 1 step back . Advances in neural information processing systems , 32 , 2019 . Zhang , Z . , Wu , Y . , Zhao , H . , Li , Z . , Zhang , S . , Zhou , X . , and Zhou , X . Semantics - aware BERT for language un - derstanding . In The Thirty - Fourth AAAI Conference on ArtiÔ¨Åcial Intelligence , AAAI 2020 , The Thirty - Second Innovative Applications of ArtiÔ¨Åcial Intelligence Confer - ence , IAAI 2020 , The Tenth AAAI Symposium on Edu - cational Advances in ArtiÔ¨Åcial Intelligence , EAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 , pp . 9628 ‚Äì 9635 . AAAI Press , 2020 . URL https : / / ojs . aaai . org / index . php / AAAI / article / view / 6510 . Zhou , B . , Liu , J . , Sun , W . , Chen , R . , Tomlin , C . J . , and Yuan , Y . pbsgd : Powered stochastic gradient descent methods for accelerated non - convex optimization . In IJCAI , pp . 3258 ‚Äì 3266 , 2020a . Zhou , J . , Li , Z . , and Zhao , H . Parsing all : Syntax and semantics , dependencies and spans . In Cohn , T . , He , Y . , and Liu , Y . ( eds . ) , Findings of the Association for Computational Linguistics : EMNLP 2020 , Online Event , 16 - 20 November 2020 , volume EMNLP 2020 of Find - ings of ACL , pp . 4438 ‚Äì 4449 . Association for Compu - tational Linguistics , 2020b . doi : 10 . 18653 / v1 / 2020 . Ô¨Åndings - emnlp . 398 . URL https : / / doi . org / 10 . 18653 / v1 / 2020 . findings - emnlp . 398 . Zhuang , J . , Tang , T . , Ding , Y . , Tatikonda , S . C . , Dvornek , N . , Papademetris , X . , and Duncan , J . Adabelief optimizer : Adapting stepsizes by the belief in observed gradients . Advances in neural information processing systems , 33 : 18795 ‚Äì 18806 , 2020 . Zinkevich , M . Online convex programming and generalized inÔ¨Ånitesimal gradient ascent . In Proceedings of the 20th international conference on machine learning ( icml - 03 ) , pp . 928 ‚Äì 936 , 2003 . 11 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Appendix A . Related Work As an important part of machine learning and deep learning , optimizers have received much attention in recent years . The optimizer plays a prominent role in the convergence speed and the convergence effect of the model . To seek good properties like fast convergence , good generalization and robustness , many algorithms have been put forward recently , and they can be divided into four families according to their characteristics and motivation . SGD Family In this family , the optimizers adopt the method of update like Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t m t , where Œ∏ t denotes the parameter to be optimized at iteration step t and m t refers to some combination of past gradients ( such as EMA ) , which can be represented as f 1 ( g 1 , g 2 , . . . , g t ) . Original SGD ( Robbins & Monro , 1951 ) directly minus the product of global learning rate and the gradient at each step . Despite of its simplicity , it is still widely used in many datasets . However , SGD is blamed for its low convergence rate and high Ô¨Çuctuation , thus many methods have been proposed to accelerate the speed and smooth the update process . One efÔ¨Åcient optimizer to tackle this issue is SGDM ( Sutskever et al . , 2013 ) , which uses a exponential moving average ( EMA , also known as momentum ) to replace the gradient with an exponential weight decay of past gradients . SGDM - Nesterov ( Nesterov , 1983 ) is a variant of SGDM which modiÔ¨Åes the momentum by computing gradient based on the approximation of the next position and thus changing the descent direction . Experiments have shown that Nesterov momentum tends to achieve a higher speed and performance . Adam Family The Adam family optimizers usually update parameters by Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t m t / ‚àö v t , where v t is the adaptive item and can be represented as f 2 ( g 21 , g 22 , . . . , g 2 t ) . Compared to SGD family , instead of using a uniform learning rate , this kind of optimizer computes an individual learning rate for each parameter due to the effect of the denominator ‚àö v t in the equation . v t is usually a dimension - reduction approximation to the matrix which contains the information of second order curvature , such as Fisher matrix ( Pascanu & Bengio , 2013 ) . Adadelta ( Zeiler , 2012 ) , Adagrad ( Duchi et al . , 2011 ) and RMSprop ( Tieleman & Hinton , 2012 ) are early optimizers in this family . A stand out generation is Adam ( Kingma & Ba , 2014 ) which combines the RMSprop with Adagrad . It has been widely used in a wide range of datasets and works well even with sparse gradients . However , there are problems with Adam with respect to convergence and generalization , thus many methods have been proposed to make improvements Based on the large variance in the early stage that may lead to a bad optimum , heuristic warmup ( Vaswani et al . , 2017 ; Popel & Bojar , 2018 ) and RAdam ( Liu et al . , 2019 ) are proposed , of which the former starts with a small initial learning rate and the latter introduces a rectiÔ¨Åed item . To Ô¨Åx the convergence error , ( Reddi et al . , 2019 ) proposed AMSGrad which requires the non - decreasing property of the second momentum . In fact , this method can be interpolated into other Adam family algorithms to guarantee the convergence in convex situations . Considering the curvature of the loss function , AdaBelief ( Zhuang et al . , 2020 ) and AdaMomentum ( Wang et al . , 2021 ) are proposed . More recently , there are still numerous studies devoted to improving Adam , such as AdaX ( Li et al . , 2020a ) and AdaFamily ( Fassold , 2022 ) . However , we notice that most researchers put a solid emphasis on modifying the second momentum term , i . e . , the adaptive item and ignore the possibility to make a relative overall change to the algorithms . Stochastic Second - Order Family In the stochastic second - order optimizers , parameters are updated using second - order information related to Hessian matrix . The update process is typically written as Œ∏ t = Œ∏ t ‚àí 1 ‚àí Œ± t H ‚àí 1 m t , where H is the Hessian matrix or approximation matrix to it . Ideally , they can achieve better results than the Ô¨Årst order optimizers ( like Adam family and SGD family ) , but their practicality is limited due to the large computational cost of the second order information , like the Fisher / Hessian matrix . Some methods have been proposed using low - rank decomposition and approximating to hessian diagonal to reduce the cost , like Apollo ( Ma , 2020 ) , AdHessian ( Yao et al . , 2021 ) and Shampoo ( Anil et al . , 2020 ) . 12 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Starting Point Optimum Solution SGD with EMA ( SGDM : vanilla momentum ) Momentum Œ≤ = 0 . 90 SGD with DEMA ( Ours ) Momentum Œª = 0 . 90 Momentum Œ≤ = 0 . 60 Learning rate Œ± = 0 . 0015 Figure 3 . EMA vs . DEMA in SGD optimizer . Please refer to our online demo https : / / sites . google . com / view / optimizer - admeta for more comparison . Other Optimizers There are some algorithms that are not convenient to be categorized into the above families and we list some examples here . Motivated by PID controller , SGD - PID ( An et al . , 2018 ) takes an analogy between the gradient and the input error in an automatic control system . Analysis shows that it can reduce the overshoot problem in SGD and SGD variants . Furthermore , ( Weng et al . , 2022 ) applied PID to Adam and proposed the AdaPID optimizer . Lookahead ( Zhang et al . , 2019 ) optimizer updates two sets of weight wherein ‚Äùfast weights‚Äù function as a guide to search for the direction and ‚Äùslow weights‚Äù follow the guide to achieve better optimization . Ranger ( Wright , 2019 ) optimizer further combines RAdam and Lookahead to get a compound algorithm and shows a better convergence performance . Discussion To show the advantage of bidirectional looking , we propose A DMETA optimizer . SpeciÔ¨Åcally , it is based on the idea of considering backward - looking and forward - looking , wherein DEMA plays a important role in the former aspect and dynamic asymptotic forward - looking strategy serves for the latter aspect . In practical use , we provide two versions , A DMETA S and A DMETA R , using the framework of A DMETA and based on SGDM and RAdam respectively . SpeciÔ¨Åcally , A DMETA S replaces the traditionally used EMA in backward - looking part of SGDM with DEMA and adds the forward - looking part which is derived from Lookahead optimizer . A DMETA R is based on RAdam in the same way . The second order family is also introduced above because the framework of A DMETA can also be applied in this family , and it is remained as the future work . B . EMA vs . DEMA To corroborate our analysis of EMA and DEMA , we compared the optimization process of EMA and DEMA on the SGD optimizer according to the practice of ( Goh , 2017 ) . Using the same learning rate Œ± and starting from the same starting point , the convergence process is shown in Figure 3 . The decent surface in the Ô¨Ågure is the convex quadratic , which is a useful model despite its simplicity , for it comprises an important structure , the ‚Äúvalleys‚Äù , which is often studied as an example in momentum - based optimizers . As demonstrated in Figure 3 , on the one hand , DEMA achieves faster speed than EMA , which can be easily seen by comparing the distance to the optimal point at the same time ; on the other hand , DEMA achieves better convergence results than EMA as can be seen in the distance between the point of convergence and optimum . C . Proof of Convergence In this section , following ( Chen et al . , 2018 ) , ( Alacaoglu et al . , 2020 ) and ( Reddi et al . , 2019 ) , we provide detailed proofs of convergence for A DMETA R and A DMETA S optimizers in convex and non - convex situations . 13 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers C . 1 . Convergence Analysis in Convex and Non - convex Optimization Optimization problem For deterministic problems , the problem to be optimized is min Œ∏ ‚ààF f ( Œ∏ ) , where f denotes the loss function . For online optimization , the problem is min Œ∏ ‚ààF (cid:80) Tt = 1 f t ( Œ∏ ) , where f t is the loss function of the model with the given parameters at the t - th step . The criteria for judging convergence in convex and non - convex cases are different . For convex optimization , following ( Reddi et al . , 2019 ) , the goal is to ensure R ( T ) = o ( T ) , i . e . , lim T ‚Üí‚àû R ( T ) / T = 0 . For non - convex optimization , following ( Chen et al . , 2018 ) , the goal is to ensure min t ‚àà [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 = o ( T ) . Theorem C . 1 . ( Convergence of A DMETA R for convex optimization ) Let { Œ∏ t } be the sequence obtained from A DMETA R , 0 ‚â§ Œª , Œ≤ 1 , Œ≤ 2 < 1 , Œ≥ = Œ≤ 21 Œ≤ 2 < 1 , Œ± t = Œ± ‚àö t and v t ‚â§ v t + 1 , ‚àÄ t ‚àà [ T ] . Suppose x ‚àà F , where F ‚äÇ R d and has bounded diameter D ‚àû , i . e . | | Œ∏ t ‚àí Œ∏ | | ‚àû ‚â§ D ‚àû , ‚àÄ t ‚àà [ T ] . Assume f ( Œ∏ ) is a convex function and | | g t | | ‚àû is bounded . Denote the optimal point as Œ∏ . For Œ∏ t generated , A DMETA R achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( Œ∏ t ) ‚àí f t ( Œ∏ ) ] = O ( ‚àö T ) Theorem C . 2 . ( Convergence of A DMETA R for non - convex optimization ) Under the assumptions : ‚Ä¢ ‚àá f exits and is Lipschitz - continuous , i . e , | | ‚àá f ( x ) ‚àí ‚àá f ( y ) | | ‚â§ L | | x ‚àí y | | , ‚àÄ x , y ; f is also lower bounded . ‚Ä¢ At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ‚àá f is also bounded . ‚Ä¢ The noisy gradient is unbiased , and has independent noise , i . e . g t = ‚àá f ( Œ∏ t ) + Œ¥ t , E [ Œ¥ t ] = 0 and Œ¥ i ‚ä• Œ¥ j , ‚àÄ i (cid:54) = j . Assume min j ‚àà [ d ] ( v 1 ) j ‚â• c > 0 and Œ± t = Œ± / ‚àö t , then for any T we have : min t ‚àà [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ‚â§ 1 ‚àö T ( Q 1 + Q 2 log T ) where Q 1 and Q 2 are constants independent of T . Theorem C . 3 . ( Convergence of A DMETA S for convex optimization ) Let { Œ∏ t } be the sequence obtained by A DMETA S , 0 ‚â§ Œª , Œ≤ < 1 , Œ± t = Œ± ‚àö t , ‚àÄ t ‚àà [ T ] . Suppose x ‚àà F , where F ‚äÇ R d and has bounded diameter D ‚àû , i . e . | | Œ∏ t ‚àí Œ∏ | | ‚àû ‚â§ D ‚àû , ‚àÄ t ‚àà [ T ] . Assume f ( Œ∏ ) is a convex func - tion and | | g t | | ‚àû is bounded . Denote the optimal point as Œ∏ . For Œ∏ t generated , A DMETA S achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( Œ∏ t ) ‚àí f t ( Œ∏ ) ] = O ( ‚àö T ) Theorem C . 4 . ( Convergence of A DMETA S for non - convex optimization ) Under the assumptions : ‚Ä¢ ‚àá f exits and is Lipschitz - continuous , i . e , | | ‚àá f ( x ) ‚àí ‚àá f ( y ) | | ‚â§ L | | x ‚àí y | | , ‚àÄ x , y ; f is also lower bounded . ‚Ä¢ At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ‚àá f is also bounded . ‚Ä¢ The noisy gradient is unbiased , and has independent noise , i . e . g t = ‚àá f ( Œ∏ t ) + Œ¥ t , E [ Œ¥ t ] = 0 and Œ¥ i ‚ä• Œ¥ j , ‚àÄ i (cid:54) = j . Assume Œ± t = Œ± / ‚àö t , then for any T we have : min t ‚àà [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ‚â§ 1 ‚àö T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where Q (cid:48) 1 and Q (cid:48) 2 are constants independent of T . 14 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Before formally proving the theorems , here list some remarks and preparations . Remark 1 . For brevity , we omit the rectiÔ¨Åed item of A DMETA R in the proof . However , it does not inÔ¨Çuence the proof since it can be integrated into the learning rate . Remark 2 . Following ( Luo et al . , 2019 ) , the bias correction 1 / ( 1 ‚àí Œ≤ t 1 ) of the Ô¨Årst momentum m t is omitted in the convergence of A DMETA R . Since 1 / ( 1 ‚àí Œ≤ t 1 ) is bounded above 1 and below 10 , the order of the terms used is not affected , thus hardly affecting the proof . Remark 3 . The forward - looking part is not considered in the proof . On the one hand , explanations and proofs of constant Lookahead have been given in ( Zhang et al . , 2019 ) and ( Wang et al . , 2020 ) , which can be imitated by our dynamic method . On the other hand , forward - looking part is exactly the interpolation of fast weights and slow weights at each synchronization period , and the fast weights are updated by the given optimizer . Therefore , the convergence proof is equivalent to only proving the convergence of fast weights . Remark 4 . The condition in the theorem that v t ‚â§ v t + 1 , ‚àÄ t ‚àà [ T ] does not necessarily hold in the practice of our method . Dropping this condition may lead to a non - convergence result , which can be seen in ( Reddi et al . , 2019 ) . However , the counterexample given by this article is a very artiÔ¨Åcial design , which may not represent the case in practice . Many optimizers that do not meet this condition can eventually converge in the training process and further exploration may show that this condition is not necessary . Remark 5 . If we Ô¨Åx the number of steps , the training is a Ô¨Ånite process over Ô¨Ånite data , thus the iteration is bounded . Lemma C . 5 . if (cid:107) g t (cid:107) ‚àû is bounded , i . e . (cid:107) g t (cid:107) ‚àû ‚â§ G ‚àû , ‚àÄ t ‚àà [ T ] , where G ‚àû is a constant independent of T , then I t , h t and m t are also bounded . Proof . First of all , we prove (cid:107) I t (cid:107) ‚àû ‚â§ ( 1 + Œª ) G ‚àû by induction : when t = 1 (cid:107) I 1 (cid:107) ‚àû = (cid:107) g 1 (cid:107) ‚àû ‚â§ G ‚àû Suppose t = k satisÔ¨Åes , then for t = k + 1 (cid:107) I k + 1 (cid:107) ‚àû = (cid:107) ŒªI k + g k + 1 (cid:107) ‚àû ‚â§ Œª (cid:107) I k (cid:107) ‚àû + (cid:107) g k + 1 (cid:107) ‚àû ‚â§ ( Œª + 1 ) max { (cid:107) I k (cid:107) ‚àû , (cid:107) g k + 1 (cid:107) ‚àû } ‚â§ ( 1 + Œª ) G ‚àû Next , for (cid:107) h k (cid:107) ‚àû (cid:107) h t (cid:107) ‚àû = (cid:107) Œ∫g t + ¬µI t (cid:107) ‚àû ‚â§ Œ∫ (cid:107) g t (cid:107) ‚àû + ¬µ (cid:107) I t (cid:107) ‚àû ‚â§ [ Œ∫ + ( 1 ‚àí Œª ) ¬µ ) ] G ‚àû Since m t is the moving average of h i where i = 1 , . . . , t , we can get that it is also bounded following the proof of I t . In this way , we can redeÔ¨Åne G ‚àû by enlarging it and the bounded stochastic gradient assumption in the theorem is equivalent to assuming (cid:107) g t (cid:107) ‚àû , (cid:107) I t (cid:107) ‚àû , (cid:107) h t (cid:107) ‚àû , (cid:107) m t (cid:107) ‚àû ‚â§ G ‚àû . Remark 6 . As for non - convex optimization , in the same way , the bounded noisy gradient assumption is equivalent to (cid:107) g t (cid:107) , (cid:107) I t (cid:107) , (cid:107) h t (cid:107) , (cid:107) m t (cid:107) ‚â§ H where H is a constant independent of T . This remark will be used in several places in the following proof . Lemma C . 6 ( Generalized H¬®older inequality , ( Beckenbach & Bellman , 2012 ) ) . For x , y , z ‚àà R n + and positive p , q , r such that 1 p + 1 q + 1 r = 1 , we have n (cid:88) j = 1 Œ∏ j y j z j ‚â§ (cid:107) x (cid:107) p (cid:107) y (cid:107) q (cid:107) z (cid:107) r . This is a common mathematical inequality , so the proof is omitted here . Lemma C . 7 ( nonexpansiveness property of arg min x ‚ààF (cid:107) . (cid:107) , ( McMahan & Streeter , 2010 ) ) . For any Q ‚àà S d + , i . e . Q is a Positive deÔ¨Ånite matrice and convex feasible set F ‚äÇ R d , suppose u 1 = arg min x ‚ààF (cid:107) Q 1 / 2 ( x ‚àí z 1 ) (cid:107) and u 2 = arg min x ‚ààF (cid:107) Q 1 / 2 ( x ‚àí z 2 ) (cid:107) then we have (cid:107) Q 1 / 2 ( u 1 ‚àí u 2 ) (cid:107) ‚â§ (cid:107) Q 1 / 2 ( z 1 ‚àí z 2 ) (cid:107) . 15 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Proof . First , we claim that (cid:104) u 1 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) ‚â• 0 and (cid:104) u 2 ‚àí z 2 , Q ( u 1 ‚àí u 2 ) (cid:105) ‚â• 0 ( We only prove the former as the proofs are exactly the same ) . Otherwise , consider a small Œ¥ , we have u 1 + Œ¥ ( u 2 ‚àí u 1 ) ‚àà F 1 2 (cid:104) u 1 + Œ¥ ( u 2 ‚àí u 1 ) ‚àí z 1 , Q ( u 1 + Œ¥ ( u 2 ‚àí u 1 ) ‚àí z 1 ) (cid:105) = 1 2 (cid:104) u 1 ‚àí z 1 , Q ( u 1 ‚àí z 1 ) (cid:105) + 1 2 Œ¥ 2 (cid:104) u 2 ‚àí u 1 , Q ( u 2 ‚àí u 1 ) (cid:105) + Œ¥ (cid:104) u 1 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) If there exists (cid:104) u 1 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) < 0 , Œ¥ can be chosen so small that it satisÔ¨Åes 12 Œ¥ 2 (cid:104) u 2 ‚àí u 1 , Q ( u 2 ‚àí u 1 ) (cid:105) + Œ¥ (cid:104) u 1 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) < 0 , which contradicts the deÔ¨Ånition of u 1 . Using the above claim , we further have (cid:104) u 1 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) ‚àí (cid:104) u 2 ‚àí z 2 , Q ( u 2 ‚àí u 1 ) (cid:105) ‚â• 0 ‚áí(cid:104) z 2 ‚àí z 1 , Q ( u 2 ‚àí u 1 ) (cid:105) ‚â• (cid:104) u 2 ‚àí u 1 , Q ( u 2 ‚àí u 1 ) (cid:105) ( 19 ) Also , observing the following (cid:104) ( u 2 ‚àí u 1 ) ‚àí ( z 2 ‚àí z 1 ) , Q ( ( u 2 ‚àí u 1 ) ‚àí ( z 2 ‚àí z 1 ) ) (cid:105) ‚â• 0 ‚áí(cid:104) u 2 ‚àí u 1 , Q ( z 2 ‚àí z 1 ) (cid:105) ‚â§ 1 2 [ (cid:104) u 2 ‚àí u 1 , Q ( u 2 ‚àí u 1 ) (cid:105) + (cid:104) z 2 ‚àí z 1 , Q ( z 2 ‚àí z 1 ) (cid:105) ] ( 20 ) Combining ( 19 ) and ( 20 ) , we have the required result . C . 2 . Convergence Analysis of AdmetaR for Convex Optimization Lemma C . 8 . Consider m t = Œ≤ 1 m t ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) h t , ‚àÄ t ‚â• 1 . it follows that (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ (cid:105) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí Œ∏ t ‚àí 1 (cid:105) + 1 1 ‚àí Œ≤ 1 ( (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚àí (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ (cid:105) ) . Proof . By deÔ¨Ånition of m t , h t = 1 1 ‚àí Œ≤ 1 m t ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 m t ‚àí 1 . Thus , we have (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = 1 1 ‚àí Œ≤ 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí Œ∏ (cid:105) = 1 1 ‚àí Œ≤ 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ (cid:105) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí Œ∏ t ‚àí 1 (cid:105) = 1 1 ‚àí Œ≤ 1 (cid:0) (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚àí (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ (cid:105) (cid:1) + (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ (cid:105) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí Œ∏ t ‚àí 1 (cid:105) . Lemma C . 9 ( Bound for (cid:80) Tt = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ) . Under Assumption in Theorem 1 , we have T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚â§ ( 1 ‚àí Œ≤ 1 ) Œ± ‚àö 1 + log T (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 2 16 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Proof . First , we bound (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 . From the deÔ¨Ånition of m t and v t , it follows that m t = ( 1 ‚àí Œ≤ 1 ) t (cid:88) j = 1 Œ≤ t ‚àí j 1 h j , v t = ( 1 ‚àí Œ≤ 2 ) t (cid:88) j = 1 Œ≤ t ‚àí j 2 h 2 j Then we have (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚â§ (cid:107) v ‚àí 1 / 4 t m t (cid:107) 2 = d (cid:88) i = 1 m 2 t , i v 1 / 2 t , i = d (cid:88) i = 1 (cid:16)(cid:80) tj = 1 ( 1 ‚àí Œ≤ 1 ) Œ≤ t ‚àí j 1 h j , i (cid:17) 2 (cid:113)(cid:80) tj = 1 ( 1 ‚àí Œ≤ 2 ) Œ≤ t ‚àí j 2 h 2 j , i = ( 1 ‚àí Œ≤ 1 ) 2 ‚àö 1 ‚àí Œ≤ 2 d (cid:88) i = 1 (cid:16)(cid:80) tj = 1 Œ≤ t ‚àí j 1 h j , i (cid:17) 2 (cid:113)(cid:80) tj = 1 Œ≤ t ‚àí j 2 h 2 j , i ‚â§ ( 1 ‚àí Œ≤ 1 ) 2 ‚àö 1 ‚àí Œ≤ 2 (cid:32) d (cid:88) i = 1 (cid:20) (cid:16) (cid:80) tj = 1 ( Œ≤ t ‚àí j 4 2 | h j , i | 12 ) 4 (cid:17) 14 (cid:16) (cid:80) tj = 1 ( Œ≤ 1 / 2 1 Œ≤ ‚àí 1 / 4 2 ) 4 ( t ‚àí j ) (cid:17) 14 (cid:16) (cid:80) tj = 1 ( Œ≤ t ‚àí j 1 | h j , i | ) 12 ¬∑ 2 (cid:17) 12 (cid:21) 2 (cid:113)(cid:80) tj = 1 Œ≤ t ‚àí j 2 h 2 j , i (cid:33) = ( 1 ‚àí Œ≤ 1 ) 2 ‚àö 1 ‚àí Œ≤ 2 d (cid:88) i = 1 Ô£´ Ô£≠ t (cid:88) j = 1 Œ≥ t ‚àí j Ô£∂ Ô£∏ 12 t (cid:88) j = 1 Œ≤ t ‚àí j 1 | h j , i | ‚â§ ( 1 ‚àí Œ≤ 1 ) 2 (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 t (cid:88) j = 1 Œ≤ t ‚àí j 1 | h j , i | , ( 21 ) where the Ô¨Årst inequality follows from the fact that ÀÜ v 1 / 2 t , i ‚â• v 1 / 2 t , i , the second one follows from the generalized H ¬® older inequality for Œ∏ j = Œ≤ t ‚àí j 4 2 | h j , i | 12 , y j = ( Œ≤ 1 Œ≤ ‚àí 1 / 2 2 ) t ‚àí j 2 , z j = ( Œ≤ t ‚àí j 1 | h j , i | ) 12 and p = q = 4 , r = 2 , and the third one follows from the sum of geometric series and the assumption Œ≥ = Œ≤ 21 Œ≤ 2 < 1 . In this way , we can bound (cid:80) Tt = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 . T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚â§ ( 1 ‚àí Œ≤ 1 ) 2 (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 T (cid:88) t = 1 Œ± t t (cid:88) j = 1 Œ≤ t ‚àí j 1 | h j , i | = ( 1 ‚àí Œ≤ 1 ) 2 (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 T (cid:88) j = 1 T (cid:88) t = j Œ± t Œ≤ t ‚àí j 1 | h j , i | ‚â§ ( 1 ‚àí Œ≤ 1 ) (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 T (cid:88) j = 1 Œ± j | h j , i | ‚â§ 1 ‚àí Œ≤ 1 (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 (cid:118)(cid:117)(cid:117)(cid:116) T (cid:88) j = 1 Œ± 2 j (cid:118)(cid:117)(cid:117)(cid:116) T (cid:88) j = 1 h 2 j , i ‚â§ ( 1 ‚àí Œ≤ 1 ) Œ± ‚àö 1 + log T (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 (cid:118)(cid:117)(cid:117) (cid:116) T (cid:88) t = 1 h 2 t , i = ( 1 ‚àí Œ≤ 1 ) Œ± ‚àö 1 + log T (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 17 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where the Ô¨Årst inequality follows from ( 21 ) . The Ô¨Årst equality is by changing order of summation . The second inequality follows from the fact that (cid:80) Tt = j Œ± t Œ≤ t ‚àí j 1 ‚â§ Œ± j 1 ‚àí Œ≤ 1 . The third inequality is by Cauthy - Schwartz . The last inequality is by using (cid:80) Tj = 1 1 j ‚â§ 1 + log T Theorem C . 10 . ( Convergence of A DMETA R for convex optimization ) Let { Œ∏ t } be the sequence obtained from A DMETA R , 0 ‚â§ Œª , Œ≤ 1 , Œ≤ 2 < 1 , Œ≥ = Œ≤ 21 Œ≤ 2 < 1 , Œ± t = Œ± ‚àö t and v t ‚â§ v t + 1 , ‚àÄ t ‚àà [ T ] . Suppose x ‚àà F , where F ‚äÇ R d and has bounded diameter D ‚àû , i . e . | | Œ∏ t ‚àí Œ∏ | | ‚àû ‚â§ D ‚àû , ‚àÄ t ‚àà [ T ] . Assume f ( Œ∏ ) is a convex func - tion and | | g t | | ‚àû is bounded . Denote the optimal point as Œ∏ . For Œ∏ t generated , A DMETA R achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( Œ∏ t ) ‚àí f t ( Œ∏ ) ] = O ( ‚àö T ) Proof . ‚Ä¢ Bound for (cid:80) T t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) . As x ‚àà F , we get Œ∏ t + 1 = Œ† F , ‚àö ÀÜ v t ( Œ∏ t ‚àí Œ± t ÀÜ v ‚àí 1 / 2 t m t ) = min x ‚ààF (cid:107) ÀÜ v 1 / 4 t ( x ‚àí ( Œ∏ t ‚àí Œ± t ÀÜ v ‚àí 1 / 2 t m t ) ) (cid:107) . Furthermore , Œ† F , ‚àö ÀÜ v t ( x ) = x for all x ‚àà F . Using Lemma C . 7 with u 1 = Œ∏ t + 1 and u 2 = Œ∏ , we have the following : (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t + 1 ‚àí Œ∏ ) (cid:107) 2 ‚â§ (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t ‚àí Œ± t ÀÜ v ‚àí 1 / 2 t m t ‚àí Œ∏ ) (cid:107) 2 = (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t ‚àí Œ∏ ) (cid:107) 2 + Œ± 2 t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚àí 2 Œ± t (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ( 22 ) we rearrange and divide both sides of ( 22 ) by 2 Œ± t to get (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚â§ 1 2 Œ± t (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t ‚àí Œ∏ ) (cid:107) 2 ‚àí 1 2 Œ± t (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t + 1 ‚àí Œ∏ ) (cid:107) 2 + Œ± t 2 (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 = 1 2 Œ± t ‚àí 1 (cid:107) ÀÜ v 1 / 4 t ‚àí 1 ( Œ∏ t ‚àí Œ∏ ) (cid:107) 2 ‚àí 1 2 Œ± t (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t + 1 ‚àí Œ∏ ) (cid:107) 2 + 1 2 d (cid:88) i = 1 (cid:32) ÀÜ v 1 / 2 t , i Œ± t ‚àí ÀÜ v 1 / 2 t ‚àí 1 , i Œ± t ‚àí 1 (cid:33) ( Œ∏ t , i ‚àí Œ∏ i ) 2 + Œ± t 2 (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚â§ 1 2 Œ± t ‚àí 1 (cid:107) ÀÜ v 1 / 4 t ‚àí 1 ( Œ∏ t ‚àí Œ∏ ) (cid:107) 2 ‚àí 1 2 Œ± t (cid:107) ÀÜ v 1 / 4 t ( Œ∏ t + 1 ‚àí Œ∏ ) (cid:107) 2 + D 2 ‚àû 2 d (cid:88) i = 1 (cid:32) ÀÜ v 1 / 2 t , i Œ± t ‚àí ÀÜ v 1 / 2 t ‚àí 1 , i Œ± t ‚àí 1 (cid:33) + Œ± t 2 (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ( 23 ) where the last inequality is due to the fact that ÀÜ v t , i ‚â• ÀÜ v t ‚àí 1 , i , 1 Œ± t ‚â• 1 Œ± t ‚àí 1 , and the deÔ¨Ånition of D ‚àû . Summing ( 23 ) over t = 1 , . . . T and using that ÀÜ v 0 = 0 yields T (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚â§ D 2 ‚àû 2 Œ± T d (cid:88) i = 1 ÀÜ v 1 / 2 T , i + 1 2 T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 . 18 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers ‚Ä¢ Bound for (cid:80) Tt = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) . T (cid:88) t = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) = T (cid:88) t = 2 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) = T ‚àí 1 (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ t + 1 (cid:105) ‚â§ T ‚àí 1 (cid:88) t = 1 (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107)(cid:107) ÀÜ v 1 / 4 t ( Œ∏ t + 1 ‚àí Œ∏ ) (cid:107) = T ‚àí 1 (cid:88) t = 1 (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) (cid:13)(cid:13)(cid:13) ÀÜ v 1 / 4 t [ Œ† F , ÀÜ v 1 / 2 t (cid:16) Œ∏ t ‚àí Œ± t ÀÜ v ‚àí 1 / 2 t m t (cid:17) ‚àí Œ† F , ÀÜ v 1 / 2 t ( Œ∏ t ) ] (cid:13)(cid:13)(cid:13) ‚â§ T ‚àí 1 (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107)(cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) = T ‚àí 1 (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 where the Ô¨Årst inequality follows from H¬®older inequality and the second inequality is due to lemma C . 7 ‚Ä¢ Bound for (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) . (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) ‚â§ (cid:107) ÀÜ v ‚àí 1 / 4 t m T (cid:107)(cid:107) ÀÜ v 1 / 4 t ( Œ∏ T ‚àí Œ∏ ) (cid:107) ‚â§ Œ± T (cid:107) ÀÜ v ‚àí 1 / 4 t m T (cid:107) 2 + 1 4 Œ± T (cid:107) ÀÜ v 1 / 4 t ( Œ∏ T ‚àí Œ∏ ) (cid:107) 2 ‚â§ Œ± T (cid:107) ÀÜ v ‚àí 1 / 4 t m T (cid:107) 2 + D 2 ‚àû 4 Œ± T d (cid:88) i = 1 ÀÜ v 1 / 2 T , i where the Ô¨Årst inequality follows from H ¬® older inequality and the second inequality follows from Young‚Äôs inequality . The last inequality is due to the deÔ¨Ånition of D ‚àû . After all these preparations , we obtain : T (cid:88) t = 1 (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) + T (cid:88) t = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) (cid:33) + T (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚â§ Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) D 2 ‚àû 4 Œ± T d (cid:88) i = 1 ÀÜ v 1 / 2 T , i + T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 (cid:33) + D 2 ‚àû 2 Œ± T d (cid:88) i = 1 ÀÜ v 1 / 2 T , i + 1 2 T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 = ( 2 ‚àí Œ≤ 1 ) D 2 ‚àû 4 Œ± T ( 1 ‚àí Œ≤ 1 ) d (cid:88) i = 1 ÀÜ v 1 / 2 T , i + 2 + Œ≤ 1 2 ( 1 ‚àí Œ≤ 1 ) T (cid:88) t = 1 Œ± t (cid:107) ÀÜ v ‚àí 1 / 4 t m t (cid:107) 2 ‚â§ ( 2 ‚àí Œ≤ 1 ) D 2 ‚àû ‚àö T 4 Œ± ( 1 ‚àí Œ≤ 1 ) d (cid:88) i = 1 ÀÜ v 1 / 2 T , i + ( 2 + Œ≤ 1 ) Œ± ‚àö 1 + log T 2 (cid:112) ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) d (cid:88) i = 1 (cid:107) h 1 : T , i (cid:107) 2 This proves that (cid:80) Tt = 1 (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = O ( ‚àö T ) . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Since h t = Œ∫g t + ¬µI t , h t is the same order as g t when the time is long enough , thus we have T (cid:88) t = 1 (cid:104) g t , Œ∏ t ‚àí Œ∏ (cid:105) = O ( ‚àö T ) ( 24 ) 19 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers In addition , due to the convexity of f ( . ) , we have R ( T ) = T (cid:88) t = 1 ( f t ( Œ∏ t ) ‚àí f t ( x ) ) ‚â§ T (cid:88) t = 1 (cid:104) g t , Œ∏ t ‚àí Œ∏ (cid:105) Combined with ( 24 ) , we complete the proof . C . 3 . Convergence Analysis of AdmetaR for Non - convex Optimization Lemma C . 11 . Set Œ∏ 0 (cid:44) x 1 in Algorithm ( 1 ) , and deÔ¨Åne z t as z t = Œ∏ t + Œ≤ 1 1 ‚àí Œ≤ 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) , ‚àÄ t ‚â• 1 . ( 25 ) Then the following holds true z t + 1 ‚àí z t = ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:33) m t ‚àí 1 ‚àí Œ± t h t / (cid:112) ÀÜ v t Proof . By the update of A DMETA R , we have Œ∏ t + 1 ‚àí Œ∏ t = ‚àí Œ± t m t / (cid:112) ÀÜ v t = ‚àí Œ± t ( Œ≤ 1 m t ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) h t ) / (cid:112) ÀÜ v t = Œ≤ 1 Œ± t Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 ‚àö ÀÜ v t ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ± t ( 1 ‚àí Œ≤ 1 ) h t / (cid:112) ÀÜ v t = Œ≤ 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) + Œ≤ 1 (cid:32) Œ± t Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 ‚àö ÀÜ v t ‚àí 1 (cid:33) ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ± t ( 1 ‚àí Œ≤ 1 ) h t / (cid:112) ÀÜ v t = Œ≤ 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ 1 (cid:32) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:33) m t ‚àí 1 ‚àí Œ± t ( 1 ‚àí Œ≤ 1 ) h t / (cid:112) ÀÜ v t ( 26 ) Since we also have Œ∏ t + 1 ‚àí Œ∏ t = ( 1 ‚àí Œ≤ 1 ) Œ∏ t + 1 + Œ≤ 1 ( Œ∏ t + 1 ‚àí Œ∏ t ) ‚àí ( 1 ‚àí Œ≤ 1 ) Œ∏ t Combined with ( 26 ) , we have ( 1 ‚àí Œ≤ 1 ) Œ∏ t + 1 + Œ≤ 1 ( Œ∏ t + 1 ‚àí Œ∏ t ) = ( 1 ‚àí Œ≤ 1 ) Œ∏ t + Œ≤ 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ 1 (cid:32) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:33) m t ‚àí 1 ‚àí Œ± t ( 1 ‚àí Œ≤ 1 ) h t / (cid:112) ÀÜ v t . Divide both sides by 1 ‚àí Œ≤ 1 , we have Œ∏ t + 1 + Œ≤ 1 1 ‚àí Œ≤ 1 ( Œ∏ t + 1 ‚àí Œ∏ t ) = Œ∏ t + Œ≤ 1 1 ‚àí Œ≤ 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:33) m t ‚àí 1 ‚àí Œ± t h t / (cid:112) ÀÜ v t . Lemma C . 12 . Suppose that the conditions in Theorem C . 2 hold , then E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] ‚â§ 4 (cid:88) i = 1 T i , ( 27 ) 20 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where T 1 = ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) m i ‚àí 1 (cid:105) (cid:35) T 2 = ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) T 3 = E Ô£Æ Ô£∞ t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± t ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) m i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£ª T 4 = E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13) Œ± i h i / (cid:112) ÀÜ v i (cid:13)(cid:13)(cid:13) 2 (cid:35) Proof . By the Lipschitz smoothness of ‚àá f , f ( z t + 1 ) ‚â§ f ( z t ) + (cid:104)‚àá f ( z t ) , z t + 1 ‚àí z t (cid:105) + L 2 (cid:107) z t + 1 ‚àí z t (cid:107) 2 , Based on ( C . 18 ) , we have E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] = E (cid:34) t (cid:88) i = 1 f ( z i + 1 ) ‚àí f ( z i ) (cid:35) ‚â§ E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , z i + 1 ‚àí z i (cid:105) + L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:35) = ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) m i ‚àí 1 (cid:105) (cid:35) ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) + E (cid:20) t (cid:88) i = 1 L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:21) = T 1 + T 2 + E (cid:20) t (cid:88) i = 1 L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:21) , Then , using inequality (cid:107) a + b (cid:107) 2 ‚â§ 2 (cid:107) a (cid:107) 2 + 2 (cid:107) b (cid:107) 2 and combined with lemma C . 11 , E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:35) ‚â§ T 3 + T 4 Lemma C . 13 . In this part , we bound T 1 , T 2 , T 3 Proof . ‚Ä¢ Bound for T 1 T 1 = ‚àí E (cid:34) t (cid:88) i = 2 (cid:104)‚àá f ( z i ) , Œ≤ 1 1 ‚àí Œ≤ 1 (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) m i ‚àí 1 (cid:105) (cid:35) ‚â§ E Ô£Æ Ô£∞ t (cid:88) i = 1 (cid:107)‚àá f ( z i ) (cid:107) (cid:107) m i ‚àí 1 (cid:107) (cid:18) 1 1 ‚àí Œ≤ 1 ‚àí 1 (cid:19) d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) (cid:12) (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) j (cid:12)(cid:12)(cid:12) (cid:12) Ô£π Ô£ª ‚â§ H 2 Œ≤ 1 1 ‚àí Œ≤ 1 E Ô£Æ Ô£∞ t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12)Ô£πÔ£ª 21 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers ‚Ä¢ Bound for T 3 T 3 ‚â§ LE Ô£Æ Ô£∞ t (cid:88) i = 2 (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 d (cid:88) j = 1 Ô£´ Ô£≠ (cid:32) Œ± t ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) 2 j ( m i ‚àí 1 ) 2 j Ô£∂ Ô£∏ Ô£π Ô£ª ‚â§ (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 LH 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) Œ± t ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) 2 j Ô£π Ô£ª ‚Ä¢ Bound for T 2 T 2 = ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) = ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) . ( 28 ) The second term of ( 28 ) can be bounded as ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) ‚â§ E (cid:34) t (cid:88) i = 2 1 2 (cid:107)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) (cid:107) 2 + 1 2 (cid:107) Œ± i h i / (cid:112) ÀÜ v i (cid:107) 2 (cid:35) ‚â§ L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) Œ≤ 1 1 ‚àí Œ≤ 1 Œ± i ‚àí 1 m i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) Œ± i h i / (cid:112) ÀÜ v i (cid:107) 2 (cid:35) = L 2 2 (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) Œ± i h i / (cid:112) ÀÜ v i (cid:107) 2 (cid:35) where the second inequality is due to (cid:107)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) (cid:107) ‚â§ L (cid:107) z i ‚àí Œ∏ i (cid:107) . Then consider the Ô¨Årst term of ( 28 ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , h i / (cid:112) ÀÜ v i (cid:105) (cid:35) = Œ∫E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , g i / (cid:112) ÀÜ v i (cid:105) (cid:35) + ¬µE (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , I i / (cid:112) ÀÜ v i (cid:105) (cid:35) Consider the term with Œ∫ E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , g i / (cid:112) ÀÜ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ( ‚àá f ( Œ∏ i ) + Œ¥ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , Œ¥ i / (cid:112) ÀÜ v i (cid:105) (cid:35) . ( 29 ) 22 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers For the second term in RHS of ( 29 ) , we have E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , Œ¥ i / (cid:112) ÀÜ v i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 2 (cid:104)‚àá f ( Œ∏ i ) , Œ¥ i ( Œ± i / (cid:112) ÀÜ v i ‚àí Œ± i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 ) (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 2 Œ± i ‚àí 1 (cid:104)‚àá f ( Œ∏ i ) , Œ¥ i ( 1 / (cid:112) ÀÜ v i ‚àí 1 ) (cid:105) (cid:35) + E (cid:104) Œ± 1 (cid:104)‚àá f ( x 1 ) , Œ¥ 1 / (cid:112) ÀÜ v 1 (cid:105) (cid:105) ‚â• E (cid:34) t (cid:88) i = 2 (cid:104)‚àá f ( Œ∏ i ) , Œ¥ i ( Œ± i / (cid:112) ÀÜ v i ‚àí Œ± i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 ) (cid:105) (cid:35) ‚àí 2 H 2 E Ô£Æ Ô£∞ d (cid:88) j = 1 ( Œ± 1 / (cid:112) ÀÜ v 1 ) j Ô£π Ô£ª ( 30 ) where the last equation is because given Œ∏ i , ÀÜ v i ‚àí 1 , E (cid:2) Œ¥ i ( 1 / (cid:112) ÀÜ v i ‚àí 1 ) | Œ∏ i , ÀÜ v i ‚àí 1 (cid:3) = 0 and (cid:107) Œ¥ i (cid:107) ‚â§ 2 H . Further , we have E (cid:34) t (cid:88) i = 2 (cid:104)‚àá f ( Œ∏ i ) , Œ¥ t ( Œ± i / (cid:112) ÀÜ v i ‚àí Œ± i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 ) (cid:105) (cid:35) = E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 ( ‚àá f ( Œ∏ i ) ) j ( Œ¥ t ) j ( Œ± i / ( (cid:112) ÀÜ v i ) j ‚àí Œ± i ‚àí 1 / ( (cid:112) ÀÜ v i ‚àí 1 ) j ) Ô£π Ô£ª ‚â• ‚àí E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 | ( ‚àá f ( Œ∏ i ) ) j | | ( Œ¥ t ) j | (cid:12)(cid:12)(cid:12) ( Œ± i / ( (cid:112) ÀÜ v i ) j ‚àí Œ± i ‚àí 1 / ( (cid:112) ÀÜ v i ‚àí 1 ) j ) (cid:12)(cid:12)(cid:12)Ô£πÔ£ª ‚â• ‚àí 2 H 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( Œ± i / ( (cid:112) ÀÜ v i ) j ‚àí Œ± i ‚àí 1 / ( (cid:112) ÀÜ v i ‚àí 1 ) j ) (cid:12)(cid:12)(cid:12)Ô£πÔ£ª ( 31 ) Substitute ( 30 ) and ( 31 ) into ( 29 ) , we then get ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , g i / (cid:112) ÀÜ v i (cid:105) (cid:35) ‚â§ 2 H 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( Œ± i / ( (cid:112) ÀÜ v i ) j ‚àí Œ± i ‚àí 1 / ( (cid:112) ÀÜ v i ‚àí 1 ) j ) (cid:12)(cid:12)(cid:12)Ô£πÔ£ª + 2 H 2 E Ô£Æ Ô£∞ d (cid:88) j = 1 ( Œ± 1 / (cid:112) ÀÜ v 1 ) j Ô£π Ô£ª ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) ( 32 ) Then , consider the term with ¬µ . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . In other words , we can bound it the same way as the term with Œ∫ . After all these bounds , we Ô¨Ånally get T 2 ‚â§ L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) Œ≤ 1 1 ‚àí Œ≤ 1 Œ± i ‚àí 1 m i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) Œ± i h i / (cid:112) ÀÜ v i (cid:107) 2 (cid:35) + 2 ( Œ∫ + ¬µ ) H 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12) ( Œ± i / ( (cid:112) ÀÜ v i ) j ‚àí Œ± i ‚àí 1 / ( (cid:112) ÀÜ v i ‚àí 1 ) j ) (cid:12)(cid:12)(cid:12)Ô£πÔ£ª + 2 ( Œ∫ + ¬µ ) H 2 E Ô£Æ Ô£∞ d (cid:88) j = 1 ( Œ± 1 / (cid:112) ÀÜ v 1 ) j Ô£π Ô£ª ‚àí ( Œ∫ + ¬µ ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) 23 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Lemma C . 14 . Suppose the conditions in theorem C . 2 hold . Then we have E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) ‚â§ E (cid:34) C 1 t (cid:88) i = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t ‚àö ÀÜ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 t ‚àí 1 (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 where C 1 , C 2 , C 3 , C 4 and C 5 are independent of the step . Proof . Combining lemma C . 12 and lemma C . 13 , we get E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] ‚â§ H 2 Œ≤ 1 1 ‚àí Œ≤ 1 E Ô£Æ Ô£∞ t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12)Ô£πÔ£ª + (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 LH 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) Œ± t ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) 2 j Ô£π Ô£ª + E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13) Œ± i h i / (cid:112) ÀÜ v i (cid:13)(cid:13)(cid:13) 2 (cid:35) + L 2 2 E (cid:34) t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13) Œ≤ 1 1 ‚àí Œ≤ 1 Œ± i ‚àí 1 m i ‚àí 1 / (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 2 (cid:107) Œ± i h i / (cid:112) ÀÜ v i (cid:107) 2 (cid:35) + 2 ( Œ∫ + ¬µ ) H 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12) Ô£π Ô£ª + 2 ( Œ∫ + ¬µ ) H 2 E Ô£Æ Ô£∞ d (cid:88) j = 1 ( Œ± 1 / (cid:112) ÀÜ v 1 ) j Ô£π Ô£ª ‚àí ( Œ∫ + ¬µ ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) 24 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers By merging similar terms in above inequality and noticing that Œ∫ + ¬µ > 0 , we get E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) / (cid:112) ÀÜ v i (cid:105) (cid:35) ‚â§ (cid:18) 2 H 2 + Œ≤ 1 H 2 ( 1 ‚àí Œ≤ 1 ) ( Œ∫ + ¬µ ) (cid:19) E Ô£Æ Ô£∞ t (cid:88) i = 1 d (cid:88) j = 1 (cid:12)(cid:12)(cid:12)(cid:12) (cid:32) Œ± i ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) j (cid:12)(cid:12)(cid:12)(cid:12)Ô£πÔ£ª + (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 LH 2 Œ∫ + ¬µE Ô£Æ Ô£∞ t (cid:88) i = 2 d (cid:88) j = 1 (cid:32) Œ± t ‚àö ÀÜ v i ‚àí Œ± i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:33) 2 j Ô£π Ô£ª + (cid:18) 2 L + 1 2 ( Œ∫ + ¬µ ) (cid:19) E (cid:34) t (cid:88) i = 2 (cid:107) Œ± i h i ‚àö ÀÜ v i (cid:107) 2 (cid:35) + L 2 2 ( Œ∫ + ¬µ ) (cid:18) Œ≤ 1 1 ‚àí Œ≤ 1 (cid:19) 2 E Ô£Æ Ô£∞ t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£ª + 2 H 2 E Ô£Æ Ô£∞ d (cid:88) j = 1 ( Œ± 1 / (cid:112) ÀÜ v 1 ) j Ô£π Ô£ª + 1 Œ∫ + ¬µE [ f ( z 1 ) ‚àí f ( z t + 1 ) ] = E (cid:34) C 1 t (cid:88) i = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t ‚àö ÀÜ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 t ‚àí 1 (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 ( 33 ) Theorem C . 15 . ( Convergence of A DMETA R for non - convex optimization ) Under the assumptions : ‚Ä¢ ‚àá f exits and is Lipschitz - continuous , i . e , | | ‚àá f ( x ) ‚àí ‚àá f ( y ) | | ‚â§ L | | x ‚àí y | | , ‚àÄ x , y ; f is also lower bounded . ‚Ä¢ At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ‚àá f is also bounded . ‚Ä¢ The noisy gradient is unbiased , and has independent noise , i . e . g t = ‚àá f ( Œ∏ t ) + Œ¥ t , E [ Œ¥ t ] = 0 and Œ¥ i ‚ä• Œ¥ j , ‚àÄ i (cid:54) = j . Assume min j ‚àà [ d ] ( v 1 ) j ‚â• c > 0 and Œ± t = Œ± / ‚àö t , then for any T we have : min t ‚àà [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) 2 ‚â§ 1 ‚àö T ( Q 1 + Q 2 log T ) where Q 1 and Q 2 are constants independent of T . Proof . We bound non - constant terms in RHS of ( 33 ) , which is given by E (cid:34) C 1 T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t ‚àö ÀÜ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 + C 4 T ‚àí 1 (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 ‚Ä¢ Bound the term with C 1 . 25 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Note that min j ‚àà [ d ] ( ‚àö ÀÜ v 1 ) j ‚â• min j ‚àà [ d ] | ( h 1 ) j | ‚â• c > 0 , thus we have E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t ‚àö ÀÜ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) ‚â§ E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t c (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) = E (cid:34) T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ±h t c ‚àö t (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) = E (cid:34) T (cid:88) t = 1 (cid:18) Œ± c ‚àö t (cid:19) 2 (cid:107) h t (cid:107) 2 (cid:35) ‚â§ H 2 Œ± 2 c 2 T (cid:88) t = 1 1 t ‚â§ H 2 Œ± 2 c 2 ( 1 + log T ) where the Ô¨Årst inequality is due to ( ÀÜ v t ) j ‚â• ( ÀÜ v t ‚àí 1 ) j , and the last inequality is due to (cid:80) Tt = 1 1 t ‚â§ 1 + log T . ‚Ä¢ Bound the term with C 2 . Apply the same proof as above , we get t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 ‚â§ H 2 Œ± 2 c 2 ( 1 + log T ) ‚Ä¢ Bound the term with C 3 . E (cid:34) T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 (cid:35) = E Ô£Æ Ô£∞ d (cid:88) j = 1 T (cid:88) t = 2 (cid:32) Œ± t ‚àí 1 ( (cid:112) ÀÜ v t ‚àí 1 ) j ‚àí Œ± t ( ‚àö ÀÜ v t ) j (cid:33)Ô£π Ô£ª = E Ô£Æ Ô£∞ d (cid:88) j = 1 (cid:18) Œ± 1 ( ‚àö ÀÜ v 1 ) j ‚àí Œ± T ( ‚àö ÀÜ v T ) j (cid:19)Ô£πÔ£ª ‚â§ E Ô£Æ Ô£∞ d (cid:88) j = 1 Œ± 1 ( ‚àö ÀÜ v 1 ) j Ô£π Ô£ª ‚â§ dŒ± c ( 34 ) where the Ô¨Årst equality is due to ( ÀÜ v t ) j ‚â• ( ÀÜ v t ‚àí 1 ) j and Œ± t ‚â§ Œ± t ‚àí 1 , and the second equality is due to telescope sum . ‚Ä¢ Bound the term with C 4 . E Ô£Æ Ô£∞ T ‚àí 1 (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£ª = E Ô£Æ Ô£∞ T ‚àí 1 (cid:88) t = 2 d (cid:88) j = 1 (cid:32) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:33) 2 i Ô£π Ô£ª ‚â§ E Ô£Æ Ô£∞ T ‚àí 1 (cid:88) t = 2 d (cid:88) j = 1 Œ± c (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) i Ô£π Ô£ª ‚â§ dŒ± 2 c 2 where the Ô¨Årst inequality is due to | ( Œ± t / ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 / (cid:112) ÀÜ v t ‚àí 1 ) j | ‚â§ 1 / c . Then we have for A DMETA R , E (cid:34) C 1 T (cid:88) t = 1 (cid:13)(cid:13)(cid:13)(cid:13) Œ± t h t ‚àö ÀÜ v t (cid:13)(cid:13)(cid:13)(cid:13) 2 + C 2 t (cid:88) i = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± i ‚àí 1 m i ‚àí 1 (cid:112) ÀÜ v i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 + C 3 T (cid:88) t = 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 ( 35 ) + C 4 T ‚àí 1 (cid:88) t = 2 (cid:13) (cid:13) (cid:13)(cid:13)(cid:13) Œ± t ‚àö ÀÜ v t ‚àí Œ± t ‚àí 1 (cid:112) ÀÜ v t ‚àí 1 (cid:13) (cid:13) (cid:13)(cid:13)(cid:13) 2 (cid:35) + C 5 ( 36 ) ‚â§ C 1 H 2 Œ± 2 c 2 ( 1 + log T ) + C 2 H 2 Œ± 2 c 2 ( 1 + log T ) + C 3 dŒ± c + C 4 dŒ± 2 c 2 + C 5 ( 37 ) 26 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Furthermore , due to (cid:107) g t (cid:107) ‚â§ H , we have ( ÀÜ v t ) j ‚â§ H 2 , then we get Œ± / ( (cid:112) ÀÜ v t ) j ‚â• 1 H ‚àö t Thus we have E (cid:34) T (cid:88) t = 1 Œ± i (cid:104)‚àá f ( Œ∏ t ) , ‚àá f ( Œ∏ t ) / (cid:112) ÀÜ v t (cid:105) (cid:35) ‚â• E (cid:34) T (cid:88) t = 1 1 H ‚àö t (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:35) ‚â• ‚àö T H min t ‚àà [ T ] E (cid:2) (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:3) ( 38 ) Combining ( 37 ) and ( 38 ) , we have min t ‚àà [ T ] E (cid:2) (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:3) ‚â§ H ‚àö T (cid:18) ( C 1 + C 2 ) H 2 Œ± 2 c 2 ( 1 + log T ) + C 3 dŒ± c + C 4 dŒ± 2 c 2 + C 5 (cid:19) = 1 ‚àö T ( Q 1 + Q 2 log T ) This completes the proof . C . 4 . Convergence Analysis of AdmetaS for Convex Optimization Lemma C . 16 ( Bound for (cid:80) Tt = 1 Œ± t (cid:107) m t (cid:107) 2 ) . Under Assumption in Theorem 3 , we have T (cid:88) t = 1 Œ± t (cid:107) m t (cid:107) 2 ‚â§ 2 Œ±dG 2 ‚àû ‚àö T Proof . First , we bound (cid:107) m t (cid:107) . (cid:107) m t (cid:107) 2 ‚â§ d (cid:107) m t (cid:107) 2 ‚àû ‚â§ dG 2 ‚àû ( 39 ) Now we can bound (cid:80) Tt = 1 Œ± t (cid:107) m t (cid:107) 2 T (cid:88) t = 1 Œ± t (cid:107) m t (cid:107) 2 ‚â§ dG 2 ‚àû T (cid:88) t = 1 Œ± t = Œ±dG 2 ‚àû T (cid:88) t = 1 1 ‚àö t ‚â§ 2 Œ±dG 2 ‚àû ‚àö T Theorem C . 17 . ( Convergence of A DMETA S for convex optimization ) Let { Œ∏ t } be the sequence obtained by A DMETA S , 0 ‚â§ Œª , Œ≤ < 1 , Œ± t = Œ± ‚àö t , ‚àÄ t ‚àà [ T ] . Suppose x ‚àà F , where F ‚äÇ R d and has bounded diameter D ‚àû , i . e . | | Œ∏ t ‚àí Œ∏ | | ‚àû ‚â§ D ‚àû , ‚àÄ t ‚àà [ T ] . . Assume f ( Œ∏ ) is a convex func - tion and | | g t | | ‚àû is bounded . Denote the optimal point as Œ∏ . For Œ∏ t generated , A DMETA S achieves the regret : R ( T ) = T (cid:88) t = 1 [ f t ( Œ∏ t ) ‚àí f t ( Œ∏ ) ] = O ( ‚àö T ) Proof . ‚Ä¢ Bound for (cid:80) T t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) . From the update process , we get (cid:107) Œ∏ t + 1 ‚àí Œ∏ (cid:107) 2 = (cid:107) Œ∏ t ‚àí Œ∏ ‚àí Œ± t m t (cid:107) 2 = (cid:107) Œ∏ t ‚àí Œ∏ (cid:107) 2 ‚àí 2 Œ± t (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) + Œ± 2 t (cid:107) m t (cid:107) 2 27 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers thus we have T (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) = T (cid:88) t = 1 1 2 Œ± t (cid:0) (cid:107) Œ∏ t ‚àí Œ∏ (cid:107) 2 ‚àí (cid:107) Œ∏ t + 1 ‚àí Œ∏ (cid:107) 2 (cid:1) + T (cid:88) i = 1 Œ± t 2 (cid:107) m t (cid:107) 2 Consider the left - hand side T (cid:88) t = 1 1 2 Œ± t (cid:0) (cid:107) Œ∏ t ‚àí Œ∏ (cid:107) 2 ‚àí (cid:107) Œ∏ t + 1 ‚àí Œ∏ (cid:107) 2 (cid:1) = 1 2 Œ± 1 (cid:107) Œ∏ 1 ‚àí Œ∏ (cid:107) 2 + T (cid:88) t = 2 (cid:18) 1 2 Œ± t ‚àí 1 2 Œ± t ‚àí 1 (cid:19) (cid:107) Œ∏ t ‚àí Œ∏ (cid:107) 2 ‚àí 1 2 Œ± T (cid:107) Œ∏ T + 1 ‚àí Œ∏ (cid:107) 2 ‚â§ dD 2 ‚àû 2 Œ± 1 + dD 2 ‚àû T (cid:88) t = 2 (cid:18) 1 2 Œ± t ‚àí 1 2 Œ± t ‚àí 1 (cid:19) + 0 = dD 2 ‚àû 2 Œ± T Finally , we get T (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚â§ dD 2 ‚àû 2 Œ± T + T (cid:88) i = 1 Œ± t 2 (cid:107) m t (cid:107) 2 ‚Ä¢ Bound for (cid:80) Tt = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) . T (cid:88) t = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) = T ‚àí 1 (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ t + 1 (cid:105) = T ‚àí 1 (cid:88) t = 1 (cid:104) m t , Œ± t m t (cid:105) = T ‚àí 1 (cid:88) t = 1 Œ± t (cid:107) m t (cid:107) 2 ‚Ä¢ Bound for (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) . (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) ‚â§ Œ± T (cid:107) m T (cid:107) 2 + 1 4 Œ± T (cid:107) Œ∏ T ‚àí Œ∏ (cid:107) 2 ‚â§ Œ± T (cid:107) m T (cid:107) 2 + dD 2 ‚àû 4 Œ± T where the Ô¨Årst inequality follows from Young‚Äôs inequality . Combining all these preparations , we obtain T (cid:88) t = 1 (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = 1 1 ‚àí Œ≤ (cid:0) (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) ‚àí (cid:104) m 0 , Œ∏ 0 ‚àí Œ∏ (cid:105) (cid:1) + (cid:104) m 0 , Œ∏ 0 ‚àí Œ∏ (cid:105) + T ‚àí 1 (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) + Œ≤ 1 ‚àí Œ≤ T (cid:88) t = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) = Œ≤ 1 ‚àí Œ≤ (cid:104) m T , Œ∏ T ‚àí Œ∏ (cid:105) + Œ≤ 1 ‚àí Œ≤ T (cid:88) t = 1 (cid:104) m t ‚àí 1 , Œ∏ t ‚àí 1 ‚àí Œ∏ t (cid:105) + T (cid:88) t = 1 (cid:104) m t , Œ∏ t ‚àí Œ∏ (cid:105) ‚â§ Œ≤ 1 ‚àí Œ≤ (cid:32) dD ‚àû 4 Œ± T + T (cid:88) t = 1 Œ± t (cid:107) m t (cid:107) 2 (cid:33) + dD 2 ‚àû 2 Œ± T + T (cid:88) i = 1 Œ± t 2 (cid:107) m t (cid:107) 2 ‚â§ (cid:18) Œ≤ 1 ‚àí Œ≤ + 2 (cid:19) dD ‚àû 4 Œ± T + (cid:18) 2 Œ±Œ≤ 1 ‚àí Œ≤ + Œ± (cid:19) dG 2 ‚àû ‚àö T 28 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers This proves that (cid:80) Tt = 1 (cid:104) h t , Œ∏ t ‚àí Œ∏ (cid:105) = O ( ‚àö T ) . Suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Since h t = Œ∫g t + ¬µI t , h t is the same order as g t when the time is long enough , thus we have T (cid:88) t = 1 (cid:104) g t , Œ∏ t ‚àí Œ∏ (cid:105) = O ( ‚àö T ) ( 40 ) In addition , due to the convexity of f ( . ) , we have R ( T ) = T (cid:88) t = 1 ( f t ( Œ∏ t ) ‚àí f t ( x ) ) ‚â§ T (cid:88) t = 1 (cid:104) g t , Œ∏ t ‚àí Œ∏ (cid:105) Combined with ( 40 ) , we complete the proof . C . 5 . Convergence Analysis of AdmetaS for Non - convex Optimization Lemma C . 18 . Set Œ∏ 0 (cid:44) Œ∏ 1 in Algorithm ( 2 ) , and deÔ¨Åne z t as z t = Œ∏ t + Œ≤ 1 ‚àí Œ≤ ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) , ‚àÄ t ‚â• 1 . ( 41 ) Then the following holds z t + 1 ‚àí z t = ‚àí Œ≤ 1 ‚àí Œ≤ ( Œ± t ‚àí Œ± t ‚àí 1 ) m t ‚àí 1 ‚àí Œ± t h t Proof . By the update rule of A DMETA S , we have Œ∏ t + 1 ‚àí Œ∏ t = ‚àí Œ± t m t = ‚àí Œ± t [ Œ≤m t ‚àí 1 + ( 1 ‚àí Œ≤ ) h t ] = Œ≤ Œ± t Œ± t ‚àí 1 ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ± t ( 1 ‚àí Œ≤ ) h t = Œ≤ ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) + Œ≤ (cid:18) Œ± t Œ± t ‚àí 1 ‚àí 1 (cid:19) ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ± t ( 1 ‚àí Œ≤ ) h t = Œ≤ ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ ( Œ± t ‚àí Œ± t ‚àí 1 ) m t ‚àí 1 ‚àí Œ± t ( 1 ‚àí Œ≤ ) h t ( 42 ) Since we also have Œ∏ t + 1 ‚àí Œ∏ t = ( 1 ‚àí Œ≤ ) Œ∏ t + 1 + Œ≤ ( Œ∏ t + 1 ‚àí Œ∏ t ) ‚àí ( 1 ‚àí Œ≤ ) Œ∏ t Combined with ( 42 ) , we have ( 1 ‚àí Œ≤ ) Œ∏ t + 1 + Œ≤ ( Œ∏ t + 1 ‚àí Œ∏ t ) = ( 1 ‚àí Œ≤ ) Œ∏ t + Œ≤ ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ ( Œ± t ‚àí Œ± t ‚àí 1 ) m t ‚àí 1 ‚àí Œ± t ( 1 ‚àí Œ≤ ) h t Divide both sides by 1 ‚àí Œ≤ Œ∏ t + 1 + Œ≤ 1 ‚àí Œ≤ ( Œ∏ t + 1 ‚àí Œ∏ t ) = Œ∏ t + Œ≤ 1 ‚àí Œ≤ ( Œ∏ t ‚àí Œ∏ t ‚àí 1 ) ‚àí Œ≤ 1 ‚àí Œ≤ ( Œ± t ‚àí Œ± t ‚àí 1 ) m t ‚àí 1 ‚àí Œ± t h t Lemma C . 19 . Suppose that the conditions in Theorem C . 4 hold , then E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] ‚â§ 4 (cid:88) i = 1 T i , 29 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where T 1 = ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , Œ≤ 1 1 ‚àí Œ≤ 1 ( Œ± i ‚àí Œ± i ‚àí 1 ) m i ‚àí 1 (cid:105) (cid:35) T 2 = ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) , h i (cid:105) (cid:35) T 3 = E (cid:34) t (cid:88) i = 1 L (cid:13)(cid:13)(cid:13)(cid:13) Œ≤ 1 ‚àí Œ≤ ( Œ± i ‚àí Œ± i ‚àí 1 ) m i ‚àí 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:35) T 4 = E (cid:34) t (cid:88) i = 1 L (cid:107) Œ± i h i (cid:107) 2 (cid:35) Proof . By the Lipschitz smoothness of ‚àá f , f ( z t + 1 ) ‚â§ f ( z t ) + (cid:104)‚àá f ( z t ) , z t + 1 ‚àí z t (cid:105) + L 2 (cid:107) z t + 1 ‚àí z t (cid:107) 2 , Based on ( C . 18 ) , we have E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] = E (cid:34) t (cid:88) i = 1 f ( z i + 1 ) ‚àí f ( z i ) (cid:35) ‚â§ E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , z i + 1 ‚àí z i (cid:105) + L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:35) = ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) , Œ≤ 1 ‚àí Œ≤ ( Œ± i ‚àí Œ± i ‚àí 1 ) m i ‚àí 1 (cid:105) (cid:35) ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( z i ) , h i (cid:105) (cid:35) + E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:35) Then , using inequality (cid:107) a + b (cid:107) 2 ‚â§ 2 (cid:107) a (cid:107) 2 + 2 (cid:107) b (cid:107) 2 and combined with lemma C . 18 , E (cid:34) t (cid:88) i = 1 L 2 (cid:107) z i + 1 ‚àí z i (cid:107) 2 (cid:35) ‚â§ T 3 + T 4 Lemma C . 20 . In this part , we bound T 1 , T 2 , T 3 , T 4 . We claim that the order of them is O ( log T ) . Proof . ‚Ä¢ Bound for T 1 T 1 ‚â§ E (cid:34) t (cid:88) i = 1 (cid:107)‚àá f ( z i ) (cid:107)(cid:107) m i ‚àí 1 (cid:107) Œ≤ 1 ‚àí Œ≤ | Œ± i ‚àí Œ± i ‚àí 1 | (cid:35) ‚â§ H 2 Œ≤ 1 ‚àí Œ≤ E (cid:34) t (cid:88) i = 1 | Œ± i ‚àí Œ± i ‚àí 1 | (cid:35) ‚â§ H 2 Œ≤ 1 ‚àí Œ≤ Œ± 30 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where the second and last inequality is due to the monotone decreasing property of Œ± i ‚Ä¢ Bound for T 3 T 3 ‚â§ (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 LH 2 E (cid:34) t (cid:88) i = 1 ( Œ± i ‚àí Œ± i ‚àí 1 ) 2 (cid:35) ‚â§ 2 Œ± (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 LH 2 E (cid:34) t (cid:88) i = 1 | Œ± i ‚àí Œ± i ‚àí 1 | (cid:35) ‚â§ 2 Œ± 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 LH 2 where the monotone decreasing property of Œ± i is also used ‚Ä¢ Bound for T 4 T 4 ‚â§ H 2 LŒ± 2 E (cid:34) t (cid:88) i = 1 1 t (cid:35) ‚â§ H 2 LŒ± 2 ( 1 + log T ) where the second inequality is due to (cid:80) ti = 1 1 t ‚â§ 1 + log T ‚Ä¢ Bound for T 2 T 2 = ‚àí E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , h i (cid:105) (cid:35) ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) , h i (cid:105) (cid:35) ( 43 ) The second term of ( 43 ) can be bounded as ‚àí E (cid:34) t (cid:88) i = 1 (cid:104)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) , h i (cid:105) (cid:35) ‚â§ E (cid:34) t (cid:88) i = 1 1 2 (cid:107)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) (cid:107) 2 + 1 2 (cid:107) Œ± i h i (cid:107) 2 (cid:35) ‚â§ L 2 2 E (cid:34) t (cid:88) i = 1 (cid:107) Œ≤ 1 ‚àí Œ≤ Œ± i ‚àí 1 m i ‚àí 1 (cid:107) 2 (cid:35) + 1 2 E (cid:34) t (cid:88) i = 1 (cid:107) Œ± i h i (cid:107) 2 (cid:35) ‚â§ Œ± 2 H 2 L 2 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 t (cid:88) i = 1 1 t + Œ± 2 H 2 2 t (cid:88) i = 1 1 t ‚â§ Œ± 2 H 2 2 (cid:34) L 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 + 1 (cid:35) ( 1 + log T ) where the second inequality is due to (cid:107)‚àá f ( z i ) ‚àí ‚àá f ( Œ∏ i ) (cid:107) ‚â§ L (cid:107) z i ‚àí Œ∏ i (cid:107) . 31 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Then , consider the Ô¨Årst term of ( 43 ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , h i (cid:105) (cid:35) = E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , Œ∫g i + ¬µI i (cid:105) (cid:35) ‚âà Œ∫E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) + Œ¥ i (cid:105) (cid:35) + ¬µE (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) + Œ¥ i (cid:105) (cid:35) = ( Œ∫ + ¬µ ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) (cid:105) (cid:35) The second and third equality holds for the follow reasons : on the one hand , g t = ‚àá f ( Œ∏ t ) + Œ¥ t in which E [ Œ¥ t ] = 0 , so according to ( Chen et al . , 2018 ) , given Œ∏ i , E [ Œ¥ i | Œ∏ i ] = 0 ; On the other hand , suppose the optimizer runs for a long time , the bias of EMA is small ( Zhuang et al . , 2020 ) , thus E ( I t ) approaches E ( g t ) as step increases . Finally , we can Ô¨Ånally bound T 2 T 2 ‚â§ Œ± 2 H 2 2 (cid:34) L 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 + 1 (cid:35) ( 1 + log T ) + ( Œ∫ + ¬µ ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) (cid:105) (cid:35) Theorem C . 21 . ( Convergence of A DMETA S in non - convex stochastic optimization ) Under the assumptions : ‚Ä¢ ‚àá f exits and is Lipschitz - continuous , i . e , | | ‚àá f ( x ) ‚àí ‚àá f ( y ) | | ‚â§ L | | x ‚àí y | | , ‚àÄ x , y ; f is also lower bounded . ‚Ä¢ At step t , the algorithm can access a bounded noisy gradient g t , and the true gradient ‚àá f is also bounded . ‚Ä¢ The noisy gradient is unbiased , and has independent noise , i . e . g t = ‚àá f ( Œ∏ t ) + Œ¥ t , E [ Œ¥ t ] = 0 and Œ¥ i ‚ä• Œ¥ j , ‚àÄ i (cid:54) = j . And Œ± t = Œ± / ‚àö t , then for any T we have : min t ‚àà [ T ] E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ‚â§ 1 ‚àö T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where Q (cid:48) 1 and Q (cid:48) 2 are constants independent of T . Proof . We combine lemma C . 18 , lemma C . 19 and lemma C . 20 to bound the overall expected descent of the objective . First , we have E [ f ( z t + 1 ) ‚àí f ( z 1 ) ] ‚â§ T 1 + T 2 + T 3 + T 4 ( 44 ) ‚â§ H 2 Œ≤ 1 ‚àí Œ≤ Œ± + Œ± 2 H 2 2 (cid:34) L 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 + 1 (cid:35) ( 1 + log T ) ( 45 ) ‚àí ( Œ∫ + ¬µ ) E (cid:34) t (cid:88) i = 1 Œ± i (cid:104)‚àá f ( Œ∏ i ) , ‚àá f ( Œ∏ i ) (cid:105) (cid:35) ( 46 ) + 2 Œ± 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 LH 2 + H 2 LŒ± 2 ( 1 + log T ) ( 47 ) Notice that E (cid:34) T (cid:88) t = 1 Œ± i (cid:104)‚àá f ( Œ∏ t ) , ‚àá f ( Œ∏ t ) (cid:105) (cid:35) ‚â• E (cid:34) T (cid:88) t = 1 1 ‚àö t (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:35) ‚â• ‚àö T min t ‚àà [ T ] E (cid:2) (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:3) ( 48 ) 32 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Rearrange ( 44 ) , combined with ( 48 ) and notice that Œ∫ + ¬µ > 0 , we have min t ‚àà [ T ] E (cid:2) (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 (cid:3) ‚â§ 1 ‚àö T E (cid:34) T (cid:88) t = 1 Œ± i (cid:104)‚àá f ( Œ∏ t ) , ‚àá f ( Œ∏ t ) (cid:105) (cid:35) ‚â§ 1 ‚àö T (cid:34) 1 Œ∫ + ¬µ (cid:32) Œ± 2 H 2 L 2 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 + Œ± 2 H 2 2 + H 2 LŒ± 2 (cid:33) ( 1 + log T ) + 1 Œ∫ + ¬µ (cid:32) H 2 Œ≤ 1 ‚àí Œ≤ Œ± + 2 Œ± 2 (cid:18) Œ≤ 1 ‚àí Œ≤ (cid:19) 2 LH 2 + E [ f ( z 1 ) ‚àí f ( z ‚àó ) ] (cid:33) (cid:35) = 1 ‚àö T ( Q (cid:48) 1 + Q (cid:48) 2 log T ) where z ‚àó is the optimal of f , i . e . z ‚àó = arg min z f ( z ) This completes the proof . C . 6 . Convergence Analysis of Forward - looking In this section , based on ( Wang et al . , 2020 ) , we further analysis forward - looking part to complete the convergence proof of A DMETA optimizer . According to ( Zhang et al . , 2019 ) , Lookahead is an algorithm that can be combined with any standard optimization method . The same is true for dynamic lookahead method in forward - looking part . What‚Äôs more , optimizers with forward - looking is essentially processing with two loops as discussed in the main text . The fast weight is updated by optimizers , while the slow weight is updated by interpolating with fast weight every given period . In other words , the slow weight is updated passively . Therefore , though the slow weight is relevant to optimizers , it is almost irrelevant to the selection of optimizers . For this reason , we only prove the convergence of forward - looking of A DMETA S , which can be easily extended to the A DMETA R . Remarks : ( some preliminaries ) Based on the design of the asymptotic dynamic weight Œ∑ t of the forward - looking part , it can be concluded that when it runs for a long time , Œ∑ t is highly close to the set point , at which we can safely assume that Œ∑ t is a constant and thus we denote it as Œ∑ . In this way , the analysis of a dynamic lookahead is the same as the case of static lookahead . According to algorithm of A DMETA , the slow weight œÜ t updates every k steps . We can assume that the slow weight is trained in sync with fast weight . For this purpose , all we should do is to stipulate œÜ œÑk + l = œÜ œÑk , where k denotes the synchronization period , œÑ ‚àà N ‚àó and 0 ‚â§ l < k . DeÔ¨Åne y t = Œ∑Œ∏ t + ( 1 ‚àí Œ∑ ) Œ∏ t , then according to the update of Œ∏ t and œÜ t , we have y t + 1 = y t ‚àí Œ∑Œ± t m t and on each period of synchronization , we have y œÑk ‚àí Œ∏ œÑk = ( 1 ‚àí Œ∑ ) ( œÜ œÑk ‚àí Œ∏ œÑk ) = 0 y œÑk ‚àí œÜ œÑk = Œ∑ ( Œ∏ œÑk ‚àí œÜ œÑk ) = 0 Theorem C . 22 . ( convergence of forward - looking part ) Suppose f ( . ) is L - smooth , i . e , | | ‚àá f ( x ) ‚àí ‚àá f ( y ) | | ‚â§ L | | x ‚àí y | | , ‚àÄ x , y . The bias of noisy gradient is bounded , i . e . , | Œ¥ t | ‚â§ œÉ , where Œ¥ t = ‚àá f ( Œ∏ t ) ‚àí g t . Then we have that : 1 T T (cid:88) t = 0 E (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ‚àá f ( Œ∏ t ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 ‚â§ O ( 1 ‚àö T ) Proof . Following the L - smooth property , we have f ( y t + 1 ) ‚àí f ( y t ) ‚â§ ‚àí Œ∑Œ± t (cid:104)‚àá f ( y t ) , m t (cid:105) + Œ∑ 2 Œ± 2 t L 2 (cid:107) m t (cid:107) 2 ( 49 ) 33 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Taking the expectation of both sides , E [ (cid:104)‚àá f ( y t ) , m t (cid:105) ] = E [ (cid:104)‚àá f ( y t ) , Œ∫g t + ¬µI t (cid:105) ] = Œ∫ E [ (cid:104)‚àá f ( y t ) , g t (cid:105) ] + ¬µ E [ (cid:104)‚àá f ( y t ) , I t (cid:105) ] ( 50 ) Consider the term with Œ∫ , E [ (cid:104)‚àá f ( y t ) , g t (cid:105) ] = (cid:104)‚àá f ( y t ) , ‚àá f ( Œ∏ t ) (cid:105) = 1 2 [ (cid:107)‚àá f ( y t ) (cid:107) 2 + (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 ‚àí (cid:107)‚àá f ( y t ) ‚àí ‚àá f ( Œ∏ t ) (cid:107) 2 ] ‚â• 1 2 [ (cid:107)‚àá f ( y t ) (cid:107) 2 + (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 ‚àí L 2 (cid:107) y t ‚àí Œ∏ t (cid:107) 2 ] = 1 2 [ (cid:107)‚àá f ( y t ) (cid:107) 2 + (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 ‚àí ( 1 ‚àí Œ∑ ) 2 L 2 (cid:107) œÜ t ‚àí Œ∏ t (cid:107) 2 ] ( 51 ) Suppose the optimizer runs for a long time , the bias of EMA is small enough , thus E ( I t ) approaches E ( g t ) . For this reason , we can estimate the term with ¬µ in ( 50 ) the same way as ( 51 ) . Based on the bounded bias gradient assumption and inequality that ( a + b ) 2 ‚â§ 2 a 2 + 2 b 2 , we have : E [ (cid:107) m t (cid:107) 2 ] ‚â§ 2 ¬µ 2 E [ (cid:107) I t (cid:107) 2 ] (cid:107) + 2 Œ∫ 2 E [ (cid:107) g t (cid:107) 2 ] (cid:107) ‚â§ 4 ( ¬µ 2 + Œ∫ 2 ) E (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 + 4 ( ¬µ 2 + Œ∫ 2 ) œÉ 2 ( 52 ) Combined with ( 49 ) , ( 50 ) , ( 51 ) and ( 52 ) , rearrange the inequality and take the expectation E [ f ( y t + 1 ) ] ‚â§ E [ f ( y t ) ] ‚àí Œ∑Œ± t ( ¬µ + Œ∫ ) 2 E [ (cid:107)‚àá f ( y t ) (cid:107) 2 ] ‚àí Œ∑Œ± t ( ¬µ + Œ∫ ) 2 E [ (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 ] + Œ∑Œ± t ( 1 ‚àí Œ∑ ) 2 L 2 ( ¬µ + Œ∫ ) 2 E [ (cid:107) œÜ t ‚àí Œ∏ t (cid:107) 2 ] + 2 ( ¬µ 2 + Œ∫ 2 ) Œ∑ 2 Œ± 2 t L E [ (cid:107)‚àá f ( Œ∏ t ) (cid:107) 2 ] + 2 ( ¬µ 2 + Œ∫ 2 ) Œ∑ 2 Œ± 2 t LœÉ 2 Since the learning rate is decreasing to zero , we can safely assume that after several iterations , 1 ‚àí Œ∑Œ± t L > 0 . Then , summing over one outer loop E [ f ( y ( œÑ + 1 ) k ) ] ‚àí E [ f ( y œÑk ) ] ‚â§ ‚àí Œ∑Œ± ( œÑ + 1 ) k ( ¬µ + Œ∫ ) 2 k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( y œÑk + l ) (cid:107) 2 ] + 2 ( ¬µ 2 + Œ∫ 2 ) kŒ∑ 2 Œ± 2 œÑk LœÉ 2 ‚àí Œ∑Œ± ( œÑ + 1 ) k ( ¬µ + Œ∫ ‚àí 4 ( ¬µ 2 + Œ∫ 2 ) Œ∑Œ± ( œÑ + 1 ) k L ) 2 k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + l ) (cid:107) 2 ] + Œ∑Œ± œÑk ( 1 ‚àí Œ∑ ) 2 L 2 ( ¬µ + Œ∫ ) 2 k ‚àí 1 (cid:88) l = 0 E [ (cid:107) œÜ œÑk + l ‚àí Œ∏ œÑk + l (cid:107) 2 ] ( 53 ) 34 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Consider the last term of ( 53 ) , we have E [ (cid:107) œÜ œÑk + l ‚àí Œ∏ œÑk + l (cid:107) 2 ] = E [ (cid:107) Œ∏ œÑk ‚àí Œ∏ œÑk + l (cid:107) 2 ] ‚â§ Œ± 2 œÑk E Ô£Æ Ô£∞ (cid:107) l ‚àí 1 (cid:88) j = 0 m œÑk + j (cid:107) 2 Ô£π Ô£ª = 2 Œ∫ 2 Œ± 2 œÑk E Ô£Æ Ô£∞ (cid:107) l ‚àí 1 (cid:88) j = 0 g œÑk + j (cid:107) 2 Ô£π Ô£ª + 2 ¬µ 2 Œ± 2 œÑk E Ô£Æ Ô£∞ (cid:107) l ‚àí 1 (cid:88) j = 0 I œÑk + j (cid:107) 2 Ô£π Ô£ª ‚â§ 4 Œ∫ 2 Œ± 2 œÑk E Ô£Æ Ô£ØÔ£∞ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l ‚àí 1 (cid:88) j = 0 ( g œÑk + j ‚àí ‚àá f ( Œ∏ œÑk + j ) ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£∫Ô£ª + 4 Œ∫ 2 Œ± 2 œÑk E Ô£Æ Ô£ØÔ£∞ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l ‚àí 1 (cid:88) j = 0 ‚àá f ( Œ∏ œÑk + j ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£∫Ô£ª + 4 ¬µ 2 Œ± 2 œÑk E Ô£Æ Ô£ØÔ£∞ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l ‚àí 1 (cid:88) j = 0 ( I œÑk + j ‚àí ‚àá f ( Œ∏ œÑk + j ) ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£∫Ô£ª + 4 ¬µ 2 Œ± 2 œÑk E Ô£Æ Ô£ØÔ£∞ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) l ‚àí 1 (cid:88) j = 0 ‚àá f ( Œ∏ œÑk + j ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 Ô£π Ô£∫Ô£ª ‚â§ 4 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 lŒ± 2 œÑk + 4 ( ¬µ 2 + Œ∫ 2 ) Œ± 2 œÑk E Ô£Æ Ô£ØÔ£∞ (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) l ‚àí 1 (cid:88) j = 0 ‚àá f ( Œ∏ œÑk + j ) (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13) 2 Ô£π Ô£∫Ô£ª ‚â§ 4 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 lŒ± 2 œÑk + 4 ( ¬µ 2 + Œ∫ 2 ) lŒ± 2 œÑk l ‚àí 1 (cid:88) j = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + j ) (cid:107) 2 ] where the Ô¨Årst equality using the property that Œ∏ œÑk = œÜ œÑk = œÜ œÑk + l . Summing from l = 0 to l = k ‚àí 1 , we get , k ‚àí 1 (cid:88) l = 0 E [ (cid:107) œÜ œÑk + l ‚àí Œ∏ œÑk + l (cid:107) 2 ] ‚â§ 2 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 Œ± 2 œÑk k ( k ‚àí 1 ) + 4 ( ¬µ 2 + Œ∫ 2 ) Œ± 2 œÑk k ‚àí 1 (cid:88) l = 0 l l ‚àí 1 (cid:88) j = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + j ) (cid:107) 2 ] = 2 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 Œ± 2 œÑk k ( k ‚àí 1 ) + 4 ( ¬µ 2 + Œ∫ 2 ) Œ± 2 œÑk k ‚àí 2 (cid:88) j = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + j ) (cid:107) 2 ] k ‚àí 1 (cid:88) l = j + 1 l = 2 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 Œ± 2 œÑk k ( k ‚àí 1 ) + 2 ( ¬µ 2 + Œ∫ 2 ) Œ± 2 œÑk k ‚àí 2 (cid:88) j = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + j ) (cid:107) 2 ] ( j + k ) ( k ‚àí j ‚àí 1 ) ( j + k ) ( k ‚àí j ‚àí 1 ) achieves its maximal value when j = 0 . Therefore , we have k ‚àí 1 (cid:88) l = 0 E [ (cid:107) œÜ œÑk + l ‚àí Œ∏ œÑk + l (cid:107) 2 ] ‚â§ 2 ( Œ∫ 2 + ¬µ 2 ) œÉ 2 Œ± 2 œÑk k ( k ‚àí 1 ) + 2 ( ¬µ 2 + Œ∫ 2 ) Œ± 2 œÑk k ( k ‚àí 1 ) k ‚àí 2 (cid:88) j = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + j ) (cid:107) 2 ] Here , we can Ô¨Ånally bound the the last term of ( 53 ) E [ f ( y ( œÑ + 1 ) k ) ] ‚àí E [ f ( y œÑk ) ] ‚â§ ‚àí Œ∑Œ± ( œÑ + 1 ) k ( ¬µ + Œ∫ ) 2 k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( y œÑk + l ) (cid:107) 2 ] + G + M k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( Œ∏ œÑk + l ) (cid:107) 2 ] ‚â§ ‚àí Œ∑Œ± ( œÑ + 1 ) k ( ¬µ + Œ∫ ) 2 k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( y œÑk + l ) (cid:107) 2 ] + G ( 54 ) 35 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers where G = 2 ( ¬µ 2 + Œ∫ 2 ) kŒ∑ 2 Œ± 2 œÑk LœÉ 2 + ( Œ∫ 2 + ¬µ 2 ) ( Œ∫ + ¬µ ) Œ∑ ( 1 ‚àí Œ∑ ) 2 L 2 œÉ 2 k ( k ‚àí 1 ) Œ± 3 œÑk and M = ‚àí Œ∑Œ± ( œÑ + 1 ) k ( ¬µ + Œ∫ ‚àí 4 ( ¬µ 2 + Œ∫ 2 ) Œ∑Œ± ( œÑ + 1 ) k L ) 2 + ( Œ∫ 2 + ¬µ 2 ) ( Œ∫ + ¬µ ) Œ∑ ( 1 ‚àí Œ∑ ) 2 L 2 œÉ 2 k ( k ‚àí 1 ) Œ± 3 œÑk When Œ± is small enough , M is below zero , for which the second inequality of ( 54 ) holds . Summing from œÑ = 0 to œÑ = Œ• ‚àí 1 , we get E [ f ( y Œ• k ) ] ‚àí E [ f ( y 0 ) ] ‚â§ ‚àí Œ∑ ( ¬µ + Œ∫ ) 2 Œ• ‚àí 1 (cid:88) œÑ = 0 Œ± ( œÑ + 1 ) k k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( y œÑk + l ) (cid:107) 2 ] + 2 ( ¬µ 2 + Œ∫ 2 ) kŒ∑ 2 LœÉ 2 Œ• ‚àí 1 (cid:88) œÑ = 0 Œ± 2 œÑk + ( Œ∫ 2 + ¬µ 2 ) ( Œ∫ + ¬µ ) Œ∑ ( 1 ‚àí Œ∑ ) 2 L 2 œÉ 2 k ( k ‚àí 1 ) Œ• ‚àí 1 (cid:88) œÑ = 0 Œ± 3 œÑk Following ( Wang et al . , 2020 ) , we Ô¨Årst assume the learning rate Œ± as a Ô¨Åxed constant , then rearrange the inequality above , we get 1 Œ• k Œ• ‚àí 1 (cid:88) œÑ = 0 k ‚àí 1 (cid:88) l = 0 E [ (cid:107)‚àá f ( y œÑk + l ) (cid:107) 2 ] ‚â§ 2 [ f ( y 0 ) ‚àí f inf ] Œ∑Œ± Œ• k ( ¬µ + Œ∫ ) + 4 ( ¬µ 2 + Œ∫ 2 ) Œ∑Œ±LœÉ 2 ¬µ + Œ∫ + 2 ( Œ∫ 2 + ¬µ 2 ) ( 1 ‚àí Œ∑ ) 2 Œ± 2 L 2 œÉ 2 ( k ‚àí 1 ) DeÔ¨Åne T as Œ• k and set the learning rate Œ± to 1 / ‚àö T 1 T T ‚àí 1 (cid:88) t = 0 E [ (cid:107)‚àá f ( y t ) (cid:107) 2 ] ‚â§ 2 [ f ( y 0 ) ‚àí f inf ] Œ∑ ‚àö T ( ¬µ + Œ∫ ) + 4 ( ¬µ 2 + Œ∫ 2 ) Œ∑LœÉ 2 ( ¬µ + Œ∫ ) ‚àö T + 2 ( Œ∫ 2 + ¬µ 2 ) ( 1 ‚àí Œ∑ ) 2 L 2 œÉ 2 ( k ‚àí 1 ) T = O ( 1 ‚àö T ) D . Analysis of Convergence Rate For convex situation , we adopt the regret function to estimate the convergence rate . And for non - convex situation , we adopt the minimum of the expectation of the squared gradient to estimate the convergence , which are corresponding to the proof of convergence since the process of the convergence proof is actually the process of Ô¨Ånding the convergence rate . From Table 6 , we notice that the convergence rates for all optimizers for convex case are of magnitude of O ( 1 / ‚àö T ) and for non - convex are of O ( log T / ‚àö T ) , which means in essence , algorithms based on gradient decent follow a similar rate constraint . However , the convergence speed of different optimizers may attribute to many other factors , such as on the implementation . Therefore additional statistical experiments are needed for analysis , as we did in Table 4 . E . Experimental Details E . 1 . Hyperparameter Tuning For A DMETA optimizer , we Ô¨Årst determined a rough value range for learning rate and lambda with the toy model according to the visualization as in Figure B . While for other baseline optimizers , we refer to the recommended / default hyperparameter settings in the original paper . In this way , we get the rouge range of the hyperparameter in optimizers . Then , we search the hyperparameters in the adjacent interval , which is listed in the following three subsections . 36 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Case Optim Source Convergence rate ( a rough estimation ) Convex SGD ( Zinkevich , 2003 ) D 2 ‚àû 2 Œ± T + G 2 ‚àû 2 (cid:80) Tt = 1 Œ± t AMSGrad ( Reddi et al . , 2019 ) D 2 ‚àû ‚àö T Œ± ( 1 ‚àí Œ≤ 1 ) (cid:80) di = 1 ÀÜ v 1 T , i / 2 + D ‚àû 2 ( 1 ‚àí Œ≤ 1 ) (cid:80) Tt = 1 (cid:80) di = 1 Œ≤ ÀÜ v 1 t , i / 2 Œ± t + Œ± ‚àö 1 + log T ( 1 ‚àí Œ≤ 1 ) 2 ( 1 ‚àí Œ≥ ) ‚àö 1 ‚àí Œ≤ 2 (cid:80) di = 1 (cid:107) g 1 : T , i (cid:107) A DMETA S - (cid:16) Œ≤ 1 ‚àí Œ≤ + 2 (cid:17) dD ‚àû 4 Œ± T + (cid:16) 2 Œ±Œ≤ 1 ‚àí Œ≤ + Œ± (cid:17) dG 2 ‚àû ‚àö T A DMETA R - ( 2 ‚àí Œ≤ 1 ) D 2 ‚àû ‚àö T 4 Œ± ( 1 ‚àí Œ≤ 1 ) (cid:80) di = 1 ÀÜ v 1 / 2 T , i + ( 2 + Œ≤ 1 ) Œ± ‚àö 1 + log T 2 ‚àö ( 1 ‚àí Œ≤ 2 ) ( 1 ‚àí Œ≥ ) (cid:80) di = 1 (cid:107) h 1 : T , i (cid:107) 2 Non - convex SGD - - AMSGrad ( Chen et al . , 2018 ) H ‚àö T (cid:16) C 1 H 2 c 2 ( 1 + log T ) + C 2 dc + C 3 dc 2 + C 4 (cid:17) A DMETA S - 1 ‚àö T (cid:34) 1 Œ∫ + ¬µ (cid:18) Œ± 2 H 2 L 2 2 (cid:16) Œ≤ 1 ‚àí Œ≤ (cid:17) 2 + Œ± 2 H 2 2 + H 2 LŒ± 2 (cid:19) ( 1 + log T ) + 1 Œ∫ + ¬µ (cid:18) H 2 Œ≤ 1 ‚àí Œ≤ Œ± + 2 Œ± 2 (cid:16) Œ≤ 1 ‚àí Œ≤ (cid:17) 2 LH 2 + E [ f ( z 1 ) ‚àí f ( z ‚àó ) ] (cid:19) (cid:35) A DMETA R - H ‚àö T (cid:16) ( K 1 + K 2 ) H 2 Œ± 2 c 2 ( 1 + log T ) + K 3 dŒ±c + K 4 dŒ± 2 c 2 + K 5 (cid:17) Table 6 . The comparison of convergence rate of several optimizers . Model task SGD SGDM Adam RAdam Ranger AdaBelief A DMETA R A DMETA S LR LR LR LR LR LR LR Œª LR Œ≤ ResNet - 110 CIFAR - 10 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 001 0 . 05 0 . 1 0 . 05 0 . 2 CIFAR - 100 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 01 0 . 05 0 . 05 0 . 05 0 . 1 PyramidNet CIFAR - 10 0 . 1 0 . 1 0 . 001 0 . 01 0 . 01 0 . 001 0 . 01 0 . 1 0 . 05 0 . 4 CIFAR - 100 0 . 5 0 . 5 0 . 001 0 . 01 0 . 01 0 . 001 0 . 01 0 . 1 0 . 05 0 . 1 Table 7 . Optimizer hyperparameter settings on the CIFAR task . E . 2 . Image ClassiÔ¨Åcation We conduct image classiÔ¨Åcation experiments on CIFAR - 10 and CIFAR - 100 datasets , which are trained on a single NVIDIA RTX - 3090 GPU . Typical architectures like ResNet - 110 and PyramidNet are employed as the baseline models . In the ResNet - 110 architecture , there are 54 stacked identical 3 √ó 3 convolutional layers with 54 two - layer Residual Units ( He et al . , 2016 ) . While in the PyramidNet architecture , there are 110 layers with a widening factor of 48 ( Han et al . , 2017 ) . We set the training batch size to 128 and the validation batch size to 256 . Both models are trained with 160 epochs . Milestone schedule is adopted as the learning rate decay strategy , with learning rate decaying at the end of 80 - th and 120 - th epochs by 0 . 1 . We report the hyperparameters tuning for our proposed A DMETA and other optimizers for reproduction of our experiments . For all optimizers , the weight decay is Ô¨Åxed as 1 e ‚àí 4 . The searching scheme of hyperparameter settings for each optimizer is concluded as follows : ‚Ä¢ For SGD and SGDM , the momentum is Ô¨Åxed as 0 . 9 , and the best - performing learning rate is searched from { 0 . 01 , 0 . 05 , 0 . 1 } and recommended values in original paper . For our A DMETA S , the Œª is set to Ô¨Åxed 0 . 9 and we search the best - performing Œ≤ from { 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 } and learning rate from { 0 . 01 , 0 . 05 , 0 . 1 } . ‚Ä¢ For all adaptive learning rate optimizers , hyperparameters Œ≤ 1 , Œ≤ 2 and (cid:15) are set to Œ≤ 1 = 0 . 9 , Œ≤ 2 = 0 . 999 and (cid:15) = 1e - 9 respectively . For Adam , RAdam and AdaBelief optimizer , the learning rate is searched from { 0 . 1 , 0 . 01 , 0 . 001 } . For Ranger , Œ∑ and k are set to Œ∑ = 0 . 5 and k = 6 according to ( Wright , 2019 ) . The learning rate is searched from { 0 . 1 , 0 . 01 , 0 . 001 } . And for our A DMETA R , the setting of k is the same as Ranger , and we search Œª from { 0 . 05 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 } and learning rate from { 0 . 1 , 0 . 05 , 0 . 01 } . The resulting hyperparameters reported in the paper are shown in Table 7 , where LR is the abbreviation of learning rate . 37 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Model Optim MNLI QQP QNLI SST - 2 CoLA STS - B MRPC RTE LR Œª LR Œª LR Œª LR Œª LR Œª LR Œª LR Œª LR Œª BERT base AdamW 2e - 5 ‚àí 3e - 5 ‚àí 3e - 5 ‚àí 2e - 5 ‚àí 5e - 5 ‚àí 5e - 5 ‚àí 4e - 5 ‚àí 6e - 5 ‚àí RAdam 2e - 5 ‚àí 2e - 5 ‚àí 6e - 5 ‚àí 4e - 5 ‚àí 1e - 4 ‚àí 4e - 4 ‚àí 1 . 5e - 4 ‚àí 5e - 4 ‚àí Ranger 5e - 5 ‚àí 5e - 5 ‚àí 1e - 4 ‚àí 8e - 5 ‚àí 2e - 4 ‚àí 5e - 4 ‚àí 4e - 4 ‚àí 1e - 3 ‚àí AdaBelief 5e - 4 ‚àí 5e - 4 ‚àí 5e - 4 ‚àí 8e - 4 ‚àí 4e - 4 ‚àí 6e - 4 ‚àí 5e - 4 ‚àí 6e - 4 ‚àí A DMETA R 1 . 5e - 4 0 . 08 1e - 4 0 . 36 2e - 4 0 . 03 1e - 4 0 . 03 7e - 4 0 . 02 1e - 3 0 . 08 1 . 2e - 3 0 . 3 1 . 8e - 3 0 . 36 BERT large AdamW 2e - 5 ‚àí 2e - 5 ‚àí 2e - 5 ‚àí 2e - 5 ‚àí 6e - 5 ‚àí 5e - 5 ‚àí 4e - 5 ‚àí 2e - 5 ‚àí RAdam 2e - 5 ‚àí 2e - 5 ‚àí 5e - 5 ‚àí 4e - 5 ‚àí 1e - 4 ‚àí 2e - 4 ‚àí 8e - 5 ‚àí 5e - 4 ‚àí Ranger 5e - 5 ‚àí 5e - 5 ‚àí 5e - 5 ‚àí 6e - 5 ‚àí 6e - 5 ‚àí 5e - 4 ‚àí 5e - 4 ‚àí 5e - 4 ‚àí AdaBelief 2e - 4 ‚àí 4e - 4 ‚àí 5e - 4 ‚àí 2e - 4 ‚àí 6e - 4 ‚àí 2e - 4 ‚àí 4e - 4 ‚àí 8e - 4 ‚àí A DMETA R 1 . 5e - 4 0 . 08 8e - 5 0 . 2 8e - 5 0 . 03 9e - 5 0 . 3 7e - 4 0 . 02 1e - 3 0 . 03 6e - 4 0 . 08 8e - 4 0 . 1 Table 8 . Optimizer hyperparameter settings on the GLUE benchmark . Model Optim SQuAD v1 . 1 SQuAD v2 . 0 NER - CoNLL03 LR Œª LR Œª LR Œª BERT base AdamW 5e - 5 ‚àí 5e - 5 ‚àí 6e - 5 ‚àí RAdam 1e - 4 ‚àí 5e - 5 ‚àí 5e - 5 ‚àí Ranger 1e - 4 ‚àí 8e - 5 ‚àí 1e - 4 ‚àí AdaBelief 1e - 3 ‚àí 8e - 4 ‚àí 5e - 4 ‚àí A DMETA R 4e - 4 0 . 05 3e - 4 0 . 2 2e - 4 0 . 3 BERT large AdamW 2e - 5 ‚àí 5e - 5 ‚àí 2e - 5 ‚àí RAdam 6e - 5 ‚àí 5e - 5 ‚àí 3e - 5 ‚àí Ranger 1e - 4 ‚àí 8e - 5 ‚àí 5e - 5 ‚àí AdaBelief 8e - 4 ‚àí 8e - 4 ‚àí 4e - 4 ‚àí A DMETA R 4e - 4 0 . 05 3e - 4 0 . 2 1 . 5e - 4 0 . 2 Table 9 . Hyperparameter settings of SQuAD v1 . 1 and v2 . 0 development sets . E . 3 . Natural Language Understanding In the NLU experiments , we employ a pre - trained language model BERT ( Devlin et al . , 2018 ; Zhang et al . , 2020 ) as our backbone . There are two model sizes for BERT : BERT base and BERT large , where the base model size has 12 Transformer layers with 768 hidden size , 12 self - attention heads and 110M model parameters and the large model size has 24 Transformer layers with 1024 hidden size , 16 self - attention heads and 340M parameters ( Li et al . , 2022 ) . In natural language understanding , we perform experiments on three modeling types of tasks : text classiÔ¨Åcation , machine reading comprehension and token classiÔ¨Åcation . The text classiÔ¨Åcation uses the GLUE benchmark as the evaluation data set , the machine reading comprehension uses SQuAD v1 . 1 and v2 . 0 , and the token classiÔ¨Åcation uses the NER - CoNLL03 named entity recognition data set ( Zhou et al . , 2020b ) . We train the eight tasks in GLUE benchmark for 3 epochs on a single NVIDIA RTX - 3090 GPU , except for MRPC , which is trained for 5 epochs due to its relatively small data size ( Li et al . , 2020b ) . The maximum sequence length is set to 128 and the training batch size is set to 32 . SQuAD v1 . 1 and SQuAD v2 . 0 are trained for 2 epochs with two GPUs . The maximum sequence length is set to 384 and the training batch size per device is set to 12 . And NER - CoNLL03 is trained for 3 epochs on a single GPU . The training batch size per device is set to 8 . Because of the pre - training - Ô¨Åne - tuning paradigm , we only employ the adaptive learning rate optimizer . We set Œ≤ 1 , Œ≤ 2 , (cid:15) and weight decay of these optimizers to 0 . 9 , 0 . 999 , 1e - 8 and 0 . 0 respectively . Œ∑ and k are set to 0 . 5 and 6 in the Ranger optimizer and A DMETA uses the same value of k as Ranger . We perform hyperparameter tuning on the learning rate and Œª , and the resulting hyperparameters reported in the paper are shown in Table 8 and 9 . E . 4 . Audio ClassiÔ¨Åcation Based on Wav2vec ( Schneider et al . , 2019 ) , the Wav2vec 2 . 0 ( Baevski et al . , 2020 ) is a framework for self - supervised learning of speech representations which is composed of 3 modules : feature encoder , contextualized representations and quantization module . In the feature encoder , there are 7 blocks with temporal convolutions that have 512 channels for each block and the relative positional embeddings of the convolutional layer modeling has kernel size of 128 and 16 groups . 38 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optim SUPERB Common Language LR Œª LR Œª AdamW 3e - 5 ‚àí 3e - 4 ‚àí AdaBelief 8e - 4 ‚àí 2e - 3 ‚àí Ranger 3e - 4 ‚àí 5e - 4 ‚àí RAdam 8e - 5 ‚àí 5e - 4 ‚àí A DMETA R 5e - 4 0 . 05 2e - 3 0 . 2 Table 10 . Hyperparameter settings of SUPERB and Common Language . Among the conÔ¨Ågurations of Wav2vec 2 . 0 , we choose Wav2vec 2 . 0 base model , which has 12 Transformer blocks , 95M parameters and 8 attention heads , with model dimension of 768 and inner dimension ( FFN ) of 3072 . We Ô¨Ånetune Wav2vec 2 . 0 base for keyword spotting and language identiÔ¨Åcation on SUPERB dataset ( Yang et al . , 2021 ) and Common Language ( Sinisetty et al . , 2021 ) dataset respectively . The dataset size of keyword spotting is smaller than that of language identiÔ¨Åcation , so we use a single NVIDIA RTX - 3090 GPU for training on the SUPERB dataset , and use four GPUs for parallel training on the Common Language dataset . The keyword spotting model is trained for 5 epochs with training batch size 32 and language identiÔ¨Åcation model for 10 epochs with training batch size 8 per device . Due to the same reason as in NLU experiments , i . e . the pre - training - Ô¨Åne - tuning paradigm , we only employ adaptive learning rate optimizers here . For all optimizers chosen , we Ô¨Åx Œ≤ 1 = 0 . 9 , Œ≤ 2 = 0 . 999 , (cid:15) = 1 e ‚àí 8 and set weight decay to 0 . 0 . The learning rate is searched from { 5e - 5 , 8e - 5 , 1e - 4 , 3e - 4 , 5e - 4 , 8e - 4 } , and for A DMETA R , Œª is searched from { 0 . 05 , 0 . 1 0 . 2 } . The resulting hyperparameters reported in the paper are shown in Table 10 . F . Future Work In the future work , for backward - looking part , though DEMA provides a more Ô¨Çexible way to deal with past gradients , it is still unable to intelligently judge the value of certain historical gradient information , such as discarding some obviously unreasonable gradients caused by noise . A better optimizer may have the ability to forget these wrong information and take advantage of what works , just working like human brains . For forward - looking part , our method takes the constant coefÔ¨Åcient into a dynamic one . It is kind of like milestone scheme of learning rate decay strategies to some extent . However , several experiments ( Huang et al . , 2017 ; Ma , 2020 ) have shown that cosine strategy ( Loshchilov & Hutter , 2016 ) works better . Therefore , we will follow the cosine scheme and propose a new forward - looking strategy that may work even better . G . Performance of SGDM and AdmetaS on Finetune Setting In this section , we test the performance of SGDM and A DMETA S on Ô¨Åntune setting and the results are shown in Table 11 . For keyword spotting ( SUPERB ) ( Yang et al . , 2021 ) task , we train the models for 5 epochs and use Wav2vec base ( Schneider et al . , 2019 ) as the baseline model . And for CIFAR - 10 ( Krizhevsky et al . , 2009 ) task , we train the model for 40 epochs from the checkpoint already trained with Adam using learning rate of 0 . 001 for 160 epochs . The baseline model of CIFAR - 10 is ResNet - 110 ( He et al . , 2016 ) with deep CNN architecture . We report the results of best hyperparameter settings for SGD and A DMETA S via grid searching . From Table 11 , we notice that in SUPERB task , compared to adaptive learning rate methods , SGDM achieves worse results in SUPERB task , but not by much , which shows that SGDM can also be used in Ô¨Ånetune setting . While A DMETA S can achieve better result than any other learning rate methods used in our experiment , demonstrating the advantage of our approach . This phenomenon contradicts the mainstream view that SGD family is not suitable for Ô¨Ånetune task . While for CIFAR - 10 task , SGDM and A DMETA S both improve the performance compared to the start point . However , they are both obviously worse than the performance of training the task from scratch using SGDM and A DMETA S respectively , which shows that pre - training is a very strong approach that makes the model achieve a good state . The reason why A DMETA S performs better than SGDM in Ô¨Ånetune setting may lie in two aspects . On the one hand , DEMA scheme in the backward - looking part reduces the overshoot problem that may do harm especially near convergence . On the other hand , the forward - looking part improves the stability of the training process . 39 Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non - adaptive Momentum Optimizers Optimizer SUPERB CIFAR - 10 SGDM 98 . 25 91 . 71 A DMETA S 98 . 54 91 . 87 Table 11 . Performance of SGDM and A DMETA S on Ô¨Ånetune setting . Optim CIFAR - 10 CIFAR - 100 A DMETA S 94 . 12 73 . 74 SGDM 93 . 68 72 . 07 SGDM ( lr = 0 . 5 ) 93 . 65 73 . 48 Table 12 . Performance of SGD family optimizers in CIFAR task . H . InÔ¨Çuence of Different Learning Rates in SGD Family Optimizers Since the learning rate of 0 . 5 for SGDM is a recommended value in ( Han et al . , 2017 ) but not in ( He et al . , 2016 ) , to alleviate the inÔ¨Çuence of different learning rates , we also try the performance of SGDM with a learning rate of 0 . 5 in the ResNet - 110 network and the results are listed in Table 12 . The results show that choosing a large learning rate for SGDM may increase the performance , as shown that when setting the learning rate to 0 . 5 instead of 0 . 1 , the recommended value in ResNet - 110 . However , this is not always true since the performance on CIFAR - 10 when using the learning rate of 0 . 5 does not get prompted . 40