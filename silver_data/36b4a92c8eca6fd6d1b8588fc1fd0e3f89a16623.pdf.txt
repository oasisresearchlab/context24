Empirical Analysis of Predictive Algorithms for Collaborative Filtering(cid:3) John S . Breese David Heckerman Carl Kadie Microsoft Research Redmond , WA 98052 - 6399 f breese , heckerma , carlk g @ microsoft . com Abstract Collaborative (cid:12)ltering or recommender sys - tems use a database about user preferences to predict additional topics or products a new user might like . In this paper we describe several algorithms designed for this task , in - cluding techniques based on correlation coef - (cid:12)cients , vector - based similarity calculations , and statistical Bayesian methods . We com - pare the predictive accuracy of the various methods in a set of representative problem domains . We use two basic classes of evalua - tion metrics . The (cid:12)rst characterizes accuracy over a set of individual predictions in terms of average absolute deviation . The second esti - mates the utility of a ranked list of suggested items . This metric uses an estimate of the probability that a user will see a recommen - dation in an ordered list . Experiments were run for datasets associ - ated with 3 application areas , 4 experimen - tal protocols , and the 2 evaluation met - rics for the various algorithms . Results indicate that for a wide range of con - ditions , Bayesian networks with decision trees at each node and correlation methods outperform Bayesian - clustering and vector - similarity methods . Between correlation and Bayesian networks , the preferred method de - pends on the nature of the dataset , nature of the application ( ranked versus one - by - one presentation ) , and the availability of votes with which to make predictions . Other con - siderations include the size of database , speed of predictions , and learning time . (cid:3) Appears in Proceedings of the Fourteenth Confer - ence on Uncertainty in Arti(cid:12)cial Intelligence , Madison , WI , July , 1998 . Morgan Kaufmann Publisher . 1 Introduction Typically , automated search over a corpus of items is based on a query identifying intrinsic features of the items sought . Search for textual documents ( e . g . Web pages ) uses queries containing words or describ - ing concepts that are desired in the returned docu - ments . Search for titles of compact discs , for example , requires identi(cid:12)cation of desired artist , genre , or time period . Most content retrieval methodologies use some type of similarity score to match a query describing the content with the individual titles or items , and then present the user with a ranked list of suggestions . A complementary method of identifying potentially in - teresting content uses data on the preferences of a set of users . Typically , these systems do not use any infor - mation regarding the actual content ( e . g . words , au - thor , description ) of the items , but are rather based on usage or preference patterns of other users . So called collaborative (cid:12)ltering or recommender systems [ Resnick and Varian , 1997 ] are built on the assump - tion that a good way to (cid:12)nd interesting content is to (cid:12)nd other people who have similar interests , and then recommend titles that those similar users like . Though there is increasing commercial interest in col - laborative (cid:12)ltering technology , there has been little published research on the relative performance of var - ious algorithms used in collaborative (cid:12)ltering systems . In this paper we describe various collaborative (cid:12)ltering prediction methodologies , including previously pub - lished algorithms based on correlation coe(cid:14)cients , as well as algorithms based on learning Bayesian mod - els . We present empirical data regarding the relative predictive performance of the various algorithms and extensions . Although we present some results address - ing the computational and scalability issues involved in applying the various algorithms , our primary empha - sis is the accuracy and the quality of recommendations of the predictive component . 2 Collaborative Filtering Algorithms The task in collaborative (cid:12)ltering is to predict the util - ity of items to a particular user ( the active user ) based on a database of user votes from a sample or popula - tion of other users ( the user database ) . In this paper we will examine two general classes of collaborative (cid:12)ltering algorithms . Memory - based algorithms oper - ate over the entire user database to make predictions . In Model - based collaborative (cid:12)ltering , in contrast , uses the user database to estimate or learn a model , which is then used for predictions . Collaborative (cid:12)ltering systems are often distinguished by whether they operate over implicit versus explicit votes . Explicit voting refers to a user consciously ex - pressing his or her preference for a title , usually on a discrete numerical scale . For example , GroupLens sys - tem of Resnick et al . [ 1994 ] uses a scale of one ( bad ) to (cid:12)ve ( good ) for users to rate Netnews articles , and users explicitly rate each article after reading it . Im - plicit voting refers to interpreting user behavior or se - lections to impute a vote or preference . Implicit votes can based on browsing data ( for example in Web ap - plications ) , purchase history ( for example in online or traditional stores ) , or other types of information access patterns . Regardless of the type of vote data available , collab - orative (cid:12)ltering algorithms must address the issue of missing data | we typically do not have a complete set of votes across all titles . We cannot assume that items are missing at random . In most applications , users will vote on items they have accessed , and are more likely to access ( and vote ) on items they like . Many of the applications of interest to us involve im - plicit voting , and some of the algorithms described in the next section rely on an interpretation that any vote appearing in the database indicates a positive pref - erence . We also show that by making di(cid:11)erent as - sumptions about the nature of missing data , the per - formance of collaborative (cid:12)ltering algorithms can be improved . 2 . 1 Memory - Based Algorithms Generally , the task in collaborative (cid:12)ltering is to pre - dict the votes of a particular user ( we will refer to this user as the active user ) from a database of user votes from a sample or population of other users . The user database therefore consists of a set of votes vi ; j corre - sponding to the vote for user i on item j . If Ii is the set of items on which user i has voted , then we can de(cid:12)ne the mean vote for user i as : vi = 1 j Ii j Xj2Ii vi ; j In memory - based collaborative (cid:12)ltering algorithms , we predict the votes of the active user ( indicated with a subscript a ) based on some partial information regard - ing the active user and a set of weights calculated from the user database . We assume that the predicted vote of the active user for item j , pa ; j , is a weighted sum of the votes of the other users : pa ; j = va + (cid:20) n Xi = 1 w ( a ; i ) ( vi ; j (cid:0) vi ) ( 1 ) where n is the number of users in the collaborative (cid:12)ltering database with nonzero weights . The weights w ( i ; a ) can re(cid:13)ect distance , correlation , or similarity between each user i and the active user . (cid:20) is a normal - izing factor such that the absolute values of the weights sum to unity . In the following , we distinguish between the various collaborative (cid:12)ltering algorithms in terms of the details of the \ weight " calculation . There are other possible characterizations for memory - based col - laborative (cid:12)ltering , however in this paper we restrict ourselves to the formulation described above . 2 . 1 . 1 Correlation This general formulation of statistical collaborative (cid:12)ltering ( as opposed to verbal or qualitative anno - tations ) (cid:12)rst appeared in the published literature in the context of the GroupLens project , where the Pear - son correlation coe(cid:14)cient was de(cid:12)ned as the basis for the weights [ Resnick et al . , 1994 ] . The correlation be - tween users a and i is : w ( a ; i ) = P j ( v a ; j (cid:0) v a ) ( v i ; j (cid:0) v i ) q P j ( v a ; j (cid:0) v a ) 2 P j ( v i ; j (cid:0) v i ) 2 ( 2 ) where the summations over j are over the items for which both users a and i have recorded votes . 2 . 1 . 2 Vector Similarity In the (cid:12)eld of information retrieval , the similarity be - tween two documents is often measured by treating each document as a vector of word frequencies and computing the cosine of the angle formed by the two frequency vectors [ Salton and McGill , 1983 ] . We can adopt this formalism to collaborative (cid:12)ltering , where users take the role of documents , titles take the role of words , and votes take the role of word frequencies . Note that under this algorithm , observed votes indi - cate a positive preference , there is no role for negative votes , and unobserved items receive a zero vote . The relevant weights are now w ( a ; i ) = Xj va ; j q P k2Ia v 2 a ; k vi ; j q P k2Ii v 2 i ; k ( 3 ) where the squared terms in the denominator serve to normalize votes so that users that vote on more ti - tles will not a priori be more similar to other users . Other normalization schemes , including absolute sum and number of votes , are possible . 2 . 2 Extensions to Memory - Based Algorithms We have investigated a number of modi(cid:12)cations to the standard algorithms that can improve performance . We describe these extensions here and the e(cid:11)ective - ness of each is discussed in Section 4 . 2 . 2 . 1 Default Voting Default voting is an extension to the correlation algo - rithm described in Section 2 . 1 . 1 . It arose out of the observation that when there are relatively few votes , for either the active user or the matching user , the cor - relation algorithm will not do well because it uses only votes in the intersection of the items both individuals have voted on ( I a \ I j ) . If we assume some default value as a vote for titles for which we do not have explicit votes , then we can form the match over the union of voted items , ( I a [ I j ) , where the default vote value is inserted into the formula for the appropriate unobserved items . In addition , we can assume the same default vote value d for some number of additional items k that neither user has voted on . This has the e(cid:11)ect of assuming there are some additional number of unspeci(cid:12)ed items that neither user voted on , but they would nonetheless agree on . 1 In most cases , the value for d will re(cid:13)ect a neutral or somewhat negative preference for these unobserved items . In applications with implicit voting , an observed vote is typically an indication of a positive preference ( e . g . a visit to the Web page is assigned a vote value of 1 ) . In this case the default vote can take on the value associ - ated with \ did not visit " or 0 . In this instance , default voting takes on the role of extending the data for each user with the true value for missing data . Note , how - ever , we only calculate weights for users who match the active user on at least one item . 1 In our experiments , we have used a value of 10 , 000 or k . 2 . 2 . 2 Inverse User Frequency In applications of vector similarity in information re - trieval , word frequencies are typically modi(cid:12)ed by the inverse document frequency [ Salton and McGill , 1983 ] . The idea is to reduce weights for commonly occurring words , capturing the intuition that they are not as use - ful in identifying the topic of a document , while words that occur less frequently are more indicative of topic . We can apply an analogous transformation to votes in a collaborative (cid:12)ltering database , which we term inverse user frequency . The idea is that universally liked items are not as useful in capturing similarity as less common items . We de(cid:12)ne the fj as log n nj where nj is the number of users who have voted for item j and n is the total number of users in the database . Note that if everyone has voted on a item j , then the f j is zero . To apply inverse user frequency while using the vec - tor similarity algorithm , we use a transformed vote in Equation 3 . The transformed vote is simply the orig - inal vote multiplied by the f j factor . In the case of correlation , we modify Equation 2 so that the f j is treated as a frequency and an item with a higher f j is assigned more weight in the correlation calculation . The relevant correlation weight with inverse frequency is : w ( a ; i ) = P j f j P j f j v a ; j v i ; j (cid:0) ( P j f j v a ; j ) ( P j f j v i ; j ) ) pUV where U = Xj f j ( Xj f j v 2 a ; j (cid:0) ( Xj f j v a ; j ) 2 ) V = X j f j ( X j f j v 2 i ; j (cid:0) ( X j f j v i ; j ) 2 ) 2 . 2 . 3 Case Ampli(cid:12)cation Case ampli(cid:12)cation refers to a transform applied to the weights used in the basic collaborative (cid:12)ltering pre - diction formula as in Equation 1 . We transform the estimated weights as follows w 0 a ; i = (cid:26) w (cid:26) a ; i if wa ; i (cid:21) 0 (cid:0) ( (cid:0) w (cid:26) a ; i ) if wa ; i < 0 The transform emphasizes weights that are closer to one , and punishes low weights . A typical value for (cid:26) for our experiments is 2 . 5 . 2 . 3 Model - Based Methods From a probabilistic perspective , the collaborative (cid:12)l - tering task can be viewed as calculating the expected value of a vote , given what we know about the user . For the active user , we wish to predict votes on as - yet unobserved items . If we assume that the votes are integer valued with a range for 0 to m we have : pa ; j = E ( va ; j ) = m Xi = 0 Pr ( va ; j = i j va ; k ; k 2 Ia ) i ( 4 ) where the probability expression is the probability that the active user will have a particular vote value for item j given the previously observed votes . In this paper we examine two alternative probabilistic models for collaborative (cid:12)ltering , cluster models and Bayesian networks . 2 . 3 . 1 Cluster Models One plausible probabilistic model for collaborative (cid:12)l - tering is a Bayesian classi(cid:12)er where the probability of votes are conditionally independent given membership in an unobserved class variable C taking on some rel - atively small number of discrete values . The idea is that there are certain groups or types of users cap - turing a common set of preferences and tastes . Given the class , the preferences regarding the various items ( expressed as votes ) are independent . The probability model relating joint probability of class and votes to a tractable set of conditional and marginal distributions is the standard \ naive " Bayes formulation : Pr ( C = c ; v 1 ; : : : ; v n ) = Pr ( C = c ) n Yi = 1 Pr ( v i j C = c ) The left - hand side of this expression is the probability of observing an individual of a particular class and a complete set of vote values . It is straightforwardto cal - culate the needed probability expressions for Equation 4 within this framework . This model is also known as a multinomial mixture model . The parameters of the model , the probabilities of class membership Pr ( C = c ) , and the conditional prob - abilities of votes given class Pr ( v i j C = c ) are esti - mated from a training set of user votes , the user database . Since we never observe the class variables in the database of users , we must employ methods that can learn parameters for models with hidden variables . We use the EM algorithm [ Dempster et al . , 1977 ] to learn the parameters for a model structure with a (cid:12)xed number of classes . We choose the number of classes by selecting the model structure that yields the largest ( approximate ) marginal likelihood of the data . We Beverly Hills , 90210 Watched Watched Not Watched FriendsWatched Friends Not Watched Beverly Hills , 90210 Not Watched Watched Not Watched Watched Not Watched Figure 1 : A decision tree for whether an individual watched \ Melrose Place " , with parents \ Friend’s " , and \ Beverly Hills , 90201 " . The bar charts at the bot - tom of the tree indicate the probabilities of watched and not watched for \ Melrose Place " , conditioned on viewing the parent programs . use the method of Cheeseman and Stutz ( 1995 ) to ap - proximate the marginal likelihood ( see also Chicker - ing and Heckerman , 1997 ) . In our experiments , we assume each model structure ( every possible number of classes ) is equally likely , and use a uniform prior for model parameters . We initialize the EM algorithm using the marginal - plus - noise technique described in [ Thiesson et al . , 1997 ] . 2 . 3 . 2 Bayesian Network Model An alternative model formulation for probabilistic col - laborative (cid:12)ltering is a Bayesian network with a node corresponding to each item in the domain . The states of each node correspond to the possible vote values for each item . We also include a state corresponding to \ no vote " for those domains where there is no natural interpretation for missing data . We then apply an algorithm for learning Bayesian net - works to the training data , where missing votes in the training data are indicated by the \ no vote " value . The learning algorithm searches over various model structures in terms of dependencies for each item . In the resulting network , each item will have a set of par - ent items that are the best predictors of its votes . Each conditional probability table is represented by a deci - sion tree encoding the conditional probabilities for that node . An example of such a tree , for television view - ing data ( see Section 3 . 2 ) is shown in Figure 1 . Details of the learning algorithm are discussed in Chickering et al . ( 1997 ) . In the remainder of the paper the term Bayesian network will refer to these networks with a decision tree for each title . In the experiments that follow , we use a structure prior that penalizes each additional free parameter with probability 0 . 1 , and derive parameter priors from a prior network as described in Chickering et al . , 1997 . In particular , we use a prior network that encodes a uniform distribution over all possible outcomes and an equivalent sample size of 10 . Experiments on subsets of the training data showed these parameters to pro - duce accurate results , although there was little sensi - tivity . 3 Empirical Analysis The purpose of this paper is to evaluate the predictive accuracy of the various algorithms for collaborative (cid:12)l - tering . In this section we will describe the evaluation criteria , the various protocols , and the datasets used in the analysis . We then present and discuss the re - sults regarding predictive accuracy , as well as several computational considerations . 3 . 1 Evaluation Criteria The e(cid:11)ectiveness of a collaborative (cid:12)ltering algorithm depends on manner in which recommendations will be presented to the user . To evaluate these algorithms , we have de(cid:12)ned metrics based on the type of collaborative (cid:12)ltering application and interface one is providing . There are two basic classes of collaborative (cid:12)ltering ap - plications . In the (cid:12)rst class , individual items are pre - sented one - at - a - time to the users along with a rating indicating potential interest in the topic . The original GroupLens system was in this category | each article in a GNUs - like Netnews interface has an ASCII bar - chart indicating the system’s prediction regarding the user’s possible interest in that article . Thus , each piece of content has an associated estimated rating , and the user interface displays this estimate along with a link to the content or as a part of the display or presenta - tion of the item . A second class of collaborative (cid:12)ltering applications present the user with an ordered list of recommended items . Examples of systems that present recommen - dation lists include PHOAKS [ L . Terveen et al . , 1997 ] and SiteSeer [ Rucker and Polanco , 1997 ] . In the spirit of the Internet search engines , these systems provide a ranked list of items ( Web sites , music recordings ) where highest ranked items are predicted to be most preferred . In these types of systems , the user presum - ably will investigate items in the ordered list starting at the top hoping to (cid:12)nd interesting items . We have applied two scoring metrics in our evaluations { one appropriate for individual item - by - item recommendations and the other appropriate for ranked lists . In both cases , the basic evaluation se - quence proceeds as follows . A dataset of users ( and their votes ) is divided into a training set and a test set . The data for the training set is used as the col - laborative (cid:12)ltering database or to build a probabilistic model . We then cycle through the users in the test set , treating each user as the active user . We divide the votes for each test user into a set of votes that we treat as observed , Ia , and a set that we will attempt to predict , Pa . We use the votes in Ia to predict the votes in Pa as shown in Equations 1 and 4 . For individual scoring , we look at the average absolute deviation of the predicted vote to the actual vote on items the users in the test set have actually voted on . That is , if the number of predicted votes in the test set for the active case is ma , then the average absolute deviation for a user is : Sa = 1 ma Xj2Pa j pa ; j (cid:0) va ; j j These scores are then averaged over all the users in the test set of users . This metric was also used in evaluating the GroupLens project [ Miller et al . , 1997 ] . For ranked scoring , the story is a bit more complex . In information retrieval research , ranked lists of re - turned items are evaluated in terms of recall and pre - cision . For a given number of returned items , recall is the percentage of relevant items that were returned and precision is the percentage of returned items that are relevant . In a collaborative (cid:12)ltering framework , if votes were binary ( like and dislike ) and we had com - plete preference judgments for a set of users we could develop a similar metric . However , more generally , we wish to estimate the expected utility of a particular ranked list to a user . The expected utility of a list is simply the probability of viewing a recommended item times its utility . In this analysis , we will equate the utility of an item with the di(cid:11)erence between the vote and the default or neutral vote in the domain . Furthermore , we make an estimate of how likely it is that the user will visit an item on a ranked list . We posit that each successive item in a list is less likely to be viewed by the user with an exponential decay . Then the expected utility of a ranked list of items ( sorted by index j in order of declining v a ; j ) is : R a = X j max ( v a ; j (cid:0) d ; 0 ) 2 ( j (cid:0) 1 ) = ( (cid:11) (cid:0) 1 ) ( 5 ) where d is the neutral vote and (cid:11) is the viewing hal(cid:13)ife . The hal(cid:13)ife is the number of the item on the list such that there is a 50 - 50 chance the user will review that item . For these experiments , we used a hal(cid:13)ife of 5 items . 2 2 We ran a set of experiments using a hal(cid:13)ife of 10 items and found little sensitivity of results . In scoring a ranked list generated for a user , we ap - ply Equation 5 using observed votes where available . For items that are not available , we apply the neutral vote , d , which e(cid:11)ectively removes those items from the scoring . The (cid:12)nal score for an experiment over a set of active users in the test set is R = 100 P a Ra P a R max a where R max a is the maximum achievable utility if all observed items had been at the top of the ranked list , ordered by vote value . This transformation allows us to consider results independent of the size of the test set and number of items predicted in a given experi - ment . 3 . 2 Datasets We evaluated the algorithm for three separate datasets , as follows : (cid:15) MS Web : This dataset captures individual visits to various areas ( vroots ) of the Microsoft corpo - rate web site . This is an example of an implicit voting database and application . Each vroot was characterized as being visited ( vote of one ) or not ( no vote ) . (cid:15) Television : This dataset uses Neilsen network television viewing data for individuals for a two week period in the summer of 1996 . The data was transformed into binary data indicating whether each show was watched , or not , as above . 3 (cid:15) EachMovie : This is an explicit voting example us - ing data from the EachMovie collaborative (cid:12)lter - ing site deployed by Digital Equipment Research Center from 1995 through 1997 . 4 Votes ranged in value from 0 to 5 . Table 3 . 2 provides additional information about each dataset . 3 . 3 Protocols We did two classes of experiments re(cid:13)ecting di(cid:11)ering numbers of votes available to the recommenders . In the (cid:12)rst protocol , we withhold a single randomly se - lected vote for each user in the test set , and try to predict its value given all the other votes the user has voted on . We term this protocol All but 1 . In the sec - ond set of experiments , we randomly select 2 , 5 , or 10 3 This dataset was made available for this study courtesy of Nielsen Media Research . 4 For more information see http : / / www . research . digital . com / SRC / EachMovie / . Dataset MSWEB Neilsen Eachmovie Total users 3453 1463 4119 Total titles 294 203 1623 Mean votes per user 3 . 95 9 . 55 46 . 4 Median votes per user 3 8 26 Table 1 : Number of users , titles , and votes for the datasets used in testing the algorithms . Only users with 2 or more votes are considered . votes from each test user as the observed votes , and then attempt to predict the remaining votes . We call these protocols Given 2 , Given 5 , and Given 10 . The All but 1 experiments measure the algorithms’ performance when given as much data as possible from each test user . The various Given experiments look at users with less data available , and examine the perfor - mance of the algorithms when there is relatively little known about an active user . In running the tests , if a prospective test did not have adequate votes for a trial it was eliminated from the evaluation . Thus the number of trials evaluated under each protocol vary . 4 Results In the following sections , we compare algorithms and analyze the e(cid:11)ects of individual algorithmic exten - sions . We use randomized block design where each algorithm is run on the same test cases and observed votes . We will refer to one of these comparisons as an experiment . Our analyses uses ANOVA with the Bon - ferroni procedure for multiple comparisons statistics [ McClave and Dietrich , 1988 ] . In the tables that fol - low , the value in the last row is labeled RD for Required Di(cid:11)erence . The di(cid:11)erence between any two scores in a column must be at least as big as the value in the RD row in order to be considered statistically signif - icant at the 90 % con(cid:12)dence level for the experiment as a whole . As a visual aid , a score in boldface is signi(cid:12)cantly di(cid:11)erent from the score directly below it in the table . 4 . 1 Overall Performance The following tables show the performance of the vari - ous major classes of algorithms on the various datasets and experiments . We compared the best performing variation of each algorithm on each dataset , for the di(cid:11)erent protocols . We also present the scores that result from presenting the user with the most popular items , regardless of the known votes of the individ - MS Web , Rank Scoring Algorithm Given2 Given5 Given10 AllBut1 BN 59 . 95 59 . 84 53 . 92 66 . 69 CR + 60 . 64 57 . 89 51 . 47 63 . 59 VSIM 59 . 22 56 . 13 49 . 33 61 . 70 BC 57 . 03 54 . 83 47 . 83 59 . 42 POP 49 . 14 46 . 91 41 . 14 49 . 77 RD 0 . 91 1 . 82 4 . 49 0 . 93 Table 2 : Ranked scoring results for the MS Web dataset . Higher scores indicate better performance . ual . This results in a baseline performance of a \ zero - order " collaborative (cid:12)ltering system , and is labeled as POP in the tables . The algorithm labeled CR + refers to use of the correlation technique with inverse user frequency , default voting , and case ampli(cid:12)cation ex - tensions . VSIM refers to using the vector similarity method with the inverse user frequency transforma - tion . BN and BC refer to the Bayesian network and clustering models respectively . Our results show that Bayesian networks with deci - sion trees at each node and correlation methods are the best performing algorithms over the experiments we have run . We ran 16 combinations of dataset , pro - tocol , and scoring criteria . The Bayesian network and correlation - based were each either best , or statistically equivalent , in 10 cases . Bayesian clustering was best performing in 2 cases and vector similarity was best in 3 cases . We see that the Bayesian network performs best un - der the All but 1 protocol . Generally , all the methods perform less well in the Given 2 and Given 5 protocols as might be expected . However the vector similarity and clustering methods are competitive for some of these limited - data scenarios , since these methods can use partial information e(cid:11)ectively . Table 2 shows data for rank scoring for the Microsoft web site dataset . For ranked scoring , higher scores indicate better performance . We see the Bayesian net - work model results in the best , or statistically equiv - alent to the best , score for all protocols . Correlation , with the appropriate enhancements designed to im - prove ranked scoring , is fairly close in performance . Note that correlation without default voting cannot operate on binary data with implicit voting , since all observed votes will have the same value . The vector similarity algorithm is slightly worse than correlation . All these algorithms outperform using popularity as a recommender . For the Neilsen dataset ( Table 3 ) , the Bayesian net - work outperforms the other algorithms except for the Neilsen , Rank Scoring Algorithm Given2 Given5 Given10 AllBut1 BN 34 . 90 42 . 24 47 . 39 44 . 92 CR + 39 . 44 43 . 23 43 . 47 39 . 49 VSIM 39 . 20 40 . 89 39 . 12 36 . 23 BC 19 . 55 18 . 85 22 . 51 16 . 48 POP 20 . 17 19 . 53 19 . 04 13 . 91 RD 1 . 53 1 . 78 2 . 42 2 . 40 Table 3 : Ranked scoring results for the Neilsen dataset . Higher scores indicate better performance . EachMovie , Rank Scoring Algorithm Given2 Given5 Given10 AllBut1 CR + 41 . 60 42 . 33 41 . 46 23 . 16 VSIM 42 . 45 42 . 12 40 . 15 22 . 07 BC 38 . 06 36 . 68 34 . 98 21 . 38 BN 28 . 64 30 . 50 33 . 16 23 . 49 POP 30 . 80 28 . 90 28 . 01 13 . 94 RD 0 . 75 0 . 75 0 . 78 0 . 78 Table 4 : Ranked scoring results for the EachMovie dataset . Higher scores indicate better performance . Given 2 protocol . Correlation , with extensions , and vector similarity are fairly close in performance , while Bayesian clustering performs relatively poorly . We see that the Bayesian network drops o(cid:11) in performance quite signi(cid:12)cantly for the Given 2 protocol , relative to correlation and vector similarity . We will discuss this observation below . We see a somewhat di(cid:11)erent pattern for EachMovie under ranked scoring , shown in Table 4 . Here the cor - relation algorithm is the top performer overall , with vector similarity performing well with less data . For this dataset and score , the Bayesian network performs worse than any of the other algorithms on all the Given experiments , but is the top performer and is competi - tive with correlation for the All but 1 protocol . The Bayesian networks using decision trees su(cid:11)er in the Given scenarios because they are provided with relatively little data . If a title that is held out for testing appears near the top of a tree , then it’s value is set to \ no vote " in evaluating the probability of a possibly related title . This may result in a title that is provided being ignored or having little impact , simply due to the ordering of the various predicting titles in the tree . The various All But 1 experiments are able to utilize trees to a fuller extent , and therefore perform well relative to the other methods that can use partial data . EachMovie , Absolute Deviation Algorithm Given2 Given5 Given10 AllBut1 CR 1 . 257 1 . 139 1 . 069 0 . 994 BC 1 . 127 1 . 144 1 . 138 1 . 103 BN 1 . 143 1 . 154 1 . 139 1 . 066 VSIM 2 . 113 2 . 177 2 . 235 2 . 136 RD 0 . 022 0 . 023 0 . 025 0 . 043 Table 5 : Absolute Deviation scoring results for the EachMovie dataset . Lower scores are better . For absolute deviation , we examined the EachMovie dataset and results are shown in Table 5 . This dataset has a vote range of 0 to 5 , making vote prediction a relevant task . We examine the same algorithms as in the previous table , except now we use a correla - tion algorithm without applying any of the extensions except for inverse user frequency . The other exten - sions are not e(cid:11)ective for absolute deviation scoring . This basic correlation algorithm performs best in all but the Given 2 experiments , indicating that this al - gorithm performs well when given adequate data re - garding the active case . The Bayesian clustered model does slightly better than the Bayesian network , and outperforms correlation in the Given 2 and Given 5 cases . 4 . 2 Inverse User Frequency In Section 2 . 2 . 2 we describe using inverse user fre - quency to modify vote values in applying memory - based algorithms . We performed a set of 12 experi - ments ( 3 datasets , 4 protocols ) each for vector sim - ilarity and correlation judging the e(cid:11)ect of applying inverse user frequency under ranked scoring . In all experiments , application of IUF improved the ranked score , and in 23 of 24 cases results were statistically signi(cid:12)cant . The average improvement was 1 . 9 % , with an improvement of 2 . 2 % for the vector similarity algo - rithm , and 1 . 5 % for the correlation algorithm . In 8 experiments run on the EachMovie dataset using absolute deviation scoring , the improvement averaged a more impressive 11 % . Results were signi(cid:12)cant in 6 of the 8 experiments . The average improvement was of 15 . 5 % for vector similarity , and 6 . 5 % for correlation . 4 . 3 Case Ampli(cid:12)cation Case ampli(cid:12)cation ( Section 2 . 2 . 3 ) modi(cid:12)es weights used in an memory - based algorithm to emphasize higher weights . We performed a set of 12 experiments ( 3 datasets , 4 protocols ) applying case ampli(cid:12)cation to correlation . The average improvement in the ranked score was 4 . 8 % , and results were signi(cid:12)cant in 11 of 12 experiments . There is no signi(cid:12)cant e(cid:11)ect of case am - pli(cid:12)cation on absolute deviation scoring . We also ran experiments combining case ampli(cid:12)cation and inverse user frequency , and found the bene(cid:12)ts to be additive . 4 . 4 Probabilistic Methods We used a training set to build probabilistic models for each dataset . Each title was encoded with an addi - tional explicit vote value of \ no vote " to complete the dataset for probabilistic learning . When scoring with Bayesian networks and cluster models , the \ no vote " values were explicitly entered into the network when missing , for both ranked and absolute deviation scor - ing . For the trees , the \ no vote " values were entered in each tree independently in order to generate a prob - ability for that title . For absolute deviation scoring , the expected vote was calculated by renormalizing the output probabilities , clamping the \ no vote " probabil - ity to zero . There are roughly 1600 movies in the EachMovie dataset , too many to estimate a full model in a reason - able amount of time . Therefore the Bayesian methods were trained from EachMovie for the top 300 movies in terms of overall popularity . For testing , all 1600 movies were used . In the other datasets , all items were used for training and testing . For the Bayesian networks , we applied alternate prior speci(cid:12)cations which resulted in trees of varying com - plexity . Priors that strongly penalized splits generated Bayesian networks with nodes with approximately 2 to 4 parents and 4 to 6 distributions in the decision tree representation . The model with the larger trees had somewhere between 4 and 6 predecessors and 6 to 8 distributions per variable . In all our experiments the larger trees outperformed the smaller tree so we re - strict our results to those models . Additional details are available in Breese et al . ( 1998 ) . Applying clustering to the datasets identi(cid:12)ed 3 classes for the Neilsen dataset , 7 classes for the MS Web dataset , and 9 classes for the EachMovie dataset . The classes found by clustering for the MS Web dataset are shown below . Each entry is a page area or virtual root that distinguishes this class from the others . The class names on the left were manually generated based on inspecting the resulting classes . Support Support Desktop , Knowledge Base , Win - dows95 Support , Search , NT Server Support Windows Products , Free Downloads , Windows95 , Windows95 Support , Windows Family of Prod - ucts O(cid:14)ce Products , MS O(cid:14)ce Info , Free Downloads , MS Word News , O(cid:14)ce Free Stu(cid:11) , MS O(cid:14)ce Developers Search , Training , Games , Developer Network , Job Openings Internet Explorer Internet Explorer , Free Down - loads , IE support , Net Meeting , International IE Content Internet Explorer Technical Search , Free Down - loads , Products , Internet Explorer , Internet Site Construction for Dev . IE Site Builder Internet Site Construction for Dev . , Web Site Builders Gallery , Developer Workshop , Sitebuilder Network Membership , Jakarta , Ac - tiveX Technology Dev . Among probabilistic methods , the Bayesian network with a decision tree at each item outperformed the cluster models for ranked scoring . In 12 comparisons , there was an average 41 % improvement in ranked scores , all di(cid:11)erences being statistically signi(cid:12)cant . For absolute deviation experiments run with the Each - Movie data , we found that the cluster model performed slightly better than the trees . 5 Additional Issues Although predictive accuracy is probably the most im - portant aspect in gauging the e(cid:14)cacy of a collabora - tive (cid:12)ltering algorithm , there are other considerations , including size of model , sampling , and runtime perfor - mance . If one considers the size of the overall collaborative (cid:12)l - tering prediction representation , memory - based meth - ods require a relatively small algorithm code base , plus a user database consisting of a sample of user votes . The model - based methods require the representation of the Bayesian network model , typically having much smaller memory requirements . For example , the user databases required for the memory - based methods for the EachMovie and MS Web datasets were approxi - mately 314 and 318 Kilobytes compressed , while the Bayesian network model sizes were 27 and 55 Kilobytes compressed respectively . The number of items in the usage database used for the memory - based methods was determined by exper - imenting with the scoring for various sizes of training set . Figure 2 shows the increase in ranked scoring ac - curacy as a function of size of training set . We used training set sizes ( number of users ) of 1637 for Neilsen , 5000 for EachMovie , and 32711 for MS Web . Identi - cal training sets were used as the user database for model - based methods , and as the database for learn - ing probabilistic models . Our experiments have found 55 . 00 57 . 00 59 . 00 61 . 00 63 . 00 65 . 00 0 10000 20000 30000 Training Set Size Rank Score Figure 2 : A learning curve showing the e(cid:11)ect of sample size on ranked scoring for the correlation method , All but 1 protocol , MSWeb dataset . that sample sizes on this order are adequate for pur - poses of generating recommendations . In terms of runtime performance , the probabilistic , model - based methods were approximately 4 times as fast as the memory - based methods in generating rec - ommendations , with correlation generating 3 . 2 recom - mendations per second and the Bayes net generating 12 . 9 recommendations per second on 266 MHz Pen - tium II processor ( Eachmovie dataset ) . Of course , the probabilistic models must be learned . Learning times for the models used in these experiments ranged from less than an hour for Neilsen and up to 8 hours for EachMovie and MS Web . 6 Conclusions This paper presents an extensive set of experiments re - garding the predictive performance of statistical algo - rithms for collaborative (cid:12)ltering or recommender sys - tems . Results indicate that for a wide range of con - ditions , Bayesian networks with decision trees at each node and correlation methods outperform Bayesian - clustering and vector similarity methods . Between cor - relation and Bayesian networks , the preferred method depends on the nature of the dataset , nature of the ap - plication ( ranked or one - by - one presentation ) , and the availability of votes with which to make predictions . We see that when there are relatively few votes , corre - lation and Bayesian networks have less of an advantage over the other techniques . Other considerations include the size of database , speed of predictions , and learning time . Bayesian net - works are typically have smaller memory requirements and allow for faster predictions than a memory - based technique such as correlation . However , the Bayesian methods examined here require a learning phase that can take up to several hours and results in a lag before changed behavior is re(cid:13)ected in recommendations . We plan to make the MS Web data used in this study available to learning community through the Irvine repository . As noted , the EachMovie data is currently available . We hope that the availability of this data coupled with discussion spurred by this paper will re - sult in additional examination and improvement of col - laborative (cid:12)ltering algorithms . Acknowledgements Datasets for this paper were generously provided by Digital Equipment Corporation ( EachMovie ) , Neilsen Media Research ( Neilsen ) , and Microsoft Corporation ( MS Web ) . Max Chickering , David Hovel , and Robert Rounthwaite contributed to the programming of the algorithms that were used in this study . We also thank Max Chickering , Eric Horvitz , and Chris Meek for use - ful discussions . John Riedl also provided useful com - ments . References [ Breese et al . , 1998 ] Breese , J . , Heckerman , D . , and Kadie , C . ( May , 1998 ) . An experimental compar - ison of collaborative (cid:12)ltering methods . Technical Report MSR - TR - 98 - 12 , Microsoft Research , Red - mond , WA . [ Cheeseman and Stutz , 1995 ] Cheeseman , P . and Stutz , J . ( 1995 ) . Bayesian classi(cid:12)cation ( Auto - Class ) : Theory and results . In Fayyad , U . , Piatesky - Shapiro , G . , Smyth , P . , and Uthurusamy , R . , ed - itors , Advances in Knowledge Discovery and Data Mining , pages 153 { 180 . AAAI Press , Menlo Park , CA . [ Chickering and Heckerman , 1997 ] Chickering , D . and Heckerman , D . ( 1997 ) . E(cid:14)cient approximations for the marginal likelihood of Bayesian networks with hidden variables . Machine Learning , 29 : 181 { 212 . [ Chickering et al . , 1997 ] Chickering , D . , Heckerman , D . , and Meek , C . ( 1997 ) . A Bayesian approach to learning Bayesian networks with local structure . In Proceedings of Thirteenth Conference on Un - certainty in Arti(cid:12)cial Intelligence , Providence , RI . Morgan Kaufmann . [ Dempster et al . , 1977 ] Dempster , A . , Laird , N . , and Rubin , D . ( 1977 ) . Maximum likelihood from incom - plete data via the EM algorithm . Journal of the Royal Statistical Society , B 39 : 1 { 38 . [ L . Terveen et al . , 1997 ] L . Terveen , Hill , W . , Amento , B . , McDconald , D . , and Creter , J . ( 1997 ) . PHOAKS : A system for sharing recommendations . Communications of the ACM , 40 ( 3 ) : 59 { 62 . [ McClave and Dietrich , 1988 ] McClave , J . T . and Di - etrich , F . H . ( 1988 ) . Statistics . Dellen Publishing Company , San Francisco , fourth edition . [ Miller et al . , 1997 ] Miller , B . , Riedl , J . , and Konstan , J . ( 1997 ) . Experiences with GroupLens : Making Usenet useful again . In Proceeding of the USENIX 1997 Annual Technical Conference , pages 219 { 231 , Anaheim , CA . [ Resnick et al . , 1994 ] Resnick , P . , Iacovou , N . , Suchak , M . , Bergstrom , P . , and Riedl , J . ( 1994 ) . Grouplens : An open ar - chitecture for collaborative (cid:12)ltering of netnews . In Proceedings of the ACM 1994 Conference on Com - puter Supported Cooperative Work , pages 175 { 186 , New York . ACM . [ Resnick and Varian , 1997 ] Resnick , P . and Varian , H . ( 1997 ) . Recommender systems . Communications of the ACM , 40 ( 3 ) : 56 { 58 . [ Rucker and Polanco , 1997 ] Rucker , J . and Polanco , M . J . ( 1997 ) . Siteseer : Personalized navigation of the web . Communications of the ACM , 40 ( 3 ) : 56 { 58 . [ Salton and McGill , 1983 ] Salton , G . and McGill , M . ( 1983 ) . Introduction to Modern Information Re - trieval . McGraw - Hill , New York . [ Thiesson et al . , 1997 ] Thiesson , B . , Meek , C . , Chick - ering , D . , and Heckerman , D . ( December , 1997 ) . Learning mixtures of DAG models . Technical Report MSR - TR - 97 - 30 , Microsoft Research , Red - mond , WA .