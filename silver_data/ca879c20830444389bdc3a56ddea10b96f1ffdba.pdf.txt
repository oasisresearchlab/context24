Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces Jonas Kjeldmand Jensen jkj @ di . ku . dk University of Copenhagen Copenhagen , Denmark Daniel Ashbrook dan @ di . ku . dk University of Copenhagen Copenhagen Denmark Figure 1 : Illustration showing the play sequence of the audio icons and how they loop back to the beginning . The icons are used to highlight upcoming events in the video or important user action points . In total , the how - to video contains 19 audio icons . The sequence is as follows : ( 1 ) dog bark , ( 2 ) car horn , ( 3 ) doorbell , ( 4 ) duck quack , ( 5 ) sheep bleat . ABSTRACT Voice interaction is an increasingly popular technology , allowing users to control devices and applications without the need for phys - ical interaction or ocular attention . Augmented voice playback control features , such as audio icons , have the potential to signifi - cantly improve voice navigation for instructional videos . This study evaluates audio icons for improving how - to video navigation in a Wizard - of - Oz - controlled setup with 24 participants assembling a wooden robot using a voice - controlled laptop . Results showed that audio icons helped participants complete the task faster , with fewer voice commands , and higher satisfaction . However , some usability challenges were observed . Significant differences in per - ceived usability were found between audio icons placed with visual points - of - action and the baseline , but not between the baseline and audio icons at 30 - second intervals . These findings provide valuable insights for VUI system researchers and designers to advance the use of audio icons for improving voice interface navigation . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands © 2023 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - XXXX - X / 18 / 06 . . . $ 15 . 00 https : / / doi . org / https : / / doi . org / 10 . 1145 / 3571884 . 3604302 CCS CONCEPTS • Human - centered computing → Empirical studies in inter - action design . KEYWORDS Conversational Interaction , How - to Videos , Non - Linear Instruc - tional Video , Voice - Based Navigation , Video Navigation , Wizard - of - Oz ACM Reference Format : Jonas Kjeldmand Jensen and Daniel Ashbrook . 2023 . Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces . In Proceedings of In CUI Conference in ( CUI ’23 ) . ACM , New York , NY , USA , 9 pages . https : / / doi . org / https : / / doi . org / 10 . 1145 / 3571884 . 3604302 1 INTRODUCTION Online how - to videos are a popular means for people to learn physical tasks , ranging from home repair to cooking recipes and makeup tutorials [ 1 , 21 , 37 ] . To enhance the learning experience , voice interaction has been proposed as a way to enable users to stay focused and hands - on with the task while still being able to navigate the instructional video content [ 24 , 25 ] . Learning woodworking involves managing a complex non - linear environment that demands working memory to ensure the correct materials and tools are used in the appropriate order . In addition , it necessitates tacit knowledge of manipulating materials into spe - cific shapes and configurations , domain expertise to address any CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Jonas Kjeldmand Jensen & Daniel Ashbrook problems , and multitasking skills to manage the various steps of the carpentry process . Woodworking instructional videos present a challenge and an opportunity for voice user interfaces ( VUIs ) . In current commercial voice assistants that come equipped with screens , such as Amazon Echo and Google Nest , users can navigate through professionally - annotated , step - by - step instructions using a variety of voice com - mands [ 10 ] . This annotation feature eases the user’s cognitive load and allows for easy repetition of steps , access to information on materials and procedures , and quick skipping between steps via content - based commands ( e . g . , " skip to the sawing part " ) . While YouTube offers many user - generated how - to videos , they currently lack support for voice interaction [ 40 ] . Although these videos may have limited practical application in voice assistants , they provide a valuable benchmark for advancing voice interface design . As video tutorials remain the default method for learning new skills for many people , studying new usability features to enhance their interactivity can lead to the development of more efficient and effective voice interfaces . In this study , we explore user needs for voice - based playback controls using audio icons to enhance navigation in how - to wood - working videos . To gather insights into user expectations and re - quirements in a natural setting , we conducted a Wizard - of - Oz study . The study addresses the following research questions ( RQs ) : • RQ1 : Can audio icons improve video navigation in how - to videos ? • RQ2 : What are the challenges and opportunities for design - ing voice navigation for instructional how - to videos ? In a between - subjects study with 24 participants , we evaluate the effectiveness of audio icon content - based referencing for voice - based navigation of a how - to video . We identified user strategies and patterns of query formulation and distill design recommen - dations for incorporating audio icons in non - linear instructional videos using voice . With this paper , we make the following contri - butions : • A user study exploring the effectiveness of auditory cues as feedback mechanisms for highlighting upcoming events , or point - of - interest in voice - based navigation of how - to videos • A prototype of a hybrid voice interaction system that com - bines temporal and content - based referencing to provide a more flexible and intuitive navigation experience for users • Future research and design opportunities to better support voice - based non - linear how - to video control for everyday tasks 2 RELATED WORK This work extends previous research in the areas of conventional video interface browsing and navigation , interaction techniques for how - to videos , and the design of voice user interfaces . 2 . 1 Voice Manipulation for Video Navigation Currently , voice navigation commands primarily focus on emulat - ing basic ’VHS - like’ playback controls like play , pause , and skip [ 2 ] . While these play on familiarity for the user , they also limit the potential for expanding people’s mental model of what is possible with voice assistants . Previous research has investigated alternative techniques for navigating videos , rather than relying solely on a timeline interface . For example , Dragicevic et al . [ 15 ] showed that directly manipu - lating video content through dragging interactions is effective for visual search tasks . They demonstrated this by having participants drag a billiard ball along the screen using the cursor , combining both time and content in video navigation . Swift and Swifter im - proved scrubbing techniques by displaying pre - cached thumbnails of unseen content along the bottom timeline [ 22 , 23 ] . This method is now standard across all video playback applications . Crockfordetal . [ 12 ] foundthatbasicVCR - remotecontrols ( pause , play , skip ) can both enhance and restrict a user’s browsing capa - bilities and that users choose different playback speeds based on content . Although these techniques are familiar for current voice - based video navigation , they were not designed with voice in mind . Simply importing these controls may only replicate their shortcom - ings and fail to leverage the potential advantages of voice control for video navigation [ 32 ] . 2 . 2 Auditory Feedback for Usability Audio icons are defined by Gaver [ 16 ] as " unique sounds from events , providing a powerful resource for information about a situa - tion " . They are widely used in digital systems to convey information about the status of a system or device [ 20 , 31 ] . Audio icons can range from small beeps to full vocal messages and are designed to provide information about an object , event , or situation [ 3 , 6 ] . Various types of nonverbal audio cues have been proposed , such as auditory icons , spearcons , spindices , auditory emoticons , auditory scrollbars , musicons , and others [ 13 , 14 , 36 , 38 ] . 2 . 3 Voice Interfaces Supporting Content - Based Navigation for How - To Videos Chang et al . [ 9 ] explored how users’ navigation objectives and intentions influence their word choices in voice commands . They created a lexicon of interaction types and identified the motivating factors behind these commands . They later developed a hands - free video management tool using voice commands for content - based navigation , representing a significant advancement in VUIs for instructional videos [ 8 ] . Their research has significant implications for the design of voice navigation interactions for how - to videos , revealing both challenges and opportunities . Their research on how to design for voice in how - to videos has inspired us to investigate further the user requirements for using voice interactions to navigate complex non - linear tasks such as woodworking tasks . However , their study did not address the ques - tion of how to expand on VUI - first global commands . Our study seeks to extend prior research by exploring novel techniques that facilitate content - based referencing for voice interaction in how - to videos . Specifically , we wanted to understand the effectiveness of us - ing commandable audio icons in facilitating usability for navigating voice - controlled complex how - to videos . We used a Wizard - of - Oz setup to simulate a complex multi - step task , allowing us to explore user needs without the constraints of query formulation and limi - tations of current speech technology [ 7 , 30 ] . This study addresses gaps in our understanding of user requirements and provides a foundation for future research in this area . Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Figure 2 : The experiment has three stages : briefing , voice interaction task , and post - experiment interview . Participants were given a tutorial at the briefing ( plus an additional priming session for conditions 2 & 3 ) . The voice interaction task included watching the how - to video and post - task questionnaires . The post - experiment interview concluded the study . Figure 3 : Actions from the video tutorial illustrating the subtasks in a step - by - step progression . The shown subtasks demonstrate effective placement of audio icons for drawing attention to them . 3 EXPERIMENT We used a between - participant design to investigate the augmented navigational audio icon feature as an independent variable across three conditions . The first condition served as the baseline with remote - like voice control features . In the second condition , we introduce the five - audio icon sequence which is continually placed and played in conjunction with key points - of - action in the video content . The five - icon sequence is continuously cycled , returning to the initial icon after every five action points . By utilizing the audio icons , users can navigate back and forth within the video by referencing the audio icons to backtrack to previous action points they wish to revisit , for example , by saying " go back to the last bark " . In the third condition , the audio icons are played at fixed 30 - second intervals , creating a continuous recycling loop across the five available icons . The participants can navigate within the video by recalling the audio icon associated with the desired time - point they wish to skip to . Analogously , in this condition , the audio icons serve as landmarks or anchors evenly placed throughout the entire how - to video . In contrast to condition 2 , where participants can establish associations between audio icons and their corresponding visual references on the screen , condition 3 lacks any inherent connection between the audio icons and the video content , except for the duration of the video itself . 3 . 1 Audio Icons The audio icons comprise a sequence of five recurring sounds , in - cluding a ( 1 ) dog bark , ( 2 ) car horn , ( 3 ) doorbell , ( 4 ) duck quack , and ( 5 ) sheep bleat . The audio icons appeared a total of 19 times throughout the how - to video . For instance , in condition 2 , an au - dio icon would accompany an action point such as the narrator explaining the process of drilling holes in the wooden torso block . The five audio icon sequence repeats itself and replays over again once all five icons have played . Resulting in each audio icon being associated with multiple action points in the video . The decision to assign multiple video action points to each audio icon was deliber - ate , aiming to prevent a one - to - one mapping between individual audio icons and specific action points in the video . Additionally , limiting the total number of audio icons ensured participants could comprehend and navigate the icon catalog effectively . The sounds used for audio icons were chosen for their high level of recognition and universality . None of the audio icons had any direct association with the tasks in the video to ensure abstraction from the current task . This design decision ensured that users had to recall the au - dio icons separately from the video content . The audio icons were optimized to ensure recognizability and minimize annoyance by truncating them to 150ms and decreasing their attack gain . Conditions 2 and 3 were evaluated for content - based commands related to audio icons , which were categorized into content - specific and sound - specific references . During the initial pilot studies , we observed that participants faced challenges in recalling the order or identity of the audio icons . To address this , we allowed participants to use utterances such as " go back to the last sound , " which was coded as sound - specific commands . Main Commands Popular Variants Commands available for all conditions play resume , go , start , begin pause stop , wait , hold on fast forward skip ahead , skip rewind go back , back time - specific e . g . go back 20 seconds , skip ahead 20 seconds Commands available for condition 2 and 3 sound - specific e . g . go back to last sound , skip forward to next sound content specific e . g . go back to the duck , skip ahead two bell sounds Table 1 : List of commands supported by the system . 3 . 2 Participants We recruited 24 participants ( 15 male , 9 female ) between the ages 20 and 33 ( 𝑥 = 26 , 𝜎 = 2 . 254 ) through convenience and snowball sampling . Four males participated in formative study trials to re - fine the experimental design . Five participants were assigned to CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Jonas Kjeldmand Jensen & Daniel Ashbrook condition 1 , 10 participants to condition 2 , and 5 participants to condition 3 . The selection process involved an online survey assess - ing participants’ experience with speech technology and ability to work with shop tools . No participants were excluded based on these criteria . Prior to the study session , participants were briefed on the trial procedure , data collection , and analysis . The sessions were recorded using two camera angles , and a brief post - experiment interview was conducted . 3 . 3 Study Protocol Participants were instructed to use a new VUI navigation system that incorporated audio icons at specific points of interest , referred to as " action points , " which announced important events in the video via sound . The system is designed to respond to voice com - mands in both time - ( e . g . , " go back 15 seconds " ) and content - based forms ( e . g . , " go back to the dog bark [ audio icon ] " ) . Participants used the system while following a step - by - step guide on how to assemble a wooden robot from an instructional woodworking video . Participants were given a workbench and all necessary tools and materials to complete the woodworking task , including a computer and an Amazon Echo Dot used to keep the setting realistic . During the session , participants were tasked with drilling holes in pre - cut wooden shapes , drill holes for the eyes , and assemble the shapes using twine . The wizard , located in an adjacent room , had a live broadcast of the participant’s computer on the workbench , enabling the wizard to observe and execute all voice commands . The wizard was equipped with pre - configured hotkeys to navigate the video for the participants and executed their commands immediately upon utterance . If a command was out of scope , a pre - recorded error message was played . Participants were asked not to use wake words . Figure 4 : The study captured both front - facing and overhead camera angles , showing participants using the workbench with a laptop and an Echo Dot positioned in front of them . 3 . 4 Measures To assess the efficiency and efficacy of each condition , we utilized a range of metrics . Objective measures included task completion time , number of commands , and types of commands . Additionally , we administered post - task questionnaires to evaluate usability and cognitive workload . For assessing user experience , we employed the widely - used SUS [ 5 , 11 ] and NASA - TLX [ 18 ] with a 10 - point scale for mental workload . In order to gather feedback on the navigation system , we con - ducted interviews with participants after each session [ 4 ] . The interviews aimed to identify challenges and collect suggestions for improvement . To assess the significance of observed differences among condi - tions , we conducted a one - way ANOVA followed by Tukey’s HSD post hoc test . The ANOVA examined the overall effect of condi - tions on multiple dependent variables , including time to complete , number of interactions , SUS scores , and NASA - TLX scores . The Tukey’s HSD test facilitated pairwise comparisons to identify con - dition pairs with significantly different means for each variable . A significance level of 𝛼 = 0 . 05 was used for all tests . 4 FINDINGS The study session lasted between 17 - 46 minutes . Interview data indicated that participants had a positive overall experience with the voice - based playback control and appreciated the hands - free op - eration . They found the ability to use both time - based and content - based commands to be flexible and helpful in navigating directly to their desired location . The ANOVA test did not reveal statistically significant variations in time to complete , number of interactions , or NASA Task Load Index ( NASA - TLX ) scores among the conditions . However , the SUS scores demonstrated a significant difference , sug - gesting variations in perceived usability between the conditions . 4 . 1 Cognitive Task Load Index ( NASA - TLX ) There was no significant difference in cognitive load observed across the three conditions . Participants reported a high task load for the baseline and both audio icon study variations . In condition 1 , participants reported higher frustration and phys - ical demand , as they had to repeat " go back " to locate specific temporal locations and they used more commands on average . P1 and P3 reported finding it tedious to have to repeat the same com - mands multiple times . In contrast , conditions 2 and 3 scored higher on mental demand , possibly due to the added augmented command features . Condition 1 scored lower on performance , indicating audio icons enhanced task success . Figure 5 : Boxplot displaying the NASA - TLX scores for the 20 participants , where higher scores indicate increased task load [ 18 ] . Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Semi - Structured Interview Questions Question 1 How was your experience with navigating a video via voice control ? Question 2 What was your overall strategy for completing this experiment ? Question 3 Were there any navigation functions you felt were missing ? Question 4 Would you have given more inputs if you wer able to use the keyboard ? Question 5 ( 2 & 3 ) What do you think the logic is behind the placements of the audio icons ? Question 6 ( 2 & 3 ) How can the audio icons be designed differently to make them more relevant to you ? Question 7 Can you think of a command to ’peek’ into the future to see the finished product ? Question 8 Do you have any additional comments ? Table 2 : The interview questions draw inspiration from [ 9 , 26 ] and from the pilot studies . Q5 and Q6 are only relevant to conditions 2 and 3 . 4 . 2 System Usability Score ( SUS ) Participants rated condition 2 highest ( 𝑥 = 81 , 𝜎 = 11 . 69 ) , followed by condition 3 with a mean of 79 ( 𝜎 = 2 . 4 ) , and the baseline condi - tion with a mean of 63 ( 𝜎 = 15 . 25 ) . Condition 1 received the lowest SUS rating overall with a mean of 63 ( 𝜎 = 15 . 25 ) , falling below the threshold for an " above average " design ( SUS score of 68 or higher ) . Conditions 2 and 3 achieved higher SUS scores , placing them in the top 15 % of SUS evaluated interaction system designs [ 17 ] . 1 2 3 4 5 6 7 8 9 10 𝑥 Condition 1 𝑥 2 . 80 2 . 40 3 . 25 1 . 60 2 . 80 2 . 40 3 . 20 2 . 60 3 . 20 1 . 80 2 . 61 𝜎 1 . 30 1 . 14 1 . 50 0 . 89 1 . 30 1 . 14 1 . 48 0 . 89 0 . 45 1 . 30 Condition 2 𝑥 4 . 45 2 . 45 4 . 55 1 . 91 3 . 55 2 . 55 3 . 73 3 . 09 4 . 36 1 . 82 3 . 25 𝜎 0 . 93 1 . 29 0 . 69 1 . 04 1 . 04 1 . 51 1 . 10 1 . 64 0 . 81 1 . 25 Condition 3 𝑥 4 . 00 2 . 00 4 . 50 2 . 25 3 . 75 1 . 75 3 . 50 2 . 25 4 . 25 3 . 50 3 . 18 𝜎 0 . 00 0 . 82 0 . 58 1 . 50 0 . 50 0 . 96 0 . 58 0 . 50 0 . 96 1 . 00 Table 3 : Average SUS score for each question in each condi - tion . Higher scores indicate higher usability rates [ 5 , 19 ] . The ANOVA test demonstrated a significant difference in SUS scores across the conditions ( 𝑝 = 0 . 038 ) . Post hoc analysis further revealed a significant difference in SUS scores between condition 1 and condition 2 , indicating distinct levels of perceived usability . However , no significant differences were found between condition 2 and condition 3 , as well as between condition 1 and condition 3 . These findings suggest that the temporal placement of audio icons at 30 - second intervals did not notably enhance usability com - pared to the baseline condition , highlighting limited effectiveness in improving the overall user experience . 4 . 3 Number of Commands & Command Types No statistical difference was found in the number of commands given across the conditions . Condition 1 had the highest number of commands , followed by condition 2 . Despite this , the percentage dis - tribution between pause and play commands remained consistent across all conditions , making up 71 . 19 % - 78 . 32 % of all commands given . This result is consistent with prior research on the use of the pause feature [ 33 ] . 4 . 4 Time - Specific Commands Participants in all conditions used time - specific commands more frequently than the pre - set 10 - second ( " Go back " , " Skip ahead " ) com - mands . However , there were differences in the use of time - specific commands between the conditions . Condition 3 had the highest usage of time - specific commands , while condition 2 had the lowest . The results for condition 3 were limited due to low engagement with the audio icons , but it appeared that when participants did not use the icons , their command usage pattern resembled that of condition 1 . 4 . 5 Content - Based Commands Content references were categorized as content - specific or sound - specific . Content - specific references are general descriptions , while sound - specific references use specific names to refer to audio icons for skipping to a particular point . Condition 2 recorded 39 content - specific commands with a dis - tribution of 20 content - based commands and 19 sound - specific com - mands . This suggests that participants struggled to recall which audio icon was played last from their time point of interest . Fur - ther , content - based references constituted 11 . 75 % of all commands given in condition 2 , which aligns with previous research indicating an approximate 8 - 9 : 1 ratio between temporal and content - based referencing [ 9 , 39 ] . In addition , condition 2 had 11 . 75 % of all commands given as content - based references , which is consistent with previous re - search’s report on the extent of use of content - based commands , indicating an approximate ratio of 8 - 9 : 1 between temporal and content - based referencing [ 9 , 39 ] . Total Content - Based References Content - Specific Commands Sound - Specific Commands 𝑥 3 . 55 1 . 82 1 . 73 𝜎 2 . 21 1 . 94 1 . 27 Table 5 : Mean and standard deviation for all content - based references in conditions 2 . CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Jonas Kjeldmand Jensen & Daniel Ashbrook Average # of Commands Pause Play Rewind Skip Forward Time - Specific Content - Specific Sound - Specific Condition 1 40 . 60 37 . 93 % 40 . 39 % 8 . 87 % 1 . 48 % 11 . 33 % - - Condition 2 30 . 18 34 . 94 % 38 . 25 % 6 . 33 % 0 . 90 % 7 . 83 % 6 . 02 % 5 . 72 % Condition 3 28 . 00 34 . 82 % 38 . 39 % 4 . 46 % 1 . 79 % 17 . 86 % 0 . 00 % 2 . 68 % Table 4 : Average number of commands for each condition and the distribution of command items . 4 . 6 Task Completion Time During the initial experiment instruction , participants were in - formed that their completion time would not be measured and were advised not to rush themselves . Participants interacted faster hav - ing the audio icons available in condition 2 ( 𝑥 = 2286 . 3 , 𝜎 = 856 . 84 ) than in the baseline experiment ( 𝑥 = 2553 . 2 , 𝜎 = 421 . 9 ) . Condition 3 was the fastest ( 𝜇 = 1695 , 𝜎 = 647 . 6 ) , followed by condition 2 ( 𝜇 = 2286 . 3 , 𝜎 = 856 . 84 ) . Figure 6 : Boxplot showing task completion time for all con - ditions , where a lower score indicates better usability . Figure 7 : Three examples of the finished wooden robot . 4 . 7 Semi - structured Interviews In conditions 2 and 3 , participants had difficulty associating audio icons with actions in the instructional video , as they perceived no meaningful connection between them . This difficulty resulted from the need to simultaneously monitor the audio icons , follow the video content , and complete a novel task . While some participants initially found the use of audio icons perplexing , a considerable number expressed positive sentiments regarding the potential ad - vantages of employing audio icons to facilitate video navigation when asked in the post - experiment interview . Many participants appreciated the fact that they were able to avoid repetitive use of time - based commands to reach specific points in the video ( e . g . , saying " back " multiple times in succession ) . Instead , they found it expedient to rely on a single audio icon , ensuring direct navigation to the beginning of important steps in the tutorial . Participants sug - gested several design improvements , such as displaying a Rolodex - type feature with images of the animal linked to each command , including a permanent timeline with thumbnail images of the video , or displaying a picture - in - picture frame when the video is paused . Others suggested audio feedback that more closely resembles the corresponding action or visual content , similar to spearcons . 5 DISCUSSION Several participants encountered challenges with audio icons dur - ing navigation . Some found it challenging to attend to audio icons alerting them to upcoming video action points while monitoring the visual content , adding cognitive load and hindering their un - derstanding of the audio icon’s reference . The significant difference between condition 1 and condition 2 suggests that audio icons placed alongside action points signifi - cantly improved usability . These findings highlight the importance of considering the specific placement strategies for audio icons in voice user interfaces for controlling how - to videos . Moreover , no significant difference was found between condition 1 and condition 3 , where audio icons were placed at fixed 30 - second intervals . This suggests that temporal placement alone did not enhance the user experience compared to the baseline condition . It is important to note that some participants in condition 3 did not utilize the au - dio icons , which might have influenced the results . Future studies should include a larger sample size for this condition to gain more insights . 5 . 1 Users Struggle to Associate Audio Icons with Visual Content The lack of interrelatedness among the audio icons also contributed to their reduced usage by participants . When the audio icon is the - matically dissimilar to the video content , it introduces an additional layer of abstraction that users must remember , thereby impacting usability and highlighting the trade - off between adaptive design and universality . The limited utilization of the audio icons in Con - dition 3 can be attributed to their lack of intrinsic correspondence with specific events in the video beyond the timeline . Instead , they served as sonic anchor points for users to reference . This heightened Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands level of abstraction likely made it challenging for users to grasp the purpose of the audio icons , resulting in their underutilization . 5 . 2 Pause and Play Participants showed different preferences for start and stop features . Some paused the video to complete a subtask , while others multi - tasked and watched the video while performing the physical task . Novel video playback functions have been proposed to improve instructional videos , such as identifying sections and transitions [ 27 ] , or automatic pausing at relevant points [ 29 ] . Allowing for user - controlled pausing can have advantages , as users can take breaks or adjust the pace of the video to match their own needs . For example , some users would intentionally fall behind the video to efficiently progress through the physical task while still following the upcoming steps via the slightly - ahead video or using the audio icons to alert them to upcoming steps . 5 . 3 Alignment Pause From [ 9 , 33 ] , we learn that users frequently pause and stop how - to videos at frames that show completed progressions of subtasks . P15 in condition 2 found that the placement of the audio icons did not align with the points where he wanted to skip - ahead to . He suggested that audio icons should lead to a summary of each subtask , allowing him to align the physical task with the corre - sponding instructional video frame , from where he could skip back to specific time points if needed . This suggests that providing both an icon at the beginning of each subtask and a separate icon at each summary of any given subtask could accommodate different navigation strategies and enable some users to complete the task by solely aligning still summary images with their physical task . 5 . 4 Video Pace Some participants expressed a desire to control the video playback speed . P10 suggested automatic video pacing , where the video speeds up during task demonstrations and slows down during vocal explanations . Automatic pacing can save voice inputs but at the cost of giving up control . A solution to address video pacing in a VUI context could be to add in preset pacing options as commands that can be easily toggled on or off . This approach supports the implementation of global voice commands in speech interfaces . 6 DESIGN RECOMMENDATIONS 6 . 1 Enable Globals in a VUI Context Developing VUI - first global features would prioritize voice com - mands as the primary mode of interaction , rather than being an add - on to existing visual interfaces . Such features would be de - signed to offer a wide range of voice commands that can control various aspects of the interface , allowing users to perform complex tasks without needing to rely on visual cues or manual input . For example , a VUI - first global feature in a video player might allow users to control the playback speed , navigate to different parts of the video , and adjust the volume using voice commands . Users could also use natural language queries to search for specific scenes or keywords within the video , making it easier to find the informa - tion they need . For example , users can initiate a global command such as " Pause the video every time the instructor uses the drill machine " or " Make a bookmark every time the instructor starts talking " to set the state of future navigation and reduce the need for similar subsequent commands . This is particularly useful for complex tasks that require users to switch their focus between the task and the instructional video [ 28 ] . 6 . 2 Enable Users to Define Anchor Points To enhance user control over video navigation , we propose a user - created bookmark feature that allows users to define anchor points of interest that can be accessed later via logical voice commands . This approach shifts away from system - based decision - making and empowers users to take charge of their own navigation . It is worth noting that this bookmark feature can coexist or mix with other features , such as audio icons placed at interest points , to provide a more comprehensive navigation experience . 6 . 3 Support User Expectations Enabling direct commands in navigating timeline - based videos aligns with user expectations . However , it requires pre - analysis of video content or secondary knowledge of when the user lost atten - tion , which current technology cannot support [ 34 , 35 ] . A more accessible avenue of research would be to enhance the voice interac - tion user experience by enabling the VUI system to understand and remember the context of commands given during an interaction , including recognizing previously given commands and analyzing viewing patterns . Supporting utterances like " Can I watch the part I had difficulty with one more time ? " could improve usability and reinforce the user’s perception of the VUI as intelligent . 7 CONCLUSION Our study explored the effectiveness of audio icons in enhancing voice navigation of how - to videos . Participants were tasked with assembling a wooden robot using an instructional video with a VUI - controlled laptop with augmented playback audio icon con - trols . Results showed that the audio icon conditions led to faster task completion , fewer voice commands , and higher SUS scores . The results reveal a significant difference in usability between con - dition 1 and condition 2 , but no significant differences between condition 2 and condition 3 , as well as between condition 1 and condition 3 , suggesting limited enhancement in user experience with the temporal placement of audio icons at 30 - second intervals compared to the baseline condition . Several participants reported difficulty in linking the audio icons with the corresponding video content , leading to challenges in tracking the sequence of audio icons . This issue was particularly evident in condition 3 , where some participants did not utilize the icons at all . Participants ex - pressed that the abstraction of meaning between the audio icons and the video content posed a significant hindrance to the effective utilization of the audio icons . Our findings highlight important usability challenges for researchers and designers of VUI systems and contribute to advancing the use of audio icons in improving the navigability of voice interfaces . CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Jonas Kjeldmand Jensen & Daniel Ashbrook REFERENCES [ 1 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . 2018 . Crowd - sourcing Similarity Judgments for Agreement Analysis in End - User Elicitation Studies . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology . ACM , Berlin Germany , 177 – 188 . https : / / doi . org / 10 . 1145 / 3242587 . 3242621 [ 2 ] Morteza Behrooz , Sarah Mennicken , Jennifer Thom , Rohit Kumar , and Henriette Cramer . 2019 . AUGMENTING MUSIC LISTENING EXPERIENCES ON VOICE ASSISTANTS . ( 2019 ) , 8 . [ 3 ] MeeraM . Blattner , DeniseA . Sumikawa , andRobertM . Greenberg . 1989 . Earcons and icons : their structure and common design principles . Human - Computer Interaction 4 , 1 ( March 1989 ) , 11 – 44 . https : / / doi . org / 10 . 1207 / s15327051hci0401 _ 1 [ 4 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative Research in Psychology 3 ( Jan . 2006 ) , 77 – 101 . https : / / doi . org / 10 . 1191 / 1478088706qp063oa [ 5 ] John Brooke . 1995 . SUS : A quick and dirty usability scale . Usability Eval . Ind . 189 ( Nov . 1995 ) . [ 6 ] João Paulo Cabral and Gerard Bastiaan Remijn . 2019 . Auditory icons : Design and physical characteristics . Applied Ergonomics 78 ( July 2019 ) , 224 – 239 . https : / / doi . org / 10 . 1016 / j . apergo . 2019 . 02 . 008 [ 7 ] Julia Cambre and Chinmay Kulkarni . 2020 . Methods and Tools for Prototyping Voice Interfaces . In Proceedings of the 2nd Conference on Conversational User Interfaces ( CUI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 4 . https : / / doi . org / 10 . 1145 / 3405755 . 3406148 [ 8 ] Minsuk Chang , Mina Huh , and Juho Kim . 2021 . RubySlippers : Supporting Content - based Voice Navigation for How - to Videos . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . Number 97 . Association for Computing Machinery , New York , NY , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3411764 . 3445131 [ 9 ] Minsuk Chang , Oliver Wang , Maneesh Agrawala , and Juho Kim . 2019 . How to Design Voice Based Navigation for How - To Videos . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , Glasgow Scotland Uk , 1 – 11 . https : / / doi . org / 10 . 1145 / 3290605 . 3300931 [ 10 ] Peggy Chi , Nathan Frey , Katrina Panovich , and Irfan Essa . 2021 . Automatic Instructional Video Creation from a Markdown - Formatted Tutorial . In The 34th Annual ACM Symposium on User Interface Software and Technology . ACM , Virtual Event USA , 677 – 690 . https : / / doi . org / 10 . 1145 / 3472749 . 3474778 [ 11 ] Gennaro Cordasco , Marilena Esposito , Francesco Masucci , Maria Teresa Riviello , Anna Esposito , Gérard Chollet , Stephan Schlögl , Pierrick Milhorat , and Gianni Pelosi . 2014 . Assessing Voice User Interfaces : The vassist system prototype . In 2014 5th IEEE Conference on Cognitive Infocommunications ( CogInfoCom ) . 91 – 96 . https : / / doi . org / 10 . 1109 / CogInfoCom . 2014 . 7020425 [ 12 ] Chris Crockford and Harry Agius . 2006 . An empirical investigation into user navigation of digital video using the VCR - like control set . International Journal of Human - Computer Studies 64 , 4 ( April 2006 ) , 340 – 355 . https : / / doi . org / 10 . 1016 / j . ijhcs . 2005 . 08 . 012 [ 13 ] Ádám Csapó and György Wersényi . 2013 . Overview of auditory representations in human - machine interfaces . Comput . Surveys 46 , 2 ( Dec . 2013 ) , 19 : 1 – 19 : 23 . https : / / doi . org / 10 . 1145 / 2543581 . 2543586 [ 14 ] Tilman Dingler , Jeffrey Lindsay , and Bruce N Walker . 2008 . LEARNABILTIY OF SOUND CUES FOR ENVIRONMENTAL FEATURES : AUDITORY ICONS , EARCONS , SPEARCONS , AND SPEECH . ( 2008 ) , 6 . [ 15 ] Pierre Dragicevic , Gonzalo Ramos , Jacobo Bibliowitcz , Derek Nowrouzezahrai , Ravin Balakrishnan , and Karan Singh . 2008 . Video browsing by direct ma - nipulation . In Proceeding of the twenty - sixth annual CHI conference on Hu - man factors in computing systems - CHI ’08 . ACM Press , Florence , Italy , 237 . https : / / doi . org / 10 . 1145 / 1357054 . 1357096 [ 16 ] William Gaver . 1989 . The SonicFinder : An Interface That Uses Auditory Icons . Human - Computer Interaction 4 , 1 ( March 1989 ) , 67 – 94 . https : / / doi . org / 10 . 1207 / s15327051hci0401 _ 3 [ 17 ] Debjyoti Ghosh , Pin Sym Foong , Shan Zhang , and Shengdong Zhao . 2018 . As - sessing the Utility of the System Usability Scale for Evaluating Voice - based User Interfaces . In Proceedings of the Sixth International Symposium of Chinese CHI ( ChineseCHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 11 – 15 . https : / / doi . org / 10 . 1145 / 3202667 . 3204844 [ 18 ] Sandra G . Hart . 2006 . Nasa - Task Load Index ( NASA - TLX ) ; 20 Years Later . Pro - ceedings of the Human Factors and Ergonomics Society Annual Meeting 50 , 9 ( Oct . 2006 ) , 904 – 908 . https : / / doi . org / 10 . 1177 / 154193120605000909 Publisher : SAGE Publications Inc . [ 19 ] Adriana Iñiguez Carrillo , Laura Gaytán - Lugo , Miguel Garcia - Ruiz , and Rocio Arellano . 2021 . Usability Questionnaires to Evaluate Voice User Interfaces . IEEE Latin America Transactions 100 ( March 2021 ) , 4771 . https : / / doi . org / 10 . 1109 / TLA . 2021 . 9468439 [ 20 ] Myounghoon Jeon , Thomas M . Gable , Benjamin K . Davison , Michael A . Nees , Jeff Wilson , and Bruce N . Walker . 2015 . Menu Navigation With In - Vehicle Technologies : Auditory Menu Cues Improve Dual Task Performance , Preference , and Workload . International Journal of Human - Computer Interaction 31 , 1 ( Jan . 2015 ) , 1 – 16 . https : / / doi . org / 10 . 1080 / 10447318 . 2014 . 925774 Publisher : Taylor & Francis Ltd . [ 21 ] Bridjet Lee and Kasia Muldner . 2020 . Instructional Video Design : Investigating the Impact of Monologue - and Dialogue - style Presentations . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . ACM , Honolulu HI USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3313831 . 3376845 [ 22 ] Justin Matejka , Tovi Grossman , and George Fitzmaurice . 2012 . Swift : reducing the effects of latency in online video scrubbing . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , Austin Texas USA , 637 – 646 . https : / / doi . org / 10 . 1145 / 2207676 . 2207766 [ 23 ] Justin Matejka , Tovi Grossman , and George Fitzmaurice . 2013 . Swifter : improved onlinevideoscrubbing . In ProceedingsoftheSIGCHIConferenceonHumanFactors in Computing Systems ( CHI ’13 ) . Association for Computing Machinery , New York , NY , USA , 1159 – 1168 . https : / / doi . org / 10 . 1145 / 2470654 . 2466149 [ 24 ] Chelsea Myers , Anushay Furqan , Jessica Nebolsky , Karina Caro , and Jichen Zhu . 2018 . Patterns for How Users Overcome Obstacles in Voice User Interfaces . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI’18 ) . AssociationforComputingMachinery , NewYork , NY , USA , 1 – 7 . https : / / doi . org / 10 . 1145 / 3173574 . 3173580 [ 25 ] Chelsea Myers , Anushay Furqan , and Jichen Zhu . 2018 . Adaptable Utterances in Voice User Interfaces to Increase Learnability . ( 2018 ) , 6 . [ 26 ] Chelsea M . Myers , Anushay Furqan , and Jichen Zhu . 2019 . The Impact of User Characteristics and Preferences on Performance with an Unfamiliar Voice User Interface . In Proceedings of the 2019 CHI Conference on Human Factors in Com - puting Systems ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 9 . https : / / doi . org / 10 . 1145 / 3290605 . 3300277 [ 27 ] Cuong Nguyen and Feng Liu . 2015 . Making Software Tutorial Video Responsive . In Proceedingsofthe33rdAnnualACMConferenceonHumanFactorsinComputing Systems ( CHI ’15 ) . Association for Computing Machinery , New York , NY , USA , 1565 – 1568 . https : / / doi . org / 10 . 1145 / 2702123 . 2702209 [ 28 ] Elnaz Nouri , Robert Sim , Adam Fourney , and Ryen W . White . 2020 . Step - wise RecommendationforComplexTaskSupport . In Proceedingsofthe2020Conference on Human Information Interaction and Retrieval . ACM , Vancouver BC Canada , 203 – 212 . https : / / doi . org / 10 . 1145 / 3343413 . 3377964 [ 29 ] Suporn Pongnumkul , Mira Dontcheva , Wilmot Li , Jue Wang , Lubomir Bourdev , Shai Avidan , and Michael F . Cohen . 2011 . Pause - and - play : automatically linking screencast video tutorials with applications . In Proceedings of the 24th annual ACM symposium on User interface software and technology ( UIST ’11 ) . Association for Computing Machinery , New York , NY , USA , 135 – 144 . https : / / doi . org / 10 . 1145 / 2047196 . 2047213 [ 30 ] Martin Porcheron , Joel E . Fischer , and Stuart Reeves . 2021 . Pulling Back the Curtain on the Wizards of Oz . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW3 ( Jan . 2021 ) , 243 : 1 – 243 : 22 . https : / / doi . org / 10 . 1145 / 3432942 [ 31 ] Edin Sabic , Scott Mishler , Jing Chen , and Bin Hu . 2017 . Recognition of Car Warnings : An Analysis of Various Alert Types . In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems . ACM , Denver Colorado USA , 2010 – 2016 . https : / / doi . org / 10 . 1145 / 3027063 . 3053149 [ 32 ] Arjun Srinivasan , Mira Dontcheva , Eytan Adar , and Seth Walker . 2019 . Dis - covering natural language commands in multimodal interfaces . In Proceedings of the 24th International Conference on Intelligent User Interfaces ( IUI ’19 ) . As - sociation for Computing Machinery , New York , NY , USA , 661 – 672 . https : / / doi . org / 10 . 1145 / 3301275 . 3302292 [ 33 ] Sylvaine Tuncer , Barry Brown , and Oskar Lindwall . 2020 . On Pause : How Online Instructional Videos are Used to Achieve Practical Tasks . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . ACM , Honolulu HI USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3313831 . 3376759 [ 34 ] Alexandra Vtyurina . 2019 . Towards Non - Visual Web Search . In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval ( CHIIR ’19 ) . Association for Computing Machinery , New York , NY , USA , 429 – 432 . https : / / doi . org / 10 . 1145 / 3295750 . 3298976 [ 35 ] Alexandra Vtyurina and Adam Fourney . 2018 . Exploring the Role of Conversa - tional Cues in Guided Task Support with Virtual Assistants . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . Association for Computing Machinery , New York , NY , USA , 1 – 7 . https : / / doi . org / 10 . 1145 / 3173574 . 3173782 [ 36 ] Bruce N . Walker , Jeffrey Lindsay , Amanda Nance , Yoko Nakano , Dianne K . Pal - ladino , Tilman Dingler , and Myounghoon Jeon . 2013 . Spearcons ( Speech - Based Earcons ) ImproveNavigationPerformanceinAdvancedAuditoryMenus . Human Factors 55 , 1 ( Feb . 2013 ) , 157 – 182 . https : / / doi . org / 10 . 1177 / 0018720812450587 Publisher : SAGE Publications Inc . [ 37 ] Sarah Weir , Juho Kim , Krzysztof Z . Gajos , and Robert C . Miller . 2015 . Learn - ersourcing Subgoal Labels for How - to Videos . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing ( CSCW ’15 ) . Association for Computing Machinery , New York , NY , USA , 405 – 416 . https : / / doi . org / 10 . 1145 / 2675133 . 2675219 [ 38 ] Pavani Yalla and Bruce N . Walker . 2008 . Advanced auditory menus : design and evaluation of auditory scroll bars . In Proceedings of the 10th international ACM SIGACCESS conference on Computers and accessibility ( Assets ’08 ) . Association for Exploring Audio Icons for Content - Based Navigation in Voice User Interfaces CUI ’23 , July 19 – 21 , 2023 , Eindhoven , The Netherlands Computing Machinery , New York , NY , USA , 105 – 112 . https : / / doi . org / 10 . 1145 / 1414471 . 1414492 [ 39 ] Matin Yarmand , Dongwook Yoon , Samuel Dodson , Ido Roll , and Sidney S . Fels . 2019 . " Can you believe [ 1 : 21 ] ? ! " : Content and Time - Based Reference Patterns in Video Comments . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , Glasgow Scotland Uk , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300719 [ 40 ] Yaxi Zhao , Razan Jaber , Donald McMillan , and Cosmin Munteanu . 2022 . “Rewind to the Jiggling Meat Part” : Understanding Voice Control of Instructional Videos in Everyday Tasks . In CHI Conference on Human Factors in Computing Systems . ACM , New Orleans LA USA , 1 – 11 . https : / / doi . org / 10 . 1145 / 3491102 . 3502036