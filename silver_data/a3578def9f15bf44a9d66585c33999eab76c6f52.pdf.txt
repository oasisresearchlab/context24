Power Gradient Descent Marco Baiesi a , b a Department of Physics and Astronomy , University of Padova , Via Marzolo 8 , I - 35131 Padova , Italy b INFN , Sezione di Padova , Via Marzolo 8 , I - 35131 Padova , Italy Abstract The development of machine learning is promoting the search for fast and stable minimization algorithms . To this end , we suggest a change in the current gradient descent methods that should speed up the motion in ﬂat regions and slow it down in steep directions of the function to minimize . It is based on a “power gradient” , in which each component of the gradient is replaced by its versus - preserving H - th power , with 0 < H < 1 . We test three modern gradient descent methods fed by such variant and by standard gradients , ﬁnding the new version to achieve signiﬁcantly better performances for the Nesterov accelerated gradient and AMSGrad . We also propose an e ﬀ ective new take on the ADAM algorithm , which includes power gradients with varying H . Keywords : Optimization , Gradient Descent 1 . Introduction At the core of most modern machine learning tools , and in general in a huge variety of algorithms , one ﬁnds gradient de - scent ( GD ) techniques ( Mehta et al . , 2019 ) . In neural networks , usually the minimization of a complex cost function converges via some GD method to one of its local minima . Momen - tum was introduced in some algorithms ( Nesterov , 1983 ; Qian , 1999 ; Goh , 2017 ) so that the dynamics gets progressively more speed in ﬂat directions of the cost function , where otherwise the GD would get stuck for a long time ( Sutton , 1986 ) . Re - cent methods as , for instance , RMSprop ( Tieleman and Hin - ton , 2012 ) , ADAM ( Kingma and Jimmy Ba . , 2015 ) and AMS - Grad ( Reddi et al . , 2018 ) , utilize a running average of the square of the gradient to rescale the norm of the step size , thus stabi - lizing the steps in the optimization . We introduce a simple modiﬁcation that can improve the con - vergence speed and stability of all GD versions . Assuming that “small” and “large” are with respect to quantities of order of one , a power x H with exponent 0 < H < 1 of a quantity x > 0 results larger than x for small x ’s , and reduced with respect to x if this is large . Based on this simple observation , to boost the speed of GD in regions of shallow gradients and at the same time stabilize their step size in regions with large gradients , we propose to replace each component of the gradient by its versus - preserving H - th power . More precisely , for a cost function V ( θ ) that depends on the variables θ = ( θ ( 1 ) , θ ( 2 ) , . . . , θ ( N ) ) , every component of the gra - dient is modiﬁed as g ( i ) ( θ ) = ∂ V ( θ ) ∂θ ( i ) → h ( i ) ( θ ) = sign (cid:16) g ( i ) (cid:17) (cid:12) (cid:12) (cid:12) g ( i ) (cid:12) (cid:12) (cid:12) H ( 1 ) Email address : baiesi @ pd . infn . it ( Marco Baiesi ) This “power gradient” h ( θ ) preserves the direction of each com - ponent and rescales its magnitude by the H - th power . From now on we mostly focus on the H = 1 / 2 case , i . e . we take square roots | g ( i ) | H = (cid:112) | g ( i ) | if not otherwise stated . The goal is ob - taining the e ﬀ ects sketched in Fig . 1 , where normal gradients are represented by red arrows and power gradients by blue ar - rows . A large gradient ( point P ) leads to a smaller power gra - dient , while regions with small gradient ( saddle with point S ) generate power gradients with larger magnitude . Thus , any GD method adopting the power gradient should be more stable in steep gradient areas and , at the same time , less stagnant in ﬂat regions of the cost function . We will denote any power gradient variant with the notation “h - ” , for example h - ADAM denotes the ADAM algorithm with g ( i ) replaced by h ( i ) . 2 . Results The step of a vanilla h - GD is , θ t + 1 = θ t − η h ( θ t ) Figure 1 : Sketch of a cost function V and of the gradient ( red arrows ) and corresponding power gradient ( blue arrows ) at a point within a steep slope ( P ) and at a point in a shallow region near a saddle ( S ) . Preprint submitted to Elsevier June 13 , 2019 a r X i v : 1906 . 04787v1 [ c s . L G ] 11 J un 2019 Figure 2 : Row ( a ) for V 1 and ( b ) for V 2 show their contour plot in the chosen domain D ( white dots at random starting points , and blue dot at the absolute minimum ) and , for each simulated GD method , the mean value of the cost function as a function of the mean time of steps in each minimization , in CPU time . Curves are for learning rates leading to best performances : η = 10 − 2 for NAG variants and 10 − 1 for the rest . where we have simpliﬁed the notation by removing the com - ponent index ( i ) , t is the index of the current iteration , and η is the learning rate . By studying this dynamics in a simple one dimensional quadratic cost function V ( θ ) = κθ 2 / 2 , we ﬁnd the ﬁrst interesting feature of the h - GD : it does not converge to the minimum ! Even for small η values , one easily veriﬁes that the algorithm converges ( always ) to a 2 - state orbit where it keeps bouncing between − ˜ θ and + ˜ θ , with ˜ θ = η 2 κ / 4 . Hence , a sim - ple new way for testing the convergence is to check whether the sum | h ( θ t ) + h ( θ t − 1 ) | is close to zero , as it should if the two power gradients are opposite to each other . One may then esti - mate V ( θ t ) in the middle point θ t = ( θ t + θ t − 1 ) / 2 of the 2 - state orbit . With these tricks , the h - GD ﬁnds the minimum of the quadratic function while remaining stable , i . e . there is no upper threshold for the learning rate . In realistic situations there are of course deviations from quadratic V ’s around its local minima and , heuristically , we may anticipate that η needs to be small enough for keeping ˜ θ within a local quadratic well of V ( θ ) . However , this argument applies to the basic h - GD algorithm , which , similarly to the basic GD , is less e ﬃ cient than newer methods . With more complex GD methods , including momentum and second moment rescaling , we have found that the 2 - state orbits are not very relevant and that minimization can be monitored on a single last state θ t of a run . Nevertheless , it is important to be aware of the feature described above in the analysis of GD methods embodying power gradients . The behavior of standard GD techniques and of the new h - GD versions is illustrated with some two - dimensional cost functions V ( θ ) = V ( x , y ) . As representative of methods with momentum , we consider the Nesterov accelerated gradient ( NAG ) ( Nesterov , 1983 ) . To represent modern second moment methods , we study both the AMSGrad ( Reddi et al . , 2018 ) and ADAM ( Kingma and Jimmy Ba . , 2015 ) algorithms . All details of their modiﬁed versions are reported in Appendix A . We collect the statistics of minimization trajectories starting from n = 100 initial points spread randomly in a domain D , where cost functions have the lowest minimum V ( θ min ) = 0 at θ min ∈ D . Hence , to test the convergence in this simple setup we require V ( θ t ) < 10 − 4 . We start by studying the convergence speed for bounded cost functions V 1 ( x , y ) = − e − x 2 − y 2 − e − x 2 − e − y 2 + 3 ( 2 ) V 2 ( x , y ) = 11 . 5 − 10 e 125 ( − x 2 − y 2 ) − e − x 2 − y 2 − 0 . 5 cos (cid:16) − yx 2 − y 2 + xy + x + 2 y (cid:17) ( 3 ) shown respectively in Fig . 2 ( a ) and Fig . 2 ( b ) . The function V 1 has a single minimum at θ min1 = ( 0 , 0 ) and wide plateaus that hinder the convergence of points initially too far from θ min1 . This picture is a caricature of neural networks with improperly nor - malized inputs , leading to tiny gradients because the nonlinear functions , say ReLU functions , are evaluated deep in their ﬂat region . The function V 2 instead represents a complex landscape with ripples and several secondary minima in addition to the ab - solute minimum θ min2 = ( 0 , 0 ) . In Fig . 2 ( a ) there are the values of V 1 ( θ t ) averaged over the n minimizations , as a function of the average CPU time per run ; each panel refers to an algorithm in the standard and in the new version . In this example , the h - NAG is faster than NAG and h - AMSGrad is faster than AMSGrad . This means that the slightly longer code and CPU time per step is compensated by a bet - ter overall convergence of the new versions . However , ADAM does not get any improvements from the power gradient , in this example . In Fig . 2 ( b ) we show the same plots for the averaged V 2 ( θ t ) . In this case , introducing the power gradients it is at best not an 2 Figure 3 : Each column is for a given cost function ( ( a ) V 3 , ( b ) V 4 , ( c ) V Beale ) and contains its contour plot in the chosen domain D ( contours in log scale , each color shade representing a decade ; white dots at random starting points , and blue dot at the absolute minimum ) and , for each simulated GD method , the fraction of trajectories converging within a time T ( see legends ) as a function of the log of the learning rate ( log 10 η ) . Rows with gray background refer to traditional methods , white background to the respective power gradient variants with H = 1 / 2 , and the last row to the h ( t ) - ADAM implementation . advantage . The power gradient here does not help in overtaking the ripples of V 2 . Note that in this example NAG shines as it inertially wins against the little barriers , while ADAM and AMSGrad are too e ﬀ ective in adapting to the local slopes and do not ﬁnd easily the global minimum . To challenge further the GD methods , next we test the mini - mization of unbounded cost functions , V 3 ( x , y ) = 10 27 ( 2 y − x ) 4 + 10 9 ( 2 x + y ) 2 + 5 y 4 ( 4 ) V 4 ( x , y ) = log (cid:16) ( 3 x + y ) 2 + 1 (cid:17) + cosh (cid:18) 4 sin (cid:18) π 2 ( 3 x + y ) (cid:19) + x − 3 y (cid:19) − 1 ( 5 ) V Beale ( x , y ) = (cid:16) xy 3 − x + 2 . 625 (cid:17) 2 + (cid:16) xy 2 − x + 2 . 25 (cid:17) 2 + ( xy − x + 1 . 5 ) 2 ( 6 ) where the latter is Beale’s test function ( see their level plots in Fig . 3 ) . These functions are ranked by increasing complexity : V 3 has a single minimum with a simple power - law divergence ; 3 V 4 has an exponential divergence forming steep walls surround - ing a wavy ﬂat ravine , i . e . a structure that challenges inertial dy - namics and stability ; ﬁnally , the Beale’s test function contains narrow ravines , a shallow central region , and global minimum in θ minBeale = ( 3 , 0 . 5 ) plus a secondary minimum . These functions contain a mixture of ﬂat and steep directions , which , as we con - jectured , is a landscape suitable for h - GD methods . The quality of an algorithm is now measured by the fraction f D ( η , T ) of trajectories in the domain D that reached conver - gence within a time t ≤ T when the learning rate is η . We mon - itor the behavior of f D ( η , T ) vs ( integer values of ) log 10 η . This assigns importance also to the stability of the methods and fol - lows the standard procedure of exploring the performances of algorithms for η spanning several orders of magnitude ( Mehta et al . , 2019 ) . In Fig . 3 we see that , in all examples , h - NAG is better than NAG and h - AMSGrad is better than AMSGrad , corroborating the hypothesis that the power gradient is able to speed up the convergence while stabilizing the algorithm . Indeed , both the full curves for f ( η , T ) and their best case scenario at an optimal log 10 η in general are signiﬁcantly higher for the h - GD methods than for the GD ones , i . e . h - GD methods converge faster and eventually with more trajectories in the domain D . For ADAM vs h - ADAM the comparison continues to be not clearly in favor of any of the two versions . Quite likely , the structure of the ADAM algorithm ( Appendix A ) is already enhancing the same beneﬁts that the power gradient is trying to introduce . By looking closely at the curves , however , we may note that h - ADAM is quicker for V 3 ( higher yellow and green curves ) , slower for V 4 and working better than ADAM at log 10 η = 1 for V Beale . This leads to wander whether a hybrid al - gorithm may collect the good working regimes of both ADAM and h - ADAM in a single method . In fact , as a last method , we introduce h ( t ) - ADAM , in which also the exponent H becomes a function of time ( see Appendix A ) . The starting point is H 0 = 1 / 2 to exploit the power gradient at the beginning of the optimization , when conditions might be more unstable or stagnant because V ( θ 0 ) is more likely to be far from minima . Than H t converges to 1 with the iterations t to exploit the usual ADAM performances at a stage when the optimization is more reﬁned . In the last row of Fig . 3 we see that indeed h ( t ) - ADAM performs better than both ADAM and h - ADAM with ﬁxed H = 1 / 2 . 3 . Conclusions The power gradient is an intriguing modiﬁcation of the stan - dard gradient of a function , whose components are squeezed in modulus around 1 by the application of a power with expo - nent H < 1 . In fact , in general such vector is not a gradient of a function anymore . The introduction of power gradients into GD methods in our examples almost always yields better performances for Nesterov and AMSGrad methods . ADAM algorithm instead seems not particularly taking advantage of the power gradient , perhaps due to its intrinsic ability to tame steep gradients and boost ﬂat gradients . However , ADAM is known to carry an instability that can lead to its divergence from minima after some converging period ( Reddi et al . , 2018 ) , and AMSGrad was in fact introduced to solve this issue . Yet , if one decides to use ADAM , the modiﬁcation h ( t ) - ADAM should be considered , as our examples show that it achieves better per - formances than those of a basic ADAM . In h ( t ) - ADAM , also the exponent H varies . The embedding of power gradients in other GD methods ( e . g . RMSprop ( Tieleman and Hinton , 2012 ) , AdaGrad ( Duchi et al . , 2011 ) , AdaDelta ( Zeiler , 2012 ) , NADAM ( Dozat , 2016 ) ) will be tested in future works , together with other H (cid:44) 1 / 2 cases . With an idealized parabolic cost function , the basic h - GD converges always to a 2 - state orbit , regardless of the learn - ing rate . This di ﬀ erentiates it from the standard GD , which becomes unstable and diverges if the learning rate is above a threshold . We have described how to turn this peculiar feature of the h - GD to a new method for ﬁnding the minimum of the quadratic function . The results with more elaborated h - GD al - gorithms applied to two - dimensional problems have shown no signiﬁcant e ﬀ ects due to 2 - state orbits , yet understanding this feature represents another good subject for future works . In summary , our simple examples suggest that deforming gradients can improve GD . More studies are needed in general to determine the strength and the limitations of GD methods embodying power gradients . It should be interesting to check how the learning rate in deep neural networks would change if h - GDs are used . In that case , h - GDs could help surﬁng the ﬂat directions encountered in a high - dimensional landscape of cost functions while avoiding too large steps along steep slopes of the function . Acknowledgments . I thank the students of the course “Labora - tory of computational physics” , degree in “Physics of Data” at the University of Padova , for discovering and letting me know of a typo in the description of the Nesterov algorithm in the review by Mehta et al . ( 2019 ) . Appendix A . This appendix contains the h - GD algorithms used in this work . The standard versions ( H = 1 and normal gradients ) can be easily recovered from these ones or from the literature . For the sake of simplicity , we do not write the index ( i ) of the component . It is understood that each line refers to a given component and , e . g . , by ∂ θ V we mean the i - th component of the gradient . h - NAG g t = ∂ θ V ( θ t − γ v t − 1 ) h t = sign ( g t ) | g t | H v t = γ v t − 1 + η h t θ t + 1 = θ t − v t ( A . 1 ) Here v t plays the role of a ( negative ) velocity in the inertial dynamics of NAG . The damping parameter is set to γ = 0 . 99 , and H = 1 / 2 in our simulations . 4 h ( t ) - ADAM H t = β 3 H t − 1 + ( 1 − β 3 ) g t = ∂ θ V ( θ t ) h t = sign ( g t ) | g t | H t m t = β 1 m t − 1 + ( 1 − β 1 ) h t s t = β 2 s t − 1 + ( 1 − β 2 ) h 2 t ˆ m t = m t 1 − ( β 1 ) t ˆ s t = s t 1 − ( β 2 ) t θ t + 1 = θ t − η ˆ m t √ ˆ s t + (cid:15) ( A . 2 ) In this work the initial value of the exponent is set to H 0 = 1 / 2 , from which H t → 1 by iterating the algorithm with β 3 = 0 . 999 . Of course , other starting values H 0 > 0 could be chosen too . The simpler h - ADAM does include a constant H , e . g . in this work H = 1 / 2 . Other parameters β 1 = 0 . 9 and β 2 = 0 . 99 follow the typical values in the literature . The constant (cid:15) = 10 − 8 prevents the algorithm from exploding in case of null gradient . h - AMSGrad g t = ∂ θ V ( θ t ) h t = sign ( g t ) | g t | H m t = β 1 m t − 1 + ( 1 − β 1 ) h t s t = β 2 s t − 1 + ( 1 − β 2 ) h 2 t ˆ m t = m t 1 − ( β 1 ) t ˆ s t = max ( s t , s t − 1 ) θ t + 1 = θ t − η ˆ m t √ ˆ s t + (cid:15) ( A . 3 ) Parameters are as above . Note how ˆ s t is modiﬁed with respect to ADAM , with the aim of removing its potential instability . References Dozat , T . , 2016 . Incorporating nesterov momentum into adam . In : ICLR Work - shop . Vol . 1 . pp . 2013 – 2016 . Duchi , J . , Hazan , E . , Singer , Y . , 2011 . Adaptive subgradient methods for online learning and stochastic optimization . J . Mach . Learn . Res . 12 , 2121 – 2159 . Goh , G . , 2017 . Why momentum really works . Distill . URL http : / / distill . pub / 2017 / momentum Kingma , D . P . , Jimmy Ba . , . , 2015 . Adam : A method for stochastic optimiza - tion . In : Proceedings of 3rd International Conference on Learning Repre - sentations . Mehta , P . , Bukov , M . , Wang , C . - H . , Day , A . G . , Richardson , C . , Fisher , C . K . , Schwab , D . J . , 2019 . A high - bias , low - variance introduction to machine learning for physicists . Phys . Rep . 810 , 1 – 124 . Nesterov , Y . E . , 1983 . A method for unconstrained convex minimization prob - lem with the rate of convergence O ( 1 / k 2 ) . Soviet . Math . Docl . 27 , 372 – 376 . Qian , N . , 1999 . On the momentum term in gradient descent learning algorithms . Neur . Net . 12 , 145 – 151 . Reddi , S . J . , Kale , S . , Kumar , S . , 2018 . On the convergence of Adam and be - yond . In : Proceedings of ICLR . p . 1 . Sutton , R . S . , 1986 . Two problems with backpropagation and other steepest - descent learning procedures for networks . In : Proceedings of the Eighth An - nual Conference of the Cognitive Science Society . Hillsdale , NJ : Erlbaum . Tieleman , T . , Hinton , G . , 2012 . Lecture 6 . 5 - RMSprop : Divide the gradient by a running average of its recent magnitude . COURSERA : Neural Netw . Mach . Learn . 4 ( 2 ) , 26 – 31 . Zeiler , M . D . , 2012 . ADADELTA : an adaptive learning rate method . ArXiv preprint arXiv : 1212 . 5701 . 5