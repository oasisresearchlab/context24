P ROVABLE A CCELERATION OF N EURAL N ET T RAIN - ING VIA P OLYAK ’ S M OMENTUM Jun - Kun Wang & Jacob Abernethy Georgia Institute of Technology { jimwang , prof } @ gatech . edu A BSTRACT Incorporating a so - called “momentum” dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that , at least empirically , it often leads to signiﬁcantly faster convergence . At the same time , there are very few theoretical guarantees in the literature to explain this apparent acceleration effect . In this paper we show that Polyak’s momentum , in combi - nation with over - parameterization of the model , helps achieve faster convergence in training a one - layer ReLU network on n examples . We show speciﬁcally that gradient descent with Polyak’s momentum decreases the initial training error at a rate much faster than that of vanilla gradient descent . We provide a bound for a ﬁxed sample size n , and we show that gradient descent with Polyak’s momentum converges at an accelerated rate to a small error that is controllable by the number of neurons m . Prior work ( Du et al . , 2019b ) showed that using vanilla gradient descent , and with a similar method of over - parameterization , the error decays as ( 1 − κ n ) t after t iterations , where κ n is a problem - speciﬁc parameter . Our result shows that with the appropriate choice of parameters one has a rate of ( 1 −√ κ n ) t . This work establishes that momentum does indeed speed up neural net training . 1 I NTRODUCTION Momentum methods are very popular for training neural networks in various applications ( e . g . He et al . ( 2016 ) ; Vaswani et al . ( 2017 ) ; Krizhevsky et al . ( 2012 ) ) . It has been widely observed that the use of momentum helps faster training in deep learning ( e . g . Sutskever et al . ( 2013 ) ; Hoffer et al . ( 2017 ) ; Loshchilov & Hutter ( 2019 ) ; Wilson et al . ( 2017 ) ; Cutkosky & Orabona ( 2019 ) ; Liu & Belkin ( 2020 ) ) . Among all the momentum methods , the most popular one seems to be Polyak’s momentum ( a . k . a . Heavy Ball momentum ) ( Polyak , 1964 ) , which is the default choice of momen - tum in PyTorch and Tensorﬂow . 1 The success of Polyak’s momentum in deep learning is widely appreciated and almost all of the recently developed adaptive gradient methods like Adam ( Kingma & Ba ( 2015 ) ) , AMSGrad ( Reddi et al . ( 2018 ) ) , and AdaBound ( Luo et al . ( 2019 ) ) adopt the use of Polyak’s momentum , instead of Nesterov’s momentum . However , despite its popularity , little is known in theory about why Polyak’s momentum helps to accelerate training neural networks . Even for convex optimization , smooth twice continuously dif - ferentiable functions like strongly convex quadratic problems seem to be one of the few cases that Polyak’s momentum method provably achieves faster convergence than standard gradient descent ( e . g . Lessard et al . ( 2016 ) ; Goh ( 2017 ) ; Ghadimi et al . ( 2015 ) ; Gitman et al . ( 2019 ) ; Loizou & Richt´arik ( 2017 ; 2018 ) ; Can et al . ( 2019 ) ; Scieur & Pedregosa ( 2020 ) ; Flammarion & Bach ( 2015 ) ) . On the other hand , the theoretical guarantees of Adam ( Kingma & Ba ( 2015 ) ) , AMSGrad ( Reddi et al . ( 2018 ) ) , or AdaBound ( Luo et al . ( 2019 ) ) are only worse if the momentum parameter β is non - zero and the guarantees deteriorate as the momentum parameter increases , which do not show any advantage of the use of momentum ( see also e . g . Alacaoglu et al . ( 2020 ) ) . Moreover , the conver - gence rates that have been established for Polyak’s momentum in several related works ( Gadat et al . , 2016 ; Sun et al . , 2019 ; Yang et al . , 2018 ; Liu et al . , 2020 ) do not improve upon those for vanilla gra - 1 See PyTorch webpage https : / / pytorch . org / docs / stable / _ modules / torch / optim / sgd . html and Tensorﬂow webpage https : / / www . tensorflow . org / api _ docs / python / tf / keras / optimizers / SGD . 1 a r X i v : 2010 . 01618v1 [ c s . L G ] 4 O c t 2020 Algorithm 1 : Gradient descent with Polyak’s momentum Polyak ( 1964 ) ( Equivalent Version 1 ) 1 : Required : Step size parameter η and momentum parameter β . 2 : Init : w 0 ∈ R d and m − 1 = 0 ∈ R d . 3 : for t = 0 to T do 4 : Given current iterate w t , obtain gradient ∇ L ( w t ) . 5 : Update momentum m t : = βm t − 1 + ∇ L ( w t ) . 6 : Update iterate w t + 1 : = w t − ηm t . 7 : end for Algorithm 2 : Gradient descent with Polyak’s momentum Polyak ( 1964 ) ( Equivalent Version 2 ) 1 : Required : step size η and momentum parameter β . 2 : Init : w 0 = w − 1 ∈ R d 3 : for t = 0 to T do 4 : Given current iterate w t , obtain gradient ∇ L ( w t ) . 5 : Update iterate w t + 1 = w t − η ∇ L ( w t ) + β ( w t − w t − 1 ) . 6 : end for dient descent or vanilla SGD . There are even negative cases in convex optimization showing that the use of Polyak’s momentum results in divergence ( e . g . Lessard et al . ( 2016 ) ; Ghadimi et al . ( 2015 ) ) . Furthermore , Kidambi et al . ( 2018 ) construct a problem instance for which the momentum method under its optimal tuning is outperformed by other algorithms . A solid understanding of the empirical success of Polyak’s momentum in deep learning has eluded researchers for some time . In this paper , we provably show that Polyak’s momentum helps achieve faster convergence for train - ing a one - hidden - layer ReLU network . Over the past few years there have appeared an enormous number of works considering training a one - layer ReLU network , provably showing convergence results for vanilla ( stochastic ) gradient descent ( e . g . Li & Liang ( 2018 ) ; Ji & Telgarsky ( 2020 ) ; Li & Yuan ( 2017 ) ; Du et al . ( 2019b ; a ) ; Allen - Zhu et al . ( 2019 ) ; Song & Yang ( 2019 ) ; Zou et al . ( 2019 ) ; Arora et al . ( 2019 ) ; Jacot et al . ( 2018 ) ; Lee et al . ( 2019 ) ; Chizat et al . ( 2019 ) ; Brutzkus & Globerson ( 2017 ) ; Tian ( 2017 ) ; Soltanolkotabi ( 2017 ) ; Bai & Lee ( 2020 ) ; Ghorbani et al . ( 2019 ) ; Li et al . ( 2020 ) ; Hanin & Nica ( 2020 ) ; Daniely ( 2017 ) ; Zou & Gu ( 2019 ) ; Dukler et al . ( 2020 ) ; Daniely ( 2020 ) ; Wei et al . ( 2019 ) ; Yehudai & Shamir ( 2020 ) ; Fang et al . ( 2019 ) ; Su & Yang ( 2019 ) ; Oymak & Soltanolkotabi ( 2019 ) ) as well as for other algorithms ( e . g . Zhang et al . ( 2019 ) ; Wu et al . ( 2019b ) ; Cai et al . ( 2019 ) ; Wu et al . ( 2019a ) ; Zhong et al . ( 2017 ) ; Ge et al . ( 2019 ) ; van den Brand et al . ( 2020 ) ; Lee et al . ( 2020 ) ) . However , we are not aware of any theoretical works that study the momentum method in neural net training except the work Krichene et al . ( 2020 ) . Krichene et al . ( 2020 ) show that SGD with Polyak’s momentum ( a . k . a . stochastic Heavy Ball ) with inﬁnitesimal step size , i . e . η → 0 , for training a one - hidden - layer network with inﬁnite number of neurons , i . e . m → ∞ , converges to a stationary solution asymptotically . However , the asymptotic convergence result does not explain faster convergence of momentum . In this paper we consider the discrete time setting and consider nets with inﬁnite neurons as well as nets with ﬁnitely many neurons . We provide a non - asymptotic convergence rate of Polyak’s momentum , establishing a concrete improvement rel - ative to the best known rates for vanilla gradient descent . Our result follows the same framework as previous results , e . g . Du et al . ( 2019b ) ; Arora et al . ( 2019 ) ; Song & Yang ( 2019 ) . We study training a one - hidden - layer ReLU neural net of the form , N W ( x ) : = 1 √ m m (cid:88) r = 1 a r σ ( (cid:104) w ( r ) , x (cid:105) ) , ( 1 ) where σ ( z ) : = z · 1 { z ≥ 0 } is the ReLU activation , w ( 1 ) , . . . , w ( m ) ∈ R d are the weights of m neurons on the ﬁrst layer , a 1 , . . . , a m ∈ R are weights on the second layer , x ∈ R d is the input , and N ( x ) ∈ R is the output predicted on input x . Denote W : = { w ( r ) } mr = 1 . We consider empirical loss minimization with the squared loss , L ( W ) : = 12 (cid:80) ni = 1 (cid:0) y i − N W ( x i ) (cid:1) 2 , ( 2 ) where y i ∈ R is the label of sample x i and n is the number of samples . Following previous works of of ( Du et al . , 2019b ; Arora et al . , 2019 ; Song & Yang , 2019 ) , we deﬁne a Gram matrix H ∈ R n × n 2 for the weights W and its expectation ¯ H ∈ R n × n over the random draws of w ( r ) ∼ N ( 0 , I d ) ∈ R d whose ( i , j ) entries are deﬁned as follows , H ( W ) i , j : = 1 m m (cid:88) r = 1 x (cid:62) i x j 1 { (cid:104) w ( r ) , x i (cid:105) ≥ 0 & (cid:104) w ( r ) , x j (cid:105) ≥ 0 } ¯ H i , j : = E w ( r ) ∼ N ( 0 , I d ) [ x (cid:62) i x j 1 { (cid:104) w ( r ) , x i (cid:105) ≥ 0 & (cid:104) w ( r ) , x j (cid:105) ≥ 0 } ] . ( 3 ) We note that the matrix ¯ H is also called a neural tangent kernel ( NTK ) matrix in the literature ( e . g . Jacot et al . ( 2018 ) ; Yang ( 2019 ) ; Huang et al . ( 2020 ) ; Bietti & Mairal ( 2019 ) ) . Assume that the smallest eigenvalue λ : = λ min ( ¯ H ) is strictly positive and certain conditions about the step size and the number of neurons are satisﬁed . Previous works of ( Du et al . , 2019b ; Arora et al . , 2019 ; Song & Yang , 2019 ) were able to show that gradient descent decreases the empirical risk ( 2 ) at a linear rate 1 − ηλ 2 , i . e . L ( W t ) = (cid:0) 1 − ηλ 2 (cid:1) L ( W t − 1 ) . In this paper , following the same framework as ( Du et al . , 2019b ; Arora et al . , 2019 ; Song & Yang , 2019 ) , we show that gradient descent with Polyak’s momentum decreases the empirical risk at an accelerated linear rate 1 − (cid:113) ηλ 2 to a small additive error that is controllable by the number of neurons m . 2 This shows the combined advantage of Polyak’s momentum and overparameterization . As the number of neurons m and samples n approach inﬁnity , as considered in ( Krichene et al . , 2020 ) , our analysis shows that gradient descent with Polyak’s momentum converges to any arbitrarily small error at the accelerated rate . 2 P RELIMINARIES 2 . 1 P OLYAK ’ S MOMENTUM Algorithm 1 and Algorithm 2 show two equivalent presentations of gradient descent with Polyak’s momentum . Given the same initialization , one can show that Algorithm 1 and Algorithm 2 produce exact same iterates during optimization . We note that for the ReLU activation , it is not differentiable at zero . So for solving ( 2 ) , we replace the notion of gradient in Algorithm 1 and Algorithm 2 with subgradient ∂L ( W t ) ∂w ( r ) t : = 1 √ m (cid:80) ni = 1 (cid:0) N W t ( x i ) − y i (cid:1) a r · 1 [ (cid:104) w ( r ) t , x i (cid:105) ≥ 0 ] x i and update the neuron r as w ( r ) t + 1 = w ( r ) t − η ∂L ( W t ) ∂w ( r ) t + β (cid:0) w ( r ) t − w ( r ) t − 1 (cid:1) . The most common example in the literature that demonstrates the advantage of Polyak’s momentum over vanilla gradient descent is the strongly convex quadratic problem , min x ∈ R n 12 x (cid:62) Ax + b (cid:62) x , where A ∈ R n × n (cid:31) 0 n . Applying gradient descent with Polyak’s momentum ( Algorithm 2 ) to the problem , the iterate evolves according to the following dynamics , x t + 1 − x ∗ = ( I n − ηA ) ( x t − x ∗ ) + β ( x t − x ∗ ) − β ( x t − 1 − x ∗ ) , ( 4 ) where I n is the identity matrix , η is the step size , and x ∗ satisﬁes Ax ∗ = b , which is the unique minimizer of the quadratic problem . One can re - write the recursive dynamics ( 4 ) as follows , (cid:20) x t + 1 − x ∗ x t − x ∗ (cid:21) = (cid:20) I n − ηA + βI n − βI n I n 0 n (cid:21) · (cid:20) x t − x ∗ x t − 1 − x ∗ (cid:21) . ( 5 ) A known result ( see e . g . Lessard et al . ( 2016 ) ; Polyak ( 1987 ) ) is that under an optimal tuning of the momentum parameter β . The error decays at an accelerated linear rate (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) x t + 1 − x ∗ x t − x ∗ (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:0) 1 − (cid:112) ηλ n (cid:1) t (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) x t − x ∗ x t − 1 − x ∗ (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) , ( 6 ) where λ n is the smallest eigenvalues of A . On the other hand , gradient descent only has 1 − ηλ n convergence rate ( see e . g . Lessard et al . ( 2016 ) ) . In the next section , we will show that the dynamics induced in the neural network training by the momentum method is similar to the quadratic function 2 We borrow the term “accelerated linear rate” from the convex optimization literature ( Nesterov , 2013 ) , because the result here has an resemblance to those results in convex optimization , even though the neural network training is a non - convex problem . 3 case here ( i . e . ( 4 ) and ( 5 ) ) , modulo some small terms whose magnitudes are controllable . The similarity hints at why momentum helps faster neural network training . More related works of Polyak’s momentum : There is little theory work that shows any provable advantage of Polyak’s momentum in non - convex optimization and deep learning . Even in con - vex optimization , related works make additional assumptions to show a provable advantage over standard GD or SGD . Chen & Kolar ( 2020 ) study Polyak’s momentum under a growth condition . Sebbouh et al . ( 2020 ) show that SGD with Polyak’s momentum outperforms vanilla SGD in smooth convex optimization when the data is interpolated . On the other hand , for smooth non - convex opti - mization , Wang et al . ( 2020 ) show that Polyak’s momentum helps to escape saddle points faster and ﬁnd a second order stationary point faster . Yet , they also make certain assumptions regarding some statistical properties of gradient and momentum . There are also some efforts in using continuous time techniques to analyze a broad family of momentum methods that includes Polyak’s momentum ( see e . g . Diakonikolas & Jordan ( 2019 ) ; Maddison et al . ( 2018 ) ) . 2 . 2 A SSUMPTION AND PRIOR RESULT As described in the introduction section , we assume that that the smallest eigenvalue of the Gram matrix ¯ H ∈ R n × n is strictly positive , i . e . λ : = λ min ( ¯ H ) > 0 . Du et al . ( 2019b ) show that the strict positiveness assumption is indeed mild . Speciﬁcally , they show that if no two inputs are parallel , then the least eigenvalue is strictly positive . Panigrahi et al . ( 2020 ) were able to provide a quantitative lower bound under certain conditions . Following the same framework of ( Du et al . , 2019b ) , we consider that each weight vector w ( r ) ∈ R d is initialized according to normal distribution , w ( r ) ∼ N ( 0 , I d ) , and each a r ∈ R is sampled from Rademacher distribution , i . e . a r = 1 with probability 0 . 5 ; and a r = − 1 with probability 0 . 5 . We also assume (cid:107) x i (cid:107) ≤ 1 for all samples i . As the previous works ( e . g . Li & Liang ( 2018 ) ; Ji & Telgarsky ( 2020 ) ; Du et al . ( 2019b ; a ) ; Allen - Zhu et al . ( 2019 ) ; Song & Yang ( 2019 ) ; Zou et al . ( 2019 ) ; Arora et al . ( 2019 ) ; Zou & Gu ( 2019 ) ) , we consider only training the ﬁrst layer { w ( r ) } and the second layer { a r } is ﬁxed throughout the iterations . In the following , we denote u t ∈ R n whose i th entry is the network prediction for sample i ( i . e . u t [ i ] = N W t ( x i ) ) in iteration t and y ∈ R n is the vector whose i th is the label of sample i . Now let us state a prior result of gradient descent convergence due to ( Du et al . , 2019b ) . Theorem 1 . ( Theorem 4 . 1 in Du et al . ( 2019b ) ) Assume that λ : = λ min ( ¯ H ) > 0 and that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . Set the number of nodes m = Ω ( λ − 4 n 6 δ − 3 ) and the constant step size η = O ( λn 2 ) . Then , with probability at least 1 − δ over the random initialization , vanilla gradient descent , i . e . Algorithm 1 & 2 with β = 0 , has (cid:107) u t − y (cid:107) 2 ≤ (cid:18) 1 − ηλ 2 (cid:19) t · (cid:107) u 0 − y (cid:107) 2 . ( 7 ) We note that later Song & Yang ( 2019 ) improve the network size m to m = Ω ( λ − 4 n 4 log 3 ( n / δ ) ) while obtain the same convergence rate result of vanilla gradient descent . 3 M AIN RESULTS In this section , we ﬁrst state the main results and provide the intuition behind the results and detailed analysis in the later subsections . Theorem 2 . Assume that λ : = λ min ( ¯ H ) > 0 and that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . Fix some maximum number of iterations T , set a constant step size η = O ( λn 2 ) , ﬁx momentum parameter β = (cid:0) 1 − (cid:113) ηλ 2 (cid:1) 2 , and ﬁnally set a parameter ν ≥ 0 that controls the number of network nodes , chosen as m = Ω ( λ − 4 n 4 + 2 ν log 3 ( n / δ ) ) . Suppose that the step size η satisﬁes ηλ ≤ 12 and the number of samples n satisﬁes n 1 + ν λ − 1 = Ω ( T ) . Then , with probability at least 1 − δ over the random initialization , gradient descent with Polyak’s momentum ( Algorithm 1 & Algorithm 2 ) satisﬁes for any t ≤ T , (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) u t − y u t − 1 − y (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:32) 1 − (cid:114) ηλ 2 (cid:33) t · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) u 0 − y u − 1 − y (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) + 1 2 √ 2 n ν (cid:107) u 0 − y (cid:107) . ( 8 ) 4 Note that we have the initialization satisfy w 0 = w − 1 so that u 0 = u − 1 . The conditions ηλ ≤ 12 and that n 1 + ν λ − 1 = Ω ( T ) are easily satisﬁed when the number of samples n is sufﬁciently large and that n 1 + ν λ − 1 grows faster than T . On the other hand , for the factor (cid:0) 1 − (cid:113) ηλ 2 (cid:1) T to be small , the number of iterations T should satisfy T = Ω (cid:16)(cid:113) 1 ηλ (cid:17) = Ω (cid:0) nλ − 1 (cid:1) . Both conditions can be satisﬁed by appropriately setting the parameter ν > 0 , which in turn determines the number of neurons m . Theorem 2 states that Polyak’s momentum helps to reduce the initial error to a number 1 2 √ 2 n ν (cid:107) u 0 − y (cid:107) at the accelerated rate 1 − (cid:112) ηλ / 2 , under the optimal tuning of momentum parameter β = ( 1 − (cid:112) ηλ / 2 ) 2 . An interesting result of Theorem 2 is that it shows the beneﬁt of over - parametrization . By increasing the number of neurons m , gradient descent with Polyak’s momentum will be able to maintain the accelerated rate until it reduces the error to a smaller error . Speciﬁcally , one can control the error , i . e . the last term of ( 8 ) , by specifying the parameter ν . If ν = 1 , then by setting the number of neurons m = Ω ( λ − 4 n 6 log 3 ( n / δ ) ) , Polyak’s momentum can decrease the error at the accelerated rate to a number O ( 1 n ) (cid:107) u 0 − y (cid:107) = O ( 1 √ n ) , where we use that the initial error satisﬁes (cid:107) y − u 0 (cid:107) = O ( √ n ) ( see Lemma 8 in Appendix C ) . Similarly , if ν = 1 . 5 , then Polyak’s momentum can decrease the error at the accelerated rate to a number O ( 1 n 1 . 5 ) (cid:107) u 0 − y (cid:107) = O ( 1 n ) under the condition that m = Ω ( λ − 4 n 7 log 3 ( n / δ ) ) . Moreover , when the number of samples n approaches inﬁnity , and ν is chosen appropriately , the last term of ( 8 ) vanishes and we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) u t − y u t − 1 − y (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:32) 1 − (cid:114) ηλ 2 (cid:33) t · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) u 0 − y u − 1 − y (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) . ( 9 ) Compared to the related work ( Krichene et al . , 2020 ) that shows asymptotic convergence result of Polyak’s momentum for the neural network training in the mean ﬁeld limit , our convergence rate result clearly demonstrates the advantage of Polyak’s momentum . Our result also implies that over - parametrization helps acceleration in optimization . To our knowledge , in the literature , there is little theory of understanding why over - parametrization can help training a neural network faster . The only exception that we are aware of is ( Arora et al . , 2018 ) , which shows that the dynamics of vanilla gradient descent for an over - parametrized objective function exhibit some momentum terms , although their message is very different from ours . Remark ( comparison to vanilla gradient descent ) : According to Theorem 1 , we see that for the error (cid:107) u t − y (cid:107) to decrease to γ (cid:107) u 0 − y (cid:107) for some number γ ∈ ( 0 , 1 ) , vanilla gradient descent needs a number of iterations T g 0 (cid:117) log (cid:0) γ (cid:1) log (cid:0) 1 − ηλ 2 (cid:1) On the other hand , Heavy Ball takes a number of iterations T m 0 (cid:117) log (cid:0) γ / 2 (cid:1) log (cid:0) 1 − √ ηλ 2 (cid:1) for the error (cid:107) u t − y (cid:107) to decrease to γ (cid:107) u 0 − y (cid:107) . By comparing T g 0 and T m 0 , we have that T m 0 (cid:46) log (cid:0) 1 − ηλ 2 (cid:1) log (cid:0) 1 − (cid:113) ηλ 2 (cid:1) T g 0 = : θ (cid:18) ηλ 2 (cid:19) T g 0 , ( 10 ) Figure 1 : θ ( α ) : = log ( 1 − α ) log ( 1 −√ α ) vs . α . where in the last equality , we deﬁne θ ( ηλ 2 ) : = log (cid:0) 1 − ηλ 2 (cid:1) log (cid:0) 1 − √ ηλ 2 (cid:1) . To analyze θ ( ηλ 2 ) , we plot function value of θ ( α ) : = log ( 1 − α ) log ( 1 −√ α ) for various 0 < α < 1 on Figure 1 , which clearly shows that the θ ( α ) decays extremely fast as α decreases . We have that θ ( α ) (cid:117) 0 . 1 at α = 10 − 2 ; while θ ( α ) = 0 . 01 at α = 10 − 4 . Since ηλ 2 = O ( λ 2 n 2 ) is a very small number , we have that θ ( ηλ 2 ) is close to 0 and therefore inequality ( 10 ) suggests that T m 0 is small compared to T g 0 . For example if ηλ 2 = 10 − 4 , then T m 0 ≤ 0 . 01 · T g 0 , which shows that Polyak’s momentum makes fast progress . 5 3 . 1 M ORE NOTATIONS For analysis , let us deﬁne the event A ir : = { ∃ w ∈ R d : (cid:107) w − w ( r ) 0 (cid:107) ≤ R , 1 { x (cid:62) i w ( r ) 0 } (cid:54) = 1 { x (cid:62) i w ≥ 0 } } , where R > 0 is a number to be determined later . The event A ir means that there exists a w ∈ R d which is within the R - ball centered at the initial point w ( r ) 0 such that its activation pattern of sample i is different from that of w ( r ) 0 . We also denote a set S i : = { r ∈ [ m ] : 1 { A ir } = 0 } and its complementary set S ⊥ i : = [ m ] \ S i . Furthermore , we denote H t ∈ R n × n whose ( i , j ) entry is H ( W t ) i , j = 1 m (cid:80) mr = 1 x (cid:62) i x j 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . We will use the notation ξ ∈ R n whose i th entry is ξ t [ i ] : = u t [ i ] − y [ i ] , where u t [ i ] is the network prediction N t ( x i ) : = N W t ( x i ) at time t and y [ i ] is the true label of sample i . 3 . 2 I NTUITION OF THE RESULT Applying gradient descent with Polyak’s momentum to solving the objective ( 2 ) leads to the follow - ing dynamics of training errors , ξ t + 1 [ i ] = N t + 1 ( x i ) − y i = 1 √ m (cid:80) mr = 1 a r σ ( w ( r ) (cid:62) t + 1 x i ) − y i = 1 √ m m (cid:88) r = 1 a r (cid:16) w ( r ) (cid:62) t x i − η √ m a r (cid:80) nj = 1 ξ t [ j ] 1 [ w ( r ) (cid:62) t x j ≥ 0 ] x (cid:62) j x i + β ( w ( r ) t − w ( r ) t − 1 ) (cid:62) x i (cid:17) 1 [ w ( r ) (cid:62) t + 1 x i ≥ 0 ] − y i , ( 11 ) where the last equality is due to the update rule of the algorithm . Previous works like ( Du et al . , 2019a ; Arora et al . , 2019 ; Song & Yang , 2019 ) show that under certain conditions , the activation patterns of most of the neurons do not change , i . e . 1 [ w ( r ) (cid:62) t x j ≥ 0 ] = 1 [ w ( r ) (cid:62) 0 x j ≥ 0 ] for all t . Now to get an intuition why momentum helps , let us for a moment assume that the patterns of all neurons do not change during training . Then , one can replace 1 [ w ( r ) (cid:62) t + 1 x ≥ 0 ] and 1 [ w ( r ) (cid:62) t x ≥ 0 ] with 1 [ w ( r ) (cid:62) 0 x ≥ 0 ] for any neuron r in equation ( 11 ) , which leads to ξ t + 1 [ i ] = ξ t [ i ] + β ( ξ t [ i ] − ξ t − 1 [ i ] ) − η m n (cid:88) j = 1 m (cid:88) r = 1 ξ t [ j ] · 1 (cid:104) w ( r ) (cid:62) 0 x i ≥ 0 (cid:105) 1 (cid:104) w ( r ) (cid:62) 0 x j ≥ 0 (cid:105) x (cid:62) i x j = ξ t [ i ] + β ( ξ t [ i ] − ξ t − 1 [ i ] ) − ηH 0 [ i , : ] ξ t , ( 12 ) where in the last equality we use the deﬁnition of H 0 deﬁned in Subsection 3 . 1 . Apparently we can rewrite the above equation in a matrix form , ξ t + 1 = ( I n − ηH 0 ) ξ t + β ( ξ t − ξ t − 1 ) . ( 13 ) So now we see that equation ( 13 ) and ( 4 ) are in the same form , which implies that provably showing the beneﬁt of Polyak’s momentum for the neural network training is possible . However , one has to deal with the situation that some neurons do change their activation patterns during training . Lemma 1 below deals with this issue . Lemma 1 . ( Dynamics of the residual error ) : Following the notations deﬁned in Subsection 3 . 1 , suppose that for all t ∈ [ T ] and r ∈ [ m ] , (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R , for a number R > 0 . Then , gradient descent with Polyak’s momentum ( Algorithm 1 & Algorithm 2 ) for ( 2 ) has ξ t + 1 = ( I n − ηH t ) ξ t + β ( ξ t − ξ t − 1 ) + φ t , ( 14 ) where the i th entry of φ t ∈ R n satisﬁes | φ t [ i ] | ≤ 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) . The proof of Lemma 1 is available in Appendix A . The recursive dynamics of the residual vector ξ t , ( 14 ) , can be rewritten as (cid:20) ξ t + 1 ξ t (cid:21) = (cid:20) I n − ηH t + βI n − βI n I n 0 (cid:21) (cid:20) ξ t ξ t − 1 (cid:21) + (cid:20) φ t 0 (cid:21) . ( 15 ) In the later subsection , we will show that (cid:107) φ t (cid:107) is small and controllable . Speciﬁcally , we will use the following lemma to control (cid:107) φ t (cid:107) . 6 Lemma 2 . ( Claim 3 . 12 of Song & Yang ( 2019 ) ) Fix a number R 1 ∈ ( 0 , 1 ) . Deﬁne the event A ir : = { ∃ w : (cid:107) w − w ( r ) 0 (cid:107) ≤ R 1 , 1 { x (cid:62) i w ( r ) 0 } (cid:54) = 1 { x (cid:62) i w ≥ 0 } } . Denote the set S i : = { r ∈ [ m ] : 1 { A ir } = 0 } and the set S ⊥ i : = [ m ] \ S i . With probability at least 1 − n · exp ( − mR 1 ) , we have that for all i ∈ [ n ] , | S ⊥ i | ≤ 4 mR 1 . A similar lemma also appears in ( Du et al . , 2019b ) . Lemma 2 says that the number of neurons whose activation patterns for a sample i could change during the execution is only a small faction of m if R 1 is a small number , i . e . | S ⊥ i | ≤ 4 mR 1 (cid:28) m . In the later subsection , we will set R 1 = O ( λ n 1 + ν ) , which together with the upper - bound of | φ t [ i ] | ≤ 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) in Lemma 1 will allow us to control (cid:107) φ t (cid:107) . 3 . 3 D ETAILED ANALYSIS We ﬁrst upper - bound the spectral norm of the matrix (cid:20) I n − ηH t + βI n − βI n I n 0 (cid:21) on ( 15 ) as follows . Lemma 3 . Following the setting as Theorem 2 , set m = Ω ( λ − 2 n 2 log ( n / δ ) ) and the momentum parameter β = (cid:0) 1 − (cid:113) ηλ 2 (cid:1) 2 . Then , with probability at least 1 − δ − n 2 exp ( − m ¯ R / 10 ) , for any set of weight vectors W : = { w ( 1 ) , . . . , w ( m ) } satisfying (cid:107) w ( r ) − w ( r ) 0 (cid:107) ≤ ¯ R : = λ 8 n for any r ∈ [ m ] , it holds that (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) I n − ηH ( W ) + βI n − βI n I n 0 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) 2 ≤ 1 − (cid:113) ηλ 2 . The proof of Lemma 3 is available in Appendix B . Now we are ready to prove Theorem 2 . Proof . ( of Theorem 2 ) We will denote R : = λ 64 n 1 + ν , c T : = max t ≤ T β t ∗ ( 1 + β ∗ (cid:80) t − 1 s = 0 β s ∗ ) , C : = 16 √ 2 c T ηnR (cid:107) u 0 − y (cid:107) , and β ∗ : = 1 − (cid:113) ηλ 2 ≥ 12 . We will prove for all t ∈ [ T ] , the following inequalities hold (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤ β t ∗ · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) + C 1 − β ∗ ( 16 ) (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R and (cid:107) φ t (cid:107) ≤ C . ( 17 ) The proof is by induction . For the base case t = 0 , inequality ( 16 ) and the ﬁrst inequality of ( 17 ) trivially holds . It remains to bound (cid:107) φ 0 (cid:107) . With probability at least 1 − n · exp ( − mR ) : (cid:107) φ 0 (cid:107) = (cid:113)(cid:80) ni = 1 φ 0 [ i ] 2 = (cid:113)(cid:80) ni = 1 (cid:0) 2 η √ n | S ⊥ i | m (cid:107) u 0 − y (cid:107) (cid:1) 2 ( a ) ≤ (cid:113)(cid:80) ni = 1 ( 2 η √ n 4 mR m ) 2 (cid:0) (cid:107) u 0 − y (cid:107) (cid:1) 2 = 8 ηnR (cid:107) u 0 − y (cid:107) ≤ C , where the above inequality relies on Lemma 2 , so we have that | S ⊥ i | ≤ 4 mR for all i ∈ [ n ] . Now we can conclude that ( 16 ) and ( 17 ) hold for the base case 0 . Suppose that ( 16 ) and ( 17 ) hold at time s = 0 , 1 , 2 , . . . , t − 1 . Then , (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t ξ t − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ( 15 ) ≤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) I n − ηH t − 1 + βI n − βI n I n 0 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) 2 · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ t − 1 ξ t − 2 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) + (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) φ t − 1 0 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ( a ) ≤ β ∗ · (cid:13)(cid:13)(cid:13) (cid:13) (cid:20) ξ t − 1 ξ t − 2 (cid:21)(cid:13)(cid:13)(cid:13) (cid:13) + C ( b ) ≤ β t ∗ · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) + t − 1 (cid:88) s = 0 β t − 1 − s ∗ C ≤ β t ∗ · (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ξ 0 ξ − 1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13) + C 1 − β ∗ , ( 18 ) where ( a ) is by Lemma 3 and the induction that (cid:107) φ t − 1 (cid:107) ≤ C , ( b ) is by the recursive expansion of the second inequality . So ( 16 ) holds at t . 7 Using ( 18 ) , we now show that (cid:107) φ t (cid:107) ≤ C . We have that (cid:107) φ t (cid:107) = (cid:112)(cid:80) ni = 1 φ t [ i ] 2 ≤ (cid:113)(cid:80) ni = 1 (cid:0) 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1)(cid:1) 2 ( a ) ≤ 8 ηnR (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) ( b ) ≤ 8 ηnR (cid:0) β t ∗ √ 2 (cid:107) u 0 − y (cid:107) + C 1 − β ∗ + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:0) β s ∗ √ 2 (cid:107) u 0 − y (cid:107) + C 1 − β ∗ (cid:1)(cid:1) ( c ) ≤ 8 ηnR (cid:0) β t ∗ ( 1 + β ∗ (cid:80) t − 1 s = 0 β s ∗ ) √ 2 (cid:107) u 0 − y (cid:107) + C 1 − β ∗ ( 1 + β ∗ 1 − β ∗ ) (cid:1) ( d ) ≤ C , ( 19 ) where ( a ) we use Lemma 2 so that for all i ∈ [ n ] , it holds that | S ⊥ i | ≤ 4 mR , with probability at least 1 − n · exp ( − mR ) , ( b ) is by induction that (cid:107) u t − y (cid:107) ≤ β t ∗ √ 2 (cid:107) u 0 − y (cid:107) + C 1 − β ∗ as u 0 = u − 1 , ( c ) uses that β = β 2 ∗ , and ( d ) is due to that β t ∗ ( 1 + β ∗ (cid:80) t − 1 s = 0 β s ∗ ) ≤ c T and that 8 ηnRc T √ 2 (cid:107) u 0 − y (cid:107) 1 − 8 ηnR 1 − β ∗ ( 1 + β ∗ 1 − β ∗ ) ≤ C , ( 20 ) which is proved as follows . Using the deﬁnition of β ∗ and R , we have that 1 − 8 ηnR 1 − β ∗ ( 1 + β ∗ 1 − β ∗ ) ≥ 1 − 32 nRλ ≥ 12 . So for ( 20 ) to hold , it sufﬁces to have that 16 ηnRc T √ 2 (cid:107) u 0 − y (cid:107) ≤ C , which is true by the deﬁnition of C . Now we are going to show that (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R : = λ 64 n 1 + ν . We have that (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ( a ) ≤ η √ 2 n √ m (cid:0) 2 ηλ + 32 nRc T t λ (cid:1) (cid:107) y − u 0 (cid:107) ( b ) ≤ η √ 2 n √ m (cid:0) 4 ηλ (cid:1) (cid:107) y − u 0 (cid:107) ( c ) = η √ 2 n √ m (cid:0) 4 ηλ (cid:1) O ( (cid:113) n log ( m / δ ) log 2 ( n / δ ) ) ( d ) ≤ λ 64 n 1 + ν , ( 21 ) where ( a ) is due to Lemma 4 in Appendix C , ( b ) is because 32 nRc T t λ = c T t 2 n ν ≤ T 2 √ 2 ηλn ν ≤ 2 ηλ , where we use c T ≤ 1 √ 2 ηλ , which is shown in Lemma 9 in Appendix C , and the choice of step size η = O ( λn 2 ) as well as the condition that n 1 + ν λ − 1 = Ω ( T ) , ( c ) is due to Lemma 8 in Appendix C , which states that with probability at least 1 − δ / 3 , the initial error satisﬁes (cid:107) y − u 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) , and ( d ) is by the choice of the number of neurons m = Ω ( λ − 4 n 4 + 2 ν log 3 ( n / δ ) ) . So we can conclude that ( 17 ) holds at t . Furthermore , with the choice of m , we have that 3 n 2 exp ( − mR / 10 ) ≤ δ . Finally , Lemma 9 in Appendix C shows that C 1 − β ∗ ≤ 1 2 √ 2 n ν (cid:107) y − u 0 (cid:107) . Thus , we have completed the proof . 4 C ONCLUSION In this work , we show that Polyak’s momentum helps to accelerate training a one - layer ReLU net - work . The insight is that the dynamics of the predictions by the neural network during training is not very different from the accelerated dynamics in solving the strongly convex quadratic functions by the same method , provided that the weights of neural net do not move away from its initialization too much so that most of the activation patterns of the neurons remain the same during training . We note that in the literature , this is called training a neural net in the Neural Tangent Kernel ( NTK ) regime ( Jacot et al . , 2018 ) . Recent work of ( Nakkiran et al . , 2019 ) shows that during the early stage of training , the functions that a neural net learns are some simple functions of data , and then it starts learning more complicated functions after learning the simple one . Furthermore , Hu et al . ( 2020 ) suggest that during the early stage , the network training is indeed in the NTK regime . Therefore , a possible future work is combing our results and those of ( Nakkiran et al . , 2019 ; Hu et al . , 2020 ) to show that momentum helps to learn the simple functions faster . We hope that our work sheds light on explaining why the momentum method works well in practice . 8 R EFERENCES Ahmet Alacaoglu , Yura Malitsky , Panayotis Mertikopoulos , and Volkan Cevher . A new regret anal - ysis for adam - type algorithms . ICML , 2020 . Zeyuan Allen - Zhu , Yuanzhi Li , and Zhao Song . A convergence theory for deep learning via over - parameterization . ICML , 2019 . Andersen Ang . Heavy ball method on convex quadratic problem . Lecture note , 2018 . Sanjeev Arora , Nadav Cohen , and Elad Hazan . On the optimization of deep networks : Implicit acceleration by overparameterization . ICML , 2018 . Sanjeev Arora , Simon S Du , Wei Hu , Zhiyuan Li , and Ruosong Wang . Fine - grained analysis of optimization and generalization for overparameterized two - layer neural networks . NeurIPS , 2019 . Yu Bai and Jason D . Lee . Beyond linearization : On quadratic and higher - order approximation of wide neural networks . ICLR , 2020 . Alberto Bietti and Julien Mairal . On the inductive bias of neural tangent kernels . NeurIPS , 2019 . Alon Brutzkus and Amir Globerson . Globally optimal gradient descent for a convnet with gaussian inputs . ICML , 2017 . Tianle Cai , Ruiqi Gao , Jikai Hou , Siyu Chen , Dong Wang , Di He , Zhihua Zhang , and Liwei Wang . A gram - gauss - newton method learning overparameterized deep neural networks for regression problems . arXiv . org : 1905 . 11675 , 2019 . Bugra Can , Mert G ¨ urb ¨ uzbalaban , and Lingjiong Zhu . Accelerated linear convergence of stochastic momentum methods in wasserstein distances . ICML , 2019 . You - Lin Chen and Mladen Kolar . Understanding accelerated stochastic gradient descent via the growth condition . arXiv : 2006 . 06782 , 2020 . Lenaic Chizat , Edouard Oyallon , and Francis Bach . On lazy training in differentiable programming . NeurIPS , 2019 . Ashok Cutkosky and Francesco Orabona . Momentum - based variance reduction in non - convex sgd . NeurIPS , 2019 . Amit Daniely . Sgd learns the conjugate kernel class of the network . NeurIPS , 2017 . Amit Daniely . Memorizing gaussians with no over - parameterizaion via gradient decent on neural networks . arXiv : 1909 . 11837 , 2020 . Jelena Diakonikolas and Michael I . Jordan . Generalized momentum - based methods : A hamiltonian perspective . arXiv : 1906 . 00436 , 2019 . Simon S Du , Jason D Lee , Haochuan Li , Liwei Wang , , and Xiyu Zhai . Gradient descent ﬁnds global minima of deep neural networks . ICML , 2019a . Simon S . Du , Xiyu Zhai , Barnabas Poczos , and Aarti Singh . Gradient descent provably optimizes over - parameterized neural networks . ICLR , 2019b . Yonatan Dukler , Quanquan Gu , and Guido Montufar . Optimization theory for relu neural networks trained with normalization layers . ICML , 2020 . Cong Fang , Hanze Dong , and Tong Zhang . Over parameterized two - level neural networks can learn near optimal feature representations . arXiv : 1910 . 11508 , 2019 . Nicolas Flammarion and Francis Bach . From averaging to acceleration , there is only a step - size . COLT , 2015 . S´ebastien Gadat , Fabien Panloup , and Soﬁane Saadane . Stochastic heavy ball . arXiv : 1609 . 04228 , 2016 . 9 Rong Ge , Rohith Kuditipudi , Zhize Li , and Xiang Wang . Learning two - layer neural networks with symmetric inputs . ICLR , 2019 . Euhanna Ghadimi , Hamid Reza Feyzmahdavian , and Mikael Johansson . Global convergence of the heavy - ball method for convex optimization . ECC , 2015 . Behrooz Ghorbani , Song Mei , Theodor Misiakiewicz , , and Andrea Montanari . Linearized two - layers neural networks in high dimension . arXiv : 1904 . 12191 , 2019 . Igor Gitman , Hunter Lang , Pengchuan Zhang , and Lin Xiao . Understanding the role of momentum in stochastic gradient methods . NeurIPS , 2019 . Gabriel Goh . Why momentum really works . Distill , 2017 . Boris Hanin and Mihai Nica . Finite depth and width corrections to the neural tangent kernel . ICLR , 2020 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recog - nition . Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2016 . Elad Hoffer , Itay Hubara , and Daniel Soudry . Train longer , generalize better : closing the general - ization gap in large batch training of neural networks . NIPS , 2017 . Wei Hu , Lechao Xiao , Ben Adlam , and Jeffrey Pennington . The surprising simplicity of the early - time learning dynamics of neural networks . NeurIPS , 2020 . Kaixuan Huang , Yuqing Wang , Molei Tao , and Tuo Zhao . Why do deep residual networks generalize better than deep feedforward networks ? — a neural tangent kernel perspective . arXiv : 2002 . 06262 , 2020 . Arthur Jacot , Franck Gabriel , and Clement Hongler . Neural tangent kernel : Convergence and gen - eralization in neural networks . NeurIPS , 2018 . Ziwei Ji and Matus Telgarsky . Polylogarithmic width sufﬁces for gradient descent to achieve arbi - trarily small test error with shallow relu networks . ICLR , 2020 . Rahul Kidambi , Praneeth Netrapalli , Prateek Jain , and Sham M . Kakade . On the insufﬁciency of existing momentum schemes for stochastic optimization . ICLR , 2018 . Diederik P . Kingma and Jimmy Ba . Adam : A method for stochastic optimization . ICLR , 2015 . Walid Krichene , Kenneth F . Caluyay , and Abhishek Halder . Global convergence of second - order dynamics in two - layer neural networks . arXiv : 2006 . 07867 , 2020 . Alex Krizhevsky , Ilya Sutskever , and Geoffrey E . Hinton . Imagenet classiﬁcation with deep convo - lutional neural networks . NIPS , 2012 . Jaehoon Lee , Lechao Xiao , Samuel S . Schoenholz , Yasaman Bahri , Jascha Sohl - Dickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . NeurIPS , 2019 . Jason D . Lee , Ruoqi Shen , Zhao Song , Mengdi Wang , and Zheng Yu . Generalized leverage score sampling for neural networks . arXiv : 2009 . 09829 , 2020 . Laurent Lessard , Benjamin Recht , and Andrew Packard . Analysis and design of optimization algo - rithms via integral quadratic constraints . SIAM Journal on Optimization , 2016 . Yuanzhi Li and Yingyu Liang . Learning overparameterized neural networks via stochastic gradient descent on structured data . NeurIPS , 2018 . Yuanzhi Li and Yang Yuan . Convergence analysis of two - layer neural networks with relu activation . NeurIPS , 2017 . Yuanzhi Li , Tengyu Ma , and Hongyang Zhang . Learning over - parametrized two - layer relu neural networks beyond ntk . COLT , 2020 . 10 Chaoyue Liu and Mikhail Belkin . Accelerating sgd with momentum for over - parameterized learn - ing . ICLR , 2020 . Yanli Liu , Yuan Gao , and Wotao Yin . An improved analysis of stochastic gradient descent with momentum . arXiv : 2007 . 07989 , 2020 . Nicolas Loizou and Peter Richt´arik . Momentum and stochastic momentum for stochastic gradient , newton , proximal point and subspace descent methods . arXiv : 1712 . 09677 , 2017 . Nicolas Loizou and Peter Richt´arik . Accelerated gossip via stochastic heavy ball method . Allerton , 2018 . Ilya Loshchilov and Frank Hutter . Decoupled weight decay regularization . ICLR , 2019 . Liangchen Luo , Yuanhao Xiong , Yan Liu , and Xu Sun . Adaptive gradient methods with dynamic bound of learning rate . ICLR , 2019 . Chris J . Maddison , Daniel Paulin , Yee Whye Teh , Brendan O’Donoghue , and Arnaud Doucet . Hamiltonian descent methods . arXiv : 1809 . 05042 , 2018 . Preetum Nakkiran , Gal Kaplun , Dimitris Kalimeris , Tristan Yang , Benjamin L . Edelman , Fred Zhang , and Boaz Barak . Sgd on neural networks learns functions of increasing complexity . NeurIPS , 2019 . Yurii Nesterov . Introductory lectures on convex optimization : a basic course . Springer , 2013 . Samet Oymak and Mahdi Soltanolkotabi . Towards moderate overparameterization : global conver - gence guarantees for training shallow neural networks . arXiv : 1902 . 04674 , 2019 . Abhishek Panigrahi , Abhishek Shetty , and Navin Goyal . Effect of activation functions on the train - ing of overparametrized neural nets . ICLR , 2020 . Boris T . Polyak . Introduction to optimization . Optimization Software , 1987 . B . T . Polyak . Some methods of speeding up the convergence of iteration methods . USSR Computa - tional Mathematics and Mathematical Physics , 1964 . Benjamin Recht . Lyapunov analysis and the heavy ball method . Lecture note , 2018 . Sashank J . Reddi , Satyen Kale , and Sanjiv Kumar . On the convergence of adam and beyond . ICLR , 2018 . Michael Saunders . Notes on ﬁrst - order methods for minimizing smooth functions . Lecture note , 2018 . Damien Scieur and Fabian Pedregosa . Universal average - case optimality of polyak momentum . ICML , 2020 . Othmane Sebbouh , Robert M . Gower , and Aaron Defazio . On the convergence of the stochastic heavy ball method . arXiv : 2006 . 07867 , 2020 . Mahdi Soltanolkotabi . Learning relus via gradient descent . NeurIPS , 2017 . Zhao Song and Xin Yang . Quadratic sufﬁces for over - parametrization via matrix chernoff bound . arXiv : 1906 . 03593 , 2019 . Lili Su and Pengkun Yang . On learning over - parameterized neural networks : A functional approxi - mation perspective . NeurIPS , 2019 . Tao Sun , Penghang Yin , Dongsheng Li , Chun Huang , Lei Guan , and Hao Jiang . Non - ergodic convergence analysis of heavy - ball algorithms . AAAI , 2019 . Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initializa - tion and momentum in deep learning . ICML , 2013 . 11 Yuandong Tian . An analytical formula of population gradient for two - layered relu network and its applications in convergence and critical point analysis . ICML , 2017 . Jan van den Brand , Binghui Peng , Zhao Song , and Omri Weinstein . Training ( overparametrized ) neural networks in near - linear time . arXiv : 2006 . 11648 , 2020 . Ashish Vaswani , Noam Shazeer , Niki Parmar , and et al . Attention is all you need . NIPS , 2017 . Jun - Kun Wang , Chi - Heng Lin , and Jacob Abernethy . Escaping saddle points faster with stochastic momentum . ICLR , 2020 . Colin Wei , Jason D . Lee , Qiang Liu , and Tengyu Ma . Regularization matters : Generalization and optimization of neural nets v . s . their induced kernel . NeurIPS , 2019 . Ashia C Wilson , Rebecca Roelofs , Mitchell Stern , Nathan Srebro , , and Benjamin Recht . The marginal value of adaptive gradient methods in machine learning . NIPS , 2017 . Shanshan Wu , Alexandros G Dimakis , and Sujay Sanghavi . Learning distributions generated by one - layer relu networks . NeurIPS , 2019a . Xiaoxia Wu , Simon S Du , and Rachel Ward . Global convergence of adaptive gradient methods for an over - parameterized neural network . arXiv : 1902 . 07111 , 2019b . Greg Yang . Scaling limits of wide neural networks with weight sharing : Gaussian process behavior , gradient independence , and neural tangent kernel derivation . arXiv : 1902 . 04760 , 2019 . Tianbao Yang , Qihang Lin , and Zhe Li . Uniﬁed convergence analysis of stochastic momentum methods for convex and non - convex optimization . IJCAI , 2018 . Gilad Yehudai and Ohad Shamir . Learning a single neuron with gradient methods . COLT , 2020 . Guodong Zhang , James Martens , and Roger B Grosse . Fast convergence of natural gradient descent for over - parameterized neural networks . NeurIPS , 2019 . Kai Zhong , Zhao Song , Prateek Jain , Peter L . Bartlett , and Inderjit S . Dhillon . Recovery guarantees for one - hidden - layer neural networks . ICML , 2017 . Difan Zou and Quanquan Gu . An improved analysis of training over - parameterized deep neural networks . NeurIPS , 2019 . Difan Zou , Yuan Cao , Dongruo Zhou , and Quanquan Gu . Stochastic gradient descent optimizes overparameterized deep relu networks . Machine Learning , Springer , 2019 . A P ROOF OF L EMMA 1 Lemma 1 : ( Dynamics of the residual error ) : Following the notations deﬁned in Subsection 3 . 1 , suppose that for all t ∈ [ T ] and r ∈ [ m ] , (cid:107) w ( r ) t − w ( r ) 0 (cid:107) ≤ R , for a number R > 0 . Then , gradient descent with Polyak’s momentum ( Algorithm 1 & Algorithm 2 ) for ( 2 ) has ξ t + 1 = ( I n − ηH t ) ξ t + β ( ξ t − ξ t − 1 ) + φ t , ( 22 ) where the i th entry of φ t ∈ R n satisﬁes | φ t [ i ] | ≤ 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β (cid:80) t − 1 s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) . Proof . For each sample i , we will divide the contribution to N ( x i ) into two groups . N ( x i ) = 1 √ m m (cid:88) r = 1 a r σ ( (cid:104) w ( r ) , x i (cid:105) ) = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) , x i (cid:105) ) + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) , x i (cid:105) ) . ( 23 ) 12 To continue , let us recall some notations ; the subgradient with respect to w ( r ) ∈ R d is ∂L ( W ) ∂w ( r ) : = 1 √ m n (cid:88) i = 1 (cid:0) N ( x i ) − y i (cid:1) a r x i 1 { (cid:104) w ( r ) , x (cid:105) ≥ 0 } . ( 24 ) and the Gram matrix H t whose ( i , j ) element is H t [ i , j ] : = 1 mx (cid:62) i x j m (cid:88) r = 1 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 25 ) Let us also denote H ⊥ t [ i , j ] : = 1 mx (cid:62) i x j (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 26 ) We have that ξ t + 1 [ i ] = N t + 1 ( x i ) − y i ( 23 ) = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − y i ( 27 ) For the ﬁrst term above , we have that 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) (cid:124) (cid:123)(cid:122) (cid:125) ﬁrst term = 1 √ m (cid:88) r ∈ S i a r σ ( (cid:104) w ( r ) t − η ∂L ( W t ) ∂w ( r ) t + β ( w ( r ) t − w ( r ) t − 1 ) , x i (cid:105) ) = 1 √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t − η ∂L ( W t ) ∂w ( r ) t + β ( w ( r ) t − w ( r ) t − 1 ) , x i (cid:105) · 1 { (cid:104) w ( r ) t + 1 , x i (cid:105) ≥ 0 } ( a ) = 1 √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t , x i (cid:105) · 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t , x i (cid:105) · 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S i a r (cid:104) w ( r ) t − 1 , x i (cid:105) · 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } − η 1 √ m (cid:88) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } = N t ( x i ) + β (cid:0) N t ( x i ) − N t − 1 ( x i ) (cid:1) − 1 √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t − 1 , x i (cid:105) 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } (cid:1) − η 1 √ m (cid:88) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } (cid:124) (cid:123)(cid:122) (cid:125) last term ( 28 ) where ( a ) uses that for r ∈ S i , 1 { (cid:104) w ( r ) t + 1 , x i (cid:105) ≥ 0 } = 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } = 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } as the neurons in S i do not change their activation patterns . We can further bound ( 28 ) as ( b ) = N t ( x i ) + β (cid:0) N t ( x i ) − N t − 1 ( x i ) (cid:1) − η n (cid:88) j = 1 (cid:0) N t ( x j ) − y j (cid:1) H ( W t ) i , j − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } − 1 √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } − β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } + β √ m (cid:88) r ∈ S ⊥ i a r (cid:104) w ( r ) t − 1 , x i (cid:105) 1 { (cid:104) w ( r ) t − 1 , x i (cid:105) ≥ 0 } (cid:1) , ( 29 ) 13 where ( b ) is due to that 1 √ m (cid:80) r ∈ S i a r (cid:104) ∂L ( W t ) ∂w ( r ) t , x i (cid:105) 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 } (cid:124) (cid:123)(cid:122) (cid:125) last term = 1 m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } = n (cid:88) j = 1 (cid:0) N t ( x j ) − y j (cid:1) H ( W t ) i , j − 1 m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } . ( 30 ) Combining ( 27 ) and ( 29 ) , we have that ξ t + 1 [ i ] = ξ t [ i ] + β (cid:0) ξ t [ i ] − ξ t − 1 [ i ] (cid:1) − η n (cid:88) j = 1 H t [ i , j ] ξ t [ j ] − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) . ( 31 ) So we can write the above into a matrix form . ξ t + 1 = ( I n − ηH t ) ξ t + β ( ξ t − ξ t − 1 ) + φ t , ( 32 ) where the i element of φ t ∈ R n is deﬁned as φ t [ i ] = − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i (cid:8) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:9) . ( 33 ) 14 Now let us bound φ t [ i ] as follows . φ t [ i ] = − η m n (cid:88) j = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:88) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } + 1 √ m (cid:88) r ∈ S ⊥ i (cid:8) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − βa r σ ( (cid:104) w ( r ) t , x i (cid:105) ) + βa r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:9) ( a ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + 1 √ m (cid:88) r ∈ S ⊥ i (cid:0) (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107) + β (cid:107) w ( r ) t − w ( r ) t − 1 (cid:107) (cid:1) ( b ) = η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ m (cid:88) r ∈ S ⊥ i (cid:0) (cid:107) t (cid:88) s = 0 β t − s ∂L ( W s ) ∂w ( r ) s (cid:107) + β (cid:107) t − 1 (cid:88) s = 0 β t − 1 − s ∂L ( W s ) ∂w ( r ) s (cid:107) (cid:1) ( c ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ m (cid:88) r ∈ S ⊥ i (cid:0) t (cid:88) s = 0 β t − s (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) (cid:1) ( d ) ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) + η √ n | S ⊥ i | m (cid:0) t (cid:88) s = 0 β t − s (cid:107) u s − y (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) = 2 η √ n | S ⊥ i | m (cid:0) (cid:107) u t − y (cid:107) + β t − 1 (cid:88) s = 0 β t − 1 − s (cid:107) u s − y (cid:107) (cid:1) , ( 34 ) where ( a ) is because − ηm (cid:80) nj = 1 x (cid:62) i x j ( N t ( x j ) − y j ) (cid:80) r ∈ S ⊥ i 1 { (cid:104) w ( r ) t , x i (cid:105) ≥ 0 & (cid:104) w ( r ) t , x j (cid:105) ≥ 0 } ≤ η | S ⊥ i | m (cid:80) nj = 1 | N t ( x j ) − y j | ≤ η √ n | S ⊥ i | m (cid:107) u t − y (cid:107) , and that σ ( · ) is 1 - Lipschitz so that 1 √ m (cid:80) r ∈ S ⊥ i (cid:0) a r σ ( (cid:104) w ( r ) t + 1 , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) (cid:1) ≤ | S ⊥ i | √ m | (cid:104) w ( r ) t + 1 , x i (cid:105) − (cid:104) w ( r ) t , x i (cid:105) | ≤ | S ⊥ i | √ m (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107)(cid:107) x i (cid:107) ≤ | S ⊥ i | √ m (cid:107) w ( r ) t + 1 − w ( r ) t (cid:107) ; similarly , − β √ m (cid:80) r ∈ S ⊥ i (cid:0) a r σ ( (cid:104) w ( r ) t , x i (cid:105) ) − a r σ ( (cid:104) w ( r ) t − 1 , x i (cid:105) ) (cid:1) ≤ β | S ⊥ i | √ m (cid:107) w ( r ) t − w ( r ) t − 1 (cid:107) , ( b ) is by the update rule ( Algorithm 1 ) , ( c ) is by Jensen’s inequality , ( d ) is because | ∂L ( W s ) ∂w ( r ) s | = | 1 √ m (cid:80) ni = 1 (cid:0) u s [ i ] − y i (cid:1) a r x i 1 { x (cid:62) w ( r ) t ≥ 0 } | ≤ √ nm (cid:107) u s − y (cid:107) . B P ROOF OF L EMMA 3 Lemma 3 : Following the setting as Theorem 2 , set m = Ω ( λ − 2 n 2 log ( n / δ ) ) and the momentum parameter β = (cid:0) 1 − (cid:113) ηλ 2 (cid:1) 2 . Then , with probability at least 1 − δ − n 2 · exp ( − m ¯ R / 10 ) , for any set of weight vectors W : = { w ( 1 ) , . . . , w ( m ) } satisfying (cid:107) w ( r ) − w ( r ) 0 (cid:107) ≤ ¯ R : = λ 8 n for any r ∈ [ m ] , it holds that (cid:107) (cid:20) I n − ηH ( W ) + βI n − βI n I n 0 (cid:21) (cid:107) 2 ≤ 1 − (cid:113) ηλ 2 . Proof . Denote M : = (cid:107) (cid:20) I n − ηH ( W ) + βI n − βI n I n 0 (cid:21) on ( 15 ) . Denote λ ( 1 ) ≥ λ ( 2 ) ≥ · · · ≥ λ ( n ) eigenvalues of H in a decreasing order . To obtain the spectral norm M , it sufﬁces to consider the spectral norm of the sub - matrix M k : = (cid:107) (cid:20) 1 − ηλ ( k ) + β − β 1 0 (cid:21) (cid:107) 2 ∈ R 2 × 2 for each k ∈ [ n ] , and one will have M = max k { M k } , which is a known technique in the literature ( see e . g . Saunders ( 2018 ) ; Ang ( 2018 ) ; Recht ( 2018 ) ) . The eigenvalues of the 2 × 2 matrix is given by the roots of p ( k ) : = z 2 − ( 1 + β − ηλ ( k ) ) z + β . It can be shown that when β is at least ( 1 − (cid:112) ηλ ( k ) (cid:1) 2 , then 15 the magnitude of the roots of each p ( k ) are at most √ β ( see Lemma 5 in Appendix C ) . It remains to bound λ ( n ) . We have that λ ( n ) : = λ min ( H ( W ) ) ≥ λ min ( H 0 ) − (cid:107) H 0 − H ( W ) (cid:107) F ≥ 34 λ − 14 λ ≥ λ 2 , ( 35 ) where in the second to last inequality , we use Lemma 6 in Appendix C , which states that with probability at least 1 − δ , the smallest eigenvalue satisﬁes λ min ( H 0 ) ≥ 34 λ under the condition of the number of neurons , i . e . m = Ω ( λ − 2 n 2 log ( n / δ ) ) , and we also use Lemma 7 in Appendix C , which shows that if (cid:107) w ( r ) − w ( r ) 0 (cid:107) ≤ ¯ R : = λ 8 n for all r ∈ [ m ] , then with probability at least 1 − n 2 exp ( − m ¯ R 10 ) , it holds that (cid:107) H 0 − H ( W ) (cid:107) F ≤ 2 n ¯ R = 2 n λ 8 n = λ 4 . Since ηλ ( k ) (cid:28) 1 for any k ∈ [ n ] , we have that 1 − (cid:112) ηλ ( k ) < 1 for any k ∈ [ n ] ; consequently M = max k { M k } = 1 − (cid:112) ηλ ( n ) under the choice of β . Therefore , we have that M = 1 − (cid:112) ηλ ( n ) ( 35 ) ≤ 1 − (cid:113) ηλ 2 . C S OME SUPPORTING LEMMAS We will also need the following lemma , which shows that the iterate during the execution of the algorithm is not far away from its initialization . Similar results appear in the previous works ( e . g . Li & Liang ( 2018 ) ; Ji & Telgarsky ( 2020 ) ; Du et al . ( 2019b ; a ) ; Allen - Zhu et al . ( 2019 ) ; Song & Yang ( 2019 ) ; Zou et al . ( 2019 ) ; Arora et al . ( 2019 ) ; Zou & Gu ( 2019 ) ) . Lemma 4 . Following the setting as Theorem 2 , if for any s ≤ t , the residual dynamics satisﬁes (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ β s ∗ · (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + C 1 − β ∗ , where C : = 16 √ 2 ηnRc T (cid:107) u 0 − y (cid:107) , then we have that (cid:107) w ( r ) t + 1 − w ( r ) 0 (cid:107) ≤ η √ 2 n √ m C β ∗ , t (cid:107) y − u 0 (cid:107) , for all r ∈ [ m ] , where C β ∗ , t : = 1 ( 1 − β ∗ ) 2 + 16 ηnRc T ( t + 1 ) ( 1 − β ∗ ) 2 ≤ 2 ηλ + 32 nRc T ( t + 1 ) λ . Proof . (cid:107) w ( r ) t + 1 − w ( r ) 0 (cid:107) ( a ) ≤ η t (cid:88) s = 0 (cid:107) m ( r ) s (cid:107) ( b ) = η t (cid:88) s = 0 (cid:107) s (cid:88) τ = 0 β s − τ ∂L ( W τ ) ∂w ( r ) τ (cid:107) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ (cid:107) ∂L ( W τ ) ∂w ( r ) τ (cid:107) ( c ) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ √ n √ m (cid:107) y − u τ (cid:107) ( d ) ≤ η t (cid:88) s = 0 s (cid:88) τ = 0 β s − τ √ n √ m ( β τ ∗ √ 2 (cid:107) y − u 0 (cid:107) + C 1 − β ∗ ) ( e ) ≤ η √ 2 n √ m t (cid:88) s = 0 β s ∗ 1 − β ∗ (cid:107) y − u 0 (cid:107) + η √ nC √ m ( 1 − β ∗ ) t (cid:88) s = 0 s (cid:88) τ = 0 β 2 ( s − τ ) ∗ ( f ) ≤ η √ 2 n √ m 1 ( 1 − β ∗ ) 2 (cid:107) y − u 0 (cid:107) + η √ n (cid:0) 16 √ 2 ηnRc T (cid:107) u 0 − y (cid:107) (cid:1) ( t + 1 ) √ m ( 1 − β ∗ ) ( 1 − β 2 ∗ ) ( g ) = η √ 2 n √ m C β ∗ , t (cid:107) y − u 0 (cid:107) , ( 36 ) where ( a ) , ( b ) is by the update rule of momentum , which is w ( r ) t + 1 − w ( r ) t = − ηm ( r ) t , where m ( r ) t : = (cid:80) ts = 0 β t − s ∂L ( W s ) ∂w ( r ) s , ( c ) is because (cid:107) ∂L ( W s ) ∂w ( r ) s (cid:107) = (cid:107) (cid:80) ni = 1 ( y i − u s [ i ] ) 1 √ m a r x i · 1 { (cid:104) w ( r ) s , x (cid:105) ≥ 0 } (cid:107) ≤ 1 √ m (cid:80) ni = 1 | y i − u s [ i ] | ≤ √ n √ m (cid:107) y − u s (cid:107) 2 , ( d ) is due to the assumption that (cid:107) (cid:20) ξ s ξ s − 1 (cid:21) (cid:107) ≤ β s ∗ · (cid:107) (cid:20) ξ 0 ξ − 1 (cid:21) (cid:107) + C 1 − β ∗ , ( e ) is because that β = β 2 ∗ , ( f ) is by (cid:80) ∞ k = 1 kθ k = θ ( 1 − θ ) 2 for any θ ∈ [ 0 , 1 ) , 16 and ( g ) we denote C β ∗ , t : = 1 ( 1 − β ∗ ) 2 + 16 ηnRc T ( t + 1 ) ( 1 − β ∗ ) 2 . Finally , by using that β ∗ : = ( 1 − (cid:113) ηλ 2 ) , we have that C β ∗ , t ≤ 2 ηλ + 32 nRc T ( t + 1 ) λ . The proof is completed . Lemma 5 . The roots of the characteristics equation , z 2 − ( 1 + β − ηλ k ) z + β = 0 , have magnitude | z | ≤ √ β , if β ≥ ( 1 − √ ηλ k ) 2 . Proof . The roots of z 2 − ( 1 + β − ηλ k ) z + β = 0 are z = 1 + β − ηλ k ± √ ( 1 + β − ηλ k ) 2 − 4 β 2 . The magnitude of the roots are the same when the roots are imaginary , which is | z | = (cid:113) ( 1 + β − ηλ k ) 2 + 4 β − ( 1 + β − ηλ k ) 2 4 = √ β . Simple calculation shows that ( 1 + β − ηλ k ) 2 − 4 β ≤ 0 if β ≥ ( 1 − √ ηλ k ) 2 . Lemma 6 . ( Lemma 3 . 1 in Du et al . ( 2019b ) and Song & Yang ( 2019 ) ) Denote λ : = λ min ( ¯ H ) . Set m = Ω ( λ − 2 n 2 log ( n / δ ) ) . Suppose that ˜ w 1 , . . . , ˜ w m are i . i . d . generated N ( 0 , I d ) . Then , it holds that (cid:107) H ( ˜ W ) − ¯ H (cid:107) F ≤ λ 4 and λ min (cid:0) H ( ˜ W ) (cid:1) ≥ 34 λ , with probability at least 1 − δ . Lemma 7 . ( Lemma 3 . 2 in Song & Yang ( 2019 ) ) Fix a number R 0 ∈ ( 0 , 1 ) . Suppose that ˜ w 1 , . . . , ˜ w m are i . i . d . generated N ( 0 , I d ) . Then , for any set of weight vectors w 1 , . . . , w m ∈ R d that satisfy for any r ∈ [ m ] , (cid:107) ˜ w r − w r (cid:107) 2 ≤ R 0 , it holds that (cid:107) H ( ˜ W ) − H ( W ) (cid:107) F < 2 nR 0 , with probability at least 1 − n 2 · exp ( − mR 0 / 10 ) , Lemma 8 . ( Claim 3 . 10 in Song & Yang ( 2019 ) ) Assume that w ( r ) 0 ∼ N ( 0 , I d ) and a r uniformly sampled from { − 1 , 1 } . For 0 < δ < 1 , we have that (cid:107) y − u 0 (cid:107) 2 = O ( n log ( m / δ ) log 2 ( n / δ ) ) , with probability at least 1 − δ . Lemma 9 . Denote R : = λ 64 n 1 + ν , c T : = max t ≤ T β t ∗ ( 1 + β ∗ (cid:80) t − 1 s = 0 β s ∗ ) , C : = 16 √ 2 c T ηnR (cid:107) u 0 − y (cid:107) , and β ∗ : = 1 − (cid:113) ηλ 2 . Then , c T ≤ 1 4 β ∗ √ ηλ 2 and C 1 − β ∗ ≤ 1 4 √ 2 β ∗ n ν (cid:107) y − u 0 (cid:107) . Furthermore , if ηλ ≤ 12 , then β ∗ ≥ 12 ; and consequently , c T ≤ 1 √ 2 ηλ C 1 − β ∗ ≤ 1 2 √ 2 n ν (cid:107) y − u 0 (cid:107) . Proof . We have that C 1 − β ∗ = √ 24 ηλc T (cid:107) u 0 − y (cid:107) n ν (cid:113) ηλ 2 = c T √ ηλ 2 n ν (cid:107) u 0 − y (cid:107) . ( 37 ) So it remains to bound c T : = max t ≤ T β t ∗ ( 1 + β ∗ (cid:80) t − 1 s = 0 β s ∗ ) . Let us denote x : = β t ∗ . Note that x ≤ 1 . Consider maximize h ( x ) : = x ( 1 + β ∗ 1 − x 1 − β ∗ ) = x 1 − β ∗ − β ∗ 1 − β ∗ x 2 . The derivative is ∇ h ( x ) = 1 1 − β ∗ − 2 β ∗ 1 − β ∗ x . So the maximal value is at x = 12 β ∗ and we have that h ( 12 β ∗ ) = 1 4 β ∗ ( 1 − β ∗ ) = 1 4 ( 1 − √ ηλ 2 ) √ ηλ 2 . So we have that c T ≤ 1 4 β ∗ √ ηλ 2 . Substituting it back to ( 37 ) , we have that C 1 − β ∗ ≤ 1 4 √ 2 β ∗ n ν (cid:107) y − u 0 (cid:107) . ( 38 ) 17