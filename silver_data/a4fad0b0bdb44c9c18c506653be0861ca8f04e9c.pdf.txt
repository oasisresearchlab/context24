Understanding Visual Investigation Paterns Through Digital “Field” Observations Irene Rae irenerae @ databricks . com Databricks ( work done at Google ) San Francisco , California , USA Martin Bilsing mbilsing @ google . com Google Mountain View , California , USA ABSTRACT An extensive body of work in visual analytics has examined how users conduct analyses in scientifc and academic settings , identify - ing and categorizing user goals and the actions they undertake to achieve them . However , most of this work has studied the analysis process in simulated or isolated environments , leading to a gap in connecting these fndings to large - scale business ( enterprise ) contexts , where visual analysis is most needed to make sense of the large amounts of data being generated . In this work , we con - ducted digital " feld " observations to understand how users conduct visual analyses in an enterprise setting , where they operate within a large ecosystem of systems and people . From these observations , we identifed four common objectives , six recurring visual inves - tigation patterns , and fve emergent themes . We also performed a quantitative analysis of logs over 2530 user sessions from a sec - ond visual analysis product to validate that our patterns were not product - specifc . CCS CONCEPTS • Human - centered computing → Visual analytics . KEYWORDS visual interaction , visual analytics , interaction , interaction tech - niques , information visualization ACM Reference Format : Irene Rae , Feng Zhou , Martin Bilsing , and Phillip Bunge . 2022 . Understand - ing Visual Investigation Patterns Through Digital “Field” Observations . In CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) , April 29 - May 5 , 2022 , New Orleans , LA , USA . ACM , New York , NY , USA , 16 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3517445 1 INTRODUCTION Visual analytics tools support users in synthesizing large amounts of information through interactive visualizations , allowing them to This work is licensed under a Creative Commons Attribution International 4 . 0 License . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA © 2022 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9157 - 3 / 22 / 04 . https : / / doi . org / 10 . 1145 / 3491102 . 3517445 Feng Zhou fezhou @ google . com Google Mountain View , California , USA Phillip Bunge bunge @ google . com Google Mountain View , California , USA explore their data , discover relationships between elements , and generate or confrm hypotheses . While these systems may assist users in their explorations , the user’s role in making sense of the data is crucial for ensuring that any conclusions are relevant and actionable , particularly in domain - specifc contexts . Unfortunately , the time users must devote to reaching these conclusions is one of the most expensive parts of the process . Additionally , while the increasing volume of data collection may enable insights of greater breadth or depth , it also increases the cost of perceiving , navigating , and making sense of that data as we reach the limits of the user’s perceptual system [ 1 ] . These perceptual limitations mean that visual analysis tools must provide increasingly sophisticated support to allow users to scale their eforts . To meet this urgent need , prior work has devoted numerous eforts to developing conceptual spaces [ 41 , 46 ] , frameworks [ 3 , 33 ] , models [ 43 ] , typologies [ 9 , 19 ] , and taxonomies [ 11 , 14 , 17 , 24 , 31 , 47 , 49 – 51 , 53 , 56 ] that identify and classify user objectives and actions . The intention of these past eforts has been to facilitate innovation by providing generalizable ways of understanding the space . For example , prior work has used these methods to identify analytic gaps in current visualization tools [ 3 ] , locate design tensions and target qualities [ 19 ] , predict and anticipate a user’s next actions [ 5 ] or task performance [ 10 ] , and suggest better paths to users [ 16 , 23 ] . Although this work has established a thorough theoretical ground - ing and helped to defne the problem space , these studies have pri - marily been undertaken in scientifc or academic contexts . While these contexts have enabled detailed analyses by keeping many variables consistent , they sufer from the following limitations : ( 1 ) Synthetic or non - domain datasets . In a majority of prior studies , participants have been introduced to a dataset they had no familiarity or domain expertise with—previously identifed as a critical factor for efectively generating in - sights [ 38 ] . ( 2 ) Limited data size . Many of the datasets cited in prior work have been in the order of tens to hundreds of megabytes in size , substantially smaller than what is encountered in large - scale business ( enterprise ) settings . ( 3 ) Lack of tooling experience . Participants have usually needed training on the product prior to the study , which may have had efects on their strategies and behaviors . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . ( 4 ) Isolation of context . Tool , task , and social factors were assumed to be isolated in previous studies . However , in en - terprise settings , systems and people generally exist as part of a larger ecosystem where users may : ( a ) Leverage other tools , resources , or subject - matter experts as part of their process . ( b ) Work with disparate datasets that are inconsistently avail - able between visual analysis tools . ( c ) Have multiple responsibilities that reduce their ability to conduct open - ended explorations without an initiating goal or hypothesis . These limitations make it difcult to reason about how well past fndings match user needs in large business environments and suggest the following research question : how does our cur - rent understanding of user objectives , actions , and how to support them generalize to an enterprise setting , particularly within a larger ecosystem [ 55 ] , where multiple people and products might be uti - lized ? Our approach extends prior literature , providing support for pre - viously identifed user goals and actions as well as presenting new patterns , themes , and observed behaviors . We accomplished this by frst conducting digital “feld” observations , revealing higher - level objectives , the patterns of actions users took to achieve those ob - jectives , and themes generated from the way users interacted with their full ecosystem . We also validated these patterns through an analysis of logs from a diferent product , examining 2530 user anal - ysis sessions over an 8 - day period . Our work ofers the following contributions : ( 1 ) Real - world tasks and context through digital “feld” observations . We describe a variant of feld observations , tailored for modern digital environments where work is pre - dominantly online and users are not co - located . We use this modifed method to expand on prior work , exploring user objectives , analysis patterns , and strategies in a real - world context where users performed day - to - day investigations in - situ , within their own ecosystems of products and col - leagues . ( 2 ) Four user objectives . We present four common user ob - jectives observed across 9 users and 15 analysis sessions , sourced from 8 teams working across four diferent domains and datasets . We explain when these objectives corroborate prior fndings and where we found contextual diferences . ( 3 ) Six visual investigation patterns . We matched three pat - terns identifed in prior work : ( 1 ) Drilldown [ 17 , 23 ] ; ( 2 ) Sawtooth [ 5 , 23 , 26 , 30 , 38 , 42 , 43 , 54 ] ; and ( 3 ) Side - by - side ( compare ) [ 48 , 53 , 56 ] and present three new patterns of low - level actions . ( 4 ) Validation of pattern generalizability . We validate that these patterns are not product - dependent through sequential pattern mining of 2530 user analysis session logs from a second ( diferent ) visual analytics tool . ( 5 ) Five emergent themes . We present fve themes , derived from observing the collaborative interactions and operations users undertook within their ecosystem that , to our knowl - edge , have not been reported in prior work . Next , we contextualize how our approach connects to prior fnd - ings in the visual analytics space . 2 RELATED WORK A variety of terminology in prior literature has been used in over - lapping and disparate ways , leading to additional contributions that have sought to survey past usage and align meanings [ 12 , 41 , 54 ] . To reduce confusion and structure our conversation , we borrow from the approach of others in using a hierarchy to describe the relationship between what users are trying to do ( the " why " [ 9 ] ) and how they are doing it ( the " how " [ 6 , 9 , 24 ] ) : • Objective : Previously described as the " highest level ana - lytic goal " [ 24 ] , in this work we further specify that this is what the user is trying to accomplish , such that after it has been achieved , the user will consider themselves fnished and free to context switch to work on something else . • Action : " An atomic analytic step performed by a user [ 24 ] " interacting with the interface , such as a mouse click or a key press . • Pattern : A series of actions performed repeatedly by the user to achieve their objective [ 5 , 23 ] . Having explained the defnitions we will be using for these terms , we next provide an overview of prior literature relevant to each . 2 . 1 Objectives What visual analytic users are trying to accomplish has been de - scribed as user goals [ 6 , 32 , 46 , 46 ] , tasks [ 24 ] , or objectives [ 41 ] . With each term , the concept has been abstracted at diferent levels in an efort to fnd generalizability across domains and contexts . Past taxonomies and frameworks have described user objectives that range from " insight generation " [ 39 , 45 ] ; to presentation [ 9 , 32 ] or describing an observation ( item vs . aggregate ) [ 33 ] ; to the evalu - ation or confrmation of a hypothesis [ 32 , 33 ] ; to discovery [ 9 , 18 ] or exploration [ 46 ] . In addition to taxonomies of user goals , a higher - level division has classifed the way users approach visual analysis as either top - down , when a user is driven by an a priori hypothesis [ 34 ] , or bottom - up , when a user is " engaged in an " open - ended " exploration with at - most vaguely defned tasks " [ 6 ] . As a result , approaches to studying user strategies have primarily either asked users to perform an untargeted exploration of a dataset to see what insights they can derive [ 18 , 26 , 34 , 39 , 40 , 45 ] or provided structured tasks or sub - tasks [ 24 ] to accomplish . Our approach difers in that our participants were not given a task , but instead were asked to share what they were trying to accomplish , leaving us to tie their objec - tives back to previous concepts . 2 . 1 . 1 Sub - tasks . Other work has described the user’s intent at a more granular level of abstraction , sometimes referred to as sub - tasks [ 24 ] , where the user’s goals in performing low - level inter - actions with the system are grouped to explain how they work toward a primary objective [ 24 ] . Examples of sub - tasks include : data manipulation [ 9 ] ; querying and view specifcation [ 28 , 48 ] ; orienting or getting an overview [ 48 , 54 ] ; or detecting patterns in the data [ 54 ] , such as correlations [ 56 ] or extremum [ 2 ] . Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Although more granular breakdowns of a user’s intent in sub - tasks can be helpful for designing new systems , our work primarily focuses on higher level objectives because we found it to be a better match for the way participants verbalized their reasons for performing their analyses . 2 . 2 Actions We found two main approaches to work defning user actions : ( 1 ) taxonomies of discrete steps [ 1 , 41 ] or atomic analytic actions to - ward achieving an objective [ 24 ] and ( 2 ) specifc , logged interactions with the system in the form of mouse clicks or keystrokes [ 5 , 10 , 16 ] . 2 . 2 . 1 Taxonomies of discrete steps . Work in the space of interac - tion techniques has sought to identify the basic building blocks employed by users , as facilitated by their tools [ 11 , 14 , 17 , 31 , 47 , 50 ] . For example , Amar et al . [ 2 ] described 10 low - level analysis ap - proaches ( i . e . , retrieve value , flter , compute derived value , fnd extremum , sort , determine range , characterize distribution , fnd anomalies , cluster , and correlate ) that students reported they would use to analyze datasets from particular domains , while Heer et al . [ 29 ] divided similar concepts into three high - level categories of data and view specifcation , view manipulation , and process & provenance . While the names and categorizations of these tech - niques may difer across taxonomies , they hold many operations in common ( flter , zoom , brushing and linking , etc . ) [ 18 , 53 ] , sug - gesting the minimum features that must be supported to enable sufcient user control in visual analytics tasks . Additionally , multiple contributions in this space have suggested that these actions may be grouped into predictable sequences and patterns [ 23 , 24 , 26 , 41 ] , which we will discuss in more detail in Section 2 . 3 . 2 . 2 . 2 Log - based actions . Studies taking a log - based approach have focused on extracting patterns to understand behaviors by mining low - level interactions , such as mouse clicks , and aggregating them into sequences to determine intent , current analysis stage , or to predict subsequent actions [ 5 , 10 , 16 , 23 , 25 , 26 , 35 , 52 ] . For example , Olson et al . [ 36 ] proposed statistical and grammatical techniques to understand sequential interaction behaviors . Cowley et al . [ 15 ] pro - posed Glass Box to capture user interaction data , such as copy , paste , and window activation . Gotz and Wen [ 23 ] proposed an algorithm that frst extracted meaningful interaction patterns from a library of pattern defnitions through real - world visual analytic activity logs , then used the proposed algorithm to infer users’ intended visual tasks . Brown et al . [ 10 ] examined three types of interaction data , i . e . , 1 ) state - based , 2 ) event - based , and 3 ) sequence - based to predict task performance and personality traits and Dabek et al . [ 16 ] pro - posed a grammar - based approach to identify when users were on a non - optimal path to enable the system to make recommendations . In the second part of our study , we used similar strategies to mine low - level visual elements to locate and validate sequences of actions . Our method difers from prior approaches in that we used logs analysis as a validation step , rather than to extract and discover new patterns . 2 . 3 Patterns Although many taxonomies have hinted that actions may be grouped into predictable sequences and patterns , identifcation and cover - age for these patterns have been less deeply investigated . The way patterns have been derived and presented in prior work has been heavily sourced in manual or programmatic logs analysis , but has varied in granularity and level of abstraction . Most relevant to our work , Gotz et al . identifed four repetitive sequences of actions from logs that they called : Scan ; Flip ; Drilldown ; and Swap [ 23 ] . In contrast , Guo et al . identifed four patterns , describing specifc sequences of actions used to achieve user goals , such as the goal of " Locating , " which was composed of ( Retrieve - Elaborate - Elaborate - Elaborate - Retrieve - Elaborate ) [ 26 ] . Our goal in this work is not to create new taxonomies of objec - tives or actions , but to add to and validate the existing catalog of patterns in a large - scale industrial setting . 3 METHODOLOGY To understand user objectives and their visual analysis processes , we frst conducted an exploration via digital “feld” observations . We created sequence models , transcribed verbal data , and performed inductive coding to identify emergent themes . We then performed a quantitative analysis of logs from a second visual analytics product to test whether our patterns were product - specifc . 3 . 1 Digital “feld” observations To date , much of the work in building taxonomies of analyst behav - iors has been sourced from a combination of interviews [ 18 , 38 , 48 ] , think - alouds [ 18 , 26 , 34 , 40 , 45 ] , contextual inquiries [ 48 ] , feld ob - servations [ 48 ] , and controlled experiments [ 8 , 34 ] . Relatively few have involved direct observations of expert users working within their own contexts without interruption or intervention by the experimenter . Our approach addresses these limitations by borrowing from feld observations , adapted for a digital , globally distributed , and working - from - home environment , where a great deal of day - to - day communication and task - oriented progress is achieved on - screen rather than via physical interaction . For example , the use of on - line communication channels and the generation of digital work - related artifacts , such as documents , unpublished drafts and notes , and dashboards have become increasingly common , particularly in large technology companies where visual analytics are heav - ily utilized . In these contexts , in - person feld observations may be logistically challenging and intrusive , without capturing any infor - mation beyond what is visible on - screen . Based on these factors , we conducted digital “feld” observations where participants shared their full screens , allowing fy - on - the - wall viewing and recording of their in - situ workfows in a minimally disruptive way . To begin these 90 - minute sessions , participants connected to a video chat and were asked for verifcation of their consent . Follow - ing consent , the experimenter began the recording of the session and asked four introductory questions about the context the partici - pant worked in as well as what they were planning to work on . The participant then shared their full screen and worked silently for 60 - 70 minutes . Fifteen minutes prior to the end of the session , the CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . experimenter interrupted the participant and asked follow - up ques - tions to learn more about their strategies and processes . Participants were then thanked , debriefed , and the session was ended . 3 . 1 . 1 Participants . Ten participants ( one removed at their request after the study had concluded ) were recruited who met the follow - ing criteria : ( 1 ) A full - time employee of the company for more than a year , who ( 2 ) Had more than a year of experience using PowerDrill [ 27 ] , an enterprise visual analytics product , and ( 3 ) Performed visual analytic investigations in PowerDrill at least 90 minutes per week on average . Participants were from eight diferent teams across Google , re - porting between 1 - 5 years of experience with performing large data investigations using visualizations . Five participants were identifed as power users , those whose primary roles required conducting visual analytic investigations for more than 20 hours per week . Although all participants conducted visual analyses as part of their day - to - day work , more than half did not have a formal title or role as an " Analyst , " instead undertaking these investigations to support their work as Software Engineers . In addition to being sourced from eight diferent teams , par - ticipants operated on a variety of domains and data sources . We observed participants performing analyses on diagnostic logs that monitored the health of production code , sales and accounts , travel information , and analysis of A / B experiments . The datasets users worked with ranged in size from tens of megabytes to hundreds of terabytes with schemas that ranged from tens to thousands of felds . In contrast to prior experimental studies , all of our participants had domain knowledge of the datasets they worked with and how they were situated within a larger ecosystem . Participants used this knowledge to traverse multiple products when datasets were fragmented or inaccessible from PowerDrill . 3 . 1 . 2 Analysis . We transcribed responses to the introductory ques - tions , what participants said they were trying to accomplish , and the follow - up interview . We distilled what users said they were trying to accomplish by abstracting at several levels to fnd com - monalities . Other comments were analyzed using inductive open and axial coding to identify themes . We created sequence models [ 7 ] —a record of the detailed steps a user took to accomplish tasks—as a proxy for logs data , which was unavailable due to technical constraints . Every action in all systems across the ecosystem was manually recorded from the video at a mouse click or key press level , for example : • Clicked on search box • Typed 3 letters • Moved pointer to click on third option in auto - complete dropdown • Hovered over feld name and looked at hover pop - up box containing feld defnition and business logic explanation • Clicked on searched for feld name and dragged onto chart We worked with the PowerDrill development team to identify 263 codes that captured discrete steps at a higher level of abstraction than mouse clicks and keystrokes . Codes fell into 20 categories . Examples of categories included : ( 1 ) Visual analytic actions identifed in prior work . Exam - ples : " flter , " " zoom , " " create visualization , " or " compute de - rived value " [ 2 , 18 , 29 , 53 ] . ( 2 ) Visualization type . Examples : " scatterplot , " " heatmap , " " ta - ble , " " area chart . " ( 3 ) Interaction type . Examples : " keyboard shortcut , " " click and drag , " " hover , " " click . " ( 4 ) Use of other tools . Examples : " open e - mail , " " look person up in company directory , " " edit code in IDE . " ( 5 ) Interactions with other people . Examples : " ask coworker question in messaging app , " " send screenshot to collaborator , " " request review in review tool . " ( 6 ) Product - specifc features . Examples : " reload all charts , " " switch to batch processing mode , " " change amount of data being sampled . " ( 7 ) Meta operations . Examples : " load from shared link , " " save session , " " duplicate analysis . " Codes were then combined with qualitative data transcribed from participant utterances and manually mapped into visualizations , allowing common patterns to emerge , as shown in Figure 3 . 3 . 2 Quantitative logs analysis Figure 1 : Histogram of the number of visual elements in - volved in 2530 user analysis sessions Our qualitative digital “feld” observations generated common user objectives and sequences of actions ( investigative patterns ) with a small sample of users . As noted by other eforts , user goals or objectives are difcult to intuit from logs data [ 18 , 24 , 26 ] , so we focused our work on verifying that our patterns were not product - specifc . We , therefore , conducted a quantitative analysis of logs from a second visual analytics product for 2530 user analysis ses - sions . 3 . 2 . 1 Participants and data . We selected the logs from a second , non - public visual analysis product , only available for internal use at Google—which we will refer to as DataExplorer . The feature set supported in DataExplorer is similar to that of PowerDrill , Tableau , Data Studio , or PowerBI . The details of what the users were investi - gating in DataExplorer were unknown as they were not performing Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA pre - defned synthetic tasks . However , similar to PowerDrill , the size of the data sources that the users were operating on ranged from tens of megabytes to hundreds of terabytes with schemas that ranged from tens to thousands of felds . We randomly selected eight days of logs ( the size of the logs : M = 754 . 4 gigabytes , SD = 26 . 8 gigabytes ) for analysis and grouped each day by user , considering it to be a single analysis session , which resulted in 2530 unique sessions . Visual elements within the DataExplorer interface were instrumented to allow tracking of interactions with 184 visual elements , such as clicks and key presses along with their associated timestamps . We used logs of these interactions to extract repeating sequences of visual elements , resulting in sequences that ranged in length from 4 to 6069 visual elements . The distribution of the number of the visual elements ( M = 349 . 5 , SD = 696 . 8 ) used in each session is shown in Figure 1 . 3 . 2 . 2 Analysis . We used sequential pattern mining algorithms to identify frequent co - occurrences of visual elements [ 21 ] . To im - prove efciency , we adopted the Closed FAST algorithm [ 22 ] and found closed frequent sequential patterns ( those not included in other frequent sequential patterns ) by exploiting sparse id - lists and vertical id - lists [ 44 ] that grouped sequence closure checking and space pruning in one step . 4 RESULTS We divide our results into six sections , frst describing the user objec - tives found across the 15 observed investigations ( some participants pursued multiple discrete objectives during a single observation ) . Next , we provide a brief overview of the codes that we used to visualize each investigation , followed by a description of the emer - gent patterns and their relationship to each user objective . We then highlight behavioral themes we observed regarding collaboration and use of other products within the larger ecosystem . Finally , we describe which patterns were identifable from the DataExplorer logs . 4 . 1 User objectives Prior work has described objectives as either bottom - up—an ex - ploratory analysis with open - ended objectives [ 6 , 45 , 48 ] —or top - down , where users are driven by an a priori hypothesis [ 34 ] . Our fndings paralleled observations from Battle et al . ’s work , with all of our participants approaching with a priori goals and hypotheses [ 6 ] . The starting point for all visual investigations was rooted in the larger ecosystem , triggered by one of the following : • An alert from a monitoring system . • A request from another person . • Participant knowledge about a change that had occurred ( example : code launch , A / B experiment ) . • A bug fled by another person or system . To identify user objectives , we frst transcribed how users de - scribed their goals . We then paraphrased what participants had said , looking for commonalities . Finally , we abstracted higher - level themes until we were able to identify recurring motivations . The four objectives that we identifed from this method were : ( 1 ) Check for anomalies or efects after a known change . Example : Look at data after a code change was rolled out or an experiment had concluded to report on success or shift to investigating where and why data was not what was expected . ( 2 ) Investigate where and why data was not what was ex - pected . Example : Follow - up on a reported bug or anomalous data to fx the problem . ( 3 ) Determine if data signals matched known patterns to take action . Example : Look for a spike in trafc that might indicate a local holiday or occurrence of a known issue to inform others or fx the bug . ( 4 ) Prototype or refne the defnition of a new metric . Ex - ample : Create a new derived value [ 2 ] to calculate a specifc type of trafc so that others can use it . Although we abstracted these from observations across at least four diferent domains and datasets , these objectives may be broad - ened to a higher level to map to prior taxonomies , increasing their generalizability . For example , " Investigate where and why data was not what was expected " maps directly to Lam et al . ’s " Identify main cause " " Check for anomalies or efects after a known change " and " Determine if data signals matched known patterns to take action , " are both related to Lam et al . ’s [ 33 ] " Compare entities " and " Explain diferences , " but there are two distinguishing factors that make them difcult to directly map . First , we found that these objectives depend on the user’s initial hypotheses or expectations . In the case of checking for anomalies or efects after a known change , the par - ticipant’s hypothesis may be that nothing has changed , that specifc targeted measures have changed , or they may be approaching it as an open - ended exploration where they don’t have any idea about what may have changed . These diferences in hypotheses have an efect on the measures , visualizations , and level of detail the user employs in their analysis . In the case of determining if data signals match known patterns , our observation was that participants are only able to do this when they have extremely specialized domain knowledge and when their objective and analysis is something they’ve performed repeatedly . In these cases , comparisons are not only taking place between visual - izations in the system , but also between the visualizations and the user’s mental models or knowledge —justifed belief about patterns in the data resulting from previous knowledge - generation [ 43 ] . We observed that these mental models can grow quite complex , with some participants checking more than a hundred visualizations against a mental database of patterns they had developed over years of analysis . The objective of " prototyping or refning the defnition of a new metric " does not map to any higher level goals or objectives from prior work . However , it does relate to data operations such as deriving a value [ 2 ] , with the main diference being the intention for others to use the derived measure , requiring some level of rigor and review by others . Overall , because of the tight coupling of objective to domain and context [ 24 ] , we found it difcult to directly map every objective to the high - level of abstraction used in prior work . We , therefore , continue to use our objectives defnitions throughout our work to avoid inadvertently overstating the generalizability of our fndings . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . Table 1 : The self - described goal of each investigation , the common objective each maps to , and how the user intended to act on the analysis result . Self - described goal Objective Check if anything is going wrong after a software release and investigate if they see something strange Check for anomalies or efects after a known change Document experiment results and share / present to others Compare between multiple experiment con - ditions to see if there are any unexpected diferences between conditions Check for anomalies or efects after a known change Document fndings and change experiment conditions When monitoring system indicates data is incorrect , fgure out where it is not good and why Investigate where and why data was not what was expected Identify system bug and fx Investigate a discrepancy between two data sources , generated from multiple systems , to fnd out why it is happening Investigate where and why data was not what was expected Identify faulty data source and fx bug ( s ) causing the problem Look at logs data to determine if it matches a known pattern and fgure out if identifying the pattern can be automated Determine if data matches known patterns Design and implement a new algorithm for pattern detection Analyze the efects of a multi - condition ex - periment to verify that the expected change occurred Check for anomalies or efects after a known change , leading to investigating where and why data was not what was expected Document experiment results and share / present to others Review someone else’s interpretation of logged data and verify their conclusion about the problem source Determine if data matches known patterns Approve or reject colleague’s results in ap - proval system Narrow datasets based on patterns previ - ously found to be connected to a particular event Determine if data matches known patterns Flag instances of event for colleague review and later triage by team Investigate a lead generated by a system to determine if it matches a known occurrence Determine if data matches known patterns Flag event for colleague review and later triage by team See if a data is correlated at someone’s re - quest to see if a new metric can be created Prototype or refne the defnition of new metrics / algorithms Design and implement a new metric for oth - ers to use in their analyses Investigate how much X is afecting A , B , and C , comparing to fnd low hanging fruit to enable changes Check for anomalies or efects after a known change Document investigation and recommenda - tions for team to take action on Investigate a bug and fgure out what subset of data to focus on in a later analysis Investigate where and why data was not what was expected Flag data and share analysis with members of another team so they can take over Check a software release to see if anything looks bad Check for anomalies or efects after a known change Document the investigation and if needed , fle bug ( s ) on anything that looks problem - atic because they got stuck and cannot fgure out why something is happening Investigate where and why data was not what was expected Share analysis with colleague with explana - tion of how conclusion was drawn Follow up on an alert after two other people investigated it to verify their conclusions about the problem source Investigate where and why data was not what was expected Document analysis and approve or reject conclusions in team meeting Intended action Help someone else with their investigation Table 1 shows the specifc self - reported goals participants had , the abstracted high - level objective , and the action each participant intended to take based on the analysis . 4 . 2 Codes used in visualizing patterns In Figure 2 , we show the keys for the codes , used to visualize the investigations . These codes have similarities to a number of taxonomies de - scribing analytical steps [ 11 , 14 , 17 , 31 , 47 , 50 ] , with some notable diferences : • “Session with 4 + charts” illustrates when a participant did not start an investigation from scratch , instead leveraging a template from a prior investigation created by themselves or by others . • “Duplicate session” shows when a participant chose to branch an investigation , copying their entire session . • “Paste or edit SQL” describes when a participant manipulated the data directly via code , e . g . , “data operations” [ 14 ] or computing ad hoc derived values [ 2 ] . • “Use other product” indicates when a participant opened or referred to other tools ( visual analytic or non - visual analytics products ) . A total of 53 products outside of the visual analysis tool were used throughout all observations , but these have been collapsed into a single representative icon . • “Use batch processing” is when a participant switched to a diferent instance of the visual analytics tool to increase performance through batch query processing . In addition to noting when a flter was added , we also included a " Levels " component to represent the number of flters that had been applied or how " zoomed in " an analysis was . For example , a new ses - sion where all data is displayed would be at level 1 , where no flters have been applied yet . As the participant adds flters , restricting the data in a visualization , the " depth " of the investigation increases . When flters are removed , the depth decreases . The orange bar at the top of the visualization represents when the participant jumps to a diferent product or application , such as checking e - mail . Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Figure 2 : Illustration of codes we used to visualize investigations 4 . 3 Visual investigation patterns By using codes to visualize all 15 sessions , six common patterns of investigation emerged . A visual representation of each of these patterns is shown in Figure 4 . We next describe each pattern with an illustrative user quote , highlighting the user intent and the way the pattern was being used . We also contextualize when patterns matched those found in prior work . 4 . 3 . 1 Drilldown . “ . . . in this case I would just narrow down the ( feld name ) . . . and then based on some patterns in the data that we have found to be connected to invalid activity , I would search for entities that correspond to these patterns and then I will just narrow down” – p07 Illustrated in Figure 4a , this was the most common and canonical pattern of the investigations . A Drilldown is a progressive series of fltering , restricting , or excluding data to gradually narrow the investigation space . This pattern was also found in prior work , most directly as a Drilldown [ 17 , 23 ] , but also as enriching [ 38 ] , a flter - fow model [ 47 ] , or the tendency for flter actions to be performed in chunks [ 26 ] . The Drilldown pattern was used to achieve all four objectives . 4 . 3 . 2 Sawtooth . “So then I just don’t go do it for every single one . . . okay , so most of them like , have this heavy basis on the left hill and the right hill , not in the middle . So most of them have the same more or less shape , that’s what I conclude” – p02 Frequently beginning from an ordered list of items , the participant fltered or restricted the data to just one instance , looked at it , removed the flter , and then fltered by the next item , repeatedly cycling through items to understand trends . For example , when looking at fights , a user might progressively flter by country : flter to only Japan , reset to see all fights , flter to only France , reset to all fights , etc . to build a mental model of what " normal " looks like and identify anomalies . The Sawtooth pattern either took the form of progressively fltering for diferent values ( e . g . , country by country ) or toggling the same set of flters on and of repeatedly ( e . g . , only Japan and all fights ) and primarily occurred at two granularities : ( 1 ) Chart level . When a participant was focused on fltering for a single visualization within a page of many visualizations . ( 2 ) Session level . When a participant modifed a global flter for the entire investigation , afecting all visualizations . The Sawtooth pattern , illustrated in Figure 4b bears some similar - ities to the " Scan " pattern from Gotz et al . , described as an " iterative inspection over a series of visual objects representing similar data objects . . . to visually compare attributes " [ 23 ] . We were unable to determine whether the behaviors were an exact match because it was not clear whether the " Scan " pattern included frequent returns to an unfltered dataset . It was also unclear whether comparisons were being driven by exploratory needs , matching what we ob - served , or to confrm a hypothesis . Our observed pattern is more similar to previously identifed exploratory behaviors [ 43 ] like de - tecting patterns [ 54 ] , foraging [ 5 ] , orienting [ 26 , 30 ] or schema formation [ 38 , 42 ] . If the " Scan " in Gotz et al . ’s work was conducted for confrmatory comparisons , it would be more similar to the Side - by - side pattern we observed . The Sawtooth pattern most frequently occurred when users were ( 1 ) checking for anomalies or the efects after a known change ; or ( 2 ) investigating where and why data was not what was expected , but never when determining if signals matched known patterns . 4 . 3 . 3 Sidestep . “So my session only has this now < points > as a where condition . . . < removes prior conditions in a sidestep > so now if you see here only about here , how many results come up ? So that’s what I was looking for , I want to see what kind of trafc is coming from those items . . . completely” – p06 A Sidestep pattern , illustrated in Figure 4c , takes place after a Drill - down . During a Drilldown , users " repeatedly flter down along orthogonal dimensions of a dataset [ 23 ] " to narrow the analytic focus . When a user does a Sidestep , they remove multiple flters from prior steps in the Drilldown , usually leaving only the most recent . The intent is that having narrowed their investigation to a specifc instance or set of instances , the user wants to view what the instance ( s ) look like without the prior constraints . In an example for data on fights , imagine a user has done a Drilldown on fights by progressing through : ( Filter 1 ) Oct 1 - 31 ; ( Filter 2 ) Japan ; ( Filter 3 ) Leaving from Osaka ; ( Filter 4 ) Leaving from Terminal B ; ( Filter 5 ) Departing between 10 - 11 pm . The user may then do a Sidestep , leaving ( Filter 5 ) Departing between 10 - 11 pm , but removing Filters 1 - 4 to see if the data patterns look the same for any fights departing between 10 - 11 pm . They may then subsequently either re - add prior CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . Figure 3 : Example visualization of an investigation where the user was checking if anything had gone wrong after a feature launch and following up after seeing something unexpected in the data . The levels depict the number of flters that have been applied to constrain the dataset . Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Figure 4 : Visual representations of investigative patterns : ( a ) Drilldown , a progressive series of fltering to reduce the inves - tigation space , ( b ) Sawtooth , where participants repeatedly added and removed progressive flters , ( c ) Sidestep , when most preceding flters except the most recent were removed to view the instance holistically , ( d ) Fanout , when a user opened mul - tiple branches of investigation simultaneously , ( e ) Plug and Play , when the information required for an investigation was well - defned and only required changing the parameters for predetermined visualizations , and ( f ) Side - by - side , when compar - ing similarities or diferences between two or more instances . flters to refne their hypotheses , or begin adding flters along a diferent data dimension ( example : ( Filter 1 ) Departing between 10 - 11 pm ; ( Filter 2 ) Only frst - class tickets ; ( Filter 3 ) With gold airline status ) . A Sidestep illustrates when an investigation is traversing multi - dimensional space and we were not able to identify documenta - tion of this behavior in prior work . The Sidestep pattern was only observed when participants were determining if signals matched known patterns . 4 . 3 . 4 Fanout . “And that is why I have multiple sessions . . . so I label each and every session , call it . . . what is basically the thing that I am looking at . . . and you know by the end of this investigation I’m sure I’ll have like some 40 - 50 sessions over here that I have to investigate” – p04 A Fanout was indicated when a participant opened multiple new branches of investigation simultaneously , often by creating new tabs , as illustrated in Figure 4d . Participants later iterated through each tab , often performing the same analytic steps in each , before closing out new branches that were not promising . Although the quote described having 40 - 50 sessions to investigate , the maximum number of tabs that were shown during observations was 19 . Some participants tracked promising branches of investigation outside of the visual analytics product , for example , by creating a list of instances to investigate in a spreadsheet , document , or e - mail . Fanouts bear some resemblance to the " Scan " pattern—iterative inspection over a series of visual objects representing similar data objects with the intent of visually comparing attributes , docu - mented by Gotz et al . [ 23 ] . However , we were unable to determine whether " Scan " was closer to our " Side - by - side " pattern where the intent was to compare , or more similar to the Fanout , which ap - peared to be a way of bookmarking or creating placeholders for later analyses . Additionally , the Fanout pattern may be an illustration of Sacha et al . ’s observation that users may be working on several investigation loops in parallel [ 43 ] or Battle et al . ’s description of analysts looking at a small region , moving to a new region , and re - peating the same analysis [ 5 ] . There was also some resemblance to search pattern literature and the berrypicking information foraging paradigm [ 4 ] , where users are scouting for promising information patches [ 13 ] . Fanouts were primarily observed when participants were deter - mining if signals matched known patterns . 4 . 3 . 5 Plug and Play . “Okay , so if I have to defne it based on time , 70 % of my time it’s the same template which has been defned” – p06 The Plug and Play , illustrated in Figure 4e , was primarily manifested when participants loaded a previous investigation , either theirs or someone else’s , as a template to save time . In these situations , the visualizations and flters needed for an investigation were already defned and the participant just needed to change out a variable ( example : browser , country ) . While a participant might still add felds or new visualizations to a Plug and Play , they heavily utilized existing representations to bootstrap their investigations . Although repeating analysis steps [ 5 ] and the reuse of explo - rations have been suggested in past work [ 37 ] , we believe that the Plug and Play pattern has not previously been identifed or em - pirically verifed because a majority of the literature has studied visual analysis processes in isolation . The Plug and Play pattern appears when users recognize the recurring nature of some ob - jectives , leading to attempts to reduce duplication of efort , and a conscious leveraging of their and other people’s past work . The actions leading to the generation of an analysis template would be classifed as " meta actions " —actions that are not operating on data or visualizations [ 24 , 46 ] and could be interpreted as a way for users to bridge the Worldview gap enabling them to " create , ac - quire , and transfer knowledge or metadata about important domain parameters within a dataset " [ 3 ] . Plug and Play patterns were primarily observed when partic - ipants were determining if signals matched known patterns . As CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . a note , dashboards may be considered Plug and Play investiga - tions where the visualizations and felds needed have become static or standardized , allowing others to derive a subset of predefned answers without needing to be domain experts . 4 . 3 . 6 Side - by - side Comparisons . “Normally for the heat map , I usually try to make the control and the experiment side by side and see what is the diference . ” – p05 Side - by - side patterns , shown in Figure 4f , were found when a participant opened two or more instances of an investigation , fltered by diferent criteria , and viewed them simultaneously to make comparisons . The Side - by - side pattern is synonymous with the " Compare " action from prior work [ 48 , 53 , 56 ] , but we chose to describe it as a Side - by - side because PowerDrill allows users to view visualizations with diferent flter levels or diferent datasets side - by - side , syn - chronizing views between windows . This side - by - side capability also allowed us to observe that participants never compared more than three datasets simultaneously . We hope that this additional observation will add to prior fndings and provide context for future studies that may examine design needs for comparisons in greater depth . Side - by - sides most frequently occurred when ( 1 ) participants were checking for anomalies or the efects after a known change ( e . g . , an experiment or a launch ) ; or ( 2 ) participants were investi - gating where and why data was not what was expected . 4 . 3 . 7 Use of paterns by objective . After identifying common inves - tigation patterns , we next examined how many participants used each pattern , shown in Figure 5 . N u m be r o f pa r t i c i pan t s 0 1 2 3 4 5 6 7 8 9 Drilldown Sawtooth Sidestep Fanout Plug & play Side - by - side Figure 5 : The number of participants using each investiga - tion pattern . The X - axis shows each pattern and the Y - axis shows the number of participants who exhibited that pat - tern in any of their investigations . We also looked for trends in what patterns were used when participants had diferent objectives . We excluded “Prototyping the defnition of a new metric or algorithm” because there was only one case observed during sessions . We found that when checking for anomalies or efects after a known change , Sidesteps were not used in any investigations , but the Sawtooth pattern was seen in 60 % of them . In contrast , when participants had the objective of determining if signals matched known patterns , none of the inves - tigations used the Sawtooth pattern , but 75 % contained Sidesteps , as illustrated in Figure 6 . Figure 6 : Percentages of investigations using a pattern for each objective ( excluding prototyping a new metric ) . The X - axis shows each objective and the Y - axis shows the percent - age of investigations where the pattern was used . Patterns are distinguished by color . Figure 7 : The average number of the pattern uses per inves - tigation , split by objectives . The X - axis shows each objective ( excluding prototyping a new metric ) and the Y - axis shows the average number of the pattern that was used . Patterns are distinguished by color . Looking more closely at the frequency of pattern use across all in - vestigations , as shown in Figure 7 , although Drilldowns were always used , Sawtooth patterns were equally prevalent when checking for anomalies or efects after a known change . However , Drilldowns were the main pattern of choice when investigating where and why data was not what was expected . 4 . 4 Collaboration and presentation themes All of our participants expressed an intention to share their fndings with others , documenting their investigations in a variety of ways . As a result , unlike prior work that heavily relied on participants to think out loud , the volume of digital artifacts participants cre - ated to track or describe what they were doing made it relatively easy to identify when they achieved spontaneous ( a sudden " Ah ha ! " moment of comprehension ) or knowledge - building ( learnings built from a relationally semantic knowledge base ) insights [ 12 ] or detect when they changed strategies . For example , two participants Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA took extra steps to help others understand their investigations , such as writing additional documentation to warn of “gotchas” or annotating the session with comments . Participants also involved other colleagues in their analyses , ei - ther synchronously or asynchronously , as subject matter experts , consultants , collaborators , or reviewers . This high level of involve - ment from other people calls back to Edge et al . ’s need to sup - port community and mediate between collective subjects and their shared objects [ 19 ] . We found that these needs and strategies mani - fested in three emergent themes : A majority of analyses leveraged prior work as a starting point . Almost all participants started from a template of another session , generated by themselves or others , and was showcased in the use of the Plug and Play pattern . Additionally , more than half referred back to past work as a reference during their analysis . Our participants leveraged past analyses and other people’s derived data elements [ 9 ] to not just save time , but also to bootstrap domain knowledge and to facilitate the discovery of new or relevant data elements . We believe this fnding suggests that it may be worth - while to re - examine the validity of assuming users will always start analyses from scratch . Real - time collaboration was integral to analyses . During our observations , three participants actively messaged others for help or clarifcations in the middle of their analyses , pausing or switching to a parallel workstream while they waited for a response . The real - time collaborations we observed suggest that further work in the visual analytics area may be needed to understand how behaviors displayed by individual users during designed tasks difer from how they interact with others in real - world analyses . Analyses were subjected to review by others . Five investi - gations involved a direct review of another person’s analysis to verify that a colleague would reach the same conclusions and two described expecting a review at a later time . These reviews var - ied along a spectrum , ranging from an informal review or " sec - ond opinion " to fling a formal report for auditing . We believe this expands on the goal described in other work to present , de - scribe , link to fndings [ 29 , 32 , 48 , 56 ] or track insight provenance and history [ 24 , 28 , 29 ] . While past work has explained the impor - tance of tracking and preserving past steps for the analyst’s direct use [ 24 , 28 ] or allowing annotation to document fndings [ 29 , 48 ] , to our knowledge the design needs for efciently reviewing someone else’s analysis have yet to be explored . 4 . 5 Ecosystem themes For this work , we defne a product ecosystem as a group of in - terdependent products , users , and their interactive relations and business processes [ 55 ] . We found two themes that diferentiate our observations of users operating within a product ecosystem in a large - scale industrial setting from prior work . Participants expected to act on their fndings . All of our par - ticipants described an intention to act upon their fndings and we observed participants sharing their fndings with others via screen - shots , annotations , and documentation , demonstrating overlap with previously identifed motivations for presentation [ 32 ] , sharing of links and context [ 48 ] , or describing or explaining fndings [ 33 ] . However , we feel a new contribution in this space is our observa - tion that in a large - scale industrial setting , knowledge discovery or insight generation was not an objective in and of itself . Instead , the expectation was that fndings should be sufcient to drive a subse - quent action or decision not to act . These subsequent actions involved implementing a code change , documenting recommendations for a business decision , or noting that the investigation took place and no action was needed . The expectation that fndings must result in something with a tangible impact on the business highlights the need to consider how a user is supported both within the visual analytic system as well as their transition out of the product to act in the larger ecosystem . We observed that support for these transitions varied , with some products providing extremely smooth paths to action ( example : auto - populating a bug with relevant in - formation from the analysis using a link ) and others necessitating high friction , manual copying between multiple windows . Fragmentation of communication , data , and documenta - tion resulted in frequent interleaving of other products dur - ing the analysis process . During sessions , participants leveraged 53 products outside of PowerDrill to : ( 1 ) Orient themselves to the investigation and / or reference what they were supposed to look at ; ( 2 ) Obtain critical pieces of information to conduct the investi - gation ( e . g . , experiment ID , dates ) ; ( 3 ) Get access to alternate data sources , visualization types , or system - specifc datasets ; ( 4 ) Document their investigations for themselves or others ; ( 5 ) Copy SQL or past investigation settings ; ( 6 ) Look up or ask others about business logic . This proliferation of products and frequency of accesses suggests that it may be benefcial for future work to consider the actions users may be taking outside of the visual analysis tool . For example , logs analyses may miss user actions when they take place outside of the product , particularly when users have established team processes to work within a larger product ecosystem where their work requires interacting across many systems . 4 . 6 Quantitative logs analysis fndings To understand whether patterns were product specifc , we per - formed a logs analysis on DataExplorer , a non - public visual ana - lytics product used internally at Google . We examined 2530 user analysis sessions , performed over an 8 - day period , and used the Closed FAST algorithm [ 22 ] to mine 9988 frequent sequential pat - terns with a minimum support ( i . e . , the minimum frequency of the patterns ) of 632 . 5 out of 2530 user sessions ( i . e . , 25 % ) . We used heuristic rules ( e . g . , number of repeated visual elements , signal visual elements , and periodical sub - patterns ) to identify when the investigation patterns from the feld observation occurred . The dis - tributions of pattern length and support of the identifed sequential patterns are illustrated in Figure 8 . In alignment with our observations and past work [ 23 ] , we found the Drilldown pattern to be the most common , identifying that it was included in 8920 out of 9988 patterns . The second most fre - quently used was the Fanout ( 2236 out of 9988 sequences ) followed by the Sawtooth , which was observed in 704 out of 9988 sequential patterns . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . ( a ) ( b ) Figure 8 : The distributions of ( a ) pattern length and ( b ) pattern support of the mined frequent sequential patterns . Figure 9a shows the Drilldown pattern , found in 1209 out of 2530 user sessions , where users repeatedly added flters to progres - sively narrow their investigation space until a conclusion was made . Figure 9b illustrates the Sawtooth pattern ( from the second step ) , found in 635 of 2530 user sessions , where users repeatedly removed a flter , clicked on a menu , and added a flter . Figure 9d shows the Fanout pattern with the signal visual ele - ment , i . e . , “Select Page” , where users switch to diferent pages as a way of branching their investigations , which was found in 646 of 2530 user sessions . The Play - and - Plug pattern , shown in Figure 9e , was found in 797 of 2530 user sessions , where users applied a global flter after loading a previous analysis . The Side - by - side pattern , shown in Figure 9f , was found in 960 of 2530 user sessions using a signal visual element , i . e . , “Reference Scenario , ” where users set a session state as a reference to compare other scenarios to . Note , we were not able to identify the Sidestep pattern automatically through logs because their supports were below the threshold of the algorithm . Instead , we manually examined the log data to iden - tify when the Sidestep pattern occurred . Figure 9c illustrates the Sidestep pattern , which was found in only 126 out of 2530 user sessions . 5 DISCUSSION We conducted digital “feld” observations to observe 9 users working in a large - scale business setting , where the user objectives driving their visual analyses were initiated from a variety of sources as part of their day - to - day jobs . Participants described what they were trying to do and we observed them working both within the PowerDrill interface and across their larger product ecosystem . From these observations , we identifed four high - level objectives , six repeated patterns of action , and described when these patterns corroborated prior literature and when diferences existed . We then analyzed the logs of DataExplorer , a non - public internal visual analytics product , examining 2530 user analysis sessions over an 8 - day period to validate whether our observed patterns were product specifc . Our fndings support that some of the previously presented pat - terns generalize to large - scale industrial settings ( example : Drill - downs [ 17 , 23 , 26 ] ) . Where our patterns and objectives difered from prior work , we highlighted how they might relate ( examples : Swap , Scan , or Flip [ 23 ] ) , depending on user intent . Furthermore , we described three patterns : Sidestep , Plug - and - Play , and Fanout , that we believe have not been empirically observed in prior work because they stem from interactions between users or the interplay between the visual analytic product and a larger ecosystem . These new patterns and the way they were employed toward specifc user objectives extend the existing body of work seeking to better model user actions through logs and may be applied to improve predictive power or suggest next steps that are contextually relevant to the user’s objective . We also described fve emergent themes from our observations of users collaborating and acting within their larger ecosystem that provide contrast and additional considerations for further work in the visual analysis space . We observed that participants documented their investigations during their journeys , providing clues for how we might better support annotation to help them tell the story of how they reached their conclusions . The way that our participants leveraged other products to track promising leads and augment their investigations with work from others highlights the necessity of providing rich copy / paste and export mechanisms to seamlessly transfer informa - tion between systems . We also presented fve emergent themes that revealed ways to better support users within a large product ecosystem and sug - gested new opportunities for future work : • Theme 1 : A majority of analyses leveraged prior work as a starting point . Visual analytics products may consider that providing support for users quickly locating , search - ing through , and sharing past analyses could greatly reduce analysis time . Future work might consider options for de - signing studies so that participants are not always starting from scratch . Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA ( a ) ( b ) ( c ) ( d ) ( e ) ( f ) Figure 9 : Examples of the sequential patterns identifed in DataExplorer . ( a ) Drilldown , ( b ) Sawtooth , ( c ) Sidestep , ( d ) Fanout , ( e ) Plug and Play , and ( f ) Side by side . Note the numbers in the x - axis were the sequential steps of the patterns found in another visual analytics tool . • Theme 2 : Real - time collaboration was integral to anal - yses . Designers might consider ways to support users who may need to drop into an ongoing analysis and quickly gain enough context to answer targeted questions . Future work might examine when these interactions take place in greater depth to suggest the best ways to support collaboration . • Theme 3 : Analyses were subjected to review by others . In addition to presenting fndings and providing a narrative for how a conclusion was reached , analysis products may consider how to support a rigorous review process as the feld matures . A majority of studies to date have focused on the user performing the analysis , but we see a future opportunity to understand how someone might interpret and seek to validate another person’s fndings . • Theme 4 : Participants expected to act on their fnd - ings . This theme highlighted the importance of considering design fows as users transition from the visual analysis product to other systems to take action . Future work might CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . consider what is expected to happen after a user achieves their analysis objective and how to best support their transi - tion into taking that action . • Theme 5 : Fragmentation of communication , data , and documentation resulted in frequent interleaving of other products during the analysis process . Our participants used many products during their analyses outside of the visual analytic system . We suggest that future work may need to consider what user actions and behaviors may be missed if focusing only on the visual analysis tool . 5 . 1 Limitations Although we sought to make our work as generalizable as possible , all of our participants were from Google and the second product we analyzed to validate that our patterns were not product - specifc was also part of the Google ecosystem . As a result , fndings may not gen - eralize to other user populations , companies , industrial settings , or product ecosystems . In our qualitative study , technical constraints meant that our sequence models were generated manually rather than through logs , which may have introduced some error in the actions taken , particularly when keyboard shortcuts were used . Our logs analysis method only used frequency as a criterion in generating the sequential patterns . Although this is standard prac - tice in many sequential pattern mining algorithms , other measures ( e . g . , confdence and lift ) might be explored in the future to generate sequential rules to take probability into account for the purposes of prediction [ 20 ] . Finally , some details and information about the products used by participants and our work is proprietary and therefore had to be omitted . 6 CONCLUSION As businesses generate large amounts of data , hungry for insights that will shape user behaviors and drive business decisions , the need to better understand and support a user’s ability to efec - tively perform visual analyses will only grow . The limitations of the human perceptual system and reduced pace of Moore’s law mean that we cannot assume our systems will scale to match in - creased data volumes . We must instead seek to better understand how users operate and achieve their objectives , evolving our visual analytic systems accordingly . To discover how to better support users in these scenarios , we conducted digital “feld” observations of nine users as they undertook 15 visual analytic investigations in Google’s ecosystem of tools and products . We identifed four common user objectives that participants described trying to ac - complish , six consistent action patterns , and fve emergent themes . We then performed a quantitative log analysis on DataExplorer , a non - public visual analysis tool , examining the logs of 2530 users over an 8 - day period to verify that these patterns were not product specifc . Our work presented the following contributions : • Real - world tasks and context through digital “feld” observation . We introduced a variant of feld observations and demonstrated how this method can efectively be used to understand real - world user objectives , analysis patterns , and strategies . • Four user objectives . We presented four common user objectives observed across 9 users from diferent areas of Google , working with four diferent datasets : ( 1 ) Check for anomalies or efects after a known change to either report success or investigate where and why data was not as ex - pected ; ( 2 ) Investigate where and why data was not what was expected to fx the problem ; ( 3 ) Determine if data signals matched known patterns to take action or inform others ; ( 4 ) Prototype or refne the defnition of a new metric so that others can use it . We also explained when these objectives corroborated prior fndings and where we found contextual diferences . • Six visual investigation patterns . We matched three pat - terns identifed in prior work : ( 1 ) Drilldown ; ( 2 ) Sawtooth ; ( 3 ) Side - by - side ( compare ) and presented three new patterns of low - level actions that were repeatedly leveraged by users to achieve their objectives : ( 4 ) Sidestep ; ( 5 ) Fanout ; and ( 6 ) Plug and Play . • Validation of pattern generalizability . We validated that these patterns were not product - specifc through sequential pattern mining of logs from 2530 user analysis sessions in DataExplorer , a second visual analysis tool . • Five emergent themes . We presented fve themes that emerged from observing collaboration behaviors and inter - actions within the Google ecosystem that we believe have not been considered in prior work : ( 1 ) a majority of analyses leveraged prior work as a starting point ; ( 2 ) real - time collab - oration was integral to analyses ; ( 3 ) analyses were subjected to review by others ; ( 4 ) participants expected to act on their fndings ; and ( 5 ) fragmentation of communication , data , and documentation resulted in frequent interleaving of other products during the analysis process . Our results provide evidence that some of the user goals and patterns found in prior literature generalize to a large - scale busi - ness setting and we supplemented these with the addition of three new investigative patterns . We also presented fve themes that we believe have yet to be explored , unique to our observations of visual analysis processes taking place in a full organizational ecosystem . Our fndings provide a window into how visual analyses take place within a natural ecosystem of products and people , providing a bridge to understanding how the rich foundational work grounded in scientifc and academic studies relates to the way users achieve their analysis objectives in large - scale business settings . ACKNOWLEDGMENTS We would like to thank Jorik Tangelder , Yannick Carer , and the DataExplorer team for all of their support in completing this work . REFERENCES [ 1 ] Syed Mohd Ali , Noopur Gupta , Gopal Krishna Nayak , and Rakesh Kumar Lenka . 2016 . Big data visualization : Tools and challenges . In 2016 2nd International Conference on Contemporary Computing and Informatics ( IC3I ) . IEEE , 656 – 660 . [ 2 ] Robert Amar , James Eagan , and John Stasko . 2005 . Low - level components of analytic activity in information visualization . In IEEE Symposium on Information Visualization , 2005 . INFOVIS 2005 . IEEE , 111 – 117 . [ 3 ] Robert Amar and John Stasko . 2004 . A knowledge task - based framework for design and evaluation of information visualizations . In IEEE Symposium on Infor - mation Visualization . IEEE , 143 – 150 . Understanding Visual Investigation Paterns Through Digital “Field” Observations CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA [ 4 ] Marcia J Bates . 1989 . The design of browsing and berrypicking techniques for the online search interface . Online review ( 1989 ) . [ 5 ] Leilani Battle , Remco Chang , and Michael Stonebraker . 2016 . Dynamic prefetch - ing of data tiles for interactive visualization . In Proceedings of the 2016 Interna - tional Conference on Management of Data . 1363 – 1375 . [ 6 ] Leilani Battle and Jefrey Heer . 2019 . Characterizing exploratory visual analysis : A literature review and evaluation of analytic provenance in tableau . In Computer Graphics Forum , Vol . 38 . Wiley Online Library , 145 – 159 . [ 7 ] Hugh Beyer and Karen Holtzblatt . 1999 . Contextual design . interactions 6 , 1 ( 1999 ) , 32 – 42 . [ 8 ] Ilya Boyandin , Enrico Bertini , and Denis Lalanne . 2012 . A qualitative study on the exploration of temporal changes in fow maps with animation and small - multiples . In Computer Graphics Forum , Vol . 31 . Wiley Online Library , 1005 – 1014 . [ 9 ] Matthew Brehmer and Tamara Munzner . 2013 . A multi - level typology of abstract visualization tasks . IEEE transactions on visualization and computer graphics 19 , 12 ( 2013 ) , 2376 – 2385 . [ 10 ] Eli T Brown , Alvitta Ottley , Helen Zhao , Quan Lin , Richard Souvenir , Alex En - dert , and Remco Chang . 2014 . Finding waldo : Learning about users from their interactions . IEEE Transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 1663 – 1672 . [ 11 ] Andreas Buja , Dianne Cook , and Deborah F Swayne . 1996 . Interactive high - dimensional data visualization . Journal of computational and graphical statistics 5 , 1 ( 1996 ) , 78 – 99 . [ 12 ] Remco Chang , Caroline Ziemkiewicz , Tera Marie Green , and William Ribarsky . 2009 . Defning insight for visual analytics . IEEE Computer Graphics and Applica - tions 29 , 2 ( 2009 ) , 14 – 17 . [ 13 ] Ed H Chi , Peter Pirolli , Kim Chen , and James Pitkow . 2001 . Using information scent to model user information needs and actions and the Web . In Proceedings of the SIGCHI conference on Human factors in computing systems . 490 – 497 . [ 14 ] Mei C Chuah and Steven F Roth . 1996 . On the semantics of interactive visual - izations . In Proceedings IEEE Symposium on Information Visualization’96 . IEEE , 29 – 36 . [ 15 ] Paula Cowley , Lucy Nowell , and Jean Scholtz . 2005 . Glass box : An instrumented infrastructure for supporting human interaction with information . In Proceedings of the 38th Annual Hawaii International Conference on System Sciences . IEEE , 296c – 296c . [ 16 ] Filip Dabek and Jesus J Caban . 2016 . A grammar - based approach for modeling user interactions and generating suggestions during the data exploration process . IEEE transactions on visualization and computer graphics 23 , 1 ( 2016 ) , 41 – 50 . [ 17 ] Alan Dix and Geofrey Ellis . 1998 . Starting simple : adding value to static visual - isation through simple interaction . In Proceedings of the working conference on Advanced visual interfaces . 124 – 134 . [ 18 ] Wenwen Dou , Dong Hyun Jeong , Felesia Stukes , William Ribarsky , Heather Richter Lipford , and Remco Chang . 2009 . Recovering reasoning processes from user interactions . IEEE Computer Graphics and Applications 29 , 3 ( 2009 ) , 52 – 61 . [ 19 ] Darren Edge , Nathalie Henry Riche , Jonathan Larson , and Christopher White . 2017 . Beyond tasks : An activity typology for visual analytics . IEEE transactions on visualization and computer graphics 24 , 1 ( 2017 ) , 267 – 277 . [ 20 ] Philippe Fournier - Viger , Ted Gueniche , Souleymane Zida , and Vincent S Tseng . 2014 . ERMiner : sequential rule mining using equivalence classes . In International Symposium on Intelligent Data Analysis . Springer , 108 – 119 . [ 21 ] Philippe Fournier - Viger , Jerry Chun - Wei Lin , Rage Uday Kiran , Yun Sing Koh , and Rincy Thomas . 2017 . A survey of sequential pattern mining . Data Science and Pattern Recognition 1 , 1 ( 2017 ) , 54 – 77 . [ 22 ] Fabio Fumarola , Pasqua Fabiana Lanotte , Michelangelo Ceci , and Donato Malerba . 2016 . CloFAST : closed sequential pattern mining using sparse and vertical id - lists . Knowledge and Information Systems 48 , 2 ( 2016 ) , 429 – 463 . [ 23 ] David Gotz and Zhen Wen . 2009 . Behavior - driven visualization recommendation . In Proceedings of the 14th international conference on Intelligent user interfaces . 315 – 324 . [ 24 ] David Gotz and Michelle X Zhou . 2009 . Characterizing users’ visual analytic activity for insight provenance . Information Visualization 8 , 1 ( 2009 ) , 42 – 55 . [ 25 ] Tera Marie Green , Ross Maciejewski , and Steve DiPaola . 2010 . ALIDA : Using machine learning for intent discernment in visual analytics interfaces . In 2010 IEEE Symposium on Visual Analytics Science and Technology . IEEE , 223 – 224 . [ 26 ] Hua Guo , Steven R Gomez , Caroline Ziemkiewicz , and David H Laidlaw . 2015 . A case study using visualization interaction logs and insight metrics to understand how analysts arrive at insights . IEEE transactions on visualization and computer graphics 22 , 1 ( 2015 ) , 51 – 60 . [ 27 ] Alexander Hall , Olaf Bachmann , Robert Büssow , Silviu Gănceanu , and Marc Nunkesser . 2012 . Processing a trillion cells per mouse click . arXiv preprint arXiv : 1208 . 0225 ( 2012 ) . [ 28 ] Jefrey Heer , Jock Mackinlay , Chris Stolte , and Maneesh Agrawala . 2008 . Graph - ical histories for visualization : Supporting analysis , communication , and eval - uation . IEEE transactions on visualization and computer graphics 14 , 6 ( 2008 ) , 1189 – 1196 . [ 29 ] Jefrey Heer and Ben Shneiderman . 2012 . Interactive dynamics for visual analysis . Commun . ACM 55 , 4 ( 2012 ) , 45 – 54 . [ 30 ] Daniel A Keim . 2001 . Visual exploration of large data sets . Commun . ACM 44 , 8 ( 2001 ) , 38 – 44 . [ 31 ] Daniel A Keim . 2002 . Information visualization and visual data mining . IEEE transactions on Visualization and Computer Graphics 8 , 1 ( 2002 ) , 1 – 8 . [ 32 ] Daniel A Keim , Florian Mansmann , Jörn Schneidewind , and Hartmut Ziegler . 2006 . Challenges in visual data analysis . In Tenth International Conference on Information Visualisation ( IV’06 ) . IEEE , 9 – 16 . [ 33 ] Heidi Lam , Melanie Tory , and Tamara Munzner . 2017 . Bridging from goals to tasks with design study analysis reports . IEEE transactions on visualization and computer graphics 24 , 1 ( 2017 ) , 435 – 445 . [ 34 ] Zhicheng Liu and Jefrey Heer . 2014 . The efects of interactive latency on ex - ploratory visual analysis . IEEE transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 2122 – 2131 . [ 35 ] Aidong Lu , Ross Maciejewski , and David S Ebert . 2010 . Volume composition and evaluation using eye - tracking data . ACM Transactions on Applied Perception ( TAP ) 7 , 1 ( 2010 ) , 1 – 20 . [ 36 ] Gary M Olson , James D Herbsleb , and Henry H Reuter . 1994 . Characterizing the sequential structure of interactive behaviors through statistical and grammatical techniques . Human – Computer Interaction 9 , 3 - 4 ( 1994 ) , 427 – 472 . [ 37 ] Adam Perer and Ben Shneiderman . 2008 . Systematic yet fexible discovery : guiding domain experts through exploratory data analysis . In Proceedings of the 13th international conference on Intelligent user interfaces . 109 – 118 . [ 38 ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage points for analyst technology as identifed through cognitive task analysis . In Proceedings of international conference on intelligence analysis , Vol . 5 . McLean , VA , USA , 2 – 4 . [ 39 ] Catherine Plaisant , Jean - Daniel Fekete , and Georges Grinstein . 2007 . Promoting insight - based evaluation of visualizations : From contest to benchmark repository . IEEE transactions on visualization and computer graphics 14 , 1 ( 2007 ) , 120 – 134 . [ 40 ] Khairi Reda , Andrew E Johnson , Jason Leigh , and Michael E Papka . 2014 . Eval - uating user behavior and strategy during visual exploration . In Proceedings of the Fifth Workshop on Beyond Time and Errors : Novel Evaluation Methods for Visualization . 41 – 45 . [ 41 ] Alexander Rind , Wolfgang Aigner , Markus Wagner , Silvia Miksch , and Tim Lammarsch . 2016 . Task cube : A three - dimensional conceptual space of user tasks in visualization design and evaluation . Information Visualization 15 , 4 ( 2016 ) , 288 – 300 . [ 42 ] Daniel M Russell , Mark J Stefk , Peter Pirolli , and Stuart K Card . 1993 . The cost structure of sensemaking . In Proceedings of the INTERACT’93 and CHI’93 conference on Human factors in computing systems . 269 – 276 . [ 43 ] Dominik Sacha , Andreas Stofel , Florian Stofel , Bum Chul Kwon , Geofrey Ellis , and Daniel A Keim . 2014 . Knowledge generation model for visual analytics . IEEE transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 1604 – 1613 . [ 44 ] Eliana Salvemini , Fabio Fumarola , Donato Malerba , and Jiawei Han . 2011 . Fast sequence mining based on sparse id - lists . In International Symposium on Method - ologies for Intelligent Systems . Springer , 316 – 325 . [ 45 ] Purvi Saraiya , Chris North , and Karen Duca . 2005 . An insight - based methodology for evaluating bioinformatics visualizations . IEEE transactions on visualization and computer graphics 11 , 4 ( 2005 ) , 443 – 456 . [ 46 ] Hans - Jörg Schulz , Thomas Nocke , Magnus Heitzler , and Heidrun Schumann . 2013 . A design space of visualization tasks . IEEE Transactions on Visualization and Computer Graphics 19 , 12 ( 2013 ) , 2366 – 2375 . [ 47 ] Ben Shneiderman . 2003 . The eyes have it : A task by data type taxonomy for information visualizations . In The craft of information visualization . Elsevier , 364 – 371 . [ 48 ] Rebecca R Springmeyer , Meera M Blattner , and Nelson L Max . 1992 . A charac - terization of the scientifc data analysis process . In Proceedings Visualization’92 . IEEE , 235 – 242 . [ 49 ] Tatiana von Landesberger , Sebastian Fiebig , Sebastian Bremm , Arjan Kuijper , and Dieter W Fellner . 2014 . Interaction taxonomy for tracking of user actions in visual analytics applications . In Handbook of Human Centric Visualization . Springer , 653 – 670 . [ 50 ] Leland Wilkinson . 2012 . The grammar of graphics . In Handbook of computational statistics . Springer , 375 – 414 . [ 51 ] Kai Xu , Alvitta Ottley , Conny Walchshofer , Marc Streit , Remco Chang , and John Wenskovitch . 2020 . Survey on the analysis of user interactions and visualization provenance . In Computer Graphics Forum , Vol . 39 . Wiley Online Library , 757 – 783 . [ 52 ] Jing Nathan Yan , Ziwei Gu , and Jefrey M Rzeszotarski . 2021 . Tessera : Discretiz - ing Data Analysis Workfows on a Task Level . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 15 . [ 53 ] Ji Soo Yi , Youn ah Kang , John Stasko , and Julie A Jacko . 2007 . Toward a deeper understanding of the role of interaction in information visualization . IEEE transactions on visualization and computer graphics 13 , 6 ( 2007 ) , 1224 – 1231 . [ 54 ] Ji Soo Yi , Youn - ah Kang , John T Stasko , and Julie A Jacko . 2008 . Understanding and characterizing insights : how do people gain insights using information visualization ? . In Proceedings of the 2008 Workshop on BEyond time and errors : novel evaLuation methods for Information Visualization . 1 – 6 . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Rae , et al . [ 55 ] Feng Zhou , Qianli Xu , and Roger Jianxin Jiao . 2011 . Fundamentals of product [ 56 ] Michelle X Zhou and Steven K Feiner . 1998 . Visual task characterization for ecosystem design for user experience . Research in Engineering Design 22 , 1 ( 2011 ) , automated visual discourse synthesis . In Proceedings of the SIGCHI conference on 43 – 61 . Human factors in computing systems . 392 – 399 .