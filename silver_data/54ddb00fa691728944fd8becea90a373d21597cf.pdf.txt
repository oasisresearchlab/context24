U NDERSTA NDI NG DEEP L EA RNI NG REQUI RES RE - THI NK I NG GENERA L I ZATI ON Chiyuan Zhang ∗ MassachusettsInstituteof Technology chiyuan @ mit . edu Samy Bengio GoogleBrain bengio @ goog e . com Moritz Hardt GoogleBrain mrtz @ goog e . com Benjamin Recht † University of California , Berkeley brecht @ berke ey . edu Oriol Vinyals GoogleDeepMind vinya s @ goog e . com A BSTRACT Despitetheir massivesize , successful deep artiﬁcial neural networkscan exhibit a remarkably small differencebetween training and test performance . Conventional wisdom attributessmall generalization error either to propertiesof themodel fam - ily , or to theregularization techniquesused during training . Through extensive systematic experiments , we show how these traditional ap - proaches fail to explain why large neural networks generalize well in practice . Speciﬁcally , our experimentsestablishthat state - of - the - art convolutional networks for image classiﬁcation trained with stochastic gradient methods easily ﬁt a ran - dom labeling of the training data . This phenomenon is qualitatively unaffected by explicit regularization , and occurs even if we replace the true images by com - pletely unstructured random noise . We corroborate these experimental ﬁndings with atheoretical construction showing that simpledepth two neural networksal - ready haveperfect ﬁnitesampleexpressivity assoon asthenumber of parameters exceedsthenumber of datapointsasit usually doesin practice . Weinterpret our experimental ﬁndingsby comparison with traditional models . 1 I NTRODUCTION Deep artiﬁcial neural networks often have far more trainable model parameters than the number of samples they are trained on . Nonetheless , some of these models exhibit remarkably small gener - alization error , i . e . , difference between “training error” and “test error” . At the same time , it is certainly easy to come up with natural model architectures that generalize poorly . What is it then that distinguishes neural networks that generalize well from those that don’t ? A satisfying answer to this question would not only help to make neural networks more interpretable , but it might also lead to moreprincipled and reliablemodel architecturedesign . To answer such aquestion , statistical learning theory hasproposed anumber of different complexity measuresthat arecapableof controlling generalization error . TheseincludeVC dimension ( Vapnik , 1998 ) , Rademacher complexity ( Bartlett & Mendelson , 2003 ) , and uniform stability ( Mukherjee et al . , 2002 ; Bousquet & Elisseeff , 2002 ; Poggio et al . , 2004 ) . Moreover , when the number of parameters is large , theory suggests that some form of regularization is needed to ensure small generalization error . Regularization may also beimplicit asisthecasewith early stopping . 1 . 1 O UR CONTRIBUTIONS In this work , we problematize the traditional view of generalization by showing that it is incapable of distinguishing between different neural networksthat haveradically different generalization per - formance . ∗ Work performed whileinterning at GoogleBrain . † Work performed at GoogleBrain . a r X i v : 1611 . 03530v2 [ c s . L G ] 26 F e b 2017 Randomization tests . At theheart of our methodology isavariant of thewell - known randomiza - tion test from non - parametric statistics ( Edgington & Onghena , 2007 ) . In aﬁrst set of experiments , we train several standard architectures on acopy of thedata wherethe true labels werereplaced by random labels . Our central ﬁnding can besummarized as : Deep neural networkseasily ﬁt randomlabels . More precisely , when trained on a completely random labeling of the true data , neural networks achieve 0 training error . The test error , of course , is no better than random chance as there is no correlation between the training labels and the test labels . In other words , by randomizing labels alone we can force the generalization error of a model to jump up considerably without changing the model , its size , hyperparameters , or the optimizer . We establish this fact for several different standard architectures trained on the CIFAR10 and ImageNet classiﬁcation benchmarks . While simpleto state , thisobservation hasprofound implicationsfrom astatistical learning perspective : 1 . Theeffectivecapacity of neural networksissufﬁcient for memorizing theentiredataset . 2 . Even optimization on random labels remains easy . In fact , training time increases only by asmall constant factor compared with training on thetruelabels . 3 . Randomizing labelsissolely adatatransformation , leaving all other propertiesof thelearn - ing problem unchanged . Extending on this ﬁrst set of experiments , we also replace the true images by completely random pixels ( e . g . , Gaussian noise ) and observethat convolutional neural networkscontinueto ﬁt thedata with zero training error . This shows that despite their structure , convolutional neural nets can ﬁt random noise . We furthermore vary the amount of randomization , interpolating smoothly between the case of no noise and complete noise . This leads to a range of intermediate learning problems where there remains some level of signal in the labels . We observe a steady deterioration of the generalization error as we increase the noise level . This shows that neural networks are able to capturetheremaining signal in thedata , whileat thesametimeﬁt thenoisy part using brute - force . Wediscussinfurther detail below how theseobservationsruleout all of VC - dimension , Rademacher complexity , and uniform stability as possible explanations for the generalization performance of state - of - the - art neural networks . Theroleof explicit regularization . If themodel architectureitself isn’t asufﬁcient regularizer , it remainstoseehow muchexplicit regularizationhelps . Weshow that explicit formsof regularization , such asweight decay , dropout , and dataaugmentation , do not adequately explain thegeneralization error of neural networks . Put differently : Explicit regularization may improvegeneralization performance , but isneither necessary nor by itself sufﬁcient for controlling generalization error . In contrast with classical convex empirical risk minimization , where explicit regularization is nec - essary to rule out trivial solutions , we found that regularization plays a rather different role in deep learning . It appears to be more of a tuning parameter that often helps improve the ﬁnal test error of a model , but the absence of all regularization does not necessarily imply poor generalization er - ror . Asreported by Krizhevsky et al . ( 2012 ) , 2 - regularization ( weight decay ) sometimeseven helps optimization , illustrating itspoorly understood naturein deep learning . Finite sample expressivity . We complement our empirical observations with a theoretical con - struction showing that generically large neural networks can express any labeling of the training data . Moreformally , weexhibit avery simpletwo - layer ReLU network with p = 2n + d parameters that can expressany labeling of any sampleof sizen in d dimensions . A previousconstruction due to Livni et al . ( 2014 ) achieved asimilar result with far moreparameters , namely , O ( dn ) . Whileour depth 2 network inevitably has large width , we can also come up with a depth k network in which each layer hasonly O ( n / k ) parameters . While prior expressivity results focused on what functions neural nets can represent over the entire domain , we focus instead on the expressivity of neural nets with regards to a ﬁnite sample . In contrast to existing depth separations ( Delalleau & Bengio , 2011 ; Eldan & Shamir , 2016 ; Telgarsky , 2016 ; Cohen & Shashua , 2016 ) in function space , our result shows that even depth - 2 networks of linear sizecan already represent any labeling of thetraining data . The role of implicit regularization . While explicit regularizers like dropout and weight - decay may not beessential for generalization , it iscertainly thecasethat not all modelsthat ﬁt thetraining data well generalize well . Indeed , in neural networks , we almost always choose our model as the output of running stochastic gradient descent . Appealing to linear models , we analyze how SGD acts as an implicit regularizer . For linear models , SGD always converges to a solution with small norm . Hence , the algorithm itself is implicitly regularizing the solution . Indeed , we show on small datasetsthat even Gaussian kernel methodscan generalizewell with no regularization . Though this doesn’t explain why certain architectures generalize better than other architectures , it does suggest that more investigation is needed to understand exactly what the properties are inherited by models that weretrained using SGD . 1 . 2 R ELATED WORK Hardt et al . ( 2016 ) givean upper bound on thegeneralization error of amodel trained with stochastic gradient descent in termsof thenumber of stepsgradient descent took . Their analysisgoesthrough thenotion of uniformstability ( Bousquet & Elisseeff , 2002 ) . Aswepoint out in thiswork , uniform stability of a learning algorithm is independent of the labeling of the training data . Hence , the concept is not strong enough to distinguish between the models trained on the true labels ( small generalization error ) and models trained on random labels ( high generalization error ) . This also highlightswhy theanalysisof Hardt et al . ( 2016 ) for non - convex optimizationwasrather pessimistic , allowing only avery few passesover thedata . Our resultsshow that even empirically training neural networks is not uniformly stable for many passes over the data . Consequently , a weaker stability notion isnecessary to makefurther progressalong thisdirection . Therehasbeen much work on therepresentational power of neural networks , starting from universal approximation theorems for multi - layer perceptrons ( Cybenko , 1989 ; Mhaskar , 1993 ; Delalleau & Bengio , 2011 ; Mhaskar & Poggio , 2016 ; Eldan & Shamir , 2016 ; Telgarsky , 2016 ; Cohen & Shashua , 2016 ) . All of these results are at the population level characterizing which mathematical functions certain families of neural networkscan express over the entire domain . We instead study the repre - sentational power of neural networksfor aﬁnitesampleof sizen . Thisleadsto avery simpleproof that even O ( n ) - sized two - layer perceptronshaveuniversal ﬁnite - sampleexpressivity . Bartlett ( 1998 ) proved bounds on the fat shattering dimension of multilayer perceptrons with sig - moid activations in terms of the 1 - norm of the weights at each node . This important result gives a generalization bound for neural nets that is independent of the network size . However , for RELU networks the 1 - norm is no longer informative . This leads to the question of whether there isa dif - ferent form of capacity control that bounds generalization error for large neural nets . This question was raised in a thought - provoking work by Neyshabur et al . ( 2014 ) , who argued through experi - mentsthat network sizeisnot themain form of capacity control for neural networks . An analogy to matrix factorization illustrated theimportanceof implicit regularization . 2 E FFECTIVE CAPACITY OF NEURAL NETWORKS Our goal is to understand the effective model capacity of feed - forward neural networks . Toward this goal , we choose a methodology inspired by non - parametric randomization tests . Speciﬁcally , we take a candidate architecture and train it both on the true data and on a copy of the data in which the true labels were replaced by random labels . In the second case , there is no longer any relationship between theinstancesand theclasslabels . Asaresult , learning isimpossible . Intuition suggests that this impossibility should manifest itself clearly during training , e . g . , by training not converging or slowing down substantially . To our surprise , several propertiesof thetraining process for multiple standard achitectures is largely unaffected by this transformation of the labels . This poses a conceptual challenge . Whatever justiﬁcation we had for expecting a small generalization error to begin with must no longer apply to thecaseof random labels . random pixels , the inputs are more separated from each other than natural images that originally belong to thesamecategory , therefore , easier to build anetwork for arbitrary label assignments . On theCIFAR10 dataset , Alexnet and MLPsall convergeto zero losson thetraining set . Theshaded rowsin Table1 show theexact numbersand experimental setup . Wealso testedrandomlabelson the ImageNet dataset . As shown in the last three rows of Table 2 in the appendix , although it does not reach theperfect 100 % top - 1 accuracy , 95 . 20 % accuracy isstill very surprising for amillion random labels from 1000 categories . Note that we did not do any hyperparameter tuning when switching fromthetruelabelstorandomlabels . It islikely that withsomemodiﬁcationof thehyperparameters , perfect accuracy could be achieved on random labels . The network also manages to reach ∼ 90 % top - 1 accuracy even with explicit regularizersturned on . Partially corrupted labels Wefurther inspect thebehavior of neural network training with avary - ing level of label corruptionsfrom 0 ( no corruption ) to 1 ( completerandom labels ) on theCIFAR10 dataset . The networks ﬁt the corrupted training set perfectly for all the cases . Figure 1b shows the slowdown of the convergence time with increasing level of label noises . Figure 1c depicts the test errors after convergence . Since the training errors are always zero , the test errors are the same as generalization errors . Asthenoiselevel approaches1 , thegeneralization errorsconvergeto 90 % — theperformanceof random guessing on CIFAR10 . 2 . 2 I MPLICATIONS In light of our randomization experiments , wediscusshow our ﬁndingsposeachallengefor several traditional approachesfor reasoning about generalization . Rademacher complexity and VC - dimension . Rademacher complexity is commonly used and ﬂexible complexity measure of a hypothesis class . The empirical Rademacher complexity of a hypothesisclassH on adataset { x 1 , . . . , x n } isdeﬁned as ˆ R n ( H ) = E σ [ sup h ∈ H 1 n n i = 1 σ i h ( x i ) ] ( 1 ) where σ 1 , . . . , σ n ∈ { ± 1 } are i . i . d . uniform random variables . This deﬁnition closely resembles our randomization test . Speciﬁcally , ˆ R n ( H ) measures ability of H to ﬁt random ± 1 binary label assignments . Whileweconsider multiclassproblems , it isstraightforward to consider related binary classiﬁcationproblemsfor whichthesameexperimental observationshold . Sinceour randomization tests suggest that many neural networks ﬁt the training set with random labels perfectly , we expect that ˆ R n ( H ) ≈ 1 for the corresponding model class H . This is , of course , a trivial upper bound on the Rademacher complexity that does not lead to useful generalization bounds in realistic settings . A similar reasoning applies to VC - dimension and its continuous analog fat - shattering dimension , unless we further restrict the network . While Bartlett ( 1998 ) proves a bound on the fat - shattering dimension in terms of 1 norm bounds on the weights of the network , this bound does not apply to theReLU networksthat weconsider here . Thisresult wasgeneralized to other normsby Neyshabur et al . ( 2015 ) , but even thesedo not seem to explain thegeneralization behavior that weobserve . Uniform stability . Stepping away from complexity measures of the hypothesis class , we can in - stead consider properties of the algorithm used for training . This is commonly done with some notion of stability , such as uniform stability ( Bousquet & Elisseeff , 2002 ) . Uniform stability of an algorithm A measureshow sensitivethe algorithm isto thereplacement of a single example . How - ever , it is solely a property of the algorithm , which does not take into account speciﬁcs of the data or thedistribution of thelabels . It ispossibleto deﬁneweaker notionsof stability ( Mukherjeeet al . , 2002 ; Poggio et al . , 2004 ; Shalev - Shwartz et al . , 2010 ) . The weakest stability measure is directly equivalent to bounding generalization error and does take the data into account . However , it has been difﬁcult to utilizethisweaker stability notion effectively . 3 T HE ROLE OF REGULARIZATION Most of our randomization tests are performed with explicit regularization turned off . Regularizers arethestandard tool in theory and practiceto mitigateoverﬁtting in theregimewhen therearemore Table 1 : The training and test accuracy ( in percentage ) of various models on the CIFAR10 dataset . Performance with and without data augmentation and weight decay are compared . The results of ﬁtting random labelsarealso included . model # params random crop weight decay train accuracy test accuracy Inception 1 , 649 , 402 yes yes 100 . 0 89 . 05 yes no 100 . 0 89 . 31 no yes 100 . 0 86 . 03 no no 100 . 0 85 . 75 ( ﬁtting random labels ) no no 100 . 0 9 . 78 Inception w / o BatchNorm 1 , 649 , 402 no yes 100 . 0 83 . 00 no no 100 . 0 82 . 00 ( ﬁtting random labels ) no no 100 . 0 10 . 12 Alexnet 1 , 387 , 786 yes yes 99 . 90 81 . 22 yes no 99 . 82 79 . 66 no yes 100 . 0 77 . 36 no no 100 . 0 76 . 07 ( ﬁtting random labels ) no no 99 . 82 9 . 86 MLP3x512 1 , 735 , 178 no yes 100 . 0 53 . 35 no no 100 . 0 52 . 39 ( ﬁtting random labels ) no no 100 . 0 10 . 48 MLP1x512 1 , 209 , 866 no yes 99 . 80 50 . 39 no no 100 . 0 50 . 51 ( ﬁtting random labels ) no no 99 . 34 10 . 61 parameters than data points ( Vapnik , 1998 ) . The basic idea is that although the original hypothesis istoo largeto generalizewell , regularizershelp conﬁnelearning to asubset of thehypothesisspace with manageable complexity . By adding an explicit regularizer , say by penalizing the norm of the optimal solution , the effective Rademacher complexity of the possible solutions is dramatically reduced . Aswewill see , in deep learning , explicit regularization seemsto play arather different role . Asthe bottom rows of Table 2 in the appendix show , even with dropout and weight decay , InceptionV3 is still ableto ﬁt therandom training set extremely well if not perfectly . Although not shown explicitly , on CIFAR10 , both Inception and MLPs still ﬁt perfectly the random training set with weight decay turned on . However , AlexNet with weight decay turned on fails to converge on random labels . To investigate the role of regularization in deep learning , we explicitly compare behavior of deep nets learning with and without regularizers . Instead of doing a full survey of all kinds of regularization techniques introduced for deep learn - ing , wesimply takeseveral commonly used network architectures , and comparethebehavior when turning off theequipped regularizers . Thefollowing regularizersarecovered : • Data augmentation : augment the training set via domain - speciﬁc transformations . For image data , commonly used transformations include random cropping , random perturba - tion of brightness , saturation , hueand contrast . • Weight decay : equivalent to a 2 regularizer on the weights ; also equivalent to a hard constrain of the weights to an Euclidean ball , with the radius decided by the amount of weight decay . • Dropout ( Srivastava et al . , 2014 ) : mask out each element of alayer output randomly with a given dropout probability . Only the Inception V3 for ImageNet uses dropout in our experiments . Table 1 shows the results of Inception , Alexnet and MLPs on CIFAR10 , toggling the use of data augmentation and weight decay . Both regularization techniques help to improve the generalization comparesthelearning curvesof thetwo variantsof Inception on CIFAR10 , with all theexplicit reg - ularizersturned off . Thenormalization operator helpsstablizethelearning dynamics , but theimpact on the generalization performance is only 3 ∼ 4 % . The exact accuracy is also listed in the section “Inception w / o BatchNorm” of Table1 . In summary , our observations on both explicit and implicit regularizers are consistently suggesting that regularizers , when properly tuned , could help to improvethegeneralization performance . How - ever , it isunlikely that theregularizersarethefundamental reasonfor generalization , asthenetworks continueto perform well after all theregularizersremoved . 4 F INITE - SAMPLE EXPRESSIVITY Much effort has gone into characterizing the expressivity of neural networks , e . g , Cybenko ( 1989 ) ; Mhaskar ( 1993 ) ; Delalleau & Bengio ( 2011 ) ; Mhaskar & Poggio ( 2016 ) ; Eldan & Shamir ( 2016 ) ; Telgarsky ( 2016 ) ; Cohen & Shashua ( 2016 ) . Almost all of theseresultsareat the“population level” showing what functions of the entire domain can and cannot be represented by certain classes of neural networkswiththesamenumber of parameters . For example , it isknownthat at thepopulation level depth k isgenerically morepowerful than depth k − 1 . We argue that what is more relevant in practice is the expressive power of neural networks on a ﬁnite sample of size n . It is possible to transfer population level results to ﬁnite sample results using uniform convergence theorems . However , such uniform convergence bounds would require thesample sizeto be polynomially large in the dimension of the input and exponential in thedepth of thenetwork , posing aclearly unrealistic requirement in practice . We instead directly analyze the ﬁnite - sample expressivity of neural networks , noting that this dra - matically simpliﬁesthepicture . Speciﬁcally , assoon asthenumber of parametersp of anetworksis greater than n , even simple two - layer neural networks can represent any function of the input sam - ple . Wesay that aneural network C can represent any function of asampleof sizen in d dimensions if for every sampleS ⊆ R d with | S | = n and every function f : S → R , thereexistsasetting of the weightsof C such that C ( x ) = f ( x ) for every x ∈ S . Theorem 1 . Thereexistsa two - layer neural network with ReLU activationsand 2n + d weightsthat can represent any function on a sampleof sizen in d dimensions . Theproof isgiveninSectionC in theappendix , wherewealsodiscusshow toachievewidth O ( n / k ) with depth k . We remark that it’s a simple exercise to giveboundson theweights of the coefﬁcient vectors in our construction . Lemma 1 gives a bound on the smallest eigenvalue of the matrix A . Thiscan beused to givereasonableboundson theweight of thesolution w . 5 I MPLICIT REGULARIZATION : AN APPEAL TO LINEAR MODELS Although deep neural netsremain mysteriousfor many reasons , wenotein thissection that it isnot necessarily easy to understand the source of generalization for linear models either . Indeed , it is useful to appeal to thesimplecaseof linear modelsto seeif thereareparallel insightsthat can help usbetter understand neural networks . Supposewecollect n distinct datapoints { ( x i , y i ) } wherex i ared - dimensional featurevectorsand y i are labels . Letting loss denote a nonnegative loss function with loss ( y , y ) = 0 , consider the empirical risk minimization ( ERM ) problem min w ∈ R d 1n ∑ ni = 1 loss ( w T x i , y i ) ( 2 ) If d ≥ n , then we can ﬁt any labeling . But is it then possible to generalize with such a rich model classand no explicit regularization ? Let X denote the n × d data matrix whose i - th row is x Ti . If X has rank n , then the system of equationsX w = y hasan inﬁnitenumber of solutionsregardlessof theright hand side . Wecan ﬁnd aglobal minimum in theERM problem ( 2 ) by simply solving thislinear system . But do all global minima generalize equally well ? Is there a way to determine when one global minimum will generalize whereas another will not ? One popular way to understand quality of minima is the curvature of the loss function at the solution . But in the linear case , the curvature of all optimal solutionsisthesame ( Choromanskaet al . , 2015 ) . To seethis , notethat in thecasewhen y i isascalar , ∇ 2 1n ∑ ni = 1 loss ( w T x i , y i ) = 1n X T diag ( β ) X , ( β i : = ∂ 2 loss ( z , y i ) ∂z 2 ∣∣∣ z = y i , ∀ i ) A similar formulacan befound when y isvector valued . In particular , theHessian isnot afunction of thechoiceof w . Moreover , theHessian isdegenerateat all global optimal solutions . If curvaturedoesn’t distinguish global minima , what does ? A promising direction isto consider the workhorsealgorithm , stochastic gradient descent ( SGD ) , and inspect which solution SGD converges to . Since the SGD update takes the form w t + 1 = w t − η t e t x i t where η t is the step size and e t is the prediction error loss . If w 0 = 0 , we must have that the solution has the form w = ∑ ni = 1 α i x i for some coefﬁcients α . Hence , if we run SGD we have that w = X T α lies in the span of the data points . If we also perfectly interpolate the labels we have X w = y . Enforcing both of these identities , thisreducesto thesingleequation X X T α = y ( 3 ) which has a unique solution . Note that this equation only depends on the dot - products between the data points x i . We have thus derived the “kernel trick” ( Sch¨olkopf et al . , 2001 ) —albeit in a roundabout fashion . We can therefore perfectly ﬁt any set of labels by forming the Gram matrix ( aka the kernel matrix ) on thedataK = X X T and solving thelinear system K α = y for α . Thisisan n × n linear system that can be solved on standard workstations whenever n is less than a hundred thousand , as is the casefor small benchmarkslikeCIFAR10 and MNIST . Quitesurprisingly , ﬁttingthetraininglabelsexactly yieldsexcellent performancefor convex models . On MNIST with no preprocessing , weareableto achieveatest error of 1 . 2 % by simply solving ( 3 ) . Notethat thisisnot exactly simpleasthekernel matrix requires30GB to storein memory . Nonethe - less , thissystem can besolved in under 3 minutesin on acommodity workstation with 24 coresand 256 GB of RAM with aconventional LAPACK call . By ﬁrst applying aGabor wavelet transform to thedataand then solving ( 3 ) , theerror on MNIST dropsto 0 . 6 % . Surprisingly , adding regularization doesnot improveeither model’sperformance ! Similar results follow for CIFAR10 . Simply applying a Gaussian kernel on pixels and using no regularization achieves 46 % test error . By preprocessing with a random convolutional neural net with 32 , 000 random ﬁlters , this test error drops to 17 % error 2 . Adding 2 regularization further reducesthisnumber to 15 % error . Notethat thisiswithout any dataaugmentation . Note that this kernel solution has an appealing interpretation in terms of implicit regularization . Simple algebra reveals that it is equivalent to the minimum 2 - norm solution of X w = y . That is , out of all models that exactly ﬁt the data , SGD will often converge to the solution with minimum norm . It isvery easy to construct solutionsof X w = y that don’t generalize : for example , onecould ﬁt aGaussian kernel to dataand placethecentersat random points . Another simpleexamplewould be to force the data to ﬁt random labels on the test data . In both cases , the norm of the solution is signiﬁcantly larger than theminimum norm solution . Unfortunately , this notion of minimum norm is not predictive of generalization performance . For example , returning to the MNIST example , the 2 - norm of the minimum norm solution with no preprocessing is approximately 220 . With wavelet preprocessing , the norm jumps to 390 . Yet the test error dropsby afactor of 2 . So whilethisminimum - norm intuition may providesomeguidance to new algorithm design , it isonly avery small pieceof thegeneralization story . 6 C ONCLUSION In thiswork wepresented asimpleexperimental framework for deﬁning and understanding anotion of effectivecapacity of machinelearning models . Theexperimentsweconducted emphasizethat the effective capacity of several successful neural network architectures is large enough to shatter the 2 Thisconv - net istheCoates & Ng ( 2012 ) net , but withtheﬁltersselectedat randominsteadof withk - means . training data . Consequently , thesemodelsarein principlerich enoughto memorizethetrainingdata . This situation poses a conceptual challenge to statistical learning theory as traditional measures of model complexity struggle to explain the generalization ability of large artiﬁcial neural networks . Wearguethat wehaveyet to discover apreciseformal measureunder which theseenormousmodels are simple . Another insight resulting from our experiments is that optimization continues to be empirically easy even if the resulting model does not generalize . This shows that the reasons for why optimization isempirically easy must bedifferent from thetruecauseof generalization . R EFERENCES Mart´ın Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Greg S . Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Ian Goodfellow , Andrew Harp , Geoffrey Irving , Michael Isard , Yangqing Jia , Rafal Jozefowicz , Lukasz Kaiser , Manjunath Kudlur , Josh Levenberg , Dan Man´e , Rajat Monga , Sherry Moore , Derek Murray , Chris Olah , Mike Schuster , Jonathon Shlens , Benoit Steiner , Ilya Sutskever , Kunal Talwar , Paul Tucker , Vin - cent Vanhoucke , Vijay Vasudevan , FernandaVi´egas , Oriol Vinyals , PeteWarden , Martin Watten - berg , Martin Wicke , Yuan Yu , and Xiaoqiang Zheng . TensorFlow : Large - scalemachinelearning on heterogeneous systems , 2015 . URL http : / / tensorf ow . org / . Software available from ten - sorﬂow . org . Peter L Bartlett . TheSampleComplexity of Pattern Classiﬁcation with Neural Networks - TheSize of theWeightsisMoreImportant than theSizeof theNetwork . IEEE Trans . Information Theory , 1998 . Peter L Bartlett and Shahar Mendelson . Rademacher and gaussian complexities : risk bounds and structural results . Journal of MachineLearning Research , 3 : 463 – 482 , March 2003 . Olivier Bousquet and Andr´e Elisseeff . Stability and generalization . Journal of Machine Learning Research , 2 : 499 – 526 , March 2002 . Anna Choromanska , Mikael Henaff , Michael Mathieu , G´erard Ben Arous , and Yann LeCun . The losssurfacesof multilayer networks . In AISTATS , 2015 . Adam Coates and Andrew Y . Ng . Learning feature representations with k - means . In Neural Net - works : Tricksof theTrade , Reloaded . Springer , 2012 . Nadav Cohen and Amnon Shashua . Convolutional Rectiﬁer Networks as Generalized Tensor De - compositions . In ICML , 2016 . G Cybenko . Approximation by superposition of sigmoidal functions . Mathematics of Control , Signalsand Systems , 2 ( 4 ) : 303 – 314 , 1989 . Olivier Delalleau and Yoshua Bengio . Shallow vs . Deep Sum - Product Networks . In Advances in Neural Information Processing Systems , 2011 . E . Edgington and P . Onghena . Randomization Tests . Statistics : A Series of Textbooks and Mono - graphs . Taylor & Francis , 2007 . ISBN 9781584885894 . Ronen Eldan and Ohad Shamir . The Power of Depth for Feedforward Neural Networks . In COLT , 2016 . Moritz Hardt , Benjamin Recht , and Yoram Singer . Train faster , generalize better : Stability of stochastic gradient descent . In ICML , 2016 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep Residual Learning for Image Recognition . In CVPR , 2016 . Sergey Ioffeand Christian Szegedy . Batch Normalization : Accelerating Deep Network Training by Reducing Internal CovariateShift . In ICML , 2015 . Alex Krizhevsky and Geoffrey Hinton . Learning multiplelayersof featuresfrom tiny images . Tech - nical report , Department of Computer Science , University of Toronto , 2009 . Alex Krizhevsky , Ilya Sutskever , and Geoffrey E Hinton . ImageNet Classiﬁcation with Deep Con - volutional Neural Networks . In Advancesin Neural Information Processing Systems , 2012 . Junhong Lin , Raffaello Camoriano , and Lorenzo Rosasco . Generalization Properties and Implicit Regularization for MultiplePassesSGM . In ICML , 2016 . Roi Livni , Shai Shalev - Shwartz , and Ohad Shamir . On the computational efﬁciency of training neural networks . In Advancesin Neural Information Processing Systems , 2014 . Hrushikesh Mhaskar and Tomaso A . Poggio . Deep vs . shallow networks : An approximation theory perspective . CoRR , abs / 1608 . 03287 , 2016 . URL http : / / arxiv . org / abs / 1608 . 03287 . Hrushikesh Narhar Mhaskar . Approximation propertiesof amultilayered feedforward artiﬁcial neu - ral network . Advancesin Computational Mathematics , 1 ( 1 ) : 61 – 80 , 1993 . Sayan Mukherjee , Partha Niyogi , Tomaso Poggio , and Ryan Rifkin . Statistical learning : Stability issufﬁcient for generalization and necessary and sufﬁcient for consistency of empirical risk min - imization . Technical Report AI Memo 2002 - 024 , MassachusettsInstituteof Technology , 2002 . Behnam Neyshabur , RyotaTomioka , and Nathan Srebro . In search of thereal inductivebias : On the roleof implicit regularization in deep learning . CoRR , abs / 1412 . 6614 , 2014 . Behnam Neyshabur , Ryota Tomioka , and Nathan Srebro . Norm - Based Capacity Control in Neural Networks . In COLT , pp . 1376 – 1401 , 2015 . Tomaso Poggio , Ryan Rifkin , Sayan Mukherjee , and ParthaNiyogi . General conditionsfor predic - tivity in learning theory . Nature , 428 ( 6981 ) : 419 – 422 , 2004 . Olga Russakovsky , Jia Deng , Hao Su , Jonathan Krause , Sanjeev Satheesh , Sean Ma , Zhiheng Huang , Andrej Karpathy , Aditya Khosla , Michael Bernstein , Alexander C . Berg , and Li Fei - Fei . Imagenet largescalevisual recognition challenge . International Journal of Computer Vision , 115 ( 3 ) : 211 – 252 , 2015 . ISSN 1573 - 1405 . doi : 10 . 1007 / s11263 - 015 - 0816 - y . BernhardSch¨olkopf , Ralf Herbrich , andAlex JSmola . A generalizedrepresenter theorem . InCOLT , 2001 . Shai Shalev - Shwartz , Ohad Shamir , Nathan Srebro , and Karthik Sridharan . Learnability , stability and uniform convergence . Journal of MachineLearning Research , 11 : 2635 – 2670 , October 2010 . Nitish Srivastava , Geoffrey E Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov . Dropout : asimpleway to prevent neural networksfrom overﬁtting . Journal of MachineLearning Research , 15 ( 1 ) : 1929 – 1958 , 2014 . Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jonathon Shlens , and Zbigniew Wojna . Re - thinking the inception architecture for computer vision . In CVPR , pp . 2818 – 2826 , 2016 . doi : 10 . 1109 / CVPR . 2016 . 308 . MatusTelgarsky . Beneﬁtsof depth in neural networks . In COLT , 2016 . Vladimir N . Vapnik . Statistical Learning Theory . Adaptiveand learning systemsfor signal process - ing , communications , and control . Wiley , 1998 . Yuan Yao , Lorenzo Rosasco , and Andrea Caponnetto . On early stopping in gradient descent learn - ing . ConstructiveApproximation , 26 ( 2 ) : 289 – 315 , 2007 . Conv Module C , KxK ﬁlters SxS strides Convolution C , KxK ﬁlters SxS strides Batch Norm Activation ReLU Inception Module Ch1 + Ch3 ﬁlters Conv Module Ch1 , 1x1 ﬁlters 1x1 strides Merge Concat in channels Conv Module Ch3 , 3x3 ﬁlters 1x1 strides Downsample Module Ch3 ﬁlters Conv Module Ch3 , 3x3 ﬁlters 2x2 strides Merge Concat in channels Max Pool 3x3 kernel 2x2 strides Inception ( Small ) 28x28x3 inputs Images 28x28x3 inputs Conv Module 96 , 3x3 ﬁlters 1x1 strides Inception Module 32 + 32 ﬁlters Inception Module 32 + 48 ﬁlters Downsample Module 80 ﬁlters Inception Module 80 + 80 ﬁlters Inception Module 48 + 96 ﬁlters Downsample Module 96 ﬁlters Inception Module 96 + 64 ﬁlters Inception Module 112 + 48 ﬁlters Inception Module 176 + 160 ﬁlters Inception Module 176 + 160 ﬁlters Mean Pooling 7x7 kernel ( global ) Fully Connected 10 - way outputs Figure 3 : The small Inception model adapted for the CIFAR10 dataset . On the left we show the Conv module , the Inception module and the Downsamp e module , which are used to construct the Inception architectureon theright . A E XPERIMENTAL SETUP We focus on two image classiﬁcation datasets , the CIFAR10 dataset ( Krizhevsky & Hinton , 2009 ) and theImageNet ( Russakovsky et al . , 2015 ) ILSVRC 2012 dataset . The CIFAR10 dataset contains 50 , 000 training and 10 , 000 validation images , split into 10 classes . Each image is of size 32x32 , with 3 color channels . We divide the pixel values by 255 to scale them into [ 0 , 1 ] , crop from the center to get 28x28 inputs , and then normalize them by subtract - ing the mean and dividing the adjusted standard deviation independently for each image with the per _ image _ whitening function in T ENSOR F LOW ( Abadi et al . , 2015 ) . For the experiment on CIFAR10 , we test a simpliﬁed Inception ( Szegedy et al . , 2016 ) and Alexnet ( Krizhevsky et al . , 2012 ) by adapting the architectures to smaller input image sizes . We also test standard multi - layer perceptrons ( MLPs ) with variousnumber of hidden layers . The small Inception model uses a combination of 1x1 and 3x3 convolution pathways . The detailed architecture is illustrated in Figure 3 . The small Alexnet is constructed by two ( convolution 5x5 → max - pool 3x3 → local - response - normalization ) modulesfollowed by two fully connected layers with 384 and 192 hidden units , respectively . Finally a 10 - way linear layer is used for prediction . The MLPs use fully connected layers . MLP 1x512 means one hidden layer with 512 hidden units . All of thearchitecturesusestandard rectiﬁed linear activation functions ( ReLU ) . For all experiments on CIFAR10 , we train using SGD with a momentum parameter of 0 . 9 . An initial learning rate of 0 . 1 ( for small Inception ) or 0 . 01 ( for small Alexnet and MLPs ) are used , with adecay factor of 0 . 95 per training epoch . Unlessotherwisespeciﬁed , for theexperimentswith randomized labelsor pixels , wetrain thenetworkswithout weight decay , dropout , or other formsof explicit regularization . Section 3 discussestheeffectsof variousregularizerson ﬁtting thenetworks and generalization . The ImageNet dataset contains 1 , 281 , 167 training and 50 , 000 validation images , split into 1000 classes . Each image is resized to 299x299 with 3 color channels . In the experiment on ImageNet , we use the Inception V3 ( Szegedy et al . , 2016 ) architecture and reuse the data preprocessing and experimental setup from the T ENSOR F LOW package . The data pipeline is extended to allow dis - abling of data augmentation and feeding random labels that are consistent across epochs . We run theImageNet experiment in adistributed asynchronized SGD system with 50 workers . B D ETAILED RESULTS ON I MAGENET Table 2 : The top - 1 and top - 5 accuracy ( in percentage ) of the Inception v3 model on the ImageNet dataset . We compare the training and test accuracy with various regularization turned on and off , for both true labels and random labels . The original reported top - 5 accuracy of the Alexnet on ILSVRC 2012 is also listed for reference . The numbers in parentheses are the best test accuracy during training , asareferencefor potential performancegain of early stopping . dataaug dropout weightdecay top - 1 train top - 5 train top - 1 test top - 5 test ImageNet 1000 classeswith theoriginal labels yes yes yes 92 . 18 99 . 21 77 . 84 93 . 92 yes no no 92 . 33 99 . 17 72 . 95 90 . 43 no no yes 90 . 60 100 . 0 67 . 18 ( 72 . 57 ) 86 . 44 ( 91 . 31 ) no no no 99 . 53 100 . 0 59 . 80 ( 63 . 16 ) 80 . 38 ( 84 . 49 ) Alexnet ( Krizhevsky et al . , 2012 ) - - - 83 . 6 ImageNet 1000 classeswith random labels no yes yes 91 . 18 97 . 95 0 . 09 0 . 49 no no yes 87 . 81 96 . 15 0 . 12 0 . 50 no no no 95 . 20 99 . 14 0 . 11 0 . 56 Table2 showstheperformanceon Imagenet with truelabelsand random labels , respectively . C P ROOF OF T HEOREM 1 Lemma 1 . For any two interleaving sequences of n real numbers b 1 < x 1 < b 2 < x 2 ··· < b n < x n , then × n matrix A = [ max { x i − b j , 0 } ] ij hasfull rank . Itssmallest eigenvalueismin i x i − b i . Proof . By its deﬁnition , the matrix A is lower triangular , that is , all entries with i < j vanish . A basic linear algebra fact states that a lower - triangular matrix has full rank if and only if all of the entries on the diagional are nonzero . Since , x i > b i , we have that max { x i − b i , 0 } > 0 . Hence , A is invertible . The second claim follows directly from the fact that a lower - triangular matrix has all itseigenvalueson themain diagonal . Thisin turn followsfrom theﬁrst fact , sinceA − λI can have lower rank only if λ equalsoneof thediagonal values . Proof of Theorem1 . For weight vectorsw , b ∈ R n and a ∈ R d , consider thefunction c : R n → R , c ( x ) = j = 1 w j max { 〈 a , x 〉 − b j , 0 } It iseasy to seethat c can beexpressed by adepth 2 network with ReLU activations . Now , ﬁx asampleS = { z 1 , . . . , z n } of sizen and atarget vector y ∈ R n . To provethetheorem , we need to ﬁnd weightsa , b , w so that y i = c ( z i ) for all i ∈ { 1 , . . . , n } First , choosea and bsuch that with x i = 〈 a , z i 〉 wehavetheinterleaving property b 1 < x 1 < b 2 < ··· < b n < x n . This is possible since all z i ’s are distinct . Next , consider the set of n equations in then unknownsw , y i = c ( z i ) , i ∈ { 1 , . . . , n } . We have c ( z i ) = Aw , where A = [ max { x i − b i , 0 } ] ij is the matrix we encountered in Lemma 1 . Wechosea and bso that thelemmaappliesand henceA hasfull rank . Wecan now solvethelinear system y = Aw to ﬁnd suitableweightsw . Whiletheconstruction in thepreviousproof hasinevitably high width given that thedepth is2 , it is possible to trade width for depth . The construction is as follows . With the notation from the proof and assuming w . l . o . g . that x 1 , . . . , x n ∈ [ 0 , 1 ] , partition the interval [ 0 , 1 ] into b disjoint intervals I 1 , . . . , I b so that each interval I j contains n / b points . At layer j , apply the construction from the proof to all pointsin I j . ThisrequiresO ( n / b ) nodesat level j . Thisconstruction resultsin acircuit of width O ( n / b ) and depth b + 1 which so far has b outputs ( one from each layer ) . It remains to implement a multiplexer which selects one of the b outputs based on which interval a given input x falls into . This boils down to implementing one ( approximate ) indicator function f j for each interval I j and outputting ∑ bj = 1 f j ( x ) o j , where o j is the output of layer j . This results in a single output circuit . Implementing asingleindicator function requiresconstant sizeand depth with ReLU activiations . Hence , theﬁnal sizeof theconstruction isO ( n ) and thedepth isb + cfor someconstant c . Setting k = b− c givesthenext corollary . Corollary 1 . For every k ≥ 2 , there exists neural network with ReLU activations of depth k , width O ( n / k ) and O ( n + d ) weights that can represent any function on a sample of size n in d dimensions . D R ESULTS OF IMPLICIT REGULARIZATION FOR LINEAR MODELS Table3 : Generalizing with kernels . Thetest error associated with solving thekernel equation ( 3 ) on small benchmarks . Note that changing the preprocessing can signiﬁcantly change the resulting test error . dataset pre - processing test error MNIST none 1 . 2 % MNIST gabor ﬁlters 0 . 6 % CIFAR10 none 46 % CIFAR10 random conv - net 17 % Table3 list theexperiment resultsof linear modelsdescribed in Section 5 . E F ITTING R ANDOM L ABELS WITH E XPLICIT R EGULARIZATION In Section 3 , we showed that it is difﬁcult to say that commonly used explicit regularizers count as a fundamental phase change in the generalization capability of deep nets . In this appendix , we add someexperimentsto investigatehow explicit regularizersaffect theability to ﬁt random labels . Table 4 : Results on ﬁtting random labels on the CIFAR10 dataset with weight decay and data aug - mentation . Model Regularizer Training Accuracy Inception Weight decay 100 % Alexnet Failed to converge MLP3x512 100 % MLP1x512 99 . 21 % Inception Random Cropping 1 99 . 93 % Augmentation 2 99 . 28 % From Table4 , wecan seethat for weight decay using thedefault coefﬁcient for each model , except Alexnet , all other models are still able to ﬁt random labels . We also tested random cropping and data augmentation with the Inception architecture . By changing the default weight decay factor from 0 . 95 to 0 . 999 , and running for more epochs , we observe overﬁtting to random labels in both cases . It isexpected to takelonger to convergebecausedataaugmentation explodesthetraining set size ( though many samplesarenot i . i . d . any more ) . 1 In random cropping and augmentation , a new randomly modiﬁed image is used in each epoch , but the ( randomly assigned ) labels are kept consistent for all the epochs . The “training accuracy” means a slightly different thing hereasthetraining set isdifferent in each epoch . Theglobal averageof theonlineaccuracy at each mini - batch on theaugmented samplesisreported here . 2 Dataaugmentation includesrandom left - right ﬂipping and random rotation up to 25 degrees .