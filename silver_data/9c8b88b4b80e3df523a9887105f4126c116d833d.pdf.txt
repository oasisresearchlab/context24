Learning Interpretable Models of Aircraft Handling Behaviour by Reinforcement Learning from Human Feedback Tom Bewley ∗ , Jonathan Lawry † and Arthur Richards ‡ University of Bristol , Bristol , United Kingdom We propose a method to capture the handling abilities of fast jet pilots in a software model via reinforcement learning ( RL ) from human preference feedback . We use pairwise preferences over simulated flight trajectories to learn an interpretable rule - based model called a reward tree , which enables the automated scoring of trajectories alongside an explanatory rationale . We train an RL agent to execute high - quality handling behaviour by using the reward tree as the objective , and thereby generate data for iterative preference collection and further refinement of both tree and agent . Experiments with synthetic preferences show reward trees to be competitive with uninterpretable neural network reward models on quantitative and qualitative evaluations . I . Introduction Pilots of fast jet aircraft require exceptional handling abilities , acquired over years of advanced training . There would be significant practical value in a method capable of distilling the skills , knowledge and preferences of pilots and other domain experts into a software model that captures realistic handling behaviour . The scalability of such a model would make it useful for strategic planning exercises , training , and development and testing of other software systems . This would enable greater return from the scarce resource of human piloting expertise . This vision faces the practical challenge of accurately eliciting the desired knowledge for codification into an automated system . As in many contexts requiring intuitive decision - making and rapid motor control , experts know good handling when they see it , but cannot always express why in formal or linguistic terms [ 1 ] . ∗ An explicit knowledge elicitation strategy would also likely be time - intensive , as would any approach relying on expert demonstration . This motivates a learning - based approach using a sparser data source . In light of the importance of transparency for safety - critical aviation applications [ 2 , 3 ] , it is crucial that any such approach learns an interpretable ( i . e . human - readable and comprehensible ) model of the expert knowledge , to facilitate trust and verification . This paper proposes a possible solution to this brief . We use an artificial reinforcement learning ( RL ) agent to generate a dataset of simulated flight trajectories , then consult an expert to obtain pairwise preferences over those trajectories , indicating which is preferred as a solution to a given task of interest . Pairwise preference elicitation is known to be robust and time - efficient , and provides a basis for combining data from multiple experts without the challenge of agreeing a common scoring system . We then use statistical learning algorithms to construct an interpretable explanatory model of the gathered preferences in the form of a rule - based tree structure . In turn , the tree is used as a reward function to train the agent to generate higher - quality trajectories , and the process is iterated to convergence . The end result is two distinct outputs that could form valuable components of future planning , training and development software : 1 ) A tree - structured reward function ( referred to as a reward tree ) , which captures tacit expert preferences in a human - readable form . A reward tree may be used for consistent automated scoring of flight trajectories executed by human or artificial pilots in a way that is aligned with the judgement that the original expert would have made , alongside an explanatory rationale that can be used to justify , verify and improve handling behaviour . 2 ) An RL agent capable of executing high - quality handling behaviour with respect to the objective specified by the reward tree , for use in simulation ( e . g . as a scalable demonstrator for pilot training ) . RL for aviation is well - studied , with impressive results using neural networks ( NNs ) , but typically requires reward to be heuristically ( thus potentially erroneously ) defined rather than flexibly learnt , and lacks interpretability of the underlying model . Our proposal is a model that is effective , interpretable and learnable from human feedback . ∗ PhD Student , Department of Engineering Mathematics . Email : tom . bewley @ bristol . ac . uk . † Professor , Department of Engineering Mathematics . Email : j . lawry @ bristol . ac . uk . ‡ Professor , Department of Aerospace Engineering and Bristol Robotics Laboratory . Email : arthur . richards @ bristol . ac . uk . ∗ This statement certainly underestimates the rich complexity of human expertise ; in reality , an expert’s mental model is likely to be partly tacit and partly explicit . The strategy taken in this paper is to operate as if the mental model were 100 % tacit , and explore what can be achieved under such a strong restriction . Real - world applications would likely benefit from combining this approach with some amount of hand - coded expert knowledge . 1 a r X i v : 2305 . 16924v1 [ c s . A I ] 26 M a y 2023 Having explored the fundamentals of preference - based reward tree learning in some simple benchmark cases in prior work [ 4 ] , our primary aim in this paper is to perform a proof - of - concept application to a more complex problem motivated by a real industrial need . In particular , we seek to compare the fast jet handling performance of RL agents trained using learnt reward trees to those using deep NNs as their reward learning models , which represent the state - of - the - art from prior work [ 5 , 6 ] but lack the crucial interpretability properties required for high - stakes applications . To perform this comparative evaluation at scale , we require a large quantity of preference data for multiple fast jet handling tasks . Since collecting such data from real pilots and aviation experts would be costly and logistically complex , our proof - of - concept experiments use synthetic preferences with respect to nominal oracle evaluator functions of varying complexity . Using oracles as a proxy for human evaluators is popular [ 5 – 9 ] as it enables scalable systematic comparison , with the ability to quantify performance ( and in our case , appraise learnt trees ) in terms of the reconstruction of a known ground truth . However , emulating a human with an oracle that responds with perfect rationality is unrealistic [ 10 ] . For this reason , we also examine the performance impacts of noisy and myopic oracles , and a restricted data budget . This paper describes experiments with synthetic oracle preference data for three fast jet handling tasks . We find that reward trees can be competitive with NN reward models in both quantitative and qualitative aspects of learning performance , while having the advantage of human - readability . Our secondary contributions include improvements to our original learning algorithm , and an illustrative analysis of learnt tree structures to demonstrate their interpretability . II . Background and Related Work Markov Decision Processes ( MDPs ) In this canonical formulation of sequential decision making [ 11 ] , the state of a system at time 𝑡 , 𝑠 𝑡 ∈ S , and the action of an agent , 𝑎 𝑡 ∈ A , condition the successor state 𝑠 𝑡 + 1 according to dynamics 𝐷 : S × A → Δ ( S ) . A reward function 𝑅 : S × A × S → R then outputs a scalar reward 𝑟 𝑡 + 1 given 𝑠 𝑡 , 𝑎 𝑡 and 𝑠 𝑡 + 1 . RL algorithms use exploratory data collection to learn action - selection policies 𝜋 : S → Δ ( A ) for MDPs , with the goal of maximising the expected discounted sum of future reward , E 𝐷 , 𝜋 (cid:205) ∞ ℎ = 0 𝛾 ℎ 𝑟 𝑡 + ℎ + 1 , 𝛾 ∈ [ 0 , 1 ] . RL for Aviation RL has seen widespread adoption in aviation [ 12 – 14 ] . It has been used to learn landing [ 15 ] and aerobatics [ 16 ] behaviours for fixed - wing aircraft , as an alternative or supplement to learning from costly human demonstrations [ 17 , 18 ] . Other work has retained a focus on learning from and with humans , using RL to predict pilot interactions in an airspace [ 19 ] , implement shared autonomy for single aircraft control [ 20 ] , and simulate student learning dynamics in pilot training [ 21 ] . Our proposal is an alternative integration of humans into the RL process . Reward Learning In the usual MDP framing , reward is an immutable property of the environment , which belies the practical fact that AI objectives originate in the uncertain goals and preferences of humans [ 22 ] . Reward learning [ 23 ] replaces hand - specified reward functions with models learnt from feedback signals such as demonstrations [ 24 ] , scalar evaluations [ 25 ] , approval labels [ 7 ] , and corrections [ 26 ] . In this work , we adopt the preference - based approach [ 5 ] , in which a human observes pairs of agent trajectories and expresses which of each pair they prefer as a solution to a given task of interest . A reward function is learnt to reconstruct the pattern of preferences . This approach is popular [ 6 , 27 – 29 ] and has a firm psychological basis . Experimental results indicate that humans find it easier to make relative ( vs . absolute ) quality judgements [ 30 , 31 ] and exhibit lower variance when doing so [ 32 ] . Providing preferences also incurs less cognitive burden than providing demonstrations [ 33 ] and may enable more fine - grained distinctions [ 34 ] . Explainable RL ( XRL ) Surveys of methods for making RL understandable to humans [ 35 , 36 ] divide between intrinsic approaches , which imbue agents with structure such as object representations [ 37 ] or symbolic policy primitives [ 38 ] , and post hoc analyses of learnt policies [ 39 ] , including feature importance [ 40 ] . Spatiotemporal scope varies from the local explanation of single actions [ 41 ] to the summary of entire policies via prototype trajectories [ 42 ] or states [ 43 ] . While most post hoc methods focus on fixed policies , some investigate the dynamics of agent learning [ 44 , 45 ] . Interpretable Reward Functions At the intersection of reward learning and XRL lie efforts to understand reward functions and their effects on action selection . While this area is “less developed " than others in XRL [ 46 ] , there exist both intrinsic approaches , which decompose rewards into semantic components [ 47 ] or optimise for sparsity [ 48 ] , and post hoc approaches , which apply feature importance analysis [ 49 ] , counterfactual probing [ 50 ] , or simplifying transformations [ 51 ] . Reward tree learning is an intrinsic approach , as the rule structure is inherently readable . Trees in RL Trees have a long history in RL [ 52 – 54 ] . Their use is increasingly given an XRL motivation . Applications again divide into intrinsic methods , where an agent’s policy [ 55 ] , value function [ 56 ] or dynamics model [ 57 ] is a tree , and post hoc tree approximations of an existing agent’s policy [ 58 ] or transition statistics [ 45 ] . Related to our focus on learning from humans , Cobo et al . [ 59 ] learn tree - structured MDP abstractions from demonstrations and Tambwekar et al . [ 60 ] distil a differentiable tree policy from natural language . While Sheikh et al . [ 61 ] use tree evolution to learn dense intrinsic rewards from sparse environment ones , our prior work [ 4 ] is the first to learn reward trees in the absence of any ground - truth reward signal , and the first to do so from human feedback . 2 III . Aircraft Handling Environment , Tasks and Oracles To formulate the aircraft handling problem as an MDP , we consider a simple set piece setup , in which the piloting agent is given a short time window ( called an episode ) to manoeuvre its aircraft ( the ego jet , EJ ) in a particular manner relative to a second reference jet ( RJ ) whose motion , if any , is considered part of the environment dynamics . The state space S contains the positions , attitudes , velocities and accelerations of both EJ and RJ , and the action space A consists of pitch , roll , yaw and thrust demands for EJ only . The EJ dynamics function integrates these demands with a simplified physics engine , including gravity and air resistance . RJ dynamics , as well as the conditions of episode initialisation and termination , vary between tasks ( see below ) . This set piece formulation strikes a balance between simplicity and generality ; many realistic scenarios faced by a fast jet pilot involve interaction with a single other airborne entity . It provides scope for the definition of many alternative tasks given the same state and action spaces , and largely unchanged dynamics . In this work , we consider the three tasks shown in Figure 1 : • Follow : RJ follows a linear horizontal flight path at a constant velocity , which is oriented opposite to the initial velocity of EJ . The task is to turn onto and then maintain the path up to the episode time limit of 20 timesteps ( ≈ 20 seconds , as timesteps are at approximately 1Hz ) . This constitutes a very simple form of formation flight . • Chase : RJ follows an erratic trajectory generated by random control inputs , and the task is to chase it , maintaining distance and line of sight , without taking EJ below a safe altitude . Episodes terminate after 20 timesteps . • Land : The task is to execute a safe approach towards landing on a runway , where RJ represents the ideal landing position ( central , zero altitude , slight upward pitch ) . EJ is initialised at a random altitude , pitch , roll and offset , such that landing may be challenging but always physically possible . An episode terminates if EJ passes RJ along the axis of the runway , or after 25 timesteps otherwise . Ego Jet Action : pitch , roll , yaw , thrust demands for EJ RJ EJ Follow RJ EJ Chase RJ EJ Land Reference Jet State : pose informationfor both EJ and RJ Proceedsalong linear flight path Task : turn onto same flight path and matchspeed Representsa target touchdownpose Task : execute a stable approach path , ending at target pose Fliessubject to randomcontrolinput Task : stay close to RJ with line of sight ; keep safe altitude Fig . 1 State - action space of aircraft handling domain , and diagrams of Follow , Chase and Land tasks . The central thesis of this paper is that there exists no unambiguous model of good aircraft handling behaviour . For instance , experts may agree that the Chase task involves a trade - off between speed of response and smoothness of flight , but may also have nebulous and divergent definitions of these properties and their relative importance . However , to quantitatively evaluate our method , we adopt the artificial construct of synthetic oracles based on ‘ground - truth’ reward functions , as proxies for real human evaluators . The oracles functions use their reward functions to provide evaluative preference feedback to our reward learning method according to a model defined in Section IV . The precise nature of the oracle reward functions is secondary , and those given below are among many equally reasonable alternatives , but we dedicated several hours of development time to ensuring they incentivise reasonable behaviour upon visual inspection . The difficulty and seeming arbitrariness of such a manual reward design process is precisely why reward learning ( ultimately from real human preferences ) is a compelling proposition . The oracles are all defined using a common set of state - action features , which are enumerated and described in Table 2 ( see Appendix ) : • Follow : The oracle prioritises closing the distance between EJ and RJ , and matching their upward axes : 𝑅 follow = − ( dist + 0 . 05 × closing speed + 10 × up error ) . • Chase : The oracle prioritises keeping RJ at a distance of 20 and within EJ’s line of sight , while keeping EJ oriented upright . It also has a large penalty for dropping below a safe altitude of 50 ( note that the square brackets [ · ] are Iverson notation , which is used throughout this paper for indicator functions ) : 𝑅 chase = − ( abs ( dist − 20 ) + 10 × los error + 5 × abs roll + 100 × [ alt < 50 ] ) . • Land : The oracle for this task is the most complex , including terms that incentivise continual descent , penalise g - force and engine thrust , and punish EJ for contacting the ground before the runway [ alt < 0 . 6 ] : 𝑅 land = − ( 0 . 05 × abs lr offset + 0 . 05 × alt + hdg error + abs roll + 0 . 5 × pitch error + 0 . 25 × ( yaw rate + roll rate + pitch rate ) + 0 . 1 × g force + 0 . 025 × thrust + 0 . 05 × delta thrust + [ delta dist hor > 0 ] + 2 × [ delta alt > 0 ] + [ abs lr offset > 10 ] + 10 × [ alt < 0 . 6 ] ) . 3 IV . Preference - based Reward Learning We adopt the preference - based approach to reward learning , in which a human ( or in our case , oracle ) evaluator is presented with pairs of agent trajectories ( sequences of state , action , next state transitions ) and expresses which they prefer as a solution to a given task of interest . A reward function is learnt to reconstruct the pattern of preferences . We assume that the evaluator observes a trajectory 𝜉 𝑖 as a sequence ( x 𝑖 1 , . . . , x 𝑖𝑇 𝑖 ) , where x 𝑖𝑡 = 𝜙 ( 𝑠 𝑖𝑡 − 1 , 𝑎 𝑖𝑡 − 1 , 𝑠 𝑖𝑡 ) ∈ R 𝐹 represents a single transition as an 𝐹 - dimensional feature vector . The choice of feature function 𝜙 is crucial . For our experiments , we consulted with engineers with experience in aerospace simulation and control algorithms to define 𝐹 = 30 features that are sufficiently expressive to capture the important information for all three of our target tasks , without being overly specialised to one or providing too much explicit guidance to the reward learning process . These features are given in Table 2 ( see Appendix ) . Note that all three oracles given above use a subset of the 30 features , meaning that optimal reward learning is possible in principle but requires accurate feature selection . Given a set of 𝑁 trajectories , Ξ = { 𝜉 𝑖 } 𝑁𝑖 = 1 , the evaluator is consulted to provide 𝐾 ≤ 𝑁 ( 𝑁 − 1 ) / 2 pairwise preference labels , L = { ( 𝑖 , 𝑗 ) } 𝐾𝑘 = 1 , each of which indicates that the 𝑗 th trajectory is preferred to the 𝑖 th ( denoted by 𝜉 𝑗 ≻ 𝜉 𝑖 ) . Figure 2 ( top ) shows how a preference dataset D = ( Ξ , L ) can be viewed as a directed graph . To learn a reward function from D , we must assume a generative preference model . Typically , it is assumed that the evaluator tends to prefer trajectories that have higher summed reward ( or return ) according to a latent reward function over the feature space , 𝑅 : R 𝐹 → R , which represents their tacit understanding of the task . However , they are liable to occasionally make mistakes in their judgement . This is formalised by the Bradley - Terry preference model [ 62 ] : 𝑃 ( 𝜉 𝑗 ≻ 𝜉 𝑖 | 𝑅 ) = 1 1 + exp ( 1 𝛽 ( 𝐺 ( 𝜉 𝑖 | 𝑅 ) − 𝐺 ( 𝜉 𝑗 | 𝑅 ) ) ) , ( 1 ) where 𝐺 ( 𝜉 𝑖 | 𝑅 ) = (cid:205) 𝑇 𝑖 𝑡 = 1 𝑅 ( x 𝑖𝑡 ) and 𝛽 > 0 is a temperature coefficient determining the probability of mistakes . In our oracle experiments , synthetic preferences are generated according to Equation 1 using the corresponding 𝑅 follow , 𝑅 chase or 𝑅 land , thereby adhering to the modelling assumption . In our main experiments , we set 𝛽 = 0 , which results in the oracles deterministically selecting trajectories with higher return ( ties broken uniform - randomly ) . We subsequently study cases with 𝛽 > 0 , which provide a more realistic emulation of real human preference data . Given a preference dataset and an assumed generative model such as Equation 1 , the objective of reward learning is to approximate the latent reward function 𝑅 within some learnable function class R . This problem is often formalised as minimising the negative log - likelihood ( NLL ) loss over the preferences in L [ 5 , 6 ] . Wirth et al . [ 27 ] also use the discrete 0 - 1 loss , which considers only the directions of predicted preferences rather than their strengths . Our model uses both of these losses at different stages in its learning process . They are respectively defined as follows : ℓ NLL ( D , 𝑅 ) = ∑︁ ( 𝑖 , 𝑗 ) ∈L − log 𝑃 ( 𝜉 𝑗 ≻ 𝜉 𝑖 | 𝑅 ) ; ( 2 ) ℓ 0 - 1 ( D , 𝑅 ) = ∑︁ ( 𝑖 , 𝑗 ) ∈L [ 𝑃 ( 𝜉 𝑗 ≻ 𝜉 𝑖 | 𝑅 ) ≤ 0 . 5 ] . ( 3 ) In early prior work , R was often the class of linear models 𝑅 ( x ) = w ⊤ x [ 28 ] , which are easy to interpret but have limited expressiveness , so cannot scale to complex tasks . More recently , it has been common to use deep NNs [ 5 ] ( or multi - network ensembles thereof ) , which are far more powerful , while remaining tractably learnable by gradient - based optimisation . However , the complex multilayered architecture of a deep NN resists human scrutiny and interpretation . This motivates a ‘third way’ function class that compromises between these two extremes . V . Interpretable Reward Learning with Trees As an intermediate option between the limited expressiveness of linear models and the uninterpretable complexity of NNs , our recent prior work proposes reward trees [ 4 ] . Here , R is the class of axis - aligned decision trees , which are hierarchies of local rules constructed from the feature set . Reward trees admit visual and textual representation , and produce a traceable decision path for each reward prediction . While methods exist for learning trees by gradient - based optimisation [ 63 ] , these are of the oblique ( c . f . axis - aligned ) kind , whose multi - feature rules are far harder to interpret in high dimensions . Therefore , instead of optimising one of the losses from Equations 2 and 3 end - to - end , we use a multi - stage induction method with a proxy objective at each stage . The four stages outlined below , and depicted in Figure 2 , depart from our original method in several respects , which we highlight and justify where relevant . A . Trajectory - level Return Estimation This first stage of our method considers the 𝑁 trajectories as atomic units , and uses the preference graph to construct a vector of return estimates g ∈ R 𝑁 , which should be higher for more preferred trajectories ( blue in Figure 2 ( V . A ) , c . f . red ) . This is a vanilla preference - based ranking problem , and admits a standard solution . In [ 4 ] , we use a least squares 4 A dataset of pairwise preferences over trajectories can be visualised as a directed graph , where nodes are trajectories and edges are preferences . Each trajectory is a sequence of vectors in feature space . In turn , each vector represents the ( state , action , next state ) transition that occurs at a particular timestep t . Feature vector for timestep t : mapping Given an existing tree structure and trajectories with estimated returns , we define the reward for each leaf as a timestep - weighted average over the trajectories that visit that leaf . V . B Naive assumption : all timesteps in trajectory contribute equally to return ( i . e . uniform colour ) Reward calculation for highlighted leaf : Growing a tree by adding rules recursively splits the feature space into an increasing number of leaves . In our improved growth method , we use the 0 - 1 loss as the criterion for selecting rules to add . V . C Growth tends to separate high - and low - return trajectories into different leaves " Pure " leaf containing a single trajectory After growing to a maximum size , a final pruning stage recursively removes rules to minimise the 0 - 1 loss , with an additional regularisation term to penalise large trees . This yields a reduced tree for use as a reward function . V . D Pruning merges leaves back together , while retaining the most important splits for preference prediction Reward values shown are illustrative V . A Before doing anything in the feature space , we first use the standard Bradley - Terry preference model to estimate an overall return value for each trajectory in the dataset based on its preferences . Two favourable preferences ; highest estimated return Three unfavourable preferences ; lowest estimated return Preferred to but not to ; intermediate estimated return Each edge points to preferred trajectory Fig . 2 Top : The input to preference - based reward learning is a directed graph over a trajectory set , each of which is a sequence of points in R 𝐹 ( blue connectors show mapping ) . Bottom : The four stages of reward tree learning : return estimation ( Section V . A ) ; leaf - level reward prediction ( V . B ) ; tree growth ( V . C ) ; pruning ( V . D ) . matrix method to solve for g under Thurstone’s Case V preference model [ 64 ] . For consistency with prior work , and to avoid an awkward clipping step which biases preference probabilities to enable matrix inversion , we now use a gradient method to minimise ℓ NLL under the Bradley - Terry model instead . Concretely , the objective for this stage is argmin g ∈ R 𝑁 (cid:104) ∑︁ ( 𝑖 , 𝑗 ) ∈L − log 1 1 + exp ( g 𝑖 − g 𝑗 ) (cid:105) , ( 4 ) which we optimise by gradient descent using the Adam optimiser [ 65 ] . As a minor detail , we post - normalise g so that its elements have a standard deviation equal to the mean trajectory length in Ξ , (cid:205) 𝑁𝑖 = 1 𝑇 𝑖 / 𝑁 , and a common sign ( i . e . all positive or negative ) . We anecdotally find that this aids the interpretability of the final reward tree . B . Leaf - level Reward Prediction The vector g estimates trajectory returns , but the ultimate aim of reward learning is to decompose these into sums of rewards for the constituent transitions , then generalise to make reward predictions for unseen data ( e . g . novel trajectories executed by an RL agent ) . Our core contribution is to do this using a tree model T , consisting of a hierarchy of rules that partition the transition - level feature space R 𝐹 into 𝐿 T hyperrectangular regions called leaves . Given such a tree , each leaf 𝑙 ∈ { 1 . . 𝐿 T } is associated with a reward prediction r 𝑙 as follows . Let the function leaf T : R 𝐹 → { 1 . . 𝐿 T } map a feature vector x ∈ R 𝐹 to the leaf in which it resides by propagating it through the rule hierarchy . r 𝑙 is defined as an 5 average over g , weighted by the proportion of time that each trajectory in Ξ spends in 𝑙 : r 𝑙 = 𝑁 ∑︁ 𝑖 = 1 g 𝑖 𝑇 𝑖 (cid:205) 𝑇 𝑖 𝑡 = 1 [ leaf T ( x 𝑖𝑡 ) = 𝑙 ] (cid:205) 𝑁𝑗 = 1 (cid:205) 𝑇 𝑗 𝑡 = 1 [ leaf T ( x 𝑗𝑡 ) = 𝑙 ] . ( 5 ) The effect of Equation 5 is to assign higher reward to leaves that contain more timesteps from trajectories with high g values ( i . e . those more commonly preferred in the preference dataset ) . While ostensibly naïve , we find that this time - weighted credit assignment is more robust than several more complex alternatives . It reduces the free parameters in subsequent induction stages , permits fast implementation , and provides an intuitive interpretation of predicted reward that is traceable back to a g value and timestep count for each 𝜉 𝑖 ∈ Ξ . Figure 2 ( V . B ) shows how 4 , 2 and 3 timesteps from 𝜉 1 , 𝜉 3 and 𝜉 4 are averaged over to yield the reward prediction for one leaf ( indicated by the orange shading ) . This definition provides the basis for using T as a reward function . Given an arbitrary feature vector x ∈ R 𝐹 , we simply look up the reward of the leaf in which it resides : 𝑅 T ( x ) = r leaf T ( x ) . ( 6 ) C . Tree Growth Recall that the objective of preference - based reward learning is to adjust the parameters of the reward model in order to minimise some loss function over D , such as those in Equations 2 and 3 . When the model is a tree , this is achieved by the discrete operations of growth ( adding partitioning rules to increase the number of leaves ) and pruning ( removing rules to decrease the number of leaves ) . Given a tree T , a new rule has the effect of splitting the 𝑙 th leaf with a hyperplane at a location 𝑐 ∈ C 𝑓 along the 𝑓 th feature dimension ( where C 𝑓 ⊂ R is a set of candidate split thresholds , e . g . all midpoints between unique values in Ξ ) . Let T + [ 𝑙 𝑓 𝑐 ] denote the newly - enlarged tree . Splitting recursively creates an increasingly fine partition of R 𝐹 . Figure 2 ( V . C ) shows an example of a reward tree with 23 leaves . A central issue is the criterion for selecting the next rule to add . In [ 4 ] , we use the proxy objective of minimising the local variance of g values in each leaf , which exactly corresponds to the classic CART algorithm [ 66 ] . While very fast to compute , this criterion is only loosely aligned with the reconstruction of the preferences in D . In the present work , we propose and investigate the more direct criterion of greedily minimising the ℓ 0 - 1 of the enlarged tree T + [ 𝑙 𝑓 𝑐 ] : argmin 1 ≤ 𝑙 ≤ 𝐿 T , 1 ≤ 𝑓 ≤ 𝐹 , 𝑐 ∈C 𝑓 (cid:2) ℓ 0 - 1 ( D , 𝑅 T + [ 𝑙 𝑓𝑐 ] ) (cid:3) , ( 7 ) i . e . selecting splits to minimise the number of incorrectly - predicted preferences . In Section VII , we show that switching to this criterion consistently improves reward learning and agent policy performance on the aircraft handling tasks . Recursive splitting stops when ℓ 0 - 1 cannot be reduced by any single split , or a tree size limit 𝐿 T = 𝐿 max is reached . D . Tree Pruning Growth is followed by a pruning stage which reduces the size of the tree by rule removal . This is beneficial for both performance ( Tien et al . [ 67 ] find that limiting model capacity lowers the risk of causal confusion in preference - based reward learning ) and human comprehension ( in the language of Jenner and Gleave [ 51 ] , pruning is a form of “processing for interpretability " ) . Given a tree T , one pruning operation has the effect of merging two leaves into one by removing the rule at the common parent node . Let T denote the sequence of nested subtrees induced by pruning the tree recursively back to its root , at each step removing the rule that minimises the next subtree’s ℓ 0 - 1 . We select the T ∈ T that minimises ℓ 0 - 1 , additionally regularised by a term that encourages small trees : argmin T∈ T [ ℓ 0 - 1 ( D , 𝑅 T ) + 𝛼𝐿 T ] , ( 8 ) where 𝛼 ≥ 0 is a regularisation coefficient . In the example in Figure 2 ( V . D ) , pruning yields a final tree with 3 leaves , for which illustrative leaf - level reward predictions are shown . VI . Online Learning Process A . Iterated Policy and Reward Learning Sections IV and V do not discuss the origins of the trajectories Ξ , or how reward tree learning should be integrated with the process of policy learning by RL . Following most recent work since Christiano et al . [ 5 ] , we resolve both questions with an iterative bootstrapped approach , in which the current reward tree defines the RL agent’s reward function at each point in learning . During learning episode 𝑖 , the agent uses its latest policy to produce a new trajectory 𝜉 𝑖 . We immediately connect 𝜉 𝑖 to the preference graph by asking the human ( read : oracle ) to compare it to 𝐾 batch random trajectories from the existing set . We then update the reward tree on the full preference graph via the stages in 6 Section V . We find that our original method of starting growth from the current state of the tree causes lock - in to poor initial solutions , so instead re - grow from scratch ( i . e . starting from a single leaf ) on each update . The rule structure nonetheless tends to stabilise , as the enlarging preference graph becomes increasingly similar for later updates . For the ( 𝑖 + 1 ) th episode , the RL agent then attempts to optimise its policy with respect to the newly - updated reward tree . By iterating this process up to a total preference budget 𝐾 max and / or episode budget 𝑁 max , we hope to converge to both a reward tree that reflects the human’s preferences , and an agent policy that satisfies those preferences . B . Model - based RL Algorithm Online reward learning is generally agnostic to the structure of the policy learning agent ; this modularity is hailed as an advantage over other human - agent teaching paradigms [ 23 ] . Following most recent work , in [ 4 ] we use a model - free RL agent , specifically soft actor - critic ( SAC ) [ 68 ] . However , other work [ 8 , 69 ] uses model - based RL ( MBRL ) agents that leverage learnt dynamics models and planning . MBRL is attractive in the reward learning context because it disentangles the predictive and normative aspects of decision - making . Since ( assuming no changes to the environment ) dynamics remain stationary during online reward learning , the amount of re - learning required is reduced . MBRL can also be very data - efficient ; our preliminary experiments found that switching from SAC to a model - based algorithm called PETS [ 70 ] reduces environment interaction during reward learning by orders of magnitude . PETS selects actions by decision - time planning through a learnt dynamics model 𝐷 ′ : S × A → Δ ( S ) up to a horizon 𝐻 . In state 𝑠 , planning searches for a sequence of 𝐻 future actions that maximise return according to the current reward tree : argmax ( 𝑎 0 , . . . , 𝑎 𝐻 − 1 ) ∈A 𝐻 E 𝐷 ′ (cid:104) ∑︁ 𝐻 − 1 ℎ = 0 𝛾 ℎ 𝑅 T ( 𝜙 ( 𝑠 ℎ , 𝑎 ℎ , 𝑠 ℎ + 1 ) ) (cid:105) , where 𝑠 0 = 𝑠 , 𝑠 ℎ + 1 ∼ 𝐷 ′ ( 𝑠 ℎ , 𝑎 ℎ ) . ( 9 ) The first action 𝑎 = 𝑎 0 is executed , and then the agent re - plans on the next timestep . In practice , 𝐷 ′ is an ensemble of probabilistic NNs , the expectation over 𝐷 ′ is replaced by a Monte Carlo estimate , and the optimisation is approximated by the iterative cross - entropy method . We use PETS agents in all experiments in this paper . In our implementation , 𝐷 ′ is an ensemble of five NNs , each with four hidden layers of 200 units . Planning operates over a time horizon of 𝐻 = 10 , with no discounting ( 𝛾 = 1 ) , and consists of 10 iterations of the cross - entropy method . Each iteration samples 20 candidate action sequences from an independent Gaussian , of which the top five in terms of return are identified as elites , then updates the sampling Gaussian towards the elites with a learning rate of 0 . 5 . This parameter affects how rapidly the distribution narrows towards a deterministic action sequence . In turn , this determines whether the agent only exploits actions that appear optimal under the current reward model 𝑅 T , or explores more diverse behaviour that might be preferred by the human evaluator . Correctly balancing the explore / exploit trade - off is especially crucial in the reward learning context . We find that the particular dynamics of the aircraft handling environment permit us to pre - train 𝐷 ′ on random data , and accurately generalise to states encountered during online reward learning . This means we perform no further updates to 𝐷 ′ while reward learning is ongoing . As well as improving execution speed , this avoids complexity and convergence issues arising from having two interacting learning processes ( note that simultaneous learning is unavoidable with model - free RL ) . To pre - train , we collect 1 𝑒 5 transitions using a uniform random policy , then update each network on 1 𝑒 5 independently sampled mini - batches of 256 transitions , using the mean squared error loss over next - state predictions . C . Diagram of Online Learning Process Figure 3 summarises the learning approach taken in this paper , with the details of some steps omitted for clarity . Although we obtain preferences from synthetic oracles , the process using a real human evaluator would be identical . Planner Action sequence Dynamics model Reward function Reward sequence Next action Current state Optimally - pruned tree State sequence Environment Append New trajectory Human Preferences Existing trajectories Append Append Trajectory set Sample Preference set Trajectory returns Return estimator Tree initialiser One - leaf tree Fully - grown tree Split generator Growing tree Candidate splits Loss function Optimal split Splitter Loss function Pruner Candidatepruned trees P r e f . c o ll ec ti on Pruning G r o w t h B e h a v i ou r a l e p i s od e PETSagent Fig . 3 Online preference - based reward tree learning with model - based PETS agents . 7 VII . Experiments and Results In this section , we combine quantitative and qualitative evaluations to assess the performance of reward tree learning with oracle preferences on the aircraft handling tasks , specifically in comparison to the de facto standard approach of using NNs . We also illustrate how the intrinsic interpretability of reward trees allows us to analyse what they have learnt . In all experiments , we use the following set of hyperparameters for tree learning . These were identified through informal search , and we make no claim of optimality , but they do lead to acceptable performance on the three tasks of varying complexity . The fact that we did not need to invest significant time in tuning indicates a general insensitivity of the method to precise hyperparameter values , which is widely seen as practically advantageous . • Trajectory return estimation using the Adam optimiser with a learning rate of 0 . 1 . Optimisation is stopped when the loss ℓ NLL changes by < 1 𝑒 − 5 between successive gradient steps . • Per - feature candidate split thresholds C 𝑓 defined as all midpoints between adjacent unique values in the trajectory set Ξ . These are recomputed on each update . • Tree size limit 𝐿 max = 100 . • Tree size regularisation coefficient 𝛼 = 5 𝑒 − 3 . In constructing the NN baseline , we aimed to retain as much of the algorithm structure from Figure 3 as possible , so that only the model architecture varies . The result is that we perform policy learning with PETS , trajectory pair sampling and oracle preference collection identically , and update the reward model after every episode . However , in place of the multi - stage tree growth and pruning process , we perform model updates by mini - batch gradient descent with respect to ℓ NLL from Equation 2 . In all experiments , we follow Lee et al . [ 6 ] in implementing the NN reward model as a three - layer network with 256 hidden units each and leaky ReLU activations , and performing the gradient - based updates using the Adam optimiser [ 65 ] with a learning rate of 3 𝑒 − 4 . On each update , we sample 𝑀 = 100 mini - batches of size 𝐵 = 32 and take one gradient step per mini - batch . A . Quantitative Performance In our main experiments , we use ideal , error - free oracles , which provide preferences according to Equation 1 with 𝛽 = 0 . We evaluate online reward learning with PETS using trees with the ℓ 0 - 1 split criterion , baselined against our original variance criterion , as well as the de facto standard of NN reward learning . We use 𝐾 max = 1000 preferences over 𝑁 max = 200 online trajectories ( i . e . those generated by the PETS agents during learning ) and run 10 repeats . As a headline statistic , Table 1 reports the oracle regret ratio ( ORR ) : the drop in average oracle return of PETS agents deployed using each trained reward model compared with directly using the oracle reward function , as a fraction of the drop to a random policy ( lower is better ) . This gives a normalised measure of how well reward learning performs compared with the ideal case of direct access to the true reward . We report the median and minimum ORR values across the 10 repeats for each task - model pairing . We observe that : 1 ) NN reward learning is strong on all tasks , 2 ) switching to a tree induces a small but variable performance hit , 3 ) ℓ 0 - 1 splitting outperforms the variance - based method , and 4 ) both NN and tree models sometimes exceed the direct use of the oracle for policy learning ( negative ORR ) . This counter - intuitive phenomenon has been observed before [ 29 ] and may be due to improved shaping in the learnt reward . Table 1 Median ( top ) Follow Chase Land and minimum ( bottom ) NN Tree ( 0 - 1 ) Tree ( var ) NN Tree ( 0 - 1 ) Tree ( var ) NN Tree ( 0 - 1 ) Tree ( var ) oracle regret ratios for . 000 . 120 . 284 − . 030 . 040 . 126 . 014 . 050 . 062 all tasks and models . − . 010 . 057 . 158 − . 051 − . 011 . 065 − . 030 . 011 . 010 Figure 4 expands these results with more metrics , revealing learning trends not captured by headline ORR values . Metrics are plotted as time series over the 200 learning episodes ( sliding - window medians and interquartile ranges ( IQRs ) across repeats ) . In the left column ( a ) , the ORR of online trajectories shows how agent performance converges . For Follow , there is a gap between the models , with ℓ 0 - 1 splitting visibly aiding performance but still lagging behind the NNs . The learning curves for Chase and Land are more homogeneous , and the NNs reach only slightly lower asymptotes , with overlapping IQRs . The majority of models converge to their asymptotic performance well within the 200 episode learning period ; this learning speed is made possible by the use of model - based PETS agents . For the reward tree models , ( b ) shows how the number of leaves changes over time . There is notable consistency in these trends between the repeated runs . The variance - based trees tend to grow rapidly initially before stabilising or shrinking , while the ℓ 0 - 1 trees enlarge more conservatively , suggesting this method is less liable to overfit to small preference datasets . Trees of a readily - interpretable size ( ≈ 20 leaves ) are produced for all tasks ; it is possible that performance could be improved by independently tuning the size regulariser 𝛼 per task . 8 0 0 . 5 1 F o ll o w L a n d C h a s e Online ORR a Rank correlation e NN Tree ( 0 - 1 ) Tree ( var ) 0 0 . 5 1 0 0 . 2 0 . 4 0 - 1 loss c 0 0 . 05 0 . 1 0 . 150 0 . 05 0 . 1 0 . 150 0 . 05 0 . 1 0 . 15 0 . 25 0 . 5 0 . 75 1 0 0 . 25 0 . 5 0 . 75 1 Tree size b 10 20 10 20 10 20 0 0 . 5 1 0 0 . 5 1 0 . 2 0 . 6 1 0 0 . 5 1 Reward correlation d Fig . 4 Time series of metrics for online NN - and tree - based reward learning on all three tasks . ( c ) shows how the discrete preference loss ℓ 0 - 1 changes over time , which tends to increase as the growing preference graph presents a harder reconstruction problem , though the shapes of all curves suggest convergence ( note that random prediction gives ℓ 0 - 1 = 0 . 5 ) . For Follow and Land , the trees that directly split on ℓ 0 - 1 actually achieve lower loss than the NNs ; they more accurately predict the direction of preferences in the graph . This is an encouraging result , indicating that our models perform well on the metric for which they are directly optimised , but the fact that this does not translate into lower ORR indicates that the problems of learning a good policy and replicating the preference dataset are not perfectly correlated . This subtle point has previously been made by Lindner et al . [ 9 ] . Gleave et al . [ 71 ] recently highlighted the importance of comparing and evaluating learnt reward functions in a policy - invariant manner , by using a common evaluation dataset rather than on - policy data generated by agents optimising for each reward . We report such a comparison in the final two columns , where the evaluation datasets are generated by PETS agents using the oracle reward functions , with added action randomisation to increase diversity . Using these datasets , we correlate each reward model’s predictions with its respective oracle at each point in learning , in terms of both transition - level rewards ( d ) and the ordinal ranking of trajectories by return ( e ) , the latter via the Kendall [ 72 ] 𝜏 coefficient . The curves subtly differ , indicating that it is possible to reconstruct trajectory rankings ( and by implication , any pairwise preferences ) to a given accuracy with varying fidelity at the individual reward level . However , the common overall trend is that ℓ 0 - 1 - based trees outperform variance - based ones , with NNs sometimes improving again by a smaller margin , and sometimes bringing no added benefit . Moving top - to - bottom down the tasks , the gap between models reduces from both sides ; NN performance worsens while variance - based trees improve . A potentially important factor in these experiments is that the oracle reward for Follow is a linear function , while the others contain progressively more terms and discontinuities ( see Section III ) . A trend suggested by these results is thus that the performance gap between NNs and reward trees ( on both ORR and correlation metrics ) reduces as the true reward becomes more complex and nonlinear . Further experiments would be needed to test this hypothesis . B . Visual Trajectory Inspection While useful for benchmarking , quantitative metrics provide little insight into the structure of the learnt solutions . They would also mostly be undefined when learning from real humans since the ground truth reward is unknown . We therefore complement them with a visual analysis of induced agent behaviour . Figure 5 plots 500 trajectories of PETS agents using the best repeat by ORR for each task - model combination , across a range of features as well as time ( see Table 2 for a reminder of feature definitions ) . Each trajectory is coloured on a red - blue scale according to its ORR . Dashed black curves indicate the single trajectory with the highest predicted return according to each model . We also show trajectories for PETS agents with direct oracle access , which serve as the benchmark behaviour that we aim to match via reward learning , and for random policies , which perform very poorly on all three tasks . The high - level trend is that all models are far closer to the oracle than random , with few examples of obviously unstable handling behaviour or task failure ( highlighted in red , due to colouring by ORR ) . While the NNs induce trajectories that are almost indistinguishable from the oracle , the ℓ 0 - 1 - based reward trees lag not far behind . 9 Oracle Tree ( var ) F o ll o w 0 353 - 31 31 dist c l o s i n g s p ee d 1 20 0 u p e rr o r Timestep C h a s e L a n d 50 350 a l t los error - 1 . 40 1 . 55 roll p i t c h 0 200 0 30 a l t dist hor ORR 0 1 0 - 1 20 20 150 Timestep d i s t 0 , 0 0 , 0 Random e g j h f Tree ( 0 - 1 ) a b d c Highest return accordingto oracle NN Highest return accordingto model i Fig . 5 Agent trajectories using the best models by ORR , with oracle and random for comparison . The variance - based trees exhibit more anomalies . Successes of the ℓ 0 - 1 trees include the execution of Follow with a single banked turn before straightening up , as shown by the up error time series ( a ) , where up error = 0 is level flight . Interestingly , both this tree and the NN reward model appear to favour a somewhat earlier turn than the oracle ( i . e . peak shifted to the left on this plot ) . Indeed , the trajectories for the ℓ 0 - 1 tree are almost imperceptibly different from those of the NN , despite their quantitative performance ( e . g . ORR ) differing . This underlines the importance of joint quantitative - qualitative evaluation . For Chase ( b ) , the ℓ 0 - 1 tree has clearly learnt the most safety - critical aspect of the task , which is to keep the agent above the altitude threshold alt < 50 , below which the oracle reward is strongly negative . The threshold is violated in only eight of 500 trajectories ( 1 . 6 % ) . Further evidence that the altitude threshold has been learnt correctly is discussed in Section VII . D . For Land , the ℓ 0 - 1 tree replicates the oracle in producing a gradual reduction in alt ( c ) while usually keeping pitch close to 0 ( d ) , although the spread of roll values is somewhat wider . In contrast , the agent using the variance - based tree for Follow sometimes fails to reach the target position ( e ; red trajectories ) , and also does not reliably straighten up to reduce up error ( f ) . For Chase , the altitude threshold does not appear to have been learnt precisely , and lower - altitude trajectories often fail to close the distance to RJ ( g and h ; red trajectories ) . For Land , the variance - based tree gives a later and less smooth descent ( i ) , and less consistent pitch control ( j ) , than the NN or ℓ 0 - 1 - based tree , although all models produce a somewhat higher altitude profile than the oracle . C . Sensitivity Analysis It is important to consider how learning performance degrades with reduced or corrupted data . In Figure 6 , we evaluate the effect of varying the number of preferences 𝐾 max ( with fixed 𝑁 max = 200 ) and trajectories 𝑁 max ( with fixed 𝐾 max = 1000 ) on reward learning with NNs and ℓ 0 - 1 - splitting trees . Following Lee et al . [ 10 ] , we also create more human - like preference data via two modes of oracle irrationality : preference noise ( by using a nonzero Bradley - Terry temperature 𝛽 to give a desired error rate on the coverage datasets ) and a myopic recency bias ( by exponentially discounting earlier timesteps when evaluating trajectory returns ) . We run five repeats for all cases , and report the medians and interquartile ranges of ORR ( lower is better ) and rank correlation ( closer to 1 is better ) . Both NN and tree models exhibit good robustness with respect to all four parameters . Although NNs remain superior in most cases , the gap varies , and is often reduced compared to the base cases ( bold labels ) . The budget sensitivity is low , with little improvement for 𝐾 max > 1000 and 𝑁 max > 200 , and no major drop even with 25 % of the data as the base case . For all tasks , the oracle error probability can increase to around 20 % before significant drops in performance are observed . These are promising indicators of the transferability of reward tree learning to limited and imperfect human data . Another general observation is that the trends for trees are somewhat smoother than for NNs , with fewer sharp jumps and fewer instances of very high spread across the five repeats . In the right column ( a ) , we summarise these results by taking the difference between the NN and tree metrics , and averaging across the three tasks . In all cases aside from rank correlation with 𝛽 > 0 , the NN - tree gap tends to become more favourable to the trees as the varied parameter becomes more challenging ( top - to - bottom ) . This sensitivity analysis thus indicates that reward trees are at least as robust to difficult learning scenarios as NNs , and may even be slightly more so . This is another promising result for the viability of reward tree learning from real human preferences . 10 O r a c l e m y o p i a ( ) Chase Follow P r e f e r e n c e b u d g e t ( ) T r a j e c t o r y b u d g e t ( ) NN Tree ( 0 - 1 ) Land 1000 20004000 500250 200 400800 10050 0 . 98 0 . 2 0 . 3 0 . 4 0 0 . 950 . 90 . 8 0 . 1 0 . 45 O r a c l e e rr o r p r o b a b i l i t y ( v i a ) 1 ORR 0 0 . 5 1 Rank correlation 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 10 0 . 5 1 Average NN - Tree Gap NNbetter - 0 . 25 0 0 . 25 - 0 . 25 0 0 . 25 ORR ORR ORR Treebetter Treebetter NNbetter a Rank correlation Rank correlation Rank correlation Fig . 6 Comparative sensitivity analysis of reward learning with NNs and trees . D . Tree Structure Analysis We have shown that reward learning with ℓ 0 - 1 - based trees can be competitive with NNs , but not quite as performant overall . We now turn to a concrete advantage which may tip practical trade - offs in its favour : the ability to interpret the learnt model , and analyse how its predictions relate to the underlying preference graph . In Figure 7 we opt for depth over breadth , and focus on the single best tree by ORR on the Chase task . The figure is divided into sections ( a – d ) : ( a ) This reward tree has 17 leaves . The oracle reward , printed below , uses four features , all of which are used in the tree in ways that are broadly aligned ( e . g . lower los error leads to leaves with higher reward ) . The model has learnt the crucial threshold alt < 50 , correctly assigning a low reward when it is crossed . This explains why we observe rare violations of the altitude threshold in Figure 5 . However , it has not learnt the ideal distance to RJ , dist = 20 , with 43 . 3 being the lowest value used in a rule . This could be because the underlying preference graph lacks sufficient preferences to make this distinction ; adopting an active querying scheme may help to discover such subtleties efficiently . Other features besides those used by the oracle are present in the tree , indicating some causal confusion [ 67 ] . This may not necessarily harm agent performance , as it could provide beneficial shaping ( e . g . penalising positive closing speed , which indicates increasing distance to RJ ) . That may indeed be the case for this model since ORR is actually negative . ( b ) We plot the tree’s predicted reward against the oracle reward for all timesteps in the online trajectories ( correlation = 0 . 903 ) . The predictions for each leaf lie along a horizontal line . Most leaves , including 1 and 2 , are well - aligned on this data because their oracle reward distributions are tightly concentrated around low / high averages respectively ( note that the absolute scale is irrelevant here ) . Leaf 16 has a wider oracle reward distribution , with several negative outliers . An optimal tree would likely split this leaf further , perhaps using the alt < 50 threshold . The one anomaly is leaf 13 , which contains just a single timestep from 𝜉 77 . This trajectory is the eighth best in the dataset by oracle return , but this leaf assigns that credit to a state that seemingly does not merit it , as the distance to RJ is so high ( dist > 73 ) . This may be an example of suboptimal reward learning , but the fact that its origin can be pinpointed so precisely is a testament to the value of interpretability . ( c ) We leverage the tree structure to produce a human - readable explanation of reward predictions for a single trajectory , which may be of value to an end user ( e . g . a trainee pilot seeking to understand the strengths and weaknesses of their own performance ) . We consider 𝜉 191 , a rare case that violates the altitude threshold . The time series of reward shows that the 20 timesteps are spent in leaves 16 , 15 , 11 and 7 . Rescaled oracle rewards are overlaid in teal , and show that the model’s predictions are well - aligned . To the right , we translate this visualisation into a textual form , similar to a nested program . Read top - to - bottom , the text indicates which rules of the tree are active at each timestep , and the effect this has on the predicted reward . This trajectory starts fairly positively , with reward gradually increasing over the first 16 timesteps as dist is reduced to between 43 . 3 and 73 , but then falls dramatically when the alt < 50 threshold is crossed . We are unaware of any method that could extract such a compact explanation of sequential predictions from an NN . ( d ) We isolate a subtree , starting at the root node , that splits only on dist and alt . We give a spatial representation 11 r = 2 . 48 - - - - - dist ≥ 43 . 3 ? r = 3 . 12 - - - - - alt ≥ 50 . 2 ? No r = 2 . 15 - - - - - dist ≥ 95 . 4 ? Yes ( 1 ) r = 0 . 937 No r = 3 . 17 - - - - - los error ≥ 1 . 10 ? Yes r = 3 . 22 - - - - - roll error ≥ 0 . 847 ? No r = 3 . 04 - - - - - abs roll ≥ 1 . 56 ? Yes ( 2 ) r = 3 . 35 No ( 3 ) r = 3 . 18 Yes r = 3 . 14 - - - - - closing speed ≥ 7 . 63 ? No ( 6 ) r = 2 . 92 Yes ( 4 ) r = 3 . 16 No ( 5 ) r = 2 . 47 Yes r = 2 . 31 - - - - - alt ≥ 50 . 0 ? No r = 1 . 72 - - - - - closing speed ≥ 8 . 34 ? Yes ( 7 ) r = 0 . 763 No r = 2 . 34 - - - - - dist ≥ 73 . 0 ? Yes r = 2 . 47 - - - - - abs roll ≥ 0 . 744 ? No r = 2 . 22 - - - - - alt error ≥ 2 . 14 ? Yes r = 2 . 72 - - - - - dist ≥ 68 . 7 ? No r = 2 . 37 - - - - - delta hdg error ≥ - 3 . 68 ? Yes ( 8 ) r = 2 . 78 No ( 9 ) r = 2 . 45 Yes ( 10 ) r = 3 . 20 No ( 11 ) r = 2 . 37 Yes ( 12 ) r = 2 . 76 No r = 2 . 21 - - - - - los error ≥ 0 . 0126 ? Yes ( 13 ) r = 3 . 92 No r = 2 . 21 - - - - - thrust ≥ 3 . 54 ? Yes ( 14 ) r = 1 . 70 No ( 15 ) r = 2 . 21 Yes ( 16 ) r = 1 . 76 No ( 17 ) r = 0 . 78 Yes Intermediate reward for non - leaf node by timestep - weighted average Oracle reward : a d r = 2 . 48 - - - - - dist ≥ 43 . 3 ? r = 3 . 12 - - - - - No r = 2 . 15 - - - - - dist ≥ 95 . 4 ? Yes ( 1 ) r = 0 . 937 No ( 2 ) r = 3 . 17 Yes r = 2 . 31 - - - - - alt ≥ 50 . 0 ? No ( 6 ) r = 1 . 72 Yes ( 3 ) r = 0 . 763 No r = 2 . 34 - - - - - dist ≥ 73 . 0 ? Yes ( 4 ) r = 2 . 47 No ( 5 ) r = 2 . 22 Yes 0 43 . 3 95 . 4 150 dist 0 350 a l t 50 73 alt ≥ 50 . 2 ? Total : 30 timesteps 1 2 4 5 6 3 1 and Predicted reward 0 4 . 45 ( 13 ) r = 3 . 92 ( 2 ) r = 3 . 35 ( 16 ) r = 1 . 76 ( 1 ) r = 0 . 937 , t = 2 ( 16 ) ( 15 ) ( 11 ) ( 7 ) 1 4 P r e d i c t e d r e w a r d Timestep 1 20 c Oracle reward ( rescaled ) - 300 - 200 - 100 0 1 4 Oracle reward P r e d i c t e d r e w a r d T i m e s t e p Trajectory return estimate 0 89 b Fig . 7 Analysis of a reward tree learnt for the Chase task . of the subtree , and how it is populated by the 200 online trajectories , using a 2D partition plot analogous to those in Figure 2 . Zooming into leaf 1 , which covers cases where the altitude threshold is violated , we see that it contains a total of 30 timesteps across four trajectories . By Equation 5 , the low reward for this leaf results from a weighted average of the return estimates for these four trajectories , which in turn ( by Equation 4 ) are derived from the preference graph . We can use this inverse reasoning to explain why this leaf has much lower reward than its sibling ( leaf 2 of the subtree ) . A proximal explanation comes by filtering the graph for preferences that specifically compare trajectories that visit those two leaves . 49 such preferences exist , and in all cases , the oracle prefers the trajectory that does not visit leaf 1 . Some of these preferences may be more practically salient than others . For example , we might highlight trajectories that feature more than once ( e . g . 𝜉 28 is preferred to both 𝜉 18 and 𝜉 48 ) , or cases where trajectories with low overall return estimates are nonetheless preferred to those in leaf 1 ( e . g . 𝜉 43 ≻ 𝜉 21 and 𝜉 56 ≻ 𝜉 47 ) . This ability to trace a reward tree’s predictions back to the individual preferences that influence them could be valuable for verification and active learning . VIII . Conclusions and Future Work Fast jet handling is an excellent case study of both the value and difficulty of distilling tacit human expertise into software systems . In this work , we proposed a preference - based reward learning framework , which yields both a quantitative model of human preferences with a readable tree structure , and an artificial demonstrator agent capable of executing high - quality flight trajectories with respect to that model . The intrinsic interpretability of the reward tree model enables improved insight into the structure of expert preferences , and verification of agent behaviour , compared with standard NN - based approaches . Through oracle experiments on several aircraft handling tasks , we showed that reward trees with around 20 leaves can achieve quantitative and qualitative performance close to that of NNs , with a more direct split criterion bringing consistent benefits . We found that the NN - tree gap reduces as the true reward becomes more nonlinear , and remains stable or reduces further in the presence of limited or corrupted data . Although our experiments used synthetic oracle data to enable scalable quantitative evaluation , future work should include studies using the preferences of real human experts in the aircraft handling domain . It should also be noted that any realistic preference elicitation scenario would likely involve multiple experts with differing knowledge and expertise . A natural extension of our approach , which we see as valuable future work , is to learn individual reward functions for each expert , then leverage the intrinsic interpretability to identify biases , inconsistencies and trade - offs . This suggests a further application of reward tree learning : providing a basis for evaluating and training the experts themselves . 12 Appendix Table 2 List of features used by oracles and reward learning models . Apart from those containing “ delta " or “ rate " , all features are computed over the successor state for each transition , 𝑠 𝑡 + 1 . dist Euclidean distance between EJ and RJ closing speed Closing speed between EJ and RJ ( negative = moving closer ) alt Altitude of EJ alt error Difference in altitude between EJ and RJ ( negative = EJ is lower ) delta alt error Change in alt error between 𝑠 𝑡 and 𝑠 𝑡 + 1 dist hor Euclidean distance between EJ and RJ in horizontal plane delta dist hor Change in dist hor between 𝑠 𝑡 and 𝑠 𝑡 + 1 ( negative = moving closer ) pitch error Absolute difference in pitch angle between EJ and RJ delta pitch error Change in pitch error between 𝑠 𝑡 and 𝑠 𝑡 + 1 abs roll Absolute roll angle of EJ roll error Absolute difference in roll angle between EJ and RJ delta roll error Change in roll error between 𝑠 𝑡 and 𝑠 𝑡 + 1 hdg error Absolute difference in heading angle between EJ and RJ delta hdg error Change in hdg error between 𝑠 𝑡 and 𝑠 𝑡 + 1 fwd error Angle between 3D vectors indicating forward axes of EJ and RJ delta fwd error Change in fwd error between 𝑠 𝑡 and 𝑠 𝑡 + 1 up error Angle between 3D vectors indicating upward axes of EJ and RJ delta up error Change in up error between 𝑠 𝑡 and 𝑠 𝑡 + 1 right error Angle between 3D vectors indicating rightward axes of EJ and RJ delta right error Change in right error between 𝑠 𝑡 and 𝑠 𝑡 + 1 los error Angle between forward axis of EJ and vector from EJ to RJ ( measures whether RJ is in EJ’s line of sight ) delta los error Change in los error between 𝑠 𝑡 and 𝑠 𝑡 + 1 abs lr offset Magnitude of projection of vector from EJ to RJ onto RJ’s rightward axis ( measures left - right offset between the two aircraft in RJ’s reference frame ) speed Airspeed of EJ g force Instantaneous g - force experienced by EJ pitch rate Absolute change of EJ pitch between 𝑠 𝑡 and 𝑠 𝑡 + 1 roll rate Absolute change of EJ roll between 𝑠 𝑡 and 𝑠 𝑡 + 1 yaw rate Absolute change of EJ yaw between 𝑠 𝑡 and 𝑠 𝑡 + 1 thrust Instantaneous thrust output by EJ engines delta thrust Absolute change in thrust between 𝑠 𝑡 and 𝑠 𝑡 + 1 Acknowledgments This work was supported by a Thales / EPSRC Industrial Case Award in autonomous systems . The aircraft simulator used in experiments was based on an initial implementation by Ian Henderson at Thales UK , and experimental tasks were developed in consultation with both Ian Henderson and Rachel Craddock , also at Thales UK . 13 References [ 1 ] Sternberg , R . J . , and Horvath , J . A . , Tacit knowledge in professional practice : Researcher and practitioner perspectives , Psychology Press , 1999 . [ 2 ] Rudin , C . , “Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead , ” Nature Machine Intelligence , Vol . 1 , No . 5 , 2019 , pp . 206 – 215 . [ 3 ] Brunton , S . L . , Nathan Kutz , J . , Manohar , K . , Aravkin , A . Y . , Morgansen , K . , Klemisch , J . , Goebel , N . , Buttrick , J . , Poskin , J . , Blom - Schieber , A . W . , et al . , “Data - driven aerospace engineering : reframing the industry with machine learning , ” AIAA Journal , Vol . 59 , No . 8 , 2021 , pp . 2820 – 2847 . [ 4 ] Bewley , T . , and Lecue , F . , “Interpretable Preference - based Reinforcement Learning with Tree - Structured Reward Functions , ” Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems , 2022 , pp . 118 – 126 . [ 5 ] Christiano , P . F . , Leike , J . , Brown , T . , Martic , M . , Legg , S . , and Amodei , D . , “Deep reinforcement learning from human preferences , ” Advances in Neural Information Processing Systems , Vol . 30 , 2017 . [ 6 ] Lee , K . , Smith , L . M . , and Abbeel , P . , “PEBBLE : Feedback - Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre - training , ” International Conference on Machine Learning , PMLR , 2021 , pp . 6152 – 6163 . [ 7 ] Griffith , S . , Subramanian , K . , Scholz , J . , Isbell , C . L . , and Thomaz , A . L . , “Policy shaping : Integrating human feedback with reinforcement learning , ” Advances in neural information processing systems , Vol . 26 , 2013 . [ 8 ] Reddy , S . , Dragan , A . , Levine , S . , Legg , S . , and Leike , J . , “Learning human objectives by evaluating hypothetical behavior , ” International Conference on Machine Learning , PMLR , 2020 , pp . 8020 – 8029 . [ 9 ] Lindner , D . , Turchetta , M . , Tschiatschek , S . , Ciosek , K . , and Krause , A . , “Information Directed Reward Learning for Reinforcement Learning , ” Advances in Neural Information Processing Systems , Vol . 34 , 2021 , pp . 3850 – 3862 . [ 10 ] Lee , K . , Smith , L . , Dragan , A . , and Abbeel , P . , “B - Pref : Benchmarking Preference - Based Reinforcement Learning , ” Advances in Neural Information Processing Systems , Vol . 35 , 2021 . [ 11 ] Sutton , R . S . , and Barto , A . G . , Reinforcement learning : An introduction , MIT press , 2018 . [ 12 ] Azar , A . T . , Koubaa , A . , Ali Mohamed , N . , Ibrahim , H . A . , Ibrahim , Z . F . , Kazim , M . , Ammar , A . , Benjdira , B . , Khamis , A . M . , Hameed , I . A . , et al . , “Drone deep reinforcement learning : A review , ” Electronics , Vol . 10 , No . 9 , 2021 , p . 999 . [ 13 ] Liu , H . , Kiumarsi , B . , Kartal , Y . , Koru , A . T . , Modares , H . , and Lewis , F . L . , “Reinforcement learning applications in unmanned vehicle control : A comprehensive overview , ” Unmanned Systems , 2022 , pp . 1 – 10 . [ 14 ] Razzaghi , P . , Tabrizian , A . , Guo , W . , Chen , S . , Taye , A . , Thompson , E . , Bregeon , A . , Baheri , A . , and Wei , P . , “A Survey on Reinforcement Learning in Aviation Applications , ” arXiv preprint arXiv : 2211 . 02147 , 2022 . [ 15 ] Tang , C . , and Lai , Y . - C . , “Deep reinforcement learning automatic landing control of fixed - wing aircraft using deep deterministic policy gradient , ” 2020 International Conference on Unmanned Aircraft Systems ( ICUAS ) , IEEE , 2020 , pp . 1 – 9 . [ 16 ] Clarke , S . G . , and Hwang , I . , “Deep reinforcement learning control for aerobatic maneuvering of agile fixed - wing aircraft , ” AIAA Scitech 2020 Forum , 2020 , p . 0136 . [ 17 ] Morales , E . F . , and Sammut , C . , “Learning to fly by combining reinforcement learning with behavioural cloning , ” Proceedings of the twenty - first international conference on Machine learning , 2004 , p . 76 . [ 18 ] Cao , S . , Wang , X . , Zhang , R . , Yu , H . , and Shen , L . , “From Demonstration to Flight : Realization of Autonomous Aerobatic Maneuvers for Fast , Miniature Fixed - Wing UAVs , ” IEEE Robotics and Automation Letters , Vol . 7 , No . 2 , 2022 , pp . 5771 – 5778 . [ 19 ] Yildiz , Y . , Agogino , A . , and Brat , G . , “Predicting pilot behavior in medium - scale scenarios using game theory and reinforcement learning , ” Journal of Guidance , Control , and Dynamics , Vol . 37 , No . 4 , 2014 , pp . 1335 – 1343 . [ 20 ] Vemuru , K . V . , Harbour , S . D . , and Clark , J . D . , “Reinforcement Learning in Aviation , Either Unmanned or Manned , with an Injection of AI , ” 20th International Symposium on Aviation Psychology , 2019 , p . 492 . [ 21 ] van Oĳen , J . , Poppinga , G . , Brouwer , O . , Aliko , A . , and Roessingh , J . J . , “Towards modeling the learning process of aviators using deep reinforcement learning , ” 2017 IEEE International Conference on Systems , Man , and Cybernetics ( SMC ) , IEEE , 2017 , pp . 3439 – 3444 . 14 [ 22 ] Russell , S . , Human compatible : Artificial intelligence and the problem of control , Penguin , 2019 . [ 23 ] Leike , J . , Krueger , D . , Everitt , T . , Martic , M . , Maini , V . , and Legg , S . , “Scalable agent alignment via reward modeling : a research direction , ” arXiv preprint arXiv : 1811 . 07871 , 2018 . [ 24 ] Ng , A . Y . , Russell , S . , et al . , “Algorithms for inverse reinforcement learning . ” Icml , Vol . 1 , 2000 , p . 2 . [ 25 ] Knox , W . B . , and Stone , P . , “Tamer : Training an agent manually via evaluative reinforcement , ” 2008 7th IEEE international conference on development and learning , IEEE , 2008 , pp . 292 – 297 . [ 26 ] Bajcsy , A . , Losey , D . P . , O’Malley , M . K . , and Dragan , A . D . , “Learning robot objectives from physical human interaction , ” Conference on Robot Learning , PMLR , 2017 , pp . 217 – 226 . [ 27 ] Wirth , C . , Fürnkranz , J . , and Neumann , G . , “Model - free preference - based reinforcement learning , ” Thirtieth AAAI Conference on Artificial Intelligence , 2016 . [ 28 ] Sadigh , D . , Dragan , A . D . , Sastry , S . , and Seshia , S . A . , “Active preference - based learning of reward functions , ” Proceedings of Robotics : Science and Systems ( RSS ) , 2017 . [ 29 ] Cao , Z . , Wong , K . , and Lin , C . - T . , “Weak human preference supervision for deep reinforcement learning , ” IEEE Transactions on Neural Networks and Learning Systems , Vol . 32 , No . 12 , 2021 , pp . 5369 – 5378 . [ 30 ] Kendall , M . , “Rank Correlation Methods ; Griffin , C . , Ed , ” , 1975 . [ 31 ] Wilde , N . , Blidaru , A . , Smith , S . L . , and Kulić , D . , “Improving user specifications for robot behavior through active preference learning : Framework and evaluation , ” The International Journal of Robotics Research , Vol . 39 , No . 6 , 2020 , pp . 651 – 667 . [ 32 ] Guo , Y . , Tian , P . , Kalpathy - Cramer , J . , Ostmo , S . , Campbell , J . P . , Chiang , M . F . , Erdogmus , D . , Dy , J . G . , and Ioannidis , S . , “Experimental Design under the Bradley - Terry Model . ” ĲCAI , 2018 , pp . 2198 – 2204 . [ 33 ] Ibarz , B . , Leike , J . , Pohlen , T . , Irving , G . , Legg , S . , and Amodei , D . , “Reward learning from human preferences and demonstrations in Atari , ” Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 34 ] Bıyık , E . , Losey , D . P . , Palan , M . , Landolfi , N . C . , Shevchuk , G . , and Sadigh , D . , “Learning reward functions from diverse sources of human feedback : Optimally integrating demonstrations and preferences , ” The International Journal of Robotics Research , Vol . 41 , No . 1 , 2022 , pp . 45 – 67 . [ 35 ] Puiutta , E . , and Veith , E . , “Explainable reinforcement learning : A survey , ” International cross - domain conference for machine learning and knowledge extraction , Springer , 2020 , pp . 77 – 95 . [ 36 ] Heuillet , A . , Couthouis , F . , and Díaz - Rodríguez , N . , “Explainability in deep reinforcement learning , ” Knowledge - Based Systems , Vol . 214 , 2021 , p . 106685 . [ 37 ] Zhu , G . , Huang , Z . , and Zhang , C . , “Object - oriented dynamics predictor , ” Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 38 ] Verma , A . , Murali , V . , Singh , R . , Kohli , P . , and Chaudhuri , S . , “Programmatically interpretable reinforcement learning , ” International Conference on Machine Learning , PMLR , 2018 , pp . 5045 – 5054 . [ 39 ] Zahavy , T . , Ben - Zrihem , N . , and Mannor , S . , “Graying the black box : Understanding dqns , ” International conference on machine learning , PMLR , 2016 , pp . 1899 – 1908 . [ 40 ] Huber , T . , Schiller , D . , and André , E . , “Enhancing explainability of deep reinforcement learning through selective layer - wise relevance propagation , ” Joint German / Austrian Conference on Artificial Intelligence ( Künstliche Intelligenz ) , Springer , 2019 , pp . 188 – 202 . [ 41 ] van der Waa , J . , van Diggelen , J . , Bosch , K . v . d . , and Neerincx , M . , “Contrastive explanations for reinforcement learning in terms of expected consequences , ” ĲCAI / ECAI Workshop on Explainable Artificial Intelligence , 2018 . [ 42 ] Amir , D . , and Amir , O . , “Highlights : Summarizing agent behavior to people , ” Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems , 2018 . [ 43 ] Huang , S . H . , Bhatia , K . , Abbeel , P . , and Dragan , A . D . , “Establishing appropriate trust via critical states , ” 2018 IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) , IEEE , 2018 , pp . 3929 – 3936 . 15 [ 44 ] Dao , G . , Mishra , I . , and Lee , M . , “Deep reinforcement learning monitor for snapshot recording , ” 2018 17th IEEE International Conference on Machine Learning and Applications ( ICMLA ) , IEEE , 2018 , pp . 591 – 598 . [ 45 ] Bewley , T . , Lawry , J . , and Richards , A . , “Summarising and Comparing Agent Dynamics with Contrastive Spatiotemporal Abstraction , ” ĲCAI / ECAI Workshop on Explainable Artificial Intelligence , 2022 . [ 46 ] Glanois , C . , Weng , P . , Zimmer , M . , Li , D . , Yang , T . , Hao , J . , and Liu , W . , “A Survey on Interpretable Reinforcement Learning , ” arXiv preprint arXiv : 2112 . 13112 , 2021 . [ 47 ] Juozapaitis , Z . , Koul , A . , Fern , A . , Erwig , M . , and Doshi - Velez , F . , “Explainable reinforcement learning via reward decomposition , ” ĲCAI / ECAI Workshop on Explainable Artificial Intelligence , 2019 . [ 48 ] Devidze , R . , Radanovic , G . , Kamalaruban , P . , and Singla , A . , “Explicable reward design for reinforcement learning agents , ” Advances in Neural Information Processing Systems , Vol . 34 , 2021 , pp . 20118 – 20131 . [ 49 ] Russell , J . , and Santos , E . , “Explaining reward functions in Markov decision processes , ” The Thirty - Second International Flairs Conference , 2019 . [ 50 ] Michaud , E . J . , Gleave , A . , and Russell , S . , “Understanding learned reward functions , ” arXiv preprint arXiv : 2012 . 05862 , 2020 . [ 51 ] Jenner , E . , and Gleave , A . , “Preprocessing Reward Functions for Interpretability , ” arXiv preprint arXiv : 2203 . 13553 , 2022 . [ 52 ] Chapman , D . , and Kaelbling , L . P . , “Input Generalization in Delayed Reinforcement Learning : An Algorithm and Performance Comparisons . ” Ijcai , Vol . 91 , 1991 , pp . 726 – 731 . [ 53 ] Džeroski , S . , Raedt , L . D . , and Blockeel , H . , “Relational reinforcement learning , ” International Conference on Inductive Logic Programming , Springer , 1998 , pp . 11 – 22 . [ 54 ] Pyeatt , L . D . , “Reinforcement learning with decision trees . ” 21 st IASTED International Multi - Conference on Applied Informatics , 2003 , pp . 26 – 31 . [ 55 ] Silva , A . , Gombolay , M . , Killian , T . , Jimenez , I . , and Son , S . - H . , “Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning , ” Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics , Vol . 108 , PMLR , 2020 , pp . 1855 – 1865 . [ 56 ] Liu , G . , Schulte , O . , Zhu , W . , and Li , Q . , “Toward interpretable deep reinforcement learning with linear model u - trees , ” Joint European Conference on Machine Learning and Knowledge Discovery in Databases , Springer , 2018 , pp . 414 – 429 . [ 57 ] Jiang , W . - C . , Hwang , K . - S . , and Lin , J . - L . , “An experience replay method based on tree structure for reinforcement learning , ” IEEE Transactions on Emerging Topics in Computing , Vol . 9 , No . 2 , 2019 , pp . 972 – 982 . [ 58 ] Bastani , O . , Pu , Y . , and Solar - Lezama , A . , “Verifiable Reinforcement Learning via Policy Extraction , ” Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 59 ] Cobo , L . C . , Isbell Jr , C . L . , and Thomaz , A . L . , “Automatic task decomposition and state abstraction from demonstration , ” Georgia Institute of Technology , 2012 . [ 60 ] Tambwekar , P . , Silva , A . , Gopalan , N . , and Gombolay , M . , “Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning , ” arXiv preprint arXiv : 2101 . 07140 , 2021 . [ 61 ] Sheikh , H . U . , Khadka , S . , Miret , S . , Majumdar , S . , and Phielipp , M . , “Learning intrinsic symbolic rewards in reinforcement learning , ” 2022 International Joint Conference on Neural Networks ( ĲCNN ) , IEEE , 2022 , pp . 1 – 8 . [ 62 ] Bradley , R . A . , and Terry , M . E . , “Rank analysis of incomplete block designs : I . The method of paired comparisons , ” Biometrika , Vol . 39 , No . 3 / 4 , 1952 , pp . 324 – 345 . [ 63 ] Suárez , A . , and Lutsko , J . F . , “Globally optimal fuzzy decision trees for classification and regression , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol . 21 , No . 12 , 1999 , pp . 1297 – 1311 . [ 64 ] Gulliksen , H . , “A least squares solution for paired comparisons with incomplete data , ” Psychometrika , Vol . 21 , No . 2 , 1956 , pp . 125 – 134 . [ 65 ] Kingma , D . P . , and Ba , J . , “Adam : A method for stochastic optimization , ” arXiv preprint arXiv : 1412 . 6980 , 2014 . [ 66 ] Breiman , L . , Friedman , J . H . , Olshen , R . A . , and Stone , C . J . , Classification and regression trees , Routledge , 2017 . 16 [ 67 ] Tien , J . , He , J . Z . - Y . , Erickson , Z . , Dragan , A . D . , and Brown , D . , “A Study of Causal Confusion in Preference - Based Reward Learning , ” arXiv preprint arXiv : 2204 . 06601 , 2022 . [ 68 ] Haarnoja , T . , Zhou , A . , Abbeel , P . , and Levine , S . , “Soft actor - critic : Off - policy maximum entropy deep reinforcement learning with a stochastic actor , ” International Conference on Machine Learning , PMLR , 2018 , pp . 1861 – 1870 . [ 69 ] Rahtz , M . , Varma , V . , Kumar , R . , Kenton , Z . , Legg , S . , and Leike , J . , “Safe Deep RL in 3D Environments using Human Feedback , ” arXiv preprint arXiv : 2201 . 08102 , 2022 . [ 70 ] Chua , K . , Calandra , R . , McAllister , R . , and Levine , S . , “Deep reinforcement learning in a handful of trials using probabilistic dynamics models , ” Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 71 ] Gleave , A . , Dennis , M . D . , Legg , S . , Russell , S . , and Leike , J . , “Quantifying Differences in Reward Functions , ” International Conference on Learning Representations , 2021 . [ 72 ] Kendall , M . G . , “A new measure of rank correlation , ” Biometrika , Vol . 30 , No . 1 / 2 , 1938 , pp . 81 – 93 . 17