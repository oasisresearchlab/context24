Learning Interpretable Models of Aircraft Handling Behaviour by Reinforcement Learning from Human Feedback Tom Bewley âˆ— , Jonathan Lawry â€  and Arthur Richards â€¡ University of Bristol , Bristol , United Kingdom We propose a method to capture the handling abilities of fast jet pilots in a software model via reinforcement learning ( RL ) from human preference feedback . We use pairwise preferences over simulated flight trajectories to learn an interpretable rule - based model called a reward tree , which enables the automated scoring of trajectories alongside an explanatory rationale . We train an RL agent to execute high - quality handling behaviour by using the reward tree as the objective , and thereby generate data for iterative preference collection and further refinement of both tree and agent . Experiments with synthetic preferences show reward trees to be competitive with uninterpretable neural network reward models on quantitative and qualitative evaluations . I . Introduction Pilots of fast jet aircraft require exceptional handling abilities , acquired over years of advanced training . There would be significant practical value in a method capable of distilling the skills , knowledge and preferences of pilots and other domain experts into a software model that captures realistic handling behaviour . The scalability of such a model would make it useful for strategic planning exercises , training , and development and testing of other software systems . This would enable greater return from the scarce resource of human piloting expertise . This vision faces the practical challenge of accurately eliciting the desired knowledge for codification into an automated system . As in many contexts requiring intuitive decision - making and rapid motor control , experts know good handling when they see it , but cannot always express why in formal or linguistic terms [ 1 ] . âˆ— An explicit knowledge elicitation strategy would also likely be time - intensive , as would any approach relying on expert demonstration . This motivates a learning - based approach using a sparser data source . In light of the importance of transparency for safety - critical aviation applications [ 2 , 3 ] , it is crucial that any such approach learns an interpretable ( i . e . human - readable and comprehensible ) model of the expert knowledge , to facilitate trust and verification . This paper proposes a possible solution to this brief . We use an artificial reinforcement learning ( RL ) agent to generate a dataset of simulated flight trajectories , then consult an expert to obtain pairwise preferences over those trajectories , indicating which is preferred as a solution to a given task of interest . Pairwise preference elicitation is known to be robust and time - efficient , and provides a basis for combining data from multiple experts without the challenge of agreeing a common scoring system . We then use statistical learning algorithms to construct an interpretable explanatory model of the gathered preferences in the form of a rule - based tree structure . In turn , the tree is used as a reward function to train the agent to generate higher - quality trajectories , and the process is iterated to convergence . The end result is two distinct outputs that could form valuable components of future planning , training and development software : 1 ) A tree - structured reward function ( referred to as a reward tree ) , which captures tacit expert preferences in a human - readable form . A reward tree may be used for consistent automated scoring of flight trajectories executed by human or artificial pilots in a way that is aligned with the judgement that the original expert would have made , alongside an explanatory rationale that can be used to justify , verify and improve handling behaviour . 2 ) An RL agent capable of executing high - quality handling behaviour with respect to the objective specified by the reward tree , for use in simulation ( e . g . as a scalable demonstrator for pilot training ) . RL for aviation is well - studied , with impressive results using neural networks ( NNs ) , but typically requires reward to be heuristically ( thus potentially erroneously ) defined rather than flexibly learnt , and lacks interpretability of the underlying model . Our proposal is a model that is effective , interpretable and learnable from human feedback . âˆ— PhD Student , Department of Engineering Mathematics . Email : tom . bewley @ bristol . ac . uk . â€  Professor , Department of Engineering Mathematics . Email : j . lawry @ bristol . ac . uk . â€¡ Professor , Department of Aerospace Engineering and Bristol Robotics Laboratory . Email : arthur . richards @ bristol . ac . uk . âˆ— This statement certainly underestimates the rich complexity of human expertise ; in reality , an expertâ€™s mental model is likely to be partly tacit and partly explicit . The strategy taken in this paper is to operate as if the mental model were 100 % tacit , and explore what can be achieved under such a strong restriction . Real - world applications would likely benefit from combining this approach with some amount of hand - coded expert knowledge . 1 a r X i v : 2305 . 16924v1 [ c s . A I ] 26 M a y 2023 Having explored the fundamentals of preference - based reward tree learning in some simple benchmark cases in prior work [ 4 ] , our primary aim in this paper is to perform a proof - of - concept application to a more complex problem motivated by a real industrial need . In particular , we seek to compare the fast jet handling performance of RL agents trained using learnt reward trees to those using deep NNs as their reward learning models , which represent the state - of - the - art from prior work [ 5 , 6 ] but lack the crucial interpretability properties required for high - stakes applications . To perform this comparative evaluation at scale , we require a large quantity of preference data for multiple fast jet handling tasks . Since collecting such data from real pilots and aviation experts would be costly and logistically complex , our proof - of - concept experiments use synthetic preferences with respect to nominal oracle evaluator functions of varying complexity . Using oracles as a proxy for human evaluators is popular [ 5 â€“ 9 ] as it enables scalable systematic comparison , with the ability to quantify performance ( and in our case , appraise learnt trees ) in terms of the reconstruction of a known ground truth . However , emulating a human with an oracle that responds with perfect rationality is unrealistic [ 10 ] . For this reason , we also examine the performance impacts of noisy and myopic oracles , and a restricted data budget . This paper describes experiments with synthetic oracle preference data for three fast jet handling tasks . We find that reward trees can be competitive with NN reward models in both quantitative and qualitative aspects of learning performance , while having the advantage of human - readability . Our secondary contributions include improvements to our original learning algorithm , and an illustrative analysis of learnt tree structures to demonstrate their interpretability . II . Background and Related Work Markov Decision Processes ( MDPs ) In this canonical formulation of sequential decision making [ 11 ] , the state of a system at time ğ‘¡ , ğ‘  ğ‘¡ âˆˆ S , and the action of an agent , ğ‘ ğ‘¡ âˆˆ A , condition the successor state ğ‘  ğ‘¡ + 1 according to dynamics ğ· : S Ã— A â†’ Î” ( S ) . A reward function ğ‘… : S Ã— A Ã— S â†’ R then outputs a scalar reward ğ‘Ÿ ğ‘¡ + 1 given ğ‘  ğ‘¡ , ğ‘ ğ‘¡ and ğ‘  ğ‘¡ + 1 . RL algorithms use exploratory data collection to learn action - selection policies ğœ‹ : S â†’ Î” ( A ) for MDPs , with the goal of maximising the expected discounted sum of future reward , E ğ· , ğœ‹ (cid:205) âˆ â„ = 0 ğ›¾ â„ ğ‘Ÿ ğ‘¡ + â„ + 1 , ğ›¾ âˆˆ [ 0 , 1 ] . RL for Aviation RL has seen widespread adoption in aviation [ 12 â€“ 14 ] . It has been used to learn landing [ 15 ] and aerobatics [ 16 ] behaviours for fixed - wing aircraft , as an alternative or supplement to learning from costly human demonstrations [ 17 , 18 ] . Other work has retained a focus on learning from and with humans , using RL to predict pilot interactions in an airspace [ 19 ] , implement shared autonomy for single aircraft control [ 20 ] , and simulate student learning dynamics in pilot training [ 21 ] . Our proposal is an alternative integration of humans into the RL process . Reward Learning In the usual MDP framing , reward is an immutable property of the environment , which belies the practical fact that AI objectives originate in the uncertain goals and preferences of humans [ 22 ] . Reward learning [ 23 ] replaces hand - specified reward functions with models learnt from feedback signals such as demonstrations [ 24 ] , scalar evaluations [ 25 ] , approval labels [ 7 ] , and corrections [ 26 ] . In this work , we adopt the preference - based approach [ 5 ] , in which a human observes pairs of agent trajectories and expresses which of each pair they prefer as a solution to a given task of interest . A reward function is learnt to reconstruct the pattern of preferences . This approach is popular [ 6 , 27 â€“ 29 ] and has a firm psychological basis . Experimental results indicate that humans find it easier to make relative ( vs . absolute ) quality judgements [ 30 , 31 ] and exhibit lower variance when doing so [ 32 ] . Providing preferences also incurs less cognitive burden than providing demonstrations [ 33 ] and may enable more fine - grained distinctions [ 34 ] . Explainable RL ( XRL ) Surveys of methods for making RL understandable to humans [ 35 , 36 ] divide between intrinsic approaches , which imbue agents with structure such as object representations [ 37 ] or symbolic policy primitives [ 38 ] , and post hoc analyses of learnt policies [ 39 ] , including feature importance [ 40 ] . Spatiotemporal scope varies from the local explanation of single actions [ 41 ] to the summary of entire policies via prototype trajectories [ 42 ] or states [ 43 ] . While most post hoc methods focus on fixed policies , some investigate the dynamics of agent learning [ 44 , 45 ] . Interpretable Reward Functions At the intersection of reward learning and XRL lie efforts to understand reward functions and their effects on action selection . While this area is â€œless developed " than others in XRL [ 46 ] , there exist both intrinsic approaches , which decompose rewards into semantic components [ 47 ] or optimise for sparsity [ 48 ] , and post hoc approaches , which apply feature importance analysis [ 49 ] , counterfactual probing [ 50 ] , or simplifying transformations [ 51 ] . Reward tree learning is an intrinsic approach , as the rule structure is inherently readable . Trees in RL Trees have a long history in RL [ 52 â€“ 54 ] . Their use is increasingly given an XRL motivation . Applications again divide into intrinsic methods , where an agentâ€™s policy [ 55 ] , value function [ 56 ] or dynamics model [ 57 ] is a tree , and post hoc tree approximations of an existing agentâ€™s policy [ 58 ] or transition statistics [ 45 ] . Related to our focus on learning from humans , Cobo et al . [ 59 ] learn tree - structured MDP abstractions from demonstrations and Tambwekar et al . [ 60 ] distil a differentiable tree policy from natural language . While Sheikh et al . [ 61 ] use tree evolution to learn dense intrinsic rewards from sparse environment ones , our prior work [ 4 ] is the first to learn reward trees in the absence of any ground - truth reward signal , and the first to do so from human feedback . 2 III . Aircraft Handling Environment , Tasks and Oracles To formulate the aircraft handling problem as an MDP , we consider a simple set piece setup , in which the piloting agent is given a short time window ( called an episode ) to manoeuvre its aircraft ( the ego jet , EJ ) in a particular manner relative to a second reference jet ( RJ ) whose motion , if any , is considered part of the environment dynamics . The state space S contains the positions , attitudes , velocities and accelerations of both EJ and RJ , and the action space A consists of pitch , roll , yaw and thrust demands for EJ only . The EJ dynamics function integrates these demands with a simplified physics engine , including gravity and air resistance . RJ dynamics , as well as the conditions of episode initialisation and termination , vary between tasks ( see below ) . This set piece formulation strikes a balance between simplicity and generality ; many realistic scenarios faced by a fast jet pilot involve interaction with a single other airborne entity . It provides scope for the definition of many alternative tasks given the same state and action spaces , and largely unchanged dynamics . In this work , we consider the three tasks shown in Figure 1 : â€¢ Follow : RJ follows a linear horizontal flight path at a constant velocity , which is oriented opposite to the initial velocity of EJ . The task is to turn onto and then maintain the path up to the episode time limit of 20 timesteps ( â‰ˆ 20 seconds , as timesteps are at approximately 1Hz ) . This constitutes a very simple form of formation flight . â€¢ Chase : RJ follows an erratic trajectory generated by random control inputs , and the task is to chase it , maintaining distance and line of sight , without taking EJ below a safe altitude . Episodes terminate after 20 timesteps . â€¢ Land : The task is to execute a safe approach towards landing on a runway , where RJ represents the ideal landing position ( central , zero altitude , slight upward pitch ) . EJ is initialised at a random altitude , pitch , roll and offset , such that landing may be challenging but always physically possible . An episode terminates if EJ passes RJ along the axis of the runway , or after 25 timesteps otherwise . Ego Jet Action : pitch , roll , yaw , thrust demands for EJ RJ EJ Follow RJ EJ Chase RJ EJ Land Reference Jet State : pose informationfor both EJ and RJ Proceedsalong linear flight path Task : turn onto same flight path and matchspeed Representsa target touchdownpose Task : execute a stable approach path , ending at target pose Fliessubject to randomcontrolinput Task : stay close to RJ with line of sight ; keep safe altitude Fig . 1 State - action space of aircraft handling domain , and diagrams of Follow , Chase and Land tasks . The central thesis of this paper is that there exists no unambiguous model of good aircraft handling behaviour . For instance , experts may agree that the Chase task involves a trade - off between speed of response and smoothness of flight , but may also have nebulous and divergent definitions of these properties and their relative importance . However , to quantitatively evaluate our method , we adopt the artificial construct of synthetic oracles based on â€˜ground - truthâ€™ reward functions , as proxies for real human evaluators . The oracles functions use their reward functions to provide evaluative preference feedback to our reward learning method according to a model defined in Section IV . The precise nature of the oracle reward functions is secondary , and those given below are among many equally reasonable alternatives , but we dedicated several hours of development time to ensuring they incentivise reasonable behaviour upon visual inspection . The difficulty and seeming arbitrariness of such a manual reward design process is precisely why reward learning ( ultimately from real human preferences ) is a compelling proposition . The oracles are all defined using a common set of state - action features , which are enumerated and described in Table 2 ( see Appendix ) : â€¢ Follow : The oracle prioritises closing the distance between EJ and RJ , and matching their upward axes : ğ‘… follow = âˆ’ ( dist + 0 . 05 Ã— closing speed + 10 Ã— up error ) . â€¢ Chase : The oracle prioritises keeping RJ at a distance of 20 and within EJâ€™s line of sight , while keeping EJ oriented upright . It also has a large penalty for dropping below a safe altitude of 50 ( note that the square brackets [ Â· ] are Iverson notation , which is used throughout this paper for indicator functions ) : ğ‘… chase = âˆ’ ( abs ( dist âˆ’ 20 ) + 10 Ã— los error + 5 Ã— abs roll + 100 Ã— [ alt < 50 ] ) . â€¢ Land : The oracle for this task is the most complex , including terms that incentivise continual descent , penalise g - force and engine thrust , and punish EJ for contacting the ground before the runway [ alt < 0 . 6 ] : ğ‘… land = âˆ’ ( 0 . 05 Ã— abs lr offset + 0 . 05 Ã— alt + hdg error + abs roll + 0 . 5 Ã— pitch error + 0 . 25 Ã— ( yaw rate + roll rate + pitch rate ) + 0 . 1 Ã— g force + 0 . 025 Ã— thrust + 0 . 05 Ã— delta thrust + [ delta dist hor > 0 ] + 2 Ã— [ delta alt > 0 ] + [ abs lr offset > 10 ] + 10 Ã— [ alt < 0 . 6 ] ) . 3 IV . Preference - based Reward Learning We adopt the preference - based approach to reward learning , in which a human ( or in our case , oracle ) evaluator is presented with pairs of agent trajectories ( sequences of state , action , next state transitions ) and expresses which they prefer as a solution to a given task of interest . A reward function is learnt to reconstruct the pattern of preferences . We assume that the evaluator observes a trajectory ğœ‰ ğ‘– as a sequence ( x ğ‘– 1 , . . . , x ğ‘–ğ‘‡ ğ‘– ) , where x ğ‘–ğ‘¡ = ğœ™ ( ğ‘  ğ‘–ğ‘¡ âˆ’ 1 , ğ‘ ğ‘–ğ‘¡ âˆ’ 1 , ğ‘  ğ‘–ğ‘¡ ) âˆˆ R ğ¹ represents a single transition as an ğ¹ - dimensional feature vector . The choice of feature function ğœ™ is crucial . For our experiments , we consulted with engineers with experience in aerospace simulation and control algorithms to define ğ¹ = 30 features that are sufficiently expressive to capture the important information for all three of our target tasks , without being overly specialised to one or providing too much explicit guidance to the reward learning process . These features are given in Table 2 ( see Appendix ) . Note that all three oracles given above use a subset of the 30 features , meaning that optimal reward learning is possible in principle but requires accurate feature selection . Given a set of ğ‘ trajectories , Î = { ğœ‰ ğ‘– } ğ‘ğ‘– = 1 , the evaluator is consulted to provide ğ¾ â‰¤ ğ‘ ( ğ‘ âˆ’ 1 ) / 2 pairwise preference labels , L = { ( ğ‘– , ğ‘— ) } ğ¾ğ‘˜ = 1 , each of which indicates that the ğ‘— th trajectory is preferred to the ğ‘– th ( denoted by ğœ‰ ğ‘— â‰» ğœ‰ ğ‘– ) . Figure 2 ( top ) shows how a preference dataset D = ( Î , L ) can be viewed as a directed graph . To learn a reward function from D , we must assume a generative preference model . Typically , it is assumed that the evaluator tends to prefer trajectories that have higher summed reward ( or return ) according to a latent reward function over the feature space , ğ‘… : R ğ¹ â†’ R , which represents their tacit understanding of the task . However , they are liable to occasionally make mistakes in their judgement . This is formalised by the Bradley - Terry preference model [ 62 ] : ğ‘ƒ ( ğœ‰ ğ‘— â‰» ğœ‰ ğ‘– | ğ‘… ) = 1 1 + exp ( 1 ğ›½ ( ğº ( ğœ‰ ğ‘– | ğ‘… ) âˆ’ ğº ( ğœ‰ ğ‘— | ğ‘… ) ) ) , ( 1 ) where ğº ( ğœ‰ ğ‘– | ğ‘… ) = (cid:205) ğ‘‡ ğ‘– ğ‘¡ = 1 ğ‘… ( x ğ‘–ğ‘¡ ) and ğ›½ > 0 is a temperature coefficient determining the probability of mistakes . In our oracle experiments , synthetic preferences are generated according to Equation 1 using the corresponding ğ‘… follow , ğ‘… chase or ğ‘… land , thereby adhering to the modelling assumption . In our main experiments , we set ğ›½ = 0 , which results in the oracles deterministically selecting trajectories with higher return ( ties broken uniform - randomly ) . We subsequently study cases with ğ›½ > 0 , which provide a more realistic emulation of real human preference data . Given a preference dataset and an assumed generative model such as Equation 1 , the objective of reward learning is to approximate the latent reward function ğ‘… within some learnable function class R . This problem is often formalised as minimising the negative log - likelihood ( NLL ) loss over the preferences in L [ 5 , 6 ] . Wirth et al . [ 27 ] also use the discrete 0 - 1 loss , which considers only the directions of predicted preferences rather than their strengths . Our model uses both of these losses at different stages in its learning process . They are respectively defined as follows : â„“ NLL ( D , ğ‘… ) = âˆ‘ï¸ ( ğ‘– , ğ‘— ) âˆˆL âˆ’ log ğ‘ƒ ( ğœ‰ ğ‘— â‰» ğœ‰ ğ‘– | ğ‘… ) ; ( 2 ) â„“ 0 - 1 ( D , ğ‘… ) = âˆ‘ï¸ ( ğ‘– , ğ‘— ) âˆˆL [ ğ‘ƒ ( ğœ‰ ğ‘— â‰» ğœ‰ ğ‘– | ğ‘… ) â‰¤ 0 . 5 ] . ( 3 ) In early prior work , R was often the class of linear models ğ‘… ( x ) = w âŠ¤ x [ 28 ] , which are easy to interpret but have limited expressiveness , so cannot scale to complex tasks . More recently , it has been common to use deep NNs [ 5 ] ( or multi - network ensembles thereof ) , which are far more powerful , while remaining tractably learnable by gradient - based optimisation . However , the complex multilayered architecture of a deep NN resists human scrutiny and interpretation . This motivates a â€˜third wayâ€™ function class that compromises between these two extremes . V . Interpretable Reward Learning with Trees As an intermediate option between the limited expressiveness of linear models and the uninterpretable complexity of NNs , our recent prior work proposes reward trees [ 4 ] . Here , R is the class of axis - aligned decision trees , which are hierarchies of local rules constructed from the feature set . Reward trees admit visual and textual representation , and produce a traceable decision path for each reward prediction . While methods exist for learning trees by gradient - based optimisation [ 63 ] , these are of the oblique ( c . f . axis - aligned ) kind , whose multi - feature rules are far harder to interpret in high dimensions . Therefore , instead of optimising one of the losses from Equations 2 and 3 end - to - end , we use a multi - stage induction method with a proxy objective at each stage . The four stages outlined below , and depicted in Figure 2 , depart from our original method in several respects , which we highlight and justify where relevant . A . Trajectory - level Return Estimation This first stage of our method considers the ğ‘ trajectories as atomic units , and uses the preference graph to construct a vector of return estimates g âˆˆ R ğ‘ , which should be higher for more preferred trajectories ( blue in Figure 2 ( V . A ) , c . f . red ) . This is a vanilla preference - based ranking problem , and admits a standard solution . In [ 4 ] , we use a least squares 4 A dataset of pairwise preferences over trajectories can be visualised as a directed graph , where nodes are trajectories and edges are preferences . Each trajectory is a sequence of vectors in feature space . In turn , each vector represents the ( state , action , next state ) transition that occurs at a particular timestep t . Feature vector for timestep t : mapping Given an existing tree structure and trajectories with estimated returns , we define the reward for each leaf as a timestep - weighted average over the trajectories that visit that leaf . V . B Naive assumption : all timesteps in trajectory contribute equally to return ( i . e . uniform colour ) Reward calculation for highlighted leaf : Growing a tree by adding rules recursively splits the feature space into an increasing number of leaves . In our improved growth method , we use the 0 - 1 loss as the criterion for selecting rules to add . V . C Growth tends to separate high - and low - return trajectories into different leaves " Pure " leaf containing a single trajectory After growing to a maximum size , a final pruning stage recursively removes rules to minimise the 0 - 1 loss , with an additional regularisation term to penalise large trees . This yields a reduced tree for use as a reward function . V . D Pruning merges leaves back together , while retaining the most important splits for preference prediction Reward values shown are illustrative V . A Before doing anything in the feature space , we first use the standard Bradley - Terry preference model to estimate an overall return value for each trajectory in the dataset based on its preferences . Two favourable preferences ; highest estimated return Three unfavourable preferences ; lowest estimated return Preferred to but not to ; intermediate estimated return Each edge points to preferred trajectory Fig . 2 Top : The input to preference - based reward learning is a directed graph over a trajectory set , each of which is a sequence of points in R ğ¹ ( blue connectors show mapping ) . Bottom : The four stages of reward tree learning : return estimation ( Section V . A ) ; leaf - level reward prediction ( V . B ) ; tree growth ( V . C ) ; pruning ( V . D ) . matrix method to solve for g under Thurstoneâ€™s Case V preference model [ 64 ] . For consistency with prior work , and to avoid an awkward clipping step which biases preference probabilities to enable matrix inversion , we now use a gradient method to minimise â„“ NLL under the Bradley - Terry model instead . Concretely , the objective for this stage is argmin g âˆˆ R ğ‘ (cid:104) âˆ‘ï¸ ( ğ‘– , ğ‘— ) âˆˆL âˆ’ log 1 1 + exp ( g ğ‘– âˆ’ g ğ‘— ) (cid:105) , ( 4 ) which we optimise by gradient descent using the Adam optimiser [ 65 ] . As a minor detail , we post - normalise g so that its elements have a standard deviation equal to the mean trajectory length in Î , (cid:205) ğ‘ğ‘– = 1 ğ‘‡ ğ‘– / ğ‘ , and a common sign ( i . e . all positive or negative ) . We anecdotally find that this aids the interpretability of the final reward tree . B . Leaf - level Reward Prediction The vector g estimates trajectory returns , but the ultimate aim of reward learning is to decompose these into sums of rewards for the constituent transitions , then generalise to make reward predictions for unseen data ( e . g . novel trajectories executed by an RL agent ) . Our core contribution is to do this using a tree model T , consisting of a hierarchy of rules that partition the transition - level feature space R ğ¹ into ğ¿ T hyperrectangular regions called leaves . Given such a tree , each leaf ğ‘™ âˆˆ { 1 . . ğ¿ T } is associated with a reward prediction r ğ‘™ as follows . Let the function leaf T : R ğ¹ â†’ { 1 . . ğ¿ T } map a feature vector x âˆˆ R ğ¹ to the leaf in which it resides by propagating it through the rule hierarchy . r ğ‘™ is defined as an 5 average over g , weighted by the proportion of time that each trajectory in Î spends in ğ‘™ : r ğ‘™ = ğ‘ âˆ‘ï¸ ğ‘– = 1 g ğ‘– ğ‘‡ ğ‘– (cid:205) ğ‘‡ ğ‘– ğ‘¡ = 1 [ leaf T ( x ğ‘–ğ‘¡ ) = ğ‘™ ] (cid:205) ğ‘ğ‘— = 1 (cid:205) ğ‘‡ ğ‘— ğ‘¡ = 1 [ leaf T ( x ğ‘—ğ‘¡ ) = ğ‘™ ] . ( 5 ) The effect of Equation 5 is to assign higher reward to leaves that contain more timesteps from trajectories with high g values ( i . e . those more commonly preferred in the preference dataset ) . While ostensibly naÃ¯ve , we find that this time - weighted credit assignment is more robust than several more complex alternatives . It reduces the free parameters in subsequent induction stages , permits fast implementation , and provides an intuitive interpretation of predicted reward that is traceable back to a g value and timestep count for each ğœ‰ ğ‘– âˆˆ Î . Figure 2 ( V . B ) shows how 4 , 2 and 3 timesteps from ğœ‰ 1 , ğœ‰ 3 and ğœ‰ 4 are averaged over to yield the reward prediction for one leaf ( indicated by the orange shading ) . This definition provides the basis for using T as a reward function . Given an arbitrary feature vector x âˆˆ R ğ¹ , we simply look up the reward of the leaf in which it resides : ğ‘… T ( x ) = r leaf T ( x ) . ( 6 ) C . Tree Growth Recall that the objective of preference - based reward learning is to adjust the parameters of the reward model in order to minimise some loss function over D , such as those in Equations 2 and 3 . When the model is a tree , this is achieved by the discrete operations of growth ( adding partitioning rules to increase the number of leaves ) and pruning ( removing rules to decrease the number of leaves ) . Given a tree T , a new rule has the effect of splitting the ğ‘™ th leaf with a hyperplane at a location ğ‘ âˆˆ C ğ‘“ along the ğ‘“ th feature dimension ( where C ğ‘“ âŠ‚ R is a set of candidate split thresholds , e . g . all midpoints between unique values in Î ) . Let T + [ ğ‘™ ğ‘“ ğ‘ ] denote the newly - enlarged tree . Splitting recursively creates an increasingly fine partition of R ğ¹ . Figure 2 ( V . C ) shows an example of a reward tree with 23 leaves . A central issue is the criterion for selecting the next rule to add . In [ 4 ] , we use the proxy objective of minimising the local variance of g values in each leaf , which exactly corresponds to the classic CART algorithm [ 66 ] . While very fast to compute , this criterion is only loosely aligned with the reconstruction of the preferences in D . In the present work , we propose and investigate the more direct criterion of greedily minimising the â„“ 0 - 1 of the enlarged tree T + [ ğ‘™ ğ‘“ ğ‘ ] : argmin 1 â‰¤ ğ‘™ â‰¤ ğ¿ T , 1 â‰¤ ğ‘“ â‰¤ ğ¹ , ğ‘ âˆˆC ğ‘“ (cid:2) â„“ 0 - 1 ( D , ğ‘… T + [ ğ‘™ ğ‘“ğ‘ ] ) (cid:3) , ( 7 ) i . e . selecting splits to minimise the number of incorrectly - predicted preferences . In Section VII , we show that switching to this criterion consistently improves reward learning and agent policy performance on the aircraft handling tasks . Recursive splitting stops when â„“ 0 - 1 cannot be reduced by any single split , or a tree size limit ğ¿ T = ğ¿ max is reached . D . Tree Pruning Growth is followed by a pruning stage which reduces the size of the tree by rule removal . This is beneficial for both performance ( Tien et al . [ 67 ] find that limiting model capacity lowers the risk of causal confusion in preference - based reward learning ) and human comprehension ( in the language of Jenner and Gleave [ 51 ] , pruning is a form of â€œprocessing for interpretability " ) . Given a tree T , one pruning operation has the effect of merging two leaves into one by removing the rule at the common parent node . Let T denote the sequence of nested subtrees induced by pruning the tree recursively back to its root , at each step removing the rule that minimises the next subtreeâ€™s â„“ 0 - 1 . We select the T âˆˆ T that minimises â„“ 0 - 1 , additionally regularised by a term that encourages small trees : argmin Tâˆˆ T [ â„“ 0 - 1 ( D , ğ‘… T ) + ğ›¼ğ¿ T ] , ( 8 ) where ğ›¼ â‰¥ 0 is a regularisation coefficient . In the example in Figure 2 ( V . D ) , pruning yields a final tree with 3 leaves , for which illustrative leaf - level reward predictions are shown . VI . Online Learning Process A . Iterated Policy and Reward Learning Sections IV and V do not discuss the origins of the trajectories Î , or how reward tree learning should be integrated with the process of policy learning by RL . Following most recent work since Christiano et al . [ 5 ] , we resolve both questions with an iterative bootstrapped approach , in which the current reward tree defines the RL agentâ€™s reward function at each point in learning . During learning episode ğ‘– , the agent uses its latest policy to produce a new trajectory ğœ‰ ğ‘– . We immediately connect ğœ‰ ğ‘– to the preference graph by asking the human ( read : oracle ) to compare it to ğ¾ batch random trajectories from the existing set . We then update the reward tree on the full preference graph via the stages in 6 Section V . We find that our original method of starting growth from the current state of the tree causes lock - in to poor initial solutions , so instead re - grow from scratch ( i . e . starting from a single leaf ) on each update . The rule structure nonetheless tends to stabilise , as the enlarging preference graph becomes increasingly similar for later updates . For the ( ğ‘– + 1 ) th episode , the RL agent then attempts to optimise its policy with respect to the newly - updated reward tree . By iterating this process up to a total preference budget ğ¾ max and / or episode budget ğ‘ max , we hope to converge to both a reward tree that reflects the humanâ€™s preferences , and an agent policy that satisfies those preferences . B . Model - based RL Algorithm Online reward learning is generally agnostic to the structure of the policy learning agent ; this modularity is hailed as an advantage over other human - agent teaching paradigms [ 23 ] . Following most recent work , in [ 4 ] we use a model - free RL agent , specifically soft actor - critic ( SAC ) [ 68 ] . However , other work [ 8 , 69 ] uses model - based RL ( MBRL ) agents that leverage learnt dynamics models and planning . MBRL is attractive in the reward learning context because it disentangles the predictive and normative aspects of decision - making . Since ( assuming no changes to the environment ) dynamics remain stationary during online reward learning , the amount of re - learning required is reduced . MBRL can also be very data - efficient ; our preliminary experiments found that switching from SAC to a model - based algorithm called PETS [ 70 ] reduces environment interaction during reward learning by orders of magnitude . PETS selects actions by decision - time planning through a learnt dynamics model ğ· â€² : S Ã— A â†’ Î” ( S ) up to a horizon ğ» . In state ğ‘  , planning searches for a sequence of ğ» future actions that maximise return according to the current reward tree : argmax ( ğ‘ 0 , . . . , ğ‘ ğ» âˆ’ 1 ) âˆˆA ğ» E ğ· â€² (cid:104) âˆ‘ï¸ ğ» âˆ’ 1 â„ = 0 ğ›¾ â„ ğ‘… T ( ğœ™ ( ğ‘  â„ , ğ‘ â„ , ğ‘  â„ + 1 ) ) (cid:105) , where ğ‘  0 = ğ‘  , ğ‘  â„ + 1 âˆ¼ ğ· â€² ( ğ‘  â„ , ğ‘ â„ ) . ( 9 ) The first action ğ‘ = ğ‘ 0 is executed , and then the agent re - plans on the next timestep . In practice , ğ· â€² is an ensemble of probabilistic NNs , the expectation over ğ· â€² is replaced by a Monte Carlo estimate , and the optimisation is approximated by the iterative cross - entropy method . We use PETS agents in all experiments in this paper . In our implementation , ğ· â€² is an ensemble of five NNs , each with four hidden layers of 200 units . Planning operates over a time horizon of ğ» = 10 , with no discounting ( ğ›¾ = 1 ) , and consists of 10 iterations of the cross - entropy method . Each iteration samples 20 candidate action sequences from an independent Gaussian , of which the top five in terms of return are identified as elites , then updates the sampling Gaussian towards the elites with a learning rate of 0 . 5 . This parameter affects how rapidly the distribution narrows towards a deterministic action sequence . In turn , this determines whether the agent only exploits actions that appear optimal under the current reward model ğ‘… T , or explores more diverse behaviour that might be preferred by the human evaluator . Correctly balancing the explore / exploit trade - off is especially crucial in the reward learning context . We find that the particular dynamics of the aircraft handling environment permit us to pre - train ğ· â€² on random data , and accurately generalise to states encountered during online reward learning . This means we perform no further updates to ğ· â€² while reward learning is ongoing . As well as improving execution speed , this avoids complexity and convergence issues arising from having two interacting learning processes ( note that simultaneous learning is unavoidable with model - free RL ) . To pre - train , we collect 1 ğ‘’ 5 transitions using a uniform random policy , then update each network on 1 ğ‘’ 5 independently sampled mini - batches of 256 transitions , using the mean squared error loss over next - state predictions . C . Diagram of Online Learning Process Figure 3 summarises the learning approach taken in this paper , with the details of some steps omitted for clarity . Although we obtain preferences from synthetic oracles , the process using a real human evaluator would be identical . Planner Action sequence Dynamics model Reward function Reward sequence Next action Current state Optimally - pruned tree State sequence Environment Append New trajectory Human Preferences Existing trajectories Append Append Trajectory set Sample Preference set Trajectory returns Return estimator Tree initialiser One - leaf tree Fully - grown tree Split generator Growing tree Candidate splits Loss function Optimal split Splitter Loss function Pruner Candidatepruned trees P r e f . c o ll ec ti on Pruning G r o w t h B e h a v i ou r a l e p i s od e PETSagent Fig . 3 Online preference - based reward tree learning with model - based PETS agents . 7 VII . Experiments and Results In this section , we combine quantitative and qualitative evaluations to assess the performance of reward tree learning with oracle preferences on the aircraft handling tasks , specifically in comparison to the de facto standard approach of using NNs . We also illustrate how the intrinsic interpretability of reward trees allows us to analyse what they have learnt . In all experiments , we use the following set of hyperparameters for tree learning . These were identified through informal search , and we make no claim of optimality , but they do lead to acceptable performance on the three tasks of varying complexity . The fact that we did not need to invest significant time in tuning indicates a general insensitivity of the method to precise hyperparameter values , which is widely seen as practically advantageous . â€¢ Trajectory return estimation using the Adam optimiser with a learning rate of 0 . 1 . Optimisation is stopped when the loss â„“ NLL changes by < 1 ğ‘’ âˆ’ 5 between successive gradient steps . â€¢ Per - feature candidate split thresholds C ğ‘“ defined as all midpoints between adjacent unique values in the trajectory set Î . These are recomputed on each update . â€¢ Tree size limit ğ¿ max = 100 . â€¢ Tree size regularisation coefficient ğ›¼ = 5 ğ‘’ âˆ’ 3 . In constructing the NN baseline , we aimed to retain as much of the algorithm structure from Figure 3 as possible , so that only the model architecture varies . The result is that we perform policy learning with PETS , trajectory pair sampling and oracle preference collection identically , and update the reward model after every episode . However , in place of the multi - stage tree growth and pruning process , we perform model updates by mini - batch gradient descent with respect to â„“ NLL from Equation 2 . In all experiments , we follow Lee et al . [ 6 ] in implementing the NN reward model as a three - layer network with 256 hidden units each and leaky ReLU activations , and performing the gradient - based updates using the Adam optimiser [ 65 ] with a learning rate of 3 ğ‘’ âˆ’ 4 . On each update , we sample ğ‘€ = 100 mini - batches of size ğµ = 32 and take one gradient step per mini - batch . A . Quantitative Performance In our main experiments , we use ideal , error - free oracles , which provide preferences according to Equation 1 with ğ›½ = 0 . We evaluate online reward learning with PETS using trees with the â„“ 0 - 1 split criterion , baselined against our original variance criterion , as well as the de facto standard of NN reward learning . We use ğ¾ max = 1000 preferences over ğ‘ max = 200 online trajectories ( i . e . those generated by the PETS agents during learning ) and run 10 repeats . As a headline statistic , Table 1 reports the oracle regret ratio ( ORR ) : the drop in average oracle return of PETS agents deployed using each trained reward model compared with directly using the oracle reward function , as a fraction of the drop to a random policy ( lower is better ) . This gives a normalised measure of how well reward learning performs compared with the ideal case of direct access to the true reward . We report the median and minimum ORR values across the 10 repeats for each task - model pairing . We observe that : 1 ) NN reward learning is strong on all tasks , 2 ) switching to a tree induces a small but variable performance hit , 3 ) â„“ 0 - 1 splitting outperforms the variance - based method , and 4 ) both NN and tree models sometimes exceed the direct use of the oracle for policy learning ( negative ORR ) . This counter - intuitive phenomenon has been observed before [ 29 ] and may be due to improved shaping in the learnt reward . Table 1 Median ( top ) Follow Chase Land and minimum ( bottom ) NN Tree ( 0 - 1 ) Tree ( var ) NN Tree ( 0 - 1 ) Tree ( var ) NN Tree ( 0 - 1 ) Tree ( var ) oracle regret ratios for . 000 . 120 . 284 âˆ’ . 030 . 040 . 126 . 014 . 050 . 062 all tasks and models . âˆ’ . 010 . 057 . 158 âˆ’ . 051 âˆ’ . 011 . 065 âˆ’ . 030 . 011 . 010 Figure 4 expands these results with more metrics , revealing learning trends not captured by headline ORR values . Metrics are plotted as time series over the 200 learning episodes ( sliding - window medians and interquartile ranges ( IQRs ) across repeats ) . In the left column ( a ) , the ORR of online trajectories shows how agent performance converges . For Follow , there is a gap between the models , with â„“ 0 - 1 splitting visibly aiding performance but still lagging behind the NNs . The learning curves for Chase and Land are more homogeneous , and the NNs reach only slightly lower asymptotes , with overlapping IQRs . The majority of models converge to their asymptotic performance well within the 200 episode learning period ; this learning speed is made possible by the use of model - based PETS agents . For the reward tree models , ( b ) shows how the number of leaves changes over time . There is notable consistency in these trends between the repeated runs . The variance - based trees tend to grow rapidly initially before stabilising or shrinking , while the â„“ 0 - 1 trees enlarge more conservatively , suggesting this method is less liable to overfit to small preference datasets . Trees of a readily - interpretable size ( â‰ˆ 20 leaves ) are produced for all tasks ; it is possible that performance could be improved by independently tuning the size regulariser ğ›¼ per task . 8 0 0 . 5 1 F o ll o w L a n d C h a s e Online ORR a Rank correlation e NN Tree ( 0 - 1 ) Tree ( var ) 0 0 . 5 1 0 0 . 2 0 . 4 0 - 1 loss c 0 0 . 05 0 . 1 0 . 150 0 . 05 0 . 1 0 . 150 0 . 05 0 . 1 0 . 15 0 . 25 0 . 5 0 . 75 1 0 0 . 25 0 . 5 0 . 75 1 Tree size b 10 20 10 20 10 20 0 0 . 5 1 0 0 . 5 1 0 . 2 0 . 6 1 0 0 . 5 1 Reward correlation d Fig . 4 Time series of metrics for online NN - and tree - based reward learning on all three tasks . ( c ) shows how the discrete preference loss â„“ 0 - 1 changes over time , which tends to increase as the growing preference graph presents a harder reconstruction problem , though the shapes of all curves suggest convergence ( note that random prediction gives â„“ 0 - 1 = 0 . 5 ) . For Follow and Land , the trees that directly split on â„“ 0 - 1 actually achieve lower loss than the NNs ; they more accurately predict the direction of preferences in the graph . This is an encouraging result , indicating that our models perform well on the metric for which they are directly optimised , but the fact that this does not translate into lower ORR indicates that the problems of learning a good policy and replicating the preference dataset are not perfectly correlated . This subtle point has previously been made by Lindner et al . [ 9 ] . Gleave et al . [ 71 ] recently highlighted the importance of comparing and evaluating learnt reward functions in a policy - invariant manner , by using a common evaluation dataset rather than on - policy data generated by agents optimising for each reward . We report such a comparison in the final two columns , where the evaluation datasets are generated by PETS agents using the oracle reward functions , with added action randomisation to increase diversity . Using these datasets , we correlate each reward modelâ€™s predictions with its respective oracle at each point in learning , in terms of both transition - level rewards ( d ) and the ordinal ranking of trajectories by return ( e ) , the latter via the Kendall [ 72 ] ğœ coefficient . The curves subtly differ , indicating that it is possible to reconstruct trajectory rankings ( and by implication , any pairwise preferences ) to a given accuracy with varying fidelity at the individual reward level . However , the common overall trend is that â„“ 0 - 1 - based trees outperform variance - based ones , with NNs sometimes improving again by a smaller margin , and sometimes bringing no added benefit . Moving top - to - bottom down the tasks , the gap between models reduces from both sides ; NN performance worsens while variance - based trees improve . A potentially important factor in these experiments is that the oracle reward for Follow is a linear function , while the others contain progressively more terms and discontinuities ( see Section III ) . A trend suggested by these results is thus that the performance gap between NNs and reward trees ( on both ORR and correlation metrics ) reduces as the true reward becomes more complex and nonlinear . Further experiments would be needed to test this hypothesis . B . Visual Trajectory Inspection While useful for benchmarking , quantitative metrics provide little insight into the structure of the learnt solutions . They would also mostly be undefined when learning from real humans since the ground truth reward is unknown . We therefore complement them with a visual analysis of induced agent behaviour . Figure 5 plots 500 trajectories of PETS agents using the best repeat by ORR for each task - model combination , across a range of features as well as time ( see Table 2 for a reminder of feature definitions ) . Each trajectory is coloured on a red - blue scale according to its ORR . Dashed black curves indicate the single trajectory with the highest predicted return according to each model . We also show trajectories for PETS agents with direct oracle access , which serve as the benchmark behaviour that we aim to match via reward learning , and for random policies , which perform very poorly on all three tasks . The high - level trend is that all models are far closer to the oracle than random , with few examples of obviously unstable handling behaviour or task failure ( highlighted in red , due to colouring by ORR ) . While the NNs induce trajectories that are almost indistinguishable from the oracle , the â„“ 0 - 1 - based reward trees lag not far behind . 9 Oracle Tree ( var ) F o ll o w 0 353 - 31 31 dist c l o s i n g s p ee d 1 20 0 u p e rr o r Timestep C h a s e L a n d 50 350 a l t los error - 1 . 40 1 . 55 roll p i t c h 0 200 0 30 a l t dist hor ORR 0 1 0 - 1 20 20 150 Timestep d i s t 0 , 0 0 , 0 Random e g j h f Tree ( 0 - 1 ) a b d c Highest return accordingto oracle NN Highest return accordingto model i Fig . 5 Agent trajectories using the best models by ORR , with oracle and random for comparison . The variance - based trees exhibit more anomalies . Successes of the â„“ 0 - 1 trees include the execution of Follow with a single banked turn before straightening up , as shown by the up error time series ( a ) , where up error = 0 is level flight . Interestingly , both this tree and the NN reward model appear to favour a somewhat earlier turn than the oracle ( i . e . peak shifted to the left on this plot ) . Indeed , the trajectories for the â„“ 0 - 1 tree are almost imperceptibly different from those of the NN , despite their quantitative performance ( e . g . ORR ) differing . This underlines the importance of joint quantitative - qualitative evaluation . For Chase ( b ) , the â„“ 0 - 1 tree has clearly learnt the most safety - critical aspect of the task , which is to keep the agent above the altitude threshold alt < 50 , below which the oracle reward is strongly negative . The threshold is violated in only eight of 500 trajectories ( 1 . 6 % ) . Further evidence that the altitude threshold has been learnt correctly is discussed in Section VII . D . For Land , the â„“ 0 - 1 tree replicates the oracle in producing a gradual reduction in alt ( c ) while usually keeping pitch close to 0 ( d ) , although the spread of roll values is somewhat wider . In contrast , the agent using the variance - based tree for Follow sometimes fails to reach the target position ( e ; red trajectories ) , and also does not reliably straighten up to reduce up error ( f ) . For Chase , the altitude threshold does not appear to have been learnt precisely , and lower - altitude trajectories often fail to close the distance to RJ ( g and h ; red trajectories ) . For Land , the variance - based tree gives a later and less smooth descent ( i ) , and less consistent pitch control ( j ) , than the NN or â„“ 0 - 1 - based tree , although all models produce a somewhat higher altitude profile than the oracle . C . Sensitivity Analysis It is important to consider how learning performance degrades with reduced or corrupted data . In Figure 6 , we evaluate the effect of varying the number of preferences ğ¾ max ( with fixed ğ‘ max = 200 ) and trajectories ğ‘ max ( with fixed ğ¾ max = 1000 ) on reward learning with NNs and â„“ 0 - 1 - splitting trees . Following Lee et al . [ 10 ] , we also create more human - like preference data via two modes of oracle irrationality : preference noise ( by using a nonzero Bradley - Terry temperature ğ›½ to give a desired error rate on the coverage datasets ) and a myopic recency bias ( by exponentially discounting earlier timesteps when evaluating trajectory returns ) . We run five repeats for all cases , and report the medians and interquartile ranges of ORR ( lower is better ) and rank correlation ( closer to 1 is better ) . Both NN and tree models exhibit good robustness with respect to all four parameters . Although NNs remain superior in most cases , the gap varies , and is often reduced compared to the base cases ( bold labels ) . The budget sensitivity is low , with little improvement for ğ¾ max > 1000 and ğ‘ max > 200 , and no major drop even with 25 % of the data as the base case . For all tasks , the oracle error probability can increase to around 20 % before significant drops in performance are observed . These are promising indicators of the transferability of reward tree learning to limited and imperfect human data . Another general observation is that the trends for trees are somewhat smoother than for NNs , with fewer sharp jumps and fewer instances of very high spread across the five repeats . In the right column ( a ) , we summarise these results by taking the difference between the NN and tree metrics , and averaging across the three tasks . In all cases aside from rank correlation with ğ›½ > 0 , the NN - tree gap tends to become more favourable to the trees as the varied parameter becomes more challenging ( top - to - bottom ) . This sensitivity analysis thus indicates that reward trees are at least as robust to difficult learning scenarios as NNs , and may even be slightly more so . This is another promising result for the viability of reward tree learning from real human preferences . 10 O r a c l e m y o p i a ( ) Chase Follow P r e f e r e n c e b u d g e t ( ) T r a j e c t o r y b u d g e t ( ) NN Tree ( 0 - 1 ) Land 1000 20004000 500250 200 400800 10050 0 . 98 0 . 2 0 . 3 0 . 4 0 0 . 950 . 90 . 8 0 . 1 0 . 45 O r a c l e e rr o r p r o b a b i l i t y ( v i a ) 1 ORR 0 0 . 5 1 Rank correlation 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 10 0 . 5 1 Average NN - Tree Gap NNbetter - 0 . 25 0 0 . 25 - 0 . 25 0 0 . 25 ORR ORR ORR Treebetter Treebetter NNbetter a Rank correlation Rank correlation Rank correlation Fig . 6 Comparative sensitivity analysis of reward learning with NNs and trees . D . Tree Structure Analysis We have shown that reward learning with â„“ 0 - 1 - based trees can be competitive with NNs , but not quite as performant overall . We now turn to a concrete advantage which may tip practical trade - offs in its favour : the ability to interpret the learnt model , and analyse how its predictions relate to the underlying preference graph . In Figure 7 we opt for depth over breadth , and focus on the single best tree by ORR on the Chase task . The figure is divided into sections ( a â€“ d ) : ( a ) This reward tree has 17 leaves . The oracle reward , printed below , uses four features , all of which are used in the tree in ways that are broadly aligned ( e . g . lower los error leads to leaves with higher reward ) . The model has learnt the crucial threshold alt < 50 , correctly assigning a low reward when it is crossed . This explains why we observe rare violations of the altitude threshold in Figure 5 . However , it has not learnt the ideal distance to RJ , dist = 20 , with 43 . 3 being the lowest value used in a rule . This could be because the underlying preference graph lacks sufficient preferences to make this distinction ; adopting an active querying scheme may help to discover such subtleties efficiently . Other features besides those used by the oracle are present in the tree , indicating some causal confusion [ 67 ] . This may not necessarily harm agent performance , as it could provide beneficial shaping ( e . g . penalising positive closing speed , which indicates increasing distance to RJ ) . That may indeed be the case for this model since ORR is actually negative . ( b ) We plot the treeâ€™s predicted reward against the oracle reward for all timesteps in the online trajectories ( correlation = 0 . 903 ) . The predictions for each leaf lie along a horizontal line . Most leaves , including 1 and 2 , are well - aligned on this data because their oracle reward distributions are tightly concentrated around low / high averages respectively ( note that the absolute scale is irrelevant here ) . Leaf 16 has a wider oracle reward distribution , with several negative outliers . An optimal tree would likely split this leaf further , perhaps using the alt < 50 threshold . The one anomaly is leaf 13 , which contains just a single timestep from ğœ‰ 77 . This trajectory is the eighth best in the dataset by oracle return , but this leaf assigns that credit to a state that seemingly does not merit it , as the distance to RJ is so high ( dist > 73 ) . This may be an example of suboptimal reward learning , but the fact that its origin can be pinpointed so precisely is a testament to the value of interpretability . ( c ) We leverage the tree structure to produce a human - readable explanation of reward predictions for a single trajectory , which may be of value to an end user ( e . g . a trainee pilot seeking to understand the strengths and weaknesses of their own performance ) . We consider ğœ‰ 191 , a rare case that violates the altitude threshold . The time series of reward shows that the 20 timesteps are spent in leaves 16 , 15 , 11 and 7 . Rescaled oracle rewards are overlaid in teal , and show that the modelâ€™s predictions are well - aligned . To the right , we translate this visualisation into a textual form , similar to a nested program . Read top - to - bottom , the text indicates which rules of the tree are active at each timestep , and the effect this has on the predicted reward . This trajectory starts fairly positively , with reward gradually increasing over the first 16 timesteps as dist is reduced to between 43 . 3 and 73 , but then falls dramatically when the alt < 50 threshold is crossed . We are unaware of any method that could extract such a compact explanation of sequential predictions from an NN . ( d ) We isolate a subtree , starting at the root node , that splits only on dist and alt . We give a spatial representation 11 r = 2 . 48 - - - - - dist â‰¥ 43 . 3 ? r = 3 . 12 - - - - - alt â‰¥ 50 . 2 ? No r = 2 . 15 - - - - - dist â‰¥ 95 . 4 ? Yes ( 1 ) r = 0 . 937 No r = 3 . 17 - - - - - los error â‰¥ 1 . 10 ? Yes r = 3 . 22 - - - - - roll error â‰¥ 0 . 847 ? No r = 3 . 04 - - - - - abs roll â‰¥ 1 . 56 ? Yes ( 2 ) r = 3 . 35 No ( 3 ) r = 3 . 18 Yes r = 3 . 14 - - - - - closing speed â‰¥ 7 . 63 ? No ( 6 ) r = 2 . 92 Yes ( 4 ) r = 3 . 16 No ( 5 ) r = 2 . 47 Yes r = 2 . 31 - - - - - alt â‰¥ 50 . 0 ? No r = 1 . 72 - - - - - closing speed â‰¥ 8 . 34 ? Yes ( 7 ) r = 0 . 763 No r = 2 . 34 - - - - - dist â‰¥ 73 . 0 ? Yes r = 2 . 47 - - - - - abs roll â‰¥ 0 . 744 ? No r = 2 . 22 - - - - - alt error â‰¥ 2 . 14 ? Yes r = 2 . 72 - - - - - dist â‰¥ 68 . 7 ? No r = 2 . 37 - - - - - delta hdg error â‰¥ - 3 . 68 ? Yes ( 8 ) r = 2 . 78 No ( 9 ) r = 2 . 45 Yes ( 10 ) r = 3 . 20 No ( 11 ) r = 2 . 37 Yes ( 12 ) r = 2 . 76 No r = 2 . 21 - - - - - los error â‰¥ 0 . 0126 ? Yes ( 13 ) r = 3 . 92 No r = 2 . 21 - - - - - thrust â‰¥ 3 . 54 ? Yes ( 14 ) r = 1 . 70 No ( 15 ) r = 2 . 21 Yes ( 16 ) r = 1 . 76 No ( 17 ) r = 0 . 78 Yes Intermediate reward for non - leaf node by timestep - weighted average Oracle reward : a d r = 2 . 48 - - - - - dist â‰¥ 43 . 3 ? r = 3 . 12 - - - - - No r = 2 . 15 - - - - - dist â‰¥ 95 . 4 ? Yes ( 1 ) r = 0 . 937 No ( 2 ) r = 3 . 17 Yes r = 2 . 31 - - - - - alt â‰¥ 50 . 0 ? No ( 6 ) r = 1 . 72 Yes ( 3 ) r = 0 . 763 No r = 2 . 34 - - - - - dist â‰¥ 73 . 0 ? Yes ( 4 ) r = 2 . 47 No ( 5 ) r = 2 . 22 Yes 0 43 . 3 95 . 4 150 dist 0 350 a l t 50 73 alt â‰¥ 50 . 2 ? Total : 30 timesteps 1 2 4 5 6 3 1 and Predicted reward 0 4 . 45 ( 13 ) r = 3 . 92 ( 2 ) r = 3 . 35 ( 16 ) r = 1 . 76 ( 1 ) r = 0 . 937 , t = 2 ( 16 ) ( 15 ) ( 11 ) ( 7 ) 1 4 P r e d i c t e d r e w a r d Timestep 1 20 c Oracle reward ( rescaled ) - 300 - 200 - 100 0 1 4 Oracle reward P r e d i c t e d r e w a r d T i m e s t e p Trajectory return estimate 0 89 b Fig . 7 Analysis of a reward tree learnt for the Chase task . of the subtree , and how it is populated by the 200 online trajectories , using a 2D partition plot analogous to those in Figure 2 . Zooming into leaf 1 , which covers cases where the altitude threshold is violated , we see that it contains a total of 30 timesteps across four trajectories . By Equation 5 , the low reward for this leaf results from a weighted average of the return estimates for these four trajectories , which in turn ( by Equation 4 ) are derived from the preference graph . We can use this inverse reasoning to explain why this leaf has much lower reward than its sibling ( leaf 2 of the subtree ) . A proximal explanation comes by filtering the graph for preferences that specifically compare trajectories that visit those two leaves . 49 such preferences exist , and in all cases , the oracle prefers the trajectory that does not visit leaf 1 . Some of these preferences may be more practically salient than others . For example , we might highlight trajectories that feature more than once ( e . g . ğœ‰ 28 is preferred to both ğœ‰ 18 and ğœ‰ 48 ) , or cases where trajectories with low overall return estimates are nonetheless preferred to those in leaf 1 ( e . g . ğœ‰ 43 â‰» ğœ‰ 21 and ğœ‰ 56 â‰» ğœ‰ 47 ) . This ability to trace a reward treeâ€™s predictions back to the individual preferences that influence them could be valuable for verification and active learning . VIII . Conclusions and Future Work Fast jet handling is an excellent case study of both the value and difficulty of distilling tacit human expertise into software systems . In this work , we proposed a preference - based reward learning framework , which yields both a quantitative model of human preferences with a readable tree structure , and an artificial demonstrator agent capable of executing high - quality flight trajectories with respect to that model . The intrinsic interpretability of the reward tree model enables improved insight into the structure of expert preferences , and verification of agent behaviour , compared with standard NN - based approaches . Through oracle experiments on several aircraft handling tasks , we showed that reward trees with around 20 leaves can achieve quantitative and qualitative performance close to that of NNs , with a more direct split criterion bringing consistent benefits . We found that the NN - tree gap reduces as the true reward becomes more nonlinear , and remains stable or reduces further in the presence of limited or corrupted data . Although our experiments used synthetic oracle data to enable scalable quantitative evaluation , future work should include studies using the preferences of real human experts in the aircraft handling domain . It should also be noted that any realistic preference elicitation scenario would likely involve multiple experts with differing knowledge and expertise . A natural extension of our approach , which we see as valuable future work , is to learn individual reward functions for each expert , then leverage the intrinsic interpretability to identify biases , inconsistencies and trade - offs . This suggests a further application of reward tree learning : providing a basis for evaluating and training the experts themselves . 12 Appendix Table 2 List of features used by oracles and reward learning models . Apart from those containing â€œ delta " or â€œ rate " , all features are computed over the successor state for each transition , ğ‘  ğ‘¡ + 1 . dist Euclidean distance between EJ and RJ closing speed Closing speed between EJ and RJ ( negative = moving closer ) alt Altitude of EJ alt error Difference in altitude between EJ and RJ ( negative = EJ is lower ) delta alt error Change in alt error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 dist hor Euclidean distance between EJ and RJ in horizontal plane delta dist hor Change in dist hor between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 ( negative = moving closer ) pitch error Absolute difference in pitch angle between EJ and RJ delta pitch error Change in pitch error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 abs roll Absolute roll angle of EJ roll error Absolute difference in roll angle between EJ and RJ delta roll error Change in roll error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 hdg error Absolute difference in heading angle between EJ and RJ delta hdg error Change in hdg error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 fwd error Angle between 3D vectors indicating forward axes of EJ and RJ delta fwd error Change in fwd error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 up error Angle between 3D vectors indicating upward axes of EJ and RJ delta up error Change in up error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 right error Angle between 3D vectors indicating rightward axes of EJ and RJ delta right error Change in right error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 los error Angle between forward axis of EJ and vector from EJ to RJ ( measures whether RJ is in EJâ€™s line of sight ) delta los error Change in los error between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 abs lr offset Magnitude of projection of vector from EJ to RJ onto RJâ€™s rightward axis ( measures left - right offset between the two aircraft in RJâ€™s reference frame ) speed Airspeed of EJ g force Instantaneous g - force experienced by EJ pitch rate Absolute change of EJ pitch between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 roll rate Absolute change of EJ roll between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 yaw rate Absolute change of EJ yaw between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 thrust Instantaneous thrust output by EJ engines delta thrust Absolute change in thrust between ğ‘  ğ‘¡ and ğ‘  ğ‘¡ + 1 Acknowledgments This work was supported by a Thales / EPSRC Industrial Case Award in autonomous systems . The aircraft simulator used in experiments was based on an initial implementation by Ian Henderson at Thales UK , and experimental tasks were developed in consultation with both Ian Henderson and Rachel Craddock , also at Thales UK . 13 References [ 1 ] Sternberg , R . J . , and Horvath , J . A . , Tacit knowledge in professional practice : Researcher and practitioner perspectives , Psychology Press , 1999 . [ 2 ] Rudin , C . , â€œStop explaining black box machine learning models for high stakes decisions and use interpretable models instead , â€ Nature Machine Intelligence , Vol . 1 , No . 5 , 2019 , pp . 206 â€“ 215 . [ 3 ] Brunton , S . L . , Nathan Kutz , J . , Manohar , K . , Aravkin , A . Y . , Morgansen , K . , Klemisch , J . , Goebel , N . , Buttrick , J . , Poskin , J . , Blom - Schieber , A . W . , et al . , â€œData - driven aerospace engineering : reframing the industry with machine learning , â€ AIAA Journal , Vol . 59 , No . 8 , 2021 , pp . 2820 â€“ 2847 . [ 4 ] Bewley , T . , and Lecue , F . , â€œInterpretable Preference - based Reinforcement Learning with Tree - Structured Reward Functions , â€ Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems , 2022 , pp . 118 â€“ 126 . [ 5 ] Christiano , P . F . , Leike , J . , Brown , T . , Martic , M . , Legg , S . , and Amodei , D . , â€œDeep reinforcement learning from human preferences , â€ Advances in Neural Information Processing Systems , Vol . 30 , 2017 . [ 6 ] Lee , K . , Smith , L . M . , and Abbeel , P . , â€œPEBBLE : Feedback - Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre - training , â€ International Conference on Machine Learning , PMLR , 2021 , pp . 6152 â€“ 6163 . [ 7 ] Griffith , S . , Subramanian , K . , Scholz , J . , Isbell , C . L . , and Thomaz , A . L . , â€œPolicy shaping : Integrating human feedback with reinforcement learning , â€ Advances in neural information processing systems , Vol . 26 , 2013 . [ 8 ] Reddy , S . , Dragan , A . , Levine , S . , Legg , S . , and Leike , J . , â€œLearning human objectives by evaluating hypothetical behavior , â€ International Conference on Machine Learning , PMLR , 2020 , pp . 8020 â€“ 8029 . [ 9 ] Lindner , D . , Turchetta , M . , Tschiatschek , S . , Ciosek , K . , and Krause , A . , â€œInformation Directed Reward Learning for Reinforcement Learning , â€ Advances in Neural Information Processing Systems , Vol . 34 , 2021 , pp . 3850 â€“ 3862 . [ 10 ] Lee , K . , Smith , L . , Dragan , A . , and Abbeel , P . , â€œB - Pref : Benchmarking Preference - Based Reinforcement Learning , â€ Advances in Neural Information Processing Systems , Vol . 35 , 2021 . [ 11 ] Sutton , R . S . , and Barto , A . G . , Reinforcement learning : An introduction , MIT press , 2018 . [ 12 ] Azar , A . T . , Koubaa , A . , Ali Mohamed , N . , Ibrahim , H . A . , Ibrahim , Z . F . , Kazim , M . , Ammar , A . , Benjdira , B . , Khamis , A . M . , Hameed , I . A . , et al . , â€œDrone deep reinforcement learning : A review , â€ Electronics , Vol . 10 , No . 9 , 2021 , p . 999 . [ 13 ] Liu , H . , Kiumarsi , B . , Kartal , Y . , Koru , A . T . , Modares , H . , and Lewis , F . L . , â€œReinforcement learning applications in unmanned vehicle control : A comprehensive overview , â€ Unmanned Systems , 2022 , pp . 1 â€“ 10 . [ 14 ] Razzaghi , P . , Tabrizian , A . , Guo , W . , Chen , S . , Taye , A . , Thompson , E . , Bregeon , A . , Baheri , A . , and Wei , P . , â€œA Survey on Reinforcement Learning in Aviation Applications , â€ arXiv preprint arXiv : 2211 . 02147 , 2022 . [ 15 ] Tang , C . , and Lai , Y . - C . , â€œDeep reinforcement learning automatic landing control of fixed - wing aircraft using deep deterministic policy gradient , â€ 2020 International Conference on Unmanned Aircraft Systems ( ICUAS ) , IEEE , 2020 , pp . 1 â€“ 9 . [ 16 ] Clarke , S . G . , and Hwang , I . , â€œDeep reinforcement learning control for aerobatic maneuvering of agile fixed - wing aircraft , â€ AIAA Scitech 2020 Forum , 2020 , p . 0136 . [ 17 ] Morales , E . F . , and Sammut , C . , â€œLearning to fly by combining reinforcement learning with behavioural cloning , â€ Proceedings of the twenty - first international conference on Machine learning , 2004 , p . 76 . [ 18 ] Cao , S . , Wang , X . , Zhang , R . , Yu , H . , and Shen , L . , â€œFrom Demonstration to Flight : Realization of Autonomous Aerobatic Maneuvers for Fast , Miniature Fixed - Wing UAVs , â€ IEEE Robotics and Automation Letters , Vol . 7 , No . 2 , 2022 , pp . 5771 â€“ 5778 . [ 19 ] Yildiz , Y . , Agogino , A . , and Brat , G . , â€œPredicting pilot behavior in medium - scale scenarios using game theory and reinforcement learning , â€ Journal of Guidance , Control , and Dynamics , Vol . 37 , No . 4 , 2014 , pp . 1335 â€“ 1343 . [ 20 ] Vemuru , K . V . , Harbour , S . D . , and Clark , J . D . , â€œReinforcement Learning in Aviation , Either Unmanned or Manned , with an Injection of AI , â€ 20th International Symposium on Aviation Psychology , 2019 , p . 492 . [ 21 ] van OÄ³en , J . , Poppinga , G . , Brouwer , O . , Aliko , A . , and Roessingh , J . J . , â€œTowards modeling the learning process of aviators using deep reinforcement learning , â€ 2017 IEEE International Conference on Systems , Man , and Cybernetics ( SMC ) , IEEE , 2017 , pp . 3439 â€“ 3444 . 14 [ 22 ] Russell , S . , Human compatible : Artificial intelligence and the problem of control , Penguin , 2019 . [ 23 ] Leike , J . , Krueger , D . , Everitt , T . , Martic , M . , Maini , V . , and Legg , S . , â€œScalable agent alignment via reward modeling : a research direction , â€ arXiv preprint arXiv : 1811 . 07871 , 2018 . [ 24 ] Ng , A . Y . , Russell , S . , et al . , â€œAlgorithms for inverse reinforcement learning . â€ Icml , Vol . 1 , 2000 , p . 2 . [ 25 ] Knox , W . B . , and Stone , P . , â€œTamer : Training an agent manually via evaluative reinforcement , â€ 2008 7th IEEE international conference on development and learning , IEEE , 2008 , pp . 292 â€“ 297 . [ 26 ] Bajcsy , A . , Losey , D . P . , Oâ€™Malley , M . K . , and Dragan , A . D . , â€œLearning robot objectives from physical human interaction , â€ Conference on Robot Learning , PMLR , 2017 , pp . 217 â€“ 226 . [ 27 ] Wirth , C . , FÃ¼rnkranz , J . , and Neumann , G . , â€œModel - free preference - based reinforcement learning , â€ Thirtieth AAAI Conference on Artificial Intelligence , 2016 . [ 28 ] Sadigh , D . , Dragan , A . D . , Sastry , S . , and Seshia , S . A . , â€œActive preference - based learning of reward functions , â€ Proceedings of Robotics : Science and Systems ( RSS ) , 2017 . [ 29 ] Cao , Z . , Wong , K . , and Lin , C . - T . , â€œWeak human preference supervision for deep reinforcement learning , â€ IEEE Transactions on Neural Networks and Learning Systems , Vol . 32 , No . 12 , 2021 , pp . 5369 â€“ 5378 . [ 30 ] Kendall , M . , â€œRank Correlation Methods ; Griffin , C . , Ed , â€ , 1975 . [ 31 ] Wilde , N . , Blidaru , A . , Smith , S . L . , and KuliÄ‡ , D . , â€œImproving user specifications for robot behavior through active preference learning : Framework and evaluation , â€ The International Journal of Robotics Research , Vol . 39 , No . 6 , 2020 , pp . 651 â€“ 667 . [ 32 ] Guo , Y . , Tian , P . , Kalpathy - Cramer , J . , Ostmo , S . , Campbell , J . P . , Chiang , M . F . , Erdogmus , D . , Dy , J . G . , and Ioannidis , S . , â€œExperimental Design under the Bradley - Terry Model . â€ Ä²CAI , 2018 , pp . 2198 â€“ 2204 . [ 33 ] Ibarz , B . , Leike , J . , Pohlen , T . , Irving , G . , Legg , S . , and Amodei , D . , â€œReward learning from human preferences and demonstrations in Atari , â€ Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 34 ] BÄ±yÄ±k , E . , Losey , D . P . , Palan , M . , Landolfi , N . C . , Shevchuk , G . , and Sadigh , D . , â€œLearning reward functions from diverse sources of human feedback : Optimally integrating demonstrations and preferences , â€ The International Journal of Robotics Research , Vol . 41 , No . 1 , 2022 , pp . 45 â€“ 67 . [ 35 ] Puiutta , E . , and Veith , E . , â€œExplainable reinforcement learning : A survey , â€ International cross - domain conference for machine learning and knowledge extraction , Springer , 2020 , pp . 77 â€“ 95 . [ 36 ] Heuillet , A . , Couthouis , F . , and DÃ­az - RodrÃ­guez , N . , â€œExplainability in deep reinforcement learning , â€ Knowledge - Based Systems , Vol . 214 , 2021 , p . 106685 . [ 37 ] Zhu , G . , Huang , Z . , and Zhang , C . , â€œObject - oriented dynamics predictor , â€ Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 38 ] Verma , A . , Murali , V . , Singh , R . , Kohli , P . , and Chaudhuri , S . , â€œProgrammatically interpretable reinforcement learning , â€ International Conference on Machine Learning , PMLR , 2018 , pp . 5045 â€“ 5054 . [ 39 ] Zahavy , T . , Ben - Zrihem , N . , and Mannor , S . , â€œGraying the black box : Understanding dqns , â€ International conference on machine learning , PMLR , 2016 , pp . 1899 â€“ 1908 . [ 40 ] Huber , T . , Schiller , D . , and AndrÃ© , E . , â€œEnhancing explainability of deep reinforcement learning through selective layer - wise relevance propagation , â€ Joint German / Austrian Conference on Artificial Intelligence ( KÃ¼nstliche Intelligenz ) , Springer , 2019 , pp . 188 â€“ 202 . [ 41 ] van der Waa , J . , van Diggelen , J . , Bosch , K . v . d . , and Neerincx , M . , â€œContrastive explanations for reinforcement learning in terms of expected consequences , â€ Ä²CAI / ECAI Workshop on Explainable Artificial Intelligence , 2018 . [ 42 ] Amir , D . , and Amir , O . , â€œHighlights : Summarizing agent behavior to people , â€ Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems , 2018 . [ 43 ] Huang , S . H . , Bhatia , K . , Abbeel , P . , and Dragan , A . D . , â€œEstablishing appropriate trust via critical states , â€ 2018 IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) , IEEE , 2018 , pp . 3929 â€“ 3936 . 15 [ 44 ] Dao , G . , Mishra , I . , and Lee , M . , â€œDeep reinforcement learning monitor for snapshot recording , â€ 2018 17th IEEE International Conference on Machine Learning and Applications ( ICMLA ) , IEEE , 2018 , pp . 591 â€“ 598 . [ 45 ] Bewley , T . , Lawry , J . , and Richards , A . , â€œSummarising and Comparing Agent Dynamics with Contrastive Spatiotemporal Abstraction , â€ Ä²CAI / ECAI Workshop on Explainable Artificial Intelligence , 2022 . [ 46 ] Glanois , C . , Weng , P . , Zimmer , M . , Li , D . , Yang , T . , Hao , J . , and Liu , W . , â€œA Survey on Interpretable Reinforcement Learning , â€ arXiv preprint arXiv : 2112 . 13112 , 2021 . [ 47 ] Juozapaitis , Z . , Koul , A . , Fern , A . , Erwig , M . , and Doshi - Velez , F . , â€œExplainable reinforcement learning via reward decomposition , â€ Ä²CAI / ECAI Workshop on Explainable Artificial Intelligence , 2019 . [ 48 ] Devidze , R . , Radanovic , G . , Kamalaruban , P . , and Singla , A . , â€œExplicable reward design for reinforcement learning agents , â€ Advances in Neural Information Processing Systems , Vol . 34 , 2021 , pp . 20118 â€“ 20131 . [ 49 ] Russell , J . , and Santos , E . , â€œExplaining reward functions in Markov decision processes , â€ The Thirty - Second International Flairs Conference , 2019 . [ 50 ] Michaud , E . J . , Gleave , A . , and Russell , S . , â€œUnderstanding learned reward functions , â€ arXiv preprint arXiv : 2012 . 05862 , 2020 . [ 51 ] Jenner , E . , and Gleave , A . , â€œPreprocessing Reward Functions for Interpretability , â€ arXiv preprint arXiv : 2203 . 13553 , 2022 . [ 52 ] Chapman , D . , and Kaelbling , L . P . , â€œInput Generalization in Delayed Reinforcement Learning : An Algorithm and Performance Comparisons . â€ Ijcai , Vol . 91 , 1991 , pp . 726 â€“ 731 . [ 53 ] DÅ¾eroski , S . , Raedt , L . D . , and Blockeel , H . , â€œRelational reinforcement learning , â€ International Conference on Inductive Logic Programming , Springer , 1998 , pp . 11 â€“ 22 . [ 54 ] Pyeatt , L . D . , â€œReinforcement learning with decision trees . â€ 21 st IASTED International Multi - Conference on Applied Informatics , 2003 , pp . 26 â€“ 31 . [ 55 ] Silva , A . , Gombolay , M . , Killian , T . , Jimenez , I . , and Son , S . - H . , â€œOptimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning , â€ Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics , Vol . 108 , PMLR , 2020 , pp . 1855 â€“ 1865 . [ 56 ] Liu , G . , Schulte , O . , Zhu , W . , and Li , Q . , â€œToward interpretable deep reinforcement learning with linear model u - trees , â€ Joint European Conference on Machine Learning and Knowledge Discovery in Databases , Springer , 2018 , pp . 414 â€“ 429 . [ 57 ] Jiang , W . - C . , Hwang , K . - S . , and Lin , J . - L . , â€œAn experience replay method based on tree structure for reinforcement learning , â€ IEEE Transactions on Emerging Topics in Computing , Vol . 9 , No . 2 , 2019 , pp . 972 â€“ 982 . [ 58 ] Bastani , O . , Pu , Y . , and Solar - Lezama , A . , â€œVerifiable Reinforcement Learning via Policy Extraction , â€ Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 59 ] Cobo , L . C . , Isbell Jr , C . L . , and Thomaz , A . L . , â€œAutomatic task decomposition and state abstraction from demonstration , â€ Georgia Institute of Technology , 2012 . [ 60 ] Tambwekar , P . , Silva , A . , Gopalan , N . , and Gombolay , M . , â€œSpecifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning , â€ arXiv preprint arXiv : 2101 . 07140 , 2021 . [ 61 ] Sheikh , H . U . , Khadka , S . , Miret , S . , Majumdar , S . , and Phielipp , M . , â€œLearning intrinsic symbolic rewards in reinforcement learning , â€ 2022 International Joint Conference on Neural Networks ( Ä²CNN ) , IEEE , 2022 , pp . 1 â€“ 8 . [ 62 ] Bradley , R . A . , and Terry , M . E . , â€œRank analysis of incomplete block designs : I . The method of paired comparisons , â€ Biometrika , Vol . 39 , No . 3 / 4 , 1952 , pp . 324 â€“ 345 . [ 63 ] SuÃ¡rez , A . , and Lutsko , J . F . , â€œGlobally optimal fuzzy decision trees for classification and regression , â€ IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol . 21 , No . 12 , 1999 , pp . 1297 â€“ 1311 . [ 64 ] Gulliksen , H . , â€œA least squares solution for paired comparisons with incomplete data , â€ Psychometrika , Vol . 21 , No . 2 , 1956 , pp . 125 â€“ 134 . [ 65 ] Kingma , D . P . , and Ba , J . , â€œAdam : A method for stochastic optimization , â€ arXiv preprint arXiv : 1412 . 6980 , 2014 . [ 66 ] Breiman , L . , Friedman , J . H . , Olshen , R . A . , and Stone , C . J . , Classification and regression trees , Routledge , 2017 . 16 [ 67 ] Tien , J . , He , J . Z . - Y . , Erickson , Z . , Dragan , A . D . , and Brown , D . , â€œA Study of Causal Confusion in Preference - Based Reward Learning , â€ arXiv preprint arXiv : 2204 . 06601 , 2022 . [ 68 ] Haarnoja , T . , Zhou , A . , Abbeel , P . , and Levine , S . , â€œSoft actor - critic : Off - policy maximum entropy deep reinforcement learning with a stochastic actor , â€ International Conference on Machine Learning , PMLR , 2018 , pp . 1861 â€“ 1870 . [ 69 ] Rahtz , M . , Varma , V . , Kumar , R . , Kenton , Z . , Legg , S . , and Leike , J . , â€œSafe Deep RL in 3D Environments using Human Feedback , â€ arXiv preprint arXiv : 2201 . 08102 , 2022 . [ 70 ] Chua , K . , Calandra , R . , McAllister , R . , and Levine , S . , â€œDeep reinforcement learning in a handful of trials using probabilistic dynamics models , â€ Advances in Neural Information Processing Systems , Vol . 31 , 2018 . [ 71 ] Gleave , A . , Dennis , M . D . , Legg , S . , Russell , S . , and Leike , J . , â€œQuantifying Differences in Reward Functions , â€ International Conference on Learning Representations , 2021 . [ 72 ] Kendall , M . G . , â€œA new measure of rank correlation , â€ Biometrika , Vol . 30 , No . 1 / 2 , 1938 , pp . 81 â€“ 93 . 17