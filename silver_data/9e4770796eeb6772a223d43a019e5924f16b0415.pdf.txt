Parallel Algorithms and Architectures for Rule - Based Systems Anoop Gupta , Charles Forgy , Allen Newell , and Roberl Wedig Carnegie - Mellon University Pittsburgh , Pennsylvania 15213 Abstract Rule - bascd systems , on the surfacc , appear to be capable of exploit - ing large amounts of parallelism - - it is possible to match each rule to the data memory in parallel . In practice , however , we show that the speed - up from parallelism is quite linfited , less than 10 - fold . The reasons for the small speed - up are : ( t ) the small number of rules relevant to each change to data memory ; ( 2 ) the large variation in the processing required by the relevant rules : and ( 3 ) the small number of changes made to data mcmory between synchronization steps . Furthermore , we observc that to obtain this limited factor of 0 - fold speed - up , it is necessary to exploit parallelism at a very fine granularity . We propose that a suitable architecture to exploit such fine - grain parallelism is a bus - bascd shared - memory multiprocessor with 32 - 64 processors . Using such a multiprocessor ( with individual processors working at 2 MIPS ) , it is possible to obtain execution speeds of about 3800 rule - firings / see . " Ibis speed is significantly higher than that obtained by other proposed parallel implemen - tations of rule - based systems . 1 . Introduction Rule - based systems are a widely used product of research in Artificial Intelligcnce . Thcy have bccn used for designing computer systems 16 , VLSI routing 13 , computer configuration 21 , medi - cal consultation 33 , and numerous other applications . This ever increasing application domain , especially to real - time applications 7 , has resulted in the need for machines that can execute rule - based systems at significantly higher speeds than that provided by existing implementations , Several research efforts are currently in progress to provide this extra speed up . While some research efforts concentrate on speeding up sequential implementations 18 , 26 , most of the efforts concentrate on the use of parallelism to achieve the speed up 10 , 23 , 28 , 29 , 32 , 34 . Research in the design and efficient implementation of rule - based systems ( also called production systems ) has been going on at Carnegie - Mellon University ( CMU ) since the very inception of rule - based systems 2 , 3 , 20 . As a result some of the largest rule - based systems ( a few of these systems are currently in production use in industry ) have been designed and built at CMU 14 , 16 , 19 , 21 , 30 . In this paper we present the results of recent studies exploring the use of parallclism in the implementation of rule - based systems . The studies are based on this large set of rule - based systems developed at CMU . We prescnt issues regarding ( 1 ) the choice of algorithms for parallel implementations and why the Rete - class of algorithms is highly s ' uitablc for parallel implemen - tation ( Section 3 ) ; ( 2 ) the kinds of parallelism that may be exploited This research was sponsored by the l ' ) cfenso Advanced Research Projects Agency ( DOD ) , ARPA Ordcr No . 3597 . monitorod bythc Air Force Avionics Laboratol ) ' ulld ~ ; r Contr ~ t F33615 - 81 - K - 1539 . The views ; uad conclusions contained in this document are those of the authol ~ ~ md should not be interpreted as representing the official policies , either cxpre ~ ed or implied , of the Defense Advanced Re , arch Projects Ageng , v 0 ~ " the US GovemmcnL in the Rcte - class of algorithms and the associated architectural implications ( Sections 4 and 5 ) : ( 3 ) the performance that can be obtained ( Section 6 ) : ( 4 ) how the performance compares to other proposed architectures ( Section 7 ) : and ( 5 ) the sensitivity of the results to changes in the characteristics of role - based systems , that is , the stability of the results presented in this paper ( Section 8 ) . 2 . Background This section describes the syntactic arid semantic features of the OPS5 production system language - - the language for which paral - lelism is explored in the paper . It then goes on to describe the Rete match algorithm . The Rcte algorithm is used in existing uniproces - sor implementations of OPS5 , and , also forms the basis for the proposed parallel implementation . 2 . 1 . OPS5 An OPS5 l production system is composed of a set of t ~ then rules called productions that make up the production memory , and a database of assertions called the working memory . The assertions in the working memory are called working memory elements . Each production consists of a conjunction of condition elements cor - responding to the / . / ' part of the rule ( also called the left - hand side of the production ) , and a set of actions corresponding to the then part of the rule ( also called the right - hand side of the production ) . The actions associated with a production can add , remove or modify working memory elements , or perform input - output . Figure 2 - 1 shows a production named find - colored - blk with two condition ele - ments in its left - hand side and one action in its right - hand side . ( p f tnd - eolorod - blk ( goal * type find - blk * color < c > ) ( block rid < i > tcolor < c > * selected no ) ( modify 2 * selected yes ) ) Figure 2 - 1 : A sample production . The production system interpreter is the underlying mechanism that determines the set of ~ tisfied productions and controls the execution of the production system program . The interpreter e . x - . eeutes a production system program by performing the following recognize - act cycle : • Match : In this first phase , the left - hand sides of all produc - tions are matched against the contents ofworking memory . As a result a conflict set is obtained , which consists of instantiations of all , satisfied productions . An instantiation of a production is an ordered list of working memory elements that satisfies the left - hand side of the production . • Conflict - Resolution : In this ~ cond phase , one of the produc - tion instantiations in the conflict set is chosen for execution . If no productions are satisfied , the interpreter hails . • Act : In this third phase , the actioas of the production ~ lected 0884 - 7495 / 86 / 0000 / 0028501 . 00 © 1986 IEEE 28 in the conflict - resolution phase are executed , q ~ e . se actions may change the contents of working memory , At the end of this phase , the first phase is executed again . The recognize - act cycle fomls the basic control structure in production system programs . I ) uring the match phase the knowledge of the program ( represented by the production rules ) is tested for relevance against the existing problem state ( represented by the working memory ) . During the conflict - resolution phase the most promising piece of knowledge that is relevant is selected . During the act phase , the action recommended by the selected rule is applied to the existing problem state , resulting in a new problem state . A working memory element is a parenthesized list consisting of a constant s3 , mbol called the class of the ele ' l ~ nent and zero or more attributg - value pairs . The attribut ~ are ~ mbols that are preceded by the operator t . The values are ~ mbolic or numeric constants . For example , the following working memory element has class C1 , the value 12 for attribute attrl and the value 15 for attribute at / r2 . ( el tattrl 12 tattr2 15 ) The condition elements in the left - hand side of a production are parenthesized lists similar to the working memory elements . They may optionally be preceded by the symbol - . Such condition elements are called negated condition elements . Condition elements are interpreted as partial descriptions of working memory elements . When a condition element describes a working memory element , the working memory element is said to match the condition element . A production is said to be sati , ~ qed when : ( 1 ) For every non - negated condition element in the left - hand side of the production , there exists a working memory element that matches it ; ( 2 ) For every negated condition element in the left - hand side of the production , there does not exist a working memory element that matches it . Like a working memory element , a condition element contains a class name and a sequence of attribute - value pairs . However , the condition element is less restricted than the working memory de - ment ; while the working memory element can contain only constant symbols and numbers , the condition element can contain variables , predicate symbols , and a variety of other operators as well as constants . Variables are identifiers that begin with the character " < " and end with " > " ~ for example , < i > and < e > are variables . A working memory element matches a condition element if they belong to the same class and if the value of every attribute in the condition element matches the value of the corresponding attribute in the working mcmory element . The rules for determining whether a working memory element value matches a condition element value are : ( 1 ) If the condition element value is a constant , it matches only an identical constant . ( 2 ) If the condition element valtle is a variable , it will match any value . However , if a variable occurs more than once in a left - hand side , all occurrences of the variable must match identical values . ( 3 ) If the condition element value is preceded by a predicate symbol , the working memory element value must be related to the condition element value in the indicated way . The right - hand side of a production consists of an unconditional sequence of actions which can cause input - output , and which are responsible for changes to the working memory . Three kinds of actions are provided to effect working memory changes . Make creates a new working memory element and adds it to working memory . Modify changes one or more values of an existing working memory element . Remove deletes an element from the working memory . 2 , 2 . The Rete Match Algorithm The most time consuming flep in the execution of production systems is the match step . To get a feeling for the complexity of match , con , ~ ider a production system consisting of 1000 productions and 1000 working memory clements , where each production has three condition elements . In a naive implementation each produe - lion will have to be matched against all tuples of size three from the working metnory , leading to over a trillion ( 1000 * 10003 ) com - parison operations for each execution cycle . Of course , more complex algorithms can achieve the final result using a much smaller number of operations , but even with specialized algorithms , match constitutes around 90 % of the interpretation time . The match algorithm used by uniproeessor implementations of OPS5 is called Rete 3 . This section describes the Rete algorithm in some detail as it forms the basis for the parallel implementations discussed later . ( pot ( CI tattrl ( x > ? 81tr21 ~ Q ( pp2 ( C2 ? alb ' l 15 tatlr2 ( y ) ) ( C2 ram ' 1 15 tattr2 < x ) ) ( C4 vary ! ( y ) ) . ( ( : : 3 tjm ~ < x > ) . , ) ( modify 1 tl ~ l 12 ) ) ( remove a ) ) r o m . ~ L attr ' J . I : 1 am ' - S ass - = \ / - = = / / two . ~ ma ' 14 ~ _ I / \ 7 " - . / = I pl Conflict ~ 11 p2 I Figure Z - Z : The Rete network , The Rete algorithm uses a special kind of a data - fiow network compiled from the left - hand tides of the productions to perform the match . Rgure 2 - 2 shows such a network for productions pl and p2 which appear in the top part of the figure . The nodes in the network represent abstract operations to be performed during match and are interpreted at run - time by the OPS5 interpreter . The objects that are passed between nodes in the network are called tokens . Each token consists of a list of pointers to working memory elements that match a subsequence of condition elements in a leR - hand side . The network consists of four different kinds of nodes . These are : 1 . Constant - test nodes : These nodes are used to test if the attributes in the condition element which have a constant value are satisfied . These nodes appear in the top part of the Rete network . Note that when two left - hand sides require identical nodes , the compiler shares part of the network rather than building duplicate nodes . 2 . Memory nodes : These nodes store the result of matching from previous cycles as state within them . The state stored in a memory node consists of a list of the tokens that match a part of the left - hand side of the associated production . For ex - ample , the right - most memory node in Figure 2 - 2 gores all tokens corresponding to working memory elements with " Class = C4 " . 3 . Two - input nodes : These nodes test for joint satisfaction of condition elements in the left - hand side 9fa production . Both inputs of a two - input node come from memory nodes . When 29 a token arrives on the left input of a two - input node , it is compared to each token stored in the memory node connected to the right illpuL All token pairs thai have consistent variable bindings are sent to the successors of the two - input node . Similar action is taken when a token arrives on the right input of a two - input node . 4 . Terndnal nodes : There is one such node associated with each production in the program , as can be seen at bottom of I : igure 2 - 2 . Whenever a token flows into a terminal node , the corresponding production is either inserted into or deleted from the conflict set . To avoid perfotTning the same tests repeatedly , the Rete algo - rithm stores the re . suit of the match with working memory as state within the memory nodes . This way , only changes made to the working memory by the most recent production firing have to be processed every cycle . When changes are made to the working memory , they filter through the network updating the state stored within the network . The output of the network consists of a specification of changes to the conflict set . The performance of Rete - based interpreters for the OPS family of languages has steadily improved over the years . The most widely used Rete - basod interpreter is the OPS5 interpreter . The Lisp implementation of this interpreter runs at around 8 working memory changes per second ( wine - changes / see ) on a VAX - 11 / 780 , while the Bliss based implementation runs at around 40 wrne - changes / see . In the above two interpreters a significant loss in the speed is due to the interpretation overhead of nodes . In a later version of OPS 4 this overhead has been eliminated by compiling the network directly into machine code . While it is possible to ~ eape to the interpreter for complex operations during match or for setting UP the initial conditions for the match , the majority of the match is done without an intervening interpretation level . This has led to a large speed - up and the compiled OPS runs at around 200 wine - changes / see on the VAX - 11 / 780 . Some further optimizations to the OPS compiler have been designed which would permit it to run at around 400 - 800 wine - changes / see . The aim of the parallel implementations is to take the performance still higher , in the range of 5000 - 10000 wine - changes / see . It is expected that this order of magnitude increase in speed over the best possible uniprocessor interpreters will open new application and research areas that could not be addressed before by production systems . 3 . Why Parallelize Rete ? While being an extremely efficient algorithm for match on uniprocessors , Rete is also an effective algorithm for match on parallel processors . This section discusses some of the motivations for studying Rete and the reasons why it is an appropriate algorithm for parallel implementations . 3 . 1 . State - Saving vs . Non State . Saving Match Algorithms It is possible to divide the set of match algorithms for production systems into two categories : ( 1 ) the state - saving algorithms and ( 2 ) the non state - saving algorithms . The state - saving algorithms store the results of executing match from previous recognize - act cycles , so that only the changes made to the working memory by the most recent production firing need be processed every cycle . In contrast , the non state - saving algorithms start from scratch every time , that is , they match the complete working memory against all the produc - tions on each cycle . In a state - saving algorithm the work done includes two steps : ( 1 ) compuling the fresh state corresponding to the newly inserted working memory elements and storing the fresh state in appropriate data structures : ( 2 ) identi lying the state corresponding to the deleted working memory elemenls and deleting this state from the data structures storing it . In a non state - saving Mgorithm the work done includes only one step , that of computing the state lbr the match between the complete working memory and all tile productions . ( Note that this may involve temporarily storing some of the partial state that is generated . ) In both state - saving . ' rod non state - ~ ving algorithms , state refers to the matches between condition elements and working memory elements that are computed as an intermediate step in the process of computing the match between the complete left - hand sides of productions and the working memory elements . Whether it is advantageous to store state depends on ( 1 ) the fraction of working memory that changes on each cycle , and ( 2 ) the amount of state that is stored by the state - saving algorithm . To evaluate the advantages and disadvantages more concretely , con - sider the following simple model . Consider a production system program for which the stable size of the working memory is & the average number of inserts to working memory on each cycle is I , and the average number of deletes from working memory is d . Let the cost of step - 1 ( as described in the previous paragraph ) for a single insert to working memory be c 1 and the cost for step - 2 for a single delete from working memory be c 2 , Further assume that the average cost of the temporary state computed and stored by the non - state saving algorithm is c 3 for each working memory element . Then the average cost per exc ~ : ution cycle of the state - saving algo - rithm is C . . = i . c I + d . c 2 . The average cost per execution $ 10te - ~ rv 1 - - ri . , . cycle ofthe non - state saving aJgo mm is t . non . aate . m v = s . c 3 , To evaluate the advantages of the state - saving algorithm , let us analyze the inequality C . . . . . . . < C . . . . . . . . . . . . . . For the implemen - . , . o ~ ~ c - • lit / tl - olul ~ - ~ ¢ tatlons being considered ~ r i ~ e Rete algond ~ m , the cost of an msart to working memory is the same as the cost of a delete from the working memory . As a result , substituting c 1 = c 2 in the inequality , we get ( i + d ) / s < % / c I . ~ timates from simulaiions , for the Rete algorithm indicate - that c I is approximately equal to execution of 1800 machine - code instructions , and that c 3 is approximately equal to execution of 1100 machine - code instructions 10 . Using these estimates we get the condition that state - saving algorithms are more efficient when ( i + d ) / s < 0 . 61 , that is , state - saving algorithms are better if the number of insertions plus deletions per cycle is less than 61 % of the stable size of the working memory . Measurements 8 on several OPS5 programs show that the number of inserts plus deletes per cycle constitutes less than 0 . 5 % of the stable working memory size . Thus a non state - saving algorithm will have to recover an inefficiency factor of about 20 before it breaks even with a state - saving algorithm for OPS5 - 1ike production systems . 1 3 . 2 . Rete as a Specific Instance of State - Saving Algorithms While it is generally accepted that state - saving algorithms are suitable for OPS5 production systems , there is no consensus about the amount of state that such algorithms should store . Of course , there are many possibilities . This section discusses some of the schemes that various research groups have adopted , where the Pete algorithm fits amongst these schemes , and why Rete is interesting . One possible scheme that a state - saving algorithm may use is to lln case the values of c 1 . c 2 . and c 3 are estimated from a different base algorithm , the above numbers will change somewhat . 30 only s ' torc infomlation about matches betwecn individual condition elements and working memory elements . For example , consider a production with three condition elements CE1 , Ct ~ . and CE3 . Then the algorithm stores information about all working memory ele - mcnt . s that match CF . I , ' all working memory elements that match CE2 , and all working memory element . ~ that match CE3 . However . it does not store working memory tuples that satisfy CEI and CE2 together , or CI . 2 and CI . J together , and so on , This information is recomputed on each cycle . Such a ~ cheme is used by the TREAT algorithm developed for the DADO machine at Columbia Univer - sity 22 , This scheme stands at the low end of the spectrum of state - saving algorithms , in that it stores less . ~ , ate than most other algorithms . A problem with this scheme is that much of the state has to be rccomputed on each cycle , often with the effect of increasing the total time taken by the cycle . A second possible scheme that a state - saving algorithm may use is to store in fomlation about matches between all possible combina - tions of condition elements that occur in the left - hand side of a production and the sequences of working memory elements that satisfy them . For example , consider a production with three con - dition elements CE1 , CE2 , and CE3 . Then , in this scheme , the algorithm stores information about all working memory elements that match CE1 . CE2 , and CE3 individually , about all working memory element tuples that match CEI and CF . 2 together , CE2 and CE3 together , CE1 and CE3 together , and so on . Kemal Oflazer , in his thesis 24 , has proposed a variation of this scheme to implement a highly parallel algorithm for match . This scheme stands at the high end of the spectrum of state - saving algorithms , in that it gores almost all information known about the matches between the productions and the working memory . Two possible problems with this scheme are that ( 1 ) the state may become very large , and ( 2 ) the algorithm may spend a lot of time computing and deleting state that never really gets used . that is , it never results in a production entering or leaving the conflict set . The amount of state computed by the Rete algorithm falls in between that proposed by the previous two schemes . The Rete algorithm stores infom ' ~ ation about working memory elements that match individual condition elements , as proposed in the first scheme . In addition , it also steres information about tup | es of working memory elements that match some fixed combinations of condition elements occurring in the left - hand side of a production . This is in contrast to the second s ~ hem¢ , . where information is stored about tuples of working memory ' elements that match all comBina - tions of condition elements . TI ~ choice about the combinatkzns of condition dements for which match infoa ~ aa , ti ~ a is stored js fixed at compile time , and different choices result i ~ different variants of the Rete algorithm . For example , foir a production with three condition dements CE1 , CE2 , and CE3 , the standard Rete algorithm stores information about working memory elements that match CE1 , CE2 , and CE3 individually . In addition , it stores infer / nation about working memory element tuples that match CE1 and CE2 together ( see the locations of memory - nodes in Figure 2 - ~ ) . The Retg algorithm uses this information and combines it with the infon¢ mation about working memory elements that match CE3 to generate tuples that match the complete left - hand side ( CEI , CE2 , and CE3 all together ) . The Rete algorithm does not store information about working memory tuples that match CB1 and CE3 together or those tuples that match CE2 and CE3 together , as is done by the algorithm in the second scheme . The Rete algorithm has been successfully used in current uniproce ~ sor implementations of 01 ~ 5 . It avoids ~ mc of the ¢ . ~ tta work done in the first scheme - - work done in recomputing working memory tuplcs that match combinations of condition elements , It also avoids the combinatorial explosion of stale that is possible in the second scheme by ~ refully selecting the combiitations of con - dition dements for which state is stored , llowever , which combina - tions are selected can ~ gnificantly impact the efficiency of a parallel implementation of the algorithm ( see 10 for details ) . 3 . 3 . Rete as a Parallel Algorithm The previous two sections have discussed the general suitability of the Retc algorithm for match in OPS5 systems . They , however , do not discuss the suitability of Rete for parallel implementation , This section describes some features of Rete that make it attractive for parallel implementation . The data - flow like organization of the Rete network is the key feature that permits exploitation of parallelism at a relatively fine grain . It is possible to evaluate activations of different nodes in the Rete network in parallel . It is also possible to evaluate multiple activations of the same node in parallel and to process multiple changes to working memory in parallel . More details are given in the next section . The state - saving model of Rete , where the state corresponding to only fLxed combinations of condition dements in the left - hand side is stored , does impose some sequentiality on the evaluation of nodes , as compared to other models where state corresponding to all pos , ~ ible combinations of condition elements is stored . It is , however , a reasonable trade - off in order to avoid some of the problems associated with the second scheme , as discussed in the previous section . Finally , although we have no way to prove that Rete represents the best possible algorithm for parallel implementation of produc - tion systems , for the reasons stated in the previous sections , we are confident that it is a pretty good one . Detailed simulation results for a parallel implementation 10 and comparisons to the performance numbers obtained by other researchers in the field 12 , 22 , 24 , fur - ther confirm our view ( also see Sections 6 and 7 ) . 4 . Exploiting Parallelism Within the Rete Algorithm As stated in Section 2 . 1 , there are three steps that are repeatedly performed to execute an OPS5 productionsystem program : match , copflietrl0esolution , and act . It is possible to use parallelism to speed up the A ~ icution of each of these steps . In this paper , however , we Only consider parallelism in the match step . This is because the flaakh step is the bottleneck in current implementations ( it takes abc ~ t 9070 Of the total time ) , and because there are some relatively straightforward ways of speeding up the other two steps . Even for the match step , we only look at the paralldism that can be exploited in the context of the Rete algorithm . The obvious way to use parallelism in a Rete matcher is to allow more than one token to be flowing through theY ' . graph at any one time . In fact , this is how the Rete algorithm was intended to be used , and iris relatively easy to use Rete this way provided one is careful with a few implementation details ( see 2 ) . ' Me simplest parallel implementations treat each node as an individual task ; that is , in a simple implementation O ) each node is permitted to process only one input to ' kenat a given time , and ( 2 ) the node has only a single thread of control internally . In order to increase the amount of parallelism that can be exploited , in the proposed parallel ira - 31 plemenlation , both of these restrictions are relaxed . Nodes are pemtitted It process more than one input token at a given time , artd in mine cases ~ veral tasks are run in parallel to handle the process - ing of a single token . It is beyond the scope of this paper to explain the details of how this is done : for more inlbrmation , refer to 6 . It is interesting to examine at this point the reasons for going to this fine granularity of parallelism , where the average duration of a task is only 50 - 100 machine instructions and where access to a large amount of shared data is ncces ~ ry . The di ~ dvantage of the small task duration is that one has to be very careful about the overheads incurred in ~ heduling such tasks , and the disadvantage of the access to shared data is that it limits the class of hardware architectures on which the algorithm can be implemented - - only those architectures which permit efficient access to shared data can be used . The primary alternative that has been exmnined to exploiting parallelism at this fine granularity is production parallelism . When using production parallelism nmtch for different productions is performed in parallel , but all processing required for any single production is done . serially . The main advantage of using production parallelism is that no communication is required between processes performing match for the distinct productions . This implies that the class of architectures on which such a scheme can be implemented is not so restricted , and in particular , overheads such as memory contention which arise in shared memory multiprocessors ( which need to be used for the fine grain implementations ) are not present . Further - more , much of the synchronization overhead that occurs when access to shared data must be made is not present . The idea of such coarse grain production parallelism was , however , rejected because : * Measurements show that the number of productions that are affected per change to working memory is small , about 30 . ( A production is said to be affected by a change to working memory , if the associated working memory element matches at least one of the condition elements of that production . ) In most matchers , including Rete . determining the set of affected productions is much faster than processing the state - changes associated with the affected productions . Thus the number of affected productions bounds the amount of speed - up that can be achieved using production parallelism . Furthermore , this number does not go up significantly as the total number of productions iri a program increases and as the number of working memory changes that are processed in parallel in - creases . The number ott node activations in the Rete network per change to working memory is not significantly larger than the number of affected productions . However , when multiple changes to working memory are processed in l ? arallel , the number of node activations increases proportionately , wfii¢l ~ significantly increases the maximum speed - ula that can achieved . • Although the average number of productions affected ' p ~ change to working memory is about 30 , simulations show that the actual speed - up that can be obtained using production parallelism ( even with an unbounded number of processors ) is much smaller , only about 5 - fold . This is because of th ' elarge variation in the processing requirements of the affected productions . One reason for this variation is that while only one two - input node activation is associated with most affected productions , there are multiple two - input node activations associated with the remaining affected productions . When parallelism is exploited at the granularity of node activations , the effects of this variation caused by multiple node activations per production can be largely eliminated , resulting in overall increased speed - up . The Rete algorithm , when implemented on a uniproc ~ ssor , saves a significant amount of computation time by sharing evahmtion of cemmon tests amongst multiple productions , In a parallel implementation using production parallelism such sharing has to be given up and results in a significant loss of performance , in a parallel implementation working at the granularity of node activatiotts there is no constraint to main - lain the independent identity of productious . ' lhus it is possible to share common tests between the productions . consequently avoiding the overhead incurred when using production parallelism . 5 . Architectural Implications of Parallelism in Rete Algorithm This section describes the , ' u ' ehitecture of the production system machine ( PSM ) , a hardware structure suitable for executing the parallel version of the Rete algorithm . We begin with a description of the proposed machine ( see 15gure 5 - 1 ) , and later provide justifica - tions for each of the design decisions . The major dlaracteristics of the machine are : 1 . The production system machine should be a shared - memory multiprocessor wilh about 32 - 64 processors . 2 . The individual processors should be high performance corn - purrs , each with a unall amount of private memory and a cache . 3 . The processors should be connected to the shared memory via one or more shared buses . 4 . The PSM should support a hardware task scheduler to help enqueue node activations that need to be evaluated on the task queue and to help assign pending node activations to idle processors . Figure 5 - 1 : Architecture of the Production System Machine . Th ~ first requirement for the proposed production system machine . is that it ~ p¢ld be a shared - memory multiprocessor with 32 - 64 processors . ~ ~ qain reason for using a shared - memory architecture stems from the fact that to achieve a high degree of ~ eed - up from parallelism , the parallel Reta algorithm evaluates diultiple ' activations of the same node in parallel . This requires that ffmltiple ' pr6cessors have access to the state corresponding to that node , which strongly suggests a shared - memory architecture . It is not possible to replicate the state , since keeping all copies up to data is extremely expensive . brother important reason for using a snared - memory architec - ture relates4o the load distidbution problem . In case processors do nqt share memory , the processor on which the activations of a given node ia the Rete network are evaluated must be decided at the time the network is loaded into the parallel machine . Since the number of node activations is much . lsmaller than the total number of nodes 32 in the Rete network 8 , it t ~ necessary to assign several nodes in the nctwork to a single processor . This partitioning of node : ~ amongst the processors is a very difficult problem , and in its full generality is shown to be NP - Completc 24 . Using a shared - memory architec - ture the partitioning problem is bypassed ~ nce all processors are capable of processing all node activations , and it is possible to assign p ~ rs to node activations at run - time . The suggestion of 32 - 64 processors for the multiprocessor is derived as a result of measurements and simulations done for many large production system programs 14 . 16 , 21 . Simulations show that for most production systems using more than 32 - 64 processors does not yield any additional speed - up ( see Section 6 ) . The fact that only about 32 - 64 processors are needed is also consonant with our use of a shared - memory architecture - - it is quite difficult to build shared - memory multiprocessors with very large number of proces - sors . In case it does become necessary to use a larger number of processors ( 100 - 1000 ) for some programs , the use of hierarchical multiprocessors is proposed for the parallel Rete algorithm . The second requirement f6r the proposed production system machine is that the individual processors should be high - performance computers , each with a cache and a small amount of private memory . Since simulations show that the number of proces - sors required for the proposed production system machine is small , there is no reason not to use the processors with the highest performance - - processors having wide datapaths and which use the fastest technology available . It is interesting to note that the code sequences used to execute production system programs do not include complex instructions . The instructions used most often are simple loads , compares , and branches without any complex address - ing modes 5 , 18 , 26 . For the above reasons , the reduced instruction set computers ( RISCs ) 11 , 25 , 27 form good candidates for the individual processors in the production system machine . The reason for associating a small amount of private memory with each proces - sor is to reduce the amount of traffic to the shared memory , since the traffic to shared memory results in memory and bus contention . the data structures stored in the private memory would be those that do not change very often or those that change at well defined points , an example being the data structure storing the contents of the working memory . The third requirement for the proposed production system machine is that the processors should be connected to the shared memory via sharedbuses . The reasons for suggesting a single ( or multiple ) shared - bus scheme , instead of a crossbar switch or other communication networks 0ike an Omega network or a Shuffle Exchange network ) , are : ( 1 ) it is much easier to construct sophis - ticated multi - cache coherency solutions when shared buses are used 15 , 31 , and ( 2 ) simulation results show that a single high - speed bus should be able to handle the load put on it by about 32 processors , provided that reasonable cache - hit ratios are obtained 10 . When shared objects may be stored in cache , it is also possible to imple - ment short synchronization structures , like spin - locks , very efficiently - - it is possible for the processor to loop out of the cache when the synchronization structure is busy . The fourth requirement for the proposed production system machine is that it should be able to support a hardware task scheduler , that is , a hardware mechanism to enqueue node activa - tions into a task queue and to help assign node activations in the task queue to idle processors . The hardware scheduler is also expected to ensure that multiple node activations assigned to be processed in parallel cannot interfere with each other . If a hardware mechanism is not used , the serial enqueueing and dequeueing of hundreds of fine - grain node activations from the task queue is expcetcd to become a bottleneck , " lhe hardware task mhedulcr is expected to sit on the shared bus , and the lime to schedule an activation using such a scheduler is expected to be one bus cycle 10 . An " alternative solution is to use multiple software task schedulers . We are cur - rently investigating the implications of this alternative . 6 . Performance Results To get estimates of the performance that would be obtained as a result of implementing the parallel Rete algorithm on a multiproees - sol a simulator has been constructed ( for details me 0 ) . 1he inputs to the simulator consist of ( 1 ) a detailed trace of node activations from an actual run of a production system ( the trace contains information about the dependencies between node activa - tions and other relevant information ) . ( 2 ) a cost model to help compute the cost of processing any given node activation in the trace , and ( 3 ) a specification of the parallel computational model on which the trace is to be executed ( for example , the numbers of processors in the multiprocessor , the number of memory modules , the bus latency and bus bandwidth , whether a hardware task scheduler is being used or software task queues are being used , etc . ) . The simulator in turn outputs data about the resulting speed - up , the execution speed achieved by the systems , the factors lost due to various overheads ( a simple model of memory - contention is also included ) , etc . Two of the graphs obtained from the above mentioned simulator are shown in Figures 6 - 1 and 6 . 2 . Both graphs are for simulations in which ( 1 ) multiple activations of the stone node may be processed in parallel , ( 2 ) multiple changes to working memory resulting from a production firing are processed in parallel , and ( 3 ) a hardware task scheduler is used for scheduling node activations . The production systems for which the data are presented are VT 19 , ILOG , MUD 14 , DAA16 , R1 - Soar30 , and the Eight - Puzzle - Soar17 . Figure 6 - 1 shows the average concurrency ( that is the average number of processors that are kept busy ) as a fufiction of the number of processors . As can be seen from the graph , for most production systems 32 processors are more than sufficient . Figure 6 - 2 shows the execution speed obtained by these systems as a function of the number of processors , when the individual proces - sors are assumed to work at a speed of 2 MIPS . When a 32 node multiprocessor is used , the graphs show that the average concur - rency is 15 . 92 and that the average execution speed is 9400 wme - changes / sec ( or about 3800 production - firings / sec ) . o . 401 - 36 I " 0 rl - soar • ~ Q • rl - soar ( parallel firings ) 32 I . + ela . soar ~ . 28 I " x ep - soar ( parallel firings ) 0 I1 ~ 24 0 mud dee ~ 20 12 ~ 8 4 0 8 16 24 32 40 48 56 64 72 Number of Processors Figure 6 - 1 : Concurrency 33 Looking at only the concurrency figures can be slightly mislead - ing , because it is c , ~ sy to keep lots of processors busy witho . I , them doing much actual useful work . For this rca , ~ m we also compute the true speed - up , that is , the speed - up obtained by an algorithm over the best known uniproccssor implementation . 2 In the case of the 32 node multiprocessor the average true speed - up is only 8 , 25 - fold , as compared to the concurrency of 15 . 92 , " llae lost factor of 1 . 93 ( 15 . 92 / 8 . 25 ) is due to ( 1 ) extra computation required , as a result of loss of sharing of nodes in the Rele network . ( 2 ) the node , ~ heduling overheads in the parallel implementation , and ( 3 ) the synchroniza - tion overheads in the parallel implementation , " 6 20O0O * . 18o0o ~ 16000 14000 o = " ~ 12000 1o000 E eooo 6000 4000 ~ " 2000 . 0 o 0 rl . soar • rl . soar ( parallel firings ) 4 - elo - soar x ep - soar ( parallel flrmgs ) ~ - o ilog o F I ' ' 8 16 24 32 40 48 56 64 72 Number of Processors Figure 6 - 2 : Execution speed . 7 . Comparison to Other Algorithms and A rchRectu res In order to examine the significance of the performance figures in the previous section and to evaluate the usefulness of the Rete algorithm as presented above , it is useful to contrast other efforts in production system machine design to the work presented here . In this section four other machines are considered DADO , NON - VON , Oflazer ' s machine , and PESA - 1 . 7 . 1 . DADO DADO is a tree based machine being developed at Columbia University 34 , The prototype system has node processors based on Inte18751 single chip computers . These processing elements contain 4K of EPROM and 256 bytes of on chip RAM . In addition , each processing node has 8K bytes of external RAM and a special purpose VLSI implemented switch to connect the node to its parents and children . Of the many algorithms proposed for implementing OPS5 - 1ike production systems on DADO 9 , 22 , 35 , the two al - gorithms offering the highest performance are ( 1 ) the parallel Rete algorithm and ( 2 ) the Treat algorithm . In the implementation of the Rete algorithm on DADO , the complete production system is divided into 16 - 32 partitionS . Once the production system has been partitioned , separate Rete networks are generated for each of the partitions . Each partition is then mapped onto a proee , ~ sing dement at the PM - level 3 and its as - ~ cialcd subtree of processing elements ( also called the WM - Subtree ) . Using the processor at the PM - levcl as a conlrol processor , the processing elements in the WM - sublrec are used to associalively match condition elements and working memory ele - ments , to associatively locate tokens to be deleted , and to perform other similar operations . " lhe processors above the PM - levcl are used for performing conflict - resolution . The performance of the parallel Rete algorithm on the prototype DADO ( consisting of sixteen thousand 0 . 5 MIPS 8 - bit processors ) is predicted to be around 175 wme - ehanges / sec ( for more details , assumptions , and limitations of the analysis see 9 ) , The Treat algorithm differs from the parallel Rete algorithm in that no state is saved other than working elements that satisfy a single condition element ( see Section 3 ) . In this way , working memory tuples matching multiple condition elements must be recal - culated in every match cycle . This inefficiency is justified in that it is now possible to dynamically change the evaluation order of multiple condition element satisfaction and thus possibly reduce mine state . Also . if it found that one of the condition elements of a production is not , satisfied , then no further multiple condition element computa - tions are necessary , since the production can not be satisfied . This saves computation time , Finally , by using multiple processing elements in the WM - subtree to perform the match , it is possible to perform the complex conditional element testing in parallel and therefore reduce the inefficiency due to the lack of saved state . The expected performance of Treat on the prototype DADO is predicted to be around 215 wine - changes / see 22 . 7 . 2 . NON - VON The NON - VON computer is another massively parallel tree - structured machine being developed at Columbia University 32 . The proposed machine architecture consists of a very large number ( 16 , 000 to 1 , 000 , 0000 ) of small processing elements ( SPEs ) , each with a maall local memory ( 32 - 256 bytes ) . The SPEs which , make up the leaves of the tree are also interconnected to one another with a two dimensional orthogonal mesh . Near the root of the tree , each SPE is accompanied by a / arge processing element ( LPE ) , which has a large local memory and a disk interface . The LPEs also have the ability to control the SPEs below them . Therefore , this structure allows NON - VON to work in an MIMD mode or a multiple SIMD ( MSIMD ) mode when the LPEs are feeding commands to the SPEs that they control . In the proposed implementation , both the SPEs and the LPEs are to be capable of processing around 3 million instructions per second . The implementation of Rete proposed to run on NON - VON is similar to the implementation proposed for DADO 12 . However , several changes are necessary to accommodate the data structures into the maall amount of memory associated with the SPEs . The predicted performance of NON - VON ( consisting of thirty - two 32 - bit LPEs and sixteen thousand 8 - bit SPEs , with each LPE and SPE executing at 3 MIPS ) for production system execution is around 2000 wme - changes / sec . Note that the significantly better perfor - mance of NON - VON over DADO can partly be attributed to the fact that the NON - VON processing elements are six times faster 2The best known unlprocessor implementation is taken to be the serial Rete algorithm . 3The PM - level ( the preduction - memory level ) is determined by the number of partitions that are made . For example , if the number of partitions is 16 the PM - Icvel would be 4 , and if the number of partitions is 32 the PM - level would be 5 . 34 than the prototype I ) ADO processing elcments , 7 . 3 . Oflazer ' s Machine Kemal Oflazer in his th ~ is 24 explores a number of issues related to the parallel processing of production systems : ( 1 ) he explores the task of partitioning production systems so that the work is uniformly distributed anlongst the processors , ( 2 ) he proposes a new parallel algorithm for performing match for production sys - tems , and ( 3 ) he proposes a parallel architecture to execute the proposed " algorithm . In this section we only consider the parallel algorithm and the architecture . Oflazer ' s algorithm is based on the contention that both Treat ai ~ d Rete are too conservative in the amount of state they store , and consequently require more , serial processing than is necessary . It is prnposed that the tokens matching not some but all combinations of conditions elements of a productions should be stored . He proposes a new representation for storing this state , such that the interaction era change to working memory with each token of the old state can be computed independently and in parallel . " Ilae architecture that is proposed to execute the suggested algo - rithm is tree aructured , with powerful processors at the the leaf nodes and custom communication switches at internal nodes . The overall system is expected to consist of a few hundred processors . The state corresponding to each production is handled by multiple but a fixed set of leaf processors . The assignment of productions to processors ks done at compile - time using a specified partitioning algorithm . A given processor may be assigned to multiple produc - tions , as long as it is known that those productions are not usually affected by the same set of working memory changes . Simulation results show that using a tree with about 512 16 - bit processors ( 5 - 10 MIPS each ) , the suggested scheme can process 4500 - 7000 wine - changes / see . 7 . 4 . PESA - 1 Raja Ramnarayan ' s group at Honeywell Computer Science Cen - ter is investigating a tagged dataflow processor called PESA - 1 for production system execution 28 . The principle idea exploited in this machine is to map the dataflow representation of Rete explicitly onto a dataflow processor and exploit the parallelism that becomes visible in the match . A key features of the proposed dataflow architecture is that it proposes using buses in areas that are known to have low communication traffic and to provide more direct com - munication paths in other areas . At the time of this writing , accurate performance estimates as a function of number of processors and individual processor speeds are not available for PESA - 1 . 7 . 5 . Comparison of Approaches It is interesting to note that a fairly wide spectrum of architec - tures and algorithms have been covered by the various research efforts - - the primary dimensions for the architectures being the number of processors and connectivity , the primary dimension for the algorithms being the amount of state stored by the parallel algorithm . For the architectures , on one extreme there are the NON - VON and DADO machines with tens of thousands of rela - tively weak processors ( it is assumed that it is not economically feasible to have each of these processors to be very powerful , i . e . , to have caches , wide data - paths , etc . ) and a tree - based interconnection structure . On the other extreme is the production system machine proposed in this paper with 32 - 64 highly powerful processors having shared memory and interconnected via shared buses . In between these two is Oflazer ' s machine with a few hundred relatively power - ful processors interconnected by a tree - structured switching net - work . Comparing the perfomtance of , architectures using a , ~ nall num - ber of processors ( Oflazer ' s machine and the PSM proposed in this paper ) against arehiteclures using a very klrge , mmber of proc ~ sors ( DADO and NON - VON ) , we me that the rennet architectures do significantly better . There are two main reasons for the low perfor - mance of the highly parallel ntachines : ( 1 . ) The , amount of intrinsic parallelism available in OPS5 - 1ike production systems is quite small , as shown by the fact that the number of productions affected per change to working memory is quite . small , around 30 . Thus the large number of processors available in the massively parallel machines just do not get used . ( 2 ) While a ~ hemc using a small number of processors can use expensive and very high performance processors , ~ hemes using a very large number of processors cannot afford fast processors for each of the processing nodes . The perfommnce lost in the highly parallel machines due to the weak individual process - ing nodes is difficult to recover by simply using a large number of such nodes ( since the parallelism is limited ) , It is also interesting to note that the performance of DADO is quite the same when the TREAT algorithm is used ( 2t5 wine - changes / see ) and when the Rete algorithm is used ( . 75 wme - changcs / sec ) . This indicates , at least for the highly parallel machines , that differences in the state - storing strategy between these two algorithms do not matter too much . Comparing the performance of the proposed PSM and Oflazer ' s machine , we see that PSM does somewhat better than Oflazer ' s machine , although the latter uses a larger number of more powerful processors ( thirty - two 2 MIPS processors vs . five hundred and twelve 5 - . 0 MIPS processors ) . We speculate that the reasons why Oflazer ' s machine does not perform better are : ( 1 ) the extra proces - sors are simply used up by the less conservative state - storing strategy without significantly reducing the variance in the processing cost of productions ; ( 2 ) the state - update strategy used by Oflazer introduces some extra garbage collection overheads ( see 24 for details ) , which further nullify the advantage of the reduced variance in proces ~ ng costs ; and ( 3 ) Ofiazer ' s architecture and algorithm do not permit parallel processing of multiplechanges to working memory ( this is quite a serious drawback ) . Since detailed performance analysis of the PESA - 1 machine is not available at this point , it is difficult to say too much about it . However . we feel that their effort is quite close to the architecture and algorithm proposed in this paper and should be able to achieve ~ nilar performance levels . 8 . Limitations and Extensions of the Results The graphs shown in Section 6 , of course , indicate the perfor - mance of only one design for parallel interpreters . It is therefore essential to ask whether it is possible to change the interpreter design - - or even the production systems being interpreted - - in such a way as to increase the exploitable parallelism . Of course , one is not likely to be able to give universal answers to questions like this . It is surely the case that there are applications and implemen - tation techniques for those applications that will permit quite high degrees of parallelism , and that there are other applications that will not permit much parallelism at all to be used . However , by examining the fundamental issues affecting parallelism , one can develop fairly general evaluations of the amount of parallelism that production system interpreters in general can use . This section discusses what these factors are and attempts to evaluate whether they are likely to change enough to materially affect the results presented in this paper . 35 There are three major factors affecting the amount of exploitable parallelism : * The number of changes to working memory per recognize - act cycle . The larger the number of changes , the greater is the number of node activations that may be evaluated in parallel . Measurements on a number of systems have shown that the number of changes per cycle tends to be quite small - - generally less than 0 . 5 % o . f the elements change each cycle , * The number of productions that have to be processed after each working memory change . If state - saving algorithms are used ( as the above sections , argue they should be ) then only the productions that are affected by the changes made to working memory need to be processed on each cycle . Measurements have shown that this number tends to be small regardless of the total number of rules in the system . • The amount of variability in the processing required by each affected production . Generally most of the affected produc - tions require only a small amount of processing , while a few require much more processing . The few productions account for the bulk of the processing performed during the match , and hence the amount of exploitable parallelism is largely determined by these productions . Examining each of these factors reveals that its effect on paral - lelism can be reduced somewhat , but not greatly . Beginning with the number of working memory changes per cycle , there is good reason to believe that this number cannot increase greatly : The reason for using rule - based systems in the first place is to permit a style of programming in which substantial amounts of knowledge can affect each action that the program takes . If individual rules are permitted to do much more processing , then the advantages of this programming style begin to be lost . Knowledge is brought to bear only during the match phase of the cycle , and the less frequently match phases occur , the less chance other rules have to affect the outcome of the processing . Certainly there are many applications in which it is possible to perform substantial amounts of processing without stepping back and reevaluating the state of the system , but those are not the kinds of tasks one should choose a rule - based programming language for . Before leaving this point , it should be observed that there is one way to increase the rate of working memory turnover that avoids this problem : using parallelism in the rule - based system itself . If a system has multiple threads , each one could be performing only the usual small number of working memory changes per cycle , but since there would be several threads , the total number of changes per cycle would be several times higher . Thus application - level paral - lelism will certainly help when it can be used . However , it is not likely to be useful in very many cases for two reasons : First , obviously , it can only be used in tasks that have parallel decomposi - tions , and not all interesting tasks ' will . Second , using application - level parallelism places additional burdens on the developers of the applications . They must find the parallel decompositions and then implement them in such a way that the program is both correct and efficienL The second factor affecting the amount of exploitable paral - lelism , the fact that only a few rules are affected by a typical working memory change is likely always to be the case . This is a result of the fact that rule - bases contain a wide range of knowledge in order to handle most interesting tasks . If the various rules in the system embody knowledge about many different objects and situations , then clearly most working memory elements cannot be of interest to more than a few of the rUleS , because most working memory elements describe 9 . spec ~ s of a single object or situation . The final factor , the amount of variability in the proce , , ~ sing required by the rules , may change sumewhat because researchers are actively working on techniques to do this . F . ven here , however , it is not likely that much improvement is possible . The obvious way to handle this problem is to divide the match process idto many very small tasks . This is effective , but it cannot be carried too far because the amount of overhead time ( for scheduling etc . ) , of course , goes u13 as the number of processes increases , 9 . Summary and Conclusions In this paper we explore the role of parallelism in speeding up the execution of rulc - based ~ stems . Scctions 2 and 3 describe the dataflow - like nature of the Rete match algorithm and discuss its appropriateness for parallel implementations . We observe that it is important for match algorithms to store state across multiple recognize - act cycles , although the exact amount of state that is stored does not appear to be very critical . In Sections 4 , 5 , and 6 , we discuss the parallel implementation of the Rete algorithm . Although , on the surface , rule - based systems appear to have a lot of parallelism ( the match for each rule can be performed in parallel ) , we observe that in practice the true speed - up fi ' om parallelism is quite limited , less than 10 - fold . The main reasons being that ( 1 ) the number of rules relevant to any change to working memory is small , and ( 2 ) there is a large variation in the processing requiremenls of the relevant rules . The limited paral - lelism leads us to exploit parallelism at a very fine granularity , and because of the associated communication and synchronization re - quirements , ~ , e propose the use ofa sharod - memory multiprocessor ( with 32 - 64 powerful processors and a custom hardware task scheduler ) to run the parallel Rete algorithm . Simulations of the implementation of the parallel Rete algorithm on a multiprocessor with thirty - two 2 - MIPS processors show that . on average , it is possible to keep about 16 processors busy , resulting in a perfor - mance of about 9400 wine - changes / see . In Section 7 , we compare the performance of four other architec - tures ( DADO , NON - VON , Oflazer ' s machine , and PESA - 1 ) to the multiprocessor proposed in this paper . DADO and NON - VON , which are highly parallel tree - structured machines using tens of thousands of processors , do significantly worse than the proposed multiprocessor . The main reasons for the lower performance are ( 1 ) the limited parallelism available in OPS5 - like rule - based systems , ( 2 ) the fact that it is not possible to make each of the thousands of processing elements as powerful as those in the multiprocessor , and ( 3 ) the limited communication mechanisms available within the tree - structured organization . The performance of Oflazer ' s machine , which uses a few hundred relatively powerful processors , is alsoslightly worse than that of the proposed multiprocessor . We speculate that the lower performance is due to the features of the algorithm that is used on that machine . Detailed information on PESA - 1 , which is a dataflow machine , was not available to make reasonable performance comparisons . Finally , Section 8 gives reasons why we expect the speed - up results that we have observed for existing production system programs to also hold for programs written in the future . To test our ideas , we are currently implementing Re proposed parallel approach on a VAX - 11 / 784 multiprocessor . The VAX - 11 / 784 consists of four VAX - 11 / 780 processors connected to shared memory . Since the number of processors is much maaller 36 than what we wcmld have liked , we are doing the implementation in a nmnner so that it will ~ sily port over to a larger multiprocessor that we are in the process of acquiring . To gain additional benefits from parallelism , we are also actively exploring issues related to task - level parallelism , that is , exploring how a specific task may be divided into several albtasks all of which may mn in parallel . Acknowledgments The work reported in this paper has been done as part of the Production System Machine project at Carnegie - Mellon University . Its cun ' ent and pabst members include Charles Forgy , Anoop Gupta , Ted i . ehr , Allen Newell , Kemal Oflazcr , Jim Quinlan . Leon Weaver , and Robert Wedig . We would like to acknowledge the contribu - tions of the rest of the group to the ideas presented in this paper . References 1 . Lee Brownston , Robert Farrell , Elaine Kant , and Nancy Martin . Programming Expert Systems in OPS3 : An Introduction to Rule - Based Programming . Addison - Wesley , 1985 . 2 . Charles L . Forgy . On the E . ~ cient Implementations of Production Systems . Ph . D . Th . , Carnegie - Mellon University , Pittsburgh , 1979 . 3 . Charles L . Forgy . " Rete : A Fast Algorithm for the Many Pattern / Many Object Pattern Match Problem " . ArtOTclal Intel - ligence 19 ( September 1982 ) . 4 . Charles L . Forgy . The OPS83 Report . CMU - CS - 84 - 133 , Carnegie - Mellon University , Pittsburgh , May , 1984 . 5 . Charles ForD ' , Anoop Gupta , Allen Newell , and Robert Wedig . Initial Assessment of Architectures for Production Systems . Na - tional Conference for Artificial Intelligence , AAAl - 1984 . 6 . Charles Forgy and Anoop Gupta . Preliminary Architecture of the CMU Production System Machine . Hawaii International Con - ference on System Sciences , January , 1986 . 7 . J . II . Griesmer , S . J . Hong . M . Karnaugh , J . K . Kastner , M . I . Schor , R . L . Ennis , D . A . Klein , K . R . Milliken , It . M . VanWcerkom . YES / MVS : A Continuous Real Time Expert System . National Conference on Artificial Intelligence , AAAI - 1984 . 8 . Anoop Gupta and Charles L Forgy . Measurements on Produc - tion Systems . CMU - CS - 83 - 167 , Carnegie - Mellon University , Pitts - burgh , 1983 . 9 . Anoop Gupta . Implementing OPS5 Production Systems on DADO . International Conference on Parallel Processing , IEEE , 1984 . 10 . Anoop Gupta . Parallelism in Production Systems . Ph . D . Th . , Carnegie - Mellon University , ( in preparation ) 1985 . 11 . J . L . Hennessy , N . Jouppi , S . Przybylski , C . Rowen , and T . Gross . The MIPS Machine . Computer Conference , February , 1982 . 12 . Bruce K . Hillyer and David E Shaw . Execution of OPS5 Production Systems on a Massively Parallel Machine . Columbia University , September , 1984 . 13 . Rostam Joobbani and Daniel P . Siewiorek . Weaver : A Knowledge - Based Routing l ~ perl . Design Automation Con - ference , 1985 . 14 . Gary Kahn and John MeDerrnott . The MUD Syaern . The First Conference on Artificial Intelligence Applications , IEEE Com - puter Society and AAAI , December , 1984 . 15 . R . H . Katz , S . J . Eggers , D . A . Wood , C . L . Perkins , and R . G . Sheldon . Implementing a Cache Consistency Protocol . Proceedings of 12th International Symposium on Computer Architecture , June . 1985 . 16 . Ted Kowalski and Don Thomas . The VLSI l ~ sign Auto ~ ltion Assistant : Ih ' ototype System , 201h Design Automation Conlbrence . ACM and IEI : ' E , June . 1983 . 17 . John E , laird , Paul S . Rosenbloom , and Allen Newell . Towards Chunking as a General Learning Mechanism . National Conference on Artificial Intelligence , AAAI - 1984 . 18 . Theodore I : . I , ehr . The Implementation of a Production System Machine . llawaii lntemalional Conference on System Sciences , January , 1986 . 19 . Sandra Marcus , John McDermott . Robert Roche , Tim Thompsou , Tianran Wang , , and George Wood . L ~ esign Document for VT . Carnegie - Mellon University . 20 . J . Mcl ) emlott , A , Newcll , and J . Moore , The Efficiency of Certain Production System Implementations . In Pattern - Directed Inference ~ $ : vstems . D . A . Waterman and Frederick Hayes - Roth , I ~ . , Academic Press . 1978 . 21 . John McDermott . " RI : A Rule - Based Configurer of Computer Systems " . ArtLficial Intelligence 19 , I ( 1982 ) , 39 - 88 . 22 . Daniel P , Miranker . Performance Estimates lbr the DADO Machine : A Comparison of Treat and Rete . Fifth Generation Computer Systems , ICOT , Tokyo , 1 . 984 . 23 . Kemal Oflazer . Parallel F ~ tecution of Production Systems . In - ternational Conference on Parallel Processing , II ~ EE , August . 1984 . 24 . Kemal Oflazer . Partitioning in ParalleI Processing of Production Systems . Ph . D . Th . , Carnegie - Mellon University , ( in preparation ) 1985 . 25 . D . A . Patterson and C . H . Sequin . " A VLSI RISC " . Computer9 ( 1982 ) . 26 . James Quinlan . A Comparative Analysis of Computer Architec - tures for Production System Machines . Hawaii International Con - ference on System Sciences , January , 1986 . 27 . G . Radin . " The 801 Minicomputer ' . 1BM Journal of Research and Development 27 ( May 1983 ) . 28 . Raja Ramnarayan , Gerhard Zimmerman , and Stanley Krolikoski . PESA - I : A Parallel Archit ~ ture for OPS5 ' ProdUction Systems . Hawaii International Conference on System Sciences , January , 1986 . 29 . Bruce Reed , Jr . The ASPRO Parallel Inlerence Engine & . I . E . ) : A Real Time Production Rule System . Goodyear Aerospace . 30 . Paul S . Rosenbloom , John E . Laird , John McDermott , Allen Newell , and Edmund Orciuch . R1 - Soar : An " Experiment in Knowledge - Intensive Programming in a Problem - Solving Architec - ture . IEEE Workshop on Principles of Knowledge Based Systems , 1984 . 31 . Larry Rudolph and Zary Segall . Dynamic Decentralized Cache Schemes for MIMD Parallel Processors . International Symposium on Computer Architecture , 1984 ~ 32 . David Elliot Shaw . NON - VON ' s Applicability to Three AI Task Areas . International Joint Conference on Artificial Intel - ligence , 1985 . 33 . E . H . Shortliffe . Computer - Based Medical Consultations : MYCIN . North - Holland , 1976 . 34 . Salvatore J . Stolfo , Daniel Miranker , and David E . Shaw . Ar - chitecture and Applications of DADO : A Large - Scale Parallel Com - puter for Artificial Intelligence . International Joint Conference on Artificial Intelligence , 1983 . 35 . Salvatore J . Stolf ' 0 . Five Parallel Algorithms for Production System Execution on the DADO Machine . National Conference on Artificial Intelligence , AAAI - 1984 . 37