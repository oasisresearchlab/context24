Fantastic Rewards and How to Tame Them : A Case Study on Reward Learning for Task - Oriented Dialogue Systems Anonymous Author ( s ) AffiliationAddress email Abstract When learning task - oriented dialogue ( TOD ) agents , one can naturally utilize 1 reinforcement learning ( RL ) techniques to train conversational strategies to achieve 2 user - specific goals . Existing works on training TOD agents mainly focus on 3 developing advanced RL algorithms , while the mechanical designs of reward 4 functions are not well studied . This paper discusses how we can better learn 5 and utilize reward functions for training TOD agents . Specifically , we propose 6 two generalized objectives for reward function learning inspired by the classical 7 learning to rank losses . Further , to address the high variance issue of policy gradient 8 estimation using REINFORCE , we leverage the gumbel - softmax trick to better 9 estimate the gradient for TOD policies , which significantly improves the training 10 stability for policy learning . With the above techniques , we can outperform the 11 state - of - the - art results on the end - to - end dialogue task on the Multiwoz 2 . 0 dataset . 12 1 Introduction 13 Task - Oriented Dialogue systems are designed to achieve a goal specified by a user in natural language . 14 The classical approach to solve the task usually requires to solve several sub - tasks [ 1 ] , including 15 belief state tracking [ 2 , 3 ] , dialogue management ( DM ) [ 4 ] , and natural language generation ( NLG ) 16 for response generation [ 5 ] . More recently , end - to - end task - oriented dialogue based approaches [ e . g . , 17 6 – 8 ] have been proposed , which significantly improve the overall performance . Besides , a number of 18 works developed advanced reinforcement learning algorithms [ e . g . , 9 , 10 ] to further improve the over 19 performance . However , the designs of reward functions are still heuristic based , which may lead to 20 poor performance if they are not well tuned . 21 In this paper , we study how we can better learn and utilize reward function for training TOD agents . 22 To be more concrete , we propose two generalized reward learning objectives in Section 3 . 1 , and 23 discuss how we can better utilize the reward function for dialogue agent training in Section 3 . 2 24 Further we empirically evaluate our proposed methods on Multiwoz 2 . 0 dataset in Section 4 , which 25 shows significantly improvements compared with previous state of the art approaches . 26 2 Background 27 Task Oriented Dialogue as Reinforcement Learning . We formulate the problem of task oriented 28 dialogue systems as a partially observable Markov decision process ( POMDP ) [ 11 ] , specified by 29 M = ⟨ S , A , O , P , R , γ ⟩ , where state s ∈ S consists of the previous dialogue history h and the 30 user intended goal g specified prior to the start of the dialogue ; o ∈ O is the observation that can 31 be the user utterance ; action a ∈ A can be the system response or dialogue act ; P ( s ′ | s , a ) is the 32 underlying transition probability ; R ( h , a , g ) is the intermediate reward function for giving action a 33 under dialogue history h and goal g ; and γ ∈ [ 0 , 1 ] is the discount factor . 34 Submitted to 36th Conference on Neural Information Processing Systems ( NeurIPS 2022 ) . Do not distribute . The dialogue history h t at timestep t consists of all the previous observations and actions , i . e . , 35 h t ≜ { o 0 , a 0 , . . . , o t − 1 , a t − 1 , o t } . Since the TOD agent can not directly observe the user goal g , it 36 makes decision based on the entire dialogue history h t so far . Specifically , the policy π is defined as 37 a mapping from h t to a probability distribution over A , i . e . , π ≜ π ( a t | h t ) . The training objective is 38 to find a policy π that maximizes the expected ( discounted ) cumulative reward 39 J ( π ) ≜ E µ g , π , P (cid:104)(cid:80) Tt = 0 γ t R ( h t , a t , g ) (cid:105) , where µ g is the sampling distribution of goals and T is the number of turns in a dialogue trajectory . 40 Reward Design and Learning in Task Oriented Dialogue Systems . Unlike classic RL problems 41 where the intermediate reward function is well designed and provided , we can only get the evaluation 42 metric at the end of the dialogue [ 12 ] . As a result , most of the existing works adopt the manually 43 designed intermediate reward function that only gives binary reward to indicate success or not [ e . g . , 44 13 , 14 , 10 ] : 45 R ( h t , a t , g ) : = (cid:26) R const , if goal g achieved at t , − R const or 0 , if goal g not achieved at t , where R const is a positive constant that can be 1 . However , such sparse reward signals can be one of 46 the reasons that the TOD agents learned by RL tend to have poor empirical performance [ 15 ] . 47 To address the above issue , a few number of recent works focus on learning a dense reward function 48 from demonstrations or mechanical dialogue assessments [ e . g . , 16 , 9 ] , inspired by the reward learning 49 from preferences in RL [ 17 – 19 ] . More precisely , suppose we are given two dialogue trajectories τ i and 50 τ j , with τ i ≜ { g ( i ) , ( o ( i ) 0 , a ( i ) 0 ) , . . . , ( o ( i ) T , a ( i ) T ) } , and we want to learn a parameterized reward func - 51 tion R θ ( o t , a t , g ) with parameter θ , 1 such that (cid:80) Tt = 0 R θ ( o ( i ) t , a ( i ) t , g ( i ) ) > (cid:80) Tt = 0 R θ ( o ( j ) t , a ( j ) t , g ( j ) ) 52 when the preference score for τ i is larger than τ j ( denoted by τ i ≻ τ j for short ) . Then we can follow 53 the Bradley - Terry model of preferences [ 20 ] to train the reward function by minimizing the following 54 loss : 55 ℓ ( θ ) = − (cid:80) τ i ≻ τ j log (cid:20) exp (cid:16)(cid:80) Tt = 0 R θ ( o ( i ) t , a ( i ) t , g ( i ) ) (cid:17) (cid:80) k ∈ { i , j } exp (cid:16)(cid:80) Tt = 0 R θ ( o ( k ) t , a ( k ) t , g ( k ) ) (cid:17) (cid:21) . ( 1 ) ℓ ( θ ) can also be interpreted as a pairwise ranking loss , which is formalized as a binary classification 56 in the problem of learning to rank [ 21 – 23 ] . 57 3 Main Method 58 In this section , we start with proposed objectives for reward function learning based on classical 59 approaches from learning to rank ( LTR ) literature [ 24 ] , then we describe how to incorporate the 60 learned reward function to training of MinTL to improve the overall performance . 61 3 . 1 Two Generalized Objectives for Reward Learning 62 We introduce two objectives RewardNet and RewardMLE , both of which can utilize multiple dialogue 63 trajectories to optimize the reward function in a batch . Compared with the pairwise based approach 64 described in Section 2 , these two objectives can improve efficiency of the reward learning training , 65 especially under the stochastic training settings . 66 Setup . Assume there are N ( N ≥ 2 ) dialogue trajectories denoted by D N ≜ ( τ 1 , τ 2 , . . . , τ N ) , 67 and each dialogue trajectory τ i has an automatic evaluated metric score S ( τ i ) ( here we use combine 68 score ) . For simplicity , we further assume the N dialogue trajectories are ranked : τ 1 ≻ τ 2 ≻ . . . ≻ τ N , 69 or equivalently S ( τ 1 ) ≥ S ( τ 2 ) ≥ . . . ≥ S ( τ N ) . Besides , we denote the accumulated reward of the 70 dialogue trajectory τ i by J ( τ i ; θ ) : = (cid:80) Tt = 0 R θ ( o ( i ) t , a ( i ) t , g ( i ) ) . And our goal is to learn the reward 71 function R θ ( o , a , g ) such that the accumulated reward of the trajectories can reflect the ranking order : 72 J ( τ 1 ; θ ) ≥ . . . ≥ J ( τ N ; θ ) . 73 RewardNet . The proposed RewardNet objective for reward function learning is adopted from the 74 RewardNet loss [ 25 ] in the LTR literature . Specifically , given the N trajectories , we can define the 75 RewardNet loss as the cross entropy between { J ( τ i ; θ ) } Ni = 1 and { S ( τ i ) } Ni = 1 : 76 ℓ RewardNet ( θ ; D N ) ≜ − (cid:80) Ni = 1 P S ( τ i ) · log (cid:0) P J ( τ ; θ ) ( τ i ) (cid:1) , ( 2 ) 77 with P S ( τ i ) = S ( τ i ) (cid:14)(cid:0)(cid:80) Nk = 1 S ( τ k ) (cid:1) , P J ( τ ; θ ) ( τ i ) = Φ ( J ( τ i ; θ ) ) (cid:14)(cid:0)(cid:80) Nk = 1 Φ ( J ( τ k ; θ ) ) (cid:1) , 1 We use the belief state , action and goal as the reward function input , and the belief state is part the observation o t . We also drop the dependency on h t for R θ to simplify the reward function learning . 2 where Φ ( · ) is a monotonic and positive function defined on R + , and P S ( τ i ) is the normalized prob . 78 defined by the true score of each trajectory . Also , the pairwise loss proposed in CASPI [ 9 ] can be 79 viewed as a special case of RewardNet loss where the number of trajectories N = 2 . 80 RewardMLE . The RewardMLE objective is based on the RewardMLE loss [ 26 ] , where we only 81 utilize the ranking order in the batch dialogue trajectories D N , instead of the original metric scores 82 { S ( τ i ) } Ni = 1 . Let y = rank ( S ) be the random variable that represents the rank order of the dialogue 83 trajectories ( y ( τ i ) = i if the batch trajectories D N are in rank order ) , then the RewardMLE objective 84 is derived as the negative log - likelihood of the rank order y under the Plackett - Luce choice model 85 [ 27 , 28 ] induced by { J ( τ i ; θ ) } Ni = 1 : 86 ℓ RewardMLE ( θ ; D N ) : ≜ − log P (cid:0) y | { J ( τ i ; θ ) } Ni = 1 (cid:1) , ( 3 ) 87 with P (cid:0) y | { J ( τ i ; θ ) } Ni = 1 (cid:1) = (cid:81) Ni = 1 Φ ( J ( τ i ; θ ) ) (cid:14)(cid:0)(cid:80) Nk = i Φ ( J ( τ k ; θ ) ) (cid:1) , where the trajectories in D N are in ranked order as we described in the problem setup : τ 1 ≻ . . . τ N . 88 In Eqs . ( 2 ) and ( 3 ) , the monotonic function Φ transforms the unnormalized inputs { J ( τ i ; θ ) } Ni = 1 to 89 a N - dimensional probabilistic simplex . We consider Φ as exponential function exp ( · ) and power 90 function ( · ) p ( p ∈ N ) , which are also known as the softmax and escort transforms [ 29 ] . 91 3 . 2 Policy Gradient Estimation with Learned Reward Function 92 With the learned reward function R θ ( o , a , g ) , the next step is to improve the parametric dialogue 93 agents π ϕ via policy gradient [ 30 ] . Classical approach to estimate the policy gradient is via REIN - 94 FORCE method [ 31 ] : 95 ∇ ϕ J REINFORCE ( π ϕ ) = E π [ ∇ ϕ log π ϕ ( a t | h t ) G π ( h t , a t , g ) ] , ( 4 ) where G π ( h t , a t , g ) is the discounted accumulated reward that the agents π ϕ receives , starting from 96 observation o t ( part of h t ) and action a t , given goal g . Previous work [ 9 ] indicates that when the 97 discounted factor γ > 0 , estimating G π ( a t , h t , g ) requires monte carlo sampling ( on - policy ) or 98 temporal difference learning ( off - policy ) , bot of which would require to learn an additional value 99 function network . As a result , empirically we observe that it would introduce additional instability to 100 the followed up end - to - end dialogue training . To simplify the training pipeline , we simply set the 101 discounted factor γ = 0 , and we know G π ( h t , a t , g ) = R θ ( o t , a t , g ) . 102 Though the policy gradient estimator defined in Eq . ( 4 ) is unbiased , it tends to have high variance , 103 especially when the action space is large . As a result , the policy optimization with the REINFORCE 104 estimator may diverge during the training . To address the high variance issue of REINFORCE 105 estimator , we utilize gumbel - softmax trick [ 32 , 33 ] to reduce the variance : 106 J GS ( π ϕ ) = E a t ∼ π ( · | h t ) [ R θ ( o t , a t , g ) ] = E ϵϵϵ ∼ Gumbel ( 0 , 1 ) [ R θ ( o t , f ϕ ( h t , ϵϵϵ ) , g ) ] , ( 5 ) with 107 f ϕ ( h t , ϵϵϵ ) = [ f ( 1 ) ϕ ( h t , ϵϵϵ ) , . . . , f ( | A | ) ϕ ( h t , ϵϵϵ ) ] ∈ R | A | , and f ( i ) ϕ ( h t , ϵϵϵ ) = exp ( ( σ i ( h t ; ϕ ) + ϵ i ) / λ ) (cid:80) | A | j = 1 exp ( ( σ j ( h t ; ϕ ) + ϵ j ) / λ ) , where { σ i ( h t ; ϕ ) } | A | i = 1 are the logits of the categorical distribution defined by agent π ϕ . Note that J GS ( π ϕ ) is a biased gradient estimator for policy π ϕ . To achieve bias - variance tradeoff , we combine these two estimators to obtain the loss function for agent response generation : ℓ GEN ( ϕ ) : = − ( αJ REINFORCE ( π ϕ ) + ( 1 − α ) J GS ( π ϕ ) ) , where α is a coefficient specified by users . Combining with the dialogue state tracking ( DST ) loss 108 proposed in MinTL [ 6 ] , we have the final loss for the end - to - end dialogue agent training : 109 ℓ ( ϕ ) = ℓ GEN ( ϕ ) + ℓ DST ( ϕ ) . ( 6 ) 4 Experiments 110 Dataset . We evaluate our proposed methods on the MultiWOZ 2 . 0 dataset [ 12 ] , which is a 111 representative TOD benchmark . MultiWOZ 2 . 0 is a large - scale and multi - domain dialogue corpus , 112 consisting of conversations between a tourist ( user ) and a clerk ( system ) at an information center of a 113 touristic city . This dataset has 8438 dialogues for the training set and 1000 dialogues for each of the 114 validation and test set . 115 3 Table 1 : Results of the end - to - end response generation task on the MultiWOZ 2 . 0 dataset . The best result on each metric is bold . The results of UBAR is from the reproduction by Jang et al . [ 10 ] . The results of CASPI is from our reproduction . All our provided results are the average over five random seeds . Algorithms Inform Success BLEU Combined Score SFN + RL [ 34 ] 73 . 80 53 . 60 16 . 90 83 . 10 DAMD [ 35 ] 76 . 40 64 . 35 17 . 96 88 . 34 SimpleTOD [ 7 ] 84 . 40 70 . 10 15 . 01 92 . 26 MinTL [ 6 ] 84 . 88 74 . 91 17 . 89 97 . 78 SOLOIST [ 8 ] 85 . 50 72 . 90 16 . 54 95 . 74 UBAR [ 36 ] 87 . 47 74 . 43 17 . 61 98 . 56 GPT - Critic [ 10 ] 90 . 07 76 . 63 17 . 83 101 . 13 CASPI [ 9 ] 91 . 37 82 . 80 17 . 70 104 . 78 RewardNet : N = 3 ( p = 1 ) 92 . 77 84 . 28 17 . 74 106 . 27 RewardMLE : N = 5 ( softmax ) 91 . 49 83 . 38 18 . 97 106 . 40 RewardNet : N = 3 ( p = 1 ) + GS 92 . 63 84 . 32 18 . 35 106 . 83 RewardMLE : N = 5 ( softmax ) + GS 93 . 09 83 . 90 18 . 04 106 . 54 Table 2 : Results on the simulated low resource settings , where 5 % , 10 % , and 20 % of the training data is used to train the model . The best result on each metric under each setting is bold . “Comb . " is the Combined Score . All our provided results are the average over five random seeds . Baseline results are from Lin et al . [ 6 ] . Model 5 % 10 % 20 % Inform Success BLEU Comb . Inform Success BLEU Comb . Inform Success BLEU Comb . DAMD 56 . 60 24 . 50 10 . 60 51 . 15 62 . 00 39 . 40 14 . 50 65 . 20 68 . 30 42 . 90 11 . 80 67 . 40 MinTL 75 . 48 60 . 96 13 . 98 82 . 20 78 . 08 66 . 87 15 . 46 87 . 94 82 . 48 68 . 57 13 . 00 88 . 53 RewardNet : N = 3 81 . 22 67 . 37 12 . 82 87 . 11 92 . 39 78 . 98 13 . 36 99 . 05 89 . 83 79 . 30 15 . 18 99 . 75 RewardMLE : N = 5 82 . 90 69 . 61 14 . 26 90 . 51 89 . 67 77 . 48 14 . 80 98 . 38 90 . 15 78 . 70 15 . 81 100 . 24 Evaluation Metrics . Our proposed method is evaluated on the end - to - end dialogue modeling 116 task of the MultiWOZ 2 . 0 dataset . Following the standard setup [ e . g . , 12 , 34 ] , we use four automatic 117 evaluations metrics : 1 ) Inform rate : the fraction of the dialogues where the system has provided 118 an appropriate entity ; 2 ) Success rate : the fraction of the dialogues where the system answered all 119 the requested information ; 3 ) BLEU score [ 37 ] : measures the fluency of the generated response ; 120 4 ) Combined Score [ 34 ] : an overall quality measure defined as Combined Score ≜ ( Inform + 121 Success ) × 0 . 5 + BLEU . All our provided results are the average over five random seeds . 122 Main evaluation . Table 1 compares the performance of our methods with several classical and 123 recent benchmarks , in the end - to - end response - generation task . As shown in Table 1 , our proposed 124 method not only improves the dialogue - task completion , measured by the Inform rate and the Success 125 rate ; but also generates fluent responses , reflected by the competitive BLEU scores . We note that the 126 prior work CASPI is a special case of our proposed method when using the pairwise version of the 127 RewardNet loss and when the probabilistic transform in Eq . ( 2 ) is the escort transform with power 128 one . Comparing the result of CASPI with that of simply adding one more trajectory to estimate the 129 RewardNet loss Eq . ( 2 ) , we see that the RewardNet reward - learning loss improves the performance . 130 As discussed in Section 3 . 1 , our RewardNet approach considers more information for each update of 131 the reward function , and thus could learn a more effective reward function . 132 We further improve the performance by changing the RewardNet loss Eq . ( 2 ) to the RewardMLE loss 133 Eq . ( 3 ) , with the softmax transform as in Xia et al . [ 26 ] and using two more trajectories to calculate 134 the loss . This gain may come from the relative robustness of the RewardMLE loss to small errors in 135 the scoring process , since the RewardMLE loss only uses the ranking of the provided scores , but not 136 the numerical score values as in the RewardNet loss . 137 Adding policy - gradient updates via the Gumbel - softmax method improves the performance of both 138 the RewardNet and RewardMLE models . This shows the efficacy of directly optimizing the response 139 generation model w . r . t . the learned reward function . 140 Low resource experiment . We evaluate our models on the limited - data setting by following the 141 testing strategy in Lin et al . [ 6 ] . Specifically , we use 5 % , 10 % , and 20 % of the training data to train 142 our models , RewardNet : N = 3 ( p = 1 ) and RewardMLE : N = 5 ( softmax ) , and compare them with 143 the baseline scores in Lin et al . [ 6 ] . Table 2 reports the results . It is clear that our models outperform 144 the baselines , MinTL and DAMD , showing the efficacy of our proposed method . Comparing with 145 Table 1 , our models with 20 % of the training data perform competitively with the baseline methods 146 trained on the full training set . 147 4 References 148 [ 1 ] Jason D Williams and Steve Young . Partially observable markov decision processes for spoken 149 dialog systems . Computer Speech & Language , 21 ( 2 ) : 393 – 422 , 2007 . 150 [ 2 ] Jian - Guo Zhang , Kazuma Hashimoto , Chien - Sheng Wu , Yao Wan , Philip S Yu , Richard Socher , 151 and Caiming Xiong . Find or classify ? dual strategy for slot - value predictions on multi - domain 152 dialog state tracking . arXiv preprint arXiv : 1910 . 03544 , 2019 . 153 [ 3 ] Qi Zhu , Zheng Zhang , Yan Fang , Xiang Li , Ryuichi Takanobu , Jinchao Li , Baolin Peng , 154 Jianfeng Gao , Xiaoyan Zhu , and Minlie Huang . Convlab - 2 : An open - source toolkit for building , 155 evaluating , and diagnosing dialogue systems . arXiv preprint arXiv : 2002 . 04793 , 2020 . 156 [ 4 ] Tiancheng Zhao , Kaige Xie , and Maxine Eskenazi . Rethinking action spaces for rein - 157 forcement learning in end - to - end dialog agents with latent variable models . arXiv preprint 158 arXiv : 1902 . 08858 , 2019 . 159 [ 5 ] Tsung - Hsien Wen , Milica Gasic , Nikola Mrksic , Pei - Hao Su , David Vandyke , and Steve Young . 160 Semantically conditioned lstm - based natural language generation for spoken dialogue systems . 161 arXiv preprint arXiv : 1508 . 01745 , 2015 . 162 [ 6 ] Zhaojiang Lin , Andrea Madotto , Genta Indra Winata , and Pascale Fung . MinTL : Minimalist 163 transfer learning for task - oriented dialogue systems . In Proceedings of the 2020 Conference 164 on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 3391 – 3405 , On - 165 line , November 2020 . Association for Computational Linguistics . doi : 10 . 18653 / v1 / 2020 . 166 emnlp - main . 273 . 167 [ 7 ] Ehsan Hosseini - Asl , Bryan McCann , Chien - Sheng Wu , Semih Yavuz , and Richard Socher . A 168 simple language model for task - oriented dialogue . Advances in Neural Information Processing 169 Systems , 33 : 20179 – 20191 , 2020 . 170 [ 8 ] Baolin Peng , Chunyuan Li , Jinchao Li , Shahin Shayandeh , Lars Liden , and Jianfeng Gao . 171 Soloist : Buildingtask bots at scale with transfer learning and machine teaching . Transactions of 172 the Association for Computational Linguistics , 9 : 807 – 824 , 2021 . 173 [ 9 ] Govardana Sachithanandam Ramachandran , Kazuma Hashimoto , and Caiming Xiong . Causal - 174 aware safe policy improvement for task - oriented dialogue . arXiv preprint arXiv : 2103 . 06370 , 175 2021 . 176 [ 10 ] Youngsoo Jang , Jongmin Lee , and Kee - Eung Kim . GPT - critic : Offline reinforcement learning 177 for end - to - end task - oriented dialogue systems . In International Conference on Learning 178 Representations , 2022 . 179 [ 11 ] Leslie Pack Kaelbling , Michael L Littman , and Anthony R Cassandra . Planning and acting in 180 partially observable stochastic domains . Artificial intelligence , 101 ( 1 - 2 ) : 99 – 134 , 1998 . 181 [ 12 ] Paweł Budzianowski , Tsung - Hsien Wen , Bo - Hsiang Tseng , Inigo Casanueva , Stefan Ultes , 182 Osman Ramadan , and Milica Gaši´c . Multiwoz – a large - scale multi - domain wizard - of - oz dataset 183 for task - oriented dialogue modelling . arXiv preprint arXiv : 1810 . 00278 , 2018 . 184 [ 13 ] Gellért Weisz , Paweł Budzianowski , Pei - Hao Su , and Milica Gaši´c . Sample Efficient Deep 185 Reinforcement Learning for Dialogue Systems with Large Action Spaces . arXiv : 1802 . 03753 186 [ cs , stat ] , February 2018 . 187 [ 14 ] Yuexin Wu , Xiujun Li , Jingjing Liu , Jianfeng Gao , and Yiming Yang . Switch - based active 188 deep dyna - q : Efficient adaptive planning for task - completion dialogue policy learning . In 189 Proceedings of the AAAI Conference on Artificial Intelligence , volume 33 , pages 7289 – 7296 , 190 2019 . 191 [ 15 ] Marcin Andrychowicz , Filip Wolski , Alex Ray , Jonas Schneider , Rachel Fong , Peter Welinder , 192 Bob McGrew , Josh Tobin , OpenAI Pieter Abbeel , and Wojciech Zaremba . Hindsight experience 193 replay . Advances in neural information processing systems , 30 , 2017 . 194 5 [ 16 ] Huimin Wang , Baolin Peng , and Kam - Fai Wong . Learning efficient dialogue policy from 195 demonstrations through shaping . In Proceedings of the 58th Annual Meeting of the Associ - 196 ation for Computational Linguistics , pages 6355 – 6365 , Online , July 2020 . Association for 197 Computational Linguistics . doi : 10 . 18653 / v1 / 2020 . acl - main . 566 . 198 [ 17 ] Paul F Christiano , Jan Leike , Tom Brown , Miljan Martic , Shane Legg , and Dario Amodei . Deep 199 reinforcement learning from human preferences . Advances in neural information processing 200 systems , 30 , 2017 . 201 [ 18 ] Daniel Brown , Wonjoon Goo , Prabhat Nagarajan , and Scott Niekum . Extrapolating beyond sub - 202 optimal demonstrations via inverse reinforcement learning from observations . In International 203 conference on machine learning , pages 783 – 792 . PMLR , 2019 . 204 [ 19 ] Daniel S Brown , Wonjoon Goo , and Scott Niekum . Better - than - demonstrator imitation learning 205 via automatically - ranked demonstrations . In Conference on robot learning , pages 330 – 359 . 206 PMLR , 2020 . 207 [ 20 ] Ralph Allan Bradley and Milton E Terry . Rank analysis of incomplete block designs : I . the 208 method of paired comparisons . Biometrika , 39 ( 3 / 4 ) : 324 – 345 , 1952 . 209 [ 21 ] Ralf Herbrich , Thore Graepel , and Klaus Obermayer . Support vector learning for ordinal 210 regression . IET , 1999 . 211 [ 22 ] Yoav Freund , Raj Iyer , Robert E Schapire , and Yoram Singer . An efficient boosting algorithm 212 for combining preferences . Journal of machine learning research , 4 ( Nov ) : 933 – 969 , 2003 . 213 [ 23 ] Chris Burges , Tal Shaked , Erin Renshaw , Ari Lazier , Matt Deeds , Nicole Hamilton , and Greg 214 Hullender . Learning to rank using gradient descent . In Proceedings of the 22nd international 215 conference on Machine learning , pages 89 – 96 , 2005 . 216 [ 24 ] Tie - Yan Liu . Learning to rank for information retrieval . Foundations and Trends® in Information 217 Retrieval , 3 ( 3 ) : 225 – 331 , 2009 . 218 [ 25 ] Zhe Cao , Tao Qin , Tie - Yan Liu , Ming - Feng Tsai , and Hang Li . Learning to rank : from pairwise 219 approach to listwise approach . In Proceedings of the 24th international conference on Machine 220 learning , pages 129 – 136 , 2007 . 221 [ 26 ] Fen Xia , Tie - Yan Liu , Jue Wang , Wensheng Zhang , and Hang Li . Listwise approach to learning 222 to rank : theory and algorithm . In Proceedings of the 25th international conference on Machine 223 learning , pages 1192 – 1199 , 2008 . 224 [ 27 ] Robin L Plackett . The analysis of permutations . Journal of the Royal Statistical Society : Series 225 C ( Applied Statistics ) , 24 ( 2 ) : 193 – 202 , 1975 . 226 [ 28 ] R Duncan Luce . Individual choice behavior : A theoretical analysis . Courier Corporation , 2012 . 227 [ 29 ] Jincheng Mei , Chenjun Xiao , Bo Dai , Lihong Li , Csaba Szepesvári , and Dale Schuurmans . 228 Escaping the gravitational pull of softmax . Advances in Neural Information Processing Systems , 229 33 : 21130 – 21140 , 2020 . 230 [ 30 ] Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT press , 231 2018 . 232 [ 31 ] Ronald J Williams . Simple statistical gradient - following algorithms for connectionist reinforce - 233 ment learning . Machine learning , 8 ( 3 ) : 229 – 256 , 1992 . 234 [ 32 ] Eric Jang , Shixiang Gu , and Ben Poole . Categorical reparameterization with gumbel - softmax . 235 arXiv preprint arXiv : 1611 . 01144 , 2016 . 236 [ 33 ] Chris J Maddison , Andriy Mnih , and Yee Whye Teh . The concrete distribution : A continuous 237 relaxation of discrete random variables . arXiv preprint arXiv : 1611 . 00712 , 2016 . 238 [ 34 ] Shikib Mehri , Tejas Srinivasan , and Maxine Eskenazi . Structured fusion networks for dialog . 239 arXiv preprint arXiv : 1907 . 10016 , 2019 . 240 6 [ 35 ] Yichi Zhang , Zhijian Ou , and Zhou Yu . Task - oriented dialog systems that consider multiple 241 appropriate responses under the same context . In Proceedings of the AAAI Conference on 242 Artificial Intelligence , volume 34 , pages 9604 – 9611 , 2020 . 243 [ 36 ] Yunyi Yang , Yunhao Li , and Xiaojun Quan . Ubar : Towards fully end - to - end task - oriented 244 dialog system with gpt - 2 . In Proceedings of the AAAI Conference on Artificial Intelligence , 245 volume 35 , pages 14230 – 14238 , 2021 . 246 [ 37 ] Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . Bleu : a method for automatic 247 evaluation of machine translation . In Proceedings of the 40th annual meeting of the Association 248 for Computational Linguistics , pages 311 – 318 , 2002 . 249 7