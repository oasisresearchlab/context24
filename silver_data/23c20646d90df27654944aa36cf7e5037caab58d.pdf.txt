Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2138 – 2148 , Lisbon , Portugal , 17 - 21 September 2015 . c (cid:13) 2015 Association for Computational Linguistics . Confounds and Consequences in Geotagged Twitter Data Umashanthi Pavalanathan and Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology Atlanta , GA 30308 { umashanthi + jacobe } @ gatech . edu Abstract Twitter is often used in quantitative stud - ies that identify geographically - preferred topics , writing styles , and entities . These studies rely on either GPS coordinates at - tached to individual messages , or on the user - supplied location ﬁeld in each proﬁle . In this paper , we compare these data ac - quisition techniques and quantify the bi - ases that they introduce ; we also measure their effects on linguistic analysis and text - based geolocation . GPS - tagging and self - reported locations yield measurably dif - ferent corpora , and these linguistic differ - ences are partially attributable to differ - ences in dataset composition by age and gender . Using a latent variable model to induce age and gender , we show how these demographic variables interact with geography to affect language use . We also show that the accuracy of text - based geolocation varies with population demo - graphics , giving the best results for men above the age of 40 . 1 Introduction Social media data such as Twitter is frequently used to identify the unique characteristics of geographical regions , including topics of inter - est ( Hong et al . , 2012 ) , linguistic styles and di - alects ( Eisenstein et al . , 2010 ; Gonc¸alves and S´anchez , 2014 ) , political opinions ( Caldarelli et al . , 2014 ) , and public health ( Broniatowski et al . , 2013 ) . Social media permits the aggregation of datasets that are orders of magnitude larger than could be assembled via traditional survey tech - niques , enabling analysis that is simultaneously ﬁne - grained and global in scale . Yet social media is not a representative sample of any “real world” population , aside from social media itself . Using social media as a sample therefore risks introduc - ing both geographic and demographic biases ( Mis - love et al . , 2011 ; Hecht and Stephens , 2014 ; Lon - gley et al . , 2015 ; Malik et al . , 2015 ) . This paper examines the effects of these bi - ases on the geo - linguistic inferences that can be drawn from Twitter . We focus on the ten largest metropolitan areas in the United States , and con - sider three sampling techniques : drawing an equal number of GPS - tagged tweets from each area ; drawing a county - balanced sample of GPS - tagged messages to correct Twitter’s urban skew ( Hecht and Stephens , 2014 ) ; and drawing a sample of location - annotated messages , using the location ﬁeld in the user proﬁle . Leveraging self - reported ﬁrst names and census statistics , we show that the age and gender composition of these datasets dif - fer signiﬁcantly . Next , we apply standard methods from the lit - erature to identify geo - linguistic differences , and test how the outcomes of these methods depend on the sampling technique and on the underlying demographics . We also test the accuracy of text - based geolocation ( Cheng et al . , 2010 ; Eisenstein et al . , 2010 ) in each dataset , to determine whether the accuracies reported in recent work will gener - alize to more balanced samples . The paper reports several new ﬁndings about geotagged Twitter data : • In comparison with tweets with self - reported locations , GPS - tagged tweets are written more often by young people and by women . • There are corresponding linguistic dif - ferences between these datasets , with GPS - tagged tweets including more geographically - speciﬁc non - standard words . • Young people use signiﬁcantly more geographically - speciﬁc non - standard words . Men tend to mention more geographically - speciﬁc entities than women , but these 2138 differences are signiﬁcant only for individu - als at the age of 30 or older . • Users who GPS - tag their tweets tend to write more , making them easier to geolocate . Eval - uating text - based geolocation on GPS - tagged tweets probably overestimates its accuracy . • Text - based geolocation is signiﬁcantly more accurate for men and for older people . These ﬁndings should inform future attempts to generalize from geotagged Twitter data , and may suggest investigations into the demographic prop - erties of other social media sites . We ﬁrst describe the basic data collection prin - ciples that hold throughout the paper ( § 2 ) . The following three sections tackle demographic bi - ases ( § 3 ) , their linguistic consequences ( § 4 ) , and the impact on text - based geolocation ( § 5 ) ; each of these sections begins with a discussion of meth - ods , and then presents results . We then summarize related work and conclude . 2 Dataset This study is performed on a dataset of tweets gathered from Twitter’s streaming API from February 2014 to January 2015 . During an ini - tial ﬁltering step we removed retweets , repetitions of previously posted messages which contain the “retweeted status” metadata or “RT” token which is widely used among Twitter users to indicate a retweet . To eliminate spam and automated ac - counts ( Yardi et al . , 2009 ) , we removed tweets containing URLs , user accounts with more than 1000 followers or followees , accounts which have tweeted more than 5000 messages at the time of data collection , and the top 10 % of accounts based on number of messages in our dataset . We also re - moved users who have written more than 10 % of their tweets in any language other than English , using Twitter’s lang metadata ﬁeld . Exploration of code - switching ( Solorio and Liu , 2008 ) and the role of second - language English speakers ( Eleta and Golbeck , 2014 ) is left for future work . We consider the ten largest Metropolitan Sta - tistical Areas ( MSAs ) in the United States , listed in Table 1 . MSAs are deﬁned by the U . S . Cen - sus Bureau as geographical regions of high popu - lation with density organized around a single ur - ban core ; they are not legal administrative divi - sions . MSAs include outlying areas that may be substantially less urban than the core itself . For example , the Atlanta MSA is centered on Fulton County ( 1750 people per square mile ) , but extends to Haralson County ( 100 people per square mile ) , on the border of Alabama . A per - county analysis of this data therefore enables us to assess the de - gree to which Twitter’s skew towards urban areas biases geo - linguistic analysis . 3 Representativeness of geotagged Twitter data We ﬁrst assess potential biases in sampling tech - niques for obtaining geotagged Twitter data . In particular , we compare two possible techniques for obtaining data : the location ﬁeld in the user proﬁle ( Poblete et al . , 2011 ; Dredze et al . , 2013 ) , and the GPS coordinates attached to each mes - sage ( Cheng et al . , 2010 ; Eisenstein et al . , 2010 ) . 3 . 1 Methods To build a dataset of GPS - tagged messages , we extracted the GPS latitude and longitude coordi - nates reported in the tweet , and used GIS - TOOLS 1 reverse geocoding to identify the corresponding counties . This set of geotagged messages will be denoted D G . Only 1 . 24 % of messages contain geo - coordinates , and it is possible that the individ - uals willing to share their GPS comprise a skewed population . We therefore also considered the user - reported location ﬁeld in the Twitter proﬁle , focus - ing on the two most widely - used patterns : ( 1 ) city name , ( 2 ) city name and two letter state name ( e . g . Chicago and Chicago , IL ) . Messages that matched any of the ten largest MSAs were grouped into a second set , D L . While the inconsistencies of writing style in the Twitter location ﬁeld are well - known ( Hecht et al . , 2011 ) , analysis of the intersection between D G and D L found that the two data sources agreed the overwhelming majority of the time , suggest - ing that most self - provided locations are accurate . Of course , there may be many false negatives — proﬁles that we fail to geolocate due to the use of non - standard toponyms like Pixburgh and ATL . If so , this would introduce a bias in the population sample in D L . Such a bias might have linguistic consequences , with datasets based on the location ﬁeld containing less non - standard language over - all . 1 https : / / github . com / DrSkippy / Data - Science - 45min - Intros / blob / master / gis - tools - 101 / gis _ tools . ipynb 2139 K i n g s Q u ee n s N e w Y o r k S u ff o l k B r o n x N a ss a u W e s t c h e s t e r B e r g e n M i dd l e s e x E ss e x H u d s o n M o n m o u t h O c e a n U n i o n P a ss a i c M o rr i s R i c h m o n d O t h e r S o m e r s e t R o c k l a n d 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 % o f M S A Population Tweets Users F u l t o n G w i nn e tt O t h e r C o bb D e K a l b C l a y t o n C h e r o k ee H e n r y F o r s y t h P a u l d i n g D o u g l a s C o w e t a C a rr o ll 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 % o f M S A Population Tweets Users Figure 1 : Proportion of census population , Twitter messages , and Twitter user accounts , by county . New York is shown on the left , Atlanta on the right . 3 . 1 . 1 Subsampling The initial samples D G and D L were then resam - pled to create the following balanced datasets : GPS - MSA - BALANCED From D G , we randomly sampled 25 , 000 tweets per MSA as the message - balanced sample , and all the tweets from 2 , 500 users per MSA as the user - balanced sample . Balancing across MSAs ensures that the largest MSAs do not domi - nate the linguistic analysis . GPS - C OUNTY - BALANCED We resampled D G based on county - level population ( ob - tained from the U . S . Census Bureau ) , and again obtained message - balanced and user - balanced samples . These samples are more geographically representative of the overall population distribution across each MSA . L OC - MSA - BALANCED From D L , we randomly sampled 25 , 000 tweets per MSA as the message - balanced sample , and all the tweets from 2 , 500 users per MSA as the user - balanced sample . It is not possible to obtain county - level geolocations in D L , as exact ge - ographical coordinates are unavailable . 3 . 1 . 2 Age and gender identiﬁcation To estimate the distribution of ages and genders in each sample , we queried statistics from the So - cial Security Administration , which records the number of individuals born each year with each given name . Using this information , we obtained the probability distribution of age values for each given name . We then matched the names against the ﬁrst token in the name ﬁeld of each user’s proﬁle , enabling us to induce approximate distri - butions over ages and genders . Unlike Facebook and Google + , Twitter does not have a “real name” policy , so users are free to give names that are fake , humorous , etc . We eliminate user accounts whose names are not sufﬁciently common in the social security database ( i . e . ﬁrst names which are at least 100 times more frequent in Twitter than in the social security database ) , thereby omit - ting 33 % of user accounts , and 34 % of tweets . While some individuals will choose names not typically associated with their gender , we assume that this will happen with roughly equal probabil - ity in both directions . So , with these caveats in mind , we induce the age distribution for the GPS - MSA - B ALANCED sample and the L OC - MSA - B ALANCED sample as , p ( a | name = n ) = count ( name = n , age = a ) (cid:80) a 0 count ( name = n , age = a 0 ) ( 1 ) p D ( a ) ∝ (cid:88) i ∈D p ( a | name = n i ) . ( 2 ) We induce distributions over author gender in much the same way ( Mislove et al . , 2011 ) . This method does not incorporate prior information about the ages of Twitter users , and thus assigns too much probability to the extremely young and old , who are unlikely to use the service . While it would be easy to design such a prior — for exam - ple , assigning zero prior probability to users under the age of ﬁve or above the age of 95 — we see no principled basis for determining these cutoffs . We therefore focus on the differences between the estimated p D ( a ) for each sample D . 2140 Num . L1 Dist . L1 Dist . MSA Counties Population Population vs . Users vs . Tweets New York 23 0 . 2891 0 . 2825 Los Angeles 2 0 . 0203 0 . 0223 Chicago 14 0 . 0482 0 . 0535 Dallas 12 0 . 1437 0 . 1176 Houston 10 0 . 0394 0 . 0472 Philadelphia 11 0 . 1426 0 . 1202 Washington DC 22 0 . 2089 0 . 2750 Miami 3 0 . 0428 0 . 0362 Atlanta 28 0 . 1448 0 . 1730 Boston 7 0 . 1878 0 . 2303 Table 1 : L1 distance between county - level popu - lation and Twitter users and messages 3 . 2 Results Geographical biases in the GPS Sample We ﬁrst assess the differences between the true pop - ulation distributions over counties , and the per - tweet and per - user distributions . Because coun - ties vary widely in their degree of urbanization and other demographic characteristics , this mea - sure is a proxy for the representativeness of GPS - based Twitter samples ( county information is not available for the L OC - MSA - BALANCED sample ) . Population distributions for New York and Atlanta are shown in Figure 1 . In Atlanta , Fulton County is the most populous and most urban , and is over - represented in both geotagged tweets and user ac - counts ; most of the remaining counties are corre - spondingly underrepresented . This coheres with the urban bias noted earlier by Hecht and Stephens ( 2014 ) . In New York , Kings County ( Brooklyn ) is the most populous , but is underrepresented in both the number of geotagged tweets and user ac - counts , at the expense of New York County ( Man - hattan ) . Manhattan is the commercial and enter - tainment center of the New York MSA , so resi - dents of outlying counties may be tweeting from their jobs or social activities . To quantify the representativeness of each sam - ple , we use the L1 distance | | x − y | | 1 = P c | p c − t c | , where p c is the proportion of the MSA pop - ulation residing in county c and t c is the propor - tion of tweets ( Table 1 ) . County boundaries are determined by states , and their density varies : for example , the Los Angeles MSA covers only two counties , while the smaller Atlanta MSA is spread over 28 counties . The table shows that while New York is the most extreme example , most MSAs feature an asymmetry between county population and Twitter adoption . 0 - 2 2 - 5 5 - 10 10 - 15 > 15 Number of messages by a user 0 . 0 0 . 5 1 . 0 1 . 5 N u m b e r o f u s e r s 1e4 1 0 0 5 4 7 0 7 7 4 9 3 9 1 8 4 1 1 0 8 9 1 0 0 6 2 6 9 1 4 4 9 2 4 1 9 5 6 1 1 4 4 1 6 0 4 6 4 4 6 0 1 8 8 9 6 1 6 3 6 2 Number of users in each category GPS - MSA - Balanced GPS - County - Balanced LOC - MSA - Balanced Figure 2 : User counts by number of Twitter mes - sages Usage Next , we turn to differences between the GPS - based and proﬁle - based techniques for ob - taining ground truth data . As shown in Fig - ure 2 , the L OC - MSA - BALANCED sample con - tains more low - volume users than either the GPS - MSA - BALANCED or GPS - C OUNTY - BALANCED samples . We can therefore conclude that the county - level geographical bias in the GPS - based data does not impact usage rate , but that the differ - ence between GPS - based and proﬁle - based sam - pling does ; the linguistic consequences of this dif - ference will be explored in the following sections . Demographics Table 2 shows the expected age and gender for each dataset , with bootstrap con - ﬁdence intervals . Users in the L OC - MSA - BALANCED dataset are on average two years older than in the GPS - MSA - BALANCED and GPS - C OUNTY - BALANCED datasets , which are statis - tically indistinguishable . Focusing on the differ - ence between GPS - MSA - BALANCED and L OC - MSA - BALANCED , we plot the difference in age probabilities in Figure 3 , showing that GPS - MSA - BALANCED includes many more teens and people in their early twenties , while L OC - MSA - BALANCED includes more people at middle age and older . Young people are especially likely to use social media on cellphones ( Lenhart , 2015 ) , where location tagging would be more relevant than when Twitter is accessed via a personal com - puter . Social media users in the age brackets 18 - 29 and 30 - 49 are also more likely to tag their lo - cations in social media posts than social media users in the age brackets 50 - 64 and 65 + ( Zickuhr , 2013 ) , with women and men tagging at roughly equal rates . Table 2 shows that the GPS - MSA - BALANCED and GPS - C OUNTY - BALANCED sam - ples contain signiﬁcantly more women than L OC - 2141 Sample Expected Age 95 % CI % Female 95 % CI GPS - MSA - BALANCED 36 . 17 [ 36 . 07 – 36 . 27 ] 51 . 5 [ 51 . 3 – 51 . 8 ] GPS - C OUNTY - BALANCED 36 . 25 [ 36 . 16 – 36 . 30 ] 51 . 3 [ 51 . 1 – 51 . 6 ] L OC - MSA - BALANCED 38 . 35 [ 38 . 25 – 38 . 44 ] 49 . 3 [ 49 . 1 – 49 . 6 ] Table 2 : Demographic statistics for each dataset 0 20 40 60 80 100 Age −1 . 5 −1 . 0 −0 . 5 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 P r G P S ( a g e ) − P r L O C ( a g e ) 1e−3 Figure 3 : Difference in age probability distribu - tions between GPS - MSA - BALANCED and L OC - MSA - BALANCED . MSA - BALANCED , though all three samples are close to 50 % . 4 Impact on linguistic generalizations Many papers use Twitter data to draw conclusions about the relationship between language and ge - ography . What role do the demographic differ - ences identiﬁed in the previous section have on the linguistic conclusions that emerge ? We mea - sure the differences between the linguistic corpora obtained by each data acquisition approach . Since the GPS - MSA - BALANCED and GPS - C OUNTY - BALANCED methods have nearly identical pat - terns of usage and demographics , we focus on the difference between GPS - MSA - BALANCED and L OC - MSA - BALANCED . These datasets differ in age and gender , so we also directly measure the impact of these demographic factors on the use of geographically - speciﬁc linguistic variables . 4 . 1 Methods Discovering geographical linguistic variables We focus on lexical variation , which is relatively easy to identify in text corpora . Monroe et al . ( 2008 ) survey a range of alternative statistics for ﬁnding lexical variables , demonstrating that a reg - ularized log - odds ratio strikes a good balance be - tween distinctiveness and robustness . A similar approach is implemented in SAGE ( Eisenstein et al . , 2011a ) 2 , which we use here . For each sam - 2 https : / / github . com / jacobeisenstein / jos - gender - 2014 ple — GPS - MSA - BALANCED and L OC - MSA - BALANCED — we apply SAGE to identify the twenty - ﬁve most salient lexical items for each metropolitan area . Keyword annotation Previous research has identiﬁed two main types of geographical lexi - cal variables . The ﬁrst are non - standard words and spellings , such as hella and yinz , which have been found to be very frequent in social me - dia ( Eisenstein , 2015 ) . Other researchers have fo - cused on the “long tail” of entity names ( Roller et al . , 2012 ) . A key question is the relative im - portance of these two variable types , since this would decide whether geo - linguistic differences are primarily topic - based or stylistic . It is there - fore important to know whether the frequency of these two variable types depends on proper - ties of the sample . To test this , we take the lexical items identiﬁed by SAGE ( 25 per MSA , for both the GPS - MSA - BALANCED and L OC - MSA - BALANCED samples ) , and annotate them as N ONSTANDARD - W ORD , E NTITY - N AME , or O THER . Annotation for ambiguous cases is based on the majority sense in randomly - selected exam - ples . Overall , we identify 24 N ONSTANDARD - W ORD s and 185 E NTITY - N AME s . Inferring author demographics As described in § 3 . 1 . 2 , we can obtain an approximate distri - bution over author age and gender by linking self - reported ﬁrst names with aggregate statistics from the United States Census . To sharpen these esti - mates , we now consider the text as well , build - ing a simple latent variable model in which both the name and the word counts are drawn from dis - tributions associated with the latent age and gen - der ( Chang et al . , 2010 ) . The model is shown in Figure 4 , and involves the following generative process : For each user i ∈ { 1 . . . N } , ( a ) draw the age , a i ∼ Categorical ( π ) ( b ) draw the gender , g i ∼ Categorical ( 0 . 5 ) 2142 θ φ w i n i a i g i π N 2B a i Age ( bin ) for author i g i Gender of author i w i Word counts for author i n i First name of author i π Prior distribution over age bins θ a , g Word distribution for age a and gender g φ a , g First name distribution for age a and gender g Figure 4 : Plate diagram for latent variable model of age and gender ( c ) draw the author’s given name , n i ∼ Categorical ( φ a i , g i ) ( d ) draw the word counts , w i ∼ Multinomial ( θ a i , g i ) , where we elide the second parameter of the multi - nomial distribution , the total word count . We use expectation - maximization to perform inference in this model , binning the latent age variable into four groups : 0 - 17 , 18 - 29 , 30 - 39 , above 40 . 3 Be - cause the distribution of names given demograph - ics is available from the Social Security data , we clamp the value of φ throughout the EM proce - dure . Other work in the domain of demographic prediction often involves more complex meth - ods ( Nguyen et al . , 2014 ; Volkova and Durme , 2015 ) , but since it is not the focus of our research , we take a relatively simple approach here , assum - ing no labeled data for demographic attributes . 4 . 2 Results Linguistic differences by dataset We ﬁrst con - sider the impact of the data acquisition tech - nique on the lexical features associated with each city . The keywords identiﬁed in GPS - MSA - BALANCED dataset feature more geographically - speciﬁc non - standard words , which occur at a rate of 3 . 9 × 10 − 4 in GPS - MSA - BALANCED , versus 2 . 6 × 10 − 4 in L OC - MSA - BALANCED ; this differ - ence is statistically signiﬁcant ( p < . 05 , t = 3 . 2 ) . 4 3 Binning is often employed in work on text - based age pre - diction ( Garera and Yarowsky , 2009 ; Rao et al . , 2010 ; Rosen - thal and McKeown , 2011 ) ; it enables word and name counts to be shared over multiple ages , and avoids the complexity inherent in regressing a high - dimensional textual predictors against a numerical variable . 4 We employ a paired t - test , comparing the difference in frequency for each word across the two datasets . Since we cannot test the complete set of entity names or non - standard words , this quantiﬁes whether the observed difference is ro - bust across the subset of the vocabulary that we have selected . 0 - 17 18 - 29 30 - 39 40 + Age group 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 1 . 4 1 . 6 P e r - w o r d f r e q u e n c y 1e 4 0 . 00015 0 . 00004 0 . 00002 0 . 00001 0 . 00009 0 . 00005 0 . 00002 0 . 00002 MaleFemale ( a ) non - standard words 0 - 17 18 - 29 30 - 39 40 + Age group 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 P e r - w o r d f r e q u e n c y 1e 3 0 . 00027 0 . 00037 0 . 00117 0 . 00075 0 . 00021 0 . 00038 0 . 00050 0 . 00034 MaleFemale ( b ) entity names Figure 5 : Aggregate statistics for geographically - speciﬁc non - standard words and entity names across imputed demographic groups , from the GPS - MSA - BALANCED sample . For entity names , the difference between datasets was not signiﬁcant , with a rate of 4 . 0 × 10 − 3 for GPS - MSA - BALANCED , and 3 . 7 × 10 − 3 for L OC - MSA - BALANCED . Note that these rates include only the non - standard words and entity names de - tected by SAGE as among the top 25 most distinc - tive for one of the ten largest cities in the US ; of course there are many other relevant terms that are below this threshold . In a pilot study of the GPS - C OUNTY - BALANCED data , we found few linguistic differ - ences from GPS - MSA - BALANCED , in either the aggregate word - group frequencies or the SAGE word lists — despite the geographical imbalances shown in Table 1 and Figure 1 . Informal ex - amination of speciﬁc counties shows some ex - pected differences : for example , Clayton County , which hosts Atlanta’s Hartsﬁeld - Jackson airport , includes terms related to air travel , and other coun - ties include mentions of local cities and business districts . But the aggregate statistics for under - represented counties are not substantially different from those of overrepresented counties , and are largely unaffected by county - based resampling . Demographics Aggregate linguistic statistics for demographic groups are shown in Fig - ure 5 . Men use signiﬁcantly more geographically - speciﬁc entity names than women ( p (cid:28) . 01 , t = 2143 Age Sex New York Dallas 0 - 17 F niall , ilysm , hemmings , stalk , ily fanuary , idol , lmbo , lowkey , jonas M ight , technique , kisses , lesbian , dicks homies , daniels , oomf , teenager , brah 18 - 29 F roses , castle , hmmmm , chem , sinking socially , coma , hubby , bra , swimming M drunken , manhattan , spoiler , guardians , gonna harden , watt , astros , rockets , mavs 30 - 39 F suite , nyc , colleagues , york , portugal astros , sophia , recommendations , houston , prepping M mets , effectively , cruz , founder , knicks texans , rockets , embarrassment , tcu , mississippi 40 + F cultural , affected , encouraged , proverb , un - happy determine , islam , rejoice , psalm , responsibility M reuters , investors , shares , lawsuit , theaters mph , wazers , houston , tx , harris Table 3 : Most characteristic words for demographic subsets of each city , as compared with the overall average word distribution 8 . 0 ) , but gender differences for geographically - speciﬁc non - standard words are not signiﬁcant ( p ≈ . 2 ) . 5 Younger people use signiﬁcantly more geographically - speciﬁc non - standard words than older people ( ages 0 – 29 versus 30 + , p (cid:28) . 01 , t = 7 . 8 ) , and older people mention signiﬁ - cantly more geographically - speciﬁc entity names ( p (cid:28) . 01 , t = 5 . 1 ) . Of particular interest is the intersection of age and gender : the use of geographically - speciﬁc non - standard words de - creases with age much more profoundly for men than for women ; conversely , the frequency of mentioning geographically - speciﬁc entity names increases dramatically with age for men , but to a much lesser extent for women . The observation that high - level patterns of geographically - oriented language are more age - dependent for men than for women suggests an intriguing site for future research on the intersectional construction of lin - guistic identity . For a more detailed view , we apply SAGE to identify the most salient lexical items for each MSA , subgrouped by age and gender . Table 3 shows word lists for New York ( the largest MSA ) and Dallas ( the 5th - largest MSA ) , using the GPS - MSA - BALANCED sample . Non - standard words tend to be used by the youngest authors : ilysm ( ’I love you so much’ ) , ight ( ’alright’ ) , oomf ( ’one of my followers’ ) . Older authors write more about local entities ( manhattan , nyc , houston ) , with men focusing on sports - related entities ( harden , watt , astros , mets , texans ) , and women above the age of 40 emphasizing religiously - oriented terms ( proverb , islam , rejoice , psalm ) . 5 But see Bamman et al . ( 2014 ) for a much more detailed discussion of gender and standardness . 5 Impact on text - based geolocation A major application of geotagged social media is to predict the geolocation of individuals based on their text ( Eisenstein et al . , 2010 ; Cheng et al . , 2010 ; Wing and Baldridge , 2011 ; Hong et al . , 2012 ; Han et al . , 2014 ) . Text - based geolo - cation has obvious commercial implications for location - based marketing and opinion analysis ; it is also potentially useful for researchers who want to measure geographical phenomena in social me - dia , and wish to access a larger set of individuals than those who provide their locations explicitly . Previous research has obtained impressive ac - curacies for text - based geolocation : for exam - ple , Hong et al . ( 2012 ) report a median error of 120 km , which is roughly the distance from Los Angeles to San Diego , in a prediction space over the entire continental United States . These accura - cies are computed on test sets that were acquired through the same procedures as the training data , so if the acquisition procedures have geographic and demographic biases , then the resulting accu - racy estimates will be biased too . Consequently , they may be overly optimistic ( or pessimistic ! ) for some types of authors . In this section , we explore where these text - based geolocation methods are most and least accurate . 5 . 1 Methods Our data is drawn from the ten largest metropoli - tan areas in the United States , and we formulate text - based geolocation as a ten - way classiﬁcation problem , similar to Han et al . ( 2014 ) . 6 Using our 6 Many previous papers have attempted to identify the pre - cise latitude and longitude coordinates of individual authors , but obtaining high accuracy on this task involves much more complex methods , such as latent variable models ( Eisenstein et al . , 2010 ; Hong et al . , 2012 ) , or multilevel grid struc - tures ( Cheng et al . , 2010 ; Roller et al . , 2012 ) . Tuning such 2144 user - balanced samples , we apply ten - fold cross validation , and tune the regularization parameter on a development fold , using the vocabulary of the sample as features . 5 . 2 Results Many author - attribute prediction tasks become substantially easier as more data is avail - able ( Burger et al . , 2011 ) , and text - based ge - olocation is no exception . Since GPS - MSA - BALANCED and L OC - MSA - BALANCED have very different usage rates ( Figure 2 ) , perceived dif - ferences in accuracy may be purely attributable to the amount of data available per user , rather than to users in one group being inherently harder to classify than another . For this reason , we bin users by the number of messages in our sample of their timeline , and report results separately for each bin . All errorbars represent 95 % conﬁdence intervals . GPS versus location As seen in Figure 6a , there is little difference in accuracy across sampling techniques : the location - based sample is slightly easier to geolocate at each usage bin , but the dif - ference is not statistically signiﬁcant . However , due to the higher average usage rate in GPS - MSA - BALANCED ( see Figure 2 ) , the overall accu - racy for a sample of users will appear to be higher on this data . Demographics Next , we measure classiﬁcation accuracy by gender and age , using the posterior distribution from the expectation - maximization al - gorithm to predict the gender of each user ( broadly similar results are obtained by using the prior dis - tribution ) . For this experiment , we focus on the GPS - MSA - BALANCED sample . As shown in Figure 6b , text - based geolocation is consistently more accurate for male authors , across almost the entire spectrum of usage rates . As shown in Fig - ure 6c , older users also tend to be easier to ge - olocate : at each usage level , the highest accuracy goes to one of the two older groups , and the dif - ference is signiﬁcant in almost every case . As dis - cussed in § 4 , older male users tend to mention many entities , particularly sports - related terms ; these terms are apparently more predictive than the non - standard spellings and slang favored by younger authors . models can be challenging , and the resulting accuracies might be affected by initial conditions or hyperparameters . We therefore focus on classiﬁcation , employing the familiar and well - understood method of logistic regression . 6 Related Work Several researchers have studied how adoption of Internet technology varies with factors such as so - cioeconomic status , age , gender , and living condi - tions ( Zillien and Hargittai , 2009 ) . Hargittai and Litt ( 2011 ) use a longitudinal survey methodology to compare the effects of gender , race , and topics of interest on Twitter usage among young adults . Geographic variation in Twitter adoption has been considered both internationally ( Kulshrestha et al . , 2012 ) and within the United States , using both the Twitter location ﬁeld ( Mislove et al . , 2011 ) and per - message GPS coordinates ( Hecht and Stephens , 2014 ) . Aggregate demographic statis - tics of Twitter users’ geographic census blocks were computed by O’Connor et al . ( 2010 ) and Eisenstein et al . ( 2011b ) ; Malik et al . ( 2015 ) use census demographics in spatial error model . These papers draw similar conclusions , showing that the the distribution of geotagged tweets over the US population is not random , and that higher usage is correlated with urban areas , high income , more ethnic minorities , and more young people . How - ever , this prior work did not consider the biases introduced by relying on geotagged messages , nor the consequences for geo - linguistic analysis . Twitter has often been used to study the ge - ographical distribution of linguistic information , and of particular relevance are Twitter - based stud - ies of regional dialect differences ( Eisenstein et al . , 2010 ; Doyle , 2014 ; Gonc¸alves and S´anchez , 2014 ; Eisenstein , 2015 ) and text - based geoloca - tion ( Cheng et al . , 2010 ; Hong et al . , 2012 ; Han et al . , 2014 ) . This prior work rarely considers the im - pact of the demographic confounds , or of the geo - graphical biases mentioned in § 3 . Recent research shows that accuracies of core language technol - ogy tasks such as part - of - speech tagging are cor - related with author demographics such as author age ( Hovy and Søgaard , 2015 ) ; our results on lo - cation prediction are in accord with these ﬁndings . Hovy ( 2015 ) show that including author demo - graphics can improve text classiﬁcation , a similar approach might improve text - based geolocation as well . We address the question about the impact of geographical biases and demographic confounds by measuring differences between three sampling techniques , in both language use and in the ac - curacy of text - based geolocation . Recent unpub - lished work proposes reweighting Twitter data to 2145 0 - 2 3 - 5 6 - 10 11 - 15 > 15 Number of messages by a user 0 . 14 0 . 16 0 . 18 0 . 20 0 . 22 0 . 24 0 . 26 0 . 28 A cc u r a c y LOC - MSA - Balanced GPS - MSA - Balanced ( a ) Classiﬁcation accuracy by sampling technique 0 - 2 3 - 5 6 - 10 11 - 15 > 15 Number of messages by a user 0 . 14 0 . 16 0 . 18 0 . 20 0 . 22 0 . 24 A cc u r a c y Male Female ( b ) Classiﬁcation accuracy by user gen - der 0 - 2 3 - 5 6 - 10 11 - 15 > 15 Number of messages by a user 0 . 12 0 . 14 0 . 16 0 . 18 0 . 20 0 . 22 0 . 24 A cc u r a c y 0 - 17 18 - 29 30 - 39 40 + ( c ) Classiﬁcation accuracy by imputed age Figure 6 : Classiﬁcation accuracies correct biases in political analysis ( Choy et al . , 2012 ) and public health ( Culotta , 2014 ) . Our results suggest that the linguistic differences be - tween user - supplied proﬁle locations and per - message geotags are more signiﬁcant , and that ac - counting for the geographical biases among geo - tagged messages is not sufﬁcient to offer a repre - sentative sample of Twitter users . 7 Discussion Geotagged Twitter data offers an invaluable re - source for studying the interaction of language and geography , and is helping to usher in a new gener - ation of location - aware language technology . This makes critical investigation of the nature of this data source particularly important . This paper un - covers demographic confounds in the linguistic analysis of geo - located Twitter data , but is lim - ited to demographics that can be readily induced from given names . A key task for future work is to quantify the representativeness of geotagged Twit - ter data with respect to factors such as race and so - cioeconomic status , while holding geography con - stant . However , these features may be more difﬁ - cult to impute from names alone . Another cru - cial task is to expand this investigation beyond the United States , as the varying patterns of use for so - cial media across countries ( Pew Research Center , 2012 ) implies that the ﬁndings here cannot be ex - pected to generalize to every international context . Acknowledgments Thanks to the anonymous reviewers for their useful and constructive feed - back on our submission . The following mem - bers of the Georgia Tech Computational Linguis - tics Laboratory offered feedback throughout the research process : Naman Goyal , Yangfeng Ji , Vin - odh Krishan , Ana Smith , Yijie Wang , and Yi Yang . This research was supported by the National Sci - ence Foundation under awards IIS - 1111142 and RI - 1452443 , by the National Institutes of Health under award number R01GM112697 - 01 , and by the Air Force Ofﬁce of Scientiﬁc Research . The content is solely the responsibility of the authors and does not necessarily represent the ofﬁcial views of these sponsors . References David Bamman , Jacob Eisenstein , and Tyler Schnoe - belen . 2014 . Gender identity and lexical varia - tion in social media . Journal of Sociolinguistics , 18 ( 2 ) : 135 – 160 . David A Broniatowski , Michael J Paul , and Mark Dredze . 2013 . National and local inﬂuenza surveil - lance through twitter : An analysis of the 2012 - 2013 inﬂuenza epidemic . PloS one , 8 ( 12 ) : e83672 . John D . Burger , John Henderson , George Kim , and Guido Zarrella . 2011 . Discriminating gender on twitter . In Proceedings of the Conference on Em - pirical Methods in Natural Language Processing . Guido Caldarelli , Alessandro Chessa , Fabio Pammolli , Gabriele Pompa , Michelangelo Puliga , Massimo Riccaboni , and Gianni Riotta . 2014 . A multi - level geographical study of Italian political elections from Twitter Data . PloS one , 9 ( 5 ) : e95809 . Jonathan Chang , Itamar Rosenn , Lars Backstrom , and Cameron Marlow . 2010 . ePluribus : Ethnicity on social networks . In Proceedings of the International Conference on Web and Social Media ( ICWSM ) , pages 18 – 25 , Menlo Park , California . AAAI Pub - lications . Zhiyuan Cheng , James Caverlee , and Kyumin Lee . 2010 . You are where you tweet : a content - based ap - proach to geo - locating twitter users . In Proceedings of the International Conference on Information and Knowledge Management ( CIKM ) , pages 759 – 768 . Murphy Choy , Michelle Cheong , Ma Nang Laik , and Koo Ping Shung . 2012 . Us presidential elec - tion 2012 prediction using census corrected twitter model . arXiv preprint arXiv : 1211 . 0938 . 2146 Aron Culotta . 2014 . Reducing sampling bias in so - cial media data for county health inference . In Joint Statistical Meetings Proceedings . Gabriel Doyle . 2014 . Mapping dialectal variation by querying social media . In Proceedings of the European Chapter of the Association for Computa - tional Linguistics ( EACL ) , pages 98 – 106 , Strouds - burg , Pennsylvania . Association for Computational Linguistics . Mark Dredze , Michael J Paul , Shane Bergsma , and Hieu Tran . 2013 . Carmen : A Twitter geolocation system with applications to public health . In AAAI Workshop on Expanding the Boundaries of Health Informatics Using Artiﬁcial Intelligence , pages 20 – 24 . Jacob Eisenstein , Brendan O’Connor , Noah A . Smith , and Eric P . Xing . 2010 . A latent variable model for geographic lexical variation . In Proceedings of Em - pirical Methods for Natural Language Processing ( EMNLP ) , pages 1277 – 1287 , Stroudsburg , Pennsyl - vania . Association for Computational Linguistics . Jacob Eisenstein , Amr Ahmed , and Eric P . Xing . 2011a . Sparse additive generative models of text . In Proceedings of the International Conference on Ma - chine Learning ( ICML ) , pages 1041 – 1048 , Seattle , WA . Jacob Eisenstein , Noah A . Smith , and Eric P . Xing . 2011b . Discovering sociolinguistic associations with structured sparsity . In Proceedings of the Asso - ciation for Computational Linguistics ( ACL ) , pages 1365 – 1374 , Portland , OR . Jacob Eisenstein . 2015 . Written dialect variation in online social media . In Charles Boberg , John Ner - bonne , and Dom Watt , editors , Handbook of Dialec - tology . Wiley . Irene Eleta and Jennifer Golbeck . 2014 . Multilingual use of twitter : Social networks at the language fron - tier . Computers in Human Behavior , 41 : 424 – 432 . Nikesh Garera and David Yarowsky . 2009 . Modeling latent biographic attributes in conversational genres . In Proceedings of the Association for Computational Linguistics ( ACL ) , pages 710 – 718 . Bruno Gonc¸alves and David S´anchez . 2014 . Crowd - sourcing dialect characterization through twitter . PloS one , 9 ( 11 ) : e112074 . Bo Han , Paul Cook , and Timothy Baldwin . 2014 . Text - based twitter user geolocation prediction . Journal of Artiﬁcial Intelligence Research ( JAIR ) , 49 : 451 – 500 . Eszter Hargittai and Eden Litt . 2011 . The tweet smell of celebrity success : Explaining variation in twit - ter adoption among a diverse group of young adults . New Media & Society , 13 ( 5 ) : 824 – 842 . Brent Hecht and Monica Stephens . 2014 . A tale of cities : Urban biases in volunteered geographic in - formation . In Proceedings of the International Con - ference on Web and Social Media ( ICWSM ) , pages 197 – 205 , Menlo Park , California . AAAI Publica - tions . Brent Hecht , Lichan Hong , Bongwon Suh , and Ed H Chi . 2011 . Tweets from Justin Bieber’s heart : the dynamics of the location ﬁeld in user proﬁles . In Proceedings of Human Factors in Computing Sys - tems ( CHI ) , pages 237 – 246 . Liangjie Hong , Amr Ahmed , Siva Gurumurthy , Alexander J . Smola , and Kostas Tsioutsiouliklis . 2012 . Discovering geographical topics in the twitter stream . In Proceedings of the Conference on World - Wide Web ( WWW ) , pages 769 – 778 , Lyon , France . Dirk Hovy and Anders Søgaard . 2015 . Tagging per - formance correlates with author age . In Proceed - ings of the Association for Computational Linguis - tics ( ACL ) , pages 483 – 488 , Beijing , China . Dirk Hovy . 2015 . Demographic factors improve clas - siﬁcation performance . In Proceedings of the Asso - ciation for Computational Linguistics ( ACL ) , pages 752 – 762 , Beijing , China . Juhi Kulshrestha , Farshad Kooti , Ashkan Nikravesh , and Krishna P . Gummadi . 2012 . Geographic Dis - section of the Twitter Network . In Proceedings of the International Conference on Web and Social Me - dia ( ICWSM ) , Menlo Park , California . AAAI Publi - cations . Amanda Lenhart . 2015 . Mobile access shifts social media use and other online activities . Technical re - port , Pew Research Center , April . P . A . Longley , M . Adnan , and G . Lansley . 2015 . The geotemporal demographics of twitter usage . Envi - ronment and Planning A , 47 ( 2 ) : 465 – 484 . Momin Malik , Hemank Lamba , Constantine Nakos , and J¨urgen Pfeffer . 2015 . Population bias in geo - tagged tweets . In Papers from the 2015 ICWSM Workshop on Standards and Practices in Large - Scale Social Media Research , pages 18 – 27 . The AAAI Press . Alan Mislove , Sune Lehmann , Yong - Yeol Ahn , Jukka - Pekka Onnela , and J . Niels Rosenquist . 2011 . Un - derstanding the demographics of twitter users . In Proceedings of the International Conference on Web and Social Media ( ICWSM ) , pages 554 – 557 , Menlo Park , California . AAAI Publications . Burt L Monroe , Michael P Colaresi , and Kevin M Quinn . 2008 . Fightin’words : Lexical feature se - lection and evaluation for identifying the content of political conﬂict . Political Analysis , 16 ( 4 ) : 372 – 403 . 2147 Dong Nguyen , Dolf Trieschnigg , A Seza Dogru¨oz , Ri - lana Gravel , Mari¨et Theune , Theo Meder , and Fran - ciska de Jong . 2014 . Why gender and age predic - tion from tweets is hard : Lessons from a crowd - sourcing experiment . In Proceedings of the Inter - national Conference on Computational Linguistics ( COLING ) , pages 1950 – 1961 . Brendan O’Connor , Jacob Eisenstein , Eric P . Xing , and Noah A . Smith . 2010 . A mixture model of demo - graphic lexical variation . In Proceedings of NIPS Workshop on Machine Learning for Social Comput - ing , Vancouver . Pew Research Center . 2012 . Social networking popu - lar across globe . Technical report , December . Barbara Poblete , Ruth Garcia , Marcelo Mendoza , and Alejandro Jaimes . 2011 . Do all birds tweet the same ? characterizing Twitter around the world . In Proceedings of the International Conference on Information and Knowledge Management ( CIKM ) , pages 1025 – 1030 . ACM . Delip Rao , David Yarowsky , Abhishek Shreevats , and Manaswi Gupta . 2010 . Classifying latent user at - tributes in twitter . In Proceedings of Workshop on Search and mining user - generated contents . Stephen Roller , Michael Speriosu , Sarat Rallapalli , Benjamin Wing , and Jason Baldridge . 2012 . Super - vised text - based geolocation using language mod - els on an adaptive grid . In Proceedings of Em - pirical Methods for Natural Language Processing ( EMNLP ) , pages 1500 – 1510 . Sara Rosenthal and Kathleen McKeown . 2011 . Age prediction in blogs : A study of style , content , and online behavior in pre - and Post - Social media gen - erations . In Proceedings of the Association for Com - putational Linguistics ( ACL ) , pages 763 – 772 , Port - land , OR . Thamar Solorio and Yang Liu . 2008 . Learning to pre - dict code - switching points . In Proceedings of Em - pirical Methods for Natural Language Processing ( EMNLP ) , pages 973 – 981 , Honolulu , HI , October . Association for Computational Linguistics . Svitlana Volkova and Benjamin Van Durme . 2015 . Online bayesian models for personal analytics in so - cial media . In Proceedings of the National Confer - ence on Artiﬁcial Intelligence ( AAAI ) , pages 2325 – 2331 . Benjamin Wing and Jason Baldridge . 2011 . Sim - ple supervised document geolocation with geodesic grids . In Proceedings of the Association for Com - putational Linguistics ( ACL ) , pages 955 – 964 , Port - land , OR . Sarita Yardi , Daniel Romero , Grant Schoenebeck , et al . 2009 . Detecting spam in a twitter network . First Monday , 15 ( 1 ) . Kathryn Zickuhr . 2013 . Location - based services . Technical report , Pew Research Center , Septmeber . Nicole Zillien and Eszter Hargittai . 2009 . Digital distinction : Status - speciﬁc types of internet usage * . Social Science Quarterly , 90 ( 2 ) : 274 – 291 . 2148