Why Transformers Need Adam : A Hessian Perspective Yushun Zhang ∗ 1 , 2 , Congliang Chen †1 , 2 , Tian Ding ‡2 , Ziniu Li § 1 , 2 , Ruoyu Sun ♯ 1 , 2 , and Zhi - Quan Luo ♢ 1 , 2 1 The Chinese University of Hong Kong , Shenzhen , China 2 Shenzhen Research Institute of Big Data Abstract SGD performs worse than Adam by a significant margin on Transformers , but the reason remains unclear . In this work , we provide an explanation of SGD’s failure on Transformers through the lens of Hessian : ( i ) Transformers are “heterogeneous” : the Hessian spectrum across parameter blocks vary dramatically , a phenomenon we call “block heterogeneity " ; ( ii ) Heterogeneity hampers SGD : SGD performs badly on problems with block heterogeneity . To validate that heterogeneity hampers SGD , we check various Transformers , CNNs , MLPs , and quadratic problems , and find that SGD works well on problems without block heterogeneity but performs badly when the heterogeneity exists . Our initial theoretical analysis indicates that SGD fails because it applies one single learning rate for all blocks , which cannot handle the heterogeneity among blocks . The failure could be rescued if we could assign different learning rates across blocks , as designed in Adam . 1 Introduction Transformers [ Vaswani et al . , 2017 ] have become a major workhorse behind much remarkable progress in AI development ( e . g . , [ Achiam et al . , 2023 ] ) . However , the understanding of Transformer training remains limited . For instance , Transformer training largely relies on Adam ( W ) [ Kingma and Ba , 2014 , Loshchilov and Hutter , 2017 ] . In contrast , Stochastic Gradient Descent with momentum ( SGD ) 1 , which is the de - facto optimizer for convolution neural networks [ LeCun et al . , 1998 ] ( CNNs ) , performs poorly on Transformers ( see Figure 10 and 11 in Appendix D as evidence ) . Yet , the cause of failure remains unclear . It is an intriguing question why SGD fails on Transformers . First , from a theoretical perspective , this topic helps reveal more properties of the Transformer training loss landscape , which helps boost understanding of training objectives and current optimizers . Second , from a computational perspective , Adam requires heavier memory storage than SGD . Compared with SGD , Adam needs to store an additional second - order momentum , which takes at least the same memory of the model size . The memory consumption of Adam has become a major overhead in large - scale transformer training [ Rasley et al . , 2020 ] . Identifying the cause of failure may help researchers design more efficient optimizers . In this work , we explore the failure mode of SGD through the lens of Hessian . We start by investigating the full Hessian spectrum of Transformers , i . e . , the full eigenvalue density of Hessian ( see Figure 1 as ∗ Email : yushunzhang @ link . cuhk . edu . cn . Our code is available at https : / / github . com / zyushun / hessian - spectrum . † Email : congliangchen @ link . cuhk . edu . cn ‡ Email : dingtian @ sribd . cn § Email : ziniuli @ link . cuhk . edu . cn ♯ Correspondence author . Email : sunruoyu @ cuhk . edu . cn ♢ Email : luozq @ cuhk . edu . cn 1 We introduce the update rules of Adam ( W ) and SGD in Appendix B . 1 . 1 a r X i v : 2402 . 16788v1 [ c s . L G ] 26 F e b 2024 10 11 10 8 10 5 10 2 10 1 VGG16 ( 138M ) 10 11 10 8 10 5 10 2 10 1 ResNet18 ( 11M ) 10 11 10 7 10 3 10 1 GPT2 ( 125M ) 10 14 10 10 10 6 10 2 10 2 GPT2 - nano ( 11M ) 1 . 5 1 . 0 0 . 5 0 . 0 0 . 5 1 . 0 1 . 5 10 11 10 8 10 5 10 2 10 1 ViT - base ( 86M ) ( a ) Initialization 10 13 10 9 10 5 10 1 10 3 VGG16 ( 138M ) 10 11 10 8 10 5 10 2 10 1 ResNet18 ( 11M ) 10 11 10 7 10 3 10 1 GPT2 ( 125M ) 10 9 10 6 10 3 10 0 10 3 GPT2 - nano ( 11M ) 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 10 11 10 8 10 5 10 2 10 1 ViT - base ( 86M ) ( b ) 25 % steps 10 13 10 9 10 5 10 1 10 3 VGG16 ( 138M ) 10 11 10 8 10 5 10 2 10 1 ResNet18 ( 11M ) 10 11 10 7 10 3 10 1 GPT2 ( 125M ) 10 9 10 6 10 3 10 0 10 3 GPT2 - nano ( 11M ) 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 10 11 10 8 10 5 10 2 10 1 ViT - base ( 86M ) ( c ) 50 % steps 10 13 10 9 10 5 10 1 10 3 VGG16 ( 138M ) 10 11 10 8 10 5 10 2 10 1 ResNet18 ( 11M ) 10 11 10 7 10 3 10 1 GPT2 ( 125M ) 10 9 10 6 10 3 10 0 10 3 GPT2 - nano ( 11M ) 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 10 11 10 8 10 5 10 2 10 1 ViT - base ( 86M ) ( d ) 100 % steps Figure 1 : The full Hessian spectra of CNNs ( VGG16 and ResNet18 ) and Transformers ( GPT2 , GPT2 - nano , and ViT - base ) at different training stages . The x - axis records the eigenvalues and the y - axis records the frequency in the log scale . To allow comparison in the same figure , the plotted spectra are normalized by their 10 - th largest eigenvalues . We find that the spectra on CNNs and Transformers are largely similar . an example ) . By theory , the full Hessian spectrum largely determines the behavior of gradient methods [ Nesterov , 2013 , Goh , 2017 , Sun , 2019 , Goujaud et al . , 2022 ] , so we suspect it can also help uncover the reasons behind SGD failure on Transformers . Using numerical linear algebra tools [ Bai et al . , 1996 ] , we empirically compare the full spectra of CNNs ( the scenarios where SGD is on par with Adam ) and those of Transformers ( where SGD largely lags behind Adam ) . Unfortunately , in Figure 1 , we find that the spectra on CNNs and Transformers are often largely similar despite the different optimizer behaviors on these tasks . As such , we have not identified critical phenomena in the full Hessian spectra associated with SGD’s failure on Transformers . To reveal the cause of failure , we need a more fine - grained investigation into Hessian . What else could make SGD fail on Transformers , but not on CNNs ? By dissecting the structures of CNNs and Transformers , we observe that CNN architectures are constructed by the repetitive stacking of similar parameter blocks ( convolution layers ) , while Transformers involve the interleaved stacking of disparate parameter blocks ( e . g . Query , Key , Value blocks in attention and MLP layers ) . We suspect that these design differences may lead to different properties in optimization . Intuitively , disparate parameter blocks would contribute differently to the total loss . So each block might prefer a distinct treatment by optimizers , which can be provided by Adam but not by SGD . This motivates us to investigate the Hessian spectrum of each parameter block , abbreviated as the blockwise Hessian spectrum . By inspecting the blockwise Hessian spectrum , we find a possible explanation why SGD fails on Trans - formers . We find that Transformers are “heterogeneous " : for all Transformers we checked , Hessian spectra of different parameter blocks are dramatically different . We then verify that such heterogeneity indeed hampers SGD via extensive experiments on various Transformers , CNNs , MLPs , and quadratic problems . Our contributions are summarized as follows . • Exploring the failure mode of SGD on transformers . We provide an explanation why SGD largely lags behind Adam on Transformers through the lens of blockwise Hessian spectrum . This explanation consists of two parts . First , we identify a phenomenon called “block heterogeneity " , which states that Hessian spectra are dramatically different across parameter blocks . We find this phenomenon prevalent on all Transformers in our experiments , but not on CNNs . Second , we verify that block heterogeneity hampers SGD . More specifically , on various kinds of Transformers , CNNs , MLPs , we 2 find that SGD always performs badly on problems with block heterogeneity but works well otherwise . • Theoretical results on quadratic models . We build various convex quadratic problems , some with block heterogeneity and others without block heterogeneity . We find that GD is significantly worse than Adam on problems with block heterogeneity , but performs similarly to Adam otherwise . We provide a theoretical result which shows that GD indeed works badly on quadratic problems with block heterogeneity . Our analysis suggests that one important reason for ( S ) GD failure is that it uses one single learning rate for all blocks , which cannot handle the heterogeneity among blocks . The failure could be saved if we assign different learning rates across blocks like Adam . 2 Main Results 2 . 1 Problem Settings Notations . We denote the full - batch training loss as L ( w ) ≡ 1 n ∑ ni = 1 L i ( w ) , where n is the number of minibatches , L i ( w ) is the loss of i - th minibatch and w ∈ R d is the neural network parameters . We denote the gradient and Hessian of the training loss w . r . t . neural network parameters as ∇L ( w ) ∈ R d and ∇ 2 L ( w ) ∈ R d × d , respectively . We use ∇L i ( w ) ∈ R d and ∇ 2 L i ( w ) ∈ R d × d to denote the i - th minibatch counterparts . We use [ d ] to denote the index set { 1 , 2 , · · · , d } . Given an arbitrary partition { D l } Ll = 1 over [ d ] with d l ≜ | D l | , we can split w into L parameter blocks { w l } Ll = 1 , where w l = R d l consists of parameters with indexes in the l - th block D l . We denote [ ∇ 2 L ( w ) ] l ∈ R d l × d l as the Hessian of l - th parameter - block w l , where [ ∇ 2 L ( w ) ] l , i , j = ∂ 2 ∂ wl , i ∂ wl , j L ( w l ) . Note that [ ∇ 2 L ( w ) ] l is the l - th principle block sub - matrix of ∇ 2 L ( w ) . Preliminaries . Hessian of large - scale NNs are intractable to obtain due to the numerical difficulties of computing and storing them . Fortunately , there are extensive numerical linear algebra methods to approximate the Hessian spectrum without calculating the Hessian explicitly . Among which , the Stochastic Lanczos Quadrature method ( SLQ ) [ Bai et al . , 1996 ] is known to be one of the oldest yet the most powerful methods . SLQ is a combination of the stochastic trace estimator [ Hutchinson , 1989 ] , the Gaussian quadrature [ Golub and Meurant , 2009 , Epperson , 2013 ] , and the Lanczos algorithm [ Lanczos , 1950 ] . In short , SLQ takes a handful of Hessian - vector products as input , where the vectors are sampled from the normalized Rademacher distribution . It returns a smooth curve on R that approximates the histograms of eigenvalues ( see Figure 1 as an example ) . Since the derivation of SLQ is quite involved , we defer the detailed description of SLQ to Appendix B . 2 . The algorithmic form of SLQ can be seen in Algorithm 5 in the Appendix . We provide running time analysis and detailed setup of SLQ in Appendix C . 1 . In this work , we apply SLQ to various CNNs and Transformers on different datasets . As for CNNs , we study CNNs including ResNet18 ( 11M ) and VGG16 ( 138M ) on ImageNet [ He et al . , 2016 , Simonyan and Zisserman , 2014 ] . As for Transformers , we study ViT - base ( 86M ) on ImageNet [ Dosovitskiy et al . , 2020 ] , BERT ( 40M ) on Cornell Movie - Dialogs Corpus [ Devlin et al . , 2018 ] , and GPT2 ( 125M ) on Openwebtext [ Radford et al . , 2019 ] . For a fair comparison with ResNet18 , we also consider smaller models like GPT2 - nano 2 ( 11M ) on English corpus . The largest model we conduct is GPT2 with 125 million parameters . Detailed experimental setups are shown in Appendix C . 1 . For all these Transformer - based tasks , SGD performs significantly worse than Adam ; while for CNN - based takes , Adam and SGD are similar . The performance comparison is shown in Figure 10 and 11 in the Appendix . We estimate the spectrum of ( 1 ) the full Hessian ∇ 2 L ( w ) and ( 2 ) the Hessian of all parameter blocks [ ∇ 2 L ( w ) ] l , l ∈ [ L ] . For ( 2 ) , we split w according to the default partition in PyTorch implementation , e . g . , Embedding layer , Query in 1st attention layer , Key in 1st attention layer , Value in 1st attention layer , etc . . Note that the term “block " is a bit different from the term “layer " . For instance , Query and Key could be in the same layer , but they are different parameter blocks . In the following contexts , we refer to “full 2 https : / / github . com / karpathy / nanoGPT / 3 Hessian spectrum " as the estimated spectrum of ∇ 2 L ( w ) and “blockwise Hessian spectrum " as the estimated spectrum of [ ∇ 2 L ( w ) ] l , l ∈ [ L ] . 2 . 2 Full Hessian Spectrum Is Not Informative Enough Motivated by classical theory , we start by investigating the full Hessian spectrum of Transformers . We choose to study the full Hessian spectrum for two reasons . First , as mentioned in Section 1 , the full Hessian spectrum is known to largely determine the behavior of gradient methods . For instance , the range of the spectrum will affect the convergence rate [ Nesterov , 2013 ] . Second , the full Hessian spectrum has been shown to be informative in explaining multiple phenomena in neural nets ( NNs ) , e . g . , how BatchNorm accelerates the training [ Ghorbani et al . , 2019 ] . Due to these reasons , we hypothesize that the full Hessian spectrum can also help uncover the reasons behind SGD failure on Transformers . We compare the full spectra of CNNs ( the scenarios where SGD is on par with Adam ) and those of Transformers ( where SGD largely lags behind Adam ) . The results are shown in Figure 1 . Unfortunately , our experiments show that the full Hessian spectrum might not be enough to explain the SGD failure on Transformers . We explain as follows . The primary information of the spectrum lies in its ( 1 ) dispersion , ( 2 ) shape , and ( 3 ) evolution along training . As for ( 1 ) , we find that eigenvalues are dispersed very similarly and we do not observe any notably large eigenvalues in the spectra of Transformers . So ( 1 ) does not seem to be related to the failure of SGD . We further inspect ( 2 ) and ( 3 ) . For all CNNs and Transformers in Figure 1 , we find similar phenomena : the shape of the spectrum is approximately symmetrical around 0 at initialization . As training proceeds , the majority of large negative eigenvalues disappear and the shape evolves into a combination of a “bulk " and some “outliers " . Since the spectral shape and evolution are quite similar on Transformers and CNNs , they cannot explain the failure of SGD on Transformers , either . In summary , we have not identified critical phenomena in the full Hessian spectra linked to SGD’s failure on Transformers . 2 . 3 Main Findings Through Blockwise Hessian Spectra We have shown that the full Hessian spectrum seems not sufficient for uncovering the reason why SGD fails on Transformers . What else could make SGD fail on Transformers , but not on CNNs ? We find two important features that are overlooked in the full Hessian spectrum analysis above . • ( 1 ) The Hessian structure is overlooked . There are numerical results showing that the Hessians are close to block - diagonal matrices on MLPs [ Collobert , 2004 , Roux et al . , 2007 , Martens and Grosse , 2015 ] . We restate their findings in Appendix C . 2 . Further , [ Collobert , 2004 , Section 7 ] theoretically shows that the block diagonal structure comes from ( i ) the layer - by - layer design in NNs and ( ii ) the Cross - Entropy loss . Following this line of work , we also observe a near block - diagonal Hessian in small Transformers in Figure 2 , where the variables in each principle block correspond to the parameters of each block in the Transformer . These results suggest that near block - diagonal Hessian might be common in NNs . • ( 2 ) The build - up rules of Transformers is overlooked . Transformers are built upon stacking of disparate parameter blocks , e . g . Query , Key , Value blocks in attention , and MLP layers . This is largely different from CNNs , which are constructed by the repetitive stacking of similar parameter blocks ( convolution layers ) . Since each parameter blocks in Transformers are designed differently , they might have different properties for optimization , which might further affect the optimizer behavior . Combining ( 1 ) and ( 2 ) , we hypothesize that the blockwise Hessian spectrum might provide some insights on distinguishing CNNs and Transformers . What extra information can be contained in the blockwise spectrum but not in the full spectrum ? By definition , the blockwise Hessians form the principal block sub - matrix of the full Hessian . For block diagonal matrices , blockwise Hessian encodes the location of eigenvalues , i . e . , which block does an eigenvalue ( of the full Hessian ) lie in , while the full spectrum ignores this . In the following , we study blockwise Hessian spectra on various models . We will show that blockwise spectra indeed carry more information than the full spectrum for distinguishing CNNs and Transformers . 4 ( a ) Initial Hessian ( 1 - layer ) ( b ) Initial Hessian ( 2 - layer ) ( c ) Initial Hessian ( 4 - layer ) Figure 2 : The initial Hessian of small Transformers with 1 , 2 , and 4 layers on a subset of Openwebtext . We take the absolute value of each entry to distinguish non - zero values ( including negatives ) from those near 0 . See implementation details in Appendix C . 2 . We observe a near block - diagonal structure in Hessian . We here demonstrate the shape of blockwise spectra in CNNs and Transformers . We sample four blocks per model and present the spectra in Figure 3 . In Transformers , the Hessian spectra of embedding , attention , and MLP blocks are largely different . In contrast , in CNNs , the spectra of convolution layers are similar . We further verify this observation for the rest of the parameter blocks . We calculate the Jensen - Shannon ( JS ) distance between two eigenvalue densities of all possible block pairs . We show the results in Figure 4 . 10 11 10 8 10 5 10 2 10 1 1st convolutional layer 10 11 10 8 10 5 10 2 10 1 2nd convolutional layer 10 11 10 8 10 5 10 2 10 1 3rd convolutional layer 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 10 11 10 8 10 5 10 2 10 1 4th convolutional layer ( a ) ResNet18 10 11 10 8 10 5 10 2 10 1 1st convolutional layer 10 11 10 8 10 5 10 2 10 1 2nd convolutional layer 10 11 10 8 10 5 10 2 10 1 3rd convolutional layer 0 . 25 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 1 . 25 1 . 50 10 11 10 8 10 5 10 2 10 1 4th convolutional layer ( b ) VGG16 10 11 10 8 10 5 10 2 10 1 Embedding layer 10 11 10 8 10 5 10 2 10 1 3rd Query 10 11 10 8 10 5 10 2 10 1 3rd Value 3 2 1 0 1 2 3 10 11 10 8 10 5 10 2 10 1 3rd MLP fc layer ( c ) GPT2 10 11 10 8 10 5 10 2 10 1 Embedding layer 10 11 10 8 10 5 10 2 10 1 2nd Query 10 11 10 8 10 5 10 2 10 1 2nd Value 1 . 5 1 . 0 0 . 5 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 10 11 10 8 10 5 10 2 10 1 3rd MLP fc layer ( d ) BERT 10 11 10 8 10 5 10 2 10 1 Embedding layer 10 11 10 8 10 5 10 2 10 1 1st attention block 10 11 10 8 10 5 10 2 10 1 6th attention block 1 . 5 1 . 0 0 . 5 0 . 0 0 . 5 1 . 0 1 . 5 10 11 10 8 10 5 10 2 10 1 6th MLP fc layer ( e ) ViT - base Figure 3 : The blockwise Hessian spectra of CNNs ( ResNet18 and VGG16 ) and Transformers ( GPT2 , BERT , and ViT - base ) at initialization . The x - axis records the eigenvalues and the y - axis records the frequency in the log scale . To allow comparison in the same figure , we here only present the spectra of 4 blocks in each model . The plotted spectra are normalized by their 10 - th largest eigenvalues . We find that the spectra in CNNs are similar among blocks , while the spectra in Transformers are different across blocks . The results in Figure 4 showed an interesting phenomenon : for all the Transformers we checked , the blockwise Hessian spectra are largely different across blocks . In the following , we refer to this phenomenon as “ block heterogeneity " . As a control group experiment , the blockwise Hessian spectra of CNNs are similar and the block heterogeneity is not observed . We refer to this phenomenon as “ block homogeneity " . These results indicate that block heterogeneity is informative in distinguishing CNNs and Transformers . Intuitively , the block - homogeneous Hessian in a CNN comes from the repetitively similar convolution layers , while the block - heterogeneous Hessian in a Transformer stems from the interleaved stacking disparate layers such as Query , Value , and MLPs . In the following , we will show that the block heterogeneity is strongly correlated with the failure of SGD on Transformers . 5 c o n v 1 l a y e r 1 . 0 . c o n v 1 l a y e r 1 . 0 . c o n v 2 l a y e r 1 . 1 . c o n v 1 l a y e r 1 . 1 . c o n v 2 l a y e r 2 . 0 . c o n v 1 l a y e r 2 . 0 . c o n v 2 l a y e r 2 . 1 . c o n v 1 l a y e r 2 . 1 . c o n v 2 l a y e r 3 . 0 . c o n v 1 l a y e r 3 . 0 . c o n v 2 l a y e r 3 . 1 . c o n v 1 l a y e r 3 . 1 . c o n v 2 l a y e r 4 . 0 . c o n v 1 l a y e r 4 . 0 . c o n v 2 l a y e r 4 . 1 . c o n v 1 l a y e r 4 . 1 . c o n v 2 f c conv1 layer1 . 0 . conv1 layer1 . 0 . conv2 layer1 . 1 . conv1 layer1 . 1 . conv2 layer2 . 0 . conv1 layer2 . 0 . conv2 layer2 . 1 . conv1 layer2 . 1 . conv2 layer3 . 0 . conv1 layer3 . 0 . conv2 layer3 . 1 . conv1 layer3 . 1 . conv2 layer4 . 0 . conv1 layer4 . 0 . conv2 layer4 . 1 . conv1 layer4 . 1 . conv2 fc 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( a ) ResNet18 e m b e dd i n g . t o k e n e m b e dd i n g . s e g m e n t e n c o d e r . 0 . q u e r y e n c o d e r . 0 . k e y e n c o d e r . 0 . v a l u e e n c o d e r . 0 . o u t p u t _ li n e a r e n c o d e r . 0 . f c 1 e n c o d e r . 0 . f c 2 e n c o d e r . 1 . q u e r y e n c o d e r . 1 . k e y e n c o d e r . 1 . v a l u e e n c o d e r . 1 . o u t p u t _ li n e a r e n c o d e r . 1 . f c 1 e n c o d e r . 1 . f c 2 n e x t _ s e n t e n c e . li n e a r m a s k _ l m . li n e a r embedding . token embedding . segment encoder . 0 . query encoder . 0 . key encoder . 0 . value encoder . 0 . output _ linear encoder . 0 . fc1 encoder . 0 . fc2 encoder . 1 . query encoder . 1 . key encoder . 1 . value encoder . 1 . output _ linear encoder . 1 . fc1 encoder . 1 . fc2 next _ sentence . linear mask _ lm . linear 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( b ) BERT t r a n s f o r m e r . w p e t r a n s f o r m e r . h . 0 . a tt n . c _ a tt n t r a n s f o r m e r . h . 0 . l n _ 2 t r a n s f o r m e r . h . 0 . m l p . c _ p r o j t r a n s f o r m e r . h . 1 . a tt n . c _ a tt n t r a n s f o r m e r . h . 1 . l n _ 2 t r a n s f o r m e r . h . 1 . m l p . c _ p r o j t r a n s f o r m e r . h . 2 . a tt n . c _ a tt n t r a n s f o r m e r . h . 2 . l n _ 2 t r a n s f o r m e r . h . 2 . m l p . c _ p r o j t r a n s f o r m e r . h . 3 . a tt n . c _ a tt n t r a n s f o r m e r . h . 3 . l n _ 2 t r a n s f o r m e r . h . 3 . m l p . c _ p r o j t r a n s f o r m e r . h . 4 . a tt n . c _ a tt n t r a n s f o r m e r . h . 4 . l n _ 2 t r a n s f o r m e r . h . 4 . m l p . c _ p r o j t r a n s f o r m e r . h . 5 . a tt n . c _ a tt n t r a n s f o r m e r . h . 5 . l n _ 2 t r a n s f o r m e r . h . 5 . m l p . c _ p r o j transformer . wpe transformer . h . 0 . attn . c _ attn transformer . h . 0 . ln _ 2 transformer . h . 0 . mlp . c _ proj transformer . h . 1 . attn . c _ attn transformer . h . 1 . ln _ 2 transformer . h . 1 . mlp . c _ proj transformer . h . 2 . attn . c _ attn transformer . h . 2 . ln _ 2 transformer . h . 2 . mlp . c _ proj transformer . h . 3 . attn . c _ attn transformer . h . 3 . ln _ 2 transformer . h . 3 . mlp . c _ proj transformer . h . 4 . attn . c _ attn transformer . h . 4 . ln _ 2 transformer . h . 4 . mlp . c _ proj transformer . h . 5 . attn . c _ attn transformer . h . 5 . ln _ 2 transformer . h . 5 . mlp . c _ proj 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( c ) GPT2 - nano f e a t u r e s . 0 f e a t u r e s . 2 f e a t u r e s . 5 f e a t u r e s . 7 f e a t u r e s . 10 f e a t u r e s . 12 f e a t u r e s . 14 f e a t u r e s . 17 f e a t u r e s . 19 f e a t u r e s . 21 f e a t u r e s . 24 f e a t u r e s . 26 f e a t u r e s . 28 c l a ss i f i e r . 0 c l a ss i f i e r . 3 c l a ss i f i e r . 6 features . 0 features . 2 features . 5 features . 7 features . 10 features . 12 features . 14 features . 17 features . 19 features . 21 features . 24 features . 26 features . 28 classifier . 0 classifier . 3 classifier . 6 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( d ) VGG16 p a t c h _ e m b e d . p r o j b l o c k s . 0 . a tt n . q k v b l o c k s . 0 . a tt n . p r o j b l o c k s . 0 . m l p . f c 1 b l o c k s . 0 . m l p . f c 2 b l o c k s . 6 . a tt n . q k v b l o c k s . 6 . a tt n . p r o j b l o c k s . 6 . m l p . f c 1 b l o c k s . 6 . m l p . f c 2 b l o c k s . 11 . a tt n . q k v b l o c k s . 11 . a tt n . p r o j b l o c k s . 11 . m l p . f c 1 b l o c k s . 11 . m l p . f c 2 h e a d patch _ embed . proj blocks . 0 . attn . qkv blocks . 0 . attn . proj blocks . 0 . mlp . fc1 blocks . 0 . mlp . fc2 blocks . 6 . attn . qkv blocks . 6 . attn . proj blocks . 6 . mlp . fc1 blocks . 6 . mlp . fc2 blocks . 11 . attn . qkv blocks . 11 . attn . proj blocks . 11 . mlp . fc1 blocks . 11 . mlp . fc2 head 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( e ) ViT - base w t e h . 0 . a tt n . w q h . 0 . a tt n . w k h . 0 . a tt n . w v h . 0 . a tt n . w o h . 0 . m l p . c _ f c h . 0 . m l p . c _ p r o j h . 3 . a tt n . w q h . 3 . a tt n . w k h . 3 . a tt n . w v h . 3 . a tt n . w o h . 3 . m l p . c _ f c h . 3 . m l p . c _ p r o j h . 7 . a tt n . w q h . 7 . a tt n . w k h . 7 . a tt n . w v h . 7 . a tt n . w o h . 7 . m l p . c _ f c h . 7 . m l p . c _ p r o j h . 11 . a tt n . w q h . 11 . a tt n . w k h . 11 . a tt n . w v h . 11 . a tt n . w o h . 11 . m l p . c _ f c h . 11 . m l p . c _ p r o j wte h . 0 . attn . wq h . 0 . attn . wk h . 0 . attn . wv h . 0 . attn . wo h . 0 . mlp . c _ fc h . 0 . mlp . c _ proj h . 3 . attn . wq h . 3 . attn . wk h . 3 . attn . wv h . 3 . attn . wo h . 3 . mlp . c _ fc h . 3 . mlp . c _ proj h . 7 . attn . wq h . 7 . attn . wk h . 7 . attn . wv h . 7 . attn . wo h . 7 . mlp . c _ fc h . 7 . mlp . c _ proj h . 11 . attn . wq h . 11 . attn . wk h . 11 . attn . wv h . 11 . attn . wo h . 11 . mlp . c _ fc h . 11 . mlp . c _ proj 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 ( f ) GPT2 Figure 4 : The JS distance among blockwise Hessian spectra for different models at initialization . We find that the JS distance of blockwise spectra in CNNs is significantly smaller than that in Transformers . 2 . 4 SGD Fails on Simple MLPs with Block Heterogeneity We now further link block heterogeneity to the failure of SGD . We construct a simple example without attention layers but has block heterogeneity and find that SGD still fails . We consider a 4 - layer MLP on MNIST . Using standard MLP , we find that the Hessian exhibits a homogeneous block structure and SGD converges as fast as Adam . We then scale the output of each layer by a constant c . We define c as the degree of heterogeneity since larger c would bring more heterogeneity to the Hessian ( see Table 3 in the Appendix ) . As shown in Figure 5 , we find that the gap between Adam and SGD increases with the degree of heterogeneity . This example shows that attention might not be the root cause of SGD failure , but the block heterogeneity ( brought by attention ) might be . 2 . 5 Implication on Choosing SGD or Adam The above findings could bring up an interesting empirical guidance : we can compute the blockwise spectrum of initial Hessian , and then decide whether to use Adam or SGD . Such a method could be useful in scenarios in training large models that are not mainstream Transformers or CNNs , e . g . , Mamba [ Gu and Dao , 2023 ] . In these cases , there is not much prior experience in choosing optimizers . It would be intriguing to decide whether SGD is suitable for the task before the training is launched . We present two reasons . • In large - scale model training , memory is limited but the model size grows , so Adam could become a major computational burden due to the memory of 2nd - order momentum ( as mentioned in Section 1 and [ Rasley et al . , 2020 ] ) . 6 0 10 20 30 40 Iteration 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 T e s t A cc u r a c y c = 1 c = 8 c = 9 c = 10 c = 11 c = 12 c = 13 c = 14 c = 15 ( a ) Training curves of Adam 0 10 20 30 40 Iteration 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 T e s t A cc u r a c y c = 1 c = 8 c = 9 c = 10 c = 11 c = 12 c = 13 c = 14 c = 15 ( b ) Training curves of SGD 1 8 9 10 11 12 13 14 15 Degree of block Heterogeneity c 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 F i n a l T e s t A cc u r a c y SGDAdam ( c ) Final performance of Adam and SGD Figure 5 : The performance of SGD and Adam on MNIST with 4 - layer MLPs under different degrees of block heterogeneity c . In ( c ) , each point records the best - converged test accuracy under the learning rate grid search . See more details in Appendix C . 3 . We observe that SGD performs worse as heterogeneity increases . • SGD is a lightweight alternative to Adam , but the performance of SGD may be bad . In large NN training , it might take days ( or even weeks ) to obtain informative feedback from training logs , so it is expensive to conduct trial and error to choose an appropriate optimizer . Specifically , if SGD performs badly , it would be difficult to judge whether the bad performance is due to improper learning rates or other more fundamental reasons . Then , it would be hard to decide whether to switch the optimizer . Motivated by the above reasons , we here propose a quantitative metric that could predict the failure of SGD before the training is launched . With the help of this metric , we could save much expense on the trial and error for SGD . The metric is simply the averaged JS distance among blockwise Hessian spectra at initialization , i . e . , the averaged value in each heatmap of Figure 4 . We denote it as JS 0 . We present JS 0 of various models in Table 1 . We note that JS 0 establishes a quantitative difference between the loss landscape of Transformers and CNNs . As argued in the previous sections , this quantity can predict the failure of SGD . Note that JS 0 is independent of optimizers and could be efficiently checked in priori before training . Model ResNet18 VGG16 ViT - base BERT GPT2 - nano GPT2 JS 0 0 . 10 0 . 09 286 . 41 53 . 38 14 . 93 83 . 23 Table 1 : JS 0 denotes the average JS distance between the initial Hessian spectra of each pair of parameter blocks . We find that the JS 0 on CNNs is at least 140 × smaller than on Transformers . JS 0 establishes a quantitative difference between the loss landscape of Transformers and CNNs . To summarize this section , we provide an explanation for SGD’s failure . We identify a phenomenon called block heterogeneity . This phenomenon naturally exists in Transformers , but not in CNNs . This phenomenon can exist in non - Transformer architectures as well . Our experiments suggest a strong connection between the failure of SGD and block heterogeneity . Our findings could provide empirical guidance to decide whether to use Adam or SGD . 3 Case Study of Quadratic Models and Preliminary Theory Now we construct quadratic functions with block diagonal Hessian . We will compare the performance of GD and Adam on Hessian with and without block heterogeneity . Initial theoretical results on these quadratic models will be provided . Note that insights on quadratic models could be important for understanding realistic NNs , as mentioned by researchers such as LeCun et al . [ 2002 ] and OpenAI team [ Kaplan et al . , 2020 ] . 7 3 . 1 Settings and Additional Notations We consider min w L ( w ) = 12 w T Hw − h T w where H ∈ R d × d is positive definite and h ∈ R d . We denote L ∗ as the minimum value of L ( w ) . We set H as a block diagonal matrix : H = diag ( H 1 , · · · , H L ) , where H l ∈ R d l × d l and d = ∑ Ll = 1 d l . We use w l ∈ R d l to denote the variable in the l - th block and w = ( w T 1 , · · · , w TL ) T ∈ R d . Similarly for h l ∈ R d l . Similarly , we use [ ∇ L ( w ) ] l ∈ R d l to denote the gradient in the l - th block and denote [ L ( w ) ] l = 12 ( w tl ) T H l w tl − h Tl w l as the objective function w . r . t . the l - th block . Note that L ( w ) = ∑ Ll = 1 [ L ( w ) ] l . We denote λ 1 ≥ λ 2 · · · ≥ λ d as the eigenvalues of H . Similarly for λ l , 1 · · · λ l , d l . We denote κ = λ 1 λ d and κ l = λ l , 1 λ l , dl as the condition number of H and H l , respectively . We say an algorithm has complexity ˜ O ( C ) if it takes O ( C log ( 1 / ϵ ) ) iterations to achieve error L ( w ) −L ∗ L ( w 0 ) −L ∗ ≤ ϵ , where w 0 is the initial point . 3 . 2 Experimental Observations For all experiments , we choose h = 0 . We consider four types of Hessian H as follows . • Case 1 : Hessian with Transformer - type spectra . We choose L = 4 and d l = 100 . For l ∈ [ L ] , we construct H l = Q l Λ l Q Tl where Q l are independent standard Gassian random matrix and Λ l are diagonal matrices . For the diagonal elements in Λ l , we sample d l numbers according to the spectrum of the embedding layer ; 3rd Query , 3rd Value , 3rd MLP ( fc layer ) in GPT2 . Shifting and proportional scaling are performed to ensure all elements in Λ l lie in the interval [ 1 , 2000 ] . This ensures strong convexity and controls the condition number of H equals 2000 . The spectra of H l are in Figure 6 . • Case 2 : Hessian with CNN - type spectra . We consider the same setup as Case 1 . For the diagonal elements in Λ l , we sample d l numbers according to the spectrum of the 1st to 4th convolution layers in ResNet18 . We shift and proportionally scale the elements Λ l to the interval [ 1 , 2000 ] to ensure the strong convexity and the condition number H equals 2000 . The spectra of H l are shown in Figure 7 . • Case 3 : Hessian with simplified heterogeneous spectra . We choose L = 3 and d l = 3 . For l ∈ [ L ] , we construct H l = Q l Λ l Q Tl where Q l are independent standard Gassian random matrix and Λ l are diagonal matrices . We set the diagonal elements of Λ l as { 1 , 2 , 3 } , { 99 , 100 , 101 } , { 1998 , 1999 , 2000 } for l = 1 , 2 , 3 , respectively . The spectra of H l are different due to their different supports . The condition number of Hessian H is 2000 . • Case 4 : Hessian with simplified homogeneous spectra . We consider the same setup as Case 3 . We set the diagonal elements of Λ l as { 1 , 99 , 1998 } , { 2 , 100 , 1999 } , { 3 , 101 , 2000 } for l = 1 , 2 , 3 , respectively . The spectra of H l are similar . The condition number of Hessian H is 2000 . Note that in all four cases , the Hessian share the same condition number . We study two types of optimizers : one that assigns a single learning rate for all blocks , and ones that assign different learning rates across blocks . The optimizers are introduced as follows . • Single - learning - rate optimizer . We study gradient descent ( GD ) . w t + 1 = w t − η ∇L ( w ) = w t − η ( Hw t − h ) ( 1 ) We use the optimal learning rate η = 2 µ + L [ Nesterov , 2013 ] . We use a standard Gaussian random vector as initialization . • Coordinate - wise - learning - rate optimizer . We study Adam with a constant learning rate and with no bias correction for simplicity ( Algorithm 3 ) . We set β 1 = 0 to erase the effect of momentum . This helps us to focus on the effect of coordinate - wise learning rate ( or the effect of diagonal preconditioning ) in Adam . We use ϵ = 0 . We consider β 2 = 1 and β 2 = 0 . 999 , respectively . When β 2 = 1 , Adam assigns 8 ( a ) Spectrum of H 1 ( b ) Spectrum of H 2 ( c ) Spectrum of H 3 ( d ) Spectrum of H 4 Figure 6 : Histogram of eigenvalues of each block in Case 1 ( the heterogeneous case ) . The eigenvalues in the four blocks are sampled from the spectrum of the embedding layer ; 3rd Query , 3rd Value , 3rd MLP ( fc layer ) in GPT2 , respectively . All the eigenvalues are shifted and proportionally scaled such that : the objective function is strong convex ; the condition number of Hessian equals 2000 ; their relative ranges are preserved ; and the block heterogeneity is preserved . ( a ) Spectrum of H 1 ( b ) Spectrum of H 2 ( c ) Spectrum of H 3 ( d ) Spectrum of H 4 Figure 7 : Histogram of eigenvalues of each block in Case 2 ( the homogeneous case ) . The eigenvalues in the four blocks are sampled from the spectrum of 1st to 4th convolution layers in ResNet18 , respectively . All the eigenvalues are shifted and proportionally scaled such that : the objective function is strong convex ; the condition number of Hessian equals 2000 ; their relative ranges are preserved ; and the block homogeneity is preserved . coordinate - wise learning rates according to the initial gradient , but these learning rates are fixed along iteration . The update rule is as follows . w t + 1 = w t − η ( D 0 Adam ) − 1 ∇L ( w ) = w t − η ( D 0 Adam ) − 1 ( Hw t − h ) , ( 2 ) where D 0 Adam = diag ( ∇L ( w 0 ) ◦ ∇L ( w 0 ) ) 12 and ∇L ( w 0 ) = Hw 0 − h . When β 2 < 1 , the coordinate - wise learning rates adaptively change along iteration . The update rule is as follows . w t + 1 = w t − η ( D tAdam ) − 1 ∇L ( w ) = w t − η ( D tAdam ) − 1 ( Hw t − h ) , ( 3 ) where D tAdam = diag (cid:16) ( 1 − β 2 ) (cid:16) ∑ tk = 1 β t − k 2 ∇L ( w k ) ◦ ∇L ( w k ) (cid:17) + β t diag ( ∇L ( w 0 ) ◦ ∇L ( w 0 ) ) (cid:17) 12 and ∇L ( w k ) = Hw k − h . We grid search η and use a standard Gaussian random vector as initialization . We remark that when β 2 < 1 , Adam would repeatedly bounce among non - optimal points , causing non - convergence . This will be shown in Proposition 2 . Summary of experimental observations . We now compare the performance of Adam and GD on these four types of Hessians . The results are shown in Figure 8 . For Hessian with heterogeneous blocks like Case 1 and 3 , GD largely lags behind Adam . For Hessian with homogeneous blocks ( Case 2 and 4 ) , GD is on par or even better than Adam . We emphasize that all Hessians have the same condition number in these four cases . Further , Hessian of Case 3 and 4 share all the eigenvalues ( not just the extreme ones ) . The 9 ( a ) Hessian with GPT2 block - wise spectrum ( b ) Hessian with ResNet18 blockwise spectrum 0 1000 2000 3000 4000 5000 Iteration 10 160 10 141 10 122 10 103 10 84 10 65 10 46 10 27 10 8 L o g G r a d i e n t M o r m GD Adam with 2 = 0 . 999 Adam with 2 = 1 ( c ) Hessian with simplified het - erogeneous blocks 0 1000 2000 3000 4000 5000 Iteration 10 160 10 141 10 122 10 103 10 84 10 65 10 46 10 27 10 8 L o g G r a d i e n t M o r m GD Adam with 2 = 0 . 999 Adam with 2 = 1 ( d ) Hessian with simplified ho - mogeneous blocks Figure 8 : The performance of Adam and GD on quadratic problems . For the construction of Hessian : ( a ) and ( b ) use the blockwise spectrum of GPT2 and ResNet18 , respectively ; ( c ) and ( d ) use the implied spectrum described in Case 3 and 4 in Section 3 . The condition numbers of Hessian equal to 2000 for all four cases . For Adam , we use β 1 = 0 to erase the effect of momentum . We find that : when blocks are heterogeneous , GD largely lags behind Adam ; when blocks are homogeneous , GD performs similarly to Adam . performance gap between Adam and GD is due to the different blockwise spectra caused by the different locations of eigenvalues . We hypothesize that GD performs badly because it uses one single learning rate for all blocks , which cannot handle the heterogeneity among blocks . Such heterogeneity can be better handled using different learning rates across blocks , as designed in Adam . 3 . 3 Initial Theoretical Results We now provide initial theoretical results to characterize how GD lags behind Adam in problems with heterogenous Hessian . Note that classical optimization theory depicts the rate of first - order methods by the condition number of the full Hessian κ . However , we point out that κ is not informative enough to describe the performance gap in Figure 8 since κ is the same in all four cases . To distinguish Adam and GD , we need to utilize more fine - grained quantities like blockwise spectra of sub - matrices . Unfortunately , the blockwise spectrum is rarely discussed in the optimization area . A most related notion is “block Lipschitz constant " [ Beck and Tetruashvili , 2013 ] for studying block coordinate descent ( BCD ) type methods , but it was not linked to the performance of SGD or Adam before . To our knowledge , we are not aware of any theory of Adam or GD built on the block diagonal structures or the blockwise spectra of Hessian . Further , there seems to be no result distinguishing the complexity of single - learning - rate methods and the coordinate - wise counterpart , either . We now make an initial attempt in this direction . We first present the lower bound for GD . Proposition 1 . ( Lower bound for GD . ) Consider min w L ( w ) = 12 w T Hw − h T w where H ∈ R d × d is positive definite and h ∈ R d . Let w tGD be the output of GD after t steps . There exists a block diagonal matrix H , h and an initial point w 0 , s . t . , for any η , we have : L ( w t + 1 GD ) − L ∗ ≥ (cid:18) 1 − 2 κ + 1 (cid:19) (cid:0) L ( w tGD ) − L ∗ (cid:1) ( 4 ) where κ is the condition number of H . Proposition 1 shows that GD has complexity ˜ O ( κ ) and such complexity is tight . Now we prove that : if we are allowed to choose a customized learning rate for each block sub - matrix H l , then the complexity will be improved . We call this method blockwise GD ( BGD ) . Note that BGD is impractical to implement since it is hard to choose L learning rates in advance . BGD serves as an artificial method for theoretical understanding . Blockwise GD ( BGD ) . Choose learning rates { η l } L l = 1 for all blocks in advance and performs the following : w t + 1 = w t − ( D BGD ) − 1 ∇L ( w ) = w t − ( D BGD ) − 1 ( Hw t − h ) , 10 where D BGD = diag     η 1 , · · · , η 1 (cid:124) (cid:123)(cid:122) (cid:125) d 1 times , · · · , η L , · · · , η L (cid:124) (cid:123)(cid:122) (cid:125) d L times   T   ∈ R d × d . BGD can be also viewed as a diagonally preconditioned version of GD . We now show that this diagonal preconditioner can significantly improve the complexity of GD . Theorem 1 . ( Upper bound for GD with blockwise learning rate . ) Consider min w L ( w ) = 12 w T Hw − h T w where H ∈ R d × d is positive definite and h ∈ R d . We assume H is a block diagonal matrix , i . e . , H = diag ( H 1 , · · · , H L ) , where H l ∈ R d l × d l and d = ∑ Ll = 1 d l . Let w tBGD be the output of BGD after t steps . There exists a set of { η l } Ll = 1 such that : L ( w t + 1 BGD ) − L ∗ ≤ max l ∈ [ L ] (cid:18) 1 − 1 κ l (cid:19) (cid:0) L ( w tBGD ) − L ∗ (cid:1) ( 5 ) where κ l is the condition number of H l . Theorem 1 shows that : if we are allowed to choose a learning rate for each H l , the complexity could be ˜ O ( max l ∈ [ L ] κ l ) , in lieu of ˜ O ( κ ) as in GD . That is to say , the rate of BGD only depends on the " slowest block " . Note that in certain heterogeneous block Hessian such Case 1 and 3 , κ l is about 20 × and 2000 × smaller than κ , respectively . In these cases , blockwise - learning - rate methods can be about 20 × and 2000 × faster than single - learning - rate methods like GD , respectively . Despite the advantages of BGD , it is impractical to find the proper learning rates for all blocks , especially when the number of blocks L is large . Fortunately , these learning rates could be approximated by the preconditioner in Adam , e . g . , D 0 Adam in ( 2 ) . We now provide a complexity bound of Adam . Assumption 1 . ( Bounded initialization . ) The initialization is bounded and is far from the optimal solution : C l , 1 λ l , 1 ≤ (cid:12)(cid:12)(cid:12) [ ∇L ( w 0 ) ] l , i (cid:12)(cid:12)(cid:12) ≤ C l , 2 λ l , 1 , for i ∈ [ d l ] , l ∈ [ L ] . Theorem 2 . ( Upper bound for Adam with β 2 = 1 . ) Consider the same setting as in Theorem 1 and consider Adam with β 1 = 0 and β 2 = 1 as in ( 2 ) . Assume the initialization satisfies Assumption 1 . Let w tAdam be the output of Adam after t steps . Let η = min l ∈ [ L ] 1 C l , 1 . We have L ( w t + 1 Adam ) − L ∗ ≤ max l ∈ [ L ] (cid:18) 1 − 1 κ Adam , l (cid:19) (cid:0) L ( w tAdam ) − L ∗ (cid:1) ( 6 ) where κ Adam , l = r κ l , r : = max l ∈ [ L ] (cid:16) C l , 2 C l , 1 (cid:17) 2 and κ l is the condition number of H l . The proofs of all the above theorems are shown in Appendix E . Theorem 2 states that Adam ( with β 2 = 1 ) has complexity ˜ O (cid:16) r · max l ∈ [ L ] κ l (cid:17) . We remark that condition β 2 = 1 is necessary because any β 2 < 1 causes non - convergence issue [ Da Silva and Gazeau , 2020 , Bock and Weiß , 2019 ] . We restate their results in Proposition 2 . The non - convergence is also observed in Figure 8 ( c ) , where we find that the iterates of Adam quickly converge to near - optimal solutions , and then bounce back . As such , β 2 = 1 is necessary for asymptotic analysis . As shown in [ Da Silva and Gazeau , 2020 ] , the non - convergence is due to the constant learning rate . Proposition 2 . ( Non - convergence of constant - learning - rate Adam with β 2 < 1 . ) [ Da Silva and Gazeau , 2020 , Proposition 12 ] Consider min w ∈ R L ( w ) = 12 w 2 . Consider Adam with β 1 = 0 and β 2 < 1 as in ( 3 ) . Let w tAdam be the output of Adam after t steps . There exists a discrete limit cycle for ( 3 ) , i . e . , Adam produces oscillations and lim inf t → ∞ (cid:0) L ( w tAdam ) − L ∗ (cid:1) > 0 . We now compare the complexity of Adam and that of GD . By Theorem 2 , Adam is faster than GD when r · max l ∈ [ L ] κ l ≤ κ . In the quadratic model with heterogeneous blocks ( Case 3 ) , our simulation over 1000 trials shows that r ≤ 100 with probability ≥ 23 when using standard Gaussian random initialization . Since 11 max l ∈ [ L ] κ l ≈ 1 , we have r · max l ∈ [ L ] κ l ≤ 100 , w . h . p . , and is 20 × smaller than κ = 2000 . So Adam could be 20 × faster than GD , w . h . p . . This is indeed observed in Figure 8 where Adam outperforms GD by a significant margin . We summarize the complexity of GD , BGD , and Adam in Table 2 . Optimizer GD BGD Adam with Adam with β 1 = 0 and β 2 = 1 ( 2 ) β 1 = 0 and β 2 < 1 ( 3 ) Complexity ˜ O ( κ ) ˜ O ( max l ∈ [ L ] κ l ) ˜ O (cid:16) r · max l ∈ [ L ] κ l (cid:17) ✗ Table 2 : The complexity of GD , BGD and Adam for minimizing a strongly convex quadratic function with block diagonal Hessian . The symbol ✗ means non - convergence . κ and κ l denote the condition number of the full Hessian and the block submatrix , respectively . r : = max l ∈ [ L ] (cid:16) C l , 2 C l , 1 (cid:17) 2 and C l , 1 , C l , 2 are constants in Assumption 1 . How to obtain a tighter complexity bound of Adam ? It is valid to ask whether the complexity upper bound κ Adam , l = r κ l can be tightened , e . g . , remove the factor of r . We point out it would be difficult if there is no extra structure on H l . A key technical step is to bound the condition number of the preconditioned matrix κ (cid:16) ( D 0 Adam , l ) − 1 H l (cid:17) . Intuitively , a diagonal preconditioner of H l is expected to be powerful when H l itself has a near - diagonal structure , e . g . , pure diagonal , tridiagonal or diagonal dominant . Unfortunately , it is unclear whether these structures hold in real Transformers . Without any assumption on H l , we find that the diagonal preconditioner of D 0 Adam could increase the condition number . For instance , when using standard Gaussian initialization , in case 3 , we find κ (cid:16) ( D 0 Adam , l ) − 1 H l (cid:17) equals 3 . 9 κ 1 , 14 . 1 κ 2 , 4 . 2 κ 3 for the 3 blocks , respectively ( all averaged over 1000 trials ) . It would be interesting to explore if there are special structures of H l in real Transformers such that Adam preconditioner can reduce κ l , rather than increase it . We leave it as a future direction . Although Adam preconditioner might not always reduce the “local " condition number κ l , the coefficient in the complexity is now independent of the “global " condition number κ . As argued above , such changes in coefficient could lead to considerable improvement over GD . Such improvement in complexity is attributed to the block diagonal structure in Hessian as well as its heterogeneous blockwise spectrum . To our knowledge , such improvement is not shown in the existing literature . In summary , our theory indicates that : for problems with block heterogeneity , the single - learning rate methods like GD can largely lag behind coordinate - wise learning rate methods like Adam . 4 Related Works On The failure of SGD on Transformers There is extensive literature studying the difficulties of Trans - former training . We summarize these works in Appendix A . We here focus on the literature that explores why SGD fails on Transformers . One representative hypothesis is that SGD fails because it cannot handle the heavy - tailed stochastic noise in language tasks [ Zhang et al . , 2020 ] . However , Chen et al . [ 2021 ] , Kunstner et al . [ 2023 ] reported that the gap between Adam and SGD maintains even in the full - batch case with no stochasticity , so there might be other reasons . Further , SGD performs poorly on Vision Transformers on ImageNet ( Figure 11 ) , so the data modality ( e . g . , language or image tasks ) might not be as crucial as the architecture . Zhang et al . [ 2019c ] showed that Transformers have the “unbounded smoothness " issue and SGD with gradient clipping performs better than SGD in this case . Although clipping is an effective trick , it does not save SGD from failure as we still observe a huge gap between clipped SGD and Adam 3 . So there might be other reasons that hamper SGD . Different from these works , we find SGD fails because it uses one single learning rate for all blocks , which cannot handle the Hessian heterogeneity among blocks . 3 For all NLP tasks , clipping is performed after backpropagation . So in Figure 11 , SGD in NLP tasks essentially refers to clipped SGD . 12 Understanding of Adam . There was once a long - standing debate on the possible divergence of Adam [ Reddi et al . , 2018 ] . The convergence for the unmodified versions is later established in [ Shi et al . , 2020 , Zhang et al . , 2022b ] for RMSprop and Adam . More convergence analyses of general adaptive gradient methods are listed in Appendix A . We here focus on the literature that explores the benefit of Adam . Xie et al . [ 2022 ] show that Adam can help avoid saddle points , which is an orthogonal direction to this work . Wang et al . [ 2022a ] , Crawshaw et al . [ 2022 ] , Li et al . [ 2023 ] show that Adam and its variant outperform SGD under relaxed smoothness conditions , based on the intuition that Adam can adaptively change its learning rate along iteration ( over time ) . We pointed out that the theory is not complete : even for quadratic functions where the smoothness is fixed , SGD sometimes fails while Adam works well ( Figure 8 ) . This indicates that the benefit of Adam is not merely due to its ability to adaptively change the learning rate over time , and there are other reasons for Adam’s success . We show that an important benefit of Adam is its ability to handle the heterogeneity across blocks ( over space ) . Bernstein et al . [ 2018 ] , Wu et al . [ 2020 ] , Kunstner et al . [ 2023 ] , Liu et al . [ 2023 ] build a relation between Adam and the sign - based methods . Wu et al . [ 2020 ] further showed that sign - based methods can be effective when the Hessian is diagonal and satisfies several other properties . However , as put by the authors , it seems “unclear to what extent these properties hold for real problems " . Pan and Li [ 2023 ] numerically found that the Adam can reduce the directional sharpness along trajectories , while its relation to fast convergence remains mysterious . A recent work [ Jiang et al . , 2023 ] point out that Adam biases the trajectories towards regions where Hessian has “uniform diagonal entries " while SGD cannot . The distribution of Hessian diagonal entries is also investigated in [ Liu et al . , 2023 ] . The theory in [ Jiang et al . , 2023 ] implies that Adam is faster when the Hessian is diagonal . However , as argued above , it is unclear whether the diagonal Hessian commonly holds in real problems . In fact , we find the Hessian is closer to a block - diagonal ( instead of pure diagonal ) structure on some small Transformers . In these cases , blockwise eigenvalues carry more information than diagonal entries , providing extra details such as the location of eigenvalues . We find that these extra details are important for distinguishing Adam and SGD . Hessian Spectrum Analysis . There are several important attempts to explore the Hessian spectrum of MLPs and CNNs . Sagun et al . [ 2016 , 2017 ] , Chaudhari et al . [ 2019 ] found that the Hessian spectra of MLPs and CNNs consist of a “bulk " together with a few “outliers " . Papyan [ 2020 ] , Wu et al . [ 2020 ] , Liao and Mahoney [ 2021 ] further characterized the bulks and outliers in theory . Papyan [ 2018 , 2019 ] numerically built the relation between these " outliers " and the Gauss - Newton matrix . Sankar et al . [ 2021 ] numerically explored the relation between Hessian of CNNs and Gauss - Newton matrix in each layer . They further showed that most CNN layers contribute similarly to the overall loss surface . We find that this result is restricted to CNNs and does not hold on Transformers due to the heterogeneity . Gur - Ari et al . [ 2018 ] showed that for MLPs and CNNs , gradient descent converges to a small subspace spanned by a few top eigenvectors of the Hessian . Yao et al . [ 2018 ] , Zhang et al . [ 2019b ] explored the relation between the Hessian spectrum of CNNs and some training phenomena such as the effect of batch sizes . Ghorbani et al . [ 2019 ] , Yao et al . [ 2020 ] focused on explaining the effectiveness of techniques such as BatchNorm . Note that all these works are restricted to MLPs and CNNs , while we study the Hessian of Transformers ( in addition to CNNs and MLPs ) as well as its impacts on different optimizers . 5 Conclusion In this work , we investigate the failure mode of SGD on Transformers through the lens of Hessian . By numerically explore various Transformers , CNNs , MLPs and quadratic problems , we establish a phenomenon called block heterogeneity in Hessian and link it to the failure of SGD . We point out that SGD fails because it applies one single learning rate for all blocks , which cannot handle the heterogeneity among blocks . Initial theory are provided to support the claim . 13 References J . Achiam , S . Adler , S . Agarwal , L . Ahmad , I . Akkaya , F . L . Aleman , D . Almeida , J . Altenschmidt , S . Altman , S . Anadkat , et al . Gpt - 4 technical report . arXiv preprint arXiv : 2303 . 08774 , 2023 . R . P . Adams , J . Pennington , M . J . Johnson , J . Smith , Y . Ovadia , B . Patton , and J . Saunderson . Estimating the spectral density of large implicit matrices . arXiv preprint arXiv : 1802 . 03451 , 2018 . H . Avron and S . Toledo . Randomized algorithms for estimating the trace of an implicit symmetric positive semi - definite matrix . Journal of the ACM ( JACM ) , 58 ( 2 ) : 1 – 34 , 2011 . T . Bachlechner , B . P . Majumder , H . Mao , G . Cottrell , and J . McAuley . Rezero is all you need : Fast convergence at large depth . In Uncertainty in Artificial Intelligence , pages 1352 – 1361 . PMLR , 2021 . Z . Bai and G . H . Golub . Bounds for the trace of the inverse and the determinant of symmetric positive definite matrices . Annals of Numerical Mathematics , 4 : 29 – 38 , 1996 . Z . Bai , G . Fahey , and G . Golub . Some large - scale matrix computation problems . Journal of Computational and Applied Mathematics , 74 ( 1 - 2 ) : 71 – 89 , 1996 . A . Beck and L . Tetruashvili . On the convergence of block coordinate descent type methods . SIAM journal on Optimization , 23 ( 4 ) : 2037 – 2060 , 2013 . J . Bernstein , Y . - X . Wang , K . Azizzadenesheli , and A . Anandkumar . signsgd : Compressed optimisation for non - convex problems . In International Conference on Machine Learning , pages 560 – 569 . PMLR , 2018 . S . Bock and M . Weiß . Non - convergence and limit cycles in the adam optimizer . In Artificial Neural Networks and Machine Learning – ICANN 2019 : Deep Learning : 28th International Conference on Artificial Neural Networks , Munich , Germany , September 17 – 19 , 2019 , Proceedings , Part II 28 , pages 232 – 243 . Springer , 2019 . C . Brezinski . A direct proof of the christoffel - darboux identity and its equivalence to the recurrence relationship . Journal of Computational and Applied Mathematics , 32 ( 1 - 2 ) : 17 – 25 , 1990 . P . Chaudhari , A . Choromanska , S . Soatto , Y . LeCun , C . Baldassi , C . Borgs , J . Chayes , L . Sagun , and R . Zecchina . Entropy - sgd : Biasing gradient descent into wide valleys . Journal of Statistical Mechanics : Theory and Experiment , 2019 ( 12 ) : 124018 , 2019 . C . Chen , L . Shen , F . Zou , and W . Liu . Towards practical adam : Non - convexity , convergence theory , and mini - batch acceleration . The Journal of Machine Learning Research , 23 ( 1 ) : 10411 – 10457 , 2022 . J . Chen , F . Kunstner , and M . Schmidt . Heavy - tailed noise does not explain the gap between sgd and adam on transformers . In 13th Annual Workshop on Optimization for Machine Learning , 2021 . M . X . Chen , O . Firat , A . Bapna , M . Johnson , W . Macherey , G . Foster , L . Jones , N . Parmar , M . Schuster , Z . Chen , et al . The best of both worlds : Combining recent advances in neural machine translation . arXiv preprint arXiv : 1804 . 09849 , 2018 . X . Chen , S . Liu , R . Sun , and M . Hong . On the convergence of a class of adam - type algorithms for non - convex optimization . In 7th International Conference on Learning Representations , ICLR 2019 , 2019 . A . Chowdhery , S . Narang , J . Devlin , M . Bosma , G . Mishra , A . Roberts , P . Barham , H . W . Chung , C . Sutton , S . Gehrmann , et al . Palm : Scaling language modeling with pathways . Journal of Machine Learning Research , 24 ( 240 ) : 1 – 113 , 2023 . R . Collobert . Large scale machine learning . Technical report , Université de Paris VI , 2004 . M . Crawshaw , M . Liu , F . Orabona , W . Zhang , and Z . Zhuang . Robustness to unbounded smoothness of generalized signsgd . Advances in Neural Information Processing Systems , 35 : 9955 – 9968 , 2022 . 14 J . K . Cullum and R . A . Willoughby . Lanczos algorithms for large symmetric eigenvalue computations : Vol . I : Theory . SIAM , 2002 . A . B . Da Silva and M . Gazeau . A general system of differential equations to model first - order adaptive algorithms . The Journal of Machine Learning Research , 21 ( 1 ) : 5072 – 5113 , 2020 . T . Dao , D . Fu , S . Ermon , A . Rudra , and C . Ré . Flashattention : Fast and memory - efficient exact attention with io - awareness . Advances in Neural Information Processing Systems , 35 : 16344 – 16359 , 2022 . A . Défossez , L . Bottou , F . Bach , and N . Usunier . A simple convergence proof of adam and adagrad . Transactions on Machine Learning Research , 2022 . M . Dehghani , J . Djolonga , B . Mustafa , P . Padlewski , J . Heek , J . Gilmer , A . P . Steiner , M . Caron , R . Geirhos , I . Alabdulmohsin , et al . Scaling vision transformers to 22 billion parameters . In International Conference on Machine Learning , pages 7480 – 7512 . PMLR , 2023 . J . Devlin , M . - W . Chang , K . Lee , and K . Toutanova . Bert : Pre - training of deep bidirectional transformers for language understanding . arXiv preprint arXiv : 1810 . 04805 , 2018 . Y . Dong , J . - B . Cordonnier , and A . Loukas . Attention is not all you need : Pure attention loses rank doubly exponentially with depth . In International Conference on Machine Learning , pages 2793 – 2803 . PMLR , 2021 . A . Dosovitskiy , L . Beyer , A . Kolesnikov , D . Weissenborn , X . Zhai , T . Unterthiner , M . Dehghani , M . Minderer , G . Heigold , S . Gelly , et al . An image is worth 16x16 words : Transformers for image recognition at scale . arXiv preprint arXiv : 2010 . 11929 , 2020 . J . Duchi , E . Hazan , and Y . Singer . Adaptive subgradient methods for online learning and stochastic optimization . Journal of machine learning research , 12 ( 7 ) , 2011 . J . F . Epperson . An introduction to numerical methods and analysis . 2013 . S . Gadat and I . Gavra . Asymptotic study of stochastic adaptive algorithms in non - convex landscape . The Journal of Machine Learning Research , 23 ( 1 ) : 10357 – 10410 , 2022 . B . Ghorbani , S . Krishnan , and Y . Xiao . An investigation into neural net optimization via hessian eigenvalue density . In International Conference on Machine Learning , pages 2232 – 2241 . PMLR , 2019 . G . Goh . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . G . H . Golub and G . Meurant . Matrices , moments and quadrature with applications , volume 30 . Princeton University Press , 2009 . G . H . Golub and Z . Strakoš . Estimates in quadratic formulas . Numerical Algorithms , 8 : 241 – 268 , 1994 . G . H . Golub and J . H . Welsch . Calculation of gauss quadrature rules . Mathematics of computation , 23 ( 106 ) : 221 – 230 , 1969 . B . Goujaud , D . Scieur , A . Dieuleveut , A . B . Taylor , and F . Pedregosa . Super - acceleration with cyclical step - sizes . In International Conference on Artificial Intelligence and Statistics , pages 3028 – 3065 . PMLR , 2022 . A . Gu and T . Dao . Mamba : Linear - time sequence modeling with selective state spaces . arXiv preprint arXiv : 2312 . 00752 , 2023 . G . Gur - Ari , D . A . Roberts , and E . Dyer . Gradient descent happens in a tiny subspace . arXiv preprint arXiv : 1812 . 04754 , 2018 . K . He , X . Zhang , S . Ren , and J . Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . 15 X . S . Huang , F . Perez , J . Ba , and M . Volkovs . Improving transformer optimization through better initialization . In International Conference on Machine Learning , pages 4475 – 4483 . PMLR , 2020 . M . F . Hutchinson . A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines . Communications in Statistics - Simulation and Computation , 18 ( 3 ) : 1059 – 1076 , 1989 . K . Jiang , D . Malik , and Y . Li . How does adaptive optimization impact local neural network geometry ? Advances in Neural Information Processing Systems , 36 , 2023 . J . Kaplan , S . McCandlish , T . Henighan , T . B . Brown , B . Chess , R . Child , S . Gray , A . Radford , J . Wu , and D . Amodei . Scaling laws for neural language models . arXiv preprint arXiv : 2001 . 08361 , 2020 . D . P . Kingma and J . Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . F . Kunstner , J . Chen , J . W . Lavington , and M . Schmidt . Noise is not the main factor behind the gap between sgd and adam on transformers , but sign descent might be . arXiv preprint arXiv : 2304 . 13960 , 2023 . C . Lanczos . An iteration method for the solution of the eigenvalue problem of linear differential and integral operators . 1950 . Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner . Gradient - based learning applied to document recognition . Proceedings of the IEEE , 86 ( 11 ) : 2278 – 2324 , 1998 . Y . LeCun , L . Bottou , G . B . Orr , and K . - R . Müller . Efficient backprop . In Neural networks : Tricks of the trade , pages 9 – 50 . Springer , 2002 . H . Li , A . Rakhlin , and A . Jadbabaie . Convergence of adam under relaxed assumptions . Advances in Neural Information Processing Systems , 36 , 2023 . Z . Liao and M . W . Mahoney . Hessian eigenspectra of more realistic nonlinear models . Advances in Neural Information Processing Systems , 34 : 20104 – 20117 , 2021 . L . Lin , Y . Saad , and C . Yang . Approximating spectral densities of large matrices . SIAM review , 58 ( 1 ) : 34 – 65 , 2016 . H . Liu , Z . Li , D . Hall , P . Liang , and T . Ma . Sophia : A scalable stochastic second - order optimizer for language model pre - training . arXiv preprint arXiv : 2305 . 14342 , 2023 . L . Liu , H . Jiang , P . He , W . Chen , X . Liu , J . Gao , and J . Han . On the variance of the adaptive learning rate and beyond . arxiv 2019 . arXiv preprint arXiv : 1908 . 03265 , 2019 . L . Liu , X . Liu , J . Gao , W . Chen , and J . Han . Understanding the difficulty of training transformers . arXiv preprint arXiv : 2004 . 08249 , 2020 . I . Loshchilov and F . Hutter . Decoupled weight decay regularization . arXiv preprint arXiv : 1711 . 05101 , 2017 . L . Luo , Y . Xiong , Y . Liu , and X . Sun . Adaptive gradient methods with dynamic bound of learning rate . In International Conference on Learning Representations , 2018 . J . Martens and R . Grosse . Optimizing neural networks with kronecker - factored approximate curvature . In International conference on machine learning , pages 2408 – 2417 . PMLR , 2015 . W . Merrill , V . Ramanujan , Y . Goldberg , R . Schwartz , and N . Smith . Effects of parameter norm growth during transformer training : Inductive bias from gradient descent . arXiv preprint arXiv : 2010 . 09697 , 2020 . I . Molybog , P . Albert , M . Chen , Z . DeVito , D . Esiobu , N . Goyal , P . S . Koura , S . Narang , A . Poulton , R . Silva , et al . A theory on adam instability in large - scale machine learning . arXiv preprint arXiv : 2304 . 09871 , 2023 . 16 Y . Nesterov . Introductory lectures on convex optimization : A basic course , volume 87 . Springer Science & Business Media , 2013 . T . Q . Nguyen and J . Salazar . Transformers without tears : Improving the normalization of self - attention . arXiv preprint arXiv : 1910 . 05895 , 2019 . L . Noci , S . Anagnostidis , L . Biggio , A . Orvieto , S . P . Singh , and A . Lucchi . Signal propagation in transformers : Theoretical perspectives and the role of rank collapse . Advances in Neural Information Processing Systems , 35 : 27198 – 27211 , 2022 . Y . Pan and Y . Li . Toward understanding why adam converges faster than sgd for transformers . arXiv preprint arXiv : 2306 . 00204 , 2023 . V . Papyan . The full spectrum of deepnet hessians at scale : Dynamics with sgd training and sample size . arXiv preprint arXiv : 1811 . 07062 , 2018 . V . Papyan . Measurements of three - level hierarchical structure in the outliers in the spectrum of deepnet hessians . arXiv preprint arXiv : 1901 . 08244 , 2019 . V . Papyan . Traces of class / cross - class structure pervade deep learning spectra . The Journal of Machine Learning Research , 21 ( 1 ) : 10197 – 10260 , 2020 . B . A . Pearlmutter . Fast exact multiplication by the hessian . Neural computation , 6 ( 1 ) : 147 – 160 , 1994 . A . Radford , J . Wu , R . Child , D . Luan , D . Amodei , I . Sutskever , et al . Language models are unsupervised multitask learners . OpenAI blog , 1 ( 8 ) : 9 , 2019 . J . Rasley , S . Rajbhandari , O . Ruwase , and Y . He . Deepspeed : System optimizations enable training deep learning models with over 100 billion parameters . In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 3505 – 3506 , 2020 . S . J . Reddi , S . Kale , and S . Kumar . On the convergence of adam and beyond . In International Conference on Learning Representations , 2018 . N . Roux , P . - A . Manzagol , and Y . Bengio . Topmoumoute online natural gradient algorithm . Advances in neural information processing systems , 20 , 2007 . Y . Saad . Numerical methods for large eigenvalue problems : revised edition . SIAM , 2011 . L . Sagun , L . Bottou , and Y . LeCun . Eigenvalues of the hessian in deep learning : Singularity and beyond . arXiv preprint arXiv : 1611 . 07476 , 2016 . L . Sagun , U . Evci , V . U . Guney , Y . Dauphin , and L . Bottou . Empirical analysis of the hessian of over - parametrized neural networks . arXiv preprint arXiv : 1706 . 04454 , 2017 . A . R . Sankar , Y . Khasbage , R . Vigneswaran , and V . N . Balasubramanian . A deeper look at the hessian eigenspectrum of deep neural networks and its applications to regularization . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35 , pages 9481 – 9488 , 2021 . N . Shi , D . Li , M . Hong , and R . Sun . Rmsprop converges with proper hyper - parameter . In International Conference on Learning Representations , 2020 . K . Simonyan and A . Zisserman . Very deep convolutional networks for large - scale image recognition . arXiv preprint arXiv : 1409 . 1556 , 2014 . R . Sun . Optimization for deep learning : theory and algorithms . arXiv preprint arXiv : 1912 . 08957 , 2019 . R . Sun and Y . Ye . Worst - case complexity of cyclic coordinate descent : O ( nˆ 2 ) o ( n 2 ) gap with randomized version . Mathematical Programming , 185 : 487 – 520 , 2021 . 17 S . Ubaru , J . Chen , and Y . Saad . Fast estimation of tr ( f ( a ) ) via stochastic lanczos quadrature . SIAM Journal on Matrix Analysis and Applications , 38 ( 4 ) : 1075 – 1099 , 2017 . A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , Ł . Kaiser , and I . Polosukhin . Attention is all you need . Advances in neural information processing systems , 30 , 2017 . B . Wang , Y . Zhang , H . Zhang , Q . Meng , Z . - M . Ma , T . - Y . Liu , and W . Chen . Provable adaptivity in adam . arXiv preprint arXiv : 2208 . 09900 , 2022a . B . Wang , J . Fu , H . Zhang , N . Zheng , and W . Chen . Closing the gap between the upper bound and lower bound of adam’s iteration complexity . Advances in Neural Information Processing Systems , 36 , 2023a . B . Wang , H . Zhang , Z . Ma , and W . Chen . Convergence of adagrad for non - convex objectives : Simple proofs and relaxed assumptions . In The Thirty Sixth Annual Conference on Learning Theory , pages 161 – 190 . PMLR , 2023b . H . Wang , S . Ma , L . Dong , S . Huang , D . Zhang , and F . Wei . Deepnet : Scaling transformers to 1 , 000 layers . arXiv preprint arXiv : 2203 . 00555 , 2022b . Q . Wang , B . Li , T . Xiao , J . Zhu , C . Li , D . F . Wong , and L . S . Chao . Learning deep transformer models for machine translation . arXiv preprint arXiv : 1906 . 01787 , 2019 . Wikipedia . Gaussian quadrature — Wikipedia , the free encyclopedia , 2023 . URL https : / / en . wikipedia . org / w / index . php ? title = Gaussian _ quadrature & oldid = 1191539517 . [ Online ; accessed 20 - January - 2024 ] . M . Wortsman , P . J . Liu , L . Xiao , K . Everett , A . Alemi , B . Adlam , J . D . Co - Reyes , I . Gur , A . Kumar , R . Novak , et al . Small - scale proxies for large - scale transformer training instabilities . arXiv preprint arXiv : 2309 . 14322 , 2023 . Y . Wu , X . Zhu , C . Wu , A . Wang , and R . Ge . Dissecting hessian : Understanding common structure of hessian in neural networks . arXiv preprint arXiv : 2010 . 04261 , 2020 . Z . Xie , X . Wang , H . Zhang , I . Sato , and M . Sugiyama . Adaptive inertia : Disentangling the effects of adaptive learning rate and momentum . In International conference on machine learning , pages 24430 – 24459 . PMLR , 2022 . R . Xiong , Y . Yang , D . He , K . Zheng , S . Zheng , C . Xing , H . Zhang , Y . Lan , L . Wang , and T . Liu . On layer normalization in the transformer architecture . In International Conference on Machine Learning , pages 10524 – 10533 . PMLR , 2020 . A . Yang , B . Xiao , B . Wang , B . Zhang , C . Bian , C . Yin , C . Lv , D . Pan , D . Wang , D . Yan , et al . Baichuan 2 : Open large - scale language models . arXiv preprint arXiv : 2309 . 10305 , 2023 . G . Yang , E . J . Hu , I . Babuschkin , S . Sidor , X . Liu , D . Farhi , N . Ryder , J . Pachocki , W . Chen , and J . Gao . Tensor programs v : Tuning large neural networks via zero - shot hyperparameter transfer . arXiv preprint arXiv : 2203 . 03466 , 2022 . Z . Yao , A . Gholami , Q . Lei , K . Keutzer , and M . W . Mahoney . Hessian - based analysis of large batch training and robustness to adversaries . Advances in Neural Information Processing Systems , 31 , 2018 . Z . Yao , A . Gholami , K . Keutzer , and M . W . Mahoney . Pyhessian : Neural networks through the lens of the hessian . In 2020 IEEE international conference on big data ( Big data ) , pages 581 – 590 . IEEE , 2020 . M . Zaheer , S . Reddi , D . Sachan , S . Kale , and S . Kumar . Adaptive methods for nonconvex optimization . Advances in neural information processing systems , 31 , 2018 . 18 A . Zeng , X . Liu , Z . Du , Z . Wang , H . Lai , M . Ding , Z . Yang , Y . Xu , W . Zheng , X . Xia , et al . Glm - 130b : An open bilingual pre - trained model . arXiv preprint arXiv : 2210 . 02414 , 2022 . S . Zhai , T . Likhomanenko , E . Littwin , D . Busbridge , J . Ramapuram , Y . Zhang , J . Gu , and J . M . Susskind . Stabilizing transformer training by preventing attention entropy collapse . In International Conference on Machine Learning , pages 40770 – 40803 . PMLR , 2023 . B . Zhang , I . Titov , and R . Sennrich . Improving deep transformer with depth - scaled initialization and merged attention . arXiv preprint arXiv : 1908 . 11365 , 2019a . G . Zhang , L . Li , Z . Nado , J . Martens , S . Sachdeva , G . Dahl , C . Shallue , and R . B . Grosse . Which algorithmic choices matter at which batch sizes ? insights from a noisy quadratic model . Advances in neural information processing systems , 32 , 2019b . J . Zhang , T . He , S . Sra , and A . Jadbabaie . Why gradient clipping accelerates training : A theoretical justification for adaptivity . arXiv preprint arXiv : 1905 . 11881 , 2019c . J . Zhang , S . P . Karimireddy , A . Veit , S . Kim , S . Reddi , S . Kumar , and S . Sra . Why are adaptive methods good for attention models ? Advances in Neural Information Processing Systems , 33 : 15383 – 15393 , 2020 . S . Zhang , S . Roller , N . Goyal , M . Artetxe , M . Chen , S . Chen , C . Dewan , M . Diab , X . Li , X . V . Lin , et al . Opt : Open pre - trained transformer language models . arXiv preprint arXiv : 2205 . 01068 , 2022a . Y . Zhang , C . Chen , N . Shi , R . Sun , and Z . - Q . Luo . Adam can converge without any modification on update rules . Advances in Neural Information Processing Systems , 35 : 28386 – 28399 , 2022b . D . Zhou , J . Chen , Y . Cao , Y . Tang , Z . Yang , and Q . Gu . On the convergence of adaptive gradient methods for nonconvex optimization . arXiv preprint arXiv : 1808 . 05671 , 2018 . F . Zou , L . Shen , Z . Jie , W . Zhang , and W . Liu . A sufficient condition for convergences of adam and rmsprop . In Proceedings of the IEEE / CVF Conference on computer vision and pattern recognition , pages 11127 – 11135 , 2019 . 19 A More Related Works On the difficulties of Transformer training . Transformers are known to be difficult to train . Researchers have attributed the training difficulties to various phenomena in different components of Transformers , including : the logits divergence or the rank degeneracy in the outputs of attention layers [ Dong et al . , 2021 , Noci et al . , 2022 , Wortsman et al . , 2023 , Zhai et al . , 2023 , Dehghani et al . , 2023 , Chowdhery et al . , 2023 ] ; the growth of parameter norm in attention layers [ Merrill et al . , 2020 ] ; over - reliance on residue branches [ Liu et al . , 2020 ] ; and some negative impact of layer norm [ Chen et al . , 2018 , Zhang et al . , 2019a , Huang et al . , 2020 ] . These phenomena have a strong correlation with gradient vanishing or explosion in Transformers [ Zhang et al . , 2019a , Liu et al . , 2020 , Huang et al . , 2020 , Xiong et al . , 2020 , Noci et al . , 2022 , Wang et al . , 2022b , Wortsman et al . , 2023 , Molybog et al . , 2023 ] , which leads to training difficulties . Several solutions have been proposed . Liu et al . [ 2020 ] numerically observed that adaptive gradient methods can ( partly ) overcome gradient vanishing by giving “consistent update magnitude " , while it seems unclear how consistent update magnitude would help optimization in principle . Researchers further develop training tricks such as warmup learning rate [ Liu et al . , 2019 , Xiong et al . , 2020 ] , temperature scaling [ Noci et al . , 2022 ] , better initialization [ Zhang et al . , 2019a , Huang et al . , 2020 , Wang et al . , 2022b , Bachlechner et al . , 2021 , Yang et al . , 2022 ] , and variants of Layer Norm [ Nguyen and Salazar , 2019 , Wang et al . , 2019 , Xiong et al . , 2020 , Wang et al . , 2022b , Dehghani et al . , 2023 ] . Recent researchers also suggest using z - loss regularization [ Chowdhery et al . , 2023 , Yang et al . , 2023 ] and tuning hyperparameters of Adam [ Zhang et al . , 2022b , Wortsman et al . , 2023 ] . All these tricks can help mitigate gradient explosion or vanishing . Nevertheless , training large - scale Transformers remains challenging [ Zhang et al . , 2022a , Zeng et al . , 2022 , Wortsman et al . , 2023 , Molybog et al . , 2023 , Chowdhery et al . , 2023 ] . Different from all aforementioned works , we investigate the training difficulties of Transformers through the eigenvalues of Hessian . We establish a strong correlation between the blockwise Hessian spectra of Transformers and the failure of SGD . We realize that our attempt is just a first step towards understanding Transformer training , and we believe there is rich information hidden in Hessian and we leave more fine - grained analysis as future works . Convergence analysis of adaptive gradient methods There is extensive convergence analysis for adaptive gradient methods . For instance , researchers study the convergence of AMSGrad [ Reddi et al . , 2018 , Zhou et al . , 2018 ] , RMSprop [ Zaheer et al . , 2018 ] , AdaFom [ Chen et al . , 2019 ] , AdaBound [ Luo et al . , 2018 ] , and Adam with iterate - dependent hyperparameters [ Zou et al . , 2019 , Chen et al . , 2022 , Gadat and Gavra , 2022 ] . The convergence of Adam is also explored in [ Défossez et al . , 2022 , Wang et al . , 2023a ] . There is also an active line of theoretical research on the convergence of AdaGrad [ Duchi et al . , 2011 ] , we recommend [ Wang et al . , 2023b ] for more detailed introduction . In this work , we do not focus on the convergence analysis . Rather , we explore the quantitative difference between the loss landscape of CNNs and Transformers and how it impact the behaviors of SGD and Adam . B More Preliminaries B . 1 Preliminaries on Optimizers Here we provide a detailed description of the optimizers mentioned in the full script . We consider the minimizing L ( w ) ≡ 1 n ∑ ni = 1 L i ( w ) , where n is the number of minibatches , L i ( w ) is the loss of i - th minibatch and w ∈ R d is the neural network parameters . We denote the gradient of the training loss w . r . t . neural network parameters as ∇L ( w ) ∈ R d . We use ∇L i ( w ) ∈ R d to denote the i - th minibatch counterparts . We use w t to denote the variable at the t - th step . In Algorithm 2 and 3 , ◦ , division and square - root are elementwise operations . In the line 7 and 8 of Algorithm 2 , ( β 1 ) t and ( β 2 ) t indicates the t - th power of β 1 , β 2 . In the PyTorch default setting , ( β 1 , β 2 , ϵ ) = ( 0 . 9 , 0 . 999 , 1e - 8 ) for Adam and β 1 = 0 . 9 for SGD . 20 Algorithm 1 Stochastic Gradient Descent with Momentum ( SGD ) 1 : Initialize w 0 and choose 0 ≤ β 1 < 1 and η 0 > 0 2 : for t = 1 → ∞ do 3 : Uniformly sample τ t from the index set { 1 , 2 , · · · , n } 4 : m t = β 1 m t + ∇ f τ t ( x t ) 5 : x t + 1 = x t − η t m t 6 : end for Algorithm 2 AdamW 1 : Initialize x 0 , m 0 = v 0 = 0 , 0 ≤ β 1 < 1 , 0 ≤ β 2 < 1 , ϵ > 0 , η 0 > 0 , and weight decay coefficient λ 2 : for t = 1 → ∞ do 3 : Uniformly sample τ t from the index set { 1 , 2 , · · · , n } 4 : w t + 1 = w t − η t λ w t 5 : m t = β 1 m t + ( 1 − β 1 ) ∇L τ t ( w t ) 6 : v t = β 2 v t + ( 1 − β 2 ) ∇L τ t ( w t ) ◦ ∇L τ t ( w t ) 7 : ˆ m t = m t 1 − ( β 1 ) t 8 : ˆ v t = v t 1 − ( β 2 ) t 9 : w t + 1 = w t + 1 − η t ˆ m t √ ˆ v t + ϵ 10 : end for Algorithm 3 Adam with no bias correction 1 : Initialize x 0 , m 0 = ∇L τ t ( w 0 ) , v 0 = ∇L τ t ( w 0 ) ◦ ∇L τ t ( w 0 ) , 0 ≤ β 1 < 1 , 0 ≤ β 2 < 1 , ϵ > 0 , η 0 > 0 2 : for t = 1 → ∞ do 3 : Uniformly sample τ t from the index set { 1 , 2 , · · · , n } 4 : m t = β 1 m t + ( 1 − β 1 ) ∇L τ t ( w t ) 5 : v t = β 2 v t + ( 1 − β 2 ) ∇L τ t ( w t ) ◦ ∇L τ t ( w t ) 6 : w t + 1 = w t + 1 − η t m t √ v t + ϵ 7 : end for 21 B . 2 Preliminaries on the Stochastic Lanczos Quadrature Method Additional notations . Given a real symmetric matrix H ∈ R d × d , we denote tr ( H ) as its trace and Q T Λ Q as its spectral decomposition , where Q = [ q 1 , . . . , q d ] , Λ = diag ( λ 1 , . . . , λ d ) and λ 1 ≥ λ 2 · · · ≥ λ d . We denote the condition number of H as κ = λ 1 / λ d . We define matrix function as f ( H ) : = Q T f ( Λ ) Q , where f ( Λ ) = diag ( f ( λ 1 ) , . . . f ( λ d ) ) ∈ R d × d . We use N to denote the set of positive integers . We use ∥ · ∥ 2 to denote the Euclidean norm . Approximation of the Hessian spectrum can be formulated as a trace estimation problem , as introduced in [ Lin et al . , 2016 , Ubaru et al . , 2017 ] . First , the spectrum ( eigenvalue density ) of Hessian H can written as : ϕ ( t ) = 1 d ∑ di = 1 δ ( t − λ i ) , where λ i are the eigenvalues of H and δ is the Dirac δ - function . Then , we replace the delta functions by a Gaussian blurring function : ϕ ( t ) ≈ g ( t ) : = 1 d ∑ di = 1 f ( λ i ) , where f ( λ ) : = 1 σ √ 2 π exp (cid:16) − ( t − λ ) 2 2 σ 2 (cid:17) . By definition of matrix function , it is easy to see that g ( t ) = 1 d tr ( f ( H ) ) . As such , spectrum approximation could be formulated as a trace estimation problem , i . e . , estimating 1 d tr ( f ( H ) ) , where H ∈ R d × d is a real symmetric matrix . Trace estimation problems could be solved efficiently by the Stochastic Lanczos Quadrature Method ( SLQ ) [ Golub and Strakoš , 1994 ] . For the ease of readers , we provide a detailed description of SLQ in our context . SLQ consists of the following steps . Step 1 . We Approximate the trace of matrix function as 1 d tr ( f ( H ) ) = E ( v T f ( H ) v ) ≈ 1 n v ∑ n v i v Ti f ( H ) v i , where v = u / ∥ u ∥ 2 and u is a Rademacher random vector ( each entry of u independently takes ± 1 with probability 1 / 2 ) . This step is called Huchinson’s estimation [ Hutchinson , 1989 ] . Note that we can also replace the Rademacher random vector u by a unit Gaussian vector ( i . e . , u ∼ N ( 0 , I d × d ) ) and the unbiasedness still holds [ Avron and Toledo , 2011 ] . In our implementation , we sample u ∼ N ( 0 , I d × d ) because there is an efficient built - in PyTorch function for generating Gaussian vectors . SLQ estimates v Ti f ( H ) v i for i ∈ [ n v ] and then take the average . To understand SLQ , we only need to understand how it estimates each individual quadratic form . To simplify the notation regarding i , from now on , we will discuss how to estimate v T f ( H ) v , where v = u / ∥ u ∥ 2 and u is a unit Gaussian vector . Step 2 - 1 . We rewrite v T f ( H ) v as a Riemann - Stieltjes integral [ Golub and Meurant , 2009 ] : v T f ( A ) v = d ∑ i = 1 (cid:16) v T q i (cid:17) 2 f ( λ i ) = (cid:90) λ 1 λ d f ( λ ) d µ ( λ ) , ( 7 ) where µ is a measure on ( R , B ) defined as follows ( µ ( λ ) denotes the measure of set { x ; x ≤ λ } ) : µ ( λ ) =    0 λ < λ d ∑ ki = 1 (cid:0) v T q i (cid:1) 2 λ k ≤ λ < λ k + 1 ∑ di = 1 (cid:0) v T q i (cid:1) 2 λ ≥ λ 1 . ( 8 ) Step 2 - 2 . Unfortunately , this integral is difficult to compute . This is because the measure µ are related to the eigen - pairs of H , which are unknown . It seems unclear how to directly integrate over an unknown measure . As such , we further approximate this integral by a computationally friendly quantity , such as : (cid:90) λ 1 λ d f ( λ ) d µ ( λ ) ≈ m ∑ j = 1 c j f ( x j ) . ( 9 ) We hope to design { ( c j , x j ) } mj = 1 with a reasonable number of m such that the estimation error is small . Fortunately , the Gaussian Quadrature method provides a generic design principle of { ( c j , x j ) } m j = 1 [ Golub and Meurant , 2009 , Epperson , 2013 ] . It is proved that : when f ( λ ) is not " too complicated " ( e . g . f ( λ ) is 22 a polynomial ) , then there exists { ( c j , x j ) } mj = 1 which gives a high quality estimation of integral ( 7 ) . The required number of m is related to " how complicated the f ( λ ) is " . Such { ( c j , x j ) } mj = 1 are called the Gaussian Quadrature rules . c j and x j are called the " weights " and the " nodes " of the Gaussian Quadrature rules . A representative theorem is as follows : when f ( λ ) is a polynomial with degree < 2 m , then the Gaussian Quadrature rules give the exact approximation of integral ( 7 ) . Theorem 3 . [ Rewrited based on [ Wikipedia , 2023 ] ] Suppose we have a sequence of orthogonal polynomials { p k ( x ) } mk = 1 w . r . t . measure µ , that is : (cid:82) λ 1 λ d p n ( x ) p m ( x ) d µ ( x ) = δ m , n , where δ m , n = 1 if m = n and δ m , n = 0 , otherwise . Assume f ( x ) is a polynomial with degree < 2 m , then there exists (cid:8)(cid:0) c j , x j (cid:1)(cid:9) m j = 1 s . t . (cid:82) λ 1 λ d f ( λ ) d µ ( λ ) = ∑ mi = j c j f (cid:0) x j (cid:1) . The equality holds when x j are the roots of p m ( x ) and c j = (cid:82) λ 1 λ d ∏ j ̸ = i x − x i x j − x i d µ . Such choice of (cid:8)(cid:0) c j , x j (cid:1)(cid:9) m j = 1 are called the Gaussian Quadrature rules . Theorem 3 shows the existence of good (cid:8)(cid:0) c j , x j (cid:1)(cid:9) m j = 1 and their general form . In fact , it is also shown that Gaussian Quadrature is optimal : no other (cid:8)(cid:0) c j , x j (cid:1)(cid:9) m j = 1 can achieve zero approximation error for higher degree polynomials f ( λ ) [ Golub and Meurant , 2009 ] . However , it is often difficult to find these quadrature rules [ Golub and Welsch , 1969 ] . There are at least three questions in sequel : • 1 ) how to find the orthogonal polynomials { p k ( x ) } mk = 1 w . r . t . an unknown measure µ ? • 2 ) how to efficiently find the roots of p m ( x ) , which gives the nodes x j ? • 3 ) how to efficiently calculate the weights c j = (cid:82) λ 1 λ d ∏ j ̸ = i x − x i x j − x i d µ ? We first answer question 2 ) and 3 ) and leave question 1 ) for later discussion . Now suppose that we have found the orthogonal polynomials { p k ( x ) } mk = 1 w . r . t . µ . Recall that any orthogonal polynomial has the following " three - term " recursion [ Golub and Meurant , 2009 ] : p k + 1 ( x ) = ( x − α k + 1 ) p k ( x ) − β k p k − 1 ( x ) , k = 0 , 1 , . . . , where p − 1 ( x ) ≡ 0 , p 0 ( x ) ≡ 1 , α k + 1 = ⟨ xp k , p k ⟩ ⟨ p k , p k ⟩ and β k = ⟨ p k , p k ⟩ ⟨ p k − 1 , p k − 1 ⟩ . Define P m ( x ) = ( p 0 ( x ) , p 1 ( x ) , . . . p m − 1 ( x ) ) T ∈ R m , we can rewrite the recursion formula in matrix form ( given x ) : xP m = J m P m + β m p m ( x ) e m , where e m is the last column of identity matrix I m , m and J m is called Jacobi matrix of order m : J m =   α 1 (cid:112) β 1 (cid:112) β 1 α 2 (cid:112) β 2 (cid:112) β 2 α 3 (cid:112) β 3 . . . . . . . . .   ∈ R m × m It turns out that J m can help us find the Gaussian Quadrature rules (cid:8)(cid:0) c j , x j (cid:1)(cid:9) m j = 1 and thus provide answers for question 2 ) and 3 ) . This is shown in the following theorem . Theorem 4 . [ Golub and Meurant , 2009 ] For the Gaussian Quadrature , { x j } mj = 1 are the eigenvalues of J m and { c j } mj = 1 are the squares of the first elements of the normalized eigenvectors of J m . The proof of Theorem 4 is based on Christoffel - Darboux relation [ Brezinski , 1990 ] . Now , the remaining question is : how to find the Jacobian matrix J m of a sequence of orthogonal polynomials w . r . t . an unknown measure µ ? Note that we no longer need to answer question 1 ) if J m is found , since J m is sufficient for us to find the Gaussian quadrature rules . However , it seems impossible to find J m if no information of µ is provided . The good news is : when the µ is specified as in ( 8 ) , there exists an efficient way to find J m . 23 Step 3 . When µ is specified as in ( 8 ) , J m can be exactly found in m steps using the Lanczos algorithm [ Lanczos , 1950 ] , as shown in Algorithm 4 . This method takes a real symmetric matrix as input and returns a tridiagonal matrix . It was originally proposed to solve eigenvalue problems . Later , researchers found a deep connection between the Lanczos algorithm and orthogonal polynomials , which further connects this method to the Gaussian quadrature . The method ( of finding the Gaussian quadrature by the Lanczos algorithm ) is called the Lanczos quadrature [ Golub and Strakoš , 1994 , Bai and Golub , 1996 , Golub and Meurant , 2009 ] . An extremely elegant but highly nontrivial result is as follows : Theorem 5 . [ Golub and Meurant , 2009 ] Given a real symmetric matrix H ∈ R d × d and an arbitrary vector v ∈ R d with unit Euclidean norm , we define the measure µ as in ( 8 ) based on this H and v . Then m steps of the Lanzcos algorithm return the Jacobian matrix J m of orthogonal polynomials w . r . t . to µ . After J m is found by the Lanczos algorithm , we perform spectral decomposition of J m ∈ R m × m to get its eigen - pairs . Using Theorem 4 , we successfully get the Gaussian quadrature rules and thus we can approximate the quadratic form v T f ( H ) v . By averaging over different random vectors v we can then approximate 1 d tr ( f ( H ) ) . This concludes the derivation of SLQ for the trace estimation problem . The full procedure of SLQ is shown in Algorithm 5 . We note that SLQ is efficient in theory . Ubaru et al . [ 2017 ] show that SLQ converges faster than any other polynomial expansion method for spectrum estimation ( e . g . , Chebyshev methods used in [ Adams et al . , 2018 ] ) . See [ Ubaru et al . , 2017 , Theorem 4 . 1 ] for a formal statement . We remark that there are at least four versions of the Lanczos algorithm in Step 3 . Here , we adopt the version in Algorithm 4 since it is known to be the most numerically stable version [ Cullum and Willoughby , 2002 , Saad , 2011 , Wikipedia , 2023 ] . Throughout this work , we choose f ( · ) as the Gaussian blurring function f ( λ ) : = 1 σ √ 2 π exp (cid:16) − ( t − λ ) 2 2 σ 2 (cid:17) for spectrum approximation . We plot the spectrum by sweeping t from the minimal node to the maximal node in Gaussian Quadrature rules . Algorithm 4 The Lanczos Algorithm 1 : Input a matrix - vector product Hv 1 ∈ R d , where H is a real symmetric matrix and v 1 is an arbitrary vector with Euclidean norm 1 . Choose m ∈ N 2 : Initialization : Let w ′ 1 = Hv 1 , α 1 = ( w ′ 1 ) T v 1 , w 1 = w ′ 1 − α 1 v 1 3 : for j = 2 → m do 4 : Let β j = (cid:13)(cid:13) w j − 1 (cid:13)(cid:13) 2 ( also Euclidean norm ) 5 : If β j ̸ = 0 , then let v j = w j − 1 / β j , else pick as v j an arbitrary vector with Euclidean norm 1 that is orthogonal to all of v 1 , . . . , v j − 1 6 : Let w ′ j = Av j 7 : Let α j = ( w ′ j ) T v j 8 : Let w j = w ′ j − α j v j − β j v j − 1 9 : end for 10 : Let V be the matrix with columns v 1 , . . . , v m 11 : Let T =   α 1 β 2 0 β 2 α 2 β 3 β 3 α 3 . . . . . . . . . β m − 1 β m − 1 α m − 1 β m 0 β m α m   12 : Return T 24 Algorithm 5 The Stochastic Lanczos Quadrature Method 1 : Choose num v , m ∈ N . Sample num v i . i . d . v i from normalized Rademacher distribution , i ∈ [ num v ] 2 : for i = 1 → num v do 3 : Run m steps of the Lanczos Algorithm 4 with input Hv i , returns T ∈ R m × m 4 : Compute eigenvalue decomposition T = Q Λ Q T 5 : Compute the nodes x i = ( Λ ii ) mi = 1 and weights c i = (cid:16) Q 21 , i (cid:17) m i = 1 6 : Return q i ( t ) = ∑ mi = 1 c i f (cid:0) x i ; t , σ 2 (cid:1) 7 : end for 8 : Return 1 num v ∑ num v i = 1 f (cid:0) ℓ i ; t , σ 2 (cid:1) C More Eperimental Details C . 1 Implementation Details on SLQ Implementation and Running Time Analysis . We provide a simple PyTorch implementation of SLQ . The only query SLQ makes to the neural network is the Hessian vector product , which is attained using the auto - differentiation framework [ Pearlmutter , 1994 ] . To assure the accuracy of the Lanczos algorithm , we remove all the randomness in the forward and backward passes , including : data shuffling order , data augmentation , and dropout , etc . . Since Flash Attention [ Dao et al . , 2022 ] does not support the calculation of Hessian - vector product , we implement all attention blocks in the naive way . For the calculation of the blockwise Hessian spectrum ∇ 2 L ( w l ) , we sample u l ∼ N ( 0 , I d l × d l ) and set v l = u l / ∥ u l ∥ 2 ∈ R d l . Then we run Algorithm 5 by taking ∇ 2 L ( w l ) and v l as inputs . We choose the hyperparameters as m = 100 and n v = 10 in all experiments . σ is tuned based on visual effects . These hyperparameters are reported to reach highly accurate estimation with error < 10 − 14 [ Ghorbani et al . , 2019 ] . We now briefly discuss the computational cost of SLQ . The major computational expense of SLQ is the repeated Hessian - vector product operations in Lanczos algorithm in Step 3 . Recall ∇ 2 L ( w ) d = 1 n ∑ ni = 1 ∇ 2 L i ( w ) d , so each Hessian - vector product operation requires ( i ) calculating ∇ 2 L i ( w ) d ; ( ii ) repeating ( i ) on all data . We point out that ( i ) can be computed efficiently and precisely with just two backpropagation passes [ Pearlmutter , 1994 ] . The major computational bottleneck lies in ( ii ) due to the large n . Our largest - scale experiment for Hesian spectrum is GPT2 ( 125M ) on Openwebtext , where the number of tokens n = 9 Billon . To calculate ∇ 2 L ( w ) d on all these 9B tokens , it requires about 9 GPU days on eight A100 - 80GB GPUs . Since SLQ requires at least 1 , 000 times query of ∇ 2 L ( w ) d , a complete run of SLQ would take at least 9 , 000 days on eight A100 - 80GB GPUs , which is unaffordable . In this work , we use the largest possible batch size ( with gradient accumulation tricks ) to approximate ∇ 2 L ( w ) under the constraints of GPU bandwidth and time limit . More detailed setup of SLQ are shown as follows . • ResNet18 ( 20M ) and VGG16 ( 138M ) on ImageNet . We use the code base of PyTorch Examples 4 . We use batch size = 1024 . For the calculation of the blockwise Hessian spectra , we apply SLQ to all parameter blocks except for the BatchNorm layers . In total , it takes about 3 days on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum . • ViT - base ( 86M ) on ImageNet . We use the code base of PyTorch Image Models 5 . We use batch size = 1024 . Due to the large number of parameters , we are not able to calculate the blockwise Hessian spectra for all parameter blocks . Instead , we apply SLQ to : the embedding layer ; the output layer ; the 1 - st , 6 - th , 12 - th attention blocks ; and the 1 - st , 6 - th , 12 - th MLP blocks ( note that the 12 - th attention and MLP blocks are the final ones ) . In total , it takes about 3 days on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum . 4 https : / / github . com / pytorch / examples / blob / main / imagenet / main . py 5 https : / / github . com / huggingface / pytorch - image - models 25 • BERT ( 40M ) on Cornell Movie - Dialogs Corpus . We use the code base from the blog 6 . We use batch size = 327 , 680 tokens . For the calculation of the blockwise Hessian spectra , we apply SLQ to all parameter blocks except for the LayerNorm layers . In total , it takes about 12 hours on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum . • GPT2 - nano ( 11M ) on Shakespeare . We use the code base of NanoGPT 7 . We use batch size = 163 , 840 tokens . For the calculation of the blockwise Hessian spectra , we apply SLQ to all parameter blocks with even indices , except for the LayerNorm layers . In total , it takes about 12 hours on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum . • GPT2 ( 125M ) on Openwebtext . We use the code base of NanoGPT . We use batch size = 245 , 760 tokens . Due to the large number of parameters , we are not able to calculate the blockwise Hessian spectra for all parameter blocks . Instead , we apply SLQ to : the embedding layer ; the output layer ; the 1 - st , 4 - th , 8 - th , 12 - th attention blocks ; and the 1 - st , 4 - th , 8 - th , 12 - th MLP blocks ( note that the 12 - th attention and MLP blocks are the final ones ) . In total , it takes about 7 days on one A100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum . We train all the models under the default configurations in all cases . For CNN training , we follow the rule of thumb to choose the learning rate of AdamW : we set the learning rate of AdamW to be 100 × smaller than the recommended learning rate of SGD . C . 2 Implementation Details on Figure 2 We use the code base of NanoGPT to train a decoder - only Transformers on 4 consecutive tokens randomly selected from Openwebtext . We set the model configuration as : context window = 2 , number of heads = 2 and the embedding dimension = 4 . In MLP layers , all widths equal to 4 . We change the number of layers from 1 to 4 . We remove all the Layer Norms in the model . The rest of the model configurations are set to their default values in the code base . We compute the Hessian on all the parameters blocks in attention and MLP layers . In Figure 2 ( a ) , the variables in three blue boxes corresponds to the parameters in Query and Key ; Value and Projection ; and MLP , respectively . Similarly for the rest of the figures . Due to the intensive overhead of computing and storing the whole Hessian , we have yet to check the block - diagonal structure on larger models . Rigorously speaking , so far we have not gotten sufficient evidence to claim this structure commonly holds in larger Transformers . It requires new numerical methods to efficiently check the block - diagonal Hessian structure in larger models without explicitly calculating them . We leave it as an interesting future direction . For completeness , we remark that Collobert [ 2004 ] , Roux et al . [ 2007 ] , Martens and Grosse [ 2015 ] also observed the block - diagonal structure in ( approximated ) Hessian of small - scaled MLPs . We restate their findings in Figure 9 . [ Collobert , 2004 , Section 7 ] further theoretically proved that the block diagonal structure stems from ( i ) the layer - by - layer structure and ( ii ) the Cross - Entropy loss . These results suggest that block - diagonal structure might be common in NNs . C . 3 Implementation Details on the MLP experiments in Figure 5 We train a 4 - layer MLP on MNIST . We use batch size = 128 and width = 300 , 128 , and 64 for the hidden layers . We use ReLU activation . We change the degree of heterogeneity by scaling the output of each layer with constant c ∈ N . We scale c from 1 to 15 . We record the average JS distance between the initial Hessian spectra of each pair of layers and find that the average JS distance increases as c increases . As such , we define c as the degree of block heterogeneity . For each c , we train SGD and Adam with default hyperparameters by grid - searching the learning rate from 1e - 4 to 1e - 1 and report the best test accuracy after 1 epoch . 6 https : / / medium . com / data - and - beyond / complete - guide - to - building - bert - model - from - sratch - 3e6562228891 7 https : / / github . com / karpathy / nanoGPT / 26 ( a ) Trained Hessian of a MLP in [ Collobert , 2004 ] , Figure 7 . 4 ( b ) Approximated trained Hessian of a MLP in [ Roux et al . , 2007 ] , Figure 1 ( c ) Approximated trained Hessian of a MLP in [ Martens and Grosse , 2015 ] , Figure 6 Figure 9 : The block - diagonal structure in the ( approximated ) Hessian of MLPs reported in the literature . c 1 2 3 4 5 6 7 8 9 - 15 Blockwise distance 1e - 3 1 . 61 33 . 99 65 . 32 260 . 77 298 . 39 351 . 69 353 . 48 NaN Table 3 : Consider a 4 - layer MLP on MNIST . c denotes the scaler that is multiplied to the output of each layer . Blockwise distance denotes the average JS distance between the initial Hessian spectra of each pair of layers . We find that the average JS distance increases as c increases . For c ≥ 9 , the calculation of JS distance encounters numerical instability due to the division of near - zero values and returns NaN . D More Experimental Results D . 1 Performance comparison of AdamW and SGD on Different Architectures Here , we show the performance comparison of AdamW and SGD on different models . 0 20 40 60 80 Epoch 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 T r a i n i n g A cc u r a c y AdamWSGD ( a ) ResNet18 0 20 40 60 80 Epoch 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 T r a i n i n g A cc u r a c y AdamWSGD ( b ) VGG16 Figure 10 : Performance of AdamW and SGD on CNNs including ResNet18 and VGG16 . SGD and Adam perform similarly on these tasks . 27 0 10 20 30 40 Epoch 0 . 2 0 . 4 0 . 6 0 . 8 T r a i n i n g A cc u r a c y AdamWSGD ( a ) ViT 0 20000 40000 60000 80000 100000 120000 140000 Epoch 2 4 6 8 10 T r a i n i n g L o ss AdamWSGD ( b ) BERT 0 100 200 300 400 500 Epoch 0 1 2 3 4 T r a i n i n g L o ss AdamWSGD ( c ) GPT2 - nano 0 100 200 300 400 500 600 Epoch 3 4 5 6 7 8 9 10 11 T r a i n i n g L o ss AdamWSGD ( d ) GPT2 Figure 11 : Performance of AdamW and SGD on Transformers including ViT , BERT , GPT2 - nano , and GPT2 . SGD performs significantly worse than Adam on these tasks . E Proofs E . 1 Proof of Proof of Proposition 1 Let H = (cid:20) L 0 0 µ (cid:21) , where L > µ > 0 . We choose the initial point as w 0 = ( w 01 , w 02 ) = ( (cid:112) µ / L , (cid:112) L / µ ) . By the update rule of GD , we have L ( w t + 1 ) = L (cid:0) w t − η ∇L ( w t ) (cid:1) = 1 2 ( w t − η Hw t ) T H ( w t − η Hw t ) = ( w t 1 ) 2 | 1 − η L | L + ( w t 2 ) 2 | 1 − ηµ | µ = | 1 − η L | t L µ L + | 1 − ηµ | t µ L µ = µ | 1 − η L | t + L | 1 − ηµ | t ( 10 ) To proceed , we discuss the following cases : When η ≤ 1 / L , since | 1 − η L | t and | 1 − ηµ | t are monotonically decreasing , the optimal solution is η = 1 / L . When η ≥ 1 / µ , since | 1 − η L | t and | 1 − ηµ | t are monotonically increasing , the optimal solution is η = 1 / µ . When 1 / L ≤ η ≤ 1 / µ , ( 10 ) can be written as g t ( η ) = µ ( η L − 1 ) t + L ( 1 − ηµ ) t . Take the first - order and the second - order derivative of the g , we can obtain g ′ t ( η ) = tL µ ( η L − 1 ) t − 1 − t µ L ( 1 − ηµ ) t − 1 and g ′′ t ( η ) = t ( t − 1 ) L 2 µ ( η L − 1 ) t − 2 + t ( t − 1 ) µ 2 ( 1 − ηµ ) . Since g ′′ t ( η ) ≥ 0 for all η ∈ [ 1 / L , 1 µ ] , the function g is convex . By solving the equation that g ′ t ( η ) = 0 , we can obtain η = 2 L + µ is a solution for all t . Plugging this result into ( 10 ) and rearranging the terms , we conclude the proof of Proposition 1 . E . 2 Proof of Theorem 1 Without loss of generality , we assume h = 0 . This is because minimizing L ( w ) = 12 w T Hw − h T w is equivalent to minimizing L ( w ) = 12 ( w − w ∗ ) T H ( w − w ∗ ) where w ∗ = H − 1 h . By a linear transformation z = w − w ∗ , BGD for minimizing 12 ( w − w ∗ ) T H ( w − w ∗ ) starting from w 0 is equivalent to BGD for minimizing 12 z T Hz starting from z 0 = w 0 − w ∗ . Thus we can assume w ∗ = 0 , or equivalently , h = 0 . When h = 0 , the update form of BGD becomes w t + 1 = w t − η D − 1 BGD Hw t , 28 where D BGD = diag     η 1 , · · · , η 1 (cid:124) (cid:123)(cid:122) (cid:125) d 1 times , · · · , η L , · · · , η L (cid:124) (cid:123)(cid:122) (cid:125) d L times   T   ∈ R d × d . In the following , we will first prove the rate of BGD for each block and then combine them together to get the total rate . For the l - th block , we choose η l = 1 λ l , 1 . The update rule of BGD for the l - th block becomes : w t + 1 l = w tl − 1 λ l , 1 H l w tl . Now we apply the standard convergence proof of GD to the l - th block . By descent lemma , we have [ L ( w t + 1 ) ] l ≤ [ L ( w t ) ] l + ⟨ w t + 1 l − w tl , [ ∇L ( w t ) ] l ⟩ + λ l , 1 2 ∥ w t + 1 l − w tl ∥ 22 ( 11 ) ≤ [ L ( w t ) ] l + ⟨ w t + 1 l − w tl , H l w tl ⟩ + λ l , 1 2 η 2 w tl H Tl H l w tl ( 12 ) = [ L ( w t ) ] l − 1 2 λ l , 1 w tl H Tl H l w tl ( 13 ) ≤ [ L ( w t ) ] l − λ l , d l 2 λ l , 1 w tl H l w tl ( 14 ) = (cid:18) 1 − 1 κ l (cid:19) [ L ( w t ) ] l ( 15 ) Substract both sides by L ∗ and we get [ L ( w t + 1 ) ] l − [ L ∗ ] l ≤ (cid:16) 1 − 1 κ l (cid:17) (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) . Summing up both sides over l ∈ [ L ] , we have L ( w t + 1 ) − L ∗ = L ∑ l = 1 [ (cid:16) L ( w t + 1 ) ] l − [ L ∗ ] l (cid:17) ≤ L ∑ l = 1 (cid:18) 1 − 1 κ l (cid:19) (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) ≤ max l ∈ [ L ] (cid:18) 1 − 1 κ l (cid:19) L ∑ l = 1 (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) = max l ∈ [ L ] (cid:18) 1 − 1 κ l (cid:19) (cid:0) L ( w t ) − L ∗ (cid:1) . This concludes the proof of Theorem 1 . E . 3 Proof of Theorem 2 Similarly as the proof of Theorem 1 , we assume h = 0 . The update rule of Adam becomes w t + 1 = w t − η ( D 0 Adam ) − 1 Hw t , where D 0 Adam = diag ( ∇L ( w 0 ) ◦ ∇L ( w 0 ) ) 12 = diag ( | Hw 0 | ) . We denote d t = η ( D 0 Adam ) − 1 Hw t and thus we have w t = 1 η H − 1 D 0 Adam d t and w t + 1 = w t − d t . These relations also hold for each block by changing the notation to H l w tl , D 0 Adam , and d tl , etc . . Following the framework in [ Sun and Ye , 2021 ] , we try to bound the error yet to be optimized ( a . k . a . , cost - to - go ) and the per - step improvement , respectively . The ratio of these two terms characterizes the rate of convergence . We now express both terms using d tl . For the cost - to - go term for the l - th block , we have 29 [ L ( w t ) ] l − [ L ∗ ] l = 1 2 ( w tl ) T H l w tl = 1 2 η 2 ( d tl ) T D 0 Adam , l H − 1 l D 0 Adam , l d tl . ( 16 ) For the per - step improvement , we have [ L ( w t ) ] l − [ L ( w t + 1 ) ] l = 1 2 ( w tl ) T H l w tl − 1 2 ( w t + 1 l ) T H l w t + 1 l = 1 2 ( w tl ) T H l w t + 1 l − 1 2 ( w tl − d t ) T H l ( w tl − d tl ) = ( d tl ) T H l w tl − 1 2 ( d tl ) T H l d tl = 1 2 ( d tl ) T (cid:18) 2 η D 0 Adam , l − H l (cid:19) d tl . ( 17 ) To proceed , we denote ˆ H = ( D 0 Adam ) − 1 H and we denote its eigenvalues as ˆ λ 1 ≥ ˆ λ 2 ≥ · · · ˆ λ d . Similarly , we denote ˆ H l = ( D 0 Adam , l ) − 1 H l and its eigenvalues ˆ λ l , 1 ≥ ˆ λ l , 2 ≥ · · · ˆ λ l , d l . Let η = min l ∈ [ L ] C l , 1 , we have [ L ( w t ) ] l − [ L ∗ ] l [ L ( w t ) ] l − [ L ( w t + 1 ) ] l = 1 η 2 ( d tl ) T D 0 Adam , l H − 1 l D 0 Adam , l d tl ( d tl ) T (cid:16) 2 η D 0 Adam , l − H l (cid:17) d tl ≤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 η 2 (cid:18) 2 η D 0 Adam , l − H l (cid:19) − 1 D 0 Adam , l H − 1 l D 0 Adam , l (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 ( 18 ) ( ∗ ) ≤ C 2 l , 2 λ 2 l , 1 ( min l ∈ [ L ] C 2 l , 1 ) λ l , 1 λ l , d l ( 19 ) ≤ (cid:32) max l ∈ [ L ] C 2 l , 2 C 2 l , 1 (cid:33) κ l , ( 20 ) where ( ∗ ) is due to : by Assumption 1 , D 0 Adam , l ≼ C l , 2 λ l , 1 I , 2 η D 0 Adam , l − H l ≽ (cid:16) 2 C l , 1 C 1 l , λ l , 1 − λ l , 1 (cid:17) I ≽ λ l , 1 I , where ≼ and ≽ are matrix inequalities . By rearranging both sides of ( 20 ) , we have [ L ( w t + 1 ) ] l − [ L ∗ ] l ≤   1 − 1 (cid:32) max l ∈ [ L ] C 2 l , 2 C 2 l , 1 (cid:33) κ l   (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) . Summing up both sides over l ∈ [ L ] and we conclude the proof . L ( w t + 1 ) − L ∗ = L ∑ l = 1 (cid:16) [ L ( w t + 1 ) ] l − [ L ∗ ] l (cid:17) ≤ L ∑ l = 1   1 − 1 (cid:18) max l ∈ [ L ] C 2 l , 2 C 2 l , 1 (cid:19) κ l   (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) ≤ max l ∈ [ L ]   1 − 1 (cid:18) max l ∈ [ L ] C 2 l , 2 C 2 l , 1 (cid:19) κ l   L ∑ l = 1 (cid:0) [ L ( w t ) ] l − [ L ∗ ] l (cid:1) = max l ∈ [ L ]    1 − 1 (cid:18) max l ∈ [ L ] C 2 l , 2 C 2 l , 1 (cid:19) κ l    (cid:0) L ( w t ) − L ∗ (cid:1) . 30