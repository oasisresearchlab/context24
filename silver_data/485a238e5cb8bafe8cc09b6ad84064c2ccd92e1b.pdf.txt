72 A Data - Driven Analysis of Behaviors in Data Curation Processes LEI HAN , TIANWA CHEN , GIANLUCA DEMARTINI , MARTA INDULSKA , and SHAZIA SADIQ , The University of Queensland , Australia Understanding how data workers interact with data , and various pieces of information related to data prepa - ration , is key to designing systems that can better support them in exploring datasets . To date , however , there is a paucity of research studying the strategies adopted by data workers as they carry out data preparation activities . In this work , we investigate a specific data preparation activity , namely data quality discovery , and aim to ( i ) understand the behaviors of data workers in discovering data quality issues , ( ii ) explore what factors ( e . g . , prior experience ) can affect their behaviors , as well as ( iii ) understand how these behavioral observations relate to their performance . To this end , we collect a multi - modal dataset through a data - driven experiment that relies on the use of eye - tracking technology with a purpose - designed platform built on top of iPython Notebook . The experiment results reveal that : ( i ) ‘copy – paste – modify’ is a typical strategy for writing code to complete tasks ; ( ii ) proficiency in writing code has a significant impact on the quality of task performance , while perceived difficulty and efficacy can influence task completion patterns ; and ( iii ) searching in external resources is a prevalent action that can be leveraged to achieve better performance . Furthermore , our exper - iment indicates that providing sample code within the system can help data workers get started with their task , and surfacing underlying data is an effective way to support exploration . By investigating data worker behaviors prior to each search action , we also find that the most common reasons that trigger external search actions are the need to seek assistance in writing or debugging code and to search for relevant code to reuse . Based on our experiment results , we showcase a systematic approach to select from the top best code snippets created by data workers and assemble them to achieve better performance than the best individual performer in the dataset . By doing so , our findings not only provide insights into patterns of interactions with various system components and information resources when performing data curation tasks , but also build effective and efficient data curation processes through data workers’ collective intelligence . CCS Concepts : • Information systems → Web log analysis ; Data cleaning ; • Human - centered comput - ing → User studies ; Web - based interaction ; Empirical studies in HCI ; User interface design ; User centered design ; Empirical studies in interaction design ; Additional Key Words and Phrases : Interaction behavior , search pattern , data curation ACM Reference format : Lei Han , Tianwa Chen , Gianluca Demartini , Marta Indulska , and Shazia Sadiq . 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes . ACM Trans . Inf . Syst . 41 , 3 , Article 72 ( February 2023 ) , 35 pages . https : / / doi . org / 10 . 1145 / 3567419 This work is partially supported by the ARC Discovery Project ( Grant No . DP190102141 ) , and the ARC Training Centre for Information Resilience ( Grant No . IC200100022 ) . Authors’ addresses : L . Han , T . Chen , G . Demartini , and S . Sadiq , School of Information Technology and Electrical Engineer - ing , GP South Building ( 78 ) , Staff House Road , The University of Queensland , St Lucia , QLD 4072 , Australia ; M . Indulska , School of Business , Room 514 , Joyce Ackroyd Building , The University of Queensland , St Lucia , QLD 4072 , Australia . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2023 Association for Computing Machinery . 1046 - 8188 / 2023 / 02 - ART72 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3567419 ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 2 L . Han et al . 1 INTRODUCTION Data scientists , or more generally data workers , spend up to 80 percent of their time on data explo - ration and curation activities to ensure adequate data quality for the task at hand , and to facilitate data reuse over time [ 10 , 20 ] . Such a labor intensive activity , usually without immediate and ex - plicit added value , can be a bottleneck to analytics for organizations expecting to make strategic decisions based on the data . This bottleneck often causes cost and time overruns in data science projects , and , thus , motivates the investigation of how data curation processes can be improved to save time and cost ( e . g . , by means of automatic processes ) . Data curation involves a variety of activities , such as , for example , data analysis , format transformations , de - duplication , identifica - tion of illegal or missing values , and conflict detection and resolution [ 12 , 40 ] . Among these tasks , a significant amount of human effort ( e . g . , manual inspection ) is usually invested because tools that support data curation tasks are often domain - focused ( e . g . , only designed for name or address fields ) and difficult to use in coordination with other program functionalities [ 40 ] . Although there are several tools 1 for detection and repair of data quality issues [ 48 ] , the long - tail distribution and imbalance between depth and scope of data quality issues [ 9 , 43 ] mean that algorithmic solutions are not yet effective in tackling the variety of data curation tasks . Therefore , manual work from data workers still plays an important role in improving data quality and usability [ 3 , 34 ] . This re - sults in those who use the data ( e . g . , data consumers ) undertaking data curation activities in an ad - hoc manner without following well - defined processes or guidelines . Such activities lack stan - dardization ( of the curation processes ) , repeatability , as well as scalability [ 12 ] . At the same time , the inherent complexity of data curation tasks makes it challenging to design and construct unified data curation processes and , in turn , difficult to build automatic data curation systems . Given the increasing demand for more cost - efficient data curation processes , researchers have started to look at how data workers engage with data ( e . g . , [ 34 ] ) . Such studies enable the design of data exploration and curation systems that can better support data workers in their tasks . Unfortunately , there is a paucity of research focused on how data workers interact with various pieces of information ( e . g . , code examples ) and what processes they follow while carrying out data curation activities . Yet , understanding how data workers progress in data curation tasks is crucial as it enables the discovery of ways to reduce human effort and to potentially improve the quality of data curation outcomes in data science projects . In this paper , we take a multi - modal and data - driven approach to understand how data work - ers use resources and tools to support their interaction with the given dataset to undertake data curation , specifically data quality discovery . We consider what information they interact with and what self - defined processes they follow while performing data curation tasks . To understand what factors are correlated to the performance of data workers in data curation activities , we extend pre - vious research [ 16 ] by reporting our observations of the subjective ratings provided by the partici - pants on their perceived efficacy and difficulty of the tasks as well as the reported knowledge level on data quality and programming . In other words , we study data worker behaviors as they discover data quality issues for a given dataset , and we explore how these behavioral observations relate to the curation outcome quality and their perceptions of the task ( e . g . , temporal changes in behavior with respect to the perceived difficulty ) . Furthermore , on the basis of variability in participant per - formance , we investigate ways to combine the various participant approaches and build effective and repeatable data curation processes by harnessing the collective intelligence of top - performers . Accordingly , we thus focus on the following research questions ( RQs ) : • ( RQ # 1 ) What influences the task performance of data workers in their data exploration and curation activities ? Data worker performance can be influenced by many factors , such as 1 https : / / www . informatica . com / au / data - quality - magic - quadrant . html . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 3 their previous experience in programming and data cleaning , prior knowledge of data quality dimensions , and their self - perceived task difficulty and efficacy . Thus , we explore how these factors are related to their actual performance . • ( RQ # 2 ) How do data workers search for information and use resources to support task comple - tion ? Because data workers interact with data by various means , from programming lan - guages to visualization tools [ 34 ] , we believe that surfacing the underlying data to the data workers’ view would help them working with data . We also hypothesize that providing data manipulation code snippets can help data workers in performing data curation tasks and improves their efficiency . • ( RQ # 3 ) What strategies do data workers adopt to perform data curation tasks and how do these strategies vary from worker to worker ? By exploring the relationship between observed pat - terns and strategies used to perform the tasks , we are able to examine if these patterns are consistent with those of software developers and how the observed strategies differ among different data workers with respect to their performance quality . • ( RQ # 4 ) When and why do data workers search for information ? We study the relationship between each search action and the behaviors displayed prior to the search . Specifically , we create vector representations of these behaviors and select the ( top - 3 ) most important actions before search as a proxy for what triggers the search behaviors . Different from [ 16 ] , where each search event was studied separately , we aggregate the behavior prior to each search event on a per - participant basis as the number of the performed search events may vary across different participants . This allows us to identify how the patterns triggering these searches differ from one worker to another . • ( RQ # 5 ) How can effective and efficient data curation processes be built based on the contributions made by different ( top - performing ) data workers ? We investigate the Python code constructed by experiment participants to carry out data curation tasks . By extracting ( part of ) the best code snippets , we are able to assemble them and generate a collection of the best code . This enables us to understand whether a combination of the best approaches contributed by different data workers can achieve a quality better than the best individual performer . To address these questions , we conduct a lab - based user study , using both eye - tracking tech - nology as well as a purpose - designed data curation platform built on top of iPython Notebook . 2 Through the study , involving a total of 39 participants , we create a multi - modal dataset of inter - action behaviors , including : ( i ) behavioral actions logged by our platform , ( ii ) areas of attention captured by an eye - tracker , and ( iii ) annotations created by participating data workers . Our unique collection of complementary data capturing participating data worker behavior enables us to : ( i ) an - alyze the effectiveness of different components in our platform design for them to identify data quality issues , ( ii ) recognize usage patterns for the available information resources , and ( iii ) dis - cover strategies adopted for task completion . We also collect the participants’ self - perceptions ( e . g . , perceived confidence and difficulty ) related to our tasks , through pre - and post - experiment ques - tionnaires . This additional data enables us to explore what factors ( e . g . , prior experience ) can affect their task completion process and to understand how task completion patterns relate to perceived effort . Furthermore , through the Python code provided by our data workers , we study how to pro - duce an effective and robust process assembled from top performers’ solutions to discovering data quality issues . Our paper unfolds as follows . First , in Section 2 we discuss current research related to data curation and understanding user behaviors . We then describe our experimental design and 2 https : / / ipython . org / notebook . html . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 4 L . Han et al . methodology , explaining how they relate to our research questions ( Section 3 ) . Next , in Section 4 we present our results , followed by a discussion of our findings and their implications ( Section 5 ) . Finally , in Section 6 we conclude the paper with a summary of our contributions . 2 RELATED WORK In this section , we present previous research related to understanding data workers and their be - haviors in a broader scope . 2 . 1 Data Workers and Data Quality Since the quality of data - driven analysis for knowledge mining is dependent on the quality of the data to be examined , data curation plays an essential step to generate high quality analytical models before the data can be consumed [ 12 , 34 ] . For instance , from recent interviews on data practices with AI practitioners , Sambasivan et al . [ 46 ] presented empirical evidence that conven - tional AI ( e . g . , machine learning ) practices may undervalue data quality issues and can thus lead to data cascades such as compounding events in negative and downstream effects caused by data quality issues . Their results showed that such negative effects are prevalent , invisible , and some - times delayed to observe , but are often avoidable . Therefore , such practical evidence calls for data quality discovery as an essential data preparation activity before the data can be consumed . Understanding a dataset , or broadly data - centric sensemaking , involves various interaction be - havior between the data consumer and the dataset . Koesten et al . [ 25 ] found that the three most common patterns of data - centric sensemaking include data inspection , content engagement , and placing data in broader contexts . Their results suggest that tools facilitating sensemaking and sub - sequent data reuse can be designed in a way that highlights rather than flattens out odd values . This allows the users of the data to become aware of the presence of potential data issues . As our study also focuses on data quality discovery , the design our platform aligns with these findings and requires users to add tag labels to indicate data quality issues as expected during data curation processes . Existing research has also established evidence that understanding problematic rows in a dataset and data manipulation takes a large proportion of time and effort in data analysis and cleaning ac - tivities [ 15 , 26 , 40 ] . Rahm and Do [ 40 ] summarized that major data quality issues can be classified as schema - level and instance - level problems . While schema - level issues can be addressed by schema improvement and evolution , instance - level problems ( e . g . , inconsistencies or errors ) are the pri - mary focus in data cleaning activities . In their work , the authors outlined the major steps of data cleaning and claimed that substantial manual work is still required as even advanced tools are not effective to cover the entire data quality problems in a given dataset . Thus , understanding how data workers engage with data in practice may lead to improvements in the design of data analyt - ics systems . Muller et al . [ 34 ] summarized five types of practices that data workers engage with : discovery , capture , curation , design , and summary . Their results show that the working dataset used by data workers should be cleaned and well - prepared before it can be used for analytics pur - poses . This process is time - consuming and usually requires domain - specific knowledge . Koesten et al . [ 26 ] conducted an in - depth interview to understand the challenges in collaborative practices of pre - processing raw data . They claimed that providing metadata such as table headers would increase the usability of a dataset . Their results showed that sharing code of data operations can also facilitate data comprehension through the changes that have been made . Kandel et al . [ 20 ] proposed an interactive system ( known as Wrangler ) for semi - automatic data transformations . Their results showed that the proposed system can significantly reduce the time and manual effort required and that it can benefit novice users performing data processing tasks . Guo et al . [ 15 ] aug - mented this system and proposed to proactively recommend mapping schemas to transform raw ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 5 data into the format ( e . g . , relational tables ) required for analysis . Their method , however , showed little help for novice users to reduce cognitive effort and completion time . Sutton et al . [ 48 ] pro - posed an automatic system ( known as Data Diff ) to detect data quality issues . By differentiating repeated ( i . e . , periodically collected ) data samples , their method is able to generate summary of differences , and thus to provide interpretation of the differences that are likely to be data quality issues ( e . g . , format inconsistencies ) . Such system , however , suffers from flexibility that is limited to only work on recurringly generated dataset . These existing research and tools show that while so - lutions to curate data exist , there is a lack of knowledge on how data workers interact and on what works best . Our work looks at data worker interaction with such tools and it is thus orthogonal to above research . Given the findings in existing research , in our design we surface the underlying working dataset by showing a table with headers to facilitate data workers comprehend the data . We also build our data curation platform on top of iPython Notebook where the history of the curation activities are recorded and earlier versions of the working data can be accessed if needed . Through a data - driven approach , where our observations consist of 50M data points collected from an in - lab experiment , we investigate their behaviors and how the observed behaviors relate to the quality of data curation outcomes . Prior research has looked at the new challenges incurred by computational notebook as well , as such techniques has become popular for data scientists [ 8 , 11 , 18 ] . Chattopadhyay et al . [ 8 ] sum - marized several high - impact challenges when data scientists analyze data through notebook ( e . g . , Jupyter ) , including : exploring coding history of data exploration , searching for assistance of con - structing code ( e . g . , auto completion ) , and scheduling process automation that consists of multiple processes . They also found that data scientists should spend substantial time on open resources ( e . g . , documentation ) to help them write appropriate code to complete their work , or to adjust available code and to fix issues to make the code work for their tasks . Drosos et al . [ 11 ] proposed an interaction model ( knowns as Wrex ) to facilitate data scientists to preform operations on grid and tables . By providing functionalities that allow users to manipulate the working data , their model is able to generate code snippets based on the manipulation and the users can apply this auto - constructed code to an unexplored dataset . Their programming - by - example method , how - ever , is subject to how the user samples the data to provide manipulation examples ( e . g . , the first five rows versus the last five rows in the dataset ) , which may influence the effectiveness of the generated code wrangling the entire dataset . To address such issues , we ask data workers to write code by themselves to explore the entire dataset , and we then look at what processes they follow to perform data curation operations on the entire dataset . We allow them to write or copy any code based on their own preference . As coding assistance has been shown to be important for data scientists [ 8 ] , in our experimental platform we provide several code snippets ( which are sufficient but may not be optimal to complete the tasks ) with examples for each task and present them in the direct view of our data workers . Thus , through their interactions with available information , we study how they carry out data curation activities . 2 . 2 Understanding Code Writing Behaviors Related to our goal of studying data worker behaviors , previous related work has looked at soft - ware development practices [ 14 , 37 , 56 ] . Specifically , research has looked at how developers effi - ciently manipulate ( source ) code [ 33 ] and perform debugging [ 37 ] , avoid interruptions [ 56 ] , and at how they can be supported by appropriate user interfaces [ 14 ] . In terms of behavioral patterns observed in software developers , Kim et al . [ 22 ] observed that due to some limitations of program - ming languages , developers inevitably perform copy / paste actions to replicate code snippets ( e . g . , system functions ) . Therefore , some developers start to reuse copied code as templates and then they just need to customize the pasted code to new context , which is more efficient . Ko et al . [ 24 ] ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 6 L . Han et al . showed that developers spend significant amounts of time searching for relevant program code , followed by time spent on understanding it with very limited context . They observed how such activities can negatively affect the productivity of software developers . More recently , a number of works have been looking at how software developers work with code , in particular looking at code search and code reuse strategies that can increase their productivity [ 1 , 29 , 35 , 44 , 45 , 54 ] . Narasimhan and Reichenbach [ 35 ] showed that ‘copy – paste – modify’ is a strategy of code reuse , which can generally speed up the code construction process . By studying copy / paste actions , Ahmed et al . [ 1 ] showed that developers often perform copy / paste within the same file , while users in non - development tasks usually copy content across different windows . Lin et al . [ 29 ] pro - posed a dynamic recommendation system that can automatically provide suggestions based on pasted code and the context where the developer is working . Based on this research , in our work , we look at how data workers reuse existing data curation code and at how providing data workers with a set of pre - constructed code snippets can support them in their tasks ( e . g . , allowing them to search for code and information online ) . 2 . 3 Tracking Behaviors in Practice In order to track users’ focus on specific UI elements , researchers have used eye - tracking technol - ogy [ 2 , 6 , 19 , 21 ] to measure users’ attention at the HTML DOM element - level or even pixel - level . Eye - tracking techniques , however , usually require to be run in a lab setting which makes the scale of user studies using such approaches limited . To deal with this scalability issue and to comple - ment the observations that can be done using eye - tracking , researchers have been looking for alternative approaches . Viewport time [ 27 ] has been proposed to analyze mouse scroll behavior , so as to determine the level of user engagement in the context of reading news articles . Grusky et al . [ 13 ] concluded that viewport time , which performs a balance between expensive eye - tracking lab experiments and high - level web analytics , can help researchers understand online attention ( e . g . , focusing on images ) from millions of users . Mehrotra et al . [ 32 ] has combined viewport timeline with mouse cursor and keyboard timeline as a whole set of features to measure how well a web search system is able to help users . Bhattacharya and Gwizdka [ 5 ] used purpose - designed inter - face to capture users’ search behavior and eye gaze to study the process of acquiring knowledge during search . Liu et al . [ 30 ] studied web search behaviors to understand the relationship between task through intention to behavior in search activities . While most of these works are related to web search engines , none of them focuses on behav - ioral analyses of users involved in data curation tasks , as we do in this paper . In contrast to these prior works , we make complementary use of both eye - tracking devices and behavioral log data to understand the interaction of data workers with data curation tasks . Specifically , we study the patterns of their interaction with different available resources , the changes of subjects’ attention on different parts of the interface , the strategies of writing code snippets to complete tasks , and the quality of their contributions . Following a research methodology that makes use of both qualitative and quantitative methods to conduct a comprehensive analysis [ 5 , 55 ] , we create a multi - modal dataset of interaction behaviors collected as the participating data workers are completing data curation tasks . 3 EXPERIMENTAL DESIGN To study the behaviors of data workers in data curation activities , we asked each participant to identify certain data quality issues in a given dataset through a lab experiment , where we were able to use advanced tracking devices ( e . g . , eye - trackers and activity loggers ) to capture the interaction behaviors of data workers . We recruited our participating data workers by inviting undergraduate ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 7 and master students studying data science 3 at our university on a voluntary basis . To be quali - fied for our experiment , we required all participants to have some working knowledge in Python . We did not set any limitations on their understanding of data quality ( as we covered this in the pre - study tutorial ) , nor limited them on how much time they could take to complete all tasks . By participating in the experiment , each of our data workers was offered a $ 30 voucher as compensa - tion for their time . Overall , each participant follows the same experimental procedure , which consists of four com - ponents : ( i ) pre - experiment survey , where we obtained participant demographics ( e . g . , major sub - ject , year of study , programming experience such as preferred languages and years of use , relevant courses learnt such as programming , information system and data science ) ; ( ii ) tutorial , in which we provided the definition together with examples of each type of data quality issues , followed by a practical session for participants to get familiar with our purpose - designed platform ; ( iii ) formal experiment , where participants were asked to identify the injected data quality issues in a given dataset by writing Python code through our platform ; and ( iv ) post - experiment survey , where we collected data on the perceived task load of the formal experiment . In the following , we first de - scribe our data curation tasks ( in Section 3 . 1 ) , followed by providing details of our platform design ( in Section 3 . 2 ) . Finally , in Section 3 . 3 we describe the multimodal datasets that we have collected in our experiment . 3 . 1 Data Curation Tasks In our experiment , each participant is asked to investigate a given dataset and to identify five types of data quality issues that we have injected into the given dataset . We define the identification of a particular type of data quality issue as a task , and therefore our data workers are asked to complete five tasks in the experiment . They should write Python code to explore and investigate the given dataset and can access the data through a Pandas dataframe . Upon discovery of certain data quality issues , they have to add tags to indicate that the error exists in a specific attribute for a specific record . They need to add ( or remove ) these tags through our purpose - designed platform ( explained in Section 3 . 2 ) . The dataset that participants are asked to work on contains 13 000 records 4 and four columns prepared along with Parallel Data Generation Framework ( PDGF ) [ 39 , 49 ] . We manually in - ject five types of data quality issues ( one for each task ) : ( i ) missing ( completeness ) , ( ii ) non - unique ( uniqueness ) , ( iii ) duplicate ( redundancy ) , ( iv ) imprecise ( precision ) , and ( v ) inconsistent ( consis - tency ) values . 5 Table 1 shows the definitions of these data quality issues and the number of in - stances injected in the working dataset for each task . Before the formal experiment , all participants are asked to self - report the level of knowledge they have in data quality and their confidence in using Python through a pre - experiment sur - vey . Following the same self - evaluation approach used in existing research [ 38 , 47 ] , we ask them to provide a score for both questions by using a 10 - point Likert scale . To estimate the confi - dence level of writing Python code , we adopt the rationale used in software engineering [ 47 ] to ask participants to self - evaluate their own confidence level without clarifying what we mean by 3 All data science students in our university complete a mandatory programming course in the first semester of their program . Accordingly , they have at least one - semester experience working with Python to complete their course ( e . g . , homework assignments and projects ) . 4 The number of records is guided by the consideration that participants should not be able to do it manually without writing code . 5 Toenablefollow - upresearch , wemakebothourdatasetaswellasthegroundtruthannotationsofhowweinjectedthedata quality issues publicly available through . https : / / github . com / tomhanlei / 22tois - data - quality - discovery / blob / main / dataset - with - groundtruth . csv . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 8 L . Han et al . Table 1 . Data Quality Issues Manually Injected in the Working Dataset in Formal Experiment Type # Instances Definition missing ( M ) 1263 the value of a specific attribute being empty for a record non - unique ( U ) 5001 different records identified by the same key duplicate ( D ) 5609 multiple records being identical with respect to all attributes imprecise ( P ) 8106 degree of accuracy for the same attribute varying across different records inconsistent ( C ) 15738 ∗ format of the values for the same attribute varying across different records ∗ For format inconsistency issues , each participant may choose one format as ‘standard’ and thus label other formats to be inconsistent . Therefore , we present the minimum possible number of this type of issue to show a lower bound , where we assume the participant picks the one format having the maximum number of instances in the dataset as ‘standard’ . confidence . Thus , they are able to develop their intuitive perception of confidence so as to assess their confidence level for themselves , which has been recommended for undergraduates [ 47 ] . After the pre - experiment survey , we provide to each participant a tutorial . All participants read the same tutorial material , where we explain all five types of data quality issues that are covered in the study in the order of missing , non - unique , duplicate , imprecise , and inconsistent ( see Table 1 ) . 6 We provide definitions and examples for each type of data quality issue . 7 After reading the tutorial material , each participant has an opportunity to practice with our data curation platform over an artificial dataset . The aim of this practice is to allow participants to get familiar with our platform . Thus , we use a different dataset in the practice to mitigate any learning effects . This dataset contains 100 records and four columns that are different from the dataset used in the formal experiment . We do not limit the time participants spend on the tutorial , so they can start the formal exper - iment at their discretion . Upon completion of the formal experiment , the participants are asked to self - evaluate their perceptions of the tasks through a post - experiment survey , where we use the subjective workload assessment questionnaire ( NASA - TLX ) [ 7 , 17 ] to collect their perceived workload in our experiment . As a proxy of perceived efficacy and difficulty of the tasks , we use questions about performance and effort in NASA - TLX to measure how successful and how hard the participants evaluate their task performance . The two surveys ( i . e . , pre - and post - experiment sur - veys ) allow us to understand how the performance of data curation tasks can be influenced by the proficiency in both Python and data quality as well as the intensive human effort ( e . g . , perceived difficulty ) ( see RQ # 1 ) . To facilitate ease of use by participants and avoid confusion , all ratings in the two surveys are given using a uniform 10 - point Likert scale from 1 ( lowest ) to 10 ( highest ) . 3 . 2 Data Curation Platform In this section , we describe in detail our purpose - designed data curation platform used in the experiment . Inspired by existing data exploration platforms ( e . g . , Talend Cloud API 8 ) and data analytics software ( e . g . , RapidMiner 9 ) that contain interactive functions on the left side and on the right side they show data statistics and visualizations , we design our platform having three panels : internal resources on the left , working console in the middle , and surfacing the underlying dataset on the right . By looking at how participants interact with these three panels , we are able to understand when and how these available resources can help them ( see RQ # 2 ) and to discover 6 In the formal experiment , participants decide the order in which to complete tasks by themselves . 7 All participants ( data science students from our university ) have studied a mandatory course where data quality is covered as a fundamental topic and is assessed in the exam extensively . Furthermore , our tutorial materials were written using natural language with real - world examples , and participants were allowed to refer to these material at any time during the formal experiment . Therefore , we consider the participants to have an acceptable level of the understanding of the considered data quality issues ( see Table 1 ) . 8 https : / / www . talend . com / products / application - integration / cloud - api - services / . 9 https : / / rapidminer . com / . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 9 Fig . 1 . Interface of the data curation platform used in our work . In the left panel , data workers can pick among a list of code snippets ( i . e . , DataOps ) which they can copy / paste into the Notebook and adapt to their needs . In the middle panel , users have a Jupyter Notebook where they can explore and manipulate a given dataframe . In the right panel , users can manually browse the dataset and see which elements have been annotated for data quality issues . task completion strategies ( e . g . , copy / paste , see RQ # 3 ) to achieve better performance in the experiment . Figure 1 shows the user interface ( UI ) of our data curation platform , which is based upon Jupyter Notebook . In order to provide a set of internal data investigation resources , we pre - define 21 code snippets ( which we call DataOps [ 50 ] throughout this paper ) and show them in the left panel of the UI , ranging from importing essential libraries to complex Boolean operations involving regular expressions . Among these 21 DataOps , the participants can choose from 6 to 8 code snippets and use them in combination for each task ( some code fits multiple tasks ) . All the provided code snippets were generated from our pilot study . Considering that there may exist alternative approaches , we retain different code even if it performs the same functionality . The full list of the provided DataOps can be found in Appendix A . Note that the DataOps that we have provided are sufficient ( but may not be optimal ) to complete all tasks without the need to refer to external resources . Each DataOp comes with a description of its functionality , an explanation of how to use it ( e . g . , how to adjust parameters ) , and a code snippet that participants can copy to the Notebook and use to explore the dataset and discover data quality issues in it . We also provide a copy button for each DataOp to facilitate them performing the copy action . In the middle panel , we provide a standard Jupyter Notebook for participants to perform any operations ( either copied from DataOps or written in Python from scratch ) to explore the given dataset and to identify any data quality issues . Since we aim at analyzing the interaction behaviors and understand which processes they follow to complete the given data curation tasks , we ask them not to change any executed operation that they have performed , and thus have disabled the cut button in the build - in tool panel of the Notebook . To understand the benefits of bringing the underlying data into the direct view of the participants , we introduce a data view in our UI design ( i . e . , right panel , see Figure 1 ) , where we use a table to show ( part of ) the underlying data to provide data workers with an instrument to comprehend the data structure and with the ability to make ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 10 L . Han et al . comparisons between rows and columns . In the right panel , participants can also add or remove tags indicating identified data quality issues either via the toolkit provided in the top right area of the UI or by clicking the tag labels shown in each cell in the data view . In order to enable a smooth use of the toolkit ( e . g . , to add or remove tags ) , we disable several hotkeys and shortcuts ( e . g . , ‘ d , d ’ , ‘ 0 , 0 ’ , etc . ) that are pre - defined by Jupyter Notebook . We use Chrome browser extensions to inject the left and right panels on top of the Notebook , as well as to capture the interaction behaviors by JavaScript . All participants are free to use available DataOps provided by the platform as well as to go online and look externally for information or code . Participants are only allowed , however , to use the given browser throughout the experiment as we need to track their actions . Our design of the right panel is based on the assumption that showing a sample of the working data is beneficial to data workers . Therefore , by looking at how participants interact with the given platform and perform external search actions , we are able to capture when and how these external search events are triggered and to discover search patterns ( see RQ # 4 ) . 3 . 3 Collected Data During the experiment , we collect behavioral data from two sources : ( i ) interactions with our in - terface , and ( ii ) captured user gaze with eye - tracking devices . To capture the interaction behaviors of our data workers completing the tasks , we log the following low - level actions by JavaScript : • clicks on any DataOps in the left panel ; • clicks on copy buttons beside DataOps ; • paste actions performed in the middle panel ; • clicks to add or remove tags in the right panel ; • adding or removing tags by the toolkit in the right panel ; • change between browser tabs ; • viewport size of the browser ; 10 • layout of HTML ( DOM ) elements in the browser ; and • positions of the scrolling bars . To capture the gaze position of participants during their interaction with our platform UI throughout the experiment , we use a Tobii Pro Spectrum eye tracker to capture the screen co - ordinates which they look at together with timestamps . We then use these coordinates as proxy to participants’ attentions to any element of our UI . Because of the property of the eye - tracking device which captures the gaze position over the entire screen , we ask our data workers not to zoom - in the browser while they are performing the tasks . Apart from the collected behavior dataset , we also collect the solutions written in form of Python code that our data workers have provided to perform data curation tasks . This allows us to discover possible ways to construct from the code an efficient and effective data curation process to achieve a better performance for all tasks ( see RQ # 5 ) . All participants have been informed and given consent to collect and analyze their behav - ioral data and the provided code that are needed for the study , and the study has received ethics approval by the review board of the authors’ institutions . 4 RESULTS Following the experimental design described above , we conducted a lab study involving 39 par - ticipants . Participants spent on average 63 minutes 11 to complete the experiment . Overall , we 10 We log viewport size of the browser to verify that the browser is in full screen mode , which is required by the eye - tracker . 11 We do not account for the time spent on the tutorial , which was 6 minutes on average . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 11 Table 2 . Definition of Classification Conditions for each Data Quality Issue Tag Conditions Details of definition True Positive ( TP ) the tag given by the participant exists in the ground truth True Negative ( TN ) the tag not given by the participant does not exist in the ground truth False Positive ( FP ) the tag given by the participant does not exist in the ground truth False Negative ( FN ) the tag is not given by the participant but exists in the ground truth collected 70 000 behavioral log entries , 900 000 data quality issue tags , and 53 million gaze position data points . In this section , we first present the results of our data workers performance in terms of the quality of the provided tag labels , followed by a detailed analysis of participating data worker interaction behaviors across three dimensions : ( i ) the time spent on external resources , ( ii ) changes of gaze position with respect to the three UI panels ( i . e . , the interaction with internal resources ) , and ( iii ) the source of copied content and patterns of content reuse . Then , we study information search patterns by looking at the behaviors displayed prior to each search action . We extract the most important actions that have triggered search behaviors . This analysis also provides insights on how these interaction patterns vary from worker to worker . Finally , we showcase the generation of a collective data curation process ( which we refer to as the ‘Golden Notebook’ throughout the paper 12 ) that assembles ( part of ) the code used by best performers . We use three criteria to assess the quality of the generated Golden Notebook , namely effectiveness ( i . e . , how accurately data quality issues are detected ) , robustness ( i . e . , whether the produced data processing code can be generalized to unseen data ) , and refinement ( i . e . , how high - recall and high - precision steps can be combined together to reduce the overall curation effort ) . 4 . 1 Quality of Task Performance ( RQ # 1 ) To measure the quality of the tag labels given by the participants , we use standard evaluation metrics such as precision , recall , and F - measure . Table 2 shows how we compute the different tagging cases which are then used to compute these metrics . For each provided tag , we compare it with the available ground truth tag . If the tagged data appears in ground truth , we consider this a true positive ( TP ) . Otherwise , this tag is regarded as a false positive ( FP ) . Similarly , for each tag in the ground truth , we check if the participant has added it to the created annotations . If a certain tag has not been assigned by the participant , we consider this to be a false negative ( FN ) . This allows us to compute F1 scores for the tags given by each participant and to measure the quality of the generated annotations . Note that there are five types of data quality issues ( i . e . , tasks ) that we inject into the dataset and participants may complete just some ( but not all ) of the tasks at their own discretion ( as participating in our study is completely voluntary ) . Thus , we do not consider the tasks that have not been attempted when measuring performance quality for each participant . For the ‘format inconsistency’ type , participants may elect any format as the ‘standard’ one and label others as data quality issues ( see Table 1 ) . Thus , we select the one that maximizes the number of TPs 13 to measure their performance . Since F1 scores range from 0 to 1 , we set the threshold at 0 . 5 to define high performers ( i . e . , those who have an F1 score greater than 0 . 5 ) and low performers ( i . e . , those who are not high performers ) , as the F1 scores are not evenly distributed . Overall , 11 ( out of 39 in 12 See https : / / github . com / tomhanlei / 22tois - data - quality - discovery / blob / main / golden - notebook . pdf for the generated ‘Golden Notebook’ . 13 By definition of F1 score , the one maximizing the number of TPs also maximizes F1 score . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 12 L . Han et al . Fig . 2 . Quality of data quality issue tags given by the participants in the attempted tasks . The types are sorted by the order of explanations of different data quality issues in the tutorial . Table 3 . Number of Participants in each Group Grouping Criterion Value Distribution # Participants Total High Low High Low Quality F1 Score mean = 0 . 722 , median = 0 . 724 mean = 0 . 138 , median = 0 . 024 11 28 39 Pre - Survey Python Confidence mean = 7 . 5 , median = 7 . 0 mean = 3 . 8 , median = 4 . 0 22 17 Knowledge of Data Quality mean = 6 . 7 , median = 7 . 0 mean = 2 . 8 , median = 3 . 0 19 20 Post - Survey Perceived Efficacy mean = 7 . 5 , median = 7 . 0 mean = 2 . 9 , median = 3 . 0 23 16 Perceived Difficulty mean = 8 . 0 , median = 8 . 0 mean = 3 . 8 , median = 4 . 0 27 12 total ) participating data workers are grouped as high performers while the other 28 participants are classified as low performers ( see Table 3 ) . 4 . 1 . 1 Objective Task Performance Quality . Figure 2 shows the quality of the tags given by partic - ipants , in terms of precision , recall , and F1 score over different data quality issue types . It is clear that the precision and recall of finding ‘missing’ data errors is the highest among all five types of data quality issues , which indicates that this task is the easiest one to identify among those considered in our study . The F1 score for ‘inconsistent’ data errors get the second best score on average ( median 14 ) among the five tasks , 15 while this type of data quality issue is explained at last in the tutorial materials . This shows that participant performance does not become poorer with the increase of cognitive load when they read more content in the tutorial materials ( e . g . , the task related to an issue that was explained last in the tutorial had the second best overall performance among the five tasks ) . Instead , their performance bears the implications on their preferences and the difficulty of the tasks . Considering that the definitions of ‘non - unique’ and ‘duplicate’ data are similar to some extent , we manually merge the labels for these two types of data quality issues , and thus the average ( median ) precision ( and F1 score ) of the labels for ‘non - unique’ rises from 0 . 465 ( and 0 . 053 ) to 1 . 0 ( and 0 . 099 ) . This confirms that many participants suffer some confusion when distinguishing these two data quality issues and consequently mislabel these tags . Our data also shows that precision 14 We use median values due to the fact that these scores are not interval scaled . 15 The median values of the F1 scores for the five given tasks are ( from highest to lowest ) : ( i ) missing 0 . 380 , ( ii ) inconsistent 0 . 248 , ( iii ) non - unique 0 . 053 , ( iv ) duplicate 0 . 031 and ( v ) imprecise 0 . 011 . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 13 Fig . 3 . Quality ( adjusted F1 ) over perceived efficacy . Fig . 4 . Precision ( adjusted by un - attempted tasks ) over Python confidence . is higher than recall for all tasks on average ( median values ) , 16 which shows that discovering all instances of potential data quality issues requires more sophisticated efforts than just identifying them . 4 . 1 . 2 Quality over Subjective Ratings . To understand how performance in the experiment varies across different participants , we group them according to five criteria : ( i ) the quality ( measured by F1 scores ) of the generated tags , ( ii ) Python confidence and ( iii ) levels of knowledge in data quality issues that are self - reported in the pre - experiment survey , and self - perceived ( iv ) efficacy and ( v ) difficulty of the experiment that are measured by the post - experiment survey . Table 3 shows the population in each of these groups . As subjective ratings ( i . e . , about confidence , knowledge , efficacy and difficulty ) range from 1 ( lowest ) to 10 ( highest ) , we group participants with scores greater than 5 into groups with high scores , while others are assigned to the low score groups . To investigate how various factors can influence task performance as well as the decision to select a task to complete , in this subsection we also consider the tasks that participants have not attempted and assign 0 as the quality score to the un - attempted tasks . Figure 3 shows the quality in terms of adjusted F1 by un - attempted tasks over perceived efficacy . It is evident that for the task of identifying ‘imprecise’ and ‘inconsistent’ errors , those who perceive success ( high efficacy group ) in the experiment have achieved a better quality , as compared to the low efficacy group . Using Mann - Whitney U test 17 [ 31 ] , we conclude that this difference is statistically significant ( p < 0 . 01 after Bonferroni correction ) . This observation indicates that these two tasks dominate the perception of success while the participants progress in the experiment . As we have more instances of these two types of data quality issues than other considered issue types in our experiment ( see Table 1 ) , the perceived performance quality on these two tasks affect the perceived efficacy of the entire experiment . Thus , those in the high efficacy group ( who have perceived success ) have better overall performance than those in the low efficacy group ( Mann - Whitney U test , p < 0 . 01 after Bonferroni correction ) . Figure 4 shows the quality in terms of adjusted precision over Python confidence . It is evident that in the task of identifying ‘inconsistent’ errors , those with high Python confidence achieved higher precision in comparison to those with low Python confidence ( Mann - Whitney U test , 16 In this comparison , we eliminate the effect of misunderstanding between ‘duplicate’ as ‘non - unique’ by merging the two labels . 17 WeuseMann - Whitney U testtoevaluatethestatisticalsignificancethroughout thispaper , aswefocusonthecomparison between the two groups ( see Table 3 ) and there exist some extreme values ( e . g . , number of copy / paste actions ) having an influence on interval - scaled distribution . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 14 L . Han et al . Fig . 5 . KDE of the Time spent on ( a ) the platform and ( b ) external pages . p < 0 . 01 after Bonferroni correction ) . This indicates that identifying ‘inconsistency’ issues re - quires more sophisticated use of Python ( e . g . , regular expressions ) to achieve higher precision . By other grouping criteria ( as defined in Table 3 ) , we do not find significant quality differences ( in precision , recall and F1 ) between the groups . 4 . 2 Interaction with Available Information ( RQ # 2 ) To study how data workers search for information and use available resources to support their work , we investigate the interaction activities performed by our experiment participants . In our experimental design , all participants interact with the given browser to complete tasks , starting from ( i ) practice in the tutorial page , followed by ( ii ) completing the formal tasks in the experiment page . During the experiment , they are allowed , at any point in time , to ( iii ) visit external online resources ( e . g . , Google or StackOverflow ) to assist them to complete the tasks ( they can do so by switching to a different browser tab ) . Thus , we distinguish two sources of available information : ( i ) external resources that the participants look at while completing tasks , and ( ii ) internal resources that exist in the three panels of our platform UI ( see Figure 1 ) In the following , we first report the interaction patterns with the external resources , followed by the analysis of the interaction with internal resources . This enables us to understand which type of information can assist data workers in what way ( e . g . , reducing perceived difficulty ) to carry out data curation tasks . 4 . 2 . 1 Searching External Resources . Figure 5 shows the kernel density estimation ( KDE ) of the time spent on different webpages with respect to the tag quality . The time is normalized by the entire duration of the experiments . We can see that high performers ( i . e . , those with high F1 scores ) spent less time on the data curation platform UI and more time looking at external resources , compared to low performers ( Mann - Whitney U test , p < 0 . 05 ) . This result indicates that referring to external resources is a common activity , which potentially leads to better task performance ( if used appropriately ) . To understand when referring to external resources happens throughout the experiment , we divide the experiment duration ( normalized across participants ) into five equal - length segments , and define each 20 % progress proportionate to the duration of each participant as a step . For each step , we compute the ratio of the time spent on external webpages , which allows us to capture the evolution of their attention on external resources . Figures 6 ( a ) and 6 ( b ) presents the time spent on external resources over steps with respect to Python confidence and perceived efficacy . We can see that those who are more confident in Python ( Figure 6 ( a ) ) and those who have higher perceived efficacy ( Figure 6 ( b ) ) spent more time on external resources in the early stages ( i . e . , Step 1 or first ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 15 Fig . 6 . Evolution of time spent on external resources during the experiment with respect to Python confi - dence ( a ) and perceived efficacy ( b ) , and time spent on the platform with respect to task performance ( c ) . 20 % in time ) of the experiment , compared to those in ‘low’ groups ( Mann - Whitney U test , p < 0 . 05 after Bonferroni correction ) . In other words , these results show that experienced participants often refer to external resources to support their work and that this leads to a better perceived efficacy . By comparing the time spent on external resources between the groups with respect to perceived difficulty and data quality knowledge ( see Table 3 ) , we do not observe significant differences . 4 . 2 . 2 Time Spent on Internal Resources . Given that our participants spent most of their time on the experiment page as they were completing the tasks , we also analyzed the evolution of time spent on the data curation platform . Figure 6 ( c ) shows the evolution of time spent on the platform over the progress steps . We observe that those who stick to working within the platform in the last 20 % of the experiment ( by means of the time spent ) achieve better quality ( F1 score ) , while low performers spend significantly less time interacting with the platform in the last step ( Mann - Whitney U test , p < 0 . 01 after Bonferroni correction ) . Next , we look at the time spent on each of the three ( i . e . , left , middle and right ) UI panels , to understand how areas of interest ( AOI ) have changed through the experiment . Specifically , with the dataset produced by our eye - tracker , we are able to understand how the participating data workers interact with the internal resources provided ( i . e . , DataOps in the left panel and data view in the right panel ) . To this end , we remove the data points captured by eye tracker during the period when the participant is not interacting with the platform ( e . g . , searching external resources ) . In addition , to study how useful the data view is , we do not consider the gaze data when they are looking at the toolkit panel presented at the top right corner of the UI ( see Figure 1 ) as looking at the toolkit is a mandatory action to add or remove tags if they explore the given dataset by writing code . By the definition of step ( see Section 4 . 2 . 1 ) , we conduct a temporal analysis of the evolution of the attention on the internal resources . Since the same left panel also exists in tutorial page , we consider the tutorial as an additional step prior to the first step while interacting with the left panel . Figure 7 presents the time spent on the left panel ( i . e . , DataOps ) over steps with respect to the quality of performance , Python confidence , and perceived efficacy of the tasks . It is evident that the average ( median ) time spent on DataOps is decreasing as the participants progress in the experiment , showing that they become familiar with the functionalities of these DataOps as they complete more tasks and thus do not need to refer to these resources . We also observe that high performers and those who are confident in Python spend less time on DataOps at the beginning of the experiment ( Figure 7 ( a ) , ( b ) ) , compared to their counterparts . The difference is statistically significant at Step 1 between high and low performers ( Mann - Whitney U test , p < 0 . 05 after ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 16 L . Han et al . Fig . 7 . Evolution of time spent in DataOps ( i . e . , left panel ) over steps with respect to performance quality ( a ) , Python confidence ( b ) and perceived efficacy ( c ) . Fig . 8 . Evolutionof timespentin left ( a ) middle ( c ) andright ( b ) , ( d ) panels overstepswithrespecttoperceived difficulty ( a ) , ( b ) and efficacy ( c ) , ( d ) of the experiment . Bonferroni correction ) and at the practice phase between high and low Python confidence groups ( p < 0 . 01 after Bonferroni correction ) , respectively . This can be explained by the fact that these participants do not rely much on the provided DataOps as a start - up facility to complete our tasks . In particular , those who are confident in Python are more proficient in finding relevant code exter - nally ( e . g . , from Google ) and spend more time on external resources than those with less Python confidence . Moreover , those who have higher perceived efficacy progressively spend less time on DataOps during task completion ( see Figure 7 ( c ) ) . This indicates that they become familiar with the provided DataOps as they progress in the experiment . By contrast , those who are less confi - dent in their success ( i . e . , low efficacy group ) spend a longer time on DataOps , and the difference between the two groups is statistically significant at the last step ( Mann - Whitney U test , p < 0 . 05 after Bonferroni correction ) . Figure 8 shows the evolution of time spent on the three UI panels over steps with respect to perceived difficulty and efficacy of the experiment . We can see that those who perceive the task as difficult have spent longer time on DataOps in the middle stage ( Step 2 and 3 ) of task completion process , as compared to those in low difficulty group ( Figure 8 ( a ) ) ( Mann - Whitney U test , p < 0 . 05 after Bonferroni correction ) . Meanwhile , the participants in the low difficulty group have spent longer time on data view ( i . e . , right panel ) than those who self - reported higher difficulty ( Figure 8 ( b ) ) ( p < 0 . 01 at Step 3 after Bonferroni correction ) . These observations confirm that DataOps provided assistance for those having difficulties to progress in tasks , and that surfacing the working data in the data view can reduce the perceived difficulty of the task . On the other hand , compared to those who reported lower confidence of success in the experiment , those who ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 17 Table 4 . Comparisons Among the Median Time Spent on the Three UI Panels as Well as on External Resources for each Group ( see Table 3 ) Over Steps Group Step 1 Step 2 Step 3 Step 4 Step 5 Quality high NB > Ex > Ops > DV NB > Ex > DV > Ops NB > Ex > Ops > DV NB > Ex > DV > Ops NB > DV > Ex > Ops low NB > Ops > DV > Ex NB > DV > Ops > Ex NB > Ops > DV > Ex NB > DV > Ex > Ops NB > DV > Ex > Ops Python high NB > Ops > Ex > DV NB > DV > Ex > Ops NB > DV > Ops > Ex NB > DV > Ex > Ops NB > DV > Ex > Ops Confidence low NB > Ops > DV > Ex NB > Ops > DV > Ex NB > Ex > Ops > DV NB > Ex > Ops > DV NB > DV > Ops > Ex Knowledge high NB > Ops > Ex > DV NB > Ex > DV > Ops NB > Ex > Ops > DV NB > Ex > DV > Ops NB > DV > Ex > Ops low NB > Ops > DV > Ex NB > Ops > DV > Ex NB > Ex > Ops > DV NB > Ex > DV > Ops NB > DV > Ex > Ops Efficacy high NB > Ops > Ex > DV NB > DV > Ex > Ops NB > Ex > DV > Ops NB > DV > Ex > Ops NB > DV > Ex > Ops low NB > Ops > DV > Ex NB > Ops > DV > Ex NB > Ops > Ex > DV NB > Ex > Ops > DV NB > Ops > DV > Ex Difficulty high NB > Ops > DV > Ex NB > Ops > Ex > DV NB > Ops > Ex > DV NB > Ex > DV > Ops NB > DV > Ex > Ops low NB > Ops > Ex > DV NB > DV > Ex > Ops NB > DV > Ex > Ops NB > Ex > DV > Ops NB > DV > Ex > Ops Notation : Ops : DataOps ( left panel ) , NB : Notebook ( middle panel ) , DV : data view ( right panel ) , Ex : external resources . have higher perceived efficacy spend less time on the Notebook ( i . e . , middle panel , see Figure 8 ( c ) ) ( Mann - Whitney U test , p < 0 . 05 at Step 2 after Bonferroni correction ) and longer time on the data view ( i . e . , right panel , see Figure 8 ( d ) ) ( p < 0 . 05 at Step 2 and 4 after Bonferroni correction ) . This reveals that interaction with the data view can assist participants in gaining confidence in performing tasks successfully . We do not find a significant difference of the time spent on the three UI panels ( i . e . , left , middle and right ) between the participants with high - and low - levels of self - reported knowledge in data quality ( see Table 3 ) . Table 4 shows a summary of comparisons among the median time spent in interacting with available information as well as different UI panels of the platform ( i . e . , DataOps on the left , Notebook in the middle , data view on the right and external resources ) for each group over steps . We observe that the time spent on the Notebook is the largest in all steps on average , as the participants need to write code to progress in tasks . In addition , high performers ( i . e . , those with high F1 scores ) spend the second largest proportion of time on searching external resources ( from Step 1 to 4 ) , while low performers have a lower level of external interaction . Interaction with the DataOps occupies the second largest proportion of time for those who are in the high difficulty group ( from Step 1 to 3 ) . By contrast , those who have less difficulty in the experiment spend longer time on DataOps just at Step 1 but then less time on that than on data view and external resources from Step 2 onward . This indicates that the provided DataOps are useful for them to start up the task and to refer to when they encounter difficulties . On the other hand , those with high Python confidence spend longer time on the data view than on the left panel and external resources ( from Step 2 to 5 ) , while those in low Python confidence group have a lower level of interaction with the data view ( from Step 1 to 4 ) . This shows that the participants who are more confident in Python can be more focus on the working data rather than on constructing code snippets either from DataOps or externally . Moreover , more interaction with the data view can increase the perceived efficacy as the participants in high efficacy group spend the second most of their time on the right panel ( at Step 2 , 4 , and 5 ) while those in the low efficacy group have a low level interaction with the right panel throughout the experiment . Other patterns of interaction with available information we have observed in the experiment include : ( i ) participants spend a large proportion of time on DataOps at the first step , while they spend a lot of time on the data view at the last step ; ( ii ) participants with high Python confidence spend more time on the data view than DataOps while those who are less confident in Python spend more time on DataOps than the data view ; ( iii ) those who perceive low efficacy in the tasks spend less time on the data view than DataOps while more interaction with the data view as they progress in tasks ( e . g . , from Step 2 onward ) can increase their perceived success in the experiment ; and ( iv ) high performers ( in the high F1 group ) consistently spend a longer time in searching external resources than on DataOps during the entire experiment while low performers spend ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 18 L . Han et al . Fig . 9 . Frequency ( in log scale ) of copy / paste - related actions observed in the experiment with re - spect to F1 score . Fig . 10 . A 2D projection ( by PCA ) of the behavior vectors in each observing window prior to external search actions . more time on DataOps than external resources up to Step 3 ( i . e . , having spent 60 % time relative to the entire experiment duration ) . We do not observe significant differences of interaction patterns between the groups with respect to the reported data quality knowledge levels . 4 . 3 Strategies for Content Reuse ( RQ # 3 ) To understand what patterns of content reuse are exhibited by the participating data workers , we examine their copy / paste actions that are logged during the experiment . By looking at each piece of pasted text , we check if the text matches any existing content ( i . e . , DataOps , any content in the tutorial page or in the experiment page ) . In the tutorial and experiment pages , we additionally separate code snippets and outputs as defined in a Jupyter iPython Notebook . To examine text matches , we introduce two types of match : ( i ) exact match , which indicates copy / paste without adjustments , and ( ii ) similarity - based match , where the similarity score between the pasted text and the source content exceeds a preset threshold . To compute the similarity between two pieces of text , we use the Ratcliff - Obershelp similarity metric [ 42 ] defined as the longest common sub - string divided by the length of pasted text ( length counted as the number of characters ) . Thus , by this definition , similarity scores range from 0 ( replacing the copied content by totally different text ) to 1 ( copying without modification ) . Using the method described above , we analyze the patterns of copy / paste actions exhibited by the participating data workers . We investigate these actions from two perspectives : ( i ) the source of the text they copied , and ( ii ) the way in which they first copy textual content and then mod - ify it to complete certain tasks . In terms sources of copied content , we consider five possibilities : ( i ) DataOps in the left UI panel , ( ii ) Python code snippets that the participants have previously constructed in the tutorial page while practising , ( iii ) Python code snippets that the participants wrote in previous steps 18 in the current experiment page , ( iv ) previously returned outputs in the experiment page , and ( v ) textual content from external pages . Figure 9 shows the frequency of the observations related to copy / paste actions . We observe that high performers ( with a better quality ) more frequently copy external resources , compared to low performers ( Mann - Whitney U test , p < 0 . 05 ) . Indeed , those who get higher F1 scores spend more time on external pages and less time on DataOps ( see Figure 5 ( b ) , 7 ( a ) , and Table 4 ) as compared to those with lower F1 scores . Therefore , it is evident that high performers look at external resources and reuse more code from 18 Note that we ask participants to not delete any operations that have been performed in previous steps . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 19 external resources . The DataOps that we provide are insufficient for them to achieve their intended goals . Another observation we can make is that those with high F1 scores copies their own code snip - pets ( constructed in the previous steps ) more often , compared to low performers . This can be explained by two reasons : ( i ) because we ask the participants not to change any executed opera - tions , copying what they have written and then modifying the pasted content becomes an essen - tial step to debug their code ; and ( ii ) reusing existing code is a common strategy to write code to conduct data curation . To analyze these two reasons , we compare the frequency of copying from previously generated code with the frequency of ‘ paste – modify ’ actions , with respect to their F1 scores . We find that those with high F1 scores have performed significantly more copy actions from self - constructed code than the performed ‘ paste – modify ’ actions ( Mann - Whitney U test , p < 0 . 05 after Bonferroni correction ) , while we do not observe a significant difference between the two sets of actions among low performers . On the other hand , copying from self - generated code is performed more frequently by the participants in high F1 group , compared to low perform - ers ( Mann - Whitney U test , p < 0 . 01 after Bonferroni correction ) . These results show that reusing existing code ( with or without adaptation ) is a common strategy adopted by high performers in their task completion , which potentially leads to better performance . 4 . 4 External Search Patterns ( RQ # 4 ) Motivated by the observation that high performers spend significantly more time on , and copy more text ( code snippets ) from , external resources than low performers ( see Table 4 , Figure 5 ( b ) and 9 ) , we study how the patterns triggering these external searches differ from one worker to an - other . To this end , we investigate the behaviors displayed prior to each external search action for all participants . 19 Specifically , we look at the interactions that are performed in a short period ( which we call ‘observing window’ in the following ) 20 preceding a search event . We examine three types of interaction behaviors in each observing window : ( i ) actions natively captured by JavaScript as raw logs ( e . g . , mouse clicks , adding tags , changing browser tabs ) , ( ii ) actions derived by differenti - ating two consecutive logs ( e . g . , execution of code in the middle panel determined by comparing the layout of HTML DOM elements ) , and ( iii ) actions aggregated from the observations presented in previous sections ( e . g . , copying from DataOps or external resources ) . Considering that the dis - played behaviors consisting of a sequence of actions are analogous to a document composed of a sequence of words , we adopt TF - IDF [ 41 ] as the metric to evaluate the importance of each action in the observing windows . This allows us to set up vector representations of the behaviors displayed before searching for external resources . Figure 10 visualizes the TF - IDF vectors of behaviors in each observing window using principal component analysis ( PCA ) [ 51 ] to reduce their dimen - sions . We observe that the vectors for the high and low performer group are distributed along different directions in the space . This indicates that the importance of each action in the observing window for high performers differs from that for low performers , which means that the emphasis patterns of each action before a search event vary between the high and low groups . Motivated by this result , we then look into the relationship between the ( most ) important actions and the performed search behaviors in each group . We look at how the patterns of external search vary from each participant to another . Consid - ering that one participant may search external resources multiple times during experiment , we average the behavior vectors for each participant . This allows us to extract the important actions 19 In our experiment , we have observed a total of 662 external searches . 20 Since the median value of the time span between two consecutive external search actions is 101 seconds , we define each observing window as 51 seconds . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 20 L . Han et al . Table 5 . Frequency of Top - 3 Important Actions that Trigger External Search Behaviors for each Participant High F1 Low F1 write code 100 % write code 96 . 4 % add box ( Notebook ) 63 . 6 % add tag ( click ) 42 . 8 % add tag ( panel ) 27 . 3 % add box ( Notebook ) 32 . 1 % tab practice page 27 . 3 % add tag ( panel ) 28 . 6 % copy previous code ( exp ) 27 . 3 % tab practice page 25 . 0 % left - click on DataOps 18 . 2 % left - click on DataOps 25 . 0 % copy external 18 . 2 % copy external 17 . 9 % remove tag ( click ) 9 . 1 % remove tag ( click ) 17 . 9 % add tag ( click ) 9 . 1 % remove tag ( panel ) 7 . 1 % copy DataOps 3 . 6 % copy previous result 3 . 6 % High Difficulty Low Difficulty write code 96 . 3 % write code 100 % add box ( Notebook ) 44 . 5 % add tag ( panel ) 50 . 0 % add tag ( click ) 33 . 4 % add box ( Notebook ) 33 . 4 % left - click on DataOps 29 . 6 % add tag ( click ) 33 . 4 % tab practice page 25 . 9 % tab practice page 25 . 0 % add tag ( panel ) 18 . 5 % copy external 25 . 0 % remove tag ( click ) 18 . 5 % remove tag ( click ) 8 . 3 % copy external 14 . 8 % copy previous code ( exp ) 8 . 3 % copy previous code ( exp ) 7 . 4 % remove tag ( panel ) 8 . 3 % remove tag ( panel ) 3 . 7 % left - click on DataOps 8 . 3 % copy DataOps 3 . 7 % copy previous result 3 . 7 % Actions are sorted by the number of performing participants prior to search . Frequency scores are normalized across participants in each group . preceding search events for a particular participant . To achieve this , we select the top - 3 actions by TF - IDF values from each averaged behavior vector and use the selected actions as a proxy for the reasons that triggered the search behaviors . 21 Table 5 presents the frequency of the top - 3 ( impor - tant ) actions that trigger external search behaviors for each participant . It is evident that writing code is the most common reason to search for external resources among all participants as most of the external searches are triggered by writing code in each group . In the high - performers group ( i . e . , high F1 ) , the second most popular pattern is that they search externally after ‘adding box’ in the iPython Notebook , which is performed by 63 . 6 % of the participants in the high F1 group . This shows that high performers tend to search for external resources right after code execution and before writing new code . This can be explained by participants ( i ) looking for external information to address the output from the previous operation ( e . g . , error message ) , or ( ii ) searching relevant code from scratch to perform new steps . Such patterns of code writing , however , are less popular in the low - performers group , where just 32 . 1 % of the participants in the low F1 group search after ‘adding box’ in the Notebook . Among low - performers , 42 . 8 % have clicked the data view in the right panel ( see Figure 1 ) to add tags before an external search ( ranked 2nd by frequency ) , while this search pattern among high performers appears much less ( ranked as last ) frequently . This may indicate the need to search external resources to consolidate the comprehension of data quality issues . Indeed , as low performers may not be proficient in generating code to explore the given dataset , the data view offers them effective assistance to understand the underlying data structure and format . Similarly , a half ( and another 8 . 3 % ) of those in the low difficulty group have added ( and removed ) tags by the toolkit ( see Figure 1 , the top right corner of UI ) just before external searches , while this pattern is observed by just 18 . 5 % ( and 3 . 7 % ) of the participants who perceive the experi - ment as more difficult . Because adding or removing tags through the toolkit is a mandatory action to provide annotations if the participants write code to explore the working data , this observation indicates that those who have more difficulties in the experiment are less flexible about writing or searching relevant code to perform tasks ( and thus make less use of the toolkit to provide an - notations ) . Moreover , the pattern of removing tags through the toolkit before external search has not been observed in the high F1 group , while 7 . 1 % of the low performers exhibit this pattern . This may be because high performers do not need to revise their annotations as they explore the dataset nor to refer to external resources to check their revised tag labels . Another common reason 21 This method is different from the analysis of search patterns presented in [ 16 ] , where we showed how the search patterns vary among the 662 search events . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 21 Fig . 11 . Diversity of the behaviors prior to external search actions . Behavior vectors are computed by se - quences of ( a ) single action and ( b ) two consecutive ( 2 - gram ) actions . for external search among high performers ( observed by 27 . 3 % performers ) is copying previously generated code from earlier steps of the experiment . Because we ask participants not to delete ( or change ) anything they have previously executed , a convenient way to debug their code ( when er - ror messages occur ) is to copy and paste the code into a new box to modify it . Therefore , copying their code before an external search demonstrates that the participant has to search for external resources to correct their code . By contrast , this pattern was not observed among low performers . On the other hand , as another code construction pattern , copying from external resources is more popular in low difficulty than high difficulty group . Those who perceive a higher level of diffi - culty , however , have clicked on DataOps before external search ( observed by 29 . 6 % performers ) , while this pattern is ranked as the least popular in the low difficulty group . These findings show that those who are more skilled in using external resources to perform efficient code construction and debugging perceive the experiment as less difficult , while the provided DataOps are helpful for those who encounter difficulties to progress in tasks ( e . g . , copy a DataOp to external search engine to obtain adaptation suggestions ) . To investigate the diversity of the behaviors prior to external search events , we compute the pairwise cosine similarity of the behavior vectors ( averaged for each participant ) in each group ( see Table 3 ) . Figure 11 presents the similarity of the behaviors that have triggered external search . We can see that those with a high F1 score have exhibited more similar behaviors than those in the low F1 group ( Figure 11 ( a ) ) ( Mann - Whitney U test , p < 0 . 01 after Bonferroni correction ) . This indi - cates that high performers have particular ways involving external search behaviors to construct code to achieve a higher quality . To the contrary , the participants in the high efficacy group have exhibited more diverse behaviors prior to external search events , as compared to those in the low efficacy group ( Mann - Whitney U test , p < 0 . 01 after Bonferroni correction ) . This suggests that those who are more confident in success in the experiment have performed more diverse behav - iors to external search , while those who are less confident to succeed have shown limited usage patterns of external search to help them progress in tasks . To understand how the granularity of the behaviors can affect the observation of external search patterns , we use sequences of 2 - gram ( i . e . , two consecutive ) actions 22 instead of single actions and repeat the process of setting up 22 We do not use longer n - grams as in that case too few participants share the same behaviors which limits the ability to extract commonalities . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 22 L . Han et al . Table 6 . Top 5 Performers ( PIDs ) and their Performance Scores for each Type of Error Ranking Missing ( M ) Duplicate ( D ) Non - unique ( U ) Imprecise ( P ) Inconsistent ( C ) 1 P2 , P3 , P6 , P8 , P9 , P10 , P11 P2 , P3 , P6 P46 P42 P2 P14 , P16 , P41 , P45 , P50 , P58 P8 , P11 p = 1 . 0 , r = 1 . 0 p = 1 . 0 , r = 1 . 0 p = 0 . 951 , r = 0 . 847 p = 0 . 312 , r = 1 . 0 p = 0 . 664 , r = 1 . 0 F 1 = 1 . 0 F 1 = 1 . 0 F 1 = 0 . 896 F 1 = 0 . 475 F 1 = 0 . 798 2 P46 , P47 P45 P8 , P14 P2 , P6 , P14 P6 p = 0 . 998 , r = 1 . 0 p = 0 . 996 , r = 1 . 0 p = 0 . 572 , r = 0 . 823 p = 1 . 0 , r = 0 . 302 p = 0 . 637 , r = 1 . 0 F 1 = 0 . 999 F 1 = 0 . 998 F 1 = 0 . 675 F 1 = 0 . 463 F 1 = 0 . 778 3 P5 P1 P3 , P6 , P50 , P58 P58 P14 p = 1 . 0 , r = 0 . 498 p = 0 . 989 , r = 1 . 0 p = 0 . 465 , r = 1 . 0 p = 0 . 333 , r = 0 . 302 p = 0 . 603 , r = 0 . 842 F 1 = 0 . 665 F 1 = 0 . 995 F 1 = 0 . 635 F 1 = 0 . 317 F 1 = 0 . 703 4 P33 P50 , P58 P2 P8 P9 p = 0 . 995 , r = 0 . 496 p = 1 . 0 , r = 0 . 951 p = 0 . 335 , r = 0 . 823 p = 0 . 327 , r = 0 . 302 p = 0 . 681 , r = 0 . 651 F 1 = 0 . 662 F 1 = 0 . 975 F 1 = 0 . 477 F 1 = 0 . 314 F 1 = 0 . 666 5 P49 P14 P32 P32 P42 p = 0 . 825 , r = 0 . 052 p = 0 . 891 , r = 1 . 0 p = 0 . 278 , r = 0 . 847 p = 0 . 188 , r = 0 . 301 p = 0 . 518 , r = 0 . 844 F 1 = 0 . 098 F 1 = 0 . 942 F 1 = 0 . 418 F 1 = 0 . 231 F 1 = 0 . 642 Performers are ranked by F1 score . Notation : p : precision , r : recall , F 1 : F1 score . TF - IDF vectors to represent behaviors . Figure 11 ( b ) shows the diversity of the behaviors by using 2 - grams . We can see that the differences between high and low F1 ( and efficacy ) groups remain statistically significant ( p < 0 . 01 after Bonferroni correction ) . Moreover , those in the high Python confidence group and in the low difficulty group have shown higher similarity of the behaviors than their counterparts ( Figure 11 ( b ) , Mann - Whitney U test , p < 0 . 05 after Bonferroni correction ) . This shows that more behavioral semantics are captured by 2 - grams . The participants with higher Python confidence levels and those with lower perceived difficulties have exhibited some focused use of external resources . By contrast , those who are less confident in Python or those who perceive higher difficulties in the experiment have explored to progress in tasks through external search events in a more diverse manner . We do not find significant differences of the external search pat - terns between high and low groups with respect to the reported knowledge levels in data quality . 4 . 5 Collective Wisdom of Top Performers ( RQ # 5 ) In this section , we study how we can build effective and efficient data curation processes based on the solutions provided by our experiment participants . We explore possible ways to leverage the collective wisdom of top - performing participants , which may lead to a better quality than the best individual performer . Considering that top performers may vary from task to task , we separate the Python code given by each participant per task and examine the five top performers for each task . Table 6 shows the top five performers for each task with their performance ( i . e . , precision , recall , and F1 ) scores . We can observe that the set of top performers is not completely identical for different tasks . Thus , there is an opportunity to extract ( parts of ) the best code snippets that work well for a particular task and assemble them to generate a collection of best code ( i . e . , the ‘Golden Notebook’ ) that works well for all tasks . By manually running the Golden Notebook on the same dataset as we used in our experiment , we evaluate the results from three perspectives : ( i ) effectiveness , where we investigate whether the results given by our Golden Notebook are better than those given by the best performer ; ( ii ) robustness , where we simulate several scenarios to test if the performance of the Golden Notebook drops when the types of errors become more complex ; and ( iii ) refinement , where we compute the improvement of the performance ( e . g . , F1 ) of the assembled code generated from the algorithm in a step - by - step manner . In this work , we focus on the functionality of the created Golden Notebook ; that is , we aim at generating the Golden Notebook that can maximise the coverage of the potential data quality issues and the completion ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 23 Table 7 . Example of the Assemblage of Top - N Best Code Snippets and the Improved Performance ( F1 score ) for ‘imprecise’ Errors Original Assembled ID Code F1 F1 X = pd . to _ datetime ( db [ db [ " join(cid:3)date " ] . notnull ( ) ] [ " join(cid:3)date " ] , \ 0 . 674 P2 errors = ' coerce ' ) 0 . 463 str ( list ( X . loc [ X . isnull ( ) ] . index . values ) ) P58 A = db [ db [ " contact " ] . str . contains ( " ^ + \ d \ d \ d { 9 } " , na = False ) ] 0 . 317 B = db [ db [ " contact " ] . str . contains ( " ^ \ d \ d \ d { 9 } " , na = False ) ] C = db [ db [ " contact " ] . str . contains ( " ^ 0 \ d { 9 } " , na = False ) ] db [ ∼ db . index . isin ( pd . concat ( [ A , B , C ] ) . index ) ] of the data curation tasks . If two pieces of code perform the same functionality , however , other metrics ( e . g . , the commonly used metric – source lines of code ( SLOC ) [ 52 ] ) will be used to determine which code is to be selected to create the Golden Notebook . In the following , we present the construction of the Golden Notebook as well as the results of the evaluation . 4 . 5 . 1 Effectiveness . For ‘duplicate’ and ‘missing’ data quality issues , the participants have al - ready reached the upper bound of F1 score ( i . e . , all errors of these two types have been labeled correctly ) . Thus , to generate the Golden Notebook , the code given by any of these top - 1 perform - ers for the two tasks is sufficient . By contrast , the highest F1 scores for ‘inconsistent’ , ‘imprecise’ and ‘non - unique’ are 0 . 798 , 0 . 475 , and 0 . 896 , respectively ( see Table 6 ) . Therefore , we focus on the improvement of the performance for these three types of issues . In our experiment , we inject imprecise errors into two columns : contact and join date . Based on observations , top performers , however , have worked on identifying this particular type of error within one column only . Thus , the best F1 is just 0 . 475 for this task ( see Table 6 ) although they are able to achieve 1 . 0 precision ( observed with the second best F1 ) . Table 7 shows as an example of the code given by participant P2 and P58 ( ranked second and third best , respectively 23 ) in solving the data quality issue of ‘imprecision’ . It is evident that P2 was investigating join date while P58 was dealing with problems in contact . Since there is no conflict between these two pieces of code , we can assemble them in the Golden Notebook to achieve a better performance on both of the two columns . Algorithm 1 presents the process of generating the Golden Notebook to tackle all types of errors by collective intelligence of top performers . Note that the objective of this process is to produce ( as output ) a set of code snippets assembled by the ( partial ) code provided by top performers . Thus , for a given type of error e ( Algorithm 1 Line 2 ) , we start with the code ( denoted by x ) given by who achieves the highest F1 for that task ( Algorithm 1 Line 3 ) . By investigating the given Notebook ( i . e . , code x ) , we look at in particular which column ( denoted by c ) code x works on for error e ( Algorithm 1 Line 4 ) and extract that part of code ( denoted by x ( e , c ) ) . Because we focus on the functionality of the provided code , if the code given by different performers has the same performance ( i . e . , F1 score ) , we use the one with less SLOC on the extracted snippets . Upon identifying any column ( denoted by c 1 ) that are examined by code x for error e but has not yet been inspected by the Golden Notebook G , we add this piece of code ( i . e . , x ( e , c 1 ) ) to the Golden Notebook ( Algorithm 1 Line 5 to 7 ) . If any column ( denoted by c 2 ) examined by code x has already been inspected by the Golden Notebook for error e ( i . e . , G ( e , c 2 ) ⊆ G ) , we compare the performance of x ( e , c 2 ) against G ( e , c 2 ) ( i . e . , the column - specific F 1 for e in c 2 , or F 1 ( e , c 2 , • ) ) , and replace the code block G ( e , c 2 ) in the Golden Notebook by x ( e , c 2 ) when x ( e , c 2 ) has a better performance ( Algorithm 1 Line 8 to 10 ) . We then repeat this process until all the code contributed by top performers has 23 We do not report the Notebook provided by the first best performer for this task as it was not saved properly . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 24 L . Han et al . Table 8 . Code Snippets Given by P2 and P14 to Examine the Errors of ‘imprecision’ in Column join date P2 P14 X = pd . to _ datetime ( \ regexp = re . compile ( r ' [ 0 - 9 ] { 4 } \ / [ 0 - 9 ] { 2 } \ / $ ' ) db [ db [ " join(cid:3)date " ] . notnull ( ) ] [ " join(cid:3)date " ] , \ for index , cid in enumerate ( db [ ' join(cid:3)date ' ] ) : errors = ' coerce ' ) cid = str ( cid ) str ( list ( X . loc [ X . isnull ( ) ] . index . values ) ) if regexp . search ( cid ) : imprecise _ list . append ( index ) ALGORITHM 1 : Generation of Effective Golden Notebook . Initialization : rank all performers per type of errors by F 1 scores Parameter : E : all types of errors Input : code contributed by top - N best performers for each type of error Output : Golden Notebook G ( i . e . , the code snippets tackling all types of errors ) 1 : Let G = ∅ 2 : while ∀ e ∈ E do 3 : for x ← top - 1 to top - N do 4 : C ( x e ) = { c | column c is examined by x ( e , c ) ∧ x ( e , c ) ⊆ x } 5 : while ∃ c 1 ∈ C ( x e ) such that c 1 has not been examined by G for e do 6 : G ← x ( e , c 1 ) ( x ( e , c 1 ) ⊆ x ) 7 : end while 8 : while ∃ c 2 ∈ C ( x e ) such that G ( e , c 2 ) ⊆ G ∧ F 1 ( e , c 2 , x ) > F 1 ( e , c 2 , G ) do 9 : G ( e , c 2 ) = x ( e , c 2 ) ( x ( e , c 2 ) ⊆ x ) # Replace G ( e , c 2 ) by x ( e , c 2 ) 10 : end while 11 : end for 12 : end while been processed . As we show in Table 7 , the F1 score ( performance ) for the simulation of the as - sembled code that works on ‘imprecise’ errors has increased from 0 . 463 ( P2 ) or 0 . 317 ( P58 ) to 0 . 674 ( simulation ) . Therefore , the generated Golden Notebook achieves a better performance than the individual best performer on this task ( i . e . , 0 . 475 contributed by P42 , see Table 6 ) . 4 . 5 . 2 Robustness . Investigating the code provided by P2 and P14 , we find that both of their code works well for identifying ‘imprecise’ errors in the column join date , achieving 1 . 0 F1 score for this particular column . 24 Table 8 shows the code snippets given by the two participants to examine join date in this task . We can see that P2 used a built - in function pandas . to _ datetime ( ) while P14 used a regular expression with a specific pattern to perform the task . In our experiment , the task of identifying ‘imprecise’ errors for join date requires participants to differentiate date in the format of ‘ YYYY / MM / ’ from ‘ YYYY / MM / DD ’ . In reality , however , we may have to identify more complex errors of ‘imprecision’ . To understand the robustness of the provided code , we run simulations of their code for this particular task on two artificial datasets where we randomly change 50 % instances from the format of ‘ YYYY / MM / ’ to ( i ) ‘ YYYYMM ’ and ( ii ) ‘ YYYY ’ ( removing month representation ) , respectively . Thus , we check whether their code can identify such ‘imprecise’ errors in both of these two datasets . In the simulation with ‘ YYYYMM ’ , regular expression written by P14 can no longer match this manipulated format , and thus fails to recognize these errors . By contrast , P2 uses to _ datetime ( ) function that is provided natively by pandas . This function could not recognize ‘ YYYY / MM / ’ nor ‘ YYYYMM ’ as a correct date string , and thus both formats are labeled as data quality issues . Such a 24 Note that ‘imprecise’ errors also exist in other columns . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 25 ALGORITHM 2 : Generation of Refined Code for a Specific Data Quality Issue . Parameter : e : a specific type of error Input : Code : code set contributed by all performers for error e ; DB : working dataset Output : ( refined ) code snippets tackling a specific data quality issue ( i . e . , error e ) 1 : Let RefinedCode = ∅ ; subset = ∅ 2 : R = ranked Code by (cid:11) recall , F1 (cid:12) for the task of identifying e 3 : x = extractCode (cid:2) e , R ( top - 1 ) (cid:3) 4 : RefinedCode ← x 5 : subset ← x ( e , DB ) 6 : RefinedCode ← extractCode (cid:2) e , argmax y ∈ Code (cid:2) precision ( y ( e , subset ) ) (cid:3)(cid:3) conservative method does not affect the recall 25 in this simulation . This shows that writing code in conservative logic has more tolerance or flexibility in matching different errors . In the second simulation ( with ‘ YYYY ’ ) , both solutions provided by P2 and P14 fail to identify the manipulated errors , resulting in 0 . 5 recall . In this case , pandas . to _ datetime ( ) function ( given by P2 ) recog - nizes 4 - digit years as a valid date representation . Therefore , the code cannot identify such errors . On the other hand , precision does not drop in either of the two simulations . This indicates that additional representational format does not affect the instances that have already been identified . 4 . 5 . 3 Refinement . Among all top - N performers , we observe that some participants have achieved high precision but low recall ( e . g . , P2 , P6 for ‘imprecision’ , see Table 6 ) , while some others get high recall but low precision ( e . g . , P3 , P6 for ‘non - uniqueness’ , see Table 6 ) . Thus , we consider that the performance ( in terms of F1 ) can be refined by assembling the code from those with high recall and high precision step by step . Algorithm 2 shows the process of generating refined code for a specific type of data quality issue ( denoted by e ) . At the first step , we rank the code provided by all participants based on recall values for the task of identifying error e ( Algorithm 2 Line 2 ) . We extract the code snippets dealing with e from the one with the highest recall ( as well as the highest F1 if multiple cases share the highest recall value ) ( Algorithm 2 Line 3 ) . Therefore , the extract code composes the first part of the refined code ( Algorithm 2 Line 4 ) . Using the extracted code , we select a subset of the candidates to be labeled ( Algorithm 2 Line 5 ) . This method allows us to keep the maximum coverage of the issues or even to remove the data points where the errors do not exist ( when recall equals 1 . 0 ) . Then we extract the code snippet ( from all available code ) that maximizes the precision on this sub - dataset , and use this piece of code as the second part of the refined code ( Algorithm 2 Line 6 ) . By such refined code we are able to select the instances just from where the errors are most likely in existence . Table 9 presents an example of code refinement on the task of identifying ‘non - unique’ errors . Among top - N performers ( by recall ) for this task , we use the code given by the performer with the highest recall ( e . g . , 1 . 0 recall by P6 , see Table 6 ) 26 to generate a subset of the working dataset . Then we use the code given by P46 ( whose code achieves the highest precision on this subset ) to select from this subset the data points with the errors . This process allows us to remove the instances that satisfy the condition by P46 but not covered by the results from P6 as we know that P6 has a full coverage ( i . e . , recall = 1 . 0 ) of the errors . Therefore , we can remove some data points from P46 which do not have errors . This approach results in the refinement of the performance . 25 P2 got 1 . 0 for recall while P14 got 0 . 5 in this simulation . 26 The four participants with 1 . 0 recall in this task ( i . e . , P3 , P6 , P50 and P58 ) have provided the code performing exactly the same functionality . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 26 L . Han et al . Table 9 . Refinement of Performance by Code Snippets Produced by P6 and P46 on the Task of Identifying ‘non - unique’ Errors ID Performance Code p = 0 . 465 P6 r = 1 . 0 db . loc [ db . duplicated ( [ ' customer _ id ' ] , keep = False ) ] . index . tolist ( ) F 1 = 0 . 635 P46 dbCount = db [ ' customer _ id ' ] . value _ counts ( ) p = 0 . 951 valueList = dbCount . loc [ ( dbCount . values > 1 ) ] . index . tolist ( ) r = 0 . 847 dbCount2 = db . loc [ db [ ' customer _ id ' ] . isin ( valueList ) ] F 1 = 0 . 896 dbCount3 = dbCount2 [ ' name ' ] . value _ counts ( ) valueList = dbCount3 . loc [ ( dbCount3 . values = = 1 ) ] . index . tolist ( ) db . loc [ db [ ' name ' ] . isin ( valueList ) ] . index . tolist ( ) newList = [ ] Refinement p = 1 . 0 for x in P6 . results : # selecting from P6 results ( subset of the entire DB ) ( pseudo code ) r = 0 . 847 if ( x in P46 . results ) : # satisfying the conditions set by P46 F 1 = 0 . 917 newList . append ( x ) print newList Notation : p : precision , r : recall , F 1 : F1 score . Table 9 presents after the refinement , the F1 score becomes 0 . 917 , which is better than the best score individually ( i . e . , F1 = 0 . 896 , see Table 6 ) for this task . 5 DISCUSSION From the results of our study we have observed various behaviors related to task performance quality and the factors that can affect the objective task performance . In this section , we discuss the implications of these observations on the design of data curation processes and platforms , aiming at reducing variability of the outcomes and tackling the human labor intensive nature of seeking information in data curation processes . 5 . 1 Influential Factors in Performance Quality Firstly , we find that the reported knowledge levels in data quality do not have significant impact on the actual performance quality in our experiment . As we cover this in a tutorial accompanied by examples , our data workers may quickly learn to understand different dimensions of data quality issues . In the formal experiment , they are able to explore and comprehend the working data by making comparisons through the right panel ( i . e . , the data view ) . This may serve as real world ex - amples to help them discover the quality issues existing in the given dataset . Some tasks , however , require precise understanding to achieve a better quality . For example , many participants in our experiment get confused in distinguishing ‘non - unique’ and ‘duplicate’ data ( see Section 4 . 1 . 1 ) . On the other hand , proficiency in Python can influence the actual task performance as writing code is a mandatory skill to complete the tasks and some tasks even require advanced use of Python ( e . g . , writing regular expressions ) . These observations are consistent with existing research that working memory ( i . e . , data quality knowledge in our case ) is short - term [ 4 ] and long - term mem - ory ( e . g . , Python confidence ) can reduce working memory load [ 23 ] . Thus , lack of knowledge in data quality can be somehow compensated by Python skills and the provided tutorial materials are able to equip our participants with essential knowledge to perform the tasks . Such findings bear implications on the selection criteria of recruiting data workers and on the capability of pro - viding training examples to less professional domain - experts ( e . g . , experts in Python without data quality knowledge ) to complete complex tasks ( e . g . , discovering data quality issues ) . Except for sophisticated concepts ( e . g . , ‘non - unique’ and ‘duplicate’ ) , we may for example release the con - straint on focused expertise ( e . g . , data quality ) and recruit more participants with a broader skills ( e . g . , Python ) to complete the task . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 27 5 . 2 Interaction with System Components In our experiment , we use eye - tracking devices to capture the gaze position of participants to better understand their attention to different ( i . e . , left , middle and right ) panels of our UI design . The results have strong implications on the design of data investigation and curation platforms even in the absence of eye - trackers . Despite the fact that providing a data view does not neces - sarily help more professional data workers to complete data curation tasks better , it has in fact provided consistent help throughout the experiment . We show that the time spent on the interac - tion with data view does not exhibit a clear decreasing trend ( Figure 8 ( b ) , ( d ) ) . This reveals that all the performers keep a continuous interaction with the data view to assist their task completion . In particular , those with high Python confidence ( e . g . , proficient Python developers ) keep a high level interaction with the data view to support their work ( see Table 4 ) . We also show that surfacing the underlying working data in the data view can help data workers gain self - confidence in per - forming tasks successfully ( see Figure 8 ( d ) ) and can reduce the perceived difficulty in progressing in tasks ( see Figure 8 ( b ) ) . These observations confirm the usefulness of the data view which does not only provide help to data workers to explore and comprehend the data structure but can also be used to double check their efficacy and whether they have completed the tasks . As code writ - ing expertise varies from worker to worker , the data view has shown to be of assistance for those who are not proficient in coding ( e . g . , by manually looking at the data view to compare rows and columns ) . Therefore , when designing data exploration and curation platforms , our recommenda - tion is to present data samples in platform UIs . By having such a data view component , different types of users may be able to quickly explore the working dataset , understand its structure , and rec - ognize the presence of data quality issues , as well as to get feedback about the performed tagging operations . Our results show that DataOps or libraries , accompanied by short descriptions of the function - ality , provide opportunities for data workers to learn how to perform the tasks . At early stages of the experiment ( i . e . , before spending up to 20 % of the time relative to the experiment duration ) , many participants rely on DataOps ( see Table 4 , Figure 7 , and 8 ( a ) ) to better understand the ob - jectives of the tasks either by reading the descriptive text or by doing copy / paste actions to run the code directly on the data . In our experiment , we also show that those who are more confident in Python and those who have provided better quality annotations do not rely much on DataOps as they progress in more steps ( see Table 4 and Figure 7 ( a ) , ( b ) ) . The given DataOps in the left panel , however , are useful to and indeed provide assist for those who encounter difficulties to con - tinue performing the tasks ( see Figure 8 ( a ) ) . These observations imply that internal libraries like DataOps should not be placed in a dominant position ( e . g . , the entire left panel ) in a data curation platform as they are more useful during a learning phase ( e . g . , tutorial ) and play a reference role when they progress in the tasks . But we should not completely remove DataOps from the design of data curation platforms as they may help less expert users , although in general DataOps in the left panel have shown limited use by proficient data workers . To design data investigation and cu - ration systems with effective tools for users , we may for example shorten the content of DataOps or libraries , but rather provide a search interface for users to query online forums ( e . g . , StackOver - flow ) , as our results show that high performers have spent more time on external resources ( see Figure 5 ( b ) and Table 4 ) and reused more code from there ( see Figure 9 ) , as compared to other participants . External resources ( e . g . , StackOverflow ) including broader and more general code examples have formed vast knowledge bases for data workers to refer to . This observation is consistent with exist - ing research such as [ 44 ] , which shows that code search is one of the most frequently performed activities while developing programs or software , and developers often tend to search for code ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 28 L . Han et al . they are familiar with . The participating data workers in our study have shown their own prefer - ences on where ( e . g . , Google or StackOverflow ) to search for the code that is needed to perform the tasks . For example , experienced participants ( e . g . , those with high Python confidence ) often search and refer to external resources to support their work ( see Figure 6 ( a ) ) and this potentially leads to a better perceived efficacy of the tasks ( see Figure 6 ( b ) ) . In addition , code and explanations in external resource are richer and more advanced than the available DataOps and thus can be a preferred starting point for expert users . These bear strong implications on data curation system design . Besides DataOps , integrating a code search interface would benefit experienced developers and data workers and may reduce context switches [ 44 ] . Overall , our results indicate a need for customized data interaction interfaces that can be personalized based on user expertise and data skills . 5 . 3 Task Completion Patterns and Strategies In our study , we have extracted typical interaction patterns of participants in writing code to discover data quality issues . We find that our participating data workers start to look at DataOps ( i . e . , left panel ) to produce code to perform tasks and end with examining the efficacy that they are confident with by the data view ( i . e . , right panel ) before finishing the task ( see Table 4 ) . Those who perceive the tasks as lower difficulty spend less time on DataOps ( i . e . , left panel ) and longer time on interacting with the data view ( i . e . , right panel ) ( see Figure 8 ( a ) , ( b ) and Table 4 ) . This demonstrates a typical working style that more professional data workers may not need to refer much to the provided DataOps as they have their own preference for code construction ( e . g . , from external resources ) while interacting with the working data ( from the data view ) would allow them to reduce the perceived difficulty and to perform better . Similarly , more interaction with the data view can also increase their perceived efficacy in tasks ( see Figure 8 ( d ) ) . We also find that self - perception of the efficacy in the tasks allows us to infer the actual performance quality as they can to some extent perceive their quality . Since we have unbalanced data quality issues in the experiment ( see Table 1 ) , those who discover most instances of these issues have a higher confidence in success in task completion even though they may just focus on one or two ( out of five ) specific quality issue types ( e . g . , ‘imprecision’ and ‘inconsistency’ , see Figure 3 ) . Such findings bear implications on reducing the perceived effort to complete complex tasks with labor intensive nature ( e . g . , discovering data quality issues ) . ‘Copy – paste – modify’ is a prevalent code reuse strategy that can help software developers to ef - ficiently realize similar features [ 35 , 45 ] . Exiting code snippets taken from external resources ( e . g . , StackOverflow ) , however , often require much effort and modification to fit particular task require - ments [ 54 ] . High performers have copied a lot from the code they generated in previous steps in the experiment ( see Figure 9 ) . This result is consistent with existing work showing how software developers often perform copy / paste actions within the same file that they are working on [ 1 ] . Several participants in the experiment reused code snippets that they previously constructed dur - ing the tutorial . This confirms that practice not only lets participants become familiar with the system interface , but also provides an opportunity for them to develop some working memory [ 4 ] and skills ( e . g . , how to write effective Python code ) that they can then apply to solve the tasks . 5 . 4 Leveraging Observed Behaviors Searching for external resources is a frequently performed activity while writing code . In our ex - periment , a large number of external searches have been carried out either to look for relevant code to be reused or to search for additional information that addresses open issues ( e . g . , to debug ) . Al - though the DataOps we have provided are sufficient to complete all tasks , some participants reuse our DataOps followed by external search to make refinements of the available code ( see Table 5 ) . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 29 Others use external search to acquire additional information that can consolidate their comprehen - sion of the problems they are addressing . These shed some light on the design of domain - specific ( e . g . , data curation ) systems in which writing code is required . By integrating a purpose - designed search interface indexing code repositories ( e . g . , Github ) and error messages ( e . g . , StackOverflow ) , for example , we would be able to leverage the complementary advantages of both information sources ( i . e . , executable code versus explanations ) to help data workers select and reuse code or avoid negative code usages in practice [ 28 ] . This would benefit data workers in efficient code gen - eration to perform data curation tasks ( e . g . , reducing the time spent on debugging ) . Our findings also bear implications on leveraging observed behaviors to assist the examination of performance quality and estimation of user proficiency . By measuring pairwise similarity of the participants’ behaviors prior to external search events , we show that high performers exhibit common behaviors that trigger external searches and have performed search actions in a more similar way than low performers ( see Figure 11 ) . This is consistent with existing research showing that specialists usually have a lower level of activity diversity [ 53 ] . A set of heterogeneous actions implies that the participant is less proficient ( e . g . , those with less Python confidence or lower F1 scores , see Figure 11 ( b ) ) . In addition , we also show that those who provide high quality annotations stick to working within the platform in the last step of the experiment ( i . e . , last 20 % completion in terms of the time spent relative to the entire experiment duration , see Figure 6 ( c ) ) , which distin - guishes them from those in low quality ( F1 ) group . Those with high Python confidence can also be recognized by behavioral observations ( e . g . , spending longer time in searching external resources , see Figure 6 ( a ) ) at early stages of task completion process . Such findings suggest that behavioral traces are able to be leveraged to estimate user proficiency or even to predict the actual quality of the performance in the absence of evaluation instruments ( e . g . , questionnaire ) or groundtruth labels . 5 . 5 Process Generation Finally , we present the generation of the so - called Golden Notebook , the collective intelligence of top - performing data workers . We show that the top performers may vary from task to task , suggesting the opportunity of extracting ( parts of ) the best code that works well for a particular task and assemble them to achieve a better performance than the best individual performer on the entire dataset ( see Table 7 ) . The typical process to generate the Golden Notebook consists of four main steps : ( i ) ranking the notebook created by all individual performers based on the per - task per - formance ( e . g . , F1 ) ; ( ii ) extracting ( part of ) the code that deals with a particular type of data quality issue in a given column from top - performing notebooks ; 27 ( iii ) checking the functional range on columns of the extracted code ; and ( iv ) for the columns that have not been investigated by the Golden Notebook , simply adding the extracted code into the Golden Notebook ; otherwise , if the newly extracted code has a better performance on the scoping column than the Golden Notebook , replacing the corresponding code block in the Golden Notebook by the newly extracted code until all ( top - performing ) individual notebooks have been processed . Such findings are valuable for constructing a unified and repeatable data curation process to lower the variability of the curation outcomes . When we select the best code snippets , the robustness of the code may subject to the diversity of the errors that exist in the dataset . Thus , we may want , for example , to use the code in 27 In practice , to detect the boundary of the code functionality ( e . g . , effective for a particular column ) , the simplest ( but perhaps not optimal ) way is to run the code block - by - block and to check its impact on the targeting dataset . As we focus on the functionality of the code blocks ( e . g . , an executable box in Jupyter Notebook ) , we believe that other code analysis approaches such as abstract syntax tree ( AST ) matching [ 36 ] are out of scope for this paper , but may also be applied in other settings where there is a need for a lower level of the code granularity . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 30 L . Han et al . conservative logic ( see Section 4 . 5 . 2 ) to increase its flexibility in matching different errors . We also show that it is possible to refine the performance by first applying the one that can achieve high recall to select a subset of the dataset to reduce the size of the working data , and then use the code with high precision to select the instances just from where the quality issues are likely to exist . This bears implications on the design of workflows to conduct domain - specific activities ( e . g . , data curation ) in a step - by - step manner . For example , in our setting , we could use the code provided by those who get high recall and ask them to perform the task aiming at excluding the instances where the errors are least likely in existence and selecting a subset of the data , and thus this subset of working data can be post - processed by high - precision data workers . Such a two - stage process could first provide better coverage in data quality issue identification and then require additional work to only focus on precision , which bears implications on reducing the intensive labor of human effort as well as on building multi - step data curation processes ( e . g . , through crowdsourcing platforms ) . Note that , in this case , we may need to use the code that can guarantee a full coverage of the errors ( e . g . , 1 . 0 recall ) , which can assure that the F1 score is ‘no less than’ the current best one and probably better . 6 CONCLUSIONS In this paper we have used a multi - modal dataset to understand how data workers engage in man - ual data curation activities . Our study is scoped under five main research questions , aiming at investigating what information they interact with and what processes they build while perform - ing data curation tasks . Through a lab - study involving a total of 39 participants , we study their behaviors and how these behavioral observations relate to the quality of the performance as well as their perceptions of the task ( e . g . , perceived difficulties ) . Our key observations include : • ( RQ # 1 ) The reported knowledge levels in data quality have a limited impact on the perfor - mance of data curation tasks , while proficiency in Python can significantly affect the quality of the provided annotations as well as the way our participating data workers interact with available information . The perceived difficulty and efficacy have also influenced task com - pletion process and performance , which is subject to the complexity of the task ( e . g . , using regular expression ) . • ( RQ # 2 ) Presenting a data view is an effective way to help data workers explore the dataset that they work with . Relevant code snippets provided internally in the system can help them learn how to perform well early in the task . High performers do not extensively rely on such code snippets as they are effective in finding relevant code to reuse from external sources . Compared to provided DataOps , external resources contain more detailed examples and ex - planations which can be leveraged to help data workers produce better quality results . • ( RQ # 3 ) ‘Copy – paste – modify’ is a common strategy that data workers adopt to perform data curation tasks . For high performers , the two most common patterns of code production are ( i ) refining and reusing code generated in previous steps by themselves , and ( ii ) modifying code taken from external sources . • ( RQ # 4 ) External search plays an important role in code construction to perform tasks . The three most common reasons triggering external search are : ( i ) seeking assistance in writing code , ( ii ) looking for relevant code to reuse and exploring information for code debugging , and ( iii ) enhancing the understanding of the problems that they are solving and the per - formed internal operations ( e . g . , labeling tasks ) . Some participating data workers select a DataOp internally followed by searching externally to adapt the code to progress in tasks . Overall , proficient data workers exhibit similar patterns in external searches . • ( RQ # 5 ) Building upon the quality of the performance , we present a systematic approach to select the best atomic data curation solution ( i . e . , part of the provided code ) from individual ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 31 data workers and combine them together to create a collective curation process . We evaluate the quality of the generated collection of the best code from three perspectives , namely effectiveness , robustness , and refinement , and we show that this collective process can lead to a better curation result than the best individual one on the entire dataset . The insights provided by our study on the behavior of data workers have several implications on the design of data curation platforms , the use of internal and external resources for discovering data quality issues , and the potential for building domain - specific IR systems . The use of collective intelligence ( i . e . , our Golden Notebook ) also bears implications on building effective , robust , and repeatable data curation processes that can lead to improvement in getting data ready for analyt - ics in data science projects . In our study , we limit the types of data quality issues to five specific types . In practice , data quality can also be affected by other issues like , for example , semantic inva - lidity ( e . g . , an ‘age’ attribute having a value of − 1 ) . In that case , the Golden Notebook generation algorithms ( e . g . , Algorithm 1 ) should not be affected as we examine the effectiveness of the code functionality against the ( considered ) ground truth . The optimisation of the Golden Notebook , however , requires further study and some parts of the code may have to be refined accordingly ( e . g . , regular expressions match the semantics errors ) . We also acknowledge that the participants in our study bearing programming skills in Python may not always be representative of the data workers in a real - world setting ( e . g . , Java programmers and those without programming skills ) . Thus , we recommend a follow - up study to remove the Python programming constraint . Further , data curation may include a range of activities including but not limited to data quality discovery . As an ongoing work , in the future we are going to extend our study to explore if and how complex data curation tasks can be completed by less professional crowd workers . APPENDIX A DATAOPS PROVIDED IN EXPERIMENTS Figure 12 lists all the provided DataOps in our experiments . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 32 L . Han et al . F i g . 12 . F u ll l i s t o f t h e p r o v i d e d D a t a O p s i n o u r e x p e r i m e n t s . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 33 REFERENCES [ 1 ] Tarek M . Ahmed , Weiyi Shang , and Ahmed E . Hassan . 2015 . An empirical study of the copy and paste behavior during development . In Proceedings of the 12th Working Conference on MSR . IEEE Press , 99 – 110 . [ 2 ] Anne Aula , Rehan M . Khan , Zhiwei Guan , Paul Fontes , and Peter Hong . 2010 . A comparison of visual and textual page previews in judging the helpfulness of web pages . In Proceedings of WWW . ACM , 51 – 60 . [ 3 ] Nurzety A . Azuan , Suzanne M . Embury , and Norman W . Paton . 2017 . Observing the data scientist : Using manual corrections as implicit feedback . In Proceedings of the 2nd Workshop on Human - In - the - Loop Data Analytics . 1 – 6 . [ 4 ] Alan Baddeley . 1992 . Working memory . Science 255 , 5044 ( 1992 ) , 556 – 559 . [ 5 ] Nilavra Bhattacharya and Jacek Gwizdka . 2019 . Measuring learning during search : Differences in interactions , eye - gaze , and semantic similarity to expert knowledge . In Proceedings of the 2019 Conference on Human Information Inter - action and Retrieval ( CHIIR ) . 63 – 71 . [ 6 ] Georg Buscher , Edward Cutrell , and Meredith Ringel Morris . 2009 . What do you see when you’re surfing ? Using eye tracking to predict salient regions of web pages . In Proceedings of CHI . ACM , 21 – 30 . [ 7 ] Alex Cao , Keshav K . Chintamani , Abhilash K . Pandya , and R . Darin Ellis . 2009 . NASA TLX : Software for assessing subjective mental workload . Behavior Research Methods 41 , 1 ( 2009 ) , 113 – 117 . [ 8 ] Souti Chattopadhyay , Ishita Prasad , Austin Z . Henley , Anita Sarma , and Titus Barik . 2020 . What’s wrong with compu - tational notebooks ? Pain points , needs , and design opportunities . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 9 ] Yeounoh Chung , Sanjay Krishnan , and Tim Kraska . 2017 . A data quality metric ( DQM ) : How to estimate the number of undetected errors in data sets . Proceedings of the VLDB Endowment 10 , 10 ( 2017 ) , 1094 – 1105 . [ 10 ] Tamraparni Dasu and Theodore Johnson . 2003 . Exploratory Data Mining and Data Cleaning . Vol . 479 . John Wiley & Sons . [ 11 ] Ian Drosos , Titus Barik , Philip J . Guo , Robert DeLine , and Sumit Gulwani . 2020 . Wrex : A unified programming - by - example interaction for synthesizing readable code for data scientists . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 12 ] André Freitas and Edward Curry . 2016 . Big Data Curation . Springer International Publishing , Cham , 87 – 118 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 21569 - 3 _ 6 [ 13 ] Max Grusky , Jeiran Jahani , Josh Schwartz , Dan Valente , Yoav Artzi , and Mor Naaman . 2017 . Modeling sub - document attention using viewport time . In Proceedings of CHI . ACM , 6475 – 6480 . [ 14 ] Adam Grzywaczewski and Rahat Iqbal . 2012 . Task - specific information retrieval systems for software engineers . J . Comput . System Sci . 78 , 4 ( 2012 ) , 1204 – 1218 . [ 15 ] Philip J . Guo , Sean Kandel , Joseph M . Hellerstein , and Jeffrey Heer . 2011 . Proactive wrangling : Mixed - initiative end - user programming of data transformation scripts . In Proceedings of UIST . 65 – 74 . [ 16 ] Lei Han , Tianwa Chen , Gianluca Demartini , Marta Indulska , and Shazia Sadiq . 2020 . On understanding data worker interaction behaviors . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ) . 269 – 278 . [ 17 ] Sandra G . Hart . 2006 . NASA - task load index ( NASA - TLX ) ; 20 years later . In Proceedings of the Human Factors and Ergonomics Society Annual Meeting , Vol . 50 . Sage publications Sage CA : Los Angeles , CA , 904 – 908 . [ 18 ] Andrew Head , Jason Jiang , James Smith , Marti A . Hearst , and Björn Hartmann . 2020 . Composing flexibly - organized step - by - step tutorials from linked source code , snippets , and outputs . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 19 ] Jeff Huang , Ryen W . White , and Susan Dumais . 2011 . No clicks , no problem : Using cursor movements to understand and improve search . In Proceedings of CHI . ACM , 1225 – 1234 . [ 20 ] Sean Kandel , Andreas Paepcke , Joseph Hellerstein , and Jeffrey Heer . 2011 . Wrangler : Interactive visual specifica - tion of data transformation scripts . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 3363 – 3372 . [ 21 ] Jaewon Kim , Paul Thomas , Ramesh Sankaranarayana , Tom Gedeon , and Hwan - Jin Yoon . 2015 . Eye - tracking analysis of user behavior and performance in web search on large and small screens . Journal of the Association for Information Science and Technology 66 , 3 ( 2015 ) , 526 – 544 . [ 22 ] Miryung Kim , Lawrence Bergman , Tessa Lau , and David Notkin . 2004 . An ethnographic study of copy and paste programmingpracticesinOOPL . In Proceedingsofthe2004InternationalSymposiumonEmpiricalSoftwareEngineering , 2004 . ISESE’04 . IEEE , 83 – 92 . [ 23 ] Paul A . Kirschner . 2002 . Cognitive Load Theory : Implications of cognitive load theory on the design of learning . [ 24 ] Andrew J . Ko , Brad A . Myers , Michael J . Coblenz , and Htet Htet Aung . 2006 . An exploratory study of how develop - ers seek , relate , and collect relevant information during software maintenance tasks . IEEE Transactions on Software Engineering 12 ( 2006 ) , 971 – 987 . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . 72 : 34 L . Han et al . [ 25 ] Laura Koesten , Kathleen Gregory , Paul Groth , and Elena Simperl . 2021 . Talking datasets – understanding data sense - making behaviours . International Journal of Human - Computer Studies 146 ( 2021 ) , 102562 . [ 26 ] Laura Koesten , Emilia Kacprzak , Jeni Tennison , and Elena Simperl . 2019 . Collaborative practices with structured data : Do tools support what users need ? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 27 ] Dmitry Lagun and Mounia Lalmas . 2016 . Understanding user attention and engagement in online news reading . In Proceedings of WSDM . ACM , 113 – 122 . [ 28 ] Jing Li , Aixin Sun , Zhenchang Xing , and Lei Han . 2018 . API caveat explorer – surfacing negative usages from practice : An API - oriented interactive exploratory search system for programmers . In Proceedings of SIGIR . 1293 – 1296 . [ 29 ] Yun Lin , Xin Peng , Zhenchang Xing , Diwen Zheng , and Wenyun Zhao . 2015 . Clone - based and interactive recommen - dation for modifying pasted code . In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering ( ESEC / FSE’15 ) . ACM , 520 – 531 . [ 30 ] Jiqun Liu , Matthew Mitsui , Nicholas J . Belkin , and Chirag Shah . 2019 . Task , information seeking intentions , and user behavior : Toward a multi - level understanding of web search . In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval ( CHIIR ) . 123 – 132 . [ 31 ] Henry B . Mann and Donald R . Whitney . 1947 . On a test of whether one of two random variables is stochastically larger than the other . The Annals of Mathematical Statistics ( 1947 ) , 50 – 60 . [ 32 ] R . Mehrotra , A . H . Awadallah , M . Shokouhi , E . Yilmaz , I . Zitouni , A . El Kholy , and M . Khabsa . 2017 . Deep sequential models for task satisfaction prediction . In Proceedings of CIKM . ACM , 737 – 746 . [ 33 ] Roberto Minelli , Andrea Mocci , and Michele Lanza . 2015 . I know what you did last summer : An investigation of how developers spend their time . In Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension . IEEE Press , 25 – 35 . [ 34 ] Michael Muller , Ingrid Lange , Dakuo Wang , David Piorkowski , Jason Tsay , Q . Vera Liao , Casey Dugan , and Thomas Erickson . 2019 . How data science workers work with data : Discovery , capture , curation , design , creation . In Proceed - ings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , 126 . [ 35 ] Krishna Narasimhan and Christoph Reichenbach . 2015 . Copy and paste redeemed ( T ) . 2015 30th IEEE / ACM Interna - tional Conference on Automated Software Engineering ( ASE ) ( 2015 ) , 630 – 640 . [ 36 ] Iulian Neamtiu , Jeffrey S . Foster , and Michael Hicks . 2005 . Understanding source code evolution using abstract syntax tree matching . In Proceedings of the 2005 International Workshop on Mining Software Repositories . 1 – 5 . [ 37 ] David J . Piorkowski , Scott D . Fleming , Irwin Kwan , Margaret M . Burnett , Christopher Scaffidi , Rachel K . E . Bellamy , and Joshua Jordahl . 2013 . The whats and hows of programmers’ foraging diets . In Proceedings of CHI . ACM , 3063 – 3072 . [ 38 ] Venkatesh Potluri , Priyan Vaithilingam , Suresh Iyengar , Y . Vidya , Manohar Swaminathan , and Gopal Srinivasa . 2018 . CodeTalk : Improving programming environment accessibility for visually impaired developers . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 618 . [ 39 ] Tilmann Rabl and Meikel Poess . 2011 . Parallel data generation for performance analysis of large , complex RDBMS . In Proceedings of the Fourth International Workshop on Testing Database Systems . ACM , 5 . [ 40 ] Erhard Rahm and Hong Hai Do . 2000 . Data cleaning : Problems and current approaches . IEEE Data Eng . Bull . 23 , 4 ( 2000 ) , 3 – 13 . [ 41 ] Juan Ramos et al . 2003 . Using tf - idf to determine word relevance in document queries . In Proceedings of the First Instructional Conference on Machine Learning , Vol . 242 . 133 – 142 . [ 42 ] John W . Ratcliff and David E . Metzener . 1988 . Pattern - matching : the Gestalt approach . Dr Dobbs Journal 13 , 7 ( 1988 ) , 46 . [ 43 ] Shazia Sadiq , Tamraparni Dasu , Xin Luna Dong , Juliana Freire , Ihab F . Ilyas , Sebastian Link , Renee J . Miller , Felix Naumann , Xiaofang Zhou , and Divesh Srivastava . 2018 . Data quality : The role of empiricism . ACM SIGMOD Record 46 , 4 ( 2018 ) , 35 – 43 . [ 44 ] Caitlin Sadowski , Kathryn T . Stolee , and Sebastian Elbaum . 2015 . How developers search for code : A case study . In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering ( ESEC / FSE’15 ) . ACM , 191 – 201 . [ 45 ] Ripon K . Saha , Chanchal K . Roy , Kevin A . Schneider , and Dewayne E . Perry . 2013 . Understanding the evolution of type - 3 clones : An exploratory study . In Proceedings of the 10th Working Conference on MSR . IEEE Press , 139 – 148 . [ 46 ] Nithya Sambasivan , Shivani Kapania , Hannah Highfill , Diana Akrong , Praveen Paritosh , and Lora M . Aroyo . 2021 . “Everyone wants to do the model work , not the data work” : Data cascades in high - stakes AI . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 15 . [ 47 ] Janet Siegmund , Christian Kästner , Jörg Liebig , Sven Apel , and Stefan Hanenberg . 2014 . Measuring and modeling programming experience . Empirical Software Engineering 19 , 5 ( 2014 ) , 1299 – 1334 . [ 48 ] Charles Sutton , Timothy Hobson , James Geddes , and Rich Caruana . 2018 . Data diff : Interpretable , executable sum - maries of changes in distributions for data wrangling . In Proceedings of KDD . 2279 – 2288 . [ 49 ] Y . Tay . 2011 . Data generation for application - specific benchmarking . VLDB , Challenges and Visions 7 ( 2011 ) . ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 . A Data - Driven Analysis of Behaviors in Data Curation Processes 72 : 35 [ 50 ] Ashish Thusoo and Joydeep Sarma . 2017 . Creating a Data - Driven Enterprise with DataOps . O’Reilly Media , Incorporated . [ 51 ] Michael E . Tipping and Christopher M . Bishop . 1999 . Probabilistic principal component analysis . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) 61 , 3 ( 1999 ) , 611 – 622 . [ 52 ] Jeffrey Voas and Rick Kuhn . 2017 . What happened to software metrics ? Computer 50 , 5 ( 2017 ) , 88 . [ 53 ] Isaac Waller and Ashton Anderson . 2019 . Generalists and specialists : Using community embeddings to quantify ac - tivity diversity in online platforms . In The World Wide Web Conference . 1954 – 1964 . [ 54 ] Yuhao Wu , Shaowei Wang , Cor - Paul Bezemer , and Katsuro Inoue . 2019 . How do developers utilize source code from stack overflow ? Empirical Software Engineering 24 , 2 ( 2019 ) , 637 – 673 . [ 55 ] Xiaohui Xie , Jiaxin Mao , Maarten de Rijke , Ruizhe Zhang , Min Zhang , and Shaoping Ma . 2018 . Constructing an inter - action behavior model for web image search . In Proceedings of SIGIR . 425 – 434 . [ 56 ] Manuela Züger and Thomas Fritz . 2015 . Interruptibility of software developers and its prediction using psycho - physiological sensors . In Proceedings of CHI . ACM , 2981 – 2990 . Received 16 December 2021 ; revised 24 June 2022 ; accepted 18 September 2022 ACM Transactions on Information Systems , Vol . 41 , No . 3 , Article 72 . Publication date : February 2023 .