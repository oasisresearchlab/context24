1 A Trade - off - centered Framework of Content Moderation JIALUN AARON JIANG , University of Colorado Boulder , USA PEIPEI NIE , University of Washington , USA JED R . BRUBAKER , University of Colorado Boulder , USA CASEY FIESLER , University of Colorado Boulder , USA Content moderation research typically prioritizes representing and addressing challenges for one group of stakeholders or communities in one type of context . While taking a focused approach is reasonable or even favorable for empirical case studies , it does not address how content moderation works in multiple contexts . Through a systematic literature review of 86 content moderation papers that document empirical studies , we seek to uncover patterns and tensions within past content moderation research . We find that content moderation can be characterized as a series of trade - offs around moderation actions , styles , philosophies , and values . We discuss how facilitating cooperation and preventing abuse , two key elements in Grimmelmann’s definition of moderation , are inherently dialectical in practice . We close by showing how researchers , designers , and moderators can use our framework of trade - offs in their own work , and arguing that trade - offs should be of central importance in investigating and designing content moderation . CCS Concepts : • Human - centered computing → Collaborative and social computing theory , concepts and paradigms . Additional Key Words and Phrases : Content moderation ; online communities ; social media ; literature review ACM Reference Format : Jialun Aaron Jiang , Peipei Nie , Jed R . Brubaker , and Casey Fiesler . 2022 . A Trade - off - centered Framework of Content Moderation . ACM Trans . Comput . - Hum . Interact . TBD , TBD , Article 1 ( June 2022 ) , 34 pages . https : / / doi . org / 10 . 1145 / 3534929 1 Introduction In recent years , HCI and social computing research has shown an increasing interest in content moderation , investigating its critical role in online community building and engagement , social media user safety , and broader social issues . While the philosophical exploration of handling contentious matters in online communities dates back to as early as 1978 when Gengle [ 41 ] conceptualized “Fairwitness” who were impartial citizens serving as guides of online conversations , modern scholarship of content moderation has widely accepted Grimmelmann’s definition : a mechanism to facilitate cooperation and prevent abuse [ 40 ] . The topic of content moderation has been attracting an abundance of research ever since usually focusing on how different types of moderation can help various communities . Existing content moderation studies typically prioritize Authors’ addresses : Jialun Aaron Jiang , University of Colorado Boulder , Department of Information Science , 1045 18th St . , Boulder , CO , 80309 , USA , aaron . jiang @ colorado . edu ; Peipei Nie , University of Washington , Paul G . Allen School of Computer Science and Engineering , 185 E Stevens Way NE , Seattle , WA , 98195 , USA , niep @ cs . washington . edu ; Jed R . Brubaker , University of Colorado Boulder , Department of Information Science , 1045 18th St . , Boulder , CO , 80309 , USA , jed . brubaker @ colorado . edu ; Casey Fiesler , University of Colorado Boulder , Department of Information Science , 1045 18th St . , Boulder , CO , 80309 , USA , casey . fiesler @ colorado . edu . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2022 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . 1073 - 0516 / 2022 / 6 - ART1 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3534929 ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . a r X i v : 2206 . 03450v1 [ c s . H C ] 7 J un 2022 1 : 2 J . A . Jiang et al . representing and addressing challenges for one type of people or communities in one particular type of context . While such a focused approach is reasonable and sometimes favorable for individual studies , what remains hidden are the broader insights into how content moderation works . A benefit of taking a holistic view of moderation research is that it will uncover how moderation would work differently in different contexts , and existing research has already shown evidence that such difference exists : For example , automated moderation can work consistently and quickly in a way that is required by large - scale moderation , but it lacks nuanced understandings needed by individual cases that often fall into the gray area of policies and rules [ 49 ] . Moderators in text - based online spaces rely heavily on removing and editing content , but the same methods completely break down in communities where voice chat or virtual reality is the dominant mode of interaction [ 7 , 53 ] . These two examples , and many more like them , demonstrate the complexity and difficulty of moderation in practice when it needs to satisfy multiple needs risen from multiple contexts . An examination of the broader picture of moderation will reveal patterns and distinctions in moderation’s success and challenges , as well as the reasons that contributed to them . The insights will also help researchers , designers , moderators , and regular internet users to reflect on moderation by considering factors that they may not have considered before . Through a systematic literature review , we present a trade - off - centered framework of moderation that synthesizes findings of individual papers into four major , interrelated trade - offs at increasing levels of abstraction . Our framework also further highlights the subtle and evasive trade - offs in moderation philosophies and values , and surfaces the dialectical tension between facilitating coop - eration and preventing abuse , the two elements in Grimmelmann’s [ 40 ] definition of moderation . Finally , we show how researchers , designers , and moderators can be benefited from our framework , and argue that trade - offs should be of central importance in investigating and designing content moderation . 2 Method : Systematic Literature Review To understand patterns and trends in existing literature about content moderation , we conducted a systematic literature review , following best practices established in different fields [ 69 ] , as well as rigorous review studies in the HCI and CSCW literature [ 11 , 20 , 96 ] . This section will describe our search strategy to identify candidate papers , inclusion criteria to filter the candidate papers into a corpus for analysis , and analysis techniques . 2 . 1 Search Strategy Prior work in content moderation shows that there is not one field that completely covers all content moderation research . A published reading list of content moderation [ 37 ] shows this line of research primarily happens in social computing , human - computer interaction , computational social science , and communication , spanning a wide range of ACM and AAAI conferences ( e . g . , ACM CHI , ACM CSCW , AAAI ICWSM ) and journals ( e . g . , Social Media + Society , New Media & Society , International Journal of Communication ) . Therefore , to ensure a robust coverage across venues , we used a combination of search databases . Following the method used by Chancellor et al . who did a similarly interdisciplinary meta - review [ 11 ] , we used the ACM Digital Library to search ACM journals and conferences , the AAAI Digital Library ( implemented with Google custom search ) to search AAAI publications , and Web of Science for other journal publications . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 3 Using a keyword search within the above databases , we identified an initial set of candidate papers published between 2004 1 , the earliest time that we could find content moderation research , and the time our search concluded ( October 2020 ) . Based on keywords used in all published papers about content moderation in CSCW 2018 and 2019 , two conferences with a relatively high amount of empirical content moderation work , as well as keywords that they have used to describe content moderation , supplemented with our domain knowledge , the final list of keywords included : content moderation , platform moderation , community moderation , platform gover - nance , community governance , internet governance . In order to check the validity of these keywords , we manually went through every paper ( regardless of whether it related to content moderation ) published in one conference ( ICWSM 2019 ) and one journal ( Social Media + Society papers published in 2019 ) —which constituted a total of 158 published papers—and created a subset of papers about moderation . We selected these two venues because they contained a reasonably small total number of papers for us to go through while not sacrificing the quality of our validity check . We then performed a keyword search of that conference and journal , and ensured that the keyword search did not result in any false negatives . False positives were retained , since they could be filtered out by our inclusion criteria ( described below ) . Our search strategy finally yielded 1 , 074 papers in total ( 309 from the ACM Digital Library , 35 from the AAAI Library , and 730 from Web of Science ) . 2 . 2 Inclusion Criteria Each paper identified with our keyword search needs to meet the following criteria to be included in the corpus : ( 1 ) Archival & peer - reviewed : A paper needs to be archival and peer - reviewed for inclusion , because these papers have been scrutinized by experts to ensure their validity and rigor - ousness and thus meet the publication criteria of the chosen venues . We did not include non - archival papers such as late - breaking work or workshop papers because they often include work that is incomplete and ongoing . ( 2 ) Empirical study : The paper needs to describe at least one empirical study to be included for analysis . An “empirical study” here means a study that collects data from people . This definition means : ( a ) Since we focused on real - world moderation practices and challenges validated by real mod - erators , we did not include essays or papers that are purely theoretical analysis . However , studies that use empirical evidence to validate social science theories would meet this criterion . ( b ) Papers that describe systems would only qualify if they also describe user studies , which includes formative studies before building the system , and evaluative studies of how people use the system . ( c ) We also did not include papers that only summarize or evaluate existing studies , because the qualifying studies that these papers build on would already be included in the keyword search . ( 3 ) Moderation practices , challenges , impacts , or recommendations : For this study we only focus on these four facets of moderation . Therefore , for a paper to be included , it needs to document at least one of the following : ( a ) Existing moderation practices or approaches ; ( b ) Existing moderation challenges or problems ; 1 While we did not arbitrarily limit the starting point in our initial search , our search keywords only yielded results dating back to 2004 across several databases . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 4 J . A . Jiang et al . Fig . 1 . Number of papers in our corpus by year . ( c ) Impacts and consequences of existing moderation practices ; ( d ) Recommendations , implications , or future directions for designing and implementing content moderation . Since these details may not be included in the paper titles or keywords , we also read the abstract of each paper . After manually filtering and deduplicating using the inclusion criteria above , we retained 74 papers ( 35 from the ACM Digital Library , 7 from the AAAI Library , and 32 from Web of Science ) . A number of papers did not pass the inclusion criteria because they focused on the governance of internet infrastructure , such as DNS servers and IP addresses , a result of our search keyword “internet governance . ” Additionally , we noted a lot of papers that provided valuable insights about moderation but did not pass criteria ( 2 ) or ( 3 ) , such as legal scholarships , technical reports , thought - pieces and essays , and papers that described systems but did not have any component listed in ( 3 ) . We further added 12 papers from reading the bibliographies in these papers , resulting in a total of 86 papers in our corpus . 2 . 3 Characterizing the Corpus Within our corpus , Reddit was the most commonly studied platform and was the focus of 29 papers , far exceeding other platforms . Other platforms with multiple occurrences included : Facebook ( including Facebook Groups ) ( 8 ) , Instagram ( 5 ) , YouTube ( 5 ) , Twitter ( 4 ) , Twitch ( 4 ) , Slashdot ( 3 ) , Discord ( 2 ) , Tumblr ( 2 ) , Wikipedia ( 2 ) . Platforms that only appeared in one paper include : Flickr , Tinder , Vine , Pinterest , Yelp , Yik Yak , Mayo Clinic Connect , Weibo , CNN . com , New York Times , StreamPlus . com . , FutureLearn , iFixIt , League of Legends Tribunal , Whisper , and Everything 2 . The interview studies and survey studies in our corpus had an average of 21 . 7 and 503 . 8 participants respectively . Appendix A shows a complete list of papers in our corpus , and Fig . 1 shows the ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 5 number of papers in our corpus by year . As Fig . 1 shows , there is a significant uptick in the number of papers from 2016 , suggesting that content moderation is a nascent field of study . Given that content moderation research frequently addresses negative or harmful experiences , we paid particular attention to research ethics in our corpus . 11 out of the 86 papers in our corpus had explicit discussions of research ethics considerations , beyond statements that the study was approved by an ethics review board . This relatively rare occurrence of explicit research ethics discussions is in line with the findings from a recent systematic literature review of Reddit - based research studies [ 86 ] . Papers that did discuss research ethics focused on the ethics of studying sensitive issues such as eating disorder and handling removed data , and assigning participants in experiment conditions that might incur additional harm . 2 . 4 Analysis Techniques We conducted a thematic analysis of the papers in our corpus , following the six phases outlined by Braun and Clarke [ 8 ] . The first two authors first engaged in one round of open coding by closely reading and coding a sample of the corpus . Specifically , we each sampled one paper per year for every year with any publication , with an eye toward a breadth of research paradigms ( e . g . , qualitative and / or quantitative ) and topics ( e . g . , volunteer moderation and / or commercial moderation ) , and open coded these papers . During this round of open coding , we regularly came together to discuss emergent code groups such as “removing content” and “automated moderation bots . ” Then , we open coded the rest of the corpus with an eye toward the preliminary code groups identified in the sample . Two more rounds of iterative coding led us to identifying higher - level categories such as “moderation actions” and “rules and norms . ” The first author used these categories to produce a set of descriptive theme memos that described each category grounded in the quotes from the papers . All authors then discussed the theme memo and developed the relationships between the categories . While we observed recurring themes such as “moderation transparency” or ”educational ap - proaches , ” none of these themes applied across all contexts . Furthermore , themes almost always existed in opposite pairs—for example , there were papers arguing for both providing explanations in moderation and not doing so . This pattern made us realize that the recurring theme across contexts was the competing choices—or trade - offs —in content moderation , and the resulting trade - off - centered framework we discuss below represents one way of thinking about and analyzing content moderation practices across a variety of domains . Further , while we did not analyze the papers with Grimmelmann’s categorization of moderation [ 40 ] in mind or seek to expand upon it , we found that the more concrete trade - offs around moderation actions and styles broadly aligned with his categorization . However , the same did not apply for the more abstract trade - offs that we identified . Therefore , in describing the trade - offs , we only discuss Grimmelmann’s categorization where it applies well in order to connect the trade - offs to this key piece of literature . 2 . 5 Limitations We would like to note three major limitations in our analysis . First , our analysis does not differentiate between platform - wide commercial moderation and volunteer moderation in smaller communities , because this distinction was not salient in our corpus . We suspect that the absence of such distinction was due to an overall lack of empirical research of commercial moderation practices . However , we do note that there is research related to user perceptions of commercial moderation , which is included in our corpus . As such , the framework we present is also likely more representative of volunteer moderation . Second , given the focus on empirical research in our systematic literature review , we acknowledge that our framework is inadequate in addressing numerous other perspectives in content moderation , ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 6 J . A . Jiang et al . such as legal perspectives and critical perspectives . These alternative perspectives have provided valuable insights into underrepresented topics in our analysis , such as commercial moderation and algorithmic approaches to moderation ( e . g . , [ 38 ] ) . Similarly , while the papers included in our analysis only date back to 2004 due to the empirical research criteria , there have been discussions about content moderation that predate our corpus ( e . g . , [ 2 , 45 , 63 , 89 ] ) . While beyond the scope of this paper , we encourage others to examine and expand on our framework through diverse perspectives . Last but not least , the fact that content moderation is a nascent field of study inherently bears limitations of the work presented in this paper . We acknowledge that our corpus is biased toward one platform ( Reddit ) , and while we tried to use diverse examples , references to papers are dispro - portionately skewed toward several descriptive papers that provide summaries of a broad range of practices . Therefore , the framework presented by this paper is by no means a perfect one , but we hope this paper will nonetheless contribute to the literature by providing a new perspective for examining content moderation . Fig . 2 . Diagram of our trade - off - centered framework of content moderation . The level of abstraction increases from moderation actions to moderation values . Note that elements and arrows within a single layer do not vary in the levels of abstraction . 3 A Trade - off - centered Framework of Content Moderation Our trade - off - centered framework of content moderation consists of four interrelated layers of trade - offs in increasing levels of abstraction : Moderation actions , moderation styles , moderation philosophies , and moderation values . Fig . 2 shows a diagram that visualizes this framework . Trade - offs in the more abstract layers impact those in the more concrete layers . We envision our framework to be an analytical tool that helps people examine and make sense of content moderation practices , rather than a mental model that prescribes moderators’ thought processes . The moderation actions layer represents multiple concrete moderation techniques that mod - erators can use to manage their communities , such as issuing warnings , removing content , and ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 7 banning people . While we did not analyze the moderation actions with Grimmelmann’s definition of moderation [ 40 ] in mind , we found that these actions broadly aligned with Grimmelmann’s categorization of moderation “verbs”—excluding , organizing , norm - setting , and pricing—and de - scribed them as such . These actions have varying levels of harshness , and reveal trade - offs between stifling the community and exposing the community to harm , as well as in forgoing the opportunity to educate community members about acceptable behavior by immediately removing violating behavior . The moderation styles layer goes up one level of abstraction by addressing how moderators can carry out the actions in the moderation actions layer , which we found to be similar to some of the “adverbs” in Grimmelmann’s moderation framework . The styles layer consists of three specific trade - offs representing competing choices in the ways in which any of the moderation actions could be taken : Human vs . automated , centralized vs . distributed , and transparent vs . opaque . The moderation philosophies layer is one level more abstract than the moderation styles layer , and describes the philosophies that guide tendencies toward specific choices in moderation styles and moderation actions . The moderation philosophies layer consists of three trade - offs representing competing needs in content moderation : Nurturing vs . punishing , level of activity vs . quality of contribution , and efficiency vs . quality of moderation . The moderation values layer is the topmost layer , representing the competing values that impact decisions in the trade - offs in moderation philosophies , styles , and actions . We broadly classify the values into three categories : Moderator identities , community identities , and competing stakeholders . Our analysis , which we describe later in the following sections , revealed increasing levels of abstraction in our framework : While prior literature often describes moderation actions and styles as concrete findings ( to the extent that there are existing categorizations such as Grimmelmann’s [ 40 ] ) , moderation philosophies and values are more evasive , which are often discussed only as speculations , if discussed at all . For the same reason , while we could find individual papers detailing the trade - offs in moderation actions and styles , our discussion of the trade - offs in moderation philosophies and values required a synthesis of multiple papers . How prior literature discussed the trade - offs also influenced our organization within the layers . The trade - offs in moderation styles and philosophies existed in clear , opposing binaries in our corpus , so we used arrows to represent them . However , trade - offs in moderation actions and values involved multiple possible options , and as a result , here we directly listed the categories of options instead . While we vertically order the layers to represent different levels of abstraction , it does not apply to the trade - offs within the moderation styles and philosophies layers ; these trade - offs are equal and do not vary in the levels of abstraction . It is important to note that none of the options in any of the four layers are mutually exclusive . Real content moderation practices are almost always a mixture of different options , with different actions , styles , philosophies , and values existing at the same time . Therefore , the arrows in moderation styles and philosophies are “slider scales” where the decision could fall anywhere in the middle , instead of at one extreme or the other . Similarly , choices in moderation actions and values are also not mutually exclusive . Our notion of a trade - off is not a one - vs - all choice , but a balance to achieve among many legitimate alternatives . In the remainder of this paper , we explain each of the four layers of trade - offs in detail , and close by discussing how different people can use our framework in their own work . 4 Trade - offs in Moderation Actions The first trade - off that we identified was around the moderation actions against rule - breaking behaviors , similar to the techniques , or “verbs , ” in Grimmelmann’s [ 40 ] framework of content ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 8 J . A . Jiang et al . moderation . We found that moderators took different actions ( for example , removing content or issuing warnings ) to enforce content moderation . These actions had various levels of harshness , associated with different , sometimes competing outcomes and consequences . Grimmelmann [ 40 ] categorized techniques into four broad categories : Excluding , organizing , norm - setting , and pricing . In our corpus , we also found more granular moderation actions that correspond to these four categories . One of the common actions was exclusion , which means to deprive certain people of access to an online community , and often takes the form of banning and the less harsh version of it—timeouts ( i . e . , ban from participation for a certain period of time ) . 55 out of 86 papers in our corpus mentioned some type of exclusion . Sometimes whole communities may be excluded by platforms , such as the ban of several subreddits in 2015 due to their violation of Reddit’s anti - harassment policy [ 16 ] . In communities with voice chatting functionalities , moderators also practiced muting , which excludes people from participating in voice chats but not necessarily text chats [ 53 ] . The widespread use of exclusion was captured by Seering et al . [ 97 ] : Nearly all moderators mentioned using timeouts , bans , or equivalents , though eagerness to use them varied . Communities with more laissez - faire ideologies used these only for egregious offenses , while communities intended to be safe spaces were usually quicker to use them . [ 97 , p . 14 ] Organizing , appearing in 68 papers , was the most common type of action that focuses on content rather than people . It “shapes the flow of content from authors to readers” [ 40 ] , which , in our corpus , includes removing and annotating content . While removal often intends to solely get rid of content that violates the community rules , annotating can serve a multitude of purposes . For example , post annotations in Reddit , called “flairs , ” are used as labels that categorize posts , whereas annotations in Wikipedia such as the Neutral Point of View ( NPOV ) tag are meant to notify readers that an article may be violating certain Wikipedia guidelines . In the case where the organized content is violating , prior research indicates differences between removing and annotating in the efficacy of helping community members adhere to norms . In a study of r / ChangeMyView , Srinivasan et al . [ 102 ] showed the causal effect that post removals indeed improve norm adherence . In contrast , Pavalanathan et al . [ 80 ] found that NPOV tags in Wikipedia did not help the editors to adopt the desired writing style , but did improve the overall quality of tagged articles , likely because of the contribution of other editors who edited upon the NPOV tags . In addition to direct sanctions taken on people or content , moderators also widely used warnings ( mentioned by 27 papers ) , which are less harsh , and fall into the premise of norm - setting by denouncing bad behavior [ 40 ] . Moderators issued warnings to tell rule violators that they did something wrong , and sometimes also did so publicly to educate the community more broadly . Seering et al . [ 97 ] also noted that warnings ranged from light to strong , the latter often accompanied by temporary sanctions mentioned above . Skousen et al . [ 100 ] in their study of an online health community also documented “indirect policing” practices similar to warnings to deescalate conflicts . We did not find any direct evidence of monetary pricing , perhaps due to social media platforms’ overall pursuit of a high level of user engagement and lack of incentive to inhibit participation . However , by adopting a broader view of pricing as frictions to users , Vaccaro et al . [ 105 ] docu - mented how existing appeal process on social media platforms shifted the cost of understanding documentations and formulating cohesive arguments to the users , effectively inducing a “price” on overturning enforcement . While it might seem that moderators were able to choose freely from a suite of possible mod - eration actions—excluding , organizing , norm - setting , and pricing—against rule - violating content or people , an underlying tension across these four types of actions was whether or not to remove ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 9 certain content or people . For example , while several studies documented moderators prioritiz - ing warnings over direct punishments such as removal or banning ( e . g . , [ 53 , 100 ] ) , we also saw communities that were less hesitant to employ these harsher sanctions [ 97 ] . Furthermore , with any of these actions , moderators had an additional choice to make : Whether or not to provide explanations . These different prioritizations reveal two immediate trade - offs . The first trade - off is one that is well - documented by prior research : Too much leniency may expose the community to harm , while too much harshness may stifle the community [ 43 , 64 ] . The second is more subtle : Removing violating content or people prevents them from staying in the community , but it also forgoes the opportunity to educate the community about acceptable behavior . Behind different competing choices in these trade - offs around moderation actions are differences in moderators’ philosophies and values , which we discuss later in this paper . 5 Trade - offs in Moderation Styles In addition to moderation actions , trade - offs are also present in how the moderators carry out these actions , which we term moderation styles . These moderation styles resemble “distinctions” in Grimmelmann’s [ 40 ] moderation framework ( though the identified styles here do not cover all of them ) , serving as “adverbs” that describe the actions ( “verbs” ) mentioned in the previous section . In our analysis , we identified three major trade - offs around moderation styles in our corpus : Human vs . automated , centralized vs . distributed , and transparent vs . opaque . 5 . 1 Human vs . Automated The trade - off between human and automated moderation refers to whether a moderation action was performed by a human or some type of automated system . It is important to note that current moderation systems are rarely purely human or purely automated , nor did any study in our corpus argue for a move toward either of these extremes . Moderation systems that we saw are always a hybrid of human and automated moderation , but the degrees to which they rely on humans or automation vary . Arguments for more human moderation most commonly appeared when the moderation deci - sions were difficult and required a nuanced understanding of contexts : Moderators we interviewed were happy to have tools that deal with the most obviously unwanted content , such as links to malware or pornography , but they have a strong preference to make the hard decisions themselves . [ 97 , p . 14 ] One example of such unwanted content that was not obvious was memes , which derives their meanings from multiple layers of contexts [ 52 ] . Therefore , in response to Facebook’s image recog - nition tool , Procházka [ 85 ] questioned technology’s ability to understand memes whose meanings were fluid and context - dependent , and argued for the necessity of human moderation of them . Another thread of cases that warranted more human moderation was community - building . Seering et al . [ 97 ] found that negotiation of acceptable and unacceptable behaviors was critical for community growth , and such negotiation necessarily requires human involvement . However , negotiation of community norms can take on many shapes and forms . Jhaver et al . [ 50 ] focused on a particular one of them : Providing removal explanations , and argued that subpar explanations can have detrimental effects on the community : In cases where the removal reasons are unclear , human moderators should continue to provide such explanations . . . . We expect that inaccurate removal explanations are likely to increase resentment among the moderated users rather than improve their attitudes about the community . [ 50 , pp . 22 - 23 ] ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 10 J . A . Jiang et al . Despite the benefits of human moderation , moderation research also described the pressing need for automated moderation . As online communities quickly grew into sizes that humans could not reasonably handle ( e . g , millions of users ) , automated moderation provided a solution for moderation at scale [ 15 ] . In addition to the ability to moderate large volumes of content , speed was also an advantage of automated moderation that humans struggled to achieve . As the prerequisite for human moderation was that a human had to be online and see the potentially violating content , automated moderation triumphed in timeliness by offering 24 / 7 monitoring [ 49 ] . Beyond scalability , Jhaver et al . also noted that automated moderation offered a high level of consistency , since moderation rules were hard - coded into the automated systems [ 49 ] . However , such consistency presented a trade - off when facing the unique adaptability to contexts offered by humans , which Jhaver et al . also acknowledged . Humans’ ability to understand nuanced contexts became important in complex , high - stake situations such as when distinguishing hate speech from newsworthiness [ 9 ] , where the line between violating and non - violating was critical but blurry . Prior research extensively documented the trade - off between automated tools’ ability to handle massive scale of content and human’s ability to tell the subtle difference between whether certain content is violating rules ( i . e . to reduce false positives ) , in various cases such as pro - eating disorder communities [ 12 , 13 ] , crowdsourced blocklists [ 51 ] , copyright infringement detection [ 39 ] , and even country - wide ethnic violence [ 49 ] . Chancellor et al . [ 13 ] specifically pointed out that automated tools could magnify any errors they made , as well as the remedy required to correct these errors , precisely because of their ability to scale . To summarize , studies reveal benefits in both human and automated moderation : Humans are capable of handling complex nuances , while automated systems offer the kind of moderation required by the massive scale of today’s online community . However , these very benefits can become drawbacks in different situations , and the trade - offs between human and automated moderation remain a persistent challenge to content moderation . 5 . 2 Centralized vs . Distributed The trade - off between centralized and distributed moderation refers to whether moderation deci - sions are made by designated moderators or regular users and community members . Similar to human vs . automated moderation , the configuration of centralized vs . distributed moderation is often a hybrid one in today’s online communities , landing somewhere between purely centralized and distributed . For example , Facebook has centralized moderation teams around the world to enforce their community guidelines , as well as volunteer moderators in Facebook Groups to make their own rules and enforce their own moderation [ 36 ] . Likewise , Reddit also has platform - wide moderators as well as volunteer moderators in individual subreddits that form the moderation system on Reddit that we see today [ 30 ] . Even within the premise of a single subreddit , many subreddits also allow regular members to contribute to moderation decisions such as rule making in addition to the moderators . Furthermore , Reddit users also have the ability to upvote or downvote posts , which impacts the visibility of these posts [ 30 ] . Many papers pointed out drawbacks of distributed moderation that indicate the advantages of a centralized fashion . The arguments against distributed moderation focused on the lack of expertise from regular users , as well as their personal biases which made them incapable of making decisions representative of the community ideal : r / AskHistorians moderators described a variety of reasons why they opposed using the karma system as an indication of quality . First , the majority of those who upvote responses do not have the requisite expertise to evaluate quality ; second , voting reflects ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 11 user bias ; and third , earlier comments tend to receive more upvotes , regardless of quality . [ 35 , p . 15 ] These drawbacks of distributed moderation suggested that centralized moderation would be more consistent , standardized , and made by qualified experts . Some participants in Fan and Zhang’s [ 27 ] digital jury experiment expressed a similar lack of confidence in the quality of user input . Furthermore , Duguay et al . [ 23 ] found that distributed moderation could harm minority users disproportionately : Co - moderation works against minority user groups on two levels . First , the majority of users on such a mainstream platform as Instagram are statistically more likely to be heterosexual and may have difficulty understanding the aims and culturally specific aesthetics of queer women’s photos . Secondly , those who are compelled to flag others’ photos do so because they feel strongly about the content , usually because they are offended by its violation of their personal norms , which may be sexist or homophobic . [ 23 , p . 245 ] Here , Duguay et al . suggested that decisions from distributed moderation could favor majority norms against marginalized groups , a finding echoed by Park et al . [ 79 ] in pointing out the “undesirable popularity bias” in crowdsourced moderation of news comments . However , distributed moderation also has desirable advantages . Not only was distributed modera - tion a feasible model [ 67 ] , we also saw many cases where users had higher confidence in distributed moderation over centralized moderation ( e . g . , [ 24 , 97 ] ) , with one study [ 22 ] specifically arguing that distributed deliberation practices could foster a positive digital environment . In their study , Fan and Zhang [ 27 ] found that compared to distributed moderation , centralized moderation was less democratically legitimate in the framework of procedural justice , characterized by a lack of accountability to the public . Furthermore , since centralized moderation converged moderation to a small team of moderators , they had to “spend countless hours in order to maintain the community” [ 17 ] , which suggested distributed moderation’s potential ability to diffuse moderators’ workload . The ability to reduce workload , however , was at odds with the desire of expertise in moderation , which was the major advantage of centralized moderation and typically only the moderators possessed . Lampe and Resnick [ 66 ] , in one of earliest studies of content moderation , summarized this inevitable trade - off between improving efficiency and seeking expertise : These findings highlight tensions among timeliness , accuracy , limiting the influence of individual moderators , and minimizing the effort required of individual moderators . We believe any system of distributed moderation will eventually have to make trade - offs among these goals . [ 66 , p . 8 ] In addition to the moderation work itself , the expertise desired in centralized moderation and the public accountability desired in distributed moderation also highlight another trade - off : Is the credibility derived from the experts or that derived from the public more desirable ? Kayhan et al . [ 59 ] rightfully pointed out this trade - off in perceived credibility , and came to the conclusion : It depends . [ G ] overnance credibility is a contextual variable that varies from one situation to the next . Governance mechanisms implemented in two different organizations may not be equally credible if the governors are different . In a given context , expert - governance may be perceived as being more credible than community - governance if users trust the experts more than the community members ( or vice versa ) . [ 59 , p . 75 ] ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 12 J . A . Jiang et al . In summary , we found that the trade - off between centralized and distributed moderation was one that revolved around perceived expertise , efficiency , and credibility . Just like the case of human vs . automated moderation , our analysis indicates that the centralized vs distributed trade - off may be inevitable . 5 . 3 Transparent vs . Opaque The trade - off between transparent and opaque moderation is prominent in our dataset . While this trade - off is similar to the distinction of transparently vs . secretly in Grimmelmann’s [ 40 ] moderation framework , Grimmelmann’s distinction focuses more on whether the fact that some kind of moderation had happened is explicit and public . However , the distinction between transparency and opacity here in our dataset focuses more on whether explanations are provided with sanctions , and the visibility of the act of moderation is less of a concern . We saw an undeniable push for transparency in our analysis , with ample discussion of the benefits of providing explanations . Studies found that transparency enhanced legitimacy , perceived consistency [ 109 ] , and accountability [ 27 ] , and could prevent confusion and frustration that breeded the often incorrect folk theories for why certain content was sanctioned [ 48 , 53 , 103 , 108 ] . Providing explanations also helped community members adhere to norms and improve their future behaviors [ 50 , 104 ] , and educated users about community rules [ 48 ] . Despite a multitude of benefits of being transparent , we also saw valid reasons for not providing explanations . Many studies [ 48 , 49 , 56 ] reported that providing explanations of actions by automated moderation tools enabled malicious actors to game the rules : We found that moderators do not reveal the details of exactly how AutoMod [ erator ] 2 works to their users . . . . Our participants told us that although Reddit provides them the ability to make this wiki page public , they choose not to do so to avoid additional work and to ensure that bad actors do not game the Automod rules and post undesirable content that AutoMod cannot detect . [ 49 , p . 21 ] Chancellor et al . [ 14 ] explored such circumvention of hard - coded rules in detail through a case study of how pro - eating disorder communities used lexical variation to avoid hashtag - based moderation on Instagram , which did not even publicize how it moderated hashtags . The need to prevent rule circumvention extended beyond tool configuration to community rule making itself : Many moderators chose to phrase their rules vaguely and broadly so that they could have the necessary interpretative flexibility when it came the time to enforce these rules [ 53 , 56 ] . Explanations provided by humans had different problems . Contrary to recent findings , Petrič and Petrovčweč [ 82 ] found that providing explanations did not increase users’ sense of community . Furthermore , Seering et al . [ 97 ] found that transparency could be a source of conflict within communities , because community members often would not notice unannounced moderation decisions . Possible disagreements and conflicts resulting from transparency could escalate to harms against moderators , as Gilbert [ 35 ] suggested in her study of r / AskHistorians : While the stickied [ explanation ] comment may have reduced the total number of questions and comments than the question would have received without the stickied comment , it did not solve the problem entirely and resulted in additional emotional labor as users responded to the stickied post with insults . [ 35 , p . 22 ] Several other studies echoed the emotional labor associated with moderation ( e . g . , [ 21 , 110 ] ) , but the physical labor as well . Providing explanations is a nontrivial amount of work . Jhaver et al . [ 50 ] advocated the use of automated tools to provide explanations to handle the enormous traffic that 2 AutoModerator is a system built into Reddit that allows moderators to define rules to be automatically applied to posts in their subreddit [ 1 ] . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 13 online communities often experience today . However , as we mentioned previously , automated tools have the potential to magnify their errors , and tools mistakenly providing the wrong explanations could exacerbate the conflict and hostility toward moderators . The trade - off between transparency and opacity is difficult , with no benefits of one side clearly outweighing those of the other . In an in - depth study of Reddit’s moderation transparency , Juneja et al . [ 56 ] made this trade - off prominent by showing that moderators had divided opinions on almost every issue related to moderation transparency , including whether or not to make removals obvious , to provide explanations for sanctions , to share details of AutoModerator implementations , and to make moderation logs public , for the same reasons we discuss above . The tug of war between improving behaviors , legitimacy , and accountability , and preventing rule circumvention , conflict , and attack toward moderators remained a subtle balance to achieve in content moderation . Overall , these three trade - offs in moderation styles , together with the trade - offs in modera - tion actions that we discussed in Section 4 , reflect deeper decision making rationales in content moderation , which we discuss in the next section . 6 Trade - offs in Moderation Philosophies The moderation actions and styles above reflect moderators’ varying moderation philosophies , which are prioritizations of competing needs that led to the actions and styles that the moderators chose to employ . In our dataset , we identified three major trade - offs in moderation philosophies : Nurturing vs . punishing , efficiency vs . quality of moderation , and level of activity vs . quality of contribution . 6 . 1 Nurturing vs . Punishing Nurturing and punishing both aim to create a positive online environment , but reflect different ideals in moderation’s purposes , which Ruckenstein and Turunen [ 91 ] conceptualized as “the logic of choice” and “the logic of care . ” Nurturing takes an educational approach that aims to improve or reform community members’ behavior , while punishing focuses on removing the rule - violating content from the community , and making sure that the rule - violating person receives consequences for their behavior . We saw nurturing typically associated with less harsh and more educational actions like providing warnings , offering explanations , and actively diffusing conflicts in the community . Seering et al . [ 97 ] noted that moderators who took a nurturing approach saw misbehaviors as something to be reformed rather than to be eliminated : Rather than seeing misbehavior as something that could be “cleaned up” by algorithms or bans , many moderators choose to engage personally during incidents to set an example for future interactions . [ 97 , p . 2 ] A reformative approach can be desirable especially because not all misbehaviors come from malicious perpetrators who intentionally disrupt communities . Jhaver et al . [ 48 ] found that some people broke rules simply because they misinterpreted or unintentionally overlooked the rules , and argued that it was worthwhile to nurture these sincere users by offering explanations so as to not drive them away . Furthermore , as we discussed in the transparent vs . opaque trade - off , providing explanations to educate users could improve their behavior as well as their perceptions of content moderation in their communities . These benefits had prompted researchers to argue for a nurturing rather than punitive approach in content moderation [ 48 , 108 ] . However , punishing can also be valuable in community maintenance . While arguing for a general nurturing approach to moderation , Jhaver et al . [ 48 ] also highlighted the necessity of punishment : ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 14 J . A . Jiang et al . We note that although supporting users who have the potential to be valuable con - tributors is a worthy goal , there are other constraints and trade - offs that need to be considered . For example , moderator teams , particularly on platforms like Reddit where voluntary users regulate content , often have limited human resources . Such teams may prioritize removing offensive or violent content to keep their online spaces usable . [ 48 , p . 26 ] While suggesting differential treatments of rule violation between well - meaning and malicious people , Jhaver et al . rightfully pointed out the limitation in human moderation resources—providing detailed , customized nurturing requires human work , a point we have reiterated in discussing the transparency vs . opacity trade - off . Furthermore , more human resources invested in nurturing meant less in punitive actions such as removal , which was necessary to remove harmful content to prevent them from overwhelming legitimate content . Einwiller and Kim [ 25 ] , through a study of online content providers in four countries , extended Jhaver et al . ’s [ 48 ] volunteer moderation - based arguments to commercially - moderated platforms , highlighting the heightened difficulty of a nurturing approach when the scale was much larger : [ Interviewees ] stated that decisively pointing out publicly where and why comments violated the policy and referring to the respective policy could help educate the poster and those observing . When the volume of [ harmful online content ] is large , however , doing so is often impossible . It is also a challenge to do this when a user is clearly trolling or posts are severely harming others so that they have to be removed immediately . [ 25 , p . 198 ] Einwiller and Kim identified severity as another key reason for taking a punitive approach to prevent exposing platform users to harm . Jiang et al . [ 54 ] found that platform moderation had to face a wide range of harmful content , from insensitive jokes to coordination of mass murder . The latter obviously requires immediate removal and possibly an account ban , rather than a kind , educational message saying that mass murder does not contribute to a positive online environment . The severity - based moderation philosophy applied not only to platforms , but to smaller communities as well [ 7 , 53 , 97 ] . Therefore , the configuration in the nurturing vs . punishing trade - off , like in all other trade - offs , is a hybrid one in practice , with differing tendencies toward one or the other depending on the specific community context . 6 . 2 Level of Activity vs . Quality of Contributions The trade - off between level of activity and quality of contributions is related to content in the community . It represents competing desires of a large amount of traffic in a community ( e . g . , a large number of members , a high amount of daily posts ) , and high quality contributions in the community ( e . g . , correct categorization , minimum low - effort posting 3 ) . The trade - off between level of activity and quality of contribution relates to how strictly modera - tors enforce the community rules , which represents a trade - off that we have discussed in moderation actions : Loose moderation retains community members but may also retain low quality or even harmful content , whereas strict moderation promotes high quality content but may stifle the community [ 43 , 64 ] . Srinivasan et al . [ 102 ] , for example , concluded that strict moderation through removal contributed to a high quality of community content , but also acknowledged the possibility that authors of the moderated posts might get discouraged and leave the community . Furthermore , research [ 50 ] found that providing explanations , the more nurturing and less punitive approach than mere removal , also had the potential to alienate users and drive them away , noting “moderators 3 Often called “shitposting” in online communities . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 15 may need to consider whether having high traffic is more important to them than having quality content on their community . ” The battle between traffic and quality was also one that community members realized . Jhaver et al . [ 48 ] found that community members would intentionally break rules that ensure clean organization of community content , which in their case , was a rule that mandated that questions are only posted in designated threads . While community members acknowledged the purpose and necessity of that rule , they believed that it made individual questions invisible and “stifled community interactions , ” and chose to break the rule with speculations that their posts would subsequently be removed . Here , it is clear that making their own questions visible was more important to these community members , and the potential benefits outweighed the risks of breaking the rule . However , considering the scale of today’s online community , having questions scattered in the community without a centralized repository ( e . g . , a question thread ) may overwhelm other members in the community . Similarly , members of the r / NoSleep subreddit noted that while strict regulation helped them survive the surge of newcomers as a result of becoming one of the default subreddits , it also deprived old members of the kind of freedom they used to enjoy [ 62 ] . Therefore , having to face different types of community members , the answer to the level of activity vs . quality of contribution trade - off may not be obvious to the moderators . 6 . 3 Efficiency vs . Quality of Moderation While quality of community contribution was an important consideration , so was another kind of quality—the quality of moderation . The trade - off between efficiency and quality represent two competing characteristics of content moderation work . On the one hand , moderation needs to be efficient in order to monitor and handle content in a timely manner . On the other hand , moderation also needs to fulfill goals related to quality , which converge to the central goal of making sure all content receives appropriate treatment . There is a reason for the overly - broad definition of quality . Later we will show that the meaning of quality is complex , and consists of multiple , sometimes competing factors . The reason for the need for efficiency is straightforward : Undesirable contents should not stay up for too long . Undesirable contents range from unuseful to harmful , and the longer they exist , the more impact they have on the community . Moderation research from the earliest time has expressed the desire for efficiency [ 66 , 107 ] , which became one of the primary reasons for the widespread use of automated moderation tools [ 49 ] . Minimizing delay in moderation has become even more important as online communities gain variety . The most prominent examples are communities with real - time interactions . Seering et al . [ 95 ] noted that on the live streaming platform Twitch , “due to the synchronous nature of conversations . . . moderation decisions need [ ed ] to happen immediately . ” In voice - based communities on Discord , interactions were not only in real - time but also ephemeral . Therefore , unless moderation could happen with virtually no delay , moderators needed to seek evidence of rule breaking to make sure it even happened , a problem Discord moderators were constantly facing [ 53 ] . Furthermore , the need for efficiency did not only exist for identifying the misbehavior , but also for deciding what actions to take on the misbehavior : However , participants also described aspects they did not like about deliberation . Eight people mentioned lower efficacy . One user identified a trade - off between efficacy and richer user input . [ 27 , p . 9 ] In Fan and Zhang’s digital jury experiment where they recruited users to serve as “jurors” that make moderation decisions , they found that deliberation between the jurors delayed the moderation decision in sacrifice to careful , democratic decision making . The digital jury example is exemplary ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 16 J . A . Jiang et al . of the trade - off between efficiency and quality in question , with quality represented by “richer user input . ” The meaning of moderation quality , however , is more complex . Different studies conceptualized “quality” differently , as already shown by previous sections . For example , Fan and Zhang [ 27 ] considered “quality” to be democratic legitimacy and accountability . Schoenebeck et al . [ 93 ] argued that “quality” should be customized moderation that did not fail some people while privileging others . Jhaver et al . [ 49 ] believed “quality” of moderation to be minimal incorrect decisions ( i . e . , “false negatives” and “false positives” ) , though the concept of “correct” might be just as complex as “quality . ” In a tricky case of r / AskHistorian where moderators had to choose between directly removing a post and explaining why that post was subtly harmful , Gilbert et al . [ 35 ] presented an example of almost complete surrender of efficiency for the pursuit of high - quality moderation , where a moderator biked to a nearby , paywalled library to find answers to a community member’s question . While these examples are by no means comprehensive , all of them require human deliberation and thus , a sacrifice of efficiency to various extents . As the trade - off between efficiency and quality pertains to every moving part in what we have described in the previous sections about moderation actions and styles , it might be the case that decisions in different parts will compete with each other and impact the overall efficiency vs . quality trade - off as a whole . Overall , these three trade - offs in moderation philosophies presented in this section represent competing desires for the purpose of moderation , the process of moderation , and the community content shaped by moderation . These subtle decisions in philosophies reflect values that different stakeholders in online communities hold , which we discuss in the next section . 7 Trade - offs in Moderation Values So far , we have discussed many trade - offs in moderation actions , styles , and philosophies . These trade - offs show competing needs that are all legitimate , have pros and cons , and do not have clear , “right” answers . However , facing these trade - offs , moderators must make decisions , and we found that these decisions were impacted by trade - offs in the values that they might hold . While these trade - offs were not characterized by pairs of polar opposites , any value position could come in tension with other alternatives . In our dataset , we identified three facets in the trade - offs in moderation values : Moderator identities , community identities , and competing stakeholders . 7 . 1 Moderator Identities Moderator identities are what moderators see themselves as in their communities , such as governors , teachers , and gardeners , to give a few examples . Prior research ( e . g . , [ 110 ] ) often referred to these identities as social roles characterized by designated tasks , but here we use the term “identity” to emphasize moderators’ self - perceived high - level responsibilities that transcend specific tasks . The differences in moderator identities have been a prominent theme since the earliest of moderation research in our corpus : One admin saw his role as being particularly centred on careful management of people in “keeping the peace” and maximising the potential of others , while another saw his role as being more based around the filtering of discussions and the group pool . [ 46 , p . 12 ] In their study of Flickr administrators , Holmes and Cox found moderator identities that corre - spond to the nurturing vs . punishing trade - off in moderation philosophies . As online communities evolve , the perceived identities also start to vary more . For example , Matias [ 72 ] listed a range of identities that his participants self - identified with , along with different corresponding duties . For ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 17 example , dictators “make all the decisions , ” janitors “clean up , ” and martyrs “give hell to anyone who dared to . . . threaten [ their ] communities . ” Similarly , Seering et al . [ 97 ] in their study of 56 moderators also found several identities , including arbiters , community managers , role models , etc . While we do not get into the details of the subtle differences and overlaps between the identities listed here ( which deserves its own research ) , moderators used them to justify the moderation decisions they had made . While prior research has not always made explicit connections between these identities and philosophies , styles , and actions , it is reasonable to speculate that moderators who identify as arbiters would prefer to adopt centralized moderation , and those who identify as curators would care about the quality of contributions more than the level of community activity . Overall , taking up a certain identity means to serve certain responsibilities and purposes , and to take actions accordingly [ 110 ] . Wohn [ 110 ] also pointed out that moderator identities were not mutually exclusive . The co - presence of these different , sometimes competing identities showed that there was a need for many of them—for example , moderation may need to be nurturing and punishing , instead of nurturing or punishing . Another line of research on social roles ( e . g . , [ 111 ] ) also echoed these simultaneously existing identities . Gurzick et al . [ 43 ] described how moderators were aware of the need to balance identities , and that moderators “debated the proper role that they should take and negotiated the amount of activity that would be reasonable . ” The negotiation of these identities shows that making decisions in the trade - offs in actions , styles , and philosophies may extend beyond their own pros and cons , to a deliberation of value differences . 7 . 2 Community Identities In addition to how moderators see themselves , how moderators see their communities also has an impact on how they moderate them . We call communities’ self - conception of what kind of communities they are as “community identities . ” The perceived identity of a community determines who and what is welcome or unwelcome in the community , and what purpose the community is supposed to serve . A prominent trade - off in community identities is that between , as Gibson [ 34 ] named , “free speech , ” and “safe spaces . ” The former referred to online spaces that promote free expression of opinions , while the latter emphasized mitigating potential harm that speech could cause . Gibson found that compared to “free speech” spaces , in “safe spaces” moderators removed significantly more content , indicating a punitive tendency that focused more on the quality of community content ( in this case , content that did not harm marginalized communities ) . Like Gibson , other research [ 42 , 83 ] also revealed these two often competing identities , highlighting that it is a difficult trade - off to balance : As political and ideological stratification in society continues to grow , and online communities focused on ideological commitments become more numerous , moderators of online platforms . . . face difficult challenges in how to balance the right to free expression , with broader concerns of public safety and wellbeing . [ 42 , p . 203 ] Trade - offs in community identities also existed for communities committed to certain topics , where moderators struggled to balance competing conceptualizations of the topic . For example , in r / Paleo , a subreddit for the paleo diet 4 , moderators struggled to maintain a balance between a consistent community conception of paleo diet and individualized understandings of what paleo diet is : 4 Wikipedia explains paleo diet as “a modern fad diet consisting of foods thought to mirror those eaten during the Paleolithic era” [ 3 ] . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 18 J . A . Jiang et al . Paleo faces a tension between the need to maintain some kind of coherent concept of the diet while also allowing flexibility for adherents to pursue a diet that accounts for individual differences . One way of negotiating this tension comes through the rules of the subreddit . One of the only rules that r / paleo moderators actively enforce is not to “ [ a ] ct like your One True Paleo™ is the be - all , end - all and is perfect for every human on Earth . ” [ 101 , p . 1919 ] Here , the moderators did not decide on one particular identity to pursue as a community , but simply required that members keep an open mind toward all versions of the paleo diet . The differences in how certain topics are conceptualized also exists in research of these commu - nities , with pro - eating disorder ( pro - ED ) communities the most prominent . A long line of research [ 12 – 14 , 28 , 32 ] on moderating pro - ED communities shows a clear trajectory of how pro - ED com - munities are viewed : From communities that promote eating disorders as a legitimate lifestyle , to those that support and help people with eating disorders . The co - presence of competing concep - tualizations meant that the same content could be treated differently due to ( 1 ) how they were perceived , and ( 2 ) whether that perception matched the community identity : Harm reduction provides resources for individuals who have an eating disorder , but cannot or will not recover , to stay safe and informed . Despite benefits , harm reduction resources are treated differently across eating disorder spaces online . While some communities freely permit them , others , such as one of the active subreddits in our digital ethnography , have moderation teams dedicated to removing posts related to tips or advice and carefully overseeing content related to harm reduction . [ 28 , p . 16 ] Feuston et al . [ 28 ] argued that content moderation should consider the full complexity of marginal - ized experiences such as eating disorders , and not cast negative stereotypes on content like harm reduction that might help those in need . While Feuston et al . provided an example of how fulfilling stereotypical community identities could be harmful , Gilbert [ 35 ] further complicated the issue by demonstrating how fulfilling seemingly innocent identities could also cause unintentional harm . In the same example we discussed in the efficiency vs . quality of moderation trade - off , where an r / AskHistorians member posted a question about the background of a historical photo featuring naked women in military , fulfilling the community identity became at odds with the need of being contextually sensitive : In circumstances in which biased or insensitive questions are asked , moderators are tasked with making the decision to let the question stand or remove it , and experts with the decision to respond to the question or ignore it . . . . During our interview , moderator , Mark Evans described deliberating whether or not to remove the question : “We had a discussion about removing it because the pictures are incredibly . . . exploitative . . . And we just felt so shitty as moderators , because here was our community , which is meant to be giving people answers about the past , but what it’s doing is providing Redditors with porn . And that’s what it ended up doing . And that’s why people have ended up looking at it and it’s become a platform for these poor women to become humiliated again , like 80 years after the event . Again . ” [ 35 , pp . 11 - 12 ] As Gilbert later pointed out , the issue of whether or not to provide people with answers about an exploitative past raised questions about trade - offs between centralized and distributed moderation , as well as “free speech” and “safe spaces . ” While prior research argued that community identity might not be as salient in the moderation of platforms due to the lack of strong ties [ 29 ] , the discrepancy of perceived platform identity could still be a source of conflict , like when Yelp users left one - star reviews for a merchant that employed someone who had contentious political beliefs on immigration , many of which Yelp removed [ 74 ] . While Yelp intended the reviews to be about ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 19 the commercial services of merchants , the users found them as “symbolically significant means of signaling social disapproval . ” Medeiros [ 74 ] characterized the unintended use of reviews as “a genuinely vexing moderation challenge for Yelp , suggesting a limit to the site’s ability to enforce rules that dichotomize political and commercial content . ” In both examples above , core to the problems is the different prioritization of community identities across different stakeholders . We explore the impacts of different stakeholders in the next section . 7 . 3 Competing Stakeholders Moderation is often expected to satisfy multiple stakeholders and their often different needs , which presents a difficult task for moderators who often have to make decisions that serve some over others . Matias [ 72 ] , for example , summarized volunteer moderators’ work of serving different stakeholders as their “civic labor” : This “civic labor” requires moderators to serve three masters with whom they nego - tiate the idea of moderation : the platform , reddit participants , and other moderators . Moderators differ in the pressure they receive from these parties and the weight they give them . Some face further stakeholders outside the platform . Yet attempts to make sense of moderation by focusing on any one of these relationships can bring the other actors out of focus . [ 72 , p . 8 ] While we have shown in previous sections the impact of community members and other moder - ators , platforms are also a significant factor . Volunteer moderators’ power cannot reach beyond the purview of the platform where their communities are hosted , and consequences could be severe when negotiations with platforms fail . One such example is the Reddit blackout , where many moderators shut down their communities in response to Reddit’s dismissal of an employee who routinely offered support to volunteer moderators . Matias [ 71 ] showed that such protest against the platform was still a negotiation among moderators , users , and the platform : Reddit employees played a key role in these negotiations [ with Reddit ] . . . . Across subreddits of all sizes , relations among moderators were also associated with participa - tion in the blackout . . . . Community members also played an important role in action against the platform by pressuring moderators to join the blackout , discussing and voting in decisions , and sometimes even punishing moderators who disagreed . [ 71 , pp . 1146 - 1147 ] Like volunteer moderation , commercial moderation faces the same trade - off between multiple stakeholders . The common factor was users—for example , differently politically affiliated users also perceived content moderation differently [ 47 , 98 ] . Schoenebeck et al . [ 93 ] also found that people with different backgrounds had significantly different preferences for the kinds of remedy social media sites could offer for online harassment . However , platform moderation also needed to satisfy a new set of stakeholders . First , unsur - prisingly , platforms have to operate under the requirement of local law , which often ban severely harmful content on a statutory level such as child pornography and terrorism [ 25 , 36 , 112 ] . However , these content may still provide value to someone else : Often disturbing , graphic , and controversial , human rights - related media like the Werfalli and Syrian war videos pose a dilemma for platforms hosting them , involving difficult tradeoffs between their perceived social value and their possible harms . [ 4 , p . 2 ] Banchik [ 4 ] found that even graphically abusive content may prove to be valuable documentations to various human rights workers , adding that : ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 20 J . A . Jiang et al . Practitioners I spoke with expressed added concern that biased or merely ill - informed human reviewers “without the requisite knowledge” would decide the fate of vital documentation . Moreover , most practitioners did not blame platforms alone for the removal of content , but instead saw the topography of takedowns as far more complex . [ 4 , p . 7 ] Platforms are also aware of the complexity of harmful content given their potential public value . Facebook’s Community Standards [ 26 ] , for example , states : In some cases , we allow content for public awareness which would otherwise go against our Community Standards—if it is newsworthy and in the public interest . We do this only after weighing the public interest value against the risk of harm and we look to international human rights standards to make these judgments . However , Facebook’s decision to not remove some violence - inciting message on the same ground provoked heated debate among users and various experts [ 99 ] . Furthermore , for platform designers , the fundamental need to moderate content for users becomes a trade - off to consider with the psychological health of moderators . Both academic research [ 58 , 70 , 90 ] and journalistic coverage [ 76 ] revealed the emotional impact of moderating disturbing content . As platform technologies evolve into new forms like live - streaming video , produced content are more likely to provoke intensified emotional reactions , and therefore what is asked from moderators , both logistically and emotionally , can also escalate [ 70 ] . Above are only some of the examples of the full complexity of content moderation in a multi - stakeholder environment . Realization of the needs of multiple stakeholders has prompted many studies to call out against a one - size - fits - all approach to content moderation [ 6 , 31 , 53 , 93 ] . However , as desirable as customized moderation might be , it may not be entirely feasible due to constraints in human and technological resources . Then , whose needs are prioritized , and what downstream impacts it has on various trade - offs , are critical problems to consider in content moderation . 8 How Different Stakeholders Can Use Our Framework Our framework offers a way to examine content moderation that posits trade - offs in the front and center . As an example , Seering et al . ’s [ 97 ] findings on the differences in actions taken by moderators toward misbehaviors indicate that values impact moderator actions . However , if we examine their findings through the lens of our framework , we can reveal several additional research questions related to the trade - offs that could have happened : While communities with more laissez - faire ideologies use fewer bans than communities that intended to be “safe spaces , ” what prompted the communities to side with certain ideologies over others ? Do moderators’ perceived identities differ between Reddit and Facebook , and does that have an impact on differences in the level of reliance on automated tools ? These are only a few examples of the questions we can ask from the application of our framework , and Seering et al . ’s [ 97 ] speculation of the answers to the latter question testified to the value of our framework—“The difference [ in the preference of automated tools ] likely results from the importance of continuously evolving community values in decisions made by moderators . ” Answers to these questions will offer a deep , rich understanding of the inner - working of content moderation from a new angle . The above example is only one way researchers of content moderation can use this framework as an analytical tool in their own research . For example , beyond identifying moderation actions in the community , a researcher can use our framework to go one step further and identify key trade - offs in moderators’ decision - making , investigate why moderators took certain actions instead of other actions they could have taken , and trace back to their philosophies and values behind these decisions . Furthermore , researchers can also use our framework to identify potential value tensions behind ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 21 certain philosophies , and potential caveats of recommendations they might make . For example , do certain platform affordances favor certain philosophies and values ? When recommending that the moderation of a community or a platform should be more transparent , what are the potential stakeholder tensions that may prevent it from doing so ? How can it resolve such tensions to get closer to the researchers’ ideal ? Designers of content moderation can use our framework as a heuristic for their design , either to improve an existing content moderation system , or to build a new one . Designers who wish to improve an existing moderation system can use our framework to identify key decision points that moderators may struggle with and to be critically aware of the trade - offs and tensions involved . While their designs may inevitably favor one side of a trade - off , designers can consciously find their ideal balance in the trade - off so that their designs can be more considerate of the other side . Similar to the case of researchers , some trade - offs may not be applicable or salient to some communities or platforms . While designers should focus on the trade - offs as appropriate , with our framework they can also consider making some previously invisible trade - offs more salient as a potential form of improvement . Designers who wish to build new content moderation systems can use our framework as a guide to support moderators in key decision points . For example , designers may consider explicitly showing the available actions and decisions to moderators as trade - offs instead of a simple listing , as well as the potential consequences of making different decisions . Designers can present these trade - offs not only in manuals or training materials , but also in the interface of moderators’ day - to - day work , so that moderators can be more informed when making decisions . Moderators may also benefit from our framework as a way to encourage reflexivity in their own work . For example , our framework will allow moderators to realize that when they make a decision on doing something , they are also making decisions on not doing something else . Therefore , moderators will be able to make more conscious trade - offs in their work , and have elaborate justifications for past decisions that may be valuable for revising or improving their workflow . Finally , users , or people who are moderated , may find our framework informative when par - ticipating in content moderation in various ways . As a key element in content moderation , users will be able to learn the full complexity of moderation from the trade - off - centered framework , and therefore be more informed when disputing moderation decisions , contributing to rule making , or engaging in conversations about content moderation in general . 9 Trade - offs Define Content Moderation Our framework characterizes content moderation in terms of trade - offs on multiple levels . First , we found many competing choices in trade - offs in moderation actions and styles . Each choice has its own pros and cons that , as we have shown , relate to trade - offs in moderation philosophies . For example , the trade - off between leniency and harshness and that between immediately removing harm and long - term education in moderation actions demonstrate clear connections to the level of activity vs . quality of contribution and the nurturing vs . punishing trade - offs respectively . The different pros and cons of competing moderation styles also find their way to trade - offs in philosophies . Overall , moderation philosophies reflect the fundamental needs and purposes that moderation actions and styles aim to serve . In trade - offs in moderation philosophies , many options are often believed to ( or at least be supposed to ) go hand in hand with each other : Moderation should be both educational for sincere community members and punishing for malicious actors . Moderation should be both efficient and of high quality . Moderation should maintain community members’ engagement and activities while ensuring a high quality of contribution . While these goals often seem to be congruent , in our analysis of moderation literature , we found that they were often at odds with each other . As ideal ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 22 J . A . Jiang et al . as it would be to achieve both sides of the trade - offs , we saw evidence that a tendency toward one side may necessarily be at the cost of the other . Furthermore , these philosophies trace back to Grimmelmann’s commonly - cited definition of content moderation as “the governance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse” [ 40 ] . The trade - offs in moderation philosophies echo the goals of moderation in Grimmelmann’s definition : Nurturing , moderation quality , and level of activity are different facets of facilitating cooperation , while punishing , moderation efficiency , and quality of contribution represent different dimensions of preventing abuse . However , while Grimmelmann indicates that these two goals are to be achieved at the same time , our trade - off centered analysis shows a different relationship : Facilitating cooperation and preventing abuse may be at tension with each other in practice . If the two definitional components of content moderation constitute a trade - off , then we argue that content moderation as a whole can be conceptualized as a series of trade - offs , and that moderation work is making choices and striking balances between simultaneously desirable goals . Then , how can moderators balance facilitating cooperation and preventing abuse ? The trade - offs in values may provide answers to this question . Our findings suggest that the driving force behind which component is favored more is dependent on the moderators’ perceived identities of themselves and their visions for their communities , both of which are also shaped by various stakeholders including other moderators , community members , platforms , legal requirements , etc . These forces work together and converge toward a unique decision point between facilitating cooperation and preventing abuse . While we have summarized the major trade - offs that we have identified in our corpus , there may be other , likely more granular trade - offs that we have not listed in the paper . Therefore , while using the trade - offs identified in this paper as a preliminary checklist may prove useful , we believe that a trade - off - centered perspective in content moderation will be more valuable . Therefore , when examining a decision in content moderation , we urge researchers and designers to consider them as a trade - off instead : What is the other side of the trade - off ? Does it privilege someone’s perspective and disadvantage someone else’s ? 10 Conclusion In this paper , we propose a trade - off - centered framework of content moderation . We describe four major layers of trade - offs in our framework at increasing levels of abstractions—in moderation actions , styles , philosophies , and values—and how they are related to each other . These trade - offs are pervasive in content moderation practices , and reveal the dialectic nature of content moderation . While existing literature largely conceptualizes content moderation as being built on a series of standalone actions , we believe a trade - off centered framework provides a more holistic perspective : What are pros and cons of taking a certain moderation action ? What do stakeholders gain and give up by taking up certain philosophies ? What does it mean for the community if any trade - off rationale becomes normative and codified and enforced at scale ? We believe that moderation researchers , designers , moderators , and users will all find value in taking a trade - off - centered approach to content moderation , and we hope this paper will provide a fresh agenda for content moderation research . Acknowledgments We thank Jes Feuston , Joseph Seering , and anonymous reviewers for their valuable feedback . References [ 1 ] [ n . d . ] . automoderator - reddit . com . https : / / www . reddit . com / wiki / automoderator ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 23 [ 2 ] 2002 . Problems of conflict management in virtual communities . Routledge . https : / / doi . org / 10 . 4324 / 9780203194959 - 16 Pages : 145 - 174 Publication Title : Communities in Cyberspace . [ 3 ] 2020 . Paleolithic diet . https : / / en . wikipedia . org / w / index . php ? title = Paleolithic _ diet & oldid = 979608774 Page Version ID : 979608774 . [ 4 ] Anna Veronica Banchik . 2020 . Disappearing acts : Content moderation and emergent practices to preserve at - risk human rights – related content . New Media & Society ( March 2020 ) , 1461444820912724 . https : / / doi . org / 10 . 1177 / 1461444820912724 Publisher : SAGE Publications . [ 5 ] Lindsay Blackwell , Tianying Chen , Sarita Schoenebeck , and Cliff Lampe . 2018 . When Online Harassment Is Perceived as Justified . In Twelfth International AAAI Conference on Web and Social Media . https : / / www . aaai . org / ocs / index . php / ICWSM / ICWSM18 / paper / view / 17902 [ 6 ] Lindsay Blackwell , Jill Dimond , Sarita Schoenebeck , and Cliff Lampe . 2017 . Classification and Its Consequences for Online Harassment : Design Insights from HeartMob . Proceedings of the ACM on Human - Computer Interaction 1 , CSCW ( Dec . 2017 ) , 24 : 1 – 24 : 19 . https : / / doi . org / 10 . 1145 / 3134659 [ 7 ] Lindsay Blackwell , Nicole Ellison , Natasha Elliott - Deflo , and Raz Schwartz . 2019 . Harassment in Social Virtual Reality : Challenges for Platform Governance . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 100 : 1 – 100 : 25 . https : / / doi . org / 10 . 1145 / 3359202 [ 8 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative Research in Psychology 3 , 2 ( Jan . 2006 ) , 77 – 101 . https : / / doi . org / 10 . 1191 / 1478088706qp063oa [ 9 ] Robyn Caplan . 2018 . Content or Context Moderation ? https : / / datasociety . net / output / content - or - context - moderation / [ 10 ] Alissa Centivany and Bobby Glushko . 2016 . " Popcorn Tastes Good " : Participatory Policymaking and Reddit’s . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . Association for Computing Machinery , New York , NY , USA , 1126 – 1137 . https : / / doi . org / 10 . 1145 / 2858036 . 2858516 [ 11 ] Stevie Chancellor , Eric P . S . Baumer , and Munmun De Choudhury . 2019 . Who is the " Human " in Human - Centered Machine Learning : The Case of Predicting Mental Health from Social Media . Proc . ACM Hum . - Comput . Interact . 3 , CSCW ( Nov . 2019 ) , 147 : 1 – 147 : 32 . https : / / doi . org / 10 . 1145 / 3359249 [ 12 ] Stevie Chancellor , Andrea Hu , and Munmun De Choudhury . 2018 . Norms Matter : Contrasting Social Support Around Behavior Change in Online Weight Loss Communities . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . Association for Computing Machinery , Montreal QC , Canada , 1 – 14 . https : / / doi . org / 10 . 1145 / 3173574 . 3174240 [ 13 ] StevieChancellor , YannisKalantidis , JessicaA . Pater , MunmunDeChoudhury , andDavidA . Shamma . 2017 . Multimodal Classification of Moderated Online Pro - Eating Disorder Content . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( CHI ’17 ) . Association for Computing Machinery , Denver , Colorado , USA , 3213 – 3226 . https : / / doi . org / 10 . 1145 / 3025453 . 3025985 [ 14 ] Stevie Chancellor , Jessica Annette Pater , Trustin Clear , Eric Gilbert , and Munmun De Choudhury . 2016 . # thyghgapp : Instagram Content Moderation and Lexical Variation in Pro - Eating Disorder Communities . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing ( CSCW ’16 ) . Association for Computing Machinery , San Francisco , California , USA , 1201 – 1213 . https : / / doi . org / 10 . 1145 / 2818048 . 2819963 [ 15 ] Eshwar Chandrasekharan , Chaitrali Gandhi , Matthew Wortley Mustelier , and Eric Gilbert . 2019 . Crossmod : A Cross - Community Learning - based System to Assist Reddit Moderators . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 174 : 1 – 174 : 30 . https : / / doi . org / 10 . 1145 / 3359276 [ 16 ] Eshwar Chandrasekharan , Umashanthi Pavalanathan , Anirudh Srinivasan , Adam Glynn , Jacob Eisenstein , and Eric Gilbert . 2017 . You Can’T Stay Here : The Efficacy of Reddit’s 2015 Ban Examined Through Hate Speech . Proc . ACM Hum . - Comput . Interact . 1 , CSCW ( Dec . 2017 ) , 31 : 1 – 31 : 22 . https : / / doi . org / 10 . 1145 / 3134666 [ 17 ] Eshwar Chandrasekharan , Mattia Samory , Shagun Jhaver , Hunter Charvat , Amy Bruckman , Cliff Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s Hidden Rules : An Empirical Study of Reddit Norm Violations at Micro , Meso , and Macro Scales . Proc . ACM Hum . - Comput . Interact . 2 , CSCW ( Nov . 2018 ) , Article 32 . https : / / doi . org / 10 . 1145 / 3274301 [ 18 ] Justin Cheng , Michael Bernstein , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2017 . Anyone Can Become a Troll : Causes of Trolling Behavior in Online Discussions . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW ’17 ) . Association for Computing Machinery , New York , NY , USA , 1217 – 1230 . https : / / doi . org / 10 . 1145 / 2998181 . 2998213 [ 19 ] Srayan Datta and Eytan Adar . 2019 . Extracting Inter - Community Conflicts in Reddit . Proceedings of the International AAAI Conference on Web and Social Media 13 ( July 2019 ) , 146 – 157 . https : / / www . aaai . org / ojs / index . php / ICWSM / article / view / 3217 [ 20 ] Carl DiSalvo , Phoebe Sengers , and Hrönn Brynjarsdóttir . 2010 . Mapping the Landscape of Sustainable HCI . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’10 ) . ACM , New York , NY , USA , 1975 – 1984 . https : / / doi . org / 10 . 1145 / 1753326 . 1753625 event - place : Atlanta , Georgia , USA . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 24 J . A . Jiang et al . [ 21 ] Bryan Dosono and Bryan Semaan . 2019 . Moderation Practices As Emotional Labor in Sustaining Online Communities : The Case of AAPI Identity Work on Reddit . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . ACM , New York , NY , USA , 142 : 1 – 142 : 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300372 event - place : Glasgow , Scotland Uk . [ 22 ] Nora A . Draper . 2019 . Distributed intervention : networked content moderation in anonymous mobile spaces . Feminist Media Studies 19 , 5 ( July 2019 ) , 667 – 683 . https : / / doi . org / 10 . 1080 / 14680777 . 2018 . 1458746 Publisher : Routledge _ eprint : https : / / doi . org / 10 . 1080 / 14680777 . 2018 . 1458746 . [ 23 ] StefanieDuguay , JeanBurgess , andNicolasSuzor . 2018 . Queerwomen’sexperiencesofpatchworkplatformgovernance on Tinder , Instagram , and Vine : . Convergence ( June 2018 ) . https : / / doi . org / 10 . 1177 / 1354856518781530 Publisher : SAGE PublicationsSage UK : London , England . [ 24 ] John S . Ehrett . 2016 . E - judiciaries : a model for community policing in cyberspace . Information & Communications Technology Law 25 , 3 ( Sept . 2016 ) , 272 – 291 . https : / / doi . org / 10 . 1080 / 13600834 . 2016 . 1236428 Publisher : Routledge _ eprint : https : / / doi . org / 10 . 1080 / 13600834 . 2016 . 1236428 . [ 25 ] Sabine A . Einwiller and Sora Kim . 2020 . How Online Content Providers Moderate User - Generated Content to Prevent Harmful Online Communication : An Analysis of Policies and Their Implementation . Policy & Internet 12 , 2 ( 2020 ) , 184 – 206 . https : / / doi . org / 10 . 1002 / poi3 . 239 _ eprint : https : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / poi3 . 239 . [ 26 ] Facebook . [ n . d . ] . Community Standards . https : / / www . facebook . com / communitystandards / [ 27 ] Jenny Fan and Amy X . Zhang . 2020 . Digital Juries : A Civics - Oriented Approach to Platform Governance . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , Honolulu , HI , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3313831 . 3376293 [ 28 ] Jessica L . Feuston , Alex S . Taylor , and Anne Marie Piper . 2020 . Conformity of Eating Disorders through Content Moderation . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( May 2020 ) , 040 : 1 – 040 : 28 . https : / / doi . org / 10 . 1145 / 3392845 [ 29 ] Casey Fiesler and Amy S . Bruckman . 2019 . Creativity , Copyright , and Close - Knit Communities : A Case Study of Social Norm Formation and Enforcement . Proceedings of the ACM on Human - Computer Interaction 3 , GROUP ( Dec . 2019 ) , 241 : 1 – 241 : 24 . https : / / doi . org / 10 . 1145 / 3361122 [ 30 ] Casey Fiesler , Jialun " Aaron " Jiang , Joshua McCann , Kyle Frye , and Jed R . Brubaker . 2018 . Reddit Rules ! Characterizing an Ecosystem of Governance . In Twelfth International AAAI Conference on Web and Social Media . https : / / aaai . org / ocs / index . php / ICWSM / ICWSM18 / paper / view / 17898 [ 31 ] Silvia Elena Gallagher and Timothy Savage . 2016 . Comparing learner community behavior in multiple presentations of a Massive Open Online Course . Journal of Computing in Higher Education 28 , 3 ( Dec . 2016 ) , 358 – 369 . https : / / doi . org / 10 . 1007 / s12528 - 016 - 9124 - y [ 32 ] Ysabel Gerrard . 2018 . Beyond the hashtag : Circumventing content moderation on social media . New Media & Society 20 , 12 ( Dec . 2018 ) , 4492 – 4511 . https : / / doi . org / 10 . 1177 / 1461444818776611 [ 33 ] Guiseppe Getto and Jack T . Labriola . 2016 . iFixit Myself : User - Generated Content Strategy in “The Free Repair Guide for Everything” . IEEE Transactions on Professional Communication 59 , 1 ( March 2016 ) , 37 – 55 . https : / / doi . org / 10 . 1109 / TPC . 2016 . 2527259 Conference Name : IEEE Transactions on Professional Communication . [ 34 ] Anna Gibson . 2019 . Free Speech and Safe Spaces : How Moderation Policies Shape Online Discussion Spaces : . Social Media + Society ( March 2019 ) . https : / / doi . org / 10 . 1177 / 2056305119832588 Publisher : SAGE PublicationsSage UK : London , England . [ 35 ] Sarah A . Gilbert . 2020 . " I run the world’s largest historical outreach project and it’s on a cesspool of a website . " Moderating a Public Scholarship Site on Reddit : A Case Study of r / AskHistorians . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( May 2020 ) , 019 : 1 – 019 : 27 . https : / / doi . org / 10 . 1145 / 3392822 [ 36 ] Tarleton Gillespie . 2018 . Custodians of the Internet : Platforms , Content Moderation , and the Hidden Decisions that Shape Social Media . Yale University Press . [ 37 ] Tarleton Gillespie . 2019 . Content Moderation : A Reading List . https : / / socialmediacollective . org / reading - lists / content - moderation - reading - list / [ 38 ] Robert Gorwa , Reuben Binns , and Christian Katzenbach . 2020 . Algorithmic content moderation : Technical and political challenges in the automation of platform governance . Big Data & Society 7 , 1 ( Jan . 2020 ) , 2053951719897945 . https : / / doi . org / 10 . 1177 / 2053951719897945 Publisher : SAGE Publications Ltd . [ 39 ] Joanne E Gray and Nicolas P Suzor . 2020 . Playing with machines : Using machine learning to understand automated copyright enforcement at scale . Big Data & Society 7 , 1 ( Jan . 2020 ) , 2053951720919963 . https : / / doi . org / 10 . 1177 / 2053951720919963 Publisher : SAGE Publications Ltd . [ 40 ] James Grimmelmann . 2015 . The Virtues of Moderation . Yale Journal of Law and Technology 17 , 1 ( Sept . 2015 ) . https : / / digitalcommons . law . yale . edu / yjolt / vol17 / iss1 / 2 [ 41 ] The CommuniTree Group . 1981 . Communitree . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 25 [ 42 ] Ted Grover and Gloria Mark . 2019 . Detecting Potential Warning Behaviors of Ideological Radicalization in an Alt - Right Subreddit . Proceedings of the International AAAI Conference on Web and Social Media 13 ( July 2019 ) , 193 – 204 . https : / / www . aaai . org / ojs / index . php / ICWSM / article / view / 3221 [ 43 ] David Gurzick , Kevin F . White , Wayne G . Lutters , and Lee Boot . 2009 . A view from Mount Olympus : the impact of activity tracking tools on the character and practice of moderation . In Proceedings of the ACM 2009 international conference on Supporting group work ( GROUP ’09 ) . Association for Computing Machinery , Sanibel Island , Florida , USA , 361 – 370 . https : / / doi . org / 10 . 1145 / 1531674 . 1531727 [ 44 ] Aleksej Heinze , Elaine Ferneley , and Paul Child . 2013 . Ideal Participants in Online Market Research : Lessons from Closed Communities : . International Journal of Market Research ( Nov . 2013 ) . https : / / doi . org / 10 . 2501 / IJMR - 2013 - 066 Publisher : SAGE PublicationsSage UK : London , England . [ 45 ] Susan Herring , Kirk Job - Sluder , Rebecca Scheckler , and Sasha Barab . 2002 . Searching for Safety Online : Manag - ing " Trolling " in a Feminist Forum . The Information Society 18 , 5 ( Oct . 2002 ) , 371 – 384 . https : / / doi . org / 10 . 1080 / 01972240290108186 Publisher : Routledge _ eprint : https : / / doi . org / 10 . 1080 / 01972240290108186 . [ 46 ] Paul Holmes and Andrew M . Cox . 2011 . ’Every group carries the flavour of the admins’ : leadership on Flickr . International Journal of Web Based Communities 7 , 3 ( July 2011 ) , 376 – 391 . https : / / doi . org / 10 . 1504 / IJWBC . 2011 . 041205 [ 47 ] Yiqing Hua , Mor Naaman , and Thomas Ristenpart . 2020 . Characterizing Twitter Users Who Engage in Adversarial Interactions against Political Candidates . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , Honolulu , HI , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376548 [ 48 ] Shagun Jhaver , Darren Scott Appling , Eric Gilbert , and Amy Bruckman . 2019 . " Did You Suspect the Post Would be Removed ? " : Understanding User Reactions to Content Removals on Reddit . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 192 : 1 – 192 : 33 . https : / / doi . org / 10 . 1145 / 3359294 [ 49 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - Machine Collaboration for Content Regulation : The Case of Reddit Automoderator . ACM Trans . Comput . - Hum . Interact . 26 , 5 ( July 2019 ) , Article 31 . https : / / doi . org / 10 . 1145 / 3338243 [ 50 ] Shagun Jhaver , Amy Bruckman , and Eric Gilbert . 2019 . Does Transparency in Moderation Really Matter ? User Behavior After Content Removal Explanations on Reddit . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 150 : 1 – 150 : 27 . https : / / doi . org / 10 . 1145 / 3359252 [ 51 ] Shagun Jhaver , Sucheta Ghoshal , Amy Bruckman , and Eric Gilbert . 2018 . Online Harassment and Content Moderation : The Case of Blocklists . ACM Transactions on Computer - Human Interaction 25 , 2 ( March 2018 ) , 12 : 1 – 12 : 33 . https : / / doi . org / 10 . 1145 / 3185593 [ 52 ] Jialun " Aaron " Jiang , Casey Fiesler , and Jed R . Brubaker . 2018 . " The Perfect One " : Understanding Communication Practices and Challenges with Animated GIFs . Proc . ACM Hum . - Comput . Interact . 2 , CSCW ( 2018 ) , Article 80 . https : / / doi . org / 10 . 1145 / 3274349 [ 53 ] Jialun " Aaron " Jiang , Charles Kiene , Skyler Middler , Jed R . Brubaker , and Casey Fiesler . 2019 . Moderation Challenges in Voice - based Online Communities on Discord . Proc . ACM Hum . - Comput . Interact . 3 , CSCW ( Nov . 2019 ) , 55 : 1 – 55 : 23 . https : / / doi . org / 10 . 1145 / 3359157 [ 54 ] Jialun " Aaron " Jiang , Skyler Middler , Jed R . Brubaker , and Casey Fiesler . 2020 . Characterizing Community Guidelines on Social Media Platforms . In CSCW 2020 Companion . [ 55 ] Shan Jiang , Ronald E . Robertson , and Christo Wilson . 2019 . Bias Misperceived : The Role of Partisanship and Misinfor - mation in YouTube Comment Moderation . Proceedings of the International AAAI Conference on Web and Social Media 13 ( July 2019 ) , 278 – 289 . https : / / www . aaai . org / ojs / index . php / ICWSM / article / view / 3229 [ 56 ] Prerna Juneja , Deepika Rama Subramanian , and Tanushree Mitra . 2020 . Through the Looking Glass : Study of Transparency in Reddit’s Moderation Practices . Proceedings of the ACM on Human - Computer Interaction 4 , GROUP ( Jan . 2020 ) , 17 : 1 – 17 : 35 . https : / / doi . org / 10 . 1145 / 3375197 [ 57 ] Amalia Juneström . 2019 . Online user misconduct and an evolving infrastructure of practices : a practice - based study of information infrastructure and social practices . Information Research : an international electronic journal 24 , 1 ( March 2019 ) . http : / / informationr . net / ir / 24 - 1 / isic2018 / isic1825 . html Library Catalog : informationr . net Publisher : University of Borås . [ 58 ] Sowmya Karunakaran and Rashmi Ramakrishan . 2019 . Testing Stylistic Interventions to Reduce Emotional Impact of Content Moderation Workers . Proceedings of the AAAI Conference on Human Computation and Crowdsourcing 7 , 1 ( Oct . 2019 ) , 50 – 58 . https : / / www . aaai . org / ojs / index . php / HCOMP / article / view / 5270 Number : 1 . [ 59 ] Varol Onur Kayhan and Anol Bhattacherjee . 2013 . Content Use from Websites : Effects of Governance Mechanisms . Journal of Computer Information Systems 53 , 4 ( June 2013 ) , 68 – 80 . https : / / doi . org / 10 . 1080 / 08874417 . 2013 . 11645652 Publisher : Taylor & Francis _ eprint : https : / / doi . org / 10 . 1080 / 08874417 . 2013 . 11645652 . [ 60 ] Brian C Keegan and Casey Fiesler . 2017 . The Evolution and Consequences of Peer Producing Wikipedia ’ s Rules . In Proceedings of the AAAI International Conference on Web and Social Media ( ICWSM ) . Montreal , Quebec , Canada . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 26 J . A . Jiang et al . [ 61 ] Charles Kiene , Jialun " Aaron " Jiang , and Benjamin Mako Hill . 2019 . Technological Frames and User Innovation : Exploring Technological Change in Community Moderation Teams . Proc . ACM Hum . - Comput . Interact . 3 , CSCW ( Nov . 2019 ) , Article 44 . https : / / doi . org / 10 . 1145 / 3359146 [ 62 ] Charles Kiene , Andrés Monroy - Hernández , and Benjamin Mako Hill . 2016 . Surviving an " Eternal September " : How an Online Community Managed a Surge of Newcomers . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , USA , 1152 – 1156 . https : / / doi . org / 10 . 1145 / 2858036 . 2858356 [ 63 ] P . KollockandMarcSmith . 1996 . ManagingtheVirtualCommons : CooperationandConflictinComputerCommunities . Pragmatics and Beyond New Series ( Jan . 1996 ) , 109 – 128 . [ 64 ] Robert E . Kraut , Paul Resnick , Sara Kiesler , Moira Burke , Yan Chen , Niki Kittur , Joseph Konstan , Yuqing Ren , and John Riedl . 2011 . Building Successful Online Communities : Evidence - Based Social Design . Mit Press . https : / / www . jstor . org / stable / j . ctt5hhgvw [ 65 ] Cliff Lampe and Erik Johnston . 2005 . Follow the ( Slash ) Dot : Effects of Feedback on New Members in an Online Community . In Proceedings of the 2005 International ACM SIGGROUP Conference on Supporting Group Work ( GROUP ’05 ) . ACM , New York , NY , USA , 11 – 20 . https : / / doi . org / 10 . 1145 / 1099203 . 1099206 [ 66 ] Cliff Lampe and Paul Resnick . 2004 . Slash ( Dot ) and Burn : Distributed Moderation in a Large Online Conversation Space . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’04 ) . ACM , New York , NY , USA , 543 – 550 . https : / / doi . org / 10 . 1145 / 985692 . 985761 event - place : Vienna , Austria . [ 67 ] Cliff Lampe , Paul Zube , Jusil Lee , Chul Hyun Park , and Erik Johnston . 2014 . Crowdsourcing civility : A natural experiment examining the effects of distributed moderation in online forums . Government Information Quarterly 31 , 2 ( April 2014 ) , 317 – 326 . https : / / doi . org / 10 . 1016 / j . giq . 2013 . 11 . 005 [ 68 ] Qinying Liao , Yingxin Pan , Michelle X . Zhou , and Fei Ma . 2010 . Chinese online communities : balancing man - agementcontrol and individual autonomy . In Proceedings of the SIGCHI Conference on Human Factors in Com - puting Systems ( CHI ’10 ) . Association for Computing Machinery , Atlanta , Georgia , USA , 2193 – 2202 . https : / / doi . org / 10 . 1145 / 1753326 . 1753658 [ 69 ] Alessandro Liberati , Douglas G . Altman , Jennifer Tetzlaff , Cynthia Mulrow , Peter C . Gøtzsche , John P . A . Ioannidis , Mike Clarke , P . J . Devereaux , Jos Kleijnen , and David Moher . 2009 . The PRISMA Statement for Reporting Systematic Reviews and Meta - Analyses of Studies That Evaluate Health Care Interventions : Explanation and Elaboration . PLoS Medicine 6 , 7 ( July 2009 ) . https : / / doi . org / 10 . 1371 / journal . pmed . 1000100 [ 70 ] Mufan Luo , Tiffany W . Hsu , Joon Sung Park , and Jeffrey T . Hancock . 2020 . Emotional Amplification During Live - Streaming : Evidence from Comments During and After News Events . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( May 2020 ) , 047 : 1 – 047 : 19 . https : / / doi . org / 10 . 1145 / 3392853 [ 71 ] J . Nathan Matias . 2016 . Going Dark : Social Factors in Collective Action Against Platform Operators in the Reddit Blackout . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . Association for Computing Machinery , San Jose , California , USA , 1138 – 1151 . https : / / doi . org / 10 . 1145 / 2858036 . 2858391 [ 72 ] J . Nathan Matias . 2019 . The Civic Labor of Volunteer Moderators Online : . Social Media + Society ( April 2019 ) . https : / / doi . org / 10 . 1177 / 2056305119836778 Publisher : SAGE PublicationsSage UK : London , England . [ 73 ] J . Nathan Matias and Merry Mou . 2018 . CivilServant : Community - Led Experiments in Platform Governance . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , New York , NY , USA , 9 : 1 – 9 : 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3173583 [ 74 ] Ben Medeiros . 2019 . Picketing the Virtual Storefront : Content Moderation and Political Criticism of Businesses on Yelp . International Journal of Communication 13 , 0 ( Sept . 2019 ) , 17 . https : / / ijoc . org / index . php / ijoc / article / view / 10451 Number : 0 . [ 75 ] Edward Newell , David Jurgens , Haji Mohammad Saleem , Hardik Vala , Jad Sassine , Caitrin Armstrong , and Derek Ruths . 2016 . User Migration in Online Social Networks : A Case Study on Reddit During a Period of Community Unrest . In Tenth International AAAI Conference on Web and Social Media . https : / / www . aaai . org / ocs / index . php / ICWSM / ICWSM16 / paper / view / 13137 [ 76 ] Casey Newton . 2019 . The secret lives of Facebook moderators in America . https : / / www . theverge . com / 2019 / 2 / 25 / 18229714 / cognizant - facebook - content - moderator - interviews - trauma - working - conditions - arizona [ 77 ] Chloe Nurik . 2019 . “Men Are Scum” : Self - Regulation , Hate Speech , and Gender - Based Censorship on Facebook . International Journal of Communication 13 , 0 ( June 2019 ) , 21 . https : / / ijoc . org / index . php / ijoc / article / view / 9608 Number : 0 . [ 78 ] Jonathan A . Obar and Anne Oeldorf - Hirsch . 2020 . The biggest lie on the Internet : ignoring the privacy policies and terms of service policies of social networking services . Information , Communication & Soci - ety 23 , 1 ( Jan . 2020 ) , 128 – 147 . https : / / doi . org / 10 . 1080 / 1369118X . 2018 . 1486870 Publisher : Routledge _ eprint : https : / / doi . org / 10 . 1080 / 1369118X . 2018 . 1486870 . [ 79 ] Deokgun Park , Simranjit Sachar , Nicholas Diakopoulos , and Niklas Elmqvist . 2016 . Supporting Comment Moderators in Identifying High Quality Online News Comments . In Proceedings of the 2016 CHI Conference on Human Factors ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 27 in Computing Systems ( CHI ’16 ) . Association for Computing Machinery , San Jose , California , USA , 1114 – 1125 . https : / / doi . org / 10 . 1145 / 2858036 . 2858389 [ 80 ] Umashanthi Pavalanathan , Xiaochuang Han , and Jacob Eisenstein . 2018 . Mind Your POV : Convergence of Articles and Editors Towards Wikipedia’s Neutrality Norm . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( Nov . 2018 ) , 137 : 1 – 137 : 23 . https : / / doi . org / 10 . 1145 / 3274406 [ 81 ] Anthony J . Pellicone and June Ahn . 2017 . The Game of Performing Play : Understanding Streaming as Cultural Production . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( CHI ’17 ) . Association for Computing Machinery , Denver , Colorado , USA , 4863 – 4874 . https : / / doi . org / 10 . 1145 / 3025453 . 3025854 [ 82 ] Gregor Petrič and Andraž Petrovčič . 2014 . Elements of the management of norms and their effects on the sense of virtual community . Online Information Review 38 , 3 ( Jan . 2014 ) , 436 – 454 . https : / / doi . org / 10 . 1108 / OIR - 04 - 2013 - 0083 Publisher : Emerald Group Publishing Limited . [ 83 ] Shruti Phadke and Tanushree Mitra . 2020 . Many Faced Hate : A Cross Platform Study of Content Framing and Information Sharing by Online Hate Groups . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , Honolulu , HI , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376456 [ 84 ] Liza Potts , Rebekah Small , and Michael Trice . 2019 . Boycotting the Knowledge Makers : How Reddit Demonstrates the RiseofMediaBlacklistsandSourceRejectioninOnlineCommunities . IEEETransactionsonProfessionalCommunication 62 , 4 ( Dec . 2019 ) , 351 – 363 . https : / / doi . org / 10 . 1109 / TPC . 2019 . 2946942 Conference Name : IEEE Transactions on Professional Communication . [ 85 ] Ondřej Procházka . 2019 . Making Sense of Facebook’s Content Moderation : A Posthumanist Perspective on Commu - nicative Competence and Internet Memes . Signs and Society 7 , 3 ( Sept . 2019 ) , 362 – 397 . https : / / doi . org / 10 . 1086 / 704763 Publisher : The University of Chicago Press . [ 86 ] NicholasProferes , NaiyanJones , SarahGilbert , CaseyFiesler , andMichaelZimmer . 2021 . StudyingReddit : ASystematic Overview of Disciplines , Approaches , Methods , and Ethics . Social Media + Society 7 , 2 ( 2021 ) , 20563051211019004 . https : / / doi . org / 10 . 1177 / 20563051211019004 [ 87 ] Ashwin Rajadesingan , Paul Resnick , and Ceren Budak . 2020 . Quick , Community - Specific Learning : How Distinctive Toxicity Norms Are Maintained in Political Subreddits . Proceedings of the International AAAI Conference on Web and Social Media 14 ( May 2020 ) , 557 – 568 . https : / / aaai . org / ojs / index . php / ICWSM / article / view / 7323 [ 88 ] Elissa M . Redmiles , Jessica Bodford , and Lindsay Blackwell . 2019 . “I Just Want to Feel Safe” : A Diary Study of Safety Perceptions on Social Media . Proceedings of the International AAAI Conference on Web and Social Media 13 ( July 2019 ) , 405 – 416 . https : / / www . aaai . org / ojs / index . php / ICWSM / article / view / 3356 [ 89 ] Reid . 1999 . Communities in Cyberspace . Routledge , London & amp ; New York . Pages : 107 - 133 . [ 90 ] Martin J . Riedl , Gina M . Masullo , and Kelsey N . Whipple . 2020 . The downsides of digital labor : Exploring the toll incivility takes on online comment moderators . Computers in Human Behavior 107 ( June 2020 ) , 106262 . https : / / doi . org / 10 . 1016 / j . chb . 2020 . 106262 [ 91 ] Minna Ruckenstein and Linda Lisa Maria Turunen . 2020 . Re - humanizing the platform : Content moderators and the logic of care . New Media & Society 22 , 6 ( June 2020 ) , 1026 – 1042 . https : / / doi . org / 10 . 1177 / 1461444819875990 Publisher : SAGE Publications . [ 92 ] Chandan Sarkar , Donghee Wohn , Cliff Lampe , and Kurt DeMaagd . 2012 . A quantitative explanation of governance in an online peer - production community . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . Association for Computing Machinery , Austin , Texas , USA , 2939 – 2942 . https : / / doi . org / 10 . 1145 / 2207676 . 2208701 [ 93 ] Sarita Schoenebeck , Oliver L Haimson , and Lisa Nakamura . 2020 . Drawing from justice theories to support targets of online harassment . New Media & Society ( March 2020 ) , 1461444820913122 . https : / / doi . org / 10 . 1177 / 1461444820913122 Publisher : SAGE Publications . [ 94 ] Joseph Seering , Geoff Kaufman , and Stevie Chancellor . 2020 . Metaphors in moderation . New Media & Society ( Oct . 2020 ) , 1461444820964968 . https : / / doi . org / 10 . 1177 / 1461444820964968 Publisher : SAGE Publications . [ 95 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping pro and anti - social behavior on twitch through moderation and example - setting . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW ’17 ) . ACM , New York , NY , USA , 111 – 125 . https : / / doi . org / 10 . 1145 / 2998181 . 2998277 [ 96 ] Joseph Seering , Felicia Ng , Zheng Yao , and Geoff Kaufman . 2018 . Applications of Social Identity Theory to Research and Design in Computer - Supported Cooperative Work . Proc . ACM Hum . - Comput . Interact . 2 , CSCW ( Nov . 2018 ) , 201 : 1 – 201 : 34 . https : / / doi . org / 10 . 1145 / 3274771 [ 97 ] Joseph Seering , Tony Wang , Jina Yoon , and Geoff Kaufman . 2019 . Moderator engagement and community devel - opment in the age of algorithms . New Media & Society ( Jan . 2019 ) , 1461444818821316 . https : / / doi . org / 10 . 1177 / 1461444818821316 ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 28 J . A . Jiang et al . [ 98 ] Qinlan Shen and Carolyn Rose . 2019 . The Discourse of Online Content Moderation : Investigating Polarized User Responses to Changes in Reddit’s Quarantine Policy . In Proceedings of the Third Workshop on Abusive Language Online . Association for Computational Linguistics , Florence , Italy , 58 – 69 . https : / / doi . org / 10 . 18653 / v1 / W19 - 3507 [ 99 ] Jonathan Shieber . 2020 . Zuckerberg explains why Facebook won’t take action on Trump’s recent posts . https : / / social . techcrunch . com / 2020 / 05 / 29 / zuckerberg - explains - why - facebook - wont - take - action - on - trumps - recent - posts / [ 100 ] Tanner Skousen , Hani Safadi , Colleen Young , Elena Karahanna , Sami Safadi , and Fouad Chebib . 2020 . Successful Moderation in Online Patient Communities : Inductive Case Study . Journal of Medical Internet Research 22 , 3 ( 2020 ) , e15983 . https : / / doi . org / 10 . 2196 / 15983 Company : Journal of Medical Internet Research Distributor : Journal of Medical Internet Research Institution : Journal of Medical Internet Research Label : Journal of Medical Internet Research Publisher : JMIR Publications Inc . , Toronto , Canada . [ 101 ] Tim Squirrell . 2019 . Platform dialectics : The relationships between volunteer moderators and end users on reddit . New Media & Society 21 , 9 ( Sept . 2019 ) , 1910 – 1927 . https : / / doi . org / 10 . 1177 / 1461444819834317 Publisher : SAGE Publications . [ 102 ] Kumar Bhargav Srinivasan , Cristian Danescu - Niculescu - Mizil , Lillian Lee , and Chenhao Tan . 2019 . Content Removal as a Moderation Strategy : Compliance and Other Outcomes in the ChangeMyView Community . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( Nov . 2019 ) , 163 : 1 – 163 : 21 . https : / / doi . org / 10 . 1145 / 3359265 [ 103 ] Nicolas P . Suzor , Sarah Myers West , Andrew Quodling , and Jillian York . 2019 . What Do We Mean When We Talk About Transparency ? Toward Meaningful Transparency in Commercial Content Moderation . International Journal of Communication 13 , 0 ( March 2019 ) , 18 . https : / / ijoc . org / index . php / ijoc / article / view / 9736 Number : 0 . [ 104 ] Tom Tyler , Matt Katsaros , Tracey Meares , and Sudhir Venkatesh . 2019 . Social media governance : can social media companies motivate voluntary rule following behavior among their users ? Journal of Experimental Criminology ( Dec . 2019 ) . https : / / doi . org / 10 . 1007 / s11292 - 019 - 09392 - z [ 105 ] Kristen Vaccaro , Christian Sandvig , and Karrie Karahalios . 2020 . " At the End of the Day Facebook Does What ItWants " : How Users Experience Contesting Algorithmic Content Moderation . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW2 ( Oct . 2020 ) , 167 : 1 – 167 : 22 . https : / / doi . org / 10 . 1145 / 3415238 [ 106 ] Aditya Vashistha , Edward Cutrell , Gaetano Borriello , and William Thies . 2015 . Sangeet Swara : A Community - Moderated Voice Forum in Rural India . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems ( CHI ’15 ) . Association for Computing Machinery , New York , NY , USA , 417 – 426 . https : / / doi . org / 10 . 1145 / 2702123 . 2702191 [ 107 ] GangWang , BolunWang , TianyiWang , AnaNika , HaitaoZheng , andBenY . Zhao . 2014 . Whispersinthedark : analysis of an anonymous social network . In Proceedings of the 2014 Conference on Internet Measurement Conference ( IMC ’14 ) . Association for Computing Machinery , Vancouver , BC , Canada , 137 – 150 . https : / / doi . org / 10 . 1145 / 2663716 . 2663728 [ 108 ] Sarah Myers West . 2018 . Censored , suspended , shadowbanned : User interpretations of content moderation on social media platforms . New Media & Society 20 , 11 ( Nov . 2018 ) , 4366 – 4383 . https : / / doi . org / 10 . 1177 / 1461444818773059 Publisher : SAGE Publications . [ 109 ] Alice Elizabeth Amelia Witt , Nicolas Suzor , and Anna Huggins . 2019 . The rule of law on Instagram : An evaluation of the moderation of images depicting women’s bodies . The University of New South Wales law journal 42 , 2 ( 2019 ) , 557 – 596 . http : / / www . unswlawjournal . unsw . edu . au / article / the - rule - of - law - on - instagram - an - evaluation - of - the - moderation - of - images - depicting - womens - bodies / Number : 2 Publisher : Law School , University of New South Wales . [ 110 ] Donghee Yvette Wohn . 2019 . Volunteer Moderators in Twitch Micro Communities : How They Get Involved , the Roles They Play , and the Emotional Labor They Experience . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . Association for Computing Machinery , Glasgow , Scotland Uk , 1 – 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300390 [ 111 ] Diyi Yang , Robert E . Kraut , Tenbroeck Smith , Elijah Mayfield , and Dan Jurafsky . 2019 . Seekers , Providers , Welcomers , and Storytellers : Modeling Social Roles in Online Health Communities . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3290605 . 3300574 [ 112 ] Jing Zeng , Chung - hong Chan , and King - wa Fu . 2017 . How Social Media Construct “Truth” Around Crisis Events : Weibo’s Rumor Management Strategies After the 2015 Tianjin Blasts . Policy & Internet 9 , 3 ( 2017 ) , 297 – 320 . https : / / doi . org / 10 . 1002 / poi3 . 155 _ eprint : https : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / poi3 . 155 . ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 29 A Papers Included in the Systematic Literature Review Table 1 . List of papers included in our systematic literature review . Paper Qualitative Quantitative VolunteerModeration CommercialModeration ( Seering et al . , 2020 ) [ 94 ] • • ( Vaccaro et al . , 2020 ) [ 105 ] • • • ( Rajadesingan et al . 2020 ) [ 87 ] • • ( Feuston et al . , 2020 ) [ 28 ] • • • ( Fan & Zhang , 2020 ) [ 27 ] • • ( Juneja et al . , 2020 ) [ 56 ] • • • ( Hua et al . , 2020 ) [ 47 ] • • • ( Luo et al . , 2020 ) [ 70 ] • • • ( Phadke & Mitra , 2020 ) [ 83 ] • • • ( Gilbert , 2020 ) [ 35 ] • • ( Obar & Oeldorf - Hirsch , 2020 ) [ 78 ] • • ( Riedl et al . , 2020 ) [ 90 ] • • ( Einwiller & Kim , 2020 ) [ 25 ] • • • ( Banchik , 2020 ) [ 4 ] • • ( Schoenebeck et al . , 2020 ) [ 93 ] • • ( Skousen et al . , 2020 ) [ 100 ] • • • ( Gray & Suzor , 2020 ) [ 39 ] • • • ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 30 J . A . Jiang et al . ( Datta & Adar , 2019 ) [ 19 ] • • ( Grover & Mark , 2019 ) [ 42 ] • • • ( S . Jiang et al . , 2019 ) [ 55 ] • • ( Redmiles et al . , 2019 ) [ 88 ] • • • ( Karunakaran & Ramakrishan , 2019 ) [ 58 ] • • • ( Kiene et al . , 2019 ) [ 61 ] • • ( Blackwell et al . , 2019 ) [ 7 ] • • • ( Jhaver , Bruckman , et al . , 2019 ) [ 50 ] • • ( Jhaver , Birman , et al . , 2019 ) [ 49 ] • • ( Jhaver , Appling , et al . , 2019 ) [ 48 ] • • • ( Srinivasan et al . , 2019 ) [ 102 ] • • ( J . A . Jiang et al . , 2019 ) [ 53 ] • • ( Chandrasekharanetal . , 2019 ) [ 15 ] • • • ( Dosono et al . , 2019 ) [ 21 ] • • ( Wohn , 2019 ) [ 110 ] • • ( Fiesler & Bruckman , 2019 ) [ 29 ] • • ( Gibson , 2019 ) [ 34 ] • • ( Tyler et al . , 2019 ) [ 104 ] • • ( Potts et al . , 2019 ) [ 84 ] • • ( Procházka , 2019 ) [ 85 ] • • ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 31 ( Squirrell , 2019 ) [ 101 ] • • ( Seering et al . , 2019 ) [ 97 ] • • ( Witt et al . , 2019 ) [ 109 ] • • ( Matias , 2019 ) [ 72 ] • • • ( Juneström , 2019 ) [ 57 ] • • • ( Shen & Rose , 2019 ) [ 98 ] • • ( Medeiros , 2019 ) [ 74 ] • • ( Draper , 2019 ) [ 22 ] • • ( Suzor et al . , 2019 ) [ 103 ] • • ( Nurik , 2019 ) [ 77 ] • • ( Duguay et al . , 2018 ) [ 23 ] • • ( Fiesler et al . , 2018 ) [ 30 ] • • • • ( Blackwell et al . , 2018 ) [ 5 ] • • • ( Jhaver et al . , 2018 ) [ 51 ] • • • ( Matias & Mou , 2018 ) [ 73 ] • • ( Chancellor et al . , 2018 ) [ 12 ] • • ( Pavalanathan et al . , 2018 ) [ 80 ] • • ( Chandrasekharanetal . , 2018 ) [ 17 ] • • • ( Gerrard , 2018 ) [ 32 ] • • ( West , 2018 ) [ 108 ] • • • ( Keegan & Fiesler , 2017 ) [ 60 ] • • ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 32 J . A . Jiang et al . ( Chancellor et al . , 2017 ) [ 13 ] • • ( Pellicone & Ahn , 2017 ) [ 81 ] • • ( Blackwell et al . , 2017 ) [ 6 ] • • ( Chandrasekharanetal . , 2017 ) [ 16 ] • • ( Seering et al . , 2017 ) [ 95 ] • • ( Zeng et al . , 2017 ) [ 112 ] • • ( Cheng et al . , 2017 ) [ 18 ] • • ( Newell et al . , 2016 ) [ 75 ] • • • ( Chancellor , Pater , et al . , 2016 ) [ 14 ] • • ( Park et al . , 2016 ) [ 79 ] • • ( Centivany & Glushko , 2016 ) [ 10 ] • • ( Matias , 2016 ) [ 71 ] • • • ( Gallagher & Savage , 2016 ) [ 31 ] • • ( Getto & Labriola , 2016 ) [ 33 ] • • ( Kiene et al . , 2016 ) [ 62 ] • • ( Ehrett , 2016 ) [ 24 ] • • • • ( Vashistha et al . , 2015 ) [ 106 ] • • • ( Wang et al . , 2014 ) [ 107 ] • • ( Petrič & Petrovčič , 2014 ) [ 82 ] • • • ( Lampe et al . , 2014 ) [ 67 ] • • • ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . A Trade - off - centered Framework of Content Moderation 1 : 33 ( Kayhan & Bhattacherjee , 2013 ) [ 59 ] • • ( Heinze et al . , 2013 ) [ 44 ] • • • ( Sarkar et al . , 2012 ) [ 92 ] • • ( Holmes & Cox , 2011 ) [ 46 ] • • • ( Liao et al . , 2010 ) [ 68 ] • • • • ( Gurzick et al . , 2009 ) [ 43 ] • • ( Lampe & Johnston , 2005 ) [ 65 ] • • • ( Lampe & Resnick , 2004 ) [ 66 ] • • • • ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 . 1 : 34 J . A . Jiang et al . B Components of Trade - offs and Relevant Papers Table 2 . Components of trade - offs and relevant papers . Trade - off Papers Moderation Actions Exclusion [ 7 , 10 , 12 – 19 , 21 – 25 , 27 , 28 , 30 , 32 , 34 , 35 , 43 , 47 , 49 – 51 , 53 , 56 , 61 , 62 , 68 , 71 – 73 , 75 , 77 , 79 – 81 , 83 – 85 , 93 – 95 , 97 , 98 , 100 – 103 , 105 , 108 – 110 ] Organizing [ 4 , 6 , 7 , 12 – 19 , 21 – 23 , 25 , 27 – 30 , 32 , 34 , 35 , 39 , 43 , 44 , 46 , 48 – 51 , 53 , 56 , 57 , 60 – 62 , 68 , 71 – 74 , 77 , 79 , 80 , 82 , 84 , 85 , 87 , 88 , 90 , 92 – 95 , 97 , 98 , 100 – 110 , 112 ] Norm setting [ 10 , 17 , 19 , 22 – 25 , 27 , 28 , 34 , 35 , 42 , 48 , 51 , 53 , 61 , 75 , 77 , 82 , 83 , 94 , 95 , 97 , 101 , 102 , 105 , 110 ] Pricing [ 105 ] Moderation Styles Human vs . Automated [ 12 , 13 , 15 , 39 , 49 – 51 , 85 , 94 , 97 , 105 ] Centralized vs . Distributed [ 17 , 22 – 24 , 27 , 35 , 59 , 60 , 66 , 67 , 79 , 97 ] Transparent vs . Opaque [ 14 , 27 , 35 , 49 – 51 , 55 , 56 , 82 , 87 , 92 , 94 , 97 , 103 – 105 , 109 ] Moderation Philosophies Nurturing vs . Punishing [ 25 , 50 , 94 , 97 , 100 , 105 , 108 ] Level of Activity vs . Quality of Contributions [ 50 , 60 , 62 , 100 , 102 , 105 ] Efficiency vs . Quality of Moderation [ 27 , 35 , 49 , 66 , 94 , 97 , 101 , 105 , 107 ] Moderation Values Moderator Identities [ 43 , 46 , 51 , 60 , 72 , 94 , 97 , 110 ] Community Identities [ 28 , 29 , 34 , 35 , 42 , 60 , 74 , 83 , 94 , 97 , 101 ] Competing Stakeholders [ 4 , 6 , 10 , 24 , 25 , 31 , 47 , 58 , 70 – 72 , 90 , 93 , 94 , 98 , 105 , 112 ] ACM Trans . Comput . - Hum . Interact . , Vol . TBD , No . TBD , Article 1 . Publication date : June 2022 .