6 / 26 / 17 1 Unsupervised Learning Presentor : YevgenyShapiro What is supervised learning ? Labels What is unsupervised learning ? Labels What is unsupervised learning ? • Looking for inner structure 6 / 26 / 17 2 • Tons of data , very few labels • Can improve the starting point of supervised models • Learn features without a specific task for semi - supervised learning • Self supervision • Closer to how the human brain works Why use unsupervised learning ? Classic models Clustering Autoencoders RBMs Self - supervision Unsupervised Learning in NLP • Lots of data • Highly Structured • Highly sparse בלכה היה בוחרב הובגה דומעה ירוחאמ רתתסהש ןבל NLP tasks • Lemmatization • Part - of - speech tagging • Translation • Grammar correction • Language models – The cat is [ ? ] • P ( White | The cat is ) = 0 . 1 • P ( while | The cat is ) = 0 Dogs à dog Drove à Drive 6 / 26 / 17 3 Efficient Estimation of Word Representations in Vector Space Tomas Mikolov , Kai Chen , Greg Corrado , Jeffrey Dean Why learn vector representations ? • Traditionally , words are represented as labels • The [ id443 ] cat [ id23 ] is [ id554 ] white [ id444 ] • This encoding is arbitrary • “Dog” is similar to “Dogs” • Knowledge transfer between similar words What kind of similarity ? • Semantic regularities • Dog , Cat • Israel , Jerusalem • Mother , Father • Syntactic regularities • work , works • Run , Running • Fast , Faster Previous methods • One hot • Very sparse • No knowledge transfer between similar words • LSA – Latent semantic analysis • Uses SVD for dimensionality reduction 6 / 26 / 17 4 Previous methods • LDA – Latent Dirichlet allocations • Bayesian topic model • Very expensive to compute NNLM – neural network language model • N previous words with 1 - of - V coding • First layer - linear embedding • Second layer – hidden layer • Output layer - Softmax Problem - Computational complexity • Simple models trained with lots of data are better than complex models trained with less data • Solution : Try and reduce the model complexity ! • How ? • Start by analyzing previous neural net models Computational complexity - NNLM • N previous words with 1 - of - V coding • First layer - linear embedding • Second layer – hidden layer • Output layer - Softmax • Total complexity • N – # previous words • D – size of input layer • H – size of hidden layer • V – Vocabulary size Hierarchical Softmax Bottleneck Huffman coding 6 / 26 / 17 5 • Problem : very sparse vocabulary • Softmax • Solution : Approximate standard softmax • Hierarchical Softmax Hierarchical Softmax Computational complexity - RNNM • No limit of previous words • One hidden layer • Total complexity • Word dimensionality = H • H – size of hidden layer • V – Vocabulary size Hierarchical Softmax Bottleneck Conclusions • Most of the complexity is caused by the non - linear hidden unit • To reduce complexity – remove the hidden unit ! First Model – Continuous Bag - of - Words • Similar to NNLM • Projection is shared • Total complexity • N – # input words • D – size of input layer • V – Vocabulary size NNLM : 6 / 26 / 17 6 Second Model – Continuous Skip - gram • Total complexity • C – maximum distance of the words • D – size of input layer • V – Vocabulary size • Sampling close words more Training • Adagrad – well suited for sparse data • Small learning rate for common parameters / words • Large learning rate for rare parameters / words Results • Different kinds of similarity • Computing pairs • King – Man + Woman = Queen Results 6 / 26 / 17 7 Results Context Encoders : Feature Learning by Inpainting Deepak Pathak Philipp Krahenbuhl Jeff Donahue Trevor Darrell Alexei A . Efros Motivation • The visual data is structure , but in a more complex way than text . • Humans can easily understand the structure . Context encoder • Autoencoder – might cause ordinary compression • Denoising autoencoder – learn to differ between signal and noise , nothing semantic 6 / 26 / 17 8 Previous works • Unsupervised Visual Representation Learning by Context Prediction – discriminative model • Doersch , Carl , Abhinav Gupta , and Alexei A . Efros Previous works • Unsupervised Learning of Visual Representations using Videos Xiaolong Wang , AbhinavGupta Robotics Institute , Carnegie Mellon University Previous works • Scene Completion Using Millions of Photographs • Hays , James , and Alexei A . Efros System Architecture • 1 . Encoder – similar to Alexnet , trained from scratch • Input : 227 x 227 , output : 6 x 6 x 256 ( pool5 of alexnet ) • 2 . Connection – fully connected layer • Doesn’t connect different features for computational reasons • Stride 1 convolution is done later , to connected different channels • 3 . Decoder – 5 up - convolutions • Output – original target size 6 / 26 / 17 9 Loss Function • Reconstruction Loss • - A binary mask corresponding to dropped regions • F ( X ) – encoded image Loss Function • Adversarial loss • To learn a generative model G ( = F in our case ) • We learn a discriminative model D – Log likelihood for real samples • D = 1 for real samples , and D = 0 for generated samples • Total loss function Region masks • Central block • Works well for inpainting • Learns low level boundary features which don’t generalize well • Random block • Random rectangle blocks are removed • Up to ¼ of the image • The masks still have sharp boundaries which CNN features latch on • Random region • Random masks from PASCAL VOC 2012 , deformed • Up to ¼ of the image Evaluation – Semantic Inpainting • No noise in the adversarial setting • The discriminator is not conditioned on the context • No pooling layers 6 / 26 / 17 10 Evaluation – Semantic Inpainting Evaluation – Semantic Inpainting Evaluation – Semantic Inpainting The Problem with PSNR 6 / 26 / 17 11 Evaluation – Feature learning • Adversarial loss did not converge with Alexnet • Nearest neighbor of the missing region using context features • Alexnet was trained as a supervised classification problem Evaluation • Detection : Fast – RCNN framework • Segmentation : FCNN Thank you ! RNN and LSTM 6 / 26 / 17 12 Character Level Model