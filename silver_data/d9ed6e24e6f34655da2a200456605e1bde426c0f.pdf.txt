The effect of Target Normalization and Momentum on Dying ReLU Isac Arnekvist Division of Robotics , Perception and Learning Royal Institute of Technology ( KTH ) Stockholm isacar @ kth . se J . Frederico Carvalho Division of Robotics , Perception and Learning Royal Institute of Technology ( KTH ) Stockholm jfpbdc @ kth . se Danica Kragic Division of Robotics , Perception and Learning Royal Institute of Technology ( KTH ) Stockholm dani @ kth . se Johannes A . Stork Center for Applied Autonomous Sensor Systems Örebro University Örebro johannesandreas . stork @ oru . se Abstract Optimizing parameters with momentum , normalizing data values , and using recti - ﬁed linear units ( ReLUs ) are popular choices in neural network ( NN ) regression . Although ReLUs are popular , they can collapse to a constant function and “die” , effectively removing their contribution from the model . While some mitigations are known , the underlying reasons of ReLUs dying during optimization are currently poorly understood . In this paper , we consider the effects of target normalization and momentum on dying ReLUs . We ﬁnd empirically that unit variance targets are well motivated and that ReLUs die more easily , when target variance approaches zero . To further investigate this matter , we analyze a discrete - time linear autonomous system , and show theoretically how this relates to a model with a single ReLU and how common properties can result in dying ReLU . We also analyze the gradients of a single - ReLU model to identify saddle points and regions corresponding to dying ReLU and how parameters evolve into these regions when momentum is used . Finally , we show empirically that this problem persist , and is aggravated , for deeper models including residual networks . 1 Introduction Gradient - based optimization enables learning of powerful deep NN models [ 1 , 2 ] . However , most learning algorithms remain sensitive to learning rate , scale of data values , and the choice of activation function—making deep NN models hard to train [ 3 , 4 ] . Stochastic gradient descent with momentum [ 5 , 6 ] , normalizing data values to have zero mean and unit variance [ 7 ] , and employing rectiﬁed linear units ( ReLUs ) in NNs [ 8 , 9 , 10 ] have emerged as an empirically motivated and popular practice . In this paper , we analyze a speciﬁc failure case of this practice , referred to as “dying” ReLU . Preprint . Under review . a r X i v : 2005 . 06195v1 [ c s . L G ] 13 M a y 2020 The ReLU activation function , y = max { x , 0 } is a popular choice of activation function and has been shown to have superior training performance in various domains [ 11 , 12 ] . ReLUs can sometimes be collapse to a constant ( zero ) function for a given set of inputs . Such a ReLU is considered “dead” and does not contribute to a learned model . ReLUs can be initialized dead [ 13 ] or die during optimization , the latter being a major obstacle in training deep NNs [ 14 , 15 ] . Once dead , gradients are zero making recovery possible only if inputs change distribution . Over time , large parts of a NN can end up dead which reduces model capacity . Mitigations to dying ReLU include modifying the ReLU to also activate for negative inputs [ 16 , 17 , 18 ] , training procedures with normalization steps [ 19 , 20 ] , and initialization methods [ 13 ] . While these approaches have some success in practice , the underlying cause for ReLUs dying during optimization is , to our knowledge , still not understood . 10 5 10 3 10 1 10 1 10 3 10 2 10 1 10 0 10 1 10 2 M S E Target variance Airfoil Facebook 0 10 0 10 1 10 2 10 3 Target bias Real Estate Superconduct 10 5 10 3 10 1 10 1 10 3 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 # d e a d / t o t a l Adam 10 5 10 3 10 1 10 1 SGD Airfoil Facebook Real Estate Superconduct Figure 1 : The performance of models , in terms of ﬁt to the training data , degrades as the variance ( γ ) of target values deviate from 1 , or the mean ( δ ) deviates from 0 . Here shown in terms of mean - squared error ( MSE ) on four different datasets [ 21 ] . Furthermore , decreasing γ leads to more dying ReLUs only with the use of momentum optimization [ 6 ] . Details of this experiment can be found in Appendix A . In this paper , we analyze the observation illustrated in Figure 1 that regression performance degrades with smaller target variances , and along with momentum optimization leads to dead ReLU . Although target normalization is a common pre - processing step , we believe a scientiﬁc understanding of why it is important is missing , especially with the connection to momentum optimization . For our theoretical results , we ﬁrst show that an afﬁne approximator trained with gradient descent and momentum corresponds to a discrete - time linear autonomous system . Introducing momentum into this system results in complex eigenvalues and parameters that oscillate . We further show that a single - ReLU model has two cones in parameter space ; one for which the properties of the linear system is shared , and one that corresponds to dead ReLU . We derive analytic gradients for the single - ReLU model to further gain insight and to identify critical points ( i . e . global optima and saddle points ) in parameter space . By inspection of numerical examples , we also identify regions where ReLUs tend to converge to the global optimum ( without dying ) and how these regions change with momentum . Lastly , we show empirically that the problem of dying ReLU is aggravated in deeper models , including residual neural networks . 2 Related work In a recent paper [ 13 ] , the authors identify dying ReLUs as a cause of vanishing gradients . This is a fundamental problem in NNs [ 22 , 23 ] . In general , this can be caused by ReLUs being initialized dead or dying during optimization . Theoretical results about initialization and dead ReLU NNs are presented by Lu et al . [ 13 ] . Growing NN depth towards inﬁnity and initializing parameters from symmetric distributions both lead to dead models . However , asymmetric initialization can effectively prevent this outcome . Empirical results about ReLUs dying during optimization are presented by Wu et al . [ 24 ] . Similar to us , they observe a relationship between dying ReLUs and the scale of target values . In contrast to us , they do not investigate the underlying cause . Normalization of layer input values . The effects of input value distribution has been studied for a long time , e . g . [ 25 ] . Inputs with zero mean have been shown to result in gradients that more closely resemble the natural gradient , which speeds up training [ 26 ] . In fact , a range of strategies to normalize layer input data exists [ 20 , 19 , 27 ] along with theoretical analysis of the problem [ 28 ] . Another studied area for maintaining statistics throughout the NN is initialization of the parameters 2 [ 29 , 18 , 13 ] . However , subsequent optimization steps may change the parameters such that the desired input mean and variance no longer is fulﬁlled . Normalization of target values . When the training data are available before optimization , target normalization is trivially executed . More challenging is the case where training data are accessed incrementally , e . g . as in reinforcement learning or for very large training data . Here , normalization and scaling of target values are important for the learning outcome [ 30 , 31 , 24 ] . For on - line regression and reinforcement learning , adaptive target normalization improves results and removes the need of gradient clipping [ 30 ] . In reinforcement learning , scaling rewards by a positive constant is crucial for learning performance , and is often equivalent to the scaling of target values [ 31 ] . Small reward scales have been seen to increase the risk of dying ReLUs [ 24 ] . All of these works motivate the use of target normalization empirically and a theoretical understanding is still lacking . In this paper , we provide more insight into the relationship between dying ReLUs and target normalization . 3 Preliminaries We consider regression of a target function y : R L → R from training data D = { ( x ( i ) , y ( i ) ∗ ) } i consisting of pairs of inputs x ( i ) ∈ R L and target values y ( i ) ∗ = y ( x ( i ) ) ∈ R . We analyze different regression models ˆ y , such as an afﬁne transformation in Sec . 4 and a ReLU - activated model in Sec . 5 , which are both parameterized by a vector θ . Below , we provide deﬁnitions , notations , and equalities needed for our analysis . Target normalization . Before regression , we transform target values y ( i ) ∗ according to y ( i ) = γ y ( i ) ∗ − ˆ µ ˆ σ + δ , ( 1 ) where ˆ µ and ˆ σ are mean and standard deviation of the target values from the training data . When the parameters of the transform are set to scale γ = 1 and bias δ = 0 , new target values y ( i ) correspond to z - normalization [ 32 ] with zero mean and unit variance . In our analysis , we are interested in the effects of γ changing from 1 to smaller values closer to 0 . Target function . In Sec . 4 we study regression of target functions of the form y ( x ) = Γ (cid:62) x + ∆ ( 2 ) where Γ ∈ R L × 1 and ∆ ∈ R . Similar to Douglas and Yu [ 33 ] , we consider the case where inputs in D are distributed as x ∼ N ( 0 ; I ) . For any Γ , we can ﬁnd a unitary matrix O ∈ R L × L such that OΓ = ( 0 , . . . , 0 , γ ) (cid:62) and γ = (cid:107) Γ (cid:107) . From this follow the equalities Γ (cid:62) x + ∆ = Γ (cid:62) O (cid:62) Ox + ∆ = ( 0 , . . . , γ ) Ox + ∆ . ( 3 ) Since x and Ox are identically distributed due to our assumption on x , we can equivalently study the target function y ( x ) = ( 0 , . . . , 0 , γ ) x + ∆ ( 4 ) and assume that Γ = ( 0 , . . . , 0 , γ ) (cid:62) for the remainder of this paper . For Sec . 5 we consider a ReLU - activated target function y f ( x ) = f ( Γ (cid:62) x + ∆ ) ( 5 ) where f is the ReLU activation function f ( x ) = max { x , 0 } . Regression and Objective . We consider gradient descent ( GD ) optimization with momentum for the parameters θ . The update from step t to t + 1 is given as m t + 1 = β m t + ( 1 − β ) ∇ θ t L ( θ t ) ( 6 ) for the momentum variable m and θ t + 1 = θ t − η m t + 1 ( 7 ) for the parameters θ , where L is the loss function , β ∈ [ 0 , 1 ) is the rate of momentum , and η ∈ ( 0 , 1 ) is the step size . 3 Regressions Models and Parameterization . In Sec . 4 we model the respective target function with an afﬁne transform ˆ y ( x ) = w (cid:62) x + b , ( 8 ) and in Sec . 5 we consider the nonlinear ReLU model ˆ y f ( x ) = f ( w (cid:62) x + b ) . ( 9 ) In both cases , the parameters θ = ( w (cid:62) , b ) (cid:62) ∈ R L + 1 are weights w ∈ R L and bias b ∈ R . We optimize the mean squared error ( MSE ) , such that L ( θ ) = E x (cid:20) 1 2 (cid:15) ( x ) 2 (cid:21) . ( 10 ) where (cid:15) is the signed error given by (cid:15) ( x ) = ˆ y ( x ) − y ( x ) for the afﬁne target and (cid:15) f ( x ) = ˆ y f ( x ) − y f ( x ) for the ReLU - activated target . To make gradient calculation easier , and interpretable , we will approximate the gradients for the ReLU - model by replacing y f with y . This is still a reasonable approximation , since for any choice of x we have that if y f ( x ) ≥ ˆ y ( x ) ⇒ (cid:15) f ( x ) = ˆ y f ( x ) − y f ( x ) (cid:124) (cid:123)(cid:122) (cid:125) ≥ y ( x ) = ˆ y f ( x ) − y ( x ) = (cid:15) ( x ) ( 11 ) and y f ( x ) < ˆ y f ( x ) ⇒ (cid:15) f ( x ) = ˆ y f ( x ) − y f ( x ) ≥ ˆ y f ( x ) − y ( x ) ≥ 0 . ( 12 ) That is , the error and the gradient is either identical or has the same sign evaluated at any point x and θ . To make calculating expected gradients of L easier , without introducing any further approximations , we deﬁne a unitary matrix U ( w ) ( abbreviated U ) such that the vector w rotated by U is mapped to Uw = ( 0 , 0 , . . . , (cid:107) w (cid:107) ) (cid:62) = ˜ w . ( 13 ) We refer to the rotated vector as ˜ w . Again , if x ∼ N ( 0 ; I ) , the variables x and ˜ x are identically distributed and for w (cid:54) = 0 , we also get from Eq . ( 13 ) Uw (cid:107) w (cid:107) = ( 0 , 0 , . . . , 1 ) (cid:62) . ( 14 ) Dying ReLU . A ReLU is considered dying if all inputs are negative . For inputs with inﬁnite support , we consider the ReLU as dying if outputs are non - zero with probability less than some ( small ) ε P (cid:0) w (cid:62) x + b > 0 (cid:1) < ε . ( 15 ) 4 Regression with afﬁne model In this section , we analyze the regression of the target function y from Eq . ( 4 ) with the afﬁne model ˆ y from Eq . ( 8 ) . From the perspective of the input and output space of this model , it is identical to the ReLU model in Eq . ( 9 ) for all inputs that map to positive values . On the other hand , from the parameter space perspective , we will show that the parameter evolution is identical in certain regions of the parameter space . The global optimum is also the same for both functions . This allows us to re - use some of the following results later . To study the evolution of parameters θ and momentum m during GD optimization , we formulate the GD optimization as an equivalent linear autonomous system [ 34 ] and analyze the behavior by inspection of the eigenvalues . For this analysis , we assume that the inputs are distributed as x ∼ N ( 0 ; I ) in the training data . 4 0 . 0 0 . 5 1 . 0 Re ( ) 0 . 10 0 . 05 0 . 00 0 . 05 0 . 10 I m ( ) 0 25 50 75 100 t 2 1 0 1 b / | w | = 0 . 0 = 0 . 7 = 0 . 9 Figure 2 : Left : Eigenvalues of A as β is increased from 0 ( marked as × ) to 1 . At β ≈ 0 . 7 eigenvalues the become complex and take the value 1 for β = 1 . Right : As the eigenvalues become complex , gradient descent produces oscillations . These are three examples all originating from the same parameter coordinate . Analytic gradients . By inserting ˆ y into Eq . ( 10 ) , the optimization objective can be formulated as L ( θ ) = E x (cid:20) 1 2 ( a (cid:62) x + c ) 2 (cid:21) ( 16 ) using new shorthand deﬁnitions a = w − Γ and c = b − ∆ . Considering that Γ = ( 0 , . . . , 0 , γ ) (cid:62) from Eq . ( 4 ) , the derivatives are ∂ L ∂ w = ∂ L ∂ a = E x (cid:2) ( a (cid:62) x + c ) x (cid:3) = a ( 17 ) for the weights w and ∂ L ∂b = ∂ L ∂c = c = b − ∆ . ( 18 ) for the bias b . From these results , we can see that both a and c are zero when L ( θ ) arrives at a critical point—in this case the global optimum . Parameter evolution . The parameter evolution as given by Eqs . ( 6 ) and ( 7 ) can be for - mulated as a linear autonomous system in terms of a , c , and m . The state s (cid:62) = ( [ m ] 1 , [ a ] 1 , . . . , [ m ] L , [ a ] L , [ m ] L + 1 , c ) (cid:62) consists of stacked pairs of momentum and parameters . We write the update equations Eq . ( 6 ) and Eq . ( 7 ) in the form s t + 1 = (cid:32) A . . . A (cid:33) s t , ( 19 ) where the state s evolves according to the constant matrix A = (cid:18) β 1 − β − ηβ 1 − η ( 1 − β ) (cid:19) , ( 20 ) which determines the evolution of each pair of momentum and parameter independently of other pairs . Since the pairs evolve independently , we can study their convergence by analyzing the eigenvalues of A . Given that η and β are from the ranges deﬁned in Sec . 3 , all eigenvalues are strictly inside the right half of the complex unit circle which guarantees convergence to the ground truth . For step sizes 1 < η < 2 eigenvalues will still be inside the complex unit circle , still guaranteeing convergence , but eigenvalues can be real and negative . This means that parameters will alternate signs every gradient update , denoted “ripples” [ 34 ] . Although this sign - switching can cause dying ReLU in theory , in practice learning rates are usually < 1 . We plot the eigenvalues of A in Figure 2 ( left ) for η = 0 . 1 as β increases from 0 , i . e . GD without momentum , towards 1 . We observe that the eigenvalues eventually become complex ( β ≈ 0 . 7 ) resulting in oscillation [ 34 ] ( seen on the right side ) . The fraction b (cid:107) w (cid:107) , as we will show in Sec . 5 , is a good measure of to what extent the ReLU is dying ( smaller means dying ) , and hence we plot this quantity in particular . Note that the eigenvalues , and thus the behavior is entirely parameterized by the learning rate and momentum , and independent of γ . Thus , we can not adjust η as a function of γ to make the system behave as the case γ = 1 . We now continue by showing how these properties translate to the case of a ReLU activated unit . 5 5 Regression with single ReLU - unit We now want to understand the behavior of regressing to Eq . ( 5 ) with the ReLU model in Eq . ( 9 ) . As discussed in Sec . 3 , we will approximate the gradients by considering the linear target function in Eq . 4 . Although this target can not be fully recovered , the optimal solution is still the same , and gradients share similarities , as previously discussed . Again , we consider the evolution and convergence of parameters θ and momentum m during GD optimization , and assume that the inputs are distributed as x ∼ N ( 0 ; I ) in the training data . Similarity to afﬁne model . The ReLU will output non - zero values for any x that satisﬁes w (cid:62) x + b > 0 . We can equivalently write this condition as w (cid:62) (cid:107) w (cid:107) x > − b (cid:107) w (cid:107) . ( 21 ) We can further simplify the condition from Eq . ( 21 ) using U from Sec . 3 , which lets us consider the constraint solely in the L - th dimension w (cid:62) (cid:107) w (cid:107) U (cid:62) Ux > − b (cid:107) w (cid:107) ⇒ ˜ w (cid:62) (cid:107) w (cid:107) ˜ x > − b (cid:107) w (cid:107) ⇒ [ ˜ x ] L > − b (cid:107) w (cid:107) . ( 22 ) From our assumption about the distribution of training inputs follows that [ ˜ x ] L ∼ N ( 0 , 1 ) . With this result , we can compute the probability of a non - zero output from the ReLU as P ( [ ˜ x ] L > − b (cid:107) w (cid:107) ) = 1 − P (cid:18) [ ˜ x ] L < − b (cid:107) w (cid:107) (cid:19) = Φ (cid:18) b (cid:107) w (cid:107) (cid:19) , ( 23 ) where Φ is the cumulative distribution function ( CDF ) of the standard normal distribution . Using Eq . ( 15 ) , we see that dead ReLU is equivalent of Φ (cid:16) b (cid:107) w (cid:107) (cid:17) < ε . This is equivalent of b (cid:107) w (cid:107) < − Φ − 1 ( ε ) which deﬁnes a “dead” cone in parameter space . We can also formulate a corresponding “linear” cone . In this case we get b (cid:107) w (cid:107) > Φ − 1 ( ε ) which is the same cone mirrored along the b - axis . In the linear cone , because of the similarity to the afﬁne model , we know that parameters evolve as described in Sec . 4 with increased oscillations as momentum is used . We will now investigate the analytical gradients to see how these properties translate into that perspective . Analytic gradients . By inserting ˆ y f into Eq . ( 10 ) , the optimization objective can be formulated as L ( θ ) = E x (cid:20) 1 2 1 w (cid:62) x + b > 0 ( a (cid:62) x + c ) 2 (cid:21) , ( 24 ) where we use the indicator function 1 w (cid:62) x + b > 0 to model the ReLU activation . The derivatives are ∂ L ∂ w = E x (cid:2) 1 w (cid:62) x + b > 0 ( a (cid:62) x + c ) x (cid:3) ( 25 ) for the weights and ∂ L ∂b = E x (cid:2) 1 w (cid:62) x + b > 0 ( a (cid:62) x + c ) (cid:3) ( 26 ) for the bias . As in Sec . 4 , the optimal ﬁt is given by weights w = Γ and bias b = ∆ . These parameters give a = 0 and c = 0 and set the gradients in Eq . ( 25 ) and Eq . ( 26 ) to zero . By changing base using U , we can compute the derivatives , (cid:20) U ∂ L ∂ w (cid:21) i (cid:54) = L = [ ˜ a ] i (cid:54) = L Φ (cid:18) b (cid:107) w (cid:107) (cid:19) ( 27 ) for the weights in dimensions i (cid:54) = L , and (cid:20) U ∂ L ∂ w (cid:21) L = [ ˜ a ] L Φ (cid:18) b (cid:107) w (cid:107) (cid:19) + (cid:18) c − [ ˜ a ] L b (cid:107) w (cid:107) (cid:19) φ (cid:18) b (cid:107) w (cid:107) (cid:19) . ( 28 ) for dimension L , where we used φ as density function of the standard normal distribution . For the bias b we have ∂ L ∂b = [ ˜ a ] L φ (cid:18) b (cid:107) w (cid:107) (cid:19) + c Φ (cid:18) b (cid:107) w (cid:107) (cid:19) . ( 29 ) Full derivations are listed in Appendix B . 6 1 0 1 w L 1 . 0 0 . 5 0 . 0 0 . 5 1 . 0 b = 1 . 0 1 0 1 w L = 0 . 1 1 0 1 w L = 0 . 01 1 0 1 w L = 0 . 001 010 6 10 5 10 4 10 3 10 2 10 1 10 0 g r a d i e n t n o r m Figure 3 : Parameter evolution from GD . The white dot marks the global optimum . Approaching [ w ] L = 0 in the lower half , gradient norm becomes very small , GD gets stuck , and the ReLU dies . Critical points in parameter space . With the gradients from above , we can analyze the possible parameters ( i . e . optima and saddle points ) to which GD can converge when the derivatives are zero . First of all , global optimum is easily veriﬁed to have gradient zero when ˜ a = a = 0 and c = 0 . Saddle points correspond to dead ReLU and occur when b (cid:107) w (cid:107) → −∞ since then Φ ( b (cid:107) w (cid:107) ) = 0 , φ ( b (cid:107) w (cid:107) ) = 0 , and b (cid:107) w (cid:107) φ ( b (cid:107) w (cid:107) ) → 0 . This equals the case that w = 0 for any b < 0 which can be veriﬁed by plugging these values into Eq . ( 24 ) . These saddle points form the center of the dead cone . Note in practice that these limits in practice occur already at , for example b (cid:107) w (cid:107) = ± 4 , since then φ ( ± 4 ) ≈ 10 − 4 . The implication of this is that an entire dead cone can be considered as saddle points in practice , and that parameters will converge on the surface of the cone rather than in its interior . For b > 0 , we instead have b (cid:107) w (cid:107) → ∞ and thus φ ( b (cid:107) w (cid:107) ) = 0 and Φ ( b (cid:107) w (cid:107) ) = 1 and the gradients can be veriﬁed to equal those of the afﬁne model in Sec . 4 , as expected . This veriﬁes that parameter evolution in the linear cone will be approximately identical to the afﬁne model . Simpliﬁcation to enable visualization . To continue our investigation of the parameter evolution and , in particular , focus on how the target variance γ and momentum evolve parameters into the dead cone we will make some simpliﬁcations . For this , we will assume that [ ˜ a ] i (cid:54) = L = 0 which enables us to express gradients without U as ∂ L ∂ [ w ] i (cid:54) = L = 0 . ( 30 ) For the weight [ w ] L we can prove , see Appendix C , that ∂ L ∂ [ w ] L = [ a ] L Φ (cid:18) b (cid:107) w (cid:107) (cid:19) + ρ (cid:18) c − ρ [ a ] L b (cid:107) w (cid:107) (cid:19) φ (cid:18) b (cid:107) w (cid:107) (cid:19) , ( 31 ) where ρ = sign ( [ w ] L ) and [ a ] L = [ w ] L − γ . For the bias b we get ∂ L ∂b = ρ [ a ] L φ (cid:18) b (cid:107) w (cid:107) (cid:19) + c Φ (cid:18) b (cid:107) w (cid:107) (cid:19) . ( 32 ) This means , if all [ ˜ a ] i = 0 for i (cid:54) = L , then only the weight [ w ] L and bias b evolve . We can now plot the vector ﬁelds in these parameters to see how they change w . r . t . γ . Inﬂuence of γ on convergence without momentum . The ﬁrst key take - away when decreasing γ is that the global optimum will be closer to the origin and eventually between the dead and the linear cone . This location of the optimum is particularly sensitive , since in this case the parameters in the linear cone evolve towards the dead cone , and in addition exhibits oscillatory behavior for large β . The color scheme in Figure 3 veriﬁes that , like the probability of non - zero outputs in the dead cone , the gradients also tend to zero there . In the lower right quadrant , we can see an attracting line that is shifted towards the dead cone as γ decreases , eventually ending up inside the dead cone . For this case , when γ = 0 . 001 , we can follow the lines to see that most parameters originating from the right side end up in the dead cone . Parameters originating in and near the linear cone approach the ground truth , and the lower left quadrant evolves ﬁrst towards the linear cone before evolving towards the ground truth . When adding momentum , remember that parameter evolution in and near the linear cone exhibits oscillatory behavior . The hypothesis at this moment is that parameters originating from the linear 7 cone can oscillate over into the dead cone and get stuck there . For the other regions they either evolve as before into the dead cone , or ﬁrst into the linear cone and then into the dead cone by oscillation . We will now evaluate and visualize this . Regions that converge to global optimum . We are interested in distinguishing the regions from which parameters will converge to the global optimum and dead cone respectively . For this , we apply the update rules , Eqs . ( 6 ) and ( 7 ) , with η = 0 . 1 , until updates are smaller ( in norm ) than 10 − 6 . Figure 4 : Parameter evolution from GD without momentum . Global optimum is shown as a white circle . The yellow dots correspond to initial parameter coordinates that converge at the ground truth . The blue dots all converge in the dead cone ( where they stop are depicted as black dots ) . As can be seen , small target variance γ is not alone sufﬁcient to lead to a majority of dying ReLU . Figure 4 shows the results for GD without momentum ( β = 0 ) . We see that the region that converges to the dead cone changes with γ , and eventually switches sign , when γ becomes small . The majority of initializations still converges at the ground truth . Figure 5 : Parameter evolution from GD without momentum . Colors as in Figure 4 . Momentum and small scale γ = 0 . 001 increasingly leads to dying ReLU . The black regions corresponding to converged parameters are growing in the interior of the dead cone . Figure 5 shows the results with momentum . The linear autonomous system in Sec . 4 with η = 0 . 1 has complex eigenvalues for β > 0 . 7 which lead to oscillation . This property approximately translates to the ReLU in the linear cone , where we expect oscillations for β > 0 . 7 . Indeed , we observe the same results as without momentum for β ≤ 0 . 7 , but worse results for larger β . Eventually , only small regions of initializations are able to converge to the global optimum . 6 Deeper architectures In this section we will address two questions . ( 1 ) Does the problem persist in deeper architectures , including residual networks with batch normalization ? ( 2 ) Since the ReLU is a linear operator for positive coefﬁcients , can we simply scale the weight initialization and learning rate and have an equivalent network and learning procedure ? For the latter we will show that it is not possible for deeper architectures . Relevance for models used in practice . We performed regression on the same datasets as in Sec . 1 , with γ = 10 − 4 . We conﬁrm the ﬁndings of Lu et al . [ 13 ] that ReLUs are initialized dead in deeper “vanilla” NNs , but not in residual networks due to the batch normalization layer that precedes the ReLU . Results are shown in Figure 6 . We further ﬁnd that ReLUs die more often , and faster , the deeper the architecture , even for residual networks with batch normalization . We can also conclude 8 that , in these experiments , stochastic gradient descent does not produce more dead ReLU during optimization . 0 20000 40000 60000 80000 Gradient step 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R a t i o o f d e a d R e L U i n l a y e r w i t h h i g h e s t r a t i o Multilayer perceptron 0 20000 40000 60000 80000 Gradient step Residual NN ( with batch norm . ) = 10 4 Depth : 1 , Adam Depth : 2 , Adam Depth : 4 , Adam Depth : 8 , Adam Depth : 1 , SGD Depth : 2 , SGD Depth : 4 , SGD Depth : 8 , SGD Figure 6 : Average results over four datasets showing that dead ReLU is aggravated by deeper architectures , even for residual networks with batch normalization before the ReLU activation . Horizontal axis shows the proportion of dead ReLU in the layer with the highest ratio of dead ReLU . In the multi - layer perceptron case , if one layer has only dead ReLUs , then gradients are zero for all parameters except the last biases , no matter which layer “died” . Parameter re - scaling . For γ > 0 , the ReLU function has the property f ( γ x ) = γf ( x ) and thus γ ˆ y ( x ) = f ( γ w (cid:62) x + γb ) . By rescaling the weight and bias initializations ( not learning rate ) by γ , the parameter trajectories during learning will proportional no matter the choice of γ > 0 . That is , ReLUs will die independent of γ . This is a special case though , since for any architecture ˆ y with one or more hidden layers , it is not possible to multiply parameters by a single scalar ν such that the function is identical to γ ˆ y . Proof is provided in Appendix D . Also , we still have a problem if we do not know γ in advance , which holds for example in the reinforcement learning setting . 7 Conclusion and future work Target normalization is indeed a well - motivated and used practice , although we believe a theoretical understanding of why is both important and lacking . We take a ﬁrst stab at the problem by understand - ing the properties in the smallest moving part of a neural network , a single ReLU - activated afﬁne transformation . Gradient descent on parameters of an afﬁne transformation can be expressed as a discrete - time linear system , for which we have tools to explain the behavior . We provide a geometrical understanding of how these properties translate to the ReLU case , and when it is considered dead . We further illustrated that weight initialization from large regions in parameter space lead to dying ReLUs as target variance is decreased along with momentum optimization , and show that the problem is still relevant , and even aggravated , for deeper architectures . Remaining questions to be answered include how we can extend the analysis for the single ReLU to the full network . Here , the implicit target function of the single ReLU is likely neither of the linear or piecewise linear functions , and inputs are not Gaussian distributed and vary over time . References [ 1 ] Shaveta Dargan , Munish Kumar , Maruthi Rohit Ayyagari , and Gulshan Kumar . A survey of deep learning and its applications : A new paradigm to machine learning . Archives of Computational Methods in Engineering , 2019 . ISSN 1134 - 3060 . [ 2 ] David E Rumelhart , Geoffrey E Hinton , and Ronald J Williams . Learning representations by back - propagating errors . nature , 323 ( 6088 ) : 533 – 536 , 1986 . [ 3 ] Rupesh K Srivastava , Klaus Greff , and Jürgen Schmidhuber . Training very deep networks . In Advances in neural information processing systems , pages 2377 – 2385 , 2015 . [ 4 ] Simon Du , Jason Lee , Haochuan Li , Liwei Wang , and Xiyu Zhai . Gradient descent ﬁnds global minima of deep neural networks . In International Conference on Machine Learning , pages 1675 – 1685 , 2019 . 9 [ 5 ] Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initialization and momentum in deep learning . In International conference on machine learning , pages 1139 – 1147 , 2013 . [ 6 ] Diederik P . Kingma and Jimmy Ba . Adam : A method for stochastic optimization . In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings , 2015 . URL http : / / arxiv . org / abs / 1412 . 6980 . [ 7 ] Yann A LeCun , Léon Bottou , Genevieve B Orr , and Klaus - Robert Müller . Efﬁcient backprop . In Neural networks : Tricks of the trade , pages 9 – 48 . Springer , 2012 . [ 8 ] Yann LeCun , Yoshua Bengio , and Geoffrey Hinton . Deep learning . nature , 521 ( 7553 ) : 436 , 2015 . [ 9 ] Prajit Ramachandran , Barret Zoph , and Quoc V Le . Searching for activation functions . arXiv preprint arXiv : 1710 . 05941 , 2017 . [ 10 ] Vinod Nair and Geoffrey E Hinton . Rectiﬁed linear units improve restricted boltzmann machines . In Proceedings of the 27th international conference on machine learning ( ICML - 10 ) , pages 807 – 814 , 2010 . [ 11 ] Xavier Glorot , Antoine Bordes , and Yoshua Bengio . Deep sparse rectiﬁer neural networks . In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics , pages 315 – 323 , 2011 . [ 12 ] Yi Sun , Xiaogang Wang , and Xiaoou Tang . Deeply learned face representations are sparse , selective , and robust . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2892 – 2900 , 2015 . [ 13 ] Lu Lu , Yeonjong Shin , Yanhui Su , and George Em Karniadakis . Dying relu and initialization : Theory and numerical examples . arXiv preprint arXiv : 1903 . 06733 , 2019 . [ 14 ] Ludovic Trottier , Philippe Gigu , Brahim Chaib - draa , et al . Parametric exponential linear unit for deep convolutional neural networks . In 2017 16th IEEE International Conference on Machine Learning and Applications ( ICMLA ) , pages 207 – 214 . IEEE , 2017 . [ 15 ] Abien Fred Agarap . Deep learning using rectiﬁed linear units ( relu ) . arXiv preprint arXiv : 1803 . 08375 , 2018 . [ 16 ] Andrew L Maas , Awni Y Hannun , and Andrew Y Ng . Rectiﬁer nonlinearities improve neural network acoustic models . In Proc . icml , volume 30 , page 3 , 2013 . [ 17 ] Djork - Arné Clevert , Thomas Unterthiner , and Sepp Hochreiter . Fast and accurate deep network learning by exponential linear units ( elus ) . In 4th International Conference on Learning Repre - sentations , ICLR 2016 , San Juan , Puerto Rico , May 2 - 4 , 2016 , Conference Track Proceedings , 2016 . [ 18 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Delving deep into rectiﬁers : Surpassing human - level performance on imagenet classiﬁcation . In 2015 IEEE International Conference on Computer Vision ( ICCV ) , pages 1026 – 1034 . IEEE , 2015 . [ 19 ] Jimmy Lei Ba , Jamie Ryan Kiros , and Geoffrey E Hinton . Layer normalization . arXiv preprint arXiv : 1607 . 06450 , 2016 . [ 20 ] Sergey Ioffe and Christian Szegedy . Batch normalization : Accelerating deep network training by reducing internal covariate shift . arXiv preprint arXiv : 1502 . 03167 , 2015 . [ 21 ] Dheeru Dua and Casey Graff . UCI machine learning repository , 2017 . URL http : / / archive . ics . uci . edu / ml . [ 22 ] Ben Poole , Subhaneil Lahiri , Maithra Raghu , Jascha Sohl - Dickstein , and Surya Ganguli . Exponential expressivity in deep neural networks through transient chaos . In Advances in neural information processing systems , pages 3360 – 3368 , 2016 . 10 [ 23 ] Boris Hanin . Which neural net architectures give rise to exploding and vanishing gradients ? In Advances in Neural Information Processing Systems , pages 582 – 591 , 2018 . [ 24 ] Yeah - Hua Wu , Fan - Yun Sun , Yen - Yu Chang , and Should - De Lin . Ans : Adaptive network scaling for deep rectiﬁer reinforcement learning models . arXiv preprint arXiv : 1809 . 02112 , 2018 . [ 25 ] J Sola and Joaquin Sevilla . Importance of input data normalization for the application of neural networks to complex industrial problems . IEEE Transactions on nuclear science , 44 ( 3 ) : 1464 – 1468 , 1997 . [ 26 ] Tapani Raiko , Harri Valpola , and Yann LeCun . Deep learning made easier by linear transforma - tions in perceptrons . In Artiﬁcial intelligence and statistics , pages 924 – 932 , 2012 . [ 27 ] Günter Klambauer , Thomas Unterthiner , Andreas Mayr , and Sepp Hochreiter . Self - normalizing neural networks . In I . Guyon , U . V . Luxburg , S . Bengio , H . Wallach , R . Fergus , S . Vish - wanathan , and R . Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 971 – 980 . Curran Associates , Inc . , 2017 . URL http : / / papers . nips . cc / paper / 6698 - self - normalizing - neural - networks . pdf . [ 28 ] Shibani Santurkar , Dimitris Tsipras , Andrew Ilyas , and Aleksander Madry . How does batch normalization help optimization ? In Advances in Neural Information Processing Systems , pages 2483 – 2493 , 2018 . [ 29 ] Xavier Glorot and Yoshua Bengio . Understanding the difﬁculty of training deep feedfor - ward neural networks . In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics , pages 249 – 256 , 2010 . [ 30 ] Hado P van Hasselt , Arthur Guez , Matteo Hessel , Volodymyr Mnih , and David Silver . Learning values across many orders of magnitude . In Advances in Neural Information Processing Systems , pages 4287 – 4295 , 2016 . [ 31 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger . Deep reinforcement learning that matters . In Thirty - Second AAAI Conference on Artiﬁcial Intelligence , 2018 . [ 32 ] Dina Q Goldin and Paris C Kanellakis . On similarity queries for time - series data : constraint speciﬁcation and implementation . In International Conference on Principles and Practice of Constraint Programming , pages 137 – 153 . Springer , 1995 . [ 33 ] Scott C Douglas and Jiutian Yu . Why relu units sometimes die : Analysis of single - unit error backpropagation in neural networks . In 2018 52nd Asilomar Conference on Signals , Systems , and Computers , pages 864 – 868 . IEEE , 2018 . [ 34 ] Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . Acknowledgements This work was funded by the * * * * * * * * * * * through the project * * * * * * * * * * * * . A Regression experiment details NN used for regression was of the form ˆ y ( x ) = w (cid:62) σ ( W x + b ( 1 ) ) + b ( 2 ) ( 33 ) where w ∈ R 200 × 1 and W ∈ R 200 × n where n is the dimensionality of the input data . All elements in W and b ( 1 ) were initialized from U ( − 1 √ n , 1 √ n ) . The elements of w and b ( 2 ) were initialized from U ( − 1 √ 200 , 1 √ 200 ) . Batch size was 64 and the Adam optimizer was set to use the same parameters as suggested by [ 6 ] . Every experiment was run 8 times with different seeds , effecting the random ordering of mini - batches and initialization of parameters . Each experiment was allowed 250000 gradient updates before evaluation . Evaluation was performed on a subset of the training set , as we wanted to investigate the ﬁt to the data rather than generalization . 11 B Single ReLU gradients For brevity we will use 1 in place of 1 ˜ x L > − b (cid:107) w (cid:107) below . Subscript notation on non - bold symbols here represents single dimensions of the vector counterpart . The gradient w . r . t . w is ∂ L ∂ w = E x (cid:2) 1 w (cid:62) x + b > 0 ( a (cid:62) x + c ) x (cid:3) = E x (cid:2) 1 w (cid:62) U (cid:62) U x + b > 0 ( a (cid:62) U (cid:62) U x + c ) U (cid:62) U x (cid:3) = U (cid:62) E ˜ x (cid:2) 1 ( ˜ a (cid:62) ˜ x + c ) ˜ x (cid:3) Multiplying by U from the left and looking at a dimension i (cid:54) = L , we get : (cid:20) U ∂ L ∂ w (cid:21) i = E ˜ x (cid:2) 1 ( ˜ a (cid:62) ˜ x + c ) ˜ x i (cid:3) = E ˜ x   1   ˜ x i   L (cid:88) j (cid:54) = i ˜ a j ˜ x j + c   + ˜ a i ˜ x 2 i     = E ˜ x [ 1 ˜ x i ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 E ˜ x   1   L (cid:88) j (cid:54) = i ˜ a j ˜ x j + c     + ˜ a i E ˜ x (cid:2) 1 ˜ x 2 i (cid:3) = ˜ a i Var ( ˜ x i ) (cid:124) (cid:123)(cid:122) (cid:125) = 1 E ˜ x L [ 1 ] = ˜ a i Φ (cid:18) b (cid:107) w (cid:107) (cid:19) For i = L we instead have (cid:20) U ∂ L ∂ w (cid:21) L = E ˜ x (cid:2) 1 ( ˜ a (cid:62) ˜ x + c ) ˜ x L (cid:3) = E ˜ x   1   ˜ x L L − 1 (cid:88) j = 1 ˜ a j ˜ x j + ˜ a L ˜ x 2 L + c ˜ x L     = E ˜ x [ 1 ˜ x L ]   L − 1 (cid:88) j = 1 ˜ a j E ˜ x [ 1 ˜ x j ] (cid:124) (cid:123)(cid:122) (cid:125) = 0   + ˜ a L E ˜ x (cid:2) 1 ˜ x 2 L (cid:3) + c E ˜ x [ 1 ˜ x L ] ( 34 ) We can calculate the second term with integration by parts E ˜ x (cid:2) 1 ˜ x 2 L (cid:3) = 1 √ 2 π ∞ (cid:90) − b (cid:107) w (cid:107) ˜ x L · ˜ x L e − ˜ x 2 L 2 d ˜ x L = 1 √ 2 π (cid:20) − ˜ x L e − ˜ x 2 L 2 (cid:21) ∞ − b (cid:107) w (cid:107) + 1 √ 2 π ∞ (cid:90) − b (cid:107) w (cid:107) e − ˜ x 2 L 2 d ˜ x L = − b (cid:107) w (cid:107) φ (cid:18) b (cid:107) w (cid:107) (cid:19) + Φ (cid:18) b (cid:107) w (cid:107) (cid:19) 12 The third term : ∞ (cid:90) − b (cid:107) w (cid:107) ˜ x L √ 2 π e − ˜ x 2 L 2 = − (cid:20) 1 √ 2 π e − ˜ x 2 L 2 (cid:21) ∞ − b (cid:107) w (cid:107) = − [ φ ( ˜ x L ) ] ∞− b (cid:107) w (cid:107) = φ (cid:18) b (cid:107) w (cid:107) (cid:19) . By substituting these expressions back in to Eq . ( 34 ) and simplifying we get Eq . ( 28 ) . The derivation of gradients w . r . t . the bias is very similar to the gradients w . r . t . to w and are omitted here for brevity . C Gradients after convergence in the ( L − 1 ) ﬁrst dimensions If we assume the L − 1 ﬁrst dimensions of ˜ a are zero , we can show that the L − 1 ﬁrst dimensions of w all have gradient zero . We can then also express the gradient without the unitary matrix U that otherwise needs to be calculated for every new w . We present this as a proposition by the end of this section . First , we need to show some preliminary steps . Lemma C . 1 . If ˜ a = ( 0 , . . . , 0 , ˜ a L ) (cid:62) then U Γ = ( 0 , . . . , 0 , kγ ) (cid:62) where k is either − 1 or 1 . Proof . As deﬁned in Eq . ( 13 ) , we have U w = ( 0 , . . . , 0 , (cid:107) w (cid:107) ) (cid:62) . Since ˜ a = U a = U w − U Γ = ( 0 − [ U Γ ] 1 , . . . , 0 − [ U Γ ] L − 1 , (cid:107) w (cid:107) − [ U Γ ] L ) we must have U Γ = ( 0 , . . . , 0 , (cid:96) ) (cid:62) . Since U is unitary we must have | (cid:96) | = γ which is solved by (cid:96) = ± γ . Lemma C . 2 . If ˜ a = ( 0 , . . . , 0 , ˜ a L ) (cid:62) then w = ( 0 , . . . , 0 , w L ) (cid:62) Proof . We have from Lemma C . 1 that Γ and U Γ lie in the same one - dimensional linear subspace K spanned by ( 0 , . . . , 1 ) (cid:62) . Since U is unitary and U w also is in K , then so must w . This implies that w = ( 0 , 0 , . . . , 0 , w L ) (cid:62) . Lemma C . 3 . If ˜ a = ( 0 , . . . , 0 , ˜ a L ) (cid:62) then for any vector v in the linear subspace K spanned by ( 0 , 0 , . . . , 1 ) (cid:62) we have U v = U (cid:62) v = ρ v where ρ = sign ( w L ) . Proof . From Lemma C . 2 , we have w = ( 0 , . . . , 0 , w L ) (cid:62) , which implies w (cid:107) w (cid:107) = ( 0 , . . . , 0 , ρ ) (cid:62) . By the property of U as deﬁned in Eq . ( 13 ) we have U w (cid:107) w (cid:107) = ( 0 , . . . , 0 , 1 ) (cid:62) . 13 Multiplying by U (cid:62) from the left , we have U (cid:62) U w (cid:107) w (cid:107) = U (cid:62) ( 0 , . . . , 0 , 1 ) (cid:62) = ( 0 , . . . , 0 , ρ ) (cid:62) This implies that the last row and last column of U is ( 0 , . . . , 0 , ρ ) . Then U v = U ( 0 , . . . , 0 , v L ) (cid:62) = ( 0 , . . . , 0 , ρv L ) (cid:62) = ρ v Since the last row and last column are the same for U , the above also holds for the transpose U (cid:62) . Proposition C . 1 . If ˜ a = ( 0 , . . . , 0 , ˜ a L ) (cid:62) then ∂ L ∂w L = a L Φ ( b (cid:107) w (cid:107) ) + ρ ( c − ρa L b (cid:107) w (cid:107) ) φ ( b (cid:107) w (cid:107) ) and ∂ L ∂w i = 0 for i (cid:54) = L and where ρ = sign ( w L ) and a L = w L − γ . The gradient w . r . t . the bias is ∂ L ∂b = ρa L φ ( b (cid:107) w (cid:107) ) + c Φ ( b (cid:107) w (cid:107) ) . Proof . From Lemma C . 1 and C . 3 we have that U a = U ( 0 , 0 , . . . , 0 , w L − γ ) (cid:62) = ( 0 , 0 , . . . , 0 , ρ ( w L − γ ) ) (cid:62) and therefore ˜ a L = ρa L . All occurrences of ˜ a L can now be replaced with this quantity . The rotated gradient is U ∂ L ∂w = ( 0 , 0 , . . . , 0 , ξ ) (cid:62) ∈ K where ξ is given by Eq . ( 28 ) . Hence ∂ L ∂w = U (cid:62) (cid:0) U ∂ L ∂w (cid:1) = ( 0 , 0 , . . . , 0 , ρξ ) (cid:62) by Lemma C . 3 . D Scaling of parameters Before stating the problem , we deﬁne an N - hidden layer neural network recursively ˆ y ( x ) = w (cid:62) y ( N ) ( x ) + b ( 35 ) where y ( n ) ( x ) = f ( W ( n ) y ( n − 1 ) + b ( n ) ) ( 36 ) and y ( 0 ) ( x ) = x . ( 37 ) Denote the joint collection of parameters w , b , { W ( n ) } N i = 1 and { b ( n ) } N i = 1 as θ . We denote ˆ y ( x ) = ˆ y ( x , θ ) to make explicit the dependence on θ . We now investigate whether a scaling of ˆ y can be achieved by scaling all parameters by a single number . Theorem D . 1 . Given some γ > 0 , γ (cid:54) = 1 , there exists no ν > 0 such that γ ˆ y ( x , θ ) = ˆ y ( x , νθ ) ( 38 ) 14 Proof . First , denote γ = γ N + 1 and factorize into N + 1 factors α i > 0 γ n = n (cid:89) i = 1 α i . ( 39 ) Multiplying ˆ y by γ gives : γ ˆ y ( x ) = γ N + 1 w (cid:62) y ( N ) ( x ) + γ N + 1 b ( 40 ) = γ N + 1 γ N w (cid:62) γ N y ( N ) ( x ) + γ N + 1 b ( 41 ) = α N + 1 w (cid:62) γ N y ( N ) ( x ) + γ N + 1 b ( 42 ) and similarly γ n y ( n ) ( x ) = γ n f ( W ( n ) y ( n − 1 ) ( x ) + b ( n ) ) ( 43 ) = f ( α n W ( n ) γ n − 1 y ( n − 1 ) ( x ) + γ n b ( n ) ) . ( 44 ) We ﬁnally deﬁne γ 0 y ( 0 ) ( x ) = 1 · x . If a ν exists that satisﬁes Eq . ( 38 ) , then we must have all α i = ν . If γ > 1 , then γ n = ν n > ν = α n and thus in Eqs . ( 42 ) and ( 44 ) the weights and biases are not multiplied by the same number γ n (cid:54) = α n . Similarly , this holds for γ < 1 . 15