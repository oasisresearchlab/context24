Complex QA & language models hybrid architectures , Survey XAVIER DAULL , Naval Group , Toulon Université , Aix Marseille Univ , CNRS , LIS , France PATRICE BELLOT , Aix Marseille Univ , CNRS , LIS , Marseille , France EMMANUEL BRUNO , Toulon Université , Aix Marseille Univ , CNRS , LIS , Toulon , France VINCENT MARTIN , Naval Group , France ELISABETH MURISASCO , Toulon Université , Aix Marseille Univ , CNRS , LIS , Toulon , France This paper reviews the state - of - the - art of language models architectures and strategies for " complex " question - answering ( QA , CQA , CPS ) with a focus on hybridization . Large Language Models ( LLM ) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems ( e . g . How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ? ) you may need specific architecture , knowledge , skills , methods , sensitive data protection , explainability , human approval and versatile feedback . . . Recent projects like ChatGPT and GALACTICA have allowed non - specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA . In this paper , we start by reviewing required skills and evaluation techniques . We integrate findings from the robust community edited research papers BIG , BLOOM and HELM which open source , benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy ( e . g . fairness , robustness , toxicity , . . . ) as a baseline . We discuss some challenges associated with complex QA , including domain adaptation , decomposition and efficient multi - step QA , long form and non - factoid QA , safety and multi - sensitivity data protection , multimodal search , hallucinations , explainability and truthfulness , temporal reasoning . We analyze current solutions and promising research trends , using elements such as : hybrid LLM architectural patterns , training and prompting strategies , active human reinforcement learning supervised with AI , neuro - symbolic and structured knowledge grounding , program synthesis , iterated decomposition and others . CCS Concepts : • Information systems → Question answering ; Information extraction ; • Security and privacy → Information accountability and usage control . Additional Key Words and Phrases : complex question answering , semantic search , NLP , transformers , neural language models , GPT , taxonomy , neuro - symbolic , attention , pre - training , fine - tuning , prompting , non - factoid QA , multi - hop QA , multi - step QA , long - form QA , knowledge graph , multimodal search , human - in - the - loop , RLHF , RLHP , RLAIF ACM Reference Format : Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco . 2023 . Complex QA & language models hybrid architectures , Survey . 1 , 1 ( April 2023 ) , 47 pages . https : / / doi . org / XXXXXXX . XXXXXXX Authors’ addresses : Xavier Daull , Naval Group , Toulon Université , Aix Marseille Univ , CNRS , LIS , France ; Patrice Bellot , Aix Marseille Univ , CNRS , LIS , Marseille , Marseille , France ; Emmanuel Bruno , Toulon Université , Aix Marseille Univ , CNRS , LIS , Toulon , Toulon , France ; Vincent Martin , Naval Group , France ; Elisabeth Murisasco , Toulon Université , Aix Marseille Univ , CNRS , LIS , Toulon , Toulon , France . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . © 2023 Copyright held by the owner / author ( s ) . XXXX - XXXX / 2023 / 4 - ART https : / / doi . org / XXXXXXX . XXXXXXX , Vol . 1 , No . 1 , Article . Publication date : April 2023 . a r X i v : 2302 . 09051v4 [ c s . C L ] 7 A p r 2023 2 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco 1 Introduction Research in the field of question answering could route from earliest examples of AI system designed to answer questions like ELIZA [ 1 ] developed at MIT in the 1960s ; or engineering approaches to solve complex problems ; or foundational research in the psychology field on how people understand and interpret language , prioritize and focus on relevant information in context [ 2 , 3 ] , investigate and make decision , retrieve and use information from memory to answer [ 4 ] , innovate . . . Even Socrates’ philosophical approach to questioning and critical thinking has recently been directly used to improve training [ 5 ] , common sense reasoning [ 6 ] , or helping to solve some complex questions [ 7 ] . When a question or problem is too complex to be solved as it is , such as “ how does the concept of personal freedom vary between different cultures and societies ? ” , a common strategy is to break it down into solvable questions , and then combine solutions to provide an overall answer if there is consensus , or the possible alternatives and nuances . We aim to answer complex questions , or problems formulated as a question , which are non - factoid and so require decomposition ( multi - step ) , multi - source of knowledge combination , higher order reasoning or tasks resolution . All of these may vary a lot depending on the field and question . Neural language models recently demonstrated their ability to outperform an average human on different tasks across knowledge fields , and mimic methods or uncover appropriate methods for a given problem . However even most advanced systems will fail on some basic questions [ 8 ] , or could assert totally false or biased knowledge without any caution . This can seriously impact the credibility of a system by a human . Involving humans in the question - answering loop , strengthening training , hybridizing with third parties like programs , symbolic AI or other , can greatly improves those models and help ensure ethical and safe outcomes . To be able to build those efficient hybrid systems , properly trained and aligned to human expectations in order to better solve increasingly complex problems , it is thus necessary to precisely know strengths and limitations of each language models alone or in collaboration . Therefore we use benchmarks and insights from collective papers as consensus baselines . Largest community edited papers HELM [ 9 ] , BLOOM [ 10 ] and BIG [ 11 ] focus on evaluating , democratizing and improving LLM capabilities , particularly on question answering and tasks that will also be useful for more complex questions . HELM provides a comprehensive analysis and benchmark for evaluating global strengths and weaknesses of reference large language models across a range of different scenarios and complementary metrics . Training large language models is done by resource - rich organizations and are mostly impossible to train by any individual public research group . BigScience 1 , a collaboration of hundreds of international researchers , trained on the French government - funded Jean Zay supercomputer during 3 . 5 months the largest open source multilingual language model called BLOOM and shared all models , codes and results in the BLOOM paper . BIG focus on spot and difficult capabilities to better assess current skills and limitations of language models . To overcome identified limitations and tackle more specific or complex questions , after those benchmarks and insights , we review hybrid architectural patterns solutions , training and prompting techniques , decompo - sition and reinforcement strategies to acquire the necessary knowledge , skills ( general abilities ) , tasks ( goal oriented actions ) , methods and human alignment . 1 . 1 Structure of the paper This survey covers the main topics related to complex question answering systems . In section 2 , we define key concepts that are necessary to understand the rest of the survey : definitions , typical process and architectures , new approach brought by large language models ( LLM ) and transformers . Next , we delve into complex QA definition , required tasks & skills , limitations to overcome ( section 3 ) to answer complex questions . For each limitation we link to potential resolution strategies presented later . In section 4 , we review the evaluation metrics , methods and datasets of the scientific community used to assess current tasks , skills and limitations of LLM , or to further develop them through training . We then review the complementary resolving strategies which could be combined to solve complex QAs for the target usage . We first explore training techniques in section 5 , including methods for dealing with lack of data , poor quality , and adaptation to new tasks and domains . Second , in section 6 , we review and classify different hybridization architectural patterns with their pros and cons to augment LLM . Third , in section 7 , we review prompting techniques and how it can also be 1 https : / / bigscience . huggingface . co / , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 3 used to decompose complex questions down to solvable tasks . Fourth , in section 8 , we review reinforcement techniques , continual learning , or how complex QA system might leverage its experience to solve more complex QAs . Finally , section 9 highlight tougher challenges , partial solutions that have been identified , and research avenues to overcome them . All along the survey , we provide an extensive bibliography for readers who wish to delve deeper into the subject matter ( section 10 ) . 1 . 2 Contribution This survey provides six main contributions : – a systematic review & analysis of literature on complex QA with LLM , including an enriched definition ( subsection 3 . 1 ) and taxonomy ( Figure 2 ) , and an extensive bibliography of the field . – a qualitative analysis of the skills , tasks , and limitations of LLM ( section 3 ) aimed at better framing complex QA requirements and complexity . – an overview of evaluation metrics , methods , datasets and SOTA ( section 4 ) to better evaluate skills & tasks , and estimate LLM limits and strategies . – a classification and aggregation of hybridization architectural patterns ( section 6 ) that can augment LLM and overcome their limitations in complex QA . – a set of resolving strategies to combine ( training : section 5 , hybridization : section 6 , prompting : section 7 , reinforcement : section 8 ) . – a list of major research challenges ( section 9 ) and a focus on some blind spots ( i . e . data multi sensitivity ) . 1 . 3 Survey methodology To build this survey , first , we collected surveys in the last two years related to “ complex question answering ” or “ complex problem solving ” and “ language models ” that we cite throughout the article . From these elements , we systematically extracted the major concepts and their plans ( table of contents ) . We fused all their plans into one to ensure a complete coverage of major concepts and adopt a similar methodology . Second , we gathered : – latestchallengesfromCLEFQA ( nlp . uned . es ) , NTCIR ( research . nii . ac . jp ) andSemEval ( semeval . github . io ) related to question answering ; – the list of the major conferences ; then , list research papers from latest edition of conferences SIGIR ( si - gir . org ) , NeurIPS ( nips . cc ) , NAACL ( naacl . org ) , EMNLP ( emnlp . org ) , ICLR ( iclr . cc ) , AAAI ( aaai . org ) , IJCAI ( ijcai . org ) , CIKM ( cikm2022 . org ) , SIGKDD ( kdd . org ) , WSDM ( wsdm - conference . org ) about “ complex question answering ” and “ language models ” ; – research publications from influential organizations in the field : Deepmind , OpenAI , Google , Microsoft , Meta related to " question answering " and " language models " . From these enriched data we identified and clustered major challenges and solution patterns from the field and state - of - the - art : – Domain adaptation or specialization , and maintenance . – Decomposition ( including multi - hop / step ) , question to search action design . – Safety and data sensitivity , truthfulness , veracity and hallucinations , explainability . – Reasoning ( process , methods , chain of thought , code generation , causality ) . – Scalability ( inc . routing , space reduction ) . – Alignment to human intentions , expectations and values ( e . g . RLHF , clarifications and questions expansion ) . – Long form question answering , long term dependency in reasoning , multi - document summarization . – Dialog and context improvement ( e . g . clarification question ) . – Multimodal search ( e . g . images , tables , audio ) . – Time dimension reasoning . – Biases reduction . Third , we enriched our bibliography using a search engine of scientific articles to identify all the articles published during the last four years on the main subject of this investigation ( search queries : " complex question answering " AND " language model " ; " question answering " AND " language model architecture " ; " question answering " , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 4 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco AND " language model " AND " hybrid architecture " ; " hybrid language models " OR " hybrid language model " OR " hybrid neural language models " ; " language model " " hybrid architecture " ) . The bibliography collected in all the previous steps was then used throughout this survey to ensure that we summarize the main concepts at the state - of - the - art by automatically detecting articles semantically close ( subscription to " zeta alpha " and " semantic scholar " ) . Last but not least , we searched the most cited papers in this bibliography and extended with recent connected papers or some historical papers often cited , and investigated relevant citations . Through this research , we identified three recent research papers ( HELM , BLOOM , and BIG ) each involving hundreds of researchers from many organizations , so we decided to use them as a baseline or major reference in this study . 2 Core concepts & architectures 2 . 1 Definitions In order to help the reader and to better define the scope of this study , we pose a set of definitions on the key concepts . As different definitions often exist for the same concept , we choose the one that best defines the scope of survey . Question Answering ( QA ) systems can answer questions from a specific or general knowledge base ( limited / closed - domain , or public / open - domain QA ) . Complex question answering ( CQA ) is to answer higher complexity questions . This complexity can come from the variety of skills required ( 3 ) including multiple logic , higher reasoning ( e . g . constraints solving , deduction , induction , abduction ) and domain knowledge , answer format alignment and nuances handling in non - factoid questions , the need for decomposition and multi - step resolution , multiple sources of information to include and reason over on longer reasoning distance . Complex problem solving ( CPS ) is to overcome barriers between a given state and a desired goal state by means of behavioral or cognitive multistep activities [ 12 ] . A CPS process is split into two phases , knowledge acquisition and knowledge application . Question complexity is a multidimensional construct that can be characterized by various aspects such as syntactic , lexical , semantic , discourse complexity , or cognitive complexity . We focus on this latest and consider the cognitive process required to answer a question , such as the level of concepts abstraction , the degree of reasoning , the amount and variety of knowledge , and the number of steps needed to find the answer . Non - factoid questions ( e . g . " What are the causes of poverty in cities ? " ) are typically considered much more complex than factoid questions ( e . g . " What is the capital of Suriname ? " ) . Information Retrieval ( IR ) is the general task of retrieving information for a given query / subject . Se - mantic search is the IR process of using natural language understanding techniques such as embedding to match the " meaning " of a query ( rather than keywords ) to the most relevant content in a corpus , regardless of the format of the content ( e . g . structured or unstructured text , image . . . ) . Taxonomy of QA formats is a proposed taxonomy [ 13 ] which covers question format , answer format , and source & evidence format . Question format can be in the form of natural language questions , queries , cloze format or story completion . Answer format can be in the form of extractive format , multi - choice / categorical format , freeform / generative format , or numerical . The source & evidence format can be in the form of unstructured text , semi - structured text , structured knowledge , images , audio , video or other combinations , and it can also be characterized by the amount of evidence provided , and whether it is from a single , multiple , partial or no source . Taxonomy of QA skills is presented in a further section on elementary tasks & skills types . Prompt and instructions engineering VS pre - training and fine - tuning : pre - trainingreferstotrain - ing a model on a large corpus of data to learn general understanding of knowledge and logic ; fine - tuning is the process of adapting a pre - trained model to a specific task with additional training on specific examples ; prompt engineering is the technique of optimizing input question and optional instructions sent to a pre - trained model , potentially fine - tuned , without re - training it to improve the model’s answer performance . End - to - end QA VS pipeline of specialized tasks : an end - to - end QA uses a single call to a model to answer , while a pipeline uses multiple calls , often to specialized models , to deliver final answer . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 5 Fig . 1 . IBM DeepQA Architecture ( 2010 ) [ 16 ] and CQA pipeline steps . Multi - hop / Multi - step QA is related to questions requiring reasoning over multiple pieces of evidence or multiple steps . It is often done after a question decomposition task or iteratively . ( Neural ) Language model is a statistical model that predicts the likelihood of a sequence of words , or tokens , in a language . It can understand and generate human - like text or code , perform various language - related tasks and even non - textual tasks leveraging skills extracted from text or code [ 14 ] . ( Language model with ) Knowledge in / out of model is the distinction between a model with knowl - edge explicitly encoded inside the model , and a model using external knowledge ( e . g . search engine , data sources , program ) . Standard / monomodal vs multimodal QA : monomodal can search only one type of input ( e . g text only ) while multi - modal can search multiple types of input ( text , image , video , sound . . . ) Human - in - the - loop is the practice of involving humans in the process to cover a processing task of getting feedback for improving system or processing a task instead of a system . Most frequent technique is RLHF ( reinforcement learning with human feedback ) . Hallucinations are confident generated responses including false information not justified by the model’s training data . They are classified as extrinsic , when a model adds information that is not present in the source data , and intrinsic , when the model distorts information present in the source data into a factually incorrect representation [ 15 ] . Knowledge graphs ( KG ) & ontologies are structured representations of knowledge ( ontologies for concepts , KG for real - world objects ) enabling to link and reason over knowledge . 2 . 2 Question answering typical pipeline From question capture and refinement , to answer generation and knowledge capitalisation , question answering ( QA ) or complex question answering ( CQA ) pipeline can follow a variable number of steps depending on architecture and features . Some steps are explicit and well separated , some can be implicit and fused with others in the same model operation . However , we can identify most frequent steps and options . The " IBM DeepQA Architecture " ( see figure 1 ) seems a dated architecture compared to some current end - to - end neural language models but it defines clearly some major steps : ( 1 ) Question understanding and analysis , it can include question and context refine , parsing and un - derstanding the question , context , and intent ( for task identification ) . It can also embed a dialogue management to interact with the user and understand the conversation context and state . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 6 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco ( 2 ) Query construction with optional decomposition of complex questions and multi - step queries . ( 3 ) Information retrieval ( IR ) with optional knowledge base expansion - to add new knowledge to the system . ( 4 ) ( a ) Information extraction , and ( b ) evaluation , scoring , ranking , filtering . ( 5 ) Answer generation , natural language or defined format ( e . g . language program , table , . . . ) ( 6 ) Feedback loop & new knowledge capitalisation : learning and improving from users and models feedback , plus storing linked and generated knowledge for improving answering skills . This process is only a baseline as complex questions answering can be a dynamic and progressive process , and can also be collaborative . Architectures of QA systems have importantly evolved recently with the arrival of transformers architectures and large language models . We first quickly review typical modular architectures , then the transformers with large language models and , later , the hybrid architectures . Will we go to gigantic knowledge models or more complex and composed architectures , maybe in network of smaller specialized models and other components ? 2 . 3 Modular approaches before transformers Typical architectures of QA systems before transformers could be grouped in the following complementary approaches / methods : Rule - based approach / methods - These systems are based on a set of predefined rules that the system uses to answer questions . Retrieval - based approach / methods - These systems use a search engine or database to find answers to questions . Information extraction approach / methods - These systems use natural language processing ( NLP ) to extract relevant information from text documents and often leverage information retrieval ( IR ) systems . Knowledge - based approach / methods - These systems store and retrieve information from a knowl - edge base . Case - based reasoning systems - These systems use a database of previously solved problems to find solutions to new questions . Hybrid architectures - The task oriented approaches below could sometimes be assembled ( e . g . " IBM DeepQA Architecture , 2010 " [ 16 ] ) for delivering a more advanced QA system and integrated with natural language models for understanding initial question for example . 2 . 4 LLM with transformers breakthrough . . . The emergence of deep feedforward layers , allowed to learn and infer a wide range of relationships with inputs ; Then the raise of attention mechanism , allowed a model to selectively focus on certain parts of the input for better understanding and contextualizing . It led to language models surpassing humans on certain tasks . We can group current transformer based language models in three types [ 17 ] : Encoders only ( e . g . BERT [ 18 ] , RoBERTa [ 19 ] ) encode a sequence of text ( input ) into a rich represen - tation ( vector embedding ) which can be used by a task - specific model or function for classification , named entity recognition ( NER ) , semantic similarity measure used in IR and QA or topic modeling . This is often called bidirectional attention , meaning they take the context of the words before and after the target word into account , which allows them to perform better on some tasks . BERT is one of the most well - known encoder - only models , and RoBERTa is an optimized version of BERT . Decoders only ( e . g . GPT - 3 [ 20 ] ) complete an input sequence ( mostly text prompt ) by the most probable next words ( generation ) . This left to right generation can be very rich like writing a story or answering a question ( e . g . used by ChatGPT ) . The input prompt can be formatted with appropriate instructions ( prompt & instructions engineering ) to use the generated text as a task - specific model ( e . g . classification , summarization , decomposition . . . ) . This is often called causal or autoregressive attention ( non - causal decoders exist but has limited adoption in the literature ) . GPT family of models are one of the most well - known decoder - only models , known for their ability to generate human - like text . Encoders - Decoders ( e . g . T5 [ 21 ] , BART [ 22 ] , BigBird [ 23 ] ) encode the input text and decode it into a different form ( text - to - text mapping ) , they are suitable for translation , summarization , or generating , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 7 a response to a question . They consist of both an encoder and a decoder , where the encoder generates a fixed - size representation of the input text , and the decoder generates the output text . T5 is known for its multi - tasks ability , BART is mainly used for text generation and summarization , and BigBird allows to process much longer sequence of texts than other models . 3 Analyzing : complexity , skills , tasks , and limits In order to design a system able to answer complex questions , we first propose to analyze targeted questions complexity , identify required skills and tasks , as well as limitations to handle . This analysis allows to properly define the problem to be solved gradually and constraints to integrate in order to compose among the different complementary solving approaches further reviewed ( training : section 5 , hybridization : section 6 , prompting : section 7 , experience : section 8 ) . This analysis could also be done " a posteriori " if a system fails to properly answer in order to better characterize or identify the causes . 3 . 1 What are complex questions ? Ullrich and Geierhos [ 24 ] proposes to use Bloom taxonomy to assess question complexity by combining required knowledge dimension ( from easier to harder : factual , conceptual , procedural , meta cognitive ) and cognitive process dimension ( from easier to harder : remember , understand , apply , analyze , evaluate ) . We think that a question complexity is also highly dependent on users or systems expertise involved in the response . We propose to also assess it by identifying main difficulties and efforts required by a LLM to solve target questions : • skills and knowledge required ( section 3 . 2 ) : simple memorization or higher reasoning ( e . g . evaluation , constraints solving , deduction , induction , abduction ) , single or multiple logic ; prior domain - specific knowledge , retrieve and process one easily accessible information or multiple rare information , reason over long distance between information to combine ; expected answer format and explanation ; ambiguity and nuances to handle ( especially in non - factoid questions ) . • new or challenging tasks to solve like specific decomposition , and multi - step resolution ( section 3 . 3 ) . • major limitations of LLM to overcome in this context ( sections 3 . 4 and 9 ) . • difficulty to evaluate skills , knowledge and tasks performance ( section 4 ) with existing metrics & sufficient datasets or to create . • training effort to develop those skills & tasks ( section 5 ) in a model , or in different hybrid models ( section 6 ) and , align or train , end - to - end to solve questions in a coherent way . • difficulties to engineer prompt questions on trained models such as additional context and instruc - tions required ( section 7 ) , and then decompose questions down to solvable tasks . • progressive reinforcement and knowledge capitalization , or system’s experience learning , to solve targeted complex questions ( section 8 ) . 3 . 2 Skills Considering the QA / CQA standard pipeline presented in introduction , a task of question answering requires different complementary skills in the domain of machine reading comprehension ( MRC ) , information retrieval ( IR ) , and also knowledge capitalisation and reinforcement . In this section , we leverage the QA skills taxonomy from Rogers et al . [ 13 ] which we augmented with the " Experience learning " skill concept ( see figure 2 ) , important for a CQA system . This later has shown to be a major skill to enable calibration or alignment to intent and values , and continuous improvement by usage [ 25 , 26 ] . 3 . 2 . 1 Interpreting & manipulating input Like humans , machines should capture the meaning of the individual constituent elements of the input ( words , numbers ) and the global syntax and semantic , and manipulate them in the context of the task and in respect to the language and other shared system ( e . g . mathematics ) . This requires a set of skills , including : • Linguistic skills - e . g . recognizing word categories and meaning , translating , understanding the context , relationships and implications of words and phrases ; it might be decomposed into syntactic , grammatical , and semantic skills . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 8 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco F i g . 2 . Q A / C Q A t a x o n o m y l i n k s s k i ll s , t a s k s a n d l i m i t s o f LL M t o c o m p l e m e n t a r y r e s o l v i n g s t r a t e g i e s ( t r a i n i n g , h y b r i d i z a t i o n , p r o m p t i n g a n d r e i n f o r c e m e n t ) , a s w e ll a s e v a l u a t i o n d a t a s e t s & m e t r i c s ( s k i ll s i n s p i r e d b y R o g e r s e t a l . [ 13 ] ) - E a c h c o n c e p t i s p r o v i d e d w i t h r e f e r e n c e t o s e c t i o n i n t h i s p a p e r . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 9 • Numeric skills - e . g . performing calculations , dealing with precise and imprecise numbers . • Operation on sets - e . g . selecting , combining , intersection , operating on elements of a set of input ( e . g . Alice , Bob and Charlie are in the room . Bob goes out . Who are the persons in the room ? ) ; 3 . 2 . 2 ( Information ) Retrieval It can be summarized as determining whether an answer exists , if yes , to look for it and provide most useful information : • Answerability : ability to identify whether a given query is a valid question and can be answered with provided information . Optionally identify additional information to correctly answer . • Where to look for the required knowledge ? : ability to identify the correct source of knowledge to get the best answer . It the required knowledge for the answer is in the question , process is to extract the good piece of information . Otherwise , we need to know if it is a precise fact or non factual , then where to look for it . Additionally , a proper answer may require common sense and potential domain information . 3 . 2 . 3 Inference type & reasoning ( Generation ) Inference / reasoning : it can be summarized as the process of drawing logical conclusions from available facts or other premises . Inference is used in language models to understand a text , and generate responses to questions posed . There are three main aspects to inference in language models : • Inference Strength : could draw general conclusions from specific facts ( inductive ) , or draw specific conclusions from general facts ( deductive ) . • Inference Mechanism : draw conclusions from a comparison of two or more elements ( analogy ) , draw conclusions based on the best explanation for a given situation ( best explanation ) . . . • Inference Direction : conclusion follows necessarily from the premises or from general to specific ( Deductive ) , conclusion is reached through a process of elimination or reasoning from the specific to the general ( abductive ) . 3 . 2 . 4 World modeling [ 27 ] It can be summarized as the ability to understand and make decisions based on the understanding of the world . It is a complex type of question answering skill that requires understanding of physical and mental states , as well as relationships between them . It involves the following categories : • Spatial reasoning : understand and reason about objects and their locations in space . • Temporal reasoning : understand and reason about event order , event attribution to time , script knowledge , event duration , temporal commonsense knowledge , factoid / news questions with answers where the correct answers change with time , temporal reasoning in multimodal setting . • Belief states : understand and track beliefs , opinions , and mental states . • Causal relations : understand and reason about the cause - and - effect relationships between events . • Other relations between events : understand and reason about relationships between events , such as sub - events , conditionals , and counterfactuals . • Entity properties and relations : properties of characters , physical properties , numerical properties , social interactions . • Tracking entities : understand and track entities over time , across locations , in co - reference chains . 3 . 2 . 5 Decomposing , multi - step Complex questions require decomposition down to solvable tasks and resolution in the best chain of action steps . Simple question may use multi - step resolution but all neces - sary knowledge are located in one place . Complex questions rely on several knowledge , necessitating the combination of information across sentences , paragraphs , documents , or other modalities . It also includes questions that require a combination of context and world knowledge . It can be even broader than simply combining several facts , and could also be taken as combining the “skills” from different dimensions and different methods of resolution . This decomposition and multi - step resolution can be resolved inside a model having these skills and all other necessary for the question , or distributed across multiple components . 3 . 2 . 6 Experience learning A complex QA system should be able to permanently improve itself through : reinforcement by aligning answers to target intent , format , method , values expectations with solutions which could vary with requester person ( e . g . knowledge , culture . . . ) or system ; capitalization by integrating new knowledge generated or linked in order to improve knowledge enabling to solve more complex problems . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 10 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Experience skills could be classified under meta - analysis in world modeling skills but it may not fully capture self - modeling , self - practice or integration of external feedback , incremental learning towards a coherent optimization of all skills . 3 . 3 Tasks A task of complex question answering could be solved in one inference task incorporating all the skills viewed in previous section , or subdivided in several sub - tasks , each with a set of skills and , maybe , different domains . 3 . 3 . 1 Integrated ( C ) QA task In this case , the CQA system answers from question using only one inference in the model but could include multi - step reasoning inside the model . LLM ( large language models ) should therefore embed all the necessary skills and knowledge for interpretation & manipulation , information retrieval , world knowledge , reasoning & inference , decomposition & multi - step resolution . If it is not the case , model should be further trained with adapted datasets to acquire those new skills and knowledge , or rely on task decomposition and LLM hybridation . The papers BIG [ 11 ] and HELM [ 9 ] provide a good overview of current limits of integrated task approach for QA / CQA ( see also tables " [ HELM ] SOTA QA multi - metrics " ( 3 ) and " [ BIG ] QA complex QA tasks benchmark " ( Table 1 ) ) . Table 1 . [ BIG ] QA complex tasks benchmark ( January 2013 ) , focus on decomposition , multi - step , context length , truthful , programmatic , summarization - Each taskis comparedbetween " bestmodel vsaverage humanvs experthuman " onthe samegivenmetricspecifiedinthe2ndcolumn : BLEUandExact ( string ) matchareexplainedinsection4 . 1 . 1 , " multiplechoicegrade " isaweightedmultiplechoice accuracybetween0 - 100forasetoftargetsandscoresforeachpotentialtargetarespecified , " normalizedaggregatescore " isanaggregationofvariousmetricson asamebaseline task metric avg hu - man max ex - pert maxmodel model conf deltamodel / avg h . deltamodel / expert auto _ categorization BLEU 1 7 16 PaLM - 535B - 5shots 1500 % 129 % matrixshapes exact ( string ) match 4 60 35 PaLM - 535B - 2shots 785 % - 41 % factuality _ of _ summary normalizedaggregatescore 12 25 51 BIG - G T = 0 - 137B - 0shots 321 % 102 % gre _ reading _ comprehension multiplechoicegrade 39 80 68 PaLM - 535B - 1shots 74 % - 15 % gem normalizedaggregatescore 23 30 39 PaLM - 535B - 1shots 71 % 31 % minute _ mysteries _ qa normalizedaggregatescore 4 31 7 PaLM - 535B - 0shots 68 % - 78 % misconceptions multiplechoicegrade 64 90 81 PaLM - 535B - 2shots 27 % - 10 % strategyqa multiplechoicegrade 63 90 74 PaLM - 535B - 5shots 17 % - 18 % fact _ checker multiplechoicegrade 72 89 84 PaLM - 535B - 5shots 17 % - 6 % understanding _ fables multiplechoicegrade 67 100 77 PaLM - 535B - 2shots 15 % - 23 % question _ selection multiplechoicegrade 48 100 55 PaLM - 535B - 1shots 14 % - 45 % vitaminc _ fact _ verification multiplechoicegrade 63 100 71 PaLM - 535B - 2shots 12 % - 29 % logic _ grid _ puzzle multiplechoicegrade 40 100 44 PaLM - 535B - 2shots 9 % - 56 % analytic _ entailment multiplechoicegrade 81 100 86 PaLM - 535B - 5shots 6 % - 14 % what _ is _ the _ tao multiplechoicegrade 79 100 83 PaLM - 535B - 5shots 5 % - 17 % authorship _ verification multiplechoicegrade 49 90 50 BIG - G T = 1 - 137B - 0shots 3 % - 44 % misconceptions _ russian multiplechoicegrade 65 100 59 PaLM - 535B - 5shots - 9 % - 41 % boolean _ expressions multiplechoicegrade 79 100 69 BIG - G T = 0 - 137B - 0shots - 13 % - 32 % chess _ state _ tracking normalizedaggregatescore 57 97 48 PaLM - 535B - 0shots - 16 % - 51 % evaluating _ information _ ess . multiplechoicegrade 39 70 28 BIG - G sparse - 9B - 0shots - 28 % - 60 % hhh _ alignment multiplechoicegrade 75 75 51 PaLM - 535B - 5shots - 32 % - 32 % web _ of _ lies multiplechoicegrade 81 100 54 BIG - G T = 1 - 137B - 0shots - 33 % - 46 % truthful _ qa normalizedaggregatescore 64 96 42 BIG - G T = 0 - 137B - 0shots - 35 % - 56 % spelling _ bee normalizedaggregatescore 5 9 3 BIG - G T = 1 - 137B – 1shots - 36 % - 64 % multistep _ arithmetic normalizedaggregatescore 10 25 6 BIG - G T = 0 - 137B - 0shots - 43 % - 77 % python _ programming _ challenge normalizedaggregatescore 2 39 1 BIG - G T = 1 - 137B - 0shots - 60 % - 98 % long _ context _ integration normalizedaggregatescore 5 17 2 GPT - 200B - 0shots - 62 % - 89 % tracking _ shuffled _ objects multiplechoicegrade 65 100 24 PaLM - 535B - 5shots - 63 % - 76 % checkmate _ in _ one exactstringmatch 8 70 2 PaLM - 535B - 2shots - 80 % - 98 % ascii _ word _ recognition exactstringmatch 86 100 15 PaLM - 535B - 2shots - 83 % - 85 % 3 . 3 . 2 CQA tasks decomposition and primitives Answering by decomposition could be grouped in those different categories : , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 11 – Standard sub - tasks include intent detection , word sense disambiguation , entity recognition ( NER ) and linking , topic classification , sentiment classification , information extraction , fact retrieval , ranking , and summarization , including query focused summarization . . . – Advanced sub - tasks include multi - hop & decomposition , domain oriented tasks , sources & fact checking , code generation & program synthesis , causal explanation ( or possible consequences ) , temporal explanation . . . – External sub - tasks leverage resources out of model like program synthesis [ 28 ] , using a solver . . . Wu et al [ 29 ] proposes a taxonomy of primitive tasks in decomposed and chained LLM which could be applicable to CQA : – Validate and categorize input such as classification which assigns the input to categories . Most useful for branching logic and validation ( e . g . is the question answerable ? ) . – Gather additional information from the LLM such as Factual Query to ask LLM for a fact , Gener - ation to ask LLM to do some creative “hallucination” on the input , Ideation to ask a list of ideas or examples . – Re - organize input such as Information extraction from the context , Rewriting ( 1 - 1 mapping ) input to more machine - readable formats ( e . g . JSON to natural language ) or other usage ( e . g . translation ) , Split Points ( 1 - N mapping ) for splitting contexts , digging concepts , Compose Points ( N - 1 mapping to synthesise , reverse operation of decomposition like merge multiple results back together . 3 . 3 . 3 CQA tasks hybrid program decomposition examples To better illustrate how CQA tasks could be simply decomposed between a LLM and an external software module to better solve complex problems , we invite you to see how : university level math problems ( CQA ) can be solved by splitting the task between a LLM and a Python language interpreter [ 28 ] , physical reasoning question can be solved by splitting the task between a LLM and a physics engine [ 30 ] . Those solutions design can be extrapolated to many complex QA challenges by looking into the section later about " hybrid LLM patterns " ( section 6 ) . Each of these patterns can be combined to split sub - tasks of complex problem to leverage most adapted module for the task by ensuring necessary context is provided wherever needed . 3 . 4 LLM limits Scaling up language models has been shown to predictably improve performance [ 31 ] on a wide range of downstream tasks . The HELM [ 9 ] and BIG [ 11 ] studies show that state - of - the - art on most scenarios are led by those very larges models but still lack on different sides ( e . g . fairness , robustness across tasks & domains , reasoning ) . Complex question answering are even more demanding and require to push further the limits . Many additional components are used or investigated to face limitations of those models by hybridation with LLM . For example , ChatGPT and InstructGPT [ 32 ] added reinforcement learning with human feedback to its pre - trained large language model to highly improve [ 33 ] their answer performance ( e . g . calibration , human expectation alignment ) . Therefore we decided to cover in this study the improvement of the language model itself ( see next section ) and the hybridation patterns which can overcome different limits of base models ( below ) . Those different limits or challenges have been identified in our systematic review , and we linked them to different potential solutions presented in next section . Limitation Potentialsolutions ( seealsosection9 ) Adaptating and updating - adapting models to specific domains and tasks , and ensuring they stay updated with new knowledge Hybridization : 6 . 2 , 6 . 3 , 6 . 5 , 6 . 6 , 6 . 18 ; Training : 5 . 3 . 2 , 5 . 4 . 4 , 5 . 4 . 3 ; Reinforce - ment / experience : 8 ; Prompting : 7 . Bias [ 9 ] - mitigating biases , especially those related to race , gender , and religion , in model outputs Hybridization : 6 . 9 , 6 . 18 . Reinforce - ment / experience : 8 . Scalability [ 9 ] - efficiently scaling models for training and inference while managing computational resources Hybridization : 6 . 5 , 6 . 12 , 6 . 17 . Training : 5 . 5 , 5 . 6 . Question context improvement - enhancing context understanding through clarification questions , question expansion , and dialog . Hybridization : 6 . 13 , 6 . 3 . Prompting : 7 . Re - inforcement / experience : 8 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 12 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Question decomposition strategy - breaking down complex questions into simpler sub - questions , enabling multi - hop / step reasoning and action design . Prompting : 7 . Hybridization : 6 . 4 , 6 . 10 . Re - inforcement / experience : 8 to learn spe - cificdecomposition . Reasoning [ 13 ] - incorporating higher logical reasoning , causality , and learn - ing from code to improve problem - solving capabilities . Those reasoning capabilities could be specific and added at inference time . Hybridization : 6 . 3 , 6 . 7 , 6 . 8 , 6 . 14 , 6 . 15 . Training : 5 . 4 . 2 , 5 . 3 . 3 . Alignment to human expectation & values [ 32 ] - ensuring models align with human expectations and values while managing trade - offs and cultural differences . Reinforcement / experience : 8 . Hybridiza - tion : 6 . 9 , 6 . 17 . Training : 5 . 4 . 6 , 5 . 4 . 3 . Hallucination [ 34 ] prevention , veracity , explainability and safety - re - ducing hallucination , ensuring answer accuracy , providing confidence and explanations , and maintaining security in critical domains . Reinforcement / experience : 8 . Hybridiza - tion : 6 . 9 , 6 . 11 . Training : 5 . 4 . 8 , 5 . 6 . . . Long form question answering handling - handling long context inputs , addressing long - term dependencies in reasoning , and summarizing multi - document / sources . Most LLM are designed with important limitation in input and output lengths impacting size of input knowledge , reasoning length dependencies , answer size . Hybridization : 6 . 4 , 6 . 5 , 6 . 6 , 6 . 10 , 6 . 14 . Multi - modal search and reasoning - many knowledge and world model cannot be captured with text only and requires integration with tables , and images in understanding and answering . Hybridization : 6 . 16 . . . Training : 5 . 4 . 7 , 5 . 3 . 3 , 5 . 4 . 2 . . . Time dimension - handling time - based reasoning , knowledge update , and understanding sequences or workflows . Hybridization : 6 . 9 , 6 . 19 . Data sensitivity protection - utilizing and protecting sensitive data , such as private , intellectual property , organizational , or governmental sensitive data . Hybridization : 6 . 5 , 6 . 6 ; Reinforce - ment / experience : 8 . Experience / knowledge and skills capitalization [ 35 ] - like a human , it should be able to continually improve itself by experience on skills and knowledge using implicit and explicit feedback . Reinforcement / experience : 8 ; Hybridiza - tion : 6 . 3 , 6 . 9 , 6 . 12 , 6 . 13 , 6 . 18 . 4 Evaluating : metrics , cost functions , datasets The performance of language models on question answering can vary greatly depending on factors such as the domain , question complexity , necessary subtasks , bias , fairness , toxicity , and human expectations . A large language model may perform well overall but struggle with some type of questions ( see table " [ HELM ] SOTA QA multi - metrics ( 3 ) " and " [ BIG ] QA complex QA tasks benchmark ( 1 ) " or areas of evaluation , while a model that is specialized for certain questions or domains may perform poorly on more general tasks . The following section will examine different metrics and datasets used for evaluating and training these models . 4 . 1 Metrics & performance SOTA 4 . 1 . 1 Standard metrics There are a variety of metrics [ 36 ] that can be used to evaluate the performance of QA models , each with its own strengths and weaknesses . In this section , we will discuss some of the most common metrics used to evaluate QA models : Recall @ k Measures the proportion of relevant answers retrieved by the model among the top k answers . It is a measure of the model’s ability to find all the relevant answers , regardless of their position in the ranking . The main weaknesses are that it does not take into account the position of the relevant answer in the ranking and does not penalize irrelevant answers that appear in the top k . Accuracy @ k Measures the proportion of correct answers among the top k answers returned by the model . It is a measure of the model’s ability to return the correct answer and can be used to evaluate the performance of a model in a closed - domain QA task . As with recall @ k , the main weaknesses are that it does not take into account the position of the correct answer in the ranking and does not penalize irrelevant answers that appear in the top k . nDCG Normalized Discounted Cumulative Gain , a measure of ranking quality that takes into account the relevance and position of answers . It is often used in information retrieval and web search to measure , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 13 the effectiveness of a ranking algorithm . The main weaknesses are that it does not take into account the number of irrelevant answers in the ranking MAP Mean Average Precision , a measure of the quality of a set of ranked answers . It is a commonly used metric for evaluating QA models in open - domain tasks , where the model must return a list of possible answers . its main weakness is that it does not take into account the position of the correct answer in the ranking . MRR Mean Reciprocal Rank , a measure of the quality of a set of ranked answers , with a higher value indicating better performance . It is often used in information retrieval and web search to measure the effectiveness of a ranking algorithm . Weakness : It only considers the position of the first correct answer in the ranking . CWS Cross - entropy word - level perplexity , a measure of the model’s ability to predict the next word in a sentence . It is often used to evaluate the quality of the language model . Weakness : it cannot be used for answer quality and perplexity is not well correlated with a language model tasks performance . F1 ( Macro , Micro ) F1 score , a measure of a model’s accuracy that takes into account both precision and recall . It is used to evaluate the model’s performance in a classification task . F1 macro averages the per - class F1 scores , used for imbalanced datasets , F1 micro computes metrics globally by counting total true positives , false negatives , and false positives , used for balanced datasets . Weakness : It does not take into account the relative importance of false positives and false negatives . EM Exact Match , a binary metric that measures whether the model’s answer is exactly the same as the reference answer . It is often used in closed - domain QA tasks where the correct answer is a single word or phrase . Weakness : It is not able to capture the semantic similarity between the model’s answer and the reference answer . Jacard similarity measures similarity between two sets of data , based on the size of the intersection divided by the size of the union of the sets . Weakness : It does not take into account the overall size of the sets , which can lead to errors in similarity measurements . Cosine similarity measures similarity between two non - zero vectors based on the cosine of the angle between them . Weakness : It is sensitive to the magnitude of the vectors ( e . g . zero ) , which can lead to errors in similarity measurements . BLEU Bilingual Evaluation Understudy , a measure of the similarity between a model’s answer and a reference answer . It is commonly used in machine translation and natural language generation tasks . Weakness : It does not take into account the meaning of the words ( semantic similarity ) . ROUGE Recall - Oriented Understudy for Gisting Evaluation , a measure of the similarity between a model’s answer and a reference answer . It is commonly used in natural language summarization tasks . Additionally to BLEU , it takes into account the longest common sequence ( LCS ) . Weakness : it does not take into account the meaning of the words ( semantic similarity ) . METEOR Metric for Evaluation of Translation with Explicit ORdering , a measure of the similarity between a model’s answer and a reference answer . It is similar to BLEU , but takes into account word alignment and synonymy . Weakness : It is computationally expensive and can be sensitive to the reference translations used . Human evaluation human judges provides a subjective measure of the quality of the model’s answers . It is considered the gold standard for evaluating QA models . Weakness : it is time - consuming and can vary greatly depending on the individual evaluators and their level of expertise . 4 . 1 . 2 New metrics for free - form / natural language QA In the context of free - form QA standard metrics limits question complexity [ 37 ] and do not capture many good answers semantically close . So new metrics have been proposed using PLM having higher correlations to human expectations : BERTScore [ 38 ] compute a semantic similarity score through a sum of cosine similarities between the contextualized embeddings of answer tokens and those of the reference text . BARTScore [ 39 ] similar to BERTScore , it uses the more recent model BART pre - trained using a more robust technique ( denoising autoencoding ) . A BARTScore variant adds faithfulness in the measure . MAUVE [ 40 ] : similar to BERTScore and add divergence frontiers . It claims to better correlate with human judgement and identify quality differences . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 14 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco F i g . 3 . [ H E L M ] S O T A Q A m u l t i - m e t r i c s ( D e c e m b e r 2022 ) , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 15 T5Score this hybrid metric [ 41 ] based on mT5 model is not yet compared to date with MAUVE but is globally more robust than BERTScore and BARTScore . However the paper " On the Blind Spots of Model - Based Evaluation Metrics for Text Generation " [ 42 ] ( 2022 ) highlights that all those PLM based metrics have flaws , they could assign a high likelihood to degenerate , repetitive text and could be insensitive to perturbations such as word order shuffling , negation , etc . So those blind spots should be taken in account to compose and eval [ 43 ] the best metric for targeted complex QA application . 4 . 1 . 3 HELM multi - metrics and related findings The Holistic Evaluation of Language Models ( HELM [ 9 ] ) , after referencing a large space of targeted use cases of LLM with a focus on QA , has identified 7 key categories of metrics required to create useful systems : accuracy , calibration , robustness , fairness , bias , toxicity , and efficiency ( speed / cost ) . For accurate definition of theses categories of metrics please refer to HELM taxonomy [ 9 ] . From data provided by this project , we created a new table ( see [ HELM ] SOTA QA multi - metrics 4 . 1 ) assessing best performing models on each of this metrics in QA and a systematic comparison with the current global best performer ( REF ) . We can see that performance of models are unequal and even best performer is not the best choice depending on metrics preference regarding your needs . The best performing model in these metrics was InstructGPT from OpenAI [ 44 ] . A much smaller model ( Anthropic - LM v4 - s3 ) is close to the leading model which means it should cost much less to operate . The study observed a consistent gap between the current open models and non - open models in terms of accuracy . The gap has been reducing with the recent release of open models such as BLOOM ( 176B size ) [ 10 ] by BigScience , OPT ( 175B size ) [ 45 ] by Meta , and GLM ( 130B size ) [ 46 ] by Tsinghua University . Study shows that instruction - tuning can provide a broad set of advantages in terms of accuracy , robustness , and fairness metrics . The relationship between accuracy and calibration depends on the scenario ( task & domain ) and adaptation procedure , with some scenario showing trade - offs between accuracy and calibration . Across all scenarios , there is a strong correlation between accuracy , robustness , and fairness , but trade - offs where the most accurate model is not the most robust or fair . The study also found performance disparities in models when demographic metadata is available , and low overall average biases and toxicity in model generations , but notes that targeted evaluations are needed to obtain a more detailed characterization . The study also found that there is not a strong trade - off between accuracy and efficiency , but as models become larger , accuracy improves but with higher training and inference cost . Only a subset of all models are on the accuracy - efficiency Pareto frontier for each scenario . There is no model leading on all metrics , QA performances also vary depending on the scenario ( task & domain ) and model , so weighting or defining a decision tree among the 7 metrics then evaluating on target scenario is necessary for choosing your model . 4 . 1 . 4 Which metrics for " complex " QA ? Considering that complex QA is not well defined and answers highly depends on human expectations and values , we could not identify standard metrics in the literature . Ullrich and Geierhos [ 24 ] proposes to use Bloom taxonomy to assess question complexity but does not offer a metric for measuring the relevance of answers to question . Metrics identified in previous section for free - form QA could be used when there is a gold / reference answer but shall be unstable considering that a good non - factoid answer can be semantically distance to gold answer . In this survey we aim to answer complex questions with the following characteristics : non - factoid , multi - step ( requiring decomposition ) , multi - source of knowledge , higher order of reasoning questions . We could separately measure each skill of a language model on those characteristics to estimate the capacity to answer using decomposition . Using data from BIG bench , we created a summary evaluation of similar QA capacities ( see figure 1 [ BIG ] QA complex QA tasks benchmark ) . However , it will not assess the end - to - end capacity to provide a relevant final answer . Current systems like ChatGPT [ 47 ] which solves some complex questions , with high differences in quality but a clear improvement curve , used human feedback [ 32 ] measurement ( e . g . ranking / preference ) . A path investigated in papers WebGPT [ 48 ] and Constitutional AI paper [ 25 ] is to build one or several Elo scores mapping human preferences competing on different axes ( e . g . helpfulness vs harmlessness , compute efficiency ) and maximizing frontiers . 4 . 1 . 5 Explainability , truthfulness , hallucination metrics Recent conferences highlights the recurrent problem of hallucination [ 34 ] and urge for explainability [ 49 , 50 ] and truthfulness [ 51 , 52 ] when delivering an answer . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 16 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Conventional metrics measuring quality of writing including answers are not adequate for quantifying the level of hallucination [ 34 ] . There are no mature metrics except human , first proposed are : – statistical metrics mainly focus on lexical matching [ 34 ] such as PARENT - T , bag - of - vectors sentence similarity ( BVSS ) [ 53 ] , Knowledge F1 . – model based metrics expect to handle more complex syntactic and semantic variations but mainly focus the generation compared to known gold answer : FEQA [ 54 ] , QAGS [ 55 ] , QuestEval [ 56 ] – human evaluation is the most commonly used method considering the currently imperfect automatic methods , most common usages are : ( 1 ) scoring : annotator rate the evaluation level out of a range ; ( 2 ) comparing : annotator compares the output texts with baselines or ground - truth references . We identified three types of explanation in the literature [ 50 ] : highlights , free - text , and structured expla - nations . Those explanations could be intrinsic , explaining LM internal logic , or extrinsic , related to external sources and proofs . Therefore some metrics for the extrinsic explanation could be the number of sources , authority and reliability of sources . The intrinsic explanation could be measured trough their quality : com - pactness ( short and coherent ) , sufficiency , comprehensiveness . Some indirect metrics could be related to the explaination task performance ( source identification , fact - checking , coherence . . . ) . Explaination are usually evaluated on plausibility and faithfulness ( coherent decision process ) , a common approach is to provide a chain of facts that detail the reasoning steps to reach an answer . For further details , we invite you to look into main references [ 34 , 49 , 50 , 52 ] , the taxonomy in the table 3 " Taxonomy of existing explainable evaluation metrics " [ 49 ] , and research topics in the further section " Hallucination & credibility " . Table 3 . Taxonomy of existing explainable evaluation metrics ( extracted from table of Leiter et al . [ 49 ] ) Work Type Method Goals Eval4NLP 2021 : ( Fomicheva et al . 2021 ) Various Rubino , Fujita , and Marie ( 2021 ) FI Expl . by Design AL Treviso et al . ( 2021 ) FI Various AL SemEval 15 / 16 : ( Agirre et al . 2015 , 2016 ) Various Magnolini , Feltracco , and Magnini ( 2016 ) CAl Neural Networks AL , E Yuan , Neubig , and Liu ( 2021 ) CA Generation Prob . E Adversarial Attacks ( Section 7 ) EbE Perturbations D , E Kaster , Zhao , and Eger ( 2021 ) EbS / CA Linear Regression D , E Sai et al . ( 2021 ) CA Perturbations B , D , E The first column is the research work reference . The second is the explanation types : Concept Attribution ( CA ) , Chunk Alignment ( CAl ) , Feature Importance ( FI ) , Explanation by Example ( EbE ) and Explanation by Simplification ( EbS ) . The column “Goals” specifies which aspect is measured amongst ( B ) ias detection , ( D ) iagnosis , ( E ) xpressiveness and automated labeling ( AL ) . 4 . 1 . 6 Domain / task matrix of performance As performance of a model is unequal depending on knowledge domain and tasks [ 11 ] [ 10 ] [ 9 ] , metrics assessment should be segmented per knowledge domain & tasks within a matrix of comparison or database like in the 42 scenarios of HELM [ 9 ] . 4 . 2 Cost functions Cross - entropy loss is the main objective function used for language models training . This function is adapted to each training objective ( see sections 5 . 3 , 5 . 4 , 5 . 5 , 5 . 6 ) and some complex implementations can be done for knowledge distillation [ 57 ] . We will see in later section that it is common to add a reinforcement learning mechanism to align QA with human’s expectations . In this case , the cost function will be hybrid by adding to the LM cross - entropy loss , the RL reward from the human supervisor [ 58 ] . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 17 4 . 3 Datasets To train and evaluate QA / CQA systems , a variety of datasets have been developed to cover main skills , tasks and knowledge which we reference in the following sections . They can assess current performance or train model from end - to - end question answering on different field and complexity , to specific logic or task , such as decomposition . We also cover the generation of datasets or improvement of existing ones to overcome quality issues or challenge to create domain or logic specific dataset . 4 . 3 . 1 QA / CQA text datasets ( monomodal ) Major usage and datasets of QA / CQA focus on text sources : MS Marco ( Microsoft Machine Reading Comprehension Dataset ) : largest publicly available collection of relevance judgments , with 100 , 000 to 1 , 000 , 000 human generated QA , it has been central to the progress in neural IR / QA over the past several years ( standard QA task , human performance : Rouge - L : 0 . 539 , BLEU - 1 : 0 . 485 ) [ 9 , 59 ] . SQUAD ( Stanford Question Answering Dataset ) [ 60 ] : 100 , 000 + questions posed by crowdworkers on a set of Wikipedia articles ( human performance F1 - score : 86 . 8 % ) . SQuAD v2 [ 61 ] : add 50 , 000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones , to avoid training on unreliable guesses on questions ( human performance F1 - score : 86 . 8 % ) . TriviaQA [ 62 ] : 650K question - answer - evidence triples . TriviaQA includes 95K question - answer pairs authored by trivia enthusiasts and independently gathered evidence documents , six per question on average , with distant supervision for answering . In comparison to other QA datasets in 2017 , TriviaQA ( 1 ) had relatively complex , compositional questions , ( 2 ) considerable syntactic and lexical variability between questions and corresponding answer - evidence sentences , and ( 3 ) required more cross sentence reasoning to find answers . MMLU ( Measuring Massive Multitask Language Understanding ) [ 63 ] : 15 , 908 multiple - choice questions packages across 57 different tasks / datasets from subjects in the humanities , social sciences , hard sciences , and many others ( unspecialized human performance accuracy : 34 . 5 % ) . NarrativeQA [ 64 ] : 46 , 765 human generated questions & answers requiring understanding of stories ( books , movie scripts ) requiring summarization ( human performance : Bleu - 1 : 44 . 3 , Bleu - 4 : 18 . 9 , Me - teor : 24 . 0 , Rouge - L : 57 . 1 ) NaturalQuestions ( closed - book , open - book ) [ 65 ] : 323 , 000 questions and documents ( full dataset is 42Gb ) consisting of real anonymized , aggregated queries from Google search engine providing several long documents ( e . g . 5 wikipeda pages ) with a long answer ( e . g . a paragraph ) and a short answer ( one or more entities ) if present on the pages , or marks null if no long / short answer is present ( standard human performance : short answers F1 : 57 . 5 % , long answer F1 : 73 . 4 % ) . QuAC [ 66 ] : > 100 , 000 questions and their corresponding answers , based on dialogues between two persons where many questions requires understanding of the dialog ( human performance F1 : 80 . 9 % ) . Semi - structured datasets : semi - structured data with tables - and - text are abundant on the web and in companies . Wang et al . [ 67 ] list the following datasets : HybridQA , OTT - QA , GeoTSQA , FinQA , TAT - QA , TAT - HQA , MultiHiertt . Semi - structured dataset could also be samples of JSON , XML . . . . For Structured , graph , table only or SQL like data ( not bundled with text ) , Rogers et al in " QA Dataset Explosion : A Taxonomy of NLP Resources for Question Answeringand Reading Comprehension " [ 13 ] list the following datasets : WikiTableQuestions , TableQA , WikiSQL , WikiOps . CQA on knowledge bases datasets : knowledge bases like ontologies and knowledge graphs offers valuables structured symbolic data ( e . g . Wikidata , all resources from lod - cloud . net . . . ) not always easy to query to non experts . Lan et al . [ 68 ] list the following datasets : WebQuestions , ComplexQuestions , WebQuestionsSP , ComplexWebQuestions , QALD series , LC - QuAD , LC - QuAD 2 . 0 , MetaQA Vanilla , CFQ , GrailQA , KQA Pro . Domain specific datasets are numerous such MedQA [ 69 ] and TREC - COVID [ 70 ] in medical sector with the later focusing on Covid19 , or Qasper QA about research papers in NLP [ 71 ] . 4 . 3 . 2 Multimodal QA datasets When answering a question , humans build [ 72 ] a coherent understanding of the world by actively exploring and reasoning over multiple evidences ( multi - hop ) from different modalities , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 18 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Table 4 . Multi - hop QA datasets ( data from Mavi et al . [ 89 ] ) - OD : Open domain context , MCQ A : multi - choice question with A being the number of possible answers Contextgranularity Dataset Totalcontextofdataset Contextsource Domain Numberofquestions Context perquestion Average # hops Answertype Passage HotpotQA Wikipedia Wikipedia Generic 112 , 779 10 / 0ODa 1 / 2 / 3 Span Table , Passage HybridQA Tables : 13k Passages : 293k Wikitables , Wikipedia Generic 69611 1tablepassages 2 / 3 Span Sentence NarrativeQA Books : 783 Movies : 789 Multiple Fiction 46765 1story - Generative Sentence MultiRC 871 Multiple Generic 9872 1passage 2 . 37 MCQA : 5 . 44 Passage Medhop Medline Medline Medicine 2508 OD - MCQA : 8 . 9 Passage Wikihop | Wikipedia | Wikipedia Generic 51318 OD - MCQA1 : 19 . 8 Sentence QASC Core : 928 Other : 7672 WorldTree Science 9980 OD 2 MCQA : 8 Sentence OpenBookQA Core : 1326 Other : 6000 WorldTree Science 5947 OD 2 MCQA : 4 ( multimodal ) . Therefore , natural QA requires to leverage more than text ( natural or structured ) like images , videos , sensors . . . Training & evaluation datasets are emerging like : Image QA datasets : recent survey on visual QA and visual reasoning [ 73 ] provides a full list of im - ages / visual question - answering ( VQA ) including reasoning tasks . Audio QA datasets : DAQA [ 74 ] on audio temporal reasoning , Clotho - AQA [ 75 ] on binary and multi - choice audio QA . Video QA datasets : suchas VideoQA [ 76 ] formulti - domain , MovieQA [ 77 ] / MovieFIB [ 78 ] / TVQA [ 79 ] / KnowIT VQA [ 80 ] for movies and shows , MarioQA [ 81 ] for games , PororoQA [ 82 ] for cartoons , TurorialVQA [ 83 ] for tutorials , CLEVRER [ 84 ] for physical & causal reasoning . Multi - modal QA datasets : MultiModalQA / MMQA [ 85 ] for multi - modal and multi - hop QA , WebQA [ 86 ] on web multi - modal QA , MAQA focus on negation learning and testing [ 87 ] . Unified dataset format is proposed by Xie et al . [ 88 ] to unify multiple formats of different modality to enable training , inference and evaluation on multi - tasks and sources . 4 . 3 . 3 Structured knowledge datasets As seen in first section " QA / CQA text datasets ( monomodal ) " , we can use structured ( e . g . RDBMS , graph , table ) or semi - structured ( e . g . table and text , JSON samples ) datasets to learn to extract factual information from structured knowledge sources in natural language . These datasets are also key to improve general or domain specific reasoning abilities as we see in further section 4 . 3 . 6 . 4 . 3 . 4 Decomposition and multi - hop datasets Decomposition skill is required for CQA to break down com - plexity , and the related ability to resolve it in multiple hops or steps . Table 4 shows typical multi - hop QA datasets [ 89 ] with number of hops ( steps ) required to answer , question with context description and complexity , answer format type . BIG [ 11 ] also provides advanced decomposition and multi - step tasks datasets in 4 . 3 . 6 such as strategyQA or multistep arithmetics . In order to improve problem specific decomposition and resolution ability , emerging datasets are providing reasoning decomposition examples to be provided in context like chain - of - thoughts ( e . g . FLAN CoT dataset [ 90 ] ) . They are mainly used as examples to be provided with the question but could be also used at training . In a different manner , Galactica model was trained on scientific papers where step - by - step reasoning were wrapped between 2 tokens < WORK > [ 91 ] both to explicitly learn reasoning and activate working memory which lacks in standard LLM . 4 . 3 . 5 Instructions ( IFT , CoT ) datasets Instructions fine - tuning ( IFT ) are collection of written instructions used to teach model user intent declaration to solution logic & format which can be model generated such as : Unnat - ural Instructions : [ 92 ] , large community effort Super - natural instructions [ 93 ] , small high - quality crafted [ 94 ] , converted existing large datasets to instructions [ 95 , 96 ] , NaturalInstructions [ 97 ] , PromptSource [ 98 ] . 4 . 3 . 6 Reasoning datasets Dedicated datasets for specific reasoning abilities [ 99 ] have been developed , or existing sets could be derived to take advantage of different abilities . ( 1 ) spatial reasoning : bAbI [ 100 ] , SpartQA [ 101 ] ( 2 ) temporalreasoning : eventorder ( QuAIL [ 102 ] , TORQUE [ 103 ] ) , eventattributiontotime ( TEQUILA [ 104 ] , TempQuestions [ 105 ] , script knowledge ( MCScript [ 106 ] ) , event duration ( MCTACO [ 107 ] , QuAIL [ 102 ] ) , , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 19 temporal commonsense knowledge ( MCTACO [ 107 ] , TIMEDIAL [ 108 ] ) , factoid / news questions with answers where the correct answers change with time ( ArchivalQA [ 109 ] , SituatedQA [ 110 ] ) , temporal reasoning in multimodal setting [ DAGA [ 111 ] , TGIF - QA [ 112 ] ; ( 3 ) belief states : Event2Mind [ 113 ] , QuAIL [ 102 ] ; ( 4 ) causal relations : ROPES [ 114 ] , QuAIL [ 102 ] , QuaRTz [ 115 ] , ESTER [ 116 ] ; ( 5 ) other relations between events : subevents , conditionals , counterfactuals etc . ESTER [ 116 ] ; ( 6 ) entity properties and relations : 20 social interactions ( SocialIQa [ 117 ] ) , properties of characters ( QuAIL [ 102 ] ) , physicalproperties ( PIQA [ 118 ] , QuaRel [ 119 ] ) , numericalproperties ( NumberSense [ 120 ] ) ; ( 7 ) tracking entities : across locations ( bAbI [ arXiv : 1502 . 05698 ] ) , in coreference chains ( Quoref [ 121 ] , re - sources in the Winograd Schema Challenge family [ 122 ] ) . Arguably the cloze - style resources based on named entities also fall into this category ( CBT [ 123 ] , CNN / DailyMail [ 124 ] , WhoDidWhat [ 125 ] ) 4 . 3 . 7 BIG bench : complex QA subtasks datasets - The BIG - Bench is a benchmark consisting of 204 tasks and associated datasets on diverse problems from linguistics , childhood development , math , common - sense reasoning , biology , physics , social bias , software development , and other . Many of those tasks are related to QA and IR or could be a subtasks of a complex question anwering pipeline . We created the table 1 illustrating such substasks ( most representative are in bold ) with associated metric , average human performance , top human expert performance , max state - of - the - art performance with the model name , and ratio between this model performance and human performance . 4 . 3 . 8 Explainable & truthfulness QA datasets The veracity and explainability of an answer is a significant challenge for language models where answers are mostly provided without evidence , logic , confidence / trust . Explainability can be trained by models or evaluated through different tasks like source highlighting or URL providing and importance , claim check , commonsense check , answer explanation , logic check . . . Leiter et al [ 49 ] propose three types of ground truth explanations : highlights ( rationales or feature im - portance ) , free - text explanations , and structured explanations . In table 5 " Explainability tasks datasets " we enriched an existing comparison [ 50 ] listing important datasets with explainability tasks in field with number of instances , mode of creation , explanation type and format , task . Rogers et al . [ 13 ] proposes an " evidence format " for the explainable part of a dataset composed of Modality ( Unstructured text , Semi - structured text , Structured knowledge , Images , Audio , Video , Other combinations ) and Amount of evidence ( Single source , Multiple sources , Partial source , No sources ) . 4 . 3 . 9 Local & multi - lingual datasets Multilingual datasets can not only help to assess or expand the answering skills to new languages , but also to transfer or share concepts between languages : CCQA [ 126 ] focuses on general QA pre - training , MLQA [ 127 ] on extractive question answering , MKQA [ 128 ] on open domain QA , TydiQA [ 129 ] is a topologically diverse QA dataset to learn more robust concept and answer without the need of translation , XQuAD [ 130 , 131 ] demonstrates performance on pre - training mono - lingual and fine - tuning on new languages , MGSM [ 132 ] deals with solving math problems and transfering reasoning abilities across 10 languages . 4 . 3 . 10 Dialogue datasets The " Dialogue System Technology Challenge " started as an initiative to provide a common testbed on dialog state tracking and is now a reference in terms of dialog dataset [ 133 ] with each year several tacks ( challenge ) . The most recent 10 tracks were released for DSTC10 and DSTC11 in 2022 . Some other reference datasets are : – CommaQA [ 134 ] on complex tasks solving by talking to agents . – SODA [ 135 ] with million exchange with social commonsense contextualization . – DeliData [ 136 ] for multi - party problem solving with deliberation . – TIMEDIAL [ 108 ] for temporal commonsense reasoning . 4 . 3 . 11 Generate ( e . g . synthesis ) or improve datasets Creating datasets is very expensive but often necessary for domain adaptation . A growing trend is the generation of synthetic QA datasets from models [ 137 ] or unstructured text using different techniques such as ICT [ 138 ] , GPL [ 139 ] , GenQ [ 140 ] , Promptagator [ 141 ] , COCO - DR [ 142 ] . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 20 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Table 5 . Explainability tasks datasets ( data from Wiegreffe and Marasović [ 50 ] enriched ) Extype Dataset Task ExFormat Madeby # Instances highlight TRUTHFULQA checkfalsebelief & refinQA URLref experts 817 highlight TriviaQAwithevidenceorfiltered QAwithevidence URLref crowd & experts 95k / 2099 highlight FEVERorfiltered ( GopherCite ) verifyingclaimsfromtext sentencesorURLs crowd 136K highlight WIKIQA open - domainQA sentence crowd + authors 1 , 473 highlight MULTIRC readingcomprehensionQA sentences crowd 5 , 825 highlight HOTPOTQA readingcomprehensionQA sentences crowd 112 , 779 highlight Hanselowskietal . verifyingclaimsfromtext sentences crowd 6 , 422 ( varies ) highlight CoQA conversationalQA none crowd 127K ( 1or3 ) highlight COS - Ev1 . 0 [ 100 ] commonsenseQA none crowd 8 , 56 highlight COS - Ev1 . 11 commonsenseQA none crowd 10 , 962 highlight BOOLQ readingcomprehensionQA none crowd 199 highlight SCIFACT verifyingclaimsfromtext 1 - 3sentences experts 995 ( 1 - 3 ) highlight Kutluetal . webpagerelevanceranking 2 - 3sentences crowd 700 ( 15 ) free - text NaturalQuestions QAwithevidence sentences crowd & experts 323k / 7638 orfilteredversion ( GopherCite ) free - text ELI5orfiltered ( GopherCite ) longQA sentences crowd & experts 270k / 3999 free - text Jansenetal . scienceexamQA authors 363 free - text Lingetal . solvingalgebraicwordproblems auto + crowd 101K free - text LIAR - PLUS verifyingclaimsfromtext auto 12 , 836 free - text COS - Ev1 . 0 [ 100 ] commonsenseQA crowd 8 , 56 free - text COS - Ev1 . 11 commonsenseQA crowd 10 , 962 free - text ECQA commonsenseQA crowd 10 , 962 free - text PUBHEALTH verifyingclaimsfromtext auto 11 , 832 free - text ESPRIT reasoningaboutqualitativephysics crowd 2441 ( 2 ) structured ProofWriter ReasoningQAwithproof rules , QA , chainoffacts 500k structured WORLDTREEV1 scienceexamQA explanationgraphs authors 1 , 68 structured OPENBOOKQA open - bookscienceQA 1factfromWORLDTREE crowd 5 , 957 structured WORLDTREEV2 scienceexamQA explanationgraphs experts 5 , 1 structured QED readingcomp . QA inferencerules authors 8 , 991 structured QASC scienceexamQA 2 - factchain authors + crowd 9 , 98 structured EQASC scienceexamQA 2 - factchain auto + crowd 9 , 980 ( 10 ) structured Yeetal . SQUADQA semi - structuredtext crowd + authors 164 structured Yeetal . NATURALQUESTIONSQA semi - structuredtext crowd + authors 109 structured R4C readingcomp . QA chainsoffacts crowd 4 , 588 ( 3 ) structured STRATEGYQA implicitreasoningQA reasoningstepsw / highlights crowd 2 , 780 ( 3 ) Dataset generation can also be used to robustify reasoning skills learning [ 143 ] by converting an existing QA dataset provided with question decomposition meaning representation ( QDMR ) to generate new contexts and questions . Some other technics like natural language augmentation [ 144 ] aims at enriching existing datasets for a more robust training through transformation and data filtering . An interesting paper from Yuan and Liu [ 145 ] highlights the " signals " present in datasets for learning knowledge and capabilities . They propose the RST [ 145 ] method including restructuring pre - training dataset , such as enriching to highly improve model performance learnt from the same dataset . 5 Solving with training Now that we surveyed the skills we need to develop , the tasks and challenges to solve , the datasets needed for training , the specializable blocks on skills of a hybrid architecture on which to dispatch our task , how to train for complex QA ? We will see the importance of pre - training , domain adaptation and fine - tuning . 5 . 1 Training dataset quality Even before pre - training , a very important step is to maximize the quality of the input datasets ( eg , accuracy , completeness , consistency , relevance , uniformity ) [ 146 , 147 ] . It is commonly said that machine learning project could spend up to 80 % of time on data preparation with a major part dedicated to data cleaning [ 148 ] . The efficiency of a model task is directly and heavily affected by the quality of the training dataset and its improvement [ 146 , 147 ] . We will not dig this subject because , surprisingly , our review methodology did not preselect any scientific article with the word " quality " in their title and is anecdotal in their abstracts . It might be that it is not specific to QA / CQA or language models training but an assumption in any machine learning , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 21 related subject . Developers of GPT - 3 spent important efforts to filter on a high - quality training dataset [ 20 ] , and Sun et al . [ 149 ] highlights this important preparation task of complex question answering . To improve the quality of your data , please refer to the citations above and Yuan and Liu [ 145 ] which augments the learning signals , a quality aspect , of already high - quality datasets to improve LLM learning . 5 . 2 Type of LLM training As introduced in the " core concepts " section , training can apply to : ( 1 ) a pretrained language model ( PLM ) which is trained without supervision ( unsupervised or self - supervised ) mainly on large text ( e . g . reddit , Wikipedia . . . ) to discover general knowledge and logic . It could then be re - used and augmented with a " head " to be further trained ( fine - tuned ) on specific tasks ( supervised training ) . It can also be trained on very large corpus of data ( e . g . GPT - 3 . 5 ) to uncover enough knowledge and logic to be used " as is " without additional training but oftenly some additional instructions to better align with requester expectations ( e . g . ChatGPT ) . Kalyan et al ( 2021 ) [ 150 ] identifies several type of pre - training : Pretraining from Scratch ( PTS ) , Continual Pretraining ( CPT - initialization from an existing PLM ) , Simultaneous Pretraining ( SPT - synchronized mix of general and domain - specific corpus from beginning ) , Task Adaptive Pretraining ( TAPT - continually adapts mix of general and specific training examples ) , Knowledge Inherited Pretraining ( KIPT [ 151 ] - adds knowledge distillation in the process ) . ( 2 ) fine - tuned model on specific task ( s ) in a supervised manner ( each training example is provided with input and expected output solution ) , by re - purposing a pre - trained language model . ( 3 ) adapting an existing model ( PLM or fine - tuned model ) to a new domain of knowledge ( domain adaptation - e . g . COVID19 terms and facts ) or to new task ( s ) ( knowledge transfer ) leveraging existing knowledge and logic in the model . 5 . 3 Pre - training techniques 5 . 3 . 1 Self - supervised learning ( SSL ) This type of machine learning technique , largely used for PLM , trains on a dataset without any pre - labeled outputs . Instead , the model must learn to generate the correct output itself , using information from the input data . It is often based on an unlabeled training converted to a supervised coherence task . Kalyan et al ( 2021 ) [ 150 ] identify three major techniques : Generative SSL , depending on chosen technique , the model learns to predict different scenarios : ( 1 ) next token based on current tokens ( CLM - used by GPT - 1 , GPT - 2 , GPT - 3 models ) ; ( 2 ) masked tokens in a phrase ( MLM [ 18 ] is most used technique , but variants exists such as TLM [ 152 ] , Seq2SeqLM - used by RoBERTa , XLM , XLM - R models ) ; ( 3 ) reconstruct original text which has been corrupted ( denoising autoencoder , DAE , is used by BART , mBART models ) . Contrastive SSL augments learning by comparison . It is not used alone but to further improve a model like in continual pretraining , to learn sentence - level semantics . Different techniques exist such as next - sentence prediction NSP [ 18 ] , sentence order prediction SOP [ 153 ] , simple contrastive learning with SimCSE [ 154 ] or SimCLR [ 155 ] or MoCo - v2 [ 156 ] , bootstrapping with BYOL [ 157 ] , cross lingual contrastive pretraining with XLCo [ 131 ] . Adversarial SSL learns by distinguishing corrupted tokens ( replaced or shuffled ) , can be used alone or in continual pretraining like contrastive . Different techniques exist : replaced token detection ( RTD - used by ELECTRA model ) , multi - lingual replaced token detection ( MRTD ) is used by XLM - E model , translation replaced token detection ( TRTD ) , shuffled token detection ( STD ) is used by RoBERTa model . Hybrid SSL uses more than one type of SSL - e . g . U - Palm uses up to 7 denoising objectives ( mixtures - of - denoisers [ 158 ] ) , BERT uses MLM ( generative ) and NSP ( contrastive ) , ALBERT used MLM and SOP ( contrastive ) , infoXLM uses MLM + TLM ( generative ) and XLCo ( contrastive ) . Here XLCo represents the cross lingual contrastive pretraining task . 5 . 3 . 2 Transfer learning , domain adaptation , knowledge distillation Those techniques are also used as supervised learning & finetuning which we cover in the next section . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 22 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco 5 . 3 . 3 Program execution learning This technique [ 159 ] tries to learn , or mimic , how a program works to capture its logic on the specific scope of the program or more general skills like numerical reasoning , logical reasoning , better multi - hop reasoning . This technique useful at pre - training stage can be viewed as a self - supervised learning . 5 . 4 Supervised learning & fine - tuning Supervised learning is the ancestor and most well - known ML technique . It trains on labeled dataset to predict the expected ouput from given input . This allows the model to learn from the data and make predictions about new , unseen data but similar task . This assumes that dataset is representative of new , unseen data . We will see in sections 5 . 5 and 7 that task specific fine - tuning , which can require a lot of compute and examples , can be avoided via complementary strategies like prompt engineering , tuning adapters , soft prompts prefix , late prompts . 5 . 4 . 1 ( Task specific ) Vanilla Fine - Tuning Vanilla fine - tuning , is commonly used to refer to the basic or standard method of fine - tuning a pre - trained deep learning model based on task - specific loss , the weights of the few layers near the output ( the tasks specific head ) are updated while keeping the PLM weights fixed . Kalyan et al . [ 150 ] highlights that the main drawback is that PLM having large parameters is prone to overfit on small task specific datasets limiting performance . Intermediate fine - tuning or multi - task fine - tuning overcome this . 5 . 4 . 2 Multi - task learning According to Kalyan et al . [ 150 ] , training a model to perform multiple differ - ent tasks can help the model to learn more generalizable features ( regularization effect ) , and improve its performance on multiple tasks ( transverse knowledge and skills acquired from multiple datasets ) . From MQAN ( McCann et al . [ 160 ] ) passing the decathlon of QA , and T5 ( 2019 ) [ 21 ] reaching many state - of - the - art benchmarks in one model , the field does not stop improving . This learning can be done : simultaneously on all tasks [ 161 ] , in sequence [ 162 ] , mixed [ 163 ] , or optimized learning per task with hypernetwork ( e . g . imbalanced ) [ 164 ] . Multi - task learning can spread on similar tasks from different domains and cross - language ( e . g . similar summarization tasks [ 165 , 166 ] ) , or auxiliary tasks [ 167 ] to improve different skills . 5 . 4 . 3 Instruction fine - tuning [ 90 , 92 , 94 ] Recent work from Chung et al [ 90 ] , additional to multi - task learning , demonstrates the capacity to highly increase the number of tasks , the reasoning capabilities and global performance by finetuning a pre - trained multi - task model with example with instructions . 5 . 4 . 4 Transfer learning , knowledge & domain adaptation , continual learning Those techniques leverage an already trained model which captured expected knowledge and / or logic for my target domain or application , which we then fine - tune using techniques explained above . This enables faster adaptation to target usage and allows to address a task even when there is not enough data available . Transfer learning could be further divided into inductive ( related task ) and transductive ( same task , new domain ) learning , unlabeled to labeled transfer ( similar to unsupervised pre - training to fine - tuning ) , and feature and parameter transfer ( capture high level concepts of domain ) . A model can be very efficient on a given knowledge domain but will be later enable to process new questions requiring new or updated facts , this highlights the need for continual knowledge update in models which can be covered with continual learning technique [ 168 , 169 ] . 5 . 4 . 5 Knowledge distillation ( KD ) [ 170 ] This technique enables a smaller , more efficient model ( student ) to be trained to imitate the predictions of a larger , more complex model ( teacher ) , leveraging the knowledge learned by the larger model . For a given QA question / answer pair , it not only provides answer but could provide confidence , attention map and activated features . The KD can be jointly used with active learning to reduce even more the training examples needed . 5 . 4 . 6 Active learning [ 170 – 174 ] This technique enables a language model to be trained on a small initial set of examples and then , in a iterative manner , the model can request additional labeled data based on its own uncertainty , in order to improve its accuracy on a given task with minimal effort . This highly reduces the training time and the manual creation of a dataset when required . This can also help to craft better examples and avoid overfitting due to excessive examples on a subject . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 23 5 . 4 . 7 Meta learning [ 93 , 175 , 176 ] Meta Learning is the ability to learn faster new tasks with lesser data and time , like " learning to learn " [ 177 , 178 ] . This ability is well used through the capacity to learn instructions to be addressed to a language model with the example of Tk - INSTRUCT supporting > 1600 NLP tasks from 76 types reaching a performance near SOTA supervised tasks [ 93 ] . 5 . 4 . 8 Multi - view learning [ 179 , 180 ] This technique learns multiple representations or " views " of the same input data to improve the model’s performance on a specific task . The idea is to leverage those different representations to better capture different knowledge facets or aspects of the data , leading to a more nuanced and effective representation for the task at hand . In the case of question - driven summarization , multi - view learning is used to model the relevance to the question and the interrelation among different sentences to generate a more informed and justified summary . This can improve the performance of the model compared to using a single view of the data , as the model is able to capture more diverse and complementary information from multiple perspectives . 5 . 4 . 9 Reinforcement learning [ 26 , 181 ] , Inverse reinforcement learning [ 182 ] Reinforcement learning learns by interacting with its environment and later receiving evaluation for its actions ( e . g . rewards vs punishments ) . Model can then infer the best policy to maximize expected result . This technique is used when teaching to a model exact output for each input is non trivial or evaluation is indirect ( later reward ) . Reinforcement learning with human feedback is a key technique used to improve answer alignment to human requester expectations and values . For example a QA dialog system can generate possible responses to a user request , and then a human moderator provides feedback on which response is the best . This feedback is then used to uncover best factors leading to expected result and update the system policy so that it produces better responses in the future . This key element of future CQA systems design is further discussed in section 8 . 5 . 4 . 10 Intermediate Fine - Tuning Intermediate Fine - Tuning ( IFT ) fine - tunes a model using an intermediate dataset with a large number of labeled instances to learn domain knowledge ( DAIFT - domain adaptative IFT ) or task logic ( TAIFT - task adaptative IFT ) to avoid overfitting on small final datasets . However , Kalyan et al . [ 150 ] warns that IFT may sometimes reduce the performance on final tasks [ 183 ] but authors showed that tasks requiring high - level inference and reasoning abilities work best such as QA tasks . 5 . 5 Parameter - efficient tuning ( PEFT ) of a frozen PLM As per HELM study [ 9 ] , larger models lead to better performance although some architecture allows to get them a bit smaller . Those large - scale PLM are very expensive to retrain or just fine - tune . The parameter - efficient tuning alternative targets a small fraction of model parameter update with similar performance than full - model fine - tuning , sometimes better [ 184 ] . ( 1 ) Addition - based methods introduce extra trainable neural modules or parameters that do not exist in the original model or process ( Adapters - based Tuning , Prompt - based Tuning ) . – Adapters - based Tuning . It works by adding small adapter layer modules to a PLM and only updating its own parameters when learning a task . He et al . [ 185 ] demonstrate adapter - based tuning outperforms fine - tuning on low - resource and cross - lingual tasks , is more robust to overfitting and less sensitive to changes in learning rates . Prompt - free tuning uses task - specific adapters and learnable multi - token label embeddings to enable few - shot fine - tuning with frozen PLM [ 186 ] . – Prompt - based Tuning including Prefix tuning [ 88 , 187 ] . This technique allows to integrate addi - tional knowledge for a given task , such as QA or question driven summarization , into a pre - training strategy without modifying the PLM . This prefix is learnt by training on the task with the same type of dataset but requires few examples . Then , it is injected as knowledge when running the target task . This can highly improve the performance of the target task with a small number of trainable parameters ( e . g . 0 . 1 % ) . – Late prompt tuning [ 188 ] . ( 2 ) Specification - based methods specify certain parameters in the original model to become trainable , while others are frozen , this selection is either based on heuristic , either learned . ( 3 ) Reparameterization - based methods re - parameterize existing parameters by transformation - one major technique is LoRA [ 189 ] which injects low rank trainable matrices to target layers to optimize , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 24 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco them while keeping model weights frozen claims massive training speedup and resources reduction . LoRA matches fine - tuning performance on the GLUE benchmark . ( 4 ) Some new approaches emerge such as hypernetwork [ 190 ] which produce soft prefixes or adapters through just one forward pass enabling to quickly adapt to unseen tasks . 5 . 6 Techniques for improving training Additionally to the different training options , these are complementary techniques to improve different aspects : Regularization and early - stopping techniques . Like in other deep learning training , dropout , weight decay , and early stopping are important to prevents over - fitting , reduce complexity and help to learn more generalizable features . Additionally , early stopping strategies while training reduces time and avoids over - fitting [ 191 ] . Additional mixture - of - denoisers [ 158 ] training . UL2R proposes to additionally train for few itera - tions ( 0 . 1 % ) an already trained LM to largely improve its capabilities in terms of accuracy and reasoning capabilities [ 192 ] . Tasks disambiguation for generalization . Training on ambiguous examples in different contexts [ 193 ] can dramatically improve the accuracy of language models trained without large - scale human feedback training ( RLHF ) . Red teaming for secure / harmless content . Automatic red teaming [ 194 , 195 ] are different methods to automatically generate test cases to detect offensive content in LM . Post pruning . A pruning method like SparseGPT [ 196 ] can reduce model size by 50 % of a very large model ( GPT family ) in one - shot without any retraining with a minimal loss of accuracy . Cross - lingual learning . [ 131 , 152 ] Learning from multiple languages can help the model to learn language - agnostic features , knowledge not available on target knowledge , and improve its performance on tasks that involve multiple languages . 6 Solving with hybridization ( architecture composition ) To address the different limits of LLM and skills identified for complex QA , we referenced architectural components which could augment a general - purpose base LLM such as a task - specific fine tuned model , a search engine , a software , a code interpreter . . . To help design the different ways to increase an LLM both at inference or training , even if some may overlap , we propose to classify them into the following list of key hybrid architectural patterns each with description , strengths ( S ) , weaknesses ( W ) , illustrations ( e . g . ) . 1 . LLM base transformer : model usually trained on a large corpus of web resources to properly model target language ( s ) and various degrees of knowledge , common sense , reasoning capabilities . It is the core building bloc of all the architectures below . S : leverage knowledge from large unstructured data , can handle wide and versatile knowledge , some long - range dependencies and reasoning , and serves as the foundation for all subsequent architectures . W : training is too long / costly to allow frequent update , prones to hallucinate with confidence , limited reasoning without massive scale , no mechanism to protect sensitive data . e . g . encoders BERT [ 18 ] or its optimized version RoBERTa [ 19 ] ; decoders GPT2 / GPT - 3 [ 197 ] ; encoders - decoders T5 [ 21 ] or BART . 2 . LLM + Task - specific head : connected layers or separated module trained to specialize on a particular task using output of LLM . . S : achieves SOTA performance for targeted task and / or domain with lower computational resources than retraining an entire LLM . W : requires structured dataset for training , limited to the specific task it is designed for , may struggle with more general or open - ended tasks . e . g . BART with classification head for question answering [ 22 ] , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 25 3 . LLM + Prompt / instruction tuning module : discovers best prompt , context , instructions to query a LLM for given tasks and domains ( or to better fine - tune ) . S : improve LLM performance on single or multiple task with no retraining , and can dynamically adapt reasoning skills according to context . W : highly sensitive to slight prompt variations , may require substantial context , finding the right / robust prompt can be complex . e . g . prompt optimization programming [ 198 ] , programmatic retrieval - augmented in - context learning [ 199 ] , instructions generation [ 94 ] . 4 . LLM + Question / task decompose , plan , act module : efficient break down of complex tasks into ad - dressable subtasks following a resolution plan . S : efficient at solving more complex tasks requiring multiple steps or sources by converting into several manageable subtasks and an efficient resolution plan ( a priori , iterative or recursive ) . It can benefit from incorporating external knowledge sources or reasoning capabilities . W : more time and resources to implement , depending on implementation may struggle with long context and reasoning dependencies . e . g . decomposition , planning , and task design to multiple specialized AI models using ChatGPT [ 200 ] , iterated decomposition w / reasoning process supervision [ 201 ] , links reasoning and acting decomposition [ 202 ] , unsupervised QA decomposition [ 203 ] , Text Modular Networks learns to decompose w / existing models [ 204 ] , Talk2Data for high - Level QA decomposition [ 205 ] , DeepQA uses fact - based QA decomposition [ 206 ] , learns to decompose compound QA w / RL [ 207 ] , successive prompting decomposition for CQA [ 208 ] . 5 . LLM + Semantic Information Retriever : incorporates external sources rather than storing all in the LLM model . Can be improved with a reinforcement learning ( RL ) mechanism . S : incorporates any up - to - date external sources without increasing LLM size , allowing much smaller models ( RETRO is 1 / 25 size of GPT - 3 for same perforamnce ) , control over sources ( sensitivity , explainability , knowledge update ) and a variety of retrieval techniques . W : may struggle with tasks requiring abstract or creative reasoning , may be limited by the quality and coverage of the external sources . e . g . Deepmind RETRO [ 209 ] , Facebook DrQA [ 210 ] , FiDO [ 211 ] , Atlas [ 212 ] , training RL agents to query external knowledge [ 213 ] , Toolformer [ 214 ] , minimizing search like humans [ 215 , 216 ] . 6 . LLM + Symbolic / structured Information Retriever : leverages symbolic & structured information ( e . g . KG , ontologies , SQL ) . . S : allows cold start tasks and fast domain adaptation with low data , follows rules and concepts more effectively , manages evolving information while enriching context with structured facts or concepts ( ontologies , RDBMS , graph , taxonomy , metadata ) . W : neuro - symbolic integration is complex , creating symbolic data is time - consuming and labor - intensive . e . g . UNIQORN [ 217 ] , Heterformer [ 218 ] , UnifiedSKG [ 88 ] . 7 . LLM + Program ( software or service via API ) : leverages capabilities of external task specialized sw tools with associated proven robustness and performance ( e . g . math solver , simulation , WWW search engine ) to perform tasks difficult or impossible to handle using only internal knowledge and reasoning . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 26 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco S : Leverages proven performance and robustness of external software / services in information retrieval ( IR ) , logic , world modelling . W : challenging end - to - end learning and potential complex integration ( e . g . additional pre - processing or post - processing steps ) e . g . physics with MindsEye [ 30 ] , WebGPT [ 48 ] , SeeKeR [ 219 ] , Toolformer [ 214 ] . 8 . LLM + Code interpreter : generates code to delegate complex tasks well handled by compiler / solver , can also learn complex logics by learning program input / output . . S : leverage robust reasoning & algorithmic capabilities , and language ecosystem . W : may struggle with tasks requiring deeper understanding of context or concepts , dependency on external code interpreters . e . g . PAL [ 220 ] , solving math problems via cooperative reasoning [ 221 ] or program synthesis [ 28 ] , LM self - improve its programing capabilities [ 222 ] , Codex [ 223 ] , AlphaCode [ 224 ] . 9 . LLM + Human / AI RL feedback : learns optimal policy for goals ( answer quality , safety , data sources . . . ) . S : support in most critical challenge of LLM design such as human expectation alignment and personalization , safety and quality control . W : human feedback and convergence is highly time - consuming . This could be mitigated by incorporating active learning and AI feedback learning , but that means added complexity . e . g . reinforcement learning with human feedback RLHF [ 32 , 58 , 225 ] , with AI feedback RLAIF [ 25 ] , search algorithms MCTS [ 226 , 227 ] or DiL - piKL [ 228 ] or PPO in Diplomacy [ 228 ] , experts imitation learning [ 229 ] , web search RL in WebGpt [ 48 ] , citation RL in GopherCite [ 230 ] . 10 . Cascaded / chained / looped LLM : solves complex problems by solving it by steps as a pipeline with multiple sequential requests to LLM , this sequence could be also iterative or recursive . . S : facilitates human control over design and execution process , can solve higher complexity problems than LLM core skills by breaking down tasks & design , provides a causal chain useful for explainability , allows optimization of the pipeline and leverage specialization to avoid potential bottlenecks or inefficiencies . W : may be less effective at tasks requiring extensive context and long reasoning dependencies . e . g . solving by cacasding language models [ 231 ] , AI Chains [ 29 ] , in a collaborative visual chain of prompt [ 232 ] , logical and robust reasoning with selection - inference [ 233 ] , human readable multi - step logical deduction on scientific QA improving accuracy and faithfulness [ 234 ] , iterative prompting an LLM [ 208 , 235 , 236 ] . 11 . LLM + Veracity / evidence checker : provides veracity and sources assessment . S : guarantees information credibility & reliability while mitigating hallucinations by providing verifiable sources and evidence assessment . W : limited by the quality and coverage of external sources , do not eliminate risk of misinformation . e . g . GopherCite supports answers with verified quotes [ 230 ] , logic - regularized reasoning for interpretable fact verification [ 237 ] , survey on automated fact - checking [ 238 ] , hallucinated content detection [ 239 ] , RL approach for explainability using entailment trees [ 240 ] . 12 . LLM + Router or Task discriminator : route task / domain to best model with instructions & context . S : can accelerate LLM training , improve inference LLM performance by routing to the most appropriate model ( performance vs resource ) with most appropriate instructions . W : complex to implement and maintain , shared reasoning and long term dependencies might be compromised . e . g . universal discriminator for zero - shot generalization [ 241 ] , Branch - Train - Merge for fast parallel training of experts LM [ 242 ] , DEMIXLayers [ 243 ] . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 27 13 . LLM + Dialog module : enable dialog with requester and long context capture , can include an ontology to help structure tasks definition and tracking . . S : enhances understanding of complex contexts , problems and concepts through human interaction , guidance , progressive refinement and problem - solving . W : might not fit targeted usage format , more time and resources implementation . e . g . GODEL [ 244 ] , OPAL [ 245 ] , CommaQA [ 134 ] , ChatGPT 14 . LM + Read - write memory : add an external memory to LLM allowing to process unbounded inputs , improve long - term capacities , store and pass information between multiple inferences . S : without LLM modification can simulate any algorithm , process unbounded inputs , strengthen controllability , long - term dependencies ( for reasoning , dialogs , summarization , retrieval , algorithmic . . . ) and robustness by incorporating counterfactual & irrelevant contexts . W : scalability issues with increasing model size . e . g . universal memory augmented large LM [ 246 ] , Recurrent Memory Transformer [ 247 ] , long - term open - domain conversations [ 248 ] , working memory for scientific reasoning in Galactica [ 91 ] . 15 . LLM + Generator / Verifier : innovative problem solving by generating many potential solutions , checks for consistency , groups and classifies to come up with the best alternative answers . S : can solve complex tasks , out of training knowledge , by combining generation and adversarial skills . W : resource - intensive & costly ; may not always lead to performance improvements proportional to effort . e . g . AlphaCode [ 224 ] , DiVeRSe [ 249 ] , CoRe ( cooperative reasoning ) [ 221 ] , self - consistency for chain of thought [ 250 ] , training verifiers to solve math [ 251 ] , STaR bootstrapps reasoning with reasoning [ 252 ] . 16 . LLM + Multimodal : search and reason over knowledge out of texts ( image , audio , video , sensors . . . ) . . S : leverages knowledge from non - textual sources and combines each modality to enhance understanding , reasoning and problem - solving . W : integration complexity ( representation , alignment , reasoning , generation . . . ) & cost , increased difficulty in addressing explainability and hallucination issues e . g . foundations and recent trends in multimodal ML [ 35 , 253 ] . 17 . LLM Ensembling and LLM composing : ensembling different LLM to answer QA or a subtask or automatic construction of an architecture composed of multiple components / models . . S : improves inference accuracy , generalization and stability by combining diversity . W : cost & complexity , be aware of trade - offs ( e . g . additional computation ) . e . g . ensemble learning for validation and explanation [ 254 ] , parallel training of expert LLM [ 242 ] , ensembles of LLM via iterative consensus [ 255 ] , automatic neural module composition [ 256 ] . 18 . LLM + ( multi ) Teacher : efficiently improves LLM knowledge / skills through 1 or more expert teachers on given domains / tasks . . S : accelerates and improves knowledge learning , expansion , adaption , multi - tasking , reinforces reasoning capabilities ( e . g . temporal ) . W : cost & complexity , control of teacher knowledge transfer ( e . g . domain scope , biases ) . e . g . Teacher - Student Architecture for Knowledge Learning survey [ 257 ] , better learn from multiple teach - ers [ 57 ] . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 28 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco 19 . LLM + Temporal reasoning : improves performance on time - related tasks , enhance temporal information understanding , retrieval and reasoning ( could be extended to spatial dimension ) . S : allows temporal estimation , ranking & clustering , reasoning , incoherence detection , addresses knowledge forget and update . W : still in its early stages for LLM ( few papers ) , efficient integration might be challenging . e . g . TimeBERT : Extending Pre - Trained Language Representations with Temporal Information [ 258 ] , improves temporal reasoning through added audio modality [ 111 ] . 7 Solving with prompting Now that we have trained models to acquire the required skills to solve a complex QA or rely on hybridization , let’s see how to design questions ( i . e . prompt ) with proper instructions and context . 7 . 1 Designing my question ( prompting ) LLM can highly improve its ability to solve a problem by leveraging information provided with a question ( " problem well stated is half solved " ) . Engineering a good prompt ( text provided to a LLM when posing a question ) can rival or beat model finetuning on QA in many cases [ 259 ] . Different information can be added to improve answer success probability while posing a question to a LLM such as additional context and knowledge , constraints , instructions and examples . . . Question can also be designed to be answered in multiple steps , for example just by asking to answer step by step [ 260 , 261 ] or reasoning compositionally like humans [ 262 ] , [ 208 , 263 ] . In the following sections , we present the main prompting techniques to combine for solving complex questions through prompting by re - using the taxonomy of Qiao et al . [ 99 ] . 7 . 2 Problem solving strategies 7 . 2 . 1 Single pass optimization Prompt can be optimized to get the best answer possible directly ( prompt engineering for single - pass ) : Zero - shot prompting provides a prompt for task without prior training on this task and no additional context or guidance . In - Context learning and Few - shot Prompting provides prompt with relevant context [ 197 , 264 ] or demonstration [ 265 ] for expected task helping LLM to better answer . Most well - known example is few - shot learning which provides a prompt with a few examples of expected task for helping to generate the best answer . ( Hard ) prompt tuning adjusts the initial prompt , often by trial and error , to improve answer accuracy . This improving process can be manual , automated or even programmed [ 198 ] . Soft prompt tuning creates soft prompts [ 266 ] which are concatenated to the input text . Tokens of this soft prompt are learned vectors optimized end - to - end over a training dataset . subsection 5 . 5 provides some additional details . Some innovative examples are : – Exploring Universal Intrinsic Task Subspace via Prompt Tuning : adapt to many NLP tasks with small - scale data by optimizing only a few free parameters in a unified low - dimensional intrinsic task subspace [ 267 ] . – Compositional Task Representations learn specific codebook for compositional tasks [ 268 ] . Chain - of - thought prompting [ 260 , 269 ] ask to reason step by step and can provide relevant examples of multi - steps of reasoning / thoughts up to the solution to improve reliability or more easily spot errors in the result . It largely outperforms the state - of - the - art results with zero and few - shots learning with the same model on many advanced natural language processing tasks and fine - tuned models trained with hundreds times of examples , with the advantage of being interpretable . Chain - of - hindsight or contrastive prompting [ 270 , 271 ] provides examples with qualitative feed - back ( e . g . comparison and critiques ) to better align the model output to preferences . It is mostly used for finetuning model but could be used when prompting . 7 . 2 . 2 Multi - step optimization Prompt can also be designed to enable a solving process in best iterative steps ( prompt engineering for multi - step ) : , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 29 Least - to - most prompting [ 263 ] improves chain - of - thought with multi - step examples that gradually becomes more specific or detailed ; Chain - of - thought often performs poorly on tasks requiring to solve problems harder than those in demonstration examples . To tackle this , LtM first reduces a complex problem into a list of easier subproblems , and then sequentially solves these subproblems with gradual complexity . LtM can be combined with self - consistency to improve robustness . Dynamic least - to - most prompting ( compositionality ) [ 262 ] is a refinement of least - to - most prompt - ing using the following steps : ( 1 ) prompts LLM to teach it to perform a synthatic parsing of all inputs to create a tree - structured decomposition , ( 2 ) use decomposition to select demonstration exemples , ( 3 ) linearize the decomposition tree and prompt the model to sequentially generate answers to subproblems . Successive prompting [ 208 ] develops successive prompting decomposing a complex problem into a first simple problem , with each next subproblem prediction having access to the answers to each previous subproblems . Maieutic prompting [ 6 ] is inspired by Socratic way of questioning , it generates a tree of logical expla - nations up to the truth values that max - satisfy these relations to verify its veracity . It surpass many approaches and provides intrinsic interpretations of inference . 7 . 2 . 3 Process optimization Prompt is designed to follow a parallel or iterative process optimizing the final output . Self - Optimization covers self refining processes ( e . g . calibrators , filters ) [ 272 , 273 ] . Ensemble - Optimization encompasses ensembling techniques used to generate more consistent answers by majority vote or ensembling decision process . A good example is Self - consistency [ 250 ] which generates multiples prompts , verifies and votes [ 274 ] . Iterative - Optimization fine - tunes iteratively to produce better reasoning processes and answers [ 235 , 252 , 275 ] . 7 . 2 . 4 Prompt for external module Prompt is designed to query an external dedicated tool Use of simulator ( e . g . physics engine ) to simulate processes and aid LMs in real - world reasoning [ 30 , 276 ] Use of code interpreter to enrich LLM answers for complex structures and calculations [ 220 , 277 – 279 ] . 7 . 3 Enhancing knowledge or skills Prompt can be engineered in order to enhance : Knowledge retrieval which could be divided into implicit knowledge to enrich LLM answers through Few - Shot Prompting and Reinforcement Learning [ 280 – 282 ] ; and explicit knowledge to retrieve and provide in - context labeled examples in prompt to improve explicit knowledge , reduce hallucination and enrich LLM answers [ 283 , 284 ] . Arithmetic reasoning with many different approaches : [ 132 , 198 , 220 , 249 , 275 , 278 , 285 , 286 ] Commonsense reasoning on general background knowledge and reasoning . [ 250 , 261 , 279 – 282 ] Creativity reasoning tosupportideation & creationprocess likeautomateddiversepromptingideas [ 287 – 290 ] . Logical reasoning uses examples that contain synthetic rule bases , entailment trees , and diagnostic benchmarks . [ 233 , 234 ] Symbolic reasoning [ 220 , 250 , 261 , 291 ] uses examples that contain symbolic rationales , rules . . . Multimodal reasoning incorporate existing multimodal reasoning benchmarks such as ScienceQA , ALERT , into the LLM prompting [ 292 , 293 ] . 8 Solving with reinforcement ( experience loop , knowledge capitalization ) How to create a system able to align to expectations and continually improve its solving capabilities ? The last step in the standard pipeline presented in introduction is the improvement loop and knowledge capitalization , a form of " experience " . How to learn from each answer to better align to expectations ( e . g . max usefulness vs min harmless answer ) , improve skills and knowledge . Our survey methodology has identified “reinforcement learning” and " human - in - the - loop " as the main levers . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 30 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco 8 . 1 Reinforcement learning methods We can categorize reinforcement learning ( RL ) [ 294 ] in this survey into those different techniques or approaches : • Reward - based reinforcement learning ( RL standard technique ) : the LM is trained to maximize a reward signal ( e . g . positive or negative feedback ) that is provided by a human or some other external source . This could involve providing the model with a fixed reward whenever a correct answer is generated , or using more complex reward functions that take into account the quality and specificity of the model’s answers . • Imitation learning ( includes procedure cloning [ 229 ] ) : the LM is trained to imitate the behavior of an expert ( e . g . human , system . . . ) . This can be a useful way to incorporate domain knowledge , search methodology , or other types of expertise into the model , and can help the model learn to generate high - quality answers more quickly by mimicking . • Inverse reinforcement learning [ 182 ] : here the reward is not direct , the LM attempts to infer the reward function from indirect human feedback or other forms of guidance . This can be a more flexible approach , as it allows the model to learn from a wider range of feedback signals and to adapt to changing requirements over time . Inverse RL could be classified as a type of imitation learning . 8 . 2 Human - in - the - loop ( RLHF ) , [ 295 ] Human - in - the - loop is meant to improve the outcome of an event or process via user input in the system loop . Humans can intervene at many steps in a QA system from task definition or data creation , to final answer assessment . We focus on the QA feedback loops . Human can explicitly validate , rank , correct , provide guidance for improvement , or implicitly rate via click - through . The outcome of each question expect the answer to best fit with the user’s intention [ 296 ] . Those intentions are explicit on one side ( following instructions ) and implicit on the other side ( answer is helpful , truthful , not biased , nor toxic , nor harmful ) [ 32 , 297 ] . We therefore need to : • capture user rich explicit and implicit feedback through different human - in - the - loop input feedback . • estimate and maximize user explicit and implicit intentions satisfaction for each answer and also in total through diverse reinforcement learning ( RL ) techniques . This approach is often called RLHF ( reinforcement learning with human feedback ) [ 25 , 32 , 58 , 193 , 194 , 225 ] , it can be enhanced by an AI supervision process to better scale , reduce human workload and biases , this is illustrated in the figure 4 . When compared to GPT - 3 with 175B parameters , best in class language model in 2022 , to model InstructGPT 1 . 3B more than 100 times smaller , prompt answers from this small model built with RLHF are 85 % preferred by humans [ 58 ] . However , learning from human expertise has limits : ( 1 ) scaling cost selection : manual labeling of data is slow and expensive so maybe restricted to some wealthy organizations or labeled with less expertise . To highly reduce this , Su et al . [ 284 ] proposes vote - k an unsupervised graph - based selective annotation method yielding to drast reduction and more robust learning , Anthropic et al [ 25 ] introduce different supervised and RL techniques ( SL - CAI and RL - CAI summarized to RLAIF ( RL with AI feedback ) ) which can learn from far fewer labelers and generate higher quality labeling . ( 2 ) labeler biases : longer RLHF training can bias language model with stronger political views ( e . g . gun rights , immigration ) and desires to pursue specific goals ( e . g . resource acquisition ) and their preserva - tion [ 298 ] . AI supervision from Anthropic [ 25 ] driven by principles could allow more controllable and transparent feedback behaviors . ( 3 ) expertise problem : even if same questions are challenged by different persons and agreed with majority , some questions may require specific expertise to be correctly analyzed . Problem formalization and query - teacher selection solution is discussed in " The Expertise Problem : Learning from Specialized Feedback " [ 225 ] . ( 4 ) harmlessness vs helpfulness trade - off : " helpfulness tends to increase harmfulness , since models are willing to obey pernicious requests , and conversely models trained to be harmless tend to be , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 31 Fig . 4 . From reinforcement learning with human feedback to AI feedback in order to scale and maximize helpfulness vs harmless tradeoff ( Bai et al . [ 25 ] , Ouyang et al . [ 58 ] ) more evasive and generally less helpful " [ 25 ] . This competitive objective solving is well discussed and solutions provided in Bai et al . [ 32 ] and recent Constitutional AI [ 25 ] approach . 8 . 2 . 1 Continuous improvement throughout the CQA pipeline This reinforcement loop can improve at many stage of the QA pipeline such as : ( 1 ) Question understanding and context improvement are demonstrated in reinforced clarification question generation by Pyatkin et al . [ 299 ] and within dialogue by Hu et al . [ 300 ] . ( 2 ) Decomposition strategies in learning to decompose compound questions [ 207 ] , automatic generation of socratic subquestions [ 7 ] , decision - making with multi - step expert advices on the web [ 301 ] . ( 3 ) Query construction ( prompting ) in reinforced knowledge introspector for commonsense QA [ 281 ] , optimizing discrete text prompts [ 302 ] , improving prompt in - context policy iteration [ 303 ] . ( 4 ) Informationretrievalinreinforcedbrowser - assistedQAwithhumanfeedback [ 48 ] , knowledge - grounded QA [ 26 ] , retrieval augmented process [ 181 ] , answering with verified quotes [ 230 ] , querying external knowledge [ 213 ] , reasoning and acting in multiple search [ 202 ] . ( 5 ) Answer generation reinforcement mainly with instructions / expectation alignment ( quality , safety , ambiguity . . . ) in training a helpful and harmless assistant from human feedback [ 32 ] , improving it with AI feedback [ 25 ] , aligning with natural language goals [ 182 ] , benchmarking with preference [ 304 ] . ( 6 ) Knowledge capitalization : we did not find any articles related to QA progressive capitalization with a reinforcement loop . However , QA entailment tree [ 240 , 305 , 306 ] , self - consistency [ 275 ] , compositional solving above model capacity [ 262 , 268 ] seem potential ways to capitalize from answers to answers building a faithful and truthful explainable tree of information and this kind of tree has already been used in a RL process [ 240 ] . 9 Discussion : limitations and research topics for solving more complex QA and problems In the architectural patterns section , we listed the most frequent topics identified has challenge or limits of LLM . After reviewing the collected literature and identifying different solutions in this study , some limits seem tougher research limits to enable more complex QA and problems solving : • The hallucination problem which limits a clear expectation of credibility / truthfulness in " Alignment to human expectation & values in answer " . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 32 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco • The scalability problem which we can extend to compute and costs limits . • Data availability & quality which limits " domain adaptation & task specialization " and " bias " • Data multi - sensitivity in LLM : this point is nearly uncovered . • Question decomposition of very complex problems and its explainability . A recent paper from Meta [ 307 ] surveys LLM augmentation with more details on reinforcement learning . It also proposes additional research topics such as an optimal tradeoff between getting knowledge in or out of the model , which would help to better design modules presented in section 6 ) , and to extend LLM decomposition and planning module presented in section 6 to be a central orchestrator . 9 . 1 Hallucination & credibility Recent debates about Galactica [ 91 , 308 ] and ChatGPT [ 309 ] shade the light of limits and credibility of such language models concerning hallucination . It generates plausible - looking statements that are irrelevant or factually incorrect . It predicts without giving clues about which part of a false claim goes wrong , even sources given are not trustworthy . It even has difficulty to learn correct associations between entities from factual text corpus ( e . g . Wikipedia ) . Explainability of an answer with a supported citation is a pointer but does not mean it is true . We have identified different topics of research to address the challenge of hallucinations such as : • More robust training & prompting ( self - consistency , context optimization , prompt tuning , denois - ing . . . ) [ 277 ] . • Hallucination detection [ 239 ] . • Providing references , traceability , faithful explanation logic [ 237 ] or the emerging field of entailment tree explanation [ 240 , 305 ] . • Automated fact - checking [ 238 ] . • Identifying faithfulness performance per tasks / domain [ 9 ] and biases , to better ensemble experts [ 310 ] . • Contrastive learning to reduce hallucination [ 311 ] . • Reinforcement learning from human feedback , including red teaming , boosted with AI supervision ( RLHP , RLHF , RLAIF ) seams the strongest area of research to reduce hallucination in QA [ 25 , 32 , 194 ] . 9 . 2 Compute , scaling . . . Costs More than 8 million TPUv4 hours is the time taken to train PalM 540B parameters model [ 192 ] . For a far smaller model " T5 11B paremeters " and its variants , the cost of the project is estimated $ 10 millions [ 312 ] . Those models continue to scale . Time of compute for training is therefore reserved to few organization . Operational costs for usage ( inference ) are less impressive [ 9 ] but limits the use cases considering inference latency and minimal required hardware . We can apply standard model size reductions like quantization , distillation , pruning , early stopping at training . . . Different research avenues try to reduce this computing and costs required and inverse this scaling law : • Frozen PLM techniques : we presented in previous sections prompt tuning and parameters - efficient tuning , there is a constant research on this related approach re - using already trained ( frozen ) LLM to avoid re - training it or at minimal . • Retrieval augmented LLM : keeping a maximum of information out of model while making it easy to access and update without any re - train has an important potential but is often less efficient and may require equal computation when comparing " total additional answer generation time " vs " training " , new techniques try to close the gap [ 313 ] . • Scaling in - context learning : in - context learning highly improves tasks efficiency without re - training but is limited by maximum length input constraints due to quadratic complexity in computation and memory , different techniques allow to efficiently scale it [ 314 – 316 ] . • Mixture of denoisers : in " Transcending Scaling Laws with 0 . 1 % Extra Compute " [ 192 ] , same model is train at half the budget to reach the same performance using mixture - of - denoisers ( " U - PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget - saving approx . 4 . 4 million TPUv4 hours " ) . UL2 , proposes an unification of LM learning paradigms [ 158 ] . • Improve pruning techniques : in " SparseGPT : Massive Language Models Can Be Accurately Pruned in One - Shot " [ 196 ] , same model can be reduced more than 50 % parameters with only one - shot , without any retraining , and nearly no loss in accuracy . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 33 • Mixture of experts , parameters sharing and routing techniques : " Switch Transformers : Scaling to Trillion Parameter Models with Simple and Efficient Sparsity " [ 317 ] demonstrated the usage of internal routing to expert layers in large language models to limit compute to part of the whole model to allow to scale model without augmenting compute . Many researchers propose new MoE : HetuMoE [ 318 ] , MoE distributed training system [ 318 ] , evolutional EvoMoE using dense - to - sparse gates [ 319 ] , FlexMoE with dynamic device placement [ 320 ] . • Knowledge distillation improvement [ 321 – 323 ] and dynamic composition of model [ 324 ] to com - pose optimal and smaller models . • Adaptive computation and training : all samples are equally computed in standard training . Easy samples could be less worked out than hard ones , adaptive computing enable sample - dependent computation and reached 2 / 3x computation for CALM [ 191 ] . Chinchilla [ 325 ] demonstrated that we can highly reduce inference budget while improving accuracy by using the same training budget with much more data on a much smaller LLM . • Dedicated hardware : language models are typically accelerated by running on GPU but an area of research investigate dedicated hardware ( e . g . FPGA , ASICS , ) for saving energy and costs [ 326 ] . 9 . 3 Data availability & quality The skills and training datasets section , as well as human feedback , highlighted the need for specialized data and of high quality , to acquire skills and domain knowledge as well as to calibrate to requester intents . Large language models requires huge volume of data to develop each skills and domains targeted . Wealthy organization can spend millions to clean or produce data but even them are limited . • Frugal and rich data with AI supervision : we saw in previous sections techniques like dynamic least - to - most requiring far less data for training while improving accuracy and skills , or active learning identifying best examples for improvement [ 327 ] . Those data could be more and more automatically generated like in Auto - CoT [ 328 ] , CAI [ 25 ] . • Simulation , distillation and code interpreter ( program execution ) : simulation [ 30 , 276 ] , existing models [ 323 ] , and code interpreters [ 222 ] can provide infinite examples in different domains of logic and knowledge . We also uncovered that code interpreter allow to learn logic transferable to many domains . • More signals : better leverage all available symbolic data which have clear signals , already structured ( Linked Open Data ) or re - structured [ 145 ] . • Open dataset : QA and IR field has continuously progressed through the availability of existing and new open datasets [ 329 ] for target skills and knowledge . 9 . 4 Data multi - sensitivity usage & protection How a QA can ensure protection of rules associated to each data sensitivity ? Sensitive data can be personal but also business confidential , regulated , high risk . . . The same data can have multiple sensitivity and should be restricted to only person required to know it . Large language models are currently designed without any access control mechanism for the embedded data in the model . Apart from removing any sensitive data , making it impossible for authorized persons to use them legitimately ( e . g . a doctor on a patient’s medical data ) , different techniques and strategies have been identified as avenues of research : • Data access control through retriever : language models using a retriever could avoid embedding sensitive data in the model and use sensitive data stored outside the model in a suitable system ( eg RDBMS , EDM ) incorporating a data access control mechanism . • Models per sensitivity : the different type of sensitive data could each be stored in a specific model and a common front - end could manage the access control to each model . • Privacy preserving mechanisms [ 330 – 338 ] : QA can ensure protection of private data by imple - menting privacy - preserving techniques such as homomorphic encryption , local differential privacy , or secure multiparty computation . These techniques can be applied to the input text , token embeddings , and sequence representations in order to protect the data from malicious actors . In addition , QA can ensure privacy by using secure protocols for matrix multiplication and complex non - linear functions such as Softmax , GELU activations , and LayerNorm . QA can also use TextHide which introduces an encryption step to prevent an eavesdropping attacker from recovering private text data . Finally , QA , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 34 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco can use TextFusion which employs an adversarial training regime to privatize token representations and make it difficult to accurately recover the complete plain text . • Functional encryption : this technique could use encryption to finely restrict data usage based on defined roles and functions ( e . g . right to query medical data ) [ 338 ] . • Decentralized ( federated ) approach with sensitive data : federated learning could allow to keep sensitive data at source to avoid sharing and unauthorized usage [ 338 – 340 ] . • Other techniques [ 338 ] could avoid elimination of data by perturbation , knowledge transfer . . . 9 . 5 Decomposition of very complex QA and explainability We defined complex question answering as high complexity questions which are non - factoid , multi - step requiring decomposition , higher reasoning ( constraints , deduction , induction , abduction ) , multi - source questions . This need for decomposition is central in this solving process because it breaks down a non solvable complex question ( problem ) down to solvable questions . Moreover , as we have seen in chain - of - thought and dynamic least - to - most , those traceable steps improve solving capacities of a given model but also makes the answer auditable , truthful and explainable in case of errors . However nearly all examples of papers reviewed related to decomposition are factoid questions . Dua et al . [ 208 ] used " Who kicked the longest field goal in the first half ? " as the main example of complex question , it is a factoid question . What if we ask " What are the most adapted LM hybrid architectures to answer complex questions ? " . That would be a non - factoid question , requiring multiple sources , reasoning . . . And a compatible decomposition process aligned with an acceptable and auditable scientific methodology . We could learn this decomposition behaviour by cloning a human process [ 229 ] or learn to discover it through a human contribution . Iterated decomposition [ 201 ] , a human - in - the - loop workflow for process supervision using a language model allows to address new type of problems and enables users to inspect and intervene in the LM’s reasoning process . Those two research topics seem of high potential even if it still requires a lot of human expert feedback , domain specific data and computation is an important limit . The generalization of such a process of decomposition for complex non factoid questions to many domains and practice could be accelerated by the same techniques identified for scaling RLHF , research avenues in " Data availability & quality " , " Compute , scaling . . . Costs " and " Hallucination & credibility " sections ; and improved with adapted shortest path iterative approach . 10 Conclusion In this paper , we present a comprehensive survey of language model hybrid architectures for answering complex questions . We review the various skills required and typical approach , datasets and metrics that are used , the current limits of large language models for complex QA , the potential of hybrid architectures , better training and prompting strategies for this goal . We also identify the main challenges and research avenues for solving more complex questions including knowledge capitalization . We identify the need to address multi - sensitivity data in language models architectures and potential approaches . Finally , we outline research topics and highlight the potential of exploration in this field . This paper aims to provide a comprehensive and useful resource for readers interested in the development of complex non - factoid question answering . References [ 1 ] J . Weizenbaum , “ELIZA—a computer program for the study of natural language communication between man and machine , ” 1966 . [ 2 ] C . A . CUADRA and R . V . KATTER , “OPENING THE BLACK BOX OF ‘RELEVANCE’ , ” Journal of Documentation , vol . 23 , no . 4 , pp . 291 – 303 , Jan . 1967 . [ 3 ] P . Wilson , “Situational relevance , ” Information Storage and Retrieval , vol . 9 , no . 8 , pp . 457 – 471 , Aug . 1973 . [ 4 ] D . Norman , “Memory , Knowledge , and the Answering of Questions . ” May 1972 . [ 5 ] A . Pagnoni , A . R . Fabbri , W . Kryściński , and C . - S . Wu , “Socratic Pretraining : Question - Driven Pretraining for Controllable Summarization , ” Dec . 2022 . [ 6 ] J . Jung , L . Qin , S . Welleck , F . Brahman , C . Bhagavatula , R . L . Bras , and Y . Choi , “Maieutic Prompting : Logically Consistent Reasoning with Recursive Explanations , ” Oct . 2022 . [ 7 ] K . Shridhar , J . Macina , M . El - Assady , T . Sinha , M . Kapur , and M . Sachan , “Automatic Generation of Socratic Subques - tions for Teaching Math Word Problems , ” Nov . 2022 . [ 8 ] A . Borji , A Categorical Archive of ChatGPT Failures , Feb . 2023 . [ 9 ] Liang et al . , “Holistic Evaluation of Language Models , ” Nov . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 35 [ 10 ] BigScience et al . , “BLOOM : A 176B - Parameter Open - Access Multilingual Language Model , ” Dec . 2022 . [ 11 ] BIG et al . , “Beyond the Imitation Game : Quantifying and extrapolating the capabilities of language models , ” Jun . 2022 . [ 12 ] P . Frensch and J . Funke , Complex Problem Solving—The European Perspective , Jan . 1995 . [ 13 ] A . Rogers , M . Gardner , and I . Augenstein , “QA Dataset Explosion : A Taxonomy of NLP Resources for Question Answering and Reading Comprehension , ” ACM Computing Surveys , p . 3560260 , Sep . 2022 . [ 14 ] Y . Hao , H . Song , L . Dong , S . Huang , Z . Chi , W . Wang , S . Ma , and F . Wei , “Language Models are General - Purpose Interfaces , ” Jun . 2022 . [ 15 ] P . K . Choubey , A . R . Fabbri , J . Vig , C . - S . Wu , W . Liu , and N . F . Rajani , “CaPE : Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization , ” May 2022 . [ 16 ] D . Ferrucci , E . Brown , J . Chu - Carroll , J . Fan , D . Gondek , A . Kalyanpur , A . Lally , J . W . Murdock , E . Nyberg , J . Prager , N . Schlaefer , and C . Welty , “Building Watson : An Overview of the DeepQA Project , ” AI Magazine , vol . 31 , pp . 59 – 79 , Sep . 2010 . [ 17 ] T . Lewis , v . W . Leandro , and W . Thomas , Natural Language Processing with Transformers _ Building Language Applica - tions with Hugging Face , 2022 . [ 18 ] J . Devlin , M . - W . Chang , K . Lee , and K . Toutanova , “BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding , ” May 2019 . [ 19 ] Y . Liu , M . Ott , N . Goyal , J . Du , M . Joshi , D . Chen , O . Levy , M . Lewis , L . Zettlemoyer , and V . Stoyanov , “RoBERTa : A Robustly Optimized BERT Pretraining Approach , ” Jul . 2019 . [ 20 ] M . Zong and B . Krishnamachari , “A survey on GPT - 3 , ” Dec . 2022 . [ 21 ] C . Raffel , N . Shazeer , A . Roberts , K . Lee , S . Narang , M . Matena , Y . Zhou , W . Li , and P . J . Liu , “Exploring the Limits of Transfer Learning with a Unified Text - to - Text Transformer , ” Jul . 2020 . [ 22 ] M . Lewis , Y . Liu , N . Goyal , M . Ghazvininejad , A . Mohamed , O . Levy , V . Stoyanov , andL . Zettlemoyer , “BART : Denoising Sequence - to - Sequence Pre - training for Natural Language Generation , Translation , and Comprehension , ” Oct . 2019 . [ 23 ] M . Zaheer , G . Guruganesh , A . Dubey , J . Ainslie , C . Alberti , S . Ontanon , P . Pham , A . Ravula , Q . Wang , L . Yang , and A . Ahmed , “Big Bird : Transformers for Longer Sequences , ” Jan . 2021 . [ 24 ] S . Ullrich and M . Geierhos , “Using Bloom’s Taxonomy to Classify Question Complexity , ” in Proceedings of the Fourth International Conference on Natural Language and Speech Processing ( ICNLSP 2021 ) . Trento , Italy : Association for Computational Linguistics , 2021 , pp . 285 – 289 . [ 25 ] Y . Bai , S . Kadavath , S . Kundu , A . Askell , J . Kernion , A . Jones , A . Chen , A . Goldie , A . Mirhoseini , C . McKinnon , C . Chen , C . Olsson , C . Olah , D . Hernandez , D . Drain , D . Ganguli , D . Li , E . Tran - Johnson , E . Perez , J . Kerr , J . Mueller , J . Ladish , J . Landau , K . Ndousse , K . Lukosuite , L . Lovitt , M . Sellitto , N . Elhage , N . Schiefer , N . Mercado , N . DasSarma , R . Lasenby , R . Larson , S . Ringer , S . Johnston , S . Kravec , S . E . Showk , S . Fort , T . Lanham , T . Telleen - Lawton , T . Conerly , T . Henighan , T . Hume , S . R . Bowman , Z . Hatfield - Dodds , B . Mann , D . Amodei , N . Joseph , S . McCandlish , T . Brown , and J . Kaplan , “Constitutional AI : Harmlessness from AI Feedback , ” Dec . 2022 . [ 26 ] Z . - Y . Chiu , Y . - L . Tuan , W . Y . Wang , and M . C . Yip , “Knowledge - Grounded Reinforcement Learning , ” Oct . 2022 . [ 27 ] B . Z . Li , M . Nye , and J . Andreas , “Language Modeling with Latent Situations , ” 2022 . [ 28 ] I . Drori , S . Zhang , R . Shuttleworth , L . Tang , A . Lu , E . Ke , K . Liu , L . Chen , S . Tran , N . Cheng , R . Wang , N . Singh , T . L . Patti , J . Lynch , A . Shporer , N . Verma , E . Wu , and G . Strang , “A Neural Network Solves , Explains , and Generates University Math Problems by Program Synthesis and Few - Shot Learning at Human Level , ” May 2022 . [ 29 ] T . Wu , M . Terry , and C . J . Cai , “AI Chains : Transparent and Controllable Human - AI Interaction by Chaining Large Language Model Prompts , ” in CHI Conference on Human Factors in Computing Systems , ser . CHI ’22 . New York , NY , USA : Association for Computing Machinery , Apr . 2022 , pp . 1 – 22 . [ 30 ] R . Liu , J . Wei , S . S . Gu , T . - Y . Wu , S . Vosoughi , C . Cui , D . Zhou , and A . M . Dai , “Mind’s Eye : Grounded Language Model Reasoning through Simulation , ” Oct . 2022 . [ 31 ] J . Wei , Y . Tay , R . Bommasani , C . Raffel , B . Zoph , S . Borgeaud , D . Yogatama , M . Bosma , D . Zhou , D . Metzler , E . H . Chi , T . Hashimoto , O . Vinyals , P . Liang , J . Dean , and W . Fedus , “Emergent Abilities of Large Language Models , ” Oct . 2022 . [ 32 ] Y . Bai , A . Jones , K . Ndousse , A . Askell , A . Chen , N . DasSarma , D . Drain , S . Fort , D . Ganguli , T . Henighan , N . Joseph , S . Kadavath , J . Kernion , T . Conerly , S . El - Showk , N . Elhage , Z . Hatfield - Dodds , D . Hernandez , T . Hume , S . Johnston , S . Kravec , L . Lovitt , N . Nanda , C . Olsson , D . Amodei , T . Brown , J . Clark , S . McCandlish , C . Olah , B . Mann , and J . Kaplan , “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback , ” Apr . 2022 . [ 33 ] K . Mahowald , A . A . Ivanova , I . A . Blank , N . Kanwisher , J . B . Tenenbaum , and E . Fedorenko , “Dissociating language and thought in large language models : A cognitive perspective , ” Jan . 2023 . [ 34 ] Z . Ji , N . Lee , R . Frieske , T . Yu , D . Su , Y . Xu , E . Ishii , Y . Bang , W . Dai , A . Madotto , and P . Fung , “Survey of Hallucination in Natural Language Generation , ” ACM Computing Surveys , p . 3571730 , Nov . 2022 . [ 35 ] OpenAI , “GPT - 4 Technical Report , ” 2023 . [ 36 ] W . X . Zhao , J . Liu , R . Ren , and J . - R . Wen , “Dense Text Retrieval based on Pretrained Language Models : A Survey , ” Nov . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 36 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco [ 37 ] A . Chen , G . Stanovsky , S . Singh , and M . Gardner , “Evaluating Question Answering Evaluation , ” in Proceedings of the 2nd Workshop on Machine Reading for Question Answering . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 119 – 124 . [ 38 ] T . Zhang , V . Kishore , F . Wu , K . Q . Weinberger , and Y . Artzi , “BERTScore : Evaluating Text Generation with BERT , ” Feb . 2020 . [ 39 ] W . Yuan , G . Neubig , and P . Liu , “BARTScore : Evaluating Generated Text as Text Generation , ” Oct . 2021 . [ 40 ] K . Pillutla , S . Swayamdipta , R . Zellers , J . Thickstun , S . Welleck , Y . Choi , and Z . Harchaoui , “MAUVE : Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers , ” Nov . 2021 . [ 41 ] Y . Qin , W . Yuan , G . Neubig , and P . Liu , “T5Score : Discriminative Fine - tuning of Generative Evaluation Metrics , ” Dec . 2022 . [ 42 ] T . He , J . Zhang , T . Wang , S . Kumar , K . Cho , J . Glass , and Y . Tsvetkov , “On the Blind Spots of Model - Based Evaluation Metrics for Text Generation , ” Dec . 2022 . [ 43 ] G . Frisoni , A . Carbonaro , G . Moro , A . Zammarchi , and M . Avagnano , “NLG - Metricverse : An End - to - End Library for Evaluating Natural Language Generation , ” 2022 . [ 44 ] F . Chiusano , “OpenAI InstructGPT brings Reward Models to GPT - 3 , ” Jan . 2022 . [ 45 ] S . Zhang , S . Roller , N . Goyal , M . Artetxe , M . Chen , S . Chen , C . Dewan , M . Diab , X . Li , X . V . Lin , T . Mihaylov , M . Ott , S . Shleifer , K . Shuster , D . Simig , P . S . Koura , A . Sridhar , T . Wang , and L . Zettlemoyer , “OPT : Open Pre - trained Transformer Language Models , ” Jun . 2022 . [ 46 ] A . Zeng , X . Liu , Z . Du , Z . Wang , H . Lai , M . Ding , Z . Yang , Y . Xu , W . Zheng , X . Xia , W . L . Tam , Z . Ma , Y . Xue , J . Zhai , W . Chen , P . Zhang , Y . Dong , and J . Tang , “GLM - 130B : An Open Bilingual Pre - trained Model , ” Oct . 2022 . [ 47 ] M . U . Haque , I . Dharmadasa , Z . T . Sworna , R . N . Rajapakse , and H . Ahmad , “ " I think this is the most disruptive technology " : Exploring Sentiments of ChatGPT Early Adopters using Twitter Data , ” Dec . 2022 . [ 48 ] R . Nakano , J . Hilton , S . Balaji , J . Wu , L . Ouyang , C . Kim , C . Hesse , S . Jain , V . Kosaraju , W . Saunders , X . Jiang , K . Cobbe , T . Eloundou , G . Krueger , K . Button , M . Knight , B . Chess , and J . Schulman , “WebGPT : Browser - assisted question - answering with human feedback , ” Mar . 2022 . [ 49 ] C . Leiter , P . Lertvittayakumjorn , M . Fomicheva , W . Zhao , Y . Gao , and S . Eger , “Towards Explainable Evaluation Metrics for Natural Language Generation , ” Mar . 2022 . [ 50 ] S . Wiegreffe and A . Marasović , “Teach Me to Explain : A Review of Datasets for Explainable NLP , ” undefined , 2021 . [ 51 ] M . Soprano , K . Roitero , D . La Barbera , D . Ceolin , D . Spina , S . Mizzaro , and G . Demartini , “The Many Dimensions of Truthfulness : Crowdsourcing Misinformation Assessments on a Multidimensional Scale , ” Information Processing & Management , vol . 58 , no . 6 , p . 102710 , Nov . 2021 . [ 52 ] S . Lin , J . Hilton , and O . Evans , “TruthfulQA : Measuring How Models Mimic Human Falsehoods , ” May 2022 . [ 53 ] M . Martindale , M . Carpuat , K . Duh , and P . McNamee , “Identifying Fluently Inadequate Output in Neural and Statistical Machine Translation , ” in Proceedings of Machine Translation Summit XVII : Research Track . Dublin , Ireland : European Association for Machine Translation , Aug . 2019 , pp . 233 – 243 . [ 54 ] E . Durmus , H . He , and M . Diab , “FEQA : A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization , ” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Online : Association for Computational Linguistics , 2020 , pp . 5055 – 5070 . [ 55 ] A . Wang , K . Cho , and M . Lewis , “Asking and Answering Questions to Evaluate the Factual Consistency of Summaries , ” Apr . 2020 . [ 56 ] T . Scialom , P . - A . Dray , S . Lamprier , B . Piwowarski , J . Staiano , A . Wang , and P . Gallinari , “QuestEval : Summarization Asks for Fact - based Evaluation , ” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Online and Punta Cana , Dominican Republic : Association for Computational Linguistics , Nov . 2021 , pp . 6594 – 6604 . [ 57 ] C . Wu , F . Wu , and Y . Huang , “One Teacher is Enough ? Pre - trained Language Model Distillation from Multiple Teachers , ” Jun . 2021 . [ 58 ] L . Ouyang , J . Wu , X . Jiang , D . Almeida , C . L . Wainwright , P . Mishkin , C . Zhang , S . Agarwal , K . Slama , A . Ray , J . Schulman , J . Hilton , F . Kelton , L . Miller , M . Simens , A . Askell , P . Welinder , P . Christiano , J . Leike , and R . Lowe , “Training language models to follow instructions with human feedback , ” Mar . 2022 . [ 59 ] J . Lin , R . Nogueira , and A . Yates , “Pretrained Transformers for Text Ranking : BERT and Beyond , ” Aug . 2021 . [ 60 ] P . Rajpurkar , J . Zhang , K . Lopyrev , and P . Liang , “SQuAD : 100 , 000 + Questions for Machine Comprehension of Text , ” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Austin , Texas : Association for Computational Linguistics , 2016 , pp . 2383 – 2392 . [ 61 ] P . Rajpurkar , R . Jia , and P . Liang , “Know What You Don’t Know : Unanswerable Questions for SQuAD , ” Jun . 2018 . [ 62 ] M . Joshi , E . Choi , D . S . Weld , and L . Zettlemoyer , “TriviaQA : A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension , ” May 2017 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 37 [ 63 ] D . Hendrycks , C . Burns , S . Basart , A . Zou , M . Mazeika , D . Song , and J . Steinhardt , “Measuring Massive Multitask Language Understanding , ” Jan . 2021 . [ 64 ] T . Kočiský , J . Schwarz , P . Blunsom , C . Dyer , K . M . Hermann , G . Melis , and E . Grefenstette , “The NarrativeQA Reading Comprehension Challenge , ” Dec . 2017 . [ 65 ] T . Kwiatkowski , J . Palomaki , O . Redfield , M . Collins , A . Parikh , C . Alberti , D . Epstein , I . Polosukhin , J . Devlin , K . Lee , K . Toutanova , L . Jones , M . Kelcey , M . - W . Chang , A . M . Dai , J . Uszkoreit , Q . Le , and S . Petrov , “Natural Questions : A Benchmark for Question Answering Research , ” Transactions of the Association for Computational Linguistics , vol . 7 , pp . 452 – 466 , 2019 . [ 66 ] E . Choi , H . He , M . Iyyer , M . Yatskar , W . - t . Yih , Y . Choi , P . Liang , and L . Zettlemoyer , “QuAC : Question Answering in Context , ” Aug . 2018 . [ 67 ] D . Wang , L . Dou , and W . Che , “A Survey on Table - and - Text HybridQA : Concepts , Methods , Challenges and Future Directions , ” Dec . 2022 . [ 68 ] Y . Lan , G . He , J . Jiang , J . Jiang , W . X . Zhao , and J . - R . Wen , “Complex Knowledge Base Question Answering : A Survey , ” Nov . 2022 . [ 69 ] D . Jin , E . Pan , N . Oufattole , W . - H . Weng , H . Fang , and P . Szolovits , “What Disease does this Patient Have ? A Large - scale Open Domain Question Answering Dataset from Medical Exams , ” Sep . 2020 . [ 70 ] E . Voorhees , T . Alam , S . Bedrick , D . Demner - Fushman , W . R . Hersh , K . Lo , K . Roberts , I . Soboroff , and L . L . Wang , “TREC - COVID : Constructing a Pandemic Information Retrieval Test Collection , ” May 2020 . [ 71 ] P . Dasigi , K . Lo , I . Beltagy , A . Cohan , N . A . Smith , and M . Gardner , “A Dataset of Information - Seeking Questions and Answers Anchored in Research Papers , ” May 2021 . [ 72 ] Q . Yang , Q . Chen , W . Wang , B . Hu , and M . Zhang , “Enhancing Multi - modal and Multi - hop Question Answering via Structured Knowledge and Unified Retrieval - Generation , ” Dec . 2022 . [ 73 ] R . Y . Zakari , J . W . Owusu , H . Wang , K . Qin , Z . K . Lawal , and Y . Dong , “VQA and Visual Reasoning : An Overview of Recent Datasets , Methods and Challenges , ” Dec . 2022 . [ 74 ] H . M . Fayek and J . Johnson , “Temporal Reasoning via Audio Question Answering , ” Nov . 2019 . [ 75 ] S . Lipping , P . Sudarsanam , K . Drossos , and T . Virtanen , “Clotho - AQA : A Crowdsourced Dataset for Audio Question Answering , ” Jun . 2022 . [ 76 ] Y . Zhong , J . Xiao , W . Ji , Y . Li , W . Deng , and T . - S . Chua , “Video Question Answering : Datasets , Algorithms and Challenges , ” Nov . 2022 . [ 77 ] M . Tapaswi , Y . Zhu , R . Stiefelhagen , A . Torralba , R . Urtasun , and S . Fidler , “MovieQA : Understanding Stories in Movies through Question - Answering , ” Sep . 2016 . [ 78 ] T . Maharaj , N . Ballas , A . Rohrbach , A . Courville , and C . Pal , “A dataset and exploration of models for understanding video data through fill - in - the - blank question - answering , ” Feb . 2017 . [ 79 ] J . Lei , L . Yu , M . Bansal , and T . L . Berg , “TVQA : Localized , Compositional Video Question Answering , ” May 2019 . [ 80 ] N . Garcia , M . Otani , C . Chu , and Y . Nakashima , “KnowIT VQA : Answering Knowledge - Based Questions about Videos , ” Dec . 2019 . [ 81 ] J . Mun , P . H . Seo , I . Jung , and B . Han , “MarioQA : Answering Questions by Watching Gameplay Videos , ” Aug . 2017 . [ 82 ] K . - M . Kim , M . - O . Heo , S . - H . Choi , and B . - T . Zhang , “DeepStory : Video Story QA by Deep Embedded Memory Networks , ” Jul . 2017 . [ 83 ] A . Colas , S . Kim , F . Dernoncourt , S . Gupte , Z . Wang , and D . S . Kim , “TutorialVQA : Question Answering Dataset for Tutorial Videos , ” in Proceedings of the Twelfth Language Resources and Evaluation Conference . Marseille , France : European Language Resources Association , May 2020 , pp . 5450 – 5455 . [ 84 ] J . Mao , X . Yang , X . Zhang , N . Goodman , and J . Wu , “CLEVRER - Humans : Describing Physical and Causal Events the Human Way , ” in Thirty - Sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , Oct . 2022 . [ 85 ] A . Talmor , O . Yoran , A . Catav , D . Lahav , Y . Wang , A . Asai , G . Ilharco , H . Hajishirzi , and J . Berant , “MultiModalQA : Complex Question Answering over Text , Tables and Images , ” arXiv : 2104 . 06039 [ cs ] , Apr . 2021 . [ 86 ] Y . Chang , M . Narang , H . Suzuki , G . Cao , J . Gao , and Y . Bisk , “WebQA : Multihop and Multimodal QA , ” Mar . 2022 . [ 87 ] J . Y . Li , A . Jansen , Q . Huang , J . Lee , R . Ganti , and D . Kuzmin , “MAQA : A Multimodal QA Benchmark for Negation , ” Jan . 2023 . [ 88 ] T . Xie , C . H . Wu , P . Shi , R . Zhong , T . Scholak , M . Yasunaga , C . - S . Wu , M . Zhong , P . Yin , S . I . Wang , V . Zhong , B . Wang , C . Li , C . Boyle , A . Ni , Z . Yao , D . Radev , C . Xiong , L . Kong , R . Zhang , N . A . Smith , L . Zettlemoyer , and T . Yu , “UnifiedSKG : Unifying and Multi - Tasking Structured Knowledge Grounding with Text - to - Text Language Models , ” Oct . 2022 . [ 89 ] V . Mavi , A . Jangra , and A . Jatowt , “A Survey on Multi - hop Question Answering and Generation , ” Apr . 2022 . [ 90 ] H . W . Chung , L . Hou , S . Longpre , B . Zoph , Y . Tay , W . Fedus , E . Li , X . Wang , M . Dehghani , S . Brahma , A . Webson , S . S . Gu , Z . Dai , M . Suzgun , X . Chen , A . Chowdhery , S . Narang , G . Mishra , A . Yu , V . Zhao , Y . Huang , A . Dai , H . Yu , S . Petrov , E . H . Chi , J . Dean , J . Devlin , A . Roberts , D . Zhou , Q . V . Le , and J . Wei , “Scaling Instruction - Finetuned , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 38 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco Language Models , ” Oct . 2022 . [ 91 ] R . Taylor , M . Kardas , G . Cucurull , T . Scialom , A . Hartshorn , E . Saravia , A . Poulton , V . Kerkez , and R . Stojnic , “Galactica : A Large Language Model for Science , ” Nov . 2022 . [ 92 ] O . Honovich , T . Scialom , O . Levy , and T . Schick , “Unnatural Instructions : Tuning Language Models with ( Almost ) No Human Labor , ” Dec . 2022 . [ 93 ] Y . Wang , S . Mishra , P . Alipoormolabashi , Y . Kordi , A . Mirzaei , A . Arunkumar , A . Ashok , A . S . Dhanasekaran , A . Naik , D . Stap , E . Pathak , G . Karamanolakis , H . G . Lai , I . Purohit , I . Mondal , J . Anderson , K . Kuznia , K . Doshi , M . Patel , K . K . Pal , M . Moradshahi , M . Parmar , M . Purohit , N . Varshney , P . R . Kaza , P . Verma , R . S . Puri , R . Karia , S . K . Sampat , S . Doshi , S . Mishra , S . Reddy , S . Patro , T . Dixit , X . Shen , C . Baral , Y . Choi , N . A . Smith , H . Hajishirzi , and D . Khashabi , “Super - NaturalInstructions : Generalization via Declarative Instructions on 1600 + NLP Tasks , ” Oct . 2022 . [ 94 ] Y . Wang , Y . Kordi , S . Mishra , A . Liu , N . A . Smith , D . Khashabi , and H . Hajishirzi , “Self - Instruct : Aligning Language Model with Self Generated Instructions , ” Dec . 2022 . [ 95 ] S . Iyer , X . V . Lin , R . Pasunuru , T . Mihaylov , D . Simig , P . Yu , K . Shuster , T . Wang , Q . Liu , P . S . Koura , X . Li , B . O’Horo , G . Pereyra , J . Wang , C . Dewan , A . Celikyilmaz , L . Zettlemoyer , and V . Stoyanov , “OPT - IML : Scaling Language Model Instruction Meta Learning through the Lens of Generalization , ” Jan . 2023 . [ 96 ] J . Wei , M . Bosma , V . Y . Zhao , K . Guu , A . W . Yu , B . Lester , N . Du , A . M . Dai , and Q . V . Le , “Finetuned Language Models Are Zero - Shot Learners , ” Feb . 2022 . [ 97 ] S . Mishra , D . Khashabi , C . Baral , and H . Hajishirzi , “Cross - Task Generalization via Natural Language Crowdsourcing Instructions , ” Mar . 2022 . [ 98 ] V . Sanh , A . Webson , C . Raffel , S . H . Bach , L . Sutawika , Z . Alyafeai , A . Chaffin , A . Stiegler , T . L . Scao , A . Raja , M . Dey , M . S . Bari , C . Xu , U . Thakker , S . S . Sharma , E . Szczechla , T . Kim , G . Chhablani , N . Nayak , D . Datta , J . Chang , M . T . - J . Jiang , H . Wang , M . Manica , S . Shen , Z . X . Yong , H . Pandey , R . Bawden , T . Wang , T . Neeraj , J . Rozen , A . Sharma , A . Santilli , T . Fevry , J . A . Fries , R . Teehan , T . Bers , S . Biderman , L . Gao , T . Wolf , and A . M . Rush , “Multitask Prompted Training Enables Zero - Shot Task Generalization , ” Mar . 2022 . [ 99 ] S . Qiao , Y . Ou , N . Zhang , X . Chen , Y . Yao , S . Deng , C . Tan , F . Huang , and H . Chen , “Reasoning with Language Model Prompting : A Survey , ” Dec . 2022 . [ 100 ] J . Weston , A . Bordes , S . Chopra , A . M . Rush , B . van Merriënboer , A . Joulin , and T . Mikolov , “Towards AI - Complete Question Answering : A Set of Prerequisite Toy Tasks , ” Dec . 2015 . [ 101 ] R . Mirzaee , H . Rajaby Faghihi , Q . Ning , and P . Kordjamshidi , “SPARTQA : A Textual Question Answering Benchmark for Spatial Reasoning , ” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies . Online : Association for Computational Linguistics , Jun . 2021 , pp . 4582 – 4598 . [ 102 ] A . Rogers , O . Kovaleva , M . Downey , and A . Rumshisky , “Getting Closer to AI Complete Question Answering : A Set of Prerequisite Real Tasks , ” Proceedings of the AAAI Conference on Artificial Intelligence , vol . 34 , no . 05 , pp . 8722 – 8731 , Apr . 2020 . [ 103 ] Q . Ning , H . Wu , R . Han , N . Peng , M . Gardner , and D . Roth , “TORQUE : A Reading Comprehension Dataset of Temporal Ordering Questions , ” Oct . 2020 . [ 104 ] Z . Jia , A . Abujabal , R . S . Roy , J . Stroetgen , and G . Weikum , “TEQUILA : Temporal Question Answering over Knowledge Bases , ” in Proceedings of the 27th ACM International Conference on Information and Knowledge Management , Oct . 2018 , pp . 1807 – 1810 . [ 105 ] Z . Jia , A . Abujabal , R . Saha Roy , J . Strötgen , and G . Weikum , “TempQuestions : A Benchmark for Temporal Question Answering , ” in Companion Proceedings of the The Web Conference 2018 , ser . WWW ’18 . Republic and Canton of Geneva , CHE : International World Wide Web Conferences Steering Committee , Apr . 2018 , pp . 1057 – 1062 . [ 106 ] S . Ostermann , A . Modi , M . Roth , S . Thater , and M . Pinkal , “MCScript : A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge , ” Mar . 2018 . [ 107 ] B . Zhou , D . Khashabi , Q . Ning , and D . Roth , ““Going on a vacation” takes longer than “Going for a walk” : A Study of Temporal Commonsense Understanding , ” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 3363 – 3369 . [ 108 ] L . Qin , A . Gupta , S . Upadhyay , L . He , Y . Choi , and M . Faruqui , “TIMEDIAL : Temporal Commonsense Reasoning in Dialog , ” Jun . 2021 . [ 109 ] J . Wang , A . Jatowt , and M . Yoshikawa , “ArchivalQA : A Large - scale Benchmark Dataset for Open Domain Question Answering over Historical News Collections , ” Feb . 2022 . [ 110 ] M . Zhang and E . Choi , “SituatedQA : Incorporating Extra - Linguistic Contexts into QA , ” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Online and Punta Cana , Dominican Republic : Association for Computational Linguistics , Nov . 2021 , pp . 7371 – 7387 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 39 [ 111 ] H . M . Fayek and J . Johnson , “Temporal Reasoning via Audio Question Answering , ” IEEE / ACM Transactions on Audio , Speech , and Language Processing , vol . 28 , pp . 2283 – 2294 , 2020 . [ 112 ] Y . Jang , Y . Song , Y . Yu , Y . Kim , and G . Kim , “TGIF - QA : Toward Spatio - Temporal Reasoning in Visual Question Answering , ” Dec . 2017 . [ 113 ] H . Rashkin , M . Sap , E . Allaway , N . A . Smith , and Y . Choi , “Event2Mind : Commonsense Inference on Events , Intents , and Reactions , ” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) . Melbourne , Australia : Association for Computational Linguistics , Jul . 2018 , pp . 463 – 473 . [ 114 ] K . Lin , O . Tafjord , P . Clark , and M . Gardner , “Reasoning Over Paragraph Effects in Situations , ” in Proceedings of the 2nd Workshop on Machine Reading for Question Answering . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 58 – 62 . [ 115 ] O . Tafjord , M . Gardner , K . Lin , andP . Clark , “QuaRTz : AnOpen - DomainDatasetofQualitativeRelationshipQuestions , ” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 5941 – 5946 . [ 116 ] R . Han , I . - H . Hsu , J . Sun , J . Baylon , Q . Ning , D . Roth , and N . Peng , “ESTER : A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations , ” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Online and Punta Cana , Dominican Republic : Association for Computational Linguistics , Nov . 2021 , pp . 7543 – 7559 . [ 117 ] M . Sap , H . Rashkin , D . Chen , R . Le Bras , and Y . Choi , “Social IQa : Commonsense Reasoning about Social Interactions , ” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 4463 – 4473 . [ 118 ] Y . Bisk , R . Zellers , R . L . Bras , J . Gao , and Y . Choi , “PIQA : Reasoning about Physical Commonsense in Natural Language , ” Proceedings of the AAAI Conference on Artificial Intelligence , vol . 34 , no . 05 , pp . 7432 – 7439 , Apr . 2020 . [ 119 ] O . Tafjord , P . Clark , M . Gardner , W . - t . Yih , and A . Sabharwal , “QuaRel : A Dataset and Models for Answering Questions about Qualitative Relationships , ” Nov . 2018 . [ 120 ] B . Y . Lin , S . Lee , R . Khanna , and X . Ren , “Birds have four legs ? ! NumerSense : Probing Numerical Commonsense Knowledge of Pre - Trained Language Models , ” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) . Online : Association for Computational Linguistics , Nov . 2020 , pp . 6862 – 6868 . [ 121 ] P . Dasigi , N . F . Liu , A . Marasović , N . A . Smith , and M . Gardner , “Quoref : A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning , ” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) . Hong Kong , China : Association for Computational Linguistics , Nov . 2019 , pp . 5925 – 5932 . [ 122 ] K . Sakaguchi , R . L . Bras , C . Bhagavatula , and Y . Choi , “WinoGrande : An Adversarial Winograd Schema Challenge at Scale , ” Nov . 2019 . [ 123 ] F . Hill , A . Bordes , S . Chopra , and J . Weston , “The Goldilocks Principle : Reading Children’s Books with Explicit Memory Representations , ” Apr . 2016 . [ 124 ] K . M . Hermann , T . Kočiský , E . Grefenstette , L . Espeholt , W . Kay , M . Suleyman , and P . Blunsom , “Teaching Machines to Read and Comprehend , ” Nov . 2015 . [ 125 ] T . Onishi , H . Wang , M . Bansal , K . Gimpel , and D . McAllester , “Who did What : A Large - Scale Person - Centered Cloze Dataset , ” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Austin , Texas : Association for Computational Linguistics , Nov . 2016 , pp . 2230 – 2235 . [ 126 ] P . Huber , A . Aghajanyan , B . Oğuz , D . Okhonko , W . - t . Yih , S . Gupta , and X . Chen , “CCQA : A New Web - Scale Question Answering Dataset for Model Pre - Training , ” arXiv : 2110 . 07731 [ cs ] , May 2022 . [ 127 ] P . Lewis , B . Oğuz , R . Rinott , S . Riedel , and H . Schwenk , “MLQA : Evaluating Cross - lingual Extractive Question Answering , ” May 2020 . [ 128 ] S . Longpre , Y . Lu , and J . Daiber , “MKQA : A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering , ” Aug . 2021 . [ 129 ] J . H . Clark , E . Choi , M . Collins , D . Garrette , T . Kwiatkowski , V . Nikolaev , and J . Palomaki , “TyDi QA : A Benchmark for Information - Seeking Question Answering in Typologically Diverse Languages , ” Mar . 2020 . [ 130 ] M . Artetxe , S . Ruder , and D . Yogatama , “On the Cross - lingual Transferability of Monolingual Representations , ” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020 , pp . 4623 – 4637 . [ 131 ] Z . Chi , L . Dong , F . Wei , N . Yang , S . Singhal , W . Wang , X . Song , X . - L . Mao , H . Huang , and M . Zhou , “InfoXLM : An Information - Theoretic Framework for Cross - Lingual Language Model Pre - Training , ” Apr . 2021 . [ 132 ] F . Shi , M . Suzgun , M . Freitag , X . Wang , S . Srivats , S . Vosoughi , H . W . Chung , Y . Tay , S . Ruder , D . Zhou , D . Das , and J . Wei , “Language Models are Multilingual Chain - of - Thought Reasoners , ” Oct . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 40 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco [ 133 ] C . Zhang , J . Sedoc , L . F . D’Haro , R . Banchs , and A . Rudnicky , “Automatic Evaluation and Moderation of Open - domain Dialogue Systems , ” Dec . 2021 . [ 134 ] T . Khot , K . Richardson , D . Khashabi , and A . Sabharwal , “Learning to Solve Complex Tasks by Talking to Agents , ” arXiv : 2110 . 08542 [ cs ] , Oct . 2021 . [ 135 ] H . Kim , J . Hessel , L . Jiang , X . Lu , Y . Yu , P . Zhou , R . L . Bras , M . Alikhani , G . Kim , M . Sap , and Y . Choi , “SODA : Million - scale Dialogue Distillation with Social Commonsense Contextualization , ” Dec . 2022 . [ 136 ] G . Karadzhov , T . Stafford , and A . Vlachos , “DeliData : A dataset for deliberation in multi - party problem solving , ” May 2022 . [ 137 ] V . Jeronymo , L . Bonifacio , H . Abonizio , M . Fadaee , R . Lotufo , J . Zavrel , and R . Nogueira , “InPars - v2 : Large Language Models as Efficient Dataset Generators for Information Retrieval , ” Jan . 2023 . [ 138 ] K . Lee , M . - W . Chang , and K . Toutanova , “Latent Retrieval for Weakly Supervised Open Domain Question Answering , ” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Florence , Italy : Association for Computational Linguistics , Jul . 2019 , pp . 6086 – 6096 . [ 139 ] K . Wang , N . Thakur , N . Reimers , and I . Gurevych , “GPL : Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval , ” Apr . 2022 . [ 140 ] N . Thakur , N . Reimers , A . Rücklé , A . Srivastava , and I . Gurevych , “BEIR : A Heterogenous Benchmark for Zero - shot Evaluation of Information Retrieval Models , ” Oct . 2021 . [ 141 ] Z . Dai , V . Y . Zhao , J . Ma , Y . Luan , J . Ni , J . Lu , A . Bakalov , K . Guu , K . B . Hall , and M . - W . Chang , “Promptagator : Few - shot Dense Retrieval From 8 Examples , ” Sep . 2022 . [ 142 ] Y . Yu , C . Xiong , S . Sun , C . Zhang , and A . Overwijk , “COCO - DR : Combating Distribution Shifts in Zero - Shot Dense Retrieval with Contrastive and Distributionally Robust Learning , ” 2022 . [ 143 ] H . Trivedi , N . Balasubramanian , T . Khot , and A . Sabharwal , “Teaching Broad Reasoning Skills for Multi - Step QA by Generating Hard Contexts , ” Nov . 2022 . [ 144 ] K . D . Dhole , V . Gangal , S . Gehrmann , A . Gupta , Z . Li , S . Mahamood , A . Mahendiran , S . Mille , A . Shrivastava , S . Tan , T . Wu , J . Sohl - Dickstein , J . D . Choi , E . Hovy , O . Dusek , S . Ruder , S . Anand , N . Aneja , R . Banjade , L . Barthe , H . Behnke , I . Berlot - Attwell , C . Boyle , C . Brun , M . A . S . Cabezudo , S . Cahyawijaya , E . Chapuis , W . Che , M . Choudhary , C . Clauss , P . Colombo , F . Cornell , G . Dagan , M . Das , T . Dixit , T . Dopierre , P . - A . Dray , S . Dubey , T . Ekeinhor , M . Di Giovanni , T . Goyal , R . Gupta , R . Gupta , L . Hamla , S . Han , F . Harel - Canada , A . Honore , I . Jindal , P . K . Joniak , D . Kleyko , V . Kovatchev , K . Krishna , A . Kumar , S . Langer , S . R . Lee , C . J . Levinson , H . Liang , K . Liang , Z . Liu , A . Lukyanenko , V . Marivate , G . de Melo , S . Meoni , M . Meyer , A . Mir , N . S . Moosavi , N . Muennighoff , T . S . H . Mun , K . Murray , M . Namysl , M . Obedkova , P . Oli , N . Pasricha , J . Pfister , R . Plant , V . Prabhu , V . Pais , L . Qin , S . Raji , P . K . Rajpoot , V . Raunak , R . Rinberg , N . Roberts , J . D . Rodriguez , C . Roux , V . P . H . S . , A . B . Sai , R . M . Schmidt , T . Scialom , T . Sefara , S . N . Shamsi , X . Shen , H . Shi , Y . Shi , A . Shvets , N . Siegel , D . Sileo , J . Simon , C . Singh , R . Sitelew , P . Soni , T . Sorensen , W . Soto , A . Srivastava , K . A . Srivatsa , T . Sun , M . V . T , A . Tabassum , F . A . Tan , R . Teehan , M . Tiwari , M . Tolkiehn , A . Wang , Z . Wang , G . Wang , Z . J . Wang , F . Wei , B . Wilie , G . I . Winata , X . Wu , W . Wydmański , T . Xie , U . Yaseen , M . A . Yee , J . Zhang , and Y . Zhang , “NL - Augmenter : A Framework for Task - Sensitive Natural Language Augmentation , ” Oct . 2022 . [ 145 ] W . Yuan and P . Liu , “reStructured Pre - training , ” p . 111 , 2022 . [ 146 ] L . Budach , M . Feuerpfeil , N . Ihde , A . Nathansen , N . Noack , H . Patzlaff , F . Naumann , and H . Harmouch , “The Effects of Data Quality on Machine Learning Performance , ” Nov . 2022 . [ 147 ] S . E . Whang , Y . Roh , H . Song , and J . - G . Lee , “Data Collection and Quality Challenges in Deep Learning : A Data - Centric AI Perspective , ” Dec . 2022 . [ 148 ] F . Pfisterer , J . Thomas , and B . Bischl , “Towards Human Centered AutoML , ” Nov . 2019 . [ 149 ] Z . Sun , L . Li , Y . Liu , X . Du , and L . Li , “On the Importance of Building High - quality Training Datasets for Neural Code Search , ” Feb . 2022 . [ 150 ] K . S . Kalyan , A . Rajasekharan , and S . Sangeetha , “AMMUS : A Survey of Transformer - based Pretrained Models in Natural Language Processing , ” Aug . 2021 . [ 151 ] Y . Qin , Y . Lin , J . Yi , J . Zhang , X . Han , Z . Zhang , Y . Su , Z . Liu , P . Li , M . Sun , and J . Zhou , “Knowledge Inheritance for Pre - trained Language Models , ” Apr . 2022 . [ 152 ] G . Lample and A . Conneau , “Cross - lingual Language Model Pretraining , ” Jan . 2019 . [ 153 ] Z . Lan , M . Chen , S . Goodman , K . Gimpel , P . Sharma , and R . Soricut , “ALBERT : A Lite BERT for Self - supervised Learning of Language Representations , ” Feb . 2020 . [ 154 ] T . Gao , X . Yao , and D . Chen , “SimCSE : Simple Contrastive Learning of Sentence Embeddings , ” May 2022 . [ 155 ] T . Chen , S . Kornblith , M . Norouzi , and G . Hinton , “A Simple Framework for Contrastive Learning of Visual Represen - tations , ” in Proceedings of the 37th International Conference on Machine Learning . PMLR , Nov . 2020 , pp . 1597 – 1607 . [ 156 ] X . Chen , H . Fan , R . Girshick , and K . He , “Improved Baselines with Momentum Contrastive Learning , ” Mar . 2020 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 41 [ 157 ] J . - B . Grill , F . Strub , F . Altché , C . Tallec , P . H . Richemond , E . Buchatskaya , C . Doersch , B . A . Pires , Z . D . Guo , M . G . Azar , B . Piot , K . Kavukcuoglu , R . Munos , and M . Valko , “Bootstrap your own latent : A new approach to self - supervised Learning , ” Sep . 2020 . [ 158 ] Y . Tay , M . Dehghani , V . Q . Tran , X . Garcia , J . Wei , X . Wang , H . W . Chung , D . Bahri , T . Schuster , H . S . Zheng , D . Zhou , N . Houlsby , and D . Metzler , “UL2 : Unifying Language Learning Paradigms , ” Oct . 2022 . [ 159 ] X . Pi , Q . Liu , B . Chen , M . Ziyadi , Z . Lin , Q . Fu , Y . Gao , J . - G . Lou , and W . Chen , “Reasoning Like Program Executors , ” Oct . 2022 . [ 160 ] B . McCann , N . S . Keskar , C . Xiong , and R . Socher , “The Natural Language Decathlon : Multitask Learning as Question Answering , ” Jun . 2018 . [ 161 ] X . Liu , P . He , W . Chen , and J . Gao , “Multi - Task Deep Neural Networks for Natural Language Understanding , ” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Florence , Italy : Association for Computational Linguistics , Jul . 2019 , pp . 4487 – 4496 . [ 162 ] D . Mahajan , A . Poddar , J . J . Liang , Y . - T . Lin , J . M . Prager , P . Suryanarayanan , P . Raghavan , andC . - H . Tsou , “Identification of Semantically Similar Sentences in Clinical Notes : Iterative Intermediate Training Using Multi - Task Learning , ” JMIR medical informatics , vol . 8 , no . 11 , p . e22508 , Nov . 2020 . [ 163 ] A . J . Piergiovanni , W . Li , W . Kuo , M . Saffar , F . Bertsch , and A . Angelova , “Answer - Me : Multi - Task Open - Vocabulary Visual Question Answering , ” Nov . 2022 . [ 164 ] S . Ji and P . Marttinen , “Patient Outcome and Zero - shot Diagnosis Prediction with Hypernetwork - guided Multitask Learning , ” Jan . 2023 . [ 165 ] T . Goodwin , M . Savery , and D . Demner - Fushman , “Towards Zero - Shot Conditional Summarization with Adaptive Multi - TaskFine - Tuning , ”in FindingsoftheAssociationforComputationalLinguistics : EMNLP2020 . Online : Association for Computational Linguistics , Nov . 2020 , pp . 3215 – 3226 . [ 166 ] Y . Bai , Y . Gao , and H . Huang , “Cross - Lingual Abstractive Summarization with Limited Parallel Resources , ” Jun . 2021 . [ 167 ] D . Jin , Z . Jin , J . T . Zhou , L . Orii , and P . Szolovits , “Hooks in the Headline : Learning to Generate Headlines with Controlled Styles , ” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Online : Association for Computational Linguistics , Jul . 2020 , pp . 5082 – 5093 . [ 168 ] W . Yuan , H . Yin , T . He , T . Chen , Q . Wang , and L . Cui , “Unified Question Generation with Continual Lifelong Learning , ” Proceedings of the ACM Web Conference 2022 , pp . 871 – 881 , Apr . 2022 . [ 169 ] T . Scialom , T . Chakrabarty , and S . Muresan , “Fine - tuned Language Models are Continual Learners , ” Oct . 2022 . [ 170 ] Y . Boreshban , S . M . Mirbostani , G . Ghassem - Sani , S . A . Mirroshandel , and S . Amiriparian , “Improving Question Answering Performance Using Knowledge Distillation and Active Learning , ” arXiv : 2109 . 12662 [ cs ] , Sep . 2021 . [ 171 ] J . Jukić and J . Šnajder , “Smooth Sailing : Improving Active Learning for Pre - trained Language Models with Represen - tation Smoothness Analysis , ” Dec . 2022 . [ 172 ] R . Kocielnik , S . Kangaslahti , S . Prabhumoye , M . Hari , R . M . Alvarez , and A . Anandkumar , “Can You Label Less by Using Out - of - Domain Data ? Active & Transfer Learning with Few - shot Instructions , ” Nov . 2022 . [ 173 ] Y . Yu , L . Kong , J . Zhang , R . Zhang , andC . Zhang , “AcTune : Uncertainty - awareActiveSelf - TrainingforSemi - Supervised Active Learning with Pretrained Language Models , ” May 2022 . [ 174 ] S . Budd , E . C . Robinson , and B . Kainz , “A Survey on Active Learning and Human - in - the - Loop Deep Learning for Medical Image Analysis , ” Medical Image Analysis , vol . 71 , p . 102062 , Jul . 2021 . [ 175 ] B . Deb , G . Zheng , andA . H . Awadallah , “BoostingNaturalLanguageGenerationfromInstructionswithMeta - Learning , ” Oct . 2022 . [ 176 ] R . Upadhyay , R . Phlypo , R . Saini , and M . Liwicki , “Sharing to learn and learning to share – Fitting together Meta - Learning , Multi - Task Learning , and Transfer Learning : A meta review , ” Jan . 2023 . [ 177 ] J . Baxter , “Theoretical Models of Learning to Learn , ” in Learning to Learn , S . Thrun and L . Pratt , Eds . Boston , MA : Springer US , 1998 , pp . 71 – 94 . [ 178 ] S . Thrun and L . Pratt , “Learning to Learn : Introduction and Overview , ” in Learning to Learn , S . Thrun and L . Pratt , Eds . Boston , MA : Springer US , 1998 , pp . 3 – 17 . [ 179 ] Z . Li , N . Yang , L . Wang , and F . Wei , “Learning Diverse Document Representations with Deep Query Interactions for Dense Retrieval , ” undefined , 2022 . [ 180 ] Y . Deng , W . Zhang , and W . Lam , “Multi - hop Inference for Question - driven Summarization , ” arXiv : 2010 . 03738 [ cs ] , Oct . 2020 . [ 181 ] A . Goyal , A . L . Friesen , A . Banino , T . Weber , N . R . Ke , A . P . Badia , A . Guez , M . Mirza , P . C . Humphreys , K . Konyushkova , L . Sifre , M . Valko , S . Osindero , T . Lillicrap , N . Heess , and C . Blundell , “Retrieval - Augmented Reinforcement Learning , ” arXiv : 2202 . 08417 [ cs ] , Mar . 2022 . [ 182 ] L . Zhou and K . Small , “Inverse Reinforcement Learning with Natural Language Goals , ” Dec . 2020 . [ 183 ] Y . Pruksachatkun , J . Phang , H . Liu , P . M . Htut , X . Zhang , R . Y . Pang , C . Vania , K . Kann , andS . R . Bowman , “Intermediate - Task Transfer Learning with Pretrained Models for Natural Language Understanding : When and Why Does It Work ? ” , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 42 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco May 2020 . [ 184 ] N . Ding , Y . Qin , G . Yang , F . Wei , Z . Yang , Y . Su , S . Hu , Y . Chen , C . - M . Chan , W . Chen , J . Yi , W . Zhao , X . Wang , Z . Liu , H . - T . Zheng , J . Chen , Y . Liu , J . Tang , J . Li , and M . Sun , “Delta Tuning : A Comprehensive Study of Parameter Efficient Methods for Pre - trained Language Models , ” Mar . 2022 . [ 185 ] R . He , L . Liu , H . Ye , Q . Tan , B . Ding , L . Cheng , J . Low , L . Bing , and L . Si , “On the Effectiveness of Adapter - based Tuning for Pretrained Language Model Adaptation , ” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) . Online : Association for Computational Linguistics , Aug . 2021 , pp . 2208 – 2222 . [ 186 ] R . K . Mahabadi , L . Zettlemoyer , J . Henderson , M . Saeidi , L . Mathias , V . Stoyanov , and M . Yazdani , “PERFECT : Prompt - free and Efficient Few - shot Learning with Language Models , ” Apr . 2022 . [ 187 ] R . Yuan , Z . Wang , Z . Cao , and W . Li , “Few - shot Query - Focused Summarization with Prefix - Merging , ” Nov . 2022 . [ 188 ] X . Liu , T . Sun , X . Huang , and X . Qiu , “Late Prompt Tuning : A Late Prompt Could Be Better Than Many Prompts , ” Oct . 2022 . [ 189 ] E . J . Hu , Y . Shen , P . Wallis , Z . Allen - Zhu , Y . Li , S . Wang , L . Wang , and W . Chen , “LoRA : Low - Rank Adaptation of Large Language Models , ” Oct . 2021 . [ 190 ] J . Phang , Y . Mao , P . He , and W . Chen , “HyperTuning : Toward Adapting Large Language Models without Back - propagation , ” Nov . 2022 . [ 191 ] T . Schuster , A . Fisch , J . Gupta , M . Dehghani , D . Bahri , V . Q . Tran , Y . Tay , and D . Metzler , “Confident Adaptive Language Modeling , ” Oct . 2022 . [ 192 ] Y . Tay , J . Wei , H . W . Chung , V . Q . Tran , D . R . So , S . Shakeri , X . Garcia , H . S . Zheng , J . Rao , A . Chowdhery , D . Zhou , D . Metzler , S . Petrov , N . Houlsby , Q . V . Le , and M . Dehghani , “Transcending Scaling Laws with 0 . 1 % Extra Compute , ” Nov . 2022 . [ 193 ] Anonymous , “Task Ambiguity in Humans and Language Models , ” in The Eleventh International Conference on Learning Representations , Nov . 2022 . [ 194 ] D . Ganguli , L . Lovitt , J . Kernion , A . Askell , Y . Bai , S . Kadavath , B . Mann , E . Perez , N . Schiefer , K . Ndousse , A . Jones , S . Bowman , A . Chen , T . Conerly , N . DasSarma , D . Drain , N . Elhage , S . El - Showk , S . Fort , Z . Hatfield - Dodds , T . Henighan , D . Hernandez , T . Hume , J . Jacobson , S . Johnston , S . Kravec , C . Olsson , S . Ringer , E . Tran - Johnson , D . Amodei , T . Brown , N . Joseph , S . McCandlish , C . Olah , J . Kaplan , and J . Clark , “Red Teaming Language Models to Reduce Harms : Methods , Scaling Behaviors , and Lessons Learned , ” Nov . 2022 . [ 195 ] E . Perez , S . Huang , F . Song , T . Cai , R . Ring , J . Aslanides , A . Glaese , N . McAleese , and G . Irving , “Red Teaming Language Models with Language Models , ” Feb . 2022 . [ 196 ] E . Frantar and D . Alistarh , “SparseGPT : Massive Language Models Can Be Accurately Pruned in One - Shot , ” Jan . 2023 . [ 197 ] T . B . Brown , B . Mann , N . Ryder , M . Subbiah , J . Kaplan , P . Dhariwal , A . Neelakantan , P . Shyam , G . Sastry , A . Askell , S . Agarwal , A . Herbert - Voss , G . Krueger , T . Henighan , R . Child , A . Ramesh , D . M . Ziegler , J . Wu , C . Winter , C . Hesse , M . Chen , E . Sigler , M . Litwin , S . Gray , B . Chess , J . Clark , C . Berner , S . McCandlish , A . Radford , I . Sutskever , and D . Amodei , “Language Models are Few - Shot Learners , ” Jul . 2020 . [ 198 ] L . Beurer - Kellner , M . Fischer , and M . Vechev , “Prompting Is Programming : A Query Language For Large Language Models , ” Dec . 2022 . [ 199 ] O . Khattab , K . Santhanam , X . L . Li , D . Hall , P . Liang , C . Potts , andM . Zaharia , “Demonstrate - Search - Predict : Composing retrieval and language models for knowledge - intensive NLP , ” Jan . 2023 . [ 200 ] Y . Shen , K . Song , X . Tan , D . Li , W . Lu , and Y . Zhuang , “HuggingGPT : Solving AI Tasks with ChatGPT and its Friends in HuggingFace , ” Apr . 2023 . [ 201 ] J . Reppert , B . Rachbach , C . George , L . Stebbing , J . Byun , M . Appleton , and A . Stuhlmüller , “Iterated Decomposition : Improving Science Q & A by Supervising Reasoning Processes , ” Jan . 2023 . [ 202 ] S . Yao , J . Zhao , D . Yu , N . Du , I . Shafran , K . Narasimhan , and Y . Cao , “ReAct : Synergizing Reasoning and Acting in Language Models , ” Oct . 2022 . [ 203 ] E . Perez , P . Lewis , W . - t . Yih , K . Cho , and D . Kiela , “Unsupervised Question Decomposition for Question Answering , ” arXiv : 2002 . 09758 [ cs ] , Oct . 2020 . [ 204 ] T . Khot , D . Khashabi , K . Richardson , P . Clark , and A . Sabharwal , “Text Modular Networks : Learning to Decompose Tasks in the Language of Existing Models , ” arXiv : 2009 . 00751 [ cs ] , Apr . 2021 . [ 205 ] D . Shi , Y . Guo , M . Guo , Y . Wu , Q . Chen , and N . Cao , “Talk2Data : High - Level Question Decomposition for Data - Oriented Question and Answering , ” arXiv : 2107 . 14420 [ cs ] , Jul . 2021 . [ 206 ] A . Kalyanpur , S . Patwardhan , B . K . Boguraev , A . Lally , and J . Chu - Carroll , “Fact - based question decomposition in DeepQA , ” IBM Journal of Research and Development , vol . 56 , no . 3 . 4 , pp . 13 : 1 – 13 : 11 , May 2012 . [ 207 ] H . Yang , H . Wang , S . Guo , W . Zhang , and H . Chen , “Learning to Decompose Compound Questions with Reinforcement Learning , ” Sep . 2018 . [ 208 ] D . Dua , S . Gupta , S . Singh , and M . Gardner , “Successive Prompting for Decomposing Complex Questions , ” Dec . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 43 [ 209 ] S . Borgeaud , A . Mensch , J . Hoffmann , T . Cai , E . Rutherford , K . Millican , G . van den Driessche , J . - B . Lespiau , B . Damoc , A . Clark , D . d . L . Casas , A . Guy , J . Menick , R . Ring , T . Hennigan , S . Huang , L . Maggiore , C . Jones , A . Cassirer , A . Brock , M . Paganini , G . Irving , O . Vinyals , S . Osindero , K . Simonyan , J . W . Rae , E . Elsen , and L . Sifre , “Improving language models by retrieving from trillions of tokens , ” arXiv : 2112 . 04426 [ cs ] , Feb . 2022 . [ 210 ] D . Chen , A . Fisch , J . Weston , and A . Bordes , “Reading Wikipedia to Answer Open - Domain Questions , ” Apr . 2017 . [ 211 ] M . de Jong , Y . Zemlyanskiy , J . Ainslie , N . FitzGerald , S . Sanghai , F . Sha , and W . Cohen , “FiDO : Fusion - in - Decoder optimized for stronger performance and faster inference , ” Dec . 2022 . [ 212 ] G . Izacard , P . Lewis , M . Lomeli , L . Hosseini , F . Petroni , T . Schick , J . Dwivedi - Yu , A . Joulin , S . Riedel , and E . Grave , “Atlas : Few - shot Learning with Retrieval Augmented Language Models , ” Nov . 2022 . [ 213 ] I . - J . Liu , X . Yuan , M . - A . Côté , P . - Y . Oudeyer , and A . G . Schwing , “Asking for Knowledge : Training RL Agents to Query External Knowledge Using Language , ” Jul . 2022 . [ 214 ] T . Schick , J . Dwivedi - Yu , R . Dessì , R . Raileanu , M . Lomeli , L . Zettlemoyer , N . Cancedda , and T . Scialom , “Toolformer : Language Models Can Teach Themselves to Use Tools , ” Feb . 2023 . [ 215 ] N . Varshney , M . Luo , and C . Baral , “Can Open - Domain QA Reader Utilize External Knowledge Efficiently like Humans ? ” Nov . 2022 . [ 216 ] M . Sung , J . Park , J . Kang , D . Chen , and J . Lee , “Optimizing Test - Time Query Representations for Dense Retrieval , ” Dec . 2022 . [ 217 ] S . Pramanik , J . Alabi , R . S . Roy , and G . Weikum , “UNIQORN : Unified Question Answering over RDF Knowledge Graphs and Natural Language Text , ” arXiv : 2108 . 08614 [ cs ] , Apr . 2022 . [ 218 ] B . Jin , Y . Zhang , Q . Zhu , and J . Han , “Heterformer : A Transformer Architecture for Node Representation Learning on Heterogeneous Text - Rich Networks , ” May 2022 . [ 219 ] K . Shuster , M . Komeili , L . Adolphs , S . Roller , A . Szlam , and J . Weston , “Language Models that Seek for Knowledge : Modular Search & Generation for Dialogue and Prompt Completion , ” arXiv : 2203 . 13224 [ cs ] , Mar . 2022 . [ 220 ] L . Gao , A . Madaan , S . Zhou , U . Alon , P . Liu , Y . Yang , J . Callan , and G . Neubig , “PAL : Program - aided Language Models , ” Jan . 2023 . [ 221 ] X . Zhu , J . Wang , L . Zhang , Y . Zhang , R . Gan , J . Zhang , and Y . Yang , “Solving Math Word Problem via Cooperative Reasoning induced Language Models , ” Oct . 2022 . [ 222 ] P . Haluptzok , M . Bowers , and A . T . Kalai , “Language Models Can Teach Themselves to Program Better , ” Sep . 2022 . [ 223 ] M . Chen , J . Tworek , H . Jun , Q . Yuan , H . P . d . O . Pinto , J . Kaplan , H . Edwards , Y . Burda , N . Joseph , G . Brockman , A . Ray , R . Puri , G . Krueger , M . Petrov , H . Khlaaf , G . Sastry , P . Mishkin , B . Chan , S . Gray , N . Ryder , M . Pavlov , A . Power , L . Kaiser , M . Bavarian , C . Winter , P . Tillet , F . P . Such , D . Cummings , M . Plappert , F . Chantzis , E . Barnes , A . Herbert - Voss , W . H . Guss , A . Nichol , A . Paino , N . Tezak , J . Tang , I . Babuschkin , S . Balaji , S . Jain , W . Saunders , C . Hesse , A . N . Carr , J . Leike , J . Achiam , V . Misra , E . Morikawa , A . Radford , M . Knight , M . Brundage , M . Murati , K . Mayer , P . Welinder , B . McGrew , D . Amodei , S . McCandlish , I . Sutskever , and W . Zaremba , “Evaluating Large Language Models Trained on Code , ” Jul . 2021 . [ 224 ] Y . Li , D . Choi , J . Chung , N . Kushman , J . Schrittwieser , R . Leblond , T . Eccles , J . Keeling , F . Gimeno , A . D . Lago , T . Hubert , P . Choy , C . d . M . d’Autume , I . Babuschkin , X . Chen , P . - S . Huang , J . Welbl , S . Gowal , A . Cherepanov , J . Molloy , D . J . Mankowitz , E . S . Robson , P . Kohli , N . de Freitas , K . Kavukcuoglu , and O . Vinyals , “Competition - Level Code Generation with AlphaCode , ” Feb . 2022 . [ 225 ] O . Daniels - Koch and R . Freedman , “The Expertise Problem : Learning from Specialized Feedback , ” Nov . 2022 . [ 226 ] W . Ye , P . Abbeel , and Y . Gao , “Spending Thinking Time Wisely : Accelerating MCTS with Virtual Expansions , ” Nov . 2021 . [ 227 ] J . Laurent and A . Platzer , “Learning to Find Proofs and Theorems by Learning to Refine Search Strategies , ” undefined , 2022 . [ 228 ] A . Bakhtin , D . J . Wu , A . Lerer , J . Gray , A . P . Jacob , G . Farina , A . H . Miller , and N . Brown , “Mastering the Game of No - Press Diplomacy via Human - Regularized Reinforcement Learning and Planning , ” Oct . 2022 . [ 229 ] M . Yang , D . Schuurmans , P . Abbeel , and O . Nachum , “Chain of Thought Imitation with Procedure Cloning , ” May 2022 . [ 230 ] J . Menick , M . Trebacz , V . Mikulik , J . Aslanides , F . Song , M . Chadwick , M . Glaese , S . Young , L . Campbell - Gillingham , G . Irving , and N . McAleese , “Teaching language models to support answers with verified quotes , ” Mar . 2022 . [ 231 ] D . Dohan , W . Xu , A . Lewkowycz , J . Austin , D . Bieber , R . G . Lopes , Y . Wu , H . Michalewski , R . A . Saurous , J . Sohl - dickstein , K . Murphy , and C . Sutton , “Language Model Cascades , ” Jul . 2022 . [ 232 ] T . Wu , E . Jiang , A . Donsbach , J . Gray , A . Molina , M . Terry , and C . Cai , PromptChainer : Chaining Large Language Model Prompts through Visual Programming , Mar . 2022 . [ 233 ] A . Creswell , M . Shanahan , and I . Higgins , “Selection - Inference : Exploiting Large Language Models for Interpretable Logical Reasoning , ” May 2022 . [ 234 ] A . Creswell and M . Shanahan , “Faithful Reasoning Using Large Language Models , ” Aug . 2022 . [ 235 ] B . Wang , X . Deng , and H . Sun , “Iteratively Prompt Pre - trained Language Models for Chain of Thought , ” Oct . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 44 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco [ 236 ] K . Yang , Y . Tian , N . Peng , and D . Klein , “Re3 : Generating Longer Stories With Recursive Reprompting and Revision , ” Oct . 2022 . [ 237 ] J . Chen , Q . Bao , C . Sun , X . Zhang , J . Chen , H . Zhou , Y . Xiao , and L . Li , “LOREN : Logic - Regularized Reasoning for Interpretable Fact Verification , ” Proceedings of the AAAI Conference on Artificial Intelligence , vol . 36 , no . 10 , pp . 10482 – 10491 , Jun . 2022 . [ 238 ] Z . Guo , M . Schlichtkrull , and A . Vlachos , “A Survey on Automated Fact - Checking , ” Jun . 2022 . [ 239 ] C . Zhou , G . Neubig , J . Gu , M . Diab , P . Guzman , L . Zettlemoyer , and M . Ghazvininejad , “Detecting Hallucinated Content in Conditional Neural Sequence Generation , ” Jun . 2021 . [ 240 ] T . Liu , Q . Guo , X . Hu , Y . Zhang , X . Qiu , and Z . Zhang , “RLET : A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees , ” Oct . 2022 . [ 241 ] H . Xu , Z . Lin , J . Zhou , Y . Zheng , and Z . Yang , “A Universal Discriminator for Zero - Shot Generalization , ” Nov . 2022 . [ 242 ] M . Li , S . Gururangan , T . Dettmers , M . Lewis , T . Althoff , N . A . Smith , and L . Zettlemoyer , “Branch - Train - Merge : Embarrassingly Parallel Training of Expert Language Models , ” Aug . 2022 . [ 243 ] S . Gururangan , M . Lewis , A . Holtzman , N . A . Smith , and L . Zettlemoyer , “DEMix Layers : Disentangling Domains for Modular Language Modeling , ” Aug . 2021 . [ 244 ] B . Peng , M . Galley , P . He , C . Brockett , L . Liden , E . Nouri , Z . Yu , B . Dolan , and J . Gao , “GODEL : Large - Scale Pre - Training for Goal - Directed Dialog , ” Jun . 2022 . [ 245 ] Z . Chen , Y . Liu , L . Chen , S . Zhu , M . Wu , and K . Yu , “OPAL : Ontology - Aware Pretrained Language Model for End - to - End Task - Oriented Dialogue , ” Sep . 2022 . [ 246 ] D . Schuurmans , “Memory Augmented Large Language Models are Computationally Universal , ” Jan . 2023 . [ 247 ] A . Bulatov , Y . Kuratov , and M . S . Burtsev , “Recurrent Memory Transformer , ” Dec . 2022 . [ 248 ] J . Xu , A . Szlam , and J . Weston , “Beyond Goldfish Memory : Long - Term Open - Domain Conversation , ” Jul . 2021 . [ 249 ] Y . Li , Z . Lin , S . Zhang , Q . Fu , B . Chen , J . - G . Lou , and W . Chen , “On the Advance of Making Language Models Better Reasoners , ” Jun . 2022 . [ 250 ] X . Wang , J . Wei , D . Schuurmans , Q . Le , E . Chi , S . Narang , A . Chowdhery , and D . Zhou , “Self - Consistency Improves Chain of Thought Reasoning in Language Models , ” Apr . 2022 . [ 251 ] K . Cobbe , V . Kosaraju , M . Bavarian , M . Chen , H . Jun , L . Kaiser , M . Plappert , J . Tworek , J . Hilton , R . Nakano , C . Hesse , and J . Schulman , “Training Verifiers to Solve Math Word Problems , ” Nov . 2021 . [ 252 ] E . Zelikman , Y . Wu , J . Mu , and N . D . Goodman , “STaR : Bootstrapping Reasoning With Reasoning , ” May 2022 . [ 253 ] P . P . Liang , A . Zadeh , and L . - P . Morency , “Foundations and Recent Trends in Multimodal Machine Learning : Principles , Challenges , and Open Questions , ” Sep . 2022 . [ 254 ] N . Q . Huy , T . M . Phuong , and N . X . Bach , “Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation , ” Apr . 2022 . [ 255 ] S . Li , Y . Du , J . B . Tenenbaum , A . Torralba , and I . Mordatch , “Composing Ensembles of Pre - trained Models via Iterative Consensus , ” Oct . 2022 . [ 256 ] J . Andreas , M . Rohrbach , T . Darrell , and D . Klein , “Neural Module Networks , ” Jul . 2017 . [ 257 ] C . Hu , X . Li , D . Liu , X . Chen , J . Wang , and X . Liu , “Teacher - Student Architecture for Knowledge Learning : A Survey , ” Oct . 2022 . [ 258 ] J . Wang , A . Jatowt , and M . Yoshikawa , “TimeBERT : Extending Pre - Trained Language Representations with Temporal Information , ” Aug . 2022 . [ 259 ] A . Chowdhery , S . Narang , J . Devlin , M . Bosma , G . Mishra , A . Roberts , P . Barham , H . W . Chung , C . Sutton , S . Gehrmann , P . Schuh , K . Shi , S . Tsvyashchenko , J . Maynez , A . Rao , P . Barnes , Y . Tay , N . Shazeer , V . Prabhakaran , E . Reif , N . Du , B . Hutchinson , R . Pope , J . Bradbury , J . Austin , M . Isard , G . Gur - Ari , P . Yin , T . Duke , A . Levskaya , S . Ghemawat , S . Dev , H . Michalewski , X . Garcia , V . Misra , K . Robinson , L . Fedus , D . Zhou , D . Ippolito , D . Luan , H . Lim , B . Zoph , A . Spiridonov , R . Sepassi , D . Dohan , S . Agrawal , M . Omernick , A . M . Dai , T . S . Pillai , M . Pellat , A . Lewkowycz , E . Moreira , R . Child , O . Polozov , K . Lee , Z . Zhou , X . Wang , B . Saeta , M . Diaz , O . Firat , M . Catasta , J . Wei , K . Meier - Hellstern , D . Eck , J . Dean , S . Petrov , and N . Fiedel , “PaLM : Scaling Language Modeling with Pathways , ” arXiv : 2204 . 02311 [ cs ] , Apr . 2022 . [ 260 ] J . Wei , X . Wang , D . Schuurmans , M . Bosma , B . Ichter , F . Xia , E . Chi , Q . Le , and D . Zhou , “Chain of Thought Prompting Elicits Reasoning in Large Language Models , ” Jun . 2022 . [ 261 ] T . Kojima , S . S . Gu , M . Reid , Y . Matsuo , and Y . Iwasawa , “Large Language Models are Zero - Shot Reasoners , ” Jan . 2023 . [ 262 ] A . Drozdov , N . Schärli , E . Akyürek , N . Scales , X . Song , X . Chen , O . Bousquet , and D . Zhou , “Compositional Semantic Parsing with Large Language Models , ” Sep . 2022 . [ 263 ] D . Zhou , N . Schärli , L . Hou , J . Wei , N . Scales , X . Wang , D . Schuurmans , O . Bousquet , Q . Le , and E . Chi , “Least - to - Most Prompting Enables Complex Reasoning in Large Language Models , ” May 2022 . [ 264 ] O . Rubin , J . Herzig , and J . Berant , “Learning To Retrieve Prompts for In - Context Learning , ” May 2022 . [ 265 ] S . Min , X . Lyu , A . Holtzman , M . Artetxe , M . Lewis , H . Hajishirzi , and L . Zettlemoyer , “Rethinking the Role of Demonstrations : What Makes In - Context Learning Work ? ” Feb . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 45 [ 266 ] B . Lester , R . Al - Rfou , and N . Constant , “The Power of Scale for Parameter - Efficient Prompt Tuning , ” Sep . 2021 . [ 267 ] Y . Qin , X . Wang , Y . Su , Y . Lin , N . Ding , J . Yi , W . Chen , Z . Liu , J . Li , L . Hou , P . Li , M . Sun , and J . Zhou , “Exploring Universal Intrinsic Task Subspace via Prompt Tuning , ” Nov . 2022 . [ 268 ] N . Shao , Z . Cai , H . Xu , C . Liao , Y . Zheng , and Z . Yang , “Compositional Task Representations for Large Language Models , ” in International Conference on Learning Representations , Feb . 2023 . [ 269 ] Y . Fu , H . Peng , A . Sabharwal , P . Clark , and T . Khot , “Complexity - Based Prompting for Multi - Step Reasoning , ” Jan . 2023 . [ 270 ] H . Liu , C . Sferrazza , and P . Abbeel , “Chain of Hindsight Aligns Language Models with Feedback , ” Mar . 2023 . [ 271 ] B . Paranjape , J . Michael , M . Ghazvininejad , H . Hajishirzi , and L . Zettlemoyer , “Prompting Contrastive Explanations for Commonsense Reasoning Tasks , ” in Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 . Online : Association for Computational Linguistics , Aug . 2021 , pp . 4179 – 4192 . [ 272 ] X . Ye and G . Durrett , “The Unreliability of Explanations in Few - shot Prompting for Textual Reasoning , ” Oct . 2022 . [ 273 ] S . Wiegreffe , J . Hessel , S . Swayamdipta , M . O . Riedl , and Y . Choi , “Reframing Human - AI Collaboration for Generating Free - Text Explanations , ” ArXiv , 2021 . [ 274 ] Y . Weng , M . Zhu , S . He , K . Liu , and J . Zhao , “Large Language Models are reasoners with Self - Verification , ” Dec . 2022 . [ 275 ] J . Huang , S . S . Gu , L . Hou , Y . Wu , X . Wang , H . Yu , and J . Han , “Large Language Models Can Self - Improve , ” Oct . 2022 . [ 276 ] I . Jackson and M . J . Saenz , “From Natural Language to Simulations : Applying GPT - 3 Codex to Automate Simulation Modeling of Logistics Systems , ” arXiv : 2202 . 12107 [ cs ] , Feb . 2022 . [ 277 ] Q . Lyu , S . Havaldar , A . Stein , L . Zhang , D . Rao , E . Wong , M . Apidianaki , and C . Callison - Burch , “Faithful Chain - of - Thought Reasoning , ” Feb . 2023 . [ 278 ] W . Chen , X . Ma , X . Wang , and W . W . Cohen , “Program of Thoughts Prompting : Disentangling Computation from Reasoning for Numerical Reasoning Tasks , ” Nov . 2022 . [ 279 ] A . Madaan , S . Zhou , U . Alon , Y . Yang , and G . Neubig , “Language Models of Code are Few - Shot Commonsense Learners , ” Dec . 2022 . [ 280 ] P . Wang , A . Chan , F . Ilievski , M . Chen , and X . Ren , “PINTO : Faithful Language Reasoning Using Prompt - Generated Rationales , ” Nov . 2022 . [ 281 ] J . Liu , S . Hallinan , X . Lu , P . He , S . Welleck , H . Hajishirzi , and Y . Choi , “Rainier : Reinforced Knowledge Introspector for Commonsense Question Answering , ” Oct . 2022 . [ 282 ] J . Liu , A . Liu , X . Lu , S . Welleck , P . West , R . L . Bras , Y . Choi , and H . Hajishirzi , “Generated Knowledge Prompting for Commonsense Reasoning , ” Mar . 2022 . [ 283 ] Z . Yang , J . Qin , J . Chen , L . Lin , and X . Liang , “LogicSolver : Towards Interpretable Math Word Problem Solving with Logical Prompt - enhanced Learning , ” Oct . 2022 . [ 284 ] H . Su , J . Kasai , C . H . Wu , W . Shi , T . Wang , J . Xin , R . Zhang , M . Ostendorf , L . Zettlemoyer , N . A . Smith , and T . Yu , “Selective Annotation Makes Language Models Better Few - Shot Learners , ” Sep . 2022 . [ 285 ] H . Zhou , A . Nova , H . Larochelle , A . Courville , B . Neyshabur , and H . Sedghi , “Teaching Algorithmic Reasoning via In - context Learning , ” Nov . 2022 . [ 286 ] A . Lewkowycz , A . Andreassen , D . Dohan , E . Dyer , H . Michalewski , V . Ramasesh , A . Slone , C . Anil , I . Schlag , T . Gutman - Solo , Y . Wu , B . Neyshabur , G . Gur - Ari , and V . Misra , “Solving Quantitative Reasoning Problems with Language Models , ” Jun . 2022 . [ 287 ] Y . Lee , J . J . Y . Chung , T . S . Kim , J . Y . Song , and J . Kim , “Promptiverse : Scalable Generation of Scaffolding Prompts Through Human - AI Hybrid Knowledge Graph Annotation , ” in Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems , ser . CHI ’22 . New York , NY , USA : Association for Computing Machinery , Apr . 2022 , pp . 1 – 18 . [ 288 ] S . Rhys Cox , Y . Wang , A . Abdul , C . von der Weth , and B . Y . Lim , “Directed Diversity : Leveraging Language Embedding Distances for Collective Creativity in Crowd Ideation , ” in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems , ser . CHI ’21 . New York , NY , USA : Association for Computing Machinery , May 2021 , pp . 1 – 35 . [ 289 ] T . Schick , J . Dwivedi - Yu , Z . Jiang , F . Petroni , P . Lewis , G . Izacard , Q . You , C . Nalmpantis , E . Grave , and S . Riedel , “PEER : A Collaborative Language Model , ” Aug . 2022 . [ 290 ] R . Gozalo - Brizuela and E . C . Garrido - Merchan , “ChatGPT is not all you need . A State of the Art Review of large Generative AI models , ” Jan . 2023 . [ 291 ] T . Khot , H . Trivedi , M . Finlayson , Y . Fu , K . Richardson , P . Clark , and A . Sabharwal , “Decomposed Prompting : A Modular Approach for Solving Complex Tasks , ” Oct . 2022 . [ 292 ] N . Zhang , L . Li , X . Chen , X . Liang , S . Deng , and H . Chen , “Multimodal Analogical Reasoning over Knowledge Graphs , ” Jan . 2023 . [ 293 ] P . Lu , S . Mishra , T . Xia , L . Qiu , K . - W . Chang , S . - C . Zhu , O . Tafjord , P . Clark , and A . Kalyan , “Learn to Explain : Multimodal Reasoning via Thought Chains for Science Question Answering , ” Oct . 2022 . [ 294 ] R . S . Sutton and A . G . Barto , Reinforcement Learning , Second Edition : An Introduction . MIT Press , Nov . 2018 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . 46 Xavier Daull , Patrice Bellot , Emmanuel Bruno , Vincent Martin , and Elisabeth Murisasco [ 295 ] Z . J . Wang , D . Choi , S . Xu , and D . Yang , “Putting Humans in the Natural Language Processing Loop : A Survey , ” arXiv : 2103 . 04044 [ cs ] , Mar . 2021 . [ 296 ] J . Leike , D . Krueger , T . Everitt , M . Martic , V . Maini , and S . Legg , “Scalable agent alignment via reward modeling : A research direction , ” Nov . 2018 . [ 297 ] A . Askell , Y . Bai , A . Chen , D . Drain , D . Ganguli , T . Henighan , A . Jones , N . Joseph , B . Mann , N . DasSarma , N . Elhage , Z . Hatfield - Dodds , D . Hernandez , J . Kernion , K . Ndousse , C . Olsson , D . Amodei , T . Brown , J . Clark , S . McCandlish , C . Olah , and J . Kaplan , “A General Language Assistant as a Laboratory for Alignment , ” Dec . 2021 . [ 298 ] E . Perez , S . Ringer , K . Lukoši¯ut˙e , K . Nguyen , E . Chen , S . Heiner , C . Pettit , C . Olsson , S . Kundu , S . Kadavath , A . Jones , A . Chen , B . Mann , B . Israel , B . Seethor , C . McKinnon , C . Olah , D . Yan , D . Amodei , D . Amodei , D . Drain , D . Li , E . Tran - Johnson , G . Khundadze , J . Kernion , J . Landis , J . Kerr , J . Mueller , J . Hyun , J . Landau , K . Ndousse , L . Goldberg , L . Lovitt , M . Lucas , M . Sellitto , M . Zhang , N . Kingsland , N . Elhage , N . Joseph , N . Mercado , N . DasSarma , O . Rausch , R . Larson , S . McCandlish , S . Johnston , S . Kravec , S . E . Showk , T . Lanham , T . Telleen - Lawton , T . Brown , T . Henighan , T . Hume , Y . Bai , Z . Hatfield - Dodds , J . Clark , S . R . Bowman , A . Askell , R . Grosse , D . Hernandez , D . Ganguli , E . Hubinger , N . Schiefer , and J . Kaplan , “Discovering Language Model Behaviors with Model - Written Evaluations , ” Dec . 2022 . [ 299 ] V . Pyatkin , J . D . Hwang , V . Srikumar , X . Lu , L . Jiang , Y . Choi , and C . Bhagavatula , “Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations , ” Dec . 2022 . [ 300 ] X . Hu , Z . Wen , Y . Wang , X . Li , and G . de Melo , “Interactive Question Clarification in Dialogue via Reinforcement Learning , ” Dec . 2020 . [ 301 ] P . R . Philipp , “Decision - Making with Multi - Step Expert Advice on the Web , ” 2019 . [ 302 ] M . Deng , J . Wang , C . - P . Hsieh , Y . Wang , H . Guo , T . Shu , M . Song , E . P . Xing , and Z . Hu , “RLPrompt : Optimizing Discrete Text Prompts with Reinforcement Learning , ” Oct . 2022 . [ 303 ] E . Brooks , L . Walls , R . L . Lewis , and S . Singh , “In - Context Policy Iteration , ” Oct . 2022 . [ 304 ] K . Lee , L . Smith , A . Dragan , and P . Abbeel , “B - Pref : Benchmarking Preference - Based Reinforcement Learning , ” arXiv : 2111 . 03026 [ cs ] , Nov . 2021 . [ 305 ] D . Ribeiro , S . Wang , X . Ma , R . Dong , X . Wei , H . Zhu , X . Chen , Z . Huang , P . Xu , A . Arnold , and D . Roth , “Entailment Tree Explanations via Iterative Retrieval - Generation Reasoner , ” Jul . 2022 . [ 306 ] O . Tafjord , B . D . Mishra , and P . Clark , “Entailer : Answering Questions with Faithful and Truthful Chains of Reasoning , ” Oct . 2022 . [ 307 ] G . Mialon , R . Dessì , M . Lomeli , C . Nalmpantis , R . Pasunuru , R . Raileanu , B . Rozière , T . Schick , J . Dwivedi - Yu , A . Celikyilmaz , E . Grave , Y . LeCun , and T . Scialom , “Augmented Language Models : A Survey , ” Feb . 2023 . [ 308 ] “Why Meta’s latest large language model survived only three days online , ” https : / / www . technologyreview . com / 2022 / 11 / 18 / 1063487 / meta - large - language - model - ai - only - survived - three - days - gpt - 3 - science / . [ 309 ] J . Rudolph , S . Tan , and S . Tan , “ChatGPT : Bullshit spewer or the end of traditional assessments in higher education ? ” Journal of Applied Learning and Teaching , vol . 6 , no . 1 , Jan . 2023 . [ 310 ] P . K . Choubey , J . Vig , W . Liu , and N . Rajani , “MoFE : Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization , ” Oct . 2021 . [ 311 ] W . Sun , Z . Shi , S . Gao , P . Ren , M . de Rijke , and Z . Ren , “Contrastive Learning Reduces Hallucination in Conversations , ” Dec . 2022 . [ 312 ] O . Sharir , B . Peleg , and Y . Shoham , “The Cost of Training NLP Models : A Concise Overview , ” Apr . 2020 . [ 313 ] M . de Jong , Y . Zemlyanskiy , N . FitzGerald , J . Ainslie , S . Sanghai , F . Sha , and W . Cohen , “Pre - computed memory or on - the - fly encoding ? A hybrid approach to retrieval augmentation makes the most of your compute , ” Jan . 2023 . [ 314 ] Y . Hao , Y . Sun , L . Dong , Z . Han , Y . Gu , and F . Wei , “Structured Prompting : Scaling In - Context Learning to 1 , 000 Examples , ” Dec . 2022 . [ 315 ] H . Cho , H . J . Kim , J . Kim , S . - W . Lee , S . - g . Lee , K . M . Yoo , and T . Kim , “Prompt - Augmented Linear Probing : Scaling Beyond The Limit of Few - shot In - Context Learners , ” Dec . 2022 . [ 316 ] P . H . Martins , Z . Marinho , and A . F . T . Martins , “ $ \ infty $ - former : Infinite Memory Transformer , ” Mar . 2022 . [ 317 ] W . Fedus , B . Zoph , and N . Shazeer , “Switch Transformers : Scaling to Trillion Parameter Models with Simple and Efficient Sparsity , ” Jun . 2022 . [ 318 ] X . Nie , P . Zhao , X . Miao , T . Zhao , and B . Cui , “HetuMoE : An Efficient Trillion - scale Mixture - of - Expert Distributed Training System , ” Nov . 2022 . [ 319 ] X . Nie , X . Miao , S . Cao , L . Ma , Q . Liu , J . Xue , Y . Miao , Y . Liu , Z . Yang , and B . Cui , “EvoMoE : An Evolutional Mixture - of - Experts Training Framework via Dense - To - Sparse Gate , ” Oct . 2022 . [ 320 ] X . Nie and X . Miao , “FlexMoE : Scaling Large - scale Sparse Pre - trained Model Training via Dynamic Device Placement , ” 2023 . [ 321 ] C . Blakeney , J . Z . Forde , J . Frankle , Z . Zong , and M . L . Leavitt , “Reduce , Reuse , Recycle : Improving Training Efficiency with Distillation , ” Nov . 2022 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 . Complex QA & language models hybrid architectures , Survey 47 [ 322 ] M . Zaheer , A . S . Rawat , S . Kim , C . You , H . Jain , A . Veit , R . Fergus , and S . Kumar , “Teacher Guided Training : An Efficient Framework for Knowledge Transfer , ” Aug . 2022 . [ 323 ] J . P . Wahle , “A Cohesive Distillation Architecture for Neural Language Models , ” Jan . 2023 . [ 324 ] C . Xu and J . McAuley , “A Survey on Dynamic Neural Networks for Natural Language Processing , ” Feb . 2022 . [ 325 ] J . Hoffmann , S . Borgeaud , A . Mensch , E . Buchatskaya , T . Cai , E . Rutherford , D . d . L . Casas , L . A . Hendricks , J . Welbl , A . Clark , T . Hennigan , E . Noland , K . Millican , G . van den Driessche , B . Damoc , A . Guy , S . Osindero , K . Simonyan , E . Elsen , J . W . Rae , O . Vinyals , and L . Sifre , “Training Compute - Optimal Large Language Models , ” arXiv : 2203 . 15556 [ cs ] , Mar . 2022 . [ 326 ] S . Hong , S . Moon , J . Kim , S . Lee , M . Kim , D . Lee , and J . - Y . Kim , “DFX : A Low - latency Multi - FPGA Appliance for Accelerating Transformer - based Text Generation , ” in 2022 IEEE Hot Chips 34 Symposium ( HCS ) , Aug . 2022 , pp . 1 – 17 . [ 327 ] S . Diao , P . Wang , Y . Lin , and T . Zhang , “Active Prompting with Chain - of - Thought for Large Language Models , ” Feb . 2023 . [ 328 ] Z . Zhang , A . Zhang , M . Li , and A . Smola , “Automatic Chain of Thought Prompting in Large Language Models , ” Oct . 2022 . [ 329 ] Y . Jernite , H . Nguyen , S . Biderman , A . Rogers , M . Masoud , V . Danchev , S . Tan , A . S . Luccioni , N . Subramani , I . Johnson , G . Dupont , J . Dodge , K . Lo , Z . Talat , D . Radev , A . Gokaslan , S . Nikpoor , P . Henderson , R . Bommasani , and M . Mitchell , “Data Governance in the Age of Large - Scale Data - Driven Language Technology , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency . Seoul Republic of Korea : ACM , Jun . 2022 , pp . 2206 – 2222 . [ 330 ] T . Chen , H . Bao , S . Huang , L . Dong , B . Jiao , D . Jiang , H . Zhou , J . Li , andF . Wei , “THE - X : Privacy - PreservingTransformer Inference with Homomorphic Encryption , ” Jun . 2022 . [ 331 ] M . Hao , H . Li , H . Chen , P . Xing , G . Xu , and T . Zhang , “Iron : Private Inference on Transformers , ” 2022 . [ 332 ] Y . Huang , Z . Song , D . Chen , K . Li , and S . Arora , “TextHide : Tackling Data Privacy in Language Understanding Tasks , ” Oct . 2020 . [ 333 ] D . Kim , G . Lee , and S . Oh , “Toward Privacy - preserving Text Embedding Similarity with Homomorphic Encryption , ” in FINNLP , 2022 . [ 334 ] F . Mireshghallah , K . Goyal , A . Uniyal , T . Berg - Kirkpatrick , and R . Shokri , “Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks , ” Mar . 2022 . [ 335 ] C . Qu , W . Kong , L . Yang , M . Zhang , M . Bendersky , and M . Najork , “Natural Language Understanding with Privacy - Preserving BERT , ” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management , Oct . 2021 , pp . 1488 – 1497 . [ 336 ] C . Qu , W . Kong , L . Yang , M . Zhang , M . Bendersky , and M . - A . Najork , “Privacy - Adaptive BERT for Natural Language Understanding , ” ArXiv , 2021 . [ 337 ] X . Zhou , J . Lu , T . Gui , R . Ma , Z . Fei , Y . Wang , Y . Ding , Y . Cheung , Q . Zhang , and X . Huang , “TextFusion : Privacy - Preserving Pre - trained Model Inference via Token Fusion , ” in Conference on Empirical Methods in Natural Language Processing , 2022 . [ 338 ] R . Xu , N . Baracaldo , and J . Joshi , “Privacy - Preserving Machine Learning : Methods , Challenges and Directions , ” Sep . 2021 . [ 339 ] J . Chen , R . Zhang , J . Guo , Y . Fan , and X . Cheng , “FedMatch : Federated Learning Over Heterogeneous Question Answering Data , ” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management , Oct . 2021 , pp . 181 – 190 . [ 340 ] G . Xiao , J . Lin , and S . Han , “Offsite - Tuning : Transfer Learning without Full Model , ” Feb . 2023 . , Vol . 1 , No . 1 , Article . Publication date : April 2023 .