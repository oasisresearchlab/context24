Beyond First Encounters with Analytics : Questions , Techniques and Challenges in Instructors’ Sensemaking Qiujie Li NYU Learn , New York University , New York , NY , USA ql16 @ nyu . edu Yeonji Jung NYU Learn , New York University , New York , NY , USA yeonji . jung @ nyu . edu Alyssa Friend Wise NYU Learn , New York University , New York , NY , USA alyssa . wise @ nyu . edu ABSTRACT Despite growing implementation of teacher - facing analytics in higher education , relatively little is known about the detailed pro - cesses through which instructors make sense of analytics in their teaching practices beyond their initial encounters with tools . This study unpacked the sensemaking process of thirteen instructors with analytic experience , using interviews that included walk - throughs of their analytics use . Qualitative inductive analysis was used to identify themes related to ( 1 ) the questions they asked of the analytics , ( 2 ) the techniques they used to interpret them , and ( 3 ) the challenges they encountered . Findings indicated that instructors went beyond a general curiosity to develop three types of questions of the analytics ( goal - oriented , problem - oriented , and instruction modification questions ) . Instructors also used specific techniques to read and explain data by ( a ) developing expectations about the answers the analytics would provide , and ( b ) making comparisons to reveal student diversity , identify effects of instructional revision and diagnose issues . The study found instructors faced an initial learning curve when seeking and making use of relevant infor - mation , but also continued to revisit these challenges when they were not able to develop a routine of analytics use . These findings both contribute to a conceptual understanding of instructor ana - lytic sensemaking and have practical implications for its systematic support . CCS CONCEPTS • Applied computing ; • Education ; • Human - centered com - puting ; • Human computer interaction ( HCI ) ; • Empirical studies in HCI ; KEYWORDS Human - centered analytics , Analytic sensemaking , Data - informed instruction , Instructional dashboards ACM Reference Format : Qiujie Li , Yeonji Jung , and Alyssa Friend Wise . 2021 . Beyond First Encoun - ters with Analytics : Questions , Techniques and Challenges in Instructors’ Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspecificpermission and / or a fee . Request permissions from permissions @ acm . org . LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA © 2021 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 8935 - 8 / 21 / 04 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3448139 . 3448172 Sensemaking . In LAK21 : 11th International Learning Analytics and Knowl - edge Conference ( LAK21 ) , April 12 – 16 , 2021 , Irvine , CA , USA . ACM , New York , NY , USA , 10 pages . https : / / doi . org / 10 . 1145 / 3448139 . 3448172 1 INTRODUCTION Higher education institutions are moving to design and implement teacher - facing analytics with the hope that instructors can extract deep insights about student learning and make informed decisions to improve their teaching . However , there are increasing concerns about the uptake and effectiveness of these tools . Prior research has documented varying levels of tool use as well as the difficulties of instructors in taking action based on the information provided [ 4 , 8 , 9 ] . Recent moves in the field towards human - centered learning analytics emphasize analytics as part of a socio - technical system where human factors play as critical a role as technical ones [ 5 , 27 ] . This has led to a stream of work focusing on how instructors use analytics in authentic contexts ( e . g . [ 3 , 13 , 28 ] ) . Particularly , the question of how instructors make sense of ana - lytics is receiving increasing attention along with a growing recog - nition that data do not speak for themselves and that it is non - trivial for instructors to extract valid , relevant , and actionable insights from them [ 18 ] . Initial work has examined the process of sense - making by identifying its different components ( e . g . ask questions of , read and explain data ; [ 30 , 31 ] ) as well as challenges associated with the process ( e . g . [ 3 , 4 , 13 ] ) . However , detail is lacking about the specific ways instructors engage with analytics within these components , particularly the kinds of questions they ask and the techniques they use to read and explain data . Additionally , while this work has taken place in authentic environments , much of it has been based on the relatively novice sensemaking experiences of instructors using analytic tools for the first time . As instructors gain practice using analytics in their teaching practices , they may overcome some initial sensemaking challenges and adopt different approaches to obtain insights from the data . The current study addresses these gaps by going beyond the experience of first encounters to unpack the detailed sensemaking process of thirteen instructors of whom all had prior experience with analytics ( either the current dashboard or other data visual - izations ) . Specifically , interviews with instructors , including walk - throughs of how they used a teacher - facing dashboard to inform their teaching , were conducted to probe the questions they asked of analytics , the techniques they used to read and explain them , and the challenges they faced . The findings contribute to a concep - tual model of instructor analytic sensemaking , inform the future design of teacher - facing analytics , and have implications for how to support both novice and experienced users of analytics . 344 LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Qiujie Li et al . 2 LITERATURE REVIEW Sensemaking is a conscious process of comprehending patterns and connections , figuring out meaning and anticipating the future [ 14 ] that has been the subject of research in human - computer interac - tion and information sciences for many years [ 6 , 20 , 23 ] . Specifically , informational sensemaking is defined as a process of exploring in - formation to answer task - specific questions [ 23 ] and translating interpretation into actionable insights [ 26 ] . In education , instructors have long engaged in sensemaking of information about students , mainly collected from assignments and informal observation . In doing so , they take on the tasks of “finding relevant information , synthesizing [ it ] across sources , identifying key ideas , and integrat - ing new knowledge into prior understanding and emerging work products” [ 6 ] . However , there are important differences when these kinds of activities are applied to the new , voluminous , and often complex sources of data about students provided by analytic tools [ 31 ] . 2 . 1 Instructors’ analytic sensemaking Instructor analytic sensemaking has been described as having three main components : asking questions , reading the data , and explain - ing the meaning of the data [ 31 ] . As a starting point , questions play a critical role in guiding instructors in what data to look for and the kind of meaning to draw from it [ 30 ] . Despite the powerful frame that questions can offer , several studies have found that instruc - tors do not always come to analytics use with specified questions [ 18 , 25 , 31 ] . For instance , Wise & Jung [ 31 ] , who conducted inter - views with five first - time users of an analytic dashboard , found that instructors came to analytics with a general curiosity and only started to specify questions over time as they interacted with analytics to learn what data was available and notice patterns in them . With or without specific questions , instructors can be seen to engage in a process of “reading” data as they orient to what is available , find relevant information to focus on , and often make comparisons either within the data ( e . g . relative value of students’ scores ) or to an outside standard ( e . g . using a minimum acceptable score of 80 ) ( e . g . [ 28 , 29 ] ) . For instance , Van Leeuwen [ 28 ] found that instructors who had no prior experience with analytics took the course average as a central reference point to differentiate low - and high - performing students . After reading the data , instructors typically decide on the implications of any patterns identified by ( a ) triangulating with other sources of information [ 1 , 21 ] , ( b ) con - textualizing the patterns in the local learning situation [ 13 , 18 ] , ( c ) and making attributions for why the data might appear a certain way [ 13 , 31 ] . Making attributions is of particular importance in the sensemaking process as it directly informs what actions will be taken in response . The components described above highlight two important char - acteristics of sensemaking . First , as an activity , sensemaking is situated in a particular teaching environment , requiring instructors to focus on contextually relevant information among the copious de - tailed data available [ 1 , 2 , 9 ] . To do this , instructors need to activate their existing pedagogical knowledge [ 9 , 12 , 18 ] . Second , sense - making is a complex and evolving skill that instructors develop through exposure to and practice with analytics use [ 1 , 13 ] . Even studies of instructors with limited experience with analytic tools [ 4 , 13 , 31 ] , have shown them gradually become more comfortable to explore the data , form new or more specific questions , and develop techniques to better read and explain the analytics . 2 . 2 Challenge in analytic sensemaking In developing experience with analytics , prior work has revealed two types of challenges in instructor sensemaking . First , instructors face a ( steep ) initial learning curve when starting to use analytic tools [ 3 , 4 , 22 ] . Arthars and Liu [ 3 ] interviewed 34 users of an an - alytics platform and found that , in the early stages , participants uniformly went through a frustrating process to learn to use the platform . Other studies found that the frustration of first - time use extended from incorrect understanding of the functionalities of the tools [ 13 ] to lack of clarity on what data was collected and how it was assembled to produce high - level representations of learning activities [ 4 ] . The questions and the frustration associated with the initial learning curve may limit the insights instructors draw from the data and / or lead to distrust and hesitancy to take action ; all of which can impede effective uptake [ 4 , 7 , 8 , 17 , 22 ] . In some cases , initial challenges have been seen to persist as instructors continue to use a tool [ 13 ] while in others , instructors have been able to progress through the initial learning curve to eventually benefit from a tool [ 3 ] . Even when the analytics are understood at a basic level , instructors commonly struggle with how to transform the in - formation into relevant , actionable insights [ 9 , 22 , 28 ] . For instance , Van Leeuwen [ 28 ] found instructors who incorporated a weekly analytics report into their flipped classrooms for the first time were not able to translate trace data of student behaviors ( e . g . the number of times students watched lectures ) into useful inferences about stu - dents’ progress . In addition , success in using analytics to effectively diagnose a classroom issue does not always imply a clear course of action to remedy it ( e . g . [ 13 , 28 ] ) . Such difficulties in gaining actionable insights may detract from instructor motivation to use analytics , further contributing to the low uptake observed [ 3 , 9 ] . 2 . 3 The current study Prior work has shed initial light on how instructors make sense of analytics in authentic learning environments . However , detailed information about the kinds of questions asked and techniques used to read and explain data is lacking . In addition , the majority of these studies focused on first - time users of analytic tools ( c . f . [ 3 , 13 ] ) , which provides limited insights into novel and more sophis - ticated ways instructors may approach analytics with experience , as well as the different challenges they may face over time . To ad - dress these gaps , this study goes beyond first - time encounters with analytics to probe how instructors with analytic experience make sense of the information provided by a teacher - facing dashboard to inform their practice . The study was conducted at a large univer - sity where a teacher - facing dashboard has been implemented for four years with frequent professional development opportunities and other support . Thirteen instructors who had prior experiences with analytics ( either this dashboard or other data visualizations ) participated in interviews that included walkthroughs of their an - alytics use . Specifically , three research questions were asked : ( 1 ) What kinds of questions ( if any ) did instructors ask of the analytics ? 345 Beyond First Encounters with Analytics : Questions , Techniques and Challenges in Instructors’ Sensemaking LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Figure 1 : Dashboard : Resource interaction and traffic view ( 2 ) What kind of techniques did they use to read and explain the analytics ; and ( 3 ) What kinds of challenges did they encounter in their sensemaking process ? 3 METHOD 3 . 1 Learning analytics context & tool 3 . 1 . 1 The Dashboard and Its Implementation . This study was con - ducted at a large private university in the United States in Spring 2020 . The dashboard that was the focus of this study offered vi - sualizations of data from the university’s learning management system ( LMS ) about student activity and performance in a course . At the time of this study , the dashboard had been in existence for four years . The kinds of information included and the way it was visualized in the dashboard resulted from multiple rounds of design , implementation , and revision led by the institution’s Teaching and Learning Technology unit . A total of six data views were available depending on the course tools used . First , two views presented data of when and how much students accessed course materials , with one focusing on students and the other on resource items ( see Figure 1 ) . The text analysis view displayed how frequently students mentioned key terms in the course discussion forum or assignments . The video analysis view presented data on student interaction with video materials including how often and how long students engaged with each of the videos . Finally , student assessment data was displayed in two views , one organized by question and one by student . Each view provided interactive functions that allowed instructors to search for and filter the information by date , course material , and / or individual student ( see Figure 1 ) . All data views were accessible from a main page that presented information about how to use each view , including potential questions to ask and possible actions to take in response . Dashboard data was refreshed once a day in the evening during the semester . Instructors could access the dashboard directly through their course site in the LMS . Instructors who wished to access the dashboard signed up for a one - on - one orientation session where an analytics coach walked them through tool use . Ongoing support was available in the form of one - on - one training sessions or working group activities with other faculty members to discuss a particular topic related to analytics use . Finally , instructors could directly ask individual questions about their dashboard use to the coach as needed . 3 . 1 . 2 Other Data Sources Available . In addition to the dashboard , there were several other systems that offered additional sources of data to instructors . First , the LMS itself provided counts of how often students accessed various elements within it ( tools , resources , etc . ) . Second , the front end of various learning tools also provided some level of information about the activity that took place within . For example , the discussion forum tool listed the number of posts and post views for individual students . Instructors who uploaded their video lectures to the hosted media platform also had access to information on video views presented directly in the tool . Finally , some instructors collected survey data on students’ perceptions about the course . 3 . 2 Participants and courses Thirteen of the 32 instructors using the dashboard in Spring 2020 agreed to participate . Participants generally had extensive teach - ing experiences ( > 10 years ) except for one who was in their first semester of teaching . All of the instructors reported a high level of comfort working with educational technologies , especially the LMS . Nine instructors had used the dashboard for at least one semester prior to the current one . The other four instructors were new to this specific dashboard , but all had experience working with quan - titative data and visualizations . The courses themselves showed differences in discipline , course size , and course format and were a mix of face - to - face , blended and online offerings . In mid - March 2020 , all face - to - face course components were transitioned into remote mode . 3 . 3 Data collection Two interviews were conducted with each instructor at the mid - point and end of the semester . The first interview examined how instructors made sense of analytics provided by the dashboard in their current course as well as their background and overall motivation in using the analytics , while the second interview ad - ditionally probed how they learned to use the dashboard and any of institutional support in this process . The current analysis fo - cused primarily on the data from the first interview to understand instructor analytic sensemaking . The interviews took a semi - structured format where researchers asked planned questions based on a protocol as well as responsive follow - up questions as needed . The first interview started with a 346 LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Qiujie Li et al . set of questions about instructors’ background ( e . g . teaching back - ground and prior experience in analytics use ) and their frequency of dashboard use in their current courses . Questions about factors that limited their use of the dashboard were then asked to probe the overall challenges they faced . Then , the instructors were asked to walk through their recent uses of the dashboard for each of the six views following the prompting questions : ( a ) what questions they wanted to answer through dashboard use , ( b ) how they made sense of the analytics presented in the dashboard , and ( c ) how they reacted to the analytics based on interpretation . A final set of questions asked about instructors’ thoughts about their overall use experience and plans for future use . Individual interviews lasted an hour and were conducted face - to - face or using the Zoom video conferencing tool ; responses were audio - recorded and transcribed . 3 . 4 Data analysis Analysis of the interviews was conducted following the interpretive qualitative tradition [ 11 ] . In the context of learning analytics as a multidisciplinary field , it is important to highlight the relevant standards for rigor and how they were met in this study , as they differ from the standards applicable in quantitative analyses [ 15 ] . Interpretive approaches look for themes of meaning in the data , based not only on the number of times an idea is mentioned , but also its importance and consequence to participants . Treating dif - ferences in perspective as an inherent and potential useful aspect of analysis , rather than a source of error , the traditional criteria of inter - rater reliability is eschewed in favor of those of credibility and dependability [ 11 , 19 ] . Credibility is concerned with the extent to which findings offer a valid representation of reality [ 11 , 16 ] . Following Lincoln and Guba [ 16 ] , this study established credibility via three methods : ( 1 ) triangulation ( ensuring data from multiple sources ( participants , times , areas of focus ) support each finding ) ; ( 2 ) structural corroboration ( checking findings for disconfirming evidence and possible alternative explanations ) ; and ( 3 ) member - checks ( vetting and refining initial findings with the participants during the second interview ) . Dependability is concerned with the consistency and repeatability of findings [ 16 ] . This study estab - lished dependability by using a documented processes of analysis ( described below ) , keeping an audit trail of all codes and decisions at each stage of the analysis process , and having an additional re - searcher not involved in the analysis process inspect alignment of the process , the data , and the interpretations produced . The analysis process used the constant comparative method [ 10 ] applied in three phases . In the first phase , possible ideas relevant to the research questions were identified in the transcripts through line - by - line text analysis ( e . g . question : instructors wanted to see whether all students have accessed the required materials ) . In the second phase , relevant ideas were formed into possible sub - themes ( e . g . curiosity / questions about student engagement ) by merging similar ideas and discarding less substantiated ones . Sub - themes were checked for multiple sources of supporting data , the presence of any disconfirming evidence , possible alternative explanations , and overall importance across the transcripts . Finally , in the third phase , surviving sub - themes were further integrated into high - level themes ( e . g . goal - oriented question ) to address the research questions . 4 RESULTS The original interview protocol was designed to focus on instruc - tors’ use of the specific analytic dashboard described above ; how - ever , interviews quickly revealed that instructors did not distinguish their use of this data source from others they also drew on to inform their teaching . Questions about “dashboard use” were responded to with answers that included information from LMS - provided access counts , the front end of various learning tools , and direct examina - tion of student products . In addition , instructors often incorporated information gathered through informal communication with stu - dents , observation of student behaviors , and formal solicitation of student input . Thus the original framing was expanded to consider “analytics” as the full data ecosystem that instructors used to inform their teaching . 4 . 1 RQ1 . What kinds of questions do instructors ask of the analytics ? All instructors asked specific questions of the analytics . These aligned with one of three categories : questions related to if partic - ular goals instructors had for a course were being met , questions directed at better understanding a problem they noticed in their course , and questions focused on helping them modify some aspects of their instruction . Each type of question prompted a particular way of interacting with the analytics , orienting instructors to and providing a structure for their interpretation of relevant data . 4 . 1 . 1 Goal - Oriented Questions . The first type of question asked about the extent to which instructors’ pedagogical goals were be - ing attained . This was the most common type of question , asked by all instructors but one ( InT2 ) , multiple times . Goal - oriented questions were framed as stemming from a desire to ensure that students were engaging with coursework in particular ways and / or that intended learning outcomes were achieved . For instance , one instructor shared “ I try to get a sense of when I make an announce - ment that there’s a reading available , some helpful resource to refer - ence , how many students actually use it ” ( InT4 ) . Another instructor wanted to know “ whether [ students ] are making connections between [ ideas about ] ‘work’ and ‘technologies’ that we are discussing in class” ( InT7 ) . Instructors developed goal - oriented questions by bringing to - gether their general knowledge about effective learning ( e . g . stu - dents learn better when they distribute the time they engage with the content instead of cramming it all in at once ) and the specifics of the local educational context ( e . g . , the design of the course , specific learning activities and / or characteristics of the students ) . Specif - ically , knowledge about how students learn effectively allowed instructors to define a broad goal , and knowledge about the local educational context helped them to articulate the goal within the context of specific learning activities . For instance , based on the belief that “ It’s important to give yourself time and speed work is going to be inaccurate ” , one instructor ( InT1 ) asked a goal - oriented question about a specific assignment based on the design of the assignment : “ It [ the assignment ] opens on Wednesday morning , when do they see [ it ] between Wednesday morning and Sunday ? Do they give themselves enough time to think about the prompt or do they 347 Beyond First Encounters with Analytics : Questions , Techniques and Challenges in Instructors’ Sensemaking LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA just write something very quickly to show that they have a submis - sion ? ” Knowledge about student characteristics also played into instructors’ framing of the goal and its importance . For instance , the above - mentioned instructor ( InT1 ) was particularly focused on this goal due to the student population they were working with “ have been out of school for like ten years and also the procrastination they have . ” Because goal - oriented questions were defined with respect to whether some objective was or was not being met , their formation prompted a particular form of analytics use : the comparison of data on students’ actual behaviors with instructors’ vision of optimal activity to evaluate the extent to which the goals were achieved ( resulting in a call to action if they were not ) . For instance , guided by the goal for students to make connections between the key concepts of technology and work , one instructor examined text analytics of the discussion forum posts . They found that very few students referred to the two concepts together in the same post , motivating them to highlight the connection to the class : “ What I want to do in my live session is talk to the students about it [ the connection between technology and work that was not being made ] . I want to bring it up explicitly ” ( InT7 ) . 4 . 1 . 2 Problem - Oriented Questions . The second type of question asked about the scope , underlying reasons , and potential conse - quences for a problem that instructors observed related to student learning . Problems that drove these questions , which were asked for both the group and for individuals , tended to focus on low levels of interaction with materials or activities ( e . g . , “Why are so few students contributing on our forums ” from InT13 ) and / or low performance ( e . g . , “ I have a student who just did terribly on an exam . I want to know , has that student been coming to class ” from InT3 ) . Problem - oriented questions were less common than goal - oriented ones , but still frequently asked , and raised by seven instructors multiple times . Unlike goal - oriented questions , which were proactively devel - oped based on instructors’ existing knowledge , problem - oriented questions were reactive to signs of troubles that instructors noticed as the course progressed . Instructors identified problems and de - veloped questions through either direct interaction with students or the use of analytics : “I know some students need more help than others . They come for tutoring . So I have been keeping an eye on them to see if they’re on track” ( InT1 ) . Problem - oriented questions then led instructors to focus on particular students , student subgroups , or course activities for which the potential problem was observed . Specifically , instructors investigated problems by evaluating stu - dent engagement / performance with respect to some reference point . Three instructors used the relative reference point of “other” stu - dents ( who were not judged to be having problems ) . For example , “ let me find someone who’s problematic . This one over here . This per - son missed these things . . . . . . I don’t know what’s going on with that student , but I can compare how that student did on this homework versus how the other students did” ( InT3 ) . Another instructor sug - gested comparing the behavior of low - performing students and high - performing students to “ encourage [ low - performing ] students to exhibit similar behaviors to see if that might increase their engage - ment with the class” ( InT4 ) . When engaging with problem - oriented questions , one instructor generated expectations about reasons for the problems that could be tested with analytics ( see section 4 . 2 . 1 ) . 4 . 1 . 3 Instructional Modification Questions . The final type of ques - tions asked about how to modify specific instructional practices , either proactively based on student needs , or reactively based on student reactions . This type of question was mentioned much less frequently , with six instructors describing a single instructional modification question each . Three instructors asked questions proactively about how to tailor an instructional approach to the particular needs of their current students . For example , one instructor used analytics to decide when to send announcements so that they would be well - received : “ one thing I struggle with is knowing when my students are working on this for the most part and so when I can send them an email that’s helpful to them , instead of having it buried somewhere ” ( InT5 ) . Another instructor asked a question about what content to focus on during the limited class meeting time : “ Because I only have so much time in class , I’ll look at [ the data and ] say , ‘Okay , I need to focus on problem five’ ” ( InT3 ) . The final instructor used analytics to decide what kind of support and resources to provide their students : “ I can go and check if they’re watching [ the video ] . If they don’t care and they’re too busy . . . I can put my energy into feedback on their individualized work instead or make shorter videos” ( InT7 ) . Three other instructors asked questions reactively about what materials should be revised based on student use and perceptions . These questions differed from those described above in that they were targeted at course redesign for a future offering and they were asked at the end of the term since instructors wanted to make such systematic changes based on complete data from the whole semester . For instance , “ I want to see how the reading patterns are varying from one resource to the next . If there’s a certain piece that has gotten a great deal or much less attention , it could inform what I include next time . . . I usually sort of do a debrief at the end of the semester , rather than while semesters are in progress . During the semester , I can’t really make very major changes because I don’t know that reading has been more or less successful until after we have already done it” ( InT8 ) . Instructional modification questions of either type promoted instructors to look for dominant trends to understand the needs of the majority of the students . For instance , as described above one instructor ( InT5 ) used the analytics to identify the preferred time of working of most of the students to tailor when to send announcements . After identifying that “ most of the interaction with lecture slides , PDFs or other things seems to be between Saturdays and Sundays ” , the instructor noted “ Because they’re really only going to get to it on Saturday and Sunday , so having a well - considered email is a much better idea on Saturday morning than to give them a whole bunch of stuff during the week when it’s going to get buried ” ( InT5 ) . Another instructor used analytics to find out among all the concepts of a week , which concepts were challenging to most of the students . Specifically , the instructor examined “ the percentage correct for the whole group” of each quiz and identified the quiz with lower percentages and examined the “distribution of wrong answers” to determine the concepts to focus on in class ” ( InT3 ) . 348 LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Qiujie Li et al . 4 . 2 RQ2 . What techniques do instructors use to read and explain the analytics ? 4 . 2 . 1 Generate , Test and Modify Expectations for the Data . While more than half of the instructors went from asking questions to answering them directly with data , six instructors engaged in an in - terim step of first specifying their expectations about the answer ( s ) the data would provide . Instructors generated two types of expec - tations : predictions about outcomes ( e . g . observed low engagement will lead to low performance ) and hypotheses about why an out - come occurred ( e . g . observed low engagement is due to difficulty of the material ) . For example , one instructor predicted whether the course objective that students would see the ties between two key themes was achieved as an outcome of course activities , “ I want to know where they’re making connections between work and tech - nologies . If I [ search ] ‘technology’ and ‘work , ’ I may get something because we’re talking about the fourth industrial revolution [ during the lectures ] ” ( InT7 ) . Another instructor made a prediction about the consequences of a perceived problem that was not borne out by the data , “ There are a few students that are not very vocal in my class . . . . . . . . I was a little worried about their assignments [ but it turned out okay ] ” ( InT5 ) . Different from predictions about outcomes , instructors also generated expectations to test hypothesis for why something had occurred . For example , one instructor developed a hypothesis about why some students were not actively engaged in class discussion : “ the students who weren’t participating in the discussions weren’t looking at the materials” ( InT4 ) . These expecta - tions were often developed based on instructors’ knowledge about the local context : “ I didn’t expect them [ students ] to touch [ these materials ] at all , because they’re not really looking for jobs” ( InT5 ) . In contrast to simply answering a question with data , instructors compared the data with their expectations and then accepted or rejected and revised their predictions or hypotheses . This is exem - plified in the case of one instructor ( InT4 ) who went through an iterative process of generating , testing and modifying both what they expected to see in the data and their overall interpretation of the situation . The instructor started to use the analytics motivated by a problem observed that some students “ weren’t answering any questions ” and “ were very quiet during the group based exercises ” and developed a hypothesis that these students “ weren’t looking at the materials . ” When verifying the hypothesis , the instructor examined the actual levels of student access to the course materials and found that “ they actually were looking at it just a little bit closer to class time . It might be that they have a lot of other things going on . ” The in - structor then hypothesized that these students “ were just looking at it to make sure they had at least a baseline understanding of what the content was ” and made a prediction that “ their assignments won’t be thoughtfully submitted . ” However , after reading the assignments , the instructor found that “ their assignments are very thoughtful and it’s very clear that they are engaging with the material . ” Finally , the instructor revised their initial assumption and concluded that “ they’re just doing it in a different way , in a different pace . ” This shift led the instructor to reexamine the course design : “ It might help if I gave them a little bit more of a buffer zone where they had maybe two weeks instead of only five days [ so they have the flexibility to engage with the content on their own schedule ] . ” 4 . 2 . 2 Make Comparisons to Diagnose Issues , Reveal Diversity , and Identify Effects . When reading data , instructors made comparisons across students , content , materials , and time to identify similari - ties and differences . All instructors except two ( InT7 , InT12 ) made comparisons , and three forms of comparisons , differing in their goals and the specific behaviors involved , were observed . The first form of comparison aimed at diagnosing students with lower levels of engagement or performance than others by identifying relative reference points from the data and comparing the values of indi - vidual students with the reference points . In contrast , the second form of comparison examined differences in student behavior be - tween predefined student subgroups ( e . g . full - time versus part - time students ) to understand the diverse behavioral patterns among students . The last form of comparison involved instructors compar - ing data across pre - defined time periods to identify the effects of changes in instructional practices or other contextual factors . Six instructors attempted to diagnose low performing students by setting a reference point within analytics and then identifying students substantially below it . For instance , one instructor exam - ined the differences between the behavior of the top students and all others : “ I see people that are all purple here ; that tells me that , ‘Okay , I assume that these are the ones who are doing a lot of it . ’ Then , I can compare with the other ones . . . This is the baseline , these are the people who are doing well , and here I’m getting who’s really in trouble” ( InT3 ) . In more than half of the cases , instructors went beyond the first data source to examine whether students consistently showed low engagement or performance with one type of material over time or with different types of materials . Through such compar - isons , instructors could identify students with a strong signal of needing help . For example , “ Most of the other students have been a little bit spotty . But at least they’ve been in once or twice . There’s really only the top two students on the list who have had two different weeks where they didn’t interact with the site at all” ( InT8 ) . As opposed to looking across everyone to determine who was not engaging or performing well , two instructors grouped data by student attributions and made comparisons between these pre - defined groups . Results of the comparisons revealed the various behavioral patterns among students , drew instructors’ attention to differing needs , with the potential for them to tailor their in - struction to meet this diversity . For instance one instructor ( InT5 ) compared data to examine how full - time and part - time students engaged in class differently : “ Some of the full - time students are go - ing back and revising classes more often than the part - time students . ” However , the instructor believed that the apparent low levels of access of part - time students was likely due to the part - time stu - dents’ tendency to access the materials directly through the links embedded in the emails , rather than from the LMS : “ What I think is happening is that if other people [ part - time students ] are revisiting , they’re probably using the links in the emails , versus the full - time students who probably use [ the LMS ] , because they have multiple classes . My part - time students only have one class and so this [ the LMS ] isn’t a central resource for them . ” The instructor decided to revise how the resource access links were set up so that the data of part - time students would also be accurately captured : “ I have to link to a few things in my emails every week , but I can reformat the way I do links . It’s a bit annoying to do so , but it can be done” ( InT5 ) . 349 Beyond First Encounters with Analytics : Questions , Techniques and Challenges in Instructors’ Sensemaking LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Finally , four instructors compared data across time focusing on potential changes in student behavior associated with changes in instruction or other contextual factors . Through the comparison , in - structors were able to examine the impact of changes in instruction and learning environment . For example , one instructor compared student behavior before and after the switch to remote teaching : “ The main readings were mostly around 80 % . This is when we made the transition to remote instruction . . . . . . It’s shot up to 100 % and it remains pretty high since then , higher than it was before we were doing remote instruction” ( InT8 ) . The instructor ( InT8 ) attributed the change of student behavior to the differences in instructional practices associated with the switch to online instruction : “ Perhaps because every student had to put a discussion post on the reading for the asynchronous class . . . . . . . It may indicate that when their work for a particular class is strictly to do with a piece of reading and make a comment on it , students are that much more likely to do the reading” ( InT8 ) . 4 . 3 RQ3 . What kinds of challenges did instructors encounter during sense - making of the analytics ? In the process of making sense of analytics , most instructors ( ex - cept InT12 ) encountered at least one of three sets of challenges . The first set was related to navigating the tool to seek relevant information . The second set related to making the analytics useful by interpreting them to develop localized actionable insights . The third and final set related to routinizing data seeking and interpre - tation practices . The challenges were progressive , with success in earlier sets serving as a starting point for subsequent ones : instruc - tors were not ready to engage with interpreting and making use of particular metrics until they felt comfortable using the tool to find relevant information , and once the instructors were able to obtain useful insights from the analytics , they might further attempt to incorporate the analytics use into their regular teaching practices . For this reason , the first set of challenges was most common and described by twelve instructors , the second set was reported by eight instructors who had already encountered the first set , and the third set appeared for five instructors who had already encountered the first and second sets . Moreover , some instructors described how failing to overcome a subsequent set of challenges ( e . g . routinizing use ) could create a need for them to re - engage with a previous set ( e . g . navigate relevant information ) . 4 . 3 . 1 Challenges Related to Seeking Relevant Information through Navigation . The first set of challenges occurred for twelve instruc - tors who struggled to navigate the tool in a way to find the relevant information to answer particular questions , finding it disappoint - ing when this goal was not achieved . For example , one instructor engaged in constant trial and error as they navigated the tool to look for information about how many students used the resources posted in the course site : “ Even though I have specific questions about my class , I generally don’t know how to answer them [ with the dashboard ] . . . When I go to [ the dashboard ] I usually end up leaving frustrated because I wasn’t always able to do what I wanted to do . . . It’s so frustrating to have a question that you can’t answer” ( InT9 ) . Specifically , some instructors showed a problem of finding the information relevant to a certain course component ( e . g . data of student video access ) or linking the information presented to the particular course components ( e . g . don’t know whether the data presented is of a week 1 or week 2 slide ) . This issue often came from a misalignment between how the analytics were presented and the way in which the course site was organized : “ It’s data that I want to know , but it’s hard work . Week 10 , I can’t remember when week 10 is . Actually , I don’t go by weeks , I go by days . . . This is labor intensive for me to use” ( InT11 ) To address this issue , some instructors had to re - access the course site to identify which course elements correspond to the particular analytic information : “ [ When going back to the course site in LMS ] , If I go into the week seven folder , let’s see [ weekly section ] A . I don’t know where it is . Well , it’s not even in the folder . So , I don’t know what I did wrong . I don’t know where that lecture is . Maybe I just emailed it to them because it was my first time doing it” ( InT7 ) . 4 . 3 . 2 Challenges Related to Making the Analytics Useful . The sec - ond set of challenges , reported by eight instructors , involved the core process of instructor sensemaking of analytics , which required instructors to develop actionable insights to address their ques - tions : “ Well , I don’t know if it is really information I can translate into an actionable behavior change , or action , on my part” ( InT13 ) . Instructors tended to refrain from action when encountering such challenges : “ I try not to make any decision . It doesn’t tell me what to do” ( InT8 ) . This set of challenges sometimes prompted instructors to go back to address the first challenge . For instance , when they found it difficult to understand what the analytics said about the class , instructors returned to the first challenge to seek other rele - vant information to answer the questions : “I couldn’t make sense of a lot of this information . I just wanted to see what are the most open files and where are people going to the most . That , I can get out of it , but beyond that , I find that difficult” ( InT5 ) . These challenges derived from three uncertainties instructors had about the analytics . First , instructors did not know what data were collected and how the data were processed to represent the high - level metrics of student behaviors in the dashboard . For in - stance , some instructors found it difficult to figure out which data sources in the LMS were used to measure the metrics labeled as student interaction with materials : “ What does interaction means ? Does it mean that they clicked on the assignment ? I don’t know what the analytics are actually pulling off of [ the LMS ] to judge that this particular student interacted but no one else did” ( InT13 ) . Instruc - tors also consistently encountered the basic challenges of how to accurately interpret the statistics provided : “ I did have a hard time knowing exactly what [ the metric showing 12 items were selected by 40 participants ] means , which is does it mean out of four learning checks they answered three , or so on ? I don’t really know” ( InT3 ) . The second uncertainty was about conceptually understanding what inferences could be made about student learning from the analytics . For instance , one instructor encountered challenges in understand - ing what the metrics of student revisit to the materials said about how students were engaging in the course : “ The number of times they go back [ to the activities ] is interesting , I don’t know what it tells you though . There’re so many reasons why you could go back from multiple interactions” ( InT13 ) . The last uncertainty occurred 350 LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Qiujie Li et al . when instructors were not able to figure out from the analytics what contributed to a problem identified in the analytics and thus what actions could be taken : “ I’m surprised that this particular stu - dent hasn’t accessed particular readings because the student spoke specifically to me about this topic . But , I don’t know what I would say to this individual in a conversation” ( InT11 ) . 4 . 3 . 3 Challenges in Routinizing Data Seeking and Interpretation . The third set of challenges emerged for five instructors when keep - ing working with analytics for their course and advanced to try to develop routines of seeking and interpreting the analytics . Those instructors found it important to use analytics on a regular basis of teaching practices to reduce the demanding amount of effort required to work with the tool : “ I really like to know in as efficient a manner as possible how many people are using what resource that I’m posting . But even that I won’t say that I’ve developed a profi - ciency where I can do it efficiently . . . . . . Without using something on a weekly basis , I just don’t get very efficient using it ( InT13 ) ” and to make a sound interpretation of the analytics : “ I have to think too much in [ the dashboard ] . Too much trying to figure out [ what ] the engagement grid of the dashboard view [ says ] ( InT2 ) . ” Failing to develop routines of data seeking and interpretation often prompted instructors to re - search for the relevant information that may relate to their questions : “You kind of have to relearn or refill your way through it each time . It’s just like going to it and not knowing where to go , and then spending a half an hour looking for something that shouldn’t take you a half an hour to find” ( InT13 ) . One potential reason for this challenge was instructors’ difficulty in fitting analytics use into their schedule of teaching “ I wasn’t sure how to incorporate it into my workflow or assessments early on , and that really didn’t change very much , whether [ for ] better or worse” ( InT13 ) . The challenge was exacerbated due to the shift to remote instruction during COVID - 19 where instructors had to put their energy into adaptations to course delivery and design , as well as to their personal lives : “ That’s not necessarily the fault of anything other than the fact that I am just swamped with other tasks due to the remote scenario here , the remote environment” ( InT9 ) . 5 DISCUSSION AND IMPLICATIONS 5 . 1 How instructors make sense of analytics : Questions , techniques , and overall data ecosystem This study delved into the sensemaking process of thirteen instruc - tors with analytic experience and examined in detail the kinds of questions they asked of analytics , the techniques they used to read and explain data , and the challenges they encountered while doing so . The instructors developed three kinds of specific questions each with a clear pedagogical focus : goal - oriented questions , problem - oriented questions , and instruction modification questions . This stands in contrast to prior work , in which instructors often came to the data simply with curiosity [ 31 ] , suggesting that , with increased experience , instructors may clarify their interest in analytics lead - ing to the formation of specific questions [ 24 , 30 ] . Each of the three question types prompted instructors to engage in a particular form of analytic sensemaking by activating their pedagogical knowledge and contextual information . For instance , questions related to ped - agogical goals motivated instructors to compare student activity data with desirable student behaviors or outcomes and consider actions to take if goals were not met . This shows the potentially powerful role of questions in shaping not only what is learned from the data but how it is learned . In addition , instructors in the current study interacted with the analytics in more sophisticated ways than observed previously . First , instead of simply reading data after generating a question , instructors sometimes engaged in an interim step of developing expectations about the answer ( s ) the data would provide . To de - velop expectations , instructors used their pedagogical knowledge about the specific learning context to project the implications of what they already knew onto the data to be examined . Such use of analytics to help test and modify local theories about learning in the classroom illustrates the power of conceptually - driven use , in contrast to the limited extensibility of naïve dustbowl empiri - cism [ 32 ] . Second , instructors displayed various purposes for and ways of making comparisons , a sensemaking technique described in prior research [ 28 ] . Previous studies found that instructors’ use of comparison was largely limited to the use of relative reference points to compare among students and judge progress , often taking a deficit perspective that focused low performing students [ 12 , 31 ] . While this use of comparison was observed again , the current study also shows that the technique of comparison can serve broader pur - poses , including to reveal student diversity and to identify effects of changes in instructional practices and educational environments . The use of comparison to reveal student diversity is of particular importance as it suggests that instructors took an expansive view of student differences to reach beyond a deficit perspective and consider how to make instruction suitable for all kinds of students . Finally , while the introduction of analytics has been suggested as a way to shift instructors towards making data - informed decisions , findings from this study show that , in fact , instructors incorporated analytics into an existing ecosystem of data sources that inform their teaching ( e . g . surveys , observations , student artifacts ) . Thus , it is more appropriate to conceptualize the introduction of analytics as an expansion of instructors’ existing data practices that can help paint a more comprehensive view of students’ motivation , behavior , and performance . This underscores the importance of research that studies analytics use in situ , as asking isolated questions about dashboards alone are unlikely to reflect the complex picture of actual use . 5 . 2 Progressive challenges with analytic sensemaking While prior research mainly focused on challenges in the initial learning curve of analytics use , our study found that instructors encountered three different sets of continued challenges : seeking relevant information , making analytics useful , and routinizing an - alytics use into teaching practices . Ealier sets of challenges led to subsequent ones , indicating that the process of analytic sensemak - ing is progressive in nature . Frustration in information seeking and drawing useful insights from the analytics were commonly identified with first time uses and may potentially contribute to the problems of not taking action and / or low levels of adoption [ 3 , 4 ] . In 351 Beyond First Encounters with Analytics : Questions , Techniques and Challenges in Instructors’ Sensemaking LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA addition to these previously reported sets of challenges , this study found that instructors needed to routinize their analytic sensemak - ing not to lose insights and make the practice efficient . This last set of challenges played a critical role in instructors’ analytics use , as failing to routinize analytics use , instructors reencountered earlier sets of challenges to find relevant information or interpret the data again . This highlights an under - addressed area of support needed for developing a sustainable practice of routinized analytics use in instructors’ teaching practices . 5 . 3 Towards supporting analytic sensemaking : Implications for practice The findings from this study offer a starting point for develop - ing systematic support for instructors’ analytic sensemaking by forming specific questions , developing effective data interpretation techniques , and addressing common challenges . Such support can be provided both by preparing instructors to use analytic tools and offering real - time support . For preparation , instructors need activities ( e . g . workshops or one - on - one coaching sessions ) that go beyond a basic showcase of how to navigate the analytics , to demonstrate the full loop from question to insight to action [ 13 ] . The three kinds of questions found in this study offer a useful frame - work to help instructors articulate what they want to know from the data . As each of the three question types prompt different ways of sensemaking , suggestions about possible techniques can be pro - vided to instructors based on the questions they generate . Moreover , particular attention should be given to preparing instructors for the different challenges they may encounter , especially those related to routinizing analytics use . For example , coaches can help instructors pare down their questions , identify the most relevant data view to focus on , and specify a plan of use that fits their teaching schedule ( e . g . use the student engagement view just before class to check how many students have accessed the videos ) . In addition to preparing instructors , pedagogical support is needed to facilitate instructors’ use of these questions and tech - niques during the sensemaking process . While human support is always valuable , there are also opportunities to incorporate sup - portive features into the tools themselves . Specifically , for question generation and management , an annotation function can be embed - ded for instructors to record , keep track of , and tie expectations and data to their questions throughout their analytics use . Prompts for reflection can also be added to bring instructors’ attention to their underlying assumptions about the local contexts that contribute to their expectations ( e . g . What do you think students’ levels of access to the slides will be ? Why ? ) and guide their reflection on how the analytics can be used to verify ( or refute ) their assumptions ( e . g . Do the results align with your expectations ? What does this align - ment / misalignment tell you ? ) . To facilitate the use of comparison to reveal student diversity and to identify effects of changes , analytic tools can be designed to allow instructors to not only set up individ - ual filter criteria , but use these to compare data across subgroups for multiple metrics or time periods to get a comprehensive under - standing of their differences and similarities . Lastly , for instructors to better develop and manage their overall data ecosystems , oppor - tunities to flexibly choose which information they want available from the system and the possibility to add additional structured or unstructured information to it should be considered . 5 . 4 Limitation and future research There are a few limitations to this study . First , the retrospective data collected in this study limits the temporal understanding of how in - structor use of analytics evolves . Future work could be strengthened by real - time observational studies that collect use - in - the - moment data across the semester to systematically document how instruc - tors develop specific questions and sensemaking techniques and how the challenges related to sensemaking arise , persist or are solved in the long term . However , the complicated practicalities of such work , along with the invasiveness and increased burden on instructors need to be considered carefully . In addition , the sample in this study consisted of a small group of motivated instructors in a university where long - term institutional efforts have been un - dertaken to encourage and support the adoption of teacher - facing analytics . As a result , the types of questions asked , techniques used , and challenges encountered may not generalize to the broader pop - ulation of instructors in higher education settings . One avenue for future research is thus to develop survey instruments to investi - gate at scale instructors’ use of different sensemaking practices and predictors of adopting these practices across different instruc - tor backgrounds , tools , and institutional support systems . Lastly , though not the focus of this study , action taking is a critical com - ponent of instructor analytics use [ 7 ] . While the types of questions and techniques identified in the current study can be used to draw actionable insights from analytics , future research is needed to ex - amine the extent to which these questions and techniques indeed lead to action taking , and eventually enhanced student learning . 6 CONCLUSION This study contributes to a more nuanced understanding of how instructors make sense of analytics in an authentic teaching and learning environment . Findings demonstrated that instructors go beyond actions documented in prior literature to ask focused ques - tions of analytics and to use specific techniques to read and explain data with clearer and more diversified pedagogical purposes . More - over , the study confirmed that instructors not only encounter an initial learning curve in their attempts to make analytics useful for their teaching but also continue to face such challenges when not able to routinize analytic use into their practices . Together these findings highlight the importance of and offer recommendations for the design of systems of support to better prepare and guide instructors in making sense of analytics to fulfill the promise of learning analytics to impact teaching and learning . REFERENCES [ 1 ] June Ahn , Fabio Campos , Maria Hays , and Daniela Digiacomo . 2019 . Designing in context : Reaching beyond usability in learning analytics dashboard design . Journal of Learning Analytics 6 ( 2 ) , 70 - 85 . https : / / doi . org / 10 . 18608 / jla . 2019 . 62 . 5 [ 2 ] Amara Atif , Deborah Richards , Danny Liu , and Ayse A . Bilgin . 2020 . Perceived benefits and barriers of a prototype early alert system to detect engagement and support ‘at - risk’students : The teacher perspective . Computers & Education 156 , 103954 . https : / / doi . org / 10 . 1016 / j . compedu . 2020 . 103954 [ 3 ] Natasha Arthars and Danny Y . - T . Liu . 2020 . How and why faculty adopt learning analytics . In AdoptionofDataAnalyticsinHigherEducationLearningandTeaching , Dirk Ifenthaler and David Gibson ( Eds . ) . Springer International Publishing , 201 - 220 . https : / / doi . org / 10 . 1007 / BF02766777 352 LAK21 , April 12 – 16 , 2021 , Irvine , CA , USA Qiujie Li et al . [ 4 ] Michael Brown . 2020 . Seeing students at scale : How faculty in large lecture courses act upon learning analytics dashboard data . Teaching in Higher Education 25 ( 4 ) , 384 – 400 . https : / / doi . org / 10 . 1080 / 13562517 . 2019 . 1698540 [ 5 ] Simon Buckingham Shum , Rebecca Ferguson , and Roberto Martinez - Maldonado . 2019 . Human - centred learning analytics . Journal of Learning Analytics 6 ( 2 ) , 1 - 9 . https : / / doi . org / 10 . 18608 / jla . 2019 . 62 . 1 [ 6 ] Kirsten R . Butcher and Tamara Sumner . 2011 . Self - directed learning and the sensemaking paradox . Human – Computer Interaction 26 ( 1 - 2 ) , 123 - 159 . https : / / doi . org / 10 . 1080 / 07370024 . 2011 . 556552 [ 7 ] Doun Clow . ( 2012 ) . The learning analytics cycle : closing the loop effectively . In Proceedings of the 2nd International Conference on Learning Analytics and Knowl - edge . ACM , New York , NY , 134 - 138 . https : / / doi . org / 10 . 1145 / 2330601 . 2330636 [ 8 ] Shane Dawson , Oleksandra Poquet , Cassandra Colvin , Tim Rogers , Abelardo Pardo and Dragan Gasevic . 2018 . Rethinking learning analytics adoption through complexity leadership theory . In Proceedings of the 8th International Conference on Learning Analytics and Knowledge . ACM , New York , NY , 236 - 244 . https : / / doi . org / 10 . 1145 / 3170358 . 3170375 [ 9 ] Suzanne L . Dazo , Nicholas R . Stepanek , Aarjav Chauhan , and Brian Dorn . 2017 . Examining instructor use of learning analytics . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM , New York , NY , 2504 - 2510 . https : / / doi . org / 10 . 1145 / 3027063 . 3053256 [ 10 ] William Gibson and Andrew Brown . 2009 . Working with qualitative data . Sage Publications . [ 11 ] Egon G . Guba . 1981 . Criteria for assessing the trustworthiness of naturalistic inquiries . Educational Communication and Technology Journal 29 ( 2 ) , 75 . https : / / doi . org / 10 . 1007 / BF02766777 [ 12 ] Christothea Herodotou , Bart Rienties , AvinashBoroowa , Zdenek Zdrahal , Martin Hlosta , and Galina Naydenova . 2017 . Implementing predictive learning analytics on a large scale : the teacher’s perspective . In Proceedings of the 7th International Conference on Learning Analytics and Knowledge . ACM , New York , NY , 267 - 271 . https : / / doi . org / 10 . 1145 / 3027385 . 3027397 [ 13 ] Christothea Herodotou , Bart Rienties , Martin Hlosta , Avinash Boroowa , Chrysoula Mangafa , and Zdenek Zdrahal . 2020 . The scalable implementation of predictive learning analytics at a distance learning university : Insights from a longitudinal case study . The Internet and Higher Education 45 , 100725 . https : / / doi . org / 10 . 1016 / j . iheduc . 2020 . 100725 [ 14 ] Gary Klein , Brian Moon and Robert R . Hoffman . 2006 . Making sense of sense - making 2 : A macrocognitive model . IEEE Intelligent Systems 21 ( 5 ) , 88 - 92 . https : / / doi . org / 10 . 1109 / MIS . 2006 . 100 [ 15 ] Simon Knight , Alyssa F . Wise , and Xavier Ochoa . 2019 . Fostering an impactful field of learning analytics . Journal of Learning Analytics 6 ( 3 ) , 1 - 4 . https : / / doi . org / 10 . 18608 / jla . 2019 . 63 . 1 [ 16 ] Yvonna S . Lincoln and Egon G . Guba . 1985 . Naturalistic Inquiry . London , UK . Sage Publications . [ 17 ] Leah P . Macfadyen and Shane Dawson . 2012 . Numbers are not enough . Why e - learning analytics failed to inform an institutional strategic plan . Journal of Educational Technology & Society 15 ( 3 ) , 149 - 163 . [ 18 ] Inge Molenaar and Carolien A . Knoop - van Campen . 2018 . How teachers make dashboard information actionable . IEEE Transactions on Learning Technologies 13 ( 3 ) , 347 - 355 . https : / / doi . org / 10 . 1109 / TLT . 2018 . 2851585 [ 19 ] LorelliS . Nowell , JillM . Norris , DeborahE . White , andNancyJ . Moules . 2017 . The - maticanalysis : Strivingtomeetthetrustworthinesscriteria . InternationalJournal of Qualitative Methods 16 ( 1 ) , 1 - 13 . https : / / doi . org / 10 . 1177 / 1609406917733847 [ 20 ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage points foranalysttechnologyasidentifiedthroughcognitivetaskanalysis . In Proceedings of the International Conference on Intelligence Analysis ( Vol . 5 ) . McLean , VA , 2 - 4 . [ 21 ] Mariá J . Rodríguez - Triana , Alejandra Martínez - Monés and Sara Villagra´ - Sobrino . 2016 . Learning analytics in small - scale teacher - led innovations : Ethical and data privacy issues . Journal of Learning Analytics 3 ( 1 ) , 43 - 65 . https : / / doi . org / 10 . 18608 / jla . 2016 . 31 . 4 [ 22 ] Everett M . Rogers . 2010 . Diffusion of innovations . Simon and Schuster . [ 23 ] Daniel M . Russell , Mark J . Stefii , Peter Pirolli , and Stuart K . Card . 1993 . The cost structure of sensemaking . In Proceedings of the INTERACT’93 and CHI’93 ConferenceonHumanFactorsinComputingSystems . ACM , NewYork , NY , 269 - 276 . https : / / doi . org / 10 . 1145 / 169059 . 169209 [ 24 ] StylianosSergisandDemetriosG . Sampson . 2017 . Teachingandlearninganalytics to support teacher inquiry : A systematic literature review . In Learning Analyt - ics : Fundaments , Applications , and Trends , Alejandro Peña - Ayala ( Ed . ) . Springer International Publishing , 25 – 63 . [ 25 ] Hamid Tarmazdi , Rebecca Vivian , Claudia Szabo , Katrina Falkner , and Nickolas Falkner . 2015 . Using learning analytics to visualise computer science teamwork . In Proceedings of the 2015 ACM Conference on Innovation and Technology in ComputerScience Education . ACM , New York , NY , 165 - 170 . https : / / doi . org / 10 . 1145 / 2729094 . 2742613 [ 26 ] Evan Thompson and Mog Stapleton . 2009 . Making sense of sense - making : Re - flections on enactive and extended mind theories . Topoi 28 ( 1 ) , 23 - 30 . https : / / doi . org / 10 . 1007 / s11245 - 008 - 9043 - 2 [ 27 ] Mark van Harmelen and David Workman . 2012 . Analytics for learning and teaching . CETIS Analytics Series 1 ( 3 ) , 1 - 40 . [ 28 ] Anouschka van Leeuwen . 2019 . Teachers’ perceptions of the usability of learning analytics reports in a flipped university course : when and how does information becomeactionableknowledge ? EducationalTechnologyResearchandDevelopment 67 ( 5 ) , 1043 – 1064 . https : / / doi . org / 10 . 1007 / s11423 - 018 - 09639 - y [ 29 ] Anouschka van Leeuwen , Margot van Wermeskerken , Gijsbert Erkens and Nikol Rummel . 2017 . Measuring teacher sense making strategies of learning analytics : A case study . Learning : Research and Practice 3 ( 1 ) , 42 – 58 . https : / / doi . org / 10 . 1080 / 23735082 . 2017 . 1284252 [ 30 ] Katrien Verbert , Erik Duval , Joris Klerkx , Sten Govaerts , and José . L . Santos . 2013 . Learning analytics dashboard applications . American Behavioral Scientist 57 ( 10 ) , 1500 – 1509 . https : / / doi . org / 10 . 1177 / 0002764213479363 [ 31 ] AlyssaF . WiseandYeonjiJung . 2019 . Teachingwithanalytics : Towardsasituated model of instructional decision - making . Journal of Learning Analytics 6 ( 2 ) , 53 - 69 . https : / / doi . org / 10 . 18608 / jla . 2019 . 62 . 4 [ 32 ] Alyssa F . Wise and David W . Shaffer . 2015 . Why theory matters more than ever in the age of big data . Journal of Learning Analytics 2 ( 2 ) , 5 - 13 . https : / / doi . org / 10 . 18608 / jla . 2015 . 22 . 2 353