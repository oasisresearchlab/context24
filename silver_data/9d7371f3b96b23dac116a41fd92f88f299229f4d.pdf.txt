Gradient Descent , Stochastic Optimization , and Other Tales Gradient Descent , Stochastic Optimization , and Other Tales Jun Lu jun . lu . locky @ gmail . com Abstract The goal of this paper is to debunk and dispel the magic behind the black - box optimiz - ers and stochastic optimizers . It aims to build a solid foundation on how and why the techniques work . This manuscript crystallizes this knowledge by deriving from simple intuitions , the mathematics behind the strategies . This tutorial doesn‚Äôt shy away from addressing both the formal and informal aspects of gradient descent and stochastic opti - mization methods . By doing so , it hopes to provide readers with a deeper understanding of these techniques as well as the when , the how and the why of applying these algorithms . Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks . Its stochastic version receives attention in recent years , and this is particularly true for optimizing deep neural networks . In deep neural networks , the gradient followed by a single sample or a batch of samples is employed to save computational resources and escape from saddle points . In 1951 , Robbins and Monro published A stochastic approximation method , one of the Ô¨Årst modern treatments on stochastic optimization that estimates local gradients with a new batch of samples . And now , stochastic optimization has become a core technology in machine learning , largely due to the development of the back propagation algorithm in Ô¨Åtting a neural network . The sole aim of this article is to give a self - contained introduction to concepts and mathematical tools in gradient descent and stochastic optimization . However , we clearly realize our inability to cover all the useful and interesting results concerning optimization methods and given the paucity of scope to present this discussion , e . g . , the separated analysis of trust region methods , convex optimization , and so on . We refer the reader to literature in the Ô¨Åeld of numerical optimization for a more detailed introduction to the related Ô¨Åelds . The article is primarily a summary of purpose , signiÔ¨Åcance of important concepts in optimization methods , e . g . , vanilla gradient descent , gradient descent with momentum , conjugate descent , conjugate gradient , and the origin and rate of convergence of the meth - ods which shed light on their applications . The mathematical prerequisite is a Ô¨Årst course in linear algebra and calculus . Other than this modest background , the development is self - contained , with rigorous proof provided throughout . Keywords : Gradient descent , Stochastic gradient descent , Steepest descent , Conjugate descent and conjugate gradient , Learning rate annealing , Adaptive learning rate , Second - order methods . ¬© 2022 Jun Lu . a r X i v : 2205 . 00832v1 [ c s . L G ] 2 M a y 2022 Jun Lu Contents 1 Gradient Descent 4 1 . 1 Gradient Descent by Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1 . 2 Gradient Descent by Greedy Search . . . . . . . . . . . . . . . . . . . . . . 6 1 . 3 Geometrical Interpretation of Gradient Descent . . . . . . . . . . . . . . . . 6 1 . 4 Regularization : A Geometrical Interpretation . . . . . . . . . . . . . . . . . 7 1 . 5 Quadratic Form in Gradient Descent . . . . . . . . . . . . . . . . . . . . . . 9 2 Line Search 13 2 . 1 Bisection Line Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2 . 2 Golden - Section Line Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2 . 3 Armijo Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 . 4 Quadratic Form in Steepest Descent . . . . . . . . . . . . . . . . . . . . . . 17 2 . 4 . 1 Special Case : Symmetric Quadratic Form . . . . . . . . . . . . . . . 19 2 . 4 . 2 Special Case : Symmetric with Orthogonal Eigenvectors . . . . . . . 19 2 . 4 . 3 General Convergence Analysis for Symmetric PD Quadratic . . . . . 21 3 Learning Rate Annealing and Warmup 23 3 . 1 Learning Rate Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3 . 2 Learning Rate Warmup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3 . 3 Cyclical Learning Rate ( CLR ) Policy . . . . . . . . . . . . . . . . . . . . . . 28 4 Stochastic Optimizer 31 4 . 1 Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4 . 1 . 1 Quadratic Form in Momentum . . . . . . . . . . . . . . . . . . . . . 34 4 . 2 Nesterov Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4 . 3 AdaGrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4 . 4 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4 . 5 AdaDelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4 . 6 AdaSmooth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4 . 7 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4 . 8 AdaMax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4 . 9 Nadam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4 . 10 Problem in SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 5 Second - Order Methods 51 5 . 1 Newton‚Äôs Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5 . 2 Damped Newton‚Äôs Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5 . 3 Levenberg ( - Marquardt ) Gradient Descent . . . . . . . . . . . . . . . . . . . 54 5 . 4 Conjugate Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5 . 4 . 1 Quadratic Form in Conjugate Direction ( CD ) Method . . . . . . . . 56 5 . 4 . 2 Quadratic Form in Conjugate Gradient ( CG ) Method . . . . . . . . 59 5 . 4 . 3 Convergence Analysis for Symmetric Positive DeÔ¨Ånite Quadratic . . 62 5 . 4 . 4 General Conjugate Gradient Method . . . . . . . . . . . . . . . . . . 68 2 Gradient Descent , Stochastic Optimization , and Other Tales A Taylor‚Äôs Expansion 74 3 Jun Lu 1 . Gradient Descent Gradient descent ( GD ) is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks . And this is particularly true for optimizing neural networks . The neural networks or machine learning in general Ô¨Ånd the set of parameters x ‚àà R d in order to optimize an objective function L ( x ) . The gradient descent Ô¨Ånds a sequence of parameters x 1 , x 2 , . . . , x T , ( 1 . 1 ) such that when T ‚Üí ‚àû , the objective function L ( x T ) achieves the optimal minimum value . At each iteration t , a step ‚àÜ x t is applied to change the parameters . Denoting the parameters at the t - th iteration as x t . Then the update rule becomes x t + 1 = x t + ‚àÜ x t . ( 1 . 2 ) The most naive method of gradient descents is the vanilla update : the parameter moves in the opposite direction of the gradient which Ô¨Ånds the steepest descent direction since the gradients are orthogonal to level curves ( a . k . a . , level surface , see Lemma 16 . 4 in Lu ( 2022c ) ) : ‚àÜ x t = ‚àí Œ∑ g t = ‚àí Œ∑ ‚àÇL ( x t ) ‚àÇ x t = ‚àí Œ∑ ‚àá L ( x t ) , ( 1 . 3 ) where the positive value Œ∑ is the learning rate and depends on speciÔ¨Åc problems , and g t = ‚àÇL ( x t ) ‚àÇ x t ‚àà R d is the gradient of the parameters . The learning rate Œ∑ controls how large of a step to take in the direction of negative gradient so that we can reach a ( local ) minimum . While if we follow the negative gradient of a single sample or a batch of samples iteratively , the local estimate of the direction can be obtained and is known as the stochastic gradient descent ( SGD ) ( Robbins and Monro , 1951 ) . In the SGD framework , the objective function is stochastic that is composed of a sum of subfunctions evaluated at diÔ¨Äerent subsamples of the data . However , the drawback of the vanilla update is that it is easy to get stuck in local minima ( Rutishauser , 1959 ) . For a small step size , gradient descent makes a monotonic improvement at every iter - ation . Thus , it always converges , albeit to a local minimum . However , the speed of the vanilla GD method is usually slow , while it can take an exponential rate when the curva - ture condition is poor . While choosing higher than this rate may cause the procedure to diverge in terms of the objective function . Determining a good learning rate ( either global or per - dimension ) becomes more of an art than science for many problems . Previous work has been done to alleviate the need for selecting a global learning rate ( Zeiler , 2012 ) , while it is still sensitive to other hyper - parameters . 1 . 1 Gradient Descent by Calculus An intuitive way to think of gradient descent is to imagine the path of a river originating from the top of a mountain . The goal of gradient descent is exactly what the river strives to achieve , namely , reach the bottom - most point ( at the foothill ) climbing down from the mountain . 4 Gradient Descent , Stochastic Optimization , and Other Tales To restate the problem , the objective function is L ( x ) , the input variable of L is x with d - dimension ; our goal is to use algorithm to get the minimum of L ( x ) . To make this question more precise , let‚Äôs think about what happens when we move the ball a small amount ‚àÜ x 1 in the x 1 direction , a small amount ‚àÜ x 2 in the x 2 direction , . . . , and a small amount ‚àÜ x d in the x d direction . Calculus tells us that L ( x ) changes as follows : ‚àÜ L ( x ) ‚âà ‚àÇL ‚àÇx 1 ‚àÜ x 1 + ‚àÇL ‚àÇx 2 ‚àÜ x 2 + . . . + ‚àÇL ‚àÇx d ‚àÜ x d . In this sense , we need to Ô¨Ånd a way of choosing ‚àÜ x 1 , . . . , ‚àÜ x d so as to make ‚àÜ L ( x ) negative , i . e . , we‚Äôll make the objective function decrease so as to minimize . DeÔ¨Åne ‚àÜ x = [ ‚àÜ x 1 , ‚àÜ x 2 , . . . , ‚àÜ x d ] (cid:62) to be the vector of changes in x and gradients ‚àá L ( x ) = ‚àÇL ( x ) ‚àÇ x = [ ‚àÇL‚àÇx 1 , ‚àÇL‚àÇx 2 , . . . , ‚àÇL‚àÇx d ] (cid:62) to be the gradient vector of L ( x ) 1 . Then it follows that ‚àÜ L ( x ) ‚âà ‚àÇL ‚àÇx 1 ‚àÜ x 1 + ‚àÇL ‚àÇx 2 ‚àÜ x 2 + . . . + ‚àÇL ‚àÇx d ‚àÜ x d = ‚àá L ( x ) (cid:62) ‚àÜ x . In a descent context , we want ‚àÜ L ( x ) to be negative so that a step x t + 1 = x t + ‚àÜ x t will lead to a decrease in the loss function L ( x t + 1 ) = L ( x t ) + ‚àÜ L ( x ) since ‚àÜ L ( x ) ‚â§ 0 . It can be shown that if the update step is given by ‚àÜ x = ‚àí Œ∑ ‚àá L ( x ) where Œ∑ is the learning rate , then it follows that ‚àÜ L ( x ) ‚âà ‚àí Œ∑ ‚àá L ( x ) (cid:62) ‚àá L ( x ) = ‚àí Œ∑ | | ‚àá L | | 22 ‚â§ 0 . Strictly speaking , ‚àÜ L ( x ) < 0 in the above equation , otherwise we Ô¨Ånd the optimal point with zero gradients . This shows the correctness of gradient descent . We can use the following update rule to update the next x t + 1 : x t + 1 = x t ‚àí Œ∑ ‚àá L ( x t ) . This update rule will make the objective function drop to the minimum point steadily in a convex setting or local minima in a non - convex setting . Remark 1 . 1 : Descent Condition In above construction , we let ‚àÜ x = ‚àí Œ∑ ‚àá L ( x ) where ‚àí‚àá L ( x ) is the descent direction such that ‚àÜ L ‚âà ‚àí Œ∑ ‚àá L ( x ) (cid:62) ‚àá L ( x ) < 0 ( we assume ‚àá L ( x ) (cid:54) = 0 ) . More generally , any search direction d t ‚àà R d \ { 0 } that satisÔ¨Åes the descent condition can be chosen as the descent direction : dL ( x t + Œ∑ d t ) dŒ∑ (cid:12)(cid:12)(cid:12)(cid:12) Œ∑ = 0 = ‚àá L ( x t ) (cid:62) d t < 0 . In other words , by Taylor‚Äôs formula ( Appendix A , p . 74 ) L ( x t + Œ∑ d t ) ‚âà L ( x t ) + Œ∑ ‚àá L ( x t ) (cid:62) d t implies L ( x t + Œ∑ d t ) < L ( x t ) when Œ∑ is suÔ¨Éciently small . When d t = ‚àá L ( x t ) , the descent direction is known as the steepest descent direction . When the learning rate 1 . Note the diÔ¨Äerence between ‚àÜ L ( x ) and ‚àá L ( x ) . 5 Jun Lu Œ∑ is not Ô¨Åxed and decided by exact line search , the method is called steepest descent method ( see Section 2 . 4 , p . 17 ) . Gradient descent in a convex problems We further consider the gradient descent in a convex problem . If the objective function is convex , then we have the fact that ‚àá L ( x t ) (cid:62) ( x t + 1 ‚àí x t ) ‚â• 0 implies L ( x t + 1 ) ‚â• L ( x t ) . This can be derived from the con - vex property of a convex function , i . e . , L ( x t + 1 ) ‚â• L ( x t ) (cid:62) ( x t + 1 ‚àí x t ) . In this sense , we need to make ‚àá L ( x t ) (cid:62) ( x t + 1 ‚àí x t ) ‚â§ 0 so as to make the objective function decrease . In gradient descent ‚àÜ x t = x t + 1 ‚àí x t is chosen to be negative gradient ‚àí‚àá L ( x t ) . However , there are many other descent methods , such as steepest descent , nor - malized steepest descent , newton step , and so on . The main idea of these methods is to make ‚àá L ( x t ) (cid:62) ( x t + 1 ‚àí x t ) = ‚àá L ( x t ) (cid:62) ‚àÜ x t ‚â§ 0 ( Beck , 2017 ) . 1 . 2 Gradient Descent by Greedy Search We will be considering the greedy search such that x t + 1 ‚Üê arg min x t L ( x t ) . Suppose we want to approximate x t + 1 by a linear update on x t , i . e . , x t + 1 = x t + Œ∑ v . The problem now turns to the solution of v such that v = arg min v L ( x t + Œ∑ v ) . By Taylor‚Äôs formula ( Appendix A , p . 74 ) , L ( x t + Œ∑ v ) can be approximated by L ( x t + Œ∑ v ) ‚âà L ( x t ) + Œ∑ v (cid:62) ‚àá L ( x t ) , when Œ∑ is small enough . Then a search under the condition | | v | | = 1 given positive Œ∑ is as follows : v = arg min | | v | | = 1 L ( x t + Œ∑ v ) ‚âà arg min | | v | | = 1 (cid:110) L ( x t ) + Œ∑ v (cid:62) ‚àá L ( x t ) (cid:111) . This is known as the greedy search . The optimal v can be obtained by v = ‚àí ‚àá L ( x t ) | | ‚àá L ( x t ) | | , i . e . , v is in the opposite direction of ‚àá L ( x t ) . Therefore , the update of x t + 1 is reasonable to be taken as x t + 1 = x t + Œ∑ v = x t ‚àí Œ∑ ‚àá L ( x t ) | | ‚àá L ( x t ) | | , which is usually called the gradient descent as aforementioned . If we further absorb the denominator into the step size Œ∑ , the gradient descent can be applied in the trivial way : x t + 1 = x t ‚àí Œ∑ ‚àá L ( x t ) . 1 . 3 Geometrical Interpretation of Gradient Descent 6 Gradient Descent , Stochastic Optimization , and Other Tales Lemma 1 . 2 : ( Direction of Gradients ) An important fact is that gradients are orthogonal to level curves ( a . k . a . , level surface ) . Proof [ of Lemma 1 . 2 ] This is equivalent to proving that the gradient is orthogonal to the tangent of the level curve . For simplicity , let‚Äôs Ô¨Årst look at the 2 - dimensional case . Suppose the level curve has the form f ( x , y ) = c . This implicitly gives a relation between x and y such that y = y ( x ) where y can be thought of as a function of x . Therefore , the level curve can be written as f ( x , y ( x ) ) = c . The chain rule indicates ‚àÇf ‚àÇx dx dx (cid:124)(cid:123)(cid:122)(cid:125) = 1 + ‚àÇf ‚àÇy dy dx = 0 . Therefore , the gradient is perpendicular to the tangent : (cid:28) ‚àÇf ‚àÇx , ‚àÇf ‚àÇy (cid:29) ¬∑ (cid:28) dx dx , dy dx (cid:29) = 0 . Let us now treat the problem in full generality , suppose the level curve of a vector x ‚àà R n : f ( x ) = f ( x 1 , x 2 , . . . , x n ) = c . Each variable x i can be regarded as a function of a variable t on the level curve f ( x ) = c : f ( x 1 ( t ) , x 2 ( t ) , . . . , x n ( t ) ) = c . DiÔ¨Äerentiate the equation with respect to t by chain rule : ‚àÇf ‚àÇx 1 dx 1 dt + ‚àÇf ‚àÇx 2 dx 2 dt + . . . + ‚àÇf ‚àÇx n dx n dt = 0 . Therefore , the gradients is perpendicular to the tangent in n - dimensional case : (cid:28) ‚àÇf ‚àÇx 1 , ‚àÇf ‚àÇx 2 , . . . , ‚àÇf ‚àÇx n (cid:29) ¬∑ (cid:28) dx 1 dt , dx 2 dt , . . . dx n dt (cid:29) = 0 . This completes the proof . The lemma above reveals the geometrical interpretation of gradient descent . For Ô¨Ånding a solution to minimize a convex function L ( x ) , gradient descent goes to the negative gradient direction that can decrease the loss . Figure 1 depicts a 2 - dimensional case , where ‚àí‚àá L ( x ) pushes the loss to decrease for the convex function L ( x ) . 1 . 4 Regularization : A Geometrical Interpretation The gradient descent can reveal the geometrical meaning of regularization . To avoid con - fusion , we denote the loss function without regularization by l ( z ) and the loss with reg - ularization by L ( x ) = l ( x ) + Œª x | | x | | 2 where l ( x ) : R d ‚Üí R ( the notation is used only in this section ) . When minimizing l ( x ) , the descent method will search in R d for a solution . However , in machine learning , searching in the whole space can cause overÔ¨Åtting . A partial solution is to search in a subset of the vector space , e . g . , searching in x (cid:62) x < C for some constant C . That is arg min x l ( x ) , s . t . , x (cid:62) x ‚â§ C . 7 Jun Lu x 1 2 0 2 4 6 8 x 2 2 1 0 1 2 3 4 5 6 L ( x ) 050 100 150 200 250 300 350 400 ( a ) A 2 - dimensional convex function L ( z ) x 1 x 2 L ( x ) ( b ) L ( z ) = c is a constant Figure 1 : Figure 1 ( a ) shows a convex function surface plot and a contour plot ( blue = low , yellow = high ) where the upper graph is the surface plot , and the lower one is the projection of it ( i . e . , contour ) . Figure 1 ( b ) : ‚àí‚àá L ( x ) pushes the loss to decrease for the convex function L ( x ) . ùëôùëô ( ùë•ùë• ) = ùëêùëê 1 ùë•ùë• ùëáùëá ùë•ùë• = C - ùõªùõªùëôùëô ( ùë•ùë• 1 ) ùë•ùë• 1 ùë£ùë£ 1 0 ùëôùëô ( ùë•ùë• ) = ùëêùëê 2 ùë•ùë• ùëáùëá ùë•ùë• = C - ùõªùõªùëôùëô ( ùë•ùë• 2 ) ùë•ùë• 2 ùë£ùë£ 2 0 ùë§ùë§ ùë•ùë• 1 - ùõªùõªùëôùëô ( ùë•ùë• 1 ) ùë§ùë§ ‚àí2ùúÜùúÜùë•ùë• 1 ùë•ùë• ‚àó ùë•ùë• ‚àó Figure 2 : Constrained gradient descent with x (cid:62) x ‚â§ C . The green vector w is the projection of v 1 into x (cid:62) x ‚â§ C where v 1 is the component of ‚àí‚àá l ( x ) perpendicular to x 1 . The right picture is the next step after the update in the left picture . x (cid:63) denotes the optimal solution of { min l ( x ) } . As shown above , a trivial gradient descent method will go further in the direction of ‚àí‚àá l ( x ) , i . e . , update x by x ‚Üê x ‚àí Œ∑ ‚àá l ( x ) for small step size Œ∑ . When the level curve is l ( x ) = c 1 and the current position of x = x 1 where x 1 is the intersection of x (cid:62) x = C and l ( x ) = c 1 , the descent direction ‚àí‚àá l ( x 1 ) will be perpendicular to the level curve of l ( x 1 ) = c 1 as shown in the left picture of Figure 2 . However , if we further restrict that the optimal value can only be in x (cid:62) x ‚â§ C , the trivial descent direction ‚àí‚àá l ( x 1 ) will lead x 2 = x 1 ‚àí Œ∑ ‚àá l ( x 1 ) outside of x (cid:62) x ‚â§ C . A solution is to decompose the step ‚àí‚àá l ( x 1 ) into ‚àí‚àá l ( x 1 ) = a x 1 + v 1 , 8 Gradient Descent , Stochastic Optimization , and Other Tales where a x 1 is the component perpendicular to the curve of x (cid:62) x = C , and v 1 is the compo - nent parallel to the curve of x (cid:62) x = C . Keep only the step v 1 , then the update x 2 = project ( x 1 + Œ∑ v 1 ) = project Ô£´ Ô£≠ x 1 + Œ∑ ( ‚àí‚àá l ( x 1 ) ‚àí a x 1 ) (cid:124) (cid:123)(cid:122) (cid:125) v 1 Ô£∂ Ô£∏ 2 will lead to a smaller loss from l ( x 1 ) to l ( x 2 ) and it still matches the prerequisite of x (cid:62) x ‚â§ C . This is known as the projection gradient descent . It is not hard to see that the update x 2 = project ( x 1 + Œ∑ v 1 ) is equivalent to Ô¨Ånding a vector w ( shown by the green vector in the left picture of Figure 2 ) such that x 2 = x 1 + w is inside the curve of x (cid:62) x ‚â§ C . Mathematically , the w can be obtained by ‚àí‚àá l ( x 1 ) ‚àí 2 Œª x 1 for some Œª as shown in the middle picture of Figure 2 . This is exactly the negative gradient of L ( x ) = l ( x ) + Œª | | x | | 2 such that ‚àí‚àá L ( x ) = ‚àí‚àá l ( x ) ‚àí 2 Œª x , and w = ‚àí‚àá L ( x ) leads to ‚àí‚àí‚àí‚àí‚àí‚Üí x 2 = x 1 + w = x 1 ‚àí ‚àá L ( x ) . And in practice , a small step size Œ∑ can avoid going outside the curve of x (cid:62) x ‚â§ C : x 2 = x 1 ‚àí Œ∑ ‚àá L ( x ) . 1 . 5 Quadratic Form in Gradient Descent We discuss further the ( vanilla ) gradient descent on the simplest model possible , the convex quadratic , L ( x ) = 1 2 x (cid:62) Ax ‚àí b (cid:62) x + c , x ‚àà R d , ( 1 . 4 ) where A ‚àà R d √ó d , b ‚àà R d , and c is a scalar constant . Though the quadratic form in Eq . ( 1 . 4 ) is an extremely simple model , it is rich enough to approximate many other functions , e . g . , the Fisher information matrix ( Amari , 1998 ) and capture key features of pathological curvature . The gradient of L ( x ) at point x is given by ‚àá L ( x ) = 1 2 ( A (cid:62) + A ) x ‚àí b . ( 1 . 5 ) The unique minimum of the function is the solution of the linear system 12 ( A (cid:62) + A ) x = b : x (cid:63) = 2 ( A (cid:62) + A ) ‚àí 1 b . ( 1 . 6 ) If A is symmetric ( for most of our discussions , we will restrict to symmetric A or even PD ) , the equation reduces to ‚àá L ( x ) = Ax ‚àí b . ( 1 . 7 ) 2 . where the project ( x ) will project the vector x to the closest point inside x (cid:62) x ‚â§ C . Notice here the direct update x 2 = x 1 + Œ∑ v 1 can still make x 2 outside the curve of x (cid:62) x ‚â§ C . 9 Jun Lu x 1 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 x 2 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 L ( x ) 50 0 50 100 150 200 ( a ) Positive deÔ¨Ånite matrix : A = (cid:20) 200 0 0 200 (cid:21) . x 1 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 x 2 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 L ( x ) 250 200 150 100 50 0 ( b ) Negative deÔ¨Ånite matrix : A = (cid:20) ‚àí 200 0 0 ‚àí 200 (cid:21) . x 1 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 x 2 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 L ( x ) 100 0 100 200 300 400 ( c ) Singular matrix : A = (cid:20) 200 250 200 200 (cid:21) . A line runs through the bottom of the valley is the set of so - lutions . x 1 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 x 2 1 . 000 . 750 . 500 . 250 . 000 . 250 . 500 . 751 . 00 L ( x ) 150 100 50 0 50 100 ( d ) IndeÔ¨Ånte matrix : A = (cid:20) 200 0 0 ‚àí 200 (cid:21) . Figure 3 : Loss surface for diÔ¨Äerent quadratic forms . Then the unique minimum of the function is the solution of the linear system Ax = b where A , b are known matrix or vector , x is an unknown vector ; and the optimal point of x is thus given by x (cid:63) = A ‚àí 1 b . For diÔ¨Äerent types of matrix A , the loss surface of L ( x ) will be diÔ¨Äerent as shown in Figure 3 . When A is positive deÔ¨Ånite , the surface is a convex bowl ; when A is negative deÔ¨Ånite , on the contrary , the surface is a concave bowl . A also could be singular , in which case Ax ‚àí b has more than one solution , and the set of solutions is a line ( in the 2D case of Figure 3 ( c ) ) or a hyperplane ( in the high - dimensional case ) . Moreover , A could be none of the above , then there exists a saddle point where the gradient descent may fail . In this sense , other methods , e . g . , perturbed GD ( Jin et al . , 2017 ; Du et al . , 2017 ) , can be applied to escape the saddle points . Note here we don‚Äôt have to do any gradient descent , we can just jump directly to the minimum . However , we are concerned with the iterative updates of the convex quadratic 10 Gradient Descent , Stochastic Optimization , and Other Tales function . Suppose we pick up a starting point x 1 ‚àà R d 3 . The trivial way for the update at time step t is to set the learning rate Œ∑ Ô¨Åxed ; and the gradient descent update becomes : x t + 1 = x t + Œ∑ d t . This will create a monotonically decreasing sequence of { L ( x t ) } . When the descent direction is chosen to be the negative gradient , the update becomes Vanilla GD : x t + 1 = x t ‚àí Œ∑ ( Ax t ‚àí b ) . ( 1 . 8 ) A concrete example is given in Figure 4 where A = (cid:20) 20 7 5 5 (cid:21) , b = 0 , and c = 0 . Suppose at t - th iteration , x t = [ ‚àí 3 , 3 . 5 ] (cid:62) . Figure 4 ( a ) shows the descent direction given by the negative gradient ; Figure 4 ( b ) and Figure 4 ( c ) present 10 iterations afterwards with Œ∑ = 0 . 02 and Œ∑ = 0 . 08 respectively . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 d t = L ( x t ) 8 32 56 80 104 128 152 176 200 224 L o ss V a l u e ( a ) Contour and the descent direc - tion . The red dot is the opti - mal point . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( b ) Vanilla GD , Œ∑ = 0 . 02 . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( c ) Vanilla GD , Œ∑ = 0 . 08 . Figure 4 : Illustration for the linear search of quadratic form with A = (cid:20) 20 7 5 5 (cid:21) , b = 0 , and c = 0 . The procedure is at x t = [ ‚àí 3 , 3 . 5 ] (cid:62) for the t - th iteration . Closed form for vanilla GD When A is symmetric , it admits spectral decomposition ( Theorem 13 . 1 in Lu ( 2022c ) ) : A = Q Œõ Q (cid:62) ‚àà R d √ó d leads to ‚àí‚àí‚àí‚àí‚àí‚Üí A ‚àí 1 = Q Œõ ‚àí 1 Q (cid:62) , where the columns of Q = [ q 1 , q 2 , . . . , q d ] are eigenvectors of A and are mutually orthonor - mal , and the entries of Œõ = diag ( Œª 1 , Œª 2 , . . . , Œª d ) are the corresponding eigenvalues of A which are real . If we further assume A is positive deÔ¨Ånite , then the eigenvalues are all positive . By convention , we order the eigenvalues such that Œª 1 ‚â• Œª 2 ‚â• . . . ‚â• Œª d . DeÔ¨Åne the following iterate vector at iteration t y t = Q (cid:62) ( x t ‚àí x (cid:63) ) , ( 1 . 9 ) 3 . In some texts , the starting point is denoted as x 0 , however , we will take it as x 1 in this article . 11 Jun Lu where x (cid:63) = A ‚àí 1 b if we further assume A is nonsingular as aforementioned . It then follows that y t + 1 = Q (cid:62) ( x t + 1 ‚àí x (cid:63) ) = Q (cid:62) ( x t ‚àí Œ∑ ( Ax t ‚àí b ) ‚àí x (cid:63) ) ( x t + 1 = x t ‚àí Œ∑ ‚àá L ( x t ) ) = Q (cid:62) ( x t ‚àí x (cid:63) ) ‚àí Œ∑ Q (cid:62) ( Ax t ‚àí b ) = y t ‚àí Œ∑ Q (cid:62) ( Q Œõ Q (cid:62) x t ‚àí b ) ( A = Q Œõ Q (cid:62) ) = y t ‚àí Œ∑ ( Œõ Q (cid:62) x t ‚àí Q (cid:62) b ) = y t ‚àí Œ∑ Œõ Q (cid:62) ( x t ‚àí x (cid:63) ) = y t ‚àí Œ∑ Œõ y t = ( I ‚àí Œ∑ Œõ ) y t = ( I ‚àí Œ∑ Œõ ) t y 1 where the second equality is from Eq ( 1 . 8 ) . This reveals the error at each iteration : | | x t + 1 ‚àí x (cid:63) | | 2 = | | Qy t + 1 | | 2 = | | Q ( I ‚àí Œ∑ Œõ ) t y 1 | | 2 = (cid:12) (cid:12)(cid:12)(cid:12) (cid:12) (cid:12)(cid:12)(cid:12) d (cid:88) i = 1 y 1 , i ¬∑ ( 1 ‚àí Œ∑Œª i ) t q i (cid:12) (cid:12)(cid:12)(cid:12) (cid:12) (cid:12)(cid:12)(cid:12) 2 , ( 1 . 10 ) where y 1 depends on the initial parameter x 1 , and y 1 , i is the i - th element of y 1 . An intuitive interpretation for y t + 1 is the error in the Q - basis at iteration t + 1 . By Eq ( 1 . 10 ) , we realize that the learning rate should be chosen such that | 1 ‚àí Œ∑Œª i | ‚â§ 1 , ‚àÄ i ‚àà { 1 , 2 , . . . , d } . ( 1 . 11 ) And the error is a sum of d terms , each has its own dynamics and depends on the rate of 1 ‚àí Œ∑Œª i ; the closer the rate is to 1 , the slower it converges in that dimension ( Shewchuk et al . , 1994 ; O‚Äôdonoghue and Candes , 2015 ; Goh , 2017 ) . To converge , the learning rate should be satisÔ¨Åed that | 1 ‚àí Œ∑Œª i | ‚â§ 1 . This implies 0 < Œ∑Œª i < 2 for i in { 1 , 2 , . . . , d } . And therefore , the overall rate of convergence is determined by the slowest component : rate ( Œ∑ ) = max { | 1 ‚àí Œ∑Œª 1 | , | 1 ‚àí Œ∑Œª d | } , since Œª 1 ‚â• Œª 2 ‚â• . . . ‚â• Œª d . The optimal learning rate is obtained when the Ô¨Årst and the last eigenvectors converge at the same rate , i . e . , Œ∑Œª 1 ‚àí 1 = 1 ‚àí Œ∑Œª d : optimal Œ∑ = arg min Œ∑ rate ( Œ∑ ) = 2 Œª 1 + Œª d , ( 1 . 12 ) and optimal rate = min Œ∑ rate ( Œ∑ ) = Œª 1 / Œª d ‚àí 1 Œª 1 / Œª d + 1 = Œ∫ ‚àí 1 Œ∫ + 1 , ( 1 . 13 ) where Œ∫ = Œª 1 Œª d is known as the condition number . When Œ∫ = 1 , the convergence is fast with just one step ; and when the condition number is larger , the gradient descent becomes slower . The rate of convergence ( per iteration ) is plotted in Figure 5 . The more ill - conditioned the matrix , i . e . , the larger its condition number , the slower the convergence of vanilla GD . 12 Gradient Descent , Stochastic Optimization , and Other Tales 0 20 40 60 80 100 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R a t e o f C o n v e r g e n c e Figure 5 : Rate of convergence ( per itera - tion ) in vanilla GD method . The y - axis is Œ∫ ‚àí 1 Œ∫ + 1 . 2 . Line Search In last section , we derive the gradient descent where the update step at step t is ‚àí Œ∑ ‚àá L ( x t ) = ‚àí Œ∑ g t and the learning rate Œ∑ controls how large of a step to take in the direction of negative gradient . Line search is a method that directly Ô¨Ånds the optimal learning rate in order to provide the best improvement in the gradient movement . Formally , the line search solves the following problem at the t - th step of gradient descent : Œ∑ t = arg min Œ∑ L ( x t ‚àí Œ∑ g t ) . After performing the gradient update x t + 1 = x t ‚àí Œ∑ t g t , the gradient is computed at x t + 1 for the next step t + 1 . More generally , let d t be the descent direction , then the gradient descent with line search ( to diÔ¨Äerentiate , we call it steepest descent when d t = ‚àí g t in this article , and the Ô¨Åxed learning rate GD is known as the vanilla GD ) can be described by : Œ∑ t = arg min Œ∑ L ( x t + Œ∑ d t ) . Lemma 2 . 1 : ( Orthogonality in Line Search ) The gradient of optimal point x t + 1 = x t + Œ∑ t d t of a line search is orthogonal to the current update direction d t : ‚àá L ( x t + 1 ) (cid:62) d t = 0 . Proof [ of Lemma 2 . 1 ] Suppose ‚àá L ( x t + 1 ) (cid:62) d t (cid:54) = 0 , then there exists a Œ¥ and it follows by Taylor‚Äôs formula ( Appendix A , p . 74 ) that L ( x t + Œ∑ t d t ¬± Œ¥ d t ) ‚âà L ( x t + Œ∑ t d t ) ¬± Œ¥ d (cid:62) t ‚àá L ( x t + Œ∑ t d t ) . ( 2 . 1 ) Since x t + Œ∑ t d t is the optimal move such that L ( x t + Œ∑ t d t ) ‚â§ L ( x t + Œ∑ t d t ¬± Œ¥ d t ) and Œ¥ (cid:54) = 0 . This leads to the claim d (cid:62) t ‚àá L ( x t + Œ∑ t d t ) = 0 . 13 Jun Lu We complete the proof . In line search methods , the loss function is expressed in terms of Œ∑ at iteration t : J ( Œ∑ ) = L ( x t + Œ∑ d t ) . And the problem then becomes Œ∑ t = arg min Œ∑ L ( x t + Œ∑ d t ) = arg min Œ∑ J ( Œ∑ ) . This reveals that the ( local ) minimum of Œ∑ can be obtained by Ô¨Ånding the solution of J (cid:48) ( Œ∑ ) = 0 if J ( Œ∑ ) is diÔ¨Äerentiable . The solution then follows that J (cid:48) ( Œ∑ ) = d (cid:62) t ‚àá L ( x t + Œ∑ d t ) = 0 , ( 2 . 2 ) which again proves Lemma 2 . 1 . When Œ∑ = 0 , we have ( by Remark 1 . 1 ) J (cid:48) ( 0 ) = d (cid:62) t g t ‚â§ 0 . ( 2 . 3 ) One important property of typical line search settings is that the loss function J ( Œ∑ ) when expressed in terms of Œ∑ is often a unimodal function . And if we Ô¨Ånd a Œ∑ max such that J (cid:48) ( Œ∑ max ) > 0 , the optimal learning rate is then in the range of [ 0 , Œ∑ max ] . 2 . 1 Bisection Line Search In bisection line search , we start by setting the interval [ a , b ] as [ Œ∑ min , Œ∑ max ] where Œ∑ min and Œ∑ max are minimum and maximum boundaries for the learning rate Œ∑ ( Œ∑ min can be set to 0 by Eq ( 2 . 3 ) ) . The bisection line search evaluates the loss function J ( Œ∑ ) at the midpoint a + b 2 . Since we know J (cid:48) ( a ) < 0 and J (cid:48) ( b ) > 0 , the bisection line search follows that Ô£±Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£≥ set a = a + b 2 , if J (cid:48) (cid:18) a + b 2 (cid:19) < 0 ; set b = a + b 2 , if J (cid:48) (cid:18) a + b 2 (cid:19) > 0 . The procedure is repeated until the range between a and b is small enough . The bisection line search is also known as the binary line search . And in some cases , the derivative of J ( Œ∑ ) cannot be easily obtained ; then the interval is narrowed by evaluating the objective function at two closely spaced points near a + b 2 . To be more concrete , assume J ( Œ∑ ) is convex ( since we are in the descent setting ) , we evaluate the loss function at a + b 2 and a + b 2 + (cid:15) where (cid:15) is a numerically small value like 1 e ‚àí 8 . This allows us to evaluate whether the function is increasing or decreasing at a + b 2 by determining which of the two evaluations is larger . If the function is increasing at a + b 2 , the interval is narrowed to [ a , a + b 2 + (cid:15) ] . Otherwise , it is narrowed to [ a + b 2 , b ] . Ô£±Ô£¥Ô£≤ Ô£¥Ô£≥ set b = a + b 2 + (cid:15) , if increasing at a + b 2 ; set a = a + b 2 , otherwise . And again , the procedure is repeated until the range is small enough or the interval is reached with the required level of accuracy . 14 Gradient Descent , Stochastic Optimization , and Other Tales a c 1 c 2 b 0 J ( ) ( a ) Œ∑ = a yields the mini - mum . a c 1 c 2 b 0 J ( ) ( b ) Œ∑ = c 1 yields the min - imum . a c 1 c 2 b 0 J ( ) ( c ) Œ∑ = c 2 yields the min - imum . a c 1 c 2 b 0 J ( ) ( d ) Œ∑ = b yields the mini - mum . Figure 6 : Demonstration of 4 diÔ¨Äerent update ways in golden - section line search . s 0 J ( ) J ( ) = J ( 0 ) + J 0 ( 0 ) J ( ) = J ( 0 ) + J 0 ( 0 ) Figure 7 : Demonstration of Armijo rule in a convex setting . 2 . 2 Golden - Section Line Search Similar to the bisection line search , the golden - section line search also Ô¨Ånds the best learning rate Œ∑ when J ( Œ∑ ) is a unimodal function . Again , it starts with the interval [ a , b ] as [ 0 , Œ∑ max ] . However , instead of picking a midpoint , the golden - section search picks a pair of c 1 , c 2 such that a < c 1 < c 2 < b . The procedure follows : when Œ∑ = a yields the minimum value for J ( Œ∑ ) ( among the four values J ( a ) , J ( c 1 ) , J ( c 2 ) and , J ( b ) ) , we can exclude the interval ( c 1 , b ] ; when Œ∑ = c 1 yields the minimum value , we can exclude the interval ( c 2 , b ] ; when Œ∑ = c 2 yields the minimum value , we can exclude the interval [ a , c 1 ) ; and when Œ∑ = b yields the minimum value , we can exclude the interval [ a , c 2 ) . The four situations are shown in Figure 6 . In other words , at least one of the intervals [ a , c 1 ] and [ c 2 , b ] can be dropped in the golden - section search method . To conclude , we have when J ( a ) is the minimum , exclude ( c 1 , b ] ; when J ( c 1 ) is the minimum , exclude ( c 2 , b ] ; when J ( c 2 ) is the minimum , exclude [ a , c 1 ) ; when J ( b ) is the minimum , exclude [ a , c 2 ) . As long as we exclude one of the above four intervals , the new bound [ a , b ] can be set accordingly . The process is then repeated until the range is small enough . 15 Jun Lu 2 . 3 Armijo Rule Similar to Eq ( 2 . 1 ) , by Taylor‚Äôs formula again , we have J ( Œ∑ ) = L ( x t + Œ∑ t d t ) ‚âà L ( x t ) + Œ∑ d (cid:62) t ‚àá L ( x t ) . Since d (cid:62) t ‚àá L ( x t ) ‚â§ 0 ( by Remark 1 . 1 ) , it follows that L ( x t + Œ∑ d t ) ‚â§ L ( x t ) + Œ±Œ∑ ¬∑ d (cid:62) t ‚àá L ( x t ) , Œ± ‚àà ( 0 , 1 ) . ( 2 . 4 ) Let (cid:101) J ( Œ∑ ) = J ( 0 ) + J (cid:48) ( 0 ) ¬∑ Œ∑ 4 and (cid:98) J ( Œ∑ ) = J ( 0 ) + Œ±J (cid:48) ( 0 ) ¬∑ Œ∑ , the relationship of the two functions is shown in Figure 7 when J ( Œ∑ ) is a convex function ; and we note that (cid:98) J ( Œ∑ ) > (cid:101) J ( Œ∑ ) when Œ∑ > 0 . The Armijo rule says that an acceptable Œ∑ should satisfy J ( (cid:98) Œ∑ ) ‚â§ (cid:98) J ( (cid:98) Œ∑ ) to ensure suÔ¨Écient decrease and J ( (cid:98) Œ∑ / Œ≤ ) > (cid:98) J ( (cid:98) Œ∑ / Œ≤ ) to ensure the step size is not too small where Œ≤ ‚àà ( 0 , 1 ) such that the ( local ) optimal learning rate is in the range of [ (cid:98) Œ∑ , (cid:98) Œ∑ / Œ≤ ) . By Eq ( 2 . 4 ) , the two criteria above can also be described by : (cid:40) J ( (cid:98) Œ∑ ) ‚â§ (cid:98) J ( (cid:98) Œ∑ ) ; J ( (cid:98) Œ∑ / Œ≤ ) > (cid:98) J ( (cid:98) Œ∑ / Œ≤ ) , = ‚áí (cid:40) L ( x t + (cid:98) Œ∑ d t ) ‚àí L ( x t ) ‚â§ Œ± (cid:98) Œ∑ ¬∑ d (cid:62) t ‚àá L ( x t ) ; L ( x t + (cid:98) Œ∑ / Œ≤ d t ) ‚àí L ( x t ) > Œ± (cid:98) Œ∑ / Œ≤ ¬∑ d (cid:62) t ‚àá L ( x t ) . The full algorithm to calculate the learning rate at t - th iteration is formulated in Algo - rithm 1 . In practice , the parameters are set to be Œ≤ ‚àà [ 0 . 2 , 0 . 5 ] and Œ± ‚àà [ 1 e ‚àí 5 , 0 . 5 ] . Moreover , the Armijo rule is inexact , and it works even when J ( Œ∑ ) is not unimodal . After developing the Armijo algorithm , the core idea behind the Armijo rule can be found that the descent direction J (cid:48) ( 0 ) at that starting point Œ∑ = 0 often deteriorates in terms of rate of improvement since it moves further along this direction . However , a fraction Œ± ‚àà [ 1 e ‚àí 5 , 0 . 5 ] of this improvement is acceptable . By Eq ( 2 . 4 ) , the descent update at ( t + 1 ) - th iteration L ( x t + Œ∑ d t ) is at least Œ±Œ∑ ¬∑ d (cid:62) t ‚àá L ( x t ) smaller than that of t - th iteration . Algorithm 1 Armijo Rule at t - th Iteration Require : Start with Œ∑ t = s , 0 < Œ≤ < 1 , and 0 < Œ± < 1 ; 1 : isStop = False ; 2 : while isStop is False do 3 : if L ( x t + Œ∑ t d t ) ‚àí L ( x t ) ‚â§ Œ±Œ∑ t ¬∑ d (cid:62) t ‚àá L ( x t ) then 4 : isStop = True ; 5 : else 6 : Œ∑ t = Œ≤Œ∑ t ; 7 : end if 8 : end while 9 : Output Œ∑ t ; 4 . The tangent of J ( Œ∑ ) at Œ∑ = 0 . 16 Gradient Descent , Stochastic Optimization , and Other Tales 2 . 4 Quadratic Form in Steepest Descent Following the discussion of the quadratic form in GD ( Section 1 . 5 , p . 9 ) , we now discuss the quadratic form in GD with line search . By deÔ¨Ånition , we have J ( Œ∑ ) = L ( x t + Œ∑ d t ) = 1 2 ( x t + Œ∑ d t ) (cid:62) A ( x t + Œ∑ d t ) ‚àí b (cid:62) ( x t + Œ∑ d t ) + c = L ( x t ) + Œ∑ d (cid:62) t (cid:18) 1 2 ( A + A (cid:62) ) x t ‚àí b (cid:19) (cid:124) (cid:123)(cid:122) (cid:125) = ‚àá L ( x t ) = g t + 1 2 Œ∑ 2 d (cid:62) t Ad t , which is a quadratic function with respect to Œ∑ and there exists a closed form for the line search : Œ∑ t = ‚àí d (cid:62) t g t d (cid:62) t Ad t . ( 2 . 5 ) We observe that d (cid:62) t Ad t > 0 when d t (cid:54) = 0 . As aforementioned , when the search direction is the negative gradient d t = ‚àí g t , the method is known as the steepest descent . Then the descent update becomes Steepest Descent : x t + 1 = x t + Œ∑ t d t = x t ‚àí d (cid:62) t g t d (cid:62) t Ad t d t = x t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t , ( 2 . 6 ) where d t = ‚àí g t for the gradient descent case . A concrete example is presented in Figure 8 where A = (cid:20) 20 7 5 5 (cid:21) , b = 0 , and c = 0 . Suppose at t - th iteration , the parameter is at x t = [ ‚àí 3 , 3 . 5 ] (cid:62) . Figure 8 ( a ) presents the descent direction via the negative gradient . The line search is to choose the learning rate Œ∑ t by minimizing J ( Œ∑ ) = L ( x t + Œ∑ d t ) , and it is equivalent to choosing the point on the intersection of the vertical plane through the descent direction and the paraboloid deÔ¨Åned by the loss function L ( x ) as shown in Figure 8 ( b ) . Figure 8 ( c ) further shows the parabola deÔ¨Åned by the intersection of the two surfaces . Figure 8 ( d ) shows various gradients on the line through the descent direction where the gradient at the bottommost point is orthogonal to the gradient of the previous step ‚àá L ( x t + 1 ) (cid:62) d t as proved in Lemma 2 . 1 ( p . 13 ) where the black arrows are the gradients and the blue arrows are the projection of these gradients along d t = ‚àí‚àá L ( x t ) . An intuitive reason for this orthogonality at the minimum is : the slope of the parabola ( Figure 8 ( c ) ) at any point is equal to the magnitude of the projection of the gradients onto the search direction ( Figure 8 ( d ) ) ( Shewchuk et al . , 1994 ) . These projections represent the rate of increase of the loss function L ( x ) as the point traverses the search line ; and L ( x ) is minimized where the projection is zero and where the gradient is orthogonal to the search line . The example is run with 10 iterations in Figure 8 ( e ) , Figure 8 ( f ) , and Figure 8 ( g ) by vanilla GD with Œ∑ = 0 . 02 , vanilla GD with Œ∑ = 0 . 08 , and steepest descent respec - tively . We notice the tedious choices in vanilla GD ; and the zigzag path in the steepest GD with line search due to the orthogonality between each gradient and the previous gra - dient ( Lemma 2 . 1 , p . 13 ) . While this drawback will be partly solved in conjugate descent ( Section 5 . 4 , p . 54 ) . 17 Jun Lu 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 d t = L ( x t ) 8 32 56 80 104 128 152 176 200 224 L o ss V a l u e ( a ) Contour and the descent direction . The red dot is the optimal point . x 1 4 2 0 2 4 x 2 2 1 0 1 2 3 4 5 L ( x ) 0 50 100 150 200 250 ( b ) Intersection of the loss surface and vertical plane through the descent direction . 0 . 05 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 50 100 150 200 250 300 350 J ( ) ( c ) Intersection of the loss surface and vertical plane through the descent direction in 2 - dimensional space . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 8 32 56 80 104 128 152 176 200 224 L o ss V a l u e ( d ) Various gradients on the line through the descent direction where the gradient at the bottommost point is orthogonal to the gradient of the previous step . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( e ) Vanilla GD , Œ∑ = 0 . 02 . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( f ) Vanilla GD , Œ∑ = 0 . 08 . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( g ) Steepest descent . Figure 8 : Illustration for the line search of quadratic form with A = (cid:20) 20 7 5 5 (cid:21) , b = 0 , and c = 0 . The procedure is at x t = [ ‚àí 3 , 3 . 5 ] (cid:62) for t - th iteration . 18 Gradient Descent , Stochastic Optimization , and Other Tales 2 . 4 . 1 Special Case : Symmetric Quadratic Form To further discuss the convergence results of steepest descent , we discuss some special cases . Following Shewchuk et al . ( 1994 ) , we Ô¨Årst introduce some deÔ¨Ånitions . DeÔ¨Ånition 2 . 2 : Error and Residual Vector At iteration t , the error is deÔ¨Åned as e t = x t ‚àí x (cid:63) , a vector indicates how far the iterate is from the solution , where x (cid:63) = A ‚àí 1 b when A is symmetric and nonsingular . Substituting into Eq ( 2 . 6 ) , the update for the error vector is e t + 1 = e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t . ( 2 . 7 ) Furthermore , the residual r t = b ‚àí Ax t indicates how far the iterate is from the correct value of b . Note in this case , the residual is equal to the negative gradient and the descent direction , i . e . , r t = d t = ‚àí g t when A is symmetric ( we may use ‚àí g t and r t interchangeably when A is symmetric ) . We Ô¨Årst consider the case where the error vector e t at iteration t is an eigenvector with eigenvalue Œª t , i . e . , Ae t = Œª t e t . Then the gradient vector ( for symmetric A by Eq . ( 1 . 7 ) ) g t = Ax t ‚àí b = A Ô£´ Ô£≠ x t ‚àí A ‚àí 1 b (cid:124) (cid:123)(cid:122) (cid:125) x (cid:63) Ô£∂ Ô£∏ = Ae t = Œª t e t is also an eigenvector of A with eigenvalue being Œª t , i . e . , Ag t = Œª t g t . By Eq ( 2 . 7 ) , the update for ( t + 1 ) - th iteration is e t + 1 = e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t = e t ‚àí g (cid:62) t g t Œª t g (cid:62) t g t ( Œª t e t ) = 0 . Therefore , it takes only one step further to converge to the solution when e t is an eigenvector of A . A concrete example is shown in Figure 9 ( a ) where A = (cid:20) 20 5 5 5 (cid:21) , b = 0 , c = 0 . 2 . 4 . 2 Special Case : Symmetric with Orthogonal Eigenvectors When A is symmetric , it admits spectral decomposition ( Theorem 13 . 1 in Lu ( 2022c ) ) : A = Q Œõ Q (cid:62) ‚àà R d √ó d leads to ‚àí‚àí‚àí‚àí‚àí‚Üí A ‚àí 1 = Q Œõ ‚àí 1 Q (cid:62) , where the columns of Q = [ q 1 , q 2 , . . . , q d ] are eigenvectors of A and are mutually orthonor - mal , and the entries of Œõ = diag ( Œª 1 , Œª 2 , . . . , Œª d ) with Œª 1 ‚â• Œª 2 ‚â• . . . ‚â• Œª d being the 19 Jun Lu 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( a ) Steepest GD , Ae t = Œª t e t . 2 1 0 1 2 x 1 2 . 0 1 . 5 1 . 0 0 . 5 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 x 2 4 12 20 28 36 44 52 60 68 76 L o ss V a l u e ( b ) Steepest GD , Œª 1 = Œª 2 . Figure 9 : Illustration of special cases for GD with line search of quadratic form . A = (cid:20) 20 5 5 5 (cid:21) , b = 0 , c = 0 , and starting point to descent is x t = [ ‚àí 1 . 3 , 4 . 3 ] (cid:62) for Fig 9 ( a ) . A = (cid:20) 20 0 0 20 (cid:21) , b = 0 , c = 0 , and starting point to descent is x t = [ ‚àí 1 , 2 ] (cid:62) for Fig 9 ( b ) . corresponding eigenvalues of A , which are real . Since the eigenvectors are chosen to be mutually orthonormal : q (cid:62) i q j = (cid:40) 1 , i = j ; 0 , i (cid:54) = j , the eigenvectors also span the whole space R d such that every error vector e t ‚àà R d can be expressed as a combination of the eigenvectors : e t = d (cid:88) i = 1 Œ± i q i , ( 2 . 8 ) where Œ± i indicates the component of e t in the direction of q i . Then the gradient vector ( for symmetric A by Eq . ( 1 . 7 ) ) g t = Ax t ‚àí b = Ae t = A d (cid:88) i = 1 Œ± i q i = d (cid:88) i = 1 Œ± i Œª i q i , ( 2 . 9 ) i . e . , a combination of eigenvectors with length at i - th dimension being Œ± i Œª i . Again by Eq ( 2 . 7 ) , the update for ( t + 1 ) - th iteration is e t + 1 = e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t = e t ‚àí (cid:80) di = 1 Œ± 2 i Œª 2 i (cid:80) di = 1 Œ± 2 i Œª 3 i d (cid:88) i = 1 Œ± i Œª i q i = 0 . 20 Gradient Descent , Stochastic Optimization , and Other Tales The above equation tells us that when only one component of Œ± i ‚Äôs is nonzero , the con - vergence is achieved in only one step as shown in Figure 9 ( a ) . More specially , when Œª 1 = Œª 2 = . . . = Œª d = Œª , i . e . , all the eigenvalues are the same , it then follows that e t + 1 = e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t = e t ‚àí (cid:80) di = 1 Œ± 2 i (cid:80) di = 1 Œ± 2 i e t = 0 . Therefore , it takes only one step further to converge to the solution for arbitrary e t . A concrete example is shown in Figure 9 ( b ) where A = (cid:20) 20 0 0 20 (cid:21) , b = 0 , c = 0 . 2 . 4 . 3 General Convergence Analysis for Symmetric PD Quadratic To discuss the general convergence results , we further deÔ¨Åne the energy norm for error vector by | | e | | A = ( e (cid:62) Ae ) 1 / 2 . It can be shown that minimizing | | e t | | A is equivalent to minimizing L ( x t ) since | | e | | 2 A = 2 L ( x t ) ‚àí 2 L ( x (cid:63) ) ‚àí 2 b (cid:62) x (cid:63) (cid:124) (cid:123)(cid:122) (cid:125) constant . ( 2 . 10 ) By the deÔ¨Ånition of energy norm , Eq ( 2 . 7 ) , and symmetric positive deÔ¨Åniteness of A , we have the update on the energy norm sequence : | | e t + 1 | | 2 A = e (cid:62) t + 1 Ae t + 1 = (cid:18) e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t (cid:19) (cid:62) A (cid:18) e t ‚àí g (cid:62) t g t g (cid:62) t Ag t g t (cid:19) = | | e t | | 2 A + (cid:18) g (cid:62) t g t g (cid:62) t Ag t (cid:19) 2 g (cid:62) t Ag t ‚àí 2 g (cid:62) t g t g (cid:62) t Ag t g (cid:62) t Ae t = | | e t | | 2 A ‚àí ( g (cid:62) t g t ) 2 g (cid:62) t Ag t ( Ae t = g t ) = | | e t | | 2 A ¬∑ (cid:18) 1 ‚àí ( g (cid:62) t g t ) 2 g (cid:62) t Ag t ¬∑ e (cid:62) t Ae t (cid:19) = | | e t | | 2 A ¬∑ (cid:32) 1 ‚àí ( (cid:80) di = 1 Œ± 2 i Œª 2 i ) 2 ( (cid:80) di = 1 Œ± 2 i Œª 3 i ) ¬∑ ( (cid:80) di = 1 Œ± 2 i Œª i ) (cid:33) ( by Eq ( 2 . 8 ) , Eq ( 2 . 9 ) ) = | | e t | | 2 A ¬∑ r 2 , ( 2 . 11 ) where r 2 = (cid:18) 1 ‚àí ( (cid:80) di = 1 Œ± 2 i Œª 2 i ) 2 ( (cid:80) di = 1 Œ± 2 i Œª 3 i ) ¬∑ ( (cid:80) di = 1 Œ± 2 i Œª i ) (cid:19) determines the rate of convergence . As per con - vention , we assume Œª 1 ‚â• Œª 2 ‚â• . . . ‚â• Œª d > 0 , i . e . , the eigenvalues are ordered in magnitude and positive as A is positive deÔ¨Ånite . Then the condition number is deÔ¨Åned as Œ∫ = Œª 1 Œª d . Denote further Œ∫ i = Œª i / Œª d and œÉ i = Œ± i / Œ± 1 . It follows that r 2 = (cid:32) 1 ‚àí ( Œ∫ 2 + (cid:80) di = 2 œÉ 2 i Œ∫ 2 i ) 2 ( Œ∫ 3 + (cid:80) di = 2 œÉ 2 i Œ∫ 3 i ) ¬∑ ( Œ∫ + (cid:80) di = 2 œÉ 2 i Œ∫ i ) (cid:33) . 21 Jun Lu Figure 10 : Demonstration of the rate of convergence in steepest de - scent method with 2 - dimensional parameter . When œÉ 2 = 0 , ‚àû or Œ∫ = 1 , the rate of convergence is 0 , which makes the update con - verges instantly in one step . The two cases correspond to e t being an eigenvector of A and the eigen - values being identical respectively as examples shown in Figure 9 . 0 20 40 60 80 100 2 200 150 100 50 0 50 100 150 200 R a t e o f C o n v e r g e n c e 0 . 4 0 . 2 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Therefore , the rate of convergence is further controlled by Œ∫ , œÉ i ‚Äôs , and Œ∫ i ‚Äôs . We notice | Œ∫ i | ‚â• 1 for i ‚àà { 2 , 3 , . . . , d } . 2 - dimensional case SpeciÔ¨Åcally , when d = 2 , we have r 2 = 1 ‚àí ( Œ∫ 2 + œÉ 22 ) 2 ( Œ∫ 3 + œÉ 22 ) ¬∑ ( Œ∫ + œÉ 22 ) . ( 2 . 12 ) Figure 10 depicts the value r 2 as a function of Œ∫ and œÉ 2 . When d = 2 , from Eq ( 2 . 8 ) , we have e t = Œ± 1 q 1 + Œ± 2 q 2 . ( 2 . 13 ) This conÔ¨Årms the two special examples shown in Figure 9 : when e t is an eigenvector of A , it follows that : case 1 : Œ± 2 = 0 leads to ‚àí‚àí‚àí‚àí‚àí‚Üí œÉ 2 = Œ± 2 / Œ± 1 ‚Üí 0 ; case 2 : Œ± 1 = 0 leads to ‚àí‚àí‚àí‚àí‚àí‚Üí œÉ 2 = Œ± 2 / Œ± 1 ‚Üí ‚àû , i . e . , the slope of œÉ 2 is either zero or inÔ¨Ånite , the rate of convergence goes to zero and it converges instantly in just one step ( example in Figure 9 ( a ) ) . While if the eigenvalues are identical , Œ∫ = 1 , once again , the rate of convergence is zero ( example in Figure 9 ( b ) ) . Worst case We recall that œÉ 2 = Œ± 2 / Œ± 1 decides the error vector e t ( Eq ( 2 . 8 ) or Eq ( 2 . 13 ) ) which in turn decides the point x t in the 2 - dimensional case . It is then interesting to see the worst point to descent . Holding Œ∫ Ô¨Åxed ( i . e . , A and the loss function L ( x ) are Ô¨Åxed ) , suppose further t = 1 , we want to see the worst starting point x 1 to descent . It can be shown that the rate of convergence in Eq ( 2 . 12 ) is maximized when œÉ 2 = ¬± Œ∫ : r 2 ‚â§ 1 ‚àí 4 Œ∫ 2 Œ∫ 5 + 2 Œ∫ 4 + Œ∫ 3 = ( Œ∫ ‚àí 1 ) 2 ( Œ∫ + 1 ) 2 . 22 Gradient Descent , Stochastic Optimization , and Other Tales 0 20 40 60 80 100 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R a t e o f C o n v e r g e n c e Figure 11 : Upper bound of the rate of con - vergence ( per iteration ) in steepest descent method with 2 - dimensional parameter . The y - axis is Œ∫ ‚àí 1 Œ∫ + 1 . Substitute into Eq ( 2 . 11 ) , we have | | e t + 1 | | 2 A ‚â§ | | e t | | 2 A ¬∑ ( Œ∫ ‚àí 1 ) 2 ( Œ∫ + 1 ) 2 , leads to ‚àí‚àí‚àí‚àí‚àí‚Üí | | e t + 1 | | A ‚â§ | | e 1 | | A ¬∑ (cid:18) Œ∫ ‚àí 1 Œ∫ + 1 (cid:19) t . The upper bound of the rate of convergence ( per iteration ) is plotted in Figure 11 . Again , the more ill - conditioned the matrix , the slower the convergence of steepest descent . We may notice that the ( upper bound of the ) rate of convergence is the same as that of the vanilla GD in Eq ( 1 . 13 ) . However , the two are diÔ¨Äerent in that the rate of the vanilla GD is described in terms of the y t vector in Eq ( 1 . 9 ) ; while the rate of steepest descent is presented in terms of the energy norm . Moreover , the rate of vanilla GD in Eq ( 1 . 13 ) is obtained by selecting a speciÔ¨Åc learning rate as shown in Eq ( 1 . 12 ) which is not practical in vanilla GD since the learning rate is Ô¨Åxed at all iterations . This makes the rate of vanilla GD rather a tight bound . In practice , vanilla GD converges slower than steepest descent as the examples shown in Figure 8 ( e ) , Figure 8 ( f ) , and Figure 8 ( g ) . 3 . Learning Rate Annealing and Warmup We have discussed in Eq ( 1 . 3 ) that the learning rate Œ∑ controls how large of a step to take in the direction of negative gradient so that we can reach a ( local ) minimum . In a wide range of applications , a Ô¨Åxed learning rate works well in practice . While there are other learning rate schedules that change the learning rate during learning and it is most often changed between epochs . We shall see in the sequel that per - dimension optimizers can change the learning rate in each dimension adaptively , e . g . , AdaGrad , AdaDelta , RMSProp , and AdaSmooth ( Duchi et al . , 2011 ; Hinton et al . , 2012b ; Zeiler , 2012 ; Lu , 2022a ) ; while in this section , we discuss how to decay or anneal the global learning rate , i . e . , the Œ∑ in Eq ( 1 . 3 ) . 23 Jun Lu A constant learning rate often poses a dilemma to the analyst : a small learning rate used will cause the algorithm to take too long to reach anywhere close to an optimal solution . While a large initial learning rate will allow the algorithm to come reasonably close to a good ( local ) minimum in the cost surface at Ô¨Årst ; however , the algorithm will then oscillate back and forth around the point for a very long time . One method to prevent this challenge is to slow down the parameter updates by decreasing the learning rate . This can be done manually when the validation accuracy appears to plateau . On the other hand , decaying the learning rate over time based on how many epochs through the data have been done can naturally avoid these issues . The most common decay functions are step decay , inverse decay , and exponential decay . In the next section , we shall discuss the mathematical formulas for various learning rate annealing schemes . 3 . 1 Learning Rate Annealing Step decay Step decay scheduler drops the learning rate by a factor every epoch or every few epochs . Given the iteration t , number of iterations to drop n , initial learning rate Œ∑ 0 , and decay factor d < 1 , the form of step decay is given by Œ∑ t = Œ∑ 0 ¬∑ d (cid:98) tn (cid:99) = Œ∑ 0 ¬∑ d s , where s = (cid:98) tn (cid:99) is called the step stage to decay . Therefore , the step decay policy decays the learning rate every n iterations . Multi - step decay Multi - step decay scheduler is a slightly diÔ¨Äerent version of the step decay in that the step stage is the index where the iteration t falls in the milestone vector m = [ m 1 , m 2 , . . . , m k ] (cid:62) with 0 ‚â§ m 1 ‚â§ m 2 ‚â§ . . . ‚â§ m k ‚â§ T and T being the total number of iterations ( or epochs ) 5 . To be more concrete , the step stage s at iteration t is obtained by s = Ô£±Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£≥ 0 , t < m 1 ; 1 , m 1 ‚â§ t < m 2 ; . . . k , m k ‚â§ t ‚â§ T . As a result , given the iteration t , initial learning rate Œ∑ 0 , and decay factor d < 1 , the learning rate at iteration t is obtained by Œ∑ t = Œ∑ 0 ¬∑ d s . Exponential decay Given the iteration t , the initial learning rate Œ∑ 0 , and the exponential decay factor k , the form of the exponential decay is given by Œ∑ t = Œ∑ 0 ¬∑ exp ( ‚àí k ¬∑ t ) , where the parameter k controls the rate of the decay . 5 . When T is the total number of iterations , it can be obtained by the number of epochs times the number of steps per epoch . 24 Gradient Descent , Stochastic Optimization , and Other Tales 0 20 40 60 80 100 Epoch or Step 0 . 00 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 0 . 12 L e a r n i n g R a t e ( 20 , 0 . 05 ) ( 50 , 0 . 025 ) ( 70 , 0 . 0125 ) ( 80 , 0 . 00625 ) Learning Rate Annealing Step : 0 = 0 . 1 , n = 10 , d = 0 . 5 Multi - Step : 0 = 0 . 1 , d = 0 . 5 , m = [ 20 , 50 , 70 , 80 ] Exponential : 0 = 0 . 1 , k = 0 . 1 Inverse : 0 = 0 . 1 , k = 0 . 1 Inverse Square Root : 0 = 0 . 1 , w = 20 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 100 , p = 2 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 50 , p = 2 Figure 12 : Demonstration of step decay , multi - step decay , an - nealing polynomial , inverse de - cay , inverse square root , and ex - ponential decay schedulers . One may Ô¨Ånd the exponential decay is the smoothest among the six , while multi - step decay is the least smooth one . Inverse decay The inverse decay scheduler is a slightly diÔ¨Äerent version of exponential decay in that the decaying eÔ¨Äect is applied by the inverse function . Given the iteration number t , the initial learning rate Œ∑ 0 , and the decay factor k , the form of the inverse decay is obtained by Œ∑ t = Œ∑ 0 1 + k ¬∑ t , where , again , the parameter k controls the rate of the decay . Inverse square root The inverse square root scheduler is a learning rate schedule Œ∑ t = Œ∑ 0 ¬∑ ‚àö w ¬∑ 1 (cid:112) max ( t , w ) , where t is the current training iteration and w is the number of warm - up steps , and Œ∑ 0 is the initial learning rate . This sets a constant learning rate for the Ô¨Årst steps , then exponentially decays the learning rate until pre - training is over . Annealing polynomial decay Given the iteration t , max decay iteration M , power factor p , initial learning rate Œ∑ 0 , and Ô¨Ånal learning rate Œ∑ T , the annealing polynomial decay at iteration t can be obtained by decay batch = min ( t , M ) ; Œ∑ t = ( Œ∑ 0 ‚àí Œ∑ T ) ¬∑ (cid:18) 1 ‚àí t decay batch (cid:19) p + Œ∑ T . ( 3 . 1 ) In practice , the default values for the parameters are : initial rate Œ∑ 0 = 0 . 001 , end rate Œ∑ T = 1 e ‚àí 10 , the warm up steps M = T / 2 where T is the maximal iteration number , and power rate p = 2 . Figure 12 compares step decay , multi - step decay , annealing polynomial decay , inverse decay , inverse square root , and exponential decay with a set of parameters . One may Ô¨Ånd the exponential decay is the smoothest among the six , while multi - step decay is the least smooth one . In the annealing polynomial decay , the max decay iteration M controls how fast the decay is . 25 Jun Lu ‚Ä¢ When M is small , the decay gets closer to that of the exponential scheduler or step decay ; however , the exponential decay has a longer tail . That is , the exponential scheduler decays slightly faster in the beginning iterations , while it decays slower in the last few iterations . ‚Ä¢ When M is large , the decay gets closer to that of multi - step decay ; however , the multi - step scheduler behaves much more aggressively . 0 20 40 60 80 100 Epoch 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 T r a i n L o ss Training Performance Fixed : = 0 . 01 Step : 0 = 0 . 1 , n = 10 , d = 0 . 5 Multi - Step : 0 = 0 . 1 , d = 0 . 5 , m = [ 20 , 50 , 70 , 80 ] Exponential : 0 = 0 . 1 , k = 0 . 1 Inverse : 0 = 0 . 1 , k = 0 . 1 Inverse Square Root : 0 = 0 . 1 , w = 20 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 100 , p = 2 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 50 , p = 2 ( a ) Training loss . 0 20 40 60 80 100 Epoch 97 . 5 98 . 0 98 . 5 99 . 0 99 . 5 100 . 0 T r a i n A cc u r a c y ( % ) Training Performance Fixed : = 0 . 01 Step : 0 = 0 . 1 , n = 10 , d = 0 . 5 Multi - Step : 0 = 0 . 1 , d = 0 . 5 , m = [ 20 , 50 , 70 , 80 ] Exponential : 0 = 0 . 1 , k = 0 . 1 Inverse : 0 = 0 . 1 , k = 0 . 1 Inverse Square Root : 0 = 0 . 1 , w = 20 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 100 , p = 2 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 50 , p = 2 ( b ) Training accuracy . 0 20 40 60 80 100 Epoch 0 . 30 0 . 35 0 . 40 0 . 45 0 . 50 T e s t L o ss Testing Performance Fixed : = 0 . 01 Step : 0 = 0 . 1 , n = 10 , d = 0 . 5 Multi - Step : 0 = 0 . 1 , d = 0 . 5 , m = [ 20 , 50 , 70 , 80 ] Exponential : 0 = 0 . 1 , k = 0 . 1 Inverse : 0 = 0 . 1 , k = 0 . 1 Inverse Square Root : 0 = 0 . 1 , w = 20 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 100 , p = 2 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 50 , p = 2 ( c ) Test loss . 0 20 40 60 80 100 Epoch 97 . 0 97 . 2 97 . 4 97 . 6 97 . 8 98 . 0 98 . 2 T e s t A cc u r a c y ( % ) Testing Performance Fixed : = 0 . 01 Step : 0 = 0 . 1 , n = 10 , d = 0 . 5 Multi - Step : 0 = 0 . 1 , d = 0 . 5 , m = [ 20 , 50 , 70 , 80 ] Exponential : 0 = 0 . 1 , k = 0 . 1 Inverse : 0 = 0 . 1 , k = 0 . 1 Inverse Square Root : 0 = 0 . 1 , w = 20 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 100 , p = 2 Anneal . Polynomial : 0 = 0 . 1 , T = 10 8 , M = 50 , p = 2 ( d ) Test accuracy . Figure 13 : Training and test performance with diÔ¨Äerent learning rate schemes . Toy example To see the eÔ¨Äect of diÔ¨Äerent schedulers , we employ a toy example with multi - layer perceptron ( MLP ) training on MNIST digit classiÔ¨Åcation set ( LeCun , 1998 ) 6 . Figure 13 shows the training and test performance in terms of negative log likelihood loss . The parameters for various schedulers are shown in Figure 12 ( for 100 epochs ) . We observe that the stochastic gradient descent method with Ô¨Åxed learning rate may continue 6 . It has a training set of 60 , 000 examples , and a test set of 10 , 000 examples . 26 Gradient Descent , Stochastic Optimization , and Other Tales to decrease the test loss . However , its test accuracy may get stuck at some point . The toy example shows learning rate annealing schemes in general can help optimization methods ‚ÄúÔ¨Ånd‚Äù better local minima with better performance . 3 . 2 Learning Rate Warmup This warmup idea in training neural networks receive attention in recent years ( He et al . , 2016 ; Goyal et al . , 2017 ; Smith and Topin , 2019 ) . See also discussion in Popel and Bojar ( 2018 ) for a deep understanding of why the warmup scheduler works well in neural machine translation ( NML ) . The learning rate annealing schedulers can be utilized in both epoch - and step - basis . However , the learning rate warmup schemes are usually applied in the step context where the total number of steps is the number of epochs times the number of steps per epoch as aforementioned ( Vaswani et al . , 2017 ; Howard and Ruder , 2018 ) . Note that with this scheduler , early stopping should typically be avoided . In the rest of this section , we discuss two commonly used warmup policies , namely , the slanted triangular learning rates ( STLR ) and the Noam methods . Slanted Triangular Learning Rates ( STLR ) STLR is a learning rate schedule that Ô¨Årst linearly increases the learning rate over some number of epochs and then linearly decays it over the remaining epochs . The rate at iteration t is computed by cut = (cid:100) T ¬∑ frac (cid:101) ; p = Ô£±Ô£≤ Ô£≥ t / cut , if t < cut ; 1 ‚àí t ‚àí cut cut ¬∑ ( 1 / frac ‚àí 1 ) , otherwise ; Œ∑ t = Œ∑ max ¬∑ 1 + p ¬∑ ( ratio ‚àí 1 ) ratio , where T is the number of training iterations ( the number of epochs times the number of updates per epoch ) , frac is the fraction of iterations we want to increase the learning rate , cut is the iteration when we switch from increasing to decreasing the learning rate , p is the fraction of the number of iterations we have increased or decreased the learning rate respectively , ratio speciÔ¨Åes how much smaller the lowest learning rate is from the maximum learning rate Œ∑ max . In practice , the default values are frac = 0 . 1 , ratio = 32 and Œ∑ max = 0 . 01 ( Howard and Ruder , 2018 ) . Noam The Noam scheduler is originally used in neural machine translation ( NML ) tasks and is proposed in Vaswani et al . ( 2017 ) . This corresponds to increasing the learning rate linearly for the Ô¨Årst ‚Äúwarmup steps‚Äù training steps and decreasing it thereafter proportion - ally to the inverse square root of the step number , scaled by the inverse square root of the dimensionality of the model ( linear warmup for a given number of steps followed by exponential decay ) . Given the warmup steps w and the model size d model ( the hidden size parameter which dominates the number of parameters in the model ) , the learning rate Œ∑ t at step t can be calculated by Œ∑ t = Œ± ¬∑ 1 ‚àö d model ¬∑ min (cid:18) 1 ‚àö t , t w 3 / 2 (cid:19) , 27 Jun Lu Figure 14 : Comparison of Noam and STLR schedulers . 0 2500 5000 7500 10000 12500 15000 17500 20000 Step 0 . 0000 0 . 0001 0 . 0002 0 . 0003 0 . 0004 0 . 0005 0 . 0006 0 . 0007 0 . 0008 L e a r n i n g R a t e Noam and STLR Noam : d model = 512 , w = 4000 Noam : d model = 512 , w = 8000 Noam : d model = 2048 , w = 2000 STLR : frac = 0 . 1 , ratio = 32 where Œ± is a smoothing factor . In the original paper , the warmup step w is set to w = 4000 . While in practice , w = 25000 can be a good choice . Moreover , in rare cases , the model size is set to be the same as the warmup steps which is known as the warmup Noam scheduler : Œ∑ t = Œ± ¬∑ 1 ‚àö w ¬∑ min (cid:18) 1 ‚àö t , t w 3 / 2 (cid:19) . Figure 14 compares STLR and Noam schedulers with various parameters . We may observe that , in general , the Noam scheduler decays slower when the warmup Ô¨Ånishes than the STLR . 3 . 3 Cyclical Learning Rate ( CLR ) Policy The cyclical learning rate is a kind of generalization of warmup + decay ( Noam scheme or STLR policy does just one cycle ) . The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative eÔ¨Äect and yet achieves a long term beneÔ¨Åcial eÔ¨Äect . This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise Ô¨Åxed or exponentially decreasing value where minimum and maximum boundaries are set to make the learning rate vary between them . The simplest function to adopt this idea is the triangular window function that linearly increases and then linearly decreases ( Smith , 2017 ) . Dauphin et al . ( 2014 , 2015 ) argue that the diÔ¨Éculty in minimizing the loss arises from saddle points ( toy example in Figure 3 ( d ) , p . 10 ) rather than poor local minima . Saddle points have small gradients that slow the pace of the learning process . However , increasing the learning rate allows more rapid traversal of saddle point plateaus . In this scenario , a cyclical learning rate policy with periodical increasing and decreasing of the learning rate between minimum and maximum boundaries is reasonable . The minimum and maximum boundaries are problem - speciÔ¨Åc . Usually one runs the model for several epochs for diÔ¨Äerent learning rates between low and high learning rate values . This is known as the learning rate range test . In this case , plot the accuracy versus learning rate ; when the accuracy starts to increase and when the accuracy slows , becomes ragged , or starts to fall , the two of which constitute good choices for the minimum and maximum boundaries . 28 Gradient Descent , Stochastic Optimization , and Other Tales 0 2500 5000 7500 10000 12500 15000 17500 20000 Step 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 0 . 12 0 . 14 L e a r n i n g R a t e Triangular , Triangular2 , and Exp Range Triangular : 0 = 0 . 001 , max = 0 . 1 , s = 2000 Triangular2 : 0 = 0 . 001 , max = 0 . 1 , s = 2000 Exp _ Range : 0 = 0 . 001 , max = 0 . 1 , s = 2000 , = 0 . 99994 Figure 15 : Demonstration of triangular , triangular2 , and exp range schedulers . The cyclical learning rate policies can be divided into two categories : the one based on iteration , and the one based on epoch . The former one does the annealing and warmup at each iteration and the latter one does this on an epoch - basis 7 . However , there is no big diÔ¨Äerence between the two ; any policy can be applied in either one of the two fashions . In the next paragraphs , we will discuss the update policies based on their original proposals . Triangular , Triangular2 , and Exp Range The triangular policy linearly increases the learning rate and then linearly decreases it . Given the initial learning rate Œ∑ 0 ( the lower boundary in the cycle ) , the max learning rate Œ∑ max , the step size s ( number of training iterations per half cycle ) , the learning rate Œ∑ t at iteration t can be obtained by : triangular : Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥ cycle = (cid:98) 1 + t 2 s (cid:99) ; x = abs (cid:18) t s ‚àí 2 √ó cycle + 1 (cid:19) ; Œ∑ t = Œ∑ 0 + ( Œ∑ max ‚àí Œ∑ 0 ) ¬∑ max ( 0 , 1 ‚àí x ) , where the calculated cycle records in which cycle the iteration t is . The same as the triangular policy , the triangular2 policy cuts in half at the end of each cycle : triangular 2 : Œ∑ t = Œ∑ 0 + ( Œ∑ max ‚àí Œ∑ 0 ) ¬∑ max ( 0 , 1 ‚àí x ) ¬∑ 1 2 cycle ‚àí 1 . Less aggressive than the triangular2 policy , the amplitude of a cycle in exp range policy is scaled exponentially based on Œ≥ t where Œ≥ < 1 is the scaling constant : exp range : Œ∑ t = Œ∑ 0 + ( Œ∑ max ‚àí Œ∑ 0 ) ¬∑ max ( 0 , 1 ‚àí x ) ¬∑ Œ≥ t . A comparison of the three policies is shown in Figure 15 . In practice , the step size s usually is set to 2 ‚àº 10 times the number of iterations in an epoch ( Smith , 2017 ) . 7 . The total number of iterations equals the number of epochs times the number of updates per epoch . 29 Jun Lu Cyclical cosine Cyclical cosine is a type of learning rate schedule that has the eÔ¨Äect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again . The resetting of the learning rate acts as a simulated restart of the learning process and the re - use of good weights as the starting point of the restart is referred to as a ‚Äúwarm restart‚Äù in contrast to a ‚Äúcold restart‚Äù where a new set of small random numbers may be used as a starting point ( Loshchilov and Hutter , 2016 ; Huang et al . , 2017 ) . The learning rate Œ∑ t at iteration t is calculated as follows : Œ∑ t = Œ∑ 0 2 (cid:18) cos (cid:18) œÄ mod ( t ‚àí 1 , (cid:100) T / M (cid:101) ) (cid:100) T / M (cid:101) (cid:19) + 1 (cid:19) , where T is the total number of training iterations ( note the original paper takes the iterations as epochs in this sense ( Loshchilov and Hutter , 2016 ) ) , M is the number of cycles , and Œ∑ 0 is the initial learning rate . The scheduler anneals the learning rate from its initial value Œ∑ 0 to a small learning rate approaching 0 over the course of a cycle . That is , we split the training process into M cycles as shown in Figure 16 ( a ) , each of which starts with a large learning rate Œ∑ 0 and then gets annealed to a small learning rate . The above equation can lower the learning rate at a very fast pace , encouraging the model to converge towards its Ô¨Årst local minimum after a few epochs . The optimization then continues at a larger learning rate that can perturb the model and dislodge it from the minimum 8 . The procedure is then repeated several times to obtain multiple convergences . In practice , the iteration t usually refers to the t - th epoch . More generally , any learning rate with general function f in the following form can have a similar eÔ¨Äect : Œ∑ t = f ( mod ( t ‚àí 1 , (cid:100) T / M (cid:101) ) ) . Moreover , the learning rate can be set for each batch instead of prior to each epoch to give more nuance to the updates ( Huang et al . , 2017 ) . 0 20 40 60 80 100 Epoch 0 . 000 0 . 002 0 . 004 0 . 006 0 . 008 0 . 010 0 . 012 L e a r n i n g R a t e Cyclical Cosine Cyclical Cosine : 0 = 0 . 01 , M = 5 , T = 100 ( a ) Cyclical Cosine . 0 20 40 60 80 100 Epoch 0 . 000 0 . 002 0 . 004 0 . 006 0 . 008 0 . 010 0 . 012 L e a r n i n g R a t e Cyclical Step Cyclical Step : min = 0 . 0005 , max = 0 . 01 , M = 20 ( b ) Cyclical step . Figure 16 : Cyclical cosine and cyclical step learning rate policies . 8 . The goal of the procedure is similar to the perturbed SGD that can help escape from saddle points ( Jin et al . , 2017 ; Du et al . , 2017 ) . 30 Gradient Descent , Stochastic Optimization , and Other Tales 0 20 40 60 80 100 Epoch or Step 0 . 00 0 . 02 0 . 04 0 . 06 0 . 08 0 . 10 L e a r n i n g R a t e Cyclical Polynomial Cyclical Polynomial : 0 = 0 . 1 , T = 0 , M = 100 , p = 2 Cyclical Polynomial : 0 = 0 . 1 , T = 0 , M = 80 , p = 2 Cyclical Polynomial : 0 = 0 . 1 , T = 0 , M = 55 , p = 2 Cyclical Polynomial : 0 = 0 . 1 , T = 0 , M = 30 , p = 2 Cyclical Polynomial : 0 = 0 . 1 , T = 0 , M = 10 , p = 2 Figure 17 : Demonstration of cyclical poly - nomial scheduler with various parameters . Cyclical step Similar to the cyclical cosine scheme , the cyclical step learning rate policy combines a linear learning rate decay with warm restarts ( Mehta et al . , 2019 ) : Œ∑ t = Œ∑ max ‚àí ( t mod M ) ¬∑ Œ∑ min . where in the original paper , t refers to the epoch count , Œ∑ min and Œ∑ max are the ranges for the learning rate , and M is the cycle length after which the learning rate will restart . The learning rate scheme can be seen as a variant of the cosine learning policy as discussed above and the comparison between the two policies is shown in Figure 16 . In practice , Œ∑ min = 0 . 1 , Œ∑ max = 0 . 5 , and M = 5 are set as default values in the original paper . Cyclical polynomial The cyclical polynomial is a variant of the annealing polynomial decay ( Eq ( 3 . 1 ) ) scheme where the diÔ¨Äerence is that the cyclical polynomial scheme employs a cyclical warmup similar to the exp range policy . Given the iteration number t , initial learning rate Œ∑ 0 , Ô¨Ånal learning rate , Œ∑ T , and maximal decay number M < T , the rate can be calculated by : decay batch = M ¬∑ (cid:100) t M (cid:101) Œ∑ t = ( Œ∑ 0 ‚àí Œ∑ T ) ¬∑ (cid:18) 1 ‚àí t decay batch + (cid:15) (cid:19) p + Œ∑ T , where (cid:15) = 1 e ‚àí 10 is applied for better condition when t = 0 . Figure 17 presents cyclical polynomial scheme with various parameters . 4 . Stochastic Optimizer Over the years , stochastic gradient - based optimization has become a core method in many Ô¨Åelds of science and engineering such as computer vision and automatic speech recognition processing ( Krizhevsky et al . , 2012 ; Hinton et al . , 2012a ; Graves et al . , 2013 ) . Stochas - tic gradient descent ( SGD ) and deep neural network ( DNN ) play a core role in training stochastic objective functions . When a new deep neural network is developed for a given task , some hyper - parameters related to the training of the network must be chosen heuris - tically . For each possible combination of structural hyper - parameters , a new network is typically trained from scratch and evaluated over and over again . While much progress has been made on hardware ( e . g . , Graphical Processing Units ) and software ( e . g . , cuDNN ) 31 Jun Lu Method Year Papers Method Year Papers Adam 2014 7532 AdamW 2017 45 SGD 1951 1212 Local SGD 2018 41 RMSProp 2013 293 Gravity 2021 37 Adafactor 2018 177 AMSGrad 2019 35 Momentum 1999 130 LARS 2017 31 LAMB 2019 126 MAS 2020 26 AdaGrad 2011 103 DFA 2016 23 Deep Ensembles 2016 69 Nesterov momentum 1983 22 FA 2014 46 Gradient SparsiÔ¨Åcation 2017 20 Table 1 : Data retrieved on April 27th , 2022 via https : / / paperswithcode . com / . to speed up the training time of a single structure of a DNN , the exploration of a large set of possible structures remains very slow making the need of a stochastic optimizer that is insensitive to hyper - parameters . EÔ¨Écient stochastic optimizers thus play a core role in training deep neural networks . There are several variants of SGD to use heuristics for estimating a good learning rate at each iteration of the progress . These methods either attempt to accelerate learning when suitable or to slow down learning near a local minimum . In this section , we introduce a few stochastic optimizers that are in the two categories . Table 1 shows the number of papers that uses the optimizers to do the tasks and the date of publication . For alternative reviews , one can also check Zeiler ( 2012 ) , Ruder ( 2016 ) , Goodfellow et al . ( 2016 ) , and many others . 4 . 1 Momentum If the cost surface is not spherical , learning can be quite slow because the learning rate must be kept small to prevent divergence along the steep curvature directions ( Polyak , 1964 ; Rumelhart et al . , 1986 ; Qian , 1999 ; Sutskever et al . , 2013 ) . The SGD with momentum ( that can be applied to full batch or mini - batch learning ) attempts to use the previous step to speed up learning when suitable such that it enjoys better converge rates on deep networks . The main idea behind the momentum method is to speed up the learning along dimensions where the gradient consistently points in the same direction ; and to slow the pace along dimensions in which the sign of the gradient continues to change . Figure 19 ( a ) shows a set of updates for vanilla GD where we can Ô¨Ånd the update along dimension x 1 is consistent ; and the move along dimension x 2 continues to change in a zigzag pattern . The GD with momentum keeps track of past parameter updates with an exponential decay , and the update method has the following step : ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ ‚àÇL ( x t ) ‚àÇ x t , ( 4 . 1 ) where the algorithm remembers the latest update and adds it to the present update by multiplying a parameter œÅ called momentum parameter . That is , the amount we change the parameter is proportional to the negative gradient plus the previous weight change ; the added momentum term acts as both a smoother and an accelerator . The momentum 32 Gradient Descent , Stochastic Optimization , and Other Tales parameter œÅ works as a decay constant where ‚àÜ x 1 may have an eÔ¨Äect on ‚àÜ x 100 ; however , its eÔ¨Äect is decayed by this decay constant . In practice , the momentum parameter œÅ is usually set to be 0 . 9 by default . Momentum simulates the concept of inertia in physics . It means that in each iteration , the update mechanism is not only related to the gradient descent , which refers to the dynamic term , but also maintains a component that is related to the direction of the last update iteration , which refers to the momentum . x 1 2 0 2 4 6 8 x 2 2 1 0 1 2 3 4 5 6 L ( x ) 0 50 100 150 200 250 300 350 400 ( a ) A 2 - dimensional surface plot for quadratic con - vex function . 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 25 75 125 175 225 275 325 L o ss V a l u e ( b ) The contour plot of L ( x ) . The red dot is the optimal point . Figure 18 : Figure 1 ( a ) shows a function surface and a contour plot ( blue = low , yel - low = high ) where the upper graph is the surface , and the lower one is the projection of it ( i . e . , contour ) . The quadratic function is with parameters A = (cid:20) 4 0 0 40 (cid:21) , b = [ 12 , 80 ] (cid:62) , and c = 103 . Or equivalently , L ( x ) = 2 ( x 1 ‚àí 3 ) 2 + 20 ( x 2 ‚àí 2 ) 2 + 5 and ‚àÇL ( x ) ‚àÇ x = [ 4 x 1 ‚àí 12 , 8 x 2 ‚àí 16 ] (cid:62) . The momentum works extremely better in a ravine - shaped loss curve . Ravine is an area , where the surface curves are much steeper in one dimension than in another ( see the surface and contour curve in Figure 18 , i . e . , a long narrow valley ) . Ravines are common near local minima in deep neural networks and vanilla GD or SGD has trouble navigating them . As shown by the toy example in Figure 19 ( a ) , GD tends to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum . Momentum helps accelerate gradients in the correct direction and dampens oscillations as can be seen in the example of Figure 19 ( b ) . As aforementioned , it does this by adding a fraction œÅ of the update vector of the past time step to the current update vector . When ‚àÜ x t and ‚àÜ x t ‚àí 1 are in the same direction , the momentum accelerates the update step ( e . g . , the blue arrow areas in Figure 19 ( b ) ) ; while they are in the opposite directions , the algorithm tends to update in the former direction if x has been updated in this direction for many iterations . To be more concrete , look at the blue starting point , and then look at the cyan point we get to after one step in the step of the update without Momentum ( Figure 19 ( a ) ) , they have gradients that are pretty much equal and opposite . As a result , the gradient across the ravine has been canceled out . But the gradient along the ravine has not canceled out . Along the ravine , we‚Äôre going to keep building up speed , and so , after the momentum method has settled down , it‚Äôll tend to go along the bottom of the ravine . 33 Jun Lu 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 25 75 125 175 225 275 325 375 L o ss V a l u e ( a ) Optimization without momentum . A higher learning rate may result in larger parameter up - dates in the dimension across the valley ( direc - tion of x 2 ) which could lead to oscillations back and forth across the valley . 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 x t , x t 1 same direction 25 75 125 175 225 275 325 375 L o ss V a l u e ( b ) Optimization with momentum . Though the gradients along the valley ( direction of x 1 ) are much smaller than the gradients across the val - ley ( direction of x 2 ) , they are typically in the same direction and thus the momentum term accumulates to speed up movement , dampen os - cillations and cause us to barrel through narrow valleys , small humps and ( local ) minima . Figure 19 : The starting point is [ ‚àí 2 , 5 ] (cid:62) . After 5 iterations , the squared loss from vanilla GD is 42 . 72 , and the loss from GD with momentum is 35 . 41 in this simple case . The learning rates Œ∑ are set to be 0 . 04 in both cases . From this Ô¨Ågure , the problem with the vanilla GD is that the gradient is big in the direction in which we only want to travel a small distance ; and the gradient is small in the direction in which we want to travel a large distance . However , one can easily Ô¨Ånd that the momentum term helps average out the oscillation along the short axis while at the same time adds up contributions along the long axis . In other words , although it starts oÔ¨Ä by following the gradient , however , when it has velocity , it no longer does steepest descent . We call this momentum , which makes it keep going in the previous direction . 4 . 1 . 1 Quadratic Form in Momentum Following the discussion of quadratic form in GD ( Section 1 . 5 , p . 9 ) and steepest descent ( Section 2 . 4 , p . 17 ) , we now discuss the quadratic form in GD with momentum . The update is : ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ ‚àá L ( x t ) ; x t + 1 = x t + ‚àÜ x t , where ‚àá L ( x t ) = Ax t ‚àí b if A is symmetric for the quadratic form . The update becomes ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ ( Ax t ‚àí b ) ; x t + 1 = x t + ‚àÜ x t . Again deÔ¨Åne the following iterate vectors (cid:40) y t = Q (cid:62) ( x t ‚àí x (cid:63) ) ; z t = Q (cid:62) ‚àÜ x t , 34 Gradient Descent , Stochastic Optimization , and Other Tales where x (cid:63) = A ‚àí 1 b if we further assume A is nonsingular and PD as aforementioned , A = Q Œõ Q (cid:62) is the spectral decomposition of matrix A . The construction yields the update rule : z t = œÅ z t ‚àí 1 ‚àí Œ∑ Œõ y t ; y t + 1 = y t + z t , or after rearrangement : (cid:20) z t y t + 1 (cid:21) = (cid:20) œÅ I ‚àí Œ∑ Œõ œÅ I ‚àí Œ∑ Œõ + I (cid:21) (cid:20) z t ‚àí 1 y t (cid:21) . And this leads to the per - dimension update : (cid:20) z t , i y t + 1 , i (cid:21) = (cid:20) œÅ ‚àí Œ∑Œª i œÅ 1 ‚àí Œ∑Œª i (cid:21) t (cid:20) z 0 , i y 1 , i (cid:21) = B t (cid:20) z 0 , i y 1 , i (cid:21) , ( 4 . 2 ) where z t , i and y t , i are i - th element of z t and y t respectively , and B = (cid:20) œÅ ‚àí Œ∑Œª i œÅ 1 ‚àí Œ∑Œª i (cid:21) . Note here z 0 is initialized to be a zero vector , and y 1 is initialized to be Q (cid:62) ( x 1 ‚àí x (cid:63) ) where x 1 is the initial parameter . Suppose the eigenvalue decomposition ( Theorem 11 . 1 in Lu ( 2022c ) ) of B admits B = CDC ‚àí 1 , where columns of C contain eigenvectors of B and D is a diagonal matrix diag ( Œ± , Œ≤ ) containing the eigenvalues of B . Then B t = CD t C ‚àí 1 . Alternatively , the eigenvalues of B can be calculated by solving det ( B ‚àí Œ± I ) = 0 : Œ± , Œ≤ = ( œÅ + 1 ‚àí Œ∑Œª i ) ¬± (cid:112) ( œÅ + 1 ‚àí Œ∑Œª i ) 2 ‚àí 4 œÅ 2 . We then have by Williams ( 1992 ) that B t = Ô£±Ô£≤ Ô£≥ Œ± t B ‚àí Œ≤ I Œ± ‚àí Œ≤ ‚àí Œ≤ t B ‚àí Œ± I Œ± ‚àí Œ≤ , if Œ± (cid:54) = Œ≤ ; Œ± t ‚àí 1 ( t B ‚àí ( t ‚àí 1 ) Œ± I ) , if Œ± = Œ≤ . And therefore it follows by substituting into Eq ( 4 . 2 ) that (cid:20) z t , i y t + 1 , i (cid:21) = B t (cid:20) z 0 , i y 1 , i (cid:21) , where the rate of convergence is controlled by the slower one , max { | Œ± | , | Œ≤ | } ; when max { | Œ± | , | Œ≤ | } < 1 , the GD with momentum is guaranteed to converge . When œÅ = 0 , the momentum reduces to vanilla GD , such that max { | Œ± | , | Œ≤ | } = | 1 ‚àí Œ∑Œª i | < 1 , ‚àÄ i ‚àà { 1 , 2 , . . . , d } , same as that in Eq ( 1 . 11 ) . 35 Jun Lu Following the same example in Figure 18 and Figure 19 where A = (cid:20) 4 0 0 40 (cid:21) with eigenvalues Œª 1 = 4 and Œª 2 = 40 , and matrix B in Eq ( 4 . 2 ) being B 1 = (cid:20) œÅ ‚àí 4 Œ∑ œÅ 1 ‚àí 4 Œ∑ (cid:21) and B 2 = (cid:20) œÅ ‚àí 40 Œ∑ œÅ 1 ‚àí 40 Œ∑ (cid:21) respectively . Then it can be shown that when Œ∑ = 0 . 04 , œÅ = 0 . 8 , the rate of convergence is approximated to be 0 . 894 ; Figure 20 ( b ) shows the updates for 20 iterations , though the motion is in a zigzag pattern , it can still converge . However , when Œ∑ = 0 . 04 , œÅ = 1 , the rate of convergence is equal to 1 ; Figure 20 ( c ) shows the updates for 20 iterations , the movement diverges even it passes through the optimal point . 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 x t , x t 1 same direction 25 75 125 175 225 275 325 375 L o ss V a l u e ( a ) Momentum œÅ = 0 . 2 , rate ‚âà 0 . 79 . 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 25 75 125 175 225 275 325 L o ss V a l u e ( b ) Momentum œÅ = 0 . 8 , rate ‚âà 0 . 89 . 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 25 75 125 175 225 275 325 L o ss V a l u e ( c ) Momentum œÅ = 1 , rate = 1 . Figure 20 : Momentum creates its own oscillations . The learning rates Œ∑ are set to be 0 . 04 for all scenarios . 4 . 2 Nesterov Momentum Nesterov momentum ( a . k . a . , Nesterov accelerated gradient ( NAG ) ) is a slightly diÔ¨Äerent version of the momentum update and has recently been gaining popularity . The core idea behind Nesterov momentum is that when the current parameter vector is at some position x t , then looking at the momentum update above , we know that the momentum term alone ( i . e . , ignoring the second term with the gradient ) is about to nudge the parameter vector by œÅ ‚àÜ x t ‚àí 1 . Therefore , if we are about to compute the gradient , we can treat the future approximate position x t + œÅ ‚àÜ x t ‚àí 1 as a lookahead - this is a point in the vicinity of where we are soon going to end up . Hence , it makes sense to compute the gradient at x t + œÅ ‚àÜ x t ‚àí 1 instead of at the old position x t . Finally , the step has this form : ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ ‚àÇL ( x t + œÅ ‚àÜ x t ‚àí 1 ) ‚àÇ x . Figure 21 shows the diÔ¨Äerence between momentum and Nesterov momentum . This important diÔ¨Äerence is thought to counterbalance too high velocities by ‚Äúpeeking ahead‚Äù actual objective values in the candidate search direction . In other words , one Ô¨Årst makes a big jump in the direction of the previously accumulated gradient ; then measures the gradient where you end up and make a correction . But in standard momentum , one Ô¨Årst jumps by current gradient , then makes a big jump in the direction of the previously accumulated 36 Gradient Descent , Stochastic Optimization , and Other Tales gradient : ‚àíùúÇùúÇ ùúïùúïùúïùúï ( ùë•ùë• ùë°ùë° ) ùúïùúïùë•ùë• ùë°ùë° actual step ùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëö ùëöùëöùëöùëöùë°ùë°ùëöùëö ùúåùúåùúåùúåùë•ùë• ùë°ùë° - 1 ( a ) Momentum : evaluate gradient at the current position x t , and momentum is about to carry us to the tip of the green arrow gradient : ‚àíùúÇùúÇ ùúïùúïùúïùúï ( ùë•ùë• ùë°ùë° ) ùúïùúïùë•ùë• ùë°ùë° actual step ùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëöùëö ùëöùëöùëöùëöùë°ùë°ùëöùëö ùúåùúåùúåùúåùë•ùë• ùë°ùë°‚àí1 Lookahead gradient ‚àíùúÇùúÇ ùúïùúïùúïùúï ( ùë•ùë• ùë°ùë° + ùúåùúåùúåùúåùë•ùë• ùë°ùë°‚àí1 ) ùúïùúïùë•ùë• ( b ) Nesterov momentum : evaluate the gradient at this ‚Äúlooked - ahead‚Äù position . Figure 21 : Comparison of momentum and Nesterov momentum . gradient . To make a metaphor , it turns out , if you‚Äôre going to gamble , it‚Äôs much better to gamble and then make a correction , than to make a correction and then gamble ( Hinton et al . , 2012c ) . Sutskever et al . ( 2013 ) show that Nesterov momentum has a provably better bound than gradient descent for convex , non - stochastic objectives settings . 4 . 3 AdaGrad The learning rate annealing procedure modiÔ¨Åes a single global learning rate that applies to all dimensions of the parameters ( Section 3 . 1 , p . 24 ) . Duchi et al . ( 2011 ) proposed a method called AdaGrad where the learning rate is updated on a per - dimension basis . The learning rate for each parameter depends on the history of gradient updates of that parameter in a way such that parameters with a scarce history of updates are updated faster by using a larger learning rate . In other words , parameters that have not been updated much in the past are more likely to have higher learning rates now . Denoting the element - wise vector multiplication between a and b by a (cid:12) b , formally , the AdaGrad has the following update step : ‚àÜ x t = ‚àí Œ∑ (cid:113)(cid:80) tœÑ = 1 g 2 œÑ + (cid:15) (cid:12) g t , ( 4 . 3 ) where (cid:15) is a smoothing term to better condition the division , Œ∑ is a global learning rate shared by all dimensions , g 2 œÑ indicates the element - wise square g œÑ (cid:12) g œÑ , and the denomi - nator computes the l 2 norm of a sum of all previous squared gradients in a per - dimension fashion . Though the global learning rate Œ∑ is shared by all dimensions , each dimension has its own dynamic learning rate controlled by the l 2 norm of accumulated gradient magni - tudes . Since this dynamic learning rate grows with the inverse of the accumulated gradient magnitudes , larger gradient magnitudes have smaller learning rates and smaller absolute values of gradients have larger learning rates . Therefore , the aggregated squared magnitude of the partial derivative with respect to each parameter over the course of the algorithm in the denominator has the same eÔ¨Äects as the learning rate annealing . One pro of AdaGrad is that it is very easy to implement , the code snippet in the following is the implementation of it by Python : 37 Jun Lu # Assume the gradient dx and parameter vector x cache + = dx * * 2 x + = - learning _ rate * dx / np . sqrt ( cache + 1e - 8 ) On the other hand , AdaGrad partly eliminates the need to tune the learning rate con - trolled by the accumulated gradient magnitude . However , AdaGrad‚Äôs main weakness is its unbounded accumulation of the squared gradients in the denominator . Since every added term is positive , the accumulated sum keeps growing or exploding during every training step . This in turn causes the per - dimension learning rate to shrink and eventually decrease throughout training and become inÔ¨Ånitesimally small , eventually falling to zero and stop - ping training any more . Moreover , since the magnitudes of gradients are factored out in AdaGrad , this method can be sensitive to the initialization of the parameters and the cor - responding gradients . If the initial magnitudes of the gradients are large or inÔ¨Ånitesimally huge , the per - dimension learning rates will be low for the remainder of training . This can be partly combated by increasing the global learning rate , making the AdaGrad method sensitive to the choice of learning rate . Further , AdaGrad assumes the parameter with fewer updates should favor a larger learning rate ; and one with more movement should employ a smaller learning rate . This makes it consider only the information from squared gradients or the absolute value of the gradients . And thus AdaGrad does not include information from the total move ( i . e . , the sum of updates ; in contrast to the sum of absolute updates ) . To be more succinct , AdaGrad has the following main drawbacks : 1 ) the continual decay of learning rates throughout training ; 2 ) the need for a manually selected global learning rate ; 3 ) considering only the absolute value of gradients . 4 . 4 RMSProp RMSProp is an extension of AdaGrad that overcomes the main weakness of AdaGrad ( Hin - ton et al . , 2012b ; Zeiler , 2012 ) . The original idea of RMSProp is simple : it restricts the window of accumulated past gradients to some Ô¨Åxed size w rather than t ( i . e . , current time step ) . However , since storing w previous squared gradients is ineÔ¨Écient , the RMSProp introduced in Hinton et al . ( 2012b ) ; Zeiler ( 2012 ) implements this accumulation as an ex - ponentially decaying average of the squared gradients . This is very similar to the idea of momentum term ( or decay constant ) . We Ô¨Årst discuss the exact form of the RMSProp . Assume at time t this running average is E [ g 2 ] t , then we compute : E [ g 2 ] t = œÅE [ g 2 ] t ‚àí 1 + ( 1 ‚àí œÅ ) g 2 t , ( 4 . 4 ) where œÅ is a decay constant similar to that used in the momentum method and g 2 t indicates the element - wise square g t (cid:12) g t . In other words , the estimate is achieved by multiplying the current squared aggregate ( i . e . , the running estimate ) by the decay constant œÅ and then adding ( 1 ‚àí œÅ ) times the current squared partial derivative . This running estimate is initialized to 0 which can cause some bias in early iterations ; while the bias disappears over the long term . We notice that the old gradients decay exponentially over the course of the algorithm . As Eq ( 4 . 4 ) is just the root mean squared ( RMS ) error criterion of the gradients , we can replace it with the criterion short - hand . Let RMS [ g ] t = (cid:112) E [ g 2 ] t + (cid:15) , where again a 38 Gradient Descent , Stochastic Optimization , and Other Tales constant (cid:15) is added to better condition the denominator . Then the resulting step size can be obtained as follows : ‚àÜ x t = ‚àí Œ∑ RMS [ g ] t (cid:12) g t , ( 4 . 5 ) where again (cid:12) is the element - wise vector multiplication . As aforementioned , the form in Eq ( 4 . 4 ) is originally from the exponential moving av - erage ( EMA ) . In the original form of EMA , 1 ‚àí œÅ is also known as the smoothing constant ( SC ) where the SC can be written as 2 N + 1 and the period N can be thought of as the number of past values to do the moving average calculation ( Lu , 2022b ) : SC = 1 ‚àí œÅ ‚âà 2 N + 1 . ( 4 . 6 ) The above Eq ( 4 . 6 ) links diÔ¨Äerent variables : the decay constant œÅ , the smoothing constant ( SC ) , and the period N . If œÅ = 0 . 9 , then N = 19 . That is , roughly speaking , E [ g 2 ] t at iteration t is approximately equal to the moving average of the past 19 squared gradients and the current one ( i . e . , the moving average of 20 squared gradients totally ) . The relationship in Eq ( 4 . 6 ) though is not discussed in Zeiler ( 2012 ) , it is important to decide the lower bound of the decay constant œÅ . Typically , a time period of N = 3 or 7 is thought to be a relatively small frame making the lower bound of decay constant œÅ = 0 . 5 or 0 . 75 ; when N ‚Üí ‚àû , the decay constant œÅ approaches 1 . AdaGrad is designed to converge rapidly when applied to a convex function ; while RMSProp performs better in nonconvex settings . When applied to a nonconvex function to train a neural network , the learning trajectory can pass through many diÔ¨Äerent structures and eventually arrives at a region that is a locally convex bowl . AdaGrad shrinks the learning rate according to the entire history of the squared partial derivative leading to an inÔ¨Ånitesimally small learning rate before arriving at such a convex structure . While RMSProp discards ancient squared gradients to avoid this problem . However , we can Ô¨Ånd that the RMSProp still only considers the absolute value of gradients and a Ô¨Åxed number of past squared gradients is not Ô¨Çexible which can cause a small learning rate near ( local ) minima as we will discuss in the sequel . The RMSProp is developed independently by GeoÔ¨Ä Hinton in Hinton et al . ( 2012b ) and by Matthew Zeiler in Zeiler ( 2012 ) both of which are stemming from the need to resolve AdaGrad‚Äôs radically diminishing per - dimension learning rates . Hinton et al . ( 2012b ) suggest œÅ to be set to 0 . 9 and the global learning rate Œ∑ to be 0 . 001 by default . The RMSProp further can be combined into the Nesterov momentum method ( Goodfellow et al . , 2016 ) where the comparison between the two is shown in Algorithm 2 and Algorithm 3 . 4 . 5 AdaDelta Zeiler ( 2012 ) further shows the units of the step size shown above in RMSProp do not match ( so as the vanilla SGD , the momentum , and the AdaGrad ) . To overcome this weakness , from the correctness of the second - order method ( more in Section 5 , p . 51 ) , the author considers rearranging Hessian to determine the quantities involved . It is well known that though the calculation of Hessian or approximation to the Hessian matrix is a tedious and computationally expensive task , its curvature information is useful for optimization , and the units in Newton‚Äôs method are well matched . Given the Hessian matrix H , the update 39 Jun Lu Algorithm 2 RMSProp 1 : Input : Initial parameter x 1 , constant (cid:15) ; 2 : Input : Global learning rate Œ∑ , by default Œ∑ = 0 . 001 ; 3 : Input : Decay constant œÅ ; 4 : Input : Initial accumulated squared gradients E [ g 2 ] 0 = 0 ; 5 : for t = 1 : T do 6 : Compute gradient g t = ‚àá L ( x t ) ; 7 : Compute running estimate E [ g 2 ] t = œÅE [ g 2 ] t ‚àí 1 + ( 1 ‚àí œÅ ) g 2 t ; 8 : Compute step ‚àÜ x t = ‚àí Œ∑ ‚àö E [ g 2 ] t + (cid:15) (cid:12) g t ; 9 : Apply update x t + 1 = x t + ‚àÜ x t ; 10 : end for 11 : Return : resulting parameters x t , and the loss L ( x t ) . Algorithm 3 RMSProp with Nesterov Momentum 1 : Input : Initial parameter x 1 , constant (cid:15) ; 2 : Input : Global learning rate Œ∑ , by default Œ∑ = 0 . 001 ; 3 : Input : Decay constant œÅ , momentum constant Œ± ; 4 : Input : Initial accumulated squared gradients E [ g 2 ] 0 = 0 , and update step ‚àÜ x 0 = 0 ; 5 : for t = 1 : T do 6 : Compute interim update (cid:101) x t = x t + Œ± ‚àÜ x t ‚àí 1 ; 7 : Compute interim gradient g t = ‚àá L ( (cid:101) x t ) ; 8 : Compute running estimate E [ g 2 ] t = œÅE [ g 2 ] t ‚àí 1 + ( 1 ‚àí œÅ ) g 2 t ; 9 : Compute step ‚àÜ x t = Œ± ‚àÜ x t ‚àí 1 ‚àí Œ∑ ‚àö E [ g 2 ] t + (cid:15) (cid:12) g t ; 10 : Apply update x t + 1 = x t + ‚àÜ x t ; 11 : end for 12 : Return : resulting parameters x t , and the loss L ( x t ) . step in Newton‚Äôs method can be described as follows ( Becker and Le Cun , 1988 ; Dauphin et al . , 2014 ) : ‚àÜ x t ‚àù ‚àí H ‚àí 1 g t ‚àù ‚àÇL ( x t ) ‚àÇ x t ‚àÇ 2 L ( x t ) ‚àÇ x 2 . ( 4 . 7 ) This implies 1 ‚àÇ 2 L ( x t ) ‚àÇ x 2 t = ‚àÜ x t ‚àÇL ( x t ) ‚àÇ x t , ( 4 . 8 ) i . e . , the units of the Hessian matrix can be approximated by the right - hand side term of the above equation . Since the RMSProp update in Eq ( 4 . 5 ) already involves RMS [ g ] t in the denominator , i . e . , the units of the gradients . Putting another unit of the order of ‚àÜ x t in the numerator can match the same order as Newton‚Äôs method . To do this , deÔ¨Åne another exponentially decaying average of the update steps : RMS [ ‚àÜ x ] t = (cid:112) E [ ‚àÜ x 2 ] t = (cid:113) œÅE [ ‚àÜ x 2 ] t ‚àí 1 + ( 1 ‚àí œÅ ) ‚àÜ x 2 t . ( 4 . 9 ) 40 Gradient Descent , Stochastic Optimization , and Other Tales 4000 4500 5000 5500 6000 6500 7000 7500 Number of training examples seen ( √ó1e3 ) 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 T r a i n L o ss Training Performance RMSProp Figure 22 : Demonstration of tuning param - eter after each epoch by loading the weights . We save the weights and load them after each epoch such that there are step - points while re - training after each epoch . That is , loss de - terioration is observed after each epoch . Since ‚àÜ x t for the current iteration is not known and the curvature can be assumed to be locally smoothed making it suitable to approximate RMS [ ‚àÜ x ] t by RMS [ ‚àÜ x ] t ‚àí 1 . So we can use an estimation of 1 ‚àÇ 2 L ( x t ) ‚àÇ x 2 t to replace the computationally expensive H ‚àí 1 : ‚àÜ x t ‚àÇL ( x t ) ‚àÇ x t ‚àº RMS [ ‚àÜ x ] t ‚àí 1 RMS [ g ] t . ( 4 . 10 ) This is an approximation to the diagonal Hessian using only RMS measures of g and ‚àÜ x , and results in the update step whose units are matched : ‚àÜ x t = ‚àí RMS [ ‚àÜ x ] t ‚àí 1 RMS [ g ] t (cid:12) g t . ( 4 . 11 ) The idea of AdaDelta from the second - order method overcomes the annoying choosing of learning rate . Meanwhile , a web demo developed by Andrej Karpathy can be explored to Ô¨Ånd the convergence rates among SGD , SGD with momentum , AdaGrad , and AdaDelta 9 . A last note on using the RMSProp or AdaDelta method is to carefully notice that though the accumulated squared gradients in the denominator to compensate for the per - dimension learning rates , if we save the checkpoint of the neural networks at the end of some epochs and want to re - tune the parameter by loading the weights from the checkpoint , the Ô¨Årst few batches of the re - tuning can perform poorly since there are not enough squared gradients to smooth the denominator . While this is not a big issue for the whole training progress as the loss can still go down from that point , a better choice might be saving the E [ g 2 ] t along with the weights of the neural networks . A particular example is shown in Figure 22 where we save the weights and load them after each epoch ; loss deterioration is observed after each epoch . 9 . see https : / / cs . stanford . edu / people / karpathy / convnetjs / demo / trainers . html . 41 Jun Lu 4 . 6 AdaSmooth In this section we will discuss the eÔ¨Äective ratio based on previous updates in the stochas - tic optimization process and how to apply it to accomplish adaptive learning rates per - dimension via the Ô¨Çexible smoothing constant , hence the name AdaSmooth . The idea pre - sented in the section is derived from the RMSProp method in order to improve two main drawbacks of the method : 1 ) consider only the absolute value of the gradients rather than the total movement in each dimension ; 2 ) the need for manually selected hyper - parameters . EÔ¨Äective Ratio ( ER ) Kaufman ( 2013 , 1995 ) suggested replacing the smoothing constant in the EMA formula with a constant based on the eÔ¨Éciency ratio ( ER ) . And the ER is shown to provide promising results for Ô¨Ånancial forecasting via classic quantitative strategies where the ER of the closing price is calculated to decide the trend of the asset ( Lu , 2022b ) . This indicator is designed to measure the strength of a trend , deÔ¨Åned within a range from - 1 . 0 to + 1 . 0 where the larger magnitude indicates a larger upward or downward trend . Recently , Lu and Yi ( 2022 ) show the ER can be utilized to reduce overestimation and underestimation in time series forecasting . Given the window size M and a series { h 1 , h 2 , . . . , h T } , it is calculated with a simple formula : e t = s t n t = h t ‚àí h t ‚àí M (cid:80) M ‚àí 1 i = 0 | h t ‚àí i ‚àí h t ‚àí 1 ‚àí i | = Total move for a period Sum of absolute move for each bar , ( 4 . 12 ) where e t is the ER of the series at time t . At a strong trend ( i . e . , the input series is moving in a certain direction , either up or down ) the ER will tend to 1 in absolute value ; if there is no directed movement , it will be a little more than 0 . Instead of calculating the ER of the closing price of the underlying asset , we want to calculate the ER of the moving direction in the update methods for each parameter . And in the descent methods , we care more about how much each parameter moves apart from its initial point in each period , either moving positively or negatively . So here we only consider the absolute value of the ER . To be speciÔ¨Åc , the ER for the parameters in the proposed method is calculated as follows : e t = s t n t = | x t ‚àí x t ‚àí M | (cid:80) M ‚àí 1 i = 0 | x t ‚àí i ‚àí x t ‚àí 1 ‚àí i | = | (cid:80) M ‚àí 1 i = 0 ‚àÜ x t ‚àí 1 ‚àí i | (cid:80) M ‚àí 1 i = 0 | ‚àÜ x t ‚àí 1 ‚àí i | , ( 4 . 13 ) where e t ‚àà R d whose i - th element e t , i is in the range of [ 0 , 1 ] for all i in [ 1 , 2 , . . . , d ] . A large value of e t , i indicates the descent method in the i - th dimension is moving in a certain direction ; while a small value approaching 0 means the parameter in the i - th dimension is moving in a zigzag pattern , interleaved by positive and negative movement . In practice , and in all of our experiments , the M is selected to be the batch index for each epoch . That is , M = 1 if the training is in the Ô¨Årst batch of each epoch ; and M = M max if the training is in the last batch of the epoch where M max is the maximal number of batches per epoch . In other words , M ranges in [ 1 , M max ] for each epoch . Therefore , the value of e t , i indicates the movement of the i - th parameter in the most recent epoch . Or even more aggressively , the window can range from 0 to the total number of batches seen during the whole training progress . The adoption of the adaptive window size M rather than a Ô¨Åxed one has the beneÔ¨Åt that we do not need to keep the past M + 1 steps { x t ‚àí M , x t ‚àí M + 1 , . . . , x t } 42 Gradient Descent , Stochastic Optimization , and Other Tales 2 0 2 4 6 8 x 1 2 1 0 1 2 3 4 5 6 x 2 large move small move 25 75 125 175 225 275 325 L o ss V a l u e Figure 23 : Demonstration of how the ef - fective ratio works . Stochastic optimization tends to move a large step when it is far from the ( local ) minima ; and a relatively small step when it is close to the ( local ) minima . to calculate the signal and noise vectors { s t , n t } in Eq ( 4 . 13 ) since they can be obtained in an accumulated fashion . AdaSmooth If the ER in magnitude of each parameter is small ( approaching 0 ) , the movement in this dimension is zigzag , the proposed AdaSmooth method tends to use a long period average as the scaling constant to slow down the movement in that dimension . When the absolute ER per - dimension is large ( tend to 1 ) , the path in that dimension is moving in a certain direction ( not zigzag ) , and the learning actually is happening and the descent is moving in a correct direction where the learning rate should be assigned to a relatively large value for that dimension . Thus the AdaSmooth tends to choose a small period which leads to a small compensation in the denominator ; since the gradients in the closer periods are small in magnitude when it‚Äôs near the ( local ) minima . A particular example is shown in Figure 23 , where the descent is moving in a certain direction , and the gradient in the near periods is small in magnitude ; if we choose a larger period to compensate for the denominator , the descent will be slower due to the large factored denominator . In short , we want a smaller period to calculate the exponential average of the squared gradients in Eq ( 4 . 4 ) if the update is moving in a certain direction without a zigzag pattern ; while when the parameter is updated in a zigzag fashion , the period for the exponential average should be larger ( Lu , 2022a ) . The obtained value of ER is used in the exponential smoothing formula . Now , what we want to go further is to set the time period N discussed in Eq ( 4 . 6 ) to be a smaller value when the ER tends to 1 in absolute value ; or a larger value when the ER moves towards 0 . When N is small , SC is known as a ‚Äú fast SC ‚Äù ; otherwise , SC is known as a ‚Äú slow SC ‚Äù . For example , let the small time period be N 1 = 3 , and the large time period be N 2 = 199 . The smoothing ratio for the fast movement must be as for EMA with period N 1 ( ‚Äúfast SC‚Äù = 2 N 1 + 1 = 0 . 5 ) , and for the period of no trend EMA period must be equal to N 2 ( ‚Äúslow SC‚Äù = 2 N 2 + 1 = 0 . 01 ) . Thus the new changing smoothing constant is introduced , called the ‚Äú scaled smoothing constant ‚Äù ( SSC ) , denoted by a vector c t ‚àà R d : c t = ( fast SC ‚àí slow SC ) √ó e t + slow SC . 43 Jun Lu By Eq ( 4 . 6 ) , we can deÔ¨Åne the fast decay constant œÅ 1 = 1 ‚àí 2 N 1 + 1 , and the slow decay constant œÅ 2 = 1 ‚àí 2 N 2 + 1 . Then the scaled smoothing constant vector can be obtained by : c t = ( œÅ 2 ‚àí œÅ 1 ) √ó e t + ( 1 ‚àí œÅ 2 ) , where the smaller e t , the smaller c t . For a more eÔ¨Écient inÔ¨Çuence of the obtained smoothing constant on the averaging period , Kaufman recommended squaring it . The Ô¨Ånal calculation formula then follows : E [ g 2 ] t = c 2 t (cid:12) g 2 t + (cid:0) 1 ‚àí c 2 t (cid:1) (cid:12) E [ g 2 ] t ‚àí 1 . ( 4 . 14 ) or after rearrangement : E [ g 2 ] t = E [ g 2 ] t ‚àí 1 + c 2 t (cid:12) ( g 2 t ‚àí E [ g 2 ] t ‚àí 1 ) . We notice that N 1 = 3 is a small period to calculate the average ( i . e . , œÅ 1 = 1 ‚àí 2 N 1 + 1 = 0 . 5 ) such that the EMA sequence will be noisy if N 1 is less than 3 . Therefore , the minimal value of œÅ 1 in practice is set to be larger than 0 . 5 by default . While N 2 = 199 is a large period to compute the average ( i . e . , œÅ 2 = 1 ‚àí 2 N 2 + 1 = 0 . 99 ) such that the EMA sequence almost depends only on the previous value leading to the default value of œÅ 2 no larger than 0 . 99 . Experimental study will show that the AdaSmooth update will be insensitive to the hyper - parameters in the sequel . We also carefully notice that when œÅ 1 = œÅ 2 , the AdaSmooth algorithm recovers to the RMSProp algorithm with decay constant œÅ = 1 ‚àí ( 1 ‚àí œÅ 2 ) 2 since we square it in Eq ( 4 . 14 ) . After developing the AdaSmooth method , we realize the main idea behind it is similar to that of SGD with momentum : to speed up ( compensate less in the denominator ) the learning along dimensions where the gradient consistently points in the same direction ; and to slow the pace ( compensate more in the denominator ) along dimensions in which the sign of the gradient continues to change . As discussed in the cyclical learning rate section ( Section 3 . 3 , p . 28 ) , Dauphin et al . ( 2014 , 2015 ) argue that the diÔ¨Éculty in minimizing the loss arises from saddle points rather than poor local minima . Saddle points have small gradients that slow the learning pro - cess . However , an adaptive smoothing procedure for the learning rates per - dimension can naturally Ô¨Ånd these saddle points and compensate less in the denominator or ‚Äúincrease‚Äù the learning rates when the optimization is in these areas allowing more rapid traversal of saddle point plateaus . When applied to a nonconvex function to train a neural network , the learning trajectory may pass through many diÔ¨Äerent structures and eventually arrive at a region that is a locally convex bowl . AdaGrad shrinks the learning rate according to the entire history of the squared partial derivative and may have made the learning rate too small before arriving at such a convex structure . RMSProp partly solves this drawback since it uses an exponentially decaying average to discard ancient squared gradients making it more robust in a nonconvex setting compared to the AdaGrad method . The AdaSmooth goes further in two points : 1 ) when it‚Äôs close to a saddle point , a small compensation in the denominator can help it escape the saddle point ; 2 ) when it‚Äôs close to a locally convex bowl , the small compensation further makes it converge faster . Empirical evidence shows the ER used in the simple moving average ( SMA ) with a Ô¨Åxed windows size w can also reÔ¨Çect the trend of the series / movement in quantitative strategies ( Lu , 2022b ) . However , this again needs to store w previous squared gradients in the AdaSmooth case , making it ineÔ¨Écient and we shall not adopt this extension . 44 Gradient Descent , Stochastic Optimization , and Other Tales Algorithm 4 Computing AdaSmooth : the proposed AdaSmooth algorithm . All operations on vectors are element - wise . Good default settings for the tested tasks are œÅ 1 = 0 . 5 , œÅ 2 = 0 . 99 , (cid:15) = 1 e ‚àí 6 , Œ∑ = 0 . 001 ; see Section 4 . 6 or Eq ( 4 . 6 ) for a detailed discussion on the explanation of the decay constants‚Äô default values . The AdaSmoothDelta iteration can be calculated in a similar way . 1 : Input : Initial parameter x 1 , constant (cid:15) ; 2 : Input : Global learning rate Œ∑ , by default Œ∑ = 0 . 001 ; 3 : Input : Fast decay constant œÅ 1 , slow decay constant œÅ 2 ; 4 : Input : Assert œÅ 2 > œÅ 1 , by default œÅ 1 = 0 . 5 , œÅ 2 = 0 . 99 ; 5 : for t = 1 : T do 6 : Compute gradient g t = ‚àá L ( x t ) ; 7 : Compute ER e t = | x t ‚àí x t ‚àí M | (cid:80) M ‚àí 1 i = 0 | ‚àÜ x t ‚àí 1 ‚àí i | ; 8 : Compute scaled smoothing vector c t = ( œÅ 2 ‚àí œÅ 1 ) √ó e t + ( 1 ‚àí œÅ 2 ) ; 9 : Compute normalization term E [ g 2 ] t = c 2 t (cid:12) g 2 t + (cid:0) 1 ‚àí c 2 t (cid:1) (cid:12) E [ g 2 ] t ‚àí 1 ; 10 : Compute step ‚àÜ x t = ‚àí Œ∑ ‚àö E [ g 2 ] t + (cid:15) (cid:12) g t ; 11 : Apply update x t + 1 = x t + ‚àÜ x t ; 12 : end for 13 : Return : resulting parameters x t , and the loss L ( x t ) . AdaSmoothDelta We observe that the ER can also be applied to the AdaDelta setting : ‚àÜ x t = ‚àí (cid:112) E [ ‚àÜ x 2 ] t (cid:112) E [ g 2 ] t + (cid:15) (cid:12) g t , ( 4 . 15 ) where E [ g 2 ] t = c 2 t (cid:12) g 2 t + (cid:0) 1 ‚àí c 2 t (cid:1) (cid:12) E [ g 2 ] t ‚àí 1 , ( 4 . 16 ) and E [ ‚àÜ x 2 ] t = ( 1 ‚àí c 2 t ) (cid:12) ‚àÜ x 2 t + c 2 t (cid:12) E [ ‚àÜ x 2 ] t ‚àí 1 , ( 4 . 17 ) in which case the diÔ¨Äerence in E [ ‚àÜ x 2 ] t is to choose a larger period when the ER is small . This is reasonable in the sense that E [ ‚àÜ x 2 ] t appears in the numerator while E [ g 2 ] t is in the denominator of Eq ( 4 . 15 ) making their compensation towards diÔ¨Äerent directions . Or even , a Ô¨Åxed decay constant can be applied for E [ ‚àÜ x 2 ] t : E [ ‚àÜ x 2 ] t = ( 1 ‚àí œÅ 2 ) ‚àÜ x 2 t + œÅ 2 E [ ‚àÜ x 2 ] t ‚àí 1 , The AdaSmoothDelta optimizer introduced above further alleviates the need for a hand speciÔ¨Åed global learning rate which is set to Œ∑ = 1 from the Hessian context . How - ever , due to the adaptive smoothing constants in Eq ( 4 . 16 ) and ( 4 . 17 ) , the E [ g 2 ] t and E [ ‚àÜ x 2 ] t are less locally smooth making it less insensitive to the global learning rate than the AdaDelta method . Therefore , a smaller global learning rate , e . g . , Œ∑ = 0 . 5 is favored in AdaSmoothDelta . The full procedure for computing AdaSmooth is then formulated in Algorithm 4 . We have discussed the step - points problem when reloading weights from checkpoints in the RMSProp or AdaDelta methods . However , this issue is less severe in the AdaSmooth 45 Jun Lu Figure 24 : Demonstration of tuning param - eter after each epoch by loading the weights . We save the weights and load them after each epoch such that there are step - points while re - training after each epoch . This issue is less sever in the AdaSmooth case than in the RM - SProp method . A smaller loss deterioration is observed in the AdaSmooth example than that of the RMSProp case . 4000 4500 5000 5500 6000 6500 7000 7500 Number of training examples seen ( √ó1e3 ) 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 1 . 25 1 . 50 1 . 75 2 . 00 T r a i n L o ss Training Performance RMSProp AdaSmooth setting as a typical example shown in Figure 24 where a smaller loss deterioration is observed in the AdaSmooth example than that of the RMSProp case . 0 10 20 30 40 50 60 Epoch 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 T r a i n L o ss Training Performance Momentum ( = 0 . 9 ) AdaGrad ( = 0 . 01 ) RMSProp ( = 0 . 99 ) AdaDelta ( = 0 . 99 ) AdaSmooth ( 1 = 0 . 5 , 2 = 0 . 9 ) AdaSmooth ( 1 = 0 . 5 , 2 = 0 . 95 ) AdaSmoothDelta ( 1 = 0 . 5 , 2 = 0 . 9 ) ( a ) MNIST training Loss 0 25 50 75 100 125 150 175 200 Epoch 1 . 10 1 . 15 1 . 20 1 . 25 1 . 30 1 . 35 T r a i n L o ss Training Performance Momentum ( = 0 . 9 ) AdaGrad ( = 0 . 01 ) RMSProp ( = 0 . 99 ) AdaDelta ( = 0 . 99 ) AdaSmooth ( 1 = 0 . 5 , 2 = 0 . 9 ) AdaSmooth ( 1 = 0 . 5 , 2 = 0 . 95 ) AdaSmoothDelta ( 1 = 0 . 5 , 2 = 0 . 9 ) ( b ) Census Income training loss Figure 25 : MLP : Comparison of descent methods on MNIST digit and Census Income data sets for 60 and 200 epochs with MLP . Example : Multi - Layer Perceptron To see the diÔ¨Äerence between the discussed algo - rithms by far , we conduct experiments with diÔ¨Äerent machine learning models ; and diÔ¨Äerent data sets including real handwritten digit classiÔ¨Åcation task , MNIST ( LeCun , 1998 ) 10 , and Census Income 11 data sets are used . In all scenarios , the same parameter initialization is 10 . It has a training set of 60 , 000 examples , and a test set of 10 , 000 examples . 11 . Census income data has 48842 number of samples and 70 % of them are used as the training set in our case : https : / / archive . ics . uci . edu / ml / datasets / Census + Income . 46 Gradient Descent , Stochastic Optimization , and Other Tales adopted when training with diÔ¨Äerent stochastic optimization algorithms . We compare the results in terms of convergence speed and generalization . Multi - layer perceptrons ( MLP , a . k . a . , multi - layer neural networks ) are powerful tools for solving machine learning tasks Ô¨Ånding internal linear and nonlinear features behind the model inputs and outputs . We adopt the simplest MLP structure : an input layer , a hidden layer , and an output layer . We notice that rectiÔ¨Åed linear unit ( Relu ) outperforms Tanh , Sigmoid , and other nonlinear units in practice making it the default nonlinear function in our structures . Since dropout has become a core tool in training neural networks ( Srivastava et al . , 2014 ) , we adopt 50 % dropout noise to the network architecture during training to prevent overÔ¨Åtting . To be more concrete , the detailed architecture for each fully connected layer is described by F ( (cid:104) num outputs (cid:105) : (cid:104) activation function (cid:105) ) ; and for a dropout layer is described by DP ( (cid:104) rate (cid:105) ) . Then the network structure we use can be described as follows : F ( 128 : Relu ) DP ( 0 . 5 ) F ( num of classes : Softmax ) . ( 4 . 18 ) All methods are trained on mini - batches of 64 images per batch for 60 or 200 epochs through the training set . Setting the hyper - parameter to (cid:15) = 1 e ‚àí 6 . If not especially mentioned , the global learning rates are set to Œ∑ = 0 . 001 in all scenarios . While a relatively large learning rate ( Œ∑ = 0 . 01 ) is used for AdaGrad method since its accumulated decaying eÔ¨Äect ; learning rate for the AdaDelta method is set to 1 as suggested by Zeiler ( 2012 ) and for the AdaSmoothDelta method is set to 0 . 5 as discussed in Section 4 . 6 . In Figure 25 ( a ) and 25 ( b ) we compare SGD with momentum , AdaGrad , RMSProp , AdaDelta , AdaSmooth and AdaSmoothDelta in optimizing the training set losses for MNIST and Census Income data sets respectively . The SGD with momentum method does the worst in this case . AdaSmooth performs slightly better than AdaGrad and RMSProp in the MNIST case and much better than the latters in the Census Income case . AdaSmooth shows fast convergence from the initial epochs while continuing to reduce the training losses in both the two experiments . We here show two sets of slow decay constant for AdaSmooth , i . e . , ‚Äú œÅ 2 = 0 . 9‚Äù and ‚Äú œÅ 2 = 0 . 95‚Äù . Since we square the scaled smoothing constant in Eq ( 4 . 14 ) , when œÅ 1 = œÅ 2 = 0 . 9 , the AdaSmooth recovers to RMSProp with œÅ = 0 . 99 ( so as the AdaSmoothDelta and AdaDelta case ) . In all cases , the AdaSmooth results perform better while there is almost no diÔ¨Äerence between the results of AdaSmooth with various hyper - parameters in the MLP model . Table 2 shows the best training set accuracy for diÔ¨Äerent algorithms . While we notice the best test set accuracies for various algorithms are very close ; we only present the best ones for the Ô¨Årst 5 epochs in Table 3 . In all scenarios , the AdaSmooth method converges slightly faster than other optimization methods in terms of the test accuracy for this toy example . 4 . 7 Adam Adaptive moment estimation ( Adam ) is yet another adaptive learning rate optimization algorithm ( Kingma and Ba , 2014 ) . The Adam algorithm uses a similar normalization by second - order information , the running estimates for squared gradient ; however , it also incorporates Ô¨Årst - order information into the update . In addition to storing the exponential moving average of past squared gradient ( the second moment ) like RMSProp , AdaDelta , 47 Jun Lu Method MNIST Census SGD with Momentum ( œÅ = 0 . 9 ) 98 . 64 % 85 . 65 % AdaGrad ( Œ∑ = 0 . 01 ) 98 . 55 % 86 . 02 % RMSProp ( œÅ = 0 . 99 ) 99 . 15 % 85 . 90 % AdaDelta ( œÅ = 0 . 99 ) 99 . 15 % 86 . 89 % AdaSmooth ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 9 ) 99 . 34 % 86 . 94 % AdaSmooth ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 95 ) 99 . 45 % 87 . 10 % AdaSmoothDelta ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 9 ) 99 . 60 % 86 . 86 % Table 2 : MLP : Best in - sample evaluation in training accuracy ( % ) . Method MNIST Census SGD with Momentum ( œÅ = 0 . 9 ) 94 . 38 % 83 . 13 % AdaGrad ( Œ∑ = 0 . 01 ) 96 . 21 % 84 . 40 % RMSProp ( œÅ = 0 . 99 ) 97 . 14 % 84 . 43 % AdaDelta ( œÅ = 0 . 99 ) 97 . 06 % 84 . 41 % AdaSmooth ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 9 ) 97 . 26 % 84 . 46 % AdaSmooth ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 95 ) 97 . 34 % 84 . 48 % AdaSmoothDelta ( œÅ 1 = 0 . 5 , œÅ 2 = 0 . 9 ) 97 . 24 % 84 . 51 % Table 3 : MLP : Best out - of - sample evaluation in test accuracy for the Ô¨Årst 5 epochs . and AdaSmooth , Adam also keeps an exponentially decaying average of the past gradients : m t = œÅ 1 m t ‚àí 1 + ( 1 ‚àí œÅ 1 ) g t ; v t = œÅ 2 v t ‚àí 1 + ( 1 ‚àí œÅ 2 ) g 2 t , ( 4 . 19 ) where m t and v t are running estimates of the Ô¨Årst moment ( the mean ) and the second moment ( the uncentered variance ) of the gradients respectively . The drawback of RMSProp is that the running estimate E [ g 2 ] of the second - order moment is biased in the initial time steps since it is initialized to 0 ; especially when the decay constant is large ( when œÅ is close to 1 in RMSProp ) . Observing the biases towards zero in Eq ( 4 . 19 ) as well , Adam counteracts these biases by computing the bias - free moment estimates : (cid:99) m t = m t 1 ‚àí œÅ t 1 ; (cid:98) v t = v t 1 ‚àí œÅ t 2 . The Ô¨Årst and second moment estimates are then incorporated into the update step : ‚àÜ x t = ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m t . And therefore the update becomes x t + 1 = x t ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m t . In practice , Kingma and Ba ( 2014 ) suggests to use œÅ 1 = 0 . 9 , œÅ 2 = 0 . 999 , and (cid:15) = 1 e ‚àí 8 for the parameters by default . 48 Gradient Descent , Stochastic Optimization , and Other Tales 4 . 8 AdaMax Going further from Adam , Kingma and Ba ( 2014 ) notices the high - order moment : v t = œÅ p 2 v t ‚àí 1 + ( 1 ‚àí œÅ p 2 ) | g t | p , that is numerically unstable for large p values making l 1 and l 2 norms the common choices for updates . However , when p ‚Üí ‚àû , the l ‚àû also exhibits stable behavior . Therefore , the AdaMax admits the following moment update : u t = œÅ ‚àû 2 u t ‚àí 1 + ( 1 ‚àí œÅ ‚àû 2 ) | g t | ‚àû = max ( œÅ 2 u t ‚àí 1 , | g t | ) , where we do not need to correct for initialization bias in this case , and this yields the update step ‚àÜ x t = ‚àí Œ∑ u t (cid:12) (cid:99) m t . In practice , Kingma and Ba ( 2014 ) suggests to use Œ∑ = 0 . 002 , œÅ 1 = 0 . 9 , and œÅ 2 = 0 . 999 as the default parameters . 4 . 9 Nadam Nadam ( Nesterov - accelerated Adam ) combines the ideas of Adam and Nesterov momentum ( Dozat , 2016 ) . We recall the momentum and NAG updates as follows : Momentum : g t = ‚àá L ( x t ) ; ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ g t ; x t + 1 = x t + ‚àÜ x t , NAG : g t = ‚àá L ( x t + œÅ ‚àÜ x t ‚àí 1 ) ; ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ g t ; x t + 1 = x t + ‚àÜ x t , Dozat ( 2016 ) Ô¨Årst proposes a modiÔ¨Åcation of NAG by using the current momentum vector to look ahead which we call NAG (cid:48) here . That is , apply the momentum update twice for each update : NAG (cid:48) : g t = ‚àá L ( x t ) ; ‚àÜ x t = œÅ ‚àÜ x t ‚àí 1 ‚àí Œ∑ g t ; ‚àÜ x (cid:48) t = œÅ ‚àÜ x t ‚àí Œ∑ g t ; x t + 1 = x t + ‚àÜ x (cid:48) t . By rewriting the Adam in the following form where a similar modiÔ¨Åcation according to NAG (cid:48) leads to the Nadam update : Adam : m t = œÅ 1 m t ‚àí 1 + ( 1 ‚àí œÅ 1 ) g t ; (cid:99) m t = œÅ 1 m t ‚àí 1 1 ‚àí œÅ t 1 + 1 ‚àí œÅ 1 1 ‚àí œÅ t 1 g t ; ‚àÜ x t = ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m t ; x t + 1 = x t + ‚àÜ x t , leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Nadam : m t = œÅ 1 m t ‚àí 1 + ( 1 ‚àí œÅ 1 ) g t ; (cid:99) m t = œÅ 1 m t 1 ‚àí œÅ t + 1 1 + 1 ‚àí œÅ 1 1 ‚àí œÅ t 1 g t ; ‚àÜ x t = ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m t ; x t + 1 = x t + ‚àÜ x t . 49 Jun Lu However , the œÅ 1 m t ‚àí 1 1 ‚àí œÅ t 1 in (cid:99) m t of the Adam method can be replaced in a momentum fashion ; by applying the same modiÔ¨Åcation on NAG (cid:48) , the second version of Nadam has the following form ( though it‚Äôs not originally presented in Dozat ( 2016 ) ) : Adam (cid:48) : m t = œÅ 1 m t ‚àí 1 + ( 1 ‚àí œÅ 1 ) g t ; (cid:99) m t = œÅ 1 (cid:99) m t ‚àí 1 + 1 ‚àí œÅ 1 1 ‚àí œÅ t 1 g t ; ‚àÜ x t = ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m t ; x t + 1 = x t + ‚àÜ x (cid:48) t , leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Nadam (cid:48) : (cid:99) m t = œÅ 1 m t ‚àí 1 1 ‚àí œÅ t 1 + 1 ‚àí œÅ 1 1 ‚àí œÅ t 1 g t ; (cid:99) m (cid:48) t = œÅ 1 (cid:99) m t + 1 ‚àí œÅ 1 1 ‚àí œÅ t 1 g t ; ‚àÜ x t = ‚àí Œ∑ ‚àö (cid:98) v t + (cid:15) (cid:12) (cid:99) m (cid:48) t ; x t + 1 = x t + ‚àÜ x (cid:48) t . 4 . 10 Problem in SGD Saddle points When the Hessian of loss function is positive - deÔ¨Ånite , then the optimal point x (cid:63) with vanishing gradient must be a local minimum . Similarly , when the Hessian is negative - deÔ¨Ånite , the point is a local maximum ; when the Hessian has both positive and negative eigenvalues , the point is a saddle point ( see later discussion in Eq ( 5 . 3 ) , p . 53 ) . The stochastic optimizers discussed above in practice are Ô¨Årst order optimization algorithms : they only look at the gradient information , and never explicitly compute the Hessian . Such algorithms may get stuck at saddle points ( toy example in Figure 3 ( d ) ) . In the algorithms we show above , the vanilla update , AdaGrad , AdaDelta , RMSprop , and other algorithms will have such problem . AdaSmooth may have chance to go out of the saddle points as argued in Section 4 . 6 . While , the mechanism of momentum and Nesterov momentum help point x to go over the local minimum or saddle point because they have a term of previous step size ( in general ) , but make the model more diÔ¨Écult to converge especially when momentum term œÅ is large . Low speed in SGD However , although claimed in Rong Ge‚Äôs post 12 , it is potential to converge to saddle points with high error rate . Dauphin et al . ( 2014 ) and Benjamin Recht‚Äôs post 13 point out that it is in fact super hard to converge to a saddle point if one picks a random initial point and run SGD . This is because the typical problem for both local minima and saddle points is that they are often surrounded by plateaus of small curvature in the error surface . In the SGD algorithms we discuss above , they are repelled away from a saddle point to a lower error by following the directions of negative curvature . In other words , there will be no so called saddle points problem in SGD algorithms . However , this repulsion can occur slowly due to the plateau with small curvature . While , for the second - order methods , e . g . , Newton‚Äôs method , it does not treat saddle points appropriately . This is partly because Newton‚Äôs method is designed to rapidly descend plateaus surrounding local minima by rescaling gradient steps by the inverse eigenvalues of the Hessian matrix ( we will see shortly in the sequel ) . 12 . http : / / www . oÔ¨Äconvex . org / 2016 / 03 / 22 / saddlepoints / 13 . http : / / www . oÔ¨Äconvex . org / 2016 / 03 / 24 / saddles - again / 50 Gradient Descent , Stochastic Optimization , and Other Tales As argued in Dauphin et al . ( 2014 ) , random Gaussian error functions over large d dimen - sions are increasingly likely to have saddle points rather than local minima as d increases . And the ratio of the number of saddle points to local minima increases exponentially with the dimensionality d . The author also argues that it is saddle points rather than local minima that provide a fundamental impediment to rapid high dimensional non - convex optimization . In this sense , local minima with high errors are exponentially rare in the dimensionality of the problem . So , the computation will be slow in SGD algorithms to escape from small curvature plateaus . First - order method to escape from saddle point The post 14 by Rong Ge introduces a Ô¨Årst - order method to escape from saddle point . He claims that saddle points are very unstable : if we put a ball on a saddle point , then slightly perturb it , the ball is likely to fall to a local minimum , especially when the second - order term 12 ‚àÜ x (cid:62) H ‚àÜ x ( see later discussion in Eq ( 5 . 1 ) , p . 52 ) is signiÔ¨Åcantly smaller than 0 ( i . e . , there is a steep direction where the function value decreases , and assume we are looking for local minimum ) , which is called a strict saddle function in Rong Ge‚Äôs post . In this case , we can use noisy gradient descent : x t + 1 = x t + ‚àÜ x + (cid:15) . where (cid:15) is a noise vector that has zero mean 0 . Actually , it is the basic idea of SGD , which uses the gradient of a mini batch rather than the true gradient . However , the drawback of the stochastic gradient descent is not the direction , but the size of the step along each eigenvector . The step , along any eigen - direction q i , is given by ‚àí Œª i ‚àÜ v i ( see later discussion in Section 5 . 1 , p . 52 , feel free to skip this paragraph at Ô¨Årst reading ) , when the steps taken in the direction with small absolute value of eigenvalues , the step is small . To be more concrete , as an example where the curvature of the error surface may not be the same in all directions . If there is a long and narrow valley in the error surface , the component of the gradient in the direction that points along the base of the valley is very small ; while the component perpendicular to the valley walls is quite large even though we have to move a long distance along the base and a small distance perpendicular to the walls . This phenomenon can be seen in Figure 19 ( a ) ( though it‚Äôs partly solved by SGD with momentum ) . We normally move by making a step that is some constant times the negative gradient rather than a step of constant length in the direction of the negative gradient . This means that in steep regions ( where we have to be careful not to make our steps too large ) , we move quickly ; and in shallow regions ( where we need to move in big steps ) , we move slowly . This phenomenon again causes the SGD methods converging slower than second - order methods . 5 . Second - Order Methods We have discussed that the AdaDelta is from the correctness of the units in second - order methods ( Section 4 . 5 , p . 39 ) . In this section , we shortly review Newton‚Äôs method and its common variants , namely , damped Newton‚Äôs method , Levenberg gradient descent . Fur - thermore , we also derive the conjugate gradient from scratch that utilizes second - order information to capture the curvature shape of the loss surface in order to favor a faster convergence . 14 . http : / / www . oÔ¨Äconvex . org / 2016 / 03 / 22 / saddlepoints / 51 Jun Lu 5 . 1 Newton‚Äôs Method Newton‚Äôs method is an optimization policy by applying Taylor‚Äôs expansion to approximate the loss function by a quadratic form and it guesses where the minimum is via the ap - proximated quadratic equation . By Taylor‚Äôs formula ( Appendix A , p . 74 ) and ignoring derivatives of higher order , the loss function L ( x + ‚àÜ x ) can be approximated by L ( x + ‚àÜ x ) ‚âà L ( x ) + ‚àÜ x (cid:62) ‚àá L ( x ) + 1 2‚àÜ x (cid:62) H ‚àÜ x , ( 5 . 1 ) where H is the Hessian of the loss function L ( x ) with respect to x . The optimal point ( minimum point ) of Eq ( 5 . 1 ) is then obtained at x (cid:63) = x ‚àí H ‚àí 1 ‚àá L ( x ) . That is , the update step is rescaled by the inverse of Hessian . An intuitive interpretation of Newton‚Äôs update is the Hessian contains curvature information , when the curvature is steep , the inverse Hessian can scale more making the step smaller ; while the curvature is Ô¨Çat , the inverse Hessian scales less resulting in a larger update . Here for nonlinear L ( x ) , we cannot just be able to hop to the minimum in one step . Similar to stochastic optimization methods we have introduced in previous sections , Newton‚Äôs method is updated iteratively as formulated in Algorithm 5 ( Roweis , 1996 ; Goodfellow et al . , 2016 ) . Algorithm 5 Newton‚Äôs Method 1 : Input : Initial parameter x 1 ; 2 : for t = 1 : T do 3 : Compute gradient g t = ‚àá L ( x t ) ; 4 : Compute Hessian H t = ‚àá 2 L ( x t ) ; 5 : Compute inverse Hessian H ‚àí 1 t ; 6 : Compute update step ‚àÜ x t = ‚àí H ‚àí 1 t g t ; 7 : Apply update x t + 1 = x t + ‚àÜ x t ; 8 : end for 9 : Return : resulting parameters x t , and the loss L ( x t ) . The computational complexity of Newton‚Äôs method comes from the calculation of the inverse of Hessian at every training iteration . The number of entries in the Hessian matrix is squared in the number of parameters ( x ‚àà R d , H ‚àà R d √ó d ) making the complexity of the inverse be of order O ( d 3 ) ( Trefethen and Bau III , 1997 ; Boyd and Vandenberghe , 2018 ; Lu , 2021 ) . As a consequence , only networks with a small number of parameters , e . g . , shallow neural networks or multi - layer perceptrons , can be trained by Newton‚Äôs method in practice . Reparametrization of the space around critical point A critical point is a point x where the gradient of L ( x ) vanishes . A useful reparametrization of the loss function L around critical points is from Taylor‚Äôs expansion . Since the gradient vanishes , Eq ( 5 . 1 ) can be written as L ( x + ‚àÜ x ) ‚âà L ( x ) + 1 2‚àÜ x (cid:62) H ‚àÜ x . ( 5 . 2 ) 52 Gradient Descent , Stochastic Optimization , and Other Tales Since Hessian H is symmetric , it admits spectral decomposition ( Theorem 13 . 1 in Lu ( 2022c ) ) : H = Q Œõ Q (cid:62) ‚àà R d √ó d , where the columns of Q = [ q 1 , q 2 , . . . , q d ] are eigenvectors of H and are mutually orthonor - mal and the entries of Œõ = diag ( Œª 1 , Œª 2 , . . . , Œª d ) are the corresponding eigenvalues of A , which are real . We deÔ¨Åne the following vector ‚àÜ v : ‚àÜ v = Ô£Æ Ô£ØÔ£∞ q (cid:62) 1 . . . q (cid:62) d Ô£π Ô£∫Ô£ª ‚àÜ x = Q (cid:62) ‚àÜ x . Then the reparametrization follows L ( x + ‚àÜ x ) ‚âà L ( x ) + 1 2‚àÜ x (cid:62) H ‚àÜ x = L ( x ) + 1 2‚àÜ v (cid:62) Œõ ‚àÜ v = L ( x ) + 1 2 d (cid:88) i = 1 Œª i ( ‚àÜ v i ) 2 , ( 5 . 3 ) where ‚àÜ v i is the i - th element of ‚àÜ v . A conclusion on the type of the critical point follows immediately from the reparametrization : ‚Ä¢ If all eigenvalues are non - zero and positive , then the critical point is a local minimum ; ‚Ä¢ If all eigenvalues are non - zero and negative , then the critical point is a local maximum ; ‚Ä¢ If the eigenvalues are non - zero , and both positive and negative eigenvalues exist , then the critical point is a saddle point . In vanilla GD , if an eigenvalue Œª i is positive ( negative ) , then the step moves towards ( away ) from x along ‚àÜ v making the GD towards optimal x (cid:63) . The step along any direction q i is given by ‚àí Œª i ‚àÜ v i . While in Newton‚Äôs method , the step is rescaled by the inverse Hessian , making the step along direction q i scaled into ‚àí ‚àÜ v i . This may cause problem when the eigenvalue is negative resulting in the opposite direction as that in vanilla GD ( Dauphin et al . , 2014 ) . The reparametrization shows that rescaling the gradient along the direction of each eigenvector can result in wrong direction when the eigenvalue Œª i is negative . This suggests the rescale by its magnitude , i . e . , scale by 1 / | Œª i | rather than 1 / Œª i preserving sign of the gradient and solving the slowness of GD at the same time . From Eq ( 5 . 3 ) , when both positive and negative eigenvalues exist , both vanilla GD and Newton‚Äôs method have chance to be stuck in saddle points , achieving poor performance . However , scaling by 1 / | Œª i | can partly solve this problem , since the movement around the saddle point can either increase the loss or decrease the loss from this rescaling rather than stay where it is ( Nocedal and Wright , 1999 ; Dauphin et al . , 2014 ) . 5 . 2 Damped Newton‚Äôs Method The Newton‚Äôs method solves the slowness problem by rescaling the gradients in each direc - tion with the inverse of the corresponding eigenvalue , yielding the step ‚àÜ x t = ‚àí H ‚àí 1 t g t at iteration t . However , this approach can result in moving in the wrong direction when the 53 Jun Lu eigenvalue is negative . Newton‚Äôs step moves along the eigenvector in a direction opposite to the gradient descent step , thus increasing the error . This problem can be solved by damping the Hessian , in which case we remove negative curvature by adding a constant Œ± to its diagonal , yielding the step ‚àÜ x t = ‚àí ( H + Œ± I ) ‚àí 1 g t . We can view Œ± as the tradeoÔ¨Ä between Newton‚Äôs method and vanilla GD . When Œ± is small , it is closer to Newton‚Äôs method ; when Œ± is large , it is closer to vanilla GD . In this case , we get the step ‚àí Œª i Œª i + Œ± ‚àÜ g t . Therefore , obviously , the drawback of damping Newton‚Äôs method is that it potentially has a small step size in many eigen - directions incurred by the large damping factor Œ± . 5 . 3 Levenberg ( - Marquardt ) Gradient Descent The quadratic rule is not universally better since it assumes a linear approximation of L ( x ) which is only valid when it‚Äôs near a minimum . The Levenberg gradient descent goes further by blending the idea of damped Newton‚Äôs method and vanilla GD in which case we can use a steepest descent type method until we approach a minimum so that we switch to the quadratic rule . The distance from a minimum is described by evaluating the loss ( Levenberg , 1944 ) . If the loss is increasing , the quadratic approximation is not working well and we are not likely near a minimum , yielding a larger Œ± in damped Newton‚Äôs method ; while if the loss is decreasing , the quadratic approximation is working well and we are approaching a minimum , yielding a smaller Œ± in damped Newton‚Äôs method . Marquardt improved this method by incorporating the local curvature information , in which case one replaces the identity matrix in Levenberg‚Äôs method by the diagonal of the Hessian , resulting in the Levenberg - Marquardt gradient descent ( Marquardt , 1963 ) : ‚àÜ x t = ‚àí (cid:18) H + Œ± ¬∑ diag ( H ) (cid:19) ‚àí 1 g t . The Levenberg - Marquardt gradient descent method is nothing more than a heuristic method since it is not optimal for any well deÔ¨Åned criterion of speed or error measurements and it is merely a well thought out optimization procedure . However , it is an optimization method that works extremely well in practice especially for medium sized nonlinear models . 5 . 4 Conjugate Gradient We have shown that the vanilla GD ( with the negative gradient as the descent direction ) can move back and forth in a zigzag pattern when applied in a quadratic bowl ( a ravine - shaped loss curve , see example in Figure 4 ( c ) , p . 11 ) . The situation is severer if the learning rate is guaranteed by line search ( Section 2 , p . 13 ) , since the gradient is orthogonal to the previous update step ( Lemma 2 . 1 , p . 2 . 1 and example in Figure 8 ( g ) , p . 18 ) . The choice of orthogonal directions of descent do not preserve the minimum along the previous search directions and the line search will undo the progress we have already made in the direction of the previous line search . This results in the zigzag pattern in the movement . Instead of favoring a descent direction that is orthogonal to previous search direc - tion ( i . e . , d (cid:62) t + 1 d t = 0 ) , the conjugate descent chooses a search direction that is Hessian - orthogonal ( i . e . , d (cid:62) t + 1 Hd t = 0 , or conjugate with respect to H ) so that the movement is compensated by the curvature information of the loss function . In Figure 26 , we show 54 Gradient Descent , Stochastic Optimization , and Other Tales examples of Hessian - orthogonal pairs when the eigenvalues of the Hessian matrix H are dif - ferent or identical . When the eigenvalues of the Hessian matrix are the same , the Hessian - orthogonality reduces to trivial orthogonal cases ( this can be shown by the spectral de - composition of the Hessian matrix where the orthogonal transformation does not alter the orthogonality ( Lu , 2021 ) ) . 3 2 1 0 1 x 1 7 8 9 10 11 x 2 216 192 168 144 120 96 72 48 L o ss V a l u e ( a ) Surface plot : Hessian with diÔ¨Äerent eigenvalues . 1 0 1 2 x 1 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 x 2 24 40 56 72 88 104 120 136 152 168 L o ss V a l u e ( b ) Surface plot : Hessian with same eigenvalues . Figure 26 : Illustration of H - orthogonal for diÔ¨Äerent Hessian matrices in 2 - dimensional case : H = (cid:20) 40 5 7 10 (cid:21) for Fig 26 ( a ) and H = (cid:20) 40 0 0 40 (cid:21) for Fig 26 ( b ) . The H - orthogonal pairs are orthogonal when H has identical eigenvalues . We now give the formal deÔ¨Ånition of conjugacy as follows : DeÔ¨Ånition 5 . 1 : Conjugacy Given a positive deÔ¨Ånite matrix A ‚àà R d √ó d , the vectors u , v ‚àà R d are conjugate with respect to A if u , v (cid:54) = 0 and u (cid:62) Av = 0 . In the method of conjugate gradient ( CG ) , we Ô¨Ånd a descent direction that is conjugate to the previous search direction with respect to the Hessian matrix H such that the new update step will not undo the progress made in the previous directions : d t = ‚àí‚àá L ( x t ) + Œ≤ t d t ‚àí 1 , 55 Jun Lu where Œ≤ t is a coeÔ¨Écient controlling how much the previous direction would add back to the current search direction . Three popular methods to compute the coeÔ¨Écient are Fletcher - Reeves : Œ≤ Ft = ‚àá L ( x t ) (cid:62) ‚àá L ( x t ) ‚àá L ( x t ‚àí 1 ) (cid:62) ‚àá L ( x t ‚àí 1 ) , Polak - Ribi ` ere : Œ≤ Pt = (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) ‚àá L ( x t ) ‚àá L ( x t ‚àí 1 ) (cid:62) ‚àá L ( x t ‚àí 1 ) , Hestenes ‚Äì Stiefel : Œ≤ Ht = (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) ‚àá L ( x t ) (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) d t ‚àí 1 . When the loss function quadratic , the conjugate gradient ensures that the gradient along the previous direction does not increase in magnitude ( Shewchuk et al . , 1994 ; Nocedal and Wright , 1999 ; Iserles , 2009 ; Goodfellow et al . , 2016 ) . The full procedure of the conjugate gradient method is formulated in Algorithm 6 where we notice that the Ô¨Årst step of CG is identical to a step of steepest descent when the learning rate is calculated by exact line search since Œ≤ 1 = 0 . Algorithm 6 Fletcher - Reeves Conjugate Gradient 1 : Input : Initial parameter x 1 ; 2 : Input : Initialize d 0 = 0 and g 0 = d 0 + (cid:15) ; 3 : for t = 1 : T do 4 : Compute gradient g t = ‚àá L ( x t ) ; 5 : Compute coeÔ¨Écient Œ≤ t = g (cid:62) t g t g (cid:62) t ‚àí 1 g t ‚àí 1 ( Fletcher - Reeves : ) ; 6 : Compute descent direction d t = ‚àí g t + Œ≤ t d t ‚àí 1 ; 7 : Fixed learning rate Œ∑ t = Œ∑ or Ô¨Ånd it by line search : Œ∑ t = arg min L ( x t + Œ∑ d t ) ; 8 : Compute update step ‚àÜ x t = Œ∑ t d t ; 9 : Apply update x t + 1 = x t + ‚àÜ x t ; 10 : end for 11 : Return : resulting parameters x t , and the loss L ( x t ) . While our derivation of the conjugate gradient in the next sections is based on the assumption of symmetric positive deÔ¨Ånite A , it can be easily adapted to asymmetric ones . A comparison among vanilla GD , steepest descent , and conjugate gradient is shown in Figure 27 where we observe that the updates in CG have less zigzag pattern than vanilla GD and steepest descent . 5 . 4 . 1 Quadratic Form in Conjugate Direction ( CD ) Method Following the discussion of quadratic form in GD ( Section 1 . 5 , p . 9 ) , quadratic form in steepest descent ( Section 2 . 4 , p . 17 ) , and quadratic form in momentum ( Section 4 . 1 . 1 , p . 34 ) , we now discuss the quadratic form in CG . 56 Gradient Descent , Stochastic Optimization , and Other Tales 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( a ) Vanilla GD , Ô¨Åxed Œ∑ = 0 . 08 . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( b ) Steepest descent . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( c ) Conjugate descent , Ô¨Åxed Œ∑ = 0 . 06 . 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( d ) Conjugate descent , exact line search . Figure 27 : Illustration for the vanilla GD , steepest descent , and CG of quadratic form with A = (cid:20) 20 7 5 5 (cid:21) , b = 0 , and c = 0 . Starting point to descent is x 1 = [ ‚àí 3 , 3 . 5 ] (cid:62) . To introduce the conjugate gradient ( CG ) method , we Ô¨Årst discuss the conjugate direc - tion ( CD ) method where the distinction between them will be clear in the sequel . By the deÔ¨Ånition of conjugacy ( DeÔ¨Ånition 5 . 1 ) , it is easy to show that any set of vectors { d 1 , d 2 , . . . , d d } ‚àà R d satisfying this property with respect to the symmetric positive deÔ¨Ånite Hessian matrix H = 12 ( A (cid:62) + A ) : d (cid:62) i Hd j , ‚àÄ i (cid:54) = j , ( 5 . 4 ) is also linearly independent . That is , the set span the whole R d space : span { d 1 , d 2 , . . . , d d } = R d . Given the initial parameter x 1 and a set of conjugate directions { d 1 , d 2 , . . . , d d } ( deÔ¨Åned in Eq ( 5 . 4 ) ) , the update at time t is given by x t + 1 = x t + Œ∑ t d t , ( 5 . 5 ) where Œ∑ t is the learning rate at time t and is obtained by minimizing the one - dimensional quadratic function J ( Œ∑ ) = L ( x t + Œ∑ d t ) as that in Eq ( 2 . 5 ) ( p . 17 ) : Œ∑ t = ‚àí d (cid:62) t g t d (cid:62) t Ad t with g t = ‚àá L ( x t ) . ( 5 . 6 ) Then , we have the following theorem that the updates following from the conjugate di - rections will converge in d steps ( the dimension of the parameter ) when A is symmetric positive deÔ¨Ånite . Theorem 5 . 2 : ( Converge in d Steps ) For any initial parameter x 1 , the sequence { x t } generated by the conjugate direction algorithm Eq ( 5 . 5 ) converges to the solution x (cid:63) in at most d steps when A is symmetric positive deÔ¨Ånite . Proof [ of Theorem 5 . 2 ] Since conjugate directions { d 1 , d 2 , . . . , d d } span the whole R d space , the initial error vector e 1 = x 1 ‚àí x (cid:63) ( DeÔ¨Ånition 2 . 2 , p . 19 ) then can be expressed as a linear combination of the conjugate directions : e 1 = x 1 ‚àí x (cid:63) = Œ≥ 1 d 1 + Œ≥ 2 d 2 + . . . + Œ≥ d d d . ( 5 . 7 ) 57 Jun Lu The Œ≥ i ‚Äôs can be obtained by the following equation : d (cid:62) t He 1 = d (cid:88) i = 1 Œ≥ i d (cid:62) t Hd i = Œ≥ t d (cid:62) t Hd t ( by conjugacy , Eq ( 5 . 4 ) ) leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Œ≥ t = d (cid:62) t He 1 d (cid:62) t Ad t = d (cid:62) t H ( e 1 + (cid:80) t ‚àí 1 i = 1 Œ∑ i d i ) d (cid:62) t Hd t ( by conjugacy , Eq ( 5 . 4 ) ) = d (cid:62) t He t d (cid:62) t Hd t . When A is further symmetric and nonsingular , we have e t = x t ‚àí x (cid:63) = x t ‚àí A ‚àí 1 b and H = A . It can be shown Œ≥ t is equal to d (cid:62) t g t d (cid:62) t Ad t . This is exactly the same form ( in magnitude ) as the learning rate at time t in steepest descent : Œ≥ t = ‚àí Œ∑ t ( see Eq ( 2 . 5 ) , p . 17 ) . Substitute into Eq ( 5 . 7 ) , it follows that x (cid:63) = x 1 + Œ∑ 1 d 1 + Œ∑ 2 d 2 + . . . + Œ∑ d d d . Moreover , we have updates by Eq ( 5 . 5 ) that x d + 1 = x d + Œ∑ d d d = x d ‚àí 1 + Œ∑ d ‚àí 1 d d ‚àí 1 + Œ∑ d d d = . . . = x 1 + Œ∑ 1 d 1 + Œ∑ 2 d 2 + . . . + Œ∑ d d d = x (cid:63) , which completes the proof . The above theorem states the conjudate direction by Eq ( 5 . 5 ) converges in d steps , i . e . , x d + 1 minimizes quadratic function L ( x ) = 12 x (cid:62) Ax ‚àí b (cid:62) x + c over the whole space R d . Furthermore , we can prove at each iteration t ‚â§ d , the update x t + 1 minimizes the quadratic function over a subspace of R d . Theorem 5 . 3 : ( Expanding Subspace Minimization ) For any initial parameter x 1 , the sequence { x t } generated by the conjugate direction algorithm Eq ( 5 . 5 ) . Then it follows that g (cid:62) t + 1 d i = 0 , ‚àÄ i = 1 , 2 , . . . , t , and t ‚àà { 1 , 2 , . . . , d } , ( 5 . 8 ) where g t = Ax t ‚àí b ( i . e . , the gradient when A is symmetric ) , and x t + 1 is the minimizer of L ( x ) = 12 x (cid:62) Ax ‚àí b (cid:62) x + c with symmetric positive deÔ¨Ånite A over the subspace D t = { x | x = x 1 + span { d 1 , d 2 , . . . , d t } } . ( 5 . 9 ) Proof [ of Theorem 5 . 3 ] We Ô¨Årst prove g (cid:62) t + 1 d i by induction . When t = 1 , since Œ∑ 1 is obtained to minimize J ( Œ∑ ) = L ( x 1 + Œ∑ d 1 ) , by Lemma 2 . 1 ( p . 13 ) , we have g 2 = ‚àá L ( x 2 ) that is orthogonal to d 1 . Suppose now for general t ‚àí 1 , the induction hypothesis is satisÔ¨Åed with g (cid:62) t d i = 0 for i = 0 , 1 , . . . , t ‚àí 1 . The g t has the following update g t + 1 = Ax t + 1 ‚àí b = A ( x t + Œ∑ t d t ) ‚àí b = g t + Œ∑ t Ad t . ( 5 . 10 ) 58 Gradient Descent , Stochastic Optimization , and Other Tales By conjugacy and the induction hypothesis , we have g (cid:62) t + 1 d i = 0 for i = { 0 , 1 , . . . , t ‚àí 1 } . If we further prove g (cid:62) t + 1 d t , we complete the proof . The again follows from Lemma 2 . 1 ( p . 13 ) , the current gradient is orthogonal to the previous search direction d t . For the second part , we deÔ¨Åne f ( Œ∑ ) = L ( x 1 + Œ∑ 1 d 1 + Œ∑ 2 d 2 + . . . + Œ∑ t d t ) which is a strictly convex quadratic function over Œ∑ = [ Œ∑ 1 , Œ∑ 2 , . . . , Œ∑ t ] (cid:62) such that ‚àÇf ( Œ∑ ) ‚àÇŒ∑ i = 0 , ‚àÄ i = 1 , 2 , . . . , t . This implies ‚àá L ( x 1 + Œ∑ 1 d 1 + Œ∑ 2 d 2 + . . . + Œ∑ t d t (cid:124) (cid:123)(cid:122) (cid:125) ‚àá L ( x t + 1 ) ) (cid:62) d i = 0 , ‚àÄ i = 1 , 2 , . . . , t . That is , x t + 1 ‚àà { x | x = x 1 + span { d 1 , d 2 , . . . , d t } } is the minimizer of L ( x ) . 5 . 4 . 2 Quadratic Form in Conjugate Gradient ( CG ) Method We have stated that the conjugate gradient ( CG ) method is diÔ¨Äerent from the conjugate descent ( CD ) method . The distinction is in that the CG method computes a new vector d t + 1 using only the previous vector d t rather than all of the sequence { d 1 , d 2 , . . . , d t } . And d t + 1 will automatically be conjugate to the sequence in this sense . In the CG method , each search direction d t is chosen to be a linear combination of negative gradient ‚àí g t ( search direction in steepest descent ) and the previous direction d t ‚àí 1 : d t = ‚àí g t + Œ≤ t d t ‚àí 1 . ( 5 . 11 ) This yields Œ≤ t = g (cid:62) t Ad t ‚àí 1 d (cid:62) t ‚àí 1 Ad t ‚àí 1 . This choice of Œ≤ t and d t actually results in the conjugate sequence { d 1 , d 2 , . . . , d t } . To see this , we Ô¨Årst give the deÔ¨Ånition of the Krylov subspace of degree t for vector v with respect to matrix A : K ( v ; t ) = span { v , Av , . . . , A t ‚àí 1 v } . Theorem 5 . 4 : ( Converge in d Steps ) For any initial parameter x 1 , the sequence { x t } generated by the conjugate descent algorithm with search direction generated by Eq ( 5 . 11 ) converges to the solution x (cid:63) in at most d steps when A is symmetric positive deÔ¨Ånite . The result follows from the following claims : g (cid:62) t g i = 0 , for i = { 1 , 2 , . . . , t ‚àí 1 } ; ( 5 . 12 ) span { g 1 , g 2 , . . . , g t } = span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 } = K ( g 1 ; t ) ; ( 5 . 13 ) span { d 1 , d 2 , . . . , d t } = span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 } = K ( g 1 ; t ) ; ( 5 . 14 ) d (cid:62) t Ad i = 0 , for i = { 1 , 2 , . . . , t ‚àí 1 } , ( 5 . 15 ) 59 Jun Lu where Eq ( 5 . 15 ) says the sequence { d t } is conjugate . Proof [ of Theorem 5 . 4 ] The proof follows from induction . Eq ( 5 . 13 ) and Eq ( 5 . 14 ) is trivial when t = 1 . Suppose for t , Eq ( 5 . 13 ) , Eq ( 5 . 14 ) , and Eq ( 5 . 15 ) hold as well ; if we can show the two equations still hold for t + 1 , then we complete the proof . By the induction hypothesis , we have g t ‚àà span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 } , d t ‚àà span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 } . Left multiply by A , it follows that Ad t ‚àà span { Ag 1 , A 2 g 1 , . . . , A t g 1 } . ( 5 . 16 ) Since g t + 1 = Ax t + 1 ‚àí b = A ( x t + ‚àÜ x t ) ‚àí b = A ( x t + Œ∑ t d t ) ‚àí b = g t + Œ∑ t Ad t . ( 5 . 17 ) Then , we have g t + 1 ‚àà span { g 1 , Ag 1 , A 2 g 1 , . . . , A t g 1 } . ( 5 . 18 ) Combine Eq ( 5 . 18 ) and Eq ( 5 . 13 ) , we have span { g 1 , g 2 , . . . , g t , g t + 1 } ‚äÇ span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 , A t g 1 } . To see the reverse inclusion , by Eq ( 5 . 14 ) , it follows that A t g 1 = A ( A t ‚àí 1 g 1 ) ‚àà span { Ad 1 , Ad 2 , . . . , Ad t } . Again , by Eq ( 5 . 17 ) , we have Ad t = ( g t + 1 ‚àí g t ) / Œ∑ t . Therefore A t g 1 ‚àà span { g 1 , g 2 , . . . , g t , g t + 1 } . Combine with Eq ( 5 . 13 ) , we have span { g 1 , Ag 1 , . . . , A t ‚àí 1 g 1 , A t g 1 } ‚äÇ span { g 1 , g 2 , . . . , g t , g t + 1 } . Therefore , Eq ( 5 . 13 ) holds for t + 1 . Eq ( 5 . 14 ) follows similarly and also holds for t + 1 . To see Eq ( 5 . 15 ) holds for t + 1 , we have d (cid:62) t + 1 Ad i = ( ‚àí g t + 1 + Œ≤ t + 1 d t ) (cid:62) Ad i . By Theorem 5 . 3 , we have g (cid:62) t + 1 d i = 0 for i ‚àà { 1 , 2 , . . . , t } . ( 5 . 19 ) Further by Eq ( 5 . 16 ) and Eq ( 5 . 14 ) , we have Ad i ‚àà span { Ag 1 , A 2 g 1 , . . . , A i g 1 } ‚äÇ span { d 1 , d 2 , . . . , d i , d i + 1 } . ( 5 . 20 ) Combine Eq ( 5 . 19 ) and Eq ( 5 . 20 ) , it then follows that d (cid:62) t + 1 Ad i = 0 , for i ‚àà { 1 , 2 , . . . , t ‚àí 1 } . 60 Gradient Descent , Stochastic Optimization , and Other Tales We need to further prove d (cid:62) t + 1 Ad t = 0 ( which is trivial since we invent the algorithm to be like this ) . To see Eq ( 5 . 12 ) holds , we have d i = ‚àí g i + Œ≤ i d i ‚àí 1 . Therefore , g i ‚àà span { d i , d i ‚àí 1 } . Again by Eq ( 5 . 19 ) , we prove g (cid:62) t g i = 0 for i ‚àà { 1 , 2 , . . . , t ‚àí 1 } . Therefore , the CG method developed by Eq ( 5 . 11 ) that creates conjugate directions d t + 1 Ad t = 0 indeed Ô¨Ånds a conjugate set d t + 1 Ad i = 0 for i ‚àà { 1 , 2 , . . . , t } . By Theorem 5 . 2 , the CG method thus converges in at most d steps ( when A is symmetric PD ) . The full procedure is then formulated in Algorithm 7 . Algorithm 7 Vanilla Conjugate Gradient Method for Quadratic Function 1 : Require : Symmetric positive deÔ¨Ånite A ‚àà R d √ó d ; 2 : Input : Initial parameter x 1 ; 3 : Input : Initialize d 0 = 0 and g 0 = d 0 + (cid:15) ; 4 : for t = 1 : d do 5 : Compute gradient g t = ‚àá L ( x t ) ; 6 : Compute coeÔ¨Écient Œ≤ t = g (cid:62) t Ad t ‚àí 1 g (cid:62) t ‚àí 1 Ag t ‚àí 1 ; 7 : Compute descent direction d t = ‚àí g t + Œ≤ t d t ‚àí 1 ; 8 : Learning rate Œ∑ t = ‚àí d (cid:62) t g t d (cid:62) t Ad t ; 9 : Compute update step ‚àÜ x t = Œ∑ t d t ; 10 : Apply update x t + 1 = x t + ‚àÜ x t ; 11 : end for 12 : Return : resulting parameters x t , and the loss L ( x t ) . To further reduce the complexity of the CG algorithm , we Ô¨Årst introduce the notion of Ô¨Çoating operation ( Ô¨Çop ) counts . We follow the classical route and count the number of Ô¨Çoating - point operations ( Ô¨Çops ) that the algorithm requires . Each addition , subtraction , multiplication , division , and square root counts as one Ô¨Çop . Note that we have the con - vention that an assignment operation does not count as one Ô¨Çop . The calculation of the complexity extensively relies on the complexity of the multiplication of two matrices so that we formulate the Ô¨Ånding in the following lemma . Lemma 5 . 5 : ( Vector Inner Product Complexity ) Given two vectors v , w ‚àà R n . The inner product of the two vectors v (cid:62) w is given by v (cid:62) w = v 1 w 1 + v 2 w 2 + . . . v n w n which involves n scalar multiplications and n ‚àí 1 scalar additions . Therefore the complexity for the inner product is 2 n ‚àí 1 Ô¨Çops . The matrix multiplication thus relies on the complexity of the inner product . Lemma 5 . 6 : ( Matrix Multiplication Complexity ) For matrix A ‚àà R m √ó n and B ‚àà R n √ó k , the complexity of the multiplication C = AB is mk ( 2 n ‚àí 1 ) Ô¨Çops . 61 Jun Lu Proof [ of Lemma 5 . 6 ] We notice that each entry of C involves a vector inner product that requires n multiplications and n ‚àí 1 additions . And there are mk such entries which leads to the conclusion . By Theorem 5 . 4 , we can replace the formula for calculating learning rate into : Œ∑ t = ‚àí d (cid:62) t g t d (cid:62) t Ad t leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Œ∑ t = ‚àí g t (cid:62) g t d (cid:62) t Ad t . By Eq ( 5 . 10 ) , it follows that Œ∑ t Ad t = g t + 1 ‚àí g t . Combine with Eq ( 5 . 8 ) and Eq ( 5 . 12 ) , Œ≤ t can also be written as Œ≤ t = ‚àí g (cid:62) t g t d (cid:62) t ‚àí 1 g t ‚àí 1 = g (cid:62) t g t g (cid:62) t ‚àí 1 g t ‚àí 1 . This reduces the complexity from O ( 4 d 2 ) to O ( 4 d ) . This practical CG method is then formulated in Algorithm 8 . Algorithm 8 Practical Conjugate Gradient Method for Quadratic Function 1 : Require : Symmetric positive deÔ¨Ånite A ‚àà R d √ó d ; 2 : Input : Initial parameter x 1 ; 3 : Input : Initialize d 0 = 0 and g 0 = d 0 + (cid:15) ; 4 : for t = 1 : d do 5 : Compute gradient g t = ‚àá L ( x t ) ; 6 : Compute coeÔ¨Écient Œ≤ t = g (cid:62) t g t g (cid:62) t ‚àí 1 g t ‚àí 1 ; (cid:46) set Œ≤ 1 = 0 by convention 7 : Compute descent direction d t = ‚àí g t + Œ≤ t d t ‚àí 1 ; 8 : Learning rate Œ∑ t = ‚àí g t (cid:62) g t d (cid:62) t Ad t ; 9 : Compute update step ‚àÜ x t = Œ∑ t d t ; 10 : Apply update x t + 1 = x t + ‚àÜ x t ; 11 : end for 12 : Return : resulting parameters x t , and the loss L ( x t ) . 5 . 4 . 3 Convergence Analysis for Symmetric Positive Definite Quadratic We further discuss the convergence results of CG method . By Eq ( 5 . 14 ) , there exists a set of { œÉ 1 , œÉ 2 , . . . , œÉ t } coeÔ¨Écients such that x t + 1 = x 1 + Œ∑ 1 d 1 + Œ∑ 2 d 2 + . . . + Œ∑ t d t = x 1 + œÉ 1 g 1 + œÉ 2 Ag 1 + . . . + œÉ t A t ‚àí 1 g 1 = x 1 + P (cid:63)t ‚àí 1 ( A ) g 1 , ( 5 . 21 ) where P (cid:63)t ‚àí 1 ( A ) = œÉ 1 I + œÉ 2 A + . . . + œÉ t A t ‚àí 1 is a polynomial of degree t ‚àí 1 with coeÔ¨Écients { œÉ 1 , œÉ 2 , . . . , œÉ t } that is a special case of the polynomial of degree t ‚àí 1 with random coeÔ¨É - cients { œâ 1 , œâ 2 , . . . , œâ t } , namely P t ‚àí 1 ( A ) = œâ 1 I + œâ 2 A + . . . + œâ t A t ‚àí 1 . ( Note here , P t ‚àí 1 can take either a scalar or a matrix as its argument ) . Suppose symmetric positive deÔ¨Ånite A admits spectral decomposition ( Theorem 13 . 1 in Lu ( 2022c ) ) : A = Q Œõ Q (cid:62) ‚àà R d √ó d leads to ‚àí‚àí‚àí‚àí‚àí‚Üí A ‚àí 1 = Q Œõ ‚àí 1 Q (cid:62) , 62 Gradient Descent , Stochastic Optimization , and Other Tales where the columns of Q = [ q 1 , q 2 , . . . , q d ] are eigenvectors of A and are mutually orthonor - mal , and the entries of Œõ = diag ( Œª 1 , Œª 2 , . . . , Œª d ) with Œª 1 ‚â• Œª 2 ‚â• . . . ‚â• Œª d > 0 are the corresponding eigenvalues of A , which are real and ordered by magnitude ( the eigenvalues are positive since A is assumed to be positive - deÔ¨Ånite ) . It then follows that any eigenvector of A is also an eigenvector of P t ‚àí 1 ( A ) : P t ‚àí 1 ( A ) q i = P t ‚àí 1 ( Œª i ) q i , ‚àÄ i ‚àà { 1 , 2 , . . . , d } . Moreover , since the eigenvectors span the whole space R d , there exists a set of { ŒΩ 1 , ŒΩ 2 , . . . , ŒΩ d } coeÔ¨Écients such that the initial error vector e 1 can be expressed as e 1 = x 1 ‚àí x (cid:63) = d (cid:88) i = 1 ŒΩ i q i , ( 5 . 22 ) where x 1 is the initial parameter . Combining Eq ( 5 . 21 ) and Eq ( 5 . 22 ) , this yields the update of the error vector : e t + 1 = x t + 1 ‚àí x (cid:63) = x 1 + P (cid:63)t ‚àí 1 ( A ) g 1 ‚àí x (cid:63) = x 1 + P (cid:63)t ‚àí 1 ( A ) ( Ax 1 ‚àí A A ‚àí 1 b (cid:124) (cid:123)(cid:122) (cid:125) x (cid:63) ) ‚àí x (cid:63) = x 1 + P (cid:63)t ‚àí 1 ( A ) A ( x 1 ‚àí x (cid:63) ) ‚àí x (cid:63) = (cid:18) I + P (cid:63)t ‚àí 1 ( A ) A (cid:19) ( x 1 ‚àí x (cid:63) ) = (cid:18) I + P (cid:63)t ‚àí 1 ( A ) A (cid:19) d (cid:88) i = 1 ŒΩ i q i = d (cid:88) i = 1 (cid:18) 1 + Œª i P (cid:63)t ‚àí 1 ( A ) (cid:19) ŒΩ i q i ( 5 . 23 ) To further discuss the convergence results , we still need to use the notion of energy norm for error vector | | e | | A = ( e (cid:62) Ae ) 1 / 2 as deÔ¨Åned in Section 2 . 4 . 3 ( p . 21 ) where it can be shown that minimizing | | e t | | A is equivalent to minimizing L ( x t ) by Eq ( 2 . 10 ) ( p . 21 ) . Remark 5 . 7 : Polynomial Minimization Since we proved in Theorem 5 . 3 that x t + 1 minimizes L ( x ) over the subspace D t deÔ¨Åned in Eq ( 5 . 9 ) , it also minimizes the energy norm | | e | | A over the subspace D t at iteration t . It then follows that P (cid:63)t ‚àí 1 ( A ) minimizes over the space of all possible polynomials of degree t ‚àí 1 : P (cid:63)t ‚àí 1 ( A ) = arg min P t ‚àí 1 ( A ) | | x 1 + P t ‚àí 1 ( A ) g 1 ‚àí x (cid:63) | | A . 63 Jun Lu Then the update of the squared energy norm can be obtained by | | e t + 1 | | 2 A = e (cid:62) t + 1 Ae t + 1 = e (cid:62) t + 1 (cid:32) d (cid:88) i = 1 Œª i q i q (cid:62) i (cid:33) e t + 1 = d (cid:88) i = 1 Œª i ( e (cid:62) t + 1 q i ) 2 = d (cid:88) i = 1 Œª i Ô£´ Ô£≠ q (cid:62) i (cid:18) d (cid:88) j = 1 (cid:18) 1 + Œª j P (cid:63)t ‚àí 1 ( A ) (cid:19) ŒΩ j q j (cid:19)Ô£∂ Ô£∏ 2 ( by Eq ( 5 . 23 ) ) = d (cid:88) i = 1 (cid:18) 1 + Œª i P (cid:63)t ‚àí 1 ( Œª i ) (cid:19) 2 Œª i ŒΩ 2 i ( q (cid:62) i q j = 0 if i (cid:54) = j ) = min P t ‚àí 1 d (cid:88) i = 1 (cid:18) 1 + Œª i P t ‚àí 1 ( Œª i ) (cid:19) 2 Œª i ŒΩ 2 i ‚â§ m t d (cid:88) i = 1 Œª i ŒΩ 2 i ( m t = min P t ‚àí 1 max 1 ‚â§ j ‚â§ d ( 1 + Œª j P t ‚àí 1 ( Œª j ) ) 2 ) ‚â§ m t ¬∑ | | e 1 | | 2 A . Therefore , the rate of convergence for the CG method is controlled by m t = min P t ‚àí 1 max 1 ‚â§ j ‚â§ d ( 1 + Œª j P t ‚àí 1 ( Œª j ) ) 2 . ( 5 . 24 ) Special Case : A Has Only r Distinct Eigenvalues We then consider some special cases . Firstly , we want to show the CG method terminates in exactly r iterations if symmetric positive deÔ¨Ånite A has only r distinct eigenvalues . To see this , suppose A has distinct eigenvalues ¬µ 1 < ¬µ 2 < . . . < ¬µ r . And we deÔ¨Åne a polynomial Q r ( Œª ) by Q r ( Œª ) = ( ‚àí 1 ) r ¬µ 1 ¬µ 2 . . . ¬µ r ( Œª ‚àí ¬µ 1 ) ( Œª ‚àí ¬µ 2 ) . . . ( Œª ‚àí ¬µ r ) , where Q r ( Œª i ) = 0 for i = { 1 , 2 , . . . , d } and Q r ( 0 ) = 1 . Therefore , it follows that the polynomial R r ‚àí 1 ( Œª ) = Q r ( Œª ) ‚àí 1 Œª is a polynomial of degree r ‚àí 1 with root at Œª = 0 . Set t ‚àí 1 = r ‚àí 1 in Eq ( 5 . 24 ) , we have 0 ‚â§ m r = min P r ‚àí 1 max 1 ‚â§ j ‚â§ d ( 1 + Œª j P r ‚àí 1 ( Œª j ) ) 2 = max 1 ‚â§ j ‚â§ d ( 1 + Œª j R r ‚àí 1 ( Œª j ) ) 2 = max 1 ‚â§ j ‚â§ d Q 2 r ( Œª i ) = 0 . Therefore m r = 0 , and | | e r + 1 | | A = 0 such that x r + 1 = x (cid:63) and the algorithm terminates at iteration r . A speciÔ¨Åc example is shown in Figure 28 where Figure 28 ( a ) terminates in 2 steps since it has two distinct eigenvalues and Figure 28 ( b ) terminates in just 1 step as it has 1 distinct eigenvalue . 64 Gradient Descent , Stochastic Optimization , and Other Tales 4 2 0 2 x 1 2 1 0 1 2 3 4 5 x 2 10 40 70 100 130 160 190 220 L o ss V a l u e ( a ) CG , 2 distinct eigenvlaues . Finish in 2 steps . 3 2 1 0 1 2 3 x 1 3 2 1 0 1 2 3 x 2 8 32 56 80 104 128 152 176 L o ss V a l u e ( b ) CG , 1 distinct eigenvalue . Finish in 1 step . Figure 28 : Illustration of special cases for CG with exact line search of quadratic forms . A = (cid:20) 20 5 5 5 (cid:21) , b = 0 , c = 0 , and starting point to descent is x 1 = [ ‚àí 2 , 2 ] (cid:62) for Fig 28 ( a ) . A = (cid:20) 20 0 0 20 (cid:21) , b = 0 , c = 0 , and starting point to descent is x 1 = [ ‚àí 2 , 2 ] (cid:62) for Fig 28 ( b ) . Closed Form by Chebyshev Polynomials It can be shown that Eq ( 5 . 24 ) is minimized by a Chebyshev polynomial such that 1 + Œª j P t ‚àí 1 ( Œª j ) = T t (cid:16) Œª max + Œª min ‚àí 2 Œª Œª max ‚àí Œª min (cid:17) T t (cid:16) Œª max + Œª min Œª max ‚àí Œª min (cid:17) , where T t ( w ) = 12 (cid:104) ( w + ‚àö w 2 + 1 ) t + ( w ‚àí ‚àö w 2 ‚àí 1 ) t (cid:105) is the Chebyshev polynomial of de - gree t . Proof To see this , we can rewrite the m t in Eq ( 5 . 24 ) by m t = min P t ‚àí 1 max 1 ‚â§ j ‚â§ d ( 1 + Œª j P t ‚àí 1 ( Œª j ) ) 2 = min P t ‚àí 1 max 1 ‚â§ j ‚â§ d ( (cid:101) P t ( Œª i ) ) 2 , ( 5 . 25 ) where (cid:101) P t ( Œª ) = 1 + ŒªP t ‚àí 1 ( Œª ) = 1 + w 1 Œª + . . . + w t Œª t is a special polynomial of degree t with (cid:101) P t ( 0 ) = 1 . We note that the Chebyshev polynomial can be expressed on the region w ‚àà [ ‚àí 1 , 1 ] as T t ( w ) = cos ( t cos ‚àí 1 w ) , w ‚àà [ ‚àí 1 , 1 ] leads to ‚àí‚àí‚àí‚àí‚àí‚Üí | T t ( w ) | ‚â§ 1 , if w ‚àà [ ‚àí 1 , 1 ] . We may then notice (cid:101) P t ( Œª ) oscillates in the range ¬± T t (cid:16) Œª max + Œª min Œª max ‚àí Œª min (cid:17) ‚àí 1 on the domain [ Œª min , Œª max ] . Suppose there exists a polynomial S t ( Œª ) of degree t such that S t ( 0 ) = 1 and S t is better than (cid:101) P t on the domain [ Œª min , Œª max ] . It then follows that the S t ‚àí (cid:101) P t has a zero at Œª = 0 and other t zeros on the range [ Œª min , Œª max ] , making it has t + 1 zeros which leads to a contradiction . Therefore (cid:101) P t is the best polynomial of degree t . This completes the proof . 65 Jun Lu Therefore , it follows that | | e t + 1 | | A ‚â§ T t (cid:18) Œª max + Œª min Œª max ‚àí Œª min (cid:19) ‚àí 1 ¬∑ | | e 1 | | A = T t (cid:18) Œ∫ + 1 Œ∫ ‚àí 1 (cid:19) ‚àí 1 ¬∑ | | e 1 | | A = 2 (cid:34)(cid:18) ‚àö Œ∫ + 1 ‚àö Œ∫ ‚àí 1 (cid:19) t + (cid:18) ‚àö Œ∫ ‚àí 1 ‚àö Œ∫ + 1 (cid:19) t (cid:35) ‚àí 1 ¬∑ | | e 1 | | A , where Œ∫ = Œª max Œª min is the condition number , and (cid:16) ‚àö Œ∫ ‚àí 1 ‚àö Œ∫ + 1 (cid:17) t ‚Üí 0 as iteration t grows . A weaker inequality is then obtained by | | e t + 1 | | A ‚â§ 2 (cid:18) ‚àö Œ∫ ‚àí 1 ‚àö Œ∫ + 1 (cid:19) t ¬∑ | | e 1 | | A . Figure 29 then compares the rate of convergence in steepest descent and CG per iteration . The convergence of CG is much faster than that of steepest descent . 0 20 40 60 80 100 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R a t e o f C o n v e r g e n c e ( a ) Rate of convergence in steepest descent per iter - ation ( same as Figure 11 ) . The y - axis is Œ∫ ‚àí 1 Œ∫ + 1 . 0 20 40 60 80 100 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R a t e o f C o n v e r g e n c e ( b ) Rate of convergence in CG per iteration . The y - axis is ‚àö Œ∫ ‚àí 1 ‚àö Œ∫ + 1 . Figure 29 : Illustration of the rate of convergence for CG and steepest descent . Preconditioning Since the smaller the condition number Œ∫ , the faster the convergence ( Figure 29 ( b ) ) . We can accelerate the convergence of CG by transforming the linear system to improve the eigenvalue distribution of A ; the procedure is known as preconditioning . The variable x is transformed to (cid:98) x via a nonsingular matrix P such that (cid:98) x = P x ; (cid:98) L ( (cid:98) x ) = 1 2 (cid:98) x (cid:62) ( P ‚àí(cid:62) AP ‚àí 1 ) (cid:98) x ‚àí ( P ‚àí(cid:62) b ) (cid:62) (cid:98) x + c . 66 Gradient Descent , Stochastic Optimization , and Other Tales When A is symmetric , the solution of (cid:98) L ( (cid:98) x ) is equivalent to the solution of the linear equation ( P ‚àí(cid:62) AP ‚àí 1 ) (cid:98) x = P ‚àí(cid:62) b leads to ‚àí‚àí‚àí‚àí‚àí‚Üí P ‚àí(cid:62) Ax = P ‚àí(cid:62) b leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Ax = b . That is , we can solve Ax = b indirectly by solving P ‚àí(cid:62) Ax = P ‚àí(cid:62) b . Therefore , the rate of convergence of the quadratic form (cid:98) L ( (cid:98) x ) depends on the condition number of P ‚àí(cid:62) AP ‚àí 1 that can be controlled by the nonsingular matrix P . Intuitively , the preconditioning is a procedure to stretch the quadratic form to make it more spherical so that the eigenvalues are clustered in a smaller range . A speciÔ¨Åc example is given in Figure 28 that we want to transform the elliptical contour in Figure 28 ( a ) into the spherical contour in Figure 28 ( b ) . Based on Algorithm 8 , the preconditioned CG method is formulated in Algorithm 9 . Algorithm 9 Transformed - Preconditioned CG for Quadratic Function 1 : Require : Symmetric positive deÔ¨Ånite A ‚àà R d √ó d ; 2 : Input : Initial parameter (cid:98) x 1 ; 3 : Input : Initialize (cid:98) d 0 = 0 and (cid:98) g 0 = (cid:98) d 0 + (cid:15) ; 4 : for t = 1 : d do 5 : Compute gradient (cid:98) g t = ‚àá (cid:98) L ( (cid:98) x t ) = ( P ‚àí(cid:62) AP ‚àí 1 ) (cid:98) x ‚àí P ‚àí(cid:62) b ; (cid:46) = P ‚àí(cid:62) g t 6 : Compute coeÔ¨Écient (cid:98) Œ≤ t = (cid:98) g (cid:62) t (cid:98) g t (cid:98) g (cid:62) t ‚àí 1 (cid:98) g t ‚àí 1 ; (cid:46) = g (cid:62) t ( P (cid:62) P ) ‚àí 1 g t g (cid:62) t ‚àí 1 ( P (cid:62) P ) ‚àí 1 g t ‚àí 1 7 : Compute descent direction (cid:98) d t = ‚àí (cid:98) g t + (cid:98) Œ≤ t (cid:98) d t ‚àí 1 ; (cid:46) = ‚àí P ‚àí(cid:62) g t + (cid:98) Œ≤ t (cid:98) d t ‚àí 1 8 : Learning rate (cid:98) Œ∑ t = ‚àí (cid:98) g (cid:62) t (cid:98) g t (cid:98) d (cid:62) t ( P ‚àí(cid:62) AP ‚àí 1 ) (cid:98) d t ; (cid:46) = ‚àí g t (cid:62) ( P (cid:62) P ) ‚àí 1 g t (cid:98) d (cid:62) t ( P ‚àí(cid:62) AP ‚àí 1 ) (cid:98) d t 9 : Compute update step ‚àÜ (cid:98) x t = (cid:98) Œ∑ t (cid:98) d t ; 10 : Apply update (cid:98) x t + 1 = (cid:98) x t + ‚àÜ (cid:98) x t ; 11 : end for 12 : Return : resulting parameters x t = P ‚àí 1 (cid:98) x t , and the loss L ( x t ) . However , the procedure in Algorithm 9 is not desirable since we need to transform x into (cid:98) x = P x and untransformed back by x = P ‚àí 1 (cid:98) x as highlighted in blue texts of Algorithm 9 . This may cause extra computation . Let M = P (cid:62) P , Algorithm 10 then formulates the untransformed - preconditioned CG that is more eÔ¨Écient than Algorithm 9 . Second perspective of preconditioning The matrices M ‚àí 1 A and P ‚àí(cid:62) AP ‚àí 1 have the same eigenvalues . To see this , suppose the eigenpair of M ‚àí 1 A is ( M ‚àí 1 A ) v = Œª v , it follows that ( P ‚àí(cid:62) AP ‚àí 1 ) ( P v ) = P ‚àí(cid:62) Av = P P ‚àí 1 P ‚àí(cid:62) Av = P M ‚àí 1 Av = Œª ( P v ) . Therefore , the preconditioning can be understood from two perspectives . While the second perspective is to solve M ‚àí 1 Ax = M ‚àí 1 b where the condition number is decided by matrix M ‚àí 1 A . The simplest preconditioner M ‚àí 1 is thus a diagonal matrix whose diagonal entries are identical to those of A , known as diagonal preconditioning , in which case we scale the quadratic form along the coordinate axes . In contrast , the perfect preconditioner is M = A 67 Jun Lu Algorithm 10 Untransformed - Preconditioned CG for Quadratic Function 1 : Require : Symmetric positive deÔ¨Ånite A ‚àà R d √ó d ; 2 : Input : Initial parameter x 1 ; 3 : Input : Initialize d 0 = 0 and g 0 = d 0 + (cid:15) ; 4 : for t = 1 : d do 5 : Compute gradient g t = ‚àá L ( x t ) ; (cid:46) Same as that of Algorithm 8 6 : Compute coeÔ¨Écient (cid:98) Œ≤ t = g (cid:62) t M ‚àí 1 g t g (cid:62) t ‚àí 1 M ‚àí 1 g t ‚àí 1 ; (cid:46) Same as that of Algorithm 9 7 : Compute descent direction (cid:101) d t = ‚àí M ‚àí 1 g t + (cid:98) Œ≤ t (cid:101) d t ‚àí 1 ; (cid:46) = ‚àí P ‚àí 1 (cid:98) d t in Algorithm 9 8 : Learning rate (cid:98) Œ∑ t = ‚àí ( g t (cid:62) M ‚àí 1 g t ) / ( (cid:101) d (cid:62) t A (cid:101) d t ) ; (cid:46) Same as that of Algorithm 9 9 : Compute update step ‚àÜ x t = (cid:98) Œ∑ t (cid:101) d t ; (cid:46) = ‚àí P ‚àí 1 ‚àÜ (cid:98) x t in Algorithm 9 10 : Apply update x t + 1 = x t + ‚àÜ x t ; 11 : end for 12 : Return : resulting parameters x t , and the loss L ( x t ) . 3 2 1 0 1 2 3 x 1 3 2 1 0 1 2 3 x 2 8 24 40 56 72 88 104 120 136 152 L o ss V a l u e ( a ) Contour plot of quadratic function with A . 3 2 1 0 1 2 3 x 1 3 2 1 0 1 2 3 x 2 0 . 4 1 . 6 2 . 8 4 . 0 5 . 2 6 . 4 7 . 6 8 . 8 L o ss V a l u e ( b ) Contour plot of quadratic function with P ‚àí(cid:62) AP ‚àí 1 . Figure 30 : Illustration of preconditioning for A = (cid:20) 20 5 5 5 (cid:21) . P is obtained by the Cholesky decomposition such that M = A = P (cid:62) P . such that M ‚àí 1 A = I , whose condition number is 1 , in which case the quadratic form is scaled along its eigenvector directions . In this sense , the P can be obtained by the ( pseudo ) Cholesky decomposition ( Theorem 2 . 1 in Lu ( 2022c ) ) such that M = A = P (cid:62) P . Figure 30 shows the perfect preconditioning on M = A = (cid:20) 20 5 5 5 (cid:21) such that the eigenvalues of P ‚àí(cid:62) AP ‚àí 1 are identical and the condition number is thus equal to 1 . 5 . 4 . 4 General Conjugate Gradient Method Now we come back to the general CG method as introduced in Fletcher and Reeves ( 1964 ) . The method is already formulated in Algorithm 6 ; we may notice the Fletcher - Reeves Con - jugate Gradient method ( Algorithm 6 ) is just the same as the Practical Conjugate Gradient 68 Gradient Descent , Stochastic Optimization , and Other Tales method ( Algorithm 8 ) when the loss function is strongly convex quadratic and the learning rate Œ∑ t is selected to be exact line search . To see why the Fletcher - Reeves Conjugate Gradient algorithm ( Algorithm 6 ) works , the search direction d t must satisfy the descent condition ( Remark 1 . 1 , p . 5 ) such that g (cid:62) t d t < 0 . The descent condition is satisÔ¨Åed when the learning rate is calculated by exact line search in which case the gradient ‚àá L ( x t ) = g t is orthogonal to search direction d t ‚àí 1 ( Lemma 2 . 1 , p . 13 ) : g (cid:62) t d t ‚àí 1 = 0 . Therefore , g (cid:62) t d t = g (cid:62) t ( ‚àí g t + Œ≤ t d t ‚àí 1 ) = ‚àí | | g t | | 2 + Œ≤ t g (cid:62) t d t ‚àí 1 < 0 when Œ∑ t is calculated by exact line search . However , when Œ∑ t is Ô¨Åxed or calculated by inexact line search , the descent condition g (cid:62) t d t may not be satisÔ¨Åed . This problem , however , can be attacked by strong Wolfe conditions ( Nocedal and Wright , 1999 ) ; and we will not give the details . Polak - Ribi ` ere conjugate gradient We have mentioned previously that the Œ≤ t can also be computed by the Polak - Ribi ` ere coeÔ¨Écient : Polak - Ribi ` ere : Œ≤ Pt = (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) ‚àá L ( x t ) ‚àá L ( x t ‚àí 1 ) (cid:62) ‚àá L ( x t ‚àí 1 ) = ( g t ‚àí g t ‚àí 1 ) (cid:62) g t g (cid:62) t ‚àí 1 g t ‚àí 1 . When the loss function is strongly convex quadratic and the learning rate is chosen by exact line search , the Polak - Ribi ` ere coeÔ¨Écient Œ≤ Pt is identical to the Fletcher - Reeves coeÔ¨Écient Œ≤ Ft since g (cid:62) t ‚àí 1 g t = 0 by Theorem 5 . 4 . Hestenes ‚Äì Stiefel conjugate gradient Hestenes ‚Äì Stiefel coeÔ¨Écient is yet another variant of the Polak - Ribi ` ere coeÔ¨Écient : Hestenes ‚Äì Stiefel : Œ≤ Ht = (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) ‚àá L ( x t ) (cid:18) ‚àá L ( x t ) ‚àí ‚àá L ( x t ‚àí 1 ) (cid:19) (cid:62) d t ‚àí 1 = ( g t ‚àí g t ‚àí 1 ) (cid:62) g t ( g t ‚àí g t ‚àí 1 ) (cid:62) d t ‚àí 1 . When the loss function is strongly convex quadratic and the learning rate is chosen by exact line search , the Hestenes ‚Äì Stiefel coeÔ¨Écient Œ≤ Ht is identical to the Fletcher - Reeves coeÔ¨Écient Œ≤ Ft since g (cid:62) t ‚àí 1 g t = 0 by Theorem 5 . 4 and g (cid:62) t d t ‚àí 2 = g (cid:62) t ‚àí 1 d t ‚àí 2 = 0 by Theorem 5 . 3 . Moreover , numerical experiments show that the Polak - Ribi ` ere coeÔ¨Écient and Hestenes ‚Äì Stiefel coeÔ¨Écient are more robust than Fletcher - Reeves coeÔ¨Écient in nonconvex settings ( Nocedal and Wright , 1999 ) . 69 Jun Lu References Shun - Ichi Amari . Natural gradient works eÔ¨Éciently in learning . Neural computation , 10 ( 2 ) : 251 ‚Äì 276 , 1998 . 9 Amir Beck . First - order methods in optimization . SIAM , 2017 . 6 Sue Becker and Yann Le Cun . Improving the convergence of back - propagation learning with . 1988 . 40 Stephen Boyd and Lieven Vandenberghe . Introduction to applied linear algebra : vectors , matrices , and least squares . Cambridge university press , 2018 . 52 Yann Dauphin , Harm De Vries , and Yoshua Bengio . Equilibrated adaptive learning rates for non - convex optimization . Advances in neural information processing systems , 28 , 2015 . 28 , 44 Yann N Dauphin , Razvan Pascanu , Caglar Gulcehre , Kyunghyun Cho , Surya Ganguli , and Yoshua Bengio . Identifying and attacking the saddle point problem in high - dimensional non - convex optimization . Advances in neural information processing systems , 27 , 2014 . 28 , 40 , 44 , 50 , 51 , 53 Timothy Dozat . Incorporating nesterov momentum into adam . 2016 . 49 , 50 Simon S Du , Chi Jin , Jason D Lee , Michael I Jordan , Aarti Singh , and Barnabas Poczos . Gradient descent can take exponential time to escape saddle points . Advances in neural information processing systems , 30 , 2017 . 10 , 30 John Duchi , Elad Hazan , and Yoram Singer . Adaptive subgradient methods for online learning and stochastic optimization . Journal of machine learning research , 12 ( 7 ) , 2011 . 23 , 37 Reeves Fletcher and Colin M Reeves . Function minimization by conjugate gradients . The computer journal , 7 ( 2 ) : 149 ‚Äì 154 , 1964 . 68 Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . 12 Ian Goodfellow , Yoshua Bengio , and Aaron Courville . Deep learning . MIT press , 2016 . 32 , 39 , 52 , 56 Priya Goyal , Piotr Doll¬¥ar , Ross Girshick , Pieter Noordhuis , Lukasz Wesolowski , Aapo Ky - rola , Andrew Tulloch , Yangqing Jia , and Kaiming He . Accurate , large minibatch sgd : Training imagenet in 1 hour . arXiv preprint arXiv : 1706 . 02677 , 2017 . 27 Alex Graves , Abdel - rahman Mohamed , and GeoÔ¨Ärey Hinton . Speech recognition with deep recurrent neural networks . In 2013 IEEE international conference on acoustics , speech and signal processing , pages 6645 ‚Äì 6649 . Ieee , 2013 . 31 Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 ‚Äì 778 , 2016 . 27 70 Gradient Descent , Stochastic Optimization , and Other Tales GeoÔ¨Ärey Hinton , Li Deng , Dong Yu , George E Dahl , Abdel - rahman Mohamed , Navdeep Jaitly , Andrew Senior , Vincent Vanhoucke , Patrick Nguyen , Tara N Sainath , et al . Deep neural networks for acoustic modeling in speech recognition : The shared views of four research groups . IEEE Signal processing magazine , 29 ( 6 ) : 82 ‚Äì 97 , 2012a . 31 GeoÔ¨Ärey Hinton , Nitish Srivastava , and Kevin Swersky . Neural networks for machine learn - ing lecture 6a overview of mini - batch gradient descent . Cited on , 14 ( 8 ) : 2 , 2012b . 23 , 38 , 39 GeoÔ¨Ärey Hinton , Nitish Srivastava , and Kevin Swersky . Neural networks for machine learn - ing lecture 6c the momentum method . Cited on , 14 ( 8 ) : 2 , 2012c . 37 Jeremy Howard and Sebastian Ruder . Universal language model Ô¨Åne - tuning for text classi - Ô¨Åcation . arXiv preprint arXiv : 1801 . 06146 , 2018 . 27 Gao Huang , Yixuan Li , GeoÔ¨Ä Pleiss , Zhuang Liu , John E Hopcroft , and Kilian Q Wein - berger . Snapshot ensembles : Train 1 , get m for free . arXiv preprint arXiv : 1704 . 00109 , 2017 . 30 Arieh Iserles . A Ô¨Årst course in the numerical analysis of diÔ¨Äerential equations . Number 44 . Cambridge university press , 2009 . 56 Chi Jin , Rong Ge , Praneeth Netrapalli , Sham M Kakade , and Michael I Jordan . How to escape saddle points eÔ¨Éciently . In International Conference on Machine Learning , pages 1724 ‚Äì 1732 . PMLR , 2017 . 10 , 30 Perry J Kaufman . Smarter trading , 1995 . 42 Perry J Kaufman . Trading Systems and Methods , + Website , volume 591 . John Wiley & Sons , 2013 . 42 Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . 47 , 48 , 49 Alex Krizhevsky , Ilya Sutskever , and GeoÔ¨Ärey E Hinton . Imagenet classiÔ¨Åcation with deep convolutional neural networks . Advances in neural information processing systems , 25 , 2012 . 31 Yann LeCun . The MNIST database of handwritten digits . http : / / yann . lecun . com / exd - b / mnist / , 1998 . 26 , 46 Kenneth Levenberg . A method for the solution of certain non - linear problems in least squares . Quarterly of applied mathematics , 2 ( 2 ) : 164 ‚Äì 168 , 1944 . 54 Ilya Loshchilov and Frank Hutter . SGDR : Stochastic gradient descent with warm restarts . arXiv preprint arXiv : 1608 . 03983 , 2016 . 30 Jun Lu . Numerical matrix decomposition and its modern applications : A rigorous Ô¨Årst course . arXiv preprint arXiv : 2107 . 02579 , 2021 . 52 , 55 71 Jun Lu Jun Lu . AdaSmooth : An adaptive learning rate method based on eÔ¨Äective ratio . arXiv preprint arXiv : 2204 . 00825 , 2022a . 23 , 43 Jun Lu . Exploring classic quantitative strategies . arXiv preprint arXiv : 2202 . 11309 , 2022b . 39 , 42 , 44 Jun Lu . Matrix decomposition and applications . arXiv preprint arXiv : 2201 . 00145 , 2022c . 4 , 11 , 19 , 35 , 53 , 62 , 68 Jun Lu and Shao Yi . Reducing overestimating and underestimating volatility via the aug - mented blending - ARCH model . Applied Economics and Finance , 9 ( 2 ) : 48 ‚Äì 59 , 2022 . 42 Donald W Marquardt . An algorithm for least - squares estimation of nonlinear parameters . Journal of the society for Industrial and Applied Mathematics , 11 ( 2 ) : 431 ‚Äì 441 , 1963 . 54 Sachin Mehta , Mohammad Rastegari , Linda Shapiro , and Hannaneh Hajishirzi . Espnetv2 : A light - weight , power eÔ¨Écient , and general purpose convolutional neural network . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pages 9190 ‚Äì 9200 , 2019 . 31 Jorge Nocedal and Stephen J Wright . Numerical optimization . Springer , 1999 . 53 , 56 , 69 Brendan O‚Äôdonoghue and Emmanuel Candes . Adaptive restart for accelerated gradient schemes . Foundations of computational mathematics , 15 ( 3 ) : 715 ‚Äì 732 , 2015 . 12 Boris T Polyak . Some methods of speeding up the convergence of iteration methods . Ussr computational mathematics and mathematical physics , 4 ( 5 ) : 1 ‚Äì 17 , 1964 . 32 Martin Popel and OndÀárej Bojar . Training tips for the transformer model . arXiv preprint arXiv : 1804 . 00247 , 2018 . 27 Ning Qian . On the momentum term in gradient descent learning algorithms . Neural net - works , 12 ( 1 ) : 145 ‚Äì 151 , 1999 . 32 Herbert Robbins and Sutton Monro . A stochastic approximation method . The annals of mathematical statistics , pages 400 ‚Äì 407 , 1951 . 4 Sam Roweis . Levenberg - marquardt optimization . Notes , University Of Toronto , 1996 . 52 Sebastian Ruder . An overview of gradient descent optimization algorithms . arXiv preprint arXiv : 1609 . 04747 , 2016 . 32 David E Rumelhart , GeoÔ¨Ärey E Hinton , and Ronald J Williams . Learning representations by back - propagating errors . nature , 323 ( 6088 ) : 533 ‚Äì 536 , 1986 . 32 Heinz Rutishauser . Theory of gradient methods . In ReÔ¨Åned iterative methods for compu - tation of the solution and the eigenvalues of self - adjoint boundary value problems , pages 24 ‚Äì 49 . Springer , 1959 . 4 Jonathan Richard Shewchuk et al . An introduction to the conjugate gradient method without the agonizing pain , 1994 . 12 , 17 , 19 , 56 72 Gradient Descent , Stochastic Optimization , and Other Tales Leslie N Smith . Cyclical learning rates for training neural networks . In 2017 IEEE winter conference on applications of computer vision ( WACV ) , pages 464 ‚Äì 472 . IEEE , 2017 . 28 , 29 Leslie N Smith and Nicholay Topin . Super - convergence : Very fast training of neural net - works using large learning rates . In ArtiÔ¨Åcial intelligence and machine learning for multi - domain operations applications , volume 11006 , page 1100612 . International Society for Optics and Photonics , 2019 . 27 Nitish Srivastava , GeoÔ¨Ärey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhut - dinov . Dropout : A simple way to prevent neural networks from overÔ¨Åtting . The journal of machine learning research , 15 ( 1 ) : 1929 ‚Äì 1958 , 2014 . 47 Ilya Sutskever , James Martens , George Dahl , and GeoÔ¨Ärey Hinton . On the importance of initialization and momentum in deep learning . In International conference on machine learning , pages 1139 ‚Äì 1147 . PMLR , 2013 . 32 , 37 Lloyd N Trefethen and David Bau III . Numerical linear algebra , volume 50 . Siam , 1997 . 52 Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , (cid:32)Lukasz Kaiser , and Illia Polosukhin . Attention is all you need . Advances in neural information processing systems , 30 , 2017 . 27 Kenneth S Williams . The n th power of a 2 √ó 2 matrix . Mathematics Magazine , 65 ( 5 ) : 336 ‚Äì 336 , 1992 . 35 Matthew D Zeiler . Adadelta : An adaptive learning rate method . arXiv preprint arXiv : 1212 . 5701 , 2012 . 4 , 23 , 32 , 38 , 39 , 47 73 Jun Lu Appendix A . Taylor‚Äôs Expansion Theorem A . 1 : ( Taylor‚Äôs Expansion with Lagrange Remainder ) Let f ( x ) : R ‚Üí R be k - times continuously diÔ¨Äerentiable on the closed interval I with endpoints x and y , for some k ‚â• 0 . If f ( k + 1 ) exists on the interval I , then there exists a x (cid:63) ‚àà ( x , y ) such that f ( x ) = f ( y ) + f (cid:48) ( y ) ( x ‚àí y ) + f (cid:48)(cid:48) ( y ) 2 ! ( x ‚àí y ) 2 + . . . + f ( k ) ( y ) k ! ( x ‚àí y ) k + f ( k + 1 ) ( x (cid:63) ) ( k + 1 ) ! ( x ‚àí y ) k + 1 = k (cid:88) i = 0 f ( i ) ( y ) i ! ( x ‚àí y ) i + f ( k + 1 ) ( x (cid:63) ) ( k + 1 ) ! ( x ‚àí y ) k + 1 . The Taylor‚Äôs expansion can be extended to a function of vector f ( x ) : R n ‚Üí R or a function of matrix f ( X ) : R m √ó n ‚Üí R . The Taylor‚Äôs expansion , or also known as the Taylor‚Äôs series , approximates the function f ( x ) around the value of y by a polynomial in a single indeterminate x . To see where does this series come from , we recall from the elementary calculus course that the approximated function around Œ∏ = 0 for cos ( Œ∏ ) is given by cos ( Œ∏ ) ‚âà 1 ‚àí Œ∏ 2 2 . That is , the cos Œ∏ is approximated by a polynomial with degree of 2 . Suppose we want to approximate cos Œ∏ by the more general polynomial with degree of 2 by f ( Œ∏ ) = c 1 + c 2 Œ∏ + c 3 Œ∏ 2 . A intuitive idea is to match the gradients around the 0 point . That is , Ô£±Ô£¥Ô£≤ Ô£¥Ô£≥ cos ( 0 ) = f ( 0 ) ; cos (cid:48) ( 0 ) = f (cid:48) ( 0 ) ; cos (cid:48)(cid:48) ( 0 ) = f (cid:48)(cid:48) ( 0 ) ; leads to ‚àí‚àí‚àí‚àí‚àí‚Üí Ô£±Ô£¥Ô£≤ Ô£¥Ô£≥ 1 = c 1 ; ‚àí sin ( 0 ) = 0 = c 2 ; ‚àí cos ( 0 ) = ‚àí 1 = 2 c 3 . This makes f ( Œ∏ ) = c 1 + c 2 Œ∏ + c 3 Œ∏ 2 = 1 ‚àí Œ∏ 2 2 and agrees with our claim that cos ( Œ∏ ) ‚âà 1 ‚àí Œ∏ 2 2 around the 0 point . We shall not give the details of the proof . 74