HAL Id : hal - 01136945 https : / / hal . archives - ouvertes . fr / hal - 01136945 Submitted on 30 Mar 2015 HAL is a multi - disciplinary open access archive for the deposit and dissemination of sci - entific research documents , whether they are pub - lished or not . The documents may come from teaching and research institutions in France or abroad , or from public or private research centers . L’archive ouverte pluridisciplinaire HAL , est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche , publiés ou non , émanant des établissements d’enseignement et de recherche français ou étrangers , des laboratoires publics ou privés . From Averaging to Acceleration , There is Only a Step - size Nicolas Flammarion , Francis Bach To cite this version : Nicolas Flammarion , Francis Bach . From Averaging to Acceleration , There is Only a Step - size . Pro - ceedings of The 28th Conference on Learning Theory , ( COLT ) , 2015 , Paris France . ￿hal - 01136945￿ From Averaging to Acceleration , There is Only a Step - size Nicolas Flammarion and Francis Bach INRIA - Sierra project - team D´epartement d’Informatique de l’Ecole Normale Sup´erieure Paris , France nicolas . flammarion @ ens . fr , francis . bach @ ens . fr April 7 , 2015 Abstract We show that accelerated gradient descent , averaged gradient descent and the heavy - ball method for non - strongly - convex problems may be reformulated as constant pa - rameter second - order diﬀerence equation algorithms , where stability of the system is equivalent to convergence at rate O ( 1 / n 2 ) , where n is the number of iterations . We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical sys - tem , showing various oscillatory and non - oscillatory behaviors , together with a sharp stability result with explicit constants . We also consider the situation where noisy gra - dients are available , where we extend our general convergence result , which suggests an alternative algorithm ( i . e . , with diﬀerent step sizes ) that exhibits the good aspects of both averaging and acceleration . 1 Introduction Many problems in machine learning are naturally cast as convex optimization problems over a Euclidean space ; for supervised learning this includes least - squares regression , logistic regression , and the support vector machine . Faced with large amounts of data , practitioners often favor ﬁrst - order techniques based on gradient descent , leading to algorithms with many cheap iterations . For smooth problems , two extensions of gradient descent have had important theoretical and practical impacts : acceleration and averaging . Acceleration techniques date back to Nesterov ( 1983 ) and have their roots in momentum techniques and conjugate gradient ( Polyak , 1987 ) . For convex problems , with an appropri - ately weighted momentum term which requires to store two iterates , Nesterov ( 1983 ) showed that the traditional convergence rate of O ( 1 / n ) for the function values after n iterations of gradient descent goes down to O ( 1 / n 2 ) for accelerated gradient descent , such a rate being optimal among ﬁrst - order techniques that can access only sequences of gradients ( Nesterov , 2004 ) . Like conjugate gradient methods for solving linear systems , these methods are how - ever more sensitive to noise in the gradients ; that is , to preserve their improved convergence rates , signiﬁcantly less noise may be tolerated ( d’Aspremont , 2008 ; Schmidt et al . , 2011 ; Devolder et al . , 2014 ) . 1 Averaging techniques which consist in replacing the iterates by the average of all iterates have also been thoroughly considered , either because they sometimes lead to simpler proofs , or because they lead to improved behavior . In the noiseless case where gradients are exactly available , they do not improve the convergence rate in the convex case ; worse , for strongly - convex problems , they are not linearly convergent while regular gradient descent is . Their main advantage comes with random unbiased gradients , where it has been shown that they lead to better convergence rates than the unaveraged counterparts , in particular because they allow larger step - sizes ( Polyak and Juditsky , 1992 ; Bach and Moulines , 2011 ) . For example , for least - squares regression with stochastic gradients , they lead to convergence rates of O ( 1 / n ) , even in the non - strongly convex case ( Bach and Moulines , 2013 ) . In this paper , we show that for quadratic problems , both averaging and acceleration are two instances of the same second - order ﬁnite diﬀerence equation , with diﬀerent step - sizes . They may thus be analyzed jointly , together with a non - strongly convex version of the heavy - ball method ( Polyak , 1987 , Section 3 . 2 ) . In presence of random zero - mean noise on the gradients , this joint analysis allows to design a novel intermediate algorithm that exhibits the good aspects of both acceleration ( quick forgetting of initial conditions ) and averaging ( robustness to noise ) . In this paper , we make the following contributions : – We show in Section 2 that accelerated gradient descent , averaged gradient descent and the heavy - ball method for non - strongly - convex problems may be reformulated as constant parameter second - order diﬀerence equation algorithms , where stability of the system is equivalent to convergence at rate O ( 1 / n 2 ) . – In Section 3 , we provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system , showing various oscillatory and non - oscillatory behaviors , together with a sharp stability result with explicit constants . – In Section 4 , we consider the situation where noisy gradients are available , where we extend our general convergence result , which suggests an alternative algorithm ( i . e . , with diﬀerent step sizes ) that exhibits the good aspects of both averaging and acceleration . – In Section 5 , we illustrate our results with simulations on synthetic examples . 2 Second - Order Iterative Algorithms for Quadratic Func - tions Throughout this paper , we consider minimizing a convex quadratic function f : R d → R deﬁned as : f ( θ ) = 1 2 (cid:104) θ , Hθ (cid:105) − (cid:104) q , θ (cid:105) , ( 1 ) with H ∈ R d × d a symmetric positive semi - deﬁnite matrix and q ∈ R d . Without loss of generality , H is assumed invertible ( by projecting onto the orthogonal of its null space ) , though its eigenvalues could be arbitrarily small . The solution is known to be θ ∗ = H − 1 q , 2 but the inverse of the Hessian is often too expensive to compute when d is large . The excess cost function may be simply expressed as f ( θ n ) − f ( θ ∗ ) = 12 (cid:104) θ n − θ ∗ , H ( θ n − θ ∗ ) (cid:105) . 2 . 1 Second - order algorithms In this paper we study second - order iterative algorithms of the form : θ n + 1 = A n θ n + B n θ n − 1 + c n , ( 2 ) started with θ 1 = θ 0 in R d , with A n ∈ R d × d , B n ∈ R d × d and c n ∈ R d for all n ∈ N ∗ . We impose the natural restriction that the optimum θ ∗ is a stationary point of this recursion , that is , for all n ∈ N ∗ : θ ∗ = A n θ ∗ + B n θ ∗ + c n . ( θ ∗ - stationarity ) By letting φ n = θ n − θ ∗ we then have φ n + 1 = A n φ n + B n φ n − 1 , started from φ 0 = φ 1 = θ 0 − θ ∗ . Thus , we restrict our problem to the study of the convergence of an iterative system to 0 . In connection with accelerated methods , we are interested in algorithms for which f ( θ n ) − f ( θ ∗ ) = 12 (cid:104) φ n , Hφ n (cid:105) converges to 0 at a speed of O (cid:0) 1 / n 2 (cid:1) . Within this context we impose that A n and B n have the form : A n = n n + 1 A and B n = n − 1 n + 1 B ∀ n ∈ N with A , B ∈ R d × d . ( n - scalability ) By letting η n = nφ n = n ( θ n − θ ∗ ) , we can now study the simple iterative system with constant terms η n + 1 = Aη n + Bη n − 1 , started at η 0 = 0 and η 1 = θ 0 − θ ∗ . Showing that the function values remain bounded , we directly have the convergence of f ( θ n ) to f ( θ ∗ ) at the speed O (cid:0) 1 / n 2 (cid:1) . Thus the n - scalability property allows to switch from a convergence problem to a stability problem . For feasibility concerns the method can only access H through matrix - vector products . Therefore A and B should be polynomials in H and c a polynomial in H times q , if possible of low degree . The following theorem clariﬁes the general form of iterative systems which share these three properties ( see proof in Appendix B ) . Theorem 1 . Let ( P n , Q n , R n ) ∈ ( R [ X ] ) 3 for all n ∈ N , be a sequence of polynomials . If the iterative algorithm deﬁned by Eq . ( 2 ) with A n = P n ( H ) , B n = Q n ( H ) and c n = R ( H ) q sat - isﬁes the θ ∗ - stationarity and n - scalability properties , there are polynomials ( ¯ A , ¯ B ) ∈ ( R [ X ] ) 2 such that : A n = 2 n n + 1 (cid:18) I − (cid:18) ¯ A ( H ) + ¯ B ( H ) 2 (cid:19) H (cid:19) , B n = − n − 1 n + 1 (cid:18) I − ¯ B ( H ) H (cid:19) and c n = (cid:18) n ¯ A ( H ) + ¯ B ( H ) n + 1 (cid:19) q . Note that our result prevents A n and B n from being zero , thus requiring the algorithm to strictly be of second order . This illustrates the fact that ﬁrst - order algorithms as gradient descent do not have the convergence rate in O ( 1 / n 2 ) . 3 We now restrict our class of algorithms to lowest possible order polynomials , that is , ¯ A = αI and ¯ B = βI with ( α , β ) ∈ R 2 , which correspond to the fewest matrix - vector products per iteration , leading to the constant - coeﬃcient recursion for η n = nφ n = n ( θ n − θ ∗ ) : η n + 1 = ( I − αH ) η n + ( I − βH ) ( η n − η n − 1 ) . ( 3 ) Expression with gradients of f . The recursion in Eq . ( 3 ) may be written with gradients of f in multiple ways . In order to preserve the parallel with accelerated techniques , we rewrite it as : θ n + 1 = 2 n n + 1 θ n − n − 1 n + 1 θ n − 1 − nα + β n + 1 f (cid:48) (cid:18) n ( α + β ) nα + β θ n − ( n − 1 ) β nα + β θ n − 1 (cid:19) . ( 4 ) It may be interpreted as a modiﬁed gradient recursion with two potentially diﬀerent aﬃne ( i . e . , with coeﬃcients that sum to one ) combinations of the two past iterates . This refor - mulation will also be crucial when using noisy gradients . The allowed values for ( α , β ) ∈ R 2 will be determined in the following sections . 2 . 2 Examples Averaged gradient descent . We consider averaged gradient descent ( referred to from now on as “Av - GD” ) ( Polyak and Juditsky , 1992 ) with step - size γ ∈ R deﬁned by : ψ n + 1 = ψ n − γf (cid:48) ( ψ n ) , θ n + 1 = 1 n + 1 n + 1 (cid:88) i = 1 ψ i . When computing the average online as θ n + 1 = θ n + 1 n + 1 ( ψ n + 1 − θ n ) and seeing the average as the main iterate , the algorithm becomes ( see proof in Appendix B . 2 ) : θ n + 1 = 2 n n + 1 θ n − n − 1 n + 1 θ n − 1 − γ n + 1 f (cid:48) (cid:0) nθ n − ( n − 1 ) θ n − 1 (cid:1) . This corresponds to Eq . ( 4 ) with α = 0 and β = γ . Accelerated gradient descent . We consider the accelerated gradient descent ( referred to from now on as “Acc - GD” ) ( Nesterov , 1983 ) with step - sizes ( γ , δ n ) ∈ R 2 : θ n + 1 = ω n − γf (cid:48) ( ω n ) , ω n = θ n + δ n ( θ n − θ n − 1 ) . For smooth optimization the accelerated literature ( Nesterov , 2004 ; Beck and Teboulle , 2009 ) uses the step - size δ n = 1 − 3 n + 1 and their results are not valid for bigger step - size δ n . However δ n = 1 − 2 n + 1 is compatible with the framework of Lan ( 2012 ) and is more convenient for our set - up . This corresponds to Eq . ( 4 ) with α = γ and β = γ . Note that accelerated techniques are more generally applicable , e . g . , to composite optimization with smooth functions ( Nesterov , 2013 ; Beck and Teboulle , 2009 ) . 4 Heavy ball . We consider the heavy - ball algorithm ( referred to from now on as “HB” ) ( Polyak , 1964 ) with step - sizes ( γ , δ n ) ∈ R 2 : θ n + 1 = θ n − γf (cid:48) ( θ n ) + δ n ( θ n − θ n − 1 ) , when δ n = 1 − 2 n + 1 . We note that typically δ n is constant for strongly - convex problems . This corresponds to Eq . ( 4 ) with α = γ and β = 0 . 3 Convergence with Noiseless Gradients We study the convergence of the iterates deﬁned by : η n + 1 = ( I − αH ) η n + ( I − βH ) ( η n − η n − 1 ) . This is a second - order iterative system with constant coeﬃcients that it is standard to cast in a linear framework ( see , e . g . , Ortega and Rheinboldt , 2000 ) . We may rewrite it as : Θ n = F Θ n − 1 , with Θ n = (cid:18) η n η n − 1 (cid:19) and F = (cid:18) 2 I − ( α + β ) H βH − I I 0 (cid:19) ∈ R 2 d × 2 d . Thus Θ n = F n Θ 0 . Following O’Donoghue and Candes ( 2013 ) , if we consider an eigenvalue decomposition of H , i . e . , H = P Diag ( h ) P (cid:62) with P an orthogonal matrix and ( h i ) the eigenvalues of H , sorted in decreasing order : h d = L ≥ h d − 1 ≥ · · · ≥ h 2 ≥ h 1 = µ > 0 , then Eq . ( 3 ) may be rewritten as : P (cid:62) η n + 1 = ( I − α Diag ( h ) ) P (cid:62) η n + ( I − β Diag ( h ) ) (cid:0) P (cid:62) η n − P (cid:62) η n − 1 (cid:1) . ( 5 ) Thus there is no interaction between the diﬀerent eigenspaces and we may consider , for the analysis only , d diﬀerent recursions with η in = p (cid:62) i η n , i ∈ { 1 , . . . , d } , where p i ∈ R d is the i - th column of P : η in + 1 = ( 1 − αh i ) η in + ( 1 − βh i ) (cid:0) η in − η in − 1 (cid:1) . ( 6 ) 3 . 1 Characteristic polynomial and eigenvalues In this section , we consider a ﬁxed i ∈ { 1 , . . . , d } and study the stability in the corresponding eigenspace . This linear dynamical system may be analyzed by studying the eigenvalues of the 2 × 2 - matrix F i = (cid:18) 2 − ( α + β ) h i βh i − 1 1 0 (cid:19) . These eigenvalues are the roots of its characteristic polynomial which is : det ( XI − F i ) = det ( X ( X − 2 + ( α + β ) h i ) + 1 − βh i ) = X 2 − 2 X (cid:16) 1 − (cid:16) α + β 2 (cid:17) h i (cid:17) + 1 − βh i . To compute the roots of the second - order polynomial , we compute its reduced discriminant : ∆ i = (cid:16) 1 − (cid:16) α + β 2 (cid:17) h i (cid:17) 2 − 1 + βh i = h i (cid:16)(cid:16) α + β 2 (cid:17) 2 h i − α (cid:17) . Depending on the sign of the discriminant ∆ i , there will be two real distinct eigenvalues ( ∆ i > 0 ) , two complex conjugate eigenvalues ( ∆ i < 0 ) or a single real eigenvalue ( ∆ i = 0 ) . 5 0 1 2 3 4 0 1 2 AvGD AccGDHB Real Complex βh i αh i Figure 1 : Area of stability of the algorithm , with the three traditional algorithms repre - sented . In the interior of the triangle , the convergence is linear . We will now study the sign of ∆ i . In each diﬀerent case , we will determine under what conditions on α and β the modulus of the eigenvalues is less than one , which means that the iterates ( η in ) n remain bounded and the iterates ( θ n ) n converge to θ ∗ . We may then compute function values as f ( θ n ) − f ( θ ∗ ) = 12 n 2 (cid:80) di = 1 ( η in ) 2 h i = 12 (cid:80) di = 1 ( φ in ) 2 h i . The various regimes are summarized in Figure 1 : there is a triangle of values of ( αh i , βh i ) for which the algorithm remains stable ( i . e . , the iterates ( η n ) n do not diverge ) , with either complex or real eigenvalues . In the following lemmas ( see proof in Appendix C ) , we provide a detailed analysis that leads to Figure 1 . Lemma 1 ( Real eigenvalues ) . The discriminant ∆ i is strictly positive and the algorithm is stable if and only if α ≥ 0 , α + 2 β ≤ 4 / h i , α + β > 2 (cid:112) α / h i . We then have two real roots r ± i = r i ± √ ∆ i , with r i = 1 − ( α + β 2 ) h i . Moreover , we have : ( φ in ) 2 h i = ( φ i 1 ) 2 h i 4 n 2 (cid:2) ( r i + √ ∆ i ) n − ( r i − √ ∆ i ) n (cid:3) 2 ∆ i . ( 7 ) Therefore , for real eigenvalues , ( ( φ in ) 2 h i ) n will converge to 0 at a speed of O ( 1 / n 2 ) however the constant ∆ i may be arbitrarily small ( and thus the scaling factor arbitrarily large ) . Furthermore we have linear convergence if the inequalities in the lemmas are strict . Lemma 2 ( Complex eigenvalues ) . The discriminant ∆ i is stricly negative and the algorithm is stable if and only if α ≥ 0 , β ≥ 0 , α + β < (cid:112) α / h i . We then have two complex conjugate eigenvalues : r ± i = r i ± √− 1 √− ∆ i . Moreover , we have : ( φ i n ) 2 h i = ( φ i 1 ) 2 n 2 sin 2 ( ω i n ) (cid:0) α − ( α + β 2 ) 2 h i (cid:1) ρ 2 n . ( 8 ) with ρ i = √ 1 − βh i , and ω i deﬁned through sin ( ω i ) = √− ∆ i / ρ i and cos ( ω i ) = r i / ρ i . 6 Therefore , for complex eigenvalues , there is a linear convergence if the inequalities in the lemma are strict . Moreover , ( ( φ in ) 2 h i ) n oscillates to 0 at a speed of O ( 1 / n 2 ) even if h i is arbitrarily small . Coalescing eigenvalues . When the discriminant goes to zero in the explicit formulas of the real and complex cases , both the denominator and numerator of ( ( φ in ) 2 h i ) n will go to zero . In the limit case , when the discriminant is equal to zero , we will have a double real eigenvalue . This happens for β = 2 (cid:112) α / h i − α . Then the eigenvalue is r i = 1 −√ αh i , and the algorithm is stable for 0 < α < 4 / h i , we then have ( φ in ) 2 h i = h i ( φ i 1 ) 2 ( 1 − √ αh i ) 2 ( n − 1 ) . This can be obtained by letting ∆ i goes to 0 in the real and complex cases ( see also Appendix C . 3 ) . Summary . To conclude the iterate ( η in ) n = ( n ( θ in − θ i ∗ ) ) n will be stable for α ∈ [ 0 , 4 / h i ] and β ∈ [ 0 , 2 / h i − α / 2 ] . According to the values of α and β this iterate will have a diﬀerent behavior . In the complex case , the roots are complex conjugate with magnitude √ 1 − βh i . Thus , when β > 0 , ( η in ) n will converge to 0 , oscillating , at rate √ 1 − βh i . In the real case , the two roots are real and distinct . However the product of the two roots is equal to √ 1 − βh i , thus one will have a higher magnitude and ( η in ) n will converges to 0 at rate higher than in the complex case ( as long as α and β belong to the interior of the stability region ) . Finally , for a given quadratic function f , all the d iterates ( η in ) n should be bounded , therefore we must have α ∈ [ 0 , 4 / L ] and β ∈ [ 0 , 2 / L − α / 2 ] . Then , depending on the value of h i , some eigenvalues may be complex or real . 3 . 2 Classical examples For particular choices of α and β , displayed in Figure 1 , the eigenvalues are either all real or all complex , as shown in the table below . Av - GD Acc - GD Heavy ball α 0 γ γ β γ γ 0 ∆ i ( γh i ) 2 − γh i ( 1 − γh i ) − γh i ( 1 − γh i 4 ) r ± i 1 , 1 − γh i √ 1 − γh i e ± iω i e ± iω i cos ( ω i ) √ 1 − γh i 1 − γ 2 h i ρ i √ 1 − γh i 1 Averaged gradient descent loses linear convergence for strongly - convex problems , because r + i = 1 for all eigensubspaces . Similarly , the heavy ball method is not adaptive to strong convexity because ρ i = 1 . However , accelerated gradient descent , although designed for non - strongly - convex problems , is adaptive because ρ i = √ 1 − γh i depends on h i while α and β do not . These last two algorithms have an oscillatory behavior which can be observed in practice and has been already studied ( Su et al . , 2014 ) . 7 Note that all the classical methods choose step - sizes α and β either having all the eigenvalues real either complex ; whereas we will see in Section 4 , that it is signiﬁcant to combine both behaviors in presence of noise . 3 . 3 General bound Even if the exact formulas in Lemmas 1 and 2 are computable , they are not easily inter - pretable . In particular when the two roots become close , the denominator will go to zero , which prevents from bounding them easily . When we further restrict the domain of ( α , β ) , we can always bound the iterate by the general bound ( see proof in Appendix D ) : Theorem 2 . For α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α , we have ( η in ) 2 ≤ min (cid:26) 2 ( η i 1 ) 2 αh i , 8 ( η i 1 ) 2 n ( α + β ) h i , 16 ( η i 1 ) 2 ( α + β ) 2 h 2 i (cid:27) . ( 9 ) These bounds are shown by dividing the set of ( α , β ) in three regions where we obtain speciﬁc bounds . They do not depend on the regime of the eigenvalues ( complex or real ) ; this enables us to get the following general bound on the function values , our main result for the deterministic case . Corollary 1 . For α ≤ 1 / L and 0 ≤ β ≤ 2 / L − α : f ( θ n ) − f ( θ ∗ ) ≤ min (cid:26) (cid:107) θ 0 − θ ∗ (cid:107) 2 αn 2 , 4 (cid:107) θ 0 − θ ∗ (cid:107) 2 ( α + β ) n (cid:27) . ( 10 ) We can make the following observations : – The ﬁrst bound (cid:107) θ 0 − θ ∗ (cid:107) 2 αn 2 corresponds to the traditional acceleration result , and is only relevant for α > 0 ( that is , for Nesterov acceleration and the heavy - ball method , but not for averaging ) . We recover the traditional convergence rate of second - order methods for quadratic functions in the singular case , such as conjugate gradient ( Polyak , 1987 , Section 6 . 1 ) . – While the result above focuses on function values , like most results in the non - strongly convex case , the distance to optimum (cid:107) θ n − θ ∗ (cid:107) 2 typically does not go to zero ( although it remains bounded in our situation ) . – When α = 0 ( averaged gradient descent ) , then the second bound 4 (cid:107) θ 0 − θ ∗ (cid:107) 2 ( α + β ) n provides a convergence rate of O ( 1 / n ) if no assumption is made regarding the starting point θ 0 , while the last bound of Theorem 2 would lead to a bound 8 (cid:107) H − 1 / 2 ( θ 0 − θ ∗ ) (cid:107) 2 ( α + β ) 2 n 2 , that is a rate of O ( 1 / n 2 ) , only for some starting points . – As shown in Appendix E by exhibiting explicit sequences of quadratic functions , the inverse dependence in αn 2 and ( α + β ) n in Eq . ( 10 ) is not improvable . 8 0 1 2 3 4 0 1 2 AvGD AccGDOurAlgorithms βh i αh i Figure 2 : Trade - oﬀ between averaged and accelerated methods for noisy gradients . 4 Quadratic Optimization with Additive Noise In many practical situations , the gradient of f is not available for the recursion in Eq . ( 4 ) , but only a noisy version . In this paper , we only consider additive uncorrelated noise with ﬁnite variance . 4 . 1 Stochastic diﬀerence equation We now assume that the true gradient is not available and we rather have access to a noisy oracle for the gradient of f . In Eq . ( 4 ) , we assume that the oracle outputs a noisy gradient f (cid:48) (cid:0) n ( α + β ) nα + β θ n − ( n − 1 ) β nα + β θ n − 1 (cid:1) − ε n + 1 . The noise ( ε n ) is assumed to be uncorrelated zero - mean with bounded covariance , i . e . , E [ ε n ⊗ ε m ] = 0 for all n (cid:54) = m and E [ ε n ⊗ ε n ] (cid:52) C , where A (cid:52) B means that B − A is positive semi - deﬁnite . For quadratic functions , for the reduced variable η n = nφ n = n ( θ n − θ ∗ ) , we get : η n + 1 = ( I − αH ) η n + ( I − βH ) ( η n − η n − 1 ) + [ nα + β ] ε n + 1 . ( 11 ) Note that algorithms with α (cid:54) = 0 will have an important level of noise because of the term nαε n + 1 . We denote by ξ n + 1 = (cid:18) [ nα + β ] ε n + 1 0 (cid:19) and we now have the recursion : Θ n + 1 = F Θ n + ξ n + 1 , ( 12 ) which is a standard noisy linear dynamical system ( see , e . g . , Arnold , 1998 ) with uncorrelated noise process ( ξ n ) . We may thus express Θ n directly as Θ n = F n Θ 0 + (cid:80) nk = 1 F n − k ξ k , and its expected second - order moment as , E (cid:0) Θ n Θ n (cid:1) (cid:62) = F n Θ 0 Θ (cid:62) 0 ( F n ) (cid:62) + (cid:80) nk = 1 F n − k E (cid:0) ξ k ξ (cid:62) k (cid:1) ( F n − k ) (cid:62) . In order to obtain the expected excess cost function , we simply need to compute tr (cid:18) 0 H 0 0 (cid:19) E (cid:0) Θ n Θ n (cid:1) (cid:62) , which thus decomposes as a term that only depends on initial conditions ( which is exactly the one computed and studied in Section 3 . 3 ) , and a new term that depends on the noise . 9 4 . 2 Convergence result For a quadratic function f with arbitrarily small eigenvalues and uncorrelated noise with ﬁnite covariance , we obtain the following convergence result ( see proof in Appendix F ) ; since we will allow the parameters α and β to depend on the time we stop the algorithm , we introduce the horizon N : Theorem 3 ( Convergence rates with noisy gradients ) . With E [ ε n ⊗ ε n ] = C for all n ∈ N , for α ≤ 1 L and 0 ≤ β ≤ 2 L − α . Then for any N ∈ N , we have : E f ( θ N ) − f ( θ ∗ ) ≤ min (cid:26) (cid:107) θ 0 − θ ∗ (cid:107) 2 αN 2 + ( αN + β ) 2 αN tr ( C ) , 4 (cid:107) θ 0 − θ ∗ (cid:107) 2 ( α + β ) N + 4 ( αN + β ) 2 α + β tr ( C ) (cid:27) . ( 13 ) We can make the following observations : – Although we only provide an upper - bound , the proof technique relies on direct moment computations in each eigensubspace with few inequalities , and we conjecture that the scalings with respect to n are tight . – For α = 0 and β = 1 / L ( which corresponds to averaged gradient descent ) , the second bound leads to 4 L (cid:107) θ 0 − θ ∗ (cid:107) 2 N + 4tr ( C ) L , which is bounded but not converging to zero . We recover a result from Bach and Moulines ( 2011 , Theorem 1 ) . – For α = β = 1 / L ( which corresponds to Nesterov’s acceleration ) , the ﬁrst bound leads to L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + ( N + 1 ) tr ( C ) L , and our bound suggests that the algorithm diverges , which we have observed in our experiments in Appendix A . – For α = 0 and β = 1 / L √ N , the second bound leads to 4 L (cid:107) θ 0 − θ ∗ (cid:107) 2 √ N + 4tr ( C ) L √ N , and we recover the traditional rate of 1 / √ N for stochastic gradient in the non - strongly - convex case . – When the values of the bias and the variance are known we can choose α and β such that the trade - oﬀ between the bias and the variance is optimal in our bound , as the following corrollary shows . Note that in the bound below , taking a non zero β enables the bias term to be adaptive to hidden strong - convexity . Corollary 2 . For α = min (cid:110) (cid:107) θ 0 − θ ∗ (cid:107) 2 √ tr CN 3 / 2 , 1 / L (cid:111) and β ∈ [ 0 , min { Nα , 1 / L } ] , we have : E f ( θ N ) − f ( θ ∗ ) ≤ 2 L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + 4 √ tr C (cid:107) θ 0 − θ ∗ (cid:107) √ N . 4 . 3 Structured noise and least - square regression When only the noise total variance tr ( C ) is considered , as shown in Section 4 . 4 , Corollary 2 recover existing ( more general ) results . Our framework however leads to improved result for structured noise processes frequent in machine learning , in particular in least - squares regression which we now consider but this goes beyond ( see , e . g . Bach and Moulines , 2013 ) . 10 Assume we observe independent and identically distributed pairs ( x n , y n ) ∈ R d × R and we want to minimize the expected loss f ( θ ) = 12 E [ ( y n − (cid:104) θ , x n (cid:105) ) 2 ] . We denote by H = E ( x n ⊗ x n ) the covariance matrix which is assumed invertible . The global minimum of f is attained at θ ∗ ∈ R d deﬁned as before and we denote by r n = y n − (cid:104) θ ∗ , x n (cid:105) the statistical noise , which we assume bounded by σ . We have E [ r n x n ] = 0 . In an online setting , we observe the gradient ( x n ⊗ x n ) ( θ − θ ∗ ) − r n x n , whose expectation is the gradient f (cid:48) ( θ ) . This corresponds to a noise in the gradient of ε n = ( H − x n ⊗ x n ) ( θ − θ ∗ ) + r n x n . Given θ , if the data ( x n , y n ) are almost surely bounded , the covariance matrix of this noise is bounded by a constant times H . This suggests to characterize the noise convergence by tr ( CH − 1 ) , which is bounded even though H has arbitrarily small eigenvalues . However , our result will not apply to stochastic gradient descent ( SGD ) for least - squares , because of the term ( H − x n ⊗ x n ) ( θ − θ ∗ ) which depends on θ , but to a “semi - stochastic” recursion where the noisy gradient is H ( θ − θ ∗ ) − r n x n , with a noise process ε n = r n x n , which is such that E [ ε n ⊗ ε n ] (cid:52) σ 2 H , and has been used by Bach and Moulines ( 2011 ) and Dieuleveut and Bach ( 2014 ) to prove results on regular stochastic gradient descent . We conjecture that our algorithm ( and results ) also applies in the regular SGD case , and we provide encouraging experiments in Section 5 . For this particular structured noise we can take advantage of a large β : Theorem 4 ( Convergence rates with structured noisy gradients ) . Let α ≤ 1 L and 0 ≤ β ≤ 32 L − α 2 . For any N ∈ N , E f ( θ N ) − f ( θ ∗ ) is upper - bounded by : min (cid:26) (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 α + ( αN + β ) 2 αβN 2 tr ( CH − 1 ) , 4 L (cid:107) θ 0 − θ ∗ (cid:107) 2 ( α + β ) N + 8 ( αN + β ) 2 tr ( CH − 1 ) ( α + β ) 2 N (cid:27) . ( 14 ) We can make the following observations : – For α = 0 and β = 1 / L ( which corresponds to averaged gradient descent ) , the second bound leads to 4 L (cid:107) θ 0 − θ ∗ (cid:107) 2 N + 8tr ( CH − 1 ) N . We recover a result from Bach and Moulines ( 2013 , Theorem 1 ) . Note that when C (cid:52) σ 2 H , tr ( CH − 1 ) (cid:54) σ 2 d . – For α = β = 1 / L ( which corresponds to Nesterov’s acceleration ) , the ﬁrst bound leads to L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + tr ( CH − 1 ) , which is bounded but not converging to zero ( as opposed to the the unstructured noise where the algorithm may diverge ) . – For α = 1 / ( LN a ) with 0 ≤ a ≤ 1 and β = 1 / L , the ﬁrst bound leads to L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 − a + tr ( CH − 1 ) N a . We thus obtain an explicit bias - variance trade - oﬀ by changing the value of a . – When the values of the bias and the variance are known we can choose α and β with an optimized trade - oﬀ , as the following corrollary shows : Corollary 3 . For α = min (cid:26) (cid:107) θ 0 − θ ∗ (cid:107) √ L tr ( CH − 1 ) N , 1 / L (cid:27) and β = min { Nα , 1 / L } we have : E f ( θ N ) − f ( θ ∗ ) ≤ max (cid:26) 5 tr ( CH − 1 ) N , 5 (cid:112) tr ( CH − 1 ) L (cid:107) θ 0 − θ ∗ (cid:107) N , 2 (cid:107) θ 0 − θ ∗ (cid:107) 2 L N 2 (cid:27) . ( 15 ) 11 4 . 4 Related work Acceleration and noisy gradients . Several authors ( Lan , 2012 ; Hu et al . , 2009 ; Xiao , 2010 ) have shown that using a step - size proportional to 1 / N 3 / 2 accelerated methods with noisy gradients lead to the same convergence rate of O (cid:0) L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + (cid:107) θ 0 − θ ∗ (cid:107) √ tr ( C ) √ N (cid:1) than in Corollary 2 , for smooth functions . Thus , for unstructured noise , our analysis provides insights in the behavior of second - order algorithms , without improving bounds . We get signiﬁcant improvements for structured noises . Least - squares regression . When the noise is structured as in least - square regression and more generally in linear supervised learning , Bach and Moulines ( 2011 ) have shown that using averaged stochastic gradient descent with constant step - size leads to the convergence rate of O (cid:0) L (cid:107) θ 0 − θ 0 (cid:107) 2 N + σ 2 dN (cid:1) . It has been highlighted by D´efossez and Bach ( 2014 ) that the bias term L (cid:107) θ 0 − θ ∗ (cid:107) 2 N may often be the dominant one in practice . Our result in Corollary 3 leads to an improved bias term in O ( 1 / N 2 ) with the price of a potentially slightly worse constant in the variance term . However , with optimal constants in Corollary 3 , the new algorithm is always an improvement over averaged stochastic gradient descent in all situations . If constants are unknown , we may use α = 1 / ( LN a ) with 0 ≤ a ≤ 1 and β = 1 / L and we choose a depending on the emphasis we want to put on bias or variance . Minimax convergence rates . For noisy quadratic problems , the convergence rate nicely decomposes into two terms , a bias term which corresponds to the noiseless problem and the variance term which corresponds to a problem started at θ ∗ . For each of these two terms , lower bounds are known . For the bias term , if N ≤ d , then the lower bound is , up to constants , L (cid:107) θ 0 − θ ∗ (cid:107) 2 / N 2 ( Nesterov , 2004 , Theorem 2 . 1 . 7 ) . For the variance term , for the general noisy gradient situation , we show in Appendix H that for N ≤ d , it is ( tr C ) / ( L √ N ) , while for least - squares regression , it is σ 2 d / N ( Tsybakov , 2003 ) . Thus , for the two situations , we attain the two lower bounds simultaneously for situations where respectively L (cid:107) θ 0 − θ ∗ (cid:107) 2 ≤ ( tr C ) / L and L (cid:107) θ 0 − θ ∗ (cid:107) 2 ≤ dσ 2 . It remains an open problem to achieve the two minimax terms in all situations . Other algorithms as special cases . We also note as shown in Appendix G that in the special case of quadratic functions , the algorithms of Lan ( 2012 ) ; Hu et al . ( 2009 ) ; Xiao ( 2010 ) could be uniﬁed into our framework ( although they have signiﬁcantly diﬀerent formulations and justiﬁcations in the smooth case ) . 5 Experiments In this section , we illustrate our theoretical results on synthetic examples . We consider a matrix H that has random eigenvectors and eigenvalues 1 / k m , for k = 1 , . . . , d and m ∈ N . We take a random optimum θ ∗ and a random starting point θ 0 such that r = (cid:107) θ 0 − θ ∗ (cid:107) = 1 ( unless otherwise speciﬁed ) . In Appendix A , we illustrate the noiseless results 12 0 1 2 3 4 5 −4 −3 −2 −1 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] Structured noisy gradients , d = 20 OAOA−atAGDAccGDAC−SASAGEAccRDA 0 1 2 3 4 5 −4 −2 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] Structured noisy gradients , d = 20 OAOA−atAGDAccGDAC−SASAGEAccRDA Figure 3 : Quadratic optimization with regression noise . Left σ = 1 , r = 1 . Right σ = 0 . 1 , r = 10 . of Section 3 , in particular the oscillatory behaviors and the inﬂuence of all eigenvalues , as well as unstructured noisy gradients . In this section , we focus on noisy gradients with structured noise ( as described in Section 4 . 3 ) , where our new algorithms show signiﬁcant improvements . We compare our algorithm to other stochastic accelerated algorithms , that is , AC - SA ( Lan , 2012 ) , SAGE ( Hu et al . , 2009 ) and Acc - RDA ( Xiao , 2010 ) which are presented in Ap - pendix G . For all these algorithms ( and ours ) we take the optimal step - sizes deﬁned in these papers . We show results averaged over 10 replications . Homoscedastic noise . We ﬁrst consider an i . i . d . zero mean noise whose covariance ma - trix is proportional to H . We also consider a variant of our algorithm with an any - time step - size function of n rather than N ( for which we currently have no proof of convergence ) . In Figure 3 , we take into account two diﬀerent set - ups . In the left plot , the variance domi - nates the bias ( with r = (cid:107) θ 0 − θ ∗ (cid:107) = σ ) . We see that ( a ) Acc - GD does not converge to the optimum but does not diverge either , ( b ) Av - GD and our algorithms achieve the optimal rate of convergence of O ( σ 2 d / n ) , whereas ( c ) other accelerated algorithms only converge at rate O ( 1 / √ n ) . In the right plot , the bias dominates the variance ( r = 10 and σ = 0 . 1 ) . In this situation our algorithm outperforms all others . Application to least - squares regression . We now see how these algorithms behave for least - squares regressions and the regular ( non - homoscedastic ) stochastic gradients described in Section 4 . 3 . We consider normally distributed inputs . The covariance matrix H is the same as before . The outputs are generated from a linear function with homoscedatic noise with a signal - to - noise ratio of σ . We consider d = 20 . We show results averaged over 10 replications . In Figure 4 , we consider again a situation where the bias dominates ( left ) and vice versa ( right ) . We see that our algorithm has the same good behavior than in the homoscedastic noise case and we conjecture that our bounds also hold in this situation . 13 0 1 2 3 4 5 −3 −2 −1 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] Least−Square Regression , d = 20 OAOA−atAGDAC−SASAGEAccRDA 0 1 2 3 4 5 −4 −2 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] Least−Square Regression , d = 20 OAOA−atAGDAC−SASAGEAccRDA Figure 4 : Least - Square Regression . Left σ = 1 , r = 1 . Right σ = 0 . 1 , r = 10 . 6 Conclusion We have provided a joint analysis of averaging and acceleration for non - strongly - convex quadratic functions in a single framework , both with noiseless and noisy gradients . This allows to deﬁne a class of algorithms that can beneﬁt simultaneously of the known improve - ments of averaging and accelerations : faster forgetting of initial conditions ( for acceleration ) , and better robustness to noise when the noise covariance is proportional to the Hessian ( for averaging ) . Our current analysis of our class of algorithms in Eq . ( 4 ) , that considers two diﬀerent aﬃne combinations of previous iterates ( instead of one for traditional acceleration ) , is limited to quadratic functions ; an extension of its analysis to all smooth or self - concordant - like functions would widen its applicability . Similarly , an extension to least - squares regression with natural heteroscedastic stochastic gradient , as suggested by our simulations , would be an interesting development . Acknowledgements This work was partially supported by the MSR - Inria Joint Centre and a grant by the European Research Council ( SIERRA project 239993 ) . The authors would like to thank Aymeric Dieuleveut for helpful discussions . References A . Agarwal , P . L . Bartlett , P . Ravikumar , and M . J . Wainwright . Information - theoretic lower bounds on the oracle complexity of stochastic convex optimization . Information Theory , IEEE Transactions on , 58 ( 5 ) : 3235 – 3249 , 2012 . L . Arnold . Random dynamical systems . Springer Monographs in Mathematics . Springer - Verlag , 1998 . 14 F . Bach and E . Moulines . Non - Asymptotic Analysis of Stochastic Approximation Algo - rithms for Machine Learning . In Advances in Neural Information Processing Systems , 2011 . F . Bach and E . Moulines . Non - strongly - convex smooth stochastic approximation with con - vergence rate O ( 1 / n ) . In Advances in Neural Information Processing Systems , December 2013 . A . Beck and M . Teboulle . A fast iterative shrinkage - thresholding algorithm for linear inverse problems . SIAM J . Imaging Sci . , 2 ( 1 ) : 183 – 202 , 2009 . A . d’Aspremont . Smooth optimization with approximate gradient . SIAM J . Optim . , 19 ( 3 ) : 1171 – 1183 , 2008 . A . D´efossez and F . Bach . Constant step size least - mean - square : Bias - variance trade - oﬀs and optimal sampling distributions . Technical Report 1412 . 0156 , arXiv , 2014 . O . Devolder , F . Glineur , and Y . Nesterov . First - order methods of smooth convex optimiza - tion with inexact oracle . Math . Program . , 146 ( 1 - 2 , Ser . A ) : 37 – 75 , 2014 . A . Dieuleveut and F . Bach . Non - parametric Stochastic Approximation with Large Step sizes . Technical Report 1408 . 0361 , arXiv , August 2014 . C . Hu , W . Pan , and J . T . Kwok . Accelerated gradient methods for stochastic optimization and online learning . In Advances in Neural Information Processing Systems , 2009 . G . Lan . An optimal method for stochastic composite optimization . Math . Program . , 133 ( 1 - 2 , Ser . A ) : 365 – 397 , 2012 . Y . Nesterov . A method of solving a convex programming problem with convergence rate O ( 1 / k 2 ) . Soviet Mathematics Doklady , 27 ( 2 ) : 372 – 376 , 1983 . Y . Nesterov . Introductory Lectures on Convex Optimization , volume 87 of Applied Opti - mization . Kluwer Academic Publishers , Boston , MA , 2004 . A basic course . Y . Nesterov . Gradient methods for minimizing composite functions . Math . Program . , 140 ( 1 , Ser . B ) : 125 – 161 , 2013 . B . O’Donoghue and E . Candes . Adaptive restart for accelerated gradient schemes . Foun - dations of Computational Mathematics , pages 1 – 18 , 2013 . J . M . Ortega and W . C . Rheinboldt . Iterative solution of nonlinear equations in several variables , volume 30 of Classics in Applied Mathematics . Society for Industrial and Applied Mathematics ( SIAM ) , Philadelphia , PA , 2000 . B . T . Polyak . Some methods of speeding up the convergence of iteration methods . { USSR } Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . B . T . Polyak . Introduction to Optimization . Translations Series in Mathematics and Engi - neering . Optimization Software , Inc . , Publications Division , New York , 1987 . 15 B . T . Polyak and A . B . Juditsky . Acceleration of stochastic approximation by averaging . SIAM J . Control Optim . , 30 ( 4 ) : 838 – 855 , 1992 . M . Schmidt , N . Le Roux , and F . Bach . Convergence Rates of Inexact Proximal - Gradient Methods for Convex Optimization . In Advances in Neural Information Processing Sys - tems , December 2011 . W . Su , S . Boyd , and E . Candes . A Diﬀerential Equation for Modeling Nesterov’s Accelerated Gradient Method : Theory and Insights . In Advances in Neural Information Processing Systems , 2014 . A . B . Tsybakov . Optimal rates of aggregation . In Proceedings of the Annual Conference on Computational Learning Theory , 2003 . L . Xiao . Dual averaging methods for regularized stochastic learning and online optimization . J . Mach . Learn . Res . , 11 : 2543 – 2596 , 2010 . A Additional experimental results In this appendix , we provide additional experimental results to illustrate our theoretical results . A . 1 Deterministic convergence Comparaison for d = 1 . In Figure 5 , we minimize a one - dimensional quadratic function f ( θ ) = 12 θ 2 for a ﬁxed step - size α = 1 / 10 and diﬀerent step - sizes β . In the left plot , we compare Acc - GD , HB and Av - GD . We see that HB and Acc - GD both oscillate and that Acc - GD leverages strong convexity to converge faster . In the right plot , we compare the behavior of the algorithm for diﬀerent values of β . We see that the optimal rate is achieved for β = β ∗ deﬁned to be the one for which there is a double coalescent eigenvalue , where the convergence is linear at speed O ( 1 − √ αL ) n . When β > β ∗ , we are in the real case and when β < β ∗ the algorithm oscillates to the solution . Comparison between the diﬀerent eigenspaces . Figure 6 shows interactions between diﬀerent eigenspaces . In the left plot , we optimize a quadratic function of dimension d = 2 . The ﬁrst eigenvalue is L = 1 and the second is µ = 2 − 8 . For Av - GD the convergence is of order O ( 1 / n ) since the problem is “not” strongly convex ( i . e . , not appearing as strongly convex since nµ remains small ) . The convergence is at the beginning the same for HB and Acc - GD , with oscillation at speed O ( 1 / n 2 ) , since the small eigenvalue prevents Acc - GD from having a linear convergence . Then for large n , the convergence becomes linear for Acc - GD , since µn becomes large . In the right plot , we optimize a quadratic function in dimension d = 5 with eigenvalues from 1 to 0 . 1 . We show the function values of the projections of the iterates η n on the diﬀerent eigenspaces . We see that high eigenvalues ﬁrst dominate , but converge quickly to zero , whereas small ones keep oscillating , and converge more slowly . 16 0 50 100 −30 −20 −10 0 n l og 10 [ f ( θ ) − f ( θ * ) ] Minimization of f ( θ ) = θ 2 / 2 AccGD HBAvGD 0 50 100 −100 −50 0 n l og 10 [ f ( θ ) − f ( θ * ) ] Minimization of f ( θ ) = θ 2 / 2 β = 1 . 5 β * β = 0 . 5 β * β = β * Figure 5 : Deterministic case for d = 1 and α = 1 / 10 . Left : classical algorithms , right : diﬀerent oscillatory behaviors . 0 2 4 −30 −20 −10 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] d = 2 , hi = i −8 AccGD HBAvGD 0 10 20 0 0 . 5 1 n f ( η i ) − f ( η * ) d = 5 , α = 0 . 5 , β = 0 . 5 h1 = 0 . 1 h2 = 0 . 3 h3 = 0 . 6 h4 = 0 . 8 h5 = 1 . 0 Figure 6 : Left : Deterministic quadratic optimization for d = 2 . Right : Function value of the projection of the iterate on the diﬀerent eigenspaces ( d = 5 ) . 0 2 4 −20 −15 −10 −5 0 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] d = 20 , hi = i −2 AccGD HBAvGD 0 1 2 3 4 −8 −6 −4 −2 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] d = 20 , hi = i −8 AccGD HBAvGD Figure 7 : Deterministic case for d = 20 and γ = 1 / 10 . Left : m = 2 . Right : m = 8 . Comparison for d = 20 . In Figure 7 , we optimize two 20 - dimensional quadratic functions with diﬀerent eigenvalues with Av - GD , HB and Acc - GD for a ﬁxed step - size γ = 1 / 10 . In the left plot , the eigenvalues are 1 / k 2 and in the right one , they are 1 / k 8 , for k = 1 , . . . , d . We see that in both cases , Av - GD converges at a rate of O ( 1 / n ) and HB at a rate of 17 O ( 1 / n 2 ) . For Acc - GD the convergence is linear when µ is large ( left plot ) and becomes sublinear at a rate of O ( 1 / n 2 ) when µ becomes small ( right plot ) . A . 2 Noisy convergence with unstructured additive noise We optimize the same quadratic function , but now with noisy gradients . We compare our algorithm to other stochastic accelerated algorithms , that is , AC - SA ( Lan , 2012 ) , SAGE ( Hu et al . , 2009 ) and Acc - RDA ( Xiao , 2010 ) , which are presented in Appendix G . For all these algorithms ( and ours ) we take the optimal step - sizes deﬁned in these papers . We plot the results averaged over 10 replications . We consider in Figure 8 an i . i . d . zero mean noise of variance C = I . We see that all the accelerated algorithms achieve the same precision whereas Av - GD with constant step - size does not converge and Acc - Gd diverges . However SAGE and AC - SA are anytime algorithms and are faster at the beginning since their step - sizes are decreasing and not a constant ( with respect to n ) function of the horizon N . 0 1 2 3 4 5 −2 −1 . 5 −1 −0 . 5 0 0 . 5 log 10 ( n ) l og 10 [ f ( θ ) − f ( θ * ) ] Noisy gradients , d = 20 , hi = i 4 OAAvGDAccGDAC−SASAGEAccRDA Figure 8 : Quadratic optimization with additive noise . B Proofs of Section 2 B . 1 Proof of Theorem 1 Let ( P n , Q n , R n ) ∈ ( R [ X ] ) 3 for all n ∈ N be a sequence of polynomials . We consider the iterates deﬁned for all n ∈ N ∗ by θ n + 1 = P n ( H ) θ n + Q n ( H ) θ n − 1 + R ( H ) q , started from θ 0 = θ 1 ∈ R d . The θ ∗ - stationarity property gives for n ∈ N ∗ : θ ∗ = P n ( H ) θ ∗ + Q n ( H ) θ ∗ + R n ( H ) q . 18 Since θ ∗ = H − 1 q we get for all q ∈ R d H − 1 q = P n ( H ) H − 1 q + Q n ( H ) H − 1 q + R n ( H ) q . For all ˜ q ∈ R d we apply this relation to vectors q = H ˜ q : ˜ q = P n ( H ) ˜ q + Q n ( H ) ˜ q + R n ( H ) H ˜ q ∀ ˜ q ∈ R d , and we get I = P n ( H ) + Q n ( H ) + R n ( H ) H ∀ n ∈ N ∗ . Therefore there are polynomials ( ¯ P n , ¯ Q n ) ∈ ( R [ X ] ) 2 and q n ∈ R for all n ∈ N ∗ such that we have for all n ∈ N : P n ( X ) = ( 1 − q n ) I + X ¯ P n ( X ) Q n ( X ) = q n I + X ¯ Q n ( X ) R n ( X ) = − ( ¯ P n ( X ) + ¯ Q n ( X ) ) . ( 16 ) The n - scalability property means that there are polynomials ( P , Q ) ∈ ( R [ X ] ) 2 independent of n such that : P n ( X ) = n n + 1 P ( X ) , Q n ( X ) = n − 1 n + 1 Q ( X ) . And in connection with Eq . ( 16 ) we can rewrite P and Q as : P ( X ) = ¯ p + X ¯ P ( X ) , Q ( X ) = ¯ q + X ¯ Q ( X ) , with ( ¯ p , ¯ q ) ∈ R 2 and ( ¯ P , ¯ Q ) ∈ ( R [ X ] ) 2 . Thus for all n ∈ N : q n = n − 1 n + 1 ¯ q ( 17 ) ¯ Q n ( X ) = n − 1 n Q ( X ) n n + 1 ¯ p = ( 1 − q n ) ( 18 ) ¯ P n ( X ) = n n + 1 P ( X ) . Eq . ( 17 ) and Eq . ( 18 ) give : n n + 1 ¯ p = (cid:18) 1 − n − 1 n + 1 ¯ q (cid:19) . Thus for n = 1 , we have ¯ p = 2 . Then − n − 1 n + 1 ¯ q = 2 n n + 1 − 1 = n − 1 n + 1 and ¯ q = − 1 . Therefore P n ( H ) = 2 n n + 1 I + n n + 1 ¯ P ( H ) H Q n ( H ) = − n − 1 n I + ¯ Q ( H ) H R n ( H ) = − (cid:18) n ¯ P ( H ) + ( n − 1 ) ¯ Q ( H ) n + 1 (cid:19) . 19 We let ¯ A = − ( ¯ P + ¯ Q ) and ¯ B = ¯ Q so that we have : P n ( H ) = 2 n n + 1 (cid:18) I − (cid:18) ¯ A ( H ) + ¯ B ( H ) 2 (cid:19) H (cid:19) Q n ( H ) = − n − 1 n (cid:0) I − ¯ B ( H ) H (cid:1) R n ( H ) = (cid:18) n ¯ A ( H ) + ¯ B ( H ) n + 1 (cid:19) , and with φ n = θ n − θ ∗ for all n ∈ N , the algorithm can be written under the form : φ n + 1 = (cid:20) I − (cid:18) n n + 1 ¯ A ( H ) + 1 n + 1 ¯ B ( H ) (cid:19) H (cid:21) φ n + (cid:18) 1 − 2 n + 1 (cid:19) (cid:2) I − ¯ B ( H ) H (cid:3) ( φ n − φ n − 1 ) . B . 2 Av - GD as two steps - algorithm We show now that when the averaged iterate of Av - GD is seen as the main iterate we have that Av - GD with step - size γ ∈ R is equivalent to : θ n + 1 = 2 n n + 1 θ n − n − 1 n + 1 θ n − 1 − γ n + 1 f (cid:48) (cid:0) nθ n − ( n − 1 ) θ n − 1 (cid:1) . We remind ψ n + 1 = ψ n − γf (cid:48) ( ψ n ) , θ n + 1 = θ n + 1 n + 1 ( ψ n + 1 − θ n ) . Thus , we have : θ n + 1 = θ n + 1 n + 1 ( ψ n + 1 − θ n ) = θ n + 1 n + 1 ( ψ n − γf (cid:48) ( ψ n ) − θ n ) = θ n + 1 n + 1 ( θ n + ( n − 1 ) ( θ n − θ n − 1 ) − γf (cid:48) ( θ n + ( n − 1 ) ( θ n − θ n − 1 ) ) − θ n ) = 2 n n + 1 θ n − n − 1 n + 1 θ n − 1 − γ n + 1 f (cid:48) ( nθ n − ( n − 1 ) θ n − 1 ) . C Proof of Section 3 C . 1 Proof of Lemma 1 The discriminant ∆ i is strictly positive when (cid:0) α + β 2 (cid:1) 2 h i − α > 0 . This is always true for α strictly negative . For α positive and for h i (cid:54) = 0 , this is true for | α + β 2 | > (cid:112) α / h i . Thus the discriminant ∆ i is strictly positive for α < 0 or α ≥ 0 and (cid:110) β < − α − 2 (cid:112) α / h i or β > − α + 2 (cid:112) α / h i (cid:111) . 20 0 1 2 3 4 0 1 2 βh i αh i r + i ≤ 1 r − i ≥ − 1 ∆ i > 0 r + i = 1 r − i = − 1 ∆ i = 0 Figure 9 : Stability in the real case , with all contraints plotted . Then we determine when the modulus of the eigenvalues is less than one ( which corresponds to − 1 ≤ r − i ≤ r + i ≤ 1 ) . r + i ≤ 1 ⇔ (cid:118)(cid:117)(cid:117)(cid:116) h i (cid:32)(cid:18) α + β 2 (cid:19) 2 h i − α (cid:33) ≤ (cid:18) β + α 2 (cid:19) h i ⇔ h i (cid:32)(cid:18) β + α 2 (cid:19) 2 h i − α (cid:33) ≤ (cid:20)(cid:18) β + α 2 (cid:19) h i (cid:21) 2 and α + β 2 ≥ 0 ⇔ h i α ≥ 0 and α + β 2 ≥ 0 ⇔ α ≥ 0 and α + β ≥ 0 . Moreover , we have : r − i ≥ − 1 ⇔ (cid:118)(cid:117)(cid:117)(cid:116) h i (cid:32)(cid:18) β + α 2 (cid:19) 2 h i − α (cid:33) ≤ 2 − (cid:18) β + α 2 (cid:19) h i ⇔ h i (cid:32)(cid:18) β + α 2 (cid:19) 2 h i − α (cid:33) ≤ (cid:20) 2 − (cid:18) β + α 2 (cid:19) h i (cid:21) 2 and 2 − (cid:18) β + α 2 (cid:19) h i ≥ 0 ⇔ h i (cid:32)(cid:18) β + α 2 (cid:19) 2 h i − α (cid:33) ≤ 4 − 4 (cid:18) β + α 2 (cid:19) h i + (cid:20)(cid:18) β + α 2 (cid:19) h i (cid:21) 2 and (cid:18) β + α 2 (cid:19) ≤ 2 / h i ⇔ − h i α ≤ 4 − 4 (cid:18) β + α 2 (cid:19) h i and β 2 ≤ 2 / h i − α 2 ⇔ β ≤ 2 / h i − α / 2 and β ≤ 4 / h i − α . Figure 9 ( where we plot all the constraints we have so far ) enables to conclude that the discriminant ∆ i is strictly positive and the algorithm is stable when the following three 21 conditions are satisﬁed : α ≥ 0 α + 2 β ≤ 4 / h i α + β ≥ 2 (cid:112) α / h 1 . For any of those α et β we will have : η in = c 1 ( r − i ) n + c 2 ( r + i ) n . Since η i 0 = 0 , c 1 + c 2 = 0 and for n = 1 , c 1 = η i 1 / ( r − i − r + i ) ; we thus have : η in = η i 1 2 ( r + i ) n − ( r − i ) n √ ∆ i . Thus , we get the ﬁnal expression : ( φ in ) 2 h i = ( φ i 1 ) 2 4 n 2 (cid:8)(cid:2) r i + √ ∆ i (cid:3) n − (cid:2) r i − √ ∆ i (cid:3) n (cid:9) 2 ∆ i / h i . C . 2 Proof of Lemma 2 0 1 2 3 4 0 1 2 βh i αh i ∆ 1 < 0 | r ± i | ≤ 1 | r ± i | = 1 ∆ i = 0 Figure 10 : Stability in the complex case , with all constraints plotted . The discriminant ∆ i is strictly negative if and only if (cid:0) α + β 2 (cid:1) 2 h i − α < 0 . This implies | α + β 2 | < (cid:112) α / h i . The modulus of the eigenvalues is | r ± i | 2 = 1 − βh i . Thus the discriminant ∆ i is strictly negative and the algorithm is stable for α , β ≥ 0 α + β < (cid:112) α / h i , as shown in Figure 10 . 22 For any of those α et β we have : η in = [ c 1 cos ( ω i n ) + c 2 sin ( ω i n ) ] ρ ni , with ρ i = √ 1 − βh i , sin ( ω i ) = √− ∆ i / ρ i and cos ( ω i ) = r i / ρ i . Since η i 0 = 0 , c 1 = 0 and we have for n = 1 , c 2 = η i 1 / ( sin ( ω i ) ρ i ) . Therefore η in = η i 1 sin ( ω i n ) √− ∆ i ( 1 − βh i ) n / 2 , and ( φ in ) 2 h i = ( φ i 1 ) 2 n 2 sin 2 ( ω i n ) sin 2 ( ω i ) / h i ( 1 − βh i ) n − 1 . C . 3 Coalescing eigenvalues When β = 2 (cid:112) α / h i − α , the discriminant ∆ i is equal to zero and we have a double real eigenvalue : r i = 1 − (cid:112) αh i . Thus the algorithm is stable for α < 4 h i . For any of those α et β we have : η in = ( c 1 + nc 2 ) r n . This gives with η i 0 = 0 , c 1 = 0 and c 2 = η i 1 / r . Therefore η in = nη i ( 1 − (cid:112) αh i ) n − 1 , and : ( φ in ) 2 h i = h i ( φ i 1 ) 2 ( 1 − (cid:112) αh i ) 2 ( n − 1 ) . In the presence of coalescing eigenvalues the convergence is linear if 0 < α < 4 / h i and h i > 0 , however one might worry about the behavior of ( ( φ in ) 2 h i ) n when h i becomes small . Using the bound x 2 exp ( − x ) ≤ 1 for x ≤ 1 , we have for α < 4 / h i : h i ( 1 − (cid:112) αh i ) 2 n = h i exp ( 2 n log ( | 1 − (cid:112) αh i | ) ) ≤ hi exp ( − 2 n min { (cid:112) αh i , 2 − (cid:112) αh i } ) ≤ h i min { √ αh i , 2 − √ αh i } 2 ≤ max (cid:26) 1 α , h i ( 2 − √ αh i ) 2 (cid:27) . Therefore we always have the following bound for α < 4 / h i : ( φ in ) 2 h i ≤ ( φ i 1 ) 2 4 n 2 max (cid:26) 1 α , h i ( 2 − √ αh i ) 2 (cid:27) . Thus for αh i ≤ 1 we get : ( φ in ) 2 h i ≤ ( φ i 1 ) 2 4 n 2 α . 23 D Proof of Theorem 2 D . 1 Sketch of the proof 0 1 2 3 4 0 1 2 βh i αh i Lemma 3 Figure 11 : Validity of Lemma 3 0 1 2 3 4 0 1 2 βh i αh i Lemma 4 Figure 12 : Validity of Lemma 4 0 1 2 3 4 0 1 2 βh i αh i Lemma 5 Figure 13 : Validity of Lemma 5 0 1 2 3 4 0 1 2 βh i αh i Lemma 5 Lemma 4 Lemma 3 Theorem 2 Figure 14 : Area of Theorem 2 We divide the domain of validity of Theorem 2 in three subdomains as explained in Figure 14 . On the domain described in Figure 11 we have a ﬁrst bound on the iterate η in : Lemma 3 . For 0 ≤ α ≤ 1 / h i and 1 − √ 1 − αh i < βh i < 1 + √ 1 − αh i , we have : ( η in ) 2 ≤ ( η i 1 ) 2 αh i . And on the domain described Figure 12 we also have : Lemma 4 . For 0 ≤ α ≤ 1 / h i and β ≤ α we have : ( η in ) 2 ≤ 2 ( η i 1 ) 2 αh i . These two lemmas enable us to prove the ﬁrst bound of Theorem 2 since the domain of this theorem is included in the intersection of the two domains of these lemmas as shown in Figure 14 . Then we have the following bound on domain described in Figure 13 : 24 Lemma 5 . For 0 ≤ α ≤ 2 / h i and 0 ≤ β ≤ 2 / h i − α , we have : | η in | ≤ min (cid:40) 2 √ 2 n (cid:112) ( α + β ) h i , 4 ( α + β ) h i (cid:41) . Since the domain of deﬁnition of Theorem 2 is included in the domain of deﬁnition of Lemma 5 ( as shown in Figure 14 ) , this lemma proves the last two bounds of the theorem . D . 2 Outline of the proofs of the Lemmas – We ﬁnd a Lyapunov function G from R 2 to R such that the sequence ( G ( η in , η in − 1 ) ) decrease along the iterates . – We also prove that G ( η i n , η i n − 1 ) dominates c (cid:107) η i n (cid:107) 2 when we want to have a bound on (cid:107) η in (cid:107) 2 of the form 1 c G ( η i 1 , η i 0 ) = 1 c G ( θ i 0 − θ i ∗ , 0 ) . For readability , we remove the index i and take h i = 1 without loss of generality . D . 3 Proof of Lemma 3 We ﬁrst consider a quadratic Lyapunov function (cid:18) η n η n − 1 (cid:19) (cid:62) G 1 (cid:18) η n η n − 1 (cid:19) with G 1 = (cid:18) 1 α − 1 α − 1 1 − α (cid:19) . We note that G 1 is symmetric positive semi - deﬁnite for α ≤ 1 . We recall F i = (cid:18) 2 − ( α + β ) β − 1 1 0 (cid:19) . For the result to be true we need for 0 ≤ α ≤ 1 and 1 − √ 1 − α < β < 1 + √ 1 − α two properties : F (cid:62) i G 1 F i (cid:52) G 1 , ( 19 ) and α (cid:18) 1 0 0 0 (cid:19) (cid:52) G 1 . ( 20 ) Proof of Eq . ( 20 ) . We have : G 1 − α (cid:18) 1 0 0 0 (cid:19) = ( 1 − α ) (cid:18) 1 1 1 1 (cid:19) (cid:60) 0 for α ≤ 1 . Proof of Eq . ( 19 ) . Since β (cid:55)→ F i ( β ) (cid:62) G 1 F i ( β ) − G 1 is convex in β ( G 1 is symmetric positive semi - deﬁnite ) , we only have to show Eq . ( 19 ) for the boundaries of the interval in β . For x ∈ R ∗ + : (cid:18) x 2 − x x 1 0 (cid:19) (cid:62) (cid:18) 1 − x 2 − x 2 x 2 (cid:19) (cid:18) x 2 − x x 1 0 (cid:19) − (cid:18) 1 − x 2 − x 2 x 2 (cid:19) = − ( 1 − x 2 ) 2 (cid:18) 1 0 0 0 (cid:19) (cid:52) 0 . This especially shows Eq . ( 19 ) for the boundaries of the interval with x = ±√ 1 − α . 25 Bound . Thus , because η 0 = 0 , we have αη 2 n + 1 ≤ Θ (cid:62) n G 1 Θ n ≤ Θ (cid:62) n − 1 G 1 Θ n − 1 ≤ Θ (cid:62) 0 G 1 Θ 0 ≤ η 21 . This shows that for 0 ≤ α ≤ 1 / h i and 1 − √ 1 − αh i < βh i < 1 + √ 1 − αh i : ( η in ) 2 ≤ ( η i 1 ) 2 αh i . D . 4 Proof of Lemma 4 We consider now a second Lyapunov function G 2 ( η n , η n − 1 ) = ( η n − rη n − 1 ) 2 − ∆ ( η n − 1 ) 2 . We have : G 2 ( η n , η n − 1 ) = ( η n − rη n − 1 ) 2 − ∆ η 2 n − 1 = ( rη n − 1 − ( 1 − β ) η n − 2 ) 2 − ∆ η 2 n − 1 = ( r 2 − ∆ ) η 2 n − 1 + ( 1 − β ) 2 η 2 n − 2 − 2 ( 1 − β ) rη n − 1 η n − 2 = ( ( 1 − β ) η 2 n − 1 + ( 1 − β ) ( r 2 − ∆ ) η 2 n − 2 − 2 ( 1 − β ) rη n − 1 η n − 2 = ( 1 − β ) [ ( η n − 1 − rη n − 2 ) 2 − ∆ ( η n − 2 ) 2 ] . = ( 1 − β ) G 2 ( η n − 1 , η n − 2 ) . Where we have used twice r 2 − ∆ = ( 1 − β ) and η n = 2 rη n − 1 − ( 1 − β ) η n − 2 . Moreover G 2 ( η n , η n − 1 ) can be rewritten as : G 2 ( η n , η n − 1 ) = ( 1 − α + β 2 ) ( η n − η n − 1 ) 2 + α − β 2 ( η n − 1 ) 2 + α + β 2 ( η n ) 2 . Thus for α + β ≤ 2 and β ≤ α we have : α 2 ( η n ) 2 ≤ G 2 ( η n , η n − 1 ) = ( 1 − β ) n − 1 G 2 ( η 1 , η 0 ) = ( 1 − β ) n − 1 ( η 1 ) 2 . Therefore for α + β ≤ 2 / h i and β ≤ α , we have : ( η i n ) 2 ≤ 2 ( η i 1 ) 2 αh i . D . 5 Proof of Lemma 5 We may write η n as η n = rη n − 1 + ( r + ) n + ( r − ) n . Moreover , we have : | ( r + ) n + ( r − ) n | ≤ 2 , therefore for α + β ≤ 2 , | η n | ≤ r | η n − 1 | + 2 ≤ 21 − r n 1 − r ≤ 21 − ( 1 − ( α + β 2 ) ) n ( α + β 2 ) . 26 Thus | η n | ≤ 2 ( α + β 2 ) h . Moreover for all u ∈ [ 0 , 1 ] and n ≥ 1 we have 1 − ( 1 − u ) n ≤ √ nu , since 1 − ( 1 − u ) n ≤ 1 and 1 − ( 1 − u ) n = u (cid:80) ( 1 − u ) k ≤ nu . Thus | η n | ≤ 2 √ n (cid:113) ( α + β 2 ) . Therefore for 0 ≤ α ≤ 2 / h i and α + β ≤ 2 / h i we have : | η in | ≤ min (cid:40) 2 √ 2 n (cid:112) ( α + β ) h i , 4 ( α + β ) h i (cid:41) . E Lower bound We have the following lower - bound for the bound shown in Corollary 1 , which shows that depending on which of the two terms dominates , we may always ﬁnd a sequence of functions that makes it tight . Proposition 1 . Let L ≥ 0 . For all sequences 0 ≤ α n ≤ 1 / L and 0 ≤ β n ≤ 2 / L − α n , such that α n + β n = o ( nα n ) there exists a sequence of one - dimensional quadratic functions ( f n ) n with second - derivative less than L such that : lim α n n 2 ( f n ( θ n ) − f n ( θ ∗ ) ) = (cid:107) θ 0 − θ ∗ (cid:107) 2 2 . For all sequences 0 ≤ α n ≤ 1 / L and 0 ≤ β n ≤ 2 / L − α n , such that nα n = o ( α n + β n ) , there exists a sequence of one - dimensional quadratic functions ( g n ) n with second - derivative less than L such that : lim n ( α n + β n ) ( g n ( θ n ) − g n ( θ ∗ ) ) = ( 1 − exp ( − 2 ) ) 2 (cid:107) θ 0 − θ ∗ (cid:107) 2 4 . Proof of the ﬁrst lower - bound . For the ﬁrst lower bound we consider 0 ≤ α n ≤ 1 / L and 0 ≤ β n ≤ 2 / L − α , such that α n + β n = o ( nα n ) . We deﬁne f n = π 2 / ( 4 α n n 2 ) and we consider the sequence of quadratic functions f n ( θ ) = f n θ 2 2 . We consider the iterate ( η n ) n deﬁned by our algorithm . We will show that lim α n f n ( η n ) = η 21 2 . We have , from Lemma 2 , f n ( η n ) = η 2 n f n 2 = η 21 sin 2 ( ω n n ) ρ 2 nn 2 α n ( 1 − π 2 ( α n + β n ) 2 ( 4 α n n ) 2 ) . 27 Moreover , ρ 2 nn = (cid:18) 1 − β n π 2 4 α n n 2 (cid:19) n = exp (cid:18) n log (cid:18) 1 − β n π 2 4 α n n 2 (cid:19)(cid:19) = 1 + o ( 1 ) , since β n α n n = o ( 1 ) . Also , 1 − π 2 ( α n + β n ) 2 ( 4 α n n ) 2 = 1 + o ( 1 ) , since α n + β n = o ( nα n ) . Moreover sin ( ω n ) = √− ∆ n ρ n = √ f n (cid:113) α n − ( α n + β n ) 2 4 f n √ 1 − β n f n = π / ( 2 n ) + o ( 1 / n ) , thus ω n = π / ( 2 n ) + o ( 1 / n ) and sin ( nω n ) = 1 + o ( 1 ) . Proof of the second lower - bound . We consider now the situation where the second bound is active . Thus we take sequences ( α n ) and ( β n ) , such that nα n = o ( α n + β n ) . We deﬁne g n = 2 n ( α n + β n ) + 4 α n ( α n + β n ) 2 and consider the sequence of quadratic functions g n ( θ ) = g n θ 2 2 . We will show for the iterate ( η n ) deﬁned by our algorithm that : lim n ( α n + β n ) ( g n ( θ n ) − g n ( θ ∗ ) ) = ( 1 − exp ( − 2 ) ) 2 (cid:107) θ 0 − θ ∗ (cid:107) 2 4 . We will use Lemma 1 . We ﬁrst have ∆ n = (cid:18) α n + β n 2 (cid:19) 2 g 2 n − α n g n = g n (cid:18) α n + β n 2 (cid:19) 1 n . Thus ( n ∆ n ) / g n = (cid:16) α n + β n 2 (cid:17) and (cid:112) ∆ n = (cid:115)(cid:18) 1 n (cid:19) 2 + 2 α n n ( α n + β n ) = 1 n (cid:114) 1 + 2 α n n α n + β n = 1 n + α n α n + β n + o (cid:18) α n α n + β n (cid:19) . Moreover r n = 1 − α n + β n 2 g n = 1 − 1 n − 2 α n α n + β n . Thus r + = 1 − α n α n + β n + o (cid:18) α n α n + β n (cid:19) , and r n + = exp ( n log ( r + ) ) = exp (cid:18) − nα n α n + β n (cid:19) + o (cid:18) nα n α n + β n (cid:19) = 1 + o ( 1 ) . Furthermore r − = 1 − 2 n − 3 α n α n + β n + o (cid:18) α n α n + β n (cid:19) , 28 and r n − = exp ( n log ( r + ) ) = exp (cid:18) − 2 − 3 α n n α n + β n (cid:19) + o (cid:18) nα n α n + β n (cid:19) = exp ( − 2 ) + o ( 1 ) . Thus ( r n + − r n − ) 2 = ( 1 − exp ( − 2 ) ) 2 + o ( 1 ) . Finally , we have : ( α n + β n ) n [ g n ( θ n ) − g n ( θ ∗ ) ] = α n + β n 2 n (cid:107) θ 0 − θ ∗ (cid:107) 2 [ r n + − r n − ] 2 4∆ n / g n = (cid:107) θ 0 − θ ∗ (cid:107) 2 4 [ r n + − r n − ] 2 = (cid:107) θ 0 − θ ∗ (cid:107) 2 4 ( 1 − exp ( − 2 ) ) 2 + o ( 1 ) . F Proofs of Section 4 F . 1 Proofs of Theorem 3 and Theorem 4 We decompose again vectors in an eigenvector basis of H with η in = p (cid:62) i η n and ε in = p (cid:62) i ε n : η in + 1 = ( 1 − αh i ) η in + ( 1 − βh i ) ( η in − η in − 1 ) + ( nα + β ) ε in + 1 . We denote by ξ in + 1 = (cid:18) [ nα + β ] ε in + 1 0 (cid:19) and we have the reduced equation : Θ in + 1 = F i Θ in + ξ in + 1 . Unfortunately F i is not Hermitian and this formulation will not be convenient for calculus . Without loss of generality , we assume r − i (cid:54) = r + i even if it means having r − i − r + i goes to 0 in the ﬁnal bound . Let Q i = (cid:18) r − i r + i 1 1 (cid:19) be the transfer matrix of F i , i . e . , F i = Q i D i Q − 1 i with D i = (cid:18) r − i 0 0 r + i (cid:19) and Q − 1 i = 1 r − i − r + i (cid:18) 1 − r + i − 1 r − i (cid:19) . We can reparametrize the problem in the following way : Q − 1 i Θ in + 1 = Q − 1 i F i Θ in + Q − 1 i ξ in + 1 = Q − 1 i F i Q i Q − 1 i Θ in + Q − 1 i ξ in + 1 = D i ( Q − 1 i Θ in ) + Q − 1 i ξ in + 1 . With ˜Θ in = Q − 1 i Θ in and ˜ ξ in = Q − 1 i ξ in we now have : ˜Θ in + 1 = D i ˜Θ in + ˜ ξ in + 1 , ( 21 ) with now D i Hermitian ( even diagonal ) . 29 Thus it is easier to tackle using standard techniques for stochastic approximation ( see , e . g . , Polyak and Juditsky , 1992 ; Bach and Moulines , 2011 ) : ˜Θ in = D ni ˜Θ i 0 + n (cid:88) k = 1 D n − k i ˜ ξ ik . Let M i = (cid:32) h 1 / 2 i h 1 / 2 i 0 0 (cid:33) , we then get using standard martingale square moment inequalities , since for n (cid:54) = m , ε in and ε im are uncorrelated ( i . e . , E [ ε in ε im ] = 0 ) : E (cid:107) M i ˜Θ in (cid:107) 2 = (cid:107) M i D ni ˜Θ i 0 (cid:107) 2 + E n (cid:88) k = 1 (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 . This is a bias - variance decomposition ; the left term only depends on the initial condition and the right term only depends on the noise process . We have with M i = (cid:32) h 1 / 2 i h 1 / 2 i 0 0 (cid:33) , M i Q − 1 i = (cid:32) 0 h 1 / 2 i 0 0 (cid:33) , and M i ˜Θ in = (cid:18) √ h i η in 0 (cid:19) . Thus , we have access to the function values through : (cid:107) M i ˜Θ in (cid:107) 2 = h i ( η in ) 2 . Moreover we have Θ i 0 = (cid:18) φ i 1 / ( r − i − r + i ) − φ i 1 / ( r − i − r + i ) (cid:19) . Thus (cid:107) M i D ni ˜Θ i 0 (cid:107) 2 = ( φ i 1 ) 2 h i (cid:0) ( r + i ) n − ( r − i ) n (cid:1) 2 ( r + i − r − i ) 2 . This is the bias term we have studied in Section 3 . 3 which we bound with Theorem 2 . The variance term is controlled by the next proposition . Proposition 2 . With E [ ( ε in ) 2 ] = c i for all n ∈ N , for α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α , we have 1 n 2 E n (cid:88) k = 1 (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 ≤ min (cid:26) 2 ( αn + β ) 2 αβ ( 4 − ( α + 2 β ) h i ) n 2 c i h i , 16 ( nα + β ) 2 n ( α + β ) 2 c i h i , 2 ( αn + β ) 2 nα c i , 8 ( nα + β ) 2 α + β c i (cid:27) . The last two bounds prove Theorem 3 . We note that if we restrict β to β ≤ 3 / ( 2 h i ) − α / 2 , then 4 − ( α + 2 β ) h i ≥ 1 and the ﬁrst bound of Proposition 2 is simpliﬁed to 2 ( αn + β ) 2 αβn 2 c i h i . This allows to conclude to prove Theorem 4 . 30 F . 2 Proof of Corollary 3 We let ν = (cid:107) θ 0 − θ ∗ (cid:107) √ L tr ( CH − 1 ) and consider three diﬀerent regimes depending on ν and L . If ν < 1 / L , we have ν / N < 1 / L and thus α = ν / N and β = ν . Therefore (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 α + ( αN + β ) 2 αβN 2 tr ( CH − 1 ) = (cid:107) θ 0 − θ ∗ (cid:107) 2 νN + 4 tr ( CH − 1 ) N ≤ (cid:112) L tr ( CH − 1 ) (cid:107) θ 0 − θ ∗ (cid:107) N + 4 tr ( CH − 1 ) N ≤ 5 tr ( CH − 1 ) N , where we have used √ L (cid:107) θ 0 − θ ∗ (cid:107) < (cid:112) tr ( CH − 1 ) since ν < 1 / L . If ν > 1 / L and ν < N / L , we have α = ν / N and β = 1 / L . Therefore (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 α + ( αN + β ) 2 αβN 2 tr ( CH − 1 ) ≤ (cid:107) θ 0 − θ ∗ (cid:107) 2 νN + 4 tr ( CH − 1 ) LνN ≤ (cid:112) L tr ( CH − 1 ) (cid:107) θ 0 − θ ∗ (cid:107) N + 4 tr ( CH − 1 ) N ≤ 5 (cid:112) L tr ( CH − 1 ) (cid:107) θ 0 − θ ∗ (cid:107) N , where we have used √ L (cid:107) θ 0 − θ ∗ (cid:107) > (cid:112) tr ( CH − 1 ) since ν > 1 / L . If ν > N / L , we have α = 1 / L and β = 1 / L . Therefore (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 α + ( α ( N − 1 ) + β ) 2 αβN 2 tr ( CH − 1 ) = L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + tr ( CH − 1 ) ≤ L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 + L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 ≤ 2 L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 , where we have used that the real bound in Proposition 2 is in fact in ( N − 1 ) α + β , ( see Lemma 6 ) and that tr ( CH − 1 ) < L (cid:107) θ 0 − θ ∗ (cid:107) 2 N 2 since ν > N / L . F . 3 Proof of Proposition 2 F . 3 . 1 Proof outline To prove Proposition 2 we will use Lemmas 6 , 7 and 8 , that are stated and proved in Section F . 3 . 2 . We want to bound E [ (cid:80) nk = 1 (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 ] and according to Lemma 6 , we have an explicit expansion using the roots of the characteristic polynomial : E (cid:107) M i D ki ˜ ξ ik (cid:107) 2 = h i ( ( k − 1 ) α + β ) 2 E [ ( ε i ) 2 ] [ ( r − i ) n − k − ( r + i ) n − k ] 2 ( r − i − r + i ) 2 . 31 Thus , by bouding ( k − 1 ) α + β by ( n − 1 ) α + β , we get E n (cid:88) k = 1 (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 ≤ h i ( ( n − 1 ) α + β ) 2 E [ ε i 2 ] n (cid:88) k = 1 [ ( r − i ) n − k − ( r + i ) n − k ] 2 ( r − i − r + i ) 2 . ( 22 ) Then , we have from Lemma 7 the inequality : n (cid:88) k = 1 [ ( r − i ) k − ( r + i ) k ] 2 [ ( r − i ) − ( r + i ) ] 2 ≤ 2 − βh i 4 αβh 2 i ( 1 − ( 14 α + 12 β ) h i ) . Therefore E n (cid:88) k = 1 (cid:107) M 1 / 2 i D n − k i ˜ ξ ik (cid:107) 2 ≤ E [ ε i 2 ] h i ( ( n − 1 ) α + β ) 2 4 αβ 2 − βh i ( 1 − ( 14 α + 12 β ) h i ) . This allows to prove the ﬁrst part of the bound . The other parts are much simpler and are done in Lemma 8 . Thus , adding these bounds gives for α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α : 1 n 2 E n (cid:88) k = 1 (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 ≤ min (cid:26) 2 ( α ( n − 1 ) + β ) 2 αβn 2 ( 4 − ( α + 2 β ) h i ) c h i , 16 ( ( n − 1 ) α + β ) 2 n ( α + β ) 2 c h i , 2 ( α ( n − 1 ) + β ) 2 nα c i , 8 ( ( n − 1 ) α + β ) 2 α + β c i (cid:27) . F . 3 . 2 Some technical Lemmas We ﬁrst compute an explicit expansion of the noise term as a function of the eigenvalues of the dynamical system . Lemma 6 . For all α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α we have E (cid:107) M i D ki ˜ ξ ik (cid:107) 2 = h i ( ( k − 1 ) α + β ) 2 E [ ( ε i ) 2 ] [ ( r − i ) n − k − ( r + i ) n − k ] 2 ( r − i − r + i ) 2 . Proof . We ﬁrst turn the Euclidean norm into a trace , using that tr [ AB ] = tr [ BA ] for two matrices A and B and that tr [ x ] = x for a real x . E (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 = Tr D n − k i M i (cid:62) M i D n − k i E [ ˜ ξ ik ( ˜ ξ ik ) (cid:62) ] , ( 23 ) This enables us to separate the noise term from the rest of the formula . Then we compute the latter from the deﬁnition of ˜ ξ ik in Eq . ( 21 ) : E [ ˜ ξ ik ( ˜ ξ ik ) (cid:62) ] = ( ( k − 1 ) α + β ) 2 ( r − i − r + i ) 2 E [ ( ε i ) 2 ] (cid:18) 1 − 1 − 1 1 (cid:19) . And the ﬁrst part of Eq . ( 23 ) is equal to : D n − k i M i (cid:62) M i D n − k i = h i (cid:32) ( r − i ) 2 ( n − k ) ( r − i ) ( n − k ) − ( r + i ) ( n − k ) ( r − i ) ( n − k ) − ( r + i ) ( n − k ) ( r + i ) 2 ( n − k ) (cid:33) , 32 because D i = (cid:18) r − i 0 0 r + i (cid:19) and M i = (cid:32) h 1 / 2 i h 1 / 2 i 0 0 (cid:33) . Therefore : E (cid:107) M i D n − k i ˜ ξ ik (cid:107) 2 = h i ( ( k − 1 ) α + β ) 2 ( r − i − r + i ) 2 E [ ε i 2 ] [ ( r − i ) n − k − ( r + i ) n − k ] 2 . In the following leamma , we bound a certain sum of powers of the roots . Lemma 7 . For all α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α we have n (cid:88) k = 1 (cid:2) ( r − i ) k − ( r + i ) k ] 2 [ ( r − i ) − ( r + i ) (cid:3) 2 ≤ 2 − βh i 4 αβh 2 i ( 1 − ( 14 α + 12 β ) h i ) . We ﬁrst note that when the two roots become close , the denominator and the numerator will go to zero , which prevents from bounding the numerator easily . We also note that this bound is very tight since the diﬀerence between the two terms goes to zero when n goes to inﬁnity . Proof . We ﬁrst expand the square of the diﬀerence of the powers of the roots and compute their sums . n (cid:88) k = 1 (cid:2) ( r − i ) k − ( r + i ) k (cid:3) 2 = n (cid:88) k = 1 (cid:2) r + i 2 k + r − i 2 k − 2 ( r + i r − i ) k (cid:3) = 1 − r + i 2 n 1 − r + i 2 + 1 − r − i 2 n 1 − r − i 2 − 21 − ( r + i r − i ) n 1 − ( r + i r − i ) = 1 1 − r + i 2 + 1 1 − r − i 2 − 2 1 − ( r + i r − i ) − (cid:20) r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 − 2 ( r + i r − i ) n 1 − ( r + i r − i ) (cid:21) = 1 1 − r + i 2 + 1 1 − r − i 2 − 2 1 − ( r + i r − i ) − I n , with I n = (cid:20) r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 − 2 ( r + i r − i ) n 1 − ( r + i r − i ) (cid:21) . This sum is therefore equal to the sum of one term we will compute explicitly and one other term which will go to zero . We have for the ﬁrst term : 1 1 − r + i 2 + 1 1 − r − i 2 − 2 1 − ( r + i r − i ) = ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) − ( 1 − r − i 2 ) ( 1 − r + i 2 ) ( 1 − r + i 2 ) ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) + ( 1 − r + i 2 ) ( 1 − ( r + i r − i ) ) − ( 1 − r − i 2 ) ( 1 − r + i 2 ) ( 1 − r + i 2 ) ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) , 33 with ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) − ( 1 − r − i 2 ) ( 1 − r + i 2 ) = ( 1 − r − i 2 ) [ ( 1 − ( r + i r − i ) ) − ( 1 − r + i 2 ) ] = r + i ( 1 − r − i 2 ) ( r + i − r − i ) , and ( 1 − r + i 2 ) ( 1 − ( r + i r − i ) ) − ( 1 − r − i 2 ) ( 1 − r + i 2 ) = − r − i ( 1 − r − i 2 ) ( r + i − r − i ) , and r + i ( 1 − r − i 2 ) ( r + i − r − i ) − r − i ( 1 − r − i 2 ) ( r + i − r − i ) = ( r + i − r − i ) [ r + i ( 1 − r − i 2 ) − r − i ( 1 − r − i 2 ) ] = ( r + i − r − i ) [ r + i − r − i + r + i r − i ( r + i − r − i ) ] = ( r + i − r − i ) 2 [ 1 + r + i r − i ] . Therefore the ﬁrst term is equal to : 1 1 − r + i 2 + 1 1 − r − i 2 − 2 1 − ( r + i r − i ) = ( r + i − r − i ) 2 [ 1 + r + i r − i ] ( 1 − r + i 2 ) ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) , and the sum can be expanded as : n (cid:88) k = 1 [ ( r − i ) k − ( r + i ) k ] 2 [ r − i − r + i ] 2 = [ 1 + r + i r − i ] ( 1 − r + i 2 ) ( 1 − r − i 2 ) ( 1 − ( r + i r − i ) ) − J n , with J n = I n [ ( r − i ) − ( r + i ) ] 2 . Then we simplify the ﬁrst term of this sum using the explicit values of the roots . We recall r ± i = r i ± √ ∆ i = 1 − α + β 2 h i ± (cid:114)(cid:16) α + β 2 (cid:17) 2 h 2 i − αh i , therefore r + i r − i = r 2 i − ∆ 2 i = (cid:18) 1 − (cid:18) α + β 2 (cid:19) h i (cid:19) 2 − (cid:20)(cid:18) α + β 2 (cid:19) h i (cid:21) 2 + αh i = 1 − βh i , and ( 1 − r + i 2 ) ( 1 − r − i 2 ) = [ ( 1 − r − i ) ( 1 − r + i ) ] [ ( 1 + r + i ) ( 1 + r + i ) ] = [ ( 1 − r i + (cid:112) ∆ i ) ( ( 1 − r i − (cid:112) ∆ i ) ] [ ( 1 + r i + (cid:112) ∆ i ) ( ( 1 + r i − (cid:112) ∆ i ) ] = [ ( 1 − r i ) 2 − ∆ i ] [ ( 1 + r i ) 2 − ∆ i ] [ ( 1 − r i ) 2 − ∆ i ] = 4 αh i (cid:18) 1 − (cid:18) 1 4 α + 1 2 β (cid:19) h i (cid:19) . Thus n (cid:88) k = 1 [ ( r − i ) k − ( r + i ) k ] 2 [ ( r − i ) − ( r + i ) ] 2 = 2 − βh i 4 αβh 2 i ( 1 − ( 14 α + 12 β ) h i ) − J n . 34 Even if J n will be asymptotically small , we want a non - asymptotic bound , thus we will show that J n is always positive . In the real case [ ( r − i ) − ( r + i ) ] 2 ≥ 0 and using a 2 + b 2 ≥ 2 ab , for all ( a , b ) ∈ R 2 , we have r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 ≥ 2 ( r + i r − i ) n (cid:113) ( 1 − r + i 2 ) ( 1 − r − i 2 ) , and using r + i 2 + r − i 2 ≥ 2 r − i r − i we have (cid:113) ( 1 − r + i 2 ) ( 1 − r − i 2 ) ≤ 1 − ( r + i r − i ) , since ( 1 − r + i 2 ) ( 1 − r − i 2 ) − [ 1 − ( r + i r − i ) ] 2 = 1 − r + i 2 − r − i 2 + ( r + i r − i ) 2 − 1 + 2 r + i r − i − ( r + i r − i ) 2 = 2 r + i r − i − r + i 2 − r − i 2 ≤ 0 . Thus r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 − 2 ( r + i r − i ) n 1 − ( r + i r − i ) ≥ 0 . and J n ≥ 0 in the real case . In the complex case , [ ( r − i ) − ( r + i ) ] 2 ≤ 0 , and using z 2 + ¯ z 2 ≤ 2 z ¯ z for all z ∈ C , we have r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 ≤ 2 ( r + i r − i ) n (cid:113) ( 1 − r + i 2 ) ( 1 − r − i 2 ) , and using r + i 2 + r − i 2 ≤ 2 r − i r − i we have (cid:113) ( 1 − r + i 2 ) ( 1 − r − i 2 ) ≥ 1 − ( r + i r − i ) . Thus r + i 2 n 1 − r + i 2 + r − i 2 n 1 − r − i 2 − 2 ( r + i r − i ) n 1 − ( r + i r − i ) ≤ 0 . and J n ≥ 0 in the complexe case . Therefore we always have : J n ≥ 0 , and n (cid:88) k = 1 [ ( r − i ) k − ( r + i ) k ] 2 [ ( r − i ) − ( r + i ) ] 2 ≤ 2 − βh i 4 αβh 2 i ( 1 − ( 14 α + 12 β ) h i ) . 35 However we can also bound roughly Eq . ( 22 ) using Theorem 2 since we recall we have η in = [ ( r − i ) n − k − ( r + i ) n − k ] 2 ( r − i − r + i ) 2 . This gives us the following lemma which enables to prove the second part of Proposition 2 . Lemma 8 . For all α ≤ 1 / h i and 0 ≤ β ≤ 2 / h i − α we have E n (cid:88) k = 1 (cid:107) M 1 / 2 i D n − k i ˜ ξ ik (cid:107) 2 ≤ E [ ( ε i ) 2 ] n ( ( n − 1 ) α + β ) 2 min (cid:26) 2 α , 8 n α + β , 16 h i ( α + β ) 2 (cid:27) . Proof . From Lemma 6 , we get E n (cid:88) k = 1 (cid:107) M 1 / 2 i D n − k i ˜ ξ ik (cid:107) 2 = h i E [ ( ε i ) 2 ] n (cid:88) k = 1 ( ( k − 1 ) α + β ) 2 [ ( r − i ) n − k − ( r + i ) n − k ] 2 ( r − i − r + i ) 2 ≤ h i E [ ( ε i ) 2 ] ( ( n − 1 ) α + β ) 2 n min (cid:26) 2 αh i , 8 n ( α + β ) h i , 16 ( α + β ) 2 h 2 i (cid:27) ≤ E [ ( ε i ) 2 ] n ( ( n − 1 ) α + β ) 2 min (cid:26) 2 α , 8 n α + β , 16 h i ( α + β ) 2 (cid:27) . G Comparison with additional other algorithms G . 1 Summary When the objective function f is quadratic and for correct choices of step - sizes , the AC - SA algorithm of Lan ( 2012 ) , the SAGE algorithm of Hu et al . ( 2009 ) and the Accelerated RDA algorithm of Xiao ( 2010 ) are all equivalent to : θ n + 1 = [ I − δ n + 1 H n + 1 ] θ n + n − 2 n + 1 [ I − δ n + 1 H n + 1 ] ( θ n − θ n − 1 ) + δ n + 1 ε n + 1 , where we use H n θ + ε n as an unbiased estimate of the gradient and δ n as step - size which values will be speciﬁed later . Lan ( 2012 ) and Hu et al . ( 2009 ) only consider bounded cases by projecting their iterates on a bounded space . Xiao ( 2010 ) deals with the unbounded case and prove the following convergence result : Theorem 5 . ( Xiao , 2010 , Theorem 6 ) . With E [ ε n ⊗ ε n ] = C , for step - size δ n ≤ n − 1 n γ with γ ≤ 1 / L , we have E f ( θ n ) − f ( θ ∗ ) ≤ 4 (cid:107) θ 0 − θ ∗ (cid:107) 2 n 2 γ + nγσ 2 tr C 3 . This result is signiﬁcantly more general than ours since it is valid for composite optimization and general noise on the gradients . We now present the diﬀerent algorithms and show they all share the same form . 36 G . 2 AC - SA Lemma 9 . AC - SA algorithm with step size γ n and β n and gradient estimate H n + 1 θ n + ε n + 1 is equivalent to : θ n + 1 = ( I − γ n β n H n + 1 ) θ n + β n − 1 − 1 β n ( I − γ n β n H n + 1 ) ( θ n − θ n − 1 ) + γ n β n ε n + 1 . Proof . We recall the general AC - SA algorithm : • Let the initial points x ag 1 = x 1 , and the step - sizes { β n } n ≤ 1 and { γ n } n ≤ 1 be given . Set n = 1 • Step 1 . Set x mdn = β − 1 n x n + ( 1 − β − 1 n ) x agn , • Step 2 . Call the Oracle for computing G ( x mdn , ξ n ) where E [ G ( x mdn , ξ n ) ] = f (cid:48) ( x mdn ) . Set x n + 1 = x n − γ n G ( x mdn , ξ n ) , x agn + 1 = β − 1 n x n + 1 + ( 1 − β − 1 n ) x agn , • Step 3 . Set n → n + 1 and go to step 1 . When f is quadratic we will have G ( x mdn , ξ n ) = H n + 1 x mdn − ε n + 1 , thus x n + 1 = x n − γ n H n + 1 x mdn + γ n ε n + 1 , and : x agn + 1 = β − 1 n x n + 1 + ( 1 − β − 1 n ) x agn = β − 1 n ( x n − γ n H n + 1 x mdn + γ n ε n + 1 ) + ( 1 − β − 1 n ) x agn = β − 1 n ( β n x mdn + ( 1 − β n ) x agn − γ n H n + 1 x mdn + γ n ε n + 1 ) + ( 1 − β − 1 n ) x agn = x mdn − γ n β n H n + 1 x mdn + γ n β n ε n + 1 , and x mdn = β − 1 n x n + ( 1 − β − 1 n ) x agn = β − 1 n β n − 1 x agn + β − 1 n ( 1 − β n − 1 ) x agn − 1 + ( 1 − β − 1 n ) x agn = x agn + β n − 1 − 1 β n [ x agn − x agn − 1 ] . These give the result for θ n = x agn . G . 3 SAGE Lemma 10 . The algorithm SAGE with step - sizes L n and α n is equivalent to : θ n + 1 = ( I − 1 / L n + 1 H n + 1 ) θ n + ( 1 − α n ) α n + 1 α n [ I − 1 / L n + 1 H n + 1 ] ( θ n − θ n − 1 ) + 1 / L n + 1 ε n + 1 . 37 Proof . We recall the general SAGE algorithm : • Let the initial points x 0 = z 0 = 0 , and the step - sizes { β n } n ≤ 1 and { L n } n ≤ 1 be given . Set n = 1 • Step 1 . Set x n = ( 1 − α n ) y n − 1 + α n z n − 1 , • Step 2 . Call the Oracle for computing G ( x n , ξ n ) where E [ G ( x n , ξ n ) ] = f (cid:48) ( x n ) . Set y n = x n − 1 / L n G ( x n , ξ n ) , z n = z n − 1 − α − 1 n ( x n − y n ) • Step 3 . Set n → n + 1 and go to step 1 . We have y n = ( I − 1 / L n H n ) x n + γ n ε n , and z n = z n − 1 − α − 1 n ( x n − y n ) = z n − 1 − α − 1 n [ ( 1 − α n ) y n − 1 + α n z n − 1 − y n ] = α − 1 n y n − α − 1 n ( 1 − α n ) y n − 1 . Thus x n = ( 1 − α n ) y n − 1 + α n z n − 1 = ( 1 − α n ) y n − 1 + α n [ α − 1 n − 1 y n − 1 − α − 1 n − 1 ( 1 − α n − 1 ) y n − 2 ] = y n − 1 + ( 1 − α n − 1 ) α n α n − 1 [ y n − 1 − y n − 2 ] . These give the result for θ n = y n . G . 4 Accelerated RDA method Lemma 11 . The algorithm AccRDA with step - sizes β and α n is equivalent to : θ n + 1 = ( I − γ n + 1 H n + 1 ) θ n + ( 1 − α n ) α n + 1 α n [ I − γ n + 1 H n + 1 ] ( θ n − θ n − 1 ) + γ n + 1 ε n + 1 , with γ n = α n θ n L + β . Proof . We recall the general Accelerated RDA method : • Let the initial points w 0 = v 0 , A 0 = 0 , ˜ g 0 = 0 and the step - sizes { α n } n ≤ 1 and { β n } n ≤ 1 be given . Set n = 1 38 • Step 1 . Set A n = A n − 1 + α n and θ n = α n A n . • Step 2 . Compute the query point u n = ( 1 − θ n ) w n − 1 + θ n v n − 1 • Step 3 . Call the Oracle for computing g n = G ( u n , ξ n ) where E [ G ( u n , ξ n ) ] = f (cid:48) ( u n ) , and update the weighted average ˜ g n ˜ g n = ( 1 − θ n ) ˜ g n − 1 + θ n g n . • Step 4 . Set v n = v 0 − A n L + β n ˜ g n . • Step 5 . Set w n = ( 1 − θ n ) w n − 1 + θ n v n . • Step 6 . Set n → n + 1 and go to step 1 . First we have v n = v 0 − A n L + β n ˜ g n = v 0 − A n L + β n [ ( 1 − θ n ) ˜ g n − 1 + θ n g n ] = v 0 − A n L + β n [ ( 1 − θ n ) ˜ g n − 1 + θ n ( H n + 1 u n + ε n + 1 ) ] = v 0 + ( 1 − θ n ) A n ( L + β n − 1 ) ( L + β n ) A n − 1 v n − 1 − A n L + β n θ n ( H n + 1 u n + ε n + 1 ) ] = v 0 + ( 1 − θ n ) A n ( L + β n − 1 ) ( L + β n ) A n − 1 v n − 1 − α n L + β n ( H n + 1 u n + ε n + 1 ) ] . With β n = β we have v n = v n − 1 − α n L + β ( H n + 1 u n + ε n + 1 ) ] and w n = ( I − α n θ n L + β H n + 1 ) u n + α n θ n L + β ε n + 1 . Since v n − 1 = θ − 1 n − 1 w n − 1 − θ − 1 n − 1 ( 1 − θ n − 1 ) w n − 2 , then u n = ( 1 − θ n ) w n − 1 + θ n ( θ − 1 n − 1 w n − 1 − θ − 1 n − 1 ( 1 − θ n − 1 ) w n − 2 ) , and u n = w n − 1 + α n A n − 2 α n − 1 A n [ w n − 1 − w n − 2 ] . H Lower bound for stochastic optimization for least - squares In this section , we show a lower bound for optimization of quadratic functions with noisy access to gradients . We follow very closely the framework of Agarwal et al . ( 2012 ) and use 39 their notations . The only diﬀerence with their Theorem 1 in the diﬀerent choice of two functions f + i and f − i , which we choose to be : f ± i ( x ) = c i ( x i ± r 2 ) 2 , with a non - increasing sequence ( c i ) to be chosen later . The function g α that is optimized is thus : g α ( x ) = 1 d d (cid:88) i = 1 (cid:8) ( 1 2 + α i δ ) f + i ( x ) + ( 1 2 − α i δ ) f − i ( x ) (cid:9) . This function is quadratic and its Hessian has eigenvalues equal to 2 c i / d . Thus , its largest eigenvalue is 2 c 1 / d , which we choose equal to L . Noisy gradients are obtained by sampling d independent Bernoulli random variables b i , i = 1 , . . . , d , with parameters ( 1 2 + α i δ ) and using the gradient of the random function 1 d (cid:80) di = 1 (cid:8) b i f + i ( x ) + ( 1 − b i ) f − i ( x ) (cid:9) . The variance of the random gradient is equal to V = d (cid:88) i = 1 1 d 2 var (cid:16) b i (cid:2) c i ( x i + r / 2 ) − c i ( x i − r / 2 ) (cid:3)(cid:17) = 1 d 2 d (cid:88) i = 1 c 2 i r 2 ( 1 / 4 − δ 2 ) . The function g α is minimized for x = − αδr , and the discrepancy measure between two functions g α and g β is greater than 1 d d (cid:88) i = 1 (cid:26) inf x (cid:8) f + i ( x ) + f − i ( x ) (cid:9) − inf x f + i ( x ) − inf x f − i ( x ) (cid:27) 1 α i (cid:54) = β i (cid:62) 1 d d (cid:88) i = 1 3 c i r 2 δ 2 4 1 α i (cid:54) = β i (cid:62) 1 d 3 c d r 2 δ 2 4 ∆ ( α , β ) . Since the vectors α , β ∈ { − 1 , 1 } d are so that their Hamming distance ∆ ( α , β ) (cid:62) d / 4 for α (cid:54) = β , we have a discrepancy measure greater than 3 c d r 2 δ 2 16 . Thus , for a an approxi - mate optimality of ε = c d r 2 δ 2 38 , we have , following the proof of Theorem 1 ( equation ( 29 ) ) from Agarwal et al . ( 2012 ) , for N iterations of any method that accesses a random gradient , we have : 1 / 3 (cid:62) 1 − 216 Ndδ 2 + log 2 d log ( 2 / √ e ) . Thus , for d large , we get , up to constants , δ 2 (cid:62) 1 / N and thus ε (cid:62) r 2 c d N . For c 1 = 2 Ld and c i = L √ d for the remaining ones , we get ( up to constants ) : ε (cid:62) V L √ d N . This leads to the desired result for N (cid:54) d . 40