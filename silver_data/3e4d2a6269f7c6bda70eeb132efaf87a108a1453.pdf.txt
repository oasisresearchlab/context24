Working Paper 2023 / 70 / STR Working Paper is the author’s intellectual property . It is intended as a means to promote research to interested readers . Its content should not be copied or hosted on any server without written permission from publications . fb @ insead . edu Find more INSEAD papers at https : / / www . insead . edu / faculty - research / research Copyright © 2023 INSEAD How Groups Differ from Individuals in Learning from Experience : Evidence from a Contest Platform Tianyu He * National University of Singapore , t . he @ nus . edu Marco Minervini * IE Business School , IE University , marco . minervini @ ie . edu Phanish Puranam INSEAD , Phanish . puranam @ insead . edu * The first two authors contributed equally to this work ; the order is alphabetical . Forthcoming in Organization Science We examine how groups differ from individuals in how they tackle two fundamental trade - offs in learning from experience – namely , between exploration and exploitation , and between over - and under - generalization from noisy data ( which is also known as the “bias - variance” trade - off in the machine learning literature ) . Using data from an online contest platform ( Kaggle ) featuring groups and individuals competing on the same learning task , we found that groups , as expected , not only generate a larger aggregate of alternatives but also explore a more diverse range of these alternatives compared to individuals , even when accounting for the greater number of alternatives . However , we also discovered that this abundance of alternatives may make groups struggle more than individuals at generalizing the feedback they receive into a valid understanding of their task environment . Building on these findings , we theorize about the conditions under which groups may achieve better learning outcomes than individuals . Specifically , we propose a self - limiting nature to the group advantage in learning from experience : the group advantage in generating alternatives may result in potential disadvantages in the evaluation and selection of these alternatives . Keywords : Learning - by - doing Cycle ; Aggregation ; Organizational Learning Electronic copy available at : https : / / ssrn . com / abstract = 4663547 Acknowledgements The authors extend their sincere gratitude to Vikas A . Aggarwal , Piyush Gulati , Arianna Marchetti , Ella Miron - Spektor , Sanghyun Park , Henning Piezunka , Roderick Swaab , Stefan Thau , and Andy Yap for their insightful comments on various drafts of this paper and to Astha Jakher for her research assistance . Appreciation is also directed towards the editor and the anonymous reviewers for their constructive feedback . The authors acknowledge the participants of the 2020 remote research sprint group for their invigorating early challenges , as well as the seminar attendees from the University of Pittsburgh and Case Western University . Furthermore , heartfelt thanks are extended to The INSEAD Ian Potter ’93D PhD Award and The Desmarais Fund at INSEAD for their generous support towards the first two authors . How Groups and Individuals Differ in Learning from Experience Learning , a change in beliefs and behavior as a consequence of experience , occurs at both individual and organizational levels ( Argote 2013 , Argote and Miron - Spektor 2011 , Crossan et al . 1999 , Edmondson 2002 ) . However , the details of how a collection of minds differs from individuals in terms of learning from experience ( a . k . a . , learning by doing ) remain poorly understood . This is despite the fact that the respective literatures on group and individual learning is each voluminous ( Argote 2013 , p . 137 – 138 ) , as is the literature comparing individuals and groups in decision making and problem solving in general ( Hill 1982 , Kerr et al . 1996 ) . The limited number of empirical studies to date that compare learning from feedback in groups and individuals paint an ambiguous picture . Some studies have found groups to outperform individuals in learning tasks , attributed to their higher cognitive capacity ( Gabbert et al . 1986 , Kirschner et al . 2009 , Michaelsen et al . 1989 ) and their ability to aggregate not merely information but also their evolving understanding of the task environment ( Charness et al . 2007 , Cooper and Kagel 2005 , Kocher and Sutter 2007 , Laughlin and Barth 1981 , Watson et al . 1991 ) . However , others have reported that groups are slower to learn about changed task conditions than individuals ( Lejarraga et al . 2014 ) . Yet others report no performance differences between groups and individuals in learning tasks ( Davis et al . 1970 , Hart et al . 2017 ) , with no evidence of a group advantage even in tasks where the aggregation of understanding about the task environment was in principle valuable ( Chen et al . 2004 , Schulze et al . 2020 ) . These mixed findings suggest a need to perhaps step back from thinking directly about differences in performance between groups and individuals on learning tasks , and instead rethink the basic mechanisms that may lead to differences in group and individual learning from experience . We believe such an attempt is important because groups are basic building blocks in complex organizations ( Hackman 2003 , Mowday and Sutton 1993 , Puranam 2018 ) , and are often the locus of crucial decisions ( Christensen et al . 2021 , Csaszar and Eggers 2013 ) that involve iterative problem - solving in a noisy environment . Advancing our understanding of organizational learning requires us to understand the nature of this difference ( Argote et al . 2020 ) . In this paper , we exploit an empirical context that allows us to examine how groups and individuals differ by offering us observations of how they differ in two distinct stages of learning 2 Groups and Individuals in Learning from Experience from experience : alternative generation and alternative evaluation ( Bunderson and Sanner 2017 on divergence and convergence in learning , as well as Gersick 1989 , Knudsen and Levinthal 2007 ) . Each of these stages is associated with a distinctive trade - off . The first trade - off , well known in organization science , involves balancing exploration and exploitation during the stage of gathering experience about different alternatives ( i . e . , alternative generation ) ( see Billinger et al . 2021 for a recent review , Holland 1975 , March 1991 ) . Alternative generation requires taking costly actions to receive feedback and requires finding a balance between the potential for new insight against the opportunity costs of underutilizing what is already known . This trade - off arises whenever payoff surfaces are multi - peaked , presenting risks of missed opportunities ( Levinthal 1997 ) . For instance , when developing new products by testing prototypes , one must balance the benefits of possibly discovering new and vastly better products against the costs of failing to leverage incremental but still beneficial improvements on existing products . The second trade - off pertains to the stage of alternative evaluation , and is well recognized in the machine learning literature as the bias - variance trade - off . This trade - off arises whenever one has to extract insights about the task environment based on noisy ( even if unbiased ) feedback . It involves formulating an understanding of the task environment that neither underfits nor overfits the gathered experience ( i . e . , the various alternatives generated and the feedback received about them ) ( Hastie et al . 2009 ) . For instance , when predicting demand for potential new products , one must consider that the customer feedback collected during the prototype testing could be a noisy indicator of actual demand . Placing too much reliance on this information can be problematic – we cannot disregard everything we learn , but if we strictly adhere to the preferences of those who participated in the protype test , we may end up releasing a product that is only appealing to a sliver of the target market . While prior literature leads us to expect that groups may produce more numerous and varied alternatives compared to individuals , their relative performance on the second trade - off remains unclear . We examine the evidence on these questions using data from a contest platform called “Kaggle” , which features large numbers of groups and individuals competing on the same learning 3 Groups and Individuals in Learning from Experience task . Kaggle competitions require contestants , as groups or as individuals , to apply statistical techniques to analyze data , test various models , and receive feedback in a trial - and - error process ( alternative generation ) , followed by selecting a limited number of models for final submission ( alternative evaluation ) . The models submitted by different contestants are compared on their predictive power in a hold - out sub - sample of the same dataset they had used previously . The structured process of Kaggle competitions allows for clear measurement of how groups and individuals tackle the exploration - exploitation and bias - variance trade - offs , with identifiable stages of alternative generation and evaluation , strong incentives ( i . e . , significant prize money ) , controlled learning tasks ( i . e . , all contestants in a particular competition face an identical learning problem ) , and a common metric of success at learning ( i . e . , predictive accuracy in hold - out sample at the final stage of the contest ) . However , a key limitation with this context is that while the contests feature both group and individual learners , these are not randomly assigned conditions . We attempt to account for this statistically through matching procedures . Our findings indicate that groups in our study , on average , explore more , as evidenced by the greater variety of alternatives produced , even after controlling for the number of alternatives , compared to individuals . However , while there is some evidence that groups are less likely to overfit ( i . e . , placing too much reliance on existing feedback ) relative to individuals when evaluating alternatives , this tendency diminishes when groups evaluate a large number of alternatives . This suggests a self - limiting nature to the group advantage in learning from experience . Groups may excel in generating many and more varied alternatives during the alternative generation stage , but they may not surpass individuals in terms of learning from experience . This is because the larger and more varied set of alternatives they generate can cause groups to struggle more with alternative evaluation . Put simply , the success of groups at alternative generation ( relative to individuals ) may diminish their capacity for alternative evaluation . Combined with properties of the task environment ( such as the number of local peaks and the noisiness of feedback inherent to a task environment ) , we believe this 4 Groups and Individuals in Learning from Experience self - limiting property may help explain why in prior literature , groups sometimes appear advantaged with respect to individuals on some learning tasks , but not always . Two Stages of Learning and Associated Trade - offs We begin by noting that all learning entails a change in beliefs , potentially followed by a change in behaviour ( Argote 2013 , Argote and Miron - Spektor 2011 ) . This definition of learning is shared across streams of research on learning at individual , group and organizational levels ( Argote et al . 2020 , Crossan et al . 1999 , Daft and Weick 1984 , Ellis et al . 2003 , Kolb 2015 ) . We also take the view that learning is a process whose results in terms of changes in beliefs and behaviours may or may not be beneficial to the learner ( Levinthal and March 1993 , March 1991 ) , even though the goal of learning is often to improve performance ( Edmondson 2002 ) . We draw on a useful distinction between two stages of learning from experience established in prior work : the generation and evaluation of alternatives ( Bunderson and Sanner 2017 , Knudsen and Levinthal 2007 ) . In the alternative generation ( or experience gathering ) stage , a learner ( either an individual or a group ) can take a series of actions and receive feedback on these actions , update beliefs and choose actions again . This collection of chosen actions and their associated feedback constitute experience ( Knudsen and Levinthal 2007 ) . As is typical in the organizational learning literature inspired by the Carnegie school ( Argote 2013 , p . 201 , Cyert and March 1963 , Simon 1947 , 1958 ) , we conceptualize the alternative generation stage as involving repeated cycles of ( 1 ) taking actions ; ( 2 ) receiving feedback on outcomes to those actions and ( 3 ) updating beliefs and possibly behaviors . This basic cycle underlying learning - by - doing , namely of beliefs → actions → feedback → ( updating ) beliefs → actions… underlies various previous accounts of learning from experience , differing primarily in which part of the cycle they emphasize . Many have emphasized the connection between feedback and belief revision , such as knowledge creation , transfer and retention when discussing organizational learning ( e . g . Argote et al . 2020 ) or individual learning by students ( e . g . Kirschner et al . 2009 ) , and how various group cognitions ( e . g . , team mental model ) may moderate this 5 Groups and Individuals in Learning from Experience process ( e . g . Edmondson et al . 2007 , Ellis 2006 ) . Others have emphasized the feedback generation process ( Edmondson 1999 , Edmondson et al . 2001 ) between actions and feedback – which depends crucially on properties of the focal learner’s task environment ( Edmondson 1996 ) , and may in fact include other learners ( Bunderson and Boumgarden 2010 ) . Yet others have highlighted that the experience of success and failure both lead to learning albeit through different rates and processes of belief updating ( Ellis and Davidi 2005 , Moreland and McMinn 2010 ) . Formal theories have also been developed to represent abstract versions of the learning - by - doing cycle using computational models of reinforcement learning ( See Puranam et al . 2015 for a review ) . At some point the learning - by - doing cycles come to an end , typically because of time constraints ( e . g . , a scheduled date for launching an official product after cycles of prototyping ) . In the alternative evaluation stage , learners consolidate their experiences to finalize an understanding of the environment 1 . This understanding can range from the learner’s general causal interpretations of how things work ( e . g . , technological advancement suggests changes are needed in marketing methods ) to specific solutions to problems at hand ( e . g . , hiring social media influencers can increase access to young customers ) . These insights are consequential as they guide future plans and the course of actions . The accuracy of this understanding is a key measure of the performance of the learning process . Two fundamental trade - offs associated with alternative generation and evaluation have been noted by scholars in widely different disciplines . We describe these below . 1 In principle , the available experience for learners may include not only that obtained in the form of feedback on the learner’s own actions , but also what is obtained more passively and indirectly from the experience of others ( e . g . vicarious learning ) ( Bresman 2010 , Darr et al . 1995 ) . However , adding to the available stock of information through feedback on own actions is a distinctive and central feature of learning from experience - the “doing” is salient in the notion of “learning by doing” , which is often used synonymously with learning from experience . Indeed , recent thinking suggests that even vicarious learning is not a passive process , with learners having to proactively take actions ( Myers 2018 ) . Further even vicarious learning may ultimately leverage somebody else’s learning by doing ( e . g . , Park and Puranam , 2023 ) . 6 Groups and Individuals in Learning from Experience Exploration - Exploitation Trade - off in Alternative Generation First , because learning from experience involves action and feedback , and actions have costs , a trade - off between exploration and exploitation arises . This has been recognized in diverse literatures in complex adaptive systems , organization science and statistical learning ( Crossan et al . 1999 , Gittins 1979 , Holland 1975 , March 1991 ) . Given that actions incur costs , it seems intuitive to align them with our current understanding to avoid inefficiencies and make the most of what already we know ( i . e . , exploitation ) . However , this can prevent the discovery of superior actions when the current understanding fails to accurately characterize a multi - peaked task environment ( Levinthal 1997 ) . An alternative that seems good may not necessarily be the best or even particularly good – it may just be a local peak . This disadvantage to exploiting also arises if the task environment is unstable and has evolved away from what we believe . For instance , consider a scenario where , based on past market prototype tests , Product A consistently receives positive reviews , but nothing is known about other product variants . If we keep testing and refining Product A ( exploit what we know ) and never test other different products ( explore ) , we might never learn that a hypothetical Product C is dramatically more appealing than A to customers . On the other hand , if we explore all the time , then we are likely testing many bad products instead of using the opportunities to receive feedback on how to improve those that are known to be relatively good . The trade - off therefore is between restricting actions only to those deemed most promising based on the feedback gathered so far , or also including “wildcards” that may or may not surpass current front - runners ( Denrell 2005 , Denrell and March 2001 , March 1991 ) . Learners who explore more therefore have a more varied set of experiences and effectively generate a more varied set of alternatives over time , as compared to those who explore less ( Billinger et al . 2021 ) . Bias - Variance Trade - off in Alternative Evaluation Second , evaluating the gathered experiences and finalizing insights from them at the alternative evaluation stage is also not a trivial matter . Since the task environment often produces noisy feedback 7 Groups and Individuals in Learning from Experience ( Levinthal and March 1993 , Puranam and Swamy 2016 ) , interpreting and distinguishing valid insights from invalid ones poses a second trade - off in learning from experience ( Argote 2013 Chapter 5 , Denrell and March 2001 , Levinthal and March 1993 ) . Suppose no further opportunities exist to refine one’s understanding by gathering experience ( there is no further exploration - exploitation trade - off to consider ) . Should we consider the experience gathered so far ( i . e . , the relationship between actions and their feedback ) as an accurate representation of the task environment ? The answer , surprisingly , may be “no” . If the feedback contained in experience is noisy , there is a risk of overfitting ( i . e . , overgeneralizing ) from experience . We may develop interpretations that closely explain feedback observed so far , yet fail as a basis for future actions since they model too closely the idiosyncratic noise in the experience gathered . The trade - off between not weighting current experience closely enough ( underfitting ) and too closely ( overfitting ) is known as the bias - variance trade - off in the literature on machine learning ( Hastie et al . 2009 ) . Here , “bias” refers to errors arising from underfitting experience , while “variance” denotes errors in applying the insights to guide future actions in the task environment ( Choi and Levinthal 2022 ) 2 . This challenge is universal to situations when learners need to extrapolate patterns from existing noisy data for future use . For instance , when deciding on a product to launch officially , it is important to evaluate if the feedback from prototype testers accurately represents the preferences of the broader market . Even with representative sampling , some level of sampling error is inevitable . It may well be that while product Version A have received the most favourable reviews among all tested versions , the formal launch reveals instead that Version B , which received fewer positive ratings , could actually be more 2 While the exploration - exploitation trade - off is widely understood in organization science since it was popularized by March ( 1991 ) , the bias - variance trade - off has received less explicit attention . In part , this may be because in many situations of managerial decision making under uncertainty , incentives encourage managers to take actions that maximize cumulative payoffs ( which implies an exploration - exploitation trade - off ) even , if necessary , at the expense of an accurate understanding the task environment ( which entails the bias - variance trade - off ) . Nonetheless , the risks of superstitious learning ( Levinthal and March 1993 , Puranam and Swamy 2016 ) and inappropriate generalization from experience ( e . g . Haleblian and Finkelstein 1999 , Kolev and Haleblian 2018 , Zollo and Winter 2002 ) are familiar and closely related to the bias - variance trade - off . In fact they reflect stronger versions of the bias - variance trade - off in the sense that feedback from a non - comparable population may be overfit , creating bias and not merely noise . 8 Groups and Individuals in Learning from Experience popular . In other words , the most positively reviewed product version also captures idiosyncratic aspects of the testers or the testing procedures , producing noisy measures . The challenge lies in the extent to which a learner should rely on gathered experience to inform future actions without overly or insufficiently using it . When a learner deviates from the best action revealed by known experience , and opts , counterintuitively , for a seemingly inferior action in the evaluation stage , we can say they are avoiding overfitting . 3 While alternative evaluation is contingent on alternatives generated , it is unclear if the nature and diversity of these alternatives play a systematic role in shaping the tendency to overfit by learners . When presented with a set of alternatives exhibiting high variance , selecting the best might seem more straightforward due to the distinct feedback . However , these alternatives might also present distinct qualitative differences , making the best alternative appear as a unique outlier . Therefore , while the nature and diversity of the alternatives can vary , we do not have reasons to believe that this variance would make a learner more or less inclined to select the best alternative discovered during the generation phase . Groups vs Individuals : How do they tackle learning trade - offs ? Groups differ from individuals in two distinct ways . First , groups consist of more individuals , inherently possessing a greater scale of cognitive resources . In addition , and more critically , groups contain greater heterogeneity in those cognitive resources , as the individuals within them are not identical . These two distinctions between groups and individuals point to different ways that they may tackle the two fundamental trade - offs embedded in learning from experience . Exploration by Groups vs . Individuals During the alternative generation stage , the greater scale of cognitive resources available in a group allows the multiple individuals to take a greater number of actions to gather experience , as 3 This also includes situations where a learner might finalize on an action for future that has never received any feedback ( i . e . , an alternative that was never tested ) if they deem all known actions as unsatisfactory . 9 Groups and Individuals in Learning from Experience more individuals can simply exert more ( cognitive ) efforts towards generating them . The greater scale of cognitive resources available in a group can manifest as better memory and more information processing capacity . For instance , groups have been found to outperform individuals reliably on letters - to - numbers tasks ( e . g . Laughlin et al . 2003 ) in which performance scales with total cognitive effort . Additionally , the presence of others may also motivate more effort from members than completing tasks alone ( Hertel et al . 2000 , Paulus et al . 2018 ) . Consequently , a mechanical increase in effort during alternative generation can be expected in groups relative to individuals . This scale advantage should persist as long as each individual can act and learn independently in the task environment as opposed to the group taking a singular collective action ( Piezunka et al . 2022 ) . Further , the greater heterogeneity of cognitive resources within a group suggest that there is invariably some diversity of information , the way it is processed and represented , and preferences across individuals in a group . These differences can create more diverse opinions in groups ( De Dreu and Nijstad 2008 , Hüffmeier and Hertel 2011 , van Knippenberg et al . 2004 , Nijstad and Stroebe 2006 ) . The distinct ways that group members acquire and process information further aid this diversity ( Martins and Sohn 2021 ) . Indeed , teams with members of diverse cognitive styles have been found to employ a wider array of approaches and priorities to solve the same problems ( Aggarwal et al . 2015 , 2019 , Aggarwal and Woolley 2013 ) . This could allow multiple , qualitatively different , forms of experience to be gathered in a way that would not have occurred in a single mind ( Page 2014 ) . Research on brainstorming corroborates this argument . Compared to groups with the presence of social influence from others , nominal groups ( where members do not interact ) that preserve the distinct perspectives of each individual are often able to produce more novel ideas ( e . g . Dunnette et al . 1963 , Rietzschel et al . 2006 , Taylor et al . 1958 ) . It is certainly possible that free - riding within groups may lower efforts relative to the best possible scenario ( Karau and Williams 1993 , e . g . Kollock 1994 ) . However , to lower collective effort by a group to below that of an individual seems implausible except in cases of egregious incentive misalignment . While the heterogeneity in groups may also diminish in the process of group member 10 Groups and Individuals in Learning from Experience interaction due to frictions in communication ( Nijstad and Stroebe 2006 , Stroebe and Diehl 1994 ) , interpersonal influence that leads to similarity in ideas due to conformity ( Paulus et al . 2018 , Paulus and Yang 2000 ) or copying ( Shore et al . 2015 ) , it is unlikely that the diversity in thinking would become lower than what is available to one single mind . Existing literature thus strongly suggests that a group , compared to one individual , is likely to explore more during alternative generation in learning from experience . This should manifest as groups generating a greater variety , and not just number , of alternatives compared to individuals ( Billinger et al . 2021 , Mason and Watts 2012 ) . Overfitting by Groups vs . Individuals When it comes to the question of how individuals may differ from groups in managing the bias - variance trade - off , we are on less firm ground . Some research suggests that individuals alone are more prone to utilizing existing noisy experience inappropriately when applying it to unknown situations . This often manifests as applying the same principles across contexts that differ in their nature – a form of overfitting their past experiences – leading to inaccurate understanding of unfamiliar situations ( Epley et al . 2004 , Krueger et al . 2006 , Ross et al . 1977 ) . Individuals frequently develop learned helplessness , for instance , where they generalize experienced failures from one context to novel situations where they face new problems , disregarding the differences in each distinct situation ( Klein et al . 1976 ) . Groups , while also susceptible to overfitting ( Finkelstein and Haleblian 2002 , Haleblian and Finkelstein 1999 ) , might be better equipped to avoid it due to the need for compromise and mutual adjustment among group members with diverse perspectives . The process to reach “a revised understanding of the task that is shared by all” ( Bunderson and Sanner 2017 , p . 3 ) may safeguard groups against overfitting when converging to a finalized belief ( Hong and Page 2004 ) . First of all , having to accommodate different preferences can result in error cancellation through aggregation of individual judgments ( Larrick et al . 2012 ) . If groups could agree to an aggregation rule ( e . g . , average ) , they could converge on how to update beliefs based on noisy feedback in a manner that offsets individual errors ( Almaatouq et al . 2020 , Hastie and Kameda 2005 , Larrick and Soll 2006 , Palley and 11 Groups and Individuals in Learning from Experience Satopää 2023 ) . Even if groups could only “agree to disagree” ( i . e . , reach compromise ) , a possibility that does not arise within the mind of a single individual , it might allow for multiple interpretations to coexist and influence collective understanding ( Page 2014 ) , and may eventually guard against overfitting . In addition , groups have been shown to exhibit more risk - seeking behaviors than individuals ( Zajonc et al . 1968 ) , suggesting they might favor less promising known options ( i . e . , avoid overfitting ) which are riskier . Yet , the difference in risk taking between groups and individuals hinges highly on the context knowledge and option structure ( Pachur et al . 2012 , Sanders 1978 ) . It is possible that if feedback is known to be noisy , ignoring this knowledge is the riskier option . Further , these advantages for groups only materialize given their ability to reach consensus and align preferences despite the complexities of increased number of decision - makers . In practice , there is no guarantee that groups are always able to avoid an impasse where no decisions can be made . It is therefore hard to make a directional prediction . Despite these potential advantages in avoid overfitting , groups , like individuals , are ultimately limited by bounded rationality due to finite cognitive resources ( Simon 1997 ) . This limitation should cause the benefits of the group to diminish with the amount of information to be processed , making groups prefer simpler choices ( Schulze et al . 2020 ) . Further , the challenge of reaching agreement ( either in the form of an aggregation rule or compromise ) may become insurmountable with extensive information processing demands . For example , Dahlander and Piezunka ( 2015 ) have shown that receiving an excessive number of ideas would lead companies to revert back to their default choices . Therefore , at very high levels of information processing needed to update beliefs about large volumes and variety of feedback , the diversity of perspectives may offset any advantages groups have over individuals . Summary For learning from experience to be effective in producing a valid understanding of the task environment ( that is , the finalized belief reflects accurately the environment ) in which it takes place , it 12 Groups and Individuals in Learning from Experience is important that the learner manages the trade - off between exploration and exploitation during the alternative generation stage , and the bias - variance trade - off during alternative evaluation . The presence of “multiple distinct minds” in groups , resulting in varied perspectives and preferences relative to a single mind , is likely to enable a different approach to navigate the two stages of learning . While there is strong evidence to suggest that groups excel over individuals in producing a higher volume and greater variety of alternatives during the alternative generation stage , it is less certain whether groups would also be more successful at avoiding overfitting during alternative evaluation . To the extent that learning from experience requires confronting both trade - offs , predicting if and when groups might surpass individuals in forming an accurate understanding of their task environment is challenging . We empirically investigate this question in a setting that allows unusual opportunities for measuring how individuals and groups tackle the trade - offs associated with alternative generation and evaluation . Method Kaggle as an Empirical Context Scholars have studied organizational learning using groups and teams as tractable and simple models of more complex organizations ( Argote 2013 , Csaszar 2012 ) . Field studies have allowed scholars to examine how real work groups , from surgery teams to sales teams , learn ( Ben - Menahem et al . 2015 , Edmondson 1999 , Ellis et al . 2003 ) . However , task environments in the real world can be complicated , with heterogeneous goals across groups and individuals , and ambiguous metrics to evaluate the success of learning . Studies of groups in the laboratory complement field studies with randomized assignment to work as a group or individually , and clear success metrics , focusing on specific aspects of learning ( e . g . , learning by doing vs . vicarious learning ) . However , study participants typically do not experience as strong incentives to complete the task as real workers in the field , and the tasks involved are often picked by design to be simple rather than representative of real world phenomena ( Argote 2013 ) . 13 Groups and Individuals in Learning from Experience Kaggle provides some unique advantages compared to both the laboratory and the field . Kaggle is an online platform where “seekers” ( organizations such as companies and research institutes ) can share their dataset and state their problems to seek solutions in the format of competitions . Contestants can sign up either as individuals or as groups . Kaggle competitions typically involve supervised machine learning prediction problems . However , they are also a useful abstraction of a variety of learning tasks beyond those using machine learning tools to make predictions . We outline the typical process of a Kaggle competition schematically in Figure 1 . Each contestant has access to two datasets , a training dataset and a test dataset ( of which a subset is the hold - out sample , defined below ) . The training set contains all the potential predictor variables and a column with the outcome of interest , which is a random sample of the full dataset provided by the seeker . The test set contains all the predictor variables too , but the column with the outcome of interest is not shown to contestants . The goal of each contestant is to employ the training dataset to develop models that that can accurately predict outcomes in the test dataset . This is the central learning task – identifying robust patterns in the training dataset for effective prediction in the test dataset . This process is aided by the use of statistical models ( often supervised machine learning models ) . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Figure 1 about here - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Why Kaggle contests represent learning from experience tasks In the first stage , a contestant can build a multiplicity of models and learn from interim feedback on the quality of their models . When executed on the test dataset , each model generates a solution – the predicted outcome of interest ( i . e . , the missing column in the test dataset ) . Upon uploading a solution , Kaggle randomly splits the rows of the test dataset into two predetermined ( identical for but unknown to all contestants ) subsets and compute an accuracy score for each of subset : a public ( accuracy ) score , which is visible to the contestant as feedback on the models , and a private ( accuracy ) score , which is not visible to the contestant until the end of the competition . 14 Groups and Individuals in Learning from Experience By taking actions ( creating models and submitting solutions ) and receiving feedback ( public score of solutions ) , the contestants engage in a classic form of learning from experience . This corresponds to the alternative generation stage of learning . As in all such instances of learning from feedback on actions taken , feedback is only available on the models they submitted - they cannot see the models others have submitted ( though they can see their scores ) . Further , the number of models that can be submitted during this learning stage is limited – the limited daily submissions for contestants , identical for groups and individuals 4 , exemplify the exploration - exploitation trade - off . Contestants face decisions regarding the number and diversity of models to create and test , weighing the opportunity cost of each submission . They must choose between refining existing models based on positive feedback ( exploitation ) or experimenting with novel models that might yield significantly better or worse results ( exploration ) . In the second stage , before the competition deadline , contestants need to choose a few solutions ( models ) as their final submissions out of all the alternatives they have generated and received feedback on 5 . They are not , however , able to choose an entirely new model that has never received any feedback . This corresponds to the alternative evaluation stage of learning . The outcome of a competition is solely reliant on the accuracy of these final models in predicting the outcome variable in the hold - out dataset that the contestants never received feedback on during the alternative generation stage 6 . This stage presents the bias - variance trade - off . In choosing their ( restricted number of ) submissions for the final stage , the contestants must balance the risk of underfitting ( not selecting their best performing models for the final stage ) and overfitting ( selecting only their top performing 4 For example , if the daily limit of submissions is 5 per day , a group of 3 members can submit 5 submissions in total per day . Thus , it is not 3x the models as an individual contestant , but exactly the same . 5 The number of final submissions allowed per contestant in a given competition is determined by Kaggle . Final ranking among contestants is computed according to the best private score among the set of solutions selected by each contestant . 6 This final ranking in Kaggle is called the Private Leaderboard . The Private Leaderboard is then used to award cash prizes , to a very restricted number of contestants ( minimum 1 to maximum 10 ) ; and symbolic badges , in the form of gold , silver and bronze medals , according to percentile position and size of the competition . The exact rules of how medals are awarded in different competitions is described on https : / / www . kaggle . com / getting - started / 44919 15 Groups and Individuals in Learning from Experience models that may not do as well in the final stage ) . The trade - off arises because the hold - out dataset is not identical to the data used for training and testing because of differences produced by sampling error . Kaggle has several appealing features that are unlikely to be obtained using field data or experimental methods alone . First , similar to laboratory studies , Kaggle has a single task environment , where the purpose of competitions is to find good prediction models for a dataset , by providing opportunities to learn from feedback and revising existing models iteratively . Second , each competition employs quantifiable and objective metrics of learning performance for contestants . In addition , Kaggle contestants are in a setting where they are strongly incentivised ( seekers provide on average $ 55 , 000 to the winners ) . Finally , the process that Kaggle follows leaves a clear digital trail , allowing us to measure what contestants do to address the exploration - exploitation trade - off , bias - variance trade - off , and the final accuracy of their understanding of the task environment . Description of Data 7 This paper utilizes a dataset , called “Meta Kaggle” , publicly shared by Kaggle that includes data on competitions hosted on the platform 8 . We focus on Featured Competitions , which are “full - scale machine learning challenges which pose difficult , generally commercially - purposed prediction problems” and “offer prize pools going as high as a million dollars” , 9 due to their clear , singular task environment , defined success metrics , and strong incentives . To align with our research questions , we restricted the sample to competitions that encompass both public and final scoring of solutions ( to allow clear measurement of exploration - exploitation and bias - variance trade - off ) , thereby excluding 36 competitions . Our final dataset includes 129 competitions , 164 , 395 contestants , and 2 , 744 , 393 proposed solutions . In total , 8 . 7 % of contestants are groups and Kaggle does not systematically collect additional information on the demographics of contestants . 7 Additional analyses related to empirical methods and results can be accessed via the online appendix . 8 More detail about the dataset can be found on : https : / / www . kaggle . com / datasets / kaggle / meta - kaggle 9 The exact description of different types of competition can be accessed : https : / / www . kaggle . com / docs / competitions # types - of - competitions 16 Groups and Individuals in Learning from Experience The unit of analysis in the present research is at the contestant - competition level , with each contestant either being a single individual or a group of individuals . Typically , prizes and badges are awarded at the contestant level , irrespective of the contestant being an individual or a group . Dependent Variables 10 All our dependent variables and some of the control variables rely directly or indirectly on accuracy scores . Because the specific rules and scoring for each competition can vary significantly , we standardized the scores within each competition to account for the possible heterogeneity across competitions . For competitions where winners were determined by the lowest ( instead of the highest ) prediction accuracy measure , we inverted the sign of the score first . Some competitions employed unbounded accuracy measures ( e . g . [ - ∞ , 1 ] ) and were characterized by the presence of very large negative outlier scores . In these cases , we replace the scores of solutions in the 1st percentile of that competition with the lowest score in the respective 2 nd percentile . The results are qualitatively robust if we instead replace the 2 nd or 3 rd percentile scores with the lowest scores in the percentile just above them . Exploration . Learners who generate a more varied set of alternatives over time have explored more than those who have generated a less varied set of alternatives ( Billinger et al . 2021 ) . When exploration is low , the individual actions ( i . e . , models generated ) would be similar ( to the extent that they may be identical ) ; when exploration is high , learners take dissimilar actions . We therefore used the standard deviation of public scores on various solutions submitted during the first stage to measure exploration . This is because while we do not have direct access to all the models being used by contestants , the more widely varying their scores are , it is reasonable to assume the models are also different . 11 The extent of exploration reflects how contestants navigate the exploration - exploitation trade - off in learning from experience . 10 A more detailed description of the Dependent Variables is available in online Appendix A . 11 In a subset of submissions where it is possible to observe the content of the code , we find that there is a positive correlation between absolute differences in scores between submissions and differences in the 17 Groups and Individuals in Learning from Experience Avoid overfitting . While it may seem intuitive to choose the models that perform the best in the first stage of the competition when selecting for the final submission , such a choice runs the risk of overfitting - the test dataset from which feedback was generated may be different from the hold - out dataset because of sampling error . Previous research using Kaggle data has shown that while some contestants do adopt the alternative evaluation strategy of choosing some of their lower performing models for the final submissions , very few do so in practice ( Lee et al . 2018 ) . While most competitions allowed for more than a single final submission , Kaggle only records the best performing one ( on the hold out data set ) . We thus constructed a measure , “avoid overfitting” using a dummy variable . This took on a value of “1” when the contestant’s best submission for the final stage was not among the k best performing submissions in terms of public score among all the submissions made by the contestant in the initial stage , where k is the number of submission allowed at the final stage . This measure implies that the contestant’s final submissions include at least one model that was not among its best k models in the test data . We interpret the likelihood of introducing at least one underperforming model into the final solution set as indicating that the contestant is avoiding overfitting . Final performance . We used the best score of the final submissions chosen by contestants to measure their final performance . Final performance represents how accurately the contestants were able to predict the unseen hold - out dataset . It captures their finalized understanding of the task environment . Independent Variables Group contestant . We used a dummy variable to capture the type of contestant in each competition where 1 indicates that the contestant is a group and 0 if the contestant is an individual . We also measured Contestant Size , the number of individuals that “compose” a contestant . Among underlying code ( computed as differences in the functions used in the codes ) . To access the additional analyses demonstrating the relationship between models and their accuracy scores , see online Appendix B . 18 Groups and Individuals in Learning from Experience the group contestants , only 625 groups ( about 5 % ) have worked together in the past . For the results presented below , ( 1 ) controlling for having been in the same group in the past or having at least two members that worked together in the past ; ( 2 ) or excluding these groups , do not affect our results . Control Variables Search effort . Distinguishing from exploration , we also measured the contestant’s search effort using the total number of solutions submitted by contestants for each given competition in stage one . Our theoretical interest is in exploration since groups are mechanically likely to have greater search effort when compared to individuals , so that it is important to control for it . We controlled for the expertise of a contestant by measuring : ( 1 ) the number of badges won by a contestant in all Kaggle competitions that had concluded before the beginning of the focal competition ( Past Badges ) ; and ( 2 ) the number of competitions joined by a contestant before the beginning of the focal competition ( Past Competitions ) . If a contestant was a group , we summed the expertise of all group members . Results using group averages instead of sums are presented in the online Appendix J . We also controlled for the propensity of competing as a group by measuring the proportion of competitions that a contestant participated in as part of a group before the beginning of the focal competition ( GC Preference ) . If a contestant was a group , we computed the average preference for all group members . First submission score measures the standardized private score of the first submission made by a contestant in each given competition . The starting point in a learning process represents an initial level of understanding that is updated through feedback . These may be different across groups and individuals as well as correlate with learning behaviours and outcomes and is thus important to control for . As competitions vary in a series of characteristics such as the maximum number of daily solutions allowed to get interim feedback , number of final submissions allowed for final scoring , 19 Groups and Individuals in Learning from Experience number of contestants awarded monetary prizes and badges , prize money , length , and proportion of data in the holdout sample , all our regression models are estimated using competition fixed effects . Nearest Neighbor Matching In our data , individuals made their own choices when deciding to participate as a single individual or as part of a group . This potentially introduces selection bias such that individuals who are more likely to join groups may also be more likely to expend greater search efforts , produce variety in exploration , and avoid overfitting ( See Lee et al . 2018 , and Ren et al . 2020 for description of competition participants ) . While our regression analyses control for the expertise of both individual and group contestants , we used nearest neighbor matching to further reduce potential sources of omitted variable bias , by identifying individual contestants that were the closest to each group member on their observable attributes . This matching procedure allows us to simulate a process in which groups can be seen as composed of individuals who are identical on average ( on observable measures , Past Competitions , Past Badges , and GC Preference ) to the individuals who participate alone 12 . Additional details of the matching process are available in the online Appendix C . Results in the following sections are shown both with and without this matching process , indicated by “Matched Sample” ( “Yes” or “No” ) in tables . The Matched Sample is the union of the set of matched group contestants and of the set of matched individual contestants . Findings Table 1 provides descriptive statistics and pairwise correlations of the key variables . We report these for the matched sample . To conserve space , we report the descriptive statistics and pairwise correlations for the full sample in the online Appendix D . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 1 Here - - - - - - - - - - - - - - - - - - - - - - - - - - - 12 The three characteristics of contestants used to create the matched sample were also included as control variables across all analyses ( Ho et al . 2007 ) . This matching process does not mean that we are able to eliminate confounding factors but represents an attempt to construct a sample as close to an experimental counterfactual as possible . 20 Groups and Individuals in Learning from Experience Groups and Individuals Differ in Their Exploration We first explore the differences between how groups and individuals differ in their extent of exploration during the alternative generation stage ( Table 2 ) . Model 1 includes only control variables and shows that search effort and participation in past competitions are positively associated with exploration . In contrast , the higher the first submission score , and the greater the number of badges won in past competitions , the lower the exploration by the contestant , suggesting that higher skilled contestants in general show lower variability in their alternative generation process . Model 2 shows that groups explore more as compared to individuals , with 25 % more variability . Model 3 shows that these patterns replicate when contestant size is the predictor instead of group dummy . Models 4 and 5 confirm that results are similar when analyses are performed on the full unmatched sample . All models in Appendix E confirm that the results are robust when exploration is treated as an ordered categorical rather than a continuous variable . - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 2 about here - - - - - - - - - - - - - - - - - - - - - - - - - - - Groups and Individuals Differ in Avoiding Overfitting Only for Low Search Effort Next , we explore whether groups and individuals differ in the tendency to discard their best solutions ( i . e . avoid overfitting ) during the evaluation stage . Table 3 summarizes the regression analyses on the likelihood of the contestant avoiding overfitting . We use linear probability models for intuitive interpretation . Model 1 only includes control variables , and shows that contestants that exert greater search efforts , have participated more often as groups and with higher first submission scores are more likely to avoid overfitting . Model 2 shows that the propensity of groups to avoid overfitting does not seem to be significantly higher as compared to individuals . This does not change if contestant size is used instead of group contestant dummy ( Model 3 ) . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 3 about here - - - - - - - - - - - - - - - - - - - - - - - - - - - However , in additional exploratory analysis ( Model 4 ) we find that groups do show a greater propensity to avoid overfitting than individuals until their search effort surpasses the 75 th percentile . 21 Groups and Individuals in Learning from Experience Beyond that , individuals avoid overfitting more than groups . The pattern is the same when including Contestant Size as control ( Model 5 ) and in the unmatched sample ( Model 9 ) . In other words , while both groups and individuals are more likely to avoid submitting their best solutions when they exert a lot of search effort and accumulate many solutions , groups are more likely to do so than individuals as long as the number of alternatives to evaluate is not too high . At the median value of search efforts , groups tend to avoid overfitting about 50 % more than individuals . With more solutions to consider , the difference between groups and individuals disappears ( see Figure 2 ) . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Figure 2 about here - - - - - - - - - - - - - - - - - - - - - - - - - - - While we do not have access to the internal decision processes of groups in our sample , if we assume groups select final submissions based on a certain level of agreement among group members , it is possibly more difficult for groups to converge on what to submit when there are many potential alternatives . In this case , perhaps groups default back to simpler heuristics such as choosing the subset of “best” solutions , eliminating the difference between groups and individuals when their search efforts have produced a large number of solutions . In models 6 and 7 we explore how contestant size , instead of the mere distinction between individuals and groups , interact with the number of alternatives to evaluate . When including individuals in the sample , the interaction term between Contestant Size and Search Effort is negative and statistically significant ( Model 6 ) , showing a similar pattern to Model 4 . However , when restricting the analysis to groups only ( Model 7 ) , we observe no effect of the interaction between Contestant Size and Search Efforts , suggesting that the difference in probability of overfitting is mostly driven by the distinction between groups ( of any size ) and individuals . Additional analyses also show that this challenge of overload during the evaluation stage created by greater search effort for groups is not related to their extent of exploration ( Model 8 ) . This indicates that the challenge for groups relative to individuals in terms of converging on final submissions without overfitting during the alternative evaluation stage , is driven by the number rather than the variety of solutions they have uncovered during the alternative generation stage . 22 Groups and Individuals in Learning from Experience In sum , we find partial evidence that groups are more likely to discard their best solutions when finalizing submissions , but this tendency is weakened with the number of solutions ( search effort ) that groups have generated . This suggests a self - limiting property of groups – while groups exert more search effort , this can also make it harder for them to tackle the bias - variance trade - off . Perhaps , we speculate , this is because of the difficulty of converging on which model to use in place of the best alternatives they have discarded . Additional Analysis : Exploration , Avoiding Overfitting , and Final Performance To gain at least correlational insight into the performance implications of the observed differences between groups and individuals , we explored the relationship between exploration and avoidance of overfitting on final performance ( Table 4 ) . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 4 about here - - - - - - - - - - - - - - - - - - - - - - - - - - - Model 1 in Table 4 reports the effects of control variables only . All variables ( except for past badges ) have positive effects on final performance , a pattern that is robust across models . Model 2 shows that exploration is positively correlated with final performance . Specifically , a one standard deviation increase in exploration is associated withs This confirms that variety and not just scale of the experiences gathered ( i . e . , alternatives generated ) matters for learning from experience ( Billinger et al , 2020 ; Argote et al . 2020 ) . By including the squared form of exploration , Model 3 shows that exploration has diminishing marginal effects on learning performance , as we would expect from prior theory ( e . g . , March 1991 ) . Model 4 shows that the tendency of contestants to avoid overfitting ( by discarding their best solutions from the first stage ) is not significantly correlated with final performance . In Model 5 , we explore whether the relationship between avoidance of overfitting and final performance shows a similar overload effect observed in our prior analyses comparing groups and individuals on their tendency to avoid overfitting . Model 5 supports this expectation . The performance benefits of avoidance of overfitting decrease as search efforts increase for all contestants , across groups and individuals . The inflection point of the relationship occurs when the number of solutions exceeds 62 23 Groups and Individuals in Learning from Experience ( 92 nd percentile ) . It suggests that contestants face stronger challenges in selection of alternatives when the set of alternatives is particularly large , perhaps making reliance on a simpler heuristic , like choosing the best solutions according to public score , a more likely decision strategy . This pattern remains robust when including the squared term of exploration ( Model 6 ) as well as when using the full unmatched sample ( Model 8 ) . In Model 7 , we explore whether the variety of solutions ( Exploration ) generates a similar overloading effect as quantity of solutions for avoid overfitting . However , the interaction between Avoidance of Overfitting and Exploration is not statistically significant . These results suggest that both exploration and the tendency to avoid overfitting are crucial for final performance , but only quantity and not variety of solution generates a significant overload effect on the impact of avoidance of overfitting on Final Performance . Additional robustness checks ( bootstrapping analysis in Appendix F , and exclusion of contestants that do not have sufficient search efforts to avoid overfitting in Appendix G ) are included in the online Appendix . While Final Performance measures the accuracy of understanding the task environment achieved through learning , we may also consider if the learner experiences performance improvement – a change in performance ( a measure of learning rate ) . When using Performance Improvement - the difference between the final score and the score of the very first attempt made by the contestant during the first stage of the competition – as the dependent variable ( Model 8 ) , the results are qualitatively identical to those reported above with final performance as the measure ( see online Appendix H for the complete set of analyses with Performance Improvement as dependent variable ) . Group advantage and its self - limiting nature . We also explored whether the difference in the process of learning between groups and individuals would translate into final performance . Table 5 summarizes the regression analyses examining final performance across groups and individuals on search efforts , exploration , and the avoidance of overfitting . Groups significantly outperform individuals ( Model 1 ) , even when controlling for exploration and avoidance of overfitting ( Model 2 ) . 24 Groups and Individuals in Learning from Experience Contestant size is positively associated with Final Performance ( Model 3 ) . Model 4 shows that the interaction term between group dummy and search efforts is negatively correlated with final performance . This suggests that as contestants produce larger numbers of solutions , the performance advantage of groups diminishes , and the inflection point is when the number of solutions exceeds 35 . Figure 3 visually presents this relationship . Results are robust both when we further control for contestant size ( Model 6 ) , and in the full unmatched sample ( Model 8 in Table 5 ) and with bootstrapping ( Appendix F ) . Consistent with what we observed in previous analyses , we do not observe a significant interaction term between group dummy and exploration ( Model 7 ) . Results are qualitatively similar when using Performance Improvement as dependent variable ( Model 8 ) . Thus , the difference in overall final performance between groups and individuals is particularly marked at lower levels of search efforts and it inverts as the number of solutions increases . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 5 Here - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Figure 3 Here - - - - - - - - - - - - - - - - - - - - - - - - - - - Additional analyses on the impact of the size of the groups , task complexity , and incentives are included in the online Appendix ( Appendix I ) . Additional Analysis : The impact of centralization Our theoretical argument assumes that the diversity in perspectives in groups can materialize as diverse actions . However , within a group , centralization – inequality in how much individuals can influence the group’s behavior ( Argote et al . 1989 , Bunderson 2003 , Hage and Aiken 1967 ) – may manifest . It is thus possible that influence processes ( Bond 2005 ) or power dynamics ( Bunderson and Reagans 2011 ) create conformity pressures and so suppress the variety and number of actions . Hence , for groups to produce a greater variety of alternatives compared to individuals , the decision - making process in groups cannot resemble a dictatorship . If groups were so centralized in their actions as to prohibit the influence ( direct or indirect ) of all but one member , this would leave no difference 25 Groups and Individuals in Learning from Experience between the group and the individual . The fact that we detected differences makes this less likely to have occurred in our data , but we conducted some additional analyses on this point . To measure centralization , we compute the Herfindal - Hirschman Index ( HHI ) of Search Efforts for each group contestant in each competition . Within the daily limits imposed by each competition at the contestant level , every member of a group can submit solutions in the alternative generation stage . The HHI measures the degree to which the act of submitting solutions in the alternative generation stage is concentrated or dispersed across group members 13 . If only one group member submitted all the attempts within a group , the HHI would take value 1 , and it would leave no difference between the group and the individual as the effects of cognitive diversity would disappear . Therefore , we compare groups with HHI equal to 1 and individuals to examine whether they differ in terms of search effort , exploration , avoidance of overfitting ( at low search effort levels ) , and final performance . Indeed , when we compare groups with HHI of 1 and their matched individuals , there is no significant difference between their degree of exploration , their tendency to avoid overfitting , and their final performance ( Table 6 ) . Varying levels of centralization ( on average the HHI in our sample of groups is 0 . 771 ) should also impact the exploration , avoidance of overfitting , and final performance among groups . We find that within the sub - sample of groups , those with a higher HHI tend to explore less , and perform worse , but are relatively less subject to the challenge of choosing among an abundant set of alternatives ( Table 7 ) . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 6 and 7 Here - - - - - - - - - - - - - - - - - - - - - - - - - - - Another approach to testing the effects of centralization is to compare groups with nominal groups , as the latter represents a group structure with zero ( centralization of ) influence and would preserve the most diverse set of perspectives and behaviors ( Table 8 ) . Each nominal group we constructed is matched to a real group by constructing the nominal group out of individuals matched 13 While HHI does not explicitly reveal the variance of power concentration or informal influence within groups , it does reflect instances in which actions are centralized from a few to all group members ( inequality in participation ) . Consequently , it aids in elucidating the mechanism - the diversity of perspectives expressed in distinct actions . 26 Groups and Individuals in Learning from Experience to those in the real group . As we expect , we found that nominal groups explore more than real groups , which in turn explore more than individuals . We find that real groups tend to avoid overfitting more than the random or the best members representing the nominal group ( who also engage in some avoidance of overfitting ) until the number of submissions becomes so large that the direction of the effect is reverted – exactly as we find in the comparison between groups and individuals . - - - - - - - - - - - - - - - - - - - - - - - - - - - Insert Table 8 Here - - - - - - - - - - - - - - - - - - - - - - - - - - - In summary , our results indicate that groups explore more and avoid overfitting more at low levels of search efforts due to the heterogeneity of cognitive resources in a group relative to an individual . When groups act in ways that dampen the expression of this potential heterogeneity of cognitive resources , as when there is a high concentration of action and influence , their tendency to explore and avoid overfitting at low levels of search efforts reduces . However , the heterogeneity of cognitive resources in a group may also have some overloading effects when an abundance of alternatives is generated . Discussion Compared to individuals , groups – because they comprise multiple individuals – potentially have an advantage not only in terms of scale ( in being able to marshal more cognitive resources ) but also in terms of diversity ( in the sense of differences in perspectives ) when it comes to learning from experience ( Aggarwal et al . 2015 , 2019 , Argote 2013 : Chapter 5 ) . It would take an inordinate degree of free riding or centralization to reduce the quantity and variety of perspectives a group can bring to bear to levels below that of a single individual . Therefore , it seems intuitive that groups may have an edge in the alternative generation stage in learning from experience by allowing for more varied exploration . Whether this advantage can translate to better learning depends further on if groups may mitigate the risk of overfitting noisy data better than individuals , a challenge in learning from experience about which we lack sufficient knowledge . Kaggle provided an unusually effective opportunity to explore how both groups and individuals handle the key trade - offs in learning . On this contest platform , informally organized groups and 27 Groups and Individuals in Learning from Experience individuals compete against each other in tasks that require learning from experience , involve strong financial incentives , and leave a digital record of their activities . We found that groups explore more widely than individuals . The advantage in exploration is beneficial as long as the search efforts by groups do not result in an overwhelming number of alternatives , at which point they become more susceptible to overfitting when generalizing from the gathered experience . This suggests a potential self - limiting nature to the group advantage . Groups generate more as well as more varied alternatives than individuals , yet , groups may lose their advantage over individuals in terms of avoiding overgeneralization when they have generated a ( too ) large number of alternatives to evaluate . Validating this self - limiting property , through a series of additional analyses , we show that the performance benefits of avoidance of overfitting decrease as search efforts increase for all contestants , whether groups or individuals . Subsequently , the performance advantage of groups over individuals diminishes with their search effort , suggesting that the overload created by multiple alternatives makes it particularly hard for groups to evaluate alternatives 14 . Finally , we show evidence that groups with higher centralization tend to underperform and explore less compared to those with lower centralization ( also see Woolley et al . 2010 ) , though in aggregate the average group outperforms the average individual at the learning tasks in our context . This highlights again that process losses within a group may indeed diminish group performance , but not necessarily to levels below that of individuals ( Miner 1984 ) . With this analysis , we make three contributions to prior research . First , we explicate the two generic trade - offs that all learners , whether groups or individuals , confront in learning from own experience – the exploration - exploitation trade - off and the bias - variance trade - off . We position these trade - offs within the learning - by - doing cycle ( iteration between beliefs → actions → feedback → updated beliefs →… ) at the links between beliefs and actions and between feedback and beliefs 14 A similar overloading effect has been observed in the context of crowdsourcing by Piezunka and Dahlander ( 2015 ) . Firms that receive a large number of suggestions can be overwhelmed , and default to examining only the subset that are most related to current practice , hence defeating the purpose of crowdsourcing . While our result arises from overload created for a group that both generates and evaluates alternatives , the similarity is striking . 28 Groups and Individuals in Learning from Experience respectively . At a sufficient level of abstraction , to the extent any system learns from experience in task environments that feature multiple peaks and noisy payoffs , it cannot avoid these two trade - offs . The learning - by - doing cycle provides a useful general framework to organize prior literatures on the processes connecting the elements of the cycle , such as feedback seeking ( Edmondson 1999 ) , after event review ( Ellis and Davidi 2005 ) and knowledge transfer ( Argote et al . 2020 ) . For example , the review of successful versus failed events may challenge the learner to navigate the bias - variance trade - off differently when they extract insights from the outcomes of these events ( Ellis and Davidi 2005 ) . By highlighting how the superstitious learning and overgeneralization arguments in prior research on organizational learning relate to the more general problem of the bias - variance trade - off , we suggest this second trade - off also deserves a prominence , comparable with that of the exploration - exploitation trade - off ( Choi and Levinthal 2022 ) . This recognition in turn gives us the analytical leverage to probe deeper into the differences in handling of these trade - offs by groups and individuals , which is our second contribution . Specifically , our analysis revealed the unanticipated discovery that the group advantage in learning from experience may be self - limiting . Prior work has recognized that divergence and convergence processes both matter in learning ( Bunderson and Sanner 2017 , Gersick 1989 ) . Our results suggest that while groups can increase divergence relative to individuals , there is also the possibility of option - overload , such that the sheer number of alternatives generated can make it harder for a group to evaluate them and converge on what lessons to draw from experience during alternative evaluation . This overload can arise from either the quantity and / or diversity of options , depending on which burdens a learner ' s ability to evaluate the options more effectively . In our context , for example , the impact of greater quantity but not diversity appears to produce more difficulty for evaluation . Our arguments assumed two important boundary conditions : first , that groups do not collapse to a single individual by virtue of a sort of dictatorship emerging within the group , and second that they effectively manage their diverse viewpoints to avoid deadlock ( Hoever and van Knippenberg 2021 , van Knippenberg and Schippers 2007 ) . Neither condition was hard to meet in our empirical context . 29 Groups and Individuals in Learning from Experience For instance , in the alternative generation stage ( i . e . , obtaining feedback via public scores ) , there is no restriction on who within the team can submit a model for feedback . The possibility of dictatorship is hence low in the groups we study ( though we cannot rule it out entirely ) . Similarly , deadlocks are unlikely if the number of alternatives that learners must select is not too restrictive to allow for compromise , or the incentives to reach an agreement are strong enough . Crucially , Kaggle allows contestants to submit multiple entries even in the final stage and by default the platform submits the best performing models that the group developed in the alternative generation stage ( making deadlock unlikely ) and leaves the group free to organize its internal processes . We can contrast this with situations involving learning by participation ( Piezunka et al . 2022 ) in which a group must agree on a single course of action , making deadlock more likely . Therefore while we find a decline in final performance , exploration and search effort with inequality of participation , overall , groups in our setting managed to avoid becoming mere reflections of a single individual such that we could still observe differences on average between them and individuals . The unique features of our context that allow us to observe the advantages of groups for learning from experience ( e . g . , exploration , discarding best solutions ) also limit immediate generalization . The type of tasks that Kaggle provides allow individuals , within a group or by themselves , to all engage in learning from experience . This parallelism is not available for all learning tasks . For example , in a committee structure , one can only learn from feedback on the committee’s joint decisions ( Piezunka et al . 2022 ) . Furthermore , the tasks on Kaggle , characterized by relatively objective performance metrics ( i . e . , accuracy of prediction ) , may facilitate the process of consensus in groups ( Bonner et al . 2021 ) . But this is by no means a demonstrably correct task , because the highest score for a model in the first stage does not necessarily translate to the highest score in final submissions ( Laughlin 1999 ) . Absent easy demonstrability of the quality of alternatives , groups may struggle much more to benefit from the diversity of alternatives they generate . An additional feature of the context is that most groups on Kaggle platform are distributed spatially . This prevents conformity pressures often found in face - to - face interactions ( e . g . Bond 2005 ) . We cannot therefore 30 Groups and Individuals in Learning from Experience use the results from Kaggle to say groups will outperform individuals at learning tasks universally . In fact , our results make it clear that such a general statement will almost certainly be wrong . However , an implication we can draw is that the superiority of groups to individuals in terms of learning from experience should be contingent on balance between their advantage in alternative generation ( through more and varied alternatives generated ) and possible disadvantage at alternative evaluation as the number of alternatives produces overload . This suggests concretely that individuals have an edge over groups in learning from experience under several predictable conditions . First , the task environment should allow for limited benefits to increased search efforts ( e . g . , a small search space ) , exploration ( i . e . , a single - peaked landscape ) and avoidance of overfitting ( i . e . , noise - free feedback ) . Consistent with this , we note that the study by Lejarraga and colleagues ( 2014 ) in which groups learn less rapidly than individuals after a regime shock , involves a task with only two possible alternatives . Second , the wealth of experience gathered in learning should create difficulty for converging on its interpretation . Learning tasks that do not allow a clear yardstick of performance may therefore favor individuals relative to groups . Paradoxically , freedom from constraints in terms of time or budget during the learning process may actually serve to benefit individuals , because they may heighten the risk of groups being overwhelmed by the “burden of riches” generated by the advantage in efforts at search and exploration . For instance , in Cooper and Kagel ( 2005 ) , there are demonstrably correct answers and the set of possible alternatives is relatively small , putting individuals at a disadvantage . Third , group processes that suppress the diversity of perspectives ( i . e . , dictatorships ) , or fail to prevent fruitless disagreement ( i . e . , deadlock ) , as may have occurred in our context for groups with overwhelming number of solutions to choose from , tilt the advantage towards the individual . In terms of limitations , the two most salient and possibly addressable in future work relate to causal inference and observability of group process . While we have attempted through matching and control variables to mitigate the risk of spurious associations , lacking randomization of individuals into groups , we cannot rule out that unobserved features of individuals correlate both with 31 Groups and Individuals in Learning from Experience participation as a group as well as tendencies to search extensively , explore and avoid overfitting . Further , while we are able to observe numerous aspects of the task environment and the individuals and groups embedded in it , we do not have insight into the group processes during the competitions . Exactly how the presence of multiple individuals and therefore perspectives in a group influence exploration and the avoidance of overfitting remain a matter of conjecture in our study . Future research , possibly in a lab setting with randomization , could be helpful to address such questions . Practical implication When should we deploy individuals and when should we form groups to complete organizational learning tasks ? Our findings show that managers should carefully assess the task environment ' s characteristics . To begin , it is critical to distinguish between tasks that can be completed independently by individuals and those requiring collective , interdependent effort . For instance , talent “head - hunters” working independently to interview candidates ( i . e . , an alternative generation stage where each head hunter can function independently to seek a varied pool of candidates ) and then selecting final candidates ( i . e . , alternative evaluation where a few candidates are selected ) is an example of a context to which our findings can generalize . The search for molecules during drug discovery could be another , where alternative generation by group members could occur in parallel followed by a group level evaluation of alternatives ( Ben - Menahem et al . 2015 ) . In such contexts , assembling groups should produce better outcomes than depending on individuals if the amount of labor is vast and providing varied alternative solutions is vital to quality output . However , when finding the optimal solution is relatively more important to task output , as arguably in drug discovery as opposed to talent headhunting , managers should be aware that more is not necessarily better and devise mechanisms to avoid groups from becoming overwhelmed by creating too many alternative solutions to assess . To properly capitalize on the benefit of groups , managers may need to limit the time that teams can spend investigating options , impose a limit on the number of solutions or even step in to be the “dictator” to break deadlocks or curtail excessive exploration . 32 Groups and Individuals in Learning from Experience Conclusion Understanding how groups differ from individuals at learning from experience requires zooming into how they differ in both the alternative generation and evaluation stages . To do so , we explore how the multi - individual nature of groups is distinct from individuals in both stages of learning when navigating the exploration - exploitation and bias - variance trade - off . We find that groups expend more search efforts , explore more broadly in alternative generation , and may also be more able to avoid overfitting their experiences as long as they are not overwhelmed by the magnitude of that experience in alternative evaluation . Consequently , groups can outperform individuals at learning tasks that significantly reward exploration and penalize overfitting but pose low risks of overburdening groups with too many alternatives to evaluate easily . Kaggle as a context illustrates one such example . REFERENCES Aggarwal I , Woolley AW ( 2013 ) Do you see what I see ? The effect of members’ cognitive styles on team processes and errors in task execution . Organizational Behavior and Human Decision Processes 122 ( 1 ) : 92 – 99 . Aggarwal I , Woolley AW , Chabris CF , Malone TW ( 2015 ) Cognitive Diversity , Collective Intelligence , and Learning in Teams . Proceedings of Collective Intelligence . Aggarwal I , Woolley AW , Chabris CF , Malone TW ( 2019 ) The Impact of Cognitive Style Diversity on Implicit Learning in Teams . Front . Psychol . 10 . Almaatouq A , Noriega - Campero A , Alotaibi A , Krafft PM , Moussaid M , Pentland A ( 2020 ) Adaptive social networks promote the wisdom of crowds . PNAS 117 ( 21 ) : 11379 – 11386 . Argote L ( 2013 ) Organizational learning : creating , retaining and transferring knowledge Second edition . ( Springer , New York Heidelberg Dordrecht London ) . Argote L , Lee S , Park J ( 2020 ) Organizational Learning Processes and Outcomes : Major Findings and Future Research Directions . Management Science . Argote L , Miron - Spektor E ( 2011 ) Organizational Learning : From Experience to Knowledge . Organization Science 22 ( 5 ) : 1123 – 1137 . Argote L , Turner ME , Fichman M ( 1989 ) To centralize or not to centralize : The effects of uncertainty and threat on group structure and performance . Organizational Behavior and Human Decision Processes 43 ( 1 ) : 58 – 74 . Ben - Menahem SM , von Krogh G , Erden Z , Schneider A ( 2015 ) Coordinating Knowledge Creation in Multidisciplinary Teams : Evidence from Early - Stage Drug Discovery . AMJ 59 ( 4 ) : 1308 – 1338 . Billinger S , Srikanth K , Stieglitz N , Schumacher TR ( 2021 ) Exploration and exploitation in complex search tasks : How feedback influences whether and where human agents search . Strategic Management Journal 42 ( 2 ) : 361 – 385 . 33 Groups and Individuals in Learning from Experience Bond R ( 2005 ) Group Size and Conformity . Group Processes & Intergroup Relations 8 ( 4 ) : 331 – 354 . Bonner BL , Shannahan D , Bain K , Coll K , Meikle NL ( 2021 ) The Theory and Measurement of Expertise - Based Problem Solving in Organizational Teams : Revisiting Demonstrability . Organization Science . Bunderson JS ( 2003 ) Team Member Functional Background and Involvement in Management Teams : Direct Effects and the Moderating Role of Power Centralization . AMJ 46 ( 4 ) : 458 – 474 . Bunderson JS , Boumgarden P ( 2010 ) Structure and Learning in Self - Managed Teams : Why “Bureaucratic” Teams Can Be Better Learners . Organization Science 21 ( 3 ) : 609 – 624 . Bunderson JS , Reagans RE ( 2011 ) Power , Status , and Learning in Organizations . Organization Science 22 ( 5 ) : 1182 – 1194 . Bunderson JS , Sanner B ( 2017 ) How and When Can Social Hierarchy Promote Learning in Groups ? Argote L , Levine JM , eds . The Oxford Handbook of Group and Organizational Learning . ( Oxford University Press ) . Charness G , Karni E , Levin D ( 2007 ) Individual and group decision making under risk : An experimental study of Bayesian updating and violations of first - order stochastic dominance . Journal of Risk & Uncertainty 35 ( 2 ) : 129 – 148 . Chen J , Hong H , Huang M , Kubik JD ( 2004 ) Does Fund Size Erode Mutual Fund Performance ? The Role of Liquidity and Organization . The American Economic Review 94 ( 5 ) : 1276 – 1302 . Choi J , Levinthal D ( 2022 ) Wisdom in the Wild : Generalization and Adaptive Dynamics . Organization Science . Christensen M , Dahl CM , Knudsen T , Warglien M ( 2021 ) Context and Aggregation : An Experimental Study of Bias and Discrimination in Organizational Decisions . Organization Science : orsc . 2021 . 1502 . Cohen TR , Thompson L ( 2011 ) Chapter 1 When are Teams an Asset in Negotiations and when are they a Liability ? A . Mannix E , A . Neale M , R . Overbeck J , eds . Negotiation and Groups . Research on Managing Groups and Teams . ( Emerald Group Publishing Limited ) , 3 – 34 . Cooper DJ , Kagel JH ( 2005 ) Are Two Heads Better than One ? Team versus Individual Play in Signaling Games . The American Economic Review 95 ( 3 ) : 477 – 509 . Crossan MM , Lane HW , White RE ( 1999 ) An Organizational Learning Framework : From Intuition to Institution . AMR 24 ( 3 ) : 522 – 537 . Csaszar FA ( 2012 ) Organizational structure as a determinant of performance : Evidence from mutual funds . Strategic Management Journal 33 ( 6 ) : 611 – 632 . Csaszar FA , Eggers JP ( 2013 ) Organizational Decision Making : An Information Aggregation View . Management Science 59 ( 10 ) : 2257 – 2277 . Cyert RM , March JG ( 1963 ) A behavioral theory of the firm ( Prentice - Hall , Englewood Cliffs , N . J . ) . Daft RL , Weick KE ( 1984 ) Toward a Model of Organizations as Interpretation Systems . AMR 9 ( 2 ) : 284 – 295 . Davis JH , Hornik J , Hornseth JP ( 1970 ) Group decision schemes and strategy preferences in a sequential response task . Journal of Personality and Social Psychology 15 ( 4 ) : 397 – 408 . De Dreu CKW , Nijstad BA ( 2008 ) Motivated information processing in group judgment and decision making . Personality and Social … . 34 Groups and Individuals in Learning from Experience Denrell J ( 2005 ) Why Most People Disapprove of Me : Experience Sampling in Impression Formation . Psychological Review 112 ( 4 ) : 951 – 978 . Denrell J , March JG ( 2001 ) Adaptation as Information Restriction : The Hot Stove Effect . Organization Science 12 ( 5 ) : 523 – 538 . Dunnette MD , Campbell J , Jaastad K ( 1963 ) The effect of group participation on brainstorming effectiveness for 2 industrial samples . Journal of Applied Psychology 47 ( 1 ) : 30 – 37 . Edmondson AC ( 1996 ) Learning from Mistakes is Easier Said Than Done : Group and Organizational Influences on the Detection and Correction of Human Error . The Journal of Applied Behavioral Science 32 ( 1 ) : 5 – 28 . Edmondson AC ( 1999 ) Psychological Safety and Learning Behavior in Work Teams . Administrative Science Quarterly 44 ( 2 ) : 350 – 383 . Edmondson AC ( 2002 ) The Local and Variegated Nature of Learning in Organizations : A Group - Level Perspective . Organization Science 13 ( 2 ) : 128 – 146 . Edmondson AC , Bohmer RM , Pisano GP ( 2001 ) Disrupted Routines : Team Learning and New Technology Implementation in Hospitals . Administrative Science Quarterly 46 ( 4 ) : 685 – 716 . Edmondson AC , Dillon JR , Roloff KS ( 2007 ) Three Perspectives on Team Learning . ANNALS 1 ( 1 ) : 269 – 314 . Ellis APJ ( 2006 ) System Breakdown : The Role of Mental Models and Transactive Memory in the Relationship between Acute Stress and Team Performance . AMJ 49 ( 3 ) : 576 – 589 . Ellis APJ , Hollenbeck JR , Ilgen DR , Porter COLH , West BJ , Moon H ( 2003 ) Team learning : Collectively connecting the dots . Journal of Applied Psychology 88 ( 5 ) : 821 – 835 . Ellis S , Davidi I ( 2005 ) After - Event Reviews : Drawing Lessons From Successful and Failed Experience . Journal of Applied Psychology 90 ( 5 ) : 857 – 871 . Epley N , Keysar B , Van Boven L , Gilovich T ( 2004 ) Perspective Taking as Egocentric Anchoring and Adjustment . Journal of Personality and Social Psychology 87 ( 3 ) : 327 – 339 . Finkelstein S , Haleblian J ( 2002 ) Understanding Acquisition Performance : The Role of Transfer Effects . Organization Science 13 ( 1 ) : 36 – 47 . Gabbert B , Johnson DW , Johnson RT ( 1986 ) Cooperative Learning , Group - to - Individual Transfer , Process Gain , and the Acquisition of Cognitive Reasoning Strategies . The Journal of Psychology 120 ( 3 ) : 265 – 278 . Gersick CJG ( 1989 ) Marking Time : Predictable Transitions in Task Groups . AMJ 32 ( 2 ) : 274 – 309 . Gittins JC ( 1979 ) Bandit Processes and Dynamic Allocation Indices . Journal of the Royal Statistical Society : Series B ( Methodological ) 41 ( 2 ) : 148 – 164 . Hackman JR ( 2003 ) Learning more by crossing levels : evidence from airplanes , hospitals , and orchestras . Journal of Organizational Behavior 24 ( 8 ) : 905 – 922 . Hage J , Aiken M ( 1967 ) Relationship of Centralization to Other Structural Properties . Administrative Science Quarterly 12 ( 1 ) : 72 – 92 . Haleblian J , Finkelstein S ( 1999 ) The Influence of Organizational Acquisition Experience on Acquisition Performance : A Behavioral Learning Perspective . Administrative Science Quarterly 44 ( 1 ) : 29 – 56 . 35 Groups and Individuals in Learning from Experience Hart E , Marciano D , Winter E ( 2017 ) Groups , Group Members and Individuals : Choices and Impulses in Repeated Risky Decisions . Maurice Falk Institute for Economic Research in Israel . Discussion paper series . ( 7 ) : 1 – 43 . Hastie R , Kameda T ( 2005 ) The Robust Beauty of Majority Rules in Group Decisions . Psychological Review 112 ( 2 ) : 494 – 508 . Hastie T , Tibshirani R , Friedman J ( 2009 ) The elements of statistical learning : data mining , inference , and prediction ( Springer Science & Business Media ) . Hertel G , Kerr NL , Messé LA ( 2000 ) Motivation gains in performance groups : Paradigmatic and theoretical developments on the Köhler effect . Journal of Personality and Social Psychology 79 ( 4 ) : 580 – 601 . Hill GW ( 1982 ) Group Versus Individual Performance : Are N + 1 Heads Better Than One ? Psychological Bulletin 91 ( 3 ) : 517 – 539 . Ho DE , Imai K , King G , Stuart EA ( 2007 ) Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference . Political Analysis 15 ( 3 ) : 199 – 236 . Hoever IJ , van Knippenberg D ( 2021 ) Chapter 5 - How diversity promotes team creativity : Two bumpy roads to collective inspiration . McKay AS , Reiter - Palmon R , Kaufman JC , eds . Creative Success in Teams . Explorations in Creativity Research . ( Academic Press ) , 81 – 99 . Holland JH ( 1975 ) Adaptation in natural and artificial systems : An introductory analysis with applications to biology , control , and artificial intelligence . ( U Michigan Press , Oxford , England ) . Hong L , Page SE ( 2004 ) Groups of diverse problem solvers can outperform groups of high - ability problem solvers . PNAS 101 ( 46 ) : 16385 – 16389 . Hüffmeier J , Hertel G ( 2011 ) When the whole is more than the sum of its parts : Group motivation gains in the wild . Journal of Experimental Social Psychology 47 ( 2 ) : 455 – 459 . Karau SJ , Williams KD ( 1993 ) Social loafing : A meta - analytic review and theoretical integration . Journal of Personality and Social Psychology 65 ( 4 ) : 681 – 706 . Kerr NL , MacCoun RJ , Kramer GP ( 1996 ) Bias in judgment : Comparing individuals and groups . Psychological Review 103 ( 4 ) : 687 – 719 . Kirschner F , Paas F , Kirschner PA ( 2009 ) Individual and group - based learning from complex cognitive tasks : Effects on retention and transfer efficiency . Computers in Human Behavior 25 ( 2 ) : 306 – 314 . Klein DC , Fencil - Morse E , Seligman MEP ( 1976 ) Learned helplessness , depression , and the attribution of failure . Journal of Personality and Social Psychology 33 ( 5 ) : 508 – 516 . van Knippenberg D , De Dreu CKW , Homan AC ( 2004 ) Work Group Diversity and Group Performance : An Integrative Model and Research Agenda . Journal of Applied Psychology 89 ( 6 ) : 1008 – 1022 . van Knippenberg D , Schippers MC ( 2007 ) Work group diversity . Annual Review of Psychology . Knudsen T , Levinthal DA ( 2007 ) Two Faces of Search : Alternative Generation and Alternative Evaluation . Organization Science 18 ( 1 ) : 39 – 54 . Kocher MG , Sutter M ( 2007 ) Individual versus group behavior and the role of the decision making procedure in gift - exchange experiments . Empirica 34 ( 1 ) : 63 – 88 . Kolb DA ( 2015 ) Experiential learning : experience as the source of learning and development Second edition . ( Pearson Education , Inc , Upper Saddle River , New Jersey ) . 36 Groups and Individuals in Learning from Experience Kollock P ( 1994 ) The Emergence of Exchange Structures : An Experimental Study of Uncertainty , Commitment , and Trust . Am J Sociol 100 ( 2 ) : 313 – 345 . Krueger J , Acevedo M , Robbins J ( 2006 ) Self as a Sample . Information Sampling and Adaptive Cognition . ( Cambridge University Press , New York , NY , USA ) , 353 – 377 . Larrick RP , Mannes AE , Soll JB ( 2012 ) The social psychology of the wisdom of crowds . Social judgment and decision making . ( Psychology Press ) , 227 – 242 . Larrick RP , Soll JB ( 2006 ) Intuitions About Combining Opinions : Misappreciation of the Averaging Principle . Management Science 52 ( 1 ) : 111 – 127 . Laughlin PR ( 1999 ) Collective Induction : Twelve Postulates . Organizational Behavior and Human Decision Processes 80 ( 1 ) : 50 – 69 . Laughlin PR , Barth JM ( 1981 ) Group - to - individual and individual - to - group problem - solving transfer . Journal of Personality and Social Psychology 41 ( 6 ) : 1087 – 1093 . Laughlin PR , Zander ML , Knievel EM , Tan TK ( 2003 ) Groups perform better than the best individuals on letters - to - numbers problems : Informative equations and effective strategies . Journal of Personality and Social Psychology 85 ( 4 ) : 684 – 694 . Lee HCB , Ba S , Li X , Stallaert J ( 2018 ) Salience Bias in Crowdsourcing Contests . Information Systems Research 29 ( 2 ) : 401 – 418 . Lejarraga T , Lejarraga J , Gonzalez C ( 2014 ) Decisions from experience : How groups and individuals adapt to change . Mem Cogn 42 ( 8 ) : 1384 – 1397 . Levinthal DA ( 1997 ) Adaptation on rugged landscapes . Manage Sci . Levinthal DA , March JG ( 1993 ) The Myopia of Learning . Strategic Management Journal 14 : 95 – 112 . March JG ( 1991 ) Exploration and Exploitation in Organizational Learning . Organization Science 2 ( 1 ) : 71 – 87 . Martins LL , Sohn W ( 2021 ) How Does Diversity Affect Team Cognitive Processes ? Understanding the Cognitive Pathways Underlying the Diversity Dividend in Teams . ANNALS . Mason W , Watts DJ ( 2012 ) Collaborative learning in networks . PNAS 109 ( 3 ) : 764 – 769 . Michaelsen LK , Watson WE , Black RH ( 1989 ) A realistic test of individual versus group consensus decision making . Journal of Applied Psychology 74 ( 5 ) : 834 – 839 . Miner FC ( 1984 ) Group versus individual decision making : An investigation of performance measures , decision strategies , and process losses / gains . Organizational Behavior and Human Performance 33 ( 1 ) : 112 – 124 . Moreland RL , McMinn JG ( 2010 ) Group reflexivity and performance . R . Thye S , J . Lawler E , eds . Advances in Group Processes . Advances in Group Processes . ( Emerald Group Publishing Limited ) , 63 – 95 . Mowday RT , Sutton RI ( 1993 ) Organizational Behavior : Linking Individuals and Groups to Organizational Contexts . Annual Review of Psychology 44 ( 1 ) : 195 – 229 . Nijstad BA , Stroebe W ( 2006 ) How the Group Affects the Mind : A Cognitive Model of Idea Generation in Groups . Personality & Social Psychology Review ( Lawrence Erlbaum Associates ) 10 ( 3 ) : 186 – 213 . Pachur T , Hertwig R , Steinmann F ( 2012 ) How do people judge risks : Availability heuristic , affect heuristic , or both ? Journal of Experimental Psychology : Applied 18 ( 3 ) : 314 – 330 . Page SE ( 2014 ) Where diversity comes from and why it matters ? European Journal of Social Psychology 44 ( 4 ) : 267 – 279 . 37 Groups and Individuals in Learning from Experience Palley AB , Satopää VA ( 2023 ) Boosting the Wisdom of Crowds Within a Single Judgment Problem : Weighted Averaging Based on Peer Predictions . Management Science : mnsc . 2022 . 4648 . Paulus PB , Baruah J , Kenworthy JB ( 2018 ) Enhancing Collaborative Ideation in Organizations . Frontiers in Psychology 9 . Paulus PB , Yang HC ( 2000 ) Idea Generation in Groups : A Basis for Creativity in Organizations . Organizational Behavior and Human Decision Processes 82 ( 1 ) : 76 – 87 . Piezunka H , Aggarwal VA , Posen HE ( 2022 ) The Aggregation – Learning Trade - off . Organization Science 33 ( 3 ) : 1094 – 1115 . Puranam P ( 2018 ) The microstructure of organizations ( Oxford University Press , Oxford , United Kingdom ; New York , NY ) . Puranam P , Stieglitz N , Osman M , Pillutla MM ( 2015 ) Modelling Bounded Rationality in Organizations : Progress and Prospects . ANNALS 9 ( 1 ) : 337 – 392 . Puranam P , Swamy M ( 2016 ) How Initial Representations Shape Coupled Learning Processes . Organization Science 27 ( 2 ) : 323 – 335 . Ren R , Yan B , Jian L ( 2020 ) Show me your expertise before teaming up : Sharing online profiles predicts success in open innovation . Internet Research 30 ( 3 ) : 845 – 868 . Rietzschel EF , Nijstad BA , Stroebe W ( 2006 ) Productivity is not enough : A comparison of interactive and nominal brainstorming groups on idea generation and selection . Journal of Experimental Social Psychology 42 ( 2 ) : 244 – 251 . Ross L , Greene D , House P ( 1977 ) The “false consensus effect” : An egocentric bias in social perception and attribution processes . Journal of Experimental Social Psychology 13 ( 3 ) : 279 – 301 . Sanders GS ( 1978 ) An integration of shifts toward risk and caution in gambling situations . Journal of Experimental Social Psychology 14 ( 4 ) : 409 – 416 . Schulze C , Gaissmaier W , Newell BR ( 2020 ) Maximizing as satisficing : On pattern matching and probability maximizing in groups and individuals . Cognition 205 : 104382 . Shore J , Bernstein E , Lazer D ( 2015 ) Facts and Figuring : An Experimental Investigation of Network Structure and Performance in Information and Solution Spaces . Organization Science 26 ( 5 ) : 1432 – 1446 . Simon HA ( 1947 ) Administrative Behavior : A study of Decision - making Processes in Administrative Organizations” , Mac Millan : Chicago . Simon HA ( 1997 ) Models of bounded rationality : Empirically grounded economic reason ( MIT press ) . Simon JGMHA ( 1958 ) Organizations Stroebe W , Diehl M ( 1994 ) Why Groups are less Effective than their Members : On Productivity Losses in Idea - generating Groups . European Review of Social Psychology 5 ( 1 ) : 271 – 303 . Taylor DW , Berry PC , Block CH ( 1958 ) Does Group Participation When Using Brainstorming Facilitate or Inhibit Creative Thinking ? Administrative Science Quarterly 3 ( 1 ) : 23 – 47 . Watson WE , Michaelsen LK , Sharp W ( 1991 ) Member competence , group interaction , and group decision making : A longitudinal study . Journal of Applied Psychology 76 ( 6 ) : 803 – 809 . Woolley AW , Chabris CF , Pentland A , Hashmi N , Malone TW ( 2010 ) Evidence for a Collective Intelligence Factor in the Performance of Human Groups . Science 330 ( 6004 ) : 686 – 688 . 38 Groups and Individuals in Learning from Experience Zajonc RB , Wolosin RJ , Wolosin MA , Sherman SJ ( 1968 ) Individual and group risk - taking in a two - choice situation . Journal of Experimental Social Psychology 4 ( 1 ) : 89 – 106 . How Groups and Individuals Differ in Learning from Experience TABLES TABLE 1 Descriptive Statistics and Pairwise Correlations of Key Variables a , b , c Matched Sample ( N = 45849 ; Competitions = 127 ) Variables Mean S . D . min max 1 2 3 4 5 6 7 8 9 1 . Final Performance . 04 1 . 045 - 8 . 943 9 . 261 2 . Group Contestant . 311 . 463 0 1 0 . 119 3 . Contestant Size 1 . 534 1 . 089 1 40 0 . 107 0 . 729 4 . Search Efforts 21 . 532 40 . 764 1 671 0 . 214 0 . 286 0 . 304 5 . Exploration . 519 . 677 0 6 . 416 0 . 156 0 . 097 0 . 080 0 . 061 6 . Avoid Overfitting . 043 . 202 0 1 0 . 097 0 . 089 0 . 085 0 . 268 0 . 005 7 . Past Badges . 856 3 . 692 0 100 0 . 100 0 . 131 0 . 168 0 . 335 - 0 . 013 0 . 112 8 . Past Competitions 2 . 155 7 . 265 0 166 0 . 107 0 . 156 0 . 194 0 . 342 - 0 . 008 0 . 113 0 . 922 9 . GC Preference . 088 . 234 0 1 0 . 067 0 . 043 0 . 026 0 . 107 - 0 . 022 0 . 043 0 . 241 0 . 245 10 . First Submission Score - . 752 1 . 527 - 9 . 36 3 . 678 0 . 493 - 0 . 005 - 0 . 003 0 . 034 - 0 . 446 0 . 059 0 . 056 0 . 058 0 . 058 Note : a Contestant Size measures the size of both individual and group contestants . b In the sample of matched group contestants only ( N = 14276 ) , Group Size has a mean of 2 . 715 , a standard deviation of 1 . 337 , a minimum value of 2 , and a maximum of 40 . The median group size is 2 ; the 75th percentile is 3 . c Descriptive Statistics and Correlations of the full sample ( including matched and non - matched contestants ) are available in the online appendix ( Appendix D ) . Correlations between variables in the matched and full samples are substantially similar . 2 Groups and Individuals in Learning from Experience Table 2 . Regression Analysis Results on Differences between Groups and Individuals in Exploration ( Groups Explore more than Individuals ) a ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) Exploration Exploration Exploration Exploration Exploration Search Efforts 0 . 0013 * * * ( 9 . 349 ) 0 . 0009 * * * ( 6 . 782 ) 0 . 0009 * * * ( 6 . 800 ) 0 . 0017 * * * ( 11 . 615 ) 0 . 0018 * * * ( 11 . 594 ) Past Badges - 0 . 0078 * * * ( - 4 . 519 ) - 0 . 0059 * * * ( - 3 . 472 ) - 0 . 0065 * * * ( - 3 . 584 ) - 0 . 0009 ( - 0 . 731 ) - 0 . 0009 ( - 0 . 660 ) GC Preference - 0 . 0014 ( - 0 . 117 ) - 0 . 0021 ( - 0 . 170 ) 0 . 0031 ( 0 . 255 ) - 0 . 0023 ( - 0 . 245 ) 0 . 0037 ( 0 . 399 ) Past Competitions 0 . 0032 * * * ( 3 . 910 ) 0 . 0018 * ( 2 . 220 ) 0 . 0019 * ( 2 . 145 ) - 0 . 0012 ( - 1 . 882 ) - 0 . 0014 * ( - 2 . 058 ) First Submission Score - 0 . 2069 * * * ( - 27 . 568 ) - 0 . 2060 * * * ( - 27 . 336 ) - 0 . 2061 * * * ( - 27 . 508 ) - 0 . 2117 * * * ( - 27 . 836 ) - 0 . 2118 * * * ( - 27 . 896 ) Group Contestant 0 . 1198 * * * ( 14 . 556 ) 0 . 0956 * * * ( 12 . 300 ) Contestant Size 0 . 0412 * * * ( 9 . 718 ) 0 . 0341 * * * ( 8 . 501 ) Constant 0 . 3365 * * * ( 57 . 587 ) 0 . 3093 * * * ( 52 . 110 ) 0 . 2818 * * * ( 36 . 517 ) 0 . 3059 * * * ( 52 . 967 ) 0 . 2744 * * * ( 41 . 609 ) Competition F . E . Yes Yes Yes Yes Yes Matched Sample Yes Yes Yes No No R2 0 . 227 0 . 233 0 . 230 0 . 234 0 . 233 Observations 45849 45849 45849 165255 165255 a t statistics in parentheses . Standard errors clustered at competition level . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 3 Groups and Individuals in Learning from Experience Table 3 . Regression Analysis Results on Differences between Groups and Individuals in Avoidance of Overfitting ( Groups Avoid Overfitting more than Individuals ) a ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Search Efforts 0 . 0013 * * * ( 9 . 295 ) 0 . 0013 * * * ( 9 . 157 ) 0 . 0013 * * * ( 9 . 190 ) 0 . 0017 * * * ( 8 . 756 ) 0 . 0017 * * * ( 8 . 756 ) 0 . 0015 * * * ( 9 . 058 ) 0 . 0013 * * * ( 8 . 266 ) 0 . 0013 * * * ( 9 . 153 ) 0 . 0018 * * * ( 10 . 444 ) Exploration 0 . 0024 ( 1 . 448 ) 0 . 0021 ( 1 . 282 ) 0 . 0024 ( 1 . 435 ) 0 . 0009 ( 0 . 558 ) 0 . 0010 ( 0 . 575 ) 0 . 0016 ( 0 . 982 ) 0 . 0022 ( 0 . 670 ) 0 . 0028 ( 1 . 692 ) 0 . 0001 ( 0 . 115 ) Past Badges 0 . 0014 ( 1 . 303 ) 0 . 0015 ( 1 . 357 ) 0 . 0014 ( 1 . 303 ) 0 . 0015 ( 1 . 410 ) 0 . 0015 ( 1 . 402 ) 0 . 0013 ( 1 . 215 ) 0 . 0009 ( 0 . 631 ) 0 . 0015 ( 1 . 352 ) 0 . 0020 * * * ( 3 . 552 ) GC Preference 0 . 0099 * ( 2 . 251 ) 0 . 0098 * ( 2 . 246 ) 0 . 0099 * ( 2 . 267 ) 0 . 0096 * ( 2 . 181 ) 0 . 0094 * ( 2 . 158 ) 0 . 0090 * ( 2 . 063 ) 0 . 0192 * ( 2 . 204 ) 0 . 0098 * ( 2 . 238 ) 0 . 0079 * ( 2 . 399 ) Past Competitions - 0 . 0001 ( - 0 . 135 ) - 0 . 0001 ( - 0 . 235 ) - 0 . 0001 ( - 0 . 135 ) 0 . 0000 ( 0 . 068 ) 0 . 0000 ( 0 . 103 ) 0 . 0001 ( 0 . 276 ) 0 . 0001 ( 0 . 157 ) - 0 . 0001 ( - 0 . 229 ) - 0 . 0001 ( - 0 . 983 ) First Submission Score 0 . 0041 * * ( 3 . 320 ) 0 . 0041 * * ( 3 . 309 ) 0 . 0041 * * ( 3 . 320 ) 0 . 0037 * * ( 3 . 138 ) 0 . 0037 * * ( 3 . 134 ) 0 . 0038 * * ( 3 . 194 ) 0 . 0055 * * ( 2 . 803 ) 0 . 0041 * * ( 3 . 311 ) 0 . 0032 * * * ( 3 . 394 ) Group Contestant 0 . 0040 ( 1 . 634 ) 0 . 0153 * * * ( 4 . 237 ) 0 . 0174 * * * ( 3 . 848 ) 0 . 0052 ( 1 . 763 ) 0 . 0215 * * * ( 4 . 437 ) Contestant Size - 0 . 0000 ( - 0 . 004 ) - 0 . 0014 ( - 0 . 892 ) 0 . 0036 * ( 2 . 204 ) 0 . 0007 ( 0 . 345 ) - 0 . 0024 ( - 1 . 604 ) Group Contestant x Search Efforts - 0 . 0006 * * * ( - 4 . 290 ) - 0 . 0006 * * * ( - 4 . 269 ) - 0 . 0007 * * * ( - 5 . 816 ) Contestant Size x Search Efforts - 0 . 0001 * ( - 2 . 464 ) - 0 . 0000 ( - 0 . 810 ) Group Contestant x Exploration - 0 . 0022 ( - 1 . 047 ) Constant 0 . 0145 * * * ( 4 . 862 ) 0 . 0137 * * * ( 4 . 600 ) 0 . 0145 * * * ( 4 . 854 ) 0 . 0081 * ( 2 . 253 ) 0 . 0094 * ( 2 . 602 ) 0 . 0079 * ( 2 . 061 ) 0 . 0211 * * ( 3 . 039 ) 0 . 0134 * * * ( 4 . 503 ) 0 . 0109 * * * ( 4 . 100 ) Competition F . E . Yes Yes Yes Yes Yes Yes Yes Yes Yes Matched Sample Yes Yes Yes Yes Yes Yes Yes Yes No Groups Only Sample No No No No No No Yes No No R2 0 . 116 0 . 116 0 . 116 0 . 119 0 . 119 0 . 118 0 . 143 0 . 117 0 . 098 Observations 45849 45849 45849 45849 45849 45849 14276 45849 165255 t statistics in parentheses . Standard errors clustered at competition level . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 4 Groups and Individuals in Learning from Experience Table 4 . Regression Analysis Results on Impact of Exploration and Avoidance of Overfitting on Final Performance a ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) Final Performance Final Performance Final Performance Final Performance Final Performance Final Performance Final Performance Final Performance Performance Improvement Search Efforts 0 . 0050 * * * ( 10 . 639 ) 0 . 0041 * * * ( 11 . 223 ) 0 . 0035 * * * ( 10 . 577 ) 0 . 0041 * * * ( 11 . 161 ) 0 . 0049 * * * ( 13 . 598 ) 0 . 0041 * * * ( 13 . 010 ) 0 . 0041 * * * ( 11 . 105 ) 0 . 0050 * * * ( 14 . 291 ) 0 . 0041 * * * ( 13 . 010 ) Past Badges - 0 . 0073 * * ( - 2 . 632 ) - 0 . 0020 ( - 0 . 861 ) 0 . 0001 ( 0 . 050 ) - 0 . 0019 ( - 0 . 859 ) - 0 . 0023 ( - 0 . 980 ) - 0 . 0002 ( - 0 . 084 ) - 0 . 0019 ( - 0 . 851 ) 0 . 0014 ( 0 . 669 ) - 0 . 0002 ( - 0 . 084 ) GC Preference 0 . 1030 * * * ( 7 . 565 ) 0 . 1040 * * * ( 7 . 984 ) 0 . 1081 * * * ( 9 . 214 ) 0 . 1040 * * * ( 8 . 027 ) 0 . 1010 * * * ( 7 . 795 ) 0 . 1056 * * * ( 9 . 066 ) 0 . 1045 * * * ( 8 . 064 ) 0 . 0775 * * * ( 8 . 461 ) 0 . 1056 * * * ( 9 . 066 ) Past Competitions 0 . 0053 * * * ( 4 . 112 ) 0 . 0032 * * ( 2 . 704 ) 0 . 0026 * ( 2 . 347 ) 0 . 0032 * * ( 2 . 705 ) 0 . 0031 * * ( 2 . 617 ) 0 . 0025 * ( 2 . 283 ) 0 . 0031 * * ( 2 . 698 ) 0 . 0025 * * ( 2 . 659 ) 0 . 0025 * ( 2 . 283 ) First Submission Score 0 . 3124 * * * ( 27 . 783 ) 0 . 4530 * * * ( 25 . 766 ) 0 . 4534 * * * ( 26 . 488 ) 0 . 4530 * * * ( 25 . 780 ) 0 . 4517 * * * ( 25 . 761 ) 0 . 4524 * * * ( 26 . 494 ) 0 . 4530 * * * ( 25 . 784 ) 0 . 4872 * * * ( 24 . 620 ) - 0 . 5476 * * * ( - 32 . 074 ) Exploration 0 . 6798 * * * ( 11 . 604 ) 1 . 0793 * * * ( 14 . 944 ) 0 . 6798 * * * ( 11 . 606 ) 0 . 6761 * * * ( 11 . 561 ) 1 . 0672 * * * ( 14 . 884 ) 0 . 6844 * * * ( 11 . 711 ) 1 . 0517 * * * ( 15 . 411 ) 1 . 0672 * * * ( 14 . 884 ) Exploration 2 - 0 . 1553 * * * ( - 8 . 925 ) - 0 . 1518 * * * ( - 8 . 896 ) - 0 . 1362 * * * ( - 9 . 205 ) - 0 . 1518 * * * ( - 8 . 896 ) Avoid Overfitting - 0 . 0058 ( - 0 . 183 ) 0 . 2306 * * * ( 6 . 465 ) 0 . 1899 * * * ( 5 . 303 ) 0 . 1023 * ( 2 . 396 ) 0 . 1819 * * * ( 7 . 080 ) 0 . 1899 * * * ( 5 . 303 ) Avoid Overfitting x Search Efforts - 0 . 0037 * * * ( - 8 . 977 ) - 0 . 0031 * * * ( - 7 . 982 ) - 0 . 0037 * * * ( - 10 . 293 ) - 0 . 0031 * * * ( - 7 . 982 ) Avoid Overfitting x Exploration - 0 . 2009 ( - 1 . 865 ) Constant 0 . 1533 * * * ( 15 . 231 ) - 0 . 0755 * * ( - 3 . 201 ) - 0 . 1562 * * * ( - 6 . 867 ) - 0 . 0754 * * ( - 3 . 188 ) - 0 . 0883 * * * ( - 3 . 843 ) - 0 . 1652 * * * ( - 7 . 378 ) - 0 . 0778 * * ( - 3 . 306 ) - 0 . 1527 * * * ( - 8 . 037 ) - 0 . 1652 * * * ( - 7 . 378 ) Competition F . E . Yes Yes Yes Yes Yes Yes Yes Yes Yes Matched Sample Yes Yes Yes Yes Yes Yes Yes No Yes R2 0 . 343 0 . 493 0 . 512 0 . 493 0 . 496 0 . 514 0 . 494 0 . 540 0 . 713 Observations 45849 45849 45849 45849 45849 45849 45849 165255 45849 a t statistics in parentheses . Standard errors clustered at competition level . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 . 5 Groups and Individuals in Learning from Experience Table 5 . Additional Regression Analysis Results for Final Performance and Performance Improvement a ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) Final Performance Final Performance Final Performance Final Performance Final Performance Final Performance Final Performance Performance Improvement Past Badges - 0 . 0048 ( - 1 . 731 ) - 0 . 0008 ( - 0 . 336 ) - 0 . 0012 ( - 0 . 531 ) - 0 . 0004 ( - 0 . 166 ) - 0 . 0003 ( - 0 . 136 ) - 0 . 0008 ( - 0 . 361 ) 0 . 0040 * ( 2 . 190 ) - 0 . 0003 ( - 0 . 136 ) GC Preference 0 . 1022 * * * ( 7 . 506 ) 0 . 1036 * * * ( 8 . 002 ) 0 . 1066 * * * ( 8 . 205 ) 0 . 1017 * * * ( 7 . 987 ) 0 . 1028 * * * ( 8 . 097 ) 0 . 1034 * * * ( 7 . 945 ) 0 . 0737 * * * ( 8 . 106 ) 0 . 1028 * * * ( 8 . 097 ) Past Competitions 0 . 0035 * ( 2 . 611 ) 0 . 0023 ( 1 . 903 ) 0 . 0024 * ( 2 . 082 ) 0 . 0035 * * ( 3 . 047 ) 0 . 0033 * * ( 2 . 973 ) 0 . 0023 ( 1 . 911 ) 0 . 0020 * ( 2 . 233 ) 0 . 0033 * * ( 2 . 973 ) Group Contestant 0 . 1570 * * * ( 12 . 019 ) 0 . 0762 * * * ( 7 . 859 ) 0 . 1725 * * * ( 13 . 385 ) 0 . 1578 * * * ( 11 . 267 ) 0 . 0880 * * * ( 5 . 656 ) 0 . 1380 * * * ( 9 . 288 ) 0 . 1578 * * * ( 11 . 267 ) Search Efforts 0 . 0045 * * * ( 10 . 053 ) 0 . 0039 * * * ( 10 . 689 ) 0 . 0039 * * * ( 10 . 703 ) 0 . 0075 * * * ( 14 . 826 ) 0 . 0075 * * * ( 14 . 826 ) 0 . 0039 * * * ( 10 . 652 ) 0 . 0066 * * * ( 13 . 501 ) 0 . 0075 * * * ( 14 . 826 ) First Submission Score 0 . 3135 * * * ( 27 . 680 ) 0 . 4526 * * * ( 25 . 731 ) 0 . 4529 * * * ( 25 . 753 ) 0 . 4496 * * * ( 25 . 695 ) 0 . 4496 * * * ( 25 . 701 ) 0 . 4526 * * * ( 25 . 758 ) 0 . 4866 * * * ( 24 . 360 ) - 0 . 5504 * * * ( - 31 . 460 ) Exploration 0 . 6748 * * * ( 11 . 554 ) 0 . 6770 * * * ( 11 . 557 ) 0 . 6648 * * * ( 11 . 525 ) 0 . 6646 * * * ( 11 . 518 ) 0 . 6807 * * * ( 12 . 064 ) 0 . 6978 * * * ( 12 . 107 ) 0 . 6646 * * * ( 11 . 518 ) Avoid Overfitting - 0 . 0074 ( - 0 . 237 ) - 0 . 0058 ( - 0 . 185 ) - 0 . 0287 ( - 0 . 924 ) - 0 . 0286 ( - 0 . 920 ) - 0 . 0075 ( - 0 . 241 ) - 0 . 0208 ( - 0 . 887 ) - 0 . 0286 ( - 0 . 920 ) Contestant Size 0 . 0234 * * * ( 5 . 324 ) 0 . 0094 ( 1 . 934 ) 0 . 0099 ( 1 . 785 ) 0 . 0094 ( 1 . 934 ) Group Contestant x Search Efforts - 0 . 0048 * * * ( - 15 . 259 ) - 0 . 0049 * * * ( - 15 . 320 ) - 0 . 0042 * * * ( - 12 . 274 ) - 0 . 0049 * * * ( - 15 . 320 ) Group Contestant x Exploration - 0 . 0203 ( - 0 . 939 ) Constant 0 . 1177 * * * ( 11 . 287 ) - 0 . 0910 * * * ( - 3 . 860 ) - 0 . 1054 * * * ( - 4 . 530 ) - 0 . 1388 * * * ( - 5 . 520 ) - 0 . 1481 * * * ( - 5 . 995 ) - 0 . 0936 * * * ( - 4 . 093 ) - 0 . 1129 * * * ( - 5 . 538 ) - 0 . 1481 * * * ( - 5 . 995 ) Competition F . E . Yes Yes Yes Yes Yes Yes Yes Yes Matched Sample Yes Yes Yes Yes Yes Yes No Yes R2 0 . 348 0 . 494 0 . 494 0 . 500 0 . 500 0 . 494 0 . 528 0 . 705 Observations 45849 45849 45849 45849 45849 45849 165255 45849 a t statistics in parentheses . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 . Standard errors clustered at competition level . 6 Groups and Individuals in Learning from Experience Table 6 . Regression Analysis Results Comparing Groups with HHI = 1 and Their Matched Individuals a , b ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Exploration Search Efforts Avoid Overfitting Avoid Overfitting Final Performance Final Performance Search Efforts 0 . 0036 * * * ( 11 . 118 ) 0 . 0016 * * * ( 7 . 506 ) 0 . 0016 * * * ( 6 . 809 ) 0 . 0085 * * * ( 12 . 717 ) 0 . 0085 * * * ( 12 . 880 ) Past Badges - 0 . 0074 ( - 1 . 521 ) 1 . 0031 * ( 2 . 318 ) - 0 . 0020 ( - 0 . 751 ) - 0 . 0020 ( - 0 . 751 ) 0 . 0072 ( 1 . 122 ) 0 . 0072 ( 1 . 127 ) GC Preference 0 . 0070 ( 0 . 388 ) 1 . 3098 ( 1 . 754 ) - 0 . 0121 * ( - 2 . 156 ) - 0 . 0121 * ( - 2 . 157 ) 0 . 0762 * * * ( 3 . 487 ) 0 . 0764 * * * ( 3 . 499 ) Past Competitions 0 . 0039 ( 1 . 765 ) 0 . 1190 ( 0 . 873 ) 0 . 0025 * ( 2 . 417 ) 0 . 0025 * ( 2 . 413 ) 0 . 0031 ( 1 . 195 ) 0 . 0031 ( 1 . 207 ) First Submission Score - 0 . 2000 * * * ( - 23 . 261 ) 0 . 3739 * ( 2 . 399 ) 0 . 0031 * * ( 3 . 361 ) 0 . 0031 * * ( 3 . 366 ) 0 . 5438 * * * ( 30 . 952 ) 0 . 5437 * * * ( 30 . 916 ) Group Contestant 0 . 0165 ( 1 . 614 ) 1 . 1772 * * * ( 3 . 465 ) - 0 . 0014 ( - 0 . 595 ) - 0 . 0014 ( - 0 . 581 ) 0 . 0100 ( 0 . 774 ) 0 . 0134 ( 0 . 755 ) Exploration - 0 . 0004 ( - 0 . 237 ) - 0 . 0004 ( - 0 . 236 ) 0 . 7686 * * * ( 12 . 450 ) 0 . 7686 * * * ( 12 . 437 ) Group Contestant ´ Search Efforts 0 . 0000 ( 0 . 017 ) - 0 . 0003 ( - 0 . 354 ) Avoid Overfitting - 0 . 0173 ( - 0 . 417 ) - 0 . 0173 ( - 0 . 417 ) Constant 0 . 2633 * * * ( 33 . 924 ) 12 . 0151 * * * ( 69 . 742 ) 0 . 0103 * * * ( 4 . 298 ) 0 . 0103 * * * ( 4 . 104 ) - 0 . 1305 * * * ( - 5 . 465 ) - 0 . 1316 * * * ( - 5 . 614 ) Competition Fixed Effect Yes Yes Yes Yes Yes Yes Observations 19135 19135 19135 19135 19135 19135 R 2 0 . 227 0 . 037 0 . 098 0 . 098 0 . 580 0 . 580 a t statistics in parentheses . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 . Standard errors clustered at competition level . b The sample includes the matched group contestant with HHI = 1 and the individual contestants that were originally matched with members of the groups with HHI = 1 . We restricted the sample of groups to exclusively the ones having an HHI equal to 1 , i . e . , the groups where only one group member submitted solutions in the alternative generation stage . As a consequence , we kept in the sample of “counterfactual” individuals only the individuals that were matched to the group members of groups having an HHI equal to 1 . 7 Groups and Individuals in Learning from Experience Table 7 . Regression Analysis on Impact of Concentration of Action and Influence in Groups on Exploration , Avoidance of Overfitting and Final Performance a , b ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) Exploration Avoid Overfitting Avoid Overfitting Final Performance Final Performance Search Efforts - 0 . 0005 * * * ( - 4 . 133 ) 0 . 0011 * * * ( 8 . 196 ) 0 . 0007 * * ( 2 . 899 ) 0 . 0025 * * * ( 8 . 450 ) - 0 . 0020 * * * ( - 5 . 924 ) Past Badges - 0 . 0076 * * * ( - 3 . 869 ) 0 . 0010 ( 0 . 661 ) 0 . 0010 ( 0 . 692 ) - 0 . 0024 ( - 0 . 973 ) - 0 . 0019 ( - 0 . 792 ) GC Preference - 0 . 0351 ( - 1 . 729 ) 0 . 0193 * ( 2 . 230 ) 0 . 0166 ( 1 . 885 ) 0 . 1698 * * * ( 6 . 661 ) 0 . 1395 * * * ( 5 . 784 ) Past Competitions 0 . 0019 * ( 2 . 034 ) 0 . 0000 ( 0 . 016 ) 0 . 0001 ( 0 . 191 ) 0 . 0016 ( 1 . 179 ) 0 . 0030 * ( 2 . 295 ) First Submission Score - 0 . 1908 * * * ( - 24 . 981 ) 0 . 0054 * * ( 2 . 730 ) 0 . 0050 * ( 2 . 607 ) 0 . 2732 * * * ( 13 . 770 ) 0 . 2697 * * * ( 13 . 899 ) Contestant Size - 0 . 0015 ( - 0 . 378 ) - 0 . 0016 ( - 0 . 981 ) - 0 . 0005 ( - 0 . 292 ) - 0 . 0033 ( - 0 . 787 ) 0 . 0090 * ( 2 . 452 ) HHI - 0 . 4966 * * * ( - 19 . 051 ) - 0 . 0200 ( - 1 . 910 ) - 0 . 0414 * * ( - 2 . 708 ) - 0 . 3530 * * * ( - 11 . 566 ) - 0 . 5929 * * * ( - 14 . 070 ) Exploration 0 . 0010 ( 0 . 301 ) - 0 . 0002 ( - 0 . 067 ) 0 . 4638 * * * ( 7 . 966 ) 0 . 4498 * * * ( 7 . 982 ) HHI ´ Search Efforts 0 . 0007 * ( 2 . 483 ) 0 . 0081 * * * ( 9 . 561 ) Avoid Overfitting 0 . 0185 ( 0 . 527 ) - 0 . 0004 ( - 0 . 011 ) Constant 0 . 8855 * * * ( 32 . 779 ) 0 . 0449 * * * ( 3 . 667 ) 0 . 0567 * * * ( 3 . 761 ) 0 . 3088 * * * ( 7 . 578 ) 0 . 4419 * * * ( 11 . 279 ) Competition Fixed Effect Yes Yes Yes Yes Yes Observations 14276 14276 14276 14276 14276 R 2 0 . 246 0 . 143 0 . 145 0 . 377 0 . 392 a t statistics in parentheses . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 . Standard errors clustered at competition level . b The sample includes exclusively groups in the matched sample . 8 Groups and Individuals in Learning from Experience Table 8 . Regression Analysis Results on Differences Between Interacting and Nominal Groups in their Exploration and Avoidance of Overfitting Across multiple Samples a , b ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) Exploration Exploration Exploration Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Avoid Overfitting Interacting Group - 0 . 2321 * * * ( - 15 . 040 ) 0 . 0455 * * * ( 3 . 760 ) 0 . 1297 * * * ( 12 . 956 ) 0 . 0158 * * * ( 4 . 552 ) - 0 . 0034 ( - 1 . 082 ) 0 . 0103 * * ( 2 . 752 ) 0 . 0036 ( 1 . 366 ) 0 . 0147 * * * ( 3 . 999 ) Search Efforts - 0 . 0000 ( - 0 . 063 ) 0 . 0003 * * ( 2 . 943 ) 0 . 0005 * * * ( 4 . 544 ) 0 . 0012 * * * ( 8 . 967 ) 0 . 0013 * * * ( 8 . 873 ) 0 . 0017 * * * ( 7 . 466 ) 0 . 0012 * * * ( 9 . 283 ) 0 . 0018 * * * ( 8 . 895 ) Past Badges - 0 . 0061 * * * ( - 3 . 831 ) - 0 . 0057 * * ( - 2 . 851 ) - 0 . 0062 * * * ( - 3 . 600 ) 0 . 0019 ( 1 . 708 ) 0 . 0018 ( 1 . 393 ) 0 . 0017 ( 1 . 366 ) 0 . 0015 ( 1 . 213 ) 0 . 0014 ( 1 . 176 ) GC Preference - 0 . 0675 * * ( - 3 . 234 ) - 0 . 0180 ( - 1 . 356 ) - 0 . 0173 ( - 1 . 177 ) 0 . 0184 * ( 2 . 610 ) 0 . 0173 * ( 2 . 156 ) 0 . 0172 * ( 2 . 132 ) 0 . 0118 * ( 2 . 277 ) 0 . 0115 * ( 2 . 209 ) Past Competitions 0 . 0017 * ( 2 . 260 ) 0 . 0021 * ( 2 . 222 ) 0 . 0023 * * ( 2 . 909 ) - 0 . 0005 ( - 1 . 135 ) - 0 . 0003 ( - 0 . 566 ) - 0 . 0001 ( - 0 . 249 ) - 0 . 0002 ( - 0 . 306 ) - 0 . 0000 ( - 0 . 040 ) First Submission Score - 0 . 1862 * * * ( - 24 . 314 ) - 0 . 2321 * * * ( - 35 . 537 ) - 0 . 2047 * * * ( - 25 . 887 ) 0 . 0036 * * ( 2 . 878 ) 0 . 0047 * * * ( 3 . 558 ) 0 . 0050 * * * ( 3 . 713 ) 0 . 0041 * * ( 2 . 839 ) 0 . 0039 * * ( 2 . 782 ) Interacting Group x Search Efforts - 0 . 0006 * * * ( - 3 . 460 ) - 0 . 0007 * * * ( - 4 . 347 ) Constant 0 . 7176 * * * ( 54 . 412 ) 0 . 3849 * * * ( 50 . 574 ) 0 . 3130 * * * ( 44 . 637 ) 0 . 0064 ( 1 . 168 ) 0 . 0237 * * * ( 5 . 460 ) 0 . 0144 * ( 2 . 508 ) 0 . 0177 * * * ( 4 . 766 ) 0 . 0096 * ( 2 . 226 ) Competition F . E . Yes Yes Yes Yes Yes Yes Yes Yes Matched Sample Yes Yes Yes Yes Yes Yes Yes Yes Comparison Sample Nominal Group Pooled Best Individual Random Individual Nominal Group Pooled : Choose Best Submission Best Individual Best Individual Random Individual Random Individual Exclude Excess Submission Yes No No Yes No No No No R2 0 . 227 0 . 292 0 . 234 0 . 120 0 . 123 0 . 125 0 . 128 0 . 131 Observations 24906 28520 28520 24906 28520 28520 28520 28520 a t statistics in parentheses . * p < 0 . 05 , * * p < 0 . 01 , * * * p < 0 . 001 . Standard Errors Clustered at Competition Level . b As nominal groups do not face the daily limits of submissions imposed on real contestants , we identified the nominal groups that would have violated such a constraint . Models 1 and 4 excludes from the sample the nominal groups that would have violated the constraint and the corresponding interacting groups . 9 Groups and Individuals in Learning from Experience FIGURES Figure 1 . Representation of the Process of Kaggle Competitions a a Note : Kaggle data does not allow us to directly observe the full set of final submissions chosen by the contestant in the alternative evaluation phase . It only allows us to observe which was the best - performing submission on the hold - out data set among all the final submissions made by the contestants . For example , if a contestant submitted Models A , B , and C , and Model B performed the best in the hold - out dataset , we only observe this one submission and do not have access to the fact that the contestant also submitted Models A and C . We classify a contestant as Avoiding Overfitting if the best performing submission on the holdout data set in stage 2 was not among the best k models in terms of public score ( accuracy of the model on the test set ) in stage 1 . 10 Groups and Individuals in Learning from Experience Figure 2 . Interaction Plot for Group Contestant and Search Efforts on Avoidance of Overfitting Note : For visualization purposes , the Search Efforts axis is limited to the 95 th percentile of Search Efforts . The two lines represent the predictive margins for Group and Individual Contestants at multiple levels of Search Efforts . The histograms on the background represent the distribution of observations across the different levels of search efforts . 11 Groups and Individuals in Learning from Experience Figure 3 . Interaction Plot for Group Contestant and Search Efforts on Final Performance Note : For visualization purposes , the Search Efforts axis is limited to the 95 th percentile of Search Efforts . The two lines represent the predictive margins for Group and Individual Contestants at multiple levels of Search Efforts . The histograms on the background represent the distribution of observations across the different levels of search efforts .