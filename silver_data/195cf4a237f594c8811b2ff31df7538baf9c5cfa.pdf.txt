a r X i v : 2203 . 13292v1 [ c s . H C ] 24 M a r 2022 Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? Jie Cai New Jersey Institute of Technology Newark , USA jie . cai @ njit . edu Donghee Yvette Wohn New Jersey Institute of Technology Newark , USA yvettewohn @ gmail . com ABSTRACT Volunteer moderators ( mods ) play signiﬁcant roles in developing moderation standards and dealing with harmful content in their micro - communities . However , little work explores how volunteer mods work as a team . In line with prior work about understanding volunteer moderation , we interview 40 volunteer mods on Twitch — a leading live streaming platform . We identify how mods col - laborate on tasks ( oﬀ - streaming coordination and preparation , in - stream real - time collaboration , and relationship building both oﬀ - stream and in - stream to reinforce collaboration ) and how mods contribute to moderation standards ( collaboratively working on the community rulebookand individually shaping community norms ) . We uncover how volunteer mods work as an eﬀective team . We also discuss how the aﬀordances of multi - modal communication and informality of volunteer moderation contribute to task collab - oration , standards development , and mod’s roles and responsibili - ties . CCS CONCEPTS • Human - centered computing → Empirical studies in HCI . KEYWORDS Teamwork ; volunteermods ; content moderation ; coordination ; col - laboration ; rules and norms ; live streaming ; online community ACM Reference Format : Jie Cai and Donghee Yvette Wohn . 2022 . Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Com - munities ? . In CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) , April 29 - May 5 , 2022 , New Orleans , LA , USA . ACM , New York , NY , USA , 14 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3517628 1 INTRODUCTION Harmful content , such as hate speech , online abuses , harassment , and cyberbullying , proliferates across all diﬀerent types of online communities . Live streaming is a promising and fast - growing in - dustry . Many social media platforms have live streaming services ( e . g . , Facebook , Instagram , YouTube ) , even text - based online com - munities like Reddit have a live streaming subreddit . Reports show Permission to make digital or hard copies of all or part of this work for personal or classroomuseisgranted without fee provided that copiesarenot made ordistributed for proﬁt or commercial advantage and that copies bear this notice and the full cita - tion on the ﬁrst page . Copyrights for components of this work owned by others than ACMmustbehonored . Abstractingwith credit ispermitted . Tocopy otherwise , orre - publish , topost on serversortoredistributeto lists , requirespriorspeciﬁcpermission and / or a fee . Request permissions from permissions @ acm . org . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA © 2022 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 9157 - 3 / 22 / 04 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3491102 . 3517628 that all main live streaming platforms have big gains during the pandemic [ 17 ] and that the live streaming market is predicted to be worth over $ 247 billion by 2027 [ 73 ] . Live streaming as a novel type of online community provides ways for thousands of users ( viewers ) to entertain and engage with a broadcaster ( streamer ) in real - time in the chatroom [ 72 ] . While the streamer has the camera on and the screen shared , tens of thousands of viewers are watch - ing and messaging in real - time , resulting in concerns about harass - ment and cyberbullying to the streamer [ 67 , 75 ] . To keep a safe and civil online space , platforms and community administrators develop guidelines and enforce them with a combi - nation of algorithmic approaches and human moderators ( mods ) ( either paid or volunteer ) to deal with harmful content , which is termed as “content moderation” . Live streaming as a mixed media contains some unique attributes such as synchronicity and authen - ticity [ 29 ] , making real - time moderation challenging . Given that many platforms heavily rely on human mods who are voluntary to support their communities like Twitch [ 61 , 70 ] , it is essential to understand how these volunteer mods collabo - rate as a team , which has been by large overlooked with many HCI scholarships focusing only on the labor aspect [ 16 ] , mods’ decision - making process [ 8 , 9 ] and engagement [ 61 , 70 ] , and the moderation challenges [ 37 ] . Volunteer mods are not professional employees with good training and moderate content in an infor - mal manner . While some studies more or less have mentioned that mods involve rules development and communication with other mods , the discussion about human mods usually treats mods as in - dividual entities . There is a lack of a speciﬁc piece to understand mods’ coordination and collaboration , a gap this work intends to ﬁll . Exploringmoderation team coordinationand collaborationcan potentially help avoid team conﬂicts , improve team relationships , and consequently , help build an eﬀective team to grow the commu - nity . In line with recent work to unpack the opaque operation of con - tent moderation ( e . g . , [ 36 , 64 , 70 ] ) , we interview 40 volunteer mods in live streaming communities to explore how they contribute to the moderation standards and collaborate on tasks as a team . Our work mainly contributes to understanding the team collaboration of volunteer mods in the moderation context . We reveal how the streamer and mods work as an eﬀective team and the nuanced dif - ferences of volunteer moderation in live streaming communities . Our ﬁndings can beneﬁt multi - stakeholders , such as the commu - nity admin , each volunteer mod , and the admin and modsas a team for the community growth . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee 2 RESEARCH CONTEXT : LIVE STREAMING PLATFORM TWITCH Twitch is the leading live streaming platform and consists of thou - sands of streaming channels centered on streamers . Reports show that 3 . 8 million streamers were broadcasting on Twitch in 2020 [ 73 ] . Each streamer owns the power to assign or revoke other users as mods with special badges to indicate their mod status . When the streamer goes live , each viewer can clearly see how many viewers are watching at the time and check the viewer list to know other viewers’ usernames . Each viewer can also see the active mods on - line at the top of the viewer list . Most viewers apply pseudony - mous usernames without showing much information about their identities . Viewers can send messages in the public chatroom or send a Whisper message to a viewer privately by clicking on the viewer’s username . Streamers can post their streaming schedules to their homepages so that viewers can know in advance . Followed or subscribed viewers can also receive email notiﬁcations before the streaming . Twitch applies the combination of human mods and modera - tion tools to govern online communities . At the platform level , it has community guidelines and terms of service , which are en - forced by paid staﬀ to mainly deal with video content violation and user reporting . At the channel or micro - community level , it allows streamers and their appointed mods to set up channel rules and even speciﬁc chat rules . Mods are selected from the stream - ers’ close friends or active viewers in the community [ 70 ] . In some micro - communities , the experienced mods can be the head mods . Many micro - communities consist of only the streamer and other mods without a clear hierarchy . A streamer can also be considered a mod with full access to moderation functions . To support mod - eration , Twitch also provides a moderation tool called “AutoMod” to facilitate mods to ﬁlter certain words and punish violators with commands ( ban , timeout , delete ) ; Twitch also provides open API to third - party developers to develop bots with more customized features to support the team to conduct moderation tasks [ 7 ] . We chose Twitch as the research context because it is a leading platform in a promising industry , but there is limited understand - ing about volunteer moderation teams . In this study , we focus on volunteer mods led by the streamer to form a moderation team and explore how they work together to moderate the chat messages and viewers in an interactive environment . 3 RELATED WORK 3 . 1 Volunteer Moderation and Remote Collaboration 3 . 1 . 1 Content Moderation , Volunteer Moderators , and Moderation Practices . Content moderationrefers to “thegovernance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse” [ 27 ] and is achieved by the collaboration of hu - man mods and algorithms for most platforms . Algorithmic con - tent moderation can deal with obvious violations at scale but lacks of context - sensitivity , as Gillespie [ 25 ] points out that “automated content moderation is not a panacea for the ills of social media , and maybe contrary to the principlesof governance that platforms should be pursuing . ” Human mods are still heavily relied on by most plat - forms , either paid laborershired by orcontractedwith the platform ( commercial mods ) [ 24 , 58 ] , or free laborers with voluntary partic - ipation to help the community ( volunteer mods ) [ 70 ] . Prior work in HCI and CSCW about content moderation has broadly discussed the physical and emotional labor [ 16 , 58 ] , psy - chologicalwell - being [ 64 ] , rules and norms [ 10 ] , moderationmech - anisms such as ﬂag , removal , and ban [ 12 , 42 , 63 ] , relation with either the end - users or the admins [ 8 , 70 ] , and the support with various design interventions ( see meta - analysis by [ 20 ] ) . In many online communities consisting of thousands of micro - communities ( e . g . , Reddit with subreddits , Discord with servers , and Twitch with streamer channels ) , volunteer mods play a major role in governing their micro - communities . A group of research in - volving volunteer mods often treats them as individual entities to conduct the moderation practices , such as the moderation tools they have used [ 7 , 35 ] and the diﬀerent roles they have played in the community [ 55 , 59 , 70 ] . These roles involve various tasks and moderation strategies , such as helping the streamer to man - age viewers [ 71 ] and communicating with violators [ 9 ] . The application of moderation strategies and doing tasks re - quires much communication and coordination . Though some work points out the discussion among other mods or the admin ( e . g . , [ 9 , 61 ] ) , there is a lack of work to investigate mods collaboration and coordination mechanism . This work extends existent research in volunteer moderationregarding mods’ individual decision - making to the collective decision - making process for two reasons . First , compared with commercial mods working in the formal environ - ment with training and clear instructions to follow [ 26 ] , volunteer mods might need more collaboration and coordination to ﬁgure out everything on their own because of the informal environment with vague moderation guidelines , consequently taking more of the decision - making roles in the process . Second , since volunteer moderation often involves many users in the governance process [ 27 ] , it is essential to understand how these users work as a group becausemany online communities need tobreak intomicro - communities as they grow . 3 . 1 . 2 RemoteCollaborationand Teamwork . Existentresearch about remote collaboration is well - grounded from oﬄine to online in both HCI and CSCW . In an oﬄine context , remote collaboration is well - established from empirical and theoretical perspectives . For example , lots of work has explored paradigms and prototypes to overcome the shortcomings of remote work in decades , such as dis - tance and physical presence ( e . g . , [ 18 , 33 , 44 ] ) . In socio - technological conditions , Olson and Olson [ 54 ] propose that four key compo - nents determine eﬀective remote collaboration , also called “dis - tance framework” [ 2 ] : common ground , coupling / dependency of work , collaboration readiness , and collaboration technology readi - ness , and suggest thatgroups , with high common groundand loosely coupled work , with readiness both for collaboration and collabora - tion technology , have the potential to succeed with teleworking , and that deviations from each component restrict teammates’ per - formance and require changes during the collaboration . Collaboration is a high order type of collective action and has higher interaction , intensity , and integration compared with co - ordination [ 38 , 69 ] . Coordination is a process of integrating and Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA aligning the actions , knowledge , and objectives of group members to achieve common goals [ 22 , 57 ] . There are two types of coordina - tion : explicit coordination requiring communication to coordinate group activities and implicit coordination requiring shared men - tal models and common grounds to avoid overt communications [ 22 , 47 ] . In this work , we disentangle coordination and collabo - ration ; we use collaboration to describe practices involving much communication ; we use coordination to describe practices involv - ing more guidance and direction and less interaction . In online collaboration , there are two main threads of research in HCI and CSCW . One thread is about distributed teamwork with organized , stable , and long - last collaboration in contexts such as teleworking , software development , and education ( e . g . , [ 2 , 3 , 19 , 31 , 62 ] ) ; another is about virtual teamwork with informal , unsta - ble , and short - term collaboration in contexts such as gaming ( e . g . , [ 15 , 22 , 46 ] ) and peer production communities ( e . g . , [ 39 – 41 ] ) . In the informal , unstable , and short - term collaborative process , many scholars have investigated the group characteristics of the volun - teer workers and how these characteristics are related to group productivity and outcome quality ( see meta - analysis by [ 1 , 51 ] ) . For example , the interest and experience diversity of the editor team on Wikipedia can positively inﬂuence the quality of articles [ 65 ] ; teams should maintain a balance between administrators and content creators as both contribute to the collaborative process [ 1 ] . As the community grows , the conﬂicts increase , and costs of coordination , such as conﬂict resolution , consensus building , and community management , also increase [ 41 ] . Coordination mecha - nisms were not always eﬀective for managing diﬀerent conﬂicts in the team [ 40 ] , and both implicit coordination through editor con - centration and explicit coordination through communication can potentially improve the article quality when they were used [ 39 ] . While most scholarships focus on the editorial team and the visi - ble outcome – though some have mentioned that editors can revert vandalism , little work explores the moderation team and the invis - ible content management , namely , how volunteer mods work as a team to get rid of harmful content , such as vandalism on Wikipedia and harassment on Twitch . While remote collaboration is well - established in contexts tar - geting professional workers and unprofessional volunteers in peer production communities , it is less clear how unprofessional volun - teers collaborate in the content moderation system , usually lack - ing transparency [ 34 , 53 , 58 ] . These mods are voluntary to work behind the scenes to deal with harmful content . However , little is known about how they collaborate on tasks . The synchronicity of live streaming might also cause the team to work diﬀerently compared with asynchronous communities , because the real - time aﬀordance brings all mods online simultaneously , and mods have to make immediate decisions as the chat messages ﬂow . The syn - chronicity also indicates that a single mod can’t eﬀectively concen - trate on much information generated in real - time ; working as a team with instant and continuous monitoring is imperative . Thus , we ask : • RQ1 : How do mods collaborate on moderation tasks in the moderation team ? 3 . 2 Community Norms and Rules Norms are “regularities in attitudes and behavior that characterize a social group” [ 30 ] . Norms play signiﬁcant roles in shaping on - line communities by indicating the group identity and regulating user behaviors [ 10 , 13 ] . Online community norms have been well - established over the past few decades and have not halted as online spaces evolve and diversify ( e . g . , [ 4 , 13 , 45 , 48 , 74 ] ) . In online communities applying volunteer moderation , norms vary across micro - communities , diﬀerent fromguidelines and terms of service aggregated by the platform and applied to all stakehold - ers . Much work has explored norms’ impact on user’s perceptions and behaviors , such as setting a good interaction example [ 60 ] and clarifying norms [ 9 , 36 ] to mitigate harassment behaviors . Norms are diﬀerent from but interrelated with rules and can be transferred regarding the explicitness [ 14 ] . Some work does not distinguish rules and norms and uses them interchangeably ( e . g . , [ 23 , 66 ] ) ; other work applies the formalization to disentangle rules and norms like formalized rules and informal norms [ 11 , 21 ] . Over - all , norms can be ﬁnalized into well - written rules and guidelines . Some work focuses explicitly on the rules and tries to understand rule content and expression [ 6 , 21 ] . Norms are usually about the user’s perception of other users’ thoughts and about the identity and behavioral conformity with other users in the group [ 14 ] . We focus on the mods’ perception of rules and norms they follow . In this work , we use “moderation standard” to indicate what mods follow to conduct moderation , in - cluding the explicitly written text of rules ( e . g . , chat rules , Twitch community guidelines , terms of service ) and the implicit norms among the moderation team or in the micro - community . Much priorwork explores the “what” question aboutrules and highlights the signiﬁcance of rules and norms for community development ( e . g . , [ 10 , 11 ] ) . Some have mentioned that the mods can help the admin to develop rules for the community [ 61 ] , but lack speciﬁcity in the nuanced context in volunteer moderation and clear answers to the “how” question . Thus , we ask : • RQ2 : How do mods contribute to community moderation standards ? 4 METHODS 4 . 1 Participants Recruitment This project was approved by the school’s Institutional Review Board ( IRB ) . We recruited 40 volunteer mods on Twitch in many ways . First , we used our lab’s Twitter accounts to post recruit - ing messages and also to reach Twitter users who were tagged as “Twitch mod” and “moderator on Twitch” . We obtained ten mods throughTwitter . Second , we directlycontactedactive modson Twitch . The authors and the research assistants used their Twitch accounts and randomly browsed the recommended channels on the Twitch homepage and other channels that had many active mods . Tak - ing advantage of the Twitch Whisper function , we directly sent recruiting messages to each mod and asked them to refer to their friends if they were not interested . We obtained 12 mods through Twitch Whisper . Third , we used the email list that we collected from Twitch Convention 2019 and obtained six mods . Fourth , we asked the Twitch streamers who had participated in our research CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee to recommend mods and obtained ﬁve mods . Lastly , two research assistants who were mods on Twitch used their networks to ob - tain seven mods . We intended to keep the sample diverse , so we searched for participants with diverse experiences such as moder - ation years and categories . Demographics were detailed in Table 1 . The average moderation tenure was around three years ( M = 2 . 74 , SD = 1 . 63 ) , ranging from half a year to eight years . The average age was around 27 ( M = 26 . 51 , SD = 8 . 20 ) . Most mods were White ( 65 % ) , followed by Asian ( 12 . 5 % ) , Africa American ( 10 % ) , Hispanic ( 5 % ) , and Paciﬁc Islander ( 2 . 5 % ) . Most were male ( 67 . 5 % ) , followed by female ( 27 . 5 % ) , transgender male ( 2 . 5 % ) , and transgender fe - male ( 2 . 5 % ) . The main streaming content was gaming . 4 . 2 Interviews and Analysis We conducted semi - structured interviews through Discord ( a VoIP communication application that is often used by Twitch streamers and mods ) . In the interview , we ﬁrst asked some general questions abouttheir moderationexperience and the contentcategories , such as “Who are you a mod for ? ” and “How long have they been a mod for ? ” . Then we asked some questions related to our research ques - tions , such as 1 ) How do you decide what is appropriate or not ? 2 ) Who comes up with the criteria ? With follow - up questions about how do you / they come up with moderation criteria ? 3 ) Do you communicate with other moderators ? How ? Why ? 4 ) How do you coordinate ( during stream ) if multiple people are modding at the same time ? In the end , we asked for their demographic informa - tion such as age , race , and gender . All interviews were audio - taped , transcribed by speech recognition software , and then reviewed by researchers . We imported all transcriptions into ATLAS . ti 1 for open cod - ing . we used thematic analysis [ 5 ] to code answers into concepts and group the relevant concepts into themes . First , two researchers went througheach transcriptand the interview protocoltooverview the questions and answers ; consequently , they highlighted ques - tions related to our research questions . Second , they conducted the open coding for the ﬁrst two transcripts individually . In a weekly meeting , they discussed and clariﬁed their codes to reach a mutual agreement about the coding process . Next , one researcher com - pleted the coding following the criteria developed through the dis - cussion in several weeks , each week with a calibration meeting to present and clarify codes . The coding process lasted for approxi - mately one month . Finally , two researchers exported the 117 codes into the Miro 2 whiteboard to collaboratively and iteratively orga - nize codes into groups , categories , and high - level themes . 5 RESULTS 5 . 1 Ways to Collaborate on Tasks The ﬁrst research question inquired how mods collaborated on moderation tasks in the team . To make sure there were enough active mods who could actually work with the team when the streamer wasonline , the team usedmany ways tocollaborate : coor - dinating and preparing oﬀ - stream , collaborating in real - time in the stream , and fostering relationships both oﬀ - stream and in - stream 1 https : / / atlasti . com / 2 https : / / miro . com / toreinforce collaboration . Thoughmany modsexpressed thatas in - dividuals , the tasks had no ﬁxed schedule and were random , they were on call as a team . 5 . 1 . 1 Coordinateand Prepare Oﬀ - stream . Oﬀ thestream , thestreamer and other mods purposely recruited new mods from diﬀerent time zones to cover the streamer’s streaming slot ; the streamer or the head mods also notiﬁed the team to prepare for streaming events . All these methods were to ensure enough active mods online in the stream . The moderationtasks faced many challenges . Some channels ap - pointedmany modsbutfailed tohave enough active mods . P36said that all mods had diﬀerent schedules , such as jobs and schools , and no one would like to moderate at night for a long time . Similarly , P1 ( M , 18 ) stated , “There have been some incidents where it’s been 3 am and I am being in chat , and I’m the only mod in chat . Literally , I have to wait until I see another mod show up before I can actually go to bed . ” If mods worked for multiple channels at the same time , they would like to prioritize their preferences . For example , P36 ( F , 20 ) complained , “We got some new mods , but most of them are moderators for other channels . So they prioritize those . So sometimes it’s really like a struggle to ﬁnd moderators that are active . ” Accord - ing to P36 , though you have many active mods , they might not engage in moderation in the expected channel . Additionally , too many mods reduced the workload but also caused “competition be - tween mods” ( P21 , F , 33 ) . Many mods said that if there were two to three active mods in the chat , they would self - deactivate to work on their own things . Purposely Recruit Mods from Diﬀerent Time Zones . Since individ - ual mods had diﬀerent schedules in their oﬄine life and failed to cover the streaming slots all the time , the moderation team would like to recruit mods from diﬀerent time zones to “make sure there’s plenty of coverage and availability” ( P31 , F , 34 ) . Usually , whenever forthe largechats when they wanna assign shifts and want 2 moderators at every time , what they’ll end up doing they’ll have a schedule , and you have to be at this time , this time , and this time . They’ll end up taking 3 mods from CA , so they’re on US west . We’ll have 3 mods from NY , so they’re on the US east and 3 mods from Europe ; they just have 3 mods from every time zone they can ﬁnd . . . but for smaller channels , it’s not really a big deal because the streamer themselves could moderate the chat on their own . It’s just whenever someone shows up and is willing to help ; then it’s beautiful . ( P15 , n / a ) P15 showed a good example for a large stream with a clear schedule to have mods in diﬀerent time zones to cover the stream - ing slots . He also pointed out that for small streams , the streamer might work as a mod to handle the chat in a comparatively ﬂexible manner . NotifyEach Otherof theAttendanceBefore theStreaming . Stream - ers as the leaders in the moderation team would notify mods and schedule attendance for particular events with the expectation to have a lot of viewers ( e . g . , Twitch front - page streaming , anniver - sary stream ) or with the prediction of an amount of moderation Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Table 1 : Participant Table ID Mod ( yrs ) Age Race Gender Content ID Mod ( yrs ) Age Race Gender Content P1 2 18 White M gaming P21 1 33 White F gaming , creative P2 - - White M - P22 1 . 5 24 White M gaming P3 1 . 5 37 Asian F board games P23 6 31 White M gaming , chatting IRL , e - sports P4 2 35 White M gaming P24 2 . 5 23 Hispanic M art , bodypainting P5 2 . 5 41 White M music , creative P25 1 . 5 18 White M gaming P6 1 29 White M - P26 3 27 African American F gaming P7 2 19 White M gaming P27 2 21 Asian M gaming , chatting IRL P8 2 40 White F gaming P28 4 18 White F gaming , video editing P9 3 - 4 40 White M gaming P29 3 29 White M gaming , life advice , politics , drama P10 1 45 Africa American Trans male gaming P30 4 21 Hispanic F gaming P11 2 - 2 . 5 23 Asian M gaming P31 3 . 5 34 White F gaming P12 5 27 Africa American F gaming P32 3 19 White F gaming P13 1 20 White M gaming P33 1 19 White M gaming P14 1 21 - M gaming P34 5 26 Asian M gaming P15 4 - - M gaming P35 3 24 Paciﬁc Islander M gaming , car racing P16 4 24 White M - P36 1 20 White F gaming P17 3 21 White M gaming P37 4 19 African American M gaming P18 2 - White Trans female board games P38 2 18 Asian M gaming P19 5 31 White M - P39 0 . 5 15 White M rhythm & music game P20 - 43 White M gaming , product reviewing P40 8 28 White F gaming , chatting IRL , politics work ( e . g . , a restart of streaming after the ban , streaming new con - tent ) . This method was usually applied to large streams with big events . Several mods mentioned that they coordinated to prepare for events . The streamer might “need all hands on deck” and asked mods “as many as possible to be there at this time . ” ( P19 , M , 31 ) . For example , P31 ( F , 34 ) shared her experience , “Recently Z got told certain dates and times that his streams will be on the front page of Twitch , and he was like , Hey , just so you know , we’re probably gonna have more people for these days . I’m letting you all know ahead of time . ” When there’s a big thing going on , like when peo - ple get put on the front page of Twitch , when the streamer we moderate for does that , we deﬁnitely talk about who can be there because it’s a lot of peo - ple . The front page of Twitch can bring in a lot of people . So , we need to be able to reference who can be there , and we need at least three or four people there . So yeah , we do sometimes when big events are happening for sure . ( P28 , F , 18 ) In these cases , streamers needed to coordinate with the stream to ensure “at least at least three or four” , “more” , even “as many as possible” mods online . Not informing mods before events made moderation work overwhelming and caused discomfort and com - plaint from mods ( P18 , Trans , n / a ) . If the streamer predicted that incoming streaming needed more content moderation than regular streaming , they might also in - form the team to ensure there were enough active mods online . For example , P34 stated that the streamer planned to stream new content with no idea about their performance and viewers’ reac - tion , so they scheduled a time to ensure mods were online , similar logic in another case reported by P24 ( M , 23 ) : “That streamer that got banned and she says this is going to be my ﬁrst night back and I’m going to have a huge target on my back and people are going to be scrutinizing everything I do . I need you guys here . So we’ll actu - ally sit down and say we need to decide who’s going to show up , who can make it , and what are we going to do . ” According to P24 , the streamer restarted streaming and predicted to have a lot of harass - ment in the chat , so that they “sit down” to discuss which mods should “show up” . Mods also informed streamers if they couldn’t attend events or streaming . P21 ( F , 33 ) shared her experience to inform the team of her availability so the team could prepare in advance : “I’m saying if I went to my parents or something , I would let the streamer know that I’m not going to be there for the week and I would also say in the mod chat , so they’d be aware and so somebody else might for that stuﬀ . ” P23 ( M , 31 ) further explained , “If you’re part of a moderation team , you gotta let people know so that they know what to expect from you . If you set appropriate expectations , people are going to be ﬁne 99 % of the time . ” Update Information in the Team . Mods also updated each other oﬀ the stream about the streaming or chat in general . P24 ( M , 23 ) would like to have mod meetings to update everything : “I try and at least do it every two weeks because people are busy . Right ? It’s diﬃcult to make time sometimes cause they’re busy . So I tell them it’s even if you can’t make the meeting , you know , to at least take some notes , and you know , look at what we can do going forward . ” P24 suggested mods take notes to keep updated . P30 ( F , 21 ) coor - dinated with other mods to keep each other updated : “Just to keep in check with what’s going on ; like if I miss a day , keep them up - date me on what’s happened ; or if he misses a day , I’ll update him on what’s happening . It’s just always to be clear of what’s happen - ing in chat and in the stream . ” These updates usually happened in Discord with text messages so that everyone in the team could see them . “If somebody misses something , they can always go there and catch up on it . ” ( P23 , M , 31 ) . 5 . 1 . 2 Collaborate in Real – time In the Stream . In the stream , mods and the streamer involved a lot of communication via mainly third - party platforms like Discord and Skype . They also used the Whis - per chat on Twitch for small group discussion and immediate com - munication . P28 ( F , 18 ) said that they were all in the moderation discord for mods only and “talk in there when things are happening CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee or when we need to get to know each other . ” Most moderation teams preferred Discord as the main communication channel because it could easily reach out to the whole team , was the main space to socialize oﬀ - stream , and archived all important information . Monitor andInformtheTeamRegardingPotential Incidents . Mods shared information in the team so each team member was well - informed and notiﬁed during the moderation process , such as a shared list of repeated violators , a full list of information that the team could refer to in speciﬁc situations , and a notiﬁcation of spe - ciﬁc activities and violators in the Discord . Some mods notiﬁed the team to monitor speciﬁc activities or viewers . For example , We’re all on the same page , so somebody will get to it ﬁrst before someone else , it’s ﬁne . If somebody bans somebody or if somebody times out somebody out that’s ﬁne , we trust each other . If there’s anything strange going on we will talk in the moderator chat in the Discord and let people know , “hey this person is saying kind of funny things , let’s watch them , ” or sometimes we’ll DM each other , but usually the mod chat is better because all the mods and the streamer can see it . ( P10 , Trans , 45 ) P10 stated that mods in the team trusted each other’s moder - ation actions ; they notiﬁed the team on Discord about the suspi - cious activities in the streaming chat and also preferred the Dis - cord chat to the Twitch chat because all team members could see it . Some mods also recognized viewers from other streams and no - tiﬁed mods in the current stream : “Since I mod for a lot of channels , I might recognize a viewer from a diﬀerent stream . So I’ll go over to the mod channel and say , ‘hey , that viewer , he caused a problem in another stream , so you guys know about that’ ” ( P5 , M , 41 ) . When they saw these viewers who might potentially target the commu - nity , they “just head up , look out for these people” ( P27 , M , 21 ) . The moderation team also collaboratively tracked violation ac - tivities with violators’ information . Mods kept working on lists of violators and records of violation and used these pieces of infor - mation to supplement their monitoring . P23 ( M , 31 ) said , “In that community , in particular , we have a shared Google document . It has a list of , um , I’m not ﬁnding the word , but people that have caused problems in the past for timeout or bans or whatever . If I lack context in a situation in that community , then I can go to that spreadsheet . ” Similarly , Well , ﬁrst oﬀ , you know , we monitor as a group . An - other thing , I have ourDiscord open while the stream is there , and if it’s something more not sure about , you know , we’ve got a record where we report any action that we’ve done . Like if someone comes in and starts causing some things , some issues , or they start saying some weird things , I can be like ‘Okay , that name sounds familiar’ . I can go search in our Dis - cord and be like , ‘Oh they’ve caused problems before’ . Then , I’ll know to go after it . ( P31 , F , 34 ) Coordinate with Active Online Mods for Task Support and Transi - tion . During the streaming events , Some mods were active in the live streaming chatroom ; some were active in both the live stream - ing chatroom and the Discord server ; some might only be online in the Discord server without active engagement . These only on - line in Discord were on call anytime . Mods working on moderation tasks would ask other active mods for support if necessary and co - ordinate tasks with other mods before leaving . Mods sometimes had to seek support due to the increased work - load or the shortage of active mods in the chat . P7 ( M , 19 ) said , “It’s more like the streamer goes live , and there will probably be some mods that go and watch him as well . We don’t have schedules , like you mod now , and now you have to mod . It’s more like okay , he’s live , and if it’s really necessary , people can tag all of the moderators in the Discord and tell them if you can come help out . ” According to P7 , the mods usually had no schedule for the stream , but they were all online in Discord . They were on call to be active mods if others asked for support . For example , P36 ( F , 20 ) said sometimes there was only one active mod in the chat , and the others were inactive , so he typed in the discord channel to ask other mods to take over while he did something else . In some cases , the stream experienced a sudden viewer increase , called a “raid” , increasing the mods workload , so that they would like to ask other mods for help : “If a particular stream is having trouble or has extra viewers that they’re not used to , it has a big raid or something , they might post in the mod channel ‘hey , can anyone come help out ? ’ ” ( P5 , M , 41 ) . Mods usually took responsibility and did whatever they could to help the community . However , they also needed breaks or do something for oﬄine lives . In these cases , they would coordinate tasks with other mods to ensure the tasks were transferred . P4 ( M , 35 ) stated that he usually took points and looked at the chat all the time when he was active ; other mods would glance at the chat every so often ; if he had to leave , he would ask other mods to re - place his role . Modswould back each other up , like P27 ( M , 21 ) said , “Sometimes it’s just like , hey , is anyone here ? I need to go make food , and then we’ll reply like , yeah , I’m here . ” As a head mod , the responsibility was larger ; if they had to leave , they needed to ensure that the team worked well . P7 ( M , 19 ) was the head mod for the channel and the discord admin as well : “I made the discord group , and it’s more like once I made that , I always keep in an eye on it and I make sure everything is in order and that nothing is set up incorrectly , but at this point like I could just go oﬀ for months and it’s not like the whole place would burn down . I also appointed another mod as discord admin who also has like extra permissions , so in case I can’t jump in , he can just do what I would do . ” According to P7 , the head mods assigned other mods as admins to gain “extra permission” before leaving . Discuss withOtherModsandtheStreamerto CollaborativelyMake Live Group Decisions . To ensure moderation democracy and not make decisions by one single mod , mods would like to involve other mods to discuss what they should do and achieve a consen - sus . P15 ( M , n / a ) said , A lot of things I usually end up asking are if we’re going to make a severe change , like if the entire chat is spamming something like a really long copypaste . Usually , whenever I’m in a voice chat , I’ll ask them , “how long do you want to let this run ? ” And instead of me being a dictator or letting someone be a dic - tator . . . I like to try and let other people be involved Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA with the decision - making process where we all dis - cuss how long we’re going to let this run for and we come to a clear concise consensus to where this ends here . According to P15 , mods avoided being “dictators” and discussed with other mods to “come to a clear concise consensus” . Similarly , P11 ( M , 23 ) would like to ask other mods to “mutually agree on what to do with the person . ” Usually , individual mods felt it diﬃ - cult to make the ﬁnal decision and needed a “second opinion” ( P32 , F , 19 ) . P29 ( M , 29 ) elaborated , “We don’t usually have any direct instruction from the streamer . . . We kind of work stuﬀ through on ourselves . We have a Discord . We all kind of collaborate , and if we feel diﬀerently about things that we can talk stuﬀ through . ” In rare cases with severe issues , the streamer was also involved . For exam - ple , P10 ( Trans Male , 45 ) stated , “If there are questionable situations , we’ll have discussions amongst the mods or with the streamers about what to do in certain situations . In niche cases where we don’t know about this , we have a discussion about it on Discord or in a private message about what guidelines we want to have . ” In this case , there were no clear standards for all situations , and mods had to discuss to decide the “guidelines” . 5 . 1 . 3 Foster Relationship both Oﬀ - stream and In - stream to Rein - force Task Collaboration . In the stream , mods got to know each other and built relationships within the team . We clariﬁed that mods also built no - task - oriented relationships ; here , we focused on how they fostered relationships to reinforce task collaboration . Oﬀ - stream interaction such as sharing interests and playing video games can also help build friendships . Many mods would like to consider these with frequent interactions on Discord or meeting oﬄine as friends who shared similar values and interests ; at the same time , they might consider these with little or no communica - tion as colleagues . P24 ( M , 23 ) said , “I’ve made friends with a few of them . I wouldn’t say that I’m friends with every moderator in every stream , but at least the ones I did talk to we’re on good terms . ” Some - times , they tried to be friends but were not sure about other mods’ thoughts . P25 ( M , 18 ) stated , “I feel like we’ve become friends over time , but again , everybody’s hidden behind the username , and you can’t really become friends . But I feel like for a long period of time you can get to know somebody , but you don’t know their true self un - til you meet them in person . ” In this case , though mods considered these with “a long period of time” communication friends but still doubted their “true self” . Mods also tried to build friendships with others and believed that communication was important because the relationships can facilitate task collaboration and avoid team conﬂicts . Good relationships can make you “get along a lot easier” with other mods ( P28 , F , 18 ) . Mod : Now , I try to get along with all the mods be - cause I think . . . I haven’t experienced this myself , but I’ve seen it ; mod in - ﬁghting is a really bad thing for the stream , the streamer , the mods , you know , that’s just a bad thing all around for everybody . Interviewer : What is mod in - ﬁghting ? Mod : When mods don’t agree with each other , and they let their egos get in the way of doing their job , and it just causes problems with othermods , and they just argue . ” ( P19 , M , 21 ) According to P19 , his experience to see mod in - ﬁghting made him try to build good relationships with the team . 5 . 2 Ways to Contribute to Moderation Standards The second research question inquired how mods contributed to the moderation standards in the team . Each community on Twitch has diﬀerent standards . Mods mainly contributed to moderation standards in twoways : ( 1 ) collaborativelyworking withthe streamer on the explicit community rulebook , including chat rules , chan - nel rules , and completed list of rules and records only visible to the moderation team , and ( 2 ) individually conﬁguring the implicit norms and criteria if there was no rulebook . We clariﬁed that rule - book and norms were not exclusive ; sometimes , though mods had the rulebook , they still used implicit norms to deal with speciﬁc situations . 5 . 2 . 1 Collaboratively Work with the Streamer on the Community Rulebook . Mods contributed to the community rulebook in two ways : assisting the streamer topolish / updatethe rulebookthrough discussion with the streamer if the streamer initialized it , or work - ing with the streamer or the head mod to establish the rulebook if the community did not have one . Assist theStreamerto Polish the Rulebook . 14 modsexplicitly said thatthe streamer set upthe guidelines for their channels , and many of them stated that the mods worked together with the streamer to polish the rulebook , though streamers made the ﬁnal decisions . This process involved both the streamer’s individual work and the mods - streamer collaboration . P13 ( M , 20 ) reported that the initial setof ruleswasusuallybasic ; over time , variousthings had cropped up ; mods had to adjust the rules as needed . “The initial one was him , and then as a group . I think our streamer gets the ﬁnal say on what he wants his rules to be , and we just enforce them . But he does take feedback very well , ” P31 ( F , 34 ) added . Mods’ contributions might be diﬀerent based on streamers’ ex - periences and preferences . Less experienced streamers would rely more on mods’ feedback ; more experienced streamers might domi - nate the development of the rulebookbut also consider mods’ opin - ions . After they’ve brought up their rules , they’re like , this is what I would like to see . But they don’t have the experience with chat . And if they’re a new stream or whatever , then they’ll come up with their rules , and then they’ll ask their moderators . What do you think aboutthese ? In yourexperience ? Dothese rules work ? Is there anything you’d like to change ? So ul - timately , the rules are decided by the streamer and then the moderators , then either one to uphold those rules . ( P23 , M , 31 ) In this case , if the streamer had no moderation experience with chat messages or started a new stream without a clear idea about moderation in those categories , mods supported the streamer to uphold and modify the rules as needed . Other streamers might not take much input from mods : “The rules , the streamer often does , but we , the moderators do have a lot of input , well I wouldn’t say a lot , CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee but we do have a bit of an opinion as to what should count as a rule or not” ( P38 , M , 18 ) . Mods’ experience and their relationship with the streamer made their contribution to the rulebook diﬀerently . The streamer sits down with the most trusted admins of the moderators . They’re more referred to as the admins because , you know , they’re trusted people , so there’s a list of three people for our speciﬁc streamer who he goes to every time to ﬁgure out what to add , what to change , what to do . Then , it ﬁlters out to the rest of the moderators what’s happening , and kind of , we ask their opinion second , so it ends up going around like everyone gets to put their opinion in , but the main criteria are decided by the streamer himself and three individuals that he trusts a lot . ( P28 , F , 18 ) According to P28 , experienced mods can be considered trusted mods or admins ; these mods contributed to the rulebook more sig - niﬁcantly than other mods . The streamer mostly discussed with them and informed other mods . Work with the Streamer or Head Mod with the Support of the Streamer to Establish the Rulebook . Some channels did not have a rulebook . Mods would like to try to proactively inﬂuence the com - munity using their experts . P27 ( M , 21 ) shared his experience about how to help a new streamer establish rules : “If I’m helping a new streamer set up , I will ask them like , Hey , what do you want this com - munity to like what ? Like , give me an overall view of how you want the community , and then from there , we derive some rules from it . ” In this case , the mods discussed with the streamer to have an “overall view” and then derived some rules together . Mods also suggested the streamer with no rulebook borrow rules from other channels that usually had a complete list of rules . Some micro - communities had a clear hierarchy – the streamer , the head mod , other mods . Head mods usually engaged in the com - munities for a long time to know the streamers’ preferences ; they led the rulebook development and took care of other mods . When I got modded in the beginning , there weren’t really guidelines or anything . It was just like other mods told you , oh yeah , you should do this stuﬀ , but that was more like tips . There weren’t any like set guidelines . Now that I have sort of taken on the role of the head mod , and I have made comprehensive guides and a channel full of info . You know , stuﬀ that might be not very clear to moderators so they can fall back on that if they have any questions . ( P7 , M , 19 ) It always comes from the streamer , but some stream - ers have a head mod . For example , like a mod that takes care of all the mods , so if the mods have ques - tions , they can just hit up that one person instead of always asking the question to the streamer directly . Um , but it’s usually what the streamer wants . And yeah , I know some big streamers , like , for example , I have a friend who’s a head mod for this streamer . . . I know he’s the one who decides of the rules and ev - erything , like he knows the streamer and he knows what he likes . So I know he’s the one who takes care of all of that . ( P36 , F , 20 ) In these cases , Head mods helped the streamer to develop rules from the ground . Thus , the head mods developed most of the rule - bookwith either the streamer’s or other mods’ inputs . The beneﬁts of establishing rulebooks were very obvious . Mods avoided using subjectivity to make decisions and take actions ( P2 , M , n / a ) . 5 . 2 . 2 Work Individually with Limited Collaboration to Shape Com - munity Norms . If streamers didn’t have a rulebook or didn’t want to , mods ﬁgured out the moderation standards in various ways . We pointed out that most of these methods involved more individual work unless it is necessary to involve other mods or the streamer . Streamer’s Expectation and Mods’ Experience and Judgment as Implicit Guidelines . Mods used streamers’ expectations and pref - erences as implicit guidelines . Obviously , the preferences can be shown in the explicit rulebook . In case of no rulebook , mods ﬁg - uredoutstreamers’ expectationsvia directly talking with thestream - ers : “I usually try to get the streamer to sit down with me and to say what is the expected outcome of your community” ( P24 , M , 23 ) . Mods also stayed in the community for a long time , such as P19 ( M , 31 ) : “I already knew what he expected , just from conversations we’ve had , you know , not about modding . I watched his stream for many , many years and I knew basically just what is expected . ” According to P19 , One way to ﬁgure out the streamer’s expectations was to stay in the community . It can also help mods learn the community norms . That’s through understanding , like being part of that community . Because you’ve been a part of the com - munity , you’ve been interacting in a way ; you will al - ready understand what is acceptable and what isn’t acceptable and the usual guidelines for how things are dealt with . The streamer in question usually has a particularway that they deal with things , and that’s been evoked many times , so that’s how you deal with it in the future . ( P9 , M , 40 ) According to P9 , mods understood streamers’ expectations by watching “evoked” violations and punishments ( norms about how other mods collaborated ) , also by interacting with other users to understand “what is acceptable and what isn’t acceptable” ( norms about how users behaved ) . In case streamers with no clear expectation or deﬁnition about rulebook , streamers either trustmods’judgment orempower mods to moderate with their experiences , as P20 ( M , 43 ) said , “A lot of times , they don’t even tell you . they trust you to use your judgment , just beingbasichumanbeingsandknowing what’s rightwhat’s wrong . ” The personal experience was about not only engaging in online communities but also reﬂecting on real - life scenarios . P01 ( M , 18 ) described his judgment of the appropriateness of the content by imagining that these viewers were talking face to face : “One thing that I kind of really focus on also is ‘would you say this to a person directly face to face with that person ? ’ You know what I mean . If you met this person uh . . . in real life , would you say that to their face ? If you wouldn’t actually say that to that person , then you will proba - bly get banned for it . ” Mos also relied on working experience and educational background as references for their decisions . P04 ( M , 35 ) stated , “I have a bachelor’s in law , and that certainly helps me Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA deal with people , and I also helped a little kid for a year , and also it’s helpful . ” As the implicit guidelines are vague and can cause mods to over - moderate in some situations , some mods said that streamers would correct them if their decisions were not in line with streamers’ ex - pectations . For example , P22 ( M , 24 ) said he had no rulebook to follow and often used his common judgment : “If it’s wrong in your eyes and the streamer doesn’t think it’s wrong , he’s going to let you know . ” Similarly , P12 ( F , 27 ) said she never communicated with the streamer to discuss their expectation and usually used her judg - ment : “Generally , I can apply my own rules to it , and if they don’t like it , he will actually say ‘this is how I want timeout or ban to happen . ’ ” Accordingly , streamers always had the ﬁnal say about moderation decisions and could rectify the moderation action if necessary . MaintainStandardswithOtherSimilarCommunities . Some mods referred to similar communities with either clear rules or observed community norms . It can happen either within or outside the plat - form . P1 ( M , 18 ) said , “If you can see this in other streamer’s chat and get banned for , then you can get bannedin this chat . ” In this case , the mods also watched other live streaming channels ; they might be viewers for other channels and saw violations and punishments in these communities and applied the norms to their moderated communities . Some mods speciﬁcally mentioned that they relied on rules on external platforms , which had more strict and complete rules . For example , P8 ( F , 40 ) gave an example of one community that he moderated : “Heroes Hype has very strict community guidelines and whatever because they are associated directly with Blizzard . So we kind of have to maintain the same community standards that Bliz - zard has on their own channel . ” Similarly , P11 ( M , 23 ) stated , “The Coalition , which is the studio who makes Gears of War , and Microsoft , came up with stream rules . So , if something goes against those rules , then we have to purge them or ban the person who said it . ” In these cases , the games might have oﬃcial streaming accounts on Twitch , the gaming companies might apply developed rules for gaming to live streaming , or the mods had to maintain the same standards . Stick to the High - level Terms of Service . Twitch keeps updating the community guidelines , which are complete than the channel rules . Some mods preferred to stick to the complete community - wide guidelines instead of the channel - speciﬁc rules . For example , For the channel like the speciﬁc guidelines for the channel , we don’t really do it much because we can mistake the speciﬁc guideline we have . We can’t just gowith that ; you know whatImean . If anything needs to be added in the future , we will add and notify the modsaboutwhy this is being addedorwhy we are do - ing whatever it is . But Twitch’s guideline as a whole is a really interesting subject right now because they are changing their entire terms of service . ( P1 , M , 18 ) P1 preferred the termsof service of Twitchas guidelines because it was comprehensive . Additionally , P2 ( M , n / a ) stated , “Twitch has changed their terms of service because there had been a lot of com - plaints that rules weren’t being clear enough and that Twitch admins were doing pretty much whatever they wanted . ” P1 pointed out the limitation of channel rules since they were developed by the mod - eration team and needed to be updated all the time but could still fall into mistakes . P2 pointed out the superiority of the updated terms of service . The high - level terms of service were complete , transparent , and developed by the platform ; consequently , it was safe and easy to follow . In this case , there was not much commu - nication and collaboration in the moderation team . 6 DISCUSSION This work supplements prior work about volunteer moderation and explores how mods and the streamer work as a team to work on moderation standards and tasks . We identiﬁed three high - level themes about how the moderation team collaborates on tasks : oﬀ - streaming coordination and preparation ( recruiting , notifying , and updating ) , in - stream real - time collaboration ( monitoring and in - forming the team regarding potential incidents , coordinating for task support and transition , and discussing with the team to make live group decisions ) , and relationship building both oﬀ - stream and in - stream to reinforce task collaboration . We also found that mods contribute to moderation standards in mainly two ways : col - laborativelyworking onthe community rulebookwith thestreamer and other mods ( assisting the streamer in polishing or working with the streamer / head mod to develop the rulebook ) , and individ - ually working the community norms if there is not clear commu - nity rulebook ( streamers’ expectation and mods’ judgment , stan - dards from similar communities , and high - level terms of service ) . We clarify that this work mainly contributes to content modera - tion and live streaming context , and we use remote collaboration to explain these phenomena . In this section , we ﬁrst discuss the nuanced diﬀerences that contribute to volunteer moderation ; then , we discuss how the streamer and mods work as an eﬀective team to some extent , using Olson and Olson’s four components [ 54 ] . 6 . 1 Moderation Standards , Coordination Mechanism , Mods’ Roles and Responsibilities 6 . 1 . 1 Multi - layered Standards Both Visible and Invisible to the Pub - lic . Rules and norms are important to volunteer moderation . We found that the moderation team uses a mix of collaborative and in - dividual processes to work on moderation standards development . A lot of work has disused the transparency and clarity of rules and norms ( e . g . , [ 6 , 23 , 34 , 36 ] ) . Though users complain about the opaque nature of content moderation and many scholars argue for transparency , there is still something invisible to the public . The rulebook includes the visible chat and channel rules to the public and a list of details and shared documents to handle speciﬁc sit - uations that are only visible to the moderation team . While the visible rulebook helps regulate viewers’ behaviors and boost the community atmosphere , the rulebook for the team’s internal refer - ence might play a prominent role for mods to do tasks . Modsshape community norms by learning the moderationteam norms and engaging with other viewers to learn the chat norms . These are not explicitly written but understood by the mods and shared in the team . Mods also refer to other similar communities that have clear rules . In this sense , we provide a nuanced under - standing of themoderationstandards and how modswork forthem . CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee Future work can explore the mechanism behind multi - layered stan - dards like how they inﬂuence the platform governance , particu - larly how the social governance and algorithmic governance mech - anisms inﬂuence each other [ 52 ] . 6 . 1 . 2 The informality of Voluntary Work Dilutes Mods’ Roles and Responsibilities . Prior work suggests that an eﬀective virtual team must have both implementer and completer - ﬁnisher but might not need the project coordinators [ 19 ] . In the volunteer moderation context , the streamer performs the leading role of coordinator dur - ing the oﬀ - stream coordination and rulebook development , thus can be the completer to aggregate or update the rulebook develop - ment . In the stream , modsallbecomeimplementers ; there is no spe - ciﬁc completer - ﬁnisher ; the streamer mainly focuses on streaming instead of moderating . Accordingly , streamers as coordinatorsplay signiﬁcant roles in ensuring enough active mods online , which is prominent in live streaming communities . Priorwork also suggests that team leaders threaten team survival if they are too controlling [ 43 ] . Similarly , we found that streamers empower mods to do the moderation ( e . g . , trust mod’s judgment ) as a way to build the team . Mods’ roles and responsibilities can also weigh diﬀerently . Oﬀ the stream , the streamer - mod collaboration is the prominent col - laboration ; in the stream , the moderation process involves more mods collaboration . Prior work has classiﬁed many diﬀerent roles modsplay tohelp the streamer orthe community [ 49 , 59 , 70 ] . In the moderation team , we found that there is no speciﬁc role regarding the task collaboration except that the head mods assign and notify things . Many mods state that being active online is important and that the task is more like “ﬁrst come ﬁrst served” withoutﬁxed roles and responsibilities . Their roles seems contingent and ﬂuid [ 59 ] . The informality of the volunteer moderation might explicate this phenomenon , diﬀerentiating it from other distributed team - work . Informal work includes all activities leading to the produc - tion of services for others outside a legal framework and can be community - oriented and based on mutual help and moral obliga - tion [ 56 ] . Mods as individuals consider moderation an informal work without the imposed regularity . They are not paid laborers and have no forced scheduleto workwith the streamer . Some mods with strong moral obligations notify each other and coordinate the availability ; others describe that no show - up is also acceptable . Ad - ditionally , some mods might do more based on their availability , but there is no fairness about the workload ( who should do what at what time ) . 6 . 1 . 3 Synchronicity Facilitates Explicit and Implicit Coordination . Most of the task coordination identiﬁed can be considered explicit coordination . The moderation team uses explicit coordination oﬀ and in the stream to ensure active online mods ( e . g . , recruiting new mods ) and seek task support ( e . g . , asking for support ) . Mods also involve much implicit coordination when multiple mods are active and dealing with violators in real - time . The implicit coordination is highly interdependent , time - sensitive , stressful because of the synchronous interaction in the chat . The implicit coordination derives from the shared mental model [ 57 ] . Shared mental modelscan be developed by staying in the com - munity for a long time and also by using guidelines and policies [ 40 ] . In this sense , mod’s contribution to moderation standards is a good case of shared mental model development , actively engaging in the rulebook development and also community norms develop - ment via staying in the community for a long time . Thus , they mu - tually understand the community standards and what they should do . For example , if they see too many mods in the chat , some will self - deactivate and work on personal things , or only keep a glance of the chat without intervention unless some trouble happens or workload increases . Mods anticipate what other mods are likely to do and adapt their behaviors to facilitate the moderation tasks without explicitly discussing who should do what . Though sometimes there is a task conﬂict , they trust each other and don’t see it as a big problem . The aﬀordances of live stream - ing moderation bring all the active mods online at the same time with the same content [ 29 ] . Though multiple mods notice a viola - tion and moderate it simultaneously ( e . g . , timeout twice ) , they can easily see it almost at the same time and consequently revoke the moderation action . Some mods can also have brief real - time con - versations toresolve the conﬂict . Tosome extent , thesynchronicity of live streaming facilitates implicit coordination and makes mod - eration function smoothly without clear roles and responsibilities . 6 . 2 The Streamer and Volunteer Mods Form an Eﬀective Team Olson and Olson’s four - component framework [ 54 ] is usually ap - plied in the professional and formal context . Thinking about how ourresults may align withthe framework , we foundthatthis frame - work can potentially be applied to this voluntary and informal con - text to show how volunteer mods and the streamer can work eﬀec - tively as a team . The collaboration and technology are ready ; the common ground is clearly built ; the dependency of work is varied but mostly loose ; thus , the remote collaboration is generally eﬀec - tive and to some extent productive , though mods experience some challenges . 6 . 2 . 1 Common Ground . Prior work suggests that sharing knowl - edge and understanding the awareness of the state of other team members is an essential part of common ground , and working for a long time on a certain project can build common ground [ 2 , 54 ] . Olson and Olson pointed out that people who have established a lot of common ground can communicate well even over impov - erished media [ 54 ] . In that sense , mods’ contribution to modera - tion standards involves the common ground building : collabora - tively working with a streamer / head mod to develop the rulebook and sharing it with the team , individually working on community norms such as staying in the micro - community for a long time to know streamer’s expectation or to learn by observing . Task col - laboration process also contributes to common ground buildings , for example , oﬀ - stream coordination such as updating information and notifying each other , and in - stream communication . Building good relationships with other mods is another way to build com - mon ground and , in turn , can facilitate communication and avoid intragroup conﬂict . 6 . 2 . 2 Dependency / Coupling of Work . Coupling of work refers to “the extent and kind of communication required by the work” [ 54 ] . Work with strong dependency is usually non - routine and ambigu - ous , requiring the collective work of team members engaging in frequent and complex communication with short feedback loops , Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA and vice versa [ 54 ] . In the volunteer moderation team , we found that mods’ tasks involve both tight and loose dependency of work . In communities with clearmoderationstandards , thecommonground about the violations and punishment is clear , so the moderation task is procedural and straightforward . In this sense , the depen - dency of work is comparatively loose . In communities with no clear moderation standards ( e . g . , mods’ own judgment ) , or scenar - ios that can’t be covered by these standards , the ambiguity of the tasks requires communication to clarify . In this sense , the depen - dency of work is comparatively tight . Most of the time , modera - tion tasks are routine tasks that any active mods can do it . In the organizational context , the tightly coupled work is challenging to do remotely , and technology does not support the rapid back and forth in conversation or awareness and repair of ambiguity [ 54 ] . However , we did not see a clear challenge or diﬃculty regarding the technology usage , in line with prior work [ 2 ] . We explain this in technology readiness . 6 . 2 . 3 Collaboration Readiness . Since most volunteer mods are se - lected from active viewers and viewers having a close relation - ship with the streamer [ 70 ] , either having similar value with the streamer or having a positive inﬂuence on the community value . Their background and experience are homogeneous and consis - tent ; there is less barrier to collaborate remotely from the begin - ning [ 2 ] . The nature of moderation work is online and remote and has a culture of sharing and collaboration . Mods assume to work with others remotely . Thus , mods are appropriately ready to col - laborate remotely . 6 . 2 . 4 TechnologyReadiness . The currenttechnologies provide multi - modal interaction in the team . Mods use diﬀerent technologies , including the Whisper function within the Twitch platform and third - party tools like Discord and Skype . While mods can work as a small group ( 2 - 3 ) via Whisper function tohave a group chat , most communication is parallel and happens on Discord , oﬀering both voice chat and text chat . The text chat is mostly served as asynchro - nous communication supporting time - delayed interaction and pro - vides anytime / anywhere ﬂexibility , while the voice chat is mainly served as synchronous communication with real - time interaction , allowing immediate discussion and clariﬁcation [ 62 ] . Additionally , the streamer can directly answer mods’ questions or remedy mods’ actions in the chat room as mods are doing the moderation tasks . Both live streaming and third - party platforms cover video , audio , and textual communication . Mods did not complain much about the technological infrastructure for remote collaboration . Twitch provides various moderation tools for the team to do tasks [ 7 ] , but our results reveal that most of the communication happens on Discord . It seems like the team neither uses moder - ation tools to communicate nor uses communication tools to do tasks . There should have an artifact that supports both tasks and communication . However , we don’t know whether the team pur - posely separates them on diﬀerent platforms or they are forced to do so because of limited choices , requiring further investigation . 6 . 3 Design Implications Though mods do not complain about the technology usage , they do apply various available tools that are not deliberately designed for moderation teamwork . They also face challenges to ensure a number of active mods online when the streamer goes live and to consistently enforce moderation standards that are vague and varied , to some extent . We provide several suggestions to simplify the tools they have used to switch back and forth and reduce the cost of collaboration . These features are necessary to moderation teamwork and should be integrated into the ecosystem of the live streaming platform by either Twitch or third - party developers . 6 . 3 . 1 Coordination Support Design . Tools to Coordinate Team Availability . The current Twitch plat - form only allows streamers to post their schedules to the public but lacks a function to coordinate them with mods’ schedules . We found that schedule coordination involves much back - and - forth communication on the third - party platform . Thus , we suggest a collaborativecalendar - like design toautomaticallycoordinate sched - ules for mods and streamers . For example , mods can also add their availability to the streamer’s schedule and indicate their availabil - ity level ( High , medium , low ) . The design can be embedded into either the live streaming platform or the moderation tools . Tools to Signal Attention and Roles . As we discussed , there is no speciﬁc role in moderating the chat , and only ensuring active mods can sometimes engender ﬂuid roles and task conﬂicts . Mods nei - ther know other mods’ preferences nor have a clear idea of other mods’ roles and might have to ﬁgure out who is doing what . We recommend an attention indicator design with social signals [ 32 ] to show mods’ anticipated roles in the chatroom , such as bot man - ager , socializer , punisher , rule poster . For example , Twitch has a rich badge system to indicate viewers’ contribution and participa - tion ; it may add another layer of badges to show mod’s roles in the moderation team only visible from a mod’s view . These role indicators can also give the streamer an overview in advance and coordinate mods with speciﬁc roles they need . 6 . 3 . 2 Rulebook Development Design . Tools toSupportVisibleRulebookDevelopment . Twitchhas acom - munity guideline targeting general users with easy - to - read terms . This guideline also suﬀers from vague deﬁnitions for some terms and does not cover speciﬁcneeds for some micro - communities . We suggest a customizedmechanism thatcan aggregate chat and chan - nel rules based on streamers’ ( e . g . , female or marginalized stream - ers with speciﬁc needs [ 67 ] ) and communities’ characteristics ( e . g . , same game category , similar topics ) to provide templates for new streamers or streamers starting new topics / categories . These com - munities might suﬀer from similar harmful content and inherit similar values and visions . The aggregate rules can clearly show what is allowed or not in the communities , and streamers can also tailor the generated template . Additionally , a mechanism to evalu - ate the rule eﬃcacy by comparing the existing rules with blocked content to help mods diﬀerentiate “good” or “bad” rules , and as a result , to update the rulebook [ 68 ] , can also be considered . Tools to Support Invisible Rulebook Development . A clear rule - book helps mods reduce the subjectivity of the decision - making process . The invisible rulebook is critical for mods to make the ap - propriate decision . Mods currently use other artifacts to develop the invisible rulebook and share them on third - party platforms . To CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee better support the teamwork , we suggest an artifact that can facili - tate the invisible rulebookdevelopment , for example , a mechanism to summarize violations and automatically organize the list of re - peated violators . The streamer or head mods can give its access to the moderation team and allow all mods to update and manage . 6 . 3 . 3 Relationship Building Design . Communication and coordi - nation might facilitate relationship building and in turn , build com - mon ground . While mods have diﬀerent expectations regarding their relationship with the streamer and viewers [ 70 ] , we found that modstry to foster a relationship with other mods but only con - sider frequently communicated mods friends . As volunteer mods , the perceived relationship might directly inﬂuence their continu - ous engagement in the community . For example , conﬂicts about what should be labeled as “bad” behaviors in the moderation team can cause mod resignations and status removal [ 50 ] . Thus , design to support relationship building and avoid team conﬂict is also crit - ical . We recommend mechanisms to facilitate informal communica - tion and collaboration , similar to the community - based groupware supporting lightweight communication to allow easy initiation of interaction and an overview representation of the community that shows who is available and what individual task they are working on [ 28 ] . For example , in Discord mod - only channels , by adding la - bels about social topics , mods can choose labels to show their inter - est and intention , to facilitate mods to form sub - groups and com - municate more . The initialized communication at an early stage can also increase their commitment through its inﬂuence on the group atmosphere [ 15 ] . 6 . 4 Limitations This study also has several limitations . First , we only interviewed mods to explore teamwork , but we found streamers as team lead - ers also played signiﬁcant roles . Future work can explore stream - ers’ perspectives about teamwork to supplement our research . Sec - ond , we only focused on a single live streaming platform — Twitch , which has some unique aﬀordances . We do not know how well these ﬁndings can be generalized to other live streaming platforms or even asynchronous communities applying volunteer modera - tion . Third , our results indicate that community size might be an important factor for teamwork , but we cannot quantify how com - munity size inﬂuences the eﬀectiveness ofteamwork . Itseems larger communities with lots of mods need more well - organized team - work . Futurework can explore the relationship between the thresh - old of community size and team evolution dynamics . Lastly , we do not consider mods’ individual characteristics in the analysis . Ac - cording to our ﬁndings , tenure might lead mods to senior / trusted mods and rely more on implicit coordination in the collaborative process . Future work can integrate mods’ individual characteris - tics and explore their impacts on moderation teamwork . 7 CONCLUSION In this paper , mods contributed to moderation standards in two ways and collaborated on tasks in three ways . We outlined how volunteer mods work as an eﬀective team : their contribution to the moderation standards facilitates common ground building ; the multi - platform , multi - modal communication , and the nature of ac - tive community users engender collaboration readiness and tech - nology readiness ; and most routine tasks were loosely dependent . We also identiﬁed the multi - layered standards , the coordination mechanism , and mods’roles and responsibilities and discussedhow synchronicity of communicationand informality of volunteermod - eration contributed to these phenomena . Designs to facilitate col - laboration support , rulebook development , and relationship man - agement are also suggested . ACKNOWLEDGMENTS This research was funded by National Science Foundation ( Award No . 1928627 ) . Thanks to the research assistants in SocialXLab at NJIT for data collection . REFERENCES [ 1 ] Ofer Arazy , Oded Nov , Raymond Patterson , and Lisa Yeo . 2011 . Infor - mation quality in wikipedia : The eﬀects of group composition and task conﬂict . Journal of Management Information Systems 27 , 4 ( 2011 ) , 71 – 98 . https : / / doi . org / 10 . 2753 / MIS0742 - 1222270403 [ 2 ] Pernille Bjørn , Morten Esbensen , Rasmus Eskild Jensen , and Stina Matthiesen . 2014 . Does distance still matter ? Revisiting the CSCW fundamentals on dis - tributed collaboration . ACM Transactions on Computer - Human Interaction 21 , 5 ( 2014 ) , 1 – 26 . https : / / doi . org / 10 . 1145 / 2670534 [ 3 ] Pernille Bjørn and Ojelanki Ngwenyama . 2009 . Virtual team collabo - ration : building shared meaning , resolving breakdowns and creating translucence . Information Systems Journal 19 , 3 ( 5 2009 ) , 227 – 253 . https : / / doi . org / 10 . 1111 / j . 1365 - 2575 . 2007 . 00281 . x [ 4 ] Anita L . Blanchard , Jennifer L . Welbourne , and Marla D . Boughton . 2010 . A model of online trust : The mediating role of norms and sense of vir - tual community . Information , Communication & Society 14 , 1 ( 2010 ) , 76 – 106 . https : / / doi . org / 10 . 1080 / 13691181003739633 [ 5 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative Research in Psychology 3 , 2 ( 1 2006 ) , 77 – 101 . https : / / doi . org / 10 . 1191 / 1478088706qp063oa [ 6 ] Jie Cai , Cameron Guanlao , and Donghee Y . Wohn . 2021 . Understand - ing Rules in Live Streaming Micro Communities on Twitch . In ACM International Conference on Interactive Media Experiences . 290 – 295 . https : / / doi . org / 10 . 1145 / 3452918 . 3465491 [ 7 ] Jie Cai and Donghee Y . Wohn . 2019 . Categorizing Live Streaming Moderation Tools . International Journal of Interactive Communication Systems and Technolo - gies 9 , 2 ( 7 2019 ) , 36 – 50 . https : / / doi . org / 10 . 4018 / IJICST . 2019070103 [ 8 ] Jie Cai and Donghee Y . Wohn . 2021 . After Violation But Before Sanction : Un - derstandingVolunteer Moderators’ProﬁlingProcessesTowardViolatorsinLive Streaming Communities . Proceedings of the ACM on Human - Computer Interac - tion 5 , CSCW2 ( 2021 ) , 1 – 25 . https : / / doi . org / 10 . 1145 / 3479554 [ 9 ] Jie Cai , Donghee Y . Wohn , and Mashael Almoqbel . 2021 . Moderation Visibility : Mapping the Strategies of Volunteer Moderators in Live Streaming Micro Com - munities . In Proceedings of ACM International Conference on Interactive Media Experiences . 61 – 72 . https : / / doi . org / 10 . 1145 / 3452918 . 3458796 [ 10 ] Stevie Chancellor , Andrea Hu , and Munmun De Choudhury . 2018 . Norms Mat - ter : ContrastingSocialSupport Around BehaviorChangein OnlineWeight Loss Communities . In Proceedings of the ACM Conference on Human Factors in Com - puting Systems . 1 – 14 . https : / / doi . org / 10 . 1145 / 3173574 . 3174240 [ 11 ] EshwarChandrasekharan , MattiaSamory , ShagunJhaver , HunterCharvat , Amy Bruckman , Cliﬀ Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s Hidden Rules : An Empirical Study of Reddit Norm Violations at Micro , Meso , and Macro Scales . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) , 1 – 25 . https : / / doi . org / 10 . 1145 / 3274301 [ 12 ] Eshwar Chandrasekharan , MattiaSamory , Anirudh Srinivasan , and Eric Gilbert . 2017 . The Bag of Communities : Identifying Abusive Behavior Online with Pre - existing Internet Data . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 3175 – 3187 . https : / / doi . org / 10 . 1145 / 3025453 . 3026018 [ 13 ] RobertB . Cialdini , CarlA . Kallgren , andRaymondR . Reno . 1991 . AFocusTheory of Normative Conduct : A Theoretical Reﬁnement and Reevaluation of the Role of Norms in Human Behavior . In Advances in Experimental Social Psychology . Vol . 24 . 201 – 234 . https : / / doi . org / 10 . 1016 / S0065 - 2601 ( 08 ) 60330 - 5 [ 14 ] Robert B Cialdini and Melanie R Mr Trost . 1998 . Social inﬂuence : Social norms , conformity and compliance . In The handbook of social psychology , Vols . 1 - 2 , 4th ed . McGraw - Hill , New York , NY , US , 151 – 192 . Coordination and Collaboration : How do Volunteer Moderators Work as a Team in Live Streaming Communities ? CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA [ 15 ] LauraDabbish , RobertKraut , andJordanPatton . 2012 . Communicationandcom - mitment in an online game team . In Conference on Human Factorsin Computing Systems - Proceedings . 879 – 888 . https : / / doi . org / 10 . 1145 / 2207676 . 2208529 [ 16 ] Bryan Dosono and Bryan Semaan . 2019 . Moderation Practices as Emotional Labor in Sustaining Online Communities . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 1 – 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300372 [ 17 ] Blake Droesch . 2021 . The Livestreaming Landscape 2021 - Insider Intelligence Trends , Forecasts & Statistics . https : / / www . emarketer . com / content / livestreaming - landscape - 2021 [ 18 ] Scott Elrod , Richard Bruce , Rich Gold , David Goldberg , Frank Halasz , William Janssen , David Lee , Kim McCall , Elin Pedersen , Ken Pier , John Tang , and Brent Welch . 1992 . Liveboard : a large interactive display supporting group meetings , presentationsandremotecollaboration . In ConferenceonHumanFactorsinCom - puting Systems - Proceedings , Vol . 92 . 599 – 607 . [ 19 ] Dawn L . Eubanks , Michael Palanski , Joy Olabisi , Adam Joinson , and James Dove . 2016 . Team dynamics in virtual , partially distributed teams : Opti - mal role fulﬁllment . Computers in Human Behavior 61 ( 8 2016 ) , 556 – 568 . https : / / doi . org / 10 . 1016 / j . chb . 2016 . 03 . 035 [ 20 ] ChenluFeng andDonghee Yvette Wohn . 2020 . CategorizingOnline Harassment Interventions . In International Symposium on Technology and Society , Proceed - ings , Vol . 2020 - Novem . Institute of Electrical and Electronics Engineers ( IEEE ) , 255 – 265 . https : / / doi . org / 10 . 1109 / ISTAS50296 . 2020 . 9462206 [ 21 ] Casey Fiesler , Jialun " Aaron " Jiang , Joshua McCann , Kyle Frye , and Jed R Brubaker . 2018 . Reddit Rules ! Characterizing an Ecosystem of Governance . In Proceedingsofthe6thInternationalAAAIConferenceonWeblogsandSocialMedia . 72 – 81 . https : / / doi . org / 10 . 1016 / j . tvjl . 2007 . 05 . 023 [ 22 ] Guo Freeman and Donghee Yvette Wohn . 2019 . Understanding eSports Team Formation and Coordination . Computer Supported Cooperative Work ( CSCW ) 28 , 1 - 2 ( 4 2019 ) , 95 – 126 . https : / / doi . org / 10 . 1007 / s10606 - 017 - 9299 - 4 [ 23 ] Sarah A . Gilbert . 2020 . " I run the world’s largest historical outreach project and it’s on a cesspool of a website . " Moderating a Public Scholarship Site on Reddit : A Case Study of r / AskHistorians . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( 5 2020 ) , 1 – 27 . https : / / doi . org / 10 . 1145 / 3392822 [ 24 ] Tarleton Gillespie . 2018 . Custodians of the internet : Platforms , content moderation , and the hidden decisions that shape so - cial media . Yale University Press , New Haven . 1 – 288 pages . https : / / www . scopus . com / inward / record . uri ? eid = 2 - s2 . 0 - 85051469782 & partnerID = 40 & md5 = 8d850b5298b7e5dc1a1fcf4c427fe3da [ 25 ] Tarleton Gillespie . 2020 . Content moderation , AI , and the question of scale . Big Data and Society 7 , 2 ( 7 2020 ) . https : / / doi . org / 10 . 1177 / 2053951720943234 [ 26 ] Tarleton Gillespie , Patricia Aufderheide , Elinor Carmi , Ysabel Gerrard , Robert Gorwa , Ariadna Matamoros - Fernández , Sarah T . Roberts , Aram Sinnreich , and SarahMyersWest . 2020 . Expanding thedebateaboutcontent moderation : Schol - arly research agendas for the coming policy debates . Internet Policy Review 9 , 4 ( 2020 ) , 1 – 29 . https : / / doi . org / 10 . 14763 / 2020 . 4 . 1512 [ 27 ] James Grimmelmann . 2015 . The Virtues of Modera - tion . Yale Journal of Law and Technology 17 , 1 ( 2015 ) , 68 . https : / / digitalcommons . law . yale . edu / yjolt / vol17 / iss1 / 2 [ 28 ] Carl Gutwin , Saul Greenberg , Roger Blum , Jeﬀ Dyck , KimberlyTee , and Gregor McEwan . 2008 . Supporting informal collaboration in shared - workspace group - ware . In Journal of Universal Computer Science , Vol . 14 . 1411 – 1434 . [ 29 ] William A Hamilton , OliverGarretson , and Andruid Kerne . 2014 . Streaming on Twitch : Fostering Participatory Communities of Play within Live Mixed Media . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 1315 – 1324 . https : / / doi . org / 10 . 1145 / 2556288 . 2557048 [ 30 ] Michael A . Hogg and Scott A . Reid . 2006 . Social Identity , Self - Categorization , and the Communication of Group Norms . CommunicationTheory 16 , 1 ( 2 2006 ) , 7 – 30 . https : / / doi . org / 10 . 1111 / j . 1468 - 2885 . 2006 . 00003 . x [ 31 ] Liaquat Hossain and Rolf T . Wigand . 2004 . ICT enabled virtual collaboration through trust . https : / / doi . org / 10 . 1111 / j . 1083 - 6101 . 2004 . tb00233 . x [ 32 ] Jane Im , Sonali Tandon , Eshwar Chandrasekharan , Taylor Denby , and Eric Gilbert . 2020 . Synthesized Social Signals : Computationally - Derived Social Sig - nals from Account Histories . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 1 – 12 . https : / / doi . org / 10 . 1145 / 3313831 . 3376383 [ 33 ] HiroshiIshiiand BryggUllmer . 1997 . Tangiblebits : Towardsseamlessinterfaces between people , bits and atoms . In Conference on Human Factors in Computing Systems - Proceedings . 234 – 241 . [ 34 ] Shagun Jhaver , Darren Scott Appling , Eric Gilbert , and Amy Bruckman . 2019 . “Did You Suspect the Post Would be Removed ? " : Understanding User Reactions to Content Removals on Reddit . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 33 . https : / / doi . org / 10 . 1145 / 3359294 [ 35 ] Shagun Jhaver , Iris Birman , Eric Gilbert , and Amy Bruckman . 2019 . Human - machine collaboration for content regulation : The case of reddit automod - erator . ACM Transactions on Computer - Human Interaction 26 , 5 ( 2019 ) , 35 . https : / / doi . org / 10 . 1145 / 3338243 [ 36 ] Shagun Jhaver , Amy Bruckman , and Eric Gilbert . 2019 . Does Transparency in Moderation ReallyMatter ? : UserBehaviorAfterContent RemovalExplanations on Reddit . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 27 . https : / / doi . org / 10 . 1145 / 3359252 [ 37 ] Jialun Aaron Jiang , Charles Kiene , Skyler Middler , Jed R . Brubaker , and Casey Fiesler . 2019 . Moderation Challenges in Voice - based Online Communities on Discord . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 11 2019 ) , 1 – 23 . https : / / doi . org / 10 . 1145 / 3359157 [ 38 ] Robyn Keast , Kerry Brown , and Myrna Mandell . 2007 . Getting The Right Mix : Unpacking Integration Meanings and Strategies . International Public Manage - ment Journal 10 , 1 ( 2 2007 ) , 9 – 33 . https : / / doi . org / 10 . 1080 / 10967490601185716 [ 39 ] Aniket Kittur and Robert E Kraut . 2008 . Harnessing the wisdom of crowds in wikipedia . In Proceedings of the ACM 2008 conference on Computer sup - ported cooperative work - CSCW ’08 . ACM Press , New York , New York , USA , 37 . https : / / doi . org / 10 . 1145 / 1460563 . 1460572 [ 40 ] Aniket Kittur and Robert E . Kraut . 2010 . Beyond Wikipedia : Coor - dination and Conﬂict in Online Production Groups . In Proceedings of the ACM Conference on Computer Supported Cooperative Work . 215 – 224 . https : / / doi . org / 10 . 1145 / 1718918 . 1718959 [ 41 ] Aniket Kittur , Bongwon Suh , Bryan A . Pendleton , and Ed H . Chi . 2007 . He says , she says : conﬂict and coordination in Wikipedia . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 453 – 462 . https : / / doi . org / 10 . 1145 / 1240624 . 1240698 [ 42 ] Yubo Kou and Xinning Gui . 2021 . Flag and Flaggability in Auto - mated Moderation . In Proceedings of the 2021 CHI Conference on Hu - man Factors in Computing Systems . ACM , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3411764 . 3445279 [ 43 ] Robert E . Kraut and Andrew T . Fiore . 2014 . The role of founders in build - ing online groups . In Proceedings of the 17th ACM conference on Computer sup - ported cooperative work & social computing . ACM , New York , NY , USA , 722 – 732 . https : / / doi . org / 10 . 1145 / 2531602 . 2531648 [ 44 ] Hideaki Kuzuoka . 1992 . Spatial workspace collaboration : a sharedview video supportsystemforremotecollaborationcapability . In ConferenceonHumanFac - tors in Computing Systems - Proceedings . 533 – 540 . [ 45 ] Stefan Larsson , Mans Svensson , Marcin de Kaminski , Kari Rönkkö , and Johanna Alkan Olsson . 2012 . Law , norms , piracy and online anonymity : Practices of de - identiﬁcation in the global ﬁle sharing com - munity . Journal of Research in Interactive Marketing 6 , 4 ( 2012 ) , 260 – 280 . https : / / doi . org / 10 . 1108 / 17505931211282391 [ 46 ] Alex Leavitt , Brian C . Keegan , and Joshua Clark . 2016 . Ping to win ? Non - verbal communicationandteamperformanceincompetitiveonline multiplayergames . In Conference on Human Factors in Computing Systems - Proceedings . 4337 – 4350 . https : / / doi . org / 10 . 1145 / 2858036 . 2858132 [ 47 ] Jean MacMillan , Elliot E . Entin , and Daniel Serfaty . 2004 . Communication over - head : The hidden cost of team cognition . In Team cognition : Understanding the factors that drive process and performance . American Psychological Association , Washington , 61 – 82 . https : / / doi . org / 10 . 1037 / 10690 - 004 [ 48 ] Kris M . Markman . 2012 . A Networked Self : Identity , Community and Cul - ture on Social Network Sites . New Media & Society 14 , 7 ( 2012 ) , 1240 – 1242 . https : / / doi . org / 10 . 1177 / 1461444812453432 [ 49 ] J . Nathan Matias . 2019 . The CivicLaborof Volunteer Moderators Online . Social Media + Society 5 , 2 ( 2019 ) , 12 . https : / / doi . org / 10 . 1177 / 2056305119836778 [ 50 ] Aiden McGillicuddy , Jean Grégoire Bernard , and Jocelyn Craneﬁeld . 2016 . Con - trolling bad behavior in online communities : An examination of moderation work . In Proceeding of the 36th International Conference on Information Systems . 1 – 11 . https : / / doi . org / 10 . 26686 / wgtn . 12910085 . v1 [ 51 ] Mostafa Mesgari , Chitu Okoli , Mohamad Mehdi , Finn Årup Nielsen , and Arto Lanamäki . 2015 . “The sum of all human knowledge” : A systematic review of scholarly research on the content of Wikipedia . Journal of the Association for Information Science and Technology 66 , 2 ( 2 2015 ) , 219 – 245 . https : / / doi . org / 10 . 1002 / ASI . 23172 [ 52 ] Claudia Müller - Birn , Leonhard Dobusch , and James D . Herbsleb . 2013 . Work - to - rule : the emergence of algorithmic governance in Wikipedia . In Proceed - ings of the 6th International Conference on Communities and Technologies . 80 – 89 . https : / / doi . org / 10 . 1145 / 2482991 . 2482999 [ 53 ] Sarah Myers West . 2018 . Censored , suspended , shadowbanned : User interpreta - tions of content moderation on social media platforms . New Media & Society 20 , 11 ( 2018 ) , 4366 – 4383 . https : / / doi . org / 10 . 1177 / 1461444818773059 [ 54 ] Gary M Olson and Judith S Olson . 2000 . Distance Matters . Human – Computer Interaction 15 , 2 - 3 ( 2000 ) , 139 – 178 . https : / / doi . org / 10 . 1207 / S15327051HCI1523 [ 55 ] Joonsuk Park , Sally Klingel , Claire Cardie , Mary Newhart , Cynthia Farina , and Joan - JosepVallbé . 2012 . FacilitativemoderationforonlineparticipationineRule - making . In Proceedings of the 13th Annual International Conference on Digital Government Research - dg . o ’12 . ACM Press , New York , New York , USA , 173 . https : / / doi . org / 10 . 1145 / 2307729 . 2307757 [ 56 ] Birgit Pfau - Eﬃnger , Lluís Flaquer , and Per H . Jensen . 2010 . Formal and In - formal Work : The Hidden Work Regime in Europe . Routledge . 1 – 245 pages . https : / / doi . org / 10 . 4324 / 9780203881392 CHI ’22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Cai and Donghee [ 57 ] Ramón Rico , Miriam Sánchez - Manzanares , Francisco Gil , and Cristina Gib - son . 2008 . Team implicit coordination processes : A team knowledge - based approach . Academy of Management Review 33 , 1 ( 2008 ) , 163 – 184 . https : / / doi . org / 10 . 5465 / AMR . 2008 . 27751276 [ 58 ] Sarah T Roberts . 2016 . Commercial content moderation : Digital laborers’ dirty work . In The Intersectional Internet : Race , Sex , Class and Culture Online , Saﬁya Umoja Noble and Brendesha Tynes ( Eds . ) . NY : Peter Lang , New York , Chapter Commercial , 147 – 160 . https : / / doi . org / 10 . 1007 / s13398 - 014 - 0173 - 7 . 2 [ 59 ] Joseph Seering , Geoﬀ Kaufman , and Stevie Chancellor . 2022 . Metaphors in moderation . New Media & Society 24 , 3 ( 3 2022 ) , 621 – 640 . https : / / doi . org / 10 . 1177 / 1461444820964968 [ 60 ] Joseph Seering , Robert Kraut , and Laura Dabbish . 2017 . Shaping Pro and Anti - Social Behavior on Twitch Through Moderation and Example - Setting . In Pro - ceedings of the ACM Conference on Computer Supported Cooperative Work and Social Computing . 111 – 125 . https : / / doi . org / 10 . 1145 / 2998181 . 2998277 [ 61 ] Joseph Seering , Tony Wang , Jina Yoon , and Geoﬀ Kaufman . 2019 . Moderator engagement and community development in the age of algorithms . New Media & Society 21 , 7 ( 7 2019 ) , 1417 – 1443 . https : / / doi . org / 10 . 1177 / 1461444818821316 [ 62 ] Fatma Cemile Sere , Kathleen Swigger , Ferda Nur Alpaslan , Robert Brazile , George Dafoulas , and Victor Lopez . 2011 . Online collaboration : Collab - orative behavior patterns and factors aﬀecting globally distributed team performance . Computers in Human Behavior 27 , 1 ( 1 2011 ) , 490 – 503 . https : / / doi . org / 10 . 1016 / j . chb . 2010 . 09 . 017 [ 63 ] Kumar Bhargav Srinivasan , Cristian Danescu - Niculescu - Mizil , Lillian Lee , and Chenhao Tan . 2019 . Content Removal as a Moderation Strategy : Compli - ance and Other Outcomes in the ChangeMyView Community . Proceed - ings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 21 . https : / / doi . org / 10 . 1145 / 3359265 [ 64 ] Miriah Steiger , Timir J Bharucha , Sukrit Venkatagiri , Martin J . Riedl , and Matthew Lease . 2021 . The Psychological Well - Being of Content Moderators : The Emotional Labor of Commercial Moderation and Avenues for Improving Support . In Proceedings of the ACM Conference on Human Factors in Computing Systems . 1 – 14 . https : / / doi . org / 10 . 1145 / 3411764 . 3445092 [ 65 ] Marcin Sydow , Katarzyna Baraniak , and Paweł Teisseyre . 2017 . Diversity of editors and teams versus quality of cooperative work : experiments on wikipedia . Journal of Intelligent Information Systems 48 , 3 ( 6 2017 ) , 601 – 632 . https : / / doi . org / 10 . 1007 / S10844 - 016 - 0428 - 1 / TABLES / 20 [ 66 ] Katie Salen Tekinbaş , Krithika Jagannath , Ulrik Lyngs , and Petr Slovák . 2021 . Designing for Youth - Centered Moderation and Community Governance in Minecraft . ACM Transactions on Computer - Human Interaction 28 , 4 ( 2021 ) , 1 – 41 . https : / / doi . org / 10 . 1145 / 3450290 [ 67 ] JirassayaUttarapong , Jie Cai , and Donghee Y . Wohn . 2021 . Harassment Experi - ences of Women and LGBTQ Live Streamers and How They Handled Negativ - ity . In ACMInternational ConferenceonInteractiveMedia Experiences . ACM , New York , NY , USA , 7 – 19 . https : / / doi . org / 10 . 1145 / 3452918 . 3458794 [ 68 ] Sahaj Vaidya , Jie Cai , Soumyadeep Basu , Azadeh Naderi , Donghee Y . Wohn , and Aritra Dasgupta . 2021 . Conceptualizing Visual Analytic Interventions for Content Moderation . In 2021 IEEEVisualization Conference ( VIS ) . IEEE , 191 – 195 . https : / / doi . org / 10 . 1109 / VIS49827 . 2021 . 9623288 [ 69 ] Chad Whelan . 2017 . Managing dynamic security networks : Towards the strate - gic managing of cooperation , coordination and collaboration . Security Journal 30 , 1 ( 2 2017 ) , 310 – 327 . https : / / doi . org / 10 . 1057 / sj . 2014 . 20 [ 70 ] Donghee Y . Wohn . 2019 . Volunteer Moderators in Twitch Micro Communities : How TheyGetInvolved , the Roles TheyPlay , and the Emotional LaborTheyEx - perience . In Proceedings of the ACM Conference on Human Factorsin Computing Systems . 1 – 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300390 [ 71 ] Donghee Y . Wohn and Guo Freeman . 2020 . Audience Management Practices of Live Streamers on Twitch . In Proceedings of the ACM International Conference on Interactive Media Experiences . 106 – 116 . https : / / doi . org / 10 . 1145 / 3391614 . 3393653 [ 72 ] Donghee Y . Wohn , Guo Freeman , and Caitlin McLaughlin . 2018 . Explaining Viewers’ Emotional , Instrumental , and Financial Support Provision for Live Streamers . In ProceedingsoftheACMConferenceonHumanFactorsinComputing Systems . 1 – 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3174048 [ 73 ] Victor Yanev . 2021 . 37 + Live Streaming Statistics Every Marketer Should Keep In Mind in 2021 . https : / / techjury . net / blog / live - streaming - statistics / # gref [ 74 ] Tao Zhou . 2011 . Understanding online community user participation : a social inﬂuence perspective . Internet Research 21 , 1 ( 1 2011 ) , 67 – 81 . https : / / doi . org / 10 . 1108 / 10662241111104884 [ 75 ] Yingfan Zhou and Rosta Farzan . 2021 . Designing to Stop LiveStreaming Cyber - bullying . In Proceedings of the 10th International Conference on Communities & Technologies - Wicked Problems in the Age of Tech . ACM , New York , NY , USA , 138 – 150 . https : / / doi . org / 10 . 1145 / 3461564 . 3461574