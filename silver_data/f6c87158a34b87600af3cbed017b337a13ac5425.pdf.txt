BIT 34 ( 1994 ) , 556 { 577 . Least Squares Fitting of Circles and Ellipses Walter Gander , Gene H . Golub and Rolf Strebel Institut f(cid:127)ur Wissenschaftliches Rechnen Eidgen(cid:127)ossische Technische Hochschule CH - 8092 Z(cid:127)urich , Switzerland email : fgander , strebelg @ inf . ethz . ch Computer Science Department Stanford University Stanford , California 94305 email : golub @ sccm . stanford . edu Dedicated to (cid:23)Ake Bj(cid:127)orck on the occasion of his 60 th birthday . Abstract . Fitting circles and ellipses to given points in the plane is a problem that arises in many appli - cation areas , e . g . computer graphics [ 1 ] , coordinate metrology [ 2 ] , petroleum engineering [ 11 ] , statistics [ 7 ] . In the past , algorithms have been given which (cid:12)t circles and ellipses in some least squares sense without minimizing the geometric distance to the given points [ 1 ] , [ 6 ] . In this paper we present several algorithms which compute the ellipse for which the sum of the squares of the distances to the given points is minimal . These algorithms are compared with classical simple and iterative methods . Circles and ellipses may be represented algebraically i . e . by an equation of the form F ( x ) = 0 . If a point is on the curve then its coordinates x are a zero of the function F . Alternatively , curves may be represented in parametric form , which is well suited for minimizing the sum of the squares of the distances . Key words : least squares , curve (cid:12)tting , singular value decomposition . 1 Preliminaries and Introduction Ellipses , for which the sum of the squares of the distances to the given points is minimal will be referred to as \ best (cid:12)t " or \ geometric (cid:12)t " , and the algorithms will be called \ geometric " . Determining the parameters of the algebraic equation F ( x ) = 0 in the least squares sense will be denoted by \ algebraic (cid:12)t " and the algorithms will be called \ algebraic " . We will use the well known Gauss - Newton method to solve the nonlinear least squares problem ( cf . [ 15 ] ) . Let u = ( u1 ; : : : ; un ) T be a vector of unknowns and consider the nonlinear system of m equations f ( u ) = 0 . If m > n , then we want to minimize m X i = 1 fi ( u ) 2 = min : This is a nonlinear least squares problem , which we will solve iteratively by a sequence of linear least squares problems . 2 We approximate the solution ^ u by ~ u + h . Developing f ( u ) = ( f1 ( u ) ; f2 ( u ) ; : : : ; fm ( u ) ) T around ~ u in a Taylor series , we obtain f ( ~ u + h ) ' f ( ~ u ) + J ( ~ u ) h (cid:25) 0 ; ( 1 . 1 ) where J is the Jacobian . We solve equation ( 1 . 1 ) as a linear least squares problem for the correction vector h : J ( ~ u ) h (cid:25) (cid:0)f ( ~ u ) : ( 1 . 2 ) An iteration then with the Gauss - Newton method consists of the two steps : 1 . Solving equation ( 1 . 2 ) for h . 2 . Update the approximation ~ u : = ~ u + h . We de(cid:12)ne the following notation : a given point Pi will have the coordinate vector xi = ( xi1 ; xi2 ) T . The m (cid:2) 2 matrix X = [ x1 ; : : : ; xm ] T will therefore contain the coordinates of a set of m points . The 2 - norm (cid:1) 2 of vectors and matrices will simply be denoted by (cid:1) . We are very pleased to dedicate this paper to professor (cid:23) Ake Bj(cid:127)orck who has contribut - ed so much to our understanding of the numerical solution of least squares problems . Not only is he a scholar of great distinction , but he has always been generous in spirit and gentlemanly in behavior . 2 Circle : Minimizing the algebraic distance Let us (cid:12)rst consider an algebraic representation of the circle in the plane : F ( x ) = ax T x + b T x + c = 0 ; ( 2 . 1 ) where a 6 = 0 and x ; b 2 IR 2 . To (cid:12)t a circle , we need to compute the coe(cid:14)cients a , b and c from the given data points . If we insert the coordinates of the points into equation ( 2 . 1 ) , we obtain a linear system of equations Bu = 0 for the coe(cid:14)cients u = ( a ; b1 ; b2 ; c ) T , where B = 0 B @ x 2 11 + x 2 12 x 11 x 12 1 . . . . . . . . . . . . x 2 m1 + x 2 m2 x m1 x m2 1 1 C A : To obtain a non - trivial solution , we impose some constraint on u , e . g . u1 = 1 ( commonly used ) or u = 1 . For m > 3 , in general , we cannot expect the system to have a solution , unless all the points happen to be on a circle . Therefore , we solve the overdetermined system Bu = r where u is chosen to minimize r . We obtain a standard problem ( c . f . [ 3 ] ) : Bu = min subject to u = 1 : 3 This problem is equivalent to (cid:12)nding the right singular vector associated with the smallest singular value of B . If a 6 = 0 , we can transform equation ( 2 . 1 ) to (cid:18) x1 + b1 2a (cid:19) 2 + (cid:18) x2 + b2 2a (cid:19) 2 = b 2 4a 2 (cid:0) c a ; ( 2 . 2 ) from which we obtain the center and the radius , if the right hand side of ( 2 . 2 ) is positive : z = ( z1 ; z2 ) = (cid:18) (cid:0) b1 2a ; (cid:0) b2 2a (cid:19) r = s b 2 4a 2 (cid:0) c a : This approach has the advantage of being simple . The disadvantage is that we are uncertain what we are minimizing in a geometrical sense . For applications in coordinate metrology this kind of (cid:12)t is often unsatisfactory . In such applications , one wishes to minimize the sum of the squares of the distances . Figure 2 . 1 shows two circles (cid:12)tted to the set of points x 1 2 5 7 9 3 y 7 6 8 7 5 7 : ( 2 . 3 ) Minimizing the algebraic distance , we obtain the dashed circle with radius r = 3 : 0370 and center z = ( 5 : 3794 ; 7 : 2532 ) . The algebraic solution is often useful as a starting vector for methods minimizing the geometric distance . 3 Circle : Minimizing the geometric distance To minimize the sum of the squares of the distances d 2 i = ( z (cid:0) xi (cid:0) r ) 2 we need to solve a nonlinear least squares problem . Let u = ( z1 ; z2 ; r ) T , we want to determine ~ u so that m X i = 1 di ( u ) 2 = min : The Jacobian de(cid:12)ned by the partial derivatives @ di ( u ) = @ uj is given by : J ( u ) = 0 B B B B B B B @ u1 (cid:0) x11 q ( u1 (cid:0) x11 ) 2 + ( u2 (cid:0) x12 ) 2 u2 (cid:0) x12 q ( u1 (cid:0) x11 ) 2 + ( u2 (cid:0) x12 ) 2 (cid:0)1 . . . . . . . . . u1 (cid:0) xm1 q ( u1 (cid:0) xm1 ) 2 + ( u2 (cid:0) xm2 ) 2 u2 (cid:0) x2m q ( u1 (cid:0) xm1 ) 2 + ( u2 (cid:0) xm2 ) 2 (cid:0)1 1 C C C C C C C A : A good starting vector for the Gauss - Newton method may often be obtained by solving the linear problem as given in the previous paragraph . The algorithm then iteratively computes the \ best " circle . If we use the set of points ( 2 . 3 ) and start the iteration with the values obtained from the linear model ( minimizing the algebraic distance ) , then after 11 Gauss - Newton steps the norm of the correction vector is 2 : 05E(cid:0)6 . We obtain the best (cid:12)t circle with center z = ( 4 : 7398 ; 2 : 9835 ) and radius r = 4 : 7142 ( the solid circle in (cid:12)gure 2 . 1 ) . 4 - 2 0 2 4 6 8 10 12 - 2 0 2 4 6 8 10 12 Figure 2 . 1 : algebraic vs . best (cid:12)t (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Best (cid:12)t (cid:0) (cid:0) (cid:0) (cid:0) Algebraic (cid:12)t 4 Circle : Geometric (cid:12)t in parametric form The parametric form commonly used for the circle is given by x = z1 + rcos ' ( 4 . 1 ) y = z2 + rsin ' : ( 4 . 2 ) The distance di of a point Pi = ( xi1 ; xi2 ) may be expressed by d 2 i = min ' i h ( xi1 (cid:0) x ( ' i ) ) 2 + ( xi2 (cid:0) y ( ' i ) ) 2 i : Now since we want to determine z1 , z2 and r by minimizing m X i = 1 d 2 i = min ; we can simultaneously minimize for z1 , z2 , r and f ' igi = 1 : : : m ; i . e . (cid:12)nd the minimum of the quadratic function Q ( ' 1 ; ' 2 ; : : : ; ' m ; z1 ; z2 ; r ) = m X i = 1 h ( xi1 (cid:0) x ( ' i ) ) 2 + ( xi2 (cid:0) y ( ' i ) ) 2 i : 5 This is equivalent to solving the nonlinear least squares problem z1 + rcos ' i (cid:0) xi1 (cid:25) 0 z2 + rsin ' i (cid:0) xi2 (cid:25) 0 for i = 1 ; 2 ; : : : ; m : Let u = ( ' 1 ; : : : ; ' m ; z1 ; z2 ; r ) . The Jacobian associated with Q is J = rS A (cid:0)rC B ! ; where S = diag ( sin ' i ) and C = diag ( cos ' i ) are m (cid:2) m diagonal matrices . A and B are m (cid:2) 3 matrices de(cid:12)ned by : ai1 = (cid:0)1 ai2 = 0 ai3 = (cid:0)cos ' i bi1 = 0 bi2 = (cid:0)1 bi3 = (cid:0)sin ' i : For large m , J is very sparse . We note that the (cid:12)rst part (cid:0) rS (cid:0)rC (cid:1) is orthogonal . To compute the QR decomposition of J we use the orthonormal matrix Q = S C (cid:0)C S ! : Multiplying from the left we get Q T J = rI SA (cid:0) CB O CA + SB ! : So to obtain the QR decomposition of the Jacobian , we only have to compute a QR decomposition of the m (cid:2) 3 sub - matrix CA + SB = UP . Then I 0 O U T ! Q T J = rI SA (cid:0) CB O P ! ; and the solution is obtained by backsubstitution . In general we may obtain good starting values for z1 , z2 and r for the Gauss - Newton iteration , if we (cid:12)rst solve the linear problem by minimizing the algebraic distance . If the center is known , initial approximations for f ' kgk = 1 : : : m can be computed by ' k = arg ( ( xk1 (cid:0) z1 ) + i ( xk2 (cid:0) z2 ) ) : We use again the points ( 2 . 3 ) and start the iteration with the values obtained from the linear model ( minimizing the algebraic distance ) . After 21 Gauss - Newton steps the norm of the correction is 3 : 43E(cid:0)06 and we obtain the same results as before : center z = ( 4 : 7398 ; 2 : 9835 ) and radius r = 4 : 7142 ( the solid circle in (cid:12)gure 2 . 1 ) . 6 5 Ellipse : Minimizing the algebraic distance Given the quadratic equation x T Ax + b T x + c = 0 ( 5 . 1 ) with A symmetric and positive de(cid:12)nite , we can compute the geometric quantities of the conic as follows . We introduce new coordinates (cid:22)x with x = Q(cid:22)x + t , thus rotating and shifting the conic . Then equation ( 5 . 1 ) becomes (cid:22)x T ( Q T AQ ) (cid:22)x + ( 2t T A + b T ) Q(cid:22)x + t T At + b T t + c = 0 : De(cid:12)ning (cid:22) A = Q T AQ , and similarly (cid:22) b and (cid:22)c , this equation may be written (cid:22)x T (cid:22) A(cid:22)x + (cid:22) b T (cid:22)x + (cid:22)c = 0 : We may choose Q so that (cid:22) A = diag ( (cid:21)1 ; (cid:21)2 ) ; if the conic is an ellipse or a hyperbola , we may further choose t so that (cid:22) b = 0 . Hence , the equation may be written (cid:21)1(cid:22)x 2 1 + (cid:21)2(cid:22)x 2 2 + (cid:22)c = 0 ; ( 5 . 2 ) and this de(cid:12)nes an ellipse if (cid:21)1 > 0 , (cid:21)2 > 0 and (cid:22)c < 0 . The center and the axes of the ellipse in the non - transformed system are given by z = t a = q (cid:0)(cid:22)c = (cid:21)1 b = q (cid:0)(cid:22)c = (cid:21)2 : Since Q T Q = I , the matrices A and (cid:22) A have the same ( real ) eigenvalues (cid:21)1 , (cid:21)2 . It follows that each function of (cid:21)1 and (cid:21)2 is invariant under rotation and shifts . Note detA = a11a22 (cid:0) a21a12 = (cid:21)1(cid:21)2 traceA = a11 + a22 = (cid:21)1 + (cid:21)2 ; which serve as a basis for all polynomials symmetric in (cid:21)1 , (cid:21)2 . As a possible application of above observations , let us express the quotient (cid:20) = a = b for the ellipse ' s axes a and b . With a 2 = (cid:0)(cid:22)c = (cid:21)1 and b 2 = (cid:0)(cid:22)c = (cid:21)2 we get (cid:20) 2 + 1 = (cid:20) 2 = (cid:21) 2 (cid:21)1 + (cid:21) 1 (cid:21)2 = (cid:21) 2 1 + (cid:21) 2 2 (cid:21)1(cid:21)2 = ( traceA ) 2 (cid:0) 2detA detA = a 2 11 + a 2 22 + 2a 2 12 a 11 a 22 (cid:0) a 2 12 and therefore (cid:20) 2 = (cid:22) (cid:6) q (cid:22) 2 (cid:0) 1 where 7 (cid:22) = ( traceA ) 2 2detA (cid:0) 1 : To compute the coe(cid:14)cients u from given points , we insert the coordinates into equa - tion ( 5 . 1 ) and obtain a linear system of equations Bu = 0 , which we may solve again as constrained least squares problem : Bu = min subject to u = 1 . The disadvantage of the constraint u = 1 is its non - invariance for Euclidean coor - dinate transformations x = Q(cid:22)x + t ; where Q T Q = I : For this reason Bookstein [ 9 ] recommended solving the constrained least squares prob - lem x T Ax + b T x + c (cid:25) 0 ( 5 . 3 ) (cid:21) 2 1 + (cid:21) 2 2 = a 2 11 + 2a 2 12 + a 2 22 = 1 : ( 5 . 4 ) While [ 9 ] describes a solution based on eigenvalue decomposition , we may solve the same problem more e(cid:14)ciently and accurately with a singular value decomposition as described in [ 12 ] . In the simple algebraic solution by SVD , we solve the system for the parameter vector u = ( a11 ; 2a12 ; a22 ; b1 ; b2 ; c ) T ( 5 . 5 ) with the constraint u = 1 , which is not invariant under Euclidean transformations . If we de(cid:12)ne vectors v = ( b1 ; b2 ; c ) T w = ( a11 ; p 2a12 ; a22 ) T and the coe(cid:14)cient matrix S = 0 B B @ x 11 x 12 1 x 2 11 p 2x 11 x 12 x 2 12 . . . . . . . . . . . . . . . . . . x m1 x m2 1 x 2 m1 p 2x m1 x m2 x 2 m2 1 C C A ; then the Bookstein constraint ( 5 . 4 ) may be written w = 1 , and we have the reordered system S v w ! (cid:25) 0 : The QR decomposition of S leads to the equivalent system R11 R12 0 R22 ! v w ! (cid:25) 0 ; which may be solved in following steps : R22w (cid:25) 0 w = 1 : 8 Using the singular value decomposition of R22 = U(cid:6)V T , (cid:12)nding w = v3 , and then v = (cid:0)R11 (cid:0)1 R12w : Note that the problem ( S1 S2 ) v w ! (cid:25) 0 where w = 1 is equivalent to the generalized total least squares problem (cid:12)nding a matrix ^ S2 such that rank ( S1 ^ S2 ) (cid:20) 5 ( S1 ^ S2 ) (cid:0) ( S1 S2 ) = inf rank ( S1 (cid:22) S2 ) (cid:20)5 ( S1 (cid:22) S2 ) (cid:0) ( S1 S2 ) : In other words , (cid:12)nd a best rank 5 approximation to S that leaves S1 (cid:12)xed . A description of this problem may be found in [ 16 ] . - 10 - 5 0 5 10 - 8 - 6 - 4 - 2 0 2 4 6 8 10 12 Figure 5 . 1 : Euclidean - invariant algorithms (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Constraint (cid:21) 2 1 + (cid:21) 2 2 = 1 (cid:0) (cid:0) (cid:0) (cid:0) Constraint (cid:21)1 + (cid:21)2 = 1 To demonstrate the in(cid:13)uence of di(cid:11)erent coordinate systems , we have computed the ellipse (cid:12)t for this set of points : x 1 2 5 7 9 6 3 8 y 7 6 8 7 5 7 2 4 ; ( 5 . 6 ) 9 - 10 - 5 0 5 10 - 8 - 6 - 4 - 2 0 2 4 6 8 10 12 Figure 5 . 2 : Non - invariant algebraic algorithm (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Fitted ellipses (cid:0) (cid:0) (cid:0) (cid:0) Originally (cid:12)tted ellipse after transformation which are (cid:12)rst shifted by ( (cid:0)6 ; (cid:0)6 ) , and then by ( (cid:0)4 ; 4 ) and rotated by (cid:25) = 4 . See (cid:12)gures 5 . 1 { 5 . 2 for the (cid:12)tted ellipses . Since (cid:21) 2 1 + (cid:21) 2 2 6 = 0 for ellipses , hyperbolas and parabolas , the Bookstein constraint is appropriate to (cid:12)t any of these . But all we need is an invariant I 6 = 0 for ellipses | and one of them is (cid:21)1 + (cid:21)2 . Thus we may invariantly (cid:12)t an ellipse with the constraint (cid:21)1 + (cid:21)2 = a11 + a22 = 1 ; which results in the linear least squares problem 0 B @ 2x 11 x 12 x 2 22 (cid:0) x 2 11 x 11 x 12 1 . . . . . . . . . 2x m1 x m2 x 2 m2 (cid:0) x 2 m1 x m1 x m2 1 1 C Av (cid:25) 0 B @ (cid:0)x 2 11 . . . (cid:0)x 2 m1 1 C A : See the dashed ellipses in (cid:12)gure 5 . 1 . 6 Ellipse : Geometric (cid:12)t in parametric form In order to (cid:12)t an ellipse in parametric form , we consider the equations x = z + Q ( (cid:11) ) x 0 ; x 0 = acos ' bsin ' ! ; Q ( (cid:11) ) = cos(cid:11) (cid:0)sin(cid:11) sin (cid:11) cos(cid:11) ! : 10 Minimizing the sum of squares of the distances of the given points to the \ best " ellipse is equivalent to solving the nonlinear least squares problem : gi = xi1 xi2 ! (cid:0) z1 z2 ! (cid:0) Q ( (cid:11) ) acos ' i bsin ' i ! (cid:25) 0 ; i = 1 ; : : : ; m : Thus we have 2m nonlinear equations for m + 5 unknowns : ' 1 ; : : : ; ' m , (cid:11) , a , b , z1 , z2 . To compute the Jacobian we need the partial derivatives : @ gi @ ' j = (cid:0)(cid:14)ijQ ( (cid:11) ) (cid:0)asin ' i bcos ' i ! @ gi @ (cid:11) = (cid:0) _ Q ( (cid:11) ) acos ' i bsin ' i ! @ gi @ a = (cid:0)Q ( (cid:11) ) cos ' i 0 ! @ gi @ b = (cid:0)Q ( (cid:11) ) 0 sin ' i ! @ gi @ z1 = (cid:0)1 0 ! @ gi @ z2 = 0 (cid:0)1 ! where we have used the notation (cid:14)ij = ( 1 ; i = j 0 ; i 6 = j : Thus the Jacobian becomes : J = 0 B B @ (cid:0)Q (cid:0) (cid:0)as1 bc1 (cid:1) (cid:0) _ Q (cid:0) ac1 bs1 (cid:1) (cid:0)Q (cid:0) c1 0 (cid:1) (cid:0)Q (cid:0) 0 s1 (cid:1) (cid:0) (cid:0)1 0 (cid:1) (cid:0) 0 (cid:0)1 (cid:1) . . . . . . . . . . . . . . . . . . (cid:0)Q (cid:0) (cid:0)asm bcm (cid:1) (cid:0) _ Q (cid:0) acm bsm (cid:1) (cid:0)Q (cid:0) cm 0 (cid:1) (cid:0)Q (cid:0) 0 sm (cid:1) (cid:0) (cid:0)1 0 (cid:1) (cid:0) 0 (cid:0)1 (cid:1) 1 C C A ; where we have used as abbreviation si = sin ' i and ci = cos ' i . Note that _ Q ( (cid:11) ) = (cid:0)sin(cid:11) (cid:0)cos(cid:11) cos(cid:11) (cid:0)sin(cid:11) ! and therefore Q T _ Q = 0 (cid:0)1 1 0 ! : Since Q is orthogonal , the 2m (cid:2) 2m block diagonal matrix U = (cid:0)diag ( Q ; : : : ; Q ) is orthogonal , too , and U T J = 0 B B @ (cid:0) (cid:0)as1 bc1 (cid:1) (cid:0) (cid:0)bs1 ac1 (cid:1) (cid:0) c1 0 (cid:1) (cid:0) 0 s1 (cid:1) (cid:0) c (cid:0)s (cid:1) (cid:0) s c (cid:1) . . . . . . . . . . . . . . . . . . (cid:0) (cid:0)asm bcm (cid:1) (cid:0) (cid:0)bsm acm (cid:1) (cid:0) cm 0 (cid:1) (cid:0) 0 sm (cid:1) (cid:0) c (cid:0)s (cid:1) (cid:0) s c (cid:1) 1 C C A ; 11 where s = sin(cid:11) and c = cos(cid:11) . If we permute the equations , we obtain a similar structure for the Jacobian as in the circle (cid:12)t : (cid:22) J = (cid:0)aS A bC B ! : That is , S = diag ( sin ' i ) and C = diag ( cos ' i ) are two m (cid:2) m diagonal matrices and A and B are m (cid:2) 5 and are de(cid:12)ned by : A ( i ; 1 : 5 ) = [ (cid:0)bsin ' i cos ' i 0 cos(cid:11) sin (cid:11) ] B ( i ; 1 : 5 ) = [ acos ' i 0 sin ' i (cid:0)sin(cid:11) cos(cid:11) ] : We cannot give an explicit expression for an orthogonal matrix to triangularize the (cid:12)rst m columns of J in a similar way as we did in (cid:12)tting a circle . However , we can use Givens rotations to do this in m steps . Figure 6 . 1 shows two ellipses (cid:12)tted to the points given by x 1 2 5 7 9 3 6 8 y 7 6 8 7 5 7 2 4 : ( 6 . 1 ) By minimizing the algebraic distance with u = 1 we obtain the large cigar shaped dashed ellipse with z = ( 13 : 8251 ; (cid:0)2 : 1099 ) , a = 29 : 6437 , b = 1 : 8806 and residual norm r = 1 : 80 . If we minimize the sum of squares of the distances then we obtain the solid ellipse with z = ( 2 : 6996 ; 3 : 8160 ) , a = 6 : 5187 , b = 3 : 0319 and r = 1 : 17 . In order to obtain starting values for the nonlinear least squares problem we used the center , obtained by (cid:12)tting the best circle . We cannot use the approximation b0 = a0 = r , since the Jacobian becomes singular for b = a ! Therefore , we used b0 = r = 2 as a starting value . With (cid:11)0 = 0 , we needed 71 iteration steps to compute the \ best ellipse " shown in Figure 6 . 1 . 7 Ellipse : Iterative algebraic solutions In this section , we will present modi(cid:12)cations to the algebraic (cid:12)t of ellipses . The algebraic equations may be weighted depending on a given estimation | thus leading to a simple iterative mechanism . Most algorithms try to weight the points such that the algebraic solution comes closer to the geometric solution . Another idea is to favor non - eccentric ellipses . 7 . 1 Curvature weights The solution of x T Ax + b T x + c (cid:25) 0 in the least squares sense leads to an equation for each point . If the equation for point ( xi1 ; xi2 ) is multiplied by ! i > 1 , the solution will approximate this point more accurately . In [ 6 ] , ! i is set to 1 = Ri , where Ri is the curvature radius of the ellipse at a point pi associated with ( xi1 ; xi2 ) . The point pi is determined by intersecting the ray from the ellipse ' s center to ( xi1 ; xi2 ) and the ellipse . 12 - 10 - 5 0 5 10 15 20 25 30 35 40 - 25 - 20 - 15 - 10 - 5 0 5 10 15 20 Figure 6 . 1 : algebraic versus best (cid:12)t (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Best (cid:12)t (cid:0) (cid:0) (cid:0) (cid:0) Algebraic (cid:12)t ( u = 1 ) Tests on few data sets show , that this weighting scheme leads to better shaped ellipses in some cases , especially for eccentric ellipses ; but it does not systematically restrict the solutions to ellipses . Lets look at the curvature weight solution for two problems . Figure 7 . 1 shows the result for the data set ( 6 . 1 ) presented earlier : unluckily , the algorithm (cid:12)nds a hyperbola for the weighted equations in the (cid:12)rst step . On the other side , the algorithm is successful indeed for a data set close to an eccentric ellipse . Figure 7 . 2 shows the large solid ellipse ( residual norm 2 . 20 ) found by the curvature weights algorithm . The small dotted ellipse is the solution of the unweighted algebraic solution ( 6 . 77 ) ; the dashed ellipse is the best (cid:12)t solution using Gauss - Newton ( 1 . 66 ) , and the dash - dotted ellipse ( 1 . 69 ) is found by the geometric - weight algorithm described later . 7 . 2 Geometric distance weighting We are interested in weighting schemes which result in a least square solution for the geometric distance . If we de(cid:12)ne Q ( x ) = x T Ax + b T x + c ; then the simple algebraic method minimizes Q for the given points in the least squares sense . Q has the following geometric meaning : Let h ( x ) be the geometric distance from 13 - 10 - 5 0 5 10 15 20 25 30 35 40 - 25 - 20 - 15 - 10 - 5 0 5 10 15 20 Figure 7 . 1 : algebraic (cid:12)t with curvature weights (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Conic after (cid:12)rst curvature - weight step (cid:0) (cid:0) (cid:0) (cid:0) Unweighted algebraic (cid:12)t the center to point x h ( x ) = q ( x1 (cid:0) z1 ) 2 + ( x2 (cid:0) z2 ) 2 and determine pi by intersecting the ray from the ellipse ' s center to xi and the ellipse . Then , as pointed out in [ 9 ] Q ( xi ) = (cid:20) ( ( h ( xi ) = h ( pi ) ) 2 (cid:0) 1 ) ( 7 . 1 ) ' 2(cid:20) h ( xi ) (cid:0) h ( pi ) h ( pi ) , if xi ' pi ( 7 . 2 ) for some constant (cid:20) . This explains why the simple algebraic solution tends to neglect points far from the center . Thus , we may say that the algebraic solution (cid:12)ts the ellipse with respect to the relative distances , i . e . a distant point has not the same importance as a near point . If we prefer to minimize the absolute distances , we may solve a weighted problem with weights ! i = h ( pi ) for a given estimated ellipse . The resulting estimated ellipse may then be used to determine new weights ! i , thus iteratively solving weighted least squares problems . 14 0 5 10 15 20 0 2 4 6 8 10 12 14 16 18 20 Figure 7 . 2 : comparison of di(cid:11)erent (cid:12)ts (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Curvature weights solution (cid:0) (cid:0) (cid:0) (cid:0) Best (cid:12)t (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) Unweighted algebraic (cid:12)t (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) Geometric weights solution Consequently , we may go a step further and set weights so that the equations are solved in the least squares sense for the geometric distances . If d ( x ) is the geometric distance of x from the currently estimated ellipse , then weights are set ! i = d ( xi ) = Q ( xi ) : See the dash - dotted ellipse in (cid:12)gure 7 . 2 for an example . The advantage of this method compared to the non - linear method to compute the geometric (cid:12)t is , that no derivatives for the Jacobian or Hessian matrices are needed . The disadvantage of this method is , that it does not generally minimize the geometric distance . To show this , let us restate the problem : G ( x ) x 2 = min where x = 1 : ( 7 . 3 ) An iterative algorithm determines a sequence ( yi ) , where yk + 1 is the solution of G ( yk ) y 2 = min where y = 1 : ( 7 . 4 ) The sequence ( yi ) may have a (cid:12)xed point ~ y = y1 without solving ( 7 . 3 ) , since the conditions for critical points ~ x and ~ y are di(cid:11)erent for the two equations . To show this , 15 we shall use the notation dz for an in(cid:12)nitesimal change of z . For all dx with ~ x T dx = 0 the following holds 2 ( ~ x T G T dG ~ x + ~ x T G T Gdx ) = d G ~ x 2 = 0 for equation ( 7 . 3 ) . Whereas for equation ( 7 . 4 ) and ~ y T dy = 0 the condition is 2 ( ~ y T G T Gdy ) = d G ~ y 2 = 0 : This problem is common to all iterative algebraic solutions of this kind , so no matter how good the weights approximate the real geometric distances , we may not generally expect that the sequence of estimated ellipses converges to the optimal solution . We give a simple example for a (cid:12)xed point of the iteration scheme ( 7 . 4 ) , which does not solve ( 7 . 3 ) . For z = ( x ; y ) T consider G = 2 0 (cid:0)4y 4 ! ; then z0 = ( 1 ; 0 ) T is a (cid:12)xed point of ( 7 . 4 ) , but z = ( 0 : 7278 ; 0 : 6858 ) T is the solution of ( 7 . 3 ) . Another severe problem with iterative algebraic methods is the lack of convergence in the general case | especially if the problem is ill - conditioned . We will shortly examine the solution of ( 7 . 4 ) for small changes to G . Let G = U(cid:6)V T (cid:22) G = G + dG = (cid:22) U (cid:22) (cid:6) (cid:22) V T and denote with (cid:27)1 ; : : : ; (cid:27)n the singular values in descending order , with vi the associ - ated right singular vectors | where vn is the solution of equation ( 7 . 4 ) for G . Then we may bound dvn = (cid:22)vn (cid:0) vn as follows . First , we de(cid:12)ne (cid:21) = (cid:22) V T vn thus (cid:21) = 1 (cid:22) = 1 (cid:0) (cid:21)n 2 " = dG and note that (cid:27)i (cid:0) " (cid:20) (cid:22)(cid:27)i (cid:20) (cid:27)i + " for all i : We may conclude (cid:22) (cid:6)(cid:21) = (cid:22) U (cid:22) (cid:6)(cid:21) = (cid:22) Gvn and (cid:22) Gvn (cid:20) Gvn + dGvn (cid:20) (cid:27)n + " ; thus n X i = 1 (cid:21) 2 i (cid:22)(cid:27) 2 i (cid:20) (cid:27) 2 n + 2 " (cid:27)n + " 2 : 16 Using that (cid:22)(cid:27)i (cid:21) (cid:22)(cid:27)n(cid:0)1 for i (cid:20) n(cid:0)1 , and that (cid:21) = 1 , we simplify the above expression to ( 1 (cid:0) (cid:21) 2 n ) (cid:22)(cid:27) 2 n(cid:0)1 + (cid:21) 2 n (cid:22)(cid:27) 2 n (cid:20) (cid:27) 2 n + 2 " (cid:27)n + " 2 : ( 7 . 5 ) Assuming that (cid:27)n 6 = 0 ( otherwise the solution is exact ) and " (cid:20) (cid:27)n , we have (cid:27) 2 i (cid:0) 2 " (cid:27)i + " 2 (cid:20) (cid:22)(cid:27) 2 i : Applying this to inequality ( 7 . 5 ) , we get (cid:22) (cid:20) 4 " (cid:27)n (cid:22)(cid:27) 2 n(cid:0)1 (cid:0) (cid:22)(cid:27) 2 n : Note that vn (cid:0) (cid:22)vn 2 = (cid:21) (cid:0) ( 0 ; : : : ; 1 ) T 2 = ( (cid:21)n (cid:0) 1 ) 2 + ( 1 (cid:0) (cid:21) 2 n ) = 2 ( 1 (cid:0) (cid:21)n ) : Assuming that vn (cid:0) (cid:22)vn is small ( and thus (cid:21)n (cid:25) 1 , (cid:22) (cid:25) 0 ) , then we may write (cid:27) for (cid:22)(cid:27) and (cid:22) = ( 1 + (cid:21)n ) ( 1 (cid:0) (cid:21)n ) (cid:25) 2 ( 1 (cid:0) (cid:21)n ) ; thus we (cid:12)nally get vn (cid:0) (cid:22)vn (cid:20) s 4 " (cid:27)n (cid:27) 2 n(cid:0)1 (cid:0) (cid:27) 2 n (cid:20) s 2 " (cid:27) n(cid:0)1 (cid:0) (cid:27) n : This shows that the convergence behavior of iterative algebraic methods depends on how well | in a (cid:12)gurative interpretation | the solution vector ~ vn is separated from its hyper - plane with respect to the residual norm Gv . If ~ (cid:27)n(cid:0)1 (cid:25) ~ (cid:27)n , the solution is poorly determined , and the algorithm may not converge . While the analytical results are not encouraging , we obtained solutions close to the optimum using the geometric - weight algorithm for several examples . Figure 7 . 3 shows the geometric - weight ( solid ) and the best ( dashed ) solution for such an example . Note that the calculation of geometric distances is relatively expensive , so a pragmatic way to limit the cost is to perform a (cid:12)xed number of iterations , since convergence is not guaranteed anyway . Table 7 . 1 lists the points to be approximated , resulting in a residual norm of 2 . 784 , compared to 2 . 766 for the best geometric (cid:12)t . Since the algebraic method minimizes wiQi in the least squares sense , it remains to check that wiQi is proportional to the geometric distance di for the estimated conic . We compute Mi = ( wiQi ) = di ( 7 . 6 ) hi = 1 (cid:0) Mi = M 1 ( 7 . 7 ) and (cid:12)nd | as expected | that h = 1 : 1E(cid:0)5 . 7 . 3 Circle weight algorithm The main di(cid:14)culty with above algebraic methods is that the solution may be any conic | not necessarily an ellipse . To cope with this case , we extend the system by weighted equations , which favour circles and non - eccentric ellipses : ! ( a11 (cid:0) a22 ) (cid:25) 0 ( 7 . 8 ) ! 2a12 (cid:25) 0 : ( 7 . 9 ) 17 - 25 - 20 - 15 - 10 - 5 0 5 10 15 20 25 - 15 - 10 - 5 0 5 10 15 Figure 7 . 3 : geometric weight (cid:12)t vs . best (cid:12)t (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) Geometric weight solution (cid:0) (cid:0) (cid:0) (cid:0) Best (cid:12)t Note that these equations are Euclidean - invariant only if ! is the same in both equa - tions , and the problem is solved in the least squares sense . What we are really mini - mizing in this case is ! 2 ( ( a11 (cid:0) a22 ) 2 + 4a 2 12 ) = ! 2 ( (cid:21)1 (cid:0) (cid:21)2 ) 2 ; hence , the constraints ( 7 . 8 ) and ( 7 . 9 ) are equivalent to the equation ! ( (cid:21)1 (cid:0) (cid:21)2 ) (cid:25) 0 : The weight ! is (cid:12)xed by ! = (cid:15)f ( ( (cid:21)1 (cid:0) (cid:21)2 ) 2 ) ; ( 7 . 10 ) where (cid:15) is a parameter representing the badness of eccentric ellipses , and f : [ 0 ; 1 [ ! [ 0 ; 1 [ continuous , strictly increasing : The larger (cid:15) is chosen , the larger will ! be , and thus the more important are equa - tions ( 7 . 8 { 7 . 9 ) , which make the solution be more circle - shaped . The parameter ! is determined iteratively ( starting with ! 0 = 0 ) , where following conditions hold 0 = ! 0 (cid:20) ! 2 (cid:20) : : : (cid:20) ! (cid:20) : : : (cid:20) ! 3 (cid:20) ! 1 : ( 7 . 11 ) 18 xi yi 2 : 0143 10 : 5575 17 : 3465 3 : 2690 (cid:0)8 : 5257 (cid:0)7 : 2959 (cid:0)7 : 9109 (cid:0)7 : 6447 16 : 3705 (cid:0)3 : 8815 (cid:0)15 : 3434 5 : 0513 (cid:0)21 : 5840 (cid:0)0 : 6013 9 : 4111 (cid:0)9 : 0697 Table 7 . 1 : Given points close to ellipse Thus , the larger the weight ! , the less eccentric the ellipse ; we prove this in the follow - ing . Given weights (cid:31) < ! and F = 1 0 (cid:0)1 0 0 0 0 1 0 0 0 0 ! ; then we (cid:12)nd solutions x and z for the equations (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) B (cid:31)F ! x (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = min (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) B ! F ! z (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = min respectively . It follows that Bx 2 + (cid:31) 2 Fx 2 (cid:20) Bz 2 + (cid:31) 2 Fz 2 Bz 2 + ! 2 Fz 2 (cid:20) Bx 2 + ! 2 Fx 2 and by adding (cid:31) 2 Fx 2 + ! 2 Fz 2 (cid:20) (cid:31) 2 Fz 2 + ! 2 Fx 2 and since ! 2 (cid:0) (cid:31) 2 > 0 Fz 2 (cid:20) Fx 2 ; so that (cid:22) ! = " f ( Fz 2 ) (cid:20) " f ( Fx 2 ) = (cid:22)(cid:31) : This completes the proof , since f was chosen strictly increasing . One obvious choice for f is the identity function , which was used in our test programs . 8 Comparison of geometric algorithms The discussion of algorithms minimizing the geometric distance is somewhat di(cid:11)erent from the algebraic distance problems . The problems in the latter are primarily stabil - ity and \ good - looking " solutions ; the former must be viewed by their e(cid:14)ciency , too . 19 Generally , the simple algebraic solution is orders of magnitude cheaper than the geo - metric counterparts ( for accurate results about a factor 10 { 100 ) ; thus iterative algebraic methods are a valuable alternative . But a comparison between algebraic and geometric algorithms would not be very enlightening | because there is no objective criterion to decide which estimate is better . However , we may compare di(cid:11)erent nonlinear least square algorithms to compute the geometric (cid:12)t with respect to stability and e(cid:14)ciency . 8 . 1 Algorithms Several known nonlinear least square algorithms have been implemented : 1 . Gauss - Newton ( gauss ) 2 . Newton ( newton ) 3 . Gauss - Newton with Marquardt modi(cid:12)cation ( marq ) 4 . Variable projection ( varpro ) 5 . Orthogonal distance regression ( odr ) The odr algorithm ( see [ 13 ] , [ 14 ] ) solves the implicit minimization problem f ( xi + (cid:14)i ; (cid:12) ) = 0 X i (cid:14)i 2 = min where f ( x ; (cid:12) ) = (cid:12)3 ( x1 (cid:0) (cid:12)1 ) 2 + 2(cid:12)4 ( x1 (cid:0) (cid:12)1 ) ( x2 (cid:0) (cid:12)2 ) + (cid:12)5 ( x2 (cid:0) (cid:12)2 ) 2 (cid:0) 1 : Whereas the gauss , newton , marq and varpro algorithms solve the problem Q ( x ; ' 1 ; ' 2 ; : : : ; ' m ; z1 ; z2 ; r ) = m X i = 1 h ( xi1 (cid:0) x ( ' i ) ) 2 + ( xi2 (cid:0) y ( ' i ) ) 2 i = min : For a (cid:25) b , the Jacobian matrix is nearly singular , so the gauss and newton algorithms are modi(cid:12)ed to apply Marquardt steps in this case . Not surprising , this modi(cid:12)cation makes all algorithms behave similar with respect to stability if the initial parameters are accurate and the problem is well posed . 8 . 2 Results To appreciate the results given in table 8 . 1 , it must be said that the varpro algorithm is written for any separable functional and cannot take pro(cid:12)t from the sparsity of the Jacobian . The algorithms were tested with example data | each consisting of 8 points | for following problems 1 . Special set of points ( 6 . 1 ) 2 . Uniformly distributed data in a square 20 3 . Points on a circle 4 . Points on an ellipse with a = b = 2 5 . Points on hyperbola branch gauss newton marq varpro odr Special 146 85 468 1146 (cid:5) Random (cid:5) (cid:5) (cid:5) 2427 (cid:5) Circle 22 22 22 36 7 Circle + 86 67 189 717 69 Ellipse 30 37 67 143 41 Ellipse + 186 (cid:5) 633 1977 103 Hyperbola 22 22 22 36 10 Hyperbola + (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) Table 8 . 1 : Geometric (cid:12)t with initial parameters of algebraic circle # (cid:13)ops / 1000 , minimum is underlined ` (cid:5) ' if non - convergence The tests with points on a conic were done both with and without perturbations . For table 8 . 1 , the initial parameters were derived from the algebraically best (cid:12)tting circle ( radius ra , center za ) ; initial center z0 = za , axes a0 = ra , b0 = ra and (cid:11)0 = 0 . Note that these initial values are somewhat rudimentary , so they serve to check the algorithms ' stability , too . Table 8 . 1 shows the number of (cid:13)ops ( in 1000 ) for the respective algorithm and problem ; the smallest number is underlined . If the algorithm didn ' t terminate after 100 steps , it was assumed non - convergent and a ` (cid:5) ' is shown instead . Table 8 . 2 contains the results if the initial parameters were obtained from the Bookstein algorithm . Table 8 . 2 shows that all algorithms converge quickly with the more accurate initial data for exact conics . For the perturbed ellipse data , it ' s primarily the newton algorithm which pro(cid:12)ts from the starting values close to the solution . Note further that the newton algorithm does not (cid:12)nd the correct solution for the special data , since the algebraic estimation | which serves as initial approximation | is completely di(cid:11)erent from the geometric solution . General conclusions from this ( admittedly ) small test series are (cid:15) All algorithms are prohibitively expensive compared to the simple algebraic so - lution ( factor 10 { 100 ) . (cid:15) If the problem is well posed , and the accuracy of the result should be high , the newton method applied to the parameterized algorithm is the most e(cid:14)cient . 21 gauss newton marq varpro odr Special 165 896 566 1506 (cid:5) Random (cid:5) (cid:5) (cid:5) 2819 (cid:5) Circle 32 32 32 102 7 Circle + 76 63 145 574 66 Ellipse 22 22 22 112 7 Ellipse + 161 40 435 1870 74 Hyperbola (cid:5) (cid:5) (cid:5) 1747 (cid:5) Hyperbola + (cid:5) (cid:5) (cid:5) 2986 (cid:5) Table 8 . 2 : Geometric (cid:12)t with initial parameters of algebraic ellipse # (cid:13)ops / 1000 , minimum is underlined ` (cid:5) ' if non - convergence (cid:15) The odr algorithm | although a simple general - purpose optimizing scheme | is competitive with algorithms speci(cid:12)cally written for the ellipse (cid:12)tting problem . If one takes into consideration further , that we didn ' t use a highly optimized odr procedure , the method of solution is surprisingly simple and e(cid:14)cient . (cid:15) The varpro algorithm seems to be the most expensive . Reasons for its ine(cid:14)ciency are that most parameters are non - linear and that the algorithm does not make use of the special matrix structure for this problem . The programs on which these experiments have been based are given in [ 17 ] . The report [ 17 ] and the Matlab sources for the examples are available via anonymous ftp from ftp . inf . ethz . ch . REFERENCES 1 . Vaughan Pratt , Direct Least Squares Fitting of Algebraic Surfaces , ACM J . Com - puter Graphics , Volume 21 , Number 4 , July 1987 2 . M . G . Cox , A . B . Forbes , Strategies for Testing Assessment Software , NPL Report DITC 211 / 92 3 . G . Golub , Ch . Van Loan , Matrix Computations ( second ed . ) , The Johns Hopkins University Press , Baltimore , 1989 4 . D . Sourlier , A . Bucher , Normgerechter Best - (cid:12)t - Algorithmus f(cid:127)ur Freiform - Fl(cid:127)achen oder andere nicht - regul(cid:127)are Ausgleichs - Fl(cid:127)achen in Parameter - Form , Tech - nisches Messen 59 , ( 1992 ) , pp . 293 { 302 5 . H . Sp(cid:127)ath , Orthogonal Least Squares Fitting with Linear Manifolds , Numer . Math . , 48 : 441 { 445 , 1986 . 22 6 . Lyle B . Smith , The use of man - machine interaction in data (cid:12)tting problems , TR No . CS 131 , Stanford University , March 1969 ( thesis ) 7 . Y . Vladimirovich Linnik , Method of least squares and principles of the theory of observations , New York , Pergamon Press , 1961 8 . Charles L . Lawson , Contributions to the Theory of Linear Least Maximum Ap - proximation , Dissertation University of California , Los Angeles , 1961 9 . Fred L . Bookstein , Fitting Conic Sections to Scattered Data , Computer Graphics and Image Processing 9 , ( 1979 ) , pp . 56 { 71 10 . G . H . Golub , V . Pereyra , The Di(cid:11)erentiation of Pseudo - Inverses and Nonlinear Least Squares Problems whose Variables Separate , SIAM J . Numer . Anal . 10 , No . 2 , ( 1973 ) ; pp . 413 { 432 11 . M . Heidari , P . C . Heigold , Determination of Hydraulic Conductivity Tensor Using a Nonlinear Least Squares Estimator , Water Resources Bulletin , June 1993 , pp . 415 { 424 12 . W . Gander , U . von Matt , Some Least Squares Problems , Solving Problems in Scienti(cid:12)c Computing Using Maple and Matlab , W . Gander and J . H(cid:20)reb(cid:19)(cid:16)(cid:20)cek ed . , 251 - 266 , Springer - Verlag , 1993 . 13 . Paul T . Boggs , Richard H . Byrd and Robert B . Schnabel , A stable and e(cid:14)cient algorithm for nonlinear orthogonal distance regression , SIAM Journal Sci . and Stat . Computing 8 ( 6 ) : 1052 { 1078 , November 1987 . 14 . Paul T . Boggs , Richard H . Byrd , Janet E . Rogers and Robert B . Schn - abel , User ' s Reference Guide for ODRPACK Version 2 . 01 | Software for Weighted Orthogonal Distance Regression , National Institute of Standards and Technology , Gaithersburg , June 1992 . 15 . P . E . Gill , W . Murray and M . H . Wright , Practical Optimization , Academic Press , New York , 1981 . 16 . Gene H . Golub , Alan Hoffmann , G . W . Stewart , A Generalization of the Eckart - Young - Mirsky Matrix Approximation Theorem , Linear Algebra and its Appl . 88 / 89 : 317 { 327 ( 1987 ) 17 . Walter Gander , Gene H . Golub , and Rolf Strebel , Fitting of circles and ellipses | least squares solution , Technical Report 217 , Institut f(cid:127)ur Wissenschaftlich - es Rechnen , ETH Z(cid:127)urich , June 1994 . Available via anonymous ftp from ftp . inf . ethz . ch as doc / tech - reports / 1994 / 217 . ps .