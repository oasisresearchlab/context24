Federated Learning with Nesterov Accelerated Gradient Momentum Method Zhengjie Yang , Wei Bao , Dong Yuan , Nguyen H . Tran , and Albert Y . Zomaya Faculty of Engineering , The University of Sydney Abstract —Federated learning ( FL ) is a fast - developing tech - nique that allows multiple workers to train a global model based on a distributed dataset . Conventional FL employs gradient descent algorithm , which may not be efﬁcient enough . It is well known that Nesterov Accelerated Gradient ( NAG ) is more advantageous in centralized training environment , but it is not clear how to quantify the beneﬁts of NAG in FL so far . In this work , we focus on a version of FL based on NAG ( FedNAG ) and provide a detailed convergence analysis . The result is compared with conventional FL based on gradient descent . One interesting conclusion is that as long as the learning step size is sufﬁciently small , FedNAG outperforms FedAvg . Extensive exper - iments based on real - world datasets are conducted , verifying our conclusions and conﬁrming the better convergence performance of FedNAG . Index Terms —Distributed machine learning , Federated learn - ing , Edge computing , Momentum gradient descent , Nesterov accelerated gradient I . I NTRODUCTION Recent advances in edge computing drastically motives Federated Learning ( FL ) . Since tremendous date are now gen - erated at the network edge , conventional centralized machine learning is insufﬁcient when a large volume and sensitive data are required to be uploaded to datacenters . Meanwhile , the development of hardware of edge devices ( i . e . , workers ) im - proves their computing capability , allowing more complicated computing operations at edge devices . Conventional Federated Learning is based on gradient de - scent [ 1 ] at each edge device ( worker ) : In each round , each worker locally updates its weights by gradient descent for a number of times by its local dataset , and then the aggregator averages the weights from all workers and distribute them to the workers again . The above process is repeated for multiple rounds . Such implementation is referred to as FedAvg [ 2 ] . However , one disadvantage of gradient descent is its low efﬁciency and potential in oscillations . Momentum is able to improve the situation by adding inertia to accelerate the convergence by dampening oscillations and causing the algorithm to barrel through narrow valleys , small humps , and local minima [ 3 ] – [ 8 ] . In this paper , we focus on the convergence analysis of FL where each worker updates its weights based on Nesterov Accelerated Gradient ( NAG ) [ 9 ] instead of gradient descent . NAG is known to be an advantageous form of momentum , compared with Polyaks momentum [ 4 ] and gradient descent in centralized machine learning . Some existing papers [ 10 ] – [ 13 ] focused on the convergence analysis based on gradient descent in FL environment . Some papers [ 14 ] – [ 17 ] analyzed NAG in centralized learning environment . Some other papers [ 18 ] , [ 19 ] studied applications of Polyak’s momentum in FL environment . However , no prior work has rigorously analyzed convergence of NAG in the FL environment . It is also not clear how to quantify the performance gap between gradient descent and NAG in the FL environment . In this paper , we focus on a version of FL based on NAG , namely FedNAG : ( 1 ) Each worker locally updates its weights and momenta using NAG for τ iterations on its local dataset ; ( 2 ) the aggregator collects and averages the weights and momenta from all workers and distribute them to the workers again ; ( 1 ) and ( 2 ) are repeated for multiple rounds . τ is a factor to trade off local update and global aggregation . Larger τ reduces the frequency of aggregations and thus reduces communication overhead , but it also lowers the efﬁciency of local updates , causing worse convergence performance . We theoretically provide a detailed convergence analysis for FedNAG . The progress mainly includes three steps : ( 1 ) We deﬁne virtual update as if centralized NAG is conducted between two global aggregations ; ( 2 ) We bound the gap of weights w between FedNAG update and virtual update ; and ( 3 ) We bound the values of global loss functions F ( w ) between FedNAG and the optimal solution . Since the conver - gence analysis of FedAvg is provided in [ 13 ] , we compare the convergence performance of FedNAG and FedAvg and derive the conditions that FedNAG outperforms FedAvg . Experimentally , we use different models such as linear regression , logistic regression , and convolutional neural net - work ( CNN ) based on MNSIT and CIFAR - 10 detasets , to test the performance of FedNAG . We analyze the impact of different factors such as number of workers , number of local updates between two global aggregataion τ , and momentum coefﬁcient . The experiment shows that FedNAG outperforms FedAvg under a wide range of settings . II . S YSTEM M ODEL AND P RELIMINARIES A . Overview In the context of federated edge learning , there are N workers , located at different sites and communicating with an aggregator to learn a model w ∗ which is a solution to the following problem min w ∈ R d F ( w ) (cid:44) (cid:80) Ni = 1 D i F i ( w ) D , ( 1 ) a r X i v : 2009 . 08716v1 [ c s . L G ] 18 S e p 2020 TABLE I N OTATION SUMMARY i index of worker t index of update iteration k index of interval N number of workers T number of total local iterations K number of global aggregations D i number of samples for local dataset i D total number of samples η learning step size hyper parameter γ momentum hyper parameter τ number of local update steps between two global aggregations F ( w ) global loss function F i ( w ) local loss function for worker i w f practical model parameter that the learning process can obtain w ∗ theoretical model parameter that minimizes F ( w ) w ( t ) global model parameter at iteration t w i ( t ) local model parameter at iteration t for worker i v ( t ) global momentum parameter at iteration t v i ( t ) local momentum parameter at iteration t for worker i w [ k ] ( t ) model parameter for centralized NAG at iteration t in interval [ k ] v [ k ] ( t ) momentum parameter for centralized NAG at iteration t in interval [ k ] where D i is the number of data samples in worker i ; D = (cid:80) Ni = 1 D i is the total number of data samples ; and d is the dimension of w . F i ( · ) is the local loss function at worker i and F ( · ) is the global loss function . We assume F i ( · ) satisﬁes the following conditions . 1 ) F i ( w ) is convex . 2 ) F i ( w ) is ρ - Lipschitz , i . e . , (cid:107) F i ( w 1 ) − F i ( w 2 ) (cid:107) ≤ ρ (cid:107) w 1 − w 2 (cid:107) for any w 1 , w 2 . 3 ) F i ( w ) is β - smooth , i . e . , (cid:107)∇ F i ( w 1 ) − ∇ F i ( w 2 ) (cid:107) ≤ β (cid:107) w 1 − w 2 (cid:107) for any w 1 , w 2 . The above assumptions are widely adopted in a range of literature [ 13 ] , [ 19 ] – [ 21 ] . B . Algorithm Algorithm 1 : FedNAG Input : τ , T = Kτ Output : Final model parameter w f 1 Initialize v i ( 0 ) = 0 , and w i ( 0 ) as same value for all i 2 for t = 1 , 2 , . . . , T do 3 For each worker i in parallel , compute its local update as ( 2 ) and ( 3 ) . 4 if t = = kτ where k is a positive integer then 5 Aggregate v ( t ) and w ( t ) as ( 4 ) and ( 5 ) . 6 Set v i ( t ) ← v ( t ) and w i ( t ) ← w ( t ) for all i 7 end 8 end 9 Set w f as ( 6 ) . Algorithm 1 shows NAG in Federated Learning . We use w i ( t ) and v i ( t ) to denote the model parameter and momentum parameter in worker i at t th iteration . Initially , at t = 0 , we set v i ( 0 ) = 0 and a same w i ( 0 ) for all i . Each τ iterations will lead to a global aggregation . Fig . 1 . Illustration of w i ( t ) and v i ( t ) in local update step Each iteration includes a local update , followed by a global aggregation if t = kτ , k = 1 , 2 , . . . . 1 ) Local Updates : In each iteration , the following update is conducted in each worker i , v i ( t ) ← γ v i ( t − 1 ) − η ∇ F i ( w i ( t − 1 ) ) , ( 2 ) w i ( t ) ← w i ( t − 1 ) − γ v i ( t − 1 ) + ( 1 + γ ) v i ( t ) = w i ( t − 1 ) + γ v i ( t ) − η ∇ F ( w i ( t − 1 ) ) . ( 3 ) Fig . 1 gives us a graphic view of local update and red line shows the illustration of w i ( t ) . v i ( t ) are momentum terms . The above updates follow [ 22 ] , [ 23 ] . 2 ) Global Aggregation : If t = kτ , k = 1 , 2 , . . . , all workers will send v i ( t ) and w i ( t ) values to the aggregator and the aggregator calculates v ( t ) and w ( t ) as follows : v ( t ) ← (cid:80) Ni = 1 D i v i ( t ) D , ( 4 ) w ( t ) ← (cid:80) Ni = 1 D i w i ( t ) D . ( 5 ) Then aggregator will send back v ( t ) and w ( t ) to each worker i to update v i ( t ) ← v ( t ) and w i ( t ) ← w ( t ) . Note that only if t = kτ , v ( t ) and w ( t ) are aggregated in ( 4 ) and ( 5 ) . For the purpose of analysis , we deﬁne v ( t ) = (cid:80) Ni = 1 D i v i ( t ) D and w ( t ) = (cid:80) Ni = 1 D i w i ( t ) D at any iteration t so that v ( t ) and w ( t ) can be used for convergence analysis . After T = Kτ iterations , the output w f is computed as follows : w f (cid:44) arg min w ∈ { w ( kτ ) : k = 1 , 2 , . . . , K } F ( w ) . ( 6 ) C . Preliminary Analysis We present some simple preliminary analyses , which will be used in the rest of the paper . We also list important notations in Table I . 1 ) Property of F ( w ) : First , according to the assumptions , it is straightforward to show that F ( w ) is convex , ρ - Lipschitz and β - smooth by applying triangle inequalities . Interval Interval Fig . 2 . Illustration of w ( t ) , when N = 2 , τ = 2 2 ) Divergence of Gradient : The divergence of gradient , which is commonly adopted in convergence analysis [ 13 ] , [ 19 ] , [ 21 ] can be deﬁned as follows . Deﬁnition 1 . ( Gradient Divergence ) For ∀ i and ∀ w , we deﬁne δ i as the upper bound between ∇ F i ( w ) and ∇ F ( w ) , i . e . , (cid:107)∇ F i ( w ) − ∇ F ( w ) (cid:107) ≤ δ i . ( 7 ) We also deﬁne δ (cid:44) (cid:80) i D i δ i D . ( 8 ) Please note that δ i is different at different workers , indicat - ing the datasets at different workers may not be independent and identically distributed [ 13 ] . 3 ) Virtual Updates : We use [ k ] to denote interval t ∈ [ ( k − 1 ) τ , kτ ] for k = 1 , 2 , 3 , . . . , K . It shows τ iterations within two global aggregations . In each interval [ k ] , ﬁrst , at ( k − 1 ) τ , we set v [ k ] ( ( k − 1 ) τ ) ← v ( ( k − 1 ) τ ) , ( 9 ) w [ k ] ( ( k − 1 ) τ ) ← w ( ( k − 1 ) τ ) . ( 10 ) v [ k ] ( ( k − 1 ) τ ) and w [ k ] ( ( k − 1 ) τ are set as the aggregated values right after the global aggregation is conducted . Second , starting from the aggregated values , we consider virtual updates as if centralized NAG is adopted . In iterations ( k − 1 ) τ < t ≤ kτ , we conduct v [ k ] ( t ) ← γ v [ k ] ( t − 1 ) − η ∇ F ( w [ k ] ( t − 1 ) ) , ( 11 ) w [ k ] ( t ) ← w [ k ] ( t − 1 ) − γ v [ k ] ( t − 1 ) + ( 1 + γ ) v [ k ] ( t ) = w [ k ] ( t − 1 ) + γ v [ k ] ( t ) − η ∇ F ( w [ k ] ( t − 1 ) ) . ( 12 ) We repeat the above process for each [ k ] . These w [ k ] ( t ) and v [ k ] ( t ) are virtual values assuming there is a centralized update . They are used to bound the gap to prove the con - vergence shortly . Please note that w [ k ] ( kτ ) and w [ k + 1 ] ( kτ ) are different . w [ k ] ( kτ ) is calculated from w [ k ] ( ( k − 1 ) τ ) after τ iterations of centralized update , and w [ k + 1 ] ( kτ ) is directly given by w ( kτ ) . Figs . 2 and 3 illustrate the evolution of w [ k ] ( kτ ) and F ( w [ k ] ( kτ ) ) respectively . L o ss f un c ti on v a l u e . . . . . . Interval Interval Fig . 3 . Illustration of F ( w ( t ) ) III . C ONVERGENCE A NALYSIS OF F ED NAG In this section , we provide detailed convergence analysis of FedNAG . This includes two steps : We ﬁrst bound the gap of the weight w between FedNAG and virtual updates ; Then we bound the loss function F ( w ) between FedNAG and the optimal solution . A . Bounding (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) We ﬁrstly analyze the upper bound between w ( t ) and w [ k ] ( t ) , leading to the following theorem . Theorem 1 . For any interval [ k ] , ∀ t ∈ [ k ] , we have : (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) ≤ h ( t − ( k − 1 ) τ ) , ( 13 ) where we deﬁne A (cid:44) ( 1 + ηβ ) ( 1 + γ ) + (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , B (cid:44) ( 1 + ηβ ) ( 1 + γ ) − (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , E (cid:44) γA + A − 1 ( A − B ) ( γA − 1 ) , F (cid:44) γB + B − 1 ( A − B ) ( 1 − γB ) , and h ( x ) yields h ( x ) = ηδ (cid:20) E ( γA ) x + F ( γB ) x − 1 ηβ − γ 2 ( γ x − 1 ) − ( γ − 1 ) x ( γ − 1 ) 2 (cid:21) ( 14 ) for 0 < γ < 1 and any x = 0 , 1 , 2 , . . . We note that F ( w ) is ρ - Lipschitz , so we also have : F ( w ( t ) ) − F ( w [ k ] ( t ) ) ≤ ρh ( t − ( k − 1 ) τ ) . ( 15 ) Proof . See Appendix for detailed proof . We have the following observations on Theorem 1 . 1 (cid:13) Monotone of h ( x ) . h ( 0 ) = h ( 1 ) = 0 and h ( x ) increases with respect to integer x for x ≥ 1 . See Appendix for detailed proof . 2 (cid:13) Property of h ( 0 ) . When x = 0 , we have t = ( k − 1 ) τ ( the beginning of interval [ k ] ) and the upper bound in ( 13 ) is 0 . This is consistent with ( 9 ) and ( 10 ) for any k . 3 (cid:13) Property of h ( 1 ) . When x = 1 , we have t = ( k − 1 ) τ + 1 ( the beginning of second iteration of interval [ k ] ) and the upper bound in ( 13 ) is still zero . It is easy to verify that if all workers conduct global aggregation right after the end of the ﬁrst local iteration , there is no gap between FedNAG and centralized NAG . 4 (cid:13) Property of τ = 1 . When τ = 1 , we have t − ( k − 1 ) τ = 0 or 1 . Thus , for any interval k and t ∈ [ k ] , the gap in ( 13 ) and ( 15 ) is always zero . This means that FedNAG is equivalent to centralized NAG when there is only one local update step between two global aggregation steps . See Appendix for detailed discussion . 5 (cid:13) Property of τ > 1 . When τ > 1 , because t ∈ [ ( k − 1 ) τ , kτ ] , we have x = t − ( k − 1 ) τ ∈ [ 0 , τ ] . Thus , the value of x could be larger when τ is large . According to the deﬁnitions of A , B , E , and F , we can see that γA > 1 , 0 < γB < 1 , E > 0 , F > 0 . When x is large , because 0 < γ < 1 , the last term in ( 14 ) will linearly decrease with respect to x . Therefore , for ( 14 ) , E ( γA ) x dominates when x is large . It means that the upper bound in ( 13 ) will be exponentially increased with t ∈ [ k ] . 6 (cid:13) Impact of δ . h ( x ) increases linearly with respect to δ . The value of δ reﬂects the difference of data distribution in each worker . Larger divergence of data distribution leads to larger gap of h ( x ) . B . Bounding F ( w ( T ) ) − F ( w ∗ ) For convenience , we use θ [ k ] ( t ) to denote the angle between vector −∇ F ( w [ k ] ( t ) ) and v [ k ] ( t ) for t ∈ [ k ] , cos θ [ k ] ( t ) (cid:44) −∇ F (cid:0) w [ k ] ( t ) (cid:1) T v [ k ] ( t ) (cid:13)(cid:13) ∇ F (cid:0) w [ k ] ( t ) (cid:107)(cid:107) v [ k ] ( t ) (cid:107) . θ is deﬁned as the maximum value of θ [ k ] ( t ) for k ∈ [ 1 , K ] with t ∈ [ k ] θ (cid:44) max k ∈ [ 1 , K ] , t ∈ [ k ] θ [ k ] ( t ) . Then we also deﬁne p (cid:44) max k ∈ [ 1 , K ] , t ∈ [ k ] (cid:13)(cid:13) γ v [ k ] ( t ) (cid:13)(cid:13) (cid:13)(cid:13) η ∇ F (cid:0) w [ k ] ( t ) (cid:1)(cid:13)(cid:13) , q (cid:44) min k ∈ [ 1 , K ] , t ∈ ( ( k − 1 ) τ , kτ ] (cid:13)(cid:13) ∇ F (cid:0) w [ k ] ( t − 1 ) (cid:1)(cid:13)(cid:13) (cid:13) (cid:13) ∇ F (cid:0) w [ k ] ( t ) (cid:1)(cid:13) (cid:13) , ω (cid:44) min k ∈ [ 1 , K ] , t ∈ [ k ] 1 (cid:13)(cid:13) w [ k ] ( t ) − w ∗ (cid:13)(cid:13) 2 . We can obtain the following theorem to get the upper bound as follows . Theorem 2 . When all the following conditions are satisﬁed : 1 ) cos θ ≥ 0 , 0 < βη ( γ + 1 ) ≤ 1 and 0 ≤ γ < 1 , 2 ) ωα − ρh ( τ ) τε 2 > 0 , 3 ) F ( w [ k ] ( kτ ) ) − F ( w ∗ ) ≥ ε for all k , 4 ) F ( w ( T ) ) − F ( w ∗ ) ≥ ε , for some ε > 0 , the convergence upper bound of Algorithm 1 after T iterations is given by F ( w ( T ) ) − F ( w ∗ ) ≤ 1 T (cid:16) ωα − ρh ( τ ) τε 2 (cid:17) , ( 16 ) where we deﬁne α (cid:44) η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) − βη 2 γ 2 p 2 2 + γ 2 ηq ( 1 − βη ( γ + 1 ) ) cos θ . Proof . See Appendix for detailed proof . Through Theorem 2 , we can further obtain the following bound between F (cid:0) w f (cid:1) and F ( w ∗ ) . Theorem 3 . When cos θ ≥ 0 , 0 < βη ( γ + 1 ) ≤ 1 , and 0 ≤ γ < 1 , we have F ( w f ) − F ( w ∗ ) ≤ 1 2 Tωα + (cid:114) 1 4 T 2 ω 2 α 2 + ρh ( τ ) ωατ + ρh ( τ ) . ( 17 ) Proof . See Appendix for detailed proof . Please note we have the following observations on Theo - rem 3 . 1 (cid:13) Effect of τ . From Appendix , we have known that h ( τ ) ≥ 0 and increases with integer τ . Thus , for a given T , the conver - gence upper bound becomes larger when τ is larger . 2 (cid:13) Property of τ = 1 . When τ = 1 , we have h ( τ ) = 0 . We can observe that the gap converges to zero when T → ∞ . This means if we conduct global aggregation after every local update , F ( w ( t ) ) will converge to the optimal solution . 3 (cid:13) Property of τ > 1 . When τ > 1 , we have h ( τ ) > 0 . We can observe that the gap converges to a non - zero gap (cid:113) ρh ( τ ) ωατ + ρh ( τ ) when T → ∞ . This means if we conduct global aggregation after multiple local updates , there is a non - zero gap to the optimal solution . 4 (cid:13) Tradeoff between communication and convergence . Based on the Observations 2 (cid:13) and 3 (cid:13) above , τ = 1 gives the best convergence performance . However , by doing so , it will increase the communication frequency . This will lead to a tradeoff between communication overhead and convergence performance . In this paper , we do not model the costs and utilities of communication overhead ( in different types of distributed systems ) and convergence performance , so that the optimal tradeoff is left for future work . 5 (cid:13) Effect of δ . Following the Observation 6 (cid:13) of Theorem 1 , the convergence upper bound will be increased when δ is getting larger . IV . C OMPARISON BETWEEN F ED A VG AND F ED NAG In this section , we compare the performance between Fe - dAvg and FedNAG . The convergence upper bound of FedAvg has been derived in Theorem 2 in [ 13 ] as follows : F (cid:0) ˆ w f (cid:1) − F ( w ∗ ) ≤ 1 2 Tω ˆ α + (cid:115) 1 4 T 2 ω 2 ˆ α 2 + ρ ˆ h ( τ ) ω ˆ ατ + ρ ˆ h ( τ ) , ( 18 ) where ˆ h ( τ ) = δ β ( ( ηβ + 1 ) τ − 1 ) − ηδτ , ( 19 ) ˆ α (cid:44) η (cid:18) 1 − βη 2 (cid:19) . Please note that ρ , β , τ , ω , and η are deﬁned the same way as those in FedNAG in this paper . ˆ α and ˆ h ( · ) are deﬁned differently , but with similar meanings as α and h ( · ) in this paper . In order to make a fair comparison , we let FedAvg and FedNAG trained under the same environment using the same conﬁguration . Here , we note that δ and ω reﬂect the properties of data distribution . We assume the dataset is distributed in each worker in the same way in FedAvg and FedNAG , so that the values of ω and δ are same . The loss function F i ( · ) , F ( · ) , constants ρ and β , and hyper - parameters τ and η are the same . We also set the same initial value for w f , w i ( 0 ) for FedAvg and FedNAG . The only new term in FedNAG is v i ( t ) , and we set v i ( 0 ) = 0 . We use f 1 ( T ) and f 2 ( T ) to deﬁne the convergence upper bound of FedNAG and FedAvg respectively . Small function value implies better convergence performance . f 1 ( T ) (cid:44) 1 2 Tωα + (cid:114) 1 4 T 2 ω 2 α 2 + ρh ( τ ) ωατ + ρh ( τ ) , ( 20 ) f 2 ( T ) (cid:44) 1 2 Tω ˆ α + (cid:115) 1 4 T 2 ω 2 ˆ α 2 + ρ ˆ h ( τ ) ω ˆ ατ + ρ ˆ h ( τ ) . ( 21 ) To prevent the gradient descent from overshooting the minimum or failing to converge [ 24 ] , we choose a sufﬁciently small η to guarantee the convergence of FedNAG and FedAvg . The following conclusion is made when η → 0 + . Theorem 4 . When cos θ ≥ 0 , 0 < βη ( γ + 1 ) ≤ 1 and 0 < γ < 1 , FedNAG outperforms FedAvg , i . e . , f 1 ( T ) < f 2 ( T ) for any T and an arbitrarily small η → 0 + . Proof . See Appendix for detailed discussion . Please note we have the following observations on Theo - rem 4 . 1 (cid:13) Simpliﬁed conditions . As mentioned in previous section , θ is the maximum value of angle between descent direction and momentum ( velocity ) direction . From Appendix , cos θ ≥ 0 is always true if η → 0 + ( since v i ( 0 ) = 0 ) . Also , 0 < βη ( γ + 1 ) ≤ 1 is true when η → 0 + as β is a positive ﬁnite number . Therefore , as long as η → 0 + and 0 < γ < 1 , FedNAG convergence performance is better than FedAvg . 2 (cid:13) Discussion of η . In Theorem 4 , we set η → 0 + . Actually , there exists a threshold value for η called ¯ η . If η < ¯ η , cos θ ≥ 0 , 0 < βη ( γ + 1 ) ≤ 1 , and 0 < γ < 1 , then f 1 ( T ) < f 2 ( T ) is still true . Numerical method can be used to calculate the value of ¯ η . V . E XPERIMENTS In this section we evaluate the convergence performance of FedNAG compared with benchmark algorithms including Fe - dAvg , centralized SGD ( cSGD ) and centralized NAG ( cNAG ) by real - world experiments . We then discuss the impacts of hyper - parameters , including global aggregation frequency τ , momentum coefﬁcient γ , and number of workers N . A . Experimental Setup In order to evaluate the convergence performance of Fed - NAG , we employ two real - world datasets including “MNIST” [ 25 ] and “CIFAR - 10” [ 26 ] for image classiﬁcation . While MNIST dataset contains gray - scale images of 70 , 000 samples ( 60 , 000 for training and 10 , 000 for testing ) , CIFAR - 10 con - tains 60 , 000 color images ( 50 , 000 for training and 10 , 000 for testing ) . In our experiment , all samples in MNIST and CIFAR - 10 are evenly distributed in each worker . We implement FedNAG and other benchmarks using PySyft library [ 27 ] based on the PyTorch framework . PySyft can emulate various virtual workers to process federated learning jobs . The training process is run on a CPU tower server ( Intel ( R ) Xeon ( R ) Gold 6252 CPU @ 2 . 10GHz 24 cores , and 32GB DDR4 2933 MHz memory , running Ubuntu 18 . 04 . 5 LTS ) . We use three models including linear regression model , logistic regression model , and Convolutional Neural Network ( CNN ) model . Linear regression uses mean squared error loss , and logistic regression uses cross - entropy loss . The CNN model’s structure is similar to the classic one in [ 28 ] , which has two 5 × 5 convolutional layers with 32 and 64 channels respectively . In each convolutional layer , 2 × 2 max pooling is used . The last two following layers are ReLu activation and softmax . We use mini - batch in all experiments , and the batch size is 64 . We set the default learning step size η = 0 . 01 . Other parameters will be speciﬁed in each experiment . We also note that the number of global aggregation iterations is K = T / τ . B . Performance Evaluation 1 ) Convergence Performance : In Fig . 4 , we compare the convergence performance of FedNAG with other three bench - marks . The experiment is performed on two datasets . MNIST is trained by linear regression , logistic regression , and CNN ; and CIFAR - 10 is trained by CNN . The setting in this experi - ment is τ = 4 , γ = 0 . 9 , N = 4 . For MNIST , the total number of iterations T is 1000 . For CIFAR - 10 , T is set to 10000 . Figs . 4 ( a ) , 4 ( b ) , 4 ( c ) , and 4 ( d ) show the values of the global loss function and accuracy trained under different models and datasets respectively . In general , we have cNAG > FedNAG > cSGD > FedAvg . For centralized approaches , we can see cNAG performs better than cSGD in all cases . For distributed approaches , FedNAG also performs better than FedAvg . It conﬁrms that NAG is more advantageous compared with gradient decent for both centralized and federated learning environment . For cNAG and FedNAG , we can ﬁnd FedNAG performs worse . This follows our expectation shown in Theorem 3 . FedNAG performs τ local updates before a global aggregation , ( a ) Linear regression on MNIST ( b ) Logistic regression on MNIST ( c ) CNN on MNIST ( d ) CNN on CIFAR - 10 Fig . 4 . Convergence performance with benchmark algorithms ( a ) Effect of aggregation frequency τ ( b ) Total iterations when global loss reaches 0 . 5 for different τ ( c ) Total iterations when accuracy reaches 85 % for different τ ( d ) Effect of momentum coefﬁcient γ ( e ) Global loss when T reaches 500 and 1000 when 0 < γ < 1 ( f ) Effect of momentum coefﬁcient γ when γ = 1 ( g ) Effect of number of workers N Fig . 5 . Effect of τ , γ , and N when CNN trained on MNIST causing less efﬁcient updates and thus decreases the conver - gence performance . Another interesting observation is that FedNAG can perform better than cSGD in the four cases : The beneﬁts of the momentum method can outweigh the performance loss by federated learning . 2 ) Effects of Global Aggregation Frequency τ : In Figs . 5 ( a ) , 5 ( b ) and 5 ( c ) , we evaluate the impact of τ based on global loss and accuracy using the same CNN model and MNIST dataset . The setting for this experiment is γ = 0 . 5 , T = 1000 , N = 4 . From Fig . 5 ( a ) , we can observe when τ is increased , the convergence performance is reduced . With the same T , loss is larger and accuracy is lower . This matches 1 (cid:13) of Theorem 3 . The convergence upper bound increases with τ . In Figs . 5 ( b ) and 5 ( c ) , we observe the impact of τ in a wider range [ 5 , 640 ] . In Fig . 5 ( b ) , we plot the number of iterations when the global loss reaches the target value 0 . 5 . In Fig . 5 ( c ) , we plot the number of iterations when the accuracy reaches the target value 85 % . Since the global loss and accuracy may oscillate during the training process , the target global loss and accuracy may be reached several times . The red horizontal lines indicate the ﬁrst and last iterations when the target values are reached , and the bar indicates the mean of the iterations when the targets are reached . The outcome shows that larger τ causes more iterations for convergence . If we double τ when τ is small , the number of iterations to reach the targets does not increase much . However , if we double τ when τ is larger ( e . g . , τ ≥ 80 ) , then the number of iterations to reach the targets substantially increases . This matches 5 (cid:13) of Theorem 1 , which concludes that larger τ leads to exponential increase of h ( · ) . Therefore , increasing τ will more signiﬁcantly delay the training process when τ is large . 3 ) Effects of Momentum Coefﬁcient γ : In Figs . 5 ( d ) , 5 ( e ) and 5 ( f ) , we evaluate the effects of γ . The setting for this experiment is τ = 4 , T = 1000 , N = 4 . We also use the same CNN model trained on the same MNIST dataset . Fig . 5 ( d ) shows the global loss and accuracy under γ = 0 . 1 , 0 . 3 , 0 . 6 , 0 . 9 respectively . It shows that γ can increase the convergence performance ( smaller global loss value and higher accuracy ) . For Fig . 5 ( e ) , we evaluate the global loss at T = 500 and T = 1000 respectively , when γ ranges from [ 0 , 0 . 99 ] . Two horizontal lines are the benchmarks where only FedAvg is used . For both T = 500 and T = 1000 , we can see the global loss decreases when γ is getting large . Accuracy is also increased at the same time . However , from Fig . 5 ( f ) , when γ = 1 , the global loss cannot converge due to the prerequisite where 0 < γ < 1 [ 29 ] . 4 ) Effects of Number of Workers N : In Fig . 5 ( g ) , we evalu - ate the global loss and accuracy based on different number of workers N using the same CNN model and MNIST dataset . The experiment setting is τ = 4 , γ = 0 . 5 , T = 2000 . From Fig . 5 ( g ) , we can see that increasing N will cause a decline of convergence performance . This follows our expectation be - cause more workers cause more divergence among the workers and thus decrease convergence performance . However , after a sufﬁcient number of iterations , the global loss and accuracy with more workers will be closer to those with fewer workers . It shows that FedNAG is applicable when there are more workers in the system . VI . C ONCLUSION In this paper , we focus on FedNAG with detailed con - vergence analysis . FedNAG allows each worker to update its weights and momenta by its local dataset for a number of local iterations between two global aggregations . On the global aggregation step , the aggregator collects and averages the weights and momenta from all workers and distributes them to the workers . The convergence analysis shows the upper bound of the gap between the global loss function derived by FedNAG at iteration T and the optimal solution . We compare FedNAG and FedAvg and conclude that as long as the learning step size is sufﬁciently small , FedNAG performs better than FedAvg . Using the PyTorch with PySyft framework , we validate theoretical results by experiments on real - world datasets . A PPENDIX A . FedNAG vs . Centralized NAG ( Observation 4 (cid:13) in Theo - rem 1 ) Proposition 1 . When τ = 1 , FedNAG is equivalent to centralized NAG . The update rules of FedNAG yield as follows : v ( t ) = γ v ( t − 1 ) − η ∇ F ( w ( t − 1 ) ) , w ( t ) = w ( t − 1 ) − γ v ( t − 1 ) + ( 1 + γ ) v ( t ) = w ( t − 1 ) + γ v ( t ) − η ∇ F ( w ( t − 1 ) ) . Proof . When τ = 1 , we have v i ( t ) = v ( t ) and w i ( t ) = w ( t ) for all t . Thus , v ( t ) = (cid:80) Ni = 1 D i v i ( t ) D = (cid:80) Ni = 1 D i ( γ v i ( t − 1 ) − η ∇ F i ( w i ( t − 1 ) ) ) D = γ v ( t − 1 ) − η (cid:80) Ni = 1 D i ∇ F i ( w ( t − 1 ) ) D = γ v ( t − 1 ) − η ∇ F ( w ( t − 1 ) ) , where the last term in the last equality is because (cid:80) Ni = 1 D i ∇ F i ( w ) D = ∇ (cid:32)(cid:80) Ni = 1 D i F i ( w ) D (cid:33) = ∇ F ( w ) based on the linearity of the gradient operator . Then , w ( t ) = (cid:80) Ni = 1 D i w i ( t ) D = (cid:80) N i = 1 D i ( w i ( t − 1 ) − γ v i ( t − 1 ) + ( 1 + γ ) v i ( t ) ) D = w ( t − 1 ) − γ v ( t − 1 ) + ( 1 + γ ) v ( t ) . Therefore , Proposition 1 has been proven . B . Proof of Theorem 1 To prove Theorem 1 , the progress mainly includes four steps . ( 1 ) We ﬁrst introduce an important equality in Lemma 1 , which will be used later . ( 2 ) We bound (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) in Lemma 2 based on Lemma 1 . ( 3 ) Based on the result of Lemma 2 , we then bound (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) in Lemma 3 . ( 4 ) Finally , based on the result of Lemma 3 , we bound (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) , which concludes Theorem 1 . Lemma 1 . Given a t = δ i β (cid:32) 1 + ηβ + ηβγ γ − B A − B A t − 1 + ηβ + ηβγ γ − A A − B B t (cid:33) , ( 22 ) A + B = 1 + ηβ + ηβγ + γ γ = ( 1 + ηβ ) ( 1 + γ ) γ , ( 23 ) AB = 1 + ηβ γ , ( 24 ) where t = 0 , 1 , 2 , . . . , 0 < γ < 1 , ηβ > 0 , we have ( 1 + ηβ ) a t − 1 + ηβγ t − 1 (cid:88) i = 0 a i = γa t . ( 25 ) Proof of Lemma 1 . For convenience , we deﬁne C (cid:44) 1 + ηβ + ηβγ γ − B A − B = A − 1 A − B , D (cid:44) A − 1 + ηβ + ηβγ γ A − B = 1 − B A − B . Therefore , a t = δ i β ( CA t + DB t ) . According to the inverse theorem of Vieta’s formulas , we have γx 2 − ( 1 + ηβ + ηβγ + γ ) x + ηβ + 1 = 0 , ( 26 ) where x values are the roots of quadratic equation . Here , the discriminant of the quadratic equation is positive . ∆ = ( 1 + ηβ + ηβγ + γ ) 2 − 4 ( 1 + ηβ ) γ > ( 1 + ηβ + γ ) 2 − 4 ( 1 + ηβ ) γ = ( ( 1 + ηβ ) − γ ) 2 > 0 . Thus , A and B ( roots ) can be expressed as follows : A = ( 1 + ηβ ) ( 1 + γ ) + (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , ( 27 ) B = ( 1 + ηβ ) ( 1 + γ ) − (cid:112) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ . ( 28 ) Then we have ( 1 + ηβ ) a t − 1 + ηβγ t − 1 (cid:88) i = 0 a i − γa t = ( 1 + ηβ ) δ i β (cid:0) CA t − 1 + DB t − 1 (cid:1) + ηβγ δ i β C A t − 1 A − 1 + ηβγ δ i β DB t − 1 B − 1 − γ δ i β CA t − γ δ i β DB t = δ i β (cid:20) A t − 1 C 1 − A (cid:0) γA 2 − ( 1 + ηβ + ηβγ + γ ) A + 1 + ηβ (cid:1) + B t − 1 D 1 − B (cid:0) γB 2 − ( 1 + ηβ + ηβγ + γ ) B + 1 + ηβ (cid:1)(cid:21) − δ i β ηβγ (cid:18) C A − 1 + D B − 1 (cid:19) = 0 − ηδ i γ (cid:18) C A − 1 + D B − 1 (cid:19) = 0 . ( because A , B satisfy ( 26 ) ) . Thus , Lemma 1 has been proven . 1 ) Bounding (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) : To prove Lemma 2 , the progress mainly includes two steps . ( 1 ) We ﬁrst bound the gap of (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) . ( 2 ) Then we bound the gap of (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) , which concludes Lemma 2 . Lemma 2 . For any interval [ k ] , ∀ t ∈ [ ( k − 1 ) τ , kτ ] , we have (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) ≤ f i ( t − ( k − 1 ) τ ) , ( 29 ) where we deﬁne the function f i ( x ) as f i ( x ) (cid:44) δ i β ( γ x ( CA x + DB x ) − 1 ) . ( 30 ) Proof of Lemma 2 . When t = ( k − 1 ) τ , we know w i ( t ) = w ( t ) = w [ k ] ( t ) by the deﬁnition of w [ k ] ( t ) and aggregation rules . Hence , we have (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) = 0 . Meanwhile , when t = ( k − 1 ) τ , x = 0 and f i ( 0 ) = 0 . Thus , Lemma 2 holds . When t ∈ ( ( k − 1 ) τ , kτ ] , we bound the momentum gap (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) = (cid:107) γ v i ( t − 1 ) − η ∇ F i ( w i ( t − 1 ) ) − ( γ v [ k ] ( t − 1 ) − η ∇ F ( w [ k ] ( t − 1 ) ) ) (cid:107) = (cid:107) γ ( v i ( t − 1 ) − v [ k ] ( t − 1 ) ) − η [ ∇ F i ( w i ( t − 1 ) ) − ∇ F i ( w [ k ] ( t − 1 ) ) + ∇ F i ( w [ k ] ( t − 1 ) ) − ∇ F ( w [ k ] ( t − 1 ) ) ] (cid:107) ( adding a zero term ) ≤ γ (cid:107) v i ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + η (cid:107)∇ F i ( w i ( t − 1 ) ) − ∇ F i ( w [ k ] ( t − 1 ) ) (cid:107) + η (cid:107)∇ F i ( w [ k ] ( t − 1 ) ) − ∇ F ( w [ k ] ( t − 1 ) ) (cid:107) ( from triangle inequality ) ≤ γ (cid:107) v i ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + ηβ (cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + ηδ i . ( 31 ) ( from β - smoothness and ( 7 ) ) We use γ 0 , γ 1 , . . . , γ t − ( k − 1 ) τ − 1 as multipliers to multiply ( 31 ) when t , t − 1 , . . . , ( k − 1 ) τ + 1 , respectively . (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) ≤ γ (cid:107) v i ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + ηβ (cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + ηδ i , γ (cid:107) v i ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) ≤ γ ( γ (cid:107) v i ( t − 2 ) − v [ k ] ( t − 2 ) (cid:107) + ηβ (cid:107) w i ( t − 2 ) − w [ k ] ( t − 2 ) (cid:107) + ηδ i ) , . . . γ t − ( k − 1 ) τ − 1 (cid:107) v i ( ( k − 1 ) τ + 1 ) − v [ k ] ( ( k − 1 ) τ + 1 ) (cid:107) ≤ γ t − ( k − 1 ) τ − 1 ( γ (cid:107) v i ( ( k − 1 ) τ ) − v [ k ] ( ( k − 1 ) τ ) (cid:107) + ηβ (cid:107) w i ( ( k − 1 ) τ ) − w [ k ] ( ( k − 1 ) τ ) (cid:107) + ηδ i ) . For convenience , we deﬁne G i ( t ) (cid:44) (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) . Summing up all of the above inequalities , we have (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) ≤ ηβ ( G i ( t − 1 ) + γG i ( t − 2 ) + γ 2 G i ( t − 3 ) + · · · + γ t − ( k − 1 ) τ − 1 G i ( ( k − 1 ) τ ) ) + ηδ i ( 1 + γ + γ 2 + · · · + γ t − ( k − 1 ) τ − 1 ) + γ t − ( k − 1 ) τ (cid:107) v i ( ( k − 1 ) τ ) − v [ k ] ( ( k − 1 ) τ ) (cid:107) . When t = ( k − 1 ) τ , we know v i ( t ) = v ( t ) = v [ k ] ( t ) by the deﬁnition of v [ k ] ( t ) and aggregation rules . Then we have (cid:107) v i ( ( k − 1 ) τ ) − v [ k ] ( ( k − 1 ) τ ) (cid:107) = 0 , so that the last term of above inequality is zero and (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) ≤ ηβ ( G i ( t − 1 ) + γG i ( t − 2 ) + γ 2 G i ( t − 3 ) + · · · + γ t − ( k − 1 ) τ − 1 G i ( ( k − 1 ) τ ) ) + ηδ i ( 1 + γ + γ 2 + · · · + γ t − ( k − 1 ) τ − 1 ) . ( 32 ) Now , we can bound the gap between w i ( t ) and w [ k ] ( t ) . When t ∈ ( ( k − 1 ) τ , kτ ] , we have (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) = (cid:107) w i ( t − 1 ) + γ v i ( t ) − η ∇ F i ( w i ( t − 1 ) ) − ( w [ k ] ( t − 1 ) + γ v [ k ] ( t ) − η ∇ F ( w [ k ] ( t − 1 ) ) ) (cid:107) ( from ( 3 ) and ( 12 ) ) = (cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) + γ ( v i ( t ) − v [ k ] ( t ) ) − η [ ∇ F i ( w i ( t − 1 ) ) − ∇ F i ( w [ k ] ( t − 1 ) ) + ∇ F i ( w [ k ] ( t − 1 ) ) − ∇ F ( w [ k ] ( t − 1 ) ) ] (cid:107) ( adding a zero term ) ≤(cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + γ (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) + ηβ (cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + ηδ i ( from triangle inequality , β - smoothness and ( 7 ) ) = ( ηβ + 1 ) (cid:107) w i ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + γ (cid:107) v i ( t ) − v [ k ] ( t ) (cid:107) + ηδ i . ( 33 ) Substituting inequality ( 32 ) into ( 33 ) and using G i ( t ) to denote (cid:107) w i ( t ) − w [ k ] ( t ) (cid:107) for t , t − 1 , · · · , 1 + ( k − 1 ) τ , we have G i ( t ) ≤ ( ηβ + 1 ) G i ( t − 1 ) + ηβγ ( G i ( t − 1 ) + γG i ( t − 2 ) + γ 2 G i ( t − 3 ) + · · · + γ t − ( k − 1 ) τ − 1 G i ( ( k − 1 ) τ ) ) + ηδ i γ ( 1 + γ + γ 2 + · · · + γ t − ( k − 1 ) τ − 1 ) + ηδ i = G i ( t − 1 ) + ηβ ( G i ( t − 1 ) + γG i ( t − 1 ) + γ 2 G i ( t − 2 ) + γ 3 G i ( t − 3 ) + · · · + γ t − ( k − 1 ) τ G i ( ( k − 1 ) τ ) ) + ηδ i ( 1 + γ + γ 2 + · · · + γ t − ( k − 1 ) τ ) . ( 34 ) For convenience , we deﬁne g i ( x ) (cid:44) δ i β ( CA x + DB x ) , where A and B are deﬁned in Theorem 1 ; C and D are deﬁned in Lemma 1 . We have f i ( x ) = γ x g i ( x ) − δ i β . ( 35 ) Next , we use induction to prove G i ( t ) ≤ f i ( t − ( k − 1 ) τ ) . For the induction , we assume that G i ( p ) ≤ f i ( p − ( k − 1 ) τ ) ( 36 ) holds for some p ∈ ( ( k − 1 ) τ , t ) . Thus , we have G i ( t ) ≤ f i ( t − 1 − ( k − 1 ) τ ) + ηβ ( f i ( t − 1 − ( k − 1 ) τ ) + γf i ( t − 1 − ( k − 1 ) τ ) + γ 2 f i ( t − 2 − ( k − 1 ) τ ) + · · · + γ t − ( k − 1 ) τ f i ( 0 ) ) + ηδ i ( 1 + γ + γ 2 + · · · + γ t − ( k − 1 ) τ ) ( from ( 34 ) , ( 36 ) and G i ( ( k − 1 ) τ ) = f i ( 0 ) ) = γ t − 1 − ( k − 1 ) τ g i ( t − 1 − ( k − 1 ) τ ) ) − δ i β + ηβ ( γ t − 1 − ( k − 1 ) τ g i ( t − 1 − ( k − 1 ) τ ) − δ i β + γ · γ t − 1 − ( k − 1 ) τ g i ( t − 1 − ( k − 1 ) τ ) − γ · δ i β + γ 2 · γ t − 2 − ( k − 1 ) τ g i ( t − 2 − ( k − 1 ) τ ) − γ 2 · δ i β · · · + γ t − ( k − 1 ) τ g i ( 0 ) − γ t − ( k − 1 ) τ δ i β ) + ηδ i + γηδ i + · · · + γ t − ( k − 1 ) τ ηδ i ( from ( 35 ) ) = γ t − 1 − ( k − 1 ) τ ( g i ( t − 1 − ( k − 1 ) τ ) + ηβg i ( t − 1 − ( k − 1 ) τ ) + ηβγ ( g i ( t − 1 − ( k − 1 ) τ ) + g i ( t − 2 − ( k − 1 ) τ ) + · · · + g i ( 0 ) ) ) − δ i β = γ t − ( k − 1 ) τ g i ( t − ( k − 1 ) τ ) − δ i β ( from Lemma 1 and g i ( t ) = a t ) = f i ( t − ( k − 1 ) τ ) . Thus , Lemma 2 has been proven . 2 ) Bounding (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) : Based on the result of Lemma 2 , we ﬁrst bound the gap of (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) in Lemma 3 . Based on the result of Lemma 3 , we then bound the gap of (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) , which concludes Theorem 1 . Lemma 3 . For any interval [ k ] , ∀ t ∈ [ ( k − 1 ) τ , kτ ] , we have : (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) ≤ ηδ (cid:18) C ( γA ) t 0 γ ( A − 1 ) + D ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) , ( 37 ) where t 0 = t − ( k − 1 ) τ . Proof of Lemma 3 . For convenience , we deﬁne p ( t ) (cid:44) γ t ( CA t + DB t ) − 1 . ( 38 ) Therefore , we get f i ( t ) = δ i β p ( t ) . ( 39 ) From ( 2 ) and ( 4 ) , we have v ( t ) = γ v ( t − 1 ) − η (cid:80) Ni = 1 D i ∇ F i ( w i ( t − 1 ) ) D . ( 40 ) For t ∈ ( ( k − 1 ) τ , kτ ] , we have (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) = (cid:107) γ v ( t − 1 ) − η (cid:80) Ni = 1 D i ∇ F i ( w i ( t − 1 ) ) D − γ v [ k ] ( t − 1 ) + η ∇ F ( w [ k ] ( t − 1 ) ) (cid:107) ( from ( 40 ) and ( 11 ) ) ≤ γ (cid:107) v ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + η (cid:80) Ni = 1 D i (cid:107)∇ F i ( w i ( t − 1 ) ) − ∇ F i ( w [ k ] ( t − 1 ) ) (cid:107) D ≤ γ (cid:107) v ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + ηβ (cid:80) Ni = 1 D i f i ( t − 1 − ( k − 1 ) τ ) D ( from β - smoothness and Lemma 2 ) = γ (cid:107) v ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + ηδp ( t − 1 − ( k − 1 ) τ ) . ( 41 ) ( from ( 39 ) and ( 8 ) ) We use γ 0 , γ 1 , . . . , γ t − ( k − 1 ) τ − 1 as multipliers to multiply ( 41 ) when t , t − 1 , . . . , ( k − 1 ) τ + 1 , respectively . (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) ≤ γ (cid:107) v ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) + ηδp ( t − 1 − ( k − 1 ) τ ) , γ (cid:107) v ( t − 1 ) − v [ k ] ( t − 1 ) (cid:107) ≤ γ 2 ( (cid:107) v ( t − 2 ) − v [ k ] ( t − 2 ) (cid:107) + γηδp ( t − 2 − ( k − 1 ) τ ) , . . . γ t − ( k − 1 ) τ − 1 (cid:107) v ( ( k − 1 ) τ + 1 ) − v [ k ] ( ( k − 1 ) τ + 1 ) (cid:107) ≤ γ t − ( k − 1 ) τ (cid:107) v ( ( k − 1 ) τ ) − v [ k ] ( ( k − 1 ) τ ) (cid:107) + γ t − 1 − ( k − 1 ) τ ηδp ( 0 ) . Summing up all of the above inequalities , we have (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) ≤ ηδ ( γ t − 1 − ( k − 1 ) τ p ( 0 ) + · · · + γp ( t − 2 − ( k − 1 ) τ ) + p ( t − 1 − ( k − 1 ) τ ) ) ( 42 ) ( because (cid:107) v ( ( k − 1 ) τ ) − v [ k ] ( ( k − 1 ) τ ) (cid:107) = 0 from ( 9 ) ) = ηδ ( γ t − 1 − ( k − 1 ) τ C ( 1 + A + · · · + A t − 1 − ( k − 1 ) τ ) + γ t − 1 − ( k − 1 ) τ D ( 1 + B + · · · + B t − 1 − ( k − 1 ) τ ) − ( 1 + γ + · · · + γ t − 1 − ( k − 1 ) τ ) ) = ηδ (cid:18) γ t 0 − 1 C A t 0 − 1 A − 1 + γ t 0 − 1 DB t 0 − 1 B − 1 − γ t 0 − 1 γ − 1 (cid:19) = ηδ (cid:18) C ( γA ) t 0 γ ( A − 1 ) + D ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) − ηδγ t 0 − 1 (cid:18) C A − 1 + D B − 1 (cid:19) = ηδ (cid:18) C ( γA ) t 0 γ ( A − 1 ) + D ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) ( 43 ) where t 0 = t − ( k − 1 ) τ . Thus , Lemma 3 has been proven . Based on the result in Lemma 3 , we can now bound (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) . Proof of Theorem 1 . From ( 3 ) , ( 4 ) , and ( 5 ) , we have w ( t ) = w ( t − 1 ) + γ v ( t ) − η (cid:80) Ni = 1 D i ∇ F i ( w i ( t − 1 ) ) D . ( 44 ) From ( 12 ) and ( 44 ) , we have (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) = (cid:107) w ( t − 1 ) + γ v ( t ) − η (cid:80) Ni = 1 D i ∇ F i ( w i ( t − 1 ) ) D − w [ k ] ( t − 1 ) − γ v [ k ] ( t ) + η ∇ F ( w [ k ] ( t − 1 ) ) (cid:107) ≤(cid:107) w ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) + γ (cid:107) v ( t ) − v [ k ] ( t ) (cid:107) + ηδp ( t − 1 − ( k − 1 ) τ ) . ( from β - smoothness , Lemma 2 , ( 39 ) , and ( 8 ) ) Thus , according to Lemma 3 , we have (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) − (cid:107) w ( t − 1 ) − w [ k ] ( t − 1 ) (cid:107) ≤ γηδ (cid:18) C ( γA ) t 0 γ ( A − 1 ) + D ( γB ) t 0 γ ( B − 1 ) − γ t 0 − 1 γ − 1 (cid:19) + ηδ ( γ t 0 − 1 ( CA t 0 − 1 + DB t 0 − 1 ) − 1 ) ( 45 ) = ηδ (cid:18) C ( γA ) t 0 − 1 A − 1 ( γA + A − 1 ) + D ( γB ) t 0 − 1 B − 1 ( γB + B − 1 ) − γ t 0 + 1 − 1 γ − 1 (cid:19) . ( 46 ) When t = ( k − 1 ) τ , we have (cid:107) w ( t ) − w [ k ] ( t ) (cid:107) = 0 . When t ∈ ( ( k − 1 ) τ , kτ ] , we sum up ( 46 ) for t , t − 1 , . . . , ( k − 1 ) τ + 1 . Then we have (cid:107) w ( t ) − w k ( t ) (cid:107) ≤ t 0 (cid:88) x = 1 ηδ (cid:18) C ( γA ) x − 1 A − 1 ( γA + A − 1 ) + D ( γB ) x − 1 B − 1 ( γB + B − 1 ) − γ x + 1 − 1 γ − 1 (cid:19) = ηδ (cid:2) E (cid:0) ( γA ) t 0 − 1 (cid:1) + F (cid:0) ( γB ) t 0 − 1 (cid:1) − γ 2 ( γ t 0 − 1 ) − ( γ − 1 ) t 0 ( γ − 1 ) 2 (cid:21) = ηδ (cid:20) E ( γA ) t 0 + F ( γB ) t 0 − 1 ηβ − γ 2 ( γ t 0 − 1 ) − ( γ − 1 ) t 0 ( γ − 1 ) 2 (cid:21) = h ( t 0 ) , where E = γA + A − 1 ( A − B ) ( γA − 1 ) and F = γB + B − 1 ( A − B ) ( 1 − γB ) ( as deﬁned in Theorem 1 ) . E + F = 1 ηβ . t 0 = t − ( k − 1 ) τ . Thus , Theorem 1 has been proven . C . Proof of Monotone of h ( x ) ( Observation 1 (cid:13) in Theorem 1 ) We ﬁrst introduce following Lemma 4 for later use . Lemma 4 . Given A , B , C , and D according to their deﬁni - tions , then we have C ( γA ) i + D ( γB ) i ≥ ( 1 + ηβ + ηβγ ) i holds for i = 0 , 1 , 2 , 3 , . . . Proof . We note that according to the deﬁnitions of A , B , C and D , we know that γA > 1 , 0 < γB < 1 , 1 γ + 1 < B < 1 , C > 0 , D > 0 , E > 0 , and F > 0 . We also have C + D = 1 . When i = 0 , C ( γA ) i + D ( γB ) i = ( 1 + ηβ + ηβγ ) i = 1 , so the inequality holds . When i = 1 , we have C ( γA ) i + D ( γB ) i = γ ( CA + DB ) = γ (cid:18) A − 1 A − B A + 1 − B A − B B (cid:19) = γ ( A + B − 1 ) = 1 + ηβ + ηβγ , so the inequality still holds . When i > 1 , according to Jensen inequality , and f ( x ) = x i is convex , we have C ( γA ) i + D ( γB ) i ≥ ( γCA + γDB ) i = ( 1 + ηβ + ηβγ ) i . To conclude , Lemma 4 has been proven . Then we can prove the monotone of h ( x ) . Proof . It is equivalent to prove h ( x ) − h ( x − 1 ) ≥ 0 for all integer x ≥ 1 . When x = 0 or x = 1 , we have h ( 0 ) = ηδ ( E + F − 1 ηβ ) = 0 , h ( 1 ) = ηδ (cid:18) γ ( EA + FB ) − 1 ηβ − γ − 1 (cid:19) = 0 , because EA + FB = 1 + ηβ + ηβγ ηβγ . Therefore , when x = 1 , h ( x ) − h ( x − 1 ) = 0 . When x > 1 , according to Lemma 4 and ( 38 ) , we have p ( x ) = C ( γA ) x + D ( γB ) x − 1 ≥ ( 1 + ηβ + ηβγ ) x − 1 > 0 . Then we have h ( x ) − h ( x − 1 ) = ηδ (cid:18) C ( γA ) x ( γA + A − 1 ) γA ( A − 1 ) + D ( γB ) x ( γB + B − 1 ) γB ( B − 1 ) − γ x + 1 − 1 γ − 1 (cid:19) = γηδ (cid:18) C ( γA ) x γ ( A − 1 ) + D ( γB ) x γ ( B − 1 ) − γ x − 1 γ − 1 (cid:19) + ηδ ( γ x − 1 ( CA x − 1 + DB x − 1 ) − 1 ) ( because ( 46 ) equals ( 45 ) ) = γηδ ( γ x − 1 p ( 0 ) + · · · + γp ( x − 2 ) + p ( x − 1 ) ) + ηδp ( x − 1 ) ( because ( 43 ) equals ( 42 ) , x = t − ( k − 1 ) τ , and ( 38 ) ) > 0 . Thus , we have proven that h ( 0 ) = h ( 1 ) = 0 and h ( x ) increases with x when x ≥ 1 . D . Proof of Theorem 2 For convenience , we deﬁne c [ k ] ( t ) (cid:44) F ( w [ k ] ( t ) ) − F ( w ∗ ) for a given interval [ k ] , where t ∈ [ ( k − 1 ) τ , kτ ] . Proof . According to the convergence lower bound of any gradient descent methods given in Theorem 3 . 14 in [ 30 ] , we always have c [ k ] ( t ) > 0 ( 47 ) for any t and k . Then we derive the upper bound of c [ k ] ( t + 1 ) − c [ k ] ( t ) . Because F ( · ) is β - smooth , according to Lemma 3 . 4 in [ 30 ] , we have F ( x ) − F ( y ) ≤ ∇ F ( y ) T ( x − y ) + β 2 (cid:107) x − y (cid:107) 2 for arbitrary x and y . Thus , c [ k ] ( t + 1 ) − c [ k ] ( t ) = F (cid:0) w [ k ] ( t + 1 ) (cid:1) − F (cid:0) w [ k ] ( t ) (cid:1) ≤∇ F (cid:0) w [ k ] ( t ) (cid:1) T (cid:0) w [ k ] ( t + 1 ) − w [ k ] ( t ) (cid:1) + β 2 (cid:13)(cid:13) w [ k ] ( t + 1 ) − w [ k ] ( t ) (cid:13)(cid:13) 2 = γ ∇ F (cid:0) w [ k ] ( t ) (cid:1) T v [ k ] ( t + 1 ) − η (cid:107)∇ F (cid:0) w [ k ] ( t ) (cid:1) (cid:107) 2 + β 2 (cid:107) γ v [ k ] ( t + 1 ) − η ∇ F (cid:0) w [ k ] ( t ) (cid:1) (cid:107) 2 = − η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) 2 + βγ 4 2 (cid:13)(cid:13) v [ k ] ( t ) (cid:13)(cid:13) 2 + γ 2 ( 1 − βη ( γ + 1 ) ) ∇ F (cid:0) w [ k ] ( t ) (cid:1) T v [ k ] ( t ) ( replacing v [ k ] ( t + 1 ) with ( 11 ) and rearrange ) ≤ (cid:18) − η ( γ + 1 ) (cid:18) 1 − βη ( γ + 1 ) 2 (cid:19) + βη 2 γ 2 p 2 2 − γ 2 ηq ( 1 − βη ( γ + 1 ) ) cos θ (cid:1) (cid:13)(cid:13) ∇ F (cid:0) w [ k ] ( t ) (cid:1)(cid:13)(cid:13) 2 , ( 48 ) where the second term in ( 48 ) is because (cid:107) γ v [ k ] ( t ) (cid:107) ≤ p (cid:107) η ∇ F ( w [ k ] ( t ) ) (cid:107) with the deﬁnition of p . Since 0 < βη ( γ + 1 ) ≤ 1 , the third term in ( 48 ) is because − ∇ F ( w [ k ] ( t ) ) T v [ k ] ( t ) = (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) (cid:13)(cid:13) v [ k ] ( t ) (cid:13)(cid:13) cos θ [ k ] ( t ) = (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) (cid:13)(cid:13) γ v [ k ] ( t − 1 ) − η ∇ F ( w [ k ] ( t − 1 ) ) (cid:13)(cid:13) cos θ [ k ] ( t ) ≥ η (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) (cid:13)(cid:13) ∇ F ( w [ k ] ( t − 1 ) ) (cid:13)(cid:13) cos θ [ k ] ( t ) ( because cos θ [ k ] ( t − 1 ) ≥ 0 ) ≥ ηq (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) 2 cos θ [ k ] ( t ) ( because (cid:107)∇ F ( w [ k ] ( t − 1 ) ) (cid:107) ≥ q (cid:107)∇ F ( w [ k ] ( t ) ) (cid:107) ) ≥ ηq (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) 2 cos θ . ( because θ ≥ θ [ k ] ( t ) by deﬁnition and cos θ ≥ 0 ) According to the deﬁnition of α , and condition 2 of Theorem 2 with h ( τ ) ≥ 0 , we have α > 0 . Then from ( 48 ) , we have c [ k ] ( t + 1 ) ≤ c [ k ] ( t ) − α (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) 2 . ( 49 ) According to the convexity condition and Cauchy - Schwarz inequality , we have : c [ k ] ( t ) = F ( w [ k ] ( t ) ) − F ( w ∗ ) ≤ ∇ F ( w [ k ] ( t ) ) T ( w [ k ] ( t ) − w ∗ ) ≤ (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) (cid:13)(cid:13) w [ k ] ( t ) − w ∗ (cid:13)(cid:13) . Equivalently , (cid:13)(cid:13) ∇ F ( w [ k ] ( t ) ) (cid:13)(cid:13) ≥ c [ k ] ( t ) (cid:13)(cid:13) w [ k ] ( t ) − w ∗ (cid:13)(cid:13) . ( 50 ) Substituting ( 50 ) into ( 49 ) , and noting ω ≤ 1 (cid:107) w [ k ] ( t ) − w ∗ (cid:107) 2 by the deﬁnition of ω , we get c [ k ] ( t + 1 ) ≤ c [ k ] ( t ) − αc [ k ] ( t ) 2 (cid:13)(cid:13) w [ k ] ( t ) − w ∗ (cid:13)(cid:13) 2 ≤ c [ k ] ( t ) − ωαc [ k ] ( t ) 2 . Because α > 0 , c [ k ] ( t ) > 0 in ( 47 ) , and ( 49 ) , we have 0 < c [ k ] ( t + 1 ) ≤ c [ k ] ( t ) . Dividing both side by c [ k ] ( t + 1 ) c [ k ] ( t ) , we get 1 c [ k ] ( t ) ≤ 1 c [ k ] ( t + 1 ) − ωα c [ k ] ( t ) c [ k ] ( t + 1 ) . We note that c [ k ] ( t ) c [ k ] ( t + 1 ) ≥ 1 . Thus , 1 c [ k ] ( t + 1 ) − 1 c [ k ] ( t ) ≥ ωα c [ k ] ( t ) c [ k ] ( t + 1 ) ≥ ωα . ( 51 ) Summing up the above inequality by t ∈ [ ( k − 1 ) τ , kτ − 1 ] , we have 1 c [ k ] ( kτ ) − 1 c [ k ] ( ( k − 1 ) τ ) = kτ − 1 (cid:88) t = ( k − 1 ) τ (cid:18) 1 c [ k ] ( t + 1 ) − 1 c [ k ] ( t ) (cid:19) ≥ kτ − 1 (cid:88) t = ( k − 1 ) τ ωα = τωα . ( 52 ) Then , we sum up the above inequality by k ∈ [ 1 , K ] , after rearranging the left - hand side and noting that T = Kτ , we can get K (cid:88) k = 1 (cid:18) 1 c [ k ] ( kτ ) − 1 c [ k ] ( ( k − 1 ) τ ) (cid:19) = 1 c [ K ] ( T ) − 1 c [ 1 ] ( 0 ) − K − 1 (cid:88) k = 1 (cid:18) 1 c [ k + 1 ] ( kτ ) − 1 c [ k ] ( kτ ) (cid:19) ≥ Kτωα = Tωα . ( 53 ) Here , we note that 1 c [ k + 1 ] ( kτ ) − 1 c [ k ] ( kτ ) = c [ k ] ( kτ ) − c [ k + 1 ] ( kτ ) c [ k ] ( kτ ) c [ k + 1 ] ( kτ ) = F ( w [ k ] ( kτ ) ) − F ( w [ k + 1 ] ( kτ ) ) c [ k ] ( kτ ) c [ k + ] ( kτ ) ≥ − ρh ( τ ) c [ k ] ( kτ ) c [ k + ] ( kτ ) . ( 54 ) where the last inequality is because w [ k + 1 ] ( kτ ) = w ( kτ ) in ( 10 ) , and ( 15 ) in Theorem 1 . From ( 49 ) , we can get F ( w [ k ] ( t ) ) ≥ F ( w [ k ] ( t + 1 ) ) for any t ∈ [ ( k − 1 ) τ , kτ ) . Recalling condition 3 in Theorem 2 , where F ( w [ k ] ( kτ ) ) − F ( w ∗ ) ≥ ε for all k , we can obtain c [ k ] ( t ) = F ( w [ k ] ( t ) ) − F ( w ∗ ) ≥ ε for all t ∈ [ ( k − 1 ) τ , kτ ] and k . Thus , c [ k ] ( kτ ) c [ k + 1 ] ( kτ ) ≥ ε 2 . ( 55 ) According to Appendix C , we have h ( τ ) ≥ 0 . Then substitut - ing ( 55 ) into ( 54 ) , we have 1 c [ k + 1 ] ( kτ ) − 1 c [ k ] ( kτ ) ≥ − ρh ( τ ) ε 2 . ( 56 ) Substituting ( 56 ) into ( 53 ) and rearrange , we get 1 c [ K ] ( T ) − 1 c [ 1 ] ( 0 ) ≥ Tωα − ( K − 1 ) ρh ( τ ) ε 2 . ( 57 ) Recalling condition 4 in Theorem 2 , where F ( w ( T ) ) − F ( w ∗ ) ≥ ε , and noting that c [ K ] ( T ) ≥ ε , we get ( F ( w ( T ) ) − F ( w ∗ ) ) c [ K ] ( T ) ≥ ε 2 ( 58 ) Thus , 1 F ( w ( T ) ) − F ( w ∗ ) − 1 c [ K ] ( T ) = c [ K ] ( T ) − ( F ( w ( T ) ) − F ( w ∗ ) ) ( F ( w ( T ) ) − F ( w ∗ ) ) c [ K ] ( T ) = F ( w [ K ] ( T ) ) − F ( w ( T ) ) ( F ( w ( T ) ) − F ( w ∗ ) ) c [ K ] ( T ) ≥ − ρh ( τ ) ( F ( w ( T ) ) − F ( w ∗ ) ) c [ K ] ( T ) ≥ − ρh ( τ ) ε 2 , ( 59 ) where the ﬁrst inequality is because ( 15 ) in Theorem 1 when t = Kτ in interval [ K ] . Combining ( 57 ) with ( 59 ) , we get 1 F ( w ( T ) ) − F ( w ∗ ) − 1 c [ 1 ] ( 0 ) ≥ Tωα − K ρh ( τ ) ε 2 = Tωα − Tρh ( τ ) τε 2 = T (cid:18) ωα − ρh ( τ ) τε 2 (cid:19) . Noting that c [ 1 ] ( 0 ) = F ( w [ 1 ] ( 0 ) ) − F ( w ∗ ) > 0 , the above inequality can be expressed as 1 F ( w ( T ) ) − F ( w ∗ ) ≥ T (cid:18) ωα − ρh ( τ ) τε 2 (cid:19) . ( 60 ) Recalling condition 2 in Theorem 2 , where ωα − ρh ( τ ) τε 2 > 0 , we obtain that the right - hand side of above inequality is greater than zero . Therefore , taking the reciprocal of the above inequality , we ﬁnally get the result F ( w ( T ) ) − F ( w ∗ ) ≤ 1 T (cid:16) ωα − ρh ( τ ) τε 2 (cid:17) . E . Proof of Theorem 3 Proof . At the beginning , we see that condition 1 in Theorem 2 always holds due to the conditions in Theorem 3 , where cos θ ≥ 0 , 0 < βη ( γ + 1 ) ≤ 1 , and 0 ≤ γ < 1 . When ρh ( τ ) = 0 , there is always an arbitrarily small ε but great than zero that let conditions 2 – 4 in Theorem 2 hold . Under this circumstance , Theorem 2 holds . We also note that the right - hand side of ( 17 ) is equivalent to the right - hand side of ( 16 ) when ρh ( τ ) = 0 . Moreover , according to the deﬁnition of w f in ( 6 ) , we have F (cid:0) w f (cid:1) − F ( w ∗ ) ≤ F ( w ( T ) ) − F ( w ∗ ) ≤ 1 Tωα , which satisﬁes the result in Theorem 2 directly . Thus , Theo - rem 3 holds when ρh ( τ ) = 0 . When ρh ( τ ) > 0 , considering the right - hand side of ( 16 ) and let ε 0 = 1 T (cid:16) ωα − ρh ( τ ) τε 20 (cid:17) . ( 61 ) Rearranging and calculating ε 0 , we get ε 0 = 1 2 Tωα + (cid:114) 1 4 T 2 ω 2 α 2 + ρh ( τ ) ωατ . ( 62 ) Here , we take the positive solution ε 0 because ε > 0 in Theorem 2 . Considering above two equations for ε 0 , we get ε 0 > 0 and the denominator in ( 61 ) is greater than zero . We also note that ωα − ρh ( τ ) τε 2 increases with ε . Thus , when ε ≥ ε 0 , condition 2 in Theorem 2 holds . Under this circumstance , we assume that there exists ε > ε 0 that satisﬁes both condition 3 and 4 in Theorem 2 at the same time , so that Theorem 2 holds . Then we get , F ( w ( T ) ) − F ( w ∗ ) ≤ 1 T (cid:16) ωα − ρh ( τ ) τε 2 (cid:17) < 1 T (cid:16) ωα − ρh ( τ ) τε 20 (cid:17) = ε 0 , which contradicts the condition 4 in Theorem 2 . Using the proof by contradiction , we conclude that there does not exist ε > ε 0 that satisﬁes both condition 3 and 4 in Theorem 2 at the same time . Equivalently , it happens either ( 1 ) ∃ k ∈ [ 1 , K ] allows F (cid:0) w [ k ] ( kτ ) (cid:1) − F ( w ∗ ) ≤ ε 0 or ( 2 ) F ( w ( T ) ) − F ( w ∗ ) ≤ ε 0 , which follows min (cid:26) min k ∈ [ 1 , K ] F (cid:0) w [ k ] ( kτ ) (cid:1) ; F ( w ( T ) ) (cid:27) − F ( w ∗ ) ≤ ε 0 . ( 63 ) Recalling ( 15 ) in Theorem 1 , when t = kτ , we have F ( w ( kτ ) ) ≤ F ( w [ k ] ( kτ ) ) + ρh ( τ ) for any interval [ k ] . Combining it with ( 63 ) , we have min k ∈ [ 1 , K ] F ( w ( kτ ) ) − F ( w ∗ ) ≤ ε 0 + ρh ( τ ) . Recalling the deﬁnition of w f in ( 6 ) , T = Kτ , and combining w f with above inequality , we get F (cid:0) w f (cid:1) − F ( w ∗ ) ≤ ε 0 + ρh ( τ ) . Substituting ( 62 ) into above inequality , we ﬁnally get the result in ( 17 ) , which proves the Theorem 3 . F . Proof of Theorem 4 Proof . When η → 0 , we have γA (cid:39) 1 , γB (cid:39) γ , and F (cid:39) γ 2 ( 1 − γ ) 2 . Therefore , lim η → 0 h ( τ ) = lim η → 0 ηδ (cid:20) E ( γA ) τ + F ( γB ) τ − 1 ηβ − γ 2 ( γ τ − 1 ) − ( γ − 1 ) τ ( γ − 1 ) 2 (cid:21) = lim η → 0 ηδ (cid:18) E − 1 ηβ (cid:19) = lim η → 0 ηδ (cid:18) 1 ( 1 − γ ) ( γA − 1 ) − 1 ηβ (cid:19) = δ 1 − γ lim η → 0 η γA − 1 − δ β = δ 1 − γ lim η → 0 1 ( γA − 1 ) (cid:48) − δ β = δ 1 − γ 1 − γ β − δ β = 0 where the second last line is because the L’Hpital’s rule . We also have ˆ h ( τ ) (cid:39) 0 when η → 0 . Rewrite f 1 ( T ) and f 2 ( T ) , we have f 1 ( T ) = 1 2 Tωα + (cid:114) 1 + 4 T 2 ωαρh ( τ ) τ − 1 4 T 2 ω 2 α 2 + ρh ( τ ) (cid:39) 1 2 Tωα + (cid:114) 1 4 T 2 ω 2 α 2 = 1 Tωα , f 2 ( T ) = 1 2 Tω ˆ α + (cid:115) 1 + 4 T 2 ω ˆ αρ ˆ h ( τ ) τ − 1 4 T 2 ω 2 ˆ α 2 + ρ ˆ h ( τ ) (cid:39) 1 2 Tω ˆ α + (cid:114) 1 4 T 2 ω 2 ˆ α 2 = 1 Tω ˆ α . According to the deﬁnition of α , and condition 2 of Theorem 2 with h ( · ) ≥ 0 , we have α > 0 . Based on the conditions in Theorem 4 , where 0 < βη ( γ + 1 ) ≤ 1 and the deﬁnition of ˆ α , we have ˆ α > 0 . Furthermore , for any 0 < γ < 1 and η → 0 + , we have α > ˆ α . Therefore , we get f 1 ( T ) < f 2 ( T ) . G . Proof of cos θ ≥ 0 when η → 0 + ( Observation 1 (cid:13) in Theorem 4 ) Proof . When η → 0 + , recalling the deﬁnition of p , we can get (cid:107) γ v [ k ] ( t ) (cid:107) ≤ p (cid:107) η ∇ F ( w [ k ] ( t ) ) (cid:107) (cid:39) 0 . Then , We have (cid:107) v [ k ] ( t ) (cid:107) (cid:39) 0 , v [ k ] ( t ) (cid:39) 0 . Thus , we have − 2 ∇ F ( w [ k ] ( t ) ) T v [ k ] ( t ) = (cid:107)∇ F ( w [ k ] ( t ) ) (cid:107) 2 + (cid:107) v [ k ] ( t ) (cid:107) 2 − (cid:107) v [ k ] ( t ) + ∇ F ( w [ k ] ( t ) ) (cid:107) 2 (cid:39) 0 . Therefore , cos θ [ k ] ( t ) (cid:39) 0 for any k ∈ [ 1 , K ] and t ∈ [ k ] , which proves cos θ ≥ 0 . R EFERENCES [ 1 ] H . B . McMahan , E . Moore , D . Ramage , S . Hampson , and B . A . y Arcas , “Communication - efﬁcient learning of deep networks from decentralized data , ” 2016 . [ 2 ] B . McMahan , E . Moore , D . Ramage , S . Hampson , and B . A . y Arcas , “Communication - efﬁcient learning of deep networks from decentralized data , ” in Artiﬁcial Intelligence and Statistics . PMLR , 2017 , pp . 1273 – 1282 . [ 3 ] H . Rutishauser , “Theory of gradient methods , ” in Reﬁned iterative methods for computation of the solution and the eigenvalues of self - adjoint boundary value problems . Springer , 1959 , pp . 24 – 49 . [ 4 ] B . T . Polyak , “Some methods of speeding up the convergence of iter - ation methods , ” USSR Computational Mathematics and Mathematical Physics , vol . 4 , no . 5 , pp . 1 – 17 , 1964 . [ 5 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton , “Imagenet classiﬁcation with deep convolutional neural networks , ” in Advances in neural infor - mation processing systems , 2012 , pp . 1097 – 1105 . [ 6 ] I . Sutskever , J . Martens , G . Dahl , and G . Hinton , “On the importance of initialization and momentum in deep learning , ” in International conference on machine learning , 2013 , pp . 1139 – 1147 . [ 7 ] G . Goh , “Why momentum really works , ” Distill , 2017 . [ Online ] . Available : http : / / distill . pub / 2017 / momentum [ 8 ] Y . Yan , T . Yang , Z . Li , Q . Lin , and Y . Yang , “A uniﬁed analysis of stochastic momentum methods for deep learning , ” arXiv preprint arXiv : 1808 . 10396 , 2018 . [ 9 ] Y . Nesterov , “A method for unconstrained convex minimization problem with the rate of convergence o ( 1 / k2 ) , ” Doklady ANSSSR ( translated as Soviet . Math . Docl . ) , vol . 269 , pp . 543 – 547 , 1983 . [ 10 ] J . Wang and G . Joshi , “Cooperative sgd : A uniﬁed framework for the design and analysis of communication - efﬁcient sgd algorithms , ” arXiv preprint arXiv : 1808 . 07576 , 2018 . [ 11 ] P . Jiang and G . Agrawal , “A linear speedup analysis of distributed deep learning with sparse and quantized communication , ” in Advances in Neural Information Processing Systems , 2018 , pp . 2525 – 2536 . [ 12 ] H . Yu , S . Yang , and S . Zhu , “Parallel restarted sgd with faster con - vergence and less communication : Demystifying why model averaging works for deep learning , ” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol . 33 , 2019 , pp . 5693 – 5700 . [ 13 ] S . Wang , T . Tuor , T . Salonidis , K . K . Leung , C . Makaya , T . He , and K . Chan , “Adaptive federated learning in resource constrained edge com - puting systems , ” IEEE Journal on Selected Areas in Communications , vol . 37 , no . 6 , pp . 1205 – 1221 , 2019 . [ 14 ] W . Su , S . Boyd , and E . Candes , “A differential equation for modeling nesterovs accelerated gradient method : Theory and insights , ” in Ad - vances in neural information processing systems , 2014 , pp . 2510 – 2518 . [ 15 ] H . Attouch and J . Peypouquet , “The rate of convergence of nesterov’s accelerated forward - backward method is actually faster than 1 / kˆ2 , ” SIAM Journal on Optimization , vol . 26 , no . 3 , pp . 1824 – 1834 , 2016 . [ 16 ] T . Yang , Q . Lin , and Z . Li , “Uniﬁed convergence analysis of stochastic momentum methods for convex and non - convex optimization , ” arXiv preprint arXiv : 1604 . 03257 , 2016 . [ 17 ] M . Assran and M . Rabbat , “On the convergence of nesterov’s accelerated gradient method in stochastic settings , ” arXiv preprint arXiv : 2002 . 12414 , 2020 . [ 18 ] J . Wang , V . Tantia , N . Ballas , and M . Rabbat , “Slowmo : Improving communication - efﬁcient distributed sgd with slow momentum , ” arXiv preprint arXiv : 1910 . 00643 , 2019 . [ 19 ] W . Liu , L . Chen , Y . Chen , and W . Zhang , “Accelerating federated learn - ing via momentum gradient descent , ” IEEE Transactions on Parallel and Distributed Systems , vol . 31 , no . 8 , pp . 1754 – 1766 , 2020 . [ 20 ] C . Dinh , N . H . Tran , M . N . Nguyen , C . S . Hong , W . Bao , A . Zomaya , and V . Gramoli , “Federated learning over wireless net - works : Convergence analysis and resource allocation , ” arXiv preprint arXiv : 1910 . 13067 , 2019 . [ 21 ] L . Liu , J . Zhang , S . Song , and K . B . Letaief , “Client - edge - cloud hierarchical federated learning , ” in ICC 2020 - 2020 IEEE International Conference on Communications ( ICC ) . IEEE , 2020 , pp . 1 – 6 . [ 22 ] Y . Bengio , N . Boulanger - Lewandowski , and R . Pascanu , “Advances in optimizing recurrent networks , ” in 2013 IEEE International Conference on Acoustics , Speech and Signal Processing . IEEE , 2013 , pp . 8624 – 8628 . [ 23 ] F . - F . Li , R . Krishna , D . Xu , and A . Byun , CS231n Convolutional Neural Networks for Visual Recognition , 2020 . [ Online ] . Available : https : / / cs231n . github . io / neural - networks - 3 / [ 24 ] I . Goodfellow , Y . Bengio , and A . Courville , Deep learning . MIT press , 2016 . [ 25 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner , “Gradient - based learning applied to document recognition , ” Proceedings of the IEEE , vol . 86 , no . 11 , pp . 2278 – 2324 , 1998 . [ 26 ] A . Krizhevsky , G . Hinton et al . , “Learning multiple layers of features from tiny images , ” 2009 . [ 27 ] T . Ryffel , A . Trask , M . Dahl , B . Wagner , J . Mancuso , D . Rueckert , and J . Passerat - Palmbach , “A generic framework for privacy preserving deep learning , ” arXiv preprint arXiv : 1811 . 04017 , 2018 . [ Online ] . Available : https : / / github . com / OpenMined / PySyft [ 28 ] yeggasd , A . Trask , and froessler , Part 6 - Federated Learning on MNIST using a CNN , 2019 . [ Online ] . Available : https : / / github . com / OpenMined / PySyft / blob / master / examples / tutorials / [ 29 ] S . Ruder , “An overview of gradient descent optimization algorithms , ” arXiv preprint arXiv : 1609 . 04747 , 2016 . [ 30 ] S . Bubeck , “Convex optimization : Algorithms and complexity , ” arXiv preprint arXiv : 1405 . 4980 , 2014 .