K - means + + : The Advantages of Careful Seeding Sergei Vassilvitskii David Arthur ( Stanford university ) Given points in split them into similar groups . Clustering n R d k Given points in split them into similar groups . Clustering n R d k This talk : k - means clustering : Find centers , that minimize k C ! x ∈ X min c ∈C ! x − c ! 2 2 Objective : Find centers , that minimize Why Means ? k C ! x ∈ X min c ∈C ! x − c ! 2 2 For one cluster : Find that minimizes y ! x ∈ X ! x − y ! 2 2 Easy ! y = 1 | X | ! x ∈ X x Lloyd’s Method : k - means Initialize with random clusters Lloyd’s Method : k - means Assign each point to nearest center Lloyd’s Method : k - means Recompute optimum centers ( means ) Lloyd’s Method : k - means Repeat : Assign points to nearest center Lloyd’s Method : k - means Repeat : Recompute centers Lloyd’s Method : k - means Repeat . . . Lloyd’s Method : k - means Repeat . . . Until clustering does not change Analysis How good is this algorithm ? Finds a local optimum That is potentially arbitrarily worse than optimal solution Approximating k - means • Mount et al . : approximation in time • Har Peled et al . : in time • Kumar et al . : in time 9 + ! O ( n 3 / ! d ) 1 + ! O ( n + k k + 2 ! − 2 dk log k ( n / ! ) ) 1 + ! 2 ( k / ! ) O ( 1 ) nd Approximating k - means • Mount et al . : approximation in time • Har Peled et al . : in time • Kumar et al . : in time Lloyd’s method : • Worst - case time complexity : • Smoothed complexity : 9 + ! O ( n 3 / ! d ) 1 + ! O ( n + k k + 2 ! − 2 dk log k ( n / ! ) ) 1 + ! 2 ( k / ! ) O ( 1 ) nd 2 Ω ( √ n ) n O ( k ) Approximating k - means • Mount et al . : approximation in time • Har Peled et al . : in time • Kumar et al . : in time Lloyd’s method : 9 + ! O ( n 3 / ! d ) 1 + ! O ( n + k k + 2 ! − 2 dk log k ( n / ! ) ) 1 + ! 2 ( k / ! ) O ( 1 ) nd For example , Digit Recognition dataset ( UCI ) : n = 60 , 000 d = 600 Convergence to a local optimum in 60 iterations . Challenge Develop an approximation algorithm for k - means clustering that is competitive with the k - means method in speed and solution quality . Easiest line of attack : focus on the initial center positions . Classical k - means : pick points at random . k k - means on Gaussians k - means on Gaussians Easy Fix Select centers using a furthest point algorithm ( 2 - approximation to k - Center clustering ) . Easy Fix Select centers using a furthest point algorithm ( 2 - approximation to k - Center clustering ) . Easy Fix Select centers using a furthest point algorithm ( 2 - approximation to k - Center clustering ) . Easy Fix Select centers using a furthest point algorithm ( 2 - approximation to k - Center clustering ) . Easy Fix Select centers using a furthest point algorithm ( 2 - approximation to k - Center clustering ) . Sensitive to Outliers Sensitive to Outliers Sensitive to Outliers Interpolate between the two methods : Let be the distance between and the nearest cluster center . Sample proportionally to ( D ( x ) ) α = D α ( x ) k - means + + D ( x ) x Original Lloyd’s : α = 0 Contribution of to the overall error x α = ∞ Furthest Point : α = 2 k - means + + : k - Means + + k - Means + + Theorem : k - means + + is approximate in expectation . Θ ( log k ) Ostrovsky et al . [ 06 ] : Similar method is approximate under some data distribution assumptions . O ( 1 ) Proof - 1st cluster Fix an optimal clustering . C ∗ Pick first center uniformly at random Bound the total error of that cluster . Proof - 1st cluster Let be the cluster . Each point equally likely to be the chosen center . A a 0 ∈ A = 2 ! a ∈ A ! a − ¯ A ! 2 = 2 φ ∗ ( A ) E [ φ ( A ) ] = ! a 0 ∈ A 1 | A | ! a ∈ A ! a − a 0 ! 2 Expected Error : Proof - Other Clusters Suppose next center came from a new cluster in OPT . Bound the total error of that cluster . Other CLusters Let be this cluster , and the point selected . Then : B b 0 E [ φ ( B ) ] = ! b 0 ∈ B D 2 ( b 0 ) " b ∈ B D 2 ( b ) · ! b ∈ B min ( D ( b ) , ! b − b 0 ! ) 2 Key step : D ( b 0 ) ≤ D ( b ) + " b − b 0 " Cont . For any b : D 2 ( b 0 ) ≤ 2 D 2 ( b ) + 2 " b − b 0 " 2 Same for all b 0 Cost in uniform sampling D 2 ( b 0 ) ≤ 2 | B | ! b ∈ B D 2 ( b ) + 2 | B | ! b ∈ B " b − b 0 " 2 Avg . over all b : D 2 ( b 0 ) ≤ 2 | B | ! b ∈ B D 2 ( b ) + 2 | B | ! b ∈ B " b − b 0 " 2 Avg . over all b : Cont . For any b : D 2 ( b 0 ) ≤ 2 D 2 ( b ) + 2 " b − b 0 " 2 Recall : E [ φ ( B ) ] = ! b 0 ∈ B D 2 ( b 0 ) " b ∈ B D 2 ( b ) · ! b ∈ B min ( D ( b ) , ! b − b 0 ! ) 2 ≤ 4 | B | ! b 0 ∈ B ! b ∈ B " b − b 0 " 2 = 8 φ ∗ ( B ) Wrap Up If clusters are well separated , and we always pick a center from a new optimal cluster , the algorithm is - competitive . 8 Wrap Up If clusters are well separated , and we always pick a center from a new optimal cluster , the algorithm is - competitive . Intuition : if no points from a cluster are picked , then it probably does not contribute much to the overall error . 8 Wrap Up If clusters are well separated , and we always pick a center from a new optimal cluster , the algorithm is - competitive . Intuition : if no points from a cluster are picked , then it probably does not contribute much to the overall error . Formally , an inductive proof shows this method is competitive . 8 Θ ( log k ) Experiments Tested on several datasets : Synthetic • 10k points , 3 dimensions Cloud Cover ( UCI Repository ] • 10k points , 54 dimensions Color Quantization • 16k points , 16 dimensions Intrusion Detection ( KDD Cup ) • 500k points , 35 dimensions Typical Run KM + + v . KM v . KM - Hybrid 600 700 800 900 1000 1100 1200 1300 0 50 100 150 200 250 300 350 400 450 500 Stage E rr o r LLOYD HYBRID KM + + Experiments Total Error Time : k - means + + 1 % slower due to initialization . k - means km - Hybrid k - means + + Synthetic Cloud Cover Color Intrusion 6 . 02 × 10 5 5 . 95 × 10 5 6 . 06 × 10 5 670 741 712 0 . 016 0 . 015 0 . 014 32 . 9 × 10 3 3 . 4 × 10 3 − Final Message Friends don’t let friends use k - means . Thank You Any Questions ?