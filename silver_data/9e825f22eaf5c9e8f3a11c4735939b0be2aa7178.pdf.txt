Resource - rational Task Decomposition to Minimize Planning Costs Carlos G . Correa ∗ ( cgcorrea @ princeton . edu ) Princeton Neuroscience Institute , Princeton University , Princeton , NJ 08542 USA Mark K . Ho ∗ ( mho @ princeton . edu ) Fred Callaway ( fredcallaway @ princeton . edu ) Thomas L . Grifﬁths ( tomg @ princeton . edu ) Department of Psychology , Princeton University , Princeton , NJ 08542 USA ∗ Contributed equally Abstract People often plan hierarchically . That is , rather than planning over a monolithic representation of a task , they decompose the task into simpler subtasks and then plan to accomplish those . Although much work explores how people decompose tasks , there is less analysis of why people decompose tasks in the way they do . Here , we address this question by formalizing task decomposition as a resource - rational representation prob - lem . Speciﬁcally , we propose that people decompose tasks in a manner that facilitates efﬁcient use of limited cognitive re - sources given the structure of the environment and their own planning algorithms . Using this model , we replicate several existing ﬁndings . Our account provides a normative explana - tion for how people identify subtasks as well as a framework for studying how people reason , plan , and act using resource - rational representations . Keywords : planning ; task decomposition ; option discovery ; hierarchical reinforcement learning ; subgoals Introduction Your brother’s birthday is coming up , so you decide to leave work early to drop his gift off at the post ofﬁce . Although you know your way around town , you haven’t been to the post ofﬁce in several months , so you need to think . In particular , you start to plan : “How do I get to the post ofﬁce from here ? ” you ask yourself . “Well , there’s that caf´e where I sometimes get my morning coffee . If I can ﬁrst get there , then I should be able to get to the post ofﬁce easily . Now , how should I get to the caf ´ e ? I know its east of where I am , so I can walk that way until I hit the main road . . . ” Continuing this line of thought for a few moments , you come up with a plan before setting off with determination . The seemingly mundane choice to navigate to the caf´e be - fore navigating from the caf´e to the post ofﬁce is an example of task decomposition . That is , rather than reason about a task in its totality ( e . g . , going from work to the post ofﬁce ) , people decompose a task into manageable subtasks ( e . g . , going from work to the caf´e ; going from the caf´e to the post ofﬁce ) and then reason in terms of those subtasks . Planning at multiple levels of abstraction has been extensively documented in psy - chology and neuroscience ( Botvinick et al . , 2009 ) and plays an important role in developing systems that can solve com - plex , high - dimensional problems ( Sacerdoti , 1974 ) . In short , hierarchical planning and decision - making is a key element of intelligent behavior in both humans and machines . Although much research has explored how people leverage hierarchical representations ( Ribas - Fernandes et al . , 2011 ; Cushman & Morris , 2015 ; Balaguer et al . , 2016 ) , there has been less systematic investigation into the principles that de - termine task decompositions in the ﬁrst place . There are a few notable exceptions , including accounts that emphasize the value of inferring the hidden structure to guide behav - ior across tasks ( Collins & Frank , 2013 ; Tomov et al . , 2020 ) as well as accounts based on compressing a representation of optimal behavior ( Solway et al . , 2014 ; Maisto et al . , 2015 ) . But , whereas these existing models emphasize decomposi - tion in relation to statistical inference about the environment or behavior , our account focuses on a separate role that task decomposition plays : It makes reasoning easier . Here , we approach task decomposition as a resource - rational representation problem . That is , we model people as solving the problem of how to break down a task in a manner that makes efﬁcient use of planning resources . In the follow - ing sections , we provide background on related work before discussing the mathematical details of our normative account of task decomposition . We then report several simulations and show how our model can explain human data from four experiments reported by Solway et al . ( 2014 ) . Finally , we conclude by discussing future directions for resource - rational approaches to problem solving representations . Background Planning is hard because of the curse of dimensionality ( Bell - man , 1957 ) : As one attempts to plan into an increasingly distant future , over a larger state space , or under condi - tions of greater uncertainty , computation quickly becomes intractable . Nonetheless , humans have numerous strategies that allow us to plan in complex domains . Some of these strategies involve modifying the search process . For exam - ple , during search , people have been shown to limit their depth of planning ( MacGregor et al . , 2001 ; Keramati et al . , 2016 ) , prune away unpromising paths ( Huys et al . , 2012 ) , and direct their search using model - free value estimates ( An - derson , 1990 ; Newell & Simon , 1972 ; van Opheusden et al . , 2017 ) . Another strategy is to modify the problem representa - tion itself . Various forms of hierarchical planning ( Botvinick , 2012 ) and task decomposition ( Solway et al . , 2014 ; Huys et al . , 2015 ) are characteristic of this approach . But while these two types of strategies are distinct , they are also clearly in - tertwined : How one represents a problem can make search anywhere from impossible to trivial ( Kaplan & Simon , 1990 ) . a r X i v : 2007 . 13862v1 [ c s . A I ] 27 J u l 2020 The present work takes inspiration from the deep rela - tionship between search—a type of computation—and task decomposition—a type of representation—in the cognitively demanding setting of planning . Task representations can play a key role in making problem - solving computations more ef - ﬁcient ( Ho et al . , 2019 ) , and identifying general principles for automatically learning such representations is an active area of research in artiﬁcial intelligence . For instance , Jin - nai et al . ( 2019 ) examine how penalizing dynamic program - ming iterations can guide decomposition , while Harb et al . ( 2018 ) introduce a deliberation cost for switching subtasks to help shape a decomposition . Here , we extend these ideas by analyzing human task decomposition in terms of search costs . Broadly , our approach is in the spirit of resource - rational analysis ( Grifﬁths et al . , 2015 ; Lieder & Grifﬁths , 2020 ) , a formal framework for deriving cognitive models un - der the assumption that people make rational use of their lim - ited cognitive resources . Previous resource - rational analyses of planning have focused primarily on the search process it - self ( e . g . , Callaway et al . , 2018 ) . However , the interdepen - dence of computations and the representations over which they operate means that this general framework can be readily applied to the latter . Resource - Rational Task Decomposition By treating task decomposition as a resource - rational prob - lem , we assume people acquire task representations that en - able them to plan efﬁciently and perform tasks successfully . Our account distinguishes between three nested levels of op - timization ( Figure 1A ) . The lowest level is action - level plan - ning , where concrete actions are chosen that solve a subtask ( e . g . , which direction should I walk to get to the caf ´ e ) . The next level is subtask - level planning , where a sequence of sub - tasks is chosen ( e . g . , navigating to the caf ´ e and then to the post ofﬁce ) . Finally , the highest level is task decomposition , where a set of subtasks that constitute the decomposed task is chosen ( e . g . , setting the caf ´ e as a possible subgoal across multiple tasks ) . Importantly , solutions to the higher levels of optimization depend on what happens at lower levels : A good task de - composition depends on how the subtask - level planner will compose the subtasks , and the selection of subtasks depends on how the action - level planner will accomplish each one . Furthermore , a resource - rational task decomposition is sen - sitive not only to how well the planners solve their subprob - lems ( e . g . , does action - level planning identify a good route to the caf´e ? ) , but also on the computational cost of identify - ing those solutions ( e . g . how much thought did it take to ﬁnd that route ? ) . Next , we discuss each of the three levels of our model . Action - level Planning Action - level planning computes the optimal actions that one should take to reach a subgoal . Here , we focus on determin - istic , shortest path problems ( e . g . , ﬁnding a route to the caf´e ) . Formally , action - level planning occurs over a task deﬁned by a set of states , S ; a transition graph , T ⊆ S × S ; and a sub - goal state , z ∈ S . In our running example , states are possible locations ( e . g . , at the ofﬁce , at work , at the caf´e , at the post of - ﬁce ) ; the transition graph represents which locations in town are accessible to one another ; and a subgoal could be the caf´e . Given an initial state , s , and a subgoal , z , action - level planning seeks to ﬁnd a minimum - length sequence of states that begins at s and ends at z . We denote the length of this minimum - length sequence to be D ( s , z ) . For comput - ing the optimal sequence of actions , we consider two broad classes of search algorithms : uninformed search and heuristic search ( Newell & Simon , 1972 ; Russell & Norvig , 2009 ) . Uninformed Search When faced with a domain that lacks features to guide exploration , the best that a planning agent can do is blindly but systematically explore their model of the problem starting from an initial state . This strategy de - scribes a broad class of search algorithms known as unin - formed search . For example , breadth - ﬁrst search ( BFS ) ex - plores states in the order of their distance from the starting state . As a result , for an initial node s and subgoal node z , BFS will explore all states that are less than the minimum path length D ( s , z ) , as shown in Figure 1B . The cost of BFS , C BFS ( s , z ) , is proportional to the number of these states . Heuristic Search Unlike uninformed search , heuristic search leverages domain knowledge in the form of a heuris - tic function that can provide an optimistic estimate of the dis - tance to a goal . For instance , when navigating to the caf´e , you might know that it is North - East of work , leading you to consider walking North or East before South or West . The canonical heuristic search algorithm is A ∗ ( Hart et al . , 1968 ) , which considers states in the order of an optimistic estimate of the total cost of a solution passing through that state ( Fig - ure 1C ) . This estimate is the cost of reaching that state plus a lower bound on the cost from that state to the goal , which is given by the heuristic function . For example , when navigat - ing to the caf ´ e , one might use Euclidean distance as a heuris - tic , which is optimistic because it assumes you can walk di - rectly towards your destination ( e . g . , no obstacles will be in the way ) . By prioritizing states that are more promising ( as estimated by the heuristic ) , A ∗ can search far fewer states than BFS , resulting in a lower search cost , C A ∗ ( s , z ) . Subtask - Level Planning A number of formalisms have been used to model hierarchi - cal decision - making ( Sutton et al . , 1999 ; Dietterich , 2000 ; Parr & Russell , 1998 ) . Here , we assume a simple model of hierarchical planning that involves only a single level above action - level planning , which we call subtask - level planning . Formally , subtask - level planning occurs over a set of sub - goals , Z ⊂ S . 1 Given a set of subgoals , subtask - level plan - ning consists of choosing the best sequence of subgoals that accomplish a larger goal . Each subgoal is then provided to the action - level planner in turn , and the resulting action - level plans are combined into a complete plan to reach the goal Task TaskDecomposition Subtask - Level Planning Action - Level Planning MonolithicTask ( A ) ( B ) Breadth - First Search ( C ) A ∗ Search S S ( D ) Figure 1 : ( A ) Our account relates three levels of optimization during problem solving : task decomposition , subtask - level planning , and action - level planning . ( B ) Top : Planning with breadth - ﬁrst search ( BFS ) from a start state ( small circle ) to the goal state ( star ) with no subgoals . Grey squares indicate nodes visited during search process ( 49 nodes visited ) . Bottom : BFS with the optimal size 1 task decomposition , where the green circle represents a subgoal ( 26 nodes visited ) . ( C ) Top : Planning with A ∗ search with a Manhattan distance heuristic and no subgoals ( 28 nodes visited ) . Bottom : A ∗ search with the optimal size 1 task decomposition ( 13 nodes visited ) . ( D ) Indoor / Outdoor domain with optimal size 2 task decomposition . Top : The three dots represent possible start / goal states . The green and red tiles are the non - trivial subgoals in the optimal task decomposition . Bottom : Subtask - level policy when the center tile is the goal ( star ) and the diagonal corners are possible start states ( “S” ) . Colors correspond to the most likely subgoal chosen at each state . Purple is the trivial “go to goal” subtask . state . For example , when navigating to the post ofﬁce , the subtask - level planner might decide to ﬁrst go to the caf´e and then go to the post ofﬁce from there , and the action - level plan - ner would ﬁgure out the precise sequence of steps to get from work to the caf ´ e and from the caf ´ e to the post ofﬁce . The objective of the subtask - level planner is to identify the sequence of subgoals that brings the agent to the goal state while maximizing task rewards and minimizing com - putational costs . Here , we focus on tasks in which the task is simply to reach the goal state in as few steps as possible . Additionally , note that we only consider action - level planners that return the optimal shortest path . Thus , formally , the task reward associated with choosing a subgoal z from state s is the negative distance : R ( s , z ) = − D ( s , z ) . We can then compactly express the optimization problem faced by the subtask - level planner as a Bellman equation ( Bellman , 1957 ) . Given a task goal g , a set of subgoals Z , and an algorithm with a search cost function C Alg , the optimal subtask - level planning utility from any state s ∈ S is : V g Z ( s ) = max z ∈ Z (cid:8) R ( s , z ) − C Alg ( s , z ) + V g Z ( z ) (cid:9) . ( 1 ) The ﬁxed point of Equation 1 can be used to identify the op - timal subtask - level policy ( Puterman , 1994 ) . Additionally , we assume that the ultimate goal , g , is always included in Z to ensure that it is possible for the subtask - level planner 1 For readers familiar with the options framework ( Sutton et al . , 1999 ) , we note that what we call a subgoal is equivalent to a simple option where the set of initial states is the full state space , S , and the termination function is β ( s ) = 1 ( s = z ) . This means that subtask - level planning is a semi - Markov decision process . to solve the task . Finally , although we do not explore this possibility here , note that this formulation allows us to eas - ily express tradeoffs between task rewards , R , and algorithm - speciﬁc computation costs , C Alg . Task Decomposition Having deﬁned action - level planning and subtask - level plan - ning over subgoals , we can now turn to our original motivat - ing question : How should people decompose tasks ? In this context , this reduces to the problem of selecting the best set of subgoals . Importantly , we assume that people rely on a com - mon set of subgoals for all the different possible tasks that they might have to accomplish in a given environment . For example , be at the downtown train station is a good subgoal because it is often along relatively - optimal paths , whereas be at a friend’s place on the other side of town is probably not a good subgoal because it is only relevant when visiting that friend . We formalize subgoal selection as an optimization prob - lem : Identify the set of subgoals , Z ∗ , that maximize the value attained by the subtask - level planner on average . That is , Z ∗ = argmax Z E s , g [ V g Z ( s ) ] , ( 2 ) where the expectation is with respect to a task distribution , p ( s , g ) , over starting states s and goals g . Importantly , this objective takes into account both the expected task rewards and the costs of action - level planning mediated by subtask - level planning . ( A ) ( B ) ( C ) Affirm Reject 1 2 3 4 5 6 E x e c u t i o n C o s t Model Bottleneck False True ( D ) Figure 2 : Analyzing Solway et al . ( 2014 ) Experiments 1 - 3 with uninformed search . ( A ) Top : Solway et al . ( 2014 ) decomposi - tion of graph from Schapiro et al . ( 2013 ) . Bottom : Resource - rational task decomposition . ( B ) Top : Graph used in Experiment 1 from Solway et al . with proportion of “bus stops” placed at states . Bottom : Subgoals do not facilitate more efﬁcient breadth - ﬁrst search ( BFS ) in this graph , so our model does not learn to use any states as subgoals . ( C ) Top : Graph for Experiment 2 with bottlenecks depicted . Same graph was used in Experiment 3 . Bottom : Distribution over subgoals z from softmax of value of subgoal with inverse temperature of 100 . Task decomposition facilitates more efﬁcient BFS in this graph . In particular , the “bottleneck state” connecting the two regions is the optimal subgoal , followed by the adjacent states . ( D ) Top : Experiment 3 in Solway et al . probed whether participants’ plans included states in the graph . Responses were signiﬁcantly faster for bottle - neck ( purple ) compared to non - bottleneck states ( blue ) . Bottom : Simulations of probe response computations in experimental trials with the optimal task decomposition learned by our model . Our model takes fewer steps to respond when the probe is a bottleneck state , mirroring the experimental ﬁndings . Implementation The code for all the analyses we report is available at https : / / bit . ly / 2T44Tun . Here , we brieﬂy describe the imple - mentation . For both BFS and A ∗ , we calculated action - level computational costs C Alg ( s , z ) and minimum path lengths D ( s , z ) for every state s ∈ S and subgoal z ∈ S . With these quantities , a set of subgoals Z , and distribution over goals and starting states p ( s , g ) , we can deﬁne the optimal ex - pected subtask - level planning value function , E s , g (cid:2) V g Z ( s ) (cid:3) ( see Equation 1 ) . We compute this function using value it - eration with a threshold of ε = 10 − 5 ( Bellman , 1957 ) . Finally , to solve for the optimal set of subgoals , Z ∗ , we explored two methods . The ﬁrst was an exact method— enumeration and evaluation of all subgoal sets . The sec - ond was a gradient - based method . This method used a dif - ferentiable version of value iteration at the subtask - planning stage ( Haarnoja et al . , 2017 ; Ho et al . , 2020 ) and distribu - tions over subgoals instead of discrete subgoals at the task de - composition level . While enumeration is intractable for large state spaces , we found that the methods produced similar re - sults when both were feasible . Thus , we present results using the exact enumeration method when it was computationally tractable—for the small environments used in Solway et al . ( 2014 ) —and the gradient method otherwise . Gridworld Simulations To illustrate the properties of our model , we begin by an - alyzing optimal decompositions of simple gridworld tasks . The grids we tested include Open Field , 2 - Room , and In - door / Outdoor . We tested our model with both BFS and A ∗ . As shown in Figure 1 , our model produces intuitive task de - compositions as a function of a task and planning algorithm . Analysis of Solway et al . ( 2014 ) Experiments Solway et al . ( 2014 ) reported four studies that investigated how people decompose tasks and engage in hierarchical plan - ning . Here , we ask if our resource - rational model can ac - count for these ﬁndings . We ﬁrst discuss Experiments 1 - 3 ( Figure 2 ) , which relied on tasks in which people could not leverage prior knowledge and then turn to Experiment 4 ( Fig - ure 3 ) , which used the Tower of Hanoi ( Nilsson , 1971 ) , a task that allows for the use of prior knowledge . Experiments 1 - 3 : Uninformed Search Summary of Findings Experiments 1 - 3 reported by Sol - way et al . ( 2014 ) experimentally tested the hierarchical struc - ture used by participants when performing navigation tasks over abstract state spaces . Figures 2B and 2C show the con - nectivity structure of the domains people were given . A key qualitative ﬁnding reported by Solway et al . ( 2014 ) was that people’s responses reﬂected a decomposition of the ( A ) ( B ) Edit Distance to Subgoal = 1 = 2 = 3 S ( C ) ( D ) Figure 3 : Analyzing Solway et al . ( 2014 ) Experiment 4 . ( A ) Hierarchically optimal ( green ) vs . non - hierarchically optimal ( red ) paths in Tower of Hanoi . ( B ) Optimal task decomposition with BFS has one bottleneck subgoal and two neighboring non - bottleneck states as subgoals . ( C ) Edit distance to a subgoal ( green circle ) . Edit distance provides useful local information for heuristic search . ( D ) Optimal task decomposition with A * and edit distance heuristic has three equidistant bottleneck states as subgoals . state space based on bottlenecks , states or transitions that con - nect more densely - connected regions of a state space ( S¸ims¸ek & Barto , 2009 ) . A preliminary modeling result they reported recovered a task decomposition along community boundaries in a graph ( Figure 2A ) studied in Schapiro et al . ( 2013 ) . Ex - periment 1 assessed this in the task with the transition struc - ture in Figure 2B ( top ) by having participants choose a “bus stop” that would be most useful for making “deliveries” be - tween locations represented by the icons . Participants over - whelmingly chose the bottleneck states , as displayed in the ﬁgure . Experiment 2 had participants actually navigate to and from random locations in the task with the structure in Fig - ure 2C . However , on test trials , participants were asked to ei - ther identify locations along the path in any order or identify a single location on the path . Participants tended to report bottleneck states ﬁrst , suggesting they were thinking about these states ﬁrst in their planning process . Finally , Experi - ment 3 used the same domain as Experiment 2 , but partici - pants were probed about whether a state was on the optimal path between two states . Participants answered faster for bot - tleneck states compared to non - bottleneck states , providing additional evidence that these states are the ﬁrst to come to mind ( Figure 2D ) . Model Implementation and Results For the Schapiro et al . graph ( Figure 2A ) , we assumed a uniform distribution over all start and goal states , and set the number of subgoals to | Z | = 3 . The best subgoals separated the three communities at their boundaries , allowing the action - planner to ﬁrst ﬁnd the community containing the goal and then search for the goal within that community . For the second graph ( Figure 2B ; Solway et al . Experiment 1 ) , we again assumed a uniform distribution over all start and goal states , and set the number of subgoals to | Z | = 1 . Inter - estingly , no set of subgoals achieved greater value than plan - ning without subgoals : Z ∗ = / 0 . Thus , the model did not re - ﬂect the empirical results . Although “bus stop” judgments recovered bottleneck states , they may reﬂect a process that is distinct from task decomposition for planning . Further exper - iments are needed to evaluate this . We made the same assumptions for the third graph ( Fig - ure 2C ; Experiments 2 & 3 ) , and found that the bottleneck state was always in the optimal decomposition . The next - best decompositions all included one of the four states connected to the bottleneck state , indicating that navigating near the bot - tleneck state is a useful subgoal in this task . To replicate the reaction time results reported in Experi - ment 3 ( Figure 2D ) , we simulated hierarchical planning with the optimal decomposition using trials as described by Sol - way et al . Speciﬁcally , the model ﬁrst constructed a subtask - level plan . If the queried state was a subgoal state , the model replied “Afﬁrm” as soon as the subtask - level planner encoun - tered the state and “Reject” if the state was not in the com - pleted subtask - level plan . If the queried state was not a sub - goal state , the model proceeded to construct each action - level plan in turn . As soon as the queried state was encountered , the model replied “Afﬁrm” . If the ﬁnal action - level plan to the goal was completed without encountering the state , the model replied “Reject” . In either case , we used the total number of subgoals and states that were simulated before the response was produced as a proxy for reaction time . These results are plotted for bottleneck vs . non - bottleneck probes and “Afﬁrm” vs . “Reject” type probes in Figure 2D . Experiment 4 : Tower of Hanoi and Heuristic Planning Summary of Findings In a ﬁnal experiment , Solway et al . tested participants solving the Tower of Hanoi ( Nilsson , 1971 ) . The experiment focused on “problems of interest” , trials where two paths of the same length led to a goal but one crossed more bottleneck states . They found that participants preferred to take paths that crossed fewer bottleneck states . Assuming that people prefer hierarchically shorter paths ( i . e . , ones that use fewer subgoals ) , this has been taken to reﬂect a decomposition of the task based on bottleneck states . Model Implementation and Results The Tower of Hanoi is an important contrast to the tasks in the ﬁrst three exper - iments because states have features that provide clues for search . For example , the edit distance between two states pro - vides an optimistic estimate of their minimum path length : it ignores that some transitions are forbidden and assumes you can rearrange blocks arbitrarily . Much like how spatial dis - tance can guide planning in navigation tasks , heuristics like edit distance can guide problem solving in structured tasks . To understand the relationships between heuristics , task decomposition , and Solway et al . ’s results , we ran several ver - sions of our model on the Tower of Hanoi . We set the number of subgoals to | Z | = 3 and used BFS as the action - level plan - ner . Notably , the optimal subgoals under this scheme were systematically “skewed” , consisting of a bottleneck state and two nearby points ( Figure 3B ) . For our second simulation , we used the same procedure and parameters , but rather than using BFS ( uninformed search ) , we used A ∗ with an edit - distance heuristic for action - level planning ( Figure 3C ) . The top two decompositions both contained three bottleneck states in sep - arate communities ( Figure 3D ) . Unlike BFS , A ∗ can efﬁ - ciently navigate between these points , allowing for a task de - composition that spans the full extent of the problem space . Discussion We have proposed a resource - rational account of task decom - position based on the idea that subgoals are decomposed to make planning easier . Our model speciﬁes three levels of nested optimization : Task decomposition identiﬁes a set of subgoals for a given domain , subtask - level planning chooses sequences of subgoals to reach a goal , and action - level plan - ning chooses sequences of concrete actions to reach a sub - goal . Optimal task decomposition thus depends on both the structure of the environment and the computational resource usage speciﬁc to the planning algorithm . We ﬁnd that our model produces interpretable task decompositions in grid - world tasks and decompositions consistent with three of the four ﬁndings reported by Solway et al . ( 2014 ) . The model presented here departs from and complements other normative proposals in the literature . Most existing ap - proaches pose task decomposition as an inference problem : People are modeled as inferring a generative model of the en - vironment ( Collins & Frank , 2013 ; Tomov et al . , 2020 ) or as compressing optimal behavior ( Solway et al . , 2014 ; Maisto et al . , 2015 ) . In contrast , we pose task decomposition as a resource - rational representation problem : People are mod - eled as having subgoals that reduce the computational over - head of action - level planning . This change in view has sev - eral consequences worth noting . First , unlike inferential approaches that abstract away the underlying reasoning process , our framework requires spec - ifying a planning algorithm . Different assumptions at this lowest level ( e . g . using breadth ﬁrst search or A * ) can dra - matically inﬂuence the task decomposition ( e . g . , Figure 3 ) . On the one hand , this makes model identiﬁcation more chal - lenging since the space of planning algorithms and parame - terizations is vast . On the other hand , because our model is both algorithmic and normative , it can characterize the inter - play of planning computations and representations in a well - posed manner . Additionally , this approach allows us to an - alyze how behavioral suboptimality can arise from rational tradeoffs between task rewards and computation costs associ - ated with particular search algorithms . Future empirical work on resource - rational planning representations will need to ex - amine these questions in greater depth . A second difference is that inferential models generally emphasize learning from task interactions as data , while we have deliberately set aside how resource - rational decompo - sitions are learned . Speciﬁcally , our formulation assumes the existence of an optimization process that can select a de - composition , whether it be through direct experience with a task or other means . Although this temporarily defers impor - tant and interesting questions about online problem solving , characterizing any learning process requires ﬁrst identifying what is being learned ( i . e . , what is being optimized ) . It is in this sense that the model presented here is a resource - rational analysis of task decomposition ( Grifﬁths et al . , 2015 ) . More broadly , the work presented here is consistent with other recent efforts within cognitive science to understand how people engage in computationally efﬁcient decision - making ( Grifﬁths et al . , 2015 ; Lewis et al . , 2014 ; Gershman et al . , 2015 ; Lieder & Grifﬁths , 2020 ) . It is also complementary to recent work in artiﬁcial intelligence that explores the inter - action between planning and task representations ( Jinnai et al . , 2019 ; Harb et al . , 2018 ) . Our hope is that future work on human planning and problem solving will continue to inves - tigate the relationships between computation , representation , and resource - rational decision - making . Acknowledgements This work was supported by NSF grant # 1544924 , AFOSR grant FA9550 - 18 - 1 - 0077 and grant 61454 from the John Templeton Foundation . References Anderson , J . R . ( 1990 ) . The Adaptive Character of Thought . Hillsdale , NJ : Lawrence Erlbaum Associates , Inc . Balaguer , J . , Spiers , H . , Hassabis , D . , & Summerﬁeld , C . ( 2016 ) . Neural mechanisms of hierarchical planning in a virtual subway network . Neuron , 90 ( 4 ) , 893 - 903 . Bellman , R . ( 1957 ) . Dynamic programming . Princeton Uni - versity Press . Botvinick , M . M . ( 2012 ) . Hierarchical reinforcement learn - ing and decision making . Current Opinion in Neurobiol - ogy , 22 ( 6 ) , 956 – 962 . Botvinick , M . M . , Niv , Y . , & Barto , A . C . ( 2009 ) . Hier - archically organized behavior and its neural foundations : A reinforcement learning perspective . Cognition , 113 ( 3 ) , 262 – 280 . Callaway , F . , Lieder , F . , Das , P . , Gul , S . , Krueger , P . , & Grif - ﬁths , T . ( 2018 ) . A resource - rational analysis of human planning . In Proceedings of the Annual Conference of the Cognitive Science Society . Collins , A . G . , & Frank , M . J . ( 2013 ) . Cognitive control over learning : Creating , clustering , and generalizing task - set structure . Psychological Review , 120 ( 1 ) , 190 – 229 . Cushman , F . , & Morris , A . ( 2015 ) . Habitual control of goal selection in humans . Proceedings of the National Academy of Sciences , 112 ( 45 ) , 13817 – 13822 . Dietterich , T . G . ( 2000 ) . Hierarchical Reinforcement Learn - ing with the MAXQ Value Function Decomposition . Jour - nal of artiﬁcial intelligence research , 13 , 227 – 303 . Gershman , S . J . , Horvitz , E . J . , & Tenenbaum , J . B . ( 2015 ) . Computational rationality : A converging paradigm for intelligence in brains , minds , and machines . Science , 349 ( 6245 ) , 273 – 278 . Grifﬁths , T . L . , Lieder , F . , & Goodman , N . D . ( 2015 ) . Ra - tional Use of Cognitive Resources : Levels of Analysis Be - tween the Computational and the Algorithmic . Topics in Cognitive Science , 7 ( 2 ) , 217 – 229 . Haarnoja , T . , Tang , H . , Abbeel , P . , & Levine , S . ( 2017 ) . Re - inforcement learning with deep energy - based policies . In ICML ( pp . 1352 – 1361 ) . Harb , J . , Bacon , P . - L . , Klissarov , M . , & Precup , D . ( 2018 ) . When waiting is not an option : Learning options with a deliberation cost . In Thirty - Second AAAI Conference on Artiﬁcial Intelligence . Hart , P . E . , Nilsson , N . J . , & Raphael , B . ( 1968 ) . A for - mal basis for the heuristic determination of minimum cost paths . IEEE Transactions on Systems Science and Cyber - netics , 4 ( 2 ) , 100 - 107 . Ho , M . K . , Abel , D . , Cohen , J . , Littman , M . L . , & Grifﬁths , T . L . ( 2020 ) . The Efﬁciency of Human Cognition Reﬂects Planned Information Processing . In Thirty - Fourth AAAI Conference on Artiﬁcial Intelligence . Ho , M . K . , Abel , D . , Grifﬁths , T . L . , & Littman , M . L . ( 2019 ) . The value of abstraction . Current Opinion in Behavioral Sciences , 29 , 111 – 116 . Huys , Q . J . M . , Eshel , N . , O’Nions , E . , Sheridan , L . , Dayan , P . , & Roiser , J . P . ( 2012 ) . Bonsai trees in your head : how the Pavlovian system sculpts goal - directed choices by pruning decision trees . PLoS Computational Biology , 8 ( 3 ) , e1002410 . Huys , Q . J . M . , Lally , N . , Faulkner , P . , Eshel , N . , Seifritz , E . , Gershman , S . J . , . . . Roiser , J . P . ( 2015 ) . Interplay of ap - proximate planning strategies . Proceedings of the National Academy of Sciences , 112 ( 10 ) , 3098 – 103 . Jinnai , Y . , Abel , D . , Hershkowitz , D . , Littman , M . , & Konidaris , G . ( 2019 ) . Finding options that minimize plan - ning time . In ICML ( Vol . 97 , pp . 3120 – 3129 ) . Kaplan , C . A . , & Simon , H . A . ( 1990 ) . In search of insight . Cognitive Psychology , 22 ( 3 ) , 374 - 419 . Keramati , M . , Smittenaar , P . , Dolan , R . J . , & Dayan , P . ( 2016 ) . Adaptive integration of habits into depth - limited planning deﬁnes a habitual - goal – directed spectrum . Pro - ceedings of the National Academy of Sciences , 113 ( 45 ) , 12868 – 12873 . Lewis , R . L . , Howes , A . , & Singh , S . ( 2014 ) . Com - putational rationality : Linking mechanism and behavior through bounded utility maximization . Topics in Cognitive Science , 6 ( 2 ) , 279 - 311 . Lieder , F . , & Grifﬁths , T . L . ( 2020 ) . Resource - rational anal - ysis : understanding human cognition as the optimal use of limited computational resources . Behavioral and Brain Sciences , 1 – 60 . MacGregor , J . N . , Ormerod , T . C . , & Chronicle , E . P . ( 2001 ) . Information processing and insight : a process model of performance on the nine - dot and related problems . Jour - nal of Experimental Psychology : Learning , Memory , and Cognition , 27 ( 1 ) , 176 . Maisto , D . , Donnarumma , F . , & Pezzulo , G . ( 2015 ) . Divide et impera : subgoaling reduces the complexity of probabilis - tic inference and problem solving . Journal of The Royal Society Interface , 12 ( 104 ) , 20141335 – 20141335 . Newell , A . , & Simon , H . A . ( 1972 ) . Human problem solving . Englewood Cliffs , NJ : Prentice - Hall . Nilsson , N . J . ( 1971 ) . Problem - solving methods in artiﬁcial intelligence . McGraw - Hill . Parr , R . , & Russell , S . J . ( 1998 ) . Reinforcement learning with hierarchies of machines . In Advances in Neural Informa - tion Processing Systems ( pp . 1043 – 1049 ) . Puterman , M . L . ( 1994 ) . Markov decision processes : Dis - crete stochastic dynamic programming . John Wiley & Sons , Inc . Ribas - Fernandes , J . , Solway , A . , Diuk , C . , McGuire , J . , Barto , A . , Niv , Y . , & Botvinick , M . ( 2011 ) . A neural signa - ture of hierarchical reinforcement learning . Neuron , 71 ( 2 ) , 370 - 379 . Russell , S . , & Norvig , P . ( 2009 ) . Artiﬁcial intelligence : A modern approach ( 3rd ed . ) . USA : Prentice Hall Press . Sacerdoti , E . D . ( 1974 ) . Planning in a hierarchy of abstraction spaces . Artiﬁcial Intelligence , 5 ( 2 ) , 115 – 135 . Schapiro , A . C . , Rogers , T . T . , Cordova , N . I . , Turk - Browne , N . B . , & Botvinick , M . M . ( 2013 ) . Neural representations of events arise from temporal community structure . Nature Neuroscience , 16 ( 4 ) , 486 – 492 . S¸ims¸ek , ¨ O . , & Barto , A . G . ( 2009 ) . Skill characterization based on betweenness . In Advances in Neural Information Processing Systems ( pp . 1497 – 1504 ) . Solway , A . , Diuk , C . , C´ordova , N . , Yee , D . , Barto , A . G . , Niv , Y . , & Botvinick , M . M . ( 2014 ) . Optimal behavioral hierarchy . PLoS Computational Biology , 10 ( 8 ) , e1003779 . Sutton , R . S . , Precup , D . , & Singh , S . ( 1999 ) . Between mdps and semi - mdps : A framework for temporal abstraction in reinforcement learning . Artiﬁcial Intelligence , 112 ( 1 - 2 ) , 181 – 211 . Tomov , M . S . , Yagati , S . , Kumar , A . , Yang , W . , & Gershman , S . J . ( 2020 ) . Discovery of hierarchical representations for efﬁcient planning . PLOS Computational Biology , 16 ( 4 ) . van Opheusden , B . , Galbiati , G . , Bnaya , Z . , Li , Y . , & Ma , W . J . ( 2017 ) . A computational model for decision tree search . In Proceedings of the Annual Conference of the Cognitive Science Society .