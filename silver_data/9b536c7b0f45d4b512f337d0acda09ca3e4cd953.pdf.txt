Fitting linear mixed - eﬀects models using lme4 Douglas Bates U . of Wisconsin - Madison Martin Mächler ETH Zurich Benjamin M . Bolker McMaster University Steven C . Walker McMaster University Abstract Maximum likelihood or restricted maximum likelihood ( REML ) estimates of the pa - rameters in linear mixed - eﬀects models can be determined using the lmer function in the lme4 package for R . As for most model - ﬁtting functions in R , the model is described in an lmer call by a formula , in this case including both ﬁxed - and random - eﬀects terms . The formula and data together determine a numerical representation of the model from which the proﬁled deviance or the proﬁled REML criterion can be evaluated as a function of some of the model parameters . The appropriate criterion is optimized , using one of the constrained optimization functions in R , to provide the parameter estimates . We de - scribe the structure of the model , the steps in evaluating the proﬁled deviance or REML criterion , and the structure of classes or types that represents such a model . Suﬃcient detail is included to allow specialization of these structures by users who wish to write functions to ﬁt specialized linear mixed models , such as models incorporating pedigrees or smoothing splines , that are not easily expressible in the formula language used by lmer . Keywords : sparse matrix methods , linear mixed models , penalized least squares , Cholesky decomposition . Submitted to Journal of Statistical Software 1 . Introduction The lme4 package for R provides functions to ﬁt and analyze linear mixed models , generalized linear mixed models and nonlinear mixed models . In each of these names , the term “mixed” or , more fully , “mixed eﬀects” , denotes a model that incorporates both ﬁxed - and random - eﬀects terms in a linear predictor expression from which the conditional mean of the response can be evaluated . In this paper we describe the formulation and representation of linear mixed models . The techniques used for generalized linear and nonlinear mixed models will be described separately , in a future paper . The development of general software for ﬁtting mixed models remains an active area of re - search with many open problems . Consequently , the lme4 package has evolved since it was ﬁrst released , and continues to improve as we learn more about mixed models . However , we recognize the need to maintain stability and backward compatibility of lme4 so that it continues to be broadly useful . In order to maintain stability while continuing to advance mixed - model computation , we have developed several additional frameworks that draw on the basic ideas of lme4 but modify its structure or implementation in various ways . These descendants include the MixedModels package in Julia , the lme4pureR package in R , and the flexLambda development branch of lme4 . The current article is largely restricted to describ - a r X i v : 1406 . 5823v1 [ s t a t . C O ] 23 J un 2014 2 Linear Mixed Models with lme4 ing the current stable version of the lme4 package ( 1 . 1 - 7 ) , with Appendix A describing hooks into the computational machinery that are designed for extension development . The gamm4 ( Wood and Scheipl 2013 ) and blme ( Dorie 2013 ) packages currently make use of these hooks . Another goal of this article is to contrast the approach used by lme4 with previous formu - lations of mixed models . The expressions for the proﬁled log - likelihood and proﬁled REML ( restricted maximum likelihood ) criteria derived in Section 3 . 4 are similar to those presented in Bates and DebRoy ( 2004 ) and , indeed , are closely related to “Henderson’s mixed - model equations” ( Henderson 1982 ) . Nonetheless there are subtle but important changes in the formulation of the model and in the structure of the resulting penalized least squares ( PLS ) problem to be solved ( Section 3 . 6 ) . We derive the current version of the PLS problem ( Sec - tion 3 . 2 ) and contrast this result with earlier formulations ( Section 3 . 5 ) . This article is organized into four main sections ( 2 ; 3 ; 4 ; 5 ) , each of which corresponds to one of the four largely separate modules that comprise lme4 . Before describing the details of each module , we describe the general form of the linear mixed model underlying lme4 ( Section 1 . 1 ) ; introduce the sleepstudy data that will be used as an example throughout ( Section 1 . 2 ) ; and broadly outline lme4 ’s modular structure ( Section 1 . 3 ) . 1 . 1 . Linear mixed models Just as a linear model is described by the distribution of a vector - valued random response variable , Y , whose observed value is y obs , a linear mixed model is described by the distribution of two vector - valued random variables : Y , the response , and B , the vector of random eﬀects . In a linear model the distribution of Y is multivariate normal , Y ∼ N ( Xβ + o , σ 2 W − 1 ) , ( 1 ) where n is the dimension of the response vector , W is a diagonal matrix of known prior weights , β is a p - dimensional coeﬃcient vector , X is an n × p model matrix , and o is a vector of known prior oﬀset terms . The parameters of the model are the coeﬃcients β and the scale parameter σ . In a linear mixed model it is the conditional distribution of Y given B = b that has such a form , ( Y | B = b ) ∼ N ( Xβ + Zb + o , σ 2 W − 1 ) , ( 2 ) where Z is the n × q model matrix for the q - dimensional vector - valued random eﬀects variable , B , whose value we are ﬁxing at b . The unconditional distribution of B is also multivariate normal with mean zero and a parameterized q × q variance - covariance matrix , Σ , B ∼ N ( 0 , Σ ) . ( 3 ) As a variance - covariance matrix , Σ must be positive semideﬁnite . It is convenient to express the model in terms of a relative covariance factor , Λ θ , which is a q × q matrix , depending on the variance - component parameter , θ , and generating the symmetric q × q variance - covariance matrix , Σ , according to Σ θ = σ 2 Λ θ Λ > θ , ( 4 ) where σ is the same scale factor as in the conditional distribution ( 2 ) . Although equations 2 , 3 , and 4 fully describe the class of linear mixed models that lme4 can ﬁt , this terse description hides many important details . Before moving on to these details , we make a few observations : Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 3 • This formulation of linear mixed models allows for a relatively compact expression for the proﬁled log - likelihood of θ ( Section 3 . 4 , Equation 34 ) . • The matrices associated with random eﬀects , Z and Λ θ , typically have a sparse structure with a sparsity pattern that encodes various model assumptions . Sections 2 . 3 and 3 . 7 provide details on these structures , and how to represent them eﬃciently . • The interface provided by lme4 ’s lmer function is slightly less general than the model described by equations 2 , 3 , and 4 . To take advantage of the entire range of possibili - ties , one may use the modular functions ( Sections 1 . 3 and Appendix A ) or explore the experimental flexLambda branch of lme4 on Github . 1 . 2 . Example Throughout our discussion of lme4 , we will work with a data set on the average reaction time per day for subjects in a sleep deprivation study ( Belenky et al . 2003 ) . On day 0 the subjects had their normal amount of sleep . Starting that night they were restricted to 3 hours of sleep per night . The response variable , Reaction , represents average reaction times in milliseconds ( ms ) on a series of tests given each Day to each Subject ( Figure 1 ) , > str ( sleepstudy ) ' data . frame ' : 180 obs . of 3 variables : $ Reaction : num 250 259 251 321 357 . . . $ Days : num 0 1 2 3 4 5 6 7 8 9 . . . $ Subject : Factor w / 18 levels " 308 " , " 309 " , " 310 " , . . : 1 1 1 1 1 1 . . Each subject’s reaction time increases approximately linearly with the number of sleep - deprived days . However , subjects also appear to vary in the slopes and intercepts of these relationships , which suggests a model with random slopes and intercepts . As we shall see , such a model may be ﬁtted by minimizing the REML criterion ( Equation 39 ) using > fm1 < - lmer ( Reaction ~ Days + ( Days | Subject ) , sleepstudy ) The estimates of the standard deviations of the random eﬀects for the intercept and the slope are 24 . 74 ms and 5 . 92 ms / day . The ﬁxed - eﬀects coeﬃcients , β , are 251 . 4 ms and 10 . 47 ms / day for the intercept and slope . In this model , one interpretation of these ﬁxed eﬀects is that they are the estimated population mean values of the random intercept and slope ( Section 2 . 2 ) . 1 . 3 . High level modular structure The lmer function is composed of four largely independent modules . In the ﬁrst module , a mixed - model formula is parsed and converted into the inputs required to specify a linear mixed model ( Section 2 ) . The second module uses these inputs to construct an R function which takes the covariance parameters , θ , as arguments and returns negative twice the log proﬁled likelihood or the REML criterion ( Section 3 ) . The third module optimizes this objective 4 Linear Mixed Models with lme4 Days of sleep deprivation A v e r age r ea c t i on t i m e ( m s ) 200 250 300 350 400 450 l lllllllll 335 0 2 4 6 8 llllllllll 309 lllllllll l 330 0 2 4 6 8 lllllllll l 331 ll llllllll 310 0 2 4 6 8 l l llllllll 351 lllll lllll 333 0 2 4 6 8 llllllll ll 371 ll l lll l ll l 332 0 2 4 6 8 llllllllll 372 llll lll lll 369 0 2 4 6 8 lllllllll l 334 lllllll l ll 349 0 2 4 6 8 l l llllllll 352 llll l l l lll 370 0 2 4 6 8 lll lllll ll 337 lllll l llll 350 0 2 4 6 8 200 250 300 350 400 450 lll l l l l l l l 308 Figure 1 : Average reaction time versus days of sleep deprivation by subject . Subjects ordered by increasing slope of subject - speciﬁc linear regressions . function to produce maximum likelihood ( ML ) or REML estimates of θ ( Section 4 ) . Finally , the fourth module provides utilities for interpreting the optimized model ( Section 5 ) . To illustrate this modularity , we recreate the fm1 object by a series of four modular steps , > # formula module > parsedFormula < - lFormula ( formula = Reaction ~ Days + ( Days | Subject ) , + data = sleepstudy ) > > # objective function module > devianceFunction < - do . call ( mkLmerDevfun , parsedFormula ) > > # optimization module > optimizerOutput < - optimizeLmer ( devianceFunction ) > > # output module > mkMerMod ( rho = environment ( devianceFunction ) , + opt = optimizerOutput , + reTrms = parsedFormula $ reTrms , + fr = parsedFormula $ fr ) 2 . Formula module Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 5 Module R function Description Formula module ( Section 2 ) lFormula Accepts a mixed - model formula , data , and other user inputs , and returns a list of objects required to ﬁt a linear mixed model . Objective function module ( Section 3 ) mkLmerDevfun Accepts the results of lFormula and returns a function to calcu - late the deviance ( or restricted deviance ) as a function of the covariance parameters , θ . Optimization module ( Section 4 ) optimizeLmer Accepts a deviance function re - turned by mkLmerDevfun and returns the results of the opti - mization of that deviance func - tion . Output module ( Section 5 ) mkMerMod Accepts an optimized deviance function and packages the re - sults into a useful object . Table 1 : The high - level modular structure of lmer . 2 . 1 . Mixed - model formulas Like most model - ﬁtting functions in R , lmer takes as its ﬁrst two arguments a formula spec - ifying the model and the data with which to evaluate the formula . This second argument , data , is optional but recommended and is usually the name of an R data frame . In the R lm function for ﬁtting linear models , formulas take the form resp ~ expr , where resp deter - mines the response variable and expr is an expression that speciﬁes the columns of the model matrix . Formulas for the lmer function contain special random - eﬀects terms , > resp ~ FEexpr + ( REexpr1 | factor1 ) + ( REexpr2 | factor2 ) + . . . where FEexpr is an expression determining the columns of the ﬁxed - eﬀects model matrix , X , and the random - eﬀects terms , ( REexpr1 | factor1 ) and ( REexpr2 | factor2 ) , determine both the random - eﬀects model matrix , Z ( Section 2 . 3 . 1 ) , and the structure of the relative covariance factor , Λ θ ( Section 2 . 3 . 2 ) . In principle , a mixed - model formula may contain ar - bitrarily many random - eﬀects terms , but in practice the number of such terms is typically low . 2 . 2 . Understanding mixed - model formulas Before describing the details of how lme4 parses mixed - model formulas ( Section 2 . 3 ) , we provide an informal explanation and then some examples . Our discussion assumes familiarity with the standard R modelling paradigm ( Chambers 1993 ) . Each random - eﬀects term is of the form ( expr | factor ) . The expression expr is evaluated as a linear model formula , producing a model matrix following the same rules used in standard R modelling functions ( e . g . , lm or glm ) . The expression factor is evaluated as an R factor . 6 Linear Mixed Models with lme4 Formula Alternative Meaning ( 1 | g ) 1 + ( 1 | g ) Random intercept with ﬁxed mean 0 + offset ( o ) + ( 1 | g ) - 1 + offset ( o ) + ( 1 | g ) Random intercept with a priori means ( 1 | g1 / g2 ) ( 1 | g1 ) + ( 1 | g1 : g2 ) Intercept varying among g1 and g2 within g1 ( 1 | g1 ) + ( 1 | g2 ) 1 + ( 1 | g1 ) + ( 1 | g2 ) Intercept varying among g1 and g2 x + ( x | g ) 1 + x + ( 1 + x | g ) Correlated random intercept and slope x + ( x | | g ) 1 + x + ( 1 | g ) + ( 0 + x | g ) Uncorrelated random intercept and slope Table 2 : Examples of the right - hand sides of mixed - eﬀects model formulas . The names of grouping factors are denoted g , g1 , and g2 , and covariates and a priori known oﬀsets as x and o . One way to think about the vertical bar operator is as a special kind of interaction between the model matrix and the grouping factor . This interaction ensures that the columns of the model matrix have diﬀerent eﬀects for each level of the grouping factor . What makes this a special kind of interaction is that these eﬀects are modelled as unobserved random variables , rather than unknown ﬁxed parameters . Much has been written about important practical and philosophical diﬀerences between these two types of interactions ( e . g . , Henderson ( 1982 ) ; Gelman ( 2005 ) ) . For example , the random - eﬀects implementation of such interactions can be used to obtain shrinkage estimates of regression coeﬃcients ( e . g . , Efron and Morris ( 1977 ) ) , or account for lack of independence in the residuals due to block structure or repeated measurements ( e . g . , Laird and Ware ( 1982 ) ) . Table 2 provides several examples of the right - hand - sides of mixed - model formulas . The ﬁrst example , ( 1 | g ) , is the simplest possible mixed - model formula , where each level of the grouping factor , g , has its own random intercept . The mean and standard deviation of these intercepts are parameters to be estimated . Our description of this model incorporates any non - zero mean of the random eﬀects as ﬁxed - eﬀects parameters . If one wishes to specify that a random intercept has a priori known means , one may use the offset function as in the second model in Table 2 . This model contains no ﬁxed eﬀects , or more accurately the ﬁxed - eﬀects model matrix , X , has zero columns and β has length zero . We may also construct models with multiple grouping factors . For example , if the observations are grouped by g2 , which is nested within g1 , then the third formula in Table 2 can be used to model variation in the intercept . A common objective in mixed modeling is to account for such nested ( or hierarchical ) structure . However , one of the most useful aspects of lme4 is that it can be used to ﬁt random - eﬀects associated with non - nested grouping factors . For example , suppose the data are grouped by fully crossing two factors , g1 and g2 , then the fourth formula in Table 2 may be used . Such models are common in item response theory , where subject and item factors are fully crossed ( Doran , Bates , Bliese , and Dowling 2007 ) . In addition to varying intercepts , we may also have varying slopes ( e . g . , the sleepstudy data , Section 1 . 2 ) . The ﬁfth example in Table 2 gives a model where both the intercept and slope vary among the levels of the grouping factor . Specifying uncorrelated random eﬀects By default , lme4 assumes that all coeﬃcients associated with the same random - eﬀects term are correlated . To specify an uncorrelated slope and intercept ( for example ) , one may ei - ther use double - bar notation , ( x | | g ) , or equivalently use multiple random eﬀects terms , Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 7 Symbol Size n Length of the response vector , Y p Number of columns of ﬁxed eﬀects model matrix , X q = P ki q i Number of columns of random eﬀects model matrix , Z p i Number of columns of the raw model matrix , X i ‘ i Number of levels of the grouping factor indices , i i q i = p i ‘ i Number of columns of the term - wise model matrix , Z i k Number of random eﬀects terms m i = (cid:0) p i + 1 2 (cid:1) Number of covariance parameters for term i m = P ki m i Total number of covariance parameters Table 3 : Dimensions of linear mixed models . The subscript i = 1 , . . . , k denotes a speciﬁc random eﬀects term . x + ( 1 | g ) + ( 0 + x | g ) , as in the ﬁnal example of Table 2 . For example , from the printout of model fm1 of the sleepstudy data ( Section 1 . 2 ) , we see that the estimated correlation be - tween the slope for Days and the intercept is fairly low ( 0 . 066 ) ( See Section 5 . 2 . 2 below for more on how to extract the random eﬀects covariance matrix . ) We may use double - bar notation to ﬁt a model that excludes a correlation parameter : > fm2 < - lmer ( Reaction ~ Days + ( Days | | Subject ) , sleepstudy ) Although mixed models where the random slopes and intercepts are assumed independent are commonly used to reduce the complexity of random - slopes models , they do have one subtle drawback . Models in which the slopes and intercepts are allowed to have a non - zero correlation ( e . g . , fm1 ) are invariant to additive shifts of the continuous predictor ( Days in this case ) . This invariance breaks down when the correlation is constrained to zero ; any shift in the predictor will necessarily lead to a change in the estimated correlation , and in the likelihood and predictions of the model . For example , we can eliminate the correlation in fm1 simply by shifting Days by an amount equal to the ratio of the estimated among - subject standard deviations multiplied by the estimated correlation ( i . e . , σ slope / σ intercept · ρ slope : intercept ) . The use of such models should ideally be restricted to cases where the predictor is measured on a ratio scale ( i . e . , the zero point on the scale is meaningful , not just a location deﬁned by convenience or convention ) . 2 . 3 . Algebraic and computational account of mixed - model formulas The ﬁxed - eﬀects terms of a mixed - model formula are parsed to produce the ﬁxed - eﬀects model matrix , X , in the same way that the R lm function generates model matrices . However , a mixed - model formula incorporates k ≥ 1 random - eﬀects terms of the form ( r | f ) as well . These k terms are used to produce the random eﬀects model matrix , Z ( Equation 2 ; Sec - tion 2 . 3 . 1 ) , and the structure of the relative covariance factor , Λ θ ( Equation 4 ; Section 2 . 3 . 2 ) , which are matrices that typically have a sparse structure . We now describe how one might construct these matrices from the random - eﬀects terms , considering ﬁrst a single term , ( r | f ) , and then generalizing to multiple terms . Tables 3 and 4 summarize the matrices and vectors that determine the structure of Z and Λ θ . The expression , r , is a linear model formula that evaluates to an R model matrix , X i , of 8 Linear Mixed Models with lme4 Symbol Size Description X i n × p i Raw random eﬀects model matrix J i n × ‘ i Indicator matrix of grouping factor indices X ij p i × 1 Column vector containing j th row of X i J ij ‘ i × 1 Column vector containing j th row of J i i i n Vector of grouping factor indices Z i n × q i Term - wise random eﬀects model matrix θ m Covariance parameters T i p i × p i Lower triangular template matrix Λ i q i × q i Term - wise relative covariance factor Table 4 : Symbols used to describe the structure of the random eﬀects model matrix and the relative covariance factor . The subscript i = 1 , . . . , k denotes a speciﬁc random eﬀects term . size n × p i , called the raw random eﬀects model matrix for term i . A term is said to be a scalar random - eﬀects term when p i = 1 , otherwise it is vector - valued . For a simple , scalar random - eﬀects term of the form ( 1 | f ) , X i is the n × 1 matrix of ones , which implies a random intercept model . The expression f evaluates to an R factor , called the grouping factor , for the term . For the i th term , we represent this factor mathematically with a vector i i of factor indices , which is an n - vector of values from 1 , . . . , ‘ i . 1 Let J i be the n × ‘ i matrix of indicator columns for i i . Using the Matrix package ( Bates and Maechler 2014 ) in R , we may construct the transpose of J i from a factor vector , f , by coercing f to a sparseMatrix object . For example , > ( f < - gl ( 3 , 2 ) ) [ 1 ] 1 1 2 2 3 3 Levels : 1 2 3 > ( Ji < - t ( as ( f , Class = " sparseMatrix " ) ) ) 6 x 3 sparse Matrix of class " dgCMatrix " 1 2 3 [ 1 , ] 1 . . [ 2 , ] 1 . . [ 3 , ] . 1 . [ 4 , ] . 1 . [ 5 , ] . . 1 [ 6 , ] . . 1 When k > 1 we order the random - eﬀects terms so that ‘ 1 ≥ ‘ 2 ≥ · · · ≥ ‘ k ; in general , this ordering reduces “ﬁll - in” ( i . e . , the proportion of elements that are zero in the lower triangle 1 In practice , ﬁxed - eﬀects model matrices and random - eﬀects terms are evaluated with respect to a model frame , ensuring that any expressions for grouping factors have been coerced to factors and any unused levels of these factors have been dropped . That is , ‘ i , the number of levels in the grouping factor for the i th random - eﬀects term , is well - deﬁned . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 9 of Λ > θ Z > W Z Λ θ + I but not in the lower triangle of its left Cholesky factor , L θ , described below in Equation 18 ) . This reduction in ﬁll - in provides more eﬃcient matrix operations within the penalized least squares algorithm ( Section 3 . 2 ) . Constructing the random eﬀects model matrix The i th random eﬀects term contributes q i = ‘ i p i columns to the model matrix Z . We group these columns into a matrix , Z i , which we refer to as the term - wise model matrix for the i th term . Thus q , the number of columns in Z and the dimension of the random variable , B , is q = k X i = 1 q i = k X i = 1 ‘ i p i . ( 5 ) Creating the matrix Z i from X i and J i is a straightforward concept that is , nonetheless , somewhat awkward to describe . Consider Z i as being further decomposed into ‘ i blocks of p i columns . The rows in the ﬁrst block are the rows of X i multiplied by the 0 / 1 values in the ﬁrst column of J i . Similarly for the subsequent blocks . With these deﬁnitions we may deﬁne the term - wise random - eﬀects model matrix , Z i , for the i th term as a transposed Khatri - Rao product , Z i = ( J > i ∗ X > i ) > =   J > i 1 ⊗ X > i 1 J > i 2 ⊗ X > i 2 . . . J > in ⊗ X > in   ( 6 ) where ∗ and ⊗ are the Khatri - Rao 2 ( Khatri and Rao 1968 ) and Kronecker products , and J > ij and X > ij are row vectors of the j th rows of J i and X i . These rows correspond to the j th sample in the response vector , Y , and thus j runs from 1 . . . n . The Matrix package for R contains a KhatriRao function , which can be used to form Z i . For example , if we begin with a raw model matrix , > ( Xi < - cbind ( 1 , rep . int ( c ( - 1 , 1 ) , 3L ) ) ) [ , 1 ] [ , 2 ] [ 1 , ] 1 - 1 [ 2 , ] 1 1 [ 3 , ] 1 - 1 [ 4 , ] 1 1 [ 5 , ] 1 - 1 [ 6 , ] 1 1 then the term - wise random eﬀects model matrix is , > ( Zi < - t ( KhatriRao ( t ( Ji ) , t ( Xi ) ) ) ) 2 Note that the original deﬁnition of the Khatri - Rao product is more general than the deﬁnition used in Matrix package , which is the deﬁnition we use here . 10 Linear Mixed Models with lme4 6 x 6 sparse Matrix of class " dgCMatrix " [ 1 , ] 1 - 1 . . . . [ 2 , ] 1 1 . . . . [ 3 , ] . . 1 - 1 . . [ 4 , ] . . 1 1 . . [ 5 , ] . . . . 1 - 1 [ 6 , ] . . . . 1 1 In particular , for a simple , scalar term , Z i is exactly J i , the matrix of indicator columns . For other scalar terms , Z i is formed by element - wise multiplication of the single column of X i by each of the columns of J i . Because each Z i is generated from indicator columns , its cross - product , Z > i Z i is block - diagonal consisting of ‘ i diagonal blocks each of size p i . 3 Note that this means that when k = 1 ( i . e . , there is only one random - eﬀects term , and Z i = Z ) , Z > Z will be block diag - onal . These block - diagonal properties allow for more eﬃcient sparse matrix computations ( Section 3 . 7 ) . The full random eﬀects model matrix , Z , is constructed from k ≥ 1 blocks , Z = h Z 1 Z 2 . . . Z k i ( 7 ) By transposing Equation 7 and substituting in Equation 6 , we may represent the structure of the transposed random eﬀects model matrix as follows , Z > = sample 1 sample 2 . . . sample n     J 11 ⊗ X 11 J 12 ⊗ X 12 . . . J 1 n ⊗ X 1 n term 1 J 21 ⊗ X 21 J 22 ⊗ X 22 . . . J 2 n ⊗ X 2 n term 2 . . . . . . . . . . . . . . . ( 8 ) Note that the proportion of elements of Z > that are structural zeros is P ki = 1 p i ( ‘ i − 1 ) P ki = 1 p i . ( 9 ) Therefore , the sparsity of Z > increases with the number of grouping factor levels . As the number of levels is often large in practice , it is essential for speed and eﬃciency to take account of this sparsity , for example by using sparse matrix methods , when ﬁtting mixed models ( Section 3 . 7 ) . Constructing the relative covariance factor The q × q covariance factor , Λ θ , is a block diagonal matrix whose i th diagonal block , Λ i , is of size q i , i = 1 , . . . , k . We refer to Λ i as the term - wise relative covariance factor . Furthermore , 3 To see this , note that by the properties of Kronecker products we may write the cross - product matrix Z > i Z i as P n j = 1 J ij J > ij ⊗ X ij X > ij . Because J ij is a unit vector along a coordinate axis , the cross product J ij J > ij is a p i × p i matrix of all zeros except for a single 1 along the diagonal . Therefore , the cross products , X ij X > ij , will be added to one of the ‘ i blocks of size p i × p i along the diagonal of Z > i Z i . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 11 Λ i is a homogeneous block diagonal matrix with each of the ‘ i lower - triangular blocks on the diagonal being a copy of a p i × p i lower - triangular template matrix , T i . The covariance parameter vector , θ , of length m i = (cid:0) p i + 1 2 (cid:1) , consists of the elements in the lower triangle of T i , i = 1 , . . . , k . To provide a unique representation we require that the diagonal elements of the T i , i = 1 , . . . , k be non - negative . The template , T i , can be constructed from the number p i alone . In R code we denote p i as nc . For example , if we set nc < - 3 , we could create the template for term i as , > ( rowIndices < - rep ( 1 : nc , 1 : nc ) ) [ 1 ] 1 2 2 3 3 3 > ( colIndices < - sequence ( 1 : nc ) ) [ 1 ] 1 1 2 1 2 3 > ( template < - sparseMatrix ( rowIndices , colIndices , + x = 1 * ( rowIndices = = colIndices ) ) ) 3 x 3 sparse Matrix of class " dgCMatrix " [ 1 , ] 1 . . [ 2 , ] 0 1 . [ 3 , ] 0 0 1 Note that the rowIndices and colIndices ﬁll the entire lower triangle , which contains the initial values of the covariance parameter vector , θ , > ( theta < - template @ x ) [ 1 ] 1 0 0 1 0 1 ( because the @ x slot of the sparse matrix template is a numeric vector containing the non - zero elements ) . This template contains three types of elements : structural zeros ( denoted by . ) , oﬀ - diagonal covariance parameters ( initialized at 0 ) , and diagonal variance parameters ( initialized at 1 ) . The next step in the construction of the relative covariance factor is to repeat the template once for each level of the grouping factor to construct a sparse block diagonal matrix . For example , if we set the number of levels , ‘ i , to two , nl < - 2 , we could create the transposed term - wise relative covariance factor , Λ > i , using the . bdiag function in the Matrix package , > ( Lambdati < - . bdiag ( rep ( list ( t ( template ) ) , nl ) ) ) 6 x 6 sparse Matrix of class " dgTMatrix " 12 Linear Mixed Models with lme4 [ 1 , ] 1 0 0 . . . [ 2 , ] . 1 0 . . . [ 3 , ] . . 1 . . . [ 4 , ] . . . 1 0 0 [ 5 , ] . . . . 1 0 [ 6 , ] . . . . . 1 For a model with a single random eﬀects term , Λ > i would be the initial transposed relative covariance factor itself . The transposed relative covariance factor , Λ > θ , that arises from parsing the formula and data is set at the initial value of the covariance parameters , θ . However , during model ﬁtting , it needs to be updated to a new θ value at each iteration ( see Section 3 . 6 . 1 ) . This is achieved by constructing a vector of indices , Lind , that identiﬁes which elements of theta should be placed in which elements of Lambdat , > LindTemplate < - rowIndices + nc * ( colIndices - 1 ) - choose ( colIndices , 2 ) > ( Lind < - rep ( LindTemplate , nl ) ) [ 1 ] 1 2 4 3 5 6 1 2 4 3 5 6 For example , if we randomly generate a new value for theta , > thetanew < - round ( runif ( length ( theta ) ) , 1 ) we may update Lambdat as follows , > Lambdati @ x < - thetanew [ Lind ] Section 3 . 6 . 1 describes the process of updating the relative covariance factor in more detail . 3 . Objective function module 3 . 1 . Model reformulation for improved computational stability In our initial formulation of the linear mixed model ( Equations 2 , 3 , and 4 ) , the covari - ance parameter vector , θ , appears only in the marginal distribution of the random eﬀects ( Equation 3 ) . However , from the perspective of computational stability and eﬃciency , it is advantageous to reformulate the model such that θ appears only in the conditional distribu - tion for the response vector given the random eﬀects . Such a reformulation allows us to work with singular covariance matrices , which regularly arise in practice ( e . g . , during intermediate steps of the nonlinear optimizer , Section 4 ) . The reformulation is made by deﬁning a spherical 4 random eﬀects variable , U , with distribu - tion U ∼ N ( 0 , σ 2 I q ) . ( 10 ) 4 N ( µ , σ 2 I ) distributions are called “spherical” because contours of the probability density are spheres . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 13 If we set , B = Λ θ U , ( 11 ) then B will have the desired N ( 0 , Σ θ ) distribution ( Equation 3 ) . Although it may seem more natural to deﬁne U in terms of B we must write the relationship as in eqn . 11 to allow for singular Λ θ . The conditional distribution ( Equation 2 ) of the response vector given the random eﬀects may now be reformulated as , ( Y | U = u ) ∼ N ( µ Y | U = u , σ 2 W − 1 ) ( 12 ) where µ Y | U = u = Xβ + Z Λ θ u + o ( 13 ) is a vector of linear predictors , which can be interpreted as a conditional mean ( or mode ) . Similarly , we also deﬁne µ U | Y = y obs as the conditional mean ( or mode ) of the spherical random eﬀects given the observed value of the response vector . Note also that we use the u symbol throughout to represent a speciﬁc value of the random variable , U . 3 . 2 . Penalized least squares Our computational methods for maximum likelihood ﬁtting of the linear mixed model involve repeated applications of the method of penalized least - squares ( PLS ) . In particular , the pe - nalized least squares problem is to minimize the penalized weighted residual sum - of - squares , r 2 ( θ , β , u ) = ρ 2 ( θ , β , u ) + k u k 2 , ( 14 ) over " u β # , where , ρ 2 ( θ , β , u ) = (cid:13)(cid:13)(cid:13) W 1 / 2 h y obs − µ Y | U = u i(cid:13)(cid:13)(cid:13) 2 , ( 15 ) is the weighted residual sum - of - squares . This notation makes explicit the fact that r 2 and ρ 2 both depend on θ , β , and u . The reason for the word ‘penalized’ is that the term , k u k 2 , in Equation 14 penalizes models with larger magnitude values of u . In the so - called “pseudo - data” approach we write the penalized weighted residual sum - of - squares as the squared length of a block matrix equation , r 2 ( θ , β , u ) = (cid:13)(cid:13)(cid:13) (cid:13)(cid:13) " W 1 / 2 ( y obs − o ) 0 # − " W 1 / 2 Z Λ θ W 1 / 2 X I q 0 # " u β # (cid:13)(cid:13)(cid:13) (cid:13) (cid:13) 2 . ( 16 ) This pseudo - data approach shows that the PLS problem may also be thought of as a standard least squares problem for an extended response vector , which implies that the minimizing value , " µ U | Y = y obs b β θ # , satisﬁes the normal equations , " Λ > θ Z > W ( y obs − o ) X > W ( y obs − o ) # = " Λ > θ Z > W Z Λ θ + I Λ > θ Z > W X X > W Z Λ θ X > W X # " µ U | Y = y obs b β θ # , ( 17 ) where µ U | Y = y obs is the conditional mean of U given that Y = y obs . Note that this conditional mean depends on θ , although we do not make this dependency explicit in order to reduce notational clutter . 14 Linear Mixed Models with lme4 The cross - product matrix in Equation 17 can be Cholesky decomposed , " Λ > θ Z > W Z Λ θ + I Λ > θ Z > W X X > W Z Λ θ X > W X # = " L θ 0 R > ZX R > X # " L > θ R ZX 0 R X # . ( 18 ) We may use this decomposition to rewrite the penalized weighted residual sum - of - squares as , r 2 ( θ , β , u ) = r 2 ( θ ) + (cid:13)(cid:13)(cid:13) L > θ ( u − µ U | Y = y obs ) + R ZX ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 + (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 , ( 19 ) where we have simpliﬁed notation by writing r 2 ( θ , b β θ , µ U | Y = y obs ) as r 2 ( θ ) . This is an im - portant expression in the theory underlying lme4 . It relates the penalized weighted residual sum - of - squares , r 2 ( θ , β , u ) , with its minimum value , r 2 ( θ ) . This relationship is useful in the next section where we integrate over the random eﬀects , which is required for maximum likelihood estimation . 3 . 3 . Probability densities The residual sums - of - squares discussed in the previous section can be used to express various probability densities , which are required for maximum likelihood estimation of the linear mixed model 5 , f Y | U ( y obs | u ) = | W | 1 / 2 ( 2 πσ 2 ) n / 2 exp " − ρ 2 ( θ , β , u ) 2 σ 2 # , ( 20 ) f U ( u ) = 1 ( 2 πσ 2 ) q / 2 exp " − k u k 2 2 σ 2 # , ( 21 ) f Y , U ( y obs , u ) = | W | 1 / 2 ( 2 πσ 2 ) ( n + q ) / 2 exp " − r 2 ( θ , β , u ) 2 σ 2 # , ( 22 ) f U | Y ( u | y obs ) = f Y , U ( y obs , u ) f Y ( y obs ) , ( 23 ) where , f Y ( y obs ) = Z f Y , U ( y obs , u ) d u , ( 24 ) The log - likelihood to be maximized can therefore be expressed as , L ( θ , β , σ 2 | y obs ) = log f Y ( y obs ) ( 25 ) The integral in Equation 24 may be more explicitly written as , f Y ( y obs ) = | W | 1 / 2 ( 2 πσ 2 ) ( n + q ) / 2 exp   − r 2 ( θ ) − (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 2 σ 2   Z exp    − (cid:13)(cid:13)(cid:13) L > θ ( u − µ U | Y = y obs ) + R ZX ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 2 σ 2    d u , ( 26 ) 5 These expressions only technically hold at the observed value , y obs , of the response vector , Y . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 15 which can be evaluated with the change of variables , v = L > θ ( u − µ U | Y = y obs ) + R ZX ( β − b β θ ) , ( 27 ) The Jacobian determinant of the transformation from u to v is | L θ | . Therefore we are able to write the integral as , f Y ( y obs ) = | W | 1 / 2 ( 2 πσ 2 ) ( n + q ) / 2 exp   − r 2 ( θ ) − (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 2 σ 2   Z exp " − k v k 2 2 σ 2 # | L θ | − 1 d v , ( 28 ) which by the properties of exponential integrands becomes , exp L ( θ , β , σ 2 | y obs ) = f Y ( y obs ) = | W | 1 / 2 | L θ | − 1 ( 2 πσ 2 ) n / 2 exp    − r 2 ( θ ) − (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 2 σ 2    . ( 29 ) 3 . 4 . Evaluating and proﬁling the deviance and REML criterion We are now in a position to understand why the formulation in equations 2 and 3 is particularly useful . We are able to explicitly proﬁle β and σ out of the log - likelihood ( Equation 25 ) , to ﬁnd a compact expression for the proﬁled deviance ( negative twice the proﬁled log - likelihood ) and the proﬁled REML criterion as a function of the relative covariance parameters , θ , only . Furthermore these criteria can be evaluated quickly and accurately . To estimate the parameters , θ , β , and σ 2 , we minimize negative twice the log - likelihood , which can be written as , − 2 L ( θ , β , σ 2 | y obs ) = log | L θ | 2 | W | + n log ( 2 πσ 2 ) + r 2 ( θ ) σ 2 + (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 σ 2 . ( 30 ) It is very easy to proﬁle out β , because it enters into the ML criterion only through the ﬁnal term , which is zero if β = b β θ , where b β θ is found by solving the penalized least - squares problem in Equation 16 . Therefore we can write a partially proﬁled ML criterion as , − 2 L ( θ , σ 2 | y obs ) = log | L θ | 2 | W | + n log ( 2 πσ 2 ) + r 2 ( θ ) σ 2 . ( 31 ) This criterion is only partially proﬁled because it still depends on σ 2 . Diﬀerentiating this criterion with respect to σ 2 and setting the result equal to zero yields , 0 = n b σ 2 θ − r 2 ( θ ) b σ 4 θ , ( 32 ) which leads to a maximum proﬁled likelihood estimate , b σ 2 θ = r 2 ( θ ) n . ( 33 ) 16 Linear Mixed Models with lme4 This estimate can be substituted into the partially proﬁled criterion to yield the fully proﬁled ML criterion , − 2 L ( θ | y obs ) = log | L θ | 2 | W | + n " 1 + log 2 πr 2 ( θ ) n ! # . ( 34 ) This expression for the proﬁled deviance depends only on θ . Although q , the number of columns in Z and the size of Σ θ , can be very large indeed , the dimension of θ is small , frequently less than 10 . The lme4 package uses generic nonlinear optimizers ( Section 4 ) to optimize this expression over θ to ﬁnd its maximum likelihood estimate . The REML criterion The REML criterion can be obtained by integrating the marginal density for Y with respect to the ﬁxed eﬀects ( Laird and Ware 1982 ) , Z f Y ( y obs ) d β = | W | 1 / 2 | L θ | − 1 ( 2 πσ 2 ) n / 2 exp " − r 2 ( θ ) 2 σ 2 # Z exp    − (cid:13)(cid:13)(cid:13) R X ( β − b β θ ) (cid:13)(cid:13)(cid:13) 2 2 σ 2    d β , ( 35 ) which can be evaluated with the change of variables , v = R X ( β − b β θ ) . ( 36 ) The Jacobian determinant of the transformation from β to v is | R X | . Therefore we are able to write the integral as , Z f Y ( y obs ) d β = | W | 1 / 2 | L θ | − 1 ( 2 πσ 2 ) n / 2 exp " − r 2 ( θ ) 2 σ 2 # Z exp " − k v k 2 2 σ 2 # | R X | − 1 d v , ( 37 ) which simpliﬁes to , Z f Y ( y obs ) d β = | W | 1 / 2 | L θ | − 1 | R X | − 1 ( 2 πσ 2 ) ( n − p ) / 2 exp " − r 2 ( θ ) 2 σ 2 # . ( 38 ) Minus twice the log of this integral is the ( unproﬁled ) REML criterion , − 2 L R ( θ , σ 2 | y obs ) = log | L θ | 2 | R X | 2 | W | + ( n − p ) log ( 2 πσ 2 ) + r 2 ( θ ) σ 2 . ( 39 ) Note that because β gets integrated out , the REML criterion cannot be used to ﬁnd a point estimate of β . However , we follow others in using the maximum likelihood estimate , b β b θ , at the optimum value of θ = b θ . The REML estimate of σ 2 is , b σ 2 θ = r 2 ( θ ) n − p , ( 40 ) which leads to the proﬁled REML criterion , − 2 L R ( θ | y obs ) = log | L θ | 2 | R X | 2 | W | + ( n − p ) " 1 + log 2 πr 2 ( θ ) n − p ! # . ( 41 ) Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 17 3 . 5 . Changes relative to previous formulations We compare the PLS problem as formulated in Section 3 . 2 with earlier versions and describe why we use this version . What have become known as “Henderson’s mixed - model equations” are given as Equation 6 of Henderson ( 1982 ) and would be expressed as , " X > X / σ 2 X > Z / σ 2 Z > X / σ 2 Z > Z / σ 2 + Σ − 1 # " b β θ µ B | Y = y obs # = " X > y obs / σ 2 Z > y obs / σ 2 # , ( 42 ) in our notation ( ignoring weights and oﬀsets , without loss of generality ) . The matrix written as R in Henderson ( 1982 ) is σ 2 I n in our formulation of the model . Bates and DebRoy ( 2004 ) modiﬁed the PLS equations to " Z > Z + Ω Z > X X > Z X > X # " µ B | Y = y obs b β θ # = " X > y obs Z > y obs # . ( 43 ) where Ω θ = (cid:16) Λ > θ Λ θ (cid:17) − 1 = σ 2 Σ − 1 is the relative precision matrix for a given value of θ . They also showed that the proﬁled log - likelihood can be expressed ( on the deviance scale ) as − 2 L ( θ ) = log | Z > Z + Ω | | Ω | ! + n " 1 + log 2 πr 2 θ n ! # . ( 44 ) The primary diﬀerence between Equation 42 and Equation 43 is the order of the blocks in the system matrix . The PLS problem can be solved using a Cholesky factor of the system matrix with the blocks in either order . The advantage of using the arrangement in Equation 43 is to allow for evaluation of the proﬁled log - likelihood . To evaluate | Z > Z + Ω | from the Cholesky factor that block must be in the upper - left corner , not the lower right . Also , Z is sparse whereas X is usually dense . It is straightforward to exploit the sparsity of Z > Z in the Cholesky factorization when the block containing this matrix is the ﬁrst block to be factored . If X > X is the ﬁrst block to be factored it is much more diﬃcult to preserve sparsity . The main change from the formulation in Bates and DebRoy ( 2004 ) to the current formulation is the use of a relative covariance factor , Λ θ , instead of a relative precision matrix , Ω θ , and solving for the mean of U | Y = y obs instead of the mean of B | Y = y obs . This change improves stability , because the solution to the PLS problem in Section 3 . 2 is well - deﬁned when Λ θ is singular whereas the formulation in Equation 43 cannot be used in these cases because Ω θ does not exist . It is important to allow for Λ θ to be singular because situations where the parameter esti - mates , b θ , produce a singular Λ b θ do occur in practice . And even if the parameter estimates do not correspond to a singular Λ θ , it may be desirable to evaluate the estimation criterion at such values during the course of the numerical optimization of the criterion . Bates and DebRoy ( 2004 ) also provided expressions for the gradient of the proﬁled log - likelihood expressed as Equation 44 . These expressions can be translated into the current formulation . From Equation 34 we can see that ( again ignoring weights ) , ∇ ( − 2 L ( θ ) ) = ∇ log ( | L θ | 2 ) + ∇ (cid:16) n log ( r 2 ( θ ) ) (cid:17) = ∇ log ( | Λ > θ Z > Z Λ θ + I | ) + n (cid:16) ∇ r 2 ( θ ) (cid:17) / r 2 ( θ ) = ∇ log ( | Λ > θ Z > Z Λ θ + I | ) + (cid:16) ∇ r 2 ( θ ) (cid:17) / ( c σ 2 ) . ( 45 ) 18 Linear Mixed Models with lme4 The ﬁrst gradient is easy to express but diﬃcult to evaluate for the general model . The individual elements of this gradient are ∂ log ( | Λ > θ Z > Z Λ θ + I | ) ∂θ i = tr   ∂ (cid:16) Λ > θ Z > Z Λ θ (cid:17) ∂θ i (cid:16) Λ > θ Z > Z Λ θ + I (cid:17) − 1   = tr " (cid:16) L θ L > θ (cid:17) − 1 Λ > θ Z > Z ∂ Λ θ ∂θ i + ∂ Λ > θ ∂θ i Z > Z Λ θ ! # . ( 46 ) The second gradient term can be expressed as a linear function of the residual , with individual elements of the form ∂r 2 ( θ ) ∂θ i = − 2 u > ∂ Λ > θ ∂θ i Z > (cid:16) y − Z Λ θ u − X b β θ (cid:17) , ( 47 ) using the results of Golub and Pereyra ( 1973 ) . Although we do not use these results in lme4 , they are used for certain model types in the MixedModels package for Julia and do provide improved performance . 3 . 6 . Penalized least squares algorithm For eﬃciency , in lme4 itself , PLS is implemented in compiled C + + code using the Eigen tem - plated C + + package for numerical linear algebra . Here however , in order to improve read - ability we describe a version in pure R . Section 3 . 7 provides greater detail on the techniques and concepts for computational eﬃciency , which is important in cases where the nonlinear optimizer ( Section 4 ) requires many iterations . The PLS algorithm takes a vector of covariance parameters , θ , as inputs and returns the proﬁled deviance ( Equation 34 ) or the REML criterion ( Equation 41 ) . This PLS algorithm consists of four main steps : 1 . Update the relative covariance factor ( Section 3 . 6 . 1 ) 2 . Solve the normal equations ( Section 3 . 6 . 2 ) 3 . Update the linear predictor and residuals ( Section 3 . 6 . 3 ) 4 . Compute and return the proﬁled deviance ( Section 3 . 6 . 4 ) PLS also requires the objects described in Table 5 , which deﬁne the structure of the model . These objects do not get updated during the PLS iterations , and so it is useful to store various matrix products involving them ( Table 6 ) . Table 7 lists the objects that do get updated over the PLS iterations . The symbols in this table correspond to a version of lme4 that is imple - mented entirely in R ( i . e . , no compiled code as in lme4 itself ) . This implementation is called lme4pureR and is currently available on Github ( https : / / github . com / lme4 / lme4pureR / ) . PLS step I : update relative covariance factor The ﬁrst step of PLS is to update the relative covariance factor , Λ θ , from the current value of the covariance parameter vector , θ . The updated Λ θ is then used to update the random eﬀects Cholesky factor , L θ ( Equation 18 ) . The mapping from the covariance parameters to Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 19 Name / description Pseudocode Math Type Mapping from covariance param - eters to relative covariance factor mapping function Response vector y y obs ( Section 1 . 1 ) double vector Fixed eﬀects model matrix X X ( Equation 2 ) double dense a matrix Transposed random eﬀects model matrix Zt Z > ( Equation 2 ) double sparse matrix Square - root weights matrix sqrtW W 1 / 2 ( Equation 2 ) double diagonal matrix Oﬀset offset o ( Equation 2 ) double vector a In previous versions of lme4 a sparse X matrix , useful for models with categorical ﬁxed - eﬀect predictors with many levels , could be speciﬁed ; this feature is not currently available . Table 5 : Inputs into an LMM Pseudocode Math ZtW Z > W 1 / 2 ZtWy Z > W y obs ZtWX Z > W X XtWX X > W X XtWy X > W y obs Table 6 : Constant symbols involved in penalized least squares . the relative covariance factor can take many forms , but in general involves a function that takes θ into the values of the non - zero elements of Λ θ . If Λ θ is stored as an object of class " dgCMatrix " from the Matrix package for R , then we may update Λ θ from θ by , > Lambdat @ x [ ] < - mapping ( theta ) where mapping is an R function that both accepts and returns a numeric vector . The non - zero elements of sparse matrix classes in Matrix are stored in a slot called x . In the current version of lme4 ( v . 1 . 1 - 7 ) the mapping from θ to Λ θ is represented as an R integer vector , Lind , of indices , so that > mapping < - function ( theta ) theta [ Lind ] The index vector Lind is computed during the model setup and stored in the function’s environment . Continuing the example from Section 2 . 3 . 2 , consider a new value for theta , > thetanew < - c ( 1 , - 0 . 1 , 2 , 0 . 1 , - 0 . 2 , 3 ) To put these values in the appropriate elements in Lambdati , we use mapping , 20 Linear Mixed Models with lme4 Name / description Pseudocode Math Type Relative covariance factor lambda Λ θ ( Equation 4 ) sparse double lower - triangular matrix Random - eﬀects Cholesky factor L L θ ( Equation 18 ) double sparse triangular ma - trix Intermediate vector in the solu - tion of the normal equations cu c u ( Equation 48 ) double vector Block in the full Cholesky factor RZX R ZX ( Equation 18 ) double dense matrix Cross - product of the ﬁxed - eﬀects Cholesky factor RXtRX R > X R X ( Equation 50 ) double dense matrix Fixed eﬀects coeﬃcients beta β ( Equation 2 ) double vector Spherical conditional modes u u ( Section 3 . 1 ) double vector Non - spherical conditional modes b b ( Equation 2 ) double vector Linear predictor mu µ Y | U = u ( Equation 13 ) double vector Weighted residuals wtres W 1 / 2 ( y obs − µ ) double vector Penalized weighted residual sum - of - squares pwrss r 2 ( θ ) ( Equation 19 ) double Twice the log determinant random - eﬀects Cholesky factor logDet log | L θ | 2 double Table 7 : Quantities updated during an evaluation of the linear mixed model objective function . > Lambdati @ x [ ] < - mapping ( thetanew ) > Lambdati 6 x 6 sparse Matrix of class " dgTMatrix " [ 1 , ] 1 - 0 . 1 2 . 0 . . . [ 2 , ] . 0 . 1 - 0 . 2 . . . [ 3 , ] . . 3 . 0 . . . [ 4 , ] . . . 1 - 0 . 1 2 . 0 [ 5 , ] . . . . 0 . 1 - 0 . 2 [ 6 , ] . . . . . 3 . 0 This Lind approach can be useful for extending the capabilities of lme4 by using the modular approach to ﬁtting mixed models . For example , Appendix A . 2 shows how to use Lind to ﬁt a model where a random slope and intercept are uncorrelated and have the same variance . The mapping from the covariance parameters to the relative covariance factor is treated diﬀerently in other implementations of the lme4 approach to linear mixed models . At the other extreme , the flexLambda branch of lme4 and the lme4pureR package allows the capabilities for a completely general mapping . This added ﬂexibility has the advantage of allowing a much wider variety of models ( e . g . , compound symmetry , auto - regression ) . However , the disadvantage of this approach is that it becomes possible to ﬁt a much wider variety of ill - posed models . Finally , if one would like such added ﬂexibility with the current stable version of lme4 , it is always possible to use the modular approach to wrap the Lind - based deviance function in a general mapping function taking a parameter to be optimized , say φ , into θ . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 21 However , this approach is likely to be ineﬃcient in many cases . The update method from the Matrix package eﬃciently updates the random eﬀects Cholesky factor , L θ , from a new value of θ and the updated Λ θ . > L < - update ( L , Lambdat % * % ZtW , mult = 1 ) The mult = 1 argument corresponds to the addition of the identity matrix to the upper - left block on the left - hand - side of Equation 18 . PLS step II : solve normal equations With the new covariance parameters installed in Λ θ , the next step is to solve the normal equations ( Equation 17 ) for the current estimate of the ﬁxed eﬀects coeﬃcients , b β θ , and the conditional mode , µ U | Y = y obs . We solve these equations using a sparse Cholesky factorization ( Equation 18 ) . In a complex model ﬁt to a large data set , the dominant calculation in the evaluation of the proﬁled deviance ( Equation 34 ) or REML criterion ( Equation 41 ) is this sparse Cholesky factorization ( Equation 18 ) . The factorization is performed in two phases ; a symbolic phase and a numeric phase . The symbolic phase , in which the ﬁll - reducing permu - tation P is determined along with the positions of the non - zeros in L θ , does not depend on the value of θ . It only depends on the positions of the nonzeros in Z Λ θ . The numeric phase uses θ to determine the numeric values of the non - zeros in L θ . Using this factorization , the solution proceeds by the following steps , L θ c u = P Λ > θ Z > W y ( 48 ) L θ R ZX = P Λ > θ Z > W X ( 49 ) R > X R X = X > W X − R > ZX R ZX ( 50 ) (cid:16) R > X R X (cid:17) b β θ = X > W y − R ZX c u ( 51 ) L > θ P u = c u − R ZX b β ( 52 ) which can be solved using the Matrix package with , > cu [ ] < - as . vector ( solve ( L , solve ( L , Lambdat % * % ZtWy , + system = " P " ) , system = " L " ) ) > RZX [ ] < - as . vector ( solve ( L , solve ( L , Lambdat % * % ZtWX , + system = " P " ) , system = " L " ) ) > RXtRX < - as ( XtWX - crossprod ( RZX ) , " dpoMatrix " ) > beta [ ] < - as . vector ( solve ( RXtRX , XtWy - crossprod ( RZX , cu ) ) ) > u [ ] < - as . vector ( solve ( L , solve ( L , cu - RZX % * % beta , + system = " Lt " ) , system = " Pt " ) ) > b [ ] < - as . vector ( crossprod ( Lambdat , u ) ) Notice the nested calls to solve . The inner calls of the ﬁrst two assignments determine and apply the permutation matrix ( system = " P " ) , whereas the outer calls actually solve the equation ( system = " L " ) . In the assignment to u [ ] , the nesting is reversed in order to return to the original permutation . 22 Linear Mixed Models with lme4 PLS step III : update linear predictor and residuals The next step is to compute the linear predictor , µ Y | U ( Equation 13 ) , and the weighted resid - uals with new values for b β θ and µ B | Y = y obs . In lme4pureR these quantities can be computed as , > mu [ ] < - as . vector ( crossprod ( Zt , b ) + X % * % beta + offset ) > wtres < - sqrtW * ( y - mu ) where b represents the current estimate of µ B | Y = y obs . PLS step IV : compute proﬁled deviance Finally , the updated linear predictor and weighted residuals can be used to compute the proﬁled deviance ( or REML criterion ) , which in lme4pureR proceeds as , > pwrss < - sum ( wtres ^ 2 ) + sum ( u ^ 2 ) > logDet < - 2 * determinant ( L , logarithm = TRUE ) $ modulus > if ( REML ) logDet < - logDet + determinant ( RXtRX , + logarithm = TRUE ) $ modulus > attributes ( logDet ) < - NULL > profDev < - logDet + degFree * ( 1 + log ( 2 * pi * pwrss ) - log ( degFree ) ) The proﬁled deviance consists of three components : ( 1 ) log - determinant ( s ) of Cholesky factor - ization ( logDet ) , ( 2 ) the degrees of freedom ( degFree ) , and the penalized weighted residual sum - of - squares ( pwrss ) . 3 . 7 . Sparse matrix methods In ﬁtting linear mixed models , an instance of the penalized least squares ( PLS ) problem ( 17 ) must be solved at each evaluation of the objective function during the optimization ( Section 4 ) with respect to θ . Because this operation must be performed many times it is worthwhile considering how to provide eﬀective evaluation methods for objects and calculations involving the sparse matrices associated with random eﬀects terms ( Sections 2 . 3 ) . The CHOLMOD library of C functions ( Chen , Davis , Hager , and Rajamanickam 2008 ) , on which the sparse matrix capabilities of the Matrix package for R and the sparse Cholesky factorization in Julia are based , allows for separation of the symbolic and numeric phases . Thus we perform the symbolic phase as part of establishing the structure representing the model ( Section 2 ) . Furthermore , because CHOLMOD functions allow for updating L θ directly from the matrix Λ > θ Z > without actually forming Λ > θ Z > Z Λ θ + I we generate and store Z > instead of Z ( note that we have ignored the weights matrix , W , for simplicity ) . We can update Λ > θ Z > directly from θ without forming Λ θ and multiplying two sparse matrices . Although such a direct approach is used in the MixedModels package for Julia , in lme4 we ﬁrst update Λ > θ then form the sparse product Λ > θ Z > . A third alternative , which we employ in lme4pureR , is to compute and save the cross - products of the model matrices , X and Z , and the response , y , before starting the iterations . To allow for case weights , we save the products X > W X , X > W y , Z > W X , Z > W y and Z > W Z ( see Table 6 ) . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 23 We wish to use structures and algorithms that allow us to take a new value of θ and evaluate the L θ ( eqn . 18 ) eﬃciently . The key to doing so is the special structure of Λ > θ Z > W 1 / 2 . To understand why this matrix , and not its transpose , is of interest we describe the sparse matrix structures used in Julia and in the Matrix package for R . Dense matrices are stored in R and in Julia as a one - dimensional array of contiguous storage locations addressed in column - major order . This means that elements in the same column are in adjacent storage locations whereas elements in the same row can be widely separated in memory . For this reason , algorithms that work column - wise are preferred to those that work row - wise . Although a sparse matrix can be stored in a triplet format , where the row position , column position and element value the nonzeros are recorded as triplets , the preferred storage forms for actual computation with sparse matrices are compressed sparse column ( CSC ) or compressed sparse row ( CSR ) ( Davis 2006 , Ch . 2 ) . The CHOLMOD ( and , more generally , the SuiteSparse package of C libraries ) uses the CSC storage format . In this format the non - zero elements in a column are in adjacent storage locations and access to all the elements in a column is much easier than access to those in a row ( similar to dense matrices stored in column - major order ) . The matrices Z and Z Λ θ have the property that the number of nonzeros in each row , P ki = 1 p i , is constant . For CSC matrices we want consistency in the columns rather than the rows , which is why we work with Z > and Λ > θ Z > rather than their transposes . An arbitrary m × n sparse matrix in CSC format is expressed as two vectors of indices , the row indices and the column pointers , and a numeric vector of the non - zero values . The elements of the row indices and the nonzeros are aligned and are ordered ﬁrst by increasing column number then by increasing row number within column . The column pointers are a vector of size n + 1 giving the location of the start of each column in the vectors of row indices and nonzeros . Because the number of nonzeros in each column of Z > , and in each column of matrices derived from Z > ( such as Λ > θ Z > W 1 / 2 ) is constant , the vector of nonzeros in the CSC format can be viewed as a dense matrix , say N , of size ( P ni = 1 p i ) × n . We do not need to store the column pointers because the columns of Z > correspond to columns of N . All we need is N , the dense matrix of nonzeros , and the row indices , which are derived from the grouping factor index vectors i i , i = 1 , . . . , k and can also be arranged as a dense matrix of size ( P ni = 1 p i ) × n . Matrices like Z > , with the property that there are the same number of nonzeros in each column , are sometimes called regular sparse column - oriented ( RSC ) matrices . 4 . Nonlinear optimization module The objective function module produces a function which implements the penalized least squares algorithm for a particular mixed model . The next step is to optimize this function over the covariance parameters , θ , which is a nonlinear optimization problem . The lme4 package separates the eﬃcient computational linear algebra required to compute proﬁled likelihoods and deviances for a given value of θ from the nonlinear optimization algorithms , which use general - purpose nonlinear optimizers . One beneﬁt of this separation is that it allows for experimentation with diﬀerent nonlinear optimizers . Throughout the development of lme4 , the default optimizers and control param - eters have changed in response to feedback from users about both eﬃciency and convergence 24 Linear Mixed Models with lme4 properties . lme4 incorporates two built - in optimization choices , an implementation of the Nelder - Mead simplex algorithm adapted from Steven Johnson’s NLopt C library ( Johnson 2014 ) and a wrapper for Powell’s BOBYQA algorithm , implemented in the minqa package Bates , Mullen , Nash , and Varadhan ( 2014 ) as a wrapper around Powell’s original FORTRAN code ( Powell 2009 ) . More generally , lme4 allows any user - speciﬁed optimizer that ( 1 ) can work with an objective function ( i . e . , does not require a gradient function to be speciﬁed ) , ( 2 ) allows box constraints on the parameters , and ( 3 ) conforms to some simple rules about argument names and structure of the output . An internal wrapper allows the use of the op - timx package ( Nash and Varadhan 2011 ) , although the only optimizers provided via optimx that satisfy the constraints above are the nlminb and L - BFGS - B algorithms that are them - selves wrappers around the versions provided in base R . Several other algorithms from Steven Johnson’s NLopt package are also available via the nloptr wrapper package ( e . g . , alternate implementations of Nelder - Mead and BOBYQA , and Powell’s COBYLA algorithm ) . This ﬂexibility assists with diagnosing convergence problems — it is easy to switch among several algorithms to determine whether the problem lies in a failure of the nonlinear opti - mization stage , as opposed to a case of model misspeciﬁcation or unidentiﬁability or a problem with the underlying PLS algorithm . To date we have only observed PLS failures , which arise if X > W X − R > ZX R ZX becomes singular during an evaluation of the objective function , with badly scaled problems ( i . e . , problems with continuous predictors that take a very large or very small numerical range of values ) . The requirement for optimizers that can handle box constraints stems from our decision to parameterize the variance - covariance matrix in a constrained space , in order to allow for singular ﬁts . In contrast to the approach taken in the nlme package ( Pinheiro , Bates , DebRoy , Sarkar , and R Core Team 2014 ) , which goes to some lengths to use an unconstrained variance - covariance parameterization ( the log - Cholesky parameterization : Pinheiro and Bates ( 1996 ) ) , we instead use the Cholesky parameterization but require the elements of θ corresponding to the diagonal elements of the Cholesky factor to be non - negative . With these constraints , the variance - covariance matrix is singular if and only if any of the diagonal elements is exactly zero . Singular ﬁts are common in practical data - analysis situations , especially with small - to medium - sized data sets and complex variance - covariance models , so being able to ﬁt a singular model is an advantage : when the best ﬁtting model lies on the boundary of a constrained space , approaches that try to remove the constraints by ﬁtting parameters on a transformed scale will often give rise to convergence warnings as the algorithm tries to ﬁnd a maximum on an asymptotically ﬂat surface ( Bolker et al . 2013 ) . In principle the likelihood surfaces we are trying to optimize over are smooth , but in practice using gradient information in optimization may or may not be worth the eﬀort . In special cases , we have a closed - form solution for the gradients ( Equations 45 – 47 ) , but in general we will have to approximate them by ﬁnite diﬀerences , which is expensive and has limited accuracy . ( We have considered using automatic diﬀerentiation approaches to compute the gradients more eﬃciently , but this strategy requires a great deal of additional machinery , and would have drawbacks in terms of memory requirements for large problems . ) This is the primary reason for our switching to derivative - free optimizers such as BOBYQA and Nelder - Mead in the current version of lme4 , although as discussed above derivative - based optimizers based on ﬁnite diﬀerencing are available as an alternative . There is most likely further room for improvement in the nonlinear optimization module ; for example , some speed - up could be gained by using parallel implementations of derivative - free Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 25 optimizers that evaluated several trial points at once ( Klein and Neira 2013 ) . In practice users most often have optimization diﬃculties with poorly scaled or centred data — we are working to implement appropriate diagnostic tests and warnings to detect these situations . 5 . Output module Here we describe some of the methods in lme4 for exploring ﬁtted linear mixed models ( Ta - ble 8 ) , which are represented as objects of the S4 class lmerMod . We begin by describing the theory underlying these methods ( Section 5 . 1 ) and then continue the sleepstudy example introduced in Section 1 . 2 to illustrate these ideas in practice . 5 . 1 . Theory underlying the output module Covariance matrix of the ﬁxed eﬀect coeﬃcients In the lm function , the variance - covariance matrix of the coeﬃcients is the inverse of the cross product of the model matrix , times the residual variance ( Chambers 1993 ) . The inverse cross - product matrix is computed by ﬁrst inverting the upper triangular matrix resulting from the QR decomposition of the model matrix , and then taking its cross - product . Var θ , σ " µ U | Y = y obs ˆ β # ! = σ 2 " L > θ R ZX 0 R X # − 1 " L θ 0 R > ZX R > X # − 1 ( 53 ) Because of normality , the marginal covariance matrix of ˆ β is just the lower - right p - by - p block of Var θ , σ " µ U | Y = y obs ˆ β # ! . This lower - right block is Var θ , σ ( ˆ β ) = σ 2 R − 1 X ( R > X ) − 1 ( 54 ) which follows from the Schur complement identity ( Theorem 1 . 2 of Horn and Zhang ( 2005 ) ) . This matrix can be extracted from a ﬁtted merMod object as , > RX < - getME ( fm1 , " RX " ) > sigma2 < - sigma ( fm1 ) ^ 2 > sigma2 * chol2inv ( RX ) [ , 1 ] [ , 2 ] [ 1 , ] 46 . 575 - 1 . 451 [ 2 , ] - 1 . 451 2 . 389 which could be computed with lme4 as vcov ( fm1 ) . The square - root diagonal of this covariance matrix contains the estimates of the standard errors of ﬁxed eﬀects coeﬃcients . These standard errors are used to construct Wald conﬁdence intervals with confint ( . , method = " Wald " ) . Such conﬁdence intervals are approximate , and depend on the assumption that the likelihood proﬁle of the ﬁxed eﬀects is linear on the ζ scale ( Section 5 . 1 . 2 ) . 26 Linear Mixed Models with lme4 Proﬁling The theory of likelihood proﬁles is straightforward : the deviance ( or likelihood ) proﬁle , − 2 L p ( ) , for a focal model parameter P is the minimum value of the deviance conditioned on a particular value of P . For each parameter of interest , our goal is to evaluate the de - viance proﬁle for many points — optimizing over all of the non - focal parameters each time — over a wide enough range and with high enough resolution to evaluate the shape of the proﬁle ( in particular , whether it is quadratic , which would allow use of Wald conﬁdence in - tervals and tests ) and to ﬁnd the values of P such that − 2 L p ( P ) = − 2 L ( b P ) + χ 2 ( 1 − α ) , which represent the proﬁle conﬁdence intervals . While proﬁle conﬁdence intervals rely on the asymptotic distribution of the minimum deviance , this is a much weaker assumption than the log - quadratic likelihood surface required by Wald tests . An additional challenge in proﬁling arises when we want to compute proﬁles for quantities of interest that are not parameters of our PLS function . We have two problems in using the deviance function deﬁned above to proﬁle linear mixed models . First , a parameterization of the random eﬀects variance - covariance matrix in terms of standard deviations and correla - tions , or variances and covariances , is much more familiar to users , and much more relevant as output of a statistical model , than the parameters , θ , of the relative covariance factor — users are interested in inferences on variances or standard deviations , not on θ . Second , the ﬁxed - eﬀect coeﬃcients and the residual standard deviation , both of which are also of interest to users , are proﬁled out ( in the sense used above ) of the deviance function ( Section 3 . 4 ) , so we have to ﬁnd a strategy for estimating the deviance for values of β and σ 2 other than the proﬁled values . To handle the ﬁrst problem we create a new version of the deviance function that ﬁrst takes a vector of standard deviations ( and correlations ) , and a value of the residual standard de - viation , and maps them to a θ vector , and then computes the PLS as before ; it uses the speciﬁed residual standard deviation rather than the PLS estimate of the standard deviation ( Equation 33 ) in the deviance calculation . We compute a proﬁle likelihood for the ﬁxed - eﬀect parameters , which are proﬁled out of the deviance calculation , by adding an oﬀset to the linear predictor for the focal element of β . The resulting function is not useful for general nonlinear optimization — one can easily wander into parameter regimes corresponding to in - feasible ( non - positive semideﬁnite ) variance - covariance matrices — but it serves for likelihood proﬁling , where one focal parameter is varied at a time and the optimization over the other parameters is likely to start close to an optimum . In practice , the profile method systematically varies the parameters in a model , assessing the best possible ﬁt that can be obtained with one parameter ﬁxed at a speciﬁc value and comparing this ﬁt to the globally optimal ﬁt , which is the original model ﬁt that allowed all the parameters to vary . The models are compared according to the change in the deviance , which is the likelihood ratio test statistic . We apply a signed square root transformation to this statistic and plot the resulting function , which we call the proﬁle zeta function or ζ , versus the parameter value . The signed aspect of this transformation means that ζ is positive where the deviation from the parameter estimate is positive and negative otherwise , leading to a monotonically increasing function which is zero at the global optimum . A ζ value can be compared to the quantiles of the standard normal distribution . For example , a 95 % proﬁle deviance conﬁdence interval on the parameter consists of the values for which − 1 . 96 < ζ < 1 . 96 . Because the process of proﬁling a ﬁtted model can be computationally Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 27 intensive — it involves reﬁtting the model many times — one should exercise caution with complex models ﬁt to large data sets . The standard approach to this computational challenge is to compute ζ at a limited number of parameter values , and to ﬁll in the gaps by ﬁtting an interpolation spline . Often one is able to invert the spline to obtain a function from ζ to the focal parameter , which is necessary in order to construct proﬁle conﬁdence intervals . However , even if the points themselves are monotonic , it is possible to obtain non - monotonic interpolation curves . In such a case , lme4 falls back to linear interpolation , with a warning . The last part of the technical speciﬁcation for computing proﬁles is deciding on a strategy for choosing points to sample . In one way or another one wants to start at the estimated value for each parameter and work outward either until a constraint is reached ( i . e . , a value of 0 for a standard deviation parameter or a value of ± 1 for a correlation parameter ) , or until a suﬃciently large deviation from the minimum deviance is attained . lme4 ’s proﬁler chooses a cutoﬀ φ based on the 1 − α max critical value of the χ 2 distribution with a number of degrees of freedom equal to the total number of parameters in the model , where α max is set to 0 . 05 by default . The proﬁle computation initially adjusts the focal parameter p i by an amount (cid:15) = 1 . 01ˆ p i from its ML or REML estimate ˆ p i ( or by (cid:15) = 0 . 001 if ˆ p i is zero , as in the case of a singular variance - covariance model ) . The local slope of the likelihood proﬁle ( ζ ( ˆ p i + (cid:15) ) − ζ ( ˆ p i ) ) / (cid:15) is used to pick the next point to evaluate , extrapolating the local slope to ﬁnd a new (cid:15) that would be expected to correspond to the desired step size ∆ ζ ( equal to φ / 8 by default , so that 16 points would be used to cover the proﬁle in both directions if the log - likelihood surface were exactly quadratic ) . Some fail - safe testing is done to ensure that the step chosen is always positive , and less than a maximum ; if a deviance is ever detected that is lower than that of the ML deviance , which can occur if the initial ﬁt was wrong due to numerical problems , the proﬁler issues an error and stops . Parametric bootstrapping To avoid the asymptotic assumptions of the likelihood ratio test , at the cost of greatly in - creased computation time , one can estimate conﬁdence intervals by parametric bootstrapping — that is , by simulating data from the ﬁtted model , reﬁtting the model , and extracting the new estimated parameters ( or any other quantities of interest ) . This task is quite straight - forward , since there is already a simulate method , and a refit function which re - estimates the ( RE ) ML parameters for new data , starting from the previous ( RE ) ML estimates and re - using the previously computed model structures ( including the ﬁll - reducing permutation ) for eﬃciency . The bootMer function is thus a fairly thin wrapper around a simulate / refit loop , with a small amount of additional logic for parallel computation and error - catching . ( Some of the ideas of bootMer are adapted from merBoot , a more ambitious framework for bootstrapping lme4 model ﬁts which unfortunately seems to be unavailable at present ( Sánchez - Espigares and Ocaña 2009 ) . ) Conditional variances of random eﬀects It is useful to clarify that the conditional covariance concept in lme4 is based on a simpliﬁ - cation of the linear mixed model . In particular , we simplify the model by assuming that the quantities , β , Λ θ , and σ , are known ( i . e . , set at their estimated values ) . In fact , the only way to deﬁne the conditional covariance is at ﬁxed parameter values . Our approximation 28 Linear Mixed Models with lme4 here is using the estimated parameter values for the unknown “true” parameter values . In this simpliﬁed case , U is the only quantity in the statistical model that is both random and unknown . Given this simpliﬁed linear mixed model , a standard result in Bayesian linear regression modelling ( Gelman et al . 2013 ) implies that the conditional distribution of the spherical random eﬀects given the observed response vector is Gaussian , ( U | Y = y obs ) ∼ N ( µ U | Y = y obs , b σ 2 V ) ( 55 ) where , V = (cid:16) Λ > b θ Z > W Z Λ b θ + I q (cid:17) − 1 = (cid:16) L − 1 b θ (cid:17) > (cid:16) L − 1 b θ (cid:17) ( 56 ) is the unscaled conditional variance and , µ U | Y = y obs = V Λ > b θ Z > W (cid:16) y obs − o − X b β (cid:17) ( 57 ) is the conditional mean / mode . Note that in practice the inverse in Equation 56 does not get computed directly , but rather an eﬃcient method is used that exploits the sparse structures . The random eﬀects coeﬃcient vector , b , is often of greater interest . The conditional covariance matrix of B may be expressed as , b σ 2 Λ b θ V Λ > b θ ( 58 ) The hat matrix The hat matrix , H , is a useful object in linear model diagnostics ( Cook and Weisberg 1982 ) . In general , H relates the observed and ﬁtted response vectors , but we speciﬁcally deﬁne it as , (cid:16) µ Y | U = u − o (cid:17) = H ( y obs − o ) ( 59 ) To ﬁnd H we note that , (cid:16) µ Y | U = u − o (cid:17) = h Z Λ X i " µ U | Y = y obs b β θ # ( 60 ) Next we get an expression for " µ U | Y = y obs b β θ # by solving the normal equations ( Equation 17 ) , " µ U | Y = y obs b β θ # = " L > θ R ZX 0 R X # − 1 " L θ 0 R > ZX R > X # − 1 " Λ > θ Z > X > # W ( y obs − o ) ( 61 ) By the Schur complement identity ( Horn and Zhang 2005 ) , " L > θ R ZX 0 R X # − 1 =   (cid:16) L > θ (cid:17) − 1 − (cid:16) L > θ (cid:17) − 1 R ZX R − 1 X 0 R − 1 X   ( 62 ) Therefore , we may write the hat matrix as , H = ( C > L C L + C > R C R ) ( 63 ) Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 29 where , L θ C L = Λ > θ Z > W 1 / 2 ( 64 ) and , R > X C R = X > W 1 / 2 − R > ZX C L ( 65 ) The trace of the hat matrix is often used as a measure of the eﬀective degrees of freedom ( e . g . , Vaida and Blanchard ( 2005 ) ) . Using a relationship between the trace and vec operators , the trace of H may be expressed as , tr ( H ) = k vec ( C L ) k 2 + k vec ( C R ) k 2 ( 66 ) 5 . 2 . Using the output module The user interface for the output module largely consists of a set of methods ( Table 8 ) for objects of class merMod , which is the class of objects returned by lmer ( In addition to these methods , the getME function can be used to extract various objects from a ﬁtted mixed model in lme4 . ) Here we illustrate the use of several of these methods . Updating ﬁtted mixed models To illustrate the update method for merMod objects we construct a random intercept only model . This task could be done in several ways , but we choose to ﬁrst remove the random eﬀects term ( Days | Subject ) and add a new term with a random intercept , > fm3 < - update ( fm1 , . ~ . - ( Days | Subject ) + ( 1 | Subject ) ) > formula ( fm3 ) # how the updated formula is parsed Reaction ~ Days + ( 1 | Subject ) Model summary and associated accessors The summary method for merMod objects provides information about the model ﬁt . Here we consider the output of summary ( fm1 ) in four steps . The ﬁrst few lines of output indicate that the model was ﬁtted by restricted maximum likelihood ( REML ) as well as the value of the REML criterion ( Equation 39 ) at convergence ( which may also be extracted using deviance ( fm1 ) ) . The beginning of the summary also reproduces the model formula and the scaled Pearson residuals , Linear mixed model fit by REML [ ' lmerMod ' ] Formula : Reaction ~ Days + ( Days | Subject ) Data : sleepstudy REML criterion at convergence : 1744 Scaled residuals : Min 1Q Median 3Q Max - 3 . 954 - 0 . 463 0 . 023 0 . 463 5 . 179 30 Linear Mixed Models with lme4 Generic ( Section ) Brief description of return value anova ( 5 . 2 . 4 ) Decomposition of ﬁxed - eﬀects contributions or model comparison . as . function Function returning proﬁled deviance or REML criterion . coef Sum of the random and ﬁxed eﬀects for each level . confint ( 5 . 2 . 6 ) Conﬁdence intervals on linear mixed - model parameters . deviance ( 5 . 2 . 2 ) Minus twice maximum log - likelihood . ( Use REMLcrit for the REML criterion . ) df . residual Residual degrees of freedom . drop1 Drop allowable single terms from the model . extractAIC Generalized Akaike information criterion fitted Fitted values given conditional modes ( Equation 13 ) . fixef ( 5 . 2 . 2 ) Estimates of the ﬁxed eﬀects coeﬃcients , b β formula ( 2 . 2 . 1 ) Mixed - model formula of ﬁtted model . logLik Maximum log - likelihood . model . frame Data required to ﬁt the model . model . matrix Fixed eﬀects model matrix , X . ngrps ( 5 . 2 . 2 ) Number of levels in each grouping factor . nobs ( 5 . 2 . 2 ) Number of observations . plot Diagnostic plots for mixed - model ﬁts . predict ( 5 . 2 . 8 ) Various types of predicted values . print Basic printout of mixed - model objects . profile ( 5 . 1 . 2 ) Proﬁled likelihood over various model parameters . ranef ( 5 . 2 . 2 ) Conditional modes of the random eﬀects . refit ( 5 . 1 . 3 ) A model ( re ) ﬁtted to a new set of observations of the response variable . refitML ( 5 . 2 . 4 ) A model ( re ) ﬁtted by maximum likelihood . residuals ( 5 . 2 . 2 ) Various types of residual values . sigma ( 5 . 2 . 2 ) Residual standard deviation . simulate ( 5 . 2 . 8 ) Simulated data from a ﬁtted mixed model . summary ( 5 . 2 . 2 ) Summary of a mixed model . terms Terms representation of a mixed model . update ( 5 . 2 . 1 ) An updated model using a revised formula or other arguments . VarCorr ( 5 . 2 . 2 ) Estimated random - eﬀects variances , standard deviations , and correlations . vcov ( 5 . 2 . 2 ) Covariance matrix of the ﬁxed eﬀect estimates . weights Prior weights used in model ﬁtting . Table 8 : List of currently available methods for objects of the class merMod . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 31 This information may also be obtained using standard accessor functions , > formula ( fm1 ) > REMLcrit ( fm1 ) > quantile ( residuals ( fm1 , " pearson " , scaled = TRUE ) ) The second piece of summary output provides information regarding the random eﬀects and residual variation , Random effects : Groups Name Variance Std . Dev . Corr Subject ( Intercept ) 612 . 1 24 . 74 Days 35 . 1 5 . 92 0 . 07 Residual 654 . 9 25 . 59 Number of obs : 180 , groups : Subject , 18 which can also be accessed using , > print ( vc < - VarCorr ( fm1 ) , comp = c ( " Variance " , " Std . Dev . " ) ) > nobs ( fm1 ) # number of rows in sleepstudy data > ngrps ( fm1 ) # number of levels of the Subject grouping factor > sigma ( fm1 ) # residual standard deviation The print method for VarCorr hides the internal structure of VarCorr . merMod objects . The internal structure of an object of this class is a list of matrices , one for each random eﬀects term . The standard deviations and correlation matrices for each term are stored as attributes , stddev and correlation , respectively , of the variance - covariance matrix , and the residual standard deviation is stored as attribute sc . For programming use , these objects can be summarized diﬀerently using their as . data . frame method , > as . data . frame ( VarCorr ( fm1 ) ) grp var1 var2 vcov sdcor 1 Subject ( Intercept ) < NA > 612 . 090 24 . 74045 2 Subject Days < NA > 35 . 072 5 . 92213 3 Subject ( Intercept ) Days 9 . 604 0 . 06555 4 Residual < NA > < NA > 654 . 941 25 . 59182 which contains one row for each variance or covariance parameter . The grp column gives the grouping factor associated with this parameter . The var1 and var2 columns give the names of the variables associated with the parameter ( var2 is < NA > unless it is a covariance parameter ) . The vcov column gives the variances and covariances , and the sdcor column gives these numbers on the standard deviation and correlation scales . The next chunk of output gives the ﬁxed eﬀect estimates , 32 Linear Mixed Models with lme4 Fixed effects : Estimate Std . Error t value ( Intercept ) 251 . 41 6 . 82 36 . 8 Days 10 . 47 1 . 55 6 . 8 Note that there are no p values ( see Section 5 . 2 . 5 ) . The ﬁxed eﬀect point estimates may be obtained separately via fixef ( fm1 ) . Conditional modes of the random eﬀects coeﬃcients can be obtained with ranef ( see section 5 . 1 . 4 for information on the theory ) . Finally , we have the correlations between the ﬁxed eﬀect estimates Correlation of Fixed Effects : ( Intr ) Days - 0 . 138 The full variance - covariance matrix of the ﬁxed eﬀects estimates can be obtained in the usual R way with the vcov method ( Section 5 . 1 . 1 ) . Alternatively , coef ( summary ( fm1 ) ) will return the full ﬁxed - eﬀects parameter table as shown in the summary . Diagnostic plots lme4 provides tools for generating most of the standard graphical diagnostic plots ( with the exception of those incorporating inﬂuence measures , e . g . , Cook’s distance and leverage plots ) , in a way modeled on the diagnostic graphics of nlme ( Pinheiro and Bates 2000 ) . In particular , the following commands will show the standard ﬁtted vs . residual , scale - location , and quantile - quantile plots familiar from the plot method in base R for linear models ( objects of class lm ) : > plot ( fm1 , type = c ( " p " , " smooth " ) ) # # fitted vs residual > plot ( fm1 , sqrt ( abs ( resid ( . ) ) ) ~ fitted ( . ) , # # scale - location + type = c ( " p " , " smooth " ) ) > qqmath ( fm1 , id = 0 . 05 ) # # quantile - quantile ( In contrast to plot . lm , these scale - location and Q - Q plots are based on raw rather than standardized residuals . ) In addition to these standard diagnostic plots , which examine the validity of various assump - tions ( linearity , homoscedasticity , normality ) at the level of the residuals , one can also use the dotplot and qqmath methods for the conditional modes ( i . e . , ranef . mer objects generated by ranef ( fit ) ) to check for interesting patterns and normality in the conditional modes . lme4 does not provide inﬂuence diagnostics , but these ( and other useful diagnostic procedures ) are available in the dependent packages HLMdiag and inﬂuence . ME ( Loy and Hofmann 2014 ; Nieuwenhuis , Te Grotenhuis , and Pelzer 2012 ) . Finally , posterior predictive simulation ( Gelman and Hill 2006 ) is a generally useful diagnostic tool , adapted from Bayesian methods , for exploring model ﬁt . The user picks some summary statistic of interest . They then compute the summary statistic for an ensemble of simulations ( Section 5 . 2 . 8 ) , and see where the observed data falls within the simulated distribution ; if the observed data is extreme , we might conclude that the model is a poor representation of Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 33 reality . For example , using the sleep study ﬁt and choosing the interquartile range of the reaction times as the summary statistic : > iqrvec < - sapply ( simulate ( fm1 , 1000 ) , IQR ) > obsval < - IQR ( sleepstudy $ Reaction ) > post . pred . p < - mean ( obsval > = c ( obsval , iqrvec ) ) The ( one - tailed ) posterior predictive p value of 0 . 78 indicates that the model represents the data adequately , at least for this summary statistic . In contrast to the full Bayesian case , the procedure described here does not allow for the uncertainty in the estimated parameters . However , it should be a reasonable approximation when the residual variation is large . Sequential decomposition and model comparison Objects of class merMod have an anova method which returns F statistics corresponding to the sequential decomposition of the contributions of ﬁxed - eﬀects terms . In order to illustrate this sequential ANOVA decomposition , we ﬁt a model with orthogonal linear and quadratic Days terms , > fm4 < - lmer ( Reaction ~ polyDays [ , 1 ] + polyDays [ , 2 ] + + ( polyDays [ , 1 ] + polyDays [ , 2 ] | Subject ) , + within ( sleepstudy , polyDays < - poly ( Days , 2 ) ) ) > anova ( fm4 ) Analysis of Variance Table Df Sum Sq Mean Sq F value polyDays [ , 1 ] 1 23874 23874 46 . 08 polyDays [ , 2 ] 1 340 340 0 . 66 The relative magnitudes of the two sums of squares indicate that the quadratic term explains much less variation than the linear term . Furthermore , the magnitudes of the two F statistics strongly suggest signiﬁcance of the linear term , but not the quadratic term . Notice that this is only an informal assessment of signiﬁcance as there are no p values associated with these F statistics , an issue discussed in more detail in Section 5 . 2 . 5 . To understand how these quantities are computed , let R i contain the rows of R X ( Equa - tion 18 ) associated with the i th ﬁxed - eﬀects term . Then the sum of squares for term i is , SS i = b β > R > i R i b β ( 67 ) If DF i is the number of columns in R i , then the F statistic for term i is , F i = SS i b σ 2 DF i ( 68 ) For multiple arguments , the anova method returns model comparison statistics , 34 Linear Mixed Models with lme4 > anova ( fm1 , fm2 , fm3 ) refitting model ( s ) with ML ( instead of REML ) Data : sleepstudy Models : fm3 : Reaction ~ Days + ( 1 | Subject ) fm2 : Reaction ~ Days + ( ( 1 | Subject ) + ( 0 + Days | Subject ) ) fm1 : Reaction ~ Days + ( Days | Subject ) Df AIC BIC logLik deviance Chisq Chi Df Pr ( > Chisq ) fm3 4 1802 1815 - 897 1794 fm2 5 1762 1778 - 876 1752 42 . 08 1 8 . 8e - 11 fm1 6 1764 1783 - 876 1752 0 . 06 1 0 . 8 The output shows χ 2 statistics representing the diﬀerence in deviance between successive models , as well as p values based on likelihood ratio test comparisons . In this case , the sequential comparison shows that adding a linear eﬀect of time uncorrelated with the intercept leads to an enormous and signiﬁcant drop in deviance ( ∆deviance ≈ 42 , p ≈ 10 − 10 ) , while the further addition of correlation between the slope and intercept leads to a trivial and non - signiﬁcant change in deviance ( ∆deviance ≈ 0 . 06 ) . For objects of class lmerMod the default behavior is to reﬁt the models with ML if ﬁtted with REML = TRUE , which is necessary in order to get sensible answers when comparing models that diﬀer in their ﬁxed eﬀects ; this can be controlled via the refit argument . Computing p values One of the more controversial design decisions of lme4 has been to omit the output of p values associated with sequential ANOVA decompositions of ﬁxed eﬀects . The absence of analytical results for null distributions of parameter estimates in complex situations ( e . g . , unbalanced or partially crossed designs ) is a long - standing problem in mixed - model inference . While the null distributions ( and the sampling distributions of non - null estimates ) are asymptotically normal , these distributions are not t distributed for ﬁnite size samples — nor are the corre - sponding null distributions of diﬀerences in scaled deviances F distributed . Thus approximate methods for computing the approximate degrees of freedom for t distributions , or the denom - inator degrees of freedom for F statistics ( Satterthwaite 1946 ; Kenward and Roger 1997 ) , are at best ad hoc solutions . However , computing ﬁnite - size - corrected p values is sometimes necessary . Therefore , although the package does not provide them ( except via parametric bootstrapping , Section 5 . 1 . 3 ) , we have provided a help page to guide users in ﬁnding appropriate methods : > help ( " pvalues " ) This help page provides pointers to other packages that provide machinery for calculating p values associated with merMod objects . It also suggests framing the inference problem as a likelihood ratio test ( achieved by supplying multiple merMod objects to the anova method ( Section 5 . 2 . 4 ) , as well as alternatives to p values such as conﬁdence intervals ( Section 5 . 2 . 6 ) . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 35 Previous versions of lme4 provided the mcmcsamp function , which generated a Markov chain Monte Carlo sample from the posterior distribution of the parameters ( assuming ﬂat priors ) . Due to diﬃculty in constructing a version of mcmcsamp that was reliable even in cases where the estimated random eﬀect variances were near zero , mcmcsamp has been withdrawn . Computing conﬁdence intervals As described above ( Sections 5 . 1 . 1 , 5 . 1 . 2 , 5 . 1 . 3 ) , lme4 provides conﬁdence intervals ( using confint ) via Wald approximations ( for ﬁxed - eﬀect parameters only ) , likelihood proﬁling , or parametric bootstrapping ( the type argument selects the method ) . As is typical for computationally intensive proﬁle conﬁdence intervals in R , the intervals can be computed either directly from ﬁtted merMod objects ( in which case proﬁling is done as an interim step ) , or from a previously computed likelihood proﬁle ( of class thpr , for “theta proﬁle” ) . Parametric bootstrapping conﬁdence intervals use a thin wrapper around the bootMer function that passes the results to boot . ci from the boot package ( Canty and Ripley 2013 ; Davison and Hinkley 1997 ) for conﬁdence interval calculation . In the running sleep study examples , the 95 % conﬁdence intervals estimated by all three methods are quite similar . The largest change is a 26 % diﬀerence in conﬁdence interval widths between proﬁle and parametric bootstrap methods for the correlation between the intercept and slope random eﬀects ( { − 0 . 54 , 0 . 98 } vs . { − 0 . 48 , 0 . 68 } ) . General proﬁle zeta and related plots lme4 provides several functions ( built on lattice graphics ) for plotting the proﬁle zeta functions ( Section 5 . 1 . 2 ) and other related quantities . • The proﬁle zeta plot ( Figure 2 ; xyplot ) is simply a plot of the proﬁle zeta function for each model parameter ; linearity of this plot for a given parameter implies that the likelihood proﬁle is quadratic ( and thus that Wald approximations would be reasonably accurate ) . • The proﬁle density plot ( Figure 3 ; densityplot ) displays an approximation of the probability density function of the sampling distribution for each parameter . These densities are derived by setting the cumulative distribution function ( c . d . f ) to be Φ ( ζ ) where Φ is the c . d . f . of the standard normal distribution . This is not quite the same as evaluating the distribution of the estimator of the parameter , which for mixed models can be very diﬃcult , but it gives a reasonable approximation . If the proﬁle zeta plot is linear , then the proﬁle density plot will be Gaussian . • The proﬁle pairs plot ( Figure 4 ; splom ) gives an approximation of the two - dimensional proﬁles of pairs of parameters , interpolated from the univariate proﬁles as described in Bates and Watts ( 1988 , Chapter 6 ) . The proﬁle pairs plot shows two - dimensional 50 % , 80 % , 90 % , 95 % and 99 % marginal conﬁdence regions based on the likelihood ratio , as well as the proﬁle traces , which indicate the conditional estimates of each parameter for ﬁxed values of the other parameter . While panels above the diagonal show proﬁles with respect to the original parameters ( with random eﬀects parameters on the standard deviation / correlation scale , as for all proﬁle plots ) , the panels below the diagonal show plots on the ( ζ i , ζ j ) scale . The below - diagonal panels allow us to see distortions from an 36 Linear Mixed Models with lme4 z −2 −1 0 1 2 20 30 40 s 1 −0 . 5 0 . 0 0 . 5 s 2 4 6 8 10 s 3 22 24 26 28 30 s 240 250 260 270 ( Intercept ) 6 8 10 12 14 −2 −1 0 1 2 Days Figure 2 : Proﬁle zeta plot : xyplot ( prof . obj ) elliptical shape due to nonlinearity of the traces , separately from the one - dimensional distortions caused by a poor choice of scale for the parameter . The ζ scales provide , in some sense , the best possible set of single - parameter transformations for assessing the contours . On the ζ scales the extent of a contour on the horizontal axis is exactly the same as the extent on the vertical axis and both are centered about zero . For users who want to build their own graphical displays of the proﬁle , there is an as . data . frame method that converts proﬁle ( thpr ) objects to a more convenient format . Computing ﬁtted and predicted values ; simulating Because mixed models involve random coeﬃcients , one must always clarify whether predic - tions should be based on the marginal distribution of the response variable or on the distri - bution that is conditional on the modes of the random eﬀects ( Equation 12 ) . The fitted method retrieves ﬁtted values that are conditional on all of the modes of the random eﬀects ; the predict method returns the same values by default , but also allows for predictions to be made conditional on diﬀerent sets of random eﬀects . For example , if the re . form argument is set to NULL ( the default ) , then the predictions are conditional on all random eﬀects in the model ; if re . form is ~ 0 or NA , then the predictions are made at the population level ( all random eﬀect values set to zero ) . In models with multiple random eﬀects , the user can give re . form as a formula that speciﬁes which random eﬀects are conditioned on . lme4 also provides a simulate method , which allows similar ﬂexibility in conditioning on random eﬀects ; in addition it allows the user to choose ( via the use . u argument ) between conditioning on the ﬁtted conditional modes or choosing a new set of simulated conditional modes ( zero - mean Normal deviates chosen according to the estimated random eﬀects variance - Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 37 den s i t y 0 . 00 0 . 02 0 . 04 0 . 06 10 20 30 40 50 s 1 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 −0 . 5 0 . 0 0 . 5 1 . 0 s 2 0 . 0 0 . 1 0 . 2 0 . 3 4 6 8 10 s 3 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 22 24 26 28 30 s 0 . 000 . 010 . 020 . 030 . 040 . 050 . 06 230 240 250 260 270 ( Intercept ) 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 6 8 10 12 14 16 Days Figure 3 : Proﬁle density plot : densityplot ( prof . obj ) covariance matrices ) . Finally , the simulate method has a method for formula objects , which allows for de novo simulation in the absence of a ﬁtted model . In this case , the user must specify the random eﬀects ( θ ) , ﬁxed eﬀects ( β ) , and residual standard deviation ( σ ) param - eters via the newparams argument . The standard simulation method ( based on a merMod object ) is the basis of parametric bootstrapping ( Section 5 . 1 . 3 ) and posterior predictive sim - ulation ( Section 5 . 2 . 3 ) ; de novo simulation based on a formula provides a ﬂexible framework for power analysis . 6 . Conclusion Mixed models are an extremely useful but computationally intensive approach . Computa - tional limitations are especially important because mixed models are commonly applied to moderately large data sets ( 10 4 – 10 6 observations ) . By developing an eﬃcient , general , and ( now ) well - documented platform for ﬁtted mixed models in R , we hope to provide both a practical tool for end users interested in analyzing data and a reusable , modular framework for downstream developers interested in extending the class of models that can be easily and eﬃciently analyzed in R . We have learned much about linear mixed models in the process of developing lme4 , both from our own attempts to develop robust and reliable procedures and from the broad community of lme4 users ; we have attempted to describe many of these lessons here . In moving forward , our main priorities are ( 1 ) to maintain the reference implementation of lme4 on CRAN , developing relatively few new features ; ( 2 ) to improve the ﬂexibility , eﬃciency and scalability of mixed - model analysis across multiple compatible implementations , including both the MixedModels package for Julia and the experimental flexLambda branch of lme4 . 38 Linear Mixed Models with lme4 Scatter Plot Matrix . sig01 10 20 30 40 50 −3 −2 −1 0 . sig02 −0 . 5 0 . 0 0 . 5 1 . 0 0 . 5 1 . 0 0 1 2 3 . sig03 4 6 8 10 8 10 0 1 2 3 . sigma 22 24 26 28 30 26 28 30 0 1 2 3 ( Intercept ) 230 240 250 260 270 250260270 0 1 2 3 Days 6 8 10 12 14 16 0 1 2 3 Figure 4 : Proﬁle pairs plot : splom ( prof . obj ) Acknowledgements Rune Haubo Christensen , Henrik Singmann , Fabian Scheipl , Vincent Dorie , and Bin Dai contributed ideas and code to the current version of lme4 ; the large community of lme4 users has exercised the software , discovered bugs , and made many useful suggestions . Søren Højsgaard participated in useful discussions and Xi Xia and Christian Zingg exercised the code and reported problems . We would like to thank the Banﬀ International Research Station for hosting a working group on lme4 , and an NSERC Discovery grant and NSERC postdoctoral fellowship for funding . References Bates D , Maechler M ( 2014 ) . Matrix : Sparse and Dense Matrix Classes and Methods . R Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 39 package version 1 . 1 - 3 , URL http : / / CRAN . R - project . org / package = Matrix . Bates D , Mullen KM , Nash JC , Varadhan R ( 2014 ) . minqa : Derivative - free optimization algorithms by quadratic approximation . R package version 1 . 2 . 3 , URL http : / / CRAN . R - project . org / package = minqa . Bates DM , DebRoy S ( 2004 ) . “Linear mixed models and penalized least squares . ” Journal of Multivariate Analysis , 91 ( 1 ) , 1 – 17 . doi : 10 . 1016 / j . jmva . 2004 . 04 . 013 . Bates DM , Watts DG ( 1988 ) . Nonlinear Regression Analysis and Its Applications . Wiley , Hoboken , NJ . ISBN 0 - 471 - 81643 - 4 . Belenky G , Wesensten NJ , Thorne DR , Thomas ML , Sing HC , Redmond DP , Russo MB , Balkin TJ ( 2003 ) . “Patterns of performance degradation and restoration during sleep re - striction and subsequent recovery : a sleep dose - response study . ” Journal of Sleep Research , 12 , pp . 1 – 12 . Bolker BM , Gardner B , Maunder M , Berg CW , Brooks M , Comita L , Crone E , Cubaynes S , Davies T , de Valpine P , Ford J , Gimenez O , Kéry M , Kim EJ , Lennert - Cody C , Magnusson A , Martell S , Nash J , Nielsen A , Regetz J , Skaug H , Zipkin E ( 2013 ) . “Strategies for ﬁtting nonlinear ecological models in R , AD Model Builder , and BUGS . ” Methods in Ecology and Evolution , 4 ( 6 ) , 501 – 512 . ISSN 2041210X . doi : 10 . 1111 / 2041 - 210X . 12044 . URL http : / / doi . wiley . com / 10 . 1111 / 2041 - 210X . 12044 . Canty A , Ripley B ( 2013 ) . boot : Bootstrap R ( S - Plus ) Functions . R package version 1 . 3 - 9 , URL http : / / CRAN . R - project . org / package = boot . Chambers JM ( 1993 ) . “Linear Models . ” In JM Chambers , TJ Hastie ( eds . ) , Statistical Models in S , chapter 4 , pp . 95 – 144 . Chapman & Hall . Chen Y , Davis TA , Hager WW , Rajamanickam S ( 2008 ) . “Algorithm 887 : CHOLMOD , Supernodal Sparse Cholesky Factorization and Update / Downdate . ” ACM Trans . Math . Softw . , 35 ( 3 ) , 22 : 1 – 22 : 14 . ISSN 0098 - 3500 . doi : 10 . 1145 / 1391989 . 1391995 . URL http : / / doi . acm . org / 10 . 1145 / 1391989 . 1391995 . Cook RD , Weisberg S ( 1982 ) . Residuals and inﬂuence in regression . New York : Chapman and Hall . Davis T ( 2006 ) . Direct Methods for Sparse Linear Systems . SIAM , Philadelphia , PA . Davison AC , Hinkley DV ( 1997 ) . Bootstrap Methods and Their Applications . Cambridge University Press , Cambridge , England . ISBN 0 - 521 - 57391 - 2 . Doran H , Bates D , Bliese P , Dowling M ( 2007 ) . “Estimating the multilevel Rasch model : With the lme4 package . ” Journal of Statistical Software , 20 ( 2 ) , 1 – 18 . Dorie V ( 2013 ) . blme : Bayesian Linear Mixed - Eﬀects Models . R package version 1 . 0 - 1 , URL http : / / CRAN . R - project . org / package = blme . Efron B , Morris C ( 1977 ) . “Stein’s Paradox in Statistics . ” Scientiﬁc American , 236 , 119 – 127 . doi : 10 . 1038 / scientificamerican0577 - 119 . 40 Linear Mixed Models with lme4 Gelman A ( 2005 ) . “Analysis of variance — why it is more important than ever . ” The Annals of Statistics , 33 ( 1 ) , 1 – 53 . Gelman A , Carlin JB , Stern HS , Dunson DB , Vehtari A , Rubin DB ( 2013 ) . Bayesian data analysis . CRC press . Gelman A , Hill J ( 2006 ) . Data Analysis Using Regression and Multilevel / Hierarchical Models . Cambridge University Press , Cambridge , England . URL http : / / www . stat . columbia . edu / ~ gelman / arm / . Golub GH , Pereyra V ( 1973 ) . “The Diﬀerentiation of Pseudo - Inverses and Nonlinear Least Squares Problems Whose Variables Separate . ” SIAM Journal on Numerical Analysis , 10 ( 2 ) , pp . 413 – 432 . ISSN 00361429 . URL http : / / www . jstor . org / stable / 2156365 . Henderson Jr CR ( 1982 ) . “Analysis of Covariance in the Mixed Model : Higher - Level , Nonho - mogeneous , and Random Regressions . ” Biometrics , 38 ( 3 ) , 623 – 640 . ISSN 0006341X . URL http : / / www . jstor . org / stable / 2530044 . Horn R , Zhang F ( 2005 ) . “Basic Properties of the Schur Complement . ” In F Zhang ( ed . ) , The Schur Complement and Its Applications , volume 4 of Numerical Methods and Algorithms , pp . 17 – 46 . Springer US . URL http : / / dx . doi . org / 10 . 1007 / 0 - 387 - 24273 - 2 _ 2 . Johnson SG ( 2014 ) . “The NLopt nonlinear - optimization package . ” URL http : / / ab - initio . mit . edu / nlopt . Kenward MG , Roger JH ( 1997 ) . “Small sample inference for ﬁxed eﬀects from restricted maximum likelihood . ” Biometrics , 53 ( 3 ) , 983 – 997 . Khatri C , Rao CR ( 1968 ) . “Solutions to some functional equations and their applications to characterization of probability distributions . ” Sankhy¯a : The Indian Journal of Statistics , Series A , pp . 167 – 180 . Klein K , Neira J ( 2013 ) . “Nelder - Mead Simplex Optimization Routine for Large - Scale Problems : A Distributed Memory Implementation . ” Computational Economics . doi : 10 . 1007 / s10614 - 013 - 9377 - 8 . Laird NM , Ware JH ( 1982 ) . “Random - Eﬀects Models for Longitudinal Data . ” Biometrics , 38 , 963 – 974 . Loy A , Hofmann H ( 2014 ) . “HLMdiag : A Suite of Diagnostics for Hierarchical Linear Models in R . ” Journal of Statistical Software , 56 ( 5 ) , 1 – 28 . URL http : / / www . jstatsoft . org / v56 / i05 / . Nash JC , Varadhan R ( 2011 ) . “Unifying Optimization Algorithms to Aid Software System Users : optimx for R . ” Journal of Statistical Software , 43 ( 9 ) , 1 – 14 . URL http : / / www . jstatsoft . org / v43 / i09 / . Nieuwenhuis R , Te Grotenhuis M , Pelzer B ( 2012 ) . “Inﬂuence . ME : Tools for Detecting Inﬂu - ential Data in Mixed Eﬀects Models . ” R Journal , 4 ( 2 ) , 38 – 47 . Pinheiro J , Bates D , DebRoy S , Sarkar D , R Core Team ( 2014 ) . nlme : Linear and Nonlinear Mixed Eﬀects Models . R package version 3 . 1 - 117 , URL http : / / CRAN . R - project . org / package = nlme . Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 41 Pinheiro JC , Bates DM ( 1996 ) . “Unconstrained parametrizations for variance - covariance matrices . ” Statistics and Computing , 6 ( 3 ) , 289 – 296 . doi : 10 . 1007 / BF00140873 . Pinheiro JC , Bates DM ( 2000 ) . Mixed - Eﬀects Models in S and S - Plus . Springer . ISBN 0 - 387 - 98957 - 0 . Powell MJD ( 2009 ) . “The BOBYQA algorithm for bound constrained optimization without derivatives . ” Technical Report DAMTP 2009 / NA06 , Centre for Mathematical Sciences , Uni - versity of Cambridge , Cambridge , England . URL http : / / www . damtp . cam . ac . uk / user / na / NA _ papers / NA2009 _ 06 . pdf . Satterthwaite FE ( 1946 ) . “An Approximate Distribution of Estimates of Variance Compo - nents . ” Biometrics Bulletin , 2 ( 6 ) , 110 – 114 . Sánchez - Espigares JA , Ocaña J ( 2009 ) . “An R implementation of bootstrap procedures for mixed models . ” Conference presentation , useR ! Accessed 25 May 2014 , URL http : / / www . r - project . org / conferences / useR - 2009 / slides / SanchezEspigares + Ocana . pdf . Vaida F , Blanchard S ( 2005 ) . “Conditional Akaike information for mixed - eﬀects models . ” Biometrika , 92 ( 2 ) , 351 – 370 . Wood S , Scheipl F ( 2013 ) . gamm4 : Generalized additive mixed models using mgcv and lme4 . R package version 0 . 2 - 2 , URL http : / / CRAN . R - project . org / package = gamm4 . A . Appendix : modularization examples A . 1 . Homogeneous covariance among random eﬀects terms Consider the following mixed - model formula , > form < - respVar ~ 1 + ( explVar1 | groupFac1 ) + ( explVar2 | groupFac2 ) in which we have a ﬁxed intercept and two random eﬀects terms , each with a random slope and intercept . The lmer function would ﬁt this formula to data by estimating two 2 × 2 covariance matrices—one for each random eﬀects term . It is not possible to use the current version of lmer to ﬁt a model in which both terms share the same 2 × 2 covariance matrix . We illustrate the use of the modular functions in lme4 to ﬁt such a model . We simulate data from this homogeneous covariance model using a balanced design that crosses both grouping factors , > set . seed ( 1 ) > # number of groups per > # grouping factor > nGrps < - 50 > # two explanatory variables > explVar < - data . frame ( explVar1 = rnorm ( nGrps ^ 2 ) , + explVar2 = rnorm ( nGrps ^ 2 ) ) 42 Linear Mixed Models with lme4 > # two group factors > groupFac < - expand . grid ( groupFac1 = as . factor ( 1 : nGrps ) , + groupFac2 = as . factor ( 1 : nGrps ) ) > # random intercepts > randomIntercept < - expand . grid ( randomIntercept1 = rnorm ( nGrps ) , + randomIntercept2 = rnorm ( nGrps ) ) > # random slopes , negatively > # correlated with the > # intercepts > rnmdSlope < - expand . grid ( randomSlope1 = rnorm ( nGrps ) , + randomSlope2 = rnorm ( nGrps ) ) - randomIntercept > # linear predictor > linearPredictor < - apply ( randomIntercept + rnmdSlope * explVar , 1 , sum ) > # residual error > residError < - rnorm ( nGrps ^ 2 ) > # resp variable > respVar < - residError + linearPredictor > # data set > dat < - data . frame ( respVar , explVar , groupFac ) > head ( dat ) respVar explVar1 explVar2 groupFac1 groupFac2 1 - 1 . 9019 - 0 . 6265 - 1 . 8055 1 1 2 2 . 2711 0 . 1836 - 0 . 6780 2 1 3 - 3 . 1781 - 0 . 8356 - 0 . 4734 3 1 4 - 3 . 0882 1 . 5953 1 . 0274 4 1 5 - 0 . 2488 0 . 3295 - 0 . 5974 5 1 6 - 3 . 6511 - 0 . 8205 1 . 1598 6 1 To ﬁt form to dat using a homogeneous covariance model , we construct the homoLmer function , which we describe in sections . To simplify the problem , we only allow the speciﬁcation of three arguments : formula , data , and use . mkMerMod . This last argument speciﬁes how to process the ﬁtted model , which we discuss below . All other relevant parameters are set to lmer defaults , unless otherwise noted below . Here is the argument list for homoLmer , > homoLmer < - function ( formula , data , use . mkMerMod = FALSE ) { } After saving the call , the next step is to parse the formula and data using the lFormula modular function ( setting REML to FALSE for simplicity ) , > mc < - match . call ( ) > lfHetero < - lfHomo < - lFormula ( formula = formula , data = data , REML = FALSE ) We then check to make sure that this formula is a candidate for a homogeneous covariance model . In particular , a homogeneous covariance model only makes sense if each random eﬀects term has the same number of random eﬀects coeﬃcients , Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 43 > if ( length ( pRE < - unique ( sapply ( cnms < - lfHomo $ reTrms $ cnms , length ) ) ) > 1L ) { + stop ( " each random effects term must have the same number \ n " , + " of model matrix columns for a homogeneous structure " ) + } We save the number of ﬁxed eﬀects coeﬃcients , p , number of covariance parameters per term , nth , and number random eﬀects terms , n _ trms , > p < - ncol ( lfHomo $ X ) > nth < - choose ( pRE + 1 , 2 ) > n _ trms < - length ( cnms ) We now modify the parsed random eﬀects terms to have homogeneous covariance structure , which requires three lines . First , use only the ﬁrst nth elements of the covariance parameters , theta , as these will be repeated to generate common covariance structure across random eﬀects terms . Second , match the lower bounds of the covariance parameters to the new theta . Third , adjust the mapping from the new theta to the nonzero elements of the transposed relative covariance factor , Lambdat . > lfHomo $ reTrms < - within ( lfHomo $ reTrms , { + theta < - theta [ 1 : nth ] + lower < - lower [ 1 : nth ] + Lind < - rep ( 1 : nth , length = length ( lfHomo $ reTrms $ Lambdat @ x ) ) + } ) Then we construct and minimize a deviance function for the homogeneous model , > devf < - do . call ( mkLmerDevfun , lfHomo ) > opt < - optimizeLmer ( devf ) The object opt contains the optimized values of the homogeneous covariance parameters . However , in order to interpret and make inferences about this model , we need to do more work . We highlight two general approaches to doing this , which in this example are selected with the use . mkMerMod logical argument . If use . mkMerMod is set to FALSE , then the parsed data and formula object , lfHomo , deviance function , devf , and optimization results , opt , are returned , > if ( ! use . mkMerMod ) { + return ( list ( lf = lfHomo , devf = devf , opt = opt ) ) + } This option means that the convenience functions of the lme4 output module ( Section 5 ) are not available because the resulting object is not a merMod object . However , constructing a merMod object that will be treated appropriately by the convenience functions of the output module is rather diﬃcult , and sometimes impossible , and should therefore always be done with caution . In particular , mkMerMod should be used to work with speciﬁc functions from 44 Linear Mixed Models with lme4 the output module ; users should never assume that because some aspects of the output module give appropriate results with a merMod object constructed from a modular ﬁt , all aspects will work . It will sometimes be necessary , as we next illustrate , to extend the merMod class and rewrite some methods from the output module that are specialized for this new class . Next we update the unmodiﬁed model by capturing the modiﬁed model estimates of the covariance parameters , > th < - rep ( opt $ par , n _ trms ) constructing a deviance function for the unmodiﬁed model , > devf0 < - do . call ( mkLmerDevfun , lfHetero ) and installing the parameters in the environment of the unmodiﬁed deviance function , > devf0 ( opt $ par < - th ) Finally , we need to extend the merMod class , > setClass ( " homoLmerMod " , representation ( thetaUnique = " numeric " ) , + contains = " lmerMod " ) and redeﬁne some methods for this extension , > logLik . homoLmerMod < - function ( object , . . . ) { + ll < - lme4 : : : logLik . merMod ( object , . . . ) + attr ( ll , " df " ) < - length ( object @ beta ) + + length ( object @ thetaUnique ) + + object @ devcomp [ [ " dims " ] ] [ [ " useSc " ] ] + return ( ll ) + } > refitML . homoLmerMod < - function ( object , newresp , . . . ) { + if ( ! isREML ( object ) & & missing ( newresp ) ) return ( object ) + stop ( " can ' t refit homoLmerMod objects yet " ) + } > mod < - homoLmer ( form , dat , use . mkMerMod = TRUE ) > summary ( mod ) Linear mixed model fit by maximum likelihood [ ' homoLmerMod ' ] Formula : form Data : dat AIC BIC logLik deviance df . resid 7920 7950 - 3955 7910 2492 Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 45 Scaled residuals : Min 1Q Median 3Q Max - 3 . 094 - 0 . 640 - 0 . 008 0 . 648 3 . 426 Random effects : Groups Name Variance Std . Dev . Corr groupFac1 ( Intercept ) 0 . 955 0 . 977 explVar1 1 . 878 1 . 370 - 0 . 76 groupFac2 ( Intercept ) 0 . 955 0 . 977 explVar2 1 . 878 1 . 370 - 0 . 76 Residual 1 . 026 1 . 013 Number of obs : 2500 , groups : groupFac1 , 50 groupFac2 , 50 Fixed effects : Estimate Std . Error t value ( Intercept ) - 0 . 0477 0 . 1291 - 0 . 37 A . 2 . Homogeneous variance over all random eﬀects We modify the sleepstudy example , > parsedFormula < - lFormula ( formula = Reaction ~ Days + ( Days | Subject ) , + data = sleepstudy ) such that the random slope and intercept are uncorrelated and have identical variance ( be - cause the slope and intercept have diﬀerent units , this example is not entirely sensible — it is oﬀered for technical illustration only ) . The reTrms component of parsedFormula contains the relevant information , and can be modiﬁed using the within function , > parsedFormula $ reTrms < - within ( parsedFormula $ reTrms , { + # capture the dimension of + # Lambda + q < - nrow ( Lambdat ) + # alter the mapping from theta + # to the non - zero elements of + # Lambda + Lind < - rep ( 1 , q ) + # construct a diagonal Lambda + Lambdat < - sparseMatrix ( 1 : q , 1 : q , x = Lind ) + # initialize 1 - dimensional + # theta + theta < - 1 + } ) We ﬁt this modiﬁed model using the modularised functions , 46 Linear Mixed Models with lme4 > devianceFunction < - do . call ( mkLmerDevfun , parsedFormula ) > optimizerOutput < - optimizeLmer ( devianceFunction ) Because lme4 is not designed for post - processing of such models , mkMerMod cannot be expected to give sensible results . To illustrate this point , we use it anyway and point out where it makes mistakes , > mkMerMod ( rho = environment ( devianceFunction ) , + opt = optimizerOutput , + reTrms = parsedFormula $ reTrms , + fr = parsedFormula $ fr ) Linear mixed model fit by REML [ ' lmerMod ' ] REML criterion at convergence : 1759 Warning : data length is not a multiple of split variable Random effects : Groups Name Std . Dev . Corr Subject ( Intercept ) 8 . 9 Days 12 . 6 0 . 71 Residual 27 . 4 Number of obs : 180 , groups : Subject , 18 Fixed Effects : ( Intercept ) Days 251 . 4 10 . 5 Note that mkMerMod is attempting to estimate a correlation between the slope and intercept , which makes no sense within the current model . When using modular functions , the user is responsible for producing interpretable output . For example , to ﬁnd the residual and random - eﬀects standard deviation , we may use , > with ( environment ( devianceFunction ) , { + # dimensions of the problem + n < - length ( resp $ y ) + p < - length ( pp $ beta0 ) + # penalized weighted residual + # sum - of - squares + pwrss < - resp $ wrss ( ) + pp $ sqrL ( 1 ) + # residual standard deviation + sig < - sqrt ( pwrss / ( n - p ) ) + c ( Residual = sig , + # random effects variance + # requires no matrix algebra + # in this case because the + # covariance matrix of the Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 47 + # random effects + # is proportional to the + # identity matrix + Subject = sig * pp $ theta ) + } ) Residual Subject 27 . 374 8 . 899 A . 3 . Additive models It is possible to interpret additive models as a particular class of mixed models ( Wood and Scheipl 2013 ) . The main beneﬁt of this approach is that it bypasses the need to select a smoothness parameter using cross - validation or generalized cross - validation . However , it is inconvenient to specify additive models using the lme4 formula interface . The gamm4 package wraps the modularized functions of lme4 within a more convenient interface ( Wood and Scheipl 2013 ) . In particular , the strategy involves the following steps , 1 . Convert a gamm4 formula into an lme4 formula that approximates the intended model . 2 . Parse this formula using lFormula . 3 . Modify the resulting transposed random eﬀects model matrix , Z > , so that the intended additive model results . 4 . Fit the resulting model using the remaining modularized functions ( Table 1 ) . Here we illustrate this general strategy using a simple simulated data set , > library ( " gamm4 " ) > library ( " splines " ) > > set . seed ( 1 ) > # sample size > n < - 100 > # number of columns in the > # simulation model matrix > pSimulation < - 3 > # number of columns in the > # statistical model matrix > pStatistical < - 8 > # explanatory variable > x < - rnorm ( n ) > # simulation model matrix > Bsimulation < - ns ( x , pSimulation ) > # statistical model matrix > Bstatistical < - ns ( x , pStatistical ) 48 Linear Mixed Models with lme4 > # simulation model coefficients > beta < - rnorm ( pSimulation ) > # response variable > y < - as . numeric ( Bsimulation % * % beta + rnorm ( n , sd = 0 . 3 ) ) We plot the resulting data along with the predictions from the generating model , > par ( mar = c ( 4 , 4 , 1 , 1 ) , las = 1 , bty = " l " ) > plot ( x , y , las = 1 ) > lines ( x [ order ( x ) ] , ( Bsimulation % * % beta ) [ order ( x ) ] ) l l l l l l l ll l l l l l l l l l l l ll l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ll l l l l l l l −2 −1 0 1 2 −1 . 0 −0 . 5 0 . 0 0 . 5 1 . 0 x y We now set up an approximate lme4 model formula , and parse it using lFormula , > pseudoGroups < - as . factor ( rep ( 1 : pStatistical , length = n ) ) > parsedFormula < - lFormula ( y ~ x + ( 1 | pseudoGroups ) ) We now insert a spline basis into the parsedFormula object , > parsedFormula $ reTrms < - within ( parsedFormula $ reTrms , { + Bt < - t ( as . matrix ( Bstatistical ) ) [ ] + cnms $ pseudoGroups < - " spline " + Zt < - as ( Bt , class ( Zt ) ) + } ) Finally we continue with the remaining modular steps , Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 49 > devianceFunction < - do . call ( mkLmerDevfun , parsedFormula ) > optimizerOutput < - optimizeLmer ( devianceFunction ) > mSpline < - mkMerMod ( rho = environment ( devianceFunction ) , + opt = optimizerOutput , + reTrms = parsedFormula $ reTrms , + fr = parsedFormula $ fr ) > mSpline Linear mixed model fit by REML [ ' lmerMod ' ] REML criterion at convergence : 60 . 73 Random effects : Groups Name Std . Dev . pseudoGroups spline 0 . 293 Residual 0 . 300 Number of obs : 100 , groups : pseudoGroups , 8 Fixed Effects : ( Intercept ) x - 0 . 0371 - 0 . 1714 Computing the ﬁtted values of this additive model requires some custom code , and illustrates the general principle that methods for merMod objects constructed from modular ﬁts should only be used if the user knows what she is doing , > # evenly spaced x - values > # for constructing smooth > # curve > xNew < - seq ( min ( x ) , max ( x ) , length = 100 ) > # construct spline matrix > # over these x - values > newBstatistical < - predict ( Bstatistical , xNew ) > # calculate the linear > # predictor > yHat < - cbind ( 1 , xNew ) % * % getME ( mSpline , " fixef " ) + + newBstatistical % * % getME ( mSpline , " u " ) > # plot the results > par ( mar = c ( 4 , 4 , 1 , 1 ) , las = 1 , bty = " l " ) > plot ( x , y ) > lines ( xNew , yHat ) > # add true relationship > lines ( x [ order ( x ) ] , ( Bsimulation % * % beta ) [ order ( x ) ] , lty = 2 ) > legend ( " topright " , bty = " n " , c ( " fitted " , " generating " ) , lty = 1 : 2 , col = 1 ) 50 Linear Mixed Models with lme4 l l l l l l l ll l l l l l l l l l l l ll l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ll l l l l l l l −2 −1 0 1 2 −1 . 0 −0 . 5 0 . 0 0 . 5 1 . 0 x y fitted generating Aﬃliation : Douglas Bates Department of Statistics , University of Wisconsin 1300 University Ave . Madison , WI 53706 , U . S . A . E - mail : bates @ stat . wisc . edu Martin Mächler Seminar für Statistik , HG G 16 ETH Zurich 8092 Zurich , Switzerland E - mail : maechler @ stat . math . ethz . ch Benjamin M . Bolker Departments of Mathematics & Statistics and Biology McMaster University 1280 Main Street W Hamilton , ON L8S 4K1 , Canada E - mail : bolker @ mcmaster . ca Steven C . Walker Department of Mathematics & Statistics McMaster University 1280 Main Street W Douglas Bates , Martin Mächler , Ben Bolker , Steve Walker 51 Hamilton , ON L8S 4K1 , Canada E - mail : scwalker @ math . mcmaster . ca