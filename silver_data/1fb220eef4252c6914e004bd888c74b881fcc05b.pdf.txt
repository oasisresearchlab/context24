Design Scenarios as an Assessment of Adaptive Expertise * JOAN M . T . WALKER Long Island University , 720 Northern Boulevard , Brookville , NY 11548 , USA . E - mail : joan . walker @ liu . edu DAVID S . CORDRAY and PAUL H . KING Vanderbilt University , Nashville , TN 37203 , USA SEAN P . BROPHY Purdue University , Department of Engineering Education , 400 Centennial Mall Drive , ENAD 202A , West Lafayette , IN 47907 - 2016 , USA . The ability to adapt to new challenges is critical to success in rapidly advancing fields . However , educators and researchers struggle with how to measure and teach for adaptive expertise . This study used a design scenario to assess how undergraduates approach novel design challenges . A scenario presents individual students with a short realistic description of a complex , open - ended design problem . In this study , we developed a scenario from a cardiologist’s concerns about the design of an implantable defibrillator . Participants included 63 senior design students and 37 freshmen enrolled in a signal analysis course . After reading the scenario , students responded to three questions : What do you need to do to test the doctor’s hypothesis ? What questions do you have for the doctor ? and , How confident are you in your response ? The first question tapped students’ efficiency or their ability to devise an appropriate response . The second tapped students’ innovation or their ability to consider important facets of the problem . The third question estimated students’ confidence / cautiousness . Data were collected at the beginning and end of one semester . Analysis showed that seniors consistently devised more efficient and innovative solutions than did freshmen . Seniors were also more confident in their problem - solving abilities . Over time , all students became more innovative and more confident . Findings are discussed in terms of what they suggest about undergraduates’ intellectual development at entry to and exit from a standard four - year curriculum and how adaptive expertise might be assessed within the context of students’ regular academic coursework . Keywords : adaptive expertise ; efficiency ; innovation ; design . INTRODUCTION EXPERTS ARE EFFICIENT problem - solvers ; they appropriately apply their understanding , and they have a depth of understanding that makes difficult problems tractable [ 1 ] . However , not all experts are created equal . Hatano and Inagaki [ 2 ] have identified two kinds of experts : routine experts and adaptive . Routine experts are efficient and technically proficient ; however , they may fail to adapt when new types of problems develop [ 3 ] . Adaptive experts possess content knowledge and technical proficiency similar to that of routine experts , but they differ in important ways . Adaptive experts use different representa - tions and methods to solve problems ; they seek out opportunities for new learning in their field of expertise , successfully monitor their understand - ing , and conceive of knowledge as dynamic rather than static [ 1 , 4 , 5 ] . These methods and attitudes allow adaptive experts to act flexibly in novel situations [ 1 , 2 ] . The ability to adapt to new challenges and problems is critical to success in rapidly advancing fields . One growing professional field is biomedical engineering design , which involves solving open - ended problems under conditions of uncertainty and risk . For example , design products include pediatric ventilators , components for surgical procedures , and laboratory equipment . Given its demand for flexible problem - solving skills , the field of biomedical engineering design is an ideal arena in which to study the phenomenon of adaptive expertise [ 2 ] . A significant challenge to researchers interested in adaptive expertise is establishing valid and reliable ways of capturing and representing what people know , how they apply their skills , and how their performance varies over time and across problems . Recently , Schwartz , Bransford and Sears [ 6 ] have articulated several dimensions of adaptive problem - solving that lend themselves to empirical study ( see Fig . 1 ) . One dimension is efficiency . A second dimension is innovation . A third attitudinal dimension is an appropriate level of confidence . * Accepted 16 December 2005 . 645 Int . J . Engng Ed . Vol . 22 , No . 3 , pp . 645 – 651 , 2006 0949 - 149X / 91 $ 3 . 00 + 0 . 00 Printed in Great Britain . # 2006 TEMPUS Publications . EFFICIENCY Efficiency is a common metric of expertise and is often defined as the ability to retrieve and apply appropriate skills and knowledge [ 1 ] . Efficiency can also be defined in terms of consistency and accuracy [ 6 ] . For instance , expert physicians can reliably , quickly and accurately diagnose and treat a patient’s complaint . Similarly , in the context of engineering design , efficiency can be defined as the ability to devise appropriate strategies for ad - dressing a problem . In fact , when people think of engineering , efficiency is likely the first dimension to come to mind . This is because engineers often focus on computation and the derivation of accu - rate solutions . Developmental differences have been observed in students’ ability to derive efficient solutions to design challenges . For instance , when asked to define the design process , beginning design students rarely engaged in iterative processes such as evaluation and revision [ 7 ] . By contrast , more advanced students tended to progress to later stages of the design process including decision - making and project realization , and to continually evaluate the appropriateness of their decisions [ 8 , 9 ] . Taken as a whole , this work suggests that beginning design students are not as efficient or able to devise a complete , working solution as their more advanced peers . INNOVATION Innovation is a less well understood facet of expertise . Yet , understanding innovation is critical to understanding routine vs . adaptive expertise . Innovation is related to efficiency in that it involves drawing appropriately on prior know - ledge . Innovation differs from efficiency in that it requires the ability to recognize and then break away from routine approaches . In the context of engineering design , innovation can be defined as the ability to stop and consider a problem from multiple vantage points rather than foreclosing on a more immediate and smaller set of possibilities . For instance , expert designers tend to take a breadth - first approach in which varied approaches are considered [ 10 , 11 ] . By contrast , students tend to do little exploration and elaboration of the design space , often ‘getting stuck’ modeling a single alternative solution rather than considering multiple options [ 9 ] . Novices’ limited perspective - taking also appears in their tendency to design for themselves rather than considering the needs and constraints of the user , and in their limited atten - tion to contextual factors , such as safety concerns and the marketplace [ 7 , 8 , 12 ] . Taken as a whole , this work demonstrates that novice designers are not as innovative or do not define problems as carefully and elaborately as do experts . CONFIDENCE In addition to knowledge and skills , attitudes and beliefs are important to adaptive problem - solving [ 3 ] . One important attitudinal variable is confidence . As noted by Bandura [ 13 ] , confidence can be a protective factor in the face of adversity and challenge ; however , confidence can be debili - tating when it is not aligned with actual compe - tence . Thus , an important affective dimension of adaptive expertise is high caution coupled with high confidence [ 6 ] . Balance between the two is likely important because it sustains persistence , and supports the ability to model problems from multiple perspectives and ‘let go’ of assumptions that may interfere with innovation . For instance , in the field of engineering design , tentative confi - dence is likely to support a designer’s determina - tion to create novel but safe and effective products . In sum , assessments of expertise in engineering design have been useful in identifying patterns in students’ thinking , some of which may interfere with their ability to enter the professional design community . However , this literature is limited . First , the methods used are time - consuming and do not lend themselves well to use by educators , nor do they provide students with much feedback about their professional development . The findings reported here , for instance , were derived from detailed analysis of think - aloud protocols during problem - solving and observations of design activ - ities in situ [ 7 – 11 ] . Moreover , design performance has not been explicitly examined via theoretical models of adaptive problem - solving [ cf . 8 ] . This study addressed these limitations . To our know - ledge , it is one of the first efforts to bridge research in design cognition and design education to the construct of adaptive expertise [ 2 ] . Second , it tested the utility of one ‘education - friendly’ tool for comparing students’ approaches to design chal - lenges : design scenarios . DESIGN SCENARIOS AS A MEASURE OF ADAPTIVE EXPERTISE Design scenarios present individual students with a short description of a realistic design Fig . 1 . Dimensions of adaptive problem - solving identified by Schwartz , Bransford and Sears ( in press ) . J . Walker et al . 646 challenge and then ask them to describe how they would solve the problem . In developing our scenarios we had three goals in mind . First , we wanted to know if we could measure how adap - tively undergraduates approach design problems . Specifically , we wanted to know if we could reliably assess the dimensions of efficiency and innovation . Second , we wanted to know if our measure was sensitive to changes in students’ approaches over time . That is , would our measure distinguish people by their level of adaptive expert - ise ( e . g . sort beginning from advanced engineering students ) ? Third , we wanted to gain an under - standing of relations among efficiency , innovation and confidence as students acquire domain know - ledge . Our overarching goal was to enhance under - standing of students’ reasoning about design and , in turn , use that understanding to create instruc - tion that promotes the development of adaptive expertise . In this study , we developed a scenario from information provided by a cardiologist at the Vanderbilt University Medical Center . The sce - nario is presented in Fig . 2 . After reading the problem , we asked students to respond to the following questions : ( 1 ) What do you need to do to test the doctor’s hypothesis ? ( 2 ) What questions do you have for the doctor ? and , ( 3 ) How confi - dent are you in your response ? The first question was designed to tap students’ efficiency or their ability to devise an appropriate response to the problem statement . The second question was designed to tap students’ innovation or their ability to consider a range of potentially impor - tant facets of the problem . The third question was intended to estimate students’ confidence / cautiousness . HYPOTHESES We expected that , relative to beginning engin - eering students , advanced students would : 1 ) generate more efficient solutions ( i . e . generate a greater number of more complex and accurate strategies for testing the doctor’s hypothesis ) , 2 ) demonstrate more innovative thinking ( i . e . pose a greater number and variety of questions about the problem ) , and 3 ) have higher levels of confidence . Over time , we expected that both beginning and advanced students would increase in their effi - ciency , innovation and confidence . Correlations among confidence and efficiency and innovation were an open question . We also expected our results to reflect differ - ences in course content . Put another way , we expected to find differences in the form and the substance of students’ thinking based on their current educational experiences . Specifically , we expected students who were learning about the phenomenon of cardiac signal analysis to generate more questions about the heart , etc . By contrast , we expected students whose coursework was focused on design to pose questions pertaining to the design process ( e . g . customer needs , other approaches ) . PARTICIPANTS We piloted our measure with 37 students enrolled in two sections of an introductory - level engineering science course , and 63 students enrolled in a year - long senior design course . The introductory course is a one - hour credit class focused specifically on cardiac signal analysis ; it is designed to complement the skills and know - ledge students acquire in a larger 3 - credit intro - duction to the engineering course . The year - long design course is a capstone experience in which students are expected to apply the skills and knowledge they have gained in the previous three years . Students responded to the scenario at two time points , approximately four months apart . First - year students completed the problem as part of a Fig . 2 . Design scenario completed by first - year and fourth - year engineering students . Design Scenarios as an Assessment of Adaptive Expertise 647 pre - and post - test at the beginning and end of the fall semester 2003 . Fourth - year students completed the problem electronically as a homework assign - ment during fall 2003 and spring 2004 . Between assessments , fourth - year students were engaged in the process of developing a design project . Complete data was obtained for 28 freshmen ( 76 % participation rate ) and 39 seniors ( 62 % participation rate ) . ANALYSIS We began our analysis by asking the two parti - cipating course instructors to create a scoring rubric which identified a potential range of student responses and classified them on a continuum from novice to expert ( see Appendix A ) . For instance , in response to the first question , a student might offer the strategy ‘gather data from 2 sites . ’ Given its simplicity , this strategy was categorized as a novice approach . By contrast , the more complex sugges - tion ‘gather data from 1 site and compare it to 2 sites to see if 2 sites are necessary’ was categorized as an expert - level strategy . Instructors similarly identified the potential range of student questions and classified them as novice , proficient or expert . Novice questions focused on comprehension ( e . g . ‘What is ven - tricular tachycardia ? ’ ) . Questions demonstrating problem - specific knowledge of cardiac signals ( e . g . ‘Can I see a tracing of the ECG ? ’ ) were categorized as proficient . Questions that suggested a broader approach to the problem and an under - standing of design ( e . g . ‘Why is this hypothesis the best one ? ’ and ‘What other approaches have people taken to this problem ? ’ ) were categorized as expert . The rubric was applied by two graduate students , blinded to time point and class member - ship . With regard to our first question , ‘What do you need to do to test the doctor’s hypothesis ? ’ , our raters counted the number of strategies gener - ated by students and evaluated each response as novice or expert ( novice strategies received a rating of 1 ; expert strategies received a rating of 2 ) . These coders also rated the student’s overall solution in terms of its accuracy ( inaccurate strategies = 0 , somewhat accurate = . 50 , completely accurate = 1 ) . For our second question , ‘What questions do you have for the doctor ? ’ , our coders counted the number of questions , and then rated each response as novice , proficient or expert . Novice questions received a score of 1 , proficient a score of 2 , expert questions a score of 3 . Inter - rater reliability on these measures was acceptable : number of strategies > . 89 ; quality of strategies range = . 52 – . 81 ; accuracy of strategies > . 68 ; number of questions > . 88 ; quality of questions , range = . 52 – 1 . 00 . The number of strategies and questions students could generate was not limited . We then used these data to form the efficiency and innovation variables . The efficiency variable was derived as the sum of the quality of students’ strategies divided by the number of strategies generated . For instance , a student who generated one novice strategy and one expert strategy would receive a summary score of 3 ; this score was then divided by the total number of strategies gener - ated , which was 2 . The innovation variable was similarly derived : the sum of the quality of students’ questions was divided by the number of questions posed . The confidence variable was derived from student ratings , which were made on a 5 - point scale ( 1 = not at all confident , 5 = very confident ) . One - way repeated measures analysis of variance ( ANOVA ) were used to test for effects for time , year and time by year interactions . To enhance our understanding of relations among theoretically important dimensions of adaptive expertise , corre - lations among efficiency , innovation and confi - dence were calculated for each group at each time point . Descriptive statistics for all study vari - ables are summarized in Table 1 . Figure 3 is a graphic representation of results for efficiency and innovation by time and by age group . Consistent with our hypothesis , fourth - years generated more efficient solutions than did first - years ( main effect for year , F [ 1 , 64 ] = 59 . 96 , p < . 000 , eta 2 = . 49 ) . The number of strategies suggested by fourth - year students ranged from 0 – 3 on the pre - and post - tests . The number of strategies suggested by first - year students ranged from 0 – 2 on the pre - and post - tests . Fourth - year students also took a more innovative approach than did first - years and all students were more innovative on the post - test ( main effect for year , F [ 1 , 65 ] = 17 . 09 , p < . 000 , eta 2 = . 21 ; main effect for time , F [ 1 , 65 ] = 10 . 99 , p < . 000 , eta 2 = . 15 ) . The number of questions posed by fourth - year students ranged from 0 – 4 on the pre - test and 0 – 6 on the post - test . The number of questions posed by first - year students ranged from 0 – 2 on the pre - test and 0 – 3 on the post - test . Accuracy ratings for both groups ranged from 0 – 1 on the pre - and post - tests . As expected , fourth - year students were more confident and both groups’ confidence increased over time ( main effect for year , F [ 1 , 65 ] = 90 . 35 , p < . 000 , eta 2 = . 58 ; main effect for time , F [ 1 , 65 ] = 146 . 34 , p < . 000 , eta 2 = . 69 ) . We also found a small Table 1 . Descriptive statistics for student efficiency , innovation and confidence by year and by time Pre Post Efficiency Mean SD Mean SD First - year . 46 . 73 . 88 . 91 Fourth - year 1 . 86 . 94 1 . 94 . 95 InnovationFirst - year . 83 . 72 1 . 32 . 84 Fourth - year 1 . 56 . 80 1 . 80 . 57 ConfidenceFirst - year 1 . 46 . 88 4 . 53 1 . 71 Fourth - year 5 . 00 1 . 75 7 . 18 1 . 68 J . Walker et al . 648 time by year interaction ; first - years had larger gains than did seniors ( F [ 1 , 65 ] = 4 . 22 , p < . 05 , eta 2 = . 06 ) . Confidence ratings for fourth - years ranged from 1 – 4 on the pre - test and 2 – 5 on the post - test . For first - years , confidence ranged from 1 – 5 on the pre - and post - tests . To test our hypothesis that students’ approaches would reflect their educational experiences , we examined the proportion of novice , proficient and expert questions posed by each group at each time point . Fourth - year students asked more novice , proficient and expert - level questions at both time points ( main effects for year : Novice , F [ 1 , 65 ] = 12 . 20 , p < . 001 , eta 2 = . 16 ; Proficient , F [ 1 , 65 ] = 16 . 10 , p < . 00 , eta 2 = . 20 ; Expert , F [ 1 , 65 ] = 15 . 69 , p < . 000 , eta 2 = 19 ) . Consistent with their course experiences , first - year students made the largest gains in proficient - level questions ( main effect for time , F [ 1 , 65 ] = 13 . 61 , p < . 001 , eta 2 = . 17 ) . Both groups of students asked more expert questions on the post - test ( main effect for time , F [ 1 , 65 ] = 6 . 76 , p < . 01 , eta 2 = . 09 ) . Finally , we examined relations among efficiency , innovation and confidence . Table 2 summarizes these relations by year and by time point . For fourth - year students , we found consistent positive links between confidence and innovation , and a moderate correlation between efficiency and inno - vation on the pre - test . By contrast , first - year students’ confidence was related only to efficiency on the pre - test ; a correlation between efficiency and innovation on the post - test approached signif - icance . DISCUSSION This study was designed with two purposes : 1 ) to bridge research in design cognition and design education to the construct of adaptive expertise , and 2 ) to test the utility of one ‘education - friendly’ tool for comparing students’ approaches to real - istic design challenges . Specifically , we developed and used design scenarios to assess three dimen - sions of adaptive problem - solving among begin - ning and advanced engineering undergraduates . The three dimensions are efficiency ( i . e . generating accurate problem - solving strategies ) , innovation ( i . e . problem - scoping or elaborating on a problem statement ) and confidence . Consistent with other investigations of under - graduates’ reasoning about engineering design [ e . g . 7 – 9 ] , fourth - year students devised more efficient and innovative solutions than did first - years . Fourth - year students were also more confident in their problem - solving abilities . Over time , all students became more innovative and more confi - dent . As expected , much of the increase in innova - tion for beginning students appeared related to their course experience and greater understanding Fig . 3 . Efficiency and innovation by year and by time . Table 2 . Correlations among all study variables by year and by time First - years Fourth - years Pre - Test Confidence Efficiency Pre - Test Confidence Efficiency Efficiency . 66 * * Efficiency . 40 * Innovation . 24 . 11 Innovation . 39 * . 39 * Post - Test Post - Test Efficiency – . 29 Efficiency – . 01 Innovation – . 20 . 36 + Innovation . 36 * . 14 * = p < . 05 ; * * = p < . 01 ; + = p < . 06 . Design Scenarios as an Assessment of Adaptive Expertise 649 of cardiac signal analysis . That is , this group of students was able to generate more questions related to the phenomenon of cardiac signals . By contrast , increases in innovation for advanced students suggested a greater tendency to take a ‘breadth - first approach ; ’ on the post - test these students asked questions across a spectrum of issues including basic vocabulary , questions about cardiac phenomena , and awareness of broader design issues , including others’ approaches to similar problems . Advanced students were also more likely to question the problem statement as given . This is consistent with evidence that adaptive designers take the client’s wishes and demands as a starting - point rather than absolute law [ 1 ] . We were also interested in relations among the dimensions of efficiency , innovation and confi - dence . Given our definitions we might expect that the quality of students’ innovation ( i . e . elaboration on a problem statement ) would be positively related to their efficiency ( i . e . actions taken to solve the problem ) . This is because students who consider the problem from multiple vantage points gain a deeper understanding and , in turn , generate more effective problem - solving stra - tegies . Correlations between efficiency and innova - tion did not support this expectation . However , consistent with current thinking that tentative confidence underlies innovation [ 6 ] , positive links were found between advanced students’ innova - tion and confidence at both time points . No such links were found for beginning students . This study represents a promising avenue for assessing adaptive expertise within the context of students’ regular academic coursework . However , several outstanding questions remain . For instance , what is an appropriate level of confi - dence ? How do our students compare to experts ? We are currently addressing these issues by collect - ing responses to this scenario from a pool of design experts who do and do not have expertise in the area of cardiac phenomena . Grounded in research in the domain - specific and domain - generality of expertise [ 14 , 15 ] , this approach may enhance understanding of adaptive problem - solving by establishing ‘benchmarks’ when domain - specific knowledge is high and when it is not . Another concern relates to the fact that , while our measure taps the separable dimensions of efficiency and innovation , it does not necessarily allow students much ‘room’ on the efficiency dimension . For this reason we are currently devel - oping a set of design scenarios that prompt students to go beyond generating problem - solving strategies to actually enacting a solution . We are also interested in developing problem - solving resources to accompany the scenarios . Such resources may allow us to observe how students move between the dimensions of efficiency and innovation ( i . e . how they use their ability to define the problem to generate testable solutions and , in turn , how new questions emerge from testing their ideas ) . To learn more about the role of problem - specific knowledge we are currently replicating the study , assessing both groups’ know - ledge of cardiac signal analysis . Finally , our design challenge is realistic ; however , the way students complete it is not . In ‘the real world’ most design problems are taken on by teams rather than individuals . For this reason we also want to explore the quality of solutions derived by individuals and by teams . We also see opportunities for using the design scenarios as a teaching tool . For instance , students could complete the scenario in class ( as individuals or in groups ) and then compare their solutions to their peers and to the solution of an expert . Repeatedly engaging in such activities may increase students’ awareness of and ability to reason about design issues that transcend the specific content of a problem . In sum , our work suggests that design scenarios can reliably and sensitively measure dimensions of adaptive expertise in engineering design , which is an important contribution to research on adaptive expertise and to the field of engineering education . Acknowledgements —This work was supported by the Engineer - ing Research Center’s Program of the National Science Foun - dation ( Award # EEC9876363 ) . The authors extend many thanks to the participants who graciously gave their time . REFERENCES 1 . J . D . Bransford , A . L . Brown and R . R . Cocking ( eds . ) , How People Learn : Mind , Brain , Experience , and School , National Academy Press , Washington , DC ( 2000 ) . 2 . G . Hatano and K . Inagaki , Two courses of expertise , in H . Stevenson , J . Azuma and K . Hakuta ( eds . ) , Child Development and Education in Japan , W . H . Freeman & Co . , New York ( 1986 ) , pp . 262 – 272 . 3 . G . Hatano and Y . Oura , Commentary : Reconceptualizing school learning using insight from expertise research , Educational Researcher , 32 ( 8 ) ( 2003 ) , pp . 26 – 29 . 4 . F . F . Fisher and P . Peterson , A tool to measure adaptive expertise in biomedical engineering students , Proceedings of the 2001 American Society for Engineering Education Annual Conference , Albuquerque , NM ( 2001 ) . 5 . G . Hatano and J . G . Greeno , Commentary : Alternative perspectives on transfer and transfer studies , International Journal of Educational Research , 31 ( 7 ) ( 1999 ) , pp . 645 – 654 . 6 . D . L . Schwartz , J . D . Bransford and D . Sears , Innovation and efficiency in learning and transfer , in J . Mestre ( ed . ) , Transfer , Erlbaum , Mahwah , NJ ( 2005 ) . 7 . W . C . Newstetter and W . M . McCracken , Novice conceptions of design : Implications for the design of learning environments , in C . Eastman , M . McCracken and W . Newstetter ( eds . ) , Design Knowing and Learning : Cognition in Design Education , New York , Elsevier ( 2001 ) , pp . 63 – 78 . J . Walker et al . 650 8 . R . S . Adams , J . Turns and C . J . Atman , Educating effective engineering designers : The role of reflective practice , Design Studies , 24 ( 2003 ) , pp . 275 – 294 . 9 . C . J . Atman and J . Turns , Studying engineering design learning : Four verbal protocol studies , in C . Eastman , M . McCracken and W . Newstetter ( eds . ) , Design Knowing and Learning : Cognition in Design Education , New York , Elsevier ( 2001 ) , pp . 37 – 62 . 10 . L . J . Ball , J . S . Evans , I . Dennis and T . C . Ormerod , Problem - solving strategies and expertise in engineering design , Thinking and Reasoning , 3 ( 1997 ) , pp . 247 – 270 . 11 . L . M . Wills and J . L . Kolodner , Towards more creative case - based design systems . In AAAI - 94 , Seattle , WA ( 1994 ) . 12 . J . M . T . Walker , D . S . Cordray , P . H . King and R . C . Fries , Novice and expert definitions of biodesign : Developmental differences with implications for educators , International Journal of Engineering Education ( in press ) . 13 . A . Bandura , Self - efficacy : The Exercise of Control , W . H . Freeman and Company , New York ( 1997 ) . 14 . C . D . Schunn and J . R . Anderson , The generality / specificity of expertise in scientific reasoning , Cognitive Science , 23 ( 1999 ) , pp . 337 – 370 . 15 . S . Wineburg , Reading Abraham Lincoln : An expert / expert study in interpretation of historical texts , Cognitive Science , 22 ( 3 ) ( 1998 ) , pp . 319 – 346 . APPENDIX A . SCORING RUBRIC FOR SIGNAL ANALYSIS PROBLEM 1 . What do you need to do to test the doctor’s hypothesis ? . NoviceGather data from 2 sites Use multiple humans / patients . Expert Compare data from 2 sites to data obtained from 1 site to see if 1 site would have been sufficient Conduct non - human testing : animal or computer simulations 2 . At this time , what questions do you have for the doctor ? . NoviceQuestions about vocabulary ( e . g . what is SVT ? ) What are the sites that you plan to sample from ? . ProficientCanIsee a tracing of what the ECG looks like ? Will I be able to compare timing and voltages of ECG ? How is the root cause manifested in the physiology ? . ExpertWhatkind of literature is available on this subject ? What kind of team or resources is available ( equipment , money , personnel ) ? Why is this hypothesis the best one ? Why this solution ? Joan Walker is Assistant Professor of Education in the Department of Curriculum and Instruction at Long Island University . She received her M . Sc . ( 2000 ) and Ph . D . ( 2003 ) in Developmental Psychology from Vanderbilt University . Her research interests include the development of expertise , instructional design and assessment , and the role of learning environments in cognitive development . David Cordray is Professor of Public Policy and Psychology in the Department of Psychology and Human Development at Peabody College at Vanderbilt University . He received his Ph . D . ( 1979 ) from Claremont Graduate School . He has developed methodo - logical refinements of quasi - experimental designs and causal inquiry . Sean P . Brophy is the leader of the Learning Sciences Group in the VaNTH Engineering Research Center ( ERC ) ( www . vanth . org ) and an Assistant Professor in the Department of Engineering Education at Purdue University . He received a B . Sc . degree in Mechanical Engineering from the University of Michigan , an M . Sc . in Computer Science from DePaul University , and a Ph . D . in Education and Human Development from Vanderbilt Uni - versity . Dr . Brophy’s research interests relate to developing students’ problem - solving skills , formative assessment , and using simulations and models to facilitate student understanding of difficult engineering concepts . Paul H . King is an Associate Professor of Biomedical Engineering , Mechanical Engineering , and Anesthesiology at Vanderbilt University . He received his B . Sc . ( 1963 ) and M . Sc . ( 1965 ) in Engineering Science from the Case Institute of Technology , and a Ph . D . ( 1968 ) in Mechanical Engineering from Vanderbilt University . Design Scenarios as an Assessment of Adaptive Expertise 651