. Expertise vs . Bias in Evaluation : Evidence from the NIH ∗ Danielle Li Northwestern University † First draft : Aug 1 , 2011 This draft : April 17 , 2013 Abstract Experts may know more about the potential of projects in their area , but may also have preferences that impede their objectivity . This paper develops a framework for separately iden - tifying the eﬀects of expertise and bias in project evaluation and applies it in the context of peer review at the National Institutes of Health ( NIH ) . I ﬁnd that while reviewers favor applicants whose work is related to theirs , they are also more informed about the quality of those appli - cants . On net , the beneﬁts of expertise tend to dominate , indicating that policies designed to reduce the inﬂuence of personal preferences may also reduce the quality of funding decisions . ∗ I am very grateful to Pierre Azoulay , Michael Greenstone , and especially David Autor , for detailed feedback on this project . I also thank Jason Abaluck , Leila Agha , Josh Angrist , David Berger , David Chan , Esther Duﬂo , Brigham Frandsen , Richard Freeman , Bob Gibbons , Nathan Hendren , Ben Jones , Niko Matouschek , Xiao Yu May Wang , Ziad Obermeyer , Amanda Pallais , Chris Palmer , Michael Powell , Heidi Williams , and numerous seminar participants at Berkeley Haas , Boston University , Columbia GSB , Dartmouth , Harvard Business School , Harvard Kennedy School , the London School of Economics , MIT , the NBER Productivity Lunch , Northwestern University , and Purdue Krannert for helpful comments and suggestions . I am grateful to George Chacko , Raviv Murciano - Goroﬀ , Joshua Reyes , and James Vines for assistance with data . All errors are my own . † danielle - li @ kellogg . northwestern . edu 1 Introduction When decisions are complex and technical , it is natural to turn to experts for advice . This is true in a wide variety of settings : lawmakers , corporate boards , venture capital groups , and regulatory bodies , for instance , all seek input from industry insiders . But how much should we trust their advice ? While experts may have valuable insights about a project’s potential , they may also have preferences that compromise their objectivity . As a result , attempts to limit bias can come at the direct cost of reducing expertise . Understanding how experts shape investment decisions is particularly crucial in the innovative sector , where the payoﬀs to speciﬁc investments are notoriously uncertain ( Arrow , 1962 ) . Because ideas are so diﬃcult to assess and because their value may take years or even decades to be realized , there is both greater value placed on expertise and greater scope for obfuscation . 1 I develop a framework for separately identifying the eﬀect of bias on decision - making from that of subject matter expertise , and provide the ﬁrst empirical estimate of the eﬃciency tradeoﬀ between the two . I do so in a context that is extremely important for medical innovation : grant funding at the National Institutes of Health ( NIH ) . With an annual budget of 30 billion dollars , the NIH is the world’s largest funder of biomedical research , spending nearly half as much on basic and applied science as the entire US pharmaceutical industry combined . 2 NIH - sponsored research plays a role in the development of over half of all FDA approved drugs , including path - breaking treatments such as Gleevec , the ﬁrst drug therapy to selectively target cancerous cells , and Lipitor , one of the most prescribed drugs in America . 3 To receive funding for a project , individual scientists submit grant applications to the NIH , which are then evaluated by committees of other active scientists . Because the majority of NIH funds are allocated in this way , peer review is the key institution responsible for consolidating thousands of investigator - initiated submissions into a concrete , publicly funded research agenda . The success of this system , then , depends on the ability of reviewers to identify and fund the most promising ideas in their areas of speciality . Yet advice in this setting may be distorted by the very fact that reviewers have made such substantial investments—from pursuing graduate studies to establishing labs—toward acquiring their expertise . Reviewers may , for example , favor lower - quality applicants whose work is related to their own over higher - quality applicants whose work is not . In a guide aimed at scientists describing the NIH grant review process , one reviewer writes : “ If I’m sitting in an NIH study section , and I believe the real area of current interest in the ﬁeld is neurotoxicology [ his own speciality ] , I’m thinking if you’re not doing neurotoxicology , you’re not doing interesting science . ” 4 Alternatively , reviewers may be biased against applicants whose work is related if they perceive them as competitors . 1 See , for example , Aghion and Tirole ( 1994 ) and David , Mowery , and Steinmueller ( 1992 ) . 2 In 2006 , pharmaceutical companies spent close to 50 billion dollars on R & D . CBO “Research and Development in the Pharmaceuticals Industry” ( 2006 ) . 3 Over two - thirds of FDA priority review drugs cite NIH - funded research . See Sampat and Lichtenberg ( 2011 ) . 4 See http : / / www . clemson . edu / caah / research / images / What Do Grant Reviewers Really Want Anyway . pdf . 1 I formalize this intuition with a model of misaligned incentives with strategic communication derived from Crawford and Sobel ( 1982 ) and apply it in the context of grant review . In this model , there is a reviewer whose research may or may not be related to the applicant’s . A related reviewer has better information about the quality of an applicant’s proposal but is also “biased” in that she derives a personal payoﬀ from funding a related applicant , independent of that applicant’s quality . I then show that both a reviewer’s expertise and bias will aﬀect equilibrium funding decisions : speciﬁcally , reviewer bias aﬀects the level probability that a related applicant is funded , while reviewer expertise has a diﬀerential eﬀect for high - and low - quality applicants . The intuition behind this result is simple and motivates my empirical work : if intellectual relatedness improves the quality of peer review , then the eﬀect of being evaluated by a related reviewer should diﬀer for high - and low - quality applicants . In particular , high - quality applicants should beneﬁt from being evaluated by related reviewers who can more accurately observe their quality , while low - quality applicants should be hurt . If , on the other hand , reviewers simply promote ( or demote ) applicants whose work is related to theirs , then they should be systematically more ( or less ) likely to fund related applicants regardless of quality . Peer review at the NIH presents a rare opportunity to obtain empirical traction on these issues . To do so , I assemble a new , comprehensive dataset linking almost 100 , 000 NIH grant applications to the committees in which they were evaluated . I observe many characteristics of an application , including its ﬁnal score and the identity of the applicant , which allows me to construct demographic information and the applicant’s grant and publication histories . I also observe the identities of all the reviewers who attend each meeting and the capacity in which they serve . In order to separately identify the eﬀects of reviewer expertise and bias , I need two empirical ingredients : 1 ) a measure of the quality of all grant applications—this will be based on the number of publications and citations that I associate to an application ; and 2 ) a measure of relatedness between an applicant and her review committee that is not correlated with the quality of the applicant’s proposal—this will be based on how many reviewers have cited the applicant . Together , these two ingredients allow me to 1 ) estimate the causal eﬀect of being more related to a committee on an applicant’s likelihood of being funded ; 2 ) decompose this eﬀect into a portion that comes from expertise and one that comes from bias ; and 3 ) assess the eﬃciency consequences of potentially biased reviewers in terms of the quality of research that the NIH supports , under the assumption that policy - makers care about citation - based measures of grant quality . Immediately , however , one can see that this paper faces two serious challenges . First , how does one measure the quality of an application that is not funded ? Second , how does one create a citation - based measure of reviewer - applicant relationships that does not conﬂate relatedness itself with applicant or reviewer quality ? I will discuss how I address each of these challenges in turn . First , I am able to measure the quality of unfunded applications because applicants often publish the research outlined in a grant proposal even if the application itself goes unfunded . This is possible because the NIH grants I study require applicants to provide substantial preliminary results , so substantial , in fact , that researchers often submit applications based on nearly completed , 2 publishable research . To ﬁnd these related publications , I use a text - matching approach that links grant application titles with the titles and abstracts of semantically related publications by the same applicant ( see Section 5 for details ) . I also restrict my quality analysis to articles published soon enough after grant review to not be directly aﬀected by any grant funds . With these two restrictions ( that related publications be on the same topic as the grant , and published shortly after grant review ) , I then build my quality measure based on the full stream of future citations that accrue to these publications . This approach allows me to apply the same algorithm to identify publications related to both funded and unfunded applications in a way that standard publication - grant - acknowledgement data cannot . 5 Measuring the quality of funded and unfunded grants , however , does not ensure that I can consistently disentangle the eﬀects of expertise and bias . Imagine , for instance , that reviewers are biased in favor of prominent scientists and that this same bias also leads journal editors and other scientists to favor prominent scientists in terms of publications and citations . This phenomenon , known as the Matthew Eﬀect ( see Merton , 1986 and Azoulay , Stuart , and Wang , 2011 ) , means that I will tend to underestimate bias in grant review , even controlling for any direct eﬀect of funding , because this bias would look justiﬁed by the citation patterns of the scientiﬁc community at large . Thus , second , in order to address this and other similar concerns , I need to construct a measure of relatedness between applicants and review committees that is not correlated with an applicant’s quality or prominence . Formally , this is because phenomenon like the Matthew Eﬀect induce measurement error ( citations to a prominent scientist are less reﬂective of quality than citations to a less prominent scientist ) ; if I my measure of relatedness is correlated with quality , then measurement error in quality will bias my estimates of the eﬀect of relatedness . As such , the number of reviewers who cite an applicant cannot be directly used to measure relatedness because it is almost surely correlated with quality . Instead , I exploit the organizational structure of NIH review committees to generate a diﬀerent relatedness measure . Speciﬁcally , the committees I study consist of two types of members , “per - manent” and “temporary , ” who have similar qualiﬁcations as scientists but substantially diﬀerent levels of inﬂuence on the committee . 6 Instead of measuring “relatedness” in terms of the number of reviewers an applicant is cited by , I deﬁne relatedness in terms of the composition of reviewers who cite an applicant . That is , I compare two applicants reviewed in the same committee meeting who are cited by the same total number of committee members but by diﬀerent numbers of permanent reviewers . This speciﬁcation identiﬁes the eﬀect of being related to a more inﬂuential reviewer under the assumption that the quality of an applicant is not correlated with the composition of reviewers who cite her , conditional on the total number . I discuss this assumption in Section 5 . 3 and provide direct evidence for it in Figure 5 . I also show that my results are robust to an alternative identiﬁcation strategy that does not rely on the distinction between permanent and temporary members . Speciﬁcally , I estimate the 5 See Section 5 . 2 and Appendix C for discussion and robustness tests . 6 “Permanent” members are not actually permanent ; they serve four - year terms . See Section 5 . 3 for a discussion of permanent versus temporary reviewers . 3 eﬀect of being related to an addition reviewer ( either permanent or temporary ) , using applicant ﬁxed eﬀects to control for time - invariant unobserved quality . This speciﬁcation identiﬁes the eﬀect of being related to an additional reviewer under the assumption that the time - variant unobserved quality of an application is not correlated with the number of reviewers who cite her . My results are similar using both strategies . Section 5 . 3 discusses these speciﬁcations in more detail . My paper has three primary ﬁndings . First , I show that applicants whose work is exogenously related to more reviewers are more likely to be funded : holding total relatedness constant , every additional permanent member whose work is related to an applicant increases her chances of being funded by 3 . 1 percent , the equivalent of a one - quarter standard deviation increase in application quality . Second , I decompose the eﬀect of expertise and bias . I ﬁnd that while reviewers are biased in favor of applicants whose work is related to their own , they are also better able to identify high - quality research among these applicants ; the correlation between scores and funding outcomes is over 50 percent higher for applicants who work in the same area as at least one permanent member ( conditional on the same total number of reviewers with whom an applicant shares a research interest ) . Finally , on net , I show that the gains associated reviewer expertise tend to dominate the losses associated with bias . Treating applicants related to inﬂuential members as if they were unrelated—thereby reducing both bias and expertise—would reduce the quality of the NIH - supported research portfolio by two to three percent , as measured by future citations and publications . In addition to quantifying the role that bias and information play on average , I also document substantial and persistent variation in how well grant review committees perform . In particular , I show that some of this variation is attributable to diﬀerences in how well committees make use of reviewers . My empirical work is particularly relevant for innovation policy . A key debate in this litera - ture focuses on what mechanisms are most eﬀective for encouraging innovation : while patents may distort subsequent access to and use of new knowledge , a concern with research grants and other R & D subsidies , however , is that the public sector may make poor decisions about which projects to fund . Currently , there is little empirical evidence on how—and how successfully—governments make these research investments . 7 Diﬀerent organizations , moreover , allocate funds in diﬀerent ways ; NIH’s reliance on peer review of individual grants , for example , stands in contrast to major European funding agencies , which often support large groups of scientists working in predeter - mined priority areas . Understanding the strengths and weaknesses of these models is of particular importance because , by making investments in speciﬁc people , labs , and ideas , funding not only aﬀects near - term scientiﬁc output but may also shape the allocation of future research attention and resources . The tradeoﬀ between expertise and bias is also important in many empirical settings outside of innovation : social ties in the workplace may discipline employees or help them ﬁnd better jobs but may also lead to nepotism ; academics are more informed about the quality of their colleagues but 7 See Acemoglu , 2008 ; Kremer and Williams , 2010 ; Grilliches , 1992 ; and Cockburn and Henderson , 2000 for surveys . One recent exception is Hegde ( 2009 ) , which considers the political economy of NIH congressional appropriations . 4 may show bias when making promotion and editorial decisions . A large literature , including Bewley ( 1999 ) , Bayer , Ross , and Topa ( 2008 ) , and Bandiera , Barankay , and Rasul ( 2009 ) , examines the role that networks play in the labor market and a smaller literature studies the role of connections in academic promotion and publishing ( Bagues and Zinovyeva , 2012 and Brogaard , Engleberg , and Parsons , 2012 , respectively ) . A ﬁrst - order challenge in these literatures is that it is diﬃcult to attribute diﬀerences in treatment to bias or better information if one does not observe the quality of workers who are not hired or academics who are not published or promoted . This paper contributes by studying these issues in a new empirical context where these challenges can be better overcome . The remainder of this paper proceeds as follows . In the next section , I discuss the details of NIH grant review . I discuss my conceptual and statistical frameworks in Sections 3 and 4 , respectively . Section 5 explains how I construct my dataset and variables in order to identify the role of bias and information . Main results are presented in Section 6 . Section 7 discusses implications for eﬃciency , and the ﬁnal section concludes . 2 Institutional Context Each year , thousands of scientists travel to Bethesda , Maryland where they read approximately 20 , 000 grant applications and allocate over 20 billion dollars in federal grant funding . During this process , more than 80 percent of applicants are rejected even though , for the vast majority of biomedical researchers , winning and renewing NIH grants is crucial for becoming an independent investigator , maintaining a lab , earning tenure , and paying salaries ( Stephan , 2012 ; Jones , 2010 ) . The largest and most established of these grant mechanisms is the R01 , a project - based , re - newable research grant that constitutes half of all NIH grant spending and is the primary funding source for most academic biomedical labs in the United States . There are currently 27 , 000 out - standing awards , with 4 , 000 new projects approved each year . The average size of each award is 1 . 7 million dollars spread over three to ﬁve years . To apply for an R01 , the primary investigator submits an application which is then assigned to a review committee ( called a “study section” ) for scoring and to an Institute or Center ( IC ) for funding . The bulk of these applications are reviewed in one of about 180 “chartered” study sections , which are standing review committees organized around a particular theme , for instance “Cellular Signaling and Regulatory Systems” or “Clinical Neuroplasticity and Neurotransmitters . ” . 8 These committees meet three times a year in accordance with NIH’s funding cycles and , during each meeting , review between 40 to 80 applications . My analysis focuses on these committees . Study sections are typically composed of 15 to 30 “permanent” members who serve four - year terms and 10 to 20 “temporary” reviewers who are called in as needed . The division of committees into permanent and temporary members plays an important role in my identiﬁcation strategy , which I discuss in greater detail in Section 5 . 3 . Within a study section , an application is typically 8 The NIH restructured chartered study sections during my sample period and my data include observations from 250 distinct chartered study sections . These changes do not aﬀect my estimation because I use within meeting variation only . 5 assigned up to three reviewers , mostly permanent , who provide an initial assessment of its merit . The process of assigning applications to study sections and reviewers is non - random . In practice , applicants are usually aware of the identities of most permanent study section members , suggest which study section they would like to be reviewed by , and usually get their ﬁrst choice ( subject to the constraint that , for most applicants , there are only one or two study sections which are scientiﬁcally appropriate ) . Study section oﬃcers , meanwhile , assign applications to initial reviewers on the basis of intellectual ﬁt . I will discuss the implications of this non - random selection on my identiﬁcation strategy in Section 5 . 3 . Once an application has been assigned , initial reviewers read and score the application on the basis ﬁve review criteria : Signiﬁcance ( does the proposed research address an important problem and would it constitute an advance over current knowledge ? ) , Innovation ( are either the concepts , aims , or methods novel ? ) , Approach ( is the research feasible and well thought out ? ) , Investigator ( is the applicant well - qualiﬁed ? ) , and Environment ( can the applicant’s institution support the proposed work ? ) . Based on these scores , weak applications ( about one - third to one - half of all applicants ) are “triaged” or “unscored” meaning that they are rejected without further discussion . The remaining applications are discussed in the full study section meeting . During these discussions , an application’s initial reviewers ﬁrst present their opinions and then all reviewers discuss the application according to the same ﬁve review criteria listed above . Following these discussions , all study section members anonymously vote on the application , assigning it a “priority score , ” which , during the period my data come from , ranged from 1 . 0 for the best application to 5 . 0 for the worst , in increments of 0 . 1 . The ﬁnal score is the average of all member scores . This priority score is then converted into a percentile from 1 to 99 , where a percentile reﬂects the percentage of applications from the same study section and reviewed in the same year that received a better priority score . According to this system , a lower score is better but , for ease of exposition and intuition , I report inverted percentiles ( 100 - oﬃcial NIH percentile , e . g . the percent of applications that are worse ) , so that higher percentiles are better . In my data , I observe an application’s ﬁnal score ( records of scores by individual reviewers and initial scores are destroyed after the meeting ) . Once a study section has scored an application , the Institute to which it was assigned de - termines funding . Given the score , this determination is largely mechanical : an IC lines up all applications it is assigned and funds them in order of score until its budget has been exhausted . When doing this , the IC only considers the score : NIH will choose to fund one large grant instead of two or three smaller grants as long as the larger grant has a better score , even if it is only marginally better . The worst percentile score that is funded is known as that IC’s payline for the year . In very few cases ( less than four percent ) , applications are not funded in order of score ; this typically happens if new results emerge to strengthen the application . Scores are never made public . Funded applications may be renewed every three to ﬁve years , in which case they go through the same process described above . Unfunded applications may be resubmitted , during the period of my data , up to two more times . My analysis includes all applications that are reviewed in each 6 of my observed study section meetings , including ﬁrst time applications , resubmitted applications , and renewal applications . 3 How do Relationships Impact Funding Decisions ? Conceptual Framework The following model of decision - making deﬁnes what I mean by reviewer bias and by reviewer expertise . It then illustrates how the personal preferences and information of an individual reviewer can aﬀect grant allocation through strategic communication . The equilibrium of this model will then be used to motivate my empirical strategy , discussed in Section 4 . In this model , committees want to fund the best grant applications , but must rely on the recommendation of an inﬂuential reviewer who has personal preferences that are potentially diﬀerent from those of the committee . Grant applications have some true quality Q ∗ that is unobserved by the committee , but which can be observed with varying noise by the reviewer . Grant applicants are either related to this reviewer or not . If the applicant is related , then the reviewer sees a signal Q R = Q ∗ + ε R about the quality of the grant and if the applicant is not related , then the reviewer sees the signal Q UR = Q ∗ + ε UR . I assume that Var ( ε UR ) > Var ( ε R ) , meaning that a reviewer is more informed about the true quality of a related applicant’s proposal . A reviewer who is related to an applicant , however , may be biased : if the grant is funded , he receives a payoﬀ P R = Q ∗ + B . Without loss of generality , I assume that B > 0 . Neither the committee nor the unrelated reviewer are biased ; they receive payoﬀs of P C = Q ∗ and P UR = Q ∗ , respectively . If the grant goes unfunded , all parties receive a common outside option U . I assume that the committee acts as a single unit but , importantly , I do not make any assumptions about whether the committee can observe B or whether the committee can observe whether a reviewer is related . The timing works as follows : 1 . Nature draws true quality Q ∗ and the signals Q R and Q UR . 2 . The reviewer , knowing her posterior , makes a costless and unveriﬁable recommendation M ∈ M = { M 1 , . . . , M K } to the committee . 3 . The committee observes M and takes a decision D ∈ { 0 , 1 } of whether or not to fund the grant . 4 . True quality is revealed and the reviewer and committee both receive their payoﬀs . Proposition 3 . 1 The Perfect Bayesian equilibria of this game are given by : Case 1 : If the committee can observe whether a reviewer is related and if R = 0 , then all informative equilibria are payoﬀ - equivalent to a full - revelation equilibrium in which : 1 . The reviewer truthfully reports her posterior E ( Q ∗ | Q ∗ + ε UR ) . 2 . The committee funds the grant if E ( Q ∗ | Q ∗ + ε UR ) > U . 7 Case 2 : If the committee can observe whether a reviewer is related and if R = 1 then : For E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) > U , the unique informative equilibrium is partially - revealing : 1 . With probability one , the reviewer sends a signal Y if E ( Q ∗ | Q ∗ + ε R ) > U − B and N otherwise . 9 2 . The committee funds the grant if and only if it receives the signal Y . In all cases where an informative equilibrium exists , there also exist uninformative equilibria where the grant is never funded . For E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) < U , only uninformative equilibria exist and the grant is never funded . Case 3 : If the committee cannot observe whether a reviewer is related , then : For K [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε UR ) > U ) ] + ( 1 − K ) [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) ] > U , then the unique informative equilibrium is also partially - revealing : 1 . With probability one , either type of reviewer truthfully reports their Y / N preference for funding the application . 2 . The committee funds the grant if and only if it receives the signal Y . K here is the committee’s expectation that a reviewer is unrelated , conditional on observing a message of Y : K = p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) + ( 1 − p ) ( E ( Q ∗ | Q ∗ + ε R ) > U − B ) , where p is the actual probability that a reviewer is unrelated . Again , in all cases where an informative equilibrium exists , there also exist uninformative equilibria where the grant is never funded . For p [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε UR ) > U ) ] + ( 1 − p ) [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) ] < U , only uninfor - mative equilibria exist and the grant is never funded . Proof : See Appendix 3 . In the unique informative equilibria of this model , reviewers send messages according to their preferences but , as in Crawford and Sobel ( 1982 ) , information is distorted because the committee is unable to distinguish , in some cases , a situations when an application should be funded ( e . g . when E ( Q ∗ | Q ∗ + ε ) > U ) from one in which it should not ( e . g . when U > E ( Q ∗ | Q ∗ + ε ) > U − B ) . In order for an informative equilibrium to exist , committees must believe that enough information about the true quality of the grant is communicated in spite of the reviewer’s bias . The amount of information that a committee has about 1 ) whether a reviewer is related and 2 ) the size of her bias B aﬀects only the threshold for an informative equilibria to exist . If a committee is certain that a reviewer is unrelated , then an informative equilibrium can always exist . If the committee thinks there is some probability that a reviewer is biased , then an informative equilibria can only exist if the amount of bias that the committee expects is relatively small . Finally , if a 9 Although the type space of the messages is not restricted to be binary , all informative equilibria will be payoﬀ - equivalent to one in which they are . 8 committee is certain that a reviewer is biased , then it will be even harder to sustain an informative equilibrium . The form of the informative equilibrium , if it does exist , however , does not depend on what information the committee has on reviewer relatedness or bias . This is because , regardless of what the committee knows , its only decision is whether to follow the reviewer’s advice . The eﬀect of relatedness and bias on committee decisions , then , is driven completely by the reviewers’ messaging strategy , which depends only on reviewers knowing their own preferences . I will focus on the informative equilibrium , in cases when R = 0 and in cases when R = 1 ( it does not matter if the committee can distinguish ) . 10 In all these cases , the equilibrium message strategy is given by : M ( Q ) =   Y if E ( Q ∗ | Q ∗ + ε UR ) > U and R = 0 Y if E ( Q ∗ | Q ∗ + ε R ) > U − B and R = 1 N otherwise and the equilibrium decision strategy is given by : D ( M ) = (cid:40) Y if M = Y N otherwise The equilibrium decision rule can be more succinctly expressed as : D = I ( E ( Q ∗ | Q ∗ + ε UR ) > U ) (cid:124) (cid:123)(cid:122) (cid:125) baseline for unrelated + [ I ( U > E ( Q ∗ | Q ∗ + ε R ) > U − B ) ] (cid:124) (cid:123)(cid:122) (cid:125) bias for related ( + ) R ( 1 ) + [ I ( E ( Q ∗ | Q ∗ + ε R ) > U ) − I ( E ( Q ∗ | Q ∗ + ε UR ) > U ) ] (cid:124) (cid:123)(cid:122) (cid:125) additional information for related ( + / - ) R ( 2 ) In this model , committees listen to the advice of related reviewers even if they are biased because committees value expertise . Equation ( 2 ) shows that committees have some baseline per - formance that is captured by how well unrelated reviewers assess the quality of a grant . Advice from related reviewers can improve committee decisions because it increases the chances that a qualiﬁed related applicant is funded ahead of an unqualiﬁed related applicant . Related reviewers , however , can worsen committee performance by increasing the probability that an unqualiﬁed related appli - cant is funded ahead of a qualiﬁed unrelated applicant . The net eﬀect of related reviewers on the quality of decisions is thus ambiguous . Many popular critiques of NIH peer review assume that diﬀerences in funding likelihood among applicants with the same quality must be due to bias ( see Ginther et . al . , 2011 ) . Equation ( 2 ) shows , however , that this need not be the case . In particular , the diﬀerence in expected funding 10 If the equilibrium were not informative , then advice from related reviewers would not be taken ; I would ﬁnd no eﬀect of bias and perhaps a lower correlation between scores and quality for applications reviewed by related reviewers . My results are not consistent with a non - informative equilibrium . 9 likelihood between applicants with the same quality Q ∗ but diﬀerent relatedness R is given by : E [ D | Q ∗ , R = 1 ] − E [ D | Q ∗ , R = 0 ] = Pr ( U > E ( Q ∗ | Q ∗ + ε R ) > U − B ) + Pr ( E ( Q ∗ | Q ∗ + ε R ) > U ) − Pr ( E ( Q ∗ | Q ∗ + ε UR ) > U ) This expression will be non - zero if reviewers are biased ( B (cid:54) = 0 ) . Yet , even without bias , this term need not be zero . The intuition is simple : a committee may fund a related applicant ahead of an equally qualiﬁed unrelated applicant simply because they know the related applicant and know that she is high - quality ; they may not know that the unrelated applicant is just as qualiﬁed . Here , the diﬀerence in funding likelihood between similar applicants is attributable to diﬀerences in information , not to bias . As such , distinguishing between these explanations is important because they have diﬀerent implications for whether relatedness enhances the quality of peer review . 4 How do Relationships Impact Funding Decisions ? Statistical Framework Next , I assume that the committee decisions I observe are generated by the equilibrium decision rule described by Equation ( 2 ) in Section 3 . Under the assumption that ε is uniform ( ε UR ∼ U [ − a UR , a UR ] , ε R ∼ U [ − a R , a R ] ) and that E ( Q ∗ | Q ∗ + ε ) is approximated by λ ( Q ∗ + ε ) for any constant λ , the conditional mean of D is given by : 11 E [ D | Q ∗ , R ] = Pr ( λ UR ( Q ∗ + ε UR ) > U ) + Pr ( U > λ R ( Q ∗ + ε R ) > U − B ) R ( 3 ) + [ Pr ( λ R ( Q ∗ + ε R ) > U ) − Pr ( λ UR ( Q ∗ + ε UR ) > U ) ] R ( 4 ) = 1 2 a UR [ a UR − U / λ UR + Q ∗ ] + B 2 a R λ R R + (cid:18) 1 2 a R [ a R − U / λ R + Q ∗ ] − 1 2 a UR [ a UR − U / λ UR + Q ∗ ] (cid:19) R = 1 2 + 1 2 a UR (cid:124) (cid:123)(cid:122) (cid:125) Quality corr . Q ∗ + B 2 a R λ R (cid:124) (cid:123)(cid:122) (cid:125) Bias term R + (cid:20) 1 2 a R − 1 2 a UR (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) Add . corr . for related RQ ∗ − U 2 a UR λ UR + (cid:20) 1 2 a UR λ UR − 1 2 a R λ R (cid:21) RU ( 5 ) Equation ( 5 ) says that the eﬀect of relatedness arising from expertise and bias can be sepa - rately identiﬁed . First , the coeﬃcient on R tests for bias : it is non - zero if and only if B (cid:54) = 0 . Second , the coeﬃcient on RQ tests for expertise . To see this , notice that the coeﬃcient on Q ∗ captures how well the committee identiﬁes high - quality research among unrelated applicants . Speciﬁcally , a high coeﬃcient on Q ∗ means committees are more likely to fund high - quality unrelated applicants over low - quality unrelated applicants . The coeﬃcient on RQ ∗ , meanwhile , captures the additional 11 See the Appendix C for discussion and empirical tests related to these assumptions . 10 correlation between quality and likelihood of funding for related applicants . A high coeﬃcient on RQ means that a committee is more sensitive to increases in the quality of related applicants than it is to increases in the quality of unrelated applicants . This eﬀect of information is larger when the diﬀerence between the precisions of related and unrelated beliefs , 12 a R − 1 2 a UR is greater . The intuition for separately identifying expertise and bias is simple : if reviewers make diﬀerent funding decisions for related applicants because they have better information , then the eﬀect of relatedness on funding likelihood should diﬀer for high - and low - quality applicants , with the former group beneﬁting and the latter being harmed . If committees were inﬂuenced by the bias of related reviewers , then related applicants should be more likely to be funded regardless of quality . Finally , the terms U and RU control for funding selectivity ; for high cutoﬀs U , the correlation between funding and quality will be low—even in the absence of bias or diﬀerential information— because the marginal unfunded grant is already very high quality . In the model , there is no limit to the number of grants that are funded so that relationships can also aﬀect the generosity of committees . The RU term ensures that relationships are not credited for changing the correlation between funding and quality simply by lowering the threshold at which grants are funded . The exact form of Equation ( 5 ) depends on linearity assumptions but , in practice , my results are robust to allowing for non - linear eﬀects of relatedness and quality measures . These results are discussed in Appendix C and Appendix Table E . Equation ( 5 ) has a somewhat surprising feature : it says that , as long as Q ∗ is perfectly observed , I do not need exogenous variation in relatedness to identify the presence of bias . This is because exogenous variation in relationships matters only if application quality is an omitted variable . If , however , quality is observed , then exogenous variation in relatedness would not be necessary because I would be able to directly control for quality . In practice , however , I do not observe a grant’s true quality Q ∗ . Instead , I observe a noisy signal Q = Q ∗ + v . Thus , while the model suggests the following equation : S = α 0 + α 1 Q ∗ + α 2 R + α 3 RQ ∗ + α 4 U + α 5 RU + Xβ + ε ( 6 ) I can only estimate : S = a 0 + a 1 Q + a 2 R + a 3 RQ + a 4 U + a 5 RU + Xb + e . ( 7 ) where , in both equations , X includes other relevant variables that I can condition on . Proposition 4 . 1 Given observed quality Q = Q ∗ + v , the bias parameter α 2 in Equation ( 6 ) is consistently estimated by a 2 in Equation ( 7 ) as long as the following conditions are met : 1 . Cov ( R , Q ∗ | U , RU , X ) = 0 and Cov ( R 2 , Q ∗ | U , RU , X ) = 0 2 . E ( v | U , RU , X ) = 0 3 . Cov ( v , R | U , RU , X ) = 0 11 Proof : See Appendix B . Proposition 4 . 1 states my identifying conditions . Condition 1 requires that my measure of relatedness be uncorrelated , conditional on observables , with true application quality . If this were not the case , any mismeasurement in true quality Q ∗ would bias estimates of α 2 through the correlation between Q ∗ and my relatedness measure R . Thus , in my study , exogenous variation in relatedness is required only to deal with measurement error . Condition 2 requires that measurement error be conditionally mean zero . This means that , after conditioning on observable traits of the application or applicant , my quality measure cannot be systematically diﬀerent from what committees themselves are trying to maximize . Otherwise , I may conclude that committees are biased when they are actually prioritizing something I do not observe , but which is not mean zero diﬀerent from my quality measure . Finally , Condition 3 requires that the extent of measurement error not depend , conditional on observables , on whether an applicant is related to a reviewer . This may not be satisﬁed if related applicants are more likely to be funded and funding itself aﬀects my measure of quality . Suppose , for instance , that two scientists apply for a grant using proposals that are of the same quality . One scientist is related to a reviewer and is funded because of bias . The funding , however , allows her to publish more articles , meaning that my measure of quality—future citations—may mistakenly conclude that her proposal was better than the other scientist’s to begin with . Mismeasurement of ex ante grant quality makes it less likely that I would ﬁnd an eﬀect of bias . Another reason why Condition 3 may not be satisﬁed is given by the Matthew Eﬀect , a sociological phenomenon wherein credit and citations accrue to established investigators simply because they are established . Were this the case , more related applicants would receive more citations regardless of the true quality of their work , meaning that measurement error v would be correlated with relatedness . This would lead me to underestimate bias ; related applicants may get higher scores simply for being established , but this bias would look justiﬁed by a citation - based measure of quality ( which reﬂects bias in the scientiﬁc community at large ) . Together , Conditions 1 - 3 are weaker than assuming classical measurement error , but they place restrictions on how I can measure quality and relatedness . In particular , to satisfy these conditions , I construct quality and relatedness measures to meet the following standards : 1 . Quality Q = Q ∗ + v must be consistently measured for funded and unfunded grants and not be directly aﬀected aﬀected by whether an applicant receives funding . As described above , failure to measure quality properly may lead to violations of Conditions 2 and 3 . 2 . My measure of relatedness must be independent of quality , conditional on variables I can observe . This is simply Condition 1 . In the next section , I describe my data , explain how I construct my quality and relatedness measures , and provide evidence that these measures satisfy the identifying conditions in this section . 12 5 Data and Empirical Strategy 5 . 1 Data I have constructed a new dataset describing grant applications , review committee members , and their relationships for almost 100 , 000 applications evaluated in more than 2 , 000 meetings of 250 chartered study sections . My analytic ﬁle combines data from three sources : NIH administrative data for the universe of R01 grant applications , attendance rosters for NIH peer review meetings , and publication databases for life sciences research . Figure 1 summarizes how these data sources ﬁt together and how my variables are constructed from them . I begin with two primary sources : the NIH IMPAC II database , which contains administrative data on grant applications and a series of study section attendance rosters obtained from NIH’s main peer review body , the Center for Scientiﬁc Review . The application ﬁle contains information on an applicant’s full name and degrees , the title of the grant project , the study section meeting to which it was assigned for evaluation , the score given by the study section , and the funding status of the application . The attendance roster lists the full names of all reviewers who were present at a study section meeting and whether a reviewer served as a temporary member or as a permanent member . These two ﬁles can be linked using meeting - level identiﬁers available for each grant application . Thus , for my sample grant applicants , I observe the identity of the grant applicant , the identity of all committee members , and the action undertaken by the committee . Next , I construct detailed measures of applicant demographics , grant history , and prior publi - cations . Using an applicant’s ﬁrst and last name , I construct probabilistic measures of gender and ethnicity ( Hispanic , East Asian , or South Asian ) . 12 I also search my database of grant applications to build a record of an applicant’s grant history as measured by how many new and renewal grants an applicant has applied for in the past , and how many he has received . This includes data on all NIH grant mechanisms , including non - R01 grants such as post - doctoral fellowships and career training grants . To get measures of an applicant’s publication history , I use data from Thomson - Reuters Web of Science ( WoS ) and the National Library of Medicine’s PubMed database . From these , I construct information on the number of research articles that an applicant has published in the ﬁve years prior to submitting her application , her role in those publications ( in the life sciences , this is discernable from author position ) , and the impact of those publications as measured by ci - tations . In addition to observing total citations , I can also identify a publication as “high - impact” by comparing the number of citations it receives with the number of citations received by other life science articles that were published in the same year . My ﬁnal sample consists of 93 , 558 R01 applications from 36 , 785 distinct investigators over the period 1992 - 2005 . Of these applications , approximately 25 percent are funded and 20 percent are from new investigators , those who have not received an R01 in the past . This sample is derived from the set of grant applications that I can successfully match to meetings of study sections for which I 12 For more details on this approach , see Kerr ( 2008 ) . Because Black or African American names are typically more diﬃcult to distinguish , I do not include a separate control for this group . 13 have attendance records , which is about half of all R01 grants reviewed in chartered study sections . Table 1 shows that my sample appears to be comparable to the universe of R01 applications that are evaluated in chartered study sections . So far , I have discussed how I measure the prior qualiﬁcations of an applicant . As Conditions 1 - 3 of Section 4 indicate , however , I also need a direct measure of grant quality that is not directly aﬀected by funding and a measure of relatedness that is conditionally independent of my quality measure . I discuss each of these requirements in turn . 5 . 2 Measuring Quality A major strength of this project lies in my ability to go beyond using past applicant character - istics to assess application quality . Instead , I can construct a direct measure of application quality by examining the publications and citations it produces in the future . Condition 2 of my identifying conditions requires that this measure of grant quality not be systematically ( e . g . non mean zero ) diﬀerent from what committee members are looking for , after conditioning on observables . While there is no way to formally test this assumption ( as the objective function of the committee is unobserved ) , I address this concern in two ways . First , I attempt to construct quality measures that are informative and ﬂexible . While im - perfect , future citations are nonetheless a standard and useful measure of quality that can capture diﬀerent aspects of grant quality . Total citations reﬂect how well - regarded a project is on average . Policymakers , however , may care mostly about whether a publication is truly pathbreaking . In this case , I can measure whether a publication is a “hit” based on where it falls in the distribution of citations for all other publications in its cohort ( same ﬁeld , same year ) ; that is , a hit publication can be deﬁned as one which is cited at the 99th , 95th , etc . , percentile of similar publications . Further , because my sample begins in 1992 and my citation data go through 2008 , I can capture a fairly long run view of quality for almost all publications associated with my sample grants ( citations for life science articles typically peak one to two years after publication ) . This allows me to observe whether a project becomes important in the long run , even if it is not initially highly cited . If reviewers are using their expertise to maximize a welfare function based on long run impact or the number of hit publications , then my quality measure would capture this . Second , my analysis will include very detailed controls for many applicant or application characteristics—probabilistic gender and ethnicity , education , institutional aﬃliation , past publi - cation characteristics— including some that reviewers themselves cannot observe like grant appli - cation history and the number of citations that an applicant’s past publications eventually accrue after the date of grant review . This allows my framework to identify bias even if , for instance , committees take diversity preferences into account when assessing quality . Finally , even if Condition 2 is violated , my estimate of the welfare implications of seeking advice from related reviewers will still be consistent with respect to the number of citations and top hit publications produced by the NIH ( see Section 7 ) . This in itself is a metric of decision - making quality that is relevant for policy . 14 The primary challenge of constructing a measure of application quality is ﬁnding a way to do this for unfunded applications . I address this challenge by ﬁnding publications that are associated with the research described in the preliminary results section of an application . This is possible because the grants I study , the R01 , are intended for projects that have demonstrated a substantial likelihood of success , meaning that applicants describe publishable research in their preliminary results . In fact , the bar for preliminary results is so high that the NIH provides a separate grant mechanism , the R21 , for pursuing exploratory research leading to an R01 application . To ﬁnd these related publications , I look for articles published by a grant’s primary investigator around the time of grant review . Because my method of constructing quality needs to be consistent for funded and unfunded grants ( see Section 4 ) , I cannot use data on grant acknowledgements because they are , naturally , only available for funded grants . Instead , I compare the titles and abstracts of an applicant’s publications with the title of her grant proposal to determine which publications are related . For instance , if I see a grant application titled “Traumatic brain injury and marrow stromal cells” reviewed in 2001 and an article by the same investigator entitled “Treatment of traumatic brain injury in female rats with intravenous administration of bone marrow stromal cells , ” published within one year of this grant application , I conclude that this publication and its future citations can be used as a measure of the quality of the application . Text - matching ensures that I can measure quality using the same procedure for all grant applications . These publications ( and the citations that accrue to them ) for the basis of my quality measure . The identifying conditions in Section 4 also require that my quality measure not be directly aﬀected by funding . 13 This may occur in two ways : funding can be used to subsidize research on a diﬀerent topic from the original proposal or it can be used to extend research on the same topic . Text - matching limits the set of publications I use to infer application quality to those that are on the same topic as the grant . This reduces the possibility that my measure of application quality is contaminated by unrelated research that the grant is used to subsidize . Funding , however , may also increase the number of publications on the same topic as the grant . To address this concern , I also restrict my quality calculations to articles published in a short time window surrounding grant review . These articles are likely to be based on research that was already completed or underway at the time the grant application was written . To compute the appropriate window , I consider funding , publication , and research lags . A grant application is typically reviewed four months after it is formally submitted and , on average , another four to six months elapse before it is oﬃcially funded . 14 In addition to this funding lag , publication lags in the life sciences ( the time between ﬁrst submission and publication ) typically range from three months to over a year . 13 Grant funding , for instance , can be used to start new experiments related to the proposed project or to subsidize research on unrelated projects . Existing evidence on the eﬀect of grant funding on research outcomes suggests that this eﬀect is likely to be small ; using a regression - discontinuity approach , Jacob and Lefgren ( 2011 ) ﬁnd that receiving an R01 increases the number of articles a PI publishes in the next ﬁve years by 0 . 85 , from a mean of 14 . 5 . This ﬁgure includes all publications by a PI , including ones that may be on a diﬀerent topic from the original application . Jacob and Lefgren’s analysis , however , only documents the eﬀect of grant receipt for marginal applicants . The eﬀect of funding on future publications and citations could be larger elsewhere in the distribution and I take additional precautions to create a measure of quality not aﬀected by funding . 14 See http : / / grants . nih . gov / grants / grants process . htm . 15 It is thus highly unlikely that articles published up to one year after grant review would have been directly supported by that grant . Instead , the research underlying these articles are likely what would have been proposed and discussed in the preliminary results section of the grant application . Thus , my measure of an application’s quality examines the total number of citations that accrue to publications 1 ) on the same topic as the grant proposal and which are 2 ) published within one year of grant review . I also demonstrate that my results are robust to the choice of window . 5 . 2 . 1 Assessing Validity of Quality Measures Figure 2 demonstrates that my matching strategy can identify publications related to unfunded grant applications . In fact , using the measure of quality described above , I ﬁnd that funded and unfunded grants are almost equally represented among the subset of grant applications that generate many citations . Figure 2 also shows , however , that unfunded grants are more likely to produce few citations . There are two possible explanations for this ﬁnding : 1 ) unfunded applications are of lower quality , and should thus be expected to produce fewer citations ; or 2 ) funding directly improves research output , meaning that I fail to measure quality consistently for funded and unfunded grants . I distinguish between these explanations by using variation in whether grants with the same score are funded . Because budgets vary across ICs , applications from the same meeting with the same score are sometimes funded and sometimes not . If funding has a direct impact on my measure of quality , then I should mistakenly attribute higher quality to funded applications than to unfunded ones with the same score . Figure 3 shows this is not the case . Each dot represents the mean number of citations associated with funded applications that receive a particular score , regression adjusted to account for diﬀerences across meetings ; the crosses represent the same for unfunded applications . The dots do not lie systematically above the crosses , meaning that measured quality for funded grants does not appear to be systematically higher than for unfunded grants with the same score . The accompanying statistical test is reported in Table 2 . I compare measured quality for funded and unfunded grant applications with similar scores from the same meeting . Funding status can vary because pay lines at diﬀerent ICs diﬀer within the same year . Columns 1 and 2 show that , among the set of scored applications , funded grants tend to be higher quality , but this eﬀect goes away once I control for a smooth function of scores . Columns 3 and 4 expand the sample to the full set of applications , with low scores imputed for applications that were considered too low quality to be scored , and ﬁnd the same results . Together with Figure 3 , this ﬁnding mitigates concerns that my measure of quality is directly aﬀected by funding . Appendix C discusses several robustness tests for my measure of quality . It is possible , for instance , that not receiving a grant may slow down a scientist’s research ( if , for instance , they need to spend time applying for more grants ) . If this is the case , then a grant can directly impact the research quality of funded vs . non - funded applicants even before any funding dollars are disbursed . To address this concern , I estimate an alternative speciﬁcation focusing on publications on the same topic that were published one year prior to the grant funding decision ; these articles are likely to inform the grant proposal but their quality cannot be aﬀected by the actual funding decision . In 16 general , my results are robust to other windows ; this is unsurprising because I will show , in the next section , that relatedness to permanent reviewers ( conditional on relatedness to total reviewers ) is uncorrelated with applicant quality . Another potential concern with my quality measure is that it does not include later publica - tions , potentially on diﬀerent topics , that a review committee could anticipate would be supported by the grant . It is common for grant funding to subsidize research on future projects that may not be closely related to the original grant proposal ; even though reviewers are instructed to restrict their judgements to the merits of the research proposed in the grant application , is is possible that they may attempt to infer the quality of an applicant’s future research pipeline , and that related reviewers might have more information about this . Appendix C addresses this concern . To test whether my results are robust to this possibility , I use data on grant acknowledgements to match grants to all subsequent publications , not just to the research that is on the same topic or which is published within a year of grant review . Because grant acknowledgment data exist only for funded grants , this speciﬁcation can only examine whether relatedness impacts the scores that funded applicants receive . I show that results using data on grant acknowledgments are largely similar . Appendix C also reports another test of the validity of my quality measure . If my results are driven by changes in measured grant quality near the payline , then I should ﬁnd no eﬀect of relatedness on scores for in the subset of applications that are either well above or well below the payline . However , in both of these subsamples , I ﬁnd evidence that being related to a permanent member increases scores and increases the correlation between scores and quality . Because relat - edness cannot aﬀect actual funding status in these subsamples , the eﬀect I ﬁnd cannot be driven by diﬀerences in how well quality is measured . Finally , it is also worth emphasizing that , as discussed in Section 4 , overcrediting funded applications relative to unfunded applications would lead me to underestimate the extent of bias . 5 . 3 Measuring Relatedness Next , I construct a measure of applicant - committee relatedness that is uncorrelated with the quality of an application . This is done by ﬁrst determining using citation histories available in Web of Science to determine whether an applicant’s work is related to every individual reviewer present at the study section meeting . Speciﬁcally , I deﬁne an applicant’s work to be related to a reviewer if the reviewer has cited the applicant in the ﬁve years prior to the review meeting . Using citations as a measure of relatedness has several beneﬁts . First , citations capture a form of relatedness which , as demonstrated by the quote in the Introduction , may strongly inﬂuence a reviewer’s personal preferences : reviewers may prefer work that they ﬁnd useful for their own research . Second , citations can capture this form of intellectual connection more ﬁnely than other measures such as ﬁeld overlap . Because the review committees I study are highly focused , most reviewers in a committee are drawn from the same academic departments ( molecular biology , surgery , etc . ) so that many other observable measures of intellectual aﬃliation tend not to generate much variation in relatedness . Third , using data on whether the reviewer cites the 17 applicant ( as opposed to the applicant citing the reviewer ) reduces concerns that my measures of relatedness can be strategically manipulated by applicants . One may also consider more social measures of relatedness such as coauthorship or being from the same institution . These relationships , however , are often subject to NIH’s conﬂict of interest rules ; reviewers who are coauthors , advisors , advisees , or colleagues , etc . are prohibited from participating the either deliberations or voting . Intellectual relatedness is a form of relatedness that likely matters for grant review but which is not governed by conﬂict of interest rules . Table 3 describes reviewer - applicant citation patterns in my sample study sections . In total , I observe 18 , 916 unique reviewers . On average , 30 reviewers attend each meeting , 17 of whom are permanent and 13 of whom are temporary . The average applicant has been cited by two reviewers , one temporary and one permanent . The average permanent and average temporary reviewer both cite four applicants . The number of reviewers who have cited an applicant is likely to be correlated with applicant quality ; better applicants may be more likely to be cited by reviewers and may , independently , sub - mit higher quality proposals . Using this as a measure of relatedness , then , would violate Condition 1 of Section 4 . I instead exploit the structure of chartered NIH study sections to ﬁnd exogenous variation in reviewer - applicant relatedness . As discussed in Section 2 , review committees consist of “permanent” and “temporary” members . My identiﬁcation strategy examines how the number of permanent members who cite an applicant , call this R P , aﬀects the committee decision , conditional on the total number of reviewers who cite the applicant , R . That is , I compare the outcomes of scientists whose applications are reviewed in the same meeting , who have similar past performance , and who , while related to the same total number of reviewers , diﬀer in the number of inﬂuential members they are related to . In order for this strategy to be valid , I need to show that 1 ) permanent reviewers are indeed more inﬂuential within a study section but that 2 ) permanent and temporary reviewers are otherwise identical , meaning that being related to a permanent or temporary reviewer is uncorrelated with an applicant’s quality . 5 . 3 . 1 Assessing Validity of Relatedness Measures There are many reasons why permanent reviewers have more inﬂuence over an applicant’s score . Most basically , these reviewers do more work . As discussed in Section 2 , reviewers are re - sponsible for providing initial assessments of a grant application before that application is discussed by the full committee . These initial assessments are extremely important for determining a grant application’s ﬁnal score because they 1 ) determine whether a grant application even merits discus - sion by the full group and 2 ) serve as the starting point for discussion . In many study sections , there is also a rule that no one can vote for scores outside of the boundaries set by the initial scores without providing a reason . While I do not have data on who serves as one of an application’s three initial reviewers , permanent reviewers are much more likely to serve as an initial reviewer ; they are typically assigned eight to ten applications , compared with only one or two for temporary reviewers . In addition , permanent members are required to be in attendance for discussions of all 18 applications ; in contrast , temporary members are only expected to be present when their assigned grants are discussed , meaning that they often miss voting on other applications . Finally , permanent members work together in many meetings over the course of their four year terms ; they may thus be more likely to trust , or at least clearly assess , each other’s advice relative to the advice of a temporary member with whom they are not familiar . To test whether permanent members seem to have more inﬂuence , I use the fact that I observe almost 5 , 000 unique reviewers in meetings in which they are permanent and in meetings in which they are temporary . For each of these reviewers , I ﬁnd the set of applicants with whom they are related and show that a larger proportion of those applicants are funded when the reviewer is permanent rather than temporary . These regressions include controls for applicant characteristics and reviewer ﬁxed eﬀects , meaning that similarly qualiﬁed applicants related to the same reviewer are more likely to be funded when that reviewer is permanent than when the reviewer is temporary . These results are presented in Appendix C as well . In addition to providing evidence that permanent members are more inﬂuential , I also need to demonstrate that permanent members and temporary members are comparable as scientists : if this were the case , then being related to a permanent member instead of a temporary member ( conditional on total relatedness ) should not be indicative of quality . Figure 4 and Table 4 compare the observable characteristics of permanent and temporary members and show that they have similar publication histories and demographics . Figure 4 , in particular , shows that the distribution of quality , as measured by previous publications and citations , is essentially identical for permanent and temporary reviewers . The bottom panel of Table 4 suggests why this may not be surprising : permanent and temporary reviewers are often the same people ; 35 percent of permanent reviewers in a given meeting will be temporary reviewers in a future meeting in the future and 40 percent of temporary reviewers in will be permanent reviewers in the future . Even if permanent and temporary members are identical as scientists , though , there may still be concerns arising from the fact that reviewers are not randomly assigned to applications . This selection is non - random in two ways . First , membership rosters listing the permanent ( but not temporary ) members associated with a study section are publicly available , meaning that grant applicants know who some of their potential reviewers may be at the time they submit their application . The scope for strategic submissions in the life sciences , however , is small : for most grant applicants , there are only one or two appropriate study sections and , because winning grants is crucial for maintaining one’s lab and salary , applicants do not have the luxury of waiting for a more receptive set of reviewers . Second , once an application has been received , the study section administrator assigns it to initial reviewers on the basis of 1 ) intellectual match and 2 ) reviewer availability . If , for instance , not enough permanent reviewers are qualiﬁed to evaluate a grant application , then the study section administrator may call in a temporary reviewer . Temporary reviewers may also be called if the permanent members qualiﬁed to review the application have already been assigned too many other applications to review . This process may raise concerns for my identiﬁcation . For example , suppose that two ap - 19 plicants , one better known and higher quality , submit their applications to a study section that initially consists of one permanent reviewer . The permanent reviewer is more likely to be aware of the work of the better known applicant and thus there would be no need to call on a related temporary member . To ﬁnd reviewers for the lesser known applicant , however , the administrator calls on a temporary reviewer . Both applicants would then be related to one reviewer in total but , in this example , the fact that one applicant is related to a temporary member is actually correlated with potentially unobserved aspects of his quality . This would be a violation of Condition 1 in Section 5 . 3 , which says that relatedness to permanent members , conditional on total relatedness , should not be correlated with quality . I deal with this and other similar concerns in two ways . First , I provide direct evidence that the characteristics of the applicants and the quality of the applications does not appear to be systematically related to whether an applicant is related to a permanent or temporary member , conditional on total relatedness . Table 5 describes the demographic and past performance charac - teristics of grant applicants , divided by the total number of reviewers they are related to and by the composition of those reviewers . Most notably , applicants who are related to more reviewers in total tend to be more established : they are less likely to be new investigators , and have a more past publications and citations . Conditional on total related reviewers , however , there appear to be few diﬀerences among applicants : applicants related to one permanent reviewer are virtually identical as those related to one temporary reviewer . Among applicants related to two reviewers , those related to two permanent or one of each look identical . Those related to two temporary reviewers appear to have slightly fewer past publications , consistent with the concern raised above , but the diﬀerence is less than ﬁve percent of a standard deviation . Approximately 75 percent of my sample fall into the categories reported in Table 5 , but these ﬁgures are similar for applicants related to three or more reviewers . Figure 5 provides more evidence for my ﬁrst identifying condition , that the number of per - manent members an applicant is related to is not correlated with her quality , conditional on total relatedness . Instead of comparing applicant - level characteristics , however , Figure 5 directly exam - ines the quality of the submitted application itself . The upper lefthand panel shows the distribution of my measure of application quality for applicants related to exactly one reviewer . The solid line shows the distribution of quality among applicants related to one permanent member ; the dotted line , for those related to one temporary member . These distributions are essentially identical . Sim - ilarly , the upper right hand panel shows the same , but with quality measured using the number of publications associated to a grant . The bottom two panels of Figure 5 repeat this exercise for applicants who are related to a total of two reviewers . In this case , there are now three possibilities : the applicant is related to two temporary or two permanent , or one of each . In all of these cases , the distribution of applicant quality is again essentially identical . Because applicant characteristics are not correlated with the composition of related reviewers , examining the eﬀect of relatedness to permanent members addresses concerns about the Matthew Eﬀect . Because my identiﬁcation holds scientiﬁc esteem as measured by total relationships constant , 20 there is no reason to believe that applicants related to permanent members would be more or less likely to be cited than applicants related to temporary members . Second , another way to address concerns about the assignment of temporary and permanent members is to show that my results are robust to an alternative speciﬁcation that does not rely on this distinction . In my main speciﬁcations , I control for the total number of related reviewers in order to restrict my comparisons to applicants of similar quality . This approach controls for both time - varying and time - invariant unobserved quality of applicants under the assumption that the unobserved quality of an application is not correlated with the composition of permanent and temporary reviewers an applicant is related to . Another approach is to simply control for applicant ﬁxed eﬀects . In this speciﬁcation , I compare the funding outcomes of applications from the same applicant across meetings in which the applicant is related diﬀerent total numbers of reviewers . The downside of this approach is that applicant ﬁxed eﬀects only control for time - invariant unobserved quality . If there are aspects of the quality of applicant’s proposal that are not controlled for with information on past publications and grant histories , then this may bias my results . This second approach also captures a slightly diﬀerent causal eﬀect : the eﬀect of being related to an additional reviewer , as opposed to being related to a more inﬂuential reviewer . The relative magnitudes of these eﬀects are theoretically ambiguous : if only permanent reviewers have inﬂuence , then the eﬀect of being related to a permanent reviewer ( conditional on total relatedness ) will be larger than the eﬀect of being related to an additional member ( because that additional member may be temporary and thus , in this example , inconsequential ) . If , on the other hand , temporary members have as much inﬂuence as permanent ones , then the composition of related reviewers would not matter , but the number would . I show that both identiﬁcation strategies yield similar results . 5 . 4 Estimating Equations Having deﬁned my relatedness and quality measures , the causal eﬀect of being related to a more inﬂuential member can be estimated from the following regression : Decision icmt = a 0 + a 1 # Related Permanent icmt + a 2 # Related icmt + µX icmt + δ cmt + e icmt . ( 8 ) Decision icmt is a variable describing the decision ( either the score or the funding status ) given to ap - plicant i whose proposal is evaluated by committee c in meeting m of year t . # Related Permanent icmt is the number of permanent reviewers an applicant is related to and # Related icmt is the total num - ber . The covariates X icmt include indicators for sex ; whether an applicant’s name is Hispanic , East Asian , or South Asian ; quartics in an applicant’s total number of citations and publications over the past ﬁve years ; indicators for whether an applicant has an M . D . and / or a Ph . D . ; and indicators for the number of past R01 and other NIH grants an applicant has won , and indicators for how many she has applied to . The δ cmt are ﬁxed eﬀects for each committee - meeting so that my anal - 21 ysis compares outcomes for grants that are reviewed by the same reviewers in the same meeting . Standard errors are clustered at the committee - ﬁscal year level . Given these controls , a 1 captures the eﬀect of being related to an additional permanent reviewer on the likelihood that an applicant is funded . The overall impact of relatedness on an applicant’s likelihood of funding , a 1 , however , does not distinguish between bias and expertise for two reasons . First , expertise can have eﬀects on the funding process that are not captured by a 1 . This could happen if , for example , reviewers with expertise in a subject area are more likely to fund high quality research in that area and less likely to fund low quality research in that area ; this may change the identities of the related applicants who are funded without aﬀecting the overall likelihood that a related applicant is funded . Second , relatedness can increase the overall likelihood that an applicant is funded because of expertise as well as bias . This may happen if committees fund applicants whose quality they believe is above a threshold ; if the precision of signals for related applicants is higher , then more of these related applicants would be expected to fall above that threshold . To separately identify the roles of expertise and bias , I introduce information on quality . My theoretical model makes two predictions : ﬁrst , that applicants related to more or more inﬂuential reviewers will be more likely to be funded on average and , second , that funding decisions will be more responsive to information about quality . To test these predictions , I estimate : D icmt = a 0 + a 1 # Related Permanent icmt + a 2 Quality icmt × Related to Permanent icmt + a 3 Quality icmt + a 4 # Related icmt + µX icmt + δ cmt + ε icmt ( 9 ) In Equation ( 10 ) , the coeﬃcient a 1 captures the eﬀect of relatedness on funding that is at - tributable to bias : does being related to one additional permanent reviewer , conditional on total relatedness , aﬀect an applicant’s likelihood of being funded for reasons unrelated to quality ? The coeﬃcient a 2 , meanwhile , measures the eﬀect of relatedness that is attributable to expertise : are higher quality applications more likely to be funded when they are evaluated by related permanent members , conditional on total relatedness ? The model in Section 3 also includes terms RU and U to control for the degree of selectivity in a committee ( recall that U was the outside option of the unbiased reviewer ) . In my empirical implementation , I proxy for selectivity using the percentile pay line of the committee . I thus include a level control for pay line ( this is absorbed in the meeting ﬁxed eﬀect ) as well as the pay line interacted with relatedness . My results are not aﬀected by either the inclusion or exclusion of these variables . 15 15 In my alternative speciﬁcation using applicant ﬁxed eﬀects the analogous regression equation is given by : D icmt = a 0 + a 1 # Related icmt + a 2 Quality icmt × Related to a reviewer icmt + a 3 Quality icmt + µX icmt + δ i + ε icmt ( 10 ) 22 6 Main Results Table 6 considers the eﬀect of being related to a committee member on funding and scores . The ﬁrst column reports the raw within - meeting association between the number of permanent related reviewers and an applicant’s likelihood of being funded . Without controls , each additional related permanent member is associated with a 3 . 3 percentage point increase in the probability of funding , oﬀ an average of 21 . 4 percent . This translates into a 15 . 3 percent increase . Most of this correlation , however , reﬂects diﬀerences in the quality of applications ; applicants may be more highly cited by reviewers simply because they are better scientists . Column 2 adds controls for applicant characteristics such as past publication and grant history . This reduces the eﬀect of an additional permanent related reviewer on funding probability to 1 . 5 percentage points , or 7 . 1 percent . Even with these controls , relatedness may still be proxying for some unobserved aspect of application quality . Finally , I control for the total number of reviewers each applicant has been cited by . Given this , my identiﬁcation comes from variation in the composition of an applicant’s related reviewers ; I am comparing outcomes for two scientists with similar observables , who are cited by the same total number of reviewers , but by diﬀerent numbers of inﬂuential reviewers . In Column 3 , I ﬁnd that an additional permanent related reviewer increases an applicant’s chances of being funded by 0 . 7 percentage points , or 3 . 1 percent . This is my preferred speciﬁcation because it isolates variation in relatedness that is plausibly independent of an application’s quality . Columns 4 – 6 and 7 – 8 report the eﬀect of relatedness on an applicant’s percentile score and likelihood of being scored at all ( e . g . rejected early in the process due to low initial evaluations ) , respectively . In both these cases , I ﬁnd a similar pattern , though an economically smaller eﬀect . Being related to a more inﬂuential set of reviewers increases an applicant’s score by a quarter of a percentile and her likelihood of being scored by just over half a percent . Table 7 reports my main regressions , decomposing the eﬀects of bias and expertise . Columns 1 , 3 , and 5 reproduce the estimates of the level eﬀect of relatedness on funding and scores from Table 6 . Column 2 reports estimates of the coeﬃcients from Equation ( 10 ) for funding status . The coeﬃcient of . 0049 on the number of related permanent reviewers says that , due to bias , each additional related permanent reviewer increases the likelihood that an application is funded by 0 . 5 percentage points , or 2 . 3 percent . 16 The magnitude of this eﬀect appears to be sizable . To see this , notice that Column 2 also reports the increase in funding likelihood associated with an increase in application quality . The ﬁgure of 0 . 0067 means that a one standard deviation ( 302 future citations ) increase in quality is associated with a 3 . 02 * 0 . 0067 = 2 . 02 percentage point increase in an applicant’s likelihood of funding . The sensitivity of committees to changes in application quality highlights the magnitude of the bias eﬀects that I ﬁnd : being related to an additional permanent reviewer , conditional on total relatedness , increases an applicant’s chances of being funded by 0 . 5 percentage 16 This eﬀect is slightly smaller in magnitude than the overall eﬀect of relatedness estimated in Table 6 ; this is consistent with the explanation that expertise can also increase the overall likelihood of a related applicant being funded , by making committee members more conﬁdent about the quality of related applicants relative to unrelated applicants . 23 points or as much as a one - quarter standard deviation increase in quality . Column 2 of Table 7 also shows that review committee do a better job of discerning quality when an applicant is related to a permanent member , conditional on the total number of related reviewers . To see this , consider an applicant who is related to one permanent member versus an applicant who is related to one temporary member . A one standard deviation increase in quality for the former applicant increases her likelihood of funding by 0 . 42 + 0 . 67 = 1 . 09 percentage points compared to 0 . 67 percentage points for the latter applicant . Thus , despite overall positive bias in favor of related applicants , being related to a permanent member may not be beneﬁcial for all applicants . Because reviewers have more information about the quality of related applicants , related applicants with lower quality proposals end up receiving lower scores . These results are consistent with the predictions of my model : relationships increase distortion arising from bias but also decrease the variance of the committee’s signal of quality . My results also alleviate a potential concern about this empirical approach , which is that I may label reviewers biased if they are maximizing some unobserved aspect of application quality that is systematically diﬀerent from my citation - based measure ( this would violate Condition 2 , that measurement error in quality is conditionally mean zero ) . If , for example , reviewers are better at identifying “under - valued” research in their own area , then they may be more likely to fund low - citation related research over higher - citation unrelated research—not because of bias , but because of better information about the true quality of related projects . This behavior , however , would tend to decrease the correlation between citations and funding likelihood for related applicants , relative to unrelated applicants . The fact that reviewers appear to be more sensitive to citation - based counts of quality for applicants in their own area , as indicated by Column 2 , provides some evidence that citation counts do convey information about quality that reviewers care about . Columns 4 and 6 consider the eﬀect of relatedness on other outcome measures . Being related to an additional permanent reviewer increases an applicant’s score by one ﬁfth of a percentile or about as much as would be predicted by a one quarter standard deviation increase in quality . 17 I ﬁnd a positive , but not statistically signiﬁcant , eﬀect of relatedness on the correlation between quality and scores and no such information eﬀect on the likelihood of being scored . The magnitudes of these estimates suggest that relatedness and quality have a greater impact on an application’s funding status than on its score or likelihood of being scored . This suggests that reviewers both pay more attention to quality for applications at the margin of being funded , and are more likely to exercise their bias when this bias might be pivotal for funding . Table 8 reports results under an alternative identiﬁcation strategy of applicant ﬁxed eﬀects to control for unobserved application quality . This speciﬁcation identiﬁes the eﬀect of being related to an additional reviewer , as opposed to the eﬀect of being related to a greater proportion of permanent reviewers . My results , however , are similar : due to bias , an additional related reviewer increases an applicant’s chances of being funded by 0 . 71 percentage points . Interestingly , conditional on 17 To see this , note that 3 . 02 ( one standard deviation of quality ) times 0 . 24 ( the coeﬃcient on quality ) is equal to 0 . 71 percentage points . My estimate of bias is . 19 percentiles or about one quarter of the eﬀect of quality . 24 applicant ﬁxed eﬀects , the quality of an application is essentially not predictive of funding likelihood except for applicants who are related to reviewers . 7 How Do Relationships Aﬀect the Eﬃciency of Grant Provision ? My main results show that 1 ) applicants who are related to study section members are more likely to be funded , independent of quality , as measured by the number of citations that their research eventually produces ; and 2 ) that the correlation between eventual citations and fund - ing likelihood is higher for related applicants , meaning that study section members are better at discerning the quality of applicants in their own area . Next , I embed my analysis of the eﬀect of relationships on decisions into a broader analysis of their eﬀect on overall eﬃciency . Assessing the eﬃciency consequences of related experts requires taking a stand on the social welfare function that the NIH cares about ; without one , it would be impossible to assess whether distortions arising from the presence of related experts brings the the grant review process closer or further from the social optimum . In this section , I assume that policy makers care about maximizing either the number or impact of publications and citations associated with NIH - funded research . An important disclaimer to note is that an eﬃciency calculation based on this measure of welfare may not always be appropriate . If , for instance , the NIH cares about promoting investigators from disadvantaged demographic or institutional backgrounds , then a policy that increases total citations may actually move the NIH further from the goal of encouraging diversity . Yet , while citations need not be the only welfare measure that the NIH cares about , there are compelling reasons why policy - makers should take citation - based measures of quality in account when assessing the eﬃcacy of grant review . In addition to begin standard measure of quality used by both economists when studying science and by scientists themselves , citations can also be used to construct , as discussed in Section 5 , ﬂexible metrics that capture both high quality normal science and high impact work . My citation data , moreover , extend beyond my sample period , allowing me to observe the quality of a publication as judged in the long run . This alleviates concerns that citations may underestimate the importance of groundbreaking projects that may not be well cited in the short run . Given these caveats , I begin by comparing the actual funding decision for an application to the counterfactual funding decision that would have obtained in the absence of relationships . Speciﬁcally , I deﬁne : Decision Benchmark icmt = Decision icmt ( actual funding ) Decision No Relationship icmt = Decision icmt − (cid:98) a 1 # Related icmt − (cid:98) a 2 Quality icmt × Related to permanent icmt 25 where (cid:98) a 1 and (cid:98) a 2 are estimated from Equation ( 10 ) of Section 5 . 4 . 18 The counterfactual funding decision represents what the committee would have chosen had applicants related to permanent members been treated as if they were unrelated . I summarize the eﬀect of relationships by comparing the quality of the proposals that would have been funded had relationships not been taken into account with the quality of those that actually are funded . Speciﬁcally , I consider all applications that are funded and sum up the number of publications and citations that accrue to this portfolio . This is my benchmark measure of the quality of NIH peer review . I then simulate what applications would have been funded were relationships not taken into account . To do this , I ﬁx the total number of proposals that are funded in each committee meeting but reorder applications by their counterfactual funding probabilities . I sum up the number of publications and citations that accrue to this new portfolio of funded grants . The diﬀerence in the quality of the benchmark and counterfactual portfolio provides a concrete , summary measure of the eﬀect of relationships on the quality of research that the NIH supports . To get a fuller sense of how committees aﬀect decision - making , I create a measure of committee - speciﬁc performance and examine how relationships aﬀect the distribution of performance among NIH peer review committees . First , I deﬁne a committee’s value - added . Suppose two scientists sub - mit applications to the same committee meeting . A good committee is one that systematically funds the application that is higher quality . Good committees , moreover , should bring insights beyond what can simply be predicted by objective measures of an applicant’s past performance . In partic - ular , suppose now that two scientists with identical objective qualiﬁcations submit applications to the same committee meeting . A committee with high value - added is one that systematically funds the application that subsequently generates more citations , even though the applications initially look similar . My measure of committee value - added formalizes this intuition : Decision icmt = a + b cmt Quality icmt + µX icmt + δ cmt + e icmt . ( 11 ) Here , the dependent variable is either an application’s actual funding status Decision icmt = D Benchmark icmt or its counterfactual funding status Decision icmt = Decision No Relationship icmt . The committee ﬁxed ef - fects δ cmt restrict comparisons of applications to those evaluated in a single meeting and the X icmt control for past applicant qualiﬁcations . The coeﬃcients of interest are the b cmt . These are meeting - speciﬁc slopes that capture the relationship between an application’s quality Quality icmt and its likelihood of being funded Decision icmt . Each b cmt is interpreted as the percentage point change in the likelihood that an application is funded for a one unit increase in quality . This forms the basis of my committee value - added measure . This concept of committee value - added diﬀers from the classical notion of value - added com - monly used in the teacher or manager performance literature ( see Kane , Rockoﬀ , and Staiger 2007 , and Bertrand and Schoar 2003 ) . Teacher value - added , for instance , is typically estimated by re - gressing student test scores on lags of test scores , school ﬁxed eﬀects , and teacher ﬁxed eﬀects . A 18 Even though Decision No Relationship icmt is constructed using estimates from Equation ( 10 ) , it does not rely on the model to interpret those coeﬃcients . 26 teacher’s ﬁxed eﬀect , the average performance of her students purged of individual , parental , and school - wide inputs , is taken to be the basic measure of quality . This traditional measure , however , does not capture value - added in my setting . Good com - mittees are not ones in which all applications are high - performing ; after all , committees have no control over what applications get submitted . Rather , good committees are ones in which funded grants perform better than unfunded grants . I measure a committee’s performance by the relation - ship between an applicant’s quality and its likelihood of getting funded because , unlike a teacher , a committee’s job is not to improve the quality of grant applications but to distinguish between them . One concern with the estimated ˆ b cmt is that idiosyncratic variation in grant performance may lead me to conclude that some committee meetings do an excellent job of identifying high - quality applications when in fact they are simply lucky . I correct for this using a standard Bayesian shrinkage approach , discussed in Appendix D . 7 . 1 Results Table 9 estimates the eﬀect of relationships on the quality of research that the NIH supports . In eﬀect , I ask what the NIH portfolio of funded grants would have been had committees treated applicants who are related to permanent members as if they were not , holding all else ﬁxed . In my sample , I observe 93 , 558 applications , 24 , 404 of which are funded . Using this strategy , I ﬁnd that 2 , 500 or 2 . 7 percent , of these applications change funding status under the counterfactual . On average , being related to a greater composition of inﬂuential reviewers helps an applicant get funded ; ignoring them would decrease the number of related applicants who are funded by 4 . 5 percent . These applications from related reviewers , however , are on average better than the applications that would have been funded had relationships not mattered . The overall portfolio of funded grants under the counterfactual produces two to three percent fewer citations , publications , and high - impact publications . This pattern is underscored by Figure 6 , which graphs the distribution of value - added under the benchmark and counterfactual cases . FIrst , Figure 6 shows that there is substantial variation in the ability of committees to identify grant applications that subsequently produce high - impact research . For study section with median value - added , a one standard deviation increase in the quality of an application evaluated by the median committee would increase its likelihood of funding by approximately 6 . 3 percent . For 75th percentile committees , this ﬁgure is 13 . 3 percent . A striking feature of this distribution is that the bottom quarter to third of committees actively subtract value , meaning that increases in quality are correlated with decreases in the likelihood that an application is funded . As explained in Section 7 , these ﬁgures account for sampling variation so that a committee is deemed to have negative value - added only if it systematically does so from meeting to meeting . This could happen if committees systematically favor other factors that are negatively correlated with future citations , for example , the funding of new investigators . Ignoring the role of intellectually related reviewers tends to worsen committee value - added throughout the middle of the value - added distribution . The study section with the median value 27 added , as calculated using an application’s counterfactual funding status falls from 6 . 3 percent to 4 . 7 percent ; the 75th percentile falls to 11 . 8 percent and the 25th percentile falls from - 1 . 0 percent to - 2 . 9 percent . The magnitudes of these declines are small compared to the overall distribution ; understanding other reasons for this dispersion is an important area for future research . 8 Conclusion This paper develops a conceptual and statistical framework for understanding and separately identifying the eﬀects of bias and expertise in grant evaluation . My results show that , as a result of bias , being related to a more inﬂuential member of a review committee increases an application’s chances of being funded by 3 . 1 percent . Viewed in terms of how committees respond to increases in application quality , this bias increases the chances that an application is funded by the same amount as would be predicted by a one - quarter standard deviation increase in application quality . The expertise that reviewers have about research in their own area , however , also improves the quality of review : working in the same area as a permanent committee member increases the responsiveness of the committee to proposal quality by over 50 percent . On net , ignoring relationships reduces the quality of the NIH - funded portfolio as measured by numbers of citations and publications by two to three percent . My results suggest that there may be scope for improving the quality of peer review . I document signiﬁcant and persistent dispersion in the ability of committees to fund high - quality research . Finding ways to eliminate the lower tail of committees , for which increases in quality are actually associated with decreases in funding likelihood , could lead to large improvements in the quality of NIH - funded research as measured by citations . The magnitude of these potential beneﬁts are not small when viewed in dollar terms . NIH spending for my sample of approximately 25 , 000 funded grants totaled more than 34 billion dollars ( 2010 dollars ) . These grants generated approximately 170 , 000 publications and 6 . 8 million citations . 19 This means that , in my sample , the NIH spent about 250 , 000 dollars per publication , or about 5 , 000 dollars per single citation . Even if these numbers do not represent the social value of NIH - funded research , they suggest that the value generated by high - quality peer review can be substantial . This paper shows that working with reviewers who are intellectually closely related to applicants can be beneﬁcial despite the bias that it introduces . Understanding and quantifying other factors aﬀecting committee performance is an important area for future work . Here , the uniformity of NIH’s many chartered study sections is helpful because it allows for the possibility of targeted randomized experiments , holding other institutional features constant . For instance , to understand the impact of committee composition on peer - review quality , applicants could be assigned to intellectually broad or narrow committees . 19 I have 170 , 000 publications linked to grants via formal grant acknowledgments computed from the PubMed database . PubMed , however , undercounts citations because it only counts citations from a subset of articles archived in PubMed Central . To arrive at the 6 . 8 million citations ﬁgure , I use total publications calculated via text - matching ( about 100 , 000 publications ) and the total citations accruing to those publications ( 4 . 3 million ) to compute the average number of citations per publication . I then scale this by the 170 , 000 publications found in PubMed . 28 Answers to these questions can provide insights on how to improve project evaluation at the NIH and elsewhere . References [ 1 ] Acemoglu , Daron . ( 2008 ) Introduction to Modern Economic Growth , Princeton University Press . [ 2 ] Aghion , Philippe and Jean Tirole . ( 1994 ) “The Management of Innovation . ” Quarterly Journal of Economics , 109 ( 4 ) , 1185 - 1209 . [ 3 ] Arrow , Kenneth . ( 1962 ) . “Economic welfare and the allocation of resources for invention . ” in Richard Nelson , ed . , The Rate and Direction of Inventive Activity , Princeton University Press . [ 4 ] Autor , David and David Scarborough . ( 2008 ) “Does Job Testing Harm Minority Workers ? Evidence from Retail Establishments . ” Quarterly Journal of Economics , 123 ( 1 ) : 219 - 277 . [ 5 ] Azoulay , Pierre , Toby Stuart , and Yanbo Wang . ( 2011 ) “Matthew : Fact or Fable ? ” Working paper . Available online : http : / / pazoulay . scripts . mit . edu / docs / shmatus . pdf [ 6 ] Bagues , Manuel and Natalia Zinovyeva . ( 2012 ) “The Role of Connections in Academic Pro - motions . ” IZA Discussion Paper # 6821 . [ 7 ] Bayer , Patrick , Stephen Ross and Giorgio Topa . ( 2008 ) “Place of work and place of residence : informal hiring network and labor market outcomes . ” Journal of Political Economy , 116 ( 6 ) , 1150 - 1195 . [ 8 ] Bandiera , Oriana , Iwan Barankay , and Imran Rasul . ( 2009 ) “Social Connections and Incentives in the Workplace : Evidence from Personnel Data . ” Econometrica , 77 ( 4 ) : 1047 - 1094 . [ 9 ] Bewley , Truman . ( 1999 ) Why Wages Dont Fall During a Recession . Cambridge , MA : Harvard University Press . [ 10 ] Brogaard , Jonathan , Joseph Engelberg and Christopher Parsons . ( 2012 ) “Network Position and Productivity : Evidence from Journal Editor Rotations . ” mimeo . [ 11 ] Cockburn , Iain , and Rebecca Henderson . ( 2000 ) “Publicly Funded Science and the Productiv - ity of the Pharmaceutical Industry . ” Innovation Policy and the Economy , 1 : 1 - 34 . [ 12 ] Congressional Budget Oﬃce . ( 2006 ) “Research and Development in the Pharmaceuticals Indus - try . ” Available online at : http : / / www . cbo . gov / ftpdocs / 76xx / doc7615 / 10 - 02 - DrugR - D . pdf [ 13 ] Crawford , Vincent and Joel Sobel . ( 1982 ) “Strategic Information Transmission . ” Econometrica , 50 ( 6 ) : 1431 - 1451 . [ 14 ] David , Paul , Mowery David , and W . E . Steinmueller . ( 1992 ) “Assessing the Economic Payoﬀ from Basic Research . ” Economics of Innovation and New Technology 73 - 90 . [ 15 ] Ellison , Glenn . ( 2011 ) “Is Peer Review in Decline ? ” Economic Inquiry , 49 ( 3 ) : 635 - 657 . [ 16 ] Gerin , William . ( 2006 ) Writing the NIH grant proposal : a step - by - step guide . Thousand Oaks , CA : Sage Publications . [ 17 ] Ginther , Donna , Walter Schaﬀer , Joshua Schnell , Beth Masimore , Faye Liu , Laurel Haak , Raynard Kington . ( 2011 ) “Race , Ethnicity , and NIH Research Awards . ” Science , 333 ( 6045 ) : 1015 - 1019 . 29 [ 18 ] Griliches , Zvi . ( 1992 ) “The search for R & D spillovers . ” Scandinavian Journal of Economics , 94 ( supplement ) , S29 - S47 . [ 19 ] Hall , Bronwyn . ( 1994 ) “R & D Tax Policy During the Eighties : Success or Failure ? ” NBER Working Paper # 4240 [ 20 ] Hegde , Deepak . ( 2009 ) “Political Inﬂuence behind the Veil of Peer Review : An Analysis of Public Biomedical Research Funding in the United States” Journal of Law and Economics , 52 ( 4 ) : 665 - 690 . [ 21 ] Jacob , Brian and Lars Lefgren . ( 2011 ) “The Impact of Research Grant Funding on Scientiﬁc Productivity . ” Journal of Public Economics 95 ( 9 - 10 ) : 1168 - 1177 . [ 22 ] Jones , Benjamin . ( 2010 ) “Age and Great Invention . ” Review of Economics and Statistics 92 ( 1 ) : 1 - 14 . [ 23 ] Kane , Thomas , Jonah Rockoﬀ , and Doug Staiger . ( 2007 ) “What Does Certiﬁcation Tell Us About Teacher Eﬀectiveness ? Evidence from New York City . ” Economics of Education Review 27 ( 6 ) , 615 - 631 . [ 24 ] Kerr , William . ( 2008 ) “Ethnic Scientiﬁc Communities and International Technology Diﬀusion . ” The Review of Economics and Statistics , 90 ( 3 ) : 518 - 537 . [ 25 ] Kremer , Michael and Heidi Williams . ( 2010 ) “Incentivizing innovation : Adding to the toolkit , ” in Josh Lerner and Scott Stern , eds . , Innovation Policy and the Economy , Vol . 10 . Chicago , IL : University of Chicago Press . [ 26 ] Lamont , Michele . ( 2010 ) How Professors Think : Inside the Curious World of Academic Judg - ment . Cambridge , MA : Harvard University Press . [ 27 ] Lerner , Josh . ( 1999 ) “The Government as Venture Capitalist : The Long - Run Impact of the SBIR Program . ” The Journal of Business , 72 ( 3 ) : 285 - 318 . [ 28 ] Merton , Robert . ( 1968 ) “The Matthew Eﬀect in Science” Science 159 ( 3810 ) : 5663 . [ 29 ] National Institutes of Health . ( 2008 ) Oﬃce of Extramural Research . Peer Review Process . http : / / grants . nih . gov . libproxy . mit . edu / grants / peer review process . htm . [ 30 ] Nelson , Richard . ( 1959 ) “The simple economics of basic scientiﬁc research . ” Journal of Political Economy , 67 ( 3 ) , 297 - 306 . [ 31 ] Romer , Paul . ( 1990 ) “Endogenous Technological Change . ” Journal of Political Economy , 98 , S71 - S102 . [ 32 ] Sampat , Bhaven and Frank Lichtenberg . ( 2011 ) “What are the Respective Roles of the Public and Private Sectors in Pharmaceutical Innovation ? ” Health Aﬀairs , 30 ( 2 ) : 332 - 339 . [ 33 ] Stephan , Paula . ( 2012 ) How Economics Shapes Science . Cambridge , MA : Harvard University Press . 30 NIH Administrative Data on Grant Applicants 1 . Applicant names 2 . Project title 3 . Grant score and funding outcome 4 . Applicant degrees PubMed / Web of Science Publication Database 1 . Database of life sciences publications 2 . Citation information for publications Reviewer - Applicant Relatedness Applicant past publication history Applicant demographics Applicant grant history Application quality : measured by future research output Committee Attendance Rosters 1 . Full names of reviewers 2 . Capacity in which member served ( permanent or temporary ) Prior Applicant Characteristics and Qualifications Relatedness Future Application Quality Figure 1 : Data sources and variable construction 31 0 . 05 . 1 . 15 . 2 D en s i t y 0 5 10 15 20 Funding − purged Quality Measure ( # of publications , text − matched , 0 − 1 year window around grant review ) Unfunded Funded Distribution of Publications for Funded and Unfunded Applications 0 . 002 . 004 . 006 . 008 . 01 D en s i t y 0 100 200 300 400 Funding − purged Quality Measure ( # of citations , text − matched , 0 − 1 year window around grant review ) Funded Unfunded Distribution of Citations for Funded and Unfunded Applications ® Figure 2 : Distribution of application quality : funded and unfunded grants 32 − . 5 0 . 5 1 1 . 5 2 R e s i du a l F und i ng − P u r ged Q ua li t y M ea s u r e ( # c i t a t i on s , t e x t − m a t c hed , 0 − 1 y ea r a r ound g r an t r e v i e w ) − 40 − 20 0 20 40 Residual Score Funded Unfunded Citations associated to scores , adjusted for meeting effects ® Figure 3 : Mean application quality by score : funded and unfunded grants 33 0 . 005 . 01 . 015 . 02 D en s i t y 0 50 100 150 200 # of Publications Permanent Temporary Distribution of Past Publications for Permanent and Temporary Reviewers 0 . 0002 . 0004 . 0006 . 0008 D en s i t y 0 1000 2000 3000 4000 5000 # of Citations Permanent Temporary Distribution of Past Citations for Permanent and Temporary Reviewers ® Figure 4 : Distribution of past citations : permanent and temporary reviewers 34 0 . 005 . 01 . 015 D en s i t y 0 100 200 300 400 Funding − purged Quality Measure ( # of citations , text − matched , 0 − 1 year window around grant review ) 0 Perm , 1 Total 1 Perm , 1 Total Distribution of Citation − based Quality Applicants Related to 1 Reviewer 0 . 05 . 1 . 15 . 2 D en s i t y 0 5 10 15 20 Funding − purged Quality Measure ( # of publications , text − matched , 0 − 1 year window around grant review ) 0 Perm , 1 Total 1 Perm , 1 Total Distribution of Publication − based Quality Applicants Related to 1 Reviewer 0 . 002 . 004 . 006 . 008 . 01 D en s i t y 0 100 200 300 400 Funding − purged Quality Measure ( # of citations , text − matched , 0 − 1 year window around grant review ) 0 Perm , 2 Total 1 Perm , 2 Total 2 Perm , 2 Total Distribution of Citation − based Quality Applicants Related to 2 Reviewers 0 . 05 . 1 . 15 . 2 D en s i t y 0 5 10 15 20 Funding − purged Quality Measure ( # of publications , text − matched , 0 − 1 year window around grant review ) 0 Perm , 2 Total 1 Perm , 2 Total 2 Perm , 2 Total Distribution of Publication − based Quality Applicants Related to 2 Reviewers ® Figure 5 : Application quality conditional on total related reviewers 35 0 . 01 . 02 . 03 . 04 D en s i t y − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 % change in likelihood of funding for a 1 s . d . increase in quality , conditional on past performance Benchmark No Relationships kernel = epanechnikov , bandwidth = 2 . 1625 Benchmark vs . No Relationships Distribution of Committee Value Added ® Figure 6 : Distribution of meeting - level value - added 36 Sample Coverage Std . Dev . Std . Dev . # Grants 93 , 558 156 , 686 # Applicants 36 , 785 46 , 546 Years 1992 - 2005 1992 - 2005 # Study Sections 250 380 # Study Section Meetings 2 , 083 4 , 722 Grant Characteristics % Awarded 26 . 08 30 . 48 % Scored 61 . 58 64 . 04 % New 70 . 31 71 . 21 Percentile Score 70 . 05 18 . 42 71 . 18 18 . 75 # Publications , grant - publication matched ( median ) 2 5 2 5 # Citations , grant - publication matched ( median ) 36 265 38 302 PI Characteristics % Female 23 . 21 22 . 58 % Asian 13 . 96 13 . 27 % Hispanic 5 . 94 5 . 79 % M . D . 28 . 72 29 . 26 % Ph . D . 80 . 46 79 . 69 % New investigators 19 . 70 20 . 02 # Publications , past 5 years 15 60 15 55 # Citations , past 5 years 416 1431 423 1474 Roster - Matched Sample Notes : The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . Future publications refers to the number of research articles that the grant winner publishes in the year following the grant which share at least one salient word overlap between the grant project title and the publication title . Past publications include any first , second , and last authored articles published in the five years prior to applying for the grant . The full sample includes data from any new or competing R01 grant evaluated in chartered study sections from 1992 to 2005 . Investigators with common names are dropped as are any for which the covariates are missing . Social science study sections are dropped . Table 1 : Applicant Characteristics Full Sample 37 ( 1 ) ( 2 ) ( 3 ) ( 4 ) Dep var : Grant Quality No score controls Controls for smooth function of score No score controls Controls for smooth function of score 1 ( Grant is funded ) 0 . 4692 * * * - 0 . 0286 0 . 7492 * * * 0 . 0683 ( 0 . 032 ) ( 0 . 059 ) ( 0 . 035 ) ( 0 . 055 ) Observations 57 , 613 57 , 613 93 , 558 93 , 558 R - squared 0 . 1169 0 . 1221 0 . 1021 0 . 1119 Meeting Fixed Effects X X X X Subsample of Scored Applications Full Sample ( Score Imputed for Unscored Applications ) Table 2 : Does being funded directly affect my measure of quality ? Notes : Coefficients are reported from a regression of grant quality on an indicator for whether the grant was funded and meeting fixed effects . Columns ( 2 ) and ( 4 ) include controls for quartics in the applicant score . Column ( 2 ) compares grant applications with the same score and evaluated in the same meeting , but which differ in funding status because they are assigned to different Institutes with different paylines . Scores are available only for applications that were not triaged ; Columns ( 3 ) and ( 4 ) assign scores of zero to triaged applications to test with the full sample . 38 Reviewer Characteristics Std . Dev . # Reviewers 18 , 916 # Permanent reviewers per meeting 17 . 23 4 . 52 # Temporary reviewers per meeting 12 . 35 7 . 44 # Meetings per permanent reviewer 3 . 69 3 . 03 # Meetings per temporary reviewer 1 . 78 1 . 30 # Applications 53 . 73 17 . 31 Relationship Characteristics # Reviewers who cite applicant 1 . 94 2 . 81 # Permanent reviewers who cite applicant 1 . 11 1 . 73 # Applicants cited by permanent reviewers 4 . 12 5 . 32 # Applicants cited by temporary reviewers 4 . 12 5 . 09 Roster Matched Sample Table 3 : Committee Descriptives Notes : The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . Future publications refers to the number of research articles that the grant winner publishes in the 2 years following the grant which share at least one salient word overlap between the grant project title and the publication title . Past publications include any ﬁrst , second , and last authored articles published in the ﬁve years prior to applying for the grant . Investigators with common names are dropped as are any for which the covariates are missing . Social science study sections are dropped . 39 Number of reviewers Reviewer Characteristics % Female % Asian % Hispanic % M . D . % Ph . D . # Publications , past 5 years ( median ) # Citations , past 5 years ( median ) Reviewer Transitions % Permanent in the Past % Permanent in the Future % Temporary in the Past % Temporary in the Future Current Permanent Members 61 . 87 63 . 71 38 . 11 35 . 45 Current Temporary Members 16 . 25 41 . 30 32 . 73 50 . 13 Permanent Temporary 9371 25 . 85 80 . 99 21 31 . 68 14 . 99 6 . 40 27 . 42 590 Table 4 : Characteristics permanent and temporary members Notes : The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . Future publications refers to the number of research articles that the grant winner publishes in the 2 years following the grant which share at least one salient word overlap between the grant project title and the publication title . Past publications include any first , second , and last authored articles published in the five years prior to applying for the grant . Investigators with common names are dropped as are any for which the covariates are missing . Social science study sections are dropped . Transitions are calculated based on whether a reviewer is present in the roster database during the full sample years from 1992 to 2005 . Means are taken for the years 1997 to 2002 in order to allow time to observe members in the past and future within the sample . 79 . 45 22 606 14067 24 . 28 13 . 08 5 . 05 40 Related to 0 Reviewers % Female % Asian % Hispanic % M . D . % Ph . D . % New investigators # Publications , past 5 years ( median ) # Citations , past 5 years ( median ) N Related to 1 Reviewer Total % Female % Asian % Hispanic % M . D . % Ph . D . % New investigators # Publications , past 5 years ( median ) # Citations , past 5 years ( median ) N Related to 2 Reviewers Total % Female % Asian % Hispanic % M . D . % Ph . D . % New investigators # Publications , past 5 years ( median ) # Citations , past 5 years ( median ) N Notes : The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . Future publications refers to the number of research articles that the grant winner publishes in the year following the grant which share at least one salient word overlap between the grant project title and the publication title . Past publications include any first , second , and last authored articles published in the five years prior to applying for the grant . Investigators with common names are dropped as are any for which the covariates are missing . Social science study sections are dropped . Transitions are calculated based on whether a reviewer is present in the roster database during the full sample years from 1992 to 2005 . Means are taken for the years 1997 to 2002 in order to allow time to observe members in the past and future within the sample . 22 . 24 23 . 97 5 . 14 5 . 02 10980 7049 ( 1102 ) ( 1080 ) 13 . 51 ( 52 ) 15 . 09 5 . 79 5 . 57 27 . 11 26 . 71 82 . 24 1 Permanent 1 Temporary 2 Permanent 1 Each 20 . 26 20 . 89 15 15 442 443 12 . 54 13 . 17 81 . 63 28 . 64 29 . 28 79 . 88 80 . 02 19 . 76 19 . 34 ( 49 ) ( 45 ) ( 1336 ) ( 1233 ) ( 1050 ) 563 556 ( 31 ) ( 50 ) 2 Temporary 22 . 93 13 . 69 5 . 82 28 . 53 81 . 04 17 510 4841 5094 2403 15 . 88 16 . 25 17 . 06 18 18 82 . 73 27 . 50 15 . 35 37757 27 . 22 9 ( 31 ) Table 5 : Applicant characteristics , by number and composition of related reviewers 172 ( 713 ) 6 . 88 25 . 40 41 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) R e l a t e d P e r m a n e n t R e v i e w e r s 0 . 0328 * * * 0 . 0155 * * * 0 . 0066 * * * 1 . 1067 * * * 0 . 5192 * * * 0 . 23 71 * * 0 . 0500 * * * 0 . 0248 * * * 0 . 0042 * * ( 0 . 001 ) ( 0 . 001 ) ( 0 . 002 ) ( 0 . 054 ) ( 0 . 052 ) ( 0 . 093 ) ( 0 . 002 ) ( 0 . 001 ) ( 0 . 002 ) T o t a l R e l a t e d R e v i e w e r s 0 . 0066 * * * 0 . 2105 * * * 0 . 0153 * * * ( 0 . 001 ) ( 0 . 060 ) ( 0 . 001 ) O b s e r v a t i o n s 93 , 558 93 , 558 93 , 558 57 , 613 57 , 613 57 , 613 93 , 558 93 , 558 93 , 558 R - s q u a r e d 0 . 0630 0 . 0906 0 . 0909 0 . 1186 0 . 1390 0 . 1 392 0 . 0775 0 . 1230 0 . 1243 C o mm i tt ee × Y e a r × C y c l e F E X X X X X X X X X P a s t P e r f o r m a n c e , P a s t G r a n t s , a nd X X X X X X N o t e s : C o e ff i c i e n t s a r e r e p o r t e d f r o m a r e g r e ss i o n o f c o mm i tt ee d e c i s i o n s ( a b o v e p a y li n e , s c o r e , o r s c o r e d a t a ll ) o n t h e nu m b e r o f p e r m a n e n t m e m b e r s r e l a t e d t o a n a pp li c a n t , c o n t r o lli n g f o r m ee t i n g l e v e l f i x e d e f f e c t s . C o l u m n s ( 2 ) ( 5 ) a nd ( 8 ) i n c l ud e i nd i c a t o r s f o r s e x a nd w h e t h e r a n a pp li c a n t ' s n a m e i s H i s p a n i c , E a s t A s i a n , o r S o u t h A s i a n , q u a r t i c s i n a n a pp li c a n t ' s t o t a l nu m b e r o f c i t a t i o n s a nd pub li c a t i o n s o v e r t h e p a s t 5 y e a r s , i nd i c a t o r s f o r w h e t h e r a n a pp li c a n t h a s a n M . D . a nd / o r a P h . D . , a nd i nd i c a t o r s f o r t h e nu m b e r o f p a s t R 01 a nd o t h e r N I H g r a n t s a n a pp li c a n t h a s w o n a nd i nd i c a t o r s f o r h o w m a n y s h e h a s a pp li e d t o . C o l u m n s ( 3 ) ( 6 ) a nd ( 9 ) i n c l ud e a n a dd i t i o n a l c o n t r o l f o r t h e t o t a l nu m b e r o f r e l a t e d r e v i e w e r s . T h e a n a l y t i c s a m p l e i n c l ud e s n e w o r c o m p e t i n g R 01 g r a n t s e v a l u a t e d i n c h a r t e r d s t ud y s e c t i o n s f r o m 1992 t o 2005 , f o r w h i c h I h a v e s t ud y s e c t i o n a tt e nd a n c e d a t a . A r e v i e w e r i s r e l a t e d t o a n a pp li c a n t i f t h e r e v i e w e r h a s c i t e d a n y o f t h e a pp li c a n t ' s p r e v i o u s r e s e a r c h i n t h e 5 y e a r s p r i o r t o g r a n t r e v i e w . T a b l e 6 : W ha t i s t h e e ff e ct o f b e i n g r e l a t e d t o a r e v i e w e r o n an a pp l i c an t ' s l i k e l i h oo d o f f un d i n g ? 1 ( S c o r e d a t a ll ) M e a n = . 640 , S D = . 480 M e a n = 0 . 214 , S D = 0 . 410 M e a n = 71 . 18 , S D = 18 . 75 1 ( S c o r e i s a b o v e t h e p a y li n e ) S c o r e ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Related Permanent Reviewers 0 . 0066 * * * 0 . 0049 * * 0 . 2371 * * 0 . 1946 * * 0 . 0042 * * 0 . 0032 ( 0 . 002 ) ( 0 . 002 ) ( 0 . 093 ) ( 0 . 094 ) ( 0 . 002 ) ( 0 . 002 ) Related to Permanent Reviewers × Future 0 . 0042 * * * 0 . 0741 0 . 0007 ( 0 . 001 ) ( 0 . 059 ) ( 0 . 001 ) Future Citations 0 . 0067 * * * 0 . 2356 * * * 0 . 0109 * * * ( 0 . 001 ) ( 0 . 062 ) ( 0 . 001 ) Total Related Reviewers 0 . 0066 * * * 0 . 0072 * * * 0 . 2105 * * * 0 . 2282 * * * 0 . 0153 * * * 0 . 0158 * * * ( 0 . 001 ) ( 0 . 001 ) ( 0 . 060 ) ( 0 . 060 ) ( 0 . 001 ) ( 0 . 001 ) Observations 93 , 558 93 , 558 57 , 613 57 , 613 93 , 558 93 , 558 R - squared 0 . 0909 0 . 0937 0 . 1392 0 . 1405 0 . 1243 0 . 1265 Committee × Year × Cycle FE X X X X X X Past Performance , Past Grants , and Demographics X X X X X X 1 ( Scored at all ) Mean = . 640 , SD = . 480 Notes : Coefficients are reported from a regression of committee decisions ( score or funding status ) on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . Columns ( 1 ) ( 3 ) and ( 5 ) are reproduced from Table 6 . Columns ( 2 ) , ( 4 ) and ( 6 ) add controls for application quality and application quality interacted with relatedness to permanent reviewers . The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are calculated using all publications by an applicant in the year after grant review , with text matching . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . Table 7 : What is the contribution of expertise vs . bias ? 1 ( Score is above the payline ) Score Mean = 0 . 214 , SD = 0 . 410 Mean = 71 . 18 , SD = 18 . 75 43 49 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Total Related Reviewers 0 . 0065 * * * 0 . 0071 * * * 0 . 2907 * * * 0 . 2914 * * * 0 . 0112 * * * 0 . 0108 * * * ( 0 . 001 ) ( 0 . 001 ) ( 0 . 060 ) ( 0 . 061 ) ( 0 . 001 ) ( 0 . 001 ) Related to Reviewers × Future Citations 0 . 0042 * 0 . 0969 0 . 0077 * * * ( 0 . 002 ) ( 0 . 095 ) ( 0 . 002 ) Future Citations 0 . 0016 0 . 0529 - 0 . 0022 ( 0 . 002 ) ( 0 . 097 ) ( 0 . 002 ) Observations 93 , 558 93 , 558 57 , 613 57 , 613 93 , 558 93 , 558 R - squared 0 . 4524 0 . 4648 0 . 5448 0 . 5450 0 . 5629 0 . 5632 Applicant FE X X X X X X Past Performance and Past Grants X X X X X X Notes : Coefficients are reported from a regression of committee decisions ( score or funding status ) on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . All regressions include controls for applicant effects . The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are calculated using all publications by an applicant in the - 1 to 2 years after grant review , with text matching . Applicant characteristics include quartics in an applicant ' s total number of citations and publications over the past 5 years and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . Table 8 : What is the contribution of expertise vs . bias ? Applicant fixed effects 1 ( Score is above the payline ) Score 1 ( Scored at all ) Mean = 0 . 214 , SD = 0 . 410 Mean = 71 . 18 , SD = 18 . 75 Mean = . 640 , SD = . 480 44 Benchmark No Relationships Number of Funded Grants 24 , 404 24 , 404 Number of Grants that Change Funding Status 2 , 500 2 , 500 Total # Citations 6 , 751 , 209 6 , 593 , 302 ( % change relative to benchmark ) - 2 . 34 Total # Publications 151 , 662 146 , 674 ( % change relative to benchmark ) - 3 . 29 Total # in Top 99 % of Citations 6 , 642 6 , 475 ( % change relative to benchmark ) - 2 . 51 Total # in Top 90 % of Citations 13 , 010 12 , 645 ( % change relative to benchmark ) - 2 . 81 Total # Related Applicants Funded 18 , 615 17 , 782 ( % change relative to benchmark ) - 4 . 47 Table 9 : What is the effect of relationships on the quality of research that the NIH supports ? Notes : Benchmark refers to characteristics of grants ordered according to their predicted probability of funding , using the main regression in Table 6 of funding status on relationships and other characteristics . No relationships refers to ordering of grants under the assumption that relatedness to permanent members and relatedness to permanent members interacted with quality do not matter ( their coefficients are set to zero ) . Expected citations are calculated as fitted values from a regression of citations on relationships , past performance , demographics , and meeting fixed effects . The number of projects that are funded is kept constant within meeting . See text for details . 45 APPENDIX MATERIALS : FOR ONLINE PUBLICATION A Proof of Proposition 4 . 1 Nature has drawn true quality Q ∗ , and types Q = (cid:40) Q R = Q ∗ + ε R if R = 1 Q UR = Q ∗ + ε UR if R = 0 Given this , the Perfect Bayesian equilibrium for this game is characterized by : 1 . A set of beliefs that the committee has about true quality Q ∗ given the message M : µ ( Q ∗ | M ) . 2 . A message strategy M ( Q ) for a reviewer , given his or her posterior Q . 3 . A decision strategy D ( M ) for the committee , given the reviewer’s message . These strategies and beliefs must be optimal in the following sense : 1 . For each Q ∗ , (cid:82) M ∈ M µ ( Q ∗ | M ) dM = 1 . 2 . For each message M , the committee’s decision D ( M ) must maximize its expected payoﬀs given their beliefs µ ( Q ∗ | M ) : D ∈ argmax (cid:90) Q ∗ ∈ Q ∗ P C ( D , Q ∗ ) µ ( Q ∗ | M ) dQ ∗ 3 . For each posterior Q , the reviewer’s message M ( Q ) must maximize his / her payoﬀs given the committee’s strategy : M ∈ argmax (cid:90) Q ∗ ∈ Q ∗ P ( D ( M ) , Q ∗ ) f ( Q ∗ | Q ) dQ ∗ , for P = { P UR , P R } where f ( · | Q ) is the density of Q ∗ given Q . 4 . For all reviewer posteriors Q ∈ Q M that induce message M to be sent with positive proba - bility , committee beliefs µ ( Q ∗ | M ) must follow from Bayes’ Rule : µ ( Q ∗ | M ) = (cid:82) Q ∗ ∈ Q ∗ M ( Q ) f ( Q ∗ | Q ) dQ ∗ (cid:82) Q ∈ Q M (cid:82) Q ∗ ∈ Q ∗ M ( Q ) f ( Q ∗ | Q ) dQ ∗ dQ Having deﬁned the equilibrium concept , I proceed with the proof . Case 1 . Suppose that the reviewer reports her exact posterior and the committee to believes it . In this case , the committee maximizes its utility by funding the proposal if and only if Q ∗ + ε UR > U . The reviewer has no incentive to deviate from this strategy because she is receiving her highest payoﬀ as well . Suppose , now , that there were another informative equilibrium . Each message M ∈ M induces a probability of funding D ( M ) . Let the messages be ordered such that D ( M 1 ) ≤ · · · ≤ D ( M K ) 46 where M i are the set of messages M i that induce the same probability of funding D ( M i ) . For reviewers of type E ( Q ∗ | Q ∗ + ε UR ) > U , the reviewer strictly prefers that the grant be funded . She thus ﬁnds it optimal to send the message M K that maximizes the probability that the grant is funded . Call this set Y . For E ( Q ∗ | Q ∗ + ε UR ) < U the reviewer strictly prefer E ( Q ∗ | Q ∗ + ε UR ) = U . This occurs with probability zero . Thus , with probability one , the space of possible messages is equivalent to M = { Y , N } . For this equilibrium to be informative , it must be that D ( N ) < D ( Y ) . Given this , the committee’s optimal reaction is to fund when M = Y and to reject otherwise . Thus , this equilibrium is payoﬀ equivalent to the ﬁrst equilibrium . If the we allow uninformative equilibria , D ( M 1 ) = · · · = D ( M K ) and any reviewer message is permissible . It must be that D ( M i ) = 0 for all M i because the outside option U is assumed to be greater than the committee’s prior on quality . Case 2 . Now consider the case when the reviewer is related and biased . As in Case 1 , the set of messages is equivalent , with probability one , to M = { Y , N } . In this case , however , reviewers of type E ( Q ∗ | Q ∗ + ε R ) > U − B send M = Y and reviewers of type E ( Q ∗ | Q ∗ + ε R ) < U − B send M = N . The only reviewer who sends any other message is one for which E ( Q ∗ | Q ∗ + ε R ) = U − B . E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) > U Under this strategy , the committee’s expectation of Q ∗ given M = N is E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) < U − B ) . Since this is less than U , the grant goes unfunded . The committee’s expectation of Q ∗ given M = Y is E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) < U − B ) . When this is larger than U , the committee listens to the reviewer’s recommendation and we can verify that D ( Y ) > D ( N ) . There also exists an uninformative equilibria where all grants are rejected . When E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) < U − B ) < U , the grant is never funded : D ( Y ) = D ( N ) = 0 . In this case , only babbling equilibria exist . Notice that this formulation does not depend on whether the committee observes B . Case 3 . Whether the committee can observe a reviewer’s relatedness does not actually impact the reviewer’s decision of what message to send . The logic in Cases 1 and 2 still goes through : because reviewers themselves know whether they are related , they are able to form strict preferences on whether an application should be funded , depending on their relatedness and their posterior of an applicant’s quality . As such , related reviewers of type E ( Q ∗ | Q ∗ + ε R ) > U − B send M = Y and related reviewers of type E ( Q ∗ | Q ∗ + ε R ) < U − B send M = N . Similarly , unrelated reviewers of type E ( Q ∗ | Q ∗ + ε UR ) > U send M = Y and unrelated reviewers of type E ( Q ∗ | Q ∗ + ε UR ) < U send M = N . The committee , however , does not observe the reviewer’s relatedness and , as such , forms the following expectation of quality conditional on observing a Y message : K [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε UR ) > U ) ] + ( 1 − K ) [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) ] The ﬁrst term E ( Q ∗ | E ( Q ∗ | Q ∗ + ε UR ) > U ) is the committee’s expectation of quality if it knows that the M = Y message is sent by an unrelated reviewer . Similarly , the second term 47 E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) is the committee’s expectation of quality if it knows that the message is sent by a related reviewer . The term K is the probability that the committee believes a Y message comes from an unrelated reviewer , that is , K = E ( R = 0 | M = Y ) . By Bayes’ Rule , this is given by K = E ( R = 0 | M = Y ) = E ( R = 0 , M = Y ) E ( M = Y ) . The overall probability of a Y message is given by : E ( M = Y ) = p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) + ( 1 − p ) ( E ( Q ∗ | Q ∗ + ε R ) > U − B ) where p is the actual probability that a reviewer is unrelated . Similarly , the probability that the message is Y and the reviewer is unrelated is given by p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) . As such , we have K = p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) p ( E ( Q ∗ | Q ∗ + ε UR ) > U ) + ( 1 − p ) ( E ( Q ∗ | Q ∗ + ε R ) > U − B ) . So if K [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε UR ) > U ) ] + ( 1 − K ) [ E ( Q ∗ | E ( Q ∗ | Q ∗ + ε R ) > U − B ) ] > U , then the committee funds the application and , again , we can verify that D ( Y ) > D ( N ) . There also exists an uninformative equilibria where all grants are rejected . This term is less than U , then the grant is never funded : D ( Y ) = D ( N ) = 0 . In this case , only babbling equilibria exist . 48 B Proof of Proposition 5 . 1 Measurement error in Q ∗ can potentially aﬀect the estimation of α 2 in Equation ( 6 ) . The presence of U , RU , and X , however , will not aﬀect consistency ; for simplicity , I rewrite both the regression suggested by the model and the actual estimating equation with these variables partialed out . The remaining variables should then be thought of as conditional on U , RU , and X D = α 0 + α 1 Q ∗ + α 2 R + α 3 RQ ∗ + ε ( 12 ) D = a 0 + a 1 Q + a 2 R + a 3 RQ + e = a 0 + W + a 2 R + e , W = a 1 Q + a 3 RQ The coeﬃcient a 2 is given by : a 2 = Var ( W ) Cov ( D , R ) − Cov ( W , R ) Cov ( D , W ) Var ( W ) Var ( R ) − Cov ( W , R ) 2 ( 13 ) Consider Cov ( W , R ) : Cov ( W , R ) = Cov ( a 1 ( Q ∗ + v ) + a 3 R ( Q ∗ + v ) , R ) = a 1 Cov ( Q ∗ , R ) + a 1 Cov ( v , R ) + a 3 Cov ( RQ ∗ , R ) + a 3 Cov ( Rv , R ) Under the assumption that R and Q ∗ are conditionally independent , this yields : Cov ( W , R ) = a 3 Cov ( RQ ∗ , R ) + a 3 Cov ( Rv , R ) = a 3 (cid:2) E ( R 2 Q ∗ ) − E ( RQ ∗ ) E ( R ) (cid:3) + a 3 (cid:2) E ( R 2 v ) − E ( Rv ) E ( R ) (cid:3) = a 3 (cid:2) E ( R 2 ) E ( Q ∗ ) − E ( R ) 2 E ( Q ∗ ) (cid:3) + a 3 (cid:2) E ( R 2 ) E ( v ) − E ( R ) 2 E ( v ) (cid:3) = a 3 (cid:2) E ( R 2 ) 0 − E ( R ) 2 0 (cid:3) + a 3 (cid:2) E ( R 2 ) 0 − E ( R ) 2 0 (cid:3) ( 14 ) = 0 ( 15 ) With this simpliﬁcation , the expression for the estimated coeﬃcient on a 2 becomes : a 2 = Var ( W ) Cov ( D , R ) − Cov ( W , R ) Cov ( D , W ) Var ( W ) Var ( R ) − Cov ( W , R ) 2 = Var ( W ) Cov ( D , R ) Var ( W ) Var ( R ) = Cov ( D , R ) Var ( R ) = Cov ( α 0 + α 1 Q ∗ + α 2 R + α 3 RQ ∗ + ε , R ) Var ( R ) = α 2 Var ( R ) + α 3 Cov ( RQ ∗ , R ) Var ( R ) 49 = α 2 Var ( R ) + α 3 (cid:2) E ( R 2 ) E ( Q ∗ ) − E ( R ) 2 E ( Q ∗ ) (cid:3) Var ( R ) = α 2 C Robustness Checks Appendix Table A provides evidence that permanent members do indeed have more inﬂuence . In my sample , I observe almost 5 , 000 reviewers serving both as permanent and as temporary members . For this subset of reviewers , I show that a larger proportion of the applicants whom they have cited are funded when the reviewer is permanent than when the reviewer is temporary , conditional on applicant qualiﬁcations . I also show that mean scores for applicants related to a reviewer are higher when that reviewer is permanent . These regressions include reviewer ﬁxed eﬀects , meaning that an applicant related to the same reviewer is more likely to be funded when that reviewer is permanent as opposed to temporary . The next set of results in this section support the assertion that quality is measured consis - tently . Appendix Table B addresses concerns that funding may directly inﬂuence the number of citations produced by a grant by , for example , freeing up an investigator from future grant writing so that he can concentrate on research . Instead of including articles published after the grant is reviewed , Appendix Table B restricts my analysis to articles published one year before a grant is reviewed . These publications are highly likely to be based oﬀ research that existed before the grant was reviewed , but cannot have been inﬂuenced by the grant funds . Using this metric , I ﬁnd nearly identical measures of bias and information . Another potential concern is that , because I restrict my main quality measure to be based on articles that are closely related to the grant proposal topic , I am potentially missing other research that reviewers might be anticipating when they evaluate a grant proposal . To test whether this is the case , I use grant acknowledgement data recorded in the National Library of Medicine’s PubMed database to match funded grants to all the articles that it produces , regardless of topic or date of publication . For the set of funded grants , Appendix Table C reruns my core regressions using citations to publications that explicitly acknowledge a grant as my measure of quality . This analysis diﬀers slightly from my main results using citations because general citations cannot be computed for publications in PubMed . A limited set of citations can , however , be computed using publications in PubMed Central ( PMC ) . PMC contains a subset of life sciences publications made available for free . While this is not as comprehensive a universe as that of Web of Science , it contains , for recent years , all publications supported by NIH dollars . Undercounting of publications would , further , not bias my result as long as it does not vary systematically by whether an applicant is related to a permanent or to a temporary member . I ﬁnd results that are consistent with my primary ﬁndings , though of a slightly smaller magnitude . Another test of my assumption that citations are not directly aﬀected by funding is to ask whether I ﬁnd bias in the review of inframarginal grants , that is grants that are well above or well below the funding margin . All grants in either group have the same funding status so any bias I 50 ﬁnd cannot be attributed to diﬀerences in funding . Because I hold funding status constant , I can only assess the impact that related permanent members have on an applicant’s score not on an applicant’s funding status . Appendix Table D reports these results . In Columns 3 – 4 and 5 – 6 , I report estimates of the eﬀect of bias and information in the sample of funded and unfunded grants , respectively . In both cases , I still ﬁnd evidence that bias exists . The magnitudes are somewhat smaller than in my main regression ; because these are subsamples , there is no reason to expect that the magnitude of the eﬀect of relationships should be the same for high - and low - quality grants as it is for the entire sample . Appendix Table E adds nonlinearity to Equation ( 10 ) in order to show that my results are robust to the assumption that error on the reviewer’s posteriors in Section 3 is uniform and that E ( Q ∗ | Q ∗ + ε ) is approximated by λ ( Q ∗ + ε ) . Without these assumptions , the association between relatedness and quality would , in general , be nonlinear . To show that this does not make a material diﬀerence for my results , I allow for the eﬀects of quality and relatedness to vary ﬂexibly by including controls for cubics in Q , as well as cubics of Q interacted with whether an applicant is related to a permanent member . I ﬁnd similar results , both qualitatively and quantitatively . My results are robust to non - parametric controls for the total number of related applicants ( meeting by number of related reviewers ﬁxed eﬀects ) and using alternative deﬁnitions of related - ness , including using applicant - reviewer mutual citations and citations deﬁned only on publications for which applicants and reviewers are primary authors ( ﬁrst , second , and last position ) . These and other detailed tables are available from the author . D Estimating Committee Value - Added I estimate committee value - added using the following regression : Decision icmt = a + b cmt Quality icmt + µX icmt + δ cmt + e icmt ( 16 ) D icmt is either the actual or counterfactual funding decision for applicant i reviewed during meeting m of committee c in year t . Q icmt is a measure of application quality such as the number of citations it produces in the future and X icmt are detailed controls for the past performance of the applicant , including ﬂexible controls for number of past publications and citations , number and type of prior awarded grants and prior applications , and ﬂexible controls for degrees , gender , and ethnicity . Finally , δ cmt are committee meeting level ﬁxed eﬀects . The coeﬃcients b cmt capture , for each meeting , the correlation between decisions and quality , conditional on X icmt . Variation in b cmt include sampling error so that ˆ b cmt is a combination of true value - added plus a noise term . I assume this luck term to be independent and normal : ˆ b cmt = b ∗ cmt + ν cmt ( 17 ) Under this assumption , Var ( ˆ b cmt ) = Var ( b ∗ cmt ) + Var ( ν cmt ) so that the estimate of true variance is 51 upwardly biased from the additional variance arising from estimation error . To correct for this , I note that the best estimate for b ∗ cmt is given by E ( b ∗ cmt | ˆ b cmt ) = λ ct ˆ b cmt + ( 1 − λ ct ) ˆ b ct where ˆ b ct is the mean of meeting quality for that committee - year and λ ct = σ 2 b ∗ cmt σ 2 b ∗ cmt + σ 2 νcmt is a Bayesian shrinkage term constructed as the ratio of the estimated variance of true committee eﬀects , σ 2 b ∗ cmt , to the sum of estimated true variance σ 2 b ∗ cmt and estimated noise variance σ 2 ν cmt . To derive this shrinkage term , I use the correlation in meeting quality across the three diﬀerent funding cycles of a committee ﬁscal year . In particular , if meeting - speciﬁc errors are independent , then Cov ( ˆ b cmt , ˆ b cm (cid:48) t ) = Var ( b ∗ cmt ) = ˆ σ 2 b ∗ cmt . This can be estimated at the committee - year level because a committee meets three times during the year . I construct ˆ λ ct = ˆ σ 2 b ∗ cmt ˆ σ 2 b ∗ cmt + ˆ σ 2 ν cmt ( 18 ) so that the adjusted committee value - added is given by : V A cmt = ˆ λ ct ˆ b cmt ( 19 ) Because committee membership is not ﬁxed across funding cycles within the same ﬁscal year ( tem - porary members rotate , permanent members do not ) , variation in V A cmt represents a conservative lower bound on the variance of committee quality . 52 ( 1 ) ( 2 ) Proportion of Related Applicants who are Funded Average Score of Related Applicants Related Reviewer is Permanent 0 . 003 * * * 0 . 336 * * ( 0 . 001 ) ( 0 . 144 ) Observations 15871 15870 R - squared 0 . 954 0 . 571 Reviewer FE X X Past Performance , Past Grants , and Demographics X X Appendix Table A : Do permanent reviewers have more influence ? Notes : This examines how outcomes for related applicants vary by whether the related reviewer is serving in a permanent or temporary capacity . The sample is restricted to 4909 reviewers who are observed both in temporary and permanent positions . An applicant is said to be related by citations if a reviewer has cited that applicant in the 5 years prior to the meeting . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . 53 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Related Permanent Reviewers 0 . 0066 * * * 0 . 0054 * * 0 . 2371 * * 0 . 2009 * * 0 . 0042 * * 0 . 0036 * ( 0 . 002 ) ( 0 . 002 ) ( 0 . 093 ) ( 0 . 094 ) ( 0 . 002 ) ( 0 . 002 ) Related to Permanent Reviewers × Future 0 . 0051 * * 0 . 0800 0 . 0006 ( 0 . 002 ) ( 0 . 095 ) ( 0 . 002 ) Future Citations 0 . 0117 * * * 0 . 4330 * * * 0 . 0144 * * * ( 0 . 002 ) ( 0 . 093 ) ( 0 . 002 ) Total Related Reviewers 0 . 0066 * * * 0 . 0071 * * * 0 . 2105 * * * 0 . 2283 * * * 0 . 0153 * * * 0 . 0157 * * * ( 0 . 001 ) ( 0 . 001 ) ( 0 . 060 ) ( 0 . 060 ) ( 0 . 001 ) ( 0 . 001 ) Observations 93 , 558 93 , 558 57 , 613 57 , 613 93 , 558 93 , 558 R - squared 0 . 0909 0 . 0945 0 . 1392 0 . 1412 0 . 1243 0 . 1263 Committee × Year × Cycle FE X X X X X X Past Performance , Past Grants , and Demographics X X X X X X Mean = . 640 , SD = . 480 1 ( Scored at all ) Notes : Coefficients are reported from a regression of committee decisions ( score or funding status ) on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . Columns ( 1 ) ( 3 ) and ( 5 ) 1 are reproduced from Table 6 . Columns 2 and 4 add controls for application quality and application quality interacted with relatedness to permanent reviewers . The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are calculated using all publications by an applicant in the year before grant review to the year of grant review , with text matching . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . Appendix Table B : What is the contribution of expertise vs . bias ? Grant quality measured from articles published before grant review 1 ( Score is above the payline ) Score Mean = 0 . 214 , SD = 0 . 410 Mean = 71 . 18 , SD = 18 . 75 54 ( 1 ) ( 2 ) Dep var : Score Mean = 71 . 18 , SD = 18 . 75 Related Permanent Reviewers 0 . 1384 * 0 . 1285 * ( 0 . 0724 ) ( 0 . 0734 ) Related to Permanent Reviewers × Future Citations 0 . 0749 ( 0 . 1004 ) Future Citations 0 . 4806 * * * ( 0 . 0770 ) Total Related Reviewers - 0 . 0074 0 . 0086 ( 0 . 0456 ) ( 0 . 0472 ) Observations 24395 24395 R - squared 0 . 1743 0 . 1793 Committee × Year × Cycle FE X X Past Performance , Past Grants , and Demographics X X Explict Grant Acknowledgements Notes : Coefficients are reported from a regression of committee decisions ( score or funding status ) on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . The analytic sample includes all awarded R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are calculated explicit grant acknowlegments . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . Appendix Table C : What is the contribution of bias and information ? Explicit grant acknowledgements for the sample of funded grants 55 ( 1 ) ( 2 ) ( 3 ) ( 1 ) ( 2 ) ( 3 ) Dep var : Score Mean = 71 . 18 , SD = 18 . 75 All Funded Not Funded All Funded Not Funded Related Permanent Reviewers 0 . 2371 * * 0 . 1946 * * 0 . 1348 * 0 . 0942 0 . 1694 * 0 . 1383 ( 0 . 093 ) ( 0 . 094 ) ( 0 . 073 ) ( 0 . 073 ) ( 0 . 089 ) ( 0 . 091 ) Related to Permanent Reviewers × Future 0 . 0741 0 . 1064 * * 0 . 0520 ( 0 . 059 ) ( 0 . 051 ) ( 0 . 068 ) Future Citations 0 . 2356 * * * - 0 . 0590 0 . 1523 * * ( 0 . 062 ) ( 0 . 045 ) ( 0 . 063 ) Total Related Reviewers 0 . 2105 * * * 0 . 2282 * * * - 0 . 0040 0 . 0043 0 . 1387 * * 0 . 1435 * * ( 0 . 060 ) ( 0 . 060 ) ( 0 . 046 ) ( 0 . 046 ) ( 0 . 058 ) ( 0 . 058 ) Observations 57 , 613 57 , 613 24 , 395 24 , 395 33 , 218 33 , 218 R - squared 0 . 1392 0 . 1405 0 . 1728 0 . 1731 0 . 1857 0 . 1866 Committee × Year × Cycle FE X X X X X X Past Performance , Past Grants , and Demographics X X X X X X Appendix Table D : What is the contribution of expertise vs . bias ? Inframarginal grant applications Notes : Coefficients are reported from a regression of score on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are standardized to be mean zero , standard deviation 1 within each committee - year . Future citations are calculated using all publications by an applicant in the year after grant review , with text matching . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . 56 ( 1 ) ( 2 ) ( 3 ) Awarded Score Scored Related Permanent Reviewers 0 . 0036 * 0 . 1460 0 . 0026 ( 0 . 002 ) ( 0 . 095 ) ( 0 . 002 ) Related to Permanent Reviewers × Future Citations 0 . 0094 * * * 0 . 2350 * * - 0 . 0023 ( 0 . 002 ) ( 0 . 109 ) ( 0 . 003 ) Related to Permanent Reviewers × Future Citations ^ 2 - 0 . 0002 - 0 . 0043 0 . 0009 * * * ( 0 . 000 ) ( 0 . 008 ) ( 0 . 000 ) Related to Permanent Reviewers × Future Citations ^ 3 0 . 0000 0 . 0000 - 0 . 0000 * * * ( 0 . 000 ) ( 0 . 000 ) ( 0 . 000 ) Future Citations 0 . 0136 * * * 0 . 5356 * * * 0 . 0281 * * * ( 0 . 002 ) ( 0 . 111 ) ( 0 . 003 ) Future Citations ^ 2 - 0 . 0003 - 0 . 0138 - 0 . 0015 * * * ( 0 . 000 ) ( 0 . 009 ) ( 0 . 000 ) Future Citations ^ 3 0 . 0000 0 . 0001 0 . 0000 * * * ( 0 . 000 ) ( 0 . 000 ) ( 0 . 000 ) Total Related Reviewers 0 . 0071 * * * 0 . 2285 * * * 0 . 0157 * * * ( 0 . 001 ) ( 0 . 059 ) ( 0 . 001 ) Observations 93 , 558 57 , 613 93 , 558 R - squared 0 . 0953 0 . 1418 0 . 1282 Committee × Year × Cycle FE X X X Past Performance , Past Grants , and Demographics X X X Appendix Table E : What is the contribution of bias and information ? Nonlinear controls for quality and relatedness Notes : Coefficients are reported from a regression of committee decisions ( score or funding status ) on the variables reported , controlling for meeting level fixed effects and detailed applicant characteristics . The analytic sample includes new or competing R01 grants evaluated in charterd study sections from 1992 to 2005 , for which I have study section attendance data . A reviewer is related to an applicant if the reviewer has cited any of the applicant ' s previous research in the 5 years prior to grant review . Future citations are calculated using all publications by an applicant in the year after grant review , with text matching . Applicant characteristics include indicators for sex and whether an applicant ' s name is Hispanic , East Asian , or South Asian , quartics in an applicant ' s total number of citations and publications over the past 5 years , indicators for whether an applicant has an M . D . and / or a Ph . D . , and indicators for the number of past R01 and other NIH grants an applicant has won and indicators for how many she has applied to . 57