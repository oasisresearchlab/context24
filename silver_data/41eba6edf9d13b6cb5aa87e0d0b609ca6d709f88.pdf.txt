Efﬁcient Minimization of Decomposable Submodular Functions Peter Stobbe California Institute of Technology Pasadena , CA 91125 stobbe @ caltech . edu Andreas Krause California Institute of Technology Pasadena , CA 91125 krausea @ caltech . edu Abstract Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function . Submodular functions are a natural discrete analog of convex functions , and can be minimized in strongly polynomial time . Unfortunately , state - of - the - art algorithms for general submodular minimization are intractable for larger problems . In this paper , we introduce a novel subclass of submodular minimization problems that we call decomposable . Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions . We develop an algorithm , SLG , that can efﬁciently minimize decomposable submodular functions with tens of thousands of variables . Our algorithm exploits recent results in smoothed convex minimization . We apply SLG to synthetic benchmarks and a joint classiﬁcation - and - segmentation task , and show that it outperforms the state - of - the - art general purpose submodular minimization algorithms by several orders of magnitude . 1 Introduction Convex optimization has become a key tool in many machine learning algorithms . Many seemingly multimodal optimization problems such as nonlinear classiﬁcation , clustering and dimensionality reduction can be cast as convex programs . When minimizing a convex loss function , we can rest assured to efﬁciently ﬁnd an optimal solution , even for large problems . Convex optimization is a structural property of continuous optimization problems . However , many machine learning prob - lems , such as structure learning , variable selection , MAP inference in discrete graphical models , require solving discrete , combinatorial optimization problems . In recent years , another fundamental problem structure , which has similar beneﬁcial properties , has emerged as very useful in many combinatorial optimization problems arising in machine learn - ing : Submodularity is an intuitive diminishing returns property , stating that adding an element to a smaller set helps more than adding it to a larger set . Similarly to convexity , submodularity allows one to efﬁciently ﬁnd provably ( near - ) optimal solutions . In particular , the minimum of a submodular function can be found in strongly polynomial time [ 11 ] . Unfortunately , while polynomial - time solv - able , exact techniques for submodular minimization require a number of function evaluations on the order of n 5 [ 12 ] , where n is the number of variables in the problem ( e . g . , number of random variables in the MAP inference task ) , rendering the algorithms impractical for many real - world problems . Fortunately , several submodular minimization problems arising in machine learning have structure that allows solving them more efﬁciently . Examples include symmetric functions that can be solved in O ( n 3 ) evaluations using Queyranne’s algorithm [ 19 ] , and functions that decompose into attractive , pairwise potentials , that can be solved using graph cutting techniques [ 7 ] . In this paper , we introduce a novel class of submodular minimization problems that can be solved efﬁciently . In particular , we develop an algorithm SLG , that can minimize a class of submodular functions that we call decomposable : These are functions that can be decomposed into sums of concave functions applied to modular ( additive ) functions . Our algorithm is based on recent techniques of smoothed convex minimization [ 18 ] applied to the Lov´asz extension . We demonstrate the usefulness of 1 our algorithm on a joint classiﬁcation - and - segmentation task involving tens of thousands of variables , and show that it outperforms state - of - the - art algorithms for general submodular function minimization by several orders of magnitude . 2 Background on Submodular Function Minimization We are interested in minimizing set functions that map subsets of some base set E to real numbers . I . e . , given f : 2 E ! R we wish to solve for A (cid:3) 2 arg min A f ( A ) . For simplicity of notation , we use the base set E = f1 ; : : : ng , but in an application the base set may consist of nodes of a graph , pixels of an image , etc . Without loss of generality , we assume f ( ; ) = 0 . If the function f has no structure , then there is no way solve the problem other than checking all 2 n subsets . In this paper , we consider functions that satisfy a key property that arises in many applications : submodularity ( c . f . , [ 16 ] ) . A set function f is called submodular iff , for all A ; B 2 2 E , we have f ( A [ B ) + f ( A \ B ) (cid:20) f ( A ) + f ( B ) : ( 1 ) Submodular functions can alternatively , and perhaps more intuitively , be characterized in terms of their discrete derivatives . First , we deﬁne (cid:1) k f ( A ) = f ( A [ fkg ) (cid:0)f ( A ) to be the discrete derivative of f with respect to k 2 E at A ; intuitively this is the change in f ’s value by adding the element k to the set A . Then , f is submodular iff : (cid:1) k f ( A ) (cid:21) (cid:1) k f ( B ) ; for all A (cid:18) B (cid:18) E and k 2 E n B : Note the analogy to concave functions ; the discrete derivative is smaller for larger sets , in the same way that (cid:30) ( x + h ) (cid:0)(cid:30) ( x ) (cid:21) (cid:30) ( y + h ) (cid:0)(cid:30) ( y ) for all x (cid:20) y ; h (cid:21) 0 if and only if (cid:30) is a concave function on R . Thus a simple example of a submodular function is f ( A ) = (cid:30) ( jAj ) where (cid:30) is any concave function . Yet despite this connection to concavity , it is in fact ‘easier’ to minimize a submodular function than to maximize it 1 , just as it is easier to minimize a convex function . One explanation for this is that submodular minimization can be reformulated as a convex minimization problem . To see this , consider taking a set function minimization problem , and reformulating it as a mini - mization problem over the unit cube [ 0 ; 1 ] n (cid:26) R n . Deﬁne e A 2 R n to be the indicator vector of the set A , i . e . , e A [ k ] = (cid:26) 0 if k = 2 A 1 if k 2 A We use the notation x [ k ] for the k th element of the vector x . Also we drop brackets and commas in subscripts , so e kl = e fk ; lg and e k = e fkg as with the standard unit vectors . A continuous extension of a set function f is a function ~ f on the unit cube ~ f : [ 0 ; 1 ] n ! R with the property that f ( A ) = ~ f ( e A ) . In order to be useful , however , one needs the minima of the set function to be related to minima of the extension : A (cid:3) 2 arg min A22 E f ( A ) ) e A (cid:3) 2 arg min x2 [ 0 ; 1 ] n ~ f ( x ) : ( 2 ) A key result due to Lov´asz [ 16 ] states that each submodular function f has an extension ~ f that not only satisﬁes the above property , but is also convex and efﬁcient to evaluate . We can deﬁne the Lov´asz extension in terms of the submodular polyhedron P f : P f = fv 2 R n : v (cid:1) e A (cid:20) f ( A ) ; for all A 2 2 E g ; ~ f ( x ) = sup v2P f v (cid:1) x : The submodular polyhedron P f is deﬁned by exponentially many inequalities , and evaluating ~ f requires solving a linear program over this polyhedron . Perhaps surprisingly , as shown by Lov´asz , ~ f can be very efﬁciently computed as follows . For a ﬁxed x let (cid:27) : E ! E be a permutation such that x [ (cid:27) ( 1 ) ] (cid:21) : : : (cid:21) x [ (cid:27) ( n ) ] , and then deﬁne the set S k = f(cid:27) ( 1 ) ; : : : ; (cid:27) ( k ) g . Then we have a formula for ~ f and a subgradient : ~ f ( x ) = n X k = 1 x [ (cid:27) ( k ) ] ( f ( S k ) (cid:0) f ( S k(cid:0)1 ) ) ; @ ~ f ( x ) 3 n X k = 1 e (cid:27) ( k ) ( f ( S k ) (cid:0) f ( S k(cid:0)1 ) ) : Note that if two components of x are equal , the above formula for ~ f is independent of the permuta - tion chosen , but the subgradient is not unique . 1 With the additional assumption that f is nondecreasing , maximizing a submodular function subject to a cardinality constraint j A j (cid:20) M is ‘easy’ ; a greedy algorithm is known to give a near - optimal answer [ 17 ] . 2 Equation ( 2 ) was used to show that submodular minimization can be achieved in polynomial time [ 16 ] . However , algorithms which directly minimize the Lovasz extension are regarded as imprac - tical . Despite being convex , the Lov´asz extension is non - smooth , and hence a simple subgradient descent algorithm would need O ( 1 = (cid:15) 2 ) steps to achieve O ( (cid:15) ) accuracy . Recently , Nesterov showed that if knowledge about the structure of a particular non - smooth convex function is available , it can be exploited to achieve a running time of O ( 1 = (cid:15) ) [ 18 ] . One way this is done is to construct a smooth approximation of the non - smooth function , and then use an accelerated gradient descent algorithm which is highly effective for smooth functions . Connections of this work with submodularity and combinatorial optimization are also explored in [ 4 ] and [ 2 ] . In fact , in [ 2 ] , Bach shows that computing the smoothed Lov´asz gradient of a general submodular function is equivalent to solving a submodular minimization problem . In this paper , we do not treat general submodular functions , but rather a large class of submodular minimization functions that we call decomposable . ( To apply the smoothing technique of [ 18 ] , special structural knowledge about the convex function is required , so it is natural that we would need special structural knowledge about the submodular function to leverage those results . ) We further show that we can exploit the discrete structure of submodular minimization in a way that allows terminating the algorithm early with a certiﬁcate of optimality , which leads to drastic performance improvements . 3 The Decomposable Submodular Minimization Problem In this paper , we consider the problem of minimizing functions of the following form : f ( A ) = c (cid:1) e A + X j (cid:30) j ( w j (cid:1) e A ) ; ( 3 ) where c ; w j 2 R n and 0 (cid:20) w j (cid:20) 1 and (cid:30) j : [ 0 ; w j (cid:1) 1 ] ! R are arbitrary concave functions . It can be shown that functions of this form are submodular . We call this class of functions decomposable submodular functions , as they decompose into a sum of concave functions applied to nonnegative modular functions 2 . Below , we give examples of decomposable submodular functions arising in applications . We ﬁrst focus on the special case where all the concave functions are of the form (cid:30) j ( (cid:1) ) = d j min ( y j ; (cid:1) ) for some y j ; d j > 0 . Since these potentials are of key importance , we deﬁne the submodular functions (cid:9) w ; y ( A ) = min ( y ; w (cid:1) e A ) and call them threshold potentials . In Section 5 , we will show in how to generalize our approach to arbitrary decomposable submodular functions . Examples . The simplest example is a 2 - potential , which has the form (cid:30) ( jA \ fk ; lgj ) , where (cid:30) ( 1 ) (cid:0) (cid:30) ( 0 ) (cid:21) (cid:30) ( 1 ) (cid:0) (cid:30) ( 2 ) . It can be expressed as a sum of a modular function and a threshold potential : (cid:30) ( jA \ fk ; lgj ) = (cid:30) ( 0 ) + ( (cid:30) ( 2 ) (cid:0) (cid:30) ( 1 ) ) e kl (cid:1) e A + ( 2(cid:30) ( 1 ) (cid:0) (cid:30) ( 0 ) (cid:0) (cid:30) ( 2 ) ) (cid:9) e kl ; 1 ( A ) Why are such potential functions interesting ? They arise , for example , when ﬁnding the Maximum a Posteriori conﬁguration of a pairwise Markov Random Field model in image classiﬁcation schemes such as in [ 20 ] . On a high level , such an algorithm computes a value c [ k ] that corresponds to the log - likelihood of pixel k being of one class vs . another , and for each pair of adjacent pixels , a value d kl related to the log - likelihood that pixels k and l are of the same class . Then the algorithm classiﬁes pixels by minimizing a sum of 2 - potentials : f ( A ) = c (cid:1) e A + P k ; l d kl ( 1 (cid:0) j1 (cid:0) e kl (cid:1) e A j ) . If the value d kl is large , this encourages the pixels k and l to be classiﬁed similarly . More generally , consider a higher order potential function : a concave function of the number of elements in some activation set S , (cid:30) ( jA \ Sj ) where (cid:30) is concave . It can be shown that this can be written as a sum of a modular function and a positive linear combination of jSj (cid:0) 1 threshold potentials . Recent work [ 14 ] has shown that classiﬁcation performance can be improved by adding terms corresponding to such higher order potentials (cid:30) j ( jR j \ Aj ) to the objective function where the functions (cid:30) j are piecewise linear concave functions , and the regions R j of various sizes generated from a segmentation algorithm . Minimization of these particular potential functions can then be reformulated as a graph cut problem [ 13 ] , but this is less general than our approach . Another canonical example of a submodular function is a set cover function . Such a function can be reformulated as a combination of concave cardinality functions ( details omitted here ) . So all 2 A function is called modular if ( 1 ) holds with equality . It can be written as A 7 ! w(cid:1)e A for some w 2 R n . 3 functions which are weighted combinations of set cover functions can be expressed as threshold potentials . However , threshold potentials with nonuniform weights are strictly more general than concave cardinality potentials . That is , there exists w and y such that (cid:9) w ; y ( A ) cannot be expressed as P j (cid:30) j ( jR j \ Aj ) for any collection of concave (cid:30) j and sets R j . Another example of decomposable functions arises in multiclass queuing systems [ 10 ] . These are of the form f ( A ) = c (cid:1) e A + u (cid:1) e A (cid:30) ( v (cid:1) e A ) , where u ; v are nonnegative weight vectors and (cid:30) is a nonincreasing concave function . With the proper choice of (cid:30) j and w j ( again details are omitted here ) , this can in fact be reformulated as sum of the type in Eq . 3 with n terms . In our own experiments , shown in Section 6 , we use an implementation of TextonBoost [ 20 ] and augment it with quadratic higher order potentials . That is , we use TextonBoost to generate per - pixel scores c , and then minimize f ( A ) = c (cid:1) e A + P j jA \ R j jjR j nAj , where the regions R j are regions of pixels that we expect to be of the same class ( e . g . , by running a cheap region - growing heuristic ) . The potential function jA \ R j jjR j nAj is smallest when A contains all of R j or none of it . It gives the largest penalty when exactly half of R j is contained in A . This encourages the classiﬁcation scheme to classify most of the pixels in a region R j the same way . We generate regions with a basic region - growing algorithm with random seeds . See Figure 1 ( a ) for an illustration of examples of regions that we use . In our experience , this simple idea of using higher - order potentials can dramatically increase the quality of the classiﬁcation over one using only 2 - potentials , as can be seen in Figure 2 . 4 The SLG Algorithm for Threshold Potentials We now present our algorithm for efﬁcient minimization of a decomposable submodular function f based on smoothed convex minimization . We ﬁrst show how we can efﬁciently smooth the Lov ´ asz extension of f . We then apply accelerated gradient descent to the gradient of the smoothed function . Lastly , we demonstrate how we can often obtain a certiﬁcate of optimality that allows us to stop early , drastically speeding up the algorithm in practice . 4 . 1 The Smoothed Extension of a Threshold Potential The key challenge in our algorithm is to efﬁciently smooth the Lov ´ asz extension of f , so that we can resort to algorithms for accelerated convex minimization . We now show how we can efﬁciently smooth the threshold potentials (cid:9) w ; y ( A ) = min ( y ; w (cid:1) e A ) of Section 3 , which are simple enough to allow efﬁcient smoothing , but rich enough when combined to express a large class of submodular functions . For x (cid:21) 0 , the Lov´asz extension of (cid:9) w ; y is ~ (cid:9) w ; y ( x ) = sup v (cid:1) x s . t . v (cid:20) w ; v (cid:1) e A (cid:20) y for all A 2 2 E : Note that when x (cid:21) 0 , the arg max of the above linear program always contains a point v which satisﬁes v (cid:1) 1 = y , and v (cid:21) 0 . So we can restrict the domain of the dual variable v to those points which satisfy these two conditions , without changing the value of ~ (cid:9) ( x ) : ~ (cid:9) w ; y ( x ) = max v2D ( w ; y ) v (cid:1) x where D ( w ; y ) = fv : 0 (cid:20) v (cid:20) w ; v (cid:1) 1 = yg : Restricting the domain of v allows us to deﬁne a smoothed Lov ´ asz extension ( with parameter (cid:22) ) that is easily computed : ~ (cid:9) (cid:22)w ; y ( x ) = max v2D ( w ; y ) v (cid:1) x (cid:0) (cid:22) 2 kvk 2 To compute the value of this function we need to solve for the optimal vector v (cid:3) , which is also the gradient of this function , as we have the following characterization : r ~ (cid:9) (cid:22)w ; y ( x ) = arg max v2D ( w ; y ) v (cid:1) x (cid:0) (cid:22) 2 kvk 2 = arg min v2D ( w ; y ) (cid:13)(cid:13)(cid:13)(cid:13) x (cid:22) (cid:0) v (cid:13)(cid:13)(cid:13)(cid:13) : ( 4 ) To derive an expression for v (cid:3) , we begin by forming the Lagrangian and deriving the dual problem : ~ (cid:9) (cid:22)w ; y ( x ) = min t2R ; (cid:21) 1 ; (cid:21) 2 (cid:21)0 (cid:18) max v2R n v (cid:1) x (cid:0) (cid:22) 2 kvk 2 + (cid:21) 1 (cid:1) v + (cid:21) 2 (cid:1) ( w (cid:0) v ) + t ( y (cid:0) v (cid:1) 1 ) (cid:19) = min t2R ; (cid:21) 1 ; (cid:21) 2 (cid:21)0 1 2(cid:22)kx (cid:0) t 1 + (cid:21) 1 (cid:0) (cid:21) 2 k 2 + (cid:21) 2 (cid:1) w + ty : If we ﬁx t , we can solve for the optimal dual variables (cid:21) (cid:3)1 and (cid:21) (cid:3)2 componentwise . By strong duality , we know the optimal primal variable is given by v (cid:3) = 1(cid:22) ( x (cid:0) t (cid:3) 1 + (cid:21) (cid:3)1 (cid:0) (cid:21) (cid:3)2 ) . So we have : (cid:21) (cid:3)1 = max ( t (cid:3) 1 (cid:0) x ; 0 ) ; (cid:21) (cid:3)2 = max ( x (cid:0) t (cid:3) 1 (cid:0) (cid:22)w ; 0 ) ) v (cid:3) = min ( max ( ( x (cid:0) t (cid:3) 1 ) = (cid:22) ; 0 ) ; w ) : 4 This expresses v (cid:3) as a function of the unknown optimal dual variable t (cid:3) . For the simple case of 2 - potentials , we can solve for t (cid:3) explicitly and get a closed form expression : r ~ (cid:9) (cid:22)e kl ; 1 ( x ) = 8 > < > : e k if x [ k ] (cid:21) x [ l ] + (cid:22) e l if x [ l ] (cid:21) x [ k ] + (cid:22) 12 ( e kl + 1(cid:22) ( x [ k ] (cid:0) x [ l ] ) ( e k (cid:0) e l ) ) if j x [ k ] (cid:0) x [ l ] j < (cid:22) However , in general to ﬁnd t (cid:3) we note that v (cid:3) must satisfy v (cid:3) (cid:1) 1 = y . So deﬁne (cid:26) (cid:22)x ; w ( t ) as : (cid:26) (cid:22)x ; w ( t ) = min ( max ( ( x (cid:0) t 1 ) = (cid:22) ; 0 ) ; w ) (cid:1) 1 Then we note this function is a monotonic continuous piecewise linear function of t , so we can use a simple root - ﬁnding algorithm to solve (cid:26) (cid:22)x ; w ( t (cid:3) ) = y . This root ﬁnding procedure will take no more than O ( n ) steps in the worst case . 4 . 2 The SLG Algorithm for Minimizing Sums of Threshold Potentials Stepping beyond a single threshold potential , we now assume that the submodular function to be minimized can be written as a nonnegative linear combination of threshold potentials and a modular function , i . e . , f ( A ) = c (cid:1) e A + X j d j (cid:9) w j ; y j ( A ) : Thus , we have the smoothed Lov ´ asz extension , and its gradient : ~ f (cid:22) ( x ) = c (cid:1) x + X j d j ~ (cid:9) (cid:22)w j ; y j ( x ) and r ~ f (cid:22) ( x ) = c + X j d j r ~ (cid:9) (cid:22)w j ; y j ( x ) : We now wish to use the accelerated gradient descent algorithm of [ 18 ] to minimize this function . This algorithm requires that the smoothed objective has a Lipschitz continuous gradient . That is , for some constant L , it must hold that kr ~ f (cid:22) ( x 1 ) (cid:0) r ~ f (cid:22) ( x 2 ) k (cid:20) Lkx 1 (cid:0) x 2 k ; for all x 1 ; x 2 2 R n . Fortunately , by construction , the smoothed threshold extensions ~ (cid:9) (cid:22)w j ; y j ( x ) all have 1 = (cid:22) Lip - schitz gradient , a direct consequence of the characterization in Equation 4 . Hence we have a loose upper bound for the Lipschitz constant of ~ f (cid:22) : L (cid:20) D(cid:22) , where D = P j d j . Fur - thermore , the smoothed threshold extensions approximate the threshold extensions uniformly : j ~ (cid:9) (cid:22)w j ; y j ( x ) (cid:0) ~ (cid:9) w j ; y j ( x ) j (cid:20) (cid:22)2 for all x , so j ~ f (cid:22) ( x ) (cid:0) ~ f ( x ) j (cid:20) (cid:22)D2 . One way to use the smoothed gradient is to specify an accuracy " , then minimize ~ f (cid:22) for sufﬁciently small (cid:22) to guarantee that the solution will also be an approximate minimizer of ~ f . Then we simply apply the accelerated gradient descent algorithm of [ 18 ] . See also [ 3 ] for a description . Let P C ( x ) = arg min x 0 2C kx (cid:0) x 0 k be the projection of x onto the convex set C . In particular , P [ 0 ; 1 ] n ( x ) = min ( max ( x ; 0 ) ; 1 ) . Algorithm 1 formalizes our Smoothed Lov´asz Gradient ( SLG ) algorithm : Algorithm 1 : SLG : Smoothed Lov ´ asz Gradient Input : Accuracy " ; decomposable function f . begin (cid:22) = " 2D , L = D(cid:22) , x (cid:0)1 = z (cid:0)1 = 12 1 ; for t = 0 ; 1 ; 2 ; : : : do g t = r ~ f (cid:22) ( x t(cid:0)1 ) = L ; z t = P [ 0 ; 1 ] n (cid:16) z (cid:0)1 (cid:0) P ts = 0 (cid:0) s + 12 (cid:1) g s (cid:17) ; y t = P [ 0 ; 1 ] n ( x t (cid:0) g t ) ; if gap t (cid:20) " = 2 then stop ; x t = ( 2z t + ( t + 1 ) y t ) = ( t + 3 ) ; x " = y t ; Output : " - optimal x " to min x2 [ 0 ; 1 ] n ~ f ( x ) The optimality gap of a smooth convex function at the iterate y t can be computed from its gradient : gap t = max x2 [ 0 ; 1 ] n ( y t (cid:0) x ) (cid:1) r ~ f (cid:22) ( y t ) = y t (cid:1) r ~ f (cid:22) ( y t ) + max ( (cid:0)r ~ f (cid:22) ( y t ) ; 0 ) (cid:1) 1 : In summary , as a consequence of the results of [ 18 ] , we have the following guarantee about SLG : Theorem 1 SLG is guaranteed to provide an " - optimal solution after running for O ( D " ) iterations . 5 SLG is only guaranteed to provide an " - optimal solution to the continuous optimization problem . Fortunately , once we have an " - optimal point for the Lov´asz extension , we can efﬁciently round it to set which is " - optimal for the original submodular function using Alg . 2 ( see [ 9 ] for more details ) . Algorithm 2 : Set generation by rounding the continuous solution Input : Vector x 2 [ 0 ; 1 ] n ; submodular function f . begin By sorting , ﬁnd any permutation (cid:27) satisfying : x [ (cid:27) ( 1 ) ] (cid:21) : : : (cid:21) x [ (cid:27) ( n ) ] ; S k = f(cid:27) ( 1 ) ; : : : ; (cid:27) ( k ) g ; K (cid:3) = arg min k2f0 ; 1 ; : : : ; ng f ( S k ) ; C = fS k : k 2 K (cid:3) g ; Output : Collection of sets C , such that f ( A ) (cid:20) ~ f ( x ) for all A 2 C 4 . 3 Early Stopping based on Discrete Certiﬁcates of Optimality In general , if the minimum of f is not unique , the output of SLG may be in the interior of the unit cube . However , if f admits a unique minimum A (cid:3) , then the iterates will tend toward the corner e A (cid:3) . One natural question one may ask , if a trend like this is observed , is it necessary to wait for the iterates to converge all the way to the optimal solution of the continuous problem min x2 [ 0 ; 1 ] n ~ f ( x ) , when one is actually iterested in solving the discrete problem min A 22 E f ( A ) ? Below , we show that it is possible to use information about the current iterates to check optimality of a set and terminate the algorithm before the continuous problem has converged . To prove optimality of a candidate set A , we can use a subgradient of ~ f at e A . If g 2 @ ~ f ( e A ) , then we can compute an optimality gap : f ( A ) (cid:0) f (cid:3) (cid:20) max x2 [ 0 ; 1 ] n ( e A (cid:0) x ) (cid:1) g = X k2A max ( 0 ; g [ k ] ( e A [ k ] (cid:0) e EnA [ k ] ) ) : ( 5 ) In particular if g [ k ] (cid:20) 0 for k 2 A and g [ k ] (cid:21) 0 for k 2 E n A , then A is optimal . But if we only have knowledge of candidate set A , then ﬁnding a subgradient g 2 @ ~ f ( e A ) which demonstrates optimality may be extremely difﬁcult , as the set of subgradients is a polyhedron with exponentially many extreme points . But our algorithm naturally suggests the subgradient we could use ; the gradi - ent of the smoothed extension is one such subgradient – provided a certain condition is satisﬁed , as described in the following Lemma . Lemma 1 Suppose f is a decomposable submodular function , with Lov´asz extension ~ f , and smoothed extension ~ f (cid:22) as in the previous section . Suppose x 2 R n and A 2 2 E satisfy the fol - lowing property : min k2A ; l2EnA x [ k ] (cid:0) x [ l ] (cid:21) 2(cid:22) Then r ~ f (cid:22) ( x ) 2 @ ~ f ( e A ) This is a consequence of our formula for r ~ (cid:9) (cid:22) , but see the appendix of the extended paper [ 21 ] for a detailed proof . Lemma 1 states that if the components of point x corresponding to elements of A are all larger than all the other components by at least 2(cid:22) , then the gradient at x is a subgradient for ~ f at e A ( which by Equation 5 allows us to compute an optimality gap ) . In practice , this separation of components naturally occurs as the iterates move in the direction of the point e A , long before they ever actually reach the point e A . But even if the components are not separated , we can easily add a positive multiple of e A to separate them and then compute the gradient there to get an optimality gap . In summary , we have the following algorithm to check the optimality of a candidate set : Of critical importance is how to choose the candidate set A . But by Equation 5 , for a set to be Algorithm 3 : Set Optimality Check Input : Set A ; decomposable function f ; scale (cid:22) ; x 2 R n . begin (cid:13) = 2(cid:22) + max k2A ; l2EnA x [ l ] (cid:0) x [ k ] ; g = r ~ f (cid:22) ( x + (cid:13)e A ) ; gap = P k2A max ( 0 ; g [ k ] ( e A [ k ] (cid:0) e EnA [ k ] ) ) ; Output : gap , which satisﬁes gap (cid:21) f ( A ) (cid:0) f (cid:3) optimal , we want the components of the gradient r ~ f (cid:22) ( A + (cid:13) e A ) [ k ] to be negative for k 2 A and positive for k 2 E n A . So it is natural to choose A = fk : r ~ f (cid:22) ( x ) [ k ] (cid:20) 0g . Thus , if adding (cid:13) e A does not change the signs of the components of the gradient , then in fact we have found the optimal set . This stopping criterion is very effective in practice , and we use it in all of our experiments . 6 R 3 R 1 R 2 ( a ) Example Regions for Potentials 10 2 10 3 10 0 10 2 Problem Size ( n ) R unn i ng T i m e ( s ) SLG SFM3 MinNorm HYBRID PR LEX2 ( b ) Results for genrmf - long 10 2 10 3 10 0 10 2 Problem Size ( n ) R unn i ng T i m e ( s ) SLG SFM3 MinNorm LEX2 HYBRID PR ( c ) Results genrmf - wide Figure 1 : ( a ) Example regions used for our higher - order potential functions ( b - c ) Comparision of running times of submodular minimization algorithms on synthetic problems from DIMACS [ 1 ] . 5 Extension to General Concave Potentials To extend our algorithm to work on general concave functions , we note that an arbitrary concave function can be expressed as an integral of threshold potential functions . This is a simple conse - quence of integration by parts , which we state in the following lemma : Lemma 2 For (cid:30) 2 C 2 ( [ 0 ; T ] ) , (cid:30) ( x ) = (cid:30) ( 0 ) + (cid:30) 0 ( T ) x (cid:0) Z T 0 min ( x ; y ) (cid:30) 00 ( y ) dy ; 8x 2 [ 0 ; T ] This means that for a general sum of concave potentials as in Equation ( 3 ) , we have : f ( A ) = c (cid:1) e A + X j (cid:18)(cid:30) j ( 0 ) + (cid:30) 0 ( w j (cid:1) 1 ) w j (cid:1) e A (cid:0) Z w j (cid:1)1 0 (cid:9) w j ; y ( A ) (cid:30) 00j ( y ) dy(cid:19) : Then we can deﬁne ~ f and ~ f (cid:22) by replacing (cid:9) with ~ (cid:9) and ~ (cid:9) (cid:22) respectively . Our SLG algorithm is essentially unchanged , the conditions for optimality still hold , and so on . Conceptually , we just use a different smoothed gradient , but calculating it is more involved . We need to compute the integrals of the form R r ~ (cid:9) (cid:22)w ; y ( x ) (cid:30) 00 ( y ) dy . Since r ~ (cid:9) (cid:22)w ; y ( x ) is a piecewise linear function with repect to y which we can compute , we can evaluate the integral by parts so that we need only evaluate (cid:30) , but not its derivatives . We omit the resulting formulas for space limitations . 6 Experiments Synthetic Data . We reproduce the experimental setup of [ 8 ] designed to compare submodular minimization algorithms . Our goal is to ﬁnd the minimum cut of a randomly generated graph ( which requires submodular minimization of a sum of 2 - potentials ) with the graph generated by the speci - ﬁcations in [ 1 ] . We compare against the state of the art combinatorial algorithms ( LEX2 , HYBRID , SFM3 , PR [ 6 ] ) that are guaranteed to ﬁnd the exact solution in polynomial time , as well as the Minimum Norm algorithm of [ 8 ] , a practical alternative with unknown running time . Figures 1 ( b ) and 1 ( c ) compare the running time of SLG against the running times reported in [ 8 ] . In some cases , SLG was 6 times faster than the MinNorm algorithm . However the comparison to the MinNorm algorithm is inconclusive in this experiment , since while we used a faster machine , we also used a simple MATLAB implementation . What is clear is that SLG scales at least as well as MinNorm on these problems , and is practical for problem sizes that the combinatorial algorithms cannot handle . Image Segmentation Experiments . We also tested our algorithm on the joint image segmentation - and - classiﬁcation task introduced in Section 3 . We used an implementation of TextonBoost [ 20 ] , then trained on and tested subsampled images from [ 5 ] . As seen in Figures 2 ( e ) and 2 ( g ) , using only the per - pixel score from our TextonBoost implementation gets the general area of the object , but does not do a good job of identifying the shape of a classiﬁed object . Compare to the ground truth in Figures 2 ( b ) and 2 ( d ) . We then perform MAP inference in a Markov Random Field with 2 - potentials ( as done in [ 20 ] ) . While this regularization , as shown in Figures 2 ( f ) and 2 ( h ) , leads to improved performance , it still performs poorly on classifying the boundary . 7 ( a ) Original Image ( b ) Ground truth ( c ) Original Image ( d ) Ground Truth ( e ) Pixel - based ( f ) Pairwise Potentials ( g ) Pixel - based ( h ) Pairwise Potentials ( i ) Concave Potentials ( j ) Continuous ( k ) Concave Potentials ( l ) Continuous Figure 2 : Segmentation experimental results Finally , we used SLG to regularize with higher order potentials . To generate regions for our poten - tials , we randomly picked seed pixels and grew the regions based on HSV channels of the image . We picked our seed pixels with a preference for pixels which were included in the least number of previously generated regions . Figure 1 ( a ) shows what the regions typically looked like . For our ex - periments , we used 90 total regions . We used SLG to minimize f ( A ) = c(cid:1)e A + P j jA \ R j jjR j nAj , where c was the output from TextonBoost , scaled appropriately . Figures 2 ( i ) and 2 ( k ) show the clas - siﬁcation output . The continuous variables x at the end of each run are shown in Figures 2 ( j ) and 2 ( l ) ; while it has no formal meaning , in general one can interpret a very high or low value of x [ k ] to correspond to high conﬁdence in the classiﬁcation of the pixel k . To generate the result shown in Figure 2 ( k ) , a problem with 10 4 variables and 90 concave potentials , our MATLAB / mex im - plementation of SLG took 71 . 4 seconds . In comparison , the MinNorm implementation of the SFO toolbox [ 15 ] gave the same result , but took 6900 seconds . Similar problems on an image of twice the resolution ( 4(cid:2)10 4 variables ) were tested using SLG , resulting in runtimes of roughly 1600 seconds . 7 Conclusion We have developed a novel method for efﬁciently minimizing a large class of submodular functions of practical importance . We do so by decomposing the function into a sum of threshold potentials , whose Lov´asz extensions are convenient for using modern smoothing techniques of convex opti - mization . This allows us to solve submodular minimization problems with thousands of variables , that cannot be expressed using only pairwise potentials . Thus we have achieved a middle ground between graph - cut - based algorithms which are extremely fast but only able to handle very speciﬁc types of submodular minimization problems , and combinatorial algorithms which assume nothing but submodularity but are impractical for large - scale problems . Acknowledgements This research was partially supported by NSF grant IIS - 0953413 , a gift from Microsoft Corporation and an Okawa Foundation Research Grant . Thanks to Alex Gittens and Michael McCoy for use of their TextonBoost implementation . 8 References [ 1 ] Dimacs , The First international algorithm implementation challenge : The core experiments , 1990 . [ 2 ] F . Bach , Structured sparsity - inducing norms through submodular functions , Advances in Neu - ral Information Processing Systems ( 2010 ) . [ 3 ] S . Becker , J . Bobin , and E . J . Candes , Nesta : A fast and accurate ﬁrst - order method for sparse recovery , Arxiv preprint arXiv 904 ( 2009 ) , 1 – 37 . [ 4 ] F . A . Chudak and K . Nagano , Efﬁcient solutions to relaxations of combinatorial problems with submodular penalties via the Lov´asz extension and non - smooth convex optimization , Proceed - ings of the eighteenth annual ACM - SIAM symposium on Discrete algorithms , Society for Industrial and Applied Mathematics , 2007 , pp . 79 – 88 . [ 5 ] M . Everingham , L . Van Gool , C . K . I . Williams , J . Winn , and A . Zisserman , The PASCAL Visual Object Classes Challenge 2009 ( VOC2009 ) Results , http : / / www . pascal - network . org / challenges / VOC / voc2009 / workshop / index . html . [ 6 ] L . Fleischer and S . Iwata , A push - relabel framework for submodular function minimization and applications to parametric optimization , Discrete Applied Mathematics 131 ( 2003 ) , no . 2 , 311 – 322 . [ 7 ] D . Freedman and P . Drineas , Energy minimization via graph cuts : Settling what is possi - ble , IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2005 . CVPR 2005 , vol . 2 , 2005 . [ 8 ] Satoru Fujishige , Takumi Hayashi , and Shigueo Isotani , The Minimum - Norm - Point Algorithm Applied to Submodular Function Minimization and Linear Programming , ( 2006 ) , 1 – 19 . [ 9 ] E . Hazan and S . Kale , Beyond convexity : Online submodular minimization , Advances in Neural Information Processing Systems 22 ( Y . Bengio , D . Schuurmans , J . Lafferty , C . K . I . Williams , and A . Culotta , eds . ) , 2009 , pp . 700 – 708 . [ 10 ] T . Itoko and S . Iwata , Computational geometric approach to submodular function minimiza - tion for multiclass queueing systems , Integer Programming and Combinatorial Optimization ( 2007 ) , 267 – 279 . [ 11 ] S . Iwata , L . Fleischer , and S . Fujishige , A combinatorial strongly polynomial algorithm for minimizing submodular functions , Journal of the ACM ( JACM ) 48 ( 2001 ) , no . 4 , 777 . [ 12 ] S . Iwata and J . B . Orlin , A simple combinatorial algorithm for submodular function minimiza - tion , Proceedings of the Nineteenth Annual ACM - SIAM Symposium on Discrete Algorithms , Society for Industrial and Applied Mathematics , 2009 , pp . 1230 – 1237 . [ 13 ] P . Kohli , M . P . Kumar , and P . H . S . Torr , P3 & Beyond : Solving Energies with Higher Order Cliques , 2007 IEEE Conference on Computer Vision and Pattern Recognition ( 2007 ) , 1 – 8 . [ 14 ] P . Kohli , L . Ladick ´ y , and P . H . S . Torr , Robust Higher Order Potentials for Enforcing Label Consistency , International Journal of Computer Vision 82 ( 2009 ) , no . 3 , 302 – 324 . [ 15 ] A . Krause , SFO : A Toolbox for Submodular Function Optimization , The Journal of Machine Learning Research 11 ( 2010 ) , 1141 – 1144 . [ 16 ] L . Lov´asz , Submodular functions and convexity , Mathematical programming : the state of the art , Bonn ( 1982 ) , 235 – 257 . [ 17 ] G . Nemhauser , L . Wolsey , and M . Fisher , An analysis of the approximations for maximizing submodular set functions , Mathematical Programming 14 ( 1978 ) , 265 – 294 . [ 18 ] Yu . Nesterov , Smooth minimization of non - smooth functions , Mathematical Programming 103 ( 2004 ) , no . 1 , 127 – 152 . [ 19 ] M . Queyranne , Minimizing symmetric submodular functions , Mathematical Programming 82 ( 1998 ) , no . 1 - 2 , 3 – 12 . [ 20 ] J . Shotton , J . Winn , C . Rother , and A . Criminisi , TextonBoost for Image Understanding : Multi - Class Object Recognition and Segmentation by Jointly Modeling Texture , Layout , and Context , Int . J . Comput . Vision 81 ( 2009 ) , no . 1 , 2 – 23 . [ 21 ] P . Stobbe and A . Krause , Efﬁcient minimization of decomposable submodular functions , arXiv : 1010 . 5511 ( 2010 ) . 9