Semi - supervised Parametric Real - world Image Harmonization Ke Wang 1 , 2 , Micha¨el Gharbi 1 , He Zhang 1 , Zhihao Xia 1 , Eli Shechtman 1 1 Adobe Inc . 2 EECS , University of California , Berkeley { kewang , mgharbi , hezhan , zxia , elishe } @ adobe . com kewang @ berkeley . edu Composite IHT Harmonizer Ours Parametric curves & shading map Fully supervised ( previous work ) Dual - stream semi - supervised ( Ours ) Figure 1 . Visual comparisons between state - of - the - art harmonization methods IHT [ 9 ] , Harmonizer [ 14 ] , and ours . Our model is fully parametric . This gives artists full posterior control over the ﬁnal composite , makes runtime efﬁcient for high - resolution real - world inputs and regularizes training . Our model predicts global RGB curves and a local shading map ( right ) . Beneﬁting from the novel dual - stream semi - supervised training strategy , our method ( right ) produces more realistic harmonized images on real - world composites ( left ) . This new training strategy , together with the shading map , makes it the ﬁrst harmonization method to address local tonal adjustments , such as shading the face according to the sun’s direction ( top ) or selectively darkening the part of the dog inside the cave ( bottom ) . Abstract Learning - based image harmonization techniques are usu - ally trained to undo synthetic random global transforma - tions applied to a masked foreground in a single ground truth photo . This simulated data does not model many of the important appearance mismatches ( illumination , object boundaries , etc . ) between foreground and background in real composites , leading to models that do not generalize well and cannot model complex local changes . We propose a new semi - supervised training strategy that addresses this problem and lets us learn complex local appearance harmo - nization from unpaired real composites , where foreground and background come from different images . Our model is fully parametric . It uses RGB curves to correct the global colors and tone and a shading map to model local vari - ations . Our method outperforms previous work on estab - lished benchmarks and real composites , as shown in a user study , and processes high - resolution images interactively . 1 . Introduction Image harmonization [ 12 , 22 , 23 , 26 , 28 , 32 ] aims to iron out visual inconsistencies created when compositing a fore - ground subject onto a background image that was captured under different conditions [ 18 , 32 ] , by altering the fore - ground’s colors , tone , etc . , to make the composite more re - alistic . Despite signiﬁcant progress , the practicality of to - day’s most sophisticated learning - based image harmoniza - tion techniques [ 3 , 4 , 9 , 10 , 13 , 14 , 16 , 32 ] is limited by a se - vere domain gap between the synthetic data they are trained on and real - world composites . As shown in Figure 2 , the standard approach to generat - ing synthetic training composites applies global transforms ( color , brightness , contrast , etc . ) to a masked foreground subject in a ground truth photo . This is how the iHarmony 1 a r X i v : 2303 . 00157v1 [ c s . C V ] 1 M a r 2023 Global adjustments Real image Paste foreground FB Syntheticcomposite Syntheticcomposite Same lighting environment Consistentshading Perfectboundary Imperfectboundary Inconsistent shading Background Realcomposite Realcomposite Different lighting environment Lighting source of Foreground and Background F B Synthetic composite image Real - world composite image Figure 2 . Domain Gap between synthetic and real - world com - posites . The existing synthetic composites [ 4 ] ( left ) , generated by applying global transforms ( e . g . , color , brightness ) , are unable to simulate many of the appearance mismatches that occur in real composites ( right ) . This leads to a domain gap : models trained on synthetic data do not generalize well to real composites . In real composites ( right ) , the foreground and background are captured under different conditions . They have different illuminations , the shadows do not match , and the object’s boundary is inconsistent . Such mismatches do not happen in the synthetic case ( left ) . Dataset [ 2 , 4 ] was constructed . A harmonization network is then trained to recover the ground truth image from the syn - thetic input . While this approach makes supervised training possible , it is unsatisfying in simulating the real composite in that synthetic data does not simulate mismatch in illu - mination , shadows , shading , contacts , perspective , bound - aries , and low - level image statistics like noise , lens blur , etc . However , in real - world composites , the foreground subject and the background are captured under different conditions , which can have more diverse and arbitrary differences in any aspects mentioned above . We argue that using realistic composites for training is essential for image harmonization to generalize better to real - world use cases . Because collecting a large dataset of artist - created before / after real composite pairs would be costly and cumbersome , our strategy is to use a semi - supervised approach instead . We propose a novel dual - stream training scheme that alternates between two data streams . Similar to previous work , the ﬁrst is a supervised training stream , but crucially , it uses artist - retouched image pairs . Different from previous datasets , these artistic adjust - ments include global color editing but also dodge and burn shading corrections and other local edits . The second stream is fully unsupervised . It uses a GAN [ 8 ] training procedure , in which the critic compares our harmonized results with a large dataset of realistic im - age composites . Adversarial training requires no paired ground truth . The foreground and background for the com - posite in this dataset are extracted from different images so that their appearance mismatch is consistent with what the model would see at test time . To reap the most beneﬁts from our semi - supervised train - ing , we also introduce a new model that is fully paramet - ric . To process a high - resolution input composite at test time , our proposed network ﬁrst creates a down - sampled copy of the image at 512 × 512 resolution , from which it predicts global RGB curves and a smooth , low - resolution shading map . We then apply the RGB curves pointwise to the high - resolution input and multiply them by the upsam - pled shading map . The shading map enables more realistic local tonal variations , unlike previous harmonization meth - ods limited to global tone and color changes , either by con - struction [ 14 , 16 , 31 ] or because of their training data [ 4 ] . Our parametric approach offers several beneﬁts . First , by restricting the model’s output space , it regularizes the adver - sarial training . Unrestricted GAN generators often create spurious image artifacts or other unrealistic patterns [ 36 ] . Second , it exposes intuitive controls for an artist to adjust and customize the harmonization result post - hoc . This is unlike the black - box nature of most current learning - based approaches [ 3 , 4 , 9 , 10 ] , which output an image directly . And , third our parametric model runs at an interactive rate , even on very high - resolution images ( e . g . , 4k ) , whereas sev - eral state - of - the - art methods [ 4 , 9 , 10 ] are limited to low - resolution ( e . g . , 256 × 256 ) inputs . To summarize , we make the following contributions : • A novel dual - stream semi - supervised training strategy that , for the ﬁrst time , enables training from real com - posites , which contains much richer local appearance mismatches between foreground and background . • A parametric harmonization method that can capture these more complex , local effects ( using our shading map ) and produces more diverse and photorealistic harmonization results . • State - of - the - art results on both synthetic and real com - posite test sets in terms of quantitative results and visual comparisons , together with a new evaluation benchmark . We will release our code publicly upon publication . 2 . Related works Image harmonization . Traditional image harmonization methods mainly focus on adjusting the low - level appear - ance statistics ( e . g . , color statistics , gradient information ) between the foreground objects and the background [ 12 , 22 , 23 , 26 , 28 , 32 ] . Supervised learning - based approaches have been proposed and shown notable success [ 3 , 4 , 9 , 10 , 29 , 37 ] by learning image harmonization from synthetic training pairs , for instance , iHarmony Dataset [ 4 ] . Works as DIH 2 b ) Stream 2 ( 50 % ) : Unsupervised adversarial training Ourmodel Real Composite Fake Output True Real image paste foreground C Ourmodel inpaint + Extract background Extract foreground C Composite Manual image edits … Generate real composite a ) Stream 1 ( 50 % ) : Supervised training Synthesizedcomposite Output Ground truth Figure 3 . Overview of semi - supervised dual - stream training strategy . To bridge the domain gap , our proposed semi - supervised dual - stream training strategy alternates between two training streams : a ) Supervised training with artist - retouched composite image pairs ( left ) . Artist adjustments include global color editing , shading correction , and other local edits . b ) Unsupervised adversarial training with real - world composite images ( right ) . It uses a GAN [ 8 ] training procedure , comparing our harmonized results with a large dataset of realistic image composites . The foreground and background for the composite are from different images , so the appearance mismatch is consistent with what we see at test time . [ 29 ] , DovNet [ 4 ] , IHT [ 9 ] , Guo et al . [ 10 ] consider the image harmonization task as a pixel - wise image - to - image translation task , and are limited to low - resolution inputs ( typically 256 × 256 ) due to computational inefﬁciency . Re - cent work extended image harmonization to high - resolution images by designing parametric models [ 3 , 14 , 16 , 31 ] . To name a few , Liang et al . learns the spatial - separated RGB curves for high - resolution image harmonization . Ke et al . [ 14 ] directly predicts the ﬁlter arguments of several white - box ﬁlters , which can be efﬁciently applied to high - resolution images . In all of those approaches , synthetic training pairs are generated by applying global transforms to the masked foreground subjects of natural ground - truth images and hence do not simulate mismatch in illumination , shadows , shading , contact , etc . , that happen in real - world composite images . Therefore , due to the synthetic training data and model construction [ 14 , 16 ] , previous works are limited to global tone and color changes . In contrast , our model is trained on real - world composite images and artist - retouched synthetic images , which enables us to model richer image edits and produce more compelling results on real composites . Efﬁcient and high - resolution image enhancement . There has been a wide range of research focusing on designing ef - ﬁcient and high - resolution image enhancement algorithms [ 6 , 7 , 17 ] . Gharbi et al . [ 6 ] introduced a convolutional neural network ( CNN ) that predicts the coefﬁcients of a locally - afﬁne model in bilateral space from down - sampled input images . The coefﬁcients are then mapped back to the full - resolution image space . Zeng et al . [ 34 ] directly learns 3D Lookup Tables ( LUTs ) for real - time image enhancement . In our application , image harmonization can be considered as a background - guided image enhancement problem . Thus , inspired by [ 6 , 34 ] , we design a network that directly pre - dicts the coefﬁcients of RGB curves ( piece - wise linear func - tion ) from down - sampled composite inputs . We then apply the RGB curves pointwise to the high - resolution input with - out introducing extra computation costs . Image - based relighting Image - based relighting ap - proaches [ 19 , 21 , 25 , 33 ] focus on modifying the input lighting conditions and local shading to generate convinc - ing composite results . However , recent relighting methods are mainly designed for portrait applications and do not generalize well to other objects due to the limitation of Light - stage capturing only portraits but not diverse ob - jects [ 5 ] . With a similar idea of incorporating local shading edits but a different approach , our method embeds the shading layer into a network and trains on composite image datasets without explicitly leveraging scene representations ( geometry , materials , lighting ) and using full relighting models . 3 . Method Our image harmonization method corrects the fore - ground subject in a rough composite to make the overall image look more realistic using a new parametric model ( § 3 . 1 ) that can be applied to real - world high - resolution im - ages efﬁciently . Previous harmonization techniques train on synthetically - generated composite pairs [ 4 ] , where the model’s input is a global transformation of a ground truth image within a foreground subject mask . The colors are of - ten unnatural , the mask boundary is close to perfect , and there is no mismatch in appearance , illumination , or low - level image statistics since both foreground and background 3 Learned Global Color Curves E n c od e r Apply learned curves to foreground Intermediate results ( low - res ) ApplyCurves E n c od e r D ec od e r Downsampled input ( low - res ) Learned Shading map Learned Global Color Curves Input composite ( high - res ) UpsampledShading map × Intermediate results ( high - res ) Harmonized outputs ( high - res ) Low - res branch High - res branch Figure 4 . Illustration of our parametric model design . Our framework consists of a low - resolution branch and a high - resolution branch . At test time , we down - sample the given high - resolution image and predict the global RGB curves and shading map through a two - stage network . Those parametric outputs are then executed at the original resolution to produce the ﬁnal harmo - nized image . Our model can scale to any resolution . come from the same image . As a result , models trained on such data generalize poorly . Our method addresses this cru - cial issue using a novel dual - stream semi - supervised train - ing strategy ( § 3 . 2 ) that leverages high - quality artist - created image pairs and unpaired realistic composites to bridge the training - testing domain gap . See Figure 3 for an overview . 3 . 1 . Parametric image harmonization Our network design is inspired by real - world composite harmonization workﬂows 1 . An artist typically applies sev - eral image corrections sequentially , each dedicated to har - monizing a speciﬁc composite element , such as luminos - ity , color , or shading . Accordingly , our image transforma - tion model consists of two modules , applied sequentially : a pointwise global color correction using RGB curves and a local shading correction using a low - frequency multiplica - tive map . For efﬁciency , our model operates at two resolu - tions . Pipeline overview . As illustrated in Figure 4 , our harmo - nization pipeline takes as input a foreground image F ∈ R 3 hw with dimensions h , w ∈ N , a background image im - age B ∈ R 3 hw , and a compositing alpha mask M ∈ R hw . We deﬁne the unharmonized composite image as C : = M · F + ( 1 − M ) · B . At test time , we start by downsampling the inputs to a ﬁxed resolution ( 512 × 512 ) , denoting the low - resolution images by C lr , B lr , M lr respectively . We concatenate these maps and pass them to a neural network f that predicts the parameters [ θ 1 , θ 2 ] : = f ( C lr , B lr , M lr ) 1 https : / / youtu . be / g3qe4rDw1XU of our two - stage parametric image transformation . Finally , we apply the parametric transformation t 1 , t 2 sequentially on the high - resolution input to obtain the ﬁnal harmonized composite O : = t 2 ( t 1 ( C , M ; θ 1 ) , M ; θ 2 ) , where M is used to ensure only the area under the foreground mask is altered . We describe the two stages in the parametric transformation next . Global color correction curves . Our ﬁrst high - resolution processing stage t 1 applies the predicted global RGB curves for color correction . We parameterize them as 3 piecewise linear curves with 32 control points , applied to each color channel independently so that θ 1 ∈ R 32 × 2 × 3 is the set of 2D coordinates of the curve nodes . We predict these param - eters from [ C lr , B lr , M lr ] using a ResNet - 50 [ 11 ] - based network . Applying the curve is a per - pixel operation that can be efﬁciently computed at any resolution . Local low - frequency shading map . Our second stage t 2 multiplies the image with a low - frequency grayscale shad - ing map , to model local tonal corrections . It is applied on the output of the ﬁrst stage . We constrain the shading map to only model low - frequency change by generating at a low resolution : θ 2 ∈ R 64 × 64 . It is produced by a modiﬁed U - Net [ 24 ] with large receptive ﬁeld , given the low - resolution buffers [ C lr , B lr , M lr ] , together with the output of the color - correction stage at low - resolution t 1 ( C lr , M lr ; θ 1 ) At test time , we upsample the low - resolution shading map to the original high - resolution and multiply it pointwise with the color - corrected image to obtain our ﬁnal harmonized composite : O = t 1 ( C , M ; θ 1 ) · upsample ( θ 2 ) . ( 1 ) 3 . 2 . Dual - stream semi - supervised training Our semi - supervised training strategy aims to alleviate the generalization issues that plague many state - of - the - art harmonization models , as shown in Figure 2 . Our approach uses two data streams , sampled with equal probability , and minimizes a different objective on each data stream . The ﬁrst stream uses input / output composite pairs similar to pre - vious work , except that we only use artist - created image transformations instead of random augmentations . The sec - ond is unsupervised . This allows us to use more realistic im - ages obtained by compositing foreground and background from unrelated images , for which no ground truth is eas - ily obtainable . The objective for the supervised stream is the combination of (cid:96) 1 loss and adversarial loss , while we only impose adversarial loss for the unsupervised training stream . Supervised training using retouched images . The ﬁrst stream is fully supervised . Unlike previous work , we use images retouched by artists rather than mostly relying on random augmentations . We refer to this dataset as Artist - Retouched in the rest of the paper . Artists were allowed 4 to use common image editing operations such as global lu - minosity or color adjustments , but also local editing tools like brushes , e . g . , to alter the shading . Speciﬁally , we collected n = 46173 before / after retouching image pairs { I i , O i } i = 1 , . . . , n , with the mask for one foreground object M i for each pair . From each triplet , we can create 2 input composites for training : one with only the foreground re - touched M i · O i + ( 1 − M i ) · I i , and the other with only the background is retouched M i · I i + ( 1 − M i ) · O i . Since our harmonization model only alters the foreground , we use the unedited image I i , and the retouched image O i as ground truth targets for these input composites , respectively . When sampling training data from this stream , we opti - mize our model’s parameters to minimize the sum of an (cid:96) 1 reconstruction error L rec between the ground truth and our model output , and an adversarial objective [ 8 ] λ L rec + ( 1 − λ ) L G , ( 2 ) with λ balances the two losses . For our experiments , λ is empirically set to 0 . 92 . The generator , our parametric im - age harmonization model , is trained to produce outputs that cannot be distinguished from “real” images . We use a U - Net discriminator [ 30 ] D to make per - pixel real vs . fake classiﬁcations . Since our data formation model assumes the background is always correct , our discriminator is trained to predict the inverted foreground mask 1 − M . That is when shown “fake” images , i . e . , the background pixels have label 1 and the foreground 0 . For the “real” class , the target is all an all - 1s map . So the discriminator loss is given by : L D = − E I real [ log ( D ( I real ) ) ] − E I fake [ log ( ( 1 − M ) − D ( I fake ) ) ] , ( 3 ) The generator loss is : L G = − E I fake [ log ( D ( I fake ) ) ] . ( 4 ) To further increase the training diversity , we randomly augment the foreground brightness on the ﬂy without re - touching the color . Unsupervised training with real composites . Our second training stream is unsupervised . It uses randomly generated composites that are representative of real - world use cases but for which no ground truth is available . To properly re - produce the appearance mismatch in real applications , we create these composites as follows . We start from a dataset of m images { I i } i = 1 , . . . , m , each with a foreground object mask M i , from which we derive a foreground F i = M i · I i and a background B i = ( 1 − M i ) · I i . As preprocessing , we dilate the foreground mask by 30 pixels and inpaint the corresponding area in the background image using a pre - trained inpainting network ( we use LaMa [ 27 ] ) . Then dur - ing , training we sample two images i and j and create a composite by pasting the foreground j onto the inpainted background of i : C ij : = F j · M j + inpaint ( B i , M i ) · ( 1 − M j ) . ( 5 ) The triplet [ C ij , inpaint ( B i , M i ) , M j ] is passed as input to our model . Figure 3b illustrates the process . With no ground truth available when sampling compos - ites from this data stream , we only optimize the adversarial loss ( 1 − λ ) L G , as deﬁned in Eq . ( 4 ) , where again the fake samples I fake are the outputs of our model . The discriminator is trained with Eq . ( 3 ) , where I real is not a real composite , but is obtained by masking the fore - ground subject F i , inpainting the background B i , and past - ing the foreground back onto the same image , i . e . I real : = F i · M i + inpaint ( B i , M i ) · ( 1 − M i ) . ( 6 ) This is similar to how we produce a composite of two im - ages i and j , expect that we only use one image , i . This alteration of the “real” class is to prevent the discriminator from using the inpainting boundary region as a strong cue to discriminate between our model output and real images , which leads to collapse in the GAN training . GAN training is known to be unstable or cause image artifacts [ 36 ] , but because our parametric harmonization model adjusts color curves and adds low - resolution shad - ows , instead of predicting pixels directly , it has a strong reg - ularizing effect , which prevents the GAN training to degen - erate and cause spurious artifacts in the output image . We use the same discriminator ( and generator ) in both streams . 4 . Experiments We compare our parametric image harmonization model with state - of - the - art methods on several established bench - marks ( § 4 . 1 ) , as well as a test subset of Artist - Retouched dataset . Furthermore , we demonstrate our superior perfor - mance on real - world harmonization tasks via a user study and qualitative comparisons on real composites ( § 4 . 2 ) . Ab - lations of our method demonstrate the beneﬁts of our semi - supervised training strategy and the individual components of our parametric model ( § 4 . 3 ) . More results can be found in the supplementary . Evaluation metrics : For quantitative comparisons with ground truth , we report performances by Mean Square Er - ror ( MSE ) , Peak Signal - to - Noise Ratio ( PSNR ) , Structural Similarity ( SSIM ) , and Learned Perceptual Image Patch Similarity ( LPIPS ) [ 35 ] . PSNR is measured in dB and cal - culated as : PSNR = 10 log 10 255 2 MSE . Implementation details : Our model and discriminator are implemented in PyTorch [ 20 ] and trained on an NVIDIA A100 GPU using the Adam optimizer [ 15 ] for 80 epochs , with a batch size of 8 and an initial learning rate of 4 × 10 − 5 , decayed by a factor 0 . 2 every 20 epochs . 4 . 1 . Quantitative comparisons on paired data We compare our method with three recent meth - ods , DovNet [ 4 ] , Image Harmonization with Transformer 5 Composite DovNet IHT Harmonizer Ours GT a ) A r ti s t - r e t ou c h e d b ) R ea l H M Figure 5 . Representative visual comparisons between state - of - the - art harmonization results . We compared our method with compos - ite , DovNet [ 4 ] , IHT [ 9 ] , and Harmonizer [ 14 ] , and ground truth on both a ) Artist - Retouched synthetic dataset and b ) RealHM real - world composite dataset . Red boxes indicate the foreground subject in the composite image . The ground truth for RealHM benchmark [ 13 ] is expert - annotated harmonization results . Our results show better visual agreements with the ground truth in terms of color harmonization ( rows 1 , 2 and 4 ) and shading correction ( row 3 ) . ( IHT ) [ 14 ] , and Harmonizer [ 14 ] , using the pre - trained model released publicly by the authors . We ﬁrst evaluate the synthetic iHarmony benchmark [ 4 ] . For fairness , our method uses the same setup as theirs for this comparison . In particular , we train our model exclusively on the same training set as the baselines , using only our fully - supervised stream , deactivating the adversarial loss , and only pass - ing the composite C and foreground mask M as inputs . We report metrics at both at 256 × 256 resolution and at 2048 × 2048 on the HAdobe5k high - resolution subset of iHarmony . Like our parametric approach , Harmonizer can process high - res images , but the other two methods are lim - ited to 256 × 256 inputs . So , for high - res comparison , we bi - linearly downsample the input to DovNet and IHT , process the image , then bilinearly upsample the result before com - puting the metrics . Despite its simplicity , our parametric model consistently outperforms or matches the more com - plex baselines . Results are summarized in Table 1 . The iHarmony dataset is dominated by unrealistic syn - thetic image augmentations ( 71 % ) , so we also evaluate our results on more realistic retouches from human experts . The two datasets we use for evaluation are a testing split of our Artist - Retouched dataset , introduced in Section 3 . 2 , con - taining 1000 before / after pairs , and the RealHM [ 13 ] bench - mark , containing 216 real - world high - resolution compos - ites with expert annotated harmonization results as ground Size Method MSE ↓ PSNR ↑ SSIM ↑ × 10 − 2 LPIPS ↓ × 10 − 3 256 Composite 172 . 3 31 . 74 97 . 48 16 . 46 DovNet [ 4 ] 51 . 33 34 . 97 98 . 12 9 . 734 IHT [ 9 ] 30 . 46 37 . 33 98 . 77 7 . 347 Harmonizer [ 14 ] 24 . 24 38 . 25 99 . 09 7 . 349 Ours 20 . 57 38 . 30 98 . 91 7 . 270 2048 ∗ Composite 352 . 9 28 . 39 96 . 36 14 . 52 DovNet [ 4 ] 66 . 37 34 . 01 96 . 35 21 . 45 IHT [ 9 ] 47 . 34 35 . 12 96 . 53 20 . 65 Harmonizer [ 14 ] 23 . 30 38 . 33 98 . 77 7 . 148 Ours 20 . 31 38 . 29 98 . 82 7 . 123 Table 1 . Quantitative comparison on iHarmony benchmark [ 4 ] at both 256 × 256 and 2048 × 2048 . ( ∗ ) We only calculate the metrics on the Adobe5k dataset ( a subset of iHarmony4 ) for high - resolution images . Red , and Blue correspond to the ﬁrst and second best results . ↑ means higher the better , and ↓ means lower the better . truth . We compared the performance of their pre - trained models and ours trained with the full dual - stream pipeline at 2048 × 2048 resolution . Table 2 shows our method consis - tently outperforms the baselines , with around 30 % relative MSE improvements compared to Harmonizer [ 14 ] on both datasets . As shown in Figure 5 , our method produces more 6 Dataset Method MSE ↓ PSNR ↑ SSIM ↑ × 10 − 2 LPIPS ↓ × 10 − 3 Artist - Retouched Composite 603 . 20 23 . 41 91 . 19 40 . 18 DovNet [ 4 ] 352 . 4 26 . 42 90 . 83 56 . 47 IHT [ 9 ] 369 . 3 26 . 36 90 . 87 55 . 80 Harmonizer [ 14 ] 239 . 1 29 . 42 93 . 84 33 . 75 Ours 170 . 1 29 . 79 94 . 56 29 . 18 RealHM Composite 404 . 4 25 . 88 94 . 70 29 . 32 DovNet [ 4 ] 225 . 1 26 . 72 92 . 00 47 . 50 IHT [ 9 ] 264 . 0 26 . 48 92 . 46 48 . 48 Harmonizer [ 14 ] 231 . 4 27 . 40 94 . 86 27 . 62 Ours 153 . 3 28 . 34 95 . 51 23 . 09 Table 2 . Quantitative Comparison on RealHM benchmark and Artist - Retouched dataset . Our approach outperforms other meth - ods in all four metrics . realistic results , closer to the ground truth . 4 . 2 . Evaluation on real composite images Our semi - supervised training procedure allows us to train on realistic composites , where foreground and back - ground come from different sources . Just like it limits the training potential of harmonization methods , using paired data created from a single ground truth image for evaluation is unsatisfying because it is not representative of real - world use cases ( Fig . 2 ) . So , we demonstrate the practical effec - tiveness of our method in a user study with real composites . For qualitative evaluation , we also created a set of 40 high - resolution real composite images with reference images . User Study . Our user study follows a 2 alternatives forced choice protocol [ 35 ] , comparing our model with DovNet [ 4 ] , IHT [ 9 ] , and Harmonizer [ 14 ] . We selected 60 real composites from the RealHM dataset [ 13 ] , making sure there were no duplicate foregrounds or backgrounds . Since RealHM primarily focuses on portrait images , we also created 40 non - portrait real composites using free - to - use images from Unsplash 2 , giving us a total of 100 real composite images . Each of our results is compared with the unaltered input composite and the three baseline results , which gives 100 × 4 = 400 image pairs to compare in to - tal , which we submitted for evaluation to a pool of subjects on Amazon Turk 3 . Each participant was shown 50 image pairs and , for each pair , they were asked to “select which image looks more plausible” . To ensure the quality of the responses , each subject was also shown 10 ‘sentinel’ testing pairs composed of a real natural image and an extremely off - retouch image ( e . g . , where the image is all green ) . This helped us ﬁlter low - quality participants , such as users that always click ‘left’ to try and game the MTurk reward . Af - ter ﬁltering , we obtained pair - wise comparison results from 2 https : / / unsplash . com / 3 https : / / www . mturk . com / 70 subjects , contributing a total of 3500 comparisons . To analyze these results , we follow previous work [ 3 , 4 , 14 ] , and use the Bradley - Terry ( B - T ) [ 1 ] model to derive the global ranking of all methods . We normalize the B - T scores such that the sum of the scores equals one across methods . Table 3 summarizes the results . It shows that our method achieves the highest B - T score , outperforming all the base - lines , indicating our approach compares favorably in real - world applications . Methods B - T Score ↑ Composite 0 . 1025 DovNet [ 4 ] 0 . 1342 IHT [ 9 ] 0 . 2350 Harmonizer [ 14 ] 0 . 2257 Ours 0 . 3025 Table 3 . User Study Results . B - T scores of composite image , DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] are calculated on 100 real composite images . Our approach ranks ﬁrst , suggesting superior real - world performance . Real composites with captured reference . Figure 6 shows two representative examples of real composite results ( see supplemental for more ) . For this qualitative comparison , we created a dataset of 40 high - resolution real - composite images with reference images by capturing a ﬁxed set of foreground objects against multiple backgrounds , as well as a ‘background - only’ image . By segmenting the foreground object from one photo and pasting onto the ‘background - only’ image of another , we get an input composite for our model . The captured photo of the same object in the same background scene ( placed at roughly the same location ) acts as qualitative reference . Compared to other approaches , our results are visually closer to the captured reference . Background Composite IHT Harmonizer Ours Reference Figure 6 . Real composite harmonization results with captured reference . The composite is obtained by pasting the foreground subject , from a different photo ( not shown ) onto the background ( left ) . The reference ( right ) is obtained by physically placing the foreground subject in the background scene and taking a photo . We compare our method with IHT [ 9 ] , and Harmonizer [ 14 ] . Our results show better visual agreement with the captured reference ( best viewed by zooming on the digital preprint ) . 7 4 . 3 . Ablation studies We evaluate the beneﬁts of our dual - stream semi - supervised training strategy , comparing it to conventional supervised training . We also analyze the impact of our global RGB curve module and shading map . We conduct the comparisons on RealHM [ 13 ] at 512 × 512 resolution , comparing our full method ( dual - stream training + two - stage model ) with : 1 . Supervised training only ( Stream 1 ) + global curves only ; 2 . Supervised training only ( Stream 1 ) + two - stage parametric model ; 3 . Dual - stream train - ing + global curves module only . We report quantitative metrics ( MSE and PSNR ) , and the B - T score from a user study ( similar to § 4 . 2 , but this time with 68 subjects ) Ta - ble 4 and Figure 7 summarize our results . They show our shading map and our dual - stream training strategy both signiﬁcantly improve realism over the curve - only , fully - supervised model . Stream 1 Stream 2 Global Curves Shading Map MSE PSNR B - T score (cid:88) (cid:88) 288 . 7 26 . 45 0 . 201 (cid:88) (cid:88) (cid:88) 264 . 0 26 . 89 0 . 206 (cid:88) (cid:88) (cid:88) 222 . 6 27 . 29 0 . 217 (cid:88) (cid:88) (cid:88) (cid:88) 154 . 3 28 . 40 0 . 252 Composite - - 0 . 124 Table 4 . Ablation study results of training strategies and para - metric model . We compare our semi - supervised training strat - egy ( Stream 1 + Stream 2 ) with supervised training ( Stream 1 ) and compare our two - stage model ( Global Curves + Shading map ) versus the model with only the global curve module . MSE and PSNR are used for quantitative comparisons , and the B - T score is calculated from user study results . As reported in Table 4 , we observe that the dual - stream training strategy outperforms supervised training ( row 3 and 4 v . s . row 1 and 2 ) in terms of both quantitative met - rics and B - T score , which demonstrates the beneﬁts of our proposed dual - training strategy in real - world applica - tions . Inspecting the results in Figure 7 , we observe that the dual - training strategy ( column 4 and 5 ) brings advantages in color - harmonization when there is a strong foreground - background color mismatch . On the other hand , as shown in Table 4 row 3 v . s . row 4 , our proposed two - stage parametric model outperforms the global curve - only model by a large margin on RealHM benchmark , reducing the MSE by 30 % . Furthermore , as shown in Figure 7 , our full model ( last column ) includes both color harmonization and local shading to the results , achieving more plausible and harmonious results . To better visualize the roles of our two - stage parametric model , Figure 8 shows the intermediate results as well as the parametric outputs ( global curves and shading map ) of Composite CurvesSupervised Curves + Shading map Supervised Curves Dual - stream Curves + Shading map Dual - stream ( full ) Figure 7 . Visual comparison of ablations . Our full pipeline ( right ) shows more color - harmonious results than supervised training - only models ( columns 2 and 3 ) . Our local shading map adjusts local shading and produces more natural outputs ( compare columns 4 and 5 ) . a representative example . The global curves module har - monizes the global tone of the foreground sculpture and matches it with the background scene . The shading map module further performs local editing to adjust the shading of the sculpture and ﬁt it with the lighting environment bet - ter . Composite Intermediate results ( global curves ) Shading map Shading map Final results ( global curves + shading map ) Figure 8 . Intermediate results and parametric outputs . RGB curves harmonize the global color / tone ( center ) , while our shading map corrects the local shading in the harmonization output ( right ) . 5 . Conclusion In this work , we propose a novel semi - supervised dual - stream training strategy to bridge the training - testing do - main gap and mitigate the generalization issues that limit previous works for real - world image harmonization . Our method leverages high - quality artist - created image pairs and unpaired realistic composites to enable richer image edits for real - world applications . Besides , we introduce a new two - stage parametric model ( Global RGB Curves and shading map ) to reap the most beneﬁts from our training strategy and , for the ﬁrst time , enable local editing effects with learned shading map . Our method outperforms other state - of - the - art methods on established benchmarks and real composites . Furthermore , our training strategy has the po - tential to generalize to a wider range of image harmoniza - tion operations ( e . g . , matching the noise , harmonizing the boundaries , adding cast shadows ) . As a future work , we would like to include more attributes in our models and fur - ther improve the performance of real - world image harmo - nization . 8 References [ 1 ] Ralph Allan Bradley and Milton E Terry . Rank analysis of incomplete block designs : I . the method of paired compar - isons . Biometrika , 39 ( 3 / 4 ) : 324 – 345 , 1952 . 7 [ 2 ] Vladimir Bychkovsky , Sylvain Paris , Eric Chan , and Fr´edo Durand . Learning photographic global tonal adjustment with a database of input / output image pairs . In The Twenty - Fourth IEEE Conference on Computer Vision and Pattern Recognition , 2011 . 2 [ 3 ] Wenyan Cong , Xinhao Tao , Li Niu , Jing Liang , Xuesong Gao , Qihao Sun , and Liqing Zhang . High - resolution im - age harmonization via collaborative dual transformations . In Proceedings of the IEEE / CVF Conference on Computer Vi - sion and Pattern Recognition , pages 18470 – 18479 , 2022 . 1 , 2 , 3 , 7 [ 4 ] Wenyan Cong , Jianfu Zhang , Li Niu , Liu Liu , Zhixin Ling , Weiyuan Li , and Liqing Zhang . Dovenet : Deep image harmonization via domain veriﬁcation . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 8394 – 8403 , 2020 . 1 , 2 , 3 , 5 , 6 , 7 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 [ 5 ] Paul Debevec , Tim Hawkins , Chris Tchou , Haarm - Pieter Duiker , Westley Sarokin , and Mark Sagar . Acquiring the reﬂectance ﬁeld of a human face . In Proceedings of the 27th annual conference on Computer graphics and interac - tive techniques , pages 145 – 156 , 2000 . 3 [ 6 ] Micha¨el Gharbi , Jiawen Chen , Jonathan T Barron , Samuel W Hasinoff , and Fr´edo Durand . Deep bilateral learning for real - time image enhancement . ACM Transactions on Graphics ( TOG ) , 36 ( 4 ) : 1 – 12 , 2017 . 3 [ 7 ] Micha¨el Gharbi , YiChang Shih , Gaurav Chaurasia , Jonathan Ragan - Kelley , Sylvain Paris , and Fr´edo Durand . Transform recipes for efﬁcient cloud photo enhancement . ACM Trans - actions on Graphics ( TOG ) , 34 ( 6 ) : 1 – 12 , 2015 . 3 [ 8 ] Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . Generative adversarial networks . Commu - nications of the ACM , 63 ( 11 ) : 139 – 144 , 2020 . 2 , 3 , 5 [ 9 ] Zonghui Guo , Dongsheng Guo , Haiyong Zheng , Zhaorui Gu , Bing Zheng , and Junyu Dong . Image harmonization with transformer . In Proceedings of the IEEE / CVF Interna - tional Conference on Computer Vision , pages 14870 – 14879 , 2021 . 1 , 2 , 3 , 6 , 7 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 [ 10 ] Zonghui Guo , Haiyong Zheng , Yufeng Jiang , Zhaorui Gu , and Bing Zheng . Intrinsic image harmonization . In Pro - ceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 16367 – 16376 , 2021 . 1 , 2 , 3 [ 11 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceed - ings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . 4 [ 12 ] Jiaya Jia , Jian Sun , Chi - Keung Tang , and Heung - Yeung Shum . Drag - and - drop pasting . ACM Transactions on graph - ics ( TOG ) , 25 ( 3 ) : 631 – 637 , 2006 . 1 , 2 [ 13 ] Yifan Jiang , He Zhang , Jianming Zhang , Yilin Wang , Zhe Lin , Kalyan Sunkavalli , Simon Chen , Sohrab Amirghodsi , Sarah Kong , and Zhangyang Wang . Ssh : A self - supervised framework for image harmonization . In Proceedings of the IEEE / CVF International Conference on Computer Vision , pages 4832 – 4841 , 2021 . 1 , 6 , 7 , 8 , 11 , 12 [ 14 ] Zhanghan Ke , Chunyi Sun , Lei Zhu , Ke Xu , and Rynson WH Lau . Harmonizer : Learning to perform white - box image and video harmonization . arXiv preprint arXiv : 2207 . 01322 , 2022 . 1 , 2 , 3 , 6 , 7 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 [ 15 ] Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . 5 [ 16 ] Jingtang Liang , Xiaodong Cun , and Chi - Man Pun . Spatial - separated curve rendering network for efﬁcient and high - resolution image harmonization . arXiv preprint arXiv : 2109 . 05750 , 2021 . 1 , 2 , 3 [ 17 ] Sean Moran , Pierre Marza , Steven McDonagh , Sarah Parisot , and Gregory Slabaugh . Deeplpf : Deep local para - metric ﬁlters for image enhancement . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pages 12826 – 12835 , 2020 . 3 [ 18 ] Li Niu , Wenyan Cong , Liu Liu , Yan Hong , Bo Zhang , Jing Liang , and Liqing Zhang . Making images real again : A comprehensive survey on deep image composition . arXiv preprint arXiv : 2106 . 14490 , 2021 . 1 [ 19 ] Rohit Pandey , Sergio Orts Escolano , Chloe Legendre , Chris - tian Haene , Soﬁen Bouaziz , Christoph Rhemann , Paul De - bevec , and Sean Fanello . Total relighting : learning to relight portraits for background replacement . ACM Transactions on Graphics ( TOG ) , 40 ( 4 ) : 1 – 21 , 2021 . 3 [ 20 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , et al . Pytorch : An im - perative style , high - performance deep learning library . Ad - vances in neural information processing systems , 32 , 2019 . 5 [ 21 ] Julien Philip , S´ebastien Morgenthaler , Micha¨el Gharbi , and George Drettakis . Free - viewpoint indoor neural relighting from multi - view stereo . ACM Transactions on Graphics ( TOG ) , 40 ( 5 ) : 1 – 18 , 2021 . 3 [ 22 ] Francois Pitie , Anil C Kokaram , and Rozenn Dahyot . N - dimensional probability density function transfer and its ap - plication to color transfer . In Tenth IEEE International Con - ference on Computer Vision ( ICCV’05 ) Volume 1 , volume 2 , pages 1434 – 1439 . IEEE , 2005 . 1 , 2 [ 23 ] Erik Reinhard , Michael Adhikhmin , Bruce Gooch , and Peter Shirley . Color transfer between images . IEEE Computer graphics and applications , 21 ( 5 ) : 34 – 41 , 2001 . 1 , 2 [ 24 ] Olaf Ronneberger , Philipp Fischer , and Thomas Brox . U - net : Convolutional networks for biomedical image segmen - tation . In International Conference on Medical image com - puting and computer - assisted intervention , pages 234 – 241 . Springer , 2015 . 4 [ 25 ] Tiancheng Sun , Jonathan T Barron , Yun - Ta Tsai , Zexiang Xu , Xueming Yu , Graham Fyffe , Christoph Rhemann , Jay Busch , Paul E Debevec , and Ravi Ramamoorthi . Single image portrait relighting . ACM Trans . Graph . , 38 ( 4 ) : 79 – 1 , 2019 . 3 [ 26 ] Kalyan Sunkavalli , Micah K Johnson , Wojciech Matusik , and Hanspeter Pﬁster . Multi - scale image harmonization . 9 ACM Transactions on Graphics ( TOG ) , 29 ( 4 ) : 1 – 10 , 2010 . 1 , 2 [ 27 ] Roman Suvorov , Elizaveta Logacheva , Anton Mashikhin , Anastasia Remizova , Arsenii Ashukha , Aleksei Silvestrov , Naejin Kong , Harshith Goka , Kiwoong Park , and Victor Lempitsky . Resolution - robust large mask inpainting with fourier convolutions . In Proceedings of the IEEE / CVF Win - ter Conference on Applications of Computer Vision , pages 2149 – 2159 , 2022 . 5 [ 28 ] Michael W Tao , Micah K Johnson , and Sylvain Paris . Error - tolerant image compositing . In European Conference on Computer Vision , pages 31 – 44 . Springer , 2010 . 1 , 2 [ 29 ] Yi - Hsuan Tsai , Xiaohui Shen , Zhe Lin , Kalyan Sunkavalli , Xin Lu , and Ming - Hsuan Yang . Deep image harmonization . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3789 – 3797 , 2017 . 2 , 3 [ 30 ] Xintao Wang , Liangbin Xie , Chao Dong , and Ying Shan . Real - esrgan : Training real - world blind super - resolution with pure synthetic data . In Proceedings of the IEEE / CVF Inter - national Conference on Computer Vision , pages 1905 – 1914 , 2021 . 5 [ 31 ] Ben Xue , Shenghui Ran , Quan Chen , Rongfei Jia , Binqiang Zhao , and Xing Tang . Dccf : Deep comprehensible color ﬁlter learning framework for high - resolution image harmo - nization . arXiv preprint arXiv : 2207 . 04788 , 2022 . 2 , 3 [ 32 ] Su Xue , Aseem Agarwala , Julie Dorsey , and Holly Rush - meier . Understanding and improving the realism of image composites . ACM Transactions on graphics ( TOG ) , 31 ( 4 ) : 1 – 10 , 2012 . 1 , 2 [ 33 ] Yu - Ying Yeh , Koki Nagano , Sameh Khamis , Jan Kautz , Ming - Yu Liu , and Ting - Chun Wang . Learning to relight portrait images via a virtual light stage and synthetic - to - real adaptation . ACM Transactions on Graphics ( TOG ) , 2022 . 3 [ 34 ] Hui Zeng , Jianrui Cai , Lida Li , Zisheng Cao , and Lei Zhang . Learning image - adaptive 3d lookup tables for high perfor - mance photo enhancement in real - time . IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020 . 3 [ 35 ] Richard Zhang , Phillip Isola , Alexei A Efros , Eli Shecht - man , and Oliver Wang . The unreasonable effectiveness of deep features as a perceptual metric . In Proceedings of the IEEE conference on computer vision and pattern recogni - tion , pages 586 – 595 , 2018 . 5 , 7 [ 36 ] Xu Zhang , Svebor Karaman , and Shih - Fu Chang . Detecting and simulating artifacts in gan fake images . In 2019 IEEE in - ternational workshop on information forensics and security ( WIFS ) , pages 1 – 6 . IEEE , 2019 . 2 , 5 [ 37 ] Jun - Yan Zhu , Philipp Krahenbuhl , Eli Shechtman , and Alexei A Efros . Learning a discriminative model for the perception of realism in composite images . In Proceedings of the IEEE International Conference on Computer Vision , pages 3943 – 3951 , 2015 . 2 10 6 . Supplementary material In this supplementary , we ﬁrst describe how our Artist - Retouched dataset was constructed , and show represen - tative visual examples from the dataset ( introduced in § 3 . 2 ) . Then , we show the visual comparisons on iHarmony dataset [ 4 ] and report the detailed quantitative compari - son results on four subsets ( HCOCO , HAdobe5k , HFlickr , Hday2night ) . Besides , we provide more high - resolution vi - sual results of different methods on the Artist - Retouched dataset and RealHM benchmark ( supplementary to § 4 . 1 ) . We then go in - depth into our real composite dataset with captured references and present more qualitative visual comparisons ( supplementary to § 4 . 2 ) . Furthermore , we show more visual comparisons of real composite images we used as part of our user studies ( § 4 . 2 ) . Finally , we show more intermediate results and parametric outputs of our method for real - composite image harmonization ( sup - plementary to § 4 . 3 ) . 6 . 1 . Artist - Retouched dataset In this work , we propose to use a new Artist - Retouched dataset for our dual - stream training experiments . Unlike previous work , Artist - Retouched contains image pairs re - touched by artists rather than mostly relying on random color augmentations . Artists were allowed to use global luminosity or color adjustments operations , but also local editing tools like brushes , e . g . , to alter the shading . All the image editing was done using Adobe Lightroom , a software dedicated to photo adjustment . Figure S1 shows representa - tive before - after image pairs in the Artist - Retouched dataset . Artist - Retouched consists of n = 46173 before / after re - touching image pairs { I i , O i } i = 1 , . . . , n , with the foreground mask M i for each pair . As visualized in the ﬁgure , the re - touching procedure consists of global luminosity / color ad - justments ( e . g . , exposure , contrast , Highlights , Temp , Tint , Hue ) and local editing tools ( e . g . , adding shading , creat - ing soft transitions by gradient mask ) . From each triplet { I i , O i , M i } , we can generate two synthetic composites in - puts for training : one with only the foreground retouched M i · O i + ( 1 − M i ) · I i , and the other one with only the background being retouched M i · I i + ( 1 − M i ) · O i . We use the unedited image I i and the retouched image O i as ground truth targets of these composite inputs , respectively . 6 . 2 . More results on iHarmony benchmark As discussed in § 4 . 1 , we evaluate our method on the iHarmony benchmark [ 4 ] and present the quantitative re - sults on the entire dataset . In this section , we report the quantitative results on four subsets of iHarmony — HCOCO , HAdobe5k , HFlickr , Hday2night . We compare our method with DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] . Our method outperforms or matches state - of - the - art ap - proaches in all four subsets of iHarmony benchmark . Ta - ble S1 summarizes the quantitative results . Besides , Figure S2 shows a gallery of selective visual comparisons between different approaches at 512 × 512 resolution . For better visualization , we resize the images to their original aspect ratios . 6 . 3 . More visual results on Artist - retouched dataset As introduced in § 3 . 2 , we evaluate different methods on a testing split of our Artist - Retouched dataset with realis - tic retouches from human experts . In addition to the results shown in Figure 5 , Figure S3 presents more visual com - parisons on Artist - Retouched testing dataset at 1024 reso - lution . We observe that our results agree better with the ground truth images in terms of the visual quality compared to other methods ( DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] ) . Besides , we also show one failure example ( boat ) , where all methods ( including ours ) fail to retrieve the correct color of the ground truth image , though some of them look har - monious by themselves without seeing the reference . We hypothesize that , in this case , the skylight illumination in the ground - truth image is difﬁcult to infer from the back - ground . 6 . 4 . More visual results on RealHM benchmark Different from synthetic dataset [ 4 ] , RealHM [ 13 ] benchmark contains 216 real - world high - resolution com - posites with expert annotated harmonization results as ground truth . In this section , we present more visual har - monization comparisons in Figure S4 at 1024 resolution . From the results , we observe that even though there exist strong foreground / background color mismatches in the real composite images , our method produces more harmonious results compared to other approaches . 6 . 5 . Real composites with captured reference As brieﬂy introduced in § 4 . 2 , for qualitative evaluation , we created a dataset of 40 high - resolution real - composite images with captured references . As illustrated in Fig - ure S5 , we ﬁrst capture a ﬁxed set of foreground objects against multiple backgrounds ( scenes ) , as well as the cor - responding ”background - only” images . We then segment the foreground object of one photo and paste it onto the ”background - only” photo of another with roughly the same location . The captured photo of the same object in the same background scene serves as a reference for qualitative eval - uation . Figure S6 visualize selective examples of the har - monization results . We compare our method with state - of - the - art approaches . As shown in the ﬁgure , for the ﬁrst example , our result shows better visual agreements with the captured reference . For the second and third examples , though our results don’t exactly match the reference ( none of the other methods does ) , our method still produces im - ages with harmonious appearances . We will release this 11 dataset upon publication . 6 . 6 . More results on real composite images Figure S7 shows more visual comparisons on real com - posite images where we don’t have the ground truth or cap - tured reference . We use these images as part of our user studies ( § 4 . 2 ) . We compare our method with DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] . We will release these testing real composite images upon the publication of this work . 6 . 7 . More intermediate results Figure S8 presents more intermediate results and para - metric outputs on RealHM [ 13 ] real - composite benchmark . As shown in the results , our predicted RGB curves harmo - nize the global color / tone , while our learned shading map incorporates local shading to the ﬁnal outputs . By compar - ing with the human - annotated harmonization results ( right ) , we observe that our local shading maps align well with the local operation done by the human experts . For instance , for the top three rows , both our results and the human - annotated ground truth selectively darkened the bottom part of the foreground objects . For the fourth row example , our result highlights the region with incoming light while dark - ening other foreground parts , which agrees with the opera - tions done by human experts . 6 . 8 . Demo video To further better demonstrate the effectiveness of our method in real - world applications , we prepared and recorded a demo video ( see attachments of the supplemen - tary material ) . We can interactively run our demo on a sin - gle CPU without access to extensive computing resources . 12 Exposure : - 1 . 16 Contrast : + 32 … Local editing mask Exposure : - 1 . 80 Contrast : + 75 … Exposure : - 1 . 08 Contrast : + 67 … Temp : + 18 Before retouching After retouching Mask Composite 1 Composite 2 Serve as ground truth of composite 1 Serve as ground truth of composite 2 Figure S1 . Construction of Artist - Retouched dataset . Artist - Retouched dataset contains before / after artist - retouching image pairs { I i , O i } i = 1 , . . . , n with the foreground mask M i for each pair . Artist retouching procedures include both global luminosity / color ad - justments as well as local editing . Local editing masks ( images with red borders ) in the ﬁgure indicate the selective regions where artists perform local editing ( e . g . , shading ) . Two composite images ( Composite 1 and 2 ) are created and used for training from each pair of images . Unedited image I and retouched image O serve as the ground truth for composite 1 and 2 , respectively . Method HCOCO Adobe5k HFlickr Hday2night Entire dataset PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ Composite 33 . 92 0 . 9862 28 . 51 0 . 9563 28 . 44 0 . 9638 34 . 32 0 . 9741 31 . 74 0 . 9748 DovNet [ 4 ] 35 . 76 0 . 9875 35 . 05 0 . 9733 30 . 68 0 . 9711 34 . 83 0 . 9707 34 . 97 0 . 9812 IHT [ 9 ] 38 . 38 0 . 9924 37 . 02 0 . 9819 32 . 84 0 . 9810 36 . 79 0 . 9763 37 . 33 0 . 9877 Harmonizer [ 14 ] 38 . 77 0 . 9936 38 . 97 0 . 9888 33 . 71 0 . 9833 37 . 96 0 . 9813 38 . 25 0 . 9909 Ours 39 . 07 0 . 9940 38 . 53 0 . 9835 33 . 60 0 . 9793 38 . 15 0 . 9817 38 . 30 0 . 9891 Table S1 . Quantitative comparisons on subsets of iHarmony benchmark [ 4 ] at 256 × 256 resolution . We compare our method with DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] . PSNR and SSIM are used as metrics . Red , and Blue correspond to the ﬁrst and second best results . ↑ means higher the better , and ↓ means lower the better . 13 Mask Composite DovNet IHT Harmonizer Ours GT Visual comparisons on iHarmony benchmark Figure S2 . Representative visual comparisons between state - of - the - art harmonization methods on iHarmony benchmark . We compare our method with composite image , DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] , and ground truth . Foreground masks are displayed in the ﬁrst column . For better visualization , we resize the images to the original aspect ratio . Our method shows better visual alignment with the ground truth images than other state - of - the - art methods . 14 Visual comparisons on Artist - retoucheddataset Composite DovNet IHT Harmonizer Ours GT Failure example Figure S3 . More visual comparisons between state - of - the - art harmonization methods on Artist - Retouched dataset . We compare our method with composite image , DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] , and ground truth . Red boxes indicate the foreground mask of the composite images . Our method shows better visual alignment with the ground truth images compared to other state - of - the - art methods . We also present one failure example , where all methods fail to recover the ground truth appearance , though some of them look harmonious without referring to the ground truth . 15 Visual comparisons on RealHM benchmark Composite DovNet IHT Harmonizer Ours GT Figure S4 . More visual comparisons between state - of - the - art harmonization methods on RealHM benchmark . We compare our method with composite image , DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] , and ground truth . Our method shows better color consistency with the ground truth images ( row 1 , 2 , 4 , 6 , and 7 ) and deliver more harmonious results . 16 Scene 1 Scene 2 Scene 3 Paste foreground from scene 2 Composite 1 Captured reference 1 Paste foreground from scene 3 Composite 2 Captured reference 2 Paste foreground from scene 1 Composite 3 Captured reference 3 Figure S5 . Construction of composite images with captured reference . First , we capture the same foreground object against multiple backgrounds ( 3 backgrounds in the ﬁgure ) , as well as the corresponding ”background - only” photos . We then segment the foreground object from one photo and paste it onto the ”background - only” image of another to generate the composite images . The captured photo of the same object in the same background scene serves as qualitative references ( Here , captured references 1 , 2 , and 3 ) . Harmonization results on real composite with captured reference Background Composite DovNet IHT Harmonizer Ours Reference Figure S6 . Real composite harmonization results with captured reference . The composite is obtained by pasting the foreground object from a different photo ( not shown ) onto the background ( left ) . The reference ( right ) is obtained by physically placing the foreground object in the background scene and taking a photo . We compare our method with composite image , DovNet [ 4 ] , IHT [ 9 ] , Harmonizer [ 14 ] , and the captured reference . 17 Visual comparisons on real composite images Composite DovNet IHT Harmonizer Ours Figure S7 . More visual comparisons on real composite images . We compare our method with composite image , DovNet [ 4 ] , IHT [ 9 ] , and Harmonizer [ 14 ] . 18 Intermediate results and parametric outputs Composite RGB curves Intermediate Shading Final results GT ( By human expert ) Figure S8 . Intermediate results and parametric outputs on RealHM benchmark . RGB curves harmonize the global color / tone ( third column ) , while our shading map corrects the local shading in the ﬁnal harmonization outputs ( ﬁfth column ) . Our local shading maps agree well with the local shading operations done by human experts / artists ( right column ) . 19