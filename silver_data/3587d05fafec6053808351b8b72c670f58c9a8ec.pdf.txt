1 Understanding Longitudinal Behaviors of Toxic Accounts on Reddit DEEPAK KUMAR , Stanford University JEFF HANCOCK , Stanford University KURT THOMAS , Google ZAKIR DURUMERIC , Stanford University Toxic comments are the top form of hate and harassment experienced online . While many studies have investigated the types of toxic comments posted online , the effects that such content has on people , and the impact of potential defenses , no study has captured the long - term behaviors of the accounts that post toxic comments or how toxic comments are operationalized . In this paper , we present a longitudinal measurement study of 929K accounts that post toxic comments on Reddit over an 18 month period . Combined , these accounts posted over 14 million toxic comments that encompass insults , identity attacks , threats of violence , and sexual harassment . We explore the impact that these accounts have on Reddit , the targeting strategies that abusive accounts adopt , and the distinct patterns that distinguish classes of abusive accounts . Our analysis forms the foundation for new time - based and graph - based features that can improve automated detection of toxic behavior online and informs the nuanced interventions needed to address each class of abusive account . ACM Reference format : Deepak Kumar , Jeff Hancock , Kurt Thomas , and Zakir Durumeric . 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit . 1 , 1 , Article 1 ( January 2016 ) , 27 pages . DOI : 10 . 1145 / nnnnnnn . nnnnnnn 1 INTRODUCTION Content Warning : This paper studies toxic content online . When necessary for clarity , this paper directly quotes user content that contains offensive / hateful speech , profanity , and potentially triggering content related to sexual assault . Toxic comments—like insults , sexual harassment , and threats of violence—are the top form of hate and harassment experienced online [ 41 ] . Such toxic behavior reduces the emotional safety of targets and audiences who view the content . This can lead users to self - censor to avoid further attacks , leave online platforms altogether , and in some tragic cases , inﬂict self - harm [ 11 , 21 ] . Trans - parency reports from Meta estimate that 0 . 14 – 0 . 15 % of all views on Facebook in 2021 were of toxic posts [ 12 ] , while Twitter reports that it removed roughly two million accounts in the second half of 2020 due to hate and harassment [ 42 ] . Prior research into toxic comments has focused on a variety of themes including the experiences of targets [ 39 , 40 , 44 ] , the characterizations of speciﬁc , large - scale events like # GamerGate [ 8 ] ; early warnings for how toxic conversations escalate [ 47 , 49 , 50 ] , the off - platform coordination tactics for raiding and calls to incite attacks against targets [ 1 , 30 ] , and the impact of intervention techniques such as banning accounts or entire communities [ 5 , 7 , 36 ] . While these studies all paint a rich tapestry Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Request permissions from permissions @ acm . org . © 2016 ACM . XXXX - XXXX / 2016 / 1 - ART1 $ 15 . 00 DOI : 10 . 1145 / nnnnnnn . nnnnnnn , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . a r X i v : 2209 . 02533v1 [ c s . S I ] 6 S e p 2022 1 : 2 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric of toxic behaviors that occur online , none capture the long - term activities of abusive accounts ( i . e . , accounts that post toxic comments ) , such as their toxicity behaviors or their impact on the platform itself . Such analysis is crucial to understanding what interventions—such as nudges , warnings , and bans—might best reduce toxicity , or how automated detection can evolve to encapsulate longitudinal account behavior . In this work , we present the results of a longitudinal , quantitative study of abusive accounts on Reddit that post toxic comments . Over an 18 month period , we identiﬁed 929K abusive accounts that posted 14 million toxic comments , and use this perspective to study three main research questions : RQ1 : What is the aggregate scale and nature of toxic comments and abusive accounts on Reddit ? Abusive accounts that post at least one toxic comment make up 3 . 1 % of all accounts that posted to Reddit during our analysis window , with their toxic comments comprising 0 . 8 % of all content on Reddit . Based on a manual review of abusive accounts and their activities , we estimate that 63 . 4 % of toxic comments are insults , 14 . 2 % are identity - related attacks , and 5 . 5 % are threats of violence , among other classes of toxic behaviors . Toxic comments are highly visible on Reddit : 55 . 2 % of Reddit accounts post directly on a thread with a toxic comment . Unlike automated , fake accounts that solely post spam [ 15 ] , abusive accounts readily engage in non - toxic conversations , contributing an astounding 33 . 3 % of all comments to Reddit . As such , simply banning abusive accounts would have substantial additional consequences to the platform . RQ2 : What are the unique attack patterns ( e . g . , mob - like coordinated attacks on a single individual ) that abusive accounts use when posting toxic comments online ? In graphing the reply relationships between attackers and their 1 . 6M receivers 1 ( i . e . , accounts who received a toxic comment as a reply ) , we observe three classes of attacks . The vast majority of receivers ( 92 . 8 % ) experience spurious , one - off toxic interactions . These receivers of abuse rarely have existing network relationships with their attacker , suggesting that the majority of abuse on Reddit is contextual and not necessarily premeditated . However , the remaining attacks are more pernicious : 7 . 2 % of receivers experience repeated abuse , where a single abusive account continuously attacks the target , often across subreddits . Another 3 . 3 % of receivers experience ﬂooding , whereby a cluster of abusive accounts simultaneously attack the target , akin to coordinated raids . RQ3 : What are the classes of abusive accounts , and how do they inform more nuanced de - fenses against toxic behaviors ? Finally , we cluster the longitudinal behaviors and activities of abusive accounts based on their posting volume , toxicity levels , subreddit participation , and com - munity norm violations . In the process , we identify three distinct classes of attackers . Occasional abusers —accounts that post just a handful of toxic comments—make up 71 % of abusive accounts and 71 % of all toxic comments . This suggests that modest interventions , such as nudges or warnings , may be effective for more than two - thirds of the toxic behaviors on Reddit . Conversely , moderate abusers —accounts that post a substantial volume of toxic comments—make up another 24 % of abusive accounts . Serial abusers —accounts that extensively post toxic comments—make up the remaining 4 . 3 % of abusive accounts . These two latter classes pose a greater threat and require more stringent interventions , however , their volume of toxicity make them potentially easier to take action on . Combined , our ﬁndings illustrate the need for nuanced interventions in tackling varied classes of toxic accounts and unique toxicity patterns . Our analysis can also help to characterize platform - wide upheavals , whereby major world events such as the murder of George Floyd [ 27 ] trigger a wave of increased toxicity across the entire platform ( Section 7 ) . Furthermore , our ﬁndings contribute a 1 Similar to research in intimate partner violence , we intentionally avoid the term “victim” to not disempower people facing abuse . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 3 variety of features , such as reply relationships between abusive accounts , account toxicity trends over time , and community norm violations , that can improve existing toxic behavior detection mechanisms . Such features would transform isolated content - based classiﬁcation into a holistic consideration of an account’s history . To this end , we plan to release anonymized datasets to researchers on request to reproduce our analyses , develop new detection mechanisms , and further explore how toxic behaviors are operationalized online . 2 BACKGROUND AND RELATED WORK In this section , we provide the necessary background and describe prior work that we build on to conduct our analysis . 2 . 1 Accounts that exhibit anti - social behaviors Our study primarily builds on a number of quantitative and qualitative studies of accounts that exhibit anti - social behaviors online , such as trolling , bullying , and toxicity . Early work at CSCW demonstrated that “trolling” is not limited to just a small handful of motivated accounts , but rather that “anyone can become a troll” depending on a number of contextual factors , such as time of day and users’ moods [ 9 ] . Newer studies have focused on the accounts that post toxicity and hate speech and the adversarial nature of toxic interactions . Maity et al . study toxic conﬂicts on Twitter , demonstrating how context and a predisposition to toxic behaviors can cause accounts to become repeat offenders in terms of toxic interactions [ 29 ] . Most similar to our work , Mathew et al . studied longitudinal behaviors of hateful accounts on Gab , a popular fringe social platform for “unregulated speech” , and highlighted how hateful behaviors can grow over time [ 31 ] . Other lines of work have focused on the properties of abusive accounts . For example , Ribeiro et al . studied abuser properties on Twitter with the aim of detecting hateful users based on their previous comments and their place in the social graph [ 35 ] . Their research primarily focuses on properties of accounts , for example , identifying follower - following ratios and account age as a signal for hateful behavior . Beyond abusive behavior in aggregate , several studies have looked at case studies of abuse . For example , Hua et al . identify speciﬁc properties of abusers that adversarially interact with political candidates on Twitter [ 18 , 19 ] . Finally , our work builds off of the methods and techniques of research that has investigated fringe hate groups and online communities , including discourse on Gab [ 31 , 48 ] , Dissenter [ 37 ] , the Manosphere [ 17 ] , and 4chan’s politically incorrect board [ 33 ] . Our study contributes a longitudinal analysis of abuser behaviors across their entire account history on Reddit , which reveal distinct abuse and attack patterns . 2 . 2 Toxicity on Reddit We build off a number of Reddit - focused studies to inform our experimental design and analysis choices . Gilbert documented how a culture of masculinity on Reddit forms a toxic technoculture on r / AskHistorians , leaving both moderators and users subject to abuse anywhere from name - calling to “prolonged harassment , doxxing , death threats , and rape threats . ” [ 13 ] Other studies have focused on how toxicity plays a role in shaping norms of subcommunities on Reddit [ 6 , 34 ] and identiﬁed that subreddits often exhibit unique macro , meso , or micro - norms , highlighting challenges in applying a broad deﬁnition of toxicity throughout the entire platform . Such subcommunity speciﬁc - norms also lead to varied experiences with the precursors and effects of toxic discussions . Xia et al [ 47 ] leverage the Perspective API , a commonly used toxicity detection tool , to study how speciﬁc antecedents of a toxic interaction , such as an accounts’ prior history posting toxic comments and the subreddit context , can play a signiﬁcant correlative role in predicting new toxic behaviors . Finally , several studies have focused on shifts in toxicity both on - platform and off - platform due to cross community , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 4 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Collect Comments Rate Toxicity Filter Comments Aggregate Abusers reduce thresholds A C D E Filter Comments language + content length B remove over18 Subreddits Fig . 1 . Reddit Processing Pipeline —We label Reddit comments sourced from Pushshi  through the Per - spective API . We explicitly filter out comments that are not in English or are from subcommunities tagged as 18 + . We leverage these classifications to identify the 14M toxic comments and 929K abusive accounts we study . movement following notable bans [ 5 , 36 ] . Our study contributes distinct classes of abusive accounts on Reddit while taking into account both a global and subcommunity - speciﬁc set of longitudinal toxicity norms . 2 . 3 Defenses and mitigation strategies Our work is grounded in prior work in studying defenses against online toxicity . The CSCW community has contributed several design recommendations , including nudges [ 23 ] , providing realtime feedback on toxicity [ 47 ] , foregrounding norms [ 34 ] , and outright permanent bans [ 24 ] . Other research has focused on predicting potentially toxic behavior based on early warning signs in conversation [ 49 , 50 ] , and how such signs from conversation ﬂow can aid in forecasting personal attacks [ 22 ] . Similarly , our work builds on recent research from the computer security community , which has recently drawn parallels between classic cybersecurity problems ( e . g . , for - proﬁt cyber - crime ) and anti - social online abuse [ 41 ] . Toxic content bears the closest resemblance to spam , in that it is often unwanted content with harmful consequences to platform users . As such , our defensive recommendations and techniques also build on work on measuring and mitigating spam [ 15 , 28 ] and prior work that study graph - based spam propagation [ 32 ] . Our study contributes additional context to existing proposed defenses by outlining which defenses may be most effective for distinct classes of abusive accounts . Our results also motivate new graph - based and time - based features for toxicity detection , which can serve as the groundwork for future research in toxicity detection . 3 METHODOLOGY In this section , we detail our methodology for capturing a longitudinal corpus of 2 . 2 billion Reddit comments as well as our classiﬁcation techniques and thresholds for isolating abusive accounts . Figure 1 shows each step in our data collection pipeline , which we use to guide our discussion . 3 . 1 Collecting Reddit comments Our dataset consists of 18 months of comments posted on Reddit between January 2020 and June 2021 . We collected a total of 2 . 2 billion comments via Pushshift , a third - party API that aggregates Reddit comments and posts ( Figure 1 . A ) [ 3 ] . Each comment includes a timestamp , the username of the author , the subreddit ( i . e . , community ) where the comment appeared , and graph data that allows us to identify if the comment was a top - level thread ( i . e . , the author was the original poster ) , or a reply to an existing thread . From this , we re - constructed a history of posting behavior and interactions with other accounts . At the time of writing , this constitutes the most recent data available from Pushshift . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 5 3 . 2 Filtering comments Prior to classiﬁcation , we restricted our dataset to English comments using the “whatlanggo” Go library . 2 This was justiﬁed in part to enable manual analysis by the researchers , as well as due to limitations with toxicity classiﬁcation ( described in Section 3 . 3 ) , where existing models are trained primarily on English text . We further omitted comments that were less than 15 characters , or more than 300 characters in length , which is aligned with prior research on limitations of existing toxicity models for short and very long text [ 25 ] . These ﬁlters reduced our corpus to 1 . 8B comments , 32 . 1M accounts , and 845K unique subreddits . 3 . 3 Identifying toxic comments We classiﬁed the toxicity of each comment ( Figure 1 . C ) using the Perspective API , 3 a set of out - of - the - box toxicity classiﬁers from Google Jigsaw , which has been used extensively in prior research [ 18 , 18 , 38 , 47 ] . The Perspective API takes a comment as input and returns a score from 0 – 1 for several classiﬁers ( e . g . , profanity , threats , identity attacks , general toxicity ) . As such , we had to select both a classiﬁer ( s ) and a threshold to identify whether a given Reddit comment is toxic . Because the Perspective API is not explicitly trained on Reddit data , we took an additional calibration step in order to identify the best classiﬁcation thresholds for our study . To identify the best model and threshold for our context , we leveraged a public dataset that contains crowdsourced toxicity ratings for 16K Reddit comments [ 25 ] . The dataset consists of ratings from ﬁve - participants , who rated each Reddit comment on a 5 - point Likert scale from “not at all toxic” to “very toxic” . We consider a comment to be toxic by raters if the median score across all raters was “moderately toxic” or higher . We swept over each Perspective API model and threshold value ( from 0 – 1 ) , meaning we evaluated the precision the classiﬁer achieved at each threshold ( 0 . 0 , 0 . 01 , 0 . 02 , etc . ) when compared to participant labels as shown in Table 1 . Only the SEVERE TOXICITY classiﬁer achieved a precision of 0 . 75 at a threshold of 0 . 9 . As such , when identifying toxic comments , we ﬁlter based on if a comment has a SEVERE TOXICITY score > 0 . 9 . We note that we intentionally favor precision over recall , choosing to trade comment volume for data quality . This will underestimate the total volume of toxic comments in our corpus . One limitation with this public dataset is that it intentionally undersamples non - toxic comments , and thus may not reﬂect the real precision of our threshold when applied to a truly random sample of Reddit comments . We account for this bias by manually verifying the precision of our pipeline after all classiﬁcation and ﬁltering stages , in Section 3 . 5 . This dataset reﬂects a random sample of all subcommunities at Reddit at different strides of the Perspective API , and serves as a representative sample of Reddit comments to compare against . 3 . 4 Removing 18 + subreddits As part of our manual validation of the pipeline , we observed that many comments ﬂagged with high toxicity scores were sexually explicit and sourced from subcommunities that are tagged as 18 + . These subcommunities are often sexual and consensual in nature , and do not constitute the types of attacks we are interested in studying ( e . g . , bullying , threats , identity attacks , or sexual harassment ) . For example , subreddits like r / gonewild exist for adult participants to share consensual nude images , and posts often intentionally solicit sexual replies . In many of these cases , the classiﬁer’s high toxicity score likely does not match the intention of these community norms , and including them as toxic comments would likely negatively tag benign accounts . As such , we chose to explicitly exclude subcommunities that are tagged as 18 + from our study . This ﬁltering step removed 79M 2 https : / / github . com / abadojack / whatlanggo 3 https : / / perspectiveapi . com , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 6 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Classiﬁer Threshold Precision F1 IDENTITY ATTACK 0 . 9 0 . 62 0 . 02 INSULT 0 . 9 0 . 53 0 . 11 TOXICITY 0 . 9 0 . 51 0 . 24 SEVERE TOXICITY 0 . 9 0 . 75 0 . 02 THREAT 0 . 9 0 . 43 0 . 06 Table 1 . Optimal Perspective API Thresholds for Ground Truth Toxic Comments —The thresholds that maximize precision for each Perspective API classifier on Reddit are all 0 . 9 or higher , with only one classifier , SEVERE TOXICITY , achieving an acceptable precision for our measurements . Threshold Nonabuser Prec . Abuser Prec . Comment % 0 . 5 0 . 19 0 . 56 64M ( 3 . 9 % ) 0 . 7 0 . 22 0 . 68 27M ( 1 . 7 % ) 0 . 8 0 . 29 0 . 72 14M ( 0 . 8 % ) 0 . 9 0 . 0 0 . 79 1 . 7M ( 0 . 09 % ) Table 2 . Identifying Thresholds for Comments From Abusive Accounts —Reducing the toxicity thresh - old for comments posted by abusive accounts increases the volume of comments available to analyze while maintaining overall precision . ( 3 . 6 % ) of total comments . We stress that while harassment may occur in these subcommunities , including them and unintended false positives would negatively affect the quality of our results across the platform given our longitudinal , platform - wide approach to measurements . 3 . 5 Aggregating abusive accounts In the ﬁnal stage of our methodology , we leveraged our corpus of labeled , ﬁltered comments to identify “abusive” accounts ( Figure 1 . E ) . We categorized an account as abusive if it ever posted a comment with a SEVERE TOXICITY score > 0 . 9 , yielding our ﬁnal dataset with a total of 929K abusive accounts . We evaluated increasing the threshold ( i . e . , requiring abusive accounts to post n comments between 1 – 5 ) , but found it did not change our results ( Appendix A ) . However , by only considering comments from these abusive accounts that meet our strict threshold of 0 . 9 , we signiﬁcantly reduce the volume of comments available to analyze to just 1 . 7M ( 0 . 09 % ) comments . Given this potentially low comment volume , we next examined whether we could adopt a lower threshold for comments , conditioned on the account having posted at least one comment with high toxicity . 4 Our hypothesis was that if an account engages in toxic behavior , it is likely that some of their other comments may also be toxic . To evaluate this hypothesis , we randomly sampled 200 comments from abusive accounts and 200 comments from nonabusive accounts at each of four severe toxicity thresholds : 0 . 5 , 0 . 7 , 0 . 8 , and 0 . 9 . 5 We manually labeled each comment for toxicity and measured how well each threshold performed on our manual sample ( Table 2 ) . 4 We demonstrate how stricter thresholds ( e . g . , posting at least three highly toxic comments ) has little effect on the results in Appendix A . 5 For this experiment , two expert raters classiﬁed each comment as toxic or not using the deﬁnition provided by Google Jigsaw : “a rude , disrespectful , or unreasonable comment that is likely to make you leave a discussion . ” This avoided the stratiﬁed sampling bias present in the labeled Reddit corpus . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 7 For comments from known nonabusive accounts , all thresholds performed poorly , reaching only a maximum precision of 0 . 29 at a decision threshold of 0 . 8 ( nonabusive accounts posted no comments higher than 0 . 9 by deﬁnition ) . For comments posted by known abusive accounts , performance was signiﬁcantly higher , achieving a 0 . 72 precision at a threshold of 0 . 8 , and a maximum precision of 0 . 79 at a threshold of 0 . 9 . Based on this analysis , we expand our corpus of toxic comments to include all comments—predicated on originating from an abusive account—that meet a threshold of SEVERE TOXICITY score > 0 . 8 . After this secondary threshold , our ﬁnal dataset consists of 929K abusive accounts and 14M toxic comments , as well as 28 . 9M nonabusive accounts and 1 . 6B nonabusive comments . 3 . 6 Limitations We caution that our strategy for ﬁltering and classiﬁcation is not a perfect indicator of toxic behaviors on Reddit . Part of the motivation for this work is to identify contextual features that might improve toxicity classiﬁcation , and yet , in order to do so , we need to begin with what existing classiﬁers are able to identify . We may omit some toxic behaviors due to false negatives , and given that our ﬁnal per - comment precision is 0 . 72 , we may also include some nonabusive comments in our ﬁnal dataset . We stress that online hate and harassment is a nascent problem , made additionally challenging by differing opinions on what constitutes toxic content [ 14 , 25 ] . Our ﬁnal precision numbers are consistent and at times times , stricter with prior research in this area [ 18 , 19 , 34 , 38 , 47 ] , and , as we ﬁnd , still provide signiﬁcant signal for large - scale measurement analysis . 3 . 7 Ethical considerations Without proper care , targets of abuse or the abusers themselves might be inadvertently harmed by our study . To mitigate these risks , we never interact with accounts , we never attempt to deanonymize receivers of abuse or the abusive accounts themselves , and we never report accounts to the platform due to the risk of unintended false positives . Furthermore , we note that our dataset is constructed off of an existing third - party source ; we are only augmenting this existing dataset with toxicity labels using a standard approach ( Perspective API ) which is publicly available . We plan to release our labeled datasets to researchers by request , and we will remove any personally identiﬁable information ( e . g . , account names ) before release . 4 RQ1 : THE SCALE AND NATURE OF TOXICITY ON REDDIT We begin with an aggregate analysis of the 929K abusive accounts and 14M toxic comments they post to Reddit . We study the volume of toxic content , the subcommunities toxic content appears in , and the most prevalent types of toxic content ( e . g . , bullying , identity attacks , threats ) . 4 . 1 Toxicity across the platform Accounts that post at least one toxic comment represent 3 . 1 % of all accounts that post comments on Reddit . Despite their relatively small presence compared to nonabusive accounts , abusive accounts play an active and outsized role on the platform . Abusive accounts post 559M comments during our study period , which amounts to 33 . 3 % of all comments posted to Reddit during that time . Of these comments , 14M ( 2 . 9 % ) were toxic . Across all of Reddit , 0 . 8 % of all comments are toxic . We note that while abusive accounts do post a large volume of comments , simply posting signiﬁcant comments does not correlate with toxic behavior ( r = 0 . 01 , p < 0 . 01 ) , highlighting that the proclivity to post toxic comments is not simply a product of heavily using the platform . The harm caused by toxic comments is ampliﬁed when viewed by thousands of other users that engage with threads , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 8 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Fig . 2 . Subreddit Hotspots —We show subreddits by the number of abusive accounts as well as the size of the subreddit . Some subreddits ( i . e . , those in the red band ) serve as hotspots for toxic behaviors , hosting both high fractions of abusive accounts and large amounts of toxic content . where the toxic comments appear . In total , 15M accounts ( 55 . 2 % ) participate directly in a thread where a toxic comment is posted , suggesting that toxic comments are not easily avoided . 4 . 2 Toxicity per individual subcommunity A signiﬁcant number of subreddits are affected by abusive accounts and toxic comments : 146 , 831 ( 63 . 4 % ) subreddits have participation by abusive accounts , of which 51K ( 22 % ) contain at least one toxic comment . These subreddits are typically the ones with the most activity , and 100 % of highly active subreddits ( i . e . , more than 100K comments ) contain toxic comments . Figure 2 shows the fraction of abusive accounts in subreddits , with the gradient representing the fraction of comments in those subreddits that are toxic and the line at y = x indicating the subreddit consists solely of abusive accounts . Subreddits that fall into the red band are hotspots of toxic activity , which tend to be smaller subreddits that contain tens or hundreds of unique accounts ( i . e . , the bottom left corner of the graph . ) Such subcommunities tend to serve a niche user base ( e . g . , r / FortniteBad , which is a subcommunity where accounts share hateful memes targeted towards the video game Fortnite ) , and in 1 % of cases , consist entirely of abusive accounts . However , even some highly active subreddits can consist of upwards of 30 % toxic content , highlighting that even outside of hotspots of toxic activity , toxic comments are a potentially pervasive part of Reddit . 4 . 3 Toxicity trends over time Toxic comments are regularly posted to Reddit , accounting for between 0 . 75 – 1 . 05 % of all comments in any given week ( Figure 3 ) . All abusive behaviors increased over time during our study period : in particular , the raw volume of toxic comments and their relative presence compared to non - toxic comments has increased , which we conﬁrm with a Mann - Kendall trend test ( p < 0 . 01 ) . The raw and relative volume of active abusive accounts also increased over time . We note that there are four spikes in both raw and relative volumes of toxic behavior in our corpus . The ﬁrst is between May 26 and June 5 2020 , which we manually conﬁrmed related to posts covering the murder of George Floyd [ 27 ] . The second spike of toxic comments occurred in August , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 9 2020 - 01 2020 - 03 2020 - 05 2020 - 07 2020 - 09 2020 - 11 2021 - 01 2021 - 03 2021 - 05 2021 - 07 date 10K 100K 1M 5M N u m be r o f C o mm en t s Total comments Comments by Abusers Toxic Comments ( a ) Number of Comments by Week ( b ) Fraction of Comments by Week Fig . 3 . Toxic Behaviors on Reddit Over Time —Toxic comments are increasing slightly over time , and account for an average of 0 . 8 % of all comments during our observation period . Toxic comments spike several times during our study period , typically in response to real - world events that spur significant discussion , like the murder of George Floyd in May 2020 or the January 6th US Capitol insurrection in January 2021 . 2020 , which was due to an automated counter - campaign against a bot that aimed to facilitate kinder language on Reddit . The third spike period occurred on January 6th , 2021 , with toxic posts largely relating to the insurrection at the US Capitol . 6 The ﬁnal spike in toxic comments came at the end of January 2021 , and directly related to the r / WallStreetBets takeover of the Gamestop stock on Reddit . 7 As a case study , we examine the ﬁrst spike period in Section 7 . 4 . 4 Toxic comment breakdown We manually investigated a random sample of 500 toxic comments , which we coded into several categories of toxic behavior . We labeled each comment based on hate and harassment categories identiﬁed in prior work [ 2 , 25 ] : doxxing , identity attacks , identity misrepresentation , insults , sexual aggression , threats of violence , and profanity . We added one additional category we ﬁnd particularly prevalent on Reddit , a “call to leave conversation” , which typically involves the attacker telling the target to leave the conversation , subreddit , or subcommunity . We excluded comments from our analysis that were not relevant ( e . g . , a false positive or general negative sentiment ) . Table 3 shows the breakdown of attacks per category . The majority of attacks on Reddit are insults ( 63 . 4 % ) , which are typically provided in response directly to a previous commenting account or a reply to the original poster themselves . For example , in the subreddit r / ShitLiberalsSay , a community designed to mock liberal opinions , one account wrote : “I don’t know what’s more salty . Your mouth or your asshole . Not like there is a big difference between them in your case , diarrhea and entitlement come out of both ends , and they’re both just as pathetic . ” Attacks also fall into a host of other categories , including identity attacks ( 14 . 2 % ) , calls to leave the conversation ( 12 % ) , and threats of violence ( 5 . 5 % ) . One example of a call to leave is : “ . . . Get the fuck off this subreddit . You clearly don’t really care about how severe NVLD can be . There’s literally no fucking help out there for us . ” 6 https : / / apnews . com / article / capitol - siege - police - riots - congress - c632472d5e11063611b4a902859d49fb 7 https : / / www . theverge . com / 22251427 / reddit - gamestop - stock - short - wallstreetbets - robinhood - wall - street , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 10 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Category of Attack % Comments Std . Error Insult 63 . 4 % 2 . 2 % Identity Attack 14 . 2 % 1 . 6 % Call to Leave 12 . 0 % 1 . 5 % Threat 5 . 5 % 1 % Sexual Aggression 2 . 8 % 0 . 7 % Identity Misrepresentation 1 . 6 % 0 . 6 % Doxxing 0 % Targeted Toxicity 44 . 8 % 2 . 2 % Generalized Toxicity 55 . 2 % 2 . 2 % Table 3 . Types of A  acks on Reddit —A  acks fall largely into two categories—a  acks on authors or a  acks on nonauthors . A  acks on authors are largely insults or calls for the participant to leave the community , whereas nonauthor a  acks focus on larger identities ( e . g . , racial , political , etc . ) but are not explicitly against the author of a post or comment . Identity Referenced Fraction Comments Std . Error Political 50 % 9 . 4 % Racial 21 % 7 . 7 % Sexual Orientation 14 % 6 . 6 % Religion 11 % 5 . 8 % Gender 3 % 3 % Table 4 . Identity - based A  acks —Identity based a  acks on Reddit span a wide variety of identities , including politics , race , gender , and religion . Political a  acks make up the largest fraction of identity a  acks we labeled ( 50 % ) , however , racial a  acks and sexual orientation a  acks are also highly prevalent . We did not observe any instances of doxxing in our analysis . However , this is likely due to our labeling and sampling mechanism , as well as our limited manual sampling ( only 500 comments ) . More nuanced attacks like doxxing may need ﬁner grained tools for identiﬁcation [ 1 ] . 4 . 5 Targets of toxic comments During our manual analysis , we observed two classes of the targets of toxic comments : targeted toxicity ( i . e . , a comment directly related to the original poster or in reply to a comment ) , or generalized toxicity ( i . e . , toxicity towards a broad group of people or a public ﬁgure ) . Generalized toxicity is most prevalent ( 55 . 2 % ) , and typically skew towards identity - based attacks ( 22 . 4 % ) . This is signiﬁcantly larger than identity attacks launched directly on authors ( 6 . 2 % ) , and highlights the prevalence of undirected hate - based rhetoric on the platform . Many such attacks target race , gender , sexual orientation , political afﬁliation , and a myriad of other identities . For example , on a post from r / TheNewRight , a subreddit previously dedicated to extremist right - wing content before it was banned , one account commented : “They can make a better life for themselves in their own countries instead of coming into mine and turning it into just as much of a shithole as the shitholes they come crawling out of . ” , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 11 To better characterize the types of identities that abusive accounts target , we labeled each identity attack comment with the identities referenced , namely , attacks on political views , sexual orientation , racial identity , religion identity , or gender identity ( Table 4 ) . Political attacks on Reddit are most common , accounting for 50 % of all identity - based attacks . This largely follows prior research where survey participants noted that the most prominent reason they were attacked online were due to their political beliefs [ 45 ] . Racial attacks and attacks on sexual orientation are the next most prevalent , accounting for 21 % and 14 % of identity based attacks each . Explicit religious or gender - based attacks were the least common on the platform , accounting for just 11 % and 3 % of attacks respectively . Our manual sampling reveals that toxic comments on Reddit span a variety of types of hate and harassment , tactics , and targets . Ultimately , our analysis reveals a host of varied toxic behaviors that are highly prevalent and visible across the platform . 5 RQ2 : TOXICITY PATTERNS IN THE REPLY GRAPH Our analysis thus far has focused on abusive accounts and their individual toxicity behaviors . However , these behaviors rarely happen in isolation—many comments ( 56 % ) are sent in reply to other comments , creating an underlying social structure that connects accounts to one another across the platform . Understanding these relationships can provide deeper insights into toxicity patterns , how toxic comments are operationalized , and ultimately how toxic comments are experienced by their receivers ( i . e . , those that receive a toxic reply in response to their own comment ) . We examine the structure of these relationships by studying two social graphs based on the underlying reply - graph embedded in Reddit : one based on posting relationships between all types of accounts , and one based solely on toxic interactions between abusive accounts and the receivers of their abuse . 5 . 1 Building reply graphs To study latent relationships between accounts , we construct reply graphs which link participants if they interact directly with one another . Speciﬁcally , we build a weighted , directed graph G n = V n , E n where the vertices V n are Reddit accounts and a directed edge e ∈ E n represents if an account posts a response directly to another account . Edges are weighted based on repeated interactions between accounts . Using this graph construction as a baseline , we build two distinct graphs which we leverage for our analysis : ( 1 ) G 1 is generated from the entirety of the Reddit graph . Vertices V 1 represents all accounts that post a comment on the platform . This graph captures all reply - relationships throughout the posting period . ( 2 ) G 2 is generated from solely toxic interactions . Vertices represent both abusive and nonabu - sive accounts . Edges represent a directed , toxic comment - level interaction between accounts , which means an abusive account posted a toxic comment in response to a receiver account . 5 . 2 Understanding reply relationships To ﬁrst understand reply - relationships formed by abusive accounts , we investigate the structure of G 1 , which captures the full Reddit connectivity graph . The graph contains 22 . 2M accounts and 631M edges , of which 890K ( 4 % ) are abusive accounts and 21 . 3M ( 96 % ) are nonabusive accounts . We note the remaining 39K abusive accounts in our dataset only posted toxic comments as top - level replies to posts and never directly engaged with any other account on the platform . We compare reply - relationships formed by abusive accounts and nonabusive accounts to identify if abusers form fundamentally different relationships on the platform compared to nonabusive accounts . We measure these in three ways : the raw number of connections , the weight of those connections , and ﬁnally , whether accounts tend to form connections to similar accounts ( i . e . , the graph exhibits homophily ) . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 12 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric ( a ) Inward Edges ( b ) Inward Edge Weight Fig . 4 . Abuser and Nonabuser Connections —Abusive accounts form more connections than nonabusive accounts , largely due to their increased activity on the platform at large . However , both accounts share in identical average edge weight ( median 1 . 0 ) , suggesting that most connections formed by both types of accounts are spurious , one - o  interactions . Abusive Accounts Nonabusive Accounts 39 % 29 % 71 % 61 % Fig . 5 . Connectivity of Abusive and Nonabusive Accounts —Abusive accounts regularly interact with other abusive accounts on the platform , with abusive links making up for 39 % of their overall interactions . Conversely , nonabusive accounts interact less with abusive accounts , with abusive links making up an average of 29 % of their connections . Abusive accounts interact heavily . Abusive accounts interact with signiﬁcantly more accounts compared to nonabusive accounts ( Figure 4 ) . Abusive accounts have a median of 67 inward connections ( e . g . , replies from other accounts ) and 63 outwards connections ( e . g . , replies to other accounts ) compared to nonabusive accounts , who have a median 3 inward and outward connections . This aligns with our previous observation that abusive accounts are more active on the platform ( Section 4 ) and adds additional context that abusers are also engaging in more conversations online , potentially leading to an increased likelihood of a toxic interaction . Most reply relationships are spurious . Although abusive and nonabusive accounts form vastly different numbers of connections , they are similar in how deep those connections are . To measure this , we study connection weight , which is the number of times that an account repeatedly interacts with another account . Of the 169M outward edges created by abusive accounts , 147M ( 86 . 9 % ) have an edge weight of 1 , which are interactions that only occur a single time during our observational period ( Figure 4b ) . This is consistent with nonabusive accounts , for which 89 % of outward edges have an edge weight of 1 . The majority of connections on Reddit , regardless of account type , are spurious , and are rarely repeated on the platform . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 13 Abusive Responders Nonabusive Targets AbusiveTargets 70 . 6 % targets 28 . 7 % targets 61 % reciprocal 38 . 8 % reciprocal Fig . 6 . Toxic Interactions —Receivers of abuse can be nonabusive accounts ( 70 . 6 % of accounts ) or abusive accounts ( 28 . 7 % ) , highlighting a dual role that abusive accounts play in toxic interactions . Abusive accounts are more willing to extend toxic interactions on the platform than nonabusive accounts , with 61 % of abusive accounts responding to a toxic comment compared to 38 . 8 % of nonabusive accounts . Homophily is low . All accounts at baseline exhibit small amounts of homophily , which is a tendency to connect with more similar accounts than dissimilar accounts . In this context , this means that abusive accounts tend to form reply - relationships with other abusive accounts , and nonabusive accounts tend to form reply - relationships with other nonabusive accounts . To capture this , we compute the assortativity coefﬁcient , which takes a value from - 1 ( indicates accounts only interact with dissimilar accounts ) to 1 ( indicates accounts only interact with similar accounts ) . The coefﬁcient value is r = 0 . 05 , which is a small but signiﬁcant tendency for similar accounts to connect with one another , suggesting that abusive behavior alone does not explicitly connect similar accounts together . As such , connections to abusive accounts are similar when comparing abusive and nonabusive accounts ( Figure 5 ) . Abusive accounts have on average 39 % of their overall connections made up by other abusive accounts . This is marginally higher than the average fraction of connection that abusive accounts make up for nonabusive accounts ( 29 . 6 % ) . 5 . 3 Abusers and receivers We contrast our analysis of the aggregate reply - graph ( G 1 ) with G 2 , which focuses strictly on toxic interactions stemmed from abusive accounts . In the creation of this graph , we ﬁltered out any interactions where accounts replied in a toxic manner to themselves , which accounted for 0 . 8 % of toxic interactions . The resultant graph contains 1 . 8M vertices and 4 . 1M edges , of which 1 . 6M ( 89 . 3 % ) are receivers and 651K ( 36 . 3 % ) are abusers . 8 We focus our attention on the roles that accounts play and the behaviors they exhibit when engaging in toxic interactions . Abusive accounts play dual roles . The majority of toxic comments are sent towards nonabusive accounts , who make up 71 . 3 % of all receivers . However , we note that abusive accounts can play the role of both an abuser and as a receiver of abuse ( Figure 6 ) —28 . 7 % of receivers are abusive accounts , and 460K ( 70 . 6 % ) of abusive accounts play both roles in G 2 . This aligns with our earlier result that 8 We note this does not sum to 100 % because some abusers are also receivers of abuse . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 14 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric 0 % 20 % 40 % 60 % 80 % 100 % Percent of abusers that target has a prior relationship with 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 CD F T a r ge t A cc oun t s Target is Abusive Account Abuser Nonabuser Fig . 7 . Previous Relationships with Abusers —75 % of receivers do not have pre - existing relationships with their abusers . In contrast to this , 40 % of abusive accounts have pre - existing network relationships with their abusers , suggesting again that abuser - to - abuser communication may predicate future toxic interactions . many abusive accounts regularly interact with other abusive accounts on the platform . This dual role also highlights an underlying challenge with defending against toxic content , as account - level interventions may inadvertently harm abusive accounts when they are also receivers of abuse . Abusive accounts reply to toxic comments . 47 . 4 % of toxic edges have a reciprocal connection , which means the receiver of the toxic comment replied to the original comment ( Figure 6 ) . We count both toxic and non - toxic replies as a reciprocal edge . Many abusive accounts engage in discussion with other accounts , especially when the interactions are toxic . 421K ( 54 % ) of abusive accounts have reciprocal edges—this is notably different than considering behaviors in G 1 , where abusive accounts largely interacted only a single time with other participants . When abusive accounts respond to toxic comments , some ( 18 . 1 % ) will occasionally respond with a toxic comment ( 23 . 3 % of interactions ) , potentially escalating the toxicity of the conversation with their reply . For example , when one abusive account posted : “I just partially agreed with you , can you not read ? I just said if Trump knew about his acts then he should of spoken about it . There is no evidence that trump is a rapist and a pedo you fucking retard , present it me ? ” The receiving abusive account replied : “He’s literally admitted it multiple times . You pedo apologists are fucking sick . You’re blocked . I don’t talk to pedophiles or rapists or people fucked enough to try to lie for them . ” Our results suggest that only a minority of abusive accounts respond in a toxic manner , adding nuance to the types of accounts that may choose to instigate toxic interactions . Most toxic interactions are one - offs . The overwhelming majority of toxic interactions on Reddit are one - offs , meaning they occur one time between an abusive account and a receiver account and never occur again . Of the 4 . 1M abusive comments posted by an abusive account in response to receiver accounts , 95 . 6 % are one - offs , which is larger than the similar fraction measured from G 1 ( 83 . 2 % ) . Toxic interactions are thus often ﬂeeting and one - off occurrences on the platform . Some abusive accounts have pre - existing relationships with abusive accounts . We next iden - , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 15 Fig . 8 . Receiver Experiences —Receivers experience three distinct types of toxicity : spurious abuse ( e . g . , a single abusive accounts replies to a single comment ) , repeated abuse ( e . g . , a small handful of accounts repeatedly harass a target through their posting history ) , or flooding ( e . g . , one comment triggers many toxic replies ) . The line at y = x denotes an equal number of abuser and posts that trigger toxic replies . While 85 . 8 % of receiver accounts fall on the line , 14 . 2 % experience either flooding or repeated abuse . tify whether receivers have existing network relationships with abusive accounts prior to a toxic encounter , which may inform if underlying network features can help to predict toxic behaviors . 75 % receivers have no existing relationship with any of the abusive accounts that target them . This value is signiﬁcantly lower , however , when considering abuser receivers—40 % of abusive accounts have a pre - existing , non - toxic relationship with their abusers . As such , toxic interactions between abusers may be predicated by previous interaction on the platform ( Figure 7 ) . Excluding one - off abusive interactions paints a different picture , which naturally increases the number of pre - existing relationships between receivers and their abusers . When only considering receivers with more than one abuser , 44 % have a pre - existing relationship with at least one of their abusers , and 53 % of abuser receivers have a pre - existing relationship with their abusers . These underlying relationships may be useful when automatically detecting new , abusive accounts . 5 . 4 Receiver experiences Despite the fact that the majority of interactions between abusive accounts and receiver accounts are one - offs , many receivers experience many different types of toxicity . 59 . 5 % of receivers experience just one abusive interaction from one abusive account in G 2 , however , 40 . 5 % of receiver accounts experience multiple abusive encounters during their time on the platform . To better explain these experiences from a receiver perspective , we measure receiver experiences by two criteria : the number of unique abusers that send toxic comments to the receiver , and the number of posts that lead to a toxic reply ( Figure 8 ) . The line at y = x indicates an equal number of abusers and toxic interactions . In doing this , we illuminate three distinct toxicity patterns : spurious abuse , repeated abuse , and ﬂooding . We detail each below : Spurious Abuse . 92 . 8 % of receiver accounts experience spurious abuse , meaning each toxic interaction that they encounter comes from a distinct abusive account . These accounts are those that fall on the line at y = x in Figure 8 . Such experiences are spurious in nature and have more to do with the content of the discussion rather than speciﬁc toxicity directed towards the receiver account , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 16 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric themselves . Protecting these receivers from unwanted toxicity is the most challenging , as they often have no prior relationships with their abusers and attacks may happen without any explicit warning . Repeated Abuse . A signiﬁcant number of receiver accounts ( 114K , 7 . 2 % ) experience repeated abuse , which are repeated toxic comments that come from the same abusive account . In Figure 8 , these accounts fall below the line at y = x . These are accounts whose comments regularly trigger a toxic reply , but are targeted by a smaller group of abusive accounts . Most alarmingly , abusive accounts seek out and repeatedly harass 5700 ( 0 . 5 % ) of receivers across different subredddits . As an example of this type of abuse , in the subreddit r / Syracuse comments , we observe one account repeatedly harassing another account for their support of then US President Donald Trump across 20 distinct threads . The abusive account regularly antagonizes the receiver account for their beliefs and refers to the account in the second person , tacitly acknowledging the abuse : Yes , “news” must never be shared . Journalists must go out and ﬁnd their own “news” . Pffft , Are you fucking retarded ? Do you know how stupid you sound ? Of course , it’s to be expected from you . In one explicit instance , the abusive account even references their excitement to getting to abuse the receiver account again : thanks for coming back for another ass kicking . It’s Just so damn easy . LMAO , Back to the trailer . Just end it . . . Repeated abuse has a distinct behavioral footprint from the majority of toxic interactions on Reddit . As such , it may be easier to defend against as it can be more readily identiﬁed on a platform level . Despite this , we are aware of no existing proactive protections for repeated abuse on Reddit . Flooding . In contrast to those that experience repeated abuse , receiver accounts that fall above the line at y = x in Figure 8 experience ﬂooding , where a single comment may trigger many abusive interactions in reply . In our dataset , 53K ( 3 . 3 % ) receivers experience ﬂooding , with the most ﬂooded receiver experiencing 74 toxic replies to a single comment . In one such case , an account posted in the subreddit r / JusticeServed expressing skepticism about Covid - 19 vaccines , stating : Leaving this up since it’s high karma , but the entire world needs to have a good look at vaccines . I’m not quite sure how they work but putting things like that in your body cannot be good for you when herbal remedies have worked for thousands of years . People need to start thinking for themselves and stop blindly trusting so called “masters of medicine” . This comment was met with 74 distinct abusive accounts berating the author , insulting their intelli - gence , and in some cases , wishing for the author’s death—as an example : My God you’re a cunt . In a way I hope people like you stay anti - vaxx , eventually you’ll die out . These types of ﬂooding attacks impact a signiﬁcant number of receivers and happen almost in real - time , rendering post - hoc moderation limited in its effectiveness . 6 RQ3 : CATEGORIZING ABUSIVE ACCOUNTS Abusive accounts often exhibit distinct toxicity behaviors on Reddit . Our goal is to understand these behaviors and leverage them to build abuser personas , which are groups of abusers that share behavioral traits . Such personas can help to inform more effective defensive mechanisms or platform design choices . We focus on three distinct toxicity behaviors that we then use to build personas : abusive accounts’ toxicity behaviors on the platform in aggregate , their absolute toxicity behaviors , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 17 0 . 1 % 1 % 10 % 100 % Percent of Comments By Account that are Toxic 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 CD F A bu s i v e A cc oun t s Toxicity in aggregate Fig . 9 . Abuser Behaviors by Toxic Comments —We show a CDF of the fraction of toxic comments abusive accounts post in aggregate on the platform and the median amount from their subcommunities . Toxic comments account for a median 2 . 9 % of all abuser comments . 0 20 40 60 80 100 Percent of Subreddits 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 CD F A bu s i v e A cc oun t s Subreddits with toxic comments Subreddits with violated norm Fig . 10 . Abuser Behaviors by Subcommunity —Abusers post toxic comments in a median 13 % of their subcommunities and violate the toxicity norms of a median 16 . 6 % of their social homes . Accounts exhibit di  erent toxicity behaviors depending on the subcommunity they engage in , painting a fractured set of abusive behaviors that vary from account to account . in the subcommunities they participate in , and ﬁnally , their behaviors in relation to subcommunity norms . 6 . 1 Toxicity behaviors in aggregate Abusive accounts post a median six toxic comments during our study period and toxic comments make up a median 2 . 9 % of all comments posted by abusive accounts . Figure 9 shows the distribution of toxic behaviors per abuser . A small handful of accounts ( 1 . 3 % ) exclusively post toxic comments ( 100 % of their comments are toxic ) , many of which are low - activity accounts , posting just a median ﬁve comments in total . When we consider highly active users ( i . e . , ones that post more than 100 comments ) , the top 5 % of abusive accounts post only 10 . 7 % toxic comments . In the most extreme case of this , 25K ( 4 . 2 % ) highly active abusive accounts post just a single toxic - message , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 18 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Fig . 11 . Abusers in Subreddits —Most abusive accounts restrict their toxic activity to a subset of their communities , however , 1 % are highly toxic in nearly all the subcommunities they belong to . The gradient represents the median percent toxicity for each abusive account in their respective subreddits . Most abusive accounts fall into the yellow and blue portions of the graph , highlighting low overall toxicity in the majority of their communities . during our observation period , suggesting that the majority of their posting history is non - toxic . On the other end , 16K ( 2 . 6 % ) of abusive accounts post at least 100 toxic comments , and toxic comments account for at least 10 % of all comments for 18 . 7K ( 3 . 1 % ) of accounts . Given the rise in toxic comments posted to the platform in aggregate ( Section 4 ) , we also measure whether abusive accounts increase in their individual toxic behaviors over time . We compute a Mann - Kendall trend test for every abuser that posts at least a single comment every week , and ﬁnd that the majority of abusers ( 88 . 2 % ) exhibit no change in their toxicity behaviors over time . As such , toxicity behaviors are stable for most accounts and can form a foundation for building abuser personas . 6 . 2 Toxicity behaviors in subcommunities Even if an abusive account is highly toxic during their lifetime on the platform , they are rarely abusive in all of the subcommunities they post in ( Figure 10 and Figure 11 ) . Abusive accounts comment in a median 27 subreddits overall , but only post toxic comments in a median 13 % of those subreddits . Abusive accounts may selectively choose when or where to be toxic on the platform , due to a myriad of factors—for example , Cheng et al . found that seeing other trolling comments had an impact on whether an account would post a trolling comment themselves [ 9 ] . However , even when abusive accounts post toxic comments in their subreddits , it still may account for a relatively small portion of their posting volume in each subreddit . Indeed , when abusive accounts post a toxic comment in a subreddit , such comments make up a median of 12 . 5 % of the comments they post . As such , abusive accounts may not only be selective in which subcommunities to post toxic comments in , but also in how toxically they behave in those communities . Figure 11 represents this idea as a gradient , where the intensity of the gradient ( from blue to red ) indicates an abusive account that posts signiﬁcant toxic comments in all of their subcommunities . Most abusers fall between the blue and yellow portions of the graph ( 94 % ) , which indicates they typically post , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 19 only a small number of toxic comments in their subreddits . Yet , there are a small fraction ( 1 % ) of abusive accounts that choose to be highly toxic in the majority of their subcommunities ( i . e . , deep red points in the graph ) , again highlighting variance in abusive account behaviors . 6 . 3 Violating subcommunity norms Each subcommunity on Reddit is self - moderated , meaning each establishes its own set of unique rules , or norms , about what types of discussion is allowed . As such , posting toxic comments in all communities may not explicitly violate community norms , which a broad deﬁnition of toxicity ( as we have been applying in this paper ) may not appropriately capture . In order to study abusive behaviors in the context of community , we additionally consider how abusive accounts violate toxicity norms deﬁned by Rajadesingan et al . [ 34 ] . We deﬁne a subreddit’s toxicity norm as the fraction of toxic content posted in each subreddit . We restrict our analysis to subcommunities that have more than 50 comments in our dataset , and to those that exhibit a “stable” norm over our measurement period , which means the toxicity norm does not change beyond 2 % from month to month ( a stability metric deﬁned in prior work [ 34 ] ) . In addition , we ﬁlter out subcommunities that do not have a distinctive toxicity norm , which in this context means their toxicity norm is at least 2 % away from the average toxicity norm of all subreddits during our study period ( which was 2 . 4 % ) . In total , we identify 35 , 840 subreddits with stable , distinctive toxicity norms for further analysis , of which 94 . 9 % of subcommunities fall below and 5 . 1 % fall above the platform - wide toxicity norm . We note that we only investigate a subreddit for an abusive account if they post a comment in that subreddit at least 5 times , which is denoted as a “social home” in previous work [ 10 ] . We do this to avoid counting single , spurious toxic comments in one - off communities . When abusive accounts post toxic comments in their social homes , they often do so in violation of subreddit norms . Abusive accounts violate the toxicity norms of a median 16 . 7 % of their social homes ( Figure 10 ) . Again , there is signiﬁcant variance in behaviors ; 5 . 8 % of abusive accounts violate the toxicity norms of every single subreddit they are a part of . When abusive accounts do violate subcommunity norms , they often do so to signiﬁcant degrees—abusers post a median 3 . 55x more toxic comments than the community standard . Such behaviors suggest that extreme norm - violations can serve as a broader signal to detect the most toxic accounts . 6 . 4 Classes of abusive accounts We leverage our analysis of abusive behaviors to this point to ultimately build distinct abuser personas . We aggregate four key features of abuser toxicity behaviors : ( 1 ) The fraction of comments that are toxic in aggregate . ( 2 ) The fraction of subreddits that contain a toxic comment . ( 3 ) The median fraction of toxic comments for each subreddit the abuser participates in . ( 4 ) The fraction of subcommunities that each account violates a toxicity norm in . We cluster abusive accounts using K - Means with Principal Component Analysis ( PCA ) for dimensionality reduction . We reduced to three components , as they capture the majority of the variance between each variable . We note that not every abusive account has explicit norm violation statistics ( given only 35 . 8K communities have stable toxicity norms ) —we exclude these abusive accounts and cluster the 489K ( 53 % ) remaining accounts . Abusers fall into three distinct classes of abusive accounts . Each cluster’s median behavior is detailed in Table 5 . We detail each cluster below : , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 20 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Metric Sub - Metric Cluster 1 Cluster 2 Cluster 3 Cluster Size 350K ( 71 % ) 117K ( 24 % ) 21K ( 4 . 3 % ) Toxic Comments 71 % 26 % 3 . 1 % Activity Comments 521 144 25 Subreddits 63 22 3 Social Homes 18 5 1 Toxicity Agg . Toxicity 2 . 4 % 7 . 2 % 18 % Tox Subreddits 11 . 4 % 25 % 50 % Violat . Subreddits 12 . 5 % 40 % 100 % Table 5 . Abuser Personas —Abusers fall into three distinct personas that capture their toxicity behaviors on the platform . Cluster 1 abusers are occasional abusers , who post only a small handful of toxic comments but are otherwise active , regular members of their social communities . Cluster 2 abusers are moderate abusers who post more toxic comments in more of their subcommunities , but are still discerning about their toxicity behaviors and more conscious of subcommunity norms . Cluster 3 abusers are serial abusers who post a significant fraction of comments and regularly violate community norms , suggesting platform wide defenses may be the only way to curb abuse from those types of accounts . Cluster 1 : Occasional abusers 350K ( 71 . 7 % ) abusive accounts are occasional abusers , which are accounts that post relatively small fractions of toxic comments during their posting history ( median 2 . 4 % ) and contribute 70 % of all toxic comments posted to Reddit . These accounts post toxic content in a relatively small fraction of their subcommunities ( 11 . 4 % ) and tend to be regular , contributing members of the subcommunities they belong in . Indeed , these accounts are also the most active abusive accounts on the platform , posting a median 521 comments and participating in a median 18 social homes . While these accounts may have a proclivity towards toxic behaviors , they may also be the ones most amenable to nudges or other types of interventions , as toxic behaviors do not make up a signiﬁcant portion of their overall platform behaviors . Cluster 2 : Moderate abusers 117K ( 24 . 1 % ) abusive accounts are moderate abusers , which are accounts that post moderate amounts of toxic comments on the platform ( median 7 . 2 % toxic comments ) and contribute 26 % of all toxic comments posted to Reddit . Notably , these accounts are toxic in a larger fraction of their subcommunities , and violate the norms of a median 40 % of communities they participate in . This is approximately 3 . 5 times that of occasional abusers but still a minority of their subreddits , highlighting that moderate abusers are selectively abusive in a handful of communities but not others . Curbing abuse from these types of accounts is challenging , as simple nudges are likely ineffective . Instead , more robust defenses that include subcommunity speciﬁc moderation practices may be most effective . Cluster 3 : Serial abusers 21K ( 4 . 3 % ) of accounts are serial abusers , which is the least prevalent abuser persona and contribute just 3 . 1 % of all toxic comments posted to Reddit . These accounts are serial abusers —they post a median 18 . 1 % toxic comments , and post toxic comments in a median 50 % of the subreddits they participate in . To make matters worse , they violate the toxicity norms of every subcommunity they post toxic content in , often entirely disregarding established toxicity norms . Encouragingly , these types of accounts are the least active type of abuser , limiting their existing impact on the platform . Still , given their proclivity to posting toxic content , simple interstitial defenses such as nudges may not be effective in curbing abuse from these accounts . Further abuse from these accounts can likely only be curbed through platform - level action ( e . g . , bans , suspensions ) . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 21 Metric Control George Floyd Incident Toxicity Volume 68K ( 0 . 7 % ) 91K ( 0 . 9 % ) Abusive Accounts * 332K ( 15 . 4 % ) 347K ( 15 . 7 % ) Spurious Receivers 17 . 5K ( 95 . 1 % ) 23 . 2K ( 93 . 9 % ) Repeat Receivers 575 ( 3 . 1 % ) 907 ( 3 . 7 % ) Flooded Receivers 315 ( 1 . 7 % ) 584 ( 2 . 4 % ) Table 6 . Toxicity in response to the murder of George Floyd —The fraction of toxic comments increased by 30 % on Reddit in response to the murder of George Floyd . This had downstream impacts on receiver experiences , which slightly skewed away from spurious toxic interactions and towards more impactful forms of abuse , like repeated a  acks and flooding . Results are statistically significantly di  erent between the two groups except when denoted with an asterisk . 7 CASE STUDY : THE MURDER OF GEORGE FLOYD In our longitudinal analysis , we observed several spikes in toxic behaviors on Reddit . One such spike occurred between May 26th , 2020 and June 5th , 2020 , which reached a peak during a three day period between May 29th and May 31st , 2020 . This spike was primarily in response to the murder of George Floyd . In this section , we detail the impact of this real - world event on toxic interactions on the platform in aggregate , on abusive account behaviors , and on receiver experiences . To do this , we compare behaviors from this spike period against a 3 day sample of the dataset collected from May 1st , 2020 to May 4th , 2020 as a control . 7 . 1 Changes in abuser behaviors During the peak of toxic behaviors , 0 . 9 % of all comments on the platform were toxic , which marks a 30 % increase in overall average toxicity from the control period ( Table 6 ) . Overall abuser activity ( e . g . , number of comments , number of subreddits ) stayed consistent throughout the control period and spike period , suggesting that the increase in toxic comment volume was not directly related to a signiﬁcant number of new abusive accounts becoming active during this period . Rather , we observe that 90 . 2 % of abusive accounts posted either the same volume or more toxic comments during the spike period compared to the control period , which contributed overall to an increased period of toxicity throughout the platform . We observe no changes in the structure of toxic interactions ( e . g . , those discussed in Section 5 ) , suggesting that such behavioral patterns remained consistent even when abusive accounts increased their volume of toxicity . Despite this , receiver experiences shifted slightly during the spike pe - riod . While interactions largely remained spurious , the number of receivers with solely spurious interactions decreased from 95 . 1 % to 93 . 9 % , and instead shifted towards more impactful forms of abuse , like repeated abuse ( 3 . 7 % ) and ﬂooding ( 2 . 4 % ) . This skew towards repeated abuse and ﬂooding was largely in discussions of the George Floyd incident . For example , one account posted in r / PublicFreakout : “Now those * same exact people * are defending what these fascist pigs are doing . ” The comment was met with 6 different attacks berating them for their comment and insulting them . The slight shift in the types of attacks that receivers experienced during the spike of toxicity may anecdotally suggest that the types of toxic interactions may increase in intensity during heated discussion of real - world events . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 22 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric 7 . 2 Subcommunity spread Despite abuser behaviors remaining relatively consistent , we observed that the subcommunities where toxic behaviors took place changed during the spike period . Many large subreddits that were closely discussing the incidents as well as the resulting protests boomed in posting volume . As an example , r / PublicFreakout , which is a community designed for discussing videos of “people freaking out , melting down , losing their cool , or being weird in public” 9 , saw its comment volume increase by 620 % and its toxic comment volume increase by 670 % during the spike . This also impacted many smaller subcommunities—for example , the subreddit with the largest change in toxic comment volume ( 9566 % increase in toxic comments ) was r / Minneapolis , which is where the murder of George Floyd took place . Other communities with a stark increase in toxic comments were other cities where protests were taking place , for example , r / philadelphia ( 3720 % increase ) , and r / cincinnati ( 3000 % increase ) . In all of these cases , we observe that many toxic comments are posted by accounts that never post in the subcommunity prior to the event . 569 ( 18 . 2 % ) of the accounts that posted toxic com - ments in r / PublicFreakout never posted in this subreddit prior . A similar result holds true for r / Minneapolis ( 26 . 2 % new members ) , r / philadelphia ( 24 . 5 % ) and r / cincinnati ( 10 . 1 % ) , suggesting that at least some fraction of an increase in toxic content in these subcommunities comes from outsider accounts , likely joining these subcommunities to discuss ongoing incidents and post inﬂammatory content . As an example , one new account that joined r / Minneapolis posted inﬂammatory responses to accounts talking about the ongoing protests . In one instance , they wrote : “hmm , lets ruin people’s businesses , earnings they have worked for to feed their family and shit , or work that they have put years into being ruined . yeah you are a bunch of fucking retards , all of you need to be executed” Such examples highlight that some abusive accounts may actively seek out contentious discussion and participate in a toxic manner during known real - world events on the platform . 8 DISCUSSION AND LIMITATIONS In this section , we synthesize our contributions into a set of open challenges and research directions for improving the automated detection of toxic behaviors and empowering community governance . 8 . 1 Abusive Accounts Contribute Significantly to Reddit Toxicity on Reddit accounts for a relatively small fraction of comments but are highly visible on the platform : 55 . 2 % of Reddit accounts post directly on a thread with a toxic comment . Abusive accounts themselves make up just 3 . 1 % of all accounts , but make up 33 . 3 % of all comments to Reddit , which aligns with studies on other platforms ; Hindman et al . describe the challenge on Facebook as a “superuser supremeacy problem [ 16 ] . ” The majority of these accounts engage in abusive infractions throughout their lifetime , but simultaneously contribute signiﬁcant volumes of non - toxic content , ostensibly making them valuable contributors to the platform at large . As such , traditional strategies for dealing with abusive accounts ( e . g . , mass account bans ) are likely untenable , as they would signiﬁcantly reduce meaningful conversation on the platform and have potentially unforeseen consequences on platform health . Some platforms are attempting more nuanced actions . For example , Twitter deployed a strike system for handling accounts that post Covid - 19 misinformation [ 4 ] , and some subreddits have implemented similar strike systems for moderation at large [ 43 ] . Still , there is limited insight into how effective these strategies may be for handling online hate and harassment . 9 https : / / www . reddit . com / r / PublicFreakout / , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 23 The design of new defensive schemes and evaluating their efﬁcacy for these types of attacks is a potential direction for future research . 8 . 2 Time - based and Graph - based Features can Improve Toxicity Detection We uncovered three distinct attack patterns when studying the graph relationships between abusive accounts and their targets : one - off attacks , repeated toxicity , and ﬂooding . In doing so , we also uncovered several graph - and time - based features that may prove useful in providing context around toxic behaviors . For example , prior relationshps between accounts , assortatitivy of abusive accounts ( whereby clusters of abusive accounts coordinate or interact frequently ) and the target selection of abusive accounts ( such as repeatedly sending toxic comments to a single account ) provide a richer context compared to isolated attacks . These graph - based and time - based features represent a potential new direction for detecting abusive accounts , and can add additional context to existing classiﬁcation systems , which rely exclusively on text content to arrive on decisions . Incorporating these features into more comprehensive classiﬁers is a promising area of future work . 8 . 3 Classes of Abusive Accounts can Inform Defensive Design Our identiﬁcation of three distinct classes of abusive accounts ( e . g . , occasional abusers , serial abusers ) and varied community norms suggests the need for targeted interventions , rather than a one - size - ﬁts - all approach to actioning toxic behaviors . As we previously proposed , one - off attackers might beneﬁt most from inline warnings or nudges . Chang et al . previously found that temporarily blocking Wikipedia contributors substantially reduced the rate of repeated abuse [ 7 ] . Likewise , Instagram now includes a feature that warns users before they post content that appears similar to previously reported hate and harassment [ 20 ] . However , the existence of moderate and serial attackers requires more serious interventions , up to and including suspension or permanent blocking . Chandrasekharan et al . previously showed that banning subcommunities can be highly effective [ 5 ] . However , the throw - away nature of accounts on Reddit may complicate applying such a strategy to individual attackers—though this limitation may not exist for all online social networks . At the same time , some communities like r / WallStreetBets and r / RoastMe relish in offen - sive , profanity - laced discussions with other willing participants . Combined with varying personal deﬁnitions of what constitutes toxic content [ 14 , 25 ] , it is critical that platform designers consider empowering community - level moderators to best support conversational nuance online . However , it remains critical to enforce site - wide policies against hate and harassment , lest toxic subcommunities ﬂourish that negatively impact other communities or users [ 5 ] . 8 . 4 Limitations Our work is not without limitations . For one , as we note in Section 3 , leveraging the Perspective API alone for toxic comment detection is noisy , and can lead to both false positives and false negatives . We stress that we evaluated our usage of the Perspective API on Reddit comments to provide a fair assessment of its performance throughout our study . Another key limitation is the existence of sockpuppet Reddit accounts , whereby individual users create multiple accounts for different purposes ( e . g . , to spread toxic comments ) . While some techniques do exist to identify sockpuppets [ 26 , 46 ] , these studies are still nascent and can also lead to signiﬁcant false positives or negatives . While we take some precautions to account for these in our abusive account categorization ( e . g . , restricting to active subreddits , measuring norms from social homes ) , we note that the presence of sockpuppets may erroneously count multiple accounts as single individuals . Still—we argue that this perspective is the closest aligned to both community moderators and platform policy makers ( who can only operate on an account level ) , and thus our results are still applicable for their use cases . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 24 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric 9 CONCLUSION In this work , we presented the results of a longitudinal measurement study of abusive accounts posting toxic comments on Reddit . By pairing the Perspective API with different model thresholds and curated ﬁlters , we identiﬁed over 929K abusive accounts and 14 million toxic comments over an 18 month period . These accounts engaged in a variety of toxic behaviors , the most prominent of which were insults , followed by identity attacks , calls to leave the platform , and threats of violence . We examined a variety of features associated with each account—such as overall toxicity rates , toxicity rates per community , and community norm violations—and identiﬁed three distinct classes of abusive accounts that each would beneﬁt from a nuanced intervention . Similarly , we explored the graph relationships between attackers and targets and found multiple examples of mob - like attacks and long - term attack campaigns . Our measurements serve to better understand the dynamics of hate and harassment attacks in practice , as well as to identify features that might transform classiﬁcation from a per - comment decision into a holistic time - based , graph - based , and content - based assessment . REFERENCES [ 1 ] Max Aliapoulios , Kejsi Take , Prashanth Ramakrishna , Daniel Borkan , Beth Goldberg , Jeffrey Sorensen , Anna Turner , Rachel Greenstadt , Tobias Lauinger , and Damon McCoy . 2021 . A large - scale characterization of online incitements to harassment across platforms . In ACM Internet Measurement Conference . [ 2 ] Michele Banko , Brendon MacKeen , and Laurie Ray . 2020 . A uniﬁed taxonomy of harmful content . In Workshop on online abuse and harms . [ 3 ] Jason Baumgartner , Savvas Zannettou , Brian Keegan , Megan Squire , and Jeremy Blackburn . 2020 . The pushshift reddit dataset . In International AAAI conference on web and social media . [ 4 ] Ian Carlos Campbell . 2021 . Twitter will label COVID - 19 vaccine misinformation and enforce a strike system . https : / / www . theverge . com / 2021 / 3 / 1 / 22307919 / twitter - covid - 19 - vaccine - labels - ﬁve - strike - system . [ 5 ] Eshwar Chandrasekharan , Umashanthi Pavalanathan , Anirudh Srinivasan , Adam Glynn , Jacob Eisenstein , and Eric Gilbert . 2017 . You can’t stay here : The efﬁcacy of reddit’s 2015 ban examined through hate speech . In ACM Conference on Computer - Supported Cooperative Work . [ 6 ] Eshwar Chandrasekharan , Mattia Samory , Shagun Jhaver , Hunter Charvat , Amy Bruckman , Cliff Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s hidden rules : An empirical study of Reddit norm violations at micro , meso , and macro scales . In ACM Conference on Computer - Supported Cooperative Work . [ 7 ] Jonathan Chang and Cristian Danescu - Niculescu - Mizil . 2019 . Trajectories of Blocked Community Members : Redemp - tion , Recidivism and Departure . In The World Wide Web Conference . [ 8 ] Despoina Chatzakou , Nicolas Kourtellis , Jeremy Blackburn , Emiliano De Cristofaro , Gianluca Stringhini , and Athena Vakali . 2017 . Measuring # gamergate : A tale of hate , sexism , and bullying . In The World Wide Web Conference . [ 9 ] Justin Cheng , Michael Bernstein , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2017 . Anyone can become a troll : Causes of trolling behavior in online discussions . In ACM Conference on Computer - Supported Cooperative work . [ 10 ] Srayan Datta and Eytan Adar . 2019 . Extracting inter - community conﬂicts in reddit . In International AAAI Conference on Web and Social Media . [ 11 ] Maeve Duggan . 2017 . Online Harassment 2017 . https : / / www . pewresearch . org / internet / 2017 / 07 / 11 / online - harassment - 2017 / . [ 12 ] Facebook . 2021 . Transparency Center . https : / / transparency . fb . com / policies / community - standards / bullying - harassment . [ 13 ] Sarah A Gilbert . 2020 . ” I run the world’s largest historical outreach project and it’s on a cesspool of a website . ” Moderating a Public Scholarship Site on Reddit : A Case Study of r / AskHistorians . Proceedings of the ACM on Human - Computer Interaction CSCW ( 2020 ) . [ 14 ] Mitchell L Gordon , Kaitlyn Zhou , Kayur Patel , Tatsunori Hashimoto , and Michael S Bernstein . 2021 . The disagreement deconvolution : Bringing machine learning performance metrics in line with reality . In ACM CHI Conference on Human Factors in Computing Systems . [ 15 ] Chris Grier , Kurt Thomas , Vern Paxson , and Michael Zhang . 2010 . @ spam : the underground on 140 characters or less . In ACM conference on Computer and communications security . [ 16 ] Matthew Hindman , Nathanie Lubin , and Trevor Davis . 2022 . Facebook Has a Superuser - Supremacy Problem . https : / / www . theatlantic . com / technology / archive / 2022 / 02 / facebook - hate - speech - misinformation - superusers / 621617 / . [ 17 ] Manoel Horta Ribeiro , Jeremy Blackburn , Barry Bradlyn , Emiliano De Cristofaro , Gianluca Stringhini , Summer Long , Stephanie Greenberg , and Savvas Zannettou . 2020 . The Evolution of the Manosphere Across the Web . arXiv preprint , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 25 arXiv : 2001 . 07600 ( 2020 ) . [ 18 ] Yiqing Hua , Mor Naaman , and Thomas Ristenpart . 2020 . Characterizing twitter users who engage in adversarial interactions against political candidates . In ACM CHI Conference on Human Factors in Computing Systems . [ 19 ] Yiqing Hua , Thomas Ristenpart , and Mor Naaman . 2020 . Towards measuring adversarial twitter interactions against candidates in the US midterm elections . In International AAAI Conference on Web and Social Media . [ 20 ] Instagram . 2019 . Our Progress on Leading the Fight Against Online Bullying . https : / / instagram - press . com / blog / 2019 / 12 / 16 / our - progress - on - leading - the - ﬁght - against - online - bullying / . [ 21 ] Ann John , Alexander Charles Glendenning , Amanda Marchant , Paul Montgomery , Anne Stewart , Sophie Wood , Keith Lloyd , and Keith Hawton . 2018 . Self - harm , suicidal behaviours , and cyberbullying in children and young people : systematic review . Journal of medical internet research ( 2018 ) . [ 22 ] Cristian Danescu - Niculescu - Mizil Jonathan P . Chang . 2019 . Trouble on the Horizon : Forecasting the Derailment of Online Conversations as they Develop . In Empirical Methods in Natural Language Processing . [ 23 ] Matthew Katsaros , Kathy Yang , and Lauren Fratamico . 2022 . Reconsidering Tweets : Intervening During Tweet Creation Decreases Offensive Content . In Proceedings of the International AAAI Conference on Web and Social Media . [ 24 ] Yubo Kou . 2021 . Punishment and Its Discontents : An Analysis of Permanent Ban in an Online Game Community . Proceedings of the ACM on Human - Computer Interaction CSCW ( 2021 ) . [ 25 ] Deepak Kumar , Patrick Gage Kelley , Sunny Consolvo , Joshua Mason , Elie Bursztein , Zakir Durumeric , and Michael Bailey . 2021 . Designing Toxic Content Classiﬁcation for a Diversity of Perspectives . In Symposium on Usable Privacy and Security ( SOUPS ) . [ 26 ] Srijan Kumar , Justin Cheng , Jure Leskovec , and VS Subrahmanian . 2017 . An army of me : Sockpuppets in online discussion communities . In Proceedings of the 26th International Conference on World Wide Web . [ 27 ] David Leonhardt and Ian Prasad Philbrick . 2021 . One Year Later . https : / / www . nytimes . com / 2021 / 05 / 25 / brieﬁng / george - ﬂoyd - legacy - anniversary . html . [ 28 ] Kirill Levchenko , Andreas Pitsillidis , Neha Chachra , Brandon Enright , M´ark F´elegyh´azi , Chris Grier , Tristan Halvorson , Chris Kanich , Christian Kreibich , He Liu , et al . 2011 . Click trajectories : End - to - end analysis of the spam value chain . In IEEE symposium on security and privacy . [ 29 ] Suman Kalyan Maity , Aishik Chakraborty , Pawan Goyal , and Animesh Mukherjee . 2018 . Opinion conﬂicts : An effective route to detect incivility in Twitter . Proceedings of the ACM on Human - Computer Interaction CSCW ( 2018 ) . [ 30 ] Enrico Mariconti , Guillermo Suarez - Tangil , Jeremy Blackburn , Emiliano De Cristofaro , Nicolas Kourtellis , Ilias Leontiadis , Jordi Luque Serrano , and Gianluca Stringhini . 2019 . “You Know What to Do” : Proactive Detection of YouTube Videos Targeted by Coordinated Hate Attacks . ( 2019 ) . [ 31 ] Binny Mathew , Anurag Illendula , Punyajoy Saha , Soumya Sarkar , Pawan Goyal , and Animesh Mukherjee . 2020 . Hate begets hate : A temporal study of hate speech . Proceedings of the ACM on Human - Computer Interaction CSCW ( 2020 ) . [ 32 ] Shirin Nilizadeh , Franc¸ois Labr ` eche , Alireza Sedighian , Ali Zand , Jos´e Fernandez , Christopher Kruegel , Gianluca Stringhini , and Giovanni Vigna . 2017 . Poised : Spotting twitter spam off the beaten paths . In ACM SIGSAC Conference on Computer and Communications Security . [ 33 ] Antonis Papasavva , Savvas Zannettou , Emiliano De Cristofaro , Gianluca Stringhini , and Jeremy Blackburn . 2020 . Raiders of the lost kek : 3 . 5 years of augmented 4chan posts from the politically incorrect board . In International AAAI Conference on Web and Social Media . [ 34 ] Ashwin Rajadesingan , Paul Resnick , and Ceren Budak . 2020 . Quick , community - speciﬁc learning : How distinctive toxicity norms are maintained in political subreddits . In International AAAI Conference on Web and Social Media . [ 35 ] Manoel Ribeiro , Pedro Calais , Yuri Santos , Virg´ılio Almeida , and Wagner Meira Jr . 2018 . Characterizing and detecting hateful users on twitter . In International AAAI Conference on Web and Social Media . [ 36 ] Manoel Horta Ribeiro , Shagun Jhaver , Savvas Zannettou , Jeremy Blackburn , Emiliano De Cristofaro , Gianluca Stringhini , and Robert West . 2020 . Does Platform Migration Compromise Content Moderation ? Evidence from r / The Donald and r / Incels . arXiv preprint arXiv : 2010 . 10397 ( 2020 ) . [ 37 ] Erik Rye , Jeremy Blackburn , and Robert Beverly . 2020 . Reading In - Between the Lines : An Analysis of Dissenter . In ACM Internet Measurement Conference . [ 38 ] Martin Saveski , Brandon Roy , and Deb Roy . 2021 . The Structure of Toxic Conversations on Twitter . In The World Wide Web Conference . [ 39 ] Leandro Silva , Mainack Mondal , Denzil Correa , Fabr´ıcio Benevenuto , and Ingmar Weber . 2016 . Analyzing the targets of hate in online social media . In International AAAI Conference on Web and Social Media . [ 40 ] Vivek K Singh , Marie L Radford , Qianjia Huang , and Susan Furrer . 2017 . ” They basically like destroyed the school one day” On Newer App Features and Cyberbullying in Schools . In ACM Conference on Computer Supported Cooperative Work and Social Computing . [ 41 ] Kurt Thomas , Devdatta Akhawe , Michael Bailey , Dan Boneh , Elie Bursztein , Sunny Consolvo , Nicola Dell , Zakir Durumeric , Patrick Gage Kelley , Deepak Kumar , Damon McCoy , Sarah Meiklejohn , Thomas Ristenpart , and Gianluca , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . 1 : 26 Deepak Kumar , Je  Hancock , Kurt Thomas , and Zakir Durumeric Stringhini . 2021 . SoK : Hate , Harassment , and the Changing Landscape of Online Abuse . In IEEE Symposium on Security and Privacy . [ 42 ] Twitter . 2021 . Rules enforcement . https : / / transparency . twitter . com / en / reports / rules - enforcement . html # 2020 - jul - dec . [ 43 ] u / deliteplays . 2020 . New Rule [ 0 ] and Strike System - please read before posting to avoid receiving bans . https : / / www . reddit . com / r / ProgrammerHumor / comments / bymrtt / new rule0 and strike system please read before / . [ 44 ] Jessica Vitak , Kalyani Chadha , Linda Steiner , and Zahra Ashktorab . 2017 . Identifying women’s experiences with and strategies for mitigating negative effects of online harassment . In ACM Conference on Computer Supported Cooperative Work and Social Computing . [ 45 ] Emily A . Vogels . 2021 . The State of Online Harassment . https : / / www . pewresearch . org / internet / 2021 / 01 / 13 / the - state - of - online - harassment / . [ 46 ] Janith Weerasinghe , Rhia Singh , and Rachel Greenstadt . 2022 . Using Authorship Veriﬁcation to Mitigate Abuse in Online Communities . In Proceedings of the International AAAI Conference on Web and Social Media . [ 47 ] Yan Xia , Haiyi Zhu , Tun Lu , Peng Zhang , and Ning Gu . 2020 . Exploring antecedents and consequences of toxicity in online discussions : A case study on reddit . Proceedings of the ACM on Human - computer Interaction CSCW ( 2020 ) . [ 48 ] Savvas Zannettou , Barry Bradlyn , Emiliano De Cristofaro , Haewoon Kwak , Michael Sirivianos , Gianluca Stringini , and Jeremy Blackburn . 2018 . What is gab : A bastion of free speech or an alt - right echo chamber . In The World Wide Web Conference . [ 49 ] Justine Zhan , Jonathan P . Chang , Cristian Danescu - Niculescu - Mizil , Lucas Dixon , Yiqing Hua , Nithum Thain , and Dario Taraborelli . 2018 . Conversations Gone Awry : Detecting Early Signs of Conversational Failure . In Proceedings of ACL . [ 50 ] Justine Zhang , Sendhil Mullainathan , and Cristian Danescu - Niculescu - Mizil . 2020 . Quantifying the Causal Effects of Conversational Tendencies . In ACM Conference on Computer - Supported Cooperative Work . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 . Understanding Longitudinal Behaviors of Toxic Accounts on Reddit 1 : 27 Comment Threshold # Abusive Accounts Precision 1 931K ( 2 . 9 % ) 0 . 71 2 332K ( 1 % ) 0 . 68 3 183K ( 0 . 5 % ) 0 . 68 4 119K ( 0 . 4 % ) 0 . 7 5 84K ( 0 . 3 % ) 0 . 66 Table 7 . Abusive Account Thresholds —Increasing the number of toxic comments required to label an account as an abusive account does not demonstrably improve performance , while significantly reducing the account and comment volume available to study . A EVALUATING ABUSER THRESHOLDS For the scope of this study , we consider an account to be abusive if it posts just a single comment above the high precision threshold of SEVERE TOXICITY > 0 . 9 . However , we also evaluated whether increasing the number of high - precision toxic comments required to label an account as abusive could in turn increase our overall precision . To measure this , one expert rater manually sampled high - precision toxic comments from accounts that posted 1 – 5 toxic comments , and we evaluated the resultant precision to see if changing the threshold for abusive comments would increase our results . Table 7 shows the results . Ultimately , precision was stable for all samples at 0 . 7 , suggesting that increasing the threshold would not result in higher quality data while also reducing the size of the abusive account population to a tenth of the size in the most extreme case . , Vol . 1 , No . 1 , Article 1 . Publication date : January 2016 .