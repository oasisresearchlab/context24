Eurographics Conference on Visualization ( EuroVis ) 2019 M . Gleicher , H . Leitte , and I . Viola ( Guest Editors ) Volume 38 ( 2019 ) , Number 3 Characterizing Exploratory Visual Analysis : A Literature Review and Evaluation of Analytic Provenance in Tableau Leilani Battle 1 and Jeffrey Heer 2 1 Department of Computer Science , University of Maryland , College Park 2 Paul G . Allen School of Computer Science & Engineering , University of Washington Abstract Supporting exploratory visual analysis ( EVA ) is a central goal of visualization research , and yet our understanding of the pro - cess is arguably vague and piecemeal . We contribute a consistent deﬁnition of EVA through review of the relevant literature , and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau , a popular visual analysis tool . We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets . We mea - sure task performance , identify recurring patterns across participants’ analyses , and assess variance from task speciﬁcity and dataset . We ﬁnd striking differences between existing assumptions and the collected data . Participants successfully completed a variety of tasks , with over 80 % accuracy across focused tasks with measurably correct answers . The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community . We ﬁnd signiﬁcant overlap in analyses across participants , showing that EVA behaviors can be predictable . Furthermore , we ﬁnd few structural differences between behavior graphs for open - ended and more focused exploration tasks . 1 . Introduction Exploratory visual analysis ( or EVA ) involves identifying questions of interest , inspecting visualized data , and iteratively reﬁning one’s questions and hypotheses . Visual analysis tools aim to facilitate this process by enabling rapid speciﬁcation of both data transformations and visualizations , using a combination of direct manipulation and automated design . With a better understanding of users’ analysis behavior , we might improve the design of these visualization tools to promote effective outcomes . However , as an open - ended process with varied inputs and goals , exploratory visual analysis is often difﬁcult to characterize and thus appropriately design for . Existing work provides many theories and assumptions regard - ing how EVA is conceptualized and performed , which prove in - valuable in designing new EVA systems . However , we see in the literature that these contributions are generally deﬁned in small snippets spread across many research articles . Existing surveys and frameworks touch on related topics , such as task analysis ( e . g . , [ BM13 , LTM18 ] ) , provenance ( e . g . , [ ED16 , HMSA08 , RESC16 ] ) , and interaction ( e . g . , [ HS12 ] ) , but not speciﬁcally for understand - ing EVA . Furthermore , we ﬁnd several “schools of thought” on EVA , some of which may contradict one another . For example , it is argued that exploration can only be open - ended , without clear a priori goals or hypotheses ( e . g . , [ AZL ∗ 19 ] ) . In contrast , we also see speciﬁc examples where analysts come to an EVA session with a clear goal or hypothesis in mind ( e . g . , [ FPDs12 , SKL ∗ 16 ] ) . This broad dispersion of the different deﬁnitions of EVA makes it difﬁ - cult as a community to rigorously discuss , evaluate , and ultimately contribute to new research to advance EVA systems . The goal of this work is to connect the many different ideas and concepts regarding EVA together and bring them into focus , en - abling the community to more easily reﬂect on the way we moti - vate , analyze , and ultimately support visual exploration tasks . To this end , we make two major contributions : • A review of relevant EVA literature , highlighting core ideas , themes , and discrepancies from across multiple research areas . • An analysis of provenance data collected from an exploratory study using Tableau , to shed light on particularly contentious or seemingly undervalued EVA topics . We reviewed 41 papers for insights into the EVA process . Three major themes emerged , centered around : 1 ) EVA goals and intent , 2 ) EVA behaviors and structure , and 3 ) EVA performance for both the analyst and the system . Within each theme , we identify one or more research questions that appear unanswered by the literature . Our initial aim is to provide additional context—not necessarily evidence—with respect to these questions and to encourage the community to take up these questions in future work . To investigate further , we conduct a study with 27 Tableau users exploring three real - world datasets using Tableau Desktop . Our study design utilizes four task types of varying speciﬁcity , designed to match the common visual analysis tasks that occur during EVA , identiﬁed in our literature review . These tasks range from focused tasks with measurably correct answers to more open - ended , yet still goal - directed tasks . We summarize our main ﬁndings : c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohn Wiley & SonsLtd . PublishedbyJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau Evaluating Performance : We evaluate several performance metrics , such as pacing metrics ( e . g . , interaction rates , think time [ BCS16 ] ) , as well as the variety and quality of task responses . Though not an exact match , we compare our accuracy results to prior reports of false discovery rates during EVA [ ZZZK18 ] . Whereas prior work ﬁnds a 60 % false discovery rate for “insights” reported during open - ended exploration , participants responding to our goal - directed prompts exhibit over 80 % accuracy on focused tasks with measurably correct answers , and were generally cautious to avoid false discoveries . Furthermore , while interaction latency in EVA is frequently discussed [ HS12 , LH14 , CXGH08 , ZGC ∗ 16 ] , the pace of exploration lacks realistic contextualization in some parts of the literature . In particular , a subset of the literature assumes that the time between interactions ( or think time ) is limited , constraining database optimization methods ( e . g . , [ CGZ ∗ 16 , BCS16 , RAK ∗ 17 ] ) , at least for interactions of low cognitive complexity [ LH14 ] . Our results show that high think times lead to slow analysis pace , and the pace of analysts is notably slower than assumed by this prior work , even with interaction taken into account . We also observe “bursty” behavior : participants spend some of their ( think ) time planning , then performing a relatively faster sequence of interac - tions . These results suggest that visual analysis evaluations could be improved via more realistic scenarios and accurate parameters . Evaluating Goals and Structure : We next analyze participants’ analysis behavior graphs —a structural model of “states” visited . We ﬁnd that several assumptions of the structure of EVA are sup - ported by our analysis . For example , participants’ analysis ses - sions are consistent with a depth - ﬁrst search process , conﬁrming arguments made in prior work [ WMA ∗ 16b ] . However , our results also contradict other assumptions . The literature is inconsistent on whether EVA follows clear structure and patterns , and some argue that individual differences could make EVA behaviors un - predictable [ ZOC ∗ 12 ] . We ﬁnd that participants’ analyses exhibit strong similarities and are somewhat predictable , but only at spe - ciﬁc points in analysis sessions . The breadth and depth of analysis graphs are modulated by task , but the overall ratio of these mea - sures is consistent across task types . Ultimately , we ﬁnd that ana - lysts’ performance and strategies during open - ended tasks can be structurally similar to observations of more focused tasks , encour - aging us to reconsider the distinctions made between open - ended exploration and more focused analysis . Though speculative , this similarity may be explained by a model in which analysts with open - ended aims formulate a series of more focused and goal - directed sub - tasks to pursue . In sum , these results provide new perspectives on the content , structure , and efﬁcacy of EVA sessions . We conclude with a discus - sion of how our ﬁndings might be applied to further the design of not only visualization tools , but also the way we evaluate them . All anonymized data artifacts generated by this work have been shared as a community resource for further study at https : / / github . com / leibatt / characterizing - eva - tableau . 2 . Related Work Our analysis builds on several areas of related work , such as log - ging interactions , modeling analysis states , and analyzing patterns in the resulting data structures . Visualization system state is of - ten recorded via histories [ HMSA08 ] , interaction logs [ ED16 ] , and provenance tracking [ LWPL11 , CFS ∗ 06 , BCC ∗ 05 , SASF11 ] . We rely on built - in logging in Tableau [ STH02 ] for analysis . Visualization Theory & Process Models : Our work is informed by models of the visual analysis process developed through ex - ploratory studies , such as those by Isenberg et al . [ ITC08 ] and Grammel et al . [ GTS10 ] . Isenberg et al . present a framework de - scribing how teams collaborate during exploration [ ITC08 ] . Gram - mel et al . present a model of how novice users construct visual - izations in Tableau , and barriers to the design process [ GTS10 ] . Brehmer and Munzner present a multi - level typology of visual analysis tasks [ BM13 ] , synthesized from a review of the litera - ture on task analysis . Lam et al . review IEEE InfoVis design study papers to evaluate how high - level goals decompose into concrete tasks and visual analysis steps [ LTM18 ] . However , like the many papers we evaluate in our literature review , existing theoretical work generally lacks a clear deﬁnition of exploratory visual analy - sis ( also observed by Lam et al . [ LTM18 ] ) , which we aim to con - tribute in this work . Furthermore , with our focus on log analysis , our metrics are primarily quantitative in nature , providing empir - ical context for a variety of EVA assumptions from the literature . We focus on task or action - based models in our analy - sis [ GZ09 , BM13 , YKSJ07 , LTM18 ] , particularly ﬁne - grained mod - els [ HMSA08 , YKSJ07 ] . We use the task model for Tableau pro - posed by Heer et al . [ HMSA08 ] , which assigns interactions in Tableau to ﬁve categories : “ shelf ( add , remove , replace ) , data ( bin , derive ﬁeld ) , analysis ( ﬁlter , sort ) , worksheet ( add , delete , dupli - cate ) , and formatting ( resize , style ) commands . ” . Interaction Sequences : Interaction sequences reveal temporal relationships between observed interactions . Guo et al . [ GGZL16 ] identify common sequences that lead to insights . Gotz and Wen [ GW09 ] identify four common interaction sub - sequences , which they use to generate visualization recommendations . Oth - ers compute n - grams to identify common sub - sequences [ BJK ∗ 16 , BCS16 ] . Sequences are also used to build predictive ( e . g . , Markov ) models , which can be used to compare analysts’ perfor - mance [ RJPL16 ] , and predict users’ future interactions [ BCS16 , DC17 ] , task performance [ GL12 , BOZ ∗ 14a ] , or personality traits [ BOZ ∗ 14a ] . We use interaction sequences and more com - plex structures to track changes in analysis state over time . We con - tribute a new perspective on visual analysis patterns and structure using these quantitative methods . Behavior Graphs : Problem - solving behavior can be modeled as a set of states along with a set of operations for moving between states [ New72 ] , including visual analysis [ WMA ∗ 16b , STM17 , WQM ∗ 17 , SvW08 , ST15 , jJKMG07 ] . Graphs can capture more complex paths and patterns , such as back - tracking or state revis - itation . Alternate analysis paths are depicted as branches from an analysis state . Branching can occur due to manipulation of anal - ysis history ( e . g . , undo - redo interactions ) . Though many projects consider how to display history directly to users [ HMSA08 ] , past work lacks a characterization of the structure of exploratory analy - sis graphs across a range of conditions ( e . g . , tasks and datasets ) . Be - havior graphs are also used in web browsing and click stream anal - ysis [ CPVDW ∗ 01 , WHS ∗ 02 , LWD ∗ 17 , New72 ] . We leverage past work by similarly visualizing behavior graphs in Section 6 , con - c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau tributing a structural signature for analysis sessions , through which differences in analysis strategies can be measured during EVA . Insight - Based Evaluations : Insight - based evaluations attempt to catalog “insights” generated through open - ended visualization use [ SND05 , SNLD06 , LH14 , GGZL16 , ZZZK18 ] . These methods collect qualitative data about the EVA process , which researchers code and analyze . While useful for identifying meaningful cogni - tive events , the veracity of reported insights must be evaluated with care . However , having participants engage in open - ended explo - ration without clear goals and encouraging them to verbalize all insights that come to mind may decrease accuracy in visual anal - ysis tasks . We use directed task prompts representative of visual analysis tasks that commonly occur during EVA , ranging from fo - cused EVA tasks with veriﬁable answers to more open - ended , but still goal - directed , tasks . These tasks were identiﬁed through our literature review . We evaluate analysts’ performance and analysis strategies , and compare with prior insight - based studies . 3 . Themes in the EVA Literature Through a review of the EVA literature we identify and discuss three major themes that appear frequently throughout : exploration goals , structure , and performance . We then present a summary def - inition of EVA based on our ﬁndings . 3 . 1 . Review Methodology We utilized the following paper selection method for our review : 1 . Papers that analyze or design for EVA contexts were selected . 2 . Papers described or referenced by papers from step 1 as also analyzing or designing for EVA were selected . 3 . Tasks or topics from papers from step 1 that were described as relevant to EVA were identiﬁed . Task relevance suggested by paper authors or study subjects was considered ( e . g . , subjects’ comments observed by Alspaugh et al . [ AZL ∗ 19 ] ) . Then rele - vant , well - known papers that also discuss these tasks and topics were identiﬁed , such as the work by Kandel et al . [ KPHH12 ] . These papers are only used to provide context , other irrelevant tasks or topics from these papers are excluded from our review . Step 1 yielded 39 papers and step 2 yielded 2 papers [ HS12 , Shn96 ] for review . Step 3 Yielded 7 papers to provide additional context for speciﬁc EVA topics and tasks [ Lam08 , HMSA08 , KS12 , PC05 , MWN ∗ 19 , ZOC ∗ 12 , KPHH12 ] † . The selected papers were reviewed to identify major themes , and three themes emerged : EVA goals , structure , and performance . These themes occurred most frequently across the selected papers , and often as core priorities , for example Battle et al . prioritize system performance during EVA [ BCS16 ] , and Lam et al . prior - itize understanding analysis goals in various contexts , including EVA [ LTM18 ] . A subsequent review was made to capture similar - ities and differences between papers with respect to these themes . 3 . 2 . Exploration Goals Formulation and Evolution of Goals : An oft - stated goal of EVA is the production of new insights or observations from a given † The full list of papers yielded from each step , along with our reasoning for the inclusion of each paper , is provided in the supplemental materials . dataset ( insight generation ) [ ED16 , LTM18 , jJKMG07 , GGZL16 , ZGC ∗ 16 , ZZZK18 , LH14 , FPDs12 ] . Lam et al . [ LTM18 ] describe the goal of exploration as “Discover [ ing ] Observation [ s ] ” ; how - ever , this goal is vague in comparison to other visual analytic goals . Liu & Heer argue that EVA often “does not have a clear goal state” [ LH14 ] , which is a popular sentiment in both the visual - ization [ Kei01 , AZL ∗ 19 , RJPL16 ] and database [ IPC15 ] communi - ties . For example , Idreos et al . [ IPC15 ] describe EVA as a situation where analysts may not know exactly what they are looking for , but they will know something interesting when they see it . Keim makes a stronger argument : that EVA is more effective when the goals are less clear [ Kei01 ] . Alspaugh et al . [ AZL ∗ 19 ] take this idea even further by saying that exploration does not have well - formed goals ; once clear goals are formed , the analysis is no longer exploration . Others take a different view , saying that analysts’ goals evolve throughout the course of an EVA session : the analyst starts with a vague goal , and reﬁnes and sharpens this goal as they ex - plore [ RJPL16 , GW09 , WMA ∗ 16b ] . For example , Wongsupha - sawat et al . [ WMA ∗ 16b ] describe the evolution of analysts’ goals to motivate the Voyager system design : “Analysts’ interests will evolve as they browse their data , and so the gallery [ of suggested visualizations ] must be adaptable to more focused explorations . ” Bottom - Up Versus Top - Down Exploration : Exploration is of - ten described as “open - ended” , where many of the papers we re - viewed equate exploration with at most vaguely deﬁned tasks ( e . g . , [ LH14 , RJPL16 , AZL ∗ 19 , ZGC ∗ 16 , ZZZK18 ] ) : visual analysis per - formed without an explicit objective , perusing a dataset for inter - esting observations . Open - endedness seems to be tightly coupled with the notion of opportunistic analysis [ Tuk77 , LH14 , AZL ∗ 19 , RJPL16 ] . For example , Alspaugh et al . [ AZL ∗ 19 ] argue that during EVA , “ . . . actions are driven in reaction to the data , in a bottom - up fashion . . . ” . Liu & Heer [ LH14 ] suggest that “User interaction may be triggered by salient visual cues in the display . . . ” . There seems to be an argument in a subset of the literature that exploration must be unconstrained ( e . g . , by goals or tasks ) to allow for an organic “bottom - up” process of uncovering new insights from a dataset . In contrast , other projects describe scenarios where analysts come to an exploration session with a high - level goal or concrete hypothesis in mind . Liu & Heer [ LH14 ] suggest that user inter - actions during EVA may be “ . . . driven by a priori hypotheses . . . ” . Gotz & Zhou [ GZ08 ] describe a speciﬁc example with a ﬁnan - cial analyst exploring stock market data to identify and prioritize which stocks to invest in . Perer & Shneiderman [ PS08 ] recount ex - amples of domain analysts “trying to sift through gigabytes of ge - nomic data to understand the causes of inherited disease , to ﬁlter legal cases in search of all relevant precedents , or to discover be - havioral patterns in social networks with billions of people . ” Fisher et al . [ FPDs12 ] study in - depth cases of EVA with three different an - alysts with speciﬁc goals ; for example : “Sam is analyzing Twitter data to understand relationships between the use of vocabulary and sentiment . ” Kalinin et al . [ KCZ14 ] describe two motivating scenar - ios , with users exploring stock data and astronomy data for records ( i . e . , stocks , celestial objects ) with speciﬁc properties ( e . g . , stars with high brightness ) . Siddiqui et al . [ SKL ∗ 16 ] describe three spe - ciﬁc use cases , where scientists , advertisers and clinical researchers struggled to successfully explore their dataset for speciﬁc visual c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau patterns . Zgraggen et al . [ ZZZK18 ] motivate the multiple compar - isons problem in EVA with the story of “Jean , ” an employee at a non - proﬁt who is interested in exploring his organization’s data to identify the best gift to send to their donors . In all of these exam - ples , analysts are still performing EVA , but with concrete objectives to structure and focus exploration . These examples contradict the assumption of a purely bottom - up analysis strategy during EVA , indicating that , for realistic scenarios , top - down goals ( including broader organizational objectives ) need to be accounted for . From our review , we observe that discussions of EVA include a spectrum of goal speciﬁcations , from no goals at all , to clear a pri - ori goals and / or hypotheses . Analysts’ positions within this spec - trum may evolve as they learn more about their data . Furthermore , analysts may utilize both top - down ( i . e . , goal - directed ) and bottom - up ( i . e . , opportunistic ) strategies as they explore [ RJPL16 , LTM18 ] . Thus no one strategy completely represents how exploration un - folds , and both top - down and bottom - up strategies should be con - sidered when analyzing and evaluating EVA use cases . 3 . 3 . Exploration Structure Phases of Exploration : EVA may involve iteration within and oscillation between phases of exploration , with analysts pursu - ing multiple branches of analysis over time [ DR01 , HMSA08 ] . However , the literature is inconsistent in deﬁning exactly what the different phases of EVA are . Both Battle et al . [ BCS16 ] and Keim [ Kei01 ] assume that EVA follows Shneiderman’s information - seeking mantra [ Shn96 ] : “Overview ﬁrst , zoom and ﬁlter , details on demand” . Gotz & Zhou argue that users switch between two phases : browsing and querying of data to uncover insights , and recording their insights ( e . g . , writing notes ) [ GZ08 ] . Heer & Shneiderman [ HS12 ] state that EVA “typically progresses in an iterative process of view creation , exploration , and reﬁne - ment , ” where exploration happens at two levels : 1 ) as users interact with speciﬁc visualizations , and 2 ) in a larger cycle where users explore different visualizations . This concept is echoed by Gram - mel et al . [ GTS10 ] . Perer & Sheiderman [ PS08 ] say that analysts alternate between systematic exploration ( searching with thorough coverage of the data space ) and ﬂexible exploration ( or open - ended search ) . Wongsuphasawat et al . make a similar argument , inspired by earlier work [ Tuk77 ] : “Exploratory visual analysis is highly it - erative , involving both open - ended exploration and targeted ques - tion answering . . . ” [ WMA ∗ 16b ] . The common theme is that EVA involves alternating between open - ended and focused exploration . EVA and Search : Terms like “query” [ GZ08 , LKS13 , KJTN14 , DPD14 , KCZ14 , SKL ∗ 16 ] , “browse” [ LH14 , GGZL16 , BCS16 ] , and “search” [ KS12 , WMA ∗ 16b , PS08 ] are frequently associated with visual exploration . In EVA , users are often searching for novel observations in a dataset , which could inform or validate future hy - potheses [ Kei01 , LH14 , GZ08 , RJPL16 , AZL ∗ 19 , ZZZK18 ] . Jankun - Kelly et al . [ jJKMG07 ] observe that earlier EVA systems “assume visualization exploration is equivalent to navigating a multidimen - sional parameter space , ” essentially a directed search of the param - eter space of data transformations and visual encodings—a model subsequently adopted by visualization recommenders such as Com - passQL [ WMA ∗ 16a ] and Draco [ MWN ∗ 19 ] . Perer & Shneider - man [ PS08 ] make a similarly strong connection between EVA and search by incorporating support for what they call “systematic ex - ploration , ” an exploration strategy that “guarantees that all mea - sures , dimensions and features of a data set are studied . ” Oth - ers [ DPD14 , KCZ14 , VRM ∗ 15 , SKL ∗ 16 , DHPP17 ] propose tech - niques to automatically search the data space for interesting data re - gions or collections of visualizations for the user to review . The idea of searching for insights shares strong similarities with Pirolli & Card’s Information Foraging loop [ PC05 ] , in particular the “Read and extract” action , where users extract observations or “evidence” that may “trigger new hypotheses and searches” . Thus existing models of search behavior may play an important role in under - standing behavioral patterns and analysis structure in EVA . Analysis Tasks : Analysts seem to decompose their analyses into smaller tasks and subtasks [ GZ08 , RJPL16 , AES05 ] , where tasks may be re - used across datasets [ PS08 ] . In the literature , we observe a consensus that EVA involves speciﬁc low - level visual analytics tasks and that speciﬁc classes of tasks occur frequently in EVA : • understanding data correctness and semantics [ PS08 , AZL ∗ 19 , KS12 ] ( overlaps with “proﬁling” [ KPHH12 ] ) , • characterizing data distributions and relationships [ Tuk77 , PS08 , IPC15 , SKL ∗ 16 , AZL ∗ 19 , ZZZK18 , CGZ ∗ 16 , KS12 , SKL ∗ 16 , AES05 ] ( overlaps with“proﬁling” and “modeling” [ KPHH12 ] ) , • analyzing causal relationships [ PS08 , HS12 , STH02 ] ( overlaps with “modeling” [ KPHH12 ] ) , • hypothesis formulation and veriﬁcation [ PS08 , Kei01 , LH14 , RJPL16 , SKL ∗ 16 , AES05 , AZL ∗ 19 ] , • and decision making [ RJPL16 , RAK ∗ 17 , KJTN14 ] . For example , Stolte et al . [ STH02 ] describe EVA as the process of “extract [ ing ] meaning from data : to discover structure , ﬁnd pat - terns , and derive causal relationships . ” In similar spirit , Perer & Shneiderman [ PS08 ] argue that during EVA , analysts seek to “ . . . un - derstand patterns , discern relationships , identify outliers , and dis - cover gaps . ” Alspaugh et al . [ AZL ∗ 19 ] ﬁnd that analysts describe several of their own activities as exploration activities , which were re - classiﬁed by Alspaugh et al . as understanding data semantics and correctness or characterizing data distributions and relationships . Interactions : EVA involves sequences of small , incremental steps ( i . e . , interactions ) to formulate and answer questions about the data [ HMSA08 , GW09 , WMA ∗ 16b ] . Iteration could manifest as multiple interactions with the same data / visualization state , or a move to a new state . Interactions play an integral role in helping an - alysts explore their data [ YKSJ07 , HS12 , jJKMG07 , PSCO09 ] . For example , Jankun - Kelly et al . argue that “ . . . the interaction with both the data and its depiction is as important as the data and depiction itself” [ jJKMG07 ] . Intuitively this makes sense , as ( inter ) actions are the building blocks to complete low - level EVA tasks [ GZ08 ] . Predictability : EVA is also described as “unpredictable” [ STH02 ] , where it may be unclear what the user will do throughout an EVA session . Many factors may inﬂuence predictability . A crit - ical question is whether analysts will produce similar results when performing similar EVA tasks . If analysts approach an EVA task differently , then the outcomes will be hard to predict . If analysts ar - rive at similar answers , with notable overlap in strategies , then there may be opportunities to predict future outcomes [ DC17 , BCS16 ] . Ziemkiewicz et al . [ ZOC ∗ 12 ] argue that differences in users’ indi - vidual experiences drive differences in analysis outcomes with vi - c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau sual analysis tools . It is unclear whether analysts generally utilize similar analysis sequences during EVA , or arrive at similar answers to EVA tasks and subtasks , requiring further investigation . 3 . 4 . Exploration Performance An ambitious goal of visual analytics is to support “ﬂuent and ﬂex - ible use of visualizations at rates resonant with the pace of human thought” [ HS12 ] . Liu & Heer divide [ LH14 ] this goal into two spe - ciﬁc research questions : “ . . . understanding the rate of cognitive ac - tivities in the context of visualization , and supporting these cogni - tive processes through appropriately designed and performant sys - tems . ” Here we discuss themes in the literature focused on measur - ing , supporting and improving : 1 ) the exploration pace and accu - racy of end users and 2 ) the performance of EVA systems . Pacing and Analyst Performance : A number of methods have been developed to measure the pace of exploration . Interaction rate , or the number of interactions performed per unit time , is a com - mon measure of exploration pacing [ LH14 , ZZZK18 , FPH19 ] . In - sight generation rate is also a prominent pacing metric , particularly for open - ended exploration tasks [ ED16 , LH14 , GGZL16 , ZGC ∗ 16 , ZZZK18 ] . Feng et al . [ FPH19 ] propose new metrics , such as ex - ploration uniqueness , to capture more nuanced information from casual , open - ended exploration sessions on the web . Several observations regard how users’ selection of interactions can affect exploration pacing . Guo et al . [ GGZL16 ] ﬁnd that more exploration - focused interactions lead to more insights being gen - erated . More broadly , Lam [ Lam08 ] observes that high cognitive load can impact visual analytic performance . Extrapolating from this observation , high cognitive load interactions , such as writing a SQL query , could lead to a slower exploration pace . Zgraggen et al . [ ZZZK18 ] argue that not only the number of in - sights , but also the quality of insights are critical to gauging the effectiveness of EVA . Their study ﬁnds a 60 % rate of false discov - eries ( i . e . , insights that do not hold for the population - level data ) for unconstrained , open - ended exploration by novices . They ultimately argue that EVA systems should help users formulate a reliable men - tal model of the data , for example more accurate insights . Wongsuphasawat et al . [ WMA ∗ 16b ] evaluate the number of unique data attribute combinations explored by users , to gauge whether exploration sessions increase in breadth when users are provided with useful visualization recommendations . Though not a direct pacing metric , exploration breadth can contribute to an over - all understanding of analysts’ performance . System Performance : We note a general consensus within both the database and visualization communities that response time la - tency is a critical performance measure for EVA systems . For ex - ample , Liu & Heer [ LH14 ] observe that high response time laten - cies ( 500ms or longer ) can impede exploration performance and progress , where analysts may be more sensitive to high latencies for some interactions ( e . g . , brush ﬁlters ) over others ( e . g . , zooming ) . Zgraggen et al . [ ZGC ∗ 16 ] observe similar outcomes when evaluat - ing progressive visualizations . Idreos et al . [ IPC15 ] survey a range of database projects focused on optimization and performance for EVA contexts , and also observe that response time latency is the primary performance measure within these projects . To study the effects of latency , both Liu & Heer [ LH14 ] and Zgraggen et al . [ ZZZK18 ] inject latency into EVA systems and measure the resulting interaction rates of analysts to gauge system performance . The idea is that latency will likely slow the user’s ex - ploration progress , resulting in fewer interactions over time . Crotty et al . [ CGZ ∗ 16 ] propose optimizations to reduce system latency for big data EVA contexts , in an effort to improve interaction rates . Rather than measuring interaction rates , one can instead measure the average or worst case latencies observed per interaction , which several database research projects utilize to evaluate optimizations for EVA systems [ CXGH08 , KJTN14 , BCS16 , CGZ ∗ 16 , RAK ∗ 17 ] . To measure effects over an entire EVA session , alternative met - rics include total exploration time ( i . e . , the duration of a single EVA session ) [ DPD14 , FPH19 ] , and total user effort ( i . e . , total interac - tions performed ) [ DC17 , DPD14 , GW09 , FPH19 ] . These metrics are often utilized to gauge whether recommendation - focused optimiza - tions help users to spend less time and effort exploring the data to achieve their analysis goals [ DPD14 ] . Pacing Optimization Constraints : Multiple projects further constrain EVA system optimization by not only positing latency constraints ( e . g . , system response time latencies under 500ms ) , but also assuming a rapid pace of exploration , where users quickly per - form successive interactions . For example , Gotz & Zhou [ GZ08 ] argue that “During a visual analytic task , users typically perform a very large number of activities at a very fast pace , ” implying that users perform interactions quickly during most visual ana - lytic tasks ( including EVA ) . Narrowing the scope to EVA , Fisher et al . [ FPDs12 ] argue that “In exploratory data visualization , it is common to rapidly iterate through different views and queries about a data - set . ” In a more recent example , Battle et al . [ BCS16 ] deploy new optimizations to reduce response time latency for pan - and - zoom applications by prefetching data regions ( i . e . , data tiles ) that the user may pan or zoom to next . Battle et al . argue that due to the presumably fast pace of EVA , the system “ . . . may only have time to fetch a small number of tiles before the user’s next request , ” motivating a need for accurate and efﬁcient prediction and prioriti - zation of the set of tiles to prefetch before the user’s next interac - tion . This work seems to argue that due to the fast pace of EVA , the time between interactions ( or think time ) is restricted , limiting how we deploy sophisticated ( e . g . , predictive ) optimizations for EVA . 3 . 5 . Synthesized Deﬁnition of EVA Exploratory data analysis ( or EDA , originally coined by John Tukey [ Tuk77 ] ) encompasses the tasks of learning about and mak - ing sense of a new dataset . We deﬁne exploratory visual analysis ( or EVA ) as a subset of exploratory data analysis , where visualiza - tions are the primary output medium and input interface for explo - ration . EVA is often viewed as a high - level analysis goal , which can range from being precise ( e . g . , exploring an existing hypothesis or hunch ) to quite vague ( e . g . , wanting to ﬁnd something “interest - ing” in the data ) . During EVA , the analyst updates and reﬁnes their goals through subsequent interactions with and manipulation of the new dataset . Due to the inherent complexity in accomplishing high - level exploration goals , analysts often decompose their exploration into a series of more focused visual analysis subtasks , which in turn could be partitioned further into smaller subtasks , and so on . Sev - eral visual analysis subtasks are commonly associated with EVA : c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau assessing the quality and semantics of the data , discovering un - derlying relationships and statistical distributions within the data , formulating and verifying hypotheses , and evaluating causality or more complex models on the data . These subtasks are not unique to EVA , but occur often during EVA contexts nonetheless , and range from being more focused ( i . e . , data quality assessment ) to more open - ended ( i . e . , causality analysis and modeling ) . We note that this deﬁnition is quite broad , revealing a lack of precision and consistency in how EVA is discussed in the literature . 3 . 6 . Summary We now summarize our review and posit research questions , focus - ing on topics that lack consensus or corroborating evidence . Goals Summary : Discrepancies across the literature suggest that we lack a shared deﬁnition that covers the various ways EVA goals are formulated . Analysts who perform EVA may come to it with a clear intent ( i . e . , explicit goals and tasks ) , with no clear intent ( i . e . , no pre - conceived goals or tasks ) , or somewhere in between ( i . e . , vague goals , a few initial tasks ) . As such , it seems beneﬁcial to account for both top - down ( i . e . , focused ) and bottom - up ( i . e . , op - portunistic ) exploration strategies when evaluating EVA behavior . We utilize these insights in the design of our exploratory study . Structure Summary : Prior work suggests that actions and tasks during EVA are not purely opportunistic : exploration behavior is not only dependent on what is observed from the data , but also on the analyst’s goals and experience [ PS08 , RJPL16 ] . Analysts may incorporate concrete analysis steps or tasks that they have per - formed in the past . However , analysts may select among these tasks opportunistically , based on what they observe in the data . Thus ex - ploration sessions appear to have some structure to them , though the discrepancies in the literature make it difﬁcult to reason about what structural properties to expect for a speciﬁc exploration use case ( i . e . , for real log data collected from a study on EVA ) . Here , we focus on two speciﬁc aspects of EVA structure , organization and overlap ( or predictability ) : • ( S1 ) : How are focused and open - ended EVA sessions organized ( e . g . , are they breadth - or depth - oriented ) ? ( Section 6 . 2 ) • ( S2 ) : How predictable are participants’ EVA paths , given differ - ences in task speciﬁcity / open - endedness ? ( Section 6 . 3 ) Performance Summary : We ﬁnd consensus around popular performance metrics for EVA ( interaction rates , response time la - tency ) and their outcomes ( latency hinders exploration ) . However , more recent metrics and assumptions have only been measured in a limited number of experiments : the accuracy of insights , and ex - ploration breadth , uniqueness , and pace . We focus on two metrics in our performance analysis , accuracy and pacing : • ( P1 ) : How does the accuracy of EVA compare for focused , goal - directed EVA ? ( Section 5 . 1 ) • ( P2 ) : How does the pace of EVA inﬂuence available time for deploying system optimizations ? ( Section 5 . 2 ) 4 . Exploratory Study Design Our literature review answers some questions , but also produces new ones . To further investigate our research questions in Sec - tion 3 . 6 , we designed an exploratory study of analysis behavior . Our goal in the study design is to capture realistic visual analy - sis behavior during speciﬁc analytic subtasks relevant to EVA . To ensure that participants could use a familiar tool , we selected the commercial tool Tableau for analysis . Even one commercial tool can still provide useful insights into the strategies and needs of end users [ GTS10 , HMSA08 , BDM ∗ 18 ] . Therefore , we focused our ef - forts on this one tool , allowing us ensure it was properly instru - mented , designing realistic visual analysis subtasks for analysis , and recruiting local analysts with Tableau experience to participate . We discuss the limitations of our study design in Section 4 . 6 . 4 . 1 . Participants We recruited participants via university mailing lists and local Tableau User Groups ( e . g . , message boards , meet - ups ) . 27 Tableau users participated in the study ( 10 male , 17 female , age 23 - 47 years ) . 22 participants were recruited from our university cam - pus , 5 from our metropolitan area . Participants had no prior ex - perience with the study datasets , and used Tableau either for work , or through classes . Participants varied widely in Tableau and data analysis experience , from just learning Tableau ( including 13 stu - dents ) to seasoned veteran analysts to Tableau power users . 4 . 2 . Protocol Participants completed an initial survey online . Qualifying partici - pants completed a 90 - 120 minute in - person session ( on campus , or at their workplace ) , consisting of : 1 ) study overview and consent form ; 2 ) 5 minute warm - up with Tableau on a movies dataset ; 3 ) 30 minute analysis block with one dataset ; 4 ) 30 minute block with another dataset ; and 5 ) exit survey . Analysis blocks included a 5 minute warm - up for the given dataset . Task sheets were provided , with 4 visual analysis subtasks to complete per block ( 8 total ) . Sub - task prompts were printed for participants , as well as a dataset sup - plement document deﬁning the data attributes , and a map of the USA . Participants were allowed to take notes , if desired , and were compensated with a $ 25 Amazon gift card . A 15 - inch Macbook Pro with Tableau Desktop pre - installed was provided to participants . Datasets were loaded directly into Tableau Desktop 10 . 3 ( the latest version that supported the logging features required for analysis ) . 4 . 3 . Datasets We evaluate three real - world datasets , selected for similar complex - ity , sufﬁciently large size to simulate large - scale analysis , relevance to real - world questions ( irrelevant attributes were removed ) . Fur - thermore , we selected datasets used in previous studies of visual analysis and exploration behavior : ﬂight performance data [ LH14 ] , wildlife strikes [ WMA ∗ 16b ] , and weather data [ KH18 ] . 1 . FAA ( 31 columns , 34 . 5M rows , 5 . 36GB ) ‡ : recorded ﬂights , with itinerary ( destination , distance , etc . ) and performance mea - sures ( arrival delays , cancellations , etc . ) . 2 . Weather ( 35 columns , 56 . 2M rows , 3 . 53GB ) § : daily weather station reports , containing measures ( precipitation , temperature , etc . ) and observed phenomena ( e . g . , tornados , ground fog , etc . ) ‡ https : / / www . transtats . bts . gov / Tables . asp ? DB _ ID = 120 § https : / / www . ncdc . noaa . gov / ghcn - daily - description c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau Dataset Subtask Prompt Birdstrikes T1 Consider these four parts of the aircraft : engine 1 ( [ Dam Eng1 ] ) , engine 2 ( [ Dam Eng2 ] ) , the windshield ( [ Dam Windshield ] ) , and wing / rotor ( [ Dam Wing Rot ] ) . Which parts of the aircraft appear to get damaged the most ? Birdstrikes T2 Which aircraft classes ( [ Ac Class ] ) , if any , appear to be more susceptible to damage ( [ Damage ] ) from animal strikes ? Note that [ Damage ] also records when no damage has occurred . Birdstrikes T3 What relationships ( if any ) do you observe involving weather conditions ( [ Precip ] , [ Sky ] ) and strike frequency , or counts over time ( [ Incident Date ] ) ? Birdstrikes T4 What are the most common conditions for an animal strike ? Note that this is not limited to weather conditions , any dataset columns that are interesting to you can be included . FAA T1 How do cancelled ﬂights ( [ Cancelled ] ) , diverted ﬂights ( [ Diverted ] ) , and delayed ﬂights ( [ ArrDelay ] , [ DepDelay ] ) compare in terms of counts or frequency ? FAA T2 What patterns ( if any ) do you observe in the count of ﬂights over time ( [ FlightDate ] ) ? If any patterns are observed , what deviations ( if any ) do you see for individual airlines ( [ UniqueCarrier ] ) ? FAA T3 What relationships ( if any ) do you ﬁnd involving ﬂight distance ( [ Distance ] ) and arrival delays ( [ ArrDelay ] ) ? FAA T4 Suppose Delta Airlines wants to expand 3 airports . Based on your analysis of the data , which 3 airports would you recommend to Delta Airlines ( airport code DL ) ? Existing Delta Airlines airports , and / or airports that Delta doesn’t cover , can be included in your analysis . Weather T1 Consider the following weather measurements : Heavy Fog ( [ Heavy Fog ] ) , Mist ( [ Mist ] ) , Drizzle ( [ Drizzle ] ) , and Ground Fog ( [ Ground Fog ] ) . Which measurements have more data ? Which weather measures ( if any ) would you remove from the dataset ? Weather T2 How have maximum temperatures ( [ T Max ] ) and minimum temperatures ( [ T Min ] ) changed over the duration of the dataset ( i . e . , over the [ Date ] column ) ? Weather T3 How do the wind ( [ High Winds ] ) measurements compare for the northeast and southwest regions of the US ? Weather T4 What weather predictions would you make for February 14th 2018 in Seattle , and why ? Table 1 : Visual analysis subtask prompts given to participants for each dataset . 3 . Birdstrikes ( 94 columns , 173K rows , 91MB ) ¶ : incidents of aircraft ( e . g . , airplanes ) striking wildlife ( e . g . , deer , birds ) , with contextual details ( e . g . , weather conditions , total struck , etc . ) . 4 . 4 . Visual Analysis Subtasks The goal of this study is to better understand analysts’ visual ana - lytic behavior at ﬁne granularity , with respect to EVA contexts . To support a “micro - analysis” of visual analysis behavior during EVA , we focus on speciﬁc subtasks that may occur during EVA sessions , according to our literature review in Section 3 . 3 . Selecting Visual Analysis Subtasks : We ﬁnd in Section 3 . 3 that analysts utilize both a top - down and bottom - up exploration approach , which often includes performing multiple , focused sub - tasks . We ﬁnd that a variety of subtasks are observed during EVA , including tasks that are not traditionally associated with EVA . Though the precise order of these subtasks may vary from analyst to analyst , there does appear to be a common progression through the different subtask types that maximizes the effectiveness of an EVA session . First , analysts explore to learn the data’s structure and semantics , which lays the foundation for understanding more complex structures and phenomena in the data . Then analysts look for statistical patterns and relationships between different data vari - ables . Given a basic understanding of the relationships between variables , analysts can then move to deeper exploration , tied to ¶ https : / / wildlife . faa . gov / more complex subtasks such as causality analysis , forecasting , and decision making . We treat these visual analysis subtasks as sepa - rate categories , and design study tasks for each : 1 ) data quality as - sessment ( T1 ) ; 2 ) evaluation of patterns and relationships between variables ( T2 , T3 ) ; and 3 ) causality and prediction analysis ( T4 , open - ended ) . All subtasks are listed in Table 1 . Ordering Subtasks : Given that learning effects are a natural part of the EVA process — and our aim to study realistic EVA be - havior — we decided to have all participants complete subtasks in the same order ( T1 , T2 , T3 , T4 ) , dictated by the natural order of their respective categories . For example , data quality assessment ( T1 ) is often performed before causality analysis ( T4 ) [ KPHH12 ] . Managing Complexity : To ensure adequate breadth for analy - sis , multiple factors of complexity were balanced across subtasks : 1 ) total attributes analyzed per dataset , 2 ) diversity of attributes across subtasks ( e . g . , temporal versus spatial , dimensions versus measures ) , 3 ) relevance of subtasks to real - world equivalents . The complexity of all subtasks within a category ( T1 , T2 , etc . ) were balanced across the three datasets . Subtask Durations : Zgraggen et al . asked participants to per - form open - ended EVA tasks for 15 minutes [ ZZZK18 ] , where par - ticipants wrote down every insight they found . Given our more fo - cused subtasks and less rigid recording , we chose shorter durations : 7 for the ( simpler ) subtasks T1 - T3 , and 9 minutes for the more open - ended subtask T4 . We veriﬁed through pilots that these sub - tasks could easily be completed within these time frames before conducting the study . c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau 4 . 5 . Data Collection & Processing Data Sources : We collected raw logs from Tableau Desktop and Tableau Data Engine ( or TDE , an internal database system ) . Screen capture was also recorded to contextualize our log analysis . Data from one participant was removed due to experiment error . Interactions : All events were extracted from the logs and com - pared with the screen capture to identify interactions ( e . g . , add at - tributes to shelf , change mark type , etc . ) . These events were then labeled with established interaction types [ HMSA08 ] . Shelf State and Visual Encodings : Tableau automatically records shelf state ( Row , Column , Filter , and Encoding shelves ) . These entries were extracted and then stored with their correspond - ing interactions in a larger master log ( matched via timestamps ) . Data Transformations and Queries : Data transformations were extracted from Tableau shelf state ( e . g . , sum : snow , count : damage ) , as well as TDE logs . Tableau uses a separate query language to interact with the TDE . We developed a parser for the Tableau queries to perform the parameter extraction . 4 . 6 . Study Design Limitations Our study design differs signiﬁcantly from previous studies of EVA behavior , which generally preserve the natural structure of entire exploration sessions , rather than capturing a series of focused sub - tasks . Our analysis does not capture a completely faithful represen - tation of EVA , but instead simulates ( some of ) its individual parts . Understandably , this design will not allow us to make high - level inferences about EVA behavior . However , what we lose in con - textual accuracy ( i . e . , capturing complete EVA sessions ) we par - tially make up for in precision ( i . e . , knowing exactly what subtask is being performed and when ) . This study design helps us to better understand speciﬁc , low - level behavioral patterns , and the signif - icance of these patterns within common visual analysis subtasks that can occur during EVA ( as well as other analysis contexts ) . We note here that some important aspects of EVA sessions are still preserved in our design , such as selecting subtasks that occur during EVA , and imposing a subtask ordering that aims to maxi - mize the effectiveness of the selected subtasks for EVA contexts . For example , an analyst will not be able to make sense of relation - ships between data attributes ( subtasks T2 and T3 ) without ﬁrst ver - ifying their understanding of the individual attributes themselves ( subtask T1 ) . Similarly , an analyst will not be able to effectively make predictions or assess causality amongst data attributes ( sub - task T4 ) without ﬁrst identifying and understanding the underlying relationships between them ( subtasks T2 and T3 ) . 5 . Task Performance We report on the accuracy and pacing of study participants , and compare our results to assumptions in the literature . 5 . 1 . Evaluating Task Completion Rates & Accuracy ( P1 ) Most participants successfully completed all tasks : Figure 1 shows the fraction of successful completions for tasks T1 - T3 ( tasks with measurable correctness ) . Incorrect answers stemmed from im - proper analysis , such as failing to consider the low number of long - distance ﬂights for task FAA T3 , or including strike records that do dataset birdstrikes1 faa1 weather1 t1t2t3 t a s k 0 10 0 10 Total Observations 0 10 correctincompleteincorrect status Figure 1 : Task performance by dataset , labeled as : correct answer , incorrect answer , or did not ﬁnish ( incomplete ) . dataset t1t2t3 t a s k birdstrikes1 faa1 weather1 0 5 10 15 0 5 10 15 Total Users 0 5 10 15 0 1 2 3 Answer Figure 2 : Total answers observed for each dataset and task , and the total participants that arrived at each answer . not actually result in damage for Birdstrikes T2 . A small number of participants had outwardly correct answers ( i . e . , similar in text as other participants’ ) , but the observed analysis involved glaring er - rors ; we conservatively labeled these cases as incorrect . Incomplete answers are cases where the participant ran out of time . Most participants completed tasks on time . When a task was not completed , the cause often involved attempts to use complicated in - teractions , such as sophisticated ﬁlters ( e . g . , FAA task T1 ) or com - plicated grouping and binning operations ( e . g . , Weather task T3 ) . Participants provided correct answers : Overall , participants provided accurate responses ( Figure 1 ) : 80 . 6 % ( 116 / 144 ) of task sessions had correct answers . Rates per task were 81 . 3 % ( 39 / 48 ) for T1 , 91 . 7 % ( 44 / 48 ) for T2 , 68 . 8 % ( 33 / 48 ) for T3 . For each dataset , over 50 % of participants successfully completed all three tasks . Participants were cautious analysts : We designed two tasks that asked participants to assess relationships between variables for which there was no clear correlation ( Birdstrikes T3 and FAA T3 ) . We observe the worst case error rate of 25 % in FAA T3 : 4 incor - rect answers out of 16 . In all other tasks , ~ 81 % of participants who complete the task provide correct answers . Though not an exact comparison , Zgraggen et al . [ ZZZK18 ] observed a 60 % false dis - covery rate when asking participants to explore ( i . e . , search for ) interesting relationships in the data . We believe the higher accura - cies in our study stem from our focus on realistic task outcomes from experienced users , rather than all verbalized open - ended ob - servations by novices . Participants regularly compared visualizations with the raw data , to ensure that the visualizations matched their expectations . Re - gardless of correctness , participants qualiﬁed their answers with comments on their conﬁdence in the rendered results . For task FAA T3 , ﬁve participants made comments suggesting a distrust of look - ing only at their visualizations , including one participant who had given an incorrect answer . Two participants said that their hypothe - ses required further study , two participants commented on the dis - parity in available data between short and long distance ﬂights , and one participant said the delays for long distance ﬂights did not look “quite right . ” One participant described how “Just by looking , one might be tempted to say higher distance lower delay , but I wouldn’t say that because there’s more data for shorter delays , ” showing the cautious evaluation style adopted by many participants . Most participants arrive at the same answers : Figure 2 shows c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau 1 2 3 4 5 6 7 8 9 10 11 12 InteractionsPerMinute T4 T3 T2 T1 Figure 3 : Mean interaction rates and 95 % CIs of the mean . 10 11 12 13 14 15 16 17 18 19 20 Think Time ( secs ) T1 T2 T3 T4 Figure 4 : Mean think times , with 95 % CIs of the mean . the total unique answers observed for tasks T1 - T3 . We observed 1 - 4 answers per task . Most participants arrive at the same answer . For example , in the Birdstrikes T1 task , 15 participants conclude that the wing or rotor of aircraft are damaged most often during strikes . In the FAA T3 task , 10 participants conclude that there is no clear relationship between distance and arrival delays . 5 . 2 . Evaluating Pacing : Interaction Rate & Think Time ( P2 ) We observe similar interaction rates across tasks : To assess the effects of task on interaction rate , we ﬁt linear mixed effects mod - els , with task as a ﬁxed effect , and participant and dataset as random effects . We test signiﬁcance by comparing the full model to a “null” model with the target ﬁxed effect removed ( task ) , using likelihood - ratio tests . Figure 3 plots expected mean interaction rates and 95 % conﬁdence intervals . We do not ﬁnd a signiﬁcant effect of task on interaction rates ( χ 2 ( 3 , N = 192 ) = 2 . 194 , p = 0 . 533 ) . Thus , inter - action rates appear to be independent of task type . Think times are long : An ambitious goal for EVA systems is to support interaction at the “speed of human thought” [ HS12 , CGZ ∗ 16 ] . We seek to better understand the pacing of visual anal - ysis tasks that occur during EVA , where several projects seem to argue that EVA is generally fast - paced [ GZ09 , BCS16 , FPDs12 ] . Many systems , particularly from the database community , pro - pose specialized optimizations to improve performance under con - strained pacing conditions ( i . e . , short time between interactions , or think time [ BCS16 ] ) . However , the “speed of thought” might in - volve spans of viewing data , drawing comparisons , and planning next steps . If ample time is available between interactions for data pre - processing , complicated ( e . g . , predictive ) optimizations may not be needed : we can use these think times to deploy simpler op - timizations and achieve similar results . Rendering times and query times in Tableau represent a small fraction of the time between interactions ( 1 % and 14 % , respec - tively ) . Thus the majority ( 85 % ) of this ( think ) time can be at - tributed to users ( e . g . , interpreting visualizations , selecting an in - teraction , etc . ) . We operationalize think times as the time from the end of the current interaction to the start of the next one , subtracting query and rendering times . Figure 4 shows mean think time across tasks . The means are notably high , ranging from 14 to 15 seconds , depending on the task . Median think times are 5 - 7 seconds , indicat - ing skew . Depending on task , we ﬁnd that 53 - 61 % of think times are 5 seconds or longer , and 32 % - 41 % are 10 seconds or longer . Next we consider the mean think times preceding each inter - action type ( Figure 5a ) , which generally fall in the 10 - 20 sec - ond range . Median think times range from 4 . 7 ( undo ) to 29 sec - onds ( data - derive ) . From our study observations , we ﬁnd that data - derive is of high cognitive complexity , because it in - volves reasoning about and writing formula expressions . The gap between data - derive and other interactions shows that differ - ences in cognitive complexity can directly impact the pacing of EVA . Lam also ﬁnds that high cognitive complexity leads to high interaction costs [ Lam08 ] , but refers speciﬁcally to selecting from a large space of possible interactions . We ﬁnd that particular in - teractions with high complexity also lead to high interaction costs . Consider the ForeCache system [ BCS16 ] discussed in Sec - tion 3 . 4 , which predicts the user’s next interaction and fetches the corresponding data . Fetching data in ForeCache takes about 1 sec - ond , and users can perform 9 interactions ( 4 panning and 5 zooming directions ) . Suppose we observe similar interaction rates as Tableau ( median navigation think time is 9 seconds ) . Without special opti - mizations , we could simply pre - fetch the data for all 9 potential interactions , one at a time , and with high probability fetch the re - quired data before the user’s next interaction . Though hypothetical , this example shows that without the full context of latency and pac - ing , we may devise unrealistic performance constraints for EVA . Interactions appear “bursty” : We ﬁnd that participants oscil - late between spending relatively more time choosing an interaction , then less time on a subsequent sequence of interactions . Figure 5b shows the observed think times for the FAA T4 task , with relatively short think times in light blue , and longer think times in dark blue ( i . e . , below or above the mean think time for FAA T4 , respectively ) . Shorter think times appear to be clustered together , which we ob - served across tasks , with participants performing on average 3 - 4 fast interactions per sequence . These results may suggest that ana - lysts formulate a plan for the next few interactions ( i . e . , a subtask ) , and then execute their plans , before deciding what to analyze next . 6 . The Structure of Analysis Sessions We now analyze the structure of participants’ analyses and compare with existing structural assumptions . 6 . 1 . Deﬁning Analysis States & Search Trees As deﬁned by Wongsuphasawat et al . , an analysis state is the set of attributes currently being analyzed [ WMA ∗ 16b , WQM ∗ 17 ] , for which a user may specify visual encodings , apply ﬁlters , or group and aggregate the data . Interactions with Tableau can add to , re - move from , or otherwise modify the current attribute set , producing new analysis states . We ﬁrst construct a raw graph of all states and interactions Each time a participant adds a new attribute to the cur - rent visualization , a forward edge is included in the raw graph from the current attribute set to the new ( larger ) attribute set . When at - tributes are removed , a backward edge is included in the graph from the current attribute set to the new ( smaller ) set . We then remove the backward edges and self - loops to form search trees . Figure 6a shows the raw graph for one participant from task Weather T4 . We create a new state for each observed attribute set , and a directed edge between corresponding states for each interaction . Edge color encodes interaction type , and width repeated interactions . Figure 6b shows an example search tree . c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau 0 500 1 , 000 1 , 500 Number of Records 0 20 40 60 80 100 120 Think Time Before Interaction ( secs ) analysis - ﬁlter analysis - sort data - bin data - derive formatting formatting - data navigateredoshelf - add shelf - remove show - me undo worksheet - add worksheet - duplicate I n t e r ac t i o n T y p e ( a ) > mean < mean Think Time 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Interaction ID 09 109 13 21 33 37 41 45 57 65 69 81 85 89 93 u s e r i d ( b ) Figure 5 : ( a ) Means and 95 % conﬁdence intervals for think time before each interaction ( left ) , and total records per interaction type ( right ) . ( b ) Observed think times for FAA T4 , colored light blue when below mean think time , dark blue when above the mean . ( a ) Graph Weather T4 . ( b ) Tree Weather T4 . dataset 0 . 0 0 . 5 1 . 0 M ea n A s p ec t R a t i o birdstrikes1 faa1 weather1 t 1 t 2 t 3 t 4 task t 1 t 2 t 3 t 4 task t 1 t 2 t 3 t 4 task ( c ) Box Plot for Tree Mean Aspect Ratios . Figure 6 : Raw graph ( a ) and matching search tree ( b ) for one participant , task Weather T4 . ( c ) Distribution of tree aspect ratios . 6 . 2 . Breadth - Versus Depth - Oriented Analysis ( S1 ) Wongsuphasawat et al . deﬁne depth and breadth in terms of the total unique states analyzed [ WMA ∗ 16b , WQM ∗ 17 ] ( also consid - ered by Sarvghad et al . [ STM17 ] ) . However the interactions that produce these states , and thus the states themselves , are not in - dependent . As we ﬁnd in Section 3 . 3 , interactions are part of a larger , hierarchical process , where they may be clustered together within a subtask ( i . e . , an analysis branch within a search tree ) . We extend these deﬁnitions of depth and breadth to consider the full EVA structure ( i . e . , search trees ) : branches in the tree correspond to breadth in subtasks or analysis trajectories , tree depth to effort or emphasis ( i . e . , total interactions ) for a speciﬁc ( sub ) task . Greater breadth indicates that a participant more frequently backtracked to previous states and then branched off , greater depth indicates that a participant engaged in less backtracking . Sessions are primarily depth - oriented : Branching to different sub - analyses ( i . e . , subtasks ) was fairly common . Branches trans - late to leaves in the search trees ( e . g . , nodes 3 , 7 , and 9 in Fig - ure 6b ) . 51 . 0 % ( 98 / 192 ) of trees have multiple branches ( i . e . , > 1 leaf ) , and 25 / 26 participants had multiple branches in at least one task . Figure 6b shows the two forms of branching we observed in Tableau : 1 ) within - sheet branching , and 2 ) between - sheet branch - ing ( e . g . , creating sheets , returning to previous sheets ) . With the ﬁrst form of branching , the user makes small pivots from the cur - rent analysis state ( e . g . , removing one of the current data columns being analyzed ) . To assess whether search trees tend to be depth - or breadth - oriented , we calculate an aspect ratio : we divide a tree’s width ( max breadth ) by its height ( max depth ) . Figure 6c shows the distribution of aspect ratios , which consistently have aspect ra - tios below one , showing greater depth than breadth . These ﬁndings are consistent with Wongsuphasawat et al . ’s claims that analyses in Tableau are primarily depth - oriented [ WMA ∗ 16b , WQM ∗ 17 ] . 6 . 3 . Predictability & Overlap ( S2 ) We assess the predictability of EVA by measuring structural sim - ilarity between sessions , which could indicate likely outcomes for a given task , or relevant interaction patterns , such as iteration . We compute similarity in two ways : 1 ) overlap in states visited across participants and 2 ) revisitation of states ( i . e . , self - loops , iteration ) . Certain states show high overlap : Figure 7a shows a binned chart of overlaps , where the bins along the x - axis represent the to - tal participants that visited each state in the given bin ( i . e . , blue cir - cle ) . Circle area encodes the total unique states that had exactly x visitors , for each task ( y - axis ) and dataset ( columns ) . For example , relatively large circles in the far left column ( per datasets ) means that most states were visited by only 1 participant . For task T4 we observe a steep drop - off , with only a few states being seen by most users . Tasks T1 and T3 exhibit more multi - modal distribu - tions , with one cluster of states that only a few people visit , and another where many people see the respective states . To further quantify overlap , we calculate a binary histogram of visited states per participant , where each bin represents a unique state , and the bin is set to 1 if this state was visited . We compare pairs of participants using a modiﬁed version of Jaccard similarity : | A ∩ B | min ( | A | , | B | ) . This measure avoids penalizing pairings where one participant vis - ited many more states . The average similarity is as follows : T1 ( 0 . 56 ) , T2 ( 0 . 61 ) , T3 ( 0 . 57 ) , T4 ( 0 . 22 ) . We ﬁnd high average over - lap in T1 - T3 , suggesting somewhat predictable analysis steps or outcomes . Average overlap is lower for T4 . As we discuss in greater c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau detail later , we believe this is related to participants taking a high - level goal ( T4 ) and decomposing it into focused subtasks . Overlapping states align closely with task goals : We compute the top 3 overlapping nodes for each dataset and task , and compare them to participants’ task goals ( as indicated by task prompts ) . We observe that these states subsume the columns needed to complete the task for 8 of 12 tasks . Thus participants tend to overlap at the key states needed to complete a task . These particular nodes often appear later within the search trees ( i . e . , close to or as leaf nodes ) . For example , the top 3 overlapping states for each task appear as leaf nodes up to 30 % of the time , depending on the task . Self - loops signal key analysis states : Participants frequently it - erate on the same analysis state , forming self - loops in the resulting graphs . 39 . 1 % of all states have self - loops . We also ﬁnd at least 1 self - loop in 95 . 8 % ( 184 / 192 ) of analysis graphs . Most self - loops involve data transformations ( 42 . 2 % of all self loops ) , followed by shelf ( 29 . 2 % ) and formatting ( 10 . 7 % ) interactions . Most shelf in - teractions involve moving the current attributes to different shelves . Self - loops highlight states where participants place more analysis effort , showing that this state may be a useful landmark within an EVA session . We computed the fraction of observed self - loops per state , participant , dataset , and task . We then summed the results per state , across all participants . In 11 of 12 tasks , at least one of the top 3 self loop - states also appears in the top 3 overlapping states . Thus self - loops serve as indicators of signiﬁcant analysis states . 6 . 4 . Open - Ended Versus Focused Analysis ( S1 , S2 ) Speciﬁcity does not affect relative breadth / depth : We evalu - ate the effects of task on tree dimensions by ﬁtting linear mixed - effects models on the normalized max depth and breadth of the trees ( i . e . , divided by tree size ) . We include task as a ﬁxed ef - fect , and participants and dataset as random effects . We compare with a “null” model ( i . e . , with task removed ) using likelihood - ratio tests , and do not ﬁnd signiﬁcant effects of task on normal - ized max depth ( χ 2 ( 3 , N = 191 ) = 4 . 878 , p = 0 . 181 ) or breadth ( χ 2 ( 3 , N = 191 ) = 2 . 495 , p = 0 . 476 ) . Figure 7 shows means and 95 % conﬁdence intervals . Our results indicate that open - ended and focused sessions are structurally similar in depth and breadth . Open - ended tasks may decompose into focused tasks : On the surface , open - ended structures appear to diverge . For example , T4 has lower average modiﬁed Jaccard similarity ( see Section 6 . 3 ) . However , this analysis misses the hierarchical nature of more open - ended tasks , and of exploration in general , discussed in Section 3 . 2 : participants likely decompose T4 into focused subtasks to make it more manageable . Individual subtasks could be characterized and compared to obtain a more accurate similarity measure , which we plan to investigate in the future . 7 . Findings & Future Work In this paper , we explore existing deﬁnitions of EVA , and iden - tify three major themes in the literature : goal formulation , explo - ration structure , and performance . Within these themes , we high - light points of connection and contradiction , from which we for - mulate research questions for further study : organization ( S1 ) , pre - dictability ( S2 ) , accuracy ( P1 ) , and pacing ( P2 ) in EVA . We present a study in which 27 Tableau users completed a range of analysis tasks across 3 datasets , informed by our literature review . We use the resulting data to provide empirical context towards answering these research questions ; our results provide useful insights that may help to inform future analyses and system evaluations . We ﬁnd that many implicit assumptions are made about EVA across the literature . EVA is often referenced , but rarely deﬁned ( with notable exceptions , e . g . , [ AZL ∗ 19 , jJKMG07 ] ) . Lam et al . observe a similar dearth of explicit information on EVA [ LTM18 ] . We hope that this work will help to motivate our community to rethink the way we perceive , design , and evaluate in EVA contexts . 7 . 1 . Analysis Performance Most participants successfully completed the tasks , with error rates of at most 25 % per task . Our results differ from prior work [ ZZZK18 ] , which report error ( false discovery ) rates over 60 % for participants assessing data properties and relationships . Though not a perfect comparison , these differences could be at - tributed to several factors . We provided speciﬁc tasks rather than encouraging more vague open - ended explorations ; that structure may have helped to focus participants . Unlike earlier insight - based evaluations ( e . g . , [ GGZL16 , ZZZK18 , LH14 ] ) , we did not require participants to vocalize all “insights” as they went along , regard - less of overall relevance . Again , our participants were typically fo - cused on a speciﬁc goal . We plan to evaluate how the articulation of speciﬁc questions and goals—structuring open - ended exploration into more explicit , albeit evolving , tasks—may affect analysis out - comes in other contexts . While both our study and that of Zgraggen et al . involved analyzing unfamiliar data , our study involved users with prior analysis experience in real - world environments , rather than a novice , student - only population . This difference in experi - ence may have helped our participants exercise caution . Insight ( P1 ) : Analysts tend to approach analysis tasks with care , as well as a priori goals and hypotheses , which may lead to better outcomes for analysis subtasks completed during EVA contexts . Latency is a key performance metric for evaluating EVA sys - tems . However , the pace of EVA may be characterized unrealisti - cally in parts of the literature [ GZ09 , BCS16 , FPDs12 ] , to further constrain ( and sometimes complicate ) system optimization con - texts . We observe 14 - 15 seconds on average of idle time or think time between interactions , resulting in a slower pace for EVA , and providing ample time for simpler optimization methods to be de - ployed with presumably similar performance results . We measured think times for different interaction types and found similar results , with one exception : data - derive interactions take longer to reason about , resulting in longer think times . We also observe “bursty” in - teraction patterns , where analysts repeatedly have a longer think time ( presumably planning next steps ) , followed by a sequence of shorter think times ( i . e . , plan execution ) . With a better understand - ing of pacing , we can construct more realistic evaluations , and test large - scale EVA systems with appropriate parameters . Insight ( P2 ) : EVA system optimizations should consider not only latency , but also pacing and available resources , like think time . 7 . 2 . Analysis Goals and Structure Participants’ analyses were primarily depth - oriented , validating previous calls to prompt greater exploration breadth [ WMA ∗ 16b , WQM ∗ 17 ] . We found strong similarities in interaction rates , task c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau dataset T1 T2 T3 T4 t ask birdstrikes1 faa1 weather1 0 4 8 12 16 Overlap in Users 0 4 8 12 16 Overlap in Users 0 4 8 12 16 Overlap in Users ( a ) Overlap in states visited . 0 . 20 0 . 24 0 . 28 0 . 32 0 . 36 Normalized Max Breadth T4 T2 T1 T3 ( b ) Means and CIs for max tree breadth . 0 . 20 0 . 24 0 . 28 0 . 32 0 . 36 NormalizedMaxDepth T4 T2 T1 T3 ( c ) Means and CIs for max tree depth . Figure 7 : ( a ) Counts of users that overlap per state . Means and 95 % conﬁdence intervals for normalized max ( b ) breadth and ( c ) depth . answers , and relative breadth and depth across tasks , suggesting analysts use similar analysis strategies , regardless of task . Insight ( S1 ) : Analysis search trees in Tableau are primarily depth - oriented , and consistent across subtasks of varying speciﬁcity . Individual differences between analysts could result in differ - ences in EVA strategies and performance [ ZOC ∗ 12 ] , affecting the predictability of EVA behaviors . Our results hint towards a com - plex picture of predictability in analysis tasks in general . Strong overlap and predictable patterns do emerge , but in speciﬁc situa - tions , often when the analysis becomes more focused , such as dur - ing more focused subtasks , towards the end of a subtask , or during iteration ( i . e . , self - loops in analysis graphs ) . Signiﬁcant overlap in states visited across participants usually represented critical points in participants’ analyses , where they were close to achieving task goals . Consecutive interactions within a state ( i . e . , self - loops ) also correlate with critical analysis points . Less overlap was observed in more open - ended tasks , however our selected metrics may ob - scure participants’ decomposition of open - ended subtask prompts into even smaller subtasks , requiring further study . These results could inform the design of EVA behavior models learned from log data , and help to generalize existing modeling techniques ( e . g . , [ BCS16 , BOZ ∗ 14b , DC17 ] ) for EVA contexts . Our results differ from past studies of broader EDA con - texts [ SU15 ] . However a variety of techniques beyond the scope of EVA are utilized , and teams , not individuals , perform the analysis . Insight ( S2 ) : Predictable patterns do occur during analysis sub - tasks associated with EVA , but at speciﬁc points within analysis sessions in Tableau . 7 . 3 . Study Design and Analysis Our study design enables evaluation of visual analysis be - havior along multiple axes : task speciﬁcity , performance , and structure , as well as others like interaction and en - coding types . We contribute our data as a community re - source for further study at https : / / github . com / leibatt / characterizing - eva - tableau . Signiﬁcant manual effort was required to analyze native Tableau logs . A standardized process for curating system logs would greatly simplify the evaluation process , enable a variety of evaluations ( e . g . , meta - analyses , performance benchmarks ) , and improve re - producibility / comparability of results [ BCHS17 , BAB ∗ 18 , E ∗ 16 ] . Though our study incorporates core ( but decomposed ) charac - teristics of EVA contexts and tasks , analysts still behave differ - ently when in their own work environments ( e . g . , software tools , datasets , etc . ) , making it challenging to capture authentic EVA be - havior in a controlled setting . It would be beneﬁcial to repeat this study to evaluate how assumptions and insights change under dif - ferent EVA contexts . Our analysis does not examine differences between the analy - sis patterns of novice and expert analysts , a potentially interest - ing topic for future study . Furthermore , our focus on a single tool makes it difﬁcult to distinguish between general and tool - speciﬁc analysis patterns . We hope to extend our study to other tools to bet - ter understand the inﬂuence of tool design on analysis behavior . 8 . Conclusion Exploratory visual analysis ( EVA ) is often considered a critical use case for visual analysis tools , however our understanding of EVA is arguably vague and incomplete . We sought to provide a more holistic view of how EVA is discussed across the literature , sum - marize and deﬁne EVA based on our observations , and to provide additional context for how analysts behave when performing EVA ( sub - ) tasks identiﬁed from the literature . We contribute a deﬁnition of EVA synthesized from a literature review of 42 research articles , as well as an empirical evaluation of several assumptions about how EVA is performed . We present the results of a user study with 27 Tableau users . Through a quantitative analysis of Tableau log data from the study , we evaluate multiple facets of task performance and analysis structure . We ﬁnd that participants achieve over 80 % ac - curacy across focused tasks with measurably correct answers . We ﬁnd that the pacing of participants’ analyses was surprisingly slow , compared to common performance assumptions observed in the lit - erature . We ﬁnd clear patterns and overlap across participants’ anal - ysis sessions , suggesting that some predictable behaviors do occur during tasks commonly associated with EVA . Furthermore , we ﬁnd few differences between how more focused and more open - ended analysis tasks are structured . These ﬁndings suggest that analysts can be steady , cautious explorers , and that EVA may often contain familiar patterns and structures , helping us to build a more com - prehensive view of visual analysis in the context of exploration . In the future , we aim to extend our analysis to better understand how differences in tool design ( i . e . , beyond Tableau ) and analysts’ ex - perience may affect analysis performance , patterns , and structure . Acknowledgements We thank our participants for being part of our study . We thank the reviewers , the UW Interactive Data Lab , and the UMD Human - Computer Interaction Lab for their helpful comments on this pa - per . We thank Justin Talbot , Melanie Tory , and Richard Wesley for providing helpful feedback in parsing and interpreting Tableau log data . This research was funded by the Moore Foundation Data - Driven Discovery Investigator program . c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau References [ AES05 ] A MAR R . , E AGAN J . , S TASKO J . : Low - level components of analytic activity in information visualization . In IEEE Symposium on Information Visualization , 2005 . INFOVIS 2005 . ( Oct . 2005 ) , pp . 111 – 117 . doi : 10 . 1109 / INFVIS . 2005 . 1532136 . 4 [ AZL ∗ 19 ] A LSPAUGH S . , Z OKAEI N . , L IU A . , J IN C . , H EARST M . A . : Futzing and moseying : Interviews with professional data analysts on ex - ploration practices . IEEE Transactions on Visualization and Computer Graphics 25 , 1 ( Jan 2019 ) , 22 – 31 . doi : 10 . 1109 / TVCG . 2018 . 2865040 . 1 , 3 , 4 , 11 [ BAB ∗ 18 ] B ATTLE L . , A NGELINI M . , B INNIG C . , C ATARCI T . , E ICH - MANN P . , F EKETE J . - D . , S ANTUCCI G . , S EDLMAIR M . , W ILLETT W . : Evaluating visual data analysis systems : A discussion report . In Pro - ceedings of the Workshop on Human - In - the - Loop Data Analytics ( New York , NY , USA , 2018 ) , HILDA’18 , ACM , pp . 4 : 1 – 4 : 6 . URL : http : / / doi . acm . org / 10 . 1145 / 3209900 . 3209901 , doi : 10 . 1145 / 3209900 . 3209901 . 12 [ BCC ∗ 05 ] B AVOIL L . , C ALLAHAN S . P . , C ROSSNO P . J . , F REIRE J . , S CHEIDEGGER C . E . , S ILVA C . T . , V O H . T . : VisTrails : enabling in - teractive multiple - view visualizations . In VIS 05 . IEEE Visualization , 2005 . ( Oct . 2005 ) , pp . 135 – 142 . doi : 10 . 1109 / VISUAL . 2005 . 1532788 . 2 [ BCHS17 ] B ATTLE L . , C HANG R . , H EER J . , S TONEBRAKER M . : Posi - tion statement : The case for a visualization performance benchmark . In 2017 IEEE Workshop on Data Systems for Interactive Analysis ( DSIA ) ( Oct 2017 ) , pp . 1 – 5 . doi : 10 . 1109 / DSIA . 2017 . 8339089 . 12 [ BCS16 ] B ATTLE L . , C HANG R . , S TONEBRAKER M . : Dynamic Prefetching of Data Tiles for Interactive Visualization . In Proceedings of the 2016 International Conference on Management of Data ( New York , NY , USA , 2016 ) , SIGMOD ’16 , ACM , pp . 1363 – 1375 . URL : http : / / doi . acm . org / 10 . 1145 / 2882903 . 2882919 , doi : 10 . 1145 / 2882903 . 2882919 . 2 , 3 , 4 , 5 , 9 , 11 , 12 [ BDM ∗ 18 ] B ATTLE L . , D UAN P . , M IRANDA Z . , M UKUSHEVA D . , C HANG R . , S TONEBRAKER M . : Beagle : Automated extraction and in - terpretation of visualizations from the web . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( New York , NY , USA , 2018 ) , CHI ’18 , ACM , pp . 594 : 1 – 594 : 8 . URL : http : / / doi . acm . org / 10 . 1145 / 3173574 . 3174168 , doi : 10 . 1145 / 3173574 . 3174168 . 6 [ BJK ∗ 16 ] B LASCHECK T . , J OHN M . , K URZHALS K . , K OCH S . , E RTL T . : VA2 : A Visual Analytics Approach for Evaluating Visual Analyt - ics Applications . IEEE Transactions on Visualization and Computer Graphics 22 , 1 ( Jan . 2016 ) , 61 – 70 . doi : 10 . 1109 / TVCG . 2015 . 2467871 . 2 [ BM13 ] B REHMER M . , M UNZNER T . : A Multi - Level Typology of Ab - stract Visualization Tasks . IEEE Transactions on Visualization and Computer Graphics 19 , 12 ( Dec . 2013 ) , 2376 – 2385 . doi : 10 . 1109 / TVCG . 2013 . 124 . 1 , 2 [ BOZ ∗ 14a ] B ROWN E . T . , O TTLEY A . , Z HAO H . , L IN Q . , S OUVENIR R . , E NDERT A . , C HANG R . : Finding Waldo : Learning about Users from their Interactions . IEEE Transactions on Visualization and Computer Graphics 20 , 12 ( Dec . 2014 ) , 1663 – 1672 . doi : 10 . 1109 / TVCG . 2014 . 2346575 . 2 [ BOZ ∗ 14b ] B ROWN E . T . , O TTLEY A . , Z HAO H . , L IN Q . , S OUVENIR R . , E NDERT A . , C HANG R . : Finding Waldo : Learning about Users from their Interactions . IEEE Transactions on Visualization and Computer Graphics 20 , 12 ( 2014 ) , 1663 – 1672 . doi : 10 . 1109 / TVCG . 2014 . 2346575 . 12 [ CFS ∗ 06 ] C ALLAHAN S . P . , F REIRE J . , S ANTOS E . , S CHEIDEGGER C . E . , S ILVA C . T . , V O H . T . : Managing the Evolution of Dataﬂows with VisTrails . In 22nd International Conference on Data Engineer - ing Workshops ( ICDEW’06 ) ( Apr . 2006 ) , pp . 71 – 71 . doi : 10 . 1109 / ICDEW . 2006 . 75 . 2 [ CGZ ∗ 16 ] C ROTTY A . , G ALAKATOS A . , Z GRAGGEN E . , B INNIG C . , K RASKA T . : The case for interactive data exploration ac - celerators ( ideas ) . In Proceedings of the Workshop on Human - In - the - Loop Data Analytics ( New York , NY , USA , 2016 ) , HILDA ’16 , ACM , pp . 11 : 1 – 11 : 6 . URL : http : / / doi . acm . org / 10 . 1145 / 2939502 . 2939513 , doi : 10 . 1145 / 2939502 . 2939513 . 2 , 4 , 5 , 9 [ CPVDW ∗ 01 ] C ARD S . K . , P IROLLI P . , V AN D ER W EGE M . , M ORRI - SON J . B . , R EEDER R . W . , S CHRAEDLEY P . K . , B OSHART J . : In - formation Scent As a Driver of Web Behavior Graphs : Results of a Protocol Analysis Method for Web Usability . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( New York , NY , USA , 2001 ) , CHI ’01 , ACM , pp . 498 – 505 . URL : http : / / doi . acm . org / 10 . 1145 / 365024 . 365331 , doi : 10 . 1145 / 365024 . 365331 . 2 [ CXGH08 ] C HAN S . - M . , X IAO L . , G ERTH J . , H ANRAHAN P . : Main - taining interactivity while exploring massive time series . In IEEE Sym - posium on Visual Analytics Science and Technology , 2008 . VAST ’08 ( Oct . 2008 ) , pp . 59 – 66 . doi : 10 . 1109 / VAST . 2008 . 4677357 . 2 , 5 [ DC17 ] D ABEK F . , C ABAN J . J . : A Grammar - based Approach for Mod - eling User Interactions and Generating Suggestions During the Data Ex - ploration Process . IEEE Transactions on Visualization and Computer Graphics 23 , 1 ( Jan . 2017 ) , 41 – 50 . doi : 10 . 1109 / TVCG . 2016 . 2598471 . 2 , 4 , 5 , 12 [ DHPP17 ] D EMIRALP Ç . , H AAS P . J . , P ARTHASARATHY S . , P EDAPATI T . : Foresight : Rapid data exploration through guideposts . arXiv preprint arXiv : 1709 . 10513 ( 2017 ) . 4 [ DPD14 ] D IMITRIADOU K . , P APAEMMANOUIL O . , D IAO Y . : Explore - by - example : An automatic query steering framework for interactive data exploration . In Proceedings of the 2014 ACM SIGMOD Inter - national Conference on Management of Data ( New York , NY , USA , 2014 ) , SIGMOD ’14 , ACM , pp . 517 – 528 . URL : http : / / doi . acm . org / 10 . 1145 / 2588555 . 2610523 , doi : 10 . 1145 / 2588555 . 2610523 . 4 , 5 [ DR01 ] D ERTHICK M . , R OTH S . F . : Enhancing data exploration with a branching history of user operations . Knowledge - Based Systems 14 , 1 ( Mar . 2001 ) , 65 – 74 . URL : http : / / www . sciencedirect . com / science / article / pii / S0950705100001015 , doi : 10 . 1016 / S0950 - 7051 ( 00 ) 00101 - 5 . 4 [ E ∗ 16 ] E ICHMANN P . , ET AL . : Towards a benchmark for interactive data exploration . IEEE Data Eng . Bull . 39 , 4 ( 2016 ) , 50 – 61 . 12 [ ED16 ] E L T AYEBY O . , D OU W . : A Survey on Interaction Log Anal - ysis for Evaluating Exploratory Visualizations . In Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Meth - ods for Visualization ( New York , NY , USA , 2016 ) , BELIV ’16 , ACM , pp . 62 – 69 . URL : http : / / doi . acm . org / 10 . 1145 / 2993901 . 2993912 , doi : 10 . 1145 / 2993901 . 2993912 . 1 , 2 , 3 , 5 [ FPDs12 ] F ISHER D . , P OPOV I . , D RUCKER S . , SCHRAEFEL M . : Trust Me , I’M Partially Right : Incremental Visualization Lets Analysts Ex - plore Large Datasets Faster . In Proceedings of the SIGCHI Confer - ence on Human Factors in Computing Systems ( New York , NY , USA , 2012 ) , CHI ’12 , ACM , pp . 1673 – 1682 . URL : http : / / doi . acm . org / 10 . 1145 / 2207676 . 2208294 , doi : 10 . 1145 / 2207676 . 2208294 . 1 , 3 , 5 , 9 , 11 [ FPH19 ] F ENG M . , P ECK E . , H ARRISON L . : Patterns and pace : Quanti - fying diverse exploration behavior with visualizations on the web . IEEE Transactions on Visualization and Computer Graphics 25 , 1 ( Jan 2019 ) , 501 – 511 . doi : 10 . 1109 / TVCG . 2018 . 2865117 . 5 [ GGZL16 ] G UO H . , G OMEZ S . R . , Z IEMKIEWICZ C . , L AIDLAW D . H . : A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights . IEEE Transactions on Visualization and Computer Graphics 22 , 1 ( Jan . 2016 ) , 51 – 60 . doi : 10 . 1109 / TVCG . 2015 . 2467613 . 2 , 3 , 4 , 5 , 11 [ GL12 ] G OMEZ S . , L AIDLAW D . : Modeling Task Performance for a Crowd of Users from Interaction Histories . In Proceedings of the c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau SIGCHI Conference on Human Factors in Computing Systems ( New York , NY , USA , 2012 ) , CHI ’12 , ACM , pp . 2465 – 2468 . URL : http : / / doi . acm . org / 10 . 1145 / 2207676 . 2208412 , doi : 10 . 1145 / 2207676 . 2208412 . 2 [ GTS10 ] G RAMMEL L . , T ORY M . , S TOREY M . A . : How Informa - tion Visualization Novices Construct Visualizations . IEEE Transactions on Visualization and Computer Graphics 16 , 6 ( Nov . 2010 ) , 943 – 952 . doi : 10 . 1109 / TVCG . 2010 . 164 . 2 , 4 , 6 [ GW09 ] G OTZ D . , W EN Z . : Behavior - driven Visualization Recom - mendation . In Proceedings of the 14th International Conference on Intelligent User Interfaces ( New York , NY , USA , 2009 ) , IUI ’09 , ACM , pp . 315 – 324 . URL : http : / / doi . acm . org / 10 . 1145 / 1502650 . 1502695 , doi : 10 . 1145 / 1502650 . 1502695 . 2 , 3 , 4 , 5 [ GZ08 ] G OTZ D . , Z HOU M . X . : Characterizing users’ visual analytic activity for insight provenance . In 2008 IEEE Symposium on Visual Analytics Science and Technology ( Oct . 2008 ) , pp . 123 – 130 . doi : 10 . 1109 / VAST . 2008 . 4677365 . 3 , 4 , 5 [ GZ09 ] G OTZ D . , Z HOU M . X . : Characterizing Users’ Visual Analytic Activity for Insight Provenance . Information Visualization 8 , 1 ( Jan . 2009 ) , 42 – 55 . URL : https : / / doi . org / 10 . 1057 / ivs . 2008 . 31 , doi : 10 . 1057 / ivs . 2008 . 31 . 2 , 9 , 11 [ HMSA08 ] H EER J . , M ACKINLAY J . , S TOLTE C . , A GRAWALA M . : Graphical Histories for Visualization : Supporting Analysis , Communi - cation , and Evaluation . IEEE Transactions on Visualization and Com - puter Graphics 14 , 6 ( Nov . 2008 ) , 1189 – 1196 . doi : 10 . 1109 / TVCG . 2008 . 137 . 1 , 2 , 3 , 4 , 6 , 8 [ HS12 ] H EER J . , S HNEIDERMAN B . : Interactive dynamics for visual analysis . Commun . ACM 55 , 4 ( Apr . 2012 ) , 45 – 54 . URL : http : / / doi . acm . org / 10 . 1145 / 2133806 . 2133821 , doi : 10 . 1145 / 2133806 . 2133821 . 1 , 2 , 3 , 4 , 5 , 9 [ IPC15 ] I DREOS S . , P APAEMMANOUIL O . , C HAUDHURI S . : Overview of data exploration techniques . In Proceedings of the 2015 ACM SIG - MOD International Conference on Management of Data ( New York , NY , USA , 2015 ) , SIGMOD ’15 , ACM , pp . 277 – 281 . URL : http : / / doi . acm . org / 10 . 1145 / 2723372 . 2731084 , doi : 10 . 1145 / 2723372 . 2731084 . 3 , 4 , 5 [ ITC08 ] I SENBERG P . , T ANG A . , C ARPENDALE S . : An Exploratory Study of Visual Information Analysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( New York , NY , USA , 2008 ) , CHI ’08 , ACM , pp . 1217 – 1226 . URL : http : / / doi . acm . org / 10 . 1145 / 1357054 . 1357245 , doi : 10 . 1145 / 1357054 . 1357245 . 2 [ jJKMG07 ] J . J ANKUN - K ELLY T . , M A K . , G ERTZ M . : A model and framework for visualization exploration . IEEE Transactions on Visual - ization and Computer Graphics 13 , 2 ( March 2007 ) , 357 – 369 . doi : 10 . 1109 / TVCG . 2007 . 28 . 2 , 3 , 4 , 11 [ KCZ14 ] K ALININ A . , C ETINTEMEL U . , Z DONIK S . : Interactive data exploration using semantic windows . In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data ( New York , NY , USA , 2014 ) , SIGMOD ’14 , ACM , pp . 505 – 516 . URL : http : / / doi . acm . org / 10 . 1145 / 2588555 . 2593666 , doi : 10 . 1145 / 2588555 . 2593666 . 3 , 4 [ Kei01 ] K EIM D . A . : Visual exploration of large data sets . Commun . ACM 44 , 8 ( Aug . 2001 ) , 38 – 44 . URL : http : / / doi . acm . org / 10 . 1145 / 381641 . 381656 , doi : 10 . 1145 / 381641 . 381656 . 3 , 4 [ KH18 ] K IM Y . , H EER J . : Assessing effects of task and data distribution on the effectiveness of visual encodings . In Computer Graphics Forum ( 2018 ) , vol . 37 , Wiley Online Library , pp . 157 – 167 . 6 [ KJTN14 ] K AMAT N . , J AYACHANDRAN P . , T UNGA K . , N ANDI A . : Dis - tributed and interactive cube exploration . In 2014 IEEE 30th Interna - tional Conference on Data Engineering ( ICDE ) ( Mar . 2014 ) , pp . 472 – 483 . doi : 10 . 1109 / ICDE . 2014 . 6816674 . 4 , 5 [ KPHH12 ] K ANDEL S . , P AEPCKE A . , H ELLERSTEIN J . M . , H EER J . : Enterprise data analysis and visualization : An interview study . IEEE Transactions on Visualization and Computer Graphics 18 ( 12 2012 ) , 2917 – 2926 . URL : doi . ieeecomputersociety . org / 10 . 1109 / TVCG . 2012 . 219 , doi : 10 . 1109 / TVCG . 2012 . 219 . 3 , 4 , 7 [ KS12 ] K ANG Y . , S TASKO J . : Examining the use of a visual analyt - ics system for sensemaking tasks : Case studies with domain experts . IEEE Transactions on Visualization and Computer Graphics 18 ( 12 2012 ) , 2869 – 2878 . URL : doi . ieeecomputersociety . org / 10 . 1109 / TVCG . 2012 . 224 , doi : 10 . 1109 / TVCG . 2012 . 224 . 3 , 4 [ Lam08 ] L AM H . : A framework of interaction costs in information visu - alization . IEEE Transactions on Visualization and Computer Graphics 14 , 6 ( Nov 2008 ) , 1149 – 1156 . doi : 10 . 1109 / TVCG . 2008 . 109 . 3 , 5 , 9 [ LH14 ] L IU Z . , H EER J . : The effects of interactive latency on ex - ploratory visual analysis . IEEE Transactions on Visualization and Com - puter Graphics 20 , 12 ( Dec 2014 ) , 2122 – 2131 . 2 , 3 , 4 , 5 , 6 , 11 [ LKS13 ] L INS L . , K LOSOWSKI J . T . , S CHEIDEGGER C . : Nanocubes for real - time exploration of spatiotemporal datasets . IEEE Transactions on Visualization and Computer Graphics 19 , 12 ( Dec 2013 ) , 2456 – 2465 . doi : 10 . 1109 / TVCG . 2013 . 179 . 4 [ LTM18 ] L AM H . , T ORY M . , M UNZNER T . : Bridging from Goals to Tasks with Design Study Analysis Reports . IEEE Transactions on Vi - sualization and Computer Graphics 24 , 1 ( Jan . 2018 ) , 435 – 445 . doi : 10 . 1109 / TVCG . 2017 . 2744319 . 1 , 2 , 3 , 4 , 11 [ LWD ∗ 17 ] L IU Z . , W ANG Y . , D ONTCHEVA M . , H OFFMAN M . , W ALKER S . , W ILSON A . : Patterns and Sequences : Interactive Ex - ploration of Clickstreams to Understand Common Visitor Paths . IEEE Transactions on Visualization and Computer Graphics 23 , 1 ( Jan . 2017 ) , 321 – 330 . doi : 10 . 1109 / TVCG . 2016 . 2598797 . 2 [ LWPL11 ] L U J . , W EN Z . , P AN S . , L AI J . : Analytic Trails : Supporting Provenance , Collaboration , and Reuse for Visual Data Analysis by Business Users . In Human - Computer Interaction â ˘A¸S INTERACT 2011 ( Sept . 2011 ) , Lecture Notes in Computer Science , Springer , Berlin , Heidelberg , pp . 256 – 273 . URL : https : / / link . springer . com / chapter / 10 . 1007 / 978 - 3 - 642 - 23768 - 3 _ 22 , doi : 10 . 1007 / 978 - 3 - 642 - 23768 - 3 _ 22 . 2 [ MWN ∗ 19 ] M ORITZ D . , W ANG C . , N ELSON G . L . , L IN H . , S MITH A . M . , H OWE B . , H EER J . : Formalizing visualization design knowledge as constraints : actionable and extensible models in draco . To appear , IEEE Transactions on Visualization and Computer Graphics ( 2019 ) . 3 , 4 [ New72 ] N EWELL A . : Human problem solving . Prentice - Hall Engle - wood Cliffs , NJ , 1972 . 2 [ PC05 ] P IROLLI P . , C ARD S . : The sensemaking process and leverage points for analyst technology as identiﬁed through cognitive task analy - sis . In Proceedings of international conference on intelligence analysis ( 2005 ) , vol . 5 , McLean , VA , USA , pp . 2 – 4 . 3 , 4 [ PS08 ] P ERER A . , S HNEIDERMAN B . : Systematic yet ﬂexible dis - covery : Guiding domain experts through exploratory data analy - sis . In Proceedings of the 13th International Conference on In - telligent User Interfaces ( New York , NY , USA , 2008 ) , IUI ’08 , ACM , pp . 109 – 118 . URL : http : / / doi . acm . org / 10 . 1145 / 1378773 . 1378788 , doi : 10 . 1145 / 1378773 . 1378788 . 3 , 4 , 6 [ PSCO09 ] P IKE W . A . , S TASKO J . , C HANG R . , O’ CONNELL T . A . : The science of interaction . Information Visualization 8 , 4 ( 2009 ) , 263 – 274 . 4 [ RAK ∗ 17 ] R AHMAN S . , A LIAKBARPOUR M . , K ONG H . K . , B LAIS E . , K ARAHALIOS K . , P ARAMESWARAN A . , R UBINFIELD R . : I’ve seen " enough " : Incrementally improving visualizations to support rapid deci - sion making . Proc . VLDB Endow . 10 , 11 ( Aug . 2017 ) , 1262 – 1273 . URL : https : / / doi . org / 10 . 14778 / 3137628 . 3137637 , doi : 10 . 14778 / 3137628 . 3137637 . 2 , 4 , 5 [ RESC16 ] R AGAN E . D . , E NDERT A . , S ANYAL J . , C HEN J . : Charac - c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd . L . Battle & J . Heer / Characterizing Exploratory Visual Analysis in Tableau terizing Provenance in Visualization and Data Analysis : An Organiza - tional Framework of Provenance Types and Purposes . IEEE Transac - tions on Visualization and Computer Graphics 22 , 1 ( Jan . 2016 ) , 31 – 40 . doi : 10 . 1109 / TVCG . 2015 . 2467551 . 1 [ RJPL16 ] R EDA K . , J OHNSON A . E . , P APKA M . E . , L EIGH J . : Modeling and evaluating user behavior in exploratory visual analy - sis . Information Visualization 15 , 4 ( Oct . 2016 ) , 325 – 339 . URL : https : / / doi . org / 10 . 1177 / 1473871616638546 , doi : 10 . 1177 / 1473871616638546 . 2 , 3 , 4 , 6 [ SASF11 ] S ILVA C . T . , A NDERSON E . , S ANTOS E . , F REIRE J . : Using VisTrails and Provenance for Teaching Scientiﬁc Visual - ization . Computer Graphics Forum 30 , 1 ( Mar . 2011 ) , 75 – 84 . URL : http : / / onlinelibrary . wiley . com / doi / 10 . 1111 / j . 1467 - 8659 . 2010 . 01830 . x / abstract , doi : 10 . 1111 / j . 1467 - 8659 . 2010 . 01830 . x . 2 [ Shn96 ] S HNEIDERMAN B . : The Eyes Have It : A Task by Data Type Taxonomy for Information Visualizations . In Proceedings of the 1996 IEEE Symposium on Visual Languages ( Washington , DC , USA , 1996 ) , VL ’96 , IEEE Computer Society , pp . 336 – . URL : http : / / dl . acm . org / citation . cfm ? id = 832277 . 834354 . 3 , 4 [ SKL ∗ 16 ] S IDDIQUI T . , K IM A . , L EE J . , K ARAHALIOS K . , P ARAMESWARAN A . : Effortless data exploration with zen - visage : An expressive and interactive visual analytics sys - tem . Proc . VLDB Endow . 10 , 4 ( Nov . 2016 ) , 457 – 468 . URL : https : / / doi . org / 10 . 14778 / 3025111 . 3025126 , doi : 10 . 14778 / 3025111 . 3025126 . 1 , 3 , 4 [ SND05 ] S ARAIYA P . , N ORTH C . , D UCA K . : An insight - based method - ology for evaluating bioinformatics visualizations . IEEE Transactions on Visualization and Computer Graphics 11 , 4 ( July 2005 ) , 443 – 456 . 3 [ SNLD06 ] S ARAIYA P . , N ORTH C . , L AM V . , D UCA K . A . : An insight - based longitudinal study of visual analytics . IEEE Transactions on Visu - alization and Computer Graphics 12 , 6 ( Nov 2006 ) , 1511 – 1522 . 3 [ ST15 ] S ARVGHAD A . , T ORY M . : Exploiting Analysis History to Sup - port Collaborative Data Analysis . In Proceedings of the 41st Graph - ics Interface Conference ( Toronto , Ont . , Canada , Canada , 2015 ) , GI ’15 , Canadian Information Processing Society , pp . 123 – 130 . URL : http : / / dl . acm . org / citation . cfm ? id = 2788890 . 2788913 . 2 [ STH02 ] S TOLTE C . , T ANG D . , H ANRAHAN P . : Polaris : a system for query , analysis , and visualization of multidimensional relational databases . IEEE Transactions on Visualization and Computer Graph - ics 8 , 1 ( Jan 2002 ) , 52 – 65 . doi : 10 . 1109 / 2945 . 981851 . 2 , 4 [ STM17 ] S ARVGHAD A . , T ORY M . , M AHYAR N . : Visualizing dimen - sion coverage to support exploratory analysis . IEEE transactions on vi - sualization and computer graphics 23 , 1 ( 2017 ) , 21 – 30 . 2 , 10 [ SU15 ] S ILBERZAHN R . , U HLMANN E . L . : Crowdsourced research : Many hands make tight work . Nature News 526 , 7572 ( 2015 ) , 189 . 12 [ SvW08 ] S HRINIVASAN Y . B . , VAN W IJK J . J . : Supporting the analyt - ical reasoning process in information visualization . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( New York , NY , USA , 2008 ) , CHI ’08 , ACM , pp . 1237 – 1246 . URL : http : / / doi . acm . org / 10 . 1145 / 1357054 . 1357247 , doi : 10 . 1145 / 1357054 . 1357247 . 2 [ Tuk77 ] T UKEY J . W . : Exploratory data analysis , vol . 2 . Reading , Mass . , 1977 . 3 , 4 , 5 [ VRM ∗ 15 ] V ARTAK M . , R AHMAN S . , M ADDEN S . , P ARAMESWARAN A . , P OLYZOTIS N . : SeeDB : Efﬁcient Data - driven Visualization Recom - mendations to Support Visual Analytics . Proc . VLDB Endow . 8 , 13 ( Sept . 2015 ) , 2182 – 2193 . URL : http : / / dx . doi . org / 10 . 14778 / 2831360 . 2831371 , doi : 10 . 14778 / 2831360 . 2831371 . 4 [ WHS ∗ 02 ] W ATERSON S . J . , H ONG J . I . , S OHN T . , L ANDAY J . A . , H EER J . , M ATTHEWS T . : What Did They Do ? Understanding Click - streams with the WebQuilt Visualization System . In Proceedings of the Working Conference on Advanced Visual Interfaces ( New York , NY , USA , 2002 ) , AVI ’02 , ACM , pp . 94 – 102 . URL : http : / / doi . acm . org / 10 . 1145 / 1556262 . 1556276 , doi : 10 . 1145 / 1556262 . 1556276 . 2 [ WMA ∗ 16a ] W ONGSUPHASAWAT K . , M ORITZ D . , A NAND A . , M ACKINLAY J . , H OWE B . , H EER J . : Towards a general - purpose query language for visualization recommendation . In Proceedings of the Work - shop on Human - In - the - Loop Data Analytics ( 2016 ) , ACM , p . 4 . 4 [ WMA ∗ 16b ] W ONGSUPHASAWAT K . , M ORITZ D . , A NAND A . , M ACKINLAY J . , H OWE B . , H EER J . : Voyager : Exploratory Analysis via Faceted Browsing of Visualization Recommendations . IEEE Trans - actions on Visualization and Computer Graphics 22 , 1 ( Jan . 2016 ) , 649 – 658 . doi : 10 . 1109 / TVCG . 2015 . 2467191 . 2 , 3 , 4 , 5 , 6 , 9 , 10 , 11 [ WQM ∗ 17 ] W ONGSUPHASAWAT K . , Q U Z . , M ORITZ D . , C HANG R . , O UK F . , A NAND A . , M ACKINLAY J . , H OWE B . , H EER J . : Voy - ager 2 : Augmenting Visual Analysis with Partial View Speciﬁca - tions . In Proceedings of the 2017 CHI Conference on Human Fac - tors in Computing Systems ( New York , NY , USA , 2017 ) , CHI ’17 , ACM , pp . 2648 – 2659 . URL : http : / / doi . acm . org / 10 . 1145 / 3025453 . 3025768 , doi : 10 . 1145 / 3025453 . 3025768 . 2 , 9 , 10 , 11 [ YKSJ07 ] Y I J . S . , K ANG Y . A . , S TASKO J . , J ACKO J . : Toward a Deeper Understanding of the Role of Interaction in Information Visualization . IEEE Transactions on Visualization and Computer Graphics 13 , 6 ( Nov . 2007 ) , 1224 – 1231 . URL : http : / / dx . doi . org / 10 . 1109 / TVCG . 2007 . 70515 , doi : 10 . 1109 / TVCG . 2007 . 70515 . 2 , 4 [ ZGC ∗ 16 ] Z GRAGGEN E . , G ALAKATOS A . , C ROTTY A . , F EKETE J . - D . , K RASKA T . : How progressive visualizations affect exploratory analysis . IEEE Transactions on Visualization and Computer Graphics ( 2016 ) . 2 , 3 , 5 [ ZOC ∗ 12 ] Z IEMKIEWICZ C . , O TTLEY A . , C ROUSER R . J . , C HAUNCEY K . , S U S . L . , C HANG R . : Understanding visualization by understanding individual users . IEEE Computer Graphics and Applications 32 , 6 ( Nov 2012 ) , 88 – 94 . doi : 10 . 1109 / MCG . 2012 . 120 . 2 , 3 , 4 , 12 [ ZZZK18 ] Z GRAGGEN E . , Z HAO Z . , Z ELEZNIK R . , K RASKA T . : Inves - tigating the effect of the multiple comparisons problem in visual anal - ysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Montreal , QC , Canada , 2018 ) , CHI ’18 , ACM . doi : 10 . 1145 / 3173574 . 3174053 . 2 , 3 , 4 , 5 , 7 , 8 , 11 c (cid:13) 2019TheAuthor ( s ) ComputerGraphicsForum c (cid:13) 2019TheEurographicsAssociationandJohnWiley & SonsLtd .