Similarity Search on Computational Notebooks Misato Horiuchi ∗ Yuya Sasaki ∗ Chuan Xiao ∗ Makoto Onizuka ∗ Abstract Computational notebook software such as Jupyter Notebook is popular for data science tasks . Numer - ous computational notebooks are available on the Web and reusable ; however , searching for compu - tational notebooks manually is a tedious task , and so far , there are no tools to search for computa - tional notebooks eﬀectively and eﬃciently . In this paper , we propose a similarity search on computa - tional notebooks and develop a new framework for the similarity search . Given contents ( i . e . , source codes , tabular data , libraries , and outputs formats ) in computational notebooks as a query , the similar - ity search problem aims to ﬁnd top - k computational notebooks with the most similar contents . We deﬁne two similarity measures ; set - based and graph - based similarities . Set - based similarity handles each con - tent independently , while graph - based similarity cap - tures the relationships between contents . Our frame - work can eﬀectively prune the candidates of compu - tational notebooks that should not be in the top - k results . Furthermore , we develop optimization tech - niques such as caching and indexing to accelerate the search . Experiments using Kaggle notebooks show that our method , in particular graph - based similar - ity , can achieve high accuracy and high eﬃciency . Keywords : similarity search , computational note - book , subgraph matching . ∗ Graduate school of Information Science and Technol - ogy , Osaka University . { horiuchi . misato , sasaki , chuanx , onizuka } @ ist . osaka - u . ac . jp 1 INTRODUCTION Many data scientists currently use computational notebook software such as Jupyter Notebook and R Notebook for data analytics . They interactively conduct various data processing , for example , data cleaning , data mining , machine learning , and visu - alization . Due to the increasing popularity of com - putational notebooks , numerous computational note - books are available and reusable on the Web , such as GitHub and Kaggle [ 6 ] . We often search for computational notebooks to reuse them for our own data analytic tasks and to learn programming skills . When searching the com - putational notebook database , we would like to ﬁnd computational notebooks to include source codes , tabular data , libraries , and / or output formats sim - ilar to what we specify , because it is diﬃcult to as - sign the search conditions ( e . g . , it may be equiva - lent to write codes ) and there are no computational notebooks that perfectly match our conditions . For example , we look for computational notebooks with analysis of data that is similar to ours , libraries , and functions that we would like to learn . There is a great demand for a similarity search on computational notebooks . Although some works par - tially support computational notebook search , there are no existing problem deﬁnitions and solutions to eﬀectively and eﬃciently search for similar computa - tional notebooks so far . Existing methods for similar source code search [ 4 , 5 ] and tabular data search [ 8 ] can be used for computational notebook search since their search targets are either source codes or tab - ular data , which are computational notebooks con - tents . However , using them directly cannot meet our demand for computational notebook search . They 1 a r X i v : 2201 . 12786v1 [ c s . I R ] 30 J a n 2022 only compute the similarity for their targets instead of computational notebooks as they are optimized for their search targets ( i . e . , source codes or tabu - lar data ) . What they search for is only a subset of the contents in computational notebooks . Moreover , although combining their results to compute the sim - ilarity for computational notebooks is possible , it is ineﬃcient because we need to conduct two searches individually . Therefore , in this paper , we ﬁrst propose a similar - ity search problem on computational notebooks and develop a new framework for the similarity search . Our similarity search problem aims to ﬁnd the top - k computational notebooks with the most similar con - tents ( i . e . , source codes , tabular data , libraries , and output formats ) to the contents speciﬁed by a given query . To measure a similarity between the given query and computational notebooks , we deﬁne two similarity measures ; set - based and graph - based sim - ilarity . Set - based similarity is a natural deﬁnition that computes the similarity of each content indepen - dently , while graph - based similarity aims to capture the relationships between contents . In our graph - based similarity , that computational notebooks and queries are represented by directed acyclic graphs ( DAGs ) and computes the similarity based on sub - graph matching . The graph - based similarity search can ﬁnely specify user requirements . We develop a new framework that can eﬃciently ﬁnd the top - k results based on the similarity deﬁ - nitions . Our framework mainly targets graph - based similarity computation while we can use it for set - based similarity computation . Our framework ﬁrst converts computational notebooks into DAGs so that keep the execution ﬂow and relationships between contents ( e . g . , codes read tabular data ) . In the search process , our framework employs subgraph match - ing to meet user requirements , and to eﬀectively prune computational notebooks that should not be in the top - k results . The subgraph matching of DAGs requires less computational costs than com - puting the similarity scores , so it accelerates the sim - ilarity search . To further accelerate the similarity search , our framework uses four optimization techniques ; prun - ing , computation order optimizations , caching , and indexing . These techniques drastically reduce the computation costs of both set - based and graph - based similarity computations . In our experimental study , we evaluate that accu - racy and eﬃciency of our method by using 111 com - putational notebooks shared on Kaggle . We show that our method achieves high nDCG [ 1 ] through user evaluation that users manually measure the sim - ilarity of all the computational notebooks . We show that our method is eﬃcient and scalable by varying several settings such as k and data size . In particu - lar , graph - based similarity is up to 157 . 7 times faster than a baseline with a higher accuracy . We summarize our contributions as follows : • We ﬁrst propose the similarity search problems on computational notebooks . We deﬁne two sim - ilarity measures ; set - based and graph - based sim - ilarity . The graph - based similarity search can eﬀectively capture the user requirements ( Sec - tion 3 ) . • We develop a new framework that eﬃciently searches for similar computational notebooks ( Section 4 ) . • We accelerate the similarity search by using four optimization techniques ; pruning , computation ordering , caching , and indexing ( Section 5 ) . • Through experimental evaluation using Kaggle notebooks , we show that our method is accurate , eﬃcient , and scalable ( Section 6 ) . The rest of this paper is organized as follows . Sec - tion 2 describes related work . Section 3 explains our problem deﬁnition . Section 4 presents the proposed search framework . Section 5 presents optimization techniques of our search method . The experimental results are shown in Section 6 , and the paper is con - cluded in Section 7 . 2 RELATED WORK Computational notebooks are frequently used and published , yet there are some issues , for example , low reusability and diﬃculty to archive [ 2 ] . The similarity search on computational notebooks par - tially supports these issues . First , low reusability 2 derives from insuﬃcient description and meaningless ﬁle names . This indicates that the keyword search is impractical due to their ﬁle name and descrip - tions . Second , archiving is hard because traditional source control systems are diﬃcult to identify the ac - tual changes between the notebooks computational notebooks . This is because computational notebooks are written by markup languages , and the systems detect all of these markup tag changes . These is - sues can be partially solved by the similarity search on computational notebooks . It helps to reuse them by retrieving computational notebooks with contents that users want instead of ﬁle names , and archive by identifying the old version notebooks that should have similar contents to new versions . The similarity search is eﬀective for more widely spreading usages of computational notebooks . Many data scientists use tabular data in compu - tational notebooks such as Pandas DataFrames , and researchers actively develop technologies focusing on tabular data in computational notebooks . Zhang et al . [ 7 , 8 ] proposed Juneau , a fast retrieval technique for tabular data in computational notebooks . Be - sides , Yan et al . [ 6 ] developed a system that auto - matically suggests the best operations to the tabular data by learning historical operations by data scien - tists , such as processing and transformation . Chen et al . [ 3 ] proposed a method that reduces the time gap between the execution of Jupyter Notebooks’ cells and the outputs of results . This method optimizes the execution of Pandas DataFrame to improve the interactivity of the Jupyter Notebook . Among them , Juneau can be used for similarity search on computa - tional notebooks and the others are not for searching . However , Juneau is not adapted to search for other contents of computational notebooks , such as source code and outputs since it can evaluate only the sim - ilarity of tabular data . Therefore , the technique for searching tabular data is not appropriate for compu - tational notebooks themselves . We show that Juneau does not match user requirements in our experimen - tal studies ( see Figure 3 in Section 6 ) . Code similarity has been actively studied in the ﬁeld of software engineering . A similar source code detection technique [ 5 ] and a code clone detection technique [ 4 ] convert source code into directed graphs and searching for similar subgraphs . These tech - niques are not suitable for similarity search of compu - tational notebooks because they specialize in source code and cannot target complex similarity computa - tion ( e . g . , data similarity ) . In addition , they cannot handle the order of execution of cells and the rela - tionship between contents . 3 Similarity Search on Compu - tational Notebooks There are no studies of similarity search on compu - tational notebooks . After explaining computational notebooks , we propose similarity search problems on computational notebooks . We then deﬁne two simi - larity measures ; set - based and graph - based similarity . The diﬀerence between them is that the graph - based similarity aims to capture the relationships between contents instead that the set - based similarity handles each content independently . 3 . 1 Computational Notebook A computational notebook is an editable document that consists of cells with source codes . The source codes in each cell are executed to import libraries , read tabular data to DataFrame , process / analyze the data , and output analytic results . The cells are exe - cuted one by one and never simultaneously . We deﬁne computational notebooks as follows : Deﬁnition 1 ( Computational notebook . ) A computational notebook N is a 5 - pair ( S N , D N , O N , L N , C N ) , where S N is the source codes , S N (cid:54) = φ , D N is the set of tabular data , O N is the multi - set of output formats , and L N is the set of imported library names . A cell c N is a block of a subset of S N . We denote the set of c N as C N . We denote the set of N as N . Figure 1 illustrates a computational notebook on Jupyter Notebook . This computational notebook has three cells ; the ﬁrst cell imports libraries and reads a CSV ﬁle , the second one conducts preprocessing for the tabular data , and the third one analyzes the data and outputs the analytic results . 3 2020 / 12 / 24 eda - on - indian - cuisine - Jupyter Notebook localhost : 8888 / notebooks / eda - on - indian - cuisine . ipynb 1 / 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Figure 1 : An example of computational note - books Input Query graph Matched parts Database Output Notebooks with Top - ! " # $ Calc contents % # $ 0 . 5 0 . 7 0 . 8 0 . 4 Calc the notebook ' s " # $ Workflow graphs Search 1 . 0 2020 / 12 / 24 eda - on - indian - cuisine - Jupyter Notebook localhost : 8888 / notebooks / eda - on - indian - cuisine . ipynb 1 / 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Matched Subgraph matching 2020 / 12 / 24 eda - on - indian - cuisine - Jupyter Notebook localhost : 8888 / notebooks / eda - on - indian - cuisine . ipynb 1 / 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Figure 2 : A framework of similarity search 3 . 2 Problem Deﬁnition We deﬁne similarity of computational notebooks as the weighted sum of similarity of contents ( e . g . , source codes and tabular data ) . We ﬁrst deﬁne con - tent similarity as follows . Deﬁnition 2 ( Content Similarity . ) We deﬁne similarity measures of source codes , tabular data , output formats , and libraries as sim S ( S , S (cid:48) ) , sim D ( D , D (cid:48) ) , sim O ( O , O (cid:48) ) , sim L ( L , L (cid:48) ) , respec - tively . The range of the similarity measure is [ 0 , 1 ] , and similar contents have the content similarity closer to one . We can use any similarity measures such as Jaccard similarity coeﬃcient . We deﬁne our problem that we solve in this paper as follows : Problem Similarity Search on Computa - tional Notebooks . Given a query Q , a set of com - putational notebooks N , a natural number k , and non - negative weights that balance the importance of con - tents , we ﬁnd a ranked list A of k computational notebooks such that for A ∈ A and N ∈ N \ A , Sim ( Q , A ) ≥ Sim ( Q , N ) . The ranked list depends on how to compute Sim ( Q , N ) . We formulate two types of similarity : set - based and graph - based similarity . Set - based sim - ilarity handles each content independently and com - putes the similarity between queries and computa - tional notebooks by summing similarities of their en - tire contents . On the other hand , graph - based simi - larity captures the relationships between the contents and then computes the similarity of contents per cells according to subgraph matching . 3 . 2 . 1 Set - based Similarity The set - based similarity speciﬁes contents as queries regardless of cells . We deﬁne a query in set - based similarity as follows : Deﬁnition 3 ( Query . ) A query Q is a 4 - pair ( S Q , D Q , O Q , L Q ) , where each of them is source codes , a set of tabular data , output formats , and li - braries , respectively . We deﬁne the set - based computational notebook similarity as follows : Deﬁnition 4 Set - based Computational Note - book similarity . Given a query Q , a computa - 4 tional notebook N , we deﬁne a similarity measure between Q and N as Sim ( Q , N ) : Sim ( Q , N ) = α S sim S ( S Q , S N ) + α D sim D ( D Q , D N ) ( 1 ) + α O sim O ( O Q , O N ) + α L sim L ( L Q , L N ) where α S , α D , α O , α L are weights corresponding to importance of source codes , tabular data , output for - mats , and libraries , respectively . This similarity search problem can be solved by computing the sum of content similarity . 3 . 2 . 2 Graph - based Similarity The set - based similarity does not capture the ex - ecution orders of cells and relationships between contents . We model computational notebooks and queries as directed acyclic graphs ( DAGs ) to capture them . We deﬁne workﬂow graphs for computational notebooks and a query graph , respectively . Deﬁnition 5 ( Workﬂow graph . ) The execution ﬂow and contents relationships of a computa - tional notebook N is represented by a DAG W N = ( V N , E N , (cid:96) N , τ N , L N ) , and we call W N workﬂow graph . V N is a set of nodes , E N ⊆ V N × V N is a set of directed edges e ( v , v (cid:48) ) going from v ∈ V N to v (cid:48) ∈ V N , (cid:96) N is a mapping from each node to a label , τ N is a mapping from each node to a set of attributes . For v ∈ V N , (cid:96) N ( v ) and τ N ( v ) are the label and the attribute of v , respectively . As node labels in the workﬂow graph , the node corresponding to the source codes of a cell is labeled (cid:96) S , the node corresponding to tabular data is labeled (cid:96) D , and the node corresponding to the format of an output of a cell is labeled (cid:96) O . Therefore , (cid:96) N : V N → { (cid:96) S , (cid:96) D , (cid:96) O } . The node v has the attribute τ N ( v ) corresponding to the label (cid:96) N ( v ) . If (cid:96) N ( v ) is (cid:96) S , (cid:96) D , and (cid:96) O , then τ N ( v ) is a set of source codes , tabular data , and output formats , respectively . Note that one computational notebook corresponds to one workﬂow graph . We deﬁne W as the set of workﬂow graphs . A query is also represented by a DAG that consists of a set of the source codes , tabular data , libraries , and output formats . We deﬁne a query graph as fol - lows : Deﬁnition 6 ( Query Graph . ) A query graph Q is a 5 - pair ( V Q , E Q , (cid:96) Q , τ Q , L Q ) as well as a work - ﬂow graph . The node labels of the query graph are { (cid:96) S , (cid:96) D , (cid:96) O , ∗ } . Label ∗ denotes the reachability label , indicating that there is a path between its adjacent nodes . We note that nodes with label ∗ are not adja - cent . The nodes in workﬂow and query graphs repre - sent source codes in cells , tabular data , and out - puts . Thus , we can specify our requirements with ﬁner granularity than the set - based similarity . The diﬀerence between workﬂow and query graphs is that query graphs can have reachability as labels , which enables the ﬂexibility to specify the relationship be - tween contents . To compare the similarity between workﬂow and query graphs , we consider correspondences between nodes in workﬂow and query graphs . For this pur - pose , we deﬁne subgraph matching from query graphs to workﬂow graphs as follows : Deﬁnition 7 ( Subgraph Matching . ) Given a query graph Q = ( V Q , E Q , (cid:96) Q , τ Q , L Q ) and a work - ﬂow graph W N = ( V N , E N , (cid:96) N , τ N , L N ) , we deﬁne a mapping M from Q to W N as follows : 1 . If (cid:96) Q ( v ) (cid:54) = ∗ , then v corresponds to a node u ∈ V N . 2 . For ∀ v ∈ V Q , if (cid:96) Q ( v ) (cid:54) = ∗ , then (cid:96) Q ( v ) = (cid:96) N ( M ( v ) ) . 3 . If an edge e ( v i , v j ) ∈ E Q exists , (cid:96) Q ( v i ) (cid:54) = ∗ , and (cid:96) Q ( v j ) (cid:54) = ∗ , then ∃ e ; e ( M ( v i ) , M ( v j ) ) ∈ E N . 4 . If edges e ( v i , v m ) and e ( v m , v j ) exists , (cid:96) Q ( v i ) (cid:54) = ∗ , (cid:96) Q ( v j ) (cid:54) = ∗ , and (cid:96) Q ( v m ) = ∗ , then there is a path from M ( v i ) to M ( v j ) . There are multiple mapping from Q to W N . We de - note a set of mapping as M . Next , we deﬁne the similarity measure between Q and W N as follows : Deﬁnition 8 Graph - based Similarity . Given a query graph Q , a workﬂow graph W N , and a set of 5 mappings M , we deﬁne similarity between Q and W N for M ∈ M as Sim ( Q , W N , M ) : Sim ( Q , W N , M ) = β L sim L ( L Q , L N ) ( 2 ) + (cid:88) v ∈ V Q β v sim v ( τ Q ( v ) , τ N ( M ( v ) ) ) where β v are β S , β D , β O and sim v are sim S , sim D , sim O , for (cid:96) Q ( v ) = (cid:96) S , (cid:96) D , (cid:96) O , respectively . For x ∈ { S , D , O } , we denote β x = α x size x ( V Q ) if size x ( V Q ) (cid:54) = 0 and β x = 0 if size x ( V Q ) = 0 , where size x ( V ) is the number of nodes with label (cid:96) x in a set of nodes V , and β L = α L . We deﬁne a similarity between N and Q as Sim ( Q , N ) . If there are multiple mappings , the highest Sim ( Q , W N , M ) is Sim ( Q , N ) as follows : Sim ( Q , N ) = (cid:40) max M ∈ M Sim ( Q , W N , M ) M (cid:54) = φ 0 M = φ . ( 3 ) We note that β S , β D , β L , and β O are for the nor - malization of the weights since the maximum value of sum of each content similarity increases if the number of nodes increases . 4 Our Framework In this section , we describe the proposal similarity search framework . We mainly focus on graph - based similarity while our framework can be used for set - based similarity . Figure 2 illustrates our framework architecture . We input a query and weights of each content simi - larity . The workﬂow graphs of computational note - books are stored in the database . Our framework computes similarity between the query and workﬂow graphs , and outputs the top - k computational note - books with the highest similarity . The search part of the framework consists of two major components : subgraph matching and similarity computation . We ﬁrst describe how to construct the workﬂow graphs , and then we explain a search method to com - pute the top - k computational notebooks . 4 . 1 Workﬂow Graph Construction Our graph - based similarity computes the similarity between a given query and computational notebooks to compare the execution orders of cells and the re - lationships between contents . For this purpose , we transform computational notebooks into graphs with maintaining the workﬂow of cells . We describe how to transform a computational notebook N into a work - ﬂow graph W N in the following . A node set V N represent cells in C N , tabular data in D N , and outputs in O N . Each of their labels is (cid:96) S , (cid:96) D , and (cid:96) O , respectively . Node attributes τ N ( v ) are any of S N , D N , and O N corresponding to the node labels . Constructing an edge set E N is as follows . E N includes edges between nodes with (cid:96) S and between nodes with (cid:96) S and (cid:96) D or (cid:96) O . If v i , v j ∈ V N corre - sponding to the source codes S N and source codes of v i are immediately executed after that of v j , there is an edge v i to v j . A node v d ∈ V N with label (cid:96) D has an edge e ( v i , v d ) if its tabular data is stored into a variable in the source codes of the cell corresponding to v i . Furthermore , if the source codes of the cell that uses the variable corresponding to v d exists , there is an edge e ( v d , v j ) . For the node v o ∈ V N with label (cid:96) O , we add edge e ( v i , v o ) if the source codes corre - sponding to v i outputs . In these processes , we can maintain the execu - tion ﬂow and contents relationships of computational notebooks without loss of any information . 4 . 2 Search Method Our search method computes similarities between queries and computational notebooks one by one . The search method is as follows : 1 . It enumerates all matches , which are the results of subgraph matching between Q and all work - ﬂow graphs W . 2 . It calculates the content similarity sim L ( L Q , L N ) and sim v ( τ Q ( v ) , τ N ( M ( v ) ) ) , ∀ v ∈ V Q for M ∈ M . Then , after calculating Sim ( Q , W N , M ) , it calculates Sim ( Q , N ) . 3 . It repeats the above processes until obtaining the similarities of all computational notebooks . 6 Then , it returns the k computational notebooks with the highest Sim ( Q , N ) . This search method is simple and has many op - portunity to be accelerated . In the next section , we develop optimization techniques for accelerating this search method . 5 Search Optimization Our search method has many opportunity to improve the eﬃciency . First , it calculates all the content sim - ilarity , even when computational notebooks are ob - viously not in the top - k results . Second , it computes the content similarity of all the computational note - books regardless of their computational cost . Third , it computes the similarity of the same contents mul - tiple times . Finally , it performs subgraph matching even for workﬂow graphs that do not match obvi - ously . In this section , we present four optimization tech - niques to support the above opportunities . Our op - timization techniques are ( 1 ) pruning candidates of computational notebooks , ( 2 ) optimization calcula - tion orders of content similarity , ( 3 ) caching content similarity , and ( 4 ) topology - based indexing . 5 . 1 Pruning computational notebooks and content similarity computa - tion We prune computational notebooks and content simi - larity computations by comparing the similarity that is computed already . The general idea is that we prune computational notebooks if their possible max - imum similarity are not larger than the current k - th similarity . Suppose that some but not all of the content similarity sim L ( L Q , L N ) and sim v ( τ Q ( v ) , τ N ( M ( v ) ) ) have already been com - puted . Let MaxSim ( Q , W N , M ) be the possible maximum value as follows : MaxSim ( Q , W N , M ) = β L sim L ( L Q , L N ) ( 4 ) + (cid:88) v ∈ V (cid:48) Q β v sim v ( τ Q ( v ) , τ N ( M ( v ) ) ) + (cid:88) u ∈ V Q \ V (cid:48) Q β u . where V (cid:48) Q ⊂ V Q is the set of nodes that their con - tent similarity are already computed . We note that maximum content similarity is one . Given the k - th similarity Sim k , we can prune the computational notebooks if it holds the following : Sim k > MaxSim ( Q , W N , M ) . ( 5 ) Furthermore , we prune the content similarity com - putation by comparing them with the computed sim - ilarity of the same workﬂow graph . Let the mapping set which is computed the similarity be M (cid:48) ⊆ M and max M (cid:48) ∈ M (cid:48) Sim ( Q , W N , M (cid:48) ) > MaxSim ( Q , W N , M ) ( 6 ) holds , then Equation ( 3 ) clearly shows that this sim - ilarity Sim ( Q , W N , M ) cannot be the Sim ( Q , N ) . These pruning allow us to reduce similarity compu - tations that are clearly not in the top - k or the largest similarity value in the workﬂow graph without com - puting the similarity for all the constituent nodes . 5 . 2 Ordering contents similarity com - putation We can improve the eﬀectivity of pruning by opti - mizing the order of content similarity computation . Given the computation costs of content similarity , we prioritize the contents similarity computation with small costs to reduce the content similarity computa - tion with the largest cost as much as possible . In this optimization , we ﬁrst all content similarity except for the one with the largest cost ( we use (cid:96) θ as the label with largest cost ) and then obtain ten - tative similarities of computational notebooks . We sort a pair of ( W , M ) in descending order of the ten - tative similarity . After these processes , we compute the exact similarity from the highest tentative simi - larity with pruning the computational notebooks and content similarity computations in the way of Sec - tion 5 . 1 . 7 We note that the computation cost of subgraph matching is smaller enough than that of content similarity computation from our preliminary exper - iments . 5 . 3 Caching computed content simi - larity We can reduce the wastefulness of computing the similarity between the same nodes multiple times by caching computed similarity . We store the con - tents similarity once they are calculated . We reuse the stored contents similarity to reduce computation costs . 5 . 4 Topology - based indexing We prune the computational notebooks by using the topological information of the workﬂow graph . The maximum incoming degree of an edge in W is d in ( W ) , and the maximum outgoing degree is d out ( W ) . Be - fore subgraph matching of Q to W N , we know that there are no matches if either of the following holds . • ∃ x ∈ { S , D , O } , size x ( V N ) < size x ( V Q ) • d in ( W N ) < d in ( Q ) • d out ( W N ) < d out ( Q ) We can prune workﬂow graphs that do not obviously match to queries . 5 . 5 Algorithm Algorithm 1 shows the pseudo - code of the graph - based similarity search with optimization techniques . In this algorithm , M [ W N ] represents the set of map - pings between the workﬂow graph W N and the query Q . A node label (cid:96) θ ∈ { (cid:96) S , (cid:96) D , (cid:96) O } is the label cor - responding to the heaviest computation cost similar - ity . All content similarity are computed with caching . First , this algorithm computes subgraph matching using the indexes and similarity except (cid:96) θ similar - ity ( lines 1 – 11 ) . Then , it computes the (cid:96) θ similarity and the similairty of computational notebooks while pruning computational notebooks and content simi - larity ( lines 12 – 23 ) . Finally , it returns the top - k com - putational notebooks ( line 24 ) . Algorithm 1 : Graph - based similarity search with optimization techniques input : Query graph Q , set of workﬂow graphs W , natural number k , non - negative numbers β S , β D , β O , β L , index , a node label (cid:96) θ output : k computational notebooks with the k highest Sim ( Q , N ) 1 for W N ∈ W do 2 if Q has no matches in W N based on the index then 3 M [ W N ] ← φ 4 else 5 M [ W N ] ← Subgraph Matching ( Q , W N ) 6 Sim ( Q , N ) ← 0 7 for M ∈ M [ W N ] do 8 Sim ( Q , W N , M ) ← β L sim L ( L Q , L N ) 9 for v ∈ V Q do 10 if (cid:96) Q ( v ) (cid:54) = (cid:96) θ then 11 Sim ( Q , W N , M ) ← Sim ( Q , W N , M ) + β v sim v ( τ Q ( v ) , τ N ( M ( v ) ) ) 12 Sort ( W , M ) by Sim ( Q , W N , M ) 13 for ( W N , M ) ∈ ( W , M ) do 14 for v ∈ V Q do 15 if (cid:96) Q ( v ) = (cid:96) θ then 16 Sim ( Q , W N , M ) ← Sim ( Q , W N , M ) + β θ sim θ ( τ Q ( v ) , τ N ( M ( v ) ) ) 17 Update MaxSim ( Q , W N , M ) 18 if MaxSim ( Q , W N , M ) < Sim k or MaxSim ( Q , W N , M ) < Sim ( Q , N ) then 19 break 20 if Sim ( Q , N ) < Sim ( Q , W N , M ) then 21 Sim ( Q , N ) ← Sim ( Q , W N , M ) 22 A ← sort N by Sim ( Q , N ) 23 Sim k ← Sim ( Q , A [ k − 1 ] ) 24 return A [ 0 . . . k − 1 ] 8 6 EXPERIMENTAL EVALU - ATION In this section , we show the experimental evalua - tion to show accuracy and eﬃciency of our meth - ods . We assume that computational notebook sim - ilarity search is generally run on client computers . Thus , in our experiment , we use a Mac notebook with 2 . 30GHz Intel Core i5 and 16GB memory . We implement and execute all algorithms by Python . We use PostgreSQL , Neo4J and SQLite to store tabular data , workﬂow graphs , and saved query graphs , re - spectively . 6 . 1 Experimental Setting We describe the dataset , comparison methods , and queries that we used in our experimental study . Dataset . We use 111 computational notebooks shared on Kaggle as the dataset . They are used for various purposes , including machine learning , data analysis , and data visualization . We convert them into workﬂow graphs as execu - tion order of cells is from top to bottom , since most of the computational notebooks on Kaggle are usu - ally supposed to run cells in that order . In addi - tion , we obtain the contents of the tabular data by executing all the cells in the order and saving into the database . The time for converting computational notebook into workﬂow graph was as long as the running time of computational notebooks for tabular data collection because the graph construction time is shorter enough to be ignored than the data col - lection time . Tables 1 – 3 show statistics of workﬂow graphs , source codes , and tabular data , respectively . Measures of content similarity . We describe the measures of content similarity in our experiments . ( 1 ) Source code . We separate the strings of source codes in the cells into a set of words . The delimiters are space , new line , period , and equal . We deﬁne the similarity between two sets of source codes as the Jaccard similarity coeﬃcient for the sets of words . ( 2 ) Tabular data . Given two tabular data D A and D B , we separate them into columns , we deﬁne each of col A and col B as a set of unique values per Table 1 : Statistics of workﬂow graphs Elements Max Min Average Total # of nodes with (cid:96) S 123 5 29 . 92 3321 # of nodes with (cid:96) D 44 0 8 . 86 983 # of nodes with (cid:96) O 107 0 26 . 59 2952 # of edges 282 12 73 . 10 8114 Max in - degrees 8 1 2 . 71 - Max out - degrees 21 2 4 . 97 - # of libraries 22 2 6 . 86 762 # of DataFrame 43 1 4 . 28 475 # of png 46 1 8 . 79 976 # of text 47 1 13 . 52 1501 Table 2 : Statistics for lines of source codes Max Min Per cell Per notebook Total 9043 1 232 . 52 48261 . 63 772186 Table 3 : Statistics for tabular data Max [ MB ] Min [ Byte ] Average [ MB ] Total [ MB ] 248 . 7 144 85 . 9 9538 . 9 columns . We deﬁne the number of columns in D A is | D A | , and it is fewer than the number of rows in D B . We deﬁne Jacc ( col A , col B ) as the similar - ity between col A and col B which is calculated as Jaccard similarity coeﬃcient . Then we select | D A | pairs in descending order to Jacc ( col A , col B ) . Injec - tion T col : col A → col B explains correspondence of the pairs , we calculate the similarity of the table by 1 s (cid:80) col A ∈ D A Jacc ( col A , T col ( col A ) ) . ( 3 ) Library . We use the Jaccard similarity coeﬃ - cient to sets of words of library names used in the computational notebooks . ( 4 ) Output . We use similarity of outputs as whether two outputs match in the type { DataFrame , text , png } . The similarity is one if they match , and zero if they do not match . Comparison methods . We use six methods for evaluation : Graph - based , Graph - based w / o opt , Set - based , Set - based w / o opt , C , and Juneau . Graph - based and Graph - based w / o opt are the graph - based sim - ilarity methods with and without all optimization techniques , respectively . Set - based is the set - based similarity method with optimization techniques men - tioned mentioned in Sections 5 . 1 and 5 . 2 . Other op - 9 0 0 . 2 0 . 4 0 . 6 0 . 8 1 nDCG @ 5 nDCG @ 10 nDCG @ 15 n DC G Graph − based Set − based C Juneau Figure 3 : Search accuracy 0 500 1000 1500 2000 2500 3000 Q 1 Q 2 Q 3 Q 4 Q 5 Q 6 S ea r c h t i m e [ s e c ] Graph − based Graph − based w / o opt Set − based Set − based w / o opt Figure 4 : Top - 10 search time 0 500 1000 1500 2000 2500 3000 10 20 30 40 50 S ea r c h t i m e [ s e c ] k Graph − based Graph − based w / o opt Set − based Set − based w / o opt Figure 5 : Search time on rank k timization techniques cannot apply to the set - based similarity . Set - based w / o opt does not use optimiza - tion techniques and can be considered as a baseline that independently uses similarity measures for each content . C and Juneau use only code similarity and data similarity , respectively . Juneau is a data search method [ 8 ] , which approximately ﬁnds similar tabu - lar data . Queries and parameters . We use six queries whose query graphs have diﬀerent attributes for each node . Q 1 – Q 3 , and Q 4 – Q 6 are the same topology , respectively . Each of them is a fragment of a compu - tational notebook in the dataset . In set - based similarity , we set ( α S , α D , α L , α O ) as ( 32 , 2 , 1 , 1 ) for Q 1 – Q 3 , and ( 32 , 1 , 1 , 1 ) for Q 4 – Q 6 . In graph - based similarity , we set ( β S , β D , β L , β O ) as ( 8 , 1 , 1 , 1 ) for all queries . In both similarity , (cid:96) θ is (cid:96) D for all queries . 6 . 2 Results and Analysis We show the experimental results to validate the ac - curacy and eﬃciency of our method . 6 . 2 . 1 Accuracy We evaluate the accuracy of search results based on user experiments . In the user experiments , nine users manually scored similarity of computational note - books to Q 4 – Q 6 . The score range is from 1 to 5 , and we use the average of each score of the manual evaluations for the queries . Figure 3 shows nDCG [ 1 ] of Graph - based , Set - based , C , and Juneau . Graph - based and Set - based achieve high nDCG . Since Graph - based is more accurate than Set - based , we can conﬁrm that it is eﬀective to pre - serve the computation order of cells and relationships of contents . From these results , we can see that our problem deﬁnitions are useful for the similarity search . 6 . 2 . 2 Eﬃciency We evaluate the eﬃciency of our methods . Figure 4 shows the search time for the Top - 10 search . The search time of Graph - based is much smaller than that of Set - based . This indicates that subgraph matching is eﬀective to reduce the compu - tation costs without sacriﬁcing the accuracy . Opti - mization techniques can accelerate the search method for both Graph - based and Set - based . Figure 5 shows the search time with varying k . For all k , Graph - based is the most eﬃcient among all the methods . In particular , when k is small , the eﬀec - tiveness of optimization techniques is large . This is because pruning works well due to high k - th similar - ity . The search time of methods without optimiza - tion techniques is constant because they compute all computational notebooks without pruning . 7 Conclusion In this paper , we deﬁned similarity search on com - putational notebooks and proposed a new framework for searching computational notebooks that compre - hensively considers source codes , tabular data , li - braries , and outputs . We deﬁned two similarity mea - sures ; set - based and graph - based similarity . The 10 set - based similarity handles contents independently , while graph - based similarity captures the relation - ships between contents . Our framework based on graph - based similarity with optimization techniques eﬃciently ﬁnds the top - k similar computational note - books . Through our experiments using Kaggle note - books , we showed that the graph - based similarity achieves high accuracy and our methods are eﬃcient . As the future work , we aim to improve our similar - ity search to ﬂexibly specify user requirements such as diverse results and dissimilar contents . In addi - tion , it is interesting to investigate the relationships between tabular data and source codes . References [ 1 ] C . Burges , T . Shaked , E . Renshaw , A . Lazier , M . Deeds , N . Hamilton , and G . Hullender . Learn - ing to rank using gradient descent . In Proceedings of the ICML , pages 89 – 96 , 2005 . [ 2 ] S . Chattopadhyay , I . Prasad , A . Z . Henley , A . Sarma , and T . Barik . What’s wrong with com - putational notebooks ? pain points , needs , and design opportunities . In Proceedings of the ACM CHI , pages 1 – 12 , 2020 . [ 3 ] L . Chen , H . Dharam , and D . Xin . Optimizing dataframes for interactive workloads . Proceedings of ACM Conference , 2019 . [ 4 ] R . Koschke , R . Falke , and P . Frenzel . Clone de - tection using abstract syntax suﬃx trees . In Pro - ceedings of the WCRE , pages 253 – 262 , 2006 . [ 5 ] J . Krinke . Identifying similar code with program dependence graphs . In Proceedings of the WCRE , pages 301 – 309 , 2001 . [ 6 ] C . Yan and Y . He . Auto - suggest : Learning - to - recommend data preparation steps using data sci - ence notebooks . In Proceedings of the ACM SIG - MOD , pages 1539 – 1554 , 2020 . [ 7 ] Y . Zhang and Z . G . Ives . Juneau : data lake man - agement for jupyter . Proceedings of the VLDB Endowment , 12 ( 12 ) : 1902 – 1905 , 2019 . [ 8 ] Y . Zhang and Z . G . Ives . Finding related tables in data lakes for interactive data science . In Pro - ceedings of the ACM SIGMOD , pages 1951 – 1966 , 2020 . 11