Concept Decomposition for Visual Exploration and Inspiration Yael Vinker ∗ 1 , 2 Andrey Voynov 2 Daniel Cohen - Or 1 , 2 Ariel Shamir 3 1 Tel Aviv University 2 Google Research 3 Reichman University https : / / inspirationtree . github . io / inspirationtree / Original Concept “v1” “v2” “v3” “v4” Figure 1 . Our method provides a tree - structured visual exploration space for a given unique concept . The nodes of the tree ( “ v i ” ) are newly learned textual vector embeddings , injected to the latent space of a pretrained text - to - image model . The nodes encode different aspects of the subject of interest . Through examining combinations within and across trees , the different aspects can inspire the creation of new designs and concepts . Abstract A creative idea is often born from transforming , com - bining , and modifying ideas from existing visual examples capturing various concepts . However , one cannot simply copy the concept as a whole , and inspiration is achieved by examining certain aspects of the concept . Hence , it is of - ten necessary to separate a concept into different aspects to provide new perspectives . In this paper , we propose a method to decompose a visual concept , represented as a set of images , into different visual aspects encoded in a hierar - chical tree structure . We utilize large vision - language mod - els and their rich latent space for concept decomposition and generation . Each node in the tree represents a sub - concept using a learned vector embedding injected into the latent space of a pretrained text - to - image model . We use a set of regularizations to guide the optimization of the em - bedding vectors encoded in the nodes to follow the hierar - chical structure of the tree . Our method allows to explore * Work was done during an internship at Google Research and discover new concepts derived from the original one . The tree provides the possibility of endless visual sampling at each node , allowing the user to explore the hidden sub - concepts of the object of interest . The learned aspects in each node can be combined within and across trees to cre - ate new visual ideas , and can be used in natural language sentences to apply such aspects to new designs . 1 . Introduction Modeling and design are highly creative processes that often require inspiration and exploration [ 14 ] . Designers of - ten draw inspiration from existing visual examples and con - cepts - either from the real world or using images [ 9 , 16 , 27 ] . However , rather than simply replicating previous designs , the ability to extract only certain aspects of a given con - cept is essential to generate original ideas . For example , in Figure 2a , we illustrate how designers may draw inspiration from patterns and concepts found in nature . Additionally , by combining multiple aspects from vari - a r X i v : 2305 . 18203v2 [ c s . C V ] 31 M a y 2023 ( a ) ( b ) Figure 2 . Example of design inspired by visual concepts taken from other concepts . ( a ) top left - fashion design by Iris Van Her - pen and Chair by Emmanuel Touraine inspired by nature patterns , bottom left - the Lotus Temple in India , inspired by the lotus flower ( b ) Beijing National Stadium is inspired by a combination of lo - cal Chinese art forms - the crackle glazed pottery that is local to Beijing , and the heavily veined Chinese scholar stones . ©Dress by Iris van Herpen , chair by Emmanuel Touraine from Wikime - dia . Lotus flower , temple , cracked pottery , scholar stone , and bird nest are from rawpixel . com [ Public Domain ] . Beijing National Stadium photograph by Wojtek Gurak from Flickr . ous concepts , designers are often able to create something new . For instance , it is described [ 8 ] that the famous Bei - jing National Stadium , also known as the “Bird’s Nest” , was designed by a group of architects that were inspired by var - ious aspects of different Chinese concepts ( see Figure 2b ) . The designers combined aspects of these different concepts – the shape of a nest , porous Chinese scholar stones , and cracks in glazed pottery art that is local to Beijing , to create an innovative architectural design . Such a design process is highly exploratory and often unexpected and surprising . The questions we tackle in this paper is whether a ma - chine can assist humans in such a highly creative process ? Can machines understand different aspects of a given con - cept , and provide inspiration for modeling and design ? Our work explores the ability of large vision - language models to do just that - express various concepts visually , decom - pose them into different aspects , and provide almost endless examples that are inspiring and sometimes unexpected . We rely on the rich semantic and visual knowledge hid - den in large language - vision models . Recently , these mod - els have been used to perform personalized text - to - image generation [ 11 , 25 , 36 ] , demonstrating unprecedented qual - ity of concept editing and variation . We extend the idea presented in [ 11 ] to allow aspect - aware text - to - image gen - eration , which can be used to visually explore new ideas derived from the original concept . Our approach involves ( 1 ) decomposing a given visual concept into different aspects , creating a hierarchy of sub - concepts , ( 2 ) providing numerous image instances of each learned aspect , and ( 3 ) allowing to explore combinations of aspects within the concept and across different concepts . We model the exploration space using a binary tree , where each node in the tree is a newly learned vector em - bedding in the textual latent space of a pretrained text - to - image model , representing different aspects of the original concept . A tree provides an intuitive structure to separate and navigate the different aspects of a given concept . Each level allows to find more aspects of the concepts in the previous level . In addition , each node by itself contains a plethora of samples and can be used for exploration . For ex - ample , in Figure 1 , the original concept is first decomposed into its dominant semantic aspects : the wooden saucer in “v1” and the bear drawing in “v2” , next , the bear drawing is further separated into the general concept of a bear in “v3” and its unique texture in “v4” . Given a small set of images depicting the concept of in - terest as input , we build the tree gradually . For each node , we optimize two child nodes at a time to match the concept depicted in their parent . We also utilize a CLIP - based [ 31 ] consistency measurement , to ensure that the concepts de - picted in the nodes are coherent and distinct . The differ - ent aspects are learned implicitly , without any external con - straint regarding the type of separation ( such as shape or texture ) . As a result , unexpected concepts can emerge in the process and be used as inspiration for new design ideas . For ״ A chair made of v2” “A house made of v1” Original Concept Figure 3 . Combining the learned aspects in natural sentences to produce aspect - based variations . The original concept is shown on top , along with an illustration of the chosen aspects from the tree in Figure 1 . Below are three random images generated by a pre - trained text - to - image model , conditioned on the prompts above . 2 example the learned aspects can be integrated into existing concepts by combining them in natural language sentences passed to a pretrained text - to - image model ( see Figure 3 ) . They can also be used to create new concepts by combining different aspects of the same tree ( intra - tree combination ) or across different trees ( inter - tree combination ) . We provide many visual results applied to various chal - lenging concepts . We demonstrate the ability of our ap - proach to find different aspects of a given concept , explore and discover new concepts derived from the original one , thereby inspiring the generation of new design ideas . 2 . Previous Work Design and Modeling Inspiration Creativity has been studied in a wide range of fields [ 1 , 5 , 10 , 22 , 37 ] , and al - though defining it exactly is difficult , some researchers have suggested that it can be described as the act of evoking and recombinating information from previous knowledge to generate new properties [ 5 , 47 ] . It is essential , however , to be able to associate ideas in order to generate original ideas rather than just mimicking prior work [ 6 ] . Experi - enced designers and artists are more adept at connecting disparate ideas than novice designers , who need assistance in the evocation process [ 5 ] . By reviewing many exemplars , designers are able to gain a deeper understanding of design spaces and solutions [ 9 ] . In the field of human - computer interaction , a number of studies have been conducted to de - velop tools and software to assist designers in the process of ideation [ 20 , 21 , 23 , 24 ] . They are focused on providing better tools for collecting , arranging , and searching visual and textual data , often collected from the web . In contrast , our work focuses on extracting different aspects of a given visual concept and generating new images for inspiration . Our work is close to a line of work utilizing evolutionary algorithms to inspire users’ creativity [ 3 , 7 , 48 ] . However , they mostly work in the field of 3D content generation and do not decompose different aspects from existing concepts . Large Language - Vision Models With the recent ad - vancement of language - vision models [ 31 ] and diffusion models [ 28 , 33 , 34 ] , the field of image generation and edit - ing has undergone unprecedented evolution . These models have been trained on millions of images and text pairs and have shown to be effective in performing challenging vi - sion related tasks [ 2 , 4 , 13 , 30 , 38 ] . Furthermore , the strong visual and semantic priors of these models have also been demonstrated to be effective for artistic and design tasks [ 26 , 29 , 41 – 43 ] . In our work , we demonstrate how these models can be used to decompose and transform existing concepts into new ones in order to inspire the development of new ideas . Personalization Personalized text - to - image generation has been introduced recently [ 11 , 18 , 25 , 36 ] , with the goal of creating novel scenes based on user provided unique con - cepts . In addition to demonstrating unprecedented quality results , these technologies enabled intuitive editing , made design more accessible , and attracted interest even beyond the research community . We utilize these ideas to facili - tate the ideation process of designers and common users , by learning different visual aspects of user - provided concepts . Current personalization methods either optimize a set of embeddings to describe the concept [ 11 ] , or modify the de - noising network to tie a rarely used word embedding to the new concept [ 36 ] . While the latter provides more accurate reconstruction and is more robust , it uses much more mem - ory and requires a model for each object . In this regard , we choose to rely on the approach presented in [ 11 ] . It is im - portant to note that our goal is to capture multiple aspects of the given concept , and not to improve the accuracy of reconstruction as in [ 12 , 15 , 39 , 40 , 45 , 46 ] . 3 . Preliminaries Latent Diffusion Models . Diffusion models are genera - tive models trained to learn data distribution by gradually denoising a variable sampled from a Gaussian distribution . In our work , we use the publicly available text - to - image Stable Diffusion model [ 34 ] . Stable Diffusion is a type of a latent diffusion model ( LDM ) , where the diffusion pro - cess is applied on the latent space of a pretrained image autoencoder . The encoder E maps an input image x into a latent vector z , and the decoder D is trained to decode z such that D ( z ) ≈ x . As a second stage , a denoising dif - fusion probabilistic model ( DDPM ) [ 17 ] is trained to gen - erate codes within the learned latent space . At each step during training , a scalar t ∈ { 1 , 2 , . . . T } is uniformly sam - pled and used to define a noised latent code z t = α t z + σ t ϵ , where ϵ ∼ N ( 0 , I ) and α t , σ t are terms that control the noise schedule , and are functions of the diffusion process time t . The denoising network ϵ θ which is based on a UNet architecture [ 35 ] , receives as input the noised code z t , the timestep t , and an optional condition vector c ( y ) , and is tasked with predicting the added noise ϵ . The LDM loss is defined by : L LDM = E z ∼E ( x ) , y , ϵ ∼N ( 0 , 1 ) , t (cid:2) | | ϵ − ϵ θ ( z t , t , c ( y ) ) | | 22 (cid:3) ( 1 ) For text - to - image generation the condition y is a text input and c ( y ) represents the text embedding . At inference time , a random latent code z T ∼ N ( 0 , I ) is sampled , and iter - atively denoised by the trained ϵ θ until producing a clean z 0 latent code , which is passed through the decoder D to produce the image x . We next discuss the text encoder and the inversion space . 3 ( b ) Extend the table Initialized with the emb . of “object” ( a ) Generate I “v p ” T2I ( c ) Training step 𝑣 ! 𝑣 " 𝑣 ! 𝑣 # 𝑣 " . . . Text Encoder 𝐿 $ 𝜖 𝜖̂ 𝑧 % U N e t p I 0 𝑥 𝑧 Enc . 𝑦 = “A photo of ” 𝑠 ! 𝑠 " Figure 4 . High level pipeline of the “binary reconstruction” stage . We optimize two sibling nodes v l , v r at a time ( marked in red and blue ) . ( a ) We first generate a small training set of images I p depicting the concept in the parent node using a pretrained text - to - image model ( T2I ) . At the root , we use the original set of images I 0 . ( b ) We then extend the existing dictionary by adding the two new vectors , initialized with the embedding of the word “object” . ( c ) Lastly , we optimize v l , v r w . r . t . the LDM loss ( see details in the text ) . Text embedding . Given a text prompt y , for example “A photo of a cat” , the sentence is first converted into tokens , which are indexed into a pre - defined dictionary of vector embeddings . The dictionary is a lookup table that connects each token to a unique embedding vector . After retriev - ing the vectors for a given sentence from the table , they are passed to a text transformer , which processes the con - nections between the individual words in the sentence and outputs c ( y ) . The output encoding c ( y ) is then used as a condition to the UNet in the denoising process . We denote words with S , and the vector embeddings from the lookup table with V . Textual Inversion We rely on the general framework pro - posed by [ 11 ] , who choose the embedding space of V as the target for inversion . They formulate the task of inversion as fitting a new word s ∗ to represent a personal concept , de - picted by a small set of input images provided by the user . They extend the predefined lookup table with a new embed - ding vector v ∗ that is linked to s ∗ . The vector v ∗ is often initialized with the embedding of an existing word from the dictionary that has some relation to the given concept , and then optimized to represent the desired personal concept . This process can be thought of as “injecting” the new con - cept into the vocabulary . The vector v ∗ is optimized w . r . t . the LDM loss in Equation ( 1 ) over images sampled from the input set . At each step of optimization , a random image x is sampled from the set , along with a neutral context text y , derived from the CLIP ImageNet templates [ 32 ] ( such as “A photo of s ∗ ” ) . Then , the image x is encoded to z = E ( x ) and noised w . r . t . a randomly sampled timestep t and noise ϵ : z t = α t z + σ t ϵ . The noisy latent image z t , timestep t , and text embedding c ( y ) are then fed into a pretrained UNet model which is trained to predict the noise ϵ applied w . r . t . the conditioned text and timestep . This way , v ∗ is op - timized to describe the object depicted in the small training set of images . 4 . Method Given a small set of images I 0 = { I 01 . . . I 0 m } depicting the desired visual concept , our goal is to construct a rich visual exploration space expressing different aspects of the input concept . We model the exploration space as a binary tree , whose nodes V = { v 1 . . v n } are learned vector embeddings corre - sponding to newly discovered words S = { s 1 . . s n } added to the predefined dictionary , representing different aspects of the original concept . These newly learned words are used as input to a pretrained text - to - image model [ 34 ] to gener - ate a rich variety of image examples in each node . We find a binary tree to be a suitable choice for our objective , because of the ease of visualization , navigation , and the quality of the sub - concepts depicted in the nodes ( see supplemental file for further analysis ) . 4 . 1 . Tree Construction The exploration tree is built gradually as a binary tree from top to bottom , where we iteratively add two new nodes at a time . To create two child nodes , we optimize new embedding vectors according to the input image - set gen - erated from the concept depicted in the parent node . During construction , we define two requirements to encourage the learned embeddings to follow the tree structure : ( 1 ) Binary Reconstruction each pair of children nodes together should encapsulate the concept depicted by their parent node , and ( 2 ) Coherency each individual node should depict a coher - ent concept which is distinct from its sibling . Next , we de - scribe the loss functions and procedures designed to follow these requirements . Binary Reconstruction We use the reconstruction loss suggested in [ 11 ] , with some modifications tailored to our goal . The procedure is illustrated in Figure 4 – in each optimization phase , our goal is to learn two vector em - beddings v l , v r corresponding to the left and right sibling 4 ”𝑣 ! 𝑣 " ” . . . . . . . . . ”𝑣 ! ” ”𝑣 " ” 0 50 100 200 2000 Original Concept Figure 5 . Optimization iterations . The embedding of both chil - dren nodes v l , v r are initilized with the word “object” . During it - erations , they gradually depict two aspects of the original concept . Note that using both embedding together reconstructs the original parent concept . nodes , whose parent node is marked with v p ( illustrated in Figure 4 , left ) . We begin with generating a new small training set of images I p = { I p 1 . . . I p 10 } , reflecting the con - cept depicted by the vector v p ( Figure 4a ) . At the root , we use the original set of images I 0 . Next , we extend the current dictionary by adding two new vector embeddings v l , v r , corresponding to the right and left children of their parent node v p ( Figure 4b ) . To represent general concepts , the newly added vectors are initialized with the embedding of the word “object” . At each iteration of optimization ( Fig - ure 4c ) , an image x is sampled from the set I p and encoded to form the latent image z = E ( x ) . A timestep t and a noise ϵ are also sampled to define the noised latent z t = α t z + σ t ϵ ( marked in yellow ) . Additionally , a neutral context text y is sampled , containing the new placeholder words in the fol - lowing form “A photograph of s l s r ” . The noised latent z t is fed to a pretrained Stable Diffusion UNet model ϵ θ , con - ditioned on the CLIP embedding c ( y ) of the sampled text , to predict the noise ϵ . The prediction loss is backpropagated w . r . t . the vector embeddings v l , v r : { v l , v r } = arg min v E z ∼E ( x ) , y , ϵ ∼N ( 0 , 1 ) , t (cid:104) ∥ ϵ − ϵ θ ( z t , t , c ( y ) ) ∥ 22 (cid:105) . ( 2 ) This procedure encourages v l , v r together to express the vi - sual concept of their parent depicted in the set I p . Figure 5 illustrates how the two embeddings begin by representing the word “object” , and gradually converge to depict two as - pects of the input concept . We use the timestep sampling approach proposed in Re - Version [ 19 ] , which skews the sampling distribution so that a larger t is assigned a higher probability , according to the following importance sampling function : f ( t ) = 1 T ( 1 − α cos πt T ) . ( 3 ) We set α = 0 . 5 . We find that this sampling approach im - proves stability and content separation . This choice is fur - ther discussed in the supplementary file . Inconsistent Set Consistent Set Figure 6 . We demonstrate two sets of random images generated from two different vector embeddings . An example of a consistent set can be seen on the left , where the concept depicted in the node is clear . We show an inconsistent set on the right , where images appear to depict multiple concepts . Coherency The resulting pair of embeddings described above together often capture the parent concept depicted in the original images well . However , the images produced by each embedding individually may not always reflect a logi - cal sub - concept that is coherent to the observer . We find that such incoherent embeddings are frequently characterized by inconsistent appearance of the images gen - erated from them , i . e . , it can be difficult to identify a com - mon concept behind them . For example , in Figure 6 the concept depicted in the set on the right is not clear , com - pared to the set of images on the left . This issue may be related to the observation that tex - tual inversion often results in vector embedding outside of the distribution of common words in the dictionary , affect - ing editability as well [ 45 ] . It is thus possible that em - beddings that are highly unusual may not behave as “real words” , thereby producing incoherent visual concepts . In addition , textual - inversion based methods are sometimes unstable and depend on the seed and iteration selection . To overcome this issue we define a consistency test , which allows us to filter out incoherent embeddings . We be - gin by running the procedure described above to find v l , v r using k different seeds in parallel for a sufficient number of steps ( in our experiments we found that k = 4 and 200 steps are sufficient since at that point the embeddings have al - ready progressed far enough from their initialization word “object” as seen in Figure 5 ) . This gives us an initial set of k pairs of vector embed - dings V s = { v il , v ir } ki = 1 . For each vector v ∈ V s we gen - erate a random set I v of 40 images using our pre - trained text - to - image model . We then use a pretrained CLIP Image encoder [ 31 ] , to produce the embedding CLIP ( I vi ) of each image in the set . We define the consistency of two sets of images I a , I b as follows : C ( I a , I b ) = mean I ai ∈ I a , I bj ∈ I b , I ai ̸ = I bj ( sim ( CLIP ( I ai ) , CLIP ( I bj ) ) ) . ( 4 ) 5 0 . 59 0 . 79 0 . 73 0 . 59 𝑣 ! " 𝑣 # " 𝑣 # " 𝑣 ! " 0 . 73 0 . 78 0 . 81 0 . 73 𝑣 ! $ 𝑣 # $ 𝑣 # $ 𝑣 ! $ Figure 7 . Consistency scores matrix between image sample sets of nodes . The seed selection process favors pairs of siblings that have a high consistency score within themselves , and low consistency score between each other . In this example , the left pair is better than the right . Note that | C ( I a , I b ) | ≤ 1 because sim ( x , y ) = x · y | | x | | · | | y | | is the cosine similarity between a pair of CLIP embed - ding of two different images . This formulation is moti - vated by the observation that if a set of images depicts a certain semantic concept , their vector embedding in CLIP’s latent space should be relatively close to each other . Ide - ally , we are looking for pairs in which each node is coher - ent by itself , and in addition , two sibling nodes are distinct from each other . We therefore choose the pair of tokens { v ∗ l , v ∗ r } ∈ V s as follows : { v ∗ l , v ∗ r } = arg max { v il , v ir } ∈ V s (cid:2) C il + C ir + = ( min ( C il , C ir ) − C ( I v il , I v ir ) ) (cid:3) , ( 5 ) where C il = C ( I v il , I v il ) , C ir = C ( I v ir , I v ir ) . Note that we do not consider the absolute cross consistency score C ( I v il , I v ir ) , but we compute its relative difference from the node with the minimum consistency . We demonstrate this procedure in Figure 7 . We optimized two pairs of sib - ling nodes { v 1 l , v 1 r } , { v 2 l , v 2 r } using two seeds , w . r . t . the same parent node . Each matrix illustrates the consistency scores C i l , C ( I v i l , I v i r ) , C ir obtained for the sets of images of each seed . In both cases , the scores on the diagonal are high , which indicates that each set is consistent within itself . While the sets on the right obtained a higher consistency score within each node , they also obtained a relatively high score across the nodes ( 0 . 73 ) , which means they are not dis - tinct enough . After selecting the optimal seed , we continue the opti - mization of the chosen vector pair w . r . t . the reconstruction loss in Equation ( 2 ) for 1500 iterations . 5 . Results In Figures 1 , 11 and 12 , we show examples of possible trees . For each node in the tree , we use its corresponding placeholder word as an input to a pretrained text - to - image model [ 34 ] , to generate a set of random images . These images have been generated without any prompt engineer - ing or additional words within the sentence , except for the word itself . For clarity , we use the notion ” v ” next to each set of images , illustrating that the presented set depicts the concept learned in that node . As can be seen , the learned embeddings in each node capture different elements of the original concept , such as the concept of a cat and a sculp - ture , as well as the unique texture in Figure 11 . The sub - concepts captured in the nodes follow the tree’s structure , where the concepts are decomposed gradually , with two sib - ling nodes decomposing their parent node . This decomposi - tion is done implicitly , without external guidance regarding the split theme . For many more trees please see our supple - mentary file . 5 . 1 . Applications The constructed tree provides a rich visual exploration space for concepts related to the object of interest . In this section we demonstrate how this space can be used for novel combination and exploration . Intra - tree combination – the generated tree is represented via the set of optimized vectors V = { v 1 . . v n } . Once this set is learned we can use it to perform further exploration and conceptual editing within the object’s “inner world” . We can explore combinations of different aspects by composing sentences containing different subsets of V . For example , in the bottom left area of Figure 11 , we have combined v 1 and v 5 , which resulted in a variation of the original sculp - ture without the sub - concept relating to the cat ( depicted in v 6 ) . At the bottom right , we have excluded the sub - concept depicted in v 5 ( related to a blue sculpture ) , which resulted in a new representation of a flat cat with the body and texture of the original object . Such combinations can provide new perspectives on the original concept and inspiration that highlights only specific aspects . Inter - tree combination – it is also possible to combine concepts learned across different trees , since we only in - ject new words into the existing dictionary , and do not fine - tune the model’s weights as in other personalization ap - proaches [ 36 ] . To achieve this , we first build the trees independently for each concept and then visualize the sub - concepts de - picted in the nodes to select interesting combinations . In Figure 8 the generated original concepts are shown on top , along with an illustration of the concepts depicted in the rel - evant nodes . To combine the concepts across the trees , we simply place the two placeholder words together in a sen - tence and feed it into the pretrained text - to - image model . As can be seen , on the left the concept of a “saucer with a drawing” and the “creature” from the mug are combined to create many creative and surprising combinations of the 6 “v 1 _ b ” “v 5 _ a “ “v 5 _ a v 1 _ b ” “v 2 _ a v 1 _ b ” “v 1 _ b ” “v 2 _ a “ Figure 8 . Examples of inter - tree combinations . We use our method to produce trees for the four concepts depicted in the first row . We then combine aspects from different trees to generate a set of inter - tree combinations ( the chosen aspects are shown next to each concept ) . two . On the right , the blue sculpture of a cat is combined with the stone depicted at the bottom of the Buddha , which together create new sculptures in which the Buddha is re - placed with the cat . Text - based generation – the placeholder words of the learned embeddings can be composed into natural language sentences to generate various scenes based on the learned aspects . We illustrate this at the top of Figure 9 , where we integrate the learned aspects of the original concepts in new designs ( in this case of a chair and a dress ) . At the bottom of Figure 9 , we show the effect of using the learned vectors of the original concepts instead of specific aspects . We apply Textual Inversion ( TI ) [ 11 ] with the default hyperparame - ters to fit a new word depicting each concept , and choose a representative result . The results suggest that without as - pect decomposition , generation can be quite limited . For instance , in the first column , both the dress and the chair are dominated by the texture of the sculpture , whereas the concept of a blue cat is almost ignored . Furthermore , TI may exclude the main object of the sentence ( second and third columns ) , or the results may capture all aspects of the object ( fourth column ) , thereby narrowing the exploration space . 5 . 2 . Evaluations Consistency Score Validation . We first show that our consistency test proposed in Equation ( 4 ) aligns well with human perception of consistency . We conducted a percep - tual study with 35 participants in which we presented 15 pairs of random image sets depicting sub - concepts of 9 ob - jects . We asked participants to determine which of the sets is more consistent within itself in terms of the concept it de - picts ( an example of such a pair can be seen in Figure 6 ) . We also measured the consistency scores for these sets using our CLIP - based approach , and compared the results . The CLIP - based scores matched the human choices in 82 . 3 % of the cases . Original Concept “A chair made of { } ” “A dress madeof { } ” Chosen Aspect Ours TI “A chair made of { } ” “A dress made of { } ” Figure 9 . Combining the learned aspects in natural sentences to produce aspect - based variations . The original concepts are shown at the top . In the third and fourth rows are our text - based gener - ation results applied with the aspects depicted in the second row . Under “TI” we show image generation for the concepts in the first row ( without our aspect decomposition approach ) , produced us - ing [ 11 ] . Reconstruction and Separation . We quantitatively eval - uate our method’s ability to follow the tree requirements of reconstruction and sub - concept separation . We collected a set of 13 concepts ( 9 from existing personalization datasets [ 11 , 25 ] , and 4 new concepts from our dataset ) , and gener - 7 “v1” “v2” ( a ) Background leakage ( c ) Dominant concept ( d ) Large overlap “v1 v2” “v1 v2” “v1” “v2” ( b ) Incomprehensible aspects “v1” “v2” “v1 v2” “v1 v2” “v1” “v2” Figure 10 . We demonstrate four general cases of decomposition failure . ated 13 corresponding trees . Note that we chose concepts that are complex enough and have the potential to be divided into different aspects ( we discuss this in the limitations sec - tion ) . For each pair of sibling nodes v l , v r and their parent node v p , we produced their corresponding sets of images – I v l , I v r , I v p ( where for nodes in the first level we used the original set of images I 0 as I v p ) . We additionally produced the set I v l v r , depicting the joint concept learned by two sib - ling nodes . We first compute C ( I v p , I v l v r ) to measure the quality of reconstruction , i . e . , that two sibling nodes together repre - sent the concept depicted in their parent node . The average score obtained for this measurement is 0 . 8 , which suggests that on average , the concept depicted by the children nodes together is consistent with that of their parent node . Second , we measure if two sibling nodes depict distinct concepts by using C ( I v l , I v r ) . The average score obtained was 0 . 59 , in - dicating there is larger separation between siblings , but they are still close . Aspects Relevancy . We assess the ability of our method to encode different aspects connected to the input concept via a perceptual study . We chose 5 objects from the dataset above , and 3 random aspects for each object . We presented participants with a random set of images depicting one as - pect of one object at a time . We asked the participants to choose the object they believe this aspect originated from , along with the option ‘none’ . In total we collected an - swers from 35 participants , and achieved recognition rates of 87 . 8 % . These evaluations demonstrate that our method can indeed separate a concept into relevant aspects , where each new sub - concept is coherent , and the binary tree struc - ture is valid - i . e . , the combination of two children can re - construct the parent concept . 6 . Limitations Our method may fail to decompose an input concept . We divide the failure cases into four general categories illus - trated in Figure 10 : ( 1 ) Background leakage - the training images should be taken from different perspectives and with varying back - grounds ( this requirement also exists in [ 11 ] ) . When im - ages do not meet these criteria , one of the sibling nodes often captures information from the background instead of the object itself . ( 2 ) Incomprehensible aspects - some separations may not satisfy clear , interesting , aesthetic , or inspiring aspects , even when the coherency principle holds . ( 3 ) Dominant sub - concept - we illustrate this in Figure 10c , where we show a split on the second level of the concept depicted under “ v 1 v 2 ” . As shown , v1 has dominated the information , so even if the coherency term is held , decom - position to two sub - concepts has not really been achieved . ( 4 ) Large overlap when two aspects share information – we illustrate this in Figure 10d , which is a split of the second level , where both concepts depicted in v1 and v2 appear to share too similar . We hope that such limitations could be resolved in the future using additional regularization terms in the optimiza - tion process or through the development of more robust per - sonalization methods . Additionally , our method can have difficulties to create deeper trees and nodes with more than two children ( see examples in supplemental file ) . Currently , we stop the pro - cess when sub - concepts become too simple or incoherent . This could be the result of the new embeddings drifting towards out - of - distribution codes . Further investigation is needed in this subject . Currently the time for decomposing a node can reach up to approximately 40 minutes on a sin - gle A100 GPU . However , as textual inversion optimization techniques will progress , so will our method . 7 . Conclusions We presented a method to implicitly decompose a given visual concept into various aspects to construct an inspiring visual exploration space . Our method can be used to gen - erate numerous representations and variations of a certain subject , to combine aspects across objects , as well as to use these aspects as part of natural language sentences that drive visual generation of novel concepts . The aspects are learned implicitly , without external guid - ance regarding the type of separation . This implicit ap - proach also provides another small step in revealing the rich latent space of large vision - language models , allowing surprising and creative representations to be produced . We demonstrated the effectiveness of our method on a variety of challenging concepts . We hope our work will open the door to further research aimed at developing and improving existing tools to assist and inspire designers and artists . Acknowledgements We thank Rinon Gal , Kfir Aberman , and Yael Pritch for their early feedback and insightful dis - cussions . 8 Original Image “v1 v2” “v1” “v2” “v3” “v4” “v5” “v6” “v 1 v 5 ” Combining different aspects “v 1 v 6 ” Figure 11 . Exploration tree for the cat sculpture . At the bottom we show examples of possible intra - tree combinations . 9 Original Image “v1” “v2” “v3” “v4” “A chair made of v1” Text based editing “v1 v2” “A dress made of v1” Figure 12 . Exploration tree for a decorated teapot . At the bottom we show examples of possible text - based generation . 10 References [ 1 ] Teresa M . Amabile . Creativity in context : Update to the social psychology of creativity . 1996 . 3 [ 2 ] Tomer Amit , Tal Shaharbany , Eliya Nachmani , and Lior Wolf . Segdiff : Image segmentation with diffusion proba - bilistic models , 2021 . 3 [ 3 ] Melinos Averkiou , Vladimir G . Kim , Youyi Zheng , and Niloy Jyoti Mitra . Shapesynth : Parameterizing model col - lections for coupled shape exploration and synthesis . Com - puter Graphics Forum , 33 , 2014 . 3 [ 4 ] Omri Avrahami , Dani Lischinski , and Ohad Fried . Blended diffusion for text - driven editing of natural images . In Pro - ceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 18208 – 18218 , June 2022 . 3 [ 5 ] Nathalie Bonnardel and Evelyne Cauzinille - Marm ` eche . To - wards supporting evocation processes in creative design : A cognitive approach . Int . J . Hum . Comput . Stud . , 63 : 422 – 435 , 2005 . 3 [ 6 ] David C . Brown . Guiding computational design creativity research . 2008 . 3 [ 7 ] Daniel Cohen - Or and Hao Zhang . From inspired modeling to creative modeling . The Visual Computer , 32 : 7 – 14 , 2015 . 3 [ 8 ] designingbuildings wiki . Beijing national stadium design , 2021 . 2 [ 9 ] Claudia Eckert and Martin Stacey . Sources of inspiration : a language of design . Design Studies , 21 ( 5 ) : 523 – 538 , 2000 . 1 , 3 [ 10 ] Mohamed Elhoseiny and Mohamed Elfeki . Creativity in - spired zero - shot learning . 2019 IEEE / CVF International Conference on Computer Vision ( ICCV ) , pages 5783 – 5792 , 2019 . 3 [ 11 ] Rinon Gal , Yuval Alaluf , Yuval Atzmon , Or Patashnik , Amit H . Bermano , Gal Chechik , and Daniel Cohen - Or . An image is worth one word : Personalizing text - to - image gen - eration using textual inversion , 2022 . 2 , 3 , 4 , 7 , 8 , 13 , 14 [ 12 ] Rinon Gal , Moab Arar , Yuval Atzmon , Amit H . Bermano , Gal Chechik , and Daniel Cohen - Or . Designing an encoder for fast personalization of text - to - image models . ArXiv , abs / 2302 . 12228 , 2023 . 3 [ 13 ] Rinon Gal , Or Patashnik , Haggai Maron , Gal Chechik , and Daniel Cohen - Or . Stylegan - nada : Clip - guided domain adap - tation of image generators , 2021 . 3 [ 14 ] Milene Gonc¸alves , Carlos Cardoso , and Petra Badke - Schaub . What inspires designers ? preferences on inspira - tional approaches during idea generation . Design Studies , 35 : 29 – 53 , 2014 . 1 [ 15 ] Ligong Han , Yinxiao Li , Han Zhang , Peyman Milanfar , Dimitris N . Metaxas , and Feng Yang . Svdiff : Com - pact parameter space for diffusion fine - tuning . ArXiv , abs / 2303 . 11305 , 2023 . 3 [ 16 ] K Henderson . On line and on paper : visual representations , visual culture , and computer graphics in design engineering . inside technology , 1999 . 1 [ 17 ] Jonathan Ho , Ajay Jain , and Pieter Abbeel . Denoising dif - fusion probabilistic models . CoRR , abs / 2006 . 11239 , 2020 . 3 [ 18 ] Edward Hu , Yelong Shen , Phil Wallis , Zeyuan Allen - Zhu , Yuanzhi Li , Lu Wang , and Weizhu Chen . Lora : Low - rank adaptation of large language models , 2021 . 3 [ 19 ] Ziqi Huang , Tianxing Wu , Yuming Jiang , Kelvin C . K . Chan , and Ziwei Liu . ReVersion : Diffusion - based relation inver - sion from images . arXiv preprint arXiv : 2303 . 13495 , 2023 . 5 , 15 [ 20 ] Alexander Ivanov , David Ledo , Tovi Grossman , George Fitz - maurice , and Fraser Anderson . Moodcubes : Immersive spaces for collecting , discovering and envisioning inspira - tion materials . In Designing Interactive Systems Conference , DIS ’22 , page 189 – 203 , New York , NY , USA , 2022 . Associ - ation for Computing Machinery . 3 [ 21 ] Youwen Kang , Zhida Sun , Sitong Wang , Zeyu Huang , Zim - ing Wu , and Xiaojuan Ma . Metamap : Supporting vi - sual metaphor ideation through multi - dimensional example - based exploration . In Proceedings of the 2021 CHI Con - ference on Human Factors in Computing Systems , CHI ’21 , New York , NY , USA , 2021 . Association for Computing Ma - chinery . 3 [ 22 ] Anna Kantosalo , Jukka M . Toivanen , Ping Xiao , and Hannu ( TT ) Toivonen . From isolation to involvement : Adapting machine creativity software to support human - computer co - creation . In International Conference on In - novative Computing and Cloud Computing , 2014 . 3 [ 23 ] Janin Koch , Andr´es Lucero , Lena Hegemann , and Antti Oulasvirta . May ai ? : Design ideation with cooperative con - textual bandits . Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , 2019 . 3 [ 24 ] Janin Koch , Nicolas Taffin , Michel Beaudouin - Lafon , Markku Laine , Andr´es Lucero , and Wendy E . Mackay . Im - agesense : An intelligent collaborative ideation tool to sup - port diverse human - computer partnerships . Proc . ACM Hum . - Comput . Interact . , 4 ( CSCW1 ) , may 2020 . 3 [ 25 ] Nupur Kumari , Bingliang Zhang , Richard Zhang , Eli Shechtman , and Jun - Yan Zhu . Multi - concept customization of text - to - image diffusion . 2023 . 2 , 3 , 7 [ 26 ] Midjourney . Midjourney . com , 2022 . 3 [ 27 ] W . Muller . Design discipline and the significance of visuo - spatial thinking . Design Studies , 10 ( 1 ) : 12 – 23 , 1989 . 1 [ 28 ] Alex Nichol , Prafulla Dhariwal , Aditya Ramesh , Pranav Shyam , Pamela Mishkin , Bob McGrew , Ilya Sutskever , and Mark Chen . Glide : Towards photorealistic image generation and editing with text - guided diffusion models . arXiv preprint arXiv : 2112 . 10741 , 2021 . 3 [ 29 ] Jonas Oppenlaender . The creativity of text - to - image gen - eration . Proceedings of the 25th International Academic Mindtrek Conference , 2022 . 3 [ 30 ] Or Patashnik , Zongze Wu , Eli Shechtman , Daniel Cohen - Or , and Dani Lischinski . Styleclip : Text - driven manipulation of stylegan imagery . In Proceedings of the IEEE / CVF In - ternational Conference on Computer Vision ( ICCV ) , pages 2085 – 2094 , October 2021 . 3 [ 31 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , 11 Amanda Askell , Pamela Mishkin , Jack Clark , Gretchen Krueger , and Ilya Sutskever . Learning transferable vi - sual models from natural language supervision . CoRR , abs / 2103 . 00020 , 2021 . 2 , 3 , 5 [ 32 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , Gretchen Krueger , and Ilya Sutskever . Learning transferable visual models from natural language supervision , 2021 . 4 , 13 [ 33 ] Aditya Ramesh , Prafulla Dhariwal , Alex Nichol , Casey Chu , and Mark Chen . Hierarchical text - conditional image gen - eration with clip latents . arXiv preprint arXiv : 2204 . 06125 , 2022 . 3 [ 34 ] Robin Rombach , Andreas Blattmann , Dominik Lorenz , Patrick Esser , and Bj¨orn Ommer . High - resolution image syn - thesis with latent diffusion models , 2022 . 3 , 4 , 6 [ 35 ] Olaf Ronneberger , Philipp Fischer , and Thomas Brox . U - net : Convolutional networks for biomedical image segmen - tation . In International Conference on Medical image com - puting and computer - assisted intervention , pages 234 – 241 . Springer , 2015 . 3 [ 36 ] Nataniel Ruiz , Yuanzhen Li , Varun Jampani , Yael Pritch , Michael Rubinstein , and Kfir Aberman . Dreambooth : Fine tuning text - to - image diffusion models for subject - driven generation . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , 2023 . 2 , 3 , 6 [ 37 ] Mark A . Runco and Garrett J . Jaeger . The standard definition of creativity . Creativity Research Journal , 24 : 92 – 96 , 2012 . 3 [ 38 ] Shelly Sheynin , Oron Ashual , Adam Polyak , Uriel Singer , Oran Gafni , Eliya Nachmani , and Yaniv Taigman . Knn - diffusion : Image generation via large - scale retrieval , 2022 . 3 [ 39 ] Jing Shi , Wei Xiong , Zhe L . Lin , and Hyun Joon Jung . In - stantbooth : Personalized text - to - image generation without test - time finetuning . ArXiv , abs / 2304 . 03411 , 2023 . 3 [ 40 ] Yoad Tewel , Rinon Gal , Gal Chechik , and Yuval Atzmon . Key - locked rank one editing for text - to - image personaliza - tion . ArXiv , abs / 2305 . 01644 , 2023 . 3 [ 41 ] Yingtao Tian and David Ha . Modern evolution strategies for creativity : Fitting concrete images and abstract concepts . CoRR , abs / 2109 . 08857 , 2021 . 3 [ 42 ] Yael Vinker , Yuval Alaluf , Daniel Cohen - Or , and Ariel Shamir . Clipascene : Scene sketching with different types and levels of abstraction , 2022 . 3 [ 43 ] Yael Vinker , Ehsan Pajouheshgar , Jessica Y . Bo , Ro - man Christian Bachmann , Amit Haim Bermano , Daniel Cohen - Or , Amir Zamir , and Ariel Shamir . Clipasso : Semantically - aware object sketching . ACM Trans . Graph . , 41 ( 4 ) , jul 2022 . 3 [ 44 ] Patrick von Platen , Suraj Patil , Anton Lozhkov , Pedro Cuenca , Nathan Lambert , Kashif Rasul , Mishig Davaadorj , and Thomas Wolf . Diffusers : State - of - the - art diffusion models . https : / / github . com / huggingface / diffusers , 2022 . 13 [ 45 ] Andrey Voynov , Q . Chu , Daniel Cohen - Or , and Kfir Aber - man . P + : Extended textual conditioning in text - to - image generation . ArXiv , abs / 2303 . 09522 , 2023 . 3 , 5 , 13 , 14 [ 46 ] Yuxiang Wei , Yabo Zhang , Zhilong Ji , Jinfeng Bai , Lei Zhang , and Wangmeng Zuo . Elite : Encoding visual con - cepts into textual embeddings for customized text - to - image generation . ArXiv , abs / 2302 . 13848 , 2023 . 3 [ 47 ] Merryl J . Wilkenfeld and Thomas B . Ward . Similarity and emergence in conceptual combination . Journal of Memory and Language , 45 ( 1 ) : 21 – 38 , 2001 . 3 [ 48 ] Kai Xu , Hao Zhang , Daniel Cohen - Or , and Baoquan Chen . Fit and diverse . ACM Transactions on Graphics ( TOG ) , 31 : 1 – 10 , 2012 . 3 12 Appendix Table of Contents A . Implementation details 13 B . Baselines 13 C . Ablation and Analysis 14 C . 1 . Binary Tree . . . . . . . . . . . . . . 14 C . 2 . Timestep Sampling . . . . . . . . . . 15 C . 3 . Consistency Test . . . . . . . . . . . 16 C . 4 . Perceptual Study . . . . . . . . . . . 18 D . Additional Qualitative Results 18 A . Implementation details We rely on the diffusers [ 44 ] implementation of Textual Inversion [ 11 ] , based on Stable Diffusion v1 . 5 text - to - image model [ 32 ] . We used the default training parameters pro - vided in this implementation , except for changing the batch size to 2 ( which scales the learning rate to 0 . 004 respec - tively ) . We used four different seeds { 0 , 1000 , 1234 , 111 } for each sibling nodes optimization . To generate the set of 10 images for each new node we first generated a random set of 40 images , and used our proposed CLIP consistency measurement to choose a subset of 10 images that are most consistent with each other . Our code will be made available to facilitate future research . B . Baselines In the absence of existing works attempting to achieve our goal of decomposition into different aspects , we com - pare our performance in intra - tree combination with two ex - isting relevant works . We consider Textual Inversion [ 11 ] , and its more ad - vanced modification – Extended Textual Inversion [ 45 ] – designed specifically for appearance mixing ( which is most similar to our “intra - tree combination” ) . We provide a qualitative comparison to these methods in Figure 13 . On the left we aim to combine the aspect of a wooden saucer and the creature on the cup from the objects presented on top . On the right we aim to combine a part of the stone statue with some specific style aspects of the cat sculpture . Gal et al . [ 11 ] propose a style transfer application , in which their method can be used to find pseudo words rep - resenting a specific style taken from a given concept , and can then be applied in combination with other concepts . To extract the style code from a given concept , they replace the training texts with prompts of the form : “A painting in the style of S ∗ ” . For the TI baseline , we applied the original Textual In - version for the first concept ( from which we wish to take the structure ) , and for the appearance concept we used their proposed style extraction application described above . This results in a pair of textual tokens S TI 1 , S TI 2 that represent each concept . We explicitly combine these tokens in a sen - tence , providing the desired mixing description ( e . g . “ S TI 1 in the style of S TI 2 ” and use it to generate an image . Voynov et al . [ 45 ] propose an extended textual condition - ing space for a diffusion model that can be used to control style and geometry disentanglement . The main idea is to provide each diffusion UNet cross - attention layer with an independent textual prompt . The authors notice that low - resolution UNet layers are commonly responsible for geo - metrical attributes , while high - resolution input and output layers are responsible for style - related attributes . For the task of style mixing , given a pair of objects , the method performs two independent Textual Inversions to this extended prompt space ( called XTI in the paper ) . Then , the low - resolution layers are provided with the inversion of the object that donors the shape , and the high - resolution layers are provided with the inversions of the object that donors the appearance . For the comparison to XTI [ 45 ] , we use their recom - mended hyperparameters . We apply two independent Tex - tual Inversions to the extended prompt space , which brings the pair of textual tokens S XTI 1 , S XTI 2 . To use the geome - try from S XTI 1 and appearance of S XTI 2 , we provided the prompt “a photo of S XTI 1 ” to the deeper ( low - res ) layers and “a photo of S XTI 2 ” to the shallower ( high - res ) UNet layers . We tried to combine the concepts using different layers split to achieve the best possible performance . From Figure 13 , we can see that these baselines fail to combine the very specific aspects of the source objects . Textual Inversion commonly blends the attributes , while XTI is able to transfer either the whole creature’s appear - ance , or texture only , failing to extract only the shape . In contrast , using our approach it is possible to pick the two distinct aspects and combine them naturally to depict a new concept . 13 Figure 13 . Comparison of blending specific aspects of concepts . Top row are the source objects and a description of which aspect is taken . Second row are the results of blending the chosen concepts using Textual Inversion [ 11 ] . Third row are the results of blending with Extended Textual Inversion [ 45 ] ( XTI ) when only bottleneck layers are provided with the left object . Fourth row are results of XTI where a wider range of the low - resolution layers are provided with the left object . Last two rows are images generated with our proposed approach . C . Ablation and Analysis C . 1 . Binary Tree Our choice to use a binary tree stems from two main rea - sons : ( 1 ) complexity , and ( 2 ) consistency . It is technically possible with our method to build a tree with more than two children per node , however , we believe this may add redundant complexity to the method . The use of more than two children will result in a longer running time in each level since we will have to split more nodes . Additionally , after two levels we will receive 12 aspects ( for a simple scenario of three nodes ) , which may be difficult to visualize and navigate . In terms of consistency , we observe that when optimiz - ing more than two nodes at a time , the chance of receiving inconsistent nodes increases . Often , two nodes will be con - sistent , and the third node is inconsistent or may depict ir - relevant concepts such as background . We visually demon - strate this in Figure 14 , on the “red teapot” object . We present the aspects obtained from the optimal seed after 200 iterations , for the case of two nodes ( left ) and for the case of three nodes ( right ) . As can be seen , the sub - concept in v 3 for the 3 nodes optimization does not appear to be consistent or comprehensible , and therefore is not useful in achieving our goal of extracting aspects from the parent concept . For the two - node case , however , the aspects obtained provide a coherent concept in addition to decomposing the object well . At the bottom of Figure 14 , we show the concepts de - picted by two other seeds . The results show that when using two nodes , the rest of the seeds also produce relatively con - sistent results , compared to the case where three nodes were used . The following quantitative experiment further confirms this observation . We obtained 52 trees for our set of 13 ob - 14 2 Nodes 3 Nodes “v1 v2” “v1 v2 v3” “v1” “v2” “v3” Other Seeds Other Seeds “v1” “v2” Original Concept Figure 14 . Comparison of optimizing for two child nodes ( left ) v . s . three child nodes ( right ) . Using three nodes increases the chance of arriving at inconsistent or irrelevant concepts . At the top we show the results of the chosen seed among the four seeds , and below we demonstrate how two of the other seeds provide similar results , demonstrating that this trend is general . jects ( using four seeds for each object as described in the main paper ) . Each tree is a 3 - node tree with one level , re - sulting in a total number of 156 nodes . For each node , we then applied our CLIP - based consis - tency test to determine its average consistency score . For each tree , we sorted the 3 nodes according to their consis - tency and received a set of { v 1 , v 2 , v 3 } , where v 1 is the most consistent node of the three , v 2 is the second most con - sistent and v 3 is the least consistent . We then average the scores of { v 1 , v 2 , v 3 } across all objects . Results are shown in Figure 16 ; observe that there is a noticeable consistency gap between the top 2 nodes ( achieving average scores of 0 . 804 , 0 . 742 ) and the third node who achieved a score of 0 . 633 . This indicates that , on average , two of the three nodes are consistent , while the last may contain incoher - ent information . This experiment correlates well with our visual observation ( as demonstrated in Figure 14 ) . C . 2 . Timestep Sampling As discussed in Section 4 . 1 of the main paper , we use the timestep sampling approach proposed in ReVersion [ 19 ] , fa - " v1 v2 “ " v1“ " v2“ OriginalConcept Uniform Timestep Skewed Timestep ( Ours ) OriginalConcept Uniform Timestep Skewed Timestep ( Ours ) " v1 v2 “ " v1“ " v2“ “ v1 v2 “ “ v1“ “ v2“ " v1 v2 “ " v1“ " v2“ Figure 15 . Timestep sampling approach ablation . We show the effect of using a uniform sampling ( left ) , compared to using the sampling approach from ReVersion [ 19 ] , which favor larger values of t . voring larger t values . This sampling approach plays a sig - nificant role in the success of our method , as demonstrated in Figure 15 . The left side of Figure 15 shows the results obtained when using a uniform sampling approach ( which is the more common approach in LDM - based optimization ) , the right side shows the results obtained when using the sam - pling method we selected from ReVersion . In both cases , the results were obtained after 500 iter - ations with the same seed and settings . As can be seen , the uniform timestep sampling approach negatively affects both reconstruction quality ( see “v1 v2” ) and decomposi - tion quality , where for example for the cat sculpture the as - pect depicted in “v1” is unrelated to the original concept . 15 v 1 v 2 v 3 Node 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 A v g . C o n s i s t e n c y S c o r e 0 . 804 0 . 742 0 . 633 Figure 16 . Average CLIP - based consistency scores of the three child nodes sorted from left to right . These scores were measured for 3 - node trees obtained for 13 objects using 4 seeds per object . Observe that , on average , the third node tend to encode incoherent information , which encourages us to choose a binary tree structure . " v1 v2 " " v1 " " v2 " " v1 v2 " " v1 " " v2 " Seed1 Seed2 " v1 v2 " " v1 " " v2 " " v1 v2 " " v1 " " v2 " Seed3 Seed4 OriginalConcept Figure 17 . Results of four different seeds after 200 steps . The best seed is marked in green . C . 3 . Consistency Test In this section we provide examples and details regarding our proposed CLIP - based consistency test presented in Sec - tion 4 . 1 in the main paper . First , we visually demonstrate the effect of using k = 4 seeds in each run . We observe that 4 seeds are generally enough for most of the concepts , and " v1 v2 " " v1 " " v2 " " v1 v2 " " v1 " " v2 " Seed1 Seed2 " v1 v2 " " v1 " " v2 " " v1 v2 " " v1 " " v2 " Seed3 Seed4 OriginalConcept Figure 18 . Results of four different seeds after 200 steps . The best seed is marked in green . in most cases also 2 seeds may be good enough . However we do note that the variability in results among the different seeds can be quite meaningful in some cases . We demon - strate this in Figures 17 and 18 , where we show the original concept on top , along with the random set of images gener - ated for each nodes in each of the seeds . The seed that was chosen using our CLIP - based consis - tency measurement is marked in green . While the results depicted in Figure 17 were reasonable for most of the seeds , in Figure 18 we can see that seed1 and seed2 are failure cases , where in seed 1 the concept depicted in v 2 is incon - sistent and not interpretable , and in seed4 we have a case of one dominant node ( v 1 ) . Additionally , we provide an illustration to better clarify the significance of the consistency scores and their relation - ship to the patterns observed visually in the trees . In Fig - ure 19 we show examples of two trees with different char - acteristics . Next to each node are the self - consistency score ( marked in red ) and the siblings consistency score ( marked in green ) . The first score measures the degree to which the images depicted in a specific node are consistent withing themselves . The second score indicates the similarity be - tween sibling nodes . First , observe that the self consistency score for the root node ( 0 . 89 ) is the highest , since the images depicted in that node originated from the set provided by the user . This indicates the highest consistency score possible in our set - tings . In addition , we observe that the self consistency score across most nodes is relatively high and does not vary sig - nificantly as we go deeper in the tree . However , v 5 in the 16 0 . 89 0 . 79 0 . 76 0 . 85 0 . 75 0 . 77 0 . 77 0 . 60 0 . 60 0 . 64 0 . 64 0 . 72 0 . 72 0 . 89 0 . 77 0 . 78 0 . 73 0 . 79 0 . 66 0 . 79 0 . 61 0 . 61 0 . 67 0 . 67 0 . 70 0 . 70 Self consistency Sibling consistency 𝑣 ! 𝑣 " 𝑣 # 𝑣 $ 𝑣 % 𝑣 & 𝑣 ! 𝑣 " 𝑣 # 𝑣 $ 𝑣 % 𝑣 & Figure 19 . An illustration of two trees with different characteristics . The original training set is depicted at the root of the trees . Next to each node we present its self - consistency score ( in red ) and the consistency score of that node with its brother node ( in green ) . The scores were obtained using our CLIP - based consistency measurement described in the main paper . Figure 20 . Example of a question we presented in the aspect rele - vance survey . right tree obtained a self consistency score of 0 . 66 , which is relatively low , and in our scale it means that the set is not considered consistent . Considering that this node is not consistent with itself , it is obvious that it is not consistent with its sibling node , which is why , in such cases , we can ignore the score ob - tained in green for that node in this discussion . We now examine the scores in green , which indicate con - sistency across siblings . First , note that in both trees , the consistency across siblings is low ( 0 . 6 and 0 . 61 ) in the first level , suggesting that a good separation has been achieved . However , at the second level we can see that this score gen - erally increased , indicating that the quality of separation de - creases as we go deeper in the tree . Additionally , the sibling similarity correlates well with the visual information , with v 3 , v 4 in the left tree and v 3 , v 4 and v 5 , v 6 in the right tree appearing to be more consistent than the other pairs . It is important to note that in these cases , when the con - sistency among siblings is high , or when one node is incon - sistent within itself , the split will be stopped at this particu - lar level . Table 1 . Average self consistency ( left ) and sibling consistency ( right ) scores . The scores were obtained for 13 trees . Node Self Cons . Avg . Level1 Node Sibling Cons . Avg . Level2 v1 0 . 790 0 . 792 v1 0 . 580 0 . 58 v2 0 . 794 v2 0 . 580 v3 0 . 781 0 . 783 v3 0 . 711 0 . 69 v4 0 . 780 v4 0 . 711 v5 0 . 768 v5 0 . 669 v6 0 . 803 v6 0 . 669 In order to confirm this observation , we measured these scores for the set of 13 trees that were used for the other evaluations . For each node , we calculated the self consis - tency score as well as the sibling consistency score , and averaged these scores across the trees . The results are pre - sented in Table 1 . In both levels , the average self consis - tency score is high , while the average siblings consistency score increased with the transition from the first to the sec - ond level , indicating that the splits are less distinct on aver - age . The reason for this is that as we go deeper into the tree , the components are becoming increasingly simple , making it more challenging to further split them . 17 0 . 65 0 . 77 A B 0 . 69 0 . 86 0 . 84 0 . 67 A B 94 . 4 % 97 . 2 % 0 . 62 0 . 82 A B 100 % 91 . 7 % A B Figure 21 . Examples of questions asked in the consistency evalu - ation survey . On the left we show the results in percentages , indi - cating which answer was selected by the majority of people . C . 4 . Perceptual Study The following section provides additional details regard - ing our perceptual study described in section 5 . 2 of the main paper . For the consistency evaluation , we collected answers from 35 participants . Participants were presented with 15 pairs of random image sets , and they were asked to deter - mine which set in each pair is more consistent . In order to handle cases where the sets are similar , we have also added two options to choose from - “Both sets are equally con - sistent” , and “Both sets are equally not consistent” . Fig - ure 21 contains a few examples of the survey questions . On the left of each set , we also present the results in percent - ages , indicating which answer was selected by the majority of people . In the aspect relevance experiment , we collected answers from 35 participants and asked each participant 15 questions . Figure 20 provides an example of the questions . The question were obtained from 5 chosen objects , shown at the top of Figure 20 . D . Additional Qualitative Results In Figures 22 and 23 we show more examples of inter - tree combinations . At the top part of Figures 24 to 31 we show examples of trees on various objects . At the bottom part of Figures 24 to 27 and in Figures 32 and 33 we show visual examples of intra tree combinations . At the bottom part of Figures 24 , 28 and 29 and in Fig - ures 34 to 39 and 40 we show examples of text based gen - eration . 18 “v 5 _ a v 1 _ b ” “v 1 _ b ” “v 5 _ a ” “v 2 _ a v 1 _ b ” “v 1 _ b ” “v 2 _ a ” “v 3 _ a v 1 _ b ” “v 1 _ b ” “v 3 _ a ” “v 1 _ a v 2 _ b ” “v 2 _ b ” “v 1 _ a ” Figure 22 . More examples of inter - tree combinations . 19 “v 3 _ a v 1 _ b ” “v 1 _ b ” “v 3 _ a ” “v 3 _ a v 1 _ b ” “v 1 _ b ” “v 3 _ a ” “v 2 _ a v 1 _ b ” “v 1 _ b ” “v 2 _ a ” “v 2 _ a v 1 _ b ” “v 1 _ b ” “v 2 _ a ” Figure 23 . More examples of inter - tree combinations . 20 Original Image “v1” “v2” “v3” “v4” “v 1 v 3 ” Combining different aspects “A cat made of v3” “v1 v2” Figure 24 . Exploration tree for the “round bird” object . At the bottom we show examples of possible intra - tree combinations and text - based generation . 21 Original Image “v1 v2” “v1” “v2” “v3” “v4” “v 1 v 3 ” Combining different aspects “v 1 v 4 ” Figure 25 . Exploration tree for the “scary mug” object . At the bottom we show examples of possible intra - tree combinations . 22 Original Image “v1 v2” “v1” “v2” “v3” “v4” “v 1 v 3 ” Combining different aspects “v 1 v 4 ” Figure 26 . Exploration tree for the “Buddha sculpture” object . At the bottom we show examples of possible intra - tree combinations . 23 Original Image “v1 v2” “v1” “v2” “v3” “v4” “v 1 v 3 ” Combining different aspects “ v 1 v 4 ” Figure 27 . Exploration tree for the “colorful teapot” object . At the bottom we show examples of possible intra - tree combinations . 24 Original Image “v1” “v2” “v1 v2” “A house made of v2” Text based editing “A cat made of v2” “A dress made of v2” Figure 28 . Exploration tree for the “wooden pot” object . At the bottom we show examples of possible text - based generation . 25 Original Image “v1” “v2” “A dress made of v2” Text based editing “v1 v2” “A cat made of v2” “A chair made of v2” Figure 29 . Exploration tree for the “elephant” object . At the bottom we show examples of possible text - based generation . 26 Original Image “v1” “v2” “v1 v2” Original Image “v1” “v2” “v1 v2” Figure 30 . Exploration trees for the “green doll” and the “turtle” objects . 27 Original Image “v1” “v2” Original Image “v1” “v2” “v1 v2” Figure 31 . Exploration trees for the “Girona mug” and the “physics mug” . 28 Figure 32 . More examples of intra - tree combinations for the “cat sculpture” object . The full original tree is shown in the main paper . 29 Figure 33 . More examples of intra - tree combinations for the “cat sculpture” object . The full original tree is shown in the main paper . 30 Original Image “A house made of v1” “v1” “A dress made of v1” “A chair made of v1” “A cat made of v1” Figure 34 . More examples of text based generation for the “cat sculpture” object . The full original tree is shown in the main paper . 31 Original Image “A house made of v2” “v2” “A dress made of v2” “A chair made of v2” “A cat made of v2” Figure 35 . More examples of text based generation for the “cat sculpture” object . The full original tree is shown in the main paper . 32 Original Image “A house made of v3” “v3” “A dress made of v3” “A chair made of v3” “A cat made of v3” Figure 36 . More examples of text based generation for the “cat sculpture” object . The full original tree is shown in the main paper . 33 Original Image “A house made of v1” “v1” “A dress made of v1” “A chair made of v1” “A cat made of v1” Figure 37 . More examples of text based generation for the “wooden saucer bear” object . The full original tree is shown in the main paper . 34 Original Image “A house made of v2” “v2” “A dress made of v2” “A chair made of v2” “A cat made of v2” Figure 38 . More examples of text based generation for the “wooden saucer bear” object . The full original tree is shown in the main paper . 35 Original Image “A house made of v6” “v6” “A dress made of v6” “A chair made of v6” “A cat made of v6” Figure 39 . More examples of text based generation for the “wooden saucer bear” object . The full original tree is shown in the main paper . 36 Original Image “A house made of v1” “v1” “A dress made of v1” “A chair made of v1” “A cat made of v1” Figure 40 . More examples of text based generation for the “Buddha sculpture” object . 37