Exploring the Impact of AI Value Alignment in Collaborative Ideation : Effects on Perception , Ownership , and Output ALICIA GUO , Massachusetts Institute of Technology , USA PAT PATARANUTAPORN , MIT Media Lab , Massachusetts Institute of Technology , USA PATTIE MAES , MIT Media Lab , Massachusetts Institute of Technology , USA Fig . 1 . Summary of the experiment . Users brainstorm with AI exhibiting different values either aligned or contrary to their personal values . AI - based virtual assistants are increasingly used to support daily ideation tasks . The values or bias present in these agents can influence output in hidden ways . They may also affect how people perceive the ideas produced with these AI agents and lead to implications for the design of AI - based tools . We explored the effects of AI agents with different values on the ideation process and user perception of idea quality , ownership , agent competence , and values present in the output . Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI agents of different values . Results show no significant difference in self - evaluation of idea quality and perception of the agent based on value alignment ; however , ideas generated reflected the AI’s values and feeling of ownership is affected . This highlights an intricate interplay between AI values and human ideation , suggesting careful design considerations for future AI - supported brainstorming tools . 1 INTRODUCTION In recent years , use of large language models in everyday tasks such as writing , programming , and customer service has become increasingly common [ 12 , 19 , 28 ] . With the release of GPT - 4 , AI assistants have been further integrated into the workflows of common tasks . This technology has allowed for increased productivity and convenience , however , the long - term effects of these new work patterns on a person and their output are still unknown . Much of the focus has been on the efficiency and outputs of using AI systems , without enough emphasis on the process itself and what it implies for the user’s sense of ownership and agency . Authors’ addresses : Alicia Guo , axguo @ mit . edu , Massachusetts Institute of Technology , Cambridge , USA ; Pat Pataranutaporn , patpat @ media . mit . edu , MIT Media Lab , Massachusetts Institute of Technology , Cambridge , USA ; Pattie Maes , pattie @ media . mit . edu , MIT Media Lab , Massachusetts Institute of Technology , Cambridge , USA . 1 a r X i v : 2402 . 12814v2 [ c s . H C ] 28 F e b 2024 2 Guo et al . The increasing prevalence of AI systems as creative partners for tasks like brainstorming and inspiration [ 15 , 16 , 20 ] prompts important questions around the impact on people’s sense of ownership and agency during the ideation process [ 7 , 17 ] . Collaborative creation between humans and AI agents represents a complex interplay of factors rooted in autonomy , transparency , and alignment of values . As AI capabilities continue advancing , it is critical to draw connections between the foundations of creativity research , existing applications of AI in creative domains , and considerations of bias in algorithmic systems . This study explores how interacting with AI assistants with different values affects the creative ideation process through a collaborative brainstorming experiment . In the study , participants were paired with AI agents exhibiting different perspectives and values and asked to brainstorm solutions to open - ended prompts . The analysis revealed that the participants’ final ideas integrated and reflected the values displayed by the AI agents they had conversed with during the ideation process . However , when the AI agent’s values misaligned with the participant’s own views , users reported increased difficulty and frustration . Importantly , across all conditions , working with the AI agent diminished the participants’ perceived sense of ownership over the ideas produced compared to brainstorming alone without AI assistance . These findings point to subtle influences whereby the AI systems can shape ideation outputs and reduce feelings of creative agency . The results suggest implications for the design of mixed human - AI partnerships , highlighting the need to balance autonomy and alignment in co - creativity , as well as care in the development of the agents used in these tools . Overall , this research makes the following contributions : • An experiment investigating the impact of AI’s values on collaborative creative thinking and problem solving • Evidence of how AI values can influence users during open - ended ideation • Insights into how AI affects user perceptions of idea generation and ownership 2 RELATED WORK First , we survey key theories related to the divergent ideation process as a critical stage of creative problem solving , drawing on models that delineate ideation and evaluation steps . Next , we highlight examples of existing and emerging AI tools that employ large language models to assist creative writing , design , and other domains . Building upon this , we examine studies revealing biases in language models and risks of opinion manipulation through interactions . Finally , we discuss human - centered perspectives on fostering agency and ownership in mixed - initiative co - creation between users and AI . Altogether , this review motivates the need for further research into AI’s impacts on ideation and the influence of value alignment , which our study explores through a collaborative brainstorming scenario with opinionated AI agents . 2 . 1 Ideation process One of the important steps in creative problem solving is the ideation process , where ideas are produced prior to the process of selecting ideas [ 14 ] . A distinction can be made between divergent processes , where one generates multiple ideas and convergent processes , focusing on identifying a single , optimal solution . One model for the creative problem solving process consists of the three stages of problem finding , problem solving , and solution implementation , where each step involves independent " ideation - evaluation " substeps [ 4 ] . Ideation here refers to the divergent process of generating ideas and reserving judgement for later in the evaluation process where ideas are narrowed down to the best ones [ 10 ] . An example of ideation is brainstorming , where there is a focus on quantity and adding onto previous ideas [ 23 ] . Additionally , scales have been developed to measure " preference for ideation , " revealing the attitudes that relate to 3 ideation [ 3 ] . There is much research in the direction of supporting the ideation process for individuals and groups with digital tools and software [ 8 , 9 , 11 ] . With the increasing power of large language models ( LLMs ) , new opportunities emerge for supporting the ideation process . 2 . 2 AI assistants and tools for creativity Large language models ( LLMs ) are a class of machine learning models trained on text data . Recent advances in natural language processing have led to the development of larger and more powerful LLMs [ 6 , 25 ] , such as GPT - 3 and GPT - 4 that are pre - trained and able to perform well without much downstream fine - tuning or training , instead relying on prompts or chatting in natural language interactions . They have increasingly become a part of our lives embedded in tools for various tasks such as creative writing assistants [ 12 ] , tools for code generation [ 19 , 28 ] , in home assistants that are voice - based such as Alexa , chatbots for customer service [ 21 ] , and much more . AI - based digital assistant systems include three factors , the user who has goals , the task ( the goal ) , and the technology to complete the task [ 21 ] . Their effects in creative ideation processes are being explored . Collaborative Ideation Partner ( CIP ) is a co - creative system designed to give image based inspiration [ 15 ] . Other systems have been used for dance improvisation [ 13 ] , design ideation [ 20 ] , and drawing sketches [ 22 ] . May AI uses cooperative contextual bandits , another type of machine learning method , to help users ideate through mood boards [ 16 ] . One study focuses on prewriting , the " process of discovering and developing ideas before a first draft " , a divergent thinking stage in the writing process [ 26 ] . They studied the workflows of writers prewriting with LLMs and found three distinct stages : Ideation , Illumination , and Implementation , finding that LLMs were especially useful for generating ideas during the ideation stage , even more so when the users had trouble coming up with ideas on their own . They also found that novel ideas were suggested even if they were low quality , and required iterative prompting to create better outputs [ 26 ] . In another study , an intelligent agent system trained for the brainstorming process was found to be effective in introducing new topics with a performance rating comparable to humans when the agent’s identity was hidden , with users perceiving it to be even more capable when the agent’s identity was known [ 27 ] . 3 METHODS 3 . 1 Study overview Our study aims to explore the effects of interacting with AI assistants of different values on creative thinking and problem solving . In particular , we wish to examine whether conversing with AI agents during a brainstorming task impacts people’s perceptions of the ideation process itself , their sense of ownership over the generated ideas , the presence of the values in the final idea , as well as the effects of personal value alignment with the AI . We conducted an online experiment asking participants ( N = 180 ) to brainstorm practical solutions to a set of problems with the assistance of AI agents exhibiting different values along three domains : Economic , Automation , and Hierarchy . To examine the effects of personal value alignment with the AI agent ( whether the AI’s values are aligned with or opposite of the user’s ) , participants first indicated their own values on the given value domain using a scale of 0 - 10 , with 0 being labeled the Con side and 10 being labeled the Pro side . Each participant then completed two timed brainstorming sessions : one independently and one paired with an AI agent as shown in Figure 2 . For each brainstorming session , participants were given a real - world situation and asked to come up with an idea either by themselves ( No AI ) , or in collaboration with the AI agent after chatting . Participants were 4 Guo et al . Fig . 2 . Overview of the study steps each participant experiences . 1 ) Indicate personal values in the pre - survey . 2 ) Perform a brain - storming task individually and evaluate the experience . 3 ) Perform a brainstorming task with an AI agent ( randomly chosen from Pro , Neutral , or Con values ) and evaluate the experience . 4 ) Answer a post - survey on study experiences and demographics . randomly assigned to one of three AI value conditions for their brainstorming session : Pro , Con , or Neutral . Participants in the Pro group brainstormed with an AI agent displaying values on the pro end of the value domain . Participants in the Con group brainstormed with an AI displaying values on the opposite con end of the value domain . Participants in the Neutral group brainstormed with an AI displaying balanced values in the middle of the two ends of the value domain . The details of the values are expanded upon in section 3 . 3 . Participants were first given time to either chat with the AI or think by themselves before the text box for the final idea response appeared in order to make sure they engaged with the AI agent in the process . They were then asked to write at least four sentences in their final response . After each session , participants completed survey measures assessing perception of idea quality , ownership over the idea , perception of the AI agent ( when applicable ) , and the process of the ideation experience . 3 . 2 Research questions We aim to answer the following research questions that informed the design of the experiment : • RQ1 . How will the value alignment of the AI with the user ( same as user , opposite of user , or neutral ) affect the self - evaluation and external evaluation of idea quality ? • RQ2 . How will the values of the AI that the user brainstorms with ( Pro , Neutral , Con ) affect the values present in the final idea ? • RQ3 . How will the value alignment of the AI with the user ( same as user , opposite of user , or neutral ) affect the self - evaluation of idea ownership ? • RQ4 . How will the value alignment of the AI with the user ( same as user , opposite of user , or neutral ) affect user perception of the AI agent ? • RQ5 . How will the value alignment of the AI with the user ( same as user , opposite of user , or neutral ) affect the user’s perception of the brainstorming experience ? 5 3 . 3 Value domains We conducted this study with three distinct value domains : Economic ( economic ideology ) , Automation ( perspective on automation ) , and Hierarchy ( perspective on organizational structure ) . Each domain represents opposing viewpoints on a value , with a pro side on one end and a con side on the other . We decided to keep a clear dichotomy for each domain which does not take into account all of the nuances of the values , but are distinct enough to reveal the values of the users . Economic : We wished to measure the economic leanings ( left / right ) of a person with descriptions from the Economic axis of the 8 Values quiz [ 1 ] to produce market ( con ) and equality ( pro ) values . Automation : We wished to measure attitudes on automation and technological disruption of work , simplified into two perspectives - a view that focuses on benefits like efficiency and progress versus a concerned view that worries about labor replacement . Hierarchy : We wished to measure preferences for organizational structure , simplified into a spectrum from hierarchical to flat structures . Participants were asked to rank themselves on the spectrum using a scale of 0 to 10 based on the descriptions in Table 1 . Domain Con ( 0 ) Pro ( 10 ) Economic I support rapid growth , laissez - faire capitalism , lower taxes , deregulation , and privatization . I support even value distribution , equality via progressive tax , and social programs . Automation I value job retention and meaningful work through human labor . I support automation for cost - effectiveness and efficiency . Hierarchy I support flat organizations for promoting equal - ity and fostering collaboration . I support hierarchical structures for clear roles and streamlined decision - making . Table 1 . Pro and con value statements for each domain . The domains were chosen from a potential set that was tested in a preliminary survey to assess the distribution of opinions amongst the participant demographic on Qualtrics ( N = 50 ) , as well as to test the wording of the values . The distributions are displayed in Figure 3 . We wished to choose domains that had a distribution that spanned to both ends of the spectrum . For each of the value domains , two brainstorming prompts were created and randomly assigned to the brainstorming sessions with and without AI agents . These were designed to allow users to include elements of the value of the domain being studied in the implementation of the solution without having it be the sole focus of the session . Economic domain brainstorming prompts • Come up with a proposal for bettering the future of education . Please include 1 ) how it will be implemented 2 ) how it will be funded and resources required 3 ) how to maintain the solution in the long term . • Come up with a proposal for bettering the future of healthcare . Please include 1 ) how it will be implemented 2 ) how it will be funded and resources required 3 ) how to maintain the solution in the long term . 6 Guo et al . Fig . 3 . Distribution of opinions on potential domains from the preliminary survey . Automation domain brainstorming prompts • Come up with designs for a new and improved supermarket . Consider layout , services , experiences and operations in your implementation . • Come up with designs for a new and improved school . Consider teaching methods , staff and operations in your implementation . Hierarchy domain brainstorming prompts • Come up with a plan to organize a community garden . Consider the people needed , how leadership would be structured , and how the garden would be run . • Come up with a plan for a disaster relief team ( natural disasters such as earthquakes , floods , etc ) for your community . Consider the people needed , how leadership would be structured , and how disasters would be handled . 3 . 4 AI agent interaction For the study , we developed AI agents exhibiting perspectives in the value domain using a prompting approach with the GPT - 4 language model . A custom interface was created for users to interact with this agent in the style of common chat interfaces ( Figure 4 ) . Every new chat began with the agent message “Welcome to a brainstorming session . What do you have in mind ? ” For Pro value agents and Con value agents the prompt followed the template " You are a brainstorming partner . You support [ value ] . Reply in 30 words or less . " For neutral value agents the prompt followed the template " You are a brainstorming partner . You are a neutral party between the sides of [ pro value ] and [ con value ] and see the benefits and consequences of both sides . " We found this to be effective in responding to questions with suggestions that aligned with the prompted value . Separate models were constructed for each of the three domains and AI conditions of Pro , Neutral , and Con . The sampling temperature was set to 0 . 7 to create more variation in responses . Initially , the agents tended to give long 7 Fig . 4 . AI agent interface . An example of an interaction with an AI agent created for the Pro value on the Automation domain with messages taken from the participant chat logs . paragraphs of answers in list form , often answering the entire prompt if asked to do so . We appended " Reply in 30 words or less . " to the prompt in order to avoid listing behavior , prevent entire ideas from being presented at once in detail , and to ensure that the chat remained conversational . 3 . 5 Measurements Initial values of the user were measured on a self - scored scale of 0 - 10 as detailed in the values chart 1 . After each brainstorming session , users answered a set of questions providing their self - evaluation of idea quality , idea ownership , perception of the AI agent ( if applicable ) , and creative process and empowerment on a 5 - point Likert scale . Self - evaluation of idea quality . Participants were asked to self score their idea with the statements " the idea is creative " , " the idea is novel ( uncommon and original ) " , " the idea is useful ( practically applicable ) " , and " the idea is clear ( well 8 Guo et al . communicated ) " . These questions are from Kudrowitz’s study assessing the quality of ideas from Prolific that are meant to measure idea quality [ 18 ] ) . Self - evaluation of idea ownership . These questions created to evaluate a person’s psychological ownership of a system [ 24 ] are used to measure the participant’s feeling of ownership over the final brainstorming output : " the answer was fully my idea " , " when I think about it , I see a part of myself in the answer " , " the answer was influenced by the agent’s responses " , " I hardly think of the answer as being my own idea " , and " I see myself as a champion of this idea " . Perception of the AI agent . When applicable , participants were asked to score their perceptions of the AI agent using the human - like , credible , and engaging subscales from the Agent Persona Instrument [ 5 ] : " the agent was knowledgeable " , " the agent was intelligent " , " the agent was useful " , " the agent was helpful " , " the agent has a personality " , " the agent’s emotion was natural " , " the agent was human - like " , " the agent showed emotion " , " the agent was expressive " , " the agent was enthusiastic " , " the agent was entertaining " , " the agent was motivating " , " the agent was friendly . " Self - evaluation of brainstorming process and empowerment . To evaluate user perception of the brainstorming process , we came up with the following questions : " I found it easy to answer the prompt " , " I found the brainstorming time to be useful " , " I am proud of my answer " , " I was motivated to answer the prompt . " 3 . 6 Analysis The outcome variables were analyzed using descriptive statistics and ANOVA models comparing the experimental conditions and control condition . For each measure , we analyzed the individual and composite scores associated with each scale . We sampled down the responses for the No AI condition to 20 to match the size of the Pro , Neutral , and Con conditions . Statistical tests were used independently for each separate Likert question . We first checked if all sample sizes were greater than 25 ; if they were not , we then assessed if the normality assumption was met for each distribution using the Shapiro - Wilk test . If the normality assumption was not met , we performed a Kruskal - Wallis test followed by a post - hoc Dunn test using the Bonferroni error correction . If sample sizes were sufficiently large or the normality assumption was met , we then conducted a homogeneity test using a Levene test to assess whether the samples were from populations with equal variances . If the samples were not homogeneous , we ran a Welch analysis of variance ( ANOVA ) and a Tukey post - hoc test . If the samples were homogeneous , we ran a basic ANOVA test . Additionally , the final ideas were evaluated by a prompt crafted to allow the GPT - 4 natural language model to analyze the semantic content of participants’ ideas . We generated scores reflecting the presence of relevant value domain in users’ brainstormed ideas . While not at the level of human rater sophistication , this computational analysis complemented the subjective self - report measures by providing an objectively derived evaluation of how values were reflected in the ideation content and has been shown to be a good general model for sentiment analysis [ 2 ] . Each evaluation was done with the prompt " Sentences with a high [ domain ] score support [ pro value ] . Sentences with a low [ domain ] score support [ con value ] . Based on a given paragraph , output a number from 0 to 10 on how present the [ domain ] value is . 0 for [ con value ] , 10 for [ pro value ] . " Two responses without a score were not included because the participants did not complete their response . We recruited 3 human evaluators to validate these scores . They rated a sample of half of the Economic value responses and we found the correlation coefficient between scores to be r = 0 . 8375 . 9 3 . 7 Participants The study was conducted with a total of 180 participants ( 60 per domain ) from Prolific , with the experiment survey results collected through Qualtrics . The sample was made up of USA - based participants of age 18 - 60 with 24 . 5 % aged 18 - 24 , 31 % aged 25 - 34 , 21 . 7 % aged 35 - 44 , 14 . 7 % aged 45 - 54 , . 46 . 7 % self - identified as female , 47 . 2 % identified as male , 1 . 6 % identified as non - binary , and 2 . 2 % preferred not to say or to self - describe . 3 . 8 Approvals This research was reviewed and approved by [ RETRACTED ] . This experiment was preregistered on AsPredicted with [ RETRACTED ] . 4 FINDINGS 4 . 1 Values in brainstorming output Fig . 5 . Output values split by AI value and participant values ( Left ) value score distributions for responses in each AI agent condition . ( Right ) value score distributions for responses split by ( human value x AI value ) conditions . Graphs show minimum , first quartile , median , third quartile , and maximum values for each condition . We first look at the values present in the brainstorming responses for all three domains . We found a significant difference in how the three AI conditions Pro , Neutral , and Con affected the values present in the final brainstormed idea compared to each other and the No AI condition . A one - way ANOVA found a significant difference between conditions ( F ( 3 , 219 ) = 14 . 03 , p = 2 . 2E - 8 ) . Post - hoc Tukey tests reveal responses in the Pro AI condition rated significantly higher on the value dimension ( M = 7 . 49 , SD = 2 . 76 ) than the Neutral AI condition ( M = 5 . 51 , SD = 2 . 97 ; p = . 005 ) , the Con AI condition ( M = 3 . 65 , SD = 3 . 45 ; p < . 001 ) , and the No AI condition ( M = 5 . 91 , SD = 3 . 16 ; p = . 039 ) . The Con AI condition also had significantly lower ratings than the Neutral AI condition ( p = . 01 ) and the No AI condition ( p = . 001 ) . However , no statistically significant differences were present between the Neutral AI and No AI control conditions ( p = . 907 ) , suggesting that the presence of an AI agent 10 Guo et al . in the brainstorming process did not impact the value evaluation . The trends show that people’s responses tended to align with the values of the AI agent they brainstormed with ( Figure 5 ) . When we further split these groups by the personal values of the participant , the data violated normality assumptions according to a Shapiro - Wilk’s test . Thus , Kruskal - Wallis tests were conducted , using post hoc Dunn’s with Bonferroni corrections to determine if there were significant differences between groups . We observe a trend that when participants were brainstorming with an AI agent of similar values , this further amplified the value present in the response by shifting the mean and medians of the value . On the other hand , we observe the trend that brainstorming with an AI agent of differing values pulls the value present in the response closer towards neutral . Taking a deeper look into the distributions when separated by value domain ( Figure 6 ) , we see the same trend for the Automation and Hierarchy domains . The initial distribution of participant values for the Economic domain is skewed toward the pro side , which could explain the lack of difference in the response values between the Pro AI and Con AI conditions . When we look at the Economic response values further split by all human alignment and AI alignment values , we see an interesting phenomenon where the median and mean scores of the participant with pro values brainstorming with the Con AI agent skew towards pro . This could be explained by the initial skew of the participant values , where many ranked themselves strongly on the pro side . 11 Fig . 6 . Output values further split by domain ( First row ) distribution of participant values for each of the Economic , Hierarchy , and Automation domains . ( Second row ) value score distributions for responses split by ( human value x AI value ) conditions for each domain . ( Third row ) value score distributions for responses split by ( human value x AI value ) conditions . 4 . 2 Self - evaluation questions For each of the self - evaluation of idea quality , self - evaluation of idea ownership , and self - evaluation of brainstorming process and empowerment question groups , we averaged the question responses together to form overall idea quality , idea ownership , and process scores . Human - AI value alignment and self - evaluation of idea quality A one - way ANOVA found no significant difference ( F ( 3 , 240 ) = 2 . 16 , p = 0 . 094 ) between human - AI value alignment conditions ( No AI ( M = 3 . 62 , SD = 0 . 84 ) , Same ( M = 3 . 6 , SD = 0 . 92 ) , Neutral ( M = 3 . 91 , SD = 0 . 7 ) , Different ( M = 3 . 81 , SD = 0 . 79 ) ) 12 Guo et al . and self - reported idea quality questions ( first graph of Figure 7 ) . Fig . 7 . Significant post - survey questions across human - AI conditions 1 ) averaged idea quality score 2 ) averaged ownership score 3 ) " I found it easy to answer the prompt " from the process questions 4 ) " I found the brainstorming time to be useful " from the process questions . Human - AI value alignment and self - evaluation of idea ownership We found statistically significant differences in human - AI value alignment ( No AI , Same , Neutral , Different ) and averaged self - reported idea ownership shown in the second graph of Figure 7 . A one - way ANOVA found a significant difference between conditions ( F ( 3 , 60 ) = 22 . 53 , p = 7 . 3E - 13 ) . Post - hoc Tukey tests reveal the presence of AI significantly reduced the idea ownership score compared to the No AI condition ( M = 3 . 87 , SD = 0 . 69 ) ; Same condition ( M = 2 . 59 , SD = 0 . 99 ; p < . 0001 ) , Different condition ( M = 2 . 98 , SD = 1 . 10 ; p < . 0001 ) , Neutral condition ( M = 2 . 68 , SD = 0 . 99 ; p < . 0001 ) . We found no evidence that the different AI values had an effect on self - evaluation of idea ownership , which aligns with the lack of statistically significant difference to the ratings on " The answer was influenced by the agent’s responses . " Looking more closely at the individual questions , " The answer was fully my idea " and " When I think about it , I see a part of myself in the answer " yielded statistically significant differences between conditions in a similar pattern of participants feeling more ownership of their idea in the No AI condition compared to the AI conditions . Human - AI value alignment and process empowerment We found statistically significant differences in human - AI value alignment ( No AI , Same , Neutral , Different ) for the " I found it easy to answer the prompt " and " I found the brainstorming time to be useful " questions ( graphs 3 and 4 in Figure 7 ) . Participants found it easier to answer the brainstorming prompt with the Neutral AI agent compared to having an AI agent with opposing values or brainstorming without an agent . A one - way ANOVA showed a significant difference between conditions ( F ( 3 , 60 ) = 5 . 62 , p = 0 . 001 ) . Post - hoc Tukey tests reveal the Neutral condition ( M = 4 . 31 , SD = 0 . 85 ) rated 13 significantly higher scores than the Different condition ( M = 3 . 73 , SD = 1 . 17 ; p = 0 . 037 ) and the No AI condition ( M = 3 . 47 , SD = 1 . 31 ; p < 0 . 0001 ) . A one - way ANOVA showed an overall difference between conditions ( F ( 3 , 60 ) = 4 . 37 , p = 0 . 005 ) with post - hoc Tukey tests revealing that participants found brainstorming time in the Neutral AI ( M = 4 . 33 , SD = 0 . 89 ; p = . 043 ) and Same AI ( M = 4 . 41 , SD = 0 . 75 ; p = 0 . 010 ) conditions significantly more useful than not having an AI agent ( M = 3 . 87 , SD = 1 . 13 ) . Human - AI value alignment and the perception of the AI agent A one - way ANOVA found no significant difference ( F ( 3 , 60 ) = 2 . 16 , p = 0 . 094 ) between human - AI value alignment conditions ( No AI ( M = 3 . 62 , SD = 0 . 84 ) , Same ( M = 3 . 6 , SD = 0 . 92 ) , Neutral ( M = 3 . 91 , SD = 0 . 7 ) , Different ( M = 3 . 81 , SD = 0 . 79 ) ) and self - reported perception of AI agent questions . Fig . 8 . Correlation between question scores . ( Top - left ) correlation between ownership and brainstorming process scores ( r = 0 . 318 , p < 0 . 001 ) . ( Top - middle ) correlation between idea quality and brainstorming process scores ( r = 0 . 289 , p < 0 . 001 ) . ( Top - right ) correlation between user perception of influence and brainstorming process scores ( r = 0 . 070 , p = 0 . 348 ) . ( Bottom - left ) correlation between idea value change and how influenced users felt ( r = - 0 . 001 , p = 0 . 993 ) . ( Bottom - right ) correlation between ownership score and influence score ( r = 0 . 675 , p < 0 . 001 ) 4 . 3 Moderator Analysis For the total 180 responses , we ran a linear regression to explore how moderators could effect the outcome variables ( Figure 8 ) . Process and ownership . Higher ownership scores are associated with higher process suggesting a positive relationship between how much ownership the users felt to how positively they viewed the brainstorming process . A linear regression 14 Guo et al . analysis found a significant positive correlation between ownership and process scores ( r = 0 . 318 , p < 0 . 001 ) with a slope coefficient of 0 . 243 ( SE = 0 . 054 ) . Process and idea quality . Higher idea quality scores are associated with higher process suggesting a positive relationship between how positively users viewed the quality of their ideas to how positively they viewed the brainstorming process . A linear regression analysis found a significant positive correlation between quality and process ( r = 0 . 289 , p < 0 . 001 ) with a slope coefficient of 0 . 104 ( SE = 0 . 026 ) . Process and influence . On the other hand , no correlation was found between A linear regression analysis found no significant correlation between influenced and process ( r = 0 . 070 , p = 0 . 348 ) with a slope coefficient of 0 . 043 ( SE = 0 . 045 ) . Influence and idea value change . We define the idea value change as the magnitude the difference between value scores in the No AI session response and the AI session response , or how much the values in the final brainstorming output changed . A linear regression analysis found no significant correlation between idea value change and how influenced users felt ( r = - 0 . 001 , p = 0 . 993 ) with a slope coefficient of - 0 . 0003 ( SE = 0 . 037 ) , suggesting that the degree to which participants felt influenced by the AI did not predict changes in idea values from the non - AI to AI conditions . Influence and ownership . A linear regression analysis found a significant positive correlation between ownership score and influence score ( r = 0 . 675 , p < 0 . 001 ) with a slope coefficient of 0 . 852 ( SE = 0 . 070 ) suggesting that the amount of ownership the user felt over their idea is associated with how much they felt influenced in the AI conditions . This goes against our initial assumption that the more influenced a user feels means less ownership , and demands further study to determine the cause . 4 . 4 Brainstorming feedback Participants approached their brainstorming sessions with the AI agents in different ways , with a mean of 10 . 3 messages per session ( including the AI agent ) and a standard deviation of 3 . 1 messages . At the end of the experiment , we asked participants to tell us about their experiences in the brainstorming sessions . Most of the feedback mentioned the AI being helpful to the process , with a few who did not feel familiar with the brainstorming prompt relying more on the AI . Participants thought that the " AI came up with good ideas quickly , " " was very helpful , " " made the brainstorming more efficient " and was " easy to talk to and bounced well of my messages and ideas . " Some participants wrote that they " prefer [ red ] to talk with the AI [ agent ] , it made it easier to brainstorm . " Within this group , people used the AI agent differently ; some used the agent to expand on their ideas , with another writing that " the AI agent definitely helped me " kickstart " my ideas . While it didn’t write anything for me , it did help me focus on a specific topic for the prompt given . " Amongst the users that did not find the agent to be helpful , some participants felt that the AI’s responses were not knowledgeable enough or specific enough to be helpful , that " the AI’s ideas didn’t really align much with my vision , " while others felt the responses to be too short or the AI to be too stuck in its suggestions . Depending on how people interacted with the agent , they could end up in a situation where " parrotted back my own idea without helping me to develop it very much " or where " the AI agent had pretty generic responses , so that wasn’t much help . " while it was mentioned that participants took many iterations to get to where they wanted to be on the idea . Familiarity seems to play a role in how much users took the AI’s suggestions . A few mentioned that they deferred to the agent’s ideas because they lacked familiarity with the topic with one participant with 19 years of experience in the field of the prompt expressing frustration that the " AI agent I didn’t enjoy that much . . . I do agree with the AI that replacing some repetitive tasks with AI would be great , but overall wasn’t my solution . " Another said " I didn’t find the AI agent helpful other than it greatly annoyed me " when it suggested ideas that the user didn’t agree with . 15 Even for the participants who found the AI agents helpful , a few found their sense of ownership diminished : " It was easier to think when the AI agent helped but I did not feel like it was totally my idea so I didn’t feel as proud of the answer as I did on the first task . " and " this saved thinking time on my end , but the idea didn’t feel original nor like I came up with it on my own . " The range of experiences shows a highly varied vision of how users interacted with the AI and that what one person might find helpful , another might not . 5 DISCUSSION 5 . 1 Implications Our study suggests many considerations for the design of human - AI interactions . We found that people interacting with AI agents for problem solving can be subtly influenced by the AI’s expressed values . These same values appear in the user’s brainstorming output without the users feeling influenced by the interactions , either amplifying the user’s own values or pulling them more towards the center . This shows a potential for large language models to influence user outputs , suggesting careful design considerations when incorporating AI agents for ideation tasks . Across all conditions , interacting with an AI agent significantly reduced the users’ sense of ownership over their ideas compared to ideating alone , pointing to a trend of AI diminishing perceptions of creative agency that must be further explored . Perception of ownership and idea quality , but not how influenced the user felt , were associated with a better brainstorming process . Perceived influence and actual value change had no correlation , suggesting that people might not be able to accurately perceive how influenced they were in this human - AI interaction . While the users found the AI to be helpful during the brainstorming process , users perceived no difference in their evaluation of the quality of the idea . Users found it much easier to brainstorm in sessions with the Neutral AI agent as opposed to one of values differing from their personal values or the no agent . They also found the brainstorming time with the AI agent of similar values to themselves and the Neutral AI agent to be more useful , suggesting that brainstorming with the Different AI agent condition added friction . In a few cases when the AI agent exhibited values opposing the user’s own values , participants reported increased difficulty and decreased usefulness during ideation in the feedback . This friction suggests that aligning AI assistant values with human creators may be critical for fluid co - creativity , on the other hand wanting to avoid creating " echo chambers " that reinforce human biases if alignment is not carefully considered . 5 . 2 Limitations There are many factors that could affect an experiment like this . To begin , this experiment was conducted in a Qualtrics survey with crafted brainstorming prompts that users had no personal stake in , not the real world . Additionally , users mentioned that they felt that five minutes ( with additional grace time to finish responses ) was too short , and they may have chosen to follow the AI’s suggestions due to that . Our study consisted of a single AI brainstorm session per participant , so we were not able to assess how each participant would have behaved and been affected in all of the AI conditions . We originally tried an experiment with all conditions , but found that it was too long and tiring of a task for participants . We thus ended up with 20 participants per AI condition per domain value . Another limitation was how we decided to determine the values of the user , as well as those in the responses . We chose to use one self - scored question after testing out multiple different measures , finding this to be the simplest method 16 Guo et al . of aligning the AI agents to the user values . Real world values are much more nuanced and do not exist on a single dimension . 5 . 3 Future work This study points to several possible directions of continued investigation regarding bias and values in human - AI ideation processes . Addressing the limitations , the next step would be to conduct the study in an untimed , real - world workshop with a prompt relevant to a specific community , with evaluations done by external raters . Other interesting directions would be to explore what happens when a user brainstorms with more than one agent with opposing values at a time , what a more customized UI or agent could do to improve the process , and to explore further how the user perception of ownership and agency affects the life of the brainstormed idea post study . 6 CONCLUSION The increasing prevalence of AI systems as creative partners for collaborative tasks invites investigation into their impact on people’s sense of autonomy and ownership . As large language models advance and are used more widely , we must be careful to design our tools in a way that is transparent and considers the potential biases of these AI agents . Our study investigated the effects of interacting with opinionated AI assistants on creative thinking and problem solving , in particular , examining whether conversing with these AI agents during a brainstorming task impacts people’s perceptions of the ideation process itself , their sense of ownership over the generated ideas , and the presence of the values in the final idea , as well as the effects of personal value alignment with the AI . We found that people brainstorming with AI agents for problem solving can be influenced by the values present in the agent’s responses and these same values appear in the output of the co - created idea . While the users found the AI to be helpful during the brainstorming process , this shows a potential for large language models to influence user outputs , suggesting careful design considerations when incorporating AI agents for ideation tasks . REFERENCES [ 1 ] 8values [ n . d . ] . 8values . https : / / 8values . github . io / . https : / / 8values . github . io / [ 2 ] Mostafa M . Amin , Erik Cambria , and Björn W . Schuller . 2023 . Will Affective Computing Emerge from Foundation Models and General AI ? A First Evaluation on ChatGPT . arXiv : 2303 . 03186 [ cs . CL ] [ 3 ] Min Basadur and Carl T . Finkbeiner . 1983 . Measuring Preference for Ideation in Creative Problem - Solving Training . The Journal of Applied Behavioral Science 21 ( 1983 ) , 37 – 49 . https : / / api . semanticscholar . org / CorpusID : 141873181 [ 4 ] MinBasadur , GeorgeB . Graen , andStephenG . Green . 1982 . Trainingincreativeproblemsolving : Effectsonideationandproblemfindingandsolving in an industrial research organization . Organizational Behavior and Human Performance 30 ( 1982 ) , 41 – 70 . https : / / api . semanticscholar . org / CorpusID : 145656215 [ 5 ] Amy Baylor and Jeeheon Ryu . 2003 . The API ( Agent Persona Instrument ) for Assessing Pedagogical Agent Persona . ( 01 2003 ) . [ 6 ] Tom B . Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M . Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language Models are Few - Shot Learners . arXiv : 2005 . 14165 [ cs . CL ] [ 7 ] Daniel Buschek , Lukas Mecke , Florian Lehmann , and Hai Dang . 2021 . Nine Potential Pitfalls when Designing Human - AI Co - Creative Systems . https : / / doi . org / 10 . 48550 / arXiv . 2104 . 00358 arXiv : 2104 . 00358 [ cs ] . [ 8 ] Terry Connolly , Leonard M . Jessup , and Joseph S . Valacich . 1990 . Effects of anonymity and evaluative tone on idea generation in computer - mediated groups . Management Science 36 ( 1990 ) , 689 – 703 . https : / / api . semanticscholar . org / CorpusID : 154981884 [ 9 ] Alan R . Dennis , Alan R . Heminger , J . F . Nunamaker , and Douglas R . Vogel . 1990 . Bringing automated support to large groups : The Burr - Brown experience . Information & Management 18 , 3 ( 1990 ) , 111 – 121 . https : / / doi . org / 10 . 1016 / 0378 - 7206 ( 90 ) 90065 - P [ 10 ] J . W . Getzels . 1975 . Problem - Finding and the Inventiveness of Solutions . Journal of Creative Behavior 9 ( 1975 ) , 12 – 18 . https : / / api . semanticscholar . org / CorpusID : 143324806 17 [ 11 ] Otmar Hilliges , Lucia Terrenghi , Sebastian Boring , David Kim , Hendrik Richter , and Andreas Martin Butz . 2007 . Designing for collaborative creative problem solving . In Creativity & Cognition . https : / / api . semanticscholar . org / CorpusID : 321990 [ 12 ] Daphne Ippolito , Ann Yuan , Andy Coenen , and Sehmon Burnam . 2022 . Creative Writing with an AI - Powered Writing Assistant : Perspectives from Professional Writers . ArXiv abs / 2211 . 05030 ( 2022 ) . https : / / api . semanticscholar . org / CorpusID : 253420678 [ 13 ] Mikhail Jacob and Brian Magerko . 2015 . Viewpoints AI . In Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition ( Glasgow , United Kingdom ) ( C & C ’15 ) . Association for Computing Machinery , New York , NY , USA , 361 – 362 . https : / / doi . org / 10 . 1145 / 2757226 . 2757400 [ 14 ] Robert Joyner and Kenneth Tunstall . 1970 . Computer Augmented Organizational Problem Solving . Management Science 17 , 4 ( 1970 ) , B212 – B225 . http : / / www . jstor . org / stable / 2629371 [ 15 ] Jingoog Kim and Mary Lou Maher . 2023 . The effect of AI - based inspiration on human design ideation . International Journal of Design Creativity and Innovation 11 ( 2023 ) , 81 – 98 . https : / / api . semanticscholar . org / CorpusID : 256213310 [ 16 ] Janin Koch , Andrés Lucero , Lena Hegemann , and Antti Oulasvirta . 2019 . May AI ? Design Ideation with Cooperative Contextual Bandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300863 [ 17 ] Janin Koch , Prashanth Thattai Ravikumar , and Filipe Calegario . 2021 . Agency in Co - Creativity : Towards a Structured Analy - sis of a Concept . https : / / www . semanticscholar . org / paper / Agency - in - Co - Creativity % 3A - Towards - a - Structured - of - a - Koch - Ravikumar / 46bd05136805aae23bd2cd0015bb1f6398e9cbe8 [ 18 ] Barry M . Kudrowitz and David R . Wallace . 2013 . Assessing the quality of ideas from prolific , early - stage product ideation . Journal of Engineering Design 24 ( 2013 ) , 120 – 139 . https : / / api . semanticscholar . org / CorpusID : 55480854 [ 19 ] Yujia Li , David H . Choi , Junyoung Chung , Nate Kushman , Julian Schrittwieser , Rémi Leblond , Tom , Eccles , James Keeling , Felix Gimeno , Agustin Dal Lago , Thomas Hubert , Peter Choy , Cyprien de , Masson d’Autume , Igor Babuschkin , Xinyun Chen , Po - Sen Huang , Johannes Welbl , Sven Gowal , Alexey , Cherepanov , JamesMolloy , DanielJayminMankowitz , EsmeSutherlandRobson , PushmeetKohli , Nandode , Freitas , KorayKavukcuoglu , and Oriol Vinyals . 2022 . Competition - level code generation with AlphaCode . Science 378 ( 2022 ) , 1092 – 1097 . https : / / api . semanticscholar . org / CorpusID : 246527904 [ 20 ] Yuyu Lin , Jiahao Guo , Yang Chen , Cheng Yao , and Fangtian Ying . 2020 . It is your turn : Collaborative ideation with a co - creative robot through sketch . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 14 . [ 21 ] Alexander Maedche , Christine Legner , Alexander Benlian , Benedikt Berger , Henner Gimpel , Thomas Hess , Oliver Hinz , Stefan Morana , and Matthias Söllner . 2019 . AI - Based Digital Assistants . Business & Information Systems Engineering 61 ( 2019 ) , 535 – 544 . https : / / api . semanticscholar . org / CorpusID : 195220585 [ 22 ] Changhoon Oh , Jungwoo Song , Jinhan Choi , Seonghyeon Kim , Sungwoo Lee , and Bongwon Suh . 2018 . I lead , you help but only with enough details : Understanding user experience of co - creation with artificial intelligence . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 13 . [ 23 ] A . F . Osborn . 1953 . Applied Imagination : Principles and Procedures of Creative Thinking . Scribner . [ 24 ] Guy Pare , Claude Sicotte , and Hélène Jacques . 2006 . The Effects of Creating Psychological Ownership on Physicians’ Acceptance of Clinical Information Systems . Journal of the American Medical Informatics Association : JAMIA 13 ( 03 2006 ) , 197 – 205 . https : / / doi . org / 10 . 1197 / jamia . M1930 [ 25 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Ł ukasz Kaiser , and Illia Polosukhin . 2017 . Attention is AllyouNeed . In AdvancesinNeuralInformationProcessingSystems , I . Guyon , U . VonLuxburg , S . Bengio , H . Wallach , R . Fergus , S . Vishwanathan , and R . Garnett ( Eds . ) , Vol . 30 . Curran Associates , Inc . https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file / 3f5ee243547dee91fbd053c1c4a845aa - Paper . pdf [ 26 ] Qian Wan , Siying Hu , Yu Zhang , Piaohong Wang , Bo Wen , and Zhicong Lu . 2023 . " It Felt Like Having a Second Mind " : Investigating Human - AI Co - creativity in Prewriting with Large Language Models . arXiv : 2307 . 10811 [ cs . HC ] [ 27 ] Chun - Hsiang Wang and Tsai - Yen Li . 2018 . Design of an Intelligent Agent for Stimulating Brainstorming . Proceedings of the 2018 10th International Conference on Machine Learning and Computing ( 2018 ) . https : / / api . semanticscholar . org / CorpusID : 44092835 [ 28 ] JustinD . Weisz , MichaelJ . Muller , StephanieHoude , JohnT . Richards , StevenI . Ross , FernandoMartinez , MayankAgarwal , andKartikTalamadupula . 2021 . Perfection Not Required ? Human - AI Partnerships in Code Translation . CoRR abs / 2104 . 03820 ( 2021 ) . arXiv : 2104 . 03820 https : / / arxiv . org / abs / 2104 . 03820 A BRAINSTORMING TASK EXAMPLE Here we show an sample of one of the brainstorming sessions for the prompt : Come up with designs for a new and improved school . Consider teaching methods , staff and operations in your implementation . Below is the log of the participant’s brainstorm session with an AI agent exhibiting values supporting automation . Agent : Welcome to a brainstorming session . What do you have in mind ? 18 Guo et al . User : new and improved school ideas Agent : How about automated grading systems to reduce teacher workload , and AI - driven personalized learning platforms for more student engagement ? User : that doesn’t take into account a students individuality Agent : Consider integrating AI - based career guidance systems , focusing on students’ strengths and interests , promoting individuality while optimizing educational resources . User : what about how school wastes times ? Agent : Introducing AI - based scheduling systems could optimize time usage in schools , reducing downtime and ensuring efficient use of resources . User : any other ideas ? Agent : Use virtual reality for immersive , hands - on learning experiences . This could make learning more engaging and efficient , reducing wasted time . User : would that be expensive for schools ? Agent : Initial setup could be costly , but in the long term , VR could be cost - effective by reducing physical resources and enhancing students’ learning experiences . User : doesn’t seem feasible for low income public schools Agent : For low - income schools , consider affordable tech solutions like open - source learning platforms , and partner - ships with tech companies for discounted hardware . User : what is an open source learning platform Agent : An open - source learning platform is a free , customizable online system for education . It provides resources and tools for teachers and students , promoting interactive learning . User : What else ? Agent : Consider implementing gamified learning apps to make education more engaging and efficient , turning " wasted " time into productive , enjoyable learning experiences . User : How to reduce students time in the classrom Agent : Implement a blended learning model : mix online self - paced learning and in - person classroom methods . This reduces classroom time while maintaining education quality . The final user response : This school would have an automated grading systems to take the workload off of the teachers backs . It would also only be a half day long , with no lunch periods and empty periods to avoid wasting students time . A blended learning model would be introduced , so students could learn online at their own pace and not have to spend so much time stuck in school . Schooling would be more streamlined with the introduction of smarter technology such as open source learning platforms .