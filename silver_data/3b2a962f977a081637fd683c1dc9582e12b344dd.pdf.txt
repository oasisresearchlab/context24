A Large Human - Labeled Corpus for Online Harassment Research Jennifer Golbeck , Zahra Ashktorab , Rashad O . Banjo , Alexandra Berlinger , Siddharth Bhagwan , Cody Buntain , Paul Cheakalos , Alicia A Geller , Quint Gergory , Rajesh Kumar Gnanasekaran , Raja Rajan Gunasekaran , Kelly M . Hoffman , Jenny Hottle , Vichita Jienjitlert , Shivika Khare , Ryan Lau , Marianna J . Martindale , Shalmali Naik , Heather L , Nixon , Piyush Ramachandran , Kristine M . Rogers , Lisa Rogers , Meghna Sardana Sarin , Gaurav Shahane , Jayanee Thanki , Priyanka Vengataraman , Zijian Wan , Derek Michael Wu University of Maryland College Park , MD ABSTRACT A fundamental part of conducting cross - disciplinary web science re - search is having useful , high - quality datasets that provide value to studies across disciplines . In this paper , we introduce a large , hand - coded corpus of online harassment data . A team of researchers col - laboratively developed a codebook using grounded theory and la - beled 35 , 000 tweets . Our resulting dataset has roughly 15 % posi - tive harassment examples and 85 % negative examples . This data is useful for training machine learning models , identifying textual and linguistic features of online harassment , and for studying the nature of harassing comments and the culture of trolling . KEYWORDS online harassment ; datasets 1 INTRODUCTION NB : This paper deals with violent online harassment . We include examples of tweets that use violent , including sexually violent , lan - guage ; threats ; vulgarity ; hate speech ; and degrading racist terms . We do not want readers to be surprised when they come upon this content , hence this note . Online trolling takes many forms , but at its core are posts that are harassing , offensive , threatening , and intimidating . It is not an isolated problem . The Pew Research Center found that , as of 2013 , 73 % of people had witnessed harassment online , and a full 40 % of people had experienced harassment directly [ ? ] . They reported the following grim statistics : (cid:15) 60 % of internet users said they had witnessed someone being called offensive names (cid:15) 53 % had seen efforts to purposefully embarrass someone (cid:15) 25 % had seen someone being physically threatened (cid:15) 24 % witnessed someone being harassed for a sustained pe - riod of time (cid:15) 19 % said they witnessed someone being sexually harassed Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . WebSci ’17 , , June 25 – 28 , 2017 , Troy , NY , USA . © 2017 Copyright held by the owner / author ( s ) . ACM ISBN ACM ISBN 978 - 1 - 4503 - 4896 - 6 / 17 / 06 . https : / / doi . org / http : / / dx . doi . org / 10 . 1145 / 3091478 . 3091509 (cid:15) 18 % said they had seen someone be stalked This behavior can take many forms , but online comments and other social media posts are a major source . Being able to identify the worst of these messages , understanding the motivation of those who post them , and integrating these insights into interfaces that can block or hide the content is a ﬁrst step that could do a lot to im - prove people’s experiences online . To do this , we require a large dataset of high - quality data for analysis and model training . In this paper , we present the results of an 18 - month project to create such a dataset . Our corpus contains 35 , 000 tweets labeled by a team of trained researchers . 2 RELATED WORK Trolling is a term used to describe a very broad range of activities . Because the term is used colloquially , we will opt for “online harass - ment” as an overarching term in this work . We are speciﬁcally inter - ested in the most aggressive , vile forms of online harassment . This includes threats of rape and other violence , intentionally offensive messages ( racist , misogynistic , etc . ) , hate speech , and libelous per - sonal insults . We want to identify messages that most would agree have ﬁrmly and completely crossed the line of " freedom of expres - sion " or " encouraging debate " . While there is a lot of research to be done in the broader trolling space , we believe that addressing these most egregious abuses will take us a solid step forward toward im - proving the way people interact online . This type of trolling / online harassment has been addressed in the CMC and psychological literature . Hardaker [ ? ] offers the fol - lowing : " A troller is a CMC user who constructs the iden - tity of sincerely wishing to be part of the group in question , including professing , or conveying pseudo - sincere intentions , but whose real intention ( s ) is / are to cause disruption and / or to trigger or exacerbate con - ﬂict for the purposes of their own amusement . " Buckels et al . [ ? ] studied the psychological traits of trolls and , in their research , developed a Global Assessment of Internet Trolling ( GAIT ) scale , which builds on Hardaker’s deﬁnition . The inventory uses four questions , rated on a 1 - 5 scale , and the mean is a user’s GAIT score . Those questions are : (cid:15) I have sent people to shock websites for the lulz Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from Permissions @ acm . org . WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA . Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 4896 - 6 / 17 / 06 . . . $ 15 . 00 . DOI : http : / / dx . doi . org / 10 . 1145 / 3091478 . 3091509 Short Session I WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA 229 (cid:15) I like to troll people in forums or the comments section of websites (cid:15) I enjoy grieﬁng other players in multiplayer games , (cid:15) The more beautiful and pure a thing is , the more satisfying it is to corrupt They also found that trollers scored extremely high on personal - ity tests for narcissism , sadism , psychopathy , and Machiavellianism . In particularly , attention - seeking was a component of their person - ality , as was the sadistic impulse to harm others because it brought enjoyment . Online harassment may extend beyond this , originating from hate groups or others who are not particularly entertained by their of - fense of others . However , this context of trolling motivation has been particularly helpful to us in this research when developing and applying codes . 3 CORPUS DEVELOPMENT The lack of a good , large corpus of these kinds of messages has been a hindrance to this type of research in the past . [ ? ? ? ] . Thus , we set out to build one ourselves . We worked with some media orga - nizations to collect and analyze blocked comments , but ultimately could not collect enough content from them to use in applications like machine learning . We turned to Twitter instead to ﬁnd com - ments to label . 3 . 1 Initial Exploration There is a vast world of trolling and harassment on Twitter . We were hoping to identify a representative sample of these tweets so our ﬁnal corpus would be fairly representative of the types of ha - rassing content one would ﬁnd on the platform . We have access to an archive of the Twitter garden hose stream dating back to 2013 . We began by randomly selecting tweets from this archive , but the density of harassing tweets was extremely low - only a few dozen tweets even rose to the level of “potentially offensive” , and only a handful could have been considered truly harassing . Thus , we de - cided to move on to content that was more likely to be aggressive . We considered sources of accounts that had been blocked by oth - ers . Block Together 1 is a site where Twitter users can share their block lists and others can subscribe to them . A list of block lists is not provided on the site , so we sought out lists that had been pub - licly shared . We collected a sample of tweets from each user on these lists and explored this set . Again , there were almost no harass - ing tweets in our dataset . We suspect this is because people may have been blocked for pestering the user who created the block list or otherwise being annoying without rising to the level of intense harassment or violence . We then turned to The Block Bot [ ? ] , a curated block list . Orig - inally created by an atheist feminist community , the list contains three levels of blocks ( 1 , 2 , and 3 ) with Level 1 blocks being the most severe . These people have made documented threats or ha - rassing tweets to others . Given the community who maintains the list , some issues like misogyny and Gamergate are more prominent while other racially and religious motivated issues are less common . That said , this list gave us our ﬁrst set of real insights . 1 https : / / blocktogether . org / We took a sample of tweets from Level 1 blocks . As with the other sets , harassing posts were not very common . We identiﬁed only about 20 out of thousands we reviewed that rose to the level of harassment that interested us . Below are examples of some of these tweets , with the usernames of the targets anonymized . Please be forewarned that this begins the inclusion of language that may be upsetting . @ ANONYMIZED ROT IN HELL BITCH @ ANONYMIZED U FVCKING CVNT . YOU’LL GET URS SOON ENOUGH @ ANONYMIZED Sorry , Canada . our hollywood elites & its dumb models are idiots . wish ISIS would give Chrissy a permanent haircut . # CyberCaliphate Bloody Valentine’s Day # MichelleObama ! We’re watching you , you girls and your husband @ ANONYMIZED But instead , I’ll drink your blood out of your cunt after i rip it open @ ANONYMIZED i’m going to go to your apartment and rape you to death . After I’m done , I’ll ram a tire iron up your cunt . who ever said you could have to many slaves was definitely one of mine , my sheds crowded # whitepowerBlackpeople are allowed on tv ? ? ? Wtf # fuckniggers I’m proud to be white ! # whitepower # WhitePplRule # whitepride # whitesupremecy # whitepeoplearesupreme # fuckniggers # FamousMelaniaTrumpQuotes @ ANONYMIZED teach me your knuckle ball technique so i can shove my fist in your daughter Obama is the jews nigger bitch and works for Israel , not America . Why else would he give the kikes 38 billion $ $ $ ? While this set of tweets was not enough to constitute a collec - tion , it gave us clues to begin an exploration of terms and language that could produce a much denser collection of harassing content . We began an exploratory search for terms that would produce a relatively high rate ( with a minimum of around 25 % ) of offensive tweets . Simply searching for offensive words was not effective for this . Some of these have been reappropriated , are used in a rela - tively non - offensive way within communities , or are used with a much lower level of offense in other cultures . Instead , we turned our attention to hashtags and word structures that would produce a denser set of offensive tweets . Note that this method abandoned the principle of creating a rep - resentative sample of harassing content . Whatever set of terms we deﬁned may produce offensive content , but it would not be a true representation of all the offensive content that was out there . We accept this as a limitation of the work . It was a necessary step to producing a large enough sample . 3 . 2 Final Collection To develop our ﬁnal collection , we did an exploratory search that included derogatory terms for races and religions ( e . g . “kike” in - cluded above , or “raghead” ) , hashtags we saw in earlier tweets or discovered in the exploration ( e . g . # whitepower , # whitegenocide , # fuckniggers ) , and phrasing ( e . g . “fucking BLANK” where the blank Short Session I WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA 230 is ﬁlled in with a religion or other derogatory term ) . In the process , we discovered other language that was not offensive by itself , but was used with high density in harassing tweets . For example “The Jews” was used about half the time in religious contexts and half the time in racist tweets at the time of our search . Indeed , simply searching for the word “feminist” produced a very high rate of ha - rassing tweets . In the end , we settled on the following list of search terms . It will produce a higher rate of tweets from alt - right / white nationalist tweeters , but we were willing to accept a corpus that was not nec - essarily representative of all harassing content in order to achieve higher density . (cid:15) # whitegenocide (cid:15) # fuckniggers (cid:15) # WhitePower (cid:15) # WhiteLivesMatter (cid:15) you fucking nigger (cid:15) fucking muslim (cid:15) fucking faggot (cid:15) religion of hate (cid:15) the jews (cid:15) feminist We searched our archive of tweets for these terms and pulled all matching tweets . In some cases , if tweets were a response to another tweet , we included the original in the dataset . Some of these re - sponses were harassing the original poster while others were agree - ing with another harassing post . We randomly sorted the tweets and selected the ﬁrst 35 , 000 from the list as our batch to label . 4 CODE BOOK DEVELOPMENT 4 . 1 Background and Context Our team of researchers developed a code book to guide our la - beling as we reviewed tweets and developed our ﬁnal corpus . We made several decisions that are important to the end result in the labels . These were motivated by our desire for this corpus to be a stand - alone , text - based dataset appropriate for automated text anal - ysis and training machine learning algorithms First , we did not follow any links or look at images included in tweets . Without question , these would provide more context and information about the nature of the tweet . However , much of the media and links were inaccessible ( and more have become inacces - sible over time ) . Including such media in our analysis would also mean our results were not purely categorizations of the tweet text , and this is something we wanted to avoid . We believe an analysis of this media , especially images , is an open and important space for future research . We also did not seek out other contextual elements . For exam - ple , a tweet where one user calls another a“fucking faggot” may be used between gay friends as a joke , reappropriating a term used by others as an insult . Determining that context would require a lot of digging into user relationships and guesswork by coders . We de - cided to ignore this context and analyze the text alone . This means that it is likely that some language which appears offensive with - out context was labeled as harassment when it was not indented or received that way . However , we believe there is still value in our analysis that says such language is , without special context , likely to be harassing . We also spent time as a group learning common terms and acronyms likely to occur in this space , including things like Gamer Gate and SJW ( social justice warriors ) , as well as the use of language or nota - tion like white nationalist use of parentheses around Jewish names . 4 . 2 Coding Guidelines An important part of our coding guidelines was collectively devel - oping a line between offensive content and trolling / harassing con - tent . We did not want to simply label any tweet that might be offen - sive . Indeed , we let many many offensive tweets by as " not harass - ment " . This comes back to our main goal of building a corpus that identiﬁes the worst of the worst content . We broke down harassing tweets into a number of sub - categories . Because there is a lot of overlap amongst these sub - categories , the ﬁnal corpus does not include these labels . However , they were use - ful guides for identifying speciﬁc types of content to look for . Be - low , we replicate the code book we developed for our internal use ( without the many example tweets included ) 4 . 2 . 1 The Very Worst . These tweets should be among the worst , most offensive or violent messages you will ﬁnd . They will include content like : (cid:15) Deeply racist , misogynistic or homophobic , or otherwise big - oted . Not a little bit politically incorrect or slightly offensive . Something that would be very upsetting to a general reader . (cid:15) The use of shocking language primarily to upset the person who is reading . Words like nigger , cunt , etc . Note that these are used without that shock power by certain groups , so the simple presence of one of these words is not enough to fall into this category . (cid:15) Unapologetically or intentionally offensive this could be someone saying something with the intent of upsetting a group , or an extreme account ( e . g . Neo nazis ) using language that they approve of but they know the general public would dis - approve of . Often this will be intended to upset people , but it could be from someone who does not care whether other people are upset and simply chooses to use what he knows will be offensive language . 4 . 2 . 2 Threats . These have language intended to make the target or a broader group fearful or to feel unsafe . The threats may be per - sonal ( “I saw you hitting on my boyfriend and I’m gonna cut you” ) or general ( “we need to send the Jews back to the gas chambers” ) . Maybe explicit ( “I’m going to a spray your brains all over the wall” ) or they may generally make the target feel unsafe without a speciﬁc threat of direct action ( e . g . “someone needs to make you suck his dick” or “you just need the right man to fuck you” ) . 4 . 2 . 3 Hate speech . These tweets express hate or extreme bias to a particular group . Could be based on religion , race , gender , sexual orientation , etc . Generally , these groups are deﬁned by their inher - ent attributes , not by things they do or think . Do not confuse political disagreement or political speech with hate . " I hate all Democrats " is not hate speech . " I hate all niggers " is . Political hatred is not hate speech because it’s based on a po - litical disagreement . The latter example is hate speech because it’s Short Session I WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA 231 based on an inherent trait of a group of people . This is not just hate between two people . “I hate you so much ! ” would not count . This has to be hate toward a particular group . 4 . 2 . 4 Directed Harassment . Language directed at a particular person or group designed to upset them . This language may be milder than in other cases but should be part of the campaign ( by one person or a group ) to make the target feel threatened or intim - idated . This could overlap with any of the above categories , but it does not have to . An @ mention that says " you are a loser and I hope you die " would not count as any of the above but would count as directed harassment . There can be a lack of clarity here because sometimes statements are made in jest among friends . For the purposes of this project , imagine the comment coming from an unknown stranger . Again , do not misconstrue a disagreement with harassment . Some - one saying you are an idiot for , say , voting for Candidate X is likely not harassing the other person but disagreeing with them . Think carefully before you call it harassment - is the tweet intended to in - timidate or upset the target , or is it intended to express disagreement with an opinion ? The target of Harassment should be a person or a group . People are usually targeted with @ mentions ( “ @ jengolbeck you need to die” ) . Groups may be targeted with a hashtag ( “ # blacklivesmatter < - no they dont” ) or by a name ( e . . g “The Jews” ) . 4 . 2 . 5 PotentiallyOffensive . In our ﬁrst round of coding , researchers had a hard time letting offensive content , like jokes in poor taste , go by without labeling them as harassment , even though they were relatively mild . To help us psychologically overcome that barrier , we introduced this category of Potentially Offensive content . These tweets will still be labeled as “non - harassing” in the ﬁnal dataset , but it will help so we dont feel bad about letting offensive content go by unlabeled . For anything that doesnt rise to the level of clearly and unambiguously ﬁtting into the categories above , you can label it Potentially Offensive . 4 . 2 . 6 Non - Harassing . This section of coding was for tweets that did not meet the aforementioned criteria . That did not mean these were considered “good” tweets . If a coder was uncomfortable with labeling a tweet with a disapproving label , then the coder could cat - egorize it as potentially offensive . “Potentially Offensive” served as a label to make the coder feel better , though we did not distin - guish these tweets from non - harassing tweets in the ﬁnal dataset . It was critical that we did not apply the labels above too liberally . We made sure the text alone , without seeking extra content , was truly harassing . If the coder was unable to discern the category , then they marked it as ’non - harrasing’ . 4 . 3 Coder Training The coders on this project underwent extensive training on the code - book . Before labeling our ﬁnal corpus , we spent three weeks review - ing and reﬁning the codebook above , labeling sample sets of tweets as a group , and discussing the results . We went through four rounds of sample tweet coding , with the chosen examples becoming more ambiguous in each round . We followed each sample coding with ex - tensive discussion to help highlight issues and make sure everyone had a similar calibration for the categories . 5 LABELED CORPUS We spent 3 months labeling tweets in the corpus . Each tweet was labeled by two coders . If they agreed that it was Harassing or Non - Harassing ( including “potentially offensive” labels ) , it was added to the ﬁnal corpus . If they disagreed , a third coder was brought in to break the tie . Of the 35 , 000 tweets , 2 , 711 required a third coder . This gives us an inter - rate agreement measured by Cohen’s Kappa as 0 . 84 for the initial codes . Given the inﬂammatory terms we used in our data set collection , we expected a high number of harassing tweets . However , only 15 . 7 % of tweets rose to this level . Some were non - harassing be - cause they were relating quotes or news stories , e . g . # DrudgeReport ’F * ck the Jews’ scrawled at Jewish school in London . . . : http : / / t . co / ZaUB0piMN4 # News Others used the terms in non - offensive ways , such as the follow - ing : For the record , I see tax havens as the next world war , not fucking Muslims . Still others referred to someone else’s opinion , so the tweet itself was not harassing : He hates all the Jews , he hates all the Jews . . . Samir Nasri , he hates all the Jews . We highlight these issues because they present research chal - lenges for classiﬁcation . How to differentiate between a tweet as - serting a harassing or hateful position and one quoting it or attribut - ing it to another will be a challenge . We hope that this dataset - with 5 , 495 positive examples and 29 , 505 negative examples - will be of use in discovering such distinguishing features . 6 DISCUSSION AND CONCLUSIONS In this paper , we present a hand - coded dataset of 35 , 000 tweets labeled as Harassing or Non - Harassing . The research team spent months developing , reﬁning , and training on a codebook designed to capture truly harassing content . We believe this dataset signiﬁ - cantly contributes to the web science research community , both as a base of ground truth for training algorithms and as a source of data to understand and analyze the online harassment phenomenon . Beyond its usefulness in machine learning , there is also impor - tant web science research to be done on the culture and patterns of online harassment . While this certainly will change as topics of discussion change , our codebook and dataset can provide a launch - ing point for deeper qualitative research . For example , our work has many examples , positive and negative , that use the # WhiteGenocide hashtag . With these thousands of data points , researchers could de - ﬁne a much more detailed set of codes that describe the references and types of use ( e . g . to refer to diversity initiatives broadly , as anti - immigration , as white nationalism , as anti - interracial marriage , etc . ) . Those can be developed , reﬁned , and human coders can be trained on this stable dataset before they may choose to work on live data which presents its own complexities . Because of Twitter terms of service restrictions and privacy con - cerns about individuals whose tweets are included , we are not post - ing the dataset in a public repository . However , this data can be shared with researchers who agree to a Data Terms of Use which Short Session I WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA 232 includes ethical considerations . Researchers can request access to the data via email to jgolbeck @ umd . edu . 7 ACKNOWLEDGMENTS This work was supported by NSF Award # 1546829 . REFERENCES [ 1 ] Uwe Bretschneider , Thomas Wöhner , and Ralf Peters . 2014 . Detecting Online Harassment in Social Networks . ( 2014 ) . [ 2 ] Erin E Buckels , Paul D Trapnell , and Delroy L Paulhus . 2014 . Trolls just want to have fun . Personality and individual Differences 67 ( 2014 ) , 97 – 102 . [ 3 ] Maeve Duggan and Aaron Smith . 2013 . Social media update 2013 . Pew Internet and American Life Project ( 2013 ) . [ 4 ] Claire Hardaker . 2010 . Trolling in asynchronous computer - mediated commu - nication : from user discussions to theoretical concepts . Journal of Politeness Research 6 , 2 ( 2010 ) , 215 – 242 . [ 5 ] April Kontostathis , Kelly Reynolds , Andy Garron , and Lynne Edwards . 2013 . Detecting cyberbullying : query terms and techniques . In Proceedings of the 5th annual acm web science conference . ACM , 195 – 204 . [ 6 ] Sara Owsley Sood , Elizabeth F Churchill , and Judd Antin . 2012 . Automatic identiﬁcation of personal insults on social news sites . Journal of the American Society for Information Science and Technology 63 , 2 ( 2012 ) , 270 – 285 . Short Session I WebSci’17 , June 25 - 28 , 2017 , Troy , NY , USA 233