a r X i v : 1 001 . 2733v2 [ c ond - m a t . s t a t - m e c h ] 2 J u l 2010 Universality of Zipf’s Law Bernat Corominas - Murtra 1 and Ricard V . Sol´e 1 , 2 , 3 1 ICREA - Complex Systems Lab , Universitat Pompeu Fabra , Parc de Recerca Biom ` edica de Barcelona ( PRBB ) Dr . Aiguader 80 , 08003 Barcelona , Spain 2 Santa Fe Institute , 1399 Hyde Park Road , New Mexico 87501 , USA 3 Institut de Biologia Evolutiva . CSIC - UPF . Passeig Maritim de la Barceloneta , 37 - 49 , 08003 Barcelona , Spain Zipf’s law is the most common statistical distribution displaying scaling behavior . Cities , popu - lations or ﬁrms are just examples of this seemingly universal law . Although many diﬀerent models have been proposed , no general theoretical explanation has been shown to exist for its universality . Here we show that Zipf’s law is , in fact , an inevitable outcome of a very general class of stochastic systems . Borrowing concepts from Algorithmic Information Theory , our derivation is based on the properties of the symbolic sequence obtained through successive observations over a system with an ubounded number of possible states . Speciﬁcally , we assume that the complexity of the description of the system provided by the sequence of observations is the one expected for a system evolving to a stable state between order and disorder . This result is obtained from a small set of mild , physically relevant assumptions . The general nature of our derivation and its model - free basis would explain the ubiquity of such a law in real systems . I . INTRODUCTION Scaling laws are common in both natural and artiﬁcial systems [ 1 ] . Their ubiquity and universality is one of the fundamental issues in statistical physics [ 2 – 4 ] . One of the most prominent examples of power law behavior is the so called Zipf’s law [ 5 – 7 ] . It was popularized by the linguist G . K . Zipf , who observed that it accounts for the frequency of words within written texts [ 5 , 8 ] . But this law is extremely common , [ 9 ] and has been found in the distribution of populations in city sizes [ 5 , 10 – 14 ] , ﬁrm sizes in industrial countries [ 15 ] , market ﬂuctuations [ 16 ] , money income [ 17 , 18 ] , Internet ﬁle sizes [ 19 ] or family names [ 20 ] . For instance , if we rank all the cities in a country from the largest ( in population size ) to the smallest , Zipf’s law states that the probability p ( s i ) that a given individual lives in the i - th most populated city ( i = 1 , . . . , n ) falls oﬀ as p ( s i ) = 1 Z i − γ , ( 1 ) with the exponent , γ ≈ 1 , and being Z the normalization constant , i . e . , Z =  X i ≤ n i − γ   . ( 2 ) Although systems exhibiting Zipf’s - like statistics are clearly diﬀerent in their constituent units , the nature of their interactions and intrinsic structure , most of them share a few essential commonalities . One is that they are stochastic , far from equilibrium systems changing in time , under mechanisms that prevent them to become homogeneous . Within the context of economic change , for example , wider varieties of goods and attraction for people are fueled by large developed areas . Increasing returns drive further growth and feedback between econ - omy and city sizes [ 21 – 23 ] . Moreover , the presence of a scaling law seems fairly robust through time : in spite of widespread political and social changes , the statisti - cal behavior of words in written texts , cities or ﬁrms has remained the same over decades or even centuries [ 5 , 7 , 15 , 23 , 24 ] . Such robustness is remarkable , given that it indicates a large insensitivity to multiple sources of external perturbation . In spite of their disparate na - ture , all seem to rapidly achieve the Zipf’s law regime and remain there . To account for the emergence and robustness of Zipf’s law , several mechanisms have been proposed , in - cluding auto - catalytic processes [ 25 – 27 ] , extinction dy - namics [ 28 , 29 ] , intermittency [ 30 , 31 ] , coherent noise [ 32 ] , coagulation - fragmentation processes [ 33 , 34 ] , self - organized criticality [ 35 ] , communicative conﬂicts [ 36 , 37 ] , random typewriting [ 38 , 46 ] , multiplicative dynam - ics [ 39 , 40 ] or stochastic processes in systems with inter - acting units with complex internal structure [ 41 ] . The diverse character of such mechanisms sharing a common scaling exponent strongly points towards the hypothesis that some fundamental property ( beyond a given speciﬁc dynamical mechanism ) is at work . Such a universal trend asks for a generic explanation , which should avoid the use of a particular set of rules . We address the problem from a very general , mechanism - free viewpoint ; by studying the statistical properties of the sequence of successive observations over the system . More precisely , our observations can be un - derstood as a sequence of symbols of a given alphabet ( depending on the nature of the system ) following some probability distribution . The elements of this alphabet can be coded in some way - for example , bits . From this conceptual starting point , we borrow concepts from algo - rithmic information theory ( AIT ) and propose a charac - terization of a wide family of stochastic systems , to which those systems displaying Zipf’s law would belong . Such a characterization imposes special features on the behav - ior of the entropy , whose study leads us to conclude that , under generic mathematical assumptions , Zipf’s law is 2 the only solution . The paper is organized as follows : In section II we brieﬂy introduce the concept of stochastic object as de - ﬁned within the context of AIT and how it helps to under - stand our problem . In section III we ﬁnd the asymptotic solutions of the equations derived from the characteri - zation provided in section II . Section IV discusses the relevance of the obtained results . II . ALGORITHMIC COMPLEXITY OF STOCHASTIC SYSTEMS The cornerstone of our argument is an abstract charac - terization of the sequence of observations made on a given system in terms of AIT [ 42 – 45 , 47 , 48 ] - see also [ 49 ] . The key quantity of such theory is the so - called Kolmogorov complexity , which is a conceptual precursor of statistical entropy , and an indicator of the complexity ( and pre - dictability ) of a dynamical system [ 50 – 52 ] . In a nutshell , let x be a symbolic string generated by the successive observations of the system S . Its Kolmogorov Complex - ity , K ( x ) is deﬁned as the length l ( π ∗ ) - in bits - of the shortest program π ∗ executed in a universal computer in order to reproduce x . This measure has been often used in statistical physics [ 53 – 55 ] particularly in the context of symbolic dynamics [ 51 ] . In this context , K is known to be maximal for completely disordered systems , whereas it takes intermediate values when some asymmetry on the probabilities of appearance of symbols emerges . Within the framework of statistical physics , a sequence of observations performed over a given system can be in - terpreted as a sequence of independent , identically dis - tributed random variables , where the speciﬁc outcomes of the observations are obtained according to a given prob - ability distribution . In mathematical terms , such a se - quence of observations deﬁnes a stochastic object . By deﬁnition , the Kolmogorov Complexity of a stochastic object , described by a binary string x = x 1 , . . , x m of length m , satisﬁes [ 56 ] : lim m →∞ K ( x ) m = µ ∈ ( 0 , 1 ] . ( 3 ) In other words , the binary representation of a stochastic object is linearly compressible . The case where µ = 1 refers to a completely random object , and the string is called incompressible . We can generalize the concept for non binary strings , whose elements belong to a given set Σ = { s 1 , . . , s n } , be - ing | Σ | = n . This is the case of a dice , for example , whose set of outcomes is Σ dice = { 1 , 2 , 3 , 4 , 5 , 6 } . If the behavior of the system is governed by the random variable X ( n ) , accordingly , the successive observations of our stochas - tic system deﬁne a sequence of independent , identically p n - distributed random variables X 1 ( n ) , . . . , X m ( n ) taking values over the set Σ . The so - called noiseless Coding the - orem [ 47 , 57 , 58 ] , establishes that the minimum length , ( in bits ) of the string needed to code the event s i , l ∗ ( s i ) , satisﬁes l ∗ ( s i ) = − log ( p n ( s i ) ) + O ( 1 ) . ( 4 ) ( Throughout the paper , log ≡ log 2 , unless the contrary is indicated ) . The average minimum length will corre - spond to the minimum length of the code , which is , by deﬁnition , the Kolmogorov complexity . Thus we obtain [ 47 , 59 ] : lim m →∞ K ( X 1 ( n ) , . . . , X m ( n ) ) m = X i ≤ n p n ( s i ) l ∗ ( s i ) = H ( X ( n ) ) + O ( 1 ) , ( 5 ) being H ( X ( n ) ) the Shannon or statistical entropy [ 47 , 57 , 58 ] , namely : H ( X ( n ) ) = − X i ≤ n p n ( s i ) log p n ( s i ) . The complete random case is obtained when , ∀ s i ∈ Σ p n ( s i ) = 1 / n leading to l ∗ ( s i ) = log n + O ( 1 ) . This indicates that we need ≈ log n bits to code any element from Σ . Therefore , the length in bits of the sequence of m successive observations will be approximately m · log n . the average minimum length of the code will be lower than log n . Using our previous result ( 3 ) for the binary case , it is not diﬃcult to see that : lim m →∞ K ( X 1 ( n ) , . . . , X m ( n ) ) m · log n = µ ; µ ∈ ( 0 , 1 ] . ( 6 ) By deﬁning h ( n ) as the normalized entropy as : h ( n ) ≡ H ( X ( n ) ) log n , ( 7 ) and from eq . ( 5 ) , we observe that eq . ( 6 ) can be rewritten as h ( n ) ≈ µ ; µ ∈ ( 0 , 1 ] . So far we have been concerned with the algorithmic characterization of stochastic systems for which the size of the conﬁguration space is static . However , we must diﬀerentiate the properties of the systems we want to characterize from a standard stochastic object such as the ones obtained by tossing a dice or a coin . They both gen - erate a bounded number of possible outcomes - namely , 6 and 2 - with an associated probability , whereas those sys - tems exhibiting power - laws lack an a priori constraint on the potential number of available outcomes . These sys - tems are open concerning the size - or dimensionality - of the conﬁguration space . Let X ( n ) be a random variable taking values on Σ , where | Σ | = n and with associated probability distribution p n , where ( without any loss of generality ) an ordering p n ( s 1 ) ≥ p n ( s 2 ) ≥ . . . ≥ p n ( s n ) ( 8 ) is assumed . At a given time , the system satisﬁes eq . ( 6 ) , since it is a stochastic object with a given number of avail - able states . However , we assume that the system changes 3 ( generally growing ) maintaining its basic statistical prop - erties stable [ 5 , 7 , 15 , 24 ] . Using eq . ( 5 ) , condition ( 6 ) is replaced by : lim n →∞ h ( n ) = µ ( 9 ) We can replace eq . ( 9 ) alternatively by the following statement : For any ǫ > 0 there exists n ∈ N such that , for any n ′ > n : | h ( n ′ ) − µ | < ǫ . ( 10 ) The main objective of the paper is to ﬁnd the expected distribution p n ( s i ) consistent with eq . ( 10 ) . The case µ = 0 would correspond to systems where , although growing in size , its complexity ( and thus , its statistical entropy ) is bounded or grows sublinearily with log n , a case studied in [ 51 ] . Here , we are interested in the in - termediate case , where µ ∈ ( 0 , 1 ) . This characterization would depict systems with some balance among ordering and disordering forces , and thereby displaying a dissipa - tion of statistical entropy proportional to the maximum entropy achievable for the system in equilibrium . There - fore , we will refer to the problem of ﬁnding solutions for eq . ( 9 ) as the entropy restriction problem . A computa - tional test for this result can be illustrated by the model results shown in ﬁg . ( 1 ) . The picture shows a spatial snap - shot of the local population densities of a model of urban growth displaying Zipf’s law [ 23 ] . The normalized en - tropy evolves towards a stationary value µ ≈ 0 . 65 consis - tently with our discussion . This is true in spite that this model exhibits wide ﬂuctuations due to its intermittent stochastic dynamics . III . EMERGENCE OF ZIPF’S LAW IN STOCHASTIC SYSTEMS As pointed out in [ 36 ] , the main diﬃculty we face in this kind of equations is that we are not dealing with an extremal problem , since our value of entropy is pre - viously ﬁxed and it is neither minimum nor maximum , in Jaynes’ sense [ 60 ] . Thus , classical variational meth - ods , which have been widely used with great success in statistical mechanics [ 60 – 63 ] , do not apply to our prob - lem - although recently it has been shown that variational approaches using Fisher information and physically rele - vant constraints lead to Power - laws whose exponent can be close to 1 [ 64 ] . We also must take into account that the particular properties of Zipf’s law create an addi - tional diﬃculty if the studied systems display , a priori , an unbounded number of possible states . Speciﬁcally , we refer to the non - existence of ﬁnite moments and normal - ization constant in the thermodynamical limit . However , as we shall see , these apparently undesirable properties will be the key to our derivation . 10 0 10 1 10 2 10 3 10 4 city rank 10 - 6 10 - 4 10 - 2 10 0 L o c a l d e n s it y 0 100 200 300 400 500 time 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 µ - 1 . 0 b c a FIG . 1 : ( Color online ) An example of the behavior of the normalized entropy for a multiplicative stochastic process ex - hibiting Zipf’s law . Here we use the model described in [ 23 ] using a 80 × 80 lattice where each node is described by a den - sity of population ρ ( i , j ) . The rules of the model are very simple : i ) At every time step , each node loses a fraction α of its contents , which is distributed among its four nearest neighbors . ii ) At time t + 1 the local population is multi - plied , with probability p , by a factor p − 1 . Furthermore , with probability 1 − p , the population of a node is set to zero . Ad - ditionally , at each step a random number η is added to every node . In this way , we avoid falling into an absorbing state ρ = 0 . Here we use 0 < η < 0 . 01 , α = 1 / 4 and p = 3 / 4 . This is an extremly simpliﬁed ( and yet successful ) model of urban population dynamics . A snapshot ( for t = 500 ) is shown in ( a ) where we can appreciate the wide range of local densities , following Zipf’s law ( b ) . If we plot the evolution of the nor - malized entropy µ over time ( averaged over 10 2 replicas ) we observe a convergence towards a stationary value µ ≈ 0 . 65 . A . Properties of the entropies of a power law Let us brieﬂy summarize the properties of the entropies of power - law distributed systems , which will be used to derive the main results of this work ( For details , see Ap - pendix A ) . Such properties are intimately linked with the behavior of the Riemann Zeta function , ζ ( γ ) [ 65 ] : ζ ( γ ) = ∞ X k = 1 1 k γ . ( 11 ) In the real line , this function is deﬁned in the interval γ ∈ ( 1 , ∞ ) , displaying a singularity for γ → 1 + . Now , let us suppose that the system contains n states and the probability to ﬁnd the i - th most likely states decay as a power - law , i . e . , p n ( s i ) ∝ i − γ . For the sake of simplicity , we will refer to its associated entropy as H ( n , γ ) and to its normalized counterpart as h ( n , γ ) , i . e . : h ( n , γ ) = 1 log n γ Z n X i = 1 log i i γ + log Z ! . ( 12 ) The most basic properties concern the global behavior of H ( n , γ ) . It is straightforward to check that H ( n , γ ) 4 is i ) a monotonous increasing function on n t and ii ) a monotonous decreasing function on γ . Moreover , the nor - malized entropy of Zipf’s law of a system with n states converges to 1 / 2 [ 66 ] , i . e . , lim n →∞ h ( n , 1 ) = 1 2 . ( 13 ) We also note that the entropy of a power law with ex - ponent higher than one is bounded i . e . , if γ > 1 is the exponent of our power law , there exists a ﬁnite constant φ ( γ ) such that : lim n →∞ H ( n , γ ) < φ ( γ ) . ( 14 ) A key consequence of this result is that , if our ( unknown ) probability distribution is dominated [ 67 ] from some k by some power - law with exponent γ > 1 + δ ( for any δ > 0 ) , our entropy will be bounded . Furthermore , it can be shown that the normalized en - tropy of a power - law distribution in a system with n dif - ferent states , with exponent γ < 1 , converges to 1 , i . e . , lim n →∞ h ( n , γ ) = 1 . ( 15 ) Consistently , we can conclude that , if an ( unknown ) probability distribution is not dominated from any m by a power law with exponent lower than 1 − δ ( for any δ > 0 ) , the normalized entropy of our system will con - verge to 1 . Using these properties , in the following sections we proceed to derive Zipf’s using two complementary ap - proaches , namely 1 ) proposing a power - law as the as - symptotic solution of eq . ( 9 ) - section IIIB - and 2 ) As - suming that the entropy behaves in a scale - invariant way - section IIIC . B . Power Law Ansatz : Convergence of Exponents to γ = 1 In this section we make use of the power - law ansatz as a solution of our problem , i . e . , we assume that the solution is a power - law with an arbitrary exponent , i . e . , p n ( s i ) ∝ i − γ . The objective of this section is to demonstrate that , being h ( n , γ ) as deﬁned in eq . ( 12 ) , then the following limit holds : lim n →∞ h ( n , γ ) = Θ ( γ ) , ( 16 ) being Θ ( γ ) the step function , i . e . , Θ ( γ ) = 1 if γ < 1 and Θ ( γ ) = 0 if γ > 1 . It implies that , for large values of n , the whole range of normalized entropies between 0 and 1 is obtained from exponents γ arbitrarily close to γ = 1 - see ﬁg ( 2 ) . Let us rewrite the convergence assumptions provided in ( 9 , 10 ) assuming that our probability distribution is a power law : For any ǫ > 0 we can ﬁnd an n such that , for any n ′ > n we have an exponent , γ ( n ′ ) such that , | h ( n ′ , γ ( n ′ ) ) − µ | < ǫ , ( 17 ) 10 - 1 10 0 10 1 γ 0 . 0 0 . 2 0 . 5 0 . 8 1 . 0 H ( p ) / l og ( n ) N = 500000 N = 100000 N = 1000 N = 10000 N = 100 FIG . 2 : Normalized entropies of ﬁve power - law distributed systems of diﬀerent size as functions of the exponent . The curves display 5 diﬀerent sizes . n = 500000 black circles , n = 10000 white circles , n = 10000 up triangles , n = 1000 squares and n = 100 down triangles , respectively . The most interesting feature of the numerical computations is the sharp decay of the normalized entropy when the values of the ex - ponent are cllose to 1 , which implies that a wide range of normalized entropies are obtained by tuning the exponent of the power - law distribution around unity . Furthermore , we observe that the decay is sharper as the size of the system grows , concentrating an increasing range of relative entropies near the exponent 1 ( grey area ) . i . e . , the sequence of normalized entropies H , associated to system’s growth , namely H = h ( 1 , γ ( 1 ) ) , h ( 2 , γ ( 2 ) ) , . . . , h ( k , γ ( k ) ) , . . . , ( 18 ) converges to µ . Below we split the problem in two diﬀer - ent scenarios . 1 . First case : µ < 12 . We begin by exploring the following scenario : lim n →∞ h ( n , γ ( n ) ) = µ ∈ (cid:18) 0 , 1 2 (cid:19) . ( 19 ) From equation ( 13 ) we can ensure that , for large values of n , γ ( n ) > 1 . Since we assumed that the sequence H converges to µ , we can state that , for a given ǫ > 0 , there is an arbitrary n 1 such that : µ − ǫ < h ( n 1 , γ ( n 1 ) ) < µ + ǫ . ( 20 ) We know , from the properties of the entropies of power - law distributed systems , that H ( n 1 , γ ( n 1 ) ) < φ ( γ ( n 1 ) ) , where φ ( γ ( n 1 ) ) is some positive , ﬁnite constant ( see eq . ( 14 ) and appendix ) . Then , since log x is an unbounded , increasing function of x , we can ﬁnd n 2 > n 1 such that φ ( γ ( n 1 ) ) < ( µ + ǫ ) log n 2 . ( 21 ) 5 Thus , since h ( n , γ ) is a decreasing function on γ , we need to ﬁnd γ ( n 2 ) < γ ( n 1 ) such that µ − ǫ ′ < h ( n 2 , γ ( n 2 ) ) < µ + ǫ ′ , ( 22 ) with ǫ ′ ≤ ǫ , in order to satisfy the entropy restriction . Furthermore , since H ( n , 1 ) = 12 log n + O ( log ( log n ) ) , we conclude that 1 < γ ( n 2 ) < γ ( n 1 ) . Let us expand this process recursively , thus generating an inﬁnite decreasing sequence of exponents , { γ ( n k ) } ∞ k = 1 = γ ( n 1 ) , . . . , γ ( n i ) , . . . , ( 23 ) such that , for any γ ( n i ) ∈ { γ ( n k ) } ∞ k = 1 , γ ( n i ) > 1 . We notice that , for any α > 0 , we can ﬁnd a n k such that , if n j > n k , | γ ( n j ) − 1 | < α , ( 24 ) since , for every γ ( n k ) , we always ﬁnd a n j > n k such that φ ( γ ( n k ) ) < ( µ + ǫ ) log n j . ( 25 ) 2 . Second case : µ > 12 . Let us now consider the following entropy restriction problem : lim n →∞ h ( n , γ ( n ) ) = µ ∈ (cid:18) 1 2 , 1 (cid:19) . ( 26 ) From equation ( 13 ) , we can ensure that , for any n , γ ( n ) < 1 . Furthermore , from equation ( 15 ) , we again ﬁnd a problem close to the one solved above , since for n 1 large enough and γ < 1 , we have : H ( n 1 + 1 , γ ) − H ( n 1 , γ ) > µ ( log ( n 1 + 1 ) − log n 1 ) . ( 27 ) Now , since we assumed that the sequence H converges , we can state that given an arbitrary step n 1 , µ − ǫ < h ( n 1 , γ ( n 1 ) ) < µ + ǫ . ( 28 ) Since H ( n , γ ) is a decreasing function on γ , we need to ﬁnd γ ( n 2 ) > γ ( n 1 ) such that : µ − ǫ ′ < h ( n 2 , γ ( n 2 ) ) < µ + ǫ ′ , ( 29 ) with ǫ ′ ≤ ǫ , to satisfy the entropy restriction . However , from eq . ( 13 ) , we know that 1 > γ ( n 2 ) > γ ( n 1 ) . Pro - ceeding as above , we expand this process , thus generating an inﬁnite increasing sequence of exponents { γ ( n k } ∞ k = 1 . By virtue of equation ( 13 ) and equation ( 15 ) , and taking into account the decreasing behavior of h as a function of the exponent , we observe that , for any α > 0 , we can ﬁnd a n k such that , if n j > n k , | γ ( n j ) − 1 | < α . ( 30 ) In summary , under the power law ansatz , the only so - lution for eq . ( 9 ) , in the limit of large systems , is γ = 1 , i . e . , Zipf’s law . C . Scale invariance Condition The above power - law ansatz is purely mathematical , and can be replaced by a more physically realistic as - sumption . This leads us to the second strategy to solve our problem , which is based on the assumption that the mechanisms responsible for the growth and stabilization of the system do not depend on the size of the conﬁg - uration space , and , thus , a partial observation of the system will satisfy also condition ( 9 ) . We will refer to this assumption as the scale invariance condition , and it is formulated as follows . Let Σ ( k ) ⊆ Σ be the set of the ﬁrst k elements of Σ , observing a labeling consistent with the ordering of probabilities provided in eq . ( 8 ) - roughly speaking , the k most probable elements of Σ . The ran - dom variable which accounts for the observations of such k elements is notated X ( k ≤ n ) . We observe that , if X ( n ) follows the probability distribution p n , the random variable X ( k ≤ n ) obeys the following probability distri - bution , to be notated p kn : p kn ( i ) ≡ P ( s i | i ≤ k ) =  X j ≤ k p n ( s j )   − 1 p n ( s i ) . ( 31 ) Thus , if H ( X ( k ≤ n ) ) is the entropy of X ( k ≤ n ) , its normalized counterpart is deﬁned as h ( k ≤ n ) : h ( k ≤ n ) ≡ H ( X ( k ≤ n ) ) log k . ( 32 ) We remark that these derivations are valid at the limit of large systems , thereby considering that , at every step , n is arbitrarily greater than k . Furthermore , let us deﬁne ǫ ′ as : ǫ ′ ≡ | h ( k ≤ n ) − µ | + δ , ( 33 ) being δ arbitrarily small . Then , the scale invariance as - sumption for the entropy states that , for any n ≥ k ′ ≥ k , | h ( k ′ ≤ n ) − µ | < ǫ ′ . ( 34 ) In summary , condition ( 34 ) , is grounded on the assump - tion that the entropy restriction works at all levels of observation . Thus , the partial probability distributions of states we obtain must reﬂect the eﬀect of the entropy restriction , introducing a scale invariance of the normal - ized entropy of the partial samples of the system . As we saw in the above sections , the decay of this tail is strongly constrained by the entropy restriction , since only special cases avoid the normalized entropy to fall to 0 or 1 . To study in detail how it constrains the tail of the distribution we will work with the coeﬃcients f n ( k , k + 1 ) , deﬁned as : f n ( i , i + 1 ) = p n ( s i ) p n ( s i + 1 ) , instead of the raw probability distribution , to avoid mul - tiplying factors due to normalization . Now we observe 6 that , for a given , very large n , our probability distri - butions p kn must be able , as k increases , to unbound - edly increase the entropy of the whole system to reach the global value H ( X ( n ) ) , which lies in the interval ( ( µ − ǫ ) log n , ( µ + ǫ ) log n ) . Furthermore , scale invari - ance condition depicted in eq . ( 34 ) forces that , as k increases , contributions to the entropy never go neither to 0 nor to log ( k + 1 / log k ) , but lie within this interval . In other words , the sum deﬁned by the entropies must diverge as k increases over a system where n is arbitrary large , whereas the sequence of its normalized versions must converge to µ . The above derivations concerning the convergence properties of the entropy - see also Ap - pendix A - clearly state that those properties hold if p n satisﬁes , on one hand , for large i ’s , f n ( i , i + 1 ) < (cid:18) i + 1 i (cid:19) ( 1 − δ ) , ( 35 ) to avoid that h ( n ) → 1 . On the other hand , if we want to avoid that h ( n ) → 0 , the following inequality must hold : f n ( i , i + 1 ) > (cid:18) i + 1 i (cid:19) ( 1 + δ ) . ( 36 ) Therefore , the solution of our problem lies in the range deﬁned by : (cid:18) i + 1 i (cid:19) ( 1 − δ ) > f n ( i , i + 1 ) > (cid:18) i + 1 i (cid:19) ( 1 + δ ) . ( 37 ) From the study of the entropies of a power law performed in the previous section , we know that δ can be arbitrarily small if the size of the system is large enough . Thus : f n ( i , i + 1 ) = p n ( s i ) p n ( s i + 1 ) ≈ i + 1 i ( 38 ) which leads us to Zipf’s law as the unique asymptotic solution : p n ( s i ) ∝ i − 1 . ( 39 ) IV . DISCUSSION Complex , far from equilibrium systems involve a ten - sion between amplifying mechanisms and negative feed - backs able to buﬀer the impact of ﬂuctuations . In this paper we have considered the consequences of such ten - sion in terms of one of its most well known outcomes : the presence of an inverse scaling law connecting the size of observed events and its rank . The commonality of Zipf’s law in both natural and man - made systems has been a puzzle that attracted for years the attention of scien - tists , sociologists and economists alike . The fact that such a plethora of apparently unrelated systems display the same statistical pattern points towards some funda - mental , unifying principle . In this paper we treat complex systems as stochas - tic systems describable in terms of algorithmic complex - ity and thus statistical entropy . A general result from the algorithmic complexity theory is that eq . ( 3 ) holds for stochastic systems . Taking this general result as the starting point , we deﬁne a characterization of a wide class of complex systems which grasps the open nature of many complex systems , summarized in eq . ( 9 ) . The main achievement of this equation is that it encodes the concepts of growing and , even most important , the stabi - lization of complexity properties in an intermediate point between order and disorder , a feature observed in many systems displaying Zipf’s - like statistics . From this equa - tion we derived Zipf’s law as the natural outcome of sys - tems belonging to this class of stochastic systems . Our development avoids the classical procedures based on maximization ( minimization ) of some functional in order to ﬁnd the most probable conﬁguration of states , since in far from equilibrium the ensemble formalism , to - gether with Jaynes’ maximum entropy principle [ 60 ] can fail due to the open , non - reversible behavior of the sys - tems considered here . Thus we do not introduce mo - ment constraints , as it is usual in equilibrium statisti - cal mechanics [ 63 ] , but instead a constraint on the value achieved by the normalized entropy , no matter the scale we observe the system . Both a scaling ansatz and a more general scale invariance assumption lead to Zipf’s law as the unique solution for this problem . We observe that the ﬁnite size eﬀects deﬁne an interval of exponents around 1 , namely ( 1 − δ , 1 + δ ) , which could partly explain the variation observed in ﬁnite , natural systems . However , it is true that a system satisfying eq . ( 9 ) does not nec - essarily exhibit Zipf’s law . Further work should explore in depth the physically relevant conditions leading the evolution of Zipf’s like systems to remove the mathemat - ical assumptions made in this paper , thereby obtaining a complete description of them from a completely general , theoretical viewpoint . Appendix A : Entropic Properties of Power - Law distributed systems Consider a system whose behavior is described by the random variable X ( n ) taking values on the set Σ = { s 1 , . . . , s n } , | Σ | = n , according to the probabilty dis - tribution p n ( s i ) . The labeling ’ i ’ of the state is chosen in such a way that p n ( s 1 ) ≥ p n ( s 2 ) ≥ . . . ≥ p n ( s i ) ≥ . . . ≥ p n ( s n ) . The Shannon entropy of our system of n states , to be noted H ( X ( n ) ) , is deﬁned as [ 58 ] : H ( X ( n ) ) = − X k ≤ n p n ( s k ) log p n ( s k ) . ( A1 ) The normalized entropy of the system , to be written , h ( n ) , is deﬁned as : h ( n ) ≡ H ( X ( n ) ) log n . ( A2 ) 7 We will work with power - law distributions , by which p n ( s i ) = 1 Z i − γ where n is the number of available states , and Z the normalization constant , which depends on the size of the system , n . Let us rewrite the function H ( X ( n ) ) as a function of the exponent and the size of Σ , H ( n , γ ) . Consistently , h ( n , γ ) ≡ H ( n , γ ) log n . ( A3 ) This appendix is devoted to derive ﬁve properties of the entropy of power - law distributed systems . 1 . H ( n , γ ) is a continuous , monotonous decreasing function with respect to γ in the range ( 0 , ∞ ) . Indeed , the dominant term of its derivative is : ∂H ( n , γ ) ∂γ ∼ − X i ≤ n ( log i ) 2 i γ < 0 . ( A4 ) 2 . The entropy of a power - law is a monotonous , in - creasing function on the size of the system [ 68 ] . We want to show that H ( n , γ ) is a monotonous increas - ing function on n . In order to prove it , we must compute the diﬀerence H ( n + 1 , γ ) − H ( n , γ ) . For simplicity , let us deﬁne : S n ≡ X k ≤ n 1 k γ . ( A5 ) Using the trivial inequality : log (cid:18) S n + 1 ( 1 + n ) γ (cid:19) > log ( S n ) , ( A6 ) we can state that : H ( n + 1 , γ ) − H ( n , γ ) = γ S n + 1 ( 1 + n ) γ X k ≤ n + 1 log k k γ + log (cid:18) S n + 1 ( 1 + n ) γ (cid:19) − γ S n X k ≤ n log k k γ + log ( S n ) > γ X k ≤ n log k k γ 1 S n + 1 ( n + 1 ) γ − 1 S n ! + γ log ( n + 1 ) S n + 1 ( n + 1 ) γ = γ S 2 n ( n + 1 ) γ + S n   S n ( n + 1 ) γ log ( n + 1 ) − X k ≤ n log k k γ   > 0 . Finally , it is easy to check that the following properties also hold : lim γ →∞ H ( n , γ ) = 0 , ( A7 ) lim γ → 0 H ( n , γ ) = log n . ( A8 ) 3 . The normalized entropy of Zipf’s law of a system with n states ( p n ( s i ) ∝ i − 1 ) converges to 1 / 2 : We want to show that the sequence H = { h ( k , 1 ) } ∞ k = 1 = h ( 1 , 1 ) , h ( 2 , 1 ) , . . . , h ( k , 1 ) , . . . ( A9 ) converges to 12 . Let us suppose that H is a sequence satisfying the above requirements . Then , the entropy for a given n can be approached by [ 66 ] : H ( n , 1 ) = 1 2 log n + O ( log ( log n ) ) . ( A10 ) Thus , if h ( n , 1 ) = H ( n , 1 ) / log n , let us deﬁne ǫ ( n ) like : ǫ ( n ) ≡ (cid:12)(cid:12)(cid:12)(cid:12) h ( n , 1 ) − 1 2 (cid:12)(cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)(cid:12)(cid:12) O ( log ( log n ) ) log n (cid:12)(cid:12)(cid:12)(cid:12) . ( A11 ) Clearly , ǫ ( n ) is strictly decreasing on n , and , furthermore , lim n →∞ ǫ ( n ) = 0 . ( A12 ) 4 . The Entropy of a power law with exponent higher than 1 is bounded . Here we demonstrate that the entropy of a power law with exponent higher than 1 is bounded [ 69 ] . Speciﬁcally , we assume there exists a pair of positive constants Z , δ , such that : p n ( i ) = 1 Z i − ( 1 + δ ) ( A13 ) Then , the sequence of H = { h ( k , 1 + δ ) } ∞ k = 1 converges to 0 . Indeed , let us ﬁrst note that : lim n →∞ p n ( s i ) = 1 ζ ( 1 + δ ) i − ( 1 + δ ) , ( A14 ) where ζ ( 1 + δ ) ≡ ∞ X k 1 k 1 + δ ( A15 ) is the Riemann zeta - function [ 65 ] . The function is deﬁned by an inﬁnite sum which converges , in the real line , if δ > 0 , i . e . : ∞ X k 1 k 1 + δ < ∞ . ( A16 ) 8 otherwise , the sum diverges . Furthermore , it is also true that the above condition also holds for the following se - ries : ∞ X k log k k 1 + δ . ( A17 ) Indeed , note that , given an arbitrary δ > 0 there exists a ﬁnite number i ∗ such that : i ∗ ≡ min (cid:26) i : (cid:18) δ − log ( log i ) log i (cid:19) > 0 (cid:27) ( A18 ) and , if we deﬁne the following exponent , β ( i ∗ ) : β ( i ∗ ) ≡ 1 + δ − log ( log i ∗ ) log i ∗ , ( A19 ) there exists a ﬁnite constant , Ψ ( δ ) , deﬁned as : Ψ ( δ ) ≡ X i < i ∗ (cid:18) log i i 1 + δ − 1 i β ( i ∗ ) (cid:19) + ζ ( β ( i ∗ ) ) , ( A20 ) such that : ∞ X k log k k 1 + δ < Ψ ( δ ) . ( A21 ) With the above properties , it is clear that , if there exists a constant φ ( 1 + δ ) < ∞ such that : lim n →∞ H ( n , 1 + δ ) < φ ( 1 + δ ) , ( A22 ) then , the entropy of a power law with exponent higher than 1 is bounded . As we shall see , it is straightforward by checking directly the behavior of H ( n , 1 + δ ) : lim n →∞ H ( n , 1 + δ ) = 1 + δ ζ ( 1 + δ ) ∞ X i = 1 log i i 1 + δ + log ( ζ ( 1 + δ ) ) . Since H ( n , γ ) is an increasing function on n , and 1 + δ ζ ( 1 + δ ) ∞ X i = 1 log i i 1 + δ + log ( ζ ( 1 + δ ) ) < ∞ , ( A23 ) we can deﬁne a constant φ ( 1 + δ ) , φ ( 1 + δ ) ≡ lim n →∞ H ( n , 1 + δ ) + ǫ ( A24 ) ( where ǫ is any positive , ﬁnite constant ) . Clearly , H ( n , 1 + δ ) < φ ( 1 + δ ) . ( A25 ) Thus , lim n →∞ h ( n , 1 + δ ) = lim n →∞ H ( n , 1 + δ ) log n ≤ lim n →∞ φ ( 1 + δ ) log n = 0 . Consequence If an unknown probability distribution is dominated from some k by some power - law with expo - nent higher than 1 + δ , our entropy will be bounded . Consequence If an unknown probability distribution is dominated from some k by some power - law with expo - nent higher than 1 + δ , our normalized entropy will tend to 0 . 5 . The normalized entropy of a power - law distribu - tion in a system with n diﬀerent states , p n with exponent lower than 1 converges to 1 . Let us suppose that we have the following probability distribution , with 0 < δ < 1 : p n ( s i ) = 1 Z i − ( 1 − δ ) . ( A26 ) Note that [ 66 ] : X k ≤ n 1 k 1 − δ = Z n 1 1 x 1 − δ + O ( 1 ) ≈ n δ δ . ( A27 ) Applying directly the deﬁnition of entropy , H ( n , 1 − δ ) = δ ( 1 − δ ) n δ X k ≤ n log k k 1 − δ + δ log n − log δ . ( A28 ) If we compute the limit of h ( n , 1 − δ ) : lim n →∞ h ( n , 1 − δ ) = lim n →∞   δ ( 1 − δ ) log n · n δ X k ≤ n log k k 1 − δ + δ   = lim n →∞ 1 − δ log n (cid:18) log n − 1 δ (cid:19) + δ = 1 − δ + δ = 1 . Consequence If our ( unknown ) probability distribu - tion is not dominated from some k by a power law with exponent higher than 1 − δ , our normalized entropy will converge to 1 . Acknowledgments We thank D . Jou , S . Manrubia , J . Fortuny and our colleagues at the Complex Systems Lab for their useful comments . We also acknowledge the helpful comments provided by two anonymous referees . This work has been founded by the McDonnell Foundation ( BCM ) and by the Santa Fe Institute ( RS ) . 9 [ 1 ] Stanley , H - E , Amaral , L - A - N , Gopikrishnan , P , Ivanov , P - Ch , Keitt , T - H , Plerou , V . ( 2000 ) Physica A 281 , 60 - 68 [ 2 ] Stanley , H - E ( 1999 ) Rev Mod Phys . 71 , S358 - S366 . [ 3 ] Newman , M - E - J ( 2005 ) Contemporary Physics 46 323 - 351 . [ 4 ] Sol´e , R - V , Goodwin , B ( 2001 ) Signs of Life : How Com - plexity Pervades Biology . ( New York . Basic Books ) . [ 5 ] Zipf , G - K ( 1949 ) Human Behavior and the Principle of Least Eﬀort . ( Addison - Wesley ( Reading MA ) ) . [ 6 ] Auerbach , F , ( 1913 ) Patermans Geograpische Mittelun - gen 59 , 74 - 76 . [ 7 ] Gabaix , X ( 1999 ) The Quart J of Economics 114 , 739 - 767 . [ 8 ] Ferrer - i - Cancho , R , Sol´e , R - V ( 2002 ) Adv Complex Syst 5 , 1 - 6 . [ 9 ] For a very complete collection of the works concerning the topic we refer the reader to : http : / / www . nslij - genetics . org / wli / zipf / . [ 10 ] Simon , H - A ( 1955 ) Biometrika 42 , 425 - 440 . [ 11 ] Makse , H - A , Havlin , S , Stanley , H - E ( 1995 ) Nature 377 , 608 - 612 . [ 12 ] Krugman , P ( 1996 ) J Jap Int Econ 10 , 399 - 418 . [ 13 ] Blank , A , Solomon , S ( 2000 ) Physica A 287 , 279 - 288 . [ 14 ] Decker , E - H Kerkhoﬀ , A - J and Moses , M - E ( 2007 ) PLoS ONE 2 ( 9 ) ( 2007 ) , 934 . [ 15 ] Axtell , R L ( 2001 ) Science 293 , 1818 - 1820 . [ 16 ] Gabaix , X , Gopikrishnan , P , Plerou , V , Stanley , E H ( 2003 ) Nature 423 , 267 - 270 . [ 17 ] Pareto , V . ( 1896 ) Cours d’Economie Politique , ( Droz . Geneva ) . [ 18 ] Okuyama , K . , Takayasu , M . and Takayasu , H . ( 1999 ) Physica A 269 , 125 - 131 . [ 19 ] Reed , W J , Hughes , B D ( 2002 ) Phys Rev E 66 , 067103 - 067106 . [ 20 ] Zanette , D - H , Manrubia , S - C ( 2001 ) Physica A 295 , 1 - 8 . [ 21 ] Krugman , P ( 1996 ) The Self - Organizing Economy . ( Blackwell . Oxford ) . [ 22 ] Arthur , B - W ( 1996 ) Increasing returns and path depen - dence in economy . ( Michigan University Press . Michi - gan ) . [ 23 ] Manrubia , S - C , Zanette , D - H , Sol´e , R - V ( 1999 ) Fractals 7 , 1 - 8 . [ 24 ] Rozman , G ( 1990 ) East asian Urbanization in the nineteenth century : Comparisons with Europe ed der Woude et al , V ( Clarendon Press ) , pp . 61 - 63 . [ 25 ] Solomon , S , Levy , M ( 1996 ) Int J Mod Phys C 7 , 745 - 751 . [ 26 ] Malcai , O , Biham , O , Solomon , S ( 1999 ) Phys Rev E 60 , 1299 - 1303 . [ 27 ] Huang , Z - F Solomon S ( 2000 ) cond - mat / 0008026 . [ 28 ] Sol´e , R - V , Manrubia , S - C ( 1996 ) Phys Rev E 54 , R42 - R45 [ 29 ] Newman M - E - J , Palmer , R - G ( 2003 ) Modelling Extinc - tion ( Oxford University Press : New York ) [ 30 ] Zanette , D - H , Manrubia , S - C ( 1997 ) Phys Rev Lett 79 , 523 - 526 . [ 31 ] Manrubia , S - C , Zanette , D - H ( 1998 ) Phys Rev E 58 , 295 - 302 . [ 32 ] Newman , M - E - J , Sneppen , K ( 1996 ) Phys Rev E 54 , 6226 - 6231 . [ 33 ] White , W - H ( 1982 ) J Colloid Interface Sci 87 , 204 - 208 . [ 34 ] Family , F , Meakin , P ( 1989 ) Phys Rev A 40 , 3836 - 3854 . [ 35 ] Bak , P , Tang , C , Wiesenfeld , K ( 1987 ) Phys Rev Lett 59 , 381 – 384 . [ 36 ] Harremo¨es , P , Topsøe , F ( 2001 ) Entropy 3 , 191 - 226 . [ 37 ] Ferrer - i - Cancho , R , Sol´e , R V ( 2003 ) Proc Natl Acad Sci USA 100 , 788 - 791 . [ 38 ] Li , W ( 1992 ) IEEE Trans Inform Theo 38 , 1842 - 1845 . [ 39 ] Montroll , E - W and Shlesinger , M - F . ( 1982 ) Proc Natl Acad Sci USA 79 , 3380 - 3383 . [ 40 ] Kawamura , K and Hatano , N . ( 2002 ) J Phys Soc Jpn 71 ( 5 ) 1211 - 1213 . [ 41 ] Amaral , L - A - N , Buldyrev , S - V , Havlin , S , Salinger , M - A , Stanley , H - E ( 1997 ) Phys Rev Lett 80 1385 - 1388 . [ 42 ] Solomonoﬀ , R . ( 1964 ) Inform and Control 7 - 1 , 1 - 22 . [ 43 ] Kolmogorov , A ( 1965 ) Problems Inform Transmission 1 , 1 - 7 . [ 44 ] Chaitin , G - J ( 1966 ) J ACM 13 , 547 - 569 . [ 45 ] Ming , L , Vit´anyi , P ( 1997 ) An introduction to Kol - mogorov complexity and its applications . ( Springer , New York [ u . a . ] ) . [ 46 ] Perline , R . ( 1996 ) Phys Rev E 54 ( 1 ) , 220 - 223 . [ 47 ] Cover , T - M , Thomas , J - A ( 1991 ) Elements of Informa - tion Theory . ( John Wiley and Sons . New York ) . [ 48 ] Adami , C . ( 1999 ) Introduction to Artiﬁcial Life ( Springer , New York ) [ 49 ] Chaitin , G - J ( 1975 ) J Assoc Comput Mach 22 , 329 - 340 . [ 50 ] Nicolis , J - S ( 1986 ) Rep Prog Phys 49 , 1109 - 1196 . [ 51 ] Ebeling , W , Nicolis , G ( 1991 ) Europhys Lett 14 ( 3 ) , 191 - 196 . [ 52 ] Ebeling , W ( 1993 ) Physica A 194 , 563 - 575 . [ 53 ] Kaspar , F , Schuster , H - G ( 1987 ) Phys Rev A 36 ( 2 ) , 842 - 848 . [ 54 ] Zurek , W - H ( 1989 ) Nature 341 , 119 - 124 . [ 55 ] Dewey , T - G ( 1996 ) Phys Rev E 54 , R39 - R41 . [ 56 ] Gr¨unwald , P - D , Vit´anyi , P - M ( 2008 ) Handbook of the Philosophy of Science , Volume 8 : Philosophy of Informa - tion . , eds . Adriaans , P , van Benthem , J ( Elsevier Science Publishers ) , pp . 289 – 325 . [ 57 ] Shannon , C - E ( 1948 ) Bell Syst Tech J 27 , 379 - 423 . [ 58 ] Ash , R - B ( 1990 ) Information Theory . ( New York . Dover ) . [ 59 ] Gr¨unwald , P - D , Vit´anyi , P - M ( 2003 ) J Logic , Lang In - form 12 , 497 - 529 . [ 60 ] Jaynes , E - T ( 1957 ) Phys Rev 106 , 620 - 630 . [ 61 ] Haken , H ( 1978 ) Synergetics : An Introduction . Nonequi - librium Phase Transitions and Self - Organization in Physics , Chemistry and Biology ( Springer Series in Syn - ergetics ) . ( Springer ) . [ 62 ] Kapur , J - N ( 1989 ) Maximum - entropy Models in Science and Engineering . ( New Delhi . Wiley Eastern Limited ) . [ 63 ] Pathria , R - K ( 1996 ) Statistical Mechanics , Second Edi - tion . ( Butterworth - Heinemann ) . [ 64 ] Hernando , A , Puigdom ` enech , D , Villuendas , D , Vesperi - nas , C and Plastino , A . ( 2009 ) Phys Lett A 374 ( 1 ) 18 - 21 . [ 65 ] Abramowitz , M , Stegun , I ( editors ) ( 1965 ) Handbook of mathematical functions , NBS , Appl . Math . Ser . ( U . S . Government Printing oﬃce , Washington , D . C . ) Vol . 55 . [ 66 ] Jones , D - S ( 1979 ) Elementary Information Theory . ( Ox - ford University Press . Oxford ) . [ 67 ] A probability distribution is dominated from some k 10 by a power law with exponent 1 + δ if ( ∃ m ) : ( ∀ i > m ) (cid:18) p ( i + 1 ) p ( i ) < (cid:16) i i + 1 (cid:17) 1 + δ (cid:19) . [ 68 ] The reader could object that this section is unnecessary , since the axiomatic derivation of the uncertainty func - tion ( which we call entropy ) assumes that the entropy increases with the size of the system . However , the ex - plicit statement of this axiom corresponds to the spe - cial case of uniform probabilities [ 58 ] . Speciﬁcally , the axiom states that , if we have two systems A , B such that A contains n states a 1 , . . . , a n and B contains n + 1 states , b 1 , . . . , b n + 1 , then , if ( ∀ i ≤ n ) p ( a i ) = 1 / n and ( ∀ i ≤ n + 1 ) p ( b i ) = 1 / ( n + 1 ) H ( A ) < H ( B ) . Thus , if we are not dealing with this special case , we need to ex - plicitly demonstrate that it holds for our purposes . [ 69 ] This derivation is equivalent to the one found in [ 36 ] , Theorem 8 . 2 . In this theorem , the authors demonstrate that every inﬁnite distribution with inﬁnite entropy is hy - perbolic , which implies that the distribution is not dom - inated by a power law with an exponent higher than 1 .