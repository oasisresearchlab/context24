Bayesian measures of model complexity and ﬁt D . J . Spiegelhalter , N . G . Best , B . P . Carlin , and A . van der Linde ( 2002 ) Presented by : Lina Lin STAT / BIOST 572 May 21 , 2013 Classical approaches to model selection The goal is to select the “best approximating model” , which achieves the optimal balance between “ﬁt” and “complexity . ” • Cross - validation is the intuitive approach , but ( 1 ) validation data is not always readily available and ( 2 ) can be computationally intensive . • An idea that has gained much traction over recent years is the use infor - mation criterions . • Ex . Akaike’s Information Criterion ( Akaike , 1973 ) : AIC = − 2 log L ( ˆ θ ) + 2 p • Ex . Bayesian Information Criterion ( Schwarz , 1978 ) : BIC = − 2 log L ( ˆ θ ) + 2 p log n where L ( ˆ θ ) denotes the maximized likelihood , p number of free parameters and n number of observations . 2 / 22 A Generalized Criterion for Hierarchical Models For practical reasons , we would like to develop a criterion for hierarchical model ( e . g . random eﬀects model ) selection . Both AIC and BIC are characterized as a sum of a “ﬁt” term ( L ) and a “com - plexity term” ( a monotonically increasing function of p ) . (cid:17) While this is by no means rigorous justiﬁcation , we would expect our criterion to feature these components , which implies that we need to ﬁrst deﬁne a complexity measure for hierarchical models . The most ambitious attempts so far have been made in smoothing and neural network literature : see Wahba ( 1990 ) , Moody ( 1992 ) , MacKay ( 1995 ) and Ripley ( 1996 ) . 3 / 22 Eﬀective number of parameters ( p eﬀ ) Consider the following random eﬀects model : Y i | θ i ∼ N ( θ i , τ − 1 i ) θ i ∼ N ( ψ , λ − 1 ) ( 1 ) for i = 1 , . . . , p . (cid:17) Question : Is p e ﬀ = p ? The presence of a prior induces dependency between the θ i ’s , which reduces the dimensionality of the model , so the actual complexity of the model is ≤ p . The available data also inﬂuences the degree of dependency , which is consistent with the idea that complexity should reﬂect the diﬃculty in estimation . We will return to this idea later . 4 / 22 Eﬀective number of parameters ( p eﬀ ) Here is a schematic representation of the random eﬀects model ( 1 ) from the previous slide : ψ θ 1 θ p y p y 1 5 / 22 Parameter ( s ) of focus The full probability model for ( 1 ) factorizes as : p ( y , θ , ψ ) = p ( y | θ ) p ( θ | ψ ) p ( ψ ) From which we can construct the following marginal distributions : • p ( y , θ ) = p ( y | θ ) (cid:82) Ψ p ( θ | ψ ) p ( ψ ) d ψ = p ( y | θ ) p ( θ ) i . e . focused on Θ OR • p ( y , ψ ) = (cid:82) Θ p ( y | θ ) p ( θ | ψ ) p ( ψ ) d θ = p ( y | ψ ) p ( ψ ) i . e . focused on Ψ We assume , by default , the model to be focused on Θ for the remainder of this presentation . 6 / 22 A complexity measure for hierarchical models Spiegelhalter et al . deﬁne the complexity of the focused model to be : p D { y , Θ , ˜ θ ( y ) } = E θ | y [ − 2 log { p ( y | θ ) } ] + 2 log [ p { y | ˜ θ ( y ) } ] where ˜ θ is often selected to be the posterior mean of θ . We can also write p D as : p D = D ( θ ) − D ( ¯ θ ) ( 2 ) where D ( θ ) = − 2 log { p ( y | θ ) } + 2 log { f ( y ) } , which we refer to as “Bayesian deviance . ” (cid:17) Why ? The “non - Bayesian” variant : d Θ { y , Θ , ˆ θ ( y ) } = − 2 log { p ( y | θ ) } ] + 2 log [ p { y | ˆ θ ( y ) } is an approximation to the complexity term found in the Takeuchi Information Criterion ( TIC ) . 7 / 22 p D under normal approximation to the likelihood If we were to assume a normal approximation to the posterior likelihood , we can expand D ( θ ) about the posterior mean ¯ θ via a secord - order Taylor expansion and obtain the following result : p D ≈ tr ( − L (cid:48)(cid:48) ¯ θ V ) ( 3 ) where V = E { ( θ − ¯ θ ) ( θ − ¯ θ ) T } . Note that : − L (cid:48)(cid:48) ¯ θ is the observed Fisher information at ¯ θ , so p D can be thought of as the fraction of information in the likelihood about the parameters relative to the total information . (cid:17) Under negligible prior information ( i . e . ﬂat priors ) , we obtain , p D ≈ p ( 4 ) 8 / 22 A information - theoretic justiﬁcation for p D The “ mutual information ” between Y and Θ is ; I ( θ , Y ) = (cid:90) p ( θ , y ) log p ( θ , y ) p ( θ ) p ( y ) d θ dy = KL ( p ( θ , y ) , p ( θ ) p ( y ) ) and the symmetrized “mutual information” : J ( θ , Y ) = KL ( p ( θ , y ) , p ( θ ) p ( y ) ) + KL ( p ( θ ) p ( y ) , p ( θ , y ) ) where “KL” stands for Kullback - Leibler divergence , a measure of “distance” between two densities . (cid:17) “Mutual information” measures how sensitive the posterior distribution of θ is to the observations Y , and thus , corresponds to diﬃculty in estimation , which agrees with our notions on “model complexity . ” (cid:17) van der Linde ( 2005 ) showed that ( for conjugate exponential families ) : p D ≈ J ( θ post , Z ) ( 5 ) where θ post represents the posterior θ , and Z represents future observations derived from same data - generating mechanism . 9 / 22 A convincing example Consider the general hierarchical model described by Lindley and Smith ( 1972 ) . Suppose that : Y ∼ N ( A 1 θ , C 1 ) θ ∼ N ( A 2 ψ , C 2 ) ( 6 ) Then , through a series of ( tedious ) calculations , we can show that : p D = tr ( H ) = (cid:88) i h ii ( 7 ) where H is the hat matrix ( i . e . projection matrix ) . (cid:17) In other words , the eﬀective number of parameters is the sum of the individual leverages : how much of an eﬀect each Y has on the overall ﬁt . This matches previous suggestions made by Ye ( 1998 ) for model complexity . 10 / 22 Deviance Information Criterion Using this complexity measure , Spiegelhalter et al . propose the following crite - rion for comparing hierarchical models , which they term “Deviance Information Criterion , ” DIC = D ( ¯ θ ) + 2 p D ( 8 ) Recall that : AIC = − 2 log L ( ˆ θ ) + 2 p (cid:17) We can think of DIC as a “generalized” version of AIC . (cid:17) In fact , when working with ﬂat priors , DIC serves as a decent approximation of AIC since D ( ¯ θ ) ≈ − 2 log L ( ˆ θ ) and p D ≈ p in this case . 11 / 22 The Derivation DIC is an approximation of the expected posterior loss when adopting a par - ticular model , assuming a logarithmic loss function : L ( Y , ˜ θ ) = − 2 log { p ( Y | ˜ θ ) } = D ( ˜ θ ) Assume that we have a replicate dataset Z derived from the same data - generating mechanism as Y , our original dataset . We favour the model that minimizes the expected loss that is suﬀered in predicting Z : E z | θ [ L ( Y , ˜ θ ( y ) ) ] We can estimate this predicted loss using L ( Y , ˜ θ ( y ) ) - the loss suﬀered from re - predicting Y - however , this estimate is biased so we need to include an “optimism” term c ( Efron , 1986 ) : E z | θ [ L ( Y , ˜ θ ) ( y ) ] = L ( Y , ˜ θ ( y ) ) + c Θ { y , θ t , ˜ θ ( y ) } = D ( ˜ θ ) + c Θ { y , θ t , ˜ θ ( y ) } ( 9 ) 12 / 22 The Derivation The derivation mimics that used to derive the AIC ( i . e . if we were to replace ˜ θ with the MLE ˆ θ ) . We can manipulate the above expression ( 9 ) as : c Θ { y , θ , ˜ θ ( y ) } = E z | θ { D z ( ˜ θ ) − D z ( θ ) } + E z | θ { D z ( θ ) − D ( θ ) } + { D ( θ ) − D ( ˜ θ ) } We label the three components to the sum L 1 , L 2 and L 3 . By deﬁnition , we have ( 3 ) E θ | y [ L 3 ] = E θ | y { D ( θ ) − D ( ˜ θ ) } = p D . 13 / 22 The Derivation We perform a Taylor series expansion of L 1 about θ : ( 1 ) L 1 ≈ E z | θ { − 2 log [ p ( z | ˜ θ ) ] + 2 log [ p ( z | ˜ θ ) ] − 2 ( ˜ θ − θ ) T L (cid:48) z , θ − ( ˜ θ − θ ) T L (cid:48)(cid:48) z , θ ( ˜ θ − θ ) } = E z | θ { − 2 ( ˜ θ − θ ) T L (cid:48) z , θ − ( ˜ θ − θ ) T L (cid:48)(cid:48) z , θ ( ˜ θ − θ ) } We note that E z | θ (cid:110) ∂ log [ p ( z | θ ) ] ∂θ (cid:111) = 0 . L 1 ≈ E z | θ { − ( ˜ θ − θ ) T L (cid:48)(cid:48) z , θ ( ˜ θ − θ ) } = tr { − E z | θ [ L (cid:48)(cid:48) z , θ ] ( ˜ θ − θ ) T ( ˜ θ − θ ) } ≈ tr { − L (cid:48)(cid:48) ˜ θ ( ˜ θ − θ ) ( ˜ θ − θ ) T } Where the last line follows from a “good model” assumption . Taking the pos - terior means , we have E θ | y [ L 1 ] = tr { − L (cid:48)(cid:48) ˜ θ E θ | y [ ( ˜ θ − θ ) ( ˜ θ − θ ) T ] } = tr { − L (cid:48)(cid:48) ˜ θ V } ≈ p D 14 / 22 The Derivation The L 2 can be “ignored” because it can be shown to have a marginal expectation of 0 . ( 2 ) L 2 = E z | θ { − 2 log [ p ( z | θ ] + 2 log [ p ( y | θ ) ] } Taking double expectations : E y E θ | y [ L 2 ] = E y E θ | y E z | θ { − 2 log [ p ( z | θ ] + 2 log [ p ( y | θ ) ] } = E θ E y | θ E z | θ { − 2 log [ p ( z | θ ] + 2 log [ p ( y | θ ) ] } = E θ [ E z | θ { − 2 log [ p ( z | θ ] } + E y | θ { 2 log [ p ( y | θ ) ] } ] = 0 Putting all this together , we have E θ | y [ c Θ { y , θ , ˜ θ ( y ) } ] ≈ 2 p D ( 10 ) Which implies : E θ | y E z | θ [ L ( Y , ˜ θ ( y ) ) ] ≈ D ( ˜ θ ) + 2 p D = D ( θ ) + p D ( 11 ) 15 / 22 Implementation : Scottish lip cancer data We apply DIC in selecting a model for rates of lip cancer in 56 districts in Scotland ( Clayton and Kaldor , 1987 ; Breslow and Clayton , 1993 ) . We assume cancer counts y i are Poisson with mean E i exp ( θ i ) where E i is the expected number of cases for county i , i = 1 , . . . , 56 . We consider the following set of candidate models : 1 . Pooled : θ i = α 0 2 . Exchangeable : θ i = α 0 + γ i , γ i exchangeable random eﬀects 3 . Spatial : θ i = α 0 + δ i , δ i spatial random eﬀects 4 . Exchangeable + Spatial : θ i = α 0 + γ i + δ i 5 . Saturated : θ i = α i We placed an improper ﬂat prior on α 0 , zero - mean normal priors with precision λ γ on the γ i ’s , an ICAR prior on the δ i ’s ( Besag , 1974 ) with precision parameter λ δ , and weakly informative Γ ( 0 . 5 , 0 . 0005 ) on λ γ and λ δ . 16 / 22 Implementation : Scottish lip cancer data Model p µ D DIC µ p θ D DIC θ p medD DIC med Pooled 1 . 0 382 . 7 1 . 0 382 . 7 1 . 0 382 . 7 1 . 0 382 . 7 1 . 0 382 . 7 1 . 0 382 . 7 Exchangeable 42 . 8 103 . 8 43 . 3 104 . 3 43 . 4 104 . 4 42 . 9 104 . 0 43 . 4 104 . 5 43 . 5 104 . 6 Spatial 31 . 6 88 . 9 31 . 2 88 . 5 31 . 1 88 . 4 31 . 7 89 . 9 31 . 2 89 . 5 31 . 1 89 . 3 Exchangeable + Spatial 32 . 6 90 . 6 32 . 2 90 . 2 32 . 2 90 . 2 31 . 8 89 . 7 31 . 4 89 . 3 31 . 3 89 . 2 Saturated 55 . 9 111 . 9 52 . 9 108 . 9 54 . 7 110 . 7 55 . 9 111 . 7 52 . 8 108 . 6 54 . 5 110 . 4 Table : Summary of calculated p D and DIC values after running 15000 MCMC iterations following a burn - in period of 5000 iterations under the three diﬀerent parameterizations : mean ( µ ) , canonical ( θ ) , and median ( med ) 17 / 22 Implementation : Six - cities study We consider modelling a subset of data from the six - cities study , a longitudinal study of the health eﬀects of air pollution ( Fitmaurice and Laird , 1993 ) . Y ij ∼ Bern ( p ij ) p ij = g − 1 ( µ ij ) µ ij = β 0 + β 1 ( a ij − a ) + β 2 ( s i − s ) + β 3 ( s i a ij − sa ) ( 12 ) where Y ij is wheezing status ( 1 for yes , 0 for no ) of child i at time j , s i is smoking status of child i ’s mother , and a ij is age of child i at time j . The three models are : 1 . g ( p ij ) = log { p ij / ( 1 − p ij ) } logit link 2 . g ( p ij ) = Φ − 1 ( p ij ) probit link 3 . g ( p ij ) = log { − log ( 1 − p ij ) } complementary log - log link We place ﬂat priors on β 0 , . . . , β 3 , a normal prior on β with precision λ and a Γ ( 0 . 001 , 0 . 001 ) prior on λ . 18 / 22 Implementation : Six - cities study Model p µ D DIC µ p θ D DIC θ Logit 169 . 5 1334 . 3 247 . 2 1412 . 1 168 . 9 1335 . 3 248 . 7 1415 . 1 Probit 159 . 0 1306 . 0 262 . 1 1409 . 1 158 . 7 1307 . 3 262 . 7 1411 . 3 Complementary Log - Log 167 . 0 1350 . 8 224 . 1 1407 . 8 167 . 2 1348 . 1 224 . 4 1405 . 3 Table : Summary of calculated p D and DIC values after running 10000 MCMC iterations following a burn - in period of 10000 iterations under two diﬀerent parameterizations : mean ( µ ) and canonical ( θ ) . 19 / 22 BPIC DIC does have the potential to select the model that over - ﬁts since it “uses the data twice , ” and thus “under - penalizes” complexity . (cid:17) Ando ( 2007 ) developed a new criterion Bayesian Predictive Information Criterion ( BPIC ) which corrects for the over - ﬁtting . The form of BPIC is quite complicated , but its complexity term is similar to that of TIC’s . The penalty term reduces to 3 p D under similar approximations made by Spiegelhalter et al . in the DIC derivation . (cid:17) We note that DIC minimizes the posterior expected loss over a constrained space . If we were to repeat the proof above using E z | θ [ L ( Y , θ ) ] instead of E z | θ [ L ( Y , ˜ θ ( y ) ) ] as the target , we can achieve 3 p D as the penalty term , as noted by van der Linde ( 2005 ) . 20 / 22 Pros and Cons of DIC Pros Cons 1 . Can be readily computed using MCMC . 2 . Equivalent to AIC under vague priors . 1 . Not invariant to parameterization . 2 . Selection of ˜ θ arbitrary ; no guidelines 3 . Does not work for mixture models , or in general when posterior densities are non log - concave . 21 / 22 Key References • D . J . Spiegelhalter , N . G . Best , B . P . Carlin , and A . van der Linde , “Bayesian measures of model complexity and ﬁt” , Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , vol . 64 , no . 4 , pp . 538639 , 2002 . • H . Akaike , “A new look at the statistical model identiﬁcation” , Automatic Control , IEEE Transactions on , vol . 19 , no . 6 , pp . 716723 , 1974 . • S . Kullback and R . A . Leibner , “On information and suﬃciency” , Annals of Mathematical Statistics , vol . 22 , no . 1 , pp . 7986 , 1951 . • A . van der Linde . “DIC in variable selection” , Statistical Neerlandica , vol . 59 , no . 1 , pp . 45 - 56 , 2005 . • T . Ando , “Bayesian predictive information criterion for the evaluation of hierarchical bayesian and empirical bayes models” , Biometrika , vol . 94 , no . 2 , pp . 443458 , 2007 . 22 / 22