12 Blending Measures of Programming and Social Behavior into Predictive Models of Student Achievement in Early Computing Courses ADAM S . CARTER , Humboldt State University CHRISTOPHER D . HUNDHAUSEN and OLUSOLA ADESOPE , Washington State University Analyzing the process data of students as they complete programming assignments has the potential to pro - vide computing educators with insights into both their students and the processes by which they learn to program . In prior research , we explored the relationship between ( a ) students’ programming behaviors and course outcomes , and ( b ) students’ participation within an online social learning environment and course outcomes . In both studies , we developed statistical measures derived from our data that significantly corre - late with students’ course grades . Encouraged both by social theories of learning and a desire to improve the accuracy of our statistical models , we explore here the impact of incorporating our predictive measure derived from social behavior into three separate predictive measures derived from programming behaviors . We find that , in combining the measures , we are able to improve the overall predictive power of each mea - sure . This finding affirms the importance of social interaction in the learning process , and provides evidence that predictive models derived from multiple sources of learning process data can provide significantly better predictive power by accounting for multiple factors responsible for student success . CCS Concepts : • Information systems → Data analytics ; • Social and professional topics → Comput - ing education ; • Applied computing → E - learning ; Additional Key Words and Phrases : Learning analytics , learning process data , learning interventions ACM Reference format : Adam S . Carter , Christopher D . Hundhausen , and Olusola Adesope . 2017 . Blending Measures of Programming and Social Behavior into Predictive Models of Student Achievement in Early Computing Courses . ACM Trans . Comput . Educ . 17 , 3 , Article 12 ( August 2017 ) , 20 pages . https : / / doi . org / 10 . 1145 / 3120259 1 INTRODUCTION By collecting a stream of learning process data in their courses , educators create opportunities to continuously assess their students’ learning processes and progress . Using techniques from the This work is supported by the National Science Foundation , under Grant No . IIS - 1321045 . Authors’ addresses : A . S . Carter , Department of Computer Science , Humboldt State University , 1 Harpst St . , Arcata , CA 95521 ; email : adam . carter @ humboldt . edu ; C . D . Hundhausen , Human - centered Environments for Learning and Program - ming ( HELP ) Lab , School of Electrical Engineering and Computer Science , Washington State University , Pullman , WA 99164 - 2752 ; email : hundhaus @ wsu . edu ; O . Adesope , College of Education , Washington State University , Pullman , WA 99164 - 2114 ; email : olusola . adesope @ wsu . edu . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , to republish , to post on servers , to redistribute to lists , or to use any component of this work in other works requires prior specific permission and / or a fee . Permissions may be requested from Publications Dept . , ACM , Inc . , 2 Penn Plaza , Suite 701 , New York , NY 10121 - 0701 USA , fax + 1 ( 212 ) 869 - 0481 , or permissions @ acm . org . © 2017 ACM 1946 - 6226 / 2017 / 08 - ART12 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3120259 ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 2 A . S . Carter et al . fields of educational data mining and learning analytics [ 6 , 51 ] , educators can analyze these data in order to identify ways in which learning behaviors and attitudes relate to learning outcomes . Such analyses open up new opportunities to better tailor instruction to individual learners , and ultimately to improve student learning outcomes , especially among at - risk learners . In computing education , employing educational data mining and learning analytics techniques would appear to be particularly appropriate , given computing education’s “grand challenge” prob - lem of improving student retention , especially in early computing courses [ e . g . , 14 , 22 , 56 ] . Indeed , if computing educators are able to identify , at an early stage in a computing course , students who are at risk of dropping out or failing the course , then they are in a better position to improve retention by tailoring or adapting their instructional approaches . Recognizing this potential , computing education researchers have become increasingly interested in collecting log data on students’ programming processes as they work on course assignments [ 4 ] . In prior work [ 16 ] , we contributed to this research space by considering the generalizability of the Error Quotient [ 30 ] and Watwin [ 55 ] , two predictive measures of student performance based on students’ compilation behavior . We found these measures to be less accurate under our test conditions ( CS2 , C + + , Visual Studio ) than under the original conditions in which they were formulated ( CS1 , Java , BlueJ ) . In addition , we introduced the Normalized Programming State Model ( NPSM ) , a more holistic predictive model based on students’ editing , compilation , and debugging behaviors . On our dataset , the NPSM was found to produce more reliable predictions when compared to either Error Quotient or Watwin . We hypothesize that the NPSM’s increased predictive power is in part due to the fact that the measure takes into account additional programming activities not considered by either the Error Quotient or Watwin . In computer science education , it is widely recognized that students’ programming activities are crucial to their success as learners . However , any approach that attempts to predict students’ success based solely on their programming activities may overlook other factors critical to their success or failure within the discipline . For example , according to social learning theory , it makes sense to consider students’ social activities : how they interact with their peers during the program - ming process . Indeed , in prior work we discovered a significant relationship between a student’s online social activity and course performance [ 27 ] . Given our hypothesis that the NPSM outper - formed other programming - based measures because it considered multiple aspects of a student’s programming behavior , we wonder whether a model that includes social behavior might likewise outperform statistical models based exclusively on programming behaviors . To this end , we pose the following research questions : RQ1 : How might predictive measures derived from programming behavior be augmented with additional , non - programming - based data ? RQ2 : Can predictive measures that incorporate multiple data sources outperform measures derived solely from programming behaviors ? To answer these questions , this article extends prior work ( see [ 16 , 27 ] ) by exploring the impact of incorporating a social measure into three predictive models : the Error Quotient [ 30 ] , Watwin [ 55 ] , and the NPSM [ 16 ] . We find that incorporating social participation into these predictive statis - tical models yields additional explanatory power . This finding strengthens our hypothesis that pre - dictions of student achievement can be improved by considering a wide variety of student factors . 2 BACKGROUND AND RELATED WORK 2 . 1 Predictors of Success in Computing Courses A large body of educational research has explored the extent to which various learner variables are able to predict learning outcomes or future learning behaviors . These variables include the ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 3 learner’s background [ e . g . , 10 , 32 ] , prior knowledge [ e . g . , 10 , 39 ] , cognitive abilities [ e . g . , 46 ] , time - on - task [ e . g . , 48 ] , and learning attitudes [ e . g . , 8 ] . In computing education , for example , Rosson et al . [ 44 ] found strong positive correlations between a number of attitudinal variables , including self - efficacy , and a learner’s orientation toward the computing discipline . While this line of research shares our interest in predicting student learning outcomes , it differs in that it relies on only a limited number of data snapshots to make its predictions . Thus , it lacks the ability to furnish pre - dictions of student performance that are dynamic , robust , and continuously updated throughout a course . In this article , we analyze data derived from a continuous stream of data in order to identify patterns of learning associated with positive learning outcomes . As such , our work falls within the emerging areas of educational data mining and learning analytics [ 6 , 51 ] , which , in many STEM fields , have been used to gain insights into the processes that underlie student learning , and ultimately to better tailor instruction . A foundational idea is to build learner models that infer learners’ background knowledge , learning strategies , and motivations from learning process data [ 39 ] . In turn , such models are used to adapt instruction to learner needs . Within computing education , a legacy of research has studied students’ programming processes using think - aloud protocols [ e . g . , 33 , 49 ] , video analysis [ e . g . , 25 ] , and software logs [ e . g . , 21 , 23 ] . These studies have had a variety of goals , ranging from better understanding how novices ap - proach programming and debugging [ e . g . , 3 ] , to developing cognitive models of student program - ming knowledge [ e . g . , 33 , 49 ] , to evaluating novice programming environments [ 21 , 23 , 25 ] . While carrying forward its interest in studying students’ programming processes in detail , our work differs from this line of work in that it aims to make accurate advance predictions of course per - formance using learning analytics . 2 . 2 Learning Analytics In recent years , the ease with which data can be collected , coupled with the availability of low - cost , high - power machines to store and process such data , has led to an increased interest in the field of data analytics . For example , researchers have mined data repositories to describe tool usage [ 34 ] and to detect difficulty in programming tasks [ 19 ] . The application of data mining and analytics techniques to education is referred to as learning analytics [ 57 ] . Verbert and Duval [ 52 ] define learning analytics as a research area that “focuses on collecting traces that learners leave behind and using those traces to improve learning . ” They then go on to define two research approaches : one that focuses on identifying patterns of behavior and another that focuses on deriving interventions aimed at improving the learning process . In computing ed - ucation , the former approach has been used to describe compilation behavior [ e . g . , 4 , 31 ] and to identify behaviors associated with eventual success or failure in a course [ e . g . , 1 , 16 , 30 , 55 ] . In com - puting education , the latter approach ( developing learning interventions ) has been less explored , although there are some notable exceptions [ e . g . , 24 , 41 ] . Nevertheless , how to best approach this emerging field within the context of computing education remains an open question [ 57 ] . As this article is more aligned with research interested in identifying patterns of behavior , we now briefly review approaches to data collection and analysis that have been employed by learning analytics researchers . For a comprehensive review of this growing body of computing education research , see Ihantola et al . [ 28 ] . 2 . 3 Data Collection in Learning Analytics What can be accomplished through learning analytics depends heavily on the type and amount of data that can be collected . Data collection is often a technological constraint . For example , when storage and processing power are limited , it may only be possible for researchers to only take ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 4 A . S . Carter et al . snapshots of a single aspect of student behavior ( e . g . , compilation , see [ 30 ] ) . However , when such technological restrictions are lifted , it becomes possible to collect every keystroke ( e . g . , [ 38 ] ) or In - tegrated Development Environment ( IDE ) interaction ( e . g . , [ 11 ] ) made by every student . While this may seem to encompass the entirety of what researchers might collect from students , there are sev - eral additional sources of data that have not yet been explored . For example , researchers could col - lect mouse gestures ( see [ 5 ] ) , or behavioral data such as voice , facial expressions , or eye gaze ( e . g . , [ 13 ] ) . Alternatively , researchers may decide to move beyond the IDE as the data collection mecha - nism and instead consider general application usage . For example , it might be beneficial to record web searches so that we can better understand where students are getting stuck and how they are using online resources . We explore these considerations in more detail in a separate paper [ 26 ] . 2 . 4 Data Analysis Thus far , researchers have used either statistical analysis ( see [ 20 ] ) or machine learning ( see [ 12 ] ) to gain insights into the data collected . The majority of learning analytics research in computing education has attempted to develop statistical models that relate one or more behavioral charac - teristics to course outcomes . Two prior statistical predictors are the Error Quotient [ 30 , 50 ] and Watwin Score [ 54 , 55 ] . Both measures focus on quantifying a student’s ability to recover from compilation errors . This is accomplished by examining successive pairs of compilation attempts and awarding points based on whether later compilation attempts remove errors identified in ear - lier compilation attempts . In past studies , the Watwin Score has generally outperformed the Error Quotient as a predictive measure . While the Error Quotient was able to account for between 19 % [ 54 ] and 25 % [ 30 ] of the variance of final course grades , the Watwin score was able to account for between 36 % [ 54 ] and 42 % [ 55 ] of the variance in final course grades . However , a refinement of the Error Quotient published more recently appears to raise the Error Quotient’s predictive power to nearly 30 % [ 50 ] . In prior work , we introduced the NPSM [ 16 ] , which describes stu - dents’ programming behaviors using a series of programming states . In a preliminary analysis , the NPSM was able to predict students’ assignment grades with up to 41 % accuracy . Our work is similar to Blikstein et al . [ 9 ] , who also categorize students’ programming into a series of pro - gramming states . However , whereas Blikstein et al . ‘s states are derived from a rigorous analysis of a single assignment and thus are intended to be used to describe progress on that same as - signment , our states are generalized and intended to work equally well on any programming as - signment . Yet , in spite of this difference , Blikstein et al . also found a relationship between certain “sink” states and poor classroom performance . Section 3 discusses our state - based model in further detail . Recently , researchers have begun to employ machine learning as a method of analysis . For ex - ample , researchers have used machine learning to investigate the relationship between course out - comes and student factors ( e . g . , gender , see [ 1 ] ) , programming activity [ 1 ] , and patterns of typing [ 38 ] . Similarly , researchers have used machine learning to analyze common novice mistakes when writing SQL queries [ 2 ] . A weakness of this line of work is a lack of ground truth analysis—a tech - nique commonly applied in human activity recognition ( e . g . , [ 35 , 40 ] ) . In ground truth analysis , the selected machine - learning algorithm is trained on carefully labeled data obtained from controlled laboratory observations of participants performing tasks . Without obtaining a ground truth , ap - proaches utilizing machine learning become difficult to interpret and often lack generalizability . For computing education researchers , developing a ground truth will likely require laboratory ob - servations of students working on carefully constructed programming problems . We regard the establishment of a ground truth for learning analytics as an important future contribution to the research space . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 5 Table 1 . Dimensions of Program Correctness That Can Be Approximated from Programming Log Data Syntax Correct Incorrect S e m a n t i c s Incorrect Syntactically correct / Semantically incorrect Syntactically incorrect / Semantically incorrect Unknown Syntactically correct / Semantically unknown Syntactically incorrect / Semantically unknown 3 DEVELOPING A COMPREHENSIVE MODEL OF STUDENTS’ PROGRAMMING BEHAVIORS Prior measures of student achievement derived from programming behaviors ( e . g . , Error Quotient and Watwin ; see [ 30 , 55 ] ) have focused exclusively on students’ compilation activities . Students who quickly and accurately fix syntax errors in their programs are predicted to perform better than those who do not . While the ability to eliminate syntax errors from a program is an important programming skill , it is widely acknowledged that programming success also hinges on one’s ability to identify , diagnose , and repair runtime ( semantic ) errors ( see , e . g . , [ 3 ] ) . Thus , one would expect that an ability to eliminate semantic errors would also correlate positively with performance in a computing course . This observation motivates a more holistic predictive model of student performance rooted in a students’ ability to develop both syntactically and semantically correct programs . Our proposed model aims to approximate the syntactic and semantic correctness of a programming solution at any given point in time ( see Table 1 ) . Given a stream of programming data , we map a student’s current programming solution to one of the four states in this 2 × 2 space . We can determine the syntactic correctness of a program based on whether the last compilation attempt yielded an error . In contrast , semantic correctness is impossible to determine unequivocally . All we have is a rough proxy : the presence or absence of runtime exceptions in the last execution attempt . If the last execution attempt yielded a runtime exception , we classify the program as semantically in - correct . If the last execution attempt did not yield a runtime exception , we classify the program as semantically unknown . Clearly , our proxy for semantic correctness has significant limitations . For instance , a student’s program could meet the assignment specification ( and hence be “semantically correct” for the purpose of the assignment ) , but still raise a runtime exception if it encounters input data that it is not required to process . Conversely , a student’s program could run without raising a runtime exception , but its output could be incorrect . Likewise , the student could have failed to test key boundary cases that would have raised runtime exceptions . Figure 1 presents a state - transition diagram that maps our model to the stream of programming log data made available to us by Microsoft® Visual Studio® [ 53 ] , the IDE used in our study . Note that Visual Studio does not report runtime exceptions that occur outside of debug mode . Thus , our log data do not contain two potentially important transitions : those between RN / RU and YN / YU . For this reason , we are forced to determine the current state based on the results of the student’s last compilation and debug attempt . In order to switch from a syntactically incorrect state ( NU and NN ) to a syntactically correct state ( YN and YU ) , the student’s last compilation attempt must have been free of build errors . Likewise , a student switches from a semantically unknown state ( NU and NN ) to a semantically incorrect state ( YN and YU ) only if the last debug attempt yielded a runtime exception . Observe that intermediate execution states are also captured in this state transition diagram . If the student’s program is syntactically correct , it can be executed either with or without the ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 6 A . S . Carter et al . Fig . 1 . Programming state transition diagram . debugger in Visual Studio . This leads to the four leftmost “Execute” states in the diagram ( RN , RU , DN , DU ) . In contrast , if the student’s program is syntactically incorrect , it is not possible to execute it in debug mode . However , in Visual Studio , it is possible to execute the last successful build of a program . This leads to the rightmost “Execute” state ( R / ) . Two additional states are also necessary in this model . First , it is impossible to determine the state if no compilation or execution attempts have been made . To account for this situation , which commonly occurs at the beginning of a programming session , we define an additional state called “Unknown ( Start ) State” ( UU ) . Second , a prolonged period of inactivity ( which we established as 3 minutes ) in any state leads to a transition to the Idle state , in which the next editing activity causes a transition back to the previous state . Table 2 summarizes the 11 states in the model by providing a brief description of the signifi - cance of each state relative to the programming process . Most states are represented with a two - character code . The first character indicates the editing of a document coded based on syntactic correctness ( Y = correct , N = incorrect ) or non - editing behavior ( D = debugging , R = running ) . The second digit indicates our proxy for semantic correctness ( U = unknown semantic correctness , N = semantically incorrect ) . Thus , YU indicates that a student is presently editing a syntactically correct document whose semantic correctness is unknown . Similarly , DN indicates that a student is presently debugging a semantically invalid program . 3 . 1 Relating States to Student Performance The four editing states in the model just presented ( YN , YU , NU , NN ) serve as a rough proxy for the syntactic and semantic status of the program being edited . We hypothesize that students who ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 7 Table 2 . PSM States spend more of their time in syntactically correct and semantically unknown states will tend to outperform students who spend more of their time in syntactically and semantically incorrect states . The relationship between the five execution states in the model ( RN , RU , DN , DU , R / ) and student performance is murkier . Students in the RN and DN states appear to be asking the question , “Why doesn’t my program work ? ” However , students who are in state DN may be approaching that question from a more powerful position , since they are using the debugger . In a similar vein , students in the RU and DU states seem to be asking the question , “Does my program work ? ” Once again , those who choose to use the debugger ( in the DU state ) appear to be asking that question from a more powerful position . Finally , it is difficult to say just what students in the R / state are up to . We suspect many of them may not realize that they are actually executing a previous build of the program . Therefore , we hypothesize that time spent in this state may indicate that a student is struggling ; we would not expect time spent in this state to be positively associated with course performance . 3 . 2 Deriving an Explanatory Model : NPSM The programming model presented in Figure 1 maps , on a moment - to - moment basis , a student’s programming activity to one of the 11 different states . Given our intuitions about how states might correlate with student performance , how can the model be used as a basis for explaining the vari - ance in student achievement ? We begin by adopting the most straightforward approach : For each student , we record the amount of time spent in each state , and then normalize the times relative to the total time the student spent programming . In this model , each state ( e . g . , YU ) is treated as a predictor variable in statistical regression . Each variable is assigned a coefficient ranging from 0 ( no time spent in that state ) to 100 ( all time spent in that state ) . As these values have been nor - malized relative to the total time spent programming , the sum of all predictor variable coefficients will equal 100 . In a preliminary analysis of data generated from our normalized model , we observed that the Idle state tended to dominate student activity ; students tended to spend most of their day not programming . As we wanted to focus exclusively on the time students spent programming , we decided to eliminate the Idle state from the normalization process . The remaining 10 data points form the NPSM . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 8 A . S . Carter et al . Table 3 . Explanation of Variance in Individual Assignment Grades In Carter et al . [ 16 ] , we examined the NSPM’s ability to predict course grades for a given student in a computing course . The next section summarizes this work . 4 PREDICTING STUDENT PERFORMANCE IN COMPUTING COURSES BASED ON PROGRAMMING BEHAVIOR How well does the NPSM predict course achievement ? Furthermore , how does the NPSM’s pre - dictive capabilities compare to the Error Quotient and Watwin measures , two popular models that only consider compilation behavior ? In this section , we consider both of these questions . 4 . 1 Methodology Programming and grade data for this analysis were collected from a 15 - week CS 2 course offered during the Spring 2014 semester at Washington State University . In total , 140 students were en - rolled in the course , 129 of whom completed the course . Of these , 95 students ( 87 male , 8 female ) consented to release their programming log data and course grades for analysis in this study . Taught by the first author , the course used C + + as its instructional language , and required stu - dents to use the Microsoft® Visual Studio® programming environment [ 53 ] for course assign - ments . Weekly course activity consisted of three 50 - minute lectures and one 170 - minute lab . Students’ programming activities were collected using OSBIDE [ 15 ] , a plugin for Microsoft® Visual Studio® . Three performance indicators were used for the analysis : ( 1 ) students’ grades on individual assignments ; ( 2 ) students’ overall assignment average , and ( 3 ) students’ final grades , which were based on the grades received on programming assignments ( 35 % ) , labs ( 10 % ) , par - ticipation ( 5 % ) , in - class quizzes ( 10 % ) , midterm exams ( 20 % ) , and a final exam ( 20 % ) . Predictions of individual assignment grades were based exclusively on the programming log data generated while the corresponding assignment was open . ( The length of each assignment varied between 10 and 23 days ) . Predictions of students’ overall assignment averages and final grades , in contrast , were based on programming log data generated throughout the entire semester . 4 . 2 Results To evaluate the ability of the NPSM to explain the variance in individual assignment grades , we performed a linear regression with variables within the NPSM acting as predictor variables and individual assignment grades as the outcome variable . We then performed linear regression on students’ computed Error Quotient and Wawin scores based on the algorithms reported in Jadud [ 29 ] and Watson [ 55 ] . The result of this analysis is presented in Table 3 . We next aggregated an entire semester’s worth of IDE data and correlated these data with stu - dents’ overall assignment averages ( Table 4 ) . Regression analysis detected a significant relation - ship between all three measures and a student’s assignment . In our final statistical analysis , we examined the relationship between each measure and final course grades ( Table 5 ) . Regression analysis detected significant relationships between NPSM and final course grades , and between Watwin and final course grades . However , statistical analysis failed to detect a significant relationship between the Error Quotient and final course grades . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 9 Table 4 . Explanation of Variance in Average Assignment Grades Table 5 . Explanation of Variance in Final Grades 4 . 3 Discussion Our results would seem to indicate that the NPSM can be used as a moderately successful pre - dictor for student achievement on homework assignments . RN ( execute a semantically incorrect program ) and RU ( execute a semantically unknown program ) were found to be positive contribu - tors to student success . Conversely , UU ( default state before first compilation / execution action is taken in a programming session ) and NU ( syntactically incorrect program ) were found to be neg - ative contributors to student success . This seems to indicate that toying with a program’s runtime behavior , regardless of semantic correctness , is a successful programming approach . In contrast , we find that writing large portions of code without attempting to compile ( UU ) is not correlated with success . It is easy to imagine that when these students finally do compile , they quickly find themselves in NU , the other state negatively correlated with performance . Furthermore , the sig - nificance of NU as an explanatory factor aligns well with the Error Quotient and Watwin Score [ 54 ] , two prior predictive measures , as both can be seen as quantifying how students leave the NU state . In the next section , we directly compare the NPSM , Error Quotient , and Watwin measures . On this particular dataset , the NPSM is a better predictor than the Error Quotient or Watwin Score . As mentioned previously , the calculations performed by both the Error Quotient and Watwin measures are based on the least weighted significant contributor in the NPSM model : NU . Interestingly , performing a linear regression with NU as the sole predictor variable explains more variance than either the Error Quotient or Watwin Score for both assignment average , F ( 1 , 93 = 15 . 06 ) , p < 0 . 01 , Adj . R 2 = 0 . 13 , and final grade , F ( 1 , 93 = 23 . 676 ) , p < 0 . 01 , Adj . R 2 = 0 . 19 . This strongly suggests that any measurement based on programming behaviors would do well to look beyond compilation behavior . We explore this possibility further in the next section . It should be noted that the results for the Error Quotient and Watwin Score presented in this section differ substantially from the results presented in prior research . A recent study of both the Error Quotient and Watwin measures accounted for 18 % and 36 % of the variance in students’ final grades [ 54 ] as compared to merely 3 % and 12 % in our study ( Table 5 ) . How can we account for this large discrepancy ? We offer two possible explanations . First , differences in the instructional emphasis of the courses studied might have contributed to the differences in the Error Quotient and Watwin Score observed across the studies . In previous studies in which the Error Quotient and Watwin Score were calculated , student homework was worth just 25 % of the overall grade . In contrast , in our study , student homework accounted for 35 % of the overall grade . Second , the discrepancies in Error Quotient and Watwin Score measures might be related to key differences in the programming environments and languages used in the studies . Previous ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 10 A . S . Carter et al . studies focused on the BlueJ [ 36 ] and the Java programming language . In contrast , our study fo - cused on Microsoft Visual Studio and the C + + programming language . Both the Error Quotient and Watwin Score rely on the processing of compilation error messages . Given that C + + compil - ers tend to produce terser and more obtuse compilation error messages , it seems plausible that differences could have occurred with respect to students’ compilation behaviors in the two en - vironments . For example , forgetting a semicolon in BlueJ and Java results in the error message , “error : ‘ ; ’ expected , ” followed by the exact line on which a semicolon is missing . In contrast , for - getting a semicolon in Visual Studio and C + + results in nine error messages . The first message is a red - herring referencing an illegal usage of a type as an expression . For the actual cause , the user must look to the second error message , which states , “syntax error : missing ‘ ; ’ before identifier < x > , ” with < x > being the line below the statement on which a semicolon is missing . Of these two explanations , we find the second one to be the most compelling . Recall that both the Error Quotient and Watwin Score assign penalty points when subsequent compilation attempts either result in more errors , or contain the same error messages as previous compilation attempts . Given that Visual Studio and C + + generate more error messages per compilation , it stands to reason that the Error Quotient and Watwin would artificially inflate the base penalty assigned to students for each failed compilation . Furthermore , in Visual Studio / C + + , the possibility that both the Error Quotient and Watwin Score will generate false positives ( matched compilations that have the same error message but for different reasons ) increases . In contrast , the coarser approach taken by the NPSM is not affected by these differences : an error state is an error state , regardless of whether a student generated 1 or 100 errors in a given compilation . In order to increase our confidence in this explanation , we would need to conduct additional studies of the Error Quotient , Watwin Score , and NPSM using a variety of programming environments , languages , and course offerings . 5 PREDICTING STUDENT PERFORMANCE IN COMPUTING USING SOCIAL BEHAVIOR In the prior section , we attempted to predict student achievement based solely on students’ pro - gramming data . However , social cognitive theory [ 7 ] suggests that social factors heavily contribute to learners’ eventual success or failure . Indeed , prior work indicates a strong relationship between socially based affective measures and persistence within computing ( see [ 45 ] ) . While such relation - ships exist , the discovery of any differences often comes at the end of a semester , often through an end - of - course survey . While informative , this methodology prevents educators from gaining in - sights early enough to help struggling students . In this section , we consider a measure that can be automatically calculated several times throughout a given semester . Note that this section adapts work originally presented in Hundhausen et al . [ 27 ] so that it can be used in the service of devel - oping improved predictive measures . 5 . 1 Methodology For this analysis , we consider two offerings of CS2 , the same course considered in prior sections . One of these offerings ( Course 1 ) used the OSBIDE social programming environment [ 17 ] , while the other ( Course 2 ) used OSBLE [ 42 ] , a more traditional course management system . Course 1 enrolled 140 students , 129 of whom finished the course and received a grade . 108 of these students ( 100 men , 8 women ) consented to releasing their data . Course 2 took place during the 15 - week fall semester of 2013 and enrolled 123 students , 110 of whom completed the course and consented to release their grades ( 99 male , 11 female ) . Based on social learning theory [ 37 ] , which emphasizes the importance of regular participation within a learning community , we decided to construct a measure that describes students’ general ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 11 Table 6 . Definition of Participation Level Measure Table 7 . Extent to Which Participation Level Predicted Course Grades ( * = sig . at p < 0 . 05 ) social participation within a given window of time . In this study , we decided to make the timespan equal to 2 weeks—the average length of time given to students to complete programming assign - ments . Our participation level metric , described in Table 6 , ranges from 1 to 4 and is based on the number of posts and replies made by a student in a given 2 - week interval . This timeframe cor - responds with the approximate release of each new programming assignment in both conditions . The participation levels in the highest range ( 4 ) were selected because it aligned with Course 1’s participation requirement . We refer to this measure as a student’s Participation Level . For each course , we generated a timeline of each student’s social activity . In order to generate the Participation Level measure , we recorded posts and replies separately . These numbers were then averaged for the entire term and compared with each student’s final grade . 5 . 2 Results In order to determine whether or not regular participation was a reliable predictor of academic success , we considered each student’s semester - long Participation Level average . In the case of Course 1 , in which participation was mandatory , we were concerned that Participation Level might co - vary with prior computing grades : since there was academic incentive to participate in the discussion , higher - achieving students might be more motivated to participate . Indeed , in Course 1 , a correlational analysis between a student’s level of participation and prior grade in CS 1 was found to be significant ( r = 0 . 468 , p < 0 . 001 ) . However , in Course 2 , this was not the case ( r = 0 . 12 , p = 0 . 26 ) . Hence , we decided to retain prior CS 1 grade as a covariate in further analyses with Course 1 ( by using a MANCOVA ) , whereas we used a MANOVA for Course 2 , adding prior CS 1 grade as a separate independent variable for comparison purposes . Tables 7 and 8 present the results of these analyses . As Table 8 shows , CS 1 grade was not a significant predictor of student performance in Course 1 . In contrast , in Course 2 , students’ prior CS 1 grade was a significant predictor of students’ exam scores and final grade . The partial eta squared ( η 2 ) values indicate that the strength of the relationship between CS 1 grade and these two items was weak . Table 7 tells a slightly different story : Participation level predicted student performance with respect to all graded items and the final grade in both groups . Moreover , in all cases , the partial eta squared values indicate that participation level was a stronger predictor of student grades than ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 12 A . S . Carter et al . Table 8 . Extent to Which CS 1 Grade Predicted Course Grades ( * = sig . at p < 0 . 05 ) was prior CS 1 grade , as it accounted for roughly twice the variance . In both groups , a between - subjects ANOVA detected a statistically significant difference between the quartiles ( Course 1 : df = 3 , F = 11 . 62 , p < 0 . 001 ; Course 2 : df = 3 , F = 8 . 15 , p < 0 . 001 ) . 5 . 3 Discussion These results suggest that regular online social participation does , in fact , predict student grades in both treatments : Students who take a more active role in the activity stream tend to do better in the course . One possible explanation is that students who make and reply to posts are participating in valuable learning activities ; active participation in the activity stream actually enhances learning . This explanation is favored by social learning theories . Indeed , according to Self - Efficacy Theory [ 7 ] , while vicarious learning experiences in which students observe others without actively partic - ipating are seen as valuable , enactive learning experiences in which students actively participate are regarded as essential . 6 USING MULTI - FACTOR MODELS OF STUDENT BEHAVIOR TO PREDICT PERFORMANCE In the prior two sections , we considered statistical predictors of student achievement from two dis - parate data sources : programming log data and online social activity . Both measures were found to significantly correlate with student achievement . Might it be beneficial to combine program - ming and social measures into a single , multi - factor measure of student success ? In this section , we consider the statistical impact of incorporating our Participation Level measure discussed in the prior section into the programming - based predictive measures discussed in Section 4 . 6 . 1 Methodology For this analysis , we again draw on data from the dataset used in prior sections . This data was collected during the spring 2014 offering of CptS 122 ( CS2 ) at Washington State University . The course enrolled 140 students , 129 of whom finished the course and received a grade . 108 of these students ( 100 men , 8 women ) consented to releasing their data . For each programming measure , we incorporated the Participation Level measure into the fi - nal output of each programming measure . Again , data for all measures were collected using the OSBIDE social programming environment [ 17 ] . Three performance indicators were used for this analysis : ( 1 ) students’ grades on individual assignments ; ( 2 ) students’ overall assignment average , and ( 3 ) students’ final grades , which were based on the grades received on programming assign - ments ( 35 % ) , labs ( 10 % ) , participation ( 5 % ) , in - class quizzes ( 10 % ) , midterm exams ( 20 % ) , and a final exam ( 20 % ) . Predictions of individual assignment grades were based exclusively on the pro - gramming log data generated while the corresponding assignment was open . ( The length of each assignment varied between 10 and 23 days ) . Predictions of students’ overall assignment averages ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 13 Table 9 . Explanation of Variance in Individual Assignment Grades Table 10 . Explanation of Variance in Average Assignment Grades Table 11 . Explanation of Variance in Final Grades and final grades , in contrast , were based on programming log data generated throughout the entire semester . 6 . 2 Combining Programmatic and Social Measures To evaluate the ability of the two measures to explain the variance in individual assignment grades , we replicate the basic statistical procedures reported in Section 4 . However , this time , we add the Participation Level measure to each linear regression . Tables 9 , 10 , and 11 report the statistical results for each measure on a given individual assign - ment , overall assignment average , and final grades , respectively . Figures 2 , 3 , and 4 compare these results to the original results reported in Section 4 . With the exception of Error Quotient’s pre - diction of final grade , combining a programming - based measure with a socially based measure produces a statistical model that accounts for more variance than any measure does in isolation . Indeed , in most cases , we find that a simple measure of social participation is able to account for more variance than either Error Quotient or Watwin . 6 . 2 . 1 Exploring the NPSM and SNPSM . In the prior section , we noticed a marked increase in the NPSM’s predictive capabilities with respect to final grade when it incorporated the Participa - tion Level measure . In this section , we consider how the explanatory power of both the original NPSM and the NPSM augmented with Participation Level ( henceforth known as SNPSM , or “So - cial NPSM” ) vary over time . For the present analysis , we evaluated the NPSM using seven input datasets whose sizes were systematically varied , as illustrated in Figure 5 . The first dataset con - sisted solely of the data collected during the first programming assignment . The final six datasets each added an additional assignment’s worth programming data . It follows that the final dataset included all programming data from the semester . Significance values for each NPSM variable , as well as the total variance explained by the model , are presented in Table 12 . Significance values for SNPSM variables , as well as the total variance explained for each dataset , are presented in Table 13 . Figure 6 provides a visual comparison be - tween the two predictive models . In all cases , the SNPSM explains between 11 % and 16 % more ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 14 A . S . Carter et al . Fig . 2 . Comparing variance for each statistical measure with and without Participation Level for a single assignment . Fig . 3 . Comparing variance for each statistical measure with and without Participation Level for all assignments . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 15 Fig . 4 . Comparing variance for each statistical measure with and without Participation Level for final grade . Fig . 5 . Seven programming datasets of increasing size as a percentage of all programming data . Table 12 . NPSM Predictive Power and Coefficients for Final Grade using Seven Datasets of Increasing Size ( * = sig at p < 0 . 05 ) ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 16 A . S . Carter et al . Table 13 . SNPSM Predictive Power and Coefficients for Final Grade using Seven Datasets of Increasing Size ( * = sig at p < 0 . 05 ) Fig . 6 . Final grade variance explained by NPSM and SNPSM models . variance than the standard NPSM model . Likewise , UU only becomes a significant predictor with larger datasets . In contrast , NU and Social Role ( for SNPSM ) are significant predictors regardless of the size of the dataset . Using just the first dataset , the SNPSM explains 28 % of the variance in final grades . In com - parison , the NPSM model requires five assignments’ worth of data in order to achieve this level of explanatory power . In practical terms , this means that educators could use the SNPSM model to drive educational interventions much earlier in the semester—possibly before students have a chance to become severely discouraged . 6 . 3 Discussion In this section , we have quantified how statistical models composed of both social and program - ming measures could be used to model students’ classroom performance . To this end , we combined the Social Role measure explored in Section 5 with three programming - based measures . In every case in which the programming - based measure was statistically significant ( this excludes Error Quotient’s correlation with final grade ) , we see an additive effect in explanatory power . These results would seem to suggest that measures derived from a single data source ( e . g . , social data , programming data ) are likely to overlook explainable statistical variance . Therefore , it would seem prudent for researchers interested in the future development of predictive measures to consider the construction of more holistic models . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 17 In Section 6 . 2 , we reran our analysis using a combined model that incorporated Participation Level into the NPSM ( abbreviated SNPSM ) . The SPSM was able to explain up to 2 % more of the variance in assignment grades , and up to 26 % more of the variance in final grades . Moreover , introducing Participation Level as a predictor for final grades allowed us to account for 28 % ( vs . 11 % without ) of the variance when using just a single assignment’s worth of data . 7 LIMITATIONS While quasi - experimental research aims to minimize the number of confounds , it cannot com - pletely eliminate them . In this section , we discuss potential threats to validity using the method - ology outlined by Shadish et al . [ 47 ] . 7 . 1 Threats to Statistical Conclusion Validity This work likely suffers from an unreliable treatment implementation , meaning that not all stu - dents interacted with the SPE in equal proportion . Indeed , our results indicate that usage varied significantly between students . However , in order to assure a minimum level of usage among all students , we required students to install the SPE and to make a minimum number of posts and replies in the system . Yet , in doing so , we lowered our study’s internal validity . 7 . 2 Threats to Internal Validity As mentioned previously , requiring students to install and use the SPE introduces a potential con - found when we draw relationships between social participation within the SPE and academic per - formance . A critic might argue that any relationship involving SPE usage is merely a proxy for a student’s motivation to succeed in the class . To address this criticism , we included prior CS1 grade in our analysis , our proxy for motivation , as a covariate when appropriate . Of the 140 students enrolled in the primary course considered in our study , 11 students withdrew and 21 did not sign our study’s informed consent . Therefore , our analysis was conducted using 77 % of the possible student body . Given these figures , attrition is likely to be a threat to our internal validity . As such , it could be argued that any observable gains in outcome variables are simply a result of lower - performing students dropping out of the course . 7 . 3 Threats to External Validity This work suffers from threats to external validity . In particular , there exists a data collection bias as most of the courses considered in this article were taught by the primary author . Furthermore , as our data suggest , existing predictive measures have demonstrated a lack of generalizability [ 43 ] . Therefore , it remains a possibility that the statistics developed throughout this article will not generalize beyond our sample of university students enrolled in the spring 2014 offering of CS2 , the C + + language , or Visual Studio . The only way to address this limitation is to perform additional replication studies with different student populations . 8 CONCLUSION AND FUTURE WORK This article makes two contributions to the growing body of research related to predictive models of academic success within a computing course . First , we performed a replication study of two existing predictive models : the Error Quotient [ 30 ] and the Watwin Score [ 54 , 55 ] . Confirming other research [ 1 , 43 ] , our study indicated that these measures’ predictive capabilities vary widely based on setting . In our particular configuration of programming language ( C + + ) , development environment ( Visual Studio ) , and course ( CS2 ) , these measures performed much worse than what has been previously reported in the literature . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 18 A . S . Carter et al . Our second contribution relates to the modeling of student behaviors . Prior research has focused on relating low - level characteristics such as keystrokes [ 1 ] , typing behavior [ 38 ] , and compilation behavior [ 30 , 54 ] with performance . Rather than continuing this line of work , we instead con - structed a theoretical model of students’ programming activities , the Programming State Model , which describes students’ problem solving behavior by placing a student in 1 of 11 possible pro - gramming states . From the PSM , we derived a predictive measure that outperformed both the Error Quotient and Watwin Score on our dataset . In the final section of this article , we extended three programming - based measures by incor - porating an additional measure derived from a student’s online social participation . All three ex - tended models proved to be more accurate at predicting students’ course grades . These results suggest that future researchers would do well to look beyond basic programming behaviors ( e . g . , keystrokes ) and develop more intricate models of student learning . While we consider social par - ticipation in our model , we believe that future models might benefit equally from the incorporation of other student characteristics . While the PSM provided new insights into students’ programming behaviors and demonstrated the potential of holistic models , several interesting questions remain unanswered . For example , recall that our analysis of the NPSM revealed what might be a hard ceiling in the explanatory power of the model . We would like to run a replication study under similar circumstances to see if a ceiling is again observed . In addition , we would like to investigate alternate states and factors that might influence the PSM’s predictive powers . At present , the PSM only describes programming behaviors . Yet , given the results presented in the article , we wonder how we might better incorporate social participation into the PSM . In doing so , we might discover , for example , that students who are in the NU state and ask questions transition to a more productive state ( e . g . , YU ) more quickly than students who don’t ask questions . Previous work that studied the interplay of social and programming behavior suggests that relationships like this one are highly plausible ( see [ 18 ] ) . Likewise , we wonder if we can add more descriptive power to the PSM by splitting existing states based on other factors . For example , the work by Carter and Prasun [ 19 ] might allow us to better categorize editing states based on programming difficulties that are not currently detectable by the PSM . Lastly , we plan to explore how the NPSM might serve as a foundation for pedagogical interven - tions derived from a student’s PSM state . For example , a student who appears to be stuck in an unhelpful state ( e . g . , NU ) might be prompted to ask for help . Alternatively , we might be able to use programming behavior to encourage students to improve their programming techniques . For ex - ample , for students who spend a lot of time in the RN ( execute without debug ) state , an intervention could suggest using the debugger ( DN ) to troubleshoot semantic issues . For instructors , we envision an online dashboard that could present continuously updated infor - mation on students’ NPSM states and programming progress . Using this information , instructors could check in on struggling students , or perhaps devote additional lecture time to topics or strate - gies that the dashboard indicates may be problematic for many students . REFERENCES [ 1 ] A . Ahadi , R . Lister , H . Haapala , and A . Vihavainen . 2015 . Exploring machine learning methods to automatically identify students in need of assistance . In Proceedings of the 11th Annual International Conference on International Computing Education Research . [ 2 ] A . Ahadi , V . Behbood , A . Vihavainen , J . Prior , and R . Lister . 2016 . Students’ syntactic mistakes in writing seven different types of SQL queries and its application to predicting students’ success . In Proceedings of the 47th ACM Technical Symposium on Computing Science Education . [ 3 ] M . Ahmadzadeh , D . Elliman , and C . Higgins . 2005 . An analysis of patterns of debugging among novice computer science students . Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education ( ITiCSE’05 ) . ACM Press . 84 – 88 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . Blending Measures of Programming and Social Behavior into Predictive Models 12 : 19 [ 4 ] A . Altadmri and N . C . C . Brown . 2015 . 37 million compilations : Investigating novice programming mistakes in large - scale student data . In Proceedings of the 46th ACM Technical Symposium on Computer Science Education . 522 – 527 . [ 5 ] I . Arapakis , M . Lalmas , and G . Valkansas . 2014 . Understanding within - content engagement through pattern anal - ysis of mouse gestures . In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management . [ 6 ] R . S . J . Baker and G . Siemens . 2014 . Educational data mining and learning analytics . The Cambridge Handbook of the Learning Sciences . Cambridge University Press . 253 – 274 . [ 7 ] A . Bandura . 1997 . Self - Efficacy : The Exercise of Control . Worth Publishers . [ 8 ] S . Bergin , R . Reilly , and D . Traynor . 2005 . Examining the role of self - regulated learning on introductory programming performance . In Proceedings of the 2005 ACM International Computing Education Research Workshop . ACM . 81 – 86 . [ 9 ] P . Blikstein , M . Worsley , C . Piech , A . Gibbons , M . Sahami , andS . Cooper . 2014 . Programmingpluralism : Usinglearning analytics to detect patterns in novices’ learning of computer programming . International Journal of the Learning Sciences . 23 , 4 ( 2014 ) , 561 – 599 . [ 10 ] J . Bransford , A . L . Brown , andR . R . Cocking , Eds . 1999 . HowPeopleLearn : Brain , Mind , Experience , andSchool . National Academy Press . [ 11 ] N . C . C . Brown , M . Kolling , D . McCall , and I . Utting . 2014 . Blackbox : A large scale repository of novice programmers’ activity . In Proceedings of the 45th ACM Technical Symposium on Computer Science Education . [ 12 ] A . Bulling , U . Blanke , and B . Schiele . 2014 . A tutorial on human activity recognition using body - worn inertial sensors . ACM Computing Surveys . 46 , 3 ( Jan . 2014 ) . [ 13 ] T . Busjahn , C . Schulte , B . Sharif , Simon , A . Begel , M . Hansen , R . Bednarik , P . Orlov , G . Shchekotova , andM . Antropova . 2014 . Eye tracking in computing education . In Proceedings of the 10th Annual Conference on International Computing Education Research ( Glasgow , Scotland , 2014 ) . [ 14 ] P . F . Campbell and G . P . McCabe . 1984 . Predicting the success of freshmen in a computer science major . CommunI - cations of the ACM . 27 , 11 ( 1984 ) , 1108 – 1113 . [ 15 ] A . S . Carter . 2012 . Supporting the virtual design studio through social programming environments . In Proceedings of the 9th Annual International Conference on International Computing Education Researc , 157 – 158 . [ 16 ] A . S . Carter , C . D . Hundhausen , and O . Adesope . 2015 . The normalized programming state model : Predicting student performance in computing courses based on programming behavior . In Proceedings of the 11th Annual International Conference on International Computing Education Research . ACM . 141 – 150 . [ 17 ] A . S . Carter and C . D . Hundhausen . 2015 . The design of a programming environment to support greater social aware - ness and participation in early computing courses . Journal of Computing Sciences in Colleges . 31 , 1 ( 2015 ) , 143 – 153 . [ 18 ] A . S . Carter and C . D . Hundhausen . 2016 . With a little help from my friends : An empirical study of the interplay of students’ social activities , programming activities , and course success . In Proceedings of the 2016 ACM Conference on International Computing Education Research . ACM . 201 – 209 . [ 19 ] J . Carter and D . Prasun . 2010 . Are you having difficulty ? In Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work . [ 20 ] A . Field . 2013 . Discovering Statistics Using IBM SPSS Statistics . [ 21 ] D . R . Goldenson and B . J . Wang . 1991 . Use of structure editing tools by novice programmers . Empirical Studies of Programmers : Fourth Workshop . Ablex . 99 – 120 . [ 22 ] M . J . Graham , J . Federick , A . Byers - Winston , A . B . Hunber , and J . Handelsman . 2013 . Increasing persistence of college students in STEM . Science . 341 , 27 ( Sept . 2013 ) , 1455 – 1456 . [ 23 ] M . Guzdial . 1994 . Software - realized scaffolding to facilitate programming for science learning . Interactive learning Environments . 4 , 1 ( 1994 ) , 1 – 44 . [ 24 ] B . Hartmann , D . MacDougall , J . Brandt , and S . R . Klemmer . 2010 . What would other programmers do : Suggesting solutions to error messages . In Proceedings of the 28th Conference on Human Factors in Computing Systems . ACM . 1019 – 1028 . [ 25 ] C . D . Hundhausen , J . L . Brown , S . Farley , andD . Skarpas . 2006 . Amethodologyforanalyzingthetemporalevolutionof novice programs based on semantic components . In Proceedings of the 2006 ACM International Computing Education Research Workshop . ACM . 45 – 56 . [ 26 ] C . D . Hundhausen , D . M . Olivares , and A . S . Carter . 2017 . IDE - Based learning analytics for computing education : A process model , critical review , and research agenda . Transactions on Computing Education ( 2017 ) . [ 27 ] C . D . Hundhausen , A . S . Carter , and O . Adesope . 2015 . Supporting programming assignments with activity streams : An empirical study . In Proceedings of the 46th ACM Technical Symposium on Computer Science Education . 320 – 325 . [ 28 ] P . Ihantola , A . Vihavainen , A . Ahadi , M . Butler , J . Borstler , S . Edwards , E . Isohanni , A . Korhonen , A . Petersen , K . Rivers , M . A . Rubio , J . Sheard , B . Skupas , J . Spacco , C . Szabo , and D . Toll . . 2015 . Educational data mining and learning analytics in programming : Literature review and case studies . In Proceedings of the 2015 ITiCSE on Working Group Reports ( New York , NY , USA , 2015 ) , 41 – 63 . ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 . 12 : 20 A . S . Carter et al . [ 29 ] M . C . Jadud . 2006 . An Exploration of Novice Compilation Behaviour in Bluej . [ 30 ] M . C . Jadud . 2006 . Methods and tools for exploring novice compilation behaviour . In Proceedings of the 2nd Interna - tional Workshop on Computing Education Research . ACM . 73 – 84 . [ 31 ] M . C . Jadud and B . Dorn . 2015 . Aggregate compilation behavior : Findings and implications from 27 , 698 Users . In Proceedings of the 11th Annual International Conference on International Computing Education Research . [ 32 ] D . Jeske , C . Stamov - Rossnagel , and J . Backhaus . 2014 . Learner characteristics predict performance and confidence in e - Learning : An analysis of user behavior and self - evaluation . Journal of Interactive Learning Research . 25 , 4 ( 2014 ) , 509 – 529 . [ 33 ] C . M . Kessler and J . R . Anderson . 1986 . A model of novice debugging in LISP . Empirical Studies of Programmers . 198 – 212 . [ 34 ] G . Khodabandelou , C . Hug , R . Deneckere , and C . Salinesi . 2014 . Unsupervised discovery of intentional process models from event logs . In Proceedings of the 11th Working Conference on Mining Software Repositories ( 2014 ) . [ 35 ] E . Kim , S . Helal , and D . Cook . 2010 . Human activity recognition and pattern discovery . Pervasive Computing . 9 , 1 ( Jan . 2010 ) , 48 – 53 . [ 36 ] M . Kölling , B . Quig , A . Patterson , and J . Rosenberg . 2003 . The BlueJ system and its pedagogy . Journal of Compuer Science Education . 13 , 4 ( 2003 ) , 249 – 268 . [ 37 ] J . Lave and E . Wenger . 1991 . Situated Learning : Legitimate Peripheral Participation . Cambridge University Press . [ 38 ] J . Leinonen , K . Longi , A . Klami , and A . Vihavainen . 2016 . Automatic inference of programming performance and experience from typing patterns . Proceedings of the 47th ACM Technical Symposium on Computing Science Education . [ 39 ] W . Ma , O . O . Adesope , J . C . Nesbit , and Q . Liu . 2014 . Intelligent tutoring systems and learning outcomes : A meta - analytic survey . Journal of Educational Psychology . 106 , 2007 ( 2014 ) , 901 – 918 . [ 40 ] B . Minor , J . R . Doppa , and D . J . Cook . 2015 . Data - driven activity prediction : Algorithms , evaluation methodology , and applications . In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( 2015 ) . [ 41 ] D . Mujumdar , M . Hallenbach , B . Liu , and B . Hartmann . 2011 . Crowdsourcing suggestions to programming problems for dynamic web development languages . CHI’11 Extended Abstracts on Human Factors in Computing Systems , 1525 – 1530 . [ 42 ] Online Studio - Based Learning Environment . 2012 . Retrieved Jan . 14 , 2014 from https : / / osble . org / . [ 43 ] A . Peterson , J . Spacco , and A . Vihavainen . 2015 . An exploration of error quotient in multiple contexts . In Proceedings of the 15th Koli Calling Conference on Computing Education Research . [ 44 ] M . B . Rosson , J . M . Carroll , and H . Sinha . 2011 . Orientation of undergraduates toward careers in the computer and information sciences : Gender , self - efficacy and social support . ACM Transactions on Computing Education . 11 , 3 ( Oct . 2011 ) , 1 – 23 . [ 45 ] M . B . Rosson , J . M . Carroll , and H . Sinha . 2011 . Orientation of undergraduates toward careers in the computer and information sciences : Gender , self - efficacyand social support . Transactions on Computing Education 11 , 3 ( 2011 ) , 1 – 23 . [ 46 ] D . H . Schunk . 2012 . Learning Theories : An Educational Perspective . Merrill Prentice Hall . [ 47 ] W . R . Shadish , T . D . Cook , and D . T . Campbell . 2002 . Experimental and Quasi - Experimental Designs for Generalized Causal Inference . Houghton Mifflin Company . [ 48 ] R . E . Slavin . 2011 . Educational Psychology : Theory and Practice . Pearson Education . [ 49 ] J . C . Spohrer . 1992 . MARCEL : Simulating the Novice Programmer . Ablex . [ 50 ] E . S . Tabano , M . M . T . Rodrigo , andM . C . Jadud . 2011 . Predictingat - risknovicejavaprogrammersthroughtheanalysis of online protocols . In Proceedings of the 7th International Workshop on Computing Education Research , 85 – 92 . [ 51 ] U . S . Department of Education , Office of Educational Technology . 2012 . Enhancing Teaching and Learning through Educational Data Mining and Learning Analytics : An Issue Brief . [ 52 ] K . Verbert and E . Duval . 2012 . Learning analytics . Learning and Education . 1 , 8 ( 2012 ) . [ 53 ] Visual Studio - Microsoft Developer Tools . 2015 . Retrieved April 20 , 2015 from http : / / www . visualstudio . com . [ 54 ] C . Watson , F . W . B . Li , and J . L . Godwin . 2014 . No tests required : Comparing traditional and dynamic predictors of programming success . In Proceedings of the 45th ACM Technical Symposium on Computer Science Education . ACM . 469 – 474 . [ 55 ] C . Watson , F . W . B . Li , and J . L . Godwin . 2013 . Predicting performance in an introductory programming course by logging and analyzing student programming behavior . In Proceedings of the 2013 IEEE 13th International Conference on Advanced Learning Technologies ( 2013 ) , 319 – 323 . [ 56 ] B . C . Wilson and S . Shrock . 2001 . Contributing to success in an introductory computer science course : A study of twelve factors . SIGCSE Bulletin 33 , 1 ( 2001 ) , 184 – 188 . [ 57 ] A . F . Wise . 2014 . Designing pedagogical interventions to support student use of learning analytics . In Proceedings of the 4th International Conference on Learning Analytics and Knowledge , 203 – 211 . Received September 2016 ; revised June 2017 ; accepted June 2017 ACM Transactions on Computing Education , Vol . 17 , No . 3 , Article 12 . Publication date : August 2017 .