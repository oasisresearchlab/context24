A NALOGY KB : Unlocking Analogical Reasoning of Language Models with A Million - scale Knowledge Base Siyu Yuan ♥∗ , Jiangjie Chen ♠∗ , Changzhi Sun ♦ , Jiaqing Liang ♥† , Yanghua Xiao ♠♣ , Deqing Yang ♥† ♥ School of Data Science , Fudan University ♠ Shanghai Key Laboratory of Data Science , School of Computer Science , Fudan University ♦ ByteDance AI Lab ♣ Fudan - Aishu Cognitive Intelligence Joint Research Center syyuan21 @ m . fudan . edu . cn , jjchen19 @ fudan . edu . cn sunchangzhi @ bytedance . com { liangjiaqing , shawyh , yangdeqing } @ fudan . edu . cn Abstract Analogical reasoning is a fundamental cogni - tive ability of humans . However , current lan - guage models ( LMs ) still struggle to achieve human - like performance in analogical reason - ing tasks due to a lack of resources for model training . In this work , we address this gap by proposing A NALOGY KB , a million - scale analogy knowledge base ( KB ) derived from existing knowledge graphs ( KGs ) . A NALO - GY KB identiﬁes two types of analogies from the KGs : 1 ) analogies of the same relations , which can be directly extracted from the KGs , and 2 ) analogies of analogous relations , which are identiﬁed with a selection and ﬁltering pipeline enabled by large LMs ( InstructGPT ) , followed by minor human efforts for data qual - ity control . Evaluations on a series of datasets of two analogical reasoning tasks ( analogy recognition and generation ) demonstrate that A NALOGY KB successfully enables LMs to achieve much better results than previous state - of - the - art methods . 1 1 Introduction Making analogies requires identifying and map - ping a familiar domain ( i . e . , source domain ) to a less familiar domain ( i . e . , target domain ) ( Hofs - tadter and Sander , 2013 ) . As shown in Figure 1 , utilizing the analogy of the solar system can fa - cilitate comprehension of the complex structure of atoms . Analogical reasoning is an important aspect of the cognitive intelligence of humans , al - lowing us to quickly adapt our knowledge to new domains ( Hofstadter , 2001 ; Mitchell , 2021 ) , make decisions ( Hansen - Estruch et al . , 2022 ) , and solve problems ( Crouse et al . , 2018 ; Kang et al . , 2022 ) . As a result , the topic of analogy has been drawing signiﬁcant research attention in the community . ∗ Equal contribution . † Corresponding authors . 1 Resources of this paper can be found at https : / / github . com / siyuyuan / analogykb . 1 Earth Sun Newton Gravity Force Electron Nucleus Faraday Electric Force Has Has Orbit Orbit Discover Discover 1 . ( Orbit ) Nucleus is to Electrons as Sun is to Earth 2 . ( Has ) Electric Force is to Nucleus as Gravity Force is to Sun 3 . ( Discover ) Newton is to Gravity Force as Faraday is to Electric Force 4 . … Discovered Analogies Knowledge Graphs Solar System Atom Structure Figure 1 : An example of acquiring analogies from knowledge graphs . Based on the relational knowledge triples from KGs , i . e . , facts about the solar system and an atom structure , we can discover new analogies using the corresponding relations between terms . However , resources for analogical reasoning are rather limited in scale ( Mikolov et al . , 2013b ; Glad - kova et al . , 2016 ; Chen et al . , 2022 ) , which usually consist of only hundreds or thousands of data sam - ples . As a result , these datasets do not support effec - tive training of neural models , resulting in the fact that current state - of - the - art models still lag behind humans in analogical reasoning ( Ushio et al . , 2021 ; Bhavya et al . , 2022 ; Chen et al . , 2022 ) . Therefore , larger - scale data sources are needed to facilitate the research in this area . With richer analogies , a wide range of applications can be achieved with neural models , such as reasoning and explanation with explicit analogies and training specialized analogy - making models . Therefore , the research question is : How to acquire large - scale analogies at a mod - erate cost ? An analogy is determined by the relational struc - ture ( Falkenhainer et al . , 1989 ; Bartha , 2013 ) , e . g . , A : B : : C : D ( i . e . , A is to B as C is to D ) , where the re - lation between A and B is analogous to the relation between C and D ( Turney et al . , 2003 ; Boteanu and Chernova , 2015 ) . The terms A , B , C , and D can be concepts , entities , or events . As shown in Figure 1 , a r X i v : 2305 . 05994v1 [ c s . C L ] 10 M a y 2023 the “solar system” and an “atom” share a similar structure , allowing us to quickly grasp the relation between an “electron” and a “nucleus” in terms of their source domain counterparts . Such rela - tional structure , e . g . electron : nucleus : : earth : sun , can be derived from the triplet knowledge , e . g . ( electron , orbit , nucleus ) and ( earth , orbit , sun ) , in knowledge graphs ( KGs ) ( Vrandeˇci´c and Krötzsch , 2014 ; Speer et al . , 2017 ) . Therefore , such struc - ture knowledge from current KGs can be utilized and reorganized to create new analogy knowledge , supporting large - scale knowledge acquisition . In this work , we aim to build a knowledge base ( KB ) for storing analogies derived from existing KGs to improve analogical reasoning . However , due to the complicated relational structures , dis - covering analogies from KGs is not a trivial task . Although two pairs of terms with the same relation can form a valid analogy ( e . g . , lion , isA , animal and apple , isA , fruit ) , interesting and diverse analogies are implicit in the KGs , with more complex rela - tions . Terms under two distinct but similar relations in KGs can also form a reasonable analogy ( Hesse , 1959 ) . For example , chief executive ofﬁcer and head of state can both be abstracted into a meta relation , i . e . , head of organization . Therefore , they are analogous relations under a meta relation . It is important to generalize the ﬁnding of implicit analogies beyond the same relations within KGs . We present A NALOGY KB , which is a large - scale analogy knowledge base . We use Wikidata ( Vran - deˇci´c and Krötzsch , 2014 ) and ConceptNet ( Speer et al . , 2017 ) as our seed KGs and discover two types of analogies from these KGs : analogies of 1 ) same relations and 2 ) analogous relations . Analogies of the same relations can be directly extracted from existing KGs . In contrast , analogies of analogous relations are more implicit , requiring the ﬁnding of relation pairs from the KGs that can form valid analogies . However , it is costly to manually se - lect analogous relation pairs . Therefore , we use InstructGPT 002 ( Ouyang et al . , 2022 ) , a large lan - guage model ( LLM ) of great capabilities in NLP tasks , for ﬁnding and deciding the analogical se - mantics of relations . To eliminate the noise from the outputs of InstructGPT 002 ( § 3 . 5 ) , we devise two ﬁltering rules based on 1 ) the symmetry of analogy and 2 ) meta relation ( Hesse , 1959 ) sum - marization , which generalizes two relations into a more abstract meta relation . Then , we manually review the ﬁltered results to further ensure data quality . Our A NALOGY KB comprises over 1 million analogies with 943 relations , including 103 analo - gous relations . LMs trained on A NALOGY KB gain signiﬁcant improvements over the previous state - of - the - art methods , even rivaling human performance on some analogy recognition tasks . Furthermore , smaller and more specialized LMs trained in A NAL - OGY KB can achieve satisfactory analogy - making capabilities that are comparable to those of LLMs . Our contributions are summarized as follows : 1 ) To the best of our knowledge , we are the ﬁrst to construct an analogy KB ( A NALOGY KB ) that is a million scale and diverse in relational struc - tures , which consists of more than 1 million analo - gies with 943 relations . 2 ) We propose a novel framework to discover more interesting and im - plicit analogies of analogous relations ; 3 ) We con - duct extensive experiments to evaluate the effec - tiveness of A NALOGY KB , which results in a sig - niﬁcant improvement in the analogical reasoning performance of LMs . 2 Related Work Analogy Acquisition Early studies mainly ac - quire analogy knowledge via linguists ( Turney et al . , 2003 ; Boteanu and Chernova , 2015 ) , which is costly and inefﬁcient . With the development of knowledge graphs ( KGs ) , later studies consider ex - ploiting relations in KGs to build analogies ( Allen and Hospedales , 2019 ; Ulˇcar et al . , 2020 ) , which can be divided into two lines of work : 1 ) Acquiring from commonsense KGs , which leverages semantic and morphological relations from WordNet ( Miller , 1995 ) , ConceptNet ( Speer et al . , 2017 ) , etc . How - ever , some of these datasets are large - scale but of poor quality ( Li et al . , 2018 , 2020 ) , while others are of high quality but limited in size ( Mikolov et al . , 2013b ; Gladkova et al . , 2016 ) . 2 ) Acquiring from encyclopedia KGs ( Si and Carlson , 2017 ; Zhang et al . , 2022 ; Ilievski et al . , 2022 ) , which utilizes the relations from DBpedia ( Auer et al . , 2007 ) and Wikidata ( Vrandeˇci´c and Krötzsch , 2014 ) , but their empirical experiments are relatively small in size , without endeavors to build large - scale analogies . Our work absorbs the ideas of both lines of work to construct a million - scale analogy knowledge base . Analogical Reasoning Analogical reasoning aims to identify a relational structure between two domains ( Bartha , 2013 ; Mitchell , 2021 ) . Previous work adopts the word analogy task ( “A is to B as C is to D” ) ( Mikolov et al . , 2013a ; Levy and Gold - berg , 2014 ; Schluter , 2018 ) to investigate the ana - logical reasoning capability of LMs ( Fournier et al . , 2020 ; Ushio et al . , 2021 ) . While LMs can model some relational similarities in terms of word embed - ding ( Mikolov et al . , 2013b ; Gladkova et al . , 2016 ) , they are still far outperformed by humans , espe - cially for complex and abstract analogies ( Chen et al . , 2022 ; Sultan and Shahaf , 2022 ) . Recent work demonstrates that LLMs can generate some reason - able analogies ( Bhavya et al . , 2022 ) , but smaller LMs struggle to learn analogical reasoning ability due to a lack of training data . Knowledge Base Construction Knowledge base ( KB ) consists of structured knowledge to support various applications ( Ye et al . , 2022 ; Hirigoyen et al . , 2022 ) . The approaches to constructing KBs can be divided into three categories : 1 ) Manual construction ( Miller , 1995 ; Speer et al . , 2017 ) , which creates the KBs with specialized knowledge written by experts , and thus is labor - intensive ( Vrandeˇci´c and Krötzsch , 2014 ; Sap et al . , 2019 ) ; 2 ) Automatic construc - tion ( Suchanek et al . , 2007 ; Martinez - Rodriguez et al . , 2018 ) , which leverages models to extract knowledge from unstructured corpora , such as text documents ( Navigli and Ponzetto , 2012 ; Tandon et al . , 2014 ) or web pages ( Nguyen et al . , 2021 , 2022 ) . This approach can be cost - effective but leads to low data quality ; 3 ) Semi - automatic construction ( Dalvi Mishra et al . , 2017 ; Romero and Razniewski , 2020 ) , which involves manual curation and annotation of automatically extracted information ( Arnaout et al . , 2022 ; Liu et al . , 2022 ) . Our work is mostly based on automatic approaches with LLMs ( Ouyang et al . , 2022 ) only requiring small - scale ( hundreds ) human checking efforts . 3 A NALOGY KB Construction In this section , we detail the overall framework for building A NALOGY KB . We ﬁrst deﬁne the schema of A NALOGY KB ( § 3 . 1 ) . Then , we collect relations with term pairs from existing KGs ( § 3 . 2 , Step 1 ) and directly obtain analo - gies of the same relations from KGs ( § 3 . 3 , Step 2 ) . We also propose a human - in - the - loop acquisition approach with InstructGPT 002 ( text - davinci - 002 , with ≥ 175B parame - ters ) ( Ouyang et al . , 2022 ) to acquire analogies of analogous relations ( § 3 . 4 , Step 3 ) . 1 R1 : antonym Left : Right Up : Down High : Low R2 : CEO Andy Jassy : Amazon Tim Cook : Apple Elon Musk : Twitter R3 : head of state Joe Biden : USA Rishi Sunak : UK Emmanuel Macron : France Up : Down : : High : Low Tim Cook : Apple : : Joe Biden : USA Analogous Relations Same Relation Discovered Analogies Figure 2 : The relations with term pairs are stored in A NALOGY KB . We deﬁne two types of analogies , i . e . , analogies of the same relation and analogies of the anal - ogous relations , and derive them from existing KGs . 3 . 1 Schema for Analogies in A NALOGY KB In this paper , we focus on the analogy formed as A : B : : C : D , where terms as A , B , C and D can be concepts , entities , or events . The term pair A : B is analogous to C : D based on an underlying rela - tional structure . Since A NALOGY KB is built on existing KGs , we deﬁne two types of that relational structure based on KG semantics : 1 ) analogies of the same relation and 2 ) analogies of analogous relations . Data in A NALOGY KB is organized as in Figure 2 , where each relation R contains pairs of subject - object terms s : o . Within each rela - tion , analogies of the same relation can be naturally formed , e . g . , “ Up is to Down as High is to Low ” . Also , the term pairs between two relations can also form analogies , as long as they have analogous structures ( Hesse , 1959 ) . For example , “ Tim Cook is to Apple as Joe Biden is to USA ” , where R2 ( CEO ) is analogous to R3 ( head of state ) . There - fore , A NALOGY KB only has to store term pairs of each relation as well as pairs of analogous relations , from which analogies can be easily derived . We also list the deﬁnitions of each terminology with examples in Appendix A for better understanding . 3 . 2 Source Data Collection We choose two most - used KGs , i . e . , Concept - Net ( Speer et al . , 2017 ) and Wikidata ( Vrandeˇci´c and Krötzsch , 2014 ) as our data sources . For Con - ceptNet , we select the term pairs with weights big - ger than 2 . 0 to improve the data quality and ﬁnally collect 100 , 000 term pairs with 27 relations . Due to the vast amount of Wikidata , we randomly sample 5 million entities with 813 relations from Wikidata , resulting in 20 million term pairs . 3 . 3 Acquiring Analogies of the Same Relation We can directly utilize the term pairs in the KGs to generate analogies of the same relations . An I : Analogous Relations Generation / * I : Task prompt * / Choose the relations from the relation candidates that can form an analogy with the given relation . / * Examples * / Given relation : written by Relation candidates : [ lyrics by , composed by , . . . ] Answer : lyrics by , composed by , . . . / * Auto selection of analogical relations * / Given relation : chief executive ofﬁcer Relation candidates : [ head of state , . . . ] Answer : head of state , head of government , . . . II : Meta Relation Summarization / * Task prompt * / Induce two relations into a higher - level relation and explain why they can form an analogy . / * Examples * / The relation < lyrics by > and the relation < composed by > can form an analogy because both of them can be induced into a relation : created by . The relation < written by > and the relation < written sys - tem > can form an analogy because both of them can be induced into a relation : None . / * Auto - completion for meta relation * / The relation < chief executive ofﬁcer > and the relation < head of government > can form an analogy because both of them can be induced into a relation : head of organization . Table 1 : Examples of prompt for InstructGPT 002 for analogous relations generation and meta relation sum - marization via in - context learning . Green texts are gen - erated by InstructGPT 002 . important perspective is that humans usually draw upon familiar domains and situations to better un - derstand unfamiliar ones . To make our analogy KB more applicable to real - world scenarios , we rank the term pairs according to their popularity scores , which are reﬂected by pageview times ( in Wikidata ) and concept weights ( in ConceptNet ) . 3 . 4 Acquiring Analogies of Analogous Relations As deﬁned in § 3 . 1 , analogies of analogous re - lations consist of two term pairs with analogous relations R 1 and R 2 . However , it is difﬁcult to automatically check whether R 1 and R 2 are anal - ogous and manual annotation is costly . Recently , LLMs ( Brown et al . , 2020 ; Ouyang et al . , 2022 ; Chowdhery et al . , 2022 ) have shown their remark - able few - shot learning abilities with in - context learning ( Brown et al . , 2020 ) . Given a task prompt describing the task and several examples , LLMs can do the task well by completing the prompt without training . Therefore , we propose to exploit LLMs ( e . g . , InstructGPT 002 ) to acquire analogies of analogous relations . Finding Candidate Relation Pairs As men - tioned in § 3 . 2 , we collect a total number of 840 relations , leading to a potential amount of (cid:0) 8402 (cid:1) relation pairs . The relations that are semantically similar to each other can form an analogy ( Hesse , 1959 ) . Motivated by this , for each relation , we ﬁrst narrow down the candidate set from the 840 rela - tions to the 20 - most similar ones . Then we use In - structGPT embeddings ( text - embedding - ada - 002 ) to convert the relations into vectors and calculate the cosine similarity between them . By identify - ing the top 20 relations with the highest similarity as candidate relations for the query relation , the search space is signiﬁcantly reduced for further ﬁltering for analogous relations . Predicting Analogous Relation Pairs While the search space is reduced , manual annotation remains cost - prohibitive ( 840 × 20 ) . Thus , we con - tinue to adopt InstructGPT 002 to predict analogous relation pairs . An example in Table 1 ( I ) shows the acquisition of analogous relation pairs . Given examples and the query ( “ chief executive ofﬁcer ” ) , InstructGPT 002 selects the relations “ head of state ” and “ head of organization ” from the candidates to form analogies . However , our experiment shows that InstructGPT 002 struggles to ﬁlter out similar but wrong relations that can not form analogies with queries , e . g . , “ operator ” for “ chief executive ofﬁcer ” , which requires further ﬁltering . Filtering for High - quality Relation Pairs To reduce human labor for quality control , we imple - ment two automatic ﬁltering rules before conduct - ing manual ﬁltering . 1 . Rule 1 : The ﬁrst rule is based on the sym - metry of analogies : if two relations can form an analogy , InstructGPT 002 should simultane - ously select R 1 for R 2 and R 2 for R 1 . 2 . Rule 2 : The second rule is using a more ab - stract meta relation ( Hesse , 1959 ) to decide if two relations can form an analogy . The rationale behind the Rule 2 is that if two re - lations are analogous , then they can be generalized into a more abstract meta - relation ( Hesse , 1959 ; Gentner and Maravilla , 2017 ) . For example , in Ta - ble 1 ( II ) , written by and composed by can form an analogy since they can be induced to a meta relation created by . To acquire meta relations , we prompt InstructGPT 002 with a task prompt , some examples and queries , as shown in Table 1 ( II ) . If Source # Term Pair # Rel ( s ) Analogy Acc . Analogies of the Same Relation ConceptNet 75 , 019 27 98 . 50 % Wikidata 563 , 024 813 98 . 00 % Analogies of Analogous Relations ConceptNet 11 , 829 5 95 . 50 % Wikidata 382 , 168 98 96 . 00 % Total 1 , 032 , 040 943 97 . 00 % Table 2 : The statistics of our A NALOGY KB . We report the number of term pairs ( # Term Pair ) , the number of relations ( pairs if for analogous relations ) ( # Rel ( s ) ) , manually evaluated accuracy of randomly selected 200 analogies ( Analogy Acc . ) and the source KB ( Source ) for each data type . Data # Analogy # Rel Language SAT 374 - En Google 550 15 En UNIT 2 252 - En UNIT 4 480 - En BATS 1 , 998 4 En E - KAR 1251 28 En E - KAR 1655 28 Zh CA - EHN 90 , 505 763 Zh A NALOGY KB ≥ 1 , 032 , 040 943 En Table 3 : Comparison between A NALOGY KB and pre - vious analogy data source : numbers of analogies ( a set of two analogous term pairs , i . e . , A : B : : C : D ) , number of relations and language . InstructGPT 002 returns “ None ” , we discard this case . To further improve data quality , we adopt a third ﬁltering by recruiting two volunteers to manually examine the remaining results , including deleting relation pairs that fail to form analogies or adding previously unchosen relation pairs that can form analogies from candidates . Combining automatic ﬁltering with manual examination , the quality of A NALOGY KB is signiﬁcantly improved . Finally , we sort the term pairs by pageview ( Wikidata ) and weight ( ConceptNet ) . 3 . 5 Analysis of A NALOGY KB Now , we have a knowledge base with a vast num - ber of analogies . As shown in Table 2 , our evalu - ation results show that A NALOGY KB is massive , consisting of over 1 million term pairs and 943 rela - tions , which can form even more pairs of analogies . Since A NALOGY KB provides a more comprehen - sive range of relations than previous datasets , it al - lows users to select their preferred analogies within each relation ( pair ) . human : 23 . 46 % p l a c e : 2 3 . 0 5 % t h i n g : 14 . 36 % g r o u p : 5 . 2 3 % o r g a n i z a t i o n : 14 . 26 % e v e n t : 2 . 76 % s p e c i e s : 6 . 72 % t e c hn o l o g y : 4 . 56 % g a m e : 3 . 66 % l a n g u a g e : 1 . 94 % a c t o r a r t i s t a u t h o r p o li t i c i a n sc r ee n w r i t e r c o m p o s e r f il m d i r e c t o r j o u r n a li s t m u s i c i a n n o v e li s t p o e t s i n g e r city c o u n t r y r e g i o n s t a t i o n t o w n u r b a n a r e a m o v i e a l b u m b oo k i n s t r u m e n t m o un t a i n p a i n t i n g river r o a d s o n g c l u b c o m m u n i t y e t h n i c g r o u p m i n o r i t y g r o u p p o li t i c a l p a r t y t e a m c o m p a n y sc h oo l i n d u s t r y i n s t i t u t i o n w a r c o n f li c t c r i m e h i s t o r i c a l e v e n t m a mm a l o r g a n i s m p r i m a t e d e v i c e m a t e r i a l o p e r a t i n g s y s t e m p r o d u c t p r o g r a m v i d e o g a m e c o n t a c t s p o r t t e a m s p o r t e u r o p e a n l a n g u a g e m o d e r n l a n g u a g e n a t u r a l l a n g u a g e Figure 3 : Distribution of entity categories in our A NAL - OGY KB . Entities are assigned to multiple categories based on Probase ( Wu et al . , 2012 ) . Method # Total # Correct InstructGPT 002 392 97 + Rule 1 151 97 + Rule 1 & Rule 2 104 97 + Rule 1 & Rule 2 & Human 103 103 Table 4 : Ablated evaluation results of the analogous re - lation pairs . We record the total number of analogous relation pairs ( # Total ) and correct ones ( # Correct ) . Note that “Human” denotes manual modiﬁcations , in - cluding adding missing relations or deleting incorrect ones , so the results are already correct ( 103 → 103 ) . To evaluate the quality of A NALOGY KB , we randomly sample 200 analogies from each data type , i . e . , two term pairs of the same or anal - ogous relations , in the form of A : B : : C : D . The data is annotated by two annotators with Fleiss’s κ = 0 . 86 ( Fleiss et al . , 1981 ) . Results show that A NALOGY KB is of high quality , with a total ac - curacy of 97 % . Even for analogies of analogous relations , analogies are still of over 95 % accuracy . Comparison with Previous Datasets We com - pare A NALOGY KB with the resources related to the problem of analogy , as reported in Table 3 . We ﬁnd that A NALOGY KB is much larger than previ - ous data sources , with more analogies and relations . To better present the fabric of A NALOGY KB , we present the distribution of the categories of entities covered in A NALOGY KB in Figure 3 . The cate - gories are obtained from the concepts of entities from a widely - used taxonomic KB Probase ( Wu et al . , 2012 ) . We ﬁnd that A NALOGY KB exhibits Method E - KAR BATS UNIT 2 UNIT 4 Google SAT Mean Word Embeddding from RoBERTa - Large 28 . 20 72 . 00 58 . 30 57 . 40 96 . 60 56 . 70 61 . 53 T5 - Large 40 . 08 77 . 37 34 . 65 31 . 25 75 . 60 31 . 45 48 . 40 BERT - Large 36 . 64 70 . 10 32 . 89 34 . 49 90 . 40 41 . 30 50 . 97 RoBERTa - Large 46 . 70 78 . 20 46 . 05 40 . 04 96 . 90 51 . 60 59 . 92 ERNIE 40 . 83 82 . 54 34 . 21 36 . 80 82 . 40 34 . 92 51 . 95 LUKE 40 . 45 82 . 82 34 . 64 39 . 12 88 . 40 30 . 26 52 . 62 InstructGPT 002 40 . 46 78 . 89 53 . 94 50 . 00 93 . 00 44 . 21 60 . 08 InstructGPT 002 w / CoT prompting 46 . 18 75 . 36 56 . 57 55 . 55 87 . 60 46 . 88 61 . 36 InstructGPT 003 39 . 31 82 . 77 56 . 14 58 . 33 94 . 40 47 . 48 63 . 07 InstructGPT 003 w / CoT prompting 47 . 86 82 . 77 58 . 96 57 . 89 96 . 74 48 . 93 65 . 53 ChatGPT 41 . 22 81 . 71 53 . 07 52 . 31 93 . 80 49 . 26 61 . 90 ChatGPT w / CoT prompting 46 . 86 90 . 56 68 . 94 70 . 84 97 . 80 65 . 78 73 . 46 RoBERTa - Large + E - KAR 46 . 70 79 . 04 73 . 68 61 . 34 83 . 20 40 . 05 64 . 00 RoBERTa - Large + A NALOGY KB 53 . 43 90 . 93 87 . 28 76 . 15 97 . 80 59 . 05 77 . 44 Human 77 . 80 84 . 85 87 . 50 66 . 66 99 . 41 57 . 00 78 . 87 Table 5 : Accuracy on the analogy recognition task . We compare models and human performance on different benchmarks under different settings . The best results are bolded and the second best ones are underlined . high diversity in entity categories . Are the ﬁltering techniques for analogous rela - tions useful ? We evaluate the usefulness of the proposed components during ﬁltering for analo - gous relations , i . e . , symmetry ( Rule 1 ) and meta relation summarization ( Rule 2 ) , and manual cor - rection . In Table 4 , we record the total number of analogous relation pairs output by models ( # Total ) and then employ annotators to report the number of correct ones out of them ( # Correct ) . Each pair of relations is examined by two annotators with Fleiss’s κ = 0 . 83 . The results in Table 4 show that : 1 ) InstructGPT 002 cannot ﬁlter out similar but wrong relation pairs , indicating the need for further ﬁltering ; 2 ) The symmetry rule and meta re - lation summarization can improve the precision of results , and manual adjustment ensures data qual - ity . 4 A NALOGY KB Evaluation In this section , we put A NALOGY KB on analogi - cal reasoning tasks , including analogy recognition ( § 4 . 1 ) and analogy generation ( § 4 . 2 ) . 4 . 1 Analogy Recognition Evaluation Analogy recognition task aims to recognize the most analogous candidate to the query , formulated as multiple - choice question - answering . Can models trained on A NALOGY KB acquire better analogy recognition abilities ? As the backbone model , we use the RoBERTa ( large version , Liu et al . , 2019 ) and randomly sample 10 , 000 data points from A NALOGY KB to train the model in a multiple - choice question - answer format . We ﬁrst train the model on the data from A NALOGY KB and then further ﬁne - tune it on the datasets from six different benchmarks , i . e . , E - KAR ( Chen et al . , 2022 ) , BATS ( Gladkova et al . , 2016 ) , UNIT 2 / UNIT 4 ( Boteanu and Cher - nova , 2015 ) , Google ( Mikolov et al . , 2013b ) and SAT ( Turney et al . , 2003 ) . 2 As the baselines , we adopt pre - trained word embeddings from RoBERTa - Large ( Ushio et al . , 2021 ) , pre - trained language models ( Raffel et al . , 2022 ; Devlin et al . , 2019 ; Liu et al . , 2019 ) , LLMs ( Ouyang et al . , 2022 ; Leiter et al . , 2023 ) with 2 - shot learning and Chain - of - Thought prompt - ing ( Wei et al . , 2022 ) . To rule out the confounder in A NALOGY KB , such as the incorporation of the entity knowledge from ConceptNet and Wiki - data , we also add knowledge - enhanced models , ERNIE ( Zhang et al . , 2019 ) and LUKE ( Yamada et al . , 2020 ) , as the baselines , which leverage en - tity knowledge to improve model performance . We also train RoBERTa - Large on E - KAR for compari - son , which contains 870 training data . 3 The results presented in Table 5 show that : 1 ) Pre - trained word embeddings struggle with com - plex analogy recognition tasks ( E - KAR , U4 , and SAT ) ; 2 ) Incorporating entity knowledge cannot im - 2 Detailed information on the benchmarks and the construc - tion of A NALOGY KB sample data is shown in Appendix B . 1 . 3 Details about the baselines and training process are shown in Appendix B . 2 and Appendix B . 3 . We also compare our method with knowledge models shown in Appendix B . 4 E - K A R B A T S U N I T 2 U N I T 4 G o o g l e S A T M e a n 0 20 40 60 80 100 P r e - t r i a n e d A cc ( % ) 46 79 50 46 96 49 61 42 80 43 43 94 49 58 29 28 21 25 19 15 23 Data Data same Data pseudo E - K A R B A T S U N I T 2 U N I T 4 G o o g l e S A T M e a n 0 20 40 60 80 100 F i n e - t un e d A cc ( % ) 53 90 87 76 97 59 77 51 90 71 63 97 57 72 35 38 24 30 32 24 30 Figure 4 : The accuracy of RoBERTa - Large trained on different data subsets on the analogy recognition task . Data denotes the dataset sampled directly from A NAL - OGY KB , Data same denotes the dataset that only has same - relation analogies , and Data pseudo denotes the dataset with term pairs that do not form analogies . All the datasets have the same size . prove model performance on analogy recognition ; 3 ) LLMs perform poorly on the analogy recognition task but adding a chain of thought can help improve model performance ; 4 ) Training RoBERTa on E - KAR alone is insufﬁcient to improve the model performance on the Google and SAT tests , prob - ably due to the data discrepancy between them . 5 ) Training RoBERTa on A NALOGY KB can signif - icantly improve the model performance on analogy recognition by a large margin ( + 13 points ) and even approach human - level performance . How much do analogies of analogous relations in A NALOGY KB contribute to performance ? We create two ablated variants from A NALOGY KB to train the models : 1 ) Analogies of the same rela - tions , denoted as Data same : we randomly sampled 10 , 000 data of the same relations as an ablated vari - ant . This further evaluates the efﬁcacy of analogies of analogous relations . 2 ) Pseudo analogies , de - noted as Data pseudo : we randomly sampled 10 , 000 data points from A NALOGY KB which do not form analogies . This makes sure that A NALOGY KB in - deed imposes analogical reasoning ability on the model rather than simply data augmentation . The results in Figure 4 show that : 1 ) Analogies of analogous relations in A NALOGY KB are rather important for models to comprehend analogies with more abstract and complex relations , as shown by the improvements on E - KAR , UNIT 2 , UNIT 4 , and SAT . 2 ) Training models on randomly con - structed analogy - style data even drags down model performance , further emphasizing the importance 10 0 10 1 10 2 20 30 40 50 Data Size ( × 1k ) AKB AKB + E - KAR ( a ) Different data sizes . 10 2 10 3 20 30 40 50 Model Size ( M ) AKB AKB + E - KAR ( b ) Different model sizes . Figure 5 : Performance change ( Accuracy % ) for T5 on E - KAR test set with increasing training data ( 1K , 5K , 10K , 50K , 100K ) from A NALOGY KB and model size ( 60M , 220M , 770M , 3B ) . T5 is either trained on A NALOGY KB ( AKB ) or both A NALOGY KB and E - KAR ( AKB + E - KAR ) . of A NALOGY KB . 4 How do data sizes and model sizes affect perfor - mance ? In this experiment , T5 - Large is used as the base model to examine the effects of training data size on model performance . We ﬁrst train the model on data from A NALOGY KB , and ﬁne - tune it on E - KAR . As illustrated in Figure 5 ( a ) , increasing the amount of training data from A NALOGY KB improves model performance . Figure 5 ( b ) shows the results of different - sized T5 models on 10 , 000 data points from A NALOGY KB . We ﬁnd that larger models generally improve performance on E - KAR . 4 . 2 Analogy Generation Evaluation We further explore a new setting of analogical rea - soning , i . e . , the generation of analogies . This task can be formulated as a text generation task : com - pleting the D given A , B , C to form a plausible analogy A is to B as C is to D . Analogy generation is of more practical use , since the generation of familiar analogies could be helpful to comprehend the source problem . Does A NALOGY KB support analogy genera - tion ? To answer this question , we randomly sam - ple 1 million data points from A NALOGY KB , where each data point consists of two term pairs from the analogous relation . For each data point , we choose the term pair with the lower popular - ity score as the target and the other as the source . T5 - Large ( Raffel et al . , 2022 ) is used as the back - bone model for training . We compare T5 - Large ( 770M ) trained on A NALOGY KB ( named Analo - 4 We also compare the data from different KB sources , which is shown in Appendix B . 5 Model Hit @ k E - KAR UNIT 4 SAT InstructGPT 002 1 55 . 00 72 . 00 57 . 00 InstructGPT 003 1 68 . 00 76 . 00 74 . 00 ChatGPT 1 64 . 00 81 . 00 80 . 00 AnalogyT5 1 57 . 00 80 . 00 64 . 00 3 62 . 00 86 . 00 76 . 00 5 66 . 00 91 . 00 84 . 00 Table 6 : Accuracy on analogy generation task . We compare LLMs with few - shot learning and T5 ( 770M ) ﬁne - tuned on A NALOGY KB ( i . e . , AnalogyT5 ) . Input Completion Mcdonald’s is to America as Samsung is to south korea oxygen is to breathe as brain is to thinking terrestrial is to land as aquatic is to water meticulous is to careful as ascetic is to asceticism triangle is to area as cube is to volume electron is to nucleus as earth is to sun electron is to electric force as earth is to gravity electron is to atom as earth is to the solar system Table 7 : Randomly selected and novel analogy gen - erated from the AnalogyT5 . Novel generations are term pairs not found in the training set of AnalogyT5 . Whether the analogy is considered plausible or not is decided by human annotators . gyT5 ) with LLMs on 100 test data sampled from three challenging analogy benchmarks , which are not found in the training set . Given that OpenAI models are not open - sourced , we can only obtain a single generation with the highest probability ( de - noted as Hit @ 1 ) through OpenAI API . On the con - trary , AnalogyT5 can adopt a beam search ( Wise - man and Rush , 2016 ) to generate multiple results . Thus , we retain the top - k results generated by Anal - ogyT5 ( denoted as Hit @ k ) , i . e . , the prediction is considered as correct if one of the k generations is correct . 5 Each generation is evaluated by three annotators with Fleiss’s κ = 0 . 93 . The results in Table 6 show that the LLM performs poorly in the analogy generation task , while training on A NALO - GY KB allows smaller LMs to generate reasonable analogies . Case Study : Can AnalogyT5 generate novel analogies out of A NALOGY KB ? We are curi - ous whether analogy generation models trained on A NALOGY KB can generalize to novel analogies . After manual inspection , we observe from Table 7 that , AnalogyT5 can generate a reasonable term D ( e . g . , “ south korea ” ) for the input ( e . g . , mcdonald’s 5 Detailed information on the training process is shown in Appendix C . 1 . Model Acc . MRR Rec @ 5 Rec @ 10 Glove 1 . 80 2 . 20 2 . 40 3 . 10 GPT - 2 2 . 00 5 . 70 10 . 70 16 . 70 + BATS 1 . 10 2 . 20 3 . 60 5 . 10 + A NALOGY KB 5 . 12 6 . 41 12 . 77 20 . 47 + A NALOGY KB same 4 . 00 5 . 49 11 . 45 17 . 61 BERT 0 . 90 4 . 40 8 . 00 13 . 40 + BATS 0 . 40 1 . 90 3 . 10 5 . 10 + A NALOGY KB 6 . 24 10 . 36 14 . 07 18 . 24 + A NALOGY KB same 4 . 01 7 . 44 10 . 89 15 . 23 InstructGPT 002 9 . 09 9 . 97 11 . 52 - InstructGPT 003 9 . 64 12 , 83 20 . 84 - AnalogyT5 9 . 98 13 . 94 19 . 51 23 . 72 Table 8 : Analogy generation results on SCAN . We pre - train the models on BATS dataset ( + BATS ) or data sampled from A NALOGY KB ( + A NALOGY KB ) . A NALOGY KB same is the ablation variants with analo - gies of the same relations from A NALOGY KB . The best results are bolded and the second best ones are underlined . is to America as Samsung is to” ) . Furthermore , AnalogyT5 also generates reasonable analogies of analogous relations , such as “ triangle ” is to “ area ” as “ cube ” is to “ volume ” . However , analogies about adjectives are more error - prone , possibly due to the paucity of adjectives in A NALOGY KB . Addition - ally , we discover that training on A NALOGY KB enables LMs to generate reasonable analogies by changing term B while holding ﬁxed A ( i . e . , elec - tron ) and C ( i . e . , earth ) . This shows that training on A NALOGY KB can generate valid analogies . Does training on A NALOGY KB generalize to harder analogies ? Despite its high coverage of common entities ( § 3 . 5 ) , A NALOGY KB contains few analogies related to metaphor and science which are abstract and complex , posing a challenge even for humans ( Czinczoll et al . , 2022 ) . These types of data are not present in existing KGs and are hard to extract from corpora . To examine whether A NALOGY KB can generalize the ability of LMs to reason about these analogies , we test A NAL - OGY KB on the SCAN dataset ( Czinczoll et al . , 2022 ) , which has 449 analogies of metaphor and science domains . We follow the original experi - mental setup and compare the models trained on A NALOGY KB with Glove and LLMs with 2 - shot learning ( see Appendix C . 2 for details ) . The re - sults shown in Table 8 reveal that 1 ) LLMs cannot well comprehend the analogies made in complex domains ; 2 ) Training on BATS ( Gladkova et al . , 2016 ) even worsen performance on SCAN , indi - cating that previous datasets are insufﬁcient for LMs to make complex analogies ; 3 ) A NALOGY KB signiﬁcantly improves model performance . 5 Conclusion In this paper , we introduce A NALOGY KB , a million - scale analogy knowledge base to improve model performance in analogical reasoning tasks . We identify two types of analogies in existing knowledge graphs , i . e . , analogies of the same and analogous relations , and utilize InstructGPT 002 with minor human examinations to ﬁnd them . A NALOGY KB is useful to ﬁne - tune language mod - els for better analogical reasoning abilities . We evaluate A NALOGY KB using extensive bench - marks on analogy recognition and generation tasks , demonstrating its great value in assisting with the resolution of these tasks , especially with analogies of analogous relations in A NALOGY KB . We hope A NALOGY KB to be a valuable resource for advanc - ing research on analogical reasoning . Limitations First , this paper only considers analogies involving one or two relations and primarily concentrates on analogies in the form of “A is to B as C is to D” . However , in reality , analogies may involve the com - bination of multiple relations of multiple entities or even events . Second , our A NALOGY KB is con - structed using data from Wikidata and ConceptNet , which do not include analogies in other domains such as the scientiﬁc domain . Also , A NALOGY KB is stored in the form of tuples , but in practice , some analogy situations may not be easily converted to this format . Future research should address how to bridge this gap . Last but not least , effective training methods ( or prompting methods for LLMs ) tailored for analogy - making should be further explored . Ethics Statement We hereby acknowledge that all authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct . Use of Human Annotations The annotations of relation pairs in A NALOGY KB are implemented by annotators recruited by our institution . The con - struction team remains anonymous to the authors , and the annotation quality is ensured by using a double - check strategy as described in Section 3 . We ensure that the privacy rights of all annotators are respected throughout the annotation process . All annotators are compensated above the local minimum wage and consent to the use of A NALO - GY KB for research purposes , as described in our paper . Risks The database is sourced from publicly available sources , Wikidata and ConceptNet . How - ever , we cannot guarantee that it is free of socially harmful or toxic language . Additionally , analogy evaluation relies on commonsense , and different individuals with diverse backgrounds may have varying perspectives . References Carl Allen and Timothy Hospedales . 2019 . Analo - gies explained : Towards understanding word em - beddings . In International Conference on Machine Learning , pages 223 – 231 . PMLR . Hiba Arnaout , Simon Razniewski , Gerhard Weikum , and Jeff Z . Pan . 2022 . Uncommonsense : Informa - tive negative knowledge about everyday concepts . In Proceedings of the 31st ACM International Con - ference on Information & amp ; Knowledge Manage - ment , CIKM ’22 , page 37 – 46 , New York , NY , USA . Association for Computing Machinery . Sören Auer , Christian Bizer , Georgi Kobilarov , Jens Lehmann , Richard Cyganiak , and Zachary Ives . 2007 . Dbpedia : A nucleus for a web of open data . In The Semantic Web , pages 722 – 735 , Berlin , Hei - delberg . Springer Berlin Heidelberg . Paul Bartha . 2013 . Analogy and analogical reasoning . Bhavya Bhavya , Jinjun Xiong , and Chengxiang Zhai . 2022 . Analogy generation by prompting large lan - guage models : A case study of instructgpt . arXiv preprint arXiv : 2210 . 04186 . Antoine Bosselut , Hannah Rashkin , Maarten Sap , Chai - tanya Malaviya , Asli Celikyilmaz , and Yejin Choi . 2019 . COMET : Commonsense transformers for au - tomatic knowledge graph construction . In Proceed - ings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4762 – 4779 , Florence , Italy . Association for Computational Lin - guistics . Adrian Boteanu and Sonia Chernova . 2015 . Solving and explaining analogy questions using semantic networks . Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 29 ( 1 ) . Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learners . In Advances in Neural Information Processing Systems , volume 33 , pages 1877 – 1901 . Curran Associates , Inc . Jiangjie Chen , Rui Xu , Ziquan Fu , Wei Shi , Zhongqiao Li , Xinbo Zhang , Changzhi Sun , Lei Li , Yanghua Xiao , and Hao Zhou . 2022 . E - KAR : A benchmark for rationalizing natural language analogical reason - ing . In Findings of the Association for Compu - tational Linguistics : ACL 2022 , pages 3941 – 3955 , Dublin , Ireland . Association for Computational Lin - guistics . Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Sebastian Gehrmann , et al . 2022 . Palm : Scaling language modeling with pathways . arXiv preprint arXiv : 2204 . 02311 . Maxwell Crouse , Clifton McFate , and Kenneth Forbus . 2018 . Learning from unannotated qa pairs to ana - logically disambiguate and answer questions . Pro - ceedings of the AAAI Conference on Artiﬁcial Intel - ligence , 32 ( 1 ) . Tamara Czinczoll , Helen Yannakoudakis , Pushkar Mishra , and Ekaterina Shutova . 2022 . Scientiﬁc and creative analogies in pretrained language mod - els . In Findings of the Association for Computa - tional Linguistics : EMNLP 2022 , pages 2094 – 2100 , Abu Dhabi , United Arab Emirates . Association for Computational Linguistics . Bhavana Dalvi Mishra , Niket Tandon , and Peter Clark . 2017 . Domain - targeted , high precision knowledge extraction . Transactions of the Association for Com - putational Linguistics , 5 : 233 – 246 . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171 – 4186 , Minneapolis , Minnesota . Associ - ation for Computational Linguistics . Brian Falkenhainer , Kenneth D Forbus , and Dedre Gen - tner . 1989 . The structure - mapping engine : Algo - rithm and examples . Artiﬁcial intelligence , 41 ( 1 ) : 1 – 63 . Joseph L Fleiss , Bruce Levin , Myunghee Cho Paik , et al . 1981 . The measurement of interrater agree - ment . Statistical methods for rates and proportions , 2 ( 212 - 236 ) : 22 – 23 . Louis Fournier , Emmanuel Dupoux , and Ewan Dun - bar . 2020 . Analogies minus analogy test : measur - ing regularities in word embeddings . In Proceedings of the 24th Conference on Computational Natural Language Learning , pages 365 – 375 , Online . Asso - ciation for Computational Linguistics . Dedre Gentner and Francisco Maravilla . 2017 . Ana - logical reasoning . In The Routledge International Handbook of Thinking and Reasoning , pages 186 – 203 . Routledge . Anna Gladkova , Aleksandr Drozd , and Satoshi Mat - suoka . 2016 . Analogy - based detection of morpho - logical and semantic relations with word embed - dings : what works and what doesn’t . In Proceedings of the NAACL Student Research Workshop , pages 8 – 15 , San Diego , California . Association for Computa - tional Linguistics . Philippe Hansen - Estruch , Amy Zhang , Ashvin Nair , Patrick Yin , and Sergey Levine . 2022 . Bisimulation makes analogies in goal - conditioned reinforcement learning . In Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 8407 – 8426 . PMLR . Mary B Hesse . 1959 . On deﬁning analogy . In Proceed - ings of the Aristotelian Society , volume 60 , pages 79 – 100 . JSTOR . Rose Hirigoyen , Amal Zouaq , and Samuel Reyd . 2022 . A copy mechanism for handling knowledge base el - ements in SPARQL neural machine translation . In Findings of the Association for Computational Lin - guistics : AACL - IJCNLP 2022 , pages 226 – 236 , On - line only . Association for Computational Linguis - tics . Douglas R Hofstadter . 2001 . Analogy as the core of cognition . The analogical mind : Perspectives from cognitive science , pages 499 – 538 . Douglas R Hofstadter and Emmanuel Sander . 2013 . Surfaces and essences : Analogy as the fuel and ﬁre of thinking . Basic books . Filip Ilievski , Jay Pujara , and Kartik Shenoy . 2022 . Does wikidata support analogical reasoning ? In Iberoamerican Knowledge Graphs and Semantic Web Conference , pages 178 – 191 . Springer . Hyeonsu B . Kang , Xin Qian , Tom Hope , Dafna Shahaf , Joel Chan , and Aniket Kittur . 2022 . Augmenting scientiﬁc creativity with an analogical search engine . ACM Trans . Comput . - Hum . Interact . Just Accepted . Christoph Leiter , Ran Zhang , Yanran Chen , Jonas Be - louadi , Daniil Larionov , Vivian Fresen , and Stef - fen Eger . 2023 . Chatgpt : A meta - analysis after 2 . 5 months . arXiv preprint arXiv : 2302 . 13795 . Omer Levy and Yoav Goldberg . 2014 . Linguistic regularities in sparse and explicit word representa - tions . In Proceedings of the Eighteenth Confer - ence on Computational Natural Language Learning , pages 171 – 180 , Ann Arbor , Michigan . Association for Computational Linguistics . Mike Lewis , Yinhan Liu , Naman Goyal , Mar - jan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2020 . BART : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension . In Proceedings of the 58th An - nual Meeting of the Association for Computational Linguistics , pages 7871 – 7880 , Online . Association for Computational Linguistics . Peng - Hsuan Li , Tsan - Yu Yang , and Wei - Yun Ma . 2020 . CA - EHN : Commonsense analogy from E - HowNet . In Proceedings of the Twelfth Language Resources and Evaluation Conference , pages 2984 – 2990 , Mar - seille , France . European Language Resources Asso - ciation . Shen Li , Zhe Zhao , Renfen Hu , Wensi Li , Tao Liu , and Xiaoyong Du . 2018 . Analogical reasoning on Chi - nese morphological and semantic relations . In Pro - ceedings of the 56th Annual Meeting of the Associa - tion for Computational Linguistics ( Volume 2 : Short Papers ) , pages 138 – 143 , Melbourne , Australia . As - sociation for Computational Linguistics . Jingping Liu , Tao Chen , Chao Wang , Jiaqing Liang , Li - han Chen , Yanghua Xiao , Yunwen Chen , and Ke Jin . 2022 . Vocsk : Verb - oriented commonsense knowl - edge mining with taxonomy - guided induction . Artif . Intell . , 310 ( C ) . Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Man - dar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019 . Roberta : A robustly optimized bert pretraining ap - proach . arXiv preprint arXiv : 1907 . 11692 . Jose L . Martinez - Rodriguez , Ivan Lopez - Arevalo , and Ana B . Rios - Alvarado . 2018 . Openie - based ap - proach for knowledge graph construction from text . Expert Systems with Applications , 113 : 339 – 355 . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg Cor - rado , and Jeffrey Dean . 2013a . Distributed represen - tations of words and phrases and their composition - ality . In Proceedings of the 26th International Con - ference on Neural Information Processing Systems - Volume 2 , NIPS’13 , page 3111 – 3119 , Red Hook , NY , USA . Curran Associates Inc . Tomas Mikolov , Wen - tau Yih , and Geoffrey Zweig . 2013b . Linguistic regularities in continuous space word representations . In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 746 – 751 , Atlanta , Georgia . Association for Computational Linguistics . George A . Miller . 1995 . Wordnet : A lexical database for english . Commun . ACM , 38 ( 11 ) : 39 – 41 . Melanie Mitchell . 2021 . Abstraction and analogy - making in artiﬁcial intelligence . Annals of the New York Academy of Sciences , 1505 ( 1 ) : 79 – 101 . Roberto Navigli and Simone Paolo Ponzetto . 2012 . Ba - belnet : The automatic construction , evaluation and application of a wide - coverage multilingual seman - tic network . Artif . Intell . , 193 : 217 – 250 . Tuan - Phong Nguyen , Simon Razniewski , Julien Romero , and Gerhard Weikum . 2022 . Reﬁned com - monsense knowledge from large - scale web contents . IEEE Transactions on Knowledge and Data Engi - neering , pages 1 – 16 . Tuan - Phong Nguyen , Simon Razniewski , and Gerhard Weikum . 2021 . Advanced semantics for common - sense knowledge extraction . In Proceedings of the Web Conference 2021 , WWW ’21 , page 2636 – 2647 , New York , NY , USA . Association for Computing Machinery . Long Ouyang , Jeff Wu , Xu Jiang , Diogo Almeida , Car - roll L Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , et al . 2022 . Training language models to follow in - structions with human feedback . arXiv preprint arXiv : 2203 . 02155 . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , et al . 2019 . Lan - guage models are unsupervised multitask learners . OpenAI blog , 1 ( 8 ) : 9 . Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J . Liu . 2022 . Exploring the limits of transfer learning with a uniﬁed text - to - text trans - former . J . Mach . Learn . Res . , 21 ( 1 ) . Julien Romero and Simon Razniewski . 2020 . Inside quasimodo : Exploring construction and usage of commonsense knowledge . In Proceedings of the 29th ACM International Conference on Information & amp ; Knowledge Management , CIKM ’20 , page 3445 – 3448 , New York , NY , USA . Association for Computing Machinery . Maarten Sap , Ronan Le Bras , Emily Allaway , Chan - dra Bhagavatula , Nicholas Lourie , Hannah Rashkin , Brendan Roof , Noah A . Smith , and Yejin Choi . 2019 . Atomic : An atlas of machine commonsense for if - then reasoning . In Proceedings of the Thirty - Third AAAI Conference on Artiﬁcial Intelligence and Thirty - First Innovative Applications of Artiﬁcial In - telligence Conference and Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence , AAAI’19 / IAAI’19 / EAAI’19 . AAAI Press . Natalie Schluter . 2018 . The word analogy testing caveat . In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , Volume 2 ( Short Papers ) , pages 242 – 246 , New Orleans , Louisiana . Association for Computa - tional Linguistics . Mei Si and Craig Carlson . 2017 . A data - driven ap - proach for making analogies . In Proceedings of the 39th Annual Meeting of the Cognitive Science Soci - ety , CogSci 2017 , London , UK , 16 - 29 July 2017 . Robyn Speer , Joshua Chin , and Catherine Havasi . 2017 . Conceptnet 5 . 5 : An open multilingual graph of general knowledge . In Proceedings of the Thirty - First AAAI Conference on Artiﬁcial Intelligence , AAAI’17 , page 4444 – 4451 . AAAI Press . Fabian M . Suchanek , Gjergji Kasneci , and Gerhard Weikum . 2007 . Yago : A core of semantic knowl - edge . In Proceedings of the 16th International Conference on World Wide Web , WWW ’07 , page 697 – 706 , New York , NY , USA . Association for Computing Machinery . Oren Sultan and Dafna Shahaf . 2022 . Life is a circus and we are the clowns : Automatically ﬁnding analo - gies between situations and processes . In Proceed - ings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 3547 – 3562 , Abu Dhabi , United Arab Emirates . Association for Computational Linguistics . Niket Tandon , Gerard de Melo , Fabian Suchanek , and Gerhard Weikum . 2014 . Webchild : Harvesting and organizing commonsense knowledge from the web . In Proceedings of the 7th ACM International Confer - ence on Web Search and Data Mining , WSDM ’14 , page 523 – 532 , New York , NY , USA . Association for Computing Machinery . Peter D Turney , Michael L Littman , Jeffrey Bigham , and Victor Shnayder . 2003 . Combining independent modules in lexical multiple - choice problems . Re - cent Advances in Natural Language Processing III : Selected Papers from RANLP , 2003 : 101 – 110 . Matej Ulˇcar , Kristiina Vaik , Jessica Lindström , Milda Dailid ˙ enait ˙ e , and Marko Robnik - Šikonja . 2020 . Multilingual culture - independent word anal - ogy datasets . In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4074 – 4080 , Marseille , France . European Language Re - sources Association . Asahi Ushio , Luis Espinosa Anke , Steven Schockaert , and Jose Camacho - Collados . 2021 . BERT is to NLP what AlexNet is to CV : Can pre - trained language models identify analogies ? In Proceedings of the 59th Annual Meeting of the Association for Compu - tational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Vol - ume 1 : Long Papers ) , pages 3609 – 3624 , Online . As - sociation for Computational Linguistics . Denny Vrandeˇci´c and Markus Krötzsch . 2014 . Wiki - data : A free collaborative knowledgebase . Commun . ACM , 57 ( 10 ) : 78 – 85 . Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , brian ichter , Fei Xia , Ed H . Chi , Quoc V Le , and Denny Zhou . 2022 . Chain of thought prompting elicits reasoning in large language models . In Ad - vances in Neural Information Processing Systems . Sam Wiseman and Alexander M . Rush . 2016 . Sequence - to - sequence learning as beam - search op - timization . In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process - ing , pages 1296 – 1306 , Austin , Texas . Association for Computational Linguistics . Wentao Wu , Hongsong Li , Haixun Wang , and Kenny Q . Zhu . 2012 . Probase : A probabilistic taxonomy for text understanding . In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data , SIGMOD ’12 , page 481 – 492 , New York , NY , USA . Association for Computing Machinery . Ikuya Yamada , Akari Asai , Hiroyuki Shindo , Hideaki Takeda , and Yuji Matsumoto . 2020 . LUKE : Deep contextualized entity representations with entity - aware self - attention . In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) , pages 6442 – 6454 , On - line . Association for Computational Linguistics . Xi Ye , Semih Yavuz , Kazuma Hashimoto , Yingbo Zhou , and Caiming Xiong . 2022 . RNG - KBQA : Generation augmented iterative ranking for knowl - edge base question answering . In Proceedings of the 60th Annual Meeting of the Association for Compu - tational Linguistics ( Volume 1 : Long Papers ) , pages 6032 – 6043 , Dublin , Ireland . Association for Com - putational Linguistics . Ningyu Zhang , Lei Li , Xiang Chen , Xiaozhuan Liang , Shumin Deng , and Huajun Chen . 2022 . Multimodal analogical reasoning over knowledge graphs . arXiv preprint arXiv : 2210 . 00312 . Zhengyan Zhang , Xu Han , Zhiyuan Liu , Xin Jiang , Maosong Sun , and Qun Liu . 2019 . ERNIE : En - hanced language representation with informative en - tities . In Proceedings of the 57th Annual Meet - ing of the Association for Computational Linguis - tics , pages 1441 – 1451 , Florence , Italy . Association for Computational Linguistics . A Terminology Deﬁnition in A NALOGY KB To better understand the schema for analogies in A NALOGY KB , we list the terminologies in Ta - ble 10 . B Analogy Recognition Task B . 1 Benchmark We compare our methods with baselines and hu - man performance in 6 different benchmarks . For benchmarks without training sets , we only ﬁne - tune models on their validation sets . • E - KAR ( Chen et al . , 2022 ) : a knowledge - intensive analogical reasoning benchmark sourced from the publicly available Civil Ser - vice Examinations ( CSE ) of China , which con - tains linguistic , commonsense , encyclopedic , and cultural ( e . g . , idiom and historical ) knowl - edge . The SOTA model on this benchmark is proposed by Chen et al . ( 2022 ) • BATS ( Gladkova et al . , 2016 ) : This bench - mark contains more than 1 , 000 analogies , which can be divided into four categories : lex - icographic , encyclopedic , derivational and in - ﬂectional morphology . The SOTA model on this benchmark is proposed by Ushio et al . ( 2021 ) . • U2 ( Boteanu and Chernova , 2015 ) : a bench - mark using word analogy problems from an educational resource . The SOTA model on this benchmark is proposed by Ushio et al . ( 2021 ) . • U4 ( Boteanu and Chernova , 2015 ) : this bench - mark also comes from an educational resource but is harder than U2 . The SOTA model on this benchmark is proposed by Ushio et al . ( 2021 ) • Google ( Mikolov et al . , 2013b ) : a benchmark for intrinsic evaluation of word embeddings , which contains semantic and morphological relations . The SOTA model on this benchmark is proposed by Chen et al . ( 2022 ) • SAT ( Turney et al . , 2003 ) : a benchmark con - structed from a US college admission test con - sisting of 374 word analogy problems . The SOTA model on this benchmark is proposed by Ushio et al . ( 2021 ) . / * Task prompt * / Find the most analogous candidate answer that follows the relations in the query . / * Examples * / Question : candy : sugar Choice : A : wig : hair B : ocean : atmosphere C : cry : scream D : house : roof Please choose A , B , C or D . Answer : A / * Examples w COT * / Question : candy : sugar Choice : A : wig : hair B : ocean : atmosphere C : cry : scream D : house : roof Please choose A , B , C or D . Answer : Candy is made of sugar and wig is made of hair . Ocean is made of water . Cry is not made of scream . House is not made of roof . So the answer is A / * Test data * / Question : raphael : painter Choice : A : andersen : plato B : reading : berkshire C : marx : philosopher D : tolstoi : edison Please choose A , B , C or D . Answer : C Table 9 : Examples of prompt for LLMs for analogy recognition task . Generated texts by LLMs are high - lighted . To pre - train RoBERTa - Large on A NALOGY KB , we randomly sample 5 , 000 analogies of the same relation and 5 , 000 analogies of analogous rela - tions from A NALOGY KB and formulate them into the multiple - choice question - answering for - mat . Speciﬁcally , for each instance , we randomly sample a term pair from A NALOGY KB as a query and select another term pair from the analogous relation as the answer . Then we randomly sam - ple 3 term pairs from the relations that can not be analogous to the relation of the query as distrac - tions . We also randomly sample 10 , 000 data of the same relations as an ablated variant to show the effectiveness of analogies of analogous relations ( denoted as Data same ) . The construction method is similar , except that the query and answer are derived from the same relation . Additionally , we randomly sample 10 , 000 data points from A NALO - GY KB and construct analogy - style data ( denoted as Data pseudo ) . Speciﬁcally , we randomly sample Category Deﬁnition Example Analogies A : B : : C : D ( A is to B as C is to D ) Up : Down : : High : Low , Tim Cook : Apple : : Joe Biden : USA Term pairs A : B or C : D Left : Right , Tim Cook : Apple Relation pairs Two relations ( antonym , CEO ) , ( CEO , head of state ) Analogous relations Two relations that can form analogies ( CEO , head of state ) Analogies of analogous relations A : B : : C : D where the relation of A : B is different but analogous to the relation of C : D Tim Cook : Apple : : Joe Biden : USA Table 10 : The deﬁnitions of terminologies with examples in the schema for A NALOGY KB Model E - KAR BATS U2 U4 Google SAT Mean GPT - 2 - XL 29 . 01 25 . 73 23 . 68 28 . 01 33 . 20 25 . 82 27 . 58 BART - L 29 . 77 31 . 18 25 . 44 32 . 18 41 . 80 25 . 52 30 . 98 COMET G 28 . 62 30 . 29 30 . 26 31 . 94 36 . 80 29 . 08 31 . 17 COMET B 36 . 64 37 . 97 33 . 33 40 . 50 47 . 60 35 . 60 38 . 61 A NALOGY KB G 40 . 18 48 . 95 35 . 59 41 . 32 53 . 86 37 . 62 42 . 92 A NALOGY KB B 42 . 06 51 . 36 40 . 70 42 . 57 59 . 40 41 . 45 46 . 26 Table 11 : Compare the word embedding trained upon A NALOGY KB and commonsense reasoning data . 50 , 000 term pairs without considering analogous from A NALOGY KB as the data pool . For each data point , we randomly sample 5 term pairs from the data pool and choose one as the query , one as the answer , and the remaining three as distractions . B . 2 Details of Baselines Word Embedding For the method of pre - trained word embeddings , we follow the method proposed by Ushio et al . ( 2021 ) . The authors ﬁrst convert A : B : : C : D into a natural sentence to feed into the LM and then propose the marginal likelihood bi - ased perplexity to select the most plausible answer candidate . Large Language Models For LLMs , we design the instruction as shown in Table 9 and let it gen - erate the answers . We attempt prompt LLMs with two examples . All the examples are written by hu - mans . In addition , we also adopt the explanations from E - KAR before each answer to achieve chain of thought ( CoT ) prompting . B . 3 Training Process To pre - train language models on the sample data from A NALOGY KB , we follow the code from Hug - gingface 6 . The training settings are : batch size = 64 , learning rate = 3e - 5 , dropout rate = 0 . 1 and 6 https : / / huggingface . co / docs / transformers / tasks / multiple _ choice training epoch = 10 . B . 4 Comparison with Knowledge Model To prove that the gain in performance comes from A NALOGY KB rather than merely beneﬁting from commonsense knowledge from ConceptNet and Wikidata , we compare the embeddings trained on A NALOGY KB with contextualized embeddings from COMET ( Bosselut et al . , 2019 ) , a common - sense knowledge language model trained on Con - ceptNet using either GPT - 2 - Large ( Radford et al . , 2019 ) ( COMET G ) or BART - Large ( Lewis et al . , 2020 ) ( COMET B ) as the backbone . We represent queries and candidates as the sum of the embed - dings for each term pair , and the candidate with the highest cosine similarity to the query is selected as the answer . Finally , we draw two ﬁndings based on the results in Table 11 : 1 ) Contextualized word embeddings from PLMs have poor performance in analogical reasoning but can be enhanced through training on A NALOGY KB , suggesting that A NAL - OGY KB can imbue LMs with analogical embed - dings , a topic for future investigation ; 2 ) Common - sense knowledge can indeed improve model per - formance on analogy recognition . However , the model trained on A NALOGY KB prevailing over commonsense data indicates the effectiveness of A NALOGY KB . E - K A R B A T S U N I T 2 U N I T 4 G o o g l e S A T M e a n 0 20 40 60 80 100 P r e - t r i a n e d A cc ( % ) 46 79 50 46 96 49 61 39 73 39 38 89 41 53 43 79 45 45 92 48 59 Data Data con Data wiki E - K A R B A T S U N I T 2 U N I T 4 G o o g l e S A T M e a n 0 20 40 60 80 100 F i n e - t un e d A cc ( % ) 53 90 87 76 97 59 77 47 88 66 58 97 53 68 52 89 76 67 97 57 73 Figure 6 : The accuracy of RoBERTa - Large trained on different data subsets on the analogy recognition task . Data denotes the dataset sampled directly from A NALOGY KB and Data con ( or Data wiki ) denotes the analogies only from ConceptNet ( or Wikidata ) . All the datasets have the same size . / * Task prompt * / Please make analogies . / * Examples * / input : artist is to paintbrush as magician is to output : wand input : razor is to shave as knife is to output : cut / * Test data * / input : classroom is to desk as church is to output : pew Table 12 : Examples of prompt for LLMs for analogy generation task . Generated texts by LLMs are high - lighted . B . 5 Comparison with Different KB Source We also create two ablated variants to train the models to evaluate the necessity of ConceptNet and Wikidata : 1 ) Analogies from ConceptNet , de - noted as Data con : we randomly sampled 10 , 000 ( the same size as before ) data of the relations only in ConceptNet as an ablated variant . 2 ) Analogies from Wikidata , denoted as Data wiki : we randomly sampled 10 , 000 data of the relations only in Wiki - data as an ablated variant . The results in Figure 6 show that A NALOGY KB can combine the commonsense knowledge of Con - ceptNet and the entity knowledge of Wikidata and thus exhibits superior performance in improving the analogy - making ability of models compared to utilizing a single data source . Target Source Attribute mapping Argument War Debater Combatant Topic Battleground Claim Position Criticize Attack Rhetoric Maneuver Table 13 : Example mappings in SCAN . For a source term , multiple related attributes are mapped to corre - sponding attributes of the target term . C Analogy Generation Task C . 1 Training Process To pre - train T5 - Large on the 1 million data sam - pled from A NALOGY KB , we follow the code from Huggingface . 7 To construct the training data , we convert A : B : : C : D to “ A is to B as C is to D ” and let T5 - Large generate the term D given the input text “ A is to B as C is to ” . The training settings are : batch size = 32 , learning rate = 3e - 5 , dropout rate = 0 . 1 and training epoch = 20 . For LLMs , we design the instruction as shown in Table 12 and let it generate the term D . C . 2 Analogy in Complex Situation Dataset SCAN ( Czinczoll et al . , 2022 ) is an anal - ogy dataset consisting of 449 analogy instances clustered into 65 full - concept mappings . An exam - ple mapping in SCAN is shown in Table 13 . Unlike the previous analogy dataset , SCAN mainly con - tains metaphorical and scientiﬁc analogies , which are abstract and thus rarely appear in the corpus and are difﬁcult for LMs . In addition , each term in SCAN only has one token and SCAN is not conﬁned to the word analogy task due to its full - concept mappings . Baseline The original paper evaluates the analog - ical capabilities of GPT - 2 and BERT on the SCAN dataset . The authors convert the analogy instance to “ If A is like B , then C is like D ” , and force the models to predict the last token of the sentence . For GPT - 2 , the model needs to generate the last token given the input text “ If A is like B , then C is like ” . For BERT , the authors ﬁrst mask D as “ If A is like B , then C is like [ MASK ] ” and let the model predict word D . In addition , the authors ﬁne - tune the LMs on the 1 , 500 - sized training set of BATS ( i . e . , + BATS ) and investigate whether the models learn about 7 https : / / huggingface . co / docs / transformers / tasks / summarization analogical reasoning in general after training on BATS . We follow this setting and randomly sample 1 , 500 data from A NALOGY KB and ﬁne - tune the LMs on the sample data ( i . e . , + A NALOGY KB ) . To prove the necessity of analogies of analogous relations , we also randomly sample 1 , 500 analogies of the same relations as an ablated variant ( i . e . , + A NALOGY KB E ) . We further explore the performance of LLMs on the SCAN dataset . Speciﬁcally , we also adopt the prompt in Table 12 to let LLMs generate the word D . Since each entity in SCAN has only one token , we can obtain top 5 results from LLMs through the OpenAI API . C . 3 Evaluation Metrics Following Czinczoll et al . ( 2022 ) , we report accu - racy , recall @ 5 and recall @ 10 and the mean recip - rocal rank ( MRR ) to compare the performance of models . To reduce computing , we only consider the MRR of the ﬁrst token of the target word among the top 10 predicted tokens . The RR of a label is 0 if it is not in the top 10 tokens . Training Process The training settings of GPT - 2 and BERT are : batch size = 128 , learning rate = 3e - 5 , dropout rate = 0 . 1 and training epoch = 10 .