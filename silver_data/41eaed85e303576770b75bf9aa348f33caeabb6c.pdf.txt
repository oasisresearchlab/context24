505 Deradicalizing YouTube : Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos NUHA ALBADI , Taibah University , Saudi Arabia and University of Colorado Boulder , USA MARAM KURDI , Taif University , Saudi Arabia and University of Colorado Boulder , USA SHIVAKANT MISHRA , University of Colorado Boulder , USA Growing evidence suggests that YouTube’s recommendation algorithm plays a role in online radicalization via surfacing extreme content . Radical Islamist groups , in particular , have been profiting from the global appeal of YouTube to disseminate hate and jihadist propaganda . In this quantitative , data - driven study , we investigate the prevalence of religiously intolerant Arabic YouTube videos , the tendency of the platform to recommend such videos , and how these recommendations are affected by demographics and watch history . Based on our deep learning classifier developed to detect hateful videos and a large - scale dataset of over 350K videos , we find that Arabic videos targeting religious minorities are particularly prevalent in search results ( 30 % ) and first - level recommendations ( 21 % ) , and that 15 % of overall captured recommendations point to hateful videos . Our personalized audit experiments suggest that gender and religious identity can substantially affect the extent of exposure to hateful content . Our results contribute vital insights into the phenomenon of online radicalization and facilitate curbing online harmful content . CCS Concepts : • Social and professional topics → Hate speech ; • Human - centered computing → Empirical studies in collaborative and social computing ; Social media ; Social recommendation . Additional Key Words and Phrases : hate speech , Islamist radicalization , detection , algorithmic audit , radical - ization audit , YouTube recommendations , Arab HCI ACM Reference Format : Nuha Albadi , Maram Kurdi , and Shivakant Mishra . 2022 . Deradicalizing YouTube : Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 505 ( November 2022 ) , 25 pages . https : / / doi . org / 10 . 1145 / 3555618 1 INTRODUCTION YouTube , the largest video hosting website [ 83 ] , has been under intense scrutiny from both the media [ 58 , 60 , 77 , 85 ] and the academic community [ 21 , 57 , 62 , 74 , 76 ] for allegedly serving as a radicalizing instrument driving people down rabbit holes of radical and extreme content . As with any other content personalization system , YouTube’s recommendations and search results serve personalized content tailored to individual users based on online users’ behavior and personal data [ 22 , 89 ] . A major concern often associated with content personalization systems is creating what is commonly referred to as the “filter bubble” effect , in which users get served content that reinforces their beliefs and social identity , while content from opposing viewpoints and perspectives gets filtered [ 67 ] . These radicalization and filter bubble accusations against YouTube have motivated numerous CSCW / HCI researchers to audit YouTube’s recommendation algorithm to investigate its tendency to surface and steer users toward problematic content [ 35 , 65 , 66 , 74 ] . Some of these Authors’ addresses : Nuha Albadi , Taibah University , Department of Computer Science , Saudi Arabia , University of Colorado Boulder , Department of Computer Science , USA , nuha . albadi @ colorado . edu ; Maram Kurdi , Taif University , Department of Computer Science , Saudi Arabia , University of Colorado Boulder , Department of Computer Science , USA , maram . kurdi @ colorado . edu ; Shivakant Mishra , University of Colorado Boulder , Department of Computer Science , USA , mishras @ colorado . edu . © 2022 Copyright held by the owner / author ( s ) . This is the author’s version of the work . It is posted here for your personal use . Not for redistribution . The definitive Version of Record was published in Proceedings of the ACM on Human - Computer Interaction , https : / / doi . org / 10 . 1145 / 3555618 . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . a r X i v : 2207 . 00111v2 [ c s . S I ] 15 A u g 2022 505 : 2 Nuha Albadi , Maram Kurdi , and Shivakant Mishra audit studies accounted for personalization by creating multiple user accounts with different watch histories and personal data [ 35 , 66 ] , while others assessed the platform’s recommendation system for a generic ( non - personalized ) user account [ 65 , 74 , 76 ] . The focus of these studies was on misinformation [ 35 ] , conspiracy theories [ 66 ] , and radicalization within a Western context , e . g . , the radical right [ 74 , 76 ] . Evidence shows that radical Islamist groups have been benefiting from the unparalleled growth in YouTube audiences to proliferate jihadist propaganda and other radical ideologies [ 21 , 38 , 57 ] . Yet , auditing YouTube for Islamist radical content remains a largely unexplored research area . To address this gap , this paper presents a large - scale , quantitative study to investigate the prevalence of Arabic YouTube videos promoting hate against religious minorities , the tendency of YouTube’s recommendation algorithm to surface and recommend such content , and how these recommendations are affected by users’ watch history and demographics . Our study focuses on six religious groups : Muslims , Christians , Jews , atheists , Sunni ( the most prominent Islamic denomina - tion ) , and Shia ( the second - largest Islamic denomination ) . To perform a thorough assessment of YouTube’s algorithm , we conduct two types of analysis ; 1 ) non - personalized analysis , where we assess YouTube’s recommendation algorithm for a logged - out user scenario ; 2 ) personalized audits , in which we audit YouTube for several logged - in user accounts with different personal attributes . For our non - personalized analysis , our paper answers the following research questions : RQ1 What is the extent of religiously intolerant Arabic videos ? Which religious groups are targeted the most ? RQ2 How often does YouTube’s recommendation algorithm recommend hateful videos ? Does it recommend hateful videos to non - hateful ones ? To account for personalization in our assessment , our paper answers the following research question : RQ3 What is the effect of personalization based on ( a ) religious ideology ( moderate vs . radical ) , ( b ) Islamic denomination ( Sunni vs . Shia ) , and ( c ) gender ( female vs . male ) on the volume of hateful content presented to users in search results and recommendations ? We used YouTube’s API to collect data for our first two research questions concerning our non - personalized analysis . It has been shown that videos returned by the API were similar to those displayed to a logged - out user browsing YouTube [ 66 ] . Thus , using the API , we collected a seed list of 3 , 000 Arabic YouTube videos discussing various religious groups . Using crowdworkers , we acquired annotations for these videos to identify the ones promoting religious hate along with the targeted religious group ( s ) . For each video in our seed list , we collected its top four recom - mendations going five levels deep , which resulted in a total of over 350K unique recommended videos . Leveraging our annotated dataset , we developed a deep learning classifier that can ef - fectively detect hateful YouTube videos with an accuracy of 0 . 76 . We applied our classifier to the 350K recommended videos to estimate the prevalence of religiously intolerant Arabic videos . Finally , using traced recommendation relationships between these videos , we created a directed graph that resembled YouTube’s recommendation graph to measure the tendency of YouTube’s recommendation algorithm to suggest hateful content . While our first two research questions explore religiously intolerant Arabic content on YouTube for a generic ( non - personalized ) user account , our third research question seeks to understand the effect of personalization on the volume of hateful videos presented to users with different demo - graphics and watch history . In particular , we investigate the impact religious ideology ( moderate vs . radical ) , Islamic denomination ( Sunni vs . Shia ) , and gender ( female vs . male ) on the level of exposure to hateful videos in search results and recommendations . To this end , we carefully crafted eight different user profiles , each with a distinctive set of personal attributes . To establish the Islamic Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 3 denomination and religious ideology attributes , we slowly built a watch history for each of the eight profiles in a controlled environment over the course of nine weeks ( more on the methodology in Section 6 ) . We then conducted two systematic audit experiments , Search and Recommendation audits , to explore whether personalization based on religious ideology , Islamic denomination , and gender has any significant effect on the levels of exposure to hateful content . The following presents a summary of our key findings : • Our deep learning classifier can effectively detect hateful Arabic videos with an F 1 score of 0 . 69 , a promising first attempt result . We make our ground truth dataset public 1 for use by the research community . • We find that videos promoting religious hatred in the Arabic language are prevalent on search results ( 29 . 53 % ) and first - level recommendations ( 20 . 80 % ) , and they are mainly targeting Shia and atheists ( RQ1 ) . • We find that recommendations pointing to hateful videos represent about 15 % of overall recommendations . Further , about 12 % of videos recommended to non - hateful videos are hateful , while about 31 % of videos recommended to hateful videos are also hateful ( RQ2 ) . • We observe a religious ideology effect on recommendations for Shia profiles , in which radical Shia profiles were recommended 29 % more hateful videos than moderate Shia profiles ( RQ3a ) . • We find that moderate Sunni profiles were recommended 21 % more hateful videos than moderate Shia profiles , indicating an Islamic denomination effect ( RQ3b ) . • We observe a gender effect on videos recommended to Sunni profiles , in which male Sunni profiles were recommended significantly more hateful videos ( 16 % increase ) than female Sunni profiles ( RQ3c ) . • We find that personalization in general increases the risk of getting recommended hateful videos by 46 % compared to recommendations obtained from YouTube API , which are not personalized ( RQ3 ) . 2 BACKGROUND AND LITERATURE REVIEW This section provides background information on practices around religion , culture , and norms within Arab countries to help the reader better understand some of the concepts and findings presented in this paper . Additionally , to situate our research within the CSCW literature , we review scholarly works on online hate and extremism and algorithmic auditing . 2 . 1 Arabs and Religion The Middle East and North Africa ( MENA ) region consists of twenty Arab countries . Muslims make up 93 % of the region’s total population , followed by Christians ( 3 . 7 % ) , Jews ( 1 . 6 % ) , and atheists ( 0 . 6 % ) [ 70 ] . The two major denominations of Islam are Sunni ( 87 - 90 % ) and Shia ( 10 - 13 % ) [ 69 ] . While Muslims are predominantly of the Sunni denomination , Shia comprises the majority of Muslims in Bahrain ( 70 % ) , Iraq ( 65 % ) , and Lebanon ( 55 % ) [ 69 ] . To better comprehend the study methodology and results , it is essential to understand the central religious role of clerics within an Islamic context . An Islamic cleric , also known as imam , mufti , and sheik , refers to a religious leader who oversees worship , interprets religious texts , and passes fatwa , i . e . , a ruling under Islamic ( Shari’ah ) law [ 54 ] . An Islamic cleric more closely resembles in their sacred role and status a Jewish rabbi than a Christian priest [ 72 ] . Although Muslim clerics don’t serve as mediators between God and people , many Muslims feel the need to follow clerics’ rules and teachings even on the most private matters [ 72 ] . In general , Islamic clerics are regarded as well - respected community leaders , with some of them having a TV show and / or YouTube channel 1 https : / / osf . io / cf9w8 / ? view _ only = aa81f43ff28c4faaa7514ccccc6a386c Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 4 Nuha Albadi , Maram Kurdi , and Shivakant Mishra where they educate , teach , and advise people on matters of faith and everyday life . However , while many Muslim clerics adopt a moderate interpretation of Islam , some hold a more radical view of Islam centered around the ideology of militant jihad [ 59 ] . Middle Eastern societies have long suffered from civil wars and domestic tensions that are partly caused by conflicting religious beliefs [ 49 , 71 ] . The Sunni - Shia divide dates back to the seventh century [ 73 ] , and it is believed to have ignited several wars in the region [ 29 ] . International conflicts between Arabs ( predominantly Sunni ) against Israelis ( mostly Jewish ) and Iranians ( mainly Shia ) have religious aspects in addition to political , economic , and ethnic ones [ 17 , 27 ] . Apostasy , i . e . , renouncing one’s religious belief , is considered a major crime deserving capital punishment in five MENA countries [ 24 ] . This religious tension , among other factors , contributes to the region’s civil unrest and sectarian violence . However , numerous academic institutions and government agencies have devoted efforts to promoting tolerance and acceptance of stigmatized religious minorities in the region in recent years . For instance , in 2017 , Saudi Arabia established Etidal 2 , a global center for combating extremist ideology by providing counter - narratives that promote coexistence , moderation , and acceptance values . Arabic research raising awareness and mitigating online hate speech has also grown in number in recent years [ 4 , 6 – 8 , 20 , 25 , 26 , 30 , 44 , 56 , 63 ] . Our work extends these efforts by investigating the prevalence of religiously intolerant Arabic YouTube videos and building automated detection models to hinder their reach . 2 . 2 Online Radicalization and Hate Speech Online hate speech has been extensively studied in a Western context across multiple protected characteristics such as race [ 16 , 18 , 41 , 48 ] , gender [ 12 , 28 , 78 ] , religion [ 11 , 43 , 48 , 90 ] , sexual orientation [ 16 , 42 , 48 ] , and disability [ 16 , 42 ] . Some studies focused on identifying targets of hate in online social networks [ 42 , 52 , 80 ] , while others measured the growth and reach of hate speech over time [ 46 , 47 ] . Multiple machine learning techniques and algorithms have been explored such as Naive Bayes [ 41 ] support vector machines [ 12 , 43 ] , regressions [ 12 ] , decision trees [ 12 , 36 ] , and neural networks [ 7 , 12 , 91 ] . Multiple text representation techniques have been used ranging from character n - grams [ 88 ] to word and paragraph embeddings [ 12 , 23 ] . Our work extends this line of research by investigating the discriminative power of visual features extracted from video thumbnails in distinguishing online hateful videos . Although hate speech and radicalization have been studied in multiple social network sites such as Twitter , Reddit , Gab . com ( a loosely moderated social network ) , and Whisper ( an anonymous social network ) , it has been rarely studied on YouTube ( exceptions include [ 2 , 45 , 48 , 64 ] ) . The researchers in [ 64 ] observed that right - wing YouTube channels contain a high volume of hateful content against Muslims and the LGBTQ community . In [ 45 ] , the researchers developed a detection model to identify videos on YouTube that are being attacked by third - party coordinated hate raids . Mathew et al . [ 48 ] investigated the use of counter - speech in comments to tackle hateful YouTube videos . YouTube comments have been studied for other related issues such as toxicity [ 61 ] , harassment [ 3 ] , cyberbullying [ 53 ] , and moderation in general [ 39 ] . There exists limited CSCW work that investigates radicalization within an Islamist context , distinguishable exceptions include [ 6 , 7 , 40 ] . Kursuncu et al . [ 40 ] modeled Islamist extremist communications on Twitter along three dimensions : religion , ideology , and hate . The measurement study conducted by Albadi et al . [ 7 ] concluded that 42 % of Arabic tweets discussing other religions incited hatred toward religious minorities in the region . In their follow - up study [ 6 ] , they found that Twitter bots ( i . e . , automated accounts ) were responsible for 11 % of those hateful tweets . Our 2 https : / / etidal . org / en / home / Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 5 work builds on and extends these CSCW scholarships by providing important insights into Islamist radicalization and religious hate speech on YouTube . 2 . 3 Algorithmic Auditing It is well - known that all major social media , including YouTube , feed personalized content to their users based on their online activities and collected personal data . According to YouTube , these personal data include watch and search history , age , gender , geographic location , the content of subscribed channels , time of the day , and other metrics related to the quality of the video ( e . g . , whether or not other users watched the entirety of the video ) [ 22 , 89 ] . YouTube has also stated that more than 70 % of the total viewing time on their platform is the result of a recommendation - driven viewing [ 82 ] . Content personalization systems have been said to have a filter bubble effect , which refers to the intellectual and ideological isolation created by content personalization systems that trap an individual in a bubble of like - minded content , isolating them from other perspectives and viewpoints [ 67 ] . These unintended risks of content personalization systems have intensified the need for algorithmic auditing , which refers to the " process of investigating the functionality and impact of decision - making algorithms " [ 50 ] . YouTube’s filter bubble and radicalization claims have motivated the CSCW / HCI community to assess the platform’s tendency to surface and steer users toward far - right radical content [ 74 ] , misinformation and conspiracy theories [ 35 , 66 ] , and disturbing videos targeting kids [ 65 ] . Some audit studies accounted for personalization by creating Google accounts with different watch history [ 35 , 66 ] , whereas others performed random walks without being logged into a Google account [ 65 , 74 , 76 ] . On the other hand , there is a limited body of work that refutes these algorithmic radicalizations and filter bubble claims , particularly for online news consumption on YouTube and other search engines [ 13 , 31 , 34 , 75 ] . For example , Bakshy et al . [ 13 ] studied Facebook news consumption patterns and found that exposure to ideologically opposing views is more dependent on the individual choice rather than the ranking algorithm . In a recent study [ 34 ] , the authors examined radical news consumption on YouTube and found that most people who consume far - right videos arrive at such videos from search results , the home page , or an external website rather than following YouTube recommendation chains . In an empirical study [ 31 ] , the authors explored the effect of personalization on the homogeneity of Google News and found no evidence to support the filter bubble phenomenon . Our study extends these audit experiments to include an Islamist radicalization context by measuring the effect of personalization on the degree of exposure to religiously intolerant videos in the Arabic language . 3 DATA AND PRIOR ANALYSIS 3 . 1 Data Collection Our study is mainly concerned with YouTube videos promoting religious hate in the Arabic language , and thus we focus on the most common religious beliefs among Arabs . These are Islam , Christianity , Judaism , atheism , and the two main denominations of Islam : Sunni and Shia . In September 2019 , we used YouTube API v3 3 to collect data for this part of the study . YouTube API doesn’t account for personalization [ 66 ] , i . e . , videos returned by the API are not affected by watch history or any other personal data , but rather are based on content relevance to the search query and other quality and user engagement metrics . 3 https : / / developers . google . com / youtube / v3 / Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 6 Nuha Albadi , Maram Kurdi , and Shivakant Mishra Table 1 . Number of collected search result videos per religious group . Religious group Number of videos Christians 1 , 172 Muslims 1 , 003 Shia 850 Jews 782 Sunni 739 Atheists 646 All religious groups 5 , 192 Table 2 . Number of unique videos within each rec - ommendation level . Level # Number of unique videos Level 1 8 , 069 Level 2 23 , 319 Level 3 60 , 466 Level 4 137 , 789 Level 5 286 , 190 All levels 351 , 262 To assess of the prevalence of religiously intolerant Arabic videos in YouTube’s search results , we queried YouTube API using 208 impartial Arabic keywords 4 that refer to each of the aforementioned religious beliefs / groups . We manually compiled these keywords and made sure they don’t include any religious slurs , hate terms , or insults that could bias search results . For example , for Muslims , the used keywords translate to : a Muslim [ singular masculine ] , a Muslim [ singular feminine ] , Muslims [ plural masculine ] , Muslims [ plural feminine ] , Muslims [ dual masculine ] , Muslims [ dual feminine ] , Islam , the Islam , the religion of Islam , Islamic religion . The reason for having what seems like a large number of keywords is that Arabic nouns can be either singular , dual , or plural ; each can be either masculine or feminine . Additionally , Arabic nouns can have different spellings , and thus we accounted for all possible variations in spellings . YouTube API allows for selecting a sorting method by which search result videos in the API response can be sorted . This can be based on either date , relevance ( to the search query ) , rating , and view count . We selected relevance , which is also the default sorting method when accessing YouTube from a browser . For each keyword , we collected up to 50 most relevant Arabic videos . For each video , we collected its metadata ( e . g . , title , description , number of views , and number of likes ) , thumbnail , and up to 100 most recent comments . Table 1 summarizes the number of videos collected per set of keywords . In total , we collected 5 , 192 unique videos ; of these 1 , 172 were collected for Christian - related keywords , 1 , 003 for Muslim - related keywords , 850 for Shia - related keywords , 782 for Jew - related keywords , 739 for Sunni - related keywords , and 646 for atheist - related keywords . For each religious group , we randomly selected 500 videos to be annotated ( see Section 3 . 2 ) as hateful or non - hateful religious videos , which resulted in creating a ground truth dataset of 3 , 000 videos . Next , to measure the prevalence of religiously intolerant Arabic videos in YouTube recommen - dations , we used a cascaded approach where we first used YouTube API to collect the top four videos recommended for each video in our ground truth dataset . This resulted in 8 , 069 unique level 1 recommended videos . We then repeated this process of getting top four recommended videos for each video in level 1 to create level 2 recommended videos , which turned out to have 23 , 319 unique videos . We repeated this process further to create level 3 , level 4 and level 5 recommended videos , consisting of 60 , 466 , 137 , 789 and 286 , 190 unique videos respectively for a total of 351 , 262 unique recommended videos across all five levels ( refer to Table 2 ) . This also resulted in capturing 929 , 596 recommendation links between videos . 4 https : / / osf . io / cf9w8 / ? view _ only = aa81f43ff28c4faaa7514ccccc6a386c Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 7 3 . 2 Data Annotation To create a ground truth dataset of hateful and non - hateful religious videos , we used Appen [ 10 ] , a crowdsourcing service that is known for having Arabic - speaking annotators [ 55 ] , to get annotations for the 3 , 000 videos in our ground truth dataset . We provided annotators with the definition of religious hate speech as outlined in [ 5 ] , “ a speech that is insulting , offensive , or hurtful and is intended to incite hate , discrimination , or violence against an individual or a group of people on the basis of religious beliefs or lack thereof . ” We also provided annotators with examples of videos that should be classified as hateful ( e . g . , a video promoting a belief that disbelievers will burn in hell ) , non - hateful ( e . g . , a documentary about the Jewish culture ) , or unrelated to religion . Specifically , we asked annotators to watch enough of the video until they reach a judgment on whether to classify it as : a ) hateful , if the purpose of the video seems to be an incitement of hatred , intolerance , or violence against one or more religious groups ; b ) non - hateful , if the video is related to religions , but its purpose is not a promotion of religious hatred / intolerance ; or c ) unrelated , if the video is either deleted , unrelated to any religion , or in a non - Arabic language with no Arabic subtitles . If annotators decided that a video was hateful , they were asked a second question to specify one or more religious groups that the video was targeting . Three different annotators judged each video . To ensure high - quality annotations , we first created a set of 190 test questions ( refer to Appendix A ) consisting of evidently hateful , non - hateful , and unrelated videos from our seed videos that the first two authors reviewed , discussed , and agreed on their annotation . For annotators to qualify for the task , they had to pass an initial test consisting of five test questions with a minimum accuracy score of 80 % , i . e . , they had to answer at least 4 / 5 test questions correctly . Each page of work contained five videos to be annotated , one of which was a hidden test question . Annotators needed to maintain the 80 % accuracy score throughout the task . Those who couldn’t keep that accuracy were disqualified , and their annotations were excluded . To ensure that annotators actually watched part of the video before assigning annotations , we set a minimum time needed for annotators to finish a page of work . To specify this minimum time , we kept track of the time it took the first two authors to annotate each of the 190 test questions , and we found that it takes 1 . 2 minutes to annotate a video on average . To allow some flexibility , we set the minimum time for an annotator to complete a page of work consisting of five videos to five minutes . Annotators who finished a page of work in less than five minutes were disqualified , and their annotations were excluded . To make sure we only have Arabic - Speaking annotators , Google Translate was disabled , and the language for the task was set to Arabic . We paid each annotator 35 cents for completing a page of work . Given that completing a page of work was estimated to take 5 - 6 minutes , our average hourly pay ranged from $ 3 . 5 to $ 4 . 2 , which is slightly above the average hourly pay reported for a similar platform , Amazon Mechanical Turk [ 33 ] . We also note that our annotators were located within Arab countries , primarily Egypt and Algeria ; our pay was 4x the minimum pay in these countries . Upon completing the task , annotators were offered to participate in a task satisfaction survey . A total of 21 annotators participated in the survey . Overall , the task was rated 4 / 5 based on pay , clarity of instructions , fairness of test questions , and ease of the job . The pay , in particular , was rated 4 . 1 / 5 . In total , the annotation process was carried out by 151 different annotators . The interquartile mean of the time it took annotators to review a video was 1 . 3 minutes . Appen provides an inter - annotator agreement score for each question , reflecting the level of agreement between annotators’ answers weighted by their accuracy scores [ 9 ] . The first annotation question that asked to specify whether a video was hateful or not had an average agreement score of 0 . 84 , which is considered an almost perfect agreement . The second annotation question regarding identifying targeted religious Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 8 Nuha Albadi , Maram Kurdi , and Shivakant Mishra groups had an average agreement score of 0 . 46 , a moderate agreement which is expected for a question with seven options . Ethical Concerns . Although our institution’s IRB confirmed that our study doesn’t require IRB review , we acknowledge the potential risks of exposing annotators to potentially hateful content . Thus , in the annotation task description , we provided a warning for annotators regarding possible hateful , violence , and / or radical content that might be included in the videos they are about to watch . Additionally , annotators were able to quit the task at any time and get paid for their work . 3 . 3 Analysis of Ground Truth Dataset Here , we derive insights from our ground truth dataset , reporting the distribution of hateful and non - hateful videos , targeted religious groups , and relationships between hateful videos and hateful comments . To identify the distribution of hateful , non - hateful , and unrelated videos in our ground truth dataset , we considered the answer to the first annotation question with the highest confidence score . Confidence score is a score that Appen provides with each possible answer that reflects the level of agreement between annotators on that answer weighted by their accuracy scores . For example , if we have three annotators with different accuracy scores , judge a video with three possible answers ( e . g . , hateful , non - hateful , unrelated ) . Then , each answer would be accompanied by a confidence score that reflects Appen’s confidence in each answer based on the number of annotators who selected that answer weighted by their accuracy scores [ 9 ] . In our ground truth dataset , 29 . 53 % of the videos were found to be hateful , 52 . 97 % non - hateful , and 17 . 50 % unrelated . Examples of unrelated videos include videos discussing an acute condition as the word atheism in Arabic can also mean ‘acute’ , videos in the Persian language , a language different from the Arabic language but uses similar alphabets , and videos in the English language with no Arabic subtitles . To identify religious groups most targeted by hateful videos , we considered answer ( s ) to the second annotation question , that asks annotators to identify targeted religious groups , with a confidence score of 0 . 3 or higher . Since we had three annotators , selecting a lower threshold would result in having videos classified as hateful with no targeted religious group ( s ) ( i . e . , cases where annotators agreed that a video was hateful , but each selected a different targeted religion ) . Note that answers to the second annotation question were only used for analysis in this section , i . e . , they were not considered in our classifier or any later analysis . In analyzing religious groups targeted by hateful videos , we found that the most targeted religious group is the Shia , with about 34 % of hateful videos targeting them , closely followed by atheists ( 33 % ) . Christians ranked third with about 18 % of hateful videos targeting them , followed by Muslims ( 16 % ) . Jews ( 10 . 4 % ) and Sunni ( 9 . 5 % ) were among the least targeted religious groups . These percentages add up to more than 100 % as some videos were targeting more than one religious groups . To get further insight , we looked at each of the 500 videos collected for each religious group individually ( see Figure 1 ) . We observed that for the atheist dataset , more than half of the videos ( 55 % ) were deemed hateful toward them ; in the Shia dataset , almost half of the videos ( 46 % ) were considered hateful toward them . Across all datasets , the most targeted religious group turned out to be the religious group for which the dataset was collected , except in the Sunni dataset , where the most targeted religious group was the Shia rather than the Sunni . Next , we investigate whether hateful videos had a significantly larger proportion of hateful comments than non - hateful videos . To get annotations for YouTube comments , we leveraged the Twitter hate speech classifier [ 5 ] that was trained on Arabic tweets discussing the same religious groups considered in this paper . We preprocessed YouTube comments following the same preprocessing methodology that was used in training the Twitter classifier . The Twitter classifier can handle text of length up to 50 words . Thus , we truncated YouTube comments to that length . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 9 ( a ) Shia dataset ( b ) Atheists dataset ( c ) Christians dataset ( d ) Jews dataset ( e ) Muslims dataset ( f ) Sunni dataset Fig . 1 . Distribution of annotations across religious - group datasets . ( H ) denotes hateful and ( NH ) denotes non - hateful . This wasn’t an issue since YouTube comments had an average length of 14 words and a maximum of 1 , 707 words . For each video , we applied the Twitter classifier on up to 100 of its recent comments and found the most common class ( hateful vs . non - hateful ) among its comments . Figure 2 illustrates the distributions of hateful , non - hateful , and unavailable ( i . e . , disabled ) comments for both hateful and non - hateful videos . As expected , hateful videos had larger proportions of hateful comments than non - hateful videos . This difference in distributions was found to be statistically significant , 𝜒 2 ( 2 , 𝑁 = 2475 ) = 61 , 𝑝 < 0 . 001 . To gain deeper insights , we looked at comment hate class distributions for hateful and non - hateful videos across each religious group individually ( see Figure 3 ) . Across all religious groups , hateful videos had larger proportions of hateful comments than non - hateful comments . As for non - hateful videos , we can see that for Jews , atheists , and Shia the number of non - hateful videos with hateful comments exceeded the number of non - hateful videos with non - hateful comments , which is counterintuitive . This difference was the widest for the Jews dataset , i . e . , non - hateful Jews - related videos had the highest proportions of hateful comments , even higher than other religious group hateful videos . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 10 Nuha Albadi , Maram Kurdi , and Shivakant Mishra Fig . 2 . Distribution of video comment hate class for hateful and non - hateful videos . ( a ) Shia dataset ( b ) Atheists dataset ( c ) Christians dataset ( d ) Jews dataset ( e ) Muslims dataset ( f ) Sunni dataset Fig . 3 . Distribution of video comment hate class for hateful and non - hateful videos across religious - group datasets . ( H ) denotes hateful , ( NH ) denotes non - hateful . 4 DETECTION OF VIDEOS PROMOTING RELIGIOUS HATE Building on our initial analysis reported in the previous section , we describe in this section our approach to identifying Arabic YouTube videos promoting religious hate using deep learning methods . We detail the features used , describe the model architecture , and report performance metrics on a hold - out testing dataset . 4 . 1 Methods We present a binary classification task to identify Arabic YouTube videos promoting religious hate . We focus on deep learning methods given their superiority and efficiency in different , but related classification tasks on YouTube [ 65 , 66 ] . To train , validate and evaluate our model , we used the videos identified as either hateful or non - hateful ( 2 , 475 videos out of the ground truth dataset of 3 , 000 videos by excluding unrelated videos ) . We used 70 % of the data for training , 10 % for validation , and 20 % for testing . We implemented our model using Keras [ 19 ] , a Python library for developing deep learning models , and TensorFlow [ 1 ] backend . This model was trained on different combinations of features that include title , description , tags , thumbnail , and statistical features ( See the next subsection ) . We used Keras functional API as it allows for the handling of multiple inputs . We used Adam algorithm for optimization , cross entropy for the loss function , and trained our model in batches of size 32 to achieve optimal performance and stability . Finally , we evaluated our models based on precision , recall , F 1 score , accuracy , and area under the receiver operating characteristic ( AUROC ) . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 11 4 . 2 Features Description and Model Architecture Below we describe the features we considered when developing our deep learning classifiers and explain their preprocessing steps . Title . To transform video title words to word embeddings , we used an Arabic pre - trained word embedding model , AraVec 3 . 0 [ 81 ] . AraVec’s Twitter - CBOW model was trained on 67M Arabic tweets , and it has a vocabulary size of 1 . 5M words and an embedding dimensionality of 300 . We preprocessed the title following the same preprocessing steps used by Aravec to maximize the number of found word embeddings for our title vocabulary . The size of the title vocabulary was 7 , 900 words , 87 % of these had an AraVec word embedding vector . The maximum title length after preprocessing was 23 words , and the average was 10 words . We padded the title feature vector to the maximum title length . Description . We preprocessed the video description similar to how we preprocessed the video title . However , unlike video title , video description tends to be lengthy , and thus we removed stop words from them . The maximum description length after preprocessing and stop words removal was 922 words , and the average was 47 words . The size of the description vocabulary was 31 , 444 words , 69 . 5 % of these were matched against AraVec vocabulary . To not confuse our model with extremely large description word count , we truncated / padded the description feature vector to the average description length . Thumbnail . We considered two methods for extracting thumbnail features . The first one was using transfer learning [ 15 ] through a pre - trained convolutional neural network ( CNN ) model . For that purpose , we used [ 65 ] ’s thumbnail feature extractor module that internally uses Inception - v3 [ 84 ] , a pre - trained CNN model trained on millions of images from the ImageNet dataset to transform each thumbnail image to a meaningful 2048 - dimensional feature vector . The model expects images to have a dimensionality of 299 ∗ 299 ∗ 3 , and thus we down - scaled our thumbnail images from 360 ∗ 480 ∗ 3 to the required dimensionality . We also considered building our own CNN model to extract thumbnail features . However , the model using the pre - trained CNN model substantially outperformed the one using our own CNN ; thus , we decided to use a pre - trained CNN model . Tags . Video tags are descriptive keywords optionally provided by the video uploader to provide context about the video content and are used to rank videos in search results . Similar to the title and description , we used AraVec word embedding model to transform tags into word embeddings . The maximum number of tags was 93 , and the average was 21 . The size of the tags vocabulary was 10 , 398 words , 74 . 33 % of these had a match against AraVec vocabulary . We padded the tags feature vector to the maximum tags length . Statistical Features . We considered the following statistical features : video view count , video like count , video dislike count , video duration in seconds , video comment count , channel view count , channel subscriber count , and channel video count . We also considered whether a given video had a larger proportion of hateful or non - hateful comments , which we were able to discern by applying the Twitter religious hate speech classifier , as discussed in Section 3 . 3 . This resulted in having a statistical feature vector of length 11 . We normalized the statistical feature vector so that all features ranged from 0 to 1 to prevent features that tend to have higher values ( e . g . , view count ) from having more influence on the model weights learning process . 4 . 3 Model Architecture Figure 4 illustrates the architecture of our deep learning classifier . The classifier handles each feature type ( i . e . , title , description , tags , thumbnail , and statistics ) in a separate branch . Title , description , and tags features are processed in architecturally similar branches consisting of : 1 ) a trainable embedding layer that maps words to their corresponding pre - trained word vectors ; 2 ) a Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 12 Nuha Albadi , Maram Kurdi , and Shivakant Mishra Fig . 4 . Model architecture of our deep learning classifier . bidirectional long / short - term memory ( BiLSTM ) [ 79 ] layer with 240 units to capture wide - range contextual information from both preceding and succeeding words . The thumbnail is processed by a pre - trained CNN ( Inception - v3 ) that transforms each thumbnail image into a 2048 - dimensional feature vector . The statistical feature vector is fed into a fully connected dense layer with 64 units , followed by a ReLU activation . The output of that is feed into another fully connected dense layer with 32 units , followed by a ReLU activation . The outputs from all branches are then concatenated and regularized with a 0 . 25 dropout layer before making a final prediction using a sigmoid activated , one - unit dense layer . 4 . 4 Results We developed several deep learning classifiers to identify hateful Arabic videos using different sets of features . Table 3 reports performance metrics of these classifiers on a hold - out testing dataset . At first , we investigated model performances trained on each feature type individually . Among individual features , the highest F 1 score ( 0 . 64 ) was achieved when training the model on the title feature , followed by the description feature ( 0 . 53 ) . Next , we explored different combinations of textual features and found that training the model on both title and description features increased F 1 score by 3 points and enhanced recall by 10 points . Finally , to further improve the model performance , we explored training the model on the thumbnail and statistical features in addition to the best textual features ( i . e . , title and description ) . As illustrated in Table 3 , training the model on title , description , and statistical features provided the best F 1 score ( 0 . 69 ) across all combinations of features . We use this classifier for all of our upcoming analyses . To our knowledge , our classifier represents the first effort in detecting hateful Arabic content on YouTube , and thus we couldn’t directly compare our model performance to any other model . However , there are other models that were developed to detect different but related problematic content on YouTube . For example , the researchers in [ 66 ] developed a deep learning classifier that was able to detect pseudo - scientific YouTube videos with a 0 . 74 F 1 score . In [ 45 ] , the authors developed a detection model to identify YouTube videos that are being targeted by third - party coordinated hate attacks with an F 1 score of 0 . 46 . Impressively , the binary classifier reported in [ 37 ] was able to detect inappropriate videos targeting kids with an F 1 score of 0 . 82 . Our model delivers Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 13 Table 3 . Evaluation results of our classification models using different features . Models with the highest F 1 score within each feature group are highlighted in bold . Feature group Features P R F 1 Acc . AUC Individualfeatures Title 0 . 68 0 . 61 0 . 64 0 . 76 0 . 8 Description 0 . 51 0 . 56 0 . 53 0 . 62 0 . 67 Tags 0 . 56 0 . 31 0 . 4 0 . 66 0 . 65 Thumbnail 0 . 42 0 . 52 0 . 46 0 . 58 0 . 59 Statistics 0 . 57 0 . 28 0 . 37 0 . 67 0 . 63 Textualfeatures Title + description 0 . 64 0 . 71 0 . 67 0 . 76 0 . 81 Title + tags 0 . 65 0 . 65 0 . 65 0 . 75 0 . 8 Title + description + tags 0 . 61 0 . 69 0 . 65 0 . 73 0 . 78 Best textual features + other features Title + description + thumbnail 0 . 64 0 . 67 0 . 65 0 . 73 0 . 8 Title + description + statistics 0 . 67 0 . 71 0 . 69 0 . 76 0 . 79 Title + description + thumbnail + statistics 0 . 64 0 . 56 0 . 6 0 . 75 0 . 81 comparable performance to these models and can effectively detect YouTube videos promoting religious hatred with an accuracy of 0 . 76 . However , we acknowledge that this is not a perfect performance , and it reflects the subjectivity and nuance in recognizing hate speech . It also signals the need for more research efforts in this area . Given that our classifier was trained and tested on search result videos , we next validate our classifier on recommended videos . To do that , we randomly selected 100 recommended videos ( 20 videos from each recommendation level ) . To obtain ground truth for these videos , the first two authors reviewed these videos and came to consensus on their label , following the hate speech definition and criteria discussed in Section 3 . 2 . We make public the list of videos used in this validation step along with the true label that we assigned to the video and the predicted label assigned by the classifier 5 . Table 4 summarizes the performance of our classifier on level 1 to level 5 recommended videos . Due to the small sample size for each recommendation level , the model performance is highly sensitive to the selected videos in each level . For example , in level 5 recommendation , precision and recall are zero , and that is due to not having any true examples of hateful videos in the sample for that level . Overall , the classifier on recommended videos delivered comparable results to those on search result videos with only 2 points down in F 1 score . While the precision decreased by 7 points , the recall improved by 4 points . This indicates that our model detects most hateful videos with the cost of flagging some innocuous videos as hateful . We argue that in the case of detecting hateful videos , recall weighs heavier than precision as the goal is to detect all videos that could potentially be hateful and then have human moderators review these videos for a final decision . Additionally , the probability that our classifier would incorrectly classify a non - hateful video as hateful ( i . e . , false positive rate ) is only 0 . 04 . It is also worth noting that 50 % of misclassified videos across the levels had a predicted hate probability of around 0 . 5 , i . e . , they were considered edge cases for the classifier . Given these relatively good results , we believe that our classifier can be reliably used to answer our research questions . 5 https : / / osf . io / cf9w8 / ? view _ only = aa81f43ff28c4faaa7514ccccc6a386c Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 14 Nuha Albadi , Maram Kurdi , and Shivakant Mishra Table 4 . Classifier performance on recommended videos . Recommendation level P R F 1 Acc . TP FN FP TN Level 1 1 0 . 67 0 . 8 0 . 95 2 1 0 17 Level 2 0 . 5 0 . 5 0 . 5 0 . 9 1 1 1 17 Level 3 0 . 67 1 0 . 8 0 . 95 2 0 1 17 Level 4 0 . 5 1 0 . 67 0 . 95 1 0 1 18 Level 5 0 0 0 0 . 95 0 0 1 19 All levels 0 . 6 0 . 75 0 . 67 0 . 94 6 2 4 88 Table 5 . Hateful and non - hateful video distributions within each level of YouTube’s recommendation graph . Level # Non - hateful ( % ) Hateful ( % ) Level 1 6 , 391 ( 79 . 20 % ) 1 , 678 ( 20 . 80 % ) Level 2 19 , 199 ( 82 . 33 % ) 4 , 120 ( 17 . 67 % ) Level 3 51 , 192 ( 84 . 41 % ) 9 , 454 ( 15 . 59 % ) Level 4 119 , 001 ( 86 . 36 % ) 18 , 788 ( 13 . 64 % ) Level 5 252 , 086 ( 88 . 08 % ) 34 , 104 ( 11 . 92 % ) all levels 308 , 552 ( 87 . 85 % ) 42 , 710 ( 12 . 15 % ) 5 ANALYSIS In this section , we answer the first two research questions concerning getting proxy indicator of the spread of hateful videos on YouTube and the platform’s tendency to recommend hateful videos . Confirmed by a normality test , our data is not normally distributed , and thus we use Chi - squared test [ 68 ] for our comparisons . 5 . 1 The Extent of Religiously Intolerant Videos ( RQ1 ) To assess the prevalence of religiously intolerant Arabic videos , we applied our deep learning classifier to over 350K unique recommended videos collected through five levels of recommen - dations . Table 5 illustrates the proportions of hateful and non - hateful videos in each level of recommendations . These proportions represent unique videos ( no duplicates ) within each level of recommendations . For “all levels” entry , we also dropped duplicated videos collected through multiple levels of recommendations . We found that level 1 recommendations contained the highest proportions of hateful videos ( about 21 % ) . Hateful videos tended to decrease as we moved deeper into the recommendation graph , reaching about 12 % in level 5 recommendations . We viewed ran - dom samples from each recommendation level to find a possible explanation for this diminishing trend in hateful videos . We observed that the further we moved in the recommendation graph , the more videos there were that were unrelated to religious discussions . Overall , 12 % of the 351 , 262 unique recommended videos across all levels were hateful . Insight : YouTube recommendations collected by our study contain a significant proportion of religiously intolerant content , particularly prevalent in level 1 recommendations . 5 . 2 YouTube’s Tendency to Recommend Hateful Videos ( RQ2 ) We followed the methodology described in [ 65 ] to measure the tendency of YouTube to recommend hateful videos . We leveraged 929 , 596 recommendations collected through traversing five levels of Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 15 Table 6 . The distribution of transitions between hateful ( H ) and non - hateful ( NH ) videos for all recommenda - tion levels . Transition type Number of transitions ( % ) NH → NH 695 , 381 ( 74 . 8 % ) NH → H 94 , 567 ( 10 . 17 % ) H → NH 96 , 858 ( 10 . 42 % ) H → H 42 , 790 ( 4 . 61 % ) recommendations to create a directed graph that resembled YouTube recommendations in which nodes represent videos and edges represent a recommendation activity . We computed the out - degree in terms of hateful ( H ) and non - hateful ( NH ) videos for each node . Then , we counted the number of transitions the graph made between all different combinations of hate classes : NH to NH , NH to H , H to NH , and H to H . As illustrated in Table 6 , we found that recommendations pointing to hateful videos ( NH to H and H to H ) comprised about 15 % of overall recommendations . In particular , we note that 12 % of recommendations to non - hateful videos are hateful . Additionally , about 31 % of videos recommended to hateful videos are also hateful . Note that the number of unique recommended videos is 351 , 262 , while the total number of transitions is 929 , 596 which includes transitions to duplicate videos ( i . e . , same videos that YouTube recommended in multiple instances ) . Insight : Our recommendation graph analysis offers preliminary evidence on the possible radicalization role played by YouTube recommendations which could expose and / or reinforce exposure of users to hateful videos and possibly isolate them in filter bubbles of extreme content . 6 PERSONALIZED AUDITING OF YOUTUBE’S ALGORITHM ( RQ3 ) In this section , we carry out an audit assessment to measure the effect of personalization based on religious ideology , Islamic denomination , and gender on the extent of exposure to hateful content . 6 . 1 Methods Creating User Profile . To understand the impact of personalization on YouTube algorithms , our first task was to create several user profiles that differ from one another in some specific personal attributes . For this study , we considered binary values of the following personal attributes : 1 ) religious ideology ( radical vs . moderate ) ; 2 ) Islamic denomination ( Sunni vs . Shia ) ; 3 ) gender ( female vs . male ) . We carefully crafted eight Google accounts , each with a distinctive set of personal attributes : ( 1 ) Radical , Sunni , Male ; ( 2 ) Radical , Sunni , Female ; ( 3 ) Radical , Shia , Male ; ( 4 ) Radical , Shia , Female ; ( 5 ) Moderate , Sunni , Male ; ( 6 ) Moderate , Sunni , Female ; ( 7 ) Moderate , Shia , Male ; ( 8 ) Moderate , Shia , Female . We set the age for all eight accounts to 26 as this was the reported average age of individuals who have been charged with ISIS - related activities in the United States [ 87 ] . To establish the Islamic denomination and religious ideology attributes , we first needed to identify Islamic clerics from both denominations known to promote either radical or moderate ideology . To do this , we interviewed five members of the Shia ( 3 males and 2 females ) and six members of the Sunni ( 2 males and 4 females ) . Given the highly sensitive topic of the interview , those members were recruited from the authors personal network . We asked them to provide names of Islamic clerics from their denomination whom they believe promote coexistence , and acceptance values ( i . e . , moderate clerics ) and those promoting jihad and other extreme ideologies ( i . e . , radical clerics ) . We only considered clerics who got mentioned twice or more for the same type of religious ideology . Note that these interviews were conducted to only get an initial list of clerics . Afterward , we verified that these clerics indeed support radical or moderate ideology by Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 16 Nuha Albadi , Maram Kurdi , and Shivakant Mishra watching / reading some of their videos / books while paying attention to how they regard members of other religious groups , i . e . , whether they were demonizing or tolerating other religions . Radical Sunni clerics in our list , in particular , publicly supported militant jihad and the death penalty for apostates . Overall , we identified ten radical and nine moderate Sunni clerics and two radical and five moderate Shia clerics . Interestingly , all five Shia interviewees agreed on the same two radical clerics . Due to the sensitivity of the topic , we will not be sharing the names of mentioned clerics . Building Watch History . After identifying Sunni and Shia clerics who were at either end of the Islamic spectrum , we started building the watch history for each of the eight profiles . For example , the watch history for a radical Sunni profile was created by watching videos of various Radical Sunni clerics . The watching took place four days a week , for a total of nine weeks . To control for temporal effects , we randomly selected the four days of the week , which included both weekdays and weekends . The time of the day the watching took place was also chosen randomly and had mornings , afternoons , and evenings . The arrangement of the profiles ( based on religious ideology and Islamic denomination ) over the days of the week was also randomized . Finally , we randomly distributed corresponding cleric names over the weeks . The watching for the male ( by the 1st author ) and female ( by the 2nd author ) profiles of the same Islamic denomination and religious ideology happened simultaneously , and each watching session lasted for 30 minutes . To reduce the possibility of confounding factors biasing our audit assessment , all watching was conducted at the same geographic location through devices with the same specifications and connected to the same WiFi network . To eliminate noise from tracked cookies and browsing history , we browsed YouTube in private mode using Firefox . At the start of every watching session , we logged - in to the assigned profile and then searched for the same assigned Islamic cleric . There were some variations in search results between the male and female profiles , even with a clean watch history , and thus watching the same video at the same time by the two profiles was not possible . Note that our goal is to audit gender as a profile setting rather than having the profile behave in a gender - specific way . Thus , to minimize personal differences when selecting which videos to watch , both authors selected videos that were most relevant to the search query and with a high number of views . In total , we created 4 . 5 hours of watch history for each profile . 6 . 2 Search and Recommendation Audits We conducted two systematic audit experiments to investigate whether personalization contributes to greater levels of exposure to hateful content in both search results ( search audit ) and recommen - dations ( recommendation audit ) . Search Audit . The primary purpose of the search audit is to investigate the effect of person - alization on the proportion of hateful videos returned in search results . The search keywords of interest are the Arabic equivalent of the words : Shia , Sunni , Jews , Christians , and atheists . For each profile , we logged in to YouTube using that profile’s credentials . We then performed search queries using the aforementioned keywords and considered the top 10 search results for each keyword . To minimize accidental bias created by the order of profiles or searched keywords , the following measures were taken : 1 ) the user profile and keyword selection were randomized ; 2 ) we kept at least 11 minutes of interval between consecutive search to minimize the carry - over effect [ 32 ] ; 3 ) experiments were conducted in private mode using Firefox . We semi - automated the process of collecting video ids for each search result . Google was actively detecting logins using automated software , and thus fully automating the process ( e . g . , using Selenium bots ) was not possible . Thus , we wrote a script that handled the randomization of profiles and keywords , processed HTML pages to identify the top 10 video ids , and enforced wait intervals between consecutive search queries . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 17 Table 7 . Personal attributes with significant effects on recommended videos ( FDR - survived results only ) and their effect size reported using Relative Risk ( RR ) and 95 % confidence interval ( 95 % CI ) . Variable NH H Statistical test RR 95 % CI Religious ideology Moderate Shia 737 263 𝜒 2 ( 1 , 2000 ) = 13 . 66 , 𝑝 < 0 . 01 1 . 29 1 . 13 - 1 . 48 Radical Shia 661 339 Islamic denomination Moderate Shia 737 263 𝜒 2 ( 1 , 2000 ) = 7 . 44 , 𝑝 < 0 . 05 1 . 21 1 . 1 - 1 . 39 Moderate Sunni 682 318 GenderSunni female 715 285 𝜒 2 ( 1 , 2000 ) = 5 . 40 , 𝑝 < 0 . 05 1 . 16 1 . 02 - 1 . 33 Sunni male 669 331 All recommendationsNon - personalized 6 , 391 1 , 678 𝜒 2 ( 1 , 12069 ) = 136 , 𝑝 < 0 . 001 1 . 46 1 . 37 - 1 . 56 Personalized 2 , 782 1 , 218 𝑝 - values are FDR - adjusted . We manually handled the login process , typing of the keywords , and downloading of search result HTML pages . Overall , 400 video ids were collected during the search audit . Recommendation Audit . The recommendation audit aims to explore if there is a significant difference in the volume of hateful videos recommended to each profile . After completing the search audit , we logged in to each profile in private mode and collected the top 10 recommendations for all videos collected in its search audit . As with search audit , the logins , opening of video pages , and downloading of HTML pages were handled manually . The script randomly selected a profile , randomly selected a keyword , provided us with video ids collected for that keyword , and processed downloaded HTML pages to identify the top 10 recommendations for each video . For each profile , a total of 500 recommendations were collected , resulting in a total of 4 , 000 video ids collected during the recommendation audit . After completing both audits , we used YouTube API to retrieve metadata for the collected videos . 6 . 3 Results Here we conduct statistical analysis to assess the extent of biases exacerbated by the YouTube’s algorithm based on personal attributes . We use Chi Square 𝜒 2 test to test statistical significance given that all observations are independent and all expected counts are larger than 10 . As a measure of effect size , we use Relative Risk ( RR ) and 95 % confidence intervals . To adjust for multiple testing , we use Benjamini - Hochberg procedure [ 14 ] to control false discovery rate ( FDR ) at level 0 . 05 . Religious Ideology Effect . To investigate whether being at either end of the Islamic spectrum affects the returned volume of hateful videos , we first compared the two high - level groups ( the four profiles with radical attribute vs . the four profiles with moderate attribute ) . While the result was not significant for the search audit , it was close to being statistically significant for the recommendation audit at FDR - adjusted 𝑝 = 0 . 083 . We then compared subgroups for religious ideology effect . We found that radical Shia profiles were recommended significantly more hateful videos ( 30 % more ) than moderate Shia profiles , 𝜒 2 ( 1 , 2000 ) = 13 . 66 , FDR - adjusted 𝑝 < 0 . 01 ( refer to Table 7 ) . We also found that radical Shia profiles watching Sunni - related videos were recommended almost twice as much hateful videos compared to their moderate counterparts ( 𝜒 2 ( 1 , 400 ) = 7 . 68 , FDR - adjusted 𝑝 < 0 . 05 ) . As for Sunni profiles , we have not observed a religious ideology effect for them at any level . This Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 18 Nuha Albadi , Maram Kurdi , and Shivakant Mishra suggests a clear distinction between radical and moderate Shia , while the boundaries seem to be not as clear between radical and moderate Sunni . Insight : There is a religious ideology effect on recommendations to Shia profiles , in which radical Shia profiles have 1 . 29 times the risk of getting recommended hateful videos compared to moderate Shia profiles . Islamic Denomination Effect . We explored whether being a member of an Islamic denomi - nation ( Shia vs . Sunni ) affects the amount of hateful content presented to the users . Comparing all Shia vs . all Sunni profiles didn’t yield any significant difference in their exposure to hateful content . However , when comparing subgroups for Islamic denomination effect , we found that moderate Sunni profiles were recommended significantly more hateful videos ( 21 % more ) than moderate Shia profiles , 𝜒 2 ( 1 , 2000 ) = 7 . 44 , FDR - adjusted 𝑝 < 0 . 05 . The only other comparison that yielded marginally significant Islamic denomination effect ( FDR - adjusted 𝑝 = 0 . 09 ) is between radical Shia profiles and radical Sunni profiles , in which radical Shia profiles were recommended more hateful videos . Insight : There is an Islamic denomination effect on recommendations to moderate profiles , in which moderate Sunni profiles are 21 % more likely to be recommended hateful videos than moderate Shia profiles . Gender Effect . To study the effect of gender on returned search results and recommendations , we first compared all four male vs . all four female profiles . Results revealed no significant difference for both search and recommendation results . However , when comparing subgroups , the gender effect was only observed in videos recommended to Sunni profiles , 𝜒 2 ( 1 , 2000 ) = 5 . 40 , FDR - adjusted 𝑝 < 0 . 05 . Insight : There is a gender effect on Sunni profiles , in which male Sunni profiles have 16 % increase in the risk of getting recommended hateful videos compared to female Sunni profiles . Personalization Effect . To examine the effect of personalization , we compared results returned in the search and recommendation audits , which are affected by personalization , against the search and first - level recommendations returned using YouTube API ( Section 3 . 3 and 5 . 1 ) , which doesn’t account for personalization . The percentage of hateful videos increased from 20 . 80 % ( level 1 recommendation ) to 30 . 45 % ( recommendation audit ) . This difference in distribution between personalized and non - personalized recommendations was found to be statistically significant , 𝜒 2 ( 1 , 12069 ) = 136 , FDR - adjusted 𝑝 < 0 . 001 . On the other hand , we didn’t observe a significant difference between personalized and non - personalized search results . Insight : Personalization in general increases the risk of getting recommended hateful videos by 46 % . Across Keyword Differences . We investigated whether the amount of hateful content differed across the five keywords . For recommendation audit videos ( Figure 5a ) , we found that the amount of hateful content differed significantly across keywords , 𝜒 2 ( 4 , 4000 ) = 209 , 𝑝 < 0 . 001 . To find exactly which keyword significantly differed from the other , we followed up with post - hoc pairwise comparisons and used the Benjamini - Hochberg procedure to control FDR at level 0 . 05 . We found that all keywords differed significantly from each other except for the pairs ( atheists and Shia ) and ( Christians and Sunni ) . As is evident from the graph , watching videos related to Shia resulted in having the highest proportion of hateful video recommendations ( 43 . 4 % ) , followed closely by atheists ( 41 . 8 % ) . Videos related to Christians and Sunni resulted in a similar proportion ( 26 % ) of hateful video recommendations . Jews - related videos yielded the lowest amount of hateful recommended videos ( 15 . 9 % ) . For search audit videos ( Figure 5b ) , we also found a significant difference in the distribution of hateful and non - hateful videos across keywords , 𝜒 2 ( 4 , 400 ) = 97 , 𝑝 < 0 . 001 . Post - hoc tests with BH - FDR adjustment revealed that the keyword ‘atheists’ returned a significantly higher proportion of hateful videos ( 76 % ) than any other keyword . About 56 % of ‘Shia’ keyword search results were hateful , which was significantly different from all other keyword search results . The proportion of hateful videos didn’t significantly differ across the keywords Jews ( 25 % ) , Sunni ( 20 % ) , and Christians Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 19 ( a ) Recommendation audit ( b ) Search audit Fig . 5 . Distribution of hateful and non - hateful videos by keyword in ( a ) recommendation audit and ( b ) search audit . ( 14 % ) . Insight : Searching YouTube for Shia and atheists videos result in high volume of hateful videos appearing in YouTube’s search results and recommendations . 7 DISCUSSION Our analysis provides proxy indicators of the prevalence of Arabic religiously intolerant videos on YouTube . In our non - personalized analysis , we found that 30 % of videos returned in search results were hateful . This is particularly alarming given that the data collection process was based on entirely innocuous terms . However , we note that the percentage of hateful content found within Arabic religious discussions on Twitter using a similar methodology was considerably larger ( 42 % ) [ 5 ] . This could be attributed to the fact that creating content on Twitter is much easier than creating content on YouTube . It could also reflect differences in the degree of moderation and hateful content policing the two platforms are conducting . When comparing targets of hate on YouTube versus Twitter , we found that the main difference lies in the extent of hate targeting the Jewish community ; while the Jews were the most targeted religious community on Twitter [ 5 ] , they were among the least targeted religious groups on YouTube . On the other hand , Shia and Atheists remain among the top of the most targeted religious groups on both platforms . Our recommendation graph analysis based on nearly 1M captured recommendation transitions between videos suggests that YouTube recommendation algorithm can expose users arriving at non - hateful videos to hateful ones . We also found that 31 % of videos recommended to hateful videos were also hateful , which could reinforce users exposure to radical content . However , these findings don’t necessarily support the claim that YouTube recommendation definitely have a role in radicalization as radicalization can occur offline or in other social spaces . A recent research [ 34 ] found that users who consumed far - right radical videos arrived at such videos more frequently from YouTube search results or an external website rather than following YouTube recommendation chains . Additionally , our study takes into account all top four recommended videos , while in a realistic setting users make a subconscious decision on which video to watch next based on multiple factors ( e . g . , thumbnail , video title , appearing in Up Next , etc . ) . Nevertheless , it is fair to conclude that YouTube may have a role in online radicalization by exposing users to radical content through surfacing it in its search results and recommendations . Disturbingly , our personalized audits suggest that personalization in general results in surfacing more hateful content and that religious identity and gender could contribute to greater levels of exposure to hateful content . Particularly relevant to our audit experiments is an audit study by Hussein et al . [ 35 ] , in which they investigated the effect of watch history and demographics ( i . e . , age , gender , and geographical location ) on the amount of misinformative content returned in YouTube search results and recommendations . They found that in most cases , men were recommended Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 20 Nuha Albadi , Maram Kurdi , and Shivakant Mishra significantly more misinformative videos than women . In line with their findings , we found that male Sunni profiles were recommended significantly more hateful videos than female Sunni profiles . This is concerning , especially in light of the statistics that show males being more prone to radicalization than females [ 51 ] . In previous audit studies [ 35 , 66 ] , the process of developing a watch history for profiles was conducted automatically using Selenium bots ( i . e . , automated web browsing bots ) , in which each profile was controlled by a bot that watched sequentially in one session a predetermined set of videos . However , in our personalized audit experiments , the process of building the watch history was carried out manually in a controlled environment by the first two authors over nine weeks . We believe that our methodology leads to building a more realistic , human - like watch history than the one using bots . On the other hand , when assigning annotations to videos that the profiles get exposed to , we argue that doing so automatically using machine - learning classifiers rather than relying solely on human annotators would facilitate auditing recommendation algorithms at a larger scale . In 2017 , YouTube along with other major social media platforms formed the Global Internet Forum to Counter Terrorism in an effort to make their online services free from hateful and extreme content [ 86 ] . To assess YouTube’s current countermeasures in removing Arabic hateful content , we checked the current status of all videos analyzed in our study after two years from data collection . We found that only 16 . 44 % of the hateful videos were removed from the platform in these two years . In addition , 13 . 14 % of non - hateful videos were also removed . While this difference in distribution was found to be significant ( 𝜒 2 ( 1 , 351262 ) = 348 , 𝑝 < 0 . 001 ) , it is important to note that 83 . 56 % of hateful videos were still available on the platform as of September 2021 . Thus , our study signals a need for a more active and effective technique to guard against Arabic hateful content on YouTube . To our knowledge , our study represents a first effort in characterizing , identifying and understanding the spread of Arabic hateful content on YouTube , and we hope that it would serve as a starting point for other researchers to invest more efforts in ultimately making social networks’ Arabic content safer and free from radical content . Although we showed in Section 4 . 4 that our classifier delivered a comparable performance to other YouTube models developed to tackle other related issues , we acknowledge that our classifier performance is still lower than desired . This reflects the inherent difficulty of capturing hate speech as it can be highly contextual . To boost performance it is essential , though expensive , to acquire more accurately labeled data . It is also worth experimenting with other text embedding techniques such as contextualized token embeddings using Bidirectional Encoder Representations from Transformers ( BERT ) . Another limitation is that our dataset for the non - personalized analysis may not be representative of the entire Arabic religious videos on YouTube given that the volume of such content is unknown . Finally , our personalized audit study modeled a limited number of users focusing on those at opposing ends of the religious spectrum . Additionally , in a real setting , a person may be watching other types of videos , not only religious ones , which could impact the recommendation behavior of the system . Thus , auditing YouTube’s recommendations by considering a more comprehensive range of users and online behaviors would be a clear direction for future research . 8 CONCLUSION In this study , we explored the spread of Arabic YouTube videos targeting religious minorities , how often YouTube’s recommendations suggest such videos in a general sense , and the extent of biases exacerbated by YouTube’s recommendations based on personal attributes . We found that hateful videos are particularly prevalent in search results and first - level recommendations , mainly targeting Shia an atheists . Recommendations to hateful videos comprised about 15 % of overall Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 21 recommendations . Our personalized audit experiments revealed that gender , religious ideology , and Islamic denomination can contribute to greater exposure to hateful content . We showed that YouTube’s current countermeasures are ineffective in combating Arabic radical content , with about 84 % of identified hateful videos going undetected as of September 2021 . YouTube has invested some efforts into tackling harmful content in the English language . For example , YouTube now recommends debunking videos to those watching videos promoting anti - vaccination myths [ 35 ] . A similar effort should also be invested in tackling Arabic radical content , which not only fuels civil wars in the Arab region , but also contributes to terrorism worldwide . Viewers of Arabic content can also have a role in bringing down extreme content by actively reporting it to YouTube and providing counter - narratives in the comment section whenever possible . The findings and resources presented in this paper facilitate enforcing YouTube’s hateful content policy , motivate future research in the area , and raise awareness among the public and concerned agencies . REFERENCES [ 1 ] Martín Abadi , Paul Barham , Jianmin Chen , Zhifeng Chen , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Geoffrey Irving , Michael Isard , et al . 2016 . Tensorflow : A system for large - scale machine learning . In 12th { USENIX } Symposium on Operating Systems Design and Implementation ( { OSDI } 16 ) . 265 – 283 . [ 2 ] Swati Agarwal and Ashish Sureka . 2014 . A focused crawler for mining hate and extremism promoting videos on YouTube . . In Proceedings of the 25th ACM conference on Hypertext and social media . 294 – 296 . [ 3 ] Nisha Aggarwal , Swati Agrawal , and Ashish Sureka . 2014 . Mining YouTube metadata for detecting privacy invading harassment and misdemeanor videos . In 2014 Twelfth Annual International Conference on Privacy , Security and Trust . IEEE , 84 – 93 . [ 4 ] Areej Al - Hassan and Hmood Al - Dossari . 2021 . Detection of hate speech in Arabic tweets using deep learning . Multimedia Systems ( 2021 ) , 1 – 12 . [ 5 ] Nuha Albadi , Maram Kurdi , and Shivakant Mishra . 2018 . Are they our brothers ? analysis and detection of religious hate speech in the arabic twittersphere . In 2018 IEEE / ACM International Conference on Advances in Social Networks Analysis and Mining ( ASONAM ) . IEEE , 69 – 76 . [ 6 ] Nuha Albadi , Maram Kurdi , and Shivakant Mishra . 2019 . Hateful people or hateful bots ? Detection and characterization of bots spreading religious hatred in Arabic social media . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 25 . [ 7 ] Nuha Albadi , Maram Kurdi , and Shivakant Mishra . 2019 . Investigating the effect of combining GRU neural networks with handcrafted features for religious hatred detection on Arabic Twitter space . Social Network Analysis and Mining 9 , 1 ( 05 Aug 2019 ) , 41 . https : / / doi . org / 10 . 1007 / s13278 - 019 - 0587 - 5 [ 8 ] Raghad Alshalan and Hend Al - Khalifa . 2020 . A deep learning approach for automatic hate speech detection in the saudi twittersphere . Applied Sciences 10 , 23 ( 2020 ) , 8614 . [ 9 ] Appen . [ n . d . ] . How to Calculate a Confidence Score . Retrieved December 17 , 2021 from https : / / success . appen . com / hc / en - us / articles / 201855939 - How - to - Calculate - a - Confidence - Score [ 10 ] Appen . 2021 . AI Solutions with confident Training Data . Retrieved September 02 , 2021 from https : / / appen . com [ 11 ] Noman Ashraf , Abid Rafiq , Sabur Butt , Hafiz Muhammad Faisal Shehzad , Grigori Sidorov , and Alexander Gelbukh . [ n . d . ] . YouTube based religious hate speech and extremism detection dataset with machine learning baselines . Journal of Intelligent & Fuzzy Systems Preprint ( [ n . d . ] ) , 1 – 9 . [ 12 ] Pinkesh Badjatiya , Shashank Gupta , Manish Gupta , and Vasudeva Varma . 2017 . Deep learning for hate speech detection in tweets . In Proceedings of the 26th International Conference on World Wide Web Companion . International World Wide Web Conferences Steering Committee , 759 – 760 . https : / / doi . org / 10 . 1145 / 3041021 . 3054223 [ 13 ] Eytan Bakshy , Solomon Messing , and Lada A Adamic . 2015 . Exposure to ideologically diverse news and opinion on Facebook . Science 348 , 6239 ( 2015 ) , 1130 – 1132 . [ 14 ] Yoav Benjamini and Yosef Hochberg . 1995 . Controlling the false discovery rate : a practical and powerful approach to multiple testing . Journal of the Royal statistical society : series B ( Methodological ) 57 , 1 ( 1995 ) , 289 – 300 . [ 15 ] Stevo Bozinovski . 2020 . Reminder of the first paper on transfer learning in neural networks , 1976 . Informatica 44 , 3 ( 2020 ) . [ 16 ] Pete Burnap and Matthew L Williams . 2016 . Us and them : identifying cyber hate on Twitter across multiple protected characteristics . EPJ Data science 5 ( 2016 ) , 1 – 15 . [ 17 ] Thomas G Cardinali . 2013 . The Sunni - Shia Political Struggle between Iran and Saudi Arabia . Strategic Informer : Student Publication of the Strategic Intelligence Society 1 , 2 ( 2013 ) , 6 . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 22 Nuha Albadi , Maram Kurdi , and Shivakant Mishra [ 18 ] Eshwar Chandrasekharan , Umashanthi Pavalanathan , Anirudh Srinivasan , Adam Glynn , Jacob Eisenstein , and Eric Gilbert . 2017 . You can’t stay here : The efficacy of reddit’s 2015 ban examined through hate speech . Proceedings of the ACM on Human - Computer Interaction 1 , CSCW ( 2017 ) , 1 – 22 . [ 19 ] Francois Chollet et al . 2015 . Keras . Retrieved September 16 , 2021 from https : / / github . com / fchollet / keras [ 20 ] Arijit Ghosh Chowdhury , Aniket Didolkar , Ramit Sawhney , and Rajiv Shah . 2019 . ARHNet - leveraging community interaction for detection of religious hate speech in Arabic . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics : Student Research Workshop . 273 – 280 . [ 21 ] Maura Conway and Lisa McInerney . 2008 . Jihadi video and auto - radicalisation : Evidence from an exploratory YouTube study . In European Conference on Intelligence and Security Informatics . Springer , 108 – 118 . [ 22 ] Paul Covington , Jay Adams , and Emre Sargin . 2016 . Deep neural networks for youtube recommendations . In Proceedings of the 10th ACM conference on recommender systems . 191 – 198 . [ 23 ] Nemanja Djuric , Jing Zhou , Robin Morris , Mihajlo Grbovic , Vladan Radosavljevic , and Narayan Bhamidipati . 2015 . Hate speech detection with comment embeddings . In Proceedings of the 24th international conference on world wide web . ACM , 29 – 30 . https : / / doi . org / 10 . 1145 / 2740908 . 2742760 [ 24 ] Robert Evans . 2013 . Atheists face death in 13 countries , global discrimination : study . REUTERS ( 2013 ) . Retrieved August 26 , 2021 from https : / / www . reuters . com / article / us - religion - atheists / atheists - face - death - in - 13 - countries - global - discrimination - study - idUSBRE9B900G20131210 [ 25 ] Ibrahim Abu Farha and Walid Magdy . 2020 . Multitask learning for Arabic offensive language and hate - speech detection . In Proceedings of the 4th workshop on open - source Arabic corpora and processing tools , with a shared task on offensive language detection . 86 – 90 . [ 26 ] Hossam Faris , Ibrahim Aljarah , Maria Habib , and Pedro A Castillo . 2020 . Hate Speech Detection using Word Embedding and Deep Learning in the Arabic Language Context . . In ICPRAM . 453 – 460 . [ 27 ] Thomas Grant Fraser . 2015 . The Arab - Israeli Conflict . Macmillan International Higher Education . [ 28 ] Simona Frenda , Bilal Ghanem , Manuel Montes - y Gómez , and Paolo Rosso . 2019 . Online hate speech against women : Automatic identification of misogyny and sexism on twitter . Journal of Intelligent & Fuzzy Systems 36 , 5 ( 2019 ) , 4743 – 4752 . [ 29 ] Nathan Gonzalez . 2013 . The Sunni - Shia conflict : understanding sectarian violence in the Middle East . Nortia Media Ltd . [ 30 ] Hatem Haddad , Hala Mulki , and Asma Oueslati . 2019 . T - hsab : A tunisian hate speech and abusive dataset . In International Conference on Arabic Language Processing . Springer , 251 – 263 . [ 31 ] Mario Haim , Andreas Graefe , and Hans - Bernd Brosius . 2018 . Burst of the filter bubble ? Effects of personalization on the diversity of Google News . Digital journalism 6 , 3 ( 2018 ) , 330 – 343 . [ 32 ] Aniko Hannak , Piotr Sapiezynski , Arash Molavi Kakhki , Balachander Krishnamurthy , David Lazer , Alan Mislove , and Christo Wilson . 2013 . Measuring personalization of web search . In Proceedings of the 22nd international conference on World Wide Web . 527 – 538 . [ 33 ] Kotaro Hara , Abigail Adams , Kristy Milland , Saiph Savage , Chris Callison - Burch , and Jeffrey P Bigham . 2018 . A data - driven analysis of workers’ earnings on amazon mechanical turk . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM Press , New York , NY , USA , 449 . https : / / doi . org / 10 . 1145 / 3173574 . 3174023 [ 34 ] Homa Hosseinmardi , Amir Ghasemian , Aaron Clauset , Markus Mobius , David M Rothschild , and Duncan J Watts . 2021 . Examining the consumption of radical content on YouTube . Proceedings of the National Academy of Sciences 118 , 32 ( 2021 ) . [ 35 ] Eslam Hussein , Prerna Juneja , and Tanushree Mitra . 2020 . Measuring misinformation in video search platforms : An audit study on YouTube . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( 2020 ) , 1 – 27 . [ 36 ] Lisa Kaati , Enghin Omer , Nico Prucha , and Amendra Shrestha . 2015 . Detecting multipliers of jihadism on twitter . In 2015 IEEE international conference on data mining workshop ( ICDMW ) . IEEE , 954 – 960 . [ 37 ] RishabhKaushal , SrishtySaha , PayalBajaj , and PonnurangamKumaraguru . 2016 . KidsTube : Detection , characterization and analysis of child unsafe content & promoters on YouTube . In 2016 14th Annual Conference on Privacy , Security and Trust ( PST ) . IEEE , 157 – 164 . [ 38 ] Jytte Klausen , Eliane Tschaen Barbieri , Aaron Reichlin - Melnick , and Aaron Y Zelin . 2012 . The YouTube Jihadists : A social network analysis of Al - Muhajiroun’s propaganda campaign . Perspectives on Terrorism 6 , 1 ( 2012 ) , 36 – 53 . [ 39 ] Maram Kurdi , Nuha Albadi , and Shivakant Mishra . 2021 . “Think before you upload " : an in - depth analysis of unavailable videos on YouTube . Social Network Analysis and Mining 11 , 1 ( 2021 ) , 1 – 21 . [ 40 ] Ugur Kursuncu , Manas Gaur , Carlos Castillo , Amanuel Alambo , Krishnaprasad Thirunarayan , Valerie Shalin , Dilshod Achilov , I Budak Arpinar , and Amit Sheth . 2019 . Modeling islamist extremist communications on social media using contextual dimensions : religion , ideology , and hate . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 22 . [ 41 ] Irene Kwok and Yuzhou Wang . 2013 . Locate the Hate : Detecting Tweets against Blacks . . In AAAI . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 23 [ 42 ] Vittorio Lingiardi , Nicola Carone , Giovanni Semeraro , Cataldo Musto , Marilisa D’Amico , and Silvia Brena . 2020 . Mapping Twitter hate speech towards social and sexual minorities : a lexicon - based approach to semantic content analysis . Behaviour & Information Technology 39 , 7 ( 2020 ) , 711 – 721 . [ 43 ] Walid Magdy , Kareem Darwish , Norah Abokhodair , Afshin Rahimi , and Timothy Baldwin . 2016 . # isisisnotislam or # deportallmuslims ? : Predicting unspoken views . In Proceedings of the 8th ACM Conference on Web Science . ACM , 95 – 106 . [ 44 ] Walid Magdy , Kareem Darwish , and Ingmar Weber . 2016 . # FailedRevolutions : Using Twitter to study the antecedents of ISIS support . First Monday 21 , 2 ( 2016 ) . [ 45 ] Enrico Mariconti , Guillermo Suarez - Tangil , Jeremy Blackburn , Emiliano De Cristofaro , Nicolas Kourtellis , Ilias Leon - tiadis , Jordi Luque Serrano , and Gianluca Stringhini . 2019 . " You Know What to Do " Proactive Detection of YouTube Videos Targeted by Coordinated Hate Attacks . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 21 . [ 46 ] Binny Mathew , Ritam Dutt , Pawan Goyal , and Animesh Mukherjee . 2019 . Spread of hate speech in online social media . In Proceedings of the 10th ACM conference on web science . 173 – 182 . [ 47 ] Binny Mathew , Anurag Illendula , Punyajoy Saha , Soumya Sarkar , Pawan Goyal , and Animesh Mukherjee . 2020 . Hate begets hate : A temporal study of hate speech . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW2 ( 2020 ) , 1 – 24 . [ 48 ] Binny Mathew , Punyajoy Saha , Hardik Tharad , Subham Rajgaria , Prajwal Singhania , Suman Kalyan Maity , Pawan Goyal , and Animesh Mukherjee . 2019 . Thou shalt not hate : Countering online hate speech . In Proceedings of the international AAAI conference on web and social media , Vol . 13 . 369 – 380 . [ 49 ] Dimitar Mihaylov . 2017 . Why the Arab Spring Turned into Arab Winter : Understanding the Middle East Crises through Culture , Religion , and Literature . Israel Journal of Foreign Affairs 11 , 1 ( 2017 ) , 3 – 14 . [ 50 ] Brent Mittelstadt . 2016 . Automation , algorithms , and politics | Auditing for transparency in content personalization systems . International Journal of Communication 10 ( 2016 ) , 12 . [ 51 ] Anne Maria Möller - Leimkühler . 2018 . Why is terrorism a man’s business ? CNS spectrums 23 , 2 ( 2018 ) , 119 – 128 . [ 52 ] Mainack Mondal , Leandro Araújo Silva , and Fabrício Benevenuto . 2017 . A measurement study of hate speech in social media . In Proceedings of the 28th ACM conference on hypertext and social media . 85 – 94 . [ 53 ] Djedjiga Mouheb , Rutana Ismail , Shaheen Al Qaraghuli , Zaher Al Aghbari , and Ibrahim Kamel . 2018 . Detection of offensive messages in Arabic social media communications . In 2018 International Conference on Innovations in Information Technology ( IIT ) . IEEE , 24 – 29 . [ 54 ] Nabil Mouline . 2014 . The Clerics of Islam . Yale University Press . [ 55 ] Hamdy Mubarak and Kareem Darwish . 2016 . Demographic surveys of arab annotators on crowdflower . In Proceedings of ACM WebSci16 Workshop “Weaving Relations of Trust in Crowd Work : Transparency and Reputation across Platforms . [ 56 ] Hala Mulki , Hatem Haddad , Chedi Bechikh Ali , and Halima Alshabani . 2019 . L - hsab : A levantine twitter dataset for hate speech and abusive language . In Proceedings of the third workshop on abusive language online . 111 – 118 . [ 57 ] Dhiraj Murthy . 2021 . Evaluating Platform Accountability : Terrorist Content on YouTube . American Behavioral Scientist 65 , 6 ( 2021 ) , 800 – 824 . [ 58 ] Jack Nicas . 2018 . How YouTube Drives People to the Internet’s Darkest Corners . The Wall Street Journal ( 2018 ) . Retrieved August 31 , 2021 from https : / / www . wsj . com / articles / how - youtube - drives - viewers - to - the - internets - darkest - corners - 1518020478 [ 59 ] Richard A Nielsen . 2017 . Deadly clerics : Blocked ambition and the paths to jihad . Cambridge University Press . [ 60 ] Brendan Nyhan . 2021 . YouTube still hosts extremist videos . Here’s who watches them . The Washington Post ( 2021 ) . Retrieved August 31 , 2021 from https : / / www . washingtonpost . com / outlook / 2021 / 03 / 10 / youtube - extremist - supremacy - radicalize - adl - study / [ 61 ] Adewale Obadimu , Esther Mead , Muhammad Nihal Hussain , and Nitin Agarwal . 2019 . Identifying toxicity within youtube video comment . In International Conference on Social Computing , Behavioral - Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation . Springer , 214 – 223 . [ 62 ] Derek O’Callaghan , Derek Greene , Maura Conway , Joe Carthy , and Pádraig Cunningham . 2015 . Down the ( white ) rabbit hole : The extreme right and online recommender systems . Social Science Computer Review 33 , 4 ( 2015 ) , 459 – 478 . [ 63 ] Ahmed Omar , Tarek M Mahmoud , and Tarek Abd - El - Hafeez . 2020 . Comparative performance of machine learning and deep learning algorithms for Arabic hate speech detection in osns . In The International Conference on Artificial Intelligence and Computer Vision . Springer , 247 – 257 . [ 64 ] Raphael Ottoni , Evandro Cunha , Gabriel Magno , Pedro Bernardina , Wagner Meira Jr , and Virgilio Almeida . 2018 . Analyzing right - wing youtube channels : Hate , violence and discrimination . In Proceedings of the 10th ACM Conference on Web Science . ACM , 323 – 332 . [ 65 ] Kostantinos Papadamou , Antonis Papasavva , Savvas Zannettou , Jeremy Blackburn , Nicolas Kourtellis , Ilias Leontiadis , Gianluca Stringhini , and Michael Sirivianos . 2020 . Disturbed YouTube for kids : Characterizing and detecting inappro - priate videos targeting young children . In Proceedings of the international AAAI conference on web and social media , Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . 505 : 24 Nuha Albadi , Maram Kurdi , and Shivakant Mishra Vol . 14 . 522 – 533 . [ 66 ] Kostantinos Papadamou , Savvas Zannettou , Jeremy Blackburn , Emiliano De Cristofaro , Gianluca Stringhini , and Michael Sirivianos . 2022 . " It is just a flu " : Assessing the Effect of Watch History on YouTube’s Pseudoscientific Video Recommendations . In Proceedings of the international AAAI conference on web and social media . [ 67 ] Eli Pariser . 2011 . The filter bubble : How the new personalized web is changing what we read and how we think . Penguin . [ 68 ] Karl Pearson . 1900 . X . On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling . The London , Edinburgh , and Dublin Philosophical Magazine and Journal of Science 50 , 302 ( July 1900 ) , 157 – 175 . https : / / doi . org / 10 . 1080 / 14786440009463897 [ 69 ] Pew Research Center , Washington , D . C . 2009 . Mapping the Global Muslim Population . Retrieved August 26 , 2021 from https : / / www . pewforum . org / 2009 / 10 / 07 / mapping - the - global - muslim - population / [ 70 ] Pew Research Center , Washington , D . C . 2015 . Religious Composition by Country , 2010 - 2050 . Retrieved August 26 , 2021 from http : / / www . pewforum . org / 2015 / 04 / 02 / religious - projection - table / 2010 / percent / Middle _ East - North _ Africa / [ 71 ] Pew Research Center , Washington , D . C . 2017 . Global Restrictions on Religion Rise Modestly in 2015 , Reversing Downward Trend - Appendix B : Social Hostilities Index . Retrieved Accessed 24 October 2018 from http : / / assets . pewresearch . org / wp - content / uploads / sites / 11 / 2017 / 04 / 07154135 / Appendix - B . pdf [ 72 ] Daniel Pipes . 2017 . In the path of God : Islam and political power . Routledge . [ 73 ] Marshall Cavendish Reference . 2010 . Islamic Beliefs , Practices , and Cultures . Marshall Cavendish . [ 74 ] Manoel Horta Ribeiro , Raphael Ottoni , Robert West , Virgílio A . F . Almeida , and Wagner Meira . 2020 . Auditing Radicalization Pathways on YouTube ( FAT * ’20 ) . Association for Computing Machinery , New York , NY , USA , 131 – 141 . https : / / doi . org / 10 . 1145 / 3351095 . 3372879 [ 75 ] Ronald E Robertson , Shan Jiang , Kenneth Joseph , Lisa Friedland , David Lazer , and Christo Wilson . 2018 . Auditing partisan audience bias within google search . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) , 1 – 22 . [ 76 ] Daniel Röchert , Muriel Weitzel , and Björn Ross . 2020 . The homogeneity of right - wing populist and radical content in YouTube recommendations . In International Conference on Social Media and Society . 245 – 254 . [ 77 ] Kevin Roose . 2019 . The Making of a YouTube Radical . The New York Times ( 2019 ) . Retrieved August 31 , 2021 from https : / / www . nytimes . com / interactive / 2019 / 06 / 08 / technology / youtube - radical . html [ 78 ] Mattia Samory , Indira Sen , Julian Kohne , Fabian Flöck , and Claudia Wagner . 2021 . Call me sexist , but . . . : Revisiting sexism detection using psychological scales and adversarial samples . In Intl AAAI Conf . Web and Social Media . 573 – 584 . [ 79 ] Mike Schuster and Kuldip K Paliwal . 1997 . Bidirectional recurrent neural networks . IEEE transactions on Signal Processing 45 , 11 ( 1997 ) , 2673 – 2681 . [ 80 ] Leandro Silva , Mainack Mondal , Denzil Correa , Fabrício Benevenuto , and Ingmar Weber . 2016 . Analyzing the targets of hate in online social media . In Tenth international AAAI conference on web and social media . [ 81 ] Abu Bakr Soliman , Kareem Eissa , and Samhaa R El - Beltagy . 2017 . Aravec : A set of arabic word embedding models for use in arabic nlp . Procedia Computer Science 117 ( 2017 ) , 256 – 265 . [ 82 ] Joan E . Solsman . 2018 . YouTube’s AI is the puppet master over most of what you watch . CNET ( 2018 ) . Retrieved August 31 , 2021 from https : / / www . cnet . com / news / youtube - ces - 2018 - neal - mohan / [ 83 ] Statista Research Department . 2021 . YouTube - Statistics and Facts . Retrieved September 02 , 2021 from https : / / www . statista . com / topics / 2019 / youtube / [ 84 ] Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jonathon Shlens , and Zbigniew Wojna . 2015 . Rethinking the Inception Architecture for Computer Vision . CoRR abs / 1512 . 00567 ( 2015 ) . arXiv : 1512 . 00567 http : / / arxiv . org / abs / 1512 . 00567 [ 85 ] Zeynep Tufekci . 2018 . YouTube , the Great Radicalizer . The New York Times ( 2018 ) . Retrieved August 31 , 2021 from https : / / www . nytimes . com / 2018 / 03 / 10 / opinion / sunday / youtube - politics - radical . html [ 86 ] Twitter Public Policy . 2017 . Global Internet Forum to Counter Terrorism . Retrieved January 08 , 2022 from https : / / blog . twitter . com / en _ us / topics / company / 2017 / Global - Internet - Forum - to - Counter - Terrorism [ 87 ] Lorenzo Vidino and Seamus Hughes . 2015 . ISIS in America : From Retweets to Raqqa . [ 88 ] Zeerak Waseem and Dirk Hovy . 2016 . Hateful symbols or hateful people ? predictive features for hate speech detection on twitter . In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies ( NAACL - HLT ’16 ) . Association for Computational Linguistics , 88 – 93 . https : / / doi . org / 10 . 18653 / v1 / n16 - 2013 [ 89 ] YouTube . [ n . d . ] . PRODUCT FEATURES : Recommended videos . Retrieved August 26 , 2021 from https : / / www . youtube . com / howyoutubeworks / product - features / recommendations / [ 90 ] Savvas Zannettou , Joel Finkelstein , Barry Bradlyn , and Jeremy Blackburn . 2020 . A quantitative approach to under - standing online antisemitism . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 14 . 786 – 797 . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 . Characterization , Detection , and Personalization of Religiously Intolerant Arabic Videos 505 : 25 [ 91 ] Ziqi Zhang , David Robinson , and Jonathan Tepper . 2018 . Detecting hate speech on twitter using a convolution - gru based deep neural network . In European semantic web conference . Springer , 745 – 760 . A TEST QUESTIONS USED TO ASSESS ANNOTATORS Content warning : some readers may find hateful videos identified here to be upsetting or disturbing . videoid hate targetedreligions videoid hate targetedreligions videoid hate targetedreligions yhckohsG9F8 no _ lCFs _ - 4MhA no df9z6SReHt8 yes Islam 8qtoC9l6Q44 no HNvY65qyCKY unrelated WSIWbFdMGZI yes Judaism , Christianity , Atheism QFGR - axebT8 no KhUcmXUuXQA no Uw4BazBdtZ4 no dVyNS7PzUYM yes Atheism Vklc - RbEXYk no bHCtdSy - Nbw no BzxUmfY2zXg no 3HbaA9kMkEk no mt8rYzx8u _ 0 no LbAeyeGeprg unrelated VFHd2R - gpQs no 1Ja6kmTaIk8 no x4jn1lEmIIw no gHN1elpukjc unrelated Q5eJe9KOokc no h _ a3zDMW _ P0 no mT38cbTw1Cg no NLHMxeBE9Gg no Q0uLPDR9MMs yes Judaism , Shia BfXjTFRWjDU no xpmIAR _ cIlA no hRMEY40HYzE unrelated 99vGew _ NWaM no JZwgqYntWtc unrelated tkefR - Nc40A yes Shia CuPStQ - j7bg no 9 _ ARBi7KuRI unrelated FVcdVREEXL4 yes Shia jHyOWcbx7C0 no fNPPeSAk06Q unrelated io1 - szAAgz8 unrelated Ejv30YVRo6c no hSMWbX _ sPmM unrelated fRSoYn3iEcQ yes Shia o8E0JB - IoQc no 0t5G4NWAtgA unrelated 2mqL - 7kaKKk no 5AkAGc5nOXw unrelated DyUb2ixHTFU unrelated DWo4wGuIq _ Y unrelated e - G7Tt3 - P1Q unrelated b0lh4VYzdL4 unrelated fpnmULVBnUU no QwMTpa3IcFc no 82B38Npy38s unrelated AxtJAK6f5Mk no gkMfpXR _ aro no myUAndPEqCA no d8LgcHyhyp4 unrelated o _ A4 - TI5jFY unrelated 7fDMSoJghhM yes Shia gMK12E37FxE yes Shia Iw0cNWeHoTE no ZQ _ iqO - dcpg no FUx _ IyVGUAE unrelated GCtt21GPaUc unrelated ISuvq _ MdeT4 unrelated xbpp7eDqkSk yes Judaism spiiOmbDDmA no yzkLgpe5BQw no tzXx62NEhbY no GGrmb6UTleo unrelated S93XXoXgwog no hO7Wer85jyg no r _ bdGmJopkA unrelated gLY3fw0fsSo yes Shia XTLla _ Mc6EU yes Atheism EBALA4xESSM unrelated LVjD28UAjdE no JtAMz7bmXXg yes Atheism MF86I _ Cj71c no DlQojAGZu _ 8 no EjhIwOBcd88 yes Atheism Bnxoox6HB5w yes Judaism , Christianity HM1V _ 9U6h6M no Qm3ghn7tgjA yes Shia zJ7dFGAHvSc yes Christianity ZyLvudOqDG0 yes Christianity 308v3Rw - 3Vo yes Shia S7nvJY3ng2w yes Shia bY3Jvb6 - JHw yes Atheism 5rYQ - cUAe9I no ZYpBzSgUFNY unrelated gukyE6PD9ec yes Judaism , Islam MKzzrth6T _ 8 no EBN1OpRwM - c no AYN356BEbfw no HsjhJz4mpVs yes Shia DOeVkBMkl _ 0 unrelated sEEOoSuMVvs no nAP8wMCCe5k yes Shia BpuQvOGj5qM yes Shia CvYcnpH4Gyg unrelated FKbg2Lh0OFg yes Shia KkFeB1vYh _ k yes Atheism 51NJ577mVHA no Sn1KOSgLTHM no EoekkBYobYs no bTCr6VOUkJk yes Judaism , Christianity , Islam GffdGf1YPSY no 7ShmzK - 35ZQ no bX8Tae3cB - 8 yes Atheism FREy - imMDK8 unrelated r - IVThm8InE no OWOB0tDfY7M yes Judaism , Christianity mCXvytpTYyQ no SwH5QQn34Ws unrelated ndIugDoFOlo yes Shia , Sunni 9GXC5r _ SSgg unrelated ZfL3AHF4DYA unrelated s3ISnNsNX3A yes Shia gjM3cFybQFw unrelated xc - 9xVGICkk unrelated t - u - M5jD - 84 yes Judaism h _ ujLmijvEI unrelated qtNwwiavyaY yes Atheism 6gSM1GUpcng yes Shia F - AIYsYDjXQ unrelated TGv7AEF9K38 yes Shia UhrpQeWhdNk unrelated yXCxNhwYKUQ unrelated F2DiJP1kgdU yes Shia guZ9RqcEdjw unrelated 3W45TxFAas4 unrelated Rwq4rD _ qH _ 4 yes Shia W3 _ Et2yzOTE unrelated Z _ - n4E9pkmY unrelated w6akRu0VEm8 yes Shia Z7ldz6Ytw40 unrelated rXs6iJYfulM no g2iS61WCsvE yes Shia LKOoPcHxPhU unrelated 8Ax1tOZxf7g no V7p0e7gJn4M yes Shia gHoY2AU4snc unrelated E6sNbYsJ6R8 no 1m4K4f6YYE0 no sQilyC _ ICyU unrelated JlJeOeNSjlU no piJ8ro331Ak no w8Digjy8QbI unrelated PLRapMO - VUQ yes Shia , Sunni sKwAPfnUwb0 no 09P4NCJs3uU unrelated 16ntPL7Ij _ 0 unrelated bSR _ G - Ic _ Jk no YQ _ 3OAzWjQk unrelated u6b77OcRVUs yes Shia 5C - Rf61hU5o unrelated J5aseBw4BmM unrelated 2a4eIM7xIvY no n5Sz8XlOpmY unrelated cr6bh3xUWMg unrelated mvUw6Vw1NeA yes Christianity xlutMLEp40Y unrelated hHBdYzpRW48 yes Shia kohMe2iew _ 4 no GtEal9rT3RM yes Judaism , Christianity X5IkWjg1huA yes Atheism TSj2mLnqR - c no KWz8ON4vvds unrelated 2IGnJVY40vw yes Christianity tFZ _ rwHIyO0 no AG8D _ wiH7KE unrelated DW2Zkz _ zl - o unrelated UNWCrxr3Tfc no KbWD0X9bqrA unrelated qQOLPNyssGE unrelated Gxst3LPF86U no 3JsB6cuVZu0 unrelated xPSv - ThlK _ 4 unrelated NkvldtFqqNs no CSM0HR0aFWs unrelated TYdyg4XvRQ0 no 9gVOOW9HH1c no YjtitQtJzNk no ZekNtF8RV - A unrelated g7O7cOmOLTE no 1dNDGVHHm2Y yes Judaism , Christianity , Atheism , andothers Received January 2022 ; revised April 2022 ; accepted August 2022 Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 505 . Publication date : November 2022 .