Hierarchical Federated Learning with Adaptive Momentum in Multi - Tier Networks Zhengjie Yang ∗ , Sen Fu ∗ , Wei Bao ∗ , Dong Yuan † , Bing Zhou ∗ ∗ School of Computer Science , The University of Sydney , NSW , Australia † School of Electrical and Information Engineering , The University of Sydney , NSW , Australia { zhengjie . yang , wei . bao , dong . yuan , bing . zhou } @ sydney . edu . au , sefu7149 @ uni . sydney . edu . au Abstract —In this paper , we propose and analyze HierAdMo , a three - tier adaptive momentum accelerated client - edge - cloud Federated Learning ( FL ) algorithm . HierAdMo combines the momentum acceleration on both worker and edge levels . How - ever , simply combining these two levels of momenta may lead to disagreement between them , negatively inﬂuencing conver - gence performance . To this end , we embed an online adaptive method that scales down the momentum when disagreement occurs . We provide mathematical proof for the convergence of HierAdMo for non - i . i . d . data and the tighter convergence upper bound compared with a version of HierAdMo without adaptation ( HierAdMo - R ) . Finally , extensive experiments based on real - world datasets are conducted , verifying that HierAdMo outperforms existing mainstream benchmarks and achieves the optimal or near - optimal convergence performance compared with HierAdMo - R under a wide range of settings . Index Terms —wireless computing , federated learning , edge computing , momentum I . I NTRODUCTION The rapid development of Industry 4 . 0 , Internet of Things ( IoT ) , and Artiﬁcial Intelligence has led to the widespread use of machine learning applications in various ﬁelds such as image classiﬁcation [ 1 ] , automatic driving [ 2 ] , and automatic speech recognition [ 3 ] . However , as concerns about data privacy grow , individuals are becoming increasingly hesitant to share their sensitive data with the public . To address this issue , Federated Learning ( FL ) [ 4 ] has emerged as a solution . It allows individual users ( workers ) to collaborate in building a global model by only sending model updates rather than raw data . Relying on the high computation capacity of high performance computer ( aggregator ) on the cloud , workers can upload FL models to remote aggregator for model computation and aggregation . However , the two - tier FL setting is not scalable , as the communication overhead between workers and the aggregator increases proportionally with the number of workers . This overhead becomes particularly signiﬁcant in large - scale networks when a large number of workers connect to the remote cloud . The advent of edge computing [ 5 ] offers a more efﬁcient solution to address the scalability issue by introducing an edge tier between local workers and the remote cloud . This three - tier architecture differs from the traditional two - tier approach as it enables workers to ﬁrst communicate with the edge This work is supported by the Discovery Projects of Australian Research Council under Grant ID : DP200103718 . cloudserver two - tier architecture three - tier architecture edgenode edgenode workers public Internet cloudserver public Internet Fig . 1 . Two - tier architecture vs . three - tier architecture . The end - to - end connections are depicted by red dashed lines . In the two - tier architecture , there are a total of six connections that traverse the public Internet , while in the three - tier architecture , the number reduces signiﬁcantly to only two connections . The communication burdens are restrained within the local / edge networks . node for edge - level aggregation , before the edge nodes com - municate with the remote cloud for cloud - level aggregation . Since the edge node is closer to workers and is typically connected with them within the same local / edge network , the communication cost is much cheaper compared with the two - tier approach where workers are directly connected with the cloud . As seen in Fig . 1 , we can see that the presence of the edge nodes reduces the amount of trafﬁc through the public Internet , making the three - tier architecture more suitable for large - scale networks . It has attracted researchers’ attentions in recently years [ 6 ] – [ 8 ] . The three - tier FL offers an effective solution to increase the communication efﬁciency by restraining the public trafﬁc in the local networks . However , it still poses a challenge to the training efﬁciency . Due to non - i . i . d . data among edge nodes , each edge node only aggregates the updates of its local workers ( edge - level aggregation ) , which leads to discrepancies among edge nodes . The subsequent cloud - level aggregation further exacerbates the delayed synchronization between workers and the cloud , resulting in less training efﬁciency . Therefore , there 499 2023 IEEE 43rd International Conference on Distributed Computing Systems ( ICDCS ) 2575 - 8411 / 23 / $ 31 . 00 ©2023 IEEE DOI 10 . 1109 / ICDCS57875 . 2023 . 00053 2023 I EEE 43 r d I n t e r n a t i o n a l C o n f e r e n c e o n D i s t r i bu t e d C o m pu t i n g S y s t e m s ( I C D C S ) | 979 - 8 - 3503 - 3986 - 4 / 23 / $ 31 . 00 © 2023 I EEE | D O I : 10 . 1109 / I C D C S 57875 . 2023 . 00053 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . is a pressing need to develop an algorithm that can accelerate the convergence while preserving the efﬁcient communication in the three - tier hierarchical architecture . The use of momentum has been shown to be an effective method to accelerate model training in various studies , both in centralized machine learning environment [ 9 ] – [ 12 ] and in two - tier FL environment [ 13 ] – [ 16 ] . This motivates us to adopt momentum in both workers and edge nodes in three - tier ar - chitecture . However , combining two types of momentum may still cause problems as they may disagree and cancel with each other in the short run , harming the convergence performance . To overcome this challenge , we propose an online adaptive momentum method based on real - time angle information to adjust the momentum factor ( weights of the momenta ) , so as to avoid the disadvantage caused by disagreement . In this paper , we propose Hierarchical Federated Learning with Adaptive Momentum ( HierAdMo ) . HierAdMo leverages the acceleration of worker and edge momenta while mitigating the effects when they disagree with each other , ultimately improving the long - run performance . We prove the convergence guarantee of HierAdMo , showing an O (cid:2) 1 T (cid:3) convergence rate for smooth non - convex problems in T iterations under non - i . i . d . data distribution . Substantial new challenges are addressed . We develop a new method to characterize the multi - time cross - two - tier momentum interac - tion and cross - three - tier momentum interaction , which do not exist in the two - tier FL . We also prove a tighter convergence upper bound of HierAdMo compared with a reduced version of HierAdMo without adaptation ( HierAdMo - R ) , which directly combines two momenta . In the experiment , we compare the performance of HierAdMo with three categories of FL al - gorithms : 1 hierarchical FL ( HierAdMo - R , HierFAVG [ 17 ] , and CFL [ 18 ] ) , 2 momentum - based FL ( FedMom [ 19 ] , SlowMo [ 20 ] , FedNAG [ 21 ] , Mime [ 22 ] , FastSlowMo [ 23 ] , and FedADC [ 24 ] ) , and 3 FedAvg [ 4 ] . The experiment is implemented in four real - world datasets ( MNIST [ 25 ] , CIFAR - 10 [ 26 ] , ImageNet [ 27 ] , and UCI - HAI [ 28 ] ) with ﬁve machine learning models ( linear regression , logistic regression , CNN [ 29 ] , VGG16 [ 30 ] , and ResNet18 [ 27 ] ) . The experi - mental results illustrate that HierAdMo drastically outperforms benchmarks under a wide range of settings . The contributions of this paper are summarized as follows . • We prove that HierAdMo is convergent and has an O (cid:2) 1 T (cid:3) convergence rate for smooth non - convex problems for a given T iterations under non - i . i . d . data . • We prove that HierAdMo achieves the tighter conver - gence upper bound than HierAdMo - R without momen - tum adaptation . • HierAdMo is efﬁcient and improves the training speed by 1 . 30x – 4 . 36x compared with the mainstream two - tier momentum - based algorithms and three - tier algorithms . • HierAdMo achieves the optimal or near - optimal training accuracy compared with HierAdMo - R with ﬁxed momen - tum factor settings . The rest of the paper is organized as follows . In Section II , we introduce related works . The HierAdMo algorithm design is described in Section III including the updates on three tiers and the intuition of online adaptive momentum method . In Section IV , we provide theoretical results including the convergence analysis of HierAdMo and the tighter comver - gence upper bound compared with HierAdMo - R . Section V provides our experimental results and the conclusion is made in Section VI . II . R ELATED W ORK A . Momentum in Machine Learning and Federated Learning Momentum [ 31 ] is a technique that accelerates gradient descent by adding a fraction γ of the past and current model vectors’ difference . The update rule for momentum ( Polyak’s momentum ) in the classical centralized setting is deﬁned as follows : m ( t ) = γ m ( t − 1 ) − η ∇ F ( w ( t − 1 ) ) , ( 1 ) w ( t ) = w ( t − 1 ) + m ( t ) , ( 2 ) with γ ∈ [ 0 , 1 ) , t = 1 , 2 , . . . , m ( 0 ) = 0 , where γ is momentum factor ( weight of momentum ) , t is update iteration , m ( t ) is momentum term at iteration t , and w ( t ) is model parameter at iteration t . Momentum increases for dimensions whose gradients point in the same direction and reduces updates for dimensions whose gradients change directions . As a result , momentum leads to faster convergence and reduces oscillation [ 32 ] , [ 33 ] . Momentum has been investigated in both centralized ma - chine learning and FL . In the centralized environment , a variant of momentum called Nesterov Accelerate Gradient ( NAG ) [ 32 ] , [ 34 ] is proposed . NAG improves convergence performance by approximating the gradient based on the next position of the parameters , i . e . , ∇ F ( w ( t − 1 ) + γ m ( t − 1 ) ) , instead of the current position ∇ F ( w ( t − 1 ) ) used in Polyak’s momentum . In [ 9 ] , authors study a uniﬁed con - vergence analysis for both Polyak’s momentum and NAG . In [ 12 ] , authors study NAG in stochastic settings . These studies demonstrate the efﬁciency of momentum acceleration in centralized training , which has sparked researchers’ interest in applying momentum in FL environments . Depending on where the momentum is adopted , we can categorize them into worker momentum [ 21 ] , [ 22 ] , aggregator momentum [ 19 ] , [ 20 ] , and combination momentum [ 23 ] , [ 24 ] , [ 35 ] . The worker momentum and aggregator momentum algorithms utilize the momentum acceleration on either worker level or aggregator level , while the combination momentum algorithms combine the worker and aggregator momenta and they show a better convergence performance than only using either worker or aggregator momentum . The above forms of momentum are only adopted and analyzed in the two - tier FL setting and we focus on the three - tier scenarios in this paper . B . Three - tier Hierarchical Federated Learning Three - tier FL has gained increasing attention in recent years . Previous studies have shown its effectiveness in terms of convergence performance without considering momentum 500 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . TABLE I K EY N OTATIONS η worker model learning rate τ worker - edge aggregation period π edge - cloud aggregation period γ worker momentum factor γ (cid:2) edge momentum factor in edge node (cid:6) T number of total local ( worker ) iterations indexed by t K number of total edge aggregations indexed by k P number of total global ( cloud ) aggregations indexed by p L number of edge nodes indexed by (cid:6) C (cid:2) number of workers under edge node (cid:6) N number of workers in the system indexed by { i , (cid:6) } x ti , (cid:2) worker model parameter in worker { i , (cid:6) } at iteration t y ti , (cid:2) worker momentum parameter in worker { i , (cid:6) } at iteration t y t(cid:2) − aggregated worker momentum in edge node (cid:6) at iteration t x t(cid:2) − aggregated worker model in edge node (cid:6) at iteration t y t(cid:2) + updated edge momentum in edge node (cid:6) at iteration t x t(cid:2) + updated edge model in edge node (cid:6) at iteration t y t worker momentum cloud aggregation in the cloud at iteration t x t cloud model in the cloud at iteration t [ 17 ] , [ 18 ] , [ 36 ] , [ 37 ] . In [ 38 ] , researchers propose a method to optimize edge aggregation control and resource allocation . The communication overhead can be further optimized in [ 39 ] . However , these studies do not take into account the use of momentum in three - tier FL , and the convergence analysis extended from two - tier to three - tier FL is not straightforward . Unlike two - tier FL where the global aggregation is executed every τ local iterations , in three - tier FL , each worker’s local model is ﬁrst aggregated by the connected edge node every τ local iterations , and then by the cloud in another level of every π edge aggregations . When momentum is leveraged in the three - tier scenario , it additionally introduces multi - time cross - two - tier momentum interaction and cross - three - tier momentum interaction . This is completely different from the two - tier scenario . Existing two - tier analyses cannot deal with the above two new terms . We develop a two - level virtual update ( edge and cloud ) method that is able to bound the aforementioned new terms , ensuring the convergence of HierAdMo . III . H IER A D M O S YSTEM M ODEL A . Problem Formulation We consider the HierAdMo algorithm to be implemented in a three - tier Hierarchical FL system consisting of one cloud server , L edge nodes , and N clients . Each edge node , denoted by (cid:6) , serves C (cid:2) workers , so the total number of workers N = (cid:4) L(cid:2) = 1 C (cid:2) . Worker { i , (cid:6) } denotes i th worker in (cid:6) th edge node , where i = 1 , 2 , . . . , C (cid:2) . Each worker has its own local dataset , with a number of data samples denoted by D i , (cid:2) . The total data samples in edge node (cid:6) can then be denoted by D (cid:2) (cid:2) (cid:4) C (cid:2) i = 1 D i , (cid:2) , and the total training dataset for the system is D (cid:2) (cid:4) L(cid:2) = 1 D (cid:2) = (cid:4) L(cid:2) = 1 (cid:4) C (cid:2) i = 1 D i , (cid:2) . The HierAdMo algorithm aims to ﬁnd the stationary point x ∗ , which minimizes the global loss function F ( x ) . The problem can be formulated as follows : min x ∈ R d F ( x ) (cid:2) 1 D L (cid:5) (cid:2) = 1 C (cid:2) (cid:5) i = 1 D i , (cid:2) F i , (cid:2) ( x ) ( 3 ) = L (cid:5) (cid:2) = 1 D (cid:2) D C (cid:2) (cid:5) i = 1 D i , (cid:2) D (cid:2) F i , (cid:2) ( x ) ( 4 ) (cid:2) L (cid:5) (cid:2) = 1 D (cid:2) D F (cid:2) ( x ) , ( 5 ) where d is the dimension of x , and F ( x ) is the global loss function at the cloud server which is the weighted average of all workers’ loss functions F i , (cid:2) ( x ) as shown in ( 3 ) . We deﬁne the edge loss function at the edge node (cid:6) as F (cid:2) ( x ) (cid:2) (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) F i , (cid:2) ( x ) . Then , we can derive the global loss function F ( x ) is also the weighted average of all edge node functions F (cid:2) ( x ) as shown in ( 5 ) . The formula ( 3 ) is within the scope of cross - siloed FL [ 40 ] , as all workers are expected to participate in the model training with non - i . i . d . siloed data . We summarize the key notations in Table I . B . HierAdMo Algorithm In Algorithm 1 , we propose a new adaptive momentum accelerated hierarchical FL algorithm called HierAdMo . The goal of HierAdMo is to ﬁnd the ﬁnal cloud model x T to solve formula ( 3 ) . HierAdMo conducts T local iterations at worker level , K edge aggregations at edge level , and P cloud aggregations at the cloud server , where T = Kτ = Pτπ . We use τ and π to denote worker - edge aggregation period and edge - cloud aggregation period , respectively . 1 ) Worker update : At each local iteration t , each worker { i , (cid:6) } computes 1 worker momentum update y ti , (cid:2) ( Line 5 ) and 2 worker model update x ti , (cid:2) ( Line 6 ) . 1 and 2 follow the Nesterov Accelerated Gradient ( NAG ) [ 34 ] and are conducted every local iteration . Through this way , each worker can utilize its own worker momentum acceleration . 2 ) Edge update : When t = kτ , k = 1 , 2 , . . . , K , each edge node (cid:6) ﬁrst receives values ( Line 9 ) from its connected workers and then perform edge update , which comprises of three operations : 1 Adaption of edge momentum factor γ (cid:2) that will be discussed later in detail . 2 Worker momentum edge aggregation y kτ(cid:2) − ( Line 11 ) with re - distribution ( Line 14 ) . Through this way , some straggler workers with high data - heterogeneity whose local momenta y kτi , (cid:2) pointing to an in - appropriate direction 1 can be reﬁned from y kτ(cid:2) − . 3 Edge momentum y kτ(cid:2) + and model x kτ(cid:2) + update ( Lines 12 – 13 ) using the adapted γ (cid:2) followed by model re - distribution ( Line 15 ) . Through this way , each edge node utilizes the momentum acceleration on the edge level and dampens oscillations [ 32 ] . Please note that 2 and 3 are two operations on the same edge node , hence we use subscript “ − ” and “ + ” to label the mo - mentum / model right after operations 2 and 3 respectively . 1 Inappropriate direction means a small part of worker momenta y kτi , (cid:2) point to the opposite direction ( obtuse Angle ) to the edge aggregated worker momentum y kτ(cid:2) . 501 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Algorithm 1 HierAdMo algorithm . Input : τ , π , T = Kτ = Pτπ , η , γ Output : Final cloud ( global ) model parameter x T 1 : For each worker , initialize x 0 i , (cid:2) as same value for all i , (cid:6) , and y 0 i , (cid:2) = x 0 i , (cid:2) 2 : For each edge node , initialize x 0 (cid:2) + = x 0 i , (cid:2) and y 0 (cid:2) + = x 0 (cid:2) + 3 : for t = 1 , 2 , . . . , T do 4 : For each worker i = 1 , 2 , . . . , N in parallel , 5 : y ti , (cid:2) ← x t − 1 i , (cid:2) − η ∇ F i , (cid:2) ( x t − 1 i , (cid:2) ) / / Worker momentum update 6 : x ti , (cid:2) ← y ti , (cid:2) + γ ( y ti , (cid:2) − y t − 1 i , (cid:2) ) / / Worker model update 7 : if t = = kτ where k = 1 , . . . , K then 8 : For each edge node (cid:6) = 1 , 2 , . . . , L in parallel , 9 : Receive y kτi , (cid:2) , x kτi , (cid:2) , (cid:4) kτ − 1 t = ( k − 1 ) τ ∇ F i , (cid:2) ( x ti , (cid:2) ) , and (cid:4) kτ − 1 t = ( k − 1 ) τ y ti , (cid:2) from its connected workers , / / Receive values 10 : Adapt γ (cid:2) as ( 6 ) , ( 7 ) / / Adapt edge momentum factor 11 : y kτ(cid:2) − ← (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) y kτi , (cid:2) / / Worker momentum edge aggregation 12 : y kτ(cid:2) + ← x ( k − 1 ) τ (cid:2) + − (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) (cid:6) x ( k − 1 ) τ (cid:2) + − x kτi , (cid:2) (cid:7) / / Edge momentum update 13 : x kτ(cid:2) + ← y kτ(cid:2) + + γ (cid:2) (cid:6) y kτ(cid:2) + − y ( k − 1 ) τ (cid:2) + (cid:7) / / Edge model update 14 : Set y kτi , (cid:2) ← y kτ(cid:2) − for all worker i ∈ C (cid:2) / / Edge aggregated worker momentum re - distribution to workers 15 : Set x kτi , (cid:2) ← x kτ(cid:2) + for all worker i ∈ C (cid:2) / / Edge model re - distribution to workers 16 : end if 17 : if t = = pτπ where p = 1 , 2 , . . . , P then 18 : Aggregate y pτπ ← (cid:4) L(cid:2) = 1 D (cid:2) D y pτπ(cid:2) − / / Worker momentum cloud aggregation 19 : Aggregate x pτπ ← (cid:4) L(cid:2) = 1 D (cid:2) D x pτπ(cid:2) + / / Edge model cloud aggregation 20 : Set y pτπ(cid:2) − ← y pτπ for all edge node l ∈ L / / Cloud aggregated worker momentum re - distribution to edge nodes 21 : Set x pτπ(cid:2) + ← x pτπ for all edge node l ∈ L / / Cloud model re - distribution to edge nodes 22 : Set y pτπi , (cid:2) ← y pτπ(cid:2) − for all worker i ∈ C (cid:2) , l ∈ L / / Cloud aggregated worker momentum re - distribution from edge nodes to workers 23 : Set x pτπi , (cid:2) ← x pτπ(cid:2) + for all worker i ∈ C (cid:2) , l ∈ L / / Cloud model re - distribution from edge nodes to workers 24 : end if 25 : end for Operations 1 – 3 are conducted in each edge node every τ local iterations . 3 ) Adaptive edge momentum factor : When T = kτ , k = 1 , 2 , . . . , K , the adaptation of γ (cid:2) is conducted in each edge node (cid:6) : cos θ k , (cid:2) ← C (cid:2) (cid:2) i = 1 D i , (cid:2) D (cid:2) (cid:3) − (cid:4) kτ − 1 t = ( k − 1 ) τ ∇ F i , (cid:2) ( x ti , (cid:2) ) , (cid:4) kτ − 1 t = ( k − 1 ) τ y ti , (cid:2) (cid:5) (cid:6)(cid:6)(cid:6)(cid:4) kτ − 1 t = ( k − 1 ) τ ∇ F i , (cid:2) ( x ti , (cid:2) ) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:4) kτ − 1 t = ( k − 1 ) τ y ti , (cid:2) (cid:6)(cid:6)(cid:6) , ( 6 ) γ (cid:2) = ⎧⎪⎨ ⎪⎩ 0 , if − 1 ≤ cos θ k , (cid:2) ≤ 0 , cos θ k , (cid:2) , if 0 < cos θ k , (cid:2) < 0 . 99 , 0 . 99 , if 0 . 99 ≤ cos θ k , (cid:2) ≤ 1 . ( 7 ) Previous research and experiments [ 13 ] , [ 21 ] have shown that it is not necessary to adapt the worker momentum factor γ . Larger γ generally leads to better convergence performance . Therefore , we focus on adapting the edge momentum factor γ (cid:2) for each edge node (cid:6) , instead of γ . We aim to adjust γ (cid:2) to improve the momentum acceleration based on the real - time angle θ which reﬂects the level of agreement / disagreement between worker and edge momenta . A small angle means they agree with each other , so we give a large weight to the edge momentum , while a large angle means they disagree with each other , so we give a small weight . Here , the value of γ (cid:2) represents the weight of the edge node (cid:6) ’s edge momentum , which is calculated by the cosine of the angle . If cos θ > 0 ( θ < 90 ° ) , we use cos θ as the weight . If cos θ < 0 ( θ > 90 ° ) , such edge momentum will go opposite to the edge model descent direction and harm the convergence . Therefore , we set 0 weight in this case . Please note that the value of γ (cid:2) cannot be equal or greater than 1 , so we limit it by 0 . 99 to avoid divergence [ 19 ] – [ 22 ] , [ 32 ] . 4 ) Cloud update : When t = pτπ , p = 1 , 2 , . . . , P , the cloud receives edge aggregated worker momentum y pτπ(cid:2) − and edge model x pτπ(cid:2) + for all (cid:6) ∈ L and performs cloud update , which comprises of two operations : 1 Worker momentum cloud aggregation y pτπ ( Line 18 ) and re - distribution ( Lines 20 and 22 ) . Through this way , all edge nodes and workers receive the cloud aggregated worker momentum and mitigate the disadvantage caused by non - i . i . d . data heterogeneity . 2 Edge model cloud aggregation x pτπ ( Line 19 ) and cloud model re - distribution ( Lines 21 and 23 ) . The cloud will re - distribute the momentum and model to all edge nodes ﬁrst and all edge nodes will then distribute them to all workers . IV . C ONVERGENCE A NALYSIS OF H IER A D M O A . Preliminaries We assume F i , (cid:2) ( · ) satisﬁes the following standard condi - tions which are widely used in the literature [ 13 ] , [ 21 ] , [ 41 ] . Assumption 1 . F i , (cid:2) ( x ) is ρ - Lipschitz , i . e . , (cid:6) F i , (cid:2) ( x 1 ) − F i , (cid:2) ( x 2 ) (cid:6) ≤ ρ (cid:6) x 1 − x 2 (cid:6) for any x 1 , x 2 , i , (cid:6) . Assumption 2 . F i , (cid:2) ( x ) is β - smooth , i . e . , (cid:6)∇ F i , (cid:2) ( x 1 ) − ∇ F i , (cid:2) ( x 2 ) (cid:6) ≤ β (cid:6) x 1 − x 2 (cid:6) for any x 1 , x 2 , i , (cid:6) . Assumption 3 . ( Bounded diversity ) The variance of local gradient to edge gradient is bounded . i . e . , (cid:6)∇ F i , (cid:2) ( x ) − ∇ F (cid:2) ( x ) (cid:6) ≤ δ i , (cid:2) for ∀ i , ∀ (cid:6) , and ∀ x . We also deﬁne δ (cid:2) as the weighted average of δ i , (cid:2) and δ as the weighted average of δ (cid:2) , i . e . , δ (cid:2) (cid:2) (cid:4) i ∈ C (cid:2) D i , (cid:2) D (cid:2) δ i , (cid:2) and δ (cid:2) (cid:4) (cid:2) ∈ L D (cid:2) D δ (cid:2) . 502 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Given Assumptions 1 and 2 , by using the Triangle Inequality to F i , (cid:2) ( x ) , it is straightforward to show that F (cid:2) ( x ) is ρ - Lipschitz and β - smooth . Applying the Triangle Inequality to F (cid:2) ( x ) , we can also derive that F ( x ) is ρ - Lipschitz and β - smooth . Assumption 3 indicates that the data distributed to all workers are heterogeneous and non - i . i . d . . δ i , (cid:2) is used to quantify the level of gradient divergence and is different at different workers . B . Virtual Update In order to index the edge aggregation and cloud aggre - gation , the total T local iterations are divided into K edge intervals and P cloud intervals , where T = Kτ = Pτπ . We use [ k ] to indicate the edge interval for t ∈ [ ( k − 1 ) τ , kτ ] , k = 1 , 2 , . . . , K , and { p } to indicate the cloud interval for t ∈ [ ( p − 1 ) τπ , pτπ ] , p = 1 , 2 , . . . , P . Please note that edge aggregation occurs at the end of each edge interval , and cloud aggregation occurs at the end of each cloud interval . Therefore , each edge interval [ k ] comprises of τ local iterations and one edge aggregation , and each cloud interval { p } comprises of π edge intervals and one cloud aggregation , i . e . , { p } = ∪ k [ k ] for k = ( p − 1 ) π + 1 , ( p − 1 ) π + 2 , . . . , pπ . At the beginning of edge interval [ k ] when t = ( k − 1 ) τ , we set edge virtual update y ( k − 1 ) τ [ k ] , (cid:2) ← y ( k − 1 ) τ (cid:2) − , ( 8 ) x ( k − 1 ) τ [ k ] , (cid:2) ← x ( k − 1 ) τ (cid:2) + , ( 9 ) for each edge node (cid:6) , where y ( k − 1 ) τ [ k ] , (cid:2) and x ( k − 1 ) τ [ k ] , (cid:2) are set as the virtual aggregated values right after the edge aggregation occurs . Then , we further conduct edge virtual update as if model and momentum updates are conducted in the edge node . When t ∈ ( ( k − 1 ) τ , kτ ] , we conduct edge virtual update as y t [ k ] , (cid:2) ← x t − 1 [ k ] , (cid:2) − η ∇ F (cid:2) ( x t − 1 [ k ] , (cid:2) ) , ( 10 ) x t [ k ] , (cid:2) ← y t [ k ] , (cid:2) + γ ( y t [ k ] , (cid:2) − y t − 1 [ k ] , (cid:2) ) . ( 11 ) We repeat ( 8 ) – ( 11 ) for each edge interval [ k ] where k = 1 , 2 , . . . , K . Please note that only if t = kτ , k = 1 , . . . , K , y t(cid:2) − and x t(cid:2) + are computed . For ease of analysis , we de - ﬁne intermediate value x t(cid:2) − = (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) x ti , (cid:2) and y t(cid:2) − = (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) y t i , (cid:2) that are meaningful at any iteration t . Same as edge intervals , for each cloud interval { p } where p = 1 , 2 , . . . , P , the cloud virtual update is also conducted : y ( p − 1 ) τπ { p } ← y ( p − 1 ) τπ , ( 12 ) x ( p − 1 ) τπ { p } ← x ( p − 1 ) τπ , ( 13 ) when t = ( p − 1 ) τπ , and y t { p } ← x t − 1 { p } − η ∇ F ( x t − 1 { p } ) , ( 14 ) x t { p } ← y t { p } + γ ( y t { p } − y t − 1 { p } ) , ( 15 ) when p ∈ ( ( p − 1 ) τπ , pτπ ] . By employing virtual updates for momenta and models on both edge nodes and the cloud , we effectively establish a connection between real updates and these virtual updates . Such connection allows us to bound the discrepancy between the two types of updates that can be then used to prove the convergence . The concept of virtual updates is an intermediate process which plays a vital role in the convergence analysis and is one of our contributions in this paper . C . Convergence Analysis In this section , we provide the convergence analysis of HierAdMo . In Theorem 1 , we ﬁrst bound the distance between edge intermediate value x t(cid:2) − and edge virtual update x t [ k ] , (cid:2) within interval [ k ] . Theorem 1 . For any edge interval [ k ] , ∀ t ∈ ( ( k − 1 ) τ , kτ ] and ∀ (cid:6) ∈ L , we have (cid:6) x t(cid:2) − − x t [ k ] , (cid:2) (cid:6) ≤ h ( t − ( k − 1 ) τ , δ (cid:2) ) , ( 16 ) where h ( x , δ (cid:2) ) is h ( x , δ (cid:2) ) = ηδ (cid:2) (cid:12) I ( γA ) x + J ( γB ) x − 1 ηβ − γ 2 ( γ x − 1 ) − ( γ − 1 ) x ( γ − 1 ) 2 (cid:13) , ( 17 ) and A , B , I , and J are constants deﬁned in Appendix A , for 0 < γ < 1 and any positive integer x . Please note that when t = ( k − 1 ) τ for all [ k ] , we have (cid:6) x t(cid:2) − − x t [ k ] , (cid:2) (cid:6) = 0 = h ( 0 , δ (cid:2) ) , which also satisﬁes ( 17 ) . Also , F (cid:2) ( x ) is ρ - Lipschitz , so that we also have F (cid:2) ( x t(cid:2) − ) − F (cid:2) ( x t [ k ] , (cid:2) ) ≤ ρh ( t − ( k − 1 ) τ , δ (cid:2) ) . ( 18 ) Proof . See Appendix A for the complete proof . In Theorem 2 , we then bound the edge momentum update between x kτ(cid:2) + and x kτ(cid:2) − within interval [ k ] . Theorem 2 . For any edge interval [ k ] in any edge node (cid:6) ∈ L , suppose 0 < γ < 1 , 0 < γ (cid:2) < 1 , and any τ = 1 , 2 , . . . , we have (cid:6) x kτ(cid:2) + − x kτ(cid:2) − (cid:6) ≤ s ( τ ) , ( 19 ) where s ( τ ) is s ( τ ) = γ (cid:2) τηρ ( γμ + γ + 1 ) ( 20 ) and constant μ is deﬁned in Appendix B . Proof . See Appendix B for the complete proof . By combining the results of Theorem 1 and Theorem 2 , we can telescope the bound within edge interval [ k ] to the cloud interval { p } where k = ( p − 1 ) π + 1 , ( p − 1 ) π + 2 , . . . , pπ . Then , in Theorem 3 , we are ready to bound the gap between weighted average of edge virtual update (cid:4) L(cid:2) = 1 D (cid:2) D x pτπ [ pπ ] , (cid:2) and cloud virtual update x pτπ { p } . Theorem 3 . For any cloud interval { p } , 0 < γ < 1 , and 0 < γ (cid:2) < 1 , when edge interval [ k ] = [ pπ ] ( the last edge 503 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . interval in cloud interval { p } ) , and ∀ τ , π ∈ { 1 , 2 , . . . } we have (cid:6) x pτπ [ pπ ] − x pτπ { p } (cid:6) ≤ h ( τπ , δ ) + π L (cid:5) (cid:2) = 1 D (cid:2) D ( h ( τ , δ (cid:2) ) + s ( τ ) ) , ( 21 ) where we deﬁne x pτπ [ pπ ] (cid:2) (cid:4) L(cid:2) = 1 D (cid:2) D x pτπ [ pπ ] , (cid:2) , for ∀ (cid:6) ∈ L . Proof . See Appendix C for complete proof . Theorem 4 . Under the following conditions : ( 1 ) 0 < βη ( γ + 1 ) ≤ 1 , 0 < γ < 1 , 0 < γ (cid:2) < 1 , and ∀ τ , π ∈ { 1 , 2 , . . . } ; ( 2 ) ∃ ε > 0 , ( 2 . 1 ) ωασ 2 − ρj ( τ , π , δ (cid:2) , δ ) τπε 2 > 0 ; ( 2 . 2 ) F ( x pτπ { p } ) − F ( x ∗ ) ≥ ε , ∀ p ; and ( 2 . 3 ) F ( x T ) − F ( x ∗ ) ≥ ε are satisﬁed ; Algorithm 1 gives F ( x T ) − F ( x ∗ ) ≤ 1 T (cid:6) ωασ 2 − ρj ( τ , π , δ (cid:2) , δ ) τπε 2 (cid:7) . ( 22 ) where j ( τ , π , δ (cid:2) , δ ) is j ( τ , π , δ (cid:2) , δ ) = h ( τπ , δ ) + ( π + 1 ) L (cid:5) (cid:2) = 1 D (cid:2) D ( ( h ( τ , δ (cid:2) ) + s ( τ ) ) ) . ( 23 ) We deﬁne F ( x ∗ ) as the minimum value , if there exists some ϕ > 0 such that F ( x ∗ ) ≤ F ( x ) for all x within distance ϕ of x ∗ . Constant μ is deﬁned in Appendix B and constants ω , σ , and α are deﬁned in Appendix D . Proof . See Appendix D for complete proof . We have demonstrated that the gap between the global loss function value F ( x T ) and the stationary point F ( x ∗ ) is upper bounded by a function of T ( T = Kτ = Pτπ ) which is inversely proportional to T . It converges with the convergence rate O (cid:2) 1 T (cid:3) for smooth non - convex problems under non - i . i . d . data distribution . According to Theorem 4 , we observe that the overall gap F ( x T ) − F ( x ∗ ) decreases when T is larger . From ( 39 ) , we have h ( x ) ≥ 0 for any x = 1 , 2 , . . . , and it increases with x . According to ( 20 ) , s ( τ ) increases with τ . According to ( 23 ) , j ( τ , π ) increases with τ and π . Therefore , the value of ρj ( τ , π ) τπε 2 increases with τ and π so as to increase the overall bound F ( x T ) − F ( x ∗ ) . However , in order to let the Condition ( 2 . 1 ) in Theorem 4 hold , we cannot set a very large τ and π , implying that convergence is guaranteed when j ( τ , π ) is below a certain threshold . Experiments on the effects of τ and π further verify that larger τ and π decreases the convergence performance . In Theorem 5 , we compare the convergence upper bound of HierAdMo with HierAdMo - R . HierAdMo - R is the reduced version of HierAdMo when the two momenta are directly combined without adaptation . In HieAdMo - R , both worker momentum factor γ and edge momentum factor γ (cid:2) are ﬁxed , while in HieAdMo , γ (cid:2) is adaptive . Theorem 5 . HierAdMo with adaptive γ (cid:2) achieves a tighter bound of F ( x T ) − F ( x ∗ ) than the reduced version of Hier - AdMo ( HierAdMo - R ) with ﬁxed γ (cid:2) . Proof . See Appendix E for the complete proof . Theorem 5 demonstrates that the expected mean of conver - gence upper bound for HierAdMo is smaller than HierAdMo - R , implying the better convergence performance of HierAdMo . Experiments in Fig . 2 further illustrates that HierAdMo out - performs HierAdMo - R under a wide range of settings . V . E XPERIMENTAL R ESULTS A . Experimental Setup We conduct image classiﬁcation experiments using three real - world datasets : MNIST [ 25 ] , CIFAR - 10 [ 26 ] , and Im - ageNet [ 27 ] , [ 42 ] . Additionally , we explore UCI - HAR [ 28 ] for human activity recognition . All training and testing data samples are randomly shufﬂed and distributed among the workers . It’s important to note that there are no restrictions on how the data is distributed among the workers . Therefore , different levels of non - i . i . d . data distribution captured by δ i , (cid:2) is different in each worker { i , (cid:6) } . To investigate various non - i . i . d . data scenarios , we generate x - class non - i . i . d . datasets for different workers , where each worker is assigned only x number of classes from the dataset . The training process is carried out on a GPU tower server equipped with 4 NVIDIA GeForce RTX 2080Ti GPUs . We utilize ﬁve different models for our experiments : linear regression , logistic regression , CNN , VGG16 , and ResNet18 . For linear regression , we em - ploy the mean squared error loss , while logistic regression uses the cross - entropy loss . The CNN model follows the classic structure outlined in [ 29 ] . The structures of VGG16 and ResNet18 can be found in [ 27 ] , [ 30 ] , respectively . We utilize mini - batch processing with a batch size of 64 . The learning rate η is set to 0 . 01 . Speciﬁc hyper - parameters for each experiment are deﬁned and speciﬁed accordingly . B . Performance Comparison Table II presents a comparison of the convergence perfor - mance between HierAdMo and other benchmark algorithms . The accuracy values displayed in the table correspond to the results obtained after running different algorithms for a speciﬁed number of local iterations T . The experiments cover linear regression , logistic regression , CNN , VGG16 , and ResNet18 models . We set T = 1000 ( MNIST ) , T = 4000 ( UCI - HAR ) , or T = 10000 ( CIFAR10 and ImageNet ) , γ = 0 . 5 , γ (cid:2) = 0 . 5 . For three - tier algorithms , we use four workers , two edge nodes and one cloud server , with each edge node serving two workers , while for two - tier algorithms , four workers are directly connected with the cloud server . For two - tier algorithms , we set τ = 20 ( convex model ) or τ = 40 ( non - convex model ) . For three - tier algorithms , we set τ = 10 , π = 2 ( convex model ) or τ = 20 , π = 2 ( non - convex model ) . Since π is not applicable to the two - tier algorithms , we set the τ value for the two - tier algorithms to match the τπ value used in the three - tier algorithms to ensure a fair comparison . These hyper - parameter settings are commonly employed in existing works [ 13 ] , [ 14 ] , [ 21 ] . 504 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . TABLE II P ERFORMANCE COMPARISON OF DIFFERENT FL ALGORITHMS ( ACCURACY % ) . Linear on MNIST Logistic on MNIST CNN on MNIST CNN on CIFAR10 VGG16 on CIFAR10 ResNet18 on ImageNet CNN on UCI - HAR HierAdMo 86 . 16 86 . 16 86 . 16 ± 0 . 04 89 . 88 89 . 88 89 . 88 ± 0 . 03 97 . 25 97 . 25 97 . 25 ± 0 . 05 66 . 74 66 . 74 66 . 74 ± 0 . 10 90 . 88 90 . 88 90 . 88 ± 0 . 14 69 . 83 69 . 83 69 . 83 ± 0 . 09 89 . 41 89 . 41 89 . 41 ± 0 . 08 HierAdMo - R 85 . 97 ± 0 . 03 89 . 23 ± 0 . 04 96 . 13 ± 0 . 07 64 . 18 ± 0 . 08 90 . 06 ± 0 . 15 69 . 64 ± 0 . 12 88 . 36 ± 0 . 06 HierFAVG [ 17 ] 83 . 62 ± 0 . 03 87 . 00 ± 0 . 05 93 . 40 ± 0 . 07 38 . 46 ± 0 . 13 89 . 46 ± 0 . 12 68 . 63 ± 0 . 10 54 . 56 ± 0 . 11 CFL [ 18 ] 83 . 36 ± 0 . 04 86 . 98 ± 0 . 06 93 . 58 ± 0 . 06 38 . 79 ± 0 . 11 89 . 80 ± 0 . 11 68 . 87 ± 0 . 09 69 . 19 ± 0 . 09 FastSlowMo [ 23 ] 85 . 79 ± 0 . 05 89 . 02 ± 0 . 05 95 . 90 ± 0 . 05 59 . 39 ± 0 . 07 88 . 53 ± 0 . 09 67 . 05 ± 0 . 10 88 . 15 ± 0 . 06 FedADC [ 24 ] 85 . 51 ± 0 . 04 88 . 18 ± 0 . 05 95 . 09 ± 0 . 07 56 . 00 ± 0 . 11 89 . 38 ± 0 . 08 67 . 76 ± 0 . 12 85 . 14 ± 0 . 09 FedMom [ 19 ] 84 . 84 ± 0 . 06 88 . 05 ± 0 . 05 94 . 74 ± 0 . 05 54 . 87 ± 0 . 07 88 . 03 ± 0 . 10 66 . 91 ± 0 . 11 84 . 69 ± 0 . 07 SlowMo [ 20 ] 84 . 82 ± 0 . 06 88 . 00 ± 0 . 06 94 . 88 ± 0 . 05 54 . 43 ± 0 . 06 88 . 47 ± 0 . 09 66 . 84 ± 0 . 09 83 . 03 ± 0 . 10 FedNAG [ 21 ] 84 . 97 ± 0 . 04 88 . 14 ± 0 . 05 95 . 04 ± 0 . 06 55 . 54 ± 0 . 09 88 . 33 ± 0 . 06 66 . 81 ± 0 . 14 84 . 69 ± 0 . 06 Mime [ 22 ] 84 . 41 ± 0 . 06 87 . 73 ± 0 . 06 93 . 89 ± 0 . 08 48 . 24 ± 0 . 15 81 . 76 ± 0 . 11 64 . 33 ± 0 . 21 76 . 75 ± 0 . 11 FedAvg [ 4 ] 83 . 57 ± 0 . 04 86 . 89 ± 0 . 05 93 . 31 ± 0 . 08 37 . 79 ± 0 . 19 88 . 27 ± 0 . 15 66 . 59 ± 0 . 09 53 . 31 ± 0 . 12 Fig . 2 . ( a ) – ( c ) : Accuracy comparison for HierAdMo under different settings of τ and π when CNN is trained on MNIST . ( d ) : Accuracy comparison for HierAdMo for large N = 100 when CNN is trained on MNIST . ( e ) – ( g ) : Accuracy comparison under 3 - class ( e ) , 6 - class ( f ) , and 9 - class ( g ) non - i . i . d . data when CNN is trained on MNIST . ( h ) and ( l ) : Total training time comparison to reach 0 . 95 accuracy under settings 1 and 2 when CNN is trained on MNIST . The time to reach 0 . 95 accuracy is labeled in the legends . ( h ) : Setting 1 γ = 0 . 5 , γ (cid:2) = 0 . 5 , τ = 20 ( two - tier ) or τ = 10 , π = 2 ( three - tier ) . ( l ) : Setting 2 γ = 0 . 5 , γ (cid:2) = 0 . 5 , τ = 40 ( two - tier ) or τ = 20 , π = 2 ( three - tier ) . ( i ) – ( k ) : Accuracy comparison of adaptive γ (cid:2) with exhausted enumeration of γ (cid:2) for γ = 0 . 3 , 0 . 6 , 0 . 9 at T = 5000 when CNN is trained on CIFAR10 . We divide the benchmarks into four categories : 1 three - tier FL with momentum but without momentum adaption ( HierAdMo - R ) , 2 three - tier FL without momentum ( Hier - FAVG and CFL ) , 3 two - tier FL with momentum ( Fast - SlowMo , FedADC , FedNAG , FedMom , SlowMo , and Mime ) , and 4 two - tier FL without momentum ( FedAvg ) . Please note for presentation convenience , we use “ > ” to indicate “is better than” . The observations are as follows : • HierAdMo > all benchmarks . This conﬁrms that applying momentum on both worker - level and edge - level with momentum adaption in three - tier architecture achieves the best performance . • HierAdMo > 1 . This veriﬁes that embedding the adap - tive momentum method achieves the better convergence performance . It also demonstrates the tighter convergence upper bound in Theorem 5 . • 1 > 2 and 3 > 4 . This veriﬁes that the momentum can accelerate the convergence in both three - tier archi - tecture and two - tier architecture . • 1 > 3 and 2 > 4 . This veriﬁes that the three - tier architecture outperforms the two - tier architecture . The additional edge aggregation decreases the effect of data 505 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . heterogeneity among workers under the same edge node , so as to improve the performance . • For DNN , 2 > 3 . For convex model and CNN , 3 > 2 . This shows that for complicated models , the three - tier architecture plays a more signiﬁcant role to accelerate the convergence while for less complicated models , the momentum plays a more signiﬁcant role to accelerate the convergence . We also compare the training accuracy when 100 workers participate in the training to showcase the cross - siloed FL [ 40 ] ( typically up to one hundred participants ) . The results in Fig . 2 ( d ) exhibit a similar trend to those presented in Table II . In Fig . 2 ( e ) – ( g ) , we evaluate the effects of different levels of non - i . i . d . data distribution when CNN is trained on MNIST . The curves illustrate the training accuracy . To quantify the level of non - i . i . d . data distribution , we explicitly assign only x < 10 out of 10 classes of data to each worker . The class of data is randomly allocated to each worker . Smaller x represents higher level of non - i . i . d . setting . Although higher level of non - i . i . d . setting decreases convergence performance for all algorithms , it is worth noting that HierAdMo consistently outperforms the benchmark algorithms . C . Effects of Hyper - parameters In Fig . 2 ( a ) – ( c ) , we evaluate the effects of τ and π , and their joint effects . The curves represent the accuracy when CNN is trained on MNIST . We set T = 1000 , γ = 0 . 5 . There are 16 workers and 4 edge nodes with each edge node serving 4 workers . When π and τ are ﬁxed in Fig . 2 ( a ) and Fig . 2 ( b ) respectively , we observe that larger τ or π lowers the performance . This observation matches our expectation and veriﬁes the result of Theorem 4 , which indicates that the larger τ or π leads to larger convergence upper bound . When τ · π ( the product of τ and π ) is ﬁxed in Fig . 2 ( c ) , we observe smaller τ ( corresponding to larger π ) leads to better performance . This shows that more frequent edge aggregation is more effective compared to more frequent cloud aggregation . In Fig . 2 ( i ) – ( k ) , we evaluate adaptive γ (cid:2) ( when HierAdMo is employed ) compared to the exhausted enumeration of γ (cid:2) ( when HierAdMo - R is employed ) . We train CNN on CIFAR10 and the setting is τ = 20 , π = 2 , T = 5000 . There are 4 workers and 2 edge nodes with each edge node serving 2 workers . The green bar represents the training accuracy achieved with the exhausted enumeration of γ (cid:2) , while the orange bar represents the training accuracy obtained with adaptive γ (cid:2) . We observe adaptive γ (cid:2) performs well , which is comparable to the best ﬁxed γ (cid:2) , despite the best ﬁxed γ (cid:2) varying in different situations ( 0 . 9 , 0 . 8 , and 0 . 2 in the three situations respectively ) . This veriﬁes that HierAdMo has the capability to self - tune γ (cid:2) for each edge node , achieving optimal or near - optimal performance in various scenarios . D . Trace - driven Simulation We emulate the real - world three - tier hierarchical FL envi - ronment and compare the total training time with an expected learning accuracy ( 0 . 95 ) on HierAdMo and other benchmarks when CNN in trained on MNIST . We train the model in the GPU tower server and keep the trace of the sequence of iterations . We use real - world devices as workers ( one laptop with Intel Core i3 M380 CPU , three Android phones : Nubia z17s with Qualcomm Snapdragon 835 CPU , Realme GT Neo with MTK Dimensity 1200 CPU , Redmi K30 Ultra with MTK Dimensity 1000 + CPU ) to sample worker computation delays . We use Macbook Pro 2018 with Intel Core i7 - 8750H CPU as the edge node to sample the edge computation delays . The GPU tower server is regarded as the cloud server and the cloud computation delays are sampled on it . The workers are connected to a HUAWEI Honor router X2 + with 5GHz WiFi . The edge node is also connected to the router with a wired cable ( 1 Gbps Ethernet ) . The router is then connected to the public Internet . The cloud server is connected to the Internet via another ISP’s access network . The worker communication delays are sampled between the workers and the edge node . The edge communication delays are sampled between the edge node and the server via the public Internet . Please note that for two - tier FL algorithms , since the workers directly com - municate with the cloud , the worker - to - cloud communication delays are sampled as the delays from the devices to the server . We use the trace of the sequence of iterations and the sampled delays to ﬁgure out the overall delays as if the training process is conducted in real - world three - tier or two - tier FL environment . In Fig . 2 ( h ) and ( l ) , we compare the total training time of HierAdMo and benchmarks when CNN is trained on MNIST . The experiment is conducted under two settings : 1 γ = 0 . 5 , γ (cid:2) = 0 . 5 , τ = 20 ( two - tier ) or τ = 10 , π = 2 ( three - tier ) and 2 γ = 0 . 5 , γ (cid:2) = 0 . 5 , τ = 40 ( two - tier ) or τ = 20 , π = 2 ( three - tier ) . There are 4 workers and 2 edge nodes with each edge node serving 2 workers ( three - tier algorithm ) . There are 4 workers directly served by the cloud ( two - tier algorithm ) . We observe that to reach the accuracy 0 . 95 , HierAdMo spends 360 . 97s under setting 1 and 351 . 59s under setting 2 . In comparison , other benchmarks spend 558 . 94s – 1544 . 76s under setting 1 and 459 . 48s – 1532 . 65s under setting 2 to reach the same accuracy level . This demonstrates HierAdMo can greatly reduce the overall training time and improve the training speed by 1 . 30x – 4 . 36x compared to the benchmark algorithms . VI . C ONCLUSION In this paper , we propose HieAdMo , a three - tier hierarchical FL algorithm with adaptive momentum . We provide conver - gence analysis of HierAdMo with a convergence rate of O (cid:2) 1 T (cid:3) for smooth non - convex problems under non - i . i . d . data . We also prove the tighter convergence upper bound of HierAdMo compared with HierAdMo - R . We experimentally analyze the effects of hyper - parameters and beneﬁts of adaptive momen - tum over non - adaptation . It empirically shows that HierAdMo consistently improves the convergence performance compared with benchmarks and achieves the optimal or near - optimal convergence performance compared with HierAdMo - R under a wide range of settings . 506 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . A PPENDIX A P ROOF OF T HEOREM 1 A . Equivalent Update First , we deﬁne v ti , (cid:2) (cid:2) y ti , (cid:2) − y t − 1 i , (cid:2) with v 0 i , (cid:2) = 0 for all i , (cid:6) . We can obtain x t − 1 i , (cid:2) = y t − 1 i , (cid:2) + γ v t − 1 i , (cid:2) . The worker momentum / model update in Lines 5 – 6 in Algorithm 1 can then be equivalently written as v ti , (cid:2) ← γ v t − 1 i , (cid:2) − η ∇ F i , (cid:2) ( x t − 1 i , (cid:2) ) , ( 24 ) x ti , (cid:2) ← x t − 1 i , (cid:2) + γ v ti , (cid:2) − η ∇ F i , (cid:2) ( x t − 1 i , (cid:2) ) . ( 25 ) The aggregated value v t(cid:2) and the intermediate value x t(cid:2) − can also be equivalently written as v t(cid:2) ← C (cid:2) (cid:5) i = 1 D i , (cid:2) D (cid:2) v ti , (cid:2) , x t(cid:2) − ← C (cid:2) (cid:5) i = 1 D i , (cid:2) D (cid:2) x ti , (cid:2) . ( 26 ) Similarly , the edge and cloud virtual updates ( 10 ) – ( 11 ) and ( 14 ) – ( 15 ) can be equivalently written as v t [ k ] , (cid:2) ← γ v t − 1 [ k ] , (cid:2) − η ∇ F (cid:2) ( x t − 1 [ k ] , (cid:2) ) , x t [ k ] , (cid:2) ← x t − 1 [ k ] , (cid:2) + γ v t [ k ] , (cid:2) − η ∇ F (cid:2) ( x t − 1 [ k ] , (cid:2) ) , ( 27 ) v t { p } ← γ v t − 1 { p } − η ∇ F ( x t − 1 { p } ) , x t { p } ← x t − 1 { p } + γ v t { p } − η ∇ F ( x t − 1 { p } ) . ( 28 ) We employ the above equivalent update format ( 24 ) – ( 28 ) to complete the proof in the rest of the Appendix . B . Constant Deﬁnition We deﬁne the constants as follows , which are more conve - niently used in the rest of the Appendix . A (cid:2) ( 1 + ηβ ) ( 1 + γ ) + (cid:7) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , B (cid:2) ( 1 + ηβ ) ( 1 + γ ) − (cid:7) ( 1 + ηβ ) 2 ( 1 + γ ) 2 − 4 γ ( 1 + ηβ ) 2 γ , I (cid:2) γA + A − 1 ( A − B ) ( γA − 1 ) , J (cid:2) γB + B − 1 ( A − B ) ( 1 − γB ) , U (cid:2) 1 + ηβ + ηβγ γ − B A − B = A − 1 A − B , V (cid:2) A − 1 + ηβ + ηβγ γ A − B = 1 − B A − B . C . Derivation of Theorem 1 Since Theorem 1 focuses on a speciﬁc edge node (cid:6) with C (cid:2) connected workers . It is equivalent to consider a two - tier FL system , where edge node (cid:6) represents the aggre - gator . We can see that ( 24 ) – ( 27 ) are equivalent to equa - tions ( 4 ) – ( 7 ) , ( 13 ) , and ( 14 ) in [ 21 ] . Therefore , we re - place v i , w i , v , w , v [ k ] , w [ k ] , F i , F , D i , D , δ i , δ , and N in [ 21 ] with v i , (cid:2) , x i , (cid:2) , v (cid:2) , x (cid:2) − , v [ k ] , (cid:2) , x [ k ] , (cid:2) , F i , (cid:2) , F (cid:2) , D i , (cid:2) , D (cid:2) , δ i , (cid:2) , δ (cid:2) , and C (cid:2) in this paper respectively . According to [ 21 , Theorem 1 ] , we can directly complete the derivation of Theorem 1 . A PPENDIX B P ROOF OF T HEOREM 2 Based on the edge momentum update rules in Lines 12 – 13 in Algorithm 1 , and ( 25 ) we have x kτ(cid:2) + − x kτ(cid:2) − = γ (cid:2) (cid:8) x kτ(cid:2) − − x ( k − 1 ) τ (cid:2) − (cid:9) = γ (cid:2) kτ − 1 (cid:2) t = ( k − 1 ) τ (cid:10) x t + 1 (cid:2) − − x t(cid:2) − (cid:11) = γ (cid:2) kτ − 1 (cid:2) t = ( k − 1 ) τ C (cid:2) (cid:2) i = 1 D i , (cid:2) D (cid:2) (cid:10) x t + 1 i , (cid:2) − x ti , (cid:2) (cid:11) = γ (cid:2) kτ − 1 (cid:2) t = ( k − 1 ) τ C (cid:2) (cid:2) i = 1 D i , (cid:2) D (cid:2) (cid:10) γ 2 v ti , (cid:2) − η ( γ + 1 ) ∇ F i , (cid:2) ( x ti , (cid:2) ) (cid:11) , ( 29 ) and we deﬁne μ (cid:2) max p ∈ [ 1 , P ] , ∀ t , (cid:2) , i (cid:14) (cid:6) γ ( v t { p } ) (cid:6) (cid:6) η ∇ F ( x t { p } ) (cid:6) , (cid:6) γ ( v ti , (cid:2) ) (cid:6) (cid:6) η ∇ F i , (cid:2) ( x ti , (cid:2) ) (cid:6) (cid:15) . ( 30 ) Because F i , (cid:2) ( · ) is ρ - Lipschitz , and according to [ 43 , Lecture 2 , Lemma 1 ] , we have (cid:6)∇ F i , (cid:2) ( · ) (cid:6) 2 ≤ ρ 2 . Therefore , based on the deﬁnition of μ and ( 29 ) , we can derive (cid:16)(cid:16) x kτ(cid:2) + − x kτ(cid:2) − (cid:16)(cid:16) ≤ γ (cid:2) kτ − 1 (cid:5) t = ( k − 1 ) τ C (cid:2) (cid:5) i = 1 D i , (cid:2) D (cid:2) (cid:16)(cid:16) γ 2 v ti , (cid:2) − η ( γ + 1 ) ∇ F i , (cid:2) ( x ti , (cid:2) ) (cid:16)(cid:16) ≤ γ (cid:2) kτ − 1 (cid:5) t = ( k − 1 ) τ C (cid:2) (cid:5) i = 1 D i , (cid:2) D (cid:2) ( γμη + η ( γ + 1 ) ) ρ = γ (cid:2) τρη ( γμ + γ + 1 ) . ( 31 ) We complete the proof of Theorem 2 . A PPENDIX C P ROOF OF T HEOREM 3 First , we deﬁne edge virtual update which is meaningful in cloud interval { p } as y t { p } , (cid:2) and x t { p } , (cid:2) . The value synchro - nization and edge virtual update on { p } are conducted as y ( p − 1 ) τπ { p } , (cid:2) ← y ( p − 1 ) τπ , ( 32 ) x ( p − 1 ) τπ { p } , (cid:2) ← x ( p − 1 ) τπ , ( 33 ) when t = ( p − 1 ) τπ , and y t { p } , (cid:2) ← x t − 1 { p } , (cid:2) − η ∇ F (cid:2) ( x t − 1 { p } , (cid:2) ) , ( 34 ) x t { p } , (cid:2) ← y t { p } , (cid:2) + γ ( y t { p } , (cid:2) − y t − 1 { p } , (cid:2) ) , ( 35 ) when p ∈ ( ( p − 1 ) τπ , pτπ ] . According to Theorem 1 , we have proved the gap between intermediate worker update on the edge (cid:4) C (cid:2) i = 1 D i , (cid:2) D (cid:2) x ti , (cid:2) and edge virtual update x t [ k ] , (cid:2) . Equivalently , the gap between the intermediate edge virtual update on the cloud (cid:4) L(cid:2) = 1 D (cid:2) D x t { p } , (cid:2) and the cloud virtual update x t { p } can be derived as the same way as Theorem 1 . The only difference is the gradient divergence . The edge - level gradient divergence is δ (cid:2) and the cloud - level gradient diver - gence is δ . Therefore , for any cloud interval { p } , ∀ t ∈ [ ( p − 1 ) τπ , pτπ ] , ∀ (cid:6) ∈ L , we have (cid:16)(cid:16)(cid:16)(cid:4) L(cid:2) = 1 D (cid:2) D x t { p } , (cid:2) − x t { p } (cid:16)(cid:16)(cid:16) ≤ h ( t − ( p − 1 ) τπ , δ ) . At the end of cloud interval { p } , when 507 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . t = pτπ , we have (cid:16)(cid:16)(cid:16)(cid:4) L(cid:2) = 1 D (cid:2) D x pτπ { p } , (cid:2) − x pτπ { p } (cid:16)(cid:16)(cid:16) ≤ h ( τπ , δ ) . Based on the deﬁnition of x pτπ [ pπ ] in Theorem 3 and the deﬁnition of x pτπ { p } , (cid:2) , we obtain (cid:16)(cid:16)(cid:16) x pτπ [ pπ ] − (cid:4) L(cid:2) = 1 D (cid:2) D x pτπ { p } , (cid:2) (cid:16)(cid:16)(cid:16) ≤ (cid:4) L(cid:2) = 1 D (cid:2) D (cid:16)(cid:16)(cid:16) x pτπ [ pπ ] , (cid:2) − x pτπ { p } , (cid:2) (cid:16)(cid:16)(cid:16) ≤ π (cid:4) L(cid:2) = 1 D (cid:2) D ( h ( τ , δ (cid:2) ) + s ( τ ) ) . Combining above two inequalities , we complete the proof of Theorem 3 . A PPENDIX D P ROOF OF T HEOREM 4 For convenience , we deﬁne c { p } ( t ) (cid:2) F ( x t { p } ) − F ( x ∗ ) for a given cloud interval { p } , where t ∈ [ ( p − 1 ) τπ , pτπ ] . We also deﬁne the following constants in this section . ω (cid:2) min p ∈ [ 1 , P ] , t ∈ { p } 1 (cid:6) x t { p } − x ∗ (cid:6) 2 , σ (cid:2) min p ∈ [ 1 , P ] , t 1 , t 2 ∈ { p } (cid:6)∇ F ( x t 1 { p } ) (cid:6) (cid:6)∇ F ( x t 2 { p } ) (cid:6) , ( 36 ) α (cid:2) η ( γ + 1 ) (cid:12) 1 − βη ( γ + 1 ) 2 (cid:13) − βη 2 γ 2 μ 2 2 − ηγμ ( 1 − βη ( γ + 1 ) ) . ( 37 ) According to the convergence lower bound of any gradient descent methods given in [ 44 , Theorem 3 . 14 ] , we always have c { p } ( t ) > 0 for any t and p . Then we derive the upper bound of c { p } ( t + 1 ) − c { p } ( t ) , where t ∈ [ ( p − 1 ) τπ , pτπ − 1 ] . Because F ( · ) is β - smooth , based on [ 44 , Lemma 3 . 4 ] , we have c { p } ( t + 1 ) − c { p } ( t ) = F ( x t + 1 { p } ) − F ( x t { p } ) ≤(cid:5)∇ F ( x t { p } ) , x t + 1 { p } − x t { p } (cid:6) + β 2 (cid:7) x t + 1 { p } − x t { p } (cid:7) 2 = γ (cid:5)∇ F ( x t { p } ) , v t + 1 { p } (cid:6) − η (cid:7)∇ F ( x t { p } ) (cid:7) 2 + β 2 (cid:7) γ v t + 1 { p } − η ∇ F ( x t { p } ) (cid:7) 2 ( a ) = − η ( γ + 1 ) (cid:12) 1 − βη ( γ + 1 ) 2 (cid:13) (cid:7)∇ F ( x t { p } ) (cid:7) 2 + βγ 4 2 (cid:7) v t { p } (cid:7) 2 + γ 2 ( 1 − βη ( γ + 1 ) ) (cid:5)∇ F ( x t { p } ) , v t { p } (cid:6) ( b ) ≤ (cid:12) − η ( γ + 1 ) (cid:12) 1 − βη ( γ + 1 ) 2 (cid:13) + βη 2 γ 2 μ 2 2 + ηγμ ( 1 − βη ( γ + 1 ) ) ) (cid:7)∇ F ( x t { p } ) (cid:7) 2 , ( 38 ) where ( a ) is replacing v t + 1 { p } by ( 28 ) and rearranging the formula ; ( b ) is because (cid:6) γ v t { p } (cid:6) ≤ μ (cid:6) η ∇ F ( x t { p } ) (cid:6) with the deﬁnition of μ . According to Cauchy - Schwarz inequality , we can obtain (cid:11)∇ F ( x t { p } ) , v t { p } (cid:12) ≤ (cid:6)∇ F ( x t { p } ) (cid:6)(cid:6) v t { p } (cid:6) ≤ μηγ (cid:6)∇ F ( x t { p } ) (cid:6) 2 . According to the deﬁnition of α , and Con - dition ( 2 . 1 ) of Theorem 4 with h ( τ , δ (cid:2) ) ≥ 0 , h ( τπ , δ ) ≥ 0 ( 39 ) which can be directly proved by [ 21 , Appendix C ] , we have α > 0 . Then from ( 38 ) , we have c { p } ( t + 1 ) ≤ c { p } ( t ) − α (cid:6)∇ F ( x t { p } ) (cid:6) 2 . ( 40 ) Because F ( · ) is ρ - Lipschitz , and according to [ 43 , Lec - ture 2 , Lemma 1 ] , there exists a point x t 2 { p } such that F ( x t { p } ) − F ( x ∗ ) = (cid:11)∇ F ( x t 2 { p } ) , x t { p } − x ∗ (cid:12) . Hence , by Cauchy - Schwarz inequality , we have c { p } ( t ) = F ( x t { p } ) − F ( x ∗ ) ≤ (cid:6)∇ F ( x t 2 { p } ) (cid:6)(cid:6) x t { p } − x ∗ (cid:6) . Based on the deﬁnition of σ , and replacing t with t 1 , we have (cid:6)∇ F ( x t { p } ) (cid:6) ≥ σ (cid:6)∇ F ( x t 2 { p } ) (cid:6) . Thus , (cid:6)∇ F ( x t { p } ) (cid:6) ≥ σ (cid:6)∇ F ( x t 2 { p } ) (cid:6) ≥ σc { p } ( t ) (cid:5) x t { p } − x ∗ (cid:5) . Substituting above inequality into ( 40 ) , and noting ω ≤ 1 (cid:5) x t { p } − x ∗ (cid:5) 2 by the deﬁnition of ω , we get c { p } ( t + 1 ) ≤ c { p } ( t ) − ασ 2 c { p } ( t ) 2 (cid:5) x t { p } − x ∗ (cid:5) 2 ≤ c { p } ( t ) − ωασ 2 c { p } ( t ) 2 . Because α > 0 , c { p } ( t ) > 0 , and ( 40 ) , we have 0 < c { p } ( t + 1 ) ≤ c { p } ( t ) . Dividing both sides by c { p } ( t + 1 ) c { p } ( t ) , we get 1 c { p } ( t ) ≤ 1 c { p } ( t + 1 ) − ωασ 2 c { p } ( t ) c { p } ( t + 1 ) . We note that c { p } ( t ) c { p } ( t + 1 ) ≥ 1 . Thus , 1 c { p } ( t + 1 ) − 1 c { p } ( t ) ≥ ωασ 2 c { p } ( t ) c { p } ( t + 1 ) ≥ ωασ 2 . Summing up the above inequality by t ∈ [ ( p − 1 ) τπ , pτπ − 1 ] , we have 1 c { p } ( pτπ ) − 1 c { p } ( ( p − 1 ) τπ ) = (cid:4) pτπ − 1 t = ( p − 1 ) τπ (cid:6) 1 c { p } ( t + 1 ) − 1 c { p } ( t ) (cid:7) ≥ (cid:4) pτπ − 1 t = ( p − 1 ) τπ ωασ 2 = τπωασ 2 . Then , we sum up the above inequality by p ∈ [ 1 , P ] , after rearranging the left - hand side and noting that T = Pτπ , we can get P (cid:2) p = 1 (cid:12) 1 c { p } ( pτπ ) − 1 c { p } ( ( p − 1 ) τπ ) (cid:13) = 1 c { p } ( T ) − 1 c { 1 } ( 0 ) − P − 1 (cid:2) p = 1 (cid:12) 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) (cid:13) ≥ Pτπωασ 2 = Tωασ 2 . ( 41 ) Following ( 41 ) , we note that 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) = c { p } ( pτπ ) − c { p + 1 } ( pτπ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ { p + 1 } ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = F ( x pτπ { p } ) − F ( x pτπ [ pπ ] ) + (cid:8) F ( x pτπ [ pπ ] ) − F ( x pτπ ) (cid:9) c { p } ( pτπ ) c { p + 1 } ( pτπ ) ( a ) ≥ − ρ (cid:4) L(cid:2) = 1 D (cid:2) D ( h ( τ , δ (cid:2) ) + s ( τ ) ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) ( b ) + − ρ (cid:8) h ( τπ , δ ) + π (cid:4) L(cid:2) = 1 D (cid:2) D ( h ( τ , δ (cid:2) ) + s ( τ ) ) (cid:9) c { p } ( pτπ ) c { p + 1 } ( pτπ ) = − ρj ( τ , π , δ (cid:2) , δ ) c { p } ( pτπ ) c { p + 1 } ( pτπ ) , ( 42 ) where ( a ) is because of combining Theorem 1 and Theorem 2 ; ( b ) is because of Theorem 3 . From ( 40 ) , we can get F ( x t { p } ) ≥ F ( x t + 1 { p } ) for any t ∈ [ ( p − 1 ) τπ , pτπ ) . Recalling Condition ( 2 . 2 ) in Theorem 4 , where F ( x { p } ( pτπ ) ) − F ( x ∗ ) ≥ ε for all p , we can obtain c { p } ( t ) = F ( x t { p } ) − F ( x ∗ ) ≥ ε for all t ∈ [ ( p − 1 ) τπ , pτπ ] and p . Thus , c { p } ( pτπ ) c { p + 1 } ( pτπ ) ≥ ε 2 . Based on ( 39 ) , substituting above inequalities into ( 42 ) , we obtain 1 c { p + 1 } ( pτπ ) − 1 c { p } ( pτπ ) ≥ − ρj ( τ , π , δ (cid:2) , δ ) ε 2 . Substituting the above inequality into ( 41 ) and rearrange , we get 1 c { p } ( T ) − 1 c { 1 } ( 0 ) ≥ Tωασ 2 − ( P − 1 ) ρj ( τ , π , δ (cid:2) , δ ) ε 2 . ( 43 ) 508 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . Recalling Condition ( 2 . 3 ) in Theorem 4 , where F ( x T ) − F ( x ∗ ) ≥ ε , and noting that c { p } ( T ) ≥ ε , we get ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ ε 2 . Thus , 1 F ( x T ) − F ( x ∗ ) − 1 c { p } ( T ) = c { p } ( T ) − ( F ( x T ) − F ( x ∗ ) ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) = F ( x T { p } ) − F ( x T ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ − ρj ( τ , π , δ (cid:2) , δ ) ( F ( x T ) − F ( x ∗ ) ) c { p } ( T ) ≥ − ρj ( τ , π , δ (cid:2) , δ ) ε 2 , ( 44 ) where the ﬁrst inequality follows the same method to prove ( 42 ) . Combining ( 43 ) with ( 44 ) , we get 1 F ( x T ) − F ( x ∗ ) − 1 c { 1 } ( 0 ) ≥ Tωασ 2 − P ρj ( τ , π , δ (cid:2) , δ ) ε 2 = Tωασ 2 − T ρj ( τ , π , δ (cid:2) , δ ) τπε 2 = T (cid:6) ωασ 2 − ρj ( τ , π , δ (cid:2) , δ ) τπε 2 (cid:7) . Noting that c { 1 } ( 0 ) = F ( x 0 { 1 } ) − F ( x ∗ ) > 0 , the above inequality can be expressed as 1 F ( x T ) − F ( x ∗ ) ≥ T (cid:6) ωασ 2 − ρj ( τ , π , δ (cid:2) , δ ) τπε 2 (cid:7) . Recalling Condi - tion ( 2 . 1 ) in Theorem 4 , where ωασ 2 − ρj ( τ , π , δ (cid:2) , δ ) τπε 2 > 0 , we obtain that the right - hand side of above inequality is greater than zero . Therefore , taking the reciprocal of the above inequality , we ﬁnally complete the proof of Theorem 4 . A PPENDIX E P ROOF OF T HEOREM 5 Since we do not know the actual γ (cid:2) for each edge aggre - gation , we ﬁrst calculate the probability density function of γ (cid:2) . Then we calculate the expected mean and variance of γ (cid:2) to further analyze the expected mean of ﬁnal bound of HierAdMo and HierAdMo - R . Please note that the same proof process holds for other distributions . Here , we use the uniform distribution as an example . We assume cos θ k , (cid:2) in ( 6 ) follows uniform distribution cos θ k , (cid:2) ∼ U ( − 1 , 1 ) . ( 45 ) Similarly , we also assume the choice of ﬁxed γ (cid:2) that is denoted by ˜ γ (cid:2) follows uniform distribution ˜ γ (cid:2) ∼ U ( 0 , 1 ) . ( 46 ) Then , we can obtain the probability density function ( PDF ) p ( · ) for cos θ k , (cid:2) is p ( cos θ k , (cid:2) ) = ⎧⎨ ⎩ 1 2 , if − 1 ≤ cos θ k , (cid:2) ≤ 1 , 0 , otherwise . ( 47 ) Similarly , p ( ˜ γ (cid:2) ) = (cid:14) 1 , if 0 ≤ ˜ γ (cid:2) < 1 , 0 , otherwise . ( 48 ) We are now ready to derive the expected mean E ( · ) and variance D ( · ) . Based on ( 47 ) and ( 7 ) , we have E ( γ (cid:2) ) = 14 and D ( γ (cid:2) ) = 548 . Based on ( 48 ) , we have E ( ˜ γ (cid:2) ) = 12 and D ( ˜ γ (cid:2) ) = 1 12 . Then , according to Chebyshev’s Theorem , when K → ∞ , we have ¯ γ (cid:2) P → 14 , ¯˜ γ (cid:2) P → 12 . Based on Theorem 2 , we can obtain that smaller γ (cid:2) leads to smaller s ( τ ) and we ﬁnish the proof of tighter bound of Theorem 2 . We note that tighter bound of Theorem 2 leads to tighter bound of F ( x T ) − F ( x ∗ ) , and therefore Theorem 5 is proved . R EFERENCES [ 1 ] D . Lu and Q . Weng , “A survey of image classiﬁcation methods and tech - niques for improving classiﬁcation performance , ” International journal of Remote sensing , vol . 28 , no . 5 , pp . 823 – 870 , 2007 . [ 2 ] J . E . Naranjo , C . Gonz´alez , R . Garc´ıa , and etc . , “Power - steering control architecture for automatic driving , ” IEEE transactions on intelligent transportation systems , vol . 6 , no . 4 , pp . 406 – 415 , 2005 . [ 3 ] D . Yu and L . Deng , Automatic speech recognition . Springer , 2016 , vol . 1 . [ 4 ] B . McMahan , E . Moore , D . Ramage , and etc . , “Communication - efﬁcient learning of deep networks from decentralized data , ” in Artiﬁcial Intelli - gence and Statistics . PMLR , 2017 , pp . 1273 – 1282 . [ 5 ] W . Y . B . Lim , N . C . Luong , D . T . Hoang , Y . Jiao , Y . - C . Liang , Q . Yang , D . Niyato , and C . Miao , “Federated learning in mobile edge networks : A comprehensive survey , ” IEEE Communications Surveys & Tutorials , vol . 22 , no . 3 , pp . 2031 – 2063 , 2020 . [ 6 ] M . S . H . Abad , E . Ozfatura , D . Gunduz , and O . Ercetin , “Hierarchical federated learning across heterogeneous cellular networks , ” in ICASSP , 2020 , pp . 8866 – 8870 . [ 7 ] C . Briggs , Z . Fan , and P . Andras , “Federated learning with hierarchical clustering of local updates to improve training on non - iid data , ” in IJCNN , 2020 , pp . 1 – 9 . [ 8 ] L . Liu , J . Zhang , S . Song , and K . B . Letaief , “Hierarchical federated learning with quantization : Convergence analysis and system design , ” IEEE Transactions on Wireless Communications , vol . 22 , no . 1 , pp . 2 – 18 , 2023 . [ 9 ] Y . Yan , T . Yang , Z . Li , Q . Lin , and Y . Yang , “A uniﬁed analysis of stochastic momentum methods for deep learning , ” in IJCAI , 2018 , pp . 2955 – 2961 . [ 10 ] C . Liu and M . Belkin , “Accelerating SGD with momentum for over - parameterized learning , ” in ICLR , 2020 . [ 11 ] S . Vaswani , F . Bach , and M . Schmidt , “Fast and faster convergence of sgd for over - parameterized models and an accelerated perceptron , ” in The 22nd International Conference on Artiﬁcial Intelligence and Statistics . PMLR , 2019 , pp . 1195 – 1204 . [ 12 ] M . Assran and M . Rabbat , “On the convergence of nesterov’s accelerated gradient method in stochastic settings , ” in ICML , 2020 , pp . 410 – 420 . [ 13 ] W . Liu , L . Chen , Y . Chen , and W . Zhang , “Accelerating federated learn - ing via momentum gradient descent , ” IEEE Transactions on Parallel and Distributed Systems , vol . 31 , no . 8 , pp . 1754 – 1766 , 2020 . [ 14 ] H . Yu , R . Jin , and S . Yang , “On the linear speedup analysis of communi - cation efﬁcient momentum sgd for distributed non - convex optimization , ” in ICML . PMLR , 2019 , pp . 7184 – 7193 . [ 15 ] H . Yang , M . Fang , and J . Liu , “Achieving linear speedup with partial worker participation in non - iid federated learning , ” in ICLR , 2021 . [ 16 ] H . Gao , A . Xu , and H . Huang , “On the convergence of communication - efﬁcient local sgd for federated learning , ” in AAAI , vol . 35 , 2021 . [ 17 ] L . Liu , J . Zhang , S . Song , and K . B . Letaief , “Client - edge - cloud hierarchical federated learning , ” in ICC . IEEE , 2020 , pp . 1 – 6 . [ 18 ] Z . Wang , H . Xu , J . Liu , H . Huang , C . Qiao , and Y . Zhao , “Resource - efﬁcient federated learning with hierarchical aggregation in edge com - puting , ” in INFOCOM . IEEE , 2021 , pp . 1 – 10 . [ 19 ] Z . Huo , Q . Yang , B . Gu , L . C . Huang et al . , “Faster on - device training using new federated momentum algorithm , ” arXiv preprint arXiv : 2002 . 02090 , 2020 . [ 20 ] J . Wang , V . Tantia , N . Ballas , and M . Rabbat , “SlowMo : Improving communication - efﬁcient distributed sgd with slow momentum , ” in In - ternational Conference on Learning Representations , 2020 . [ 21 ] Z . Yang , W . Bao , D . Yuan , N . H . Tran , and A . Y . Zomaya , “Federated learning with nesterov accelerated gradient , ” IEEE Transactions on Parallel and Distributed Systems , vol . 33 , no . 12 , pp . 4863 – 4873 , 2022 . [ 22 ] S . P . Karimireddy , M . Jaggi , S . Kale , M . Mohri , S . J . Reddi , S . U . Stich , and A . T . Suresh , “Mime : Mimicking centralized stochastic algorithms in federated learning , ” arXiv preprint arXiv : 2008 . 03606 , 2020 . [ 23 ] Z . Yang , S . Fu , W . Bao , D . Yuan , and A . Y . Zomaya , “FastSlowMo : Federated learning with combined worker and aggregator momenta , ” IEEE Transactions on Artiﬁcial Intelligence , 2022 . [ 24 ] E . Ozfatura , K . Ozfatura , and D . G¨und¨uz , “FedADC : Accelerated feder - ated learning with drift control , ” in 2021 IEEE International Symposium on Information Theory ( ISIT ) . IEEE Press , 2021 , p . 467 – 472 . 509 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply . [ 25 ] Y . Lecun , L . Bottou , Y . Bengio , and P . Haffner , “Gradient - based learning applied to document recognition , ” Proceedings of the IEEE , vol . 86 , no . 11 , pp . 2278 – 2324 , 1998 . [ 26 ] A . Krizhevsky et al . , “Learning multiple layers of features from tiny images , ” N / A , 2009 . [ 27 ] T . Moon and T . Ryffel , Pytorch - Tiny - ImageNet , jun 2020 . [ Online ] . Available : https : / / github . com / tjmoon0104 / pytorch - tiny - imagenet [ 28 ] D . Anguita , A . Ghio , L . Oneto , X . Parra Perez , and J . L . Reyes Ortiz , “A public domain dataset for human activity recognition using smart - phones , ” in Proceedings of the 21th ESANN , 2013 , pp . 437 – 442 . [ 29 ] yeggasd , A . Trask , and froessler , FL on MNIST using a CNN , 2021 . [ Online ] . Available : https : / / notebook . community / OpenMined / PySyft / examples / tutorials / Part - 6 - Federated - Learning - on - MNIST - using - a - CNN [ 30 ] S . Gross , S . Chintala , N . Hug , L . Yeager , and E . R . etc . , Pytorch - VGG , may 2021 . [ Online ] . Available : https : / / github . com / pytorch / vision / blob / master / torchvision / models / vgg . py [ 31 ] B . T . Polyak , “Some methods of speeding up the convergence of iter - ation methods , ” USSR Computational Mathematics and Mathematical Physics , vol . 4 , no . 5 , pp . 1 – 17 , 1964 . [ 32 ] S . Ruder , “An overview of gradient descent optimization algorithms , ” arXiv preprint arXiv : 1609 . 04747 , 2016 . [ 33 ] G . Goh , “Why momentum really works , ” Distill , 2017 . [ Online ] . Available : http : / / distill . pub / 2017 / momentum [ 34 ] Y . Nesterov , “A method for unconstrained convex minimization problem with the rate of convergence o ( 1 / k2 ) , ” Doklady ANSSSR ( translated as Soviet . Math . Docl . ) , vol . 269 , pp . 543 – 547 , 1983 . [ 35 ] A . Xu and H . Huang , “Coordinating momenta for cross - silo federated learning , ” in Proceedings of the AAAI Conference on Artiﬁcial Intelli - gence , vol . 36 , no . 8 , 2022 , pp . 8735 – 8743 . [ 36 ] J . Wang , S . Wang , R . - R . Chen , and M . Ji , “Demystifying why local aggregation helps : Convergence analysis of hierarchical sgd , ” in Pro - ceedings of the AAAI Conference on Artiﬁcial Intelligence , 2022 . [ 37 ] T . Castiglia , A . Das , and S . Patterson , “Multi - level local sgd for heterogeneous hierarchical networks , ” arXiv preprint arXiv : 2007 . 13819 , 2020 . [ 38 ] B . Xu , W . Xia , W . Wen , P . Liu , H . Zhao , and H . Zhu , “Adaptive hier - archical federated learning over wireless networks , ” IEEE Transactions on Vehicular Technology , vol . 71 , no . 2 , pp . 2070 – 2083 , 2022 . [ 39 ] Y . Deng , F . Lyu , J . Ren , Y . Zhang , Y . Zhou , Y . Zhang , and Y . Yang , “Share : Shaping data distribution at edge for communication - efﬁcient hierarchical federated learning , ” in 41th ICDCS , 2021 , pp . 24 – 34 . [ 40 ] P . Kairouz , H . B . McMahan , B . Avent , A . Bellet , M . Bennis , A . N . Bhagoji , K . Bonawitz , Z . Charles , G . Cormode , R . Cummings et al . , “Advances and open problems in federated learning , ” Foundations and Trends® in Machine Learning , vol . 14 , no . 1 – 2 , pp . 1 – 210 , 2021 . [ 41 ] S . Wang , T . Tuor , T . Salonidis , K . K . Leung , C . Makaya , T . He , and K . Chan , “Adaptive federated learning in resource constrained edge computing systems , ” IEEE JSAC , vol . 37 , no . 6 , pp . 1205 – 1221 , 2019 . [ 42 ] J . Deng , W . Dong , R . Socher , L . - J . Li , K . Li , and L . Fei - Fei , “Imagenet : A large - scale hierarchical image database , ” in 2009 IEEE conference on computer vision and pattern recognition . Ieee , 2009 , pp . 248 – 255 . [ 43 ] I . Mitliagkas and J . Gallego , “Ift 6085 : Theoretical principles for deep learning , ” in University of Montreal . University of Montreal , 2021 . [ Online ] . Available : http : / / mitliagkas . github . io / ift6085 - dl - theory - class / [ 44 ] S . Bubeck , “Convex optimization : Algorithms and complexity , ” arXiv preprint arXiv : 1405 . 4980 , 2014 . 510 Authorized licensed use limited to the terms of the applicable license agreement with IEEE . Restrictions apply .