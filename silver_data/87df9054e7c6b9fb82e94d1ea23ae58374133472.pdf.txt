Sharpest possible clustering bounds using robust random graph analysis Judith Brugman , Johan S . H . van Leeuwaarden and Clara Stegehuis Complex network theory crucially depends on the assumptions made about the degree distribu - tion , while ﬁtting degree distributions to network data is challenging , in particular for scale - free networks with power - law degrees . We present a robust assessment of complex networks that does not depend on the entire degree distribution , but only on its mean , range and dispersion : summary statistics that are easy to obtain for most real - world networks . By solving several semi - inﬁnite linear programs , we obtain tight ( the sharpest possible ) bounds for correlation and clustering measures , for all networks with degree distributions that share the same summary statistics . We identify var - ious extremal random graphs that attain these tight bounds as the graphs with speciﬁc three - point degree distributions . We leverage the tight bounds to obtain robust laws that explain how degree - degree correlations and local clustering evolve as function of node degrees and network size . These robust laws indicate that power - law networks with diverging variance are among the most extreme networks in terms of correlation and clustering , building further theoretical foundation for widely reported scale - free network phenomena such as correlation and clustering decay . I . INTRODUCTION Degree heterogeneity drives many complex network properties , with the spread of a virus over a network as a striking example . In homogeneous networks , when dif - ferences between node connectivity are relatively small , classical network theory says an epidemic can arise when the average number of secondary infections caused by a single infected individual , R , exceeds one . In scale - free networks with high degree ﬂuctuations , on the other hand , this is not a good predictor , as individuals who are infected early on may be diﬀerent from the average individual . Indeed , these individuals typically have more contacts so that an epidemic can develop even if R is close to zero . A virus then spreads extremely quickly and can hardly be contained . Many real - world networks , in fact , often have extremely heterogeneous degrees that can be approximated with power - laws , so that the proportion of nodes having k neighbors scales as k − τ with exponent τ between 2 and 3 [ 17 , 26 , 46 ] . Power - law degrees imply various intriguing scale - free network properties such as the absence of an epidemic threshold for τ < 3 [ 24 , 35 ] , ultra - small distances [ 32 ] and eﬃcient embedding meth - ods [ 3 ] . Because of this degree heterogeneity , the analysis of such networks is complex . Network properties such as the friendship paradox , and more generally the connections between nodes with vastly diﬀerent degrees , are studied in network theory in the form of so - called degree - degree correlations and clustering . Degree - degree correlations measure correlation between the degrees of two connected nodes , often captured in terms of a ( k ) , the average de - gree of a neighbor of a degree - k node . By clustering we mean the creation of triangular connections ( triadic clo - sure ) , quantiﬁed in terms of c ( k ) , the probability that two neighbors of a degree - k node are neighbors themselves . In uncorrelated networks the a ( k ) and c ( k ) are indepen - dent of k . However , the majority of real - world networks , and scale - free networks in particular , have a ( k ) and c ( k ) functions that decay in k , ﬁrst observed in technological networks such as the Internet [ 34 , 39 ] . Figure 1 shows the same fall - oﬀ for a social network : YouTube users as ver - tices , and edges indicating friendships between them [ 29 ] . When a ( k ) decreases in k , the network is said to be disassortative , so that high - degree vertices typically con - nect to low - degree vertices . When c ( k ) decreases in k , this may indicate the presence of hierarchy . A hierar - chical topology arises , for example , when the rare high - degree nodes together form a backbone , and the low - degree nodes are located in clusters of low - degree nodes that are connected to one of the high - degree nodes . These core peripheries are found in complex networks created by both humans and nature [ 18 ] . This view of a hierarchi - cal network explains both the negative degree - degree cor - relations , because most low - degree nodes are connected to a single high - degree node , and the clustering fall - oﬀ , because the core periphery mainly consists of triadic clo - sures between low - degree communities while high - degree nodes rarely participate in triangles and communities . Network features such as decaying degree correlations are broadly studied through random graphs , mathemati - cally tractable models that can generate random samples of a graph in which nodes have i . i . d . degrees [ 2 , 20 , 21 , 23 , 43 ] . Random graph models take the degree distri - bution as input . Conditional on the degree distribution , random graph properties such as average distance and clustering can be characterized and tested against mea - surements from real - world network data with the same degree distribution . Motivated by the wide range of examples of networks with heavy - tailed degrees , the power - law distribution has become a popular choice as an input degree dis - tribution for random graph models . Fitting a power law to real - world data , however , is statistically challeng - ing [ 8 , 13 , 47 ] . For small values , a power law is usually not a good ﬁt . For this reason , lower bounds for the power - laws or additional slowly - varying functions are of - ten introduced , but these form extra functions that need a r X i v : 2 2 08 . 13532v2 [ phy s i c s . s o c - ph ] 10 N ov 2022 2 10 0 10 1 10 2 10 3 10 4 10 2 10 3 k a ( k ) ( a ) 10 1 10 2 10 3 10 4 10 − 3 10 − 2 10 − 1 k c ( k ) ( b ) FIG . 1 . a ) a ( k ) and b ) c ( k ) for the YouTube social network to be ﬁtted as well . Larger values of the power law also present challenges . Most real - world data sets only follow a power law up to some maximal degree , which is often modeled by an exponential cutoﬀ [ 15 , 30 , 31 ] . Real - world networks are ﬁnite by deﬁnition , while a power law allows inﬁnitely large values . An inherent disadvantage of network theory that rests on random graphs is the dependency on precise statisti - cal assumptions about the degree distributions . Network theory should not be overly sensitive to the assumed de - gree distribution , especially when the assumption is hard to justify statistically . For power laws for instance , the tail exponent τ implies vastly diﬀerent network proper - ties . One reason for this is the variance of the degree dis - tribution . When the number of nodes n becomes large , the variance grows to inﬁnity for τ < 3 , while the vari - ance remains ﬁnite for τ > 3 . This diﬀerence in variance growth crucially inﬂuences the network structure and its degree - degree correlations [ 42 , 48 ] . To overcome this sensitivity of network null models to precise statistical assumptions on the presence of power - laws or other speciﬁc degree distributions , we here char - acterize degree correlations and clustering in random graphs that only require partial information about the degree distribution . Inspired by the complicated assess - ment of power laws , we assume only the mean , dispersion and cutoﬀ of the degree distribution are known . Here we consider two measures of dispersion : the variance and the mean absolute deviation ( MAD ) of the degree distribu - tion . The MAD is an alternative to variance for measur - ing dispersion around the mean , and may be more appro - priate in case of heavy tails . Indeed , MAD can deal with distributions that do not possess a ﬁnite variance , in par - ticular the class of power - law distribution with τ ∈ ( 2 , 3 ) , for which MAD remains ﬁnite while variance becomes in - ﬁnite in the large - network limit when n → ∞ . We will establish the maximal correlation and max - imal clustering that can be achieved by all degree dis - tributions that share the same mean , cutoﬀ and disper - sion . By constructing and solving optimization problems , we ﬁnd the extremal degree distributions that maximize the degree - degree correlation and clustering . These opti - mization problems take the form max P ∈P E P [ graph property ] , ( 1 ) where P is the set that contains all degree distributions that comply with the limited information , such as the mean , cutoﬀ and dispersion . Hence , within the set P we ﬁnd the degree distribution P ∗ that maximizes the expected graph property . We will refer to the random graph with the extremal degree distribution that attains the maximum as the extremal random graph . We solve optimization problems as in ( 1 ) for the hidden - variable model [ 6 , 12 ] , a random graph model that generates graphs with degrees that approximately follow some given distribution . The optimization problems in this paper give rise to semi - inﬁnite linear programs and can be solved using methods from distributionally robust optimization . Using a primal - dual approach we can solve these semi - inﬁnite linear programs in closed form , and ﬁnd the precise description of the degree distribution P ∗ that attains the largest expected graph property . Since this distribution is by deﬁnition contained in the set P , the bound E P [ graph property ] ≤ E P ∗ [ graph property ] , ∀ P ∈ P . ( 2 ) is the best possible ( tight ) bound for all degree distribu - tions that share the same summary statistics as is P . Distributionally robust optimization ﬁnds applications in many domains [ 1 , 16 , 37 ] , but applications in the area of network science are rare . In fact , we are only aware of two papers that apply distributionally robust opti - mization to study complex networks . The ﬁrst paper investigates the maximal possible subgraph counts un - der a restrictive cutoﬀ scheme that creates uncorrelated networks [ 45 ] . The second paper provides a distribution - ally robust model for the inﬂuence maximization problem where the inﬂuence diﬀusion is adversarially adapted to the choice of seed set [ 10 ] . Here the authors aim to de - tect a seed set whose worst - case expected inﬂuence is maximized , and show that this diﬀers from the standard model in which inﬂuence is assumed to diﬀuse indepen - dently across the diﬀerent edges . We term the largely unexplored approach taken in this paper distributionally robust random graph analysis , referring to the combination of classical random graph analysis and the optimization framework that only con - ditions on partial distributional information . This view on random graphs trades precise results that hold for a given degree distribution for robust statements that hold for classes of degree distributions . Such robust results ﬁt well with the search for universal properties of complex networks . Here are the main contributions of this paper : ( i ) For all degree distributions with a given mean , vari - ance and cutoﬀ , we obtain the maximal degree - degree correlations and local clustering . We show that these bounds for a ( k ) and c ( k ) decay in k , as observed in most real - world networks and random graph models . ( ii ) We show that the maximal values of a ( k ) and c ( k ) are often attained by uncorrelated graphs . In par - 3 ticular , the sharpest possible bounds for a ( k ) and c ( k ) for all degree distributions with the same mean and variance are attained by uncorrelated degrees distributions , as long as it is possible to create such uncorrelated distributions with the given mean and variance . ( iii ) We compare the extremal graph models that pro - vide the highest correlations and clustering to ex - isting results for power - law random graphs . While power - laws are often thought of as degree distri - butions that lead to extreme behavior , the power - laws are not the degree distributions that possess the largest possible values of a ( k ) and c ( k ) when τ > 2 . ( iv ) We provide a method to detect whether any given real - world data set can be modeled by hidden - variable models for properties of interest . We show that for several real - world data sets , no possible hidden - variable model can model the particular real - world data set . We introduce the hidden - variable model and assump - tions on the degree distribution in Section II . We then solve the maximization problem that ﬁnds the extremal random graph that generates the maximal degree - degree correlation in Section III . The scaling laws for cluster - ing as function of the network size are presented in Sec - tion IV , and in Section V we do this for clique counts . In Section VI we obtain results for the setting when the MAD instead of the variance is used as dispersion mea - sure . In Section VII we compare the robust bounds ob - tained in earlier sections with existing results for scale - free networks with power - law degrees , and with data from real - world networks . II . RANDOM GRAPHS AND HIDDEN ( RANDOM ) VARIABLES Our analysis of degree - degree correlations and cluster - ing will be based on the hidden - variable model , a random graph model in which every vertex i ∈ [ n ] has a weight h i , and edges are formed between pairs of nodes with a probability that depends on both weights . More specif - ically , every pair of vertices is connected independently with probability p ( h i , h j ) = min (cid:16) h i h j h 2 s , 1 (cid:17) = min (cid:16) h i h j µn , 1 (cid:17) , ( 3 ) where µ denotes the average weight , and h s is the struc - tural cutoﬀ set to √ µn throughout this paper , in line with its typical choice for power - law networks [ 4 , 6 , 9 , 14 ] . This choice of cutoﬀ ensures that the weight of a vertex is close to its degree [ 4 , 44 ] . The structural cutoﬀ de - scribes the maximal degree of vertices that are not prone to degree - degree correlations [ 6 ] . As soon as the degree of a vertex becomes larger than the structural cutoﬀ , it is forced to connect to lower degree vertices , as only few high degree vertices can be present while keeping the av - erage degree ﬁxed . The natural cutoﬀ describes the constraint on the largest possible network degree , or the largest possible weight , h c . In many real - world networks as well as in networks generated from power - law degrees , the largest observed degree is much larger than the structural cutoﬀ of √ µn . For example , when the degrees of the vertices follow a power - law distribution with degree - exponent τ , the largest degree scales as n 1 / ( τ − 1 ) . This means that the network contains vertices that are prone to degree - degree correlations and connection probabilities become non - convex , which makes the network analysis techni - cally more challenging . The hidden - variable model has several properties that make it amenable to analytical analysis . First of all , when the connection probabilities are chosen suitably , the weight h and the degree k of a vertex are similar with high probability . Indeed , the expected degree d j of vertex j given its weight , satisﬁes E [ d j | h j ] = (cid:88) j (cid:54) = i min (cid:16) h i h j µn , 1 (cid:17) ≈ (cid:88) i (cid:54) = j h i h j µn ≈ h j . ( 4 ) To be more precise , when h (cid:29) 1 , then k = h ( 1 + o ( 1 ) ) [ 20 ] . This makes it possible to interchange weights and degrees , which is convenient as the connection prob - abilities are deﬁned in terms of weights . Secondly , when all hidden variables are assigned , most network statistics of interest can be computed as a function of the hidden variables . For example , the average degree of all neigh - bors of a vertex with weight h can be written as a ( h ) = 1 h n (cid:88) i = 1 h i min (cid:16) hh i µn , 1 (cid:17) , ( 5 ) where the sum is over all vertices in the network , and multiplies the weight of a vertex with the probability that vertex i connects to the weight - h vertex . The local clustering coeﬃcient denotes the probabil - ity that two randomly chosen neighbors of a vertex with weight h are neighbors themselves . This statistic can again be written as a function of the hidden variables . Formally , c ( h ) = 1 h 2 (cid:88) 1 ≤ i < j ≤ n min (cid:16) hh i µn , 1 (cid:17) min (cid:16) hh j µn , 1 (cid:17) min (cid:16) h i h j µn , 1 (cid:17) . ( 6 ) Here the sum is over all pairs of vertices in the network , and the term inside the summation computes the proba - bility that these vertices form a triangle with the weight - h vertex . 4 At ﬁrst sight , degree correlations and clustering and seem unrelated , as the former is deﬁned in terms of edges and the latter in terms of triplets of nodes . Still , intu - itively a ( k ) and c ( k ) are related in the case of hidden - variable models . Indeed , the average neighbor of a ver - tex of a weight k vertex is a ( k ) . The probability that two such vertices connect scales as a ( k ) 2 / n , when a ( k ) is suﬃciently small . This probability can be interpreted as the probability that two ‘average’ neighbors of a weight - k vertex connect . It turns out that this intuitive reasoning provides the correct scaling of c ( k ) in some cases . That is , c ( k ) ∼ a ( k ) 2 / n [ 42 ] . When studying real - world data sets , we can only ob - serve ¯ c ( k ) and ¯ a ( k ) , the local clustering coeﬃcient and average degree of the neigbors of a degree - k vertex , rather than a weight - h vertex . Still , the property of the hidden - variable model that degrees and weights are close makes the diﬀerence between these two statistics small in the large - network limit [ 20 ] . In traditional hidden - variable models , the weights h 1 , . . . , h n are assumed independent and following some distribution P . The natural cutoﬀ can then be calculated from the distribution P . In this paper , however , we only specify partial information about the weight ( i . e . degree ) distribution . We will assume that for the weights we know the minimal value , their maximal value ( the nat - ural cutoﬀ h s ) , the mean and the dispersion , ﬁrst mea - sured in variance and later in terms of mean absolute deviation ( MAD ) . Let h denote a generic weight . We ﬁrst assume that the weights are sampled independently from a distribution such that ( i ) h = h i has support supp ( h ) = [ a , h c ] with −∞ < a ≤ h c < ∞ , ( ii ) E [ h ] = µ and ( iii ) E (cid:2) ( h − µ ) 2 (cid:3) = σ 2 . This deﬁnes the ambiguity set P ( µ , σ 2 ) = { P : supp ( h ) ⊆ [ a , h c ] , E [ h ] = µ , E (cid:2) ( h − µ ) 2 (cid:3) = σ 2 } . ( 7 ) Hence , when we now analyze the hidden - variable model under the assumption that the weight distribution be - longs to P ( µ , σ 2 ) , we perform a distributionally robust analysis of the random graph model . The variance of the degree distribution is often highly aﬀected by the choice of the natural cutoﬀ . In power - law random graphs for example , the variance σ 2 grows as h 3 − τ c . Indeed , for τ ∈ ( 2 , 3 ) , the variance of the weights can be computed as (cid:90) h c 1 x 2 − τ dx − µ 2 ∼ h 3 − τ c . ( 8 ) The MAD on the other hand , always satisﬁes the inequal - ity d < 2 µ . Thus , as long as the average degree is ﬁnite , the MAD will not grow as a function of h c . In the rest of this paper , we will focus on the graph properties mentioned above , and ﬁrst seek for the weight distribution P that solves max P ∈P ( µ , σ 2 ) E P [ graph property ] ( 9 ) with P ( µ , σ 2 ) as in ( 7 ) . This means that we take a distri - butionally robust approach for the input weights of the hidden - variable model under the assumption that their distribution belongs to P ( µ , σ 2 ) . When the graph prop - erty ( 9 ) can be viewed as a convex function of the generic weight random variable h , ( 9 ) is optimized for a speciﬁc distribution with support on three points [ 45 ] . Indeed , due to the convex nature of the function , an optimizer aims to put as much weights on the extremal points a and h c , while still adhering to the constraints on the average weight and its variance . This leads to a speciﬁc three - point distribution with probability mass on a , h c and µ . However , in the setting we now consider with natural cutoﬀ h c > √ µn , the connection probability ( 3 ) is not convex , and therefore most graph properties will also not be convex in the hidden variables . In the next sections , we therefore apply a primal - dual based approach to ﬁnd the distributionally robust graph properties . III . ROBUST DEGREE - DEGREE CORRELATIONS BOUNDS The deﬁnition of a ( h ) in ( 5 ) assumes that the hidden variables are known . Instead , we now assume that all hidden variables are drawn from some probability dis - tribution P , so that the expected value of a ( h ) can be computed as E P [ a ( h ) ] = n h E P (cid:20) h (cid:48) min (cid:16) hh (cid:48) µn , 1 (cid:17)(cid:21) . ( 10 ) We then search for the weight distribution P that solves max P ∈P ( µ , σ 2 ) E P [ a ( h ) ] ( 11 ) with P ( µ , σ 2 ) as in ( 7 ) . Hence , when we now analyze the hidden - variable model under the assumption that the weight distribution belongs to P ( µ , σ 2 ) , we perform a robust analysis for all distributions with a given mean , variance and cutoﬀ . The optimization problem ( 11 ) can be written as max P ( x ) ≥ 0 (cid:90) x g ( x ) d P ( x ) s . t . (cid:90) x x 2 d P ( x ) = µ 2 + σ 2 , (cid:90) x x d P ( x ) = µ , (cid:90) x d P ( x ) = 1 , ( 12 ) where g ( x ) = x min ( hx / ( µn ) , 1 ) . In optimization the - ory , ( 12 ) is called a semi - inﬁnite linear optimization problem ( LP ) . The Richter - Rogosinski Theorem ( see , e . g . , [ 19 , 40 , 41 ] ) says there exists an extremal distribu - tion for problem ( 12 ) with at most three support points . While ﬁnding these points in closed form is typically not possible for general semi - inﬁnite problems , we next show that this is possible for the problem at hand by resorting 5 to the dual problem ; see e . g . [ 22 ] and [ 37 ] . This dual problem of ( 12 ) is given by min λ 1 , λ 2 , λ 3 λ 1 ( µ 2 + σ 2 ) + λ 2 µ + λ 3 s . t . g ( x ) − λ 1 x 2 − λ 2 x − λ 3 ≤ 0 ∀ x ∈ [ a , h c ] , ( 13 ) and aims to ﬁnd a tightest quadratic majorant of g ( x ) that minimizes λ 1 ( µ 2 + σ 2 ) + λ 2 µ + λ 3 . Now g ( x ) has a quadratic part up to min ( µn / h , h c ) , and a linear part . Figure 2 shows that this function has two possible tight - est quadratic majorants . The ﬁrst , F 1 ( x ) , is given by λ 1 = h / ( µn ) , λ 2 = λ 3 = 0 and has objective value h ( µ 2 + σ 2 ) / ( µn ) . The second one , F 2 ( x ) , is given by λ 2 = 1 , λ 1 = λ 3 = 0 , and has objective value µ . Which of the two majorants has the smallest objective value de - pends on h . For low values of h , the ﬁrst majorant gives the lowest objective value , whereas for high values of h , the linear one dominates . The changing point is when h = µ 2 n / ( µ 2 + σ 2 ) . ( 14 ) The next step is to ﬁnd a feasible solution for the pri - mal problem that yields the same objective value as the solution to the dual problem . By weak duality of semi - inﬁnite linear programming , a feasible solution to the dual problem gives a valid upper bound for the optimal primal solution value . A feasible primal solution with an objective value equal to this upper bound would show that strong duality holds . Next , we provide a construc - tive approach , based on the condition of complementary slackness , to ﬁnd such a primal solution . Assume that strong duality holds . The primal and dual objective are then equal for the primal maximizer P ∗ and the dual minimizer ( λ ∗ 1 , λ ∗ 2 , λ ∗ 3 ) , and we can substitute the primal constraints in the dual objective . Hence , we obtain the relation (cid:90) x g ( x ) d P ∗ ( x ) = (cid:90) x λ ∗ 1 x 2 + λ ∗ 2 x + λ ∗ 3 d P ∗ ( x ) . ( 15 ) Since λ 1 ( µ 2 + σ 2 ) + λ 2 µ + λ 3 − g ( x ) ≥ 0 pointwise by weak duality , ( 15 ) implies that P ∗ is supported only on the points where λ ∗ 1 x 2 + λ ∗ 2 x + λ ∗ 3 coincides with g ( x ) . We now show that in both cases , a three - point distribution achieves the dual objective value . In the ﬁrst case , h < µ 2 n / ( µ 2 + σ 2 ) , and the dual objective value is given by h ( µ 2 + σ 2 ) / ( µn ) . Now con - sider the three - point distribution ( for ease of notation , we assume that a = 0 , and denote l = min ( µn / h , h c ) ) on the points 0 , µ , l , so indeed the quadratic majorant and g ( x ) coincide . The probabilities are chosen such that the moment conditions are satisﬁed . We obtain p 0 = σ 2 lµ , p µ = 1 − σ 2 lµ − σ 2 l ( l − µ ) , p l = σ 2 l ( l − µ ) . ( 16 ) This is only a proper probability distribution when σ 2 / ( lµ ) + σ 2 / ( l ( l − µ ) ) ≤ 1 , which can be rewritten as σ 2 + µ 2 ≤ µl . We ﬁrst assume that min ( µn / h , h c ) = 0 µ µn / h 0 h s 0 µn / h 0 h s h g ( x ) F 1 ( x ) F 2 ( x ) FIG . 2 . The two possible tightest majorants for the function g ( x ) µn / h , so we need to check that σ 2 + µ 2 ≤ µ 2 n / h . This follows directly from the assumption on h . When min ( µn / h , h c ) = h c , we should satisfy σ 2 + µ 2 ≤ µh c . This is always the case , as the maximal variance of a pri - mal solution with mean µ is given by the primal solution 1 − p 0 = p h c = µ / h c , giving as variance σ 2 ≤ µh c − µ 2 . This three - point distribution gives as objective value for the primal problem (cid:18) 1 − σ 2 lµ − σ 2 l ( l − µ ) (cid:19) µ 2 h µn + σ 2 l ( l − µ ) l 2 h µn = ( σ 2 + µ 2 ) h µn , ( 17 ) which is equal to the dual objective value . Thus , by duality , this three - point degree distribution generates the extremal random graph for a ( k ) , and the result that for h < µ 2 n / ( µ 2 + σ 2 ) , max P ∈P ∗ ( µ , σ 2 ) E P [ a ( h ) ] = µ 2 + σ 2 µ . ( 18 ) In the second case , h > µ 2 n / ( µ 2 + σ 2 ) , and the dual objective value is given by µ . Here , consider the three - point distribution p 0 = 1 − p µn / h − p h c , p µn / h = h 2 (cid:0) − h c µ + µ 2 + σ 2 (cid:1) µn ( µn − h c h ) , p h c = µ 2 ( h − n ) + hσ 2 h c ( h c h − µn ) . ( 19 ) This is only a proper distribution when σ 2 < h c µ − µ 2 , which always holds by a similar reasoning as for the ﬁrst case . The second condition of σ 2 > h c µ − µ 2 − µn / h 2 ( h c h − µn ) is more involved , and is in fact not nec - essary . Indeed , in ( 19 ) , we chose the third point of the distribution at h c . However , as the tightest majorant in Figure 2 touches on an entire line , it is also possible to choose the third point somewhere else in [ µn / h , h c ] while achieving the same optimal value . Choosing an - other point for the three - point distribution also results in diﬀerent conditions on σ 2 . The lowest such constraint is when p 0 = 1 − p µn / h in ( 19 ) , yielding σ 2 = µ 2 n / h − µ 2 , which is ensured by our assumption on h . Under this 6 three - point distribution , E [ X ] = µ , E [ ( X − µ ) 2 ] = σ 2 and the primal objective value E [ g ( X ) ] = µ . Thus , by duality , the three - point distribution is the variance - based extremal random graph for a ( k ) , giving that for h > µ 2 n / ( µ 2 + σ 2 ) , max P ∈P ( µ , σ 2 ) E P [ a ( h ) ] = µn h . ( 20 ) Notice that the three - point distribution ( 19 ) is not a unique optimum , as the dual function F 2 ( x ) coincides with g ( x ) on the entire interval [ µn / h , h c ] . Therefore , one can construct an arbitrary ( discrete , continuous or mixed ) probability distribution with support on the in - terval [ µn / h , h c ] , as long as the mean and variance condi - tions are satisﬁed . Similarly , the three - point distribution ( 16 ) is also not unique . This yields the following theorem : Theorem III . 1 . When p ( h , h (cid:48) ) = min ( hh (cid:48) / ( µn ) , 1 ) , and h c (cid:28) n , max P ∈P ( µ , σ 2 ) E P [ a ( h ) ] = (cid:40) µ 2 + σ 2 µ h < µ 2 n / ( µ 2 + σ 2 ) µnh h > µ 2 n / ( µ 2 + σ 2 ) . ( 21 ) The theorem distinguishes two regimes : one constant regime for low h , and a decaying regime for high enough h . In the constant regime , vertices are not prone to degree - degree correlations : all vertices have the same av - erage nearest neighbor degree as long as h < µ 2 n / ( µ 2 + σ 2 ) . Furthermore , higher degree variance implies a lower threshold , and hence more vertices that are subject to degree - degree correlations . This is consistent with the intuition that degree - degree correlations arise because high - degree vertices are forced to connect to lower - degree vertices due to the lack of suﬃciently many high - degree vertices . When h c < µ 2 n / ( µ 2 + σ 2 ) , it is possible to create entirely uncorrelated networks . This condition is more general than the often used h c (cid:28) √ n constraint for uncorrelated networks that was found for power - law net - works [ 6 ] , as here we do not assume any speciﬁc weight distribution . IV . ROBUST CLUSTERING BOUNDS We now consider the probability that two randomly chosen neighbors of a vertex of weight h are connected to one another as well , c ( h ) . Again , in ( 6 ) the hidden variables are assumed to be ﬁxed . We assume that the hidden variables are drawn from some distribution P , so that the expected value of c ( h ) becomes E P [ c ( h ) ] = n 2 h 2 E P (cid:20) min (cid:16) hh 1 µn , 1 (cid:17) min (cid:16) hh 2 µn , 1 (cid:17) min (cid:16) h 1 h 2 µn , 1 (cid:17)(cid:21) ( 22 ) Now , instead of optimizing over only one hidden - variable as in ( 12 ) , we need to jointly optimize the weight distri - butions of both neighbors . Under variance - based opti - mization , the optimization problem for c ( h ) correspond - ing to ( 12 ) becomes max P ( x ) ≥ 0 (cid:90) x 1 (cid:90) x 2 g ( x 1 , x 2 ) d P ( x 2 ) d P ( x 1 ) s . t . (cid:90) x x 2 d P ( x ) = µ 2 + σ 2 , (cid:90) x x d P ( x ) = µ , (cid:90) x d P ( x ) = 1 , ( 23 ) where g ( x 1 , x 2 ) = min (cid:16) x 1 x 2 µn , 1 (cid:17) min (cid:16) x 1 h µn , 1 (cid:17) min (cid:16) x 2 h µn , 1 (cid:17) . ( 24 ) It turns out that this optimization problem is diﬃcult to solve , due to the constraint that the two variables x 1 and x 2 are i . i . d . . We therefore instead solve a relaxation of ( 23 ) , where we allow these two variables to be corre - lated , or drawn from diﬀerent distributions . This relaxed problem takes the form max P ( x 1 , x 2 ) ≥ 0 (cid:90) x 1 (cid:90) x 2 g ( x 1 , x 2 ) d P ( x 1 , x 2 ) s . t . (cid:90) x 1 (cid:90) x 2 x 21 x 22 d P ( x 1 , x 2 ) = ( µ 2 + σ 2 ) 2 , (cid:90) x 1 (cid:90) x 2 x 1 x 2 d P ( x 1 , x 2 ) = µ 2 , (cid:90) x 1 (cid:90) x 2 d P ( x 1 , x 2 ) = 1 , ( 25 ) which again is a semi - inﬁnite linear optimization prob - lem . Here , instead of optimizing over a single distribu - tion from which both weights are drawn , we optimize over a joint , symmetric distribution over the weights of the other two vertices involved in a triangle , P ( x 1 , x 2 ) . As a consequence , it is possible that the optimal weight distributions found by solving the relaxed problem are correlated so that the two vertices jointly optimize their weights to make a triangle formation more likely . Inter - estingly , it turns out that this is not the case . In Ap - pendix A 1 , we show that the optimizer of the relaxed optimization problem , that thus allows for correlations and is easier to solve , is in fact an uncorrelated distri - bution , and therefore also the solution of the original optimization problem ( 23 ) . We are able to derive this optimizer , because the dual version of this problem min λ 1 , λ 2 , λ 3 λ 1 ( µ 2 + σ 2 ) 2 + λ 2 µ 2 + λ 3 s . t . g ( x 1 , x 2 ) − λ 1 x 21 x 22 − λ 2 x 1 x 2 − λ 3 ≤ 0 ∀ x 1 , x 2 ∈ [ a , h c ] , is relatively easy to solve . This gives the following theo - rem on the distributionally robust optimizer for c ( h ) : 7 Theorem IV . 1 . When p ( h , h (cid:48) ) = min ( hh (cid:48) / ( µn ) , 1 ) and σ 2 < µ max ( √ µn , µn / h ) − µ 2 , max P ∈P ( µ , σ 2 ) E P [ c ( h ) ] = (cid:40) min (cid:16) ( µ 2 + σ 2 ) 2 µ 3 n , 1 (cid:17) h < µ 2 n / ( µ 2 + σ 2 ) µnh 2 h > µ 2 n / ( µ 2 + σ 2 ) . ( 26 ) This theorem only holds when σ 2 is not too large . We conjecture that when σ 2 is larger than the range pre - scribed by the theorem , a primal - dual gap of ( A2 ) ap - pears , indicating that the optimization problem cannot be solved anymore through primal - dual methods . In - deed , for larger σ 2 , the best dual solution remains fea - sible as it does not depend on σ 2 . However , it is then impossible to construct a probability distribution with the required variance on the set of values where the dual constraints are tight . This suggests that a primal - dual gap is present in that case , as there is no primal feasi - ble solution that satisﬁes complementary slackness . For large σ 2 , this implies that the primal problem has to be solved without the help of the dual problem , which makes the problem signiﬁcantly more challenging . V . ROBUST CLIQUE COUNTS Whereas a ( k ) and c ( k ) measure two - and three - point correlations between nodes , we demonstrate in this sec - tion that robust bounds can also be obtained for network statistics that include more than three nodes . We focus on the number of cliques of size k , denoted as N ( K k ) , and use that E [ N ( K k ) ] = (cid:88) 1 ≤ i 1 < i 2 ··· < i k (cid:89) u < v p ( h i u , h i v ) = (cid:88) 1 ≤ i 1 < i 2 ··· < i k (cid:89) u < v min (cid:16) h i u h i v µn , 1 (cid:17) , ( 27 ) as a clique is present if and only if all possible edges between nodes i 1 , i 2 . . . , i k are present . To establish the variance - based bound on the expected number of cliques , we formulate the multivariate optimization problem max P ( x ) ≥ 0 (cid:90) x 1 · · · (cid:90) x k g ( x 1 , x 2 , . . . , x k ) d P ( x 1 ) . . . d P ( x k ) s . t . (cid:90) x x 2 d P ( x ) = µ 2 + σ 2 , (cid:90) x x d P ( x ) = µ , (cid:90) x d P ( x ) = 1 ( 28 ) with g ( x 1 , . . . , x k ) = (cid:89) 1 ≤ i < j ≤ k min (cid:16) x i x j µn , 1 (cid:17) . As for clustering , this optimization problem appears in - tractable due to the i . i . d . hidden variables . We there - fore again solve a relaxation of ( 28 ) instead that drops the i . i . d . assumption . In Appendix A 1 we solve this re - laxed optimization problem and prove the following ro - bust bounds for clique counts : Theorem V . 1 . When σ 2 ≤ µ ( √ µn − µ ) , k > 3 , and as n → ∞ , max P ∈P ( µ , σ 2 ) E P [ N ( K k ) ] = ( µ 2 + σ 2 ) k µ k k ! ( 1 + o ( 1 ) ) . ( 29 ) Furthermore , for k = 3 , σ 2 ≤ µ ( √ µn − µ ) and for any n , max P ∈P ( µ , σ 2 ) E P [ N ( K k ) ] = ( µ 2 + σ 2 ) k µ k k ! . ( 30 ) This theorem shows that cliques signiﬁcantly increase when the variance grows , as is the case for heavy - tailed weight distributions . The theorem only holds asymptot - ically ( except for k = 3 ) , as we create primal and dual solutions with a small gap between their respective op - timal values that vanishes as n → ∞ . Whereas Theo - rems III . 1and IV . 1 gave exact ( non - asymptotic ) results , here the relaxed optimization method that provided ex - act results for Theorem IV . 1 gives non i . i . d . weight dis - tributions . Thus , this method does not provide exact bounds on clique counts . Instead , we ﬁrst solve the dual problem , and then construct i . i . d . primal weight distri - butions that asymptotically achieve the dual value , and are therefore asymptotically optimal . As in Theorem IV . 1 , the theorem contains a condition on σ 2 . We conjecture that for larger σ 2 , a larger primal - dual gap is present that does not vanish in the large - network limit , so that the optimization problem ( 28 ) can - not be solved through its dual variant , similarly as for Theorem IV . 1 . VI . MAD INSTEAD OF VARIANCE We now turn to a second measure of dispersion : mean absolute deviation . While the variance of a random vari - able can be inﬁnite , the MAD is always bounded by 2 µ , so that even in networks with heavy - tailed degree dis - tributions this quantity remains ﬁnite . For maximizing based on MAD , the ambiguity set now becomes P ( µ , d ) = { P : supp ( h ) ⊆ [ a , h c ] , E [ h ] = µ , E [ | h − µ | ] = d } . ( 31 ) As for the variance - based approach , we then aim to ﬁnd the probability distribution P ∈ P ( µ , d ) that maximizes the network statistics a ( h ) and c ( h ) . We can use the same approach of constructing an optimization problem based on the constraints formed by the ambiguity set and ﬁnding the optimal primal - dual solution . As shown in Appendix A 2 , we obtain the following result for a ( h ) : 8 Theorem VI . 1 . When p ( h , h (cid:48) ) = min ( hh (cid:48) / ( µn ) , 1 ) , and h c → ∞ as n → ∞ and h c (cid:28) n , max P ∈P ( µ , d ) E P [ a ( h ) ] = d 2 µ min (cid:16) h c , µn h (cid:17) ( 1 + o ( 1 ) ) . ( 32 ) This optimal value of a ( h ) is attained by the 3 - point distribution p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( l − µ ) , p l = d 2 ( l − µ ) , ( 33 ) where l = min ( µn / h , h c ) . To obtain results for c ( k ) with MAD as well , we need to solve the optimization problem max P ( x ) ≥ 0 (cid:90) x 1 (cid:90) x 2 g ( x 1 , x 2 ) d P ( x 2 ) d P ( x 1 ) s . t . (cid:90) x | x − µ | d P ( x ) = d , (cid:90) x x d P ( x ) = µ , (cid:90) x d P ( x ) = 1 , ( 34 ) similarly to ( 23 ) . Again , this optimization problem is diﬃcult to solve , due to the constraint that the two vari - ables x 1 and x 2 are i . i . d . . We could solve this problem by proceeding as in Section IV , by writing an unconstrained version of this optimization problem , where we allow the two variables to be non - identical or correlated . However , these techniques then lead to the dual problem min λ 1 , λ 2 , λ 3 λ 1 d 2 + λ 2 µ 2 + λ 3 s . t . g ( x 1 , x 2 ) − λ 1 | x 1 − µ | | x 2 − µ | − λ 2 x 1 x 2 − λ 3 ≤ 0 ∀ x 1 , x 2 ∈ [ a , h c ] . Compared with ( 26 ) , this dual function is no longer quadratic , but a product of absolute values summed with a linear term . The dual then tries to ﬁnd the tightest ma - jorant of this two - dimensional function of the non - convex function g ( x 1 , x 2 ) illustrated in Figure 3 . However , ﬁnd - ing the tightest majorant of λ 1 | x 1 − µ | | x 2 − µ | − λ 2 x 1 x 2 − λ 3 to a general function is not obvious , as it is a function with a kink and diﬀerent behavior near the x and y axes from the line y = x , as illustrated in Figure 4 . This makes it more diﬃcult to ﬁnd the values of λ 1 , λ 2 , λ 3 that ﬁnd this tightest majorant . Therefore , we take a diﬀerent approach instead . We ﬁrst focus on the one - dimensional problem : for ﬁxed x 1 , what is the distribution of x 2 that maximizes g ( x 1 , x 2 ) ? This problem can now be solved by a one - dimensional dual problem , which is easy to solve , similarly as in Sec - tion III . We then take the distribution of the optimal x 2 ( which may depend on the value of x 1 ) , and then opti - mize over the distribution of x 1 as well . Now this iter - ative approach may introduce correlations between the distributions : it is possible that the optimal distribution for x 1 is diﬀerent from , or dependent on , the optimal dis - tribution of x 2 . Still , in some cases it may be true that 2 4 2 4 2 4 6 8 x y FIG . 3 . The function g ( x , y ) . 1 2 3 4 2 4 20 x y FIG . 4 . The function 2 | x − 1 | | y − 1 | + xy . the output of this less restrictive optimization problem gives an i . i . d . distribution of x 1 and x 2 . In that case , this method also solves ( 34 ) . In the case when g ( x 1 , x 2 ) is convex , this is known to be true . Unfortunately , for the case of computing c ( h ) , g ( x 1 , x 2 ) is not convex . And indeed , the iterative optimizer is not always i . i . d . in this case . Still , we show that asymptotically , an i . i . d . opti - mizer achieves the same maximal value of c ( h ) . Applying this iterative method , as shown in Appendix A 2 , leads to the following results : Theorem VI . 2 . When p ( h , h (cid:48) ) = min ( hh (cid:48) / ( µn ) , 1 ) and h c → ∞ as n → ∞ , max P ∈P ( µ , d ) E P [ c ( h ) ] =   1 h (cid:28) √ µn and 2 µ ( 1 − (cid:112) µ / n ) < d < 2 µ ( 1 − h / n ) d 2 4 µ 2 ( 1 + o ( 1 ) ) h (cid:28) √ µn and d < 2 µ ( 1 − (cid:112) µ / n ) d 2 n 4 µh 2 ( 1 + o ( 1 ) ) √ µn (cid:28) h (cid:28) n . ( 35 ) While from the perspective of i . i . d . weight sampling of the random graph it is natural to constrain the two 9 vertices that form a triangle together with the degree - h node to be sampled from the same distribution , in the optimization problem it is also possible to ﬁnd the op - timal pair of correlated distributions over the two nodes that form a triangle together with the weight - h node . In that case , the nodes that form a triangle together with the weight - h node are both sampled from the ambiguity set P ( µ , d ) , so that they still both have mean weight µ and MAD d . However , the weights of the two nodes are now allowed to be correlated . In the proof of Theorem VI . 2 , we show that in the ranges where Theorem VI . 2 is valid , adding correlations between the distributions of h 1 and h 2 does not make a diﬀerence in the scaling for c ( h ) . Indeed , the upper bound of 1 is always valid for c ( h ) , so that lifting the constraint on the i . i . d . distributions of h 1 and h 2 cannot increase the optimal value of c ( h ) . For the second set - ting where √ µn (cid:28) h (cid:28) n , we in fact show in the proof of Theorem VI . 2 that by optimizing c ( h ) over a set of prob - ability distributions on h 1 and h 2 which are allowed to be correlated , we end up with i . i . d . distributions as the optimizer . So here also lifting the constraint on the joint distribution will not increase c ( h ) . Furthermore , in the case h (cid:28) √ µn and d is small , allowing correlations only increases the non - leading order terms , so that asymptot - ically , c ( h ) cannot be increased by allowing correlations between the distributions of h 1 and h 2 . VII . COMPARISON WITH OTHER NETWORKS We now compare the extremal values of a ( k ) and c ( k ) to several synthetic and real network data . a . Erd˝os - R´enyi model . In the Erd˝os - R´enyi model , h i = µ , ∀ i , and all vertices connect with probability µ / n . Thus , σ 2 = 0 and d = 0 . The zero variance and MAD means that there is only one distribution in the ambi - guity sets ( 7 ) and ( 31 ) , which is h i = µ , ∀ i . Therefore , Theorem III . 1 and VI . 1 predict that the maximal value of a ( h ) = µ , which is the exact average weight of a neigh - bor in an Erd˝os - R´enyi random graph , as all vertices have weight µ . Furthermore , Theorem IV . 1 and VI . 2 predict that the maximal value of c ( h ) = µ / n , which is also equal to c ( h ) in an Erd˝os R´enyi model , as the probability that two neighbors of a vertex connect is µ / n , the same as the connection probability for all pairs of vertices . Thus , for Erd˝os - R´enyi random graphs , our bounds are tight . b . Poisson random graph . When the hidden - variables have a Poisson distribution with mean µ , the second moment of the weight distribution is µ + µ 2 . For such networks , hh (cid:48) < µn almost surely for all h (cid:28) n . Thus , ( 10 ) gives E [ a ( h ) ] = 1 + µ . Applying Theorem III . 1 yields that the maximal possible a ( h ) value among all networks with the same mean and variance of the weights also equals 1 + µ . Similarly E [ c ( h ) ] = ( µ + µ 2 ) 2 / ( µ 3 n ) for Poisson random graphs by Equation ( 22 ) , while by Theorem IV . 1 , the maximal value of c ( k ) among all power - law random graphs also equals ( µ + µ 2 ) 2 / ( µ 3 n ) . Thus , Poisson random graphs achieve the maximal bounds for a ( h ) and c ( h ) exactly . When looking at the MAD - based bounds , the picture changes drastically . Indeed , for a Poisson random vari - able with integer mean µ [ 38 ] d = 2 µ µ + 1 e − µ / µ ! , ( 36 ) so that by Theorem VI . 1 the maximal value of a ( h ) scales for low h as h c µ µ e − µ / µ ! , which can be much larger than the achieved value of 1 + µ . For c ( k ) , Theorem VI . 2 yields that for h low , the max - imal value of c ( h ) for random graphs with the same mean and MAD as the Poisson random graph equals ( µ µ e − µ / µ ! ) 2 , which is an n - independent constant , in con - trast to the achieved value of c ( h ) = ( µ + µ 2 ) 2 / ( µ 3 n ) , which decays in n . Thus , our variance - based bounds can be signiﬁcantly lower than the ones based on equal MAD . c . Comparing power - law a ( h ) to extremal a ( h ) . We now turn to random graphs with power - law distributed weights . We ﬁrst compare the maximal scaling of a ( h ) given by Theorem III . 1 to the value of a ( h ) attained by the power - law weight distribution P ( h > x ) = Cx 1 − τ . ( 37 ) When sampling n i . i . d . weights from this distribution , the maximal weight scales as h c = n 1 / ( τ − 1 ) with high probability . In such power - law Chung - Lu models [ 42 , 48 ] , a ( h ) ∼ (cid:40) n ( 3 − τ ) / ( τ − 1 ) h (cid:28) n ( τ − 2 ) / ( τ − 1 ) ( n / h ) 3 − τ h (cid:29) n ( τ − 2 ) / ( τ − 1 ) . ( 38 ) We now investigate how close this value of a ( h ) is to the maximal possible values among all Chung - Lu mod - els with the same mean and variance as the power - law distribution . For power - law distributed weights , σ 2 ∼ n ( 3 − τ ) / ( τ − 1 ) , as derived in ( 8 ) . Thus , Theorem III . 1 yields max P ∈P ( µ , n ( 3 − τ ) / ( τ − 1 ) ) E P [ a ( h ) ] = µ 2 + n ( 3 − τ ) / ( τ − 1 ) µ ∼ n ( 3 − τ ) / ( τ − 1 ) , ( 39 ) when h < n ( 2 τ − 4 ) / ( τ − 1 ) , while max P ∈P ( µ , n ( 3 − τ ) / ( τ − 1 ) ) E P [ a ( h ) ] = µn h , ( 40 ) when h > n ( 2 τ − 4 ) / ( τ − 1 ) For large h , the scaling in ( 38 ) only agrees with the value of µn / h of ( 40 ) for τ = 2 . For low h , the power - law value of a ( h ) of ( 38 ) agrees with the scaling of ( 39 ) for all τ ∈ ( 2 , 3 ) . This indicates that a power - law distribution asymptotically achieves the most extreme values of a ( h ) possible for h small among all random graphs with the same mean and variance of the degree distribution . For larger h , the extremal a ( h ) scaling is only attained for power - law random graphs for 10 τ = 2 . Indeed , Figure 5a illustrates that the variance - based upper bound on a ( k ) is close to the value achieved by a power - law Chung - Lu model when τ ≈ 2 , and that power - law graphs with higher degree - exponents have a closer gap with the maximal possible a ( h ) value . We now again compare a ( h ) of the the power - law dis - tribution ( 38 ) to a matching extremal value , but now the extremal random graph based on a matching mean and MAD . For power - laws [ 45 ] , d = C ( 2 µ 2 − τ − 1 − h 2 − τ c ) ) τ − 2 + Cµ ( − 2 µ 1 − τ + 1 + h 1 − τ c ) ) τ − 1 . ( 41 ) Thus , for τ > 2 , d is approximately constant . Com - paring Theorem VI . 1 where we take d constant and h c = n 1 / ( τ − 1 ) with ( 38 ) shows that max P ∈P ∗ ( µ , d ) E P [ a ( h ) ] = (cid:40) d 2 µ n 1 / ( τ − 1 ) h < µn ( τ − 2 ) / ( τ − 1 ) dn 2 h h > µn ( τ − 2 ) / ( τ − 1 ) . ( 42 ) Comparing this with ( 38 ) , shows that for τ = 2 , the maximal degree - degree correlations among all Chung - Lu random graphs with given MAD , and µ scales the same as for the power - law distribution . Still , Figure 5b shows that the diﬀerences in constants in the change point as well as in the maximal scaling make the power - law a ( h ) to be quite far from the MAD - based bound , even for τ ≈ 2 . Note also that the MAD - based bounds sometimes drops below the actual power - law value for h large . This is because we plot the values of a ( k ) all the way up to h = n , while the ( τ - dependent ) cutoﬀ lies already at h c = n 1 / ( τ − 1 ) , which is close to 400 for τ = 2 . 9 for example . Thus , for all h in [ 1 , h c ] , the MAD - bound is a valid upper bound . d . Comparing power - law c ( h ) to extremal c ( h ) . We now turn to c ( k ) . In power - law Chung - Lu models with cutoﬀ h c = n 1 / ( τ − 1 ) [ 42 ] , c ( h ) ∼    n 2 − τ ln ( n ) h (cid:28) n ( τ − 2 ) / ( τ − 1 ) n 2 − τ ln ( n / h ) n ( τ − 2 ) / ( τ − 1 ) (cid:28) h (cid:28) √ n h 2 τ − 6 n 5 − 2 τ h (cid:29) √ n . ( 43 ) We now compare this scaling to the scaling obtained by Theorem IV . 1 . Using ( 8 ) shows that Theorem IV . 1 pre - dicts that max P ∈P ( µ , n ( 3 − τ ) / ( τ − 1 ) ) E P [ c ( h ) ] ∼ min ( n ( 7 − 3 τ ) / ( τ − 1 ) , 1 ) ( 44 ) for h small , while it scales as n / h 2 for h large . For τ = 2 , this coincides with ( 43 ) . Therefore , Theorem IV . 1 implies that the maximal c ( h ) scaling among all Chung - Lu ran - dom graphs with given σ 2 and µ for h (cid:29) √ n is achieved by the power - law distribution for τ = 2 . However , for h (cid:28) √ n , the power - law distribution does not attain the largest possible value of c ( h ) . Figure 5c illustrates this . The power - law c ( k ) is even in its constant , very close to the variance - based maximal value of c ( k ) . For τ ≈ 2 , the changing point between the constant regime and the decaying regime becomes close , while for larger values of τ , the diﬀerence in changing point is larger . For the MAD - based optimizer , we can obtain similar statements . Theorem VI . 2 predicts that max P ∈P ∗ ( µ , d ) E P [ c ( h ) ] ∼ n / h 2 ( 45 ) for h suﬃciently high . Therefore , Theorem VI . 2 implies that the maximal c ( h ) scaling among all Chung - Lu ran - dom graphs with given MAD , h c and µ for h (cid:29) √ n is achieved by the power - law distribution for τ = 2 . Indeed , Figure 5d shows that the MAD - based optimal value of c ( k ) is close to the power - law based one for τ ≈ 2 , but that the bound can be far oﬀ otherwise . Furthermore , note that for τ = 2 . 1 the MAD - based optimal bound drops below the power - law achieved value . This is be - cause of ﬁnite - size eﬀects that are not included in Theo - rem VI . 2 . e . Comparing power - law clique counts to extremal clique counts . We now compare the maximal amount of cliques predicted by Theorem V . 1 to the amount of cliques achieved by a power - law random graph . When 2 < τ < 3 , under a cutoﬀ at b = √ µn , the expected number of cliques in a power - law random graph with degree - exponent τ equals [ 25 , Eq . ( 1 . 7 ) ] E pl [ N K k ] ≈ n k / 2 ( 3 − τ ) µ k / 2 ( 1 − τ ) k ! (cid:18) C k − τ (cid:19) k . ( 46 ) Now under a cutoﬀ at b = √ µn , for power - law random graphs , σ 2 = C 3 − τ √ µn 3 − τ . Plugging in σ 2 = C 3 − τ √ µn 3 − τ from the power - law distribution into ( 29 ) yields max P ∈P ∗ ( µ , σ 2 ) E P [ N K k ] = n k / 2 ( 3 − τ ) µ k / 2 ( 1 − τ ) k ! (cid:18) C 3 − τ (cid:19) k . ( 47 ) This agrees in terms of scaling in n and µ with the power - law Chung - Lu number of cliques scaling of ( 46 ) . So this proves that for variance - based clique optimization , power - laws contain the most number of cliques in terms of scaling in n when using a cutoﬀ at b = √ µn . Still , the leading constant in ( 47 ) is higher than the one in ( 46 ) for k > 3 , so that the extremal random graph for cliques still achieves a higher total number of cliques in the leading order constant . When h c = n 1 / ( τ − 1 ) , then E pl [ K k ] ∼ n k ( 3 − τ ) / 2 . Fur - thermore , in that setting , σ 2 ∼ n ( 3 − τ ) / ( τ − 1 ) , while µ does not grow in n . Therefore , in this case , Theo - rem V . 1 does not apply for τ < 7 / 3 , as σ 2 ≥ √ n when τ < 7 / 3 , so that the condition of Theorem V . 1 does not apply . For τ > 7 / 3 , plugging in the power - law value of σ 2 ∼ n ( 3 − τ ) / ( τ − 1 ) into Theorem V . 1 yields max P ∈P ∗ ( µ , σ 2 ) E P [ N ( K k ) ] ∼ n k ( 3 − τ ) / ( τ − 1 ) , ( 48 ) which is larger than the power - law scaling of n k ( 3 − τ ) / 2 . Therefore , power - law random graphs are not the graphs that contain the most cliques among all random graphs with the same mean and variance . 11 10 0 10 1 10 2 10 3 10 4 10 0 10 1 10 2 10 3 h a ( h ) τ = 2 . 1 τ = 2 . 5 τ = 2 . 9 ( a ) 10 0 10 1 10 2 10 3 10 4 10 0 10 1 10 2 10 3 10 4 h a ( h ) τ = 2 . 1 τ = 2 . 5 τ = 2 . 9 ( b ) 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 − 5 10 − 3 10 − 1 h c ( h ) τ = 2 . 1 τ = 2 . 5 τ = 2 . 9 ( c ) 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 − 5 10 − 3 10 − 1 h c ( h ) τ = 2 . 1 τ = 2 . 5 τ = 2 . 9 ( d ) FIG . 5 . Maximal scaling compared with power - law with the same parameters ( dashed line ) for n = 10 5 on a ) a ( k ) , variance basis ( solid line ) , b ) a ( k ) , MAD - basis ( solid line ) c ) c ( k ) , variance basis ( solid line ) , d ) c ( k ) , MAD - basis ( solid line ) . f . Data We now apply our bounds for a ( k ) and c ( k ) to three real - world network data sets . Figure 6 compares the variance - based upper bound of Theorem III . 1 with empirical observations . For all data sets , the true value of a ( k ) exceeds the variance - based maximizer at some point . The MAD - based maximizer on the other hand , remains an upper bound for a ( k ) in almost all data sets . This highlights the importance of the right choice of com - parison model : The hidden - variable model with ﬁtted variance cannot explain the degree - degree correlations in these data sets , while the same model with ﬁtted MAD can . Figure 7 shows for three real - world networks c ( k ) and the variance - and MAD - based bounds . The c ( k ) - values of the Gowalla data set are close to , or below the MAD and the variance - based optimizer , respectively . This suggests that these data sets can be suitably modeled by some hidden - variable model that matches the c ( k ) distribution of this data set . For the two other data sets on the other hand , the value of c ( k ) in the data sets is higher than can be achieved by any hidden - variable model . Therefore , no hidden - variable model is able to match these data sets in terms of c ( k ) , which is likely caused by the locally - tree like nature of the hidden - variable model . VIII . CONCLUSIONS AND DISCUSSION Our robust network perspective , in terms of ambiguity and partial information on the degree distribution , comes with substantial mathematical challenges . We created an optimization framework for identifying , within some am - biguity set , the extreme degree distribution that gener - ates the upper bound for the degree - degree correlations and clustering . We therefore had to combine probabilis - tic models ( random graphs ) with optimization models ( stochastic programs ) . For successfully applying our ro - bust perspective it is crucial to solve a given stochastic program in closed form . Here we distinguish between 1D programs such as the semi - inﬁnite LP for the ex - pected degree - degree correlation , and 2D programs such as for the expected clustering . For the 1D programs with both variance and MAD information , we were able to apply standard primal - dual techniques , solving the dual problem with the tightest majorant , and indeed ﬁnding a closed - form extremal distribution . The 2D programs for expected clustering proved to be more challenging , being semi - inﬁnite programs with two i . i . d . random variables . This i . i . d . assumption creates nonlinear conditions that prohibits the usage of stan - dard primal - dual techniques . For variance information , we therefore applied a relaxation technique that replaces 12 10 0 10 1 10 2 10 3 0 200 400 600 800 k a ( k ) data σ 2 d ( a ) 10 0 10 1 10 2 0 20 40 60 80 k a ( k ) data σ 2 d ( b ) 10 0 10 1 10 2 10 3 10 4 0 2 , 000 4 , 000 6 , 000 8 , 000 k a ( k ) data σ 2 d ( c ) FIG . 6 . a ( k ) and MAD and variance - based bounds for 3 real - world networks a ) Enron email network [ 27 ] , b ) Pretty Good Privacy network [ 5 ] , c ) Gowalla social network [ 11 ] . 10 0 10 1 10 2 10 3 0 0 . 2 0 . 4 0 . 6 k c ( k ) data σ 2 d ( a ) 10 0 10 1 10 2 0 0 . 2 0 . 4 k c ( k ) data σ 2 d ( b ) 10 0 10 1 10 2 10 3 10 4 0 0 . 1 0 . 2 0 . 3 k c ( k ) data σ 2 d ( c ) FIG . 7 . c ( k ) and MAD and variance - based bounds for 3 real - world networks , a ) Enron email network [ 27 ] , b ) Pretty Good Privacy network [ 5 ] , c ) Gowalla social network [ 11 ] . the original program with its counterpart that allows cor - relations . That relaxed program proved solvable with the primal - dual technique , despite the additional challenges of 2D instead of 1D . Surprisingly , the found extremal joint distribution was a product - form distribution after all , so that we could thus show that the relaxed pro - gram has the same solution as the original program . We showed that similar relaxations for programs of higher di - mensions could be also be used to establish tight ( asymp - totic ) bounds for clique counts . However , such relaxations proved cumbersome , if not intractable , for MAD information . In that case , we opted for a diﬀerent relaxation , in which we solve the 2D pro - gram in two steps : ﬁrst ﬁnding the worst - case distribu - tions of variable 1 , and then , given this worst - case dis - tribution , ﬁnding the worst - case distribution distribution of variable 2 . This relaxation also allows correlation be - tween the random variables , but possibly of a diﬀerent nature . This second type of relaxation turned out to give worst - case distributions that become independent ( product - form ) distributions when the network size grows to inﬁnity . Hence , in this way , we could solve the original 2D MAD program asymptotically for n → ∞ . This paper forms an important step towards a more complete theory of Distributionally Robust Random Graphs ( DRRGs ) . This theory exchanges full informa - tion random graphs with partial information models , for instance regarding the degree distribution . This means that the theory applies to a large class of distributions , and can possibly explain complex network phenomena in a more universal manner , less dependent on the speciﬁc distributional assumptions . Hence , we consider this a robust way of studying complex networks . Here we mention a few open problems and research di - rections . For solving the 2D program we have introduced two relaxations , one for variance and one for MAD . Do both relaxations work for both variance and MAD ? Can we understand when the two relaxations are equivalent , and when they are not ? Do these relaxations also work in higher - dimensional stochastic programs , with more than two i . i . d . random variables ? In this paper we have suc - cessfully applied one such higher - dimensional relaxation for cliques of arbitrary size . Another avenue for future research concerns model extensions . We considered the Chung - Lu - type random graph , a tractable model that can accommodate a wide range of degree sequences . Drawbacks of this model are its locally tree - like nature and the slow convergence to the large - network limit [ 25 ] . This motivates to consider extended models such as random geometric graphs [ 36 ] or generalizations of the popular hyperbolic random graph [ 7 , 28 ] , where the connection probability of two 13 vertices scales as the product of their weights , divided by their distance . What is the maximal value of c ( k ) for given mean and variance on the degrees and given mean and variance on the inter - distances ? This question will lead to a more involved optimization problem with more variables due to the underlying geometry . Such up - per bounds for increasing model complexity could detect what level of complexity is necessary to model a speciﬁc network property correctly . Furthermore , while we investigated robust degree dis - tributions , one can also think of other network proper - ties . For example , in temporal network models one can obtain results for robust edge time - stamps . For hyper - graphs , one can think of robust hyperdegrees , or of ro - bust positions for geometric models . We believe that this framework can provide robust upper bounds on several network properties , and quantify the sensitivity of net - work models to speciﬁc assumptions on their parameters . Finally , it is worthwhile to compare extremal random graphs as studied in this paper with entropy - maximizing random graph ensembles [ 33 ] . While we maximize a given network property ( such as clustering or degree - correlations ) , maximum entropy random graphs maxi - mize the entropy instead , and then compute the value of the property . This calls to investigate for classes of degree distributions and network properties whether extremal random graph and maximum entropy random graphs are comparable or not . ACKNOWLEDGMENTS We thank Dick den Hertog , Wouter van Eekelen and Pieter Kleer for various thought exchanges about the relaxations introduced in this paper for solving semi - inﬁnite linear programs , and robust optimization in gen - eral . JB is supported by an NWO Mathematics Clusters grant , JvL is supported by an NWO VICI grant . CS is supported by NWO VENI grant 202 . 001 and NWO M2 grant 0 . 379 . [ 1 ] B . - T . A . and E . Hochman . More bounds on the expecta - tion of a convex function of a random variable . Journal of Applied Probability , 9 ( 4 ) : 803 – 812 , dec 1972 . [ 2 ] G . Bianconi and M . Marsili . Loops of any size and hamilton cycles in random scale - free networks . Jour - nal of Statistical Mechanics : Theory and Experiment , 2005 ( 06 ) : P06005 , 2005 . [ 3 ] T . Blasius , T . Friedrich , A . Krohmer , and S . Laue . Eﬃcient embedding of scale - free graphs in the hyper - bolic plane . IEEE / ACM Transactions on Networking , 26 ( 2 ) : 920 – 933 , 2018 . [ 4 ] M . Bogu˜n´a and R . Pastor - Satorras . Class of correlated random networks with hidden variables . Phys . Rev . E , 68 : 036112 , 2003 . [ 5 ] M . Bogu˜n´a , R . Pastor - Satorras , A . D´ıaz - Guilera , and A . Arenas . Models of social networks based on social distance attachment . Phys . Rev . E , 70 ( 5 ) , 2004 . [ 6 ] M . Bogun´a , R . Pastor - Satorras , and A . Vespignani . Cut - oﬀs and ﬁnite size eﬀects in scale - free networks . The European Physical Journal B , 38 ( 2 ) : 205 – 209 , 2004 . [ 7 ] K . Bringmann , R . Keusch , and J . Lengler . Geometric inhomogeneous random graphs . Theoretical Computer Science , 760 : 35 – 54 , feb 2019 . [ 8 ] A . D . Broido and A . Clauset . Scale - free networks are rare . Nature Communications , 10 ( 1 ) , mar 2019 . [ 9 ] M . Catanzaro , M . Bogu˜n´a , and R . Pastor - Satorras . Gen - eration of uncorrelated random scale - free networks . Phys . Rev . E , 71 : 027103 , Feb 2005 . [ 10 ] L . Chen , D . Padmanabhan , C . C . Lim , and K . Natara - jan . Correlation robust inﬂuence maximization . 34th Conference on Neural Information Processing Systems ( NeurIPS 2020 ) , Vancouver , Canada , Oct . 2020 . [ 11 ] E . Cho , S . A . Myers , and J . Leskovec . Friendship and mobility : user movement in location - based social net - works . In Proceedings of the 17th ACM SIGKDD in - ternational conference on Knowledge discovery and data mining , pages 1082 – 1090 . ACM , 2011 . [ 12 ] F . Chung and L . Lu . The average distances in random graphs with given expected degrees . Proc . Natl . Acad . Sci . USA , 99 ( 25 ) : 15879 – 15882 , 2002 . [ 13 ] A . Clauset , C . R . Shalizi , and M . E . J . Newman . Power - law distributions in empirical data . SIAM Rev . , 51 ( 4 ) : 661 – 703 , 2009 . [ 14 ] P . Colomer - de Simon and M . Bogu˜n´a . Clustering of ran - dom scale - free networks . Phys . Rev . E , 86 : 026120 , 2012 . [ 15 ] H . Ebel , L . - I . Mielsch , and S . Bornholdt . Scale - free topol - ogy of e - mail networks . Physical Review E , 66 ( 3 ) , sep 2002 . [ 16 ] W . van Eekelen , D . den Hertog , and J . S . H . van Leeuwaarden . MAD dispersion measure makes extremal queue analysis simple . 2019 . [ 17 ] M . Faloutsos , P . Faloutsos , and C . Faloutsos . On power - law relationships of the internet topology . In ACM SIG - COMM Computer Communication Review , volume 29 , pages 251 – 262 . ACM , 1999 . [ 18 ] R . J . Gallagher , J . - G . Young , and B . F . Welles . A clar - iﬁed typology of core - periphery structure in networks . Science Advances , 7 ( 12 ) , mar 2021 . [ 19 ] S . Han , M . Tao , U . Topcu , H . Owhadi , and R . M . Mur - ray . Convex optimal uncertainty quantiﬁcation . SIAM Journal on Optimization , 25 ( 3 ) : 1368 – 1387 , 2015 . [ 20 ] R . van der Hofstad , A . J . E . M . Janssen , J . S . H . van Leeuwaarden , and C . Stegehuis . Local clustering in scale - free networks with hidden variables . Phys . Rev . E , 95 ( 2 ) : 022307 , 2017 . [ 21 ] R . van der Hofstad , J . S . H . van Leeuwaarden , and C . Stegehuis . Optimal subgraph structures in scale - free conﬁguration models . The Annals of Applied Probability , 31 ( 2 ) : 501 – 537 , 2021 . [ 22 ] K . Isii . On sharpness of Tchebycheﬀ - type inequali - ties . Annals of the Institute of Statistical Mathematics , 14 ( 1 ) : 185 – 197 , 1962 . [ 23 ] S . Itzkovitz , R . Milo , N . Kashtan , G . Ziv , and U . Alon . Subgraphs in random networks . Physical review E , 14 68 ( 2 ) : 026127 , 2003 . [ 24 ] S . Janson . On percolation in random graphs with given vertex degrees . Electron . J . Probab . , 14 : 86 – 118 , 2009 . [ 25 ] A . J . E . M . Janssen , J . S . H . van Leeuwaarden , and S . Shneer . Counting cliques and cycles in scale - free inhomogeneous random graphs . Journal of Statistical Physics , 175 ( 1 ) : 161 – 184 , feb 2019 . [ 26 ] H . Jeong , B . Tombor , R . Albert , Z . N . Oltvai , and A . - L . Barab´asi . The large - scale organization of metabolic networks . Nature , 407 ( 6804 ) : 651 – 654 , 2000 . [ 27 ] B . Klimt and Y . Yang . Introducing the Enron Corpus . In CEAS , 2004 . [ 28 ] D . Krioukov , F . Papadopoulos , M . Kitsak , A . Vahdat , and M . Bogun´a . Hyperbolic geometry of complex net - works . Phys . Rev . E , 82 ( 3 ) : 036106 , 2010 . [ 29 ] J . Leskovec and A . Krevl . SNAP Datasets : Stanford large network dataset collection . http : / / snap . stanford . edu / data , 2014 . Date of access : 14 / 03 / 2017 . [ 30 ] S . Mossa , M . Barth´el´emy , H . E . Stanley , and L . A . N . Amaral . Truncation of power law behavior in “scale - free” network models due to information ﬁltering . Physical Review Letters , 88 ( 13 ) , mar 2002 . [ 31 ] M . E . J . Newman . The structure of scientiﬁc collabora - tion networks . Proceedings of the National Academy of Sciences , 98 ( 2 ) : 404 – 409 , jan 2001 . [ 32 ] M . E . J . Newman , S . H . Strogatz , and D . J . Watts . Ran - dom graphs with arbitrary degree distributions and their applications . Phys . Rev . E , 64 ( 2 ) : 026118 , 2001 . [ 33 ] J . Park and M . E . J . Newman . Statistical mechanics of networks . Phys . Rev . E , 70 : 066117 , Dec 2004 . [ 34 ] R . Pastor - Satorras , A . V´azquez , and A . Vespignani . Dy - namical and correlation properties of the internet . Phys . Rev . Lett . , 87 : 258701 , 2001 . [ 35 ] R . Pastor - Satorras and A . Vespignani . Epidemic spread - ing in scale - free networks . Phys . Rev . Lett . , 86 ( 14 ) : 3200 , 2001 . [ 36 ] M . Penrose . Random Geometric Graphs . Oxford Univer - sity Press , may 2003 . [ 37 ] I . Popescu . A semideﬁnite programming approach to optimal - moment bounds for convex classes of distribu - tions . Mathematics of Operations Research , 30 ( 3 ) : 632 – 657 , 2005 . [ 38 ] T . A . Ramasubban . The mean diﬀerence and the mean deviation of some discontinuous distributions . Biometrika , 45 ( 3 / 4 ) : 549 , dec 1958 . [ 39 ] E . Ravasz and A . - L . Barab´asi . Hierarchical organization in complex networks . Phys . Rev . E , 67 : 026112 , 2003 . [ 40 ] W . W . Rogosinski . Moments of non - negative mass . Pro - ceedings of the Royal Society of London . Series A . Math - ematical and Physical Sciences , 245 ( 1240 ) : 1 – 27 , 1958 . [ 41 ] A . Shapiro , D . Dentcheva , and A . Ruszczy´nski . Lec - tures on Stochastic Programming : Modeling and Theory . SIAM , 2009 . [ 42 ] C . Stegehuis . Degree correlations in scale - free ran - dom graph models . Journal of Applied Probability , 56 ( 3 ) : 672 – 700 , 2019 . [ 43 ] C . Stegehuis , R . van der Hofstad , and J . S . H . van Leeuwaarden . Variational principle for scale - free network motifs . Scientiﬁc Reports , 9 ( 1 ) : 6762 , 2019 . [ 44 ] C . Stegehuis , R . van der Hofstad , J . S . H . van Leeuwaar - den , and A . J . E . M . Janssen . Clustering spectrum of scale - free networks . Phys . Rev . E , 96 ( 4 ) : 042309 , 2017 . [ 45 ] J . S . H . van Leeuwaarden and C . Stegehuis . Robust sub - graph counting with distribution - free random graph anal - ysis . Physical Review E , 104 ( 4 ) : 044313 , oct 2021 . [ 46 ] A . V´azquez , R . Pastor - Satorras , and A . Vespignani . Large - scale topological and dynamical properties of the internet . Phys . Rev . E , 65 : 066130 , 2002 . [ 47 ] I . Voitalov , P . van der Hoorn , R . van der Hofstad , and D . Krioukov . Scale - free networks well done . Physical Review Research , 1 ( 3 ) : 033034 , 2019 . [ 48 ] D . Yao , P . van der Hoorn , and N . Litvak . Average nearest neighbor degrees in scale - free networks . Internet Mathe - matics , 2018 . Appendix A : Proofs 1 . Proof of the variance - based maximizer for c ( k ) and the number of cliques Proof of Theorem IV . 1 . The optimization problem ( 23 ) is equivalent to max P ( x ) ≥ 0 (cid:90) x 1 (cid:90) x 2 g ( x 1 , x 2 ) d P ( x 2 ) d P ( x 1 ) s . t . (cid:90) x 1 (cid:90) x 2 x 21 x 22 d P ( x 2 ) d P ( x 1 ) = ( µ 2 + σ 2 ) 2 (cid:90) x 1 (cid:90) x 2 x 1 x 2 d P ( x 2 ) d P ( x 1 ) = µ 2 , (cid:90) x 1 (cid:90) x 2 d P ( x 2 ) d P ( x 1 ) = 1 . ( A1 ) We now consider a relaxed version of this optimization problem . Instead of drawing from a single measure P for both x 1 and x 2 , we allow for a dual measure P ( x 1 , x 2 ) , where we only require the product of the means and sec - ond moments to be equal to µ 2 and ( µ 2 + σ 2 ) 2 , respec - tively . We thus drop the i . i . d . assumption for now . This gives the problem max P ( x 1 , x 2 ) ≥ 0 (cid:90) x 1 (cid:90) x 2 g ( x 1 , x 2 ) d P ( x 1 , x 2 ) s . t . (cid:90) x 1 (cid:90) x 2 x 21 x 22 d P ( x 1 , x 2 ) = ( µ 2 + σ 2 ) 2 (cid:90) x 1 (cid:90) x 2 x 1 x 2 d P ( x 1 , x 2 ) = µ 2 , (cid:90) x 1 (cid:90) x 2 d P ( x 1 , x 2 ) = 1 . ( A2 ) The dual problem then becomes min λ 1 , λ 2 , λ 3 λ 1 ( µ 2 + σ 2 ) 2 + λ 2 µ 2 + λ 3 s . t . g ( x 1 , x 2 ) − λ 1 x 21 x 22 − λ 2 x 1 x 2 − λ 3 ≤ 0 ∀ x 1 , x 2 ∈ [ a , h c ] , with g ( x 1 , x 2 ) = min (cid:16) x 1 x 2 µn , 1 (cid:17) min (cid:16) x 1 h µn , 1 (cid:17) min (cid:16) x 2 h µn , 1 (cid:17) . ( A3 ) 15 We will now solve the optimization problem by con - structing a primal and dual solution that achieve the same objective value , and therefore optimize ( A2 ) . Fur - thermore , the constructed optimal probability distribu - tion turns out to be of product form , so that they must also be optimizers of the original , more constrained opti - mization problem ( A1 ) . These primal and dual solutions depend on h , n , µ and σ , in the following cases : Case 1 : h ≤ √ µn . We take the dual solution λ 1 = h 2 / ( µn ) 3 , λ 2 = λ 3 = 0 . This gives as objective value ( µ 2 + σ 2 ) 2 h 2 / ( µn ) 3 . When σ 2 ≤ ( √ µn − µ ) µ , for the primal problem , con - sider the 3 - point distribution p 0 = σ 2 √ µnµ , p µ = 1 − σ 2 µ ( √ µn − µ ) , p √ µn = σ 2 √ µn ( √ µn − µ ) . ( A4 ) This is a proper distribution by the condition on σ 2 . Then , E [ g ( X 1 , X 2 ) ] = h 2 ( µn ) 3 (cid:16) p 2 µ µ 4 + 2 p µ p √ µn µ 2 ( µn ) + p 2 √ µn ( µn ) 2 (cid:17) = h 2 ( µn ) 3 (cid:16) p µ µ 2 + p √ µn µn (cid:17) 2 = h 2 ( µn ) 3 ( µ 2 + σ 2 ) 2 . ( A5 ) Thus , by strong duality , this is the optimizer of ( 23 ) . When σ 2 > ( √ µn − µ ) µ , consider the three - point dis - tribution p 0 = 1 − p √ µn − p µn / h , p √ µn = µ 2 + σ 2 − µn / h · µ √ µn ( √ µn − µn / h ) , p µn / h = µ 2 + σ 2 − √ µn · µ µn / h ( µn / h − √ µn ) . ( A6 ) This is only a proper distribution when σ 2 < ( µn / h − µ ) µ . This three - point distribution gives E [ c ( h ) ] = 1 , so that it achieves the maximum c ( h ) . Therefore , we can imme - diately conclude that this primal solution is optimal . Case 2 : √ µn < h < µ 2 n / ( µ 2 + σ 2 ) . We take as dual solution again λ 1 = h 2 / ( µn ) 3 , λ 2 = λ 3 = 0 . This gives as objective value ( µ 2 + σ 2 ) 2 h 2 / ( µn ) 3 . For the primal problem , consider the 3 - point distribu - tion p 0 = σ 2 µ ( µn / h ) , p µ = 1 − σ 2 µ ( µn / h − µ ) , p µn / h = σ 2 µn / h ( µn / h − µ ) . ( A7 ) Again , this is only a distribution when σ 2 ≤ µ ( µn / h − µ ) , which is ensured by the condition on h . Then , E [ g ( X 1 , X 2 ) ] = h 2 ( µn ) 3 (cid:16) p 2 µ µ 4 + 2 p µ p µn / h µ 2 ( µn ) 2 / h 2 + p 2 µn / h ( µn / h ) 4 (cid:17) = h 2 ( µn ) 3 (cid:16) p µ µ 2 + p µn / h ( µn / h ) 2 (cid:17) 2 = h 2 ( µn ) 3 ( µ 2 + σ 2 ) 2 . ( A8 ) Thus , the primal solution achieves the same value as the dual solution . Therefore , by strong duality , this is the optimizer of ( 23 ) . Case 3 : h ≥ µ 2 n / ( µ 2 + σ 2 ) . We take as dual solution λ 2 = 1 / ( µn ) , λ 1 = λ 3 = 0 . This gives as objective value µ / n . For the primal problem , consider again the 3 - point dis - tribution that is given in ( A6 ) . This is only a proper dis - tribution when σ 2 > ( µn / h − µ ) µ , which is satisﬁed by our condition on σ 2 , and σ 2 ≤ ( √ µn − µ ) µ . Under this three - point distribution , E [ X ] = µ , E [ ( X − µ ) 2 ] = σ 2 and E [ g ( X 1 , X 2 ) ] = µ / n . Thus , by strong duality , this is the optimal solution . Proof of Theorem V . 1 . The relaxed optimization prob - lem corresponding to ( 28 ) gives the dual problem min λ 1 , λ 2 , λ 3 λ 1 ( µ 2 + σ 2 ) k + λ 2 µ k + λ 3 s . t . g ( x 1 , . . . , x k ) − λ 1 x 21 · · · x 2 k − λ 2 x 1 x 2 · · · x k − λ 3 ≤ 0 ∀ x 1 , . . . , x k ∈ [ a , h c ] , with g ( x 1 , . . . , x k ) = (cid:89) 1 ≤ i < j ≤ k min (cid:16) x i x j µn , 1 (cid:17) . ( A9 ) Consider the dual solution λ 1 = 1 / ( µn ) k , giving as ob - jective value ( µ 2 + σ 2 ) k / ( µn ) k . Consider the 3 - point distribution p 0 = 1 − p m − p √ µn , p m = µ √ µn − µ 2 − σ 2 m ( √ µn − m ) , p √ µn = µ 2 + σ 2 − mµ √ µn ( √ µn − m ) , ( A10 ) with m = µ ( 1 + k ) / 4 n ( 3 − k ) / 4 . Note that m < √ µn for k > 3 , and m = µ for k = 3 , and that this is only a proper distribution when σ 2 ≤ µ ( √ µn − µ ) . Now E [ g ( X 1 , . . . , X k ) ] = E (cid:2) X k − 1 1 (cid:3) k ( µn ) k ( k − 1 ) / 2 . ( A11 ) Furthermore , E (cid:2) X k − 1 1 (cid:3) = (cid:16) µ k + 14 n 3 − k 4 (cid:17) k − 2 (cid:0) µ √ µn − µ 2 − σ 2 (cid:1) √ µn − µ k + 14 n 3 − k 4 + √ µn k − 2 (cid:16) µ 2 + σ 2 − µ k + 54 n 34 − k 4 (cid:17) √ µn − µ k + 14 n 3 − k 4 . ( A12 ) 16 Now when k > 3 , then n ( 3 − k ) / 4 = o ( 1 ) . Therefore , for k > 3 , E (cid:2) X k − 1 1 (cid:3) = √ µn k − 2 (cid:0) µ 2 + σ 2 (cid:1) √ µn ( 1 + o ( 1 ) ) . ( A13 ) Thus , also E [ g ( X 1 , . . . , X k ) ] = √ µn ( k − 3 ) k ( µ 2 + σ 2 ) k ( µn ) k ( k − 1 ) / 2 ( 1 + o ( 1 ) ) = ( µ 2 + σ 2 ) k ( µn ) k ( 1 + o ( 1 ) ) , ( A14 ) making the 3 - point distribution asymptotically optimal , as it asymptotically achieves the same value as the dual solution . Furthermore , for k = 3 , E (cid:2) X 2 (cid:3) = µ 2 + σ 2 , by the conditions in P ( µ , σ 2 ) . Thus , for k = 3 , E [ g ( X 1 , . . . , X k ) ] = ( µ 2 + σ 2 ) 3 ( µn ) 3 , ( A15 ) which is the exact same value as the dual objective value . Hence , by strong duality , this is the optimal solution . 2 . Proofs for MAD - based maximizers of a ( k ) and c ( k ) Proof of Theorem VI . 1 . The function h (cid:48) min ( hh (cid:48) µn , 1 ) is piecewise convex in h (cid:48) . In particular , it is quadratic up to l = min ( µn / h , h c ) (cid:29) µ , where it has slope 2 , and it is linear with slope 1 after that . Thus , to optimize over the distribution of h (cid:48) , we need to solve max P ∈P ( µ , d ) (cid:90) x f ( x ) d P ( x ) s . t . (cid:90) x | x − µ | d P ( x ) = d , (cid:90) x x d P ( x ) = µ , (cid:90) x d P ( x ) = 1 , ( A16 ) where f ( x ) = x min ( hx µn , 1 ) . Similarly to the derivation of [ 16 , Eq . ( 6 ) ] , this results in the dual problem min λ 1 , λ 2 , λ 3 λ 1 d + λ 2 µ + λ 3 s . t . f ( x ) − λ 1 | x − µ | − λ 2 x − λ 3 ≤ 0 ∀ x ∈ [ 0 , h s ] . ( A17 ) For simplicity of notation , we assume that a = 0 . Thus , this dual problem aims to ﬁnd the tightest piecewise lin - ear majorant of f ( x ) with a kink at µ that minimizes the objective value . Consider the majorant F 1 ( x ) = hl 2 µn | x − µ | + ( hl 2 µn + hn ) x − hl 2 n . Now F 1 ( x ) has as its objective value hl 2 µnd + ( hl 2 µn + h n ) µ − hl 2 n = µh n + hdl 2 µn . ( A18 ) By weak duality of semi - inﬁnite linear programming , we know that a feasible solution to the dual problem pro - vides us with a valid upper bound for the optimal primal solution value . Thus , we now ﬁnd a feasible primal so - lution with an objective value equal to this upper bound results to achieve strong duality . As the tightest majo - rant of the dual problem touches f ( x ) at the points a , µ and l = min ( µn / h , h c ) , we consider the three - point dis - tribution p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( l − µ ) , p l = d 2 ( l − µ ) , ( A19 ) which is a distribution since µn / h (cid:29) µ . This yields as objective value for the primal problem µ 2 h µn (cid:16) 1 − d 2 µ − d 2 ( l − µ ) (cid:17) + l 2 h µn d 2 ( l − µ ) = µh n + hdl 2 µn . ( A20 ) Thus , we have strong duality as the primal objective from ( A20 ) and the dual optimal value are the same . Therefore , E P [ a ( h ) ] = n h (cid:16) µh n + hdl 2 µn (cid:17) = µ + dl 2 µ . ( A21 ) Now when h c tends to inﬁnity the second term dominates as l = min ( h c , µn / h ) (cid:29) 1 when h (cid:28) n and , E P [ a ( h ) ] = d 2 µ min ( h c , µn h ) ( 1 + o ( 1 ) ) . ( A22 ) Proof of Theorem VI . 2 . Case 1 : h (cid:28) √ µn and 2 µ ( 1 − (cid:112) µ / n ) < d < 2 µ ( 1 − h / n ) . In this case , consider the three - point distribution p 0 = d 2 µ , p √ µn = ( d − 2 µ ) l + 2 µ 2 2 µ (cid:0) √ µn − l (cid:1) , p l = ( d − 2 µ ) √ µn + 2 µ 2 2 µ (cid:0) l − √ µn (cid:1) , ( A23 ) for l = µn / h , which is a proper distribution under the condition 2 µ ( 1 − (cid:112) µ / n ) < d < 2 µ ( 1 − h / n ) . For this three - point distribution , E P [ c ( h ) ] = n 2 h 2 (cid:16) p 2 √ µn µnh 2 ( µn ) 2 + 2 p l p √ µn √ µnh µn + p 2 l (cid:17) = 1 . ( A24 ) As c ( h ) is a probability , we have E [ c ( h ) ] ≤ 1 , so that it coincides with the upper bound . Case 2 : h (cid:29) √ µn . We now apply the three - point optimization problem in two steps : ﬁrst for the optimal distribution of h i , then for h j . We will show that these two optimal distributions are identical and independent , 17 0 µ µnh ∧ µnh 2 µnh ∨ µnh 2 h s 0 hh 2 ∧ h 2 h 1 h s T2 T1 h f ( x ) F 1 ( x ) T4 FIG . 8 . The tightest majorant for f ( x ) describing the opti - mizer over h 1 of c ( h ) so that this method shows that optimizing the distribu - tion of h i and h j while constraining them to be equal yields this same optimal distribution . The function we would like to optimize is E (cid:20) min ( hh 1 µn , 1 ) min ( hh 2 µn , 1 ) min ( h 1 h 2 µn , 1 ) (cid:21) . ( A25 ) Step 1 : optimizing over h 1 . If we optimize only over the distribution of h 1 and ﬁx h 2 , this is equivalent to optimizing E (cid:20) min ( hh 1 µn , 1 ) min ( h 1 h 2 µn , 1 ) (cid:21) . ( A26 ) Thus , we again want to maximize ( A16 ) and there - fore minimize its dual problem ( A17 ) , but now with f ( x ) = min ( hxµn , 1 ) min ( xh 2 µn , 1 ) . We again focus on the dual problem , which is again equivalent to minimiz - ing λ 1 d + λ 2 µ + λ 3 over all tightest majorants of f ( x ) . Now f ( x ) is again piecewise convex : it is quadratic in h 1 for h 1 < min ( µn / h , µn / h 2 ) , linear for h 1 ∈ [ min ( µn / h , µn / h 2 ) , max ( µn / h , µn / h 2 ) ] , and constant for h 1 ∈ [ max ( µn / h , µn / h 2 ) , h c ] . Thus , f ( x ) is shaped as the function in Figure 8 ( where for simplicity a = 0 ) . In the next computations , we assume for simplic - ity of notation that a = 0 . Now the tightest majo - rant of Figure 8 , F 1 ( x ) , can be parametrized by λ 1 = min ( h , h 2 ) / ( 2 µn ) , λ 2 = min ( h , h 2 ) / ( 2 µn ) + hh 2 / ( µn 2 ) , λ 3 = − min ( h , h 2 ) / ( 2 n ) . This gives as objective value for the dual program ( A17 ) d min ( h , h 2 ) 2 µn + µ (cid:16) min ( h , h 2 ) 2 µn + hh 2 µn 2 (cid:17) − min ( h , h 2 ) 2 n = d 2 µn min ( h , h 2 ) + hh 2 n 2 . ( A27 ) We now again consider the primal problem ( A16 ) . The solution to the dual problem ( A17 ) , F 1 ( x ) has three touching points of f ( x ) : at 0 , µ and min ( µn / h 2 , µn / h ) . Thus , we will now show that c ( h ) is maximized over h 1 by the three - point distribution p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( min ( µn / h 2 , µn / h ) − µ ) , p min ( µn / h 2 , µn / h ) = d 2 ( min ( µn / h 2 , µn / h ) − µ ) . ( A28 ) This gives an objective value of ( A16 ) of 0 · d 2 µ + µ 2 hh 2 ( µn ) 2 (cid:16) 1 − d 2 µ − d 2 ( min ( µn / h 2 , µn / h ) − µ ) (cid:17) + min ( µn / h 2 , µn / h ) 2 hh 2 ( µn ) 2 · d 2 ( min ( µn / h 2 , µn / h ) − µ ) = d 2 µn min ( h , h 2 ) + hh 2 n 2 . ( A29 ) Thus , the objective value of the three - point distribu - tion is equal to the objective value of the dual problem in ( A27 ) . Thus , by strong duality , ( A28 ) is the optimal distribution maximizing ( A26 ) . Step 2 : optimizing over h 2 . We now plug the optimal three - point distribution of h 1 ( A28 ) into ( A25 ) and then optimize only over the distribution of h 2 . We then need to optimize E (cid:20) ( 1 − d 2 µ − d 2 ( min ( µn / h 2 , µn / h ) − µ ) ) hh 2 n 2 min ( hh 2 µn , 1 ) (cid:21) + E (cid:20) d 2 ( min ( µn / h 2 , µn / h ) − µ ) min ( h h 2 , h 2 h ) min ( hh 2 µn , 1 ) (cid:21) . ( A30 ) This is a function in h 2 that looks like Figure 9 : ﬁrst a convex part , then a linear part , and then again a convex part . The tightest majorant F 1 ( x ) is depicted in Figure 9 as well . F 1 ( x ) is characterized by λ 1 = ˜ Ch / ( 2 ( nµ ) 2 ) , λ 2 = ˜ C ( h 2 / ( µ 2 n 3 ) − h / ( 2 ( nµ ) 2 ) ) and λ 3 = − ˜ Ch / ( 2 n 2 µ ) , with ˜ C = µ 2 + dnµ / ( 2 h ) . This gives as dual objective λ 1 d + λ 2 µ + λ 3 = ( dn + 2 hµ ) 2 4 µn 3 . ( A31 ) We now consider the 3 - point distribution for h 2 on the 18 0 µ µnh h h s 0 y µ ˆ y µ y h T2 T1 h 2 f ( x ) F 1 ( x ) T2 FIG . 9 . The tightest majorant for f ( x ) describing the opti - mizer over h 2 of c ( h ) touching points 0 , µ and µn / h : p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( µn / h − µ ) , p µn / h = d 2 ( µn / h − µ ) . ( A32 ) This gives as objective value for the primal problem E (cid:20)(cid:16) 1 − d 2 µ − d 2 ( µn / h − µ ) (cid:17) h 2 µn 3 h 22 (cid:21) + E (cid:20) d 2 ( µn / h − µ ) h 22 µn (cid:21) = ( dn + 2 hµ ) 2 4 µn 3 , ( A33 ) so that by strong duality , this is the optimal three - point distribution for h 2 . As h (cid:29) √ µn , this means that h 2 < h for all three values of the three - point distribution . Then , the three - point distribution for ( A28 ) reduces to ( A32 ) . Thus , by optimizing the distributions of h 1 and h 2 sep - arately , we obtain the same three - point distribution for both . Therefore , the optimization of the distributions of h 1 and h 2 where they are constrained to have the same distribution also gives the three - point distribution ( A32 ) as its solution . Indeed , max x , y : x = y f ( x , y ) ≤ max y max x f ( x , y ) , ( A34 ) as all combinations f ( x , x ) are also encountered on the right - hand side . Furthermore , let x ∗ and y ∗ denote the optimizers obtained in the right - hand side , and suppose that x ∗ = y ∗ . Then , max x , y : x = y f ( x , y ) ≥ f ( x ∗ , y ∗ ) = max y max x f ( x , y ) . ( A35 ) Thus , when optimizing the distributions of h 1 and h 2 separately yields an optimizer where both distributions are equal , then this is also the optimization of the distri - butions of h 1 and h 2 where they are constrained to have the same distribution . This gives for c ( h ) max P ∈P ( µ , d ) E P [ c ( h ) ] = n 2 h 2 ( dn + 2 hµ ) 2 4 µn 3 = d 2 n 4 µh 2 ( 1 + o ( 1 ) ) . ( A36 ) Case 3 : h (cid:28) √ µn and d < 2 µ ( 1 − µ / h ) . We optimize c ( h ) here by again ﬁrst optimizing over the distribution of h i only , and then over the distribution of h j . There - fore , up until ( A30 ) we follow the same steps for opti - mizing over h i . We then optimize for the distribution of h j . Consider the majorant of the dual problem ˜ F 1 ( x ) described by λ 1 = h 2 ( dn + 2 hµ ) 4 µ 2 n 3 , λ 2 = h ( h + 2 µ ) ( dn + 2 hµ ) 4 µ 2 n 3 and λ 3 = − h 2 ( dn + 2 hµ ) 4 µn 3 , giving as dual objective value λ 1 d + λ 2 µ + λ 3 = h (cid:0) dh + 2 µ 2 (cid:1) ( dn + 2 hµ ) 4 µ 2 n 3 . ( A37 ) For the primal problem , consider the three - point distri - bution for h 2 of p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( h − µ ) , p h = d 2 ( h − µ ) , ( A38 ) which is a proper distribution as long as d < 2 µ ( 1 − µ / h ) . Plugging this into ( A30 ) for the distribution of h j gives E (cid:20)(cid:16) 1 − d 2 µ − d 2 ( µn / h − µ ) (cid:17) h 2 µn 3 h 22 (cid:21) + E (cid:20) d 2 ( µn / h − µ ) h 22 µn (cid:21) = h (cid:0) dh + 2 µ 2 (cid:1) ( dn + 2 hµ ) 4 µ 2 n 3 , ( A39 ) so that by strong duality , this is the optimal distribution for h j . This yields for c ( h ) that max P 1 , P 2 ∈P ( µ , d ) E P 1 , P 2 [ c ( h ) ] = n 2 h 2 h (cid:0) dh + 2 µ 2 (cid:1) ( dn + 2 hµ ) 4 µ 2 n 3 = d 2 4 µ 2 ( 1 + o ( 1 ) ) . ( A40 ) However , the maximal value of c ( h ) is now attained by two diﬀerent distributions for h i and h j , while our objective was to maximize c ( h ) with i . i . d . distribution for h i and h j . We therefore now consider the uncorrelated three - point distribution P 3 for h i and h j of p 0 = d 2 µ , p µ = 1 − d 2 µ − d 2 ( √ µn − µ ) , p √ µn = d 2 ( √ µn − µ ) , ( A41 ) which is a proper distribution as long as d < 2 µ ( 1 − 19 (cid:112) µ / n ) . This yields as expected value for c ( h ) of E P 3 [ c ( h ) ] = n 2 h 2 h 2 ( µn ) 3 E P 3 [ h 2 ] 2 = 1 µ 3 n (cid:16) 1 2 d √ µn + µ 2 (cid:17) 2 = d 2 4 µ 2 ( 1 + o ( 1 ) ) , ( A42 ) which is the same leading order term as in ( A40 ) , where correlations between h i and h j are allowed . Since max P 1 , P 2 ∈P ( µ , d ) E P 1 , P 2 [ c ( h ) ] ≥ E P 3 [ c ( h ) ] , ( A43 ) P 3 is asymptotically optimal . Furthermore , an increase in d does not aﬀect the set of feasible dual solutions for the unconstrained problem . Thus , ( A37 ) is still an upper bound of the maximal c ( h ) . As by ( A40 ) this dual value is asymptotically equal to d 2 / ( 4 µ 2 ) , and ( A42 ) achieves the same value asymptotically , this must imply that P 3 is asymptotically optimal when it is a proper probability distribution , thus for all d < 2 µ ( 1 − (cid:112) µ / n ) .