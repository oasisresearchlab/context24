a r X i v : 1506 . 08187v1 [ m a t h . O C ] 26 J un 2015 A geometric alternative to Nesterov’s accelerated gradient descent S´ebastien Bubeck Microsoft Research sebubeck @ microsoft . com Yin Tat Lee ∗ MIT yintat @ mit . edu Mohit Singh Microsoft Research mohits @ microsoft . com June 29 , 2015 Abstract We propose a new method for unconstrained optimization of a smooth and strongly convex function , which attains the optimal rate of convergence of Nesterov’s accelerated gradient descent . The new algorithm has a simple geometric interpretation , loosely inspired by the ellipsoid method . We provide some numerical evidence that the new method can be superior to Nesterov’s accelerated gradient descent . 1 Introduction Let f : R n → R be a β - smooth and α - strongly convex function . Thus , for any x , y ∈ R n , we have f ( x ) + ∇ f ( x ) ⊤ ( y − x ) + α 2 | y − x | 2 ≤ f ( y ) ≤ f ( x ) + ∇ f ( x ) ⊤ ( y − x ) + β 2 | y − x | 2 . Let κ = β / α be its condition number . It is a one line calculation to verify that a step of gradient descent on f will decrease ( multiplicatively ) the squared distance to the optimum by 1 − 1 / κ . In this paper we propose a new method , which can be viewed as some combination of gradient descent and the ellipsoid method , for which the squared distance to the optimum decreases at a rate of ( 1 − 1 / √ κ ) ( and each iteration requires one gradient evaluation and two line - searches ) . This matches the optimal rate of convergence among the class of ﬁrst - order methods , [ Nesterov ( 1983 ) , Nesterov ( 2004 ) ] . 1 . 1 Related works Nesterov’s acceleration ( i . e . , replacing κ by √ κ in the convergence rate ) has proven to be of funda - mental importance both in theory and in practice , see e . g . [ Bubeck ( 2014 ) ] for references . However ∗ Most of this work were done while the author was at Microsoft Research , Redmond . The author was supported by NSF awards 0843915 and 1111109 . 1 the intuition behind Nesterov’s accelerated gradient descent is notoriously difﬁcult to grasp , and this has led to a recent surge of interest in new interpretations of this algorithm , as well as the rea - sons behind the possibility of acceleration for smooth problems , see [ Allen - Zhu and Orecchia ( 2014 ) , Lessard et al . ( 2014 ) Lessard , Recht , and Packard , Su et al . ( 2014 ) Su , Boyd , and Cand ` es , Flammarion and Bach ( 2015 ) In this paper we propose a new method with a clear intuition and which achieves acceleration . Since the function is strongly convex , gradient at any point gives a ball , say A , containing the optimum solution . Using the fact that the function is smooth , one can get an improved bound on the radius of this ball . The algorithm also maintains a ball B containing the optimal solution obtained via the information from previous iterations . A simple calculation then shows that the smallest ball enclosing the intersection of A and B already has a radius shrinking at the rate of 1 − 1 κ . To achieve the accelerated rate , we make the observation that the gradient information in this iteration can also be used to shrink the ball B and therefore , the radius of the enclosing ball containing the intersection of A and B shrinks at a faster rate . We detail this intuition in Section 2 . The new optimal method is described and analyzed in Section 3 . We conclude with some experiments in Section 4 . 1 . 2 Preliminaries We write | · | for the Euclidean norm in R n , and B ( x , r 2 ) : = { y ∈ R n : | y − x | 2 ≤ r 2 } ( note that the second argument is the radius squared ) . We deﬁne the map line search : R n × R n → R n by line search ( x , y ) = argmin t ∈ R f ( x + t ( y − x ) ) , and we denote x + = x − 1 β ∇ f ( x ) , and x + + = x − 1 α ∇ f ( x ) . Recall that by strong convexity one has ∀ y ∈ R n , f ( y ) ≥ f ( x ) + ∇ f ( x ) ⊤ ( y − x ) + α 2 | y − x | 2 , which implies in particular : x ∗ ∈ B (cid:18) x + + , | ∇ f ( x ) | 2 α 2 − 2 α ( f ( x ) − f ( x ∗ ) ) (cid:19) . Furthermore recall that by smoothness one has f ( x + ) ≤ f ( x ) − 12 β | ∇ f ( x ) | 2 which allows to shrink the above ball by a factor of 1 − 1 κ and obtain the following : x ∗ ∈ B (cid:18) x + + , | ∇ f ( x ) | 2 α 2 (cid:18) 1 − 1 κ (cid:19) − 2 α ( f ( x + ) − f ( x ∗ ) ) (cid:19) ( 1 ) 2 Intuition In Section 2 . 1 we describe a geometric alternative to gradient descent ( with the same convergence rate ) which gives the core of our new optimal method . Then in Section 2 . 2 we explain why one can expect to accelerate this geometric algorithm . 2 | g | √ 1 − ε | g | 1 √ 1 − ε Figure 1 : One ball shrinks . √ 1 − ε | g | p 1 − ε | g | 2 p 1 −√ ε Figure 2 : Two balls shrink . The left diagram shows the intersection shrinks at the same rate if only one of the ball shrinks ; the right diagram shows the intersection shrinks much faster if two balls shrinks at the same absolute amount . 2 . 1 A suboptimal algorithm Assume that we are given a guarantee R 0 > 0 on the distance from some point x 0 to the optimum , that is x ∗ ∈ B ( x 0 , R 20 ) . Combining this original enclosing ball for x ∗ and the one obtained by ( 1 ) ( with f ( x ∗ ) ≤ f ( x + 0 ) ) one obtains x ∗ ∈ B ( x 0 , R 20 ) ∩ B (cid:18) x 0 − 1 α ∇ f ( x 0 ) , | ∇ f ( x 0 ) | 2 α 2 (cid:18) 1 − 1 κ (cid:19)(cid:19) . If | ∇ f ( x 0 ) | 2 α 2 ≤ R 20 ( 1 − 1 κ ) then the second ball already shrinks by a factor of ( 1 − 1 κ ) . In the other case when | ∇ f ( x 0 ) | 2 α 2 > R 20 ( 1 − 1 κ ) , the center of the two balls are far apart and therefore there is a much smaller ball containing the intersection of two balls . Formally , it is an easy calculation to see that for any g ∈ R n , ε ∈ ( 0 , 1 ) , there exists x ∈ R n such that B ( 0 , 1 ) ∩ B ( g , | g | 2 ( 1 − ε ) ) ⊂ B ( x , 1 − ε ) . ( Figure 1 ) In particular the two above display implies that there exists x 1 ∈ R n such that x ∗ ∈ B (cid:18) x 1 , R 20 (cid:18) 1 − 1 κ (cid:19)(cid:19) . Denote by T the map from x 0 to x 1 deﬁned implicitely above , and let ( x k ) be deﬁned by x k + 1 = T ( x k ) . Then we just proved | x ∗ − x k | 2 ≤ (cid:18) 1 − 1 κ (cid:19) k R 20 . In other words , after 2 κ log ( R 0 / ε ) iterations where each iteration cost one call to the gradient oracle ) one obtains a point ε - close to the minimizer of f . 3 2 . 2 Why one can accelerate Assume now that we are give a guarantee R 0 > 0 such that x ∗ ∈ B ( x 0 , R 20 − 2 α ( f ( y ) − f ( x ∗ ) ) ) where f ( x 0 ) ≤ f ( y ) ( say by choosing y = x 0 ) . Using the fact that f ( x + 0 ) ≤ f ( x 0 ) − 12 β | ∇ f ( x 0 ) | 2 ≤ f ( y ) − 12 ακ | ∇ f ( x 0 ) | 2 , we obtain that x ∗ ∈ B (cid:18) x 0 , R 20 − | ∇ f ( x 0 ) | 2 α 2 κ − 2 α (cid:0) f ( x + 0 ) − f ( x ∗ ) (cid:1)(cid:19) which , intuitively , allows us the shrink the radius squared from R 20 to R 20 − | ∇ f ( x 0 ) | 2 α 2 κ using the local information at x 0 . From ( 1 ) , we have x ∗ ∈ B (cid:18) x + + 0 , | ∇ f ( x 0 ) | 2 α 2 (cid:18) 1 − 1 κ (cid:19) − 2 α (cid:0) f ( x + 0 ) − f ( x ∗ ) (cid:1)(cid:19) . Now , intersecting the above two shrunk balls and using Lemma 1 ( see below and also see Figure 2 ) , we obtain that there is an x ′ 1 such that x ∗ ∈ B (cid:18) x ′ 1 , R 20 (cid:18) 1 − 1 √ κ (cid:19) − 2 α (cid:0) f ( x + 0 ) − f ( x ∗ ) (cid:1)(cid:19) giving us an acceleration in shrinking of the radius . To carry the argument for the next iteration , we would have required that f ( x ′ 1 ) ≤ f ( x + 0 ) but it may not hold . Thus , we choose x 1 by a line search x 1 = line search (cid:0) x ′ 1 , x + 0 (cid:1) which ensures that f ( x 1 ) ≤ f ( x + 0 ) . To remedy the fact that the ball for the next iteration is centered at x ′ 1 and not x 1 , we observe that the line search also ensures that ∇ f ( x 1 ) is perpendicular to the line going through x 1 and x ′ 1 . This geometric fact is enough for the algorithm to work at the next iteration as well . In the next section we describe precisely our proposed algorithm which is based on the above insights . 3 An optimal algorithm Let x 0 ∈ R n , c 0 = x + + 0 , and R 20 = (cid:0) 1 − 1 κ (cid:1) | ∇ f ( x 0 ) | 2 α 2 . For any k ≥ 0 let x k + 1 = line search (cid:0) c k , x + k (cid:1) , and c k + 1 ( respectively R 2 k + 1 ) be the center ( respectively the squared radius ) of the ball given by ( the proof of ) Lemma 1 which contains B (cid:18) c k , R 2 k − | ∇ f ( x k + 1 ) | 2 α 2 κ (cid:19) ∩ B (cid:18) x + + k + 1 , | ∇ f ( x k + 1 ) | 2 α 2 (cid:18) 1 − 1 κ (cid:19)(cid:19) . The formulas for c k + 1 and R 2 k + 1 are given in Algorithm 1 . 4 Theorem 1 For any k ≥ 0 , one has x ∗ ∈ B ( c k , R 2 k ) , R 2 k + 1 ≤ (cid:16) 1 − 1 √ κ (cid:17) R 2 k , and thus | x ∗ − c k | 2 ≤ (cid:18) 1 − 1 √ κ (cid:19) k R 20 . Proof We will prove a stronger claim by induction that for each k ≥ 0 , one has x ∗ ∈ B (cid:18) c k , R 2 k − 2 α (cid:0) f ( x + k ) − f ( x ∗ ) (cid:1)(cid:19) . The case k = 0 follows immediately by ( 1 ) . Let us assume that the above display is true for some k ≥ 0 . Then using f ( x ∗ ) ≤ f ( x + k + 1 ) ≤ f ( x k + 1 ) − 12 β | ∇ f ( x k + 1 ) | 2 ≤ f ( x + k ) − 12 β | ∇ f ( x k + 1 ) | 2 , one gets x ∗ ∈ B (cid:18) c k , R 2 k − | ∇ f ( x k + 1 ) | 2 α 2 κ − 2 α (cid:0) f ( x + k + 1 ) − f ( x ∗ ) (cid:1)(cid:19) . Furthermore by ( 1 ) one also has B (cid:18) x + + k + 1 , | ∇ f ( x k + 1 ) | 2 α 2 (cid:18) 1 − 1 κ (cid:19) − 2 α (cid:0) f ( x + k + 1 ) − f ( x ∗ ) (cid:1)(cid:19) . Thus it only remains to observe that the squared radius of the ball given by Lemma 1 which en - closes the intersection of the two above balls is smaller than (cid:16) 1 − 1 √ κ (cid:17) R 2 k − 2 α ( f ( x + k + 1 ) − f ( x ∗ ) ) . We apply Lemma 1 after moving c k to the origin and scaling distances by R k . We set ε = 1 κ , g = | ∇ f ( x k + 1 ) | α , δ = 2 α (cid:0) f ( x + k + 1 ) − f ( x ∗ ) (cid:1) and a = x + + k + 1 − c k . The line search step of the algorithm implies that ∇ f ( x k + 1 ) ⊤ ( x k + 1 − c k ) = 0 and therefore , | a | = | x + + k + 1 − c k | ≥ | ∇ f ( x k + 1 ) | / α = g and Lemma 1 applies to give the result . Lemma 1 Let a ∈ R n and ε ∈ ( 0 , 1 ) , g ∈ R + . Assume that | a | ≥ g . Then there exists c ∈ R n such that for any δ > 0 , B ( 0 , 1 − εg 2 − δ ) ∩ B ( a , g 2 ( 1 − ε ) − δ ) ⊂ B (cid:0) c , 1 − √ ε − δ (cid:1) . Proof First observe that if g 2 ≤ 1 / 2 then one can take c = a since 12 ( 1 − ε ) ≤ 1 − √ ε . Thus we assume now that g 2 > 1 / 2 , and note that we can also clearly assume that n = 2 . Consider the segment joining the two points at the intersection of the two balls under consideration . We denote c for the point at the intersection of this segment and [ 0 , a ] , and x = | c | ( that is c = x a | a | ) . A simple picture reveals that x satisﬁes 1 − εg 2 − δ − x 2 = g 2 ( 1 − ε ) − δ − ( | a | − x ) 2 ⇔ x = 1 + | a | 2 − g 2 2 | a | . When x ≤ | a | , neither of the balls covers more than half of the other ball and hence the intersection of the two balls is contained in the ball B (cid:16) x a | a | , 1 − εg 2 − δ − x 2 (cid:17) ( See ﬁgure 2 ) . Thus it only remains to show that x ≤ | a | and that 1 − εg 2 − δ − x 2 ≤ 1 − √ ε − δ . The ﬁrst 5 Algorithm 1 : Minimum Enclosing Ball of the Intersection to Two Balls Input : a ball centered at x A with radius R A and a ball centered at x B with radius R B . if | x A − x B | 2 ≥ | R 2 A − R 2 B | then c = 12 ( x A + x B ) − R 2 A − R 2 B 2 | x A − x B | 2 ( x A − X B ) . R 2 = R 2 B − ( | x A − x B | 2 + R 2 B − R 2 A ) 2 4 | x A − x B | 2 . else if | x A − x B | 2 < R 2 A − R 2 B then c = x B . R = R B . else c = x A . R = R A . a end Output : a ball centered at c with radius R . a If we assume | x A − x B | ≥ R B as in Lemma 1 , this extra case does not exist . inequality is equivalent to | a | 2 + g 2 ≥ 1 which follows from | a | 2 ≥ g 2 ≥ 1 / 2 . The second inequality to prove can be written as εg 2 + ( 1 + | a | 2 − g 2 ) 2 4 | a | 2 ≥ √ ε , which is straightforward to verify ( recall that | a | 2 ≥ g 2 ≥ 1 / 2 ) . Algorithm 2 we give is more agressive than Theorem 1 , for instance , using line search instead of ﬁxed step size . The correctness of this version follows from a similar proof as Theorem 1 . This algorithm does not require the smoothness parameter and the number of iterations ; and it guarantees the function value is strictly decreasing . They are useful properties for machine learning applications because the only required parameter α is usually given . Furthermore , we believe that the integration of zeroth and ﬁrst order information about the function makes our new method particularly well - suited in practice . 4 Experiments In this section , we compare Geometric Descent method ( GeoD ) with a variety of full gradient meth - ods . It includes steepest descent ( SD ) , accelerated full gradient method ( AFG ) , accelerated full gradient method with adaptive restart ( AFGwR ) and quasi - Newton with limited - memory BFGS updating ( L - BFGS ) . For SD , we compute the gradient and perform an exact line search on the gra - dient direction . For AFG , we use the ‘Constant Step Scheme II’ in [ Nesterov ( 2004 ) ] . For AFGwR , [ ODonoghue and Candes ( 2013 ) ] , we use the function restart scheme and replace the gradient step by an exact line search to improve its performance . For both AFG and AFGwR , the parameter is chosen among all powers of 2 for each dataset individually . For L - BFGS , we use the software developed by Mark Schmidt with default settings ( see [ Schmidt ( 2012 ) ] ) . In all experiments , the minimization problem is of the form P i ϕ ( a Ti x ) where computing a Ti x is the computational bottleneck . Therefore , if we reuse the calculations carefully , each iteration of all mentioned methods requires only one calculation of a Ti x for some x . In particular , the cost 6 Algorithm 2 : Geometric Descent Method ( GeoD ) Input : parameters α and initial points x 0 . x + 0 = line search ( x 0 , x 0 − ∇ f ( x 0 ) ) . c 0 = x 0 − α − 1 ∇ f ( x 0 ) . R 20 = | ∇ f ( x 0 ) | 2 α 2 − 2 α (cid:0) f ( x 0 ) − f ( x + 0 ) (cid:1) . for i ← 1 , 2 , · · · do Combining Step : x k = line search ( x + k − 1 , c k − 1 ) . Gradient Step : x + k = line search ( x k , x k − ∇ f ( x k ) ) . Ellipsoid Step : x A = x k − α − 1 ∇ f ( x k ) . R 2 A = | ∇ f ( x k ) | 2 α 2 − 2 α (cid:0) f ( x k ) − f ( x + k ) (cid:1) . x B = c k − 1 . R 2 B = R 2 k − 1 − 2 α (cid:0) f ( x + k − 1 ) − f ( x + k ) (cid:1) . Let B ( c k , R 2 k ) is the minimum enclosing ball of B ( x A , R 2 A ) ∩ B ( x B , R 2 B ) . end Output : x T . of exact line searches is negligible compares with the cost of computing a Ti x . Hence , we simply report the number of iterations in the following experiments . 4 . 1 Binary Classiﬁcation We evaluate the algorithms via the binary classiﬁcation problem on the 40 datasets 1 from LIBSVM data , [ Chang and Lin ( 2011 ) ] . The problem is to minimize the regularized empirical risk : f ( x ) = 1 n n X i = 1 ϕ ( b i a Ti x ) + λ 2 | x | 2 where a i ∈ R d , b i ∈ R are given by the datasets , λ is the regularization coefﬁcient and ϕ is the smoothed hinge loss function given by ϕ ( z ) =      0 if z ≥ 1 12 − z if z ≤ 0 12 ( 1 − z ) 2 otherwise . We solve this problem with different regularization coefﬁcients λ ∈ { 10 − 4 , 10 − 5 , 10 − 6 , 10 − 7 , 10 − 8 } and report the median and 90th percentile of the number of steps required to achieve a certain accu - racy . In ﬁgure 3 , we see that GeoD is better than SD , AFG and AFGwR , but worse than L - BFGS . Since L - BFGS stores and uses the gradients of the previous iterations , it is interesting to see if GeoD will be competitive to L - BFGS if it computes the intersection of multiple balls instead of 2 balls . 1 We omitted all datasets of size ≥ 100 MB for time consideration . 7 10 −8 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 0 10 1 10 2 10 3 Required Accuracy It e r a t i on s Median of # Iteration for 200 Problems SDAFGAFGwRGeoDL−BFGS 10 −8 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 0 10 1 10 2 10 3 Required Accuracy It e r a t i on s 90th percentile of # Iteration for 200 Problems SDAFGAFGwRGeoDL−BFGS Figure 3 : Comparison of full gradient methods on 40 datasets and 5 regularization coefﬁcients with smoothed hinge loss function . The left diagram shows the median of the number of iterations required to achieve a certain accuracy and the right diagram shows the 90th percentile . 4 . 2 Worst Case Experiment In this section , we consider the minimization problem f ( x ) = β 2 ( 1 − x 1 ) 2 + n − 1 X i = 1 ( x i − x i + 1 ) 2 + x 2 n ! + 1 2 n X i = 1 x 2 i ( 2 ) where β is the smothness parameter . Within the ﬁrst n iterations , it is known that any iterative methods uses only the gradient information cannot minimize this function faster than the rate 1 − Θ ( β − 1 / 2 ) . In ﬁgure 4 , we see that every method except SD converge in the same rate with different constants for the ﬁrst n iterations . However , after Θ ( n ) iterations , both SD and AFG continue to converge in the rate the theory predicted while other methods converge much faster . We remark that the memory size of L - BFGS we are using is 100 and if in the right example we choose n = 100 instead of 200 , L - BFGS will converge at n = 100 immediately . It is surprising that the AFGwR and GeoD can achieve a comparable result by using “memory size” being 1 . References [ Allen - Zhu and Orecchia ( 2014 ) ] Z . Allen - Zhu and L . Orecchia . Linear coupling : An ultimate uniﬁcation of gradient and mirror descent . Arxiv preprint arXiv : 1407 . 1537 , 2014 . [ Bubeck ( 2014 ) ] S . Bubeck . Theory of convex optimization for machine learning . Arxiv preprint arXiv : 1405 . 4980 , 2014 . [ Chang and Lin ( 2011 ) ] Chih - Chung Chang and Chih - Jen Lin . Libsvm : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology ( TIST ) . , 2 ( 3 ) : 27 , 2011 . Software available at http : / / www . csie . ntu . edu . tw / cjlin / libsvm . 8 Number of Iterations 0 200 400 600 800 1000 E rr o r 10 - 5 10 0 10 5 n = 10000 , β = 10 4 SD AFG AFGwR L - BFGS GeoD Number of Iterations ( x100 ) 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 E rr o r 10 - 5 10 0 10 5 n = 200 , β = 10 6 SD AFG AFGwR L - BFGS GeoD Figure 4 : Comparision of full gradient methods for the function ( 2 ) [ Flammarion and Bach ( 2015 ) ] N . Flammarion and F . Bach . From averaging to acceleration , there is only a step - size . In Proceedings of the 28th Annual Conference on Learning Theory ( COLT ) , 2015 . [ Lessard et al . ( 2014 ) Lessard , Recht , and Packard ] L . Lessard , B . Recht , and A . Packard . Analy - sis and design of optimization algorithms via integral quadratic constraints . Arxiv preprint arXiv : 1408 . 3595 , 2014 . [ Nesterov ( 1983 ) ] Y . Nesterov . A method of solving a convex programming problem with conver - gence rate o ( 1 / k 2 ) . Soviet Mathematics Doklady , 27 ( 2 ) : 372 – 376 , 1983 . [ Nesterov ( 2004 ) ] Y . Nesterov . Introductory lectures on convex optimization : A basic course . Kluwer Academic Publishers , 2004 . [ ODonoghue and Candes ( 2013 ) ] Brendan ODonoghue and Emmanuel Candes . Adaptive restart for accelerated gradient schemes . Foundations of computational mathematics , 15 ( 3 ) : 715 – 732 , 2013 . [ Schmidt ( 2012 ) ] M Schmidt . minfunc : unconstrained differentiable multivariate optimization in matlab . URL http : / / www . di . ens . fr / mschmidt / Software / minFunc . html , 2012 . [ Su et al . ( 2014 ) Su , Boyd , and Cand ` es ] W . Su , S . Boyd , and E . Cand ` es . A differential equation for modeling nesterovs accelerated gradient method : Theory and insights . In Advances in Neural Information Processing Systems ( NIPS ) , 2014 . 9