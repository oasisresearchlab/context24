Accepted Manuscript Workﬂows and e - Science : An overview of workﬂow system features and capabilities Ewa Deelman , Dennis Gannon , Matthew Shields , Ian Taylor PII : S0167 - 739X ( 08 ) 00086 - 1 DOI : 10 . 1016 / j . future . 2008 . 06 . 012 Reference : FUTURE 1670 To appear in : Future Generation Computer Systems Received date : 1 December 2007 Revised date : 8 May 2008 Accepted date : 25 June 2008 Please cite this article as : E . Deelman , D . Gannon , M . Shields , I . Taylor , Workﬂows and e - Science : An overview of workﬂow system features and capabilities , Future Generation Computer Systems ( 2008 ) , doi : 10 . 1016 / j . future . 2008 . 06 . 012 This is a PDF ﬁle of an unedited manuscript that has been accepted for publication . As a service to our customers we are providing this early version of the manuscript . The manuscript will undergo copyediting , typesetting , and review of the resulting proof before it is published in its ﬁnal form . Please note that during the production process errors may be discovered which could affect the content , and all legal disclaimers that apply to the journal pertain . A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Workﬂows and e - Science : An Overview of Workﬂow System Features and Capabilities . Ewa Deelman a Dennis Gannon b Matthew Shields c Ian Taylor d a Information Sciences Institute , University of Southern California , USA b Department of Computer Science , Indiana University , USA c School of Computer Science , Cardiﬀ University , UK d School of Computer Science , Cardiﬀ University , UK and the Center for Computation and Technology , LSU , USA Abstract Scientiﬁc workﬂow systems have become a necessary tool for many applications , enabling the composition and execution of complex analysis on distributed resources . Today there are many workﬂow systems , often with overlapping functionality . A key issue for potential users of work - ﬂow systems is the need to be able to compare the capabilities of the various available tools . There can be confusion about system functionality and the tools are often selected without a proper functional analysis . In this paper we extract a taxonomy of features from the way sci - entists make use of existing workﬂow systems and we illustrate this feature set by providing some examples taken from existing workﬂow systems . The taxonomy provides end users with a mechanism by which they can assess the suitability of workﬂow in general and how they might use these features to make an informed choice about which workﬂow system would be a good choice for their particular application . Key words : Scientiﬁc Workﬂow , Grid computing , Computation , Web Services , Distributed Computing , Distributed Systems , Cyberinfrastructure , Automation of Scientiﬁc processes , e - Science . 1 . Introduction Over the past two decades , we have seen a revolution in the way science and engineer - ing are conducted . Computation has become an established “third branch” of science alongside theory and experiment . Supercomputers simulate large , physically complex systems modelled by partial diﬀerential equations . Computational tools are adopted in applications that involve complex data analysis and visualization steps . A typical ex - perimental scenario is a repetitive cycle of moving data to a supercomputer for analysis Preprint submitted to Elsevier 7 May 2008 Main Manuscript A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT or simulation , launching the computations and managing the storage of the output re - sults . Scientiﬁc workﬂow systems aim at automating this cycle in a way to make it easier for scientists to focus on their research and not computation management . At the same time , the business community also addressed the automation of their business logic and tools and commercial computer companies soon followed to support this process . What emerged was a primitive science of workﬂow design . Workﬂow Orchestration refers to the activity of deﬁning the sequence of tasks needed to manage a business or computational science or engineering process . A Workﬂow is a template for such an orchestration . A Workﬂow Instance is a speciﬁc instantiation of a workﬂow for a particular problem and includes the deﬁnition of input data . Within the scientiﬁc and engineering community these terms have a slightly broader meaning and here in this paper , we focus on the classiﬁcation of such by considering four broad areas that describe the various aspects of the workﬂow life cycle ( composition , mapping , execution and provenance ) . These four areas are then further subdivided to attempt to build a classiﬁcation scheme for workﬂow systems through the use of a rich feature set that outlines a taxonomy to provide the basis for a categorisation axis for various workﬂow systems within the scientiﬁc workﬂow community . There have been other publications that have compared a subset of workﬂow sys - tems for scientiﬁc applications ( 1 ) but such classiﬁcations potentially suﬀer from the ever - changing world of distributed computing and the evolution of new workﬂow solu - tions that are appearing at a rather frequent rate . Other workﬂow taxonomy papers ( 2 ) promote speciﬁc aspects desired for workﬂow systems , such as the need for adaptive workﬂow management . Therefore , rather than focusing on a speciﬁc aspect or comparing every workﬂow solution that exists today , we focus here on the scientist’s or distributed - application developer’s view and attempt to extract a feature set that encapsulates the functionality exposed by various workﬂow systems in an attempt to create a generalised taxonomy for the ﬁeld as a whole rather than comparing speciﬁc solutions . Therefore , the systems we include in this paper only constitute a subset of the many workﬂow systems that exist within the research and commercial communities and provide an illustrative means for the justiﬁcation of our feature set . It is hoped that such a taxonomy of features will be useful not only to scientists or application - developers when considering which features might be important for their problem domain but also to the workﬂow community as a whole in order to provide some basis for classiﬁcation of systems in the future and possible interoperability . It is as inconceivable at this stage to see a one - size - ﬁts - all workﬂow system that encompasses all of the desirable features in one solution as it is to ask for one programming language to be best for all problems . Therefore any decision about the choice of one workﬂow system or systems used must be informed in order to reduce the development time of applications as a whole in this area . 2 . Workﬂow Capabilities and the Workﬂow Lifecycle At one level a workﬂow is a high - level speciﬁcation of a set of tasks and the depen - dencies between them that must be satisﬁed in order to accomplish a speciﬁc goal . For example , a data analysis protocol consisting of a sequence of pre - processing , analysis , simulation and post - processing steps is a typical workﬂow scenario in e - Science applica - 2 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT tions . At the level of representation and execution , a workﬂow is a computer program and it can be expressed in any modern programming language . However , the task of writing a computer program in Perl or Java or Python to orchestrate a set of tasks on a wide - area distributed system goes well beyond the programming skills or patience of most scientiﬁc users . The goal of e - Science workﬂow systems ( 3 ) is to provide a specialized programming environment to simplify the programming eﬀort required by scientists to orchestrate a computational science experiment . In general we can classify four diﬀerent phases of the workﬂow lifecycle ( as seen in Figure 1 ) : ( i ) Composition , Representation and Data Model : the composition of the work - ﬂow ( abstract or executable ) through a number of diﬀerent means e . g . text , graph - ical , semantic . ( ii ) Mapping : Involves the mapping from the ( abstract ) workﬂow to the underlying resources . ( iii ) Execution : Enactment of the mapped workﬂow on the underlying resources . ( iv ) Metadata and Provenance : The recording of metadata and provenance infor - mation during the various stages of the workﬂow lifecycle . Fig . 1 . The Workﬂow Lifecycle : Composition , Mapping , Execution , and Provenance Capture . During workﬂow composition the user creates a workﬂow either from scratch or by modifying a previously designed workﬂow . The user can rely on workﬂow component and data catalogs . The workﬂow composition process can be iterative , where portions of the workﬂow need to be executed before subsequent parts of the workﬂow are designed . Once the workﬂow is deﬁned , all , or portions of the workﬂow can be sent for mapping and then execution . During that phase various optimizations and scheduling decisions can be made . Finally , the data and all associated metadata and provenance information are recorded and placed in a variety of registries which can then be accessed to design a new workﬂow . Even though we delineate data recording as a separate phase of the workﬂow lifecycle , this activity can and often is part of the workﬂow execution . In the following four sections , we examine these four areas in detail by extracting a feature set within each category to deﬁne the common aspects of functionality that are inherent across workﬂow systems in general . 3 . Composition , Representation and Execution Models 3 . 1 . Workﬂow Composition The composition of workﬂows is an important step in the workﬂow process . It allows a workﬂow user or programmer to specify the steps and dependencies in either a concrete or abstract fashion that represent the desired overall analysis . There are a number of mechanisms for specifying such ﬂows or dependencies and the resulting graph can be stored in a number of diﬀerent formats , described in the next section . In this subsection , we outline existing methods for editing or composing workﬂows that exist in a number 3 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT of workﬂow systems today . We categorise the composition methods into three broad categories : textual , graphical and mechanism - based semantic models . 3 . 1 . 1 . Textual Workﬂow Editing Many workﬂow systems use a particular workﬂow language or representation ( BPEL ( 4 ) , SCUFL ( 5 ) , DAGMan ( 6 ) , DAX ( 7 ) ) , which has a speciﬁcation that can be composed by hand using a plain text editor . While this has worked well for users of some systems , such as DAGMan , there are many examples where the task of writing the textual workﬂow program by hand can be extremely diﬃcult or error - prone . This is particularly true if the workﬂow language has poor support for standard programming control constructs . For example , workﬂows for parameter sweeps can be pretty simple before parallelization but then become extremely complex after the parallelization has taken place , where the same algorithm is applied to multiple data sets and each data set is represented by one branch of the overall workﬂow graph . For these purposes , a scientist typically uses a script in a high - level language like Python or Ruby which is designed for expressing complex control and using it to generate the lower - level workﬂow primitives . For example , Pegasus ( 7 ) takes a workﬂow description in a form of a Directed Acyclic Graph in XML format ( DAX ) . The DAX can be generated using a Java API , any type of scripting language , or with the use of semantic technologies such as Wings ( 8 ) . In some cases , scientiﬁc applications want to provide users with an interface , which is only in the form of a metadata query . For example , in astronomy , users often do not want to know the details of the underlying system , instead they want to retrieve images of an area of the sky of interest to them . In such cases Pegasus is usually integrated into a portal environment where the user is presented with a Web form to ﬁll in the desired metadata attributes . Inside the portal , the workﬂow instance is generated automatically based on the user’s input and is given to Pegasus for mapping and then to DAGMan for execution . Examples of this approach can be seen in the Montage project ( 9 ; 10 ) ( an astronomy application ) , the Telescience portal ( 11 ) ( a neuroscience application ) , and the Earthworks portal ( 12 ) ( an earthquake science application ) . In all these applications , Pegasus and DAGMan are being used to run the application workﬂows on a national scale infrastructure such as the TeraGrid . 3 . 1 . 2 . Graphical Workﬂow Editing To simplify the life of scientiﬁc users , most e - Science workﬂow systems provide a graph - ical tool for composing workﬂows . While graphical programming has been a subject of experiment for many years , it has not proven to be a substitute for traditional text based programming for general purpose use . However it has achieved considerable success in the domains where the primary task is one of composition and orchestration . For exam - ple , systems such as AVS ( 13 ) and SciRun ( 14 ) allow users to design complex graphics applications by composing graphical ﬁlers and rendering modules . e - Science workﬂow systems such as Kepler ( 15 ) , Triana ( 16 ) , and Vistrails ( 17 ) have sophisticated graphical composition tools for building workﬂow around the idea of composing graphs where the nodes represent tasks and edges represent dependencies . BPEL ( Business Process Execution Language ) ( 4 ) is the de facto standard for Web - service - based workﬂows with a number of implementations from Microsoft , IBM , BEA and Oracle as well as open source organizations . BPEL is a Turing complete program - 4 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT ming language expressed in XML . Very few people write BPEL code by hand and a host of graphical tools exist to allow users to compose BPEL workﬂows . Several such tools ex - ist for e - Science applications . For example , XBaya ( 18 ) is used in the LEAD project ( 19 ) and in some bioinformatics applications . XBaya includes a compiler that translates a graphical representation into several diﬀerent textual forms . For example , in addition to BPEL , it can compile a graphical workﬂow representation into a Python program . Eclipse BPEL Designer ( 20 ) uses primarily visual modelling . Composition of invocations of ser - vice partner operations and control - ﬂow expressions are speciﬁed through various BPEL primitives . The BPEL knowledge editors are mainly GUIs but there is some preliminary developments on Web - based modelling environments . Graphical renderings of a workﬂow are easy for small workﬂows with fewer than a few dozen tasks . However many e - Science workﬂows are far more complex . Consequently most graphical tools allow some form of graphical nesting based on sub - workﬂow hierarchies . Another source of graphical complexity involves expressing “for - each” concurrency in a workﬂow . However , this problem can be addressed by providing specialized control primitives in the graphical vocabulary . We return to this topic in the following section . Workﬂow systems can be generally classiﬁed into two broad categories : Task - based or service - based . Task - based systems ( e . g . Pegasus ) generally focus on the mapping and execution capabilities and leave the higher - level composition tasks to other tools , even employing the use of semantics , as described in Section 3 . 1 . 4 . On - going work with both Kepler and Triana is also being undertaken with Pegasus to provide graphical choreogra - phy interfaces to the system . However , whereas task - level workﬂow systems focus on the resource - level functionality and fault - tolerance , service - level systems generally provide interfaces to certain classes of services for management and composition . One important factor therefore to their take up is the availability of current tools and services that scien - tists can build on in order to create their applications . Such service availability forms part of the composition process since it represents the available tools that can be composed within a system . 3 . 1 . 3 . Worfklow Components One of Taverna’s key values for example is the availability of services to the core system , current ﬁgures estimate this to be around 3500 mainly concentrated in the bioinformatics problem domain . Taverna has also began to share workﬂows through the myExperiment project ( 21 ) in order to make such workﬂows available to the community as a whole . Taverna has a GUI - based desktop application that uses semantic annotations associated with services . It employs the use of semantic - enabled helper functions which will be made available in the next public release of the software . Developers can incorporate new services through simple means and can load a pre - existing workﬂow as a service deﬁnition within the service palette , which can then be used as a service instance within the current workﬂow ( i . e . to support grouping ) . Services within the pre - existing workﬂow can also be instantiated individually within the current workﬂow and a developer can create user - deﬁned perspectives that allow a panel of pre - existing components to be speciﬁed . Within Taverna , there is also support for the conﬁguration of the appearance of the graphical representation of the current workﬂow , so that , for example , a workﬂow can be suppressed to give higher level views ( e . g . to remove the data translation ( shim ) services ) . 5 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT One of the most powerful aspects of Triana on the other hand is its graphical user interface . It has evolved in its Java form for over 10 years and contains a number of powerful editing capabilities , wizards for on - the - ﬂy creation of tools and GUI builders for creating user interfaces . Triana’s editing capabilities include : multi - level grouping for simplifying workﬂows , cut / copy / paste / undo , the ability to edit input / output nodes ( to make copies of data and add parameter dependencies , remote controls or plug - ins ) , zoom functions , various cabling types , optional inputs , type checking and so on . Since Triana came from the gravitational wave ﬁeld ( 22 ) the system contains a wide ranging palette of tools ( around 400 ) for the analysis and manipulation of one - dimensional data , which are mostly written in Java ( with some in C ) . Recently , other extensive toolkits have been added for audio analysis ( 23 ) , image processing , text editing , for creating retinopathy workﬂows 1 and even data mining based on conﬁgurable Web Services ( 24 ; 25 ) to aid in the composition process . The CoG Kit’s Karajan ( 26 ) includes mechanisms for the orchestration of tasks , parallel blocks , futures ( a place holder for a data product that has not been generated yet ) , loops , functional programming language , hierarchical graphs and parallel control structures , which are a particular focus of the system . It uses either a scripting language , GridAnt , a simple graphical editor to create the workﬂows . Kepler inherits a GUI for workﬂow composition and editing called Vergil from Ptolemy II with some modiﬁcations and extensions to make it more appropriate for scientiﬁc workﬂow composition . In this workﬂow editor , users can discover components called actors ; computational controllers called directors ; and data sets , discoverable through remote catalogs ; then drag them to the canvass and connect them via ports . Complex sub - workﬂows can be aggregated into so - called composite actors . Like atomic actors , they have well - deﬁned input and output ports as well as parameters . Parameters can be seen as special input ports that are usually , but not always , kept ﬁxed during a workﬂow execution . Like Triana and Taverna , Kepler supports the standard composition features such as copy , paste , etc . Kepler has greater ﬂexibility when it comes to representing ports for streaming data from a database type source , each column in a table can be exposed as a diﬀerent port , enabling a tuple or record to be broken up into individual ﬁelds ; alternatively the whole tuple can be sent as a single data object , typically what happens in Triana ; or thirdly a set of records can be sent as an array object . This functionality can be reproduced in other systems but normally requires speciﬁc components rather than the system itself to perform it . 3 . 1 . 4 . Semantic Composition Scientiﬁc workﬂows may consist of a number of execution components , where each com - ponent may run parameter sweep applications or similarly complex computations that involve dividing the data sets into smaller ones in order to support concurrent processing . Within such scenarios , the entire workﬂow can quickly run into several thousands of jobs for execution , especailly in systems which do not support abstract workﬂow deﬁnitions . In order to generate such workﬂows scientists can create ad hoc scripts that convert the iterative nature of sets of jobs into workﬂow variants but this can get rather complex since data needs to be moved to the correct location , executables need to be placed and 1 UK STFC TRIACS project ST / F002033 / 1 6 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT so on . Such an ad hoc approach therefore can run into problems when scaling to larger problem domains or translating to diﬀerent problems . Some scientists ( 27 ) have conducted research into providing fully automated work - ﬂow generation using artiﬁcial intelligence planning techniques for assisted workﬂow composition ( 28 ; 29 ) . This is achieved through combining semantic representations of workﬂow components with formal properties of correct workﬂows . The work has been motivated through working with two application domains : physics - based seismic hazard analysis ( 30 ) and data - intensive natural language processing ( 31 ) . Wings ( 8 ) uses rich semantic descriptions of components and workﬂow templates ex - pressed in terms of domain ontologies and constraints . Wings has a workﬂow template editor to compose components and their data ﬂow and that assists the user by enforcing the constraints speciﬁed for the workﬂow components . It also assists the user with data selection , to ensure the data sets selected conform to the requirements of the workﬂow template . With this information , Wings generates a workﬂow instance that speciﬁes the computations ( but not where they will take place ) and the new data products . For all the new data products , it generates metadata attributes by propagating metadata from the input data through the descriptions and constraints speciﬁed for each of the components . Other projects have used similar techniques in diﬀerent domains to support workﬂow composition through planning and automated reasoning ( 32 ; 33 ; 34 ) and semantic rep - resentations ( 35 ) . As workﬂow representations become more declarative and expressive , they enable signiﬁcant improvements in automation and assistance for workﬂow compo - sition and in general for managing and automating complex scientiﬁc processes . 3 . 2 . Workﬂow Representation Workﬂow representation can take a number of forms , at its most abstract level all workﬂows are merely a series of functional units , whether they are components , tasks , jobs or services , and the dependencies between them which deﬁne the order in which the units must be executed . There are a number of models from mathematics and computer science upon which the various workﬂow representation languages are based . The models include the ubiquitous directed graph variants , Petri nets ( 36 ) , Uniﬁed Modelling Language ( UML ) ( 37 ) , Business Process Modelling Notation ( BPMN ) ( 38 ) as well as several lesser known process modelling tools . The group of workﬂow languages based on these models changes very rapidly as new projects start and old ones become redundant so here we only provide a snapshot of the current state . The actual representation used should usually matter little to the average user of an individual workﬂow system until it comes to the matter of interoperability between workﬂow systems discussed in Section 7 . 3 . 3 . Directed Graphs The most common representation is the directed graph , either acyclic ( DAG ) or the less used cyclic ( DCG ) , which allow loops . Although many workﬂow systems use graphs , the actual representation of the graph in a language or format that can be used by the workﬂow tool varies , most systems use an XML based representation . In some cases , BPEL , the business domain workﬂow language for Web services , is being used directly for scientiﬁc workﬂows , examples include OMII - BPEL ( 39 ) , the e - HTPX project for high 7 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT throughput protein crystallography ( 40 ) and GPEL ( 41 ) used in LEAD . Condor’s DAG - Man is another commonly used DAG format for specifying workﬂows of dependent jobs , it is used by many users directly and is the underlying execution workﬂow representation for Pegasus . Pegasus uses a diﬀerent representation , the DAX ( DAG in XML ) , for its abstract workﬂow deﬁnition , whichi is the input to the Pegasus’ mapping process . The Taverna workﬂow environment concentrates on the workﬂow composition of Web services and uses an XML - based DAG format called SCUFLE . Triana uses a DCG format of its own although it is able to import and export from diﬀerent formats including DAGMan and the Virtual Data Language ( 42 ) . Sedna / Eclipse BPEL Designer uses an XML - based representation of BPEL workﬂows and syntactic aspects are deﬁned via XML Schemas . However , since BPEL is an event - driven workﬂow language based on messages , develop - ers can use simple state machine representation to capture state at any point in time : messages received , sent , waiting for and BPEL process activity . Therefore , a snapshot can be made of the state of BPEL process . Kepler uses yet another directed graph representa - tion MOML ( MOdel Markup Language ) , as with Kepler’s GUI , MOML is inherited from Ptolemy II . MOML is ﬂexible but was not designed with scientiﬁc workﬂow applications in mind so Kepler has developed a new workﬂow format called KAR ( Kepler Archives ) that uses MOML but also incorporates JAR / TAR archive ideas to make workﬂows more self - contained and easier to share . The idea of self contained workﬂow archives is cur - rently being addressed by both the Taverna and Triana projects , particularly in response to the myExperiment project and the ability to be able to share workﬂows in a portal environment . 3 . 4 . Petri Nets Petri nets are a specialised class of directed graph and models based on these have been used by several workﬂow systems . Petri Nets are a formalism for describing dis - tributed processes by extending state machines with concurrency , they graphically de - pict the structure of a distributed system as a directed bipartite graph with annotations . As such , a Petri Net has place nodes , transition nodes , and directed arcs connecting places with transitions . They are the basis for the Grid Workﬂow Description Language ( GWorkﬂowDL ) ( 43 ) the workﬂow language for K - Wf Grid project ( 44 ) , and the Grid Flow Description Language ( GFDL ) ( 45 ) from the Grid - Flow project ( 46 ) . 3 . 5 . UML The Uniﬁed Modelling Language ( UML ) is a standardised language for the modelling of object - oriented software . One of its aspects is an activity diagram which can be used to model dependencies between diﬀerent activities and hence can be used as a model for workﬂow . Askalon ( 47 ) uses UML activity diagrams for the graphical representation of its workﬂow applications . These are then translated into the Abstract Grid Workﬂow Language ( AGWL ) ( 48 ) an XML representation for execution and storage . 8 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT 3 . 6 . Workﬂow Execution Control Models Most if not all workﬂow models fall into one of two classes : control or data ﬂows . The two classes are similar in that they specify the interaction between individual activi - ties within the group that comprise the workﬂow , but they diﬀer in their methods of implementing that interaction . In control - driven workﬂows , or control ﬂows , the connections between the activities in a workﬂow represent a transfer of control from the preceding task to the one that follows . This includes control structures such as sequences , conditionals , and iterations . Data - driven workﬂows , or data ﬂows , are designed to support data - driven applications . The dependencies represent the ﬂow of data between workﬂow activities from data producer to data consumer . There is also a growing set of hybrid workﬂow representations based on a combination of control and data ﬂows . These hybrids use both types of dependencies as appropriate but are normally biased toward either data ﬂow or control ﬂow , using the other to better handle certain conditions . For instance , in a data - ﬂow system such as Triana , there are situations where a downstream task needs to be activated but the upstream task produces no output . In this case , a trigger is used to switch the ﬂow of control . In other hybrid control - ﬂow systems , such as the CoG Kit’s Karajan workﬂow , data dependencies can be represented by a future , the concept of data that has not yet been produced , which can block the control ﬂow with a data - ﬂow dependency . A diﬀerent , more ﬂexible approach is taken in Kepler , where the semantics of the workﬂow computation is a pluggable component . The user designs the workﬂow using various workﬂow components , known as actors and indicating the dependencies between them . However the semantics of the computation model are then indicated by the user based on the director , a speciﬁc model of computation , that a user chooses . In that way , Kepler supports a variety of semantics such as data ﬂow , control ﬂow , continuous time where the dependencies represent the the value of a continuous time signal at some point in time , and discrete event , where the workﬂow components communicate through a queue of events in time . Kepler can also nest the models , for example at the top - level workﬂow there may be a process network ( PN ) director for loosely coupled , independent processes ( possibly on diﬀerent machines ) or independent threads ( proxying for processes on diﬀerent machines ) , while at deeper levels inside of composite actors , more ﬁne - grained , local computations might be described using an SDF director , which executes in a single thread . 3 . 7 . Control Flow Model Most control ﬂow languages provide support not only for simple ﬂows of control be - tween components or services in the workﬂow but also for more complex control inter - actions such as loops and conditionals . Sometimes this support is implicit , as is the case with Petri Nets , and sometimes explicit , as in Karajan . Users of workﬂow systems will often want more than the simple control constructs available to them . The ability to branch workﬂow based on conditions and loop over sub - sections of the workﬂow repeatedly is important for all but the simplest of applications . The argument is not whether these facilities should exist but how to represent them in the 9 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT workﬂow language and to what degree the language should support them . For instance , is a single simple loop construct enough , or should the language support all loop types ? ( i . e . while , for . . . next , repeat . . . until . ) In the case of conditional behaviour , the problem is determining whether the incoming value and the conditional value are equivalent . XBaya supports a set of control primitives including for - each , conditional , while and exception . These control constructs are overlays on the dataﬂow graph to simplify the expression of the workﬂow . For example , a for - each construct can be used to encode a fan - out of a data ﬂow graph where the degree of fan - out is not known until runtime . An exception construct is required when the workﬂow designer needs to express a sub - workﬂow alter - native path when a part of the ﬂow may be subject to potential , known runtime failures . BPEL has control structures including branching and looping built into the language but only for pre - deﬁned fairly simple data types . For simple cases where we are comparing integers or simple strings , checking the condition is straightforward and unambiguous . The problem appears when we have to compare complex , structured scientiﬁc data in scientiﬁc workﬂows . This type of data often needs domain - speciﬁc knowledge in order to perform comparisons . Consequently , the evaluation of the comparison must be accom - plished by an external service or agent , or a separate component , as part of the workﬂow execution . 3 . 8 . Data - Flow Models Most data ﬂow representations are very simple in nature , and unlike their control ﬂow counterparts , contain nothing apart from component or service descriptions and the data dependencies between them ; control constructs such as loops are generally not included . In Triana’s workﬂow language , there are no control constructs at all ; the dependencies between tasks are data dependencies , ensuring the data producer has ﬁnished before the consumer may start . Looping and conditional behaviour is performed through the use of speciﬁc components ; a branch component with two or more output connections will output data on diﬀerent connections , depending upon some condition . Loops are handled by making a circular connection in the workﬂow and having a conditional component break the loop upon a ﬁnishing condition , outputting to continue normal workﬂow execu - tion . The beneﬁt of both of these solutions to control behaviour in data ﬂows is that the language representations remain simple . The downside is that the potential for running the workﬂow on diﬀerent systems is reduced since the other system must have access not only to the workﬂow but to the components or services that perform the control operations . 3 . 8 . 1 . Support for Data Collections Although most data ﬂow systems support the notion of single data object , some sys - tems also introduce the notion of data collections as ﬁrst - class entities in the workﬂow . The computational units in the workﬂow then operate not on single ﬁles for example , but on entire ﬁle collections . Examples of such systems are Askalon ( 49 ) and Wings ( 8 ) , which also supports the notion of nested collections and collections of processing com - ponents . Askalon supports mapping a portion of a data set to an activity , independently distributing multiple data sets , not necessarily with the same number of data elements , onto loop iterations . COMAD ( 50 ) , Collection Oriented Modelling and Design extensions 10 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT to Kepler support both nested data collections and collection token streams , where data streams can be nested using a technique similar to SAX - based parsing of XML documents using paired open and closing tokens to delineate the streams . At the user level there is essentially only one input and one output port per actor ; but actors can be conﬁgured to pick up only relevant parts of the “fat data stream . ” It is clear that both control and data ﬂow techniques are needed for scientiﬁc workﬂow languages . Limiting the language to one or the other limits the usefulness of the tools built to use the language . It is also clear that constantly extending the language to include every programming construct will bloat the language and increase the complexity of the engines needed to execute it . Simple hybrid models with limited control constructs and support for data ﬂow appear to stand the best chance of being interoperable with the most tools and frameworks but still contain enough functionality to be able to represent real scientiﬁc applications . However , inevitably there will not be a single model suitable for all situations . 4 . Mapping Workﬂows to Resources Workﬂow mapping refers to the process of generating an executable workﬂow based on a resource - independent workﬂow description , frequently called an abstract workﬂow or worfklow instance . In some case the user performs the mapping directly by selecting the appropriate resources . In other cases , the workﬂow system performs the mapping . In the latter case , users are allowed to design workﬂows at a level of abstraction above that of the target execution environment , which uses low - level commands and detailed resource management constructs . Depending on the underlying execution model that of standalone applications , or in - dividual services , diﬀerent approaches are taken to the mapping process . In the case of service - based workﬂows , mapping consists of ﬁnding and binding to services appropriate for the execution of a high - level functionality . Service - based workﬂows also can consider quality of service issues ( 51 ) when performing the mapping . In the case of workﬂows composed of stand - alone applications , the mapping issue not only involves ﬁnding the necessary resources to execute the computations and perform various optimizations but may also include modiﬁcations to the original workﬂow . 4 . 1 . Resource Deﬁnition and Discovery Workﬂow systems have diﬀerent meanings for the concept of resource . In many systems , a resource is a service that can perform one of the tasks in the workﬂow deﬁnition . This service may be rendered as a REST ( 52 ) or WSDL - based Web service ( 53 ) , or it may be a refer to a computational agent . A resource may even be a person . For example , workﬂows may require a human step , such as veriﬁcation by a scientist that the workﬂow is proceeding correctly or it may require an administrator’s approval prior to proceeding to the next step . In other cases a resource may be a computer upon which a required application is deployed or data archival system for storing results or containing required data sets . In the case of Web service resources , most workﬂow systems use a registry which con - tains descriptions of the services . As a new service is added to the application collection , 11 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT its description is added to the registry . Public Web service discovery search engines also exist and these can be useful in posting collections of Web services that are important for speciﬁc domains . For example there are a large number of public Web services for bioinformatics applications that are used by the Taverna system ( Section 3 . 1 . 3 ) . When the resources are computers and data systems , the resource discovery is typically done through a Grid information systems ( 54 ) , which can list all the computers available to a virtual organization ( 55 ) , their current load and status . As described below , this can be used by the mapping systems at runtime to select the best resource . 4 . 2 . User - deﬁned workﬂow mapping Among the workﬂow systems that rely on the user to make the choice of resources or services are : Kepler , Sedna , Taverna , and VisTrails ( 17 ) . In the case of Taverna , the user can provide a set of services which match a particular workﬂow component , so if errors occur , an alternate service can be automatically invoked . The newer versions of Taverna will include late service binding capabilities . Wings is a workﬂow composition tool which interfaces to the Pegasus workﬂow system and relies on its capabilities to perform the mapping of the Wings - generated abstract workﬂow onto the distributed resources . 4 . 3 . Workﬂow mapping using an internal scheduler or external broker Today , P - GRADE ( 56 ) can support both with task - and service - based workﬂows . The original version of the P - GRADE workﬂow engine that is based on Condor DAGMan supports only task - based workﬂows . The GEMLCA ( 57 ) extension of P - GRADE can support workﬂows where tasks and services can be applied in a mixed way within the same workﬂow . This extension of P - GRADE still uses DAGMan to control the node de - pendencies of the workﬂow graph but the task submission mechanism is extended with the services invocation mechanism of GEMLCA . When designing P - GRADE / GEMLCA workﬂows , the user is able to indicate that tasks / services can be scheduled by a bro - ker . P - GRADE interfaces to three diﬀerent brokers : GTBroker ( 58 ) for Globus - based deployments , the LHC - broker ( 59 ) for LHC - based grids , and the gLite - broker ( 60 ) for gLite - based grids . These brokers can be used in a mixed way within the same workﬂow and hence diﬀerent nodes of a P - GRADE workﬂow can simultaneously be executed on several grids ( 61 ) . When the user chooses a broker as a means of scheduling a job in the workﬂow , resource requirements for the job can be provided as well . An experimental enhancement of P - GRADE is combined with MOTEUR ( 62 ) by relying on two diﬀerent execution engines . For task - based workﬂows , it interfaces to DAGMan , for services - based workﬂows , it uses MOTEUR . MOTEUR is able to determine during execution the data parallelism present in workﬂow and exploit it if the necessary resources are available . Triana is able to interface to a variety of execution environments using the GAT ( Grid Application Toolkit ) ( 63 ) for task - based workﬂows and the GAP ( Grid Application Prototype ) ( 64 ) for service - based workﬂows . In the case of service - based workﬂow a user can provide the information about the services to invoke ( or locate them via a repository ) or a user can create a workﬂow and then map part of the workﬂow ( using a group ) to distributed services through the use of one of the internal scripts e . g . parallel or pipeline . In this mode , Triana distributes workﬂows by using ( or deploying on - the - ﬂy ) 12 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT distributed Triana services that can accept a Triana taskgraph as input . In the case of task - based workﬂow , the user can designate portions of the workﬂow as compute - intensive and Triana will send the tasks to the available resources for execution . It can for example use the GAT interface to the Gridlab GRMS broker ( 65 ) to perform the resource selection at runtime . Workﬂows can also be speciﬁed using a number of built - in scripts that can be used to map from a simple workﬂow speciﬁcation ( e . g . specifying a loop for example ) to multiple distributed resources in order to simplify the orchestration process for distributed rendering . Such scripts can map sub - workﬂows onto available resources by using any of the service - oriented bindings available e . g . WS - RF ( 66 ) , Web and P2P services using built - in deployment services for each binding . Karajan ( 67 ) supports dynamic binding of tasks to resources . When deﬁning a work - ﬂow , the user can specify the tasks at an abstract or concrete level , where a single workﬂow can be composed of a mix of tasks at diﬀerent levels of abstraction . The con - crete tasks are executed directly by an engine , whereas abstract tasks are sent at runtime to a scheduler for mapping onto a resource . Karajan supports pluggable schedulers and provides a simple built - in implementation . Karajan also supports user - deﬁned task clus - tering , where the cluster is sent to a single resource for execution . Similarly to Karajan , workﬂows speciﬁed in DAGMan can be a mixture of concrete and abstract tasks . When DAGMan is interfaced to a Condor task execution system ( 68 ) the abstract tasks are matched dynamically to the Condor resources . The selection is done by the Condor matchmaker ( 69 ) , which matches the requirements of an abstract task speciﬁed in a classad with the resource preferences published in their classads . In the case of workﬂows based on BPEL , resource selection can be explicit , i . e . spe - ciﬁc service instances are selected prior to workﬂow enactment . However , it is often the case that the abstract workﬂow is bound to abstract WSDL descriptions . The process of selecting resources can be left to an external agent which matches the abstract WSDL documents to concrete instances at runtime . In the LEAD project , the science appli - cations are deployed on a variety of remote supercomputers . Each remote application deployment is managed by a virtual Web service . The resource broker examines the exe - cution queues on the remote resources and selects the appropriate resource and realizes the corresponding virtual Web service and passes the service invocation from the BPEL engine . As described later , this has an additional advantage of allowing the broker to monitor execution and to reschedule the application invocation on a diﬀerent resource in case of failure . 4 . 4 . Workﬂow Optimizations Pegasus performs a mapping of the entire workﬂow , portions of the workﬂow , or in - dividual tasks onto the available resources . In the simplest case Pegasus chooses the sources of input data ( assuming that it can be replicated in the environment ) and the locations where the tasks are to be executed . Pegasus provides an interface to a user - deﬁned scheduler and includes four basic scheduling algorithms ( 70 ) : HEFT ( 71 ) , min - min , round - robin , and random . As with many scheduling algorithms , the quality of the schedule depends on the quality of the information both of the execution time of the tasks and data access as well as the information about the resources . In addition to the basic mapping algorithm , Pegasus can perform the following optimizations : tasks clustering , 13 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT data reuse , data cleanup , and partitioning ( 7 ) . Before the workﬂow mapping , the orig - inal workﬂow can be partitioned into any number of sub - workﬂows . The granularity of the partitioning sets the mapping horizon for the workﬂow . For example if we have only one task per partition , then this is equivalent to just - in - time scheduling . On the other end of the spectrum , having the entire workﬂow in one partitions results in full - ahead planning . Pegasus can also reuse intermediate data products if they are available and thus possibly reduce the amount of computation that needs to be performed . Pegasus also adds data cleanup nodes to the workﬂow , which remove the data at the execution sites when they are no longer needed ( 72 ) . This often results in a reduced workﬂow data storage footprint . Finally , Pegasus can also perform task clustering , treating a set of tasks as one for the purposed of scheduling to a remote location . The execution of the cluster at the remote site can be sequential or parallel ( if applicable ) . Task clustering can be beneﬁcial for ﬁne granularity computational workﬂows . Pegasus has also been used in conjunction with resource provisioning techniques to improve the overall workﬂow performance ( 30 ; 73 ; 74 ) . The Askalon system , designed to support task - level workﬂows , has a rich environment for mapping workﬂows onto resources . It not only does the resource assignment but can also automatically provision the resources ahead of the workﬂow execution . Askalon contains two major components responsible for workﬂow scheduling : the scheduler and the resource management system ( GridARM ) . GridARM serves as a data repository which provides the scheduler with all the information needed for scheduling , including the available resources and the applications deployed on the Grid . Apart from the ba - sic functionalities , GridARM can also provide more advanced resource and application discovery techniques based on quality - of - service matching , and it can guarantee advance reservation of resources . The scheduler uses the information obtained from GridARM in order to perform the mapping of high level tasks speciﬁed in AGWL . In order to sup - port the scheduler , Askalon has developed a performance analysis and prediction system which can estimate the runtime of the workﬂow tasks as well as data transfer times of data between tasks . The scheduler makes full - graph scheduling of scientiﬁc workﬂows , using one of the implemented scheduling algorithms . Currently , these include the HEFT algorithm , an advance reservation - based algorithm based on HEFT ( 75 ) , and a general - purpose bi - criteria scheduling algorithm . As the Scheduler maps DAGs and the AGWL workﬂows may include loops , parallel loops , and conditional constructs , a preliminary workﬂow conversion is performed before the actual scheduling can start . Among the im - plemented workﬂow transformations are techniques employed in parallel compilers and include : branch prediction , parallel loop unrolling , and sequential loop elimination . If the choices made during the transformation ( such as branch prediction ) were erroneous , the workﬂow is adjusted and rescheduled at runtime . Once the DAG is created , it is mapped onto the available resources based on a scheduling algorithm . 5 . Execution This section examines the execution of a workﬂow through the use of an execution engine or enactment subsystem . We cover three diﬀerent aspects : the execution model , the fault tolerance mechanisms and the ability to adapt workﬂows ( or sub - workﬂows ) based on previous analysis steps in the cycle . 14 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT 5 . 1 . Execution Model As discussed in Section 3 . 1 , workﬂow systems generally deal with services or jobs ( or combinations or both ) . In this section we outline the models used by some workﬂow sys - tems to illustrate the various execution scenarios that are currently available in systems . Pegasus can map workﬂows onto a variety of target resources such as those managed by PBS ( 76 ) , LSF ( 77 ) , Condor ( 78 ) , and individual machines . Authentication to remote resources is done via GSI ( 79 ) and Pegasus uses the DAGMan workﬂow engine for the ex - ecution of jobs . DAGMan interfaces to a local Condor queue managed by the schedd ( 80 ) . DAGMan uses the sched’s API and logs to submit , query , and manipulate jobs , and does not directly interact with jobs independently . DAGMan can also uses Condor’s Grid abilities ( Condor - G ) to submit to many other batch and grid systems . DAGMan reads the logs of the underlying batch system to follow the status of submitted jobs rather than invoking interactive tools or service APIs . By relying on ﬁle - based I / O DAGMan’s implementation can be simpler , more scalable and reliable across many platforms , and therefore more robust . Askalon supports workﬂow composition and modelling using the Uniﬁed Modelling Language ( UML ) standard and provides an XML - based Abstract Grid Workﬂow Lan - guage ( AGWL ) for application developers to use . The AGWL is given to a WSRF - based runtime system for scheduling and execution . Askalon contains a resource manager ( Gri - dARM based on the Globus tools ( 81 ) ) that provides resource discovery , advanced reser - vation and virtual organization - wide authorization along with a dynamic registration framework for activity types and activity deployments . The Java CoG Kit Karajan workﬂow framework can support hierarchical workﬂows based on DAGs with control structures and parallel constructs , it makes use of underlying Grid tools such as Globus GRAM ( 82 ) for the actual job submission . Workﬂows can be visualized and tracked by an engine and modiﬁed at runtime through interaction with a workﬂow repository or schedulers for dynamic allocation of resources to tasks . It has been demonstrated to scale to hundreds of thousands of jobs due to its eﬃcient scalability - oriented threading mechanisms . Triana supports job level execution through integration with the GridLab GAT , which can make use of GRMS , GRAM or Condor for the actual job submission ; it also supports service - level execution through the GAP bindings to Web , WS - RF and P2P Services ; and still contains an internal run - time execution engine for the local components written in Java or C . Kepler similarly supports Web services and Grid jobs through speciﬁc “actors” and local components through its own run - time engine . 5 . 2 . Fault Tolerance Fault tolerance again can be speciﬁc to the types of workﬂows being executed . At the job level , a number of mechanisms can be used at the operating system level for saving the state of an execution and resuming after a failure . Condor , for example , takes this approach . Other mechanisms can be employed , such as application - level check - pointing in order to migrate an execution to an other machine that may be more adept to executing the particular code . The Cactus worm ( 83 ) is a good illustration of this approach . For service - based systems , fault tolerance might involve retrying a service upon a time out 15 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT or discovering another equivalent service that may be running elsewhere in order to continue with the execution . More typically in most of the current workﬂow systems fault tolerance , exception handling and recovery are ad hoc tasks undertaken by individual workﬂow designers rather than being part of the systems themselves . Triana employs a passive approach by informing the user when a failure has occurred . The workﬂow can be debugged through examining the inbuilt provenance trace imple - mentation and through a debug screen that produces verbose output during the execution process . During the execution , Triana will identify failures for components and provide feedback to the user if a component fails but it does not contain fail - safe mechanisms within the system for retrying a service for example . Kepler also has little or no generic ca - pabilities for fault tolerance , individual workﬂow designers usually deal with the problem by encoding some of the exception handling and recovery mechanisms into the workﬂow itself . An extension to Kepler is being developed , a “smart re - run” feature based on data dependency information that can avoid unnecessary re - computations , similar to the way in which Pegasus works . Such a system would have similar knowledge about the work - ﬂow execution state even after an interruption so should enable the restart of a workﬂow from a check point . Kepler can also cope with unreliable data transfer , specifying a retry or fallback to an alternative transport protocol . This is a prototype of the “templates and frames” approach ( 84 ) which allows a structured composition of data - ﬂow networks and control - ﬂow ( state - machines / transducers ) . Yet another approach is easily handled in COMAD : if an upstream actor creates or detects a fault , a special error tag / token can be injected into the COMAD data stream . A downstream “exception catcher” can then specify what to do with the faulty data ; most actors would simply skip over data tagged as faulty . Askalon supports fallback , task - level recovery , checkpointing and workﬂow - level redun - dancy . With all of the workﬂow systems that use external components or services the level of checkpointing is limited unless the components themselves support checkpointing that the systems can access . Generally checkpointing is done at the workﬂow application level and so it is only possible to restart a workﬂow from between components and not within a component itself . Askalon provides an implementation of this “light weight” checkpointing by providing URLs to intermediate data sets . DAGMan also supports a number of recovery techniques . If DAGMan has crashed while submitting jobs to the underlying batch system , and the batch system continues to run jobs , DAGMan can recover its state upon restart ( by reading logs provided by the batch system ) without loosing information about the executing workﬂow . The DAG - Man workﬂow management includes not only job submission and monitoring but also job preparation , cleanup , throttling , retry , and other actions necessary to ensure the success - ful workﬂow execution ( 85 ) . DAGMan attempts to overcome or work around as many execution errors as possible , and in the face of errors it cannot overcome , it provides a rescue DAG and allows the user to resolve the problem manually and then resume the workﬂow from the point where it last left oﬀ . This can be thought of as a “checkpointing” of the workﬂow , just as some batch systems provide checkpointing of jobs . Another approach is to use an external monitoring system . For example , if resource selection is deferred to runtime then the agent which selects the resource can monitor the part of the workﬂow using that resource . If the resource or application fails on that resource it is possible for the resource selection services to bring in a substitute . This approach is used in the LEAD project when running on the TeraGrid distributed 16 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT computing resources ( 86 ) . The system is also capable of running multiple versions of the same workﬂow application component at the same time on diﬀerent and continuing the workﬂow with the ﬁrst instance that completes . This can improve throughput in time critical applications . 5 . 3 . Adaptive Workﬂow Although data - driven systems have been around since the 1980s , the term Dynamic Data Driven Application Systems ( DDDAS ) was coined during a National Foundation for Science ( NFS ) workshop in 2000 ( 87 ) . DDDAS describes the ability for an application to dynamically incorporate data into an executing application and also for that data or some analysis of that data to steer the application process . This allows data being produced in real time by sensors or various other instruments to be fed into the computational workﬂow in order to allow the system to respond to data changes on - the - ﬂy , which was considered a signiﬁcant challenge for Grid computing . DDDAS techniques can take a number of forms at its simplest this involves following one branch instead of another in the workﬂow based upon a decision made using the input data . At the other end of the scale this may involve dynamic rewriting of the workﬂow itself to create an entirely new workﬂow , however in traditional programming models self - modifying code has been considered with some suspicion because of the possibilities for abuse . DDDAS systems require decision - making capabilities to be included within the workﬂow process . At the very least logic processes ( e . g . if . . . then ) should be supported and looping constructs are also often needed to support detection of certain thresholds or states ( e . g . do . . . while loops or similar ) . Many of the workﬂow systems incorporate such features either at the language level or at the system level through speciﬁc components or constructs that support this type of behaviour . There are a number of domains in which DDDAS can be applied including manu - facturing process controls , resource management , weather and climate prediction , traf - ﬁc management , disaster control systems , systems engineering , civil engineering , geo - exploration , social and behavioural modelling , cognitive measurement and bio - sensing . The LEAD ( 19 ) project for example is addressing the fundamental IT research chal - lenges , and associated development , needed to create an integrated , scalable framework for identifying , accessing , preparing , assimilating , predicting , managing , analyzing , min - ing , and visualizing a broad array of meteorological data and model output independent of format and physical location . The storm modelling scenario is a good example of how this new paradigm aﬀects the whole range of infrastructures involved in a system . One possible use of these techniques is the situation where a computational solver of some description is used to explore a parameter space . If the solver is computationally expensive and the search space large , we could use a workﬂow with a course grained set of input parameters that explores the space at a high level . Areas of interest where the output data is evaluated to be above a certain threshold can be explored at a ﬁner grained level by spawning a sub - workﬂow with a new input parameter set . This use case has been explored in Triana using Aspect Oriented Programming ( AOP ) techniques to perform the workﬂow rewriting , eﬀectively creating sub - workﬂows that execute and feed back into the main workﬂow , and using Cactus as the computational solver . Other workﬂow tools could use this or other techniques to modify the workﬂow during execution . In principle 17 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Kepler workﬂows can modify themselves during execution . Ptolmey ( on which Kepler is based ) has some demos , e . g . where a variable number of instances of a “bank counter” are created ; this is a “predictable” change of the workﬂow . More exciting things can be done : Sending a workﬂow ( as if it were data ) from one actor to another , where it is then executed . Nothing prevents you from changing the workﬂow in - ﬂight . Users and tool builders have to be aware of the potential for abuse if open systems can be sent arbitrary workﬂows that can modify themselves , security and trust issues that have not been considered here then come into play . 6 . Provenance Data Provenance is a record of the history of the creation of a data object . If the data object was created as the result of a workﬂow then there must be a way to record the history of that creation . Speciﬁcally , the chain / graph of processes ( including time stamp , program version number , component or service version number , execution host , library versions , etc ) and intermediate data products back to the source data used to initialize the workﬂow . The importance of data provenance cannot be underestimated . A complete provenance record for a data object allows us the possibility to reproduce the result and reproducibil - ity is a critical component of the scientiﬁc method . Recently , several workﬂow systems and other provenance - related research projects par - ticipated in a series of Provenance Challenge Workshops ( 88 ) . From the challenge it became clear that the various system capture similar information about workﬂow execu - tion although they may present it in diﬀerent ways . Some systems use internal structures to manage provenance information , some rely on external services , which can be quiet generic . Within Triana , for example , provenance is recorded locally as an internal format that has various levels of output . It can show the components executed , their parameters and even record the data sets in the provenance trace that pass through during execu - tion . Triana is also integrated with external services such as those provided by the EU provenance project ( 89 ) . The Karma ( 90 ) provenance system is one that is largely workﬂow representation independent . Karma provides a searchable database of data provenance that has extensive capabilities for formulating data provenance queries . It can gather data from workﬂow systems in several diﬀerent ways , but the simplest is for it to listen for the the state change events published during the execution of a workﬂow . For example , the Web services used in the LEAD project are all instrumented to produce a stream of events each time they are invoked . These events are published as WS - Eventing messages that are sent to a notiﬁcation broker . The Karma system simply subscribes to the event stream . However , to make this work the workﬂow system must include workﬂow identiﬁcation information along with each remote service invocation . This identiﬁcation information is passed along in the service invocation events and is used by Karma to piece together the workﬂow history . Event streams are not the only way Karma works . Any workﬂow system that generates a detailed history of the actual execution could , in principle record that information in Karma . 18 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT 6 . 1 . Design Provenance In addition to tracking the provenance of data we can track the provenance of a workﬂow as it evolves from version to version . The VisTrails system ( 91 ) has an extensive set of capabilities for managing workﬂow design provenance , including the ability to explore variations on the design history or to see if two diﬀerent workﬂows may have common elements or how they evolved from a common root . The workﬂow design in VisTrails is done via a graphical user interface . When a user creates or modiﬁes the workﬂow the system captures the user - made modiﬁcations . These can be additions / deletions of nodes and dependencies , modiﬁcations to parameter set - tings , etc . As a result the user is able to trace back exactly how the workﬂow was created , even including the avenues explored but not ultimately pursued . This type of provenance can also be very beneﬁcial for workﬂow sharing or education , when collaborators are trying to understand how a particular workﬂow was created . 6 . 2 . Provenance for Transformed Workﬂow Execution Since workﬂow systems may signiﬁcantly modify the user - generated workﬂow before it is executed , the user may have trouble interpreting the results solely based on the execution provenance . For example , Pegasus is exploring the use of provenance tracking capabilities to record the workﬂow transformations performed during the workﬂow map - ping process . This provenance includes information about which nodes in the workﬂow were clustered , which instance of the input data was selected for the computation , what intermediate data was selected for reuse , which execution sites where selected and why , and other information related to the workﬂow mapping process . As part of this work , Pegasus has been integrated with the PASOA provenance system ( 92 ; 93 ) . 7 . Interoperability With so many workﬂow systems out there and this paper touches only upon some of them , can we make a case for workﬂow interoperability , and if so what would this entail ? Applications today can be composed of very heterogeneous components , some which involve having the user in the loop , some which deal with streaming data , some which require high - performance resources for their execution , etc . Currently , there is no single workﬂow system that can accommodate all these various requirements , just as there is no single programming language used for all applications . Therefore , workﬂow developers would like to be able to use a variety of engines for their work . Users do not necessarily want full interoperability between the various workﬂow systems , but they would like to invoke one workﬂow from another and to re - use their workﬂow descriptions . In scientiﬁc workﬂows , deﬁning and reﬁning a workﬂow to a point where it can be relied on to produce scientiﬁcally meaningful results can be extremely time - consuming and can take months or years of experimentation and validation . Yet if it is encoded completely in a language for a workﬂow system that becomes extinct the capability to execute it is lost . Of course this problem is a very general software longevity concern that goes well beyond workﬂows . However , if there is a standard workﬂow provenance model we could use that to encode the nature of the workﬂow execution independent 19 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT from the underling execution engine . if this representation is precise enough it may allow us to build “compilers” that can translate the provenance record directly back to a new workﬂow representation . 8 . Conclusion As e - Science applications have grown in complexity from simple batch executions of data analysis tasks , workﬂow has emerged as an important enabling technology . A host of tools supporting workﬂow design and enactment have been developed and are now in use in the scientiﬁc community . In many cases these scientiﬁc workﬂow systems were developed in close collaboration with the scientists and resulting systems are well designed to handle the use - cases of that community . Because scientiﬁc research is so diverse in the method used from one discipline to another , the resulting collection of workﬂow tools demonstrate a wide variety of capabilities . In this paper we attempted to demonstrate this diversity of capabilities found in e - Science workﬂow . We have ﬁrst explored it from the perspective of modes of expression . The language for deﬁning a workﬂow can be as traditional as the textual representations used in conventional programming . However scientists have found that a graphical com - position model is convenient for many applications . The most novel approaches may be those that compile a workﬂow from the speciﬁcation of a scientists high - level queries . This last area is one that is the subject of current research . We have also described workﬂow systems along the dimensions of internal represen - tation and execution . In many cases graphs are used for the internal model , but several other interesting representations are also described . Execution models also vary widely from system to system . In many cases a data - driven data - ﬂow model is used by the enactment engine . In other cases , data ﬂow is seen as limiting , so it is enhanced with various control constructs . Mapping a distributed scientiﬁc workﬂow to a set of computational and data resources is a task that may be done prior to execution , but it it may also be done on - the - ﬂy at runtime . In the later case , it is possible to be very adaptive in how the resources are selected and it may also enable better fault handling . Finally , having good workﬂow tools has enabled us to automate the process of building data and workﬂow provenance . Combined with good data catalogs and data management systems , it is now possible to provide complete experimental workbenches for entire com - munities of scientiﬁc users . Data and workﬂows can be shared and , through community use and reﬁnement , evolved to meet new challenges . Having data provenance allows a scientist to return to the point of creation of a data object to understand the workﬂow that created it and the original source of data that was used . 9 . Acknowledgements We would like to thank a number of contributors from the workﬂow systems we dis - cussed in this paper : Tom Oinn , Dave De Roure ad Carole Goble ( Taverna ) , Peter Kacsuk ( P - Grade ) , Gregor von Laszewski ( CoG Kit Karajan ) , Bruno Wassermann and Wolfgang Emmerich ( Eclipse BPEL Designer ) , Thomas Fahringer and Radu Prodan ( Askalon ) , 20 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Bertram Lud¨ascher ( Kepler ) . Finally , we would like to thank all the contributors to Workﬂows for eScience book that also helped with the input to this text . Ewa Deelman acknowledges the support of the National Science Foundation under grants : OCI - 0722019 and CCF - 0725332 . Ian Taylor and Matthew Shields also acknowl - edges support for Triana under the STFC GridOneD , TRIACS and DART grants and the OMII UK WHIP project for helping provide portal workﬂow plug - ins . References [ 1 ] J . Yu , R . Buyya , A Taxonomy of Workﬂow Management Systems for Grid Com - puting , Tech . Rep . GRIDS - TR - 2005 - 1 , Grid Computing and Distributed Systems Laboratory , University of Melbourne ( March 2005 ) . URL http : / / www . gridbus . org / reports / GridWorkflowTaxonomy . pdf [ 2 ] Y . Han , A . Sheth , C . Bussler , A Taxonomy of Adaptive Workﬂow Management , in : Conference on Computer – Supported Cooperative Work ( CSCW - 98 ) , Seattle , WA , 1998 . URL http : / / ccs . mit . edu / klein / cscw98 / [ 3 ] I . Taylor , E . Deelman , D . Gannon , M . Shields ( Eds . ) , Workﬂows for e - Science , Springer , New York , Secaucus , NJ , USA , 2007 . [ 4 ] T . Andrews , F . Curbera , H . Dholakia , Y . Goland , J . Klein , F . Leymann , K . Liu , D . Roller , D . Smith , S . Thatte , I . Trickovic , S . Weerawarana , Business Process Ex - ecution Language for Web Services Version 1 . 1 . [ 5 ] T . Oinn , M . Addis , J . Ferris , D . Marvin , M . Senger , M . Greenwood , T . Carver , K . Glover , M . R . Pocock , A . Wipat , P . Li , Taverna : A Tool for the Composition and Enactment of Bioinformatics Workﬂows , Bioinformatics 20 ( 17 ) ( 2004 ) 3045 – 3054 . [ 6 ] Condor Team , DAGMan : A Directed Acyclic Graph Manager , http : / / www . cs . wisc . edu / condor / dagman / ( July 2005 ) . [ 7 ] E . Deelman , G . Singh , M . - H . Su , J . Blythe , Y . Gil , C . Kesselman , G . Mehta , K . Vahi , G . B . Berriman , J . Good , A . Laity , J . C . Jacob , D . Katz , Pegasus : a Framework for Mapping Complex Scientiﬁc Workﬂows onto Distributed Systems , Scientiﬁc Pro - gramming Journal 13 ( 3 ) ( 2005 ) 219 – 237 . [ 8 ] Y . Gil , V . Ratnakar , E . Deelman , G . Mehta , J . Kim , Wings for Pegasus : Creating Large - Scale Scientiﬁc Applications Using Semantic Representations of Computa - tional Workﬂows , Proceedings of the 19th Annual Conference on Innovative Appli - cations of Artiﬁcial Intelligence ( IAAI ) Vancouver , British Columbia , Canada . [ 9 ] G . Berriman , J . Good , A . Laity , A . Bergou , J . Jacob , D . Katz , E . Deelman , C . Kessel - man , G . Singh , M . Su , et al . , Montage : A Grid Enabled Image Mosaic Service for the National Virtual Observatory , Astronomical Data Analysis Software and Sys - tems ( ADASS ) XIII . [ 10 ] G . Berriman , E . Deelman , J . Good , J . Jacob , D . Katz , C . Kesselman , A . Laity , T . Prince , G . Singh , M . Su , Montage : a grid - enabled engine for delivering custom science - grade mosaics on demand , Proceedings of SPIE 5493 ( 2004 ) 221 – 232 . [ 11 ] A . Lathers , M . Su , A . Kulungowski , A . Lin , G . Mehta , S . Peltier , E . Deelman , M . El - lisman , Enabling Parallel Scientiﬁc Applications with Workﬂow Tools , Proceedings 21 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT of Challenges of Large Applications in Distributed Environments ( CLADE ) . [ 12 ] J . Muench , et al . , SCEC Earthworks Science Gateway : Widening SCEC Community Access to the TeraGrid , TeraGrid 2006 Conference . [ 13 ] H . Lord , Improving the application development process with modular visualization environments , ACM SIGGRAPH Computer Graphics 29 ( 2 ) ( 1995 ) 10 – 12 . [ 14 ] S . G . Parker , M . Miller , C . D . Hansen , C . R . Johnson , An Integrated Problem Solving Environment : The SCIRun Computational Steering System , in : Proceedings of the 31st . Hawaii International Conference on System Sciences ( HICSS - 31 ) , 1998 , pp . 147 – 156 . [ 15 ] I . Altintas , C . Berkley , E . Jaeger , M . Jones , B . Lud¨ascher , S . Mock , Kepler : An Extensible System for Design and Execution of Scientiﬁc Workﬂows , in : 16th Inter - national Conference on Scientiﬁc and Statistical Database Management ( SSDBM ) , IEEE Computer Society , New York , 2004 , pp . 423 – 424 . [ 16 ] I . Taylor , M . Shields , I . Wang , A . Harrison , Visual Grid Workﬂow in Triana , Journal of Grid Computing 3 ( 3 - 4 ) ( 2005 ) 153 – 169 . [ 17 ] S . Callahan , J . Freire , E . Santos , C . Scheidegger , C . Silva , H . Vo , Managing the Evolution of Dataﬂows with VisTrails , IEEE Workshop on Workﬂow and Data Flow for Scientiﬁc Applications ( SciFlow 2006 ) . [ 18 ] S . Shirasuna , XBaya Workﬂow Composer , http : / / www . extreme . indiana . edu / xgws / xbaya . [ 19 ] K . Droegemeier , et al . , Service - Oriented Environments in Research and Education for Dynamically Interacting with Mesoscale Weather , Computing in Science and Engineering 7 ( 6 ) ( 2005 ) 12 – 29 . [ 20 ] Eclipse , Eclipse BPEL Project , see Web site at http : / / www . eclipse . org / bpel . [ 21 ] The myExperiment Project , http : / / www . myexperiment . org . [ 22 ] I . Taylor , Triana Generations , in : Scientiﬁc Workﬂows and Business workﬂow stan - dards in e - Science in conjunction with Second IEEE International Conference on e - Science , Amsterdam , Netherlands , 2006 . [ 23 ] I . Taylor , E . Al - Shakarchi , S . D . Beck , Distributed Audio Retrieval using Triana ( DART ) , in : International Computer Music Conference ( ICMC ) 2006 , November 6 - 11 , at Tulane University , USA . , 2006 , pp . 716 – 722 . [ 24 ] Data Mining Tools and Services for Grid Computing Environments ( DataMining - Grid ) , http : / / www . datamininggrid . org / . [ 25 ] A . Ali , O . Rana , I . Taylor , Web Services Composition for Distributed Data Mining , in : ICPP 2005 Workshops , International Conference Workshops on Parallel Process - ing , IEEE , New York , 2005 , pp . 11 – 18 . [ 26 ] G . von Laszewski , M . Hategan , Java CoG Kit Karajan / Gridant Workﬂow Guide , Tech . rep . , Technical Report , Argonne National Laboratory , Argonne , IL , USA , 2005 . [ 27 ] Y . Gil , Workﬂows for e - Science , Springer , New York , 2007 , Ch . Workﬂow Composi - tion : Semantic Representations for Flexible Automation , pp . 244 – 257 . [ 28 ] J . Kim , M . Spraragen , Y . Gil , An Intelligent Assistant for Interactive Workﬂow Com - position , in : IUI ’04 : Proceedings of the 9th international conference on Intelligent user interface , ACM Press , New York , 2004 , pp . 125 – 131 . [ 29 ] P . Maechling , H . Chalupsky , M . Dougherty , E . Deelman , Y . Gil , S . Gullapalli , V . Gupta , C . Kesselman , J . Kim , G . Mehta , B . Mendenhall , T . Russ , G . Singh , M . Spraragen , G . Staples , K . Vahi , Simplifying Construction of Complex Workﬂows for Non - Expert Users of the Southern California Earthquake Center Community 22 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Modeling Environment , ACM SIGMOD Record 34 ( 3 ) ( 2005 ) 24 – 30 . [ 30 ] P . Maechling , E . Deelman , L . Zhao , R . Graves , G . Mehta , N . Gupta , J . Mehringer , C . Kesselman , S . Callaghan , D . Okaya , H . Francoeur , V . Gupta , Y . Cui , K . Vahia , T . Jordan , E . Field , Workﬂows for e - Science , Springer , New York , 2007 , Ch . SCEC CyberShake Workﬂows – Automating Probabilistic Seismic Hazard Analysis Calcu - lations , pp . 143 – 166 . [ 31 ] K . Knight , D . Marcu , Machine Translation in the Year 2004 , in : Proceedings of the 2005 IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , Vol . 5 , IEEE Computer Society , New York , 2005 , pp . 965 – 968 . [ 32 ] S . Thakkar , J . L . Ambite , C . A . Knoblock , Composing , Optimizing , and Execut - ing Plans for Bioinformatics Web services , VLDB Journal , Special Issue on Data Management , Analysis and Mining for Life Sciences 14 ( 3 ) ( 2005 ) 330 – 353 . [ 33 ] D . McDermott , Estimated - Regression Planning for Interactions with Web Services , in : M . Ghallab , J . Hertzberg , P . Traverso ( Eds . ) , 6th International Conference on Artiﬁcial Intelligence Planning and Scheduling , AAAI Press , Menlo Park , CA , 2002 . [ 34 ] S . McIlraith , T . Son , Adapting Golog for Programming in the Semantic Web , in : Fifth International Symposium on Logical Formalizations of Commonsense Reason - ing , In press , 2001 , pp . 195 – 202 . [ 35 ] myGrid , http : / / www . mygrid . org . uk / . [ 36 ] ISO / IEC 15909 - 1 , High - Level Petri Nets — Part 1 : Concepts , Deﬁnitions and Graph - ical Notation ( 2004 ) . [ 37 ] M . Fowler , K . Scott , UML Distilled , Addison - Wesley , 1997 . [ 38 ] T . Fletcher , C . Ltd , P . Furniss , A . Green , R . Haugen , BPEL and Business Transac - tion Management : Choreology Submission to OASIS WS - BPELTechnical Commit - tee . , Published on Web . [ 39 ] W . Emmerich , B . Butchart , L . Chen , B . Wassermann , S . L . Price , Grid Service Orchestration using the Business Process Execution Language ( BPEL ) , Journal of Grid Computing 3 ( 3 – 4 ) ( 2005 ) 283 – 304 . [ 40 ] R . Allan , D . Meredith , e - HTPX – HPC , Grid and Web - Portal Technologies in High Throughput Protein Crystallography , Proceedings of the UK e - Science All Hands Meeting , Nottingham , UK . [ 41 ] Y . Wang , C . Hu , J . Huai , A new grid workﬂow description language , Proc . of the 2005 IEEE Int’l Conf . on Services Computing 2 257 – 260 . [ 42 ] I . Foster , J . Voeckler , M . Wilde , Y . Zhao , Chimera : A Virtual Data System for Representing , Querying , and Automating Data Derivation , in : 14th International Conference on Scientiﬁc and Statistical Database Management ( SSDBM’02 ) , IEEE Computer Society Press , New York , 2002 , pp . 37 – 46 . [ 43 ] M . Alt , A . Hoheisel , H . - W . Pohl , S . Gorlatch , A Grid Workﬂow Language Using High - Level Petri Nets , in : R . Wyrzykowski , J . Dongarra , N . Meyer , J . Wasniewski ( Eds . ) , Proceedings of the 6th International Conference on Parallel Processing and Applied Mathematics PPAM’2005 , Vol . 3911 of Lecture Notes in Computer Science , Springer , New York , 2006 , pp . 715 – 722 . [ 44 ] KWF . URL http : / / www . kwfgrid . eu / [ 45 ] Z . Guan , et al . , Grid - ﬂow : A Grid - enabled Scientiﬁc Workﬂow System with a Petri Net - based Interface , Ph . D . thesis , University of Alabama at Birmingham ( 2006 ) . [ 46 ] GridFlow Home Page . 23 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT URL http : / / gridflow . ca / [ 47 ] T . Fahringer , R . Prodan , R . Duan , F . Nerieri , S . Podlipnig , J . Qin , M . Siddiqui , H . - L . Truong , A . Villazon , M . Wieczorek , ASKALON : A Grid Application Development and Computing Environment , in : 6th International Workshop on Grid Computing , IEEE Computer Society Press , New York , 2005 , pp . 122 – 131 . [ 48 ] T . Fahringer , J . Qin , S . Hainzer , Speciﬁcation of Grid Workﬂow Applications with AGWL : An Abstract Grid Workﬂow Language , in : International Symposium on Cluster Computing and the Grid ( CCGRID 2005 ) , Vol . 2 , IEEE Computer Society Press , New York , 2005 , pp . 676 – 685 . [ 49 ] J . Qin , T . Fahringer , Advanced Data Flow Support for Scientiﬁc Grid Workﬂow Applications , In Proceedings of the International Conference on High Performance Computing , Networking , Storage and Analysis ( Supercomputing 2007 , SC—07 ) . [ 50 ] S . Bowers , T . McPhillips , B . Ludaescher , A Provenance Model for Collection - Oriented Scientiﬁc Workﬂows , Concurrency and Computation : Practice and Ex - perience . [ 51 ] I . Brandic , S . Pllana , S . Benkner , An approach for the high - level speciﬁcation of QoS - aware grid workﬂows considering location aﬃnity , Scientiﬁc Programming 14 ( 3 ) ( 2006 ) 231 – 250 . [ 52 ] R . T . Fielding , Architectural Styles and the Design of Network - based Software Ar - chitectures , Ph . D . thesis , University of California , Irvine ( 2000 ) . URL http : / / www . ics . uci . edu / ~ fielding / pubs / dissertation / top . htm [ 53 ] Web Services Description Language ( WSDL ) 1 . 1 , Tech . rep . , W3C ( 2001 ) . URL http : / / www . w3 . org / TR / wsdl [ 54 ] K . Czajkowski , S . Fitzgerald , I . Foster , C . Kesselman , Grid Information Services for Distributed Resource Sharing , in : HPDC ’01 : Proceedings of the 10th IEEE International Symposium on High Performance Distributed Computing ( HPDC - 10’01 ) , IEEE Computer Society , Washington , 2001 , pp . 181 – 184 . [ 55 ] I . Foster , C . Kesselman , S . Tuecke , The Anatomy of the Grid : Enabling Scalable Virtual Organization , The International Journal of High Performance Computing Applications 15 ( 3 ) ( 2001 ) 200 – 222 . [ 56 ] A . Kert´esz , G . Sipos , P . Kacsuk , Brokering Multi - grid Workﬂows in the P - GRADE Portal , in : Euro - Par 2006 : Parallel Processing , Vol . 4375 , Springer , Berlin , 2007 , pp . 138 – 149 . [ 57 ] T . Delaitre , T . Kiss , A . Goyeneche , G . Terstyanszky , S . Winter , P . Kacsuk , GEMLCA : Running Legacy Code Applications as Grid Services , Journal of Grid Computing 3 ( 1 – 2 ) ( 2005 ) 75 – 90 . [ 58 ] A . Kertesz , Brokering solutions for Grid middlewares , Pre - proc . of 1st Doctoral Workshop on Mathematical and Engineering Methods in Computer Science . [ 59 ] A . Peris , P . Lorenzo , F . Donno , A . Sciab , S . Campana , R . Santinelli , LCG - 2 User Guide , LHC Computing Grid Manuals Series . Available via https : / / edms . cern . ch / file / 454439 / / LCG - 2 - UserGuide . pdf . [ 60 ] S . Burke , S . Campana , A . Peris , F . Donno , P . Lorenzo , R . Santinelli , A . Sciaba , gLite 3 . 0 User Guide ( 2006 ) . [ 61 ] P . Kacsuk , T . Kiss , G . Sipos , Solving the Grid Interoperability Problem by P - GRADE Portal at Workﬂow Level , Proc . of the Grid - Enabling Legacy Applications and Supporting End User Workshop , in conjunction with HPDC 6 3 – 7 . [ 62 ] T . Glatard , G . Sipos , J . Montagnat , Z . Farkas , P . Kacsuk , Workﬂows for e - Science , 24 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Springer , New York , 2007 , Ch . Workﬂow - Level Parametric Study Support by MO - TEUR and the P - GRADE Portal , pp . 279 – 299 . [ 63 ] G . Allen , K . Davis , K . N . Dolkas , N . D . Doulamis , T . Goodale , T . Kielmann , A . Merzky , J . Nabrzyski , J . Pukacki , T . Radke , M . Russell , E . Seidel , J . Shalf , I . Taylor , Enabling Applications on the Grid : A GridLab Overview , International Journal of High Performance Computing Applications : Special Issue on Grid Com - puting : Infrastructure and Applications 17 ( 4 ) ( 2003 ) 449 – 466 . [ 64 ] I . Taylor , M . Shields , I . Wang , O . Rana , Triana Applications within Grid Computing and Peer to Peer Environments , Journal of Grid Computing 1 ( 2 ) ( 2003 ) 199 – 217 . [ 65 ] J . Nabrzyski , Grid ( Lab ) Resource Management System ( GRMS ) , Tech . rep . , Grid - Lab ( 2004 ) . URL http : / / www . man . poznan . pl / coe / documents / Gridlab \ _ GRMS \ _ WP . pdf [ 66 ] K . Czajkowski , D . F . Ferguson , I . Foster , J . Frey , S . Graham , I . Sedukhin , D . Snelling , S . Tuecke , W . Vambenepe , The WS - Resource Framework , Tech . rep . , The Globus Alliance ( 2004 ) . [ 67 ] G . von Laszewski , M . Hategan , D . Kodeboyina , Workﬂows for e - Science , Springer , New York , 2007 , Ch . Java CoG Kit Workﬂow , pp . 143 – 166 . [ 68 ] P . Couvares , T . Kosar , A . Roy , J . Weber , K . Wenger , Workﬂows for e - Science , Springer , New York , 2007 , Ch . Workﬂow Management in Condor , pp . 357 – 375 . [ 69 ] D . Epema , M . Livny , R . van Dantzig , X . Evers , J . Pruyne , A Worldwide Flock of Condors : Load Sharing Among Workstation Clusters , Future Generation Computer Systems ( Special Issue : Resource Management in Distributed Systems ) 12 ( 1 ) ( 1996 ) 53 – 65 . [ 70 ] T . Braun , H . Siegel , N . Beck , L . B¨ol¨oni , M . Maheswaran , A . Reuther , J . Robertson , M . Theys , B . Yao , D . Hensgen , et al . , A Comparison of Eleven Static Heuristics for Mapping a Class of Independent Tasks onto Heterogeneous Distributed Computing Systems , Journal of Parallel and Distributed Computing 61 ( 6 ) ( 2001 ) 810 – 837 . [ 71 ] H . Topcuoglu , S . Hariri , M . Wu , Performance - Eﬀective and Low - Complexity Task Scheduling for Heterogeneous Computing . [ 72 ] A . Ramakrishnan , G . Singh , H . Zhao , E . Deelman , R . Sakellariou , K . Vahi , K . Black - burn , D . Meyers , M . Samidi , Scheduling data - intensiveworkﬂows onto storage - constrained distributed resources , in : CCGRID ’07 : Proceedings of the Seventh IEEE International Symposium on Cluster Computing and the Grid , IEEE Computer So - ciety , Washington , DC , USA , 2007 , pp . 401 – 409 . [ 73 ] G . Singh , C . Kesselman , E . Deelman , Adaptive pricing for resource reservations in shared environments , in : GRID , 2007 , pp . 74 – 80 . [ 74 ] G . Singh , C . Kesselman , E . Deelman , A provisioning model and its comparison with best - eﬀort for performance - cost optimization in grids , in : HPDC ’07 : Proceedings of the 16th international symposium on High performance distributed computing , ACM , New York , NY , USA , 2007 , pp . 117 – 126 . [ 75 ] M . Wieczorek , M . Siddiqui , A . Villazon , R . Prodan , T . Fahringer , Applying Ad - vance Reservation to Increase Predictability of Workﬂow Execution on the Grid , in : E - SCIENCE ’06 : Proceedings of the Second IEEE International Conference on e - Science and Grid Computing , IEEE Computer Society , Washington , DC , USA , 2006 , p . 82 . [ 76 ] R . Henderson , D . Tweten , Portable Batch System : External reference speciﬁcation , Ames Research Center . 25 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT [ 77 ] S . Zhou , LSF : load sharing in large - scale heterogeneous distributed systems , Proc . Workshop on Cluster Computing ( 1992 ) 1995 – 1996 . [ 78 ] M . Litzkow , M . Livny , M . Mutka , Condor — A Hunter of Idle Workstations , in : Proceedings of the 8th International Conference on Distributed Computing Systems , IEEE Computer Society , New York , 1988 , pp . 104 – 111 . [ 79 ] V . Welch , F . Siebenlist , I . Foster , J . Bresnahan , K . Czajkowski , J . Gawor , C . Kessel - man , S . Meder , L . Pearlman , S . Tuecke , Security for Grid Services , in : Twelfth In - ternational Symposium on High Performance Distributed Computing ( HPDC - 12 ) , IEEE Computer Society Press , New York , 2003 , pp . 48 – 57 . [ 80 ] J . Meehean , M . Livny , A service migration case study : Migrating the Condor schedd , in : Midwest Instruction and Computing Symposium , 2005 . [ 81 ] The Globus Alliance , see Web site at http : / / www . globus . org . [ 82 ] K . Czajkowski , I . Foster , N . Karonis , C . Kesselman , S . Martin , W . Smith , S . Tuecke , A Resource Management Architecture for Metacomputing Systems , in : D . G . Fei - telson , L . Rudolph ( Eds . ) , IPPS / SPDP ’98 : Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing , Vol . 1459 of Lecture Notes in Com - puter Science , Springer Verlag , London , 1998 , pp . 62 – 82 . URL http : / / www . globus . org [ 83 ] G . Allen , D . Angulo , I . Foster , G . Lanfermann , C . Liu , T . Radke , E . Seidel , J . Shalf , The Cactus Worm : Experiments with dynamic resource discovery and allocation in a Grid environment , The International Journal of High Performance Computing Applications 15 ( 4 ) ( 2001 ) 345 – 358 . URL citeseer . ist . psu . edu / article / allen01cactus . html [ 84 ] S . Bowers , B . Ludaescher , A . Ngu , T . Critchlow , Enabling Scientiﬁc Workﬂow Reuse through Structured Composition of Dataﬂow and Control - Flow , Proceedings of the 22nd International Conference on Data Engineering Workshops ( ICDEW’06 ) - Volume 00 . [ 85 ] P . Couvares , T . Kosar , A . Roy , J . Weber , K . Wenger , Workﬂows for e - Science , Springer , New York , 2007 , Ch . Workﬂow Management in Condor , pp . 357 – 375 . [ 86 ] The TeraGrid Project , http : / / www . teragrid . org / . URL http : / / www . teragrid . org / [ 87 ] F . Darema , Grid Computing and Beyond : The Context of Dynamic Data Driven Applications Systems , in : Proceedings of the IEEE , Vol . 93 , March 2005 , pp . 692 – 697 . [ 88 ] R . Bose , I . Foster , L . Moreau , Report on the International Provenance and Anno - tation Workshop : ( IPAW’06 ) 3 - 5 May 2006 , Chicago , ACM SIGMOD Record 35 ( 3 ) ( 2006 ) 51 – 53 . [ 89 ] L . Chen , V . Tan , F . Xu , A . Biller , P . Groth , S . Miles , J . Ibbotson , L . Moreau , A proof of concept : Provenance in a service oriented architecture , Proceedings of the UK OST e - Science Second All Hands Meeting 2005 ( AHM05 ) . [ 90 ] Y . L . Simmhan , B . Plale , D . Gannon , Performance Evaluation of the Karma Prove - nance Framework for Scientiﬁc Workﬂows , in : International Provenance and Anno - tation Workshop ( IPAW ) , Springer , Berlin , 2006 . [ 91 ] S . Callahan , J . Freire , E . Santos , C . Scheidegger , C . Silva , H . Vo , VisTrails : visualiza - tion meets data management , Proceedings of the 2006 ACM SIGMOD international conference on Management of data ( 2006 ) 745 – 747 . [ 92 ] S . Miles , E . Deelman , P . Groth , K . Vahi , G . Mehta , L . Moreau , Connecting Scientiﬁc 26 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Data to Scientiﬁc Experiments with Provenance . [ 93 ] S . Miles , P . Groth , E . Deelman , K . Vahi , G . Mehta , L . Moreau , Provenance : The bridge between experiments and data , Computing in Science and Engineering 10 ( 3 ) ( 2008 ) 38 – 46 . 27 A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Figure ( s ) A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Ewa Deelman is an Assistant Research Professor at the USC Computer Science Department and a Project Leader at the USC Information Sciences Institute . Dr . Deelman ' s research interests include the design and exploration of collaborative scientific environments based on distributed technologies , with particular emphasis on workflow management as well as the management of large amounts of data and metadata . At ISI , Dr . Deelman is leading the Pegasus project , which designs and implements workflow mapping techniques for large - scale workflows running in distributed environments . Prior to joining ISI in 2000 , she was a Post Doctoral fellow at UCLA conducting research in the area of performance prediction of large - scale applications on high performance machines . Dr . Deelman received her PhD from Rensselaer Polytechnic Institute in Computer Science in 1997 in the area of parallel discrete event simulation . Dr . Deelman is an Associate Editor responsible for Grid Computing for the Scientific Programming Journal and a chair of the OGF Workflow Management Research Group . Ewa Deelman Biog A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT DeelmanClick here to download high resolution image A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Gannon ' s research interests include cyberinfrastructure , programming systems and tools , distributed computing , computer networks , parallel programming , computational science , problem solving environments and performance analysis of Grid and MPP systems . He led the DARPA HPC + + project and he was one of the architects of the Department of Energy SciDAC Common Component Architecture ( CCA ) project . This work has led to a framework for building component - based scientific applications . He was a partner in the NSF Computational Cosmology Grand Challenge project and the NCSA Alliance where he is helped to lead an effort to design Grid " Portals " which are desktop frameworks for Grid access . He was a co - founder the Java Grande Forum . He is the co - chair of the Global Grid Forum working groups on Grid Computing Environments and he was on the steering committee of the GGF . He is currently on the Executive Steering Committee of the NSF Teragrid Project and deeply involved with the Science Gateways efforts there . He is also a co - pi on the NSF LEAD project which is building cyberinfrastructure for dynamic , adaptive weather prediction . He is the Science Director for the Indiana Pervasive Technology Labs and the past Chair of the Department of Computer Science at Indiana University . Dr . Gannon was the Program Chair for the IEEE 2002 High Performance Distributed Computing Conference . He also served as General Chair of the 1998 International Symposium on Scientific Object Oriented Programming Environments ( ISCOPE ) and the 2000 ACM Java Grande Conference , and Program Chair for the 1997 ACM International Conference on Supercomputing as well as the 1995 IEEE Frontiers of Massively Parallel Processing . He was the Program Chair for the International Grid Conference , Barcelona , 2006 . Gannon biog A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT gannonClick here to download high resolution image A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Matthew is currently a Senior Research Associate at the Computer Science at Cardiff University . His research is concerned with the investigation of technologies to help developers and application scientists utilise Grid computing and he is the main developer of the Triana workflow environment , discussed in this paper . His research interests include loosely coupled , standards based message passing interfaces for distributed computing , such as Web and Grid services , and Peer to Peer computing . Matthew is one of the co - editors for Workflows for eScience , published by Springer . Matthew Shields Biog A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Matt Shields Photo Click here to download high resolution image A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Ian Taylor is a Senior lecturer in distributed systems at the School of Computer Science and concurrently holds an adjunct Assistant Professorship at the Center for Computation & Technology at Louisiana State University . He has a Ph . D . in Physics and Music and is the co - ordinator of Triana activities at Cardiff ( http : / / www . trianacode . org ) . Through this he has been active in many major projects including GridLab , CoreGrid and GridOneD applying distributed techniques and workflow for Grid and P2P computing in application areas ranging from healthcare and distributed audio to astrophysics . Ian is a member of oversight committee for DPAC project providing data analysis for the ESA GAIA mission to map the Galaxy . Ian has written a professional book for Springer entitled P2P , Web Services and Grids and has been the lead edited another called Workflows for eScience with Dennis Gannon , Ewa Deelman and Matthew Shidles . He has published over 50 scientific papers , been a guest editor for the Journal of Grid Computing on workflow and he is a co - chair for the Workflow Management Research Group in the OGF . taylor A CC EP T E D M A N U S C R I P T ACCEPTED MANUSCRIPT Ian Taylor Photo Click here to download high resolution image