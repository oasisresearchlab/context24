Live Video Analytics at Scale with Approximation and Delay - Tolerance Haoyu Zhang (cid:63) † , Ganesh Ananthanarayanan (cid:63) , Peter Bodik (cid:63) , Matthai Philipose (cid:63) Paramvir Bahl (cid:63) , Michael J . Freedman † (cid:63) Microsoft † Princeton University Abstract Video cameras are pervasively deployed for security and smart city scenarios , with millions of them in large cities worldwide . Achieving the potential of these cam - eras requires efﬁciently analyzing the live videos in real - time . We describe VideoStorm , a video analytics system that processes thousands of video analytics queries on live video streams over large clusters . Given the high costs of vision processing , resource management is cru - cial . We consider two key characteristics of video ana - lytics : resource - quality tradeoff with multi - dimensional conﬁgurations , and variety in quality and lag goals . VideoStorm ’s ofﬂine proﬁler generates query resource - quality proﬁle , while its online scheduler allocates re - sources to queries to maximize performance on quality and lag , in contrast to the commonly used fair sharing of resources in clusters . Deployment on an Azure clus - ter of 101 machines shows improvement by as much as 80 % in quality of real - world queries and 7 × better lag , processing video from operational trafﬁc cameras . 1 Introduction Video cameras are pervasive ; major cities worldwide like New York City , London , and Beijing have millions of cameras deployed [ 8 , 12 ] . Cameras are installed in build - ings for surveillance and business intelligence , while those deployed on streets are for trafﬁc control and crime prevention . Key to achieving the potential of these cam - eras is effectively analyzing the live video streams . Organizations that deploy these cameras—cities or po - lice departments—operate large clusters to analyze the video streams [ 5 , 9 ] . Sufﬁcient bandwidth is provisioned ( ﬁber drops or cellular ) between the cameras and the cluster to ingest video streams . Some analytics need to run for long periods ( e . g . , counting cars to control trafﬁc light durations ) while others for short bursts of time ( e . g . , reading the license plates for AMBER Alerts , which are raised in U . S . cities to identify child abductors [ 1 ] ) . Video analytics can have very high resource demands . Tracking objects in video is a core primitive for many scenarios , but the best tracker [ 69 ] in the VOT Challenge 2015 [ 59 ] processes only 1 frame per second on an 8 - core machine . Some of the most accurate Deep Neural Networks for object recognition , another core primitive , require 30GFlops to process a single frame [ 75 ] . Due to the high processing costs and high data - rates of video streams , resource management of video analytics queries is crucial . We highlight two properties of video analytics queries relevant to resource management . Resource - quality trade - off with multi - dimensional conﬁgurations . Vision algorithms typically contain various parameters , or knobs . Examples of knobs are video resolution , frame rate , and internal algorithmic pa - rameters , such as the size of the sliding window to search for objects in object detectors . A combination of the knob values is a query conﬁguration . The conﬁguration space grows exponentially with the number of knobs . Resource demand can be reduced by changing conﬁgu - rations ( e . g . , changing the resolution and sliding window size ) but they typically also lower the output quality . Variety in quality and lag goals . While many queries require producing results in real - time , others can tolerate lag of even many minutes . This allows for temporarily reallocating some resources from the lag - tolerant queries during interim shortage of resources . Such shortage hap - pens due to a burst of new video queries or “spikes” in resource usage of existing queries ( for example , due to an increase in number of cars to track on the road ) . Indeed , video analytics queries have a wide variety of quality and lag goals . A query counting cars to control the trafﬁc lights can work with moderate quality ( approx - imate car counts ) but will need them with low lag . Li - cense plate readers at toll routes [ 16 , 17 ] , on the other hand , require high quality ( accuracy ) but can tolerate lag of even many minutes because the billing can be delayed . However , license plate readers when used for AMBER Alerts require high quality results without lag . Scheduling large number of streaming video queries with diverse quality and lag goals , each with many con - ﬁgurations , is computationally complex . Production systems for stream processing like Storm [ 4 ] , Stream - Scope [ 62 ] , Flink [ 2 ] , Trill [ 36 ] and Spark Stream - ing [ 89 ] allocate resources among multiple queries only based on resource fairness [ 7 , 10 , 27 , 43 , 51 ] common to cluster managers like Yarn [ 3 ] and Mesos [ 49 ] . While simple , being agnostic to query quality and lag makes fair sharing far from ideal for video stream analytics . We present VideoStorm , a video analytics system that scales to processing thousands of live video streams over large clusters . Users submit video analytics queries con - taining many transforms that perform vision signal pro - cessing on the frames of the incoming video . At its core , VideoStorm contains a scheduler that efﬁciently gener - ates the query’s resource - quality proﬁle for its different knob conﬁgurations , and then jointly maximizes the qual - ity and minimizes the lag of streaming video queries . In doing so , it uses the generated proﬁles , and lag and qual - ity goals . It allocates resources to each query and picks its conﬁguration ( knob values ) based on the allocation . Challenges and Solution . The major technical chal - lenges for designing VideoStorm can be summarized as follows : ( i ) There are no analytical models for resource demand and quality for a query conﬁguration , and the large number of conﬁgurations makes it expensive to even estimate the resource - quality proﬁle . ( ii ) Express - ing quality and lag goals of individual queries and across all queries in a cluster is non - trivial . ( iii ) Deciding al - locations and conﬁgurations is a computationally hard problem exponential in the number of queries and knobs . To deal with the multitude of knobs in video queries , we split our solution into ofﬂine ( or proﬁling ) and online phases . In the ofﬂine phase , we use an efﬁcient proﬁler to get the resource - quality proﬁle of queries without ex - ploring the entire combinatorial space of conﬁgurations . Using greedy search and domain - speciﬁc sampling , we identify a handful of knob conﬁgurations on the Pareto boundary of the proﬁle . The scheduler in the online phase , thus , has to consider only these conﬁgurations . We encode quality and lag goals of a query in a util - ity function . Utility is a weighted combination of the achieved quality and lag , with penalties for violating the goals . Penalties allow for expressing priorities between queries . Given utilities of multiple queries , we schedule for two natural objectives – maximize the minimum util - ity , or maximize the total utility . The former achieves fairness ( max - min ) while the latter targets performance . Finally , in the online phase , we model the scheduling problem using the Model - Predictive Control [ 67 ] to pre - dict the future query lag over a short time horizon , and use this predicted lag in the utility function . The sched - uler considers the resource - quality proﬁle of queries dur - ing allocation , and allows for lagging queries to “catch up . ” It also deals with inevitable inaccuracies in resource usages in the resource - quality proﬁles . While we focus VideoStorm on video analytics using computer vision algorithms , approximation and lag are aspects that are fundamental to all machine learning al - gorithms . To that end , the techniques in our system are broadly applicable to all stream analytics systems that employ machine learning techniques . Machine Manager Machine Manager Worker Process transform track object transform classify object Worker 2 Worker Process transform detect license plates Worker Process transform decode transform b / g subtract control flow data flow query1 query2 Worker 1 VideoStorm Manager Scheduler + Profiler Machine Manager transform decode Machine Manager Figure 1 : VideoStorm System Architecture . Contributions . Our contributions are as follows : 1 . We designed and built a system for large - scale an - alytics of live video that allows users to submit queries with arbitrary vision processors . 2 . We efﬁciently identify the resource - quality proﬁle of video queries without exhaustively exploring the combinatorial space of knob conﬁgurations . 3 . We designed an efﬁcient scheduler for video queries that considers their resource - quality proﬁle and lag tolerance , and trades off between them . We considered streaming databases with approxima - tion [ 19 , 37 , 68 ] as a starting point for our solution . How - ever , they only consider the sampling rate of data streams and used established analytical models [ 38 ] to calculate the quality and resource demand . In contrast , vision queries are more complex black - boxes with many more knobs , and do not have known analytical models . More - over , they optimize only one query at a time , while our focus is on scheduling multiple concurrent queries . Deployment on 101 machines in Azure show that VideoStorm ’s scheduler allocates resources in hundreds of milliseconds even with thousands of queries . We evaluated using real video analytics queries over video datasets from live trafﬁc cameras from several large cities . Our ofﬂine proﬁling consumes 3 . 5 × less CPU resources compared to a basic greedy search . The on - line VideoStorm scheduler outperforms fair scheduling of resources [ 3 , 31 , 49 ] by as much as 80 % in quality of queries and 7 × in terms of lag . 2 System Description We describe the high - level architecture of VideoStorm and the speciﬁcations for video queries . 2 . 1 VideoStorm Architecture The VideoStorm cluster consists of a centralized man - ager and a set of worker machines that execute queries , 1 " name " : " LicensePlate " , 2 " transforms " : [ 3 { " id " : " 0 " , 4 " class _ name " : " Decoder " , 5 " parameters " : { 6 " CameraIP " : " 134 . 53 . 8 . 8 " , 7 " CameraPort " : 8100 , 8 " @ OutputResolution " : " 720P " , 9 " @ SamplingRate " : 0 . 75 } 10 } , 11 { " id " : " 1 " , 12 " input _ transform _ id " : " 0 " , 13 " class _ name " : " OpenALPR " , 14 " parameters " : { 15 " @ MinSize " : 100 , 16 " @ MaxSize " : 1000 , 17 " @ Step " : 10 } 18 } ] Figure 2 : VideoStorm Query for license plate reader . see Figure 1 . Every query is a DAG of transforms on live video that is continuously streamed to the cluster ; each transform processes a time - ordered stream of messages ( e . g . , video frames ) and passes its outputs downstream . Figure 1 shows two example queries . One query runs across two machines ; after decoding the video and sub - tracting the background , it sends the detected objects to another machine for tracking and classiﬁcation . The other query for detecting license plates runs on a single machine . We assume there is sufﬁcient bandwidth provi - sioned for cameras to stream their videos into the cluster . Every worker machine runs a machine manager which start worker processes to host transforms . The machine manager periodically reports resource utilizations as well as status of the running transforms to the VideoStorm manager . The scheduler in the manager uses this infor - mation to allocate resources to queries . The VideoStorm manager and the machine managers are not on the query data path ; videos are streamed directly to the decoding transforms and thereon between the transforms . 2 . 2 Video Queries Speciﬁcation Queries submitted to the VideoStorm manager are strung together as pipelines of transforms . Figure 2 shows a sample VideoStorm pipeline with two transforms . The ﬁrst transform decodes the live video to produce frames that are pushed to the second transform to ﬁnd license plate numbers using the OpenALPR library [ 13 ] . Each transform contains an id and class name which is the class implementing the transform ( § 7 ) . The in - put transform id ﬁeld speciﬁes the transform whose output feeds into this transform , thus allowing us to describe a pipeline . VideoStorm allows arbitrary DAGs including multiple inputs and outputs for a transform . Source trans - forms , such as the “Decoder” , do not specify input trans - C D Q A1 1 0 . 6 A2 2 0 . 7 A3 3 0 . 8 ( a ) Query A C D Q B1 1 0 . 1 B2 2 0 . 3 B3 3 0 . 9 ( b ) Query B Query A Query B Time R C D A Q L C D A Q L 0 4 A2 2 2 0 . 7 - B2 2 2 0 . 3 - 10 2 A1 1 1 0 . 6 - B1 1 1 0 . 1 - 22 4 A2 2 2 0 . 7 - B2 2 2 0 . 3 - ( c ) Fair allocation Query A Query B Time R C D A Q L C D A Q L 0 4 A1 1 1 0 . 6 - B3 3 3 0 . 9 - 10 2 A1 1 1 0 . 6 - B3 3 1 0 . 9 - 22 4 A1 1 1 0 . 6 - B2 2 3 0 . 3 8s 38 4 A1 1 1 0 . 6 - B3 3 3 0 . 9 - ( d ) Performance - based allocation Table 1 : Tables ( a ) and ( b ) show queries A and B with three conﬁgurations each , resource demand D and quality Q . Ta - bles ( c ) and ( d ) show the time and capacity R , and for each query the chosen conﬁguration C , demand D , allocation A , achieved quality Q , and lag L for the fair and performance - based schedulers . Notice in ( d ) that query B achieves higher quality between times 10 and 22 than with the fair sched - uler in ( c ) , and never lags beyond its permissible 8s . form , but instead directly connect to the camera source ( speciﬁed using IP and port number ) . Each transform contains optional knobs ( parameters ) ; e . g . , the minimum and maximum window sizes ( in pix - els ) of license plates to look for and the step increments to search between these sizes for the OpenALPR trans - form ( more in § 5 ) . Knobs whose values can updated dy - namically start with the ‘ @ ’ symbol . The VideoStorm manager updates them as part of its scheduling decisions . 3 Making the Case for Resource Allocation We make the case for resource management in video an - alytics clusters using a simple example ( § 3 . 1 ) and real - world video queries ( § 3 . 2 ) . 3 . 1 Motivating Example Cluster managers such as Yarn [ 3 ] , Apollo [ 31 ] and Mesos [ 49 ] commonly divide resources among multiple queries based on resource fairness . Being agnostic to query quality and lag preferences , fair allocation is the best they can do . Instead , scheduling for performance leads to queries achieving better quality and lag . The desirable properties of a scheduler for video ana - lytics are : ( 1 ) allocate more resources to queries whose qualities will improve more , ( 2 ) allow queries with built - up lag in their processing to “catch up , ” and ( 3 ) adjust query conﬁguration based on the resource allocated . Tables 1a and 1b shows two example queries A and B with three knob conﬁgurations each ( A x and B x , re - spectively ) . Query A’s improvement in quality Q is less pronounced than B’s for the same increase in resource demand D . Note that D is the resource to keep up with the incoming data rate . Query A cannot tolerate any lag , but B can tolerate up to 8 seconds of lag . Lag is deﬁned as the difference between the time of the last - arrived frame and the time of the last - processed frame , i . e . , how much time’s worth of frames are queued - up unprocessed . Let a single machine with resource capacity R of 4 run these two queries . Its capacity R drops to 2 after 10 seconds and then returns back to 4 after 12 more seconds ( at 22 seconds ) . This drop could be caused by another high - priority job running on this machine . Fair Scheduling . Table 1c shows the assigned conﬁgu - ration C , query demand D , resource allocation A , quality Q and lag L with a fair resource allocation . Each query selects the best conﬁguration to keep up with the live stream ( i . e . , keeps its demand below its allocation ) . Us - ing the fair scheduler , both queries get an allocation of 2 initially , picking conﬁgurations A2 and B2 respectively . Between times 10 to 22 , when the capacity drops to 2 , the queries get an allocation of 1 each , and pick conﬁgu - rations A1 and B1 . At no point do they incur any lag . Performance - based Scheduling . As Table 1d shows , a performance - based scheduler allocates resources of 1 and 3 to queries A and B at time 0 ; B can thus run at con - ﬁguration B3 , achieving higher quality compared to the fair allocation ( while A’s quality drops only by 0 . 1 ) . This is because the scheduler realizes the value in providing more resources to B given its resource - quality proﬁle . At time 10 when capacity drops to 2 , the scheduler allocates 1 unit of resource to each to the queries , but re - tains conﬁguration B3 for B . Since resource demand of B3 is 3 , but B has been allocated only 1 , B starts to lag . Speciﬁcally , every second , the lag in processing will in - crease by 2 / 3 of a second . However , query B will still produce results at quality 0 . 9 , albeit delayed . At time 22 , the capacity recovers and query B has built up a lag of 8 seconds . The scheduler allocates 3 resource units to B but switches it to conﬁguration B2 ( whose demand is only 2 ) . This means that query B can now catch up – ev - ery second it can process 1 . 5 seconds of video . Finally , at time 38 , all the lag has been eliminated and the scheduler switches B to conﬁguration B3 ( quality 0 . 9 ) . The performance - based scheduler exhibited the three properties listed above . It allocated resources to optimize 480p 576p 720p 900p1080p Frame Resolution 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Quality CPU ( a ) License Plate — Resolution ( sampling rate = 0 . 12 ) 0 . 1 0 . 2 0 . 3 0 . 4 Sampling Rate 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Quality CPU ( b ) License Plate — Sampling ( resolution = 480p ) 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Sampling Rate 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Quality CPU ( c ) DNN — Sampling DIST HIST SURF SIFT Object Mapping Metric 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Quality CPU ( d ) Tracker — Object Mapping Figure 3 : Resource - quality proﬁles for real - world video queries . For simplicity , we plot one knob at a time . for quality and allowed queries to catch up to built - up lag , while accordingly adjusting their conﬁgurations . 3 . 2 Real - world Video Queries Video analytics queries have many knob conﬁgurations that affect output quality and resource demand . We highlight the resource - quality proﬁles of four real - world queries— license plate reader , car counter , DNN classi - ﬁer , object tracker —of interest to the cities we are part - nering with and obtained videos from their operational trafﬁc cameras ( § 8 . 1 ) . For clarity , we plot one knob at a time and keep other knobs ﬁxed . Quality is deﬁned as the F1 score ∈ [ 0 , 1 ] ( the harmonic mean between precision and recall [ 83 ] ) with reference to a labeled ground truth . License Plate Reader . The OpenALPR [ 13 ] library scans the video frame to detect potential plates and then recognizes the text on plates using optical charac - ter recognition . In general , using higher video resolution and processing each frame will detect the most license plates accurately . Reducing the resolution and process - ing only a subset of frames ( e . g . , sampling rate of 0 . 25 ) dramatically reduces resource demand , but can also re - duce the quality of the output ( i . e . , miss or incorrectly read plates ) . Figures 3a and 3b plots the impact of reso - lution and sampling rate on quality and CPU demand . 1 Car Counter . Resolution and sampling rate are knobs that apply to almost all video queries . A car counter monitors an “area of interest” and counts cars passing the area . In general , its results are of good quality even with low resolution and sampling rates ( plots omitted ) . 1 Sampling rate of 0 . 75 drops every fourth frame from the video . Deep Neural Network ( DNN ) Classiﬁer . Vision pro - cessing is employing DNNs for key tasks including ob - ject detection and classiﬁcation . Figure 3c proﬁles a Caffe DNN [ 54 ] model trained with the widely - used Im - ageNet dataset [ 41 ] to classify objects into 1 , 000 cate - gories . We see a uniform increase in the quality of the classiﬁcation as well as resource consumption with the sampling rate . As DNN models get compressed [ 45 , 46 ] , reducing their resource demand at the cost of quality , the compression factor presents another knob . Object Tracker . Finally , we have also proﬁled an ob - ject tracker . This query continuously models the “back - ground” in the video , identiﬁes foreground objects by subtracting the background , and tracks objects across frames using a mapping metric . The mapping metric is a key knob ( Figure 3d ) . Objects across frames can be mapped to each other using metrics such as distance moved ( DIST ) , color histogram similarity ( HIST ) , or matched over SIFT [ 14 ] and SURF [ 15 ] features . Resource - quality proﬁles based on knob conﬁgura - tions is intrinsic to video analytics queries . These queries typically identify “events” ( like license plates or car acci - dents ) , and using datasets where these events are labeled , we can empirically measure precision and recall in iden - tifying the events for different query conﬁgurations . In contrast to approximate SQL query processing , there are no analytical models to estimate the relationship between resource demand and quality of video queries and it depends on the speciﬁc video feeds . For example , reducing video resolution may not reduce OpenALPR quality if the camera is zoomed in enough . Hence queries need to be proﬁled using representative video samples . 3 . 3 Summary and Challenges Designing a scheduler with the desirable properties in § 3 . 1 for real - world video queries ( § 3 . 2 ) is challenging . First , the conﬁguration space of a query can be large and there are no analytical models to estimate the re - source demand and result quality of each conﬁguration . Second , trading off between the lag and quality goals of queries is tricky , making it challenging to deﬁne scheduling objectives across all queries in the cluster . Third , resource allocation across all queries in the cluster each with many conﬁgurations is computationally intractable , presenting scalability challenges . 4 Solution Overview The VideoStorm scheduler is split into ofﬂine proﬁling and online phases ( Figure 4 ) . In the ofﬂine phase , for every query , we efﬁciently generate its resource - quality proﬁle – a small number of conﬁgurations on the Pareto Workers start / stop / migrate query / transform Profiler § 5 query Scheduler Resource Allocation § 6 . 2 Placement § 6 . 3 query profile configuration changes report machine , query stats ( periodic ) resource changes ( periodic ) ( periodic ) utility func . § 6 . 1 offline online WorkersWorkers Figure 4 : VideoStorm Scheduler Components . curve of the proﬁle , § 5 . This dramatically reduces the conﬁgurations to be considered by the scheduler . In the online phase , the scheduler periodically ( e . g . , every second ) considers all running queries and adjusts their resource allocation , machine placement , and con - ﬁgurations based on their proﬁles , changes in demand and / or capacity ( see Figure 4 ) . We encode the quality and lag requirements of each individual query into its utility function , § 6 . 1 . The performance goal across all queries in a cluster is speciﬁed either as maximizing the minimum utility or the sum of utilities , § 6 . 2 and § 6 . 3 . 5 Resource - Quality Proﬁle Estimation When a user submits a new query , we start running it im - mediately with a default proﬁle ( say , from its previous runs on other cameras ) , while at the same time we run the query through the ofﬂine proﬁling phase . The query proﬁler has two goals . 1 ) Select a small subset of con - ﬁgurations ( Pareto boundary ) from the resource - quality space , and 2 ) Compute the query proﬁle , P k , i . e . , the re - source demand and result quality of the selected conﬁg - urations . The proﬁle is computed either against a labeled dataset or using the initial parts of the video relative to a “golden” query conﬁguration which might be expensive but is known to produce high - quality results . 5 . 1 Proﬁle estimation is expensive We revisit the license plate reader query from § 3 . 2 in de - tail . As explained earlier , frame resolution and sampling rate are two important knobs . The query , built using the OpenALPR library [ 13 ] , scans the image for license plates of size MinSize , then multiplicatively increases the size by Step , and keeps repeating this process until the size reaches MaxSize . The set of potential license plates is then sent to an optical character recognizer . We estimate the quality of each knob conﬁguration ( i . e . , combination of the ﬁve knobs above ) on a labeled dataset using the F1 score [ 83 ] , the harmonic mean be - tween precision and recall , commonly used in machine 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 01 0 . 1 1 10 100 1000 qua li t y , F 1 sc o r e resource demand [ CPU cores , log scale ] Figure 5 : Resource - quality for license plate query on a 10 minute video ( 414 conﬁgurations ) ; x - axis is resource de - mand to keep up with live video . Generating this took 20 CPU days . The black dashed line is the Pareto boundary . learning ; 0 and 1 represent the lowest and highest qual - ities . For example , increasing MinSize or decreasing MaxSize reduces the resources needed but can miss some plates and decrease quality . Figure 5 shows a scatter plot of resource usage vs . quality of 414 conﬁgurations generated using the ﬁve knobs . There is four orders of magnitude of difference in resource usage ; the most expensive conﬁguration used all frames of a full HD resolution video and would take over 2 . 5 hours to analyze a 1 minute video on 1 core . No - tice the vast spread in quality among conﬁgurations with similar resource usage as well as the spread in resource usage among conﬁgurations that achieve similar quality . 5 . 2 Greedy exploration of conﬁgurations We implement a greedy local search to identify con - ﬁguration with high quality ( Q ) and low demand ( D ) ; see Table 2 . Our baseline proﬁler implements hill - climbing [ 74 ] ; it selects a random conﬁguration c , com - putes its quality Q ( c ) and resource demand D ( c ) by run - ning the query with c on a small subset of the video dataset , and calculates X ( c ) = Q ( c ) − β D ( c ) where β trades off between quality and demand . Next , we pick a neighbor conﬁguration n ( by changing the value of a ran - dom knob in c ) . If X ( n ) > X ( c ) , then n is better than c in quality or resource demand ( or both ) ; we set c = n and repeat . When we cannot ﬁnd a better neighbor ( i . e . , our exploration indicates that we are near a local optimum ) , we repeat by picking another random c . Several enhancements signiﬁcantly increase the efﬁ - ciency of our search . To avoid starting with an expen - sive conﬁguration and exploring its neighbors , ( which are also likely to be expensive , thus wasting CPU ) , we pick k random conﬁgurations and start from the one with the highest X ( c ) . We found that using even k = 3 can successfully avoid starting in an expensive part of the search space . Second , we cache intermediate results in the query’s DAG and reuse them in evaluating conﬁgura - tions with overlapping knob values . Term Description P k proﬁle of query k c k ∈ C k speciﬁc conﬁguration of query k Q k ( c ) quality under conﬁguration c D k ( c ) resource demand under conﬁguration c L k , t measured lag at time t U k utility Q Mk ( min ) quality goal L Mk ( max ) lag goal a k resources allocated Table 2 : Notations used , for query k . While our simple proﬁler is sufﬁciently efﬁcient for our purpose , sophisticated hyperparameter searches ( e . g . , [ 76 ] ) can potentially further improve its efﬁciency . Pareto boundary . We are only interested in a small subset of conﬁgurations that are on the Pareto boundary P of the resource - quality space . Let Q ( c ) be the quality and D ( c ) the resource demand under conﬁguration c . If c 1 and c 2 are two conﬁgurations such that Q ( c 1 ) ≥ Q ( c 2 ) and D ( c 1 ) ≤ D ( c 2 ) , then c 2 is not useful in practice ; c 1 is better than c 2 in both quality and resource demand . The dashed line in Figure 5 shows the Pareto boundary of such conﬁgurations for the license plate query . We ex - tract the Pareto boundary of the explored conﬁgurations and call it the resource - quality proﬁle P of the query . We can generate the same proﬁle as the baseline pro - ﬁler on the license plate query with 3 . 5 × less CPU re - sources ( i . e . , 5 . 4 CPU hours instead of 19 CPU hours ) . 6 Resource Management In the online phase , the VideoStorm cluster sched - uler considers the utilities of individual queries and the cluster - wide performance objectives ( deﬁned in § 6 . 1 ) and periodically performs two steps : resource allocation and query placement . In the resource allocation step , § 6 . 2 , the scheduler assumes the cluster is an aggregate bin of resources and uses an efﬁcient heuristic to maxi - mize the cluster - wide performance by adjusting query al - location and conﬁguration . In the query placement step , § 6 . 3 , the scheduler places new queries to machines in the cluster and considers migrating existing queries . 6 . 1 Utility : Combining Quality and Lag Each query has preferences on the desired quality and lag . What is the minimum quality goal ( Q M ) ? How much does the query beneﬁt from higher quality than the goal ? What is the maximum lag ( L M ) it can toler - ate and how sensitive are violations to this goal ? ( See Table 2 for notations . ) We encode these preferences in 0 3 6 0 0 . 5 1 U Q quality Q query 1 query 2 Q M1 Q M2 - 6 - 3 0 0 5 10 U L lag L [ sec ] query 1 query 2 L M1 L M2 Figure 6 : Examples for the second ( U Q ) and third terms ( U L ) in equation 1 . ( Left ) Query 1’s quality goal is relatively lenient , Q M 1 = 0 . 2 , but its utility grows slowly with increase in quality beyond Q M 1 . Query 2 is more stringent , Q M 2 = 0 . 6 , but its utility grows sharply thereon . ( Right ) Query 1 has lag target of L M 1 = 5 beyond which it incurs a penalty . Query 2 has a stricter lag goal of L M 2 = 1 and also its utility drops much faster with increased lag . utility functions , an abstraction used extensively in eco - nomics [ 65 , 73 ] and computer systems [ 22 , 55 ] . Our utility function for a query has the following form , where ( x ) + is the positive part of x . We omit the query index k for clarity . U ( Q , L ) = U B + U Q ( Q ) + U L ( L ) = U B + α Q · ( Q − Q M ) + − α L · ( L − L M ) + ( 1 ) U B is the “baseline” utility for meeting the quality and lag goals ( when Q = Q M and L = L M ) . The second term U Q describes how the utility responds to achieved quality Q above Q M , the soft quality goal ; the multiplier α Q and Q M are query - speciﬁc and set based on the application analyzing the video . Results with quality below Q M are typically not useful to the users . The third term , U L , represents the penalty for results arriving later than the maximum lag goal of L M . 2 Recall that lag is the difference between the current time and the arrival time of the last processed frame , e . g . , if at time 10 : 30 we process a frame that arrived at 10 : 15 , the lag is 15 minutes . Similar to latency SLOs in clusters , there is no bonus for lag being below L M . See Figure 6 for examples of U Q and U L in queries . Scheduling objectives . Given utilities of individual queries , how do we deﬁne utility or performance of the whole cluster ? Previous work has typically aimed to maximize the minimum utility [ 61 , 64 ] or sum of util - ities [ 61 , 63 ] , which we adopt . When deployed as a “service” in the public cloud , utility will represent the revenue the cluster operator generates by executing the query ; penalties and bonuses in utility translate to loss and increase in revenue . Therefore , maximizing the sum of utilities maximizes revenue . In a private cluster that is shared by many cooperating entities , achieving fairness is more desirable . Maximally improving the utility of the worst query provides max - min fairness over utilities . 2 Multiplier α L is in ( 1 / second ) , making U L dimensionless like U Q . To simplify the selection of utility functions in practi - cal settings , we can provide only a few options to choose from . For example , the users could separately pick the minimum quality ( 40 % , 60 % , or 80 % ) and the maximum lag ( 1 , 10 , or 60 minutes ) for a total of nine utility func - tion templates . Users of cloud services already make similar decisions ; for example , in Azure Storage [ 32 ] , they separately select data redundancy ( local , zone , or geo - distributed ) and data access pattern ( hot vs . cool ) . 6 . 2 Resource Allocation Given a proﬁle P k and a utility function U k for each query k , the scheduler allocates resources a k to the queries and picks their query conﬁguration ( c k ∈ P k ) . The scheduler runs periodically ( e . g . , every few seconds ) and reacts to arrival of new queries , changes in query demand and lag , and changes in resource capacity ( e . g . , due to other high - priority non - VideoStorm jobs ) . 6 . 2 . 1 Scheduling Using Model - Predictive Control The scheduler aims to maximize the minimum or sum of query utilities , which in turn depend on their quality and lag . A key point to understand is that while we can near - instantaneously control query quality by adjusting its conﬁguration , query lag accumulates over time if we allocate less resources than query demand . Because of this accumulation property , the scheduler cannot optimize the current performance , but only aims to improve performance in the near future . We formulate the scheduling problem using the Model - Predictive Con - trol ( MPC [ 67 ] ) framework ; where we model the cluster performance over a short time horizon T as a function of query conﬁguration and allocation . In each step , we se - lect the conﬁguration and allocation to maximize perfor - mance over the near future ( described in detail in § 6 . 2 . 2 ) . To predict future performance , we need to predict query lag ; we use the following formula : L k , t + T ( a k , c k ) = L k , t + T − T a k D k ( c k ) ( 2 ) We plug in the predicted lag L k , t + T into the utility function ( Equation 1 ) to obtain the predicted utility . 6 . 2 . 2 Scheduling Heuristics We describe resource allocation assuming each query to contain only one transform , which we relax in § 6 . 4 . Maximizing sum of utilities . The optimization prob - lem for maximizing sum of utilities over time horizon T is as follows . Sum of allocated resources a k cannot ex - ceed cluster resource capacity R . max a k , c k ∈ P k ∑ k U k ( Q k ( c k ) , L k , t + T ) ( 3 ) s . t . ∑ k a k ≤ R Maximizing the sum of utilities is a variant of the knap - sack problem where we are trying to include the queries at different allocation and conﬁguration to maximize the total utility . The maximization results in the best distri - bution of resources ( as was illustrated in § 3 . 1 ) . When including query k at allocation a k and conﬁgu - ration c k , we are paying cost of a k and receiving value of u k = U k ( Q k ( c k ) , L k , t + T ) . We employ a greedy approxi - mation based on [ 40 ] where we prefer queries with high - est value of u k / a k ; i . e . , we receive the largest increase in utility normalized by resource spent . Our heuristic starts with a k = 0 and in each step we consider increasing a i ( for all queries i ) by a small ∆ ( say , 1 % of a core ) and consider all conﬁgurations of c i ∈ P i . Among these options , we select query i ( and correspond - ing c i ) with largest increase in utility . 3 We repeat this step until we run out of resources or we have selected the best conﬁguration for each query . ( Since we start with a k = 0 and stop when we run out of resources , we will not end up with infeasible solutions . ) Maximizing minimum utility . Below is the optimiza - tion problem to maximize the minimum utility predicted over a short time horizon T . We require that all utilities be ≥ u and we maximize u . max a k , c k ∈ P k u ( 4 ) s . t . ∀ k : U k ( Q k ( c k ) , L k , t + T ) ≥ u ∑ k a k ≤ R We can improve u only by improving the utility of the worst query . Our heuristic is thus as follows . We start with a k = 0 for all queries . In each step , we select query i = argmin k U k ( Q k ( c k ) , L k , t + T ) with the lowest utility and increase its allocation by a small ∆ , say 1 % of a core . With this allocation , we compute its best conﬁguration c i as argmax c ∈ P i U i ( Q i ( c ) , L i , t + T ) . We repeat this process until we run out of resources or we have picked the best conﬁguration for each query . 6 . 3 Query Placement After determining resource allocation and conﬁguration of each query , we next describe the placement of new queries and migration of existing queries . We quantify 3 We use a concave version of the utility functions obtained using linear interpolation to ensure that each query has a positive increase in utility , even for small ∆ . the suitability of placing a query q on machine m by com - puting a score for each of the following goals : high uti - lization , load balancing , and spreading low - lag queries . ( i ) Utilization . High utilization in the cluster can be achieved by packing queries in to machines , thereby min - imizing fragmentation and wastage of resources . Pack - ing has several well - studied heuristics [ 44 , 71 ] . We de - ﬁne alignment of a query relative to a machine using a weighted dot product , p , between the vector of machine’s available resources and the query’s demands ; p ∈ [ 0 , 1 ] . ( ii ) Load Balancing . Spreading load across the cluster ensures that each machine has spare capacity to handle changes in demand . We therefore prefer to place q on a machine m with the smallest utilization . We capture this in score b = 1 − M + D M max ∈ [ 0 , 1 ] , where M is the current utilization of machine m and D is demand of query q . ( iii ) Lag Spreading . Not concentrating many low - lag queries on a machine provides slack to accumulate lag for some queries when resources are scarce , without hav - ing to resort to migration of queries or violation of their lag goal L M . We achieve this by maintaining high av - erage L M on each machine . We thus compute score l ∈ [ 0 , 1 ] as the average L M after placing q on m . The ﬁnal score s q , m is the average of the three scores . For each new query q , we place it on a machine with the largest s q , m . For each existing query q , we migrate from machine m 0 to a new machine m 1 only if its score improves substantially ; i . e . , s ( q , m 1 ) − s ( q , m 0 ) > τ . 6 . 4 Enhancements Incorrect resource proﬁle . The proﬁled resource de - mand of a query , D k ( c k ) , might not exactly correspond to the actual query demand , e . g . , when demand depends on video content . Using incorrect demand can negatively impact scheduling ; for example , if D k ( c ) = 10 , but actual usage is R k = 100 , the scheduler would estimate that al - locating a k = 20 would reduce query lag at the rate of 2 × , while the lag would actually grow at a rate of 5 × . To address this , we keep track of a running average of mis - estimation µ = R k / D k ( c ) , which represents the mul - tiplicative error between the predicted demand and actual usage . We then incorporate µ in the lag predictor from Equation 2 , L k , t + T ( a k , c k ) = L k , t + T − T a k D k ( c k ) ( 1 µ ) . Machine - level scheduling . As most queries ﬁt on a single machine , we can respond to changes in demand or lag at the machine - level , without waiting for the cluster - wide decisions . We therefore execute the allocation step from § 6 . 2 on each machine , which makes the scheduling logic much more scalable . The cluster - wide scheduler still runs the allocation step , but only for the purposes of determining query placement and migration . DAG of transforms . Queries consisting of a DAG of transforms could be placed across multiple machines . We ﬁrst distribute the query resource allocation , a k , to in - dividual transforms based on per - transform resource de - mands . We then place individual transforms to machines as described in § 6 . 3 while accounting for the expected data ﬂow across machines and network link capacities . 7 VideoStorm Implementation We now discuss VideoStorm ’s key implementation de - tails and the interfaces implemented by transforms . 7 . 1 Implementation Details In contrast to widely - deployed cluster frameworks like Yarn [ 3 ] , Mesos [ 49 ] and Cosmos [ 31 ] , we highlight the differences in VideoStorm ’s design . First , VideoStorm takes the list of knobs , resource - quality proﬁles and lag goals as inputs to allocate resources . Second , machine - level managers in the cluster frameworks pull work , whereas the VideoStorm manager pushes new queries and conﬁguration changes to the machine - managers . Finally , VideoStorm allows machine managers to au - tonomously handle short - term ﬂuctuations ( § 6 . 4 ) Flow control . We implemented ﬂow control across transforms of a query to minimize the buffering inside the query pipeline , and instead push queuing of unpro - cessed video to the front of the query . This helps for two reasons . First , decoded frames can be as much as 300 × larger than the encoded video ( from our benchmarks on HD videos ) . Buffering these frames will signiﬁcantly in - ﬂate memory usage while spilling them to disk affects overall performance . Second , buffering at the front of query enables the query to respond promptly to conﬁgu - ration changes . It prevents frames from being processed by transforms with old inconsistent knob values . Migration . As described in § 6 . 3 , VideoStorm migrates queries depending on the load in the cluster . We imple - ment a simple “start - and - stop” migration where we start a copy of a running query / transform on the target ma - chine , duplicate its input stream to the copy , and stop the old query / transform after a short period . The whole process of migration is data - lossless and takes roughly a second ( § 8 . 3 ) , so the overhead of duplicated processing during the migration is very small . Resource Enforcement . VideoStorm uses Job Ob - jects [ 18 ] for enforcing allocations . Similar to Linux Containers [ 11 ] , Job Objects allow controlling and re - sizing the CPU / memory limits of running processes . 7 . 2 Interfaces for Query Transforms Transforms implement simple interfaces to process data and exchange control information . • Processing . Transforms implement byte [ ] Pro - cess ( header , data ) method . header contains metadata such as frame id and timestamp . data is the input byte array , such as decoded frame . The transform returns another byte array with its result , such as the detected license plate . Each transform maintains its own state , such as the background model . • Conﬁguration . Transforms can also implement Up - date ( key , value ) to set and update knob values to change query conﬁguration at runtime . 8 Evaluation We evaluate the VideoStorm prototype ( § 7 ) using a clus - ter of 101 machines on Microsoft Azure with real video queries and video datasets . Our highlights : 1 . VideoStorm outperforms the fair scheduler by 80 % in quality of outputs with 7 × better lag . ( § 8 . 2 ) 2 . VideoStorm is robust to errors in query proﬁles and allocates nearly the same as correct proﬁles . ( § 8 . 3 ) 3 . VideoStorm scales to thousands of queries with little systemic execution overheads . ( § 8 . 4 ) 8 . 1 Setup Video Analytics Queries . We evaluate VideoStorm us - ing four types of queries described and proﬁled in § 3 . 2 – license plate reader , car counter , DNN classiﬁer , object tracker . These queries are of major interest to the cities we are partnering with in deploying our system . Video Datasets . The above queries run on video datasets obtained from real and operational trafﬁc cam - eras in Bellevue and Seattle cities for two months ( Sept . – Oct . , 2015 ) . In our experiments , we stream the recorded videos at their original frame - rate ( 14 to 30 fps ) and res - olution ( 240P to 1080P ) thereby mimicking live video streams . The videos span a variety of conditions ( sun - ny / rainy , heavy / light trafﬁc ) that lead to variation in their processing workload . We present results on multiple dif - ferent snippets from the videos . Azure Deployment . We deploy VideoStorm on 101 D3 v2 instances on Azure’s West - US cluster [ 6 ] . D3 v2 in - stances contain 4 cores of the 2 . 4GHz Intel Xeon proces - sor and 14GB RAM . One machine ran the VideoStorm global manager on which no queries were scheduled . Baseline . We use the work - conservative fair scheduler as our baseline . It’s the widely - used scheduling pol - icy for cluster computing frameworks like Mesos [ 49 ] , Yarn [ 3 ] and Cosmos [ 31 ] . When a query , even at its best conﬁguration , cannot use its fair share , it distributes the excess resources among the other queries . The fair scheduler places the same number of queries evenly on all available machines in a round - robin fashion . 0 50 100 150 200 250 300 350 400 Burst Duration , N ( seconds ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 U t ili t y Fair Scheduler ( a ) Utility 0 50 100 150 200 250 300 350 400 Burst Duration , N ( seconds ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Q ua li t y VideoStorm MaxMin ( Allocation Only ) ( b ) Quality 0 50 100 150 200 250 300 350 400 Burst Duration , N ( seconds ) 0 5 10 15 20 25 F r a m e s ( % ) Lagg i ng B e y ond G oa l VideoStorm MaxMin ( c ) Lag Figure 7 : VideoStorm outperforms the fair scheduler as the duration of burst of queries in the experiment is varied . Without its placement but only its allocation ( “ VideoStorm MaxMin ( Allocation Only ) ” ) , its performance drops by a third . Metric . The three metrics of interest to us are quality , frames ( % ) exceeding the lag goal in processing , and util - ity ( § 6 . 1 ) . We compare the improvement ( % ) ; if a metric ( say , quality ) with VideoStorm and the fair scheduler is X V and X f , we measure X V − X f X f × 100 % . 8 . 2 Performance Improvements Our workload consists of a mix of queries with lenient and stringent goals . We start with a set of 300 queries picked from the four types ( § 8 . 1 ) on 300 distinct video datasets at the beginning of the experiment . 60 % of these queries have a lag goal L M of 20s while the remaining are more lenient with a lag goal of 300s . All of them have a quality goal Q M of 0 . 25 . We set the lag multiplier α L = 1 for these long - lived video analyses . Burst of N seconds : At a certain point , a burst of 200 li - cense plate queries arrive and last for N seconds ( which we will vary ) . These queries have a lag goal Q L of 20s , a high quality goal ( 1 . 0 ) , and higher α L = 2 . They mimic short - term deployment of queries like AMBER Alerts with stringent accuracy and lag goals . We eval - uate VideoStorm ’s reaction to the burst of queries up to several minutes ; note that the improvements will carry over when tolerant delay and bursts are much longer . 8 . 2 . 1 Maximize the Minimum Utility ( MaxMin ) We ran a series of experiments with burst duration N from 10 seconds to 400 seconds . Figure 7a plots the minimum query utility achieved in each of the experi - ments , when VideoStorm maximizes the minimum util - ity ( § 6 . 2 . 2 ) . For each point in the ﬁgure , we ob - tain the minimum utility , quality and lag over an inter - val that includes a minute before and after the N sec - ond burst . VideoStorm ’s utility ( “ VideoStorm - MaxMin” ) drops only moderately with increasing burst duration . Its placement and resource allocations ensure it copes well with the onset of and during the burst . Contrast with the fair scheduler’s sharp drop with N . The improvement in utility comes due to smartly ac - counting for the resource - quality proﬁle and lag goal of the queries ; see Figures 7b and 7c . Quality ( F1 score 0 50 100 150 200 250 Time ( seconds ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 S ha r e o f C l u s t e r C P U s Lag Goal = 300s Lag Goal = 20s High - Quality , Lag Goal = 20s 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Q ua li t y Lag Goal = 300s Lag Goal = 20s High - Quality , Lag Goal = 20s 0 50 100 150 200 250 Time ( seconds ) 020406080100120 Lag ( s e c ) Figure 8 : ( Top ) CPU Allocation for burst duration N = 150 s , and ( bottom ) quality and lag averaged across all queries in each of the three categories . [ 83 ] ; ∈ [ 0 , 1 ] ) with the fair scheduler is 0 . 2 lower than VideoStorm to begin with , but reduces signiﬁcantly to nearly 0 . 5 for longer bursts ( higher N ) , while quality with VideoStorm stays at 0 . 9 , or nearly 80 % better . The rest of VideoStorm ’s improvement comes by ensuring that despite the accumulation in lag , fewer than 5 % of the frames exceed the query’s lag goal whereas with the fair scheduler it grows to be 7 × worse . How valuable is VideoStorm ’s placement ? Figure 7 also shows the “ VideoStorm MaxMin ( Allocation Only ) ” graphs which lie in between the graphs for the fair sched - uler and VideoStorm . As described in § 6 . 3 , VideoStorm ﬁrst decides the resource allocation and then places them onto machines to achieve high utilization , load balancing and spreading of lag - sensitive and lag - tolerant queries . As the results show , not using VideoStorm ’s placement heuristic ( instead using our baseline’s round - robin place - ment ) considerably lowers VideoStorm ’s gains . Figure 8 ( top ) explains VideoStorm ’s gains by plotting the allocation of CPU cores in the cluster over time , for burst duration N = 150s . We group the queries into 0 100 200 300 400 Burst Duration , N ( seconds ) 0 . 80 0 . 85 0 . 90 0 . 95 1 . 00 Q ua li t y Lag Goal = 20s , ® L = 1 High Quality , Lag Goal = 20s , ® L = 2 0 100 200 300 400 Burst Duration , N ( seconds ) 0246810 F r a m e s ( % ) Lagg i ng B e y ond G oa l Figure 9 : Impact of α L . Queries with higher α L have fewer frames lagging beyond their goal . three categories — the burst of queries with 20s lag goal and quality goal of 1 . 0 , those with 20s lag goal , and 300s lag goal ( both with quality goal of 0 . 25 ) . We see that VideoStorm adapts to the burst and allocates nearly 60 % of the CPU cores in the cluster to the burst of li - cense plate queries which have a high quality and tight lag goals . VideoStorm also delays processing of lag - tolerant queries ( allocating less than 10 % of CPUs ) . Fig - ure 8 ( bottom ) shows the resulting quality and lag , for queries in each category . We see that because the delay - tolerant queries have small allocation , their lag grows but stays below the goal . The queries with 20s lag goal re - duce their quality to adapt to lower allocation and keep their lag ( on average ) within the bound . Impact of α L . Figure 9 plots the distinction in treat - ment of queries with the same lag goal ( L M ) but differ - ent α L and quality goals . While the ﬁgure on the left shows that VideoStorm does not drop the quality of the query with Q M = 1 . 0 , it also respects the difference in α L ; fewer frames of the query with α L = 2 lag beyond the goal of 20s ( right ) . This is an example of how utility functions encode priorities . 8 . 2 . 2 Maximize the Total Utility ( MaxSum ) Recall from § 6 . 2 . 2 that VideoStorm can also maximize the sum of utilities . We measure the average utility , qual - ity , and frames ( % ) exceeding the lag goal ; maximiz - ing for the total utility and average utility are equivalent . VideoStorm achieves 25 % better quality and 5 × better lag compared with the fair scheduler . Per Query Performance . While MaxMin scheduling , as expected , results in all the queries achieving similar quality and lag , MaxSum priorities between queries as the burst duration increases . Our results show that the license plate query , whose utility over its resource de - mand is relatively lower , is de - prioritized with MaxSum ( reduced quality as well as more frames lagging ) . With its high quality ( 1 . 0 ) and low lag ( 20s ) goals , the sched - uler has little leeway . The DNN classiﬁer , despite having comparable resource demand does not suffer from a re - duction in quality because of its tolerance to lag ( 300s ) . 50 100 150 200 250 300 Number of New Queries 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Q ua li t y Fair Scheduler ( a ) Quality 50 100 150 200 250 300 Number of New Queries 0 5 10 15 20 F r a m e s ( % ) Lagg i ng B e y ond G oa l VideoStorm MaxMin ( b ) Lag Figure 10 : VideoStorm vs . fair scheduler as the number of queries in the burst during the experiment is varied . 0 10 20 30 M 1 C P U 0 50 100 150 200 Time ( seconds ) 0 10 20 30 M 2 C P U Q 1 ( migrated ) Q 2 Q 3 Q 4 ( lag - tolerant ) Figure 11 : Q 1 migrated between M 1 and M 2 . Resource for the only lag - tolerant query Q 4 ( on M 2 ) is reduced for Q 1 . 8 . 2 . 3 Varying the Burst Size We next vary the size of the burst , i . e . , number of queries that arrive in the burst . Note that the experiments above had varied the duration of the burst but with a ﬁxed size of 200 queries . Varying the number of queries in the burst introduces different dynamics and reactions in VideoStorm ’s scheduler . We ﬁx the burst duration to 200s . Figure 10 plots the results . The fair allocation causes much higher fraction of frames to exceed the lag goal when the burst size grows . VideoStorm better han - dles the burst and consistently performs better . Note that beyond a burst of 200 queries , resources are insufﬁcient even to satisfy the lowest conﬁguration ( least resource demand ) , causing the degradation in Figure 10b . 8 . 3 VideoStorm ’s Key Features We now highlight VideoStorm ’s migration of queries and accounting for errors in the resource demands . 8 . 3 . 1 Migration of Queries Recall from § 6 . 3 and § 7 that VideoStorm migrates queries when necessary . We evaluate the value of migra - tion by making the following addition to our experiment described at the beginning of § 8 . 2 . During the experi - ment , we allocate half the resources in 50 % of our ma - chines to other non - VideoStorm jobs . After a few min - utes , the non - VideoStorm jobs complete and leave . Such jobs will be common when VideoStorm is co - situated 0 100 200 300 400 500 Time ( seconds ) 15 20 25 30 35 40 45 C P U ( w / o adap t a t i on ) ( a ) Without Adaptation 0 100 200 300 400 500 Time ( seconds ) 15 20 25 30 35 40 45 C P U ( w / adap t a t i on ) Accurate Twice Half ( b ) With Adaptation 0 100 200 300 400 500 Time ( seconds ) 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 ¹ ( c ) µ Over Time Figure 12 : We show three queries on a machine whose resource demands in their proﬁles are synthetically doubled , halved , and unchanged . By learning the proportionality factor µ ( 12c ) , our allocation adapts and converges to the right allocations ( 12b ) as opposed to without adaptation ( 12a ) . with other frameworks in clusters managed by Yarn [ 3 ] or Mesos [ 49 ] . We measure the migration time , and com - pare the performance with and without migration . Figure 11 plots the timeline of two machines , M 1 and M 2 ; M 1 where a non - VideoStorm job was scheduled and M 2 being the machine to which a VideoStorm query Q 1 , originally on M 1 , was migrated . Q 1 shifts from running on M 1 to M 2 in only 1 . 3s . We migrate Q 1 back to M 1 when the non - VideoStorm job leaves at ∼ 150 s . Shifting Q 1 to M 2 ( and other queries whose machines were also allocated non - VideoStorm jobs , correspond - ingly ) ensured that we did not have to degrade the qual - ity or exceed the lag goals . Since our placement heuristic carefully spread out the queries with lenient and stringent lag goals ( § 6 . 3 ) , we ensured that each of the machines had sufﬁcient slack . As a result , when Q 1 was migrated to M 2 which already was running Q 2 and Q 4 , we could delay the processing of the lag - tolerant Q 4 without vi - olating any lag goals . The allocations of these delayed queries were ramped up for them to process their back - log as soon as the queries were migrated back . As a consequence , the quality of queries with migra - tion is 12 % better than without migration . Crucially , 18 × more frames ( 4 . 55 % instead of 0 . 25 % ) would have ex - ceeded the lag goal without migration . 8 . 3 . 2 Handling Errors in Query Proﬁle VideoStorm deals with difference between the resource demands in the resource - quality proﬁle and the actual demand by continuously monitoring the resource con - sumption and adapting to errors in proﬁled demand ( µ in § 6 . 4 ) . We now test the effectiveness of our correction . We synthetically introduce errors in our proﬁles , as if they were proﬁles with errors , and use the erroneous proﬁles for our resource allocation . Consequently , the actual resource demands when the query executes do not match . In the workload above , we randomly make the proﬁle to be half the actual resource demand for a third of the queries , twice the demand for another third , and unchanged ( accurate ) for the rest . VideoStorm ’s adaptive correction ensures that the quality and lag of queries with Mean Standard Action Duration ( ms ) Deviation ( ms ) Start Transform 60 . 37 3 . 96 Stop Transform 3 . 08 0 . 47 Conﬁg . Change 15 2 . 0 Resource Change 5 . 7 1 . 5 Table 3 : Latency of VideoStorm ’s actions . erroneous proﬁles are nearly 99 . 6 % of results obtained if the proﬁles were perfectly accurate . In Figure 12 , we look at a single machine where VideoStorm placed three license plate queries , one each of the three different error categories . An ideal allocation ( in the absence of errors ) should be a third of the CPU to each of the queries . Figure 12a , however , shows how the allocation is far from converging towards the ideal with - out adaptation , because erroneous proﬁles undermine the precision of utility prediction . In contrast , with the adap - tation , despite the errors , resource allocations converge to and stay at the ideal ( Figure 12b ) . This is because the µ values for the queries with erroneous proﬁles are cor - rectly learned as 2 and 0 . 5 ; the query without any error introduced its proﬁle has its µ around 1 ( Figure 12c ) . 8 . 4 Scalability and Efﬁciency Latency of VideoStorm ’s actions . Table 3 shows the time taken for VideoStorm to start a new transform ( ship - ping binaries , process startup ) , stop a transform , and change a 100 - knob conﬁguration and resource allocation of 10 running queries . We see that VideoStorm allows for near - instantaneous operations . Scheduling Decisions . Figure 13a plots the time taken by VideoStorm ’s scheduler . Even with thousands of queries , VideoStorm make its decisions in just a few sec - onds . This is comparable to the scalability of schedulers in big data clusters , and video analytics clusters are un - likely to exceed them in the number of queries . Com - bined with the low latency of actions ( Table 3 ) , we be - lieve VideoStorm is sufﬁciently scalable and agile . 500 1000 2000 4000 8000 Number of Queries 0123456 S c hedu li ng T i m e ( s ) Number of Machines 100 200 500 1000 ( a ) Scheduling Scalability Car Counter 010203040506070 La t en cy ( m s ) Vanilla DAG Local Single Transform DAG Distributed ( b ) Overheads Figure 13 : Overheads in scheduling and running queries . Transform Overheads . Finally , we measure the over - head of running a vision algorithm inside VideoStorm . We compare the latency in processing a frame while run - ning as a vanilla process , inside a single transform , as a DAG of transforms on one machine , and as a DAG distributed across machines . Figure 13b shows that the overheads are limited . Running as a single transform , the overhead is < 3 % . When possible , VideoStorm places the transforms of a query DAG locally on one machine . 9 Related Work Cluster schedulers . Cluster schedulers [ 3 , 31 , 39 , 42 , 44 , 49 , 86 ] do not cater to the performance objectives of streaming video analytics . They take resource demands from tasks ( not the proﬁles ) , mostly allocate based on fairness / priorities , and do not resize running containers , key to dealing with resource churn in VideoStorm ( § 7 ) . Deadline - Based Scheduling . Many systems [ 22 , 39 , 42 , 56 , 85 ] adaptively allocate resources to meet dead - lines of batch jobs or reduce lag of streaming queries . Scheduling in real - time systems [ 52 , 87 ] has also consid - ered using utility functions to provide ( soft ) deadlines to running tasks . Crucially , these systems do not consider approximation together with resource allocation to meet deadlines and do not optimize across multiple queries and servers . Streaming and Approximate Query Processing Sys - tems . Load shedding has been a topic of interest in streaming systems [ 25 , 68 ] to manage memory usage of SQL operators but they do not consider lag in processing . Aurora , Medusa , and Borealis [ 19 , 33 , 37 ] and follow - up works [ 78 , 79 , 81 , 82 , 88 ] use QoS graphs to capture lag and sampling rate but they consider them separately and do not trade - off between them , a key aspect in our so - lution . In contrast to JetStream [ 72 ] , that degrades data quality based on WAN bandwidths , VideoStorm identi - ﬁes the best knobs to use automatically and adjusts al - locations jointly across queries . Stream processing sys - tems used in production [ 2 , 4 , 62 , 89 ] do not consider load - shedding , and resource - quality tradeoff and lag in their design ; Google Cloud Dataﬂow [ 21 ] requires man - ual trade - off speciﬁcations . Approximation is also used by recent [ 20 , 23 , 84 ] and older [ 47 , 53 ] batch querying systems using statistical models for SQL operators [ 38 ] . Relative to the above literature , our main contributions are three - fold : ( i ) considering quality and lag of video queries together for multiple queries using predictive control , ( ii ) dealing with multitude of knobs in vision al - gorithms , and ( iii ) proﬁling black - box vision transforms with arbitrary user code ( not standard operators ) . Utility functions . Utility functions are used exten - sively throughout economics [ 65 , 73 ] , compute sci - ence [ 48 , 55 , 57 , 63 ] , and other disciplines to map how users beneﬁt from performance [ 50 , 58 , 80 ] . In stream processing systems , queries describe their requirements for throughput , latency , and fraction of dropped tu - ples [ 22 , 34 , 60 , 79 ] . With multiple entities , previous work has typically maximized the minimum utility [ 61 , 64 ] or sum of utilities [ 61 , 63 ] , which is what we also use . Util - ity elicitation [ 28 , 30 , 35 ] helps obtain the exact shape of the utility function . Autonomic Computing . Autonomic computing [ 24 , 26 , 29 , 66 , 70 , 77 ] allocate resources to VMs and web ap - plications to maximize their quality of service . While some of them used look - ahead controllers based on MPC [ 67 ] , they mostly ignored our main issues on the large space of conﬁgurations and quality - lag trade - offs . 10 Conclusion VideoStorm is a video analytics system that scales to processing thousands of video streams in large clusters . Video analytics queries can adapt the quality of their re - sults based on the resources allocated . The core aspect of VideoStorm is its scheduler that considers the resource - quality proﬁles of queries , each with a variety of knobs , and tolerance to lag in processing . Our scheduler opti - mizes jointly for the quality and lag of queries in allocat - ing resources . VideoStorm also efﬁciently estimates the resource - quality proﬁles of queries . Deployment on an Azure cluster of 101 machines show that VideoStorm can signiﬁcantly outperform a fair scheduling of resources , the widely - used policy in current clusters . Acknowledgments We are grateful to Xiaozhou Li , Qifan Pu , Logan Stafman and Shivaram Venkataraman for reading early versions of the draft and providing feedback . We also thank our shepherd George Porter and the anonymous NSDI reviewers for their constructive feedback . This work was partially supported by NSF Awards CNS - 0953197 and IIS - 1250990 . References [ 1 ] AMBER Alert , U . S . Department of Justice . http : / / www . amberalert . gov / faqs . htm . [ 2 ] Apache Flink . https : / / flink . apache . org / . [ 3 ] Apache Hadoop NextGen MapReduce ( YARN ) . https : / / hadoop . apache . org / docs / r2 . 7 . 1 / hadoop - yarn / hadoop - yarn - site / YARN . html . [ 4 ] Apache Storm . https : / / storm . apache . org / . [ 5 ] Avigilon . http : / / avigilon . com / products / . [ 6 ] Azure Instances . https : / / azure . microsoft . com / en - us / pricing / details / virtual - machines / . [ 7 ] Capacity Scheduler . https : / / hadoop . apache . org / docs / r2 . 4 . 1 / hadoop - yarn / hadoop - yarn - site / CapacityScheduler . html . [ 8 ] China’s 100 Million Surveillance Cameras . https : / / goo . gl / UK3Obl . [ 9 ] Genetec . https : / / www . genetec . com / . [ 10 ] Hadoop Fair Scheduler . https : / / hadoop . apache . org / docs / r2 . 4 . 1 / hadoop - yarn / hadoop - yarn - site / FairScheduler . html . [ 11 ] Linux Containers LXC Introduction . https : / / linuxcontainers . org / lxc / introduction / . [ 12 ] One Surveillance Camera for Every 11 People in Britain , Says CCTV Survey . https : / / goo . gl / cHLqiK . [ 13 ] Open ALPR . http : / / www . openalpr . com . [ 14 ] OpenCV Documentation : Introduction to SIFT ( Scale - Invariant Feature Transform ) . http : / / docs . opencv . org / 3 . 1 . 0 / da / df5 / tutorial _ py _ sift _ intro . html . [ 15 ] OpenCV Documentation : Introduction to SURF ( Speeded - Up Robust Features ) . http : / / docs . opencv . org / 3 . 0 - beta / doc / py _ tutorials / py _ feature2d / py _ surf _ intro / py _ surf _ intro . html . [ 16 ] SR 520 Bridge Tolling , WA . https : / / www . wsdot . wa . gov / Tolling / 520 / default . htm . [ 17 ] Turnpike Enterprise Toll - by - Plate , FL . https : / / www . tollbyplate . com / index . [ 18 ] Windows Job Objects . https : / / msdn . microsoft . com / en - us / library / windows / desktop / ms684161 ( v = vs . 85 ) . aspx . [ 19 ] D . J . Abadi et al . The Design of the Borealis Stream Pro - cessing Engine . In CIDR , Jan . 2005 . [ 20 ] S . Agarwal , B . Mozafari , A . Panda , M . H . , S . Madden , and I . Stoica . BlinkDB : Queries with Bounded Errors and Bounded Response Times on Very Large Data . In ACM EuroSys , Apr . 2013 . [ 21 ] T . Akidau et al . The Dataﬂow Model : A Practical Ap - proach to Balancing Correctness , Latency , and Cost in Massive - Scale , Unbounded , Out - of - order Data Process - ing . Proceedings of the VLDB Endowment , Aug . 2015 . [ 22 ] L . Amini , N . Jain , A . Sehgal , J . Silber , and O . Verscheure . Adaptive Control of Extreme - Scale Stream Processing Systems . In IEEE ICDCS , July 2006 . [ 23 ] G . Ananthanarayanan , M . C . - C . Hung , X . Ren , I . Stoica , A . Wierman , and M . Yu . GRASS : Trimming Stragglers in Approximation Analytics . In USENIX NSDI , Apr . 2014 . [ 24 ] A . AuYoung , A . Vahdat , and A . C . Snoeren . Evaluat - ing the Impact of Inaccurate Information in Utility - Based Scheduling . In Proceedings of the Conference on High Performance Computing Networking , Storage and Analy - sis , Nov . 2009 . [ 25 ] B . Babcock , M . Datar , and R . Motwani . Load Shedding for Aggregation Queries over Data Streams . In IEEE ICDE , Mar . 2004 . [ 26 ] A . Beloglazov and R . Buyya . Energy Efﬁcient Resource Management in Virtualized Cloud Data Centers . In IEEE CCGRID , May 2010 . [ 27 ] A . Bhattacharya , D . Culler , E . Friedman , A . Ghodsi , S . Shenker , and I . Stoica . Hierarchical Scheduling for Di - verse Datacenter Workloads . In ACM SoCC , Nov . 2014 . [ 28 ] J . Blythe . Visual Exploration and Incremental Utility Elicitation . In AAAI , July 2002 . [ 29 ] N . Bobroff , A . Kochut , and K . Beaty . Dynamic Placement of Virtual Machines for Managing SLA Violations . In IFIP / IEEE International Symposium on Integrated Net - work Management , 2007 . [ 30 ] C . Boutilier , R . Patrascu , P . Poupart , and D . Schuurmans . Regret - based Utility Elicitation in Constraint - based Deci - sion Problems . In IJCAI , 2005 . [ 31 ] E . Boutin , J . Ekanayake , W . Lin , B . Shi , J . Zhou , Z . Qian , M . Wu , and L . Zhou . Apollo : Scalable and Coordinated Scheduling for Cloud - Scale Computing . In USENIX OSDI , 2014 . [ 32 ] B . Calder et al . Windows Azure Storage : A Highly Avail - able Cloud Storage Service with Strong Consistency . In ACM SOSP , 2011 . [ 33 ] D . Carney , U . C¸etintemel , M . Cherniack , C . Convey , S . Lee , G . Seidman , M . Stonebraker , N . Tatbul , and S . Zdonik . Monitoring Streams : a New Class of Data Management Applications . In VLDB , 2002 . [ 34 ] D . Carney , U . C¸etintemel , A . Rasin , S . Zdonik , M . Cher - niack , and M . Stonebraker . Operator Scheduling in a Data Stream Manager . In VLDB , 2003 . [ 35 ] U . Chajewska , D . Koller , and R . Parr . Making Rational Decisions Using Adaptive Utility Elicitation . In AAAI , 2000 . [ 36 ] B . Chandramouli , J . Goldstein , M . Barnett , R . DeLine , D . Fisher , J . Wernsing , and D . Rob . Trill : A High - Performance Incremental Query Processor for Diverse Analytics . In USENIX NSDI , 2014 . [ 37 ] M . Cherniack , H . Balakrishnan , M . Balazinska , D . Car - ney , U . Cetintemel , Y . Xing , and S . Zdonik . Scalable Distributed Stream Processing . In CIDR , Jan . 2003 . [ 38 ] G . Cormode , M . Garofalakis , P . J . Haas , and C . Jer - maine . Synopses for Massive Data : Samples , His - tograms , Wavelets , Sketches . Foundations and Trends in Databases , Jan . 2012 . [ 39 ] C . Curino , D . E . Difallah , C . Douglas , S . Krishnan , R . Ra - makrishnan , and S . Rao . Reservation - based Scheduling : If You’re Late Don’t Blame Us ! Nov . 2014 . [ 40 ] G . B . Dantzig . Discrete - Variable Extremum Problems . Operations Research 5 ( 2 ) : 266288 , 1957 . [ 41 ] J . Deng , W . Dong , R . Socher , L . - J . Li , K . Li , and L . Fei - Fei . ImageNet : A Large - Scale Hierarchical Image Database . In CVPR , 2009 . [ 42 ] A . D . Ferguson , P . Bodik , S . Kandula , E . Boutin , and R . Fonseca . Jockey : Guaranteed Job Latency in Data Par - allel Clusters . In ACM EuroSys , 2012 . [ 43 ] A . Ghodsi , M . Zaharia , B . Hindman , A . Konwinski , S . Shenker , and I . Stoica . Dominant Resource Fairness : Fair Allocation of Multiple Resource Types . In USENIX NSDI , 2011 . [ 44 ] R . Grandl , G . Ananthanarayanan , S . Kandula , S . Rao , and A . Akella . Multi - Resource Packing for Cluster Sched - ulers . In ACM SIGCOMM , 2014 . [ 45 ] S . Han , H . Mao , and W . J . Dally . Deep Compres - sion : Compressing Deep Neural Network with Pruning , Trained Quantization and Huffman Coding . Computing Research Repository , Nov . 2015 . [ 46 ] S . Han , H . Shen , M . Philipose , S . Agarwal , A . Wolman , and A . Krishnamurthy . MCDNN : An Approximation - Based Execution Framework for Deep Stream Processing Under Resource Constraints . In ACM MobiSys , 2016 . [ 47 ] J . M . Hellerstein , P . J . Haas , and H . J . Wang . Online Ag - gregation . In ACM SIGMOD , 1997 . [ 48 ] A . Hernando , R . Sanz , and R . Calinescu . A Model - Based Approach to the Autonomic Management of Mo - bile Robot Resources . In International Conference on Adaptive and Self - Adaptive Systems and Applications , 2010 . [ 49 ] B . Hindman , A . Konwinski , M . Zaharia , A . Ghodsi , A . D . Joseph , R . Katz , S . Shenker , and I . Stoica . Mesos : A Platform for Fine - Grained Resource Sharing in the Data Center . In USENIX NSDI , 2011 . [ 50 ] D . E . Irwin , L . E . Grit , and J . S . Chase . Balancing Risk and Reward in a Market - Based Task Service . In IEEE In - ternational Symposium on High Performance Distributed Computing , 2004 . [ 51 ] J . Jaffe . Bottleneck Flow Control . IEEE Transactions on Communications , 29 ( 7 ) : 954 – 962 , 1981 . [ 52 ] E . D . Jensen , P . Li , and B . Ravindran . On Recent Ad - vances in Time / Utility Function Real - Time Scheduling and Resource Management . IEEE International Sympo - sium on Object and Component - Oriented Real - Time Dis - tributed Computing , 2005 . [ 53 ] C . Jermaine , S . Arumugam , A . Pol , and A . Dobra . Scal - able Approximate Query Processing with the DBO En - gine . ACM Transactions on Database Systems , 33 ( 4 ) : 23 , 2008 . [ 54 ] Y . Jia , E . Shelhamer , J . Donahue , S . Karayev , J . Long , R . Girshick , S . Guadarrama , and T . Darrell . Caffe : Con - volutional Architecture for Fast Feature Embedding . In ACM International Conference on Multimedia , 2014 . [ 55 ] R . Johari and J . N . Tsitsiklis . Efﬁciency Loss in a Net - work Resource Allocation Game . Mathematics of Oper - ations Research , 29 ( 3 ) : 407 – 435 , 2004 . [ 56 ] S . A . Jyothi , C . Curino , I . Menache , S . M . Narayana - murthy , A . Tumanov , J . Yaniv , ´I . Goiri , S . Krishnan , J . Kulkarni , and S . Rao . Morpheus : towards automated SLOs for enterprise clusters . In USENIX OSDI , 2016 . [ 57 ] F . P . Kelly , A . K . Maulloo , and D . K . Tan . Rate Control for Communication Networks : Shadow Prices , Propor - tional Fairness and Stability . Journal of the Operational Research Society , 49 ( 3 ) : 237 – 252 , 1998 . [ 58 ] J . O . Kephart . Research Challenges of Autonomic Com - puting . In ACM ICSE , 2005 . [ 59 ] M . Kristan , J . Matas , A . Leonardis , M . Felsberg , L . Ce - hovin , G . Fernandez , T . Vojir , G . Hager , G . Nebehay , and R . Pﬂugfelder . The Visual Object Tracking ( VOT ) Chal - lenge Results . In IEEE ICCV Workshops , Dec . 2015 . [ 60 ] V . Kumar , B . F . Cooper , and K . Schwan . Distributed Stream Management Using Utility - Driven Self - Adaptive Middleware . In IEEE ICAC , 2005 . [ 61 ] R . Levy , J . Nagarajarao , G . Paciﬁci , M . Spreitzer , A . Tantawi , and A . Youssef . Performance management for cluster based web services . In Integrated Network Management VIII , pages 247 – 261 . Springer , 2003 . [ 62 ] W . Lin , Z . Qian , J . Xu , S . Yang , J . Zhou , and L . Zhou . StreamScope : Continuous Reliable Distributed Process - ing of Big Data Streams . In USENIX NSDI , Mar . 2016 . [ 63 ] S . H . Low and D . E . Lapsley . Optimization Flow Control - I : Basic Algorithm and Convergence . IEEE / ACM Trans - actions on Networking , 7 ( 6 ) : 861 – 874 , 1999 . [ 64 ] P . Marbach . Priority Service and Max - Min Fairness . In IEEE INFOCOM , 2002 . [ 65 ] R . C . Merton . Continuous - Time Finance . Blackwell , 1990 . [ 66 ] D . Minarolli and B . Freisleben . Utility - Based Resource Allocation for Virtual Machines in Cloud Computing . In IEEE Symposium on Computers and Communications , pages 410 – 417 , 2011 . [ 67 ] M . Morari and J . H . Lee . Model Predictive Control : Past , Present and Future . Computers & Chemical Engineering , 23 ( 4 ) : 667 – 682 , 1999 . [ 68 ] R . Motwani , J . Widom , A . Arasu , B . Babcock , S . Babu , M . Datar , G . Manku , C . Olston , J . Rosenstein , and R . Varma . Query Processing , Resource Management , and Approximation in a Data Stream Management System . In CIDR , 2003 . [ 69 ] H . Nam and B . Han . Learning Multi - Domain Convolu - tional Neural Networks for Visual Tracking . Computing Research Repository , abs / 1510 . 07945 , 2015 . [ 70 ] P . Padala , K . G . Shin , X . Zhu , M . Uysal , Z . Wang , S . Singhal , A . Merchant , and K . Salem . Adaptive Con - trol of Virtualized Resources in Utility Computing Envi - ronments . In ACM SIGOPS Operating Systems Review , volume 41 , pages 289 – 302 , 2007 . [ 71 ] R . Panigrahy , K . Talwar , L . Uyeda , and U . Wieder . Heuristics for Vector Bin Packing . In Microsoft Research Technical Report , Jan . 2011 . [ 72 ] A . Rabkin , M . Arye , S . Sen , V . Pai , and M . Freedman . Aggregation and Degradation in JetStream : Streaming Analytics in the Wide Area . In USENIX NSDI , 2014 . [ 73 ] B . T . Ratchford . Cost - Beneﬁt Models for Explaining Consumer Choice and Information Seeking Behavior . Management Science , 28 , 1982 . [ 74 ] S . J . Russell and P . Norvig . Artiﬁcial Intelligence : A Mod - ern Approach . Pearson Education , 2nd edition , 2003 . [ 75 ] K . Simonyan and A . Zisserman . Very Deep Convo - lutional Networks for Large - Scale Image Recognition . Computing Research Repository , abs / 1409 . 1556 , 2014 . [ 76 ] J . Snoek , H . Larochelle , and R . P . Adams . Practical Bayesian Optimization of Machine Learning Algorithms . In NIPS , Dec . 2012 . [ 77 ] M . Steinder , I . Whalley , D . Carrera , I . Gaweda , and D . Chess . Server Virtualization in Autonomic Manage - ment of Heterogeneous Workloads . In IFIP / IEEE Inter - national Symposium on Integrated Network Management , 2007 . [ 78 ] N . Tatbul , U . C¸etintemel , and S . Zdonik . Staying Fit : Ef - ﬁcient Load Shedding Techniques for Distributed Stream Processing . In VLDB , 2007 . [ 79 ] N . Tatbul , U . C¸etintemel , S . Zdonik , M . Cherniack , and M . Stonebraker . Load Shedding in a Data Stream Man - ager . In VLDB , 2003 . [ 80 ] G . Tesauro , R . Das , W . E . Walsh , and J . O . Kephart . Utility - Function - Driven Resource Allocation in Auto - nomic Systems . In ICAC , 2005 . [ 81 ] Y . - C . Tu , M . Hefeeda , Y . Xia , S . Prabhakar , and S . Liu . Control - Based Quality Adaptation in Data Stream Man - agement Systems . In Database and Expert Systems Ap - plications , 2005 . [ 82 ] Y . - C . Tu , S . Liu , S . Prabhakar , and B . Yao . Load Shed - ding in Stream Databases : a Control - Based Approach . In VLDB , 2006 . [ 83 ] C . J . Van Rijsbergen . Information Retrieval . Butterworth , 2nd edition , 1979 . [ 84 ] S . Venkataraman , A . Panda , G . Ananthanarayanan , M . J . Franklin , and I . Stoica . The Power of Choice in Data - aware Cluster Scheduling . In USENIX OSDI , 2014 . [ 85 ] A . Verma , L . Cherkasova , and R . H . Campbell . ARIA : Automatic Resource Inference and Allocation for Mapre - duce Environments . In ICAC , 2011 . [ 86 ] A . Verma , L . Pedrosa , M . Korupolu , D . Oppenheimer , E . Tune , and J . Wilkes . Large - scale cluster management at Google with Borg . In ACM EuroSys , 2015 . [ 87 ] E . Wandeler and L . Thiele . Real - time interfaces for interface - based design of real - time systems with ﬁxed pri - ority scheduling . In Proceedings of the 5th ACM interna - tional conference on Embedded software , pages 80 – 89 . ACM , 2005 . [ 88 ] Y . Wei , V . Prasad , S . H . Son , and J . A . Stankovic . Prediction - Based QoS Management for Real - Time Data Streams . In IEEE RTSS , 2006 . [ 89 ] M . Zaharia , T . Das , H . Li , T . Hunter , S . Shenker , and I . Stoica . Discretized Streams : Fault - Tolerant Streaming Computation at Scale . In ACM SOSP , 2013 .