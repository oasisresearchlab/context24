Cognitive Mirage : A Review of Hallucinations in Large Language Models Hongbin Ye , Tong Liu , Aijia Zhang , Wei Hua , Weiqiang Jia Zhejiang Lab { yehongbin , jiaweiqiang } @ zhejianglab . com Abstract As large language models continue to develop in the field of AI , text generation systems are susceptible to a worrisome phenomenon known as hallucination . In this study , we summa - rize recent compelling insights into halluci - nations in LLMs . We present a novel taxon - omy of hallucinations from various text genera - tion tasks , thus provide theoretical insights , de - tection methods and improvement approaches . Based on this , future research directions are proposed . Our contribution are threefold : ( 1 ) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks ; ( 2 ) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods ; ( 3 ) We propose several research directions that can be developed in the future . As hallucinations gar - ner significant attention from the community , we will maintain updates on relevant research progress 1 . 1 Introduction In the ever - evolving realm of large language mod - els ( LLMs ) , a constellation of innovative creations has emerged , such as GPT - 3 ( Brown et al . , 2020 ) , InstructGPT ( Ouyang et al . , 2022 ) , FLAN ( Wei et al . , 2022a ) , PaLM ( Chowdhery et al . , 2022 ) , LLaMA ( Touvron et al . , 2023 ) and other notable contributors ( Bai et al . , 2022 ; Zhang et al . , 2022 ; Zeng et al . , 2023 ; Xu et al . , 2023 ) . These models implicitly encode global knowledge within their pa - rameters during the pre - training phase ( Han et al . , 2021 ; Huang and Chang , 2023 ) , offering valuable insights as knowledge repositories for downstream tasks ( Pu and Demberg , 2023 ; Kojima et al . , 2022 ; Wei et al . , 2022b ) . Nevertheless , the generalization of knowledge can result in memory distortion , an inherent limitation that may give rise to potential inaccuracies ( Yu et al . , 2023b ) . Moreover , their 1 https : / / github . com / hongbinye / Cognitive - Mirage - Hallucinations - in - LLMs In which sport did the Czech stars Daniel Vacek and Hana Mandlíková gain professional status ? Daniel Vacek and Hana Mandlíková both gained professional status in cricket . ◆ Daniel Vacek ( born 1 April 1971 ) is a former tennis player from Czechoslovakia and the Czech Republic who turned professional in 1990 . ◆ Hana Mandlíková ( born 19 February 1962 ) is a former professional tennis player from Czechoslovakia who later obtained Australian citizenship . Daniel Vacek and Hana Mandlíková both gained professional status in tennis . Figure 1 : Illustration of Hallucination in LLMs . While the initial response appears fluent , it fails to align with the world knowledge retrieved from the external knowl - edge base . ability to represent knowledge is constrained by model scale and faces challenges in addressing long - tailed knowledge problems ( Kandpal et al . , 2023 ; Mallen et al . , 2023 ) . While the privacy and timeliness of data in the real world ( Lazaridou et al . , 2022 ; Shi et al . , 2023b ) unfortunately exacerbate this problem , leaving models difficult to maintain a comprehensive and up - to - date understanding of the facts . These challenges present a serious obsta - cle to the reliability of LLMs , which we refer to as hallucination . ( Yu et al . , 2022 ) . A prominent example of this drawback is that models typically generate statements that appear reasonable but are either cognitively irrelevant or factually incorrect . In light of this observation , hallucinations remain a critical challenge in medical ( Dash et al . , 2023 ; Umapathi et al . , 2023 ) , financial ( Gill et al . , 2023 ) and other knowledge - intensive fields due to the exacting accuracy requirements . Particularly , the applications for legal case drafting showcase plau - sible interpretation as an aggregation of diverse subjective perspectives ( Curran et al . , 2023 ) . a r X i v : 2309 . 06794v1 [ c s . C L ] 13 S e p 2023 Mechanism Analysis ( § 2 ) Data Collection Knowledge Gap Optimization Process Taxonomy of Hallucination ( § 3 ) Machine Translation Question and Answer Dialog System Summarization System Knowledge Graph with LLMs Cross - modal System Hallucination Detection ( § 4 ) Inference Classifier Uncertainty Metric Self - Evaluation Evidence Retrieval Hallucination Correction ( § 5 ) Parameter Adaptation Post - hoc Attribution and Edit Technology Leverage External Knowledge Assessment Feedback Mindset Society Future Directions ( § 6 ) Data Construction Management Downstream Task Alignment Reasoning Mechanism Exploitation Multi - modal Hallucination Survey Hallucinationin LLMs Definition Related work Figure 2 : The overview structure of this review . We firstly analyze three crucial factors that contribute to hallu - cinations and refine the categorization of hallucinations across text generation tasks . Subsequently , we dutifully report current methods for detecting and mitigating hallucinations . Finally , we propose several potential research directions to address evolving problems of hallucinations . Definition of Hallucination . As depicted in Fig - ure 1 , hallucination refers to the generation of texts or responses that exhibit grammatical correctness , fluency , and authenticity , but deviate from the pro - vided source inputs ( faithfulness ) or do not align with factual accuracy ( factualness ) ( Ji et al . , 2023 ) . In traditional NLP tasks ( Maynez et al . , 2020 ) , hal - lucinations are often synonymous with faithfulness : conflicting information leads to Intrinsic Hallu - cination , i . e . , LMs conflict with the input infor - mation when generating a response ; Conversely , generating ambiguous supplementary information may leads to Extrinsic Hallucination , i . e . , LMs pro - duce personal names , historical events , or technical documents that is challenging to verify . LLMs - oriented hallucinations instead prioritize factual - ness , focusing on whether the result can be evi - denced or negated by reference to external facts in the real world . Uncritical trust in LLMs can give rise to a phenomenon Cognitive Mirage , contribut - ing to misguided decision - making and a cascade of unintended consequences ( Zhang et al . , 2023a ) . Present work To effectively control the risk of hallucinations , we summarize recent progress in hallucination theories and solutions in this paper . We propose to organize relevant work by a compre - hensive survey ( Figure 2 ) : • Theoretical insight and mechanism analy - sis . We provide in - depth theoretical and mech - anism analysis from three typical perspectives : data collection , knowledge gap and optimiza - tion process , reviewing the recent and relevant theories related to hallucinations ( § 2 ) . • Taxonomy of hallucination in LLMs . We conduct the comprehensive review of halluci - nation in LLMs together with task axis . We re - view the task - specific benchmarks with a com - prehensive comparison and summary ( § 3 ) . • Wide coverage on emerging hallucination detection and correction methods . We pro - pose a comprehensive investigation into the proactive detection ( § 4 ) and mitigation of hal - lucinations ( § 5 ) in the era of LLMs . This is critical to study the most popular techniques for inspiring future research directions ( § 6 ) . Related work As this topic is relatively nascent , only a few surveys exist . Closest to our work , Ji et al . ( 2023 ) analyzes hallucinatory content in task - specific research progress , which focuses on early works in natural language generation field . Cur - rently there are significant efforts to address hal - lucination in LLMs . Wang et al . ( 2023f ) covers methods for effectively collecting high - quality in - structions for LLM alignment , including the use of NLP benchmarks , human annotations , and leverag - ing strong LLMs . Pan et al . ( 2023 ) discusses self - correcting methods where LLM itself is prompted or guided to correct the hallucinations from its own outputs . Despite some benchmarks ( Lin et al . , 2022 ; Li et al . , 2023b ; Mihindukulasooriya et al . , 2023 ) is constructed to evaluate whether LLMs are able to generate factual responses , these works scattered among various tasks have not been sys - tematically reviewed and analyzed . Different from those surveys , in this paper , we conduct a litera - ture review on hallucinations in LLMs , hoping to systematically understand the methodologies , com - pare different methods and inspire new ideas . 2 Mechanism Analysis For the sake of clean exposition , this section pro - vides theoretical insight into mechanism analysis for hallucinations in LLMs . As a regular LLM , the generative objective is modeled by a parameterized probabilistic model p gen , and sampled to predict the next token in the sentence , thus generating the entire sentence : p gen ( y i ) = F θ ( I , D , x , y i < ) ( 1 ) where y i represents probable tokens at each step that can be selected by beam search from vocabu - lary V . Note that the instructions I utilize a variety of predefined templates according to different tasks ( Yin et al . , 2023a ) . Multifarious and high - quality in - context demonstrations D are aimed at provid - ing analogy samples to reduce the cost of adapt - ing models to new tasks ( Chen et al . , 2022a ) . Pa - rameters θ implicitly memorize corpus knowledge through diverse architectural F such as decoder - only , encoder - only , or encoder - decoder LLMs . As LLM - based systems can exhibit a variety of hallu - cinations , we summarise three primary mechanism types for theoretical analysis , and each mechanism is correlated with a distinct training factor . Data Collection The parameters are implicitly stored within model as a priori knowledge acquired during pre - training process . Given the varying qual - ity and range of knowledge within pre - trained cor - pus , the information incorporated into the LLMs may be incomplete or outdated . In cases where pertinent memories are unavailable , the LLM’s performance may deteriorates , resorting to rudi - mentary corpus - based heuristics that rely on term frequencies to render judgements ( McKenna et al . , 2023 ) . Another bias stems from the capacity for contextual learning ( Chan et al . , 2022 ) when a few demonstrations are introduced as input to the prefix context . Previous research ( Wang et al . , 2023d ; Lu et al . , 2022 ) has demonstrated that the acquisition of knowledge through model learning demonstra - tions depends on disparities in label categories and the order of demonstration samples . Likewise , mul - tilingual LLMs encounter challenges related to hal - lucinations , particularly in handling language pairs with limited resources or non - English translations ( Guerreiro et al . , 2023a ) . Furthermore , cutting - edge Large Vision - Language Models ( LVLMs ) ex - hibit instances of hallucinating common objects within visual instructional datasets and prone to objects that frequently co - occur in the same image ( Liu et al . , 2023b ; Li et al . , 2023i ) . Knowledge Gap Knowledge gaps are typically attributed to differences in input format between the pre - training and fine - tuning stages ( Zheng et al . , 2023b ) . Even when considering the automatic up - dating of textual knowledge bases , the output can deviate from the expected corrections ( Huang et al . , 2023 ) . For example , questions often do not align effectively with stored knowledge , and the avail - able information remains unknown until the ques - tions are presented . This knowledge gap poses thorny challenges in balancing memory with re - trieved evidence , which is construed as a passive defense mechanism against the misuse of retrieval ( Gao et al . , 2023a ) . To in - depth analyses this is - sue , Zhang et al . ( 2023b ) and Halawi et al . ( 2023 ) propose that disregarding retrieved evidence intro - duces biased model knowledge , while mis - covering and over - thinking disrupt model behavior . Further - more , in scenarios where a cache component is utilized to offer historical memory during training ( Wan et al . , 2023 ) , the model also experiences in - consistency between the present hidden state and the hidden state stored in the cache . Optimization Process The maximum likelihood estimation and teacher - forcing training have the po - tential to result in a phenomenon known as stochas - tic parroting ( Chiesurin et al . , 2023 ) , wherein the model is prompted to imitate the training data with - out comprehension ( Kang and Hashimoto , 2020 ) . Specifically , exposure bias between the training and testing stages have been demonstrated to lead to hallucinations within LLMs , particularly when generating lengthy responses ( Wang and Sennrich , 2020 ) . Besides , sampling techniques characterized by high uncertainty ( Lee et al . , 2022 ) , such as top - p and top - k , exacerbate the issue of hallucination . Furthermore , Zhang et al . ( 2023a ) observes that LLMs tend to produce snowballing hallucinations to maintain coherence with earlier hallucinations , and even when directed with prompts as " Let’s think step by step " , they still generate ineffectual chains of reasoning ( Kojima et al . , 2022 ) . 3 Taxonomy of Hallucination In this paper , we mainly consider representative hallucinations , which are widely observed in var - ious downstream tasks , i . e . Machine Translation , Question and Answer , Dialog System , Summariza - tion System , Knowledge graph with LLMs , and Vi - sual Question Answer . As shown in Table 1 , these hallucinations are identified complex taxonomy in numerous mainstream tasks associated with LLMs . In the following sections , we will introduce repre - sentative types of hallucinations to be resolved . • Machine Translation . Since perturbations ( e . g . , spellings or capital errors ) can induce hallucina - tions reliably , traditional machine translation mod - els tend to validate instances memorised by the model when subjected to perturbations ( Bawden and Yvon , 2023 ; Hendy et al . , 2023 ) . It is worth noting that hallucinations generated by LLMs are mainly translation off - target , over - generation , or failed translation attempts ( Guerreiro et al . , 2023a ) . While in low - resource language setting , most mod - els exhibit subpar performance due to the lack of annotated data ( Dale et al . , 2023 ) . In contrast , they are vulnerable to increased amount of pre - trained languages in multilingual setting ( Conneau et al . , 2020 ) . Subsequently , familial LLMs trained on dif - ferent scales of monolingual data are proved to be viscous ( Guerreiro et al . , 2023a ) , as the source of oscillatory hallucination pathology . • Question and Answer . Imperfect responses suf - fer from flawed external knowledge , knowledge recall cues and reasoning instruction ( Zheng et al . , 2023b ) . For example , LLMs are mostly unable to avoid answering when provided with no relevant information , instead provide incomplete and plausi - ble answers ( Adlakha et al . , 2023 ) . In additon to ex - ternal knowledge , memorized information without accurate , reliable and accessible source also con - tributes to different types of hallucinations ( Umap - athi et al . , 2023 ) . Though scaling laws suggest that perplexity on the training distribution is positively correlated with parameter size , ( Lin et al . , 2022 ) further discovers that scaling up models should increase the rate of imitative falsehoods . • Dialog System . Some studies view dialogue models as unobtrusive imitators , which simulates the distributional properties of data instead of gen - erating faithful output . For example , Uncooper - ativeness responses ( Dziri et al . , 2022b ) originat - ing from discourse phenomena inclines to output an exact copy of the entire evidence . Das et al . ( 2022 ) reports more nuanced hallucinations in KG - grounded dialogue systems as analyzed through human feedback . Similarly , FaithDial ( Dziri et al . , 2022a ) , BEGIN ( Dziri et al . , 2022c ) , MixCL ( Sun et al . , 2023 ) all implement experiments on the WoW dataset to conduct a meta - evaluation of the hallucination in knowledge grounded dialogue . • Summarization System . Automatically gener - ated abstracts based on LLMs may be fluent , but they still typically lack faithfulness to the source document . Compared to the human evaluation of traditional summarization models ( Maynez et al . , 2020 ) , the summarizations generated by LLMs can be categorized into two major types : intrinsic hal - lucinations that distort the information present in the document ; extrinsic hallucinations that provide additional information that cannot be directly at - tributed to the document ( Qiu et al . , 2023 ) . Note that extrinsic hallucination as a metrics of factu - ally consistent continuation of inputs in LLMs is given more attention in summarisation systems ( Tam et al . , 2023 ; Shen et al . , 2023 ) . Furthermore , Cao et al . ( 2022 ) subdivides extrinsic hallucina - tions into factual and non - factual hallucinations . The former provides additional world knowledge , which may benefit comprehensive understanding . • Knowledge Graph with LLMs . Despite the promising progress in knowledge - based text gen - eartion , it encounters intrinsic hallucinations in - herent to the process where the generated text not only covers the input information but also incor - porates redundant details derived from its internal memorized knowledge ( Yuan and Färber , 2023 ) . To address this , Yu et al . ( 2023a ) establish a distinc - tion between correctly generated knowledge and knowledge hallucinations in terms of knowledge creation . Notably , the Virtual Knowledge Extrac - tion ( Zhu et al . , 2023d ) underscores the potential generalization capabilities of LLMs in the realms of constructing and inferring from knowledge graphs . Mihindukulasooriya et al . ( 2023 ) further empower LLMs to produce interpretable fact - checks through a neural symbolic approach . Based on their fidelity to the source , hallucinations are defined as subject hallucination , relation hallucination , and object hallucination . • Cross - modal System . Augmented by the supe - rior language capabilities of LLMs , performance Paper Task Architecture Resources Hallucination Types Research Method Raunak et al . ( 2021 ) MachineTranslation Enc - Dec IWSLT - 2014 Under perturbation , Natu - ral hallucination Source perturbation Guerreiro et al . ( 2023b ) MachineTranslation Enc - Dec WMT2018 Oscillatory hallucination , Largely fluent hallucina - tion Consider a natural scenario Dale et al . ( 2023 ) MachineTranslation Enc - Dec FLORES - 200 , Jig - saw , Wikipedia Full hallucination , Partial hallucination , Word - level hallucination Introduce pathology detection Pfeiffer et al . ( 2023 ) MultilingualSeq2seq Enc - Dec XQuAD , TyDi , XNLI , XL - Sum , MASSIVE Source language halluci - nation Evaluate source lan - guage hallucination Lin et al . ( 2022 ) QuestionandAnswer Enc - Dec , Only - Dec TruthfulQA Imitative falsehoods Cause imitative false - hoods Zheng et al . ( 2023b ) QuestionandAnswer Only - Dec HotpotQA , BoolQ Comprehension , Factual - ness , Specificity , Infer - ence Hallucination Manual analysis of re - sponses Adlakha et al . ( 2023 ) QuestionandAnswer Enc - Dec , Only - Dec NQ , HotpotQA , TopiOCQA Semantic equivalence , Symbolic equivalence , Intrinsic ambiguity , Gran - ularity discrepancies , Incomplete , Enumeration , Satisfactory Subset Evaluate retrieval aug - mented QA Umapathi et al . ( 2023 ) QuestionandAnswer Only - Dec MEDMCQA , Headqa , USMILE , Medqa , Pubmed Reasoning hallucination , Memory - based hallucina - tion Medical benchmark Med - HALT Dziri et al . ( 2022b ) DialogSystem Enc - Dec , Only - Dec WoW , CMU - DOG , TopicalChat Hallucination , Partial hal - lucination , Generic , Un - cooperative Infer exclusively from the knowledge - snippet Das et al . ( 2022 ) DialogSystem Only - Dec OpenDialKG Extrinsic - Soft / Hard / Grouped , Intrinsic - Soft / Hard / Repetitive , History Corrupted Analyze entity - level fact hallucination Dziri et al . ( 2022a ) DialogSystem Enc - Dec , Only - Dec WoW Hallucination , Generic , Uncooperativeness Hallucination - free benchmark FaithDial Dziri et al . ( 2022c ) DialogSystem Enc - Dec , Only - Enc , Only - Dec WoW , CMU - DOG , TopicalChat Fully attributable , Not at - tributable , Generic Knowledge - grounded interaction benchmark Begin Sun et al . ( 2023 ) DialogSystem Enc - Dec , Only - Dec WoW Intrinsic hallucination , Extrinsic hallucination Sample responses for conversation Tam et al . ( 2023 ) SummarizationSystem Enc - Dec , Only - Dec CNN / DM , XSum Factually inconsistent summaries Generate summaries from given models Cao et al . ( 2022 ) SummarizationSystem Enc - Dec , Only - Dec MENT Non - hallucinated , Factual hallucination , Non - factual hallucination , Intrinsic hallucination Label factual entities from summarizations Shen et al . ( 2023 ) SummarizationSystem Enc - Dec , Only - Enc NHNet News headline hallucina - tion Majority vote of jour - nalism degree holders Qiu et al . ( 2023 ) SummarizationSystem MultipleADapters XL - Sum Intrinsic hallucination , Extrinsic hallucination In a cross - lingual transfer setting Yu et al . ( 2023a ) Knowledge - based text generation Enc - Dec , Only - Dec Encyclopedic , ETC Knowledge hallucination Evaluate knowledge creating ability given known facts Mihindukulasooriyaetal . ( 2023 ) Knowledgegraphgener - ation Only - Dec TekGen , WebNLG Subject hallucination , re - lation hallucination , ob - ject hallucination Ontology driven KGC benchmark Text2KGBench Li et al . ( 2023i ) Visual Ques - tion Answer Enc - Dec MSCOCO Object hallucination Caption hallucination assessment Table 1 : List of Representative Hallucination H a ll u c i n a ti on D e t ec ti on Inference Classifier FIB ( Tam et al . , 2023 ) , ExHalder ( Shen et al . , 2023 ) , HaluEval ( Li et al . , 2023b ) , Fact - checking ( Mahmood et al . , 2023 ) Uncertainty Metric BARTScore ( Yuan et al . , 2021 ) , KoK ( Amayuelas et al . , 2023 ) , SLAG ( Hase et al . , 2023 ) , KLD ( Pezeshkpour , 2023 ) , POLAR ( Zhao et al . , 2023b ) , ASTSN ( Varshney et al . , 2023 ) Self - Evaluation LM - know ( Kadavath et al . , 2022 ) , SelfCheckGPT : ( Manakul et al . , 2023 ) , Do - LLM - Know ( Agrawal et al . , 2023 ) , EOH ( Li et al . , 2023i ) , Self - Checker ( Li et al . , 2023d ) , LM - vs - LM ( Cohen et al . , 2023 ) , Self - Contradictory ( Mündler et al . , 2023 ) Evidence Retrieval FActScore ( Min et al . , 2023 ) , CCV ( Chen et al . , 2023b ) , RSE ( Huo et al . , 2023 ) , FacTool ( Chern et al . , 2023 ) Figure 3 : Taxonomy of Hallucination Detection . of cross - modal tasks achieves promising progress ( Zhu et al . , 2023a ; Liu et al . , 2023b ) . However , de - spite replacing the original language encoder with LLMs , Large Visual Language Models ( LVLMs ) ( Wang et al . , 2022 ) still generate object descriptions that not present in the target image , denoted as ob - ject hallucinations ( Li et al . , 2023i ) . Especially , the various failure cases could be typically found in Visual Question Answering ( Li et al . , 2023i ) , Image Captioning ( Biten et al . , 2022 ; Petryk et al . , 2023 ; Ning et al . , 2023 ) and Report Generation ( Mahmood et al . , 2023 ) etc . cross - modal tasks . 4 Hallucination Detection Conventional hallucination detection mainly de - pends on task - specific metrics , such as ROUGE and BLEU to evaluate the information overlap be - tween source and target texts in summarization tasks ( Pagnoni et al . , 2021 ) , while knowledge F1 to estimate the knowledge - aware ability of response generation ( Li et al . , 2022 ) . These metrics focus on measuring faithfulness of references and fail to pro - vide an assessment of factualness . Despite some reference - free works are proposed , plugin - based methods ( Dong et al . , 2022 ) suffer from world knowledge limitation . QA - based matching metrics ( Durmus et al . , 2020 ) lack knowledge completeness of source information . NLI - based methods ( Dziri et al . , 2022c ) are unable to support finer - grained hallucination checking as they are sentence - level , besides entailment and hallucination problems are not equivalent . As the paradigm shift in hallucina - tion detection arising from the rapid development of LLMs , we present a novel taxonomy in Fig 3 and introduce each category in following sections . • Inference Classifier . The most straightforward strategy involves adopting classifiers to assess the likelihood of hallucinations . Concretely , given a question Q and an answer A , an inferential classi - fier C can be asked to determine whether the answer contains hallucinatory content H via computing p ( H ) = F C ( Q , A ) . Therefore , Shen et al . ( 2023 ) employs the state - of - the - art LLMs to do end - to - end text generation of detection results . Some other studies ( Li et al . , 2023b ) found that adding chains of thought precede output may intervene in the final judgement , whereas retrieving the knowledge prop - erly resulted in gains . Furthering this concept , the hinted classifer and explainer ( Shen et al . , 2023 ) , used to generate intermediate process labels and high - quality natural language explanations , were demonstrated to enhance the final predicted class from a variety of perspectives . Subsequently , Tam et al . ( 2023 ) suggests adopting a different classi - fier model to the generated model , contributing to easier judgement of factual consistency . For radiol - ogy report generation , binary classifiers ( Mahmood et al . , 2023 ) can be leveraged to measure the relia - bility by combining image and text embedding . • Uncertainty Metric . It is important to examine the correlation between the hallucination metric and the quality of output from a variety of per - spectives . One intuitive approach is to employ the probabilistic output of the model itself , as ASTSN ( Varshney et al . , 2023 ) calculates the models’ un - certainty about the identified concepts by utilising the logit output values . Similarly , BARTSCORE ( Yuan et al . , 2021 ) employs a universal notion that models trained to convert generated text to reference output or source text will score higher when the generated text is superior . It is an unsu - pervised metric that supports the addition of ap - propriate prompts to improve the measure design , without human judgement to train . Furthermore , KoK ( Amayuelas et al . , 2023 ) based on the work of Pei and Jurgens ( 2021 ) evaluates answer un - certainty from three categories , i . e . , subjectivity , hedges and text uncertainty . However , SLAG ( Hase  . * V  Z L W K  / / 0 V  ' L D O R J  6 \ V W H P  6 X P P D U L ] D W L R Q  6 \ V W H P  4 X H V W L R Q  D Q G  $ Q V Z H U  0 D F K L Q H  7 U D Q V O D W L R Q  0 L Q G V H W  6 R F L H W \  $ V V H V V P H Q W  ) H H G E D F N  / H Y H U D J H  ( [ W H U Q D O  . Q R Z O H G J H  3 R V W  K R F  $ W W U L E X W L R Q  D Q G  ( G L W  3 D U D P H W H U  $ G D S W D W L R Q Figure 4 : Sankey diagram of hallucination correction methods with different mainstream NLP tasks . et al . , 2023 ) measures consistent factual beliefs in terms of paraphrase , logic , and entailment . In ad - dition to this , KLD ( Pezeshkpour , 2023 ) combines information theory - based metrics ( e . g . , entropy and KL - divergence ) to capture knowledge uncertainty . Beside expert - stipulated programmatic supervision , POLAR ( Zhao et al . , 2023b ) introduces Pareto opti - mal learning assessed risk score for estimating the confidence level of a response . • Self - Evaluation . To self - evaluate is challeng - ing since the model might be overconfident about its generated samples being correct . The moti - vating idea of SelfCheckGPT ( Manakul et al . , 2023 ) is to use the ability of the LLMs them - selves to sample multiple responses and iden - tify fictitious statements by measuring the con - sistency of information among responses . Ka - davath et al . ( 2022 ) further illustrates that both the increase in size and the demonstration of as - sessment can improve self - assessment . Beyond repetitive multiple direct queries , Agrawal et al . ( 2023 ) uses open - ended indirect queries and com - pares their answers to each other for an agreed - upon score outcome . Self - Contradictory ( Mündler et al . , 2023 ) imposes appropriate con - straints on the same LLM to generate pairs of sen - tences triggering self - contradictions , which prompt the detection . In contrast , Polling - based query - ing ( Li et al . , 2023i ) reduce the complexity of judgement by randomly sampling query objects . Besides , Self - Checker ( Li et al . , 2023d ) de - composes complex statements into multiple simple statements , fact - checking them one by one . How - ever , Cohen et al . ( 2023 ) introduces two LLMs interacting cross examination to drive the complex fact - checking reasoning process . • Evidence Retrieval . Evidence retrieval accom - plishes factual detection by retrieving supporting evidence related to hallucinations . To this end , Designing a claim - centric pipeline allows for a question - retrieve - summary chain to effectively col - lect original evidence ( Chen et al . , 2023b ; Huo et al . , 2023 ) . Consequently , FActScore ( Min et al . , 2023 ) calculates the percentage of atomic facts supported by the given knowledge source . To - wards adapting the tasks that users in interaction with generative models , FacTool ( Chern et al . , 2023 ) proposes to integrate a variety of tools into a task - agnostic and domain - agnostic detection frame - work , in order to assemble evidence about the au - thenticity of the generated content . 5 Hallucination Correction In this section , we delve into the methods to cor - rect hallucination in terms of different aspects , i . e . Parameter Adaptation , Post - hoc Attribution and Edit Technology , Leverage External Knowl - edge , Assessment Feedback , Mindset Society . As shown in Figure 4 , these hallucination correction paradigms have demonstrated strong dominance in many mainstream NLP tasks . Note that these methods are not entirely orthogonal but could com - plement each other as required by the tasks in prac - tical applications . In the following sections , we will introduce each methods as shown in Figure 5 . • Parameter Adaptation . Parameters in LLMs store biases learned in pre - training , are often un - aligned with user intent . A cutting - edge strategy is to guide effective knowledge through parame - ter conditioning , editing , and optimisation . For example , CLR ( Sun et al . , 2023 ) optimises to re - duce the generation probability of negative sam - ples at span level utilising contrastive learning parameters . While introducing contextual knowl - edge background that contradicts the model’s in - trinsic prior knowledge , TYE ( Shi et al . , 2023a ) effectively reduces the weight of prior knowledge through context - aware decoding method . Besides , PURR ( Chen et al . , 2023a ) corrupts noise into the text , fine - tune compact editors , and denoise by merging relevant evidence . To introduce additional cache component , HISTALIGN ( Wan et al . , 2023 ) discovers that its hidden state is not aligned with the current hidden state , and proposes sequence information contrastive learning to improve the reliability of memory parameters . Consequently , H a ll u c i n a ti on C o rr ec ti on ParameterAdaptation Factual - Nucleus ( Lee et al . , 2022 ) , CLR ( Sun et al . , 2023 ) , Edit - TA ( Ilharco et al . , 2023 ) , EWR ( Daheim et al . , 2023 ) , PURR ( Chen et al . , 2023a ) , mmT5 ( Pfeiffer et al . , 2023 ) , HISTALIGN ( Wan et al . , 2023 ) , TYE ( Shi et al . , 2023a ) , ALLM ( Luo et al . , 2023 ) , Inference - Time ( Li et al . , 2023c ) , TRAC ( Li et al . , 2023f ) , EasyEdit ( Wang et al . , 2023c ) Post - hoc Attribution and Edit Technology NP - Hunter ( Dziri et al . , 2021 ) , CoT ( Wei et al . , 2022b ) , ORCA ( Han and Tsvetkov , 2022 ) , RR ( He et al . , 2023 ) , TRAK ( Park et al . , 2023 ) , Data - Portraits ( Marone and Durme , 2023 ) , Self - Refine ( Madaan et al . , 2023 ) , Reflexion ( Shinn et al . , 2023 ) , QUIP ( Weller et al . , 2023 ) , Verify - and - Edit ( Zhao et al . , 2023a ) Leverage External Knowledge RETRO ( Borgeaud et al . , 2022 ) , IRCoT ( Trivedi et al . , 2023 ) , POPQA ( Mallen et al . , 2023 ) , LLM - AUGMENTER ( Peng et al . , 2023 ) , In - Context RALM ( Tam et al . , 2023 ) , GeneGPT ( Jin et al . , 2023 ) , cTBL ( Ding et al . , 2023 ) , CoK ( Li et al . , 2023h ) , FLARE ( Jiang et al . , 2023 ) , Gorilla ( Patil et al . , 2023 ) , RETA - LLM ( Liu et al . , 2023c ) , KnowledGPT ( Wang et al . , 2023e ) AssessmentFeedback LSHF ( Stiennon et al . , 2020 ) , TLM ( Menick et al . , 2022 ) , BRIO ( Liu et al . , 2022 ) , LM - know ( Kadavath et al . , 2022 ) , Chain - of - Hindsight ( Liu et al . , 2023a ) , ZEROFEC ( Huang et al . , 2023 ) , CRITIC ( Gou et al . , 2023 ) , VIVID ( Ning et al . , 2023 ) , LMH - Snowball ( Zhang et al . , 2023a ) , MixAlign ( Zhang et al . , 2023b ) , REFEED ( Yu et al . , 2023b ) , PaD ( Zhu et al . , 2023c ) , ALCE ( Gao et al . , 2023c ) , Do - LLM - Know ( Agrawal et al . , 2023 ) , CRL ( Dixit et al . , 2023 ) Mindset Society HLMTM ( Guerreiro et al . , 2023a ) , Multiagent - Debate ( Du et al . , 2023 ) , MAD ( Liang et al . , 2023 ) , FORD ( Xiong et al . , 2023 ) , LM - vs - LM ( Cohen et al . , 2023 ) , PRD ( Li et al . , 2023e ) , SPP ( Wang et al . , 2023g ) Figure 5 : Taxonomy of Hallucination Correction . Edit - TA ( Ilharco et al . , 2023 ) mitigates the bi - ases learnt in pre - training from a task algorithm perspective . An intuition behind it is that parame - ter variations learnt through negative example tasks could be perceived through weight variances . How - ever as this fails to take the importance of different negative examples into account , therefore EWR ( Da - heim et al . , 2023 ) proposes Fisher information ma - trices to measure the uncertainty of their estimation , which is applied for the dialogue systems to execute a parameter interpolation and remove hallucination . EasyEdit ( Wang et al . , 2023c ) summarises meth - ods for parameter editing , while minimising the influence to irrelevant parameter . An efficient alternative is to identify task - specific parameters and exploit them . For exam - ple , ALLM ( Luo et al . , 2023 ) aligns the param - eter module with task - specific knowledge , and then generates the relevant knowledge as addi - tional context in background augmented prompts . Similarly , mmT5 ( Pfeiffer et al . , 2023 ) utilises language - specific modules during pre - training to separate language - specific information from language - independent information , demonstrating that adding language - specific modules can allevi - ate the curse of multilinguality . Instead , TRAC ( Li et al . , 2023f ) combines conformal prediction and global testing to augment retrieval - based QA . The conservative strategy formulation ensures that a se - mantically equivalent answer to the truthful answer is included in the prediction set . Another parameter adaptation idea focuses on flexible sampling consistent with user requirements . For instance , Lee et al . ( 2022 ) observes that the randomness of sampling is more detrimental to fac - tuality when generating the latter part of a sentence . The factual - nucleus sampling algorithm is intro - duced to keep the faithfulness of the generation while ensuring the quality and diversity . Besides , Inference - Time ( Li et al . , 2023c ) firstly iden - tifies a set of attentional heads with high linear probing accuracy , and then shifts activation in the inference process along the direction associated with factual knowledge . • Post - hoc Attribution and Edit Technology . A source of hallucination is that LLMs may leverage the patterns observed in the pre - training data for inference in a novel form . Recently , ORCA ( Han and Tsvetkov , 2022 ) reveals problematic patterns in the behaviour of models by probing supporting data evidences from pre - training data . Similarly , TRAK ( Park et al . , 2023 ) and Data - Portraits ( Marone and Durme , 2023 ) analyse whether mod - els plagiarise or reference existing resources by means of data attribution . QUIP ( Weller et al . , 2023 ) further demonstrates that providing text that has been observed in the pre - training phase can improve the ability of LLMs to generate more fac - tual information . Furthermore , motivated by the gap between LLMs and human modes of think - ing , one intuition is to align the two modes of reasoning . Thus CoT ( Wei et al . , 2022b ) elicits faithful reasoning via a kind of Chain - of - Thought ( CoT ) ( Kojima et al . , 2022 ) prompts . Similarly , RR ( He et al . , 2023 ) retrieves relevant external knowledge based on decomposed reasoning steps obtained from a CoT prompt . Since LLMs not often produce the best output on the first attempt , Self - Refine ( Madaan et al . , 2023 ) implements self - refinement algorithms through iterative feed - back and improvement . Reflexion ( Shinn et al . , 2023 ) also employs verbal reinforcement to gener - ate reflective feedback by learning about prior fail - ings . Verify - and - Edit ( Zhao et al . , 2023a ) proposes a CoT - prompted verify - and - edit frame - work , which improves the fidelity of predictions by post - editing the inference chain based on ex - ternally retrieved knowledge . Another source of hallucinations is to describe factual content with incorrect retrievals . To recify this , NP - Hunter ( Dziri et al . , 2021 ) follows a generate - then - refine strategy whereby a generated response is amended using the KG so that the dialogue systemable to cor - rect potential hallucinations by querying the KG . • Leverage External Knowledge . As an at - tempt to extend the language model for halu - cination mitigation , a suggestion is to retrieve relevant documents from large textual databases . RETRO ( Borgeaud et al . , 2022 ) splits the input sequence into chunks and retrieves similar doc - uments , while In - Context RALM ( Tam et al . , 2023 ) places the selected document before the in - put text to improve the prediction . Furthermore , IRCoT ( Trivedi et al . , 2023 ) interweaves CoT gen - eration and document retrieval steps to guide LLMs . Since scaling mainly improves the memory for common knowledge but does not significantly im - prove the memory for factual knowledge in the long tail , POPQA ( Mallen et al . , 2023 ) retrieves only non - parametric memories when necessary to improve performance . LLM - AUGMENTER ( Peng et al . , 2023 ) also bases the responses of LLMs on integrated external knowledge and automated feedback to improve the truthfulness score of the answers . Another work , CoK ( Li et al . , 2023h ) iteratively analyses future content of upcoming sentences , and then applies them as a query to retrieve relevant documents for the purposes of re - generating sentences when they contain low con - fidence tokens . Similarly , RETA - LLM ( Liu et al . , 2023c ) creates a complete pipeline to assist users in building their own domain - based LLM retrieval systems . Note that in addition to document re - trieval , diverse external knowledge queries coule be assembled into retrieval - augmented LLM sys - tems . For example , FLARE ( Jiang et al . , 2023 ) leverages structured knowledge bases to support complex queries and provide more straightforward factual statements . Further , KnowledGPT ( Wang et al . , 2023e ) adopts program of thoughts ( PoT ) prompting , which generates codes to interact with knowledge bases . While cTBL ( Ding et al . , 2023 ) proposes to enhance LLMs with tabular data in con - versation settings . Besides , GeneGPT ( Jin et al . , 2023 ) demonstrates that expertise can be accessed more easily and accurately by detecting and exe - cuting API calls through contextual learning and augmented decoding algorithms . To support poten - tially millions of ever - changing APIs , Gorilla ( Patil et al . , 2023 ) explores self - instruct fine - tuning and retrieval for efficient API exploitation . • Assessment Feedback . As language models become more sophisticated , evaluation feedback can significantly improve the quality of generated text , as well as reduce the appearance of halluci - nations . To realise this concept , LSHF ( Stiennon et al . , 2020 ) predicts human - preferred summariza - tions through model and employs this as the reward function to fine - tune the summarization strategy using reinforcement learning . However , this ap - proach builds on models crafted by human anno - tators , which makes them inefficient in terms of data utilization . Therefore , TLM ( Menick et al . , 2022 ) proposes to improve the reliability of the system by selecting a few questions to refuse to answer , which significantly improves the reliabil - ity of the system , through reinforcement learning from human preferences . Whereas reinforcement learning often suffers from imperfect reward func - tions and relies on challenging optimisation . Thus , Chain - of - Hindsight ( Liu et al . , 2023a ) con - verts feedback preferences into sentences , which are then fed into models with fine - tuning for en - hanced language comprehension . In addition to enabling the model to learn di - rectly from the feedback of factual metrics in a sample - efficient manner ( Dixit et al . , 2023 ) , it is also important to build in a self - evaluation func - tion of the model to filter candidate generated texts . For example , BRIO ( Liu et al . , 2022 ) empow - ers summarization model assessment , estimating probability distributions of candidate outputs to rate the quality of candidate summaries . While LM - know ( Kadavath et al . , 2022 ) is devoted to in - vestigating whether LLMs can evaluate the validity of their own claims by detecting the probability that they know the answer to a question . Con - sequently , Do - LLM - Know ( Agrawal et al . , 2023 ) queries exclusively with black - box LLMs , and the results of queries repeatedly generated multiple times are compared with each other to pass con - sistency checks . Besides , the black - box LLM is augmented with a plug - and - play retrieval module ( Liu et al . , 2023a ; Huang et al . , 2023 ) to generate feedback could improve the model response . As missing citation quality evaluation affects the final performance , ALCE ( Gao et al . , 2023c ) employs a natural language reasoning model to measure citation quality and extends the integrated retrieval system . Similarly , CRITIC ( Gou et al . , 2023 ) proposes to interact with appropriate tools to assess certain aspects of the text , and then to modify the output based on the feedback obtained during the verification process . Note that automated error checking can also utilise LLMs to generate text that conforms to tool interfaces . PaD ( Zhu et al . , 2023c ) distills the LLMs with a synthetic inference procedure , and the synthesis program obtained can be automatically compiled and executed by an ex - plainer . Further , iterative refinement processes are validated to effectively identify appropriate details ( Ning et al . , 2023 ; Zhang et al . , 2023b ; Yu et al . , 2023b ) , and can stop early invalid reasoning chains , beneficially reducing the phenomenon of halluci - nation snowballing ( Zhang et al . , 2023a ) . • Mindset Society . Human intelligence thrives on the concept of cognitive synergy , where collabo - ration between different cognitive processes pro - duces better results than isolated individual cogni - tive processes . " Society of minds " ( Minsky , 1988 ) is believed have the potential to significantly im - prove the performance of LLMs and pave the way for consistency in language production and com - prehension . For the purpose of addressing halluci - nations in large - scale multilingual models across different translation scenarios , HLMTM ( Guerreiro et al . , 2023a ) proposes a hybrid setting in which other translation systems can be requested to act as a back - up system when the original system is hallu - cinating . Consequently , Multiagent - Debate ( Du et al . , 2023 ) employs multiple LLMs in sev - eral rounds to propose and debate their individ - ual responses and reasoning processes to reach a consensus final answer . As a result of this pro - cess , the models are encouraged to construct an - swers that are consistent with both internal crit - icisations and responses from other agents . Be - fore a final answer is presented , the resultant com - munity of models can hold and maintain multiple reasoning chains and possible answers simultane - ously . Based on this idea , MAD ( Liang et al . , 2023 ) adds a judge - managed debate process , demonstrat - ing that adaptive interruptions of debate and con - trolled " tit - for - tat " states help to complete factual debates . Furthermore , FORD ( Xiong et al . , 2023 ) proposes roundtable debates that include more than two LLMs and emphasises that competent judges are essential to dominate the debate . LM - vs - LM ( Cohen et al . , 2023 ) also proposes multi - round in - teractions between LM and another LM to check the factualness of original statements . Besides , PRD ( Li et al . , 2023e ) proposes a peer rank and discussionbased evaluation framework to arrive at a well - recognised assessment result that all peers are in agreement with . In an effort to maintain strong reasoning , SPP ( Wang et al . , 2023g ) utilises LLMs to assign several fine - grained roles , which effectively stimulates knowledge acquisition and reduces hallucinations . 6 Future Directions Though numerous technical solutions have been proposed in the survey for hallucinations in LLMs , there exist some potential directions : • Data Construction Management . As previously discussed , the style , and knowledge of LLMs is ba - sically learned during model pre - training . High quality data present promising opportunities for fa - cilitating the reduction of hallucinations in LLMs ( Kirstain et al . , 2022 ) . Inspired by the basic rule of machine learning models : " Garbage input , garbage output " , Zhou et al . ( 2023 ) proposes the superfi - cial alignment hypothesis , which views alignment as learning to interact with the user . The results of simple fine - tuning on a few high - quality sam - ples demonstrate that data quality and diversity outweigh the importance of fine - tuning large - scale instructions ( Mishra et al . , 2021 ; Wei et al . , 2022a ; Sanh et al . , 2022 ) and RLHF ( Bai et al . , 2022 ; Ouyang et al . , 2022 ) . To perform efficiently in knowledge - intensive verticals , we argue that con - struction of entity - centred fine - tuned instructions ( Bao et al . , 2023 ; Gui et al . , 2023 ; Wei Zhu and Wang , 2023 ) is a promising direction that it can combine the structured knowledge and semantic rel - evance of knowledge graphs to enhance the factual - ity of generated entity information . Another feasi - ble proposal is to incorporate a self - curation phase ( Li et al . , 2023g ) in the instruction construction process to rate the quality of candidate pairs . Dur - ing the iteration process , quality evaluation ( Chen et al . , 2023c ) based on manual or automated rule constraints could provide self - correction capacity . • Downstream Task Alignment . Generic LLMs have a certain degree of natural language prob - lem comprehension in a variety of open environ - ments . However , the main problem still remains in the deviation from the application requirements , which leads to emergence of diverse hallucina - tions . Thus , downstream task alignment especially built on vertical domain cognition necessitates ex - panded symbolic reasoning , decomposition and planning of complex tasks , and faithful external knowledge injection . Specifically , while expert in language processing , LLMs struggle to make break - throughs in mathematical abilities , a deficiency at - tributable to the textual training objective . Though some researches for symbolic math word problems ( Gaur and Saunshi , 2023 ; Zhu et al . , 2023b ) have been proposed , enhancing symbolic reasoning and answering numerical questions remains to be ex - plored extensively . Additionally , for story genera - tion tasks that demand diverse outputs ( Yang et al . , 2022 , 2023 ) , fascinating storylines are required in addition to avoiding factual contradictions . There - fore , achieving a balance between faithfulness and creativity in the model inference process remains a crucial challenge . Moreover , integrating new knowledge to deal with knowledge - intensive tasks involves handling joint reasoning between the im - plicit knowledge of LLMs and the explicit knowl - edge of external knowledge graphs . There arises a challenge to design knowledge - aware methods to incorporate structured information from the knowl - edge graph into the pre - training process of LLMs . Alternatively , the reasoning process is expected to be dynamically injected with knowledge graph information ( Wen et al . , 2023 ) . The utilization of LLMs as an evaluation tool is a burgeoning application , but limited by the size of models , the effect of instruction adjustments , and the different forms of inputs ( Agrawal et al . , 2023 ) . Note attempts of LLMs to act as judges for scoring have to overcome all kinds of biases induced by position , verbosity , self - enhancement ( Zheng et al . , 2023a ; Berglund et al . , 2023 ; Wang et al . , 2023b ) . Therefore , we forecast that future research about designing task - specific mechanisms for analysing and correcting processes of emerging downstream tasks is an area deserving long - term attention . • Reasoning Mechanism Exploitation . The emerging CoT technique ( Wei et al . , 2022b ) stim - ulates the emergent reasoning ability of LLMs by imitating intrinsic stream of thought . Construct - ing a logically intermediate reasoning step has been proved to significantly improve the problem - solving ability . Recently , A primary improvement is Self - consistency with CoT ( CoT - SC ) ( Yao et al . , 2023 ) , which is a method for generating multiple CoT options and then selecting the optimal result as feedback . Further , Tree of Thoughts ( ToT ) ( Yao et al . , 2023 ) introduces a strict tree architecture into the thought process , which facilitates the develop - ment with different paths of thought and provides a novel roll - back function . Since previous methods have no storages for intermediate results , Cumu - lative Reasoning ( CR ) ( Zhang et al . , 2023c ) uses LLMs in a cumulative and iterative manner to sim - ulate human thought processes , and decompose the task into smaller components . However , the ac - tual thinking process creates a complex network of ideas , as an example , people could explore a particular chain of reasoning , backtrack or start a new chain of reasoning . In particular when aware that an idea from a previous chain of reasoning can be combined with the currently explored idea , they could be merged into a new solution . More excitingly , Graph of Thoughts ( GoT ) ( Zhang et al . , 2023c ) extends the dependencies between thoughts by constructing vertices with multiple incoming edges to aggregate arbitrary thoughts . In addition , Program - aided language models ( PAL ) ( Gao et al . , 2023b ) and Program of Thoughts prompting ( PoT ) ( Chen et al . , 2022b ) introduce programming logic into the language space ( Bi et al . , 2023 ) , expand - ing the ability to invoke external explainers . As a summary , we believe that research based on human cognition helps to provide brilliant and insightful insights into the analysis of hallucinations , such as Dual Process Theory ( Frankish , 2010 ) , Three layer mental model ( Stanovich , 2011 ) , Computational Theory of Mind ( Piccinini , 2004 ) , and Connection - ism ( Thorndike , 1898 ) . • Multi - modal Hallucination Survey . It has be - come a community consensus to establish powerful Multimodal Large Language Models ( MLLMs ) ( Li et al . , 2023a ; Dai et al . , 2023 ; Ye et al . , 2023 ) by taking advantage of excellent comprehension and reasoning capabilities of LLMs . Li et al . ( 2023i ) confirms the severity of hallucinations in MLLM by object detecting and polling - based querying . The results indicate that the models are highly suscep - tible to object hallucination , and the generated de - scription does not match the target image . Besides , Shao et al . ( 2023 ) that MLLMs have limited multi - modal reasoning ability as well as dependence on spurious cues . Though current study ( Yin et al . , 2023b ) provides a broad overview of MLLMs , the causation of hallucinations has not been compre - hensively investigated . The hallucinations in LLMs come mainly from misknowledge in the training data , whereas the challenge of MLLMs lies in accu - rately relaying the abstract visual encoding into the semantic space . Existing MLLMs are fine - tuned with instructions to make their target outputs follow human intentions . However , misalignment between visual and textual modes may lead to biased dis - tribution . Further , the lack of visual constraints results in a serious problem of hallucination in MLLMs . Thus a potential improvement is to pe - nalise deviating attention to images ( Wang et al . , 2023a ) or to enhance understanding of visual com - mon sense . In terms of fine - grained visual and tex - tual modal alignment , focusing on local features of images and corresponding textual descriptions can provide faithful modal interactions . In addition , the performance of some MLLMs such as MiniGPT - 4 ( Zhu et al . , 2023a ) is highly dependent on the choice of prompts and requires careful selection . Note that a controlled trade - off between diversity and hallucinations is needed for user convenience . In the future , as more sophisticated multi - model applications emerge , improving MLLMs reasoning paths is also a promising research direction . 7 Conclusion and Vision In this paper , we provide an overview of halluci - nations in LLMs with new taxonomy , theoretical insight , detection methods , correction methods and several future research directions . Note that it is crucial to ensure we can continuously utilize LLMs in a responsible and beneficial manner , so we ex - plore the causation of hallucinations and taxonomy in task axes to analyse potential directions for im - provement . In the future , we envision a more potent synergy between LLMs and external knowledge bases , resulting in a credible interactive system with the dual - wheel drive . We hope that sophisti - cated and efficient detection methods are proposed to contribute further to improving the performance of LLMs . Furthermore , we hope that community maintains a proactive attitude towards mitigating the effects of hallucinations . With creative correc - tive methods proposed for various aspects , LLMs will provide human with reliable and secure infor - mation in broad application scenarios . References Vaibhav Adlakha , Parishad BehnamGhader , Xing Han Lu , Nicholas Meade , and Siva Reddy . 2023 . Eval - uating correctness and faithfulness of instruction - following models for question answering . CoRR , abs / 2307 . 16877 . Ayush Agrawal , Lester Mackey , and Adam Tauman Kalai . 2023 . Do language models know when they’re hallucinating references ? CoRR , abs / 2305 . 18248 . Alfonso Amayuelas , Liangming Pan , Wenhu Chen , and William Yang Wang . 2023 . Knowledge of knowl - edge : Exploring known - unknowns uncertainty with large language models . CoRR , abs / 2305 . 13712 . Yuntao Bai , Andy Jones , Kamal Ndousse , Amanda Askell , Anna Chen , Nova DasSarma , Dawn Drain , Stanislav Fort , Deep Ganguli , Tom Henighan , Nicholas Joseph , Saurav Kadavath , Jackson Kernion , Tom Conerly , Sheer El Showk , Nelson Elhage , Zac Hatfield - Dodds , Danny Hernandez , Tristan Hume , Scott Johnston , Shauna Kravec , Liane Lovitt , Neel Nanda , Catherine Olsson , Dario Amodei , Tom B . Brown , Jack Clark , Sam McCandlish , Chris Olah , Benjamin Mann , and Jared Kaplan . 2022 . Train - ing a helpful and harmless assistant with rein - forcement learning from human feedback . CoRR , abs / 2204 . 05862 . Zhijie Bao , Wei Chen , Shengze Xiao , Kuang Ren , Jiaao Wu , Cheng Zhong , Jiajie Peng , Xuanjing Huang , and Zhongyu Wei . 2023 . Disc - medllm : Bridging gen - eral large language models and real - world medical consultation . Rachel Bawden and François Yvon . 2023 . Investigat - ing the translation performance of a large multilin - gual language model : the case of BLOOM . CoRR , abs / 2303 . 01911 . Lukas Berglund , Asa Cooper Stickland , Mikita Balesni , Max Kaufmann , Meg Tong , Tomasz Korbak , Daniel Kokotajlo , and Owain Evans . 2023 . Taken out of context : On measuring situational awareness in llms . Zhen Bi , Ningyu Zhang , Yinuo Jiang , Shumin Deng , Guozhou Zheng , and Huajun Chen . 2023 . When do program - of - thoughts work for reasoning ? Ali Furkan Biten , Lluís Gómez , and Dimosthenis Karatzas . 2022 . Let there be a clock on the beach : Reducing object hallucination in image captioning . In IEEE / CVF Winter Conference on Applications of Computer Vision , WACV 2022 , Waikoloa , HI , USA , January 3 - 8 , 2022 , pages 2473 – 2482 . IEEE . Sebastian Borgeaud , Arthur Mensch , Jordan Hoffmann , Trevor Cai , Eliza Rutherford , Katie Millican , George van den Driessche , Jean - Baptiste Lespiau , Bogdan Damoc , Aidan Clark , Diego de Las Casas , Aurelia Guy , Jacob Menick , Roman Ring , Tom Hennigan , Saffron Huang , Loren Maggiore , Chris Jones , Albin Cassirer , Andy Brock , Michela Paganini , Geoffrey Irving , Oriol Vinyals , Simon Osindero , Karen Si - monyan , Jack W . Rae , Erich Elsen , and Laurent Sifre . 2022 . Improving language models by retrieving from trillions of tokens . In International Conference on Machine Learning , ICML 2022 , 17 - 23 July 2022 , Bal - timore , Maryland , USA , volume 162 of Proceedings of Machine Learning Research , pages 2206 – 2240 . PMLR . Tom B . Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M . Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learners . In Ad - vances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Process - ing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual . Meng Cao , Yue Dong , and Jackie Chi Kit Cheung . 2022 . Hallucinated but factual ! inspecting the factuality of hallucinations in abstractive summarization . In Proceedings of the 60th Annual Meeting of the As - sociation for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2022 , Dublin , Ireland , May 22 - 27 , 2022 , pages 3340 – 3354 . Association for Computa - tional Linguistics . Stephanie Chan , Adam Santoro , Andrew K . Lampinen , Jane Wang , Aaditya Singh , Pierre H . Richemond , James L . McClelland , and Felix Hill . 2022 . Data distributional properties drive emergent in - context learning in transformers . In NeurIPS . Anthony Chen , Panupong Pasupat , Sameer Singh , Hon - grae Lee , and Kelvin Guu . 2023a . PURR : efficiently editing language model hallucinations by denoising language model corruptions . CoRR , abs / 2305 . 14908 . Jifan Chen , Grace Kim , Aniruddh Sriram , Greg Dur - rett , and Eunsol Choi . 2023b . Complex claim veri - fication with evidence retrieved in the wild . CoRR , abs / 2305 . 11859 . Lichang Chen , Shiyang Li , Jun Yan , Hai Wang , Kalpa Gunaratna , Vikas Yadav , Zheng Tang , Vijay Srini - vasan , Tianyi Zhou , Heng Huang , and Hongxia Jin . 2023c . Alpagasus : Training A better alpaca with fewer data . CoRR , abs / 2307 . 08701 . Mingda Chen , Jingfei Du , Ramakanth Pasunuru , Todor Mihaylov , Srini Iyer , Veselin Stoyanov , and Zornitsa Kozareva . 2022a . Improving in - context few - shot learning via self - supervised training . In Proceed - ings of the 2022 Conference of the North American Chapter of the Association for Computational Lin - guistics : Human Language Technologies , NAACL 2022 , Seattle , WA , United States , July 10 - 15 , 2022 , pages 3558 – 3573 . Association for Computational Linguistics . Wenhu Chen , Xueguang Ma , Xinyi Wang , and William W . Cohen . 2022b . Program of thoughts prompting : Disentangling computation from rea - soning for numerical reasoning tasks . CoRR , abs / 2211 . 12588 . I - Chun Chern , Steffi Chern , Shiqi Chen , Weizhe Yuan , Kehua Feng , Chunting Zhou , Junxian He , Graham Neubig , and Pengfei Liu . 2023 . Factool : Factual - ity detection in generative AI - A tool augmented framework for multi - task and multi - domain scenar - ios . CoRR , abs / 2307 . 13528 . Sabrina Chiesurin , Dimitris Dimakopoulos , Marco An - tonio Sobrevilla Cabezudo , Arash Eshghi , Ioannis Papaioannou , Verena Rieser , and Ioannis Konstas . 2023 . The dangers of trusting stochastic parrots : Faithfulness and trust in open - domain conversational question answering . In Findings of the Association for Computational Linguistics : ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 947 – 959 . Associa - tion for Computational Linguistics . Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Sebastian Gehrmann , Parker Schuh , Kensen Shi , Sasha Tsvyashchenko , Joshua Maynez , Abhishek Rao , Parker Barnes , Yi Tay , Noam Shazeer , Vin - odkumar Prabhakaran , Emily Reif , Nan Du , Ben Hutchinson , Reiner Pope , James Bradbury , Jacob Austin , Michael Isard , Guy Gur - Ari , Pengcheng Yin , Toju Duke , Anselm Levskaya , Sanjay Ghemawat , Sunipa Dev , Henryk Michalewski , Xavier Garcia , Vedant Misra , Kevin Robinson , Liam Fedus , Denny Zhou , Daphne Ippolito , David Luan , Hyeontaek Lim , Barret Zoph , Alexander Spiridonov , Ryan Sepassi , David Dohan , Shivani Agrawal , Mark Omernick , An - drew M . Dai , Thanumalayan Sankaranarayana Pil - lai , Marie Pellat , Aitor Lewkowycz , Erica Moreira , Rewon Child , Oleksandr Polozov , Katherine Lee , Zongwei Zhou , Xuezhi Wang , Brennan Saeta , Mark Diaz , Orhan Firat , Michele Catasta , Jason Wei , Kathy Meier - Hellstern , Douglas Eck , Jeff Dean , Slav Petrov , and Noah Fiedel . 2022 . Palm : Scaling language mod - eling with pathways . CoRR , abs / 2204 . 02311 . Roi Cohen , May Hamri , Mor Geva , and Amir Glober - son . 2023 . LM vs LM : detecting factual errors via cross examination . CoRR , abs / 2305 . 13281 . Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzmán , Edouard Grave , Myle Ott , Luke Zettle - moyer , and Veselin Stoyanov . 2020 . Unsupervised cross - lingual representation learning at scale . In Pro - ceedings of the 58th Annual Meeting of the Associa - tion for Computational Linguistics , ACL 2020 , On - line , July 5 - 10 , 2020 , pages 8440 – 8451 . Association for Computational Linguistics . Shawn Curran , Sam Lansley , and Oliver Bethell . 2023 . Hallucination is the last thing you need . CoRR , abs / 2306 . 11520 . Nico Daheim , Nouha Dziri , Mrinmaya Sachan , Iryna Gurevych , and Edoardo M . Ponti . 2023 . Elastic weight removal for faithful and abstractive dialogue generation . CoRR , abs / 2303 . 17574 . Wenliang Dai , Junnan Li , Dongxu Li , Anthony Meng Huat Tiong , Junqi Zhao , Weisheng Wang , Boyang Li , Pascale Fung , and Steven C . H . Hoi . 2023 . Instructblip : Towards general - purpose vision - language models with instruction tuning . CoRR , abs / 2305 . 06500 . David Dale , Elena Voita , Janice Lam , Prangthip Hansanti , Christophe Ropers , Elahe Kalbassi , Cyn - thia Gao , Loïc Barrault , and Marta R . Costa - jussà . 2023 . Halomi : A manually annotated benchmark for multilingual hallucination and omission detection in machine translation . CoRR , abs / 2305 . 11746 . Souvik Das , Sougata Saha , and Rohini K . Srihari . 2022 . Diving deep into modes of fact hallucinations in dialogue systems . In Findings of the Association for Computational Linguistics : EMNLP 2022 , Abu Dhabi , United Arab Emirates , December 7 - 11 , 2022 , pages 684 – 699 . Association for Computational Lin - guistics . Debadutta Dash , Rahul Thapa , Juan M . Banda , Akshay Swaminathan , Morgan Cheatham , Mehr Kashyap , Nikesh Kotecha , Jonathan H . Chen , Saurabh Gom - bar , Lance Downing , Rachel Pedreira , Ethan Goh , Angel Arnaout , Garret Kenn Morris , Honor Magon , Matthew P . Lungren , Eric Horvitz , and Nigam H . Shah . 2023 . Evaluation of GPT - 3 . 5 and GPT - 4 for supporting real - world information needs in health - care delivery . CoRR , abs / 2304 . 13714 . Zijian Ding , Arvind Srinivasan , Stephen MacNeil , and Joel Chan . 2023 . Fluid transformers and creative analogies : Exploring large language models’ capac - ity for augmenting cross - domain analogical creativ - ity . In Creativity and Cognition , C & C 2023 , Virtual Event , USA , June 19 - 21 , 2023 , pages 489 – 505 . ACM . Tanay Dixit , Fei Wang , and Muhao Chen . 2023 . Improv - ing factuality of abstractive summarization without sacrificing summary quality . In Proceedings of the 61st Annual Meeting of the Association for Compu - tational Linguistics ( Volume 2 : Short Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 902 – 913 . Association for Computational Linguistics . Yue Dong , John Wieting , and Pat Verga . 2022 . Faithful to the document or to the world ? mitigating hal - lucinations via entity - linked knowledge in abstrac - tive summarization . In Findings of the Association for Computational Linguistics : EMNLP 2022 , Abu Dhabi , United Arab Emirates , December 7 - 11 , 2022 , pages 1067 – 1082 . Association for Computational Linguistics . Yilun Du , Shuang Li , Antonio Torralba , Joshua B . Tenenbaum , and Igor Mordatch . 2023 . Improving factuality and reasoning in language models through multiagent debate . CoRR , abs / 2305 . 14325 . Esin Durmus , He He , and Mona T . Diab . 2020 . FEQA : A question answering evaluation framework for faith - fulness assessment in abstractive summarization . In Proceedings of the 58th Annual Meeting of the As - sociation for Computational Linguistics , ACL 2020 , Online , July 5 - 10 , 2020 , pages 5055 – 5070 . Associa - tion for Computational Linguistics . Nouha Dziri , Ehsan Kamalloo , Sivan Milton , Osmar R . Zaïane , Mo Yu , Edoardo Maria Ponti , and Siva Reddy . 2022a . Faithdial : A faithful benchmark for information - seeking dialogue . Trans . Assoc . Comput . Linguistics , 10 : 1473 – 1490 . Nouha Dziri , Andrea Madotto , Osmar Zaïane , and Avishek Joey Bose . 2021 . Neural path hunter : Re - ducing hallucination in dialogue systems via path grounding . In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process - ing , EMNLP 2021 , Virtual Event / Punta Cana , Do - minican Republic , 7 - 11 November , 2021 , pages 2197 – 2214 . Association for Computational Linguistics . Nouha Dziri , Sivan Milton , Mo Yu , Osmar R . Zaïane , and Siva Reddy . 2022b . On the origin of hallucina - tions in conversational models : Is it the datasets or the models ? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , NAACL 2022 , Seattle , WA , United States , July 10 - 15 , 2022 , pages 5271 – 5285 . Association for Computational Linguistics . Nouha Dziri , Hannah Rashkin , Tal Linzen , and David Reitter . 2022c . Evaluating attribution in dialogue sys - tems : The BEGIN benchmark . Trans . Assoc . Com - put . Linguistics , 10 : 1066 – 1083 . Keith Frankish . 2010 . Dual - process and dual - system theories of reasoning . Philosophy Compass , 5 ( 10 ) : 914 – 926 . Luyu Gao , Zhuyun Dai , Panupong Pasupat , Anthony Chen , Arun Tejasvi Chaganty , Yicheng Fan , Vin - cent Y . Zhao , Ni Lao , Hongrae Lee , Da - Cheng Juan , and Kelvin Guu . 2023a . RARR : researching and revising what language models say , using language models . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Vol - ume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 16477 – 16508 . Association for Computational Linguistics . Luyu Gao , Aman Madaan , Shuyan Zhou , Uri Alon , Pengfei Liu , Yiming Yang , Jamie Callan , and Gra - ham Neubig . 2023b . PAL : program - aided language models . In International Conference on Machine Learning , ICML 2023 , 23 - 29 July 2023 , Honolulu , Hawaii , USA , volume 202 of Proceedings of Machine Learning Research , pages 10764 – 10799 . PMLR . Tianyu Gao , Howard Yen , Jiatong Yu , and Danqi Chen . 2023c . Enabling large language models to generate text with citations . CoRR , abs / 2305 . 14627 . Vedant Gaur and Nikunj Saunshi . 2023 . Reasoning in large language models through symbolic math word problems . In Findings of the Association for Com - putational Linguistics : ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 5889 – 5903 . Association for Computational Linguistics . Sukhpal Singh Gill , Minxian Xu , Panos Patros , Huam - ing Wu , Rupinder Kaur , Kamalpreet Kaur , Stephanie Fuller , Manmeet Singh , Priyansh Arora , Ajith Ku - mar Parlikad , Vlado Stankovski , Ajith Abraham , Soumya K . Ghosh , Hanan Lutfiyya , Salil S . Kanhere , Rami Bahsoon , Omer F . Rana , Schahram Dustdar , Rizos Sakellariou , Steve Uhlig , and Rajkumar Buyya . 2023 . Transformative effects of chatgpt on mod - ern education : Emerging era of AI chatbots . CoRR , abs / 2306 . 03823 . Zhibin Gou , Zhihong Shao , Yeyun Gong , Yelong Shen , Yujiu Yang , Nan Duan , and Weizhu Chen . 2023 . CRITIC : large language models can self - correct with tool - interactive critiquing . CoRR , abs / 2305 . 11738 . Nuno Miguel Guerreiro , Duarte M . Alves , Jonas Waldendorf , Barry Haddow , Alexandra Birch , Pierre Colombo , and André F . T . Martins . 2023a . Hallucina - tions in large multilingual translation models . CoRR , abs / 2303 . 16104 . Nuno Miguel Guerreiro , Elena Voita , and André F . T . Martins . 2023b . Looking for a needle in a haystack : A comprehensive study of hallucinations in neural machine translation . In Proceedings of the 17th Conference of the European Chapter of the Asso - ciation for Computational Linguistics , EACL 2023 , Dubrovnik , Croatia , May 2 - 6 , 2023 , pages 1059 – 1075 . Association for Computational Linguistics . Honghao Gui , Jintian Zhang , Hongbin Ye , and Ningyu Zhang . 2023 . Instructie : A chinese instruction - based information extraction dataset . CoRR , abs / 2305 . 11527 . Danny Halawi , Jean - Stanislas Denain , and Jacob Stein - hardt . 2023 . Overthinking the truth : Understanding how language models process false demonstrations . CoRR , abs / 2307 . 09476 . Xiaochuang Han and Yulia Tsvetkov . 2022 . ORCA : interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data . CoRR , abs / 2205 . 12600 . Xu Han , Zhengyan Zhang , Ning Ding , Yuxian Gu , Xiao Liu , Yuqi Huo , Jiezhong Qiu , Yuan Yao , Ao Zhang , Liang Zhang , Wentao Han , Minlie Huang , Qin Jin , Yanyan Lan , Yang Liu , Zhiyuan Liu , Zhiwu Lu , Xipeng Qiu , Ruihua Song , Jie Tang , Ji - Rong Wen , Jinhui Yuan , Wayne Xin Zhao , and Jun Zhu . 2021 . Pre - trained models : Past , present and future . AI Open , 2 : 225 – 250 . Peter Hase , Mona T . Diab , Asli Celikyilmaz , Xian Li , Zornitsa Kozareva , Veselin Stoyanov , Mohit Bansal , and Srinivasan Iyer . 2023 . Methods for measuring , updating , and visualizing factual beliefs in language models . In Proceedings of the 17th Conference of the European Chapter of the Association for Compu - tational Linguistics , EACL 2023 , Dubrovnik , Croatia , May 2 - 6 , 2023 , pages 2706 – 2723 . Association for Computational Linguistics . Hangfeng He , Hongming Zhang , and Dan Roth . 2023 . Rethinking with retrieval : Faithful large language model inference . CoRR , abs / 2301 . 00303 . Amr Hendy , Mohamed Abdelrehim , Amr Sharaf , Vikas Raunak , Mohamed Gabr , Hitokazu Matsushita , Young Jin Kim , Mohamed Afify , and Hany Has - san Awadalla . 2023 . How good are GPT models at machine translation ? A comprehensive evaluation . CoRR , abs / 2302 . 09210 . Jie Huang and Kevin Chen - Chuan Chang . 2023 . To - wards reasoning in large language models : A survey . In Findings of the Association for Computational Linguistics : ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 1049 – 1065 . Association for Computa - tional Linguistics . Kung - Hsiang Huang , Hou Pong Chan , and Heng Ji . 2023 . Zero - shot faithful factual error correction . In Proceedings of the 61st Annual Meeting of the As - sociation for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 5660 – 5676 . Association for Computa - tional Linguistics . Siqing Huo , Negar Arabzadeh , and Charles L . A . Clarke . 2023 . Retrieving supporting evidence for llms gener - ated answers . CoRR , abs / 2306 . 13781 . Gabriel Ilharco , Marco Túlio Ribeiro , Mitchell Worts - man , Ludwig Schmidt , Hannaneh Hajishirzi , and Ali Farhadi . 2023 . Editing models with task arithmetic . In The Eleventh International Conference on Learn - ing Representations , ICLR 2023 , Kigali , Rwanda , May 1 - 5 , 2023 . OpenReview . net . Ziwei Ji , Nayeon Lee , Rita Frieske , Tiezheng Yu , Dan Su , Yan Xu , Etsuko Ishii , Yejin Bang , Andrea Madotto , and Pascale Fung . 2023 . Survey of halluci - nation in natural language generation . ACM Comput . Surv . , 55 ( 12 ) : 248 : 1 – 248 : 38 . Zhengbao Jiang , Frank F . Xu , Luyu Gao , Zhiqing Sun , Qian Liu , Jane Dwivedi - Yu , Yiming Yang , Jamie Callan , and Graham Neubig . 2023 . Active retrieval augmented generation . CoRR , abs / 2305 . 06983 . Qiao Jin , Yifan Yang , Qingyu Chen , and Zhiyong Lu . 2023 . Genegpt : Augmenting large language models with domain tools for improved access to biomedical information . CoRR , abs / 2304 . 09667 . Saurav Kadavath , Tom Conerly , Amanda Askell , Tom Henighan , Dawn Drain , Ethan Perez , Nicholas Schiefer , Zac Hatfield - Dodds , Nova DasSarma , Eli Tran - Johnson , Scott Johnston , Sheer El Showk , Andy Jones , Nelson Elhage , Tristan Hume , Anna Chen , Yuntao Bai , Sam Bowman , Stanislav Fort , Deep Ganguli , Danny Hernandez , Josh Jacobson , Jack - son Kernion , Shauna Kravec , Liane Lovitt , Ka - mal Ndousse , Catherine Olsson , Sam Ringer , Dario Amodei , Tom Brown , Jack Clark , Nicholas Joseph , Ben Mann , Sam McCandlish , Chris Olah , and Jared Kaplan . 2022 . Language models ( mostly ) know what they know . CoRR , abs / 2207 . 05221 . Nikhil Kandpal , Haikang Deng , Adam Roberts , Eric Wallace , and Colin Raffel . 2023 . Large language models struggle to learn long - tail knowledge . In In - ternational Conference on Machine Learning , ICML 2023 , 23 - 29 July 2023 , Honolulu , Hawaii , USA , vol - ume 202 of Proceedings of Machine Learning Re - search , pages 15696 – 15707 . PMLR . Daniel Kang and Tatsunori Hashimoto . 2020 . Improved natural language generation via loss truncation . In Proceedings of the 58th Annual Meeting of the As - sociation for Computational Linguistics , ACL 2020 , Online , July 5 - 10 , 2020 , pages 718 – 731 . Association for Computational Linguistics . Yuval Kirstain , Patrick S . H . Lewis , Sebastian Riedel , and Omer Levy . 2022 . A few more examples may be worth billions of parameters . In Findings of the Association for Computational Linguistics : EMNLP 2022 , Abu Dhabi , United Arab Emirates , December 7 - 11 , 2022 , pages 1017 – 1029 . Association for Com - putational Linguistics . Takeshi Kojima , Shixiang Shane Gu , Machel Reid , Yu - taka Matsuo , and Yusuke Iwasawa . 2022 . Large lan - guage models are zero - shot reasoners . In NeurIPS . Angeliki Lazaridou , Elena Gribovskaya , Wojciech Stokowiec , and Nikolai Grigorev . 2022 . Internet - augmented language models through few - shot prompting for open - domain question answering . CoRR , abs / 2203 . 05115 . Nayeon Lee , Wei Ping , Peng Xu , Mostofa Patwary , Pas - cale Fung , Mohammad Shoeybi , and Bryan Catan - zaro . 2022 . Factuality enhanced language models for open - ended text generation . In NeurIPS . Junnan Li , Dongxu Li , Silvio Savarese , and Steven C . H . Hoi . 2023a . BLIP - 2 : bootstrapping language - image pre - training with frozen image encoders and large language models . In International Conference on Machine Learning , ICML 2023 , 23 - 29 July 2023 , Honolulu , Hawaii , USA , volume 202 of Proceedings of Machine Learning Research , pages 19730 – 19742 . PMLR . Junyi Li , Xiaoxue Cheng , Wayne Xin Zhao , Jian - Yun Nie , and Ji - Rong Wen . 2023b . Halueval : A large - scale hallucination evaluation benchmark for large language models . CoRR , abs / 2305 . 11747 . Kenneth Li , Oam Patel , Fernanda B . Viégas , Hanspeter Pfister , and Martin Wattenberg . 2023c . Inference - time intervention : Eliciting truthful answers from a language model . CoRR , abs / 2306 . 03341 . Miaoran Li , Baolin Peng , and Zhu Zhang . 2023d . Self - checker : Plug - and - play modules for fact - checking with large language models . CoRR , abs / 2305 . 14623 . Ruosen Li , Teerth Patel , and Xinya Du . 2023e . PRD : peer rank and discussion improve large language model based evaluations . CoRR , abs / 2307 . 02762 . Shuo Li , Sangdon Park , Insup Lee , and Osbert Bas - tani . 2023f . TRAC : trustworthy retrieval augmented chatbot . CoRR , abs / 2307 . 04642 . Xian Li , Ping Yu , Chunting Zhou , Timo Schick , Luke Zettlemoyer , Omer Levy , Jason Weston , and Mike Lewis . 2023g . Self - alignment with instruction back - translation . CoRR , abs / 2308 . 06259 . Xingxuan Li , Ruochen Zhao , Yew Ken Chia , Bosheng Ding , Lidong Bing , Shafiq R . Joty , and Soujanya Poria . 2023h . Chain of knowledge : A framework for grounding large language models with structured knowledge bases . CoRR , abs / 2305 . 13269 . Yifan Li , Yifan Du , Kun Zhou , Jinpeng Wang , Wayne Xin Zhao , and Ji - Rong Wen . 2023i . Eval - uating object hallucination in large vision - language models . CoRR , abs / 2305 . 10355 . Yu Li , Baolin Peng , Yelong Shen , Yi Mao , Lars Li - den , Zhou Yu , and Jianfeng Gao . 2022 . Knowledge - grounded dialogue generation with a unified knowl - edge representation . In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL 2022 , Seattle , WA , United States , July 10 - 15 , 2022 , pages 206 – 218 . As - sociation for Computational Linguistics . Tian Liang , Zhiwei He , Wenxiang Jiao , Xing Wang , Yan Wang , Rui Wang , Yujiu Yang , Zhaopeng Tu , and Shuming Shi . 2023 . Encouraging divergent thinking in large language models through multi - agent debate . CoRR , abs / 2305 . 19118 . Stephanie Lin , Jacob Hilton , and Owain Evans . 2022 . Truthfulqa : Measuring how models mimic human falsehoods . In Proceedings of the 60th Annual Meet - ing of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2022 , Dublin , Ireland , May 22 - 27 , 2022 , pages 3214 – 3252 . Association for Computational Linguistics . Hao Liu , Carmelo Sferrazza , and Pieter Abbeel . 2023a . Chain of hindsight aligns language models with feed - back . CoRR , abs / 2302 . 02676 . Haotian Liu , Chunyuan Li , Qingyang Wu , and Yong Jae Lee . 2023b . Visual instruction tuning . CoRR , abs / 2304 . 08485 . Jiongnan Liu , Jiajie Jin , Zihan Wang , Jiehan Cheng , Zhicheng Dou , and Ji - Rong Wen . 2023c . RETA - LLM : A retrieval - augmented large language model toolkit . CoRR , abs / 2306 . 05212 . Yixin Liu , Pengfei Liu , Dragomir R . Radev , and Graham Neubig . 2022 . BRIO : bringing order to abstractive summarization . In Proceedings of the 60th Annual Meeting of the Association for Computational Lin - guistics ( Volume 1 : Long Papers ) , ACL 2022 , Dublin , Ireland , May 22 - 27 , 2022 , pages 2890 – 2903 . Associ - ation for Computational Linguistics . Yao Lu , Max Bartolo , Alastair Moore , Sebastian Riedel , and Pontus Stenetorp . 2022 . Fantastically ordered prompts and where to find them : Overcoming few - shot prompt order sensitivity . In Proceedings of the 60th Annual Meeting of the Association for Compu - tational Linguistics ( Volume 1 : Long Papers ) , ACL 2022 , Dublin , Ireland , May 22 - 27 , 2022 , pages 8086 – 8098 . Association for Computational Linguistics . Ziyang Luo , Can Xu , Pu Zhao , Xiubo Geng , Chongyang Tao , Jing Ma , Qingwei Lin , and Daxin Jiang . 2023 . Augmented large language models with parametric knowledge guiding . CoRR , abs / 2305 . 04757 . Aman Madaan , Niket Tandon , Prakhar Gupta , Skyler Hallinan , Luyu Gao , Sarah Wiegreffe , Uri Alon , Nouha Dziri , Shrimai Prabhumoye , Yiming Yang , Sean Welleck , Bodhisattwa Prasad Majumder , Shashank Gupta , Amir Yazdanbakhsh , and Peter Clark . 2023 . Self - refine : Iterative refinement with self - feedback . CoRR , abs / 2303 . 17651 . Razi Mahmood , Ge Wang , Mannudeep K . Kalra , and Pingkun Yan . 2023 . Fact - checking of ai - generated reports . CoRR , abs / 2307 . 14634 . Alex Mallen , Akari Asai , Victor Zhong , Rajarshi Das , Daniel Khashabi , and Hannaneh Hajishirzi . 2023 . When not to trust language models : Investigating effectiveness of parametric and non - parametric mem - ories . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Vol - ume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 9802 – 9822 . Association for Computational Linguistics . Potsawee Manakul , Adian Liusie , and Mark J . F . Gales . 2023 . Selfcheckgpt : Zero - resource black - box hal - lucination detection for generative large language models . CoRR , abs / 2303 . 08896 . Marc Marone and Benjamin Van Durme . 2023 . Data portraits : Recording foundation model training data . CoRR , abs / 2303 . 03919 . Joshua Maynez , Shashi Narayan , Bernd Bohnet , and Ryan T . McDonald . 2020 . On faithfulness and fac - tuality in abstractive summarization . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , ACL 2020 , Online , July 5 - 10 , 2020 , pages 1906 – 1919 . Association for Com - putational Linguistics . Nick McKenna , Tianyi Li , Liang Cheng , Moham - mad Javad Hosseini , Mark Johnson , and Mark Steedman . 2023 . Sources of hallucination by large language models on inference tasks . CoRR , abs / 2305 . 14552 . Jacob Menick , Maja Trebacz , Vladimir Mikulik , John Aslanides , H . Francis Song , Martin J . Chadwick , Mia Glaese , Susannah Young , Lucy Campbell - Gillingham , Geoffrey Irving , and Nat McAleese . 2022 . Teaching language models to support answers with verified quotes . CoRR , abs / 2203 . 11147 . Nandana Mihindukulasooriya , Sanju Tiwari , Carlos F . Enguix , and Kusum Lata . 2023 . Text2kgbench : A benchmark for ontology - driven knowledge graph gen - eration from text . CoRR , abs / 2308 . 02357 . Sewon Min , Kalpesh Krishna , Xinxi Lyu , Mike Lewis , Wen - tau Yih , Pang Wei Koh , Mohit Iyyer , Luke Zettlemoyer , and Hannaneh Hajishirzi . 2023 . Factscore : Fine - grained atomic evaluation of fac - tual precision in long form text generation . CoRR , abs / 2305 . 14251 . Marvin Minsky . 1988 . Society of mind . Simon and Schuster . Swaroop Mishra , Daniel Khashabi , Chitta Baral , and Hannaneh Hajishirzi . 2021 . Natural instructions : Benchmarking generalization to new tasks from nat - ural language instructions . CoRR , abs / 2104 . 08773 . Niels Mündler , Jingxuan He , Slobodan Jenko , and Mar - tin T . Vechev . 2023 . Self - contradictory hallucinations of large language models : Evaluation , detection and mitigation . CoRR , abs / 2305 . 15852 . Munan Ning , Yujia Xie , Dongdong Chen , Zeyin Song , Lu Yuan , Yonghong Tian , Qixiang Ye , and Li Yuan . 2023 . Album storytelling with iterative story - aware captioning and large language models . CoRR , abs / 2305 . 12943 . Long Ouyang , Jeffrey Wu , Xu Jiang , Diogo Almeida , Carroll L . Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , John Schulman , Jacob Hilton , Fraser Kelton , Luke Miller , Maddie Simens , Amanda Askell , Peter Welin - der , Paul F . Christiano , Jan Leike , and Ryan Lowe . 2022 . Training language models to follow instruc - tions with human feedback . In NeurIPS . Artidoro Pagnoni , Vidhisha Balachandran , and Yulia Tsvetkov . 2021 . Understanding factuality in abstrac - tive summarization with FRANK : A benchmark for factuality metrics . In Proceedings of the 2021 Con - ference of the North American Chapter of the Asso - ciation for Computational Linguistics : Human Lan - guage Technologies , NAACL - HLT 2021 , Online , June 6 - 11 , 2021 , pages 4812 – 4829 . Association for Com - putational Linguistics . Liangming Pan , Michael Saxon , Wenda Xu , Deepak Nathani , Xinyi Wang , and William Yang Wang . 2023 . Automatically correcting large language models : Sur - veying the landscape of diverse self - correction strate - gies . CoRR , abs / 2308 . 03188 . Sung Min Park , Kristian Georgiev , Andrew Ilyas , Guil - laume Leclerc , and Aleksander Madry . 2023 . TRAK : attributing model behavior at scale . In International Conference on Machine Learning , ICML 2023 , 23 - 29 July 2023 , Honolulu , Hawaii , USA , volume 202 of Proceedings of Machine Learning Research , pages 27074 – 27113 . PMLR . Shishir G . Patil , Tianjun Zhang , Xin Wang , and Joseph E . Gonzalez . 2023 . Gorilla : Large lan - guage model connected with massive apis . CoRR , abs / 2305 . 15334 . Jiaxin Pei and David Jurgens . 2021 . Measuring sentence - level and aspect - level ( un ) certainty in sci - ence communications . In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan - guage Processing , EMNLP 2021 , Virtual Event / Punta Cana , Dominican Republic , 7 - 11 November , 2021 , pages 9959 – 10011 . Association for Computa - tional Linguistics . Baolin Peng , Michel Galley , Pengcheng He , Hao Cheng , Yujia Xie , Yu Hu , Qiuyuan Huang , Lars Liden , Zhou Yu , Weizhu Chen , and Jianfeng Gao . 2023 . Check your facts and try again : Improving large language models with external knowledge and automated feed - back . CoRR , abs / 2302 . 12813 . Suzanne Petryk , Spencer Whitehead , Joseph E . Gon - zalez , Trevor Darrell , Anna Rohrbach , and Marcus Rohrbach . 2023 . Simple token - level confidence im - proves caption correctness . CoRR , abs / 2305 . 07021 . Pouya Pezeshkpour . 2023 . Measuring and modifying factual knowledge in large language models . CoRR , abs / 2306 . 06264 . Jonas Pfeiffer , Francesco Piccinno , Massimo Nicosia , Xinyi Wang , Machel Reid , and Sebastian Ruder . 2023 . mmt5 : Modular multilingual pre - training solves source language hallucinations . CoRR , abs / 2305 . 14224 . Gualtiero Piccinini . 2004 . The first computational the - ory of mind and brain : a close look at mcculloch and pitts’s “logical calculus of ideas immanent in nervous activity” . Synthese , 141 : 175 – 215 . Dongqi Pu and Vera Demberg . 2023 . Chatgpt vs human - authored text : Insights into controllable text summa - rization and sentence style transfer . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics : Student Research Work - shop , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 1 – 18 . Association for Computational Linguis - tics . Yifu Qiu , Yftah Ziser , Anna Korhonen , Edoardo M . Ponti , and Shay B . Cohen . 2023 . Detecting and miti - gating hallucinations in multilingual summarisation . CoRR , abs / 2305 . 13632 . Vikas Raunak , Arul Menezes , and Marcin Junczys - Dowmunt . 2021 . The curious case of hallucinations in neural machine translation . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Hu - man Language Technologies , NAACL - HLT 2021 , On - line , June 6 - 11 , 2021 , pages 1172 – 1183 . Association for Computational Linguistics . Victor Sanh , Albert Webson , Colin Raffel , Stephen H . Bach , Lintang Sutawika , Zaid Alyafeai , Antoine Chaffin , Arnaud Stiegler , Arun Raja , Manan Dey , M Saiful Bari , Canwen Xu , Urmish Thakker , Shanya Sharma Sharma , Eliza Szczechla , Taewoon Kim , Gunjan Chhablani , Nihal V . Nayak , Debajyoti Datta , Jonathan Chang , Mike Tian - Jian Jiang , Han Wang , Matteo Manica , Sheng Shen , Zheng Xin Yong , Harshit Pandey , Rachel Bawden , Thomas Wang , Tr - ishala Neeraj , Jos Rozen , Abheesht Sharma , An - drea Santilli , Thibault Févry , Jason Alan Fries , Ryan Teehan , Teven Le Scao , Stella Biderman , Leo Gao , Thomas Wolf , and Alexander M . Rush . 2022 . Multi - task prompted training enables zero - shot task gener - alization . In The Tenth International Conference on Learning Representations , ICLR 2022 , Virtual Event , April 25 - 29 , 2022 . OpenReview . net . Wenqi Shao , Yutao Hu , Peng Gao , Meng Lei , Kaipeng Zhang , Fanqing Meng , Peng Xu , Siyuan Huang , Hongsheng Li , Yu Qiao , and Ping Luo . 2023 . Tiny lvlm - ehub : Early multimodal experiments with bard . CoRR , abs / 2308 . 03729 . Jiaming Shen , Jialu Liu , Daniel Finnie , Negar Rahmati , Mike Bendersky , and Marc Najork . 2023 . " why is this misleading ? " : Detecting news headline hallu - cinations with explanations . In Proceedings of the ACM Web Conference 2023 , WWW 2023 , Austin , TX , USA , 30 April 2023 - 4 May 2023 , pages 1662 – 1672 . ACM . Weijia Shi , Xiaochuang Han , Mike Lewis , Yulia Tsvetkov , Luke Zettlemoyer , and Scott Wen - tau Yih . 2023a . Trusting your evidence : Hallucinate less with context - aware decoding . CoRR , abs / 2305 . 14739 . Weijia Shi , Sewon Min , Michihiro Yasunaga , Minjoon Seo , Rich James , Mike Lewis , Luke Zettlemoyer , and Wen - tau Yih . 2023b . REPLUG : retrieval - augmented black - box language models . CoRR , abs / 2301 . 12652 . Noah Shinn , Beck Labash , and Ashwin Gopinath . 2023 . Reflexion : an autonomous agent with dynamic mem - ory and self - reflection . CoRR , abs / 2303 . 11366 . Keith Stanovich . 2011 . Rationality and the reflective mind . Oxford University Press , USA . Nisan Stiennon , Long Ouyang , Jeffrey Wu , Daniel M . Ziegler , Ryan Lowe , Chelsea Voss , Alec Radford , Dario Amodei , and Paul F . Christiano . 2020 . Learn - ing to summarize with human feedback . In Advances in Neural Information Processing Systems 33 : An - nual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual . Weiwei Sun , Zhengliang Shi , Shen Gao , Pengjie Ren , Maarten de Rijke , and Zhaochun Ren . 2023 . Con - trastive learning reduces hallucination in conversa - tions . In Thirty - Seventh AAAI Conference on Artifi - cial Intelligence , AAAI 2023 , Thirty - Fifth Conference on Innovative Applications of Artificial Intelligence , IAAI 2023 , Thirteenth Symposium on Educational Advances in Artificial Intelligence , EAAI 2023 , Wash - ington , DC , USA , February 7 - 14 , 2023 , pages 13618 – 13626 . AAAI Press . Derek Tam , Anisha Mascarenhas , Shiyue Zhang , Sarah Kwan , Mohit Bansal , and Colin Raffel . 2023 . Evalu - ating the factual consistency of large language mod - els through news summarization . In Findings of the Association for Computational Linguistics : ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 5220 – 5255 . Association for Computational Linguistics . Edward L Thorndike . 1898 . Animal intelligence . Na - ture , 58 ( 1504 ) : 390 – 390 . Hugo Touvron , Thibaut Lavril , Gautier Izacard , Xavier Martinet , Marie - Anne Lachaux , Timothée Lacroix , Baptiste Rozière , Naman Goyal , Eric Hambro , Faisal Azhar , Aurélien Rodriguez , Armand Joulin , Edouard Grave , and Guillaume Lample . 2023 . Llama : Open and efficient foundation language models . CoRR , abs / 2302 . 13971 . Harsh Trivedi , Niranjan Balasubramanian , Tushar Khot , and Ashish Sabharwal . 2023 . Interleaving retrieval with chain - of - thought reasoning for knowledge - intensive multi - step questions . In Proceedings of the 61st Annual Meeting of the Association for Com - putational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 10014 – 10037 . Association for Computational Lin - guistics . Logesh Kumar Umapathi , Ankit Pal , and Malaikannan Sankarasubbu . 2023 . Med - halt : Medical domain hallucination test for large language models . CoRR , abs / 2307 . 15343 . Neeraj Varshney , Wenlin Yao , Hongming Zhang , Jian - shu Chen , and Dong Yu . 2023 . A stitch in time saves nine : Detecting and mitigating hallucinations of llms by validating low - confidence generation . CoRR , abs / 2307 . 03987 . David Wan , Shiyue Zhang , and Mohit Bansal . 2023 . Histalign : Improving context dependency in lan - guage generation by aligning with history . CoRR , abs / 2305 . 04782 . Chaojun Wang and Rico Sennrich . 2020 . On exposure bias , hallucination and domain shift in neural ma - chine translation . In Proceedings of the 58th Annual Meeting of the Association for Computational Lin - guistics , ACL 2020 , Online , July 5 - 10 , 2020 , pages 3544 – 3552 . Association for Computational Linguis - tics . Junyang Wang , Yiyang Zhou , Guohai Xu , Pengcheng Shi , Chenlin Zhao , Haiyang Xu , Qinghao Ye , Ming Yan , Ji Zhang , Jihua Zhu , Jitao Sang , and Haoyu Tang . 2023a . Evaluation and analysis of hallu - cination in large vision - language models . CoRR , abs / 2308 . 15126 . Peiyi Wang , Lei Li , Liang Chen , Zefan Cai , Dawei Zhu , Binghuai Lin , Yunbo Cao , Qi Liu , Tianyu Liu , and Zhifang Sui . 2023b . Large language models are not fair evaluators . Peng Wang , An Yang , Rui Men , Junyang Lin , Shuai Bai , Zhikang Li , Jianxin Ma , Chang Zhou , Jingren Zhou , and Hongxia Yang . 2022 . OFA : unifying ar - chitectures , tasks , and modalities through a simple sequence - to - sequence learning framework . In Inter - national Conference on Machine Learning , ICML 2022 , 17 - 23 July 2022 , Baltimore , Maryland , USA , volume 162 of Proceedings of Machine Learning Research , pages 23318 – 23340 . PMLR . Peng Wang , Ningyu Zhang , Xin Xie , Yunzhi Yao , Bozhong Tian , Mengru Wang , Zekun Xi , Siyuan Cheng , Kangwei Liu , Guozhou Zheng , and Huajun Chen . 2023c . Easyedit : An easy - to - use knowledge editing framework for large language models . CoRR , abs / 2308 . 07269 . Sirui Wang , Kaiwen Wei , Hongzhi Zhang , Yuntao Li , and Wei Wu . 2023d . Let me check the examples : Enhancing demonstration learning via explicit imita - tion . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Vol - ume 2 : Short Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 1080 – 1088 . Association for Computational Linguistics . Xintao Wang , Qianwen Yang , Yongting Qiu , Jiaqing Liang , Qianyu He , Zhouhong Gu , Yanghua Xiao , and Wei Wang . 2023e . Knowledgpt : Enhancing large language models with retrieval and storage access on knowledge bases . CoRR , abs / 2308 . 11761 . Yufei Wang , Wanjun Zhong , Liangyou Li , Fei Mi , Xingshan Zeng , Wenyong Huang , Lifeng Shang , Xin Jiang , and Qun Liu . 2023f . Aligning large language models with human : A survey . CoRR , abs / 2307 . 12966 . Zhenhailong Wang , Shaoguang Mao , Wenshan Wu , Tao Ge , Furu Wei , and Heng Ji . 2023g . Unleash - ing cognitive synergy in large language models : A task - solving agent through multi - persona self - collaboration . CoRR , abs / 2307 . 05300 . Jason Wei , Maarten Bosma , Vincent Y . Zhao , Kelvin Guu , Adams Wei Yu , Brian Lester , Nan Du , An - drew M . Dai , and Quoc V . Le . 2022a . Finetuned language models are zero - shot learners . In The Tenth International Conference on Learning Representa - tions , ICLR 2022 , Virtual Event , April 25 - 29 , 2022 . OpenReview . net . Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Brian Ichter , Fei Xia , Ed H . Chi , Quoc V . Le , and Denny Zhou . 2022b . Chain - of - thought prompt - ing elicits reasoning in large language models . In NeurIPS . Wenjing Yue Wei Zhu and Xiaoling Wang . 2023 . Shennong - tcm : A traditional chinese medicine large language model . https : / / github . com / michael - wzhu / ShenNong - TCM - LLM . Orion Weller , Marc Marone , Nathaniel Weir , Dawn J . Lawrie , Daniel Khashabi , and Benjamin Van Durme . 2023 . " according to . . . " prompting language mod - els improves quoting from pre - training data . CoRR , abs / 2305 . 13252 . Yilin Wen , Zifeng Wang , and Jimeng Sun . 2023 . Mindmap : Knowledge graph prompting sparks graph of thoughts in large language models . CoRR , abs / 2308 . 09729 . Kai Xiong , Xiao Ding , Yixin Cao , Ting Liu , and Bing Qin . 2023 . Examining the inter - consistency of large language models : An in - depth analysis via debate . CoRR , abs / 2305 . 11595 . Can Xu , Qingfeng Sun , Kai Zheng , Xiubo Geng , Pu Zhao , Jiazhan Feng , Chongyang Tao , and Daxin Jiang . 2023 . Wizardlm : Empowering large lan - guage models to follow complex instructions . CoRR , abs / 2304 . 12244 . Kevin Yang , Dan Klein , Nanyun Peng , and Yuandong Tian . 2023 . DOC : improving long story coherence with detailed outline control . In Proceedings of the 61st Annual Meeting of the Association for Compu - tational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 3378 – 3465 . Association for Computational Linguistics . Kevin Yang , Yuandong Tian , Nanyun Peng , and Dan Klein . 2022 . Re3 : Generating longer stories with recursive reprompting and revision . In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , EMNLP 2022 , Abu Dhabi , United Arab Emirates , December 7 - 11 , 2022 , pages 4393 – 4479 . Association for Computational Linguistics . Shunyu Yao , Dian Yu , Jeffrey Zhao , Izhak Shafran , Thomas L . Griffiths , Yuan Cao , and Karthik Narasimhan . 2023 . Tree of thoughts : Deliberate problem solving with large language models . CoRR , abs / 2305 . 10601 . Qinghao Ye , Haiyang Xu , Guohai Xu , Jiabo Ye , Ming Yan , Yiyang Zhou , Junyang Wang , Anwen Hu , Pengcheng Shi , Yaya Shi , Chenliang Li , Yuanhong Xu , Hehong Chen , Junfeng Tian , Qian Qi , Ji Zhang , and Fei Huang . 2023 . mplug - owl : Modularization empowers large language models with multimodality . CoRR , abs / 2304 . 14178 . Fan Yin , Jesse Vig , Philippe Laban , Shafiq Joty , Caim - ing Xiong , and Chien - Sheng Wu . 2023a . Did you read the instructions ? rethinking the effectiveness of task definitions in instruction learning . In Proceed - ings of the 61st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 3063 – 3079 . Association for Computational Linguis - tics . Shukang Yin , Chaoyou Fu , Sirui Zhao , Ke Li , Xing Sun , Tong Xu , and Enhong Chen . 2023b . A sur - vey on multimodal large language models . CoRR , abs / 2306 . 13549 . Jifan Yu , Xiaozhi Wang , Shangqing Tu , Shulin Cao , Daniel Zhang - li , Xin Lv , Hao Peng , Zijun Yao , Xi - aohan Zhang , Hanming Li , Chunyang Li , Zheyuan Zhang , Yushi Bai , Yantao Liu , Amy Xin , Nianyi Lin , Kaifeng Yun , Linlu Gong , Jianhui Chen , Zhili Wu , Yunjia Qi , Weikai Li , Yong Guan , Kaisheng Zeng , Ji Qi , Hailong Jin , Jinxin Liu , Yu Gu , Yuan Yao , Ning Ding , Lei Hou , Zhiyuan Liu , Bin Xu , Jie Tang , and Juanzi Li . 2023a . Kola : Carefully benchmarking world knowledge of large language models . CoRR , abs / 2306 . 09296 . Wenhao Yu , Zhihan Zhang , Zhenwen Liang , Meng Jiang , and Ashish Sabharwal . 2023b . Improving lan - guage models via plug - and - play retrieval feedback . CoRR , abs / 2305 . 14002 . Wenhao Yu , Chenguang Zhu , Zaitang Li , Zhiting Hu , Qingyun Wang , Heng Ji , and Meng Jiang . 2022 . A survey of knowledge - enhanced text generation . ACM Comput . Surv . , 54 ( 11s ) : 227 : 1 – 227 : 38 . Shuzhou Yuan and Michael Färber . 2023 . Evaluat - ing generative models for graph - to - text generation . CoRR , abs / 2307 . 14712 . Weizhe Yuan , Graham Neubig , and Pengfei Liu . 2021 . Bartscore : Evaluating generated text as text genera - tion . In Advances in Neural Information Processing Systems 34 : Annual Conference on Neural Informa - tion Processing Systems 2021 , NeurIPS 2021 , De - cember 6 - 14 , 2021 , virtual , pages 27263 – 27277 . Aohan Zeng , Xiao Liu , Zhengxiao Du , Zihan Wang , Hanyu Lai , Ming Ding , Zhuoyi Yang , Yifan Xu , Wendi Zheng , Xiao Xia , Weng Lam Tam , Zixuan Ma , Yufei Xue , Jidong Zhai , Wenguang Chen , Zhiyuan Liu , Peng Zhang , Yuxiao Dong , and Jie Tang . 2023 . GLM - 130B : an open bilingual pre - trained model . In The Eleventh International Conference on Learning Representations , ICLR 2023 , Kigali , Rwanda , May 1 - 5 , 2023 . OpenReview . net . Muru Zhang , Ofir Press , William Merrill , Alisa Liu , and Noah A . Smith . 2023a . How language model hallucinations can snowball . CoRR , abs / 2305 . 13534 . Shuo Zhang , Liangming Pan , Junzhou Zhao , and William Yang Wang . 2023b . Mitigating lan - guage model hallucination with interactive question - knowledge alignment . CoRR , abs / 2305 . 13669 . Susan Zhang , Stephen Roller , Naman Goyal , Mikel Artetxe , Moya Chen , Shuohui Chen , Christopher Dewan , Mona T . Diab , Xian Li , Xi Victoria Lin , Todor Mihaylov , Myle Ott , Sam Shleifer , Kurt Shus - ter , Daniel Simig , Punit Singh Koura , Anjali Srid - har , Tianlu Wang , and Luke Zettlemoyer . 2022 . OPT : open pre - trained transformer language mod - els . CoRR , abs / 2205 . 01068 . Yifan Zhang , Jingqin Yang , Yang Yuan , and An - drew Chi - Chih Yao . 2023c . Cumulative reasoning with large language models . CoRR , abs / 2308 . 04371 . Ruochen Zhao , Xingxuan Li , Shafiq Joty , Chengwei Qin , and Lidong Bing . 2023a . Verify - and - edit : A knowledge - enhanced chain - of - thought framework . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 5823 – 5840 . Association for Com - putational Linguistics . Theodore Zhao , Mu Wei , J . Samuel Preston , and Hoifung Poon . 2023b . Llm calibration and auto - matic hallucination detection via pareto optimal self - supervision . Lianmin Zheng , Wei - Lin Chiang , Ying Sheng , Siyuan Zhuang , Zhanghao Wu , Yonghao Zhuang , Zi Lin , Zhuohan Li , Dacheng Li , Eric P . Xing , Hao Zhang , Joseph E . Gonzalez , and Ion Stoica . 2023a . Judg - ing llm - as - a - judge with mt - bench and chatbot arena . CoRR , abs / 2306 . 05685 . Shen Zheng , Jie Huang , and Kevin Chen - Chuan Chang . 2023b . Why does chatgpt fall short in answering questions faithfully ? CoRR , abs / 2304 . 10513 . Chunting Zhou , Pengfei Liu , Puxin Xu , Srini Iyer , Jiao Sun , Yuning Mao , Xuezhe Ma , Avia Efrat , Ping Yu , Lili Yu , Susan Zhang , Gargi Ghosh , Mike Lewis , Luke Zettlemoyer , and Omer Levy . 2023 . LIMA : less is more for alignment . CoRR , abs / 2305 . 11206 . Deyao Zhu , Jun Chen , Xiaoqian Shen , Xiang Li , and Mohamed Elhoseiny . 2023a . Minigpt - 4 : Enhancing vision - language understanding with advanced large language models . CoRR , abs / 2304 . 10592 . Xinyu Zhu , Junjie Wang , Lin Zhang , Yuxiang Zhang , Yongfeng Huang , Ruyi Gan , Jiaxing Zhang , and Yu - jiu Yang . 2023b . Solving math word problems via cooperative reasoning induced language models . In Proceedings of the 61st Annual Meeting of the As - sociation for Computational Linguistics ( Volume 1 : Long Papers ) , ACL 2023 , Toronto , Canada , July 9 - 14 , 2023 , pages 4471 – 4485 . Association for Computa - tional Linguistics . Xuekai Zhu , Biqing Qi , Kaiyan Zhang , Xingwei Long , and Bowen Zhou . 2023c . Pad : Program - aided distil - lation specializes large models in reasoning . CoRR , abs / 2305 . 13888 . Yuqi Zhu , Xiaohan Wang , Jing Chen , Shuofei Qiao , Yixin Ou , Yunzhi Yao , Shumin Deng , Huajun Chen , and Ningyu Zhang . 2023d . Llms for knowledge graph construction and reasoning : Recent capabili - ties and future opportunities . CoRR , abs / 2305 . 13168 .