Gesture Similarity Analysis on Event Data Using a Hybrid Guided Variational Auto Encoder Kenneth Stewart University of California , Irvine Department of Computer Science kennetms @ uci . edu Andreea Danielescu Accenture Labs San Francisco , CA andreea . danielescu @ accenture . com Lazar Supic Accenture Labs San Francisco , CA lazar . supic @ accenture . com Timothy Shea Accenture Labs San Francisco , CA timothy . m . shea @ accenture . com Emre Neftci University of California , Irvine Department of Cognitive Science Department of Computer Science eneftci @ uci . edu Abstract While commercial mid - air gesture recognition systems have existed for at least a decade , they have not become a widespread method of interacting with machines . This is primarily due to the fact that these systems require rigid , dramatic gestures to be performed for accurate recognition that can be fatiguing and unnatural . The global pandemic has seen a resurgence of interest in touchless interfaces , so new methods that allow for natural mid - air gestural inter - actions are even more important . To address the limitations of recognition systems , we propose a neuromorphic gesture analysis system which naturally declutters the background and analyzes gestures at high temporal resolution . Our novel model consists of an event - based guided Variational Autoencoder ( VAE ) which encodes event - based data sensed by a Dynamic Vision Sensor ( DVS ) into a latent space rep - resentation suitable to analyze and compute the similarity of mid - air gesture data . Our results show that the features learned by the VAE provides a similarity measure capable of clustering and pseudo labeling of new gestures . Further - more , we argue that the resulting event - based encoder and pseudo - labeling system are suitable for implementation in neuromorphic hardware for online adaptation and learning of natural mid - air gestures . 1 . Introduction Computers that can interact with users through touch and spoken language have become ubiquitous . However , people also communicate a great deal of information through body language and mid - air gestures , which are often closely cou - pled with speech [ 13 ] . This mode of interaction is beneﬁcial in a variety of human computer interaction applications be - cause it is natural , touchless , and efﬁcient . For example , mid - air gestures can be used to interact with car infotain - ment systems , making interaction safer by eliminating the need for drivers to look at a touch interface instead of the road while driving [ 51 ] [ 32 ] . Mid - air gesture interaction can also be beneﬁcial in high trafﬁc public areas , reduc - ing the need for touch based systems which can transmit pathogens from person to person . Additionally , interactive systems that leverage mid - air gestures are more accessible for people with hearing or speech impairments than other touchless alternatives , such as voice [ 5 ] . Despite numerous potential applications and recent progress in sensors capable of detecting mid - air gestures [ 29 ] [ 54 ] [ 23 ] [ 48 ] , accurate recognition of mid - air ges - tures remains a challenge . Changing backgrounds and light - ing conditions and wide variation in how users perform the same gesture are just a few of the reasons that accurate recognition in real - world environments is difﬁcult . As a re - sult , existing mid - air gesture recognition systems constrain users to performing predeﬁned , rigid gestures and require large data sets to train . But this approach results in reduced accuracy in real - world settings because people have difﬁ - a r X i v : 2104 . 00165v1 [ c s . N E ] 31 M a r 2021 culty reproducing or remembering rigid , precise movements [ 19 ] . A reliably accurate gesture recognition system needs to address these issues by being able to recognize natural gestures despite their wide ranging variability and changing backgrounds , and without relying on large datasets recorded in users’ homes . Recording large vision datasets from users homes and sending the data to the cloud for model training would be both an invasion of privacy and a security risk for home owners and their families . One way to address these issues is to adapt in real - time to user’s individual gestures , by measuring the similarity of the incoming gesture to existing gesture classes . Neuromorphic Dynamic Vision Sensors ( DVS ) inspired by the biological retina are particularly well suited to this task [ 7 ] , because they can capture temporal , pixel - wise intensity changes as a sparse stream of binary events , potentially enabling efﬁcient learning at the edge [ 15 ] . This method of visual sensing has key advantages over traditional RGB cameras , such as faster response times , better temporal resolution , and invariance to static image features like lighting and background . How - ever , effectively processing DVS event streams remains an open challenge . Unlike conventional frame - based cameras , events are asynchronous and spatially sparse , making it im - possible to directly apply conventional vision algorithms [ 16 , 15 ] . Spiking Neural Networks ( SNNs ) can efﬁciently process and learn from event - based data while taking advantage of temporal information [ 35 ] . SNN models are inspired by the biological cortex and can be used for hierarchical fea - ture extraction from the precise timing of events through event - by - event processing [ 18 ] . Recent work demonstrated how SNNs can be trained end - to - end using gradient back - propagation in time and standard autodifferentiation tools , making the integration of SNNs possible as part of modern machine learning and deep learning methods [ 53 , 6 , 39 ] . Here , we take advantage of this capability by incorpo - rating a convolutional SNN into a Variational Autoencoder ( VAE ) to encode spatio - temporal streams of events recorded by the DVS ( ﬁgure 1 ) . The goal of the VAE is to embed the streams of DVS events into a latent space which facil - itates the evaluation of gesture similarity . In order to best use the underlying hardware , we implement a hybrid VAE algorithm to process the DVS data , where the encoder is SNN - based and the decoder is based on a conventional con - volutional network . As some labelled data is included in the used dataset , we leverage a guided , supervised VAE to further disentangle the factors that account for gesture vari - ation . Our Hybrid Guided - VAE encodes the gesture data in a way that allows us to automatically measure and analyze the similarity of gestures , cluster similar gestures , and assign pseudo - labels to novel gestures . The key contributions of this work are : 1 . End - to - end trainable event - based SNNs for processing neuromorphic sensor data event - by - event and embed - ding them in a latent space . 2 . A Hybrid Guided - VAE that encodes event - based cam - era input and generates a latent space representation of the data that analyzes and calculates gesture similarity for clustering and pseudo - labeling . The ability to measure gesture similarity from DVS data is a key feature to enable mid - air gesture recognition systems that are less rigid and more natural because they can adapt to each user . 2 . Related Work 2 . 1 . Measuring Gesture Similarity Mid air gesture datasets are obtained through gesture elicitation studies . The goal of gesture elicitation studies is to collect user - deﬁned gestures and then group similar ges - tures together to determine the gestural language for an ap - plication . One way the language is determined is by cluster - ing gestures together based on a set of criteria [ 1 ] [ 47 ] . Ex - amples of criteria used include but are not limited to which body part is used , body part trajectory , and body pose . For accuracy , humans typically observe which gestures are sim - ilar and cluster them . The goal of clustering the gestures and ﬁnding an agreement , or similarity measure is twofold . ( 1 ) the humans clustering the data provide labels for the data and explanations as to why they labeled a gesture a certain way based on the clustering criteria ; and ( 2 ) the type of ges - ture most frequently used for a particular interaction can be identiﬁed . There are several issues with current clustering methods . The clustering criteria used to evaluate which gestures are similar are chosen subjectively by researchers and therefore vary from study to study [ 9 ] [ 10 ] [ 40 ] [ 44 ] . Using subjective criteria contrasts with the goal of gesture elicitation studies , which is to ﬁnd an objective consensus among the elicited gestures . The class labels given to the mid air gestures are also subjective and can vary across cultures . After cluster - ing , a similarity score is calculated , with researchers contin - uously reﬁning the appropriate similarity score to use [ 50 ] [ 46 ] [ 43 ] [ 45 ] . Additionally , because human labelers are needed , the process is tedious and expensive , which is why researchers are turning to crowd sourcing to reduce the time taken to cluster gesture data [ 1 ] [ 2 ] . To circumvent these issues , our work seeks to automate the process of computing gesture similarity from data for clustering . With recent advances in deep learning , the pro - cess of class labeling for image classiﬁcation and object recognition tasks can be automated [ 4 ] or semi - automated [ 38 ] . Because mid air gesture data can be collected in the form of images or video , deep learning can be used for au - tomatic gesture class labeling by learning a similarity mea - sure between the gestures . However , it is still a challeng - ing problem because salient image features may be missing in a scene or can be mistaken as part of the background . Additionally , mid air gestures are dynamic movements , so spatio - temporal information ( i . e . video ) processing is nec - essary for accurate recognition , which can be computation - Figure 1 : The Hybrid Guided - VAE architecture . Streams of gesture events recorded using a Dynamics Vision Sen - sor ( DVS ) are input into a Spiking Neural Network ( SNN ) that encodes the spatio - temporal features of the input data into a latent structure z . P and Q are pre - synaptic traces and U is the membrane potential of the spiking neuron . For clarity , only a single layer of the SNN is shown here and refractory states R are omitted . To help disentangle the latent space , a portion of the z equal to the number of target features y ∗ is input into a classiﬁer that trains each latent variable to encode these features ( Exc . Loss ) . The remaining z , noted \ m are input into a different classiﬁer that adversarially trains the latent variables to not encode the target features so they encode for other features in - stead ( Inh . Loss ) . The latent state z is decoded back into x ∗ using the conventional deconvolutional decoder lay - ers . ally expensive . Here , we automate the process of clus - tering mid air gestures for labeling using spatio - temporal event - based data streamed from a DVS . To automatically and objectively measure the similarity of gestures between and across classes for clustering and automatic labeling , we build a novel hybrid VAE model that can take advantage of the DVS’ temporal resolution and robustness while being end - to - end trainable using gradient descent on a variational objective . 2 . 2 . Variational Autoencoders VAEs are a type of generative model which deal with models of distributions p ( x ) , deﬁned over data points x ∈ X [ 24 ] . A VAE commonly consists of two networks , 1 ) an encoder ( Enc ) that encodes the captured dependencies of a data sample x into a latent representation z ; and 2 ) a decoder ( Dec ) that decodes the latent representation back to the data space making a reconstruction ˜ x . Using Gaussian assumptions for the latent space : Enc ( x ) = q ( z | x ) = N ( z | µ ( x ) , Σ ( x ) ) ( 1 ) ˜ x ≈ Dec ( z ) = p ( x | z ) , ( 2 ) where q is the encoding probability model into latent states z that are likely to produce x , and p is the decoding prob - ability model conditioned z . The functions µ ( x ) and Σ ( x ) are deterministic functions whose parameters can be trained through gradient - based optimization . Using a variational approach , the VAE loss consists of the sum of two terms resulting from the variational lower bound : log p ( x ) ≥ E z ∼ q log p ( x | z ) (cid:124) (cid:123)(cid:122) (cid:125) L ll − D KL ( q ( z | x ) | | p ( z ) ) (cid:124) (cid:123)(cid:122) (cid:125) L prior . ( 3 ) The ﬁrst term is the expected log likelihood of the recon - structed data computed using samples of the latent space , and the second term acts as a prior , where D KL is the Kullback - Leibler divergence . The VAE loss is thus formu - lated to maximize the variational lower bound by maximiz - ing −L prior and L ll . VAE’s latent space captures salient information for representing the data X , and thus similari - ties in the data [ 26 ] . 2 . 3 . Disentangling Variational Autoencoders VAEs do not necessarily disentangle all the factors of variation , which can make the latent space difﬁcult to inter - pret and use . Several approaches have been developed to improve the disentangling of the latent representation , such as Beta VAE [ 20 ] and Total Correlation VAE [ 8 ] . Because existing gesture datasets are often already labeled by user , lighting condition and gesture class , it would be most ad - vantageous to exploit these labels to guide the training of the VAE . For this reason , we employ a Guided - VAE , which has been developed speciﬁcally to disentangle the latent space representation of key features in a supervised fashion [ 12 ] . We describe here the supervised Guided - VAE algorithm , which is the basis of our hybrid model described in the next section . To learn a disentangled representation , a supervised Guided - VAE trains latent variables to encode existing ground - truth labels while making the rest of the latent vari - ables uncorrelated with that label . The supervised Guided - VAE model targets the generic generative modeling task by using an adversarial excitation and inhibition formulation . This is achieved by minimizing the discriminative loss for the desired latent variable while maximizing the minimal classiﬁcation error for the rest of the latent variables . For N training data samples X = ( x 1 , . . . , x N ) and M features with ground - truth labels , let z = ( z 1 , . . . , z m , . . . z M ) ⊕ z \ m where the z m deﬁne the “guided” latent variable capturing feature m , and z \ m represents the rest of the latent vari - ables . Let y m ( x n ) be a one - hot vector representing the ground - truth label for the m - th feature of sample x n . For each feature m , the excitation and inhibition losses are de - ﬁned as follows : L Exc ( z , m ) = max c m (cid:32) N (cid:88) n = 1 E q ( z m | x n ) log p c m ( y = y m ( x n ) | z m ) (cid:33) ( 4 ) L Inh ( z , m ) = max k m (cid:32) N (cid:88) n = 1 E q ( z \ m | x n ) log p k m ( y = y m ( x n ) | z \ m ) (cid:33) ( 5 ) where c m is a classiﬁer making a prediction on the m - th feature in the guided space and k m is a classiﬁer making a prediction over m in the unguided space z \ m . By train - ing these classiﬁers adversarially with the encoder part of the VAE , the encoder learns to disentangle the latent repre - sentation , with z m representing the target features and z \ m representing any features other than the target features . 3 . Methods 3 . 1 . Dynamic Vision Sensors and Preprocessing Dynamic Vision Sensors ( DVS ) are a type of event - based sensor that record event streams at a high temporal resolution and are compatible with SNNs [ 31 ] . Inspired by the human retina , DVS sensors detect events – or brightness changes – on a logarithmic scale with a user - tunable threshold , instead of RGB pixels like typical cameras . An event consists of the x , y location , timestamp t , and the polarity p ∈ [ OFF , ON ] representing direction of the intensity change . The DVS event stream is denoted S tDV S , x , y , p ∈ { 0 , 1 } , where 1 indicates the presence of a spike at space - time coordinates ( x , y , p , t ) . Mapping DVS events to Network Inputs : Recorded DVS events are streamed to a Hybrid Guided - VAE network implemented on a GPU ( network dynamics described in the following section ) . At each time bin , the number of DVS events of each coordinate ( x , y , p ) are mapped onto two 2D channels of a convolutional layer , one for each polarity . With the selected time bin , most pixels take value zero , and very few take values larger than two . Note that time is not represented as a separate dimension in the convolutional layer , but through the dynamics of the SNN . Mapping DVS events to VAE Targets : Time surfaces ( TS ) are widely used to preprocess event - based spatio - temporal features [ 25 ] . TS can be constructed by convolv - ing an exponential decay kernel through time in the event stream as follows : TS tx , y , p = (cid:15) t ∗ S tDV S , x , y , p with (cid:15) t = e − tτ ( 6 ) where τ is a time constant . Here , we convolve over the time length of the input gesture data stream S tDV S , x , y , p . This results in two 2D images , one for each polarity , that are used for the reconstruction loss . 3 . 2 . Hybrid Guided Variational Auto - Encoder Gestures recorded using a DVS camera produce streams of events containing rich spatio - temporal patterns of the gesture . Event - based computer vision algorithms typi - cally extract hand encoded statistics and use these in their models . While efﬁcient , this approach discards important spatio - temporal features from the data [ 15 ] . Rather than manually selecting a feature set , we process the raw DVS events while preserving key spatio - temporal features using a spiking neural network ( SNN ) trained end - to - end in the Hybrid Guided - VAE architecture shown in ﬁgure 1 . A key advantage of the VAE is that the loss can be op - timized using gradient backpropagation . To retain this ad - vantage in our hybrid VAE , we must ensure that the encoder SNN is also trainable through gradient descent . Until re - cently , several challenges hindered this : the spiking nature of neurons’ nonlinearity makes it non - differentiable and the continuous - time dynamics raise a challenging temporal credit assignment problem . These challenges are solved by the surrogate gradients approach [ 35 ] , which formulates the SNN as an equivalent binary RNN , and employs a smooth surrogate network for the purposes of computing the gradi - ents . Our Hybrid Guided - VAE uses a convolutional SNN to encode the spatio - temporal streams in the latent space , and a non - spiking convolutional decoder to reconstruct the TS of the data . Our choice of the event - based encoder is mo - tivated by the fact that the SNN can bridge computational time scales , by extracting slow and relevant factors of vari - ation in the gesture [ 49 ] from fast event streams recorded by the DVS . Our choice of the conventional ( non - spiking ) decoder is motivated by the fact that ( 1 ) for gesture similarity es - timation , we are mainly interested in the latent structure produced by the encoder , rather than the generative fea - tures of the network , ( 2 ) In a dedicated neuromorphic hard - ware implementation [ 21 ] , only the encoder would be nec - essary to obtain this latent structure , and ( 3 ) SNN training is compute - and memory - intensive . Thus a conventional decoder enables us to dedicate more resources to the SNN encoder . Hybrid VAEs that combine both spiking and ANN layers have been used before on DVS event data for predicting op - tical ﬂow , and found that the hybrid architecture efﬁciently processes the sparse spatio - temporal event inputs while pre - serving the spatio - temporal nature of the events [ 27 ] . Our Hybrid Guided - VAE network architecture is shown in ﬁgure 1 and the architecture description is provided in Table 1 . Descriptions of the excitatory and inhibitory net - works are provided in the Supplementary Information ( SI ) . The SNN encoder consists of four spiking convolutional layers followed by linear layers , and outputs a pair of vec - tors ( µ , Σ ) for sampling the latent state z . According to the guided VAE , part of or all of latent state z is input into one of three connected networks , the excitation classiﬁer , the adversarial inhibition classiﬁer , or the decoder . The target features for the excitatory network are given as one - hot encoded vectors of length M . The excitation classiﬁer is jointly trained with the encoder to train the ﬁrst M latent variables to only encode information relevant to the corresponding target feature . The inhibition classiﬁer takes as input the remaining latent variables in the latent space , z \ m , and are adversarially trained on two sets of tar - gets . One set of targets are the same target features that the excitation classiﬁer trains on . The other set of target fea - tures is a vector of length M but all of the values are set to 0 . 5 indicating that none of the values correspond to any target . The inhibition classiﬁer is jointly trained with the encoder to train the remaining z \ m latent variables to not encode any information relevant to target features forcing them to instead encode information for other features . The decoder is a transposed convolutional network that takes the full latent state z as input to construct the TS , denoted ˜ x in ﬁgure 1 . Table 1 : Hybrid Guided - VAE Architecture . Layer Kernel Output Layer Type input 32 × 32 × 2 DVS128 1 2a 16 × 16 × 2 SNN LIF Encoder 2 32c7p0s1 16 × 16 × 32 3 1a 16 × 16 × 32 4 64c7p0s1 16 × 16 × 64 5 2a 8 × 8 × 64 6 64c7p0s1 8 × 8 × 64 7 1a 8 × 8 × 64 8 128c7p0s1 8 × 8 × 128 9 1a 8 × 8 × 128 10 - 128 11 - 100 µ ( U t ) ( ANN ) 12 - 100 Σ ( U t ) ( ANN ) 13 - 128 ANN Decoder 14 128c4p0s2 4 × 4 × 128 15 64c4p1s2 8 × 8 × 64 16 32c4p1s2 16 × 16 × 32 output 2c4p1s2 32 × 32 × 2 Time Surface Notation : Ya represents YxY sumpooling , XcYpZsS represents X convolutionﬁlters ( YxY ) with padding Z and stride S . 3 . 3 . Encoder SNN Dynamics To take full advantage of the event - based nature of the DVS input stream and its rich temporal features , the data is encoded using an SNN . SNNs can be formulated as a type of recurrent neural network with binary activation functions ( Figure 1 ) [ 35 ] . With this formulation , SNN training can be carried out using standard tools of autodifferentiation . In particular , to best match the dynamics of existing digital neuromorphic hardware implementing SNNs [ 11 , 14 ] , our neuron model consists of a discretized Leaky Integrate and Fire ( LIF ) neuron model with time step ∆ t [ 22 ] : U ti = (cid:88) j W ij P tj − U th R ti + b i , P t + ∆ t j = αP tj + ( 1 − α ) Q tj , Q t + ∆ t j = βQ tj + ( 1 − β ) S tin , j , S ti = Θ ( U ti ) , ( 7 ) where the constants α = exp ( − ∆ t τ mem ) and β = exp ( − ∆ t τ syn ) reﬂect the decay dynamics of the membrane potential and the synaptic state during a ∆ t timestep , where τ mem and τ syn are membrane and synaptic time constants , respec - tively . The time step in our experiments was ﬁxed to ∆ t = 1 ms . R i here implements the reset and refractory pe - riod of the neuron , and states P i , Q i are pre - synaptic traces that capture the leaky dynamics of the membrane potential and the synaptic currents . S t i = Θ ( U t i ) represents the spik - ing non - linearity , computed using the unit step function , where Θ ( U i ) = 0 if U i < U th , otherwise 1 . We distin - guish here the input spike train S tin from the output spike train S t . Following the surrogate gradient approach [ 35 ] , for the purposes of computing the gradient , the derivative of Θ is replaced with the derivative of the fast sigmoid function [ 52 ] . Note that equation ( 7 ) is equivalent to a discrete - time version of the spike response model with linear ﬁlters [ 17 ] . Similar networks were used for classiﬁcation tasks on the DVSGesture dataset , leading to state - of - the - art accuracy on that task [ 22 , 39 ] . In the SI , we show how these equations can be obtained via discretization of the common LIF neu - ron . The SNN follows a convolutional architecture , as described in Table 1 , encoding the input sequence S tin into a membrane potential variable U t in the ﬁnal layer . The network computes µ ( U t ) and Σ ( U t ) as in a conventional VAE , but using the ﬁnal membrane potential state U t . Thanks to the chosen neural dynamics , the TS can be naturally computed by our network . In fact , using an appropriate choice of τ = τ syn for computing the TS , it becomes exactly equivalent to the pre - synaptic trace Q t of our network ( See SI ) . Hence , our choice of input and target indeed corresponds to an autoencoder in the space of pre - synaptic traces Q t . 3 . 4 . Datasets Our results are shown for datasets that were collected us - ing DVS sensors [ 28 , 37 ] , the Neuromorphic MNIST ( NM - NIST ) and IBM DvsGesture datasets . NMNIST consists of 32 × 32 , 300 ms long event data streams of MNIST im - ages recorded with a DVS [ 36 ] . The dataset contains 60 , 000 training event streams and 10 , 000 test event streams . The IBM DvsGesture dataset [ 3 ] consists of recordings of 29 different individuals performing 10 different actions such as clapping and an unspeciﬁed gesture that serves as a “catch - all” for gestures that do not ﬁt into the other 10 ac - tions for a total of 11 classes [ 3 ] . The actions are recorded under four different lighting conditions , so each gesture is also labeled with the associated lighting condition under which it was performed . Samples from the ﬁrst 23 subjects were used for training and the last 6 subjects were used for testing . The training set contains 1078 samples and the test set contains 264 samples . Each sample consists of about 6 seconds of the gesture performed . In our work we scale each sequence to 32x32 and only use a randomly sampled 200ms sequence of each sample to match real time learning conditions of the gesture data . For both datasets , to reduce memory requirements , the gradients were truncated to 100 time steps ( i . e . 100 ms worth of data ) . The input events are summed in ∆ t = 1 ms time bins and then fed to the net - work . We use the IBM DvsGesture dataset because it is currently the only full body gesture dataset available that uses recordings from an event based camera . The task is to learn a latent space encoding that can be used to both re - construct the digits or gestures and to learn the similarities in the data so that similar data points can be clustered and pseudo labels assigned . Our choice for the event - based datasets above is further mo - tivated by the nature of neuromorphic computing : the appli - cations of neuromorphic sensors and computing are justi - ﬁed primarily when applied to event - based data , and partic - ularly when features are embedded in the spatio - temporal features of the data . For this reason , we cannot meaning - fully test on standard image datasets such as CIFAR and ImageNet . 4 . Results To analyze the similarity between data samples in the la - tent space we examine the accuracy of the excitation classi - ﬁer in correctly identifying a gesture , T - SNE projections of the different parts of the latent space , latent space traversals , and the effect of using different guiding variables , namely lighting conditions . Finally , we observe the quality of the embeddings based on our own DVS recordings of novel gestures . Accuracy and Reconstruction : The excitation classi - ﬁer trained on the latent space of the hybrid VAE using the NMNIST dataset achieves a training accuracy of approxi - mately 99 % and a test accuracy of approximately 99 % , indi - cating that the SNN encoder is learning a latent representa - tion that is able to disentangle along the target features . The excitation classiﬁer results on the DvsGesture dataset with - out the eleventh “other” class achieves a training accuracy of approximately 99 % and test accuracy of approximately 87 % , indicating the SNN encoder is learning a disentangled latent representation of features unique to each gesture class but is having some difﬁculty distinguishing between certain gestures that are very similar . In Figure 2 , a sample gesture from each of the gesture classes is visualized as a time surface . Color in the sam - ples corresponds to the TS value of the events at the end of the sequence . Note that the TS leaves a signiﬁcant amount Figure 2 : Original ( top ) and reconstructed ( bottom ) time - surfaces for a sample gesture from each class . The recon - structions reﬂect the location of each gesture but with some smoothing of the detail . of ﬁne detail intact . In contrast , encoding and then decod - ing the samples results in a reconstruction that preserves the general structure of the gesture but smooths out some of the detail . Note that the ﬁdelity of the autoencoder is less important than the capacity of the model to disentan - gle the gesture classes . Furthermore , disentangling autoen - coders are known to provide lower quality reconstructions compared to the vanilla VAE [ 8 ] . Latent Space Projections : We use T - SNE to exam - ine the capacity of the network to disentangle key fea - tures of gestures in the latent space . T - SNE embeds both the local and global topology of the latent space into a low - dimensional visualization [ 34 ] , allowing us to visual - ize clustering and separation between gesture classes and lighting conditions . Figure 3 shows a T - SNE scatter plot of the guided por - tion of the latent space trained on the NMNIST dataset . As expected , each trained digit class is separated into its own cluster , demonstrating the ability of the Hybrid Guided - VAE’s to disentangle the noisy input domain in its latent space representations . Figure 4 shows a T - SNE plot of the guided portion of the latent space trained on the DVSGesture dataset . The latent space is clustered by the gesture that the excitation classi - ﬁer targeted for each of the z m , with some overlap between very similar gestures such as left arm clockwise versus left arm counterclockwise . The bottom of the ﬁgure shows sam - ples from the “other” class in the Dvs Gesture dataset in re - lation to the T - SNE projection of the guided latent space . The embedded positions of these gestures show which ges - ture classes they are most similar to . For example , gestures with certain salient features , such as a salient left hand , are placed with the cluster of the gesture class that has the same salient feature , such as the left hand wave class . Thus , the latent states obtained for these gestures can provide the ba - sis for pseudolabeling of new gestures by measuring the similarity between unclassiﬁed samples and existing ges - ture classes in the latent space . DVSGesture Latent Traversals : We use latent traver - sals to reveal additional structure within the latent space . Traversals of the guided dimensions of the latent space are shown in Figure 5 . Each image shown in the ﬁgure is a time - surface based on a position in the latent space computed using off polarity events . From left to right , the images rep - resent a traversal from the minimum to the maximum value of a given guided dimension . Figure 3 : A T - SNE plot of the z m portion of the latent space of the encoded NMNIST dataset . The digits are accurately clustered . Figure 4 : A T - SNE plot of the z m portion of the latent space of the encoded Dvs Gesture dataset . The gestures are clearly clustered by class , indicating the latent space disen - tangled each of the gestures . Gestures with similar motion , such as air drums and arm roll , are also globally more simi - lar than gestures qualitatively different motion , such as right hand wave and arm roll . Six unclassiﬁed gestures ( labeled “other” in the dataset ) are shown below as time - surfaces connected to the respective embedded positions in the la - tent space . Each of the unclassiﬁed gestures are embedded near gesture classes which exhibit similar motion . Figure 6 shows different dimensions of the latent space to demonstrate that the Hybrid Guided - VAE has learned a disentangled feature representation of the DVSGestures . The latent space variables are ﬁxed except for the second and third z m variables that represent a right hand wave and a left hand wave respectively . The top panel of Figure 6 shows reconstructions when the right hand wave z m is high ( left ) and the rest of the z m are low while the right - most im - age has the left hand wave z m high and the remaining z m are low . The images in - between show the incremental tran - sition between these values . Figure 6 ( bottom ) shows the effect of the z \ m latent variables on a right hand gesture . As the values of the z \ m variables change the gesture remains the same , however the position , orientation , and saliency of the hand and arm changes suggesting that the Hybrid Guided - VAE represents the variability within the gesture type . Guiding on Light Conditions : A key feature of the guided VAE is to incorporate alternative features not di - rectly related to the gesture class to disentangle the factors of variation in the data . To demonstrate this , in a separate experiment , we trained on the lighting condition provided in the DVSGesture dataset instead of the gesture class . The T - SNE projection of the z m latent space is shown in Figure 7a . The model clusters the lighting conditions with some overlap between LED lighting and the other lighting condi - tions . This is likely due to the fact that the lighting condi - tions under which the gestures are performed are combina - tions of the labeled lighting conditions , with the label given to the most prominent lighting condition used [ 3 ] . Figure 7b shows a T - SNE projection of the z \ m latent space but is labeled by gesture class instead of lighting . The projec - tion shows clustering of the gesture classes , particularly the classes that share similarities such as right hand wave , right arm clockwise , and right arm counterclockwise . This shows evidence that while the z m space is learning features key to the lighting , the z \ m is learning features key to the gesture class instead . Embeddings of Self - Recorded Gestures : So far , all the gesture data used for the VAE originated from the DVS - Gestures dataset , which featured well controlled lighting and positioning conditions . Here , we evaluate how such a VAE would perform under less controlled conditions . We recorded two new classes of gestures not present in the DVSGesture dataset on a different DVS sensors ( the DAVIS 240C sensor [ 7 ] ) and input to the Hybrid Guided - VAE . We recorded three instances of each class , right swipe down and left swipe down for approximately 3 s by the same sub - ject under the same lighting condition . Figure 8 shows the gesture time surfaces and their associated T - SNE embed - dings in the z m portion of the latent space . All of the right swipe down gestures were evaluated by our VAE as being most similar to right hand wave gestures , which is seen in their position in the T - SNE of the latent space . The excita - tion classiﬁer labeled each of the right swipe down gestures as right hand waves . All of the left swipe down gestures were evaluated by our VAE as being most similar to left hand wave gestures , which is seen in their position in the T - SNE of the latent space . The excitation classiﬁer labeled each of the left swipe down gestures as left hand waves . The results shown in Figure 8 demonstrate that our Hybrid Guided - VAE is able to appropriately process , cluster , and label novel gestures . 5 . Discussion and Future Work We presented a novel Hybrid Guided - VAE algorithm to process DVS sensor data that uses an SNN encoder to en - code spike events into a latent representation that is jointly Figure 5 : Reconstructionsoflatenttraversals across each guided latent dimension . Each z m represents a gesture class . Figure 6 : Examination of the disentangled latent space : ( Top ) Beginning with the right hand wave latent variable maximized and the left hand wave variable minimized , tra - verse along the latent space by gradually decreasing the right hand wave latent variable and increasing the left hand wave latent variable . ( Bottom ) Latent traversal along the latent space of non target z \ m latent variables . ( a ) ( b ) Figure 7 : ( a ) A T - SNE plot of the guided portion of z using lighting condition labels . ( b ) A T - SNE plot of the corre - sponding unguided variables guided but labeled with ges - ture classes . The unguided latent space is encoding features relevant to the gesture being performed , such as which parts of the bodies perform the gesture . trained by two classiﬁers to disentangle and encode target features . The goal of the algorithm is to understand the sim - ilarity of data , such as gestures , streamed to the network to existing classes in order to apply pseudo labels . Our results demonstrate that the Hybrid Guided - VAE is able to disentangle the latent space of encoded event data based on the features the classiﬁer is guided on . Therefore the VAE is learning a similarity measure for the event data that can be used to cluster the disentangled latent space by the guided target features for gesture similarity measure - ments . We also showed that with a SNN encoder and latent Figure 8 : Projections of the z m portion of the latent space of encoded new gestures we recorded using a different DVS , and not part of the DVSGesture dataset . space classiﬁer , the model can be used to determine pseudo labels for incoming unlabeled data based on its similarity to classes of previously seen data which can be used for self - supervised or semi - supervised learning . Thanks to the sparse nature of event - based data and pro - cessing , our SNN encoding implementation is interesting for applications at the edge , such as in a home or on a mobile device , where computing power is limited and pri - vacy is paramount . While our GPU - based solution does not run in real - time and does not leverage spatio - temporal spar - sity , dedicated neuromorphic hardware can leverage this sparsity for real - time , compact , ultra - low power process - ing [ 11 , 30 , 33 ] . Looking forward to such an implemen - tation , our SNN model was chosen to be compatible with a previously demonstrated method of few - shot learning at low - power on the Intel Loihi Neuromorphic Research Chip [ 41 ] . One limitation of this work is the lack of benchmark - ing or a dataset that allows for comparison between differ - ent similarity measures . Additionally , there is no publicly available data in event - based , frame - based , and pose - point datasets using the same data samples that allows for com - parison to other methods to such as [ 45 ] ’s dissimilarity mea - surement using body pose data . 6 . Conclusion This work sets the foundation for a real - time neuro - morphic implementation which can induce a gesture opti - mized latent state corresponding to novel gestures streamed from an event - based camera . The generated signals can be used to perform online self - supervised learning to update a recognition model on the ﬂy [ 42 ] , enabling applications such as life long learning for full body gesture interaction . References [ 1 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . Crowdsourcing similarity judgments for agree - ment analysis in end - user elicitation studies . UIST ’18 , page 177 – 188 , New York , NY , USA , 2018 . Association for Com - puting Machinery . [ 2 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . Crowdlicit : A system for conducting distributed end - user elicitation and identiﬁcation studies . In Proceed - ings of the 2019 CHI Conference on Human Factors in Com - puting Systems , CHI ’19 , page 1 – 12 , New York , NY , USA , 2019 . Association for Computing Machinery . [ 3 ] Arnon Amir , Brian Taba , David Berg , Timothy Melano , Jef - frey McKinstry , Carmelo Di Nolfo , Tapan Nayak , Alexander Andreopoulos , Guillaume Garreau , Marcela Mendoza , et al . A low power , fully event - based gesture recognition system . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7243 – 7252 , 2017 . [ 4 ] M Bah , Adel Haﬁane , and Raphael Canals . Deep learn - ing with unsupervised data labeling for weed detection in line crops in uav images . Remote Sensing , 10 ( 11 ) : 1690 , Oct 2018 . [ 5 ] Fabio Ballati , Fulvio Corno , and Luigi De Russis . Assessing virtual assistant capabilities with italian dysarthric speech . In Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility , ASSETS ’18 , page 93 – 101 , New York , NY , USA , 2018 . Association for Computing Machinery . [ 6 ] Guillaume Bellec , Franz Scherr , Elias Hajek , Darjan Salaj , Robert Legenstein , and Wolfgang Maass . Bio - logically inspired alternatives to backpropagation through time for learning in recurrent neural nets . arXiv preprint arXiv : 1901 . 09049 , 2019 . [ 7 ] Christian Brandli , Raphael Berner , Minhao Yang , Shih - Chii Liu , and Tobi Delbruck . A 240 × 180 130 db 3 µ s latency global shutter spatiotemporal vision sensor . IEEE Journal of Solid - State Circuits , 49 ( 10 ) : 2333 – 2341 , 2014 . [ 8 ] Ricky T . Q . Chen , Xuechen Li , Roger B Grosse , and David K Duvenaud . Isolating sources of disentanglement in variational autoencoders . In S . Bengio , H . Wallach , H . Larochelle , K . Grauman , N . Cesa - Bianchi , and R . Garnett , editors , Advances in Neural Information Processing Systems , volume 31 . Curran Associates , Inc . , 2018 . [ 9 ] Sabrina Connell , Pei - Yi Kuo , Liu Liu , and Anne Marie Piper . A wizard - of - oz elicitation study examining child - deﬁned gestures with a whole - body interface . In Proceedings of the 12th International Conference on Interaction Design and Children , IDC ’13 , page 277 – 280 , New York , NY , USA , 2013 . Association for Computing Machinery . [ 10 ] Sabrina Connell , Pei - Yi Kuo , Liu Liu , and Anne Marie Piper . A wizard - of - oz elicitation study examining child - deﬁned gestures with a whole - body interface . In Proceedings of the 12th International Conference on Interaction Design and Children , IDC ’13 , page 277 – 280 , New York , NY , USA , 2013 . Association for Computing Machinery . [ 11 ] M . Davies , N . Srinivasa , T . H . Lin , G . Chinya , P . Joshi , A . Lines , A . Wild , and H . Wang . Loihi : A neuromorphic manycore processor with on - chip learning . IEEE Micro , PP ( 99 ) : 1 – 1 , 2018 . [ 12 ] Zheng Ding , Yifan Xu , Weijian Xu , Gaurav Parmar , Yang Yang , Max Welling , and Zhuowen Tu . Guided variational autoencoder for disentanglement learning . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) , June 2020 . [ 13 ] Marlen Fr¨ohlich , Christine Sievers , Simon W . Townsend , Thibaud Gruber , and Carel P . van Schaik . Multimodal com - munication and language origins : integrating gestures and vocalizations . Biological Reviews , 94 ( 5 ) : 1809 – 1829 , 2019 . [ 14 ] Steve B Furber , Francesco Galluppi , Sally Temple , Luis Plana , et al . The spinnaker project . Proceedings of the IEEE , 102 ( 5 ) : 652 – 665 , 2014 . [ 15 ] Guillermo Gallego , Tobi Delbruck , Garrick Orchard , Chiara Bartolozzi , Brian Taba , Andrea Censi , Stefan Leuteneg - ger , Andrew Davison , J¨org Conradt , Kostas Daniilidis , et al . Event - based vision : A survey . arXiv preprint arXiv : 1904 . 08405 , 2019 . [ 16 ] G . Gallego , T . Delbruck , G . M . Orchard , C . Bartolozzi , B . Taba , A . Censi , S . Leutenegger , A . Davison , J . Conradt , K . Daniilidis , and D . Scaramuzza . Event - based vision : A sur - vey . IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 1 – 1 , 2020 . [ 17 ] W . Gerstner and W . Kistler . Spiking Neuron Models . Sin - gle Neurons , Populations , Plasticity . Cambridge University Press , 2002 . [ 18 ] Wulfram Gerstner , Werner M Kistler , Richard Naud , and Liam Paninski . Neuronal dynamics : From single neurons to networks and models of cognition . Cambridge University Press , 2014 . [ 19 ] Noorkholis Luthﬁl Hakim , Timothy K . Shih , Sandeli Priyan - wada Kasthuri Arachchi , Wisnu Aditya , Yi - Cheng Chen , and Chih - Yang Lin . Dynamic hand gesture recogni - tion using 3dcnn and lstm with fsm context - aware model . Sensors ( Basel , Switzerland ) , 19 ( 24 ) : 5429 , Dec 2019 . 31835404 [ pmid ] . [ 20 ] I . Higgins , Lo¨ıc Matthey , A . Pal , C . Burgess , Xavier Glorot , M . Botvinick , S . Mohamed , and Alexander Lerchner . beta - vae : Learning basic visual concepts with a constrained vari - ational framework . In ICLR , 2017 . [ 21 ] G . Indiveri , B . Linares - Barranco , T . J . Hamilton , A . van Schaik , R . Etienne - Cummings , T . Delbruck , S . - C . Liu , P . Dudek , P . H¨aﬂiger , S . Renaud , J . Schemmel , G . Cauwen - berghs , J . Arthur , K . Hynna , F . Folowosele , S . Saighi , T . Serrano - Gotarredona , J . Wijekoon , Y . Wang , and K . Boahen . Neuromorphic silicon neuron circuits . Frontiers in Neuro - science , 5 : 1 – 23 , 2011 . [ 22 ] Jacques Kaiser , Hesham Mostafa , and Emre Neftci . Synap - tic plasticity dynamics for deep continuous local learning ( decolle ) . Frontiers in Neuroscience , 14 : 424 , 2020 . [ 23 ] Leonid Keselman , John Iselin Woodﬁll , Anders Grunnet - Jepsen , and Achintya Bhowmik . Intel realsense stereoscopic depth cameras . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) Work - shops , July 2017 . [ 24 ] Diederik P Kingma and Max Welling . Auto - encoding varia - tional bayes . arXiv preprint arXiv : 1312 . 6114 , 2013 . [ 25 ] X . Lagorce , G . Orchard , F . Galluppi , B . E . Shi , and R . B . Benosman . Hots : A hierarchy of event - based time - surfaces for pattern recognition . IEEE Transactions on Pattern Anal - ysis and Machine Intelligence , 39 ( 7 ) : 1346 – 1359 , 2017 . [ 26 ] Anders Boesen Lindbo Larsen , Søren Kaae Sønderby , Hugo Larochelle , and Ole Winther . Autoencoding beyond pixels using a learned similarity metric . volume 48 of Proceed - ings of Machine Learning Research , pages 1558 – 1566 , New York , New York , USA , 20 – 22 Jun 2016 . PMLR . [ 27 ] Chankyu Lee , Adarsh Kumar Kosta , Alex Zihao Zhu , Ken - neth Chaney , Kostas Daniilidis , and Kaushik Roy . Spike - ﬂownet : Event - based optical ﬂow estimation with energy - efﬁcient hybrid neural networks , 2020 . [ 28 ] P . Lichtsteiner , C . Posch , and T . Delbruck . A 128x128 120 dB 15 us latency asynchronous temporal contrast vision sen - sor . Solid - State Circuits , IEEE Journal of , 43 ( 2 ) : 566 – 576 , Feb . 2008 . [ 29 ] P . Lichtsteiner , C . Posch , and T . Delbruck . An 128x128 120dB 15 µ s - latency temporal contrast vision sensor . IEEE J . Solid State Circuits , 43 ( 2 ) : 566 – 576 , 2008 . [ 30 ] Qian Liu , Ole Richter , Carsten Nielsen , Sadique Sheik , Gi - acomo Indiveri , and Ning Qiao . Live demonstration : face recognition on an ultra - low power event - driven convolu - tional neural network asic . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops , pages 0 – 0 , 2019 . [ 31 ] S . - C . Liu and T . Delbruck . Neuromorphic sensory systems . Current Opinion in Neurobiology , 20 ( 3 ) : 288 – 295 , 2010 . [ 32 ] Keenan R . May , Thomas M . Gable , and Bruce N . Walker . Designing an in - vehicle air gesture set using elicitation meth - ods . In Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Ap - plications , AutomotiveUI ’17 , page 74 – 83 , New York , NY , USA , 2017 . Association for Computing Machinery . [ 33 ] Christian Mayr , Sebastian Hoeppner , and Steve Furber . Spinnaker 2 : A 10 million core processor system for brain simulation and machine learning . arXiv preprint arXiv : 1911 . 02385 , 2019 . [ 34 ] Sudipto Mukherjee , Himanshu Asnani , Eugene Lin , and Sreeram Kannan . Clustergan : Latent space clustering in gen - erative adversarial networks . Proceedings of the AAAI Con - ference on Artiﬁcial Intelligence , 33 ( 01 ) : 4610 – 4617 , Jul . 2019 . [ 35 ] E . O . Neftci , H . Mostafa , and F . Zenke . Surrogate gradi - ent learning in spiking neural networks : Bringing the power of gradient - based optimization to spiking neural networks . IEEE Signal Processing Magazine , 36 ( 6 ) : 51 – 63 , Nov 2019 . [ 36 ] Garrick Orchard , Ajinkya Jayawant , Gregory K . Cohen , and Nitish Thakor . Converting static image datasets to spiking neuromorphic datasets using saccades . Frontiers in Neuro - science , 9 , nov 2015 . [ 37 ] C . Posch , D . Matolin , and R . Wohlgenannt . A qvga 143 db dynamic range frame - free pwm image sensor with lossless pixel - level video compression and time - domain cds . Solid - State Circuits , IEEE Journal of , 46 ( 1 ) : 259 – 275 , jan . 2011 . [ 38 ] Krittaphat Pugdeethosapol , M . Bishop , Dennis B . Bowen , and Q . Qiu . Automatic image labeling with click supervision on aerial images . 2020 International Joint Conference on Neural Networks ( IJCNN ) , pages 1 – 8 , 2020 . [ 39 ] Sumit Bam Shrestha and Garrick Orchard . Slayer : Spike layer error reassignment in time . In Advances in Neural In - formation Processing Systems , pages 1412 – 1421 , 2018 . [ 40 ] Chaklam Silpasuwanchai and Xiangshi Ren . Jump and shoot ! prioritizing primary and alternative body gestures for intense gameplay . In Proceedings of the SIGCHI Con - ference on Human Factors in Computing Systems , CHI ’14 , page 951 – 954 , New York , NY , USA , 2014 . Association for Computing Machinery . [ 41 ] K . Stewart , G . Orchard , S . B . Shrestha , and E . Neftci . On - line few - shot gesture learning on a neuromorphic processor . IEEE Journal on Emerging and Selected Topics in Circuits and Systems , 10 ( 4 ) : 512 – 521 , 2020 . [ 42 ] K . Stewart , G . Orchard , S . B . Shrestha , and E . Neftci . On - line few - shot gesture learning on a neuromorphic processor . IEEE Journal on Emerging and Selected Topics in Circuits and Systems , 10 ( 4 ) : 512 – 521 , Oct 2020 . [ 43 ] Theophanis Tsandilas . Fallacies of agreement : A critical re - view of consensus assessment methods for gesture elicita - tion . ACM Trans . Comput . - Hum . Interact . , 25 ( 3 ) , June 2018 . [ 44 ] Radu - Daniel Vatavu . User - deﬁned gestures for free - hand tv control . In Proceedings of the 10th European Conference on Interactive TV and Video , EuroITV ’12 , page 45 – 48 , New York , NY , USA , 2012 . Association for Computing Machin - ery . [ 45 ] Radu - Daniel Vatavu . The dissimilarity - consensus approach to agreement analysis in gesture elicitation studies . In Pro - ceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , page 1 – 13 , New York , NY , USA , 2019 . Association for Computing Machinery . [ 46 ] Radu - Daniel Vatavu and Jacob O . Wobbrock . Formalizing agreement analysis for elicitation studies : New measures , signiﬁcance test , and toolkit . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems , CHI ’15 , page 1325 – 1334 , New York , NY , USA , 2015 . Association for Computing Machinery . [ 47 ] Santiago Villarreal - Narvaez , Jean Vanderdonckt , Radu - Daniel Vatavu , and Jacob O . Wobbrock . A systematic re - view of gesture elicitation studies : What can we learn from 216 studies ? In Proceedings of the 2020 ACM Designing In - teractive Systems Conference , DIS ’20 , page 855 – 872 , New York , NY , USA , 2020 . Association for Computing Machin - ery . [ 48 ] Saiwen Wang , Jie Song , Jaime Lien , Ivan Poupyrev , and Ot - mar Hilliges . Interacting with soli : Exploring ﬁne - grained dynamic gesture recognition in the radio - frequency spec - trum . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology , UIST ’16 , page 851 – 860 , New York , NY , USA , 2016 . Association for Com - puting Machinery . [ 49 ] Laurenz Wiskott and Terrence J Sejnowski . Slow feature analysis : Unsupervised learning of invariances . Neural com - putation , 14 ( 4 ) : 715 – 770 , 2002 . [ 50 ] Jacob O . Wobbrock , Meredith Ringel Morris , and Andrew D . Wilson . User - deﬁned gestures for surface computing . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’09 , page 1083 – 1092 , New York , NY , USA , 2009 . Association for Computing Machinery . [ 51 ] Gareth Young , Hamish Milne , Daniel Grifﬁths , Elliot Pad - ﬁeld , Robert Blenkinsopp , and Orestis Georgiou . Design - ing mid - air haptic gesture controlled user interfaces for cars . Proceedings of the ACM on Human - Computer Interaction , 4 ( EICS ) : 1 – 23 , Jun 2020 . [ 52 ] Friedemann Zenke and Surya Ganguli . Superspike : Super - vised learning in multi - layer spiking neural networks . arXiv preprint arXiv : 1705 . 11146 , 2017 . [ 53 ] F . Zenke and E . O . Neftci . Brain - inspired learning on neu - romorphic substrates . Proceedings of the IEEE , pages 1 – 16 , 2021 . [ 54 ] Z . Zhang . Microsoft kinect sensor and its effect . IEEE Mul - tiMedia , 19 ( 2 ) : 4 – 10 , 2012 .