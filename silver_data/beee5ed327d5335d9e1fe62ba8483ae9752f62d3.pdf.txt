Participatory Evaluation with Autistic Children Katharina Spiel 1 , Laura Malinverni 2 , Judith Good 3 , Christopher Frauenberger 1 , 3 1 TU Wien ( Vienna University of Technology ) , Austria 2 University Pompeu Fabra , Barcelona , Spain 3 University of Sussex , Brighton , United Kingdom katharina . spiel @ tuwien . ac . at laura . malinverni @ upf . edu j . good @ sussex . ac . uk christopher . frauenberger @ tuwien . ac . at ABSTRACT Participatory Design ( PD ) has become a standard methodology in HCI , however , the evaluation of the outcomes of participa - tory processes is often exclusively driven by researcher deﬁned measures of success . Through our work with autistic children , who have radically different life worlds from our own , it be - came evident that their criteria for the success of a project are most likely also very different . In order to address the limita - tions of researcher deﬁned and led evaluations in this context , we developed an approach for participatory evaluation called PEACE ( Participatory Evaluation with Autistic ChildrEn ) . Us - ing this approach , we were able to include autistic children in dedicated evaluation phases through the co - deﬁnition of goals and methods , joint processes of data gathering and the co - interpretation of results . We discuss three case studies in which we successfully applied our approach and conclude with a reﬂection on the novel insights created through participatory evaluation and researchers’ roles in such a process . ACM Classiﬁcation Keywords H . 5 . m . Information Interfaces and Presentation ( e . g . HCI ) : Miscellaneous Author Keywords Children ; Autism ; Participatory Evaluation INTRODUCTION Participatory Design ( PD ) researchers include users in shap - ing the technologies with which they interact . Often , though , researchers’ goals in PD projects deﬁne the success of the resulting prototypes , as well as the focus of evaluation [ 2 ] . Par - ticipants are reduced to providing data to support researchers’ hypotheses . When the development of technology moves be - yond the pragmatic evaluation of efﬁciency and efﬁcacy , novel questions about what is worth evaluating come to the fore [ 21 ] . Expanding the role of research participants in the evaluation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspeciﬁcpermission and / or a fee . Request permissions from Permissions @ acm . org . CHI 2017 , May 06 - 11 , 2017 , Denver , CO , USA Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 4655 - 9 / 17 / 05â˘A˛e $ 15 . 00 DOI : http : / / dx . doi . org / 10 . 1145 / 3025453 . 3025851 of technology further opens up perspectives beyond the safe space of researchers’ expectations . Action Research , a ﬁeld in the Social Sciences with similar theoretical and moral underpinnings , already includes research participants in evaluation [ 22 ] . It allows for different stances that , when combined , deliver a richer description of agendas and use contexts than the researchers’ perspective could pro - vide on its own . Unfortunately though , participatory evalua - tion ( PE ) has received scant attention within Human Computer Interaction ( HCI ) – and even less when children are involved . When working with autistic children 1 in particular , it is im - portant to acknowledge that they perceive the world in which they live very differently compared to non - autistic , adult re - searchers [ 9 ] . In recent years , an increasing number of projects have involved autistic children in participatory design ( e . g . , [ 36 , 17 , 1 , 34 , 26 ] ) . However , their participation in the evalua - tion phase has been nearly non - existent because it is deemed very difﬁcult to elicit concrete feedback from autistic children [ 15 ] . While communication with autistic children is indeed complex , it may be that a lack of methods offers a better ex - planation for why they have been included in co - design , but not in evaluation . PEACE – our approach to PE with autistic children , addresses this gap and , in addition , offers researchers working with neurodiverse [ 8 ] user groups in general a tool through which to engage them in evaluation processes . After detailing current approaches for participatory evaluation and the unique challenges inherent in working with autistic children , we introduce PEACE , our approach to PE with autis - tic children . Through three case studies in the context of the OutsideTheBox project , we show how it can be successfully applied in very different ways to yield rich situated knowledge . We discuss our insights together with a critical reﬂection on the roles taken by children and formal researchers alike . Fi - nally , we conclude with a view to the development of further versions of this approach . 1 We acknowledge that different individuals on the autism spectrum have different preferences when it comes to person - ﬁrst ( individuals with autism ) vs . identity - ﬁrst ( autistic individuals ) language . How - ever , since there is an indication that identity - ﬁrst is the predominantly self - chosen form ( see [ 27 ] ) , we use it in this paper . PARTICIPATORY EVALUATION Participatory Evaluation has been sporadically used within and outside of HCI . However , participatory evaluation with autistic children requires a different approach to account for their uniquely situated way of perceiving , and making sense of , the world around them . To our knowledge , no framework or approach has so far addressed this issue . History of Participatory Evaluation The ﬁrst conceptualisations of Participatory Evaluation ( PE ) date from the 1960s and 1970s [ 4 ] , with initial applications in economic development [ 19 ] and patient centred health - care [ 7 ] . Three primary characteristics of PE play a role in the process , but can be conﬁgured differently : control of the evaluation ( e . g . , participants – researchers ) , stakeholder selection ( e . g . , individuals – group representatives ) and depth of participation ( e . g . , determining goals and methods – gathering data ) [ 7 ] . PE can be used pragmatically or with a transformative agenda . In the ﬁrst form , PE is implemented because it yields richer knowledge , and results in greater acceptance by participants ; in the second form , participants are invited to use PE as a platform for emancipation and empowerment [ 7 ] . Separating these two agendas makes sense in politically tense environ - ments such as the implementation of policies in economic development contexts , but in technology contexts both agen - das converge – especially with marginalised user groups . There are several issues commonly discussed in PE contexts , among them ethical questions about data ownership and the deﬁnition of technical quality [ 7 ] . Relatedly , power differences between researchers and participants and protocols for interac - tion that aim to overcome these differences are considered in the literature [ 3 ] . Particularly marginalised user groups , such as autistic children , often face multiple power differences . In this speciﬁc case , non - autistic researchers have higher social status , are better equipped to function in a world with neurotyp - ical demands and are not least physically taller , manifesting the power inequality through appearance . It is thus vital to the success of PE to reﬂect on existing power differences and ways in which they can be countered before conducting any participatory research , be it design or evaluation . Participatory Evaluation in HCI Participatory evaluation is not very prominent within HCI . The few cases in which it has been formalised include a cooperative work context [ 37 ] and a trauma resuscitation context [ 31 ] . The ﬁrst context led to the development of the PETRA ( partic - ipatory evaluation through redesign and analysis ) framework , which states that one of the beneﬁts of participatory evaluation within technological work settings is that viewpoints of theory - driven evaluators and design - based participants can be com - bined . In this way , evaluators gain access to the participants’ perspective , and vice versa [ 37 ] . However , the framework focuses on the evaluators’ goals and methods , and reduces the participatory aspect to the execution of the methodological ap - proach , through which evaluators and participants co - construct meaning about the technology . In the end , researchers perform the ﬁnal analysis by themselves . In the second context , trauma resuscitation , initial research by Kusanaki and Saracenic suggests using participatory design methods for participatory evaluation [ 30 ] , effectively merging phases of enquiry into the use context , design and evaluation [ 31 ] . They describe applications of their framework to projects where participatory evaluation could function as a way to include users who cannot or do not want to commit to a fully ﬂedged participatory design process . To date , however , they have not published a ﬁnal version of their framework . Bossen et al . [ 2 ] propose seven questions for researchers con - ducting evaluation in participatory design processes : what is the purpose of the evaluation , who is conducting it , who is par - ticipating in it , who has the power to deﬁne evaluation criteria , which methods are used , who is the intended audience and what is expected of the evaluation . In participatory evaluation , these questions are not only asked of researchers , but mutually decided upon by research participants and formal researchers . What is currently lacking , however , is a methodology that ties together both the participatory design and participatory evaluation aspects of a project in such a way that the same participants can co - construct the evaluation of a technology in a way that is meaningful to them . Participatory Evaluation with Children Involving children in PE , poses several challenges : it is impor - tant that a child can be met at the level of their abilities so that they can contribute in a way which is meaningful to them . Par - ticipating children must be able to express themselves , while researchers must attempt to ensure that they understand what has been expressed in a contextual way ; much in the tradition of listening as ‘an active process of communication involving hearing , interpreting and constructing meanings , not limited to the spoken word’ [ 5 ] . This can be achieved through observa - tion , interviews , questionnaires , structured activities ( such as role - play with dolls / puppets or game activities ) , multi - sensory explorations [ 5 ] , analysing children’s photographs [ 13 ] , inter - views guided by these photographs [ 25 ] , drawings / paintings [ 16 ] , photo / video tours or journals [ 11 ] . Children already have different roles in technology research . Druin describes these roles as ‘user’ , ‘tester’ , ‘informant’ and ‘design partner’ , implicitly indicating that this order also fol - lows an order of participation and agency [ 12 ] . Participation along this categorisation seems to be limited to design phases only , however . As soon as technology is developed , chil - dren are relegated to the role of testers without any agency in deﬁning what makes a technology successful and desirable or , conversely , a failure . Initial approaches to participatory evaluation with neurotypi - cally developing children fall back on a combination of heuris - tic evaluation tied into participatory design methods , which inherently have predeﬁned goals and methods [ 42 ] . Hence , meaning is again constructed primarily by researchers through the input the children deliver , without the children being able to actively intervene or put their own interests forward . Overall , a range of methods exist for accessing children’s opinions , but none for structurally inviting them to participate directly in decisions about the goals and methods of an evalua - tion . They do not decide where , or how , to collect data , and their individual interests around the evaluation of a technology are not taken into consideration . Through more participatory approaches , children could be encouraged to reﬂect on the data acquired ( be it quantitative or qualitative , from others or from themselves ) . Furthermore , given that they are also part of the user group , such an approach would ultimately yield additional knowledge about what they deem important . EVALUATING TOGETHER WITH AUTISTIC CHILDREN Most technologies for autistic children are grounded in an interventionist perspective ( see [ 28 ] for an overview ) . As a consequence , their evaluation has generally been framed around the researchers’ goals of functionally understanding their effectiveness with respect to behavioural outcomes . In both the development of such technologies and their evalua - tion , the perspective of autistic children becomes secondary . When co - creating technologies with autistic children , it be - came very apparent to us how necessary it is to include them in evaluation procedures from the start . Starting with the deﬁnition of the goals of the evaluation , autis - tic children may already challenge researchers’ pre - conceived expectations in terms of the purpose and evaluation criteria as well as the intended audience and the required methods . In classical researcher - driven evaluation , the selection of methods is generally derived from a combination of the research ques - tions and the epistemological stance of the researchers ( see [ 20 ] ) . However , when working in PE it becomes important to decide on methods based on the abilities of the participants , and with a view to ensuring that the resulting data are mean - ingful to all involved . By separating the deﬁnition of goals of participatory evaluation from the methods , we separate the questions of what is evaluated from how it is evaluated . Both parts inherit different aspects of meaning making , agency and participation – as we will detail in PEACE below . When conducting participatory evaluation with autistic chil - dren , co - deﬁning what to evaluate can be challenging due to the abstract nature of the task . In order to make it possible for the children to meaningfully participate in the evaluation pro - cedure , some of these questions have to be asked via concrete illustrations . For example , when discussing how to ﬁnd out more about the broader desirability of a technology , providing details of who those ‘others’ might be , and relating them to populations familiar to the child , such as classmates or ‘chil - dren your age’ , helps them to process the abstract notion of the appeal of a technology . Certain goals implicitly or explicitly include or exclude certain methods . If we want to know whether other people like a tech - nology , or how useful it might be beyond its intended use case , observations of use are not sufﬁcient . Similarly , if the goal is to better understand the reliability of a technology , it is less useful to interview others about the look and feel . However , each evaluation goal comes with a set of choices regarding data acquisition methods . If a method fails to address the way in which an autistic child makes sense of their environment , the participatory evaluation will be less successful . Autism & Sense Making Autism is a spectrum condition with a plethora of symptoms which can differ greatly for individuals with the same diagno - sis . While the cause remains unclear , it is likely a combination of environmental and genetic factors [ 14 ] . Symptoms can include differences in reciprocal socio - communicative inter - action with non - autistic people as well as repetitive interests and behaviours , which are also the main diagnostic criteria . As awareness increases , the condition is being recognised and subsequently diagnosed more frequently , and it is estimated that about 1 in 68 children are on the spectrum [ 38 ] . Autistic individuals experience the world around them differ - ently and make sense of it in a different way [ 9 ] . The strategies that autistic individuals have for dealing with heightened sen - sory input include repetitive behaviours and self - stimulatory actions ( also known as ‘stimming’ ) . Repetitive communica - tion is often meaningful for an autistic individual , but not necessarily in a way a non - autistic person might expect . For example , the repetition of a fact like ’the door is open’ can indicate distress ( e . g . , ‘I want it closed’ ) , worry ( e . g . , ‘What if the cat runs out ? ’ ) , something that pleases them and they want to share , or something that allows them to block out other sen - sory inputs in a stressful environment . Whenever non - autistic people interpret autistic behaviour and communication , this has to be done very carefully , and in a context dependent man - ner . Consequently , non - autistic researchers often shy away from explicitly including autistic modes of communication , since it is very difﬁcult to ensure that expressions are inter - preted and handled appropriately . This is even more true in the case of autistic children , as non - autistic researchers have several modes of relative power in societal hierarchies ( such as age , social status , attributed agency and so on ) . Insights from Participatory Design Given that participatory evaluation has rarely been explored within HCI , we lack proven methods , particularly in the con - text of marginalised user groups such as autistic children . For - tunately , in the past few years , several projects have explored the participatory design space opened up by the collaboration with autistic children . The expertise which has developed around how to plan interactive sessions with autistic children and make their contributions count is invaluable when concep - tualising participatory evaluation with them . The IDEAS framework [ 1 ] emphasises structural features of participatory design sessions to involve autistic children . It is – to our knowledge – the most detailed and prominent approach for PD with autistic children . IDEAS consists of a set of seven guidelines which we have adapted for participatory evaluation with autistic children . We have retained the headlines , but re - situated their meaning to an evaluation context : 1 . Concept of Meaning : ensure that children have a meaningful understanding of the goals and methods of an evaluation ; 2 . Distractibility : adapt to the child’s hobbies and interests ; include them when framing questions or offering methods ; 3 . Concrete vs . Abstract Thinking : present options clearly and unambiguously ; favour contextual closed questions over abstract ones [ 15 ] ; Stage Child Researcher Setting Goals and Methods Articulating goals Offering different ways in which to discuss potential goals and methods Determining ﬁnal goal rankings and methods for answering research questions Gathering Data Taking the lead on gathering data Prompting and supporting the child Coordinating data gathering Interpreting Results Interpreting data Pre - processing raw data Discussing all interpretations of data Table 1 . Stages in PEACE together with children’s and researchers’ contributions alike . 4 . Organising and Sequencing / Visual vs . Auditory Learning : provide visual identiﬁers for available options , e . g . , goals or methods ; 5 . Excessive Anxiety / Prompt Dependence : always have a set of alternatives when planning to engage with the child ; 6 . Strong Impulses : build on the abilities of the child and refrain from demanding modes of interaction they dislike ; 7 . Involve several individuals when conducting sessions in order to ﬂexibly provide assistance . Additionally , providing a delicate balance between freedom and structure is essential when working with autistic children [ 32 ] . One way to achieve this is by providing a narrative frame in a freely explorable space [ 34 ] , such that children are guided in their interaction by a background story which they can ad - here to in their interaction or not . Malinverni et al . [ 33 ] also structured the evaluation activities of children , but left space for individually guided exploration of the technology being evaluated . By focusing their evaluation on aspects like the ‘children’s focus of attention’ they allow for implicit participa - tory evaluation that is guided by how the children attend to the technology and what they ﬁnd interesting about it . Ultimately , even methods designed to reﬂect on activities such as design exposés [ 17 ] can be useful in mapping out the potential evalua - tion space for and with autistic children . All of these concepts were inﬂuential in the development of our own approach to participatory evaluation with autistic children – PEACE . PEACE – PARTICIPATORY EVALUATION WITH AUTISTIC CHILDREN The PEACE framework comprises three stages , which roughly follow the PE phases identiﬁed in the principles of community engagement described in [ 35 ] . In our version the ‘planning’ stage became setting goals and methods , the ‘implementation’ stage became gathering data , while the ‘completion’ and ‘dis - semination’ stages have been combined in interpreting the results . Throughout the description we reference the individ - ual guidelines from the IDEAS framework [ 1 ] , describing how they are relevant to a particular stage . Table 1 details these stages and the individual contributions of children and researchers alike . Setting Goals and Methods Since participatory evaluation is a process of collaborative meaning making , researchers also bring their evaluation goals to the table and are able to negotiate them with a child’s ideas of evaluation . Contributions from researchers and children are addressed in the following way : • Researchers’ perspectives are part of the evaluation only if they make sense to the child as well ; • Children’s perspectives are only disregarded if there is no way to answer them . No goal gets dropped , rather , re - searchers work to determine the intent behind the goal and then re - frame the question . Following these principles , the child’s perspective is privi - leged in order to counteract the traditional power dynamics of research projects . Researchers have the task of interpreting the child’s communicative acts in a mindful way . Sometimes , goals might be hidden in a drawing or in the particular way that the child interacts with prototypes or classmates . Previ - ously conducted participatory design sessions can also provide an indication of the evaluation goals that are meaningful to the children . These goals should be ranked by researchers and children together in order to establish which questions deﬁnitely need to be answered . This ranking of goals in order of mutual priority also allows the child to withdraw from the evaluation at the point where they feel satisﬁed that their goals have been met . Given that many autistic children ﬁnd it difﬁcult to think in the abstract , questions about the goals of an evaluation should be grounded in the concrete . Goals can be identiﬁed through questions such as : “What does my family think about the technology ? ” , “Is it still fun to interact with the technology after two months ? ” or “Could this technology be a commercial success ? ” 2 . If possible , methods should be adapted and re - framed so that the child feels competent in conducting part of the data gather - ing . Often , it helps to take a look at the hobbies or interests a child has and go from there . Not being able to adjust methods in a way that allows the child to participate in data gathering runs the risk that the child will feel and be less involved in the evaluation as a whole . Gathering Data The child can gather data on their own ( e . g . , recording the number of times they interacted with the object ) or together with the researcher ( e . g . , interviewing other people ) . Often an evaluation has several goals , which are addressed by different 2 While the latter is technically an extremely abstract question , in our experience the concept has been used as a stand - in for the desirability of a technology . methods . Researchers should ensure that there are several options within a method available to the child . For exam - ple , if it’s important to know what others think , researchers could suggest interviews or questionnaires . Within that there are different questions to specify and also different ways to conduct them ( e . g . , on the street with strangers or at home with family or anything between these two poles ) . Choices should be presented in a sequenced order so that the child is not overwhelmed . Researchers can assist the child in carrying out the chosen method ( s ) by giving prompts and encouraging the child to go to the next step , e . g . , the next question in an interview . If researchers are not present , they can give support and feedback on data gathering in evaluation meetings . Interpreting Results While some children might enjoy processing and interpreting the raw data , data should in most cases be pre - processed by researchers , and in a way which acknowledges individual chil - dren’s preferences in terms of , for example , visual or auditory modalities . Even though every processing step adds a level of interpretation to the data , it also helps create opportuni - ties for discussion with the child . With pre - processed data , and options for alternative interpretations , a child can make qualiﬁed judgements and challenge the presented research per - spective on the data . For example , visualising which questions in interviews attracted the most attention ( e . g . , by length of answer ) or at which times a technology was used and then debating the importance of that information gives the child the opportunity to acknowledge different angles from which to see their technology , but also gives the researchers insight into what the child deems important to know about a ( co - designed ) technology , and why . This step might , again , necessitate a non - verbal mode of communication , such as a Picture Exchange System , sign language , or spatial positioning and interpreted behaviour / reactions towards the pre - processed data . CASE STUDIES Within OutsideTheBox 3 we co - design technologies with autis - tic children that target their holistic well - being . The resulting technological artifacts range from small tokens to large body interaction devices . Over the span of at least one full school year , we met each child bi - weekly for an hour long session , usually at their school . The main purpose of the technolo - gies we individually develop with the children is that they make sense in their lives and enable them to share the positive experiences they have with those technologies . Hence , our evaluation focuses on this high level goal from the researchers’ perspective in all cases . In the ﬁrst year we attempted to evaluate outcomes in a more traditional manner in the sense that the goals and methods were decided solely by the research team . Data gathering processes were difﬁcult and often unfruitful , because while they matched the families’ abilities , they placed too high a cognitive load on them ( e . g . , remembering to ﬁll out a questionnaire at regular intervals over a certain period of time ) [ 40 ] . The three cases presented here are from the second year of OutsideTheBox , 3 http : / / outsidethebox . at during which we attempted participatory evaluation with three different children : Quentin , Mia and Yvan 4 . Quentin When we ﬁrst met Quentin , he was nine years old and went to a mainstream school . Our collaboration spanned 16 months and 15 meetings overall , with large breaks in between due to summer vacation periods . Quentin was diagnosed with Aspergers when he was in pre - school . Tinkering and crafting were well - loved activities , but only for the purpose of creating a ﬁnished object that has a use ( even if it is not necessarily obvious to outsiders ) . Figure 1 . Sound Cubes – developed together with Quentin After a phase of Contextual Inquiry [ 23 ] , we decided to use Digital Fabrication [ 18 ] as our design generating method . On our design journey , we mostly met in an empty class - room , although two sessions were conducted at the university , where 3D - printers , a laser printer , a CNC machine and sev - eral smaller fabrication tools are available . Inspired by the potential of these machines and a prototype from a different research project 5 , we dove into the development of Sound Cubes ( see Figure 1 ) . The Sound Cubes were realised as a pair , however it is tech - nically feasible to create additional cubes such that any cube could function with any other . The cubes can record a sound message , replay it or move it to another cube . Every cube can also receive messages from any other and play these . Each side of the cube is dedicated to a different function : one for the speakers , one for the microphone and recording , one for message replay , one for receiving messages , one for dropping a message ( via direct contact ) and ﬁnally , one to place the cube on . The last surface can also be used to individualise 4 All names have been changed . 5 http : / / igw . tuwien . ac . at / sparkling the cube so that e . g . , each family member has their own , or opened to tinker with the technical components of the cube . Setting Goals and Methods Throughout our collaboration , Quentin was constantly making sure that he performed well . He asked about the quality of his work and – because he was quite aware of the research project and its context , and knew of other children we worked with – asked how his work related to that of others . Whenever he asked for feedback from his classmates , he presented what he had constructed in that session , but did not let them interact with the objects . These utterances and observations made it clear to us that it was important for Quentin that the cubes be desirable to others as well as functional and reliable . The latter point coincided with our own interest , as we also wanted to know who would use the cubes , and how . Since Quentin was also an enthusiastic member of the ‘Science Club’ at his school , we decided on a data gathering method that could generate numerical data . That way , we could create visuals and data that was expected of a positivist approach , which is the dominant language through which science is represented in popular media . We expected that such data would be most meaningful to Quentin . Additionally , he had his own tablet device , which he liked interacting with . We developed a set of relevant questions together ( see Table 2 ) , which had the added beneﬁt of giving us the opportunity to discuss with Quentin the different types of questions that can be used in questionnaires . Question 7 was particularly important to him : " Others should want the device , but I will be the only one who has it ! " . Our questionnaire platform was QuickTapSurvey , which allowed Quentin to gather data on his own device while also giving the research team access to the questionnaire responses . # Question Type Question 1 Binary Did you use the Cubes yourself ? 2 Number How often did you use them so far ? 3 Ranking What attribute of the Cubes is most important to you ? 4 Scale How well do the Cubes function ? 5 Text What would you change about them ? 6 Mult . Choice What do you least like about them ? 7 Binary Do you want to have a Cube for yourself ? 8 Text Do you have an idea for a different name ? Table 2 . Quentin’s Questionnaire , showing speciﬁc questions and ques - tion type Gathering Data & Interpreting Results Our plan was that Quentin would gather data independently at home using the tablet version of the questionnaire and , if possible , gather more responses together with us at school . Unfortunately , there were strict rules imposed on using pri - vate tablets on school grounds , which rendered the second endeavour impossible . Additionally , Quentin did not collect any responses himself . From this , we understood that there were two problems with our approach . While Quentin wanted to do science related activities , they focused on constructing physical objects rather than the general ideas behind scientiﬁc methods . The purpose of gathering answers to the questionnaires was ultimately not tangible enough for him to ﬁnd it interesting . Additionally , it turned out that he did not really use the Sound Cubes , but instead wanted to show his idea and creation to others in order to gather positive afﬁrmation . The evaluation methods we had planned together – even though jointly agreed upon – did not match this desire appropriately . Quentin’s case study illustrated the need to be more concrete and more situated in activities the children already enjoy when conducting participatory evaluation in order to make it fruit - ful and meaningful for everyone involved . Additionally , it encouraged us to reﬂect on the “tyranny of participation” [ 6 ] and how not every participant will be equally interested in every part of the design and evaluation process . However , by creating a space in which Quentin could have more power over outcomes if he chose to do so , we learnt more about the interaction between Quentin and the Sound Cubes . The joint formulation of questions helped us in particular to understand which aspects of the Sound Cubes were important to him . Mia Mia had recently been diagnosed with Autism Spectrum Con - dition when we met her . The nine - year old was aware of her diagnosis , but she and her family were still ﬁguring out what it meant to them and their daily life . Mia’s school acted as a supportive anchor in that it provided dedicated , additional teaching staff for her class on some days during the week . She loves everything related to the Super Mario games , with Toad and Yoshi being her favourite characters . She also likes playing outdoor sports and drawing , but dislikes handwriting . Using the semantics of a Super Mario game world , we used the - atre methods [ 39 ] and augmented them with playful elements to learn more about Mia’s life context [ 41 ] . We established that she ﬁnds getting up in the morning extremely annoying and difﬁcult – so difﬁcult , in fact , that already during our sec - ond of 16 meetings , she suggested that we create a cushion that wakes her up by vibrating next to her instead of the dis - turbing sound made by her then - current alarm clock . Once we understood how important ﬁrst impressions of a day are in creating a good mood , we also understood the potential positive impact of such a technology . The Rattle Alarm System ( see Figure 2 ) consists of three parts . At the core , there is an alarm clock module – aesthetically modelled after Toad’s head – which displays the current time through blue lights on a NeoPixel light ring with 24 LEDs . The alarm time can be set through a light touch on top of the module and is displayed with a green light . When the alarm goes off , the Super Mario theme song plays in an endless loop and the cushion vibrates . The alarm can be stopped by getting up and stepping on the pressure mat , which functions as an off - switch . Shortly after the alarm turned off , the clock plays a little melody – different every day – which sets the mood of the day as a stand - in for a horoscope . Figure 2 . The Rattle Alarm System – Developed together with Mia – left : cushion , middle : alarm clock , right : pressure mat Setting Goals and Methods Most of our sessions with Mia took place at her school in an empty classroom adjacent to her own classroom . After each session , she had the possibility to show what she had accomplished that day to her teacher and afterwards to selected friends . During one of those after - sessions in class , one of her friends told us that Mia occasionally distributes her own newspaper with news for the class ( albeit with not much text written in them ) . It was deemed desirable to belong to the group of people who were allowed to read the newspaper . Figure 3 . Announcement of the Rattle Cushion Alarm System In order to co - establish goals for the evaluation , we asked Mia to create an advert of her technology that focuses on desired effects and how mornings are shaped with the Rattle Alarm System . Figure 3 shows the core part of her ﬁnished advert . All parts of the system are depicted with their functionality : the clock plays Super Mario music , the cushion rattles and the mat gives a horoscope . The text below reads ( translated ) : " The way to get up happy ! " . In addition to giving us an idea about her own mental model of the functionality ( e . g . , placing the melody - as - horoscope functionality on the mat instead of the clock ) , the advert also tells us that one of her goals in the evaluations is to ﬁnd out whether the system has a positive impact on her morning routine . The research team was interested in how the technology could be truly integrated in Mia’s life . Methodologically , we created a pool of methods with which we could answer these questions – all of them newspaper related . • A report about how the Rattle Alarm System and who uses it . This would inform us about the elements and contexts of use that are important to Mia . • A review of the Rattle Alarm System with an accompanying rating . This would give us an insight into what works well already and what has to be still improved in order to make the technology robust enough to be part of Mia’s life . The review and report also potentially tell us about how the interaction shapes the morning routine . • Interviews in which Mia asks others about their opinion of the system . The questions can tell us more about Mia’s interests , the answers provide an understanding of others’ interpretation of the system and the choice of interviewees also shows us whose opinion is important to Mia . • An illustration of the system in actual use , which we could then use and compare to the advert to see how the two drawings relate to each other . We also agreed that we would bundle all contributions together to create an issue of her newspaper that focuses solely on the Rattle Alarm System . Gathering Data Mia could either gather data on her own at home or during the sessions with support from a researcher – and she chose both , resulting in a report on the design process and two interviews . The report about the design process was integrated into one of her usual newspaper issues , in which she picked up on daily news with her own take on the European Soccer Cham - pionships and the performance of the Austrian team on the front page as well as comics and jokes on the last page . On the pages in between she detailed the design process within its context of a research project with autistic children . She also made clear that she saw herself as an inventor of something that might be useful to others as well . " In an interview ( sic ! ) , [ Mia ] said : ( . . . ) I can understand that others have similar problems with getting up in the morning . " Several weeks after receiving the prototype to test out at home , we conducted an evaluation oriented meeting . In that meeting , Mia role played the situation in the mornings and gave critical feedback on the sensitivity of the top button , the structure of the cable connecting the mat and the alarm clock as well as the intensity of the vibration of the cushion . She then decided to interview her mother and a co - located researcher . She asked about the quality of the system and about what the interviewees felt should be changed . She also asked how many ’stars’ on a scale of one to ﬁve the system would get , and enquired what they would invent if they were part of a similar project . During both interviews , she had a professional demeanour . One of the researchers prompted her to ask further questions when she appeared to be stuck . This additional structure enabled her to get through all questions without being required to read handwriting – a task she ﬁnds exhausting . Interpreting Results From the interview with her mother , Mia could establish that others indicated a positive inﬂuence of the design process and the technology on her life . During the session , she herself noted that she agreed with that assessment , even though she had so far only used the system occasionally . We were also able to identify necessary improvements that would make the system more robust : a better protected and tighter connection between the alarm clock and the mat , more vibration power in the cushion as well as better distribution of the vibration next to an adjustment of the sensitivity of the top button on the alarm clock . We decided on further meetings to improve those open points . An actionable plan also shows all participants , formal researchers and child alike , what the effects of an evaluation can be and how to act on its results . When asking for the star rating during the interviews , Mia’s mother awarded ﬁve stars for the aesthetics and two for func - tionality ( motivated by the lack of robustness ) . The other interviewee awarded four stars . Afterwards , Mia announced that she now had collected eleven stars and that she wanted to see how many stars she could earn in total . That way , she re - appropriated the interpretation of the rating and gamiﬁed the evaluation process on her own . Additionally , Mia’s report indicated that the process and resulting prototype were having a positive and afﬁrmative effect on her life . From working with Mia , we learnt that data gathering methods can be interesting to a child , if framed correctly and if the child can assign a potential utility to the activity . Even if researchers and the child differ in their interpretation of a method – as seen with the stars – the child’s interpretation is more important and more indicative of their own perspective on the technology to evaluate , but also the evaluation process as such . Yvan All things related to planets and space travel are of great in - terest to Yvan . The eight - year - old , diagnosed with Autism Spectrum Condition , talked at length about these topics when - ever he could ; not always considering whether his audience was actually interested in listening . At the beginning of our cooperation , which , to date , spans 13 sessions , Yvan was edu - cated in a special needs class , but transitioned into a multi - age classroom later . At the ﬁrst couple of design workshops we conducted Con - textual Inquiry [ 23 ] . That way , we not only learnt more about Yvan’s core interests , but also how important his ﬁve year - old brother Hank is to him . During our ideation phases he con - stantly envisioned his brother in potential use contexts . Once we settled on the idea of a Time Machine , with which we could travel through time and space to different temporal stages and different planets , we explored the actualisation of this idea through means of Digital Fabrication [ 18 ] . While the Time Machine initially had the form of a pyramid and later followed the concept of a travel tower , we eventually decided that it would consist of two parts : an immersive light blanket and a navigation interface ( see also , Figure 4 ) . Figure 4 . Time Machine – developed together with Yvan – foreground : navigation interface , background : immersive light blanket While the technological parts of the Time Machine are compar - atively simple , the smartness of the technology emerges in use . Through the navigation interface , a user can control different light patterns on the blanket . They only become meaningful through the narrative established between users . Yvan then tells elaborate stories in which he travels to different planets at different points in time . Once he ‘lands’ , he steps out of the machine and grabs different things in the environment , but gives them a different meaning , appropriate to the time and place he travelled to . This type of pretend play is notori - ously difﬁcult for autistic children [ 24 ] . However , the Time Machine introduces just enough structure for Yvan to do so cooperatively . Another effect of the Time Machine is that it becomes a productive release for Yvan’s specialised knowl - edge that engages the other person as well . They experience the immersive space together and can both shape the narrative . The specialised knowledge becomes part of a joint adventure instead of a one - sided lecture . Setting Goals and Methods Next to the evaluation goal – established through our collabo - ration – that the Time Machine should be enjoyable for Hank as well , we asked Yvan directly , what would deﬁne success for his technology , since he was using the concept himself previously . He stated that others should deem the Time Ma - chine as cool and also , that it should be really fast . Since the speed of the time machine is virtually constructed by the given story context , we re - interpreted that goal and wanted to ﬁnd out whether the Time Machine suitably supports immersive narratives for Yvan . Assessing the speed of the Time Machine remained part of the evaluation as a goal by itself . Methodologically , we established a TV show called Research with Yvan , in which he presented the Time Machine and re - ported on trips undertaken with it . This was requested by Yvan himself , who was continuously fascinated by the camera which recorded all of our sessions together . We also set up a showcase of the Time Machine in Yvan’s classroom and – somewhat impromptu in an evaluation session together with Hank – let both brothers create a drawing of the use context of the Time Machine . Gathering Data All data gathering was done jointly with the researchers . The TV show was ﬁlmed during a latter design session in order to evaluate how well the blanket idea and initial prototypes matched Yvan’s expectations . He animatedly reported about a trip to Saturn and Jupiter that he had just undertaken with one of the researchers , detailing especially how fast the Time Machine was and what kinds of things they found on these planets during their trip . When Yvan received the ﬁrst fully functional prototype , we also showcased it in class . He presented photos from the design process and led some children under the blanket to take a trip to different planets . Unfortunately , a soldering seam became loose and the lights did not work properly . Many children were confused about what exactly was happening , although some were open to the frame of pretend play and talked about their own perspective on the time travel . Figure 5 . Drawing by both users of the Time Machine on how they inter - act with it – real name initials have been smudged After several weeks in which they used the Time Machine in a home setting , Yvan and Hank created the drawing depicted in Figure 5 . It shows both of them ( drawn in each of their favourite colours with size following age ) under a fully illu - minated blanket with two different navigation boards , even though only one exists . On the upper left , there is an alien they encounter on the planets they visit ( depicted at the top ) . Interpreting Results Yvan took the opportunity to reﬂect on the perceived speed of the Time Machine himself during the recording of Research with Yvan . He enthusiastically established that the machine was , indeed , really fast . Since he is the only person who can judge this , his opinion is the most important to consider when trying to answer this question . The research team also inferred from this that immersion was successfully supported . Due to the malfunction of the blanket in the showcase , only a couple of classmates were enthusiastic about the invention . Yvan ( and the formal researchers ) had hoped for more suc - cess . While he was initially very excited about presenting his technology , he did not want to do another showcase once the short circuit was ﬁxed . The research team would have liked to investigate this question further , however , it was answered for Yvan , therefore no further enquiries were undertaken . Hank expressed being very happy with the Time Machine . While Yvan and Hank were drawing Figure 5 , Hank said that he would like to play even more with it , but Yvan did not want to play as often . Yvan also reported that whenever they went on time travel , they did so together ( which has also been conﬁrmed by their parents ) . From this , we inferred that Hank enjoyed the Time Machine as much or even more than Yvan . DISCUSSION PEACE achieved what the original concept of PE set out to do : involve the primary stakeholders of an intervention or design directly in assessing its effect . Through this , we created a space in which researchers acknowledge the children’s agency and meaningful interpretations of technologies and the ways in which they can be assessed . Pragmatically , this provides us with insights that were previously unattainable . Autistic children had control of the evaluation , and were individually involved and deeply engaged in conducting the evaluation . As such , all three characteristics [ 7 ] of PE are present . PEACE could allow for a range of use contexts and is adaptable for other user groups as well . Each speciﬁc case provided us with unique insights which we could not have obtained without participatory evaluation . From Quentin , we learnt that methods must be chosen in tune with activities that the child likes . Mia showed us the neces - sity of carefully managing structures and freedoms to create a space in which she could perform evaluation . Finally , Yvan challenged our pre - conceived notion of the team always being able to reach agreement by interpreting a one - time failure as absolute whereas the researchers still saw it as an open ques - tion – and how researchers have to take their own agenda back in order to not accidentally override a child’s interpretation . In addition to these individual insights , we also learnt what is important to autistic children across these case studies . We discuss this as well as the roles and responsibilities of everyone involved in PEACE in this section . Insights from Case Studies Across the case studies , we understood that the success of a PE endeavour – as of any participatory project – relies on the interest that participants have in undertaking the action . Some participants are more interested in creating the outcomes of a participatory design project and do not feel the need to enquire more formally about the process or outcomes . Furthermore , these cases showed us that it is important to understand that participants’ goals do not necessarily have to be interesting to formal researchers or make sense to them . However , for participatory evaluation to earn its name , it is vital that we take these goals seriously , assign proper methods to evaluate them and create results that are meaningful to all . Those results should be reﬂected on together with the participant in order to ensure that they see their questions answered , especially if researchers interpret a goal differently . When data is created , the same data can be interpreted differ - ently . We saw this with the stars assigned to the Rattle Alarm System , where they became part of something that had to be earned in sum , or when the research team attributed others’ lack of enthusiasm to a system failure , whereas the child was acknowledging a more generic failure and was no longer in - terested in further enquiring into that evaluation goal . Both interpretations must be seen as equally valid , with action being taken based on the child’s agreement and interest , not solely motivated by researchers’ agendas . It was interesting to us to see that all children included some form of external validation within the evaluation procedure . They wanted their objects to be desirable to others as well , be it out of envy ( Quentin ) , a desire to be meaningful for more people ( Mia ) or to make sure that a speciﬁc person beneﬁts from the design as well ( Yvan ) . Roles & Responsibilities When we started conceptualising PEACE , we thought care - fully about the roles that researchers might take in Partici - patory Evaluation processes . After all , one could argue that the reason why researchers take over the evaluation of PD processes is because they possess the necessary skills to do so . In PEACE , researchers facilitate the evaluation process and encourage autistic children to explore angles of their work they might not have explored otherwise . Through appropriate framing and structuring , they provide space for the children to answer the questions that interest them . In order to suitably support the child , researchers must not only know the child’s interests and preferred activities well , they must also have a wide pool of evaluation methods from which they can draw . They choose a set of potentially useful methods , introduce them to the child and agree the ﬁnal ones together . They must also have additional methods at the ready that they can ﬂexibly switch to if a child shows decreased interest in the established one . Researchers’ skills , competency and experi - ence , together with the child’s interests , focus and questions , are the ingredients of a successful participatory evaluation . In some cases , researchers may have additional research ques - tions which they are trying to answer that might be unimpor - tant or meaningless to the child . While these questions cannot be addressed within the PEACE framework , we encourage researchers to augment the process with established modes of evaluation that can yield different types of knowledge . What PEACE offers , however , is a previously unexplored perspec - tive on the experiences of autistic children with technologies : their own . CONCLUSION In this paper , we motivated participatory evaluation with autis - tic children and reﬂected on the concept in its historical context as well as considering how it has been previously discussed in HCI and with children . After presenting the parameters and particularities involved in participatory evaluation with autistic children , we presented the PEACE framework , which comprises three essential steps : deﬁning goals and methods , gathering data and interpreting the results – all done in a par - ticipatory and evolving fashion . Three case studies provided us with insights into potential pitfalls , as well as insights that can be gained from unexpected outcomes , even if particular steps within the evaluation are not carried out according to plan . Finally , we discussed the implications for participatory evaluation across these case studies , and considered the role of researchers in such a process . Despite all this , further work is needed in order to establish PEACE as a fully ﬂedged research tool . For one , it would be useful to try out a range of different methods in order to create a methodological tool - set that is available to researchers . We also have yet to see how concepts from Participatory Design methods , such as narrative framing [ 10 ] , temporal displace - ment [ 43 ] or the metaphor of magic [ 29 ] might transfer to participatory evaluation . Finally , our approach requires inti - mate knowledge of one child’s abilities , interests and favourite activities . It is not yet clear how this transfers to a group of children , particularly a group with mixed abilities . What we have delivered here is a proof of concept that it is possible to include autistic children actively in the evalua - tion of the technologies they co - design . PEACE is the ﬁrst framework that allows researchers to conduct participatory evaluation with autistic children in a way that appropriately acknowledges their agency , needs and abilities . ACKNOWLEDGEMENTS This research is funded by the Austrian Science Fund ( FWF ) : [ P26281 - N23 ] “OutsideTheBox - Rethinking Assistive Tech - nologies with Children with Autism” project . Our deepest gratitude to Julia Makhaeva for being part of this research endeavour . Geraldine Fitzpatrick , Oezge Subasi , Matthias Wunsch , Francisco Nunez and anonymous reviewers helped reﬁne earlier versions of this paper . Kearsley Schieder - Wethy assisted in proof reading and grammatical corrections . We would also like to thank our participating families , schools and the local government , in particular the department for inclusion in public education which facilitated access to our participants . Above all , we thank the children who continue to inspire our work . REFERENCES 1 . Laura Benton , Hilary Johnson , Emma Ashwin , Mark Brosnan , and Beate Grawemeyer . 2012 . Developing IDEAS : Supporting Children with Autism Within a Participatory Design Team . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , New York , NY , USA , 2599 – 2608 . DOI : http : / / dx . doi . org / 10 . 1145 / 2207676 . 2208650 2 . Claus Bossen , Christian Dindler , and Ole Sejer Iversen . 2016 . Evaluation in Participatory Design : A Literature Survey . In Proceedings of the 14th Participatory Design Conference : Full Papers - Volume 1 ( PDC ’16 ) . ACM , New York , NY , USA , 151 – 160 . DOI : http : / / dx . doi . org / 10 . 1145 / 2940299 . 2940303 3 . Tone Bratteteig and Ina Wagner . 2012 . Disentangling power and decision - making in participatory design . In Proceedings of the 12th Participatory Design Conference : Research Papers - Volume 1 ( PDC ’12 ) . ACM , Roskilde , Denmark , 41 – 50 . DOI : http : / / dx . doi . org / 10 . 1145 / 2347635 . 2347642 4 . Sharon Brisolara . 1998 . The history of participatory evaluation and current debates in the ﬁeld . New Directions for Evaluation 1998 , 80 ( 1998 ) , 25 – 41 . DOI : http : / / dx . doi . org / 10 . 1002 / ev . 1115 5 . Alison Clark * . 2005 . Listening to and involving young children : a review of research and practice . Early Child Development and Care 175 , 6 ( 2005 ) , 489 – 505 . DOI : http : / / dx . doi . org / 10 . 1080 / 03004430500131288 6 . Bill Cooke and Uma Kothari . 2001 . Participation : The new Tyranny ? Zed Books , London , UK . 7 . J . Bradley Cousins and Elizabeth Whitmore . 1998 . Framing participatory evaluation . New Directions for Evaluation 1998 , 80 ( 1998 ) , 5 – 23 . DOI : http : / / dx . doi . org / 10 . 1002 / ev . 1114 8 . Nick S . Dalton . 2013 . Neurodiversity HCI . interactions 20 , 2 ( March 2013 ) , 72 – 75 . DOI : http : / / dx . doi . org / 10 . 1145 / 2427076 . 2427091 9 . Hanne De Jaegher . 2013 . Embodiment and sense - making in autism . Frontiers in Integrative Neuroscience 7 ( 2013 ) , 15 . DOI : http : / / dx . doi . org / 10 . 3389 / fnint . 2013 . 00015 10 . Christian Dindler and Ole Sejer Iversen . 2007 . Fictional Inquiry - design collaboration in a shared narrative space . CoDesign 3 , 4 ( Dec . 2007 ) , 213 – 234 . DOI : http : / / dx . doi . org / doi : 10 . 1080 / 15710880701500187 11 . Sue Dockett , Sarah Main , and Lynda Kelly . 2011 . Consulting Young Children : Experiences from a Museum . Visitor Studies 14 , 1 ( April 2011 ) , 13 – 33 . DOI : http : / / dx . doi . org / 10 . 1080 / 10645578 . 2011 . 557626 12 . Allison Druin . 2002 . The Role of Children in the Design of New Technology . Behaviour and Information Technology 21 , 1 ( 2002 ) , 1 – 25 . http : / / hcil . cs . umd . edu / trs / 99 - 23 / 99 - 23 . html 13 . Johanna Einarsdottir . 2005 . Playschool in pictures : children’s photographs as a research method . Early Child Development and Care 175 , 6 ( 2005 ) , 523 – 541 . DOI : http : / / dx . doi . org / 10 . 1080 / 03004430500131320 14 . Marc Fakhoury . 2015 . Autistic spectrum disorders : A review of clinical features , theories and diagnosis . International Journal of Developmental Neuroscience 43 ( June 2015 ) , 70 – 77 . DOI : http : / / dx . doi . org / 10 . 1016 / j . ijdevneu . 2015 . 04 . 003 15 . Christopher Frauenberger , Judith Good , Alyssa Alcorn , and Helen Pain . 2013 . Conversing Through and About Technologies : Design Critique as an Opportunity to Engage Children with Autism and Broaden Research ( er ) Perspectives . International Journal of Child - Computer Interaction 1 , 2 ( May 2013 ) , 38 – 49 . DOI : http : / / dx . doi . org / 10 . 1016 / j . ijcci . 2013 . 02 . 001 16 . Christopher Frauenberger , Judith Good , and Wendy Keay - Bright . 2010 . Phenomenology , a framework for participatory design . In Proceedings of the 11th Biennial Participatory Design Conference . ACM , ACM , Sydney , Australia , 187 – 190 . 17 . Christopher Frauenberger , Julia Makhaeva , and Katharina Spiel . 2016 . Designing Smart Objects with Autistic Children : Four Design Exposès . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , USA , 130 – 139 . DOI : http : / / dx . doi . org / 10 . 1145 / 2858036 . 2858050 18 . Christopher Frauenberger and Irene Posch . 2014 . Exploring Future Technologies Through Digital Fabrication With Autistic Children . In Participatory Design in Digital Fabrication workshop at FabLearn’14 . FabLearn’14 , Stanford , CA , USA , Article 1 , 4 pages . http : / / outsidethebox . at / files / fabwithasd . pdf 19 . Gila B . Garaway . 1995 . Participatory evaluation . Studies in Educational Evaluation 21 , 1 ( 1995 ) , 85 – 102 . DOI : http : / / dx . doi . org / 10 . 1016 / 0191 - 491X ( 95 ) 00007 - H 20 . Egon G . Guba and Yvonna S . Lincoln . 1994 . Competing Paradigms in Qualitative Research . In Handbook of qualitative research , Norman K . Denzin and Yvonna S . Lincoln ( Eds . ) . Sage Publications , Inc . , London , UK , 105 – 117 . 21 . Steve Harrison , Phoebe Sengers , and Deborah Tatar . 2011 . Making epistemological trouble : Third - paradigm HCI as successor science . Interacting with Computers 23 ( 2011 ) , 385 – 392 . DOI : http : / / dx . doi . org / 10 . 1016 / j . intcom . 2011 . 03 . 005 22 . Gillian R . Hayes . 2011 . The Relationship of Action Research to Human - computer Interaction . ACM Transactions on Computer - Human Interaction 18 , 3 ( 2011 ) , 15 : 1 – 15 : 20 . DOI : http : / / dx . doi . org / 10 . 1145 / 1993060 . 1993065 23 . K . Holtzblatt and S . Jones . 1993 . Contextual inquiry : A participatory technique for system design . In Participatory design : Principles and practice . Erlbaum , Hillsdale , NJ , USA , 180 – 193 . 24 . Christopher Jarrold . 2003 . A Review of Research into Pretend Play in Autism . Autism 7 , 4 ( 2003 ) , 379 – 390 . DOI : http : / / dx . doi . org / 10 . 1177 / 1362361303007004004 25 . Jane Jorgenson and Tracy Sullivan . 2009 . Accessing Children’s Perspectives Through Participatory Photo Interviews . Forum Qualitative Sozialforschung / Forum : Qualitative Social Research 11 , 1 ( 2009 ) , Art . 8 . http : / / www . qualitative - research . net / index . php / fqs / article / view / 447 26 . Wendy E . Keay - Bright . 2007 . The Reactive Colours Project : Demonstrating Participatory and Collaborative Design Methods for the Creation of Software for Autistic Children . Digital Creativity 1 , 2 ( 2007 ) , 7 – 16 . http : / / ijg . cgpublisher . com / product / pub . 154 / prod . 17 27 . Lorcan Kenny , Caroline Hattersley , Bonnie Molins , Carole Buckley , Carol Povey , and Elizabeth Pellicano . 2015 . Which terms should be used to describe autism ? Perspectives from the UK autism community . Autism 20 , 4 ( July 2015 ) , 1362361315588200 . DOI : http : / / dx . doi . org / 10 . 1177 / 1362361315588200 28 . Julie A . Kientz , Matthew S . Goodwin , Gillian R . Hayes , and Gregory D . Abowd . 2013 . Interactive Technologies for Autism . Synthesis Lectures on Assistive , Rehabilitative , and Health - Preserving Technologies 2 , 2 ( 2013 ) , 1 – 177 . DOI : http : / / dx . doi . org / 10 . 2200 / S00533ED1V01Y201309ARH004 29 . Mike Kuniavsky . 2007 . Magic as a Metaphor for Ubiquitous Computing . Ambidextrous Magazine Spring ( 2007 ) , 36 – 37 . 30 . Diana Kusunoki and Aleksandra Sarcevic . 2012 . Applying Participatory Design Theory to Designing Evaluation Methods . In CHI ’12 Extended Abstracts on Human Factors in Computing Systems ( CHI EA ’12 ) . ACM , New York , NY , USA , 1895 – 1900 . DOI : http : / / dx . doi . org / 10 . 1145 / 2212776 . 2223725 31 . Diana S . Kusunoki and Aleksandra Sarcevic . 2013 . A Participatory Framework for Evaluation Design . In iConference 2013 Proceedings . iSchools , Fort Worth , Texas , 860 – 864 . DOI : http : / / dx . doi . org / 10 . 9776 / 13439 32 . Julia Makhaeva , Christopher Frauenberger , and Katharina Spiel . 2016 . Creating Creative Spaces for Co - designing with Autistic Children : The Concept of a " Handlungsspielraum " . In Proceedings of the 14th Participatory Design Conference : Full Papers - Volume 1 ( PDC ’16 ) . ACM , New York , NY , USA , 51 – 60 . DOI : http : / / dx . doi . org / 10 . 1145 / 2940299 . 2940306 33 . Laura Malinverni , Joan Mora - Guiard , and Narcis Pares . 2016 . Towards methods for evaluating and communicating participatory design : A multimodal approach . International Journal of Human - Computer Studies 94 ( 2016 ) , 53 – 63 . DOI : http : / / dx . doi . org / 10 . 1016 / j . ijhcs . 2016 . 03 . 004 34 . Laura Malinverni , Joan MoraGuiard , Vanesa Padillo , MariaAngeles Mairena , Amaia Hervás , and Narcis Pares . 2014 . Participatory Design Strategies to Enhance the Creative Contribution of Children with Special Needs . In Proceedings of the 2014 Conference on Interaction Design and Children ( IDC ’14 ) . ACM , New York , NY , USA , 85 – 94 . DOI : http : / / dx . doi . org / 10 . 1145 / 2593968 . 2593981 35 . CDC / ATSDR Committee on Community Engagement . 2011 / Accessed July 14 , 2016 . Principles of Community Engagement . ( 2011 / Accessed July 14 , 2016 ) . https : / / www . atsdr . cdc . gov / communityengagement / 36 . Narcis Pares , Paul Masri , Gerard van Wolferen , and Chris Creed . 2005 . Achieving dialogue with children with severe autism in an adaptive multisensory interaction : the " MEDIATE " project . IEEE Transactions on Visualization and Computer Graphics 11 , 6 ( Nov . 2005 ) , 734 – 743 . DOI : http : / / dx . doi . org / 10 . 1109 / TVCG . 2005 . 88 37 . Susi Ross , Magnus Ramage , and Yvonne Rogers . 1995 . PETRA : participatory evaluation through redesign and analysis . Interacting with Computers 7 , 4 ( 1995 ) , 335 – 360 . DOI : http : / / dx . doi . org / 10 . 1016 / 0953 - 5438 ( 96 ) 87697 - 1 38 . Ginny Russell , Stephan Collishaw , Jean Golding , Susan E Kelly , and Tamsin Ford . 2015 . Changes in diagnosis rates and behavioural traits of autism spectrum disorder over time . British Journal of Psychiatry Open 1 , 2 ( 2015 ) , 110 – 115 . DOI : http : / / dx . doi . org / 10 . 1192 / bjpo . bp . 115 . 000976 39 . Steve Sato and Tony Salvador . 1999 . Methods & Tools : Playacting and Focus Troupes : : Theater Techniques for Creating Quick , Intense , Immersive , and Engaging Focus Group Sessions . interactions 6 , 5 ( Sept . 1999 ) , 35 – 41 . DOI : http : / / dx . doi . org / 10 . 1145 / 312683 . 312715 40 . Katharina Spiel , Christopher Frauenberger , Geraldine Fitzpatrick , and Eva Hornecker . 2017 . When Empathy Is Not Enough : Assessing the Experiences of Autistic Children withTechnologies . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( CHI’17 ) . ACM , New York , NY , USA , tbd . DOI : http : / / dx . doi . org / 10 . 1145 / 3025453 . 3025785 41 . Katharina Spiel , Christopher Frauenberger , Julia Makhaeva , and Fares Kayali . 2016 . Talking about Myself - Playful Inquiry into an Absent Life World . In Workshop on Games as HCI Method at CHI’16 . . , San Jose , USA , Article 0 , 4 pages . 42 . Jean Lee Tan , Dion Hoe - Lian Goh , Rebecca P . Ang , and Vivien S . Huan . 2013 . Participatory evaluation of an educational game for social skills acquisition . Computers & Education 64 ( 2013 ) , 70 – 80 . DOI : http : / / dx . doi . org / 10 . 1016 / j . compedu . 2013 . 01 . 006 43 . Giasemi Vavoula and Mike Sharples . 2007 . Future technology workshop : A collaborative method for the design of new learning technologies and activities . International Journal of Computer - Supported Collaborative Learning 2 , 4 ( 2007 ) , 393 – 419 . DOI : http : / / dx . doi . org / 10 . 1007 / s11412 - 007 - 9026 - 0