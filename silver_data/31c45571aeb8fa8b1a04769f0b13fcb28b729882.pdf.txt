Co - clustering Document - term Matrices by Direct Maximization of Graph Modularity Melissa Ailem François Role Mohamed Nadif LIPADE , Université Paris Descartes 45 , rue des Saints - Pères 75006 Paris , France { melissa . ailem , francois . role , mohamed . nadif } @ parisdescartes . fr ABSTRACT We present Coclus , a novel diagonal co - clustering algorithm which is able to eﬀectively co - cluster binary or contingency matrices by directly maximizing an adapted version of the modularity measure traditionally used for networks . While some eﬀective co - clustering algorithms already exist that use network - related measures ( normalized cut , modularity ) , they do so by using spectral relaxations of the discrete op - timization problems . In contrast , Coclus allows to get even better co - clusters by directly maximizing modularity using an iterative alternating optimization procedure . Extensive comparative experiments performed on various document - term datasets demonstrate that our algorithm is very eﬀec - tive , stable and outperforms other co - clustering algorithms . Categories and Subject Descriptors H . 3 . 3 [ Information Search and Retrieval ] : Clustering Keywords Co - Clustering , Modularity 1 . INTRODUCTION In the era of big data , there is more than ever a need for techniques that simultaneously group words and documents into meaningful clusters , thus making large data sets easier to handle and interpret . Co - clustering techniques just serve this purpose . Given a data matrix X of size n × d where I is the set of n rows , and J the set of d columns , a co - cluster k is a submatrix I k × J k ( I k ⊆ I , J k ⊆ J ) where rows and columns follow some consistent patterns . For a survey of the diﬀerent structures of co - clusters and the diﬀerent algorithms employed the reader is referred to [ 6 , 8 , 2 , 3 ] . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Request permissions from Permissions @ acm . org . CIKM’15 , October 19 - 23 , 2015 , Melbourne , VIC , Australia Copyright 2015 ACM . ISBN 978 - 1 - 4503 - 3794 - 6 / 15 / 10 . . . $ 15 . 00 DOI : http : / / dx . doi . org / 10 . 1145 / 2806416 . 2806639 In this paper , we are especially concerned with algorithms that seek an optimal block diagonal co - clustering , mean - ing that data points and features have the same number of clusters and that , after proper permutation of the rows and columns , the algorithm produces as result a block di - agonal matrix ( see ﬁgure 1 ) . In the context of document - term matrices , this co - clustering model has the advantage of directly producing interpretable descriptions of the re - sulting document clusters . For example , Li [ 5 ] has proposed a block diagonal clustering algorithm that , given a binary document - term matrix , produces a block diagonal matrix of ones . This algorithm consists in alternating the clustering Figure 1 : Left : Original data - Middle : data re - organized according to row partition - Right : data reorganized according to row and column partitions . of rows and columns minimizing the squared error between the original X data and its approximation AB T where A and B are binary matrices . Another notable co - clustering algorithm is the bipartite spectral graph partitioning algo - rithm described in [ 1 ] . This algorithm ﬁnds the optimal min - imum cut partitions in a bipartite document - term graph by computing the second left and right singular vector of the normalized document - term matrix , thus using a real relax - ation of the discrete optimization problem . More recently , another attempt to use network - related criteria in the ﬁeld of block diagonal structure is the spectral co - clustering max - imizing a generalization of the modularity [ 4 ] . Compared to other binary clustering methods based on nonnegative ma - trix factorization or tri - factorization , this algorithm appears to perform better in the ﬁeld of document clustering . The two last - mentioned co - clustering algorithms [ 1 , 4 ] both try to optimize criteria initially used in the studies of networks ( normalized cut , modularity ) . However they do so by using a spectral relaxation of the discrete optimization problem . In contrast , our algorithm , Coclus , is based on a direct max - imization of one of these criteria , namely bipartite modu - larity . We show that this approach outperforms previous block diagonal co - clustering algorithms , as demonstrated by comparative experiments conducted on numerous datasets . Notation . We will consider the partition of the sets I of n objects and the set J of d attributes into g non overlapping 1807 clusters , where g may be greater or equal to 2 . Let us deﬁne an n × g index matrix Z and an d × g index matrix W with one column for each cluster ; Z = ( Z 1 | Z 2 | . . . | Z g ) and W = ( W 1 | W 2 | . . . | W g ) . Each column Z k or W k is deﬁned as follows : z ik = 1 if object i belongs to cluster Z k and z ik = 0 otherwise , and in the same manner w jk = 1 if attribute j belongs to cluster W k and w jk = 0 otherwise . 2 . MODULARITY MEASURE In this section we ﬁrst review the standard graph modu - larity measure and then present the adaptation needed for using modularity in the co - clustering context . 2 . 1 Standard Modularity Modularity is a quality criterion for graph clustering , which has received considerable attention in several disciplines since the seminal work presented in [ 9 ] . Maximizing the modu - larity measure can be expressed in the form of an integer linear programming . Given the graph G = ( V , E ) , let A be a binary , symmetric adjacency matrix with ( i , i (cid:48) ) as entry ; and a ii (cid:48) = 1 if there is an edge between the nodes i and i (cid:48) . If there is no edge between nodes i and i (cid:48) , a ii (cid:48) is equal to zero . Finding a partition of the set of nodes V into homogeneous subsets leads to the resolution of the following integer linear program : max C Q ( A , C ) where Q ( A , C ) is the modularity measure Q ( A , C ) = 12 | E | (cid:80) ni , i (cid:48) = 1 ( a ii (cid:48) − a i . a i (cid:48) . 2 | E | ) (cid:80) gk = 1 z ik z i (cid:48) k . Taking c ii (cid:48) = (cid:80) gk = 1 z ik z i (cid:48) k , the expression of Q becomes Q ( A , C ) = 1 2 | E | n (cid:88) i , i (cid:48) = 1 ( a ii (cid:48) − a i . a i (cid:48) . 2 | E | ) c ii (cid:48) , ( 1 ) where 2 | E | = (cid:80) i , i (cid:48) a ii (cid:48) = a . . is the total number of edges and a i . = (cid:80) i (cid:48) a ii (cid:48) the degree of i . Let δ = ( δ ii (cid:48) ) be the ( n × n ) data matrix deﬁned by ∀ i , i (cid:48) δ ii (cid:48) = a i . a i (cid:48) . 2 | E | , the expression ( 1 ) becomes Q ( A , C ) = 12 | E | Trace [ ( A − δ ) C ] . The researched binary matrix C is deﬁned by ZZ t which models a partition in a relational space and therefore must check the properties of an equivalence relation . 2 . 2 Modularity for Co - clustering In a bipartite context , the basic idea is to model the si - multaneous row and column partitions using the relation C deﬁned on I × J and called block seriation in [ 7 ] . Noting that C = ZW t and the general term can be expressed as follows : c ij = 1 if object i is in the same block as attribute j and c ij = 0 otherwise . Then c ij = (cid:80) g k = 1 z ik w jk . Now , given a rectangular matrix A deﬁned on I × J , modularity can be reformulated as follows in the co - clustering context : Q ( A , C ) = 1 a . . n (cid:88) i = 1 d (cid:88) j = 1 g (cid:88) k = 1 ( a ij − a i . a . j a . . ) z ik w jk , ( 2 ) where a . . = (cid:80) i , j a ij = | E | is the total weight of edges and a i . = (cid:80) j a ij ( the degree of i ) and a . j = (cid:80) i a ij ( the degree of j ) . This modularity measure can also take the following form : Q ( A , C ) = 1 a . . Trace [ ( A − δ ) t ZW t ] = Q ( A , ZW t ) . ( 3 ) Matrix ZW t represents a block seriation relation respect - ing the binary , assignment constraints and triad impossible properties ( see [ 7 ] for further details ) . As the objective func - tion ( 3 ) is linear with respect to C and as the constraints that C must respect are linear equations , the problem can theoretically be solved using an integer linear programming solver . However , this problem is NP hard , and as a result , in practice , we use heuristics for dealing with large data sets . 3 . CO - CLUSTERING ALGORITHM Hereafter , we propose to tackle the co - clustering problem by maximizing the modularity criterion . Proposition 1 : Let A be a ( n × d ) binary or contin - gency data and C be a ( n × d ) deﬁning a block seriation , the modularity measure Q ( A , C ) can be rewritten as 1 . Q ( A , C ) = 1 a . . Trace [ ( A W − δ W ) t Z ] = Q ( A W , Z ) where A W : = { a ik = (cid:80) dj = 1 w jk a ij ; i = 1 , . . . , n ; k = 1 , . . . , g } and δ W : = { δ Wik = a i . a W . k a . . ; i = 1 , . . . , n ; k = 1 , . . . , g } with a W . k = (cid:80) dj = 1 w jk a . j 2 . Q ( A , C ) = 1 a . . Trace [ ( A Z − δ Z ) t W ] = Q ( A Z , W ) where A Z : = { a jk = (cid:80) ni = 1 z ik a ij ; k = 1 , . . . , g ; j = 1 , . . . , d } and δ Z : = { δ Zjk = a j . a Z . k a . . ; k = 1 , . . . , g ; j = 1 , . . . , d } with a Z . k = (cid:80) n i = 1 z ik a i . Proof : Q ( A , C ) = 1 a . . n (cid:88) i = 1 d (cid:88) j = 1 ( a ij − a i . a . j a . . ) c ij = 1 a . . n (cid:88) i = 1 d (cid:88) j = 1 g (cid:88) k = 1 ( a ij − a i . a . j a . . ) z ik w jk = 1 a . . n (cid:88) i = 1 g (cid:88) k = 1 z ik d (cid:88) j = 1 w jk ( a ij − a i . a . j a . . ) = 1 a . . n (cid:88) i = 1 g (cid:88) k = 1 z ik (cid:32) d (cid:88) j w jk a ij − a i . a . . d (cid:88) j = 1 w jk a . j (cid:33) = 1 a . . n (cid:88) i = 1 g (cid:88) k = 1 (cid:18) a Wik − a i . a W . k a . . (cid:19) z ik = 1 a . . Trace [ ( A W − δ W ) t Z ] = Q ( A W , Z ) In the same manner , we can show that Q ( A , C ) = 1 a . . d (cid:88) j = 1 g (cid:88) k = 1 ( a Zjk − a j . a Z . k a . . ) w jk = 1 a . . Trace [ ( A Z − δ Z ) t W ] = Q ( A Z , W ) Using proposition 1 , the goal is to maximize modularity by alternatively maximizing Q ( A W , Z ) and Q ( A Z , W ) . The op - timal classiﬁcation binary matrices Z ∗ and W ∗ are respec - tively deﬁned by Z ∗ = arg max Z Trace ( A W − δ W ) t Z and W ∗ = arg max W Trace ( A Z − δ Z ) t W . This alternated co - clustering is described in more details in algorithm 1 . Here - after , we illustrate an execution by means of a small example ( a binary matrix A of size ( 5 × 4 ) with g = 2 co - clusters ) . INPUT : Let A =    1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0    be a data matrix , W =    0 1 1 0 0 1 0 1    , Z =   0 1 1 0 0 1 1 0 1 0   initial indices matrices , then Q ( A , ZW t ) = 0 . 16 . 1808 Algorithm 1 Coclus Input : binary or contingency data A , number of co - clusters g Output : partition matrices Z and W 1 . Initialization of W repeat2 . Compute A W 3 . Compute Z maximizing Q ( A W , Z ) by z ik = arg max 1 ≤ (cid:96) ≤ g (cid:18) a Wi(cid:96) − a i . a W . (cid:96) a . . (cid:19) ∀ i = 1 , . . . , n ; k = 1 , . . . , g 4 . Compute A Z 5 . Compute W maximizing Q ( A Z , W ) by w jk = arg max 1 ≤ (cid:96) ≤ g (cid:18) a Zj(cid:96) − a j . a Z . (cid:96) a . . (cid:19) ∀ j = 1 , . . . , d ; k = 1 , . . . , g 6 . Compute Q ( A , ZW t ) until noChange of Q ( A , ZW t ) Compute Z given W : To this end , we compute ( A W − δ W ) A W =   0 2 1 1 0 2 1 1 0 2   δ W =   0 . 4 1 . 6 0 . 4 1 . 6 0 . 4 1 . 6 0 . 4 1 . 6 0 . 4 1 . 6   ⇒ ( A W − δ W ) =   − 0 . 4 0 . 4 0 . 6 − 0 . 6 − 0 . 4 0 . 4 0 . 4 − 0 . 6 − 0 . 4 0 . 4   then Z =   0 1 1 0 0 1 1 0 0 1   and Q ( A W , Z ) = 0 . 24 . Compute W given Z : To this end we compute ( A Z − δ Z ) A Z =   0 3 2 0 0 3 2 0   δ Z =   1 . 2 1 . 8 0 . 8 1 . 2 1 . 2 1 . 8 0 . 8 1 . 2   ⇒ ( A Z − δ Z ) =   − 1 . 2 1 . 2 1 . 2 − 1 . 2 − 1 . 2 1 . 2 1 . 2 − 1 . 2   then W =   0 1 1 0 0 1 1 0   and Q ( A Z , W ) = 0 . 48 . OUTPUT : W =   0 1 1 0 0 1 1 0   Z =   0 1 1 0 0 1 1 0 0 1   and we obtain a reorga - nized version of the matrix A according to ( Z , W )   1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1   . Coclus is computationally eﬃcient for sparse data and its complexity can be shown to be O ( nz . it . ( 2 . g ) ) where nz is the number of non - zero values in the input data and it the number of iterations which is small . Empirically , about 15 iterations seemed suﬃcient for the datasets we used in our experiments . 4 . NUMERICAL EXPERIMENTS To test the clustering performance of Coclus against other algorithms , we ran it on 10 real datasets with diﬀerent sizes and balances 1 coeﬃcient ( table 1 ) . In Figure 2 , we report the distributions of document sizes 2 deﬁned by a i . = (cid:80) j a ij . We observe that , unlike other datasets , SPORTS and RE - VIEWS have very diﬀerent document sizes . We will see how 1 The balance coeﬃcient is deﬁned as the ratio of the number of documents in the smallest class to the number of docu - ments in the largest class . 2 Number of terms in a document . Table 1 : Description of Datasets Datasets Characteristics documents words # clusters Sparsity ( % ) Balance CSTR 475 1000 4 96 . 60 0 . 399 WEBKB4 4199 1000 4 93 . 90 0 . 307 WEBACE 2340 1000 20 91 . 83 0 . 169 NG2 500 2000 2 96 . 90 1 CLASSIC3 3891 4303 3 98 . 0 0 . 71 CLASSIC4 7095 5896 4 99 . 41 0 . 323 NG20 19949 43586 20 99 . 99 0 . 991 RCV1 9625 29992 4 99 . 75 0 . 766 SPORTS 8580 14870 7 99 . 99 0 . 036 REVIEWS 4069 18483 5 99 . 99 0 . 098 we can exploit this remark . Figure 2 : Boxplot of document sizes of each dataset The competitive algorithms retained for comparison with ours are : the spectral graph partitioning proposed in [ 1 ] de - noted as Spec in the sequel and the spectral co - clustering maximizing a generalized bipartite modularity , normalized by the row and column cluster size [ 4 ] , denoted as SpecCo . We chose these algorithms because they seek a block diago - nal co - clustering of terms and documents , just as ours . For Spec we used the Scikit - learn implementation of Dhillon’s algorithm 3 , and we reimplemented the SpecCo algorithm . To evaluate Coclus , we consider three situations : original data , binarized data and ﬁnally after normalized TF - IDF transformation . The used TF - IDF weighting scheme is the following : w ij = tf ij ( 1 + log ( 1 + n 1 + d j ) ) , where w ij is the weight of term i in document j , tf ij is the frequency of term i in document j , n is the total number of documents and d j is the number of documents containing term j . The L 2 normaliza - tion was applied on the obtained TF - IDF matrix . When the data sets are binary or original we perform a normalization only when the datasets present high variance in terms of document sizes . In this case , we propose to divide each row by its norm before applying the co - clustering algorithms in order to eliminate the biases induced by the length of a doc - ument . For instance , SPORTS and REVIEWS require this normalization . For the other datasets , this normalization does not increase signiﬁcantly the performance of compared algorithms . To validate the obtained results , we used the clustering accuracy and normalized mutual information . Clustering accuracy , denoted Acc , measures the extent to which each cluster contains data points from the corresponding class and is deﬁned as follows : Acc = 1 n max [ (cid:80) C k , L m T ( C k , L m ) ] , where C k is the k th cluster in the ﬁnal results , and L m is the true m th class . T ( C k , L m ) is the number of entities which be - long to class m and are assigned to cluster k . The greater the clustering accuracy , the better the clustering performance . We also used the normalized mutual information ( NMI ) , 3 http : / / scikit - learn . org / stable / 1809 Figure 3 : NMI Comparison of Spec , SpecCo and Coclus . Left : binary data , Middle : Original data , Right : TF - IDF data . Table 2 : T - tests results comparing SpecCo and Coclus Binary Original TF - IDF datasets per . SpecCo Coclus p - value SpecCo Coclus p - value SpecCo Coclus p - value CSTR Acc 0 . 89 ± 0 . 0005 0 . 90 ± 0 . 007 < 0 . 001 0 . 82 ± 3 . 3e - 16 0 . 85 ± 0 . 007 < 0 . 001 0 . 89 ± 0 . 003 0 . 89 ± 0 . 02 0 . 43 NMI 0 . 77 ± 0 . 001 0 . 78 ± 0 . 02 < 0 . 001 0 . 72 ± 2 . 8e - 16 0 . 65 ± 0 . 003 < 0 . 001 0 . 77 ± 0 . 004 0 . 73 ± 0 . 04 < 0 . 001 WEBKB4 Acc 0 . 69 ± 0 . 007 0 . 72 ± 0 . 008 < 0 . 001 0 . 70 ± 0 . 003 0 . 71 ± 0 . 009 < 0 . 001 0 . 73 ± 0 . 0002 0 . 77 ± 0 . 03 < 0 . 001 NMI 0 . 44 ± 0 . 0007 0 . 47 ± 0 . 01 < 0 . 001 0 . 44 ± 0 . 001 0 . 46 ± 0 . 01 < 0 . 001 0 . 51 ± 0 . 0002 0 . 52 ± 0 . 002 < 0 . 001 WEBACE Acc 0 . 52 ± 0 . 03 0 . 58 ± 0 . 02 < 0 . 001 0 . 45 ± 0 . 02 0 . 56 ± 0 . 01 < 0 . 001 0 . 47 ± 0 . 04 0 . 58 ± 0 . 01 < 0 . 001 NMI 0 . 61 ± 0 . 01 0 . 60 ± 0 . 007 0 . 045 0 . 56 ± 0 . 009 0 . 57 ± 0 . 003 0 . 01 0 . 56 ± 0 . 02 0 . 59 ± 0 . 002 < 0 . 001 NG2 Acc 0 . 89 ± 0 . 004 0 . 93 ± 0 . 0007 < 0 . 001 0 . 89 ± 5 . 6e - 16 0 . 94 ± 0 . 02 < 0 . 001 0 . 94 ± 0 . 001 0 . 92 ± 0 . 01 < 0 . 001 NMI 0 . 53 ± 0 . 009 0 . 65 ± 0 . 02 < 0 . 001 0 . 52 ± 1 . 1e - 16 0 . 67 ± 0 . 05 < 0 . 001 0 . 68 ± 0 . 004 0 . 61 ± 0 . 04 < 0 . 001 CLASSIC3 Acc 0 . 98 ± 3 . 3e - 16 0 . 98 ± 0 . 0005 < 0 . 001 0 . 98 ± 4 . 6e - 05 0 . 98 ± 0 . 002 < 0 . 001 0 . 98 ± 0 . 0003 0 . 99 ± 0 . 0006 < 0 . 001 NMI 0 . 90 ± 3 . 46e - 16 0 . 91 ± 0 . 003 < 0 . 001 0 . 91 ± 0 . 0002 0 . 92 ± 0 . 004 < 0 . 001 0 . 91 ± 0 . 0007 0 . 94 ± 0 . 003 < 0 . 001 CLASSIC4 Acc 0 . 45 ± 2 . 8e - 16 0 . 91 ± 0 . 006 < 0 . 001 0 . 58 ± 6 . 8e - 05 0 . 90 ± 0 . 01 < 0 . 001 0 . 45 ± 1 . 7e - 16 0 . 85 ± 0 . 02 < 0 . 001 NMI 0 . 02 ± 1 . 1e - 17 0 . 73 ± 0 . 01 < 0 . 001 0 . 48 ± 0 . 0003 0 . 73 ± 0 . 02 < 0 . 001 0 . 01 ± 1 . 7e - 18 0 . 67 ± 0 . 02 < 0 . 001 NG20 Acc 0 . 19 ± 0 . 01 0 . 39 ± 0 . 02 < 0 . 001 0 . 26 ± 0 . 01 0 . 37 ± 0 . 03 < 0 . 001 0 . 15 ± 5 . 7e - 05 0 . 37 ± 0 . 05 < 0 . 001 NMI 0 . 41 ± 0 . 01 0 . 53 ± 0 . 02 < 0 . 001 0 . 46 ± 0 . 01 0 . 50 ± 0 . 02 < 0 . 001 0 . 38 ± 0 . 0002 0 . 45 ± 0 . 02 < 0 . 001 RCV1 Acc 0 . 3 ± 0 . 0006 0 . 58 ± 0 . 09 < 0 . 001 0 . 3 ± 1 . 6e - 17 0 . 67 ± 0 . 06 < 0 . 001 0 . 30 ± 2 . 2e - 16 0 . 56 ± 0 . 07 < 0 . 001 NMI 0 . 02 ± 0 . 002 0 . 40 ± 0 . 05 < 0 . 001 0 . 01 ± 1 . 7e - 18 0 . 44 ± 0 . 04 < 0 . 001 0 . 008 ± 6 . 1e - 18 0 . 30 ± 0 . 05 < 0 . 001 SPORTS Acc 0 . 59 ± 0 . 0001 0 . 68 ± 0 . 02 < 0 . 001 0 . 68 ± 0 . 0002 0 . 67 ± 0 . 02 < 0 . 001 0 . 61 ± 0 . 0007 0 . 65 ± 0 . 05 < 0 . 001 NMI 0 . 45 ± 0 . 0001 0 . 53 ± 0 . 03 < 0 . 001 0 . 59 ± 0 . 0001 0 . 57 ± 0 . 004 < 0 . 001 0 . 45 ± 0 . 0008 0 . 56 ± 0 . 03 < 0 . 001 REVIEWS Acc 0 . 56 ± 0 . 003 0 . 64 ± 0 . 03 0 . 006 0 . 45 ± 5 . 6e - 17 0 . 61 ± 0 . 05 < 0 . 001 0 . 46 ± 0 . 0001 0 . 60 ± 0 . 03 < 0 . 001 NMI 0 . 36 ± 0 . 002 0 . 52 ± 0 . 03 < 0 . 001 0 . 342 . 7e - 16 0 . 51 ± 0 . 02 < 0 . 001 0 . 35 ± 0 . 0001 0 . 50 ± 0 . 02 < 0 . 001 which is estimated by : NMI = (cid:80) k , (cid:96) Nk , (cid:96) n log nNk , (cid:96) Nk ˆ N(cid:96) (cid:114) ( (cid:80) k Nkn log Nkn ) ( (cid:80) (cid:96) ˆ N(cid:96)n log ˆ N(cid:96)n ) , where N k denotes the number of data contained in the clus - ter C k ( 1 ≤ k ≤ g ) , ˆ N (cid:96) is the number of data belonging to the class L (cid:96) ( 1 ≤ (cid:96) ≤ g ) , and N k , (cid:96) denotes the number of data that are in the intersection between cluster C k and class L (cid:96) . The larger the NMI , the better the quality of clustering . The results presented in ﬁgure 3 and table 2 , were obtained by running each algorithm 100 times with random initialization . NMI values reported in ﬁgure 3 show that our algorithm is very stable and has a good behavior whatever the nature of the data ( binary , contingency or TF - IDF ) . The results show that Coclus is better than SpecCo which is almost always better than Spec . To conﬁrm this , we perform t - tests comparing the two best algorithms Coclus and SpecCo . In Table 2 , we observe that Coclus outperfoms signiﬁcantly SpecCo in almost all situations . 5 . CONCLUSION We have presented a novel diagonal co - clustering algo - rithm which directly maximizes a modularity measure adapted to a bipartite context . In particular , we have shown that this modularity can be maximized using an iterative alternat - ing optimization , which led to a new , eﬀective co - clustering algorithm . Experimental results obtained on several real datasets indeed demonstrated the eﬀectiveness of our algo - rithm . The experiments also showed its robustness and ﬂex - ibility since it can achieve very good results both on binary and contingency document - term tables . Paths for future research might include extending the approach to fuzzy co - clustering and assessing the number of co - clusters . References [ 1 ] I . Dhillon . Co - clustering documents and words using bipartite spectral graph partitioning . KDD’01 , pages 269 – 274 , 2001 . [ 2 ] G . Govaert and M . Nadif . Block clustering with bernoulli mixture models : Comparison of diﬀerent approaches . Computational Statistics & Data Analysis , 52 : 3233 – 3245 , 2008 . [ 3 ] G . Govaert and M . Nadif . Co - Clustering : Models , Algorithms and Applications . Wiley . 2013 . [ 4 ] L . Labiod and M . Nadif . Co - clustering for binary and categorical data with maximum modularity . In ICDM’11 , pages 1140 – 1145 , 2011 . [ 5 ] T . Li . A general model for clustering binary data . In KDD ’05 , pages 188 – 197 , 2005 . [ 6 ] S . C . Madeira and A . L . Oliveira . Biclustering algorithms for biological data analysis : A survey . IEEE / ACM Trans . Comput . Biol . Bioinformatics , 1 : 24 – 45 , 2004 . [ 7 ] F . Marcotorchino . Seriation problems : An overview . Applied Stochastic Models And Data Analysis , 7 : 199 – 202 , 1991 . [ 8 ] I . V . Mechelen , H . Bock , and P . De Boeck . Two - mode clustering methods : a structured overview . Statistical methods in medical research , 13 , ( 5 ) : 363 – 394 , 2004 . [ 9 ] M . E . J . Newman and M . Girvan . Finding and evaluating community structure in networks . 2004 . 1810