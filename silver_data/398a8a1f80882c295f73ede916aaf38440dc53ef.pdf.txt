1 Beyond Deep Reinforcement Learning : A Tutorial on Generative Diffusion Models in Network Optimization Hongyang Du ∗ , Ruichen Zhang ∗ , Yinqiu Liu ∗ , Jiacheng Wang ∗ , Yijing Lin ∗ , Zonghang Li ∗ , Dusit Niyato , Fellow , IEEE , Jiawen Kang , Zehui Xiong , Shuguang Cui , Fellow , IEEE , Bo Ai , Fellow , IEEE , Haibo Zhou , Dong In Kim , Fellow , IEEE Abstract —Generative Diffusion Models ( GDMs ) have emerged as a transformative force in the realm of Generative Artificial Intelligence ( GAI ) , demonstrating their versatility and efficacy across a variety of applications . The ability to model complex data distributions and generate high - quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning . Furthermore , their iterative nature , which involves a series of noise addition and denoising steps , is a powerful and unique approach to learning and generating data . This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks . We delve into the strengths of GDMs , emphasizing their wide applicability across various domains , such as vision , text , and audio generation . We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks . The paper first provides a basic background of GDMs and their applications in network opti - mization . This is followed by a series of case studies , showcasing the integration of GDMs with Deep Reinforcement Learning ( DRL ) , incentive mechanism design , Semantic Communications ( SemCom ) , Internet of Vehicles ( IoV ) networks , etc . These case studies underscore the practicality and efficacy of GDMs in real - world scenarios , offering insights into network design . We conclude with a discussion on potential future directions for GDM research and applications , providing major insights into how they can continue to shape the future of network optimization . Index Terms —Diffusion model , deep reinforcement learning , H . Du is with the School of Computer Science and Engineering , the Energy Research Institute @ NTU , Interdisciplinary Graduate Program , Nanyang Technological University , Singapore ( e - mail : hongyang001 @ e . ntu . edu . sg ) . Y . Liu , J . Wang and D . Niyato are with the School of Com - puter Science and Engineering , Nanyang Technological University , Sin - gapore ( e - mail : yinqiu001 @ e . ntu . edu . sg , jiacheng . wang @ ntu . edu . sg , dniy - ato @ ntu . edu . sg ) . R . Zhang is with the School of Computer and Information Technology , Beijing Jiaotong University , Beijing 100044 , China ( e - mail : ruichen . zhang @ bjtu . edu . cn ) . Y . Lin is with the State Key Laboratory of Net - working and Switching Technology , Beijing University of Posts and Telecom - munications , China ( e - mail : yjlin @ bupt . edu . cn ) . Z . Li is with the School of In - formation and Communication Engineering , University of Electronic Sciences and Technology of China , Chengdu , China . ( Email : lizhuestc @ gmail . com ) . J . Kang is with the School of Automation , Guangdong University of Technology , China . ( e - mail : kavinkang @ gdut . edu . cn ) . Z . Xiong is with the Pillar of Infor - mation Systems Technology and Design , Singapore University of Technology and Design , Singapore ( e - mail : zehui xiong @ sutd . edu . sg ) . S . Cui is with the School of Science and Engineering ( SSE ) and the Future Network of Intelligence Institute ( FNii ) , The Chinese University of Hong Kong ( Shen - zhen ) , Shenzhen , China ( e - mail : shuguangcui @ cuhk . edu . cn ) . B . Ai is with the State Key Laboratory of Rail Traffic Control and Safety , Beijing Jiaotong University , Beijing 100044 , China ( email : boai @ bjtu . edu . cn ) . Haibo Zhou is with School of Electronic Science and Engineering , Nanjing University , Nanjing , Jiangsu 210093 , China ( email : haibozhou @ nju . edu . cn ) . D . I . Kim is with the Department of Electrical and Computer Engineering , Sungkyunkwan University , Suwon 16419 , South Korea ( email : dikim @ skku . ac . kr ) . ∗ means equal contribution . generative AI , AI - generated content , network optimization I . I NTRODUCTION A . Background The emergence of Generative Artificial Intelligence ( GAI ) has marked a significant milestone , offering a transformative potential that extends beyond the traditional boundaries of Artificial Intelligence ( AI ) [ 1 ] . Unlike conventional AI ( also so - called discriminative AI ) models that focus primarily on analyzing or classifying existing data , GAI can create new data , including text , image , audio , synthetic time - series data , and more [ 1 ] . This potential of GAI has far - reaching impli - cations across diverse sectors , from business and science to society at large [ 2 ] , [ 3 ] . For instance , in the business sector , GAI can power customer service bots or generate product de - signs , thereby maximizing efficiency and boosting competitive advantages [ 4 ] . According to Accenture’s 2023 Technology Vision report [ 5 ] , 97 % of global executives agree that GAI will revolutionize how AI is used , enabling connections across data types and industries . In the natural science research com - munity , GAI can aid in generating synthetic data for research , e . g . , protein sequences for disease prediction models [ 6 ] , and accelerating the pace of discoveries [ 3 ] . Furthermore , GAI can augment human creativity in our society , enabling the creation of new art , music , and literary work , thereby enriching our cultural heritage [ 7 ] . GAI is not a singular technique but a collection of various models and methods , each of which is with its unique strengths and applications . Each of these models has contributed to the advancement of AI in different ways , forming the backbone of the current GAI landscape , in which major examples include : • Transformers : Transformers [ 8 ] have revolutionized Natural Language Generation ( NLG ) tasks , as exempli - fied by OpenAI’s ChatGPT [ 9 ] . They excel in applying context , a critical aspect of language understanding , and allow for greater parallelization of computing during training and inference . • Generative Adversarial Networks ( GANs ) : GANs [ 10 ] have been instrumental in the field of image synthesis . They consist of a generative model and a discriminative model that interact and compete against each other , leading to continuous improvement in performance . a r X i v : 2308 . 05384v1 [ c s . N I ] 10 A ug 2023 2 • Variational Autoencoders ( VAEs ) : VAEs [ 11 ] transform input data into a set of parameters in a latent space , which are then used to generate new data that closely aligns with the original distribution . • Flow - based Generative Models : Flow - based mod - els [ 12 ] use probabilistic flows for data generation . They employ back - propagation for gradient computation , enhancing learning efficiency . Their ability to directly compute the probability density function during gener - ation makes them computationally efficient , especially in mobile edge networks . • Energy - based Generative Models : Energy - based mod - els [ 13 ] represent data using energy values . They define an energy function and optimize it to minimize the input data’s energy value . These models are intuitive , flexible , and capable of capturing dependencies by associating an non - normalized probability scalar with each configuration of observed and latent variables . • Generative Diffusion Models ( GDMs ) : Initially pro - posed in [ 14 ] , the concept of GDMs drew inspiration from the thermodynamic diffusion process . This ther - modynamic correlation not only sets GDMs apart from other generative models but also establishes intriguing associations with score - based models [ 15 ] and stochas - tic differential equations [ 16 ] , thereby enabling unique avenues for further research and applications . Amidst these techniques , GDMs stand out due to their unique approach to data generation and their ability to model complex data distributions [ 17 ] . Recently , the versatility and potency of GDMs have been demonstrated in numerous applications , particularly in AI - generated Content ( AIGC ) domains . For instance , Stable Diffusion [ 18 ] , a diffusion - model based image generation application , has amassed over 10 million daily users , showcasing the practical utility and popularity of dif - fusion models . Furthermore , GDMs have been leveraged in various fields . In Computer Vision ( CV ) , they have been used to generate high - quality images from noise , with models such as Denoising Diffusion Probabilistic Models ( DDPM ) [ 19 ] and Denoising Diffusion Implicit Models ( DDIM ) [ 20 ] . They have also been employed in text generation tasks , enhancing the controllability and coherence of the generated text [ 21 ] . In the audio domain , GDMs have been used for tasks like symbolic music generation and text - to - speech conversion [ 22 ] , [ 23 ] . Be - yond traditional domains , GDMs have been utilized in graph generation [ 24 ] – [ 26 ] , molecular and material generation [ 27 ] – [ 29 ] , and in synthesizing tabular data to electrocardiogram signal synthesis [ 30 ] – [ 32 ] . The widespread adoption of GDMs can be attributed to several key advantages over other GAI methods . • High - quality data generation ability . GDMs employ a forward and reverse diffusion process [ 33 ] , enabling them to accurately capture complex data distributions and embrace high - quality . This stands in contrast to GANs , which can suffer from mode collapse , and VAEs , which can yield blurry results due to their Gaussian assumption [ 34 ] . • Flexibility . GDMs are adaptable to various types of data and applications due to their reliance on stochastic differential equations [ 17 ] . This flexibility is a significant advantage over Transformer - based models , which , while powerful , are primarily designed for sequence data . • Simplicity of Implementation . GDMs’ structure , featur - ing a fixed bottom - up path defined by a diffusion process and a top - down path parameterized by Deep Neural Networks ( DNNs ) , simplifies their implementation [ 35 ] , [ 36 ] . This is a notable advantage over GANs and VAEs , which often require complex architectures and training procedures [ 37 ] . B . Motivations The successful applications of GDMs across a diverse range of domains have inspired us to support intelligent network optimization [ 38 ] , [ 39 ] . However , future intelligent networks such as Integrated Sensing and Communications ( ISAC ) [ 40 ] , Semantic Communications ( SemCom ) [ 41 ] , and Internet of Vehicles ( IoV ) [ 42 ] are characterized by high - dimensional configurations , non - linear relationships , and intricate decision - making processes that are tightly linked with semantics and interpretations [ 43 ] . For example , SemCom networks require a deep understanding of semantic information to facilitate efficient and accurate communication [ 44 ] , and IoV networks involve the interaction of numerous highly - mobile entities with heterogeneous communication capabilities [ 42 ] , [ 45 ] . In all these cases , they exhibit complex dynamics with significant dependencies on prior and current states , as well as the environment , leading to high dimensional and multimodal state distributions [ 46 ] . This calls for sophisticated network management models , like GDMs . GDMs in this context are capable of capturing such high - dimensional and complex structures , and effectively dealing with numerous decision - making processes and optimization problems , understanding and capturing the nuances of the complex trade - offs involved in the operation and optimization of intelligent networks [ 47 ] . GDMs have been increasingly recognized for their poten - tial in optimization tasks , particularly in enhancing decision making and Deep Reinforcement Learning ( DRL ) . In decision - making scenarios , GDMs have been adapted to represent com - plex dynamics , incorporating additional conditioning variables such as constraints , and demonstrating scalability over long time horizons [ 48 ] , [ 49 ] . In the realm of DRL , GDMs have been employed as policy representations , capturing multi - modal action distributions and improving performance in offline RL tasks [ 50 ] . GDMs have also been used to introduce diffusion - based generative behavior models , demonstrating superior performance compared to conventional offline RL methods [ 51 ] . These initial explorations highlight the versa - tility and potential of GDMs in complex optimization tasks , setting the stage for more detailed discussions in Section II and Section III . Despite the promising advantages of GDMs in network optimization , we acknowledge that GDMs also come with their own set of challenges , e . g . , the computational com - plexity introduced by the iterative nature of GDMs . This complexity could potentially pose difficulties in large - scale 3 Survey Contributions Emphasis [ 17 ] Discuss generative diffusion models and their applications in CV , speech , bioinformatics , and NLP General review of GAMs [ 33 ] Provide an overview of diffusion models research , categorized into efficient sampling , improved likelihood estimation , and handling data with special structures [ 53 ] Discuss use of diffusion models for medical image analysis and various applications [ 54 ] Discuss diffusion models in image generation from text and recent advancements in GAI models Focus on the applications of GDMs on CV [ 55 ] Survey efficient diffusion models for vision and their applications in CV tasks [ 34 ] Survey diffusion models in vision and their applications in various vision tasks [ 56 ] Provide an overview of diffusion models in NLP , discussing text generation , translation , and summarization Focus on NLP [ 57 ] Discuss diffusion models in non - autoregressive text generation for improving text generation efficiency Focus on non - autoregressive text generation [ 58 ] Analyze the applications of diffusion models for time series data crucial in finance , weather , and healthcare Focus on time series data [ 59 ] Discuss knowledge distillation in diffusion models , transferring complex knowledge to simplify models Focuses on knowledge distillation [ 60 ] Focuse on using diffusion models for generating molecules , pro - teins , and materials in drug discovery and materials science Focus on several specific scientific applications [ 61 ] Discuss audio diffusion models in speech synthesis and recent advancements in GAI models Focus on audio and speech [ 62 ] Provide an overview of diffusion models in bioinformatics , includ - ing key concepts and various applications Focus on the applications in bioin - formatics [ 63 ] Present a survey on generative diffusion models on graphs , pro - viding a state - of - the - art overview Focus on the applications of GDMs on graphs TABLE I : Overview of survey papers on GDMs : Supplementary reading for our tutorial DRL tasks , such as those involving the optimization of exten - sive communication networks [ 52 ] . Additionally , GDMs might face challenges when dealing with data distributions that are characterized by high levels of noise or irregularities . This is particularly relevant in the context of real - world network traffic data [ 33 ] . Nevertheless , these challenges should not overshadow the potential of GDMs in network optimization . Instead , the challenges should be viewed as areas of oppor - tunity for further research and development . The refinement and adaptation of traditional GDMs to address these issues effectively could pave the way for significant advancements in the field of network optimization . C . Contributions The continuous advancements of GDMs in addressing op - timization problems have inspired researchers to use them in specific design challenges within intelligent networks , such as optimizing incentive mechanisms [ 38 ] and selecting ser - vice providers [ 64 ] . Despite these developments , we believe that the full potential of GDMs has yet to be explored , in which GDMs are expected to revolutionize the paradigm of AI - driven intelligent network management . While there are several surveys on GDMs , as shown in Table I , these works either provide a broad overview or focus on a specific area such as CV or Natural Language Processing ( NLP ) , leaving a gap in the comprehensive understanding of GDMs in the context of network optimization . This tutorial bridges this gap by providing an extensive introduction to GDMs , emphasizing their applications in network optimization challenges . Cru - cially , we present specific case studies drawn from several significant intelligent network scenarios . The contributions of our tutorial are listed below : • We provide a comprehensive tutorial on the applications of GDMs , particularly in intelligent network optimiza - tion . This tutorial aims to offer a broad understanding of the origin , development , and major strength of GDMs , and to detail how the GDMs can be effectively imple - mented to solve complex optimization problems in the dynamic wireless environment . • We provide several case studies regarding the integra - 4 tion of GDMs with future intelligent network scenarios , e . g . , DRL , Incentive Mechanism Design , ISAC , SemCom , and IoV Networks . These case studies demonstrate the practicality and efficacy of GDMs in emerging network technologies . • We discuss potential directions for GDM research and applications , providing insights into how GDMs can evolve and continue to influence future intelligent net - work design . As shown in Fig . 1 , the rest of the tutorial is structured as follows : We first study the applications of GDM in network optimization in Section II . The role of GDM in DRL is then explored in Section III . In Section IV , we present GDM’s role in incentive mechanism design . SemCom enhanced by GDMs are discussed in Section V , and Section VI focuses on applying GDMs in IoV Networks . In Section VII , we discuss the applications of GDM to several other network issues , i . e . , channel estimation , error correction coding , and channel denoising . Furthermore , we outline potential research directions in Section VIII . Section IX concludes this tutorial . II . N ETWORK O PTIMIZATION VIA G ENERATIVE D IFFUSION M ODELS This section presents an overview of GDMs , their ap - plications , principles , and extensions to facilitate network optimization . A step - by - step tutorial is provided , using a simple , yet representative , sum rate maximization problem as a demonstrative example , to illustrate the applications of GDMs in wireless environments . A . Applications of GDMs The distinct capability of GDMs , combined with their theo - retical elegance and the recent advancements in their training and sampling efficiency , has led to the widespread adoption of GDMs across a spectrum of domains . Specifically , 1 ) Computer Vision : The evolution and applications of GDMs in the field of vision have been marked by a series of interconnected advancements . Beginning with the DDPM [ 19 ] and DDIM [ 20 ] , the field has shifted towards dynamic and flexible frameworks that can generate high - quality images from noise . Building on this foundation , the reflected diffusion models [ 65 ] integrated constraints into the generative process , leading to more faithful samples and expanding the potential applications of GDMs . This concept of flexibility and adapt - ability was further extended by the DiffCollage model [ 66 ] , which demonstrated the ability of GDMs to generate large - scale content in parallel . The latent flow diffusion models [ 67 ] then bridged the gap between image and video generation , synthesizing optical flow sequences [ 68 ] in the latent space to create videos with realistic spatial details and temporal motion . Furthermore , the video diffusion models [ 69 ] marked a signif - icant milestone in generative modeling research , showcasing the potential of GDMs in generating temporally coherent , high - fidelity videos . 2 ) Text : Unlike Transformer - based models such as GPT , which focus primarily on sequence data , GDMs offer a unique advantage in their ability to model complex data distributions , making them more versatile for various tasks . Integrating language models into the diffusion process by Diffusion - LM [ 21 ] has enhanced the controllability and coherence of the generated text , demonstrating the adaptability of GDMs to different text generation tasks . This adaptability was further evidenced by the latent diffusion energy - based model [ 70 ] , which introduced an energy - based model into the diffusion process , thereby improving the interpretability and quality of text modeling . The versatility of GDMs was showcased by the DiffuSeq [ 71 ] and DiffuSum [ 72 ] models , which applied GDMs to diverse tasks such as sequence - to - sequence generation and extractive summarization . Lastly , the innovative approach of the DiffusER model [ 73 ] in formulating text editing as a diffusion process further expanded the scope of GDM applications , demonstrating their potential in complex text editing tasks . 3 ) Audio : GDMs have been leveraged to create a trans - formative shift in audio generation . The symbolic music generation model [ 22 ] demonstrated the potential of GDMs in generating complex symbolic music . The ProDiff model [ 23 ] further showcases the ability of GDMs to generate high - quality text - to - speech outputs rapidly . The MM - Diffusion model [ 74 ] further extended the versatility of GDMs , demonstrating their capability to generate joint audio and video content . The DiffWave model [ 75 ] and the DiffSinger model [ 76 ] enhanced audio synthesis by generating high - fidelity waveforms and expressive singing voices , respectively . Moreover , the CRASH model [ 77 ] used the GDM in raw audio synthesis , demon - strating GDMs’ ability to generate high - resolution percussive sounds , offering a more flexible generation capability com - pared to traditional methods . 4 ) Others : GDMs were also applied widely to other appli - cation domains . In graph generation , GDMs have been utilized to generate intricate graph structures , as demonstrated by the works in [ 24 ] – [ 26 ] . These models have effectively harnessed the power of GDMs to handle discrete data types , showcasing their adaptability in representing complex relationships and structures inherent in graph data . This adaptability extends to the field of molecular and material generation , where models like MolDiff [ 27 ] , DiffDock - PP [ 28 ] , and MDM [ 29 ] demonstrated how GDMs can be utilized to generate intricate molecular structures , such as proteins in the field of molec - ular biology and material science . GDMs have shown great potential in handling heterogeneous features and synthesizing diverse tabular and time - series data types . The models pre - sented in CoDi [ 30 ] , TabDDPM [ 31 ] , and DiffECG [ 32 ] have demonstrated the versatility of GDMs in tasks ranging from synthesizing tabular data to ECG signal synthesis . The exceptional performance and broad applicability of GDMs can be attributed to their unique design . This has garnered significant attention , particularly in generating di - verse high - resolution images , with large - scale models such as GLIDE [ 78 ] , DALLE - 2 [ 79 ] , Imagen [ 80 ] , and the fully open - source Stable Diffusion [ 18 ] being developed by leading organizations like OpenAI , Nvidia , and Google . Given the 5 T u t o r i a l ← GD M i n I n t e lli g e n t N e t w o r k O p ti m i z a ti o n → A l go r it h m S ce na r i o s Section I : Introduction System modeling and performance evaluation of SemCom I . A Background I . B Motivations I . C Contributions Section II : Network Optimization via Generative Diffusion Models II . B Principles of the GDMs Forward Diffusion Process Reverse Diffusion Process II . A Applications of GDMs Others Vision Text Audio II . C Motivations of using GDMs in Network Optimization II . D Tutorial with an Example Problem Formulation GDM as the solution Insights Section I : Introduction System modeling and performance evaluation of SemCom I . A Background I . B Motivations I . C Contributions Section II : Network Optimization via Generative Diffusion Models II . B Principles of the GDMs Forward Diffusion Process Reverse Diffusion Process II . A Applications of GDMs Others Vision Text Audio II . C Motivations of using GDMs in Network Optimization II . D Tutorial with an Example Problem Formulation GDM as the solution Insights Section III . Deep Reinforcement Learning III . A Fundamentals of DRL Deep Q - Network Proximal Policy Optimization Prioritized DQN Deep Recurrent Q - Network REINFORCE Soft Actor - Critic Rainbow III . B Application of GDM in DRL III . C Case Study : AIGC Service Provider Selection System Model GDM - based Optimal Decision Generation Numerical Results Section III . Deep Reinforcement Learning III . A Fundamentals of DRL Deep Q - Network Proximal Policy Optimization Prioritized DQN Deep Recurrent Q - Network REINFORCE Soft Actor - Critic Rainbow III . B Application of GDM in DRL III . C Case Study : AIGC Service Provider Selection System Model GDM - based Optimal Decision Generation Numerical Results Section IV . Incentive Mechanism Design IV . A Fundamentals of Incentive Mechanisms Shapley Value Auction Stackelberg Game Contract Theory IV . B Application of GDM in Incentive Mechanism Design IV . C Case Study : GDM - based Contract Theoretic Incentive Mechanism Background System Model GDM - based Optimal Contract Generation Utility Formulation Numerical Results Section V . Semantic Communications V . A Fundamentals of Semantic Communications Integration Gain Coordination Gain V . B Applications of GDMs in Semantic Communications V . C Case Study : GDM - based Resource Allocation for SemCom - aided AIGC services Motivation GDM - based Resource Allocation Scheme Generation Problem Formulation Numerical Results Semantic Encoder Wireless Channels Semantic Decoder VI . A Fundamentals of IoV Networks Recovery of Images sent by vehicles Optimization Based on GDM VI . B Applications of GDM in IoV Networks VI . C Case Study : A GAI - driven IoV network System Model GDM - based Joint Channel Selection and Power Allocation Problem Formulation Numerical Results Section VI . Internet of Vehicles Networks Section VIII : Future directions Section IX : Conclusion Section VIII : Future directions Section IX : Conclusion Section VII . Miscellaneous Issues VII . A Channel Estimation VII . B Error Correction Coding VII . C Channel Denoising Section VII . Miscellaneous Issues VII . A Channel Estimation VII . B Error Correction Coding VII . C Channel Denoising Motivations Case Study Motivations Case Study Motivations Case Study Fig . 1 : Structure of Our Tutorial : We initiate our discussion with the foundational knowledge of GDM and the motivation behind their applications in network optimization . This is followed by exploring GDM’s wide applications and fundamental principles and a comprehensive tutorial outlining the steps for using GDM in network optimization . In the context of intelligent networks , we study the impact of GDM on algorithms , e . g . , DRL , and its implications for key scenarios , e . g . , incentive mechanism design , SemCom , IoV networks , channel estimation , error correction coding , and channel denoising . We conclude our tutorial by discussing potential future research directions and summarizing the key contributions . widespread use and success of GDMs in the CV domain , we introduce the principles and theory of GDMs in this context in Section II - B . This is a foundation for our subsequent discussion on how GDMs can be extended to facilitate network optimization in Section II - C . B . Principles of the GDMs Unlike GANs that generate samples from a latent vector in a single forward pass through the Generator network [ 81 ] , GDMs utilize a denoising network to iteratively converge to 6 x 0 x 1 x t - 1 x t x T - 1 x T   1 t t q x x  … …   1 t t p x x   Forward diffusion Reverse diffusion Target Distribution : Approximated Distribution : Learnable Parameters ( Denoising Neural Network ) Fig . 2 : Illustration of the forward and reverse diffusion processes . The forward diffusion process involves the addition of noise , typically Gaussian noise , to the existing training data . Subsequently , the reverse diffusion process , also referred to as “denoising , ” aims to recover the original data from the noise - added version . an approximation of a real sample x ∼ q ( x ) over a series of estimation steps [ 82 ] , where q ( x ) is the data distribution . This unique design has made GDMs emerge as a powerful tool in the field of generative modeling [ 54 ] . As shown in Fig . 2 , the underlying principle of GDMs is simple . With an initial input , GDMs progressively introduce Gaussian noise through a series of steps , i . e . , the forward diffusion process , which generates the targets for the denoising neural network . Subsequently , the neural network is trained to reverse the noising process and recover the data and content [ 19 ] . The reverse diffusion process allows for the generation of new data . We explain the detailed process with an example of image generation for illustration . 1 ) Forward Diffusion Process : The forward diffusion pro - cess can be modeled as a Markov chain with T steps . Let x 0 denote the original image . At each step , i . e . , t , in the Markov chain , a Gaussian noise with a variance of β t is added to x t − 1 to yield x t with the distribution q ( x t | x t − 1 ) . This process is represented as q ( x t | x t − 1 ) = N (cid:16) x t ; µ t = (cid:112) 1 − β t x t − 1 , Σ t = β t I (cid:17) , ( 1 ) where q ( x t | x t − 1 ) is a normal distribution , characterized by the mean µ t and the variance Σ , and I is the identity matrix indicating that each dimension has the same standard deviation β t . Then , from the original data x 0 to the final x T , the posterior probability can be expressed in a tractable form as q ( x 1 : T | x 0 ) = T (cid:89) t = 1 q ( x t | x t − 1 ) ( 2 ) However , according to ( 2 ) , sampling x t ( t ∈ { 0 , 1 , . . . , T } ) necessitates t times of calculation , which becomes computa - tionally intensive when t is large . To avoid this , we define α t = 1 − β t and ¯ α t = t (cid:81) j = 0 α j , enabling us to express x t as x t = (cid:112) 1 − β t x t − 1 + (cid:112) β t ϵ t − 1 = √ α t x t − 2 + √ 1 − α t ϵ t − 2 = · · · = √ ¯ α t x 0 + √ 1 − ¯ α t ϵ 0 , ( 3 ) where ϵ 0 , . . . , ϵ t − 1 ∼ N ( 0 , I ) . Consequently , x t can be obtained using the following distribution : x t ∼ q ( x t | x 0 ) = N (cid:0) x t ; √ ¯ α t x 0 , ( 1 − ¯ α t ) I (cid:1) . ( 4 ) Given that β t is a hyperparameter , we can precompute α t and ¯ α t for all timesteps . This allows us to sample noise at any timestep t and obtain x t . Therefore , we can sample our latent variable x t at any arbitrary timestep . The variance parameter β t can be fixed to a constant or chosen under a β t - schedule [ 19 ] over T timesteps . 2 ) Reverse Diffusion Process : When T is large , x T approx - imates an isotropic Gaussian distribution [ 19 ] . If we can learn the reverse distribution q ( x t − 1 | x t ) , we can sample x T from N ( 0 , I ) , execute the reverse process , and obtain a sample from q ( x 0 ) . However , statistical estimates of q ( x t − 1 | x t ) require com - putations involving the data distribution , which is practically intractable . Therefore , our aim is to estimate q ( x t − 1 | x t ) with a parameterized model p θ as follows : p θ ( x t − 1 | x t ) = N ( x t − 1 ; µ θ ( x t , t ) , Σ θ ( x t , t ) ) . ( 5 ) Subsequently , we can obtain the trajectory from x T to x 0 as p θ ( x 0 : T ) = p θ ( x T ) T (cid:89) t = 1 p θ ( x t − 1 | x t ) . ( 6 ) By conditioning the model on timestep t , it can learn to predict the Gaussian parameters , i . e . , the mean µ θ ( x t , t ) and the covariance matrix Σ θ ( x t , t ) for each timestep . The training of the GDM involves an optimization of the negative log - likelihood of the training data . According to [ 19 ] , adding the condition information , e . g . , g , in the denoising process , p θ ( x t − 1 | x t , g ) can be modeled as a noise prediction model with the covariance matrix fixed as Σ θ ( x t , g , t ) = β t I , ( 7 ) and the mean is constructed as µ θ ( x t , g , t ) = 1 √ α t (cid:18) x t − β t √ 1 − ¯ α t ϵ θ ( x t , g , t ) (cid:19) . ( 8 ) 7 We first sample x T ∼ N ( 0 , I ) and then from the reverse diffusion chain parameterized by θ as x t − 1 | x t = x t √ α t − β t (cid:112) α t ( 1 − ¯ α t ) ϵ θ ( x t , g , t ) + (cid:112) β t ϵ , ( 9 ) where ϵ ∼ N ( 0 , I ) and t = 1 , . . . , T . Furthermore , the authors in [ 19 ] introduced simplifications to the original loss function by disregarding a specific weighting term : L t = E x 0 , t , ϵ (cid:104)(cid:13)(cid:13) ϵ − ϵ θ (cid:0) √ ¯ a t x 0 + √ 1 − ¯ a t ϵ , t (cid:1)(cid:13)(cid:13) 2 (cid:105) . ( 10 ) This effectively shows that instead of predicting the mean of the distribution , the model predicts the noise ϵ at each timestep t . C . Motivations of using GDMs in Network Optimization The motivation of using GDMs in network optimization , particularly in intelligent networks , stems from their unique characteristics and capabilities . First , GDMs possess a robust generative capability , which is suitable in dynamic network optimization with or without expert datasets , i . e . , labeled optimal solutions . Unlike con - ventional applications of GDMs , such as in image or text domains , network optimization does not typically have access to large datasets suitable for offline training [ 83 ] . The lack of an expert dataset presents challenges when applying GDMs to facilitate network optimization . Fortunately , in addressing this challenge , the reverse diffusion process of GDMs , involving a denoising network , can be effectively utilized . Specifically , instead of relying on the standard loss function as illustrated in ( 10 ) , the denoising network can be trained to maximize the value of the final generated solution output [ 38 ] . Here , the value is related to the optimization objective function , which is designed to either maximize or minimize a specific outcome based on the given application . In network optimization , the value can be a performance metric like sum rate , latency , or energy efficiency . This training process can be achieved by executing the generated solution within the network environ - ment , followed by network parameter adjustments based on the received feedback . Thus , the obstacle presented by the absence of a suitable dataset transmutes into an opportunity for dynamic online learning and optimization [ 64 ] . Notably , when expert datasets are accessible , adjustments can be made to minimize the loss between the expert and the generated solutions . These adjustments enable the GDM to continuously refine its output based on loss , leading to progressively more optimized network solutions with higher objective values . Second , GDMs can easily incorporate conditioning infor - mation into the denoising process . In intelligent networks , optimal solutions , e . g . , power allocation schemes and incentive mechanism designs , typically change with the dynamic wire - less environment [ 84 ] . Therefore , the wireless environment information , such as path loss and small - scale fading channel parameters , can be used as the conditioning information in the denoising process [ 85 ] . After sufficient training , the denoising network should be able to generate the optimal solution given any dynamic wireless environment condition [ 38 ] . This ability to adapt to dynamic environments and generate optimal solutions is valuable in wireless network optimization . Furthermore , the relationship between GDMs and DRL in intelligent network optimization is not just the substitution or competition but rather a compliment and / or supplement of each other that allows for mutual enhancement and learning . Specifically , training the denoising network in GDMs , which is guided by feedback from the external environment , embodies a reinforcement learning paradigm [ 38 ] . Thus , techniques such as Q - networks can facilitate more effective training of the denoising network [ 86 ] . Moreover , GDMs can be leveraged to enhance the performance of various DRL algorithms [ 64 ] . For instance , the robust generative capabilities of GDMs can be harnessed in imitation learning , thereby augmenting the performance of offline DRL [ 35 ] , [ 52 ] . In addition , GDMs can substitute the action network in DRL algorithms , where actions are treated as the output of the denoising process [ 50 ] . D . Tutorial with an Example In this part , we representatively formulate an optimiza - tion problem in a wireless network and show a step - by - step tutorial to solve it by using GDMs . We compare the solutions generated by GDMs with the traditional DRL meth - ods , such as Soft Actor - Critic ( SAC ) [ 87 ] and Proximal Policy Optimization ( PPO ) [ 88 ] . The code is available at https : / / github . com / HongyangDu / GDMOPT . 1 ) Problem Formulation : Consider a wireless communi - cation network where a base station with total power P T serves a set of users over multiple orthogonal channels . The objective is to maximize the sum rate of all channels by optimally allocating power among the channels . Let g n denote the channel gain for the n th channel and p n denote the power allocated to that channel . The sum rate of all M orthogonal channels is given by the sum of their individual rates [ 89 ] , which can be expressed as M (cid:88) m = 1 log 2 ( 1 + g m p m / N 0 ) , ( 11 ) where N 0 is the noise level that can be set as 1 without loss of generality for the analysis . The problem is to find the power allocation scheme { p 1 , . . . , p M } that maximizes the capacity C under the power budget and the non - negativity constraints as max { p 1 , . . . , p M } C = M (cid:80) m = 1 log 2 ( 1 + g m p m ) s . t . ,   p m ≥ 0 , ∀ m , M (cid:80) m = 1 p m ≤ P T . ( 12 ) The dynamic nature of the wireless environment presents a sig - nificant challenge , as the values of the channel gains , denoted as { g 1 , . . . , g M } , can fluctuate within a range . This variability is illustrated in Fig . 3 , which depicts the sum rate values for different power allocation schemes and channel gains when M = 3 . It is evident that changes in channel conditions can significantly impact the optimal power allocation scheme . While various solutions have been proposed to address this issue , the following problems exist : 8 ( a ) The three orthogonal channel gains are 1 , 0 . 5 , and 2 . 5 , respectively . ( b ) The three orthogonal channel gains are 3 , 1 , and 3 , respectively . ( c ) The three orthogonal channel gains are 1 , 3 , and 1 , respectively . Fig . 3 : The sum rate values for different power allocation schemes and different channel gains with M = 3 and total power is 10 W . We con observe that the optimal power allocation scheme and the corresponding peak sum rate values keep changing because of the dynamic wireless environment . • Traditional mathematical solutions depend on accurate channel estimation [ 90 ] . However , even with precise estimation , the resources and energy consumed by pilot signals and the algorithm to perform the estimation are considerable and also introduce latency . • Heuristic algorithms [ 91 ] can achieve near - optimal solu - tions ; but they involve multiple iterations in the solution process , leading to increased energy consumption and additional delays . • The water - filling algorithm [ 92 ] , which can optimally solve this problem and provide an upper bound on the achievable sum rate , involves an iterative process to determine the correct number of channels for power allocation . The iteration stems from the fact that power is added to channels until the marginal increase in capacity is equal across all channels , or the power budget is consumed [ 92 ] . This process can be computationally intensive , particularly when dealing with a large number of channels . Given these challenges , AI - based solutions have been pro - posed . For example , despite requiring a certain overhead , DRL allows for direct model deployment once training is complete . The delay in inferring an optimal solution for a given wireless environment is minimal . However , as the performance of the DRL algorithms continues to improve , the model design becomes more complex . For example , the SAC [ 87 ] , a state - of - the - art DRL method , involves five networks , including two Q - networks and their target networks and a policy network , which increases the complexity of the model . As discussed in Section II - C , GDMs are characterized by their simplicity , directness , and robustness . Furthermore , GDMs can easily incorporate the wireless environment as the condition in the denoising process , leveraging their strong generative capacity to generate optimal solutions . For example , the environmental factors such as channel gains and noise , that can influence the optimal solution can be modeled as a vector g in ( 9 ) . 2 ) GDM as the solution : Next , we demonstrate how to solve the problem using GDMs . The GDM is trained to generate a power allocation scheme that maximizes the sum rate . The steps to solve the problem using diffusion models are as follows : 1 ) Solution Space Definition : The first step in wireless network optimization is to define the solution space . The AI - generated solution represents the optimal power allocation scheme that maximizes the sum rate . This scheme is generated by the GDM through a series of denoising steps applied to Gaussian noise . As shown in Algorithm 1 line 2 , in the considered problem , the dimension of the solution vector should be the number of channels , i . e . , M . Then , it should be performed in the wireless environment , as shown in Algorithm 1 lines 3 - 7 . 2 ) Objective Function Definition : The next step is to define the objective function to be maximized or min - imized . In this context , the training objective of the diffusion model is to maximize the sum rate achieved by the GDM - generated power allocation , as shown in Algorithm 1 line 8 . The upper bound can be provided by the water - filling algorithm [ 92 ] . 9 Observe current wireless environment Find the expert solution , i . e . , the optimal solution based on the current state 1 71 35 18 52 0 . 3 0 0 . 1 0 . 2 Channel number T r a n s m it po w e r 1 71 35 18 52 0 . 3 0 0 . 1 0 . 2 Channel number T r a n s m it po w e r Add noise to disrupt the expert solution 0 1 t t t      0 x x  Calculate loss , update solution generation policy network Solution evaluation network D y na m i c e n v i r on m e n t r e p r e s e n t a ti on T i m e R e p r e s e n t a ti on Step 1 . Step 2 . Step 3 . Step 4 . Input Randomly to generate Gaussian noise and time 1 71 35 18 52 0 . 3 - 0 . 3 - 0 . 1 0 . 1 Channel number T r a n s m it po w e r Gaussian noise Time Noise schedule Solution generation network Predict the noise by using the solution generation policy 1 71 35 18 52 0 . 3 0 0 . 1 0 . 2 Channel number T r a n s m it po w e r Step 5 . Input Step 6 . G a u ss i a n no i s e Denoising → U pda t e t h e pa r a m e t e rs o f t h e GD M Expert solution Noise After diffusion Generated solution Observe current wireless environment Step 1 . Solution generation network Predict the noise by using the solution generation policy 1 71 35 18 52 0 . 3 0 0 . 1 0 . 2 Channel number T r a n s m it po w e r G a u ss i a n no i s e Denoising → Generated solution Predict the objective function value after performing the generated solution Step 2 . Step 3 . When the expert dataset is available When the expert dataset is unavailable     2 0 argmin 1 , , t t a a t              x g        0 ~ 0 a r g m i n ( ) , Q          p g p          0 2 ~ 0 0 a r g m i n ( ) , , Q Q r Q            p g p g p   Perform the solution in intelligent network environment , Record the objective function value   0 , r g p Step 4 . Step 5 . U pd a t e t h e s o l u ti on g e n e r a ti on a nd e v a l u a ti on n e t w o r k s ` p a r a m e t e r s acc o r d i ng t o t h e l o ss f un c ti on s Check the availability of the expert dataset A B Fig . 4 : GDM training approaches with and without an expert dataset . Part A illustrates the GDM training scenario when an expert database is accessible . The process learns from the GDM applications in the image domain : the optimal solution is retrieved from the expert database upon observing an environmental condition , followed by the GDM learning to replicate this optimal solution through forward diffusion and reverse denoising process . Part B presents the scenario where no expert database exists . In this case , GDM , with the assistance of a jointly trained solution evaluation network , learns to generate the optimal solution for a given environmental condition by actively exploring the unknown environment . Algorithm 1 Objective function and solution space definitions 1 : procedure C OMPUTE O BJECTIVE ( env state , solutions ) 2 : # solutions . dimension = N 3 : total power ← P T , e . g . , 10 4 : weights ← solutions / sum ( solutions ) 5 : a ← weights ∗ total power 6 : snr ← g n ∗ a 7 : rate ← np . log 2 ( 1 + snr ) 8 : value ← np . sum ( rate ) 9 : # upper bound : water ( g n , total power ) 10 : return value 3 ) Dynamic Environment Definition : In wireless net - works , the channel conditions can vary among different users , resulting in a dynamic and diverse environment . To accommodate this variability , GDM is designed to generate the optimal power allocation scheme corre - sponding to a given set of channel conditions . Thus , we consider a general case that each channel gains , e . g . , g m ( m = 1 , . . . , M ) , change randomly over a range , e . g . , ( 0 . 5 , 2 . 5 ) , as shown in Algorithm 2 . Note that here we consider the general case . In practice , the uniform distribution can also be replaced with a specific channel Algorithm 2 Dynamic Environment Definition 1 : procedure G ENERATE S TATE 2 : env state ← np . zeros ( N ) 3 : env state [ 0 ] ← np . random . uniform ( min , max ) 4 : · · · 5 : env state [ N − 1 ] ← np . random . uniform ( min , max ) 6 : return env state fading distribution , e . g . , Rayleigh , Rician , or Nakagami - m . The upper and lower bounds of the channel gains can be chosen correspondingly as needed . 4 ) Training and Inference : The conditional GDM is pro - posed to generate the power allocation scheme . This approach diverges from back - propagation algorithms in neural networks or DRL techniques that directly opti - mize model parameters . Instead , GDMs strive to gener - ate the optimal power allocation scheme by denoising the initial distribution . The power allocation scheme designed in the given environment is denoted as p . The GDM that maps environment states to power allocation schemes is referred to as the solution generation net - work , i . e . , ϵ θ ( p | g ) with neural network parameters θ . The objective of ϵ θ ( p | g ) is to output a deterministic 10 power allocation scheme that maximizes the expected objective function values as defined in Algorithm 1 . The solution generation network is represented via the reverse process of a conditional GDM , according to ( 9 ) . The end sample of the reverse chain is the final chosen power allocation scheme . According to whether the expert dataset , i . e . , the optimal p under given g , is available , there are two ways to train the ϵ θ : 4 . 1 ) When there is no expert dataset : A solution evaluation network Q υ is introduced , which can assign a Q - value that represents the expected objective function to an environment - power allocation pair , i . e . , g and p . Here , the Q υ network acts as a guidance tool for the training of the GDM network , i . e . , solution generation network ϵ θ . The optimal ϵ θ is the network that generates the power allocation scheme p 0 according to ( 9 ) that has the highest expected Q - value . Thus , the optimal solution generation network can be computed by arg min ϵ θ L ϵ ( θ ) = − E p 0 ∼ ϵ θ [ Q υ ( g , p 0 ) ] . ( 13 ) The training goal of the solution evaluation network Q υ is to minimize the difference between the predicted Q - value by the current network and the real Q - value . Thus , the optimization of Q υ is arg min Q υ L Q ( υ ) = E p 0 ∼ π θ (cid:104) ∥ r ( g , p 0 ) − Q υ ( g , p 0 ) ∥ 2 (cid:105) , ( 14 ) where r denotes the objective function value when the generated power allocation scheme p 0 is performed in the environment g . Then , the network structure for training is shown in Part B of Fig 4 , and the overall algorithm of GDM in sum rate maximization is given in Algorithm 3 . 4 . 2 ) When an expert database is available : In some instances of intelligent network optimization , a dataset of expert solutions might already be available . For example , by applying traditional optimization schemes over time , it is feasible to obtain the optimal power allocation schemes corresponding to various channel conditions . Utilizing this expert dataset , the loss function can be designed to minimize the gap between the generated power allocation and the expert schemes as follows : arg min π θ L ( θ ) = E p 0 ∼ π θ (cid:104) ∥ r ( g , p 0 ) − r exp ( g ) ∥ 2 (cid:105) , ( 15 ) where r exp ( g ) is the objective function value under the given g . To achieve efficient training , we can use a similar process to that used for GDM in the image domain . Let x 0 denote the expert solution r exp . As shown in Part A of Fig 4 , to train GDM by forward diffusion and inverse denoising processes , the optimization of the loss function of the GDM network can be expressed as arg min π θ L ( θ ) = E (cid:104)(cid:13)(cid:13) ϵ − ϵ θ (cid:0) √ ¯ a t x 0 + √ 1 − ¯ a t ϵ , t , g (cid:1)(cid:13)(cid:13) 2 (cid:105) , ( 16 ) where ϵ is the added Gaussian noise , √ ¯ a t x 0 + √ 1 − ¯ a t ϵ denotes the expert solution after the forward diffusion process , and the network ϵ θ can accurately predict the added noise with the inputs including the disrupted expert solution , the timestep information t , and the environment information condition g . After training , when the channel conditions change again , the GDM network ϵ θ is capable of efficiently generating the corresponding optimal solution according to ( 9 ) . Remark 1 . The Algorithm 3 is designed for scenarios where an optimal solution needs to be obtained under specific environmental conditions . However , in intelligent networking , there are many situations where the value of the objective function is not immediately obtained after executing a solution in the environment [ 93 ] , [ 94 ] . A typical example of this is the service provider selection problem , where tasks from users are allocated across various servers , each of which is with unique computing capability [ 64 ] , [ 95 ] , [ 96 ] . The total utility of all users , which is designed as the objective function to be maximized , can only be calculated after a long period of the allocation process . As a result , a decision - making process , such as allocating user tasks to desired servers , has to be modeled by forming a Markov chain [ 97 ] . In such cases , our proposed Algorithm 3 remains useful with minor adjustments . Specifically , the reward part in Algorithm 3 ( lines 7 - 13 ) needs to be adjusted to take into account the dynamics of the Markov chain and add the discount factor in the loss function model . More details on how to do this , along with examples , are discussed in Section III . Remark 2 . In situations where expert strategies are not avail - able for guidance , GDM can leverage a solution evaluation network during the training phase . This is inspired by the Q - network commonly used in DRL [ 98 ] – [ 100 ] . The solution evaluation network estimates the quality of a given solution , e . g . , the power allocation scheme in the discussed example , under specific environmental conditions . This quality assess - ment guides the GDM during its iterative denoising process . Moreover , other advanced techniques from the DRL field can be adopted to make GDM training even more efficient . For example , the double Q - learning technique [ 101 ] , which aims at reducing over - estimation in Q - learning , can be adopted . This approach maintains two Q - networks , using the smaller Q - value for updates , thus offering a conservative estimate and mitigating over - optimistic solution assessments [ 101 ] , [ 102 ] . Incorporating such methods can augment GDM training , promoting robustness and efficiency . 3 ) Insights : To better understand the proposed GDM method , we implemented Algorithm 3 to solve the optimiza - tion problem in ( 12 ) and observed the results . We denote the sum rate obtained by performing the power allocation scheme generated by the GDM in the training process as the test sum rate and use the water - filling algorithm [ 92 ] to obtain the upper bound , i . e . , the achievable sum rate . The experimental platform for running our proposed algorithms was built on a generic Ubuntu 20 . 04 system with an AMD Ryzen Threadripper PRO 3975WX 32 - Cores CPU and an 11 Algorithm 3 GDM in Network Optimization Training Phase : 1 : Input hyper - parameters : denoising step N , exploration noise ϵ 2 : # # Initialize Neural Networks 3 : Initialize solution generation network ε θ with weights θ , solution evaluation network Q υ with weights υ 4 : # # Begin Learning Process 5 : Initialize a random process N for power allocation explo - ration 6 : while not converge do 7 : At the j th time moment , observe the current environ - ment g ( j ) , which can be simulated by using Algorithm 2 8 : Set p N as Gaussian noise . Generate power alloca - tion p ( j ) 0 by denoising p N using ε θ , according to ( 9 ) 9 : Add the exploration noise to p ( j ) 0 10 : Apply the generated power allocation scheme p ( j ) 0 to the environment and observe the objective function value by using Algorithm 1 . 11 : Record the real objective function value r ( j ) (cid:16) g ( j ) , p ( j ) 0 (cid:17) 12 : Update the Q υ according to ( 14 ) 13 : Update the ε θ according to ( 13 ) 14 : return The trained solution generation network ε θ Inference Phase : 1 : Observe the environment vector g 2 : Generate the optimal power allocation p 0 by denoising Gaussian noise using ε θ 3 : return The optimal power allocation p 0 NVIDIA RTX A5000 GPU . First , we considered a scenario with N = 3 channels . The channel gain values were randomly selected from 0 . 5 to 2 . 5 . Note that the upper and lower channel gain limits here can be changed accordingly depending on the actual channel conditions . The number of denoising steps , denoted by T , was set to 9 . We then investigated the impact of different learning rates and β schedulers on the algorithm’s performance . Figure 5 illustrates the gap between achievable and test sum rates against the training epoch . We can observe that the conventional DRL method , i . e . , PPO , exhibits more significant fluctuations and less effective convergence . The challenges are from the problem’s inherent complexity , the environmental variability , or the influence of specific hyperparameters . How - ever , despite these challenges , both GDM methods outperform the PPO method , irrespective of their learning rates . In the first case , the GDM with a learning rate of 0 . 001 , demonstrates the fastest convergence towards zero . This swift learning process underscores the efficiency of the GDM approach . In the second one , the GDM , with a learning rate of 0 . 0005 , converges slower but still approaches zero . This slower convergence rate indicates a more gradual learning process , partly due to the fact that more minor adjustments are made per training iteration . Despite this slower convergence rate , the model still effectively learns the power allocation strategy , demon - 0 50 100 150 200 250 Training Epoch - 0 . 5 - 0 . 45 - 0 . 4 - 0 . 35 - 0 . 3 - 0 . 25 - 0 . 2 - 0 . 15 - 0 . 1 - 0 . 05 0 G a p B e t w ee n A c h i e v a b l e a nd T e s t S u m R a t e GDM - Learning rate : 0 . 0005 GDM - Learning rate : 0 . 001 DRL : PPO Fig . 5 : Test reward curves of GDM - aided and DRL - aided optimization methods under different learning rate values , with the number of channels M = 3 , and the channel gains vary within 0 . 5 and 2 . 5 . 0 20 40 60 80 100 Training Epoch - 0 . 5 - 0 . 45 - 0 . 4 - 0 . 35 - 0 . 3 - 0 . 25 - 0 . 2 - 0 . 15 - 0 . 1 - 0 . 05 0 G a p B e t w ee n A c h i e v a b l e a nd T e s t S u m R a t e GDM - Random Seed : 123 GDM - Random Seed : 312 GDM - Random Seed : 231 Fig . 6 : Test reward curves of GDM - aided optimization meth - ods under different random seed values , with the number of channels M = 3 , and the channel gains vary within 0 . 5 and 2 . 5 . strating the robustness of the GDM approach . This superior performance manifests the GDM’s ability to capture complex patterns and relationships between observations , leading to more accurate action decisions . This ability is advantageous in network optimization problems requiring high - performance , time - efficient , fast - converging solutions . Fig . 6 further shows the robustness of the GDM methods , examining how varying random seeds influence the training performance . The figure delineates three distinct curves , each corresponding to a different random seed . While the random seed is known to significantly sway outcomes in image - related GDM applications such as Stable Diffusion [ 18 ] , our findings reveal a contrasting scenario . After about 50 timesteps , all three cases stabilize , maintaining a gap to zero ( where zero sig - nifies the theoretical upper bound ) within a negligible margin 12 0 50 100 150 200 250 300 350 400 Training Epoch - 0 . 8 - 0 . 7 - 0 . 6 - 0 . 5 - 0 . 4 - 0 . 3 - 0 . 2 - 0 . 1 0 G a p B e t w ee n A c h i e v a b l e a nd T e s t S u m R a t e s Fig . 7 : Test reward curves of GDM - aided and DRL - aided optimization methods , with the number of channels M = 5 , and the channel gains vary within 0 . 5 and 5 . 0 50 100 150 200 250 300 350 400 Training Epoch - 0 . 8 - 0 . 7 - 0 . 6 - 0 . 5 - 0 . 4 - 0 . 3 - 0 . 2 - 0 . 1 0 G a p B e t w ee n A c h i e v a b l e a nd T e s t S u m R a t e GDM - Denoising Step : 12 GDM - Denoising Step : 3 GDM - Denoising Step : 6 Fig . 8 : Test reward curves of GDM - aided optimization meth - ods under different denoising steps , with the number of channels M = 5 , and the channel gains vary within 0 . 5 and 5 . of 0 . 05 . This observation shows that , unlike in image - related applications where identical text prompts can yield vastly different images based on the seed , the random seed’s impact on performance in this context is minimal . This insight high - lights the GDM’s resilience against varying initial conditions , suggesting its consistent ability to learn the power allocation scheme and achieve near - optimal performance , especially in similar network optimization problems . Then we consider a more complex case that the number of channels is 5 and the channel gains of these 5 channels vary within 0 . 5 and 5 . We compare the performance of GDM and DRL algorithms and study the impact of denoising steps . In Fig . 7 , we examine the performance of the GDM method compared to two DRL methods , i . e . , SAC and PPO . All three methods demonstrate convergence , while the final gap values for GDM and SAC are closer to zero , indicating a better power 0 100 200 300 400 500 600 Training Epoch - 15 - 10 - 5 0 G a p B e t w ee n A c h i e v a b l e a nd T e s t S u m R a t e GDM - With Expert Dataset GDM - Without Expert Dataset Average Allocation Random Allocation Fig . 9 : Test reward curves of GDM - aided optimization meth - ods with and without expert dataset , with the number of channels is 71 , i . e . , M = 71 , and the channel gains vary within 2 and 25 . allocation scheme . In contrast , PPO exhibits larger fluctuations and slower convergence . While the final results of GDM and SAC are similar , GDM converges faster , which is attributed to its ability to capture complex patterns and relationships more efficiently . This faster convergence of GDM is particularly beneficial in scenarios where time efficiency is crucial . Furthermore , we study the impact of different denoising steps on the performance of the GDM in Fig . 8 . The fig - ure presents three curves , each corresponding to a different number of denoising steps . The first curve , representing 6 denoising steps , exhibits the fastest convergence . The second curve , corresponding to 3 denoising steps , converges slower . This slower convergence rate could be attributed to insufficient denoising when the number of steps is small , leading to greater uncertainty in generated power allocation schemes . However , when the number of steps is too larger , as in the third curve where the number of denoising steps is 12 , the convergence is slowest . This could be due to the model losing its ability to explore the environment effectively , as excessive denoising might lead to overfitting the training data . This analysis underscores the importance of carefully selecting the number of denoising steps in the GDM , striking a balance between sufficient denoising and maintaining the GDM’s ability to explore the environment . Fig . 9 shows the test reward curves for GDM - aided opti - mization methods , both with and without access to an expert dataset , in a scenario with 71 channels , i . e . , M = 71 , and channel gains varying between 2 and 25 . The figure further validates the efficacy of the GDM approaches , irrespective of the availability of the expert dataset . Using an expert dataset in GDM training significantly accelerates the convergence process . However , even without an expert dataset , the GDM approach can independently decrease the gap between the achieved sum rate and the upper bound . Furthermore , two straightforward power allocation schemes , namely average and random allocation , are also presented for comparison . 13 ( a ) The first denoising step 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 T r a n s m it po w e r ( W ) ( b ) The second denoising step 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 0 . 4 T r a n s m it po w e r ( W ) ( c ) The third denoising step 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 T r a n s m it po w e r ( W ) ( d ) The fourth denoising step 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 T r a n s m it po w e r ( W ) ( e ) The fifth denoising step 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 T r a n s m it po w e r ( W ) ( f ) The optimal power allocation scheme 0 20 40 60 Channel number 0 0 . 1 0 . 2 0 . 3 T r a n s m it po w e r ( W ) Fig . 10 : Sub - figures ( a ) to ( e ) illustrate the process of 5 - step denoising Gaussian noise into the transmit power allocation schemes using a well - trained GDM . Here , we consider 71 channels with the total transmission power of 12 W . In these 71 channels , the channel gains differ randomly . Some channels fall within the range of 2 to 5 , others between 10 to 15 , and the remaining channels exhibit gains varying from 20 to 25 . We simulate using a set of observations obtained by random sampling . Sub - figure ( f ) is the optimal power allocation scheme obtained by the water - filling algorithm [ 92 ] . Average allocation , which evenly distributes power among the channels , outperforms random allocation , which arbitrarily assigns power . However , GDM , with its advanced learning capability , outperforms both strategies . Fig . 10 visualizes the process of the well - trained GDM generating the power allocation scheme from the Gaussian noise . We consider 71 channels with a total transmission power of 12 W , where the specific channel gains of the 71 channels randomly vary between ( 2 , 5 ) , ( 10 , 15 ) , or ( 20 , 25 ) . Figs . 10 ( a ) - ( e ) show the progressive refinement of the power allocation scheme through the denoising process . Fig . 10 ( f ) presents the optimal power allocation scheme obtained by the water - filling algorithm [ 92 ] . This series of figures demonstrates the capability of GDM to generate near - optimal power allocation schemes through iterative denoising , even when confronted with complex and variable channel conditions . It also high - lights the close agreement between the GDM - generated and water - filling algorithm - generated power allocation schemes , emphasizing the effectiveness of GDM in learning and imi - tating expert solutions . The gap between the sum rate under the power allocation scheme shown in Fig . 10 ( e ) and the upper bound is 0 . 11 bit / s / Hz . Lesson Learned : From the above showcase discussions , we glean several insights into the application of GDMs in network optimization . Firstly , the superior performance of GDMs over traditional DRL methods underscores the transformative potential of GDMs in complex optimization tasks . This is particularly notable in scenarios where rapid convergence and high performance are paramount . Secondly , the learning - related parameters in GDM , such as learning rates and denoising steps , facilitate a novel balance between exploration and exploitation . Notably , the denoising process , acting as a pivotal mechanism in GDMs , introduces a fresh perspective to this classic trade - off in RL as we discussed in Fig . 8 . Thirdly , the resilience of GDMs to varying initial conditions and their consistent near - optimal performance , even in the absence of an expert dataset , show the robustness and adaptability . This robustness is particularly crucial in real - world applications where conditions can be unpredictable and data may be imperfect or incomplete . Lastly , the ability of GDMs to generate near - optimal power allocation schemes that are closely aligned with expert solutions underscores their capacity for sophisticated pattern recognition and imitation . This suggests that GDMs can be used as a powerful tool for learning from and leveraging expert knowledge in complex domains in network optimization tasks . III . D EEP R EINFORCEMENT L EARNING This section first discusses the DRL algorithms and their applications in network optimization . Then , the integration of GDMs within DRL and a case study on AIGC service provider selection in edge networks are studied . A . Fundamentals of DRL DRL is a powerful approach that combines the strengths of both deep learning and reinforcement learning , enabling the development of algorithms capable of learning to make optimal decisions through interactions with their environ - ment [ 103 ] , [ 104 ] . The DRL framework comprises two main components : the agent and the environment [ 105 ] . The agent , a decision - making entity , learns to interact optimally with 14 the environment to maximize a cumulative reward [ 106 ] . The environment provides feedback to the agent in the form of rewards based on the actions taken by the agent [ 107 ] . This interaction forms the basis of the learning process in DRL . We summarize several representative DRL algorithms as • Deep Q - Network ( DQN ) : DQN uses a deep neural network for approximating the Q - value function , enabling it to handle high - dimensional state spaces . However , it struggles with high - dimensional or continuous action spaces [ 108 ] . • Prioritized DQN : This variant of DQN prioritizes ex - periences with high temporal - difference error , leading to faster learning but introducing additional complex - ity [ 109 ] . • Deep Recurrent Q - Network ( DRQN ) : DRQN extends DQN with recurrent neural networks for tasks requiring memory of past information , which is however challeng - ing to train [ 110 ] . • PPO : PPO is a stable policy gradient method that keeps policy updates close to zero , which however may require more samples to learn effectively [ 88 ] , [ 111 ] . • REINFORCE : REINFORCE directly optimizes the pol - icy function , making it widely applicable but suffering from high variance [ 112 ] . • SAC : SAC maximizes both the expected return and the policy’s entropy , leading to better performance in complex environments at the cost of computational com - plexity [ 87 ] . • Rainbow : Rainbow combines seven DQN improvements , enhancing performance but increasing implementation complexity [ 113 ] . In the context of wireless communications , DRL offers several advantages . First , DRL is adept at handling complex network optimization problems , enabling network controllers to find optimal solutions even without complete and precise network information [ 103 ] , [ 114 ] . This strength is further complemented by DRL’s capacity to enable network entities to learn and accumulate knowledge about the communica - tion and networking environment . This facilitates learning optimal policies without knowing the channel model and mobility pattern [ 103 ] , [ 115 ] . Furthermore , DRL supports autonomous decision - making , reducing communication over - heads and boosting network security and robustness [ 64 ] , [ 116 ] . Given these advantages , DRL has found extensive applica - tions in network optimizations [ 117 ] . However , it is important to note that DRL also has its limitations , which , however , may be mitigated by the introduction of GDMs : • Sample Inefficiency : DRL often requires a large num - ber of interactions with the environment to learn effec - tively , which can be computationally expensive and time - consuming [ 103 ] . GDMs , with the strong ability to model complex data distributions , could reduce the number of samples required . • Hyperparameter Sensitivity : The performance of DRL algorithms can be significantly influenced by hyperpa - rameters , demanding meticulous tuning for diverse tasks [ 118 ] . GDMs , with their flexible structure and adaptabil - ity to various data distributions , could provide a more robust solution . • Difficulty in Modeling Complex Environments : DRL algorithms may struggle with environments characterized by complex and high - dimensional state and action spaces . By accurately capturing the underlying data distributions , GDMs could provide a more efficient representation of the environment . • Instability and Slow Convergence : DRL algorithms may suffer from instability and slow convergence . The unique structure of GDMs involves a diffusion process , potentially offering a more stable and efficient learning process . B . Applications of GDM in DRL The distinctive characteristics of GDMs have been effec - tively utilized to enhance DRL . These advantages include high expressiveness , the ability to capture multi - modal ac - tion distributions , and the potential to integrate with other RL strategies seamlessly . One notable application of GDMs in DRL is presented in [ 50 ] , where the authors introduced Diffusion Q - learning ( Diffusion - QL ) . This innovative method utilized a GDM as the policy representation , more specifically , a DDPM [ 19 ] based on a Multilayer Perceptron ( MLP ) . The authors incorporated the Q - learning guidance into the reverse diffusion chain , facilitating optimal action selection . Through this integration , they demonstrated the expressiveness of GDMs in capturing multi - modal action distributions and showcased their effectiveness in enhancing behavior cloning and policy improvement processes . As a result , Diffusion - QL surpassed previous methods across several D4RL benchmark tasks [ 119 ] for offline RL . Complementarily , the work in [ 51 ] improves offline RL further by addressing the limitations of distributional expressivity in policy models . In contrast to the approach in [ 50 ] , the authors in [ 51 ] decoupled the learned policy into a generative behavior model and an action evaluation model . This separation facilitated the introduc - tion of a diffusion - based generative behavior model capable of modeling diverse behaviors such as agent’s trajectories . The optimal selection of actions from this behavior model was achieved through importance sampling in concert with an action evaluation model . They also incorporated an in - sample planning technique to mitigate extrapolation error and enhance computational efficiency . The resulting methodol - ogy outperformed traditional offline RL methods on D4RL datasets [ 119 ] and showed proficiency in learning from het - erogeneous datasets . These highlighted studies represent just a subset of the burgeoning body of work on GDMs in DRL . For an extended discussion , Table II reviews various key contributions and their impacts . In summary , the integration of GDMs into DRL , as demon - strated by these representative studies and further summarized in Table II , leverages several key advantages offered by GDMs . The key advantages that GDMs offer to address the disadvantages of DRL as we discussed in Section III - A are listed below : 15 Paper Key Contributions Results [ 120 ] Leverage Language Augmented Diffusion ( LAD ) mod - els for language - based skills in RL Achieve an average success rate of 72 % on the CALVIN language robotics benchmark [ 50 ] Propose Diffusion Q - learning ( Diffusion - QL ) for of - fline RL and represent the policy as a GDM Achieve state - of - the - art performance on the majority of D4RL benchmark tasks [ 51 ] Decouple policy learning into behavior learning and action evaluation and introduce a generative approach for offline RL Achieve superior performance on complex tasks such as AntMaze on D4RL [ 49 ] Develop a diffusion probabilistic model for trajectory optimization and introduce a model directly amenable to trajectory optimization Demonstrate effectiveness in control settings emphasiz - ing long - horizon decision - making and test - time flexi - bility [ 37 ] Introduce Contrastive Energy Prediction ( CEP ) for learning the exact guidance in diffusion sampling Demonstrate effectiveness in offline RL and image synthesis , outperforming existing state - of - the - art algo - rithms on D4RL benchmarks [ 35 ] Propose a robust version of the Diffusion Implicit Models ( DIMs ) for better generalization to unseen states in RL Show the new approach provides more stable policy improvement and outperforms the baseline DIM meth - ods on various complex tasks [ 121 ] Treat procedure planning as a distribution fitting prob - lem , remove the expensive intermediate supervision and use task labels instead Achieve state - of - the - art performance on three instruc - tional video datasets across different prediction time horizons without task supervision [ 122 ] Introduce the Equivariant Diffuser for Generating Inter - actions ( EDGI ) , an algorithm for MBRL and planning Improve sample efficiency and generalization in 3D navigation and robotic object manipulation environ - ments [ 123 ] Propose a general adversarial training framework for multi - agent systems using diffusion learning , enhanc - ing robustness to adversarial attacks Demonstrate enhanced robustness to adversarial attacks in simulations with FGM and DeepFool perturbations [ 52 ] Introduce a new imitation learning framework that leverages both conditional and joint probability of the expert distribution , and explore the use of different generative models in the framework Outperform baselines in various continuous control tasks including navigation , robot arm manipulation , dexterous manipulation , and locomotion [ 124 ] Introduce a self - evolving method for diffusion - based planners in offline reinforcement learning , demonstrat - ing an ability to improve planning performance for both known and unseen tasks Outperform the previous state - of - the - art Diffuser by 20 . 8 % on Maze2D and 7 . 5 % on MuJoCo locomotion , and show better adaptation to new tasks , e . g . , KUKA pick - and - place , by 27 . 9 % [ 125 ] Introduce innovations for diffusion models in sequen - tial environments Accurately model complex action distributions , outper - form state - of - the - art methods on a simulated robotic benchmark , and scale to model human gameplay in complex 3D environments [ 48 ] Apply conditional generative modeling to the problem of sequential decision - making and investigate condi - tioning on constraints and skills Outperform existing offline RL approaches and demon - strate the flexible combination of constraints and com - position of skills at test time TABLE II : Extended summary of papers on GDM in DRL • Expressiveness : GDMs are capable of modeling complex data distributions , making them well - suited for represent - ing policies in DRL [ 126 ] . For instance , in a dynamic traffic routing scenario , the policy needs to adapt to various traffic conditions , road structures , and vehicle behaviors [ 127 ] . GDMs can effectively model such a policy . • Sample Quality : GDMs are known for generating high - quality samples [ 23 ] , [ 128 ] . In the context of DRL , this translates into the generation of high - quality actions or strategies [ 129 ] . For example , in a network resource allocation task , the quality of the generated allocation de - cisions directly impacts the network performance . GDMs can generate high - quality decisions , leading to improved 16 network performance . • Flexibility : The ability of GDMs to model diverse be - haviors is particularly useful in DRL , where the agent needs to adapt to a variety of situations and tasks [ 130 ] . In a network management task , for instance , the network may need to adapt to various traffic conditions and user demands . GDMs can model a wide range of behaviors , enabling the network to adapt to these diverse conditions . • Planning Capability : GDMs can be used for planning by iteratively denoising trajectories , providing a novel per - spective on the decision - making processes in DRL [ 52 ] . For example , a DRL agent could use a GDM to plan the network operations , iteratively refining the plan to optimize the network efficiency [ 124 ] , [ 125 ] . While GDMs offer promising advantages in DRL , they also present certain challenges . The iterative nature of GDMs can lead to increased computational complexity , which could be a hurdle in large - scale DRL tasks such as optimizing city - wide communication networks [ 52 ] . Additionally , GDMs may struggle to accurately model certain data distributions , espe - cially those with high noise levels or irregularities . This could pose challenges in DRL tasks involving real - world network traffic data , which may contain stronhg noise and outliers [ 33 ] . While these challenges underline the limitations of GDMs , they also present opportunities for innovative approaches that can effectively harness the benefits of GDMs while mitigating their shortcomings . Leveraging GDMs within advanced DRL algorithms offers a promising solution to both computational complexity and modeling limitations . An example could be found in combining GDMs with SAC [ 64 ] , a state - of - the - art DRL method known for its efficient learning and robustness . This combination capitalizes on the strength of GDMs in modeling complex action distributions while utilizing the optimization capabilities of SAC , yielding a hybrid model with the potential for enhanced performance and efficiency in complex network optimization tasks . To illustrate this , we delve into a case study , introducing an innovative combination of GDM and SAC . C . Case Study : AIGC Service Provider Selection 1 ) System Model : The AIGC service provider selection problem depicted in Fig . 11 and detailed in [ 64 ] , can be regarded as an extension of the resource - constrained task assignment problem . This is a well - known challenge in wire - less networks where resources are scarce and their efficient utilization is critical to achieving the desired performance [ 131 ] . Specifically , we consider a set of sequential tasks and available ASPs , each of which possesses a unique utility function . The objective is to assign users’ AIGC tasks to ASPs in a way that maximizes the overall user utility . This user utility is a function of the required computing resource for each task and it is related to the AIGC model that performs the task . In addition , we acknowledge that the computing resources of each ASP is limited . From a mathematical perspective , the ASP selection prob - lem can be modeled as an integer programming problem , with the decision variables representing the sequence of task … … Edge Server 1 Edge Server 2 Edge Server I User 1 User 2 User J … … I want to see ` ` A dress to wear with high heels . ` ` ASP 1 ASP 2 ASP I AIGC Service Provider Selection Problem U p li n k : T a s k s , R e qu i r e d R e s ou r ce D o w n li n k : F i n i s h e d T a s k s AIGC - as - a - Service Deploy the trained AIGC model to the network edge server Fig . 11 : AIGC service provider selection problem . Following the paradigm of “AIGC - as - a - Service” , various ASPs deploy their AIGC models onto network edge servers . With user requests arriving , an optimal task scheduler should be designed for real - time user task allocation . The goal is to maximize total user QoE , considering the unique capabilities of each AIGC model and the computing resource constraints of edge servers [ 64 ] . Critic 1 Critic 2 Double Critic Target Critic Critic 1 Critic 2 GDM - based Network Q eval Q target Target Actor GDM - based Network Actor Soft Update Optimizer Critic Loss Update GDM - based Network P T P t P 0 … … Reverse Diffusion Chain Execute Action G au ss i an N o i s e O p ti m a l A c ti on Actor v Probability Action v Sampling AaaS Environment Observation s l Experience Replay Memory Observation s l + 1 Reward r l T r a j ec t o r y C o ll ec t i o n ( s l , a l , s l + 1 , r l ) Data Batch 0 10 20 0 . 5 1 . 0 Fig . 12 : The overall architecture of the D2SAC algorithm [ 64 ] . assignments to available ASPs . The formulation also incor - porates constraints that capture the limitations on available resources . Failing to meet these constraints can have severe consequences , such as the crash of an ASP and the subsequent termination and restart of its running tasks . 2 ) GDM - based Optimal Decision Generation : The authors in [ 64 ] applied GDM to the actor - critic architecture - based DRL paradigm and propose the Deep Diffusion Soft Actor - Critic ( D2SAC ) as a deep diffusion reinforcement learning algorithm . As shown in Fig . 12 , the D2SAC algorithm in - corporates several key components to optimize the policy , including an actor network , a double critic network , a target 17 actor , a target critic , an experience replay memory , and the environment . Here’s a summary and explanation of these components and their roles : • Trajectory Collection : The agent observes the environ - ment and collects transitions of state by executing actions in the environment . These transitions are regarded as ex - periences and are added to the experience replay memory . The actor network generates an action distribution over all possible actions given an environment observation and samples an action from this distribution . This action is performed , transitioning to a new state and returning an immediate reward as feedback . • GDM as the Policy : The core of the actor network is the GDM , which effectively encodes the observation’s representation . It captures the dependencies between the observation and the action space . • Experience Replay Memory : This is a method to handle the delay in receiving reward feedback . Experiences are stored and the missing reward is filled in later before updating the GDM - based network . Off - policy training is used to improve the handling of delayed feedback [ 132 ] . • Double Critic Network : During the policy improvement process , the actor network is optimized by sampling mini - batches of transitions from the experience replay memory . The double critic network , composed of two separate critic networks , is used to reduce the overestimation bias by providing a conservative estimate of the Q - value function [ 101 ] . • Policy Improvement : The actor learns to maximize the expected cumulative reward for each action at the current state . The maximization problem is solved using the gradient ascent algorithm [ 133 ] . Specifically , gradients are calculated over a mini - batch of transitions sampled from the experience replay memory , and the actor net - work is updated by performing gradient descent on these gradients . • Action Entropy Regularization : An entropy regular - ization term is introduced to prevent the policy from becoming overly confident in certain actions and con - verging prematurely to a suboptimal solution [ 134 ] . This encourages exploration . • Q - function Improvement : The Q - function , used for es - timating the future rewards of actions , must be accurately estimated for successful optimization . To achieve this , the Temporal Difference ( TD ) error between two Q networks is minimized during training [ 135 ] . Next , we discuss the performance of D2SAC and compare it with seven DRL algorithms as discussed in Section III - A . Furthermore , we demonstrate the efficacy of D2SAC across various benchmark tasks within the DRL domain . 3 ) Numerical Results : Fig . 13 shows the performance of D2SAC compared to benchmark reinforcement learning algo - rithms : DQN , DRQN , Prioritized - DQN , and Rainbow ( Fig . 13 a ) ; and REINFORCE , PPO , and SAC ( Fig . 13 b ) . Across both figures , D2SAC’s reward acquisition over time demonstrates its superior ability to balance exploration and exploitation , resulting in more optimal policy decisions . Table III presents comparative performance metrics of var - ious control tasks in the Gym environment [ 137 ] • Acrobot - v1 : A two - link pendulum simulation , with the goal of maintaining an upright position . The reward system is designed to favor lesser negative values . • CartPole - v1 : A cart - pole system model , where the ob - jective is to prevent a pole from falling . The performance measure here is the average reward , with higher values being desirable . • CoinRun - v0 : A platform game task where the agent’s goal is to collect a coin while avoiding obstacles . The performance is gauged through the average reward per episode , aiming for higher values . • Maze - v0 : A maze navigation task , where reaching the goal while taking fewer steps is rewarded . Similar to the previous tasks , higher average reward values indicate better performance . These benchmarks cover a diverse range of problems , includ - ing physics - based control ( Acrobot - v1 , CartPole - v1 ) , strategy ( CoinRun - v0 ) , and pathfinding ( Maze - v0 ) . A closer examina - tion of the table reveals that D2SAC significantly outperforms most of the compared policies on these tasks . Specifically , for the Acrobot - v1 task , D2SAC achieves the least negative reward , implying superior performance in the complex task of manipulating the two - link pendulum . In the CartPole - v1 and CoinRun - v0 tasks , D2SAC matches the top - performing algorithms with perfect average rewards of 500 and 10 , respec - tively , indicating a consistent ability to keep the pole upright and successfully collect coins in the platform game . The per - formance on Maze - v0 , although not the highest , is competitive and within the performance range of top - performing policies . IV . I NCENTIVE M ECHANISM D ESIGN In this section , we investigate the applicability of GDM for shaping robust and efficient incentive mechanisms in network designs . A . Fundamentals of Incentive Mechanisms Incentive mechanism [ 64 ] , [ 85 ] plays an important role in network optimization for maintaining the network opera - tionality and long - term economic sustainability . Specifically , the mechanism rewards the network participants who share computing , communication , and information resources and services . Take CrowdOut [ 138 ] , a mobile crowdsourcing sys - tem for road safety , as an example . Drivers ( using smartphones or vehicular sensors ) can report road safety situations that they experience in their urban environments , e . g . , speeding , illegal parking , and damaged roads , to the central manage - ment center . However , the drivers consume their computing and communication resources , e . g . , battery power , CPU , and wireless bandwidth , to sense and report issues . They might be discouraged from actively joining such cooperations without appropriate rewards , especially in the long term . Accordingly , the incentive mechanisms aim at answering the following series of questions : 1 ) how to encourage the network entities to behave in a certain way that is beneficial to the network , e . g . , through the use of rewards , reputation , or credit [ 139 ] , 18 0 2 4 6 8 10 Environment Steps 10 5 0 100 200 300 400 500 R e w a r d s DQNPrioritized - DQN DRQNRainbbowD2SAC ( a ) D2SAC vs DQN , DRQN , Prioritized - DQN , and Rainbow 0 2 4 6 8 10 Environment Steps 10 5 0 100 200 300 400 500 R e w a r d s PPOREINFORCESACD2SAC ( b ) D2SAC vs REINFORCE , PPO , and SAC Fig . 13 : Comparison of test reward curves of D2SAC and benchmarks in the AIGC service provider selection task [ 64 ] TABLE III : Performance Comparisons on General Benchmark Tasks . Policy Acrobot - v1 CartPole - v1 CoinRun - v0 Maze - v0 DRL DQN - 81 . 81 ± 17 . 19 499 . 80 ± 0 . 14 6 . 00 ± 4 . 90 3 . 00 ± 4 . 58 Prioritized - DQN - 105 . 20 ± 14 . 74 498 . 70 ± 1 . 43 5 . 00 ± 5 . 00 2 . 00 ± 4 . 00 DRQN - 82 . 26 ± 14 . 34 132 . 50 ± 69 . 79 − − REINFORCE - 104 . 80 ± 14 . 51 500 . 00 ± 0 . 00 0 . 00 ± 0 . 00 0 . 00 ± 0 . 00 PPO - 77 . 22 ± 8 . 45 499 . 90 ± 0 . 33 0 . 00 ± 0 . 00 2 . 00 ± 4 . 00 Rainbow - 158 . 10 ± 55 . 48 478 . 30 ± 29 . 28 5 . 00 ± 5 . 00 2 . 00 ± 4 . 00 SAC - 121 . 00 ± 35 . 31 500 . 00 ± 0 . 00 10 . 00 ± 0 . 00 3 . 00 ± 4 . 58 Online [ 136 ] , [ 137 ] A2C - 86 . 62 ± 25 . 10 499 . 90 ± 1 . 67 − − ACER - 90 . 85 ± 32 . 80 498 . 62 ± 23 . 86 − − ACKTR - 91 . 28 ± 32 . 52 487 . 57 ± 63 . 87 − − PPO2 - 85 . 14 ± 26 . 27 500 . 00 ± 0 . 00 − − DQN - 88 . 10 ± 33 . 04 500 . 00 ± 0 . 00 − − TRPO − 485 . 39 ± 70 . 51 − − PPO + IMPALA − − 8 . 95 9 . 88 Rainbow + IMPALA − − 5 . 50 4 . 24 Ours D2SAC - 70 . 77 ± 4 . 12 500 . 00 ± 0 . 00 10 . 00 ± 0 . 00 7 . 00 ± 4 . 58 2 ) how to motivate the contribution of resources , 3 ) how to discourage and prevent the malicious behavior , and 4 ) how to ensure the fairness . To do so , the incentive mechanisms should be designed to satisfy several properties , including but not limited to Individual Rationality ( IR ) , Incentive Com - patibility ( IC ) , fairness , Pareto Efficiency ( PE ) , Collusion Resistant ( CR ) , and Budget Balance ( BB ) [ 140 ] . With years of research , various incentive mechanisms have been presented and widely adopted in network optimization . We consider the following representative techniques for developing incentive mechanisms , including Stackelberg game , auction , contract theory , and Shapley value . 1 ) Stackelberg Game : In game theory , the Stackelberg game refers to an iterative process , in which a leader makes the first move and the remaining followers move sequentially , until reaching the equilibrium [ 141 ] . In the network context , the leader , typically a network operator , first determines the resource prices or service charges . Network users , i . e . , follow - ers , then determine their resource demands based on the given prices , with a goal of balancing their utility against the cost that they paid for the resources . At the Stackelberg equilib - rium , the followers cannot increase their utility by changing their demands , and the leader cannot increase its profit by altering the price . In this way , the network efficiency and the participants’ utilities can be balanced , thereby promoting the efficient cooperation . With wide adoption , the Stackelberg game provides a robust foundation for designing network incentive mechanisms . 2 ) Auction : An auction mechanism is widely adopted for incentivizing resource trading [ 142 ] . Specifically , an auction - eer conducts an auction for trading network resources , e . g . , bandwidth or computing power , that are subject to allocation among bidders . The auction process begins with the auctioneer announcing the resources to be traded and soliciting bids . Each 19 bidder evaluates its demand and willingness to pay , submitting a bid accordingly . The auctioneer then chooses a subset of bidders as the winners based on the bid amount or more complex rules . Finally , the auctioneer calculates the payment from each winner , which could be the bid amount or another value depending on the auction type , and performs the resource allocation . Auctions can foster competition among bidders , aiming to maximize the social welfare in terms of network utilities while satisfying certain constraints like budget bal - ance , i . e . , the auctioneer’s revenue should be positive . 3 ) Contract Theory : Contract - theoretic incentive mecha - nisms can effectively address network information asymme - try [ 143 ] . In this setup , an employer ( typically the network operator or service provider ) and an employee ( the network user ) engage in a contractual agreement . The employer designs contracts specifying service charges , Quality of Service ( QoS ) levels , and resource allocations . However , it may not have complete information about the employees’ preferences and behaviors , which is called information asymmetry [ 143 ] . With contract theory , the employers can launch a series of contracts , which ensures the IR , i . e . , the utility of the employee is higher than the threshold and IC , i . e . , the employees can acquire the highest utility by faithfully following the contracts that they signed properties of the employees . Hence , the employees behave honestly , driven by utilities , circumventing the undesirable effects , such as selfish strategies , caused by the information asymmetry . Contract - theoretic incentive mech - anisms have been widely adopted in various network scenarios and have many variants to support high - dimension resource allocation , heterogeneous employees , etc . 4 ) Shapley Value : The Shapley Value ( SV ) is a solution from cooperative game theory , quantifying a player’s marginal contribution across potential coalitions . In the incentive mech - anism design , the players contribute to the network and are subject to being rewarded . Hence , SV for each player , denoted by i , can be defined as SV ( i ) = (cid:88) S ⊆ N \ i | S | ! ( | N | − | S | − 1 ) ! | N | ! [ v ( S ∪ i ) − v ( S ) ] , ( 17 ) where S represents a coalition without i , v represents the value function , n is the total number of players . SV can be used to allocate rewards , reputation , or credits , in which the player contributing more resources to the network will have higher SVs , thereby encouraging cooperation and resource contribution to the network . B . Applications of GDM in Incentive Mechanism Design From the above description , we can observe that the overall procedure of incentive mechanism design is to model the participants’ utility and thus formulate an optimization prob - lem under constraints . Hence , the problem becomes solving an optimization and finding the optimal incentive mechanism strategies that can maximize the utility . Traditionally , re - searchers find the optimal solutions following the optimization principle . Nonetheless , this method requires complete and ac - curate information about the network and , more importantly , is not applicable to complex network scenarios with complicated utility functions . Thanks to the strong ability to model complex environments , GDMs provide new possibilities for solving optimization problems . A typical process of adopting GDMs to design incentive mechanisms contains the following steps . • Model the network states : The first step is to model the network states . To do so , we typically use a vector , say e , which contains many factors , e . g . , the upstream and downstream bandwidth , number of participants , bit error rate , and other scenario - specific factors , to depict the given network environment . • Formulate the utilities of participants : Based on the factors in e and other hyperparameters , e . g . , the weights of these factors , we can formulate the utility function , as well as the associated constraints . Generally , the incentive mechanism design problem is to maximize the utility while satisfying all the constraints . • Customize the GDM settings : Thirdly , we customize the GDM settings according to the incentive mechanism design task . The solution space is the universe of all the possible incentive mechanism strategies . For instance , the action space contains all the possible contracts in the contract - theoretic incentive mechanism . The objective function takes the value of the utility function acquired in Step 2 if all the constraints are satisfied . Otherwise , it takes a large negative value as the constraint violation punishment . The dynamic environment is the vector e . • Train GDM and perform inference : Finally , we can perform GDM training . The well - trained GDM can then be used for finding the optimal incentive mechanism design in any given network state e . The details of the training process are elaborated in Section II - D . C . Case Study : GDM - based Contract - Theoretic Incentive Mechanism 1 ) Background : In this part , we conduct a case study to illustrate how to apply GDMs in a practical incentive mech - anism design problem . Specifically , we consider an emerging network scenario , namely mobile AIGC [ 85 ] , [ 144 ] . Currently , the success of ChatGPT ignited the boom of AIGC , while the substantial resource costs of large AIGC models pre - vent numerous end users from enjoying the easy - accessible AIGC services . To this end , researchers recently presented the concept of mobile AIGC , employing Mobile AIGC Service Providers ( MASPs ) to provide low - latency and customized AIGC inferences , leveraging mobile communications and edge computing capabilities . Hence , the mobile AIGC network is composed of users and MASPs . The former requests AIGC services from MASPs , and the latter operates the local AIGC models to perform inferences . Given that AIGC inferences are resource - intensive , we utilize contract theory to design an incentive mechanism that rewards the MASPs according to their contributed resources . 2 ) System Model : Considering the diversity and hetero - geneity of the current AIGC models , we divide all MASPs into Z levels according to the complexity of their local models , i . e . , from level - 1 to level - Z . The model complexity of each level of MASPs ( denoted by θ 1 , . . . , θ Z ) can be quantified from dif - ferent aspects , such as the number of model parameters [ 145 ] . 20 Typically , the higher the model complexity , the powerful the model is , and simultaneously , the more computing resources are required during the inference [ 146 ] . In our system , we let the index of level follow the ascending order of model complexity , i . e . , the higher the model complexity , the higher the index . Finally , we use p z to denote the proportion of level - z ( z ∈ { 1 , 2 , . . . , Z } ) MASPs in the entire mobile AIGC network . 3 ) Utility Formulation : For simplicity , we assume users evaluate the AIGC services using the most fundamental metric , i . e . , the service latency . Considering the heterogeneity of MASPs , the expected service quality and the required service fees for different levels of MASPs are different . Hence , the utility of users towards level - z ( z ∈ { 1 , 2 , . . . , Z } ) MASPs can be defined as [ 143 ] U z U = (cid:2) α 1 ( θ z ) β 1 − α 2 ( L z / L max ) β 2 ] − R z , ( 18 ) where (cid:2) α 1 ( θ z ) β 1 − α 2 ( L z / L max ) β 2 ] is a complexity - latency metric [ 143 ] , indicating the revenue that the client can gain . L z is the latency requirement of users for level - z MASPs , while L max is the maximum expected latency . α 1 , α 2 , β 1 , and β 2 are weighting factors . R z represents the rewards that users need to pay for level - z MASPs . For MASPs , they sell the computational resources by per - forming AIGC inferences for users . Therefore , the utility of level - z MASPs can be defined as U z SP = R z − (cid:104) ( L max − L z ) L z · θ z (cid:105) , ( 19 ) where (cid:104) ( L max −L z ) L z · θ z (cid:105) represents the costs of level - z MASPs , which is determined by two factors , the model complexity θ z and the latency L z . Firstly , with θ z fixed , the higher the L z , i . e . , longer latency can be tolerated by the users , the smaller the costs . Meanwhile , the larger the θ z , the larger the costs of MASPs , since we have mentioned that complex models typically consume more resources for inference . 4 ) GDM - based Optimal Contract Generation : Based on the above descriptions , we design the following contract - theoretic incentive mechanism . Specifically , the users produce a specific contract , formed by { L z , R z } ( z ∈ { 1 , 2 , . . . , Z } ) , for each level of MASPs , which then decide whether to sign . The contract design should be optimal , maximizing U C while satisfying the IR and IC constraints , i . e . , max L z , R z Z (cid:80) z = 1 p z U z U ( L z , R z , θ z ) , s . t . ( IR ) : U z SP ( L z , R z , θ z ) ≥ U th , z ∈ { 1 , . . . , Z } , ( IC ) : U z SP ( L z , R z , θ z ) ≥ U z SP ( L j , R j , θ z ) , z , j ∈ { 1 , . . . , Z } , z ̸ = j , ( 20 ) where U th is the utility lower bound for MASPs . Finally , we apply the aforementioned four - step procedure to formulate the GDM training paradigm and find the optimal contract design . • Model the network state : For simplicity , we consider two types of MASPs in the mobile AIGC network . Hence , the network state vector in our case is defined as [ n , L max , p 1 , p 2 , θ 1 , θ 2 ] . 0 1 2 3 4 5 Training Epoch 10 4 - 4 - 3 - 2 - 1 0 1 2 R e w a r d 10 4 GDMDRL Fig . 14 : Test reward curves of GDM and DRL , i . e . , PPO , for optimal contract finding task [ 50 155 . 08 , 0 . 57 , 0 . 43 , 25 , 115 ] [ 50 , 140 . 78 , 0 . 04 , 0 . 96 , 26 , 52 ] [ 50 , 127 . 98 , 0 . 49 , 0 . 51 , 73 , 112 ] 0 50 100 150 200 Network state ( [ n , L max , p 1 , p 2 , q 1 , q 2 ] ) 23 . 10 144 . 50 V a l u e s L 1 R 1 L 2 R 2 Utility of clients 48 . 98 Fig . 15 : Generated contracts of GDM under different network states . • Formulate the utility of participants : There are two utility functions in our case , i . e . , U U and U SP . The former is the major utility that we intend to maximize . The latter is used in calculating the constraints , i . e . , IR and IC . • Customize the GDM settings : The space is formed as the universe of the contract design . Each item is formed as { L 1 , R 1 , L 2 , R 2 } . The hyperparameters α 1 , α 2 , β 1 , and β 2 are set as 30 , 5 , 1 and 1 , respectively . • Train GDM and perform inference : We train the GDM for more than 50000 epochs . The numerical results are discussed below . 5 ) Numerical Results : Fig . 14 shows the test reward curves of GDM and the baseline , i . e . , PPO . We can observe that the coverage speeds of GDM and PPO are roughly the same . However , GDM outperforms PPO in terms of rewards . The reason is two - fold : 1 ) the solution generation network is fine - tuned by the diffusion process , and 2 ) more policies can be 21 tested thanks to GDM’s high sample quality . As shown in Fig . 15 , the high positive rewards mean that GDM can stabilize and ensure high utility of the users while satisfying the IR and IC constraints in any given network states . V . S EMANTIC C OMMUNICATIONS In this section , we consider the SemCom technique and discuss the role of GDM in SemCom . A . Fundamentals of Semantic Communications SemCom [ 41 ] refers to extracting and transmitting the most relevant semantic information from raw data to the receivers using AI technology . It aims to lower network loads by selectively transmitting meaningful and contextually relevant information instead of transmitting the entire raw data [ 147 ] . SemCom consists of three main components : the semantic encoder , the wireless channel , and the semantic decoder . 1 ) Semantic Encoder : It is responsible for extracting and transmitting relevant semantic information from the raw data provided by the transmitting users . This is typically achieved by utilizing neural networks , which encode the raw data into meaningful semantic representations . The semantic encoder employs various techniques such as feature extraction and dimensionality reduction to capture the essential semantic information [ 148 ] . 2 ) Wireless Channels : However , during transmission , the semantic information is subject to physical noise introduced by the wireless channel [ 149 ] . Physical noise refers to external factors that interfere with the transmission of the message . It can result in noise - corrupted semantic information , which is then transmitted to the receivers for further processing . The channel component of SemCom handles the transmission of this noise - corrupted semantic information , taking into account the wireless channel characteristics and the potential effects of noise and interference . 3 ) Semantic Decoder : The receivers employ a semantic decoder , e . g . , implemented by neural networks , to decode the received noise - corrupted semantic information and reconstruct the distorted data . The semantic decoder utilizes its learning capabilities to reverse the encoding process and extract the intended semantic meaning from the received information [ 150 ] . Semantic noise arises from the use of symbols that are ambiguous to the receivers . It can also occur when there is a mismatch in understanding between the sender and receiver . By employing sophisticated neural network architectures , the semantic decoder aims to minimize the effects of semantic noise and accurately obtain the original semantic data . The ultimate objective of SemCom is to effectively convey the intended meaning of the transmitted symbols , rather than transmitting the raw bits directly , thereby reducing communi - cation overhead and enhancing communication effectiveness [ 151 ] . B . Case Study : GDM - based Resource allocation for SemCom - aided AIGC services 1 ) Motivation : There are several examples of integrating GAI technologies in SemCom [ 152 ] . For instance , GANs DataCollection SemanticExtraction Text / Image / Audio / Video AIGCTraining AIGC Inference SemanticInformation GraphicRendering CallbackResults Digitalcontents Step1 : Solution SpaceDefinition AvailableBandwidth Step2 : Objective Function Definition Utility Step3 : Dynamic Environment Definition ChannelConditionsandComputingAbilities Step4 : Training and Inference TheOptimalStrategy Fig . 16 : Resource allocation problem in an AIGC service scenario . First , the edge devices collect raw data , e . g . , photos , and extract semantic information . Then , the AIGC service providers use the received semantic information to perform the AIGC inference using GAI models to obtain meaningful content , e . g . , animated style photos . These contents are further used by the multimedia service provider , e . g . , Metaverse service provider , to render digital content for the users , e . g . , animated style avatars [ 44 ] . have been employed to develop semantic decoders that tackle the out - of - distribution problem of SemCom [ 153 ] . GANs are used to generate realistic and meaningful semantic informa - tion based on the available data . Additionally , a variational autoencoder ( VAE ) is utilized to calculate the lower bound of semantic distortion and derive the corresponding loss function [ 154 ] . By incorporating GANs and VAEs , SemCom can en - hance the accuracy and fidelity of semantic decoding , thereby improving the overall communication performance . To further explore the applications of GDMs in SemCom , we consider an AIGC service process as shown in Fig . 16 , where edge devices initially collect raw data , such as photographs , and extract semantic information . This semantic information is then utilized by AIGC service providers , who apply GAI models to perform AIGC inference and generate meaningful content , such as stylized animations . Subsequently , multimedia service providers , like Metaverse service providers , use this content to create digital content for users , such as animated avatars [ 44 ] . We formulate a unified resource allocation problem , con - sidering the limited computing and communication resources allocated to the semantic extraction , AIGC inference , and graphic rendering modules . The objective is to maximize the overall utility by efficiently allocating these resources . 2 ) Problem Formulation : The integration gain includes the computing time for semantic extraction ( T comp s ) , AIGC infer - ence ( T compa ) , and graphic rendering ( T compm ) . These times are influenced by the available computing resources and the cur - rent computing resource congestion , introducing uncertainty to the utility optimization problem . Concurrently , the transmis - sion time is associated with the transfer of semantic informa - tion ( T comma ) , AIGC content ( T commm , u ) , and rendering results ( T commm , d ) . These times are affected by the allocated com - 22 munication resources to each part . Specifically , we consider the allocation of bandwidth resources with W ma , W sm , and W as denoting the bandwidths for semantic information , AIGC content , and rendering results transmissions , respectively . The objective function is given by ln ( R as ) + ln ( R ma ) + ln ( R sm ) , where R as , R as and R as are the data rates for the transmissions of semantic information , AIGC content , and rendering results , respectively . The logarithmic form is used as we assume that the subjective user experience follows a logarithmic law to the objective performance metrics [ 155 ] . The objective function is considered as the reward in the GDM - based resource allocation scheme to find a near - optimal strategy . Following [ 156 ] , [ 157 ] , we construct the bandwidth allocation problem as follows : max W ma , W sm , W as ln ( R as ) + ln ( R ma ) + ln ( R sm ) , s . t . T comps + T comma + T compa + T commm , u + T commm , d + T compm ≤ T max , W ma + W sm + W as ≤ W max . ( 21 ) 3 ) GDM - based Resource Allocation Scheme Generation : The optimal bandwidth resource allocation scheme can be generated according to the following steps • Step 1 : Solution Space Definition : The solution space in the proposed problem encompasses allocating available bandwidth for transmission among the semantic extrac - tion , AIGC inference , and rendering modules . The goal is to optimize the utilization of bandwidth resources to en - sure efficient communication and collaboration between these modules . • Step 2 : Objective Function Definition : The training objective of the proposed problem is to maximize the utility of the system , which is served as rewards that are obtained by dynamic resource allocation strategies . It should consider the total tolerable transmission time and available resources among these modules . • Step 3 : Dynamic Environment Definition : GDMs are utilized to generate an optimal bandwidth allocation scheme based on a given set of wireless channel con - ditions and computing capabilities involved in the three modules , such as the semantic entropy and the transmit power . Semantic entropy is defined as the minimum expected number of semantic symbols about the data that is sufficient to predict the task [ 157 ] . The semantic entropy and the transmit power are randomly varied within a specific range associated with a given task . • Step 4 : Training and Inference : The conditional GDM generates the optimal bandwidth allocation strategy by mapping different environments to bandwidth allocation designs . The optimal strategy is achieved through the reverse process , where the GDM trains and infers the cor - responding allocation policies to maximize the expected cumulative utility . 4 ) Numerical Results : As depicted in Fig . 17 , Diffusion outperforms PPO regarding the obtained utilities . Fig . 18 is the generated strategies of Diffusion and PPO under dynamic environments GDM 1 , GDM 2 , PPO 1 , and PPO 2 [ 44 ] . GDM 1 0 50 100 150 200 250 300 350 400 Training Epoch 0 2 4 6 8 10 12 14 16 R e w a r d DRLGDM Fig . 17 : Test reward curves of GDM and DRL , i . e . , PPO , in SemCom [ 44 ] in bandwidth allocation task 1140 1150 1160 1170 1180 1190 1200 1210 1220 U tilit y PPO 1 PPO 2 GDM 1 GDM 2 0 10 20 30 40 50 60 70 80 90 B a nd w i d t h Fig . 18 : Generated strategies of GDM and PPO under dynamic environments [ 44 ] and GDM 2 are distinct environment definitions employed to generate utilities , which are aligned with PPO 1 and PPO 2 . We can also learn from Fig . 18 that Diffusion outperforms PPO in terms of generated strategies under dynamic envi - ronments . This superiority can be attributed to the optimal bandwidth allocation mechanism inferred by GDMs , which enables fine - tuning output through denoising steps and fa - cilitates exploration . Consequently , the proposed mechanism exhibits enhanced flexibility , mitigating the effects of un - certainty and noise encountered during the transmission and computing among semantic extraction , AIGC inference , and graphic rendering modules . VI . I NTERNET OF V EHICLES N ETWORKS In this section , we introduce the concept of IoV networks , discuss the role of GDM in IoV networks and give an example [ 158 ] . A . Fundamentals of IoV Networks Drawing inspiration from the Internet of Things ( IoT ) , the IoV network turns moving vehicles into information - gathering 23 nodes [ 159 ] , [ 160 ] . Harnessing emerging information and communication technologies facilitates network connectivity between vehicles and other elements , i . e . , other vehicles , users , infrastructure , and service platforms . For the IoV network , the goal is to enhance the overall intelligence of the vehicle , as well as improve the safety , fuel efficiency , and driving experience [ 161 ] . In the IoV network , vehicles are regarded as data agents for collecting and disseminating data such as traffic patterns , road conditions , and navigation guidance [ 162 ] . Managing large amounts of data in the IoV network is a very complex task . As a remedy , GAI is proposed . In particular , GAI performs the critical functions of organizing and restoring the data collected within the IoV . Additionally , it can generate synthetic data , enhancing the efficacy of machine learning model training within the network . Furthermore , the contributions of GAI go beyond simple data management . It utilizes the collected data to inform the real - time decision - making process . This includes predicting traffic conditions , identifying potential hazards , and determining the best route for the driver . B . Applications of GDM in IoV Networks The field of GAI is composed of several models , and each model brings unique capabilities to various applications . The GDM has attracted much attention among these models due to its unique advantages . Applying the GDM model within IoV networks yields promising results . In particular , there are two specific applications as follow : 1 ) Recovery of Images sent by vehicles : In IoV networks , vehicles usually transmit images to communicate information about their environment for safe driving . However , these images may be distorted or lose quality due to transmission errors , noise , or interference . The GDM , with its ability to generate high - quality images , can be employed to recover the original quality of these transmitted images . In particular , the vehicles adopt semantic technology to extract information from images , i . e . , as a prompt at the transmitter , and recover it using GDM at the receiver . By doing so , the transmitted data and communication delays can be reduced in IoV . 2 ) Optimization Based on GDM : The GDM iterative frame - work suits the IoV network optimization tasks , including path planning and resource allocation [ 163 ] . Using stochastic differential equations ( SDEs ) , the model refines solutions progressively via a diffusion process . For example , in path planning , GDM begins with a random path , making iterative refinements based on performance criteria such as travel time and energy consumption . The model uses gradients of these metrics to guide the path updates toward an optimal or near - optimal solution , stopping iterations when updates become negligible . Therefore , thanks to the ability to recover high - quality im - ages from transmitted data and iteratively optimize solutions , the GDM provides a powerful tool for enhancing the efficiency and robustness of IoV networks . C . Case Study : A GAI - driven IoV network In this part , we conduct a case study to illustrate how to apply GDMs in IoV design . Vehicular 1 Camera Capture FeaturesTensor Video Picture Text Text Encoder Text Encoder FeaturesTensor Features Picture Vehicular 2 CLIP Text Encoder Adapters Noise Noise NetworkorktwNetwNetwtw (cid:133)(cid:133) (cid:133)(cid:133) Decoder T T FeaturesTensor FeaturesTensor Network Conv RB RB p i x e l un s hu ff l e D o w n s a m p l e ConvRB RB (cid:133) T2I - Adapter Details C ond iti on GDM - aided joint optimization framework Fig . 19 : GAI - enabled IoV network , where the semantic infor - mation extraction step , image skeleton extraction step , wireless transmission step , GAI - enabled image generation step and image reconstruction step are involved [ 158 ] . 1 ) System Model : Under the 3GPP V2X standard [ 164 ] , we consider a GAI - driven IoV network with multiple V2V links as shown in Fig . 19 . We aim to ensure reliable , real - time information transmission in our considered network . The orthogonal frequency division multiplexing technology is adopted , where each V2V link can achieve dynamic transmis - sion rates on different sub - channels . Moreover , a successful image transmission rate is introduced as a constraint . This rate is affected by different parameters such as achievable transmis - sion rate , image similarity measure , channel coherence time , and generated image payload . 2 ) Problem Formulation : In our considered work , We con - sider transmission rate and image similarity as the performance indicators , and hence they are combined into a unified QoE indicator and used as the optimization goal . As described in ( 22 ) , an optimization problem is formulated to maximize the system QoE under the constraints of the transmission power budget and the probability of successful transmission for each vehicle , where the channel selection strategy , the transmission power for each vehicle , and the diffusion steps for inserting the skeleton are jointly optimized . max { P v , d v , c v } (cid:88) v ∈ V QoE ( v ) ( 22a ) s . t . (cid:88) v ∈ V p v ≤ P max , ( Power Budget ) ( 22b ) Pr ( v ) ≥ Pr min , ( Transmission Constraint ) ( 22c ) c v ∈ C , ( Channel Selection Constraint ) ( 22d ) d v ∈ N + , ( Diffusion Steps Constraint ) ( 22e ) ∀ v ∈ V . 3 ) GDM - based Joint Channel Selection and Power Allo - cation : For the formulated problem , a GDM - based DDPG 24 0 50 100 150 200 250 300 Training Epoch 0 0 . 5 1 1 . 5 2 2 . 5 3 R e w a r d GDMDRL - DDPG DRL - DQN GreedyRandom Fig . 20 : Test reward curves of different solutions , i . e . , GDM , DRL - DDPG , DRL - DQN , greedy , and random schemes , versus the training episodes in GDM - enabled IoV networks . approach is proposed , where the corresponding three tuples of MDP and the network design are as follows . • MDP design : The state space consists of the current information and previously selected actions , where the current information includes the channel information of each V2V link , the transmission rate of each V2V link , and the generated image payload . The action space con - sists of the selectable channel , the transmit power , and the diffusion steps for inserting the skeleton . The reward function consists of an instant reward term and a penalty term . Accordingly , the agent can achieve high QoE while satisfying the corresponding constraints . • GDM - based network design : In our proposed approach , we adopt the GDM - based network instead of the tradi - tional DRL neural network . Specifically , the GDM maps the state of the environment to a “solution generation network , ” representing the obtained resource allocation scheme . This approach can be fine - tuned to generate sam - ples spanning multiple time steps , enhancing its ability to handle tasks with long - term dependencies . 4 ) Numerical Results : Fig . 20 shows average cumulative rewards obtained by different types of schemes versus the number of training episodes . It shows that our proposed GDM - based approach always outperforms other baselines ( i . e . , DRL - DDPG , DRL - DQN , greedy , and random schemes ) under the same parameter settings when all schemes converge . It can also be observed that although the proposed GDM - based DDPG approach and DDPG - based obtain roughly similar rewards during the training phase , the proposed GDM - based DDPG approach outperforms DDPG after convergence . The reason is that the GDM network can fine - tune the output through the denoising step , thus facilitating exploration for better actions . VII . M ISCELLANEOUS I SSUES In this section , we discuss the applications of GDM to a number of other network issues , including channel estimation , error correction coding , and channel denoising . A . Channel Estimation 1 ) Motivations : In wireless communication systems , the wireless channel depends on a variety of factors such as fading , interference , and noise , which can lead to distortions in the received signal . Consequently , researchers introduce the channel estimation techniques to estimate the channel response , which can be used to mitigate the impacts caused by the aforementioned factors , thereby enhancing the quality of the received signal . As such , accurate channel estimation is crucial for reliable communication and efficient use of the available bandwidth [ 165 ] . So far , several kinds of channel estimation techniques have been proposed , including pilot - based , compressed sensing - based , etc . The pilot - based methods use known pilot symbols inserted in the transmitted signal to estimate the channel re - sponse . For instance , the minimum mean square error ( MMSE ) based method achieves channel estimation by multiplying the received signal with the conjugate of the transmitted signal , followed by division by the sum of the power of the transmitted signal and the noise variance . This method not only minimizes the mean square error between the received signal and the estimated signal but also considers the noise variance , which is important for determining the reliability of the estimated channel coefficients [ 166 ] . The compressed sensing - based methods exploit the sparsity of the channel response to estimate it from a small number of measurements . For example , the authors in [ 167 ] create a training signal using a random sequence with a known pilot sequence . At the receiver , first - order statistics and the compressed sensing method are applied to estimate the wireless channels with sparse impulse response . Unlike these two methods , data - driven methods employ machine learning algorithms to learn the channel response from the received signal without relying on any prior knowledge of the channel during the offline training phase . After trained , the data - driven methods can esti - mate the channel in an online phase . For instance , the authors in [ 168 ] first use the convolutional neural network ( CNN ) to extract channel response feature vectors , and then employs recurrent neural network ( RNN ) for channel estimation . Be - sides , there are some other techniques , such as optimization - based methods , which use mathematical optimization , such as convex optimization , to estimate the channel response , and hybrid methods that combine different techniques to improve the accuracy and efficiency of channel estimation . While effective , existing methods still faces several chal - lenges . One of the main challenges is the dynamic nature of the channel , which means that the channel can change rapidly due to various factors such as mobility and interference . This requires channel estimation to be robust to test - time distributional shifts [ 169 ] . These shifts naturally occur when the test environment no longer matches the algorithm design conditions , especially at the user side ( could be transmitter or receiver ) , where the propagation conditions may change from indoor to outdoor , whenever the user is moving . An effective solution to this challenge is to use GAI for robust channel estimation , because of the following main reasons . • The GAI model can extract complex patterns from large 25 Training phase Inference phase Diffusion model ( frozen ) t h Pilot consistency y   0 , 2 t I   t  1 t  h Loss function h h    2 0 , I     log H p  h   Diffusion model Fig . 21 : During training , the noise is first added to h to obtain ˜h . Then a regression target for the gradient of log p H (cid:16) ˜h (cid:17) is produced . After that , the l 2 - loss is used to train the pa - rameters of the deep neural network via back - propagation . After training , the current channel estimate is updated by a pilot consistency term , a diffusion update , and added noise , to achieve inference . amount of data and learn in a changing environment . This not only enhances the model’s generalization ability but also enables it to adapt to the dynamic characteristics of the channel , thereby improving the robustness of the estimation . • The GAI model can directly learn the distribution of channel responses from the received signals and use the structure captured by the deep generative model as a prior for inference , eliminating the need for prior knowledge of the sparsifying basis . Next , we further illustrate applications of GAI in channel estimation , using MIMO channel estimation via GAI model as a case study [ 169 ] . 2 ) Case Study : MIMO Channel Estimation Utilizing Diffu - sion Model : Channel estimation using diffusion model [ 169 ] primarily involves two phases : the training phase and the inference phase , as shown in Fig . 21 . The training phase involves using a deep neural network to learn an underlying structure of the channel from a set of noisy channel estimates . The main steps include the following : • Step 1 : Using the received pilot symbols to calculate the noisy channel estimation h . • Step 2 : Adding the noise to the training channel h to produce a perturbed channel ˜h . • Step 3 : Computing the gradient of log p H (cid:16) ˜h (cid:17) . • Step 4 : Producing a regression target for the gradient using the diffusion model . • Step 5 : Training the parameters of the deep neural network using back - propagation and the l 2 - loss . The inference stage involves utilizing the trained model to estimate the channel based on a set of received pilot symbols . The primary steps are as follows : • Step 1 : Updating the current channel estimation via the pilot consistency term , which enforces consistency between the received pilot symbols and the estimated channel . • Step 2 : The diffusion update is applied to the channel estimate , which smooths out the estimate and helps to reduce noise . • Step 3 : To prevent the model from converging to a sub - optimal solution , noise is added to the updated channel estimate at each step . • Step 4 : The process is repeated until convergence , at which point the final estimate of the channel is produced . It is noteworthy that the iterative algorithm operates inde - pendently of the training phase and can accommodate other impairments such as interference scenarios or few - bit quanti - zation of the received pilots . The proposed model is evaluated by training an NC - SNv2 model [ 170 ] on complex - valued channel matrices . The model architecture , RefineNet [ 171 ] , comprises eight layers and approximately 5 . 2 million parameters . To accommodate complex - valued inputs , the real and imaginary components of the matrix are processed as two separate input channels . Train - ing is performed on a dataset of 20 , 000 channel realizations , derived from the clustered delay line ( CDL ) channel model , with an equal distribution between two antenna spacings [ 169 ] . Fig . 2 in [ 169 ] presents the test results for in - distribution CDL channels in a blind SNR configuration with α = 0 . 4 . The top plot reveals that the comparison algorithm , WGAN [ 172 ] , captures some aspects of the channel structure for very low antenna spacing . However , its performance peaks , about - 26 dB , rapidly in high SNR conditions . Another comparative algorithm , i . e . , Lasso [ 173 ] , similarly exhibits a trend , with its peak value approximately at - 22 dB . This effect is more pronounced with an antenna spacing of half wavelength and fewer structural components , indicating that neither baseline employs a suitable prior knowledge . In contrast , the diffusion - based approach exhibits a near - linear reduction in the normal - ized mean square error ( NMSE ) , aligning with the theoretical findings in [ 174 ] , without explicit learning of a prior . At an SNR level of 15 dB , the NMSE of the diffusion - based approach is over 12 dB lower than both baseline methods , underscoring the superiority of the diffusion - based approach . B . Error Correction Coding 1 ) Motivations : For wireless communications , it is crucial to design codes that can be decoded robustly in noisy channels . Fundamental decoding methods can be divided into hard decoding and soft decoding [ 175 ] . Specifically , hard decoding considers only the most probable value of the received signal , disregarding the information about signal quality . Conversely , soft decoding not only considers the most probable signal value , but also leverages additional information about signal quality to enhance decoding performance . Although these basic decoding strategies may be sufficiently effective in some cases , efficient decoding for more complex encoding systems , such as algebraic block codes , remains an unresolved issue [ 175 ] . In particular , decoding according to the maximum likelihood rule , that is , finding the codeword that makes the received signal most likely to appear , has been 26 m ! " # ( $ ) % ( • ) ReverseGDM ! ( • ) " # + $ $ ~ % ( 0 , & ’ * - ) G BPSK . / 1 Encoder Decoder Fig . 22 : The denoising diffusion error correction codes ar - chitecture , where the decoding is performed via the reverse diffusion process [ 178 ] . proved to be a non - deterministic polynomial - time hard prob - lem . In other words , an exponential search may be required to find the best decoding result , which is not feasible in practice . Recently , some works represented by model - free machine learning methods tried to solve this problem . In particular , in [ 176 ] , a transformer - based decoder was proposed to embed the encoder into the considered architecture , where the results showed that it outperforms existing methods by a large margin at a fraction of the time complexity . However , this model - free approach suffers from several major drawbacks . First , it requires a lot of space and memory , which can be a problem on resource - constrained hardware . Second , since it does not em - ploy an iterative solution , the same computationally intensive neural decoding process is required regardless of the degree of codeword corruption . To this end , the GDM is considered for decoding [ 177 ] . Specifically , GDM decodes the channel codewords in an itera - tive manner , which not only greatly reduces the computational complexity , but also adapts to different degrees of codeword corruption efficiently . In addition , GDM is able to regard the pollution of channel codewords as a forward diffusion process , and in this process , the channel corruption can be reversed by an adaptive denoising diffusion probability model . 2 ) Case Study : Denoising Diffusion Error Correction Codes : As shown in Fig . 22 , the elements of the denoising diffusion used for decoding and the proposed architecture are summarized , where the training process is as follows . • Decoding as a Reverse Diffusion Process In this stage , a process of “forward diffusion” is used to process code - words sampled from a particular encoding distribution . Specifically , the process gradually transmits codewords by gradually adding a small amount of Gaussian noise , with the size of each step controlled by a specific variance table . Next , data transmission over a noisy communication channel is regarded as a modified iterative diffusion process that requires inversion at the receiving end to decode the original data . Finally , decoding is regarded as a reverse diffusion process , transforming the posterior probability into a Gaussian distribution as per the Bayesian theorem [ 176 ] . The goal of the decoder can be defined to predict the noise of the channel . • Denoising via Parity Check Conditioning In the de - coding process , it is regarded as the reverse denoising process of the GDM , which relies on time steps and can reverse the entire diffusion process by sampling Gaussian noise corresponding to the final step . During training , a time step is randomly sampled , generating noise and a syndrome requiring correction . Owing to its invariance to the transmitted codeword , diffusion decoding can be trained using a single codeword . During inference , the denoising model predicts multiplicative noise , converts it into additive noise , and performs the gradient step in the original additive diffusion process . Fig . 4 in [ 178 ] shows BER obtained by three schemes in terms of the normalized SNR values , i . e . , E b / N 0 ( EbNo ) , over the Rayleigh fading channel environment . It shows that with the increment of the value of EbNo , the GDM - based scheme is superior to other benchmarks . In particular , when the EbNo is 4 dB , the BER obtained by GDM scheme is 50 % of that obtained by binary phase ( BP ) scheme , and 11 % of that obtained by error correction code transformer ( ECCT ) scheme [ 176 ] . The reason is that the GDM is able to learn to decode , even under some serious noisy fading channels . C . Channel Denoising 1 ) Motivations : GDM - based models are characterized by the ability to gradually add Gaussian noise to the training data , and then learn to restore the original data from the noise through a backsampling process . The process is similar to a receiver in a wireless communication system , which is required to recover the transmitted signal from the noisy received signal . Thus , in [ 179 ] , a GDM - based wireless communication channel denoising model is designed , which can be used as a new module to predict and remove channel noise after channel equalization , thus improving the overall performance . In particular , it relies entirely on the forward diffusion process without any received signal . When applied to a semantic communication system based on joint source channel coding ( JSCC ) , whether in Rayleigh fading channels or additive white Gaussian noise ( AWGN ) channels , the GDM - based channel denoising model can effectively reduce the distance between the transmitted signal and the received signal . 2 ) Case Study : GDM - based Channel Denoising Model : As shown in Fig . 23 , the joint GDM and JSCC architecture is summarized , where the training process is as follows . • Conditional Distribution of The Received Signals : Real - valued and complex - valued symbols are trans - formed and transmitted in the wireless channel , where the transformation combines the effects of Rayleigh fading gain and additive white Gaussian noise . The received signal is then processed through an MMSE equalizer to produce an equalized complex signal . Study conditional 27 Rx Channel Estimation Rayleigh Channel Equalization ( MMSE ) AWGN Channel WirelessChannel ! yy " # JSCCDecoder s x JSCCEncoder GDM Fig . 23 : The joint GDM and JSCC system architecture , where GDM is trained using a specialized noise schedule [ 179 ] . distributions of real - valued vectors using known signal and channel state information . Based on the noise impact and channel state , the signal is reparameterized and a GDM - based channel denoising model is trained to obtain noise estimates . • Training Algorithm of GDM : In the training process of GDM , the original source signal is first represented in a new parameterized form . At the beginning of training , the Kullback - Leibler divergence [ 145 ] is mainly used to optimize the variational upper bound of the negative log - likelihood . During training , the optimal value of a key hyper - parameter is required to be determined . Next , the optimization objective for a series of loss functions is simplified by re - parameterization and re - weighting methods . Finally , the overall loss function is minimized , effectively recovering the original source signal . Figs . 5 and 6 in [ 179 ] show PSNR obtained by three schemes in terms of the SNR over the AWGN channel and Rayleigh fading channel environments . To achieve op - timal performance , both GDM - based JSCC scheme and JSCC scheme are required to be retrained for a given SNR . It shows that for different values of SNR , the GDM - based JSCC scheme is superior to others . For example , over Rayleigh fading channel with SNR of 20 dB , compared with the JSCC scheme , the GDM - based JSCC scheme can obtain about 1 . 06 dB gain . VIII . F UTURE D IRECTIONS This section elucidates potential research avenues warrant - ing further examination . A . Space - air - ground Integrated Network The Space - Air - Ground Integrated Network ( SAGIN ) is a promising paradigm for future wireless networks , character - ized by its three - dimensional coverage , high capacity , and re - liable communications [ 180 ] – [ 182 ] . However , the optimization of SAGIN is a complex task due to the high dimensionality of the network configuration , the heterogeneity of the network elements , and the dynamic nature of the network environ - ment [ 183 ] , [ 184 ] . GDMs , with their ability in complex data distribution modeling , could be a powerful tool for optimizing SAGIN . • Dynamic Network Environment Modeling and Pre - diction : The dynamic nature of the SAGIN environment poses a significant challenge for its optimization [ 181 ] , [ 185 ] . GDMs can be used to model and predict these dynamic network environments . This would allow for more efficient resource allocation , network scheduling , and routing strategies , as the predictions could provide valuable insights into future network states [ 186 ] . • Synthetic Network Scenario Generation : Testing and validating network optimization algorithms require a va - riety of network scenarios [ 187 ] . GDMs can generate synthetic network scenarios that closely mimic real - world conditions , providing a robust platform for testing and validating these algorithms . • Network Scheduling and Routing : SAGIN involves a variety of network elements , each with its unique characteristics and requirements [ 188 ] , [ 189 ] . GDMs can capture these unique characteristics and model the complex interactions between different network elements , facilitating more efficient network scheduling and routing strategies . B . Extremely Large - Scale MIMO Extremely Large - Scale MIMO ( XL - MIMO ) is an emerging technology that is expected to play a pivotal role in the 6G of wireless mobile networks [ 190 ] , [ 191 ] . XL - MIMO offers vast spatial degrees of freedom by deploying an extremely large number of antennas , leading to significant enhancements in spectral efficiency and spatial degrees of freedom . However , implementing XL - MIMO introduces new challenges , includ - ing the need for more flexible hardware designs , a much larger number of antennas , smaller antenna spacing , new electromag - netic characteristics , and near - field - based signal processing schemes [ 192 ] , [ 193 ] . GDMs can be instrumental in addressing these challenges and optimizing the performance of XL - MIMO systems . Here are some potential research directions : • Hybrid Channel Estimation and Modeling : XL - MIMO systems involve a large number of antennas , leading to high - dimensional data [ 194 ] , and also the co - existence of near - field and far - field channels within the coverage of cellular networks . Especially , in the near - field chan - nel , the channel response vectors depend on both the distance and direction between the transceiver of each antenna element , unlike the far - field channel . Therefore , the increased “huge” complexity for near - field channel estimation may not be resolved with the conventional approaches . GDMs can be used to model and estimate such hybrid channel state information efficiently . They can exploit the inherent graph structure in the spatial domain , where antennas can be considered as nodes and the spatial correlation between antennas as edges . This can lead to more accurate and efficient channel estimation methods . 28 • Signal Processing : The signal processing in XL - MIMO systems can be complex due to the large number of antennas and the near - field communication characteris - tics . Especially , in the latter case , the interference caused by multi - user transmissions can be effectively mitigated by utilizing the higher degree of freedom existing in the distance and direction of near - field channel response vectors . GDMs can be used to develop efficient signal processing algorithms that can handle high - dimensional data and exploit the spatial correlation in the antenna array . This can lead to improved performance in terms of data rate and reliability . • Hardware Design and Implementation : XL - MIMO systems involve different hardware designs , such as uni - form linear array ( ULA ) - based , uniform planar array ( UPA ) - based , and continuous aperture phased ( CAP ) - based XL - MIMO . GDMs can be used to model and analyze these different designs , helping to understand their characteristics and interrelationships . This can guide the design and implementation of XL - MIMO systems . C . Integrated Sensing and Communications The ISAC unifies wireless sensing and communication systems to efficiently employ limited resources for mutual benefits [ 195 ] . It is a key element in future wireless systems , supporting various applications like autonomous driving and indoor localization [ 40 ] , [ 196 ] . The GDM can be utilized in ISAC systems for both data processing and generation . As a processing technique , it can classify and recover ISAC - related data . Moreover , it can generate synthetic ISAC data , a vital function for boosting the training efficiency of neural networks within the ISAC systems . Specifically , GDM has applications in various aspects of the ISAC system . • ISAC Data Generation : The GDM can be used to gen - erate samples for ISAC network training . For example , in indoor localization based on received signal strength indication ( RSSI ) , the authors in [ 197 ] proposed a GAN for RSSI data augmentation . This network generates fake RSSI based on a small set of real collected labeled data . Using these data , the experimental results show that over - all localization accuracy of the system has improved by 15 . 36 % . Compared to GAN , GDM has stronger inference capabilities , which enable it to generate better fake data , thereby further enhancing system performance . • ISAC Data Processing : Apart from data generation , GAI models are also commonly used to process ISAC data [ 198 ] . For instance , given that the GAN - based semi - supervised learning can handle unlabeled and labeled data , the authors in [ 199 ] introduced a complement generator that uses a limited amount of unlabeled data to generate samples for training the discriminator . Building on this , they further adjust the number of probability outputs and utilize manifold regularization to stabilize the learning process , enhancing the human activity recogni - tion performance in both semi - supervised and supervised scenarios . D . Movable Antenna System The future of wireless communication networks is expected to be shaped significantly by the integration of movable antennas [ 200 ] , [ 201 ] . Movable or fluid antennas , unlike conventional fixed - position antennas , have the capability of flexible movement and can be deployed at positions with more favorable channel conditions to achieve higher spatial diversity gains [ 202 ] . This flexibility enables better coverage and adaptability to changing environmental conditions . By strategically relocating the antenna , it becomes possible to mitigate signal blockage or interference caused by various obstacles , including buildings and vegetation . Therefore , the movable antennas can reap the full diversity in the given spatial region [ 202 ] . The complex and dynamic nature of wireless environments , characterized by high - dimensional configura - tions and non - linear relationships , necessitates sophisticated models like GDMs that can capture such high - dimensional and complex structures . • Optimization of Antenna Positioning : GDMs can be used to optimize the positioning of movable antennas in real time . By modeling the wireless environment and the effects of different antenna positions , GDMs can generate optimal antenna positions that maximize signal strength and minimize interference . • Dynamic Resource Allocation : GDMs can be applied to the dynamic resource allocation problem in movable an - tennas . By modeling the resource demands and availabil - ity in the network , GDMs can generate optimal resource allocation strategies that balance the needs of different network users and maximize network efficiency [ 203 ] . • Predictive Maintenance : Based on historical data , GDMs can be used to predict potential failures in mov - able antennas . By modeling antenna performance and failure patterns , GDMs can generate predictions about future failures , allowing for proactive maintenance and minimizing network downtime . • Integration with Reinforcement Learning : As demon - strated in Section III , the integration of GDMs with reinforcement learning techniques can be further explored in the context of movable antennas . This can lead to more robust and efficient resource slicing and scheduling strategies , enhancing the performance of 5G networks [ 204 ] and autonomous vehicles [ 205 ] . IX . C ONCLUSIONS In this tutorial , the transformative potential of GDMs in the realm of intelligent network optimization has been thoroughly explored . The unique strengths of GDMs , including their broad applicability and capability to model complex data distributions , were studied . We highlighted their potential in enhancing the DRL algorithms and providing solutions in several key intelligent network scenarios , such as incentive mechanism design , SemCom , IoV networks , channel estima - tion , error correction coding , and channel denoising . These explorations demonstrated the practicality and efficacy of GDMs in real - world applications . The tutorial concluded by emphasizing the research directions of GDMs in shaping the 29 future of intelligent network optimization and encouraging further exploration in this promising field . R EFERENCES [ 1 ] M . Jovanovic and M . Campbell , “Generative artificial intelligence : Trends and prospects , ” Computer , vol . 55 , no . 10 , pp . 107 – 112 , Oct . 2022 . [ 2 ] P . Korzynski , G . Mazurek , A . Altmann , J . Ejdys , R . Kazlauskaite , J . Paliszkiewicz , K . Wach , and E . Ziemba , “Generative artificial intelligence as a new context for management theories : Analysis of ChatGPT , ” Central Eur . Manag . J . , 2023 . [ 3 ] R . Peres , M . Schreier , D . Schweidel , and A . Sorescu , “On ChatGPT and beyond : How generative artificial intelligence may affect research , teaching , and practice , ” Int . J . Res . Mark . , 2023 . [ 4 ] C . van Dun , L . Moder , W . Kratsch , and M . R¨oglinger , “ProcessGAN : Supporting the creation of business process improvement ideas through generative machine learning , ” Decis . Support Syst . , vol . 165 , p . 113880 , 2023 . [ 5 ] Accenture , “2023 technology vision report , ” https : / / www . accenture . com / us - en / insights / technology / technology - trends - 2023 . [ 6 ] B . Ni , D . L . Kaplan , and M . J . Buehler , “Generative design of de novo proteins based on secondary - structure constraints using an attention - based diffusion model , ” Chem , 2023 . [ 7 ] R . Srinivasan and K . Uchino , “Biases in generative art : A causal look from the lens of art history , ” in Proc . ACM Conf . Fair . Account . Transp . , 2021 , pp . 41 – 51 . [ 8 ] A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , Ł . Kaiser , and I . Polosukhin , “Attention is all you need , ” Proc . Adv . Neural Inf . Process . Syst . , vol . 30 , 2017 . [ 9 ] O . AI , “Gpt - 4 technical report , ” arXiv preprint arXiv : 2303 . 08774 , 2023 . [ 10 ] I . Goodfellow , J . Pouget - Abadie , M . Mirza , B . Xu , D . Warde - Farley , S . Ozair , A . Courville , and Y . Bengio , “Generative adversarial net - works , ” Commun . ACM , vol . 63 , no . 11 , pp . 139 – 144 , Nov . 2020 . [ 11 ] D . P . Kingma , M . Welling et al . , “An introduction to variational autoencoders , ” Found . Trends Mach . Learn . , vol . 12 , no . 4 , pp . 307 – 392 , Apr . 2019 . [ 12 ] D . Rezende and S . Mohamed , “Variational inference with normalizing flows , ” in Proc . Int . Conf . Mach . Learn . , Lille , France , Jul . 2015 , pp . 1530 – 1538 . [ 13 ] J . Zhao , M . Mathieu , and Y . LeCun , “Energy - based generative adver - sarial networks , ” in Proc . Int . Conf . Mach . Learn . , 2017 . [ 14 ] J . Sohl - Dickstein , E . Weiss , N . Maheswaranathan , and S . Ganguli , “Deep unsupervised learning using nonequilibrium thermodynamics , ” in Proc . Int . Conf . Mach . Learn . , 2015 , pp . 2256 – 2265 . [ 15 ] Y . Song , J . Sohl - Dickstein , D . P . Kingma , A . Kumar , S . Ermon , and B . Poole , “Score - based generative modeling through stochastic differential equations , ” in Proc . Int . Conf . Learn . Represent . , 2021 . [ 16 ] S . Peng and S . Peng , Stochastic differential equations . Springer , 2019 . [ 17 ] H . Cao , C . Tan , Z . Gao , G . Chen , P . - A . Heng , and S . Z . Li , “A survey on generative diffusion model , ” arXiv preprint arXiv : 2209 . 02646 , 2022 . [ 18 ] S . AI , “Stable diffusion , ” https : / / stability . ai / . [ 19 ] J . Ho , A . Jain , and P . Abbeel , “Denoising diffusion probabilistic models , ” Adv . Neural Inf . Process . Syst . , vol . 33 , pp . 6840 – 6851 , 2020 . [ 20 ] J . Song , C . Meng , and S . Ermon , “Denoising diffusion implicit mod - els , ” Proc . Int . Conf . Learn . Represent . , 2020 . [ 21 ] X . Li , J . Thickstun , I . Gulrajani , P . S . Liang , and T . B . Hashimoto , “Diffusion - lm improves controllable text generation , ” Adv . Neural Inf . Process . Syst . , vol . 35 , pp . 4328 – 4343 , 2022 . [ 22 ] G . Mittal , J . Engel , C . Hawthorne , and I . Simon , “Symbolic music generation with diffusion models , ” Proc . Int . Society Music Inf . Retr . Conf . , 2021 . [ 23 ] R . Huang , Z . Zhao , H . Liu , J . Liu , C . Cui , and Y . Ren , “Prodiff : Progressive fast diffusion model for high - quality text - to - speech , ” in Proc . ACM Int . Conf . Multimedia , 2022 , pp . 2595 – 2605 . [ 24 ] C . Niu , Y . Song , J . Song , S . Zhao , A . Grover , and S . Ermon , “Permu - tation invariant graph generation via score - based generative modeling , ” in Proc . Int . Conf . Artif . Intell . Stat . PMLR , 2020 , pp . 4474 – 4484 . [ 25 ] C . Vignac , I . Krawczuk , A . Siraudin , B . Wang , V . Cevher , and P . Frossard , “DiGress : Discrete denoising diffusion for graph gener - ation , ” arXiv preprint arXiv : 2209 . 14734 , 2022 . [ 26 ] X . Chen , J . He , X . Han , and L . - P . Liu , “Efficient and degree - guided graph generation via discrete diffusion modeling , ” arXiv preprint arXiv : 2305 . 04111 , 2023 . [ 27 ] X . Peng , J . Guan , Q . Liu , and J . Ma , “MolDiff : Addressing the atom - bond inconsistency problem in 3D molecule diffusion generation , ” arXiv preprint arXiv : 2305 . 07508 , 2023 . [ 28 ] M . A . Ketata , C . Laue , R . Mammadov , H . Stark , M . Wu , G . Corso , C . Marquet , R . Barzilay , and T . S . Jaakkola , “DiffDock - PP : Rigid protein - protein docking with diffusion models , ” in Proc . Int . Conf . Learn . Represent . , 2023 . [ 29 ] L . Huang , H . Zhang , T . Xu , and K . - C . Wong , “MDM : Molec - ular diffusion model for 3D molecule generation , ” arXiv preprint arXiv : 2209 . 05710 , 2022 . [ 30 ] C . Lee , J . Kim , and N . Park , “CoDi : Co - evolving contrastive dif - fusion models for mixed - type tabular synthesis , ” arXiv preprint arXiv : 2304 . 12654 , 2023 . [ 31 ] A . Kotelnikov , D . Baranchuk , I . Rubachev , and A . Babenko , “TabD - DPM : Modelling tabular data with diffusion models , ” arXiv preprint arXiv : 2209 . 15421 , 2022 . [ 32 ] N . Neifar , A . Ben - Hamadou , A . Mdhaffar , and M . Jmaiel , “DiffECG : A generalized probabilistic diffusion model for ECG signals synthesis , ” arXiv preprint arXiv : 2306 . 01875 , 2023 . [ 33 ] L . Yang , Z . Zhang , Y . Song , S . Hong , R . Xu , Y . Zhao , Y . Shao , W . Zhang , B . Cui , and M . - H . Yang , “Diffusion models : A com - prehensive survey of methods and applications , ” arXiv preprint arXiv : 2209 . 00796 , 2022 . [ 34 ] F . - A . Croitoru , V . Hondru , R . T . Ionescu , and M . Shah , “Diffusion models in vision : A survey , ” IEEE Trans . Pattern Anal . Mach . Intell . , to appear , 2023 . [ 35 ] M . Reuss , M . Li , X . Jia , and R . Lioutikov , “Goal - conditioned imi - tation learning using score - based diffusion policies , ” arXiv preprint arXiv : 2304 . 02532 , 2023 . [ 36 ] Y . Li , Y . Lu , R . Zhang , B . Ai , and Z . Zhong , “Deep learning for energy efficient beamforming in MU - MISO networks : A GAT - based approach , ” IEEE Wireless Commun . Lett . , vol . 12 , no . 7 , pp . 1264 – 1268 , 2023 . [ 37 ] C . Lu , H . Chen , J . Chen , H . Su , C . Li , and J . Zhu , “Contrastive energy prediction for exact energy - guided diffusion sampling in offline reinforcement learning , ” arXiv preprint arXiv : 2304 . 12824 , 2023 . [ 38 ] H . Du , J . Wang , D . Niyato , J . Kang , Z . Xiong , and D . I . Kim , “AI - generated incentive mechanism and full - duplex semantic communica - tions for information sharing , ” IEEE J . Sel . Areas Commun . , to appear , 2023 . [ 39 ] B . Du , H . Du , H . Liu , D . Niyato , P . Xin , J . Yu , M . Qi , and Y . Tang , “YOLO - based semantic communication with generative AI - aided resource allocation for digital twins construction , ” arXiv preprint arXiv : 2306 . 14138 , 2023 . [ 40 ] X . Cheng , D . Duan , S . Gao , and L . Yang , “Integrated sensing and com - munications ( ISAC ) for vehicular communication networks ( VCN ) , ” IEEE Internet Things J . , vol . 9 , no . 23 , pp . 23441 – 23451 , 2022 . [ 41 ] W . Yang , H . Du , Z . Q . Liew , W . Y . B . Lim , Z . Xiong , D . Niyato , X . Chi , X . S . Shen , and C . Miao , “Semantic communications for future internet : Fundamentals , applications , and challenges , ” IEEE Commun . Surv . Tut . , to appear , 2023 . [ 42 ] L . - M . Ang , K . P . Seng , G . K . Ijemaru , and A . M . Zungeru , “De - ployment of IoV for smart cities : Applications , architecture , and challenges , ” IEEE Access , vol . 7 , pp . 6473 – 6492 , 2018 . [ 43 ] H . Zhou , H . Zhou , J . Li , K . Yang , J . An , and X . Shen , “Heteroge - neous ultra - dense networks with traffic hotspots : A unified handover analysis , ” IEEE Internet Things J . , to appear , 2023 . [ 44 ] Y . Lin , Z . Gao , H . Du , D . Niyato , J . Kang , A . Jamalipour , and X . S . Shen , “A Unified Framework for Integrating Semantic Com - munication and AI - Generated Content in Metaverse , ” arXiv preprint arXiv : 2305 . 11911 , 2023 . [ 45 ] H . Zhou , B . Liu , T . H . Luan , F . Hou , L . Gui , Y . Li , Q . Yu , and X . Shen , “Chaincluster : Engineering a cooperative content distribution framework for highway vehicular communications , ” IEEE Trans . Intell . Transp Syst . , vol . 15 , no . 6 , pp . 2644 – 2657 , Jun . 2014 . [ 46 ] R . Zhang , K . Xiong , X . Tian , Y . Lu , P . Fan , and K . B . Letaief , “Inverse reinforcement learning meets power allocation in multi - user cellular networks , ” in IEEE INFOCOM 2022 - IEEE Conference on Computer Communications Workshops ( INFOCOM WKSHPS ) , 2022 , pp . 1 – 2 . [ 47 ] H . Du , D . Niyato , J . Kang , Z . Xiong , K . - Y . Lam , Y . Fang , and Y . Li , “Spear or shield : Leveraging generative AI to tackle security threats of intelligent network services , ” arXiv preprint arXiv : 2306 . 02384 , 2023 . [ 48 ] A . Ajay , Y . Du , A . Gupta , J . Tenenbaum , T . Jaakkola , and P . Agrawal , “Is conditional generative modeling all you need for decision - making ? ” arXiv preprint arXiv : 2211 . 15657 , 2022 . 30 [ 49 ] M . Janner , Y . Du , J . B . Tenenbaum , and S . Levine , “Plan - ning with diffusion for flexible behavior synthesis , ” arXiv preprint arXiv : 2205 . 09991 , 2022 . [ 50 ] Z . Wang , J . J . Hunt , and M . Zhou , “Diffusion policies as an expres - sive policy class for offline reinforcement learning , ” arXiv preprint arXiv : 2208 . 06193 , 2022 . [ 51 ] H . Chen , C . Lu , C . Ying , H . Su , and J . Zhu , “Offline reinforcement learning via high - fidelity generative behavior modeling , ” arXiv preprint arXiv : 2209 . 14548 , 2022 . [ 52 ] H . - C . Wang , S . - F . Chen , and S . - H . Sun , “Diffusion model - augmented behavioral cloning , ” arXiv preprint arXiv : 2302 . 13335 , 2023 . [ 53 ] A . Kazerouni , E . K . Aghdam , M . Heidari , R . Azad , M . Fayyaz , I . Hacihaliloglu , and D . Merhof , “Diffusion models for medical image analysis : A comprehensive survey , ” arXiv preprint arXiv : 2211 . 07804 , 2022 . [ 54 ] C . Zhang , C . Zhang , M . Zhang , and I . S . Kweon , “Text - to - image diffusion model in generative AI : A survey , ” arXiv preprint arXiv : 2303 . 07909 , 2023 . [ 55 ] A . Ulhaq , N . Akhtar , and G . Pogrebna , “Efficient diffusion models for vision : A survey , ” arXiv preprint arXiv : 2210 . 09292 , 2022 . [ 56 ] H . Zou , Z . M . Kim , and D . Kang , “Diffusion models in NLP : A survey , ” arXiv preprint arXiv : 2305 . 14671 , 2023 . [ 57 ] Y . Li , K . Zhou , W . X . Zhao , and J . - R . Wen , “Diffusion models for non - autoregressive text generation : A survey , ” arXiv preprint arXiv : 2303 . 06574 , 2023 . [ 58 ] L . Lin , Z . Li , R . Li , X . Li , and J . Gao , “Diffusion models for time series applications : A survey , ” arXiv preprint arXiv : 2305 . 00624 , 2023 . [ 59 ] W . Luo , “A comprehensive survey on knowledge distillation of diffu - sion models , ” arXiv preprint arXiv : 2304 . 04262 , 2023 . [ 60 ] M . Zhang , M . Qamar , T . Kang , Y . Jung , C . Zhang , S . - H . Bae , and C . Zhang , “A survey on graph diffusion models : Generative AI in science for molecule , protein and material , ” arXiv preprint arXiv : 2304 . 01565 , 2023 . [ 61 ] C . Zhang , C . Zhang , S . Zheng , M . Zhang , M . Qamar , S . - H . Bae , and I . S . Kweon , “Audio diffusion model for speech synthesis : A survey on text to speech and speech enhancement in generative AI , ” arXiv preprint arXiv : 2303 . 13336 , 2023 . [ 62 ] Z . Guo , J . Liu , Y . Wang , M . Chen , D . Wang , D . Xu , and J . Cheng , “Diffusion models in bioinformatics : A new wave of deep learning revolution in action , ” arXiv preprint arXiv : 2302 . 10907 , 2023 . [ 63 ] W . Fan , C . Liu , Y . Liu , J . Li , H . Li , H . Liu , J . Tang , and Q . Li , “Generative diffusion models on graphs : Methods and applications , ” arXiv preprint arXiv : 2302 . 02591 , 2023 . [ 64 ] H . Du , Z . Li , D . Niyato , J . Kang , Z . Xiong , H . Huang , and S . Mao , “Generative AI - aided optimization for AI - generated content ( AIGC ) services in edge networks , ” arXiv preprint arXiv : 2303 . 13052 , 2023 . [ 65 ] A . Lou and S . Ermon , “Reflected diffusion models , ” Proc . Int . Conf . Mach . Learn . , 2023 . [ 66 ] Q . Zhang , J . Song , X . Huang , Y . Chen , and M . - Y . Liu , “DiffCollage : Parallel generation of large content with diffusion models , ” in Proc . IEEE Conf . Comput . Vis . Pattern Recognit . , 2023 , pp . 10188 – 10198 . [ 67 ] H . Ni , C . Shi , K . Li , S . X . Huang , and M . R . Min , “Conditional image - to - video generation with latent flow diffusion models , ” in Proc . IEEE Conf . Comput . Vis . Pattern Recognit . , 2023 , pp . 18444 – 18455 . [ 68 ] W . Enkelmann , “Investigations of multigrid algorithms for the esti - mation of optical flow fields in image sequences , ” Computer Vision , Graphics , and Image Processing , vol . 43 , no . 2 , pp . 150 – 177 , Feb . 1988 . [ 69 ] J . Ho , T . Salimans , A . Gritsenko , W . Chan , M . Norouzi , and D . J . Fleet , “Video diffusion models , ” arXiv : 2204 . 03458 , 2022 . [ 70 ] P . Yu , S . Xie , X . Ma , B . Jia , B . Pang , R . Gao , Y . Zhu , S . - C . Zhu , and Y . Wu , “Latent diffusion energy - based model for interpretable text modeling . ” in Proc . Int . Conf . Mach . Learn . , 2022 . [ 71 ] S . Gong , M . Li , J . Feng , Z . Wu , and L . Kong , “Diffuseq : Sequence to sequence text generation with diffusion models , ” Proc . Int . Conf . Learn . Represent . , 2022 . [ 72 ] H . Zhang , X . Liu , and J . Zhang , “Diffusum : Generation enhanced extractive summarization with diffusion , ” Assoc . Comput . Linguist . , 2023 . [ 73 ] M . Reid , V . J . Hellendoorn , and G . Neubig , “Diffuser : Diffusion via edit - based reconstruction , ” 2023 . [ 74 ] L . Ruan , Y . Ma , H . Yang , H . He , B . Liu , J . Fu , N . J . Yuan , Q . Jin , and B . Guo , “Mm - diffusion : Learning multi - modal diffusion models for joint audio and video generation , ” in Proc . IEEE Conf . Comput . Vis . Pattern Recognit . , 2023 , pp . 10219 – 10228 . [ 75 ] Z . Kong , W . Ping , J . Huang , K . Zhao , and B . Catanzaro , “DiffWave : A versatile diffusion model for audio synthesis , ” in Proc . Int . Conf . Learn . Represent . , 2021 . [ 76 ] J . Liu , C . Li , Y . Ren , F . Chen , and Z . Zhao , “Diffsinger : Singing voice synthesis via shallow diffusion mechanism , ” in Proc . AAAI Conf . Artif . Intell . , vol . 36 , no . 10 , 2022 , pp . 11020 – 11028 . [ 77 ] S . Rouard and G . Hadjeres , “CRASH : Raw audio score - based genera - tive modeling for controllable high - resolution drum sound synthesis , ” Proc . Int . Society Music Inf . Retr . Conf . , 2021 . [ 78 ] A . Q . Nichol , P . Dhariwal , A . Ramesh , P . Shyam , P . Mishkin , B . Mc - grew , I . Sutskever , and M . Chen , “GLIDE : Towards photorealistic image generation and editing with text - guided diffusion models , ” in Int . Conf . Mach . Learn . , 2022 , pp . 16784 – 16804 . [ 79 ] OpenAI , “Dall·e 2 , ” https : / / openai . com / dall - e - 2 . [ 80 ] B . T . Google Research , “Imagen , ” https : / / imagen . research . google / . [ 81 ] J . Gui , Z . Sun , Y . Wen , D . Tao , and J . Ye , “A review on generative adversarial networks : Algorithms , theory , and applications , ” IEEE Trans . Knowl . Data Eng . , vol . 35 , no . 4 , pp . 3313 – 3332 , Apr . 2021 . [ 82 ] H . Du , R . Zhang , D . Niyato , J . Kang , Z . Xiong , D . I . Kim , X . S . Shen , and H . V . Poor , “Exploring collaborative distributed diffusion - based AI - generated content ( AIGC ) in wireless networks , ” IEEE Netw . , no . 99 , pp . 1 – 8 , 2023 . [ 83 ] A . Zappone , M . Di Renzo , M . Debbah , T . T . Lam , and X . Qian , “Model - aided wireless artificial intelligence : Embedding expert knowl - edge in deep neural networks for wireless system optimization , ” IEEE Veh . Technol . Mag . , vol . 14 , no . 3 , pp . 60 – 69 , Mar . 2019 . [ 84 ] X . Lin , N . B . Shroff , and R . Srikant , “A tutorial on cross - layer optimization in wireless networks , ” IEEE J . Sel . Areas Commun . , vol . 24 , no . 8 , pp . 1452 – 1463 , Aug . 2006 . [ 85 ] Y . Liu , H . Du , D . Niyato , J . Kang , Z . Xiong , D . I . Kim , and A . Jamalipour , “Deep generative model and its applications in efficient wireless network management : A tutorial and case study , ” arXiv preprint arXiv : 2303 . 17114 , 2023 . [ 86 ] I . Osband , C . Blundell , A . Pritzel , and B . Van Roy , “Deep exploration via bootstrapped DQN , ” Adv . Neural Inf . Process . Syst . , vol . 29 , 2016 . [ 87 ] T . Haarnoja , A . Zhou , P . Abbeel , and S . Levine , “Soft actor - critic : Off - policy maximum entropy deep reinforcement learning with a stochastic actor , ” in Proc . Int . Conf . Mach . Learn . PMLR , 2018 , pp . 1861 – 1870 . [ 88 ] J . Schulman , F . Wolski , P . Dhariwal , A . Radford , and O . Klimov , “Prox - imal policy optimization algorithms , ” arXiv preprint arXiv : 1707 . 06347 , 2017 . [ 89 ] A . Goldsmith , Wireless communications . Cambridge university press , 2005 . [ 90 ] B . Zheng and R . Zhang , “Intelligent reflecting surface - enhanced OFDM : Channel estimation and reflection optimization , ” IEEE Wireless Commun . Lett . , vol . 9 , no . 4 , pp . 518 – 522 , Apr . 2019 . [ 91 ] S . Desale , A . Rasool , S . Andhale , and P . Rane , “Heuristic and meta - heuristic algorithms and their relevance to the real world : A survey , ” Int . J . Comput . Eng . Res . Trends , vol . 351 , no . 5 , pp . 2349 – 7084 , May 2015 . [ 92 ] W . Yu , W . Rhee , S . Boyd , and J . M . Cioffi , “Iterative water - filling for gaussian vector multiple - access channels , ” IEEE Trans . Inf . Theory , vol . 50 , no . 1 , pp . 145 – 152 , Jan . 2004 . [ 93 ] A . Feriani and E . Hossain , “Single and multi - agent deep reinforcement learning for AI - enabled wireless networks : A tutorial , ” IEEE Commun . Surv . Tut . , vol . 23 , no . 2 , pp . 1226 – 1252 , Feb . 2021 . [ 94 ] Y . Yu , T . Wang , and S . C . Liew , “Deep - reinforcement learning multiple access for heterogeneous wireless networks , ” IEEE J . Sel . Areas Commun . , vol . 37 , no . 6 , pp . 1277 – 1290 , Jun . 2019 . [ 95 ] H . Du , Z . Li , D . Niyato , J . Kang , Z . Xiong , D . I . Kim et al . , “Enabling AI - generated content ( AIGC ) services in wireless edge networks , ” arXiv preprint arXiv : 2301 . 03220 , 2023 . [ 96 ] H . Zhou , Y . Wu , Y . Hu , and G . Xie , “A novel stable selection and reliable transmission protocol for clustered heterogeneous wireless sensor networks , ” Comput . Commun . , vol . 33 , no . 15 , pp . 1843 – 1849 , 2010 . [ 97 ] W . - K . Ching and M . K . Ng , “Markov chains , ” Models , algorithms and applications , 2006 . [ 98 ] X . Xu , B . Shen , S . Ding , G . Srivastava , M . Bilal , M . R . Khosravi , V . G . Menon , M . A . Jan , and M . Wang , “Service offloading with deep Q - network for digital twinning - empowered internet of vehicles in edge computing , ” iEEE Trans . Industr . Inform . , vol . 18 , no . 2 , pp . 1414 – 1423 , Feb . 2020 . [ 99 ] M . Ohira , K . Takano , and Z . Ma , “A novel deep - Q - network - based fine - tuning approach for planar bandpass filter design , ” IEEE Microw . Wireless Compon . Lett . , vol . 31 , no . 6 , pp . 638 – 641 , Jun . 2021 . 31 [ 100 ] A . Iqbal , M . - L . Tham , and Y . C . Chang , “Double deep Q - network - based energy - efficient resource allocation in cloud radio access net - work , ” IEEE Access , vol . 9 , pp . 20440 – 20449 , 2021 . [ 101 ] H . Hasselt , “Double Q - learning , ” Proc . Adv . Neural Inf . Process . Syst . , vol . 23 , 2010 . [ 102 ] S . Vimal , M . Khari , R . G . Crespo , L . Kalaivani , N . Dey , and M . Kaliappan , “Energy enhancement using multiobjective ant colony optimization with double Q learning algorithm for IoT based cognitive radio networks , ” Comput . Commun . , vol . 154 , pp . 481 – 490 , 2020 . [ 103 ] N . C . Luong , D . T . Hoang , S . Gong , D . Niyato , P . Wang , Y . - C . Liang , and D . I . Kim , “Applications of deep reinforcement learning in communications and networking : A survey , ” IEEE Commun . Surv . Tut . , vol . 21 , no . 4 , pp . 3133 – 3174 , Apr . 2019 . [ 104 ] Y . Xu , W . Xu , Z . Wang , J . Lin , and S . Cui , “Load balancing for ultradense networks : A deep reinforcement learning - based approach , ” IEEE Internet Things J . , vol . 6 , no . 6 , pp . 9399 – 9412 , Jun . 2019 . [ 105 ] R . Zhang , K . Xiong , W . Guo , X . Yang , P . Fan , and K . B . Letaief , “Q - learning - based adaptive power control in wireless RF energy harvesting heterogeneous networks , ” IEEE Syst . J . , vol . 15 , no . 2 , pp . 1861 – 1872 , 2021 . [ 106 ] X . Tian , K . Xiong , R . Zhang , P . Fan , D . Niyato , and K . B . Letaief , “Sum rate maximization in muti - cell muti - user networks : An inverse reinforcement learning - based approach , ” IEEE Wireless Commun . Lett . , pp . 1 – 1 , 2023 . [ 107 ] R . Zhang , K . Xiong , Y . Lu , B . Gao , P . Fan , and K . B . Letaief , “Joint coordinated beamforming and power splitting ratio optimization in MU - MISO SWIPT - enabled hetnets : A multi - agent DDQN - based approach , ” IEEE J . Sel . Areas Commun . , vol . 40 , no . 2 , pp . 677 – 693 , 2022 . [ 108 ] V . Mnih , K . Kavukcuoglu , D . Silver , A . A . Rusu , J . Veness , M . G . Bellemare , A . Graves , M . Riedmiller , A . K . Fidjeland , G . Ostrovski et al . , “Human - level control through deep reinforcement learning , ” Nature , vol . 518 , no . 7540 , pp . 529 – 533 , 2015 . [ 109 ] T . Schaul , J . Quan , I . Antonoglou , and D . Silver , “Prioritized experience replay , ” arXiv preprint arXiv : 1511 . 05952 , 2015 . [ 110 ] M . Hausknecht and P . Stone , “Deep recurrent Q - learning for partially observable MDPs , ” in AAAI Fall Symposium Series , 2015 . [ 111 ] R . Zhang , K . Xiong , Y . Lu , P . Fan , D . W . K . Ng , and K . B . Letaief , “Energy efficiency maximization in RIS - assisted SWIPT networks with rsma : A PPO - based approach , ” IEEE J . Sel . Areas Commun . , vol . 41 , no . 5 , pp . 1413 – 1430 , 2023 . [ 112 ] R . J . Williams , “Simple statistical gradient - following algorithms for connectionist reinforcement learning , ” Reinforcement Learning , pp . 5 – 32 , 1992 . [ 113 ] M . Hessel , J . Modayil , H . Van Hasselt , T . Schaul , G . Ostrovski , W . Dabney , D . Horgan , B . Piot , M . Azar , and D . Silver , “Rainbow : Combining improvements in deep reinforcement learning , ” in Proc . AAAI Conf . Artif . Intell . , vol . 32 , no . 1 , 2018 . [ 114 ] C . Wang , L . Liu , C . Jiang , S . Wang , P . Zhang , and S . Shen , “Incorpo - rating distributed DRL into storage resource optimization of space - air - ground integrated wireless communication network , ” IEEE J . Sel . Top . Signal Process . , vol . 16 , no . 3 , pp . 434 – 446 , Mar . 2021 . [ 115 ] Y . Li , X . Hu , Y . Zhuang , Z . Gao , P . Zhang , and N . El - Sheimy , “Deep reinforcement learning ( DRL ) : Another perspective for unsupervised wireless localization , ” IEEE Internet Things J . , vol . 7 , no . 7 , pp . 6279 – 6287 , Jul . 2019 . [ 116 ] J . Tang , A . Mihailovic , and H . Aghvami , “Constructing a DRL decision making scheme for multi - path routing in all - IP access network , ” in Proc . IEEE Global Commun . Conf . IEEE , 2022 , pp . 3623 – 3628 . [ 117 ] Y . Zhang , Y . Lu , R . Zhang , B . Ai , and D . Niyato , “Deep reinforcement learning for secrecy energy efficiency maximization in ris - assisted networks , ” IEEE Trans . Veh . Technol . , pp . 1 – 6 , 2023 . [ 118 ] N . M . Ashraf , R . R . Mostafa , R . H . Sakr , and M . Rashad , “Optimizing hyperparameters of deep reinforcement learning for autonomous driv - ing based on whale optimization algorithm , ” Plos one , vol . 16 , no . 6 , p . e0252754 , 2021 . [ 119 ] J . Fu , A . Kumar , O . Nachum , G . Tucker , and S . Levine , “D4RL : Datasets for deep data - driven reinforcement learning , ” arXiv preprint arXiv : 2004 . 07219 , 2020 . [ 120 ] E . Zhang , Y . Lu , W . Wang , and A . Zhang , “LAD : Language augmented diffusion for reinforcement learning , ” arXiv preprint arXiv : 2210 . 15629 , 2022 . [ 121 ] H . Wang , Y . Wu , S . Guo , and L . Wang , “PDPP : Projected diffu - sion for procedure planning in instructional videos , ” arXiv preprint arXiv : 2303 . 14676 , 2023 . [ 122 ] J . Brehmer , J . Bose , P . De Haan , and T . Cohen , “EDGI : Equiv - ariant diffusion for planning with embodied agents , ” arXiv preprint arXiv : 2303 . 12410 , 2023 . [ 123 ] Y . Cao , E . Rizk , S . Vlaski , and A . H . Sayed , “Multi - agent adver - sarial training using diffusion learning , ” in ICASSP 2023 - 2023 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 2023 , pp . 1 – 5 . [ 124 ] Z . Liang , Y . Mu , M . Ding , F . Ni , M . Tomizuka , and P . Luo , “Adapt - Diffuser : Diffusion models as adaptive self - evolving planners , ” arXiv preprint arXiv : 2302 . 01877 , 2023 . [ 125 ] T . Pearce , T . Rashid , A . Kanervisto , D . Bignell , M . Sun , R . Georgescu , S . V . Macua , S . Z . Tan , I . Momennejad , K . Hofmann et al . , “Imitating human behaviour with diffusion models , ” arXiv preprint arXiv : 2301 . 10677 , 2023 . [ 126 ] F . Vargas , T . Reu , and A . Kerekes , “Expressiveness remarks for denois - ing diffusion models and samplers , ” arXiv preprint arXiv : 2305 . 09605 , 2023 . [ 127 ] R . Liu , H . Liu , D . Kwak , Y . Xiang , C . Borcea , B . Nath , and L . Iftode , “Balanced traffic routing : Design , implementation , and evaluation , ” Ad Hoc Networks , vol . 37 , pp . 14 – 28 , 2016 . [ 128 ] D . Watson , W . Chan , J . Ho , and M . Norouzi , “Learning fast samplers for diffusion models by differentiating through sample quality , ” in Int . Conf . Learn . Represent . , 2021 . [ 129 ] S . Hong , G . Lee , W . Jang , and S . Kim , “Improving sample quality of diffusion models using self - attention guidance , ” arXiv preprint arXiv : 2210 . 00939 , 2022 . [ 130 ] Z . Lyu , X . Xu , C . Yang , D . Lin , and B . Dai , “Accelerating diffu - sion models via early stop of the diffusion process , ” arXiv preprint arXiv : 2205 . 12524 , 2022 . [ 131 ] M . Dai , L . Luo , J . Ren , H . Yu , and G . Sun , “PSACCF : Prioritized on - line slice admission control considering fairness in 5G / B5G networks , ” IEEE Trans . Netw . Sci . Eng . , vol . 9 , no . 6 , pp . 4101 – 4114 , Jun . 2022 . [ 132 ] S . Gu , E . Holly , T . Lillicrap , and S . Levine , “Deep reinforcement learn - ing for robotic manipulation with asynchronous off - policy updates , ” in Proc . IEEE Int . Conf . Robot . Autom . , 2017 , pp . 3389 – 3396 . [ 133 ] N . Khaneja , T . Reiss , C . Kehlet , T . Schulte - Herbr¨uggen , and S . J . Glaser , “Optimal control of coupled spin dynamics : design of NMR pulse sequences by gradient ascent algorithms , ” J . Magn . Reson . , vol . 172 , no . 2 , pp . 296 – 305 , Feb . 2005 . [ 134 ] S . Zhao , M . Gong , T . Liu , H . Fu , and D . Tao , “Domain generalization via entropy regularization , ” Adv . Neural Inf . Process . Syst . , vol . 33 , pp . 16096 – 16107 , 2020 . [ 135 ] G . Tesauro et al . , “Temporal difference learning and TD - Gammon , ” Commun . ACM , vol . 38 , no . 3 , pp . 58 – 68 , 1995 . [ 136 ] K . Cobbe , C . Hesse , J . Hilton , and J . Schulman , “Leveraging procedural generation to benchmark reinforcement learning , ” in Proc . Int . Conf . Mach . Learn . , vol . 119 . PMLR , 13 – 18 Jul 2020 , pp . 2048 – 2056 . [ 137 ] A . Raffin , “Rl baselines zoo , ” https : / / github . com / araffin / rl - baselines - zoo , 2018 . [ 138 ] E . Aubry , T . Silverston , A . Lahmadi , and O . Festor , “Crowdout : A mobile crowdsourcing service for road safety in digital cities , ” in 2014 IEEE International Conference on Pervasive Computing and Communication Workshops ( PERCOM WORKSHOPS ) , 2014 , pp . 86 – 91 . [ 139 ] Y . Liu , K . Wang , Y . Lin , and W . Xu , “ LightChain : A lightweight blockchain system for industrial Internet of Things , ” IEEE Trans . Indust . Inform . , vol . 15 , no . 6 , pp . 3571 – 3581 , June . 2019 . [ 140 ] R . Zeng , C . Zeng , X . Wang , B . Li , and X . Chu , “A comprehensive survey of incentive mechanism for federated learning , ” arXiv preprint arXiv : 2106 . 15406 , 2021 . [ 141 ] D . Yang , G . Xue , J . Zhang , A . Richa , and X . Fang , “Coping with a smart jammer in wireless networks : A stackelberg game approach , ” IEEE Trans . Wireless Commun . , vol . 12 , no . 8 , pp . 4038 – 4047 , Aug . 2013 . [ 142 ] X . Chen , Y . Deng , G . Zhu , D . Wang , and Y . Fang , “From resource auction to service auction : An auction paradigm shift in wireless networks , ” IEEE Wirel . Commun . , vol . 29 , no . 2 , pp . 185 – 191 , Apr . 2022 . [ 143 ] J . Kang , Z . Xiong , D . Niyato , D . Ye , D . I . Kim , and J . Zhao , “Toward secure blockchain - enabled internet of vehicles : Optimizing consensus management using reputation and contract theory , ” IEEE Trans . Veh . Technol . , vol . 68 , no . 3 , pp . 2906 – 2920 , 2019 . [ 144 ] Y . Liu , H . Du , D . Niyato , J . Kang , Z . Xiong , C . Miao , Xuemin , Shen , and A . Jamalipour , “Blockchain - empowered lifecycle management for AI - Generated Content ( AIGC ) products in edge networks , ” arXiv preprint arXiv : 2303 . 02836 , 2023 . 32 [ 145 ] A . Kumar , “Model complexity , ” https : / / vitalflux . com / model - complexity - overfitting - in - machine - learning / . [ 146 ] Microsoft , “The relationshio between model size and performance , ” https : / / learn . microsoft . com / en - us / semantic - kernel / prompt - engineering / llm - models . [ 147 ] H . Du , J . Wang , D . Niyato , J . Kang , Z . Xiong , J . Zhang , and X . Shen , “Semantic communications for wireless sensing : RIS - aided encoding and self - supervised decoding , ” IEEE J . Sel . Areas Commun . , to appear , 2023 . [ 148 ] H . Du , J . Wang , D . Niyato , J . Kang , Z . Xiong , M . Guizani , and D . I . Kim , “Rethinking wireless communication security in semantic internet of things , ” IEEE Wireless Commun . Mag . , to appear , 2023 . [ 149 ] J . Kang , H . Du , Z . Li , Z . Xiong , S . Ma , D . Niyato , and Y . Li , “Per - sonalized saliency in task - oriented semantic communications : Image transmission and performance analysis , ” IEEE J . Sel . Areas Commun . , vol . 41 , no . 1 , pp . 186 – 201 , Jan . 2022 . [ 150 ] Y . Lin , Z . Gao , Y . Tu , H . Du , D . Niyato , J . Kang , and H . Yang , “A Blockchain - based Semantic Exchange Framework for Web 3 . 0 toward Participatory Economy , ” IEEE Commun Mag , 2023 . [ 151 ] Y . Lin , Z . Gao , H . Du , D . Niyato , J . Kang , R . Deng , and X . S . Shen , “A unified blockchain - semantic framework for wireless edge intelligence enabled web 3 . 0 , ” IEEE Wirel Commun , 2023 . [ 152 ] Y . Lin , H . Du , D . Niyato , J . Nie , J . Zhang , Y . Cheng , and Z . Yang , “Blockchain - aided secure semantic communication for AI - generated content in metaverse , ” IEEE Open J . Comput . Soc . , vol . 4 , pp . 72 – 83 , 2023 . [ 153 ] H . Zhang , S . Shao , M . Tao , X . Bi , and K . B . Letaief , “Deep learning - enabled semantic communication systems with task - unaware transmit - ter and dynamic data , ” IEEE J . Sel . Areas Commun . , vol . 41 , no . 1 , pp . 170 – 185 , 2022 . [ 154 ] A . A . Alemi , I . Fischer , J . V . Dillon , and K . Murphy , “Deep variational information bottleneck , ” arXiv preprint arXiv : 1612 . 00410 , 2016 . [ 155 ] H . Du , J . Liu , D . Niyato , J . Kang , Z . Xiong , J . Zhang , and D . I . Kim , “Attention - aware resource allocation and QoE analysis for metaverse xURLLC services , ” IEEE J . Sel . Areas Commun . , to appear , 2023 . [ 156 ] Y . Liu , H . Yu , S . Xie , and Y . Zhang , “Deep reinforcement learning for offloading and resource allocation in vehicle edge computing and networks , ” IEEE Trans . Veh . Technol . , vol . 68 , no . 11 , pp . 11158 – 11168 , 2019 . [ 157 ] L . Yan , Z . Qin , R . Zhang , Y . Li , and G . Y . Li , “Qoe - aware resource allocation for semantic communication networks , ” in Proc . IEEE Global Commun . Conf . IEEE , 2022 , pp . 3272 – 3277 . [ 158 ] R . Zhang , K . Xiong , H . Du , D . Niyato , J . Kang , X . Shen , and H . V . Poor , “Generative AI - enabled vehicular networks : Fundamentals , framework , and case study , ” arXiv preprint arXiv : 2304 . 11098 , 2023 . [ 159 ] H . Zhou , W . Xu , J . Chen , and W . Wang , “Evolutionary V2X technolo - gies toward the internet of vehicles : Challenges and opportunities , ” Proc . IEEE , vol . 108 , no . 2 , pp . 308 – 323 , Feb . 2020 . [ 160 ] W . Duan , J . Gu , M . Wen , G . Zhang , Y . Ji , and S . Mumtaz , “Emerging technologies for 5G - IoV networks : Applications , trends and opportu - nities , ” IEEE Net . , vol . 34 , no . 5 , pp . 283 – 289 , 2020 . [ 161 ] H . Zhou , W . Xu , J . Chen , and W . Wang , “Evolutionary v2x technolo - gies toward the Internet of Vehicles : Challenges and opportunities , ” Proc . IEEE , vol . 108 , no . 2 , pp . 308 – 323 , 2020 . [ 162 ] L . Liang , G . Y . Li , and W . Xu , “Resource allocation for D2D - enabled vehicular communications , ” IEEE Trans . Commun . , vol . 65 , no . 7 , pp . 3186 – 3197 , 2017 . [ 163 ] S . Huang , Z . Wang , P . Li , B . Jia , T . Liu , Y . Zhu , W . Liang , and S . - C . Zhu , “Diffusion - based generation , optimization , and planning in 3D scenes , ” in Proc . IEEE CVPR , June 2023 , pp . 16750 – 16761 . [ 164 ] L . Liang , H . Ye , and G . Y . Li , “Spectrum sharing in vehicular networks based on multi - agent reinforcement learning , ” IEEE J . Sel . Areas Commun . , vol . 37 , no . 10 , pp . 2282 – 2292 , 2019 . [ 165 ] K . Dovelos , M . Matthaiou , H . Q . Ngo , and B . Bellalta , “Channel estimation and hybrid combining for wideband terahertz massive mimo systems , ” IEEE J . Sel . Areas Commun . , vol . 39 , no . 6 , pp . 1604 – 1620 , Jun . 2021 . [ 166 ] Y . Liu , Z . Tan , H . Hu , L . J . Cimini , and G . Y . Li , “Channel estimation for OFDM , ” IEEE Commun . Surv . Tutor . , vol . 16 , no . 4 , pp . 1891 – 1908 , Apr . 2014 . [ 167 ] S . J . Nawaz , K . I . Ahmed , M . N . Patwary , and N . M . Khan , “Superim - posed training - based compressed sensing of sparse multipath channels , ” IET Commun . , vol . 6 , no . 18 , pp . 3150 – 3156 , 2012 . [ 168 ] Y . Liao , Y . Hua , X . Dai , H . Yao , and X . Yang , “Chanestnet : A deep learning based channel estimation for high - speed scenarios , ” in ICC 2019 - 2019 IEEE international conference on communications ( ICC ) . IEEE , 2019 , pp . 1 – 6 . [ 169 ] M . Arvinte and J . I . Tamir , “MIMO channel estimation using score - based generative models , ” IEEE Trans . Wireless Commun . , 2022 . [ 170 ] Y . Song and S . Ermon , “Improved techniques for training score - based generative models , ” Adv . Neural Inf . Process . Syst . , vol . 33 , pp . 12438 – 12448 , 2020 . [ 171 ] G . Lin , A . Milan , C . Shen , and I . Reid , “Refinenet : Multi - path refinement networks for high - resolution semantic segmentation , ” in Proceedings of the IEEE conference on computer vision and pattern recognition , 2017 , pp . 1925 – 1934 . [ 172 ] E . Balevi , A . Doshi , A . Jalal , A . Dimakis , and J . G . Andrews , “High dimensional channel estimation using deep generative networks , ” IEEE J . Sel . Areas Commun . , vol . 39 , no . 1 , pp . 18 – 30 , 2020 . [ 173 ] P . Schniter and A . Sayeed , “Channel estimation and precoder design for millimeter - wave communications : The sparse way , ” in 2014 48th Asilomar conference on signals , systems and computers . IEEE , 2014 , pp . 273 – 277 . [ 174 ] A . Jalal , M . Arvinte , G . Daras , E . Price , A . G . Dimakis , and J . Tamir , “Robust compressed sensing mri with deep generative priors , ” Adv . Neural Inf . Process . Syst . , vol . 34 , pp . 14938 – 14954 , 2021 . [ 175 ] E . Biglieri , Coding for wireless channels . Springer Science & Business Media , 2005 . [ 176 ] Y . Choukroun and L . Wolf , “Error correction code transformer , ” Adv . Neural Inf . Process . Syst . , vol . 35 , pp . 38695 – 38705 , 2022 . [ 177 ] Z . Chen , J . Qing , T . Xiang , W . L . Yue , and J . H . Zhou , “Seeing beyond the brain : Conditional diffusion model with sparse masked modeling for vision decoding , ” in Proc . IEEE / CVF CVPR , June 2023 , pp . 22710 – 22720 . [ 178 ] Y . Choukroun and L . Wolf , “Denoising diffusion error correction codes , ” arXiv preprint arXiv : 2209 . 13533 , 2022 . [ 179 ] T . Wu , Z . Chen , D . He , L . Qian , Y . Xu , M . Tao , and W . Zhang , “CDDM : Channel denoising diffusion models for wireless communi - cations , ” arXiv preprint arXiv : 2305 . 09161 , 2023 . [ 180 ] N . Cheng , W . Xu , W . Shi , Y . Zhou , N . Lu , H . Zhou , and X . Shen , “Air - ground integrated mobile edge networks : Architecture , challenges , and opportunities , ” IEEE Commun . Mag . , vol . 56 , no . 8 , pp . 26 – 32 , Aug . 2018 . [ 181 ] X . Cao , B . Yang , C . Yuen , and Z . Han , “Hap - reserved communications in space - air - ground integrated networks , ” IEEE Trans . Veh . Tech . , vol . 70 , no . 8 , pp . 8286 – 8291 , Aug . 2021 . [ 182 ] H . Du , D . Niyato , Y . - A . Xie , Y . Cheng , J . Kang , and D . I . Kim , “Performance analysis and optimization for jammer - aided multiantenna UAV covert communication , ” IEEE J . Sel . Areas Commun . , vol . 40 , no . 10 , pp . 2962 – 2979 , Oct . 2022 . [ 183 ] D . Li , S . Wu , J . Jiao , N . Zhang , and Q . Zhang , “Age - oriented transmission protocol design in space - air - ground integrated networks , ” IEEE Trans . Wireless Commun . , vol . 21 , no . 7 , pp . 5573 – 5585 , Jul . 2022 . [ 184 ] Z . Jia , M . Sheng , J . Li , and Z . Han , “Toward data collection and transmission in 6G space - air - ground integrated networks : Cooperative HAP and LEO satellite schemes , ” IEEE Internet Things J . , vol . 9 , no . 13 , pp . 10516 – 10528 , Sept . 2021 . [ 185 ] H . Cui , J . Zhang , Y . Geng , Z . Xiao , T . Sun , N . Zhang , J . Liu , Q . Wu , and X . Cao , “Space - air - ground integrated network ( SAGIN ) for 6G : Requirements , architecture and challenges , ” China Commun . , vol . 19 , no . 2 , pp . 90 – 108 , Feb . 2022 . [ 186 ] N . Cheng , W . Quan , W . Shi , H . Wu , Q . Ye , H . Zhou , W . Zhuang , X . Shen , and B . Bai , “A comprehensive simulation platform for space - air - ground integrated network , ” IEEE Wireless Commun . , vol . 27 , no . 1 , pp . 178 – 185 , Jan . 2020 . [ 187 ] J . Liu , Y . Shi , Z . M . Fadlullah , and N . Kato , “Space - air - ground integrated network : A survey , ” IEEE Commun . Surveys Tuts . , vol . 20 , no . 4 , pp . 2714 – 2741 , Apr . 2018 . [ 188 ] J . Ye , S . Dang , B . Shihada , and M . - S . Alouini , “Space - air - ground inte - grated networks : Outage performance analysis , ” IEEE Trans . Wireless Commun . , vol . 19 , no . 12 , pp . 7897 – 7912 , Dec . 2020 . [ 189 ] W . Mao , K . Xiong , Y . Lu , P . Fan , and Z . Ding , “Energy consumption minimization in secure multi - antenna UAV - assisted MEC networks with channel uncertainty , ” IEEE Trans . Wireless Commun . , pp . 1 – 1 , 2023 . [ 190 ] Z . Wang , J . Zhang , H . Du , E . Wei , B . Ai , D . Niyato , and M . Debbah , “Extremely large - scale MIMO : Fundamentals , challenges , solutions , and future directions , ” IEEE Wireless Commun . , 2023 . [ 191 ] H . Du , J . Zhang , J . Cheng , and B . Ai , “Millimeter wave communica - tions with reconfigurable intelligent surfaces : Performance analysis and optimization , ” IEEE Trans . Commun . , vol . 69 , no . 4 , pp . 2752 – 2768 , Apr . 2021 . 33 [ 192 ] Z . Wang , J . Zhang , B . Ai , C . Yuen , and M . Debbah , “Uplink per - formance of cell - free massive MIMO with multi - antenna users over jointly - correlated Rayleigh fading channels , ” IEEE Trans . Wireless Commun . , vol . 21 , no . 9 , pp . 7391 – 7406 , Sept . 2022 . [ 193 ] H . Du , J . Zhang , K . Guan , D . Niyato , H . Jiao , Z . Wang , and T . K¨urner , “Performance and optimization of reconfigurable intelligent surface aided THz communications , ” IEEE Trans . Commun . , vol . 70 , no . 5 , pp . 3575 – 3593 , May 2022 . [ 194 ] Z . Wang , J . Zhang , H . Q . Ngo , B . Ai , and M . Debbah , “Uplink precoding design for cell - free massive MIMO with iteratively weighted MMSE , ” IEEE Trans . Commun . , vol . 71 , no . 3 , pp . 1646 – 1664 , Mar . 2023 . [ 195 ] Y . Cui , F . Liu , X . Jing , and J . Mu , “Integrating sensing and communi - cations for ubiquitous IoT : Applications , trends , and challenges , ” IEEE Netw . , vol . 35 , no . 5 , pp . 158 – 167 , May . 2021 . [ 196 ] J . Wang , Z . Tian , X . Yang , and M . Zhou , “TWPalo : Through - the - wall passive localization of moving human with Wi - Fi , ” Computer Commun . , vol . 157 , pp . 284 – 297 , 2020 . [ 197 ] W . Njima , M . Chafii , A . Chorti , R . M . Shubair , and H . V . Poor , “Indoor localization using data augmentation via selective generative adversarial networks , ” IEEE Access , vol . 9 , pp . 98337 – 98347 , 2021 . [ 198 ] X . Chen , H . Li , C . Zhou , X . Liu , D . Wu , and G . Dudek , “Fido : Ubiquitous fine - grained wifi - based localization for unlabelled users via domain adaptation , ” in Proc . Web Conf . , 2020 , pp . 23 – 33 . [ 199 ] C . Xiao , D . Han , Y . Ma , and Z . Qin , “CsiGAN : Robust channel state information - based activity recognition with gans , ” IEEE Internet Things J . , vol . 6 , no . 6 , pp . 10191 – 10204 , 2019 . [ 200 ] W . K . New , K . - K . Wong , H . Xu , K . - F . Tong , and C . - B . Chae , “Fluid antenna system : New insights on outage probability and diversity gain , ” IEEE Trans . Wireless Commun . , to appear , 2023 . [ 201 ] M . Khammassi , A . Kammoun , and M . - S . Alouini , “A new analytical approximation of the fluid antenna system channel , ” IEEE Trans . Wireless Commun . , to appear , 2023 . [ 202 ] A . Shojaeifard , K . - K . Wong , K . - F . Tong , Z . Chu , A . Mourad , A . Haghighat , I . Hemadeh , N . T . Nguyen , V . Tapio , and M . Juntti , “MIMO evolution beyond 5g through reconfigurable intelligent sur - faces and fluid antenna systems , ” Proc . IEEE , vol . 110 , no . 9 , pp . 1244 – 1265 , Sept . 2022 . [ 203 ] L . Tlebaldiyeva , G . Nauryzbayev , S . Arzykulov , A . Eltawil , and T . Tsiftsis , “Enhancing QoS through fluid antenna systems over corre - lated nakagami - m fading channels , ” in Proc . IEEE Wireless Commun . Netw . Conf . IEEE , 2022 , pp . 78 – 83 . [ 204 ] Y . Zhao , F . Zhou , L . Feng , W . Li , and P . Yu , “Madrl - based 3d deployment and user association of cooperative mmwave aerial base stations for capacity enhancement , ” Chinese J . Electron . , vol . 32 , no . 2 , pp . 283 – 294 , 2023 . [ 205 ] Y . Lin , Z . Gao , H . Du , J . Kang , D . Niyato , Q . Wang , J . Ruan , and S . Wan , “DRL - based adaptive sharding for blockchain - based federated learning , ” IEEE Trans . Commun . , 2023 .