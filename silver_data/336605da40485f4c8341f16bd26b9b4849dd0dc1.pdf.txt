Characterizing Online Discussion Using Coarse Discourse Sequences Amy X . Zhang MIT CSAIL Cambridge , MA , USA axz @ mit . edu Bryan Culbertson Google Mountain View , CA , USA bryan . culbertson @ gmail . com Praveen Paritosh Google Mountain View , CA , USA pkp @ google . com Abstract In this work , we present a novel method for classifying com - ments in online discussions into a set of coarse discourse acts towards the goal of better understanding discussions at scale . To facilitate this study , we devise a categorization of coarse discourse acts designed to encompass general online discus - sion and allow for easy annotation by crowd workers . We col - lect and release a corpus of over 9 , 000 threads comprising over 100 , 000 comments manually annotated via paid crowd - sourcing with discourse acts and randomly sampled from the site Reddit . Using our corpus , we demonstrate how the analy - sis of discourse acts can characterize different types of discus - sions , including discourse sequences such as Q & A pairs and chains of disagreement , as well as different communities . Fi - nally , we conduct experiments to predict discourse acts using our corpus , ﬁnding that structured prediction models such as conditional random ﬁelds can achieve an F1 score of 75 % . We also demonstrate how the broadening of discourse acts from simply question and answer to a richer set of categories can improve the recall performance of Q & A extraction . Introduction As more social interaction takes place online , researchers have become interested in studying the discourse occur - ring in online social media . From these studies , researchers can examine how people conduct conversations and argu - ments ( Hasan and Ng 2014 ; Tan et al . 2016 ) as well as ex - tract information for applications such as search ( Cong et al . 2008 ) . While many studies have focused their analyses on metadata surrounding community discussions , other stud - ies have attempted to analyze the textual content of discus - sions . But this can be difﬁcult as language and interactions are complex and variable from discussion to discussion and community to community . One method for understanding discussion is through an - alyzing the high level discourse structures inherent within conversations . Much research has demonstrated the power of using discourse acts , also known as speech acts , which are categories of utterances that pertain to their role in the discussion ( e . g . “question” or “answer” ) . Researchers have used discourse acts towards applications such as building conversational bots ( Allen , Ferguson , and Stent 2001 ) and Copyright c (cid:13) 2017 , Association for the Advancement of Artiﬁcial Intelligence ( www . aaai . org ) . All rights reserved . summarizing spoken discourse ( Murray et al . 2006 ) . How - ever , a great deal of research using discourse acts has fo - cused solely on extracting questions and answers ( Hong and Davison 2009 ) or considered only communities for help or technical support ( Kim , Wang , and Baldwin 2010 ) . In this work , we develop a richer categorization of dis - course acts towards characterizing a wide range of discus - sions from a variety of communities . Our 9 discourse act categories , developed over many iterations with experts and with the crowd , is designed to cover general discourse and be simple enough for crowd annotators to classify . We use as our source of data discussions from the website Reddit , one of the top ten most visited sites in the U . S , according to Alexa . com . By sampling over 9 , 000 discussion threads from the entirety of Reddit , which is comprised of thousands of “subreddits” , we can gain information on discourse from many kinds of communities . From these threads , which are made of chains of comments replying to one another , we use a crowd system to annotate each comment within a thread with its discourse act as well as its discourse relation , or the comment to which it is responding . We are releasing this dataset 1 , which is to our knowledge the largest manually an - notated dataset of discourse acts in online discussions . From an analysis of the major discourse acts and se - quences within our corpus , we uncover patterns of discourse that correspond to well - known blocks of interactions , such as arguments and Q & A . This allows us to identify subred - dits that behave much like community Q & A ( CQA ) sites like Quora , or that are more argumentative in nature . Through building supervised models for classifying dis - course acts , we ﬁnd that structured prediction models such as conditional random ﬁelds ( CRFs ) achieve the greatest performance , with a 75 % average F1 score on our dataset . We also analyze how well our best model classiﬁes question and answer comments compared with models that contain fewer discourse acts . We ﬁnd that our model with 9 dis - course acts has overall better recall and slightly better F1 scores compared with models that only label question and answer comments . This suggests that having an enriched un - derstanding of discourse structure beyond simply questions and answers can improve Q & A extraction . 1 https : / / github . com / dmorr - google / coarse - discourse Related Work Discourse Act Classiﬁcation Prior work has sought to de - velop a categorization of discourse acts for the purpose of characterizing discussion . Some early work focused only on conversational speech ( Austin 1975 ; Searle 1969 ) . Since then , researchers have developed standard taxonomies of spoken discourse acts such as DAMSL ( Stolcke et al . 2000 ) and DiAML ( Bunt et al . 2010 ) . However , many of these discourse acts for spoken discourse do not translate to on - line asynchronous mediums . When it comes to online dis - cussion , researchers have developed categories for discus - sions within e - mail ( Cohen , Carvalho , and Mitchell 2004 ) , online classrooms ( Feng et al . 2006 ) , newsgroups ( Xi , Lind , and Brill 2004 ) , and help forums ( Kim , Wang , and Bald - win 2010 ) . Much of this work on taxonomy development informs the ﬁnal categories that we use . However , we de - velop a novel categorization that can be applied broadly in unstructured online forums of any topic or function . Techniques for developing categories in prior work usu - ally involve manual inspection and reﬁnement by a knowl - edgeable annotator , such as one of the researchers . From there , the annotated dataset is used to build supervised ( Kim , Wang , and Baldwin 2010 ) or semi - supervised ( Jeong , Lin , and Lee 2009 ) models for predicting categories . Some re - search has attempted to learn categories using an unsuper - vised approach ( Ritter , Cherry , and Dolan 2010 ) . In our work , we chose to use manual annotation of categories from a set of acts reﬁned by the authors . However , since our dataset is an order of magnitude larger than any prior manu - ally annotated dataset , we turn to crowd workers to conduct the annotation . While prior work has had as many as 40 cat - egories , we are limited in the number of categories as well as the level of detail we ask each annotator to provide . Argumentation and Online Education Another line of work that is relevant to ours is the study of back - and - forth argumentation . Researchers have mined arguments online to learn how people take stances ( Hasan and Ng 2014 ) or have developed systems for structured arguing ( Klein 2011 ) . Some of our work has overlap with work identifying argu - mentation , such as the classiﬁcation of agreements and dis - agreements . This allows us to characterize communities by their proportion of and average length of arguments . The work mentioned above is separate from systems that study argumentation within a single piece of text , such as within an opinion article or legal statement . Most of these have annotations at the sentence level . Classiﬁcations in this area include Rhetorical Structure Theory ( RST ) ( Mann and Thompson 1988 ) and the Claim - Premises scheme ( Freeman 1991 ) . Because we analyzed text at the comment level as opposed to sentence level , we did not apply these classiﬁca - tions directly , though some categories have overlap . Discourse acts have also been studied in the context of education ( De Wever et al . 2006 ; Scheuer et al . 2010 ) . Much of this work overlaps with the research on argumen - tation , as educators seek to understand and identify pro - ductive argumentation within the classroom . Researchers have developed novel categorizations to ﬁnd evidence of critical thinking ( Jeong 2003 ) and have also looked at dis - course sequences , including adjacency pairs and chains ( Lu , Chiu , and Law 2011 ; Ros´e et al . 2008 ) . Like the ARGU - NAUT system ( McLaren , Scheuer , and Mikˇs´atko 2010 ) , we take a closer look at identifying and understanding “chains of opposition” as well as popular discussion pairs such as “question - answer” . However , many of our categorizations are different as we seek to characterize general discussion , while these works focus on classroom discussions and have categories more similar to argumentation systems . Discourse Acts in Online Discussion In recent years , re - searchers have become interested in extracting useful in - formation from online discussion . However , many analyses only focus on a particular community ( Tan et al . 2016 ) . Ad - ditionally , there has been little work analyzing online com - munities through the lens of high level discourse acts . Re - search in this area has focused on extraction of Q & A con - tent from online forums ( Cong et al . 2008 ; Hong and Davi - son 2009 ) or characterizing the types and quantity of Q & A content on different community platforms ( Agichtein et al . 2008 ; Morris , Teevan , and Panovich 2010 ) . Other research expands beyond Q & A but still focus on areas such as tech - nical help forums ( Kim , Wang , and Baldwin 2010 ) . Instead , we characterize a wide range of online communities using a richer classiﬁcation of discourse acts . Discourse Act Annotation Discourse Acts We developed a set of 9 discourse act categories using a manual iterative process with experts coupled with pilots using the crowd . While there has been prior work on de - veloping discourse acts for online forums , many do not ﬁt our purposes because they are too detailed or too narrow in scope ( Kim , Wang , and Baldwin 2010 ) . Also , most have not released their annotated data or details of their coding scheme . In the end , our set of acts most closely resembles efforts such as ( Feng et al . 2006 ) , ( Fortuna , Rodrigues , and Milic - Frayling 2007 ) , and ( Xi , Lind , and Brill 2004 ) . To build the discourse act categories , the ﬁrst author ran - domly sampled threads from Reddit and , using prior work as a guide , classiﬁed comments into categories in an iterative process . After achieving a stable set of categories from mul - tiple iterations , the authors then ran three pilots with crowd workers on datasets of 40 threads also randomly sampled , and iterated based on the inter - rater reliability returned . We also solicited qualitative feedback from the crowd workers , who were the same people throughout the annotation pro - cess . Some categories were eventually discarded due to too much overlap with other categories ( A NECDOTE , FYI ) , or too low volume ( S UMMARY , R ESOLUTION ) . Discourse Relations As discourse acts are usually understood in relation to an - other piece of discourse , we collected both the discourse act of a comment as well as the discourse relation of that com - ment , also known as a link to a prior comment that the com - ment is responding to , if it exists . For instance , an A NSWER is always related to a prior Q UESTION . Some categories may not always be in relation to another comment , such as a new Q UESTION or an A NNOUNCEMENT . In some catego - rizations of discourse , such as RST ( Mann and Thompson 1988 ) , there are only discourse relations , and the relations themselves are grouped into categories and named . In our case , we do not name types of discourse relations explicitly , but they are implicitly inferred by the discourse acts they link . For instance , a hypothetical discourse relation “An - swers” would always link A NSWER to Q UESTION . Discourse Act Deﬁnitions Detailed information about each discourse act and the rela - tions allowed are given below . For our annotators , we pro - vided a lengthier tutorial and several examples for each act , which we will release with our dataset . Q UESTION : A comment with a question or a request seeking some form of feedback , help , or other kinds of re - sponses . While the comment may contain a question mark , it is not required . For instance , it might be posed in the form of a statement but still soliciting a response . Also , not every - thing that has a question mark is automatically a Q UESTION . For instance , rhetorical questions are not seeking a response . Relation : This comment might be the ﬁrst in a thread and have no relation to another comment . Or , it could be a clari - fying or follow - up Q UESTION linking to any prior comment . A NSWER : A comment that is responding to a Q UESTION by answering the question or fulﬁlling the request . There can be more than one A NSWER responding to a Q UESTION . Relation : An A NSWER is always linked to a Q UESTION . A NNOUNCEMENT : A comment that is presenting some new information to the community , such as a piece of news , a link to something , a story , an opinion , a review , or insight . Relation : This comment has no relation to a prior com - ment and is always the initial post in a thread . A GREEMENT : A comment that is expressing agreement with some information presented in a prior comment . It can be agreeing with a point made , providing supporting evi - dence , providing a positive example or experience , or con - ﬁrming or acknowledging a point made . Relation : This comment is always linked to a prior com - ment to which it is agreeing . A PPRECIATION : A comment that is expressing thanks , appreciation , excitement , or praise in response to another comment . In contrast to A GREEMENT , it is not evaluating the merits of the points brought up . Comments of this cate - gory are more interpersonal as opposed to informational . Relation : This comment is always linked to a prior com - ment for which it is expressing appreciation . D ISAGREEMENT : A comment that is correcting , criticiz - ing , contradicting , or objecting to a point made in a prior comment . It can also be providing evidence to support its disagreement , such as an example or contrary anecdote . Relation : This comment is always linked to a prior com - ment to which it is disagreeing . N EGATIVE R EACTION : A comment that is expressing a negative reaction to a previous comment , such as attacking or mocking the commenter , or expressing emotions like dis - gust , derision , or anger , to the contents of the prior comment . This comment is not discussing the merits of the points made in a prior comment or trying to correct them . Relation : This comment is always linked to a prior com - ment to which it is negatively reacting . E LABORATION : A comment that is adding additional information on to another comment . Oftentimes , one can imagine it simply appended to the end of the comment it elaborates on . One can elaborate on many kinds of com - ments , for instance , a question - asker elaborating on their question to provide more context , or someone elaborating on an answer to add more information . Relation : This comment is always linked to a prior com - ment upon which it is elaborating . H UMOR : This comment is primarily a joke , a piece of sarcasm , or a pun intended to get a laugh or be silly but not trying to add information . If a comment is sarcastic but using sarcasm to make a point or provide feedback , then it may belong in a different category . Relation : At times , this comment links to another com - ment but other times it may not be responding to anything . Data Collection Sampling Reddit Threads We randomly sampled from the full Reddit dataset start - ing from its inception to the end of May 2016 , which is made available publicly as a dump on Google BigQuery 2 . We chose to sample from the entire dataset as opposed to a set of subreddits to ensure a wide variety of communities within our dataset . The full dataset of Reddit from this time period contains 238 million threads . However , we performed several ﬁlters on the data before sampling as we were inter - ested in collecting substantial back - and - forth discussion . Minimum Replies : As our goal is to better understand discussion , we chose to only take threads that had at least two reply comments to the initial post so that there was some amount of back - and - forth . Disqualifying these threads de - creased the dataset to 87 . 5 million threads . We took a ran - dom sample of 50 , 000 threads from this dataset , and on this smaller set , we performed the following additional ﬁlters . Deleted Comments : We disqualiﬁed any threads that contained a deleted comment or deleted portions of the ini - tial post , as it would be difﬁcult to interpret replies to deleted comments . Non - English : As our annotators were English - speaking , we ignored any threads coming from subreddits primarily in a different language . We manually went through the most frequent several hundred subreddits in our dataset and added them to a blacklist if their homepage was primarily in a dif - ferent language . Annotators were also instructed to skip any threads that were in a different language . NSFW : In order to not subject our annotators to pornogra - phy , we additionally blacklisted 693 subreddits labeled Not Safe For Work ( NSFW ) by a third - party subreddit catego - rization site 3 that is community - sourced . This does not in - 2 https : / / bigquery . cloud . google . com / dataset / fh - bigquery : reddit posts 3 http : / / redditlist . com / nsfw clude subreddits that discuss potentially illegal or explicit content , which are still included in our dataset . Trading : We also wished to avoid subreddits that were primarily for trading or coordination , mostly in the context of gaming , because these subreddits have little to no ac - tual discussion . Examples include / r / friendsafari or / r / fireteams . We developed manual rules , such as if the subreddit name ends with the word “swap” or “trade” , as well as manually curated a short blacklist . After conducting ﬁltering , we had 32 , 728 threads or 65 % of our random sample . We chose to sample link - post threads , or threads where the body of the post is a link to a picture , video , or webpage , at 10 % of our sample , leaving 90 % of our sample to be self - post threads , or threads where the body of the post is a piece of text written to the community . This was so that we could collect a higher proportion of Q & A - related threads , since this data is particularly valuable in a search and information retrieval context . In our ﬁltered dataset , we had 10 , 145 self - post threads ( 31 % ) and 22 , 583 link - post threads ( 69 % ) . From our ﬁl - tered dataset , we sampled 9 , 000 self - posts threads and 1 , 000 link - post threads . During annotation , some threads were dis - carded due to bugs that occurred so that in the end , 9 , 701 threads were fully annotated . Annotation Annotation of discourse acts was conducted by crowd work - ers contracted from a paid crowdsourcing platform . In total , 25 annotators were hired , and they were paid an hourly rate above federal and state minimum wage . They were required to be native English speakers . To divvy up the work , each thread comprised a task , and users were given batches of 40 tasks at a time . For each task , annotators were asked to mark the discourse act of each of the comments within the page as well as the relation of each comment to a prior comment , if it existed . Each comment was annotated by three annotators . As comments sometimes perform multiple functions , we instructed crowd annotators to consider the content at the comment level as opposed to sentence or paragraph level to make the task simpler . We did allow annotators to add a second discourse category to a comment when it was doing two separate actions in series , such as answering a question and then asking a new question . Secondary categories were annotated infrequently in our dataset ( less than 3 % of anno - tations ) , so we ignore them going forward in our analyses . However , they are available in the released dataset . Com - ments may also sometimes be responding to multiple other comments , such as thanking multiple A NSWER comments at once . In these cases , we asked annotators to only anno - tate one relation to the closest comment in terms of thread distance that they were responding to . Finally , we allowed annotators to annotate a comment as O THER if they could not place it into any of the categories ; this was the majority category in 1 . 8 % of comments . Before workers began annotating , they were presented with an instruction manual that explained each category and showed examples of annotated discussions . They were also given a few warm - up threads to do as practice before begin - ning annotation . The annotation was done using a Chrome Category Krippendorf’s Alpha All Categories 0 . 645 Question 0 . 823 Answer 0 . 785 Announcement 0 . 732 Appreciation 0 . 611 Agreement 0 . 426 Elaboration 0 . 401 Disagreement 0 . 383 Humor 0 . 343 Negative Reaction 0 . 330 Table 1 : Inter - rater reliability of the different discourse acts . browser extension that allowed annotators to easily click - to - highlight comments , annotate and link comments , and cycle through the different threads in their task . Finally , some threads on Reddit have hundreds of com - ments or more . As this is too much work for an annotator to perform in one sitting , we limited the thread length to 40 replies . This was done using Reddit’s default “best” sorting 4 by appending ? limit = 40 to every URL in our dataset . These threads represented less than 1 % of our dataset . Annotator Agreement In Table 1 , we present the inter - rater reliability of each dis - course act using Krippendorf’s Alpha . As can be seen , some acts had more agreement between annotators than others . The least reliable categories were N EGATIVE R EACTION and H UMOR . From analyzing comments where annotators disagreed , we noticed examples where , in the case of H U - MOR , a comment was being sarcastic or silly but it was not obvious without knowing the context . We also noticed that several categories had some overlap with E LABORATION , such as A GREEMENT and D ISAGREEMENT . This was per - haps because annotators did not agree on the degree to which a comment was primarily agreeing or disagreeing with a prior comment versus more neutrally elaborating on a prior comment with more information . As mentioned , we also asked annotators to link each com - ment to a prior comment to which it was in relation , if such a comment existed . From analyzing the relation annotations , we found that 98 . 9 % of comments had a majority link be - tween the three annotations . Of those comments , the average percent agreement with the majority link was 95 . 6 % . Thus , while some categories had lower agreement , the relation an - notations were almost entirely in agreement . In the end , for the rest of our analyses , we consider the comments that had a majority category that was not O THER across the three annotators . After removing comments with - out a majority category , this resulted in 9 , 131 threads with 101 , 525 comments ( 87 . 5 % of the original number of com - ments ) , posted from 2 , 837 communities by 61 , 174 unique author accounts . While we do not analyze the comments without a majority category , we are releasing individual an - notations in our public dataset as potential future work . 4 https : / / redditblog . com / 2009 / 10 / 15 / reddits - new - comment - sorting - system / Discourse Act % Self - Post % Link - Post Total % TotalCount Answer 42 . 3 % 15 . 8 % 41 . 5 % 41658 Elaboration 18 . 1 % 26 . 1 % 18 . 8 % 18927 Question 17 . 5 % 15 . 2 % 17 . 6 % 17681 Appreciation 8 . 3 % 14 . 1 % 8 . 8 % 8807 Agreement 5 . 0 % 5 . 2 % 5 . 1 % 5072 Disagreement 3 . 3 % 4 . 0 % 3 . 4 % 3436 Humor 2 . 1 % 6 . 6 % 2 . 4 % 2409 Announcement 1 . 6 % 8 . 0 % 2 . 0 % 2024 Negative Reaction 1 . 7 % 5 . 0 % 1 . 9 % 1899 Table 2 : The percentage and count of comments from each discourse act in our dataset in total , and the percentage bro - ken down by link - post and self - post threads . DiscourseSequence % Self - Post % Link - Post Total % TotalCount Ques - Ans 47 . 4 % 18 . 4 % 39 . 2 % 39394 Ans - Elab 6 . 6 % 3 . 2 % 5 . 5 % 5545 Elab - Elab 5 . 4 % 7 . 3 % 4 . 7 % 4749 Ques - Ans - Elab 14 . 0 % 6 . 5 % 13 . 6 % 5271 Ques - Ans - Appr 8 . 8 % 3 . 6 % 8 . 6 % 3322 Ques - Ans - Ques 8 . 1 % 3 . 2 % 7 . 8 % 3036 Table 3 : The percentage and count of the three most frequent 2 - chain and 3 - chain discourse sequences in our dataset in total , and percentage broken down by self - post and link - post threads . Data Analysis We now present analyses of the discourse acts and sequences in our annotated dataset that contained a majority anno - tated discourse act . In Table 2 , we present the proportion of each discourse act in our dataset , as well as broken down by threads started with link - posts and self - posts , and in Ta - ble 3 , we present the most frequent 2 - chain and 3 - chain dis - course sequences in our dataset , where a chain constitutes a series of replies . As can be seen , Q UESTION s and A NSWER s make up a large portion of the dataset , partially due to sam - pling more heavily from self - posts . However , even the least frequent discourse act , N EGATIVE R EACTION , has nearly 2 , 000 comments , which is on its own larger than many en - tire datasets ( Kim , Wang , and Baldwin 2010 ) . We also have more A GREEMENT s than D ISAGREEMENT s , echoing prior work on blogs ( Gilbert , Bergstrom , and Karahalios 2009 ) . Next we consider the discourse relations that were anno - tated . Of the annotations that had a majority relation and were not the ﬁrst comment in the thread , 98 . 3 % of these relations were to the direct parent of the comment , as des - ignated by Reddit’s threaded structure . Thus , on Reddit , the reply relation available via the site is already a close approx - imation of the proper discourse relation . Discourse Sequences We analyze prevalent discourse sequences to better under - stand the major types of discussion in our dataset . For in - stance , the ﬁrst comment in a thread can signal what hap - pens in the rest of the thread . In our dataset , 78 % of threads Most Questions Total % Most Answers Total % iama 44 % askwomen 69 % casualiama 44 % weddingplanning 65 % fakeid 34 % shittyadvice 64 % jailbreak 34 % askreddit 64 % techsupport 31 % explainlikeimﬁve 63 % buildapcforme 30 % manga 63 % feedthebeast 29 % music 62 % tipofmytongue 28 % anime 62 % Table 4 : Subreddits with highest proportion of Q UESTION comments and A NSWER comments . started as a Q UESTION , while 22 % of threads started as an A NNOUNCEMENT . However , threads starting out as Q UES - TION s are concentrated among the self - post threads , with 82 % of self - post threads starting out as a Q UESTION , while only 17 % of link - post threads start out as a Q UESTION . Q & A Q & A pairs are well - studied discourse sequences in research ( Cong et al . 2008 ; Morris , Teevan , and Panovich 2010 ) because of their applications to information retrieval and relation to CQA sites . Using our dataset , we can look at discourse sequences that go beyond Q & A pairs to pro - vide richer information about discussions that begin with a question . Focusing on the 7 , 150 threads in our dataset that start as questions , 88 % of immediate replies to the ﬁrst com - ment are A NSWER s and 6 % are follow - up Q UESTION s . The Q UESTION s that are in response to a Q UESTION may be of interest as “clarifying” questions for overly broad requests . As seen in Table 3 , Q & A pairs are followed primarily by E LABORATION ( 33 % ) , A PPRECIATION ( 21 % ) , and Q UES - TION ( 18 % ) . E LABORATION s could be seen as extensions or augmentations of the A NSWER comment , which could be useful for informational retrieval applications . A PPRE - CIATION s could be seen as an additional signal of quality , on top of signals such as “accepted” answers in some CQA sites such as Yahoo ! Answers or community upvotes . In our dataset , 73 % of A PPRECIATION comments in response to A NSWER s were by the question - asker . While other works have estimated the number of ques - tions and answers in other social platforms ( Morris , Teevan , and Panovich 2010 ; Paul , Hong , and Chi 2011 ) , we can pro - vide the ﬁrst estimate towards the Reddit corpus . Because of our ﬁlters , we cannot provide an estimate of the number of total Q UESTION s , including unanswered ones . Instead we can make an estimate of around 29 . 4 million ( ± 0 . 3 million ) self - post threads and 8 . 8 million ( ± 1 . 5 million ) link - post threads that start with a Q UESTION and have at least two replies , using a 95 % conﬁdence interval . Given our other ﬁl - ters , this estimate is a lower bound on the entirety of Reddit . We can also examine Q & A at the community level to ﬁnd sites that behave much like CQA sites like StackOverﬂow or Quora . We focus our analysis on the 186 communities that have 100 or more comments in our dataset . Looking at Figure 4 we show the subreddits with the highest pro - portion of Q UESTION s and A NSWER s . While some subred - dits are clearly dedicated to Q & A , such as / r / askwomen or / r / explainlikeimfive , other subreddits such as Community Total % Avg Chain Len canada 21 . 7 % 1 . 4 changemyview 20 . 0 % 1 . 5 politicaldiscussion 17 . 1 % 1 . 8 smite 17 . 1 % 1 . 3 dndnext 12 . 3 % 1 . 6 reddevils 10 . 9 % 1 . 1 politics 10 . 5 % 1 . 1 atheism 10 . 1 % 1 . 7 Table 5 : Subreddits with highest proportion of D ISAGREE - MENT comments , shown with their average length of chains of D ISAGREEMENT . / r / weddingplanning or / r / manga are not obviously about Q & A from their name but may operate like a CQA site for a speciﬁc domain . We also found that the top sub - reddits for A NSWER are different than the top subreddits for Q UESTION . This suggests that some subreddits may have more A NSWER s per Q UESTION on average than others . In our dataset , Q UESTION s that appear as the initial post re - ceived on average 3 . 99 A NSWER s ( SD = 3 . 57 ) . This signal could be useful towards the task of predicting whether a par - ticular question or a community overall is informational or conversational ( Harper , Moy , and Konstan 2009 ) . Arguments Another sequence of interest is the “chain of disagreement” or sequence of D ISAGREEMENT comments replying to each other , which signify an argument occur - ring ( McLaren , Scheuer , and Mik ˇ s ´ atko 2010 ) . Overall , we had 2 , 712 chains of D ISAGREEMENT of size 1 to 7 com - ments , with 17 % of chains longer than 1 comment , and an average chain length of 1 . 23 comments . From this data , we can analyze what concludes arguments if anything . Fo - cusing only on the pages where there were 40 comments or fewer , so no comments were excluded from annotation , we found that 61 % of D ISAGREEMENT s were followed by nothing . Arguments followed with an E LABORATION 18 % of the time , which can be interpreted as a comment elaborating on the arguments of a prior comment , or con - tinuing the argument . D ISAGREEMENT chains ended with A GREEMENT only 7 % of the time , which may character - ize a concession in the disagreement or a resolution . In Ta - ble 5 , we show the subreddits with the highest proportion of D ISAGREEMENT comments out of the communities with over 100 comments in our dataset . We also calculate av - erage chain length , ﬁnding that some subreddits that are more dedicated to debate , such as / r / changemyview or / r / politicaldiscussion have longer arguments than other subreddits such as / r / politics . Announcements There is less research into the kinds of discussions that start out as an A NNOUNCEMENT . However , these threads do constitute a large portion of Reddit , given that 59 % of the Reddit corpus with 2 replies is link - post threads , and over 80 % of link - post threads in our dataset be - gin with A NNOUNCEMENT . To understand the major types of discussion in reply to an A NNOUNCEMENT , we cluster the 2 , 024 threads by their proportion of discourse acts in the replies , using k - means with 4 clusters . The best silhouette score ( Rousseeuw 1987 ) determined the cluster number . • Appreciation ( 18 % ) : One cluster has a high aver - age proportion of A PPRECIATION comments at 62 % . Threads in this cluster come primarily from subreddits such as / r / keto ( related to the ketogenic diet ) and / r / stopdrinking ( about abstaining from alcohol ) , where people post updates on their personal goals and re - ceive encouragement . • Arguments ( 21 % ) : Another cluster has a higher pro - portion of H UMOR ( 37 % ) , D ISAGREEMENT ( 35 % ) , and A GREEMENT ( 34 % ) comments . Threads in this clus - ter come from subreddits like / r / politics and / r / atheism , where most announcements are news ar - ticles , and arguments and jokes occur in the replies . • Q & A ( 29 % ) : A third cluster is predominantly threads with Q & A , at 35 % of discourse pairs . Some notable subreddits represented include / r / pcmasterrace ( re - lated to PC gaming ) and / r / ultrahardcore ( related to a mode in the Minecraft game ) , where announcements more readily lead to requests for more information . • Elaboration ( 32 % ) : The ﬁnal cluster is primarily E LABORATION comments , at 85 % . The predominant subreddit in this cluster is a gaming community , / r / leagueoflegends . In this cluster , users might pass around stories , tips , or opinions building on each other regarding a particular topic . Predicting Discourse Acts We investigate how well supervised models for extracting discourse acts perform , experimenting with both structured and unstructured models . Because our annotated dataset has shown that discourse relations map well to the existing Red - dit reply structure , we focus only on the discourse act multi - class classiﬁcation task . Features Content + Punctuation : We collect unigrams , bigrams , and trigrams from the text of the comment . If the comment has a title , in the case of the initial post , then the n - grams of the title are counted separately from the n - grams of the body . We use a word tokenizer that tokenizes punctuation instead of stripping it so that we count potentially important punc - tuation like question marks or exclamation points . We use TF - IDF weighting and set a minimum document frequency of 50 comments . Structure : We calculate several features related to the structure of the comment and its position . One feature is the depth of the comment according to Reddit’s threaded struc - ture , which we collect as both a raw count and normalized by the number of comments in the discussion . We also cal - culate number of sentences , number of words , and number of characters of both the body and the title of the comment . We computed these values for both the current comment and the parent comment . Author : We collect features about the author of the com - ment , including a binary feature for whether the current Model Accuracy Precision Recall F1 All Answers 0 . 43 0 . 16 0 . 41 0 . 23 Q - Mark & Answers 0 . 55 0 . 30 0 . 52 0 . 38 LogReg 0 . 672 0 . 657 0 . 672 0 . 648 SVM - HMM 0 . 708 0 . 673 0 . 708 0 . 680 CRF 0 . 763 0 . 747 0 . 763 0 . 747 Table 6 : Results of the models to predict discourse acts . commenter is also the commenter of the initial post and a bi - nary feature for whether the current commenter is the same as the parent commenter . Thread : We calculate features that are the same across all comments in the thread . One feature is the total num - ber of comments in the discussion . Another is the num - ber of unique branches in the discussion tree . We also record whether the discussion originated as a self - post or a link - post . Finally , we collect the average length of all the branches or threads of discussion in the discussion tree . Community : We have a feature naming the subreddit that the thread came from , as some subreddits have a greater por - portion of some types of discourse and not others . Other experiments we conducted were with features such as word overlap between parent and current comment , dis - course act priors for each author across the training set , num - ber of replies to the current comment , and sentiment analy - sis . These are omitted here for space reasons and because they did not lead to improvements in performance . Data and Models For comparison with our models , we designed two baselines , one where all questions are labeled as A NSWER ( All An - swers ) and a slightly more sophisticated one where all initial posts and also comments containing a question mark in the text are labeled as Q UESTION , while all other comments are labeled as A NSWER ( Q - Mark & Answers ) . Our ﬁrst model is a standard logistic regression model using L2 regularization and the LibLinear optimization , implemented in scikit - learn 5 . Our next two models are structured prediction models that take into account the se - quence of comments . The ﬁrst is a hidden Markov model with second - order transition dependencies and no emis - sion dependencies , using the SVM hmm library ( Joachims 2008 ) . Finally , we build a conditional random ﬁeld , using CRFSuite ( Okazaki 2007 ) , with the Orthant - Wise Limited - memory Quasi - Newton ( OWL - QN ) training algorithm and L1 regularization . These models were chosen because prior work has suggested that models such as these that capture structural dependencies within a sequence of labels provide important information for identifying discourse acts ( Ding et al . 2008 ; Kim , Wang , and Baldwin 2010 ) . To split our training and testing sets , we conducted strati - ﬁed 10 - fold cross validation , splitting our data at the thread level . This is so all comments from a single discussion are in the same training or testing group . Additionally , the structured prediction models require items provided in a se - 5 http : / / scikit - learn . org Figure 1 : Average F1 scores of each model broken down by each discourse act along with their prevalence in the dataset . quence . However , discussions on Reddit branch outwards like a tree instead of being append - only . Thus , for those models , we constructed for each discussion tree all possible branches as individual sequences . If a comment has multiple replies , each of those would be part of a separate sequence of comments . When it came to evaluating the results of a struc - tured prediction model , as a comment may be represented in multiple sequences and thus tagged multiple times , we col - lect all the predicted tags for a given comment and assign the most common tag to that comment . Thus , our evaluation metrics only count each comment in the test dataset once . Results We report results of our experiments in Table 6 . The metrics shown are all for prediction at the comment level , as op - posed to at the thread level . As can be seen , the CRF model performs the best overall , achieving an average F1 score of 75 % . Both structured prediction models perform better than the logistic regression model , demonstrating that the con - text , encapsulated by the preceding comment classiﬁcations , is important towards determining the discourse act of the current comment . Finally , all models perform better than the two baselines given . In Figure 1 , we break down the average F1 scores by dis - course act . The CRF model performs best across all the cat - egories , though some categories such as D ISAGREEMENT , H UMOR , and N EGATIVE R EACTION have relatively low F1 scores . These categories may be more difﬁcult to distinguish for humans as well , as these had the lowest inter - rater reli - ability in Table 1 . A test for correlation between inter - rater reliability and CRF F1 score yields a strong positive corre - lation ( Spearman’s rank , ρ = 0 . 917 , p < 0 . 001 ) . Given the differences in performance across the discourse acts and also different frequencies of discourse acts in dif - ferent communities , we would expect that some subreddits would have higher performance overall . Considering only the subreddits with over 100 predictions made , we saw 4 subreddits have an average F1 score above 90 % across the 10 folds , with the highest being / r / boardgames with Features Accuracy Precision Recall F1 All 0 . 763 0 . 747 0 . 763 0 . 747 All - Author 0 . 762 0 . 746 0 . 763 0 . 744 All - Thread 0 . 761 0 . 747 0 . 760 0 . 746 All - Community 0 . 743 0 . 738 0 . 742 0 . 739 All - Content 0 . 587 0 . 538 0 . 588 0 . 550 All - Structure 0 . 540 0 . 564 0 . 539 0 . 507 Table 7 : Results of feature ablation experiments , removing one feature group at a time . 94 % average F1 . There was also 38 subreddits with an aver - age F1 between 80 % and 90 % . On the other hand , 16 sub - reddits had an average F1 below 70 % , with the lowest being / r / funny at 45 % average F1 . Feature Ablation We examine the importance of the dif - ferent feature groups we used by performing a set of feature ablation experiments . Removing one of the feature groups but retaining the rest for each of the feature groups , we calculate evaluation metrics using the best - performing CRF model . As can be seen in Table 7 , the most important fea - ture groups included the structural and content features . In contrast , the thread and author feature groups had the least impact on the classiﬁcation accuracy . Comparison to Q & A - Only Models Finally , we focus on our best model’s performance on predicting the categories of Q UESTION and A NSWER due to their outsize importance in information retrieval research . In many works , these are the only discourse acts considered when performing Q & A pre - diction . However , the introduction of additional categories could worsen performance on Q & A prediction by introduc - ing more confusion between Q & A and the other labels . We consider how our models would perform if they only had to predict Q UESTION or A NSWER comments , or both , with an O THER category signifying the rest . In Table 8 , we can see how well our CRF model per - forms with regards to classifying Q UESTION s , when we vary the number of discourse act labels . The best precision is achieved when the model is only a binary classiﬁer be - tween Q UESTION and O THER . However , the best recall and F1 on Q UESTION prediction is achieved when all 9 of our discourse acts are used as labels . As the difference in F1 is small between the Q + A + Other model and the 9 categories model , we conduct an 1 - way ANOVA test using the 10 cross validation folds from evaluations with each model . From this test , we ﬁnd there is a statistically signiﬁcant difference be - tween the F1 scores of the two models ( F = 10 . 97 , p < 0 . 005 ) . In Table 9 , we show results for classifying A NSWER com - ments . Precision is relatively low when only A NSWER and O THER categories are used and is best when classifying Q UESTION , A NSWER , and O THER . This may be because A NSWER comments are dependent on having a preceding Q UESTION , and Q UESTION comments may be easier to identify . The model with all 9 discourse acts has the best recall and shares the best F1 score with the Q & A model . Altogether , for predicting Q UESTION and A NSWER , the CRF model containing all 9 discourse acts performs bet - Categories Precision Recall F1 Question + Other 0 . 877 0 . 791 0 . 832 Q + A + Other 0 . 875 0 . 784 0 . 827 All 9 categories 0 . 854 0 . 823 0 . 837 Table 8 : Results for predicting Q UESTION using the CRF model and varying the number of discourse acts represented . Categories Precision Recall F1 Answer + Other 0 . 793 0 . 837 0 . 815 Q + A + Other 0 . 87 0 . 898 0 . 885 All 9 categories 0 . 855 0 . 917 0 . 885 Table 9 : Results for predicting A NSWER using the CRF model and varying the number of discourse acts represented . ter or on par with a model predicting only Q UESTION or A NSWER , or both , due mainly to improvements in recall . The improvements may be because having a richer discourse act categorization would allow for more ﬁne - grained transi - tion probabilities . On the other hand , overall precision de - creases slightly , due to the greater number of classiﬁcations for which a comment could be mistakenly classiﬁed . Discussion In this work , we present a new coarse discourse act catego - rization for online discussion as well as a new dataset of dis - cussions labeled with discourse acts and relations from a di - versity of communities . We demonstrate how discourse acts can tell us more about common sequences of discourse and isolate CQA - like communities . We show that using struc - tured models such as CRF , we can build classiﬁers to predict discourse acts at a 75 % F1 score . Our model with 9 cate - gories also improves in recall over models with only Q & A labels for the tasks of Q UESTION and A NSWER prediction . New applications become possible with the ability to tag comments with discourse acts . For instance , labeled dis - course acts could help moderators know whether existing questions have been answered ( Kim , Li , and Kim 2010 ) or step in to resolve lingering disputes . Users with questions could be routed towards more CQA - like communities when there may be several subreddits dedicated to the same topic , such as / r / askscience versus / r / science . Another area that could use discourse acts is discussion summarization ( Murray et al . 2006 ; Rambow et al . 2004 ) . Most automatic summarization techniques are built for long individual documents as opposed to a sequence of discourse acts . It is also unclear what an ideal summary for a dis - cussion would look like . One consideration is that different types of discussions could warrant different types of sum - maries . For instance , an argument might be summarized by summarizing the arguments on one side followed by the ar - guments on the other side . Quantities might also be useful for the summary of an argument , for instance how many comments had one stance versus another . Our discourse acts for A GREEMENT and D ISAGREEMENT may be useful for stance classiﬁcation ( Rosenthal and McKeown 2015 ) . Al - ternatively , the summary for a Q UESTION followed by a series of A NSWER s might instead be a short sentence ex - tracted from the Q UESTION comment and the highest voted answer , the most frequent answer , or a series of common an - swers , depending on the nature of the question . Knowing the discourse structures may help determine what kind of sum - mary is needed and from which comments to pull sentences , if the summarization strategy is extractive . This information could also help support existing systems for manual discus - sion summarization ( Zhang , Verou , and Karger 2017 ) . Finally , this dataset can be useful to improve search en - gines and natural dialogue systems such as chat bots and virtual assistants . Search engines and virtual assistants that gather answers to queries from documents on the web can use discourse acts to better characterize community search results . For instance , snippets could be taken from A N - SWER comments as opposed to other portions of the thread . Queries that return more conversational Q & A threads with many answers to a question could trigger a different inter - face or interaction than more informational queries , such as clusters of answers grouped by stance , sentiment , or topic . Answers that are controversial , meaning they are followed by an argument , could be marked as such . Future Work We conducted our analysis using the site Reddit , which has some particular characteristics that may not transfer to com - munities on other sites . For instance , Reddit is a threaded discussion forum while many forums are append - only . In the future , we plan to analyze a non - threaded forum like TripAdvisor . Expanding also allows us to look beyond the overall Reddit community , which has biases compared to the average internet user . This dataset and analysis was based on a discourse act classiﬁcation that we developed , which may not be suitable for particular tasks . For instance , some researchers may de - sire a more ﬁne - grained categorization for a particular dis - course act . Future work could expand our classiﬁcation to create a taxonomy and augment our dataset with more de - tailed annotations , or use other datasets ( Sameki , Barua , and Paritosh 2016 ) in concert with ours . For instance , our set of Q UESTION comments could be further labeled into informa - tional and conversational questions ( Harper , Moy , and Kon - stan 2009 ) . Future work could also build on our dataset by collecting annotations at the sub - comment level or collect - ing additional tags or relations per comment . We imagine empirical analyses of online discussions could be furthered using this dataset . Prior studies on question - answering ( Harper et al . 2008 ) , argumentation ( Tan et al . 2016 ) , echo chambers ( Gilbert , Bergstrom , and Kara - halios 2009 ) , and gratitude ( Spiro , Matias , and Monroy - Hern´andez 2016 ) have used datasets signiﬁcantly smaller than our dataset or focused on only one or a few commu - nities . Other work includes observing how characteristics of communities and authors relate to discourse structures , such as the role of social and administrator moderation in shaping discourse or how structural properties such as community size , diversity of users , and age can cause discourse to vary . Finally , our dataset suggests further work in question - answering . For instance , much research looks at Q & A at the start of a thread . However , as shown in our dataset , many Q & A pairs exist deeper in discussion threads . Future work could work on determining which Q & A pairs can be under - stood on their own , and ﬁnding ways to resolve ambiguity and bring in context for Q & A pairs that require context . Conclusion Using a novel discourse act categorization , we present one of the largest manually annotated datasets of threads of dis - cussion sampled from thousands of communities on Reddit , with each comment in each thread annotated with its dis - course act and relation . From our dataset , we observe com - mon patterns of discourse sequences , including Q & A and arguments , and use these signals to characterize communi - ties . Finally , we conduct experiments on classiﬁcation of dis - course acts , with a structured CRF model achieving a 75 % F1 score . We additionally demonstrate how our use of 9 dis - course acts overall improves recall of Q & A detection over a model that only labels questions and answers . Acknowledgements We would like to thank Ka Wong , Akihiro Matsukawa , Olivia Rhinehart , and Nancy Chang for their input and as - sistance , as well as our annotators . References Agichtein , E . ; Castillo , C . ; Donato , D . ; Gionis , A . ; and Mishne , G . 2008 . Finding high - quality content in social media . In WSDM ’08 , 183 – 194 . ACM . Allen , J . ; Ferguson , G . ; and Stent , A . 2001 . An architecture for more realistic conversational systems . In IUI ’01 , 1 – 8 . ACM . Austin , J . L . 1975 . How to do things with words . Oxford University Press . Bunt , H . ; Alexandersson , J . ; Carletta , J . ; Choe , J . - W . ; Fang , A . C . ; Hasida , K . ; Lee , K . ; Petukhova , V . ; Popescu - Belis , A . ; Romary , L . ; et al . 2010 . Towards an iso standard for dia - logue act annotation . In Seventh conference on International Language Resources and Evaluation ( LREC’10 ) . Cohen , W . W . ; Carvalho , V . R . ; and Mitchell , T . M . 2004 . Learning to classify email into“speech acts” . In EMNLP ’04 , 309 – 316 . Cong , G . ; Wang , L . ; Lin , C . - Y . ; Song , Y . - I . ; and Sun , Y . 2008 . Finding question - answer pairs from online forums . In SIGIR ’08 , 467 – 474 . ACM . De Wever , B . ; Schellens , T . ; Valcke , M . ; and Van Keer , H . 2006 . Content analysis schemes to analyze transcripts of on - line asynchronous discussion groups : A review . Computers & education 46 ( 1 ) : 6 – 28 . Ding , S . ; Cong , G . ; Lin , C . - Y . ; and Zhu , X . 2008 . Using conditional random ﬁelds to extract contexts and answers of questions from online forums . In ACL ’08 , volume 8 , 710 – 718 . Citeseer . Feng , D . ; Shaw , E . ; Kim , J . ; and Hovy , E . 2006 . Learn - ing to detect conversation focus of threaded discussions . In NAACL HLT ’06 , 208 – 215 . ACL . Fortuna , B . ; Rodrigues , E . M . ; and Milic - Frayling , N . 2007 . Improving the classiﬁcation of newsgroup messages through social network analysis . In CIKM ’07 , 877 – 880 . ACM . Freeman , J . B . 1991 . Dialectics and the macrostructure of arguments : A theory of argument structure , volume 10 . Walter de Gruyter . Gilbert , E . ; Bergstrom , T . ; and Karahalios , K . 2009 . Blogs are echo chambers : Blogs are echo chambers . In HICSS ’09 , 1 – 10 . IEEE . Harper , F . M . ; Raban , D . ; Rafaeli , S . ; and Konstan , J . A . 2008 . Predictors of answer quality in online q & a sites . In CHI ’08 , 865 – 874 . ACM . Harper , F . M . ; Moy , D . ; and Konstan , J . A . 2009 . Facts or friends ? : distinguishing informational and conversational questions in social q & a sites . In CHI ’09 , 759 – 768 . ACM . Hasan , K . S . , and Ng , V . 2014 . Why are you taking this stance ? identifying and classifying reasons in ideological de - bates . In EMNLP ’14 , 751 – 762 . Hong , L . , and Davison , B . D . 2009 . A classiﬁcation - based approach to question answering in discussion boards . In SI - GIR ’09 , 171 – 178 . ACM . Jeong , M . ; Lin , C . - Y . ; and Lee , G . G . 2009 . Semi - supervised speech act recognition in emails and forums . In EMNLP ’09 , 1250 – 1259 . ACL . Jeong , A . C . 2003 . The sequential analysis of group inter - action and critical thinking in online . The American Journal of Distance Education 17 ( 1 ) : 25 – 43 . Joachims , T . 2008 . Svm - hmm : Sequence tagging with struc - tural support vector machines . Kim , J . ; Li , J . ; and Kim , T . 2010 . Towards identifying unre - solved discussions in student online forums . In NAACL HLT ’10 , 84 – 91 . ACL . Kim , S . N . ; Wang , L . ; and Baldwin , T . 2010 . Tagging and linking web forum posts . In CoNLL ’10 , 192 – 202 . ACL . Klein , M . 2011 . How to harvest collective wisdom on com - plex problems : An introduction to the mit deliberatorium . Center for Collective Intelligence working paper . Lu , J . ; Chiu , M . M . ; and Law , N . W . 2011 . Collaborative ar - gumentation and justiﬁcations : A statistical discourse anal - ysis of online discussions . Computers in Human Behavior 27 ( 2 ) : 946 – 955 . Mann , W . C . , and Thompson , S . A . 1988 . Rhetorical struc - ture theory : Toward a functional theory of text organiza - tion . Text - Interdisciplinary Journal for the Study of Dis - course 8 ( 3 ) : 243 – 281 . McLaren , B . M . ; Scheuer , O . ; and Mikˇs´atko , J . 2010 . Sup - porting collaborative learning and e - discussions using artiﬁ - cial intelligence techniques . International Journal of Artiﬁ - cial Intelligence in Education 20 ( 1 ) : 1 – 46 . Morris , M . R . ; Teevan , J . ; and Panovich , K . 2010 . What do people ask their social networks , and why ? : a survey study of status message q & a behavior . In CHI ’10 , 1739 – 1748 . ACM . Murray , G . ; Renals , S . ; Carletta , J . ; and Moore , J . 2006 . In - corporating speaker and discourse features into speech sum - marization . In NAACL HLT ’06 , 367 – 374 . ACL . Okazaki , N . 2007 . Crfsuite : a fast implementation of condi - tional random ﬁelds ( crfs ) . Paul , S . A . ; Hong , L . ; and Chi , E . H . 2011 . Is twitter a good place for asking questions ? a characterization study . In ICWSM ’11 . AAAI . Rambow , O . ; Shrestha , L . ; Chen , J . ; and Lauridsen , C . 2004 . Summarizing email threads . In NAACL HLT ’04 , 105 – 108 . ACL . Ritter , A . ; Cherry , C . ; and Dolan , B . 2010 . Unsupervised modeling of twitter conversations . In NAACL HLT ’10 , 172 – 180 . ACL . Ros´e , C . ; Wang , Y . - C . ; Cui , Y . ; Arguello , J . ; Stegmann , K . ; Weinberger , A . ; and Fischer , F . 2008 . Analyzing collabo - rative learning processes automatically : Exploiting the ad - vances of computational linguistics in computer - supported collaborative learning . International journal of computer - supported collaborative learning 3 ( 3 ) : 237 – 271 . Rosenthal , S . , and McKeown , K . 2015 . I couldn’t agree more : The role of conversational structure in agreement and disagreement detection in online discussions . In 16th An - nual Meeting of the Special Interest Group on Discourse and Dialogue , 168 . Rousseeuw , P . J . 1987 . Silhouettes : a graphical aid to the interpretation and validation of cluster analysis . Journal of computational and applied mathematics 20 : 53 – 65 . Sameki , M . ; Barua , A . ; and Paritosh , P . 2016 . Rigorously collecting commonsense judgments for complex question - answer content . In HCOMP ’16 . AAAI . Scheuer , O . ; Loll , F . ; Pinkwart , N . ; and McLaren , B . M . 2010 . Computer - supported argumentation : A review of the state of the art . International Journal of Computer - Supported Collaborative Learning 5 ( 1 ) : 43 – 102 . Searle , J . R . 1969 . Speech acts : An essay in the philosophy of language , volume 626 . Cambridge University Press . Spiro , E . S . ; Matias , J . N . ; and Monroy - Hern ´ andez , A . 2016 . Networks of gratitude : Structures of thanks and user expec - tations in workplace appreciation systems . In ICWSM ’16 . AAAI . Stolcke , A . ; Coccaro , N . ; Bates , R . ; Taylor , P . ; Van Ess - Dykema , C . ; Ries , K . ; Shriberg , E . ; Jurafsky , D . ; Martin , R . ; and Meteer , M . 2000 . Dialogue act modeling for automatic tagging and recognition of conversational speech . Compu - tational linguistics 26 ( 3 ) : 339 – 373 . Tan , C . ; Niculae , V . ; Danescu - Niculescu - Mizil , C . ; and Lee , L . 2016 . Winning arguments : Interaction dynamics and per - suasion strategies in good - faith online discussions . In WWW ’16 , 613 – 624 . Xi , W . ; Lind , J . ; and Brill , E . 2004 . Learning effective rank - ing functions for newsgroup search . In SIGIR ’04 , 394 – 401 . ACM . Zhang , A . X . ; Verou , L . ; and Karger , D . 2017 . Wikum : Bridging discussion forums and wikis using recursive sum - marization . In CSCW 17 . ACM .