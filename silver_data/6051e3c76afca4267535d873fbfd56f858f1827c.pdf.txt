Q UICKLY F INDING A B ENIGN R EGION VIA H EAVY B ALL M OMENTUM IN N ON - C ONVEX O PTIMIZATION Jun - Kun Wang & Jacob Abernethy Georgia Institute of Technology { jimwang , prof } @ gatech . edu A BSTRACT The Heavy Ball Method ( Polyak , 1964 ) , proposed by Polyak over ﬁve decades ago , is a ﬁrst - order method for optimizing continuous functions . While its stochas - tic counterpart has proven extremely popular in training deep networks , there are almost no known functions where deterministic Heavy Ball is provably faster than the simple and classical gradient descent algorithm in non - convex optimization . The success of Heavy Ball has thus far eluded theoretical understanding . Our goal is to address this gap , and in the present work we identify two non - convex problems where we provably show that the Heavy Ball momentum helps the it - erate to enter a benign region that contains a global optimal point faster . We show that Heavy Ball exhibits simple dynamics that clearly reveal the beneﬁt of using a larger value of momentum parameter for the problems . The ﬁrst of these optimization problems is the phase retrieval problem , which has useful ap - plications in physical science . The second of these optimization problems is the cubic - regularized minimization , a critical subroutine required by Nesterov - Polyak cubic - regularized method ( Nesterov & Polyak ( 2006 ) ) to ﬁnd second - order sta - tionary points in general smooth non - convex problems . 1 I NTRODUCTION Poylak’s Heavy Ball method ( Polyak ( 1964 ) ) has been very popular in modern non - convex optimiza - tion and deep learning , and the stochastic version ( a . k . a . SGD with momentum ) has become the de facto algorithm for training neural nets . Many empirical results show that the algorithm is better than the standard SGD in deep learning ( see e . g . Hoffer et al . ( 2017 ) ; Loshchilov & Hutter ( 2019 ) ; Wilson et al . ( 2017 ) ; Sutskever et al . ( 2013 ) ) , but there are almost no corresponding mathematical results that show a beneﬁt relative to the more standard ( stochastic ) gradient descent . Despite its pop - ularity , we still have a very poor justiﬁcation theoretically for its success in non - convex optimization tasks , and Kidambi et al . ( 2018 ) were able to establish a negative result , showing that Heavy Ball momentum cannot outperform other methods in certain problems . Furthermore , even for convex problems it appears that strongly convex , smooth , and twice differentiable functions ( e . g . strongly convex quadratic functions ) are one of just a handful examples for which a provable speedup over standard gradient descent can be shown ( e . g ( Lessard et al . , 2016 ; Goh , 2017 ; Ghadimi et al . , 2015 ; Gitman et al . , 2019 ; Loizou & Richt´arik , 2017 ; 2018 ; Gadat et al . , 2016 ; Scieur & Pedregosa , 2020 ; Sun et al . , 2019 ; Yang et al . , 2018a ; Can et al . , 2019 ; Liu et al . , 2020 ; Sebbouh et al . , 2020 ; Flam - marion & Bach , 2015 ) ) . There are even some negative results when the function is strongly convex but not twice differentiable . That is , Heavy Ball momentum might lead to a divergence in convex optimization ( see e . g . ( Ghadimi et al . , 2015 ; Lessard et al . , 2016 ) ) . The algorithm’s apparent success in modern non - convex optimization has remained quite mysterious . In this paper , we identify two non - convex optimization problems for which the use of Heavy Ball method has a provable advantage over vanilla gradient descent . The ﬁrst problem is phase re - trieval . It has some useful applications in physical science such as microscopy or astronomy ( see e . g . ( Cand´es et al . , 2013 ) , ( Fannjiang & Strohmer , 2020 ) , and ( Shechtman et al . , 2015 ) ) . The objective is min w ∈ R d f ( w ) : = 1 4 n (cid:80) ni = 1 (cid:0) ( x (cid:62) i w ) 2 − y i (cid:1) 2 , ( 1 ) 1 a r X i v : 2010 . 01449v1 [ c s . L G ] 4 O c t 2020 where x i ∈ R d is the design vector and y i = ( x (cid:62) i w ∗ ) 2 is the label of sample i . The goal is to recover w ∗ up to the sign that is not recoverable ( Cand´es et al . , 2013 ) . Under the Gaussian design setting ( i . e . x i ∼ N ( 0 , I d ) ) , it is known that the empirical risk minimizer ( 1 ) is w ∗ or − w ∗ , as long as the number of samples n exceeds the order of the dimension d ( see e . g . Bandeira et al . ( 2014 ) ) . Therefore , solving ( 1 ) allows one to recover the desired vector w ∗ ∈ R d up to the sign . Unfortunately the problem is non - convex which limits our ability to efﬁciently ﬁnd a minimizer . For this problem , there are many specialized algorithms that aim at achieving a better computational complexity and / or sample complexity to recover w ∗ modulo the unrecoverable sign ( e . g . ( Cai et al . , 2016 ; Cand´es & Li , 2014 ; Cand´es et al . , 2015 ; 2013 ; Chen & Cand´es , 2017 ; Duchi & Ruan , 2018 ; Ma et al . , 2017 ; 2018 ; Netrapalli et al . , 2013 ; Qu et al . , 2017 ; Tu et al . , 2016 ; Wang et al . , 2017a ; b ; Yang et al . , 2018b ; Zhang et al . , 2017a ; b ; Zheng & Lafferty , 2015 ) ) . Our goal is not about providing a state - of - the - art algorithm for solving ( 1 ) . Instead , we treat this problem as a starting point of understanding Heavy Ball momentum in non - convex optimization and hope for getting some insights on why Heavy Ball ( Algorithm 1 and 2 ) can be faster than the vanilla gradient descent in non - convex optimization and deep learning in practice . If we want to understand why Heavy Ball momentum leads to acceleration for a complicated non - convex problem , we should ﬁrst understand it in the simplest possible setting . We provably show that Heavy Ball recovers the desired vector w ∗ , up to a sign ﬂip , given a random isotropic initialization . Our analysis divides the execution of the algorithm into two stages . In the ﬁrst stage , the ratio of the projection of the current iterate w t on w ∗ to the projection of w t on the perpendicular component keeps growing , which makes the iterate eventually enter a benign region which is strongly convex , smooth , twice differentiable , and contains a global optimal point . Therefore , in the second stage , Heavy Ball has a linear convergence rate . Furthermore , up to a value , a larger value of the momentum parameter has a faster linear convergence than the vanilla gradient descent in the second stage . Yet , most importantly , we show that Heavy Ball momentum also has an important role in reducing the number of iterations in the ﬁrst stage , which is when the iterate might be in a non - convex region . We show that the higher the momentum parameter β , the fewer the iterations spent in the ﬁrst stage ( see also Figure 1 ) . Namely , momentum helps the iterate to enter a benign region faster . Consequently , using a non - zero momentum parameter leads to a speedup over the standard gradient descent ( β = 0 ) . Therefore , our result shows a provable acceleration relative to the vanilla gradient descent , for computing a global optimal solution in non - convex optimization . The second of these is solving a class of cubic - regularized problems , min w f ( w ) : = 12 w (cid:62) Aw + b (cid:62) w + ρ 3 (cid:107) w (cid:107) 3 , ( 2 ) where the matrix A ∈ R d × d is symmetric and possibly indeﬁnite . Problem ( 2 ) is a sub - routine of the Nesterov - Polyak cubic - regularized method ( Nesterov & Polyak ( 2006 ) ) , which aims to minimize a non - convex objective F ( · ) by iteratively solving w t + 1 = arg min w ∈ R d { ∇ F ( w t ) (cid:62) ( w − w t ) + 12 ( w − w t ) (cid:62) ∇ 2 F ( w ) ( w − w t ) + ρ 3 (cid:107) w − w t (cid:107) 3 } , ( 3 ) With some additional post - processing , the iterate w t converges to an ( (cid:15) g , (cid:15) h ) second order stationary point , deﬁned as { w : (cid:107)∇ f ( w ) (cid:107) ≤ (cid:15) g and ∇ 2 f ( w ) (cid:23) − (cid:15) h I d } for any small (cid:15) g , (cid:15) h > 0 . However , their algorithm needs to compute a matrix inverse to solve ( 2 ) , which is computationally expensive when the dimension is high . A very recent result due to Carmon & Duchi ( 2019 ) shows that vanilla gradient descent approximately ﬁnds the global minimum of ( 2 ) under mild conditions , which only needs a Hessian - vector product and can be computed in the same computational complexity as computing gradients ( Pearlmutter , 1994 ) , and hence is computationally cheaper than the matrix inversion of the Hessian . Our result shows that , similar to the case of phase retrieval , the use of Heavy Ball momentum helps the iterate to enter a benign region of ( 3 ) that contains a global optimal solution faster , compared to vanilla gradient descent . To summarize , our theoretical results of the two non - convex problems provably show the beneﬁt of using Heavy Ball momentum . Compared to the vanilla gradient descent , the use of momentum helps to accelerate the optimization process . The key to showing the acceleration in getting into benign regions of these problems is a family of simple dynamics due to Heavy Ball momentum . We will argue that the simple dynamics are not restricted to the two main problems considered in this paper . Speciﬁcally , the dynamics also naturally arise when solving the problem of top eigenvector computation ( Golub & Loan , 1996 ) and the problem of saddle points escape ( e . g . ( Jin et al . , 2017 ; Wang et al . , 2020 ) ) , which might imply the broad applicability of the dynamics for analyzing Heavy Ball in non - convex optimization . 2 Algorithm 1 : Heavy Ball method ( Polyak , 1964 ) ( Equivalent version 1 ) 1 : Required : step size η and momentum parameter β ∈ [ 0 , 1 ] . 2 : Init : w 0 = w − 1 ∈ R d 3 : for t = 0 to T do 4 : Update iterate w t + 1 = w t − η ∇ f ( w t ) + β ( w t − w t − 1 ) . 5 : end for Algorithm 2 : Heavy Ball method ( Polyak , 1964 ) ( Equivalent version 2 ) 1 : Required : step size η and momentum parameter β ∈ [ 0 , 1 ] . 2 : Init : w 0 ∈ R d and m − 1 = 0 . 3 : for t = 0 to T do 4 : Update momentum m t : = βm t − 1 + ∇ f ( w t ) . 5 : Update iterate w t + 1 : = w t − ηm t . 6 : end for 2 M ORE RELATED WORKS Heavy Ball ( HB ) : HB has two exactly equivalent presentations in the literature ( see Algorithm 1 and 2 ) . Given the same initialization , both algorithms generate the same sequence of { w t } . In Algorithm 2 , we note that the momentum m t can be written as m t = (cid:80) ts = 0 β t − s ∇ f ( w s ) and can be viewed as a weighted sum of gradients . As we described in the opening paragraph , there is little theory of showing a provable acceleration of the method in non - convex optimization . The only exception that we are aware of is ( Wang et al . , 2020 ) . They show that HB momentum can help to escape saddle points faster and ﬁnd a second - order stationary point faster for smooth non - convex optimization . They also observed that stochastic HB solves ( 1 ) and that using higher values of the momentum parameter β leads to faster convergence . However , while their work focused on the stochastic setting , their main result required some assumptions on the statistical properties of the sequence of observed gradients ; it is not clear whether these would hold in general . In appendix A , we provide a more detailed literature review of HB . To summarize , current results in the literature imply that we are still very far from understanding deterministic HB in non - convex optimization , let alone understanding the success of stochastic HB in deep learning . Hence , this work aims to make progress on a simple question : can we give a precise advantage argument for the acceleration effect of Heavy Ball in the deterministic setting ? Phase retrieval : The optimization landscape of problem ( 1 ) and its variants has been studied by ( Davis et al . , 2018 ; Soltanolkotabi , 2014 ; Sun et al . , 2016 ; White et al . , 2016 ) , which shows that as long as the number of samples is sufﬁciently large , it has no spurious local optima . We note that the problem can also be viewed as a special case of matrix sensing ( e . g . Li et al . ( 2018 ) ; Gunasekar et al . ( 2017 ) ; Li & Lin ( 2020 ) ; Li et al . ( 2019 ) ; Gidel et al . ( 2019 ) ; You et al . ( 2020 ) ) ; in Appendix A , we provide a brief summary of matrix sensing . For solving phase retrieval , Mannellia et al . ( 2020 ) study gradient ﬂow , while Chen et al . ( 2018 ) show that the standard gradient descent with a random initialization like Gaussian initialization solves ( 1 ) and recovers w ∗ up to the sign . Tan & Vershynin ( 2019 ) show that online gradient descent with a simple random initialization can converge to a global optimal point in an online setting where fresh samples are required for each step . In this paper , we show that Heavy Ball converges even faster than the vanilla gradient descent . Zhou et al . ( 2016 ) propose leveraging Nesterov’s momentum to solve phase retrieval . However , their approach requires delicate and computationally expensive initialization like spectral initialization so that the initial point is already within the neighborhood of a minimizer . Similarly , Xiong et al . ( 2018 ; 2020 ) show local convergence of Nesterov’s momentem and Heavy Ball momentum for phase retrieval , but require the initial point to be in the neighborhood of an optimal point . Jin et al . ( 2018 ) propose an algorithm that uses Nesterov’s momentum together as a subroutine with perturbation for ﬁnding a second - order stationary point , which could be applied for solving phase retrieval . Compared to ( Zhou et al . , 2016 ; Jin et al . , 2018 ; Xiong et al . , 2018 ; 2020 ) , we consider directly applying gradient descent with Heavy Ball momentum ( i . e . HB method ) to the objective function with simple random initialization , e . g . Gaussian initialization , which is what people do in practice and is what we want to understand . The goals of the works are different . 3 P HASE R ETRIEVAL 3 . 1 P RELIMINARIES Following the works of Cand´es et al . ( 2013 ) ; Chen et al . ( 2018 ) , we assume that the design vectors { x i } ( which are known a priori ) are from Gaussian distribution x i ∼ N ( 0 , I d ) . Furthermore , without 3 ( a ) Obj . value ( 1 ) vs . t ( b ) | w (cid:107) t | vs . t ( c ) (cid:107) w ⊥ t (cid:107) vs . t Figure 1 : Performance of HB with different β = { 0 , 0 . 3 , 0 . 5 , 0 . 7 , 0 . 9 , 1 . 0 → 0 . 9 } for phase retrieval . Here “ 1 . 0 → 0 . 9 ” stands for using parameter β = 1 . 0 in the ﬁrst few iterations and then switching to using β = 0 . 9 after that ; Algorithm 3 and Algorithm 4 in Appendix H describe the procedures . All the lines are obtained by the same initialization and the same step size ( see Appendix H for more details ) . ( a ) : Objective value ( 1 ) vs . iteration t . We see that the higher the momentum parameter β , the faster the algorithm enters the linear convergence regime . ( b ) : The size of projection of w t on w ∗ over iterations ( i . e . | w (cid:107) t | vs . t ) , which is non - decreasing throughout the iterations until reaching an optimal point ( here , (cid:107) w ∗ (cid:107) = 1 ) . ( c ) : The size of the perpendicular component over iterations ( i . e . (cid:107) w ⊥ t (cid:107) vs . t ) , which is increasing in the beginning and then it is decreasing towards zero after some point . We see that the slope of the curve corresponding to a larger momentum parameter β is steeper than that of a smaller one , which conﬁrms Lemma 1 and Lemma 3 . loss of generality , we assume that w ∗ = e 1 ( so that (cid:107) w ∗ (cid:107) = 1 ) , where e 1 is the standard unit vector whose ﬁrst element is 1 . We also denote w (cid:107) t : = w t [ 1 ] and w t , ⊥ : = [ w t [ 2 ] , . . . , w t [ d ] ] (cid:62) . That is , w (cid:107) t is the projection of the current iterate w t on w ∗ , while w ⊥ t is the perpendicular component . Throughout the paper , the subscript t is an index of the iterations while the subscript i is an index of the samples . Before describing the main results , we would like to provide a preliminary analysis to show how momentum helps . Applying gradient descent with Heavy Ball momentum ( Algorithm 1 ) to ob - jective ( 1 ) , we see that the iterate is generated according to w t + 1 = w t − η 1 n (cid:80) ni = 1 (cid:0) ( x (cid:62) i w t ) 3 − ( x (cid:62) i w t ) y i (cid:1) x i + β ( w t − w t − 1 ) . On the other hand , the population counterpart ( i . e . when the number of samples n is inﬁnite ) of the update rule turns out to be the key to understanding momentum . The population gradient ∇ F ( w ) : = E x ∼ N ( 0 , I n ) [ ∇ f ( w ) ] is ( proof is available in appendix B ) ∇ F ( w ) = (cid:0) 3 (cid:107) w (cid:107) 2 − 1 (cid:1) w − 2 ( w (cid:62)∗ w ) w ∗ . ( 4 ) Using the population gradient ( 4 ) , we have the population update , w t + 1 = w t − η ∇ F ( w t ) + β ( w t − w t − 1 ) , which can be decomposed as follows : w (cid:107) t + 1 = (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) (cid:1) w (cid:107) t + β ( w (cid:107) t − w (cid:107) t − 1 ) w ⊥ t + 1 = (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) (cid:1) w ⊥ t + β ( w ⊥ t − w ⊥ t − 1 ) . ( 5 ) Assume that the random initialization satisﬁes (cid:107) w 0 (cid:107) 2 (cid:28) 13 . From the population recursive system ( 5 ) , both the magnitude of the signal component w (cid:107) t and the perpendicular component w ⊥ t grow exponentially in the ﬁrst few iterations . Lemma 1 . For a positive number θ and the momentum parameter β ∈ [ 0 , 1 ] , if a non - negative sequence { a t } satisﬁes a 0 ≥ a − 1 > 0 and that for all t ≤ T , a t + 1 ≥ ( 1 + θ ) a t + β ( a t − a t − 1 ) , ( 6 ) then the effective dynamics satisﬁes a t + 1 ≥ (cid:16) 1 + (cid:0) 1 + β 1 + θ (cid:1) θ (cid:17) a t , ( 7 ) for every t = 1 , . . . , T + 1 . Similarly , if a non - positive sequence { a t } satisﬁes a 0 ≤ a − 1 < 0 and that for all t ≤ T , a t + 1 ≤ ( 1 + θ ) a t + β ( a t − a t − 1 ) , then the effective dynamics satisﬁes a t + 1 ≤ (cid:16) 1 + (cid:0) 1 + β 1 + θ (cid:1) θ (cid:17) a t , ( 8 ) for every t = 1 , . . . , T + 1 . 4 One can view a t in Lemma 1 as the projection of the current iterate w t onto a vector of interest . The lemma says that with a larger value of the momentum parameter β , the magnitude of a t is increasing faster . It also implies that if the projection due to vanilla gradient descent satisﬁes a t + 1 ≥ ( 1 + θ ) a t , then the magnitude of the projection only grows faster with the use of Heavy Ball momentum . The effective dynamics are the keys to showing that Heavy Ball momentum accelerates the process of entering a benign ( convex ) region . The factor β 1 + θ θ in ( 7 ) and ( 8 ) represents the contribution due to the use of momentum , and the contribution is larger with a larger value of momentum parameter β . Now let us apply Lemma 1 to the recursive system ( 5 ) and pretend that the magnitude of (cid:107) w t (cid:107) was a constant for a moment . Denote θ t : = 3 η ( 1 − (cid:107) w t (cid:107) 2 ) and ˜ θ t : = η ( 1 − 3 (cid:107) w t (cid:107) 2 ) and notice that θ t > ˜ θ t > 0 when (cid:107) w t (cid:107) 2 < 13 . We can rewrite the recursive system as w (cid:107) t + 1 = ( 1 + θ t ) w (cid:107) t + β ( w (cid:107) t − w (cid:107) t − 1 ) w ⊥ t + 1 = ( 1 + ˜ θ t ) w ⊥ t + β ( w ⊥ t − w ⊥ t − 1 ) . ( 9 ) Since the above system is in the form of ( 6 ) , the effective dynamics of ( 7 ) or ( 8 ) in Lemma 1 suggest that the larger the momentum parameter β , the faster the growth rate of the magnitude of the signal component w (cid:107) t and the perpendicular component w ⊥ t . Moreover , the magnitude of the signal component w (cid:107) t grows faster than that of the perpendicular component w ⊥ t . Both components will grow until the size of iterate (cid:107) w t (cid:107) 2 is sufﬁciently large ( i . e . (cid:107) w t (cid:107) 2 > 13 ) . After that , the magnitude of the perpendicular component w ⊥ t starts decaying , while | w (cid:107) t | keeps growing until it approaches 1 . Furthermore , we have that the larger the momentum parameter β , the faster the decay rate of the ( magnitude of the ) perpendicular component w ⊥ t . In other words , | w (cid:107) t | converges to 1 and w ⊥ t converges to 0 quickly . Lemma 3 in Appendix C , which is a counterpart of Lemma 1 , can be used to explain the faster decay of the magnitude due to a larger value of the momentum parameter β . By using the population recursive system ( 5 ) and Lemma 1 & 3 as the tool , we obtain a high - level insight on how momentum helps ( see also Figure 1 ) . The momentum helps to drive the iterate to enter the neighborhood of a w ∗ ( or − w ∗ ) faster . 3 . 2 M AIN RESULTS We denote dist ( w t , w ∗ ) : = min { (cid:107) w t − w ∗ (cid:107) 2 , (cid:107) w t + w ∗ (cid:107) 2 } as the distance between the current iterate w t and w ∗ , modulo the unrecoverable sign . Note that both ± w ∗ achieve zero testing errors . We will divide the iterations into two stages . The ﬁrst stage consists of those iterations that satisfy 0 ≤ t ≤ T ζ , where T ζ is deﬁned as T ζ : = min { t : | | w (cid:107) t | − 1 | ≤ ζ 2 and (cid:107) w ⊥ t (cid:107) ≤ ζ 2 } , ( 10 ) and ζ > 0 is sufﬁciently small so that it makes w T ζ be in the neighborhood of w ∗ or − w ∗ which is smooth , twice differentiable , strongly convex , see e . g . ( Ma et al . , 2017 ; Soltanolkotabi , 2014 ) . Speciﬁcally , there exists a constant ϑ > 0 so that ∇ 2 f ( w ) (cid:23) ϑI d for all w ∈ B ζ ( ± w ∗ ) , where B ζ ( ± w ∗ ) represents the balls centered at ± w ∗ with a radius ζ . Observe that if | | w (cid:107) t | − 1 | ≤ ζ 2 and (cid:107) w ⊥ t (cid:107) ≤ ζ 2 , we have that dist ( w t , w ∗ ) ≤ (cid:107) w t − w ∗ (cid:107) ≤ | | w (cid:107) t | − 1 | + (cid:107) w ⊥ t (cid:107) ≤ ζ . The second stage consists of those iterations that satisfy t ≥ T ζ . Given that dist ( w T ζ , w ∗ ) ≤ ζ , the iterate w t would be in a benign region at the start of this stage , which allows linear convergence to a global optimal point . That is , we have that dist ( w t , w ∗ ) ≤ ( 1 − ν ) t − T ζ ζ for all t ≥ T ζ , where 1 > ν > 0 is some number . Since the behavior of the momentum method in the second stage can be explained by the existing results ( e . g . Section 3 of Saunders ( 2018 ) ) , the goal is to understand why momentum helps to drive the iterate into the benign region faster . To deal with the case that only ﬁnite samples are available in practice , we will consider some per - turbations from the population dynamics ( 5 ) . In particular , we consider w (cid:107) t + 1 = (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) + ηξ t (cid:1) w (cid:107) t + β ( w (cid:107) t − w (cid:107) t − 1 ) w ⊥ t + 1 [ j ] = (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + ηρ t , j (cid:1) w ⊥ t [ j ] + β ( w ⊥ t [ j ] − w ⊥ t − 1 [ j ] ) , ( 11 ) where { ξ t } and { ρ t , j } for 1 ≤ j ≤ d − 1 are the perturbation terms . The perturbation terms are used to model the deviation from the population dynamics . In this paper , we assume that there exists a 5 small number c n > 0 such that for all iterations t ≤ T ζ and all j ∈ [ d − 1 ] , max { | ξ t | , | ρ t , j | } ≤ c n , where the value c n should decay when the number of samples n is increasing and c n (cid:117) 0 when there are sufﬁciently large number of samples n . Theorem 1 . Suppose that the approximated dynamics ( 11 ) holds with max { | ξ t | , | ρ t , j | } ≤ c n for all iterations t ≤ T ζ and all dimensions j ∈ [ d − 1 ] , where c n ≤ ζ . Assume that the initial point w 0 satisﬁes | w (cid:107) 0 | (cid:38) 1 √ d log d and (cid:107) w 0 (cid:107) < 13 . Set the momentum parameter β ∈ [ 0 , 1 ] . Assume that the norm of the momentum (cid:107) m t (cid:107) is bounded for all t ≤ T ζ , i . e . (cid:107) m t (cid:107) ≤ c m for some constant c m > 0 . If the step size η satisﬁes η ≤ 1 36 (cid:0) 1 + ζ (cid:1) max { c m , 1 } , then Heavy Ball ( Algorithm 1 & 2 ) takes at most T ζ (cid:46) log d 1 + c η β number of iterations to enter the benign region B ζ ( w ∗ ) or B ζ ( − w ∗ ) , where c η : = 1 1 + η / 2 ≤ 1 . Furthermore , for all t ≥ T ζ , the distance is shrinking linearly for some values of η and β < 1 . That is , we have that dist ( w t , w ∗ ) ≤ ( 1 − ν ) t − T ζ ζ , for some number 1 > ν > 0 . The theorem states that the number of iterations required for gradient descent to enter the linear convergence is reduced by a factor of ( 1 + c η β ) , which clearly demonstrates that momentum helps to drive the iterate into a benign region faster . The constant c η suggests that the smaller the step size η , the acceleration due to the use of momentum is more evident . The reduction can be about (cid:117) 1 + β for a small η . After T ζ , Heavy Ball has a locally linear convergence to w ∗ or − w ∗ . Speciﬁcally , if w (cid:107) 0 > 0 , then we will have that w (cid:107) t > 0 for all t and that the iterate will converge to w ∗ ; otherwise , we will have w (cid:107) t < 0 for all t and that the iterate will converge to − w ∗ . The proof of Theorem 1 is in Appendix D . Remark 1 : The initial point is required to satisfy (cid:104) w 0 , w ∗ (cid:105) (cid:38) 1 √ d log d , which can be achieved by generating w 0 from a Gaussian distribution or from a unit sphere with high probability ( see e . g . Chapter 2 of ( Blum et al . , 2018 ) ) . We note that the condition that the norm of the momentum is bounded is also assumed in ( Wang et al . , 2020 ) . Remark 2 : Our theorem indicates that in the early stage of the optimization process , the momentum parameter β can be as large as 1 , which is also veriﬁed in the experiment ( Figure 1 ) . However , to guarantee convergence after the iterate is in the neighborhood of a global optimal solution , the parameter β must satisfy β < 1 ( Polyak , 1964 ; Lessard et al . , 2016 ) . Remark 3 : The number ν of the local linear convergence rate due to Heavy Ball momentum actually depends on the smoothness constant L and strongly convexity constant µ in the neighborhood of a global solution , as well as the step size η and the momentum parameter β ( see e . g . Section 3 of ( Saunders , 2018 ) or the original paper ( Polyak , 1964 ) ) . By setting the step size η = 4 / ( √ L + √ µ ) 2 and the momentum parameter β = max { | 1 − √ ηL | , | 1 − √ ηµ | } 2 , ν will depend on the squared root of the condition number √ κ : = (cid:112) L / µ instead of κ : = L / µ , which means that an optimal local convergence rate is achieved ( e . g . ( Bubeck , 2014 ) ) . In general , up to a certain threshold , a larger value of β leads to a faster rate than that of standard gradient descent . 4 C UBIC - R EGULARIZED PROBLEM 4 . 1 N OTATIONS We begin by introducing the notations used in this section . For the symmetric but possibly indeﬁnite matrix A , we denote its eigenvalue in the increasing order , where any λ ( i ) might be negative . We de - note the eigen - decomposition of A as A : = (cid:80) d i = 1 λ ( i ) ( A ) v i v (cid:62) i , where each v i ∈ R d is orthonormal . We also denote γ : = − λ ( 1 ) ( A ) , and γ + = max { γ , 0 } , and (cid:107) A (cid:107) 2 : = max { | λ ( 1 ) ( A ) | , | λ ( d ) ( A ) | } . For any vector w ∈ R d , we denote w ( i ) as the projection on the eigenvector of A , w ( i ) = (cid:104) w , v i (cid:105) . De - note w ∗ as a global minimizer of the cubic - regularized problem ( 2 ) and denote A ∗ : = A + ρ (cid:107) w ∗ (cid:107) I d . Previous works of Nesterov & Polyak ( 2006 ) ; Carmon & Duchi ( 2019 ) show that the minimizer w ∗ has a characterization ; it satisﬁes ρ (cid:107) w ∗ (cid:107) ≥ γ and ∇ f ( w ∗ ) = A ∗ w ∗ + b = 0 . Furthermore , the 6 minimizer w ∗ is unique if ρ (cid:107) w ∗ (cid:107) > γ . In this paper , we assume that the problem has a unique minimizer so that ρ (cid:107) w ∗ (cid:107) > γ . The gradient of the cubic - regularized problem ( 2 ) is ∇ f ( w ) = Aw + b + ρ (cid:107) w (cid:107) w = A ∗ ( w − w ∗ ) − ρ (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w (cid:107) (cid:1) w . ( 12 ) By applying the Heavy Ball algorithm ( Algorithm 1 ) to the cubic - regularized problem ( 2 ) , we see that it generates the iterates via w t + 1 = w t − η ∇ f ( w t ) + β ( w t − w t − 1 ) = ( I d − ηA − ρη (cid:107) w t (cid:107) I d ) w t − ηb + β ( w t − w t − 1 ) . ( 13 ) 4 . 2 E NTERING A BENIGN REGION FASTER For the cubic - regularized problem , we deﬁne a different notion of a benign region from that for the phase retrieval . The benign region here is smooth , contains the unique global optimal point w ∗ , and satisﬁes a notion of one - point strong convexity ( Kleinberg et al . , 2018 ; Li & Yuan , 2017 ; Safran et al . , 2020 ) , w ∈ R d : (cid:104) w − w ∗ , ∇ f ( w ) (cid:105) ≥ ϑ (cid:107) w − w ∗ (cid:107) 2 , where ϑ > 0 . ( 14 ) We note that the standard strong convexity used in the deﬁnition of a benign region for phase retrieval could imply the one - point strong convexity here , but not vice versa ( Hinder et al . , 2020 ) . Previous work of Carmon & Duchi ( 2019 ) shows that if the norm of the iterate is sufﬁciently large , i . e . ρ (cid:107) w (cid:107) ≥ γ − δ for any sufﬁciently small δ > 0 , the iterate is in the benign region that contains the global minimizer w ∗ . To see this , by using the gradient expression of ∇ f ( w ) , we have that (cid:104) w − w ∗ , ∇ f ( w ) (cid:105) = ( w − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w − w ∗ ) + ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w (cid:107) (cid:1) 2 ( (cid:107) w (cid:107) + (cid:107) w ∗ (cid:107) ) . ( 15 ) The ﬁrst term on the r . h . s . of the equality becomes nonnegative if the matrix A ∗ + ρ 2 ( (cid:107) w (cid:107)−(cid:107) w ∗ (cid:107) ) I d becomes PSD . Since A ∗ (cid:23) ( − γ + ρ (cid:107) w ∗ (cid:107) ) I d , it means that if ρ (cid:107) w (cid:107) ≥ γ − (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , the matrix becomes PSD ( note that by the characterization , ρ (cid:107) w ∗ (cid:107) − γ > 0 ) . Furthermore , if the size of iterate ρ (cid:107) w (cid:107) satisﬁes ρ (cid:107) w (cid:107) > γ − (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , the matrix becomes positive deﬁnite and consequently , we have that A ∗ + ρ 2 ( (cid:107) w (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:31) ϑI d for a number ϑ > 0 . Therefore , ( 15 ) becomes (cid:104) w − w ∗ , ∇ f ( w ) (cid:105) ≥ ϑ (cid:107) w − w ∗ (cid:107) 2 , ϑ > 0 . ( 16 ) Therefore , the benign region of the cubic regularized problem can be characterized as B : = { w ∈ R d : ρ (cid:107) w (cid:107) > γ − (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) } . ( 17 ) What we are going to show is that HB with a larger value of the momentum parameter β enters the benign region B faster . We have the following theorem , which shows that the size of the iterate ρ (cid:107) w t (cid:107) will grow very fast to exceed any level below γ . Furthermore , the larger the momentum parameter β , the faster the growth , which shows the advantage of Heavy Ball over vanilla gradient descent . Theorem 2 . Fix any number δ that satisﬁes δ > 0 . Deﬁne T δ : = min { t : ρ (cid:107) w t + 1 (cid:107) ≥ γ − δ } . Suppose that the initialization satisﬁes w ( 1 ) 0 b ( 1 ) ≤ 0 . Set the momentum parameter β ∈ [ 0 , 1 ] . If the step size η satisﬁes η ≤ 1 (cid:107) A (cid:107) 2 + ρ (cid:107) w ∗ (cid:107) , then Heavy Ball ( Algorithm 1 & 2 ) takes at most T δ ≤ 2 ηδ ( 1 + β / ( 1 + ηδ ) ) log (cid:0) 1 + γ 2 + ( 1 + β / ( 1 + ηδ ) ) 4 ρ | b ( 1 ) | (cid:1) number of iterations required to enter the benign region B . Note that the case of β = 0 in Theorem 2 reduces to the result of Carmon & Duchi ( 2019 ) which analyzes vanilla gradient descent . The lemma implies that the higher the momentum parameter β , the faster that the iterate enters the benign region for which a linear convergence is possible ; see also Figure 2 for the empirical results . Speciﬁcally , β reduces the number of iterations T δ by a factor of ( 1 + β / ( 1 + ηδ ) ) ( ignoring the β in the log factor as its effect is small ) , which also implies that for a smaller step size η , the acceleration effect due to the use of momentum is more evident . The factor can be approximately 1 + β for a small η . Lastly , the condition w ( 1 ) 0 b ( 1 ) ≤ 0 in Theorem 2 can be satisﬁed by w 0 = w − 1 = − r b (cid:107) b (cid:107) for any r > 0 . 7 ( a ) norm (cid:107) w t (cid:107) vs . iteration ( b ) f ( w t ) − f ( w ∗ ) vs . iteration Figure 2 : Solving ( 2 ) with different values of momentum parameter β . The empirical result shows the clear advantage of Heavy Ball momentum . More discussions and the experiment setup are available in Appendix H . Proof . ( sketch ; detailed proof is available in Appendix E ) The theorem holds trivially when γ ≤ 0 , so let us assume γ > 0 . Recall the notation that w ( 1 ) represents the projection of w on the eigenvector v 1 of the least eigenvalue λ ( 1 ) ( A ) , i . e . w ( 1 ) = (cid:104) w , v 1 (cid:105) . From the update , we have that w ( 1 ) t + 1 − ηb ( 1 ) = ( I d + ηγ − ρη (cid:107) w t (cid:107) ) w ( 1 ) t − ηb ( 1 ) + 1 + β ( w ( 1 ) t − w ( 1 ) t − 1 ) − ηb ( 1 ) . ( 18 ) Denote a t : = w ( 1 ) t − ηb ( 1 ) and in the detailed proof we will show that a t ≥ 0 for all t ≤ T δ . We can rewrite ( 18 ) as a t + 1 = ( 1 + ηγ − ρη (cid:107) w t (cid:107) ) a t + 1 + β ( a t − a t − 1 ) ≥ ( 1 + ηδ ) a t + 1 + β ( a t − a t − 1 ) , where the inequality is due to that ρ (cid:107) w t (cid:107) ≤ γ − δ for t ≤ T δ . So we can now see that the dynamics is essentially in the form of ( 6 ) except that there is an additional 1 on the r . h . s of the inequality . Therefore , we can invoke Lemma 1 to show that the higher the momentum , the faster the iterate enters the benign region . In Appendix E , we consider the presence of 1 on the r . h . s and obtain a tighter bound than what Lemma 1 can provide . We have shown that the simple dynamic results in entering a benign region that is one - point strongly convex to w ∗ faster . However , different from the case of phase retrieval , we are not aware of any prior results of showing that the iterate generated by Heavy Ball keeps staying in a region that has the property ( 16 ) once the iterate is in the region . Carmon & Duchi ( 2019 ) show that for the cubic regularized problem , the iterate generated by vanilla gradient descent stays in the region under certain conditions , which leads to a linear convergence rate after it enters the benign region . Showing that the property holds for HB is not in the scope of this paper , but we empirically observe that Heavy Ball stays in the region . Subﬁgure ( a ) on Figure 2 shows that the norm (cid:107) w t (cid:107) is monotone increasing for a wide range of β , which means that the iterate stays in the benign region according to ( 17 ) . Assuming that the iterate stays in the region , in Appendix F , we show a locally linear convergence of HB for which up to a certain threshold of β , the larger the β the better the convergence rate . 5 D ISCUSSION AND CONCLUSION Let us conclude by a discussion about the applicability of the simple dynamics to other non - convex optimization problems . Let A be a positive semi - deﬁnite matrix . Consider applying HB to top eigenvector computations , i . e . solving min w ∈ R d : (cid:107) w (cid:107)≤ 1 − 12 w (cid:62) Aw , which is a non - convex optimiza - tion problem as it is about maximizing a convex function . The update of HB for this objective is w t + 1 = ( I d + ηA ) w t + β ( w t − w t − 1 ) . By projecting the iterate w t + 1 on an eigenvector u i of the matrix A , we have that (cid:104) w t + 1 , u i (cid:105) = ( 1 + ηλ i ) (cid:104) w t , u i (cid:105) + β ( (cid:104) w t , u i (cid:105) − (cid:104) w t − 1 , u i (cid:105) ) . ( 19 ) We see that this is in the form of the simple dynamics in Lemma 1 again . So one might be able to show that the larger the momentum parameter β , the faster the top eigenvector computation . This connection might be used to show that the dynamics of HB momentum implicitly helps fast saddle points escape . In Appendix G , we provide further discussion and show some empirical evidence . We conjecture that if a non - convex optimization problem has an underlying structure like the ones in this paper , then HB might be able to exploit the structure and hence makes progress faster than vanilla gradient descent . 8 R EFERENCES Naman Agarwal , Zeyuan Allen - Zhu , Brian Bullins , Elad Hazan , and Tengyu Ma . Finding approxi - mate local minima faster than gradient descent . STOC , 2017 . Zeyuan Allen - Zhu and Yuanzhi Li . Neon2 : Finding local minima via ﬁrst - order oracles . NeurIPS , 2018 . Afonso S . Bandeira , Jameson Cahill , Dustin G . Mixon , and Aaron A . Nelson . Saving phase : Injec - tivity and stability for phase retrieval . Applied and Computational Harmonic Analysis , 2014 . Avrim Blum , John Hopcroft , and Ravindran Kannan . Foundations of data science . Neural compu - tation , 2018 . S´ebastien Bubeck . Convex optimization : Algorithms and complexity . Foundations and Trends in Machine Learning , 2014 . T . Tony Cai , Xiaodong Li , and Zongming Ma . Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger ﬂow . The Annals of Statistics , 2016 . Bugra Can , Mert G ¨ urb ¨ uzbalaban , and Lingjiong Zhu . Accelerated linear convergence of stochastic momentum methods in wasserstein distances . ICML , 2019 . Emmanuel J . Cand ´ es and Xiaodong Li . Solving quadratic equations via phaselift when there are about as many equations as unknowns . Foundations of Computational Mathematics , 2014 . Emmanuel J . Cand ´ es , Thomas Strohmer , and Vladislav Voroninski . Phaselift : Exact and stable signal recovery from magnitude measurements via convex programming . Communications on Pure and Applied Mathematics , 2013 . Emmanuel J . Cand ´ es , Xiaodong Li , and Mahdi Soltanolkotabi . Phase retrieval via wirtinger ﬂow : Theory and algorithms . IEEE Transactions on Information Theory , 2015 . Yair Carmon and John Duchi . Gradient descent ﬁnds the cubic - regularized nonconvex newton step . SIAM Journal on Optimization , 2019 . Yair Carmon , John Duchi , Oliver Hinder , and Aaron Sidford . Accelerated methods for nonconvex optimization . SIAM Journal of Optimization , 2018 . You - Lin Chen and Mladen Kolar . Understanding accelerated stochastic gradient descent via the growth condition . arXiv : 2006 . 06782 , 2020 . Yuxin Chen and Emmanuel J . Cand ´ es . Solving random quadratic systems of equations is nearly as easy as solving linear systems . Communications on Pure and Applied Mathematics , 2017 . Yuxin Chen , Yuejie Chi , Jianqing Fan , Cong Ma , and Yuling Yan . Gradient descent with random ini - tialization : Fast global convergence for nonconvex phase retrieval . Mathematical Programming , 2018 . Hadi Daneshmand , Jonas Kohler , Aurelien Lucchi , and Thomas Hofmann . Escaping saddles with stochastic gradients . ICML , 2018 . Damek Davis , Dmitriy Drusvyatskiy , and Courtney Paquette . The nonsmooth landscape of phase retrieval . IMA Journal on Numerical Analysis , 2018 . Jelena Diakonikolas and Michael I . Jordan . Generalized momentum - based methods : A hamiltonian perspective . arXiv : 1906 . 00436 , 2019 . Simon S . Du , Chi Jin , Jason D . Lee , Michael I . Jordan , Barnabas Poczos , and Aarti Singh . Gradient descent can take exponential time to escape saddle points . NIPS , 2017 . John Duchi and Feng Ruan . Solving ( most ) of a set of quadratic equalities : Composite optimization for robust phase retrieval . Information and Inference , 2018 . Cong Fang , Zhouchen Lin , and Tong Zhang . Sharp analysis for nonconvex sgd escaping from saddle points . COLT , 2019 . 9 Albert Fannjiang and Thomas Strohmer . The numerics of phase retrieval . Acta Numerica , 2020 . Nicolas Flammarion and Francis Bach . From averaging to acceleration , there is only a step - size . COLT , 2015 . S´ebastien Gadat , Fabien Panloup , and Soﬁane Saadane . Stochastic heavy ball . arXiv : 1609 . 04228 , 2016 . Rong Ge , Furong Huang , Chi Jin , and Yang Yuan . Escaping from saddle points — online stochastic gradient for tensor decomposition . COLT , 2015 . Euhanna Ghadimi , Hamid Reza Feyzmahdavian , and Mikael Johansson . Global convergence of the heavy - ball method for convex optimization . ECC , 2015 . Gauthier Gidel , Francis Bach , and Simon Lacoste - Julien . Implicit regularization of discrete gradient dynamics in linear neural networks . NeurIPS , 2019 . Igor Gitman , Hunter Lang , Pengchuan Zhang , and Lin Xiao . Understanding the role of momentum in stochastic gradient methods . NeurIPS , 2019 . Gabriel Goh . Why momentum really works . Distill , 2017 . Gene H . Golub and Charles F . Van Loan . Matrix computations . Johns Hopkins University Press , 1996 . Suriya Gunasekar , Blake Woodworth , Srinadh Bhojanapalli , Behnam Neyshabur , and Nathan Sre - bro . Implicit regularization in matrix factorization . NIPS , 2017 . Oliver Hinder , Aaron Sidford , and Nimit Sohoni . Near - optimal methods for minimizing star - convex functions and beyond . COLT , 2020 . Elad Hoffer , Itay Hubara , and Daniel Soudry . Train longer , generalize better : closing the general - ization gap in large batch training of neural networks . NIPS , 2017 . Chi Jin , Rong Ge , Praneeth Netrapalli , Sham M . Kakade , and Michael I . Jordan . How to escape saddle points efﬁciently . ICML , 2017 . Chi Jin , Praneeth Netrapalli , and Michael I . Jordan . Accelerated gradient descent escapes saddle points faster than gradient descent . COLT , 2018 . Chi Jin , Praneeth Netrapalli , Rong Ge , Sham M . Kakade , and Michael I . Jordan . Stochastic gradient descent escapes saddle points efﬁciently . arXiv : 1902 . 04811 , 2019 . Rahul Kidambi , Praneeth Netrapalli , Prateek Jain , and Sham M . Kakade . On the insufﬁciency of existing momentum schemes for stochastic optimization . ICLR , 2018 . Robert Kleinberg , Yuanzhi Li , and Yang Yuan . An alternative view : When does sgd escape local minima ? ICML , 2018 . Walid Krichene , Kenneth F . Caluyay , and Abhishek Halder . Global convergence of second - order dynamics in two - layer neural networks . arXiv : 2006 . 07867 , 2020 . Jason D . Lee , Ioannis Panageas , Georgios Piliouras , Max Simchowitz , Michael I . Jordan , and Ben - jamin Recht . First - order methods almost always avoid strict saddle - points . Mathematical Pro - gramming , Series B , 2019 . Laurent Lessard , Benjamin Recht , and Andrew Packard . Analysis and design of optimization algo - rithms via integral quadratic constraints . SIAM Journal on Optimization , 2016 . Kﬁr Y . Levy . The power of normalization : Faster evasion of saddle points . arXiv : 1611 . 04831 , 2016 . Huan Li and Zhouchen Lin . Provable accelerated gradient method for nonconvex low rank opti - mization . Machine Learning , 2020 . Yuanxin Li , Cong Ma , Yuxin Chen , and Yuejie Chi . Nonconvex matrix factorization from rank - one measurements . AISTATS , 2019 . 10 Yuanzhi Li and Yang Yuan . Convergence analysis of two - layer neural networks with relu activation . NeurIPS , 2017 . Yuanzhi Li , Tengyu Ma , and Hongyang Zhang . Algorithmic regularization in over - parameterized matrix sensing and neural networks with quadratic activations . COLT , 2018 . Yanli Liu , Yuan Gao , and Wotao Yin . An improved analysis of stochastic gradient descent with momentum . arXiv : 2007 . 07989 , 2020 . Nicolas Loizou and Peter Richt´arik . Momentum and stochastic momentum for stochastic gradient , newton , proximal point and subspace descent methods . arXiv : 1712 . 09677 , 2017 . Nicolas Loizou and Peter Richt´arik . Accelerated gossip via stochastic heavy ball method . Allerton , 2018 . Ilya Loshchilov and Frank Hutter . Decoupled weight decay regularization . ICLR , 2019 . Cong Ma , Kaizheng Wang , Yuejie Chi , and Yuxin Chen . Implicit regularization in nonconvex statis - tical estimation : Gradient descent converges linearly for phase retrieval , matrix completion , and blind deconvolution . Foundations of Computational Mathematics , 2017 . Junjie Ma , Ji Xu , and Arian Maleki . Optimization - based amp for phase retrieval : The impact of initialization and l2 - regularization . IEEE Transactions on Information Theory , 2018 . Chris J . Maddison , Daniel Paulin , Yee Whye Teh , Brendan O’Donoghue , and Arnaud Doucet . Hamiltonian descent methods . arXiv : 1809 . 05042 , 2018 . Stefano Sarao Mannellia , Giulio Birolib , Chiara Cammarotac , Florent Krzakalab , Pierfrancesco Ur - bania , and Lenka Zdeborov ´ a . Complex dynamics in simple neural networks : Understanding gradient ﬂow in phase retrieval . arXiv : 2006 . 06997 , 2020 . Yurii Nesterov and B . T . Polyak . Cubic regularization of newton method and its global performance . Math . Program . , Ser . A 108 , 177 – 205 , 2006 . Praneeth Netrapalli , Prateek Jain , and Sujay Sanghavi . Phase retrieval using alternating minimiza - tion . NIPS , 2013 . Barak A Pearlmutter . Fast exact multiplication by the hessian . Neural computation , 1994 . B . T . Polyak . Some methods of speeding up the convergence of iteration methods . USSR Computa - tional Mathematics and Mathematical Physics , 1964 . Qing Qu , Yuqian Zhang , Yonina C . Eldar , and John Wright . Convolutional phase retrieval via gradient descent . NIPS , 2017 . Sashank Reddi , Manzil Zaheer , Suvrit Sra , Barnabas Poczos , Francis Bach , Ruslan Salakhutdinov , and Alex Smola . A generic approach for escaping saddle points . AISTATS , 2018 . Itay Safran , Gilad Yehudai , and Ohad Shamir . The effects of mild over - parameterization on the optimization landscape of shallow relu neural networks . arXiv : 2006 . 01005 , 2020 . Michael Saunders . Notes on ﬁrst - order methods for minimizing smooth functions . Lecture note , 2018 . Damien Scieur and Fabian Pedregosa . Universal average - case optimality of polyak momentum . ICML , 2020 . Othmane Sebbouh , Robert M . Gower , and Aaron Defazio . On the convergence of the stochastic heavy ball method . arXiv : 2006 . 07867 , 2020 . Yoav Shechtman , Yonina C . Eldar , Oren Cohen , Henry Nicholas Chapman , Jianwei Miao , and Mordechai Segev . Phase retrieval with application to optical imaging : a contemporary overview . IEEE signal processing magazine , 2015 . 11 Mahdi Soltanolkotabi . Algorithms and theory for clustering and nonconvex quadratic programming . Stanford University Ph . D . Dissertation , 2014 . Matthew Staib , Sashank J . Reddi , Satyen Kale , Sanjiv Kumar , and Suvrit Sra . Escaping saddle points with adaptive gradient methods . ICML , 2019 . Ju Sun , Qing Qu , and John Wright . A geometrical analysis of phase retrieval . International Sympo - sium on Information Theory , 2016 . Tao Sun , Penghang Yin , Dongsheng Li , Chun Huang , Lei Guan , and Hao Jiang . Non - ergodic convergence analysis of heavy - ball algorithms . AAAI , 2019 . Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initializa - tion and momentum in deep learning . ICML , 2013 . Yan Shuo Tan and Roman Vershynin . Online stochastic gradient descent with arbitrary initialization solves non - smooth , non - convex phase retrieval . arXiv : 1910 . 12837 , 2019 . Stephen Tu , Ross Boczar , Max Simchowitz , Mahdi Soltanolkotabi , and Benjamin Recht . Low - rank solutions of linear matrix equations via procrustes ﬂow . ICML , 2016 . Gang Wang , Georgios B . Giannakis , and Yonina C . Eldar . Solving systems of random quadratic equations via truncated amplitude ﬂow . IEEE Transactions on Information Theory , 2017a . Gang Wang , Georgios B . Giannakis , Yousef Saad , and Yonina C . Eldar . Solving most systems of random quadratic equations . NIPS , 2017b . Jun - Kun Wang , Chi - Heng Lin , and Jacob Abernethy . Escaping saddle points faster with stochastic momentum . ICLR , 2020 . Chris D . White , Sujay Sanghavi , and Rachel Ward . The local convexity of solving systems of quadratic equations . Results in Mathematics , 2016 . Ashia C Wilson , Rebecca Roelofs , Mitchell Stern , Nathan Srebro , , and Benjamin Recht . The marginal value of adaptive gradient methods in machine learning . NIPS , 2017 . Huaqing Xiong , Yuejie Chi , Bin Hu , and Wei Zhang . Convergence analysis of accelerated ﬁrst - order methods for phase retrieval . MTNS , 2018 . Huaqing Xiong , Yuejie Chi , Bin Hu , and Wei Zhang . Analytical convergence regions of accelerated gradient descent in nonconvex optimization under regularity condition . Automatica , 2020 . Yi Xu , Jing Rong , and Tianbao Yang . First - order stochastic algorithms for escaping from saddle points in almost linear time . NeurIPS , 2018 . Tianbao Yang , Qihang Lin , and Zhe Li . Uniﬁed convergence analysis of stochastic momentum methods for convex and non - convex optimization . IJCAI , 2018a . Zhuoran Yang , Lin Yang , Ethan Fang , Tuo Zhao , Zhaoran Wang , and Matey Neykov . Misspeciﬁed nonconvex statitical optimization for sparse phase retrival . Mathematical Programming , 2018b . Chong You , Zhihui Zhu , Qing Qu , and Yi Ma . Robust recovery via implicit bias of discrepant learning rates for double over - parameterization . arXiv : 2006 . 08857 , 2020 . Huishuai Zhang , Yuejie Chi , and Yingbin Liang . Provable non - convex phase retrieval with outliers : Median truncated wirtinger ﬂow . ICML , 2017a . Huishuai Zhang , Yi Zhou , Yingbin Liang , and Yuejie Chi . A nonconvex approach for phase retrieval : Reshaped wirtinger ﬂow and incremental algorithms . JMLR , 2017b . Qinqing Zheng and John Lafferty . A convergent gradient descent algorithm for rank minimization and semideﬁnite programming from random linear measurements . NIPS , 2015 . Yi Zhou , Huishuai Zhang , and Yingbin Liang . Geometrical properties and accelerated gradient solvers of non - convex phase retrieval . IEEE Allerton Conference on Communication , Control , and Computing , 2016 . 12 A R ELATED WORKS A . 1 H EAVY B ALL We ﬁrst note that Algorithm 1 and Algorithm 2 generate the same sequence of the iterates { w t } given the same initialization w 0 , the same step size η , and the same momentum parameter β . Lessard et al . ( 2016 ) analyze the Heavy Ball algorithm for strongly convex quadratic functions by using tools from dynamical systems and prove its accelerated linear rate . Ghadimi et al . ( 2015 ) also show an O ( 1 / T ) ergodic convergence rate for general smooth convex problems , while Sun et al . ( 2019 ) show the last iterate convergence on some classes of convex problems . Nevertheless , the convergence rate of both results are not better than gradient descent . Maddison et al . ( 2018 ) and Diakonikolas & Jordan ( 2019 ) study a class of momentum methods which includes Heavy Ball by a continuous time analysis . Can et al . ( 2019 ) prove an accelerated linear convergence to a stationary distribution for strongly convex quadratic functions under Wasserstein distance . Gitman et al . ( 2019 ) analyze the stationary distribution of the iterate of a class of momentum methods that includes SGD with momentum for a quadratic function with noise , as well as studying the condition of its asymptotic convergence . Loizou & Richt´arik ( 2017 ) show linear convergence results of the Heavy Ball method for a broad class of least - squares problems . Loizou & Richt ´ arik ( 2018 ) study solving a average consensus problem by HB , which could be viewed as a strongly convex quadratic function . Sebbouh et al . ( 2020 ) show a convergence result of stochastic HB under a smooth convex setting and show that it can outperform SGD under the assumption that the data is interpolated . Chen & Kolar ( 2020 ) study stochastic HB under a growth condition . Yang et al . ( 2018a ) show an O ( 1 / √ T ) rate of convergence in expected gradient norm for smooth non - convex problems , but the rate is not better than SGD . Liu et al . ( 2020 ) provide an improved analysis of SGD with momentum . They show that SGD with momentum can converge as fast as SGD for smooth nonconvex settings in terms of the expected gradient norm . Krichene et al . ( 2020 ) show that in the continuous time regime , i . e . inﬁnitesimal step size is used , stochastic HB converges to a stationary solution asymptotically for training a one - hidden - layer network with inﬁnite number of neurons , but the result does not show a clear advantage compared to standard SGD . Lastly , Wang et al . ( 2020 ) show that the Heavy Ball’s momentum can help to escape saddle points faster and ﬁnd a second order stationary point faster for smooth non - convex optimization . However , while their work focused on the stochastic setting , their main result required two assumptions on the statistical properties of the sequence of observed gradients ; it is not clear whether these would hold in general . Speciﬁcally , they make an assumption called APAG ( A lmost P ositively A ligned with G radient ) , i . e . E t [ (cid:104)∇ f ( w t ) , m t − g t (cid:105) ] ≥ − 12 (cid:107)∇ f ( w t ) (cid:107) 2 , where ∇ f ( w t ) is the deterministic gradient , g t is the stochastic gradient , and m t is the stochastic momentum . They also make an assumption called APCG ( A lmost P ositively C orrelated with G radient ) , i . e . E t [ (cid:104)∇ f ( w t ) , M t m t (cid:105) ] ≥ − c (cid:48) ησ max ( M t ) (cid:107)∇ f ( w t ) (cid:107) 2 , where M t is a PSD matrix that is related to a local optimization landscape . We also note that there are negative results regarding Heavy Ball ( see e . g . Lessard et al . ( 2016 ) ; Ghadimi et al . ( 2015 ) ; Kidambi et al . ( 2018 ) ) . A . 2 M ATRIX SENSING Problem ( 1 ) can also be viewed as a special case of matrix factorization or matrix sensing . To see this , one can rewrite ( 1 ) as min U ∈ R d × 1 14 n (cid:80) ni = 1 (cid:0) y i −(cid:104) A i , UU (cid:62) (cid:105) (cid:1) 2 , where A i = x i x (cid:62) i ∈ R d × d and the dot product represents the matrix trace . Li et al . ( 2018 ) show that when the matrices { A i } satisfy restricted isometry property ( RIP ) and U ∈ R d × d , gradient descent can converge to a global solution with a close - to - zero random initialization . Yet , if the matrix A i is in the form of a rank - one matrix product , the matrix might not satisfy RIP and a modiﬁcation of the algorithm might be required ( Li et al . ( 2018 ) ) . Li et al . ( 2019 ) , a different group of authors , show that with a carefully - designed initialization ( e . g . spectral initialization ) , gradient descent will be in a benign region in the beginning and will converge to a global optimal point . In our work , we do not assume that x i x (cid:62) i satisﬁes RIP neither do we assume a carefully - designed initialization like spectral initialization is available . Li & Lin ( 2020 ) show a local convergence to an optimal solution by Nesterov’s momentum for a matrix factorization problem ; the initial point needs to be in the neighborhood of an optimal solution . In contrast , we study Polyak’s momentum and are able to establish global convergence to an optimal solution with a simple random initialization . Gunasekar et al . ( 2017 ) ; Gidel et al . ( 2019 ) ; You et al . 13 ( 2020 ) study implicit regularization of gradient descent for the matrix sensing / matrix factorization problem . The directions are different from ours . B P OPULATION GRADIENT Lemma 2 . Assume that (cid:107) w ∗ (cid:107) = 1 . E x ∼ N ( 0 , I d ) (cid:2) ( x (cid:62) w ) 3 x − ( x (cid:62) w ∗ ) 2 ( x (cid:62) w ) x (cid:3) = (cid:0) 3 (cid:107) w (cid:107) 2 − 1 (cid:1) w − 2 ( w (cid:62)∗ w ) w ∗ . Proof . In the following , denote Q : = E [ ( x (cid:62) w ) 3 x ] R : = E [ ( x (cid:62) w ∗ ) 2 ( x (cid:62) w ) x ] . Now deﬁne h ( w ) : = E [ ( x (cid:62) w ) 4 ] . We have that h ( w ) = 3 (cid:107) w (cid:107) 4 as x (cid:62) w ∼ N ( 0 , (cid:107) w (cid:107) 2 ) ( i . e . the fourth order moment ) . Then , ∇ h ( w ) = 4 E [ ( x (cid:62) w ) 3 x ] = 4 Q . So Q = 14 ∇ h ( w ) = 3 (cid:107) w (cid:107) 2 w . For the other term , deﬁne g ( w ) : = E [ ( x (cid:62) w ∗ ) 2 ( x (cid:62) w ) 2 ] . Given that (cid:20) x (cid:62) w ∗ x (cid:62) w (cid:21) ∼ N (cid:0) (cid:20) 00 (cid:21) , (cid:20) (cid:107) w ∗ (cid:107) 2 w (cid:62)∗ w w (cid:62)∗ w (cid:107) w (cid:107) 2 (cid:21) (cid:1) , ( 20 ) we can write x (cid:62) w ∗ d ∼ θ 1 x (cid:62) w + θ 2 z , where z ∼ N ( 0 , 1 ) , θ 1 : = w (cid:62)∗ w (cid:107) w (cid:107) 2 , and θ 22 : = (cid:107) w ∗ (cid:107) 2 − ( w (cid:62)∗ w ) 2 (cid:107) w (cid:107) 2 . Then , g ( w ) = θ 21 E [ ( x (cid:62) w ) 4 ] + 2 θ 1 θ 2 E [ ( x (cid:62) w ) 3 z ] + θ 22 E [ z 2 ( x (cid:62) w ) 2 ] = 3 ( w (cid:62)∗ w ) 2 + θ 22 E [ z 2 ] E [ ( x (cid:62) w ) 2 ] = 2 ( w (cid:62)∗ w ) 2 + (cid:107) w (cid:107) 2 (cid:107) w ∗ (cid:107) 2 . So we have that ∇ g ( w ) = 2 E [ ( x (cid:62) w ∗ ) 2 ( x (cid:62) w ) x ] = 2 R , which in turn implies that R = 1 2 ∇ g ( w ) = 1 2 (cid:0) 4 ( w (cid:62)∗ w ) w ∗ + 2 (cid:107) w ∗ (cid:107) w (cid:1) = 2 ( w (cid:62)∗ w ) w ∗ + (cid:107) w ∗ (cid:107) 2 w . Combining the above results , we have that ∇ F ( w ) = Q − R = 3 (cid:107) w (cid:107) 2 w − (cid:107) w ∗ (cid:107) 2 w − 2 ( w (cid:62)∗ w ) w ∗ = (cid:0) 3 (cid:107) w (cid:107) 2 − 1 (cid:1) w − 2 ( w (cid:62)∗ w ) w ∗ . ( 21 ) C S IMPLE LEMMAS Lemma 1 : For a positive number θ and the momentum parameter β ∈ [ 0 , 1 ] , if a non - negative sequence { a t } satisﬁes a 0 ≥ a − 1 > 0 and that for all t ≤ T , a t + 1 ≥ ( 1 + θ ) a t + β ( a t − a t − 1 ) , ( 22 ) then the effective dynamics satisﬁes a t + 1 ≥ (cid:16) 1 + (cid:0) 1 + β 1 + θ (cid:1) θ (cid:17) a t , ( 23 ) for every t = 1 , . . . , T + 1 . Similarly , if a non - positive sequence { a t } satisﬁes a 0 ≤ a − 1 < 0 and that for all t ≤ T , a t + 1 ≤ ( 1 + θ ) a t + β ( a t − a t − 1 ) , then the effective dynamics satisﬁes a t + 1 ≤ (cid:16) 1 + (cid:0) 1 + β 1 + θ (cid:1) θ (cid:17) a t , ( 24 ) for every t = 1 , . . . , T + 1 . 14 Proof . Let us ﬁrst prove the ﬁrst part of the statement . In the following , we denote c : = 11 + θ . For the base case t = 1 , we have that a 2 ≥ ( 1 + θ ) a 1 + β ( a 1 − a 0 ) ≥ ( 1 + θ ) a 1 + βcθa 1 , ( 25 ) where the last inequality holds because a 1 − a 0 ≥ θca 1 ⇐⇒ a 1 ≥ 1 1 − θc a 0 , as a 1 ≥ ( 1 + θ ) a 0 = 1 1 − θc a 0 . Now suppose that it holds at iteration t , a t + 1 ≥ ( 1 + ( 1 + βc ) θ ) a t . Consider iteration t + 1 , we have that a t + 2 ≥ ( 1 + θ ) a t + 1 + β ( a t + 1 − a t ) ≥ ( 1 + θ ) a t + 1 + θcβa t + 1 , ( 26 ) where the last inequality holds because a t + 1 − a t ≥ θca t + 1 ⇐⇒ a t + 1 ≥ 1 1 − θc a t as a t + 1 ≥ ( 1 + ( 1 + βc ) θ ) a t ≥ 1 1 − θc a t , given the assumption at t and that c : = 11 + θ . The second part of the statement can be proved similarly . Lemma 3 . For a positive number θ < 1 and the momentum parameter β ∈ [ 0 , 1 ] that satisfy ( 1 + β 1 − θ ) θ < 1 , if a non - negative sequence { b t } satisﬁes b 0 ≤ b − 1 and that for all t ≤ T , b t + 1 ≤ ( 1 − θ ) b t + β ( b t − b t − 1 ) , ( 27 ) then the effective dynamics satisﬁes b t + 1 ≤ (cid:16) 1 − (cid:0) 1 + β 1 − θ (cid:1) θ (cid:17) b t , ( 28 ) for every t = 1 , . . . , T + 1 . Similarly , if a non - positive sequence { b t } satisﬁes b 0 ≥ b − 1 and that for all t ≤ T , b t + 1 ≥ ( 1 − θ ) b t + β ( b t − b t − 1 ) , then the effective dynamics satisﬁes b t + 1 ≥ (cid:16) 1 − (cid:0) 1 + β 1 − θ (cid:1) θ (cid:17) b t , ( 29 ) for every t = 1 , . . . , T + 1 . Proof . Let us ﬁrst prove the ﬁrst part of the statement . In the following , we denote c : = 11 − θ . For the base case t = 1 , we have that b 2 ≤ ( 1 − θ ) b 1 + β ( b 1 − b 0 ) ≤ ( 1 − θ ) b 1 − βcθb 1 , ( 30 ) where the last inequality holds because b 1 − b 0 ≤ − θcb 1 ⇐⇒ b 1 ≤ 1 1 + θcb 0 , ( 31 ) as b 1 ≤ ( 1 − θ ) b 0 = 1 1 + θc b 0 due to that c = 11 − θ . Now suppose that it holds at iteration t . b t + 1 ≤ ( 1 − ( 1 + βc ) θ ) b t . Consider iteration t + 1 , we have that b t + 2 ≤ ( 1 − θ ) b t + 1 + β ( b t + 1 − b t ) ≤ ( 1 − θ ) b t + 1 − θcβb t + 1 , ( 32 ) where the last inequality holds because b t + 1 − b t ≤ − θcb t + 1 ⇐⇒ b t + 1 ≤ 1 1 + θcb t ( 33 ) as b t + 1 ≤ ( 1 − ( 1 + βc ) θ ) b t ≤ 1 1 + θc b t , due to the induction at t and that c = 11 − θ . The second part of the statement can be proved similarly . 15 D P ROOF OF T HEOREM 1 Recall the recursive system ( 11 ) . w (cid:107) t + 1 = (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) + ηξ t (cid:1) w (cid:107) t + β ( w (cid:107) t − w (cid:107) t − 1 ) w ⊥ t + 1 [ j ] = (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + ηρ t , j (cid:1) w ⊥ t [ j ] + β ( w ⊥ t [ j ] − w ⊥ t − 1 [ j ] ) , ( 34 ) where { ξ t } and { ρ t , j } are the perturbation terms . In the analysis , we will show that the sign of w (cid:107) t never changes during the execution of the algorithm . Our analysis divides the iterations of the ﬁrst stage to several sub - stages . We assume that the size of the initial point w 0 is small so that it begins in Stage 1 . 1 . • ( Stage 1 . 1 ) considers the duration when (cid:107) w t (cid:107) ≤ (cid:113) 49 + c n , which lasts for at most T 0 iterations , where T 0 is deﬁned in Lemma 4 . A by - product of our analysis shows that | w (cid:107) T 0 + 1 | ≥ (cid:113) 49 + c n . • ( Stage 1 . 2 ) considers the duration when the perpendicular component (cid:107) w ⊥ t (cid:107) is decreasing and eventually falls below ζ / 2 , which consists of all iterations T 0 ≤ t ≤ T 0 + T b , where T b is deﬁned in Lemma 5 . • ( Stage 1 . 3 ) considers the duration when | w (cid:107) t | is converging to the interval [ 1 − ζ 2 , 1 + ζ 2 ] , if it was outside the interval , which consists of all iterations T 0 + T b ≤ t ≤ T 0 + T b + T a , where T a is deﬁned in Lemma 6 . In stage 1 . 1 , both the signal component | w (cid:107) t | and (cid:107) w ⊥ t (cid:107) grow in the beginning ( see also Figure 1 ) . The signal component grows exponentially and consequently it only takes a logarithm number of iterations T 0 + 1 to reach | w (cid:107) T 0 + 1 | > (cid:113) 49 + c n , which also means that (cid:107) w T 0 + 1 (cid:107) > (cid:113) 49 + c n . Moreover , the larger the momentum parameter β , the smaller the number of T 0 . After (cid:107) w t (cid:107) passing the threshold , the iterate enters Stage 1 . 2 . Lemma 4 . ( Stage 1 . 1 ) Denote c a : = 1 1 + η 2 . There will be at most T 0 : = log ( √ 49 + c n / | w (cid:107) 0 | ) log (cid:0) 1 + ( 1 + c a β ) η 53 (cid:1) iterations such that w (cid:107) t ≤ (cid:113) 49 + c n . Furthermore , for all t in this stage , (cid:107) w t (cid:107) ≤ 56 − c n 3 . In stage 1 . 2 , we have that (cid:107) w t (cid:107) 2 ≥ | w (cid:107) t | 2 ≥ 49 + c n > 13 so that the perpendicular component (cid:107) w ⊥ t (cid:107) is decaying while the signal component | w (cid:107) t | keeps growing before reaching 1 ( see also Figure 1 ) . In particular , the perpendicular component decays exponentially so that at most additional T b iterations is needed to fall below ζ 2 ( i . e . (cid:107) w ⊥ t (cid:107) ≤ ζ 2 ) , with larger β leading to a smaller number of T b . Notice that if w (cid:107) t satisﬁes | | w (cid:107) t | − 1 | ≤ ζ 2 when (cid:107) w ⊥ t (cid:107) falls below ζ 2 , we immediately have that T ζ ≤ T 0 + T b ; otherwise , the iterate enters the next stage , Stage 1 . 3 . Lemma 5 . ( Stage 1 . 2 ) Denote c b : = 1 1 − η 3 and ¯ ω : = max t (cid:107) w ⊥ t (cid:107) ≤ (cid:113) 49 + c n . There will be at most T b ≤ log ( ζ 2¯ ω ) log ( 1 − η 3 ( 1 + c b β ) ) iterations such that (cid:113) 49 + c n ≤ | w (cid:107) t | and (cid:107) w ⊥ t (cid:107) ≥ ζ 2 . In stage 1 . 3 , we show that | w (cid:107) t | converges towards 1 linearly , given that (cid:107) w ⊥ t (cid:107) ≤ ζ 2 . Speciﬁcally , after at most additional T a iterations , we have that | | w (cid:107) t | − 1 | ≤ ζ 2 and that larger β reduces the number of iterations T a . Lemma 6 . ( Stage 1 . 3 ) Denote c ζ , g : = 1 1 + η ζ 2 , c ζ , d : = 1 1 − ηζ , and ω : = max t | w (cid:107) t | ≤ (cid:113) 109 + 13 c n . There will be at most T a : = max (cid:0) log ( ( 1 − ζ / 2 ) / √ 49 + c n ) log ( 1 + η ζ 2 ( 1 + c ζ , g β ) ) , log ( 1 + ζ / 2 ω ) log ( 1 − ηζ ( 1 + c ζ , d β ) ) (cid:1) iterations such that | | w (cid:107) t | − 1 | ≥ ζ 2 and (cid:107) w ⊥ t (cid:107) ≤ ζ 2 . 16 By combining the result of Lemma 4 , Lemma 5 and Lemma 6 , we have that T ≤ T 0 + T b + T a = log ( √ 49 + c n | w (cid:107) 0 | ) log (cid:0) 1 + η 53 ( 1 + c a β ) (cid:1) + log ( ζ 2¯ ω ) log ( 1 − η 3 ( 1 + c b β ) ) + max (cid:0) log ( 1 − ζ / 2 √ 49 + c n ) log ( 1 + η ζ 2 ( 1 + c ζ , g β ) ) , log ( 1 + ζ / 2 ω ) log ( 1 − ηζ ( 1 + c ζ , d β ) ) (cid:1) (cid:46) 6 log ( √ 49 + c n | w (cid:107) 0 | ) 5 η ( 1 + c a β ) + 6 log ( 2¯ ωζ ) η ( 1 + c b β ) + max (cid:0) 2 log ( 1 − ζ / 2 √ 49 + c n ) ηζ ( 1 + c ζ , g β ) ) , 2 log ( ω 1 + ζ / 2 ) ηζ ( 1 + c ζ , d β ) ) (cid:1) (cid:46) log d 1 + c η β , ( 35 ) where the last inequality uses that | w (cid:107) 0 | (cid:38) 1 √ d log d due to the random isotropic initialization . The detailed proof of Lemma 4 - 6 is available in the following subsections . After t ≥ T ζ , the iterate enters a benign region that is locally strong convex , smooth , twice dif - ferentiable , and contains w ∗ ( or − w ∗ ) Ma et al . ( 2017 ) ; White et al . ( 2016 ) , which allows us to use the existing result of gradient descent with Heavy Ball momentum for showing its lin - ear convergence . In particular , the result of local landscape ( e . g . Ma et al . ( 2017 ) ; White et al . ( 2016 ) ) and the known convergence result of gradient descent with Heavy Ball momentum ( e . g . Saunders ( 2018 ) ; Polyak ( 1964 ) ; Lessard et al . ( 2016 ) ) can be used to show that for all t > T ζ , dist ( w t , w ∗ ) ≤ ( 1 − ν ) t − T ζ dist ( w T ζ , w ∗ ) ≤ ( 1 − ν ) t − T ζ ζ , for some number 1 > ν > 0 . D . 1 S TAGE 1 . 1 Lemma : 4 ( Stage 1 . 1 ) Denote c a : = 1 1 + η 2 . There will be at most T 0 : = (cid:100) log ( √ 49 + c n / | w (cid:107) 0 | ) log (cid:0) 1 + ( 1 + c a β ) η 53 (cid:1) (cid:101) iterations such that (cid:107) w t (cid:107) ≤ (cid:113) 49 + c n . Furthermore , we have that | w (cid:107) T 0 + 1 | > (cid:113) 49 + c n . Proof . Let us ﬁrst assume that w (cid:107) t > 0 and denote a t : = w (cid:107) t . By using that (cid:107) w t (cid:107) ≤ (cid:113) 49 + c n in this stage , we can lower - bound the growth rate of a t as a t + 1 ≥ (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) − η | ξ t | (cid:1) a t + β ( a t − a t − 1 ) ≥ (cid:0) 1 + 3 η ( 1 − 4 9 + c n ) − c n (cid:1) a t + β ( a t − a t − 1 ) ≥ (cid:0) 1 + η 5 3 (cid:1) a t + β ( a t − a t − 1 ) ≥ (cid:0) 1 + ( 1 + c a β ) η 5 3 (cid:1) a t , ( 36 ) where in the last inequality we use Lemma 1 and that c a = 1 1 + η 53 . ( 37 ) Observe that (cid:0) 1 + ( 1 + c a β ) η 53 (cid:1) > 1 . Consequently , the sign of w (cid:107) t never change in this stage . So for w (cid:107) t ≥ (cid:113) 49 + c n it takes number of iterations at most T 0 : = (cid:100) log ( √ 4 9 + c n | w (cid:107) 0 | ) log (cid:0) 1 + ( 1 + c a β ) η 53 (cid:1) (cid:101) ≤ 2 log ( √ 4 9 + c n | w (cid:107) 0 | ) ( 1 + c a β ) η 53 . ( 38 ) Similar , when w (cid:107) t < 0 , we can show that after at most T 0 iterations , w (cid:107) t falls below − (cid:113) 49 + c n . Since (cid:107) w t (cid:107) > | w (cid:107) t | , it means that there will be at most T 0 iterations such that (cid:107) w t (cid:107) ≤ (cid:113) 49 + c n . 17 D . 2 S TAGE 1 . 2 Lemma 5 : ( Stage 1 . 2 ) Denote c b : = 1 1 − η 3 and ¯ ω : = max t (cid:107) w ⊥ t (cid:107) ≤ (cid:113) 49 + c n . There will be at most T b : = (cid:100) log ( ζ 2¯ ω ) log ( 1 − η 3 ( 1 + c b β ) ) (cid:101) iterations such that (cid:113) 49 + c n ≤ | w (cid:107) t | and (cid:107) w ⊥ t (cid:107) ≥ ζ 2 . Proof . Let t (cid:48) be the last iteration of the previous stage . We have that (cid:107) w t (cid:48) (cid:107) ≥ (cid:107) w (cid:107) t (cid:48) (cid:107) ≥ (cid:113) 49 + c n . Denote a t : = | w (cid:107) t | . In this stage , we have that a t keeps increasing until (cid:107) w t (cid:107) 2 (cid:38) 1 . Moreover , w (cid:107) t remains the same sign as the previous stage . Now ﬁx an element j (cid:54) = 1 and denote b t : = | w ⊥ t [ j ] | . From Lemma 7 , we know that the magnitude b t : = | w ⊥ t [ j ] | is non - increasing in this stage . Furthermore , we can show the decay of b t as follows . If w t [ j ] , w t − 1 [ j ] > 0 , w t + 1 [ j ] ≤ (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + η | ρ t , j | (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≤ (cid:0) 1 + η (cid:0) 1 − 3 ( 4 9 + c n ) (cid:1) + ηc n (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≤ (cid:0) 1 − η 3 (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≤ (cid:0) 1 − η 3 ( 1 + c b β ) (cid:1) w t [ j ] , ( 39 ) where in the last inequality we used Lemma 3 , as the condition ( 1 + β 1 − η 3 ) η 3 < 1 is satisﬁed , and we denote that c b : = 1 1 − η / 3 . ( 40 ) On the other hand , if w t [ j ] , w t − 1 [ j ] < 0 , w t + 1 [ j ] ≥ (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + η | ρ t , j | (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≥ (cid:0) 1 + η (cid:0) 1 − 3 ( 4 9 + c n ) (cid:1) + ηc n (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≥ (cid:0) 1 − η 3 (cid:1) w t [ j ] + β ( w t [ j ] − w t − 1 [ j ] ) ≥ (cid:0) 1 − η 3 ( 1 + c b β ) (cid:1) w t [ j ] , ( 41 ) where in the last inequality we used Lemma 3 , as the condition ( 1 + β 1 − η 3 ) η 3 < 1 is satisﬁed , and we denote that c b : = 1 1 − η / 3 . ( 42 ) The inequalities of ( 39 ) and ( 41 ) allow us to write | w t + 1 [ j ] | ≤ (cid:0) 1 − η 3 ( 1 + c b β ) (cid:1) | w t [ j ] | . Taking the square of both sides and summing all dimension j (cid:54) = 1 , we have that (cid:107) w ⊥ t + 1 (cid:107) 2 ≤ (cid:0) 1 − η 3 ( 1 + c b β ) (cid:1) 2 (cid:107) w ⊥ t (cid:107) 2 . ( 43 ) Consequently , for (cid:107) w ⊥ t (cid:107) to fall below ζ / 2 , it takes at most T b : = (cid:100) log ( ζ / 2 (cid:107) w ⊥ t (cid:48) (cid:107) ) log ( 1 − η 3 ( 1 + c b β ) ) (cid:101) ≤ (cid:100) log ( ζ 2¯ ω ) log ( 1 − η 3 ( 1 + c b β ) ) (cid:101) ( 44 ) iterations . Lastly , Lemma 7 implies that at the time that the magnitude of w ⊥ t [ j ] starts decreasing , (cid:107) w t (cid:107) 2 ≤ 49 + c n , which in turn implies that ¯ ω : = max t (cid:107) w ⊥ t (cid:107) ≤ (cid:113) 49 + c n . 18 D . 3 S TAGE 1 . 3 Lemma 6 : ( Stage 1 . 3 ) Denote c ζ , g : = 1 1 + η ζ 2 , c ζ , d : = 1 1 − ηζ , and ω : = max t | w (cid:107) t | ≤ (cid:113) 109 + 13 c n . There will be at most T a : = max (cid:0) log ( ( 1 − ζ / 2 ) / √ 49 + c n ) log ( 1 + η ζ 2 ( 1 + c ζ , g β ) ) , log ( 1 + ζ / 2 ω ) log ( 1 − ηζ ( 1 + c ζ , d β ) ) (cid:1) iterations such that | | w (cid:107) t | − 1 | ≥ ζ 2 and (cid:107) w ⊥ t (cid:107) ≤ ζ 2 . Proof . Denote t (cid:48) the last iteration of the previous stage . We have that t (cid:48) ≤ T 0 + T b . Since w (cid:107) t does not change the sign in stage 1 . 1 and 1 . 2 , w . l . o . g , we assume that w (cid:107) t > 0 . Denote a t : = w (cid:107) t . We consider a t (cid:48) in two cases : a t (cid:48) ≤ 1 − ζ 2 and a t (cid:48) ≥ 1 + ζ 2 . If a t (cid:48) ≤ 1 − ζ 2 , then we have that for all t in this stage , (cid:107) w t (cid:107) 2 ≤ a 2 t + (cid:107) w ⊥ t (cid:107) 2 ≤ ( 1 − ζ 2 ) 2 + ( ζ 2 ) 2 ≤ 1 − ζ 2 , for any sufﬁciently small ζ . So a t grows as follows . a t + 1 ≥ (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) − η | ξ t | (cid:1) a t + β ( a t − a t − 1 ) ≥ (cid:0) 1 + 3 η ζ 2 − ηc n (cid:1) a t + β ( a t − a t − 1 ) ( a ) ≥ (cid:0) 1 + η ζ 2 (cid:1) a t + β ( a t − a t − 1 ) ( b ) ≥ ( 1 + η ζ 2 ( 1 + c ζ , g β ) ) a t , ( 45 ) where ( a ) is by ζ ≥ c n and ( b ) is due to that Lemma 1 and that c ζ , g = 1 1 + η ζ 2 . ( 46 ) Consequently , it takes at most (cid:100) log 1 − ζ / 2 √ 49 + c n log ( 1 + η ζ 2 ( 1 + c ζ , g β ) ) (cid:101) ( 47 ) number of iterations in this stage for a t to rise above 1 − ζ 2 . On the other hand , if a t (cid:48) ≥ 1 + ζ 2 , then we can lower bound (cid:107) w t (cid:107) 2 in this stage as (cid:107) w t (cid:107) 2 ≥ a 2 t ≥ ( 1 + ζ 2 ) 2 . We have that a t + 1 ≤ (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) + η | ξ t | (cid:1) a t + β ( a t − a t − 1 ) ≤ (cid:0) 1 − 3 ηζ + c n (cid:1) a t + β ( a t − a t − 1 ) ( a ) ≤ (cid:0) 1 − ηζ (cid:1) a t + β ( a t − a t − 1 ) ( b ) ≤ ( 1 − ηζ ( 1 + c ζ , d β ) ) a t , ( 48 ) where ( a ) uses ζ ≥ c n and ( b ) uses Lemma 3 c ζ , d : = 1 1 − ηζ . ( 49 ) That is , a t is decreasing towards 1 + ζ / 2 . Denote ω : = max t | w (cid:107) t | . we see that it takes at most (cid:100) log 1 + ζ / 2 ω log ( 1 − ηζ ( 1 + c ζ , d β ) ) (cid:101) ( 50 ) number of iterations in this stage for a t to fall below 1 + ζ / 2 . Lastly , Lemma 8 implies that at the time that the magnitude of w (cid:107) t starts decreasing , (cid:107) w t (cid:107) 2 ≤ 109 + 13 c n , which in turn implies that ω : = max t | w (cid:107) t | ≤ (cid:113) 10 9 + 1 3 c n . On the other hand , by Lemma 10 , the magnitude of the perpendicular component is non - increasing in this stage , and hence (cid:107) w ⊥ t (cid:107) keeps staying below ζ / 2 . Similar analysis holds for w (cid:107) t < 0 , hence we omitted the details . 19 D . 4 S OME SUPPORTING LEMMAS Lemma 7 . Suppose that η satisﬁes η ≤ 1 36 (cid:0) 13 + c n (cid:1) max { c m , 1 } . Let t 0 be the ﬁrst time such that (cid:107) w t 0 (cid:107) 2 ≥ 13 + c n . Then , there exists a time τ ≤ t 0 such that w ⊥ τ + 1 [ j ] ≤ w ⊥ τ [ j ] , if w ⊥ τ [ j ] ≥ 0 ; Similarly , w ⊥ τ + 1 [ j ] ≥ w ⊥ τ [ j ] , if w ⊥ τ [ j ] ≤ 0 . Furthermore , we have that (cid:107) w t 0 (cid:107) 2 ≤ 49 + c n . Proof . Recall that w ⊥ t + 1 [ j ] = (cid:0) 1 + η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + ηρ t , j (cid:1) w ⊥ t [ j ] + β ( w ⊥ t [ j ] − w ⊥ t − 1 [ j ] ) . W . l . o . g , let us consider w ⊥ 0 [ j ] > 0 . Assume that w ⊥ t [ j ] ≥ w ⊥ t − 1 [ j ] for all t ≤ t 0 ; otherwise , there exists a time τ < t 0 such that w ⊥ τ [ j ] ≤ w ⊥ τ − 1 [ j ] . Denote λ t 0 , j : = η ( 1 − 3 (cid:107) w t 0 (cid:107) 2 ) + ηρ t 0 , j . Since (cid:107) w t 0 (cid:107) 2 ≥ 13 + c n , we have that λ t 0 , j ≤ 0 . We can rewrite the dynamics as (cid:20) w ⊥ t 0 + 1 [ j ] w ⊥ t 0 [ j ] (cid:21) = (cid:20) 1 + λ t 0 , j + β − β 1 0 (cid:21) (cid:20) w ⊥ t 0 [ j ] w ⊥ t 0 − 1 [ j ] (cid:21) ( 51 ) We have that (cid:107) (cid:20) w ⊥ t 0 + 1 [ j ] w ⊥ t 0 [ j ] (cid:21) (cid:107) ≤ (cid:107) (cid:20) 1 + λ t 0 , j + β − β 1 0 (cid:21) (cid:107) 2 · (cid:107) (cid:20) w ⊥ t 0 [ j ] w ⊥ t 0 − 1 [ j ] (cid:21) (cid:107) . ( 52 ) We will show that w ⊥ t 0 + 1 [ j ] ≤ w ⊥ t 0 − 1 [ j ] if w ⊥ t 0 − 1 [ j ] > 0 , which means that the magnitude of w ⊥ [ j ] has stopped increasing . It sufﬁces to show that the spectral norm of the matrix (cid:107) (cid:20) 1 + λ t 0 , j + β − β 1 0 (cid:21) (cid:107) 2 is not greater than 1 . Note that the roots of the characteristic equation of the matrix , z 2 − ( 1 + λ t 0 , j + β ) z + β are (cid:0) 1 + λ t 0 , j + β (cid:1) ± √ ( 1 + λ t 0 , j + β ) 2 − 4 β 2 . If the roots are complex conjugate , then the magnitude of the roots is at most √ β ; consequently the spectral norm is at most √ β ≤ 1 . On the other hand , if the roots are real , to show that the larger root is not larger than 1 , it sufﬁces to show that (cid:112) ( 1 + λ t 0 , j + β ) 2 − 4 β ≤ 1 − λ t 0 , j − β , which is guaranteed if λ t 0 , j ≤ 0 . To show that the smaller root is not greater than − 1 , we need to show that (cid:112) ( 1 + λ t 0 , j + β ) 2 − 4 β ≤ 3 + λ t 0 , j + β , which is guaranteed if λ t 0 , j ≥ − 1 . By deﬁnition , (cid:107) w t 0 − 1 (cid:107) 2 < 13 + c n . If η ≤ min { 1 36 (cid:107) w t 0 − 1 (cid:107) c m , 1 18 c m } , then by invoking Lemma 9 , we have that (cid:107) w t 0 (cid:107) 2 − (cid:107) w t 0 − 1 (cid:107) 2 ≤ 19 and consequently , we have that (cid:107) w t 0 (cid:107) 2 ≤ 49 + c n . Thus , by choosing η satisﬁes η ≤ 1 36 ( 1 / 3 + c n ) c m , we have that (cid:107) w t 0 (cid:107) 2 ≤ 49 + c n . Using the upper - bound of (cid:107) w t 0 (cid:107) 2 and the constraint of η , we have that λ t 0 , j ≥ − η ( 13 + c n ) ≥ − 1 . Similar analysis when w ⊥ t [ j ] is negative ; and hence omitted . Lemma 8 . Suppose that η satisﬁes η ≤ 1 36 (cid:0) 1 + 13 c n (cid:1) max { c m , 1 } . Let t 1 be the ﬁrst time ( if exist ) such that (cid:107) w t 1 (cid:107) 2 ≥ 1 + 13 c n . Then , there exists a time τ ≤ t 1 such that w (cid:107) τ + 1 ≤ w (cid:107) τ , if w (cid:107) τ ≥ 0 . Similarly , w (cid:107) τ + 1 ≥ w (cid:107) τ , if w (cid:107) τ ≤ 0 . Furthermore , we have that (cid:107) w t 1 (cid:107) 2 ≤ 109 + 13 c n . Proof . Recall that w (cid:107) t + 1 = (cid:0) 1 + 3 η ( 1 − (cid:107) w t (cid:107) 2 ) + ηξ t (cid:1) w (cid:107) t + β ( w (cid:107) t − w (cid:107) t − 1 ) . W . l . o . g , let us consider w (cid:107) 0 > 0 . Assume that w (cid:107) t ≥ w (cid:107) t − 1 for all t ≤ t 1 ; otherwise , there exists a time τ < t 1 such that w (cid:107) τ ≤ w (cid:107) τ − 1 . Denote λ t 1 : = 3 η ( 1 − (cid:107) w t 1 (cid:107) 2 ) + ηξ t 1 . Since (cid:107) w t 1 (cid:107) 2 ≥ 1 + 13 c n , we have that λ t 1 ≤ 0 . We can rewrite the dynamics as (cid:34) w (cid:107) t 1 + 1 w (cid:107) t 1 (cid:35) = (cid:20) 1 + λ t 1 + β − β 1 0 (cid:21) (cid:34) w (cid:107) t 1 w (cid:107) t 1 − 1 (cid:35) ( 53 ) 20 We have that (cid:107) (cid:34) w (cid:107) t 1 + 1 w (cid:107) t 1 (cid:35) (cid:107) ≤ (cid:107) (cid:20) 1 + λ t 1 + β − β 1 0 (cid:21) (cid:107) 2 · (cid:107) (cid:34) w (cid:107) t 1 w (cid:107) t 1 − 1 (cid:35) (cid:107) . ( 54 ) The analysis essentially follows the same lines as Lemma 7 . Speciﬁcally , to show that w (cid:107) t 1 + 1 ≤ w (cid:107) t 1 − 1 , it sufﬁces to ensure that λ t 1 : = 3 η ( 1 − (cid:107) w t 1 (cid:107) 2 ) + ηξ t 1 ∈ [ − 1 , 0 ] . We have that λ t 1 ≤ 0 by the deﬁnition of t 1 . Furthermore , by the deﬁnition of t 1 − 1 , we have that (cid:107) w t 1 − 1 (cid:107) 2 < 1 + 13 c n . So if η ≤ min { 1 36 (cid:107) w t 1 − 1 (cid:107) c m , 1 18 c m } , then by invoking Lemma 9 , we have that (cid:107) w t 1 (cid:107) 2 − (cid:107) w t 1 − 1 (cid:107) 2 ≤ 19 and consequently , we have that (cid:107) w t 1 (cid:107) 2 ≤ 109 + 13 c n . Thus , by choosing η satisﬁes η ≤ 1 36 (cid:0) 1 + 13 c n (cid:1) c m , we have that (cid:107) w t 1 (cid:107) 2 ≤ 109 + 13 c n . Using this upper - bound of (cid:107) w t 1 (cid:107) 2 and the constraint of η , we have that λ t 1 ≥ − η ( 13 + 2 c n ) ≥ − 1 . Therefore , we have completed the proof . Lemma 9 . Assume that the norm of the momentum is bounded for all t ≤ T ζ , i . e . (cid:107) m t (cid:107) ≤ c m , ∀ t ≤ T ζ . Set the step size η satisﬁes η ≤ min { 1 36 (cid:107) w t − 1 (cid:107) c m , 1 18 c m } . Then , we have that (cid:107) w t (cid:107) 2 − (cid:107) w t − 1 (cid:107) 2 ≤ 1 9 . Proof . To see this , we will use the alternative presentation Algorithm 2 , which shows that w t = w t − 1 − ηm t − 1 , where the momentum m t − 1 stands for the weighted sum of gradients up to ( and including ) iteration t − 1 , i . e . m t − 1 = (cid:80) t − 1 s = 0 β t − 1 − s ∇ f ( w s ) . Using the expression , we can expand (cid:107) w t (cid:107) 2 − (cid:107) w t − 1 (cid:107) 2 as (cid:107) w t (cid:107) 2 − (cid:107) w t − 1 (cid:107) 2 = (cid:107) w t − 1 − ηm t − 1 (cid:107) 2 − (cid:107) w t − 1 (cid:107) 2 = − 2 η (cid:104) w t − 1 , m t − 1 (cid:105) + η 2 (cid:107) m t − 1 (cid:107) 2 ≤ 2 η (cid:107) w t − 1 (cid:107)(cid:107) m t − 1 (cid:107) + η 2 (cid:107) m t − 1 (cid:107) 2 ≤ 1 9 . ( 55 ) where the last inequality holds if η ≤ min { 1 36 (cid:107) w t − 1 (cid:107) c m , 1 18 c m } . Lemma 10 . Fix an index j . Set η ≤ 1 36 ( 1 + 13 c n ) max { c m , 1 } and β ≤ 1 . Suppose that (cid:107) w t (cid:107) 2 ≥ 13 + c n . If for a number R > 0 , we have that (cid:107) (cid:20) w ⊥ t [ j ] w ⊥ t − 1 [ j ] (cid:21) (cid:107) ≤ R , then (cid:107) (cid:20) w ⊥ t + 1 [ j ] w ⊥ t [ j ] (cid:21) (cid:107) ≤ R . Proof . The proof is similar to that of Lemma 7 . We have that (cid:107) (cid:20) w ⊥ t + 1 [ j ] w ⊥ t [ j ] (cid:21) (cid:107) ≤ (cid:107) (cid:20) 1 + λ t , j + β − β 1 0 (cid:21) (cid:107) 2 · (cid:107) (cid:20) w ⊥ t [ j ] w ⊥ t − 1 [ j ] (cid:21) (cid:107) . ( 56 ) Denote λ t , j : = η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + ηρ t , j . As the proof of Lemma 7 , to show that the spectral norm of the matrix (cid:107) (cid:20) 1 + λ t , j + β − β 1 0 (cid:21) (cid:107) 2 is not greater than one . It sufﬁces to have β ≤ 1 and that λ t , j ∈ [ − 1 , 0 ] . By the assumption , it holds that (cid:107) w t (cid:107) 2 ≥ 13 + c n , so we have that λ t , j ≤ 0 . Furthermore , by Lemma 8 , if the step size η satisﬁes η ≤ 1 36 (cid:0) 1 + 13 c n (cid:1) max { c m , 1 } , we have that (cid:107) w t (cid:107) 2 ≤ 109 + 13 c n . Therefore , using the upper - bound of the step size and the norm , we can obtain that λ t , j : = η ( 1 − 3 (cid:107) w t (cid:107) 2 ) + ηρ t , j ≥ − 1 . Hence , we have completed the proof . 21 E P ROOF OF T HEOREM 2 To prove Theorem 2 , we will need the following lemma . Lemma 11 . Fix any number δ > 0 . Deﬁne T δ : = min { t : ρ (cid:107) w t + 1 (cid:107) ≥ γ − δ } . Assume that η ≤ 1 (cid:107) A (cid:107) 2 + ρ (cid:107) w ∗ (cid:107) . Suppose that w ( 1 ) 0 b ( 1 ) ≤ 0 . Then , we have that w ( 1 ) t b ( 1 ) ≤ 0 , for all 0 ≤ t ≤ T δ . Proof . The lemma holds trivially when γ ≤ 0 , so let us assume γ > 0 . Recall the Heavy Ball generates the iterates as w ( 1 ) t + 1 = ( 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) ) w ( 1 ) t − ηb ( 1 ) + β ( w ( 1 ) t − w ( 1 ) t − 1 ) . We are going to show that for all t , w ( 1 ) t b ( 1 ) ≤ 0 and ( w ( 1 ) t − w ( 1 ) t − 1 ) b ( 1 ) ≤ − c bw w ( 1 ) t b ( 1 ) , ( 57 ) for any constant c bw ≥ 0 . The initialization guarantees that w ( 1 ) 0 b ( 1 ) ≤ 0 and that ( w ( 1 ) 0 − w ( 1 ) − 1 ) b ( 1 ) = 0 ≤ − c bw w ( 1 ) 0 b ( 1 ) . Suppose that ( 57 ) is true at iteration t . Consider iteration t + 1 . w ( 1 ) t + 1 b ( 1 ) = ( 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) ) w ( 1 ) t b ( 1 ) − η ( b ( 1 ) ) 2 + β ( w ( 1 ) t − w ( 1 ) t − 1 ) b ( 1 ) ≤ ( 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) − βc bw ) w ( 1 ) t b ( 1 ) − η ( b ( 1 ) ) 2 ≤ 0 , ( 58 ) where the ﬁrst inequality is by induction at iteration t and the second one is true if ( 1 − ηλ ( 1 ) A − ρη (cid:107) w t (cid:107) − βc bw ) ≥ 0 , which gives a constraints about η , 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) ≥ c bw . ( 59 ) Now let us switch to show that ( w ( 1 ) t + 1 − w ( 1 ) t ) b ( 1 ) ≤ − c bw w ( 1 ) t + 1 b ( 1 ) , which is equivalent to showing that w ( 1 ) t + 1 b ( 1 ) ≤ 1 1 + c bw w ( 1 ) t b ( 1 ) . From ( 58 ) , it sufﬁces to show that ( 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) − βc bw ) w ( 1 ) t b ( 1 ) − η ( b ( 1 ) ) 2 ≤ 1 1 + c bw w ( 1 ) t b ( 1 ) . ( 60 ) Since w ( 1 ) t b ( 1 ) ≤ 0 , a sufﬁcient condition of the above inequality is 1 − ηλ ( 1 ) ( A ) − ρη (cid:107) w t (cid:107) − βc bw − 1 1 + c bw ≥ 0 . ( 61 ) Now using that ρ (cid:107) w t (cid:107) ≤ γ − δ and that 11 + x ≤ 1 − 12 x for x ∈ [ 0 , 1 ] . It sufﬁces to have that 1 − ηλ ( 1 ) ( A ) − η ( γ − δ ) − βc bw − 1 1 + c bw ≥ 1 + ηδ − βc bw − 1 + 1 2 c bw ≥ ηδ − 1 2 c bw ≥ 0 . ( 62 ) By setting c bw = 0 , we have that the inequality is satisﬁed . Substituting c bw = 0 to ( 59 ) , we have that η ≤ 1 λ ( 1 ) ( A ) + ρ (cid:107) w t (cid:107) , Recall that ρ (cid:107) w t (cid:107) ≤ γ − δ for all t ≤ T δ . So we have that η ≤ 1 (cid:107) A (cid:107) 2 + (cid:0) γ − δ (cid:1) · 1 γ − δ ≥ 0 . Furthermore , by using that ρ (cid:107) w ∗ (cid:107) > γ , it sufﬁces to have that η ≤ 1 (cid:107) A (cid:107) 2 + ρ (cid:107) w ∗ (cid:107) . We have completed the proof . Given Lemma 11 , we are ready for proving Theorem 2 . Proof . ( of Theorem 2 ) The lemma holds trivially when γ ≤ 0 , so let us assume γ > 0 . Recall the notation that w ( 1 ) represents the projection of w on the eigenvector v 1 of the least eigenvalue λ ( 1 ) ( A ) , i . e . w ( 1 ) = (cid:104) w , v 1 (cid:105) . From the update rule , we have that w ( 1 ) t + 1 − ηb ( 1 ) = ( I d + ηγ − ρη (cid:107) w t (cid:107) ) w ( 1 ) t − ηb ( 1 ) + 1 + β ( w ( 1 ) t − w ( 1 ) t − 1 ) − ηb ( 1 ) . ( 63 ) 22 Denote a t : = w ( 1 ) t − ηb ( 1 ) . We can rewrite ( 63 ) as a t + 1 = ( 1 + ηγ − ρη (cid:107) w t (cid:107) ) a t + 1 + β ( a t − a t − 1 ) ≥ ( 1 + ηδ ) a t + 1 + β ( a t − a t − 1 ) , ( 64 ) where the inequality is due to that ρ (cid:107) w t (cid:107) ≤ γ − δ for t ≤ T δ . Now we are going to show that , a t + 1 ≥ ( 1 + ηδ + ηδ 1 + ηδ β ) a t + 1 . For the above inequality to hold , it sufﬁces to show that a t − a t − 1 ≥ ηδ 1 + ηδ a t . That is , a t ≥ 1 1 − ηδ / ( 1 + ηδ ) a t − 1 . The base case t = 1 holds because a 1 ≥ ( 1 + ηδ ) a 0 + 1 ≥ 1 1 − ηδ / ( 1 + ηδ ) a 0 . Suppose that at iteration t , we have that a t ≥ 1 1 − ηδ / ( 1 + ηδ ) a t − 1 . Consider t + 1 , we have that a t + 1 ≥ ( 1 + ηδ ) a t + 1 + β ( a t − a t − 1 ) ≥ ( 1 + ηδ ) a t + 1 ≥ 1 1 − ηδ / ( 1 + ηδ ) a t , where the second to last inequality is because a t ≥ 1 1 − ηδ / ( 1 + ηδ ) a t − 1 implies a t ≥ a t − 1 . Therefore , we have completed the induction . So we have shown that w ( 1 ) t + 1 − ηb ( 1 ) = ( I d + ηγ − ρη (cid:107) w t (cid:107) ) w ( 1 ) t − ηb ( 1 ) + 1 + β ( w ( 1 ) t − w ( 1 ) t − 1 ) − ηb ( 1 ) ≥ ( 1 + ηδ + ηδ 1 + ηδ β ) w ( 1 ) t − ηb ( 1 ) + 1 . Recursively expanding the inequality , we have that w ( 1 ) t + 1 − ηb ( 1 ) ≥ ( 1 + ηδ + ηδ 1 + ηδ β ) w ( 1 ) t − ηb ( 1 ) + 1 ≥ ( 1 + ηδ + ηδ 1 + ηδ β ) 2 w ( 1 ) t − 1 − ηb ( 1 ) + ( 1 + ηδ + ηδ 1 + ηδ β ) + 1 ≥ . . . ≥ 1 ηδ ( 1 + β 1 + ηδ ) (cid:0) ( 1 + ηδ + ηδ 1 + ηδ β ) t − 1 (cid:1) . ( 65 ) Therefore , γ − δρ ( a ) ≥ (cid:107) w T δ (cid:107) ≥ | w ( 1 ) T δ | ( b ) ≥ | b ( 1 ) | δ + δβ / ( 1 + ηδ ) (cid:0) ( 1 + ηδ + ηδ 1 + ηδ β ) T δ − 1 (cid:1) where ( a ) uses that for t ≤ T δ , ρ (cid:107) w t (cid:107) ≤ γ − δ , and ( b ) uses ( 65 ) and Lemma 11 that w ( 1 ) t b ( 1 ) ≤ 0 . Consequently , T δ ≤ log (cid:0) 1 + ( γ − δ ) ( δ + δβ / ( 1 + ηδ ) ) ρ | b ( 1 ) | (cid:1) log ( 1 + ηδ + ηδ 1 + ηδ β ) ( a ) ≤ 2 ηδ ( 1 + β / ( 1 + ηδ ) ) log (cid:0) 1 + ( γ − δ ) δ ( 1 + β / ( 1 + ηδ ) ) ρ | b ( 1 ) | (cid:1) ( b ) ≤ 2 ηδ ( 1 + β / ( 1 + ηδ ) ) log (cid:0) 1 + γ 2 + ( 1 + β / ( 1 + ηδ ) ) 4 ρ | b ( 1 ) | (cid:1) , ( 66 ) where ( a ) uses that log ( 1 + x ) ≥ x 2 for any x ∈ [ 0 , ∼ 2 . 51 ] , and ( b ) uses γδ − δ 2 ≤ γ 2 + 4 . F C ONVERGENCE OF HB FOR THE CUBIC - REGULARIZED PROBLEM Theorem 3 . Assume that the iterate stays in the benign region that exhibits one - point strong con - vexity to w ∗ , i . e . B : = { w ∈ R d : ρ (cid:107) w (cid:107) ≥ γ − δ } for a number δ > 0 , once it enters the benign region . Denote c converge : = 1 − 2˜ c 0 1 − 2 ηc δ (cid:0) ρ (cid:107) w ∗ (cid:107)− γ (cid:1) , where c δ and ˜ c 0 are some constants that satisfy 0 . 5 > c δ > 0 and ˜ c 0 > 0 . Suppose that there is a number R , such that the size of the iterate satisﬁes (cid:107) w t (cid:107) ≤ R for all t during the execution of the algorithm and that (cid:107) w ∗ (cid:107) ≤ R . Denote L : = (cid:107) A (cid:107) 2 + 2 ρR . Also , suppose that the momentum parameter β ∈ [ 0 , 0 . 65 ] and that the step size η satisﬁes η ≤ min (cid:0) 1 4 ( (cid:107) A ∗ (cid:107) + 26 ρR ) , ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107)− γ (cid:1) ( 50 L + 2 ( 26 ) 2 ) ( (cid:107) A ∗ (cid:107) + ρR ) ( 1 + (cid:107) A ∗ (cid:107) + ρR ) + 1 . 3 L (cid:1) . Set w 0 = w − 1 = − r b (cid:107) b (cid:107) for any sufﬁciently small r > 0 . Then , in the benign region it takes at most t (cid:46) ˆ T : = 1 ηc δ ( ρ (cid:107) w ∗ (cid:107)− γ ) ( 1 + βc converge ) log ( ( (cid:107) A (cid:107) 2 + 2 ρR ) ˜ c 2 (cid:15) ) number of iterations to reach an (cid:15) - approximate error , where ˜ c : = 4 R 2 ( 1 + η ˜ C ) with ˜ C = 43 ˜ c 0 ( ρ (cid:107) w ∗ (cid:107) − γ ) + 3 (cid:0) ˜ c 0 c δ ( ρ (cid:107) w ∗ (cid:107) − γ ) + 10 max { 0 , γ } (cid:1) . Note that the constraint of η ensures that ηc δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) ≤ 18 ; as a consequence , c converge ≤ 1 . Theorem 3 indicates that up to an upper - threshold , a larger value of β reduces the number of iterations to linearly converge to an (cid:15) - optimal point , and hence leads to a faster convergence . Let us now make a few remarks . First , we want to emphasize that in the linear convergence rate regime , a constant factor improvement of the convergence rate ( i . e . of ˆ T here ) means that the slope of the curve in the log plot of optimization value vs . iteration is steeper . Our experimental result 23 ( Figure 2 ) conﬁrms this . In this ﬁgure , we can see that the curve corresponds to a larger momentum parameter β has a steeper slope than that of the smaller ones . The slope is steeper as β increases , which justiﬁes the effectiveness of the momentum in the linear convergence regime . Our theoretical result also indicates that the acceleration due to the use of momentum is more evident for a small step size η . When η is sufﬁciently small , the number c converge are close to 1 , which means that the number of iterations can be reduced approximately by a factor of 1 + β . Secondly , from the theorem , one will need | b ( 1 ) | be non - zero , which can be guaranteed with a high probability by adding some Gaussian perturbation on b . We refer the readers to Section 4 . 2 of Carmon & Duchi ( 2019 ) for the technique . To prove Theorem 3 , we will need a series of lemmas which is given in the following subsection . F . 1 S OME SUPPORTING LEMMAS Lemma 12 . Denote sequences d t : = ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) (cid:107) w t − w ∗ (cid:107) 2 , e t : = − ( w t − w ∗ ) (cid:62) ( A ∗ − 2 η ˜ c w A 2 ∗ ) ( w t − w ∗ ) , g t : = − ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w t (cid:107) − 4 ηρ ˜ c w (cid:107) w t (cid:107) 2 (cid:1) , where c w : = 2 β 1 − β and ˜ c w : = ( 1 + c w ) + ( 1 + c w ) 2 . For all t , we have that (cid:104) w t − w ∗ , w t − w t − 1 (cid:105) + c w (cid:107) w t − w t − 1 (cid:107) 2 ≤ η t − 1 (cid:88) s = 1 β t − 1 − s ( d s + e s + g s ) . Proof . We use induction for the proof . The base case t = 0 holds , because w 0 = w − 1 by ini - tialization and both sides of the inequality is 0 . Let us assume that it holds at iteration t . That is , (cid:104) w t − w ∗ , w t − w t − 1 (cid:105) + c w (cid:107) w t − w t − 1 (cid:107) 2 ≤ η (cid:80) t − 1 s = 1 β t − 1 − s ( d s + e s + g s ) . Consider iteration t + 1 . We want to prove that (cid:104) w t + 1 − w ∗ , w t + 1 − w t (cid:105) + c w (cid:107) w t + 1 − w t (cid:107) 2 ≤ η t (cid:88) s = 1 β t − s ( d s + e s + g s ) . Denote ∆ : = w t + 1 − w t . It is equivalent to showing that (cid:104) ∆ , w t + ∆ − w ∗ (cid:105) + c w (cid:107) ∆ (cid:107) 2 ≤ η (cid:80) ts = 1 β t − s ( d s + e s + g s ) , or (cid:104)− η ∇ f ( w t ) + β ( w t − w t − 1 ) , w t − w ∗ − η ∇ f ( w t ) + β ( w t − w t − 1 ) (cid:105) + c w (cid:107) − η ∇ f ( w t ) + β ( w t − w t − 1 ) (cid:107) 2 ≤ η t (cid:88) s = 1 β t − s ( d s + e s + g s ) . ( 67 ) which is in turn equivalent to showing that − η (cid:104)∇ f ( w t ) , w t − w ∗ (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) ( a ) + η 2 ( 1 + c w ) (cid:107)∇ f ( w t ) (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) ( b ) − 2 ηβ ( 1 + c w ) (cid:104)∇ f ( w t ) , w t − w t − 1 (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) ( c ) + β (cid:104) w t − w t − 1 , w t − w ∗ (cid:105) + β 2 (cid:107) w t − w t − 1 (cid:107) 2 + c w β 2 (cid:107) w t − w t − 1 (cid:107) 2 ≤ η t (cid:88) s = 1 β t − s ( d s + e s + g s ) . ( 68 ) For term ( a ) , we have that (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) = ( w t − w ∗ ) (cid:62) A ∗ ( w t − w ∗ ) + ρ ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) ( (cid:107) w t (cid:107) 2 − w (cid:62)∗ w t ) = ( w t − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) + ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) . = ( w t − w ∗ ) (cid:62) A ∗ ( w t − w ∗ ) + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) (cid:107) w t − w ∗ (cid:107) 2 + ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) . ( 69 ) Notice that A ∗ (cid:23) − γ + ρ (cid:107) w ∗ (cid:107) I d (cid:23) 0 I d , as ρ (cid:107) w ∗ (cid:107) ≥ γ . On the other hand , for term ( b ) , we get that (cid:107)∇ f ( w t ) (cid:107) 2 = (cid:107) A ∗ ( w t − w ∗ ) − ρ ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) w t (cid:107) 2 ≤ 2 ( w t − w ∗ ) (cid:62) A 2 ∗ ( w t − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 (cid:107) w t (cid:107) 2 . ( 70 ) 24 For term ( c ) , we can bound it as − 2 ηβ ( 1 + c w ) (cid:104)∇ f ( w t ) , w t − w t − 1 (cid:105) ≤ (cid:107)√ 2 η ( 1 + c w ) ∇ f ( w t ) (cid:107)(cid:107)√ 2 β ( w t − w t − 1 ) (cid:107) ≤ 1 2 (cid:0) 2 η 2 ( 1 + c w ) 2 (cid:107)∇ f ( w t ) (cid:107) 2 + 2 β 2 (cid:107) w t − w t − 1 (cid:107) 2 (cid:1) = η 2 ( 1 + c w ) 2 (cid:107)∇ f ( w t ) (cid:107) 2 + β 2 (cid:107) w t − w t − 1 (cid:107) 2 . ( 71 ) Combining the above , we have that − η (cid:104)∇ f ( w t ) , w t − w ∗ (cid:105) + η 2 ( 1 + c w ) (cid:107)∇ f ( w t ) (cid:107) 2 − 2 ηβ ( 1 + c w ) (cid:104)∇ f ( w t ) , w t − w t − 1 (cid:105) + β (cid:104) w t − w t − 1 , w t − w ∗ (cid:105) + β 2 (cid:107) w t − w t − 1 (cid:107) 2 + c w β 2 (cid:107) w t − w t − 1 (cid:107) 2 ≤ − η (cid:104)∇ f ( w t ) , w t − w ∗ (cid:105) + η 2 ˜ c w (cid:107)∇ f ( w t ) (cid:107) 2 + β (cid:104) w t − w t − 1 , w t − w ∗ (cid:105) + β 2 ( 2 + c w ) (cid:107) w t − w t − 1 (cid:107) 2 ≤ − η ( w t − w ∗ ) (cid:62) (cid:0) A ∗ − 2 η ˜ c w A 2 ∗ (cid:1) ( w t − w ∗ ) − η ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w t (cid:107) − 4 ηρ ˜ c w (cid:107) w t (cid:107) 2 (cid:1) + η ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) (cid:107) w t − w ∗ (cid:107) 2 + β (cid:104) w t − w t − 1 , w t − w ∗ (cid:105) + β 2 ( 2 + c w ) (cid:107) w t − w t − 1 (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) = β (cid:104) w t − w t − 1 , w t − w ∗ (cid:105) + βc w (cid:107) w t − w t − 1 (cid:107) 2 ≤ η ( d t + e t + g t ) + ηβ t − 1 (cid:88) s = 1 β t − 1 − s ( d s + e s + g s ) : = η t (cid:88) s = 1 β t − s ( d s + e s + g s ) . ( 72 ) where in the second to last inequality we used β ( 2 + c w ) = c w and the last inequality we used the assumption at iteration t . So we have completed the induction . Lemma 13 . Assume that for all t , (cid:107) w t (cid:107) ≤ R for some number R . Following the notations used in Lemma 12 , we denote sequences d t : = ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) (cid:107) w t − w ∗ (cid:107) 2 , e t : = − ( w t − w ∗ ) (cid:62) ( A ∗ − 2 η ˜ c w A 2 ∗ ) ( w t − w ∗ ) , g t : = − ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w t (cid:107) − 4 ηρ ˜ c w (cid:107) w t (cid:107) 2 (cid:1) , where c w : = 2 β 1 − β and ˜ c w : = ( 1 + c w ) + ( 1 + c w ) 2 . Let us also denote c β : = ( 2 β 2 + 4 β + Lβ ) , L : = (cid:107) A (cid:107) 2 + 2 ρR , and z t : = (cid:80) ts = 0 β t − s + 1 ∇ f ( w s ) . If η satisﬁes : ( 1 ) η ≤ 1 + β 2 ρR , and ( 2 ) η ≤ 1 + β 4 (cid:107) A ∗ (cid:107) 2 , then we have that for all t , (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + η 2 ( w t − 1 − w ∗ ) (cid:62) ( 2 c β A 2 ∗ + 2 β 2 LI d + 2 c β ρ 2 R 2 I d ) ( w t − 1 − w ∗ ) + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 0 β t − 2 − s ( d s + e s + g s ) . ( 73 ) Proof . Recall that the update of heavy ball is w t + 1 = w t − η ∇ f ( w t ) + β ( w t − w t − 1 ) . ( 74 ) 25 So the distance term can be decomposed as (cid:107) w t + 1 − w ∗ (cid:107) 2 = (cid:107) w t − η ∇ f ( w t ) + β ( w t − w t − 1 ) − w ∗ (cid:107) 2 = (cid:107) w t − w ∗ (cid:107) 2 − 2 η (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) + η 2 (cid:107)∇ f ( w t ) (cid:107) 2 + β 2 (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ ( w t − w t − 1 ) (cid:62) ∇ f ( w t ) + 2 β ( w t − w ∗ ) (cid:62) ( w t − w t − 1 ) = (cid:107) w t − w ∗ (cid:107) 2 − 2 η (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) + η 2 (cid:107)∇ f ( w t ) (cid:107) 2 + β 2 (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) + 2 β ( w t − w ∗ ) (cid:62) ( w t − w t − 1 ) = (cid:107) w t − w ∗ (cid:107) 2 − 2 η ( 1 + β ) (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) ( a ) + η 2 (cid:107)∇ f ( w t ) (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) ( b ) + ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) + 2 β ( w t − 1 − w ∗ ) (cid:62) ( w t − w t − 1 ) . ( 75 ) For term ( a ) , (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) , by using that ∇ f ( w ) = A ∗ ( w − w ∗ ) − ρ ( (cid:107) w ∗ (cid:107) − (cid:107) w (cid:107) ) w , we can bound it as (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) = ( w t − w ∗ ) (cid:62) A ∗ ( w t − w ∗ ) + ρ ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) ( (cid:107) w t (cid:107) 2 − w (cid:62)∗ w t ) = ( w t − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) + ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) . ( 76 ) On the other hand , for term ( b ) , (cid:107)∇ f ( w t ) (cid:107) 2 , we get that (cid:107)∇ f ( w t ) (cid:107) 2 = (cid:107) A ∗ ( w t − w ∗ ) − ρ ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) w t (cid:107) 2 ≤ 2 ( w t − w ∗ ) (cid:62) A 2 ∗ ( w t − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 (cid:107) w t (cid:107) 2 . ( 77 ) By combining ( 75 ) , ( 76 ) , and ( 77 ) , we can bound the distance term as (cid:107) w t + 1 − w ∗ (cid:107) 2 = (cid:107) w t − w ∗ (cid:107) 2 − 2 η ( 1 + β ) (cid:104) w t − w ∗ , ∇ f ( w t ) (cid:105) + η 2 (cid:107)∇ f ( w t ) (cid:107) + ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) + 2 β ( w t − 1 − w ∗ ) (cid:62) ( w t − w t − 1 ) ≤ (cid:107) w t − w ∗ (cid:107) 2 − 2 η ( 1 + β ) (cid:0) ( w t − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) (cid:1) − ρη ( 1 + β ) (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) + η 2 (cid:0) 2 ( w t − w ∗ ) (cid:62) A 2 ∗ ( w t − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 (cid:107) w t (cid:107) 2 (cid:1) + ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) + 2 β ( w t − 1 − w ∗ ) (cid:62) ( w t − w t − 1 ) . ( 78 ) Now let us bound the terms on the last line of ( 78 ) . For the second to the last term , it is equal to − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) = − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t − 1 ) (cid:105) − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) − ∇ f ( w t − 1 ) (cid:105) , ( 79 ) On the other hand , the last term of ( 78 ) is equal to 2 β ( w t − 1 − w ∗ ) (cid:62) ( w t − w t − 1 ) = − 2 βη (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) (cid:105) + 2 β 2 (cid:104) w t − 1 − w ∗ , w t − 1 − w t − 2 (cid:105) . ( 80 ) By Lemma 12 , for all t , we have that (cid:104) w t − 1 − w ∗ , w t − 1 − w t − 2 (cid:105) ≤ − c w (cid:107) w t − 1 − w t − 2 (cid:107) 2 + η t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) . ( 81 ) Therefore , by combining ( 79 ) , ( 80 ) , and ( 81 ) , we have that ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w ∗ − w t − 1 , ∇ f ( w t ) (cid:105) + 2 β ( w t − 1 − w ∗ ) (cid:62) ( w t − w t − 1 ) ≤ ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) − ∇ f ( w t ) (cid:105) − 2 c w β 2 (cid:107) w t − 1 − w t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) . ( 82 ) 26 Combining ( 78 ) and ( 82 ) leads to the following , (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:107) w t − w ∗ (cid:107) 2 − 2 η ( 1 + β ) (cid:0) ( w t − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) (cid:1) − ρη ( 1 + β ) (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) + η 2 (cid:0) 2 ( w t − w ∗ ) (cid:62) A 2 ∗ ( w t − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 (cid:107) w t (cid:107) 2 (cid:1) + ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) − ∇ f ( w t ) (cid:105) . − 2 c w β 2 (cid:107) w t − 1 − w t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) . ( 83 ) Let us now bound the terms on the second to the last line ( 83 ) above , ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) − ∇ f ( w t ) (cid:105) . ( 84 ) First , note that (cid:107)∇ 2 f ( w ) (cid:107) ≤ (cid:107) A (cid:107) 2 + 2 ρ (cid:107) w (cid:107) . So we know that f is L : = (cid:107) A (cid:107) 2 + 2 ρR smooth on { w : (cid:107) w (cid:107) ≤ R } . Second , denote z t : = t (cid:88) s = 0 β t − s + 1 ∇ f ( w s ) , ( 85 ) we can bound (cid:107) w t − w t − 1 (cid:107) as (cid:107) w t − w t − 1 (cid:107) = (cid:107) − η t − 1 (cid:88) s = 0 β t − 1 − s ∇ f ( w s ) (cid:107) ≤ η (cid:107)∇ f ( w t − 1 ) (cid:107) + η (cid:107) t − 2 (cid:88) s = 0 β t − 1 − s ∇ f ( w s ) (cid:107) : = η (cid:107)∇ f ( w t − 1 ) (cid:107) + η (cid:107) z t − 2 (cid:107) , (cid:107) w t − w t − 1 (cid:107) 2 = (cid:107) − η t − 1 (cid:88) s = 0 β t − 1 − s ∇ f ( w s ) (cid:107) 2 ≤ 2 η 2 (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + 2 η 2 (cid:107) t − 2 (cid:88) s = 0 β t − 1 − s ∇ f ( w s ) (cid:107) 2 : = 2 η 2 (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + 2 η 2 (cid:107) z t − 2 (cid:107) 2 , ( 86 ) Using the results , we can bound ( 84 ) as ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) − ∇ f ( w t ) (cid:105) ( 86 ) ≤ 2 η 2 ( β 2 + 2 β ) ( (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + (cid:107) z t − 2 (cid:107) 2 ) + 2 η 2 βL (cid:107) w t − 1 − w ∗ (cid:107) ( (cid:107)∇ f ( w t − 1 ) (cid:107) + (cid:107) z t − 2 (cid:107) ) ≤ 2 η 2 ( β 2 + 2 β ) ( (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + (cid:107) z t − 2 (cid:107) 2 ) + η 2 βL ( (cid:107) w t − 1 − w ∗ (cid:107) 2 + (cid:107)∇ f ( w t − 1 ) (cid:107) 2 ) + η 2 βL ( (cid:107) w t − 1 − w ∗ (cid:107) 2 + (cid:107) z t − 2 (cid:107) 2 ) = 2 η 2 ( β 2 + 2 β ) ( (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + (cid:107) z t − 2 (cid:107) 2 ) + 2 η 2 βL (cid:107) w t − 1 − w ∗ (cid:107) 2 + η 2 βL (cid:107)∇ f ( w t − 1 ) (cid:107) 2 + η 2 βL (cid:107) z t − 2 (cid:107) 2 . ( 87 ) Note that (cid:107)∇ f ( w t − 1 ) (cid:107) 2 = (cid:107) A ∗ ( w t − 1 − w ∗ ) − ρ ( (cid:107) w ∗ (cid:107) − (cid:107) w t − 1 (cid:107) ) w t − 1 (cid:107) 2 ≤ 2 ( w t − 1 − w ∗ ) (cid:62) A 2 ∗ ( w t − 1 − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t − 1 (cid:107) (cid:1) 2 (cid:107) w t − 1 (cid:107) 2 . ( 88 ) Denote c β : = ( 2 β 2 + 4 β + Lβ ) . ( 89 ) By ( 87 ) , ( 88 ) , we have that ( β 2 + 2 β ) (cid:107) w t − w t − 1 (cid:107) 2 − 2 ηβ (cid:104) w t − 1 − w ∗ , ∇ f ( w t − 1 ) − ∇ f ( w t ) (cid:105) . ≤ 2 η 2 c β ( w t − 1 − w ∗ ) (cid:62) A 2 ∗ ( w t − 1 − w ∗ ) + 2 η 2 c β ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t − 1 (cid:107) (cid:1) 2 (cid:107) w t − 1 (cid:107) 2 + 2 η 2 β 2 L (cid:107) w t − 1 − w ∗ (cid:107) 2 + η 2 c β (cid:107) z t − 2 (cid:107) 2 . ( 90 ) 27 Let us summarize the results so far , by ( 83 ) and ( 90 ) , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:107) w t − w ∗ (cid:107) 2 − 2 η ( 1 + β ) (cid:0) ( w t − w ∗ ) (cid:62) (cid:0) A ∗ + ρ 2 ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) (cid:1) − ρη ( 1 + β ) (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 ( (cid:107) w t (cid:107) + (cid:107) w ∗ (cid:107) ) + η 2 (cid:0) 2 ( w t − w ∗ ) (cid:62) A 2 ∗ ( w t − w ∗ ) + 2 ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) 2 (cid:107) w t (cid:107) 2 (cid:1) + 2 η 2 c β ( w t − 1 − w ∗ ) (cid:62) A 2 ∗ ( w t − 1 − w ∗ ) + 2 η 2 c β ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t − 1 (cid:107) (cid:1) 2 (cid:107) w t − 1 (cid:107) 2 + 2 η 2 β 2 L (cid:107) w t − 1 − w ∗ (cid:107) 2 + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( 91 ) where we also used that w t − 1 − w t − 2 = − ηβ z t − 2 . We can rewrite the inequality above further as (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ ( w t − w ∗ ) (cid:62) (cid:0) I d − 2 η ( 1 + β ) A ∗ ( I d − η 1 + β A ∗ ) − ηρ ( 1 + β ) ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) − ηρ ( 1 + β ) ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) 2 (cid:0) (cid:107) w t (cid:107) ( 1 − 2 ηρ (cid:107) w t (cid:107) 1 + β ) + (cid:107) w ∗ (cid:107) (cid:1) (cid:124) (cid:123)(cid:122) (cid:125) ≤ 0 + 2 η 2 c β ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t − 1 (cid:107) (cid:1) 2 (cid:107) w t − 1 (cid:107) 2 + η 2 ( w t − 1 − w ∗ ) (cid:62) ( 2 c β A 2 ∗ + 2 β 2 LI d ) ( w t − 1 − w ∗ ) + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( 92 ) where we used that − ηρ ( 1 + β ) ( (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) ) 2 (cid:0) (cid:107) w t (cid:107) ( 1 − 2 ηρ (cid:107) w t (cid:107) 1 + β ) + (cid:107) w ∗ (cid:107) (cid:1) ≤ 0 for any η that satisﬁes η ≤ 1 + β 2 ρR , ( 93 ) as (cid:107) w t (cid:107) ≤ R for all t . Let us simplify the inequality ( 92 ) further by writing it as (cid:107) w t + 1 − w ∗ (cid:107) 2 ( a ) ≤ ( w t − w ∗ ) (cid:62) (cid:0) I d − 2 η ( 1 + β ) A ∗ ( I d − η 1 + β A ∗ ) − ηρ ( 1 + β ) ( (cid:107) w t (cid:107) − (cid:107) w ∗ (cid:107) ) I d (cid:1) ( w t − w ∗ ) + η 2 ( w t − 1 − w ∗ ) (cid:62) ( 2 c β A 2 ∗ + 2 β 2 LI d + 2 c β ρ 2 R 2 I d ) ( w t − 1 − w ∗ ) + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) . ( b ) ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + η 2 ( w t − 1 − w ∗ ) (cid:62) ( 2 c β A 2 ∗ + 2 β 2 LI d + 2 c β ρ 2 R 2 I d ) ( w t − 1 − w ∗ ) + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( 94 ) where ( a ) is because that ( (cid:107) w ∗ (cid:107)−(cid:107) w t − 1 (cid:107) ) 2 (cid:107) w t − 1 (cid:107) 2 ≤ (cid:107) w ∗ − w t − 1 (cid:107) 2 (cid:107) w t − 1 (cid:107) 2 ≤ (cid:107) w ∗ − w t − 1 (cid:107) 2 R 2 as (cid:107) w t (cid:107) ≤ R for all t , while ( b ) is by another constraint of η , η ≤ 1 + β 4 (cid:107) A ∗ (cid:107) 2 , ( 95 ) so that 2 ηA ∗ ( I d − η 1 + β A ∗ ) (cid:23) 32 ηA ∗ (cid:23) 32 η ( − γ + ρ (cid:107) w ∗ (cid:107) ) I d . 28 Lemma 14 . Fix some numbers c 0 , c 1 > 0 . Denote ω βt − 2 : = (cid:80) t − 2 s = 0 β . Following the notations and assumptions used in Lemma 12 and Lemma 13 . If η satisﬁes : ( 1 ) η ≤ c 0 β 2 c β (cid:107) A ∗ (cid:107) 22 + 2 β 2 L + 2 c β ρ 2 R 2 , ( 2 ) η ≤ c 1 β ( 2˜ c w + Lβω βt − 2 ) (cid:107) A ∗ (cid:107) , ( 3 ) η ≤ c 1 ρβ 2 Lω βt − 2 R , ( 4 ) η ≤ 1 4 ρ ˜ c w R , and ( 5 ) η ≤ c 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) Lβ 2 ω βt − 2 ( (cid:107) A ∗ (cid:107) 2 + ρ 2 R 2 ) for all t , then we have that for all t , (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w t (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) . Proof . From Lemma 13 , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + η 2 ( w t − 1 − w ∗ ) (cid:62) ( 2 c β A 2 ∗ + 2 β 2 LI d + 2 c β ρ 2 R 2 I d ) ( w t − 1 − w ∗ ) + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( a ) ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + η 2 ( c β − 2 c w ) (cid:107) z t − 2 (cid:107) 2 + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( b ) ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 η 2 ( c β − 2 c w ) β 2 ω βt − 2 t − 2 (cid:88) s = 0 β t − 2 − s ( w s − w ∗ ) (cid:62) ( A 2 ∗ + ρ 2 R 2 I d ) ( w s − w ∗ ) + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( d s + e s + g s ) , ( 96 ) where ( a ) is by a constraint of η so that η ( 2 c β (cid:107) A ∗ (cid:107) 22 + 2 β 2 L + 2 c β ρ 2 R 2 ) ≤ c 0 β , and ( b ) is due to the following , denote ω βt : = (cid:80) ts = 0 β , we have that (cid:107) z t (cid:107) 2 : = (cid:107) t (cid:88) s = 0 β t − s + 1 ∇ f ( w s ) (cid:107) 2 = β 2 ( ω βt ) 2 (cid:107) t (cid:88) s = 0 β t − s ω βt ∇ f ( w s ) (cid:107) 2 ≤ β 2 ( ω βt ) 2 (cid:0) t (cid:88) s = 0 β t − s ω βt (cid:107)∇ f ( w s ) (cid:107) 2 (cid:1) ≤ 2 β 2 ω βt (cid:0) t (cid:88) s = 0 β t − s ( w s − w ∗ ) (cid:62) ( A 2 ∗ + ρ 2 R 2 I d ) ( w s − w ∗ ) (cid:1) . ( 97 ) where the ﬁrst inequality of ( 97 ) is due to Jensen’s inequality and the second inequality of ( 97 ) is due to the following upper - bound of the gradient norm , (cid:107)∇ f ( w s ) (cid:107) 2 = (cid:107) A ∗ ( w s − w ∗ ) − ρ ( (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) ) w s (cid:107) 2 ≤ 2 ( w s − w ∗ ) (cid:62) A 2 ∗ ( w s − w ∗ ) + 2 ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) ) 2 (cid:107) w s (cid:107) 2 ≤ 2 ( w s − w ∗ ) (cid:62) A 2 ∗ ( w s − 29 w ∗ ) + 2 ρ 2 (cid:107) w ∗ − w s (cid:107) 2 R 2 . By the deﬁnitions of d s , e s , g s , we further have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w s − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 ( (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) ) I d (cid:1) ( w s − w ∗ ) + 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) η ( 2˜ c w + ( c β − 2 c w ) ω βt − 2 ) A 2 ∗ (cid:1) ( w s − w ∗ ) + 2 η 2 ρ 2 R 2 β 2 ω βt − 2 ( c β − 2 c w ) t − 2 (cid:88) s = 1 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − ηβ 2 ρ t − 2 (cid:88) s = 1 β t − 2 − s ( (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w s (cid:107) − 4 ηρ ˜ c w (cid:107) w s (cid:107) 2 (cid:1) + η 2 ( c β − 2 c w ) 2 β t ω βt − 2 (cid:0) (cid:107) A ∗ (cid:107) 22 + ρ 2 R 2 ) (cid:107) w 0 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) + 2 ηβc 1 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) ( A ∗ + ρRI d ) ( w s − w ∗ ) − ηβ 2 ρ t − 2 (cid:88) s = 1 β t − 2 − s ( (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w s (cid:107) − 4 ηρ ˜ c w (cid:107) w s (cid:107) 2 (cid:1) + η 2 ( c β − 2 c w ) 2 β t ω βt − 2 (cid:0) (cid:107) A ∗ (cid:107) 22 + ρ 2 R 2 ) (cid:107) w 0 − w ∗ (cid:107) 2 , ( 98 ) where the last inequality is due to ( 1 ) : 2 η 2 β 2 ( 2˜ c w + ( c β − 2 c w ) ω βt − 2 ) A 2 ∗ (cid:22) 2 η 2 β 2 ( 2˜ c w + Lβω βt − 2 ) A 2 ∗ (cid:22) 2 ηβc 1 A ∗ , as c β − 2 c w ≤ Lβ and that η ≤ c 1 β ( 2˜ c w + Lβω βt − 2 ) (cid:107) A ∗ (cid:107) , and ( 2 ) that 2 η 2 ρ 2 R 2 β 2 ω βt − 2 ( c β − 2 c w ) ≤ 2 η 2 ρ 2 R 2 Lβ 3 ω βt − 2 ≤ 2 ηρβc 1 R , as η ≤ c 1 ρβ 2 Lω βt − 2 R . To continue , let us bound the last two terms on ( 98 ) . For the second to the last term , by using that (cid:107) w t (cid:107) ≤ R for all t and that η ≤ 1 4 ρ ˜ c w R , we have that (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w s (cid:107) − 4 ηρ ˜ c w (cid:107) w s (cid:107) 2 (cid:1) ≥ (cid:107) w ∗ (cid:107) . Therefore , we have that the second to last term on ( 98 ) is non - positive , namely , − ηβ 2 ρ (cid:80) t − 2 s = 1 β t − 2 − s ( (cid:107) w ∗ (cid:107)−(cid:107) w s (cid:107) ) 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w s (cid:107)− 4 ηρ ˜ c w (cid:107) w s (cid:107) 2 (cid:1) ≤ 0 . For the last term on ( 98 ) , by using that η ≤ c 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) Lβ 2 ω βt − 2 ( (cid:107) A ∗ (cid:107) 2 + ρ 2 R 2 ) , we have that η 2 ( c β − 2 c w ) 2 β t ω βt − 2 (cid:0) (cid:107) A ∗ (cid:107) 22 + ρ 2 R 2 ) (cid:107) w 0 − w ∗ (cid:107) 2 ≤ η 2 2 Lβ t + 1 ω βt − 2 (cid:0) (cid:107) A ∗ (cid:107) 22 + ρ 2 R 2 ) (cid:107) w 0 − w ∗ (cid:107) 2 ≤ 2 ηβ t − 1 c 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) (cid:107) w 0 − w ∗ (cid:107) 2 . Combining the above results , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) . ( 99 ) 30 The following lemma will be used for getting the iteration complexity from Lemma 14 . Lemma 15 . For a non - negative sequence { y t } t ≥ 0 , suppose that it satisﬁes y t + 1 ≤ py t + qy t − 1 + rβ ¯ y t − 2 + xβ t − 2 (cid:88) s = τ β t − 2 − s y s + zβ t − τ + 1 , for all t ≥ τ , ( 100 ) for non - negative numbers p , q , r , x , z ≥ 0 and β ∈ [ 0 , 1 ) , where we denote ¯ y t : = t (cid:88) s = 0 β t − s y s . ( 101 ) Fix some numbers φ ∈ ( 0 , 1 ) and ψ > 0 . Deﬁne θ = − p + √ p 2 + 4 ( q + φ ) 2 . Suppose that β ≤ p + √ p 2 + 4 ( q + φ ) 2 φ r + φ + x and β ≤ p + (cid:112) p 2 + 4 ( q + φ ) 2 − 1 ψ . then we have that for all t ≥ τ , y t ≤ (cid:0) p + (cid:112) p 2 + 4 ( q + φ ) 2 (cid:1) t − τ c τ , β , ( 102 ) where c τ , β is an upper bound that satisﬁes , for any t ≤ τ , y t + θy t − 1 + φ ¯ y t − 2 + βψz ≤ c τ , β , ( 103 ) Proof . For a non - negative sequence { y t } t ≥ 0 , suppose that it satisﬁes y t + 1 ≤ py t + qy t − 1 + rβ ¯ y t − 2 + xβ t − 2 (cid:88) s = τ β t − 2 − s y s + zβ t − τ + 1 , for all t ≥ τ , ( 104 ) for non - negative numbers p , q , r , x , z and β ∈ [ 0 , 1 ) , where we denote ¯ y t : = t (cid:88) s = 0 β t − s y s . ( 105 ) y t + 1 + θy t + φ ¯ y t − 1 + ψzβ t − τ + 2 ( a ) ≤ ( p + θ ) y t + qy t − 1 + rβ ¯ y t − 2 + φ ¯ y t − 1 + xβ t − 2 (cid:88) s = τ β t − 2 − s y s + ( ψβ + 1 ) zβ t − τ + 1 ( b ) = ( p + θ ) y t + ( q + φ ) y t − 1 + ( rβ + φβ ) ¯ y t − 2 + xβ t − 2 (cid:88) s = τ β t − 2 − s y s + ( ψβ + 1 ) zβ t − τ + 1 ≤ ( p + θ ) ( y t + ( q + φ ) p + θ y t − 1 ) + β ( r + φ + x ) ¯ y t − 2 + ( ψβ + 1 ) zβ t − τ + 1 ( c ) ≤ ( p + θ ) ( y t + θy t − 1 ) + β ( r + φ + x ) ¯ y t − 2 + ( ψβ + 1 ) zβ t − τ + 1 ( d ) ≤ ( p + θ ) ( y t + θy t − 1 + φ ¯ y t − 2 + ψzβ t − τ + 1 ) ≤ ( p + θ ) 2 ( y t − 1 + θy t − 2 + φ ¯ y t − 3 + ψzβ t − τ − 1 ) ≤ . . . : = ( p + θ ) t − τ + 1 c τ , β . ( 106 ) where ( a ) is due to the dynamics ( 100 ) , ( b ) is because ¯ y t − 1 = y t − 1 + β ¯ y t − 2 , ( c ) is by , q + φ p + θ ≤ θ , ( 107 ) and ( d ) is by β ≤ ( p + θ ) φ r + φ + x and β ≤ p + θ − 1 ψ . ( 108 ) 31 Note that ( 107 ) holds if , θ ≥ − p + (cid:112) p 2 + 4 ( q + φ ) 2 . ( 109 ) Let us choose the minimal θ = − p + √ p 2 + 4 ( q + φ ) 2 . So we have p + θ = p + √ p 2 + 4 ( q + φ ) 2 , which completes the proof . Lemma 16 . Assume that for all t , (cid:107) w t (cid:107) ≤ R for some number R . Fix the numbers c 0 , c 1 > 0 in Lemma 14 so that c 1 ≤ c 0 40 ( (cid:107) A ∗ (cid:107) + ρR ) . Suppose that the step size η satisﬁes ( 1 ) η ≤ c 0 β 2 c β (cid:107) A ∗ (cid:107) 22 + 2 β 2 L + 2 c β ρ 2 R 2 , ( 2 ) η ≤ c 1 β ( 2˜ c w + Lβω βt − 2 ) (cid:107) A ∗ (cid:107) , ( 3 ) η ≤ c 1 ρβ 2 Lω βt − 2 R , ( 4 ) η ≤ 1 4 ρ ˜ c w R , ( 5 ) η ≤ c 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) Lβ 2 ω βt − 2 ( (cid:107) A ∗ (cid:107) 2 + ρ 2 R 2 ) , ( 6 ) η ≤ 1 + β 2 ρR , and ( 7 ) η ≤ 1 + β 4 (cid:107) A ∗ (cid:107) 2 for all t , where c β : = ( 2 β 2 + 4 β + Lβ ) , L : = (cid:107) A (cid:107) 2 + 2 ρR , c w : = 2 β 1 − β , ˜ c w : = ( 1 + c w ) + ( 1 + c w ) 2 and ω βt − 2 : = (cid:80) t − 2 s = 0 β . Furthermore , sup - pose that the momentum parameter β satisﬁes β ≤ min { 1011 (cid:0) 1 − η ( 1 + β ) δ (cid:1) , 1 − η ( 1 + β ) δ − 0 . 1 (cid:9) . Fix numbers δ , δ (cid:48) so that 12 ( ρ (cid:107) w ∗ (cid:107) − γ ) > δ > 0 and c 0 20 ≥ δ (cid:48) > 0 . Assume that (cid:107) w t (cid:107) is non - decreasing . If ρ (cid:107) w t (cid:107) ≥ γ − 12 ( ρ (cid:107) w ∗ (cid:107) − γ ) + δ and ρ (cid:107) w t (cid:107) ≥ γ − δ (cid:48) for some t ≥ t ∗ , then we have that (cid:107) w t − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) δ + 2 ηβc 0 1 − η ( 1 + β ) δ (cid:1) t − t ∗ c β , t ∗ where c β , t ∗ is a number that satisﬁes for any t ≤ t ∗ , (cid:107) w t − w ∗ (cid:107) 2 + 2 ηc 0 β 1 − η ( 1 + β ) δ (cid:107) w t − 1 − w ∗ (cid:107) 2 + ηβc 0 (cid:80) t − 2 s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 + max { 0 , β 20 η (cid:80) t ∗ − 1 s = 1 β t ∗ − 1 − s (cid:0) γ 2 − ρ 2 (cid:107) w s (cid:107) (cid:1) (cid:107) w s − w ∗ (cid:107) 2 } ≤ c β , t ∗ . Proof . From Lemma 14 , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) (cid:2) ρ (cid:107) w t (cid:107) − ( γ − ρ (cid:107) w ∗ (cid:107) − γ 2 ) (cid:3)(cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 ( (cid:107) A (cid:107) ∗ + ρR ) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) . Using that for all t ≥ t ∗ , ρ (cid:107) w t (cid:107) ≥ γ − 12 ( ρ (cid:107) w ∗ (cid:107) − γ ) + δ , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) δ (cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 ( (cid:107) A (cid:107) ∗ + ρR ) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) = (cid:0) 1 − η ( 1 + β ) δ (cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 ( (cid:107) A (cid:107) ∗ + ρR ) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 − 2 ηβ 2 t − 2 (cid:88) s = t ∗ β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) − 2 ηβ 2 t ∗ − 1 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) . ( 110 ) We bound the second to last term of ( 110 ) as follows . Note that for s ≥ t ∗ , we have that ρ (cid:107) w s (cid:107) ≥ γ − δ (cid:48) by the assumption that (cid:107) w t (cid:107) is non - decreasing . Therefore , once ρ (cid:107) w t (cid:107) exceeds any level 32 blow γ − δ (cid:48) , it will not fall below γ − δ (cid:48) . So we have that A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d ( a ) (cid:23) A ∗ − ρ 2 (cid:107) w ∗ (cid:107) I d + γ − δ (cid:48) 2 I d ( b ) (cid:23) ( − γ + ρ (cid:107) w ∗ (cid:107) ) I d − ρ 2 (cid:107) w ∗ (cid:107) I d + γ − δ (cid:48) 2 I d ( c ) (cid:23) − δ (cid:48) 2 I d ( 111 ) where ( a ) uses that ρ (cid:107) w s (cid:107) ≥ γ − δ (cid:48) for some number δ (cid:48) > 0 , ( b ) uses the fact that A ∗ (cid:23) ( − γ + ρ (cid:107) w ∗ (cid:107) ) I d , and ( c ) uses the fact that ρ (cid:107) w ∗ (cid:107) ≥ γ . Using the result , we can bound the second to the last term of ( 110 ) as − 2 ηβ 2 t − 2 (cid:88) s = t ∗ β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) ≤ ηβ 2 δ (cid:48) t − 2 (cid:88) s = t ∗ β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 . ( 112 ) Now let us switch to the last term of ( 110 ) . Using the fact that A ∗ (cid:23) ( − γ + ρ (cid:107) w ∗ (cid:107) ) I d , and that ρ (cid:107) w ∗ (cid:107) ≥ γ by the characterization of the optimizer (cid:107) w ∗ (cid:107) , we have that A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:23) ρ 2 (cid:0) (cid:107) w ∗ (cid:107) + (cid:107) w s (cid:107) (cid:1) I d − γ (cid:23) (cid:0) ρ 2 (cid:107) w s (cid:107) − γ 2 (cid:1) I d . ( 113 ) So we can bound the last term as − 2 ηβ 2 t ∗ − 1 (cid:88) s = 1 β t − 2 − s ( w s − w ∗ ) (cid:62) (cid:0) A ∗ − ρ 2 (cid:0) (cid:107) w ∗ (cid:107) − (cid:107) w s (cid:107) (cid:1) I d (cid:1) ( w s − w ∗ ) ≤ 2 ηβ t − t ∗ + 1 t ∗ − 1 (cid:88) s = 1 β t ∗ − 1 − s ( w s − w ∗ ) (cid:62) (cid:0) γ 2 − ρ 2 (cid:107) w s (cid:107) (cid:1) I d ( w s − w ∗ ) : = 2 ηβ t − t ∗ + 1 D t ∗ , ( 114 ) where we denote D t ∗ : = (cid:80) t ∗ − 1 s = 1 β t ∗ − 1 − s ( w s − w ∗ ) (cid:62) (cid:0) γ 2 − ρ 2 (cid:107) w s (cid:107) (cid:1) I d ( w s − w ∗ ) . Combining ( 110 ) , ( 112 ) , ( 114 ) , we have that (cid:107) w t + 1 − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) δ (cid:1) (cid:107) w t − w ∗ (cid:107) 2 + ηβc 0 (cid:107) w t − 1 − w ∗ (cid:107) 2 + 2 ηβc 1 ( (cid:107) A (cid:107) ∗ + ρR ) t − 2 (cid:88) s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 + ηβ 2 δ (cid:48) t − 2 (cid:88) s = t ∗ β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 + 2 ηβ t − t ∗ + 1 D t ∗ . ( 115 ) Now we are ready to use Lemma 15 . Set p , q , r , x , z in Lemma 15 as follows . • p = 1 − η ( 1 + β ) δ • q = ηβc 0 • r = 2 η ( (cid:107) A ∗ (cid:107) + ρR ) c 1 • x = ηβδ (cid:48) • z = 2 ηD t ∗ 1 { D t ∗ ≥ 0 } . • φ = q = ηβc 0 • ψ = 10 . 33 we have that (cid:107) w t − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) δ + (cid:112) ( 1 − η ( 1 + β ) δ ) 2 + 4 ( 2 ηβc 0 ) 2 (cid:1) t − t ∗ c β , t ∗ ≤ (cid:0) 1 − η ( 1 + β ) δ + 2 ηβc 0 1 − η ( 1 + β ) δ (cid:1) t − t ∗ c β , t ∗ ( 116 ) where we use that a + a √ 1 + 4 b / a 2 2 ≤ a + a ( 1 + 2 ba 2 ) 2 = a + ba for number a , b that satisfy 4 b / a 2 ≥ − 1 . Now let us check the conditions of Lemma 15 to see if β ≤ p + √ p 2 + 4 ( q + φ ) 2 φ r + φ + x and β ≤ p + √ p 2 + 4 ( q + φ ) 2 − 1 ψ . For the ﬁrst condition , β ( r + φ + x ) ≤ p + √ p 2 + 4 ( q + φ ) 2 φ is equivalent to β (cid:0) 2 η ( (cid:107) A ∗ (cid:107) + ρR ) c 1 + ηβc 0 + ηβδ (cid:48) (cid:1) ≤ p + √ p 2 + 4 ( q + φ ) 2 ηβc 0 , which can be satisﬁed by c 1 ≤ c 0 40 ( (cid:107) A ∗ (cid:107) + ρR ) and δ (cid:48) ≤ c 0 20 , leading to an upper bound of β , which is β ≤ 1011 p + √ p 2 + 4 ( q + φ ) 2 . Using the expression of p , q , φ , and ψ , it sufﬁces to have that β ≤ 1011 (cid:0) 1 − η ( 1 + β ) δ (cid:1) . On the other hand , for the second condition , β ≤ p + √ p 2 + 4 ( q + φ ) 2 − 1 ψ , by using the expression of p , q , φ , and ψ , it sufﬁces to have that β ≤ 1 − η ( 1 + β ) δ − 0 . 1 . F . 2 P ROOF OF T HEOREM 3 Proof . Let δ = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) for some number c δ < 0 . 5 . Denote c convergeηδ : = 1 − 2˜ c 0 1 − 2 ηδ for some number ˜ c 0 > 0 . By Lemma 16 , we have that (cid:107) w t − w ∗ (cid:107) 2 ≤ (cid:0) 1 − η ( 1 + β ) δ + 2 ηβ ˜ c 0 δ 1 − η ( 1 + β ) δ (cid:1) t − t ∗ c β , t ∗ ≤ (cid:0) 1 − ηδ ( 1 + βc convergeηδ ) (cid:1) t − t ∗ c β , t ∗ ( 117 ) where c β , t ∗ is a number that satisﬁes for any t ≤ t ∗ , (cid:107) w t − w ∗ (cid:107) 2 + 2 ηc 0 β 1 − η ( 1 + β ) δ (cid:107) w t − 1 − w ∗ (cid:107) 2 + ηβc 0 (cid:80) t − 2 s = 0 β t − 2 − s (cid:107) w s − w ∗ (cid:107) 2 + max { 0 , β 20 η (cid:80) t ∗ − 1 s = 1 β t ∗ − 1 − s (cid:0) γ 2 − ρ 2 (cid:107) w s (cid:107) (cid:1) (cid:107) w s − w ∗ (cid:107) 2 } ≤ c β , t ∗ . Note that we can obtain a trivial upper - bound c β , t for any t as follows . Using that (cid:107) w t − w ∗ (cid:107) 2 ≤ 4 R 2 and that c 0 ← ˜ c 0 δ and that δ ← c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , we can upper - bound the term as c β , t ≤ 4 R 2 (cid:0) 1 + 2 η ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) β 1 − η ( 1 + β ) c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) + ηβ ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) 1 − β t 1 − β + β 10 η max { 0 , γ } 1 − β t 1 − β (cid:1) ≤ 4 R 2 ( 1 + ηβ ˜ C β ) : = ˜ c β . ( 118 ) where we deﬁne ˜ C β : = 2˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107)− γ (cid:1) 1 − η ( 1 + β ) c δ (cid:0) ρ (cid:107) w ∗ (cid:107)− γ (cid:1) + 1 1 − β (cid:0) ˜ c 0 c δ ( ρ (cid:107) w ∗ (cid:107)− γ ) + 10 max { 0 , γ } (cid:1) and ˜ c β : = 4 R 2 ( 1 + ηβ ˜ C β ) . Now denote c converge : = 1 − 2˜ c 0 1 − 2 ηc δ (cid:0) ρ (cid:107) w ∗ (cid:107)− γ (cid:1) . We have that f ( w t ∗ + t ) − f ( w ∗ ) ( a ) ≤ (cid:107) A (cid:107) 2 + 2 ρR 2 (cid:107) w t ∗ + t − w ∗ (cid:107) 2 ( b ) ≤ (cid:107) A (cid:107) 2 + 2 ρR 2 ˜ c β exp (cid:0) − ηc δ ( ρ (cid:107) w ∗ (cid:107) − γ ) ( 1 + βc converge ) t (cid:1) ( 119 ) where ( a ) uses the ( (cid:107) A (cid:107) 2 + 2 ρR ) - smoothness of function f ( · ) in the region of { w : (cid:107) w (cid:107) ≤ R } , and ( b ) uses ( 117 ) , ( 118 ) and that δ : = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) . So we see that the number of iterations in the linear convergence regime is at most t ≤ ˆ T : = 1 ηc δ ( ρ (cid:107) w ∗ (cid:107) − γ ) ( 1 + βc converge ) log ( ( (cid:107) A (cid:107) 2 + 2 ρR ) ˜ c β 2 (cid:15) ) . ( 120 ) 34 Lastly , let us check if the step size η satisﬁes the constraints of Lemma 16 . Recall the notations that c w : = 2 β 1 − β , ˜ c w : = ( 1 + c w ) + ( 1 + c w ) 2 , c β : = ( 2 β 2 + 4 β + Lβ ) , and L : = (cid:107) A (cid:107) 2 + 2 ρR . Lemma 16 has the following constraints , ( 1 ) η ≤ c 0 β 2 c β (cid:107) A ∗ (cid:107) 22 + 2 β 2 L + 2 c β ρ 2 R 2 , ( 2 ) η ≤ c 1 β ( 2˜ c w + Lβ / ( 1 − β ) ) (cid:107) A ∗ (cid:107) , ( 3 ) η ≤ c 1 ρβ 2 LR / ( 1 − β ) , ( 4 ) η ≤ 1 4 ρ ˜ c w R , ( 5 ) η ≤ c 1 (cid:0) (cid:107) A ∗ (cid:107) + ρR (cid:1) Lβ 2 ( (cid:107) A ∗ (cid:107) 2 + ρ 2 R 2 ) / ( 1 − β ) , ( 6 ) η ≤ 1 + β 2 ρR , and ( 7 ) η ≤ 1 + β 4 (cid:107) A ∗ (cid:107) 2 . For the constraints of ( 1 ) , using c 0 = ˜ c 0 δ and that δ = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , it can be rewritten as η ≤ ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) 2 ( 2 β + 4 + L ) (cid:0) (cid:107) A ∗ (cid:107) 22 + ρ 2 R 2 (cid:1) + 2 βL . ( 121 ) For the constraints of ( 2 ) , using c 1 ≤ ˜ c 0 δ 40 ( (cid:107) A ∗ (cid:107) + ρR ) and that δ = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , it can be rewritten as η ≤ ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) 40 β ( (cid:107) A ∗ (cid:107) 2 + ρR (cid:107) A ∗ (cid:107) ) ( 2˜ c w + Lβ / ( 1 − β ) ) . ( 122 ) The constraints of ( 3 ) can be written as , using c 1 ≤ ˜ c 0 δ 40 ( (cid:107) A ∗ (cid:107) + ρR ) and that δ = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , η ≤ ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) 40 ρR ( (cid:107) A ∗ (cid:107) + ρR ) β 2 L / ( 1 − β ) . ( 123 ) The constraints of ( 5 ) translates into , using c 1 ≤ ˜ c 0 δ 40 ( (cid:107) A ∗ (cid:107) + ρR ) and that δ = c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) , η ≤ ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) Lβ 2 ( (cid:107) A ∗ (cid:107) 2 + ρ 2 R 2 ) / ( 1 − β ) . ( 124 ) Considering all the above constraints , it sufﬁces to let η satisﬁes η ≤ min (cid:0) 1 4 ( (cid:107) A ∗ (cid:107) + ˜ c w ρR ) , ˜ c 0 c δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) C β ( (cid:107) A ∗ (cid:107) + ρR ) ( 1 + (cid:107) A ∗ (cid:107) + ρR ) + 2 βL (cid:1) , ( 125 ) where C β : = max (cid:0) 4 β + 8 + 2 L , 40 β 2 L 1 − β + 80 β ˜ c w , 4˜ c w (cid:1) . Note that the constraint of η satisﬁes ηc δ (cid:0) ρ (cid:107) w ∗ (cid:107) − γ (cid:1) ≤ 14 c δ ≤ 18 . Using this inequality , we can simplify the constraint regarding the parameter β in Lemma 16 , which leads to β ∈ [ 0 , 0 . 65 ] . Consequently , we can simplify and upper bound the constants ˜ c w , C β and ˜ C β , which leads to the theorem statement . Thus , we have completed the proof . G M ORE DISCUSSIONS Recall the discussion in the main text , we showed that the iterate w t + 1 generated by HB satisﬁes (cid:104) w t + 1 , u i (cid:105) = ( 1 + ηλ i ) (cid:104) w t , u i (cid:105) + β ( (cid:104) w t , u i (cid:105) − (cid:104) w t − 1 , u i (cid:105) ) , ( 126 ) which is in the form of dynamics shown in Lemma 1 . Hence , one might be able to show that with the use of the momentum , the growth rate of the projection on the eigenvector u i ( i . e . | (cid:104) w t + 1 , u i (cid:105) | ) is faster as the momentum parameter β increases . Furthermore , the top eigenvector projection | (cid:104) w t + 1 , u 1 (cid:105) | is the one that grows at the fastest rate . As the result , after normalization ( i . e . w T | w T | ) , the normalized solution will converge to the top eigenvector after a few iterations T . However , we know that power iteration or Lanczos method are the standard , specialized , state - of - the - art algorithms for computing the top eigenvector . It is true that HB is outperformed by these methods . But in the next subsection , we will show an implication of the acceleration result , com - pared to vanilla gradient descent , of top eigenvector computations . 35 ( a ) η = 1 × 10 − 2 ( b ) η = 5 × 10 − 3 ( c ) η = 1 × 10 − 3 ( d ) η = 5 × 10 − 4 Figure 3 : Distance to the top leading eigenvector vs . iteration when applying HB for solving min w 12 w (cid:62) Aw . The acceleration effect due to the use of momentum is more evident for small η . Here we construct the matrix A = BB (cid:62) ∈ R 10 × 10 with each entry of B ∈ R 10 × 10 sampled from N ( 0 , 1 ) . G . 1 I MPLICATION : ESCAPE SADDLE POINTS FASTER Recent years there is a growing trend in designing algorithms to quickly ﬁnd a second order sta - tionary point in non - convex optimization ( e . g . Carmon et al . ( 2018 ) ; Agarwal et al . ( 2017 ) ; Allen - Zhu & Li ( 2018 ) ; Xu et al . ( 2018 ) ; Ge et al . ( 2015 ) ; Levy ( 2016 ) ; Fang et al . ( 2019 ) ; Jin et al . ( 2017 ; 2018 ; 2019 ) ; Daneshmand et al . ( 2018 ) ; Staib et al . ( 2019 ) ; Wang et al . ( 2020 ) ) . The com - mon assumptions are that the gradient is L - Lipschitz : (cid:107)∇ f ( x ) − ∇ f ( y ) (cid:107) ≤ L (cid:107) x − y (cid:107) and that the Hessian is ρ - Lipschitz : (cid:107)∇ 2 f ( x ) − ∇ 2 f ( y ) (cid:107) ≤ ρ (cid:107) x − y (cid:107) , while some related works make some additional assumptions . All the related works agree that if the current iterate w t 0 is in the region of strict saddle points , deﬁned as that the gradient is small ( i . e . (cid:107)∇ f ( w t 0 ) (cid:107) ≤ (cid:15) g ) but the least eigenvalue of the Hessian is strictly negative ( i . e . λ min ( ∇ 2 f ( w t 0 ) ) (cid:22) − (cid:15) h I d ) , then the eigenvector corresponding the least eigenvalue λ min ( ∇ 2 f ( w t 0 ) ) is the escape direction . To elaborate , by ρ - Lipschitzness of the Hessian , f ( w t 0 + t ) − f ( w t 0 ) ≤ (cid:104)∇ f ( w t 0 ) , w t 0 + t − w t 0 (cid:105) + 1 2 ( w t 0 + t − w t 0 ) (cid:62) ∇ 2 f ( w t 0 ) ( w t 0 + t − w t 0 ) (cid:124) (cid:123)(cid:122) (cid:125) exhibit negative curvature + ρ 3 (cid:107) w t 0 + t − w t 0 (cid:107) 3 . So if w t 0 + t − w t 0 is in the direction of the bottom eigenvector of ∇ 2 f ( w t 0 ) , then 12 ( w t 0 + t − w t 0 ) (cid:62) ∇ 2 f ( w t 0 ) ( w t 0 + t − w t 0 ) ≤ − c (cid:48) (cid:15) h for some c (cid:48) > 0 . Together with the fact that the gradient is small when in the region of saddle points and the use of a sufﬁciently small step size can guarantee that the function value decreases sufﬁciently ( i . e . f ( w t 0 + t ) − f ( w t 0 ) ≤ − c(cid:15) h for some c > 0 ) . Therefore , many related works design fast algorithms by leveraging the problem structure to quickly compute the bottom eigenvector of the Hessian ( see e . g . Carmon et al . ( 2018 ) ; Agarwal et al . ( 2017 ) ; Allen - Zhu & Li ( 2018 ) ; Xu et al . ( 2018 ) ) . An interesting question is as follows “ If the Heavy Ball algorithm is used directly to solve a non - convex optimization problem , can it escape possible saddle points faster than gradient descent ? ” Before answering the question , let us ﬁrst conduct an experiment to see if the Heavy Ball algorithm can accelerate the process of escaping saddle points . Speciﬁcally , we consider a problem that was consider by Staib et al . ( 2019 ) ; Reddi et al . ( 2018 ) ; Wang et al . ( 2020 ) for the challenge of escaping saddle points . The problem is min w f ( w ) : = 1 n (cid:80) ni = 1 (cid:0) 12 w (cid:62) Hw + x (cid:62) i w + (cid:107) w (cid:107) 1010 (cid:1) ( 127 ) with H : = (cid:20) 1 0 0 − 0 . 1 (cid:21) . where x i ∼ N ( 0 , diag ( [ 0 . 1 , 0 . 001 ] ) ) and the small variance in the second component will provide smaller component of gradient in the escape direction . At the origin , we have that the gradient is small but that the Hessian exhibits a negative curvature . For this problem , Wang et al . ( 2020 ) observe that SGD with momentum escapes the saddle points faster but they make strong assumptions in their analysis . We instead consider the Heavy Ball algorithm ( i . e . Algo - rithm 1 , the deterministic version of SGD with momentum ) . Figure 4 shows the result and we see that the higher the momentum parameter β , the faster the process of escaping the saddle points . We are going to argue that the observation can be explained by our theoretical result that the Heavy Ball algorithm computes the top eigenvector faster than gradient descent . Let us denote h ( w ) : = 1 n (cid:80) ni = 1 (cid:0) x (cid:62) i w + (cid:107) w (cid:107) 10 10 (cid:1) . We can rewrite the objective ( 127 ) as f ( w ) : = 1 2 w (cid:62) Hw + h ( w ) . Then , 36 the Heavy Ball algorithm generates the iterate according to w t + 1 = (cid:20) 1 − η 0 0 1 + η 10 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) : = A w t + β ( w t − w t − 1 ) − η ∇ h ( w t ) . ( 128 ) By setting η ≤ 1 , we have that the top eigenvector of A is u 1 = e 2 , which is the es - cape direction . Now observe the similarity ( if one ignores the term η ∇ h ( w t ) ) between the up - date ( 128 ) and 19 . It suggests that a similar analysis could be used for explaining the es - cape process . In Appendix G . 2 , we provide a detailed analysis of the observation . How - ever , problem ( 127 ) has a ﬁxed Hessian and is a synthetic objective function . For general smooth non - convex optimization , can we have a similar explanation ? Consider applying the Heavy Ball algorithm to min w f ( w ) and suppose that at time t 0 , the iterate is in the re - gion of strict saddle points . We have that w t + 1 + t 0 − w t 0 = (cid:0) I d − η ∇ 2 f ( w t 0 ) (cid:1)(cid:0) w t + t 0 − w t 0 (cid:1) + β (cid:0) ( w t + t 0 − w t 0 ) − ( w t + t 0 − 1 − w t 0 ) (cid:1) + η (cid:0) ∇ 2 f ( w t 0 ) ( w t + t 0 − w t 0 ) − ∇ f ( w t + t 0 ) (cid:124) (cid:123)(cid:122) (cid:125) deviation (cid:1) . Figure 4 : Solving ( 127 ) ( with n = 10 ) by the Heavy Ball al - gorithm with different β . By setting η ≤ 1 L with L being the smoothness constant of the problem , we have that the top eigenvector (cid:0) I d − η ∇ 2 f ( w t 0 ) (cid:1) is the eigenvector that corresponds to the smallest eigenvalue of the Hessian ∇ 2 f ( w t 0 ) , which is an escape direction . Therefore , if the deviation term can be controlled , the dynamics of the Heavy Ball algorithm can be viewed as implicitly and approximately comput - ing the eigenvector , and hence we should expect that higher val - ues of momentum parameter accelerate the process . To control ∇ 2 f ( w t 0 ) ( w t + t 0 − w t 0 ) −∇ f ( w t + t 0 ) , one might want to exploit the ρ - Lipschitzness assumption of the Hessian and might need further mild assumptions , as Du et al . ( 2017 ) provide examples showing that gradient descent can take exponential time T = Ω ( exp ( d ) ) to escape saddles points . Previous work of Wang et al . ( 2020 ) makes strong assumptions to avoid the result of exponential time to escape . On the other hand , Lee et al . ( 2019 ) show that ﬁrst order methods can escape strict saddle points almost surely . So we conjecture that under additional mild conditions , if gradient descent can escape in polynomial time , then using Heavy Ball momentum ( β (cid:54) = 0 ) will accelerate the process of escape . We leave it as a future work . G . 2 E SCAPING SADDLE POINTS OF ( 127 ) Recall the objective is min w f ( w ) : = 1 n (cid:80) ni = 1 (cid:0) 12 w (cid:62) Hw + x (cid:62) i w + (cid:107) w (cid:107) 1010 (cid:1) ( 129 ) with H : = (cid:20) 1 0 0 − 0 . 1 (cid:21) . ( 130 ) where x i ∼ N ( 0 , diag ( [ 0 . 1 , 0 . 001 ] ) ) and the small variance in the second component will provide smaller component of gradient in the escape direction . At the origin , we have that the gradient is small but that the Hessian exhibits a negative curvature . Let us denote h ( w ) : = 1 n (cid:80) ni = 1 (cid:0) x (cid:62) i w + (cid:107) w (cid:107) 1010 (cid:1) . We can rewrite the objective as f ( w ) : = 12 w (cid:62) Hw + h ( w ) . Then , the Heavy Ball algorithm generates the iterate according to w t + 1 = w t − ηHw t + β ( w t − w t − 1 ) − η ∇ h ( w t ) = (cid:20) 1 − η 0 0 1 + 0 . 1 η (cid:21) w t + β ( w t − w t − 1 ) − η ∇ h ( w t ) : = Aw t + β ( w t − w t − 1 ) − η ∇ h ( w t ) , ( 131 ) where the matrix A is deﬁned as A : = (cid:20) 1 − η 0 0 1 + 0 . 1 η (cid:21) . By setting η ≤ 1 , we have that the top eigen - vector of A is u 1 = e 2 = (cid:20) 0 1 (cid:21) , which is the escape direction . In the following , let us 37 denote u 2 : = e 1 = (cid:20) 10 (cid:21) and denote ¯ x = 1 n (cid:80) ni = 1 x i . Since (cid:107) w (cid:107) pp : = (cid:0) (cid:80) di = 1 | w i | p (cid:1) and that ∂ (cid:107) w (cid:107) pp ∂w j : = p | w j | p − 1 ∂ | w j | ∂w j = p | w j | p − 2 w j , we have that ∇ h ( w ) = ¯ x + 10 abs ( w ) 8 ◦ w , ( 132 ) where ◦ denotes element - wise product and abs ( · ) denotes the absolution value of its argument in the element - wise way . By initialization w 0 = w − 1 = 0 , we have that ( w (cid:62) 0 u 1 ) = ( w (cid:62)− 1 u 1 ) = ( w (cid:62) 0 u 2 ) = ( w (cid:62)− 1 u 2 ) = 0 and the dynamics w (cid:62) t + 1 u 2 = ( 1 − η ) w (cid:62) t u 2 + β ( w (cid:62) t u 2 − w (cid:62) t − 1 u 2 ) − ηu (cid:62) 2 ∇ h ( w t ) w (cid:62) t + 1 u 1 = ( 1 + η 10 ) w (cid:62) t u 1 + β ( w (cid:62) t u 1 − w (cid:62) t − 1 u 1 ) − ηu (cid:62) 1 ∇ h ( w t ) , ( 133 ) while we also have the initial condition that w (cid:62) 1 u 2 = − ηu (cid:62) 2 ∇ h ( w 0 ) = − ηu (cid:62) 2 ∇ h ( (cid:20) 00 (cid:21) ) = − ηu (cid:62) 2 ¯ x = − η ¯ x [ 1 ] w (cid:62) 1 u 1 = − ηu (cid:62) 1 ∇ h ( w 0 ) = − ηu (cid:62) 1 ∇ h ( (cid:20) 00 (cid:21) ) = − ηu (cid:62) 1 ¯ x = − η ¯ x [ 2 ] . ( 134 ) That is , w 1 = − η ¯ x . ( 135 ) Using ( 132 ) , we can rewrite ( 133 ) as w (cid:62) t + 1 u 2 = ( 1 − η ) w (cid:62) t u 2 + β ( w (cid:62) t u 2 − w (cid:62) t − 1 u 2 ) − η ¯ x [ 1 ] − 10 η ( w (cid:62) t + 1 u 2 ) 9 w (cid:62) t + 1 u 1 = ( 1 + η 10 ) w (cid:62) t u 1 + β ( w (cid:62) t u 1 − w (cid:62) t − 1 u 1 ) − η ¯ x [ 2 ] − 10 η ( w (cid:62) t + 1 u 1 ) 9 . ( 136 ) Note that we have that ∇ f ( w ) : = Hw + ∇ h ( w ) = Hw + ¯ x + 10 abs ( w ) 8 ◦ w = (cid:20) 1 + 10 | w [ 1 ] | 8 0 0 − 0 . 1 + 10 | w [ 2 ] | 8 (cid:21) w + ¯ x ( 137 ) So the stationary points of the objective function satisfy ( 1 + 10 | w [ 1 ] | 8 ) w [ 1 ] + ¯ x [ 1 ] = 0 ( − 0 . 1 + 10 | w [ 2 ] | 8 ) w [ 2 ] + ¯ x [ 2 ] = 0 ( 138 ) To check if the stationary point is a local minimum , we can use the expression of the Hessian ∇ 2 f ( w ) : = H + ∇ 2 h ( w ) = (cid:20) 1 + 90 | w [ 1 ] | 8 0 0 − 0 . 1 + 90 | w [ 2 ] | 8 (cid:21) . ( 139 ) Therefore , the Hessian at a stationary point is positive semi - deﬁnite as long as | w [ 2 ] | 8 ≥ 1900 , which can be guaranteed by some realizations of ¯ x [ 2 ] according to ( 138 ) . Now let us illustrate why higher momentum β leads to the faster convergence in a high level way , From ( 138 ) , we see that one can specify the stationary point by determining ¯ x [ 1 ] and ¯ x [ 2 ] . Fur - thermore , from ( 139 ) , once the iterate w t satisﬁes | w t [ 2 ] | 8 > 1900 , the iterate enters the locally strongly convex and smooth region , for which the local convergence of the Heavy Ball algorithm is known ( Ghadimi et al . ( 2015 ) ) . W . l . o . g , let us assume that ¯ x [ 2 ] is negative . From ( 134 ) , we have that w (cid:62) 1 u 1 > 0 when ¯ x [ 2 ] is negative . Moreover , from ( 136 ) , if , before the iterate satisﬁes | w t [ 2 ] | 8 > 1900 , we have that 110 ( w (cid:62) t u 1 ) − 10 ( w (cid:62) t u 1 ) 9 − ¯ x [ 2 ] > 0 , then the contribution of the projection on the escape direction ( i . e . w (cid:62) t + 1 u 1 ) due to the momentum term β ( w (cid:62) t u 1 − w (cid:62) t − 1 u 1 ) is positive , which also implies that the larger β , the larger the contribution and hence the faster the growing rate of | w t [ 2 ] | . Now let us check the condition , 110 ( w (cid:62) t u 1 ) − 10 ( w (cid:62) t u 1 ) 9 − ¯ x [ 2 ] > 0 before | w t [ 2 ] | 8 > 1900 . A sufﬁcient condition is that ( 110 − 190 ) ( w (cid:62) t u 1 ) − ¯ x [ 2 ] > 0 , which we immediately see that it is true given that ¯ x [ 2 ] is negative and that w (cid:62) 1 u 1 > 0 and that the magnitude of w (cid:62) t u 1 is increasing . A similar reasoning can be conducted on the other coordinate w t + 1 [ 1 ] : = w (cid:62) t + 1 u 2 . 38 H E MPIRICAL RESULTS H . 1 P HASE RETRIEVAL Figure 5 : Performance of gradient descent with Heavy Ball momentum with different values of β = { 0 , 0 . 3 , 0 . 5 , 0 . 7 , 0 . 9 , 1 . 0 → 0 . 9 } for solving the phase retrieval problem ( 1 ) . The case of β = 0 corresponds to the standard gradient descent . Left : we plot the convergence to the true model w ∗ , deﬁned as min ( (cid:107) w t − w ∗ (cid:107) , (cid:107) w t + w ∗ (cid:107) ) , as the global sign of the objective equation 1 is unrecoverable . Right : we plot the objective value ( 1 ) vs . iteration t . All the lines are obtained by initializing the iterate at the same point w 0 ∼ N ( 0 , I n / ( 10000 n ) ) and using the same step size η = 5 × 10 − 4 . Here we set w ∗ = e 1 and sample x i ∼ N ( 0 , I n ) with dimension n = 10 and number of samples m = 200 . We see that the higher the momentum parameter β , the faster the algorithm enters the linear convergence regime . For the line represented by HB + ( β = 1 . 0 → 0 . 9 ) , it means switching to the use of β = 0 . 9 from β = 1 after some iterations . Below , Algorithm 3 and Algorithm 4 , we show two equivalent presentations of this practice . In our experiment , for the ease of implementation , we let the criteria of the switch be 1 { f ( w 1 ) − f ( w t ) f ( w 1 ) ≥ 0 . 5 } , i . e . if the relative change of objective value compared to the initial value has been increased to 50 % . Algorithm 3 : Switching β = 1 to β < 1 1 : Required : step size η and momentum parameter β ∈ [ 0 , 1 ) . 2 : Init : u = v = w 0 ∈ R d and ˆ β = 1 . 3 : for t = 0 to T do 4 : Update iterate w t + 1 = w t − η ∇ f ( w t ) + ˆ β ( u − v ) . 5 : Update auxiliary iterate u = v 6 : Update auxiliary iterate v = w t . 7 : If { Criteria is met } 8 : ˆ β = β . 9 : u = v = w t + 1 # ( reset momentum ) 10 : end 11 : end for H . 2 C UBIC - REGULARIZED PROBLEM Figure 2 shows empirical results of solving the cubic - regularized problem by Heavy Ball with dif - ferent values of momentum parameter β . Subﬁgure ( a ) shows that larger momentum parameter β results in a faster growth rate of (cid:107) w t (cid:107) , which conﬁrms Lemma 2 and shows that it enters the benign region B faster with larger β . Note that here we have that (cid:107) w ∗ (cid:107) = 1 . It suggests that the norm is non - decreasing during the execution of the algorithm for a wide range of β except very large β For β = 0 . 9 , the norm starts decreasing only after it arises above (cid:107) w ∗ (cid:107) . Subﬁgure ( b ) show that higher β also accelerates the linear convergence , as one can see that the slope of a line that corresponds to a higher β is steeper than that of the lower one ( e . g . compared to β = 0 ) , which veriﬁes Theo - rem 3 . We also observe a very interesting phenomenon : when β is set to a very large value ( e . g . 0 . 9 39 Algorithm 4 : Switching β = 1 to β < 1 . 1 : Required : step size η and momentum parameter β ∈ [ 0 , 1 ) . 2 : Init : w 0 ∈ R d , m − 1 = 0 d , ˆ β = 1 . 3 : for t = 0 to T do 4 : Update momentum m t : = ˆ βm t − 1 + ∇ f ( w t ) . 5 : Update iterate w t + 1 : = w t − ηm t . 6 : If { Criteria is met } 7 : ˆ β = β . 8 : m t = 0 . # ( reset momentum ) 9 : end 10 : end for here ) , the pattern is intrinsically different from the smaller ones . The convergence is not monotone and its behavior ( bump and overshoots when decreasing ) ; furthermore , the norm of (cid:107) w t (cid:107) generated by the high β is larger than (cid:107) w ∗ (cid:107) of the minimizer at some time during the execution of the algo - rithm , which is different from the behavior due to using smaller values of β ( i . e . non - decreasing of the norm until the convergence ) . Our theoretical results cannot explain the behavior of such high β , as such value of β exceeds the upper - threshold required by the theorem . An investigation and understanding of the observation might be needed in the future . Now let us switch to describe the setup of the experiment . We ﬁrst set step size η = 0 . 01 , dimension d = 4 , ρ = (cid:107) w ∗ (cid:107) = (cid:107) A (cid:107) 2 = 1 , γ = 0 . 2 and gap = 5 × 10 − 3 . Then we set A = diag ( [ − γ ; − γ + gap ; a 33 ; a 44 ] ) , where the entries a 33 and a 44 are sampled uniformly random in [ − γ + gap ; (cid:107) A (cid:107) 2 ] . We draw ˜ w = ( A + ρ (cid:107) w ∗ (cid:107) I d ) − ξ θ , where θ ∼ N ( 0 ; I d ) and log 2 ξ is uniform on [ − 1 , 1 ] . We set w ∗ = (cid:107) w ∗ (cid:107) (cid:107) ˜ w (cid:107) ˜ w and b = − ( A + ρ (cid:107) w ∗ (cid:107) I d ) w ∗ . The procedure makes w ∗ the global minimizer of problem instance ( A , b , ρ ) . Patterns shown on this ﬁgure exhibit for other random problem instances as well . 40