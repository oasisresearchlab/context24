Training neural networks using Metropolis Monte Carlo and an adaptive variant Stephen Whitelam 1 , ∗ Viktor Selin 2 , Ian Benlolo 2 , Corneel Casert 3 , and Isaac Tamblyn 2 , 4 † 1 Molecular Foundry , Lawrence Berkeley National Laboratory , 1 Cyclotron Road , Berkeley , CA 94720 , USA 2 Department of Physics , University of Ottawa , ON , K1N 6N5 , Canada 3 Department of Physics and Astronomy , Ghent University , 9000 Ghent , Belgium 4 Vector Institute for Artiﬁcial Intelligence , Toronto , ON M5G 1M1 , Canada We examine the zero - temperature Metropolis Monte Carlo algorithm as a tool for training a neural network by minimizing a loss function . We ﬁnd that , as expected on theoretical grounds and shown empirically by other authors , Metropolis Monte Carlo can train a neural net with an accuracy comparable to that of gradient descent , if not necessarily as quickly . The Metropolis algorithm does not fail automatically when the number of parameters of a neural network is large . It can fail when a neural network’s structure or neuron activations are strongly heterogenous , and we introduce an adaptive Monte Carlo algorithm , aMC , to overcome these limitations . The intrinsic stochasticity and numerical stability of the Monte Carlo method allow aMC to train deep neural networks and recurrent neural networks in which the gradient is too small or too large to allow training by gradient descent . Monte Carlo methods oﬀer a complement to gradient - based methods for training neural networks , allowing access to a distinct set of network architectures and principles . I . INTRODUCTION The Metropolis Monte Carlo algorithm was developed in the 1950s in order to simulate molecular systems [ 1 – 4 ] . The Metropolis algorithm consists of small , random moves of particles , accepted probabilistically . It is , along with other Monte Carlo ( MC ) algorithms , widely used as a tool to equilibrate molecular systems [ 5 ] . Equilibrating a molecular system is similar in key respects to train - ing a neural network : both involve optimizing quantities derived from many degrees of freedom that interact in a nonlinear way . Despite this similarity , the Metropolis algorithm and its variants are not widely used as a tool for training neural networks by minimizing a loss func - tion ( for exceptions , see e . g . Refs . [ 6 – 8 ] ) . Instead , this is usually done by gradient - based algorithms [ 9 , 10 ] , and sometimes by population - based evolutionary or genetic algorithms [ 11 – 13 ] to which Monte Carlo methods are conceptually related . In this paper we address the potential of the zero - temperature Metropolis Monte Carlo algorithm and an adaptive variant thereof as tools for neural - network train - ing [ 14 ] . The class of algorithm we consider consists of taking a neural network of ﬁxed structure , adding ran - dom numbers to all weights and biases simultaneously , and accepting this change if the loss function does not increase . For uncorrelated Gaussian random numbers this procedure is equivalent , for small updates , to nor - malized or clipped gradient descent in the presence of Gaussian white noise [ 15 – 17 ] [ 18 ] , and so its ability to train a neural network should be similar to that of sim - ple gradient descent ( GD ) . We show in Section II A that , for a particular supervised - learning problem , this is the case , a ﬁnding consistent with results presented by other authors [ 6 – 8 ] . ∗ swhitelam @ lbl . gov † isaac . tamblyn @ uottawa . ca It is sometimes stated that the ability of stochastic algorithms to train neural networks diminishes sharply as the number of network parameters increases ( particularly if all network parameters are updated simultaneously ) . However , population - based evolutionary algorithms have been used to train many - parameter networks [ 19 ] , and in Section II B we show that the ability of Metropolis Monte Carlo to train a fully - connected neural network is similar for networks with of order a hundred parameters or of order a million : there is not necessarily a sharp decline of acceptance rate with increasing network size . What does thwart the Metropolis Monte Carlo algo - rithm is network heterogeneity . For instance , if the number of connections entering neurons diﬀers markedly throughout the network ( as is the case for networks with convolutional - and fully - connected layers ) or if the out - puts of neurons in diﬀerent parts of a network diﬀer markedly ( as is the case for very deep networks ) then stochastic weight changes of a ﬁxed scale will saturate neurons in some parts of the network and scarcely ef - fect change in other parts . The result is an inability to train . To address this problem we introduce a set of simple adaptive modiﬁcations of the Metropolis al - gorithm – a momentum - like term , an adaptive step - size scheduler , and a means of enacting heterogenous weight updates – that are borrowed from ideas commonly used with gradient - based methods . The resulting algorithm , which we call adaptive Monte Carlo or aMC , is substan - tially more eﬃcient than the non - adaptive Metropolis al - gorithm in a variety of settings . In Section II B we show , for one particular problem , that the acceptance rate of aMC remains much higher than that of the Metropolis algorithm at low values of loss , and can be made almost insensitive to network width , depth , and size . In Sec - tion II C we show that its momentum - like term speeds the rate at which aMC can learn the high - frequency fea - tures of an objective function , much as adaptive meth - ods such as Adam [ 20 ] can learn high - frequency features faster than regular gradient descent . In Section II D we a r X i v : 2205 . 07408v2 [ c s . L G ] 9 A ug 2022 2 show that the Monte Carlo method can train simple re - current neural networks in the presence of small or large gradients , where gradient - based methods fail . In Sec - tion II E we show aMC can train deep neural networks in which gradients are too small for gradient - based meth - ods to train . In Section II F we comment on the fact that best practices for training nets using Monte Carlo methods await development . We introduce the elements of aMC throughout Section II , and summarize the algo - rithm in Section III . Our conclusion is that the Metropolis Monte Carlo al - gorithm and its adaptive variants such as aMC are viable ways of training neural networks , if less developed and optimized than modern gradient - based methods . In par - ticular , Monte Carlo algorithms can , for small updates , eﬀectively sense the gradient , and they do not fail sim - ply because the number of parameters of a neural net - work becomes large . Monte Carlo algorithms should be considered a complement to gradient - based algorithms because they admit diﬀerent design principles for neu - ral networks . Given a network that permits gradient ﬂow , modern gradient - based algorithms are fast and ef - fective [ 9 , 10 , 21 ] . For large neural nets with tens of mil - lions of parameters we ﬁnd gradient - based methods to be considerably faster than Monte Carlo ( Section II F ) . However , Monte Carlo algorithms free us from the re - quirement of ensuring reliable gradient ﬂow ( and gradi - ents can be unreliable even in diﬀerentiable systems [ 22 ] ) . As a result , we ﬁnd that Monte Carlo methods can train deep neural networks and simple recurrent neural net - works in which gradients are too small ( or too large ) for gradient - based methods to work . There already exist so - lutions to these problems , namely the introduction of skip connections or more elaborate recurrent neural network architectures , but aMC requires neither of these things . One type of solution is architectural , the other algorith - mic , and having both options oﬀers more possibilities than having only one . II . RESULTS A . Metropolis Monte Carlo and its connection to gradient descent We start with the zero - temperature Metropolis Monte Carlo algorithm . The zero - temperature limit is not of - ten used in molecular simulation , but it and its vari - ants are widely used ( and sometimes called random - mutation hill climbing ) for optimizing non - diﬀerentiable systems such as cellular automata [ 23 , 24 ] . Consider a neural network with N parameters ( weights and biases ) x = { x 1 , . . . , x i , . . . , x N } , and associated loss function U ( x ) . If we propose the simultaneous change of each neural - network parameter by a Gaussian random num - ber [ 25 ] , x i → x i + (cid:15) i with (cid:15) i ∼ N ( 0 , σ 2 ) , ( 1 ) ( a ) ( b ) 0 . 85 0 . 9 0 . 95 1 A 10 − 4 10 − 2 100 σ or α GDMC 0 . 85 0 . 9 0 . 95 1 A 10 − 4 10 − 2 100 σ or α 2   , ↵ ( 19 ) depth 64 ( 20 ) depth 128 ( 21 ) 0 0 . 1 U 102 103 104 105 n 0 0 . 5 1 A 102 103 104 105 n 0 . 85 0 . 9 0 . 95 1 A 10 − 4 10 − 2 100 σ or α GDMC 0 . 85 0 . 9 0 . 95 1 A 10 − 4 10 − 2 100 σ or α GDMC C ( 1 ) adaptive ( 2 ) Monte ( 3 ) Carlo ( 4 ) aMC ( 5 ) 4 t 0 / 5 ( 6 ) t 0 ( 7 ) ⌃ ⌃ 0 ( 8 )   ( 9 ) h ( 10 ) ﬁxed ( 11 ) depth ( 12 ) variable ( 13 ) depth ( 14 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 15 ) P ( a ) a ( 16 ) t = 520 ( 17 ) Adam ( 18 ) C 10 5 ( 1 ) adaptive ( 2 ) Monte ( 3 ) Carlo ( 4 ) aMC ( 5 ) 4 t 0 / 5 ( 6 ) t 0 ( 7 ) ⌃ ⌃ 0 ( 8 )   ( 9 ) h ( 10 ) ﬁxed ( 11 ) depth ( 12 ) variable ( 13 ) depth ( 14 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 15 ) P ( a ) a ( 16 ) t = 520 ( 17 ) Adam ( 18 ) FIG . 1 . Comparison of zero - temperature Metropolis Monte Carlo ( MC ) and gradient descent ( GD ) used to train a neu - ral network to minimize the mean - squared error U on the MNIST training set . ( a ) MNIST test - set accuracy C 10 5 af - ter 10 5 epochs of batch learning as a function of MC step size σ ( blue ) or GD learning rate α ( gray ) . ( b ) Training loss U and test - set accuracy C versus epoch n for MC step size σ = 2 × 10 − 3 ( blue ) and GD learning rate α = 4 . 5 × 10 − 2 ( gray ) . and accept the proposal if the loss does not increase , then , when the basic move scale σ is small , the values x i of the neural - net parameters evolve according to the Langevin equation [ 17 ] d x i d n = − σ √ 2 π 1 | ∇ U ( x ) | ∂U ( x ) ∂x i + η i ( n ) . ( 2 ) Here n is training time ( epoch ) , and η is a Gaussian white noise with zero mean and variance (cid:104) η i ( n ) η j ( n (cid:48) ) (cid:105) = ( σ 2 / 2 ) δ ij δ ( n − n (cid:48) ) . That is , small stochastic perturba - tions of a network’s weights and biases , accepted if the loss function does not increase , is equivalent to noisy clipped or normalized gradient descent on the loss func - tion . Given the success of gradient - based training methods , this correspondence shows the potential of the Metropolis algorithm to train neural networks . Consistent with this expectation , we show in Fig . 1 that the zero - temperature Metropolis algorithm can train a neural network . For comparison , we also train the network using simple gra - dient descent , x → x − α ∇ U ( x ) , ( 3 ) where α is the learning rate , U ( x ) the loss function , and ∇ ≡ ( ∂ / ∂x 1 , . . . , ∂ / ∂x N ) the gradient operator with re - spect to the neural - network parameters x . We consider a standard supervised - learning prob - lem , recognizing MNIST images [ 26 , 27 ] using a fully - connected , two - layer neural net . The net has 784 in - put neurons , 16 neurons in each hidden layer , and one output layer of 10 neurons . The hidden neurons have hyperbolic tangent activation functions , and the output neurons comprise a softmax function . The net has in to - tal 13 , 002 parameters [ 28 ] . We did batch learning , with the loss function U being the mean - squared error on the MNIST training set of size 6 × 10 4 ( in the standard way 3 we consider the ground truth for each training example to be a 1 - hot encoding of the class label , and take the 10 outputs of the neural network as its prediction vector ) . Fig . 1 ( a ) shows the classiﬁcation accuracy C 10 5 on the MNIST test set of size 10 4 after 10 5 epochs of training . We show results for MC ( blue ) and GD ( gray ) , for a range of values of step size σ and learning rate α , respec - tively . The initial neural - net parameters for MC simula - tions were x i ∼ N ( 0 , σ 2 ) . The two algorithms behave in a similar manner : each has a range of its single param - eter over which it is eﬀective , and displays a maximum at a particular value of that parameter . The value of the maximum for GD is slightly higher than that for MC ( about 96 % compared to about 95 % ) , and GD achieves near - maximal results over a broader range of its single parameter than does MC . Fig . 1 ( b ) shows loss U and classiﬁcation accuracy C as a function of epoch for two examples from panel ( a ) . GD trains faster with n initially , but results are comparable near the end of the learning process . The learning dy - namics of these algorithms is not the same : in the limit of small steps , the zero - temperature Metropolis algorithm approaches normalized or clipped gradient descent , not standard gradient descent ( and its equivalence to the for - mer would only be seen with an appropriately rescaled horizontal axis ) [ 29 ] . Nonetheless , MC can in eﬀect sense the gradient , as long as the step size is relatively small , and for this problem the range of appropriate step sizes is small compared to the eﬀective GD step size . GD therefore trains faster , but MC has similar capacity for learning . The computational cost per epoch of the two algorithms is of similar order , with MC being cheaper per epoch for batch learning : each MC step requires a for - ward pass through the data , and each GD step a forward and a backward pass . There are many things that could be done to im - prove the learning precision of these algorithms ( no pre - processing of data was done , and a basic neural net was used [ 30 ] ) , but this comparison , given a neural net and a data set , conﬁrms that Metropolis Monte Carlo can achieve results roughly comparable to gradient descent , even on a problem for which gradients are available . For this problem GD trains faster , but MC works . It is worth noting that this conclusion follows from considering a range of step sizes σ : for a single choice of step size it would be possible to conclude that MC doesn’t work at all . Similar ﬁndings have been noted previously : simu - lated annealing on the Metropolis algorithm [ 6 , 7 ] and a variant of zero - temperature Metropolis MC ( applied weight - by - weight ) [ 8 ] were used to train neural nets with an accuracy comparable to that of gradient - based algo - rithms . These results , and the correspondence described in Ref . [ 17 ] establish both theoretically and empirically the ability of Metropolis MC to train neural nets . We next turn to the question of how the eﬃciency of Monte Carlo training scales with net parameters , and how to improve this eﬃciency by introducing adaptivity to the algorithm . B . Metropolis acceptance rate as a function of net size To examine how the eﬃciency of the Metropolis al - gorithm changes with neural - net size and architecture , we consider in this section a supervised - learning prob - lem in which a neural net is trained by zero - temperature Metropolis MC to express the sine function f 0 ( θ ) = sin ( 2 πθ ) on the interval θ ∈ [ 0 , 1 ] . The loss U is the mean - squared error between f 0 ( θ ) and the neural - net output f x ( θ ) , evaluated at 10 3 evenly - spaced points on the in - terval . The neural net has one input neuron , which is fed the value θ , and one output , which returns f x ( θ ) . Its internal structure is fully connected , with hyperbolic tangent nonlinearities . To explore the eﬀect of varying width ( panels ( a ) and ( c ) of Fig . 2 ) we set the depth to 2 and varied the width from 10 to 10 3 , these choices cor - responding to about 10 2 to about 10 6 parameters . To explore the eﬀect of varying depth ( panels ( b ) and ( d ) of Fig . 2 ) we set the width to 25 and varied the depth from 2 to 10 , these choices corresponding to about 10 2 to about 10 4 parameters . In Fig . 2 ( a , left ) we show loss U as a function of Metropolis acceptance rate A for three diﬀerent neural - net widths . The acceptance rate tells us , for ﬁxed step size , the fraction of directions that point downhill in loss . It provides information similar to that shown in plots of the index of the critical points of a loss surface [ 31 , 32 ] , conﬁrming that at large loss there are more downhill di - rections than at small loss . In Fig . 2 ( a , right ) we plot the acceptance rate A ( U 0 ) at ﬁxed loss U 0 as a function of the number of net parameters N ( obtained by taking horizontal cuts across panel ( a ) ; note that more net sizes are shown in panel ( b ) than panel ( a ) ) . The acceptance rate decreases with increasing net size , but relatively slowly . Upon increasing the size of the net by 4 orders of magnitude , the acceptance rate decreases by about 1 order of magnitude . We have indicated an N − 1 scaling as a guide to the eye . In the extreme limit , if N simultaneous parameter updates each had to be indi - vidually productive , the acceptance rate would decrease exponentially with N , which is clearly not the case . The more dramatic decrease in acceptance rate is with loss : at small loss the acceptance rate becomes very small . Similar trends are seen with depth in Fig . 2 ( b ) . The ac - ceptance rate declines sharply with loss . It also declines with the number of parameters , slightly more rapidly than in panel ( a ) but not as rapidly as N − 1 . Empirically , therefore , we do not see evidence of a fun - damental inability of MC to cope with large numbers of parameters . In Section II E we discuss how network het - erogeneity can impair the Metropolis algorithm’s ability to train a network . The solution , as we discuss there , is to introduce an adaptive Monte Carlo ( aMC ) variant of the Metropolis algorithm . To motivate the introduction 4 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N 10 − 4 10 − 2 10 0 A ( U 0 ) 10 2 10 3 10 4 N ( b ) ( a ) 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A w = 10 w = 10 2 w = 10 3 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A d = 2 d = 6 d = 10 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A d = 2 d = 6 d = 10 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 ( c ) ( d ) varying width ( 1 ) 2 t 0 / 5 ( 2 ) 3 t 0 / 5 ( 3 ) 4 t 0 / 5 ( 4 ) t 0 ( 5 ) ⌃ ⌃ 0 ( 6 )   ( 7 ) h ( 8 ) ﬁxed ( 9 ) depth ( 10 ) variable ( 11 ) depth ( 12 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 13 ) P ( a ) a ( 14 ) t = 520 ( 15 ) Adam ( 16 )   or ↵ ( 17 ) depth 64 ( 18 ) varying width ( 1 ) MC ( 2 ) aMC ( 3 ) 4 t 0 / 5 ( 4 ) t 0 ( 5 ) ⌃ ⌃ 0 ( 6 )   ( 7 ) h ( 8 ) ﬁxed ( 9 ) depth ( 10 ) variable ( 11 ) depth ( 12 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 13 ) P ( a ) a ( 14 ) t = 520 ( 15 ) Adam ( 16 )   or ↵ ( 17 ) depth 64 ( 18 ) varying width ( 1 ) MC ( 2 ) aMC ( 3 ) 4 t 0 / 5 ( 4 ) t 0 ( 5 ) ⌃ ⌃ 0 ( 6 )   ( 7 ) h ( 8 ) ﬁxed ( 9 ) depth ( 10 ) variable ( 11 ) depth ( 12 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 13 ) P ( a ) a ( 14 ) t = 520 ( 15 ) Adam ( 16 )   or ↵ ( 17 ) depth 64 ( 18 ) 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 10 − 4 10 − 2 10 0 A ( U 0 ) 10 2 10 3 10 4 N 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 varying depth ( 1 ) adaptive ( 2 ) Monte ( 3 ) Carlo ( 4 ) aMC ( 5 ) 4 t 0 / 5 ( 6 ) t 0 ( 7 ) ⌃ ⌃ 0 ( 8 )   ( 9 ) h ( 10 ) ﬁxed ( 11 ) depth ( 12 ) variable ( 13 ) depth ( 14 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 15 ) P ( a ) a ( 16 ) t = 520 ( 17 ) Adam ( 18 ) 10 − 8 10 − 4 10 0 U 10 − 6 10 − 3 10 0 A w = 10 w = 10 2 w = 10 3 10 − 6 10 − 3 10 0 A ( U 0 ) 10 2 10 4 10 6 N N − 1 U 0 = 5 × 10 − 4 U 0 = 1 × 10 − 4 U 0 = 5 × 10 − 5 FIG . 2 . For a particular neural - network supervised - learning task , the acceptance rate of the Metropolis MC method decreases sharply with loss but not model size . ( a ) Left : Loss U versus acceptance rate A for Metropolis MC for a deep neural net of three diﬀerent widths . Right : Acceptance rate A ( U 0 ) at ﬁxed loss U 0 for models of diﬀerent widths , as a function of the number of model parameters N . ( b ) Analog of ( a ) for models of increasing depth . ( c ) Analog of ( a ) for adaptive Monte Carlo , aMC . ( d ) Analog of ( b ) for aMC . MC step size : σ = 10 − 3 . aMC hyperparameters : ( σ 0 , (cid:15) , n s , signal norm ) = ( 10 − 2 , 10 − 2 , 10 2 , on ) . Each data point is the average of 5 independent simulations . of this algorithm we show in panels ( c ) and ( d ) the aMC analog of panels ( a ) and ( b ) , respectively . The trends experienced by Metropolis have been annulled , the ac - ceptance rates of aMC remaining large and essentially constant with loss or model size over the range of param - eters considered . We now turn to a step - by - step introduction of the el - ements of aMC . C . Adaptivity speeds learning , particularly of high - frequency features Modern gradient - based methods are adaptive , allowing the learning rate for each neural - net parameter to diﬀer and to change as a function of the gradients encountered during training [ 10 ] ( adaptive learning is also used in evo - lutionary algorithms [ 33 – 35 ] ) . We can copy this general idea in a simple way within a zero - temperature Metropo - lis Monte Carlo scheme by changing the proposed move of Eq . ( 1 ) to (cid:15) i ∼ N ( µ i , σ 2 ) . ( 4 ) The parameters µ i , set initially to zero , are updated after every accepted move according to µ i → µ i + (cid:15) ( (cid:15) i − µ i ) , ( 5 ) where (cid:15) is a hyperparameter of the method . This form of adaptation is similar to the inclusion of momentum in a gradient - based scheme : the center µ i of each parameter’s move - proposal distribution shifts in the direction of the last accepted move (cid:15) i , with the aim of increasing the probability of generating moves that will be accepted . In addition , to remove the need for a search over the step - size parameter σ , we introduce a simple adaptive learning - rate schedule by setting σ → 0 . 95 σ ( and µ i = 0 ) after n s consecutive rejected moves . We take the initial step size to be σ = σ 0 . We refer to this adaptive Monte Carlo algorithm as aMC , speciﬁed in Section III . The aMC parameter (cid:15) can be used to inﬂuence the rate of learning . In Fig . 3 we provide a simple illustration of this fact using the two - dimensional Rosenbrock function f ( x , y ) = ( 1 − x ) 2 + 100 ( y − x 2 ) 2 , ( 6 ) often used as a test function for optimization meth - ods [ 36 – 38 ] . The Rosenbrock function has a global min - imum at ( x , y ) = ( 1 , 1 ) that is set within a long valley surrounded by steep slopes on either side . A particle on this function moving under pure gradient descent takes a long time to reach the global minimum because gradi - ents within the valley are small : the particle will quickly reach the valley and then move slowly along the valley ﬂoor [ 36 , 37 ] . Placing a particle at the point ( − 2 , 2 ) , outside the valley , we evolve the position x = ( x , y ) of the particle using aMC . We used the initial step size σ 0 = 10 − 3 and rescaling parameter n s = 100 , and carried out three simulations for three values of (cid:15) . As shown in the ﬁgure , the larger is (cid:15) the more rapidly is the global minimum attained , with the diﬀerence between zero and nonzero epsilon being considerable . This form of adap - tivity has an eﬀect similar to that of momentum with 5 ° 1 . 5 0 . 0 1 . 5 x 0 1 2 y 0 5000 10000 n 10 0 10 ° 12 10 ° 24 f ( x , y ) ≤ = 0 ≤ = 0 . 01 ≤ = 0 . 1 ( a ) ( b ) n ( 1 ) t = 0 ( 2 ) t = T ( 3 ) v 0 ( 4 ) D ( 5 ) t / t 0 = 0 . 12 ( 6 ) t / t 0 = 0 . 2 ( 7 ) adaptive ( 8 ) Monte ( 9 ) Carlo ( 10 ) aMC ( 11 ) 4 t 0 / 5 ( 12 ) t 0 ( 13 ) ⌃ ⌃ 0 ( 14 )   ( 15 ) h ( 16 ) ﬁxed ( 17 ) depth ( 18 ) hidden ( 19 ) depth ( 20 ) x y ( 1 ) t = 0 ( 2 ) t = T ( 3 ) v 0 ( 4 ) D ( 5 ) t / t 0 = 0 . 12 ( 6 ) t / t 0 = 0 . 2 ( 7 ) adaptive ( 8 ) Monte ( 9 ) Carlo ( 10 ) aMC ( 11 ) 4 t 0 / 5 ( 12 ) t 0 ( 13 ) ⌃ ⌃ 0 ( 14 )   ( 15 ) h ( 16 ) ﬁxed ( 17 ) depth ( 18 ) hidden ( 19 ) depth ( 20 ) x y ( 1 ) t = 0 ( 2 ) t = T ( 3 ) v 0 ( 4 ) D ( 5 ) t / t 0 = 0 . 12 ( 6 ) t / t 0 = 0 . 2 ( 7 ) adaptive ( 8 ) Monte ( 9 ) Carlo ( 10 ) aMC ( 11 ) 4 t 0 / 5 ( 12 ) t 0 ( 13 ) ⌃ ⌃ 0 ( 14 )   ( 15 ) h ( 16 ) ﬁxed ( 17 ) depth ( 18 ) hidden ( 19 ) depth ( 20 ) x y ( 1 ) f ( 2 ) t = T ( 3 ) v 0 ( 4 ) D ( 5 ) t / t 0 = 0 . 12 ( 6 ) t / t 0 = 0 . 2 ( 7 ) adaptive ( 8 ) Monte ( 9 ) Carlo ( 10 ) aMC ( 11 ) 4 t 0 / 5 ( 12 ) t 0 ( 13 ) ⌃ ⌃ 0 ( 14 )   ( 15 ) h ( 16 ) ﬁxed ( 17 ) depth ( 18 ) hidden ( 19 ) depth ( 20 ) ° 1 . 5 0 . 0 1 . 5 x 0 1 2 y 0 5000 10000 n 10 0 10 ° 12 10 ° 24 f ( x , y ) ≤ = 0 ≤ = 0 . 01 ≤ = 0 . 1 ° 1 . 5 0 . 0 1 . 5 x 0 1 2 y 0 5000 10000 n 10 0 10 ° 12 10 ° 24 f ( x , y ) ≤ = 0 ≤ = 0 . 01 ≤ = 0 . 1 FIG . 3 . Adaptivity speeds convergence on a test function . We use aMC with three values of (cid:15) to simulate a particle moving on the Rosenbrock function f ( x , y ) , Eq . ( 6 ) . Panel ( a ) shows a 2000 - step simulation for each of three values of (cid:15) ; the global minimum is indicated by the red dot at ( 1 , 1 ) . Panel ( b ) shows the value of the function along each trajectory when continued to n = 10 4 steps . aMC hyperparameters : σ 0 = 10 − 3 , n s = 20 . gradient descent [ 39 ] . Returning to neural - network simulation , the aMC pa - rameter (cid:15) can speed the rate at which a neural net - work can learn high - frequency features , much as adaptive methods do with gradient - based algorithms . The extent to which high - frequency features should be learned varies by application . For instance , if a training set contains high - frequency noise then we may wish to attenuate an algorithm’s ability to learn this noise in order to enhance its ability to generalize . This is the idea expressed in Fig . 2 of Ref . [ 40 ] . Empirical studies show that non - adaptive versions of gradient descent sometimes generalize better than their adaptive counterparts [ 41 ] , in some cases be - cause of the diﬀerent abilities of these things to learn high - frequency features . In Fig . 4 we consider a supervised - learning problem inspired by Fig . 2 of Ref . [ 40 ] . We use gradient - based methods ( GD and Adam ) and aMC to train a neural network to learn the function f 0 ( θ ) = ln ( 1 + 5 θ ) + 1 10 sin ( 20 πθ ) ( 7 ) on θ ∈ [ 0 , 1 ] . This function contains a low - frequency term , the logarithm , and a high - frequency term , the sine . The neural network has one input neuron , which is fed the value θ , one output neuron , which returns f x ( θ ) , and a single hidden layer of 100 neurons with tanh activa - tions . The parameters x of the network were set initially to random values x i ∼ N ( 0 , σ 20 ) . In Fig . 4 ( a ) we show the training loss , the mean - squared diﬀerence U between f 0 ( θ ) and the neural - net output f x ( θ ) , evaluated at 1000 evenly - spaced points 100 10 − 3 10 − 6 U 100 103 106 n GDAdam 100 10 − 3 10 − 6 U 100 103 106 n GDAdam 100 10 − 3 10 − 6 U 100 103 106 n 10 − 2 10 − 4 100 U " 100 103 106 n gradients ( 1 ) no gradients ( 2 ) variable depth ( 3 ) ﬁxed ( 4 ) depth ( 5 ) variable ( 6 ) depth ( 7 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 8 ) P ( a ) a ( 9 ) t = 520 ( 10 ) Adam ( 11 )   or ↵ ( 12 ) depth 64 ( 13 ) depth 128 ( 14 ) 10 − 2 10 − 4 100 U " 100 103 106 n ! < 0 ! = 0 ! > 0 10 − 2 10 − 4 100 U " 100 103 106 n ! < 0 ! = 0 ! > 0 ( a ) ( b ) ( c ) 10 − 2 10 − 4 100 U " 100 103 106 n ! < 0 ! = 0 ! > 0 10 − 2 10 − 4 100 U " 100 103 106 n ! < 0 ! = 0 ! > 0 C 10 5 ( 1 ) adaptive ( 2 ) Monte ( 3 ) Carlo ( 4 ) aMC ( 5 ) 4 t 0 / 5 ( 6 ) t 0 ( 7 ) ⌃ ⌃ 0 ( 8 )   ( 9 ) h ( 10 ) ﬁxed ( 11 ) depth ( 12 ) variable ( 13 ) depth ( 14 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 15 ) P ( a ) a ( 16 ) t = 520 ( 17 ) Adam ( 18 ) C 10 5 ( 1 ) adaptive ( 2 ) Monte ( 3 ) Carlo ( 4 ) aMC ( 5 ) 4 t 0 / 5 ( 6 ) t 0 ( 7 ) ⌃ ⌃ 0 ( 8 )   ( 9 ) h ( 10 ) ﬁxed ( 11 ) depth ( 12 ) variable ( 13 ) depth ( 14 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 15 ) P ( a ) a ( 16 ) t = 520 ( 17 ) Adam ( 18 ) 10 − 2 10 − 4 100 U " 100 103 106 n 100 10 − 3 10 − 6 U 100 103 106 n FIG . 4 . Adaptivity speeds learning of high - frequency features . We train a neural network to learn the function ( 7 ) using two gradient - based methods ( GD and Adam ) and aMC with three values of the parameter (cid:15) . ( a ) Loss U as a function of training time n . ( b ) Pseudo - loss U (cid:48) , which expresses the diﬀerence between the net function and the low - frequency term ( the logarithm ) of ( 7 ) . ( c ) Output of neural nets trained by aMC ( colored lines ) at ﬁxed training time . The thicker and thinner black lines are the function ( 7 ) and its low - frequency term , respectively . GD and Adam learning rate : α = 5 × 10 − 3 ; aMC hyperparameters : ( σ 0 , n s ) = ( 0 . 1 , 50 ) . over the interval . At left we show results produced using GD and Adam , and at right we show results produced us - ing aMC for three values of (cid:15) , one positive ( (cid:15) = 5 × 10 − 3 ) , one negative ( (cid:15) = − 5 × 10 − 3 ) , and zero . Of the gradient - based methods Adam trains faster than GD , while for aMC the training loss U decreases fastest for positive (cid:15) and slowest for negative (cid:15) . ( We did not carry out a systematic search of learning rates in order to compare directly the gradient - based and Monte Carlo methods ; our intent here is to illustrate how adaptivity matters within the two classes of algorithm . ) In Fig . 4 ( b ) we show the pseudo - loss U (cid:48) that expresses the mean - squared diﬀerence between the net function f x ( θ ) and the low - frequency logarithmic term of f 0 . The net is not trained to minimize U (cid:48) , but it so happens dur - ing training that U (cid:48) becomes small as the net ﬁrst learns the low - frequency component of f 0 . Subsequently , U (cid:48) increases as the net also learns the high - frequency com - ponent of f 0 . Of the gradient - based methods Adam learns high - frequency features more rapidly than GD . As it does so , the value of the pseudo - loss U (cid:48) increases . For aMC , the parameter (cid:15) controls the separation of timescales between 6 ( a ) ( b ) 100 10 − 3 10 − 6 U 100 103 106 n GDAdam 0 0 . 05 0 . 1 U 100 102 104 n GDMCaMC ( c ) 10 − 2 10 − 1 100101102 102 103 104 105 n U | ∇ U | 10 − 2 10 − 1 100101102 102 103 104 105 n 0 . 5 0 . 75 1 A 102 103 104 105 n 100 10 − 3 10 − 6 U 100 103 106 n GD Adam 0 0 . 05 0 . 1 U 100 102 104 n GDMCaMC 0 0 . 5 1 U 102 103 104 105 n FIG . 5 . The intrinsic stochasticity and numerical stability of the Monte Carlo method allows it to learn in the presence of numerically small and large gradients . ( a ) Training - set loss U and ( b ) test - set classiﬁcation accuracy C as a function of training epoch n for a simple RNN instructed to distinguish MNIST 0s from 1s . The RNN is presented with each image as an unrolled sequence of length 784 . The gradient - based methods GD ( gray ) and Adam ( blue ) cannot train ; aMC ( green ) trains to an accuracy of 99 . 8 % . ( c ) Size of the gradient ( blue ) associated with the models produced by aMC . The algorithm trains productively in regimes in which | ∇ U | (cid:28) U and | ∇ U | (cid:29) U . The loss ( green ) is reproduced from panel ( a ) . Learning rate for GD and Adam : α = 10 − 3 ; aMC hyperparameters : σ 0 = 10 − 3 , n s = 100 , (cid:15) = 0 . the learning of the low - and high - frequency components of f 0 . If we want to learn f 0 as quickly as possible then positive (cid:15) is the best choice . But if we consider the high - frequency component of f 0 to be noise , and regard U (cid:48) as a measure of the network’s generalization error , then negative (cid:15) is the best choice . Panel ( c ) shows the aMC net functions at a time n such that the net trained using (cid:15) = 0 has begun to learn the high - frequency features of f 0 . At the same time the nets trained using positive and negative (cid:15) have learned these features completely or not at all , respectively . D . Monte Carlo algorithms can train a neural network even when gradients are unreliable Metropolis Monte Carlo and aMC can eﬀectively sense the gradient [ 17 ] , and so can train a neural network to similar levels of accuracy as gradient descent . However , Monte Carlo algorithms can also train a neural network when gradients become unreliable , such as when they vanish or explode . Vanishing gradients can be overcome by the intrin - sic stochasticity of the Monte Carlo method . In the absence of gradients , pure gradient - based methods re - ceive no signal [ 42 , 43 ] . However , the Metropolis Monte Carlo procedure ( 1 ) is equivalent , for vanishing gradi - ents , to the diﬀusive dynamics ˙ x i = ξ i ( n ) , where ξ is a Gaussian white noise with zero mean and variance (cid:104) ξ i ( n ) ξ j ( n (cid:48) ) (cid:105) = σ 2 δ ij δ ( n − n (cid:48) ) . Thus Metropolis MC will , in the absence of gradients , enact diﬀusion in parameter space until nonvanishing gradients are encountered , at which point learning can resume . Monte Carlo algorithms can also cope with exploding gradients . Moves are proposed without reference to the gradient , and so can be made on a landscape for which the gradient varies rapidly or is numerically large . Monte Carlo algorithms are also numerically stable , with moves that would induce large increases in loss being rejected but otherwise not harming the training process . We show in this section that the ability to cope with vanishing and exploding gradients allows aMC to train recurrent neural networks that gradient - based methods cannot . Recurrent neural networks ( RNNs ) are designed to act on sequences , and have been applied to problems involving text , images , speech , and music [ 44 – 47 ] . An RNN possesses a vector known as its hidden state , which acts as a form of memory . This vector is updated each time the RNN views a position in a sequence . The size of the gradient scales in general exponentially with sequence length , and so when the sequence is long , and long - term dependencies exist , the gradient tends to vanish or ex - plode . As a result , it can be diﬃcult to train simple RNNs using ﬁrst - order gradient - based methods in order to learn long - term dependencies in sequence data [ 9 , 48 – 51 ] . One solution to this problem is the introduction of more elaborate and computationally expensive RNN architectures such as long short - term memory [ 42 ] and gated recurrent units [ 52 , 53 ] . These architectures can be more reliably trained by gradient - based methods than can simple RNNs . Here we demonstrate an algorithmic solution to the problem rather than an architectural one , by showing that Monte Carlo methods can train an RNN in circum - stances in which gradient - based methods cannot . We train a simple RNN with tanh activation functions to distinguish the digits of class 0 and 1 in the MNIST data set . We binarized the data , and used a vector of length 2 and a one - hot encoding to represent black or white pix - els . The RNN was shown an unrolled version of each image , a sequence of 784 pixels . The RNN architecture is a two - layer stack in which the hidden state at each site 7 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 100 U 102 104 106 n 10 − 12 10 − 8 10 − 4 100 U 102 104 106 n U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) A 10 5 ( 2 ) aMC ( 3 )   or ↵ ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) A 10 5 ( 2 ) GD ( 3 ) Adam ( 4 )   or ↵ ( 5 ) depth 16 ( 6 ) depth 128 ( 7 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) A 10 5 ( 2 ) GD ( 3 ) Adam ( 4 )   or ↵ ( 5 ) depth 16 ( 6 ) depth 128 ( 7 ) i ( 1 ) variable depth ( 2 ) ﬁxed ( 3 ) depth ( 4 ) variable ( 5 ) depth ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 7 ) P ( a ) a ( 8 ) t = 520 ( 9 ) Adam ( 10 )   or ↵ ( 11 ) depth 64 ( 12 ) depth 128 ( 13 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 8 ( 4 ) depth 32 ( 5 ) depth 64 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) ( a ) ( b ) ( c ) stochasticity of run : 32 , 64 , 128 U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n GDAdamaMCaMC , sn oﬀ 10 − 12 10 − 8 10 − 4 100 U 102 104 106 n i ( 1 ) variable depth ( 2 ) ﬁxed ( 3 ) depth ( 4 ) variable ( 5 ) depth ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 7 ) P ( a ) a ( 8 ) t = 520 ( 9 ) Adam ( 10 )   or ↵ ( 11 ) depth 64 ( 12 ) depth 128 ( 13 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 8 ( 4 ) depth 32 ( 5 ) depth 64 ( 6 ) U ( x ) = K   1 X j [ f x ( j / K )   f 0 ( j / K ) ] 2 ( 1 ) f x ( ✓ ) ( 2 ) aMC ( 3 ) depth 4 ( 4 ) depth 16 ( 5 ) depth 128 ( 6 ) 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n 10 − 12 10 − 8 10 − 4 10 0 U 10 2 10 4 10 6 n FIG . 6 . Intrinsic stochasticity and heterogenous weight - scale updates allow aMC to train deep nets in which the gradient is too small for gradient - based methods to work . ( a ) Loss function U versus epoch n for deep nets trained to express a step function by GD ( gray ) , Adam ( blue ) , and aMC with signal norm oﬀ ( green dashed ) and on ( green ) . The latter is able to train for all depths shown . ( b ) Net outputs after 10 6 epochs ( colored ) against the target function ( black ) for two depths . ( c ) Loss U of 10 independent simulations for each of Adam and aMC for the deeper nets . GD learning rate : α = 10 − 3 ; Adam hyperparameters : ( α , β 1 , β 2 ) = ( 10 − 4 , 0 . 9 , 0 . 999 ) ; aMC hyperparameters : ( σ 0 , (cid:15) , n s ) = ( 10 − 2 , 10 − 2 , 10 2 ) . in the ﬁrst layer is used as input for the second layer . The dimension of the hidden state is set to 64 , and the ﬁnal hidden state is sent into a linear classiﬁer . The loss func - tion is the cross entropy between the neural network’s prediction and the ground truth . In Fig . 5 ( a , b ) we show the results of training the RNN using aMC and gradient - based optimizers . We stochas - tically subsample the training data set for each update ( “mini - batching” ) , using 1500 samples at each step . We used gradient descent and the Adam optimizer , both with learning rate 10 − 3 , with gradient clipping turned on ( this has been shown to solve the exploding - gradient problem for some data sets [ 54 ] ) , and aMC with hyperparameters σ 0 = 10 − 3 , n s = 100 , (cid:15) = 0 ( which is the Metropolis al - gorithm with an adaptive step - size scheduler ) . Gradient descent and Adam fail to train ( a search over learning rates between the values 10 − 5 and 1 did not result in lower loss values ) , while aMC trains to small values of loss and a test - set accuracy of 99 . 8 % . In Fig . 5 ( c ) we show the size of the gradient associated with the models produced by aMC . aMC does not calcu - late or make use of the gradient , but we can nonetheless evaluate it for the models produced by the training pro - cess . Two distinct regimes can be seen , with the size of the gradient changing by about three orders of magnitude between them . The gradient - based algorithms cannot es - cape from the small - gradient regime , and when initiated from the large - gradient regime that is encountered by aMC , in which | ∇ U | (cid:29) U , the gradient - based integra - tors explode . Thus Monte Carlo can train productively in the face of two of the classic obstacles to training by gradient - based methods , small and large gradients . Simple RNNs are known be harder to train by gradi - ent descent than more complex RNNs , but simple RNNs possess similar or greater capacity per parameter than more complex architectures [ 55 ] . Methods that can train simple RNNs may therefore allow more widespread use of those architectures . E . For nets with heterogeneous structures or neuron activations , the weight - update scale must be made heterogenous For some architectures , particularly those with struc - tural heterogeneity or heterogenous neuron activations , it is necessary to scale the Monte Carlo step - size parameter σ for each neural - net parameter individually . In this sec - tion we address this problem using deep neural networks for which , as in Section II D , gradients are too small for gradient - based methods to train . Choosing a set of heterogeneous Monte Carlo step - size parameters can be done by adapting ideas used in the development of gradient - based methods [ 56 ] . Guided by that work we modify the proposal distribution of ( 4 ) to read (cid:15) i ∼ N ( µ i , σ 2 i ) , ( 8 ) 8 where σ i = λ i σ . The λ i are parameters that are either set to unity ( a condition we call “signal norm oﬀ” ) or ac - cording to Eq . ( 18 ) in Section III B ( “signal norm on” ) . The parameters µ i and σ are adjusted as in Section II C . The parameters λ i , which are straightforwardly calcu - lated during a forward pass through the net , ensure that the scale of signal change to each neuron is roughly con - stant . The intent of signal norm is similar to that of layer norm [ 57 ] , except that the latter is an architectural solu - tion – it entails a modiﬁcation of the net , and is present at test time – while the former is an algorithmic solution and plays no role once the neural net has been trained . In Fig . 6 we show the results of neural networks of depth d trained by aMC and by gradient - based methods to express a step function f 0 ( θ ) that is equal to 1 / 2 if 1 / 2 < θ < 3 / 4 and is zero otherwise . The neural nets have one input neuron , which is fed the value θ , and one output neuron , which returns f x ( θ ) . They have 10 neurons in the penultimate hidden layer , and 4 neurons in each of the other d − 1 hidden layers , the intent being to allow very deep nets with relatively few neurons . All neurons have tanh activation functions . In Fig . 6 ( a ) we show loss U as a function of epoch n for four algorithms : GD ( gray ) ; Adam ( blue ) ; and aMC with signal norm oﬀ ( green dotted ) and on ( green ) . For each algorithm we ran 20 independent simulations , 10 using Kaiming initialization [ 58 ] and 10 initialized with Gaus - sian random numbers x i ∼ N ( 0 , σ 20 ) , where σ 0 = 10 − 2 . We plot the simulation having the smallest U after 10 6 epochs . As the depth of the network increases beyond 4 layers , GD and aMC with signal norm oﬀ stop learning on the timescales shown . Above 32 layers , Adam also stops learning on the timescale shown . ( For depth 64 we tried a broad range of learning rates for GD and Adam , from 10 to 10 − 6 , none of which was successful . We also varied the Adam hyperparameters β 1 , β 2 over a small range of values , without success . It may be that hyperparameters that enable training do exist , but we were not able to ﬁnd them . ) aMC with signal norm on continues to learn up to a depth of 128 ( we also veriﬁed that aMC trains nets of depth 256 ) , and so can successfully train deep nets in which gradient - based algorithms receive too little signal to train . In Fig . 6 ( b ) we show net outputs at n = 10 6 epochs for three algorithms and two depths . As discussed in Sec - tion II C , the adaptive algorithms Adam and aMC learn the sharp features of the target function more quickly than does GD . For the deeper net , the gradient - based al - gorithms GD and Adam do not receive suﬃcient signal to train . In Fig . 6 ( c ) we show 10 simulations for each of Adam and aMC for the deeper nets . The outcome of training is stochastic , resembling a nucleation dynamics with an induction time that increases with net depth . For some initial conditions both algorithms fail to train on the al - lotted timescale . In general , the rate of nucleation is higher for aMC than Adam , and remains measurable for all depths shown FIG . 7 . As for gradient - based methods , diﬀerent forms of batching can speed neural - net training by MC . ( a ) As Fig . 1 ( b ) , with the addition of a simulation done us - ing aMC ( green ) with hyperparameters ( σ 0 , (cid:15) , n s , s . n . ) = ( 10 − 2 , 0 , 20 , on ) . ( b ) The aMC result from panel ( a ) , which uses batch learning , compared with aMC using conventional minibatching ( blue ) and progressive batching ( cyan ) . 100 10 − 3 10 − 6 U 100 103 106 n GDAdam 0 0 . 05 0 . 1 U 100 102 104 n GDMCaMC ( a ) ( b ) 100 10 − 3 10 − 6 U 100 103 106 n GDAdam 0 0 . 05 0 . 1 U 100 102 104 n GDMCaMC 10 − 6 10 − 3 100 U 100 102 104 106 n 0 0 . 5 1 C 100 102 104 106 n FIG . 8 . Training - set loss U ( a ) and test - set classiﬁcation accu - racy C ( b ) for MNIST classiﬁcation done using ResNET - 18 , a deep neural network of ≈ 1 . 11 × 10 7 parameters . GD learning rate was 10 − 3 ; aMC hyperparameters were σ 0 = 10 − 3 , n s = 100 , and (cid:15) = 10 − 3 , with signal norm oﬀ . We veriﬁed that introducing skip connections [ 59 ] or layer norm [ 57 ] to the neural nets enabled Adam to train at depth 64 ( and enables aMC without signal norm to train at all depths ) . These architectural modiﬁcations allow gradient - based algorithms to train , but they are not required for Monte Carlo . This comparison illus - trates the fact that design principles for nets trained by Monte Carlo methods diﬀer from those trained by gradient - based methods . F . Best practices for training neural nets using MC await development In this section we ﬁrst revisit Fig . 1 using aMC , and we present data indicating that numerical best practices for training nets using MC may diﬀer from those developed for gradient - based algorithms . 9 In Fig . 7 ( a ) we reproduce Fig . 1 ( b ) with the addition of an aMC simulation ( green ) in which we use aMC’s adaptive step - size attenuation ( with n s = 20 ) and signal norm . These features allow us to choose an initial step - size parameter σ 0 = 10 − 2 larger than the optimum value for Metropolis MC ( see Fig . 1 ( a ) ) . As a result , training proceeds faster than for MC , at a rate comparable to the GD result shown . Training - set loss and test - set accuracy at long times are similar for all three algorithms . In Fig . 7 ( b ) we compare the aMC result of panel ( a ) , which uses batch learning ( green ) , with two additional aMC results . The ﬁrst ( cyan ) uses conventional mini - batching with a minibatch size of 2000 . Minibatch learn - ing proceeds faster than batch learning , as happens with gradient - based methods , but achieves slightly larger val - ues of training - set loss and test - set error than does batch learning . We speculate that this happens because of com - peting sources of stochasticity , that of the minibatch and that intrinsic to the MC algorithm . The second ( cyan ) uses progressive batching : training begins with a mini - batch of size 500 , which doubles every time the clas - siﬁcation error rate on the minibatch falls below 10 % ( aMC moves are conditioned against minibatch loss , in the usual way , not minibatch classiﬁcation error , but the latter is the trigger for the doubling of the minibatch size ) . After doubling the minibatch size the aMC al - gorithm is reset ( σ → σ 0 and µ i → 0 ) . Progressive - batch learning proceeds faster than batch learning , and reaches similar ﬁnal values of training - set loss and test - set accuracy . This comparison suggests that MC may respond diﬀerently than GD to procedures such as mini - batch training ; best practices for MC training of neural networks await development . We note that the noise intrinsic to the Monte Carlo method provides a means of exploration even when the batch identity is kept ﬁxed ( noise also provides a way to eﬀect change in the presence of vanishing gradients ; see Section II E and Section II D ) . For the problem dis - cussed in this section there is a variation of about 1 % in values of test - set accuracy at 10 5 epochs for 30 inde - pendent MC trajectories propagated with the same set of hyperparameters . Such ﬂuctuations could provide the basis for an additional form of importance sampling that identiﬁed and propagated the best - performing networks in a population . Finally , we show in Fig . 8 the analog of Fig . 1 ( b ) for ResNET - 18 . ResNET - 18 is a large , deep neural network with ≈ 1 . 11 × 10 7 parameters [ 59 ] ( we changed the num - ber of input channels for the ﬁrst layer in order to ap - ply it to MNIST ) . We applied layer normalization [ 57 ] , which homogenizes neuron inputs and removes ( or re - duces ) the need for signal norm when using aMC . The gradient descent learning rate was set to 10 − 3 and we applied gradient clipping for | ∇ U | > 1 ; the aMC hyper - parameters were σ 0 = 10 − 3 , n s = 100 , and (cid:15) = 10 − 3 . Two points are apparent from the plot : gradient descent trains faster than aMC , but aMC has similar ability to train in the long - time limit : both GD and aMC train to small loss and about 99 . 2 % accuracy on the MNIST test set . Thus for large modern architectures such as ResNET - 18 we ﬁnd training with gradients faster than training by Monte Carlo , but the latter has similar ca - pacity for learning , suggesting that it is a promising tool for training neural networks when gradients are unreli - able : see Section II D and Section II E . III . SUMMARY OF AMC A . An adaptive version of the Metropolis algorithm for training neural networks In this section we summarize aMC , the adaptive Monte Carlo algorithm used in this paper . It is based on the Metropolis MC algorithm , modiﬁed to allow the move - proposal distribution to adapt in response to accepted and rejected moves . The Metropolis acceptance crite - rion is min ( 1 , e − ∆ U / T ) , where ∆ U is the change of loss and T is temperature . For nonzero temperature the al - gorithm allows moves uphill in loss . We focus here on the limit of zero temperature , which allows no uphill moves in loss . This choice is motivated by the success of gradient - descent algorithms and the intuition in deep learning ( suggested by the structure of high - dimensional Gaussian random surfaces ) that at large loss most sta - tionary points on the loss surface are saddle points that can be escaped by moving downhill [ 31 , 32 ] . aMC is speciﬁed by four hyperparameters : σ 0 , the ini - tial move scale ; (cid:15) , the rate at which the mean of the move - proposal distribution is modiﬁed ; n s , the number of consecutive rejected moves allowed before rescaling the parameters of the move - proposal distribution ; and by the choice of signal norm being on or oﬀ . We introduce a counter n cr = 0 to record the number of consecutive rejected moves . We initialize the parame - ters ( weights and biases ) x = { x 1 , . . . , x i , . . . , x N } of the neural network ( e . g . using Gaussian random numbers x i ∼ N ( 0 , σ 20 ) ) , and set the centers µ i of each parame - ter’s move - proposal distribution to zero . aMC proceeds as follows . 1 . Current state . Record the current neural - network parameter set x . Select the data ( deﬁning the batch , episode , etc . ) and record the current value of the loss U ( x ) on the data ( for batch learning the value U ( x ) is known from the previous step of the algorithm ) . If signal norm is on , calculate the val - ues λ i speciﬁed by Eq . ( 18 ) , the required quantities having been calculated in the course of calculating U ( x ) . 2 . Proposed move . Propose a change x i → x (cid:48) i = x i + (cid:15) i with (cid:15) i ∼ N ( µ i , σ 2 i ) ( 9 ) of each neural - network parameter i , where σ i = λ i σ . Initially , µ i = 0 and σ = σ 0 , where σ 0 is the 10 initial move scale . The parameters λ i are set either to unity ( “signal norm oﬀ” ) or by Eq . ( 18 ) ( “signal norm on” ) . Evaluate the loss U ( x (cid:48) ) at the set of coordinates x (cid:48) resulting from the proposal ( 9 ) . If U ( x (cid:48) ) ≤ U ( x ) [ 60 ] then we accept the move and go to Step 3 . Otherwise we reject the move and go to Step 4 . 3 . Accept move . Make the proposed coordinates x (cid:48) the current coordinates x . Set n cr = 0 . For each neural - network parameter i , set µ i → µ i + (cid:15) ( (cid:15) i − µ i ) ( 10 ) using the values (cid:15) i calculated in ( 9 ) . Return to Step 1 . 4 . Reject move . Retain the set of coordinates x recorded in Step 1 . Set n cr → n cr + 1 . If n cr = n s then set n cr = 0 , σ → 0 . 95 σ , and ( for all i ) µ i = 0 . Return to Step 1 . The computational cost of one move is the cost to draw N Gaussian random numbers and to calculate the loss function twice ( once for batch learning ) . The memory cost is the cost to hold two versions of the model in memory , and ( if (cid:15) (cid:54) = 0 and signal norm is on ) the val - ues µ i and λ i for each neural - net parameter . Note that the algorithm requires calculation of the loss U ( x ) only , and not of gradients of the loss with respect to the net parameters . We refer to this algorithm as aMC , for adaptive Monte Carlo ( the term “adaptive Metropolis algorithm” has been used in a diﬀerent context [ 61 ] ) . Standard zero - temperature Metropolis Monte Carlo is recovered in the limit (cid:15) = 0 , n s = ∞ , and λ i = 1 . B . Signal norm : enacting heterogenous weight updates in order to keep roughly constant the change of neuron inputs The proposal step ( 9 ) contains the parameter step size σ i = λ i σ . For some applications , particularly involving deep or heterogeneous networks , it is useful to choose the λ i in order to keep the scale of updates for each neuron approximately equal , following ideas applied to gradient - based methods [ 56 ] . We call this concept signal norm ; when signal norm is oﬀ , all λ i = 1 . When it is on , we proceed as follows . Consider the class of neural networks for which the input to neuron j ( its pre - activation ) is I αj = N j (cid:88) i → j x i S αi , ( 11 ) where the sum runs over all weights x i feeding into neu - ron j ; N j is the fan - in of j ( the number of connections entering j ) ; and S αi is the output of neuron i ( the neu - ron that the weight x i connects to neuron j ) given one particular evaluation α of the neural network . Under the proposal ( 9 ) the change of input to neuron j is approxi - mately [ 62 ] ∆ αj = N j (cid:88) i → j (cid:15) i S αi . ( 12 ) We therefore have (cid:104) ∆ αj (cid:105) = N j (cid:88) i → j µ i S αi ( 13 ) and (cid:104) ( ∆ αj ) 2 (cid:105) = N j (cid:88) i → j N j (cid:88) k → j (cid:2) µ i µ k ( 1 − δ ik ) + ( σ 2 i + µ 2 i ) δ ik (cid:3) S αi S αk , ( 14 ) where (cid:104)·(cid:105) is the expectation over the move - proposal dis - tribution ( 9 ) , and δ ik is the Kronecker delta . The ex - pected approximate variance of the change of input to neuron j under the move ( 9 ) is therefore (cid:104) ( ∆ αj ) 2 (cid:105) − (cid:104) ∆ αj (cid:105) 2 = σ 2 N j (cid:88) i → j λ 2 i ( S αi ) 2 . ( 15 ) This quantity , averaged over all N data neural - net calls required to calculate the loss , is [ (cid:104) ( ∆ αj ) 2 (cid:105) − (cid:104) ∆ αj (cid:105) 2 ] data = σ 2 N − 1 data N data (cid:88) α = 1 N j (cid:88) i → j λ 2 i ( S αi ) 2 . ( 16 ) We can choose the values of the λ i in order to ensure that the right - hand side of ( 16 ) is always σ 2 . A simple way to do so is to set equal the λ i for all weights x i feeding neuron j , in which case λ i =   N − 1 data N data (cid:88) α = 1 N j (cid:88) i (cid:48) → j ( S αi (cid:48) ) 2   − 1 / 2 . ( 17 ) If all neuron outputs S αi (cid:48) appearing in ( 17 ) vanish identi - cally then the expression must be regularized ; one option is to set λ i = 0 for weights feeding a neuron whose input neurons are zero for a given pass through the data . Recall that the sum α runs over the input data ; the sum i (cid:48) → j runs over all neurons i (cid:48) whose connections feed j ; N j is the fan - in of j ( the number of connections entering j ) ; and S αi (cid:48) is the output of neuron i (cid:48) given a particular eval - uation α of the neural network . The values ( 17 ) can be calculated from the pass through the data immediately before the proposed move . Under ( 17 ) , weights on connections that feed into a neuron receiving many other connections will experience a smaller basic move scale than weights on connections that feed into a neuron receiving few connections . Sim - ilarly , weights on connections fed by active neurons will 11 experience a smaller basic move scale than weights on connections fed by relatively inactive neurons . Finally , if the parameter x i is a bias we choose λ i = 1 . To summarize , we consider two settings for the param - eters λ i that set the step - size parameters σ i = λ i σ in the proposal ( 9 ) . The ﬁrst setting , “signal norm oﬀ” ( used in Fig . 1 , Fig . 2 ( a , b ) , Fig . 4 , and Fig . 5 ) , has λ i = 1 for all parameters x i . The second setting , called “signal norm on” ( used in Fig . 2 ( c , d ) , Fig . 6 , and Fig . 7 ) , has λ i = (cid:40) 1 if x i is a bias ; Eq . ( 17 ) if x i is a weight into neuron j . ( 18 ) IV . CONCLUSIONS We have examined the Metropolis Monte Carlo algo - rithm as a tool for training neural networks , and have introduced aMC , an adaptive variant of it . Monte Carlo methods are closely related to evolutionary algorithms , which are used to train neural networks [ 11 – 13 , 19 ] , but the latter are usually applied to populations of neural networks ; the MC algorithms we have considered here are applied to populations of size 1 , just as gradient descent is . For suﬃciently small moves the Metropolis algorithm is eﬀectively gradient descent in the presence of white noise [ 17 ] . Thus on theoretical grounds the Metropolis algorithm should possess the ability to train a neural net - work to values of a loss function similar to those achieved by GD ; this is indeed what we ( and others [ 6 – 8 ] ) have observed empirically , both for simple neural nets and for large , modern architectures . This correspondence does not guarantee similar training times , however , and we have found gradient - based methods to be faster in gen - eral , particularly for large and heterogenous neural nets . aMC is an adaptive version of the Metropolis algo - rithm . The eﬃciency of aMC diminishes less quickly with decreasing loss and increasing net size than does the ef - ﬁciency of the Metropolis algorithm , and aMC can train faster than Metropolis , much as adaptive gradient - based methods can train faster than pure gradient descent . The Metropolis algorithm and aMC oﬀer a complement to gradient - based methods in that they can sense the gra - dient when it exists but can work without it . In particu - lar , aMC can train nets in which the gradient is too small ( or too large ) to allow gradient - based methods to train on the timescales simulated . We have shown here that aMC can train deep neural networks and recurrent neu - ral networks that gradient descent cannot train . In both cases there exist modiﬁcations to those networks that can be trained by gradient - based methods , but aMC does not require those modiﬁcations . The design principles of neu - ral nets optimal for Monte Carlo algorithms are largely unexplored but are likely distinct from those optimal for gradient - based methods , and having both sets of algo - rithms oﬀers more choices for net design than having only one . Finally , we note that while Metropolis and aMC have a fundamental connection to gradient - based methods in the limit of small step size , Monte Carlo algorithms more generally can enact large - scale nonlocal or collective changes that cannot be made by integrating gradient - based equations of motion [ 5 , 63 – 67 ] . The analogy sug - gests that improved Monte Carlo algorithms for training neural networks await development . V . CODE AVAILABILITY Calculations using gradient descent and Adam were done in PyTorch [ 58 ] . Monte Carlo calculations were done in C and in PyTorch . A PyTorch implementation of the aMC optimizer ( with signal norm oﬀ ) and an example RNN optimization are available here [ 68 ] . VI . ACKNOWLEDGMENTS This work was performed as part of a user project at the Molecular Foundry , Lawrence Berkeley National Lab - oratory , supported by the Oﬃce of Science , Oﬃce of Ba - sic Energy Sciences , of the U . S . Department of Energy under Contract No . DE - AC02 – 05CH11231 . This work used resources of the National Energy Research Scien - tiﬁc Computing Center ( NERSC ) , a U . S . Department of Energy Oﬃce of Science User Facility operated un - der Contract No . DE - AC02 - 05CH11231 . I . T . acknowl - edges funding from the National Science and Engineering Council of Canada . C . C . acknowledges a mobility grant from Research Foundation – Flanders ( FWO ) . [ 1 ] Nicholas Metropolis , Arianna W Rosenbluth , Marshall N Rosenbluth , Augusta H Teller , and Edward Teller , “Equation of state calculations by fast computing ma - chines , ” The Journal of Chemical Physics 21 , 1087 – 1092 ( 1953 ) . [ 2 ] James E Gubernatis , “Marshall Rosenbluth and the Metropolis algorithm , ” Physics of plasmas 12 , 057303 ( 2005 ) . [ 3 ] Marshall N Rosenbluth , “Genesis of the Monte Carlo al - gorithm for statistical mechanics , ” in AIP Conference Proceedings , Vol . 690 ( American Institute of Physics , 2003 ) pp . 22 – 30 . [ 4 ] Madeline Helene Whitacre , Arianna Wright Rosenbluth , Tech . Rep . ( Los Alamos National Lab . ( LANL ) , Los Alamos , NM ( United States ) , 2021 ) . [ 5 ] Daan Frenkel and Berend Smit , Understanding molecu - lar simulation : from algorithms to applications , Vol . 1 12 ( Academic Press , 2001 ) . [ 6 ] Randall S Sexton , Robert E Dorsey , and John D John - son , “Beyond backpropagation : using simulated anneal - ing for training neural networks , ” Journal of Organi - zational and End User Computing ( JOEUC ) 11 , 3 – 10 ( 1999 ) . [ 7 ] LM Rasdi Rere , Mohamad Ivan Fanany , and Aniati Murni Arymurthy , “Simulated annealing algo - rithm for deep learning , ” Procedia Computer Science 72 , 137 – 144 ( 2015 ) . [ 8 ] Rohun Tripathi and Bharat Singh , “Rso : A gradient free sampling based approach for training deep neural net - works , ” arXiv preprint arXiv : 2005 . 05955 ( 2020 ) . [ 9 ] J¨urgen Schmidhuber , “Deep learning in neural networks : An overview , ” Neural networks 61 , 85 – 117 ( 2015 ) . [ 10 ] Ian Goodfellow , Yoshua Bengio , and Aaron Courville , Deep learning ( MIT press , 2016 ) . [ 11 ] John H Holland , “Genetic algorithms , ” Scientiﬁc ameri - can 267 , 66 – 73 ( 1992 ) . [ 12 ] David B Fogel and Lauren C Stayton , “On the eﬀective - ness of crossover in simulated evolutionary optimization , ” BioSystems 32 , 171 – 182 ( 1994 ) . [ 13 ] David J Montana and Lawrence Davis , “Training feed - forward neural networks using genetic algorithms . ” in IJ - CAI , Vol . 89 ( 1989 ) pp . 762 – 767 . [ 14 ] Zero temperature means that moves that increase the loss are not accepted . This choice is motivated by the empirical success in machine learning of gradient - descent methods , and by the intuition , derived from Gaussian random surfaces , that loss surfaces possess more downhill directions at large values of the loss [ 31 , 32 ] . [ 15 ] K Kikuchi , M Yoshida , T Maekawa , and H Watanabe , “Metropolis Monte Carlo method as a numerical tech - nique to solve the Fokker - Planck equation , ” Chemical Physics Letters 185 , 335 – 338 ( 1991 ) . [ 16 ] K Kikuchi , M Yoshida , T Maekawa , and H Watanabe , “Metropolis Monte Carlo method for Brownian dynamics simulation generalized to include hydrodynamic interac - tions , ” Chemical Physics letters 196 , 57 – 61 ( 1992 ) . [ 17 ] Stephen Whitelam , Viktor Selin , Sang - Won Park , and Isaac Tamblyn , “Correspondence between neuroevolu - tion and gradient descent , ” Nature Communications 12 , 1 – 10 ( 2021 ) . [ 18 ] Note that algorithms of this nature do not constitute ran - dom search . The proposal step is random ( related con - ceptually to the idea of weight guessing , a method used in the presence of vanishing gradients [ 42 ] ) but the ac - ceptance criterion is a form of importance sampling , and leads to a dynamics equivalent to noisy gradient descent . [ 19 ] Tim Salimans , Jonathan Ho , Xi Chen , Szymon Sidor , and Ilya Sutskever , “Evolution strategies as a scalable alternative to reinforcement learning , ” arXiv preprint arXiv : 1703 . 03864 ( 2017 ) . [ 20 ] Diederik P Kingma and Jimmy Ba , “Adam : A method for stochastic optimization , ” arXiv preprint arXiv : 1412 . 6980 ( 2014 ) . [ 21 ] Yann LeCun , Yoshua Bengio , and Geoﬀrey Hinton , “Deep learning , ” nature 521 , 436 – 444 ( 2015 ) . [ 22 ] Luke Metz , C Daniel Freeman , Samuel S Schoenholz , and Tal Kachman , “Gradients are not all you need , ” arXiv preprint arXiv : 2111 . 05803 ( 2021 ) . [ 23 ] Melanie Mitchell , John Holland , and Stephanie Forrest , “When will a genetic algorithm outperform hill climb - ing ? ” Advances in neural information processing systems 6 ( 1993 ) . [ 24 ] Melanie Mitchell , An introduction to genetic algorithms ( MIT press , 1998 ) . [ 25 ] In Metropolis Monte Carlo simulations of molecular sys - tems it is usual to propose moves of one particle at a time . If we consider neural - net parameters to be akin to parti - cle coordinates then the analog would be to make changes to one neural - net parameter at a time ; see e . g . Ref . [ 8 ] . However , there is no formal mapping between particles and a neural network , and we could equally well consider the neural - net parameters to be akin to the coordinates of a single particle , in a high - dimensional space , in an external potential equal to the loss function . In the lat - ter case the analog would be to propose a change of all neural - net parameters simultaneously , as we do here . [ 26 ] http : / / yann . lecun . com / exdb / mnist / . [ 27 ] Yann LeCun , L´eon Bottou , Yoshua Bengio , Patrick Haﬀner , et al . , “Gradient - based learning applied to doc - ument recognition , ” Proceedings of the IEEE 86 , 2278 – 2324 ( 1998 ) . [ 28 ] https : / / www . youtube . com / 3blue1brown . [ 29 ] We have also found the GD - MC equivalence to break down in other circumstances : for certain learning rates α , the discrete - update equation ( 3 ) sometimes results in moves uphill in loss , in which case the discrete update is not equivalent to the equation ˙ x = − ( α / ∆ t ) ∇ U ( x ) , while the latter is equivalent to the small - step - size limit of the ﬁnite - temperature Metropolis algorithm [ 15 – 17 ] . [ 30 ] In Fig . 8 we show that GD and MC can both train a large modern neural network to a classiﬁcation accuracy in excess of 99 % on the same problem . [ 31 ] Yann N Dauphin , Razvan Pascanu , Caglar Gulcehre , Kyunghyun Cho , Surya Ganguli , and Yoshua Bengio , “Identifying and attacking the saddle point problem in high - dimensional non - convex optimization , ” Advances in neural information processing systems 27 ( 2014 ) . [ 32 ] Yasaman Bahri , Jonathan Kadmon , Jeﬀrey Pennington , Sam S Schoenholz , Jascha Sohl - Dickstein , and Surya Ganguli , “Statistical mechanics of deep learning , ” An - nual Review of Condensed Matter Physics ( 2020 ) . [ 33 ] Nikolaus Hansen and Andreas Ostermeier , “Completely derandomized self - adaptation in evolution strategies , ” Evolutionary computation 9 , 159 – 195 ( 2001 ) . [ 34 ] Nikolaus Hansen , Sibylle D M¨uller , and Petros Koumoutsakos , “Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation ( cma - es ) , ” Evolutionary computation 11 , 1 – 18 ( 2003 ) . [ 35 ] Nikolaus Hansen , “The cma evolution strategy : a com - paring review , ” Towards A New Evolutionary Computa - tion , 75 – 102 ( 2006 ) . [ 36 ] HoHo Rosenbrock , “An automatic method for ﬁnding the greatest or least value of a function , ” The computer jour - nal 3 , 175 – 184 ( 1960 ) . [ 37 ] Yun - Wei Shang and Yu - Huang Qiu , “A note on the ex - tended rosenbrock function , ” Evolutionary Computation 14 , 119 – 126 ( 2006 ) . [ 38 ] Iyanuoluwa Emiola and Robson Adem , “Comparison of minimization methods for rosenbrock functions , ” in 2021 29th Mediterranean Conference on Control and Automa - tion ( MED ) ( IEEE , 2021 ) pp . 837 – 842 . [ 39 ] Gabriel Goh , “Why momentum really works , ” Distill 2 , e6 ( 2017 ) . 13 [ 40 ] David E Rumelhart , Richard Durbin , Richard Golden , and Yves Chauvin , “Backpropagation : The basic the - ory , ” Backpropagation : Theory , architectures and appli - cations , 1 – 34 ( 1995 ) . [ 41 ] Jinghui Chen , Dongruo Zhou , Yiqi Tang , Ziyan Yang , Yuan Cao , and Quanquan Gu , “Closing the general - ization gap of adaptive gradient methods in training deep neural networks , ” arXiv preprint arXiv : 1806 . 06763 ( 2018 ) . [ 42 ] Sepp Hochreiter and J¨urgen Schmidhuber , “Long short - term memory , ” Neural computation 9 , 1735 – 1780 ( 1997 ) . [ 43 ] Sepp Hochreiter , “The vanishing gradient problem dur - ing learning recurrent neural nets and problem solu - tions , ” International Journal of Uncertainty , Fuzziness and Knowledge - Based Systems 6 , 107 – 116 ( 1998 ) . [ 44 ] Larry R Medsker and LC Jain , “Recurrent neural net - works , ” Design and Applications 5 , 64 – 67 ( 2001 ) . [ 45 ] Alex Graves , Abdel - rahman Mohamed , and Geoﬀrey Hinton , “Speech recognition with deep recurrent neu - ral networks , ” in 2013 IEEE international conference on acoustics , speech and signal processing ( Ieee , 2013 ) pp . 6645 – 6649 . [ 46 ] Ilya Sutskever , Oriol Vinyals , and Quoc V Le , “Sequence to sequence learning with neural networks , ” Advances in neural information processing systems 27 ( 2014 ) . [ 47 ] Alex Graves and J¨urgen Schmidhuber , “Oﬄine hand - writing recognition with multidimensional recurrent neu - ral networks , ” Advances in neural information processing systems 21 ( 2008 ) . [ 48 ] Daan Wierstra , Alexander F¨orster , Jan Peters , and J¨urgen Schmidhuber , “Recurrent policy gradients , ” Logic Journal of the IGPL 18 , 620 – 634 ( 2010 ) . [ 49 ] Yoshua Bengio , Patrice Simard , and Paolo Frasconi , “Learning long - term dependencies with gradient descent is diﬃcult , ” IEEE transactions on neural networks 5 , 157 – 166 ( 1994 ) . [ 50 ] James Martens and Ilya Sutskever , “Learning recur - rent neural networks with hessian - free optimization , ” in ICML ( 2011 ) . [ 51 ] Yoshua Bengio , Nicolas Boulanger - Lewandowski , and Razvan Pascanu , “Advances in optimizing recurrent net - works , ” in 2013 IEEE international conference on acous - tics , speech and signal processing ( IEEE , 2013 ) pp . 8624 – 8628 . [ 52 ] Kyunghyun Cho , Bart Van Merri¨enboer , Caglar Gul - cehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio , “Learning phrase rep - resentations using rnn encoder - decoder for statistical machine translation , ” arXiv preprint arXiv : 1406 . 1078 ( 2014 ) . [ 53 ] Sekitoshi Kanai , Yasuhiro Fujiwara , and Sotetsu Iwa - mura , “Preventing gradient explosions in gated recurrent units , ” Advances in neural information processing sys - tems 30 ( 2017 ) . [ 54 ] Razvan Pascanu , Tomas Mikolov , and Yoshua Bengio , “On the diﬃculty of training recurrent neural networks , ” in International conference on machine learning ( PMLR , 2013 ) pp . 1310 – 1318 . [ 55 ] Jasmine Collins , Jascha Sohl - Dickstein , and David Sus - sillo , “Capacity and trainability in recurrent neural net - works , ” arXiv preprint arXiv : 1611 . 09913 ( 2016 ) . [ 56 ] Yann LeCun , L´eon Bottou , Genevieve B Orr , and Klaus - Robert M¨uller , “Eﬃicient backprop , ” in Neural Net - works : Tricks of the Trade ( 1996 ) . [ 57 ] Jimmy Lei Ba , Jamie Ryan Kiros , and Geof - frey E Hinton , “Layer normalization , ” arXiv preprint arXiv : 1607 . 06450 ( 2016 ) . [ 58 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zem - ing Lin , Natalia Gimelshein , Luca Antiga , et al . , “Py - torch : An imperative style , high - performance deep learn - ing library , ” Advances in neural information processing systems 32 ( 2019 ) . [ 59 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun , “Deep residual learning for image recognition , ” in Proceedings of the IEEE conference on computer vision and pattern recognition ( 2016 ) pp . 770 – 778 . [ 60 ] For ﬁnite temperature T the move is accepted if ξ < e ( U ( x ) − U ( x (cid:48) ) ) / T , where ξ is a random number drawn uni - formly on ( 0 , 1 ] . [ 61 ] Jeﬀrey S Rosenthal et al . , “Optimal proposal distribu - tions and adaptive MCMC , ” Handbook of Markov Chain Monte Carlo 4 ( 2011 ) . [ 62 ] This approximation assumes that the output neurons do not change under the move . This is not true , but the intent here is to set the basic move scale , and absolute precision is not necessary . [ 63 ] Robert H Swendsen and Jian - Sheng Wang , “Nonuniver - sal critical dynamics in monte carlo simulations , ” Physi - cal review letters 58 , 86 ( 1987 ) . [ 64 ] Ulli Wolﬀ , “Collective Monte Carlo updating for spin sys - tems , ” Physical Review Letters 62 , 361 ( 1989 ) . [ 65 ] Bin Chen and J Ilja Siepmann , “Improving the eﬃciency of the aggregation - volume - bias Monte Carlo algorithm , ” The Journal of Physical Chemistry B 105 , 11275 – 11282 ( 2001 ) . [ 66 ] Jiwen Liu and Erik Luijten , “Rejection - free geometric cluster algorithm for complex ﬂuids , ” Physical Review Letters 92 , 035504 ( 2004 ) . [ 67 ] Stephen Whitelam and Phillip L Geissler , “Avoiding unphysical kinetic traps in Monte Carlo simulations of strongly attractive particles , ” The Journal of Chemical Physics 127 , 154101 ( 2007 ) . [ 68 ] https : / / github . com / reproducible - science / aMC .