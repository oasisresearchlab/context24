1 2 P OPULATION D ESCENT : A NATURAL - SELECTION BASED HYPER - PARAMETER TUNING FRAMEWORK Abhinav Pomalapally ∗ 1 , Bassel Mabsout ∗ 2 , Renato Mancuso 2 A BSTRACT First - order gradient descent has been the base of the most successful optimization algorithms ever implemented . On supervised learning problems with very high dimensionality , such as neural network optimization , it is almost always the algo - rithm of choice , mainly due to its memory and computational efficiency . However , it is a classical result in optimization that gradient descent converges to local min - ima on non - convex functions . Even more importantly , in certain high - dimensional cases , escaping the plateaus of large saddle points becomes intractable . On the other hand , black - box optimization methods are not sensitive to the local struc - ture of a loss function’s landscape but suffer the curse of dimensionality . Instead , memetic algorithms aim to combine the benefits of both . Inspired by this , we present Population Descent , a memetic algorithm focused on hyperparameter op - timization . We show that an adaptive m - elitist selection approach combined with a normalized - fitness - based randomization scheme outperforms more complex state - of - the - art algorithms by up to 13 % on common benchmark tasks . 1 I NTRODUCTION Today’s machine learning methods almost entirely rely on Gradient Descent as a core optimization technique . Many recent deep learning tasks , whether it is supervised learning , or unsupervised , include Neural Network ( NN ) optimization with ever - growing parameter spaces . State - of - the - art Large Language Models Cheng et al . ( 2023 ) are currently parameterized by billions of parameters . This inherently limits the methods used for optimization to ones that can effectively run in linear time with respect to the dimensionality of the parameters . To ensure efficiency at scale , there is a large body of research within this space , from momentum - based methods Goh ( 2017 ) , to the forward - forward algorithm Hinton ( 2022 ) . However , all of these methods include hyperparameters that need to be tuned for the task at hand . Therefore , many hyperparameter tuning and meta - learning methods have been proposed Akiba et al . ( 2019 ) ; Rogachev & Melikhova ( 2020 ) ; Gad ( 2022 ) , each with its own limitations . One important limitation shared across these methods is the lack of active hyperparameter optimization . Instead of modifying hyperparameters as training proceeds , these algorithms only control initial hyperameter values . They then let the dynamical system evolve until completion Liu & Theodorou ( 2019b ) — training until a time - limit or a convergence condition is met . Thus , they are limited by the same problems of finding optimal controllers for open - loop dynamical systems . Instead , ( 1 ) performing closed - loop control , ( 2 ) observing the behavior as these local - optimization iterations occur , and ( 3 ) making informed decisions along the path results in an approach that can more efficiently solve complex non - convex optimization tasks . As evidence for the benefits of dy - namic control , a large body of work on learning - rate schedules exists , showing very significant im - provements over using static rates Liu & Theodorou ( 2019a ) ; Li et al . ( 2017 ) ; Darken et al . ( 1992 ) ; Li & Arora ( 2019 ) . Schedules are limited in that they are only functions of the number of iterations taken , remaining ”blind” to the performance of the algorithm and generating more hyperparam - eters to tune . Also , because the loss landscapes for many neural networks can be complex and 1 University of California , Berkeley 2 Boston University ∗ Co - first authors 1 a r X i v : 2310 . 14671v1 [ c s . L G ] 23 O c t 2023 non - smooth , local search methods may get ”stuck” in high - dimensional places / saddle points Choro - manska et al . ( 2015 ) ; Brea et al . ( 2019 ) . Instead , global optimization methods , usually population - based , employ non - differentiable opti - mization for non - convex loss functions . By adding random noise , individuals can search the loss space , eventually converging at a global minima instead of local minima . Evolutionary / genetic al - gorithms B¨ack ( 1996 ) ; Liashchynskyi & Liashchynskyi ( 2019 ) are one of the most popular methods that utilize mutations every iteration , and differential evolution Storn & Price ( 1995 ) is a subset that uses differentiable mutations for faster convergence Karaboga & Cetinkaya ( 2005 ) . Still , evolu - tionary algorithms , can be prohibitively expensive and intractable by stochastic nature , and struggle against fine - tuned local search solutions . Thus , to take advantage of the efficiency of local search and the improved global search of evo - lutionary algorithms , memetic algorithms combine both Moscato ( 1999 ) ; Yuenyong & Nishihara ( 2014 ) ; D’Angelo & Palmieri ( 2021 ) ; Borna & Hashemi ( 2014 ) . Recent work Cui et al . ( 2018 ) ; Xue et al . ( 2021 ) has proposed memetic algorithms for optimization on deep - learning workloads which track model fitness to make adjustments ( in this case , mutations ) during training . However , these methods themselves required hyper - parameter tuning for different benchmark machine learn - ing tasks . Futhermore , their implementability is limited by the complexity of the employed genetic recombination schemes . To overcome the aforementioned limitations , we propose Population Descent ( P OP D ESCENT ) , an m - elitist population - based memetic algorithm for hyperparameter optimization . The key idea in P OP D ESCENT is actively choosing how much to explore the parameter and hyperparameter space or exploit a specific location on the loss curve . This decision is taken based on a normalized fitness score representing our model’s progress during iterations of local updates . We show in our evalu - ations that calculating fitness based on a cross - validation set , and linking mutation strength to an individual’s performance produces better results compared to regular hyperparameter tuning meth - ods such as grid search , and specialised memetic algorithms such as ESGD Cui et al . ( 2018 ) . As opposed to ESGD , P OP D ESCENT is significantly less sensitive to changes in its own hyperparame - ters , as precisely quantified in our evaluation—see Section 3 . Unlike ESGD , P OP D ESCENT keeps the hyperparameters themselves unchanged across all the benchmarks . To demonstrate P OP D ESCENT ’s ability to more effectively traverse the loss landscape on real deep - learning workloads , we apply the algorithm to the FMNIST Xiao et al . ( 2017 ) and CIFAR - 10 Krizhevsky et al . ( 2009 ) classification benchmarks . We also conduct ablation studies justifying the effectiveness of the choices made in P OP D ESCENT . P OP D ESCENT achieves better test and train - ing loss on every experiment we performed , while taking a lower number of total gradient steps . We claim the following contributions : • A biased selection process for choosing replacement individuals that are of higher fitness on a cross - validation set . This directs the parameter search towards models that are not overfit on the training set ; • A variable mutation strength based on the fitness of an individual , balancing whether it should exploit the progress made by gradient - descent or perturb the model’s location in the loss and hyperparameter space ; • An example adaptive learning / regularization rate randomization technique ensuring that training is not static but rather based on progress , emphasizing how poorly - performing models are adjusted appropriately ; • An open source reference implementation based on Tensorflow 2 which can be used directly in machine learning tasks . 2 P OPULATION D ESCENT P OP D ESCENT is a memetic algorithm , meaning it combines both meta - optimization and gradient - based optimization into a single scheme . We define the pseudocode of P OP D ESCENT in Alogirthm 1 which we hereafter describe in detail . 2 2 . 1 A LGORITHM D EFINITION The goal of P OP D ESCENT is to find an optimal set of parameters forming what we term an individual . An individual is constructed from sets of parameters θ , and hyperparameters α . We search for individuals which maximize a user - provided F ITNESS function . These individuals are maximized on batches from a Test distribution that remains unseen during the procedure . Namely individual ∗ = ⟨ θ ∗ , α ∗ ⟩ = sup ⟨ θ , α ⟩∈ Individuals E batch ∼ Test [ F ITNESS ( ⟨ θ , α ⟩ , batch ) ] However , since the Test distribution must remain unseen , we are forced to make use of available proxy data in the form of a Training distribution and a CrossValidation distribution . This is standard in machine learning workloads . We do not make assumptions on the differentiability of the provided F ITNESS function . This allows one to use of common metrics of performance such as accuracy . Since the dimensionality of the parameter space can be exceedingly large ( such as with Neural Networks ) , we allow the use of a L OCAL U PDATE function which can efficiently update the bulk of the parameters held in the θ of every individual . We assume that invocations of L OCAL U PDATE maximizes the individual’s expected F ITNESS over the Training set . An example of such a function is Stochastic Gradient Descent ( SGD ) as defined in Algorithm 2 . SGD makes use of gradient - backpropagation to update θ in linear time . L OCAL U PDATE minimizes a differentiable L OSS as a proxy for maximizing F ITNESS with respect to θ . However , the L OCAL U PDATE function does not modify the α hyperparameters . This can for example be the learning rate in SGD , and it can also be the regularization magnitude . In order to find the best hyperparameters , P OP D ESCENT takes an m - elitist approach by holding onto a candidate set of individuals called a Population . In each iteration , The m fittest individuals from the Population are kept untouched ( m - elite ) , while the weakest ( | Population | − m ) individuals are always replaced . We then pick replacements from the Population but bias our choices towards fitter individuals . These replacements then go through a M UTATE operation provided by the user . The mutation magnitude depends on the fitness of the individual . That is , we mutate individuals more when they perform poorly . In a sense , the normalized F ITNESS value allows the algorithm to be aware of progress made during optimization , and explore more space when that is more beneficial . Throughout the algorithm , | Population | remains an invariant . 3 Algorithm 1 P OP D ESCENT Require : individual : θ × α Require : F ITNESS : individual × Batch → [ 0 , 1 ] Require : C ONVERGED : individual → { 0 , 1 } Require : L OCAL U PDATE : individual × Batch → individual Require : M UTATE : individual × [ 0 , 1 ] → individual Require : Training : Distr [ Batch ] Require : CrossValidation : Distr [ Batch ] Require : Population : { individual , . . . } Require : m : N 1 : while ¬ C ONVERGED ( Population ) do 2 : batch Training ∼ Training 3 : Optimized ← { L OCAL U PDATE ( individual , batch Training ) | individual ∈ Population } 4 : batch CV ∼ CrossValidation 5 : F ITNESS CV ( x ) = F ITNESS ( x , batch CV ) 6 : WeightedMultinomial ← Pr ( X = x ) = (cid:40) x ∈ Optimized F ITNESS CV ( x ) (cid:80) o ∈ Optimized F ITNESS CV ( o ) x ̸∈ Optimized 0 7 : Mutated ← ∅ 8 : Strong ← Optimized 9 : for 1 . . . ( | Population | − m ) do 10 : weak ← M INIMUM F ITNESS CV ( Strong ) 11 : Strong ← Strong / { weak } 12 : replacement ∼ WeightedMultinomial 13 : Mutated ← Mutated ∪ { M UTATE ( replacement , 1 − F ITNESS CV ( replacement ) ) } 14 : end for 15 : Population ← Strong ∪ Mutated 16 : end while 17 : return M AXIMUM F ITNESS CV ( Population ) P OP D ESCENT terminates when the user - defined C ONVERGED function outputs 1 ( line 1 ) . Then , at each iteration : 1 . Lines 2 - 3 : The individuals in the Population all take a L OCAL U PDATE step over a batch sampled form the Training distribution . This produces a set of Optimized individuals ; 2 . Lines 4 - 5 : A batch is sampled from the CV distribution , upon which we build F ITNESS CV , i . e . , the fitness function for that batch ; 3 . Line 6 : We use F ITNESS CV to build a WeightedMultinomial probability distribution whose sam - ples are individuals from the Optimized set . The probability of each individual is defined by normalizing their fitness values , so that the probabilities sum to 1 . This distribution is biased towards choosing fitter replacements ; 4 . Line 7 - 14 : We iterate ( | Population | − m ) times replacing the ( | Population | − m ) lowest fitness individuals by a mutated replacement . We find replacement individuals via sampling from the WeightedMultinomial distribution ( Line 12 ) . Then the replacement is mutated by an amount dependent on its fitness : the lower the fitness , the more it will be mutated ; 5 . Line 15 : Population is now updated to include the m Strong individuals and the ( | Population | − m ) mutated replacements ; 6 . Line 17 : Finally , we return the individual in the Population with the largest fitness . 4 Algorithm 2 Example function implementations Require : L OSS : individual × Batch → R Require : β 1 , β 2 : R function L OCAL U PDATE ( individual , Batch ) optimized ← individual optimized θ ← individual θ + individual α ∇ individual θ L OSS ( individual , Batch ) return optimized end function function M UTATE ( individual , magnitude ) mutated ← individual mutated θ ∼ Gaussian ( individual θ , β 1 magnitude ) mutated α ∼ LogNormal ( individual α , β 2 magnitude ) return mutated end function In the example function implementations in Algorithm 2 , we also show a sample M UTATE function where we randomize the θ parameters via a Gaussian distribution whose standard deviation is de - fined by the mutation magnitude . We opt to modify the learning rate geometrically via a LogNormal distribution so that multiplying the learning rate by 0 . 1 and 10 is equally as likely with a standard deviation of 1 . Note that when the magnitude is at 0 , None of the parameters would change . 2 . 2 K EY POINTS IN P OP D ESCENT ’ S DESIGN • We designed P OP D ESCENT to naturally select individuals which generalize well to a dataset not seen during local updates . We hypothesize that this would allow proper selection of regularization values rather than using ad - hoc techniques such as early stopping . This is evaluated in Section 3 . 3 . • If we remove the selection and mutation procedure then P OP D ESCENT simply becomes the ran - dom hyperparameter search algorithm , since after initialization , the individuals will be undergoing only iterations of SGD . • P OP D ESCENT is also amenable to parallelization and the only synchronization required occurs in the replacements step . • P OP D ESCENT has a few hyperparameters itself ( depending on the implementation of M UTATE ) , but we have left these values constant across our experiments to showcase the effectiveness of the method and its low sensitivity to specific values of these parameters . 2 . 3 L IMITATIONS Due to the no free lunch theorem Wolpert & Macready ( 1997 ) , there will always be a case where this algorithm will be worse than a purely random approach at maximizing our F ITNESS . For example , if the learning rate is initialized too high , too many randomization steps would be needed for making progress , due to the random walk nature of the mutation method used . Another limitation is that the algorithm does not take into account the best individual ever observed , meaning there is no guarantee that the algorithm will always improve in performance with more iterations . This is due to the decision to always take a L OCAL U PDATE with respect to the Population . 3 E VALUATIONS In this section , we demonstrate that 1 ) P OP D ESCENT ’s active tuning framework achieves better performance than existing tuning and memetic algorithms on the FMNIST and CIFAR - 10 bench - marks ; 2 ) While significantly simpler , P OP D ESCENT converges at rates similar to the state - of - the - art memetic algorithm in a fair comparison ; 3 ) P OP D ESCENT ’s specific randomization scheme con - tributes to its results ; and 4 ) P OP D ESCENT is remarkably insensitive to changes in its own hyperpa - rameters , allowing it to tune the target parameters without having to tune the framework itself . 5 3 . 1 B ENCHMARKS We compare P OP D ESCENT against 1 ) grid search , due to its prevalence . 2 ) KerasTuner ( KT ) : Ran - domSearch , due to its popularity ( KT Rogachev & Melikhova ( 2020 ) and Optuna Akiba et al . ( 2019 ) are the most popular hyperparameter tuning frameworks , currently totalling around 2 million down - loads per month ) . And 3 ) Evolutionary stochastic gradient descent ( ESGD ) Cui et al . ( 2018 ) , as it is the state - of - the - art memetic algorithm for benchmark machine learning workloads ( to our knowl - edge ) . For clarification , KT’s RandomSearch algorithm does not just randomly sample a subset of hyper - parameters than would be explored during a grid search . Sampling is not limited to discrete rates ( i . e . it can choose from continuous distributions ) . Also , RandomSearch chooses the ”best” hyperpa - rameters after testing parameter combinations on the first few ( in our case , two ) epochs of training , then resetting the model again , seeing which combination has the best validation loss early on . This allows it to test more parameter combinations in fewer gradient steps . Some notes on benchmarks . For the FMNIST and CIFAR - 10 benchmarks , we opted to train larger models ( 4 , 575 , 242 and 186 , 250 parameters respectively ) as they are more prone to overfitting Arpit et al . ( 2017 ) leading to higher sensitivity in hyperparameter choice , a problem well - suited to eval - uate these tuning frameworks . The ”With Regularization” models use the same model with l2 ker - nel regularization added . To compare fairly against the available implementation of ESGD which does not implement regularization , we exclude comparisons with regularization . All algorithms use cross - validation loss as the metric for evaluating model fitness during training . All algorithms use the Adam optimizer for local search , except for ESGD , which uses SGD . Grid search ”Without Regularization” trains five models each with a unique learning rate ( [ 0 . 01 , 0 . 001 , 0 . 0001 , 0 . 00001 , 0 . 000001 ] ) . For ”With Regularization , ” we let grid search enumerate the cartesian product of the five aforementioned learning rates and five different regularization rates producing 25 trained mod - els . We use KT RandomSearch with 25 trials ( # of combinations it tests ) . It samples learning rates from the continuous range of [ 0 . 01 − 0 . 0001 ] , and regularization rates from [ 0 . 1 − 0 . 000001 ] . RandomSearch and ESGD train over the whole dataset , and P OP D ESCENT and grid search sam - ple portions of the data . A gradient step is defined by a single step taken by a local optimizer over one batch . We calculate the total number of gradients steps taken by every algorithm via total = iterations × number of models × batches . We choose how many gradient steps to run each algorithm by observing when they converge . Our objective is minimizing the final test loss . FMNIST Benchmark . We tested each algorithm on the Fashion MNIST image classification dataset in Table 1 , containing a 60k image training - set and a 10K image test - set ( we split the test - set into two halves for a validation - set for all methods except ESGD , which uses the full test - set for validation ) . Each image is size 28x28 , with 1 color channel . An identical Convolutional Neural Net was used for each test ( 4 , 575 , 242 parameters ) , with three convolutional layers and two fully connected . For all tests , we set the batch size to 64 , and ESGD and P OP D ESCENT are initialized with default learning rates of 0 . 001 . CIFAR - 10 Benchmark . We tested each algorithm on the CIFAR - 10 image classification dataset in Table 1 , containing a 50k image training - set and a 10K image test - set , splitting test / validation loss exactly the same as for FMNIST . Each image is size 32x32 with 3 color channels . An identical Convolutional Neural Net was used for each test ( 186 , 250 parameters ) , with four convolutional layers and two fully connected . We set the batch size to 64 in all cases , except for ESGD where we set it to 8 ; as is done in Masters & Luschi ( 2018 ) . ESGD is initialized with a learning rate of 0 . 01 , and P OP D ESCENT with 0 . 001 ( ESGD performs much better on CIFAR - 10 with 0 . 01 over 0 . 001 in our tests ) . Benchmark Results . P OP D ESCENT finds models with the lowest overall test loss across the board . It is also always taking the fewest or near fewest gradient steps . Both grid search and Random - Search cannot adjust their parameters on - line , their convergence rates thus suffer . ESGD is our closest comparison to P OP D ESCENT as a memetic algorithm , but does not tune any hyperparame - ters . These results show ESGD’s mutations perform well , but it relies on either a static learning rate or a schedule , both of which remain unchanged throughout gradient updates . On larger models that are prone to overfitting , P OP D ESCENT ’s ability to constantly monitor a model’s performance on the cross - validation set and accelerate or decelerate its learning / regularization proves to be performant in these benchmarks . 6 Table 1 : Benchmark comparison Algorithm Test Loss ± σ Train Loss ± σ Gradient Steps FMNIST Without Regularization Basic Grid Search 0 . 251 ± 0 . 010 0 . 037 ± 0 . 006 64 , 000 KT RandomSearch 0 . 277 ± 0 . 023 0 . 112 ± 0 . 034 46 , 800 ESGD 0 . 276 ± 0 . 009 0 . 114 ± 0 . 007 46 , 800 Population Descent 0 . 249 ± 0 . 020 0 . 124 ± 0 . 052 32 , 000 FMNIST With Regularization Basic Grid Search 0 . 309 ± 0 . 009 0 . 251 ± 0 . 007 160 , 000 KT RandomSearch 0 . 400 ± 0 . 061 0 . 295 ± 0 . 077 46 , 800 Population Descent 0 . 262 ± 0 . 019 0 . 152 ± 0 . 033 32 , 000 CIFAR - 10 Without Regularization Basic Grid Search 1 . 176 ± 0 . 182 1 . 052 ± 0 . 250 19 , 200 KT RandomSearch 1 . 512 ± 0 . 275 1 . 343 ± 0 . 296 39 , 000 ESGD 0 . 998 ± 0 . 025 0 . 966 ± 0 . 033 93 , 750 Population Descent 0 . 863 ± 0 . 014 0 . 577 ± 0 . 060 25 , 600 CIFAR - 10 With Regularization Basic Grid Search 0 . 970 ± 0 . 027 0 . 770 ± 0 . 043 96 , 000 KT RandomSearch 1 . 195 ± 0 . 209 1 . 030 ± 0 . 249 39 , 000 Population Descent 0 . 843 ± 0 . 030 0 . 555 ± 0 . 070 25 , 600 3 . 2 C ONVERGENCE Memetic algorithms like ESGD often rely on mutation lenghts , reproductive factors , mixing num - bers , etc . ; their genetic schemes are complex , and thus difficult to implement . On the other hand , P OP D ESCENT ’s mutation step only adds independent noise to the parameters , and uses a simple rank - based ( m - elitist ) recombination step . However , when comparing convergence of the highest fitness model in the population , Figure 1 shows P OP D ESCENT converges to a lower validation loss faster than existing tuning methods and memetic algorithms . We train each algorithm on six random seeds , running them for more iterations than optimal to show convergence / divergence over time ( Grid Search : 100 iterations , KT RandomSearch : 25 , P OP D E - SCENT 115 , and ESGD : 15 ) . We plot the mean exponential moving average ( bold line ) of the cross - validation loss of the best model for each algorithm across all seeds , with the standard devia - tion ( shading ) for each algorithm’s trials , as a function of gradient steps taken . Figure 1 : FMNIST validation loss progress . 7 In Figure 1 , RandomSearch is flat until about 46K gradients steps because it takes gradient steps to test which parameters are best without actually training the model ; it only trains the model af - ter 46K steps ( 25 trails ) . Grid search and RandomSearch both struggle to reach a low loss due to non - dynamic tuning . P OP D ESCENT and ESGD are most succesful during training , though P OP D E - SCENT achieves better final test loss with lower standard deviation , and requires fewer tunable hy - perparameters to implement its global step . 3 . 3 A BLATION S TUDY This section analyzes how 1 ) the randomization scheme of NN weights / learning rate / regularization rate , and 2 ) the use of cross - validation loss to evaluate the fitness of individuals affects P OP D ES - CENT ’s performance . To emphasize the differences , we add l2 kernel regularization to every layer in the benchmark FMNIST model , and reduced the training set size to 10K . All tests are initialized with a default learning and regularization rate of 0 . 001 . We choose | Population | = 10 and m = 5 . Table 2 : Ablation study FMNIST Randomization CV Selection Regularization Test Loss ± σ Train Loss ± σ Ablation Study Over P OP D ESCENT Randomization ✓ ✓ ✓ 0 . 345 ± 0 . 006 0 . 139 ± 0 . 028 ✗ ✓ ✓ 0 . 412 ± 0 . 005 0 . 118 ± 0 . 077 Ablation Study Over Cross - Validation Fitness ✓ ✓ ✗ 0 . 356 ± 0 . 009 0 . 163 ± 0 . 019 ✓ ✗ ✗ 1 . 140 ± 0 . 147 0 . 0003 ± 0 . 0002 The top half of Table 2 shows how P OP D ESCENT ’s randomization ( NN weights , learning and reg - ularization rates ) lowers test loss by 25 % . Adding noise and choosing models that perform well on cross - validation loss helps the models explore more space while selecting models that prevent overfitting , as see with a lower test loss . The bottom half shows how deciding between training or cross - validation loss as the fitness function acts as a substantial heuristic when minimizing test loss , genetically ”forcing” a model without regularization to still achieve decent test loss . Even when regularization is turned off , and also when cross - validation selection is turned off ( now , training loss becomes the heuristic for minimization instead of validation loss ) , we still observe similar perfor - mance improvements due to randomization being turned on versus being turned off . We present the most pronounced differences in Table 2 to best highlight P OP D ESCENT ’s features . 3 . 4 H YPERPARAMETER S ENSITIVITY In this section , we show that local search parameters affect other algorithms more than P OP D ES - CENT on the CIFAR - 10 dataset . We run each algorithm with a constant seed and constant hyper - parameters except one ( either learning rate or the number of iterations ) . One iteration defines one local and global update together . gradient updates the algorithm takes before performing a mutation . P OP D ESCENT defaults to a batch size of 64 , a learning rate of 0 . 001 with Adam , and 30 iterations for the FMNIST benchmark . ESGD defaults to a batch size of 8 , a learning rate of 0 . 01 with SGD , and 3 iterations for the FMNIST benchmark ( P OP D ESCENT trains over 128 batches per iteration , ESGD over the whole training set ) . Table 4 shows how changes in local training parameters affect ESGD’s test loss results more than P OP D ESCENT ’s in Table 3 ( almost 275 % higher standard deviation of results ) . P OP D ESCENT also has a much lower test loss across trials ( avg . 19 . 2 % lower ) . Complex memetic algorithms such as ESGD have a high number of adjustable hyperparameters , and their performance depends sig - nificantly on their specific values . As long as the parameters chosen are not extreme values , the specificity of P OP D ESCENT ’s hyperparameters is not particularly important . Another important note is the lack of need to tune P OP D ESCENT over different problems , while still yielding the best model . All tests for P OP D ESCENT across this entire paper ( except the ablation 8 Table 3 : Population Descent training with variable local parameters Learning Rate Iterations Test Loss ± σ Across Trials σ as % of Test Loss All Hyperparameters Constant Except Learning Rate [ 0 . 01 , 0 . 05 , 0 . 001 ] 30 1 . 049 ± 0 . 172 16 . 35 % All Hyperparameters Constant Except Total Iterations 0 . 001 [ 10 , 30 , 50 ] 0 . 958 ± 0 . 191 19 . 94 % Table 4 : ESGD training with variable local parameters Learning Rate Iterations Test Loss ± σ Across Trials σ as % of Test Loss Everything Constant Except Training Learning Rate [ 0 . 01 , 0 . 05 , 0 . 001 ] 3 1 . 325 ± 0 . 582 43 . 95 % Everything Constant Except Total Iterations 0 . 001 [ 1 , 3 , 5 ] 1 . 159 ± 0 . 455 39 . 22 % tests ) use the same population size ( 5 ) and randomization scheme ( same Gaussian noise distribu - tions ) for the global step , and the same default learning rate ( 0 . 001 ) , regularization rate ( 0 . 001 ) , and batch size ( 64 ) for the local step ( except when they are changed for this experiment ) . Discussion on learning rate schedules . Learning rate schedules ( having the learning rate be set ac - cording to the number of gradient steps taken ) are one of the most common way to ”actively” adjust hyperparameters during training . However , most schedules are only a function of the number of gra - dient steps taken , which are only a prediction of training rather than analyzing real time how a model is performing like P OP D ESCENT does . Specifially , Table 4 shows how non - dynamic optimization algorithms ( most existing methods ) rely on problem - specific hyperparameters to be pre - determined . Modifications to the idea of learning rate schedules do exist , in order to pay attention to a model’s progress Wu et al . ( 2019 ) , though they are very complex and have many hyperparameters , running into sensitivity issues like ESGD . 4 R ELATED WORKS Gradient - Based Optimizers . Stochastic Gradient Descent ( SGD ) offers quick convergence on complex loss spaces Kleinberg et al . ( 2018 ) . As an improvement to SGD , momentum - based opti - mizers like Adam Goh ( 2017 ) ; Zeiler ( 2012 ) better traverse loss functions via utilizing momentum with the learning rate to more quickly escape plateaus or slow down learning as to not skip a min - ima . Adam’s weight decay term also limits exploding gradients , and acts as a regularizer preventing overfitting . Other options like the newer Shampoo optimizer , which use ”preconditioning matri - ces , ” promises even faster convergence Gupta et al . ( 2018 ) . P OP D ESCENT relies on efficient local optimizers , hence such works are orthogonal . Grid Search / Tuning Frameworks . Grid search is the most commonly used method for searching the space of hyperparameters due to the ease of implementability yielding generally acceptable re - sults for NN training . Essentially , it is an exhaustive search of the cartesian product of hyperparam - eters whose cardinality scales exponentially with dimentionality . Popular hyperparameter tuning frameworks like KerasTuner ( KT ) and Optuna employ more efficient versions of grid search Ro - gachev & Melikhova ( 2020 ) , like bayesian search ( uses Gaussian probabilities to check the ”best” combination ) Mockus ( 1989 ) , random search ( randomly samples the search space ) Bergstra et al . ( 2011 ) , or hyperband tuning ( a variation of random search that chooses better individuals after half of the iterations ) Rogachev & Melikhova ( 2020 ) . They can sample different batches , batch sizes , learning / regularization rates , and even NN layers / units in order to find the best architecture for the task at hand . They often find a good set of hyperparameters within a constant amount of time as opposed to grid search’s brute force method Liashchynskyi & Liashchynskyi ( 2019 ) . However , 9 these methods do not allow for dynamic hyperparameter optimization ; each run is independent of progress made in previous runs , and most algorithms simply return hyperparameters that the model should be initialized with during training . One common approach for adding dynamicity to the hyper - parameters is the use of schedules Li & Arora ( 2019 ) . Learning rate schedules , for exam - ple , are often defined by three to five parameters , and have been proposed to improve convergence speed Dauphin et al . ( 2015 ) ; Darken et al . ( 1992 ) . These approaches are fundamentally limited as they are based on predictions about training , rather than a model’s current loss . Multiple works also explore cyclical , cosine - based , random - restart schedules adjusting the learning rate at every epoch Wu et al . ( 2019 ) . However , they do not employ population - based selection , and thus explore less space . They introduce extra hyperparameters , causing many to instead use static schedules . Memetic Algorithms Memetic algorithms take advantage of both global and local learning , and are increasingly being used for supervised learning benchmarks D’Angelo & Palmieri ( 2021 ) ; Moscato ( 1999 ) ; Borna & Hashemi ( 2014 ) ; Xue et al . ( 2021 ) ; Yuenyong & Nishihara ( 2014 ) . Evolutionary stochastic gradient descent ( ESGD ) Cui et al . ( 2018 ) utilizes Gaussian mutations for model param - eters using an m - elitist average strategy to choose the best models after randomization and SGD optimization for local search . Performing well on CIFAR - 10 classification tests , ESGD is a prime example of how adding stochastic noise benefits a strong local optimizer . Nonetheless , state - of - the - art memetic algorithms like ESGD suffer from having an extensive amount of training hyperparam - eters , both global ( ie . mutation strength , population size , etc . ) and local ( ie . batch size , learning rate , etc . ) . Motivated by the difficulties in using a memetic framework for fine - tuning hyperparameters . P OP D ESCENT instead investigates how it is possible to tune models and prompt them to explore more space with simple stochastic noise efficiently . It strives to be a tuning framework that is not problem - specific , but can be easily applied elsewhere . 5 C ONCLUSION In this paper , we propose P OP D ESCENT , a memetic algorithm that acts as a hyperparameter tuning framework using a simple population - based evolution methodology . Our tuning framework helps local search methods explore more space on the loss function . In turn , it more effectively traverses a non - convex search - space compared to methods relying only on static momentum terms or schedules . P OP D ESCENT performs better than existing tuning frameworks which do not adapt to a model’s cur - rent progress . Four extensive experiments over common supervised learning benchmarks FMNIST and CIFAR - 10 show the effectiveness of P OP D ESCENT . 10 6 R EPRODUCIBILITY S TATEMENT We take many efforts to make sure that our experiments can be reevaluated effectively : • We use the number of gradient steps as our metric of ”time” , so that these values remain indepen - dent of the computational hardware available • We always seed every experiment taken , and those seeds are available in our source - code . • We provide versioned source code with specific commits referenced for each test taken , and pro - vide a README of instructions to follow to replicate our results • We provide our reference anonymized implementation of P OP D ESCENT and supplementary material at https : / / github . com / anonymous112234 / anonymousPopulationDescent . git • We provide a flake . nix file which exactly pins the versions of all the packages used in our tests R EFERENCES Takuya Akiba , Shotaro Sano , Toshihiko Yanase , Takeru Ohta , and Masanori Koyama . Optuna : A next - generation hyperparameter optimization framework , 2019 . Devansh Arpit , Stanislaw Jastrzebski , Nicolas Ballas , David Krueger , Emmanuel Bengio , Maxin - der S . Kanwal , Tegan Maharaj , Asja Fischer , Aaron C . Courville , Yoshua Bengio , and Si - mon Lacoste - Julien . A closer look at memorization in deep networks . 2017 . URL https : / / api . semanticscholar . org / CorpusID : 11455421 . Thomas B ¨ ack . Evolutionary Algorithms in Theory and Practice : Evolution Strategies , Evolu - tionary Programming , Genetic Algorithms . Oxford University Press , Inc . , USA , 1996 . ISBN 0195099710 . James Bergstra , R ´ emi Bardenet , Yoshua Bengio , and Bal ´ azs K ´ egl . Algorithms for hyper - parameter optimization . 24 , 2011 . URL https : / / proceedings . neurips . cc / paper _ files / paper / 2011 / file / 86e8f7ab32cfd12577bc2619bc635690 - Paper . pdf . Keivan Borna and Vahid Haji Hashemi . An improved genetic algorithm with a local optimiza - tion strategy and an extra mutation level for solving traveling salesman problem . CoRR , abs / 1409 . 3078 , 2014 . URL http : / / arxiv . org / abs / 1409 . 3078 . Johanni Brea , Berfin Simsek , Bernd Illing , and Wulfram Gerstner . Weight - space symmetry in deep networks gives rise to permutation saddles , connected by equal - loss valleys across the loss land - scape . CoRR , abs / 1907 . 02911 , 2019 . URL http : / / arxiv . org / abs / 1907 . 02911 . Wenhua Cheng , Weiwei Zhang , Haihao Shen , Yiyang Cai , Xin He , and Kaokao Lv . Optimize weight rounding via signed gradient descent for the quantization of llms , 2023 . Anna Choromanska , Mikael Henaff , Michael Mathieu , G´erard Ben Arous , and Yann LeCun . The loss surfaces of multilayer networks , 2015 . Xiaodong Cui , Wei Zhang , Zolt´an T¨uske , and Michael Picheny . Evolutionary stochastic gradient descent for optimization of deep neural networks . CoRR , abs / 1810 . 06773 , 2018 . URL http : / / arxiv . org / abs / 1810 . 06773 . C . Darken , J . Chang , and J . Moody . Learning rate schedules for faster stochastic gradient search . In Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop , pp . 3 – 12 , Aug 1992 . doi : 10 . 1109 / NNSP . 1992 . 253713 . Yann Dauphin , Harm De Vries , and Yoshua Bengio . Equilibrated adaptive learning rates for non - convex optimization . Advances in neural information processing systems , 28 , 2015 . Gianni D’Angelo and Francesco Palmieri . Gga : A modified genetic algorithm with gradient - based local search for solving constrained optimization problems . Information Sciences , 547 : 136 – 162 , 2021 . ISSN 0020 - 0255 . doi : https : / / doi . org / 10 . 1016 / j . ins . 2020 . 08 . 040 . URL https : / / www . sciencedirect . com / science / article / pii / S0020025520308069 . 11 Ahmed G . Gad . Particle swarm optimization algorithm and its applications : A systematic review - archives of computational methods in engineering , 2022 . Gabriel Goh . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . URL http : / / distill . pub / 2017 / momentum . Vineet Gupta , Tomer Koren , and Yoram Singer . Shampoo : Preconditioned stochastic tensor opti - mization , 2018 . Geoffrey Hinton . The forward - forward algorithm : Some preliminary investigations , 2022 . Nurhan Karaboga and Bahadir Cetinkaya . Performance comparison of genetic and differential evo - lution algorithms for digital fir filter design . In Tatyana Yakhno ( ed . ) , Advances in Information Systems , Berlin , Heidelberg , 2005 . Springer Berlin Heidelberg . Robert Kleinberg , Yuanzhi Li , and Yang Yuan . An alternative view : When does sgd escape local minima ? , 2018 . Alex Krizhevsky , Vinod Nair , and Geoffrey Hinton . Cifar - 10 ( canadian institute for advanced re - search ) . 2009 . URL http : / / www . cs . toronto . edu / ˜ kriz / cifar . html . Qianxiao Li , Cheng Tai , and Weinan E . Stochastic modified equations and adaptive stochastic gradient algorithms , 2017 . Zhiyuan Li and Sanjeev Arora . An exponential learning rate schedule for deep learning . CoRR , abs / 1910 . 07454 , 2019 . URL http : / / arxiv . org / abs / 1910 . 07454 . Petro Liashchynskyi and Pavlo Liashchynskyi . Grid search , random search , genetic algorithm : A big comparison for NAS . CoRR , abs / 1912 . 06059 , 2019 . URL http : / / arxiv . org / abs / 1912 . 06059 . Guan - Horng Liu and Evangelos A . Theodorou . Deep learning theory review : An optimal control and dynamical systems perspective , 2019a . Guan - Horng Liu and Evangelos A . Theodorou . Deep learning theory review : An optimal control and dynamical systems perspective , 2019b . Dominic Masters and Carlo Luschi . Revisiting small batch training for deep neural networks . CoRR , abs / 1804 . 07612 , 2018 . URL http : / / arxiv . org / abs / 1804 . 07612 . Jonas Mockus . The bayesian approach to local optimization . pp . 125 – 156 , 1989 . doi : 10 . 1007 / 978 - 94 - 009 - 0909 - 0 7 . URL https : / / doi . org / 10 . 1007 / 978 - 94 - 009 - 0909 - 0 _ 7 . Pablo Moscato . Memetic algorithms : a short introduction . 1999 . URL https : / / api . semanticscholar . org / CorpusID : 57168143 . A F Rogachev and E V Melikhova . Automation of the process of selecting hyperparameters for arti - ficial neural networks for processing retrospective text information . 577 ( 1 ) : 012012 , sep 2020 . doi : 10 . 1088 / 1755 - 1315 / 577 / 1 / 012012 . URL https : / / dx . doi . org / 10 . 1088 / 1755 - 1315 / 577 / 1 / 012012 . Rainer Storn and Kenneth Price . Differential evolution : A simple and efficient adaptive scheme for global optimization over continuous spaces . Journal of Global Optimization , 23 , 01 1995 . D . H . Wolpert and W . G . Macready . No free lunch theorems for optimization . IEEE Transactions on Evolutionary Computation , 1 ( 1 ) : 67 – 82 , 1997 . doi : 10 . 1109 / 4235 . 585893 . Yanzhao Wu , Ling Liu , Juhyun Bae , Ka Ho Chow , Arun Iyengar , Calton Pu , Wenqi Wei , Lei Yu , and Qi Zhang . Demystifying learning rate polices for high accuracy training of deep neural networks . CoRR , abs / 1908 . 06477 , 2019 . URL http : / / arxiv . org / abs / 1908 . 06477 . Han Xiao , Kashif Rasul , and Roland Vollgraf . Fashion - mnist : a novel image dataset for benchmark - ing machine learning algorithms , 2017 . 12 Ke Xue , Chaojun Qian , Ling Xu , and Xudong Fei . Evolutionary gradient descent for non - convex optimization . In International Joint Conference on Artificial Intelligence , 2021 . URL https : / / api . semanticscholar . org / CorpusID : 237101090 . Sumeth Yuenyong and Akinori Nishihara . A hybrid gradient - based and differential evolution algo - rithm for infinite impulse response adaptive filtering . International Journal of Adaptive Control and Signal Processing , 28 ( 10 ) : 1054 – 1064 , 2014 . doi : https : / / doi . org / 10 . 1002 / acs . 2427 . URL https : / / onlinelibrary . wiley . com / doi / abs / 10 . 1002 / acs . 2427 . Matthew D . Zeiler . Adadelta : An adaptive learning rate method , 2012 . 13