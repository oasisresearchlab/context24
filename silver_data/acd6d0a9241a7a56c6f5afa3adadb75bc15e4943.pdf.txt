JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 1 Understanding Business Users’ Data - Driven Decision - Making : Practices , Challenges , and Opportunities Sneha Gathani , Zhicheng Liu , Peter Haas , and Ça˘gatay Demiralp Abstract —Business users perform data analysis to inform decisions for improving business processes and outcomes despite having limited formal technical training . While earlier work has focused on data analysts’ and data scientists’ practices and challenges , little is known about business users’ decision - making practices and how they incorporate data and visual analytics into their workflows . To address this gap , we first conduct an interview study with 22 business users to understand the general practices and challenges in their data - driven decision - making processes . We contribute an end - to - end model of business users’ data - driven decision - making processes elaborating the tasks , tools , and challenges at each stage . We find that business users analyze data without relying on data analysts due to various practical constraints and considerations . However , their existing tools are inadequate , particularly in helping understand the relationship between data variables and business goals and facilitating the exploration of what - if scenarios . These findings suggest a need for advanced predictive and prescriptive analytics ( PPA ) tools to support what - if analysis . Motivated by this need , we perform a follow - up , task - based study to understand PPA’s role and potential in business users’ decision - making processes . We find that PPA helps improve efficiency and confidence in decision - making . However , business users also believe that PPA - powered what - if analysis tools are currently in their nascent stages and report improvements before fully integrating them into their decision - making processes . Building upon these findings , we discuss the opportunities and challenges in incorporating PPA into data - driven decision - making and its implications for future data and visual analytics systems . Index Terms —Business Intelligence , What - if Analysis , Predictive and Prescriptive Analytics , Decision Making , Interview Study ✦ 1 I NTRODUCTION B USINESS users , such as sales managers , marketing managers , product managers , and operations managers , play a vital role within enterprises . Their primary responsibility involves making decisions to improve various business processes and outcomes . Examples of crucial decisions they make for business success include What is the best use of the $ 200K marketing budget across advertising channels ? Which activities should the sales team prioritize to increase the deal closing rate ? How to increase customer retention ? These business users possess domain knowledge , rely on using data to gain insights into relevant variables , explore alternative scenarios , and then leverage such data - driven analyses to inform and improve business decisions . However , they often lack formal training in technical domains like computer science , machine learning , or statistics . This distinct contrast between the desire to make data - driven decisions and the limited technical expertise presents a compelling research question : how do business users , despite their limited technical expertise , harness and incorporate visual analytics into their workflows and practices for data - driven decision - making ? Existing research on data and visual analytics has primarily focused on understanding the practices , processes , and challenges of data analysts [ 1 ] , [ 2 ] , [ 3 ] and data scientists [ 4 ] , [ 5 ] , [ 6 ] . However , business users constitute a significant user segment [ 7 ] , [ 8 ] that has received little attention in the literature . Some prior work studies how data analysis systems are used for business decision - making ( e . g . , [ 9 ] , [ 10 ] , [ 11 ] , [ 12 ] , [ 13 ] , [ 14 ] , [ 15 ] ) . Parts of this work em - • S . Gathani and Z . Liu are with the University of Maryland , College Park . E - mail : sgathani @ umd . edu , leozcliu @ umd . edu • P . J . Haas is with University of Massachusetts Amherst . E - mail : phaas @ cs . umass . edu • Ç . Demiralp is with MIT CSAIL . E - mail : cagatay @ csail . mit . edu phasize the role of visualization tools [ 15 ] such as dashboards [ 14 ] rather than data analytics or focuses on broader organizational decision - making [ 15 ] , going beyond business users and for - profit enterprises . Others delve into particular business user groups , such as sales [ 14 ] and marketing managers [ 10 ] , [ 13 ] and consider early legacy data analysis systems [ 9 ] , [ 11 ] , [ 12 ] the relevance of which has evolved over time , or offer decision - making models [ 9 ] without a systematic empirical data collection effort to understand real - world workflows and the use of business data analysis tools therein . While a few prior works study users with limited technical expertise in other domains such as journalism , government , and community engagement [ 16 ] , [ 17 ] , [ 18 ] , [ 19 ] , [ 20 ] , their emphasis is on sense - making instead of decision - making : their subject users seek to extract and understand narratives or relationships embedded in a dataset often in the form of text documents . In comparison , business users’ tasks are drastically different : they need to perform analysis of structured data tables involving many quantitative variables and formulate concrete actions towards better business outcomes . The findings from these works , thus , are not readily applicable to business decision - making . To the best of our knowledge , no studies have investigated the role of data and visual analytics in facilitating data - driven decision - making for business users . To address this gap , we first conduct an interview study with 22 business users 1 who have limited formal technical training to understand general practices and challenges in their data - driven decision - making processes . To ground the interviews , we asked the participants to walk through a realistic business case of maximizing sales by making investment decisions in advertising . Our findings reveal that participants often analyze the data themselves without relying on data analysts due to ( 1 ) the limited availability of data analysts in enterprises , ( 2 ) the inefficiency of communicating with data analysts , ( 3 ) the business pressure to make quick decisions , 1 . In the remaining of the paper , we use the terms business users and users interchangeably . a r X i v : 2212 . 13643v2 [ c s . H C ] 17 O c t 2023 JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 2 Fig . 1 . Summary of our study findings : ( Top ) Tasks ( yellow ) and challenges ( red ) in business users’ data - driven decision - making model . ( Bottom Left ) Key takeaways . ( Bottom Right ) Benefits and concerns of using PPA in data - driven decision - making . and ( 4 ) the importance of incorporating domain knowledge , which data analysts often lack . We distill their descriptions to contribute an end - to - end process model of business users’ data - driven decision - making processes ( Figure 1 ) , comprising of five stages : ( 1 ) Acquire and Calibrate Data , ( 2 ) Augment Drivers and KPIs using Business Expertise , ( 3 ) Understand and Model Driver - KPI Relationships , ( 4 ) Communicate with Stakeholders , and ( 5 ) Validate Feasibility through A / B Testing . Unlike previous models that overlook the role of data [ 9 ] , [ 21 ] , [ 22 ] , our model focuses on how data is incorporated into their workflows . We elaborate on the tasks , tools , and challenges shared by the participants for each stage . In particular , for the third stage ( Understand and Model Driver - KPI Relationships ) , findings reveal the inadequacies of exploratory or descriptive analysis tools in supporting decision - making , in line with prior studies [ 15 ] , [ 23 ] . To overcome these challenges , participants can benefit from automated what - if analysis tools that offer advanced capabilities like predictive and prescriptive analytics ( PPA ) . Predictive analytics evaluates what could most likely happen to the business goal in the future given a set of values for variables and prescriptive analytics tries to determine the optimal values of the variables to achieve predefined business goals . Motivated by business users’ need of PPA - powered what - if analysis for data - driven decision - making , we run a follow - up study to empirically understand its role and potential . We provide the participants with the same realistic business use case as in the previous interview study and ask them to make decisions using a visual analytics tool that supports four example PPA functionalities : ( 1 ) Driver Importance Analysis , ( 2 ) Sensitivity Analysis , ( 3 ) Goal - Seeking Analysis , and ( 4 ) Constrained Analysis . We perform a detailed mixed - methods analysis of participants’ experience and feedback and find that PPA improves efficiency and boosts confidence in decision - making . However , participants also feel that automated analytics like PPA is still in its nascent stages and requires further advancement to help business users prepare the datasets ( e . g . , consolidate data from multiple sources , consistently define KPI goals ) , understand the risk factors of predictive and prescriptive recommendations ( e . g . , confidence and trust ) , and incorporate their domain knowledge ( e . g . , capture volatile ecological conditions such as COVID , war , inflation ) for its effective use in their decision - making . In summary , this paper makes the following contributions : • An interview study to understand business users’ data - driven analytics practices and decision - making challenges . • A novel process model describing five stages in business users’ data - driven decision - making processes , with in - depth discussion on the tasks , tools , and challenges in each stage . • A task - based study soliciting business users’ hands - on experi - ence and feedback of using a PPA - powered what - if analysis tool for data - driven decision - making . • A discussion of the opportunities and challenges for incor - porating PPA in data - driven decision - making , as well as it’s implications for future data and visual analytics systems . 2 R ELATED W ORK Our work is related to prior studies that investigate the analytic workflows and needs of users involved in decision - making in various domains . However , it differs from them in three ways : ( 1 ) our study centers on an overlooked population of users : business users , ( 2 ) these business users’ tasks are different because they actively engage in data - driven business decision - making while having limited formal technical training , and ( 3 ) we focus on assessing potential data and visual analytic solutions to aid these business users in their data - driven decision - making . 2 . 1 Understanding How Users Perform Data Analysis Prior work has examined how different types of users perform data analysis . We categorize them along two critical dimensions : task ( data exploration and sense - making vs . improving outcomes and decision - making ) and technical expertise ( having formal technical training vs . no formal training ) , as depicted in Figure 2 . Previous work predominantly studies users in quadrant I who are characterized by their technical expertise , such as data analysts , data scientists , and business analysts , but do not have the primary JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 3 Fig . 2 . Illustration of data - analyzing users studied in the literature . We map the user groups considering the task ( x - axis ) and technical ex - pertise ( y - axis ) dimensions . Our research focuses on business users . responsibility of making and improving decisions . For instance , Kandel et al . [ 1 ] identify challenges of analysts in their data analysis pipeline , while Alspaugh et al . [ 2 ] interview analysts broadly on exploratory analysis . Demiralp et al . [ 6 ] focus on data scientists’ exploratory data analysis , while Zhang et al . investigate business analysts’ needs in generating data - driven reports [ 24 ] and narratives [ 25 ] . Although some of our findings about challenges business users face overlap with ones previously found ( e . g . , the need to bridge the programming gap for analysts who are less savvy in programming , and the challenges of data integration ) , our findings highlight the importance of performing data - driven what - if analysis to explore alternative scenarios and the unique challenges related to decision - making in terms of time pressure , uncertainty , and domain knowledge . Bartram et al . [ 20 ] study “data workers” who don’t identify themselves as analysts or scientists , but perform data analysis as part of their daily work . These users often have less technical expertise ( quadrant III and IV ) , and business users fit the description of data workers well . Previous studies have focused on the workflows of data workers in non - business domains , such as political scientists [ 17 ] , community leaders [ 16 ] , and law enforcement officers [ 18 ] . However , the high - level tasks performed by these users are data exploration and sense - making , and usually do not explicitly involve making decisions . Both sense - making and decision - making involve consolidating and analyzing data , but they differ in significant ways : sense - making is about understanding potentially complex and ambiguous information , and creating a mental framework or narrative to characterize a particular situation or phenomenon [ 26 ] , [ 27 ] , [ 28 ] , [ 29 ] ; decision - making , on the other hand , is about exploring multiple alternative scenarios and choosing a particular course of action [ 30 ] , [ 31 ] , [ 32 ] . The important aspects of decision - making such as assessing alternatives and choosing actions that can improve their outcomes have not been studied in these works . In addition , current work on sense - making primarily focuses on unstructured data such as text documents , whereas in the case of business uses , they primarily deal with structured tables . In this paper , we present an end - to - end process model describing not only data integration and augmentation , but also exploration of multiple scenarios through what - if analyses and validation of course of action , which are vital in decision - making processes . 2 . 2 Use of Dashboards in Enterprises Previous work also investigates the role of visualization and dashboards in enterprise data analysis and decision - making . For example , Newburger and Elmqvist [ 33 ] study the role of visual - ization for statisticians . Researchers find that business users , in particular sales managers [ 14 ] , [ 34 ] , often employ dashboards in their workflows [ 14 ] , [ 35 ] . However , the current usage of dashboards is limited to communicating findings and actions to stakeholders , rather than making data - driven decisions . Dimara and Stasko [ 36 ] thus highlight the lack of user studies evaluating users’ decision - making in visualization research . Dimara et al . [ 15 ] further show how visualizations can be embedded in strategic or long - term decision - making . In general , however , business users express challenges in creating these dashboards and performing analysis on them to drive decisions [ 37 ] , [ 38 ] . These difficulties are attributed to incomplete information shown in dashboards and their limited interactivity , which prevents users from drilling into the data to explore different scenarios or conduct simulations . Visualization alone without data analytics is insufficient . Therefore , instead of focusing on the role of visualizations and dashboards in decision - making , our research dives deeper into the current data analytic pro - cesses for decision - making and investigates how advanced analytics like PPA may facilitate or hinder business users’ decision - making . 2 . 3 Advanced Analytical Tools Marx et al . [ 12 ] and Bergeron et al . [ 11 ] studied the historical use of legacy information systems in organizations , often referred to as Executive Information Systems ( EIS ) . Van et al . [ 13 ] explore a taxonomy of systems to support marketing management . These works identified a lack of sophisticated decision - making support , particularly in areas such as drill - down and scenario analysis , simulation analysis , and optimization . More recent studies by Crisan and Correll [ 39 ] and Bartram et al . [ 20 ] also echoed these sentiments , although their primary focus was not business decision - making . Additionally , Zhang et al . [ 24 ] and Honeycutt et al . [ 40 ] highlight the importance of integrating users’ domain knowledge with advanced AI models and explore its impact on users’ trust when designing intelligent systems . Our study provides further evidence for the need of effectively supporting business users’ data - driven decision - making . This further motivated us to investigate the role of PPA in the decision - making processes , where we identify challenges and opportunities to integrate domain - specific constraints , considerations , and expertise with advanced analytics . Existing work on human - in - the - loop machine learning mostly focuses on data scientists and data analysts , and research shows that data analysts are willing to consider automating decision - making [ 41 ] or collaborating with models [ 42 ] to benefit from advanced analytics . Crisan and Fiore - Gartland [ 4 ] explore attitudes toward automation among data scientists and their team leaders , focusing on its role in automating routine work , rapid prototyping , and democratizing data science within enterprises . Our study differs from these works in two ways : first , we aim to understand how business users with minimal or no technical expertise perceive advanced capabilities like PPA , a form of AutoML used for making data - informed decisions ; second , we go beyond interviews to let business users perform hands - on decision - making tasks by using example PPA functionalities . In addition , we deliver an end - to - end process model of business users’ data - driven decision - making and further delve into the specific implications of PPA for their decision - making . Many business intelligence ( BI ) tools [ 43 ] , [ 44 ] , [ 45 ] , [ 46 ] and spreadsheet applications , provide some advanced analytics functionalities . However , these systems are typically descriptive and exploratory , answering only the what and why in the data , but lack predictive and prescriptive functionalities , failing to answer follow - up questions of now what ? to plan subsequent actions . Spreadsheet applications generally enable optimization - based data analysis through add - ins and native formulas . For example , Excel’s SOLVER [ 47 ] and GOAL SEEK [ 48 ] features allow solving for a de - sired output of a formula by changing its inputs [ 20 ] . However , our JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 4 Fig . 3 . Method summary . First , an interview study with 22 business users ( 12 female , 10 male ) from marketing ( 7 ) , sales ( 5 ) , product ( 5 ) , and operations ( 5 ) departments to understand their general practices and challenges of decision - making . Motivated by business users’ need for PPA , then , a follow - up task - based study with the same business users to understand the role and potential of PPA in their decision - making . findings suggest that manually creating and entering these formulas is difficult for business users and detrimental to iterative , interactive analysis , which our example PPA functionalities overcome . 2 . 4 Decision - Making in Management and Organization Research The fields of management and organizational theory categorize decisions into a three - tier pyramid based on factors like time , impact , and the individuals involved . These tiers include strategic decisions made at the highest executive level , tactical decisions made by middle managers and department heads , and opera - tional decisions handled by front - line employees in their daily routines [ 49 ] . Our work specifically delves into the processes of tactical decision - making . A few high - level models exist to describe decision - making processes . Simon [ 21 ] proposed an abstract three - step model ( also observed in [ 15 ] ) : intelligence ( where information is gathered and problems are identified ) , design ( where possible solutions for the problems are generated and evaluated ) , and choice ( where the best alternative is selected ) . While our model covers these three steps , it offers a much more detailed analysis of users’ processes , focused on data and describing their tasks and challenges . Berisha - Shaqiri [ 22 ] presents a eight - step model : identification of decision’s purpose ( pre - done before the decision - making process ) , information gathering ( Stage 1 ) , principles for judging the alternatives ( Stage 1 & 2 ) , brainstorm and analyze choices ( Stage 3 ) , evaluating alternatives ( Stage 4 ) , decision execution , and results evaluation ( Stage 5 ) . In contrast , our model provides multiple levels of description : stages and tasks , with some of these eight steps framed as tasks . Jun [ 50 ] focuses solely on Stage 3 of our model , centering on hypothesis formalization based on statistical analyses conducted by data scientists . Perkins et al . [ 10 ] study the impact of marketing managers’ experience on decisions made and the information used to make them . In contrast , we run an empirical study with managers across various departments to develop a practical process model of their data - driven decision - making processes . Finally , Little [ 9 ] reports on a real use case study on marketing managers from over 40 years ago , highlighting very similar processes followed by business users even today , but with the aid of descriptive and exploratory tools . In all these previous work on decision - making , the role of data and analysis is largely simplified or overlooked . In contrast , our model provides a detailed account of how data is incorporated into the workflows of business users’ decision - making . 3 I NTERVIEW S TUDY The interview study aimed to understand business users’ data - driven decision - making practices and challenges . 3 . 1 Methods Figure 3 illustrates an overview of our interview study methods . Participants . We recruited business users via an online survey [ 51 ] through the User Interviews website [ 52 ] and our company’s professional mailing lists . Our selection criteria ensured that : ( 1 ) participants used the organization , customer , or product data to guide decisions to attain their business goals , ( 2 ) they made business decisions on at least a weekly basis , and ( 3 ) they were not experts in data science or programming but analyzed data for making decisions . The final participant pool comprised 22 business users from different companies and across four departments : 7 marketing , 5 sales , 5 product , and 5 operations managers . The supplementary material [ 53 ] provides more details on the participants’ company sector and size . Protocol . We held semi - structured interviews remotely , each lasting 60 to 70 minutes . One interviewer was present to conduct and record the session . First , we asked participants 21 open - ended questions to know their experiences , such as “describe recent business questions you are trying to answer” and “talk us through the process of coming up with strategies or decisions on your business questions” . Questions broadly fit into the following four categories that understood business users’ : ( 1 ) role and team , ( 2 ) business goals , ( 3 ) data , tools , and analysis practices , and ( 4 ) ideal tool . Then , we asked the participants to walk us through analyzing and making decisions for a realistic business use case of Marketing Mix Modeling ( MMM ) using their choice of features and tools . Marketing Mix Modeling Business Use Case . Participants were asked to imagine themselves as marketing managers of a TV company with the business goal of maximizing TV sales for the next quarter by assessing a historical dataset . The dataset consisted of weekly data on the investments made on five advertising channels ( SMS , TV , Internet , Radio , Newspaper ) , economic factors ( Demand , Supply , Unit Price , Consumer Confidence Index , Producer Price Index , Consumer Price Index , Gross Rating Points or reach of advertising channels ) , and company’s sales . We chose this scenario since it is representative of a real - world business use case . Moreover , it was familiar to all participants , many being marketing managers or having some interaction with marketing teams . The data was sourced from Kaggle [ 54 ] and assumed to be sufficient and relatable for participants’ analysis and own business goals respectively . The assumptions and validity of the use case were informed by the marketing team members of our company , who were our local experts . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 5 Analysis . We recorded and transcribed the participant responses from both parts : the open - ended and the MMM use case and per - formed iterative coding to identify themes in the data . This method was chosen over other methods like grounded theory to learn busi - ness users’ workflows specifically along four categories . The two parts of the study were first coded separately with one researcher coding each response and grouping it into high - level themes under the four question categories . From the codes , we derived the five stages of business users’ data - driven decision - making processes . While the codes from the MMM business use case specifically pro - vided deeper insights into the relationship modeling stage ( Stage 3 in Sec . 3 . 3 ) , they also complemented and corroborated the codes rel - evant to the other stages from the open - ended part of the study . For example , the participants described data acquisition and transforma - tion along with communication when discussing the use case . As a result , the codes from the two parts of the study were later merged into an end - to - end five - stage model , with details on tasks , tools , and challenges in each stage . The themes were adjusted and refined as more codes were produced . The researcher double - checked with the other researchers whenever questions arose or clarifications were needed . The raw data were coded through 3 to 5 iterations . 3 . 2 Business Users’ Goals and Decisions Goals . Regardless of the participants’ roles and companies , they worked to achieve the common goal of improving their business outcomes and making their businesses efficient so that their compa - nies can succeed and get an edge over their competitors . Business outcomes were quantified as Key Performance Indicators ( KPIs ) , e . g . , sales , deal closing rate , and retention rate . The participants made decisions to maximize two predominant KPIs—the revenue and sales of their company—while optimizing time and resource constraints . Since the goal to maximize these two primary KPIs was too complex to be pursued , planned , and implemented by an individual user or team , it was broken into multi - faceted lower - level goals and KPIs that were managed by business users working in dif - ferent departments – marketing , sales , product , and operations . For example , marketing managers P1 and P8 needed to make decisions for the goal of turning prospective customers into real ones , which is measured by the KPI conversion rate . In another example , product managers P4 and P12 were interested in preventing existing cus - tomers from leaving , which is measured by the KPI retention rate . Decisions . Given the goals , business users needed to make decisions on what actions to take regarding the drivers , which were data variables about customers and products . Examples of drivers included search engine optimization ( SEO ) data ( e . g . , traffic , impressions ) , product usage data ( e . g . , feature usage frequency , time spent ) , and customer activities ( e . g . , sign - ups , clicks ) . To increase conversion rate , for example , a plausible decision was “providing 3 months free trial on the product on signing up for a demo” ( which involved the “demo sign up” driver ) , or “sending out targeted marketing emails showing how the product works for prospective customers’ use cases” ( which involved the “marketing emails sent” driver ) . Similarly , to increase retention rate , plausible decision was “changing placements of product features and highlighting certain features using pop - ups that match customers in a certain geography” ( which involved the “feature usage frequency” driver ) , or “providing tutorial use cases of product mapped to customers’ business niche” ( which involved the “customer vertical” driver ) . More examples of goals and decisions are included in our supplementary material [ 53 ] . For a given goal , participants needed to make decisions to not only choose what drivers to take action on , but also determine how the chosen drivers should be changed . 3 . 3 Data - Driven Decision - Making Processes All participants echoed prior findings by [ 4 ] , [ 15 ] that they were heavily dependent on data analysis to inform their decisions . We identified participants’ common data - driven decision - making practices and distilled them into a model containing the following five stages , as illustrated in Figure 1 : 1 Acquire and Calibrate Data . This is the first stage of data - driven decision - making , where participants gathered data from various sources and tried to understand their values . Tasks . Participants performed the following tasks in this stage : • Collected data stored at various sources . • Understood data fields by searching their definitions . • Joined datasets if needed . • Identified data fields that were relevant to business goals . • Calibrated data values to determine what values were reasonable and desired for both KPIs and drivers . • Formatted data by removing and rearranging columns , changing data types , and performing filter and group operations . Tools . Participants used data warehouses , Customer Relationship Management ( CRM ) systems , and third - party analytics tools ( supplementary [ 53 ] ) for simple formatting , aggregation , and consolidation of data . They then exported the data to spreadsheet - based applications , mostly Excel , to analyze it further . Challenges . Regardless of the company size , all participants shared that they preferred data analysts to handle the above tasks , but had to perform them on their own due to a shortage of analysts . Similarly , other challenges that data analysts face—data integration , data quality , and data preparation [ 1 ] , [ 4 ] —are exacerbated for business users . Participants shared that they needed to consolidate data from two to six different sources ( supplementary [ 53 ] ) in one place to perform their analyses . The lack of technical background , combined with incompatibilities between data sources , aggravated their difficulty of joining , and toggling between the data sources . Thus the participants wanted simpler methods to integrate different sources , CRMs , and databases . As another example , participants spent a lot of time validating data quality ( “ . . . who gave the data ? , do [ we ] rely on it ? can [ we ] trust it . . . to be clean . . . and accurate ? . . . ” ( P21 ) ) and double checking the data ( “painstaking to check and recheck , again and again , plug in numbers from one source to another” ( P11 ) ) . Data quality can be adversely affected by human errors , which in turn leads to ineffec - tive decisions . The participants also shared the struggle of providing common definitions of KPI goals and drivers across multiple teams . 2 Augment Drivers and KPIs using Business Expertise . In this stage , participants augmented the initial driver and KPI data columns by incorporating their domain knowledge , so that the data was more amenable for analysis . For example , the demand / supply ratio may have high predictive power than the drivers individually ; binning a quantitative driver into discrete categories enhances it’s semantic meaning . Tasks . Participants performed the following tasks in this stage : • Derived additional columns from existing ones : Gross sales and ROI derived from existing sales and expense columns . • Added new columns from other data sources : For example , breakdown of expense across channels or campaign types and distribution of sales attained from different channels . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 6 • Simplified data by aggregating , merging , and removing columns : P2 , for example , wanted to “group [ expense ] based on low , medium , high spend because coming up with formulas [ was ] hard . . ” . • Organized and grouped columns : Copied tables in multiple sheets , each having different sets of columns for different focuses . For instance , P1 maintained three tables – overall budget ( money they get and spend ) , budget breakdown ( spending across advertising channels ) , and running campaigns ( trails , emails , and so on ) for the MMM use case . Tools . Spreadsheet - based applications , especially Excel , comprised the primary tools for this stage . Challenges . This stage was not particularly challenging for the participants but came with some concerns like the participants were not always sure what additional columns related to the KPI goals and desirable for their analysis : should investment and reach on a channel be used as two separate drivers , or should a derived column of their ratio be added ? Another challenge was dealing with overlapping drivers . For instance , it was unclear to P19 what the difference was between two related data columns , “calls” and ”meetings” . They either needed to search for the definitions online , or ask others in their team , or test out combinations ( e . g . , should they perform more calls or more meetings or both ? ) Despite these concerns , the participants clearly stated that they preferred performing the tasks on their own without relying on data analysts because the analysts often lacked the needed domain knowledge and experience and that communicating them to the analysts was time - consuming . 3 Understand and Model Driver - KPI Relationships . In this stage , participants tried to understand the relationships between the independent driver variables and the dependent KPI business goals to form hypotheses about driver variables as set of decision actions . Tasks . Participants mentioned the following tasks in this stage : • Frame Questions : Given the KPI goals ( e . g . , maximizing sales , optimizing media mix ) , the participants first developed decision questions such as “what is the relation between the drivers and the KPI ? ” ( P22 ) , “what penetrates” ( P15 ) or “is driving” ( P17 , P20 ) the sales , and what to do to “diversify spending [ across channels ] to minimize diminishing ROI” ( P3 ) . Many of these questions were about exploring plausible scenarios in the style of what - if analysis . • Formulate Hypotheses : Often the participants had preconceived hypotheses of how the drivers behave wrt the KPI goals based on their experience , expertise , and external happenings in the world . For example , multiple participants ( P1 , P3 , P4 ) shared hypothesis with P8 that they “ . . . expect [ ed ] newspaper ads to be less effective than internet” . • Translate Hypotheses into Models : The participants then refined their hypotheses into simple quantitative models . As an example , a marketer ( P19 ) tried to optimize the impression rate ( i . e . , number of customers reached or tapped by a marketing action ) for their product . To do so , they sought to understand the relationship between marketing channels ( e . g . , ads , website ) and the impression rate KPI by weighing how much each channel was contributing towards one impression . They first came up with a rough hypothesis that when a potential customer looked at two paid media ads and visited the website , they were more likely to have a complete impression which was translated into a simple linear formula ( e . g . , 2 × paid media ads + 1 × website visit = 1 impression ) . • Revise Models with Data : The participants often played with the simple models or formulas , varying parameters based on intuition and trial - and - error strategies . Data was an essential part of this experiment , and the participants relied on existing BI reports and data visualizations , as well as raw data , to revise their models . For example , after observing additional data and formulating and playing with more models , P19 realized that watching their website video led to a more successful impression and changed their formula to 2 × paid media ads + 1 × website visit + 1 × video completion rate = 1 impression instead . This example illustrates business users’ need to test multiple driver - action scenarios [ 15 ] , [ 41 ] and observe it’s impacts on a given KPI for the purpose of their decision - making . • Explore Multiple Scenarios through What - if Analyses : So far , the higher - level goal was exploring what - if scenarios . Overall , more than a third of the participants tried to manipulate the driver or KPI values and observed the effects on the other variables in the model , and the other participants were interested in such advanced analytics . Examples of what - if analysis mentioned in our interviews included : – 17 out of 22 participants wanted to understand which drivers ( e . g . , advertising channels ) were influencing the sales KPI which is an example of driver importance analysis . – Observing the impact of changing a driver ( e . g . , investments ) on the KPI ( e . g . , sales ) . For example , P7 wanted to know if “they should do something to the unit price so the return [ sales ] is better ? ” . This process is an example of sensitivity analysis [ 55 ] . – The participants came up with a set of driver actions to trigger the desired changes their KPI goals . P7 , for example shared “ . . . send to different email , . . . change subject line , . . . colors , . . . time” actions to run an email promotional campaign to get “1000 more downloads” . This is similar to the simplest form of goal - seeking functionality ( GOAL SEEK in Excel ) . – Participants explicitly shared budget and resource constraints while making decisions . For instance , P2 shared they “ob - viously have a limited budget about what campaigns to do . . . ” . This is an example of constrained analysis functionality . • Formulate an Initial Set of Actions : After exploring multiple scenarios , participants formulated executable decision actions based on their model - driven analysis . For example , based on P19’s revised model , the initial decision actions would be doubling resources on paid media ads , having the sales managers push prospective customers to visit the website and see the complete video explicitly when they call or meet with the prospective customers . Tools . All the participants performed analysis using spreadsheet - like applications , primarily Excel . This is a significant difference between business users and data analysts . The latter employ wider range of tools from programming environments to off - the - shelf machine learning models [ 1 ] , [ 2 ] , [ 4 ] . Operations on spreadsheets included filter and pivot to manually slice and dice the raw data in different ways . Only one participant , P10 , reported using advanced functions ( e . g . , LOOKUP , VLOOKUP to retrieve data from large tables ) and features ( e . g . , GOAL SEEK ) but also highlighted a lack of time to set up and use the complex features . The participants also created charts to observe patterns ( e . g . , seasonal trends ) , behaviors ( e . g . , sales best in the west region ) , or anomalies ( e . g . , sudden sales drop every 5 years ) . Only 5 out of 22 participants visualized their data in dashboards ( static and interactive ) using a variety of BI tools like Power BI , Salesforce , JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 7 and Tableau , while other 17 participants used Excel charts . For those who had access to data teams in their companies ( 8 out of 22 ) , common reports generated by the data teams were used on a daily , weekly , or monthly time - frame . Additionally , they also expressed communicating using reports that were generated by yet other tools such as Google Analytics , Marketo , and Pendo . Participants who could not get customized reports easily from data teams often struggled with using the BI tools on their own . Of the 22 participants , 17 mentioned that their decision - making processes required advanced and complex analyses , but they relied largely on manual processes . Examples of advanced analyses mentioned include correlation analysis between drivers , correlation analysis between drivers and KPI , outlier analysis , trend analysis , and segmentation analysis . However , they performed these analyses using very simple and manual processes like updating of tables and using formulas . Challenges . Even though participants in our study claimed that the primary spreadsheet - based tools they used were “super powerful” ( P11 ) , they also cautioned that the tools “need a lot of experience with macros , etc . which not everyone has ! ” ( P11 ) . The participants also found these tools to be inefficient , time - consuming , and prone to human errors . P11 shared the frustration of dealing with extremely large boilerplate Excel sheets made 20 years ago by some individual and how “absolutely non - replicable the process is and how the person who developed it is still contacted at times to understand some niches” . Some participants liked using BI tools because they enabled visual data exploration . Multiple participants praised Tableau calling it “spectacular” ( P5 ) and “Excel on steroids ! ” ( P4 ) . However , they expressed that BI tools were mostly used for the exploratory visual analysis and rarely for anything more like advanced analytics . Additionally , most participants complained about having to learn to use the BI tools on their own and the lack of storing analysis states for future use . Similarly , P3 found these tools to be “scarily powerful” and many participants also showed a strong dislike for them because they “need [ ed ] to have a developer support to use it . . . and extensive training” ( P4 ) and was “clunky . . . [ with the ] filters , . . . not very user friendly compared to others” ( P5 ) . A participant , an employee at one of the BI tool companies also shared how they “are not proud of not using [ their tool ] , but it does have a steep learning curve” ( P21 ) . We learned that current descriptive dashboards and exploratory BI tools were static or minimally interactive , informing them of only the statistics of the data , the percentage of goals achieved , and a few drilled - down scenarios , and did not help in understanding driver - KPI relationships . Therefore , participants did not find these tools to be sufficient on their own to generate and confirm their hypotheses . They wanted more control over the reports to use the numbers and insights more proactively . Further , participants expressed that their analysis process of creating hypotheses and representing them as models was highly complex , tedious , and challenging . Participants shared that they needed to incorporate many driver variables ( at least 10 to 25 ) and business factors ( e . g . , budget constraints , market conditions ) in their analysis . It was cognitively demanding to deduce relationships between the drivers and goals , formulate them as decision actions , and justify the reasonings . The different possible combinations of these variables led to an explosion of possible decision action scenarios that exacerbated the problem [ 13 ] . Data analysts lack the business domain knowledge and expertise needed not only to define KPIs and drivers , as discussed previously , but also to quickly experiment with a multitude of what - if driver - KPI scenarios . This made it even more challenging for business users to rely on data analysts , and so participants performed manual analysis on their own and lacked usable decision - making systems [ 4 ] , [ 15 ] . They had to rely on their intuition , experience , and expertise , which were valuable in making decisions [ 56 ] , but also trial - and - error based strategies which were inefficient and error - prone resulting in less confidence in the decisions . 4 Communicate with Stakeholders . In this stage , participants communicated their hypotheses , analyses , and decision actions with stakeholders ( i . e . , their team , executive managers , and other business teams ) to incorporate feedback and align opinions before getting final approval from the higher - level executives to implement the decision . Tasks . Most of the participants sought feedback from the senior members of the team and companies who might have made similar decisions previously and were aware of potential difficulties ( e . g . , personal biases , shortcuts or oversights in decisions ) . Decisions were always revised based on feedback , through re - analysis , before implementation . Participants also needed to communicate their proposed actions to the company executives , who were in charge of approving the decisions . Tools . The participants remarked that communication was often ad - hoc and mostly happened over team stand - ups or one - on - one meetings . They mentioned that they were expected to give presentations and reports about their decisions , backed by analysis and data . The participants visualized their findings using Excel charts or BI tools like Tableau and Power BI , which they populated onto presentations and reports . Challenges . Participants expressed that getting approval for the decision actions from executives was challenging since decision - making at enterprises was driven in a hierarchical manner . For example , P10 complained that oftentimes their “solutions [ action decisions ] are overwritten” . To push for the approval of their decisions , participants additionally found it difficult to formulate a strong narrative to justify their hy - potheses and actions formed in the previous stage to the stakehold - ers for its execution . These narratives could not be based on mere intuition or trial - and - error strategies , and needed support from data . There were no formal means to systematically manage and review the feedback from the stakeholders , which included changes in strategic goals , newly introduced constraints ( e . g . , fed rate hikes , bank collapses ) , or previous experiences when taking similar actions . Since the feedback was in ad - hoc forms like verbal conversations , notes , emails , or messages , it was easy to misinterpret and lose track of them . As a result , the process was prone to mistakes and required rounds of iterations . 5 Validate Feasibility through A / B Testing . Finally , the participants validated the feasibility of the decision actions by conducting A / B testing before finalizing the decisions and executing them at full scale . Tasks . The participants reached out to one or more teams to collab - oratively implement the proposed decision actions as A / B tests in controlled environments . The testing usually started with a sample set of 6 - 10 customers for a period of 1 - 4 weeks , where the impacts and results of the decision actions , as well as qualitative feedback JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 8 ( e . g . , action worked / didn’t work , liked / not liked , etc . ) were captured for further analysis . The A / B testing was conducted over at least 2 - 3 iterations , where the pool of customers was increased by 20 - 30 with every iteration . The participants concluded their test once the actions seemed to succeed in achieving the KPI goals for a given time period ( e . g . , quarter ) . More analysis and tests were again per - formed in subsequent quarters to attain subsequent business goals . Tools . The participants used CRM and Enterprise Risk Management ( ERM ) softwares to find customers for A / B test and report their feedback after the test . Emails , messaging , team stand - ups , and one - on - one meetings were used to communicate with collaborators . Challenges . Many business teams are involved in running the A / B test , and communication across all the teams is a major challenge . For instance , if the marketing team came up with a decision to improve the sales of the software product by increasing online presence , they may have the sales team run an email campaign to attract customers , the product team run demo and trial series , the sales team capture customer feedback , loop it back to the product team , and then iterate this cycle . Similar to the previous stage , managing and reviewing customer feedback continued to be a challenge . Customers feedback received was very important as it captured the difficulty with current products and external constraints like company growth ( e . g . , increase in the number of customers , products ) and economic factors ( e . g . , inflation , war ) that must be absorbed into future features and products . Such feedback from customers was often collected verbally by the sales managers—who only sometimes added them ( manually ) to CRMs and ERMs—and eventually reported to the product and marketing teams . This incurred the additional challenge of time - consuming execution of decisions , since business users had to run multiple A / B tests . Moreover , since these A / B tests and data were not formally recorded , they were not replicable and had to be performed from scratch further adding time to make decisions . The participants emphasized that their decisions were “not set in stone” ( P17 ) and their mental models kept “constantly evolving” ( P13 ) with every iteration . Hence , this uncertainty made decision - making difficult . Variation in Participants’ Processes . Although all participants shared of the five stages , their processes varied in stage importance and stage transition . Some participants emphasized certain stages more than others . For instance , participants from smaller companies ( P3 , P13 , P17 ) spent majority of their time acquiring and calibrating the data ( Stage 1 ) . For participants at larger companies , data acquisition was relatively easy but calibrating data was still difficult . Participants from smaller companies reported spending less time in understanding and modeling driver - KPI relationships ( Stage 3 ) . Transitioning between stages also differed . Most participants shared performing multiple iterations of Stage 5 using the qualitative feedback from A / B customer groups , while a few made multiple iterations over both Stages 4 and 5 before making a decision . Very few revisited earlier stages to retest their hypotheses . These variations mostly occurred due to the differences in team and company sizes , which also influenced the decision - making time . 3 . 4 Takeaways from the Interview Study Data - Driven Decision - Making without Relying on Data Ana - lysts . All participants shared that companies , regardless of their size , lacked sufficient analysts to handle business users’ requests . They mentioned that data teams in their companies also have limited bandwidth and prioritize company - wide initiatives ( e . g . , “developing models for churn rate analysis” ( P4 , P5 ) , “customer ordering patterns” ( P6 ) , “staff hiring models” ( P5 , P6 ) ) and automation over their needs of data preparation and ad - hoc analysis queries . Participants who could obtain help from analysts expressed frustration due to their lack of domain knowledge and experience . They also found the communication to be inefficient and “a lot of added work [ being an inconvenience ] in itself” ( P3 ) . Additionally , due to the business pressure to make quick decisions , business users often did not have the time or patience to engage with analysts . Therefore , the participants often performed analysis on their own without relying on data analysts or data scientists . 20 out of 22 participants reported that they spent 1 - 4 hours every day analyzing data and the remaining two spent 4 - 6 hours per day . These findings however contrasted with previous work that presented ideal collab - orations between business users and data analysts in a variety of industries [ 57 ] , [ 58 ] , though requiring clear communication , shared understanding , and alignment of goals between the two parties . Unique Challenges Beyond Data Analysis . While some tasks and challenges described in our model overlap with those previously found by data analysts [ 1 ] , [ 2 ] , [ 3 ] , unique ones exist for business users . Business users need to balance multiple business aspects like budget constraints , economic laws ( e . g . , increasing unit price of a product beyond a threshold will only decrease sales ) , market conditions ( e . g . , COVID effects , high interest rates ) , and social conditions ( e . g . , work - life balance of employees ) , all under the pressure of making accurate decisions quickly . Advanced Analytics using Rudimentary Methods . The challenges even of data analysis alone are exacerbated for business users due to their lack of technical expertise . In Section 3 . 3 , we detailed how business users heavily relied on spreadsheet - based applications and descriptive and exploratory tools to perform not only data preparation tasks but also advanced analytics . However , these tools are often insufficient to support the decision - making process , especially participants’ need to frequently explore what - if scenarios ( Stage 3 ) . This highlights vast opportunities for the visual analytics research community , particularly in predictive and prescriptive analytics ( PPA ) tools [ 4 ] , [ 15 ] , [ 23 ] , [ 41 ] . These tools directly address the problem of understanding how changes in independent variables impact outcomes and identifying optimal variable values to achieve predefined outcomes . 4 F OLLOW - U P T ASK - B ASED S TUDY Motivated by business users’ desire for independent data - driven decisions and advanced analytics , we conducted a follow - up study to understand the role and challenges of prescriptive and predictive analytics ( PPA ) in stage 3 of our data - driven decision - making model . We focused on stage 3 as visual analytics solutions directly impact primary tasks and address challenges in this stage . There - fore , we use a task - based method , where the participants perform predefined subtasks for MMM using a visual analytics system . 4 . 1 Methods . Figure 3 illustrates an overview our follow - up study methods . Participants and Protocol . The same 22 business users from the interview study participated . We met with each participant again individually in a separate recorded remote session , each lasting no longer than 60 minutes . Participants received a $ 175 Amazon gift card for their time in both studies . After informing them of the study’s aim , we introduced them to a visual analytics prototype ( hosted on AWS ) that we implemented prior to the study . The proto - type operationalized advanced analytics via four example predictive JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 9 Fig . 4 . Interface for the four - example PPA functionalities running the MMM business use case : ( 1 ) driver importance analysis bar chart ( A ) with toggle - able drivers ( B ) , ( 2 ) sensitivity analysis perturbation pane ( C ) , bar chart ( D ) , and uplift pane ( E ) , ( 3 ) goal - seeking analysis prediction ( F ) , ( 4 ) constrained analysis options and constraints pane ( G ) , and prediction ( H ) , and the R 2 accuracy score pane ( I ) . and prescriptive analytics ( PPA ) functionalities : ( 1 ) Driver Impor - tance Analysis , ( 2 ) Sensitivity Analysis , ( 3 ) Goal - Seeking Analysis , and ( 4 ) Constrained Analysis . These functionalities correspond to Stage 3 analysis tasks of our model and thereby served as a probe to solicit participants’ thoughts on the role and potential of PPA to sup - port data - driven decision - making . The functionalities and prototype we employed for business use cases can support broader audiences ( e . g . , finance experts , supply chain operators , climatologists ) interested to analyze the behavior of a specific dependent variable ( i . e . KPI ) in relation to other dependent variables ( i . e . drivers ) . Each participant watched a 15 - minute recorded demo video [ 53 ] ( for consistency ) explaining the tool usage and example functionalities via another realistic business use case of deal closing analysis —addressed by sales managers to increase the rate of acquiring customers by analyzing the prospect interaction data with the product and the organization . Then , they were given the same Marketing Mix Modeling ( MMM ) business use case as in the interview study to achieve the business outcome of maximizing TV sales using PPA functionalities of the tool . We structured the study to have participants solely focus on the data - driven analysis part of the process ( Stage 3 ) , thereby use the PPA functionalities , and not the other stages . Based on a pilot study with 3 participants ( not included in the 22 ) , we gave each participant 25 minutes and broke down the task into 10 subtasks to ensure manageability within the time limit . The participants were asked to complete as many subtasks as possible and to think out loud . We provided a document outlining each subtask and offered assistance when participants faced challenges . An example of subtask was “ using the driver importance analysis functionality , what are the top three and least three drivers driving the sales ? ” and “ using the sensitivity analysis functionality , what is the sales achieved if you increase the Demand driver by 6 % and the Supply driver by 3 % ? ” To capture their daily analysis processes beyond the given subtasks , we also asked the participants to share how each subtask might translate to decision - making processes in their own work . Following completion of the task , participants were asked for their feedback on the role , potential benefits , and concerns of incorporating advanced analytics into decision - making , as well as their trust , confidence , and perceived effectiveness in an open - feedback session . Finally , the participants completed a post - study questionnaire [ 59 ] that compared their existing decision - making experiences with PPA - supported experiences . Details on all these are included in our supplementary materials [ 53 ] . Analysis . We observed participants’ use of the PPA functionalities , tracked their subtasks completion , enquired about PPA functionalities integration into their processes ( Section 4 . 3 ) , and gathered feedback on its role , potential ( Section 4 . 4 ) , and concerns ( Section 4 . 5 ) in decision - making . We applied the same iterative coding analysis method to the follow - up study data to come up with the foregoing themes . 4 . 2 Description of the Four - Example PPA Functionalities Before discussing participants’ feedback on PPA , we describe the four PPA functionalities used in the follow - up study using the MMM business use case assigned to the participants in the study ( see demo [ 53 ] ) . 1 Driver Importance Analysis . This functionality helps understand the relative importance of driver variables ( as hypothesized by business users ) in predicting the values of business KPI , which was often requested from participants in the interview study , For example , P3 wanted to “choose the top three advertising channels and then distribute expenses there” . The driver importance analysis interface shows an ordered list of drivers ( Figure 4B ) , with a bar chart ( Figure 4A ) on the left indicating the computed importance values . The top three and bottom three drivers for the sales KPI were CCI , PPI , and Supply , and Newspaper Ad Expenses , GRP of Radio , and GRP of Newspaper respectively . An alternative interface design is to show the importance values as a waterfall chart , as in Einstein , but the participants were more comfortable reading simple bar charts according to our interview study . 2 Sensitivity Analysis . This functionality helps observe the effects of perturbing ( increasing or decreasing ) driver values on KPI , addressing a common request from multiple participants in the interview study to manipulate the driver values and inspect how the KPI value changes ; for example , P19 wanted to know “which mix [ i . e . , combination of channels ] is producing most revenue ? ” . Existing tools like Google Analytics , Power BI and Salesforce answer these questions as text - based insights or narratives , or require users to manually adjust driver values and set up own JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 10 formulas , which participants shared to be highly cognitively demanding in the interview study . To this end , the sensitivity analysis interface is in the form of coordinated views , comprising a perturbations pane ( Figure 4C ) , a bar chart ( Figure 4D ) , and an uplift pane ( Figure 4E ) . The average sales observed on the original data was 96 million dollars ( blue bar ) ; upon perturbing the “supply” driver by 3 % and “demand” driver by 5 % , the predicted sales increases by 3 . 63 % ( equivalent to 3 . 5 million dollars ) ( green texts ) , making the overall predicted sales 100 million dollars ( yellow bar ) . 3 Goal - Seeking Analysis . This functionality helps predict the required driver values to achieve an optimum or user - desired aggre - gate KPI value . In the interview study , participants shared how they wanted tools that directly identify the actions needed to maximize their KPI . Such a functionality only exists as the GOAL SEEK feature in Excel , which is complex and requires manual set up . This goal - seeking analysis interface used in our study ( Figure 4F ) is interactive , where users choose their optimization goal ( maximize , minimize , or set a target goal ) , observe the pre - specified constraints on all drivers to initially be optimized between 0 % to 300 % , and run the analysis . On maximizing the KPI goal , a prediction of 549 . 07 million dollars sales can be achieved ( 470 . 11 % uplift ) along with the values of the drivers that enable this prediction . 4 Constrained Analysis . This functionality tries to incorporate users’ domain knowledge in the form of business constraints ( e . g . , boundary , equality or inequality conditions ) on one or more drivers and run goal - seeking analysis to get driver values satisfying these constraints . In the interview study , the participants mentioned the need to include budget and resource constraints in decision - making . Currently , they hypothesized driver values by calculating and adjusting to their constraints manually in their data analysis . The constrained analysis functionality allows users to specify bounds on driver values so the the real - world business logic is enforced . For example , when using the previous goal - seeking analysis functionality , users observed that in order to maximize sales , the “unit price” driver needed to increase by 297 . 81 % or 74000 dollars , which was customers would not practically pay . In response , they could add a constraint on the “unit price” driver to restrict its change between 10 % to 15 % in the constraints pane ( Figure 4G ) and run constrained analysis . The new decision solution predicted the “unit price” driver to change by 14 . 08 % ( Figure 4H ) , which was within users’ specified constraints . The sales prediction drops accordingly from 549 . 07 million dollars to 277 . 84 million dollars , still uplifting the original sales by 188 . 49 % . Each functionality was also supported with accuracy ( R 2 measure ) or error scores ( error rate ) of the underlying models used for making the predictions . MMM use case shows a fair accuracy score of 0 . 82 ( Figure 4I ) . 4 . 3 PPA - Driven Decision - Making Approach We learned from the open - feedback session and post - study question - naire that all participants saw the benefit of adopting PPA for their own decision - making . Every participant finished all 10 subtasks of the MMM use cases within the allotted time comfortably . All participants made similar decisions to increase TV’s unit price , increase spending on SMS advertising , and remove spending on Newspaper advertising . Participants found the functionalities to be simple and easy to understand . All but one participant were excited about PPA , noting the functionalities to be super “helpful” ( P2 , P10 , P12 , P17 , P20 ) , “interesting” ( P3 , P8 ) , “powerful” ( P13 , P16 ) , and quick to “set [ them ] in the right direction” ( P7 , P17 ) . The remaining participant , an operations manager , found the func - tionalities to be “okay and wanted [ them ] to be more dummy - proof” ( P15 ) suggesting their usefulness , but need for domain specificity . Fig . 5 . ( A ) Ratings of the four example PPA functionalities from most to least useful . Sensitivity analysis is the most popular functionality . ( B ) Desired PPA functionalities to achieve the goal of maximizing sales in the MMM use case . Figure 5A shows the number of participants ( within each bar stack ) who ranked each of the four example PPA functionalities from most useful ( rank 1 ) to least useful ( rank 5 ) ( colored in saturation from most to least ) . The participants ranked all the functionalities useful , with the sensitivity analysis being the most useful . Constrained analysis was ranked second , followed by goal - seeking analysis and driver importance analysis respectively . However , a quarter of the participants needed additional guidance and clarification on the goal - seeking analysis functionality . For example , P7 was “confused [ and wanted ] explanations [ again ] . ” Participants provided mixed opinions on the accuracy or error scores of the model . Multiple participants echoed P4’s thoughts that “it was easy to ignore the accuracy score especially when . . . prediction is what is wanted” but also commented on PPA being “a help nonetheless” ( P10 ) and “better than sitting looking at the data” ( P9 ) . The participants’ need of advanced analytics to perform data - driven decision - making was further validated by their feedback in the post - study questionnaire , since all participants found PPA to be very useful in understanding the behavior between the data drivers and business KPIs . They also mentioned that it was better in comparison to their existing workflows . The participants were excited to apply the functionalities to their own decision - making , to “play around with own data [ since it would be ] super helpful when setting goals for the quarter” ( P3 ) and learn data behaviors like “how it can relate to their business” ( P6 ) , “how it would be useful for drilled - down segments [ subsets of their data ] ” ( P4 ) . The participants also were interested in exploring additional functionalities of PPA as shared in the open - feedback session , including correlation analysis and comparison analysis ; see Figure 5B . In addition , the partici - pants also wanted additional enhancement of PPA functionalities . For instance , P21 wanted the ability to integrate all of the example functionalities to one another in a seamless way like plugging in the predictions from goal - seeking analysis back into sensitivity analysis and then continuing analysis . P1 , P8 , and P14 also wanted methods to store their analysis results at desired states in order to come back to them for comparison with other analyses later . 4 . 4 Potential of PPA in Decision - Making Helps Accelerate Decision - Making . Participants appreciated the ability of PPA to provide a “holistic view” ( P13 ) of driver - KPI relationships unlike the manual means of attaining it in spreadsheet - like tools . More importantly , participants were happy that they could quickly observe the predictions for many scenarios consisting various combination of drivers all together without any external expertise of data analysts or teams . Consequently , all participants thought PPA would speed up their decision - making and “increase [ their ] efficiency” ( P6 ) . Participants were also surprised by the capabilities of PPA since they may have seen “something similar but not anywhere close to this [ PPA ] . . . to customize it or make your own if you want anything like this [ PPA ] . . . ” ( P11 ) . Therefore , participants suggested how PPA will be a huge win to accelerate their decision - making process . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 11 Increases Confidence in Decisions . The participants expressed that PPA would help them move away from guessing and trial - and - error strategies , which would boost their confidence in their deci - sions [ 60 ] by narrowing down the vast possibilities of hypotheses to those observed in the data . For instance , P1 happily shared how PPA can “help understand where to put more effort ! ” Moreover , participants shared that using PPA , especially in interactive ways like in our follow - up study , could help them think and test out a lot more corner cases and scenarios that they might miss in current manual analysis . Further , participants shared how they could play with many more drivers to observe their consequences on the business KPI and be able to understand the market , product , and customer behavior more widely and better . All these reasons combined led the participants to excitedly share how they can build a strong narrative or story supporting their decision actions . P10 for example expressed how PPA will enable “everyone in the company [ to ] be on the same page about a decision . . . no one’s decision will be overwritten” . Therefore , participants showed promise in PPA enabling them to be confident in their decisions and ready for the upcoming stages of the process like communicating with the stake - holders ( Stage 4 ) and validating them with A / B testing ( Stage 5 ) . Standardizes Decision - Making and Facilitates Collaboration . Almost all participants agreed that their existing analysis processes could not be replicated easily to justify the decision actions for the A / B testing iterations . For instance , multiple participants commented that their current decision actions were based on “trial and error” ( P3 , P14 , P19 , P21 ) , which they often deemed to be inefficient and challenging to rely on . The participants therefore saw value in using PPA to make it easier to perform , replicate , and fol - low up on the decisions made and taken . This consequently allowed for “consistent” ( P3 , P10 ) [ 1 ] and “smoother” ( P22 ) decision - making . Further , in addition to PPA helping them in their own analysis , participants also added that it will help facilitate collab - orative decision - making and make it simple for all stakeholders to reason on decisions together . For example , P4 mentioned how they could “take [ this ] to their job today without the support of IT . . . also give it to other team members” . Therefore , participants explained that PPA would help make robust decisions as business users could get external perspectives and benefits from others’ experiences and expertise through standardized and collaborative analysis processes . 4 . 5 Concerns Regarding Use of PPA in Decision - Making Currently Inaccessible to Business Users . Many participants like P1 , P7 , P8 mentioned that even though they had heard of features similar to PPA in existing commercial BI tools ( e . g . , key influencers in Power BI , importance and correlation analysis in Einstein , etc . ) , they had “no idea or not know too much [ about them ] or barely used them” . Participants reasoned that the cost of such tools were too high . Participants also expressed concerns that the tools often comprised of many options of models and parameters to choose from . It was difficult to maneuver due to an overwhelming number of features . The automatically generated results could be too dense , and lacked explanations for users to draw conclusions . All these required technical skills that business users did not readily have . Moreover , business users lack the time and incentives to learn or to keep up with such technologies , which are rapidly evolving . Business users are definitely excited to use PPA in their own work , but it seems that tools are not currently developed specifically to meet their needs . The vast space of possible scenarios seems to be a major problem in particular . P15 , for instance , frustratingly mentioned how they have “ . . . to [ currently ] commit to just solving the first thing . . . that is going to be the most bang for the buck . . . . [ because ] . . . have 900 problems happening all at once . . . ” . Such expressions inform us that business users have no time to implement all possible scenarios but need to quickly identify the first set of actionable decisions that yield favorable business outcomes , a concept akin to “satisficing” [ 21 ] , which PPA can help them do . Difficulties in Interpreting and Understanding Predictions . As much as participants were excited about using PPA after the follow - up study , they also made it clear that decision - making could not be based on PPA alone . Their domain expertise and business experience played a major role at every stage of the decision - making process [ 41 ] . For instance , P4 pointed to how “common human understanding may not translate to the model without [ their ] external involvement” and gave an example of how an increase of unit price over a threshold will decrease demand , thereby decreasing sales rather than increasing it . The participants emphasized that for them to use PPA recommendations effectively , they needed more information about how the predictions were made . Specifically , they wanted to learn when and to what extent should they adopt the recommendations . For example , the participants wanted clarification on what to do when PPA recommends “unrealistic [ numbers ] ” ( P5 , P6 , P22 ) , or “so many factors [ or decision paths ] ” ( P2 ) , and how to “ [ include ] factors that cannot be captured in the data” ( P12 , P17 ) . Another common concern raised was about interpreting the accuracy of the recommendations ( low v / s fair v / s high ) and translating it to confidence in the recommendations [ 60 ] . For example , should they take low scored models as worst - case scenarios and perform additional manual analysis to make decisions ? Or should they take medium scored models as baseline and expect risks in lower scored models ? While senior and more experienced business users understood that results from PPA should be taken with a grain of salt , they mentioned that it was important to train junior users to interpret predictions so that they don’t blindly follow the recommendations . Therefore , although PPA is useful , it is still in its nascent stages and requires further development in order to be used effectively by businesses . Lack of Support for Data Preparation and External Constraints . The participants in our study also shared challenges on data preparation tasks ( Section 3 . 3 ) : data gathering , consolidation , featurization ( e . g . , curating new hypothesized drivers for KPI goals ) , aggregation from multiple sources , and cleaning it to work around missing , inaccurate , and outlier data . These operations are already hard for expert analysts [ 1 ] , [ 2 ] , [ 4 ] , [ 15 ] , and our study confirms that it can be all the more daunting for business decision - making . For example , the participants raised some fundamental issues with adding qualitative data ( procured over calls , meetings , chats , and reviews ) as drivers into models , treating connected drivers ( e . g . , calls and meetings ) , and resolving ambiguities about the drivers or KPIs as they could mean different things to different teams at times . Therefore , participants suggested co - designing and tight integration of PPA functionalities with data wrangling and preparation systems . The usefulness and power of functionalities like goal - seeking and constrained analysis were acknowledged by the participants for incorporating their domain expertise . However , they worried about the mechanisms “on setting the right constraints , coming up with them , how to set them right and optimize on that” ( P5 ) . On a related note , some participants expressed concerns over not being aware of ideal conditions for the team and company , thereby gauging the constraints either conservatively or over - confidently . For example , P5 mentioned only knowing that they “had five million dollars for advertising . . . [ and ] can set only overall constraint over all channels” making it challenging to set constraints on each of the channels . Therefore , participants requested help setting constraints on the data . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 12 Further , participants also wanted to incorporate economical ( e . g . , inflation , fed rate hike ) and social ( e . g . , COVID , war , street protests ) constraints into PPA ( P6 , P9 , P10 , P15 ) since they have had very significant impacts on their businesses . Participants often dealt with these constraints for making decisions by turning to their intuition [ 41 ] and domain expertise . As a consequence , participants indicated that PPA in its current form cannot be relied on to navigate through decision - making but can definitely “use it as an adjunct” ( P8 ) . Therefore , future tools need to focus on feedback mechanisms between business users and advanced analytics like PPA which will allow users’ expertise as well as external constraints to be integrated into predictions [ 24 ] , [ 40 ] . 5 I MPLICATIONS FOR D ATA AND V ISUAL A NALYTICS Our studies confirm that business users are highly motivated to make data - driven decisions independently and that advanced ana - lytics like PPA can significantly benefit them . We acknowledge that our limited participant pool , covering four types of business users , the specific MMM use case , and four example PPA functionalities only partially represent business users , use cases , and advanced ana - lytics at large . Nonetheless , our results help in informing the design of future data and visual analytics systems , which we discuss below . Enhance Interpretation and Confidence . It is clear from our study that business users are not naive . They are quite savvy in scrutiniz - ing the decision recommendations from automated tools with their domain expertise and common sense . All participants agreed that recommendations from PPA could not be followed blindly [ 41 ] . They were aware that predictions were modeled from data , and since their data did not represent all possible constraints and domain knowledge , predictions would not be 100 % accurate . However , they wanted to use advanced analytics to assist and complement their decision - making processes and back their actions with concrete data . To ensure that users are comfortable with data - driven analytics guiding their decision - making , tools should convey confidence levels in the predictions and provide users with means of calibrating the confidence with actionable next steps . For example , participants highlighted the replacement of accuracy or error scores of the model with risk factors communicating possibilities of negative outcomes like financial loss numbers , new competitors , or new products that may enter the market . They also desired guidance into what to do next for various risk factors ( e . g . , medium risk factor means gather - ing more data and then re - running the analysis ) . Recent studies try to understand general users’ perceptions of trust and performance metrics [ 24 ] , [ 40 ] , [ 61 ] . Conducting such studies specifically for business users can help understand what helps them trust recom - mendations better and , as an extension , also help learn how to lever - age PPA better in their service . Further , it would be interesting for future work to study the needs and challenges of users who are al - ready technically proficient in the context of making optimized busi - ness decisions . This provides a large scope for the VIS community to conduct deeper studies and for the data and visual analytics com - munity to develop PPA techniques better suited for business users . Facilitate Communication and Collaboration . All participants agreed that PPA will help business users back their decision actions with historical data and give them a voice in navigating the decision - making hierarchy ( Section 3 . 3 Stage 4 ) . Since business users need the involvement and approval of executives , it is important for them to convincingly deliver narratives around the data that led them to their proposed decision actions . This is already challenging based on current practices , given that both the business users and the executives are not data scientists or machine learning experts . Introducing PPA into the decision - making process will exacerbate this problem . In addition , business users also need to incorporate the experience and feedback of collaborators into their models so that mistakes are not repeated [ 62 ] . This entails vast opportunities for visual and interactive techniques and tools to facilitate com - munication and iteration on the processes and results of automated analyses . Next - generation natural language - based question - and - answering interfaces for analysis and interactive storytelling platforms must incorporate support for explaining and modifying PPA results to facilitate communication and collaboration . Scale What - if Analyses with Decision Management Support . Participants shared that as the number of drivers and constraints increased , they would experiment with an exponential number of scenarios leading to exponentially different business outcomes . Although PPA helps compute these scenarios and observe multiple decision paths quickly , it is still very difficult to manage and reuse the analyses at scale , especially with live data . To avoid business users from “getting lost in the sauce” ( P9 ) after using PPA for a few scenarios , it is critical to develop decision management systems that help monitor , track , and log their analyses . Such systems are often developed for software developers [ 63 ] , [ 64 ] , data analysts [ 65 ] , [ 66 ] , [ 67 ] , data scientists [ 68 ] , [ 69 ] , [ 70 ] , and statisticians or researchers [ 71 ] , [ 72 ] , [ 73 ] , [ 74 ] but none for domain users like business users . Additional support for ranking , exploring , comparing , and recommending decision paths generated from PPA will be essential for business users to incorporate PPA into their decision - making . Furthermore , it is critical that these systems and applications are developed in ways that business users can use them . For instance , complex features or programming languages will not be useful for business users . Therefore , besides the vast scope of ML modeling and optimization research , there is a huge opportunity for user experience and data and visual analytics communities . 6 C ONCLUSION In this paper , we first presented our findings from an interview study on business users’ data - driven decision - making practices and challenges . Business users’ tasks and challenges differ from that of users performing data analysis in notable aspects . For example , business users need to experiment with multiple what - if scenarios , take internal and external business constraints and socio - economic factors into account , and evaluate optimal course of actions to take effective decisions . The findings show business users typically conduct data analysis unassisted by data analysts due to the lack of data analysts and the inefficiencies of collaboration and communication . Despite their lack of formal technical training , they can benefit from advanced analytics like predictive and prescriptive analytics ( PPA ) since the current methods and tools that they rely on are insufficient for their decision - making processes . To understand how PPA helps business users address these issues , we discuss the findings of a follow - up study conducted to better understand the present and viable potential and challenges of PPA in data - driven decision - making . These findings point to the promise of PPA in improving the efficiency and confidence in decision - making . However , business users also feel that the usability of PPA functionalities needs to improve to provide better support for preparing and integrating datasets , evaluating risk factors , and incorporating domain knowledge and external factors . A CKNOWLEDGMENTS We thank Joe Hellerstein , Arjun Srinivasan , and the anonymous reviewers for their helpful feedback on the paper draft . JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 13 R EFERENCES [ 1 ] S . Kandel , A . Paepcke , J . M . Hellerstein , and J . Heer , “Enterprise data analysis and visualization : An interview study , ” TVCG , vol . 18 , no . 12 , pp . 2917 – 2926 , 2012 . [ 2 ] S . Alspaugh , N . Zokaei , A . Liu , C . Jin , and M . A . Hearst , “Futzing and moseying : Interviews with professional data analysts on exploration practices , ” TVCG , vol . 25 , no . 1 , p . 22 – 31 , jan 2019 . [ 3 ] E . Kandogan , A . Balakrishnan , E . M . Haber , and J . S . Pierce , “From data to insight : work practices of analysts in the enterprise , ” CGA , vol . 34 , no . 5 , pp . 42 – 50 , 2014 . [ 4 ] A . Crisan and B . Fiore - Gartland , “Fits and starts : Enterprise use of automl and the role of humans in the loop , ” in CHI , 2021 , pp . 1 – 15 . [ 5 ] P . Mikalef , M . N . Giannakos , I . O . Pappas , and J . Krogstie , “The human side of big data : Understanding the skills of the data scientist in education and industry , ” in EDUCON 2018 . IEEE , 2018 , pp . 503 – 512 . [ 6 ] Ç . Demiralp , P . J . Haas , S . Parthasarathy , and T . Pedapati , “Foresight : Rapid data exploration through guideposts , ” CoRR , vol . abs / 1709 . 10513 , 2017 . [ 7 ] Report , “Advanced Analytics Report , 2022 - 2030 . ” [ 8 ] —— , “Data Visualization Statistics 2023 . ” [ 9 ] J . D . Little , “Decision support systems for marketing managers , ” JM , vol . 43 , no . 3 , pp . 9 – 26 , 1979 . [ 10 ] W . S . Perkins and R . C . Rao , “The role of experience in information use and decision making by marketing managers , ” JMR , vol . 27 , no . 1 , pp . 1 – 10 , 1990 . [ 11 ] F . Bergeron , L . Raymond , S . Rivard , and M . - F . Gara , “Determinants of eis use : Testing a behavioral model , ” Decision Support Systems , vol . 14 , no . 2 , pp . 131 – 146 , 1995 . [ 12 ] F . Marx , J . H . Mayer , and R . Winter , “Six principles for redesigning executive information systems—findings of a survey and evaluation of a prototype , ” ACM TMIS , vol . 2 , no . 4 , pp . 1 – 19 , 2012 . [ 13 ] G . H . Van Bruggen , B . Wierenga et al . , “Marketing decision making and decision support : Challenges and perspectives for successful marketing management support systems , ” Foundations and Trends in Marketing , vol . 4 , no . 4 , pp . 209 – 332 , 2010 . [ 14 ] O . Velcu - Laitinen and O . M . Yigitbasioglu , “The use of dashboards in performance management : Evidence from sales managers . ” IJDAR , vol . 12 , 2012 . [ 15 ] E . Dimara , H . Zhang , M . Tory , and S . Franconeri , “The unmet data visualization needs of decision makers within organizations , ” TVCG , 2021 . [ 16 ] M . Jasim , E . Hoque , A . Sarvghad , and N . Mahyar , “Communitypulse : Facilitating community input analysis by surfacing hidden insights , reflections , and priorities , ” in DIS , 2021 , pp . 846 – 863 . [ 17 ] L . South , M . Schwab , N . Beauchamp , L . Wang , J . Wihbey , and M . A . Borkin , “Debatevis : Visualizing political debates for non - expert users , ” in VIS . IEEE , 2020 , pp . 241 – 245 . [ 18 ] Y . Kang and J . Stasko , “Examining the use of a visual analytics system for sensemaking tasks : Case studies with domain experts , ” TVCG , vol . 18 , no . 12 , pp . 2869 – 2878 , 2012 . [ 19 ] J . Liu , N . Boukhelifa , and J . Eagan , “Making sense of data workers’ sense making practices , ” in CHI Workshop , 2019 , p . 4 . [ 20 ] L . R . Bartram , M . Correll , and M . K . Tory , “Untidy data : The unreasonable effectiveness of tables , ” TVCG , vol . PP , pp . 1 – 1 , 2021 . [ 21 ] H . A . Simon , “The new science of management decision . ” 1960 . [ 22 ] A . Berisha - Shaqiri , “Management information system and decision - making , ” AJIS , vol . 3 , no . 2 , p . 19 , 2014 . [ 23 ] S . Gathani , M . Hulsebos , J . Gale , P . J . Haas , and Ç . Demiralp , “Augment - ing decision making via interactive what - if analysis , ” in CIDR 2022 . [ On - line ] . Available : https : / / www . cidrdb . org / cidr2022 / papers / p49 - gathani . pdf [ 24 ] Y . Zhang , Q . V . Liao , and R . K . Bellamy , “Effect of confidence and explanation on accuracy and trust calibration in AI - assisted decision making , ” in CFAT , 2020 , pp . 295 – 305 . [ 25 ] Z . Zhang , S . Malik , S . Guo , J . Hoffswell , R . Rossi , F . Du , and E . Koh , “CODAS : Integrating Business Analytics and Report Authoring , ” in EuroVA , J . Bernard and M . Angelini , Eds . , 2022 . [ 26 ] G . Klein , B . Moon , and R . R . Hoffman , “Making sense of sensemaking 2 : A macrocognitive model , ” IEEE Intelligent Systems , vol . 21 , no . 5 , pp . 88 – 92 , 2006 . [ 27 ] D . M . Russell , M . J . Stefik , P . Pirolli , and S . K . Card , “The cost structure of sensemaking , ” in INTERACT and CHI , 1993 , pp . 269 – 276 . [ 28 ] G . Klein , J . K . Phillips , E . L . Rall , and D . A . Peluso , “A data - frame theory of sensemaking , ” in Expertise out of context : Proceedings of the sixth international conference on naturalistic decision making , vol . 113 , 2007 . [ 29 ] P . Pirolli and S . Card , “The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis , ” in CIA , vol . 5 , 2005 , pp . 2 – 4 . [ 30 ] F . C . Lunenburg , “The decision making process . ” in National Forum of Educational Administration & Supervision Journal , vol . 27 , no . 4 , 2010 . [ 31 ] E . Bruch and F . Feinberg , “Decision - making processes in social contexts , ” Annual review of sociology , vol . 43 , pp . 207 – 227 , 2017 . [ 32 ] C . Chen , “An information - theoretic view of visual analytics , ” CGA , vol . 28 , no . 1 , pp . 18 – 23 , 2008 . [ 33 ] N . E . Eric Newburger , “Visualization according to statisticians : An interview study on the role of visualization for inferential statistics , ” 2023 . [ 34 ] W . Noonpakdee , T . Khunkornsiri , A . Phothichai , and K . Danaisawat , “A framework for analyzing and developing dashboard templates for small and medium enterprises , ” in ICIEA . IEEE , 2018 , pp . 479 – 483 . [ 35 ] P . Bera , “How colors in business dashboards affect users’ decision making , ” CACM , vol . 59 , no . 4 , pp . 50 – 57 , 2016 . [ 36 ] E . Dimara and J . Stasko , “A critical reflection on visualization research : Where do decision making tasks hide ? ” TVCG , vol . 28 , no . 1 , pp . 1128 – 1138 , 2021 . [ 37 ] A . Sarikaya , M . Correll , L . Bartram , M . Tory , and D . Fisher , “What do we talk about when we talk about dashboards ? ” TVCG , vol . 25 , no . 1 , pp . 682 – 692 , 2018 . [ 38 ] M . Tory , L . Bartram , B . Fiore - Gartland , and A . Crisan , “Finding their data voice : Practices and challenges of dashboard users , ” CGA , 2021 . [ 39 ] A . Crisan and M . Correll , “User ex machina : Simulation as a design probe in human - in - the - loop text analytics , ” in CHI , 2021 , pp . 1 – 16 . [ 40 ] D . Honeycutt , M . Nourani , and E . Ragan , “Soliciting human - in - the - loop user feedback for interactive machine learning reduces user trust and impressions of model accuracy , ” in AAAI , vol . 8 , 2020 , pp . 63 – 72 . [ 41 ] D . Wang , J . D . Weisz , M . Muller , P . Ram , W . Geyer , C . Dugan , Y . Tausczik , H . Samulowitz , and A . Gray , “Human - ai collaboration in data science : Exploring data scientists’ perceptions of automated ai , ” Proc . ACM Hum . - Comput . Interact . , vol . 3 , no . CSCW , nov 2019 . [ 42 ] F . Poursabzi - Sangdeh , D . G . Goldstein , J . M . Hofman , J . W . Wortman Vaughan , and H . Wallach , “Manipulating and measuring model interpretability , ” in CHI , 2021 , pp . 1 – 52 . [ 43 ] A . Ferrari and M . Russo , Introducing Microsoft Power BI . Microsoft Press , 2016 . [ 44 ] Tableau , “What is tableau business science ? ” 2021 . [ Online ] . Available : https : / / www . tableau . com / about / blog / 2021 / 3 / what - is - tableau - business - science [ 45 ] —— , “Einstein discovery , ” 2021 . [ Online ] . Available : https : / / www . tableau . com / products / add - ons / einstein - discovery [ 46 ] SAS , “Sas visual analytics , ” 2021 . [ Online ] . Available : https : / / www . sas . com / en _ us / software / visual - analytics . html [ 47 ] Microsoft , “Solver : Microsoft Excel Documentation , ” 2021 . [ 48 ] —— , “Goal Seek : Microsoft Excel Documentation , ” 2021 . [ 49 ] R . Ballou , Business Logistics Management , ser . Prentice - Hall International Series in Management . Prentice Hall , 1992 . [ Online ] . Available : https : / / books . google . com / books ? id = Bcd _ QgAACAAJ [ 50 ] E . Jun , “Empowering domain experts to author valid statistical analyses , ” in UIST , 2022 , pp . 1 – 5 . [ 51 ] S . Gathani , Z . Liu , P . J . Haas , and Ç . Demiralp , “Online Form for Participant Recruitment . ” [ 52 ] “User interviews , ” 2021 . [ Online ] . Available : https : / / www . userinterviews . com JOURNAL OF L A TEX CLASS FILES , VOL . 14 , NO . 8 , AUGUST 2015 14 [ 53 ] S . Gathani , Z . Liu , P . J . Haas , and Ç . Demiralp , “Supplementary Material . ” [ 54 ] Kaggle , “Kaggle Datasets . ” [ 55 ] A . Rappaport , “Sensitivity analysis in decision making , ” The Accounting Review , vol . 42 , no . 3 , pp . 441 – 456 , 1967 . [ 56 ] N . H . Lurie and C . H . Mason , “Visual representation : Implications for decision making , ” JM , vol . 71 , no . 1 , pp . 160 – 177 , 2007 . [ 57 ] T . H . Davenport , J . G . Harris , and R . Morison , Analytics at work : Smarter decisions , better results . Harvard Business Press , 2010 . [ 58 ] F . Provost and T . Fawcett , Data Science for Business : What you need to know about data mining and data - analytic thinking . " O’Reilly Media , Inc . " , 2013 . [ 59 ] S . Gathani , Z . Liu , P . J . Haas , and Ç . Demiralp , “Post Study Questionnaire . ” [ 60 ] H . Elhamdadi , A . Gaba , Y . - S . Kim , and C . Xiong , “How do we measure trust in visual data communication ? ” in BELIV , 2022 , pp . 85 – 92 . [ 61 ] M . Langer , T . Hunsicker , T . Feldkamp , C . J . König , and N . Grgi´c - Hlaˇca , ““look ! it’sa computer program ! it’s an algorithm ! it’s ai ! ” : does terminology affect human perceptions and evaluations of algorithmic decision - making systems ? ” in CHI , 2022 , pp . 1 – 28 . [ 62 ] H . Spitzeck and E . G . Hansen , “Stakeholder governance : how stakeholders influence corporate decision making , ” Corporate Governance : The inter - national journal of business in society , vol . 10 , no . 4 , pp . 378 – 391 , 2010 . [ 63 ] A . Schreiber , L . von Kurnatowski , A . Meinecke , and C . de Boer , “An interactive dashboard for visualizing the provenance of software development processes , ” in VISSOFT . IEEE , 2021 , pp . 100 – 104 . [ 64 ] H . S . Packer , A . Chapman , and L . Carr , “Github2prov : Provenance for supporting software project management . ” in TaPP , 2019 . [ 65 ] Z . Cutler , K . Gadhave , and A . Lex , “Trrack : A library for provenance - tracking in web - based visualizations , ” in VIS , 2020 , pp . 116 – 120 . [ 66 ] K . Madanagopal , E . D . Ragan , and P . Benjamin , “Analytic provenance in practice : The role of provenance in real - world visualization and data analysis environments , ” CGA , vol . 39 , no . 6 , pp . 30 – 45 , 2019 . [ 67 ] E . D . Ragan , A . Endert , J . Sanyal , and J . Chen , “Characterizing provenance in visualization and data analysis : an organizational framework of provenance types and purposes , ” TVCG , vol . 22 , no . 1 , pp . 31 – 40 , 2015 . [ 68 ] M . H . Namaki , A . Floratou , F . Psallidas , S . Krishnan , A . Agrawal , Y . Wu , Y . Zhu , and M . Weimer , “Vamsa : Automated provenance tracking in data science scripts , ” in SIGKDD , 2020 , pp . 1542 – 1551 . [ 69 ] R . Souza , L . G . Azevedo , V . Lourenço , E . Soares , R . Thiago , R . Brandão , D . Civitarese , E . Vital Brazil , M . Moreno , P . Valduriez et al . , “Workflow provenance in the lifecycle of scientific machine learning , ” CCPE , vol . 34 , no . 14 , p . e6544 , 2022 . [ 70 ] A . Kumar , R . McCann , J . Naughton , and J . M . Patel , “Model selection management systems : The next frontier of advanced analytics , ” SIGMOD , vol . 44 , no . 4 , pp . 17 – 22 , 2016 . [ 71 ] E . Jun , M . Daum , J . Roesch , S . Chasins , E . Berger , R . Just , and K . Reinecke , “Tea : A high - level language and runtime system for automating statistical analysis , ” in UIST , 2019 , pp . 591 – 603 . [ 72 ] E . Jun , M . Birchfield , N . De Moura , J . Heer , and R . Just , “Hypothesis formalization : Empirical findings , software limitations , and design implications , ” TOCHI , vol . 29 , no . 1 , pp . 1 – 28 , 2022 . [ 73 ] E . Jun , A . Seo , J . Heer , and R . Just , “Tisane : Authoring statistical models via formal reasoning from conceptual and data relationships , ” in CHI , 2022 , pp . 1 – 16 . [ 74 ] K . Gu , E . Jun , and T . Althoff , “Understanding and supporting debugging workflows in multiverse analysis , ” in CHI , 2023 , pp . 1 – 19 . Sneha Gathani is a PhD student in the Department of Computer Science at the University of Maryland . Her research is focused on developing interactive visual data systems that can serve as intuitive and impactful tools for facilitating data - driven decision - making processes through the perspective of human - centered AI . She is affiliated with the Human Data Interaction Research ( HDI ) Group and the Human Computer Interaction Lab ( HCIL ) . She has an MSc . in Computer Science from University and a BSc . in Computer Engineering from University of Pune , India . Zhicheng Liu is an assistant professor in the Department of Computer Science at University of Maryland , College Park . He directs the Human - Data Interaction Research Group . His focus is to support interactive data analysis and visual data communication through human - centered techniques and systems . He was previously a research scientist at Adobe Research . He has an PhD from Georgia Institute of Technology . Peter J . Haas is a professor and the doctoral program director in the College of Information and Computer Sciences , and also an Adjunct Profes - sor of Industrial Engineering , at UMass Amherst . His research centers on the application of tech - niques from applied probability and statistics to the design , performance analysis , and control of systems for information management , mining , integration , exploration , and learning . His other projects focus on techniques for modeling , sim - ulation , design , and control of complex systems , especially discrete - event stochastic systems , as well as on the interface of simulation and bot information management and machine learning . Ça˘gatay Demiralp is a visiting researcher in the Data Systems Group at MIT CSAIL . Previously , he was Chief Research Scientist at Sigma Computing , a SaaS startup for cloud business data analytics , where he built and led the multidisciplinary data systems research group . His research interests are broadly in making data - centric systems intelligent to augment and automate user and system tasks . He also co - founded Fitnescity to provide easy access and data analytics for wellness lab tests .