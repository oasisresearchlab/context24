This is a version of a publication in Please cite the publication as follows : DOI : Copyright of the original publication : This is a parallel published version of an original publication . This version can differ from the original published article . published by The Role of Gamification in Participatory Environmental Sensing : A Study in the Wild Palacin - Silva Maria , Knutas Antti , Ferrario Maria Angela , Porras Jari , Ikonen Jouni , Chea Chandara Palacin - Silva , M . , Knutas , A . , Ferrario , M . A . , Porras , J . , Ikonen , J . , Chea , C . The Role of Gamification in Participatory Environmental Sensing : A Study In the Wild . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( p . 221 ) . ACM . Author ' s accepted manuscript ( AAM ) Association for Computing Machinery Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems 10 . 1145 / 3173574 . 3173795 © 2018 ACM The Role of Gamiﬁcation in Participatory Environmental Sensing : A Study in the Wild Maria Palacin - Silva 1 * , Antti Knutas 2 , Maria Angela Ferrario 3 , Jari Porras 1 , Jouni Ikonen 1 , Chandara Chea 1 1 Lappeenranta University of Technology Skinnarilankatu 34 , Finland name . lastname @ lut . ﬁ 2 Dublin City University Dublin 13 , Glasnevin , Ireland antti . knutas @ lero . ie 3 Lancaster University Bailrigg , Lancaster LA1 4YW , UK m . ferrario @ lancaster . ac . uk ABSTRACT Participatory sensing ( PS ) and citizen science hold promises for a genuinely interactive and inclusive citizen engagement in meaningful and sustained collection of data about social and environmental phenomena . Yet the underlying motivations for public engagement in PS remain still unclear particularly regarding the role of gamiﬁcation , for which HCI research ﬁndings are often inconclusive . This paper reports the ﬁndings of an experimental study speciﬁcally designed to further under - stand the effects of gamiﬁcation on citizen engagement . Our study involved the development and implementation of two versions ( gamiﬁed and non - gamiﬁed ) of a mobile application designed to capture lake ice coverage data in the sub - arctic region . Emerging ﬁndings indicate a statistically signiﬁcant effect of gamiﬁcation on participants’ engagement levels in PS . The motivation , approach and results of our study are outlined and implications of the ﬁndings for future PS design are reﬂected . ACM Classiﬁcation Keywords H . 5 . m . Information Interfaces and Presentation ( e . g . HCI ) : Miscellaneous Author Keywords participatory sensing ; citizen science ; civic technology ; environmental sensing ; human behavior ; engagement ; gamiﬁcation INTRODUCTION The human - computer interaction ( HCI ) community has for long been investigating the role of digital technology in citizen participation [ 14 , 25 , 63 , 22 , 1 ] including its role in under - standing the impact of environmental change and in supporting sustainable practices [ 32 , 26 , 31 , 53 ] . Views that citizens can be persuaded into acting ‘more sustainably’ have also been challenged [ 25 , 30 ] together with the reductionist view of the rational “Resource Man” adapting his / her behavior to data [ 54 ] . However , it is undeniable that technology has enabled Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Request permissions from permissions @ acm . org . CHI 2018 , April 21 – 26 , 2018 , Montreal , QC , Canada © 2018 ACM . ISBN 978 - 1 - 4503 - 5620 - 6 / 18 / 04 . . . $ 15 . 00 DOI : https : / / doi . org / 10 . 1145 / 3173574 . 3173795 citizen participation to far - reaching causes on a massive scale [ 7 ] , offering the opportunity for a deeper understanding of the mechanisms underpinning societal and environmental changes not only at global , but also at local level [ 50 , 64 , 1 , 28 ] . Participatory Sensing ( PS ) - often also referred as citizen sci - ence , crowd - sensing and crowdsourcing - is a form of civic technology designed to support public participation in the collection of meaningful , located data [ 27 , 5 ] . Over the last decade , PS has gained popularity with applications such as eBird , Fold . it , Waze , Ushahidi , and Galaxyzoo ; such tools have been actively supporting citizen - driven data collection for a varied of purposes including scientiﬁc research and crisis communication [ 16 , 21 ] , whilst serving as means for inclu - sive engagement , education , and public outreach [ 2 , 24 , 13 ] . Despite its widespread popularity , the motivations underlying citizen engagement are still unclear [ 39 ] and so is the role of gamiﬁcation in this context [ 49 , 39 , 30 ] . In particular , it is still unclear how effective gamiﬁcation is and in which PS contexts [ 24 ] , highlighting the need for empirical studies [ 12 , 23 , 43 , 52 , 60 ] . The aim of this paper is to further investigate the role of gami - ﬁcation in citizen participation in PS through empirical obser - vation . We do so by running a user study involving forty - one participants monitoring ice coverage of sub - arctic lakes . By using two versions ( gamiﬁed and non - gamiﬁed ) of a bespoke mobile app , we observe that the gamiﬁed version of the appli - cation has a statistically signiﬁcant higher PS engagement than the non - gamiﬁed one . Engagement is here measured by the number of submitted observations . From the study qualitative feedback , we also note that an increased use of the application can have additional impact on participants’ attitudes and be - havior , including spending more time outdoor , becoming more appreciative of the local environment and how it changes over time . The main contribution of this paper is a further insight into the role of gamiﬁcation in PS . Speciﬁcally , we ﬁnd that adding gamiﬁcation to an environmental monitoring application sig - niﬁcantly increases the number of participants’ observations . This paper outlines the rational , approach and results of our study and reﬂects on implications for future PS design . RELATED WORK It is argued that “environmental issues are best handled with the participation of all concerned citizens” [ 58 , 57 , 56 ] . This is a result of the signiﬁcant evolution of the relationship be - tween governments and citizens in the past 60 years from consciousness raising in the 1960s , the incorporation of local perspectives in the 1970s , the recognition of local knowledge in the 1980s , the participation as a norm as part of the sustain - able development agenda of the 1990s , and the e - participation governance in the 2000s [ 57 , 56 , 47 , 4 , 64 ] . Technology has played a key role in the evolution of partic - ipation . The publics are now actively engaging with diverse causes via ICT ( e . g . activism , mobilizations , public campaign - ing , community monitoring ) [ 7 ] . In this context , systems such as participatory sensing ( PS ) - often also referred to as citizen science , crowd - sensing or crowdsourcing - have emerged as an opportunity to monitor social and environmental phenomenon in detail ; since mobile technology has become pervasive and able to capture , classify and transmit location , image , voice and other data autonomously [ 49 , 6 , 16 , 21 ] . Participatory sensing is data collection and interpretation enabled by tech - nology [ 5 , 21 ] . The data being submitted in PS represents a deliberative act of public participation by the interaction of the public with the technology - enabled services . In the early 2000s , city develop - ment , urban crime surveillance , and forest conservation were highlighted as promising applications of participatory sensing [ 45 ] . Over a decade later , the applications in those domains ( among many others ) have widely spread among the publics . For example , FixMyStreet 1 FixMyStreet allows citizens to report city issues e . g . broken pavement ) to enhance city main - tenance ; Ushahidi 2 helped the Kenyan government in 2007 to map violent acts across the country and has been used in more than 10 countries since then ; eBird 3 was launched in 2002 to gather basic data about bird distribution across the globe . By now eBird has collected hundreds of millions of ob - servations from most countries in the world . Finally , Safecast 4 was launched by citizens’ initiative to monitor the radiation levels in Japan after the nuclear accident in Fukushima in 2011 . By now , it has become the largest monitoring network in the history of the planet . PS systems are built based on public participation , but they also serve the public in solving daily problems ( e . g . ﬁnding the best route home ) . Hence , citizens are found at these ap - plications’ very operational core for two main reasons : ﬁrstly , because of the way they are operated , any participatory sens - ing platform is doomed to fail if it has no participants ; and secondly , because of the great value of local knowledge and the intimate understanding the public has on the patterns and anomalies in their communities . This local knowledge can complement expert assessments as it includes important con - textual information [ 5 ] . Among the different challenges that PS holds ( from technology leaps , privacy and security con - cerns to data quality and standardization ) , active public en - gagement remains a key challenge in this ﬁeld . 1 FixMyStreet : http : / / www . fixmystreet . com 2 Ushahidi : http : / / www . ushahidi . com 3 eBird website : http : / / www . ebird . org 4 Safecast website : http : / / blog . safecast . org However , the underlying public motivation to actively engage and participate in PS is still unclear [ 19 , 15 ] . Particularly , the impact of gamiﬁcation mechanisms on human behavior in PS remains under discussion : with current HCI ﬁndings [ 49 , 39 , 30 , 17 , 18 ] often inconclusive / contradictory and do - main agnostic . For example , Knowles et al . [ 30 ] doubt that gamiﬁcation works based on the claim that game elements activate negative achievement values which might not support pro - environmental behavior change . In the other hand , Ross [ 49 ] , claims that pervasive games empower humans to take actions to improve society , and Massung et al . [ 39 ] argue that gamiﬁcation can impact extrinsic motivators that enhance pro - environmental practices . Meanwhile , one of the most successful participatory sensing projects so far is Foldit 5 , an online puzzle game which challenges participants to fold pro - tein structures as perfectly as possible using the game tools . Scientists can analyze the applicability of the highest scoring solutions for curing diseases ; it has been shown that the best Foldit players can match or exceed computational solutions [ 8 ] . By 2014 Foldit already exceeded 200 , 000 players [ 37 , 59 ] . Given this context , gamiﬁcation was selected as the method to study engagement , as it is a technique with a solid theoretical basis for impacting human behavior [ 52 ] . As such , gamiﬁ - cation is the application of game - like elements to non - game environments [ 11 ] . It has also been deﬁned as a process of enhancing a service with affordances for gameful experiences in order to support user’s overall value creation [ 29 ] . Ap - proaches that use some elements of gamiﬁcation have been shown to increase user motivation and engagement in a variety of environments [ 52 ] , including participatory sensing [ 42 ] . However , it is still not clear how well gamiﬁcation works and in which contexts [ 23 ] . For example in citizen science , gamiﬁcation has been used to encourage [ 38 , 39 , 36 ] and improve [ 66 ] participation . The results in these studies were positive [ 36 ] or inconclusive [ 38 , 39 , 66 ] . In yet another study from the ﬁeld of sustainability gamiﬁcation was used to engage customers with positive outcomes [ 20 ] . Therefore there have been calls for rigorous empirical studies to be performed to better understand the effects of gamiﬁcation [ 12 , 23 , 43 , 52 , 60 ] . For these reasons , we carried out a user study in - the - wild to further understand the effects of gamiﬁcation on human engagement and the perception in the context of environmental sensing . METHODOLOGY This study was aimed at understanding the effects of gam - iﬁcation on user engagement and user experience in a par - ticipatory sensing application . Both engagement and user experience were measured by quantitative indicators ( see table 1 ) . Nonetheless , it is important to highlight that a variable name might have a wider meaning in other contexts [ 67 ] . The user engagement was measured by three indicators : • Involvement : Total number of submissions per user , 5 Foldit website : http : / / www . fold . it • Activeness : Number of active users from beginning to end of the study . In this context , an active user is understood as a user that submits observations in a daily basis . • Dropout : Total number of users who left the study . On the other hand , the user experience was measured in terms of : • Effectiveness : Ratio of the number of submissions and application openings ( per user ) , • Learnability : Time to become familiar with the application at the ﬁrst use , • Satisfaction : Rates ( in a ﬁve Likert scale ) ease of use and usability satisfaction statements Table 1 . Variables and Indicators . Variable Indicator Measurement Engagement Involvement Number of submissions Activeness Number of users who were active for the entire duration of the study Dropout Number of who did not complete the study User Experience Effectiveness Number of submissions per app usage Learnability Time to learn the app during the ﬁrst use Satisfaction Survey - based opinion During this study , the participants had to submit photos of the ice status of the largest lake in Finland ( lake Saimaa ) during a seasonal change . For this purpose , two applications were designed : a gamiﬁed application and a non - gamiﬁed application . Then , the participants were divided in two groups : a control group and an experimental group ; and they were randomly assigned to use one of the applications for 20 days . The length of our study is limited by the time between the freeze - up and break - up of the lake Saimaa , which is typically about 4 months [ 33 ] . This sets a maximum observation time frame for the public to be able to perform monitoring . Experiment Design We designed our experiment following the guidelines of Wohlin et al . [ 67 ] . In our study , the independent variable was gamiﬁcation elements and the dependent variables were a ) engagement and b ) user experience . Two hypotheses were deﬁned in order to understand the effects of gamiﬁcation on the dependent variables 1 ) Hypothesis for Engagement Variable : • Null hypothesis H0 1 : The use of gamiﬁed elements in a lake monitoring application produces equal or less user engagement than a non - gamiﬁed application . • Alternative hypothesis H1 1 : The use of gamiﬁed elements in a lake monitoring application produces a greater user engagement than a non - gamiﬁed application 2 ) Hypothesis for User Experience Variable : • Null hypothesis H0 2 : The use of gamiﬁed elements in a lake monitoring application produces the same user experience as a with a non - gamiﬁed application . • Alternative hypothesis H1 2 : The use of gamiﬁed elements in a lake monitoring application produces a better user ex - perience than a non - gamiﬁed application . Participants The selection followed a non - probabilistic convenience sam - pling where invitations to participate were sent to university students through mailing lists . As a result , 41 volunteers ( a person who carries and activity without being paid ) signed up to participate in the experimental study which took place from 24 March to 12 April 2017 ( 20 days ) . After signing informed consent agreement , the participants were randomly divided into 2 group ( see ﬁgure 1 ) : a ) the control group ( 20 participants ) received a non - gamiﬁed application and , b ) the experimental group ( 22 participants ) received a gamiﬁed appli - cation . The participants received an instruction session before the start of the experiment . This session provided informa - tion about the experiment motives , data treatment , application functionality and answered their questions . Figure 1 . Participatory Sensing System Architecture . System Design In this study , two mobile applications - one non - gamiﬁed ap - plication and one gamiﬁed - were developed both for Android and iOS platforms . The architecture of these applications is presented in Figure 1 . The development process was mainly designer - led with feedback of different versions of the systems collated from students and researchers . Non - gamiﬁed application : Jarvida Jarvida was a regular water bodies monitoring application , inspired from existing tools in use ( such as Järviwiki 6 , Finnish 6 Järviwiki website : http : / / www . jarviwiki . fi Hydrological Citizen Observation Network 7 , CreekWatch 8 ) . The application logic was straightforward ( Figure 2 ) ; it would start from providing some educational examples about ice types on a lake then , the participants could proceed to submit . The user had to select a location for this observation ( so that the user would not be bounded to make a submission when the photo is taken but rather to be able to submit at any point of the day ) . Also , the submission could have an image attached or it could be just a report of the ice status , there were three observation values : no ice ( water is not frozen ) , partially ice - covered ( water is partially frozen or melted ) and compactly ice - covered ( water is compactly frozen and ice thickness can be measured ) . Finally the participant could , check the statis - tics about the ice status based on all the observations in his area . A use case example starts with the user logging in , then information about the ice types is displayed along some ed - ucational images . Next , the user can proceed to submit an observation and add locations along ice values . Finally , the user can see the statistics of ice condition in the area . Figure 2 . Jarvida logic Gamiﬁed application : Jarvigo Jarvigo provided the same features for environmental mon - itoring as Jarvida , but with game elements integrated . The gamiﬁcation elements [ 68 , 48 ] added to this version were as follows : • Interactive map : All submitted observations appeared on the map as pins . Users could tap on the map to add a new pin or use an existing pin then , they could submit an observation . Pins would change of color ( red to green ) when a submission was done . • Storytelling : A story describe the context of the task . Then it gives a mission that users need to accept ( challenge ) . The story was designed to raise participant’s awareness about environmental monitoring , and making users feel that their contributions make an impact to their community and the world . • Challenge : Each observation task was seen as a challenge or mission that users needed to carry on for a speciﬁc period of time . • Points : Points are the most basic but compulsory element in gamiﬁed systems . In this application , participants who submitted observations were rewarded the experience points ( XP ) . 7 Finnish hydrological citizen observation network website : http : / / www . kansalaishavainnot . fi / lumi 8 CreekWatch website : http : / / creekwatch . ca Figure 3 . Gamiﬁcation elements in Jarvigo app . • Leaderboard : Participants were ranked according to their earned points . Top ten users were displayed in the leader - board . • Feedback : This technique was used to make users feel that their contributions were not taken for granted and it gave users the feeling of satisfaction from seeing their own progress and points earned . The logic in Jarvigo ( Figure 4 ) , would start with a story re - lated to global warming , highlighting the importance of lakes monitoring . Then it asks participants to accept a challenge ( submitting 20 observations in 20 days ) . After accepting the challenge , participants could view a map of the area with all the pins where other observations have been submitted . Then , they could submit an observation by tapping on the location they have observed . Again for the submission the user could attach an image or just report the ice status ( no ice , partially ice - covered and compact ice ) . When they submit an obser - vation , 20 points are awarded . Participants could track their progress and see the top ten ranked participants ( based on scores ) on the leaderboard . Finally the participant could also check the statistics about the ice status in their area . In order to balance the challenge effect among groups and avoid a " Hawthorne effect " ( where participants modify their behavior simply because they are being observed ) among the participants using the gamiﬁed application , participants who used the non - gamiﬁed application were recommended during the introductory meeting to use the application every day dur - ing the study duration ( which was the equivalent of the goal in the gamiﬁed version ) . Figure 4 . Jarvigo logic . Analysis Method The source data for this study came from the applications usage logs and pre - and post - questionnaires . Given the partici - pant pool size , the independence of the test groups , the ordinal and non - normal nature of the dataset , Mann - Whitney U test [ 67 ] - a nonparametric test for independent samples - was used to calculate the statistical signiﬁcance of the data sample and thus , reject or keep the null hypothesis . The pre - 9 and post - 10 surveys were designed to collect quan - titative and qualitative data regarding the perception and ex - perience of the participants during this study . Standardized questionnaires such as the systems usability scale [ 3 ] , the IBM computer usability satisfaction questionnaire [ 35 ] , the user acceptance of information technology scale [ 62 ] , and perceived playfulness questions [ 61 ] ; were included in the pre - and post - questionnaires . The overall data collected was : participants’ demographics and environmental interests and perceptions about user experience , usage habits , gamiﬁcation features , enjoyment and playfulness . FINDINGS 41 volunteers aged 20 – 35 participated in this study which lasted 20 days . Participants were randomly divided into two independent groups . Hence , there were 20 participants using non - gamiﬁed application ( Jarvida ) and other 21 participants using gamiﬁed application ( Jarvigo ) . A total of 304 observa - tions were submitted via both applications . A major concern during this period was the weather conditions as the temper - ature ranged between - 8C°and 12C° and , participants had to record their observations outdoors . The ﬁndings in this section are presented by variable and indicators . These ﬁndings derive from statistical tests and descriptive statistics onto each one of the indicators ( involvement , activeness , dropout , learnability , effectiveness and satisfaction ) . Also , to ensure data consis - tency and to avoid common issues in this type of applications ( such as spam or duplicate submissions ) [ 40 ] , the dataset was checked for duplicated submissions ( e . g . same observation submitted multiple times in order to gain points or by mistake ) . Engagement In this context , engagement refers to the number of obser - vations submitted by participants ( involvement ) , the level of 9 pre questionnaire : http : / / goo . gl / JJB5k2 10 post questionnaire : http : / / goo . gl / 8rk1sG activeness of the users from beginning to end in the study ( ac - tiveness ) and the number of participants abandoning the study ( dropout ) . The overall results ( see Table 2 ) , show that the involvement and activeness among the participants who used the gamiﬁed application was signiﬁcantly higher compared to the ones who used the non - gamiﬁed application . On the other hand , the dropout percentage was similar in both applications . Given this results , the null hypothesis is rejected and the al - ternative hypothesis is accepted " H1 1 : The use of gamiﬁed elements in a lake monitoring application produces equal or less user engagement than a non - gamiﬁed application . " . Each indicator and its results are explained below : Table 2 . Engagement Indicators : Results ( where : n ) not signiﬁ - cant , * * ) statistically signiﬁcant at p < 0 . 05 , * * * ) statistically signiﬁcant at p < 0 . 01 , * * * * ) statistically signiﬁcant at p < 0 . 001 ) . Indicator Measurment Non - gamiﬁed Jarvida GamiﬁedJarvigo Difference and signiﬁcance Involvement Totalsubmissions 44 260 216 * * * Activeness Very Active users from beginning to end 17 % 50 % 33 % ( N / A ) Dropout Number of participantswhodidnotcompletethe study 10 % 5 % 5 % ( N / A ) • Involvement : The results show that the observations sub - mitted in the experimental group ( using the gamiﬁed ap - plication ) was statistically signiﬁcantly higher at than the control group ( using the non - gamiﬁed application ) ( U = 346 ; p = 0 . 0003612 ; at p < 0 , 001 ) . Figure 5 shows the distribution of the submissions from both applications over time . There were a total of 304 submissions from both applica - tions . From which , 44 observations came from the non - gamiﬁed application users ( control group ) and 260 came from the gamiﬁed application users ( experimental group ) . Hence , the submissions from the gamiﬁed application were 71 % higher . This result might be related with the user experience of participants . From the qualitative analysis of comments from participants in the post survey , we found that the par - ticipants who used the non - gamiﬁed application found the application easy to use and useful ( e . g . " It’s easy to use and ﬂexible " ; " Satisfactory and useful for understanding the climate change " ( P15 , P9 ) ) . While , the participants who used the gamiﬁed application found it easy to use , useful , interesting and fun ( e . g . " I found app very easy to use and effective way to get live data from different location . It was fun and a good experience to provide data for such cause . " ; " it was fun and desire to submit observations incentivized me to walk more : So I would say that it is useful not only for lakes , but for people as well . " ; " A great way to study such an important issue . Hopefully this will continue and I would participate also in the future . " ( P20 , P35 , P1 ) ) • Activeness : This indicator refers to the number of partici - pants who were very active using the application during the study . There were 18 participants using the non - gamiﬁed application ( from beginning to end ) , out of which , only three of them were very active users ( submitting observa - tions daily ) corresponding to 17 % of the group population . Figure 5 . Submitted Observations per application over time . On the other hand , among the 20 participants using the gamiﬁed application ( from beginning to end ) , there were 10 very active participants corresponding to the 50 % of its population . • Dropout : The study started with 20 participants using the non - gamiﬁed application . Two of them did not complete the experiment to the end ( dropout = 10 % ) . On the other hand , with the gamiﬁed application , there were 21 partici - pants in the beginning , out of which 1 abandoned the study ( dropout = 5 % ) . This result shows that there were not major differences in the dropout rate between the two applications . User Experience : In this study , the user experience refers to : the submissions completed when the applications were open ( effectiveness ) , the time it took to the participants to understand their applica - tion ( learnability ) and the overall perceptions and satisfaction with their applications usability ( satisfaction ) . The overall results ( see Table 3 ) , show that the effectiveness among the participants who used the gamiﬁed application was twice as high compared to the ones who used the non - gamiﬁed appli - cation . On the other hand , there were no major differences on the learnability and satisfaction indicators between both appli - cations , which mean that participants were satisﬁed with both applications . Given this results , the null hypothesis is kept and the alternative hypothesis is rejected " H2 0 : The use of gami - ﬁed elements in a lake monitoring application produces the same user experience as a with a non - gamiﬁed application " . Each indicator and its results are explained below : • Learnability : This indicator measures the time a participant spends to get familiar with the application . Participants were called to attend a one to one meeting for this pur - pose , the meeting started with an explanation about the experiment purpose and a consent agreement signing . Then , participants were given their corresponding application on a testing device . Each participant would use the application until he / she would feel that has reached an understanding on Table 3 . User Experience Indicators : Results ( where : n ) not signiﬁ - cant , * * ) statistically signiﬁcant at p < 0 . 05 , * * * ) statistically signiﬁcant at p < 0 . 01 , * * * * ) statistically signiﬁcant at p < 0 . 001 ) . Indicator Measurment Non - gamiﬁed Jarvida GamiﬁedJarvi Differenceandsig . Effectiveness Submission success rate 36 % 68 % 32 % * * Learnability Time to learn to use the app 166sec 167sec 1 sec n Satisfaction Average ease to use pre survey 2 , 9 3 0 . 1 n Average ease to use post survey 3 2 , 8 0 . 2 n Average usability satisfaction 4 , 4 4 , 2 0 . 2 n how the application works , once that feeling was reached , a voice signal was given to the researcher ( e . g . " I understand the application " ) . Then , the researcher gave a tutorial about the features of the corresponding application and answered questions and comments . The non - gamiﬁed application users spent 166 seconds in average to understand their application during their ﬁrst interaction . In comparison , the participants who used the gamiﬁed application spent 167 seconds in average . Simi - larly , the analysis of the dataset of this indicator shows that the learnability time in the experimental group ( who used the gamiﬁed application ) was similar to the control group ( who used the non - gamiﬁed application ) ( U = 144 . 5 ; p = 0 . 08693 ) . • Effectiveness : This indicator measured the submissions success rate . In this study , the effectiveness ratio was : E f fectiveness ( % ) = TotalSubmissions TimesApplicationOpenx 100 % As it can be seen in Figure 6 ( Where P is a participant ) , the effectiveness in the non - gamiﬁed application was 36 % ( to - tal submissions = 44 ; times the application was open = 122 ) . While the effectiveness in the gamiﬁed application was 68 % ( total submission = 260 ; times the application was open = 381 ) . Hence , the effectiveness was twice higher in the gamiﬁed application compared to the non - gamiﬁed one . Furthermore , the analysis show that the submission effectiveness in the experimental group ( who used the gam - iﬁed application ) was statistically signiﬁcantly higher than the control group ( who used the non - gamiﬁed application ) ( U = 111 ; p = 0 . 009844 ; at p < 0 . 01 ) . Participants were asked in the post survey to detail the major issues they experienced while using their corresponding application . The major reported issue was the reliability , in particular , application crashes and slow map loading . This issues can be observed in the previous calculations from the times the applications were open . This issue is further discussed in the next section . • Satisfaction : This indicator was measured by the rate that participants gave - in a ﬁve step Likert scale - to a list of statements in the pre and post surveys ( from strongly disagree to strongly agree ) . These statements were based on standardized questionnaires from [ 3 , 35 , 62 , 61 ] . Figure 6 . Comparison of Effectiveness Rates among Participants . In the pre survey statements about ease to use from [ 3 ] were included . As it can be seen in Table 4 , participants of both groups perceived their applications equally usable ( as no statistically signiﬁcant difference was found during the analysis ) . Table 4 . Mann - Whitney U test of pre survey ease to use statements ( where : n ) not signiﬁcant , * * ) statistically signiﬁcant at p < 0 . 05 , * * * ) sta - tistically signiﬁcant at p < 0 . 01 , * * * * ) statistically signiﬁcant at p < 0 . 001 ) . Statements U p - value Signiﬁcance I think that I would like to use this system frequently 183 0 . 6394 n I found the system unnecessarily complex 186 0 . 6786 n I thought the system was easy to use 204 0 . 9024 n I would imagine that most people would learn to use this system very quickly 247 0 . 1492 n I found the system very hard to use 203 0 . 9210 n I needed to learn a lot of things before I could get going with this system 171 0 . 3582 n In the post survey , the same statements about ease to use from [ 3 ] were included and a statement about usability satis - faction [ 35 ] was added . Table 5 presents the obtained results . Alike the pre survey ( Table 4 ) , both groups perceived their applications equally usable ( as no statistically signiﬁcant difference was found during the analysis ) . This is also the case for the overall satisfaction ( U = 158 ; p = 0 . 4889 ) . Gamiﬁcation Findings Below we present the gamiﬁcation related results from the study post survey : • Gamiﬁcation Mechanisms : Participants who used the gam - iﬁed application were asked to rank statements ( from 1 = strongly disagree to 5 = strongly agree , in a ﬁve Likert scale ) about their experiences with each gamiﬁed mechanism in the post survey . The result show that the statements were ranked in the following order : 1 . Seeing my name in the leaderboard motivated me to submit more observations ( 3 . 4 ) Table 5 . Mann - Whitney U test of post survey ease to use and usability satisfaction statements ( where : n ) not signiﬁcant , * * ) statistically signif - icant at p < 0 . 05 , * * * ) statistically signiﬁcant at p < 0 . 01 , * * * * ) statistically signiﬁcant at p < 0 . 001 ) . Statements U p - value Signiﬁcance I think that I would like to use this system frequently 132 . 5 0 . 142 n I found the system unnecessarily complex 181 0 . 987 n I thought the system was easy to use 144 . 5 0 . 2484 n I think that I would need the support of a technical person to be able to use this system 159 0 . 2597 n I would imagine that most people would learn to use this system very quickly 157 0 . 4533 n I found the system very clumsy or difﬁcult to use 197 0 . 5877 n I needed to learn a lot of things before I could get going with this system 140 0 . 1464 n I ﬁnd it easy to get the system to do what I want it to do 166 0 . 6701 n Overall , I am satisﬁed with how easy it was to use the system 158 0 . 4889 n 2 . I followed my progress on the activity tab ( 3 . 2 ) 3 . I learned about global warming with the storyboard ( 2 . 9 ) 4 . Seeing my points reduced motivated me to submit new observations ( 2 . 9 ) 5 . I achieved my challenge of submitting 20 observations in 20 days ( 2 . 4 ) The qualitative data shows that participants paid particular attention to features such as points , leaderboard and the story . We found qualitative evidence that some participants tracked their points and reported issues they considered rele - vant such as : " didn’t limit observations or daily point " ( P18 ) or " I didn’t see that any points would have been taken from me on those days that I did submit a pic " ( P1 ) . Also , the leaderboard appeared to be a popular feature , during the study the researchers observed that some participants no - ticed the presence of some of their friends in the leaderboard . This triggered a healthy social competition . A participant who was involved in this sort competition pointed out that " it was fun and desire to submit observations incentivized me to walk more , So I would say that it is useful not only for lakes , but for people as well " ( P35 ) . Finally , the story mechanism seems to be an effective mean to raise aware - ness . Jarvigo participants read the task context from the story , our qualitative analysis of their post survey comments show that they considered lakes monitoring an important issue that needs monitoring ( " A great way to study such an important issue " ( P1 ) , " I think that measuring ice thick - ness would be useful for monitoring " ( P35 ) , " It was fun and a good experience to provide data for such cause " ( P20 ) ) . Also , participants were critic about the quality of their ob - servations for monitoring ( " I noticed I took many pictures of interesting rather than representative ice conditions , eg small area with no ice when the whole lake was otherwise frozen " ( P24 ) ) . • Enjoyment : In the post survey , statements about enjoyment from [ 62 ] were included . Table 6 presents the obtained results . Both groups perceived their applications equal in terms of : being a good idea , interesting , liking to use the applications , enjoyable and pleasant . However , the analysis show that the experimental group ( who used the gamiﬁed application ) had a slight higher perception of fun ( U = 197 ; p = 0 . 09659 ; at p < 0 . 1 ) than the control group ( who used the non - gamiﬁed application ) . This small difference in perception also arose in the qual - itative analysis . Where , Jarvigo participants reported that they found this application interesting , effective , fun and likeable ( " It was very interesting to participate in to this project " ( P16 ) , " effective way to get live data from different location " ( P20 ) , " Otherwise it was fun and desire to submit observations incentivized me to walk more " ( P35 ) , " Gen - erally I like the idea and the app " ( P35 ) ) . In contrast the Jarvida participants , found their application only interest - ing and relevant ( " Good idea and an important subject to study " ( P7 ) , " Good quest and raising awareness " ( P11 ) , " useful for understanding the climate change " ( P9 ) , " I think the idea of using the application for the lake monitoring is interesting and could make some help . And the society in general would be happy to assist " ( P23 ) ) . Also , in terms of motivation we noticed a inner motivated participant in Jarvigo who highlighted an interest in participating in future environmental sensing studies ( " Hopefully this will continue and I would participate also in the future " ( P1 ) ) . On the other hand , a Jarvida participant highlighted a lack of mo - tivation ( " I had some lack of motivation in the sense that I didn’t feel how pictures from my phone could help in lake condition monitoring " ( P23 ) ) . This puts in evidence a fun - damental challenge for participatory sensing , inner human motivation . The inner motivation or lack of it has a great role onto the behavior of participants . A participant who already has an inner motivation will see it increased by the use of this mechanisms . On the other hand a participant who lacks of it can experience an opposite effect . Table 6 . Mann - Whitney U test of post survey enjoyment statements ( where : n ) not signiﬁcant , * * ) statistically signiﬁcant at p < 0 . 05 , * * * ) sta - tistically signiﬁcant at p < 0 . 01 , * * * * ) statistically signiﬁcant at p < 0 . 001 ) . Statements U p - value Signiﬁcance Using the mobile software for measurement is a good idea 166 . 5 0 . 6648 n The program makes measuring tasks more interesting 197 0 . 6027 n Working with the program is fun 233 . 5 0 . 09659 n I like using the program to measure lakes 189 0 . 7904 n I found using the system to be enjoyable 186 0 . 8642 n The actual process of using the system is pleasant 171 . 5 0 . 8031 n • Playfulness : In the post survey , statements about playful - ness from [ 61 ] were included . Participants were asked to characterize themselves when they use their corresponding application ( " How would you characterize yourself when you used the system ? " ) . The results show ( Fig 7 ) that the participants who used the non - gamiﬁed application would characterize themselves mainly as ﬂexible , spontaneous and creative . On the other hand , the participants who used the gamiﬁed application would characterize themselves mainly as spontaneous , ﬂexible and playful . Figure 7 . Application Playfulness Perceived by Participants . Summary and Reﬂection In summary , through our study we ﬁnd that the gamiﬁed fea - tures of the app increased participants’ submissions without affecting the perceived usability of the application . Perception of user experience vs . actual user behavior One of the most interesting results of this study is that per - ceived user experience and engagement did not vary between the test groups , however the test shows that the actual engage - ment ( in terms of participants’ submission ) did vary . Self - reporting survey are a popular in research and several engage - ment scales have previously been successfully validated [ 65 , 55 ] . However , in our study , the actual logging data tells a different story to the results collected from our self - reporting survey - this is in line with Moller et al . ﬁndings [ 41 ] . Gamiﬁcation literature suggests that effective gamiﬁcation in - volves leveraging on the game elements to foster users’ three innate needs for intrinsic motivation 11 [ 52 ] , originally adapted from Deci and Ryan’s self - determination theory ( SDT ) [ 9 ] . These principles were used to foster internal motivation in the application in regard to relatedness , competence , and auton - omy . Observed impact of gamiﬁcation onto engagement Gamiﬁcation depends on speciﬁc mixture and preparation of a system , which cannot be deconstructed by pieces and thus measured individually . Gamiﬁcation is about " experiences , not elements " [ 10 ] . A game - like system has to be fun and there is a limit to deconstruction until the individual elements stop being engaging [ 10 , 34 ] . Prior work in the ﬁeld of par - ticipatory sensing and HCI [ 39 , 46 ] have studied single game features such as the leaderboard , using also an experimental approach . The ﬁndings in this study bring insights about the 11 Intrinsic motivation in gamiﬁcation literature ; autonomous motiva - tion in self - determination theory literature effect of gamiﬁcation as a bundle of features onto human en - gagement . This reﬂects that there is no plug and play solution to engage humans into environmental sensing . Hence , gamiﬁ - cation design should move from stimulus - effect determinism to providing fun , engaging and playful experiences [ 10 ] . Design Reﬂections Our study provides new insights into the use of gamiﬁcation to enhance human engagement and user experience in participa - tory sensing systems for environmental monitoring . Based on our ﬁndings we suggest the following design considerations for future PS developments : • Support personalized notiﬁcation triggers : While some participants appreciate to not be hassled with notiﬁcations , others need notiﬁcations to remind them to submit obser - vations " The app could send reminds for us to use it more often " ( P2 ) , " Sometimes you need to have a reminder for submitting an observation , because it is easy to forget ! " ( P5 ) . The amount of notiﬁcations a participant can consider appropriate may change from person to person . Thus , PS should embedded mechanisms such as sliders to allow par - ticipants to adjust how often they wish to receive reminders . • Support customizable challenges to avoid negative feed - back ( e . g . discouragement ) triggered from the lack of achievement : Competition tends to encourage those on top of the leaderboard but , it can demotivate those who are not among the top [ 39 ] . A participant who ranked among the top in the jarvigo said that " it was fun and desire to submit observations incentivized me to walk more " ( P35 ) . On the other hand , there were participants whose lifestyle limited the amount of time they could spent using the ap - plication , they reported this limitation as a major issue " I wasn’t visiting the lake so often . " ( P2 ) " Perhaps the major issue was trying to use Jarvi more often , especially when walking near Lakes . Unfortunately , I didn’t spent much time near the Lake due to busy schedule " ( P38 ) . Mecha - nisms that allow the customization of the challenges ( e . g . choose a challenge ) and the leaderboards ( e . g . choose with whom to compete or weekly resets ) with the lifestyles of the participants could result into effective mechanisms for motivation . • Support social interaction between users : A number of participants were motivated by competing with other par - ticipants they knew beforehand . This triggered social in - teractions in person between the participants . Applications such as Pokemon GO provide opportunities for their players to meet and tackle challenges together ( e . g . raid battles ) . The sense of community is important in PS and support - ing social interaction beyond likes is a promising path that requires further research . • Allow users to explore submitted data : A number of par - ticipants were interested in seeing the photos recorded by other participants " Would have been nice to see the pic - tures of other people what they submitted . " ( P3 ) , " I Would have liked to get access to my own photos - and others’ as well . " ( P19 ) . This supports the previous design considera - tion related to social interaction . Participants do their bit but also want to be part of a community and see what others are doing , applications such as Instagram have designed simple mechanisms to have such interaction . Hence , opening the submitted data to the participants could strengthen the sense of purpose by allowing participants to see the effect and reach of their submissions . • Enhance indoor experiences : Going out to nature is part of the " fun " of contributing with an environmental PS ini - tiative " I like going to nature " ( P3 ) . However , ways to contribute indoors should be enhanced as well , so that par - ticipants can contribute even if they cannot go outdoors during a certain period of time ( e . g . weather conditions " It was a bit uncomfortable to use the app in cold conditions " ( P24 ) ) . This would enhance the feelings of competence and achievement . Applications such as Galaxy Zoo are built around indoor experiences , where participants perform classiﬁcation tasks at their computers . • Support interactive feedback : Several participants sug - gested new features and improvements to the current sys - tem via the surveys ; Maybe instead of google maps it would more reliable and robust to just have a static map like pic - ture with some coordinates system on it " ( P28 ) , " suggestion is to provide some lighter version of the application , espe - cially for slow connection internet " ( P32 ) , " In my opinion , the steps of adding new spot and then adding ice informa - tion to that spot could integrated into one step . Also , there could be some note that you can use the same spot for mul - tiple times to record observations . " ( P33 ) . This comments unveil an opportunity to involve the participants as a co - creators . This behavior needs to be enhanced via design mechanisms that encourage feedback and co - creation from participants . LIMITATIONS AND FUTURE WORK Limitations The ﬁrst limitation of this study is its short duration which was not enough to explore how participants’ behavior change over time . However , this was out of scope of this study . Although some insight about the individual effect of each gamiﬁcation feature onto engagement was illustrated using the qualitative data collected via surveys . This analysis is limited by the current study data ( which is mainly quantitative ) and could be improved by more usage data statistics in future studies . Another limitation is the unfamiliarity of few participants with Android devices , as the developed sensing applications were only available for iOS and Android . As a result , six participants received devices with their respective sensing application installed for the time of the study . This might have had an effect on the learnability measure , because the participants had to learn to use a new device on top of learning how to use the application . Future Work Participatory Sensing faces several challenges including pri - vacy and security concerns , data quality and standardization . Yet , participation is fundamental to capture contextual local knowledge about patterns that often are ignored in bigger scales . Some studies have already looked into what motivates people to get involved in participatory sensing projects [ 19 , 15 ] . However , it still remains unanswered how could the effect of ICT on engagement be scaled up to larger time spans , and how PS could be used to engage citizens as designers and innovators . Following this study , we will design an action research intervention to explore the participants’ engagement over a longer period of time . This subsequent study will ex - plore whether co - creation as a design approach enhances both the feeling and perception of engagement , and longer - term effects [ 51 ] . CONCLUSIONS In this work , we present the results from an experimental user study that explored the impact of gamiﬁcation on engagement and user experience . We designed , deployed and evaluated two environmental sensing applications ( one gamiﬁed and one non - gamiﬁed ) via a 20 days experiment with volunteers . We found that gamiﬁcation affected the participants’ engagement in a positive way ( producing more submissions ) , without im - proving nor compromising their user experience . This led us to think that in order to produce human engagement , alter - ing interfaces is not enough . This supports the review results of Nacke et al . [ 44 ] that adding simple visual manifestation of gamiﬁcation elements or deterministic mechanics to the interface is not enough without considering other aspects of engagement . Deterding et al . propose [ 10 ] that gamiﬁed moti - vation design should move from stimulus - effect determinism to providing fun , engaging and playful experiences . Our par - ticipants mentioned " proving data for a good cause " , " I like going out to nature " and " it is useful not only for lakes , but for people as well " as motivating factors . This leads us to con - clude that the gamiﬁcation design for environmental sensing had a positive effect on participants’ engagement . Even when the underlying technology is still evolving , par - ticipatory sensing has already shown its great potential , not only as a tool for citizens’ collecting data but also as a vehicle for engaging a large public community in solving social and environmental challenges . These systems have the potential to close the gaps among researchers , environmental experts , decision - makers , and the people , while collecting data and building a whole new level of services ( from the people , for the people ) . However , the success of participatory sensing relies heavily on continuous citizen participation and the com - putational capacity to extract patterns from the data being collected . ACKNOWLEDGMENTS We thank all the study participants for their time and dedica - tion . Also , we would like to thank the ﬁnancial and academic support of the following organizations : the European Eras - mus Mundus programme PERCCOM : Pervasive Computing and COMmunications for sustainable development , the Ulla Tuominen foundation , the UKRC grant n . EP / P002285 / 1 and the M . McLuhan Centenary Fellowship in Digital Sustainabil - ity , DCI , iSchool , University of Toronto . REFERENCES 1 . Mariam Asad , Christopher A Le Dantec , Becky Nielsen , and Kate Diedrick . 2017 . Creating a Sociotechnical API : Designing City - Scale Community Engagement . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM , 2295 – 2306 . http : / / dl . acm . org / citation . cfm ? id = 3025963 2 . Rick Bonney , Caren b Cooper , Janis Dickinson , Steve Kelling , Tina Phillips , Kenneth V Rosenberg , and Jennifer Shirk . 2009 . Citizen Science : A Developing Tool for Expanding Science Knowledge and Scientiﬁc Literacy . BioScience 59 , 11 ( 2009 ) , 977 – 984 . http : / / bioscience . oxfordjournals . org / content / 59 / 11 / 977 . short 3 . John Brooke and others . 1996 . SUS - A quick and dirty usability scale . Usability evaluation in industry 189 , 194 ( 1996 ) , 4 – 7 . 4 . G Brundtland , M Khalid , S Agnelli , S Al - Athel , B Chidzero , L Fadika , V Hauff , I Lang , M Shijun , M . M de Botero , and M Singh . 1987 . Our Common Future . Technical Report . 300 pages . DOI : http : / / dx . doi . org / 10 . 1080 / 07488008808408783 5 . Jeffrey A Burke , D Estrin , Mark Hansen , Andrew Parker , Nithya Ramanathan , Sasank Reddy , and Mani B Srivastava . 2006 . Participatory sensing . In WSW’06 at SenSys ’06 . ACM , Colorado , USA . http : / / sensys . acm . org / 2006 / 6 . Alan Chamberlain , Mark Paxton , Kevin Glover , Martin Flintham , Dominic Price , Chris Greenhalgh , Steve Benford , Peter Tolmie , Eiman Kanjo , Amanda Gower , Andy Gower , Dawn Woodgate , and Danaë Stanton Fraser . 2013 . Understanding mass participatory pervasive computing systems for environmental campaigns . Personal and Ubiquitous Computing ( 2013 ) , 1 – 18 . DOI : http : / / dx . doi . org / 10 . 1007 / s00779 - 013 - 0756 - x 7 . Jason Chilvers , Irene Lorenzoni , Geraldine Terry , Paul Buckley , John K Pinnegar , and Stefan Gelcich . 2014 . Public engagement with marine climate change issues : ( Re ) framings , understandings and responses . Global Environmental Change 29 ( 2014 ) , 165 – 179 . 8 . Seth Cooper , Firas Khatib , Adrien Treuille , Janos Barbero , Jeehyung Lee , Michael Beenen , Andrew Leaver - Fay , David Baker , Zoran Popovi´c , and others . 2010 . Predicting protein structures with a multiplayer online game . Nature 466 , 7307 ( 2010 ) , 756 – 760 . 9 . Edward L . Deci and Richard M . Ryan . 2012 . Motivation , personality , and development within embedded social contexts : An overview of self - determination theory . The Oxford handbook of human motivation ( 2012 ) , 85 – 107 . 10 . Sebastian Deterding . 2014 . Eudaimonic Design , or : Six Invitations to Rethink Gamiﬁcation . SSRN Scholarly Paper ID 2466374 . Social Science Research Network , Rochester , NY . https : / / papers . ssrn . com / abstract = 2466374 11 . Sebastian Deterding , Dan Dixon , Rilla Khaled , and Lennart Nacke . 2011 . From Game Design Elements to Gamefulness : Deﬁning " Gamiﬁcation " . In Proceedings of the 15th International Academic MindTrek Conference : Envisioning Future Media Environments ( MindTrek ’11 ) . ACM , New York , NY , USA , 9 – 15 . DOI : http : / / dx . doi . org / 10 . 1145 / 2181037 . 2181040 12 . Darina Dicheva , Christo Dichev , Gennady Agre , and Galia Angelova . 2015 . Gamiﬁcation in education : A systematic mapping study . Educational Technology & Society 18 , 3 ( 2015 ) , 75 – 88 . 13 . Janis L . Dickinson , Jennifer Shirk , David Bonter , Rick Bonney , Rhiannon L . Crain , Jason Martin , Tina Phillips , and Karen Purcell . 2012 . The current state of citizen science as a tool for ecological research and public engagement . Frontiers in Ecology and the Environment 10 , 6 ( 2012 ) , 291 – 297 . DOI : http : / / dx . doi . org / 10 . 1890 / 110236 14 . Sheena Erete and Jennifer O Burrell . 2017 . Empowered Participation : How Citizens Use Technology in Local Governance . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM , 2307 – 2319 . 15 . Enrique Estelles - Arolas and Fernando Gonzalez - Ladron - de Guevara . 2012 . Towards an integrated crowdsourcing deﬁnition . Journal of Information Science 38 , 2 ( 2012 ) , 189 – 200 . http : / / jis . sagepub . com / content / 38 / 2 / 189 . full . pdf + html 16 . Deborah Estrin , K Mani Chandy , R Michael Young , Larry Smarr , Andrew Odlyzko , David Clark , Viviane Reding , Toru Ishida , Sharad Sharma , Vinton G Cerf , and others . 2010 . Participatory sensing : applications and architecture [ internet predictions ] . IEEE Internet Computing 14 , 1 ( 2010 ) , 12 – 42 . 17 . Derek Foster , Mark Blythe , Paul Cairns , and Shaun Lawson . 2010 . Competitive carbon counting : can social networking sites make saving energy more enjoyable ? . In CHI’10 Extended Abstracts on Human Factors in Computing Systems . ACM , 4039 – 4044 . 18 . Jon Froehlich , Tawanna Dillahunt , Predrag Klasnja , Jennifer Mankoff , Sunny Consolvo , Beverly Harrison , and James A Landay . 2009 . UbiGreen : investigating a mobile tool for tracking and supporting green transportation habits . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , 1043 – 1052 . 19 . Mohammad Gharesifard and Uta Wehn . 2016 . To share or not to share : Drivers and barriers for sharing data via online amateur weather networks . Journal of Hydrology 535 ( 2016 ) , 181 – 190 . DOI : http : / / dx . doi . org / 10 . 1016 / j . jhydrol . 2016 . 01 . 036 20 . Benjamin Gnauk , Lars Dannecker , and Martin Hahmann . 2012 . Leveraging gamiﬁcation in demand dispatch systems . In Proceedings of the 2012 Joint EDBT / ICDT workshops . ACM , 103 – 110 . 21 . Jeffrey Goldman , Katie Shilton , Jeff Burke , Deborah Estrin , Mark Hansen , Nithya Ramanathan , Sasank Reddy , and Vids Samanta . 2009 . Participatory Sensing : A citizen - powered approach to illuminating the patterns that shape our world . Technical Report May . UCLA’s Center for Embedded Networked Sensing . 22 . Connie Golsteijn , Sarah Gallacher , Licia Capra , and Yvonne Rogers . 2016 . Sens - Us : Designing Innovative Civic Technology for the Public Good . In Proceedings of the 2016 ACM Conference on Designing Interactive Systems . ACM , 39 – 49 . http : / / dl . acm . org / citation . cfm ? id = 2901877 23 . J . Hamari , J . Koivisto , and H . Sarsa . 2014 . Does Gamiﬁcation Work ? – A Literature Review of Empirical Studies on Gamiﬁcation . In 2014 47th Hawaii International Conference on System Sciences ( HICSS ) . 3025 – 3034 . DOI : http : / / dx . doi . org / 10 . 1109 / HICSS . 2014 . 377 24 . Eric Hand . 2010 . People Power . Nature 466 , 5 ( 2010 ) , 685 – 687 . DOI : http : / / dx . doi . org / 10 . 1038 / news . 2010 . 106 25 . Mike Harding , Bran Knowles , Nigel Davies , and Mark Rounceﬁeld . 2015 . HCI , Civic Engagement & Trust . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM , 2833 – 2842 . 26 . Mike Hazas and James Scott . 2012 . Sustainability Does Not Begin with the Individual . Interactions 19 , 5 ( 2012 ) , 14 – 17 . http : / / dl . acm . org / citation . cfm ? id = 2334189 27 . Scott Heggen . 2013 . Participatory sensing : repurposing a scientiﬁc tool for STEM education . interactions 20 , 1 ( 2013 ) , 18 – 21 . 28 . Mike Hulme . 2010 . Problems with making and governing global kinds of knowledge . Global Environmental Change 20 , 4 ( 2010 ) , 558 – 564 . DOI : http : / / dx . doi . org / 10 . 1016 / j . gloenvcha . 2010 . 07 . 005 29 . Kai Huotari and Juho Hamari . 2012 . Deﬁning Gamiﬁcation : A Service Marketing Perspective . In Proceeding of the 16th International Academic MindTrek Conference ( MindTrek ’12 ) . ACM , New York , NY , USA , 17 – 22 . DOI : http : / / dx . doi . org / 10 . 1145 / 2393132 . 2393137 30 . Bran Knowles . 2013 . Re - Imagining Persuasion : Designing for Self - Transcendence . ( 2013 ) . http : / / dl . acm . org / citation . cfm ? id = 2479498 31 . Bran Knowles , Lynne Blair , Paul Coulton , and Mark Lochrie . 2014 . Rethinking Plan A for Sustainable HCI . In Proceedings of the 32nd annual ACM conference on Human factors in computing systems . ACM , 3593 – 3596 . http : / / dl . acm . org / citation . cfm ? id = 2557311 32 . Bran Knowles and Janet Davis . 2017 . Is Sustainability a Special Case for Persuasion ? Interacting with Computers 29 , 1 ( 2017 ) , 58 – 70 . DOI : http : / / dx . doi . org / 10 . 1093 / iwc / iww005 33 . Johanna Korhonen and Eliisa Haavanlammi . 2012 . Hydrologinen vuosikirja 2006 – 2010 / Hydrological Yearbook 2006 – 2010 . ( 2012 ) . 34 . Raph Koster . 2013 . Theory of fun for game design . " O’Reilly Media , Inc . " . 35 . James R . Lewis . 1995 . IBM computer usability satisfaction questionnaires : psychometric evaluation and instructions for use . International Journal of Human - Computer Interaction 7 , 1 ( 1995 ) , 57 – 78 . DOI : http : / / dx . doi . org / 10 . 1080 / 10447319509526110 36 . Yefeng Liu , Todorka Alexandrova , and Tatsuo Nakajima . 2011 . Gamifying intelligent environments . In Proceedings of the 2011 international ACM workshop on Ubiquitous meta user interfaces . ACM , 7 – 12 . 37 . John Markoff . 2010 . In a Video Game , Tackling the Complexities of Protein Folding . ( 2010 ) . http : / / www . nytimes . com / 2010 / 08 / 05 / science / 05protein . html 38 . Aaron D Mason , Georgios Michalakidis , and Paul J Krause . 2012 . Tiger nation : Empowering citizen scientists . In Digital Ecosystems Technologies ( DEST ) , 2012 6th IEEE International Conference on . IEEE , 1 – 5 . 39 . Elaine Massung , David Coyle , Kirsten F Cater , Marc Jay , and Chris Preist . 2013 . Using crowdsourcing to support pro - environmental community activism . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , 371 – 380 . 40 . Tanushree Mitra , Clayton J Hutto , and Eric Gilbert . 2015 . Comparing person - and process - centric strategies for obtaining quality data on amazon mechanical turk . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM , 1345 – 1354 . 41 . Andreas Möller , Matthias Kranz , Barbara Schmid , Luis Roalter , and Stefan Diewald . 2013 . Investigating self - reporting behavior in long - term studies . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , 2931 – 2940 . 42 . B . Morschheuser , J . Hamari , and J . Koivisto . 2016 . Gamiﬁcation in Crowdsourcing : A Review . In 2016 49th Hawaii International Conference on System Sciences ( HICSS ) . 4375 – 4384 . DOI : http : / / dx . doi . org / 10 . 1109 / HICSS . 2016 . 543 43 . Lennart E . Nacke and Sebastian Deterding . 2017a . The maturing of gamiﬁcation research . Computers in Human Behavior 71 ( June 2017 ) , 450 – 454 . DOI : http : / / dx . doi . org / 10 . 1016 / j . chb . 2016 . 11 . 062 44 . Lennart E . Nacke and Sebastian Deterding . 2017b . The maturing of gamiﬁcation research . Computers in Human Behavior 71 , Supplement C ( 2017 ) , 450 – 454 . DOI : http : / / dx . doi . org / https : / / doi . org / 10 . 1016 / j . chb . 2016 . 11 . 062 45 . Timothy Nyerges , Michael Barndt , and Kerry Brooks . 2006 . Public Participation Geographic Information Systems . Annals of the Association of American Geographers Volume 96 , Smith ( 2006 ) , 224 – 233 . 46 . Chris Preist , Elaine Massung , and David Coyle . 2014 . Competing or aiming to be average ? : normiﬁcation as a means of engaging digital volunteers . In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing . ACM , 1222 – 1233 . 47 . Mark S Reed . 2008 . Stakeholder participation for environmental management : A literature review . 1 ( 2008 ) . DOI : http : / / dx . doi . org / 10 . 1016 / j . biocon . 2008 . 07 . 014 48 . Andrew Rollings and Ernest ( Ernest W . ) Adams . 2003 . Andrew Rollings and Ernest Adams on game design . New Riders . 621 pages . 49 . Joel Ross . 2011 . Pervasive Negabehavior Games for Environmental Sustainability . In CHI’11 . ACM , Vancouver , 1085 – 1088 . 50 . Sonia Royo , Ana Yetano , and Basilio Acerete . 2011 . E - participation and climate change : Are local governments actively promoting responsible behaviors and offering opportunities for citizen involvement ? Proceedings of the Annual Hawaii International Conference on System Sciences ( 2011 ) , 2462 – 2471 . DOI : http : / / dx . doi . org / 10 . 1109 / HICSS . 2012 . 248 51 . Elizabeth B - N Sanders and Pieter Jan Stappers . 2008 . Co - creation and the new landscapes of design . Co - design 4 , 1 ( 2008 ) , 5 – 18 . 52 . Katie Seaborn and Deborah I . Fels . 2015 . Gamiﬁcation in theory and action : A survey . International Journal of Human - Computer Studies 74 ( Feb . 2015 ) , 14 – 31 . DOI : http : / / dx . doi . org / 10 . 1016 / j . ijhcs . 2014 . 09 . 006 53 . Six Silberman , Eli Blevis , Elaine Huang , Bonnie Nardi , Lisa Nathan , Daniela Busse , Chris Preist , and Samuel Mann . 2014 . What Have We Learned ? A SIGCHI HCI & Sustainability Workshop . In CHI’14 Extended Abstracts on Human Factors in Computing Systems . ACM , 143 – 146 . http : / / dl . acm . org / citation . cfm ? id = 2559238 54 . Yolande Strengers . 2014 . Smart energy in everyday life : Are you designing for resource man ? Interactions 21 , 4 ( 2014 ) , 24 – 31 . http : / / dl . acm . org / citation . cfm ? id = 2621931 55 . Sonia Talwar , Arnim Wiek , and John Robinson . 2011 . User engagement in sustainability research . Science and Public Policy 38 , 5 ( 2011 ) , 379 – 390 . 56 . UNECE . 1998 . Aarhus Convention : Convention on Access to Information , Public Participation in Decision - making and Access to Justice in Environmental Matters . Aarus Convention June ( 1998 ) , 25 . DOI : http : / / dx . doi . org / 10 . 1017 / CBO9780511494345 . 010 57 . United Nations . 1992a . Agenda 21 . In United Nations Conference on Environment & Development Rio de Janerio . 351 . DOI : http : / / dx . doi . org / 10 . 1007 / s11671 - 008 - 9208 - 3 58 . United Nations . 1992b . RIO DECLARATION ON ENVIRONMENT AND DEVELOPMENT . ( 1992 ) . http : / / www . un . org / documents / ga / conf151 / aconf15126 - 1annex1 . htm 59 . University of Washington . 2014 . Foldit game engages the public in research . ( 2014 ) . https : / / www . engr . washington . edu / news / trend / aut10 60 . Rob van Roy and Bieke Zaman . 2015 . Moving Beyond the Effectiveness of Gamiﬁcation . In CHI ’15 workshop ‘Researching Gamiﬁcation : Strategies , Opportunities , Challenges , Ethics’ . Seoul , South Korea . 61 . Viswanath Venkatesh . 2000 . Determinants of perceived ease of use : Integrating control , intrinsic motivation , and emotion into the technology acceptance model . Information systems research 11 , 4 ( 2000 ) , 342 – 365 . DOI : http : / / dx . doi . org / 10 . 1287 / isre . 11 . 4 . 342 . 11872 62 . Viswanath Venkatesh , Michael G . Morris , Gordon B . Davis , and Fred D . Davis . 2003 . User Acceptance of Information Technology : Toward a Uniﬁed View . MIS Quarterly 27 , 3 ( 2003 ) , 425 – 478 . 63 . Vasilis Vlachokyriakos , Eric Gordon , Clara Crivellaro , Pete Wright , Christopher Le Dantec , and Patrick Olivier . 2016 . Digital civics : Citizen empowerment with and through technology . In Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems . ACM , San Jose , CA , 1096 – 1099 . http : / / dl . acm . org / citation . cfm ? id = 2886436 64 . Uta Wehn , Maria Rusca , Jaap Evers , and Vitavesca Lanfranchi . 2015 . Participation in ﬂood risk management and the potential of citizen observatories : A governance analysis . Environmental Science & Policy 48 ( 2015 ) , 225 – 236 . DOI : http : / / dx . doi . org / 10 . 1016 / j . envsci . 2014 . 12 . 017 65 . Eric N Wiebe , Allison Lamb , Megan Hardy , and David Sharek . 2014 . Measuring engagement in video game - based environments : Investigation of the User Engagement Scale . Computers in Human Behavior 32 ( 2014 ) , 123 – 132 . 66 . Maximilian Witt , Christian Scheiner , and Susanne Robra - Bissantz . 2011 . Gamiﬁcation of online idea competitions : Insights from an explorative case . Informatik schafft Communities 192 ( 2011 ) . 67 . Claes Wohlin , Per Runeson , Martin Höst , Magnus C Ohlsson , Björn Regnell , and Anders Wesslén . 2012 . Experimentation in software engineering . Springer Science & Business Media . 68 . Gabe Zichermann and Christopher . Cunningham . 2011 . Gamiﬁcation by design : implementing game mechanics in web and mobile apps . O’Reilly Media . 182 pages .