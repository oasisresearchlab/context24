Learning Relational Representations by Analogy using Hierarchical Siamese Networks Gaetano Rossiello † , Alﬁo Gliozzo ‡ , Robert Farrell ‡ , Nicolas Fauceglia ‡ , Michael Glass ‡ † Department of Computer Science , University of Bari , Italy ‡ IBM Research AI , Yorktown Heights , NY , US gaetano . rossiello @ uniba . it , gliozzo @ us . ibm . com , robfarr @ us . ibm . com , nicolas . fauceglia @ ibm . com , mrglass @ us . ibm . com Abstract We address relation extraction as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions . In our assumption , if two pairs of entities belong to the same rela - tion , then those two pairs are analogous . Fol - lowing this idea , we collect a large set of anal - ogous pairs by matching triples in knowledge bases with web - scale corpora through distant supervision . We leverage this dataset to train a hierarchical siamese network in order to learn entity - entity embeddings which encode rela - tional information through the different lin - guistic paraphrasing expressing the same re - lation . We evaluate our model in a one - shot learning task by showing a promising gen - eralization capability in order to classify un - seen relation types , which makes this approach suitable to perform automatic knowledge base population with minimal supervision . More - over , the model can be used to generate pre - trained embeddings which provide a valuable signal when integrated into an existing neural - based model by outperforming the state - of - the - art methods on a downstream relation ex - traction task . 1 Introduction The task of identifying semantic relationships be - tween entities in unstructured textual corpora , namely Relation Extraction ( RE ) , is often a pre - requisite for many other natural language under - standing tasks , e . g . automatic knowledge base population , question answering , etc . RE is com - monly addressed as a classiﬁcation task ( Bunescu et al . , 2005 ) , where a model is trained to classify relation mentions in text among a predeﬁned set of relation types . For instance , given the sentence “Robert Plant is the singer of the band Led Zep - pelin” , an effective RE system might extract the triple memberOf ( R OBERT P LANT , L ED Z EP - PELIN ) , where memberOf is a relation label ex - pressed by the linguistic context “is the singer of the band” . Since a given relation can be expressed us - ing different textual patterns surrounding entities , the state - of - the - art RE models which follow this approach need a considerable amount of exam - ples for each relation to reach satisfactory perfor - mance . Distant supervision ( Mintz et al . , 2009 ) instead uses training examples from a knowl - edge base , guaranteeing a large amount of ( pop - ular ) relation examples without human interven - tion , which can be used effectively by neural net - works ( Lin et al . , 2016 ; Glass et al . , 2018 ) . How - ever , even with this technique , approaching RE as a classiﬁcation task presents several limitations : ( 1 ) distant supervision models are not accurate in extracting relations with a long - tailed distribution , because they typically have a small set of instances in knowledge bases ; ( 2 ) in most domains , relation types are very speciﬁc and only a few examples of each relation are available ; ( 3 ) these models can - not be applied to recognize new relation types not observed during training . In this paper , we address RE from a different perspective by reducing it to an analogy problem . Our assumption states that if two pairs of entities , ( A , B ) and ( C , D ) , have at least one relation in common r , then those two pairs are analogous . Viceversa , solving proportional analogies , such as A : B = C : D , consists of identifying the im - plicit relations shared between two pairs of enti - ties . For example , R OME : I TALY = P ARIS : F RANCE is a valid analogy because capitalOf is a rela - tion in common . Based on this idea , we propose an end - to - end neural model able to measure the degree of ana - logical similarity between two entity pairs , instead of predicting a conﬁdence score for each relation type . An entity pair is represented through its mentions in a textual corpus , sequences of sen - tences where entities in the pair co - occur . If a mention represents a speciﬁc relation type , then this relationship is expressed by the linguistic con - text surrounding the two entities . E . g . , “Rome is the capital of Italy” or “The capital of France is Paris” referring to the example above . Thus , given two analogous entity pairs represented by their textual mentions sets as input , the model is trained to minimize the difference between the represen - tations of relations having the same linguistic pat - terns . In other words , the model learns the differ - ent paraphrases expressing the same relation . In our research hypothesis , a model trained in such way is able to recognize analogies between unseen entity pairs belonging to new unseen relation types by : ( 1 ) generalizing over the sequence of words in the mentions ; ( 2 ) projecting the sequence of words in the mentions into a vector space representing relational semantics . This approach poses several research questions : ( RQ1 ) How to collect and or - ganize a dataset for training ? ( RQ2 ) What kinds of models are effective for this task ? ( RQ3 ) How should the model be evaluated ? Knowledge bases , such as Wikidata or DBpedia , consist of large relational data sources organized in the form of triples , predicate ( S UBJECT , O BJECT ) . We ex - ploit this information to build a reliable set of analogous facts used as ground truth . Then , we adopt distant supervision to retrieve relation men - tions in web - scale textual corpora by matching the subject - object entities which co - occur in the same sentences ( Riedel et al . , 2010 ; ElSahar et al . , 2018 ; Glass and Gliozzo , 2018a ) . Through this technique we can train our model on millions of analogy examples without human supervision . Since our goal is to train a model able to com - pute the relational similarity given two sets of textual mentions , we use siamese networks to learn discriminative features between those two instances ( Hadsell et al . , 2006 ) . This kind of neu - ral network has been used in both computer vi - sion ( Koch et al . , 2015 ) and natural language pro - cessing ( Mueller and Thyagarajan , 2016 ; Necu - loiu et al . , 2016 ) in order to map two similar in - stances close in a feature space . However , in our setting each instance consists of a set of mentions , therefore it is inherently a multi - instance learning task 1 . We propose a hierarchical siamese network 1 Due to the weak supervision , the whole set of mentions with an attention mechanism at both word level ( Yang et al . , 2016 ) and at the set level ( Ilse et al . , 2018 ) in order to select the textual mention which better describes the relation . To the best of our knowledge , this is the ﬁrst application of a siamese network by pairing sets of instances , so it can be considered a novelty of this work . We evaluate the generalization capability of our model in recognizing unseen relation types through an one - shot relational classiﬁcation task introduced in this paper . We train the parameters of the model on a subset of most frequent rela - tions of one of three different distantly supervised datasets used in our experiments . Then , we eval - uate it on the long - tailed relations of each dataset . During the test phase , only a single example for each unseen relation is provided . This example is not used to update the parameters of the model as in a classiﬁcation task , but rather to produce the vector representation of the relation itself . Entity pairs having mention sets close to this representa - tion are more likely to be analogous . The experi - ments show promising results of our approach on this task , compared with the recent deep models commonly used for encoding textual representa - tions ( Conneau et al . , 2017 ) . However , when the number of the unseen relation types increases , the performance of our model become far from the re - sults obtained in the one - shot image classiﬁcation ( Koch et al . , 2015 ) , opening an interesting chal - lenge for future work . Finally , our model shows a transfer capability in other tasks through the use of its pre - trained vec - tors . Indeed , a branch of the hierarchical siamese network can be used to generate entity - entity rep - resentations given sets of mentions as input , that we call analogy embeddings . In our experiments , we integrate those representations into an exist - ing end - to - end model based on convolutional net - works ( Glass and Gliozzo , 2018b ) , outperfoming the state - of - the - art systems on two shared datasets commonly used for distantly supervised relation extraction . 2 Related Work Relation Extraction Several approaches have been proposed in the literature to address the prob - lem of extracting relations from text with minimal supervision . The bootstrapping method ( Agichtein and Gra - is labeled , but each individual mention in the set is unlabeled . vano , 2000 ) collects the textual patterns between a few example pairs of entities iteratively , and uses them to retrieve other pairs of entities from a cor - pus . This method is limited by the semantic drift issue since wrong patterns might be collected . OpenIE ( Mausam et al . , 2012 ) is an unsu - pervised method for extracting triples from text , where the relations are linguistic phrases . The lack of a canonical form for the extracted rela - tions makes this approach not suitable to populate knowledge bases with a ﬁxed schema . Universal schema ( Riedel et al . , 2013 ) ad - dresses RE by combining the OpenIE and knowl - edge base relations through a matrix factoriza - tion technique typically adopted in the collabo - rative ﬁltering approach of recommendation sys - tems . The column - less ( Toutanova et al . , 2015 ) and row - less ( Verga and McCallum , 2016 ) exten - sions of this method can handle unseen entity pairs and textual relations when combined ( Verga et al . , 2017 ) . The one - shot RE has been addressed by ( Yuan et al . , 2017 ) , who adopt a siamese network to ex - tract ﬁne - grained relations which typically have few training examples . This model has two main limitations . Firstly , it works only by pairing two single mentions and is not able to handle a whole set of mentions referring to a relation instance . Our hierarchical siamese network overcomes this issue by using an attention mechanism at both word and mention level . Moreover , their one - shot evaluation mainly focuses on extracting the same relation types seen during training . Instead , the goal of our one - shot task is to evaluate the trans - fer capability in extracting unseen relation types across domains using a single pre - trained model . Recently , ( Levy et al . , 2017 ) propose to reduce RE slot - ﬁlling to a question answering problem . The main idea is to build a set of question - answer pairs for the relations in knowledge bases and train a reading comprehension model using this dataset . This approach shows promising zero - shot capabil - ity in extracting unseen relation types . However , the schema queriﬁcation phase requires a crowd - sourcing effort . Our method uses distant supervi - sion , so it does not need any kind of manual anno - tations . Word Analogy The analogy problem , from a computational linguistic perspective , was origi - nally addressed by ( Turney , 2006 ) who investi - gate several similarity measures for solving word analogy questions in the Scholastic Aptitude Test dataset . The authors provide an interesting argu - ment regarding the different types of similarities , attributional and relational , and their use in solv - ing word analogies . Attributional similarity , typi - cal of the word vector space models , is useful for synonym detection , word sense disambiguation and so on . Instead , relational similarity is suitable for understanding analogies between two pairs of words . Our neural - based analogy approach is in - spired by this ﬁnding . Recently , word analogies , namely the propor - tional analogy between two word pairs such as a : b = c : d , have been used by ( Mikolov et al . , 2013 ; Pennington et al . , 2014 ) to show the capabil - ity of word embeddings to discover linguistic reg - ularities in word contexts using vector offsets ( e . g . king − man + woman = queen ) . The works in ( Gladkova et al . , 2016 ; Vylomova et al . , 2016 ) explore the use of word vectors to model the se - mantic relations . The proportional analogy is also adopted by ( Liu et al . , 2017 ) as analogical infer - ence in order to learn multi - relational embeddings which are evaluated on knowledge base comple - tion benchmarks . However , in order to apply word embedding models to proportional analogy , the model must have seen the words during training . This ap - proach is unsuitable for computing the analogy between out - of - vocabulary words . Our approach overcomes this limitation , since it works by con - sidering the contexts where the entities occur . 3 Learning Relations by Analogy Given two pairs of entities , ( A , B ) and ( C , D ) , their semantic relations can be expressed by their mentions in text , ( A , B ) = { S i } and ( C , D ) = { S j } . Speciﬁcally , { S i } and { S j } are the sets of sentences where ( A , B ) and ( C , D ) co - occur in the same set . Two pairs of entities are analo - gous , A : B = C : D , iff their mentions sets , or part of them , express the same relation r . Knowl - edge bases , such as Wikidata , contain millions of trusted facts in form of triples , r ( A , B ) , namely pairs of entities in known relationships . We lever - age these relational data sources as ground truth in order to collect a set of proportional analogy state - ments . Then , we build a dataset through the distant supervision technique by retrieving the mentions sets from web - scale corpora . Our idea is to train a neural network to solve Figure 1 : Learning relations by analogy through matching the facts from knowledge bases with textual corpora . the analogy problem between any two entity pairs in this dataset , as long as they are described by the textual contexts where they co - occur . In other words , this task is reduced to a binary classiﬁca - tion of determining whether the relational similar - ity between the representations of two sets of men - tions exceeds a threshold . The network is trained by feeding two sets of mentions related to two dif - ferent pairs , and it is optimized to return a positive label if the two entity pairs are analogous , namely they share at least one relation , or a negative label otherwise . Figure 1 provides an example of this pro - cess . For the relation memberOf , the entity pairs ( R OBERT P LANT , L ED Z EPPELIN ) and ( D AVID G ILMOUR , P INK F LOYD ) are sampled . The two entity pairs are converted into their respectively mentions sets gathered from a textual corpus , such as Wikipedia . Since these two entity pairs are analogous , the network is optimized to learn the representations of the two textual contexts to be close into the feature space . In fact , the ﬁrst sen - tences of both pairs represent the concept of mem - bership of a band , even if they are expressed using different words . Based on our assumption , the aim is to learn how to encode the relational representa - tions through the different paraphrases of the same relation . However , the model also needs negative examples during the training phase . We randomly select an entity pair from a different relation for each positive example , such as ( R OBERT P LANT , L ED Z EPPELIN ) and ( P ARIS , F RANCE ) . Since we cast the problem as a binary classiﬁcation task , we create a balanced dataset of positive and negative examples . Siamese neural networks ( Bromley et al . , 1993 ) are well suited to this task because they are specif - ically designed to compute the similarity between two instances . A siamese network has symmetric twin sub - networks which share the same param - eters , but are joined by an energy function at the head . Weight sharing forces the two similar in - stances to be mapped to very close locations in feature space because both of the sub - networks are optimized using the same function . In com - puter vision , siamese architectures based on con - volutional neural networks ( Hadsell et al . , 2006 ; Koch et al . , 2015 ; Vinyals et al . , 2016 ) have shown promising performance in learning highly discrim - inative features by pairing images that belong to the same class . Likewise , our hypothesis is that a siamese network trained by matching two distinct mentions sets that share the same relation is able to learn how to map patterns of words across the sen - tences containing the two pairs of entities so as to capture the semantics of the relation . For instance , given the example in Figure 1 an effective siamese network should determine that the patterns for “is the lead singer” and “was the guitarist” express the same relation , memberOf . To train a siamese network based on our ap - proach , we have to face the following challenges : ( 1 ) the language may be highly variable and the same relation expressed in a multitude of differ - ent ways ; ( 2 ) the mentions set of an entity pair consists of several sentences , each of which might express different relations , hence this is a multi - instance learning problem ; ( 3 ) distant supervision could provide a wrong labeling , namely sentences which do not express any speciﬁc relations . Figure 2 : Hierarchical siamese network . 4 Hierarchical Siamese Network To face these challenges , we propose a Hierarchi - cal Siamese Network ( HSN ) architecture as shown in Figure 2 . In the following paragraphs , we de - scribe the details of each component . Input Representation The HSN takes as input two entity pairs represented by their mentions sets . Since the twin sub - networks of the HSN are the same , we focus only on one of these . Given a triple r ( A , B ) from a knowledge base , the rela - tion r can be expressed through the set of sen - tences in a textual corpus where the the two en - tities co - occur : r ( A , B ) = { S 1 , S 2 , . . . , S n } , with S i = { w i 1 , w i 2 , . . . , w ik } , where w ij represents the j - th word in the sentences S i , ∀ i ∈ 1 ≤ i ≤ n and ∀ j ∈ 1 ≤ i ≤ k . The purpose of a sub - network is to learn a low - rank vector representa - tion r A , B for the relation r expressed by the pair ( A , B ) . This is done by hierarchically composing the word and sentence representations . Gated Recurrent Unit for Sentence Encoder Given a sentence S i = { w i 1 , w i 2 , . . . , w ik } , we map each one - hot word representation of w ij into its word embedding x ij = Ew ij , where E d , | V | is a matrix of real - valued vectors of size d , and V is a ( ﬁxed ) vocabulary . Word embeddings are designed to encode syntactic and semantic fea - tures of words and can be randomly initialized or pre - trained on large corpora . We use pre - trained GloVe ( Pennington et al . , 2014 ) embeddings for our purposes . We encode the whole sentence S i into a low - rank representation by composing its constituent word embeddings . An effective way to perform such an encoding is using recurrent neural networks ( RNN ) which are able to compose word embeddings by taking into account their positions in the sentence conditioned on the previous words . For our model , this capability is critical in order to detect sequences of words which express a par - ticular relation , such as “is the capital of” . We use a bidirectional GRU ( Bahdanau et al . , 2014 ) to gather the information from both directions for words . Formally , given −→ h ij = −−−→ GRU ( x ij ) and ←− h ij = ←−−− GRU ( x ij ) , the hidden state h ij = [ −→ h ij , ←− h ij ] is a new dense representation of w ij which en - codes also the information of the whole sentence . Word Attention with Context Vector How - ever , only certain words in a sentence express the semantics of a relation , therefore we need a strategy to automatically identify them during the training . For example , the words “singer” and “guitarist” at both sides of Figure 1 are good candidates to express the relation member . We use the attention mechanism with a context vec - tor proposed in ( Yang et al . , 2016 ) to reward such words which are important to the mean - ing of a relation and then aggregate their infor - mation in the sentence representation . In detail , s i = (cid:80) k α ik h ik , where α ij = exp ( u Tij u w ) (cid:80) k exp ( u Tik u w ) , and u ij = tanh ( W w h ij + b w ) . The vector s i rep - resents the sentence S i and is computed as the weighted sum of the GRU - based word vectors h ij using the normalized attention weights α ij . The parameters for the attention mechanism are the weights and biases W w , b w and u w , the context vector , a global ﬁxed vector which , independently from a speciﬁc word , represents a kind of query which helps to inform what is the most informa - tive word for each analogy . The context vector u w essentially works like a memory mechanism , as described in ( Sukhbaatar et al . , 2015 ; Kumar et al . , 2016 ) . Attention for Multi - instance Relation Repre - sentation Once all sentences in the mentions set are encoded , the aim of the last layer of a sub - network is to produce the vector r A , B which repre - sents the pair ( A , B ) . However , while weak super - vision guarantees a large amount of training data without any human intervention , wrongly labeled sentences inevitably occur . For instance , the S2 of the pair on the right side in the Figure 1 does not express the relation member precisely , therefore a wrong bias could propagate during the training phase . This issue is typically addressed through a multi - instance setting , where a model should identify the correct instance ( s ) from a bag . Re - cently , end - to - end neural architectures have been proposed to address this multi - instance classiﬁca - tion problem ( Wang et al . , 2018 ; Feng and Zhou , 2017 ) by proposing several ways to aggregate the unlabeled instances , such as taking their average . Our goal is to have a model which is able to prop - erly select the most relevant sentences by ascribing different weights to the encoded sentences . For this purpose , we adopt an attention mechanism at the sentence level . It is important to point out that the sentences in the mentions set do not have any temporal relationship , therefore we adapt the stan - dard attention strategies as described in ( Ilse et al . , 2018 ) . In detail , r A , B = (cid:80) i α i s i is the embed - ding of the relation r given the pair ( A , B ) , with α i = exp ( u Ti u s ) (cid:80) k exp ( u Tk u s ) , u i = tanh ( W s s i + b s ) , where W s and u s are parameters . Merging Layer and Training Strategy There are several ways to merge the output of the two sub - networks in order to learn the analogical sim - ilarity between them . For instance , ( Hadsell et al . , 2006 ) propose a contrastive loss with the aim of decreasing the distance between two instance rep - resentations . However , we adopt the strategy pro - posed in ( Koch et al . , 2015 ) , in which the met - ric distance is induced by a fully - connected layer with a sigmoidal output unit on the absolute differ - ence between the representations output by twin networks . Thus , given r A , B and r C , D the two re - lation embeddings which encode the whole men - tions sets of the two entity pairs , we can com - pute the degree of analogy between them with p = σ ( W r ( | r A , B − r C , D | ) ) , where the parame - ters W r measure the importance of each element of the difference vector , and they are learned in a end - to - end fashion , together with the relation rep - resentations . We build a training set by pairing the mentions sets of the entity - entity pairs from a KB , following the idea discussed in the next section . Thus , we can reduce the analogy task to a binary classiﬁcation problem , so that p = P ( ( A , B ) , ( C , D ) ; Θ ) is equal to 1 if A : B = C : D , 0 otherwise . We learn Θ ( all the parameters of HSN ) using a gradient - based method which min - imizes a cross - entropy loss function with the L2 regularization . NYT - FB CC - DBP T - REX Knowledge Base Freebase DBpedia Wikidata Corpus New York Times Common Crawl Wikipedia # words 239 , 877 8 , 445 , 417 4 , 062 , 498 # entity pairs 375 , 846 6 , 876 , 913 6 , 413 , 452 # relations 57 298 685 avg . mentions 1 . 9 3 . 8 3 . 2 avg . sent . length 41 37 25 Table 1 : Statistics of the distantly supervised datasets . 5 Experiments Once the analogy model is trained , it has two dif - ferent capabilities . First , the whole HSN architec - ture can be used as a binary classiﬁer in order to in - fer if two entity pairs , expressed by their mentions sets , are analogous . Second , we can use its sub - network before the merge layer as a feature ex - tractor to generate entity - entity vectors given sets of sentences as input which can be used as pre - trained analogy embeddings in other tasks . We evaluate our model on the one - shot relational clas - siﬁcation and distantly supervised relation extrac - tion benchmarks . 5 . 1 Datasets In the entire experimental protocol we exploit three different datasets ( see the supplemental ma - terial for details ) . T - REX ( ElSahar et al . , 2018 ) is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples , having 685 unique relations . NYT - FB ( Riedel et al . , 2010 ) is a stan - dard benchmark for distantly supervised relation extraction . The text of New York Times was pro - cessed with a named entity recognizer and the identiﬁed entities linked by name to Freebase . CC - DBP ( Glass and Gliozzo , 2018a ) is a web - scale KB population benchmark . It combines the text of Common Crawl with the entity - relation - entity triples from 298 frequent relations in DB - pedia . Mentions of DBpedia entities are located in text by matching the preferred label . This task is similar to NYT - FB , but it has a much larger number of relations , triples and textual contexts . The statistics of the three datasets are summa - rized in Table 1 . Aside from the difference in size and KB adopted , it worth noting also the dif - ference in terms of corpus style of these datasets . For instance , T - REX has well - written textual men - tions , because the sentences are extracted from Wikipedia . Conversely , CC - DBP and NYT - FB contain dirtier sentences which mean a high prob - ability of incurring wrong labeling . 5 . 2 Training and Implementation Details For both benchmarks , we use the same analogy model trained only once on a subset of the rela - tions in T - REX . In detail , we discard all relations having less than 20 entity pairs , collecting 482 re - lations . We sort the relations by the number of instances , and we took the most frequent 60 % of them to train the HSN . We use the remaining 20 % of the relations for validation and the least frequent 20 % as a corpus to implement one of the three one - shot classiﬁcation tasks . For the validation and test partitions we randomly select only 20 entity pairs for each relation . This becomes a useful test set for the one - shot validation . To train the HSN , we select a balanced number of positive and negative examples out of the training split based on these rules : ( 1 ) for each relation , we randomly extract a set of 20 entity pairs ; ( 2 ) out of this set , we gen - erated all possibile combinations , (cid:0) 202 (cid:1) = 190 , as positive pairing examples ; ( 3 ) for each combina - tion , we create a negative example by randomly selecting an entity pair from another relation . Af - ter these steps , we collect a bucket of 109 , 820 pro - portional analogy training examples . We iterate this process throughout the training phase by selecting a different buckets at each iter - ation to prevent overﬁtting . The training is mon - itored by computing the binary accuracy over a ﬁxed validation set , consisting of 36 , 480 analogy examples , built by adopting the same criteria de - scribed above . We initialize our word embedding layer with the pre - trained GloVe vectors consist - ing of 6B tokens with 50 dimensions . The word embedding weights are not updated during train - ing . The number of mentions for each entity pair is ﬁxed to 3 , based on their average on T - REX ( see Table 1 ) . 5 . 3 One - shot Relational Classiﬁcation Task Given an unseen entity pair ( A t , B t ) and its mentions set (cid:104) A t , B t (cid:105) , the one - shot relation classiﬁcation task is to categorize this test pair ( A t , B t ) into one of N relation types , with the restriction that for each relation type r i , ∀ i ∈ N , we are given only one entity pair ( A i , B i ) together with its mentions set (cid:104) A i , B i (cid:105) as training . We can cast the one - shot classiﬁcation in terms of a rela - tional similarity as follows : r i = arg max i sim M ( (cid:104) A t , B t (cid:105) , (cid:104) A i , B i (cid:105) ) ( 1 ) where sim M is a similarity score , using the method M , which measures the analogy between the train and test entity pairs through their men - tions sets . We implemented sim HSN using the HSN trained as described above . The two men - tions sets are given as input to the network and their similarity is computed using the sigmoidal output of the last layer . Baselines A method M should be robust in fac - ing the unseen entity pairs used for testing . Train - ing a standard RE model using just one example cannot provide a suitable baseline . Furthermore , since the two new entities that we want to clas - sify might not be present in an existing knowledge graph , we could not apply relational embeddings ( Bordes et al . , 2013 ) as well . Thus , the use of the contexts surrounding the two entities in the men - tions sets to compute the relational similarity score is needed . In other words , we cast the task of one - shot relational classiﬁcation to a problem of mea - suring textual ( i . e . mentions ) similarity with the aim to prove that our pre - trained siamese model is able to grasp the semantics of relations better than the other pre - trained text representation models . We implemented ﬁve baselines commonly used to encode textual representations . First , we use the pre - trained Word2Vec ( Mikolov et al . , 2013 ) and GloVe ( Pennington et al . , 2014 ) embeddings . The score is given by the cosine similarity between the bag - of - means for the two entity pairs , aver - aging the word vectors in the mentions sets . We also adopt Doc2Vec ( Le and Mikolov , 2014 ) to de - rive entity pair vectors , and comparing them using cosine similarity . For each entity pair , a pseudo - document embedding is created by concatenating its mentions sets . Finally , we compare HSM with the pre - trained Skip - Thought ( Kiros et al . , 2015 ) and InferSent ( Conneau et al . , 2017 ) sentence en - coders , which are the state - of - the - art in computing textual similarity . An entity pair vector is obtained by averaging the embeddings of each sentence in the mentions set . One - shot trials We follow the experimental setup described in ( Koch et al . , 2015 ) to create our one - shot benchmark . For each dataset , we select the 20 % of less frequent relations sorted by the number of entity pairs , having at least 20 instances . Therefore , we collect three different one - shot test sets of 92 , 55 and 11 unseen re - lation types for T - REX , CC - DBP and NYT - FB , ( a ) T - REX ( b ) CC - DBP ( c ) NYT - FB Figure 3 : One - shot relational classiﬁcation results for N - way unseen relation types . respectively . The reason behind this criteria is to prevent the overlap of the semantic relation types between the train data and the different one - shot test sets . In fact , frequent relations , such as location or birthPlace , are common in all three datasets , and they are used to train our HSN . Moreover , using the long - tailed relations , such as portOfRegistry , we emulate a sce - nario where only a small set of relation exam - ples are available , making more challenging the task . To evaluate the one - shot capabilities on N - way classes : ( 1 ) N different relation types are se - lected ; ( 2 ) we sample one ( shot ) entity pair exam - ple for each of the selected N relation types ; ( 3 ) we choose another entity pair used as test example from one of the N relation types . All the selec - tions in these three steps are random . If the re - lation type returned by the Eq . 1 is equal to the relation type of the selected test example , then the one - shot trial is correct , otherwise it is incorrect . We repeated this operation k times for N from 2 to 10 , for each of the three datasets . We choose k equal to 10 , 000 , so that the random baseline con - verges to 100 / N , in order to create an unbiased testbed . Results and Discussion The results are reported in Figure 3 . Our model outperforms all the base - lines on the test split of T - REX , reaching an ac - curacy range from 95 . 87 % to 65 . 33 % for N - way one - shot trials . This behavior remains constant also for the other two datasets , showing the so - lidity of HSN even though it has been trained on a different corpus using relations from an another ontology . This result conﬁrms that our model is able to generalize on the linguistic contexts ex - pressing relations , as well as the capability to learn how to transfer this information to other relations not observed before . The supplemental ﬁle reports some one - shot trial examples . The lower accuracy on CC - DBP and NYT - FB might be caused by the different style of the cor - pora ( Wikipedia vs . Web pages ) . Indeed , the test set of T - REX is build using the same corpus which HSN is trained on . The Wikipedia abstracts con - sist of well - written contents , typically the deﬁni - tion of one of the two entities in the pairs . Thus , T - REX can be considered an easier dataset com - pared with the other two . Surprisingly , the average vectors using Word2Vec and GloVe obtain remarkable per - formance compared to state - of - the - art sentence encoders . This might be due to the way how these sentence models are trained . For instance , InferSent is trained using a natural language inference dataset , which might be not suitable to learn representations which represent relations in text . Instead , HSN is trained and optimized to learn and encode relational representations . However , this aspect deserves to be dealt with more deeply , as does the comparison of HSN on the shared textual similarity benchmarks ; we think this is a clear path for future research . 5 . 4 Transfer Learning in Relation Extraction We also evaluate the ability of the analogy model to provide low - rank representations for entity pairs which are useful for more traditional relation ex - traction tasks , where a corpus of text has to be processed and relevant relations in a predeﬁned schema have to be recognized . Figure 4 : Precision - Recall curves on NYT - FB . Relation / Score Entity pair / Best mention Entity pair / Best mention doctoralStudent V ICTOR W EISSKOPF : M URRAY G ELL - M ANN J OHN B ARDEEN : N ICK H OLONYAK 0 . 95 Murray Gell - Mann , one of the principal discoverers of the quarks , is one of the distinguished pupils of Victor Weisskopf . Professor Nick Holonyak jr . was the ﬁrst phd student of Nobel Prize winner John Bardeen . approvedBy H UNDRED H ORSE C HESTNUT : G UINNESS W ORLD R ECORDS NCSA O PEN S OURCE L ICENSE : O PEN S OURCE I NITIATIVE 0 . 83 Guinness World Records has listed Hundred Horse Chestnut for the record of “greatest tree girth eve” . NCSA was formally certiﬁed as an open - source license during a March 28 , 2002 board meeting of the Open Source Initiative . architecturalStyle R OCKEFELLER C ENTER : A RT D ECO S T . M ARK B ASILICA : B YZANTINE ARCHITECTURE 0 . 63 Art Deco mural “wisdom” hangs over the entrance to the Rocke - fellerCenter andwasdesignedandsculptedbyartistLeeLawrie . St . Mark’s Basilica , the cathedral of Venice , is one of the best known examples of Byzantine architecture . Table 2 : Three examples of relational similarity between two pairs of entities computed by our HSN . For each example , we report the unseen relation type , the mentions related to each pair , and the similarity score . We report only the mention having the highest attention weight . The examples show the ability of the analogy model in providing a high score to two mentions which represent the same relation , even if they are expressed using different textual contexts . To this aim , we use the sub - network of our HSN before the merge layer , and we feed the mentions set of each entity pair of instances as found in the corpus to generate an analogy embedding as a vector of features . In detail , given a set of men - tions referring to an entity pair ( A , B ) as input , the pre - trained HSN generates an embedding r A , B ( see Figure 2 ) which represents the relation be - tween those two entities . Then , we concatenate these embeddings to the penultimate layer of a relation extraction model , PCNN - KI ( Glass and Gliozzo , 2018b ) , based on a convolutional neu - ral network , which is the state - of - the - art for this benchmark . The ﬁnal fully - connected layer uses the representation from HSN in combination with its own learned multi - instanced vector representa - tion to predict a conﬁdence score for each relation . During the training of this joint model , PCNN - KI + A NALOGY , we freeze our analogy embed - dings in order to avoid the loose the knowledge transfer capability . As for the one - shot setting described before , we use the same pre - trained the HSN on the T - REX and we used it as a feature extractor for entity pairs in both train - test standard splits of NYT - FB , as used in ( Zeng et al . , 2015 ; Lin et al . , 2016 ) . Fig - ure 4 reports the results of our evaluation . The model which uses the features generated by HSN largely improve the performances of PCNN - KI , despite the HSN is trained on a different corpus and using a different KB . In the same chart , we also report a compared evaluation for other ap - proaches proposed in the literature for the NYT - FB benchmark : PCNN + ATT ( Lin et al . , 2016 ) , CNN + ATT ( Zeng et al . , 2015 ) , MIML - RE ( Sur - deanu et al . , 2012 ) , H OFFMANN ( Hoffmann et al . , 2011 ) , M INTZ ( Mintz et al . , 2009 ) . We run the evaluation also on CC - DBP , a larger dataset for distantly supervised RE , using the same train - test setting adopted in ( Glass and Gliozzo , 2018b ) . As done for the NYT - FB dataset , we in - corporate the analogy embeddings generated by the same HSN trained on the T - REX . The results conﬁrm the improvements obtained by PCNN - KI model if it integrates our pre - trained embeddings ( Table 3 ) . AUC F1 PCNN - KI 0 . 437 0 . 468 PCNN - KI + A NALOGY 0 . 500 0 . 504 Table 3 : AUC and F1 results on CC - DBP . 6 Conclusion and Future Work In this paper , we proposed a novel approach to learn representations of relations in text . Align - ments between knowledge bases and textual cor - pora are used as ground truth in order to collect a set of analogies between entity pairs . We designed a hierarchical siamese network trained to recog - nize those analogies . The experiments showed the two main advantages of our approach . First , the model can generalize on new unseen rela - tion types , obtaining promising results in one - shot learning compared with the state - of - the - art sen - tence encoders . Second , the model can generate low - rank representations can help existing neural - based models designed for other tasks . As future work , we plan to continue our investigation by ex - tending the method with other ideas . For instance , the use of positional embeddings , as well as the use of placeholders replacing the entities in the textual mentions are promising future directions . Finally , we plan also to explore the use of anal - ogy embeddings in other tasks , such as question answering and knowledge base population . References Eugene Agichtein and Luis Gravano . 2000 . Snowball : extracting relations from large plain - text collections . In ACM DL . Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio . 2014 . Neural machine translation by jointly learning to align and translate . CoRR , abs / 1409 . 0473 . Antoine Bordes , Nicolas Usunier , Alberto Garc´ıa - Dur´an , Jason Weston , and Oksana Yakhnenko . 2013 . Translating embeddings for modeling multi - relational data . In NIPS . Jane Bromley , Isabelle Guyon , Yann LeCun , Eduard S¨ackinger , and Roopak Shah . 1993 . Signature ver - iﬁcation using a ”siamese” time delay neural net - work . In NIPS . Razvan C . Bunescu , Ruifang Ge , Rohit J . Kate , Ed - ward M . Marcotte , Raymond J . Mooney , Arun K . Ramani , and Yuk Wah Wong . 2005 . Comparative experiments on learning information extractors for proteins and their interactions . Artiﬁcial Intelligence in Medicine , 33 ( 2 ) : 139 – 155 . Alexis Conneau , Douwe Kiela , Holger Schwenk , Lo¨ıc Barrault , and Antoine Bordes . 2017 . Supervised learning of universal sentence representations from natural language inference data . In EMNLP . Hady ElSahar , Pavlos Vougiouklis , Arslen Remaci , Christophe Gravier , Jonathon S . Hare , Fr´ed´erique Laforest , and Elena Simperl . 2018 . T - rex : A large scale alignment of natural language with knowledge base triples . In LREC . Ji Feng and Zhi - Hua Zhou . 2017 . Deep MIML net - work . In AAAI . Anna Gladkova , Aleksandr Drozd , and Satoshi Mat - suoka . 2016 . Analogy - based detection of morpho - logical and semantic relations with word embed - dings : what works and what doesn’t . In SRW @ HLT - NAACL . Michael Glass and Alﬁo Gliozzo . 2018a . A dataset for web - scale knowledge base population . In ESWC . Michael Glass and Alﬁo Gliozzo . 2018b . Discovering implicit knowledge with unary relations . In ACL . Michael Glass , Alﬁo Gliozzo , Oktie Hassanzadeh , Nandana Mihindukulasooriya , and Gaetano Rossiello . 2018 . Inducing implicit relations from text using distantly supervised deep nets . In ISWC . Raia Hadsell , Sumit Chopra , and Yann LeCun . 2006 . Dimensionality reduction by learning an invariant mapping . In CVPR . Raphael Hoffmann , Congle Zhang , Xiao Ling , Luke Zettlemoyer , and Daniel S . Weld . 2011 . Knowledge - based weak supervision for information extraction of overlapping relations . In HLT . Maximilian Ilse , Jakub M . Tomczak , and Max Welling . 2018 . Attention - based deep multiple instance learn - ing . CoRR , abs / 1802 . 04712 . Ryan Kiros , Yukun Zhu , Ruslan Salakhutdinov , Richard S . Zemel , Raquel Urtasun , Antonio Tor - ralba , and Sanja Fidler . 2015 . Skip - thought vectors . In NIPS . Gregory Koch , Richard Zemel , and Ruslan Salakhut - dinov . 2015 . Siamese neural networks for one - shot image recognition . In ICML Deep Learning Work - shop . Ankit Kumar , Ozan Irsoy , Peter Ondruska , Mohit Iyyer , James Bradbury , Ishaan Gulrajani , Victor Zhong , Romain Paulus , and Richard Socher . 2016 . Ask me anything : Dynamic memory networks for natural language processing . In ICML . Quoc V . Le and Tomas Mikolov . 2014 . Distributed rep - resentations of sentences and documents . In ICML . Omer Levy , Minjoon Seo , Eunsol Choi , and Luke Zettlemoyer . 2017 . Zero - shot relation extraction via reading comprehension . In CoNLL . Yankai Lin , Shiqi Shen , Zhiyuan Liu , Huanbo Luan , and Maosong Sun . 2016 . Neural relation extraction with selective attention over instances . In ACL . Hanxiao Liu , Yuexin Wu , and Yiming Yang . 2017 . Analogical inference for multi - relational embed - dings . In ICML . Mausam , Michael Schmitz , Stephen Soderland , Robert Bart , and Oren Etzioni . 2012 . Open language learn - ing for information extraction . In EMNLP - CoNLL . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Cor - rado , and Jeff Dean . 2013 . Distributed representa - tions of words and phrases and their compositional - ity . In NIPS . Mike Mintz , Steven Bills , Rion Snow , and Dan Juraf - sky . 2009 . Distant supervision for relation extrac - tion without labeled data . In ACL . Jonas Mueller and Aditya Thyagarajan . 2016 . Siamese recurrent architectures for learning sentence similar - ity . In AAAI . Paul Neculoiu , Maarten Versteegh , and Mihai Rotaru . 2016 . Learning text similarity with siamese recur - rent networks . In Rep4NLP @ ACL . Jeffrey Pennington , Richard Socher , and Christo - pher D . Manning . 2014 . Glove : Global vectors for word representation . In EMNLP . Sebastian Riedel , Limin Yao , and Andrew McCallum . 2010 . Modeling relations and their mentions with - out labeled text . In ECML PKDD . Sebastian Riedel , Limin Yao , Andrew McCallum , and Benjamin M . Marlin . 2013 . Relation extraction with matrix factorization and universal schemas . In NAACL - HLT . Sainbayar Sukhbaatar , Arthur Szlam , Jason Weston , and Rob Fergus . 2015 . End - to - end memory net - works . In NIPS . Mihai Surdeanu , Julie Tibshirani , Ramesh Nallap - ati , and Christopher D . Manning . 2012 . Multi - instance multi - label learning for relation extraction . In EMNLP - CoNLL . Kristina Toutanova , Danqi Chen , Patrick Pantel , Hoi - fung Poon , Pallavi Choudhury , and Michael Gamon . 2015 . Representing text for joint embedding of text and knowledge bases . In EMNLP . Peter D . Turney . 2006 . Similarity of semantic relations . Computational Linguistics , 32 ( 3 ) . Patrick Verga and Andrew McCallum . 2016 . Row - less universal schema . In AKBC @ NAACL - HLT . Patrick Verga , Andrew McCallum , and Arvind Nee - lakantan . 2017 . Generalizing to unseen entities and entity pairs with row - less universal schema . In EACL . Oriol Vinyals , Charles Blundell , Tim Lillicrap , Koray Kavukcuoglu , and Daan Wierstra . 2016 . Matching networks for one shot learning . In NIPS . Ekaterina Vylomova , Laura Rimell , Trevor Cohn , and Timothy Baldwin . 2016 . Take and took , gaggle and goose , book and read : Evaluating the utility of vec - tor differences for lexical relation learning . In ACL . Xinggang Wang , Yongluan Yan , Peng Tang , Xiang Bai , and Wenyu Liu . 2018 . Revisiting multiple instance neural networks . Pattern Recognition , 74 . Zichao Yang , Diyi Yang , Chris Dyer , Xiaodong He , Alexander J . Smola , and Eduard H . Hovy . 2016 . Hi - erarchical attention networks for document classiﬁ - cation . In NAACL - HLT . Jianbo Yuan , Han Guo , Zhiwei Jin , Hongxia Jin , Xian - chao Zhang , and Jiebo Luo . 2017 . One - shot learn - ing for ﬁne - grained relation extraction via convolu - tional siamese neural network . In BigData . Daojian Zeng , Kang Liu , Yubo Chen , and Jun Zhao . 2015 . Distant supervision for relation extraction via piecewise convolutional neural networks . In EMNLP .