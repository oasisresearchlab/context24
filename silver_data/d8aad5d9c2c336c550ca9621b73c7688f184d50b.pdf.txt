Speeding Up the Xbox Recommender System Using a Euclidean Transformation for Inner - Product Spaces Yoram Bachrach Microsoft Research Yehuda Finkelstein Microsoft R & D Ran Gilad - Bachrach Microsoft Research Liran Katzir Computer Science , Technion Noam Koenigstein Microsoft R & D Nir Nice Microsoft R & D Ulrich Paquet Microsoft Research ABSTRACT A prominent approach in collaborative ﬁltering based rec - ommender systems is using dimensionality reduction ( ma - trix factorization ) techniques to map users and items into low - dimensional vectors . In such systems , a higher inner product between a user vector and an item vector indicates that the item better suits the user’s preference . Tradition - ally , retrieving the most suitable items is done by scoring and sorting all items . Real world online recommender systems must adhere to strict response - time constraints , so when the number of items is large , scoring all items is intractable . We propose a novel order preserving transformation , map - ping the maximum inner product search problem to Eu - clidean space nearest neighbor search problem . Utilizing this transformation , we study the eﬃciency of several ( approxi - mate ) nearest neighbor data structures . Our ﬁnal solution is based on a novel use of the PCA - Tree data structure in which results are augmented using paths one hamming dis - tance away from the query ( neighborhood boosting ) . The end result is a system which allows approximate matches ( items with relatively high inner product , but not necessar - ily the highest one ) . We evaluate our techniques on two large - scale recommendation datasets , Xbox Movies and Ya - hoo Music , and show that this technique allows trading oﬀ a slight degradation in the recommendation quality for a signiﬁcant improvement in the retrieval time . Categories and Subject Descriptors H . 5 [ Information systems ] : Information retrieval— retrieval models and ranking , retrieval tasks and goals Keywords Recommender systems , matrix factorization , inner product search , fast retrieval Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro ﬁ t or commercial advantageand that copies bear this notice and the full citation on the ﬁ rst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit ispermitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspeci ﬁ cpermission and / or a fee . Request permissions from permissions @ acm . org . RecSys’14 , October 6 – 10 , 2014 , Foster City , Silicon Valley , CA , USA . Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 2668 - 1 / 14 / 10 . . . $ 15 . 00 . http : / / dx . doi . org / 10 . 1145 / 2645710 . 2645741 . 1 . INTRODUCTION The massive growth in online services data gives rise to the need for better information ﬁltering techniques . In the context of recommender systems the data consists of ( 1 ) the item catalog ; ( 2 ) the users ; and ( 3 ) the user feedback ( ratings ) . The goal of a recommender system is to ﬁnd for every user a limited set of items that have the highest chance to be consumed . Modern recommender systems have two major parts . In the ﬁrst part , the learning phase , a model is learned ( oﬄine ) based on user feedback 1 . In the second part , the retrieval phase , recommendations are issued per user ( online ) . This paper studies the scalability of the retrieval phase ( the second part ) in massive recommender systems based on matrix factorization . Speciﬁcally , we introduce a new approach which oﬀers a trade - oﬀ between running time and the quality of the results presented to a user . Matrix Factorization ( MF ) is one of the most popular ap - proaches for collaborative ﬁltering . This method has re - peatedly demonstrated better accuracy than other methods such as nearest neighbor models and restricted Boltzmann machines [ 2 , 8 ] . In MF models , users and items are repre - sented by latent feature vectors . A Bayesian MF model is also at the heart of the Xbox recommendation system [ 16 ] which serves games , movies , and music recommendations to millions of users daily . In this system , users and items are represented by ( low - dimensional ) vectors in R 50 . The qual - ity of the match between a user u represented by the vector x u and the item i represented by the vector y i is given by the inner product x u · y i between these two vectors . A higher inner product implies a higher chance of the user consuming the item . The Retrieval Problem : Ideally , given a user u repre - sented by a vector x u , all the item vectors ( y 1 , . . . , y n ) are examined . For each such item vector y i , its match quality with the user ( x u · y i ) is computed , and the items sorted according to their match quality . The items with the high - est match quality in the list are then selected to form the ﬁnal list of recommendations . However , the catalog of items is often too large to allow an exhaustive computation of all the inner products within a limited allowed retrieval time . The Xbox catalog consists of millions of items of various kinds . If a linear scan is used , millions of inner product com - putations are required for each single recommendation . The 1 This phase cannot be done entirely oﬄine when a context is used to issue the recommended items . 257 user vectors can take into account contextual information 2 that is only available during user engagement . Hence , the complete user vector is computed online ( at runtime ) . As a result , the retrieval of the recommended items list can only be performed online , and cannot be pre - computed oﬄine . This task constitutes the single most computational inten - sive task imposed on the online servers . Thereby , having a fast alternative for this process is highly desirable . Our Contribution : This paper shows how to signiﬁ - cantly speed up the recommendation retrieval process . The optimal item - user match retrieval is relaxed to an approxi - mate search : retrieving items that have a high inner product with the user vector , but not necessarily the highest one . The approach combines several building blocks . First , we deﬁne a novel transformation from the inner product prob - lem to a Euclidean nearest neighbor problem ( Section 3 ) . As a pre - processing step , this transformation is applied to the item vectors . During item retrieval , another transformation is applied to the user vector . The item with the smallest Eu - clidean distance in the transformed space is then retrieved . To expedite the nearest neighbor search , the PCA - Tree [ 21 ] data structure is used together with a novel neighborhood boosting scheme ( Section 4 ) . To demonstrate the eﬀectiveness of the proposed approach , it is applied to an Xbox recommendations dataset and the publicly available Yahoo Music dataset [ 8 ] . Experiments show a trade - oﬀ curve of a slight degradation in the rec - ommendation quality for a signiﬁcant improvement in the retrieval time ( Section 5 ) . In addition , the achievable time - accuracy trade - oﬀs are compared with two baseline approaches , an implementation based on Locality Sensitive Hashing [ 1 ] and the current state of the art method for approximate rec - ommendation in matrix - factorization based CF systems [ 13 ] . We show that for a given required recommendation quality ( accuracy in picking the optimal items ) , our approach allows achieving a much higher speedup than these alternatives . Notation : We use lower - case fonts for scalars , bold lower - case fonts for vectors , and bold upper - case fonts for matrices . For example , x is a scalar , x is a vector , and X is a matrix . Given a vector x ∈ R d , let x i be the measure in dimension i , with ( x 1 , x 2 , . . . , x d ) T ∈ R d . The norm is denoted by (cid:3)·(cid:3) ; in Euclidean space (cid:3) x (cid:3) = (cid:2)(cid:3) di = 1 x 2 i . We denote by x · y a dot product ( inner product ) between x and y . Finally , we use (cid:4) a , x T (cid:5) T to denote a concatenation of a scalar a with a vector x . 2 . BACKGROUND AND RELATED WORK In this section we will explain the problem of ﬁnding best recommendations in MF models and review possible ap - proaches for eﬃcient retrieval of recommendations . 2 . 1 Matrix Factorization Based Recommender Systems In MF models , each user u is associated with a user - traits vector x u ∈ R d , and each item i with an item - traits vector y i ∈ R d . The predicted rating of a user u to an item i is denoted by ˆ r ui and obtained using the rule : ˆ r ui = μ + b u + b i + x u · y i , ( 1 ) 2 The contextual information may include the time of day , recent search queries , etc . where μ is the overall mean rating value and b i and b u repre - sent the item and user biases respectively . The above model is a simple baseline model similar to [ 14 ] . It can be readily extended to form the core of a variety of more complex MF models , and adapted to diﬀerent kinds of user feedback . While μ and b i are important components of the model , they do not eﬀect the ranking of items for any given user , and the rule ˜ r ui = b i + x u · y i will produce the same set of recommendations as that of Equation 1 . We can also concatenate the item bias b i to the user vector and reduce our prediction rule to a simple dot product : ˜ r ui = ¯ x u · ¯ y i , where ¯ x u (cid:2) ( 1 , x Tu ) T , and ¯ y i (cid:2) ( b i , y Ti ) T . Hence , computing recommendations in MF models amounts to a simple search in an inner product space : given a user vector ¯ x u , we wish to ﬁnd items with vectors ¯ y i that will maximize the inner product ¯ x u · ¯ y i . For the sake of readability , from this point onward we will drop the bar and refer to ¯ x u and ¯ y u as x u and y i . We therefore focus on the problem of ﬁnding maximal inner product matches as described above . 2 . 2 Retrieval of Recommendations in Inner - Product Spaces The problem of eﬃcient retreival of recommendations in MF models is relatively new , but it has been discussed in the past [ 10 , 11 , 13 ] . In real - world large scale systems such as the Xbox Recommender , this is a concrete problem , and we identiﬁed it as the main bottleneck that drains our online resources . Previous studies can be categorized into two basic ap - proaches . The ﬁrst approach is to propose new recommen - dation algorithms in which the prediction rule is not based on inner - product matches . This was the approach taken by Khoshneshin et al . [ 10 ] , who were ﬁrst to raise the problem of eﬃcient retrieval of recommendations in MF models . In [ 10 ] a new model is proposed in which users and items are em - bedded based on their Euclidean similarity rather than their inner - product . In a Euclidean space , the plethora of algo - rithms for nearest - neighbor search can be utilized for an eﬃ - cient retrieval of recommendations . A similar approach was taken by [ 11 ] where an item - oriented model was designed to alleviate retrieval of recommendations by embedding items in a Euclidean space . While these methods show signiﬁcant improvements in retrieval times , they deviate from the well familiar MF framework . These approaches which are based on new algorithms do not beneﬁt the core of existing MF based recommender systems in which the retrieval of rec - ommendations is still based on inner - products . The second approach to this problem is based on designing new algorithms to mitigate maximal inner - product search . These algorithms can be used in any existing MF based sys - tem and require only to implement a new data structure on top of the recommender to assist at the retrieval phase . For example , in [ 13 ] a new IP - Tree data structure was pro - posed that enables a branch - and - bound search scheme in inner - product spaces . In order to reach higher speedup val - ues , the IP - Tree was combined with spherical user clustering that allows to pre - compute and cache recommendations to similar users . However , this approach requires prior knowl - edge of all the user vectors which is not available in systems such as the Xbox recommender where ad - hoc contextual in - formation is used to update the user vectors . This work was later continued in [ 18 ] for the general problem of maximal inner - product search , but these extensions showed eﬀective - 258 ness in high - dimensional sparse datasets which is not the case for vectors generated by a MF process . This paper builds upon a novel transformation that re - duces the maximal inner - product problem to simple nearest neighbor search in a Euclidean space . On one hand the pro - posed approach can be employed by any classical MF model , and on the other hand it enables using any of the existing algorithms for Euclidean spaces . Next , we review several alternatives for solving the problem in a Euclidean Space . 2 . 2 . 1 Nearest Neighbor in Euclidean Spaces Locality Sensitive Hashing ( LSH ) was recently popular - ized as an eﬀective approximate retrieval algorithm . LSH was introduced by Broder et al . to ﬁnd documents with high Jaccard similarity [ 4 ] . It was later extended to other metrics including the Euclidean distance [ 9 ] , cosine similarity [ 5 ] , and earth mover distance [ 5 ] . A diﬀerent approach is based on space partitioning trees : KD - trees [ 3 ] is a data structure that partitions R d into hyper - rectangular ( axis parallel ) cells . In construction time , nodes are split along one coordinate . At query time , one can search of all points in a rectangular box and nearest neighbors eﬃ - ciently . Several augmented splits are used to improve the query time . For example , ( 1 ) Principal component axes trees ( PCA - Trees ) transform the original coordinates to the principal components [ 21 ] ; ( 2 ) Principal Axis Trees ( PAC - Trees ) [ 15 ] use a principal component axis at every node ; ( 3 ) Random Projection Trees ( RPT ) use a random axis at each node [ 6 ] ; and ( 4 ) Maximum Margin Trees ( MMT ) use a maximum margin axis at every node [ 20 ] . A theoretical and empirical comparison for some variants can be found [ 19 ] . Our approach makes use of PCA - trees and combines it with a novel neighborhood boosting scheme . In Section 5 we compare to alternatives such as LSH , KD - Trees , and PAC - Trees . We do not compare against MMT and RPT as we don’t see their advantage over the other methods for the particular problem at hand . 3 . REDUCIBLE SEARCH PROBLEMS A key contribution of this work is focused on the concept of eﬃcient reductions between search problems . In this sec - tion we formalize the concept of a search problem and show eﬃcient reductions between known variants . We deﬁne a search problem as : Definition 1 . A search problem S ( I , Q , s ) consists of an instance set of n items I = { i 1 , i 2 , . . . , i n } ∈ I , a query q ∈ Q , and a search function s : I × Q → { 1 , 2 , . . . , n } . Function s retrieves the index of an item in I for a given query q . The goal is to pre - process the items with g : I → I (cid:2) such that each query is answered eﬃciently . The pre - processing g can involve a transformation from one domain to another , so that a transformed search problem can oper - ate on a diﬀerent domain . The following deﬁnition formal - izes the reduction concept between search problems : Definition 2 . A search problem S 1 ( I , Q , s 1 ) is reducible to a search problem S 2 ( I (cid:2) , Q (cid:2) , s 2 ) , denoted by S 1 ≤ S 2 , if there exist functions g : I → I (cid:2) and h : Q → Q (cid:2) such that j = s 1 ( I , q ) if and only if j = s 2 ( g ( I ) , h ( q ) ) . This reduction does not apply any constraints on the run - ning time of g and h . Note that g runs only once as a pre - processing step , while h is applied at the query time . This yields a requirement that h has a O ( 1 ) running time . We formalize this with the following notation : Definition 3 . We say that S 1 ≤ O ( f ( n ) ) S 2 if S 1 ≤ S 2 and the running time of g and h are O ( f ( n ) ) and O ( 1 ) re - spectively . For a query vector in R d , we consider three search prob - lems in this paper : MIP , the maximum inner product from n vectors in R d ( MIP n , d ) ; NN , the nearest neighbor from n vectors in R d ( NN n , d ) ; MCS , the maximum cosine sim - ilarity from n vectors in R d ( MCS n , d ) . They are formally deﬁned as follows : Instance : A matrix of n vectors Y = [ y 1 , y 2 , . . . , y n ] such that y i ∈ R d ; therefore I = R d × n . Query : A vector x ∈ R d ; hence Q = R d . Objective : Retrieve an index according to s ( Y , x ) = argmax i x · y i MIP n , d s ( Y , x ) = argmin i (cid:3) x − y i (cid:3) NN n , d s ( Y , x ) = argmax i x · y i (cid:3) x (cid:3) (cid:3) y i (cid:3) MCS n , d , where i indicates column i of Y . The following section shows how transformations between these three problems can be achieved with MCS n , d ≤ O ( n ) MIP n , d ≤ O ( n ) NN n , d + 1 and NN n , d ≤ O ( n ) MCS n , d + 1 ≤ O ( n ) MIP n , d + 1 . 3 . 1 Order Preserving Transformations The triangle inequality does not hold between vectors x , y i , and y j when an inner product compares them , as is the case in MIP . Many eﬃcient search data structures rely on the triangle inequality , and if MIP can be transformed to NN with its Euclidian distance , these data structures would immediately become applicable . Our ﬁrst theorem states that MIP can be reduced to NN by having an Euclidian metric in one more dimension than the original problem . Theorem 1 . MIP n , d ≤ O ( n ) NN n , d + 1 Proof : Let φ (cid:2) max i (cid:3) y i (cid:3) and preprocess input with : ˜ y i = g ( y i ) = (cid:6)(cid:2) φ 2 − (cid:3) y i (cid:3) 2 , y Ti (cid:7) T . During query time : ˜ x = h ( x ) = (cid:4) 0 , x T (cid:5) T . As (cid:3) ˜ x (cid:3) 2 = (cid:3) x (cid:3) 2 (cid:3) ˜ y i (cid:3) 2 = φ 2 − (cid:3) y i (cid:3) 2 + (cid:3) y i (cid:3) 2 = φ 2 ˜ x · ˜ y i = (cid:2) φ 2 − (cid:3) x i (cid:3) 2 · 0 + x · y i = x · y i we have (cid:3) ˜ x − ˜ y i (cid:3) 2 = (cid:3) ˜ x (cid:3) 2 + (cid:3) ˜ y (cid:3) 2 − 2˜ x · ˜ y i = (cid:3) x (cid:3) 2 + φ 2 − 2 x · y i . Finally , as φ and x are independent of index i , j = argmin i (cid:3) ˜ x − ˜ y i (cid:3) 2 = argmax i x · y i . 259 Theorem 1 provides the main workhorse for our proposed approach ( Section 4 ) . In the remaining of this section , we present its properties as well the related transformations . If it is known that the transformed ˜ Y = [ ˜ y 1 , ˜ y 2 , . . . , ˜ y n ] is in a manifold , as given above , we might expect to recover Y by reducing back with NN n , d ≤ O ( n ) MIP n , d − 1 . However , in the general case the transformation is only possible by increasing the dimensionality by one again : Theorem 2 . NN n , d ≤ O ( n ) MIP n , d + 1 Proof : The preprocessing of the input : ˜ y i = g ( y i ) = (cid:4) (cid:3) y i (cid:3) 2 , y Ti (cid:5) T . During query time : ˜ x = h ( x ) = (cid:4) 1 , − 2 x T (cid:5) T . We have ˜ x · ˜ y i = (cid:3) y i (cid:3) 2 − 2 x · y i . Finally , j = argmax i ˜ x · ˜ y i = argmin i (cid:3) x (cid:3) 2 + (cid:3) y i (cid:3) 2 − 2 x · y i = argmin i (cid:3) x − y i (cid:3) 2 . MIP search can also be embedded in a MCS search by increasing the dimensionality by one : Theorem 3 . MIP n , d ≤ O ( n ) MCS n , d + 1 Proof : Preprocessing and query transformation are iden - tical to Theorem 1 . The preprocessing of the input : φ (cid:2) max i (cid:3) y i (cid:3) and let ˜ y i = g ( y i ) = (cid:6)(cid:2) φ 2 − (cid:3) y i (cid:3) 2 , y Ti (cid:7) T . Dur - ing query time : ˜ x = h ( x ) = (cid:4) 0 , x T (cid:5) T . Finally , j = argmax i ˜ x · ˜ y i (cid:3) ˜ x (cid:3) (cid:3) ˜ y i (cid:3) = argmax i x · y i (cid:3) x (cid:3) φ = argmax i x · y i . However , MCS is simply MIP searching over normalized vectors : Theorem 4 . MCS n , d ≤ O ( n ) MIP n , d Proof : The preprocessing of the input : ˜ y i = g ( y ) = y i (cid:3) y i (cid:3) . During query time : ˜ x = h ( x ) = x . Finally , j = argmax i ˜ x · ˜ y i = argmax i x · y i (cid:3) x (cid:3) (cid:3) y i (cid:3) . Our ﬁnal result states that a NN search can be transformed to a MCS search by increasing the dimensionality by one : Theorem 5 . NN n , d ≤ O ( n ) MCS n , d + 1 Proof : Same reduction as in Theorem 1 . The prepro - cessing of the input : φ (cid:2) max i (cid:3) y i (cid:3) and ˜ y i = g ( y i ) = (cid:6)(cid:2) φ 2 − (cid:3) y i (cid:3) 2 , y Ti (cid:7) T . During query time : ˜ x = h ( x ) = (cid:4) 0 , x T (cid:5) T . Thus by Theorem 1 , j = argmax i ˜ x · ˜ y i (cid:3) ˜ x (cid:3) (cid:3) ˜ y i (cid:3) = argmax i x · y i (cid:3) x (cid:3) φ = argmax i x · y i = argmin i (cid:3) ˜ x − ˜ y i (cid:3) 2 . Next , we utilize Theorem 1 for speeding up retrieval of rec - ommendations in Xbox and other MF based recommender systems . Algorithm 1 TransformAndIndex ( Y , d (cid:2) ) input : item vectors Y , depth d (cid:2) ≤ d + 1 output : tree t compute φ , µ , W S = ∅ for i = 1 : n do ˜ y i = g ( y i ) ; S = S ∪ ˜ y i end for return T ← PCA - Tree ( S , d (cid:2) ) 4 . AN OVERVIEW OF OUR APPROACH Our solution is based on two components , a reduction to a Euclidian search problem , and a PCA - Tree to address it . The reduction is very similar to that deﬁned in Theorem 1 , but composed with an additional shift and rotation , so that the MIP search problem is reduced to NN search , with all vectors aligned to their principal components . 4 . 1 Reduction We begin with deﬁning the ﬁrst reduction function follow - ing Theorem 1 . Let φ (cid:2) max i (cid:3) y i (cid:3) , and y (cid:2)i = g 1 ( y i ) = (cid:6)(cid:2) φ 2 − (cid:3) y i (cid:3) 2 , y Ti (cid:7) T x (cid:2) = h 1 ( x ) = (cid:8) 0 , x T (cid:9) T , ( 2 ) which , when applied to Y , gives elements y (cid:2)i ∈ R d + 1 . This reduces MIP to NN . As NN is invariant to shifts and ro - tations in the input space , we can compose the transforma - tions with PCA rotation and still keep an equivalent search problem . We mean - center and rotate the data : Let µ = 1 n (cid:3) i y (cid:2)i be the mean after the ﬁrst reduction , and M ∈ R d + 1 × n a matrix with µ replicated along its columns . The SVD of the centered data matrix is ( Y (cid:2) − M ) = WΣU T , where data items appear in the columns of Y (cid:2) . Matrix W is a ( d + 1 ) by ( d + 1 ) matrix . Each of the columns of W = [ w 1 , . . . , w d + 1 ] deﬁnes an orthogonal unit - length eigenvector , so that each w j deﬁnes a hyperplane onto which each y (cid:2)i − µ is projected . Matrix W is a r otation matrix that aligns the vectors to their principal components . 3 We deﬁne the centered rotation as our second transformation , ˜ y i = g 2 ( y (cid:2)i ) = W T ( y (cid:2)i − µ ) ˜ x = h 2 ( x (cid:2) ) = W T ( x (cid:2) − µ ) . ( 3 ) The composition g ( y i ) = g 2 ( g 1 ( y i ) ) , h ( x ) = h 2 ( h 1 ( x ) ) ( 4 ) still deﬁnes a reduction from MIP to NN . Using ˜ y i = g ( y i ) , gives us a transformed set of input vectors ˜ Y , over which an Euclidian search can be performed . Moreover , after this transformation , the points are rotated so that their compo - nents are in decreasing order of variance . Next , we index the transformed item vectors in ˜ Y using a PCA - Tree data structure . We summarize the above logic in Algorithm 1 . 3 Notice that Σ is not included , as the Euclidian metric is invariant under rotations of the space , but not shears . 260 Algorithm 2 PCA - Tree ( S , δ ) input : item vectors set S , depth δ output : tree t if δ = 0 then return new leaf with S end if j = d + 1 − δ / / principal component at depth δ m = median ( { ˜ y ij for all ˜ y i ∈ S } ) S ≤ = { ˜ y i ∈ S where ˜ y ij ≤ m } S > = { ˜ y i ∈ S where ˜ y ij > m } t . leftChild = PCA - Tree ( S ≤ , δ − 1 ) t . rightChild = PCA - Tree ( S > , δ − 1 ) return t 4 . 2 Fast Retrieve with PCA - Trees Building the PCA - Tree follows from a the KD - Tree con - struction algorithm on ˜ Y . Since the axes are aligned with the d + 1 principal components of Y (cid:2) , we can make use of a KD - tree constriction process to get a PCA - Tree data struc - ture . The top d (cid:2) ≤ d + 1 principal components are used , and each item vector is assigned to its representative leaf . Algorithm 2 deﬁnes this tree construction procedure . At the retrieval time , the transformed user vector ˜ x = h ( x ) is used to traverse the tree to the appropriate leaf . The leaf contains the item vectors in the neighborhood of ˜ x , hence vectors that are on the same side of all the splitting hyperplanes ( the top principal components ) . The items in this leaf form an initial candidates set from which the top items or nearest neighbors are selected using a direct ranking by distance . The number of items in each leaf decays exponentially in the depth d (cid:2) of the tree . By increasing the depth we are left with less candidates hence trading better speedup values with lower accuracy . The process allows achieving diﬀer - ent trade - oﬀs between the quality of the recommendations and an allotted running time : with a larger d (cid:2) , a smaller proportion of candidates are examined , resulting in a larger speedup , but also a reduced accuracy . Our empirical analy - sis ( Section 5 ) examines the trade - oﬀs we can achieve using our PCA - trees , and contrasts this with trade - oﬀs achievable using other methods . 4 . 2 . 1 Boosting Candidates With Hamming Distance Neighborhoods While the initial candidates set includes many nearby items , it is possible that some of the optimal top K vec - tors are indexed in other leafs and most likely the adjacent leafs . In our approach we propose boosting the candidates set with the item vectors in leafs that are on the “wrong” side in at most one of the median - shifted PCA hyperplane compared to ˜ x . These vectors are likely to have a small Euclidean distance from the user vector . Our PCA - Tree is a complete binary tree of height d (cid:2) , where each leaf corresponds to a binary vector of length d (cid:2) . We supplement the initial candidates set from the leaf of the user vector , with all the candidates of leafs with a Hamming distance of ‘1’ , and hence examine candidates from d (cid:2) of the 2 d (cid:2) leafs . In Section 5 . 1 . 1 we show that this approach is in - strumental in achieving the best balance between speedup and accuracy . 5 . EMPIRICAL ANALYSIS OF SPEEDUP - ACCURACY TRADEOFFS We use two large scale datasets to evaluate the speedup achieved by several methods : 1 . Xbox Movies [ 12 ] – This dataset is a Microsoft pro - priety dataset consisting of 100 million binary { 0 , 1 } ratings of more than 15K movies by 5 . 8 million users . We applied the method used in [ 12 ] to generate the vectors representing items and users . 2 . Yahoo ! Music [ 8 ] – This is a publicly available ratings dataset consisting of 252 , 800 , 275 ratings of 624 , 961 items by 1 , 000 , 990 users . The ratings are on a scale of 0 - 100 . The users and items vectors were generated by the algorithm in [ 7 ] . From both datasets we created a set of item vectors and user vectors of dimensionality d = 50 . The following evaluations are based on these vectors . Speedup Measurements and Baselines . We quantify the improvement of an algorithm A over an - other ( naive ) algorithm A 0 by the following term : Speedup A 0 ( A ) = Time taken by Algorithm A 0 Time taken by Algorithm A . ( 5 ) In all of our evaluations we measure the speedup with re - spect to the same algorithm : a naive search algorithm that iterates over all items to ﬁnd the best recommendations for every user ( i . e . computes the inner product between the user vector and each of the item vectors , keeping track of the item with the highest inner product found so far ) . Thus denoting by T naive the time taken by the naive algorithm we have : T naive = Θ ( # users × # items × d ) . The state of the art method for ﬁnding approximately optimal recommendations uses a combination of IP - Trees and user cones [ 13 ] . In the following evaluation we dubbed this method IP - Tree . The IP - Tree approach assumes all the user vectors ( queries ) are computed in advance and can be clustered into a structure of user cones . In many real - world systems like the Xbox recommender the user vectors are computed or updated online , so this approach cannot be used . In contrast , our method does not require having all the user vectors in advance , and is thus applicable in these settings . The IP - Tree method relies on an adaptation of the branch - and - bound search in metric - trees [ 17 ] to handle nearest neigh - bor search in inner - product spaces . However , the construc - tion of the underlaying metric - tree data structure , which is a space partitioning tree , is not adapted to inner - product spaces ( it partitions vectors according to Euclidean proxim - ity ) . By using the Euclidean transformation of Theorem 1 , we can utilize the data structures and algorithms designed for Euclidean spaces in their original form , without adapta - tions that may curb their eﬀectiveness . Next , we show that our approach achieves a superior computation speedup , despite having no access to any prior knowledge about the user vectors or their distribution . 4 4 We focus on online processing time , i . e . the time to choose an item to recommend for a target user . We ignore the computation time required by oﬄine preprocessing steps . 261 Theorem 1 allows using various approximate nearest - neighbor algorithms for Euclidean spaces , whose performance depends on the speciﬁc dataset used . We propose using PCA - Trees as explained in Section 4 . 2 , and show that they have an excellent performance for both the Xbox movies and Ya - hoo ! music datasets , consisting of low dimensionality dense vectors obtained by matrix factorization . A diﬀerent and arguably more popular approach for ﬁnding approximate - nearest - neighbors in Euclidean spaces is Locality - Sensitive Hashing ( LSH ) [ 1 ] . In the evaluations below we also in - clude a comparison against LSH . We emphasize that using both our PCA - Trees approach and LSH techniques is only enabled by our Euclidean transformation ( Theorem 1 ) . Our approximate retrieval algorithms introduce a trade - oﬀ between accuracy and speedup . We use two measures to quantify the quality of the top K recommendations . The ﬁrst measure Precision @ K denotes how similar the approxi - mate recommendations are to the optimal top K recommen - dations ( as retrieved by the naive approach ) : Precision @ K (cid:2) | L rec ∩ L opt | K , ( 6 ) where L rec and L opt are the lists of the top K approximate and the top K optimal recommendations respectively . Our evaluation metrics only consider the items at the top of the approximate and optimal lists . 5 A high value for Precision implies that the approximate recommendations are very similar to the optimal recommen - dations . In many practical applications ( especially for large item catalogs ) , it is possible to have low Precision rates but still recommend very relevant items ( with a high inner prod - uct between the user and item vectors ) . This motivates our second measure RMSE @ K which examines the preference to the approximate items compared to the optimal items : RMSE @ K (cid:2) (cid:10)(cid:11)(cid:11)(cid:12) 1 K K (cid:13) k = 1 (cid:8) L rec ( k ) − L opt ( k ) (cid:9) 2 , ( 7 ) where L rec ( k ) and L opt ( k ) are the scores ( predicted ratings ) of the k ’th recommended item in the approximated list and the optimal list respectively . Namely , L rec ( k ) and L opt ( k ) are the values of inner products between the user vector and k ’th recommended item vector and optimal item vector respectively . Note that the amount (cid:8) L rec ( k ) − L opt ( k ) (cid:9) is always positive as the items in each list are ranked by their scores . 5 . 1 Results Our initial evaluation considers three approximation algo - rithms : IP - Tree , LSH , and our approach ( Section 4 . 2 ) . Fig - ure 1 ( a ) depicts Precision @ 10 for the Xbox Movies dataset ( higher values indicate better performance ) . The Precision values are plotted against the average speedup values they enable . At very low speedup values the LSH algorithm shows the best trade - oﬀ between precision and speedup , but when higher speedup values are considered the LSH performance drops signiﬁcantly and becomes worst . One possible rea - son for this is that our Euclidean transformation results in transformed vectors with one dimension being very large compared with the other dimensions , which is a diﬃcult 5 Note that for this evaluation the recall is completely deter - mined by the precision . Method Enabled by Prior Neighborhood Theorem 1 knowledge boosting IP - Tree no user vectors not allowed KD - Tree yes none allowed PCA - Tree yes none allowed PAC - Tree yes none not allowed Table 1 : A summary of the diﬀerent tree ap - proaches . IP - Tree is the baseline from [ 13 ] , which requires prior knowledge of the users vectors . All other approaches ( as well as LSH ) were not feasible before Theorem 1 was introduced in this paper . input distribution for LSH approaches . 6 In contrast , the tree - based approaches ( IP - Tree and our approach ) show a similar behavior of a slow and steady decrease in Precision values as the speedup increases . The speedup values of our approach oﬀers a better precision - vs - speedup tradeoﬀ than the IP - tree approach , though their precision is almost the same for high speedup values . Figure 1 ( b ) depicts the RMSE @ 10 ( lower values indicate better performance ) vs . speedup for the three approaches . The trend shows signiﬁcantly superior results for our PCA - Tree approach , for all speedup values . Similarly to Fig - ure 1 ( a ) , we see a sharp degradation of the LSH approach as the speedup increases , while the tree - based approaches show a trend of a slow increase in RMSE values as the speedup in - creases . We note that even for high speed - up values , which yield low precision rates in Figure 1 ( a ) , the RMSE values remain very low , indicating that very high quality of rec - ommendations can be achieved at a fraction of the compu - tational costs of the naive algorithm . In other words , the recommended items are still very relevant to the user , al - though the list of recommended items is quite diﬀerent from the optimal list of items . Figure 2 depicts Precision @ 10 and RMSE @ 10 for the Ya - hoo ! Music dataset . The general trends of all three algo - rithms seem to agree with those of Figure 1 : LSH starts bet - ter but deteriorates quickly , and the tree - based approaches have similar trends . The scale of the RMSE errors in Fig - ure 1 ( b ) is diﬀerent ( larger ) because the predicted scores are in the range of 0 - 100 , whereas in the Xbox Movies dataset the predictions are binary . The empirical analysis on both the Xbox and Yahoo ! datasets shows that it is possible to achieve excellent recommenda - tions for very low computational costs by employing our Eu - clidean transformation and using an approximate Euclidean nearest neighbor method . The results indicate that tree - based approaches are superior to an LSH based approach ( except when the required speedup is very small ) . Further , the results indicate that our method yields higher quality recommendations than the IP - trees approach [ 13 ] . Note that we also compared Precision @ K and RMSE @ K for other K values . While the ﬁgures are not included in this paper , the trends are all similar to those presented above . 5 . 1 . 1 Comparing Different Tree Approaches A key building block in our approach is aligning the item vectors with their principal components ( Equation 3 ) and using PCA - Trees rather than KD - Trees . Another essential 6 The larger dimension is the auxiliary dimension ( (cid:2) φ 2 − (cid:3) y i (cid:3) 2 ) in Equation 2 . 262 10 20 30 40 50 60 70 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 Speedup P r e c i s i on @ 10 IP − Tree LSHThis Paper ( a ) Precision vs . Speedup 10 20 30 40 50 60 70 0 0 . 05 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 . 35 0 . 4 Speedup R M SE @ 10 IP Tree LSHThis Paper ( b ) RMSE vs . Speedup Figure 1 : Performance against speedup values for the Xbox Movies dataset top 10 recommendations 10 20 30 40 50 60 70 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 Speedup P r e c i s i on @ 10 IP − Tree LSH This Paper ( a ) Precision vs . Speedup 10 20 30 40 50 60 70 0 5 10 15 20 25 30 Speedup R M SE @ 10 IP Tree LSH This Paper ( b ) RMSE vs . Speedup Figure 2 : Performance against speedup values for the Yahoo ! Music dataset top 10 recommendations ingredient in our approach is the neighborhood boosting of Section 4 . 2 . 1 . One may question the vitality of PCA - Trees or the neighborhood boosting to our overall solution . We there - fore present a detailed comparison of the diﬀerent tree based approaches . For the sake of completeness , we also included a comparison to PAC - Trees [ 15 ] . Table 1 summarizes the diﬀerent data structures . Except the IP - Tree approach , all of these approaches were not feasible before Theorem 1 was introduced in this paper . Note that neighborhood boosting is possible only when the tree splits are all based on a single consistent axis system . It is therefore prohibited in IP - Tees and PAC - Trees where the splitting hyperplanes are ad - hoc on every node . We compare the approach proposed in this paper with simple KD - Trees , PAC - Trees , and with PCA - Trees without neighborhood boosting ( our approach without neighborhood boosting ) . Figure 3 depicts Precision @ 10 and RMSE @ 10 on the Yahoo ! Music dataset . As the speedup levels increase , we notice an evident advantage in favor of PCA aligned trees over KD - Trees . When comparing PCA - Trees with - out neighborhood boosting to PAC - Trees we see a mixed picture : For low speedup values PCA - Trees perform better , but for higher speedup values we notice an eminent advan - tage in favor of PAC - Trees . To conclude , we note the overall advantage for the method proposed in this paper over any of the other tree based alternatives both in terms of Precision and RMSE . 6 . CONCLUSIONS We presented a novel transformation mapping a maximal inner product search to Euclidean nearest neighbor search , and showed how it can be used to speed - up the recommenda - tion process in a matrix factorization based recommenders such as the Xbox recommender system . We proposed a method for approximately solving the Eu - clidean nearest neighbor problem using PCA - Trees , and em - pirically evaluated it on the Xbox Movie recommendations and the Yahoo Music datasets . Our analysis shows that our approach allows achieving excellent quality recommen - dations at a fraction of the computational cost of a naive ap - proach , and that it achieves superior quality - speedup trade - oﬀs compared with state - of - the - art methods . 263 10 20 30 40 50 60 70 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 Speedup P r e c i s i on @ 10 KD − Tree PAC − Tree No neighborhood boosting This Paper ( a ) Precision vs . Speedup 10 20 30 40 50 60 70 0 5 10 15 20 25 30 Speedup R M SE @ 10 KD − Tree PAC − Tree No neighborhood boosting This Paper ( b ) RMSE vs . Speedup Figure 3 : Comparing tree based methods for the Yahoo ! Music dataset top 10 recommendations 7 . REFERENCES [ 1 ] Alexandr Andoni and Piotr Indyk . Near - optimal hashing algorithms for approximate nearest neighbor in high dimensions . In FOCS , pages 459 – 468 , 2006 . [ 2 ] Robert M . Bell and Yehuda Koren . Lessons from the netﬂix prize challenge . SIGKDD Explor . Newsl . , 2007 . [ 3 ] Jon Louis Bentley . Multidimensional binary search trees used for associative searching . Commun . ACM , 18 ( 9 ) : 509 – 517 , September 1975 . [ 4 ] Andrei Broder . On the resemblance and containment of documents . In Proceedings of the Compression and Complexity of Sequences 1997 , pages 21 – 29 , 1997 . [ 5 ] Moses S . Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of the Thiry - fourth Annual ACM Symposium on Theory of Computing , pages 380 – 388 , 2002 . [ 6 ] Sanjoy Dasgupta and Yoav Freund . Random projection trees and low dimensional manifolds . In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing , pages 537 – 546 , 2008 . [ 7 ] Gideon Dror , Noam Koenigstein , and Yehuda Koren . Yahoo ! music recommendations : Modeling music ratings with temporal dynamics and item taxonomy . In Proc . 5th ACM Conference on Recommender Systems , 2011 . [ 8 ] Gideon Dror , Noam Koenigstein , Yehuda Koren , and Markus Weimer . The Yahoo ! music dataset and KDD - Cup’11 . Journal Of Machine Learning Research , 17 : 1 – 12 , 2011 . [ 9 ] Piotr Indyk and Rajeev Motwani . Approximate nearest neighbors : Towards removing the curse of dimensionality . In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing , pages 604 – 613 , 1998 . [ 10 ] Mohammad Khoshneshin and W . Nick Street . Collaborative ﬁltering via euclidean embedding . In Proceedings of the fourth ACM conference on Recommender systems , 2010 . [ 11 ] Noam Koenigstein and Yehuda Koren . Towards scalable and accurate item - oriented recommendations . In Proc . 7th ACM Conference on Recommender Systems , 2013 . [ 12 ] Noam Koenigstein and Ulrich Paquet . Xbox movies recommendations : Variational Bayes matrix factorization with embedded feature selection . In Proc . 7th ACM Conference on Recommender Systems , 2013 . [ 13 ] Noam Koenigstein , Parikshit Ram , and Yuval Shavitt . Eﬃcient retrieval of recommendations in a matrix factorization framework . In CIKM , 2012 . [ 14 ] Yehuda Koren , Robert M . Bell , and Chris Volinsky . Matrix factorization techniques for recommender systems . IEEE Computer , 2009 . [ 15 ] James McNames . A fast nearest - neighbor algorithm based on a principal axis search tree . IEEE Trans . Pattern Anal . Mach . Intell . , 23 ( 9 ) : 964 – 976 , September 2001 . [ 16 ] Ulrich Paquet and Noam Koenigstein . One - class collaborative ﬁltering with random graphs . In Proceedings of the 22nd international conference on World Wide Web , WWW ’13 , pages 999 – 1008 , 2013 . [ 17 ] Franco P . Preparata and Michael I . Shamos . Computational Geometry : An Introduction . Springer , 1985 . [ 18 ] Parikshit Ram and Alexander Gray . Maximum inner - product search using cone trees . In SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2012 . [ 19 ] Parikshit Ram and Alexander G . Gray . Which space partitioning tree to use for search ? In Christopher J . C . Burges , L´eon Bottou , Zoubin Ghahramani , and Kilian Q . Weinberger , editors , NIPS , pages 656 – 664 , 2013 . [ 20 ] Parikshit Ram , Dongryeol Lee , and Alexander G . Gray . Nearest - neighbor search on a time budget via max - margin trees . In SDM , pages 1011 – 1022 . SIAM / Omnipress , 2012 . [ 21 ] Robert F . Sproull . Reﬁnements to nearest - neighbor searching in k - dimensional trees . Algorithmica , 6 ( 4 ) : 579 – 589 , 1991 . 264