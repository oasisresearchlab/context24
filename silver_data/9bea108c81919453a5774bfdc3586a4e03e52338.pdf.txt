Multimodel Inference Understanding AIC and BIC in Model Selection KENNETH P . BURNHAM DAVID R . ANDERSON Colorado Cooperative Fish and Wildlife Research Unit ( USGS - BRD ) The model selection literature has been generally poor at reﬂecting the deep foundations of the Akaike information criterion ( AIC ) and at making appropriate comparisons to the Bayesian information criterion ( BIC ) . There is a clear philosophy , a sound criterion based in information theory , and a rigorous statistical foundation for AIC . AIC can be justiﬁed as Bayesian using a “savvy” prior on models that is a function of sample size and the number of model parameters . Furthermore , BIC can be derived as a non - Bayesian result . Therefore , arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective . The philosophical context of what is assumed about reality , approximating models , and the intent of model - based infer - ence should determine whether AIC or BIC is used . Various facets of such multimodel inference are presented here , particularly methods of model averaging . Keywords : AIC ; BIC ; model averaging ; model selection ; multimodel inference 1 . INTRODUCTION For a model selection context , we assume that there are data and a set of models and that statistical inference is to be model based . Clas - sically , it is assumed that there is a single correct ( or even true ) or , at least , best model , and that model sufﬁces as the sole model for making inferences from the data . Although the identity ( and para - meter values ) of that model is unknown , it seems to be assumed that it can be estimated—in fact , well estimated . Therefore , classical infer - ence often involves a data - based search , over the model set , for ( i . e . , selection of ) that single correct model ( but with estimated para - meters ) . Then inference is based on the ﬁtted selected model as if it were the only model considered . Model selection uncertainty is SOCIOLOGICAL METHODS & RESEARCH , Vol . 33 , No . 2 , November 2004 261 - 304 DOI : 10 . 1177 / 0049124104268644 © 2004 Sage Publications 261 262 SOCIOLOGICAL METHODS & RESEARCH ignored . This is considered justiﬁed because , after all , the single best model has been found . However , many selection methods used ( e . g . , classical stepwise selection ) are not even based on an explicit criterion of what is a best model . One might think the ﬁrst step to improved inference under model selection would be to establish a selection criterion , such as the Akaike information criterion ( AIC ) or the Bayesian information cri - terion ( BIC ) . However , we claim that the ﬁrst step is to establish a philosophy about models and data analysis and then ﬁnd a suitable model selection criterion . The key issue of such a philosophy seems to center on one issue : Are models ever true , in the sense that full real - ity is represented exactly by a model we can conceive and ﬁt to the data , or are models merely approximations ? Even minimally experi - enced practitioners of data analysis would surely say models are only approximations to full reality . Given this latter viewpoint , the issue is then really about whether the information ( “truth” ) in the data , as extractable by the models in the set , is simple ( a few big effects only ) or complex ( many tapering effects ) . Moreover , there is a fundamen - tal issue of seeking parsimony in model ﬁtting : What “size” of ﬁtted model can be justiﬁed given the size of the sample , especially in the case of complex data ( we believe most real data are complex ) ? Model selection should be based on a well - justiﬁed criterion of what is the “best” model , and that criterion should be based on a phi - losophy about models and model - based statistical inference , includ - ing the fact that the data are ﬁnite and “noisy . ” The criterion must be estimable from the data for each ﬁtted model , and the criterion must ﬁt into a general statistical inference framework . Basically , this means that model selection is justiﬁed and operates within either a likeli - hood or Bayesian framework or within both frameworks . Moreover , this criterion must reduce to a number for each ﬁtted model , given the data , and it must allow computation of model weights to quan - tify the uncertainty that each model is the target best model . Such a framework and methodology allows us to go beyond inference based on only the selected best model . Rather , we do inference based on the full set of models : multimodel inference . Very little of the extensive model selection literature goes beyond the concept of a single best model , often because it is assumed that the model set contains the true model . This is true even for major or recent publications ( e . g . , Linhart and Zucchini 1986 ; McQuarrie and Tsai 1998 ; Lahiri 2001 ) . Burnham , Anderson / MULTIMODEL INFERENCE 263 Two well - known approaches meet these conditions operationally : information - theoretic selection based on Kullback - Leibler ( K - L ) information loss and Bayesian model selection based on Bayes fac - tors . AIC represents the ﬁrst approach . We will let the BIC approxima - tion to the Bayes factor represent the second approach ; exact Bayesian model selection ( see , e . g . , Gelfand and Dey 1994 ) can be much more complex than BIC—too complex for our purposes here . The focus and message of our study is on the depth of the foundation underlying K - L information and AIC . Many people using , abusing , or refusing AIC do not know its foundations or its current depth of development for coping with model selection uncertainty ( multimodel inference ) . Moreover , understanding either AIC or BIC is enhanced by contrast - ing them ; therefore , we will provide contrasts . Another reason to include BIC here , despite AIC being our focus , is because by using the BIC approximation to the Bayes factor , we can show that AIC has a Bayesian derivation . We will not give the mathematical derivations of AIC or BIC . Neither will we say much about the philosophy on deriving a prior set of models . Mathematical and philosophical background for our purposes is given in Burnham and Anderson ( 2002 ) . There is much other relevant literature that we could direct the reader to about AIC ( e . g . , Akaike 1973 , 1981 ; deLeeuw 1992 ) and Bayesian model selec - tion ( e . g . , Gelfand and Dey 1994 ; Gelman et al . 1995 ; Raftery 1995 ; Kass and Raftery 1995 ; Key , Pericchi , and Smith 1999 ; Hoeting et al . 1999 ) . For an extensive set of references , we direct the reader to Burnham and Anderson ( 2002 ) and Lahiri ( 2001 ) . We do not assume the reader has read all , or much , of this literature . However , we do assume that the reader has a general familiarity with model selection , including having encountered AIC and BIC , as well as arguments pro and con about which one to use ( e . g . , Weakliem 1999 ) . Our article is organized around the following sections . Section 2 is a careful review of K - L information ; parsimony ; AIC as an asymptotically unbiased estimator of relative , expected K - L informa - tion ; AIC c and Takeuchi’s information criterion ( TIC ) ; scaling crite - rion values ( (cid:1) i ) ; the discrete likelihood of model i , given the data ; Akaike weights ; the concept of evidence ; and measures of precision that incorporate model selection uncertainty . Section 3 is a review of the basis and application of BIC . Issues surrounding the assumption of a true model , the role of sample size in model selection when a 264 SOCIOLOGICAL METHODS & RESEARCH true model is assumed , and real - world issues such as the existence of tapering effect sizes are reviewed . Section 4 is a derivation of AIC as a Bayesian result ; this derivation hinges on the use of a “savvy” prior on models . Often , model priors attempt to be noninformative ; however , this practice has hidden and important implications ( it is not innocent ) . Section 5 introduces several philosophical issues and com - parisons between AIC versus BIC . This section focuses additional attention on truth , approximating models of truth , and the careless notion of true models ( mathematical models that exactly express full reality ) . Model selection philosophy should not be based on simple Bayesian versus non - Bayesian arguments . Section 6 compares the performance of AIC versus BIC and notes that many Monte Carlo sim - ulations are aimed only at assessing the probability of ﬁnding the true model . This practice misses the point of statistical inference and has led to widespread misunderstandings . Section 6 also makes the case for multimodel inference procedures , rather than making inference from only the model estimated to be best . Multimodel inference often lessens the performance differences between AIC and BIC selection . Finally , Section 7 presents a discussion of the more important issues and concludes that model selection should be viewed as a way to com - pute model weights ( posterior model probabilities ) , often as a step toward model averaging and other forms of multimodel inference . 2 . AIC : AN ASYMPTOTICALLY UNBIASED ESTIMATOR OF EXPECTED K - L INFORMATION SCIENCE PHILOSOPHY AND THE INFORMATION - THEORETIC APPROACH Information theorists do not believe in the notion of true models . Models , by deﬁnition , are only approximations to unknown reality or truth ; there are no true models that perfectly reﬂect full reality . George Box made the famous statement , “All models are wrong but some are useful . ” Furthermore , a “best model , ” for analysis of data , depends on sample size ; smaller effects can often only be revealed as sample size increases . The amount of information in large data sets ( e . g . , n = 3 , 500 ) greatly exceeds the information in small data sets ( e . g . , n = 22 ) . Data sets in some ﬁelds are very large ( terabytes ) , and Burnham , Anderson / MULTIMODEL INFERENCE 265 good approximating models for such applications are often highly structured and parameterized compared to more typical applications in which sample size is modest . The information - theoretic paradigm rests on the assumption that good data , relevant to the issue , are available , and these have been collected in an appropriate manner ( Bayesians would want this also ) . Three general principles guide model - based inference in the sciences . Simplicity and parsimony . Occam’s razor suggests , “Shave away all but what is necessary . ” Parsimony enjoys a featured place in scientiﬁc thinking in general and in modeling speciﬁcally ( see Forster and Sober 1994 ; Forster 2000 , 2001 for a strictly science philo - sophy perspective ) . Model selection ( variable selection in regression is a special case ) is a bias versus variance trade - off , and this is the statistical principle of parsimony . Inference under models with too few parameters ( variables ) can be biased , while with models having too many parameters ( variables ) , there may be poor precision or iden - tiﬁcation of effects that are , in fact , spurious . These considerations call for a balance between under - and overﬁtted models—the so - called model selection problem ( see Forster 2000 , 2001 ) . Multiple working hypotheses . Chamberlin ( [ 1890 ] 1965 ) advo - cated the concept of “multiple working hypotheses . ” Here , there is no null hypothesis ; instead , there are several well - supported hypothe - ses ( equivalently , “models” ) that are being entertained . The a priori “science” of the issue enters at this important stage . Relevant empiri - cal data are then gathered and analyzed , and it is expected that the results tend to support one or more hypotheses while providing less support for other hypotheses . Repetition of this general approach leads to advances in the sciences . New or more elaborate hypothe - ses are added , while hypotheses with little empirical support are gradually dropped from consideration . At any one point in time , there are multiple hypotheses ( models ) still under consideration—the model set evolves . An important feature of this multiplicity is that the number of alternative models should be kept small ; the analysis of , say , hundreds or thousands of models is not justiﬁed , except when prediction is the only objective or in the most exploratory phases of an investigation . We have seen applications in which more than a million models were ﬁtted , even though the sample size was modest ( 60 - 200 ) ; we do not view such activities as reasonable . 266 SOCIOLOGICAL METHODS & RESEARCH Similarly , a proper analysis must consider the science context and cannot successfully be based on “just the numbers . ” Strength of evidence . Providing quantitative information to judge the “strength of evidence” is central to science . Null hypothesis testing only provides arbitrary dichotomies ( e . g . , signiﬁcant vs . nonsigniﬁ - cant ) , and in the all - too - often - seen case in which the null hypothesis is false on a priori grounds , the test result is superﬂuous . Hypothesis testing is particularly limited in model selection , and this is well docu - mented in the statistical literature . Royall ( 1997 ) provides an interest - ing discussion of the likelihood - based strength - of - evidence approach in simple statistical situations . KULLBACK - LEIBLER INFORMATION In 1951 , S . Kullback and R . A . Leibler published a now - famous paper ( Kullback and Leibler 1951 ) that quantiﬁed the meaning of “information” as related to R . A . Fisher’s concept of sufﬁcient statistics . Their celebrated result , called Kullback - Leibler infor - mation , is a fundamental quantity in the sciences and has earlier roots back to Boltzmann’s concept of entropy ( Boltzmann 1877 ) . Boltzmann’s entropy and the associated second law of thermody - namics represents one of the most outstanding achievements of nineteenth - century science . We begin with the concept that f denotes full reality or truth ; f has no parameters ( parameters are a human concept ) . We use g to denote an approximating model , a probability distribution . K - L information I ( f , g ) is the information lost when model g is used to approximate f ; this is deﬁned for continuous functions as the integral I ( f , g ) = (cid:1) f ( x ) log (cid:2) f ( x ) g ( x | θ ) (cid:3) dx . Clearly , the best model loses the least information relative to other models in the set ; this is equivalent to minimizing I ( f , g ) over g . Alternatively , K - L information can be conceptualized as a “distance” between full reality and a model . Full reality f is considered to be ﬁxed , and only g varies over a space of models indexed by θ . Of course , full reality is not a function of sample size n ; truth does not change as n changes . No concept Burnham , Anderson / MULTIMODEL INFERENCE 267 of a true model is implied here , and no assumption is made that the models must be nested . The criterion I ( f , g ) cannot be used directly in model selection because it requires knowledge of full truth , or reality , and the para - meters θ in the approximating models , g i ( or , more explicitly , g i ( x | θ ) ) . In data analysis , the model parameters must be estimated , and there is often substantial uncertainty in this estimation . Models based on estimated parameters represent a major distinction from the case in which model parameters are known . This distinction affects how K - L information must be used as a basis for model selection and ranking and requires a change in the model selection criterion to that of min - imizing expected estimated K - L information rather than minimizing known K - L information ( over the set of R models considered ) . K - L information can be expressed as I ( f , g ) = (cid:1) f ( x ) log ( f ( x ) ) dx − (cid:1) f ( x ) log ( g ( x | θ ) ) dx , or I ( f , g ) = E f [ log ( f ( x ) ) ] − E f [ log ( g ( x | θ ) ) ] , where the expectations are taken with respect to truth . The quantity E f [ log ( f ( x ) ) ] is a constant ( say , C ) across models . Hence , I ( f , g ) = C − E f [ log ( g ( x | θ ) ) ] , where C = (cid:1) f ( x ) log ( f ( x ) ) dx does not depend on the data or the model . Thus , only relative expected K - L information , E f [ log ( g ( x | θ ) ) ] , needs to be estimated for each model in the set . AKAIKE’S INFORMATION CRITERION ( AIC ) Akaike ( 1973 , 1974 , 1985 , 1994 ) showed that the critical issue for getting a rigorous model selection criterion based on K - L information was to estimate E y E x [ log ( g ( x | ˆ θ ( y ) ) ) ] , 268 SOCIOLOGICAL METHODS & RESEARCH where the inner part is just E f [ log ( g ( x | θ ) ) ] , with θ replaced by the maximum likelihood estimator ( MLE ) of θ based on the assumed model g and data y . Although only y denotes data , it is convenient to conceptualize both x and y as independent random samples from the same distribution . Both statistical expectations are taken with respect to truth ( f ) . This double expectation is the target of all model selection approaches based on K - L information ( e . g . , AIC , AIC c , and TIC ) . Akaike ( 1973 , 1974 ) found a formal relationship between K - L information ( a dominant paradigm in information and coding theory ) and likelihood theory ( the dominant paradigm in statistics ) ( see deLeeuw 1992 ) . He found that the maximized log - likelihood value was a biased estimate of E y E x [ log ( g ( x | ˆ θ ( y ) ) ) ] , but this bias was approximately equal to K , the number of estimable para - meters in the approximating model , g ( for details , see Burnham and Anderson 2002 , chap . 7 ) . This is an asymptotic result of fun - damental importance . Thus , an approximately unbiased estimator of E y E x [ log ( g ( x | ˆ θ ( y ) ) ) ] for large samples and “good” models is log ( L ( ˆ θ | data ) ) – K . This result is equivalent to log ( L ( ˆ θ | data ) ) − K = C − ˆ E ˆ θ [ I ( f , ˆ g ) ] , where ˆ g = g ( · | ˆ θ ) . This ﬁnding makes it possible to combine estimation ( i . e . , maxi - mum likelihood or least squares ) and model selection under a uni - ﬁed optimization framework . Akaike found an estimator of expected , relative K - L information based on the maximized log - likelihood func - tion , corrected for asymptotic bias : relative ˆ E ( K - L ) = log ( L ( ˆ θ | data ) ) − K . K is the asymptotic bias correction term and is in no way arbitrary ( as is sometimes erroneously stated in the literature ) . Akaike ( 1973 , 1974 ) multiplied this simple but profound result by – 2 ( for “historical reasons” ) , and this became Akaike’s information criterion : AIC = − 2 log ( L ( ˆ θ | data ) ) + 2 K . In the special case of least squares ( LS ) estimation with normally distributed errors , AIC can be expressed as AIC = n log ( ˆ σ 2 ) + 2 K , Burnham , Anderson / MULTIMODEL INFERENCE 269 where ˆ σ 2 = (cid:4) ( ˆ (cid:4) i ) 2 n , and the ˆ (cid:4) i are the estimated residuals from the ﬁtted model . In this case , K must be the total number of parameters in the model , including the intercept and σ 2 . Thus , AIC is easy to compute from the results of LS estimation in the case of linear models or from the results of a likelihood - based analysis in general ( Edwards 1992 ; Azzalini 1996 ) . Akaike’s procedures are now called information theoretic because they are based on K - L information ( see Akaike 1983 , 1992 , 1994 ; Parzen , Tanabe , and Kitagawa 1998 ) . It is common to ﬁnd literature that seems to deal only with AIC as one of many types of criteria , without any apparent understanding that AIC is an estimate of some - thing much more fundamental : K - L information . Assuming a set of a priori candidate models has been deﬁned and is well supported by the underlying science , then AIC is computed for each of the approximating models in the set ( i . e . , g i , i = 1 , 2 , . . . , R ) . Using AIC , the models are then easily ranked from best to worst based on the empirical data at hand . This is a simple , compelling concept , based on deep theoretical foundations ( i . e . , entropy , K - L informa - tion , and likelihood theory ) . Assuming independence of the sample variates , AIC model selection has certain cross - validation properties ( Stone 1974 , 1977 ) . It seems worth noting here that the large sample approximates the expected value of AIC ( for a “good” model ) , inasmuch as this result is not given in Burnham and Anderson ( 2002 ) . The MLE ˆ θ ( y ) con - verges , as n gets large , to the θ o that minimizes K - L information loss for model g . Large - sample expected AIC converges to E ( AIC ) = − 2 C + 2 I ( f , g ( · | θ o ) ) + K . IMPORTANT REFINEMENTS : EXTENDED CRITERIA Akaike’s approach allowed model selection to be ﬁrmly based on a fundamental theory and allowed further theoretical work . When K is large relative to sample size n ( which includes when n is small , for any K ) , there is a small - sample ( second - order bias correction ) 270 SOCIOLOGICAL METHODS & RESEARCH version called AIC c , AIC c = − 2 log ( L ( ˆ θ ) ) + 2 K + 2 K ( K + 1 ) n − K − 1 ( see Sugiura 1978 ; Hurvich and Tsai 1989 , 1995 ) , and this should be used unless n / K > about 40 for the model with the largest value of K . A pervasive mistake in the model selection literature is the use of AIC when AIC c really should be used . Because AIC c converges to AIC , as n gets large , in practice , AIC c should be used . People often conclude that AIC overﬁts because they failed to use the second - order criterion , AIC c . Takeuchi ( 1976 ) derived an asymptotically unbiased estimator of relative , expected K - L information that applies in general without assuming that model g is true ( i . e . , without the special conditions underlying Akaike’s derivation of AIC ) . His method ( TIC ) requires quite large sample sizes to reliably estimate the bias adjustment term , which is the trace of the product of two K - by - K matrices ( i . e . , tr [ J ( θ o ) I ( θ o ) − 1 ] ; details in Burnham and Anderson 2002 : 65 - 66 , 362 - 74 ) . TIC represents an important conceptual advance and further justiﬁes AIC . In many cases , the complicated bias adjustment term is approximately equal to K , and this result gives further credence to using AIC and AIC c in practice . In a sense , AIC is a parsimo - nious approach to TIC . The large - sample expected value of TIC is E ( TIC ) = − 2 C + 2 I ( f , g ( · | θ o ) ) + tr [ J ( θ o ) I ( θ o ) − 1 ] . Investigators working in applied data analysis have several power - ful methods for ranking models and making inferences from empiri - cal data to the population or process of interest . In practice , one need not assume that the “true model” is in the set of candidates ( although this is sometimes mistakenly stated in the technical liter - ature on AIC ) . These information criteria are estimates of relative , expected K - L information and are an extension of Fisher’s likelihood theory ( Akaike 1992 ) . AIC and AIC c are easy to compute and quite effective in a very wide variety of applications . (cid:1) i VALUES The individual AIC values are not interpretable as they contain arbitrary constants and are much affected by sample size ( we have Burnham , Anderson / MULTIMODEL INFERENCE 271 seen AIC values ranging from – 600 to 340 , 000 ) . Here it is imperative to rescale AIC or AIC c to (cid:1) i = AIC i − AIC min , where AIC min is the minimum of the R different AIC i values ( i . e . , the minimum is at i = min ) . This transformation forces the best model to have (cid:1) = 0 , while the rest of the models have positive values . The constant representing E f [ log ( f ( x ) ) ] is eliminated from these (cid:1) i values . Hence , (cid:1) i is the information loss experienced if we are using ﬁtted model g i rather than the best model , g min , for infer - ence . These (cid:1) i allow meaningful interpretation without the unknown scaling constants and sample size issues that enter into AIC values . The (cid:1) i are easy to interpret and allow a quick strength - of - evidence comparison and ranking of candidate hypotheses or models . The larger the (cid:1) i , the less plausible is ﬁtted model i as being the best approximating model in the candidate set . It is generally important to know which model ( hypothesis ) is second best ( the ranking ) , as well as some measure of its standing with respect to the best model . Some simple rules of thumb are often useful in assessing the relative merits of models in the set : Models having (cid:1) i ≤ 2 have substantial support ( evidence ) , those in which 4 ≤ (cid:1) i ≤ 7 have considerably less sup - port , and models having (cid:1) i > 10 have essentially no support . These rough guidelines have similar counterparts in the Bayesian literature ( Raftery 1996 ) . Naive users often question the importance of a (cid:1) i = 10 when the two AIC values might be , for example , 280 , 000 and 280 , 010 . The difference of 10 here might seem trivial . In fact , large AIC values contain large scaling constants , while the (cid:1) i are free of such constants . Only these differences in AIC are interpretable as to the strength of evidence . LIKELIHOOD OF A MODEL GIVEN THE DATA The simple transformation exp ( − (cid:1) i / 2 ) , for i = 1 , 2 , . . . , R , provides the likelihood of the model ( Akaike 1981 ) given the data : L ( g i | data ) . ( Recall that Akaike deﬁned his AIC after multiplying through by – 2 ; otherwise , L ( g i | data ) = exp ( (cid:1) i ) would have been the case , with (cid:1) redeﬁned in the obvious way ) . This is a likelihood 272 SOCIOLOGICAL METHODS & RESEARCH function over the model set in the sense that L ( θ | data , g i ) is the like - lihood over the parameter space ( for model g i ) of the parameter θ , given the data ( x ) and the model ( g i ) . The relative likelihood of model i versus model j is L ( g i | data ) / L ( g j | data ) ; this is termed the evidence ratio , and it does not depend on any of the other models under consideration . Without loss of gen - erality , we may assume that model g i is more likely than g j . Then , if this evidence ratio is large ( e . g . , > 150 is quite large ) , model g j is a poor model relative to model g i , based on the data . AKAIKE WEIGHTS , w i It is convenient to normalize the model likelihoods such that they sum to 1 and treat them as probabilities ; hence , we use w i = exp ( − (cid:1) i / 2 ) (cid:4) Rr = 1 exp ( − (cid:1) r / 2 ) . The w i , called Akaike weights , are useful as the “weight of evi - dence” in favor of model g i ( · | θ ) as being the actual K - L best model in the set ( in this context , a model , g , is considered a “parameter” ) . The ratios w i / w j are identical to the original likelihood ratios , L ( g i | data ) / L ( g j | data ) , and so they are invariant to the model set , but the w i values depend on the full model set because they sum to 1 . However , w i , i = 1 , . . . , R are useful in additional ways . For exam - ple , the w i are interpreted as the probability that model i is , in fact , the K - L best model for the data ( strictly under K - L information the - ory , this is a heuristic interpretation , but it is justiﬁed by a Bayesian interpretation of AIC ; see below ) . This latter inference about model selection uncertainty is conditional on both the data and the full set of a priori models considered . UNCONDITIONAL ESTIMATES OF PRECISION , A TYPE OF MULTIMODEL INFERENCE Typically , estimates of sampling variance are conditional on a given model as if there were no uncertainty about which model to use ( Breiman called this a “quiet scandal” ; Breiman 1992 ) . When model selection has been done , there is a variance component due to model selection uncertainty that should be incorporated into estimates of Burnham , Anderson / MULTIMODEL INFERENCE 273 precision . That is , one needs estimates that are “unconditional” on the selected model . A simple estimator of the unconditional variance for the maximum likelihood estimator ˆ θ from the selected ( best ) model is (cid:5) var ( ˆ¯ θ ) = (cid:6) R (cid:7) i = 1 w i [ (cid:5) var ( ˆ θ i | g i ) + ( ˆ θ i − ˆ¯ θ ) 2 ] 1 / 2 (cid:8) 2 , ( 1 ) where ˆ¯ θ = R (cid:7) i = 1 w i ˆ θ i , and ˆ ¯ θ represents a form of “model averaging . ” The notation ˆ θ i here means that the parameter θ is estimated based on model g i , but θ is a parameter in common to all R models ( even if its value is 0 in model k , so then we use ˆ θ k = 0 ) . This estimator , from Buckland , Burnham , and Augustin ( 1997 ) , includes a term for the conditional sampling variance , given model g i ( denoted as (cid:5) var ( ˆ θ i | g i ) here ) , and a variance component for model selection uncertainty , ( ˆ θ i − ˆ ¯ θ ) 2 . These variance components are multiplied by the Akaike weights , which reﬂect the relative support , or evidence , for model i . Burnham and Anderson ( 2002 : 206 - 43 ) provide a number of Monte Carlo results on achieved conﬁdence interval coverage when information - theoretic approaches are used in some moderately challenging data sets . For the most part , achieved conﬁdence interval coverage is near the nominal level . Model averaging arises naturally when the uncondi - tional variance is derived . OTHER FORMS OF MULTIMODEL INFERENCE Rather than base inferences on a single , selected best model from an a priori set of models , inference can be based on the entire set of models . Such inferences can be made if a parameter , say θ , is in common over all models ( as θ i in model g i ) or if the goal is prediction . Then , by using the weighted average for that parameter across models ( i . e . , ˆ¯ θ = (cid:4) w i ˆ θ i ) , we are basing point inference on the entire set of models . This approach has both practical and philosophical advan - tages . When a model - averaged estimator can be used , it often has a 274 SOCIOLOGICAL METHODS & RESEARCH more honest measure of precision and reduced bias compared to the estimator from just the selected best model ( Burnham and Anderson 2002 , chaps . 4 – 6 ) . In all - subsets regression , we can consider that the regression coefﬁcient ( parameter ) β p for predictor x p is in all the models , but for some models , β p = 0 ( x p is not in those models ) . In this situation , if model averaging is done over all the models , the resultant estimator (cid:9) β p has less model selection bias than ˆ β p taken from the selected best model ( Burnham and Anderson 2002 : 151 - 3 , 248 - 55 ) . Assessment of the relative importance of variables has often been based only on the best model ( e . g . , often selected using a stepwise test - ing procedure ) . Variables in that best model are considered “impor - tant , ” while excluded variables are considered not important . This is too simplistic . Importance of a variable can be reﬁned by mak - ing inference from all the models in the candidate set ( see Burnham and Anderson 2002 , chaps . 4 – 6 ) . Akaike weights are summed for all models containing predictor variable x j , j = 1 , . . . , R ; denote these sums as w + ( j ) . The predictor variable with the largest predictor weight , w + ( j ) , is estimated to be the most important ; the variable with the smallest sum is estimated to be the least important predictor . This procedure is superior to making inferences concerning the rel - ative importance of variables based only on the best model . This is particularly important when the second or third best model is nearly as well supported as the best model or when all models have nearly equal support . ( There are “design” considerations about the set of models to consider when a goal is assessing variable importance . We do not discuss these considerations here—the key issue is one of balance of models with and without each variable . ) SUMMARY At a conceptual level , reasonable data and a good model allow a separation of “information” and “noise . ” Here , information relates to the structure of relationships , estimates of model parameters , and components of variance . Noise , then , refers to the residuals : variation left unexplained . We want an approximating model that minimizes information loss , I ( f , g ) , and properly separates noise ( noninforma - tion or entropy ) from structural information . In a very important sense , Burnham , Anderson / MULTIMODEL INFERENCE 275 we are not trying to model the data ; instead , we are trying to model the information in the data . Information - theoretic methods are relatively simple to understand and practical to employ across a very large class of empirical situa - tions and scientiﬁc disciplines . The methods are easy to compute by hand if necessary , assuming one has the parameter estimates , the con - ditional variances (cid:5) var ( ˆ θ i | g i ) , and the maximized log - likelihood values for each of the R candidate models from standard statistical software . Researchers can easily understand the heuristics and application of the information - theoretic methods ; we believe it is very important that people understand the methods they employ . Information - theoretic approaches should not be used unthinkingly ; a good set of a priori models is essential , and this involves professional judgment and inte - gration of the science of the issue into the model set . 3 . UNDERSTANDING BIC Schwarz ( 1978 ) derived the Bayesian information criterion as BIC = − 2 ln ( L ) + K log ( n ) . As usually used , one computes the BIC for each model and selects the model with the smallest criterion value . BIC is a misnomer as it is not related to information theory . As with (cid:1) AIC i , we deﬁne (cid:1) BIC i as the difference of BIC for model g i and the minimum BIC value . More complete usage entails computing posterior model probabilities , p i , as p i = Pr { g i | data } = exp ( − 12 (cid:1) BIC i ) (cid:4) Rr = 1 exp ( − 12 (cid:1) BIC r ) ( Raftery 1995 ) . The above posterior model probabilities are based on assuming that prior model probabilities are all 1 / R . Most applications of BIC use it in a frequentist spirit and hence ignore issues of prior and posterior model probabilities . The model selection literature , as a whole , is confusing as regards the following issues about BIC ( and about Bayesian model selection in general ) : 276 SOCIOLOGICAL METHODS & RESEARCH 1 . Does the derivation of BIC assume the existence of a true model , or , more narrowly , is the true model assumed to be in the model set when using BIC ? ( Schwarz’s derivation speciﬁed these conditions . ) 2 . What do the “model probabilities” mean ? That is , how should we interpret them vis - ` a - vis a “true” model ? Mathematically ( we emphasize mathematical here ) , for an iid sam - ple and a ﬁxed set of models , there is a model—say , model g t —with posterior probability p t such that as n → ∞ , then p t → 1 and all other p r → 0 . In this sense , there is a clear target model that BIC “seeks” to select . 3 . Does the above result mean model g t must be the true model ? The answers to questions 1 and 3 are simple : no . That is , BIC ( as the basis for an approximation to a certain Bayesian integral ) can be derived without assuming that the model underlying the deriva - tion is true ( see , e . g . , Cavanaugh and Neath 1999 ; Burnham and Anderson 2002 : 293 - 5 ) . Certainly , in applying BIC , the model set need not contain the ( nonexistent ) true model representing full reality . Moreover , the convergence in probability of the BIC - selected model to a target model ( under the idealization of an iid sample ) does not logically mean that that target model must be the true data - generating distribution . The answer to question 2 involves characterizing the target model to which the BIC - selected model converges . That model can be char - acterized in terms of the values of the K - L discrepancy and K for the set of models . For model g r , the K - L “distance” of the model from the truth is denoted I ( f , g r ) . Often , g r ≡ g r ( x | θ ) would denote a para - metric family of models for θ ∈ (cid:6) , with (cid:6) being a K r - dimensional space . However , we take g r generally to denote the speciﬁc family member for the unique θ o ∈ (cid:6) , which makes g r , in the family of models , closest to the truth in K - L distance . For the family of models g r ( x | θ ) , θ ∈ (cid:6) , as n → ∞ ( with iid data ) , the MLE , and the Bayesian point estimator of θ converge to θ o . Thus , asymptotically , we can characterize the particular model that g r represents : g r ≡ g r ( x | θ o ) ( for details , see Burnham and Anderson 2002 and references cited therein ) . Also , we have the set of corresponding minimized K - L dis - tances : { I ( f , g r ) , r = 1 , . . . , R } . For an iid sample , we can represent these distances as I ( f , g r ) = nI 1 ( f , g r ) , where the I 1 ( f , g r ) do not Burnham , Anderson / MULTIMODEL INFERENCE 277 depend on sample size ( they are for n = 1 ) . The point of this repre - sentation is to emphasize that the effect of increasing sample size is to scale up these distances . We may assume , without loss of generality , that these models are indexed worst ( g 1 ) to best ( g R ) in terms of their K - L distance and dimension K r ; hence , I ( f , g 1 ) ≥ I ( f , g 2 ) ≥ · · · ≥ I ( f , g R ) . Figures 1 through 3 show three hypothetical scenarios of how these ordered distances might appear for R = 12 models , for unspeciﬁed n ( since n serves merely to scale the y - axis ) . Let Q be the tail - end subset of the so - ordered models , deﬁned by { g r , r ≥ t , 1 ≤ t ≤ R | I ( f , g t − 1 ) > I ( f , g t ) = · · · = I ( f , g R ) } . Set Q exists because t = R ( and t = 1 ) is allowed , in which case the K - L best model ( of the R models ) is unique . For the case when subset Q contains more than one model ( i . e . , 1 ≤ t < R ) , then all of the models in this sub - set have the same K - L distance . Therefore , we further assume that models g t to g R are ordered such that K t < K t + 1 ≤ · · · ≤ K R ( in principle , K t = K t + 1 could occur ) . Thus , model g t is the most parsimonious model of the subset of models that are tied for the K - L best model . In this scenario ( iid sample , ﬁxed model set , n → ∞ ) , the BIC - selected model converges with probability 1 to model g t , and p t converges to 1 . However , unless I ( f , g t ) = 0 , model g t is not identical to f ( nominally considered as truth ) , so we call it a quasi - true model . The only truth here is that in this model set , models g t + 1 to g R provide no improvement over model g t —they are unnecessarily general ( independent of sample size ) . The quasi - true model in the set of R models is the most parsimonious model that is closest to the truth in K - L information loss ( model 12 in Figures 1 and 3 , model 4 in Figure 2 ) . Thus , the Bayesian posterior model probability p r is the inferred probability that model g r is the quasi - true model in the model set . For a “very large” sample size , model g t is the best model to use for inference . However , for small or moderate sample sizes obtained in practice , the model selected by BIC may be much more parsimonious than model g t , especially if the quasi - true model is the most general model , g R , as in Figure 1 . The concern for realistic sample sizes , then , is that the BIC - selected model may be underﬁt at the given n . The model selected by BIC approaches the BIC target model from below , as n increases , in terms of the ordering we imposed on the model 278 SOCIOLOGICAL METHODS & RESEARCH Model K L D i s t a n ce Figure 1 : Values of Kullback - Leibler ( K - L ) Information Loss , I ( f , g r ( · | θ o ) ) ≡ nI 1 ( f , g r ( · | θ o ) ) , Illustrated Under Tapering Effects for 12 Models Ordered by Decreasing K - L Information NOTE : Sample size n , and hence the y - axis is left unspeciﬁed ; this scenario favors Akaike information criterion ( AIC ) – based model selection . set . This selected model can be quite far from the BIC theoretical target model at sample sizes seen in practice when tapering effects are present ( Figure 1 ) . The situation in which BIC performs well is that shown in Figure 2 , with suitably large n . Moreover , the BIC target model does not depend on sample size n . However , we know that the number of parameters we can expect to reliably estimate from ﬁnite data does depend on n . In particular , if the set of ordered ( large to small ) K - L distances shows tapering effects ( Figure 1 ) , then a best model for making inference from the data may well be a more parsimonious model than the BIC target model ( g 12 in Figure 1 ) , such as the best expected estimated K - L model , which is the AIC target model . As noted above , the target model for AIC is the model that minimizes E f [ I ( f , g r ( · | ˆ θ ) ) ] , r = 1 , . . . , R . This target model is speciﬁc for the sample size at hand ; hence , AIC seeks Burnham , Anderson / MULTIMODEL INFERENCE 279 0 5 10 15 20 Model K L D i s t a n ce Figure 2 : Values of Kullback - Leibler ( K - L ) Information Loss , I ( f , g r ( · | θ o ) ) ≡ nI 1 ( f , g r ( · | θ o ) ) , Illustrated When Models 1 ( Simplest ) to 12 ( Most General ) Are Nested With Only a Few Big Effects NOTE : Model 4 is a quasi - true model , and Models 5 to 12 are too general . Sample size n , and hence the y - axis is left unspeciﬁed ; this scenario favors Bayesian information criterion ( BIC ) – based model selection . a best model as its target , where best is heuristically a bias - variance trade - off ( not a quasi - true model ) . In reality , one can only assert that BIC model selection is asymp - totically consistent for the ( generally ) unique quasi - true model in the set of models . But that BIC - selected model can be quite biased at not - large n as an estimator of its target model . Also , from an infer - ence point of view , observing that p t is nearly 1 does not justify an inference that model g t is truth ( such a statistical inference requires an a priori certainty that the true model is in the model set ) . This issue is intimately related to the fact that only differences such as I ( f , g r ) − I ( f , g t ) are estimable from data ( these K - L differences are closely related to AIC r – AIC t differences , hence to the (cid:1) ) . Hence , with model selection , the effect is that sometimes people are erroneously lulled into thinking ( assuming ) that I ( f , g t ) is 0 and 280 SOCIOLOGICAL METHODS & RESEARCH hence thinking that they have found ( the model for ) full reality . These ﬁtted models sometimes have seven or fewer parameters ; surely , full reality cannot be so simple in the life sciences , economics , medicine , and the social sciences . 4 . AIC AS A BAYESIAN RESULT BIC model selection arises in the context of a large - sample approxi - mation to the Bayes factor , conjoined with assuming equal priors on models . The BIC statistic can be used more generally with any set of model priors . Let q i be the prior probability placed on model g i . Then the Bayesian posterior model probability is approximated as Pr { g i | data } = exp ( − 12 (cid:1) BIC i ) q i (cid:4) Rr = 1 exp ( − 12 (cid:1) BIC r ) q r ( this posterior actually depends on not just the data but also on the model set and the prior distribution on those models ) . Akaike weights can be easily obtained by using the model prior q i as proportional to exp (cid:2) 1 2 (cid:1) BIC i (cid:3) . exp (cid:2) − 1 2 (cid:1) AIC i (cid:3) . Clearly , exp (cid:2) − 1 2 (cid:1) BIC i (cid:3) . exp (cid:2) 1 2 (cid:1) BIC i (cid:3) . exp (cid:2) − 1 2 (cid:1) AIC i (cid:3) = exp (cid:2) − 1 2 (cid:1) AIC i (cid:3) . Hence , with the implied prior probability distribution on models , we get p i = Pr { g i | data } = exp ( − 12 (cid:1) BIC i ) q i (cid:4) Rr = 1 exp ( − 12 (cid:1) BIC r ) q r = exp ( − 12 (cid:1) AIC i ) (cid:4) Rr = 1 exp ( − 12 (cid:1) AIC r ) = w i , which is the Akaike weight for model g i . Burnham , Anderson / MULTIMODEL INFERENCE 281 This prior probability on models can be expressed in a simple form as q i = C . exp (cid:2) 1 2 K i log ( n ) − K i (cid:3) , ( 2 ) where C = 1 (cid:4) Rr = 1 exp ( 12 K r log ( n ) − K r ) . ( 3 ) Thus , formally , the Akaike weights from AIC are ( for large samples ) Bayesian posterior model probabilities for this model prior ( more details are in Burnham and Anderson 2002 : 302 - 5 ) . Given a model g ( x | θ ) , the prior distribution on θ will not and should not depend on sample size . This is very reasonable . Probably following from this line of reasoning , traditional Bayesian thinking about the prior distribution on models has been that q r , r = 1 , . . . , R would also not depend on n or K r . This approach is neither necessary nor reasonable . There is limited information in a sample , so the more parameters one estimates , the poorer the average precision becomes for these estimates . Hence , in considering the prior distribution q on models , we must consider the context of what we are assuming about the information in the data , as regards parameter estimation , and the models as approximations to some conceptual underlying “full - truth” generating distribution . While q r = 1 / R seems reasonable and innocent , it is not always reasonable and is never innocent ; that is , it implies that the target model is truth rather than a best approximating model , given that parameters are to be estimated . This is an important and unexpected result . It is useful to think in terms of effects , for individual parameters , as | θ | / se ( ˆ θ ) . The standard error depends on sample size ; hence , effect size depends on sample size . We would assume for such effects that few or none are truly zero in the context of analysis of real data from complex observational , quasi - experimental , or experimental studies ( i . e . , Figure 1 applies ) . In the information - theoretic spirit , we assume meaningful , informative data and thoughtfully selected predictors and models ( not all studies meet these ideals ) . We assume tapering effects : Some may be big ( values such as 10 or 5 ) , but some are only 2 , 1 , 0 . 5 , or less . We assume we can only estimate n / m parameters reliably ; 282 SOCIOLOGICAL METHODS & RESEARCH m might be 20 or as small as 10 ( but surely , m (cid:7) 1 and m (cid:8) 100 ) . ( In contrast , in the scenario in which BIC performs better than AIC , it is assumed that there are a few big effects deﬁning the quasi - true model , which is itself nested in several or many overly general models ; i . e . , Figure 2 applies ) . These concepts imply that the size ( i . e . , K ) of the appropriate model to ﬁt the data should logically depend on n . This idea is not foreign to the statistical literature . For example , Lehman ( 1990 : 160 ) attributes to R . A . Fisher the quote , “More or less elaborate forms will be suit - able according to the volume of the data . ” Using the notation k 0 for the optimal K , Lehman ( 1990 ) goes on to say , “The value of k 0 will tend to increase as the number of observations increases and its determina - tion thus constitutes implementation of Fisher’s suggestion” ( p . 162 ) . Williams ( 2001 ) states , “We CANNOT ignore the degree of resolu - tion of the experiment when choosing our prior” ( p . 235 ) . These ideas have led to a model prior wherein conceptually , q r should depend on n and K r . Such a prior ( class of priors , actually ) is called a savvy prior . A savvy ( deﬁnition : shrewdly informed ) prior is logical under the information - theoretic model selection paradigm . We will call the savvy prior on models given by q i = C . exp (cid:2) 1 2 K i log ( n ) − K i (cid:3) ( formula 3b gives C ) the K - L model prior . It is unique in terms of pro - ducing the AIC as approximately a Bayesian procedure ( approximate only because BIC is an approximation ) . Alternative savvy priors might be based on distributions such as a modiﬁed Poisson ( i . e . , applied to only K r , r = 1 , . . . , R ) , with expected K set to be n / 10 . We looked at this idea in an all - subsets selection context and found that the K - L model prior produces a more spread - out ( higher entropy ) prior as compared to such a Poisson - based savvy prior when both produced the same E ( K ) . We are not wanting to start a cottage industry of seeking a best savvy prior because model - averaged inference seems very robust to model weights when those weights are well founded ( as is the case for Akaike weights ) . The full implications of being able to interpret AIC as a Bayesian result have not been determined and are an issue outside the scope of this study . It is , however , worth mentioning that the model - averaged Burnham , Anderson / MULTIMODEL INFERENCE 283 Bayesian posterior is a mixture distribution of each model - speciﬁc posterior distribution , with weights being the posterior model prob - abilities . Therefore , for any model - averaged parameter estimator , particularly for model - averaged prediction , alternative variance and covariance formulas are (cid:5) var ( ˆ¯ θ ) = R (cid:7) i = 1 w i [ (cid:5) var ( ˆ θ i | g i ) + ( ˆ θ i − ˆ¯ θ ) 2 ] , ( 4 ) (cid:10) cov ( ˆ¯ θ , ˆ¯ τ ) = R (cid:7) i = 1 w i [ (cid:10) cov ( ˆ θ i , ˆ τ i | g i ) + ( ˆ θ i − ˆ¯ θ ) ( ˆ τ i − ˆ¯ τ ) ] . ( 5 ) The formula given in Burnham and Anderson ( 2002 : 163 - 4 ) for such an unconditional covariance is ad hoc ; hence , we now recom - mend the above covariance formula . We have rerun many simulations and examples from Burnham and Anderson ( 1998 ) using variance formula ( 4 ) and found that its performance is almost identical to that of the original unconditional variance formula ( 1 ) ( see also Burnham and Anderson 2002 : 344 - 5 ) . Our pragmatic thought is that it may well be desirable to use formula ( 4 ) rather than ( 1 ) , but it is not necessary , except when covariances ( formula 5 ) are also computed . 5 . RATIONAL CHOICE OF AIC OR BIC FREQUENTIST VERSUS BAYESIAN IS NOT THE ISSUE The model selection literature contains , de facto , a long - running debate about using AIC or BIC . Much of the purely mathematical or Bayesian literature recommends BIC . We maintain that almost all the arguments for the use of BIC rather than AIC , with real data , are ﬂawed and hence contribute more to confusion than to understand - ing . This assertion by itself is not an argument for AIC or against BIC because there are clearly deﬁned contexts in which each method outperforms the other ( Figure 1 or 2 for AIC or BIC , respectively ) . For some people , BIC is strongly preferred because it is a Bayesian procedure , and they think AIC is non - Bayesian . However , AIC model selection is just as much a Bayesian procedure as is BIC selection . The difference is in the prior distribution placed on the model set . 284 SOCIOLOGICAL METHODS & RESEARCH Hence , for a Bayesian procedure , the argument about BIC versus AIC must reduce to one about priors on the models . Alternatively , both AIC and BIC can be argued for or derived under a non - Bayesian approach . We have given above the arguments for AIC . When BIC is so derived , it is usually motivated by the mathe - matical context of nested models , including a true model simpler than the most general model in the set . This corresponds to the context of Figure 2 , except with the added ( but not needed ) assumption that I ( f , g t ) = 0 . Moreover , the goal is taken to be the selection of this true model , with probability 1 as n → ∞ ( asymptotic consistency or sometimes dimension consistency ) . Given that AIC and BIC model selection can both be derived as either frequentist or Bayesian procedures , one cannot argue for or against either of them on the basis that it is or is not Bayesian or non - Bayesian . What fundamentally distinguishes AIC versus BIC model selection is their different philosophies , including the exact nature of their target models and the conditions under which one outperforms the other for performance measures such as predictive mean square error . Thus , we maintain that comparison , hence selection for use , of AIC versus BIC must be based on comparing measures of their perfor - mance under conditions realistic of applications . ( A now - rare version of Bayesian philosophy would deny the validity of such hypothetical frequentist comparisons as a basis for justifying inference methodo - logy . We regard such nihilism as being outside of the evidential spirit of science ; we demand evidence . ) DIFFERENT PHILOSOPHIES AND TARGET MODELS We have given the different philosophies and contexts in which the AIC or BIC model selection criteria arise and can be expected to perform well . Here we explicitly contrast these underpinnings in terms of K - L distances for the model set { g r ( x | θ o ) , r = 1 , . . . , R } , with reference to Figures 1 , 2 , and 3 , which represent I ( f , g r ) = nI 1 ( f , g r ) . Sample size n is left unspeciﬁed , except that it is large relative to K R , the largest value of K r , yet of a practical size ( e . g . , K R = 15 and n = 200 ) . Given that the model parameters must be estimated so that parsi - mony is an important consideration , then just by looking at Figure 1 , Burnham , Anderson / MULTIMODEL INFERENCE 285 we cannot say what is the best model to use for inference as a model ﬁtted to the data . Model 12 , as g 12 ( x | θ o ) ( i . e . , at θ being the K - L distance - minimizing parameter value in (cid:6) for this class of models ) , is the best theoretical model , but g 12 ( x | ˆ θ ) may not be the best model for inference . Model 12 is the target model for BIC but not for AIC . The target model for AIC will depend on n and could be , for example , Model 7 ( there would be an n for which this would be true ) . Despite that the target of BIC is a more general model than the target model for AIC , the model most often selected here by BIC will be less general than Model 7 unless n is very large . It might be Model 5 or 6 . It is known ( from numerous papers and simulations in the literature ) that in the tapering - effects context ( Figure 1 ) , AIC performs better than BIC . If this is the context of one’s real data analysis , then AIC should be used . A very different scenario is given by Figure 2 , wherein there are a few big effects , all captured by Model 4 ( i . e . , g 4 ( x | θ o ) ) , and Models 5 to 12 do not improve at all on Model 4 . This scenario generally corresponds with Model 4 being nested in Models 5 to 12 , often as part of a full sequence of nested models , g i ⊂ g i + 1 . The obvious target model for selection is Model 4 ; Models 1 to 3 are too restrictive , and models in the class of Models 5 to 12 contain unneeded parameters ( such parameters are actually zero ) . Scenarios such as that in Figure 2 are often used in simulation evaluations of model selection , despite that they seem unrealistic for most real data , so conclusions do not logically extend to the Figure 1 ( or Figure 3 ) scenario . Under the Figure 2 scenario and for sufﬁciently large n , BIC often selects Model 4 and does not select more general models ( but may select less general models ) . AIC will select Model 4 much of the time , will tend not to select less general models , but will sometimes select more general models and do so even if n is large . It is this scenario that motivates the model selection literature to conclude that BIC is consistent and AIC is not consistent . We maintain that this conclu - sion is for an unrealistic scenario with respect to a lot of real data as regards the pattern of the K - L distances . Also ignored in this conclusion is the issue that for real data , the model set itself should change as sample size increases by orders of magnitude . Also , infer - entially , such “consistency” can only imply a quasi - true model , not truth as such . 286 SOCIOLOGICAL METHODS & RESEARCH 0 5 10 15 20 Model K L D i s t a n ce Figure 3 : Values of Kullback - Leibler ( K - L ) Information Loss , I ( f , g r ( · | θ o ) ) ≡ nI 1 ( f , g r ( · | θ o ) ) , Illustrated When Models 1 ( Simplest ) to 12 ( Most General ) Are Nested With a Few Big Effects ( Model 4 ) , Then Much Smaller Tapering Effects ( Models 5 - 12 ) NOTE : Whether the Bayesian information criterion ( BIC ) or the Akaike information criterion ( AIC ) is favored depends on sample size . That reality could be as depicted in Figure 2 seems strained , but it could be as depicted in Figure 3 ( as well as Figure 1 ) . The latter scenario might occur and presents a problematic case for theoretical analysis . Simulation seems needed there and , in general , to evaluate model selection performance under realistic scenarios . For Figure 3 , the target model for BIC is also Model 12 , but Model 4 would likely be a better choice at moderate to even large sample sizes . FULL REALITY AND TAPERING EFFECTS Often , the context of data analysis with a focus on model selection is one of many covariates and predictive factors ( x ) . The conceptual truth underlying the data is about what is the marginal truth just for Burnham , Anderson / MULTIMODEL INFERENCE 287 this context and these measured factors . If this truth , conceptually as f ( y | x ) , implies that E ( y | x ) has tapering effects , then any ﬁtted good model will need tapering effects . In the context of a linear model , and for an unknown ( to us ) ordering of the predictors , then for E ( y | x ) = β 0 + β 1 x 1 + · · · β p x p , our models will have | β 1 / se ( ˆ β 1 ) | > | β 2 / se ( ˆ β 2 ) | > · · · > | β p / se ( ˆ β p ) | > 0 ( β here is the K - L best parameter value , given truth f and model g ) . It is pos - sible that | β p / se ( ˆ β p ) | would be very small ( almost zero ) relative to | β 1 / se ( ˆ β 1 ) | . For nested models , appropriately ordered , such taper - ing effects would lead to graphs such as Figure 1 or 3 for either the K - L values or the actual | β r / se ( ˆ β r ) | . Whereas tapering effects for full reality are expected to require tapering effects in models and hence a context in which AIC selec - tion is called for , in principle , full reality could be simple , in some sense , and yet our model set might require tapering effects . The effects ( tapering or not ) that matter as regards whether AIC ( Figure 1 ) or BIC ( Figure 2 ) model selection is the method of choice are the K - L values I ( f , g r ( · | β o ) ) , r = 1 , . . . , R , not what is implicit in truth itself . Thus , if the type of models g in our model set are a poor approximation to truth f , we can expect tapering effects for the corresponding K - L values . For example , consider the target model E ( y | x ) = 17 + ( 0 . 3 ( x 1 x 2 ) 0 . 5 ) + exp ( − 0 . 5 ( x 3 ( x 4 ) 2 ) ) . However , if our candidate models are all linear in the predictors ( with main effects , interactions , quadratic effects , etc . ) , we will have tapering effects in the model set , and AIC is the method of choice . Our conclusion is that we nearly always face some tapering effect sizes ; these are revealed as sample size increases . 6 . ON PERFORMANCE COMPARISONS OF AIC AND BIC There are now ample and diverse theories for AIC - and BIC - based model selection and multimodel inference , such as model averaging ( as opposed to the traditional “use only the selected best model for inference” ) . Also , it is clear that there are different conditions under which AIC or BIC should outperform the other one in measures such as estimated mean square error . Moreover , performance evaluations and comparisons should be for actual sample sizes seen in practice , 288 SOCIOLOGICAL METHODS & RESEARCH not just asymptotically ; partly , this is because if sample size increased substantially , we should then consider revising the model set . There are many simulation studies in the statistical literature on either AIC or BIC alone or often comparing them and making recommendations on which one to use . Overall , these studies have led to confusion because they often have failed to be clear on the condi - tions and objectives of the simulations or generalized ( extrapolated , actually ) their conclusions beyond the speciﬁc conditions of the study . For example , were the study conditions only the Figure 2 scenarios ( all too often , yes ) , and so BIC was favored ? Were the Figure 1 , 2 , and 3 scenarios all used but the author’s objective was to select the true model , which was placed in the model set ( and usually was a simple model ) , and hence results were confusing and often disap - pointing ? We submit that many reported studies are not appropriate as a basis for inference about which criterion should be used for model selection with real data . Also , many studies , even now , only examine operating properties ( e . g . , conﬁdence interval coverage and mean square error ) of infer - ence based on the use of just the selected best model ( e . g . , Meyer and Laud 2002 ) . There is a strong need to evaluate operating properties of multimodel inference in scenarios realistic of real data analysis . Authors need to be very clear about the simulation scenarios used vis - ` a - vis the generating model : Is it simple or complex , is it in the model set , and are there tapering effects ? One must also be careful to note if the objective of the study was to select the true model or if it was to select a best model , as for prediction . These factors and considerations affect the conclusions from simulation evaluations of model selection . Authors should avoid sweeping conclusions based on limited , perhaps unrealistic , simulation scenarios ; this error is com - mon in the literature . Finally , to have realistic objectives , the infer - ence goal ought to be that of obtaining best predictive inference or best inference about a parameter in common to all models , rather than “select the true model . ” MODEL - AVERAGED VERSUS BEST - MODEL INFERENCE When prediction is the goal , one can use model - averaged inference rather than prediction based on a single selected best model ( hereafter referred to as “best” ) . Burnham , Anderson / MULTIMODEL INFERENCE 289 It is clear from the literature that has evaluated or even considered model - averaged inferences compared to the best - model strategy that model averaging is superior ( e . g . , Buckland et al . 1997 ; Hoeting et al . 1999 ; Wasserman 2000 ; Breiman 2001 ; Burnham and Anderson 2002 ; Hansen and Kooperberg 2002 ) . The method known as boosting is a type of model averaging ( Hand and Vinciotti 2003 : 130 ; this article is also useful reading for its comments on truth and models ) . However , model - averaged inference is not common , nor has there been much effort to evaluate it even in major publica - tions on model selection or in simulation studies on model selection ; such studies all too often look only at the best - model strategy . Model averaging and multimodel inference in general are deserving of more research . As an example of predictive performance , we report here some results of simulation based on the real data used in Johnson ( 1996 ) . These data were originally taken to explore multiple regression to predict the percentage of body fat based on 13 predictors ( body mea - surements ) that are easily measured . We chose these data as a focus because they were used by Hoeting et al . ( 1999 ) in illustrating BIC and Bayesian model averaging ( see also Burnham and Anderson 2002 : 268 - 84 ) . The data are from a sample of 252 males , ages 21 to 81 , and are available on the Web in conjunction with Johnson ( 1996 ) . The Web site states , “The data were generously supplied by Dr . A . Garth Fisher , Human Performance Research Center , Brigham Young University , Provo , Utah 84602 , who gave permission to freely distribute the data and use them for non - commercial purposes . ” We take the response variable as y = 1 / D ; D is measured body density ( observed minimum and maximum are 0 . 9950 and 1 . 1089 , respectively ) . The correlations among the 13 predictors are strong but not extreme , are almost entirely positive , and range from − 0 . 245 ( age and height ) to 0 . 941 ( weight and hip circumference ) . The design matrix is full rank . The literature ( e . g . , Hoeting et al . 1999 ) supports that the measurements y and x = ( x 1 , . . . , x 13 ) (cid:10) on a subject can be suitably modeled as multivariate normal . Hence , we base simula - tion on a joint multivariate model mimicking these 14 variables by using the observed variance - covariance matrix as truth . From that full 14 × 14 observed variance - covariance matrix for y and x , as well as the theory of multivariate normal distributions , we computed 290 SOCIOLOGICAL METHODS & RESEARCH TABLE 1 : Effects , as β / se ( ˆ β ) , in the Models Used for Monte Carlo Simulation Based on the Body Fat Data to Get Predictive Mean Square Error Results by Model Selection Method ( AIC c or BIC ) and Prediction Strategy ( Best Model or Model Averaged ) i β / se ( ˆ β ) Variable j 1 11 . 245 6 2 − 3 . 408 13 3 2 . 307 12 4 − 2 . 052 4 5 1 . 787 8 6 − 1 . 731 2 7 1 . 691 1 8 − 1 . 487 7 9 1 . 422 11 10 1 . 277 10 11 − 0 . 510 5 12 − 0 . 454 3 13 0 . 048 9 NOTE : Model i has the effects listed on lines 1 to i , and its remaining β are 0 . AIC = Akaike information criterion ; BIC = Bayesian information criterion . for the full linear model of y , regressed on x , the theoretical regres - sion coefﬁcients and their standard errors . The resultant theoretical effect sizes , β i / se ( ˆ β i ) , taken as underlying the simulation , are given in Table 1 , ordered from largest to smallest by their absolute values . Also shown is the index ( j ) of the actual predictor variable as ordered in Johnson ( 1996 ) . We generated data from 13 models that range from having only one huge effect size ( generating Model 1 ) to the full tapering - effects model ( Model 13 ) . This was done by ﬁrst generating a value of x from its assumed 13 - dimensional “marginal” multivariate distri - bution . Then we generated y = E ( y | x ) + (cid:4) ( (cid:4) was independent of x ) for 13 speciﬁc models of E i ( y | x ) with (cid:4) ∼ normal ( 0 , σ 2 i ) , i = 1 , . . . , 13 . Given the generating structural model on expected y , σ i was speciﬁed so that the total expected variation ( structural plus residual ) in y was always the same and was equal to the total vari - ation of y in the original data . Thus , σ 1 , . . . , σ 13 are monotonically decreasing . For the structural data - generating models , we used E 1 ( y | x ) = β 0 + β 6 x 6 ( generating Model 1 ) , E 2 ( y | x ) = β 0 + β 6 x 6 + β 13 x 13 ( generating Model 2 ) , and so forth . Without loss of generality , we Burnham , Anderson / MULTIMODEL INFERENCE 291 used β 0 = 0 . Thus , from Table 1 , one can perceive the structure of each generating model reported on in Table 2 . Theory asserts that under generating Model 1 , BIC is relatively more preferred ( leads to bet - ter predictions ) , but as the sequence of generating models progresses , K - L - based model selection becomes increasingly more preferred . Independently from each generating model , we generated 10 , 000 samples of x and y , each of size n = 252 . For each such sample , all possible 8 , 192 models were ﬁt ; that is , all - subsets model selec - tion was used based on all 13 predictor variables ( regardless of the data - generating model ) . Model selection was then applied to this set of models using both AIC c and BIC to ﬁnd the corresponding sets of model weights ( posterior model probabilities ) and hence also the best model ( with n = 252 , and maximum K being 15 AIC c rather than AIC should be used ) . The full set of simulations took about two months of CPU time on a 1 . 9 - GHz Pentium 4 computer . The inference goal in this simulation was prediction . Therefore , after model ﬁtting for each sample , we generated , from the same gen - erating model i , one additional statistically independent value of x and then of E ( y ) ≡ E i ( y | x ) . Based on the ﬁtted models from the generated sample data and this new x , E ( y | x ) was predicted ( hence , ˆ E ( y ) ) , either from the selected best model or as the model - averaged prediction . The measure of prediction performance used was pre - dictive mean square error ( PMSE ) , as given by the estimated ( from 10 , 000 trials ) expected value of ( ˆ E ( y ) − E i ( y | x ) ) 2 . Thus , we obtained four PMSE values from each set of 10 , 000 trials : PMSE for both the “best” and “model - averaged” strategies for both AIC c and BIC . Denote these as PMSE ( AIC c , best ) , PMSE ( AIC c , ma ) , PMSE ( BIC , best ) , and PMSE ( BIC , ma ) , respectively . Absolute values of these PMSEs are not of interest here because our goal is comparison of methods ; hence , in Table 2 , we report only ratios of these PMSEs . The ﬁrst two columns of Table 2 compare results for AIC c to those for BIC based on the ratios PMSE ( AIC c , best ) PMSE ( BIC , best ) , column 1 , Table 2 PMSE ( AIC c , ma ) PMSE ( BIC , ma ) , column 2 , Table 2 . 292 SOCIOLOGICAL METHODS & RESEARCH TABLE 2 : Ratios of Predictive Mean Square Error ( PMSE ) Based on Monte Carlo Simulation Patterned After the Body Fat Data , With 10 , 000 Independent Trials for Each Generating Model PMSE Ratios of PMSE Ratios for AIC c ÷ BIC Model Averaged ÷ Best Generating Best Model Model i Model Averaged AIC c BIC 1 2 . 53 1 . 97 0 . 73 0 . 94 2 1 . 83 1 . 51 0 . 80 0 . 97 3 1 . 18 1 . 15 0 . 83 0 . 85 4 1 . 01 1 . 05 0 . 84 0 . 81 5 0 . 87 0 . 95 0 . 84 0 . 77 6 0 . 78 0 . 88 0 . 87 0 . 77 7 0 . 77 0 . 86 0 . 86 0 . 77 8 0 . 80 0 . 87 0 . 85 0 . 78 9 0 . 80 0 . 87 0 . 85 0 . 78 10 0 . 72 0 . 81 0 . 85 0 . 75 11 0 . 74 0 . 82 0 . 84 0 . 76 12 0 . 74 0 . 81 0 . 84 0 . 76 13 0 . 74 0 . 82 0 . 83 0 . 75 NOTE : Margin of error for each ratio is 3 percent ; generating model i has exactly i effects , ordered largest to smallest for Models 1 to 13 ( see Table 1 and text for details ) . AIC = Akaike information criterion ; BIC = Bayesian information criterion . Thus , if AIC c produces better prediction results for generating model i , the value in that row for columns 1 or 2 is < 1 ; otherwise , BIC is better . The results are as qualitatively expected : Under a Figure 2 scenario with only a few big effects ( or no effects ) , such as for generating Models 1 or 2 , BIC outperforms AIC c . But as we move more into a tapering - effects scenario ( Figure 1 ) , AIC c is better . We also see from Table 2 that , by comparing columns 1 and 2 , the performance difference of AIC c versus BIC is reduced under model averaging : Column 2 values are generally closer to 1 than are column 1 values , under the same generating model . Columns 3 and 4 of Table 2 compare the model - averaged to best - model strategy within AIC c or BIC methods : PMSE ( AIC c , ma ) PMSE ( AIC c , best ) , column 3 , Table 2 Burnham , Anderson / MULTIMODEL INFERENCE 293 PMSE ( BIC , ma ) PMSE ( BIC , best ) , column 4 , Table 2 . Thus , if model - averaged prediction is more accurate than best - model prediction , the value in column 3 or 4 is < 1 , which it always is . It is clear that here , for prediction , model averaging is always better than the best - model strategy . The literature and our own other research on this issue suggest that such a conclusion will hold generally . A ﬁnal comment about information in Table 2 , columns 3 and 4 : The smaller the ratio , the more beneﬁcial the model - averaging strategy compared to the best - model strategy . In summary , we maintain that the proper way to compare AIC - and BIC - based model selection is in terms of achieved performance , espe - cially prediction but also conﬁdence interval coverage . In so doing , it must be realized that these two criteria for computing model weights have their optimal performance under different conditions : AIC for tapering effects and BIC for when there are no effects at all or a few big effects and all others are zero effects ( no intermediate effects , no tapering effects ) . Moreover , the extant evidence strongly supports that model averaging ( where applicable ) produces better performance for either AIC or BIC under all circumstances . GOODNESS OF FIT AFTER MODEL SELECTION Goodness - of - ﬁt theory about the selected best model is a subject that has been almost totally ignored in the model selection litera - ture . In particular , if the global model ﬁts the data , does the selected model also ﬁt ? This appears to be a virtually unexplored question ; we have not seen it rigorously addressed in the statistical literature . Post – model selection ﬁt is an issue deserving of attention ; we present here some ideas and results on the issue . Full - blown simulation eval - uation would require a speciﬁc context of a data type and a class of models , data generation , model ﬁtting , selection , and then application of an appropriate goodness - of - ﬁt test ( either absolute or at least rel - ative to the global model ) . This would be time - consuming , and one might wonder if the inferences would generalize to other contexts . A simple , informative shortcut can be employed to gain insights into the relative ﬁt of the selected best model compared to a global model assumed to ﬁt the data . The key to this shortcut is to deal 294 SOCIOLOGICAL METHODS & RESEARCH with a single sequence of nested models , g 1 ⊂ · · · ⊂ g i ⊂ g i + 1 ⊂ · · · ⊂ g R . It sufﬁces that each model increments by one parameter ( i . e . , K i + 1 = K i + 1 ) , and K 1 is arbitrary ; K 1 = 1 is convenient as then K i = i . In this context , AIC i = AIC i + 1 + χ 21 ( λ i ) − 2 and BIC i = BIC i + 1 + χ 21 ( λ i ) − log ( n ) , where χ 21 ( λ i ) is a noncentral chi - square random variable on 1 degree of freedom with noncentrality parameter λ i . In fact , χ 21 ( λ i ) is the likelihood ratio test statistic between models g i and g i + 1 ( a type of relative , not absolute , goodness - of - ﬁt test ) . Moreover , we can use λ i = nλ 1 i , where nominally , λ 1 i is for sample size 1 . These λ are the parameter effect sizes , and there is an analogy between them and the K - L distances here : The differences I ( f , g i ) − I ( f , g i + 1 ) are analogous to and behave like these λ i . Building on these ideas ( cf . Burnham and Anderson 2002 : 412 - 14 ) , we get AIC i = AIC R + R − 1 (cid:7) j = i ( χ 21 ( nλ 1 j ) − 2 ) or , for AIC c , AIC ci = AIC cR + R − 1 (cid:7) j = i (cid:11) χ 21 ( nλ 1 j ) − 2 + 2 K i ( K i + 1 ) n − K i − 1 − 2 K i + 1 ) ( K i + 2 ) n − K i − 2 (cid:12) , and BIC i = BIC R + R − 1 (cid:7) j = i ( χ 21 ( nλ 1 j ) − log ( n ) ) . To generate these sequences of model selection criteria in a coher - ent manner from the underlying “data , ” it sufﬁces to , for example , generate the AIC i based on the above and then use AIC ci = AIC i + 2 K i ( K i + 1 ) n − K i − 1 Burnham , Anderson / MULTIMODEL INFERENCE 295 and BIC i = AIC i − 2 K i + K i log ( n ) to get the AIC ci and BIC i . Because only differences in AIC c or BIC values matter , it sufﬁces to set AIC R to a constant . Thus , for speciﬁed R , K 1 , n , and λ 1 i , we generate the needed R − 1 independent non - central chi - square random variables . Then we compute a realization of the sequences of AIC and BIC values for the underlying nested model sequence . We can then determine the best model under each model selection criterion . If model g h is selected as best under a criterion , for h < R , then the usual goodness - of - ﬁt test statistic ( for ﬁt relative to the global model g R ) is χ 2 v = R − 1 (cid:7) j = h χ 21 ( nλ 1 j ) , with degrees of freedom v = K R − K h ( = R − h when K i = i ) . Hence , we can simulate having one set of data , doing both AIC ( or AIC c ) and BIC model selection for that data , and then check the good - ness of ﬁt of each selected best model , relative to the baseline global model g R . The results apply to discrete or continuous data but do assume “large” n . These simulations generate a lot of tabular information , so we present below only a typical example . In general , we recommend that interested readers run their own simulations ( they are easy to do and run quickly ; SAS code for doing this is available from KPB ) . We have done a lot of such simulation to explore primarily one question : After model selection with AIC or BIC , does the selected model always ﬁt , as judged by the usual likelihood ratio statistic p value that tests g R versus the selected model ( this test ignores that a selection pro - cess was done ) ? Also , do the results differ for AIC versus BIC ? We found that for large enough n , so that AIC c and AIC are nearly the same , then for a Figure 1 scenario ( i . e . , realistic data ) , ( 1 ) the AIC - selected model always ﬁts relative to the global model , and ( 2 ) the BIC - selected model too often ( relative to the α - level of the test ) fails to ﬁt the data . Under a scenario such as in Figure 2 , the BIC - selected model generally ﬁts the data ; goodness - of - ﬁt ( GOF ) results for AIC model selection are about the same for all three scenarios . 296 SOCIOLOGICAL METHODS & RESEARCH TABLE 3 : Simulation of Goodness - of - Fit ( GOF ) Results After Model Selection for R = 10 Nested Models , K i = i , Effects λ 1 ( 1 ) to λ 1 ( 10 ) as 0 . 3 , 0 . 2 , 0 . 15 , 0 . 1 , 0 . 05 , 0 . 025 , 0 . 01 , 0 . 005 , 0 . 001 , and 0 . 0003 , Respectively Percentiles Relative of p Value Sample Selection Frequency Frequency Mean of Size n Method Not Fitting of h < 10 GOF p 1 5 10 25 50 AIC c 0 . 026 9 , 961 0 . 470 0 . 030 0 . 073 0 . 118 0 . 246 BIC 0 . 115 9 , 995 0 . 352 0 . 006 0 . 022 0 . 044 0 . 117 100 AIC c 0 . 004 9 , 809 0 . 511 0 . 063 0 . 120 0 . 171 0 . 296 BIC 0 . 159 9 , 995 0 . 470 0 . 003 0 . 014 0 . 030 0 . 087 200 AIC c 0 . 004 9 , 569 0 . 531 0 . 096 0 . 155 0 . 202 0 . 328 BIC 0 . 217 9 , 997 0 . 273 0 . 002 0 . 009 0 . 019 0 . 062 500 AIC c 0 . 000 9 , 178 0 . 546 0 . 127 0 . 178 0 . 224 0 . 345 BIC 0 . 281 9 , 992 0 . 236 0 . 001 0 . 005 0 . 011 0 . 041 1 , 000 AIC c 0 . 000 8 , 662 0 . 537 0 . 136 0 . 176 0 . 218 0 . 339 BIC 0 . 320 9 , 978 0 . 227 0 . 001 0 . 004 0 . 009 0 . 035 10 , 000 AIC c 0 . 000 3 , 761 0 . 448 0 . 159 0 . 171 0 . 187 0 . 244 BIC 0 . 509 9 , 295 0 . 135 0 . 000 0 . 001 0 . 002 0 . 009 NOTE : There were 10 , 000 trials at each n , α = 0 . 05 ; model g 10 was considered to always ﬁt , so results on GOF relate only to models g i , i < 10 . AIC = Akaike information criterion ; BIC = Bayesian information criterion . To be more precise , let α = 0 . 05 , so we say the selected model ﬁts if the ( relative ) goodness - of - ﬁt test p value > . 05 . Then , for the AIC - selected model , we almost always ﬁnd p > . 05 . However , for the BIC - selected model , under tapering effects , the probability that p < . 05 occurs can be much higher than the nominal α = 0 . 05 . For example , let R = 10 , K i = i , and λ 1 ( 1 ) to λ 1 ( 10 ) be 0 . 3 , 0 . 2 , 0 . 15 , 0 . 1 , 0 . 05 , 0 . 025 , 0 . 01 , 0 . 005 , 0 . 001 , and 0 . 0003 , respectively ( mim - ics Figure 1 ) . Table 3 gives some of these goodness - of - ﬁt results for AIC c and BIC under this scenario for a few values of n . In Table 3 , the key column is column 3 . It is the relative frequency at which the selected best - model g h did not ﬁt relative to Model 10 ( the global model here ) , in the sense that its GOF p value was ≤ . 05 . In calcu - lating this statistic , if the selected model was Model 10 , we assumed the model ﬁt . Hence , the lack - of - ﬁt statistic in Table 3 ( column 3 ) would be larger if it were only for when the selected best model was Models 1 through 9 . Column 4 of Table 3 gives the frequency , out of 10 , 000 trials , wherein the best model was one of Models 1 to 9 . These GOF results are striking . The model selected as best by AIC c ( which Burnham , Anderson / MULTIMODEL INFERENCE 297 is not really different here from AIC at n ≥ 200 ) rarely leads to a GOF p value < α = 0 . 05 for n ≥ 100 . The best BIC model often fails to ﬁt , relative to Model 10 , in terms of its GOF p value being ≤ . 05 ( e . g , GOF failure rate of 0 . 217 at n = 200 here ) . Columns 5 to 9 of Table 3 provide further summaries of these GOF p values when the selected best model was Models 1 through 9 . These results are not atypical under tapering effects . For the Figure 2 scenario that favors BIC , the GOF for the BIC - selected model comes much closer to nominal levels . Thus again , operating characteristics of AIC and BIC depend on the underlying scenario about reality versus the model set . What should we make of such results for the tapering - effects case ? Is it bad that the AIC - best model always ﬁts : Is it overﬁtting ? Is it bad that the BIC - best model fails to ﬁt at a much higher rate than the α - level : Is it underﬁtting ? We do not know because to have evidence about the matter , we need to have a context and actual parameters estimated and look at mean square errors and conﬁdence interval coverage ( see Burnham and Anderson 2002 : 207 - 23 ) . We make four comments on the issues . First , as regards a percep - tion of “overﬁt” by AIC , surely when one deliberately seeks a good model for analysis of data , one is seeking a good ﬁt . Thus , if the global model ﬁts , we think one would expect the best model , under a selec - tion criterion , to also ﬁt . Heuristically , it is a strange model selection criterion that often selects a best model that ﬁts poorly ; AIC does not do this . However , we also claim that the best model often allows some bias in estimates , which could be analogous to some lack of ﬁt . Therefore , second , with regard to BIC , the degree of lack of ﬁt may not matter—we do not know , so we do not claim it matters . Third , model - averaged inference further renders the GOF issue somewhat moot because all the models are being considered , not just the best model . Fourth , these observations and issues about ﬁt reinforce to us that model selection procedures should be judged on their inferential operating characteristics , such as predictive mean square error and interval coverage under realistic scenarios for the generation of data . 7 . DISCUSSION AND CONCLUSIONS The context of classical model selection proceeds in four steps : 1 . the goal is model - based inference from data , and 298 SOCIOLOGICAL METHODS & RESEARCH 2 . there is a set of R relevant models but no certainty about which model should be used ; hence , 3 . a data - based choice is made among these ( perceived as ) competing models , and 4 . then inference is made from this one selected model as if it were a priori the only model ﬁt to the data . Steps 1 and 2 are almost universal in model - based inference . Step 3 begins a ﬂawed inference scenario ; in particular , the implicit assump - tion that inference must be based on a single model is not justiﬁed by any philosophy or mathematics . To avoid the pitfalls inherent in Step 4 , we must conceptualize model selection to mean and be multimodel inference . The new Step 3 should be as follows : • There is a data - based assignment of model weights that sum to 1 . 0 ; the weight for model g i reﬂects the evidence or information concerning model g i ( uncertainty of model g i in the set of R models ) . The old Step 3 is subsumed in this new Step 3 because the model with the highest weight is the model that would be selected as the single best model . But now we avoid many of the problems that stem from old Step 4 by using a new Step 4 : • Based on the model weights , as well as the results and information from the R ﬁtted models , we use multimodel inference in some or all of its myriad forms and methods . Model selection should be viewed as the way to obtain model weights , not just a way to select only one model ( and then ignore that selection occurred ) . Among the other beneﬁts of this approach , it effectively rules out null hypothesis testing as a basis for model selection because mul - timodel inference forces a deeper approach to model selection . It means we must have an optimality criterion and selection ( weight assignment ) theory underlying the approach . Potential users should not reject or ignore multimodel inference just because it is relatively new , especially when based on AIC . There is a sound philosophical basis and likelihood framework for AIC , based on the K - L informa - tion theory , which itself has a deep foundation . An important issue about model selection based on K - L infor - mation is that AIC as such is a large - sample approximation Burnham , Anderson / MULTIMODEL INFERENCE 299 ( relative to the maximum K for the model set ) to the needed criterion . A second - order bias adjustment is needed when n / K is too small— say , ≤ 40 . While AIC c is not unique as providing the needed small - sample version of AIC , we recommend it for general use ; indeed , the evidence is that it performs well . Much confusion and misinforma - tion have resulted in the model selection literature when investigators have done simulation evaluations using AIC when they should have used AIC c ( Anderson and Burnham 2002 ) . A compatible , alternative view of AIC is that it arises from a Bayesian derivation based on the BIC statistic and a savvy prior prob - ability distribution on the R models . That prior depends on both n and K i ( i = 1 , . . . , R ) in a manner consistent with the information - theoretic viewpoint that the data at hand surely reﬂect a range of tapering effects based on a complex reality—rather than arising from a simple true model , with no tapering effects—that is in the model set . The model selection literature often errs by considering that AIC and BIC selection are directly comparable , as if they had the same objective target model . Their target models are different ( Reschenhofer 1996 ) . The target model of AIC is one that is speciﬁc for the sample size at hand : It is the ﬁtted model that minimizes expected estimated K - L information loss when ﬁtted model g r is used to approximate full reality , f . This target model changes with sample size . Moreover , in this overall philosophy , even the set of models is expected to be changed if there are large changes in n . The classical derivation of BIC assumed that there was a true model , independent of n , that generated the data ; it was a model in the model set , and this true model was the target model for selection by BIC . However , selection of this true model with probability 1 only occurs in the limit as n gets very large , and in taking that limit , the model set is kept ﬁxed . The original derivation of BIC has been relaxed , wherein we realize that such convergence only justiﬁes an inference of a quasi - true model ( the most parsimonious model closest in K - L information to truth , f ) . Even within the Bayesian framework , not all practitioners subscribe to BIC for model selection ( some Bayesians do not believe in model selection at all ) . In particular , we note the recent development of the deviance information criterion ( DIC ) by Spiegelhalter et al . ( 2002 ) . As these authors note , DIC behaves like 300 SOCIOLOGICAL METHODS & RESEARCH AIC , not like BIC , which is one reason they prefer DIC ( it avoids the defects of BIC model selection ) . Given that AIC can be derived from the BIC approximation to the Bayes factor , the distinction between AIC versus BIC model selection becomes one about the prior on models : q i = 1 / R for BIC or the K - L prior of Section 4 ( formulas 2 , 3 ) for AIC . This latter prior is a savvy prior , by which we mean that the expected number of parameters that can be estimated with useful precision depends on n and K ( which are known a priori ) . Thus , for a savvy prior , in general , q i becomes a function of n and K i —say , q i ( K i , n ) —and we think in terms of prior E ( K ) = n / m , for some m , perhaps in the 10 or 15 range . Fitting a model with too few parameters wastes information . With too many parameters in a model , some or all ( with typical correlated observational data ) of the estimated parameters are too imprecise to be inferentially useful . Objective Bayesian analysis with a single model uses an uninfor - mative ( vague ) prior such as U ( 0 , 1 ) on a parameter θ if 0 < θ < 1 . This turns out to be quite safe , sort of “innocent , ” one might say ( no lurking unexpected consequences ) . So presumably , it seemed natural , objective , and innocent when extending Bayesian methods to model selection to assume a uniform prior on models . However , we now know that this assumption has unexpected consequences ( it is not innocent ) , as regards the properties of the resultant model selection procedure . Conversely , there is a rationale for considering that the prior on models ought to depend on n and K , and so doing produces some quite different properties of the selection method as compared to the use of 1 / R . The choice of the prior on models can be impor - tant in model selection , and we maintain that q i should usually be a function of n and K . Whereas the best model selected by either BIC or AIC can be distinctly different and hence suggest partially conﬂicting inferences , model - averaged inference diminishes the perceived conﬂicts between AIC and BIC . In general , we have seen robustness of inference to variations in the model weights for rational choices of these weights . For this reason , we think that there is little need to seek alternative savvy priors to the K - L prior . Several lines of thinking motivate us to say that the compari - son of AIC and BIC model selection ought to be based on their Burnham , Anderson / MULTIMODEL INFERENCE 301 performance properties , such as the mean square error for parameter estimation ( includes prediction ) and conﬁdence interval coverage . When any such comparisons are done , the context must be spelled out explicitly because results ( i . e . , which method “wins” ) depend on context ( e . g . , Figures 1 - 3 ) . Simulation evaluations should generate realistically complex data , use AIC c , and use multimodel inference , hence going well beyond the traditional single best - model approach . We believe that data analysis should routinely be considered in the context of multimodel inference . Formal inference from more than one ( estimated best ) model arises naturally from both a science con - text ( multiple working hypotheses ) and a statistical context ( robust inference while making minimal assumptions ) . The information - theoretic procedures allowing multimodel inference are simple , both in terms of understanding and computation , and , when used properly , provide inferences with good properties ( e . g . , as regards predictive mean squared error and achieved conﬁdence interval coverage ) . Mul - timodel inference goes beyond the concepts and methods noted here ; we give a richer account in Burnham and Anderson ( 2002 ) . Model selection bias and model selection uncertainty are important issues that deserve further understanding . Multimodel inference is an new ﬁeld in which additional , innovative research and understanding are needed , and we expect a variety of important advances to appear in the years ahead . REFERENCES Akaike , Hirotugu . 1973 . “Information Theory as an Extension of the Maximum Likelihood Principle . ” Pp . 267 - 81 in Second International Symposium on Information Theory , edited by B . N . Petrov and F . Csaki . Budapest : Akademiai Kiado . ——— . 1974 . “A New Look at the Statistical Model Identiﬁcation . ” IEEE Transactions on Automatic Control AC - 19 : 716 - 23 . ——— . 1981 . “Likelihood of a Model and Information Criteria . ” Journal of Econometrics 16 : 3 - 14 . ——— . 1983 . “Information Measures and Model Selection . ” International Statistical Institute 44 : 277 - 91 . ——— . 1985 . “Prediction and Entropy . ” Pp . 1 - 24 in A Celebration of Statistics , edited by Anthony C . Atkinson and Stephen E . Fienberg . New York : Springer - Verlag . Akaike , Hirotugu . 1992 . “Information Theory and an Extension of the Maximum Likelihood Principle . ” Pp . 610 - 24 in Breakthroughs in Statistics , vol . 1 , edited by Samuel Kotz and Norman L . Johnson . London : Springer - Verlag . 302 SOCIOLOGICAL METHODS & RESEARCH ——— . 1994 . “Implications of the Informational Point of View on the Development of Statistical Science . ” Pp . 27 - 38 in Engineering and Scientiﬁc Applications : Vol . 3 . Proceed - ings of the First US / Japan Conference on the Frontiers of Statistical Modeling : An Infor - mational Approach , edited by Hamparsum Bozdogan . Dordrecht , the Netherlands : Kluwer Academic . Andserson , David R . and Kenneth P . Burnham . 2002 . “Avoiding Pitfalls When Using Information - Theoretic Methods . ” Journal of Wildlife Management 66 : 910 - 6 . Azzalini , Adelchi . 1996 . Statistical Inference Based on the Likelihood . London : Chapman & Hall . Boltzmann , Ludwig . 1877 . “Uber die Beziehung Zwischen dem Hauptsatze der Mechanis - chen Warmetheorie und der Wahrscheinlicjkeitsrechnung Respective den Satzen uber das Warmegleichgewicht . ” Wiener Berichte 76 : 373 - 435 . Breiman , Leo . 1992 . “The Little Bootstrap and Other Methods for Dimensionality Selection in Regression : X - Fixed Prediction Error . ” Journal of the American Statistical Association 87 : 738 - 54 . ——— . 2001 . “Statistical Modeling : The Two Cultures . ” Statistical Science 26 : 199 - 231 . Buckland , Steven T . , Kenneth P . Burnham , and Nicole H . Augustin . 1997 . “Model Selection : An Integral Part of Inference . ” Biometrics 53 : 603 - 18 . Burnham , KennethP . andDavidR . Anderson . 1998 . ModelSelectionandInference : APractical Information - Theoretical Approach . New York : Springer - Verlag . ——— . 2002 . ModelSelectionandMultimodelInference : APracticalInformation - Theoretical Approach . 2d ed . New York : Springer - Verlag . Cavanaugh , JosephE . andAndrewA . Neath . 1999 . “GeneralizingtheDerivationoftheSchwarz Information Criterion . ” Communication in Statistics Theory and Methods 28 : 49 - 66 . Chamberlin , Thomas . [ 1890 ] 1965 . “The Method of Multiple Working Hypotheses . ” Science 148 : 754 - 9 . deLeeuw , Jan . 1992 . “Introduction to Akaike ( 1973 ) Information Theory and an Extension of the Maximum Likelihood Principle . ” Pp . 599 - 609 in Breakthroughs in Statistics , vol . 1 , edited by Samuel Kotz and Norman L . Johnson . London : Springer - Verlag . Edwards , Anthony W . F . 1992 . Likelihood . Expanded ed . Baltimore : Johns Hopkins University Press . Forster , Malcolm R . 2000 . “Key Concepts in Model Selection : Performance and Generaliz - ability . ” Journal of Mathematical Psychology 44 : 205 - 31 . ——— . 2001 . “The New Science of Simplicity . ” Pp . 83 - 119 in Simplicity , Inference and Modelling : Keeping It Sophisticatedly Simple , edited by Arnold Zellner , Hugo A . Keuzenkamp , and Michael McAleer . Cambridge , UK : Cambridge University Press . Forster , Malcolm R . and Elliott Sober . 1994 . “How to Tell Simpler , More Uniﬁed , or Less Ad Hoc Theories Will Provide More Accurate Predictions . ” British Journal of the Philosophy of Science 45 : 1 - 35 . Gelfand , Alan and Dipak K . Dey . 1994 . “Bayesian Model Choice : Asymptotics and Exact Calculations . ” Journal of the Royal Statistical Society , Series B 56 : 501 - 14 . Gelman , Andrew , John C . Carlin , Hal S . Stern , and Donald B . Rubin . 1995 . Bayesian Data Analysis . New York : Chapman & Hall . Hand , David J . and Veronica Vinciotti . 2003 . “Local Versus Global Models for Classiﬁcation Problems : Fitting Models Where It Matters . ” The American Statistician 57 : 124 - 31 . Hansen , Mark H . and Charles Kooperberg . 2002 . “Spline Adaptation in Extended Linear Models . ” Statistical Science 17 : 2 - 51 . Hoeting , JenniferA . , DavidMadigan , AdrianE . Raftery , andChrisT . Volinsky . 1999 . “Bayesian Model Averaging : A Tutorial ( With Discussion ) . ” Statistical Science 14 : 382 - 417 . Burnham , Anderson / MULTIMODEL INFERENCE 303 Hurvich , Clifford M . and Chih - Ling Tsai . 1989 . “Regression and Time Series Model Selection in Small Samples . ” Biometrika 76 : 297 - 307 . ——— . 1995 . “Model Selection for Extended Quasi - Likelihood Models in Small Samples . ” Biometrics 51 : 1077 - 84 . Johnson , Roger W . 1996 . “Fitting Percentage of Body Fat to Simple Body Measure - ments . ” Journal of Statistics Education 4 ( 1 ) . Retrieved from www . amstat . org / publications / jse / v4n1 / datasets . johnson . html Kass , Robert E . and Adrian E . Raftery . 1995 . “Bayes Factors . ” Journal of the American Statistical Association 90 : 773 - 95 . Key , Jane T . , Luis R . Pericchi , and Adrian F . M . Smith . 1999 . “Bayesian Model Choice : What and Why ? ” Pp . 343 - 70 in Bayesian Statistics 6 , edited by Jos´e M . Bernardo , James O . Berger , A . Philip Dawid , and Adrian F . M . Smith . Oxford , UK : Oxford University Press . Kullback , Soloman and Richard A . Leibler . 1951 . “On Information and Sufﬁciency . ” Annals of Mathematical Statistics 22 : 79 - 86 . Lahiri , Partha , ed . 2001 . ModelSelection . Beachwood , OH : LectureNotes − MonographSeries , Institute of Mathematical Statistics . Lehman , Eric L . 1990 . “Model Speciﬁcation : The Views of Fisher and Neyman , and Later Observations . ” Statistical Science 5 : 160 - 8 . Linhart , H . and Walter Zucchini . 1986 . Model Selection . New York : John Wiley . McQuarrie , AlanD . R . andChih - LingTsai . 1998 . RegressionandTimeSeriesModelSelection . Singapore : World Scientiﬁc Publishing Company . Meyer , MaryC . andPurushottamW . Laud . 2002 . “PredictiveVariableSelectioninGeneralized Linear Models . ” Journal of the American Statistical Association 97 : 859 - 71 . Parzen , Emmanuel , Kunio Tanabe , and Genshiro Kitagawa , eds . 1998 . Selected Papers of Hirotugu Akaike . New York : Springer - Verlag . Raftery , Adrian E . 1995 . “Bayesian Model Selection in Social Research ( With Discussion ) . ” Sociological Methodology 25 : 111 - 95 . ——— . 1996 . “Approximate Bayes Factors and Accounting for Model Uncertainty in Gener - alized Linear Regression Models . ” Biometrika 83 : 251 - 66 . Reschenhofer , Erhard . 1996 . “Prediction With Vague Prior Knowledge . ” Communications in Statistics—Theory and Methods 25 : 601 - 8 . Royall , Richard M . 1997 . Statistical Evidence : A Likelihood Paradigm . London : Chapman & Hall . Stone , Mervyn . 1974 . “Cross - Validatory Choice and Assessment of Statistical Predictions ( With Discussion ) . ” Journal of the Royal Statistical Society , Series B 39 : 111 - 47 . ——— . 1977 . “An Asymptotic Equivalence of Choice of Model by Cross - Validation and Akaike’s Criterion . ” Journal of the Royal Statistical Society , Series B 39 : 44 - 7 . Schwarz , Gideon . 1978 . “Estimating the Dimension of a Model . ” Annals of Statistics 6 : 461 - 4 . Spiegelhalter , David J . , Nicola G . Best , Bradley P . Carlin , and Angelita van der Linde . 2002 . “BayesianMeasuresofModelComplexityandFit . ” JournaloftheRoyalStatisticalSociety , Series B 64 : 1 - 34 . Sugiura , Nariaki . 1978 . “Further Analysis of the Data by Akaike’s Information Criterion and the Finite Corrections . ” Communications in Statistics , Theory and Methods A7 : 13 - 26 . Takeuchi , Kei . 1976 . “Distribution of Informational Statistics and a Criterion of Model Fitting” ( in Japanese ) . Suri - Kagaku ( Mathematic Sciences ) 153 : 12 - 18 . Wasserman , Larry . 2000 . “Bayesian Model Selection and Model Averaging . ” Journal of Mathematical Psychology 44 : 92 - 107 . 304 SOCIOLOGICAL METHODS & RESEARCH Weakliem , David L . 1999 . “A Critique of the Bayesian Information Criterion for Model Selection . ” Sociological Methods & Research 27 : 359 - 97 . Williams , David . 2001 . Weighing the Odds : A Course in Probability and Statistics . Cambridge , UK : Cambridge University Press . Kenneth P . Burnham and David R . Anderson work at the Colorado Cooperative Fish and Wildlife Research Unit at Colorado State University in Fort Collins . They are employed by the U . S . Geological Survey , Division of Biological Resources ; they have graduate faculty appointments in the Department of Fishery and Wildlife Biology and teach a variety of quantitative graduate courses . They have worked closely together since 1973 , where they met and worked together at the Patuxent Wildlife Research Center in Maryland . Much of their joint work has been in the general areas of capture - recapture and distance sampling theory . Their interest in model selection arose during researchontheopenpopulationcapture - recapturemodelsintheearly1990s . Theyhave jointlypublished10booksandresearchmonographsand71journalpapersonavarietyofscientiﬁcissues . Most relevant here is their book Model Selection and Multimodel Inference : A Practical Information - Theoretic Approach ( Springer - Verlag , 2002 ) . Kenneth P . Burnham is a statistician and has more than 32 years ( post Ph . D . ) of expe - rience developing and applying statistical theory in several areas of the life sciences , especially ecology , ﬁsheries , and wildlife . He is a Fellow of the American Statistical Association ( 1990 ) . David R . Anderson is a theoretical ecologist and has more than 36 years working at the interface between biology and statistics . He received the Meritorious Service Award from the U . S . Department of the Interior and was awarded a senior scientist position in 1999 .