http : / / hfs . sagepub . com / Ergonomics Society of the Human Factors and Human Factors : The Journal http : / / hfs . sagepub . com / content / 50 / 5 / 745 The online version of this article can be found at : DOI : 10 . 1518 / 001872008X354183 2008 50 : 745 Human Factors : The Journal of the Human Factors and Ergonomics Society Maia B . Cook and Harvey S . Smallman From Graphical Evidence Landscapes Human Factors of the Confirmation Bias in Intelligence Analysis : Decision Support Published by : http : / / www . sagepublications . com On behalf of : Human Factors and Ergonomics Society can be found at : SocietyHuman Factors : The Journal of the Human Factors and Ergonomics Additional services and information for http : / / hfs . sagepub . com / cgi / alerts Email Alerts : http : / / hfs . sagepub . com / subscriptions Subscriptions : http : / / www . sagepub . com / journalsReprints . nav Reprints : http : / / www . sagepub . com / journalsPermissions . nav Permissions : http : / / hfs . sagepub . com / content / 50 / 5 / 745 . refs . html Citations : What is This ? - Oct 1 , 2008 Version of Record > > at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from INTRODUCTION In military and medical decision making , key tasks include analyzing , making sense of , and carefully weighing ambiguous and conflicting evidence to form diagnoses and make informed decisions . For example , in military intelligence analysis , analysts work under time pressure to piece together ambiguous information from a variety of different sources to come up with bal - anced , reasoned recommendations ( e . g . , Hutchins , Pirolli , & Card , 2004 ; Patterson , Woods , Tinapple , & Roth , 2001 ) . Alack of objectivity and a failure to consider the full evidence spectrum can result in diagnostic errors in medical decision making ( e . g . , Croskerry , 2003 ) ; similarly , military history is replete with examples of the dire consequences of proceeding with faulty intelligence . One key concern in decision making is the con - ﬁrmation bias . As deﬁned by Wason ( 1960 ) , a re - searcher largely responsible for systematizing its study , the conﬁrmation bias is the tendency to seek out information that supports a position while ignoring or minimizing inconsistent information . Wason ( 1960 , 1968 ) found that when attempting to discover or test a rule , people typically generate instances that are consistent , rather than incon - sistent , with the hypothesized rule , apparently Human Factors of the Conﬁrmation Bias in Intelligence Analysis : Decision Support From Graphical Evidence Landscapes Maia B . Cook and Harvey S . Smallman , Paciﬁc Science & Engineering Group , San Diego , California Objective : This study addresses the human factors challenge of designing and vali - dating decision support to promote less biased intelligence analysis . Background : The conﬁrmation bias can compromise objectivity in ambiguous medical and military decision making through neglect of conﬂicting evidence and judgments not reﬂective of the entire evidence spectrum . Previous debiasing approaches have had mixed suc - cess and have tended to place additional demands on users’decision making . Method : Two new debiasing interventions that help analysts picture the full spectrum of evidence , the relation of evidence to a hypothesis , and other analysts’evidence assessments were manipulated in a repeated - measures design : ( a ) an integrated graphical evidence lay - out , compared with a text baseline ; and ( b ) evidence tagged with other analysts’assess - ments , compared with participants’ own assessments . Twenty - seven naval trainee analysts and reservists assessed , selected , and prioritized evidence in analysis vignettes carefully constructed to have balanced supporting and conﬂicting evidence sets . Bias was measured for all three evidence analysis steps . Results : Abias to select a skewed distribution of conﬁrming evidence occurred across conditions . However , graphical evidence layout , but not other analysts’assessments , signiﬁcantly reduced this selec - tion bias , resulting in more balanced evidence selection . Participants systematically prioritized the most supportive evidence as most important . Conclusion : Domain ex - perts exhibited conﬁrmation bias in a realistic intelligence analysis task and apparently conﬂated evidence supportiveness with importance . Graphical evidence layout pro - moted more balanced and less biased evidence selection . Application : Results have application to real - world decision making , implications for basic decision theory , and lessons for how shrewd visualization can help reduce bias . Address correspondence to Maia B . Cook , Pacific Science & Engineering Group , 9180 Brown Deer Rd . , San Diego , CA 92121 ; MaiaCook @ pacific - science . com . H UMAN F ACTORS , Vol . 50 , No . 5 , October 2008 , pp . 745 – 754 . DOI 10 . 1518 / 001872008X354183 . Copyright © 2008 , Human Factors and Ergonomics Society . at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from 746 October 2008 – Human Factors avoiding testing hypotheses in a way that discon - ﬁrms them . In the immense and shifting literature on hu - man decision making , the conﬁrmation bias has received wide attention ( see Nickerson , 1998 , for a review of over 300 papers ) , in part because of Wason’s ( 1960 ) inﬂuential early characterization . Explanations for the conﬁrmation bias have tended to mirror the prevailing views of decision mak - ing , which have shifted over time ( see McKenzie , 2005 , for review ) but fall roughly into one of two classes . The ﬁrst class focuses on psychological pro - cesses inside the head underlying judgment . Early views maintained that the conﬁrmation bias arises from an initial anchoring that leads to biased sam - pling and , ultimately , biased decision making . These views trace back to Francis Bacon’s famous observation that “the ﬁrst conclusion colors and brings into conformity with itself all that come after” ( Bacon , 1620 / 1939 , p . 36 ) . Echoes of Bacon’s ( 1620 / 1939 ) account appear in Herb Simon’s notion of bounded rationality , which views anchoring as a necessary by - product of a recognition - driven limited cognitive archi - tecture that leaves the system prone to bias ( e . g . , Simon , 1955 ) . Also recognizing cognitive limita - tions was the famous work enumerating mental shortcuts , or heuristics , used to simplify decision - making task environments ( Tversky & Kahneman , 1973 , 1974 ) . These heuristics can lead to biased and flawed decision making when applied over - generously . Yet other recent psychological ac - counts see the confirmation bias as a desire to reduce dissonance caused by the decision itself , resulting in a selective search for conﬁrming evi - dence to avoid possible postdecision “cleanup” ( e . g . , Frey & Rosch , 1984 ) . The second class of theories stresses the role of the task environment outside the decision maker’s head . Here , heuristics and biases have been rerendered as environmental adaptations rather than imperfections . Early normative mod - els of how people “should” make decisions were based on environmental statistical optimization and ( primarily ) Bayesian probability theory ( e . g . , Peterson & Beach , 1967 ) . These models recog - nized bias as only a deviation from optimality , rather than a behavior to model . Recent accounts recognize bias as worthy of modeling and reinterpret it as appropriate , given the task environment ( e . g . , Klayman & Ha , 1987 ; McKenzie & Mikkelsen , 2000 ) . On this reinter - pretation , the positive test strategy of investigating conﬁrming evidence is actually adaptive and diag - nostic , especially when supportive evidence is rare . Attractive as these explanations are for modeling performance in Wason - like lab tasks , within the applied decision - making domain of intelligence analysis , a selective search through only conﬁrm - ing evidence will not be diagnostic . Intelligence hypotheses generally reﬂect possible world states that must be inferred from evidence that is at best only probabilistically and indirectly related to the veracity of hypotheses . For example , evidence , however rare it is , may be consistent with and hence “support” two different world states with - out discriminating between them . Given the extent of the problem and ample evidence that it exists , the applied literature on mitigating the conﬁrmation bias is rather small . Two general debiasing approaches have been taken that roughly align with the classes of explanations reviewed previously . The ﬁrst approach modiﬁes the decision maker’s cognitive processes to ﬁt the task , using methods such as educating and train - ing about biases , tutoring immediately before crit - ical decisions , encouraging critical thinking of contrary hypotheses and evidence , promoting self - challenging during assessment , and reminding about biases . These methods tend to be mentally demanding and to rely on memory , and they have had mixed success ( for a review , see Larrick , 2004 ) . The second debiasing approach focuses on modifying the environment to fit the decision maker’s cognitive processes ( for reviews , see Arnott , 2006 ; Klayman & Brown , 1993 ) . These methods have had some success through recasting evidence to accommodate a positive test strategy of seeking only for cases that match hypotheses ( e . g . , Tweney et al . , 1980 ) . However , as previously noted , a positive test strategy has limited diagnos - tic power , especially in the case of intelligence analysis . Also , recasting evidence does not ad - dress the difﬁculties of assimilating the signiﬁcant amount of textual evidence often dealt with by analysts . In the current experiment , we addressed the hu - man factors challenge to reduce bias by informing without distracting or burdening the user and by ensuring a design supportive of tasks that users perform in their natural environment . We con - fronted this challenge within the domain of intel - ligence analysis , evaluating visualizations that at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from D ECISION B IAS IN I NTELLIGENCE A NALYSIS 747 are part of a new collaborative analysis support system called Joint Intelligence Graphical Situa - tion Awareness Web ( JIGSAW ; Smallman , 2008 ; see Figure 1 ) . JIGSAW takes an environment - focused ap - proach to reducing bias , using a novel visual evidence layout that exploits users’ reliance on availability ( Tversky & Kahneman , 1973 ) and recognition ( e . g . , Goldstein & Gigerenzer , 2002 ) . This evidence layout makes the entire spectrum of evidence available and easy to assimilate , so that evidence across the spectrum , not just sup - porting evidence , stands a greater chance of being incorporated into a decision . This approach aims to reduce the neglect of conﬂicting evidence that users may be especially prone to when dealing with large amounts of evidence . Further , the spa - tial layout gives users an integrated picture of the key factors influencing judgment in an analysis space rather than a text list or matrix of flagged items , which may be difﬁcult to integrate . The ex - periment contrasted the debiasing effectiveness of this visual evidence layout with an information - ally equivalent text format . Given that aggregated assessments have been implicated in biased decision making ( e . g . , Janis’s [ 1982 ] Groupthink ) , JIGSAWpreserves and shows different analysts’assessments in an attempt to sup - port effective group decision making ( e . g . , Gigone & Hastie , 1993 ) . JIGSAWis designed to facilitate collaboration by coordinating other analysts’ as - sessments of how evidence relates to a hypothesis , and it puts that collaboration to service by using it to mitigate decision bias in two ways . First , contributing analysts’assessments of how evidence supports or conﬂicts with a hypothesis are shown . This is intended to minimize the poten - tial for only consistent information to be mentioned and shared and to reduce the chances that the re - ceiving analyst underweighs or misinterprets the evidence ( e . g . , Russo , Medvec , & Meloy , 1996 ) . Second , the receiving analyst’s semipublic process of conducting assessments on a shared display is intended to reduce bias that might otherwise occur with private assessment . The current experiment compared the debiasing effect of receiving other analysts’assessments of evidence with assessing evidence in private . These debiasing techniques of visual evidence layout and making other analysts’evidence assess - ments explicit ( illustrated in Figure 1 ) are novel in both design and intent . ( For a recent review of de - biasing techniques and technologies used in deci - sion support systems , see Arnott , 2006 . ) Put in Bacon’s ( 1620 / 1939 ) forthright terms , it should be harder to “set aside and reject” ( p . 36 ) conﬂicting Figure 1 . Analysts collaborating using the Joint Intelligence Graphical Situation Awareness Web ( JIGSAW ) graphical interface ; experimental factors are ( 1 ) the graphical evidence layout and ( 2 ) the other analyst’s assessment of evidence . The background color of the interface transitioned from red to green , from left to right , through gray in the middle . at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from 748 October 2008 – Human Factors evidence that is constantly present , in plain sight , and backed by the adherence of a peer . EXPERIMENT External validity has been raised as the Achil - les’heel of studying decision biases ( Smith & Kida , 1991 ) . Artiﬁcial lab studies forcing naive partic - ipants to make snap decisions have been harshly criticized as a poor proxy for naturalistic , real - world expert decision making ( Cohen , 1993 ) . Here , we took several steps to balance the competing constraints of stimulus control and sensitive bias measurement against external validity concerns . We developed proxy tasks mirroring the initial tasks that analysts perform and used naval trainee intelligence analysts and reservists to approximate real users . We crafted several experimental details both to be consistent with aspects of intelligence analysis and to serve as experimental controls , and created analysis vignettes with sensitive embed - ded bias measurement techniques based on ele - ments of the validated Frey and Rosch ( 1984 ) evidence selection paradigm . In Frey and Rosch’s ( 1984 ) study , participants made a decision about a ﬁctitious scenario ( re - newing a store manager’s contract ) . Afterward , they sought additional evidence from a balanced pool of supportive and conflicting evidence to evaluate their decision . In our experiment , partic - ipants were presented with ﬁctitious intelligence analysis vignettes . They selected evidence from a balanced evidence pool to evaluate intelligence hypotheses that were presented to them . The con - firmation bias was operationalized as a skewed distribution of conﬁrming evidence to investigate further or prioritized as more important , taken from this balanced pool . Frey and Rosch ( 1984 ) measured bias as the number of supporting versus conﬂicting evidence pieces that participants selected from a balanced pool . Our evidence distribution and bias metric differed slightly to more closely reﬂect complex evidence distributions encountered in intelligence analysis and to measure the degree to which the selected evidence reﬂected the entire evidence dis - tribution . Rather than a dichotomous evidence set composed of supporting and conﬂicting evidence , our evidence - hypothesis relationship was rated along a continuum , and evidence sets were bal - anced about the center point of this continuum . Our bias metric was the mean rating of evidence selected to investigate further . This approach al - lowed us to leverage the decision - making literature while honoring the differences between laboratory tasks and those undertaken by analysts . METHOD Participants Aclass of14 trainee analysts ( 12 men , 2 women ) from a U . S . Navy intelligence training facility and 13 U . S . Navy reservists ( 9 men , 4 women ) from a naval command and control facility par - ticipated . Stimuli Four ﬁctitious vignettes were created that were realistic in their topical and strategic nature and similar to the types of problems worked by certain analysts . Vignettes had conﬂicting and ambiguous aspects , attributes routinely encountered in intel - ligence analysis . Each vignette had a hypothesis , relevant back - ground information , and a set of relevant evidence elements . ( Contact the corresponding author for the complete stimulus set . ) Hypothesis referred to a potential world state or explanation to be inves - tigated further by considering related evidence ( Heuer , 1999 ) . For example , one hypothesis stated that a president of a ﬁctional country would resign before an upcoming election . Background information was presented to pro - vide sufﬁcient context for understanding how the evidence elements related to the hypotheses . For the President hypothesis , background information was given on the political parties for the president and the opposing candidate . Evidence elements were phrased as statements that , to varying degrees , supported or conflicted with the related hypothesis . Elements were pre - sented as summary titles ostensibly associated with more detailed information , similar to links in a Web browser . For the President hypothesis , one conﬂicting element reported the withdrawal of an opposing party candidate , leaving little compe - tition for the president in the upcoming election . Evidence often referenced a variety of sources , such as agency scientists , experts , analysts , and spokespersons , to increase vignette realism . Before the experiment , four judges rated the re - lationship between evidence and its corresponding hypothesis using a 7 - point Likert scale ranging from 1 = strongly refutes , through 4 = neutral , to at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from D ECISION B IAS IN I NTELLIGENCE A NALYSIS 749 7 = strongly supports . In an attempt to achieve a balanced evidence set ( rating M = 4 . 0 ) , we recur - sively reworded , and had the judges reassess , the evidence . After several iterations , the resulting evi - dence set averaged 4 . 2 and time constraints pro - hibited any further iteration . Half the evidence was above and half was below the neutral midpoint of the scale , and ﬁnal ratings were averaged and ad - justed slightly so that supporting and conﬂicting evidence were equidistant from the scale’s end points . All judges assessed the same evidence set , and the resulting interrater reliability was high ( intraclass correlation , r 4 = . 97 ) . Creating a set of evidence that related to a hy - pothesis along a continuous scale served two goals . First , it increased realism by allowing evidence of various degrees of support to be shown in a con - trolled way . Second , performance characterized by mean evidence ratings of all selected evidence al - lowed sensitive measurement of the extent of bias and any resulting changes across conditions . Vignettes were presented on a computer display along with paper instructions describing the dis - play’s three interface sections ( see Figure 2 ) . The top section consisted of a horizontal verbal and numeric rating scale for assessing the evidence - hypothesis relationship , the middle section con - tained the hypothesis , and the bottom section listed the eight relevant evidence elements , labeled a through h . Design The study manipulated the effects of two inde - pendent variables – evidence layout format ( graph - ical vs . text ) and source of evidence assessment ( analyst’s own vs . other analysts’assessments ) – in a fully repeated - measures design resulting in four conditions . A graphical region ( JIGSAW’s Intelligence Landscape , or I - Scape ; see Smallman , 2008 ) was presented beneath the rating scale for the graphical vignettes ; see Figure 2 . The I - Scape was a color - coded visual representation mapping the place - ment of evidence according to its rating score . Left - right position and color conveyed the evidence - hypothesis relationship : left and red represented conﬂicting , middle and gray corresponded to neu - tral , and right and green represented supporting . Figure 2 . Experiment interface layout and its three main sections : rating scale ( top ) , hypothesis ( middle ) , and evidence elements ( bottom ) . Graphical ( back ) and text ( front ) conditions for the same president hypothesis are shown . In the Intelligence Landscape ( I - Scape ) , the background color transitioned from red to green , from left to right , through gray in the middle . at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from 750 October 2008 – Human Factors The evidence - hypothesis relationship was rep - resented either by spatial arrangement of labeled symbols on the I - Scape ( graphical format ) or by numeric ratings corresponding to the rating scale next to each labeled evidence element ( text format ) . The evidence - hypothesis relationship was assessed either by other fictitious analysts ( our judges ) , who had ostensibly contributed the evi - dence before the experiment ( others’assessment ) , or by the participants themselves while working the vignettes ( own assessment ) . Adifferent vignette was presented in each con - dition , and condition order and assignment of vi - gnette to condition were counterbalanced across participants . Procedure After informed consent was obtained , the par - ticipants were instructed about the use of the ex - periment interfaces . The rating scales’ use and meaning were explained using a real - world exam - ple of assessing how evidence is related to a hy - pothesis . For each vignette , participants assumed the role of a primary analyst . They were allowed 5 min to complete each of the four vignettes , simulating the time pressure and potentially rapid completion of the initial steps of real analysis . To mimic the initial steps of the analysis pro - cess and afford the opportunity to measure for bias at multiple stages , we had our participants per - form three separate experimental tasks : a . Assessment : Participants assessed the relationship between evidence and hypotheses . For own assess - ment , participants assessed evidence according to how they thought it related to the hypothesis ( by spatial placement of labeled symbols in the graph - ical condition or by numeric evidence ratings in the text condition ) . For others’assessment , participants reviewed evidence already assessed by judges be - fore the experiment ( by reading the evidence and not - ing the graphical placement or text ratings of each element ) . b . Selection : Participants selected the four evidence elements from the full set of eight that they thought , if investigated in more detail , would be most useful in evaluating the hypothesis ( using check boxes ) . c . Prioritization : Participants rank - ordered the four evidence elements they had selected from most to least important for evaluating the hypothesis ( using drop - down menus ) . No difference between graphical and text for - mat was predicted for assessment . The heightened availability of the visually depicted evidence dis - tribution in the graphical versus text format was predicted to reduce bias during selection and pri - oritization . We predicted that evidence ratings would be more supportive in the own versus the others’ assessment condition . Participants were predicted to select evidence more representative of the available evidence distribution and to pri - oritize accordingly in the others’assessment con - dition , as compared with the own assessment condition , because of the simulated social pres - sure from contributing analysts’ assessments . During the experimental session , participants completed an additional problem set that exam - ined a separate experimental question about the impact of the preexisting distribution of conﬁrm - ing versus conﬂicting evidence on these analysis tasks ( details and results reported in Cook & Small - man , 2007 ) . The entire experimental session took approximately 90 min . RESULTS Three embedded dependent variables charac - terized performance for each of the three analysis tasks . Given the balanced evidence ratings , con - ﬁrmation bias would be manifested by participants ( a ) issuing supportive ratings on average for the own assessment condition , ( b ) selecting evidence with ratings averaging supportive and selecting more supportive evidence generally , and ( c ) assign - ing greater priority rankings for evidence with higher ( more supportive ) ratings . Results for each dependent variable are re - ported in turn , organized by task . Assessment Participants and judges did not assess evidence signiﬁcantly differently , both in terms of average ratings ( M = 4 . 2 for both groups ) , F ( 1 , 26 ) = 0 . 05 , p = . 83 , η 2 < . 01 , and spread along the rating scale , F ( 1 , 26 ) = 1 . 09 , p = . 31 , η 2 = . 04 . Evidence layout format did not significantly affect assessment , F ( 1 , 26 ) = 1 . 47 , p = . 24 , η 2 = . 05 . Selection Evidence elements selected as most useful in evaluating hypotheses had ratings that were sig - niﬁcantly more supportive than neutral ( 4 on the rating scale ) in all conditions ( p s < . 001 ) , demon - strating a strong bias to conﬁrm . A Format ( graphi - cal , text ) × Source of Evidence Assessment ( others’ , at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from D ECISION B IAS IN I NTELLIGENCE A NALYSIS 751 own ) repeated - measures ANOVA revealed that ratings of selected evidence were significantly lower ( less supportive ) for graphical ( M = 5 . 0 ) than for text format vignettes ( M = 5 . 3 ) , F ( 1 , 26 ) = 4 . 6 , p < . 05 , η 2 = . 15 . This reduction in ratings for the graphical versus text format is shown in Figure 3 . Bias for selected evidence was not signiﬁcantly reduced for evidence assessed by others ( judges ; M = 5 . 1 ) versus evidence assessed by participants ( M = 5 . 3 ) , F ( 1 , 26 ) = 1 . 57 , p = . 22 , η 2 = . 06 . Because mean evidence ratings represent an aggregate of selected evidence for both polarities , we also examined the percentage of supporting and conﬂicting selected evidence to better charac - terize the pattern of evidence selected . Across all conditions , the conﬁrmation bias was evidenced by an overwhelming majority of supporting evi - dence selected ( only 19 % of selected evidence was conﬂicting ) . However , the amount of conﬂicting evidence selected was suggestively higher for graphical vignettes ( 23 % ) than for text vignettes ( 15 % ) . Additionally , more than twice as much conflicting evidence was selected in the others’ graphical condition ( 25 % ) , an abstracted version of the JIGSAWsystem , as compared with the own text condition ( 11 % ) , the proxy for today’s method of intelligence analysis . Prioritization To investigate differences in evidence ratings according to priority rank , we repeated the For - mat × Source of Evidence Assessment repeated - measures ANOVA used for selection , this time with priority rank ( 1 – 4 ) as a third factor . Assess - ment ratings for selected evidence differed signif - icantly according to the priority rank given to them by participants , F ( 3 , 78 ) = 13 . 3 , p < . 001 , η 2 = . 34 ; see Figure 3 . Post hoc analyses revealed signiﬁ - cantly higher ( more supportive ) ratings for evi - dence ranked as most important ( M = 5 . 7 ) versus second - most important ( M = 5 . 3 ) , t ( 26 ) = 3 . 2 , p < . 01 ( Bonferroni post hoc adjustment ) . Further , there was a significant linear trend for priority rank , F ( 1 , 26 ) = 37 . 2 , p < . 001 , η 2 = . 59 , with more supportive evidence prioritized as more impor - tant . Priority rank did not interact with evidence format , F ( 3 , 78 ) = . 23 , p = . 87 , η 2 = . 01 , or source of evidence assessment , F ( 3 , 78 ) = . 09 , p = . 96 , η 2 < . 01 . Summary To summarize , participants were not biased when assessing evidence , compared with the judges , and assessments did not vary by evidence format . However , a conﬁrmation bias was found for selection in all conditions . This bias was sig - niﬁcantly reduced for graphical vignettes and was unaffected by who had assessed the evidence – the participants or the judges . Finally , the bias per - sisted through prioritization . The most supportive 4 5 6 7 1Most Important 2 3 4 Least Important Priority rank for selected evidence R a t i n g s f o r se l ec t e d ev i d e n ce ( ave r a g e ) _ Graphical Format Text Format S o m e w ha t S uppo r t s N eu t r a l S uppo r t s S t r ong l y S uppo r t s Figure 3 . Mean ratings of selected evidence for graphical and text formats separated by priority rank . at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from 752 October 2008 – Human Factors evidence was prioritized as the most important , with participants apparently conﬂating evidence importance with its degree of support . DISCUSSION The goals for this study were to document the extent of the conﬁrmation bias exhibited by ana - lysts working the initial steps of a problem and to investigate the efﬁcacy of two new bias - reducing techniques developed as part of a collaborative analysis support system called JIGSAW ( Small - man , 2008 ) . These goals were explored through an experiment with actual naval trainee intelligence analysts and reservists ( not naive undergraduates ) working proxy tasks representative of the first steps in analysis . Results revealed no bias at as - sessment but a strong and pervasive conﬁrmation bias for selecting and prioritizing evidence . The first bias - reducing technique , graphical evidence layout , signiﬁcantly reduced but did not eliminate bias . Graphical layout , as compared with text , led participants to choose evidence signif - icantly more representative of the balanced evi - dence pool , an important finding since analysts currently get most of their intelligence in text for - mat . The bias reduction was worth approximately a third of a unit on the 7 - point assessment scale – a small effect with regard to total scale but large in comparison to fine distinctions that subject matter experts have told us matter to them . The bias reduction for the graphical layout was attributable to evidence format and not to leading by instruction . Unlike in debiasing studies that have given hypothesis - testing strategy training dur - ing experimentation ( e . g . , Convertino , Billman , Pirolli , Massar , & Shrager , 2008 ) , participants in the current experiment were neither told about de - cision biases nor given any guidance on strategy . We presume that the graphical format helped be - cause it plays to the recognition - centered nature of decision making by making the full distribu - tion of evidence constantly available and easy to access . There are at least two possible reasons that the second technique , others’assessment , did not re - duce bias . First , all vignettes were canned , single - participant exercises , a sacriﬁce of external validity to maintain experimental control . Assessment by another analyst may simply have not been credi - ble because there were no other live analysts col - laboratively participating and no real attendant social peer pressure . Second , there was no infor - mation about the credibility or expertise of those other analysts , something that analysts in inter - views routinely report as important ( e . g . , Patterson et al . , 2001 ) . The experiment should be replicated truly collaboratively before dismissing others’as - sessment as a debiasing method . Recently , we have begun collaborative in vivo studies of group as - sessment to address these issues ( Risser & Small - man , 2008 ) . An intriguing new ﬁnding was the conﬂation of importance with degree of support when evidence was prioritized in terms of its importance to inves - tigating a hypothesis . This ﬁnding was not an arti - fact of participant instructions : A careful review revealed that importance and degree of support were in no way related in the instruction scripts . On the contrary , the real - world example given dur - ing the instructions stressed the importance of con - sidering both conﬂicting and supporting evidence . Because the overwhelming majority of selected evidence was supportive , the remaining pool of 19 % conﬂicting evidence was insufﬁcient to allow a statistical determination of whether more impor - tance was given to more extremely rated evidence . Future research could examine this issue if it is possible to induce selection and prioritization of a sufﬁcient amount of conﬂicting evidence . The support - importance conﬂation may reveal a mechanism contributing to starting one down a conﬁrming path . However , before the conﬂation is declared as a new heuristic , future research should test whether the conflation generalizes to other decision - making paradigms . Also , it is possible that support was inadvertently reinforced with im - portance by coding greater support with higher numbers . Therefore , control studies should be per - formed to test whether the conﬂation persists when the evidence scale numbers are reversed . However , note that the graphical evidence layout , per se , did not reinforce the conﬂation , as the result was just as strong in the text condition . To this point , the ﬁnding of bias has been pre - sented as maladaptive – something that if removed by “debiasing” would result in improved analysis , generally . We now consider the topical question of whether the behavior observed could have been adaptive by reviewing the task and cost structure of both the environment and the stimuli . First , consider the features of basic hypothesis - testing experiment tasks , such as the classic ones developed by Wason ( 1960 , 1968 ) . Rule certainty at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from D ECISION B IAS IN I NTELLIGENCE A NALYSIS 753 is absolute ( e . g . , if P , then Q ) , and evidence and hypothesis rarity is determinable . Evidence cred - ibility is indisputable , and there is no question of deception . Further , the relationship of evidence to the hypothesis is unambiguous , and evidence di - rectly bears on the veracity of the rule in question ( e . g . , turn over P and not - Q to test the rule ) . Contrast these task features with those of the current experiment and with the intelligence analysis domain more generally . The degree of certainty about intelligence hypotheses , which generally reﬂect complex world states ( e . g . , a ﬁc - tional country is developing nuclear weapons ) , is often not determinable , and evidence and hypoth - esis rarity is unknown . Evidence has variable cred - ibility that may be determined only weakly at times , is subject to interpretation , and often involves deception ( e . g . , complying with United Nations [ U . N . ] weapon inspectors could indicate either that the country is not developing weapons or that it is developing weapons in covert facilities ) . Conver - tino et al . ( 2008 ) put it well : “intelligence analysis must address active intent to deceive in a way that scientiﬁc domains need not” ( p . 361 ) . Evidence of - ten supports and conﬂicts with a hypothesis in ways that are difﬁcult to reconcile ( e . g . , if weapons in - spectors were granted access to one facility but were blocked at a different facility ) . As Heuer ( 1999 ) pointed out , evidence available to intelli - gence analysts may indicate when a hypothesis is unlikely , but generally cannot eliminate a hypoth - esis entirely . Although a positive test strategy may be adap - tive in certain cases , such as an attempt to predict a rare event or when instances are unusual ( Klay - man & Ha , 1987 ) , its utility in intelligence anal - ysis is uncertain . Compared to simplistic basic hypothesis - testing tasks , hypothesis testing in intelligence analysis lacks any of the clear features necessary to make the positive test strategy viable for evaluating and discriminating between hy - potheses . Thus , the behavior we observed was not adaptive given the task environment . We next consider participant expectations and the implied cost structure of the decision environ - ment . Unbiased decision making would have en - tailed consideration of evidence along the entire evidence spectrum and , therefore , equal consid - eration of supporting and conflicting evidence . However , given the participants’ expertise , it is possible that they considered the relative costs of correctly and incorrectly predicting actual world states when performing the experimental tasks . Their evidence selection may have been affected by what they perceived these costs to be . Figure 4 gives a signal detection analogy of hypothesis - testing outcomes of one of our ficti - tious hypotheses . In this example , the implied cost for the miss action ( complacency in the face of a legitimate threat ) may be perceived as higher than that for the false alarm action ( an unjustiﬁed U . N . action against the country ) . However , the relative costs for misses and false alarms were more equiv - ocal for our other experiment hypotheses ( e . g . , the resignation of the fictional country’s presi - dent ) . Thus , across hypotheses , cost structure does not appear to be the primary reason for the conﬁr - mation bias in selection . Given how germane these issues are to intelligence hypotheses , though , fu - ture research should manipulate the perceived costs of positive and negative hypothesis out - comes to disentangle their impact on the conﬁr - mation bias . In this study , we addressed the human factors challenge of attempting to reduce the conﬁrmation bias in intelligence analysis , and the results were revealing from basic and applied perspectives . On the basic side , new methods for reducing bias that Hypothesis about Country X ( possible world state ) Pursuing nuclear weapons Not pursuing nuclear weapons Hit Miss Pursuing nuclear weapons Justified U . N . action to address legitimate threat Complacency in the face of legitimate threat False alarm Correct rejection Actual world state Not pursuing nuclear weapons Unjustified U . N . action against Country X Appropriate lack of action against Country X Figure 4 . Signal detection analogy for hypothesis - testing outcomes for a ﬁctional hypothesis . Example actions , with implied costs , for each actual and possible world state combination are italicized . at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from 754 October 2008 – Human Factors respect the recognition - driven nature of decision making have been investigated and a possible new heuristic has been identiﬁed . On the applied side , graphical evidence layout reduced bias , support - ing its value in decision support for intelligence analysis . The graphical format itself was a mod - ification of the task environment rather than an approach that burdens the decision maker with remembering and being wary of bias . In the future , graphical evidence layouts could be extended to accommodate multiple competing hypotheses . Additionally , several potentially pow - erful factors could be integrated with graphical evi - dence layouts and evaluated for their effectiveness in debiasing , such as contributors’ credibility , evidence rarity and diagnosticity , and the implied cost structure of the way hypotheses are framed . ACKNOWLEDGMENTS The authors thank Gene Averett , Chiesha Stevens , and Tiffany Adams of Paciﬁc Science & Engineering Group for technical assistance and two anonymous reviewers for their constructive comments that helped to strengthen the paper . This research was sponsored by the Ofﬁce of Naval Re - search , Program Manager Dr . Michael P . Letsky . Any opinions , ﬁndings , conclusions , or recommen - dations expressed herein are those of the authors and do not necessarily reﬂect the views of the De - partment of Defense . REFERENCES Arnott , D . ( 2006 ) . Cognitive biases and decision support systems devel - opment : Adesign science approach . Information Systems Journal , 16 , 55 – 78 . Bacon , F . ( 1939 ) . Novum organum . In E . A . Burtt ( Ed . ) , The English philosophers from Bacon to Mill ( pp . 24 – 123 ) . New York : Random House . ( Original work published 1620 ) Cohen , M . S . ( 1993 ) . The naturalistic basis of decision biases . In G . A . Klein , J . Orasanu , R . Calderwood , & C . E . Zsambok ( Eds . ) , Decision making in action : Models and methods ( pp . 51 – 99 ) . Norwood , NJ : Ablex . Convertino , G . , Billman , D . , Pirolli , P . , Massar , J . P . , & Shrager , J . ( 2008 ) . The CACHE study : Group effects in computer - supported collabo - rative analysis . Computer Supported Cooperative Work , 17 , 353 – 393 . Cook , M . B . , & Smallman , H . S . ( 2007 ) . Visual evidence landscapes : Reducing bias in collaborative intelligence analysis . In Proceedings of the Human Factors and Ergonomics Society51st Annual Meeting ( pp . 303 – 307 ) . Santa Monica , CA : Human Factors and Ergonomics Society . Croskerry , P . ( 2003 ) . The importance of cognitive errors in diagnosis and strategies to minimize them . Academic Medicine , 78 , 775 – 780 . Frey , D . , & Rosch , M . ( 1984 ) . Information seeking after decisions : The roles of novelty of information and decision reversibility . Personality and Social Psychology Bulletin , 10 , 91 – 98 . Gigone , D . , & Hastie , R . ( 1993 ) . The common knowledge effect : Infor - mation sharing and group judgment . Journal of Personality and Social Psychology , 65 , 959 – 974 . Goldstein , D . G . , & Gigerenzer , G . ( 2002 ) . Models of ecological ratio - nality : The recognition heuristic . Psychological Review , 109 , 75 – 90 . Heuer , R . J . , Jr . ( 1999 ) . The psychology of intelligence analysis . Washing - ton , DC : Center for the Study of Intelligence . Hutchins , S . G . , Pirolli , P . L . , & Card , S . K . ( 2004 , June ) . Anew perspec - tive on use of the critical decision method with intelligence analysts . Paper presented at the Ninth International Command and Control Research Symposium ( ICCTRS ) , San Diego , CA . Janis , I . L . ( 1982 ) . Groupthink ( 2nd rev . ed . ) . Boston : Houghton Mifﬂin . Klayman , J . , & Brown , K . ( 1993 ) . Debias the environment instead of the judge : An alternative approach to reducing error in diagnostic ( and other ) judgment . Cognition , 49 , 97 – 122 . Klayman , J . , & Ha , Y . - W . ( 1987 ) . Conﬁrmation , disconﬁrmation , and information in hypothesis testing . Psychological Review , 94 , 211 – 228 . Larrick , R . P . ( 2004 ) . Debiasing . In D . J . Koehler & N . Harvey ( Eds . ) , Blackwell handbook of judgment and decision making ( pp . 316 – 337 ) . Oxford , UK : Blackwell . McKenzie , C . R . M . ( 2005 ) . Judgment and decision making . In K . Lamberts & R . L . Goldstone ( Eds . ) , Handbook of cognition ( pp . 321 – 338 ) . London : Sage . McKenzie , C . R . M . , & Mikkelsen , L . A . ( 2000 ) . The psychological side of Hempel’s paradox of conﬁrmation . Psychonomic Bulletin and Review , 7 , 360 – 366 . Nickerson , R . S . ( 1998 ) . Conﬁrmation bias : Aubiquitous phenomenon in many guises . Review of General Psychology , 2 , 175 – 220 . Patterson , E . S . , Woods , D . D . , Tinapple , D . , & Roth , E . M . ( 2001 ) . Using cognitive task analysis ( CTA ) to seed design concepts for intelligence analysts under data overload . In Proceedings of the Human Factors and Ergonomics Society45th Annual Meeting ( pp . 439 – 443 ) . Santa Monica , CA : Human Factors and Ergonomics Society . Peterson , C . R . , & Beach , L . R . ( 1967 ) . Man as an intuitive statistician . Psychological Bulletin , 68 , 29 – 46 . Risser , M . R . , & Smallman , H . S . ( 2008 , June ) . Networked collaborative intelligence assessment . Paper presented at the 13th International Command and Control Research Symposium ( ICCTRS ) , Belle - vue , WA . Russo , J . E . , Medvec , V . H . , & Meloy , M . G . ( 1996 ) . The distortion of information during decisions . Organizational Behavior and Human Decision Processes , 66 , 102 – 110 . Simon , H . A . ( 1955 ) . Abehavioral model of rational choice . Quarterly Journal of Economics , 69 , 99 – 118 . Smallman , H . S . ( 2008 ) . JIGSAW – Joint Intelligence Graphical Sit - uation Awareness Web for collaborative intelligence analysis . In M . P . Letsky , N . Warner , S . Fiore , & C . A . P . Smith ( Eds . ) , Macrocog - nition in teams : Theories and methodologies ( pp . 321 – 337 ) . Alder - shot , UK : Ashgate . Smith , J . F . , & Kida , T . ( 1991 ) . Heuristics and biases : Expertise and task realism in auditing . Psychological Bulletin , 109 , 472 – 489 . Tversky , A . , & Kahneman , D . ( 1973 ) . Availability : Aheuristic for judg - ing frequency and probability . Cognitive Psychology , 5 , 207 – 232 . Tversky , A . , & Kahneman , D . ( 1974 ) . Judgment under uncertainty : Heuristics and biases . Science , 185 , 1124 – 1131 . Tweney , R . D . , Doherty , M . E . , Worner , W . J . , Pliske , D . B . , Mynatt , C . R . , Gross , K . A . , et al . ( 1980 ) . Strategies of rule discovery in an inference task . Quarterly Journal of Experimental Psychology , 32 , 109 – 123 . Wason , P . C . ( 1960 ) . On the failure to eliminate hypotheses in a conceptual task . Quarterly Journal of Experimental Psychology , 12 , 129 – 140 . Wason , P . C . ( 1968 ) . Reasoning about a rule . Quarterly Journal of Experimental Psychology , 20 , 273 – 281 . Maia B . Cook is a research scientist at Paciﬁc Science & Engineering Group , Inc . She received her Ph . D . in cog - nitive science in 2005 from the University of California , Irvine . Harvey S . Smallman is a senior scientist at Paciﬁc Sci - ence & Engineering Group , Inc . He received his Ph . D . in experimental psychology in 1993 from the University of California , San Diego . Date received : February 1 , 2008 Date accepted : September 26 , 2008 at St Petersburg State University on January 3 , 2014 hfs . sagepub . com Downloaded from