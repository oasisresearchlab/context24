Zelda : Video Analytics using Vision - Language Models Francisco Romero ∗ Stanford University faromero @ stanford . edu Caleb Winston ∗ Stanford University calebwin @ stanford . edu Johann Hauswald Stanford University johannh @ stanford . edu Matei Zaharia Stanford University matei @ cs . stanford . edu Christos Kozyrakis Stanford University christos @ cs . stanford . edu ABSTRACT Advances in ML have motivated the design of video analytics sys - tems that allow for structured queries over video datasets . However , existing systems limit query expressivity , require users to specify an ML model per predicate , rely on complex optimizations that trade off accuracy for performance , and return large amounts of redundant and low - quality results . This paper focuses on the re - cently developed Vision - Language Models ( VLMs ) that allow users to query images using natural language like “cars during daytime at traffic intersections . ” Through an in - depth analysis , we show VLMs address three limitations of current video analytics systems : general expressivity , a single general purpose model to query many predicates , and are both simple and fast . However , VLMs still re - turn large numbers of redundant and low - quality results , which can overwhelm and burden users . We present Zelda : a video analytics system that uses VLMs to return both relevant and semantically diverse results for top - K queries on large video datasets . Zelda prompts the VLM with the user’s query in natural language and additional terms to improve accuracy and identify low - quality frames . Zelda improves result diversity by leveraging the rich semantic information encoded in VLM embeddings . We evaluate Zelda across five datasets and 19 queries and quantitatively show it achieves higher mean average precision ( up to 1 . 15 × ) and improves average pairwise similarity ( up to 1 . 16 × ) compared to using VLMs out - of - the - box . We also compare Zelda to a state - of - the - art video analytics engine and show that Zelda retrieves results 7 . 5 × ( up to 10 . 4 × ) faster for the same accuracy and frame diversity . PVLDB Reference Format : Francisco Romero , Caleb Winston , Johann Hauswald , Matei Zaharia , and Christos Kozyrakis . Zelda : Video Analytics using Vision - Language Models . PVLDB , XX ( XX ) : XXX - XXX , XXXX . doi : XX . XX / XXXXX . XXXXX ∗ Denotes equal contribution . This work is licensed under the Creative Commons BY - NC - ND 4 . 0 International License . Visit https : / / creativecommons . org / licenses / by - nc - nd / 4 . 0 / to view a copy of this license . For any use beyond those covered by this license , obtain permission by emailing info @ vldb . org . Copyright is held by the owner / author ( s ) . Publication rights licensed to the VLDB Endowment . Proceedings of the VLDB Endowment , Vol . XX , No . XX ISSN XXXX - XXXX . doi : XX . XX / XXXXX . XXXXX Current Systems Z ELDA Model1 = “car” AND Model2 = “daytime” AND camID LIKE “intersection % ” VLM = “cars during daytime at traffic intersections” Q u e r y R es u l t s ( a ) ( b ) Figure 1 : Current systems ( a ) limit predicate expressivity , require user expertise for model selection , and are complex to use . Our sys - temZelda ( b ) allowsexpressivequeryinginnaturallanguageusing a single fast model while returning relevant and diverse results . 1 INTRODUCTION Video analytics systems have become a topic of significant interest due to the availability of vast video collections and the recent ad - vances in ML . Systems like VIVA [ 24 ] , EVA [ 58 ] , BlazeIt [ 21 ] , and others [ 6 , 22 , 27 ] allow users to query videos for objects , individuals , actions , or complex scenes by specifying a list of predicates that ap - ply ML models on video frames . However , as we show in Figure 1a , today’s video analytics systems are limited in the following ways : • Limited Expressivity : Most existing video analytics systems limit query predicates to the ML model’s trained classes which rarely exceed 1000 classes [ 13 , 31 , 45 ] . Some systems like VO - CAL [ 8 , 9 ] and SeeSaw [ 36 ] require human - in - the - loop anno - tations to fine - tune models on new classes . A user is unable to query for “convertible” if the model is only trained on “cars” un - less they augment the model with “convertible” training data or approximately map cars to convertibles , which is error - prone . • Multiple Models Required : Existing video analytics systems require users to match query predicates to models . Users have to train a new model if none exist for their desired predicate ( s ) . In Figure 1a , a user must specify the models to use for finding “car” and “daytime” . Then , they need to reason about whether to use metadata like the camera location or train an additional model to filter by traffic intersections . • Complex and Slow : Systems like BlazeIt , NoScope , VIVA , and FiGO trade off performance for accuracy by using model variants . Other systems like Everest [ 27 ] and Probabilistic Predicates [ 34 ] train query - specific proxy models on the critical path of a query . A system’s query optimizer may choose to use a faster but less accurate car classifier , or train a query - specific binary classifier for daytime instead of using a larger model . These techniques make current systems significantly more complex . a r X i v : 2305 . 03785v1 [ c s . D B ] 5 M a y 2023 • Redundant and Low - Quality Results : When querying large video datasets , a large number of frames may match the user’s query . Many of these results will look visually similar and lack di - versity . Existing systems will produce outputs similar to Figure 1a , where the same car at different timesteps is ranked highest based on the model’s confidence . These results may also contain low - quality blurry results . Needing to manually remove low - quality and redundant results unnecessarily burdens users . Vision - Language Models ( VLMs ) are a new class of ML models for tasks such as image classification ( CLIP [ 43 ] ) , object detection ( De - tic [ 65 ] and ViLD [ 12 ] ) , and video understanding ( VideoCLIP [ 57 ] ) . VLMs jointly represent natural language text ( known as prompts ) and images in the same embedding space . The cosine distance of prompt and image embeddings represents how semantically sim - ilar they are [ 16 , 38 ] . The general architecture of VLMs is shown in Figure 2 . These models are pretrained on large datasets of cap - tioned images with the goal of maximizing the similarity between text - image pairs that are semantically related , while minimizing the similarity of non - corresponding pairs [ 49 ] . The resulting em - beddings have been shown to encode rich and complex semantic information about images [ 12 , 16 , 43 , 65 ] . In this paper , we advocate for using VLMs to address the limitations of existing video analytics systems . Specifically , we perform an in - depth analysis across a wide range of datasets and queries to demonstrate VLMs help to address the first three challenges : • General Expressivity : Users can express a wide range of predi - cates because VLMs understand natural language . For example , “car” and “convertible” are close in embedding space allowing a VLM to retrieve semantically similar frames . • A General Purpose Model : VLMs are zero - shot : they do not have a pre - configured set of classes making them applicable for a wide range of queries . VLMs can be used out - of - the - box as a general purpose model with high accuracy or alongside special - purpose models ( e . g . , find specific car types ) . • Simple and Fast : Using a single type of model reduces system complexity and simplifies a data analyst’s workflow . This elimi - nates the need to manage model variants or employ additional logic to train query - specific models . Some VLMs are fast ( 1000s of frames per second on an NVIDIA T4 GPU [ 37 ] ) since they only require embedding comparisons . However , VLMs do not address the fourth challenge : current sys - tems overwhelm users with large numbers of frames that may be too similar or low - quality . Recent efforts to address this problem are insufficient . Systems like NoScope use pixel - wise difference detectors to diversify results . However , such detectors require per - query tuning which is error - prone , and only use pixel differences instead of richer semantic information . Systems like VOCALEx - plore require users to manually identify low - quality frames . This is burdensome to users , especially since it must be repeated for every new video dataset they wish to explore . Everest limits results presented to users but does nothing to address diversity or quality of their results . We present Zelda , a system for Z ero - shot E xpressive , re L evant , and D iverse video A nalytics . Zelda uses VLMs to generate relevant and semantically diverse results for top - K queries on large video datasets . As shown in Figure 1b , Zelda takes natural language text as input and generates candidate frames without any user annotations or model fine - tuning . To increase accuracy , we develop the right VLM prompting strategy for video analytics . In addition to the user’s query , Zelda prompts the VLM with a diverse set of labels that act as discriminators and terms that help identify low - quality frames like “blurry , grainy” . Zelda removes low - quality frames by determining when that VLM is more confident that a term like “blurry” matches a frame compared to the user’s query . Zelda then uses the precomputed VLM embeddings to score candidates for their semantic similarity using the diversity term introduced by the Maximal Marginal Relevance ( MMR ) algorithm [ 7 ] . This allows Zelda to prune frames that are too similar while considering their relevance to the query and the number of results requested by the user . Finally , Zelda returns the top - K frames that are both relevant and semantically diverse . We built Zelda on top VIVA [ 44 ] , an open - source query engine for video analytics , using CLIP developed by OpenAI [ 43 ] . We eval - uated Zelda using five video datasets and 19 queries that feature both single and multiple predicates . We show that Zelda’s ranking pipeline has 1 . 16 × better average pairwise similarity ( APS ) and up to 1 . 15 × higher retrieval mean average precision ( MAP ) com - pared to using a VLM out - of - the - box . We also compare Zelda to VIVA [ 44 ] , a state - of - the - art baseline for top - K video analytics , and show that Zelda retrieves results 7 . 5 × ( up to 10 . 4 × ) faster for the same accuracy and frame diversity . In summary we make the following contributions : • We highlight the potential of applying VLMs to video analyt - ics showing their ease of use , high accuracy , and performance benefits across diverse set of queries ( Sections 3 . 1 and 3 . 2 ) . • We identify the limitations inhibiting VLMs from returning out - of - the - box relevant and diverse results ( Section 3 . 3 ) . • We present Zelda , a video analytics system that uses VLMs to enable queries in natural language and automatically produce high quality , relevant , and semantically diverse results ( Section 4 ) . • We quantitatively show that Zelda improves result diversity compared to just using VLMs out - of - the - box . We demonstrate that by leveraging VLMs , Zelda simplifies query optimization and improves performance over existing state - of - the - art video analytics systems ( Section 5 ) . 2 VISION - LANGUAGE MODEL PRIMER A Vision - Language Model ( VLM ) 1 is a type of ML model that is trained on image - text pairs such as an image of Jake Tapper with the caption “Jake Tapper , a news anchor” [ 16 , 50 ] . The model learns from complex scenes and their natural language descriptions to encode visual and semantic information . As shown in Figure 2 , the model is composed of an image encoder and a text encoder . These encoders generate images and text vector embeddings , relying on transformers to capture the sequence - to - sequence nature of both language and vision [ 10 , 56 ] . The goal of the model’s image and text encoders is to learn similar embeddings for similar image - text pairs . The embeddings can be used to estimate similarity between image and text data . The embeddings can also be used for text - text and image - image similarity analysis . VLMs are pretrained on very large datasets of image - text pairs mined from the Internet , which 1 VLMs are also referred to as Image Language Models ( ILMs ) . We use VLM throughout this work without loss of generality . Language Encoder VisionEncoder “Jake Tapper” [ 0 . 9121 , 0 . 32345 , 0 . 65234 , . . . ] [ 0 . 0048 , 0 . 29038 , 0 . 98734 , . . . ] [ 0 . 4878 , 0 . 23784 , 0 . 09982 , . . . ] [ 0 . 3928 , 0 . 62838 , 0 . 10928 , . . . ] 0 . 34 0 . 76 0 . 68 Similarity Vision Encoder “Jake Tapper” [ 0 . 9121 , 0 . 32345 , 0 . 65234 , . . . ] [ 0 . 0048 , 0 . 29038 , 0 . 98734 , . . . ] [ 0 . 4878 , 0 . 23784 , 0 . 09982 , . . . ] [ 0 . 3928 , 0 . 62838 , 0 . 10928 , . . . ] 0 . 34 0 . 76 0 . 68 Similarity L a ngu a g e E n c od e r Frame Embeddings Prompt Embedding Similarity Scores Text Prompt Video Frames Language Encoder VisionEncoder “Jake Tapper” [ 0 . 9121 , 0 . 32345 , 0 . 65234 , . . . ] [ 0 . 0048 , 0 . 29038 , 0 . 98734 , . . . ] [ 0 . 4878 , 0 . 23784 , 0 . 09982 , . . . ] [ 0 . 3928 , 0 . 62838 , 0 . 10928 , . . . ] 0 . 34 0 . 76 0 . 68 Similarity Frame Embeddings Prompt Embedding Similarity Scores Text Prompt Video Frames F1 “Jake Tapper” [ 0 . 3928 , 0 . 62838 , 0 . 10928 , . . . ] 0 . 34 0 . 76 0 . 68 Similarity Frame Embeddings Prompt Embedding Similarity Scores Text Prompt Video Frames F1 Vision Encoder Prompt Encoder E1 E2 P1 Prompt Embedding 15 % match 83 % match F2 F1 “Jake Tapper” Image Encoder Text Encoder 0 . 15 0 . 90 Cosine Similarity Embeddings “Jake Tapper” Vision Encoder Prompt Encoder 15 % match 83 % match Similarity Embeddings Score Figure 2 : General architecture of vision - language models . allows them to be used zero - shot ( i . e . , without fine - tuning ) on a variety of downstream tasks [ 49 ] . VLMs can estimate the similarity of video frames to a given natural language prompt . The image and text encoders generate embeddings representing the video frames and the input prompt , respectively . Prompts often extend a template such as “a photo of { prompt } ” to assimilate a caption [ 1 ] . Multiple prompts may be com - pared to one or more frames . The frame and prompt embeddings are compared using cosine similarity where frame - prompt pairs with highest similarities are most likely to be related , as shown in Figure 2 . Computing frame and prompt embeddings , and their cosine similarities is a fast operation . The cosine similarities use highly optimized vector search libraries like FAISS developed by Meta [ 19 ] or vector search databases like PineconeDB [ 41 ] . In Sec - tion 3 . 2 , we compare the out - of - the - box performance of a VLM to models commonly used in video analytics . CLIP is an example of a VLM that achieves state - of - the - art accu - racy on zero - shot image classification [ 43 ] . CLIP was trained on 400 million image - text pairs mined from the Internet . Other VLMs like ALIGN [ 18 ] and DeCLIP [ 30 ] show similar accuracies to CLIP using noisy or less training data , respectively . VLMs have also been used for object detection [ 12 , 65 ] , visual - question answering and cap - tioning [ 3 ] , image - text retrieval [ 60 ] , and video understanding [ 57 ] . Our work aims to take advantage of the unique characteristics of VLMs as general image and text encoders and will benefit from the evolving research in this field . 3 VLMS FOR VIDEO ANALYTICS In this section , we explore applying VLMs to video analytics . We first show how a user would query videos using a VLM - based system compared to current systems . We then quantitatively and qualitatively evaluate the accuracy and throughput of VLMs com - pared to ML models commonly existing in video analytics systems . Finally , we identify the challenges limiting the use of VLMs for top - K video analytics queries . 3 . 1 Query Workflow We compare the video analytics query workflow using systems like VIVA [ 24 ] and EVA [ 58 ] to a VLM - based system . We consider Fig - ure 1’s query , where an analyst wants to find “cars during daytime at traffic intersections” . Current Systems . Since this query has three predicates — “cars” , “daytime” , and “traffic intersections” — the analyst needs to use three models . Commonly - used object detectors are trained to identify cars and are typically used in existing video analytics systems . However , finding models for “daytime” and “traffic intersections” is more challenging . To detect daytime scenes , the analyst can either label frames with day and night to train a small binary classifier or , if accurate metadata is available , use timestamps to filter by daytime . Similarly , the analyst could either train a model to identify intersections by labeling training data or use camera metadata if they include location information . In Figure 1 , we assume the analyst has a model for daytime and metadata for intersections . This workflow is burdensome and error - prone for the analyst : it often requires manual labeling and model training , or carefully validating available metadata . These issues must be revisited every time the query changes . VLM System . The analyst simply prompts the VLM with the nat - ural language query “cars during daytime at traffic intersections” . The VLM has been trained on many examples of cars , daytime scenes , and traffic intersections so it can find likely matches with - out the need for a specialized model or timestamps . This requires significantly less work from the analyst than what current systems present since there is no need map predicates to models or label data to train new models . The results using this system are shown in Figure 1b . We note the results are visually different because we designed a VLM - based system that removes low - quality frames and considers semantic diversity in the results returned ( further described in Section 4 ) . Key Takeaway – Current systems rely on users’ expert knowledge about what models to use for a given predicate , or when to use other sources of information like camera metadata . In contrast , a VLM - based system allows users to focus on expressing their query in natural language and leaves it to the system to return relevant results . 3 . 2 Out - of - the - box Accuracy and Throughput We evaluate the out - of - the - box accuracy and performance of VLMs with CLIP as our reference VLM . While CLIP has shown impres - sive zero - shot results on a range of image classification tasks [ 43 ] , we compare its accuracy against predicates and models commonly used for video analytics . We consider two scenarios with single and multiple predicates . In current video analytics systems , queries with multiple predicates introduce additional complexity in select - ing the best order for model execution [ 44 , 58 ] . We show a single VLM prompt is just as effective as a query with multiple predi - cates . We consider 3 different tasks : traffic analysis on dashcam footage [ 61 ] , animal recognition on trap camera footage [ 5 ] , and action recognition on a popular action detection dataset [ 52 ] . Single Predicate Accuracy . For the three tasks we study , we se - lect queries where there is an accurate reference model and other queries where there is no such reference model available ( marked NR = No Reference ) . The tasks and predicates studied are summa - rized in Table 1 . An accurate reference model includes the query predicate as one of the classes it was trained on . For traffic anal - ysis and animal recognition , we compare to a slow but accurate “YOLO - accurate” objection detection model [ 55 ] commonly used in video analytics [ 2 , 6 , 21 , 22 , 27 ] . For the action recognition , we Table 1 : Accuracy ( F1 ) of VLMs compared to Reference models ( Ref . ) commonly used in video analytics . For traffic analysis and animal recognition , we use a YOLO object detector . For action recognition , we use SlowFast . NR = No Reference . Task ( # Predicates ) Predicates VLM Ref . Traffic analysis ( 8 ) cars , pedestrian , . . . 0 . 45 0 . 70 Traffic analysis ( NR ) ( 3 ) traffic sign , motorcycle rider 0 . 21 – Animal recognition ( 2 ) dog , cat 0 . 21 0 . 17 Animal recognition ( NR ) ( 6 ) deer , raccoon , . . . 0 . 29 – Action recognition ( 25 ) baseball pitching , archery , . . . 0 . 65 0 . 50 Table 2 : Accuracy ( F1 ) of VLMs when running multiple predicates separately versus as a single predicate . VLMs achieve similar accu - racies without needing to parse predicates . Multiple Predicates F1 Single Predicate F1 cars ∩ night ∩ traffic intersections 0 . 50 cars at night at traffic intersections 0 . 52 pedestrians ∩ street 0 . 21 pedestrianscrossingstreet 0 . 22 car ∪ truck ∪ bus ∪ . . . 1 . 00 vehicle 0 . 88 basketball ∪ fencing ∪ . . . 0 . 72 sports 0 . 77 compare to SlowFast , a state - of - the - art model recently published and used in prior work on video analytics [ 42 , 44 ] . For each task , the VLM is prompted with all the classes of the reference model . For example , YOLO is trained on coco80 [ 31 ] , a set of 80 common classes ( e . g . , “car , truck , person , dog , cat” ) . We evaluate top - 5 F1 ac - curacy 2 computed using ground truth labels and averaged across all queries . For action recognition , we use top - 1 F1 because SlowFast only emits a single label per group of frames . Using the top - 1 or top - 5 F1 score is consistent with prior research in VLMs [ 43 ] . Our results in Table 1 show that CLIP has competitive accuracy with the SlowFast and YOLO models . For two of three tasks where a reference model is available , CLIP outperforms the competitive baselines . YOLO - accurate has better accuracy for traffic analysis , which is expected as it was trained for classes common in traffic videos . When no reference model ( “NR” ) is available ( e . g . , for pred - icates like “motorcycle rider” ) , CLIP is still able to identify some results . We highlight this scenario for cases where an analyst does not have a model available and can still use a VLM to identify some results . These results can either suffice for the analyst or can be used as a starting point to train a model for less common classes . Multiple Predicate Accuracy . Next , we evaluate the accuracy of VLMs on queries containing multiple predicates like “pedestrians next to car at traffic light” . Current systems require either the user or a system like Galois [ 46 ] to split the query into multiple predi - cates where model is executed for each predicate and the merged results are returned to the user . In a VLM - based system , a VLM can be prompted with either multiple predicates like “pedestrian ∩ car ∩ traffic light” where the result is the intersection ( or the union ) of frames that contain each predicate . We take the union or intersection of frames based on the original intent of the query . We compare this to passing the entire query “pedestrians next to car at traffic light” as a single predicate to the VLM . Our results in Table 2 compare the top - 5 F1 VLM accuracy with single and multiple predicates . For queries with multiple predi - cates ( e . g . , “car at night at traffic intersections” ) , parsing the query into separate predicates ( e . g . , “cars ∩ night ∩ traffic intersections” ) does not improve accuracy . For queries like “vehicle” , parsing into multiple predicates for different subordinate terms “car ∪ truck ∪ bus ∪ . . . ” slightly improves accuracy . We generally found a single predicate achieves high accuracy . This further highlights the expressivity VLMs provide since users can submit an entire query without having to reason about breaking down the query into multiple predicates . Qualitative Results . Figure 3 shows the three highest - scoring results for a variety of single and multiple predicate queries . We consider two cases : the predicate is present in the input videos and the predicate is not present in the input videos . These results demonstrate CLIP can distinguish animals , people , and actions like crossing a street . Even for predicates not present in the input video , CLIP still finds close matches using the semantic information en - coded in the embeddings . For single predicates like “Poppy Harlow” , “FedEx truck” , and “Golden Retriever” , CLIP finds people , delivery trucks , and dogs respectively . For multiple predicate queries like “pickup truck in parking garage” , CLIP finds pickup trucks and park - ing garages . A VLM’s ability to find close matches even when the predicate is not present demonstrates the potential of its zero - shot capability for video analytics . Performance . Table 3 compares CLIP’s throughput to models commonly used in video analytics . We take these measurements on an NVIDIA T4 GPU [ 37 ] and measure throughput in frames per second ( FPS ) . All models are preloaded on the GPU and benefit from batching frames to the GPU . For the VLM , the performance includes encoding a single prompt and 10K frames . The cosine similarity is calculated using FAISS , a fast vector search library developed by Meta [ 19 ] . We compare to a fast but less accurate “YOLO - fast” , and the slow but more accurate “YOLO - accurate” object detection model for the traffic analysis and animal recognition tasks . Both models are trained on the same classes and are used for both tasks . For the action recognition task , we compare to SlowFast . CLIP’s throughout is an order of magnitude better than both the SlowFast and YOLO models . We note the YOLO models provide bounding boxes of objects detected while CLIP assigns a single label per frame . Prior work has also investigated making CLIP an object detector by executing it multiple times over subframes [ 12 , 65 ] . 3 . 3 Analyzing Top - K Results Section 3 . 2 showed VLMs out - of - the - box are both accurate and fast . However , VLMs can easily overwhelm an analyst with results that are either redundant or low - quality . Ideally , the analyst would like to review the top most relevant ( answers the query ) and semantically diverse ( variety in the predicate’s context ) results . Hence , we focus on improving the quality of top - K results in a VLM - based video analytics systems . Selecting the top - K results is a popular search functionality across many forms of data [ 17 , 27 , 35 , 54 ] . Figures 1a and 3 show what an analyst can expect using current systems or with VLM out - of - the - box using cosine similarity ranking . We highlight obvious limitations in the top - 3 results of Figure 3 . 2 For F1 accuracy , top - K refers to the order of the label confidences , not the order of the frames returned . deer Jake Tapper S i ng l e P r e d i ca t e M u l t i p l e P r e d i ca t es Predicate in input videos Predicate not in input videos deer raccoon people inside diner cars at night pedestrians crossing street pickup truck in parking garage green Volkswagen Beetle people inside diner wearing orange shirts Golden Retriever Poppy Harlow FedEx truck deer Jake Tapper Single Predicate Multiple Predicates Present Not Present deer raccoon people inside diner cars at night pedestrians crossing street pickup truck in parking garage green Volkswagen Beetle people inside diner wearing orange shirts Golden Retriever Poppy Harlow FedEx truck Predicate in frame Related predicates in frame Predicate not in frame Figure 3 : Top - 3 VLM results for various queries . We study two cases : predicates in input videos and predicates not in input videos . To differ - entiate between “Predicate not in frame” and “Related predicates in frame” , we make a subjective assessment . For example , seeing trucks is useful for “FedEx truck” , while blurry frames for “Golden Retriever” is not . In both cases studied , VLMs return relevant results . Table 3 : Throughput of CLIP on an NVIDIA T4 compared to commonly used video analytics models ( measured with 10K 360p frames ) . CLIP achieves up to 95 . 6 × higher throughput . Tasks Model FPS Traffic analysis Animal recognition YOLO - fast 131 YOLO - accurate 22 Action recognition SlowFast 12 All CLIP 1149 Low - Quality . Low - quality ( e . g . , blurry ) frames can be present in a video dataset after decoding . For the query “Golden Retriever” , the first and third results are blurry and indistinct . The second result shows a dog but is partially occluded and distant . These frames rank highly when using a VLM’s cosine similarity metric but should not be prioritized over results where the predicate is more visible . Duplicate Frames . Videos have high temporal similarity , even when processed at a low frame rate ( e . g . , 1FPS ) . Thus , while having near - duplicate results are not surprising , they are not ideal when processing large video datasets . Identical frames can also be tem - porally distant . For example , a TV channel stream may include a video sequence that is repeated when the same anchor is introduced . For the query “Jake Tapper” , the first and third results are nearly identical visually . The duplicate result should be ranked lower and another frame should be prioritized where the predicate is in a different setting ( e . g . , Jake Tapper at a round table ) . LackofSemanticDiversity . Showingsemanticallydiverseframes is important for interactive queries , where an analyst may gradually refine the query as they see the results [ 58 ] . For the query “cars at night” in Figure 3 , all results are visually distinct but semantically similar since they contain street scenes . While all results are rel - evant , the lack of semantic diversity requires the analyst to scan further to gather the same information . For example , with greater semantic diversity , the top results for this query could show cars at night on streets , highways , or bridges . Key Takeaway – VLMs enable expressive querying using natural language using a single model that is fast and accurate . This drasti - cally reduces model management overheads and query development complexity . However , VLMs do not consistently provide relevant and semantically diverse top - K results out - of - the - box . 4 ZELDA Zelda is a system for Z ero - shot E xpressive , re L evant , and D iverse video A nalytics . It is designed to meet the following requirements : ( 1 ) Retrieve relevant results given a natural language query input . ( 2 ) Prune low - quality and near duplicate frames , retaining a diverse set of candidate results . ( 3 ) Return high confidence results that are semantically diverse . Candidates Frames VLM Text Encoder Image Encoder Step 1 : Candidate Generation Step 2 : Candidate Diversification Label Set InputQuery Softmax ( Cosine Similarity ) Top - K Image Embeddings SortBy ( Confidence ) Step 3 : Top - K Ranking Softmax ( Cosine Similarity ) Quality Descriptors Diversify ( Frames ) Candidates Image Embeddings QualityFilter ( Frames ) Input Query Text Embeddings Figure 4 : Zelda’s architecture : inputs are in green ( light ) , operations in purple ( dark ) , and results in yellow ( circle ) . Dashed boxes are user - provided . ( 1 ) Zelda generates candidate frames by using VLM query scores and a diverse label set , ( 2 ) Zelda improves candidate diversity by removing similar frames and removing low - quality frames , ( 3 ) Zelda ranks the final candidates to produce the top - K results . Existing top - K video analytics systems like Everest [ 27 ] provide probabilistic guarantees for top - K results but do nothing to improve diversity . They are also limited to the classes provided by the model used ( typically an object detector ) . Other video analytics systems like VIVA [ 24 ] , EVA [ 58 ] , and BlazeIt [ 21 ] do not offer support for top - K or diversifying results . None of these systems provide a natural language query input interface . Figure 4 illustrates Zelda’s architecture . Zelda first uses the VLM to generate a candidate set of frames ( Section 4 . 1 ) . Next , Zelda improves result diversity and quality using two techniques : ( a ) fil - tering to remove candidates that are visually and semantically too similar to each other , and ( b ) filtering to remove low - quality frames ( Section 4 . 2 ) . Finally , Zelda ranks the remaining semantically di - verse candidates using the confidence of the VLM to generate the top - K results returned to the user ( Section 4 . 3 ) . We now describe these components in detail . 4 . 1 Candidate Generation Zelda’s first step is to generate candidate frames to be considered for the final top - K results . The user - provided inputs to candidate generation are video frames and a natural language query . As shown in Section 3 . 2 , VLMs can provide accurate results when prompted with just the classes that the user may query and using the cosine distance to select relevant results . However , there are queries where knowing how a VLM scores prompts relative to each other can improve accuracy ( evaluated in Section 5 . 4 ) . For example , for the query “cat” , other animals like a raccoon may look similar . Thus , knowing their relative scores to each other can help to better distinguish between the two . To address this , we pass a large set of labels to Zelda that serve as discriminators . We use the 1 , 203 object categories from LVIS [ 13 ] . LVIS is a recently - proposed dataset for studying instance segmentation algorithms and has been used in recent VLM work for long - tail object recognition [ 12 , 65 ] . We found LVIS categories to be effective additional prompts across the wide range of queries and datasets we studied because they represent every day concepts that were carefully chosen to be distinct . Zelda is not limited to LVIS categories and alternative distinct label sets can be used . We also pass Zelda terms used to identify low - quality data using prompts like “blurry , grainy , low resolution , foggy , sepia” . Zelda prompts the VLM’s text encoder with ( a ) the user’s query , ( b ) the distinct label set classes , and ( c ) the low - quality descriptors . The input frames are fed into the VLM’s image encoder . Next , Zelda computes the cosine similarity between the frame embeddings and the embeddings generated by the text encoder . For example , using LVIS as the distinct label set and five low - quality descriptors , Zelda will compute 1 , 209 cosine similarities per - frame . The cosine similarities are then softmaxed to produce confidence scores . This enables Zelda to compare the scores relative to each other . Zelda then ranks the frames by the softmax confidence score of the query before passing them to the Candidate Diversification stage . Similar to other ranking systems using language models like PLAID [ 47 ] , Zelda does not impose a limit on the number of candidates since they will be pruned in subsequent stages . This also avoids the need for Zelda to set or tune a threshold for how many candidates to generate , which can be challenging when also considering frame similarity and quality . Algorithm 1 Result Diversity Algorithm 1 : procedure DiversifyFrames ( 𝐶𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝐹𝑟𝑎𝑚𝑒𝑠 , 𝑃𝑟𝑢𝑛𝑒𝑇ℎ𝑟𝑒𝑠ℎ ) 2 : 𝑆𝑐𝑜𝑟𝑒𝑑𝐹𝑟𝑎𝑚𝑒𝑠 ← [ ] 3 : for candidate in CandidateFrames do 4 : 𝑀𝑎𝑥𝐶𝑜𝑠𝑆𝑖𝑚 ← 0 5 : for scored in ScoredFrames do 6 : 𝐶𝑜𝑠𝑆𝑖𝑚 ← 𝐶𝑜𝑠𝑖𝑛𝑒𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 ( 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒 , 𝑠𝑐𝑜𝑟𝑒𝑑 ) 7 : if 𝐶𝑜𝑠𝑆𝑖𝑚 > 𝑀𝑎𝑥𝐶𝑜𝑠𝑆𝑖𝑚 then 8 : 𝑀𝑎𝑥𝐶𝑜𝑠𝑆𝑖𝑚 ← 𝐶𝑜𝑠𝑆𝑖𝑚 9 : end if 10 : end for 11 : 𝑆𝑐𝑜𝑟𝑒𝑑𝐹𝑟𝑎𝑚𝑒𝑠 . 𝑎𝑝𝑝𝑒𝑛𝑑 ( ( 𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒 , 𝑀𝑎𝑥𝐶𝑜𝑠𝑆𝑖𝑚 ) ) 12 : end for 13 : return 𝑃𝑟𝑢𝑛𝑒𝐷𝑖𝑣𝑒𝑟𝑠𝑒 ( 𝑆𝑐𝑜𝑟𝑒𝑑𝐹𝑟𝑎𝑚𝑒𝑠 , 𝑃𝑟𝑢𝑛𝑒𝑇ℎ𝑟𝑒𝑠ℎ ) 14 : end procedure 4 . 2 Candidate Diversification Given the candidates produced by the Candidate Generation stage , Zelda next uses two techniques to improve result diversity while maintaining high accuracy : diversity filtering and quality filtering . Improving result diversity . Videos have high temporal similar - ity where neighboring frames may only differ by a small number of pixels . There can also be repeated segments that look visually similar to each other like a TV news opening segment used multi - ple times . Since VLMs ( and ML models generally ) produce similar confidences for frames that are visually similar , there can be several highly - ranked but redundant frames after the Candidate Genera - tion stage . While this leads to high accuracy top - K results , it is not ideal for users looking for a diverse set of query results that can be further refined with their feedback . Zelda improves result diversity using Algorithm 1 . Zelda visits the frames produced by the Candidate Generation stage in descend - ing ranked order ( Line 3 ) . For each frame , we compute pairwise cosine similarities with each of the other previously - scored frames in ScoredFrames . Zelda assigns each frame a semantic similarity score that is its maximum pairwise cosine similarity ( Lines 5 - 11 ) . This step of the algorithm uses the second term ( diversity scoring ) of the Maximum Marginal Relevance ( MMR ) algorithm [ 7 ] . We drop the first term of MMR ( relevance scoring ) because we already have similarity scores generated by the VLM . MMR has been used extensively in information retrieval settings , most notably in text summarization [ 32 ] . The frames are then ranked based on their se - mantic similarity score and pruned based on a system - set threshold that is query agnostic ( Line 13 ) . We empirically found a threshold of 0 . 80 preserves relevant results while filtering those that are too similar to the top confidence ones . We use this threshold for results presented in Section 5 . To avoid over - filtering frames , Zelda en - sures there are at least K frames available , regardless of the diversity score . Existing video analytics systems , like NoScope [ 22 ] and Bog - gart [ 2 ] , use visual similarity to reason about result diversity . These systems build visual difference detectors ( VDDs ) , whose thresh - old must be set per - query . Using visual difference only removes frames whose pixels are nearby to each other . Visual difference is not sufficient to reason about the concepts in frames with respect to user’s query ( e . g . , cars versus trucks ) . Zelda is able to reason about semantic similarity because VLM embeddings are trained such that more semantically similar frames have a higher cosine similarity to each other . We demonstrate that using semantic simi - larity improves the diversity of results in Section 5 . 6 . Zelda can be extended to use additional diversity functions . For example , using the differences in labels assigned between two frames or systems like Everest use object count to rank . Pruning low - quality frames . Video often contains low - quality data like blurry or grainy frames . Systems like VOCAL and VOCAL - Explore [ 8 , 9 ] ask users to manually identify low - quality data so it can be skipped when producing results . Not only is this burdensome on users , but can diminish the quality of the top - K results even if the frame contains the query . As described in Section 4 . 1 , Zelda auto - mates low - quality data removal by additionally prompting the VLM with terms such as “blurry , grainy , low resolution , foggy , sepia” . Each of these terms is separately passed to the VLM’s text encoder . At the end of the Candidate Generation stage , Zelda has produced the probability confidence of these terms relative to the query and the label set by using a softmax layer . During the quality pruning stage , Zelda uses this probability confidence to prune frames that have a higher confidence than the query . Like similarity pruning , Zelda ensures there are at least K frames available to be ranked . 4 . 3 Top - K Ranking After pruning frames to improve result diversity in the Candidate Diversity stage , Zelda’s final step is to produce the top - K ranked frames . To do so , Zelda ranks the remaining frames by the prob - ability confidence of the input query and returns K results . This ensures of the remaining frames , the highest confidence ones scored by the VLM will be shown first . By ranking after diversifying frames , Zelda produces highly accurate , relevant , and diverse results that meet the following requirements : • A diverse set of frames matching the query in the case when multiple high confidence frames are found . For example , when querying for a car in dashcam footage , Zelda will produce dif - ferent types and views of cars . • A diverse set of frames that are semantically similar when few high confidence frames are found . For example , when querying for baseball pitching in an action dataset , Zelda will show match - ing frames while also including frames from related sports like cricket . 4 . 4 Zelda Implementation We implemented Zelda using CLIP as the VLM on top of the VIVA video analytics engine [ 44 ] . VIVA is an open - source system for large - scale video analytics based on Spark [ 62 ] . VIVA optimizes complex query plans to meet user accuracy goals by automatically applying user - specified , domain knowledge about models . In implementing Zelda , we leveraged VIVA’s common model registration interface to add CLIP as the reference VLM . This allowed us to define a user - defined function ( UDF ) that is executed on Spark’s workers . We use VIVA and Spark’s query optimizers out - of - the - box , which take advantage of structured query optimizations for reading data and assigning tasks to workers . After executing the VLM on the input frames , VIVA invokes the Zelda pipeline to select candidates , Table 4 : Datasets , frames , and predicates . We use a mix of single and multiple predicate queries from recent work in video analytics . Dataset Description Frames Predicates BDD [ 61 ] Dashcam footage 1000 vehicle , car , cars at night at traffic intersections , pedestrians crossing street CCT [ 5 ] Camera traps 5000 deer , raccoon Movies [ 40 ] Movie footage 10000 vehicle , people inside diner , car , pedestrians crossing street News [ 4 , 15 ] News interviews 10000 vehicle , Bernie Sanders , car , Jake Tapper UCF101 [ 52 ] Action recognition 4337 sports , baby , cake , baseball pitching , archery identify low - quality data and similar frames , and return the top - K results . The authors of CLIP noted several limitations we also observed in queries using Zelda . This includes lower accuracy for queries that require fine - grained spatial understanding like “VW Golf turn - ing left at night” , counting objects in frames like “frames with 3 cars” , or very specific predicates like car brands [ 43 ] . VLMs are a fast advancing field and we expect their accuracy for such queries will continue improving [ 18 , 29 , 30 , 33 ] . Since Zelda builds on VIVA , it already supports non - VLM predicates , allowing us to use specialized models alongside VLMs . The VLM can quickly and ac - curately identify a diverse set of frames that specialized models can subsequently analyze . For example , after identifying cars in a complex scene described in natural language , an object detector can be used to count cars or keep track of their trajectory . We leave this for future work . 5 EVALUATION Our evaluation aims to answer the following research questions : ( 1 ) Can Zelda be used to produce relevant and semantically diverse top - K results ? ( Section 5 . 2 ) ( 2 ) How does Zelda compare to existing approaches for top - K video analytics ? ( Section 5 . 3 ) ( 3 ) How do Zelda’s different components contribute to producing top - K results ? ( Section 5 . 4 ) ( 4 ) Can Zelda automatically remove low - quality data ? ( Section 5 . 5 ) ( 5 ) How does using CLIP’s semantic embeddings compare to using pixel - wise similarity to diversify results ? ( Section 5 . 6 ) 5 . 1 Setup System Configuration . We deployed Zelda on Google Cloud Platform ( GCP ) . We used a n1 - highmem - 16 instance ( 16 vCPUs , 104 GB of DRAM ) with an NVIDIA T4 GPU . This instance features Intel Xeon E5 - 2699 v4 CPUs operating at 2 . 20GHz , Ubuntu 20 . 04 with 5 . 15 . 0 kernel . We use a pretrained ViT - B / 32 CLIP model [ 43 ] . Queries and Datasets . Table 4 shows the datasets , number of frames , and predicates used to evaluate Zelda . Each predicate is passed as a single prompt to Zelda . BDD [ 61 ] is a dashcam footage dataset used for evaluating models for self - driving vehicles . CCT [ 5 ] is an animal trap dataset used for ecological applications . Movies [ 40 ] is a collection of ∼ 1 hour long films that can be studied for performing actor demographic analysis . TV news data has been used to explore a decade of US cable news [ 4 , 14 ] . UCF101 [ 52 ] is a collection of activities used for action recognition . We constructed a diverse set of queries with both single and multiple predicates drawn from recent work in video analytics [ 6 , 21 – 24 , 34 , 59 ] . Metrics . We use the following metrics in our evaluation : • Retrieval Mean Average Precision ( MAP ) : to measure whether Zelda can return relevant results and rank relevant results ef - fectively , we use retrieval MAP . It is a commonly - used metric in information retrieval for evaluating ranking systems [ 39 , 53 ] . It measures the order that matching results are returned to a user . For a ranked set of 𝐾 frames returned , the average precision ( AP ) is computed by : 𝐴𝑃 = 1 𝑅𝐹 𝐾 ∑︁ 𝑘 = 1 𝑃 ( 𝑘 ) 𝑟 ( 𝑘 ) Where 𝑅𝐹 is the total number of matching frames returned , 𝑃 ( 𝑘 ) is the precision up to 𝑘 frames , 𝑟 ( 𝑘 ) is the relevance of the 𝑘 𝑡ℎ frame ( 1 if the predicate is in the frame , if not 0 ) . We can then compute MAP across all queries in a dataset as : 𝑀𝐴𝑃 = 1 𝑄 𝑄 ∑︁ 𝑞 = 1 𝐴𝑃 ( 𝑞 ) Where 𝐴𝑃 ( 𝑞 ) is the AP for query 𝑞 , and 𝑄 is the number of queries . MAP ranges from 0 to 1 , where a higher MAP indicates relevant results are ranked higher . Zelda maximizes MAP . • Average Pairwise Similarity ( APS ) : to measure how distinct each of the ranked results are from each other , we use APS . This metric is typically used in measuring similarity in data clustering prob - lems [ 11 ] . APS computes the average cosine similarity between pairs of frames returned in the top - K results . APS ranges from 0 to 1 , where a lower APS indicates frames are dissimilar from one another . APS will vary more in video datasets where there is larger visual differences like movies and interviews ( e . g . , many different people and scenes ) . It will have lower variation for datasets with a fixed view ( e . g . , camera trap ) or many similar frames ( e . g . , dashcam footage ) . Zelda minimizes APS . Baselines . We evaluate Zelda against the following baselines . For the VLM - based baselines , neither baseline uses additional label sets or low - quality terms as prompts , consistent with how we analyzed CLIP in Section 3 : • CLIP - Relevant : this baseline ranks frames based on the cosine similarity of the input query embedding and frame embeddings using CLIP out - of - the - box . It represents what a user would expect if all frame embeddings were stored in a vector database and the K - nearest frames to the query prompt were retrieved . This baseline maximizes MAP but does nothing to minimize APS . • CLIP - Diverse : this baseline ranks frames based on the pairwise cosine similarity of frame embeddings produced by CLIP out - of - the - box . It represents what a user would expect if they wanted to retrieve results that were as semantically different as possible from each other . This baseline minimizes APS but does nothing to maximize MAP . • VIVA : this baseline represents state - of - the - art video analytics systems like VIVA , BlazeIt , and Everest . It executes queries with a combination of fast , low accuracy proxy models and slow , more accurate larger models and uses pixel difference for similarity CLIP - Relevant CLIP - Diverse Zelda 5 10 20 K 0 . 0 0 . 5 1 . 0 M AP ( a ) BDD 5 10 20 K 0 . 0 0 . 5 1 . 0 ( b ) CCT 5 10 20 K 0 . 0 0 . 5 1 . 0 ( c ) News 5 10 20 K 0 . 0 0 . 5 1 . 0 ( d ) Movies 5 10 20 K 0 . 0 0 . 5 1 . 0 ( e ) UCF101 Figure 5 : Mean average precision ( MAP ) for Zelda and the baselines . Higher is better . 5 10 20 K 0 . 0 0 . 5 1 . 0 APS ( a ) BDD 5 10 20 K 0 . 0 0 . 5 1 . 0 ( b ) CCT 5 10 20 K 0 . 0 0 . 5 1 . 0 ( c ) News 5 10 20 K 0 . 0 0 . 5 1 . 0 ( d ) Movies 5 10 20 K 0 . 0 0 . 5 1 . 0 ( e ) UCF101 Figure 6 : Average pairwise similarity ( APS ) for Zelda and the baselines . Lower is better . pedestrians crossing street Z e l d a baseball pitching Z e l d a C L I P - D i ve r se C L I P - D i ve r se Z e l d a C L I P - D i ve r se Figure 7 : Top - K results from Zelda compared to CLIP - Diverse for two queries . Zelda produces relevant and more semantically di - verse top - K results compared to VLMs out - of - the - box . analysis . This baseline represents what a user would expect using existing non - VLM based systems . 5 . 2 Producing Diverse , Relevant Ranked Results We first show Zelda is able to produce relevant , diverse top - K results . We compare Zelda to CLIP - Relevant and CLIP - Diverse . CLIP - Relevant optimizes for higher MAP while CLIP - Diverse optimizes for lower APS . Similar to prior work , we consider top - K values of 5 , 10 , and 20 [ 20 , 28 ] . Figures 5 and 6 show the MAP and APS for each of the five datasets , respectively . We generally see CLIP - Relevant has the highest MAP , but also the highest APS . Similarly , CLIP - Diverse generally has the lowest MAP , but also the lowest APS . Zelda provides the best of both baselines : on average equivalent to CLIP - Relevant in MAP , but has better diversity ( 1 . 16 × higher APS on average ) . Compared to CLIP - Diverse , Zelda’s APS is 1 . 09 × higher on average but achieves higher MAP by 1 . 15 × on average . Figures 7 shows results for Zelda and CLIP - Diverse for 2 queries . We see Zelda produces more relevant but still semantically diverse results . For “baseball pitching” , CLIP - Diverse favors diversity which re - turns very few relevant results . Conversely , Zelda finds the right balance automatically . We now discuss how Zelda compares to the two baselines for each dataset in Figures 5 and 6 . BDD . Zelda’s improved MAP over CLIP - Diverse ( 1 . 11 × across all K ) primarily comes from the query for finding pedestrians crossing street , where non - matching frames tend to be empty crosswalks or intersections . CLIP - Relevant ’s reduced accuracy comes from showing these non - matching frames early , and since many look similar it ranks them higher . Despite the majority of frames con - taining cars and vehicles , Zelda still shows a diverse set of results : only 1 . 02 × worse APS across all K compared to CLIP - Diverse and 1 . 15 × better than CLIP - Relevant . CCT . Raccoon and deer are rare animals in CCT and can be difficult to identify especially at night . They can also be mistaken as other an - imals ( e . g . , coyotes or cats ) . Thus , as K increases , the MAP decreases for Zelda and the baselines . For top - 5 results , CLIP - Diverse ’s MAP is the highest since it only shows one deer as the top result . Zelda and CLIP - Relevant show additional deer results , but not all consecutively as the top ranked ones , which slightly decreases their MAP @ 5 . Across all K , Zelda’s MAP is within 1 . 01 × of equivalent to CLIP - Diverse on average . Zelda’s APS is 1 . 11 × better than CLIP - Relevant across all K . News . As shown in Figure 3 , VLMs return relevant results for queries like “Jake Tapper” . However , CLIP - Relevant produces re - sults that are visually similar . Zelda and CLIP - Diverse produce a more diverse set of results by pruning semantically similar frames . Car Cars at night 0 10 20 30 40 50 La t en cy ( s ) 1 . 00 1 . 00 1 . 00 1 . 00 1 . 00 0 . 99 VIVA - Accurate VIVA - Fast Zelda Figure 8 : Zelda compared to VIVA - Accurate and VIVA - Fast . AP @ 20 is annotated above each bar showing almost identical retrieval ac - curacies . Zelda improves performance by up to 10 . 4 × compared to state - of - the - art video analytics systems . BDD CCT Movies News UCF101 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 M A P CLIP - Relevant + Label Set + Diversity Rank BDD CCT Movies News UCF101 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 A P S CLIP - Relevant + Label Set + Diversity Rank Figure 9 : Ablated mean average precision ( MAP ) and average pair - wise similarity ( APS ) for top - 20 results across all datasets . Each of Zelda’s components help it produce diverse , relevant results with high MAP and low APS . For example , both Zelda and CLIP - Diverse show results with polls featuring Bernie Sanders . This improves Zelda’s APS by 1 . 28 × across all K compared to CLIP - Relevant . By ranking more confi - dent results higher after removing similar frames , Zelda improves MAP over CLIP - Diverse by 1 . 12 × across all K . Movies . Similar to BDD , Zelda’s improvements for MAP over both baselines primarily comes from the query for finding pedestrians crossing street . For the cars query , we also observe multiple blurry frames identified as cars and ranked high . By using “blurry” as an additional prompt , Zelda was able to eliminate these , thus improv - ing MAP ( explored further in Section 5 . 5 ) . Even after removing these low - quality frames , Zelda improves APS by 1 . 14 × across all K compared to CLIP - Relevant . UCF101 . Both Zelda and CLIP - Diverse produce more diverse results than CLIP - Relevant ( e . g . , other sports for archery ) . This enables Zelda to improve APS by 1 . 19 × on average compared to CLIP - Relevant . By ranking the most relevant results at the top , Zelda’s MAP is 1 . 41 × higher than CLIP - Diverse across all K . 5 . 3 Performance versus Existing Systems We next show by using VLMs , Zelda improves latency by an order of magnitude over state - of - the - art systems and optimizations in video analytics . We consider a baseline implemented with VIVA that is representative of BlazeIt [ 21 ] and Everest [ 27 ] . In this base - line , queries are executed using a combination of fast , low accuracy proxy models and slow , more accurate models . This baseline aims to use the fast model as much as possible . If the proxy model’s con - fidence is lower than a fixed threshold , it will fall back to the more accurate , slow model . We compare 2 scenarios : ( 1 ) VIVA - Accurate : the threshold is set higher ( 0 . 9 ) , which will call the accurate model more often . ( 2 ) VIVA - Fast : the threshold is set lower ( 0 . 2 ) , which will call the accurate model less often and instead rely primarily on the fast model’s results . We consider a single predicate query — “car” — and a multiple predicate query — “cars at night” . For “car” , the baselines use the same YOLO - fast and YOLO - accurate models from Table 3 to represent a fast , less accurate model and a slow , more accurate model . For “night” , the baselines use an SVM time of day detector previously used by VIVA [ 44 ] . Figure 8 shows the latencies of the baselines compared to Zelda . Annotated above each bar is the AP @ 20 where we see all baselines have almost identical accuracies . As expected , VIVA - Accurate is the slowest because the confidence requirement is high and most frames are labelled by the more accurate model . VIVA - Fast is faster than VIVA - Accurate , but is slower than Zelda . Zelda benefits from using CLIP : a single , fast VLM . On average Zelda is 10 . 4 × faster than VIVA - Accurate and 4 . 7 × faster than VIVA - Fast . We also post - process VIVA - Accurate and VIVA - Fast ’s top - K results with CLIP to compute their APS @ 20 . Both baselines have equivalent APS @ 20 per - query . Since neither baseline has a way to diversify results , Zelda’s APS @ 20 is 1 . 11 × better for the car query and 1 . 02 × better for the cars at night query . 5 . 4 Ablating Zelda We now show how each of Zelda’s different components contribute to producing relevant and semantically diverse top - K results . We show three different ablated versions of Zelda : ( 1 ) CLIP - Relevant : ranked based on cosine similarity of prompt , ( 2 ) + Label Set : prompting with the LVIS label set and data quality descriptors ( “blurry , grainy , low resolution , foggy , sepia” ) , and ( 3 ) + Diversity Rank : ranking results after removing frames that are visually similar . We focus on top - 20 results for all five datasets . Figure 9 shows the MAP and APS for the ablated results for each of the five datasets , respectively . Adding the LVIS label set and data quality prompts ( + Label Set ) improves MAP by 1 . 04 × on average . Label sets also slightly improves APS by 1 . 03 × . As described in Section 4 . 1 , Zelda uses label sets and softmaxes the output to “discriminate” between nearby objects in the embedding space . For example , animals can typically be confused with each other in the CCT dataset . Two nearby animals to raccoon in the LVIS label set are cat and squirrel , which serve as discriminators . This improves the AP @ 20 for raccoon from 0 . 84 to 0 . 94 . By ranking results after removing frames that are similar to each other ( + Diversity Rank ) , Zelda’s MAP slightly decreases compared to + Label Set . However , Zelda’s APS is 1 . 15 × better than + Label Set . Table 5 : Number of blurry frames and average precision in top - 20 results . No dataquality is a version of Zelda that does not include prompts to remove low - quality ( blurry ) frames . Zelda automati - cally removes low - quality results . Query Dataset # blurryframes ( AveragePrecision @ 20 ) CLIP - Relevant CLIP - Diverse No dataquality Zelda cars BDD 1 ( 1 . 00 ) 1 ( 0 . 89 ) 3 ( 1 . 00 ) 0 ( 1 . 00 ) vehicles BDD 1 ( 1 . 00 ) 1 ( 1 . 00 ) 2 ( 1 . 00 ) 0 ( 1 . 00 ) carsatnightattrafficintersections BDD 3 ( 0 . 79 ) 3 ( 0 . 67 ) 3 ( 0 . 63 ) 1 ( 0 . 69 ) cars Movies 16 ( 0 . 75 ) 3 ( 0 . 86 ) 3 ( 0 . 92 ) 2 ( 0 . 96 ) archery UCF101 0 ( 1 . 00 ) 2 ( 0 . 45 ) 3 ( 1 . 00 ) 1 ( 1 . 00 ) baseballpitching UCF101 1 ( 0 . 95 ) 2 ( 0 . 43 ) 2 ( 1 . 00 ) 1 ( 1 . 00 ) B l u rr y f r a m es S e p i a f r a m es Figure 10 : Zelda is able to identify frames prompted with “blurry , sepia” that are consequently pruned . 5 . 5 Removing Low - Quality Data By prompting Zelda with terms like “blurry , grainy , low resolution , foggy , sepia” , we show Zelda can remove low - quality results . This improves ease - of - exploration for users , while maintaining high MAP . We compare Zelda to CLIP - Diverse , CLIP - Relevant , and No dataquality : a version of Zelda that does not include the low - quality terms . We consider queries in the BDD , Movies , and UCF101 datasets where there were at least two blurry frames from No dataquality , and again focus on top - 20 results . Table 5 shows the number of blurry results and the average precision @ 20 ( AP @ 20 ) . Figure 10 shows some example frames that were removed by using the data quality prompts . By prompting Zelda with “blurry , grainy , low resolution , foggy , sepia” , Zelda identifies and reduces the number of low - quality results . The term “blurry” is the most prevalent and has the highest cosine similarity with our datasets . This is expected videos can have rapid cam - era movements . For queries in the BDD dataset , Zelda only in - cludes one low - quality top - 20 result . AP @ 20 is either equivalent or higher than No dataquality and CLIP - Diverse . Compared to CLIP - Relevant , Zelda only has lower AP @ 20 for “cars at night at traffic intersections” since a high - ranked blurry frame is replaced with one that contains cars at night but not in a traffic intersection . For queries in the Movies and UCF101 dataset , Zelda reduces the number of low - quality results . Querying the Movies dataset for “car” with CLIP - Relevant had a majority of low - quality car results which also impacted its AP @ 20 . For the “car” query in the Movies dataset , as noted in Section 5 . 2 , Zelda improves AP by removing high - ranked blurry results ( up to 1 . 28 × ) . For queries in the UCF101 dataset , AP @ 20 is equivalent to No dataquality because blurry frames were low - ranked results of other sports . Figure 10 shows examples of sepia - toned frames Zelda can automatically remove by using the prompt “sepia” . The left frame appears in the top - 20 results of both CLIP - Relevant and CLIP - Diverse when searching for cars in the Movies dataset , while the right frame appears in the results of CLIP - Relevant . We loosely translated other video analytics predicates to prompts that could act as filters and saw mixed results . Estimated object counts ( e . g . , “many , a lot , few , some” ) and relative locations in the image ( e . g . , “right , left , east , west” ) did poorly while colors ( e . g . , “red , blue” ) produced results that were occasionally attributed to the predicate . For example , when prompting for “blue cars” , the VLM would return a mix of blue cars and cars with blue in other parts of the frame ) . As the accuracy of these models increase , we expect the accuracy of prompting VLMs with more specific terms to increase . This will make VLMs even more useful for video analytics . 5 . 6 Comparing Visual and Semantic Similarity Finally , we show by leveraging the semantic understanding of VLM embeddings , Zelda diversifies top - K results and eliminates the need to perform extensive tuning of visual difference detectors ( VDDs ) . As detailed in Section 2 , VLMs are jointly trained with text and images . Their embeddings encode a rich , semantic relationship used for comparing text and images . We consider the case where Zelda’s use of cosine distance of VLM embeddings in the result diversity algorithm ( Algorithm 1 ) is replaced with a VDD . As is done in NoScope , the VDD computes the mean squared error between frames : higher mean squared error ( MSE ) indicates frames are more dissimilar from each other . For this experiment , we empirically set the minimum MSE between frames to be 1 . 5 : higher values lead to an insufficient number of top - 20 frames while lower values lead to redundant frames being selected in the top - 20 . We consider all queries and datasets from Table 4 . Figure 11 shows the MAP ( higher is better ) and APS ( lower is better ) for top - 20 queries for diversifying results using semantic versus visual similarity . By using semantic similarity to improve diversity , Zelda improves APS by 1 . 06 × on average . Given the candidate results , we did not expect a significant difference because pixel - wise similarity already distinguishes between visually distinct frames well . However , this shows that VLMs can further diversify the results using both visual and semantic information about the frames without requiring an additional model or technique because the embeddings are already available . The MAP for the two is close ( 1 . 02 × difference ) since Zelda ranks the remaining frames in its final stage for both cases . The largest improvement comes for the BDD dataset’s queries — particularly “cars at night at traffic inter - sections” and “pedestrians crossing street” . For example , with visual similarity the former query’s results tends to show diverse sets of cars driving down the street , but many of them are not at a traffic intersection . In contrast , using semantic similarity produces more results with stoplights and traffic intersections . Overall for BDD , using semantic similarity results in 1 . 10 × and 1 . 16 × improvement for MAP and APS respectively . In practice , as shown by NoScope , the best MSE value can vary widely per query and dataset . BDD CCT Movies News UCF101 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 M AP Visual Similarity Semantic Similarity BDD CCT Movies News UCF101 0 . 00 0 . 25 0 . 50 0 . 75 1 . 00 APS Visual Similarity Semantic Similarity Figure 11 : MAP and APS for top - 20 results across all datasets when comparing visual and semantic similarity . Semantic similar - ity ( used by Zelda ) requires less tuning than visual similarity and improves APS by 1 . 06 × while maintaining high MAP . 6 RELATED WORK ExploratoryVideoAnalytics . SeeSaw [ 36 ] andVOCALExplore [ 9 ] are the most relevant works to this paper . They also explore har - nessing the capabilities of VLMs in the context of video analytics for interactively searching a large scale video dataset . They propose systems where a human - in - the - loop provides feedback and annota - tions . VOCALExplore uses this feedback to remove low - quality data and train specialized query - specific models . In this work , we demon - strate VLMs produce accurate results without the need to train a query - specific model . Zelda automatically removes low - quality data without the need for manual annotations . SeeSaw refines the results based on user feedback and an adaptive resolution optimiza - tion that analyzes different spatial locations in a frame . However , neither SeeSaw nor VOCALExplore consider result diversification , which is important when presenting top - K results to users . Top - K Queries . Efficient and accurate top - K query processing is a widely researched topic in the database community [ 17 , 35 , 54 ] . There has been recent focus on top - K querying processing for ML workloads . Everest [ 27 ] investigates providing probabilistic guarantees for top - K queries for video analytics . Their work uses models with fixed classes and requires training a model per - query . ColBERT [ 25 , 48 ] and PLAID [ 47 ] are recent work that explore top - K query processing for document retrieval using large scale language models . To the best of our knowledge , this is the first work that explores top - K querying for video analytics using VLMs . Vision - Language Models . CLIP [ 43 ] popularized the use of VLMs and showed state - of - the - art results on zero - shot image classifi - cation . Other work has explored VLMs for object detection [ 12 , 65 ] , visual - question answering and captioning [ 3 ] , image - text re - trieval [ 60 ] , and video understanding [ 57 ] . Zelda takes advantage of the unique characteristics of VLMs as general image and text encoders and will benefit from the continued research in this field . Model Prompting . Segment Anything [ 26 ] , a large segmentation model released by Meta , makes their model “promptable” by taking bounding boxes , natural language , or segment points as inputs . This shows the ongoing trend towards more expressive interfaces for ML models . AutoPrompt [ 51 ] shows that prompting models is challeng - ing and error - prone , and investigates techniques to automatically learn prompts in embedding space . CoOp [ 64 ] and CoCoOp [ 63 ] investigate turning additional context one might add to a prompt into learnable vectors . They also show improvement gains over manually crafted prompts . The techniques developed in our work would complementarily benefit from these techniques that improve the accuracy of prompting . They may also enable different ways of prompting a model . 7 CONCLUSION Today’s video analytics systems limit expressivity , need multiple models to match query predicates , require complex optimizations to achieve high accuracy and low latency , and return redundant and low - quality results . In this work , we advocated for using VLMs to address these limitations given their general expressivity , ability to be a general purpose model , and performance . We showed VLMs are easy to use , widely applicable to many queries , highly accurate , and fast for both single and multiple predicate queries . To reduce the number of results a user needs to interpret , we built Zelda : a system for Z ero - shot E xpressive , re L evant , and D iverse video A nalytics . Using a natural language input , Zelda generates relevant candidate frames without any user annotation or query specific training . Zelda automatically removes low - quality frames using VLMs and uses VLM - generated semantic embeddings to improve diversity over existing approaches while still returning relevant results . Across five datasets and 19 queries , Zelda achieves higher retrieval mean average precision ( up to 1 . 15 × ) compared to out - of - the - box VLMs while improving similarity between frames by up to 1 . 16 × . Zelda is on average 7 . 5 × ( up to 10 . 4 × ) faster at retrieving results than a state - of - the - art video analytics system . REFERENCES [ 1 ] 2022 . CLIP Prompts for Image Classification . https : / / github . com / openai / CLIP / blob / main / data / prompts . md . [ 2 ] Neil Agarwal and Ravi Netravali . 2023 . Boggart : Towards General - Purpose Acceleration of Retrospective Video Analytics . In NSDI . [ 3 ] Jean - Baptiste Alayrac , Jeff Donahue , Pauline Luc , Antoine Miech , Iain Barr , Yana Hasson , Karel Lenc , Arthur Mensch , Katherine Millican , Malcolm Reynolds , et al . 2022 . Flamingo : a visual language model for few - shot learning . Advances in Neural Information Processing Systems 35 ( 2022 ) , 23716 – 23736 . [ 4 ] Internet Archive . 2022 . TV News Archive . https : / / archive . org / details / tv . [ 5 ] Sara Beery , Grant Van Horn , and Pietro Perona . 2018 . Recognition in Terra Incognita . arXiv : 1807 . 04975 [ 6 ] Jiashen Cao , Karan Sarkar , Ramyad Hadidi , Joy Arulraj , and Hyesoon Kim . 2022 . FiGO : Fine - Grained Query Optimization in Video Analytics . In SIGMOD . [ 7 ] Jaime Carbonell and Jade Goldstein . 1998 . The use of MMR , diversity - based reranking for reordering documents and producing summaries . [ 8 ] MaureenDaum , EnhaoZhang , DongHe , MagdalenaBalazinska , BrandonHaynes , Ranjay Krishna , Apryle Craig , and Aaron Wirsing . 2022 . VOCAL : Video Organi - zation and Interactive Compositional AnaLytics . In CIDR . [ 9 ] Maureen Daum , Enhao Zhang , Dong He , Stephen Mussmann , Brandon Haynes , Ranjay Krishna , and Magdalena Balazinska . 2023 . VOCALExplore : Pay - as - You - Go Video Data Exploration and Model Building . arXiv preprint arXiv : 2303 . 04068 ( 2023 ) . [ 10 ] Alexey Dosovitskiy , Lucas Beyer , Alexander Kolesnikov , Dirk Weissenborn , Xi - aohua Zhai , Thomas Unterthiner , Mostafa Dehghani , Matthias Minderer , Georg Heigold , Sylvain Gelly , et al . 2020 . An image is worth 16x16 words : Transformers for image recognition at scale . arXiv preprint arXiv : 2010 . 11929 ( 2020 ) . [ 11 ] Ana LN Fred and Anil K Jain . 2006 . Learning pairwise similarity for data cluster - ing . In 18th International Conference on Pattern Recognition ( ICPR’06 ) . IEEE . [ 12 ] Xiuye Gu , Tsung - Yi Lin , Weicheng Kuo , and Yin Cui . 2021 . Open - vocabulary object detection via vision and language knowledge distillation . arXiv preprint arXiv : 2104 . 13921 ( 2021 ) . [ 13 ] Agrim Gupta , Piotr Dollar , and Ross Girshick . 2019 . Lvis : A dataset for large vocabulary instance segmentation . , 5356 – 5364 pages . [ 14 ] James Hong , Will Crichton , Haotian Zhang , Daniel Y Fu , Jacob Ritchie , Jeremy Barenholtz , Ben Hannel , Xinwei Yao , Michaela Murray , Geraldine Moriba , et al . 2020 . Analyzing who and what appears in a decade of US cable TV news . arXiv : 2008 . 06007 [ 15 ] James Hong , Will Crichton , Haotian Zhang , Daniel Y . Fu , Jacob Ritchie , Jeremy Barenholtz , Ben Hannel , Xinwei Yao , Michaela Murray , Geraldine Moriba , Ma - neesh Agrawala , and Kayvon Fatahalian . 2021 . Analysis of Faces in a Decade of US Cable TV News . In SIGKDD . [ 16 ] HuggingFace . 2023 . A Dive into Vision - Language Models . https : / / huggingface . co / blog / vision _ language _ pretraining [ 17 ] Ihab F Ilyas , George Beskales , and Mohamed A Soliman . 2008 . A survey of top - k query processing techniques in relational database systems . ACM Computing Surveys ( CSUR ) ( 2008 ) . [ 18 ] Chao Jia , Yinfei Yang , Ye Xia , Yi - Ting Chen , Zarana Parekh , Hieu Pham , Quoc Le , Yun - Hsuan Sung , Zhen Li , and Tom Duerig . 2021 . Scaling up visual and vision - language representation learning with noisy text supervision . In International Conference on Machine Learning . PMLR , 4904 – 4916 . [ 19 ] Jeff Johnson , Matthijs Douze , and Hervé Jégou . 2019 . Billion - scale similarity search with GPUs . IEEE Transactions on Big Data 7 , 3 ( 2019 ) , 535 – 547 . [ 20 ] Daniel Kang , Nikos Arechiga , Sudeep Pillai , Peter D . Bailis , and Matei Zaharia . 2022 . Finding Label and Model Errors in Perception Data With Learned Observa - tion Assertions . In SIGMOD . [ 21 ] Daniel Kang , Peter Bailis , and Matei Zaharia . 2019 . BlazeIt : Optimizing Declara - tive Aggregation and Limit Queries for Neural Network - Based Video Analytics . In PVLDB . [ 22 ] Daniel Kang , John Emmons , Firas Abuzaid , Peter Bailis , and Matei Zaharia . 2017 . NoScope : optimizing neural network queries over video at scale . In PVLDB . [ 23 ] Daniel Kang , John Guibas , Peter Bailis , Tatsunori Hashimoto , and Matei Zaharia . 2022 . Semantic Indexes for Machine Learning - based Queries over Unstructured Data . In SIGMOD . [ 24 ] Daniel Kang , Francisco Romero , Peter Bailis , Christos Kozyrakis , and Matei Zaharia . 2022 . VIVA : An End - to - End System for Interactive Video Analytics . In CIDR . [ 25 ] Omar Khattab and Matei Zaharia . 2020 . Colbert : Efficient and effective passage search via contextualized late interaction over bert . In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 39 – 48 . [ 26 ] Alexander Kirillov , Eric Mintun , Nikhila Ravi , Hanzi Mao , Chloe Rolland , Laura Gustafson , Tete Xiao , Spencer Whitehead , Alexander C Berg , Wan - Yen Lo , et al . 2023 . Segment Anything . arXiv preprint arXiv : 2304 . 02643 ( 2023 ) . [ 27 ] Ziliang Lai , Chris Liu , Chenxia Han , Pengfei Zhang , Eric Lo , and Ben Kao . 2022 . Everest : A Top - K Deep Video Analytics System . In Proceedings of the 2022 Inter - national Conference on Management of Data . 2357 – 2360 . [ 28 ] Linjie Li , Jie Lei , Zhe Gan , Licheng Yu , Yen - Chun Chen , Rohit Pillai , Yu Cheng , Luowei Zhou , Xin Eric Wang , William Yang Wang , et al . 2021 . VALUE : A Multi - Task Benchmark for Video - and - Language Understanding Evaluation . In 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) Track on Datasets and Benchmarks . [ 29 ] Liunian Harold Li , Pengchuan Zhang , Haotian Zhang , Jianwei Yang , Chunyuan Li , Yiwu Zhong , Lijuan Wang , Lu Yuan , Lei Zhang , Jenq - Neng Hwang , et al . 2022 . Grounded language - image pre - training . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . [ 30 ] Yangguang Li , Feng Liang , Lichen Zhao , Yufeng Cui , Wanli Ouyang , Jing Shao , Fengwei Yu , and Junjie Yan . 2021 . Supervision exists everywhere : A data efficient contrastive language - image pre - training paradigm . arXiv preprint arXiv : 2110 . 05208 ( 2021 ) . [ 31 ] Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Dollár , and C Lawrence Zitnick . 2014 . Microsoft coco : Common objects in context . In ECCV . Springer . [ 32 ] Yang Liu and Mirella Lapata . 2019 . Text summarization with pretrained encoders . arXiv preprint arXiv : 1908 . 08345 ( 2019 ) . [ 33 ] Jiasen Lu , Christopher Clark , Rowan Zellers , Roozbeh Mottaghi , and Aniruddha Kembhavi . 2022 . Unified - io : A unified model for vision , language , and multi - modal tasks . arXiv preprint arXiv : 2206 . 08916 ( 2022 ) . [ 34 ] Yao Lu , Aakanksha Chowdhery , Srikanth Kandula , and Surajit Chaudhuri . 2018 . Accelerating Machine Learning Inference with Probabilistic Predicates . In SIG - MOD . [ 35 ] Amélie Marian , Nicolas Bruno , and Luis Gravano . 2004 . Evaluating top - k queries over web - accessible databases . ACM Transactions on Database Systems ( TODS ) 29 , 2 ( 2004 ) , 319 – 362 . [ 36 ] Oscar Moll , Manuel Favela , Samuel Madden , and Vijay Gadepally . 2022 . SeeSaw : interactive ad - hoc search over image databases . [ 37 ] NVIDIA . 2023 . NVIDIAT4 . https : / / www . nvidia . com / en - us / data - center / tesla - t4 / . [ 38 ] OpenAI . 2023 . A Dive into Vision - Language Models . https : / / openai . com / research / clip [ 39 ] AdamPaszke , SamGross , FranciscoMassa , AdamLerer , JamesBradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Köpf , Edward Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . PyTorch : An Imperative Style , High - Performance Deep Learning Library . In NeurIPS . [ 40 ] 1091 Pictures . 2023 . Free Movies . https : / / www . youtube . com / @ FREEMOVIESNOW / featured . [ 41 ] PineconeDB . 2023 . Pinecone : Vector Database for Vector Search . https : / / www . pinecone . io / [ 42 ] PyTorch . 2022 . 3D ResNet Video Classification in PyTorch . https : / / pytorch . org / hub / facebookresearch _ pytorchvideo _ resnet / [ 43 ] Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , etal . 2021 . Learningtransferablevisualmodelsfromnaturallanguagesupervision . In International conference on machine learning . PMLR , 8748 – 8763 . [ 44 ] Francisco Romero , Johann Hauswald , Aditi Partap , Daniel Kang , Matei Zaharia , and Christos Kozyrakis . 2022 . Optimizing Video Analytics with Declarative Model Relationships . Proceedings of the VLDB Endowment 16 , 3 , 447 – 460 . [ 45 ] Olga Russakovsky , Jia Deng , Hao Su , Jonathan Krause , Sanjeev Satheesh , Sean Ma , Zhiheng Huang , Andrej Karpathy , Aditya Khosla , Michael Bernstein , et al . 2015 . Imagenet large scale visual recognition challenge . International journal of computer vision 115 , 3 ( 2015 ) , 211 – 252 . [ 46 ] Mohammed Saeed , Nicola De Cao , and Paolo Papotti . 2023 . Querying Large Language Models with SQL . arXiv : 2304 . 00472 [ 47 ] Keshav Santhanam , Omar Khattab , Christopher Potts , and Matei Zaharia . 2022 . PLAID : an efficient engine for late interaction retrieval . In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 1747 – 1756 . [ 48 ] KeshavSanthanam , OmarKhattab , JonSaad - Falcon , ChristopherPotts , andMatei Zaharia . 2021 . Colbertv2 : Effective and efficient retrieval via lightweight late interaction . arXiv preprint arXiv : 2112 . 01488 ( 2021 ) . [ 49 ] Christoph Schuhmann . 2021 . Laion - 400 - Million Open Dataset . https : / / laion . ai / blog / laion - 400 - open - dataset / [ 50 ] Christoph Schuhmann . 2023 . Laion - 400 - Million Open Dataset : KNN Front - end . https : / / knn . laion . ai / [ 51 ] TaylorShin , YasamanRazeghi , RobertLLoganIV , EricWallace , andSameerSingh . 2020 . Autoprompt : Elicitingknowledgefromlanguagemodelswithautomatically generated prompts . arXiv preprint arXiv : 2010 . 15980 ( 2020 ) . [ 52 ] Khurram Soomro , Amir Roshan Zamir , and Mubarak Shah . 2012 . UCF101 : A dataset of 101 human actions classes from videos in the wild . [ 53 ] PyTorch Lightning Team . 2021 . TorchMetrics v0 . 3 — Information Retrieval metrics and more . https : / / devblog . pytorchlightning . ai / torchmetrics - v0 - 3 - 0 - information - retrieval - metrics - and - more - c55265e9b94f [ 54 ] Martin Theobald , Holger Bast , Debapriyo Majumdar , Ralf Schenkel , and Ger - hard Weikum . 2008 . TopX : efficient and versatile top - k query processing for semistructured data . The VLDB Journal ( 2008 ) . [ 55 ] Ultralytics . 2022 . Yolov5 Object Detection in PyTorch . https : / / github . com / ultralytics / yolov5 . [ 56 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Łukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . Advances in neural information processing systems 30 ( 2017 ) . [ 57 ] Hu Xu , Gargi Ghosh , Po - Yao Huang , Dmytro Okhonko , Armen Agha - janyan , Florian Metze , Luke Zettlemoyer , and Christoph Feichtenhofer . 2021 . VideoCLIP : Contrastive Pre - training for Zero - shot Video - Text Understanding . arXiv : 2109 . 14084 [ 58 ] Zhuangdi Xu , Gaurav Kakker , Joy Arulraj , and Umakishore Ramachandran . 2022 . EVA : A Symbolic Approach to Accelerating Exploratory Video Analytics with Materialized Views . In SIGMOD . [ 59 ] Zhihui Yang , Zuozhi Wang , Yicong Huang , Yao Lu , Chen Li , and X . Sean Wang . 2022 . Optimizing Machine Learning Inference Queries with Correlative Proxy Models . In PVLDB . [ 60 ] Lewei Yao , Runhui Huang , Lu Hou , Guansong Lu , Minzhe Niu , Hang Xu , Xiao - dan Liang , Zhenguo Li , Xin Jiang , and Chunjing Xu . 2021 . FILIP : fine - grained interactive language - image pre - training . arXiv preprint arXiv : 2111 . 07783 ( 2021 ) . [ 61 ] Fisher Yu , Haofeng Chen , Xin Wang , Wenqi Xian , Yingying Chen , Fangchen Liu , Vashisht Madhavan , and Trevor Darrell . 2020 . Bdd100k : A diverse driving dataset for heterogeneous multitask learning . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . [ 62 ] Matei Zaharia , Mosharaf Chowdhury , Michael J Franklin , Scott Shenker , and Ion Stoica . 2010 . Spark : Cluster computing with working sets . . In HotCloud . [ 63 ] KaiyangZhou , JingkangYang , ChenChangeLoy , andZiweiLiu . 2022 . Conditional prompt learning for vision - language models . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition . 16816 – 16825 . [ 64 ] Kaiyang Zhou , Jingkang Yang , Chen Change Loy , and Ziwei Liu . 2022 . Learning to prompt for vision - language models . International Journal of Computer Vision 130 , 9 ( 2022 ) , 2337 – 2348 . [ 65 ] Xingyi Zhou , Rohit Girdhar , Armand Joulin , Philipp Krähenbühl , and Ishan Misra . 2022 . Detecting twenty - thousand classes using image - level supervision . In Computer Vision – ECCV 2022 : 17th European Conference , Tel Aviv , Israel , October 23 – 27 , 2022 , Proceedings , Part IX . Springer , 350 – 368 .