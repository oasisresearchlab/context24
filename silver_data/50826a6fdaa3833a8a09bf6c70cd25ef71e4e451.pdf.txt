REVIEW Communicated by Bruce Graham Memory Capacities for Synaptic and Structural Plasticity Andreas Knoblauch andreas . knoblauch @ honda - ri . de Honda Research Institute Europe GmbH , D - 63073 Offenbach , Germany G¨unther Palm guenther . palm @ uni - ulm . de Institut f¨ur Neuroinformatik , Fakult¨at f¨ur Ingenieurwissenschaften und Informatik , Universit¨at Ulm , D - 89069 Ulm , Germany Friedrich T . Sommer fsommer @ berkeley . edu University of California at Berkeley , Redwood Center for Theoretical Neuroscience , Berkeley , CA 94720 - 3220 , U . S . A . Neural associative networks with plastic synapses have been proposed as computational models of brain functions and also for applications such as pattern recognition and information retrieval . To guide biolog - ical models and optimize technical applications , several deﬁnitions of memory capacity have been used to measure the efﬁciency of associative memory . Here we explain why the currently used performance measures bias the comparison between models and cannot serve as a theoretical benchmark . We introduce fair measures for information - theoretic capac - ity in associative memory that also provide a theoretical benchmark . In neural networks , two types of manipulating synapses can be dis - cerned : synaptic plasticity , the change in strength of existing synapses , and structural plasticity , the creation and pruning of synapses . One of the new types of memory capacity we introduce permits quantifying how structural plasticity can increase the network efﬁciency by compressing the network structure , for example , by pruning unused synapses . Speciﬁ - cally , weanalyzeoperatingregimesintheWillshawmodelinwhichstruc - tural plasticity can compress the network structure and push performance to the theoretical benchmark . The amount C of information stored in each synapse can scale with the logarithm of the network size rather than being constant , as in classical Willshaw and Hopﬁeld nets ( ≤ ln 2 ≈ 0 . 7 ) . Fur - ther , the review contains novel technical material : a capacity analysis of the Willshaw model that rigorously controls for the level of retrieval qual - ity , an analysis for memories with a nonconstant number of active units ( where C ≤ 1 / e ln 2 ≈ 0 . 53 ) , and the analysis of the computational com - plexity of associative memories with and without network compression . Neural Computation 22 , 289 – 341 ( 2010 ) C (cid:2) 2009 Massachusetts Institute of Technology 290 A . Knoblauch , G . Palm , and F . Sommer 1 Introduction 1 . 1 Conventional Versus Associative Memory . In the classical von Neumann computing architecture , computation and data storage is per - formed by separate modules , the central processing unit and the random access memory , respectively ( Burks , Goldstine , & von Neumann , 1946 ) . A memory address sent to the random access memory gives access to the data content of one particular storage location . Associative memories are comput - ing architectures in which computation and data storage are not separated . For example , an associative memory can store a set of associations between pairs of ( binary ) patterns { ( u μ → v μ ) : μ = 1 , . . . , M } . Similar to random ac - cess memory , a query pattern u μ entered in associative memory can serve as an address for accessing the associated pattern v μ . However , the tasks performed by the two types of memory differ fundamentally . Random ac - cess is deﬁned only for query patterns that are valid addresses , that is , for the set of u patterns used during storage . The random access task consists of returning the data record at the addressed location ( look - up ) . In contrast , associative memories accept arbitrary query patterns ˜u , and the computa - tion of any particular output involves all stored data records rather than a single one . Speciﬁcally , the associative memory task consists of compar - ing a query ˜u with all stored addresses and returning an output pattern equal ( or similar ) to the pattern v μ associated with the address u μ most similar to the query . Thus , the associative memory task includes the ran - dom access task but is not restricted to it . It also includes computations such as pattern completion , denoising , or data retrieval using incomplete cues . In this review , we compare different implementations of associative memories : First , we study associative networks , that is , parallel implementa - tions of associative memory in a network of neurons in which associations are stored in a set of synaptic weights A between neurons using a local Hebbian learning rule . Associative networks are closely related to Hebbian cell assemblies and play an important role in neuroscience as models of neural computation for various brain structures , for example , neocortex , hippocampus , cerebellum , and mushroom body ( Hebb , 1949 ; Braitenberg , 1978 ; Palm , 1982 ; Fransen & Lansner , 1998 ; Pulverm¨uller , 2003 ; Rolls , 1996 ; Kanerva , 1988 ; Marr , 1969 , 1971 ; Albus , 1971 ; Laurent , 2002 ) . Second , we study compressed associative networks , that is , networks with additional optimal or suboptimal schemes to represent the information contained in the synaptic weight structure efﬁciently . The analysis of this implementation will enable us to derive a general performance benchmark and understand the difference between structural and synaptic plasticity . Third , we study sequential implementation of associative memories , that is , computer programs that implement storage ( compressed or un - compressed ) and memory recall for technical applications and run on an ordinary von Neumann computer . Memory Capacities for Synaptic and Structural Plasticity 291 1 . 2 Performance Measures for Associative Memory . To judge the per - formance of a computing architecture , one has to relate the size of the achieved computation with the size of required resources . The ﬁrst popular performance measure for associative memories was the pattern capacity , that is , the ratio between the number of storable association patterns and the number of neurons in the network ( Hopﬁeld , 1982 ) . However , in two respects , the pattern capacity is not general enough . First , to compare as - sociative memory with sparse and with dense patterns , the performance measure has to reﬂect information content of the patterns , not just the count of stored associations . Thus , performance should be measured by the channel capacity of the memory channel , that is , the maximal mutual information ( or transinformation ) between the stored patterns v μ and the retrieved patterns ˆv μ ( Cover & Thomas , 1991 ; Shannon & Weaver , 1949 ) : T ( v 1 , v 2 , . . . , v M ; ˆv 1 , ˆv 2 , . . . , ˆv M ) . Second , the performance measure should take into account the true required storage resources rather than just the number of neurons . The count of neurons in general does not convey the size of the connectivity structure between neurons , which is the substrate where the associations are stored in associative memories . As we will dis - cuss , there is not one universal measure to quantify the storage substrate in associative memories . To reveal theoretical limitations as well as the ef - ﬁciency of technical and biological implementations of speciﬁc models of associative memory , different aspects of the storage substrate are critical . Here we deﬁne and compare three performance measures for associative memory models that deviate in how the required storage resources are taken into account . First , We deﬁne ( normalized ) network capacity C as the channel capacity of the associative memory with given network structure , normalized to the number of synaptic contacts between neurons that can accommodate synapses : C = T ( v 1 , v 2 , . . . , v M ; ˆv 1 , ˆv 2 , . . . , ˆv M ) # contacts [ bit / contact ] . ( 1 . 1 ) In particular , this deﬁnition assumes ( in contrast to the following two deﬁni - tions ) that the network structure is ﬁxed and independent of the stored data . Deﬁnition 1 coincides with the earlier deﬁnitions of information - theoretical storage capacity , for example , as employed in Willshaw , Buneman , and Longuet - Higgins ( 1969 ) , Palm ( 1980 ) , Amit , Gutfreund , and Sompolinsky ( 1987b ) , Nadal ( 1991 ) , Frolov and Murav’ev ( 1993 ) , and Palm and Sommer ( 1996 ) . The network capacity balances computational beneﬁts with the re - quired degree of connectivity between circuit elements . Such a trade - off is important in many contexts , such as chip design and neuroanatomy of the brain . Network capacity quantiﬁes the resources required in a model by just counting contacts between neurons , regardless of the entropy per contact . This property limits the model class for which network capacity deﬁnes a 292 A . Knoblauch , G . Palm , and F . Sommer benchmark . Only for associative memories with binary contacts is the net - work capacity bounded by the value C = 1 , which marks the achievable op - timum as the absolute benchmark . For binary synapses , the normalization constant in the network capacity equals the maximum entropy or Shannon information I A of the synaptic weight matrix A , assuming statistically in - dependent connections : C = T / max [ I A ] . However , in general , network ca - pacity has no benchmark value . Because it does not account for entropy per contact , this measure tends to overestimate the performance of models rely - ing on contacts with high entropy , and conversely , it underestimates models that require contacts with low entropy ( cf . , Bentz , Hagstroem , & Palm , 1989 ) . Second , to account for the actual memory requirement of an individual model , we deﬁne information capacity as the channel capacity normalized by the total entropy in the connections C I = T / I ( A ) : C I = T ( v 1 , v 2 , . . . , v M ; ˆv 1 , ˆv 2 , . . . , ˆv M ) # bits of required physical memory . ( 1 . 2 ) The information capacity is dimensionless and possesses a model - independent upper bound C I opt = 1 that deﬁnes a general benchmark for associative network models ( Knoblauch , 2003a , 2003b , 2005 ) . Note that for efﬁcient implementation of associative memory , large information capacity is necessary but not sufﬁcient . For example , models that achieve large in - formation capacity with low entropy connections rely on additional mecha - nisms of synaptic compression and decompression to make the implemen - tation efﬁcient . Various compression mechanisms and their neurobiological realizations will be proposed and analyzed in this review . Note further that for models with binary synapses , information capacity is an upper bound of network capacity : C ≤ C I ≤ 1 ( because the memory requirement of the most wasteful model cannot exceed 1 bit per contact ) . Third , we deﬁne synaptic capacity C S as the channel capacity of the asso - ciative memory normalized by the number of nonsilent synapses , C S = T ( v 1 , v 2 , . . . , v M ; ˆv 1 , ˆv 2 , . . . , ˆv M ) # nonsilent synapses [ bit / synapse ] , ( 1 . 3 ) where nonsilent synapses are chemical synapses that transmit signals to the postsynaptic cell and have to be metabolically maintained . Two reasons motivate deﬁnition 3 . First , the principal cost of neural sig - naling appears to be restoring and maintaining ionic balances following postsynaptic potentials ( Lennie , 2003 ; Laughlin & Sejnowski , 2003 ; Attwell & Laughlin , 2001 ) . This suggests that the most critical resource for stor - ing memories in the brain is the physiological maintenance of nonsilent synapses . Thus , our deﬁnition of synaptic capacity assesses the number of active synapses commensurate with metabolic energy consumption in - volved in synaptic transmission . Memory Capacities for Synaptic and Structural Plasticity 293 The second reason is that silent synapses are irrelevant for information retrieval in associative networks ( although they are required for storing new information ) and could therefore be pruned and replaced by synapses at more useful locations . This idea assumes that the network structure can be adapted to the stored data and has close relations to theoretical consid - erations about structural plasticity ( Stepanyants , Hof , & Chklovskii , 2002 ; Poirazi & Mel , 2001 ; Fusi , Drew , & Abbott , 2005 ) . These ideas are also in line with recent neurobiological ﬁndings suggesting that structural plasticity ( including synaptogenesis and dendritic and axonal growth and remodel - ing ) is a common feature in the physiology of adult brains ( Woolley , 1999 ; Witte , Stier , & Cline , 1996 ; Engert & Bonhoeffer , 1999 ; Lamprecht & LeDoux , 2004 ) . Indeed , we have shown in further modeling studies ( Knoblauch , 2006 , 2009 ) how ongoing structural plasticity and synaptic consolidation , for example , induced by hippocampal memory replay , can “place” the rare synapses of a sparsely connected network at the most useful locations and thereby greatly increase the information stored per synapse in accordance with our new performance measure C S . The synaptic capacity is related to the previous deﬁnitions of capacity . First , synaptic capacity is an upper bound of the network capacity C ≤ C S . Second , for binary synapses with low entropy , the synaptic capacity and the information capacity are proportional C S ≈ α C I : For r (cid:6) mn nonsi - lent synapses in an m × n - dimensional connectivity matrix A , we have I A ≈ mnI ( r / mn ) with the single synapse entropy I ( r / mn ) ≈ r log ( mn ) ( see appendix A ) and therefore α = log ( mn ) . Thus , associative memories with binary low - entropy synapses can be implemented by synaptic pruning , and the upper benchmark is given by C S opt = log ( mn ) . Finally , we give an example illustrating when and how the three differ - ent performance measures are applicable . Consider storing 1 kilo bits of information in a neural network A of 100 × 100 binary synapses , and let 150 of the 10 , 000 synapses have weight 1 . Then the network capacity of the static fully connected net is simply C = 1000 / 10 , 000 = 0 . 1 bit per binary synapse . However , the synaptic weight matrix A has only sparsely one - entries with a single synapse entropy of I ( 150 / 10 , 000 ) = 0 . 1124 bit . Then A can be compressed such that the memory requirements for a computer implementation could decrease to only I ( A ) = 1124 bit . Thus , the infor - mation capacity would be C I = 1000 / 1124 = 0 . 89 . In a sparsely connected biological network endowed with structural plasticity , it would be possible to prune silent synapses , regenerate new synapses at random locations , and consolidate synapses only at useful positions . Such a network could get along with only 150 nonsilent synapses such that the resulting synaptic capacity is C S = 1000 / 150 = 6 . 7 bits per synapse . 1 . 3 Associative Memory Models and Their Performance . How do known associative models perform in terms of the capacities we have intro - duced ? The network capacity was ﬁrst applied to the Willshaw or Steinbuch 294 A . Knoblauch , G . Palm , and F . Sommer model ( Willshaw et al . , 1969 ; Palm , 1980 ) , a feedforward neural associative network with binary neurons and synapses ﬁrst proposed by Steinbuch ( 1961 ; see section 2 . 2 of this review ) . The feedforward heteroassociative Willshaw model can achieve a network capacity of C = ln 2 ≈ 0 . 7 bits per contact . The model performs high compared to alternative neural imple - mentations of associative memory with nonbinary synapses and feedback network architectures , which became very popular in the 1980s ( Hopﬁeld , 1982 , 1984 ; Hopﬁeld & Tank , 1986 ; Hertz , Krogh , & Palmer , 1991 ) . The net - work capacity of the original ( nonsparse ) Hopﬁeld model stays with 0 . 14 bits per contact ( Amit , Gutfreund , & Sompolinsky , 1987a ; Amit et al . , 1987b ) far below the one for the Willshaw model ( see Schwenker , Sommer , & Palm , 1996 ; Palm , 1991 ) . The difference in network capacity between the Willshaw model and the Hopﬁeld model turns out to be due to differences in the stored mem - ory patterns . The Willshaw model achieves high network capacity with extremely sparse memory patterns , that is , with a very low ratio between active and nonactive neurons . Conversely , the original Hopﬁeld model is designed for nonsparse patterns with even ratio between active and nonac - tive neurons . Using sparse patterns in the feedforward Hopﬁeld network with accordingly adjusted synaptic learning rule ( Palm , 1991 ; Dayan & Willshaw , 1991 ; Palm & Sommer , 1996 ) increases the network capacity to 1 / ( 2 ln 2 ) ≈ 0 . 72 ( Tsodyks & Feigel’man , 1988 ; Palm & Sommer , 1992 ) . Thus , in terms of network capacity , the sparse Hopﬁeld model outperforms the Willshaw model , but only marginally . The picture is similar in terms of synaptic capacity since the number of nonsilent synapses is the same in both models . However , the comparison between Willshaw and Hopﬁeld model changes signiﬁcantly when estimating the information capacities . If one assumes a ﬁxed number of bits h assigned to represent each synaptic contact , the network capacity deﬁnes a lower bound on the information ca - pacity by C I ≥ C / h ≥ C / # { bits per contact } . Thus , for the Willshaw model ( with h = 1 ) , the information capacity is C I ≥ 0 . 69 . In contrast , assuming h = 2 in the sparse Hopﬁeld model yields a signiﬁcantly lower information capacity of C I ≥ 0 . 72 / 2 = 0 . 36 . In practice , h > 2 is used to represent the synapses with sufﬁcient precision , which increases the advantage of the Willshaw model even more . 1 . 4 The Willshaw Model and Its Problems . Since the Willshaw model is not only among the simplest realizations of content - addressable memory but is also promising in terms of information capacity , it is interesting for ap - plications as well as for modeling the brain . However , the original Willshaw model suffers from a number of problems that prevented broader technical application and limited its biological relevance . First , the basic Willshaw model approaches C = ln 2 only for very large ( i . e . , not practical ) numbers n of neurons , and the retrieval accuracy at maximum network capacity is low ( Palm , 1980 ; Buckingham & Willshaw , 1992 ) . Various studies have Memory Capacities for Synaptic and Structural Plasticity 295 shown , however , that modiﬁcations of the Willshaw model can overcome this problem : Iterative and bidirectional retrieval schemes ( Schwenker et al . , 1996 ; Sommer & Palm , 1999 ) , improved threshold strategies ( Buckingham & Willshaw , 1993 ; Graham & Willshaw , 1995 ) , and retrieval with spiking neurons ( Knoblauch & Palm , 2001 ; Knoblauch , 2003b ) can signiﬁcantly im - prove network capacity and retrieval accuracy in small memory networks . But two other problems of the Willshaw model and its derivates remain so far unresolved . The ﬁrst open question is the sparsity problem that is , the question of whether there is a way to achieve high capacity outside the regime of extreme sparseness in which the number of one - entries k in memory patterns is logarithmic in the pattern size n : k = c log n for a constant c ( cf . Figure 3 ) . In the standard model , even small deviations from this sparseness condition reduce network capacity drastically . Although it was possible for some applications to ﬁnd coding schemes that fulﬁll the strict requirements for sparseness ( Bentz et al . , 1989 ; Rehn & Sommer , 2006 ) , the sparse coding problem cannot be solved in general . The extreme sparsity requirement is problematic not only for applications ( e . g . , see Rachkovskij & Kussul , 2001 ) but also for brain modeling because it is questionable whether neural cell assemblies that satisfy the sparseness condition are stable with realistic rates of spontaneous activity ( Latham & Nirenberg , 2004 ) . At least for sparsely connected networks realizing only a small given fraction P of the possible synapses , it is possible to achieve nonzero capacities up to 0 . 53 ≤ C ≤ 0 . 69 for a larger but still logarithmic pattern activity k = c log n , where the optimal c → ∞ increases with decreasing P → 0 ( Graham & Willshaw , 1997 ; Bosch & Kurfess , 1998 ; Knoblauch , 2006 ) . The second open question concerning the Willshaw model is the capacity gap problem , that is , the question of why the optimal capacity C = ln 2 is separated by a gap of 0 . 3 from the theoretical optimum C = 1 . This question implicitly assumes that the optimal representation of the binary storage matrix is the matrix itself—that is , the distinction between the capacities C and C I deﬁned here is simply overlooked . For many decades , the capacity gap was considered an empirical fact for distributed storage ( Palm , 1991 ) . Although we cannot solve the capacity gap and sparsity problems for the classical deﬁnition of C , we propose models optimizing C I ( or C S ) that can achieve C I = 1 ( or C S = log n ) without requiring extremely sparse activity . 1 . 5 Organization of the Review . In section 2 we deﬁne the compu - tational task of associative memory , including different levels of retrieval quality . Further , we describe the particular model of associative memory under investigation , the Willshaw model . Section 3 contains a detailed analysis of the classical Willshaw model , capturing its strengths and weaknesses . We revisit and extend the classical capacity analysis , yielding a simple formula how the optimal network ca - pacity of C = 0 . 69 bits per contact decreases as a function of the noise level in the address pattern . Further , we demonstrate that high values of network 296 A . Knoblauch , G . Palm , and F . Sommer capacity are tightly conﬁned to the regime of extreme sparseness and , in addition , that ﬁnite - sized networks cannot achieve high network capacity at a high retrieval quality . In section 4 , the capacity analysis is extended to the new capacity mea - sures we have deﬁned in section 1 , to information capacity and synaptic capacity . The analysis of information capacity reveals two efﬁcient regimes that curiously do not coincide with the regime of logarithmic sparseness in which the network capacity is optimal . Interestingly , in the two efﬁcient regimes , the ultrasparse regime ( k < c log n ) and the regime of moderate sparseness ( k > c log n ) , the information capacity becomes even optimal , that is , C I = 1 . Thus , our analysis shows that the capacity gap problem is caused by the bias inherent in the deﬁnition of network capacity . Further , the discovery of a regime with optimal information capacity at moderate sparseness points to a solution of the sparsity problem . The analysis of synaptic capacity reveals that if the number of active synapses rather than the total number of synaptic contacts is the critical constraint , the capacity in ﬁnite - size associative networks increases from less than 0 . 5 bit per synaptic contact to about 5 to 10 bits per active synapse . In section 5 we consider the computational complexity of the retrieval process . We focus on the time complexity for a sequential implementation on a digital computer , but the results can also be interpreted metabolically in terms of energy consumption since retrieval time is dominated by the number of synaptic operations . In particular , we compare two - layer im - plementations of the Willshaw model to three - layer implementations or look - up tables with an additional hidden grandmother cell layer . After the discussion in section 6 , appendix A gives an overview of binary channels . Appendix B reviews exact formulas for analyzing the Willshaw models with ﬁxed pattern activity that are used to verify the results of this review and compute exact capacities for various ﬁnite network sizes ( see Table 2 ) . Appendix C points out some fallacies with previous analyses , for example , relying on gaussian approximations of dendritic potential distributions . Finally , appendix D extends our theory to random pattern activity , where it turns out C ≤ 1 / ( e ln 2 ) . 2 Associative Memory : Computational Task and Network Model 2 . 1 The Memory Task . Associative memories store information about a set of memory patterns . For retrieving memories , three different computa - tional tasks have been discussed in the literature . The ﬁrst task is familiarity discrimination , a binary classiﬁcation of input patterns into known and un - known patterns ( Palm & Sommer , 1992 ; Bogacz , Brown , & Giraud - Carrier , 2001 ) . The second task is autoassociation or pattern completion , which in - volves completing a noisy query pattern to the memory pattern that is most similar to the query ( Hopﬁeld , 1982 ) . Here we focus on the third task , heteroassociation , which is most similar to the function of a random Memory Capacities for Synaptic and Structural Plasticity 297 access memory . The memorized patterns are organized in association pairs { ( u μ (cid:9)→ v μ ) : μ = 1 , . . . , M } . During retrieval , the memory performs asso - ciations within the stored pairs of patterns . If a pattern u μ is entered , the associative memory produces the pattern v μ ( Kohonen , 1977 ) . Thus , in analogy to random access memories , the u - patterns are called address pat - terns and the v - patterns are called content patterns . However , the associative memory task is more general than a random access task in that arbitrary query patterns are accepted , not just the set of u - patterns . A query pattern ˜u will be compared to all stored u - patterns , and the best match μ will be determined . The memory will return an output pattern ˆv that is equal or similar to the stored content pattern v μ . Note that autoassociation is a spe - cial case of heteroassociation ( for u μ = v μ ) and that both tasks are variants of the best match problem in Minsky and Papert ( 1969 ) . Efﬁcient solutions of the best match problem have widespread applications , for example , for cluster analysis , speech and object recognition , or information retrieval in large databases ( Kohonen , 1977 ; Prager & Fallside , 1989 ; Greene , Parnas , & Yao , 1994 ; Mu , Artiklar , Watta , & Hassoun , 2006 ; Rehn & Sommer , 2006 ) . 2 . 1 . 1 Properties of Memory Patterns . In this review , we focus on the case of binary pattern vectors . The address patterns have dimension m , and the content patterns have dimension n . The number of one - entries in a pattern is called the pattern activity . The mean activity in each address pattern u μ is k , which means that , on average , it has k one - entries and m − k zero - entries . Analogously , the mean activity in each content pattern v μ is l . Typically the patterns are sparse , which means that the pattern activity is much smaller than the vector size ( e . g . , k (cid:6) m ) . For the analyses , we assume that the M pattern pairs are generated randomly according to one of the following two methods . First , in the case of ﬁxed pattern activity , each pattern has exactly the same activity . For address patterns , for example , this means that each of the (cid:2) mk (cid:3) binary vectors of size m and activity k has the same chance to be chosen . Second , in the alternative case of random pattern activity , pattern components are independently generated . For address patterns , for example , this means that a pattern component u μ i is one with probability k / m and zero otherwise , independent of other components . It turns out that the distinction between constant and random pattern activity is relevant only for address patterns , not for content patterns . Binary memory patterns can be distorted by two distinct types of noise : add noise means that false one - entries are added , and miss noise means that one - entries are deleted . The rates of these error types in query and output patterns determine two key features of associative memories : noise tolerance and retrieval quality . 2 . 1 . 2 Noise Tolerance . To assess how much query noise can be tolerated by the memory model , we form query patterns ˜u by adding random noise to the u - patterns . For our analyses in the main text , we assume that a query pattern ˜u has exactly λ k “correct” and κ k “false” one - entries . Thus , query 298 A . Knoblauch , G . Palm , and F . Sommer patterns have ﬁxed pattern activity ( λ + κ ) k ( see appendix D for random query activity ) . Query noise and cross - talk between the stored memories can lead to noise in the output of the memory . Output noise is expressed in deviations between retrieval output ˆv and the stored v - patterns . 2 . 1 . 3 Retrieval Quality . Increasing the number M of stored patterns will eventuallyincreasetheoutputnoiseintroducedbycross - talk . Thus , interms of the introduced capacity measures , there will be a trade - off between mem - ory load that increases capacity and the level of output noise that decreases capacity . In many situations , a substantial information loss due to output errors can be compensated by the high number of stored memories , and the capacity is maximized at high levels of output errors . For applications , however , this low - ﬁdelity regime is not interesting , and one has to assess ca - pacity at speciﬁed low levels of output noise . Based on the expectation E μ of errors per output pattern or Hamming distance h ( v μ , ˆv μ ) : = (cid:4) nj = 1 | v μ j − ˆ v μ j | , we deﬁne different retrieval qualities ( RQ ) that will be studied : (cid:2) RQ0 : E μ h ( v μ , ˆv μ ) = lp 10 + ( n − l ) p 01 ≤ ρ 0 n (cid:2) RQ1 : E μ h ( v μ , ˆv μ ) = lp 10 + ( n − l ) p 01 ≤ ρ 1 l (cid:2) RQ2 : E μ h ( v μ , ˆv μ ) = lp 10 + ( n − l ) p 01 ≤ ρ 2 (cid:2) RQ3 : E μ h ( v μ , ˆv μ ) = lp 10 + ( n − l ) p 01 ≤ ρ 3 / M , where p 10 : = pr [ ˆv μ j = 0 | v μ j = 1 ] and p 01 : = pr [ ˆv μ j = 1 | v μ j = 0 ] are the component error probabilities and ρ 0 , ρ 1 , ρ 2 , ρ 3 are ( typically small ) con - stants . NotethattherequiredqualityisincreasingfromRQ0toRQ3 . Asymp - totically for n → ∞ , RQ0 requires small constant error probabilities , RQ1 requires the expected number of output errors per pattern to be a small fraction of pattern activity l , RQ2 requires the expected number of output errors per pattern to be small , and RQ3 requires the total number of errors ( summed over the recall of all M stored patterns ) to be small . Making these distinctions explicit allows a uniﬁed analysis of associative networks and reconciles discrepancies between previous works ( cf . Nadal , 1991 ) . 2 . 2 The Willshaw Model . To represent the described associative mem - ory task in a neural network , neurons with binary values are sufﬁcient , although for the computation neurons with continuous values can be bene - ﬁcial ( Anderson , Silverstein , Ritz , & Jones , 1977 ; Anderson , 1993 ; Hopﬁeld , 1984 ; Treves & Rolls , 1991 ; Sommer & Dayan , 1998 ) . The patterns u μ and v μ describe the activity states of two populations of neurons at time μ . In neu - ral associative memories , the associations are stored in the synaptic matrix or memory matrix . 2 . 2 . 1 Storage . In the Willshaw or Steinbuch model ( Willshaw et al . , 1969 ; Steinbuch , 1961 ; Palm , 1980 , 1991 ) , not only neurons but also synapses Memory Capacities for Synaptic and Structural Plasticity 299 1 1 1 1 0 0 0 u \ 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 target patterns v : n = 8 , l = 3 μ a dd r e ss p a tt e r n s u : m = 7 , k = 4 μ u ~ 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 u w it h 1 λ = 2 / 4 ; κ = 0 u ~ ( 1 ) Learning patterns i 0 0 1 1 1 0 0 0 1 1 0 0 0 u \ 2 v 2 1 0 0 0 0 1 v 1 1 0 1 j 1 memory matrix A ( 2 ) Retrieving patterns 0 0 2 1 2 2 0 ( Θ = 2 ) ^ A v 1 1 0 1 0 1 0 0 0 A Figure 1 : Learning and retrieving patterns in the binary Willshaw model . Dur - ing learning ( left ) , the associations between a set of address patterns u μ and content patterns v μ are stored in the synaptic memory matrix A by clipped Hebbian learning ( see equation 2 . 1 ) . For retrieval ( right ) , a query pattern ˜u is propagated through the synaptic network by a vector - matrix multiplication fol - lowed by a threshold operation ( see equation 2 . 2 ) . In the example , the query pattern contains half of the one - entries of u 1 , and the retrieval output ˆv equals v 1 for an optimal threshold (cid:6) = | ˜u | = 2 . have binary values . The storage and retrieval processes work as follows . The pattern pairs are stored heteroassociatively in a binary memory matrix A ∈ { 0 , 1 } m × n ( see Figure 1 ) , where A ij = min ⎛ ⎝ 1 , M (cid:7) μ = 1 u μ i · v μ j ⎞ ⎠ ∈ { 0 , 1 } . ( 2 . 1 ) The network architecture is feedforward ; thus , an address population u consists of m neurons projects via the synaptic matrix A to a content popu - lation v consisting of n neurons . Note that the memory matrix is formed by local Hebbian learning , that is , A ij is a ( nonlinear ) function of the activity values in the pre - and postsynaptic neuron u i and v j regardless of other activity in the network . Note further that for the autoassociative case u = v ( i . e . , if address and content populations are identical ) , the network can be interpreted as an undirected graph with m = n nodes and edge matrix A , where patterns correspond to cliques of k = l nodes . 300 A . Knoblauch , G . Palm , and F . Sommer 2 . 2 . 2 Retrieval . Stored information can be retrieved by entering a query pattern ˜u . First , a vector - matrix - multiplication yields the dendritic poten - tials x = ˜u · A in the content neurons . Second , a threshold operation in each content neuron results in the retrieval output ˆv , ˆ v j = ⎧ ⎪ ⎨ ⎪⎩ 1 , x j = (cid:14) m (cid:7) i = 1 ˜ u i A ij (cid:15) ≥ (cid:6) 0 , otherwise . ( 2 . 2 ) A critical prerequisite for high - retrieval quality is the right choice of the threshold value (cid:6) . Values that are too low will lead to high rates of add - errors , whereas values that are too high will result in high rates of miss - errors . A good threshold value is the number of correct one elements in the address pattern because it yields the lowest rate of add errors in the retrieval while still avoiding miss errors entirely . Depending on the types of errors present in the address , this threshold choice can be simple or rather difﬁcult . For the cases of error - free addresses ( λ = 1 and κ = 0 ) and pattern part retrieval , that is , when the address contains miss errors only ( 0 < λ ≤ 1 and κ = 0 ) , the optimal threshold value is a simple function of the address pattern (cid:6) = | ˜u | : = (cid:4) mi = 0 ˜ u i . This threshold value was used in the original Willshaw model , and therefore we will refer to it as the Willshaw threshold . This threshold setting can be easily implemented in technical systems and is also biologically very plausible , for example , based on feedforward inhi - bition via “shadow” interneurons ( cf . Knoblauch & Palm , 2001 ; Knoblauch , 2003b , 2005 ; Aviel , Horn , & Abeles , 2005 ) . For the general case of noisy addresses , including miss and add errors ( 0 < λ ≤ 1 , κ ≥ 0 ) the optimal threshold is no simple function of the address pattern ˜u . In this case , the number of correct ones is uncertain given the address , and therefore the threshold strategies have to estimate this value based on priori knowledge of κ and λ . 2 . 3 Two - Layer Associative Networks and Look - Up Tables . Essentially the Willshaw model is a neural network with a single layer of neurons v that receive inputs from an address pattern u . A number of memory models in the literature can be regarded as an extension of the Willshaw model by adding an intermediate layer of neurons w ( see Figure 2 ) . If for each asso - ciation to be learned , u μ → v μ , one would activate an additional random pattern w μ , the two memory matrices A 1 and A 2 would store associations u μ → w μ and w μ → v μ , respectively . Thus , the two - layer memory would function analogously to the single - layer model ( see equation 2 . 1 ) . However , the two - layer model can be advantageous if address and content patterns are nonrandom or nonsparse because in such cases , the performance of the single - layer model is severely impaired ( Knoblauch , 2005 ; Bogacz and Brown , 2003 ) . The advantage of two - layer models is related to the fact that single - layer perceptrons can learn only linearly separable mappings u μ → Memory Capacities for Synaptic and Structural Plasticity 301 A 1 A 2 u u A v w v Figure 2 : Single - layer Willshaw model ( left ) and two - layer extension ( right ) where an additional cell layer w mediates between address layer u and content layer v . v μ , while arbitrary mappings require at least a second ( hidden ) layer . In - stead of choosing random patterns w μ , one can also try to optimize the inter - mediary pattern representations . Another interesting model of a two - layer memory is the Kanerva network , where the ﬁrst memory matrix A 1 is a ﬁxed random projection , and only the second synaptic projection A 2 is learned by Hebbian plasticity ( Kanerva , 1988 ) . In addition , two - layer memories are neural implementations of look - up tables if the intermediary layer w has a single active ( grandmother ) neuron for each association to be stored . In this case , the two memory matrices A 1 and A 2 degenerate to simple look - up tables where the μ th row contains the μ th pattern , respectively . In section 5 , we will compare the single - layer model to the two - layer ( grandmother cell or look - up table ) model . Surprisingly , we will ﬁnd that the performance of the grandmother cell model is superior to that of the single - layer model in many cases . This is true at least for technical applications , while for biology , the large number of neurons required in the middle layer may be unrealistic , even when it would be possible to select single cells in a WTA - like manner . 3 Analysis of Network Capacity 3 . 1 Asymptotic Analysis of Network Capacity . This section summa - rizes and extends the classical asymptotic analysis of the Willshaw model ( Willshaw et al . , 1969 ; Palm , 1980 ) . The fraction of one - entries in the memory matrix p 1 : = (cid:4) ij A ij / mn is a monotonic function of the number of stored patterns and will therefore be referred to as matrix load or memory load . The probability that a physically present synapse is not activated by the associ - ation of one pattern pair is 1 − kl / mn . Therefore , after learning M patterns , the matrix load is given by p 1 = 1 − (cid:16) 1 − kl mn (cid:17) M . ( 3 . 1 ) It is often convenient to use equation 3 . 1 to determine the number of stored patterns , M = ln ( 1 − p 1 ) ln ( 1 − kl / mn ) ≈ − mn kl ln ( 1 − p 1 ) , ( 3 . 2 ) where the approximation is valid for kl (cid:6) mn . 302 A . Knoblauch , G . Palm , and F . Sommer The general analysis of retrieval includes queries ˜u that contain both noise types , that is , λ · k “correct” and κ · k “false” one - entries ( 0 < λ ≤ 1 ; 0 ≤ κ ) . For clarity , we start with the analysis of pattern part retrieval where the query pattern contains no add noise , that is , κ = 0 ( for investigations of the general case , see section 4 . 5 ) . For pattern part retrieval with ﬁxed query activity and Willshaw threshold (cid:6) = | ˜u | = λ k , the probability of add noise in the retrieval is p 01 = p ( ˆ v i = 1 | v μ i = 0 ) (cid:2) p 1 λ k . ( 3 . 3 ) ( For exact formulas , see equations B . 6 to B . 8 in appendix B . For random query activity , see appendix D . ) The following analysis is based on the binomial approximation equation 3 . 3 , which assumes independently gen - erated one - entries in a subcolumn of the memory matrix . Although this is obviously not true for distributed address patterns with k > 1 , the approx - imation is sufﬁciently exact for most parameter ranges . Knoblauch ( 2007 , 2008 ) shows that equation 3 . 3 is generally a lower bound and becomes exact at least for k = O ( n / log 4 n ) . With the error probability p 01 , one can compute the mutual information between the memory output and the original content . The mutual infor - mation in one pattern component is T ( l / n , p 01 , 0 ) ( see equation A . 5 ) . When Mn such components are stored , the network capacity C ( k , l , m , n , λ , M ) of equation 1 . 1 is C = M m T (cid:16) l n , p 01 , 0 (cid:17) ≤ ld p 01 ln ( 1 − p 1 ) k ( 3 . 4 ) ≤ λ · ld p 1 · ln ( 1 − p 1 ) ≤ λ ln 2 , ( 3 . 5 ) where we used the bound equation A . 6 and the binomial approximation equation 3 . 3 . The ﬁrst equality is strictly correct only for random activity of address patterns , but still a tight approximation for ﬁxed address pattern activity . The ﬁrst bound becomes tight at least for ( l / n ) / p 01 → 0 ( see equa - tion A . 6 ) , the second bound for k ∼ O ( n / log 4 n ) ( see references above ) , and the third bound for p 1 = 0 . 5 and M ≈ 0 . 69 mn / kl . Thus , the original Willshaw model can store at most C = ln 2 ≈ 0 . 69 bits per synapse for λ = 1 ( however , for random query activity , we achieve at most C = 1 / ( e ln 2 ) ≈ 0 . 53 bits per synapse ; see appendix D ) . The up - per bound can be reached for sufﬁciently sparse patterns , l (cid:6) n , k (cid:6) m , and balanced memory matrix with an equal number of active and inactive synapses . Strictly speaking , the requirement ( l / n ) / p 01 (cid:6) 1 implies only low retrieval quality , with the number of false one - entries exceeding the number of correct one - entries l . The following section shows that the upper bound can also be reached at higher levels of retrieval quality . Memory Capacities for Synaptic and Structural Plasticity 303 3 . 2 Capacity Analysis for Deﬁned Grades of Retrieval Quality . To ensure a certain retrieval quality , we bound the error probability p 01 by p 01 (cid:7) , p 01 ≤ p 01 (cid:7) : = (cid:7) l n − l ⇔ λ ≥ λ (cid:7) : ≈ ln (cid:7) l n − l k ln p 1 , ( 3 . 6 ) where we call (cid:7) > 0 the ﬁdelity parameter . For the approximation of minimal address pattern fraction λ (cid:7) , we again used the binomial approximation equation 3 . 3 . Note that for p 10 = 0 and constant (cid:7) , this condition ensures retrieval quality of type RQ1 ( see section 2 . 1 ) . More generally , to ensure retrieval quality RQ0 - 3 at levels ρ 0 − ρ 3 , the ﬁdelity parameter (cid:7) has to fulﬁll the following conditions : (cid:2) RQ0 : (cid:7) ≤ ρ 0 nl (cid:2) RQ1 : (cid:7) ≤ ρ 1 (cid:2) RQ2 : (cid:7) ≤ ρ 2 / l (cid:2) RQ3 : (cid:7) ≤ ρ 3 1 Ml . As one stores more and more patterns , the matrix load p 1 increases , and the noise level λ (cid:7) that can be afforded in the address to achieve the speciﬁed retrieval quality drops . Therefore , the maximum number of patterns that can be stored is reached at the point where λ (cid:7) reaches the required fault tolerance : λ (cid:7) = λ ( see equation 3 . 6 ) . Accordingly , the maximum matrix load ( and the optimal activity of address patterns ) is given by p 1 (cid:7) ≈ (cid:16) (cid:7) l n − l (cid:17) 1 λ · k (cid:14) ⇔ k ≈ ld (cid:7) l n − l λ ld p 1 (cid:7) (cid:15) . ( 3 . 7 ) Thus , with equations 3 . 2 and 3 . 4 , we obtain the maximal number of stored patterns : the pattern capacity M (cid:7) and the network capacity C (cid:7) ( k , l , m , n , λ , (cid:7) ) ≈ M (cid:7) m − 1 T ( l / n , (cid:7) l / ( n − l ) , 0 ) , M (cid:7) ≈ − λ 2 · ( ld p 1 (cid:7) ) 2 · ln ( 1 − p 1 (cid:7) ) · k l · mn (cid:2) ld n − l (cid:7) · l (cid:3) 2 ( 3 . 8 ) C (cid:7) ≈ λ · ld p 1 (cid:7) · ln ( 1 − p 1 (cid:7) ) · η , ( 3 . 9 ) where η : = T (cid:2) ln , (cid:7) l n − l , 0 (cid:3) − ln ld (cid:7) l n − l = T (cid:2) ln , (cid:7) l n − l , 0 (cid:3) I ( ln ) · (cid:14) 1 1 + ln (cid:7) ln ( l / n ) − ln ( 1 − l / n ) ln ( l / n ) + ( n − l ) ld ( 1 − l / n ) l ld ( (cid:7) l / n ) (cid:15) ( 3 . 10 ) 304 A . Knoblauch , G . Palm , and F . Sommer ≈ 1 1 + ln (cid:7) ln ( l / n ) . ( 3 . 11 ) The approximation equation 3 . 11 is valid for small (cid:7) , l / n (cid:6) 1 . For high - ﬁdelity recall with small (cid:7) (cid:6) 1 , the error e I of approximating T by I becomes negligible and even T / I = ( 1 − e I ) → 1 for l / n → 0 ( see equa - tion A . 9 for details ) . For sparse content patterns with l / n (cid:6) 1 , we have I ( l / n ) ≈ − ( l / n ) ld ( l / n ) ( see equation A . 1 ) , and the right summand in the brackets can be neglected . Finally , the left summand in the brackets of equation 3 . 10 becomes 1 for ln (cid:7) / ln ( l / n ) → 0 . The next two ﬁgures illustrate the results of this analysis with an exam - ple : a Willshaw network with a square - shaped memory matrix ( m = n ) . The address and content patterns have the same activity ( k = l ) , and the input is noiseless , that is , λ = 1 , κ = 0 . Figure 3 presents results for a network with n = 100 , 000 neurons , a number that corresponds roughly to the number of neurons below 1 square millimeter of cortex surface ( Braitenberg & Sch¨uz , 1991 ; Hellwig , 2000 ) . Figure 3a shows that high network capacity is assumed in a narrow range around the optimum pattern activity k opt = 18 and de - creases rapidly for larger or smaller values . For the chosen ﬁdelity level (cid:7) = 0 . 01 , the maximum network capacity is C (cid:7) ≈ 0 . 5 , which is signiﬁcantly below the asymptotic bound . The dashed line shows how the memory load p 1 (cid:7) increases monotonically with k from 0 to 1 . The maximum network capacity is assumed near p 1 (cid:7) = 0 . 5 , similar to the asymptotic calculation . Note that the number of patterns M (cid:7) becomes maximal at smaller values p 1 (cid:7) < 0 . 5 ( M (cid:7) ≈ 29 . 7 · 10 6 for k = 8 and p 1 (cid:7) ≈ 0 . 17 ) . Figure 3b explores the case where pattern activity is ﬁxed to the value k = 18 , which was optimal in Figure 3a , for variable levels of ﬁdelity . The most important observation is that not only the maximum number of pat - terns , but also the maximum network capacity is obtained for low ﬁdelity : C ≈ 0 . 64 occurs for (cid:7) ≈ 1 . 4 . This means that in a ﬁnite - sized Willshaw net - work , a high number of stored patterns outbalances the information loss due to the high level of output errors , an observation made also by Nadal and Toulouse ( 1990 ) and Buckingham and Willshaw ( 1992 ) . However , most applications require low levels of output errors and therefore cannot use the maximum network capacity . Technically the pattern capacity M is un - bounded since M (cid:7) → ∞ for (cid:7) → n / l − 1 . However , this transition corre - sponds to p 1 (cid:7) → 1 and p 01 (cid:7) → 1 , which means that the stored patterns cannot be retrieved anymore . The contour plots in Figures 3c to 3e give an overview of how network capacity , memory load , and the maximum num - ber of stored patterns vary with pattern activity and ﬁdelity level . High - quality retrieval with small (cid:7) requires generally larger assembly size k . For ﬁxed ﬁdelity level (cid:7) , optimal k for maximal M is generally smaller than optimal k for maximal C ( the latter has about double size ; cf . Knoblauch , Palm , & Sommer , 2008 ) . Memory Capacities for Synaptic and Structural Plasticity 305 0 . 001 0 . 01 0 . 05 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 67 0 . 001 0 . 01 0 . 05 0 . 1 n = 10 5 λ = 1 0 2 4 6 8 10 12 14 −10 −8 −6 −4 −2 0 2 4 6 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 n = 10 5 λ = 1 0 2 4 6 8 10 12 14 −10 −8 −6 −4 −2 0 2 4 6 0 . 999 0 . 99 0 . 9 0 . 7 0 . 5 0 . 3 0 . 1 0 . 01 0 . 001 0 . 2 0 . 4 0 . 6 0 . 8 n = 10 5 λ = 1 0 2 4 6 8 10 12 14 −10 −8 −6 −4 −2 0 2 4 6 log k 2 pattern activity p 1 s t o r a g e ca p ac it y C , m e m o r y l o a d p 1 s t o r a g e ca p ac it y C , m e m o r y l o a d log 10 ε high fidelity log k 2 l og 10 ε log k 2 l og 10 ε log k 2 l og 10 ε # patterns log M ε 10 memory load 1ε p storage capacity C ε 0 2 4 6 8 10 12 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 p 1 ε M ε C ε n = 10 5 ε = 0 . 01 λ = 1 0 2 4 6 8 10 12 0 4e6 8e6 12e6 16e6 20e6 24e6 28e6 32e6 36e6 40e6 −14 −12 −10 −8 −6 −4 −2 0 2 4 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 p 1 ε C ε M ε n = 10 5 k = 18 λ = 1 −14 −12 −10 −8 −6 −4 −2 0 2 4 0 15e6 30e6 45e6 60e6 75e6 90e6 105e6 120e6 135e6 150e6 # p a tt e r n s M # p a tt e r n s M ( a ) ( b ) ( e ) ( d ) ( c ) Figure 3 : Classical capacity measures C and M for a ﬁnite Willshaw network with m = n = 10 5 neurons assuming equal pattern activities , k = l , and zero input noise , λ = 1 , κ = 0 . ( a ) Network capacity C (cid:7) ( bold line ) , pattern capacity M (cid:7) ( thin line ) , and memory load p 1 (cid:7) ( dashed line ) as functions of pattern activity k ( log scale ) . The ﬁdelity level is (cid:7) = 0 . 01 . The maximum C (cid:7) ≈ 0 . 49 is reached for k = 18 . For larger or smaller k , the capacity decreases rapidly . The memory load p 1 (cid:7) increases monotonically with k and is near 0 . 5 at maximum capacity . ( b ) Same quantities as in a plotted as functions of (cid:7) ( log scale ) assuming ﬁxed k = 18 . The maximum C (cid:7) ≈ 0 . 63 is reached at low ﬁdelity ( (cid:7) ≈ 1 ) where the retrieval result contains a high level of add noise . ( c – e ) : Contour plots in the plane spanned by pattern activity k and high - ﬁdelity parameter (cid:7) for network capacity C (cid:7) ( c ) , memory load p 1 (cid:7) ( d ) , and pattern capacity M (cid:7) ( e ) . 3 . 3 Reﬁned Asymptotic Analysis for Large Networks . Section 3 . 2 de - lineated a theory for the Willshaw associative memory that predicts pattern capacity and network capacity for ﬁnite network sizes and deﬁned levels of retrieval quality . Here we use this theory to specify the conditions under which large networks reach the optima of network capacity C (cid:7) → λ ln 2 and pattern capacity M (cid:7) . We focus on the case k ∼ l , which applies to autoasso - ciative memory tasks and heteroassociative memory tasks if the activities of address and content patterns are similar . The results displayed in Figure 4 can be compared to the predictions of the classical analysis recapitulated in section 3 . 1 . Several important observations can be made : (cid:2) The upper bound of network capacity can in fact be reached by equa - tion 3 . 9 for arbitrary small constant (cid:7) , that is , at retrieval - quality 306 A . Knoblauch , G . Palm , and F . Sommer memory load p 1ε log 10 n # neurons 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 1 . 1 1 . 2 1 . 3 M ε ∼ − ( ld p 1 ε ) 2 ln ( 1−p 1 ε ) C ε = ld p 1 ε ln ( 1−p 1 ε ) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 k = ld ( n ) k = 5 k = sqrt ( n ) ε = 0 . 01 / 1 λ = 1 s t o r a g e ca p ac it y C , # p a tt e r n s M s t o r a g e ca p ac it y C ( a ) ( b ) Figure 4 : Classical capacity measures C and M for the Willshaw network in the asymptotic limit n → ∞ . Other parameter settings are as in Figure 3 : m = n , k = l , λ = 1 , and κ = 0 . ( a ) Network capacity C (cid:7) → ld p 1 (cid:7) ln ( 1 − p 1 (cid:7) ) ( bold line ; see equation 3 . 9 ) and pattern capacity M (cid:7) / ( mn / ( ld n ) 2 ) → − ( ld p 1 (cid:7) ) 2 ln ( 1 − p 1 (cid:7) ) ( thin line ; see equation 3 . 8 ) as functions of the matrix load p 1 (cid:7) ( see equation 3 . 7 ) . C (cid:7) is maximal for p 1 (cid:7) = 0 . 5 , whereas M (cid:7) is maximal for p 1 (cid:7) ≈ 0 . 16 . ( b ) Network capacity C (cid:7) as a function of n for different functions of pattern activity k ( n ) . Black lines correspond to high - ﬁdelity retrieval with (cid:7) = 0 . 01 , gray lines to low ﬁdelity with (cid:7) = 1 . Bold lines : square root sparseness ; solid lines : logarithmic sparseness ; thin lines : low , constant activity ( k = 5 ) . grade RQ1 at arbitrary high ﬁdelity : C (cid:7) → λ ln 2 for m , n → ∞ and p 1 (cid:7) → 0 . 5 . The latter condition requires logarithmic pattern sparse - ness k = ld n / λ ( see equation 3 . 7 ) . (cid:2) At retrieval quality grade RQ1 , network capacity and pattern capac - ity assume their optima for somewhat different parameter settings . The pattern capacity M (cid:7) ( see equation 3 . 8 ) peaks at a memory load p 1 (cid:7) ≈ 0 . 16 , which also requires logarithmic sparseness in the memory patterns , but with a smaller constant than for maximizing network capacity : k = ld n / ( λ ld6 . 25 ) . The optimal pattern capacity grows with mn / ( log n ) 2 ( see equation 3 . 8 ) . (cid:2) The optimal bound of network capacity is approached only for log - arithmic sparseness k ∼ log n , the asymptotically optimal choice of sparseness . For weaker sparseness ( e . g . , k ∼ √ n ) or stronger sparse - ness ( e . g . , k = 5 ) , the network capacity peaks at some ﬁnite network size and vanishes asymptotically . The rate of convergence toward the asymptotic capacity ln 2 depends strongly on the required level of ﬁ - delity . For high ﬁdelity ( e . g . , (cid:7) = 0 . 01 ) , this convergence is quite slow , for low ﬁdelity much faster ( e . g . , (cid:7) = 1 ) . With regard to the ﬁrst statement , it is interesting to ask for what grades of retrieval quality higher than RQ1 the upper bound of network capacity Memory Capacities for Synaptic and Structural Plasticity 307 C = λ ln 2 can be achieved . The ﬁrst statement relies on η → 1 ( in equa - tion 3 . 9 ) , that requires (cid:7) > l / n , a condition that is always fulﬁlled for the retrieval quality regimes RQ0 and RQ1 . It also holds for RQ2 ( requiring (cid:7) ∼ 1 / l ) if l is sufﬁciently small , for example , l / n d → 0 for any d > 0 . In particular , this ansatz describes the usual case of logarithmic sparseness k ∼ l and k ∼ log n . However , in the strictest “no - error” quality regime RQ3 , the upper bound of network capacity is unachievable because it re - quires (cid:7) ∼ 1 / ( Ml ) = k / ( mn ln ( 1 − p 1 ) ) ∼ k / ( mn ) , whichisincompatiblewith η → 1 or ln (cid:7) / ln ( l / n ) → 0 . For example , assuming m ∼ n yields η → 1 / 3 and therefore the upper bound of network capacity for RQ3 becomes C = ( λ ln 2 ) / 3 ≤ 0 . 23 . Note that this result is consistent with the Gardner bound 0 . 29 ( Gardner & Derrida , 1988 ) and suggests that previous estimates of RQ3 capacity are wrong or misleading . For example , the result 0 . 346 com - puted by Nadal ( 1991 ) is correct only for very small - content populations , for example , n = 1 , where (cid:7) ∼ k / m and η → 1 / 2 . In summary , the Willshaw model achieves the optimal capacity ln 2 ( or 1 / e ln 2 for random query activity ; see appendix D ) at surprisingly high grades of retrieval quality . Recall that the Hopﬁeld model achieves nonzero capacity only in the retrieval quality regime RQ0 ( Amit et al . , 1987a ) . However , to date no ( distributed ) associative memory model is known that equals look - up tables in their ability to store an arbitrary large number of patterns without any errors ( see section 5 ) . Note that our method of asymptotic analysis is exact , relying only on the binomial approximation , equation 3 . 3 , which has recently been shown to be accurate for virtually any sublinearly sparse patterns ( see Knoblauch , 2007 , 2008 ; see also appendix C for linearly sparse and nonsparse patterns ) . Furthermore , we are able to compute exact capacities even for small networks and thus verify our asymptotic results ( see appendixes B and D and Table 2 ) . In contrast , many classical analyses , for example , based on statistical physics ( e . g . , Tsodyks & Feigel’man , 1988 ; Golomb , Rubin , & Sompolinsky , 1990 ) , become reliable only for very large networks , assume an inﬁnite relaxation time , and apply only to autoassociation with a recurrent symmetric weight matrix . However , some more recent attempts apply nonequilibrium methods for studying the behavior of recurrent neural networks with symmetric or asymmetric connections far from equilibrium and relaxation ( for review , see Coolen , 2001a , 2001b ) . Alternative approaches based on signal - to - noise theory ( e . g . , Dayan & Willshaw , 1991 ; Palm & Sommer , 1996 ) are better suited for ﬁnite feedforward networks with asymmetric weight matrix but require gaussian assumptions on the distribution of dendritic potentials , which may lead to inaccurate results even for very large networks , in particular if patterns are very sparse or nonsparse ( see appendix C ) . Before we proceed to compute synaptic capacity and information capacity for the Willshaw network , we characterize promising working regimes where the synaptic matrix has low entropy , and therefore compression is possible . 308 A . Knoblauch , G . Palm , and F . Sommer 3 . 4 Regimes of Balanced , Sparse and Dense Potentiation . Historically , most analyses and model extensions of the Willshaw model have focused on the regime of balanced potentiation with a balanced memory load 0 < p 1 (cid:7) < 1 in which the network capacity becomes optimal ( Willshaw et al . , 1969 ; Palm , 1980 ; Nadal , 1991 ; Buckingham & Willshaw , 1992 ; Sommer & Palm , 1999 ) . Our extended analysis can reveal the optimal values p 1 (cid:7) for arbitrary parameter settings , and it certainly suggests avoiding the regimes p 1 (cid:7) → 0 or p 1 (cid:7) → 1 . Equations 3 . 5 and 3 . 9 and Figure 4a illustrate that in these regimes , the network capacity drops to zero . It is easy to show that in the limit n → ∞ , the following equivalences hold : C (cid:7) > 0 ⇔ k ∼ log n ⇔ 0 < p 1 (cid:7) < 1 . ( 3 . 12 ) To see this , we can rewrite equation 3 . 7 as p 1 (cid:7) = exp ( − d / λ c ) with c > 0 , logarithmic k = c ln n , and d : = − ln ( (cid:7) l / n ) / ln n . At retrieval quality grades RQ2 and RQ3 , d is a constant . Even at RQ1 , d remains typically constant for sublinear l ( n ) ( e . g . , d = 1 if l grows not faster than a polynomial in log n ) . Then by varying c , one can obtain asymptotically for p 1 (cid:7) all possible values in ( 0 ; 1 ) , and correspondingly for C (cid:7) all values in ( 0 ; ln 2 ] . Since p 1 (cid:7) is monotonically increasing in k , we conclude that in the limit n → ∞ , for d = − ln p 01 (cid:7) / ln n ∼ 1 and sublinear l ( n ) the equivalences 3 . 12 hold . Thus , nonzero C (cid:7) is equivalent to logarithmic k ( n ) ∼ log n and corre - sponds to the regime of balanced potentiation with p 1 (cid:7) ∈ ( 0 ; 1 ) . For sublog - arithmic k ( n ) the potentiated ( 1 - ) synapses in the memory matrix A are sparse , that is , p 1 (cid:7) → 0 , and for supralogarithmic k ( n ) potentiated synapses are dense , that is , p 1 (cid:7) → 1 . Both cases , however , imply C → 0 . We will reevaluate these cases of sparse and dense potentiation , which appear in - efﬁcient in the light of network capacity , in the following section using the performance measures of information capacity and synaptic capacity that we introduced in section 1 . 2 . 4 Analysis of Information Capacity and Synaptic Capacity 4 . 1 Information Capacity . Information capacity ( see equation 1 . 2 ) re - lates the stored ( retrievable ) information to the memory resources required by implementation of an associative memory . Thus , information capacity measures how well a speciﬁc implementation exploits its physical substrate . For example , the standard implementation of a Willshaw network allocates one bit of physical memory for each of the mn synapses . Therefore , for a matrix load of p 1 = 0 . 5 , the information capacity is identical to the network capacity studied in section 3 . However , if the memory load is p 1 (cid:14) = 0 . 5 , implementations that include a compression of the memory matrix can achieve an information capacity that exceeds the network capacity . Optimal compression of the memory matrix A by Huffman ( 1952 ) or Golomb ( 1966 ) coding ( the latter works in cases p 1 → 0 or p 1 → 1 ) can Memory Capacities for Synaptic and Structural Plasticity 309 decrease the required physical memory by a factor according to the Shannon information I ( p 1 ) : = − p 1 ld p 1 − ( 1 − p 1 ) ld ( 1 − p 1 ) of a synaptic weight ( see appendix A ) . 1 Thus , with equation 3 . 9 , the information capacity C I for optimal compression is written as C I (cid:7) : = C (cid:7) I ( p 1 (cid:7) ) ≈ λ ln p 1 (cid:7) ln ( 1 − p 1 (cid:7) ) − p 1 (cid:7) ln p 1 (cid:7) − ( 1 − p 1 (cid:7) ) ln ( 1 − p 1 (cid:7) ) η . ( 4 . 1 ) Equation 4 . 1 reveals the surprising result that in the optimally com - pressed Willshaw model , the balanced regime is outperformed by the dense and sparse regimes , which both allow approaching the theoretical upper bound of information capacity C I → λη . For small p 1 (cid:7) → 0 , we have I ( p 1 (cid:7) ) ≈ − p 1 (cid:7) ld p 1 (cid:7) and ln ( 1 − p 1 (cid:7) ) ≈ − p 1 (cid:7) , and therefore C I → λη . For large p 1 (cid:7) → 1 , we have I ( p 1 (cid:7) ) ≈ − ( 1 − p 1 (cid:7) ) ld ( 1 − p 1 (cid:7) ) , and therefore also C I ≈ ( − ln p 1 (cid:7) ) / ( 1 − p 1 (cid:7) ) → λη . Thus , a high - ﬁdelity asymptotic in - formation capacity of λ ∈ ( 0 ; 1 ] is possible for sparse and dense potenti - ation , that is , p 1 (cid:7) → 0 or p 1 (cid:7) → 1 , for n → ∞ and η → 1 ( see section 3 . 4 ; cf . Knoblauch , 2003a ) . This ﬁnding is nicely illustrated by the plots of network and informa - tion capacity in Figures 5 and 6 . The classical maximum of the network capacity C in the balanced regime coincides with the local minimum of the information capacity C I . For all values p 1 (cid:7) (cid:14) = 0 . 5 , the information capacity surmounts the network capacity and reaches in the sparse and dense regime the theoretical optimum C I = 1 . Although networks of reasonable size can - not achieve the theoretical optimum at high retrieval quality , the capacity increases are still considerable , in particular for very sparse activity ( e . g . , k = 2 ) . Moreover , there is a wide range in pattern activity k in which the information capacity C I exceeds the network capacity C assumed at its nar - row optimum . Thus , evaluating the capacity of compressed networks more appropriately by C I avoids the “sparsity” and “capacity gap” problems of C discussed in section 1 . 4 . A simple alternative method of synaptic compression would be to form target lists of sparse or dense matrix entries . One can simply store for each address neuron i an index list of postsynaptic targets or nontargets—for p 1 < 0 . 5 , the list represents the one - entries in the memory matrix and for p 1 > 0 . 5 the zero - entries . For the latter case , one can adapt the retrieval algo - rithm in an obvious way such that each 0 - synapse decreases the membrane potential of the postsynaptic neuron ( see Knoblauch , 2003b , 2006 ) . The tar - get list requires min ( p 1 , 1 − p 1 ) mn ld n bits of physical memory if we neglect 1 This compression factor is approximate since it assumes independence of the matrix elements , which is not fulﬁlled for the storage of distributed patterns . Nevertheless , nu - merical simulations described in Knoblauch et al . ( 2008 ) show that the actual compression factor comes very close to I ( p 1 ) . 310 A . Knoblauch , G . Palm , and F . Sommer −14 −12 −10 −8 −6 −4 −2 0 2 4 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 p 1 ε C ε C S ε C I ε C I ′ε n = 10 5 k = 18 λ = 1 −14 −12 −10 −8 −6 −4 −2 0 2 4 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 12 14 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 p 1 ε C I ε C ε C I ′ε C S ε n = 10 5 , ε = 0 . 01 λ = 1 0 2 4 6 8 10 12 140 1 2 3 4 5 6 7 8 9 10 p 1 p 1 pattern activity log k 2 high fidelity ε log 10 I’ I I’ I s t o r a g e ca p a c it y C S ca p ac iti e s C , C , C , m e m o r y l o a d ca p ac iti e s C , C , C , m e m o r y l o a d s t o r a g e ca p a c it y C S ( b ) ( a ) Figure 5 : Capacity measures C I and C S for a ﬁnite Willshaw network with structural compression . Parameters are as in Figure 3 ( square weight matrix with m = n = 10 5 , equal pattern activities k = l , zero input noise with λ = 1 , κ = 0 ) . The plots show information capacity C I (cid:7) for optimal Huffman - Golomb compression ( medium solid ) , information capacity C I (cid:15) (cid:7) for simple target lists ( thin line ) , and synaptic capacity C S (cid:7) ( dash - dotted line ) . For reference , the plots show also network capacity C (cid:7) ( thick solid line ) and matrix load p 1 (cid:7) ( dashed line ) . Capacities are drawn as either functions of k for ﬁxed ﬁdelity parameter (cid:7) = 0 . 01 ( a ) or functions of (cid:7) for ﬁxed k = 18 ( b ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 C ε C S ε C I ε C I ′ε k = 5 / k = sqrt ( n ) ε = 0 . 01 λ = 1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 1 . 1 C ε = ld p 1 ε ln ( 1−p 1 ε ) C I ε = C ε / I ( p 1 ε ) C S ε = C ε / min ( p 1 ε , 1−p 1 ε ) 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 10 1 2 3 4 5 6 7 8 9 10 11 log 10 n # neurons 1ε p I I’ s t o r a g e ca p ac iti e s C , C I memory load s t o r a g e ca p ac it y C S s t o r a g e ca p ac iti e s C , C , C S s t o r a g e ca p ac it y C ( a ) ( b ) Figure 6 : Capacity measures C I and C S for the compressed Willshaw model in the asymptotic limit n → ∞ . Parameters are as in Figure 4 : m = n , k = l , λ = 1 , κ = 0 . ( a ) Information capacity C I (cid:7) ( solid line ) and synaptic capacity C S (cid:7) ( dash - dotted line ) as functions of the matrix load p 1 (cid:7) . For reference , the plot also shows network capacity C (cid:7) ( bold line ) . The maximum of C at p 1 (cid:7) = 0 . 5 turns out to be the minimum of C I and C S . For sparse or dense potentiation with p 1 (cid:7) → 0 or p 1 (cid:7) → , both C I (cid:7) → 1 and C S (cid:7) ∼ ln n → ∞ achieve their theoretical bounds . ( b ) Storage capacities C (cid:7) , C I (cid:7) , C I (cid:15) (cid:7) ( thin line ) , and C S (cid:7) as functions of the network size n for pattern activities k ( n ) = 5 ( black line ) and k ( n ) = √ n ( gray line ) assuming (cid:7) = 0 . 01 ( cf . Figure 4b ) . While C (cid:7) → 0 it is C I (cid:7) → 1 and C S (cid:7) → ∞ for both functions k ( n ) . C I (cid:15) (cid:7) → 1 / k = 0 . 2 for k ( n ) = 5 . C I (cid:15) (cid:7) → 0 . 5 for k ( n ) = √ n ( see Table 1 ) . Memory Capacities for Synaptic and Structural Plasticity 311 the additional memory required for m “memory pointers” linking the tar - get lists to the memory matrix . 2 Thus , for large n , the resulting compression factor is min ( p 1 , 1 − p 1 ) ld n . With equation 3 . 9 , this yields the information capacity for the Willshaw model with the synaptic target list : C I (cid:15) (cid:7) : = C (cid:7) min ( p 1 (cid:7) , 1 − p 1 (cid:7) ) ld n ≈ λ ld p 1 (cid:7) · ln ( 1 − p 1 (cid:7) ) min ( p 1 (cid:7) , 1 − p 1 (cid:7) ) ld n η . ( 4 . 2 ) Figure 5 shows that the information capacity for target list compression C I (cid:15) stays far below the information capacity for optimal compression C I . As the asymptotic analyses below will show , target list compression achieves the theoretical optimum C I (cid:15) = 1 only for dense potentiation with nearly linear k ( n ) . Nevertheless , target list compression achieves C I (cid:15) > C for very small or quite large k ( e . g . , k ≤ 5 , k ≥ 177 for n = 10 5 ) . The next section shows that C I (cid:15) has characteristics very similar to synaptic capacity C S , which is more relevant for biological networks . 4 . 2 Synaptic Capacity . Information capacity is clearly important for technical implementations of associative memories on sequential standard computers . But for the brain and also parallel VLSI hardware , it might not be the information content of the required physical memory that really matters . Rather , what matters may be the physiological resources necessary for the physical implementation of the network . For example , the synaptic capacity deﬁned in equation 1 . 3 measures the mutual information in the memory task per functional synapse . Thus , the physiological resources taken into account are the number of functional synapses , that is , the one - entries in the synaptic matrix , while we assume that silent synapses , the zero - entries , are metabolically cheap and could even be pruned . The synaptic capacity of the Willshaw model can be written as C S (cid:7) : = C (cid:7) min ( p 1 (cid:7) , 1 − p 1 (cid:7) ) = C I (cid:15) (cid:7) ld n ≈ λ ld p 1 (cid:7) · ln ( 1 − p 1 (cid:7) ) min ( p 1 (cid:7) , 1 − p 1 (cid:7) ) η , ( 4 . 3 ) with η from equations 3 . 11 and 3 . 10 . Note that C S and C I (cid:15) in equation 4 . 2 are proportional by a factor of ld n . Another similarity to implementations with target list compression is that in the range of dense connectivity , that is , p 1 > 0 . 5 , the synaptic capacity counts the synaptic resources required by an inhibitory network implementation that represents the less frequent ( 1 − p 1 ) mn zero - entries in the memory matrix with functional synapses ( cf . Knoblauch , 2003b , 2006 ) . Such inhibitory implementations of associative memory have been proposed for the cerebellum ( Kanerva , 1988 ; Marr , 1969 ; Albus , 1971 ) and might also be relevant for the basal ganglia ( Wilson , 2004 ) . 2 This is negligible for large n if on average a matrix row contains many sparse entries , min ( p 1 , 1 − p 1 ) n (cid:16) 0 , that is , if a neuron has many functional synapses , which is usually true . 312 A . Knoblauch , G . Palm , and F . Sommer Figure 5a shows for m = n = 10 5 that the Willshaw model can store up to 8 . 5 bits per synapse for k = l = 2 , which exceeds the asymptotic network capacity C ≤ 0 . 7 bits per synapse by more than one order of magnitude . As for information capacity , the very steep capacity increase for ultrasparse patterns , k → 2 , is remarkable . For moderately sparse patterns and dense potentiation ( p 1 (cid:7) → 1 ) , our analysis ( see equation 4 . 3 ) suggests synaptic capacities of up to C S ≈ 4 . 9 bits per synapse for k = 9281 . However , it turns out that the underlying ap - proximation , equation 3 . 3 , of C S and C I can become inaccurate for large cell assemblies ( see appendixes B and C ) . Unfortunately , the true values of C S are signiﬁcantly smaller , and the maximum occurs for smaller k ( see also Table 2 for λ = 0 . 5 ) . The reason is that C S is very sensitive to the com - pression factor 1 − p 1 (cid:7) . Thus , even if the true value of M (cid:7) is only a little bit smaller than suggested by equation 3 . 8 , the corresponding value of 1 − p 1 (cid:7) , and therefore the compressibility of the memory matrix , can be strongly affected for p 1 (cid:7) → 1 ( see appendix C for more details ; see also section 4 . 4 ) . In contrast , this effect is not present for ultrasparse patterns with p 1 (cid:7) → 0 . Figures 6a and 5b suggest that C S → ∞ for p 1 (cid:7) → 0 or p 1 (cid:7) → 1 and very low ﬁdelity (cid:7) → ∞ , respectively . This means that in principle , it is possible to store an inﬁnite amount of information per synapse . Strictly speaking this is true only for inﬁnitely large networks with n → ∞ because the synaptic capacity C S is limited by the the number of possible spatial locations , that is , C S ≤ ld n . Note that this is the essential difference between the concepts of synaptic capacity and network capacity : The maximum of network capacity per ﬁxed synapse is determined only by the number of potential synaptic weight states induced by Hebbian plasticity ( 0 or 1 in the Willshaw model ) . In contrast , the maximum of synaptic capacity additionally considers the number of potential locations where the synapse can be placed by structural plasticity . The following two sections derive explicit formulas for storage capacities and memory load for the regimes of sparse and dense potentiation ( see section 3 . 4 ) . Table 1 summarizes all the results for the case m = n → ∞ , k = l , noiseless addresses λ = 1 and κ = 0 , and retrieval - quality grade RQ1 with constant (cid:7) ∼ 1 . 4 . 3 Capacities for Sparse Synaptic Potentiation . For sparse synaptic potentiation , we have p 1 (cid:7) → 0 and typically sublogarithmic pattern activity k with k / ld n → 0 ( see section 3 . 4 ; cf . Table 1 ) . With − ln ( 1 − p 1 (cid:7) ) ≈ p 1 (cid:7) and I ( p 1 (cid:7) ) ≈ − p 1 (cid:7) ld p 1 (cid:7) we obtain from equations 3 . 7 , 3 . 2 , 3 . 9 , 4 . 1 , 4 . 2 , and 4 . 3 for large m , n → ∞ : M (cid:7) ≈ (cid:16) (cid:7) l n − l (cid:17) 1 λ k mn kl ≈ (cid:7) 1 λ k m k (cid:18) n l (cid:19) 1 − 1 λ k ( 4 . 4 ) Memory Capacities for Synaptic and Structural Plasticity 313 T a b l e 1 : A s y m p t o t i c R e s u l t s f o r H i g h - F i d e li t y M e m o r y L o a d p 1 (cid:7) , S t o r a b l e P a tt e r n s M (cid:7) , a n d N e t w o r k C a p a c i t y C (cid:7) , I n f o r m a t i o n C a p a c i t i e s C I (cid:7) f o r O p t i m a l C o m p r e ss i o n a n d C I (cid:15) (cid:7) f o r S i m p l e T a r g e t L i s t s , a n d S y n a p t i c C a p a c i t y C S (cid:7) . k p 1 (cid:7) M (cid:7) C (cid:7) C I (cid:7) C I (cid:15) (cid:7) C S (cid:7) c 0 ∼ n 2 − 1 / c 0 1 1 / c ( l d n ) / c → ∞ c ( l n n ) d , 0 < d < 1 0 ∼ n 2 − 1 / ( c ( l n n ) d ) / ( l n n ) 2 d 0 1 0 ∼ ( l n n ) 1 − d → ∞ l d n 0 . 5 ( l n 2 ) n 2 / ( l d n ) 2 l n 2 ≈ 0 . 69 l n 2 0 2 l n 2 c l n n e x p ( − 1 / c ) ∼ n 2 / ( l n n ) 2 ∈ ( 0 ; l n 2 ) ∈ ( l n 2 ; 1 ) 0 ( 2 l n 2 ; ∞ ) c ( l n n ) d , 1 < d 1 ∼ n 2 l n l n n / ( l n n ) 2 d 0 1 0 ∼ l n l n n → ∞ √ n 1 0 . 5 n l n n 0 1 0 . 5 0 . 5 l d n → ∞ c n d , 0 < d < 1 1 ∼ n 2 − 2 d l n n 0 1 d d l d n → ∞ c n , 0 < c < 1 1 ( l n n ) / ( − c l n ( 1 − c ) ) 0 0 0 0 N o t e : H e r e w e c o n s i d e r o n l y t h e s p e c i a l c a s e o f k = l , m = n → ∞ , n o i s e l e ss a dd r e ss p a tt e r n s ( λ = 1 , κ = 0 ) , a n d c o n s t a n t ﬁ d e li t y p a r a m e t e r (cid:7) ∼ 1 c o rr e s p o n d i n g t o q u a li t y r e g i m e R Q 1 . 314 A . Knoblauch , G . Palm , and F . Sommer C (cid:7) ≈ (cid:2) (cid:7) l n − l (cid:3) 1 λ k ld (cid:7) l n − l k η → 0 ( 4 . 5 ) C I (cid:7) ≈ λη ≤ λ ( 4 . 6 ) C I (cid:15) (cid:7) ≈ ld (cid:7) l n − l k ld n · η ≤ 1 / k ( 4 . 7 ) C S (cid:7) ≈ ld (cid:7) l n − l k · η ≤ ld n k . ( 4 . 8 ) The second approximation in equation 4 . 4 is valid only for l (cid:6) n . Thus , for sparse potentiation , we can still store a very large number of ultrasparse patterns where M scales almost with mn , for large k . However , note that for given m , n , maximal M is obtained for logarithmic k ( cf . Figure 4a ) . The classical network capacity C vanishes for large n , but for optimal compres - sion , we obtain an information capacity with C I → 1 . For simple target lists ( see above ) , the information capacity approaches C I (cid:15) → 1 / k . Thus , C I (cid:15) is nonzero only for small constant k . For constant k = 1 , we have trivially C I (cid:15) → 1 . However , this result is not very interesting since for k = 1 , we have no really distributed storage . For k = 1 , there are only M = m possible patterns to store , and the memory matrix degenerates to a look - up table . Section 5 discusses more closely the relation between the Willshaw model and different implementations of look - up tables . For the synaptic capacity , we have C S (cid:7) ∼ log n → ∞ for constant k ∼ 1 , which comes very close to the theoretical optimum C S ≤ ld n , the informa - tion necessary to determine the target cell of a given synapse among the n potential targets in the content population . Most interestingly , C S and C I (cid:15) are independent of the fault tolerance parameter λ ( and consequently must also be independent of the high - ﬁdelity parameter (cid:7) ) . Thus , decreasing M from M = M (cid:7) to M = 1 virtually does not affect either C S or C I (cid:15) . Note that for a single stored pattern , C S = ( ld (cid:2) nl (cid:3) ) / ( kl ) ≈ ( ld n ) / k reaches the upper bound of equation 4 . 8 . 4 . 4 Capacities for Dense Synaptic Potentiation . For dense synap - tic potentiation , we have p 1 (cid:7) → 1 and typically supralogarithmic pattern activity k with k / ld n → ∞ ( see section 3 . 4 ; cf . Table 1 ) . With I ( p 1 (cid:7) ) ≈ − ( 1 − p 1 (cid:7) ) ld ( 1 − p 1 (cid:7) ) and 1 − p 1 (cid:7) ≈ − ln p 1 (cid:7) we obtain from equations 3 . 7 , 3 . 2 , 3 . 9 , 4 . 1 , 4 . 2 , and 4 . 3 for large n → ∞ : 1 − p 1 (cid:7) ≈ ln n − l (cid:7) l λ k → 0 ( 4 . 9 ) M (cid:7) ≈ mn kl (cid:16) ln ( λ k ) − ln ln n − l (cid:7) l (cid:17) ( 4 . 10 ) Memory Capacities for Synaptic and Structural Plasticity 315 C (cid:7) ≈ (cid:16) ln ( λ k ) − ln ln n − l (cid:7) l (cid:17) ld n − l (cid:7) l k · η → 0 ( 4 . 11 ) C I (cid:7) ≈ λη ≤ λ ( 4 . 12 ) C I (cid:15) (cid:7) ≈ λ · ln ( λ k ) − ln ln n − l (cid:7) l ln n ≤ λ ln k ln n ( 4 . 13 ) C S (cid:7) ≈ λ · ld ( λ k ) − ld ln n − l (cid:7) l ≤ λ ln n . ( 4 . 14 ) Although the pattern capacity M (cid:7) is much smaller than for balanced and sparse synaptic potentiation , here we can still store many more moder - ately sparse patterns than there are neurons ( M (cid:16) n ) as long as k ≤ √ n ( see equation 4 . 10 ; cf . Table 1 ) . The classical network capacity C vanishes for large n , but for optimal compression , we obtain a high information capacity C I → 1 . Surprisingly , information capacity can approach the max - imum even for nonoptimal compression . For k = n d and 0 < d < 1 , we obtain C I (cid:15) → λ d from equation 4 . 13 . Similarly , synaptic capacity achieve its upper bound , C S ≤ ld n , for k = n d with d → 1 . Note that here , C I (cid:15) and C S achieve factor two larger values than for sparse potentiation and distributed storage with k ≥ 2 ( see equations 4 . 7 and 4 . 8 ) . However , the convergence appears to be extremely slow for high ﬁdelity ( see appendix B ; see also Knoblauch , 2008 ) , and for d > 0 . 5 we obtain asymptotically only M < n ( see equation 4 . 10 ; cf . Table 1 ; see also section 5 ) . For dense synaptic potentiation , both C I (cid:15) and C S depend on the fault tolerance requirement λ and the high - ﬁdelity parameter (cid:7) , unlike sparse synaptic potentiation , where these capacities are independent from λ . Un - fortunately , requiring high ﬁdelity and fault tolerance counteracts the com - pressibility of the memory matrix because I ( p 1 ) increases for decreasing p 1 > 0 . 5 . This results in the counterintuitive fact that the amount of nec - essary physical memory increases with the decreasing number of stored patterns M . As can be seen in Figure 5a , both information capacities C I and C I (cid:15) and synaptic capacity C S exhibit local maxima at k I opt and k S opt ( = k I (cid:15) opt ) for k > ld n . In Knoblauch ( 2003b , appendix B . 4 . 2 ) these maxima are computed ( not shown here ) . The resulting asymptotic optima are approximately k S opt ∼ n · ( e √− ln (cid:7) ) −√ ln n ( 4 . 15 ) k I opt ∼ n 1 − − ln (cid:7) − √ − ln (cid:7) − ln (cid:7) − 1 . ( 4 . 16 ) Note that k S opt grows faster than n d for any d < 1 , but slower than the upper bound n / log 4 n , where our theory based on the binomial approximation equation 3 . 3 , is valid . 316 A . Knoblauch , G . Palm , and F . Sommer For linear k = cm and l = dn , the binomial approximation is invalid , and we have to use alternative methods as described in appendix C . Here the Willshaw model can store only M ∼ log m pattern associations with van - ishing storing capacities C , C I , C S → 0 . There are much better alternative modelsforthisparameterregime . Forexample , theclassicalHopﬁeldmodel can store a much larger number of M = 0 . 14 n nonsparse patterns resulting in 0 . 14 bits per ( nonbinary ) synapse ( Hopﬁeld , 1982 ; Amit et al . , 1987a , 1987b ) . Thus , for nonsparse patterns , synapses with gradual weight states such as employed in the Hopﬁeld model appear to make a big difference to binary clipped Hebbian learning , as in the Willshaw model . 4 . 5 Remarks on Fault Tolerance and Attractor Shape . How does in - creasing noise ( 1 − λ , κ ) in the query patterns ˜u affect the number of storable patterns M (cid:7) and the other capacity measures ( C (cid:7) , C I (cid:7) , C S (cid:7) ) for a given net - work size and pattern activity ? 3 It is particularly simple to answer this question for pattern part retrieval where query patterns contain miss noise only ( κ = 0 ) . Using equations 3 . 2 and 3 . 7 , we can introduce the fraction of storable patterns as a function of the query noise λ , m λ : = M (cid:7) ( λ ) M (cid:7) ( 1 ) ≈ ln ( 1 − p 1 (cid:7) ( λ ) ) ln ( 1 − p 1 (cid:7) ( 1 ) ) ∈ ( 0 ; 1 ] (cid:20) ≈ p 1 (cid:7) ( 1 ) ( 1 − λ ) / λ ) → 0 , p 1 (cid:7) ( 1 ) → 0 → 1 , p 1 (cid:7) ( 1 ) → 1 , ( 4 . 17 ) where we used ln ( 1 − p 1 (cid:7) ) ≈ − p 1 (cid:7) for p 1 (cid:7) → 0 and de l’Hˆopital’s rule for p 1 (cid:7) → 1 . The fraction of storable patterns with increasing fault tolerance differs markedly for the regimes of sparse , balanced , and dense synaptic potentiation ( cf . sections 3 . 4 , 4 . 3 , and 4 . 4 ) : Figure 7a shows that the decrease is steep for very sparse memory patterns and p 1 (cid:7) → 0 and shallow for mod - erately sparse patterns and p 1 (cid:7) → 1 . Thus , relatively large cell assemblies with k (cid:16) log n are much more robust against miss noise than small cell as - semblies with k ≤ log n ( cf . Table 1 ) . The same conclusion is true for network capacity , C (cid:7) ( λ ) : = m λ · C (cid:7) ( 1 ) ( see equations 3 . 4 and 3 . 9 ) . Increasing fault tolerance or attractor size of a memory will decrease not only M (cid:7) but also p 1 (cid:7) . Therefore , the compressibility of the memory matrix also will change . In analogy to m λ for M (cid:7) , we can compute the relative compressibility i λ for C I (cid:7) , i λ : = I ( p 1 (cid:7) ( λ ) ) I ( p 1 (cid:7) ( 1 ) ) (cid:20) ≈ p 1 (cid:7) ( 1 ) ( ( 1 − λ ) / λ ) / λ → 0 , p 1 (cid:7) ( 1 ) → 0 → 1 / λ , p 1 (cid:7) ( 1 ) → 1 , ( 4 . 18 ) 3 Note the difference between assessing fault tolerance for either a given memory load p 1 (cid:7) or given pattern activities k , l , since the former is a function of the latter . Memory Capacities for Synaptic and Structural Plasticity 317 m λ 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 5 1 1 . 5 2 2 . 5 c I λ 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 00 . 20 . 40 . 60 . 8 1 0 0 . 2 0 . 4 0 . 6 0 . 8 1 −0 . 02 0 0 . 02 0 . 04 0 . 06 p 1 λ e rr o r f ( λ , p1 ) λ λ λ i λ p = 0 . 9999 1 p = 0 . 0001 1 p = 0 . 1 1 p = 0 . 5 1 p = 0 . 0001 1 p = 0 . 1 1 p = 0 . 5 1 p = 0 . 9 1 p = 0 . 9999 1 p = 0 . 9999 1 p = 0 . 9 1 p = 0 . 5 1 p = 0 . 1 1 p = 0 . 0001 1 p = 0 . 9 1 ( a ) ( b ) ( c ) ( d ) Figure 7 : Impact of miss noise on the number of storable patterns and the compressibility of the memory matrix for different p 1 . Query patterns ˜u are assumed to contain λ k out of the k original ones , but no false ones ( κ = 0 ) . Here p 1 : = p 1 (cid:7) ( 1 ) is the maximal matrix load for λ = 1 ( see equation 3 . 7 ) . ( a ) Fraction of storable patterns m λ versus λ ( see equation 4 . 17 ) . ( b ) Relative compressibility i λ versus λ ( see equation 4 . 18 ) . ( c ) For all values of p 1 , we have c I λ : = m λ / i λ ≈ λ ( see equation 4 . 19 ) . ( d ) The error f ( λ , p 1 ) : = c I λ − λ of approximating c I λ by λ is small ( − 0 . 02 < f < 0 . 06 ) and even vanishes for p 1 → 0 and p 1 → 1 . where we used I ( p 1 ) ≈ − p 1 ld p 1 for p 1 (cid:7) ( 1 ) → 0 and de l’Hˆopital’s rule for p 1 (cid:7) ( 1 ) → 1 ( cf . Knoblauch , 2003b ) . The relative compressibility is depicted in Figure 7b . Note that always i λ < 1 for p 1 (cid:7) ( 1 ) < 0 . 5 , but usually i λ > 1 for p 1 (cid:7) ( 1 ) > 0 . 5 . The latter occurs for dense potentiation and moderately ( e . g . , supralogarithmically ) sparse address patterns ( see Table 1 ) and implies the counterintuitive fact that although fewer patterns are stored , more physical memory is required . Thus , the dependence of information capacity on miss noise is c I λ : = C I (cid:7) ( λ ) C I (cid:7) ( 1 ) = m λ i λ = λ + f ( λ , p 1 (cid:7) ( 1 ) ) ≈ λ , ( 4 . 19 ) 318 A . Knoblauch , G . Palm , and F . Sommer for a small error function f with f → 0 for p 1 (cid:7) → 0 and p 1 (cid:7) → 1 . The plots of c I λ in Figure 7c reveal the surprising result that the relative decrease in information capacity is almost linear in λ in all the regimes of pattern sparsity . One can verify numerically that − 0 . 02 < f ( λ , p 1 ) < 0 . 06 for λ , p 1 ∈ ( 0 ; 1 ) ( see Figure 7d ) . Similar considerations for the synaptic capacity C S ( that apply also to information capacity C I (cid:15) ) reveal that c S λ : = C S (cid:7) ( λ ) C S (cid:7) ( 1 ) = m λ min ( p 1 (cid:7) ( 1 ) , 1 − p 1 (cid:7) ( 1 ) ) min ( p 1 (cid:7) ( λ ) , 1 − p 1 (cid:7) ( λ ) ) ≈ (cid:20) C S (cid:7) ( 1 ) , p 1 (cid:7) ( 1 ) → 0 λ C S (cid:7) ( 1 ) , p 1 (cid:7) ( 1 ) → 1 . ( 4 . 20 ) It is remarkable that C S is independent of λ for ultrasparse patterns with k / log n → 0 and sparse potentiation p 1 (cid:7) → 0 . Thus , decreasing M from M = M (cid:7) ( 1 ) to M = M (cid:7) ( λ ) affects neither C S nor C I (cid:15) . Actually , for a single stored pattern , C S = ( ld (cid:2) nl (cid:3) ) / ( kl ) ≈ ( ld n ) / k is identical to the upper bound of equation 4 . 8 . Thus , C S (cid:7) ( λ ) actually increases for λ → 0 ( or (cid:7) → 0 ) . A theoretical analysis including add noise ( κ ≥ 0 ) is more difﬁcult ( cf . Palm & Sommer , 1996 ; Sommer & Palm , 1999 ; Knoblauch , 2003b ) . In numer - ical experiments , we have investigated retrieval quality as a function of miss noise ( λ < 1 ) and add - noise ( κ > 0 ) using exact expressions for retrieval er - rors p 01 and p 10 ( see equations B . 1 and B . 2 ) . For given network size ( here m = n = 1000 ) and sparsity level ( k = l = 4 , 10 , 50 , 100 , 300 ) , the number of stored patterns M has been chosen such that for noiseless query patterns ( λ = 1 , κ = 0 ) , a high - ﬁdelity criterion (cid:7) ≤ 0 . 01 was fulﬁlled . Then we com - puted retrieval quality for noisy query patterns ˜u with activity z : = | ˜u | . For z ≤ k , queries were pattern parts ( 0 < λ ≤ 1 , κ = 0 ) . For z > k , queries were supersets of the original address patterns ( λ = 1 , κ ≥ 0 ) . The retrieval qual - ity was measured by minimizing (cid:7) T : = ( T ( k / n , p 01 , p 10 ) − I ( k / n ) ) / I ( k / n ) with respect to the neuron threshold (cid:6) . Here (cid:7) T corresponds to the nor - malized information loss between retrieved and originally stored patterns , but using the Hamming distance based measure (cid:7) as deﬁned in section 3 . 2 leads qualitatively to the same results ( see Knoblauch et al . , 2008 ) . Figure 8a shows for each noise level the retrieval quality and Figure 8b the optimal threshold . These numerical experiments validate our theoretical results for pattern part retrieval ( without add noise ) . For λ < 1 , ultrasparse patterns ( e . g . , con - stant k = 4 ) appear to be very vulnerable to miss noise ( i . e . , (cid:7) increases very steeply with decreasing λ ) . In contrast , moderately sparse patterns ( e . g . , k = 1000 for n = 10 , 000 ) are much more robust against miss noise ( i . e . , the increase of (cid:7) is much weaker ) . On the other hand , our data also show that ultrasparse cell assemblies are very robust against add noise ( i . e . , the ﬁdelity Memory Capacities for Synaptic and Structural Plasticity 319 0 0 . 5 1 1 . 5 2 2 . 5 3 0 0 . 5 1 1 . 5 2 2 . 5 3 n = 1000 k = 4 , 10 , 50 , 100 , 300 300 100 50 10 4 ε T z / k = λ + κ Θ op t / k T z / k = λ + κ 0 0 . 5 1 1 . 5 2 2 . 5 3 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 n = 1000 k = 4 , 10 , 50 , 100 , 300 300 100 50 10 4 ( a ) ( b ) Figure 8 : Impact of query noise on the retrieval quality of the Willshaw model for m = n = 1000 neurons and different pattern activities k = l = 4 , 10 , 50 , 100 , 300 ( increasinglinethickness ) storing M = 4928 , 4791 , 663 , 207 , 27 patterns in each case ( corresponding to (cid:7) = 0 . 01 for noiseless queries ) . Data are computed from exact error probabilities ( see equations B . 1 and B . 2 ) . ( a ) Re - trieval quality (cid:7) T : = ( T ( k / n , p 01 , p 10 ) − I ( k / n ) ) / I ( k / n ) as a function of query pattern activity z = ( λ + κ ) k . The queries were noiseless for z / k = 1 , contained only miss noise for z / k < 1 ( i . e . , λ < 1 , κ = 0 ) , and contained only add noise for z / k > 1 ( i . e . , λ = 1 , κ > 0 ) . The threshold (cid:6) is chosen such that (cid:7) T ( λ , κ ) is minimized . ( b ) Optimal threshold (cid:6) opt for minimal (cid:7) T shown in a . The plots for (cid:7) instead of (cid:7) T are qualitatively the same ( Knoblauch et al . , 2008 ) . parameter (cid:7) increases only relatively slowly with increasing add noise level κ ) . In contrast , the large cell assemblies are quite vulnerable to add noise : Here (cid:7) increases very steeply with κ . Our results show that the attractors around memories u μ ( i . e . , the subspace of query patterns ˜u that map to v μ ) have only little similarity to spheres in Hamming space . Rather , for ultrasparse patterns ( k / log n → 0 ) , attractors are elongated toward query patterns with more add noise than miss noise , whereas for moderately sparse patterns ( k / log n → ∞ ) , attractors are elongated toward query pat - terns with more miss noise than add noise . Figure 8b illustrates another important difference between sparse and dense synaptic potentiation corresponding to ultrasparse or moderately sparse activity . For ultrasparse patterns , the optimal threshold depends mainly on λ , but only very weakly on κ . In contrast , for moderately sparse patterns , the optimal threshold has a strong dependence on both λ and κ . As a consequence , in particular for biological systems , it may be much easier to implement the optimal threshold for retrieving ultrasparse patterns . In a noisy regime with κ (cid:16) 0 , it will be sufﬁcient to simply choose a constant threshold identical to the assembly size , (cid:6) = k , assuming that information processing is usually accomplished with complete patterns , λ = 1 . This bears in particular the possibility of activating superpositions of many dif - ferent ultrasparse cell assemblies . Actually , a reasonable interpretation of 320 A . Knoblauch , G . Palm , and F . Sommer seemingly random or spontaneous ongoing activity ( Arieli , Sterkin , Grin - vald , & Aertsen , 1996 ; Softky & Koch , 1993 ) would be that a large number of small cell assemblies or synﬁre chains ( Abeles , 1982 ; Abeles , Bergman , Margalit , & Vaadia , 1993 ; Diesmann , Gewaltig , & Aertsen , 1999 ; Wennekers & Palm , 1996 ) are active at the same time , independent of each other . 5 Computational Complexity and Energy Requirements 5 . 1 Compressed and Uncompressed Willshaw Network . So far we have been concerned with the storage capacity and fault tolerance of the Willshaw associative memory . Another important question is how fast the information can be retrieved for implementation on a sequential digital computer . To retrieve a pattern in the Willshaw model , we have to compute potentials x = ˜uA and afterward apply a threshold on each component of x , that is , the retrieval time ( or number of retrieval steps ) is t Wseq = z · n + n ≈ zn , ( 5 . 1 ) where z : = ( λ + κ ) k is the query pattern activity . Note that retrieval time is dominated by synaptic operations . Thus , our temporal measure also has an interpretation in terms of energy consumption . However , for this interpretation , it may be more relevant to consider only nonsilent synapses ( see section 1 . 2 and Lennie , 2003 ; Laughlin & Sejnowski , 2003 ) , which is captured by the following analysis for the “compressed” model . Matrix compression ( or eliminating silent synapses ) in the sparse and dense connectivity regimes not only improves storage capacity but gener - ally accelerates retrieval . For sparse connectivity with p 1 → 0 , the memory matrix A contains sparsely one - entries , and computing the potentials x re - quires only p 1 · n steps per activated address neuron . Similarly , for dense connectivity with p 1 → 1 , we can compute the potentials by x = z − ˜uA (cid:15) where A (cid:15) : = 1 − A contains sparsely one - entries ( see also Knoblauch , 2006 ) . Thus , the retrieval time is t cWseq = c · z · n · min ( p 1 , 1 − p 1 ) , ( 5 . 2 ) where c is a ( small ) constant accounting for decompression of A ( or A (cid:15) ) , keeping track of neurons selected by A ( or A (cid:15) ) in a list , and ﬁnally applying the threshold to the neurons in that list ( note that zn min ( p 1 , 1 − p 1 ) may be (cid:6) n ) . Obviously , t cWseq / t Wseq → 0 , at least for sparse and dense potentiation with p 1 → 0 or p 1 → 1 . However , it may be unfair to compare the com - pressed to the uncompressed Willshaw model since the latter works in an optimal manner for p 1 = 0 . 5 , where compression is not possible . Thus , we may want to compare the two models for different pattern sparseness k , l . Such an approach has been conducted by Knoblauch ( 2003b ) showing that the compressed model is superior to the uncompressed even if one normal - izes the amount of retrieved information to the totally stored information . Memory Capacities for Synaptic and Structural Plasticity 321 5 . 2 Comparison to Look - Up Tables and “Grandmother Cell” Net - works . It has been pointed out that Willshaw associative memory can allow much faster access to stored pattern information than a simple look - up table ( e . g . , see Palm , 1987 ) . A look - up table implementation of associative mem - ory would require an M × m matrix U for the address pattern vectors and an M × n matrix V for the content patterns such that U μ = u μ and V μ = v μ for μ = 1 , . . . , M ( each matrix row corresponds to a pattern vector ) . We also refer to the look - up table as a grandmother cell model ( or brieﬂy grand - mother model ; cf . Knoblauch , 2005 ; Barlow , 1972 ) because its biological interpretation corresponds to a two - layer architecture where an intermedi - ary population contains M neurons , one “grandmother” cell for each stored association ( see section 2 . 3 ) . Thus , grandmother cell μ receives inputs via synapses corresponding to the μ th row of U . A winner - takes - all dynamics activates only the most excited grandmother cell , which can activate the content population according to the corresponding synaptic row in V . For naive retrieval using a query pattern ˜u , one would compare ˜u to each row of U and select the most similar u μ . If each row of U contains k (cid:6) m one - entries , we may represent each pattern by the ( ordered ) list of the positions ( indices ) of its one - entries . Then the retrieval takes only t nLUTseq = M · ( z + k ) . ( 5 . 3 ) Then for M / n → ∞ , we indeed have t nLUTseq / t Wseq ≥ M / n → ∞ . Thus , the Will - shaw model is more efﬁcient than a naive look - up table if we store more patterns M than we have content neurons n . However , in many cases , compressed look - up tables can be implemented more efﬁciently than the Willshaw model even for M (cid:16) n . So far , by rep - resenting lists of one - entries for each pattern in the look - up table , we have essentially compressed the matrix rows . However , it turns out that com - pressing the columns is always more efﬁcient ( Knoblauch , 2005 ) . If we optimally compress the columns of U ( e . g . , by Huffman or Golomb cod - ing , similar to the compressed Willshaw model ) , then information capacity becomes C I → 1 and a retrieval requires only t cLUTseq = c · z · M · k / m ( 5 . 4 ) steps . Compared with the compressed Willshaw model , this yields ν : = t seq cLUT t seqcW ≈ − ln ( 1 − p 1 ) l min ( p 1 , 1 − p 1 ) ≤ − ln ( 1 − p 1 (cid:7) ) l min ( p 1 (cid:7) , 1 − p 1 (cid:7) ) → ⎧⎪⎨ ⎪⎩ 1 / l , p 1 (cid:7) → 0 λ k l ln ( λ k ) − ln ln n (cid:7) l ln n (cid:7) l , p 1 (cid:7) → 1 , ( 5 . 5 ) 322 A . Knoblauch , G . Palm , and F . Sommer where we used 1 − p 1 (cid:7) ≈ − ln p 1 (cid:7) for p 1 (cid:7) → 1 . Remember from section 3 . 1 that the memory matrix is sparse ( p 1 (cid:7) → 0 ) , balanced ( 0 < δ < p 1 (cid:7) < 1 − δ ) , or dense ( p 1 (cid:7) → 1 ) for sublogarithmic , logarithmic , or supralogarithmic k ( n ) . Thus , the Willshaw model performs worse than the grandmother model for most parameters . The Willshaw model is unequivocally supe - rior only for asymmetric networks with large k and small l . If we require m = n and k = l ( e . g . , for autoassociation ) , the Willshaw model is superior with ν → λ d / ( 1 − d ) only for almost linear k = n d with 1 / ( 1 + λ ) < d < 1 . Look - up tables are also superior to distributed associative networks with respect to fault tolerance because they always ﬁnd the exact nearest neighbor . In order to have a fair comparison with respect to fault tolerance , we can dilute the look - up tables by randomly erasing one - entries in matrix U . This will further accelerate retrieval in look - up tables and cut even the remaining parameter range where the Willshaw model is superior ( Knoblauch et al . , 2008 ) . At least for asymmetric networks , there remains a narrow parameter range where the Willshaw model beats diluted look - up tables . This seems to be the case for large m , small l , n , and relatively small k ( but still large enough with supralogarithmic k / log n → ∞ to obtain dense potentiation ) . 5 . 3 Parallel Implementations . For full ( i . e . , synapse - ) parallel hard - ware implementations ( like brain tissue or VLSI chips ; Chicca et al . , 2003 ; Heittmann & R¨uckert , 2002 ) , the retrieval time is O ( 1 ) , and the remaining constant is mainly determined by the hardware properties . Here the lim - iting resource is the connectivity ( e . g . , the number of nonsilent synapses ) , and our analysis so far can be applied again . However , there are also neuron - parallel computers with reduced hard - ware connectivity . One big advantage of the Willshaw model is that there are obvious realizations for such architectures ( Palm & Palm , 1991 ; Ham - merstrom , 1990 ; Hammerstrom , Gao , Zhu , & Butts , 2006 ) . For example , on a computer with n processors ( one per neuron ) and a common data bus shared by all processors , a retrieval takes time t prlW = z + 1 . In comparison , a corresponding implementation of the grandmother model or a look - up table will require M processors and time t prlLUT = z + log M . In particular for M (cid:16) n , there is no obvious parallelization of look - up tables that would beat the Willshaw model . In summary , both the Willshaw and the grandmother model are efﬁcient ( t seq / M , t prl / n → 0 ) only for sparse address patterns . Nonsparse patterns re - quire additionally a sparse recoding ( or indexing ) as is done in multi - index hashing ( Greene et al . , 1994 ) . Although there are quite efﬁcient computer implementations , it appears that distributed neural associative memories have only minor advantages over compressed look - up tables or multi - index hashing , at least for solving the best match problem on sequential computers . On particular parallel computers , the Willshaw model remains superior . Memory Capacities for Synaptic and Structural Plasticity 323 6 Summary and Discussion Neural associative memories are promising models for computations in the brain ( Hebb , 1949 ; Anderson , 1968 ; Willshaw et al . , 1969 ; Marr , 1969 , 1971 ; Little , 1974 ; Gardner - Medwin , 1976 ; Braitenberg , 1978 ; Hopﬁeld , 1982 ; Amari , 1989 ; Palm , 1990 ) , as well as potentially useful in technical applica - tions such as cluster analysis , speech and object recognition , or information retrieval in large databases ( Kohonen , 1977 ; Bentz et al . , 1989 ; Prager & Fallside , 1989 ; Greene et al . , 1994 ; Knoblauch , 2005 ; Mu et al . , 2006 ; Rehn & Sommer , 2006 ) . In this review , we have raised the question of how to evaluate the ef - ﬁciency of associative memories , that is , how to quantify the achieved computation and the used resources . The common measure of efﬁciency is network capacity , that is , the amount of information per synapse that can be stored in a network of ﬁxed structure ( Willshaw et al . , 1969 ; Palm , 1980 , 1991 ; Amit et al . , 1987a , 1987b ; Nadal , 1991 ; Buckingham & Willshaw , 1992 ; Sommer & Palm , 1999 ; Bosch & Kurfess , 1998 ) . Here we have ar - gued that network capacity is biased because it disregards the entropy of the synapses and thus underestimates models with low synaptic entropy and overestimates models with high synaptic entropy . To account for the synaptic entropy , it was necessary to introduce information capacity , a new performance measure . Interestingly , network capacity and information ca - pacity draw radically different pictures in what range associative memories work efﬁciently . For example , the Willshaw model is known to optimize the network capacity if the distribution of 0 - synapses and 1 - synapses is even and thus the synaptic entropy is maximal ( Willshaw et al . , 1969 ; Palm , 1980 ) . In contrast , the Willshaw model reaches the optimum information capacity in regimes of small synaptic entropy if either almost all synapses remain silent ( sparse potentiation with memory load p 1 → 0 ) or if almost all synapses are active ( dense potentiation with memory load p 1 → 1 ) . We have shown that the regimes of optimal information capacity that we dis - covered have direct practical implications . Speciﬁcally , we have constructed models of associative memory using mechanisms like Huffman or Golomb coding for synaptic compression , which can outperform their counterparts without matrix compression . Further , the discovery of regimes in associative memories with high information capacity could be a key to understanding the computational function of the various types of structural plasticity in the brain . In struc - tural plasticity , functionally irrelevant silent synapses are pruned and re - placed by new synapses generated at other locations in the network . This process can lead to a sparsely connected neural network in which each synapse carries a large amount of information about previously learned patterns ( Knoblauch , 2009 ) . To quantify the effects of structural plasticity , we have introduced the deﬁnition of synaptic capacity , which measures the information stored per functionally necessary synapse ( i . e . , not counting 324 A . Knoblauch , G . Palm , and F . Sommer silent synapses , which could be pruned ) . Our model analyses indicate that information capacity and synaptic capacity become optimal in the same regimes of operation . Thus , structural plasticity can be understood as a form of synaptic compression required to optimize information capacity in biological networks . Although our new deﬁnitions of performance measures for associative memories are general , for practical reasons we had to restrict the model analysistotwosimpleyetinterestingexamplesofassociativememories . The simplest possible version is a linear associative memory in which learning corresponds to forming the correlation matrix of the data and retrieval corresponds to a matrix - vector multiplication ( Kohonen , 1977 ) . However , the efﬁciency of linear associative memories is very limited . The cross - talk can be predicted to set in if the stored patterns deviate from the principal components of the data , which will necessarily be the case if the number of stored patterns exceeds the dimension of the patterns . The Willshaw model is a feedforward neural network similar to the linear associative memory but much more efﬁcient by any standards , because nonlinearities in the neural transfer function and in the superposition of memory traces keep the cross - talk small , even if the number of stored patterns scales almost with the square of the dimension of the patterns ( Willshaw et al . , 1969 ; Palm , 1980 ) . Thus , we chose to analyze the Willshaw network . In addition , to compare neural associative memories to look - up tables ( LUT ) , the classical structure for content - addressable memory in computer science , we also analyzed a two - layer extension of the Willshaw network with winner - take - all ( WTA ) activation in the hidden layer , which implements a look - up table . Previous analyses of the Willshaw network revealed that network capac - ity is optimized in a regime in which stored patterns are sparse ( the number of active units grows only logarithmically in the network size , k ∼ log n ) and the number of stored patterns grows as n 2 / ( log n ) 2 ( Willshaw et al . , 1969 ; Palm , 1980 ) . However , these analyses determined the upper bound of the network capacity with the level of retrieval errors undeﬁned . In practice , computations rely on a speciﬁc and guaranteed level of retrieval quality . Therefore , for fair and meaningful comparisons of the three deﬁnitions of storage capacity , network , and information and synaptic capacity , we had to develop new analytical procedures to quantify the different capacities at a deﬁned level of retrieval errors . The new analyses revealed three important new results . First , implicit in classical analyses , a high network capacity 0 < C ≤ ln 2 ≈ 0 . 69 or 0 < C ≤ 1 / e ln 2 ≈ 0 . 53 is restricted to a very narrow range of logarithmic pattern sparseness ( see section 3 . 4 and appendix D ) . Second , the information and synaptic capacities assume high values for quite wide ranges of pattern ac - tivities ( see Figure 5 ) . Third , the optimal regimes of information and synap - tic capacities , C I → 1 and C S ∼ log n , coincide but are distinct from the optimal regime for network capacity . For example , the information capac - ity has the minimum in the regime of optimal network capacity and assumes Memory Capacities for Synaptic and Structural Plasticity 325 the theoretical optimum C I → 1 either for ultrasparse patterns k / log n → 0 or for moderately sparse patterns k / log n → ∞ ( see Perez - Orive et al . , 2002 ; Hahnloser , Kozhevnikov , & Fee , 2002 ; Quiroga , Reddy , Kreiman , Koch , & Fried , 2005 ; Waydo , Kraskov , Quiroga , Fried , & Koch , 2006 , for experimental evidence supporting sparse representations in the brain ) . In addition , the new analyses revealed how the robustness of content - addressable memory against different types of noise in the address patterns varies in the different regimes of operation . While the effects of additional activity ( add errors ) and missing activity ( miss errors ) were quite balanced for log - sparse patterns ( see Figure 8 ) , the effects strongly varied with error type in the ultrasparse and moderately sparse regime . Speciﬁcally , the re - trieval of ultrasparse patterns ( k (cid:6) log n ) was robust against add errors in the address pattern but vulnerable to miss errors . The inverse relation was found for the retrieval of moderately sparse patterns . Thus , the ultrasparse regime could be of particular interest if a memory has to be recognized in superpositions of many patterns , whereas the moderately sparse regime allows completing a memory pattern from a small fragment . The retrieval speed deﬁned as the time ( or number of computation steps ) required to retrieve a pattern is another important performance measure for associative memory . Previous work has hypothesized that neural asso - ciative memory is an efﬁcient means for information retrieval in the context of the best match problem ( Minsky & Papert , 1969 ) , even when imple - mented on conventional computers . For example , Palm ( 1987 ) has argued that distributed neural associative memory would have advantages over local representations such as in the LUT network . While this may hold true for plain ( uncompressed ) and parallel implementations ( Hammerstrom , 1990 ; Palm & Palm , 1991 ; Knoblauch , 2003b ; Chicca et al . , 2003 ) , we showed in section 5 that the compressed LUT network implemented on a sequential architecture outperforms the Willshaw network for almost all parameters ( seeequation5 . 5 ) . Asymptotically , sequentialimplementationsofthesingle - layer Willshaw model remain superior only for almost nonsparse patterns ( k ∼ n d with d near 1 ) or if content patterns are much sparser than address patterns . The neurobiological implications of the new efﬁcient regimes we dis - covered in the Willshaw model ( sparse and dense synaptic potentiation corresponding to ultrasparse and moderately sparse patterns ) rely on two oversimpliﬁcations that need to be addressed in future work . First , our analyses have assumed that learning starts in a fully connected network and is followed by a pruning phase , where the silent dispensable synapses can be pruned . Since neural networks of the brain have generally low connectivity at any time , this highly simpliﬁed model must be reﬁned . Currently we investigate a more realistic model for cortical memory in which a low - capacity memory buffer network ( e . g . , the hippocampus ) interacts with a high - capacity associative projection ( e . g . , a cortico - cortical synaptic connection ) , which is subject to structural plasticity . Pattern 326 A . Knoblauch , G . Palm , and F . Sommer associations are temporarily stored in the low - capacity buffer and repeat - edly replayed to the high - capacity network . The combination of repetitive training , structural plasticity , and an adequate consolidation of activated synapses emulates a fully connected network equivalent to the model analyzed in this work , although the connectivity level in the cortical module is always low ( Knoblauch , 2006 , 2009 ) . Second , it needs to be explained how the regime of moderately sparse patterns with k / log n → ∞ corresponding to dense synaptic potentiation with p 1 → 1 can be realized in realistic neuronal circuitry . This regime becomes efﬁcient in terms of high synaptic capacity or few synaptic opera - tions per retrieval but only if implemented with inhibitory neurons where the rare silent ( 0 - ) synapses are maintained and the large number of active ( 1 - ) synapses can be pruned ( Knoblauch , 2006 ) . The implementation of this regime is conceivable in brain structures that are dominated by inhibitory neurons ( e . g . , cerebellum , basal ganglia ) and also by using speciﬁc types of inhibitory interneurons in cortical microcircuits . Appendix A : Binary Channels The Shannon information I ( X ) of a binary random variable X on (cid:11) = { 0 , 1 } with p : = pr [ X = 1 ] equals I ( p ) : = − p · ld p − ( 1 − p ) · ld ( 1 − p ) ≈ (cid:20) − p · ld p , p (cid:6) 0 . 5 − ( 1 − p ) · ld ( 1 − p ) , 1 − p (cid:6) 0 . 5 ( A . 1 ) ( Shannon & Weaver , 1949 ; Cover & Thomas , 1991 ) . Note the symmetry I ( p ) = I ( 1 − p ) , and that I ( p ) → 0 for p → 0 ( and p → 1 ) . A binary mem - oryless channel is determined by the two error probabilities p 01 ( false one ) and p 10 ( false zero ) . For two binary random variables X and Y , where Y is the result of transmitting X over the binary channel , we can write I ( Y ) = I Y ( p , p 01 , p 10 ) : = I ( p ( 1 − p 10 ) + ( 1 − p ) p 01 ) ( A . 2 ) I ( Y | X ) = I Y | X ( p , p 01 , p 10 ) : = p · I ( p 10 ) + ( 1 − p ) · I ( p 01 ) ( A . 3 ) T ( X ; Y ) = T ( p , p 01 , p 10 ) : = I Y ( p , p 01 , p 10 ) − I Y | X ( p , p 01 , p 10 ) . ( A . 4 ) For the analysis of pattern part retrieval in section 3 . 1 , the case p 10 = 0 is of particular interest : T ( p , p 01 , 0 ) = I ( p + p 01 − pp 01 ) − ( 1 − p ) · I ( p 01 ) ( A . 5 ) ≤ I ( p 01 ) + I (cid:15) ( p 01 ) · ( p ( 1 − p 01 ) ) − ( 1 − p ) · I ( p 01 ) = − p ld p 01 . ( A . 6 ) Memory Capacities for Synaptic and Structural Plasticity 327 For the upper bound , we have linearized I in p 01 and used the convexity of I ( p ) — ( dI / dp ) 2 = − 1 / ( p ( 1 − p ) ln 2 ) < 0 . The upper bound becomes exact for p / p 01 → 0 . Forhighﬁdelity , wearetypicallyinterestedin p 01 (cid:6) p : = l / n ( see section 3 . 2 ) . Thus , linearization of I in p yields a better upper bound , T ( p 1 , p 01 , 0 ) ≤ I ( p ) + I (cid:15) ( p ) · ( 1 − p ) · p 01 − ( 1 − p ) · I ( p 01 ) ≤ I ( p ) , ( A . 7 ) where the approximations become exact in the limit p 01 / p → 0 . For the relative error e I of approximating T ( p , p 01 , p 10 ) by I ( p ) , we can write e I : = I ( p 1 ) − T ( p 1 , p 01 , p 10 ) I ( p 1 ) ≈ ( 1 − p 1 ) I ( p 01 ) − I (cid:15) ( p 1 ) · p 01 I ( p 1 ) ≈ I ( p 01 ) I ( p 1 ) − p 01 p 1 , ( A . 8 ) where for the last approximation , we additionally assume p (cid:6) 0 . 5 and correspondingly 1 − p ≈ 1 , I ( p ) ≈ − p ld p , and I (cid:15) ( p ) ≈ − ld p . Applying these results to our analysis of the Willshaw model in sec - tion 3 . 2 , using p : = l / n (cid:6) 0 . 5 and p 01 : = (cid:7) p for (cid:7) (cid:6) 1 , we obtain e I ≤ I ( (cid:7) ln ) I ( ln ) − (cid:7) ≈ (cid:7) · ld (cid:7) ld ( ln ) ≈ I ( (cid:7) ) − ld ( ln ) ≤ (cid:20) I ( (cid:7) ) , in any case (cid:7) , l / n ≤ (cid:7) . ( A . 9 ) Note that typically sparse patterns with l / n (cid:6) 1 / 100 are used . Thus , requir - ing for example , (cid:7) = 0 . 01 implies that the relative error of approximating T by I in equation 3 . 10 , is smaller than 1 % . Appendix B : Exact Retrieval Error Probabilities for Fixed Query Activity Our analysis so far used the binomial approximation , equation 3 . 3 . Here we give the exact expressions for ﬁxed query pattern activity , that is , when the query pattern ˜u has exactly c : = λ k correct one - entries from one of the address patterns u μ and , additionally , f : = κ k false one - entries ( 0 < λ ≤ 1 , κ ≥ 0 ) . Retrieving with threshold (cid:6) , the exact retrieval error probabilities p 01 : = pr ( ˆ v i = 1 | v μ i = 0 ) of a false one - entry and p 10 : = pr ( ˆ v i = 0 | v μ i = 1 ) of a missing one - entry are p 01 ( (cid:6) ) = c + f (cid:7) x = (cid:6) p WP ( x ; k , l , m , n , M − 1 , c + f ) ( B . 1 ) 328 A . Knoblauch , G . Palm , and F . Sommer p 10 ( (cid:6) ) = (cid:6) − 1 (cid:7) x = c p WP ( x − c ; k , l , m , n , M − 1 , f ) , ( B . 2 ) where p WP ( x ; k , l , m , n , M , z ) is the distribution of dendritic potential x when stimulating with a random query pattern having exactly z one - entries and m − z zero entries ( 0 ≤ x ≤ z ) . It is p WP ( x ; k , l , m , n , M , z ) = (cid:16) z x (cid:17) x (cid:7) s = 0 ( − 1 ) s (cid:16) x s (cid:17) (cid:16) 1 − l n ( 1 − B ( m , k , s + z − x ) ) (cid:17) M ( B . 3 ) ≈ (cid:16) z x (cid:17) x (cid:7) s = 0 ( − 1 ) s (cid:16) x s (cid:17) (cid:14) 1 − l n (cid:14) 1 − (cid:16) 1 − k m (cid:17) s + z − x (cid:15)(cid:15) M ( B . 4 ) = M (cid:7) i = 0 p B ( i ; M , l / n ) p B ( x ; z , 1 − ( 1 − k / m ) i ) , ( B . 5 ) where we used B ( a , b , c ) : = (cid:2) a − b c (cid:3) / (cid:2) ac (cid:3) = (cid:21) c − 1 i = 0 ( a − b − i ) / ( a − i ) and the bi - nomial probability p B ( x ; N , P ) : = (cid:2) Nx (cid:3) P x ( 1 − P ) N − x . Equation B . 3 is exact for ﬁxed address pattern activity , that is , if each address pattern u μ has ex - actly k one - entries and has been found by Knoblauch ( 2008 ) , generalizing a previous approach of Palm ( 1980 ) for the particular case of zero noise ( c = k , f = 0 ) . The approximations , equations B . 4 and B . 5 , would be exact for random address pattern activity , that is , if u μ i is one with probability k / m ( but still ﬁxed c , f ) . Equation B . 5 averages over the so - called unit - usage ( the number of patterns a given content neuron belongs to ) and has been found by Buckingham and Willshaw ( 1992 ) and Buckingham ( 1991 ) . The trans - formation to equation B . 4 has been found by Sommer and Palm ( 1999 ) . Equations B . 3 and B . 4 are numerically efﬁcient to evaluate for low query pattern activity c + f , whereas equation B . 5 is efﬁcient for a few stored patterns M . The distinction between ﬁxed and random address pattern activity , | u μ | , is of minor interest for moderately large networks , because then equations B . 3 to B . 5 yield very similar values ( Knoblauch , 2006 , 2008 ) . However , the distinction between ﬁxed and random query pattern activity , | ˜u | , remains important even for large networks ( see appendix D ) . For the particular case of pattern part retrieval , c = λ k and f = 0 , we can use the Willshaw threshold (cid:6) = λ k , and the error probabilities are p 10 = 0 and p 01 = λ k (cid:7) s = 0 ( − 1 ) s (cid:16) λ k s (cid:17) (cid:22) 1 − l n ( 1 − B ( m , k , s ) ) (cid:23) M − 1 ( B . 6 ) Memory Capacities for Synaptic and Structural Plasticity 329 # neurons log n 10 # neurons log n 10 2 2 . 5 3 3 . 5 4 4 . 5 5 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 1 . 1 C ε I / C ε I , approx ε = 0 . 01 , λ = 0 . 5 k = n / 2 k = n / 4 k = n 3 / 4 k = n 2 / 3 k = n 1 / 2 k = ld n k = 4 r e l a ti v e e rr o r e x ac t / a pp r ox 2 2 . 5 3 3 . 5 4 4 . 5 5 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 M ε / M ε approx ε = 0 . 01 λ = 0 . 5 k = n / 2 k = n / 4 k = n 3 / 4 k = n 2 / 3 k = n 1 / 2 k = ld n k = 4 r e l a ti v e e rr o r e x ac t / a pp r ox ( b ) ( a ) Figure 9 : Approximation quality of our analysis in sections 3 and 4 based on equation 3 . 3 for m = n , k = l , high - ﬁdelity parameter (cid:7) = 0 . 01 , when addressing with half address patterns ( λ = 0 . 5 , κ = 0 ) . ( a ) Relative approximation quality of the pattern capacity M (cid:7) / M approx (cid:7) as a function of neuron number n . The exact value M (cid:7) is computed as in Table 2 and the approximation M approx (cid:7) is computed from equation 3 . 8 . The different lines correspond to different pattern activi - ties k ( n ) = 4 , ld n , √ n , n 2 / 3 , n 3 / 4 , n / 4 , n / 2 ( increasing line thickness ; alternation of solid and dashed lines ) . Approximation quality for network capacity C (cid:7) is qualitatively the same . ( b ) Relative approximation quality similar to a , but for the information capacity C I (cid:7) . C I , approx is computed from equation 4 . 1 . Approxi - mation quality for the synaptic capacity C S (cid:7) is qualitatively the same . ≈ λ k (cid:7) s = 0 ( − 1 ) s (cid:16) λ k s (cid:17) (cid:22) 1 − l n ( 1 − ( 1 − k / m ) s ) (cid:23) M − 1 ( B . 7 ) = M − 1 (cid:7) i = 0 p B ( i ; M − 1 , l / n ) ( 1 − ( 1 − k / m ) i ) λ k ( B . 8 ) ≥ p λ k 1 . ( B . 9 ) Here equations B . 6 to B . 8 correspond to equations B . 3 to B . 5 , and the bound corresponds to the binomial approximation , equation 3 . 3 . Knoblauch ( 2007 , 2008 ) shows that this lower bound becomes tight at least for k ∼ O ( n / log 4 n ) or , for m = n , k = l , already for k ∼ O ( n / log 2 n ) . Thus , our theory based on the binomial approximation , equation 3 . 3 , becomes exact for virtually any sublinear k ( n ) . We have validated these results in extensive numerical experiments , which can be found in Knoblauch ( 2006 , 2008 ) and Knoblauch et al . ( 2008 ) . Table 2 shows some exact results when addressing with half patterns ( λ = 0 . 5 , κ = 0 ) . Figure 9 plots the quality of the binomial approximation , equation 3 . 3 , for pattern capacity M and information capacity C I for differ - ent sparsity levels and increasing network size n → ∞ . 330 A . Knoblauch , G . Palm , and F . Sommer T a b l e 2 : E x a c t C a p a c i t i e s o f t h e W ill s h a w M o d e l C o m p u t e d f r o m E q u a t i o n B . 6 f o r m = n , k = l , H i g h F i d e li t y (cid:7) = 0 . 01 W h e n A dd r e ss i n g w i t h H a l f A dd r e ss P a tt e r n s ( λ = 0 . 5 , κ = 0 ) . n = 10 0 200 500 1000 2000 5000 10 , 000 20 , 000 50 , 0 0 0 100 , 000 k = 4 4 4 4 4 4 4 4 4 4 4 M (cid:7) 7 23 102 315 951 3985 11 , 614 33 , 561 135 , 2 16 386 , 157 C (cid:7) 0 . 016 7 34 0 . 016080 0 . 013581 0 . 011749 0 . 009820 0 . 007427 0 . 005876 0 . 004581 0 . 003239 0 . 002467 C I (cid:7) 0 . 189 5 10 0 . 213911 0 . 239855 0 . 257522 0 . 272803 0 . 289919 0 . 301034 0 . 310883 0 . 322324 0 . 330003 C S (cid:7) 1 . 501 2 79 1 . 755475 2 . 087170 2 . 337024 2 . 586433 2 . 915938 3 . 165234 3 . 414622 3 . 744455 3 . 994076 k = l d n 7 8 9 10 11 12 13 14 16 17 M (cid:7) 26 73 530 1578 6825 31481 130517 410162 2239454 8958499 C (cid:7) 0 . 093 6 67 0 . 087255 0 . 136318 0 . 126214 0 . 166369 0 . 152057 0 . 185759 0 . 169994 0 . 185909 0 . 211443 C I (cid:7) 0 . 177 0 45 0 . 174203 0 . 216708 0 . 210461 0 . 239663 0 . 234620 0 . 258792 0 . 248317 0 . 254089 0 . 272940 C S (cid:7) 0 . 781 2 48 0 . 790925 0 . 863820 0 . 864564 0 . 891863 0 . 916887 0 . 938451 0 . 933671 0 . 907202 0 . 926973 k = √ n 10 14 22 32 45 71 100 141 224 316 M (cid:7) 20 67 294 791 2122 7082 17013 40294 119 8 00 271628 C (cid:7) 0 . 092 1 80 0 . 120686 0 . 150986 0 . 159572 0 . 162795 0 . 150634 0 . 136076 0 . 120799 0 . 098333 0 . 082962 C I (cid:7) 0 . 134 6 42 0 . 140982 0 . 152895 0 . 160997 0 . 175765 0 . 189566 0 . 198546 0 . 211598 0 . 224751 0 . 235512 C S (cid:7) 0 . 506 2 27 0 . 430356 0 . 347634 0 . 358847 0 . 476765 0 . 628296 0 . 745907 0 . 895062 1 . 088783 1 . 249831 k = n 2 / 3 22 34 63 100 159 292 464 737 1 3 57 2154 M (cid:7) 11 27 76 156 310 736 1371 2509 5 4 54 9662 C (cid:7) 0 . 081 6 60 0 . 086933 0 . 081497 0 . 071901 0 . 061067 0 . 046564 0 . 036626 0 . 028186 0 . 019377 0 . 014325 C I (cid:7) 0 . 083 1 80 0 . 087490 0 . 092952 0 . 097348 0 . 104483 0 . 114870 0 . 124077 0 . 134520 0 . 149156 0 . 160552 C S (cid:7) 0 . 194 1 63 0 . 191892 0 . 275016 0 . 344860 0 . 435923 0 . 575533 0 . 703203 0 . 852483 1 . 078028 1 . 268922 k = n / 4 25 50 125 250 500 1250 2500 5000 12 5 00 25000 M (cid:7) 10 16 24 31 39 49 56 64 74 82 C (cid:7) 0 . 079 1 04 0 . 063284 0 . 037970 0 . 024522 0 . 015425 0 . 007752 0 . 004430 0 . 002531 0 . 001171 0 . 000649 C I (cid:7) 0 . 079 2 41 0 . 067368 0 . 050885 0 . 042898 0 . 038121 0 . 030659 0 . 024775 0 . 021308 0 . 016677 0 . 014209 C S (cid:7) 0 . 166 3 47 0 . 177726 0 . 178703 0 . 181323 0 . 191142 0 . 183161 0 . 164436 0 . 157467 0 . 138863 0 . 128935 N o t e : T a b l e e n t r i e s c o rr e s p o n d t o n e t w o r k s i z e n , p a tt e r n a c t i v i t y k , p a tt e r n c a p a c i t y M (cid:7) , n e t w o r k c a p a c i t y C (cid:7) , i n f o r m a t i o n c a p a c i t y C I (cid:7) , a n d s y n a p t i c c a p a c i t y C S (cid:7) . Memory Capacities for Synaptic and Structural Plasticity 331 Appendix C : Fallacies for Extremely Sparse and Nonsparse Activity As discussed in section 3 . 3 , our analysis method is exact for both small and very large networks , whereas alternative methods are inaccurate for ﬁnite networks and , for some parameter ranges , even in the asymptotic limit . For example , previous analyses of feedforward associative networks with linear learning , such as the covariance rule , often compute capacity as a function of the so - called signal - to - noise ratio SNR = ( μ hi − μ lo ) 2 / σ 2 , deﬁned as the mean potential difference between “high units” ( which should be active in the retrieval result ˆv ) and “low units” ( which should be inactive ) divided by the potential variance ( Dayan & Willshaw , 1991 ; Palm , 1991 ; Palm & Som - mer , 1996 ) . Assuming gaussian dendritic potentials , such analyses propose an asymptotic network capacity C = 0 . 72 for linear associative networks with covariance learning and k / m → 0 , which seems to be better than the binary Willshaw model . However , numerical evaluations prove that even for moderate sparseness in large , ﬁnite networks , the Willshaw model per - forms better ( data not shown ) . To analyze the reason for this discrepancy , we compute the SNR for the Willshaw model , SNR Willshaw ≈ ( λ k ( 1 − p 1 ) ) 2 λ kp 1 ( 1 − p 1 ) = λ k ( 1 − p 1 ) p 1 . ( C . 1 ) The SNR for the network with linear learning and the optimal covariance rule has been found to be m / ( M ( l / n ) ( 1 − l / n ) ) for zero query noise ( Dayan & Willshaw , 1991 ; Palm & Sommer , 1996 ) . Using M as in equation 3 . 2 and assuming small p 1 → 0 , this becomes SNR Cov ≈ mkl − mn ( l / n ) ln ( 1 − p 1 ) = k − ln ( 1 − p 1 ) . ( C . 2 ) Thus , for small p 1 → 0 , the SNR will be k / p 1 for both models , which falsely suggests , assuming gaussian dendritic potentials , that the Willshaw model could also store 0 . 72 bits per synapse , which is , of course , wrong . In fact , for k / log n → 0 ( which is equivalent to p 1 (cid:7) → 0 ) , equation 3 . 12 proves zero capacity for the Willshaw model and strongly suggests the same result for the covariance rule in the linear associative memory . Further numerical experiments and theoretical considerations show that even for k ∼ log n , the Willshaw model performs better than linear covariance learning , although it cannot exceed C = 0 . 69 or C = 0 . 53 . This shows that the SNR method and the underlying gaussian approximation become reliable only for dense potentiation with p 1 (cid:7) → 1 and k / log n → ∞ ( see also Knoblauch , 2008 ; Henkel & Opper , 1990 ) . But even for dense potentiation , the gaussian assumption is inaccu - rate for linear pattern activities k = cn and l = dn with constant c and d , 332 A . Knoblauch , G . Palm , and F . Sommer falsely suggesting constant pattern capacity M (cid:7) ∼ 1 for m , n → ∞ ( note that dense potentiation may imply highly asymmetric potential distributions ; see Knoblauch , 2003b ) . In fact , M (cid:7) → ∞ diverges for RQ1 as can be seen in equation B . 8 . Moreover , we can compute upper and lower bounds for equation B . 8 by assuming that all content neurons have a unit usage i larger or smaller than Md + ξ (cid:24) Md ( 1 − d ) ( note that p 01 given i increases with i ) , p 01 ≈ ( 1 − ( 1 − c ) Md + ξ √ Md ( 1 − d ) ) λ cm . ( C . 3 ) For sufﬁciently large positive ( but for RQ1 still constant ) ξ , this approx - imation is an upper bound . For example , we can choose ξ : = G c − 1 ( (cid:7) 1 d ) with (cid:7) 1 (cid:6) (cid:7) such that only a few content neurons have a unit usage more than ξ standard deviations larger than the mean unit usage ( here , G c ( x ) : = 0 . 5erfc ( x / √ 2 ) is the gaussian tail integral ) . Similarly , for large neg - ative ξ we obtain a lower bound . Requiring p 01 ≤ (cid:7) d / ( 1 − d ) , we obtain for the pattern capacity M (cid:7) + ξ (cid:24) M (cid:7) ( 1 − d ) / d ≈ ln ( 1 − ( (cid:7) d / ( 1 − d ) ) 1 / ( λ cm ) ) d ln ( 1 − c ) ≈ ln m − d ln ( 1 − c ) . ( C . 4 ) Thus , the pattern capacity is essentially independent of ξ . However , com - pared to equations 3 . 8 and 4 . 10 , the asymptotic pattern capacity is re - duced by a factor f : = ( − ln ( 1 − c ) ) / c < 1 . This turns out to be the rea - son that the Willshaw network has zero information capacity C I → 0 and zero synaptic capacity C S → 0 for linear address pattern activ - ity k = cm . With ˜ p 0 (cid:7) : = ( 1 − cd ) f M (cid:7) → 0 ( see equation 3 . 1 ) , it is p 0 (cid:7) : = 1 − p 1 (cid:7) = ˜ p f 0 (cid:7) ( see equation 3 . 7 ) . Therefore , equation 4 . 1 becomes C I (cid:7) ∼ ld ( 1 − ˜ p 0 (cid:7) ) ( ln ˜ p 0 (cid:7) ) / ( ˜ p f 0 (cid:7) ld˜ p f 0 (cid:7) ) ∼ ˜ p 1 − f 0 (cid:7) → 0 . Similarly , equation 4 . 3 becomes C S (cid:7) ∼ ld ( 1 − ˜ p 0 (cid:7) ) ( ln ˜ p 0 (cid:7) ) / ˜ p f 0 (cid:7) ≈ ˜ p 1 − f 0 (cid:7) ln ˜ p 0 (cid:7) → 0 . Appendix D : Corrections for Random Query Activity So far , our exact theory in appendix B as well as the approximative theory in sections 3 to 5 assume that the query pattern ˜u has exactly λ k correct one - entries ( and κ k false one - entries ) . This is sufﬁcient for many applica - tions where speciﬁcations assume a minimal quality of query patterns in terms of a lower bound for the number of correct one - entries . However , in particular for small k or large λ near 1 , we may want to include the case of random query pattern activity . In the following , we assume that the address patterns have random activity—each pattern component u μ i is one with probability k / m independent of other components . Similarly , in a query pattern ˜u , a one - entry is erased with probability 1 − λ . For simplicity , Memory Capacities for Synaptic and Structural Plasticity 333 we assume no add noise ( i . e . , κ = 0 ) . Thus , a component in the query pat - tern , ˜ u i , is one with probability λ k / m . Then the query pattern activity Z is a binomially distributed random variable , pr [ Z = z ] = p B ( z ; m , λ k / m ) ( for p B , see equation B . 5 ) . For a particular Z = z , the exact error probability p 01 is given by equation B . 7 ( or equation B . 8 ) , replacing λ k by z . Averaging over all possible z yields p ∗ 01 = m (cid:7) z = 0 p B ( z ; m , λ k / m ) z (cid:7) s = 0 ( − 1 ) s (cid:16) z s (cid:17) (cid:22) 1 − l n ( 1 − ( 1 − k / m ) s ) (cid:23) M − 1 = m (cid:7) s = 0 (cid:16) − λ k m (cid:17) s (cid:16) m s (cid:17) (cid:22) 1 − l n ( 1 − ( 1 − k / m ) s ) (cid:23) M − 1 ( D . 1 ) = M − 1 (cid:7) i = 0 p B ( i ; M − 1 , l / n ) m (cid:7) z = 0 p B ( z ; m , λ k / m ) ( 1 − ( 1 − k / m ) i ) z = M − 1 (cid:7) i = 0 p B ( i ; M − 1 , l / n ) (cid:16) 1 − λ k m ( 1 − k / m ) i (cid:17) m . ( D . 2 ) The ﬁrst equation is numerically efﬁcient for small k , the last equation for small M . For the binomial approximative analyses , we can rewrite equa - tion 3 . 3 as p ∗ 01 ≈ m (cid:7) z = 0 p B ( z ; m , λ k / m ) p z 1 = (cid:16) 1 − λ k m ( 1 − p 1 ) (cid:17) m . ( D . 3 ) Controlling for retrieval quality , p ∗ 01 ≤ (cid:7) l / ( n − l ) , the maximal memory load , equation 3 . 7 , becomes p ∗ 1 (cid:7) ≈ 1 − 1 − ( (cid:7) l n − l ) 1 / m λ k / m . ( D . 4 ) Note that positive p ∗ 1 (cid:7) ≥ 0 requires (cid:7) ≥ e − λ k ( n − l ) / l or , equivalently , k ≥ ln ( ( n − l ) / ( (cid:7) l ) ) / λ . Consequently , even for logarithmic k , l = O ( log n ) , it may be impossible to achieve retrieval quality levels RQ1 or higher ( see sec - tion 2 . 1 ) . For example , k ≤ c log n with c < 1 implies diverging noise (cid:7) ≥ n 1 − c / l , while RQ1 would require constant (cid:7) ∼ 1 and RQ2 or RQ3 even vanishing (cid:7) → 0 . This is a major difference to the model with ﬁxed query pattern activity . 334 A . Knoblauch , G . Palm , and F . Sommer Writing x : = (cid:7) l / ( n − l ) and using e x = (cid:4) ∞ i = 0 x i / i ! , we obtain for the dif - ference (cid:14) p 1 (cid:7) : = p 1 (cid:7) − p ∗ 1 (cid:7) between equation 3 . 7 and equation D . 4 : (cid:14) p 1 (cid:7) ≈ e ( ln x ) / ( λ k ) − (cid:16) 1 + e ( ln x ) / m − 1 λ k / m (cid:17) ( D . 5 ) = ∞ (cid:7) i = 1 ( ln x ) i i ! ( λ k ) i − ( ln x ) i i ! λ km i − 1 ( D . 6 ) = ∞ (cid:7) i = 2 ( ln x ) i i ! ( λ k ) i ( 1 − ( λ k / m ) i − 1 ) ( D . 7 ) ≈ p 1 (cid:7) − 1 − ln p 1 (cid:7) , ( D . 8 ) where the last approximation is true for balanced potentiation with ﬁxed p 1 (cid:7) and λ k / m → 0 . Note that for sparse potentiation with p 1 (cid:7) → 0 and k / log n → 0 we have diverging (cid:14) p 1 (cid:7) . At least for dense potentiation with p 1 (cid:7) → 1 and k / log n → ∞ , the relative differences vanish : (cid:14) p 1 (cid:7) / p 1 (cid:7) → 0 and even (cid:14) p 1 (cid:7) / ( 1 − p 1 (cid:7) ) → 0 . Thus , at least for dense potentiation , the models with ﬁxed and random query pattern activity become equivalent , including all results on information capacity C I and synaptic capacity C S ( see sections 3 – 5 ) . Proceeding as in section 3 . 2 , we obtain p ∗ 1 (cid:7) ≈ 1 + ln p 1 (cid:7) ≈ 1 − ln n − l (cid:7) l λ k ( D . 9 ) p ∗ 0 (cid:7) : = 1 − p ∗ 1 (cid:7) = ln n − l (cid:7) l λ k (cid:14) ⇔ k ≈ ln n − l (cid:7) l λ p ∗ 0 (cid:7) (cid:15) ( D . 10 ) M ∗ (cid:7) = − mn kl ln p ∗ 0 (cid:7) ≈ − λ 2 p ∗ 20 (cid:7) ln p ∗ 0 (cid:7) k l mn ( ln n − l (cid:7) l ) 2 ( D . 11 ) C ∗ (cid:7) = M (cid:7) m − 1 T ( l / n , (cid:7) l / ( n − l ) , 0 ) ≈ − λ p ∗ 0 (cid:7) ld p ∗ 0 (cid:7) η . ( D . 12 ) The asymptotic bound of network capacity is thus only C ∗ (cid:7) ≤ 1 / ( e ln 2 ) ≈ 0 . 53 for p ∗ 0 (cid:7) = 1 / e ≈ 0 . 368 and retrieval quality levels RQ0 - RQ2 ( for RQ3 , the bound decreases by factor 1 / 3 as discussed in section 3 . 3 ) . Figure 10 illustrates asymptotic capacities in analogy to Figure 6 . For dense potenti - ation , p ∗ 0 (cid:7) → 0 , results are identical to the model with ﬁxed query pattern activity . For sparse potentiation , p ∗ 0 (cid:7) → 1 , we have C I ∗ (cid:7) : = C ∗ (cid:7) / I ( p ∗ 0 (cid:7) ) → 0 and still C S ∗ (cid:7) : = C ∗ (cid:7) / min ( p ∗ 0 (cid:7) , 1 − p ∗ 0 (cid:7) ) → 1 / ln 2 ≈ 1 . 44 . For k = l maximal pattern capacity is 0 . 18 λ 2 mn / ( ld n ) 2 for p ∗ 0 (cid:7) = 1 / √ e ≈ 0 . 61 . Note that our result C ∗ ≤ 0 . 53 contradicts previous analyses . For exam - ple , Nadal ( 1991 ) estimates C ∗ ≤ 0 . 236 for p ∗ 1 = 0 . 389 . We believe that our results are correct and that the discrepancies are due to inaccurate approxi - mations employed by previous work . In fact , we have veriﬁed the accuracy of our theory in two steps ( see Knoblauch , 2006 , 2008 ; Knoblauch et al . , Memory Capacities for Synaptic and Structural Plasticity 335 10 2 10 3 10 4 10 5 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 C * C * , bin C KPS / BWSP C bin C gauss 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 1 . 1 C ε * = − ( 1−p 1 ε * ) ld ( 1−p 1 ε * ) C I * ε = C ε * / I ( p 1 ε * ) C S * ε = C ε * / min ( p 1 ε * , 1−p 1 ε * ) M ε * / max ∼ − ( 1−p 1 ε * ) 2 ln ( 1−p 1 ε * ) 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 10 1 2 3 4 5 6 7 8 9 10 11 1ε p ε = 0 . 01 # neurons n e t w o r k ca p ac it y C I * * * k = sqrt ( n ) k = e log ( n ) s t o r a g e ca p ac iti e s C , C , M S * s t o r a g e ca p ac it y C memory load ∗ λ = 1 ( a ) ( b ) Figure 10 : ( a ) Asymptotic network capacity C ∗ (cid:7) , information capacity C I ∗ (cid:7) , synaptic capacity C S ∗ , and pattern capacity M ∗ (cid:7) as functions of memory load p ∗ 1 (cid:7) for the model variant with random query pattern activity . Compare to Figure 6a . ( b ) Exact and approximative network capacity for ﬁnite network sizes m = n and mean pattern activity k = l = e ln n ( thin lines ) or k = l = √ n ( bold lines ) . For random query pattern activity , the plot shows results computed with exact equation D . 1 ( C ∗ ; black solid ) and binomial approximation equation D . 9 ( C ∗ , bin ; gray solid ) . For ﬁxed query pattern activity , the plot shows results computed with exact equation B . 7 ( C BWSP ; black dashed ) and equation B . 6 ( C KPS ; black dash - dotted ) , the binomial approximation equation 3 . 7 ( C bin ; gray dashed ) , and a gaussian approximation of dendritic potentials ( C gauss ; gray dash - dotted ; see , Knoblauch , 2008 ) . Note that the binomial approximations closely approximate the exact values already for relatively small networks . In contrast , the gaussian approximation signiﬁcantly underestimates capacity even for large networks . 2008 ) . First , we have veriﬁed all our formulas for the exact error proba - bilities of the different model variants ( equations B . 1 to B . 8 and D . 1 and D . 6 ) by extensive simulations of small networks . Second , we have proven the asymptotic correctness of our binomial approximative theory ( see equa - tions 3 . 3 , 3 . 7 – 3 . 9 , and D . 9 – D . 12 ) by theoretical considerations and numerical experiments ( see also Figure 10 ) . Acknowledgments We thank Sen Cheng , Marc - Oliver Gewaltig , Edgar K¨orner , Ursula K¨orner , Bartlett Mel , and Xundong Wu for helpful discussions , as well as Pentti Kanerva for his comments to an earlier version of the manuscript . F . T . S . was supported by NSF grant IIS - 0713657 and a Google research award . References Abeles , M . ( 1982 ) . Local cortical circuits . Berlin : Springer . Abeles , M . , Bergman , H . , Margalit , E . , & Vaadia , E . ( 1993 ) . Spatio - temporal ﬁring patterns in frontal cortex of behaving monkeys . Journal of Neurophysiology , 70 , 1629 – 1643 . 336 A . Knoblauch , G . Palm , and F . Sommer Albus , J . ( 1971 ) . A theory of cerebellar function . Mathematical Biosciences , 10 , 25 – 61 . Amari , S . - I . ( 1989 ) . Characteristics of sparsely encoded associative memory . Neural Networks , 2 , 451 – 457 . Amit , D . , Gutfreund , H . , & Sompolinsky , H . ( 1987a ) . Information storage in neural networks with low levels of activity . Phys . Rev . A , 35 , 2293 – 2303 . Amit , D . , Gutfreund , H . , & Sompolinsky , H . ( 1987b ) . Statistical mechanics of neural networks near saturation . Annals of Physics , 173 , 30 – 67 . Anderson , J . ( 1968 ) . A memory storage model utilizing spatial correlation functions . Kybernetik , 5 , 113 – 119 . Anderson , J . ( 1993 ) . The BSB model : A simple nonlinear autoassociative neural network . In M . Hassoun ( Ed . ) , Associative neural memories . New York : Oxford University Press . Anderson , J . , Silverstein , J . , Ritz , S . , & Jones , R . ( 1977 ) . Distinctive features , categor - ical perception , and probability learning : Some applications of a neural model . Psychological Review , 84 , 413 – 451 . Arieli , A . , Sterkin , A . , Grinvald , A . , & Aertsen , A . ( 1996 ) . Dynamics of ongoing activity : Explanation of the large variability in evoked cortical responses . Science , 273 , 1868 – 1871 . Attwell , D . , & Laughlin , S . ( 2001 ) . An energy budget for signaling in the grey matter of the brain . Journal of Cerebral Blood Flow and Metabolism , 21 , 1133 – 1145 . Aviel , Y . , Horn , D . , & Abeles , M . ( 2005 ) . Memory capacity of balanced networks . Neural Computation , 17 , 691 – 713 . Barlow , H . ( 1972 ) . Single units and sensation : A neuron doctrine for perceptual psychology . Perception , 1 , 371 – 394 . Bentz , H . , Hagstroem , M . , & Palm , G . ( 1989 ) . Information storage and effective data retrieval in sparse matrices . Neural Networks , 2 , 289 – 293 . Bogacz , R . , & Brown , M . ( 2003 ) . Comparison of computational models of familiarity discrimination in the perirhinal cortex . Hippocampus , 13 , 494 – 524 . Bogacz , R . , Brown , M . , & Giraud - Carrier , C . ( 2001 ) . Model of familiarity discrimi - nation in the perirhinal cortex . Journal of Computational Neuroscience , 10 , 5 – 23 . Bosch , H . , & Kurfess , F . ( 1998 ) . Information storage capacity of incompletely connected associative memories . Neural Networks , 11 ( 5 ) , 869 – 876 . Braitenberg , V . ( 1978 ) . Cell assemblies in the cerebral cortex . In R . Heim & G . Palm ( Eds . ) , Theoretical approaches to complex systems ( pp . 171 – 188 ) . Berlin : Springer - Verlag . Braitenberg , V . , & Sch¨uz , A . ( 1991 ) . Anatomy of the cortex : Statistics and geometry . Berlin : Springer - Verlag . Buckingham , J . ( 1991 ) . Delicate nets , faint recollections : A study of partially connected associative network memories . Unpublished doctoral dissertation , University of Edinburgh . Buckingham , J . , & Willshaw , D . ( 1992 ) . Performance characteristics of associative nets . Network : Computation in Neural Systems , 3 , 407 – 414 . Buckingham , J . , & Willshaw , D . ( 1993 ) . On setting unit thresholds in an incompletely connected associative net . Network : Computation in Neural Systems , 4 , 441 – 459 . Burks , A . , Goldstine , H . , & von Neumann , J . ( 1946 ) . Preliminary discussion of the logical design of an electronic computing instrument ( Rep . 1946 ) . U . S . Army Ordnance Department . Memory Capacities for Synaptic and Structural Plasticity 337 Chicca , E . , Badoni , D . , Dante , V . , D’Andreagiovanni , M . , Salina , G . , Carota , L . , et al . ( 2003 ) . A VLSI recurrent network of integrate - and - ﬁre neurons connected by plastic synapses with long - term memory . IEEE Transactions on Neural Networks , 14 , 1297 – 1307 . Coolen , A . ( 2001a ) . Statistical mechanics of recurrent neural networks I : Statics . In F . Moss & S . Gielen ( Eds . ) , Handbook of biological physics ( Vol . 4 , pp . 531 – 596 ) . Amsterdam : Elsevier Science . Coolen , A . ( 2001b ) . Statistical mechanics of recurrent neural networks II : Dynamics . In F . Moss & S . Gielen ( Eds . ) , Handbook of biological physics ( Vol . 4 , pp . 597 – 662 ) . Amsterdam : Elsevier Science . Cover , T . , & Thomas , J . ( 1991 ) . Elements of information theory . New York : Wiley . Dayan , P . , & Willshaw , D . ( 1991 ) . Optimising synaptic learning rules in linear associative memory . Biological Cybernetics , 65 , 253 – 265 . Diesmann , M . , Gewaltig , M . , & Aertsen , A . ( 1999 ) . Stable propagation of syn - chronous spiking in cortical neural networks . Nature , 402 ( 6761 ) , 529 – 533 . Engert , F . , & Bonhoeffer , T . ( 1999 ) . Dendritic spine changes associated with hippocampal long - term synaptic plasticity . Nature , 399 , 66 – 70 . Fransen , E . , & Lansner , A . ( 1998 ) . A model of cortical associative memory based on a horizontal network of connected columns . Network : Computation in Neural Systems , 9 , 235 – 264 . Frolov , A . , & Murav’ev , I . ( 1993 ) . Informational characteristics of neural networks capable of associative learning based on Hebbian plasticity . Network : Computation in Neural Systems , 4 , 495 – 536 . Fusi , S . , Drew , P . , & Abbott , L . ( 2005 ) . Cascade models of synaptically stored memories . Neuron , 45 , 599 – 611 . Gardner , E . , & Derrida , B . ( 1988 ) . Optimal storage properties of neural network models . J . Phys . A : Math . Gen . , 21 , 271 – 284 . Gardner - Medwin , A . ( 1976 ) . The recall of events through the learning of associations between their parts . Proceedings of the Royal Society of London Series B , 194 , 375 – 402 . Golomb , D . , Rubin , N . , & Sompolinsky , H . ( 1990 ) . Willshaw model : Associative memory with sparse coding and low ﬁring rates . Phys . Rev . A , 41 , 1843 – 1854 . Golomb , S . ( 1966 ) . Run - length encodings . IEEE Transactions on Information Theory , 12 , 399 – 401 . Graham , B . , & Willshaw , D . ( 1995 ) . Improving recall from an associative memory . Biological Cybernetics , 72 , 337 – 346 . Graham , B . , & Willshaw , D . ( 1997 ) . Capacity and information efﬁciency of the associative net . Network : Computation in Neural Systems , 8 ( 1 ) , 35 – 54 . Greene , D . , Parnas , M . , & Yao , F . ( 1994 ) . Multi - index hashing for information retrieval . In Proceedings of the 35th Annual Symposium on Foundations of Computer Science ( pp . 722 – 731 ) . Piscataway , NJ : IEEE . Hahnloser , R . , Kozhevnikov , A . , & Fee , M . ( 2002 ) . An ultra - sparse code underlies the generation of neural sequences in a songbird . Nature , 419 , 65 – 70 . Hammerstrom , D . ( 1990 ) . A VLSI architecture for high - performance , low - cost , on - chip learning . In Proceedings of the IEEE International Joint Conference on Neural Networks 1990 ( pp . II : 537 – 543 ) . Piscataway , NJ : IEEE . 338 A . Knoblauch , G . Palm , and F . Sommer Hammerstrom , D . , Gao , C . , Zhu , S . , & Butts , M . ( 2006 ) . FPGA implementation of very large associative memories . In A . Omondi & J . Rajapakse ( Eds . ) , FPGA implementations of neural networks ( pp . 167 – 195 ) . New York : Springer . Hebb , D . ( 1949 ) . The organization of behavior : A neuropsychological theory . New York : Wiley . Heittmann , A . , & R ¨uckert , U . ( 2002 ) . Mixed mode VLSI implementation of a neural associative memory . Analog Integrated Circuits and Signal Processing , 30 , 159 – 172 . Hellwig , B . ( 2000 ) . A quantitative analysis of the local connectivity between pyrami - dal neurons in layers 2 / 3 of the rat visual cortex . Biological Cybernetics , 82 , 111 – 121 . Henkel , R . , & Opper , M . ( 1990 ) . Distribution of internal ﬁelds and dynamics of neural networks . Europhysics Letters , 11 ( 5 ) , 403 – 408 . Hertz , J . , Krogh , A . , & Palmer , R . ( 1991 ) . Introduction to the theory of neural computation . Redwood City , CA : Addison - Wesley . Hopﬁeld , J . ( 1982 ) . Neural networks and physical systems with emergent collective computational abilities . Proceedings of the National Academy of Science , USA , 79 , 2554 – 2558 . Hopﬁeld , J . ( 1984 ) . Neurons with graded response have collective computational properties like those of two - state neurons . Proceedings of the National Academy of Science , USA , 81 ( 10 ) , 3088 – 3092 . Hopﬁeld , J . , & Tank , D . ( 1986 ) . Computing with neural circuits . Science , 233 , 625 – 633 . Huffman , D . ( 1952 ) . A method for the construction of minimum redundancy codes . Proceedings of the Institute of Radio Engineers , 40 , 1098 – 1101 . Kanerva , P . ( 1988 ) . Sparse distributed memory . Cambridge , MA : MIT Press . Knoblauch , A . ( 2003a ) . Optimal matrix compression yields storage capacity 1 for binary Willshaw associative memory . In O . Kaynak , E . Alpaydin , E . Oja , & L . Xu ( Eds . ) , Artiﬁcial Neural Networks and Neural Information Processing— ICANN / ICONIP 2003 ( LNCS 2714 , pp . 325 – 332 ) . Berlin : Springer - Verlag . Knoblauch , A . ( 2003b ) . Synchronization and pattern separation in spiking associative memory and visual cortical areas . Unpublished doctoral dissertation , University of Ulm , Germany . Knoblauch , A . ( 2005 ) . Neural associative memory for brain modeling and information retrieval . Information Processing Letters , 95 , 537 – 544 . Knoblauch , A . ( 2006 ) . On compressing the memory structures of binary neural associative networks ( HRI - EU Rep . 06 - 02 ) . Offenbach / Main , Germany : Honda Research Institute Europe . Knoblauch , A . ( 2007 ) . Asymptotic conditions for high - capacity neural associative networks ( HRI - EU Rep . 07 - 02 ) . Offenbach / Main , Germany : Honda Research Institute Europe . Knoblauch , A . ( 2008 ) . Neural associative memory and the Willshaw - Palm probability distribution . SIAM Journal on Applied Mathematics , 69 ( 1 ) , 169 – 196 . Knoblauch , A . ( 2009 ) . The role of structural plasticity and synaptic consolidation for memory and amnesia in a model of cortico - hippocampal interplay . In J . Mayor , N . Ruh , & K . Plunkett ( Eds . ) , Connectionist Models of Behavior and Cognition II : Proceedings of the 11th Neural Computation and Psychology Workshop . Singapore : World Scientiﬁc . Memory Capacities for Synaptic and Structural Plasticity 339 Knoblauch , A . , & Palm , G . ( 2001 ) . Pattern separation and synchronization in spiking associative memories and visual areas . Neural Networks , 14 , 763 – 780 . Knoblauch , A . , Palm , G . , & Sommer , F . ( 2008 ) . Performance characteristics of sparsely and densely potentiated associative networks ( HRI - EU Rep . 08 - 02 ) . Offenbach / Main , Germany : Honda Research Institute Europe . Kohonen , T . ( 1977 ) . Associative memory : A system theoretic approach . Berlin : Springer . Lamprecht , R . , & LeDoux , J . ( 2004 ) . Structural plasticity and memory . Nature Reviews Neuroscience , 5 , 45 – 54 . Latham , P . , & Nirenberg , S . ( 2004 ) . Computing and stability in cortical networks . Neural Computation , 16 ( 7 ) , 1385 – 1412 . Laughlin , S . , & Sejnowski , T . ( 2003 ) . Communication in neuronal networks . Science , 301 , 1870 – 1874 . Laurent , G . ( 2002 ) . Olfactory network dynamics and the coding of multidimensional signals . Nature Reviews Neuroscience , 3 , 884 – 895 . Lennie , P . ( 2003 ) . The cost of cortical computation . Current Biology , 13 , 493 – 497 . Little , W . ( 1974 ) . The existence of persistent states in the brain . Mathematical Biosciences , 19 , 101 – 120 . Marr , D . ( 1969 ) . A theory of cerebellar cortex . Journal of Physiology , 202 ( 2 ) , 437 – 470 . Marr , D . ( 1971 ) . Simple memory : A theory for archicortex . Philosophical Transactions of the Royal Society of London , Series B , 262 , 24 – 81 . Minsky , M . , & Papert , S . ( 1969 ) . Perceptrons : An introduction to computational geometry . Cambridge , MA : MIT Press . Mu , X . , Artiklar , M . , Watta , P . , & Hassoun , M . ( 2006 ) . An RCE - based associative memory with application to human face recognition . Neural Processing Letters , 23 , 257 – 271 . Nadal , J . - P . ( 1991 ) . Associative memory : On the ( puzzling ) sparse coding limit . J . Phys . A : Math . Gen . , 24 , 1093 – 1101 . Nadal , J . - P . , & Toulouse , G . ( 1990 ) . Information storage in sparsely coded memory nets . Network : Computation in Neural Systems , 1 , 61 – 74 . Palm , G . ( 1980 ) . On associative memories . Biological Cybernetics , 36 , 19 – 31 . Palm , G . ( 1982 ) . Neural assemblies : An alternative approach to artiﬁcial intelligence . Berlin : Springer . Palm , G . ( 1987 ) . Computing with neural networks . Science , 235 , 1227 – 1228 . Palm , G . ( 1990 ) . Cell assemblies as a guideline for brain research . Concepts in Neuroscience , 1 , 133 – 148 . Palm , G . ( 1991 ) . Memory capacities of local rules for synaptic modiﬁcation . A comparative review . Concepts in Neuroscience , 2 , 97 – 128 . Palm , G . , & Palm , M . ( 1991 ) . Parallel associative networks : The PAN - system and the Bacchus - chip . In U . Ramacher , U . R¨uckert , & J . Nossek ( Eds . ) , Proceedings of the 2nd International Conference on Microelectronics for Neural Networks . Munich : Kyrill & Method Verlag . Palm , G . , & Sommer , F . ( 1992 ) . Information capacity in recurrent McCulloch - Pitts networks with sparsely coded memory states . Network , 3 , 177 – 186 . Palm , G . , & Sommer , F . ( 1996 ) . Associative data storage and retrieval in neural nets . In E . Domany , J . van Hemmen , & K . Schulten ( Eds . ) , Models of neural networks III ( pp . 79 – 118 ) . New York : Springer - Verlag . 340 A . Knoblauch , G . Palm , and F . Sommer Perez - Orive , J . , Mazor , O . , Turner , G . , Cassenaer , S . , Wilson , R . , & Laurent , G . ( 2002 ) . Oscillations and sparsening of odor representations in the mushroom body . Science , 297 , 359 – 365 . Poirazi , P . , & Mel , B . ( 2001 ) . Impact of active dendrites and structural plasticity on the memory capacity of neural tissue . Neuron , 29 , 779 – 796 . Prager , R . , & Fallside , F . ( 1989 ) . The modiﬁed Kanerva model for automatic speech recognition . Computer Speech and Language , 3 , 61 – 81 . Pulverm¨uller , F . ( 2003 ) . The neuroscience of language : On brain circuits of words and serial order . Cambridge : Cambridge University Press . Quiroga , R . , Reddy , L . , Kreiman , G . , Koch , C . , & Fried , I . ( 2005 ) . Invariant visual representation by single neurons in the human brain . Nature , 435 , 1102 – 1107 . Rachkovskij , D . , & Kussul , E . ( 2001 ) . Binding and normalization of binary sparse distributed representations by context - dependent thinning . Neural Computation , 13 , 411 – 452 . Rehn , M . , & Sommer , F . ( 2006 ) . Storing and restoring visual input with collaborative rank coding and associative memory . Neurocomputing , 69 , 1219 – 1223 . Rolls , E . ( 1996 ) . A theory of hippocampal function in memory . Hippocampus , 6 , 601 – 620 . Schwenker , F . , Sommer , F . , & Palm , G . ( 1996 ) . Iterative retrieval of sparsely coded associative memory patterns . Neural Networks , 9 , 445 – 455 . Shannon , C . , & Weaver , W . ( 1949 ) . The mathematical theory of communication . Urbana : University of Illinois Press . Softky , W . , & Koch , C . ( 1993 ) . The highly irregular ﬁring of cortical cells is incon - sistent with temporal integration of random EPSPs . Journal of Neuroscience , 13 ( 1 ) , 334 – 350 . Sommer , F . , & Dayan , P . ( 1998 ) . Bayesian retrieval in associative memories with storage errors . IEEE Transactions on Neural Networks , 9 , 705 – 713 . Sommer , F . , & Palm , G . ( 1999 ) . Improved bidirectional retrieval of sparse patterns stored by Hebbian learning . Neural Networks , 12 , 281 – 297 . Steinbuch , K . ( 1961 ) . Die Lernmatrix . Kybernetik , 1 , 36 – 45 . Stepanyants , A . , Hof , P . , & Chklovskii , D . ( 2002 ) . Geometry and structural plasticity of synaptic connectivity . Neuron , 34 , 275 – 288 . Treves , A . , & Rolls , E . ( 1991 ) . What determines the capacity of autoassociative memories in the brain ? Network , 2 , 371 – 397 . Tsodyks , M . , & Feigel’man , M . ( 1988 ) . The enhanced storage capacity in neural networks with low activity level . Europhysics Letters , 6 , 101 – 105 . Waydo , S . , Kraskov , A . , Quiroga , R . , Fried , I . , & Koch , C . ( 2006 ) . Sparse represen - tation in the human medial temporal lobe . Journal of Neuroscience , 26 ( 40 ) , 10232 – 10234 . Wennekers , T . , & Palm , G . ( 1996 ) . Controlling the speed of synﬁre chains . In C . Malsburg , W . Seelen , J . Vorbr¨uggen , & B . Sendhoff ( Eds . ) , Proceedings of the ICANN 1996 ( pp . 451 – 456 ) . Berlin : Springer - Verlag . Willshaw , D . , Buneman , O . , & Longuet - Higgins , H . ( 1969 ) . Non - holographic associative memory . Nature , 222 , 960 – 962 . Wilson , C . ( 2004 ) . Basal ganglia . In G . Shepherd ( Ed . ) , The synaptic organization of the brain ( 5th ed . , pp . 361 – 413 ) . New York : Oxford University Press . Memory Capacities for Synaptic and Structural Plasticity 341 Witte , S . , Stier , H . , & Cline , H . ( 1996 ) . In vivo observations of timecourse and distribution of morphological dynamics in Xenopus retinotectal axon arbors . Journal of Neurobiology , 31 , 219 – 234 . Woolley , C . ( 1999 ) . Structural plasticity of dendrites . In G . Stuart , N . Spruston , & M . H¨ausser ( Eds . ) , Dendrites ( pp . 339 – 364 ) . New York : Oxford University Press . Received August 9 , 2007 ; accepted April 28 , 2009 .