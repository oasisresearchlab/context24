Effects of diversity incentives on sample diversity and downstream model performance in LLM - based text augmentation Jan Cegin ♠† , Branislav Pecher ♠† , Jakub Simko † , Ivan Srba † Maria Bielikova † , Peter Brusilovsky ‡ ♠ Faculty of Information Technology , Brno University of Technology , Brno , Czechia † Kempelen Institute of Intelligent Technologies , Bratislava , Slovakia { jan . cegin , branislav . pecher , jakub . simko , ivan . srba , maria . bielikova } @ kinit . sk ‡ University of Pittsburgh , Pittsburgh , USA peterb @ pitt . edu Abstract The latest generative large language models ( LLMs ) have found their application in data augmentation tasks , where small numbers of text samples are LLM - paraphrased and then used to fine - tune the model . However , more research is needed to assess how different prompts , seed data selection strategies , filtering methods , or model settings affect the quality of paraphrased data ( and downstream models ) . In this study , we investigate three text diversity incentive methods well established in crowd - sourcing : taboo words , hints by previous out - lier solutions , and chaining on previous outlier solutions . Using these incentive methods as part of instructions to LLMs augmenting text datasets , we measure their effects on gener - ated texts’ lexical diversity and downstream model performance . We compare the effects over 5 different LLMs and 6 datasets . We show that diversity is most increased by taboo words , while downstream model performance is high - est when previously created paraphrases are used as hints 1 . 1 Introduction The emergence of large language models ( LLMs ) such as GPT - 4 , LLama , etc . , has sparked in - terest in using these models to augment textual datasets ( Ubani et al . , 2023 ; Dai et al . , 2023 ; Pied - boeuf and Langlais , 2023 ) . In these scenarios , the number of samples is expanded by paraphrasing existing ones through LLM prompting . The created paraphrases are then added to the original dataset and used for downstream model training . Such methods have been explored for various domains such as sentiment classification ( Piedboeuf and Langlais , 2023 ; Ubani et al . , 2023 ) , news classifi - cation ( Piedboeuf and Langlais , 2023 ) and health symptoms classifications ( Dai et al . , 2023 ) . How - ever , investigation of the effect of various prompts , 1 Data and code at : https : / / github . com / kinit - sk / LLM - div - incts specific instructions , and selection of seed data dur - ing the text augmentation process using LLMs is lacking . Crowdsourcing is an established practice for col - lecting training or validation examples for a variety of NLP tasks . Scenarios of data collection using human workers can be similar to those of data aug - mentation : workers create paraphrases from exist - ing sentences chosen from a dataset . The aim of such data collection is to increase data diversity and subsequent performance for models trained on the data ( Larson et al . , 2019 , 2020 ) . To pro - mote the diversity of outputs , various methods are used in crowdsourcing to guide human workers . These include taboo words ( Larson et al . , 2020 ) - where most significant words from the collected data are identified and such words are listed in the worker instructions as words to be avoided during paraphrasing , chaining ( Rhys Cox et al . , 2021 ; Lar - son et al . , 2019 ) - where outliers in the previous paraphrases are identified and are used as seed sen - tences in the next round of data collection , and hints where previous outlier paraphrases are used as ex - amples in the instructions . The hints ( Rhys Cox et al . , 2021 ; Zhou and Bhat , 2020 ) method itself is similar to in - context learning for LLMs , where examples are included in the instructions for the model to achieve better performance . All of these diversity incentive methods report increased diver - sity of collected paraphrases and some also report increased performance of the models trained on data collected . This work is inspired by the parallels between crowdsourcing and LLM prompting and by the performance of diversity incentive methods on the diversity of collected data and performance of mod - els trained on such data . We investigate the effects of various diversity incentive methods ( originat - ing in crowdsourcing ) on data augmentation using LLMs and compare them to using simple prompt - ing taken from a previous study ( Cegin et al . , 2023 ) . a r X i v : 2401 . 06643v1 [ c s . C L ] 12 J a n 2024 Specifically , we use the diversity incentive meth - ods to modify the base prompt asking for para - phrasing a given sentence ( either with taboo words , hints or chaining ) . Measuring paraphrase diversity and downstream model performance , we assess whether the diversity incentives improve LLM out - puts similarly as in crowdsourcing scenarios . In this paper , we answer the following research questions : 1 . RQ1 : Does the usage of diversity incentive methods on LLMs yield more diverse para - phrases ? ( compared to base prompt augmen - tation ) 2 . RQ2 : Do classifiers achieve better perfor - mance if trained on data augmented using diversity incentive methods on LLMs ? ( com - pared to base prompt augmentation ) To answer these questions , we have conducted a data augmentation experiment using 5 different LLMs on 6 different datasets in the tasks of sen - timent ( movie and app reviews ) , news , and intent ( flight and voice assistant commands ) classification . In this experiment , we collect data multiple times from a subset of each dataset using different diver - sity incentive methods compare the lexical diversity of the collected data and the performance of classi - fiers trained on the collected data . Additionally , we also conduct an ablation study , where we modify the diversity incentive methods with random data to validate that the inputs used by these methods ( e . g . , most influential taboo words , outlier paraphrases ) contributes to the method’s performance . The most prominent findings are following : 1 ) The taboo method increases lexical diversity of collected data compared to the baseline , 2 ) The hints method in - creases the performance of models trained on such data compared to the baseline , while also reducing standard deviation and increasing the stability of results , 3 ) The chaining method worsens both lexi - cal diversity and performance of models trained on such data compared to the baseline . 2 Related work : Crowdsourcing and LLM - based augmentation 2 . 1 Crowdsourcing diverse paraphrases Crowdsourcing of paraphrases is an established method to collected data for dataset building and augmentation for various NLP tasks ( Larson et al . , 2019 , 2020 ; Zhou and Bhat , 2020 ; Wei et al . , 2018 ; Rhys Cox et al . , 2021 ) . The process itself typi - cally entails providing an initial seed sentence to the worker that is then asked to paraphrase the seed with new variations . The most common used vari - ant is listing specific prompts ( Wei et al . , 2018 ; Larson et al . , 2020 ) for the worker detailing in - structions of what kind of paraphrases are desired , similar to prompting of LLMs . Various diversity incentive methods exist for crowdsourcing of para - phrases to increase the diversity of collected para - phrases , as research ( Larson et al . , 2020 ; Joshi and He , 2022 ; Wang et al . , 2022 ) suggests correlation between performance of downstream models and diversity of datasets on which they are trained . Hints ( Rhys Cox et al . , 2021 ; Zhou and Bhat , 2020 ) is the most common method which uses task design decisions for guiding workers towards di - verse solutions by including examples of previously created paraphrases by other workers as inspiration to the worker . Other variation of this method shows word - clouds of recommended words to be used in paraphrases or possible synonym of named entities . Hints have been used for the data collection of user utterances for task - oriented chatbots and to collect diverse motivational messages . Taboo ( Larson et al . , 2020 ) introduces specific tabooed words into the instructions for the worker that should be avoided when creating paraphrases . These words are identified in the already collected data as the most influential data using a linear SVM . The usage of tabooed words has been applied in the collection of data for intent classification tasks . Chaining ( Rhys Cox et al . , 2021 ; Larson et al . , 2019 ) identifies outliers or most diverse paraphrase from the already collected data that are then used as seed sentences for the next round of data collection . This method has been applied with different varia - tions for data collection of intent utterances and for the collection of diverse motivational messages . All of these methods listed above have shown to increase the diversity of the collected data and the performance of models trained on such data when compared to data collected via simple prompts . 2 . 2 Data augmentation via LLMs LLMs like GPT - 2 ( Radford et al . , 2019 ) or BART ( Lewis et al . , 2020 ) have been used previ - ously to create paraphrases for an existing corpora . Additional extensions also used style transfer to create paraphrases of a certain linguistic style ( Kr - ishna et al . , 2020 ) , syntax control of the gener - ated paraphrases ( Goyal and Durrett , 2020 ; Chen et al . , 2020 ) , multi - lingual paraphrases in a zero - shot setting ( Thompson and Post , 2020 ) and fine - tuning an LLM using Low Rank Adaption to then collect paraphrases for a specific domain ( Chowd - hury et al . , 2022 ) . Recent studies ( Piedboeuf and Langlais , 2023 ; Ubani et al . , 2023 ) used GPT - 3 . 5 and GPT - 4 as data augmentation techniques that were compared with previous state - of - art NLP aug - mentation techniques . Two studies report better performance in using LLMs as data augmentation techniques than using previous state - of - art tech - niques in both paraphrasing existing texts ( Dai et al . , 2023 ) and in a zero - shot setup of generat - ing new texts using specific prompts ( Ubani et al . , 2023 ) , while another study reports mixed results when GPT - 3 . 5 is compared to previous state - of - art techniques ( Piedboeuf and Langlais , 2023 ) . Re - gardless of the mixed results , GPT - like models have been already used as augmentators in domains of automated scoring ( Fang et al . , 2023 ) and low - resource language generation ( Ghosh et al . , 2023 ) . 3 Data collection process To empirically verify the effect of each diversity incentive method on various LLMs and datasets we gather data in 5 iterations for each dataset and LLM combination . We use a limited amount of labelled data by using 6 samples per one label as the seed sentences to be augmented . These seed sentences change for each data collection iteration and also for each used LLM . We gather data in parallel for each diversity in - centive method in the given iteration . Each iteration consisted of 2 rounds : first we collect data using only the prompt ( basic instructions only ) method and in the second round we collect using all the diversity incentive methods and prompt method in parallel . This is done as all of the methods need previously collected data to work correctly . The resulting datasets for each method consist of seed data and data collected in the first ( prompt only ) and second rounds . The entire data collection pro - cess is visualized in Figure 1 . To evaluate diversity of the collected data , we compare the mean vocabulary size ( no . unique words ) and the amount of unique 3 - grams in be - tween each dataset collected using a different di - versity incentive method . To evaluate the performance of models trained on these data we finetune BERT - large 5 times for each resulting dataset . We evaluate the accuracy of trained model on the test set for that given dataset specifically . This results in 25 results for the com - bination of dataset , model and diversity incentive method . Details can be found in Appendix B . We also check a subset of the collected data ( 50 % of the data ) manually to ensure its validity ( is the augmented sentence adhering to the label and seed sentence ) . 3 . 1 Diversity incentive methods used on LLMs We label the usage of simple instructions taken from ( Cegin et al . , 2023 ) as the prompt method which serves as the baseline comparison to the diversity incentive methods . For the taboo method we take the implementa - tion from ( Larson et al . , 2020 ) that uses a linear SVM trained on bag - of - words representation in a one - vs - many setting to identify the 3 most signif - icant words that are then used in the instructions . We run the computation to get taboo words on the first round of collected data that were collected us - ing the prompt method . We also filter out named entities using NLTK to not include them as taboo words . The prompt used in this method is taken from ( Cegin et al . , 2023 ) . For the chaining method we use an outlier detec - tion method from ( Larson et al . , 2019 ) where we first compute per label a mean embedding vector from the collected samples from the first round . Then , using Euclidean distance , we find the col - lected samples which are the furthest away from the mean vector to be used as seed sentences in the second round of data collection . The prompt used in this method is the same as in the prompt method . The hints method is similar to the previous ap - proach with chaining where we find outliers in the collected data the same way . Here the data are only included in the prompt itself as examples listed with the given seed sentence . The listed examples are always only those which have been created from the given seed to be paraphrased . The prompt is the same as in the prompt method with added delimiter section listing the 3 different hints for the seed sentence . Examples of the prompts can be found in Ap - pendix D . 3 . 2 LLMs used We used 5 different LLMs as data augmentators - 3 open source LLMs and 2 closed LLMs . We Figure 1 : Overview of our data collection process . For each dataset , we randomly sample 6 samples per label that are used as seed sentences for the LLM data augmentation . There , we collect data in parallel for the baseline prompt method and 3 different diversity incentive methods . BERT - large classifier is finetuned 5 times on each of the collected data . We repeat the entire process 5 times . chose the different open source LLMs based on their performance and size on the OpenLLM leader - board and we also used 2 of the most widely used closed LLMs - GPT3 . 5 ( gpt - 3 . 5 - turbo - 1106 ver - sion ) and GPT4 ( gpt - 4 - 0613 version ) . For the open source LLMs , we used the instruction finetuned versions of the LLMs , namely for LLama - 2 ( Tou - vron et al . , 2023 ) we use LLama - 2 - 70B - instruct 2 , for Platypus ( Lee et al . , 2023 ) we use Platypus - 70B - instruct 3 and for Mistral ( Jiang et al . , 2023 ) we use Mistral - 7B - instruct 4 . 3 . 3 Datasets used We used 6 different datasets for our data collection experiments from the domains of news , intent and sentiment classification . We specifically focused on multi - class datasets as the diversity incentive methods were used for such datasets . We used the 20 news ( Lang , 1995 ) and AG news ( Zhang et al . , 2015 ) datasets for news classification , FB ( Schus - ter et al . , 2019 ) and ATIS ( Hemphill et al . , 1990 ) datasets for intent classification and SST - 5 ( Socher et al . , 2013 ) and Yelp ( Zhang et al . , 2015 ) datasets for sentiment classification . We did not use all of the labels in our experiments for the news and in - tent classification datasets , but select a subset of them . More details can be found in Appendix C . 2 https : / / huggingface . co / meta - llama / Llama - 2 - 70b - chat - hf 3 https : / / huggingface . co / garage - bAInd / Platypus2 - 70B - instruct 4 https : / / huggingface . co / mistralai / Mistral - 7B - Instruct - v0 . 1 3 . 4 Ablation study setup To investigate if the diversity incetive methods actu - ally influence the diversity of the collected data and performance of classifiers trained on such data we conduct an ablation study . Here , we repeat the data collection process for the open - source LLMs ( Mis - tral , LLama - 2 , Platypus ) using modified versions of each of the diversity incentive method to inves - tigate whether the particular setup of the methods themselves as they have been used in crowdsourc - ing literature influences the results . For the taboo method , instead of using the most significant words from the previously generated paraphrases we used 3 random words from the gen - erated paraphrases . For the chaining method , in - stead of using the outliers as the next seed sentences we used any previously generated paraphrase that was randomly chosen . For the hints method we did not use outliers as hints , but instead used any pre - viously generated paraphrases that were randomly chosen as hints . 4 Paraphrase diversity and validity We analyzed the collected data from each of the dataset and LLM combination to answer RQ1 whether the usage of diversity methods yields more diverse paraphrases . The diversity incentive meth - ods have no effect on the validity of paraphrases and only the taboo method increases lexical diver - sity significantly and consistently . 4 . 1 Validity of generated paraphrases via diversity incentive methods First , we filtered for potential malformed phrases , empty phrases or duplicated phrases as per ( Cegin et al . , 2023 ) . As we collect only 5 samples per one seed sentence , we have detected no duplicated phrases . There were some malformed phrases gen - erated from all the LLMs with the exception of ChatGPT , but their number was generally low . The highest amount of mangled or empty paraphrases were detected in GPT - 4 responses , mostly when using the chaining method , where the amount of invalid paraphrases was approx . 5 % . For Mistral , LLama - 2 and Platypus we detected around 1 % of mangled paraphrases . We found no impact of diver - sity incentive on the amount of mangled or empty paraphrases for these LLMs . The detected man - gled or empty paraphrases were removed and not included in the next stages . Second , for each dataset and LLM combination , we sampled 50 % of the collected data to be man - ually validated , i . e . we checked whether the re - sulting paraphrases are semantically equivalent to the seed sentences and their labels . Among dif - ferent diversity incentive methods , we detected no invalid utterance , confirming the findings of ( Ce - gin et al . , 2023 ) , where authors reported no invalid paraphrases for intent paraphrase generation using ChatGPT . 4 . 2 Lexical diversity of the generated paraphrases Next , we investigated the effect of diversity incen - tive methods on the lexical diversity of the collected datasets . We focused on the amount of collected unique words ( vocabulary ) and the amount of col - lected unique 3 - grams for each dataset . As we re - peated the data collection process 5 times for each dataset and LLM combination , we report the mean for both no . collected unique words and 3 - grams . We report our findings in Figures 3 and 4 . In nearly all cases except for one ( ChatGPT for the AG News dataset ) the taboo method yielded both higher no . unique words and 3 - grams col - lected . We observed some trends in different dataset domains . In intent classification datasets ( ATIS and FB ) the diversity incentive methods yielded higher diversity than the baseline method in nearly all cases with the exception of the Platypus LLM where the hints method yields considerably lower diversity . In news classification datasets ( 20 News and AG News ) the chaining and hints meth - ods worked worse than the baseline for all of the LLMs with the exception of GPT - 4 , with the hints method working generally worse than the chaining method . In the sentiment classification datasets ( YELP and SST - 5 ) , the hints method offers lower diversity than the baseline for the Mistral , Platypus and LLama - 2 and ChatGPT LLMs and higher di - versity for GPT - 4 . The chaining method worked similar to the hints method with the exception of the Platypus LLM . In summary , the chaining method had better di - versity than the baseline in 9 / 30 cases for unique ngrams and in 4 / 30 cases the diversity was simi - lar . It achieved better diversity in 10 / 30 cases for unique words and similar in 9 / 10 cases . The hints method yielded similar results , achieving better no . of unique ngrams than the baseline 10 / 30 cases and similar in 5 / 30 cases , while achieving better no . unique words in 9 / 30 cases and 8 / 30 cases it was similar to the baseline . We noticed that the longer the texts to be para - phrased became , the lower the benefit of using diversity incetive methods was . The intent clas - sification datasets and the SST - 5 datasets included shorter texts and those cases the usage of hints method yielded generally good results ( except for Platypus LLM ) . However , the longer the texts be - came , the LLMs struggled to deliver higher diver - sity with this method . This might be due to the increasing context the LLMs have to deal with , as 3 long paraphrases are included in the prompt itself which might degrade the LLMs performance . 4 . 3 Ablation study results We also evaluated the results of the ablation study to further study if the diversity incentive methods themselves and their setup had an effect on the diversity of collected paraphrases as per Section 3 . 4 . Here , we compared the no . of collected n - grams and words between the ablated and non - ablated diversity incentive methods . We label methods as of similar performance if the difference in the amount of collected ngrams or words is less than 10 . We report the difference between non - ablated and ablated methods in Figures 6 and 5 . The non - ablated taboo method has betters results in both words ( 14 / 18 cases better , 4 / 18 similar ) and n - grams ( 13 / 18 cases better , 5 / 18 similar ) collected than its ablated counterpart . This would indicate that the usage of the most significant words helps the LLM to gather more diverse data in most cases . In contrast , the non - ablated chaining method yields always ( except for one case of Platypus on the Yelp dataset ) lower diversity than the ablated version . This might indicate that the outlier paraphrases are not good seed sentences to be used for LLMs if the focus is on diversity of the collected data . The non - ablated hints methods performance varies - in 3 / 18 cases the diversity is higher , in 10 / 18 cases it is similar and in 5 / 19 cases the diversity is lower than the ablated version . There seems to be no particular gain in terms of diversity of the collected data when using outliers for both chaining and hints methods , which leaves space for improvement , in particular for the hints method . We answer the RQ1 : Does the usage of diver - sity incentive methods on LLMs yield more diverse paraphrases ? ( compared to base prompt augmen - tation ) as follows : the usage of the taboo method in - creases the lexical diversity of collected data when compared to both the baseline method and the ab - lated version of the method itself . Other meth - ods however affect the diversity of collected para - phrases only randomly ( as is the case of the hints method ) or negatively ( the chaining method ) . The longer the texts to paraphrase become , the worse the effect of diversity incentive methods on the lexical diversity of the collected data becomes . 5 Finetuning models on data collected via diversity incentive methods To investigate whether the diversity incentive meth - ods improve the performance of models in data augmentation scenarios , we finetuned Bert - large 5 times for each sampled dataset and LLM com - bination using different seeds , as we work with limited data , which was found to cause large vari - ance and instability in finetuning results ( Mosbach et al . , 2020 , 2023 ; Pecher et al . , 2023 ) . As we sampled data 5 times , this resulted in 25 finetunted classifiers that we evaluate per dataset and LLM combination . We report accuracy of the finetuned models in our findings and focus on 4 separate at - tributes : mean accuracy , stability of performance ( by measuring standard deviation ) , maximum and minimum value of accuracy to evaluate whether diversity incetive methods increase performance of models trained on such data . We are most inter - ested in consistent better performance of a diversity incentive method over the prompt baseline across LLMs and datasets , as fluctuating performance could be an indicator of random performance . We report our findings in Figure 2 and in Appendix A . 5 . 1 Investigating performance of models trained on data collected via diversity incentive methods In terms of mean achieved accuracy from all the diversity incentive methods , the hints method achieved best performance across all LLM and dataset combinations by consistently outperform - ing or achieving similar mean value as the baseline prompt method in 28 out of 30 LLM and dataset combinations - in 19 cases the mean performance is better and in 9 cases the mean performance is simi - lar ( difference of less than 0 . 1 % ) . In 2 cases ( com - bination of Platypus LLM with 20 News dataset and LLama - 2 LLM with Yelp dataset ) where the hints method was worse than the baseline the datasets consisted of longer texts , meaning the increased . The taboo method did outperform the hints method 6 / 30 cases , but only managed to outperform the baseline method in 6 / 30 cases and in 8 / 30 cases achieved similar mean performance . The chaining method outperformed the baseline method only in 3 / 30 cases and in 3 / 30 cases achieved similar per - formance , thus performing the worst out of all the diversity incentive methods . In terms of performance stability , the models finetuned on data collected via the hints method achieve better stability of performance in 22 / 30 cases and similar stability of performance in 5 cases . The taboo method achieves better stability for 14 / 30 cases and similar stability in 7 / 30 cases . The chaining method achieves better stability of performance only half of the time . In terms of minimum and maximum of perfor - mance achieved , the hints method achieves 28 / 30 cases higher minimum performance than the base - line and in 17 / 30 cases achieves higher maximum performance than the baseline . The taboo method achieves higher minimum performance 18 / 30 cases and higher maximum performance in 9 / 30 cases . The chaining method has the worst performance again , achieving higher minimum performance in 13 / 30 cases and higher maximum performance in 9 / 10 cases . In nearly all cases the hints method achieves higher mean performance than the baseline prompt method and most of the time achieves higher stabil - ity of performance as seen by decreased standard deviation and increased minimum value of fine - tuned models . The taboo method more often than not increases the performance over the baseline and achieves lower stability , but only does so around 53 - 56 % of the time , which could indicate random chance at play . Models finetuned on data collected this way can also underperform significantly , as can be seen by the Mistral , ChatGPT LLM and ATIS dataset combination . The chaining method more often performs worse than the baseline method , in - dicating that the data generated from this method are of lower quality . 5 . 2 Ablation study results Similar to the Section 4 about diversity and validity of paraphrases , we evaluate the diversity incentive methods also in terms of an ablation study con - ducted via details from Section 3 . 4 to investigate whether the setup of the methods themselves con - tributes to their performance . We visualize our findings in Figure 7 . The non - ablated hints method has in all cases better mean performance than the ablated version while achieving better or similar stability in 12 out of 18 cases of LLM and dataset combination . This seems to indicate that the usage of outliers as hints for the LLMs tends to increase the quality of col - lected data in data augmentation scenarios . The non - ablated taboo method achieves better mean performance in 8 / 18 cases and similar mean perfor - mance in 3 / 18 cases when compared to the ablated version of the taboo method . The stability of the non - ablated version is better only in 7 / 18 cases compared to the ablated version . This implies that the usage of most significant words as taboo in - structions for the LLMs has more of random effect on the LLMs in a data augmentation scenario . The non - ablated version of chaining method achieves better mean performance in 9 / 18 cases and similar mean performance in 3 / 18 cases while the stability of performance is better in 8 / 18 cases . This im - plies , similar to the taboo method , that the usage of previous outliers as seed sentences has random effect on LLMs in a data augmentation scenario when compared to the usage of random previous paraphrases as seed sentences . We answer the RQ2 : Do classifiers achieve bet - ter performance if trained on data augmented using diversity incentive methods on LLMs ? ( compared to base prompt augmentation ) as follows : only models finetuned on data collected via the hints method achieve better stability and mean perfor - mance than those trained on data collected via the baseline prompt method . The hints method also achieves better mean performance and stability of performance when compared to its ablated version . The data collected via the taboo method have ran - dom influence on the performance of finetuned models , while models finetuned on the data col - lected via the chaining method achieve generally worse performance . This indicates that the usage of outliers as hints for LLMs in a data augmenta - tion scenarios is beneficial , while other methods have no advantage over the baseline of using only prompt instructions . 6 Discussion Given the result of our experiments , we note these following observations : First , contrary to the per - formance observed in related work on diversity incentive method as per Section 3 . 1 , where these methods yield better diversity of collected data and better performance of models trained on such data in crowdsourcing settings , the methods do not show such improvement when used on LLMs . The worst performing method is the chaining method , where recent works already point to the fact that LLMs perform worse and worse paraphrases when using their own data as seed sentences repeatedly ( Tripto et al . , 2023 ) , as is the case of this method . The all around best performing method for data aug - mentation is the hints method , which is similar to in - context learning where demonstrations of sam - ples are provided to the LLM as part of the prompt . This might be the reason why this method works so well , as the own paraphrases of the LLM guide it to better output , similar to in - context learning . Second , we observe that , contrary to some previ - ous works ( Larson et al . , 2020 ; Joshi and He , 2022 ; Wang et al . , 2022 ) , the lexical diversity of the para - phrases does not correlate with performance of models trained on such data . Even though the data collected using the taboo method yield highest lex - ical diversity , models trained on such data do not achieve consistently better performance than those trained on data collected via our baseline method . Third , although there is some increase in mean performance and the stability of performance when training models on data collected via the hints method , the gains can be considered small given the dataset : depending on the dataset and LLM , the increase of mean performance might be rang - ing from 0 . 2 % to 4 % accuracy . At the same time , Figure 2 : The accuracy of Bert - large classifier on test data that was trained on data collected via different diversity incentive methods using various LLMs . The best performing methods is the hints method , which generally increases mean performance of the models and stability of performance . The taboo method has close to random influence on model performance while the chaining method generally decreases model performance . Figure 3 : Results of diversity incentive methods on no . of collected unique 3 - grams per dataset and LLM combination . The taboo method generally increases the no . of collected unique 3 - grams , while the chaining and hints methods have random effects . Figure 4 : Results of diversity incentive methods on no . of collected unique words per dataset and LLM combination . The taboo method generally increases the no . of collected unique words , while the chaining and hints methods have random effects . the increase in stability is more significant , as the standard deviation is often reduced by 0 . 2 % to 2 % ( which represents up to 35 % , e . g . , going from 4 . 04 to 2 . 67 ) . Fourth , given the fact that the model perfor - mance increases are small , the practicality of us - ing the diversity incentive methods is questionable . These methods require additional computations such as identifying the most significant words using a linear SVM or computation of outliers from the already collected paraphrases . Additionally , these methods increase the context needed for LLMs ( e . g . hints use additional paraphrases in instructions of the model ) meaning higher cost of inference . Fifth , the worse results of the taboo method might be due to the fact that the method some - times picks unrelated words , as the most signifi - cant words that are tabooed are computed per label which might result in taboo words which have no relevance to the seed sentence and a more relevant filtering per seed sentence could be applied . Our promising results using the hints method open new possibilities for investigations of in - context learning scenarios for text generation sce - narios in LLMs , as the quality of such generated data using hints seems to be better than without them . This is in line with the recent results ( Cox et al . , 2023 ) that indicate that the usage of previous examples in instructions for LLMs leads to better generated data . 7 Limitations and Future Work We note several limitations to our work . First , we did not explore the combinations of individual di - versity incentive methods and the possible effects of them . Second , we did not use different types of prompts in our experiments and followed used in previous studies ( Cegin et al . , 2023 ; Larson et al . , 2020 ) . Different prompts could have effects on the quality of LLMs , but would radically increase the size of this study , and as such we decided to leave this for future work . Third , we did not finetune models other than Bert - large in our experiments for the data augmen - tation scenarios . Most notably , it could be inter - esting to investigate the performance of finetuned LLMs such as Mistral - 7B for classification using data collected from the same LLM and put it in contrast to zero - shot classification , similar to the setting in ( Mosbach et al . , 2023 ) . Fourth , we did not conduct an ablation study on ChatGPT and GPT - 4 LLMs due to time constraints and plan to do so in the future . 8 Conclusion In this work , we investigated the effects of differ - ent diversity incentive methods ( commonly used in crowdsourcing ) on the lexical diversity of LLM - augmented textual datasets and performance of models trained on such data . We compared three of such methods with a baseline of using only prompts asking the LLM to paraphrase a given seed . We experimented with 5 LLMs on 6 datasets in the tasks of sentiment ( SST - 5 and Yelp reviews datasets ) , news ( 20 News and AG News datasets ) and intent ( ATIS and FB datasets ) classification . Our results indicate that the taboo method increases lexical diversity of the collected data and affects performance only randomly , the hints method af - fects lexical diversity randomly , but increases the performance of models ( both in mean performance and stability of performance ) that were trained on data collected using this method , while the chain - ing method worsens both lexical diversity of the collected data and model performance of models trained on data collected using this method . A common downside of diversity incentive meth - ods is , that they increase the context size of the in - structions given to LLMs by additional instructions or examples in the prompts . This increases the in - ference costs needed to generate the data . Also , there is still some randomness present when using these methods , as even the best performing meth - ods do not increase lexical diversity or performance of models in all cases . Our results shed light on possibilities of us - ing crowdsourcing - inspired techniques for LLMs . Other such techniques could inspire better data gen - eration using LLMs ( Grunde - McLaughlin et al . , 2023 ) . In our future work , we plan to explore these possibilities . We also plan to investigate the effects of combining the diversity incentive methods on diversity of collected data and model performance when trained on such data . Finally , we plan to investigate different configurations for the diver - sity incentive methods , as those could improve the performance of such methods . Acknowledgements This work was partially supported by AI - CODE , a project funded by the European Union under the Horizon Europe , GA No . 101135437 , and by vera . ai , a project funded by European Union under the Horizon Europe , GA No . 101070093 . References Jan Cegin , Jakub Simko , and Peter Brusilovsky . 2023 . ChatGPT to replace crowdsourcing of paraphrases for intent classification : Higher diversity and com - parable model robustness . In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 1889 – 1905 , Singapore . Association for Computational Linguistics . Wenqing Chen , Jidong Tian , Liqiang Xiao , Hao He , and Yaohui Jin . 2020 . A semantically consistent and syn - tactically variational encoder - decoder framework for paraphrase generation . In Proceedings of the 28th International Conference on Computational Linguis - tics , pages 1186 – 1198 , Barcelona , Spain ( Online ) . International Committee on Computational Linguis - tics . Jishnu Ray Chowdhury , Yong Zhuang , and Shuyi Wang . 2022 . Novelty controlled paraphrase generation with retrieval augmented conditional prompt tuning . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36 , pages 10535 – 10544 . Samuel Rhys Cox , Ashraf Abdul , and Wei Tsang Ooi . 2023 . Prompting a large language model to generate diverse motivational messages : A comparison with human - written messages . In Proceedings of the 11th International Conference on Human - Agent Interac - tion , HAI ’23 , page 378 – 380 , New York , NY , USA . Association for Computing Machinery . Haixing Dai , Zhengliang Liu , Wenxiong Liao , Xiaoke Huang , Yihan Cao , Zihao Wu , Lin Zhao , Shaochen Xu , Wei Liu , Ninghao Liu , Sheng Li , Dajiang Zhu , Hongmin Cai , Lichao Sun , Quanzheng Li , Dinggang Shen , Tianming Liu , and Xiang Li . 2023 . Auggpt : Leveraging chatgpt for text data augmentation . Luyang Fang , Gyeong - Geon Lee , and Xiaoming Zhai . 2023 . Using gpt - 4 to augment unbalanced data for automatic scoring . Sreyan Ghosh , Chandra Kiran Evuru , Sonal Kumar , S Ramaneswaran , S Sakshi , Utkarsh Tyagi , and Di - nesh Manocha . 2023 . Dale : Generative data aug - mentation for low - resource legal nlp . In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Sentosa , Singapore . Tanya Goyal and Greg Durrett . 2020 . Neural syntactic preordering for controlled paraphrase generation . In Proceedings of the 58th Annual Meeting of the Associ - ation for Computational Linguistics , pages 238 – 252 , Online . Association for Computational Linguistics . Madeleine Grunde - McLaughlin , Michelle S . Lam , Ran - jay Krishna , Daniel S . Weld , and Jeffrey Heer . 2023 . Designing llm chains by adapting techniques from crowdsourcing workflows . Charles T . Hemphill , John J . Godfrey , and George R . Doddington . 1990 . The ATIS spoken language sys - tems pilot corpus . In Speech and Natural Language : Proceedings of a Workshop Held at Hidden Valley , Pennsylvania , June 24 - 27 , 1990 . Albert Q . Jiang , Alexandre Sablayrolles , Arthur Men - sch , Chris Bamford , Devendra Singh Chaplot , Diego de las Casas , Florian Bressand , Gianna Lengyel , Guil - laume Lample , Lucile Saulnier , Lélio Renard Lavaud , Marie - Anne Lachaux , Pierre Stock , Teven Le Scao , Thibaut Lavril , Thomas Wang , Timothée Lacroix , and William El Sayed . 2023 . Mistral 7b . Nitish Joshi and He He . 2022 . An investigation of the ( in ) effectiveness of counterfactually augmented data . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 3668 – 3681 , Dublin , Ireland . Association for Computational Linguistics . Kalpesh Krishna , John Wieting , and Mohit Iyyer . 2020 . Reformulating unsupervised style transfer as para - phrase generation . EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Process - ing , Proceedings of the Conference , pages 737 – 762 . Ken Lang . 1995 . Newsweeder : Learning to filter net - news . In Armand Prieditis and Stuart Russell , editors , Machine Learning Proceedings 1995 , pages 331 – 339 . Morgan Kaufmann , San Francisco ( CA ) . Stefan Larson , Anish Mahendran , Andrew Lee , Jonathan K . Kummerfeld , Parker Hill , Michael A . Laurenzano , Johann Hauswald , Lingjia Tang , and Ja - son Mars . 2019 . Outlier detection for improved data quality and diversity in dialog systems . NAACL HLT 2019 - 2019 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies - Proceedings of the Conference , 1 : 517 – 527 . Stefan Larson , Anthony Zheng , Anish Mahendran , Rishi Tekriwal , Adrian Cheung , Eric Guldan , Kevin Leach , and Jonathan K . Kummerfeld . 2020 . Iterative feature mining for constraint - based data collection to increase data diversity and model robustness . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 8097 – 8106 , Online . Association for Computa - tional Linguistics . Ariel N . Lee , Cole J . Hunter , and Nataniel Ruiz . 2023 . Platypus : Quick , cheap , and powerful refinement of llms . Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2020 . BART : Denoising sequence - to - sequence pre - training for natural language generation , translation , and com - prehension . In Proceedings of the 58th Annual Meet - ing of the Association for Computational Linguistics , pages 7871 – 7880 , Online . Association for Computa - tional Linguistics . Marius Mosbach , Maksym Andriushchenko , and Diet - rich Klakow . 2020 . On the stability of fine - tuning bert : Misconceptions , explanations , and strong base - lines . In International Conference on Learning Rep - resentations . Marius Mosbach , Tiago Pimentel , Shauli Ravfogel , Di - etrich Klakow , and Yanai Elazar . 2023 . Few - shot fine - tuning vs . in - context learning : A fair compari - son and evaluation . In Findings of the Association for Computational Linguistics : ACL 2023 , pages 12284 – 12314 , Toronto , Canada . Association for Computa - tional Linguistics . Branislav Pecher , Ivan Srba , and Maria Bielikova . 2023 . On the effects of randomness on stability of learning with limited labelled data : A systematic literature review . arXiv preprint arXiv : 2312 . 01082 . Frédéric Piedboeuf and Philippe Langlais . 2023 . Is ChatGPT the ultimate data augmentation algorithm ? In Findings of the Association for Computational Linguistics : EMNLP 2023 , pages 15606 – 15615 , Sin - gapore . Association for Computational Linguistics . Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 . Language models are unsupervised multitask learners . Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Chris - tian von der Weth , and Brian Y . Lim . 2021 . Directed diversity : Leveraging language embedding distances for collective creativity in crowd ideation . In Pro - ceedings of the 2021 CHI Conference on Human Factors in Computing Systems , CHI ’21 , New York , NY , USA . Association for Computing Machinery . Sebastian Schuster , Sonal Gupta , Rushin Shah , and Mike Lewis . 2019 . Cross - lingual transfer learning for multilingual task oriented dialog . In Proceedings of the 2019 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3795 – 3805 , Minneapolis , Min - nesota . Association for Computational Linguistics . Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D . Manning , Andrew Ng , and Christopher Potts . 2013 . Recursive deep models for semantic compositionality over a sentiment treebank . In Proceedings of the 2013 Conference on Empiri - cal Methods in Natural Language Processing , pages 1631 – 1642 , Seattle , Washington , USA . Association for Computational Linguistics . Brian Thompson and Matt Post . 2020 . Paraphrase gener - ation as zero - shot multilingual translation : Disentan - gling semantic similarity from lexical and syntactic diversity . In Proceedings of the Fifth Conference on Machine Translation , pages 561 – 570 , Online . Asso - ciation for Computational Linguistics . Hugo Touvron , Louis Martin , Kevin Stone , Peter Al - bert , Amjad Almahairi , Yasmine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhosale , Dan Bikel , Lukas Blecher , Cristian Canton Ferrer , Moya Chen , Guillem Cucurull , David Esiobu , Jude Fernandes , Jeremy Fu , Wenyin Fu , Brian Fuller , Cynthia Gao , Vedanuj Goswami , Naman Goyal , An - thony Hartshorn , Saghar Hosseini , Rui Hou , Hakan Inan , Marcin Kardas , Viktor Kerkez , Madian Khabsa , Isabel Kloumann , Artem Korenev , Punit Singh Koura , Marie - Anne Lachaux , Thibaut Lavril , Jenya Lee , Di - ana Liskovich , Yinghai Lu , Yuning Mao , Xavier Mar - tinet , Todor Mihaylov , Pushkar Mishra , Igor Moly - bog , Yixin Nie , Andrew Poulton , Jeremy Reizen - stein , Rashi Rungta , Kalyan Saladi , Alan Schelten , Ruan Silva , Eric Michael Smith , Ranjan Subrama - nian , Xiaoqing Ellen Tan , Binh Tang , Ross Tay - lor , Adina Williams , Jian Xiang Kuan , Puxin Xu , Zheng Yan , Iliyan Zarov , Yuchen Zhang , Angela Fan , Melanie Kambadur , Sharan Narang , Aurelien Ro - driguez , Robert Stojnic , Sergey Edunov , and Thomas Scialom . 2023 . Llama 2 : Open foundation and fine - tuned chat models . Nafis Irtiza Tripto , Saranya Venkatraman , Dominik Macko , Robert Moro , Ivan Srba , Adaku Uchendu , Thai Le , and Dongwon Lee . 2023 . A ship of theseus : Curious cases of paraphrasing in llm - generated texts . Solomon Ubani , Suleyman Olcay Polat , and Rodney Nielsen . 2023 . Zeroshotdataaug : Generating and augmenting training data with chatgpt . Haohan Wang , Zeyi Huang , Hanlin Zhang , Yong Jae Lee , and Eric P Xing . 2022 . Toward learning human - aligned cross - domain robust models by countering misaligned features . In Uncertainty in Artificial In - telligence , pages 2075 – 2084 . PMLR . Lee Wei , Hsuan Huang Chi , Wei Chang Chien , Kuang Daniel Wu Ming , Ta Chuang Kun , An Yang Po , and Cheng Hsieh Chu . 2018 . Effective quality assurance for data labels through crowdsourcing and domain ex - pert collaboration . In Advances in Database Technol - ogy - EDBT 2018 , Advances in Database Technology - EDBT , pages 646 – 649 . OpenProceedings . org . Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 . Character - level convolutional networks for text classi - fication . Advances in neural information processing systems , 28 . Jianing Zhou and Suma Bhat . 2020 . Dynamic word rec - ommendation to obtain diverse crowdsourced para - phrases of user utterances . In International Confer - ence on Intelligent User Interfaces , Proceedings IUI , pages 55 – 66 . A Full results of model performance on In this section we report full result of our experi - ments for each dataset , diversity incentive method and LLM in Tables 1 , 2 , 4 , 3 , 5 and 6 . 20 News PROMPT TABOO CHAINING HINTS ChatGPT 60 . 01 5 . 43 59 . 45 5 . 54 57 . 78 5 . 92 60 . 30 4 . 96 GPT - 4 61 . 38 2 . 80 65 . 44 2 . 58 62 . 30 3 . 84 65 . 43 2 . 95 Mistral 58 . 91 3 . 81 58 . 53 3 . 19 57 . 77 3 . 28 58 . 92 2 . 90 LLaMA - 2 60 . 62 4 . 46 59 . 32 4 . 55 59 . 58 5 . 26 60 . 87 3 . 88 Platypus 61 . 95 3 . 36 61 . 08 3 . 37 60 . 24 3 . 62 61 . 02 2 . 41 Table 1 : Performance of Bert - large classifier on the test split of the 20 News dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . AG News PROMPT TABOO CHAINING HINTS ChatGPT 79 . 45 2 . 37 79 . 32 2 . 46 78 . 14 2 . 68 80 . 43 2 . 36 GPT - 4 79 . 35 3 . 09 79 . 53 2 . 89 77 . 74 2 . 95 79 . 36 2 . 06 Mistral 83 . 38 1 . 75 83 . 26 1 . 83 82 . 79 2 . 43 83 . 40 1 . 62 LLaMA - 2 81 . 08 3 . 19 81 . 83 3 . 23 81 . 21 3 . 35 81 . 56 3 . 21 Platypus 78 . 56 4 . 34 79 . 82 3 . 45 78 . 65 4 . 35 79 . 56 3 . 80 Table 2 : Performance of Bert - large classifier on the test split of the AG News dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . B Bert - large finetuning details We used the bert - large - uncased version of the model from Huggingface , with batch size of 32 , classifier dropout set to 0 . 2 , used the AdamW opti - mizer with learning rate set to 1e - 5 and trained for 80 epochs . We evaluated the model during training after each 10 epochs and saved its performance . We reported the best test performance for each of the models during training . FB PROMPT TABOO CHAINING HINTS ChatGPT 83 . 10 2 . 32 81 . 70 2 . 06 82 . 25 2 . 19 83 . 11 1 . 51 GPT - 4 82 . 56 2 . 92 80 . 55 4 . 40 80 . 80 3 . 37 82 . 51 2 . 47 Mistral 79 . 18 3 . 12 77 . 98 4 . 14 78 . 72 4 . 10 79 . 44 3 . 68 LLaMA - 2 79 . 60 4 . 04 79 . 34 4 . 12 79 . 44 2 . 67 80 . 58 2 . 67 Platypus 80 . 75 2 . 10 79 . 60 2 . 79 79 . 86 4 . 74 82 . 23 2 . 25 Table 3 : Performance of Bert - large classifier on the test split of the FB dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . ATIS PROMPT TABOO CHAINING HINTS ChatGPT 75 . 36 16 . 26 71 . 05 16 . 62 74 . 37 16 . 37 77 . 20 13 . 85 GPT - 4 76 . 06 8 . 78 74 . 61 7 . 90 74 . 30 9 . 80 76 . 47 8 . 76 Mistral 79 . 83 9 . 75 74 . 58 7 . 60 75 . 10 11 . 15 80 . 32 9 . 21 LLaMA - 2 82 . 88 5 . 36 78 . 20 6 . 31 81 . 11 6 . 03 83 . 35 5 . 53 Platypus 83 . 53 8 . 30 81 . 29 8 . 47 81 . 18 8 . 98 83 . 73 6 . 81 Table 4 : Performance of Bert - large classifier on the test split of the ATIS dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . SST - 5 PROMPT TABOO CHAINING HINTS ChatGPT 34 . 94 2 . 51 36 . 06 2 . 70 35 . 28 2 . 03 35 . 85 2 . 06 GPT - 4 33 . 89 2 . 69 33 . 92 2 . 13 33 . 41 2 . 54 34 . 24 2 . 23 Mistral 33 . 19 2 . 94 32 . 74 2 . 94 32 . 54 2 . 98 33 . 46 2 . 43 LLaMA - 2 33 . 37 2 . 74 34 . 83 2 . 17 32 . 97 2 . 49 33 . 63 2 . 15 Platypus 33 . 89 2 . 69 33 . 92 2 . 13 33 . 41 2 . 54 34 . 24 2 . 23 Table 5 : Performance of Bert - large classifier on the test split of the SST - 5 dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . C Dataset details As we did not use all of the dataset labels and samples in each of the dataset , we list our setup here . We mostly used labels that were in the datasets with similar quantity to deal with the imbalanced datasets issue . For the 20 News dataset we used samples with labels politics , wellness , entertainment , travel , style , beauty and parenting . For the AG News , SST - 5 and Yelp datasets we Yelp PROMPT TABOO CHAINING HINTS ChatGPT 42 . 59 2 . 39 42 . 50 1 . 95 42 . 73 2 . 04 42 . 64 1 . 93 GPT - 4 41 . 50 2 . 66 41 . 59 2 . 47 40 . 69 3 . 35 41 . 94 2 . 83 Mistral 39 . 43 2 . 83 40 . 08 2 . 76 38 . 94 2 . 21 39 . 69 2 . 62 LLaMA - 2 43 . 08 3 . 30 43 . 07 2 . 82 42 . 09 2 . 70 42 . 64 2 . 96 Platypus 43 . 37 3 . 05 42 . 79 2 . 75 43 . 15 2 . 79 43 . 35 2 . 66 Table 6 : Performance of Bert - large classifier on the test split of the Yelp dataset after being trained 5 times for each of the repeated 5 data collection rounds . We report the mean performance and standard deviation . The hints method generally increases mean performance and stability of performance when compared to baseline prompt method . used all the samples . For the ATIS dataset we used samples with labels atis _ abbreviation , atis _ aircraft , atis _ airfare , atis _ airline , atis _ flight , atis _ flight _ time , atis _ ground _ service and atis _ quantity . For the FB dataset we used samples with labels get _ directions , get _ distance , get _ estimated _ arrival , get _ estimated _ departure , get _ estimated _ duration , get _ info _ road _ condition , get _ info _ route , get _ info _ traffic , get _ location and update _ directions . D Examples of diversity incentive prompts used in LLMs The prompt and chaining method : Rephrase an original question or statement 3 times . Original phrase : seed _ phrase . The taboo method : Rephrase an original ques - tion or statement 3 times . Original phrase : seed _ phrase . Don’t use the words “word _ 1” , “word _ 2 " or “word _ 3” in your responses . The hints method : Rephrase an original ques - tion or statement 3 times . Original phrase : seed _ phrase . # # # Example paraphrases : phrase _ 1 , phrase _ 2 , phrase _ 3 # # # E Results of the ablation study In this section we list visualizations of the results for ablated versions of different diversity incentive methods in lexical diversity in Figures 5 and 6 and in accuracy of models trained on data collected this way in Figure 7 . Figure 5 : The change in no . of collected unique 3 - grams when comparing ablated methods with non - ablated . The figure displays the change of diversity of the ablated version of the diversity incentive methods vs . the non - ablated version . The ablated version of the taboo method performs generally worse , indicating that the tabooing of most significant words increases diversity of texts collected via LLMs . Figure 6 : The change in no . of collected unique words when comparing ablated methods with non - ablated . The figure displays the change of diversity of the ablated version of the diversity incentive methods vs . the non - ablated version . The ablated version of the taboo method performs generally worse , indicating that the tabooing of most significant words increases diversity of texts collected via LLMs . Figure 7 : The change in accuracy of models trained on data collected via ablated and non - ablated diversity incentive methods . The figure displays the change of accuracy of the ablated version of the diversity incentive methods vs . the non - ablated version . The ablated version of the hints method performs generally worse , indicating that the inclusion of previous examples in the data collection yield better data .