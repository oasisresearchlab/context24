Get Another Label ? Improving Data Quality And Data Mining Using Multiple , Noisy Labelers Victor S . Sheng Foster Provost Panagiotis G . Ipeirotis Stern School , New York University Presented by : Akash Abdu Jyothi Background â€¢ Obtaining expert labeling is an integral part of KDD ( Knowledge Discovery in Databases ) preprocessing â€¢ Is it possible to obtain good data values ( â€œlabelsâ€ ) relatively cheaply from multiple noisy sources ( â€œlabelersâ€ ) ? â€¢ Used as training labels for supervised modeling Repeated Labelingâ€¦ ? â€¢ Labels are imperfect â€¢ Raghu Ramakrishnan from his SIGKDD Innovation Award Lecture ( 2008 ) â€œthe best you can expect are noisy labelsâ€ â€¢ Modeling tasks often require high quality labeling â€¢ Outsourcing labeling tasks â€¢ Quality may be lower than expert labeling â€¢ But low costs can allow massive scale Effect of Low Quality Labels Learning curves under different quality levels ( q ) of training data for classification problem Outline â€¢ Data quality with repeated labeling â€¢ Model quality with repeated labeling â€¢ Summary and future work Part 1 â€“ Quality of Repeated Labeling â€¢ Problem â€“ supervised induction of a binary classification model â€¢ Training example ( x i , y i ) â€¢ C U â€“ cost of procuring unlabeled â€œfeatureâ€ portion â€¢ C L â€“ cost of labeling x i with a label y i â€¢ Assumptions â€¢ C U and C L are constant for all examples â€¢ Labeler quality is constant regardless of the example â€¢ p j is the probability that j th labeler gets a label correct Majority voting â€“ Uniform labeler Quality â€¢ Using 2N + 1 labelers of uniform quality i . e . p j = p â€¢ Integrated labeling quality q is the sum of probabilities where we have more correct than wrong answers ğ‘ = 2ğ‘ + 1 ğ‘– . ğ‘ 2ğ‘ + 1âˆ’ğ‘– . ( 1 âˆ’ ğ‘ ) ğ‘– ğ‘ ğ‘– = 0 Majority voting â€“ Uniform labeler Quality The relationship between integrated labeling quality , individual quality , and the number of labelers Majority voting â€“ Different labeler Quality â€¢ Special case of a group of three labelers with labeling qualities p - d , p and p + d Repeated - labeling gives better quality than the best labeler ( p + d ) when d is below the curve Uncertainty Preserving Labeling â€¢ Majority voting â€“ information about label uncertainty is lost ! â€¢ Solutionâ€¦ ? 1 . Soft labels â€¢ Probabilistic label for each example â€¢ Difficult in practice â€“ not all modeling techniques and software packages accommodate this 2 . Multiplied Examples ( ME ) â€¢ Create one replica of x i with each unique label that is assigned â€¢ Assign weight ( 1 / n ) to each label based on the number of times it appears ( n ) â€¢ Can be incorporated into learning algorithms easily ! Part 2 - Repeated Labeling and Modeling â€¢ How to improve classification by modifying dataset with noisy labels ? More examples More labels per examples Part 2 - Repeated Labeling and Modeling â€¢ 12 datasets selected for binary classification problem â€¢ J48 ( decision tree ) in WEKA used for the experiments â€¢ 30 % of examples held out in each case as test data Round - robin Strategy , C U < < C L â€¢ Majority Voting ( MV ) acquires additional labels for the initial set of examples â€¢ Single Labeling ( SL ) acquires new examples and their labels Round - robin Strategy , General Costs â€¢ Define data acquisition cost ğ‘‡ ğ‘Ÿ - Number of new unlabeled samples collected ğ‘ ğ¿ - Number of samples to be labeled â€¢ ğ‘ ğ¿ = ğ‘‡ ğ‘Ÿ for single labeling , ğ‘ ğ¿ > ğ‘‡ ğ‘Ÿ for repeated labeling â€¢ New repeated labeling strategy â€“ for every new example acquired repeated labeling acquires a fixed number of labels ğ‘˜ , i . e . ğ‘ ğ¿ = ğ‘˜ âˆ— ğ‘‡ ğ‘Ÿ â€¢ Cost ratio ğœŒ is defined as ğ¶ ğ‘ˆ / ğ¶ğ¿ C D = C U âˆ— T r + C L âˆ— N L Round - robin Strategy , General Costs Increase in model accuracy vs data acquisition cost ( Ï = 3 , k = 5 ) Round - robin Strategy , General Costs Increase in model accuracy vs data acquisition cost ( Ï = 3 , k = 5 ) Round - robin Strategy , General Costs Average improvement per unit cost of repeated - labeling with majority voting over single labeling Round - robin Strategy , General Costs The learning curves of MV and ME with p = 0 . 6 , Ï = 3 , k = 5 , using the splice dataset Uncertainty - preserving repeated labeling performs at least as well as majority vote Selective Repeated Labeling â€¢ Use entropy measure to choose examples for further labeling â€¢ A small set of examples are chosen many times â€¢ More pure but incorrect examples are never visited â€¢ Entropy is scale invariant â€¢ ( 3 + , 2 - ) has the same entropy as ( 600 + , 400 - ) â€¢ Fundamental problem : Entropy is not for uncertainty , but for mixture Do not use Selective Repeated Labeling â€¢ Generalized round - robin repeated labeling outperforms entropy based selective repeated labeling Estimating Label Uncertainty ( LU ) â€¢ We compute a Bayesian estimate of the uncertainty in the class of the example â€¢ Prior distribution over the true label is assumed to be uniform in the interval [ 0 , 1 ] â€¢ Posterior probability thus follows a Beta distribution ğµ ( ğ¿ ğ‘ğ‘œğ‘  + 1 , ğ¿ ğ‘›ğ‘’ğ‘” + 1 ) S LU Beta probability density function â€¢ Tail probability below a labeling decision threshold ( 0 . 5 ) is chosen as the measure of uncertainty Estimating Model Uncertainty ( MU ) â€¢ We apply traditional active learning score ignoring the current multiset of labels â€¢ Learn a set ( ğ‘š ) of models each of which predicts the probability of a class membership , yielding the uncertainty score : â€¢ Pr ( + | ğ‘¥ , ğ» ) is the probability of classifying he example ğ‘¥ into + by the learned model ğ» â€¢ In our experiments , ğ‘š = 10 and model is set to random forest ( WEKA ) ğ‘† ğ‘€ğ‘ˆ = 0 . 5 âˆ’ 1 ğ‘š Pr + ğ‘¥ , ğ» ğ‘– âˆ’ 0 . 5 ğ‘š ğ‘– = 1 Combining Label and Model Uncertainties ( LMU ) â€¢ Finally we combine label and model uncertainty scores to get the best of both worlds MU LU LMU S S S ï‚´ ï€½ Experiment Results â€¢ In high noise setting ( ğ‘ = 0 . 6 ) , MU performs well â€“ learned models can help to choose good examples to relabel ! â€¢ LMU dominates throughout Experiment Results Average accuracies for noisy setting , ğ‘ = 0 . 6 Summary of Results â€¢ Repeated labeling can improve data quality and model quality ( but not always ) â€¢ Repeated labeling can be preferable to single labeling when labels arenâ€™t particularly cheap â€¢ When labels are relatively cheap , repeated labeling can do much better â€¢ Round - robin repeated labeling does well â€¢ Selective repeated labeling performs better Future Work â€¢ Estimating labelersâ€™ quality by observing assigned labels could allow for more sophisticated selective repeated - labelling strategies . â€¢ Study of labeling quality variation with labeler payment . â€¢ Here we introduced noise to the labels . Using real labelers should give a better understanding of the effects of repeated labeling . â€¢ We compared repeated labeling vs fixed labeling , a hybrid process of combining both based on the expected benefit of either methods could provide better data quality .