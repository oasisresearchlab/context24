Get Another Label ? Improving Data Quality And Data Mining Using Multiple , Noisy Labelers Victor S . Sheng Foster Provost Panagiotis G . Ipeirotis Stern School , New York University Presented by : Akash Abdu Jyothi Background • Obtaining expert labeling is an integral part of KDD ( Knowledge Discovery in Databases ) preprocessing • Is it possible to obtain good data values ( “labels” ) relatively cheaply from multiple noisy sources ( “labelers” ) ? • Used as training labels for supervised modeling Repeated Labeling… ? • Labels are imperfect • Raghu Ramakrishnan from his SIGKDD Innovation Award Lecture ( 2008 ) “the best you can expect are noisy labels” • Modeling tasks often require high quality labeling • Outsourcing labeling tasks • Quality may be lower than expert labeling • But low costs can allow massive scale Effect of Low Quality Labels Learning curves under different quality levels ( q ) of training data for classification problem Outline • Data quality with repeated labeling • Model quality with repeated labeling • Summary and future work Part 1 – Quality of Repeated Labeling • Problem – supervised induction of a binary classification model • Training example ( x i , y i ) • C U – cost of procuring unlabeled “feature” portion • C L – cost of labeling x i with a label y i • Assumptions • C U and C L are constant for all examples • Labeler quality is constant regardless of the example • p j is the probability that j th labeler gets a label correct Majority voting – Uniform labeler Quality • Using 2N + 1 labelers of uniform quality i . e . p j = p • Integrated labeling quality q is the sum of probabilities where we have more correct than wrong answers 𝑞 = 2𝑁 + 1 𝑖 . 𝑝 2𝑁 + 1−𝑖 . ( 1 − 𝑝 ) 𝑖 𝑁 𝑖 = 0 Majority voting – Uniform labeler Quality The relationship between integrated labeling quality , individual quality , and the number of labelers Majority voting – Different labeler Quality • Special case of a group of three labelers with labeling qualities p - d , p and p + d Repeated - labeling gives better quality than the best labeler ( p + d ) when d is below the curve Uncertainty Preserving Labeling • Majority voting – information about label uncertainty is lost ! • Solution… ? 1 . Soft labels • Probabilistic label for each example • Difficult in practice – not all modeling techniques and software packages accommodate this 2 . Multiplied Examples ( ME ) • Create one replica of x i with each unique label that is assigned • Assign weight ( 1 / n ) to each label based on the number of times it appears ( n ) • Can be incorporated into learning algorithms easily ! Part 2 - Repeated Labeling and Modeling • How to improve classification by modifying dataset with noisy labels ? More examples More labels per examples Part 2 - Repeated Labeling and Modeling • 12 datasets selected for binary classification problem • J48 ( decision tree ) in WEKA used for the experiments • 30 % of examples held out in each case as test data Round - robin Strategy , C U < < C L • Majority Voting ( MV ) acquires additional labels for the initial set of examples • Single Labeling ( SL ) acquires new examples and their labels Round - robin Strategy , General Costs • Define data acquisition cost 𝑇 𝑟 - Number of new unlabeled samples collected 𝑁 𝐿 - Number of samples to be labeled • 𝑁 𝐿 = 𝑇 𝑟 for single labeling , 𝑁 𝐿 > 𝑇 𝑟 for repeated labeling • New repeated labeling strategy – for every new example acquired repeated labeling acquires a fixed number of labels 𝑘 , i . e . 𝑁 𝐿 = 𝑘 ∗ 𝑇 𝑟 • Cost ratio 𝜌 is defined as 𝐶 𝑈 / 𝐶𝐿 C D = C U ∗ T r + C L ∗ N L Round - robin Strategy , General Costs Increase in model accuracy vs data acquisition cost ( ρ = 3 , k = 5 ) Round - robin Strategy , General Costs Increase in model accuracy vs data acquisition cost ( ρ = 3 , k = 5 ) Round - robin Strategy , General Costs Average improvement per unit cost of repeated - labeling with majority voting over single labeling Round - robin Strategy , General Costs The learning curves of MV and ME with p = 0 . 6 , ρ = 3 , k = 5 , using the splice dataset Uncertainty - preserving repeated labeling performs at least as well as majority vote Selective Repeated Labeling • Use entropy measure to choose examples for further labeling • A small set of examples are chosen many times • More pure but incorrect examples are never visited • Entropy is scale invariant • ( 3 + , 2 - ) has the same entropy as ( 600 + , 400 - ) • Fundamental problem : Entropy is not for uncertainty , but for mixture Do not use Selective Repeated Labeling • Generalized round - robin repeated labeling outperforms entropy based selective repeated labeling Estimating Label Uncertainty ( LU ) • We compute a Bayesian estimate of the uncertainty in the class of the example • Prior distribution over the true label is assumed to be uniform in the interval [ 0 , 1 ] • Posterior probability thus follows a Beta distribution 𝐵 ( 𝐿 𝑝𝑜𝑠 + 1 , 𝐿 𝑛𝑒𝑔 + 1 ) S LU Beta probability density function • Tail probability below a labeling decision threshold ( 0 . 5 ) is chosen as the measure of uncertainty Estimating Model Uncertainty ( MU ) • We apply traditional active learning score ignoring the current multiset of labels • Learn a set ( 𝑚 ) of models each of which predicts the probability of a class membership , yielding the uncertainty score : • Pr ( + | 𝑥 , 𝐻 ) is the probability of classifying he example 𝑥 into + by the learned model 𝐻 • In our experiments , 𝑚 = 10 and model is set to random forest ( WEKA ) 𝑆 𝑀𝑈 = 0 . 5 − 1 𝑚 Pr + 𝑥 , 𝐻 𝑖 − 0 . 5 𝑚 𝑖 = 1 Combining Label and Model Uncertainties ( LMU ) • Finally we combine label and model uncertainty scores to get the best of both worlds MU LU LMU S S S   Experiment Results • In high noise setting ( 𝑝 = 0 . 6 ) , MU performs well – learned models can help to choose good examples to relabel ! • LMU dominates throughout Experiment Results Average accuracies for noisy setting , 𝑝 = 0 . 6 Summary of Results • Repeated labeling can improve data quality and model quality ( but not always ) • Repeated labeling can be preferable to single labeling when labels aren’t particularly cheap • When labels are relatively cheap , repeated labeling can do much better • Round - robin repeated labeling does well • Selective repeated labeling performs better Future Work • Estimating labelers’ quality by observing assigned labels could allow for more sophisticated selective repeated - labelling strategies . • Study of labeling quality variation with labeler payment . • Here we introduced noise to the labels . Using real labelers should give a better understanding of the effects of repeated labeling . • We compared repeated labeling vs fixed labeling , a hybrid process of combining both based on the expected benefit of either methods could provide better data quality .