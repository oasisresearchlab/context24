Random Function Descent Felix Benning University of Mannheim felix . benning @ uni - mannheim . de Leif Döring University of Mannheim leif . doering @ uni - mannheim . de Abstract While gradient based methods are ubiquitous in machine learning , selecting the right step size often requires “hyperparameter tuning” . This is because backtracking procedures like Armijo’s rule depend on quality evaluations in every step , which are not available in a stochastic context . Since optimization schemes can be motivated using Taylor approximations , we replace the Taylor approximation with the conditional expectation ( the best L 2 estimator ) and propose “Random Function Descent” ( RFD ) . Under light assumptions common in Bayesian optimization , we prove that RFD is identical to gradient descent , but with calculable step sizes , even in a stochastic context . We beat untuned Adam in synthetic benchmarks . To close the performance gap to tuned Adam , we propose a heuristic extension competitive with tuned Adam . 1 Motivation and related work The classic problem of optimization in ( supervised ) machine learning is to minimize the loss L ( w ) = E [ (cid:96) ( w , ( X , Y ) ) ] e . g . = E [ (cid:107) f w ( X ) − Y (cid:107) 2 ] , ( 1 ) which is the expectation over randomly sampled data Z = ( X , Y ) of some sample loss (cid:96) derived from a model f w parametrized by w ∈ R d to predict labels Y from inputs X . Since practitioners want to be free to swap in and out models without thinking about the optimizer , machine learning requires black - box optimizers . Global minimization is generally impossible with black - box optimizers due to the curse of dimensionality . So optimization schemes can often be viewed as a variant of Algorithm 1 . Algorithm 1 : Optimization by approximation 1 while not converged do 2 Choose a local approximation ˆ L for the function L around w ; 3 w ← argmin w ˆ L ( w ) ; / / ˆ L needs to be easy to minimize 4 end An obvious example for this type of scheme is the Newton - Raphson method , which minimizes the second Taylor approximation assuming the approximation is convex . To avoid computing the Hessian and other problems [ e . g . 7 ] , ﬁrst oder methods such as gradient descent ( GD ) are however much more common in machine learning . An insightful way to view the unknown step size of GD is as an artifact of pressing ﬁrst order methods into Algorithm 1 . This view is presented in Example 1 . 1 . Example 1 . 1 ( Gradient descent ) . As the second order Taylor approximation results in a reasonable algorithm , one might reach for the ﬁrst order Taylor approximation to obtain a ﬁrst order method . To motivate Random Function Descent ( RFD ) introduced below , we use the following uncommon notation for the ﬁrst Taylor approximation which highlights its dependency on the loss and its gradient : T [ L ( w + d ) | L ( w ) , ∇L ( w ) ] : = T 1 w L ( w + d ) = L ( w ) + (cid:104)∇L ( w ) , d (cid:105) . ( 2 ) Preprint . Under review . a r X i v : 2305 . 01377v1 [ m a t h . O C ] 2 M a y 2023 Figure 1 : Two Matérn Gaussian random functions ( 8 ) ( left : ν = ∞ , s = 2 , right : ν = 2 . 4 , s = 0 . 5 ) The slope changes much more slowly in the smoother function to the left , this is reﬂected by the conditional expectation reverting much more slowly to the zero mean . The ribbon represents two conditional standard deviations around the conditional expectation ( cf . Section 5 ) . Since the minimum of linear functions is typically at inﬁnity , ( 2 ) is unsuitable for Algorithm 1 and needs to be regularized . A common way to do this [ e . g . 13 ] is with L - Lipschitz continuity of ∇L . Using the fundamental theorem of calculus and the Cauchy - Schwarz inequality this implies (cid:12)(cid:12)(cid:12) L ( w + d ) − T 1 w L ( w + d ) (cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)(cid:12)(cid:90) 1 0 (cid:104)∇L ( w + λ d ) − ∇L ( w ) , d (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) C . S . ≤(cid:107)∇L ( w + λ d ) −∇L ( w ) (cid:107)(cid:107) d (cid:107) dλ (cid:12)(cid:12)(cid:12) L - Lip . ≤ L 2 (cid:107) d (cid:107) 2 . This Lipschitz assumption therefore results in the following upper bound ˆ L on L L ( w + d ) ≤ ˆ L ( w + d ) : = T 1 w L ( w + d ) + L 2 (cid:107) d (cid:107) 2 . This upper bound ˆ L is now suitable for Algorithm 1 . The analytical solution to the minimization of ˆ L is gradient descent d ∗ = − 1 L ∇L ( w ) = argmin d ˆ L ( w + d ) . L is generally unknown and needs to be selected . Selecting a large conservative Lipschitz constant L might guarantee progress , but the step size 1 L is small and therefore progress will be slow . So it is desirable to ﬁnd a tight upper bound . However ﬁnding the smallest Lipschitz constant for the gradient is equivalent to ﬁnding the maximum of the Hessian ; this is just another global optimization problem , which means a good optimizer is needed to create a good optimizer . Adaptive step size procedures like backtracking [ e . g . 31 ] essentially try optimistic step sizes and then check whether enough progress was made by comparing the previous with the next loss . If that is not the case , the step size is reduced and the gradient step is attempted again . This means adaptive step size procedures require full loss evaluations ( or at least large batches with low variance ) to succeed . Whenever only sample losses are available , step size selection is typically solved heuristically . Machine learning frameworks provide well - tested defaults which “usually” work well in practice . If those defaults should fail , alternative parameter values are explored , a process commonly referred to as hyperparameter tuning . We highlight “usually” as the colloquial term for “with probability 1 − (cid:15) ” ; i . e . practitioners already assume L is the realization of a random function . The random function setting we formalize in Section 2 is therefore completely natural . Figure 1 visualizes the idea behind RFD : If we have to use an approximation , we should use the best ( L 2 ) approximation , which is the conditional expectation . Replacing the Taylor approximation by the conditional expectation yields a much more stable approximation to L , as the conditional expectation naturally reverts back to the mean far away from known evaluations . So we do not need to modify this approximation for the meta algorithm ( Algorithm 1 ) and use it directly to deﬁne RFD . Deﬁnition 1 . 2 ( Random Function Descent ) . Select w n + 1 as the minimum of the best ( L 2 ) approxi - mation based on the previous evaluation ( including the gradient ) only w n + 1 : = argmin w E [ L ( w ) | L ( w n ) , ∇L ( w n ) ] . ( RFD ) 2 While the deﬁnition itself is abstract , it turns out that RFD is incredibly explicit and cheap to compute ( Theorem 2 . 1 ) . Since our previous emphasis was on sample losses rather than full loss evaluations , let us brieﬂy deﬁne the corresponding ( more general ) stochastic version . Deﬁnition 1 . 3 ( Stochastic Random Function Descent ) . We still want to minimize the true loss , but only have sample losses available . So we deﬁne w n + 1 : = argmin w E [ L ( w ) | (cid:96) ( w n , Z n ) , ∇ (cid:96) ( w n , ˜ Z n ) ] . ( SRFD ) 1 . 1 Bayesian optimization The family of optimization procedures designed to minimize random functions is known as Bayesian optimization [ e . g . 18 , 30 , 11 , 2 ] . 1 To pick the next evaluation point w n + 1 , Bayesian optimiza - tion demands maximization of an “acquisition function” . Since RFD maximizes E [ −L ( w ) | L ( w n ) , ∇L ( w n ) ] , it can simply be viewed as a new type of acquisition function . For compari - son , let us highlight two other common acquisition functions 1 . The expected improvement EI ( w ) : = E (cid:2) [ ˆ L ∗ n − L ( w ) ] + (cid:12) (cid:12) L ( w 1 ) , . . . , L ( w n ) (cid:3) ( 3 ) of possible parameters w , with ˆ L ∗ n : = min { L ( w 1 ) , . . . , L ( w n ) } and [ x ] + : = max { x , 0 } . 2 . Being pressed to choose a minimum after n guesses reasonably results in the choice ˆ w ∗ n : = argmin w E [ L ( w ) | L ( w 1 ) , . . . , L ( w n ) ] . In that case the expected improvement becomes what is called the knowledge gradient KG ( w n + 1 ) : = E [ L ( ˆ w ∗ n ) − L ( ˆ w ∗ n + 1 ) | L ( w 1 ) , . . . , L ( w n ) , w n + 1 ] . So far we have presented Bayesian optimization as a strictly zero - order method . Lizotte [ 21 ] suggested conditioning on gradients as well and Wu et al . [ 35 ] incorporated gradient information into the knowledge gradient to achieve state - of - the - art performance for hyperparameter tuning . Unfortunately , these procedures scale extremely poorly with the dimension d due to their usage of the Cholesky decomposition ( O ( n 3 d 3 ) ) . Padidar et al . [ 25 ] attempted to reduce this computational costs with sparsity in a more recent work . This allowed them to perform optimization in 45 dimensions . In general these methods are used , when the input dimension of L is small and the cost of evaluating L is extremely expensive . Settings which justify expensive deliberation where to evaluate next ; like solving the meta - optimization problem of ﬁnding the right hyperparameters . In comparison to the expected improvement ( 3 ) , RFD is very basic . We do not even reserve the right to revert back to a previous minimum , so regressions matter as much as improvements . And the expected improvement is already conservative in comparison to the knowledge gradient [ 11 ] . While the main advantage for this procedure will be its signiﬁcantly cheaper computational cost , another advantage of this conservatism is that it is more suited to be adapted to SGD - type algorithms , where we do not know the running minimum . Our contributions are as follows 1 . We provide an abstract deﬁnition of RFD as the minimizer of a conditional expectation analogous to the ﬁrst Taylor approximation and prove it is scale - invariant . 2 . We derive an explicit form of RFD for isotropic , Gaussian random functions with constant expectation . The optimal step size is a function of the covariance of the random function . We show that this procedure can be used in a stochastic setting in SGD fashion . 3 . We calculate the optimal step size for a selection of important covariance models analytically . Our optimizer has therefore no need for hyperparameter tuning . 1 The term “Gaussian process” is much more common than “Gaussian random function” in Bayesian optimization [ e . g . 33 ] . But it invokes an unhelpful notion of time . Some authors like to use the term “random ﬁeld” [ e . g . 1 ] . But we feel that the term “random function” [ e . g . 22 ] is most descriptive . 3 4 . We compare RFD to Adam and NAdam on a 1000 dimensional simulated Gaussian random function , explain why tuned Adam outperforms RFD and graft the optimal - step size from RFD onto Nesterov’s momentum to obtain random function momentum ( RFM * ) . Without tuning the step size , Adam performs even worse than just RFD . RFM * is still competitive with tuned Adam and NAdam . 5 . We provide some extensions beyond the isotropy and Gaussian assumptions and discuss the case of quadratic expectation which results in a regularized descent . We also sketch how to construct an upper bound on L with probability 1 − (cid:15) for a more conservative algorithm . 2 Random function descent Random function setting : We already argued for this setting to be natural for optimal step size selection . In addition the view of L as a random function was also used by Dauphin et al . [ 7 ] in their seminal paper to answer the questions : “Why do ﬁrst order optimization methods generally outperform second order methods ? ” and “Why is it sufﬁcient to ﬁnd a local minimum ? ” This setting is therefore useful and common ( also cf . 1 . 1 Bayesian optimization ) . Deﬁning L using the unconditional expectation as in ( 1 ) does not allow for L to be random . A viable mathematical model is : Assume data Z is distributed according to µ θ , where θ is random ( it may represent the universe we live in or the problem we picked ) , i . e . Z = ( X , Y ) ∼ µ θ = P ( ( X , Y ) ∈ dz | θ ) . The loss L then becomes a random function L ( w ) = E Z ∼ µ θ [ (cid:96) ( w , Z ) ] = E [ (cid:96) ( w , Z ) | θ ] . The goal is now to minimize a realization of L , i . e . ﬁnd min w L ( w ) . Assumptions : The explicit form of RFD ( Theorem 2 . 1 ) requires assumptions common for Bayesian optimization . Some of these assumptions are not necessary and can be relaxed ( Theorem C . 5 ) . 1 . We assume L is Gaussian . It is possible to avoid this assumption , if we are satisﬁed with the “Best Linear Unbiased Estimator” ( BLUE , cf . Section B ) in place of the conditional expectation w n + 1 : = argmin w BLUE [ L ( w ) | L ( w n ) , ∇L ( w n ) ] . In fact the proofs will be based on the BLUE , which simply coincides with the conditional expectation in the Gaussian case ( Lemma B . 1 ) . To our knowledge , the possibility to avoid the Gaussian assumption is novel in Bayesian optimization . 2 . We assume constant expectation µ = E [ L ( x ) ] . We discuss this assumption in Section 4 . 3 . We assume the covariance function C ( x , y ) = Cov ( L ( x ) , L ( y ) ) is isotropic ( shift and rotational invariant ) , i . e . there exists a function C such that C ( x , y ) = C ( (cid:107) x − y (cid:107) 2 ) . A generalization to anisotropy is discussed in Remark 2 . 2 . In the Appendix we consider another generalization to rotationally invariant “intrinsically - stationary” random functions ( i . e . isotropic increments ) for the weaker optimization problem argmin w E [ L ( w ) − L ( w n ) | ∇L ( w n ) ] . 4 . To ensure L is differentiable , we essentially need C (cid:48)(cid:48) to exist . 2 This assumption is not vital for our results and is merely required for the gradient ∇L ( x n ) to be a well deﬁned random variable . It might be possible to properly deﬁne derivatives of non - differentiable random functions as random variables in the space of distributions , but this is a measure theoretic question far outside the scope of this work . 2 Note : this assumption only ensures ∇L exists in the L 2 sense . Since non - continuous random functions cause measurability problems , it is common to require C (cid:48)(cid:48) to satisfy an additional light regularity condition ensuring ∇L is almost - surely well deﬁned and continuous [ details in 1 , Sec . 1 . 4 . 2 ] . 4 For the sample errors (cid:15) 0 and (cid:15) 1 deﬁned by (cid:15) 0 = (cid:96) ( w , Z ) − L ( w ) and (cid:15) 1 = ∇ (cid:96) ( w , ˜ Z ) − ∇L ( w ) , we assume zero mean , independence from L and of each other , and the entries of (cid:15) 1 to be iid , i . e . E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 I . 2 . 1 Explicit ( S ) RFD Since the abstract form of SRFD is not particularly useful for the practitioner , the goal of this section is to ﬁnd an explicit form . Theorem 2 . 1 provides two main insights : First , the step direction ( 4 ) of RFD is given by the gradient . Therefore ( S ) RFD is simply ( stochastic ) gradient descent . Secondly , the optimal step size η ∗ is the result of an abstract 1 - dimensional minimization problem involving the covariance function and a simple real parameter ξ . In Section 2 . 2 we show , that this minimization problem can be solved analytically for common covariance functions , resulting in explicit step sizes . Theorem 2 . 1 ( Explicit RFD ) . Under the assumptions stated above , ( S ) RDF is simply ( S ) GD , but with optimal step size η ∗ : = η ∗ ( ξ ) : = argmin η C ( η 2 ) C ( 0 ) ξ − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) using parameter ξ = C ( 0 ) C ( 0 ) + E [ (cid:15) 20 ] σ 2 − C (cid:48) ( 0 ) − C (cid:48) ( 0 ) (cid:124) (cid:123)(cid:122) (cid:125) const . (cid:96) ( w , Z ) − µ (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) . More precisely , SGD with step size η ∗ minimizes the conditional expectation in SRFD , i . e . argmin d E [ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] = − η ∗ (cid:107)∇ (cid:96) ( w , Z ) (cid:107)∇ (cid:96) ( w , Z ) . ( 4 ) Remark 2 . 2 ( Anisotropy ) . In the general Theorem C . 5 , we also allow for anisotropic covariance functions of the form C ( x , y ) = C ( (cid:107) x − y (cid:107) 2 A ) , ( 5 ) where the norm (cid:107) · (cid:107) A is induced by (cid:104) x , y (cid:105) A = (cid:104) x , Ay (cid:105) . In this case , the descent direction is modiﬁed to be an expensive Newton - like step A − 1 ∇ (cid:96) ( w , ˜ Z ) . ( 6 ) A ﬁrst insight from this initial inspection of the isotropy assumption is , that it is closely linked to the usage of gradient descent . As soon as the assumption turns out to be wrong , the gradient direction is no longer optimal . And while it might have appeared that RFD was elementary compared to established Bayesian optimization methods , we now see that it is still at least as sophisticated as gradient descent – in fact , it is gradient descent under the isotropy assumption . Remark 2 . 3 . Since η ∗ ( ξ ) is generally increasing in ξ to facilitate a step size decrease when approach - ing minima , larger than average (cid:96) ( w , Z ) result in larger steps than below average sample losses . To avoid this bias it might be beneﬁcial to use a moving average for ξ . Research into multi step procedures that condition on more than the previous evaluation will likely provide more insight in the future . 2 . 2 The optimal step size To calculate the optimal step size , the covariance function is required . There are only a handful of covariance functions that typically appear in practical applications [ 33 , ch . 4 ] and for the most important , continuously differentiable ones , the optimal step sizes can be calculated explicitly . Details are in Appendix D , an overview is presented in Table 1 . Note that the squared exponential covariance model C ( x , y ) = σ 2 exp (cid:0) − (cid:107) x − y (cid:107) 2 2 s (cid:1) ( 7 ) is both the limit of the Matérn model C ( x , y ) = σ 2 2 1 − ν Γ ( ν ) (cid:16) √ 2 ν (cid:107) x − y (cid:107) s (cid:17) ν K ν (cid:0) √ 2 ν (cid:107) x − y (cid:107) s (cid:1) s > 0 , ν ≥ 0 ( 8 ) 5 Table 1 : Optimal step size Model Optimal step size η ∗ General case (cid:96) ( w , Z ) = µ Matérn ν 3 / 2 s √ 3 1 (cid:16) 1 − √ 3 s ξ (cid:17) ≈ 0 . 58 s 5 / 2 s 2 √ 5 (cid:18) 1 + (cid:113) 1 + 4 1 − √ 53 s ξ (cid:19) ≈ 0 . 72 s Squared - exponential ∞ ξ / 2 + (cid:113) ( ξ / 2 ) 2 + s 2 s Rational quadratic β s √ β Root η (cid:16) 1 + √ βξs η − ( 1 + β ) η 2 + √ βξs η 3 (cid:17) s (cid:113) β 1 + β for ν → ∞ ( the random functions induced by the Matérn model are a . s . (cid:98) ν (cid:99) - times differentiable ) , and the limit of the rational quadratic C ( x , y ) = σ 2 (cid:0) 1 + (cid:107) x − y (cid:107) 2 βs 2 (cid:1) − β / 2 β > 0 for β → ∞ . This fact is reﬂected in the optimal step sizes of Table 1 . 2 . 3 Advantages over other derivations of gradient descent The clear advantage of RFD are the explicit step sizes for many settings , a challenge that typically needs to be addressed with hyperparameter tuning methods . Additionally , in contrast to most schemes , RFD is a scale invariant optimization scheme , an unusual but enormously practical property . This means scaling input or outputs by a constant ( such as changing units such as the currency ) does not impact the optimal step - sizes . While conventional schemes ( such as GD , momentum and heuristics built on them ) , which rely on Lipschitz constants , are susceptible to scaling ( cf . Figure 2 ) . Proposition 2 . 4 ( Scale invariance ) . SRFD is scale invariant with regard to 1 . afﬁne linear scaling of the loss (cid:96) , 2 . bijections on parameter inputs w . Proof . The claim about scaling of the loss follows immediately from the deﬁnition of RFD and linearity of the conditional expectation . For the scaling of inputs , let f be a bijection and ( x n ) n ∈ N the sequence of weights generated by RFD on L ◦ f , and ( y n ) n ∈ N generated by RFD on L . Then by induction we have f ( x n ) = y n with induction step argmin x ∈ M E [ L ( f ( x ) ) | L ( f ( x n ) ) , ∇L ( f ( x n ) ) ] (cid:124) (cid:123)(cid:122) (cid:125) = x n + 1 = f − 1 (cid:16) argmin y ∈ f ( M ) E [ L ( y ) | L ( y n ) , ∇L ( y n ) ] (cid:124) (cid:123)(cid:122) (cid:125) = y n + 1 (cid:17) . The same proof works for SRFD with sample loss (cid:96) only with more notation . 3 Momentum While the ﬁrst step of RFD is optimal as it minimizes the conditional expectation , it is not necessarily optimal for multiple steps . In ill - conditioned problems , where the loss function forms a narrow ravine , gradient methods are known to converge slowly . As can be seen in Figure 2 RFD exhibits the symptoms of an ill - conditioned problem . Grafting RFD step sizes onto deterministic momentum , we achieve competitive results without the need for hyperparameter tuning ( Figure 2 ) . While classic ( heavy - ball ) momentum [ 26 ] ( HBM ) attains optimal convergence speed on quadratic functions , 3 it can fail to converge on more general functions [ 20 , Sec 4 . 6 ] . So we choose Nesterov’s 3 In the set of algorithms which select w n + 1 from the linear hull of gradients ∇L ( w 0 ) , . . . ∇L ( w n ) [ 14 , 24 , Sec 2 . 1 . 4 ] . 6 Figure 2 : 30 steps of RFD on a 1000 - dimensional Gaussian random function with squared exponential covariance ( 7 ) . Left : Projecting these gradients onto each other measures the similarity of their direction . This “similarity score” is plotted as a heat - map ( s = 0 . 05 ) . The checkerboard pattern suggests gradients zig - zag on this random function ( a characteristic of narrow ravines – high condition numbers ) . While this is only a single realization , the pattern is always visible no matter the random seed . The remaining ﬁgures plot ( n , L ( w n ) ) for w n generated by different algorithms . Length scales of ( 7 ) left to right : s = 1 , s = 0 . 1 and s = 0 . 05 . As Adam [ 17 ] and NAdam [ 9 ] are not scale invariant , this is equivalent to tuning their step size . RFD and RFM * remain invariant . momentum , which breaks apart the two updates to the parameters w n of HBM . It ﬁrst applies momentum to obtain an intermediate point y n + 1 , then evaluates the gradient at this intermediate point y n + 1 instead of w n y n + 1 = w n + β n ( w n − w n − 1 ) and w n + 1 = y n + 1 − η n ∇L ( y n + 1 ) . This procedure is guaranteed to converge on convex functions with L - Lipschitz gradient with optimal rate O ( 1 / n ) for certain selections of β n , η n [ 24 ] . Since optimization on a random function will eventually enter a convex area , this is a plausible assumption for the asymptotic behavior . We take inspiration from the conservative inﬁnite condition case without strong convexity ( µ = 0 ) . The recursion for β n from the Constant Step Scheme II [ 24 , p . 93 ] is then well approximated by η n = 1 L and β n = n − 1 n + 2 . Since β n is explicit here , only a step size η n is required to deﬁne random function momentum . Inspired by the optimal step size of gradient descent in the strongly convex case [ 14 ] η n = 2 L + µ µ = 0 = 2 L , and Example 1 . 1 we halved the step size suggested by RFD , which results in the performance improvement we call RFM * ( Figure 2 ) . The ∗ emphasizes the educated guess involved . 4 Quadratic expectation In this section we want to discuss the constant expectation assumption . In principle , any expectation function m L is possible . Separating L into mean and centered random function ¯ L = L − m L , allows us to rewrite the minimization problem of RFD into min y E [ L ( y ) | L ( x ) , ∇L ( x ) ] = min y m L ( y ) + E [ ¯ L ( y ) | ¯ L ( x ) , ∇ ¯ L ( x ) ] . In this paper we essentially discuss minimization of the second summand . The mean can act as a regularizer . In a linear model for example , the expectation turns out to be quadratic ( Section 4 . 1 ) . This represents L 2 - regularization , which adds quadratic regularization to L , i . e . f ( w ) = L ( w ) + ρ (cid:107) w (cid:107) 2 and thus ∇ f ( w ) = ∇L ( w ) + 2 ρw . While a heuristic motivation of the L 2 - regularization result in the hyperparameter ρ without interpre - tation , a quadratic expectation function is very interpretable . If the expectation of L is estimated in the model selection process , Theorem 4 . 1 explains how RFD then selects the correct L 2 - regularization parameter ρ automatically and therefore provides a theoretical foundation for this heuristic . In this theorem g ( η ) takes the place of ∇ f . 7 Theorem 4 . 1 ( Regularized RFD ) . For an isotropic random function L with expectation E [ L ( w ) ] = σ 2 (cid:107) w (cid:107) 2 + µ , the regularized gradient ( a convex combination of ∇L ( w ) and the expected gradient ) g ( η ) : = C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇L ( w ) + (cid:0) 1 − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:1) ∇ E [ L ( w ) ] (cid:124) (cid:123)(cid:122) (cid:125) = 2 σ 2 w is the result of RFD d ( ˆ η ) = − ˆ η (cid:107) g ( ˆ η ) (cid:107) g ( ˆ η ) = argmin d E [ L ( w + d ) | L ( w ) , ∇L ( w ) ] ( 9 ) with optimal step size ˆ η = argmin η σ 2 η 2 + C ( η 2 ) C ( 0 ) ( L ( w ) − m L ( w ) ) − η (cid:107) g ( η ) (cid:107) . Remark 4 . 2 . Note that both ∇L and ∇ m L are scaled in g ( η ) . So g ( η ) can only be equal to ∇ f up to a constant . But this constant is removed in ( 9 ) due to normalization . Remark 4 . 3 . An anisotropic mean ( in the sense of Remark 2 . 2 ) would similarly result in Tikhonov regularization . A very salient feature of the quadratic mean is its unboundedness towards inﬁnity . Neuronal networks with bounded activation functions and bounded data result in a bounded loss , constant expectation is therefore a more plausible assumption in this case . 4 . 1 A Linear model In this section we consider a linear model with model and loss f ( w , x ) = w T x and L ( w ) = E [ ( Y − f ( w , X ) ) 2 | θ ] , where labels and inputs are deﬁned as Y = σ √ m θ T 1 X + σ (cid:15) (cid:15) ∈ R and X = θ 2 ξ ∈ R d . We assume the random “real parameters” θ = ( θ 1 , θ 2 ) ∈ R d × R d × m and the other sources of randomness ξ ∈ R m and (cid:15) ∈ R are all uncorrelated and centered with uncorrelated entries of unit variance . Finally σ and σ (cid:15) are both positive real numbers . Proposition 4 . 4 ( Explicit loss in the linear model ) . For the linear model above , we get an explicit form for the loss L ( w ) = σ 2 m ( θ 1 − w ) T θ 2 θ T 2 ( θ 1 − w ) + σ 2 (cid:15) , and it is possible to calculate its expectation , which turns out to be quadratic E [ L ( w ) ] = σ 2 (cid:107) w (cid:107) 2 + σ 2 E [ (cid:107) θ 1 (cid:107) 2 ] + σ 2 (cid:15) (cid:124) (cid:123)(cid:122) (cid:125) = : µ . More details and some generalizations can be found in Appendix E . 5 Conservative RFD In Example 1 . 1 an upper bound was used as the approximation of Algorithm 1 . An improvement on this upper bound therefore guarantees an improvement of the loss . This guarantee was lost using the conditional expectation . It is therefore natural to ask for an equivalent upper bound . In a random setting this can only be the top of an ( 1 − (cid:15) ) conﬁdence interval . Since the conditional variance Var ( L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ) = C ( 0 ) − C ( (cid:107) d (cid:107) 2 ) 2 C ( 0 ) + E [ (cid:15) 20 ] − 2 C (cid:48) ( (cid:107) d (cid:107) 2 ) 2 − C (cid:48) ( 0 ) + σ 2 (cid:107) d (cid:107) 2 is rotation invariant ( i . e . depends only on (cid:107) d (cid:107) ) , we can denote the multiple of the conditional standard deviation which ensures an ( 1 − (cid:15) ) - conﬁdence bound by σ (cid:15) ( (cid:107) d (cid:107) ) ( cf . Figure 1 ) . An upper bound on L ( w + d ) with probability 1 − (cid:15) is therefore given by ˆ L ( w + d ) : = E [ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] + σ (cid:15) ( (cid:107) d (cid:107) ) . 8 Minimizing ˆ L then results in gradient descent with optimal step size η ∗ = argmin η C ( η 2 ) C ( 0 ) + E [ (cid:15) 20 ] ( (cid:96) ( w , Z ) − µ ) − η − C (cid:48) ( η 2 ) σ 2 − C (cid:48) ( 0 ) (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) + σ (cid:15) ( η ) . Taking multiple steps should have an averaging effect on the approximation error , so we expect faster convergence with a risk neutral minimization of the conditional expectation . For risk sensitive applications on the other hand , this would be an interesting research direction . More details in G . 6 Simulation of random functions as a synthetic benchmarks Random functions can act as an inﬁnite source of synthetic high - dimensional benchmarks for optimizers . Since the loss iterates have already very little variance over multiple optimization attempts in moderately high dimension ( d = 1000 ) , it is easy to compare optimizer performance . To allow for incremental evaluations of a differentiable Gaussian random function , we implemented a new library based on an incremental Cholesky decomposition of the covariance matrix in Julia [ 3 ] . The Pluto . jl notebook used to benchmark optimizers can be found in the same repository ( github . com / FelixBenning / IncrementalGRF . jl ) . The FluxML package [ 16 ] was used for the implementations of Adam [ 17 ] and NAdam [ 9 ] and their default hyperparameters . Since the simulation of random functions using the Cholesky decomposition has cost O ( d 3 n 3 ) , this is the only bottleneck for testing RFD in larger dimensions . RFD itself is as cheap as gradient descent . 7 Conclusion and future work In a not - so far future hyperparameters might be eliminated with the following steps : 1 . Estimate the covariance structure C from evaluations . This is also known as model se - lection [ e . g . 33 , ch . 5 ] . Since Bayesian optimization was unconcerned with high dimensional problems so far , there are a few challenges that need to be addressed ﬁrst . These start right at the beginning with the data collection . For example Muller [ 23 , ch . 4 ] suggests collecting data on a grid , but that is infeasible in high - dimension . Selecting points ( iid ) randomly on the other hand causes all points to be of similar distance due to the law of large numbers . Without a variety of distances however , covariance estimators are not stable . We plan to address these challenges in an upcoming work and ﬁnally apply RFD to real datasets . An interesting future research direction could also be to extend existing results about covariance functions of neural networks [ 32 , 6 , 19 ] to obtain the covariance structure of the induced losses directly . 2 . Use optimizers which take steps informed by C eliminating the need for hyperparam - eters . RDF is a ﬁrst step towards this goal , but a closer look at multi - step procedures will likely improve performance judging by the beneﬁt of RFM * over RFD . 3 . Use distributional knowledge about minima on random functions to decide when to stop optimization . It might be possible to calculate the distribution of minima of random functions explicitly [ 4 , 12 , 8 ] . Once that is accomplished it would be possible to estimate the potential gains of further optimization . Understanding adaptive methods : Adaptive optimization methods [ e . g . 10 , 15 , 36 , 9 ] , in partic - ular Adam [ 17 ] introduce step sizes for each coordinate . They heuristically reduce step sizes in in directions with high variance . While these methods seem to be very successful in practice , there is little theoretical foundation for their success . As they are still covered by the lower bounds mentioned before [ 14 ] , they can not converge faster than Nesterov momentum in the set of L - smooth , convex functions . At the same time counterexamples have been constructed , where adaptive methods do not converge at all [ 34 , 27 ] . Empirical benchmarks remain inconclusive [ 29 ] . Since it is provably impossible to improve the rates of Nesterov momentum in the convex case [ 14 , 24 ] , we hope that further research into optimization on random functions will ﬁnally provide a theoretical foundation for the success of adaptive methods . 9 References [ 1 ] Robert J . Adler and Jonathan E . Taylor . Random Fields and Geometry . Springer Monographs in Mathematics . New York , NY : Springer New York , 2007 . ISBN : 978 - 0 - 387 - 48112 - 8 . DOI : 10 . 1007 / 978 - 0 - 387 - 48116 - 6 . [ 2 ] Apoorv Agnihotri and Nipun Batra . “Exploring Bayesian Optimization” . In : Distill 5 . 5 ( 2020 - 05 - 05 ) , e26 . ISSN : 2476 - 0757 . DOI : 10 . 23915 / distill . 00026 . ( Visited on 2023 - 04 - 10 ) . [ 3 ] Jeff Bezanson et al . “Julia : A Fresh Approach to Numerical Computing” . In : SIAM Review 59 . 1 ( 2017 - 01 - 01 ) , pp . 65 – 98 . ISSN : 0036 - 1445 . DOI : 10 . 1137 / 141000671 . [ 4 ] Alan J . Bray and David S . Dean . “The Statistics of Critical Points of Gaussian Fields on Large - Dimensional Spaces” . In : Physical Review Letters 98 . 15 ( 2007 - 04 - 10 ) . ISSN : 0031 - 9007 , 1079 - 7114 . DOI : 10 . 1103 / PhysRevLett . 98 . 150201 . arXiv : cond - mat / 0611023 . [ 5 ] Steve Cheng . Differentiation under the Integral Sign . 2013 - 03 - 22 . URL : https : / / planetmath . org / differentiationundertheintegralsign ( visited on 2022 - 12 - 20 ) . [ 6 ] Youngmin Cho and Lawrence Saul . “Kernel Methods for Deep Learning” . In : Ad - vances in Neural Information Processing Systems . Vol . 22 . Curran Associates , Inc . , 2009 . URL : https : / / proceedings . neurips . cc / paper / 2009 / hash / 5751ec3e9a4feab575962e78e006250d - Abstract . html ( visited on 2023 - 04 - 03 ) . [ 7 ] Yann N Dauphin et al . “Identifying and Attacking the Saddle Point Problem in High - Dimensional Non - Convex Optimization” . In : Advances in Neural Information Processing Systems . Vol . 27 . Curran Associates , Inc . , 2014 . URL : https : / / proceedings . neurips . cc / paper / 2014 / hash / 17e23e50bedc63b4095e3d8204ce063b - Abstract . html ( visited on 2022 - 06 - 10 ) . [ 8 ] David S . Dean and Satya N . Majumdar . “Extreme Value Statistics of Eigenvalues of Gaussian Random Matrices” . In : Physical Review E 77 . 4 ( 2008 - 04 - 10 ) , p . 041108 . DOI : 10 . 1103 / PhysRevE . 77 . 041108 . ( Visited on 2023 - 01 - 13 ) . [ 9 ] Timothy Dozat . “Incorporating Nesterov Momentum into Adam” . In : Proceedings of the 4th International Conference on Learning Representations . ICLR . San Juan , 2016 - 02 - 18 . URL : https : / / openreview . net / forum ? id = OM0jvwB8jIp57ZJjtNEZ ( visited on 2021 - 11 - 16 ) . [ 10 ] John Duchi , Elad Hazan , and Yoram Singer . “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization” . In : The Journal of Machine Learning Research 12 ( 2011 - 07 - 01 ) , pp . 2121 – 2159 . ISSN : 1532 - 4435 . [ 11 ] Peter I . Frazier . “Bayesian Optimization” . In : Recent Advances in Optimization and Modeling of Contemporary Problems . INFORMS TutORials in Operations Research . INFORMS , 2018 - 10 , pp . 255 – 278 . ISBN : 978 - 0 - 9906153 - 2 - 3 . DOI : 10 . 1287 / educ . 2018 . 0188 . arXiv : 1807 . 02811 [ cs , math , stat ] . [ 12 ] Yan V . Fyodorov . “High - Dimensional Random Fields and Random Matrix Theory” . 2013 - 11 - 18 . arXiv : 1307 . 2379 [ cond - mat , physics : math - ph ] . URL : http : / / arxiv . org / abs / 1307 . 2379 ( visited on 2022 - 04 - 12 ) . [ 13 ] Guillaume Garrigos and Robert M . Gower . Handbook of Convergence Theorems for ( Stochas - tic ) Gradient Methods . 2023 - 02 - 17 . DOI : 10 . 48550 / arXiv . 2301 . 11235 . arXiv : 2301 . 11235 [ math ] . ( Visited on 2023 - 03 - 27 ) . preprint . [ 14 ] Gabriel Goh . “Why Momentum Really Works” . In : Distill ( 2017 - 04 - 04 ) . DOI : 10 . 23915 / distill . 00006 . ( Visited on 2021 - 06 - 19 ) . [ 15 ] Geoffrey Hinton . “Neural Networks for Machine Learning” . Massive Open Online Course ( Coursera ) . 2012 . URL : https : / / www . cs . toronto . edu / ~ hinton / coursera _ lectures . html ( visited on 2021 - 11 - 16 ) . [ 16 ] Michael Innes et al . “Fashionable Modelling with Flux” . In : CoRR abs / 1811 . 01457 ( 2018 ) . arXiv : 1811 . 01457 . URL : https : / / arxiv . org / abs / 1811 . 01457 . [ 17 ] Diederik P . Kingma and Jimmy Ba . “Adam : A Method for Stochastic Optimization” . In : Proceedings of the 3rd International Conference on Learning Representations . ICLR . San Diego , 2015 . arXiv : 1412 . 6980 . URL : http : / / arxiv . org / abs / 1412 . 6980 . [ 18 ] H . J . Kushner . “A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise” . In : Journal of Basic Engineering 86 . 1 ( 1964 - 03 - 01 ) , pp . 97 – 106 . ISSN : 0021 - 9223 . DOI : 10 . 1115 / 1 . 3653121 . ( Visited on 2023 - 03 - 31 ) . 10 [ 19 ] Jaehoon Lee et al . “Deep Neural Networks as Gaussian Processes” . In : International Confer - ence on Learning Representations . Vancouver , Canada , 2018 . arXiv : 1711 . 00165 [ stat . ML ] . URL : https : / / openreview . net / forum ? id = B1EA - M - 0Z ( visited on 2023 - 04 - 03 ) . [ 20 ] Laurent Lessard , Benjamin Recht , and Andrew Packard . “Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints” . In : SIAM Journal on Optimization 26 . 1 ( 2016 - 01 - 01 ) , pp . 57 – 95 . ISSN : 1052 - 6234 . DOI : 10 . 1137 / 15M1009597 . ( Visited on 2021 - 08 - 11 ) . [ 21 ] Daniel James Lizotte . “Practical Bayesian Optimization” . PhD thesis . Alberta , Canada : Uni - versity of Alberta , 2008 . 168 pp . [ 22 ] G . Matheron . “The Intrinsic Random Functions and Their Applications” . In : Advances in Applied Probability 5 . 3 ( 1973 - 12 ) , pp . 439 – 468 . ISSN : 0001 - 8678 , 1475 - 6064 . DOI : 10 . 2307 / 1425829 . ( Visited on 2023 - 04 - 18 ) . [ 23 ] Werner Muller . Collecting Spatial Data : Optimum Design of Experiments for Random Fields . 3 . Auﬂ . , Third Revised and Extended Edition . Berlin , Heidelberg : Springer - Verlag , 2007 . ISBN : 978 - 3 - 540 - 31174 - 4 . DOI : 10 . 1007 / 978 - 3 - 540 - 31175 - 1 . [ 24 ] Yurii Evgen’eviˇc Nesterov . Lectures on Convex Optimization . Second edition . Springer Opti - mization and Its Applications ; Volume 137 . Cham : Springer , 2018 . ISBN : 978 - 3 - 319 - 91578 - 4 . DOI : 10 . 1007 / 978 - 3 - 319 - 91578 - 4 . [ 25 ] Misha Padidar et al . “Scaling Gaussian Processes with Derivative Information Using Variational Inference” . In : Advances in Neural Information Processing Systems . Vol . 34 . Curran Associates , Inc . , 2021 , pp . 6442 – 6453 . URL : https : / / proceedings . neurips . cc / paper / 2021 / hash / 32bbf7b2bc4ed14eb1e9c2580056a989 - Abstract . html ( visited on 2023 - 03 - 31 ) . [ 26 ] Boris T . Polyak . “Some Methods of Speeding up the Convergence of Iteration Methods” . In : USSR Computational Mathematics and Mathematical Physics 4 . 5 ( 1964 - 01 - 01 ) , pp . 1 – 17 . ISSN : 0041 - 5553 . DOI : 10 . 1016 / 0041 - 5553 ( 64 ) 90137 - 5 . ( Visited on 2021 - 05 - 27 ) . [ 27 ] Sashank J . Reddi , Satyen Kale , and Sanjiv Kumar . “On the Convergence of Adam and Beyond” . In : 6th International Conference on Learning Representations . Vancouver , Canada , 2018 . arXiv : 1904 . 09237 . [ 28 ] Martin Schlather and Olga Moreva . “A Parametric Model Bridging between Bounded and Unbounded Variograms” . In : Stat 6 . 1 ( 2017 ) , pp . 47 – 52 . ISSN : 2049 - 1573 . DOI : 10 . 1002 / sta4 . 134 . [ 29 ] Robin M . Schmidt , Frank Schneider , and Philipp Hennig . “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers” . In : Proceedings of the 38th International Conference on Machine Learning . International Conference on Machine Learning . PMLR , 2021 - 07 - 01 , pp . 9367 – 9376 . arXiv : 2007 . 01547 . URL : https : / / proceedings . mlr . press / v139 / schmidt21a . html ( visited on 2023 - 03 - 23 ) . [ 30 ] Bobak Shahriari et al . “Taking the Human Out of the Loop : A Review of Bayesian Optimiza - tion” . In : Proceedings of the IEEE 104 . 1 ( 2016 - 01 ) , pp . 148 – 175 . ISSN : 1558 - 2256 . DOI : 10 . 1109 / JPROC . 2015 . 2494218 . [ 31 ] Tuyen Trung Truong and Tuan Hang Nguyen . “Backtracking Gradient Descent Method for General C 1 Functions , with Applications to Deep Learning” . 2019 - 04 - 04 . arXiv : 1808 . 05160 [ cs , math , stat ] . URL : http : / / arxiv . org / abs / 1808 . 05160 ( visited on 2021 - 09 - 27 ) . [ 32 ] Christopher Williams . “Computing with Inﬁnite Networks” . In : Advances in Neural Informa - tion Processing Systems . Vol . 9 . MIT Press , 1996 . URL : https : / / proceedings . neurips . cc / paper / 1996 / hash / ae5e3ce40e0404a45ecacaaf05e5f735 - Abstract . html ( vis - ited on 2022 - 12 - 01 ) . [ 33 ] Christopher K . I . Williams and Carl Edward Rasmussen . Gaussian Processes for Machine Learning . 2nd ed . Adaptive Computation and Machine Learning 3 . MIT press Cambridge , MA , 2006 . 248 pp . ISBN : 0 - 262 - 18253 - X . URL : http : / / gaussianprocess . org / gpml / chapters / RW . pdf ( visited on 2023 - 04 - 06 ) . [ 34 ] Ashia C Wilson et al . “The Marginal Value of Adaptive Gradient Methods in Machine Learn - ing” . In : Advances in Neural Information Processing Systems . Vol . 30 . Curran Associates , Inc . , 2017 . arXiv : 1705 . 08292 . URL : https : / / proceedings . neurips . cc / paper / 2017 / hash / 81b3833e2504647f9d794f7d7b9bf341 - Abstract . html ( visited on 2023 - 03 - 23 ) . [ 35 ] Jian Wu et al . “Bayesian Optimization with Gradients” . In : Advances in Neural Information Processing Systems . Vol . 30 . Curran Associates , Inc . , 2017 . URL : https : / / proceedings . neurips . cc / paper / 2017 / hash / 64a08e5f1e6c39faeb90108c430eb120 - Abstract . html ( visited on 2022 - 06 - 02 ) . 11 [ 36 ] Matthew D . Zeiler . “ADADELTA : An Adaptive Learning Rate Method” . 2012 - 12 - 22 . arXiv : 1212 . 5701 [ cs ] . A Random functions Deﬁnition A . 1 ( Random function ) . A family of random variables ( Z ( x ) ) x ∈ R d is a random function over R d . Deﬁnition A . 2 ( Strict stationarity ) . A random function Z is called ( strictly ) stationary if it is invariant to shifts in distribution , i . e . ( Z ( x 1 ) , . . . , Z ( x k ) ) d = ( Z ( x 1 + d ) , . . . , Z ( x k + d ) ) ∀ d ∈ R d . Deﬁnition A . 3 ( Covariance function ) . The covariance function of a random function Z is deﬁned as C ( x , y ) : = Cov ( Z ( x ) , Z ( y ) ) . ( 10 ) Deﬁnition A . 4 ( Weak stationarity ) . A random function Z is called ( weakly ) stationary , if for all x , y ∈ R d C ( x , y ) = C ( x − y ) and E [ Z ( x ) ] = µ for a ﬁxed µ ∈ R d . Remark A . 5 . Strictly stationary implies weakly stationary . The deﬁnitions coincide for Gaussian random variables . Throughout this paper we use “stationary” for “weakly stationary” . Deﬁnition A . 6 ( Isotropy ) . A stationary random function Z is called isotropic if the covariance is also rotation invariant , i . e . C ( x , y ) = C ( x − y ) = C ( (cid:107) x − y (cid:107) 2 ) . Deﬁnition A . 7 ( Intrinsic stationarity ) . A random function Z , for which its increments Z d ( x ) : = Z ( x + d ) − Z ( x ) are stationary for every d , is called intrinsically stationary if its mean is continuous . For any intrinsically stationary random function , the ( centred ) variogram is deﬁned as γ ( d ) = 12 Var ( Z d ( x ) ) , which is independent of x due to stationarity of Z d . For the uncentered variogram use uncentered second moments instead of the variance . Lemma A . 8 ( Intrinsically stationary mean ) . An intrinsically stationary random function Z has mean E [ Z ( x ) ] = E [ Z ( 0 ) ] (cid:124) (cid:123)(cid:122) (cid:125) µ 0 + (cid:104) µ ∇ , x (cid:105) for some µ ∇ ∈ R d . If Z is differentiable , we therefore have E [ ∇ Z ( x ) ] = ∇ E [ Z ( x ) ] = µ ∇ . Proof . As Z d is stationary , µ ( d ) : = E [ Z d ( x ) ] = E [ Z ( x + d ) − Z ( x ) ] is well deﬁned . We therefore have E [ Z ( d ) ] = E [ Z ( 0 ) ] + µ ( d ) , so if we can prove µ is linear we are done . Additivity is easy µ ( d 1 + d 2 ) = E [ Z ( x + d 1 + d 2 ) − Z ( x + d 1 ) ] (cid:124) (cid:123)(cid:122) (cid:125) = µ ( d 2 ) + E [ Z ( x + d 1 ) − Z ( x ) ] (cid:124) (cid:123)(cid:122) (cid:125) µ ( d 1 ) . For scalability we start with natural numbers µ ( m d ) = m (cid:88) k = 1 E [ Z ( x + k d ) − Z ( x + ( k − 1 ) d ) ] (cid:124) (cid:123)(cid:122) (cid:125) µ ( d ) = mµ ( d ) . As this implies µ ( d ) = µ ( m 1 m d ) = mµ ( 1 m d ) , we can generalize this to rational factors . To ﬁnish the proof we apply the continuity of the mean we required in the deﬁnition of intrinsic stationarity . 12 Lemma A . 9 ( Covariance from Variogram ) . If Z is intrinsically stationary we have for Y s : = Z − Z ( s ) and C s ( x , y ) : = Cov ( Y s ( x ) , Y s ( y ) ) = Cov ( Z x − s ( s ) , Z y − s ( s ) ) an explicit representation using the variogram C s ( x , y ) = γ ( x − s ) + γ ( y − s ) − γ ( x − y ) symm . = γ ( x − s ) + γ ( y − s ) − γ ( y − x ) Remark A . 10 ( Antisymmetry ) . We therefore have γ ( x − y ) = γ ( y − x ) or in other words the variogram is symmetric . So if it is differentiable , then ∇ γ is antisymmetric with ∇ γ ( 0 ) = 0 . Proof ( suggested to us by Martin Schlather in verbal communication ) . Assume Z is centred without loss of generality , otherwise center Z resulting in ˜ Z and notice that ˜ Y s is the centered version of Y s C s ( x , y ) = E [ ( Z ( x ) − Z ( s ) ) ( Z ( y ) − Z ( s ) ) ] = E [ ( Z ( x ) − Z ( y ) + Z ( y ) − Z ( s ) ) ( Z ( y ) − Z ( x ) + Z ( x ) − Z ( s ) ) ] = − E [ ( Z ( x ) − Z ( y ) ) 2 ] + E [ ( Z ( x ) − Z ( y ) ) ( Z ( x ) − Z ( s ) ) ] (cid:124) (cid:123)(cid:122) (cid:125) = : I + C s ( y , x ) + E [ ( Z ( y ) − Z ( s ) ) ( Z ( y ) − Z ( x ) ) ] (cid:124) (cid:123)(cid:122) (cid:125) = : II And we have I = E [ ( Z ( x ) − Z ( s ) + Z ( s ) − Z ( y ) ) ( Z ( x ) − Z ( s ) ) ] = 2 γ ( x − s ) − C s ( y , x ) and similarly II = 2 γ ( y − s ) − C s ( y , x ) . Together this results in C s ( x , y ) = − 2 γ ( x − y ) + 2 γ ( x − s ) − C s ( y , x ) + 2 γ ( y − s ) − (cid:24)(cid:24)(cid:24)(cid:24) C s ( y , x ) + (cid:24)(cid:24)(cid:24)(cid:24) C s ( y , x ) . Reordering and symmetry of the covariance results in the claim . A . 1 Derivatives of Random functions Lemma A . 11 ( Covariance and derivative ) . Let L be a random function . Then we have Cov ( ∂ i L ( x ) , L ( y ) ) = ∂ x i C ( x , y ) Cov ( ∂ i L ( x ) , ∂ j L ( y ) ) = ∂ x i y j C ( x , y ) Proof . We assume E [ L ( x ) ] = 0 without loss of generality . Then covariance is just a product of expectations . So we get Cov ( ∂ i L ( x ) , L ( y ) ) = E [ ∂ x i L ( x ) L ( y ) ] = ∂ x i E [ L ( x ) L ( y ) ] def . = ∂ x i C ( x , y ) because swapping integration and differentiation is essentially always possible [ 5 ] . For a more careful analysis consult Adler and Taylor [ 1 ] . Remark A . 12 . For stationary random functions C ( x , y ) = C ( x − y ) , the covariance function C is symmetric , i . e . C ( h ) = C ( − h ) and therefore the derivative antisymmetric , i . e . ∂ i C ( − h ) = − ∂ i C ( h ) . In particular Cov ( ∇L ( x ) , L ( x ) ) = ∇C ( 0 ) = 0 . Corollary A . 13 ( Gaussian Case ) . If L is a stationary gaussian random function , L ( x ) and ∇L ( x ) are independent multivariate gaussian for every x . Lemma A . 14 ( Variogram and Derivative ) . Assuming Z is an intrinsically stationary , differentiable random function , we have 1 . Cov ( Z ( x + d ) − Z ( x ) , ∇ Z ( x ) ) = ∇ γ ( d ) 2 . Cov ( ∇ Z ( x ) , ∇ Z ( y ) ) = ∇ 2 γ ( x − y ) ( this implies ∇ Z is stationary ) 3 . Cov ( Z ( x + d ) − Z ( x ) , ∇ 2 Z ( x ) ) = ∇ 2 γ ( 0 ) − ∇ 2 γ ( d ) 13 Proof . 1 . We have ∂ i Z ( x ) = lim h → 0 Z ( x + he i ) − Z ( s ) − ( Z ( x ) − Z ( s ) ) h = ∂ i Y s ( x ) ∀ x , s and therefore Cov ( ∂ i Z ( x ) , Z ( x + d ) − Z ( x ) ) Lem A . 11 = ∂ x i C x ( x , x + d ) = ∂ i γ ( 0 ) (cid:124) (cid:123)(cid:122) (cid:125) = 0 ( Remark A . 10 ) − ∂ i γ ( − d ) , because the variogram is symmetric and the derivative therefore antisymmetric . 2 . Here we use ∂ i Z ( x ) = ∂ i Y s ( x ) again Cov ( ∂ i Z ( x ) , ∂ j Z ( y ) ) = Cov ( ∂ i Y s ( x ) , ∂ j Y s ( y ) ) Lem A . 11 = ∂ y j x i C s ( x , y ) Lem A . 9 = ∂ y j x i [ γ ( x − s ) + γ ( y − s ) − γ ( x − y ) ] = ∂ y j [ ∂ i γ ( x − s ) − ∂ i γ ( x − y ) ] = ∂ ij γ ( x − y ) . 3 . Using the previous results about gradients , we get 1 h Cov ( ∇ Z ( x + he i ) − ∇ Z ( x ) , Z d ( x ) ) = 1 h (cid:20) Cov ( ∇ Z ( x + he i ) , Z ( x + d ) − Z ( x + he i ) ) + Cov ( ∇ Z ( x + he i ) , Z ( x + he i ) − Z ( x ) ) − Cov ( ∇ Z ( x ) , Z x ( d ) ) (cid:21) = 1 h [ ∇ γ ( d − he i ) − ∇ γ ( − he i ) − ∇ ( d ) ] = 1 h [ ( ∇ γ ( he i ) − ∇ γ ( 0 ) ) − ( ∇ γ ( d ) − ∇ γ ( d − he i ) ) ] → ∂ i ∇ γ ( 0 ) − ∂ i ∇ γ ( d ) where we have used antisymmetry of ∇ γ for the last equation . Lemma A . 15 ( Derivatives of Rotation Invariant Variograms ) . For a rotation invariant variogram γ ( d ) = φ ( (cid:107) d (cid:107) 2 ) , we have 1 . ∇ γ ( d ) = 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) d 2 . ∇ 2 γ ( d ) = 4 φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) dd T + 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) I 3 . ∂ kij γ ( d ) = 8 φ (cid:48)(cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d k d i d j + 4 φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) [ δ ki d j + δ kj d i + δ ij d k ] 4 . ∂ lkij γ ( 0 ) = 4 φ (cid:48)(cid:48) ( 0 ) [ δ ki δ jl + δ kj δ il + δ ij δ kl ] Proof . Product and chain rule . B Best linear unbiased estimator ( BLUE ) Let us recapitulate what a BLUE is . A linear estimator ˆ Y of Y using X 1 , . . . , X n is of the form ˆ Y ∈ span { X 1 , . . . , X n } + R . The set of unbiased linear estimators is deﬁned as LUE = LUE [ Y | X 1 , . . . , X n ] = { ˆ Y ∈ span { X 1 , . . . , X n } + R : E [ ˆ Y ] = E [ Y ] } ( 11 ) = { ˆ Y + E [ Y ] : ˆ Y ∈ span { X 1 − E [ X 1 ] , . . . , X n − E [ X n ] } } . And the BLUE is the best linear unbiased estimator , i . e . BLUE [ Y | X 1 , . . . , X n ] : = argmin ˆ Y ∈ LUE E [ (cid:107) ˆ Y − Y (cid:107) 2 ] . ( 12 ) Other risk functions to minimize are possible , but this is the usual one . 14 Lemma B . 1 . If X , Y 1 , . . . , Y n are multivariate normal distributed , then we have BLUE [ Y | X 1 , . . . , X n ] = E [ Y | X 1 , . . . , X n ] (cid:18) = argmin ˆ Y ∈ { f ( X 1 , . . . , X n ) : f meas . } E [ (cid:107) Y − ˆ Y (cid:107) 2 ] (cid:19) . Proof . We simply observe that the well known conditional expectation of Gaussian random variables is linear . So as a linear function its L 2 risk must be larger or equal to that of the BLUE . And as an L 2 projection the conditional expectation was already optimal . Remark B . 2 . As Bayesian Optimization typically assumes Gaussian random functions , working with the BLUE is really equivalent to working with the conditional expectation in that setting . C RFD details We will present the most general version of the results here . This means : 1 . We avoid the Gaussian assumption using the BLUE we deﬁned in Section B with Lemma B . 1 in mind for the Gaussian case . 2 . We make the generalization over isotropy , using (cid:104) x , y (cid:105) A : = (cid:104) x , Ay (cid:105) for some symmetric , positive deﬁnite A which also induces (cid:107) x (cid:107) A . Isotropy is then replaced by C ( x , y ) = C ( (cid:107) x (cid:107) 2 A ) . 3 . We consider sample losses (cid:96) ( w , Z ) = L ( w ) + (cid:15) 0 ∇ (cid:96) ( w , ˜ Z ) = ∇L ( w ) + (cid:15) 1 where the errors (cid:15) 0 , (cid:15) 1 have zero expectation , ﬁnite variance , are uncorrelated and also not correlated with L . (cid:15) 0 and (cid:15) 1 being uncorrelated likely requires ˜ Z is a different data sample than Z although in practice this will likely be ignored . C . 1 Stationarity Under stationarity we have by Remark A . 12 Cov ( L ( w ) , ∇L ( w ) ) = 0 . So we can easily “condition” on both L ( w ) and ∇L ( w ) . The same translates to the sample losses , since (cid:15) i are uncorrelated . Lemma C . 1 . Assume L is a stationary and differentiable random function with covariance function C and mean µ = E [ L ( w ) ] . Using the notation ¯ L : = L − µ and ¯ (cid:96) : = (cid:96) − µ , we have BLUE [ ¯ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] = C ( d ) C ( 0 ) + E [ (cid:15) 20 ] ¯ (cid:96) ( w , Z ) − ∇C ( d ) T ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ∇ (cid:96) ( w , ˜ Z ) if L is additionally isotropic , i . e . C ( d ) = C ( (cid:107) d (cid:107) 2 A ) and E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 A , this reduces to = C ( (cid:107) d (cid:107) 2 A ) C ( 0 ) + E [ (cid:15) 20 ] ¯ (cid:96) ( w , Z ) − C (cid:48) ( (cid:107) d (cid:107) 2 A ) σ 2 − C (cid:48) ( 0 ) (cid:104) d , ∇ (cid:96) ( w , ˜ Z ) (cid:105) . Proof . By working with ¯ L ( and ¯ (cid:96) ) instead of L ( and (cid:96) ) , we can assume without loss of generality that L ( and (cid:96) ) is centered . So we know that any element ˆ L ( w + d ) from LUE has the form ˆ L ( w + d ) = a(cid:96) ( w , Z ) + (cid:104) b , ∇ (cid:96) ( w , ˜ Z ) (cid:105) 15 for a ∈ R , b ∈ R d . So we want to minimize g ( a , b ) = E [ (cid:107)L ( w + d ) − ˆ L ( w + d ) (cid:107) 2 ] = E [ L ( w + d ) 2 ] − 2 a E [ L ( w + d ) (cid:96) ( w , Z ) ] (cid:124) (cid:123)(cid:122) (cid:125) = E [ L ( w + d ) L ( w ) ] + a 2 E [ (cid:96) ( w , Z ) 2 ] (cid:124) (cid:123)(cid:122) (cid:125) = E [ L ( w ) 2 ] + E [ (cid:15) 20 ] − 2 (cid:104) b , E [ L ( w + d ) ∇ (cid:96) ( w , ˜ Z ) ] (cid:124) (cid:123)(cid:122) (cid:125) = E [ L ( w + d ) ∇L ( w ) ] (cid:105) + (cid:104) b , E [ ∇ (cid:96) ( w , ˜ Z ) ∇ (cid:96) ( w , ˜ Z ) T ] (cid:124) (cid:123)(cid:122) (cid:125) = E [ ∇L ( w ) ∇L ( w ) T ] + E [ (cid:15) 1 (cid:15) T 1 ] b (cid:105) − 2 a (cid:104) b , E [ (cid:96) ( w , Z ) ∇ (cid:96) ( w , ˜ Z ) ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 ( Remark A . 12 ) (cid:105) = C ( 0 ) − 2 a C ( d ) + a 2 ( C ( 0 ) + E [ (cid:15) 20 ] ) − 2 (cid:104) b , ∇C ( − d ) (cid:124) (cid:123)(cid:122) (cid:125) A . 12 = −∇C ( d ) (cid:105) + (cid:104) b , ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) b (cid:105) with respect to a and b , where we have used Lemma A . 11 and stationarity for the last line . Fortunately there are no interactions between a and b , as L ( w ) is uncorrelated from ∇L ( w ) . First order conditions result in a = C ( d ) C ( 0 ) + E [ (cid:15) 20 ] and b = − ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ∇C ( d ) . These are minima , since −∇ 2 C ( 0 ) is positive deﬁnite , because of (cid:104) b , −∇ 2 C ( 0 ) b (cid:105) = E [ (cid:104) b , ∇L ( w ) (cid:105) 2 ] ≥ 0 . Now we just need to address the C ( d ) = C ( (cid:107) d (cid:107) 2 A ) case . We have ∇C ( d ) = 2 C (cid:48) ( (cid:107) d (cid:107) 2 A ) A d ∇ 2 C ( d ) = 4 C (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 A ) A dd T A + 2 C (cid:48) ( (cid:107) d (cid:107) 2 A ) A and therefore ∇ 2 C ( 0 ) = 2 C (cid:48) ( 0 ) A . Plugging these results into the general formula together with E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 A yields the claim BLUE [ ¯ L ( w + d ) | L ( w ) , ∇L ( w ) ] = C ( (cid:107) d (cid:107) 2 A ) C ( 0 ) + E [ (cid:15) 20 ] ¯ L ( w ) − C (cid:48) ( (cid:107) d (cid:107) 2 A ) σ 2 − C (cid:48)(cid:48) ( 0 ) ( A d ) T A − 1 (cid:124) (cid:123)(cid:122) (cid:125) = d T ∇L ( w ) . C . 2 Intrinsic stationarity Intrinsic stationarity forces us to consider loss differences and stops us from being able to condition on the loss itself . But we can still condition on the gradient alone , as the gradient is a loss difference . Lemma C . 2 . If L is a intrinsically stationary , differentiable random function , with variogram γ and mean E [ L ( w ) ] = µ 0 + (cid:104) µ ∇ , w (cid:105) , then BLUE [ L ( w + d ) − L ( w ) (cid:124) (cid:123)(cid:122) (cid:125) = : L w ( d ) | ∇ (cid:96) ( w , ˜ Z ) ] = (cid:104) µ ∇ , d (cid:105) + ∇ γ ( d ) T ( ∇ 2 γ ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ( ∇ (cid:96) ( w , ˜ Z ) − µ ∇ ) if the variogram is rotation invariant , i . e . γ ( d ) = φ ( (cid:107) d 2 (cid:107) A ) , this simpliﬁes to = (cid:104) µ ∇ , d (cid:105) + φ (cid:48) ( (cid:107) d (cid:107) 2 A ) φ (cid:48) ( 0 ) (cid:104) d , ∇ (cid:96) ( w , ˜ Z ) − µ ∇ (cid:105) Proof . Virtually identical to the stationary case , but using Lemma A . 14 and Lemma A . 15 . Remark C . 3 . If L is stationary we have γ ( d ) = C ( 0 ) − C ( d ) . Therefore ∇ γ = −∇C and ∇ 2 γ = −∇ 2 C which means we can replace the variogram with the covariance in this case . 16 In the stationary case we also have µ ∇ = 0 and it does not matter whether one tries to estimate the loss difference L w ( d ) or ¯ L ( w + d ) . That is because we have BLUE [ L w ( d ) | ∇L ( w ) ] = BLUE [ ¯ L ( w + d ) | ∇L ( w ) ] − BLUE [ ¯ L ( w ) | ∇L ( w ) ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 , as L ( w ) and ∇L ( w ) are uncorrelated ( or independent in the gaussian case ) . So we get BLUE [ ¯ L ( w + d ) | ∇L ( w ) ] = [ ∇ 2 C ( 0 ) ] − 1 (cid:104)∇C ( d ) , ∇L ( w ) (cid:105) = C (cid:48) ( (cid:107) d (cid:107) 2 ) C (cid:48) ( 0 ) (cid:104) d , ∇L ( w ) (cid:105) . Notice that in the case L ( w ) = µ or ¯ L ( w ) = 0 this result coincides with the stationary case . So if we minimize the sophisticated estimator utilizing L ( w ) BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] , we can get the minimization of the simpler estimator BLUE [ L ( w + d ) | ∇L ( w ) ] for free by plugging in ¯ L ( w ) = 0 . C . 3 Random function descent Lemma C . 4 . Functions of the form B ( d ) = g 0 ( (cid:107) d (cid:107) 2 A ) + g 1 ( (cid:107) d (cid:107) 2 A ) (cid:104) d , ∇ (cid:96) ( w , ˜ Z ) (cid:105) are minimized by a gradient step ( modiﬁed by A − 1 ) , i . e . − ˆ η A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A = argmin d B ( d ) with optimal step size ˆ η = argmin η B (cid:16) − η A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A (cid:17) = argmin η g 0 ( η 2 ) − g 1 ( η 2 ) η (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A Proof . Essentially all the occurrences of d in B are (cid:107) · (cid:107) A rotation invariant , so only the step size matters . The only place where that is not the case is the scalar product with the gradient . Therefore it determines the direction . We prove this by splitting the minimization problem min d B ( d ) = min η ≥ 0 min d : (cid:107) d (cid:107) A = η B ( d ) = min η ≥ 0 min d : (cid:107) d (cid:107) A = η g 0 ( η 2 ) + g 1 ( η 2 ) (cid:104) d , ∇ (cid:96) ( w , ˜ Z ) (cid:105) . First we consider the inner optimization problem , depending on the sign of g 1 ( η 2 ) , we either want to maximize or minimize opt d : (cid:107) d (cid:107) A = η (cid:104) d , ∇ (cid:96) ( w , ˜ Z ) (cid:105) = opt d : (cid:107) d (cid:107) A = η (cid:104) d , A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:105) A due to Cauchy - Schwarz we have − (cid:107) d (cid:107) A (cid:124) (cid:123)(cid:122) (cid:125) = η (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A ≤ (cid:104) d , A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:105) A ≤ (cid:107) d (cid:107) A (cid:124) (cid:123)(cid:122) (cid:125) = η (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A where the inequalities are equalities for d = ± η A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A . Since we minimized over positive η before , we can drop the ± by allowing η < 0 to obtain B ( d ) = min η g 0 ( η 2 ) − g 1 ( η 2 ) η (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A . Theorem C . 5 ( Random Function Descent ) . ( A - Distorted ) stochastic gradient descent , i . e . d ( η ) = − η (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A A − 1 ∇ (cid:96) ( w , ˜ Z ) = − ˜ ηA − 1 ∇ (cid:96) ( w , ˜ Z ) minimizes the 1 - step 1 - order BLUE of A - rotation invariant random function L in the following sense 17 1 . for an isotropic random function L with C ( d ) = C ( (cid:107) d (cid:107) 2 A ) and E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 A , we have d ( ˆ η ) = argmin d BLUE [ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] ˆ η = argmin η C ( η 2 ) C ( 0 ) (cid:96) ( w , Z ) − µ (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A (cid:124) (cid:123)(cid:122) (cid:125) = : ξ − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) 2 . for a intrinsically stationary , differentiable random function L with centered increments ( µ ∇ = 0 ) and rotation invariant variogram γ ( d ) = φ ( (cid:107) d (cid:107) 2 A ) and E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 A , we have d ( ˆ η ) = argmin d BLUE [ L ( w + d ) − L ( w ) | ∇ (cid:96) ( w , ˜ Z ) ] ˆ η = argmax η η φ (cid:48) ( η 2 ) φ (cid:48) ( 0 ) + σ 2 Proof . We simply apply Lemma C . 4 to the results from Lemma C . 2 and C . 1 and note that the minimizer does not change if we divide the minimization problem by (cid:107) A − 1 ∇ (cid:96) ( w , ˜ Z ) (cid:107) A . D Covariance Models D . 1 Squared Exponential In this section we are going to assume C ( d ) = C ( (cid:107) d (cid:107) 2 ) with C ( (cid:107) d (cid:107) 2 ) = σ 2 e − (cid:107) d (cid:107) 2 2 s 2 . From Theorem C . 5 we know we just have to minimize min η C ( η 2 ) C ( 0 ) ξ − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) η = min η e − η 2 2 s 2 ( ξ − η ) This results in the ﬁrst order condition 0 ! = − η s 2 e − η 2 2 s 2 ( ξ − η ) − e − η 2 2 s 2 = e − η 2 2 s 2 s 2 ( η 2 − ηξ − s 2 ) . Since the exponential can never be zero . But that is simply a quadratic equation . Therefore we obtain the following theorem : Theorem D . 1 . For a random function L with squared exponential covariance function C ( d ) = σ 2 e − (cid:107) d (cid:107) 2 2 s 2 and constant expectation µ , we have d ( ˆ η ) = argmin d BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] with ˆ η = ξ 2 + (cid:115)(cid:18) ξ 2 (cid:19) 2 + s 2 . Remark D . 2 . Optimization will soon result in L ( w ) ≤ E [ L ( w ) ] = µ . a : = − ξ / 2 s √ a 2 + s 2 18 In that case the learning rate is the hypotenuse of a triangle minus the side a ˆ η = (cid:112) a 2 + s 2 − a . With increasing a = − ξ / 2 ∝ − (cid:96) ( w , Z ) − µ 2 (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) , i . e . decreasing loss (cid:96) ( w , Z ) → −∞ and / or decreasing gradients (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) → 0 the step size converges to zero . For a = 0 ( i . e . (cid:96) ( w , Z ) = E [ (cid:96) ( w , Z ) ] = µ ) , we have ˆ η = s . D . 2 Matérn Deﬁnition D . 3 . The Matérn model C ν is given by C ν ( (cid:107) d (cid:107) ) = σ 2 2 1 − ν Γ ( ν ) (cid:32) √ 2 ν (cid:107) d (cid:107) s (cid:33) ν K ν (cid:32) √ 2 ν (cid:107) d (cid:107) s (cid:33) s > 0 , ν ≥ 0 where K ν is the modiﬁed Bessel function . For ν = p + 12 with p ∈ N 0 , it can be simpliﬁed [ cf . 33 , sec . 4 . 2 . 1 ] to C ν = p + 12 ( (cid:107) d (cid:107) ) = σ 2 exp (cid:16) − √ 2 ν (cid:107) d (cid:107) s (cid:17) p ! ( 2 p ) ! p (cid:88) k = 0 ( 2 p − k ) ! ( p − k ) ! k ! (cid:16) 2 √ 2 ν s (cid:107) d (cid:107) (cid:17) k Remark D . 4 ( [ based on 33 ] ) . The Matérn model encompasses • the nugget effect for ν = 0 ( independent randomness ) • the exponential model for ν = 12 ( Ornstein - Uhlenbeck process ) • the squared exponential model for ν → ∞ with the same scale s and variance σ 2 . I . e . the smoothness of the model increases with increasing ν . While the exponential covariance model with ν = 12 results in a random function which is not yet differentiable , larger ν result in increasing differentiability . As differentiability starts with ν = 32 and we have a more explicit formula for ν = p + 12 the cases ν = 32 and ν = 52 are of particular interest . “ [ F ] or ν ≥ 7 / 2 , in the absence of explicit prior knowledge about the existence of higher order derivatives , it is probably very hard from ﬁnite noisy training examples to distinguish between values of ν ≥ 7 / 2 ( or even to distinguish between ﬁnite values of ν and ν → ∞ , the smooth squared exponential , in this case ) ” [ 33 , p . 85 ] . Recall we have for ν = p + 12 C ν = p + 12 ( η ) = σ 2 exp ( − f ν ( (cid:107) d (cid:107) ) ) p (cid:88) k = 0 p ! ( 2 p ) ! ( 2 p − k ) ! ( p − k ) ! k ! 2 k (cid:124) (cid:123)(cid:122) (cid:125) = : a p ( k ) f ν ( (cid:107) d (cid:107) ) k ( 13 ) with f ν ( η ) : = √ 2 νs η . Taking the derivative results in C (cid:48) ν = p + 12 ( η ) = − σ 2 exp ( − f ν ( η ) ) f ν (cid:48) ( η ) (cid:20) p (cid:88) k = 0 a p ( k ) f ν ( η ) k − p (cid:88) k = 1 a p ( k ) kf ν ( η ) k − 1 (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) = a p ( p ) f ν ( η ) p + (cid:80) p − 1 k = 0 [ a p ( k ) − ( k + 1 ) a p ( k + 1 ) ] f ν ( η ) k ( 14 ) = − σ 2 exp (cid:16) − f ν ( η ) (cid:17) f ν (cid:48) ( η ) p (cid:88) k = 0 ˜ a p ( k − 1 ) f ν ( η ) k ( 15 ) = − σ 2 exp (cid:16) − f ν ( η ) (cid:17) f ν (cid:48) ( η ) p − 1 (cid:88) k = 0 ˜ a p ( k ) f ν ( η ) k + 1 = − σ 2 ( 2 p + 1 ) s 2 η exp (cid:16) − f ν ( η ) (cid:17) p − 1 (cid:88) k = 0 ˜ a p ( k ) f ν ( η ) k . 19 Where we used f ν (cid:48) ( η ) f ν ( η ) = 2 p + 1 s 2 η and deﬁne ˜ a p ( k ) : = (cid:26) a p ( k + 1 ) − ( k + 2 ) a p ( k + 2 ) k ≤ p − 2 a p ( p ) k = p − 1 ( 14 ) and note that ˜ a p ( k ) = a p ( k + 1 ) − ( k + 2 ) a p ( k + 2 ) = p ( 2 p ) ! (cid:20) ( 2 p − ( k + 1 ) ) ! ( p − ( k + 1 ) ) ! ( k + 1 ) ! 2 k + 1 − ( k + 2 ) ( 2 p − ( k + 2 ) ) ! ( p − ( k + 2 ) ) ! ( k + 2 ) ! 2 k + 2 (cid:21) = p ( 2 p ) ! ( 2 p − ( k + 2 ) ) ! ( p − ( k + 1 ) ) ! ( k + 1 ) ! 2 k + 1 (cid:20) ( 2 p − ( k + 1 ) ) − ( p − ( k + 1 ) ) 2 1 (cid:21) = p ( 2 p ) ! ( 2 ( p − 1 ) − k ) ! ( ( p − 1 ) − k ) ! ( k + 1 ) ! 2 k + 1 [ k + 1 ] = (cid:40) a p − 1 ( k ) 2 p − 1 0 ≤ k ≤ p − 2 0 k = − 1 . ( 15 ) We can further calculate a few special cases a p ( 0 ) = (cid:83)(cid:83) p ! (cid:8)(cid:8)(cid:8) ( 2 p ) ! (cid:24)(cid:24)(cid:24) (cid:24) ( 2 p − 0 ) ! (cid:88)(cid:88)(cid:88)(cid:88) ( p − 0 ) ! 0 ! 2 0 = 1 p ≥ 0 ( 16 ) a p ( 1 ) = p ! ( 2 p ) ! ( 2 p − 1 ) ! ( p − 1 ) ! 1 ! 2 1 = 2 p 2 p = 1 p ≥ 1 ( 17 ) a p ( p ) = p ! ( 2 p ) ! (cid:24)(cid:24)(cid:24)(cid:24) ( 2 p − p ) ! ( p − p ) ! (cid:19)(cid:19) p ! 2 p = 1 ( 2 p − 1 ) ! ! p ≥ 1 ( 18 ) Assuming C ( η ) = C ( η 2 ) implies C (cid:48) ( η 2 ) = 12 η C (cid:48) ( η ) . So we have C (cid:48) ( η 2 ) = − σ 2 ( 2 p + 1 ) 2 s 2 exp (cid:16) − f ν ( η ) (cid:17) p − 1 (cid:88) k = 0 ˜ a p ( k ) f ν ( η ) k with ˜ a p ( k ) = (cid:40) a p − 1 ( k ) 2 p − 1 0 ≤ k ≤ p − 2 a p ( p ) = 1 ( 2 p − 1 ) ! ! k = p − 1 . Theorem D . 5 . Assuming L is a centered random function with Matérn covariance C p + 12 for p ∈ N , we have d ( ˆ η ) = argmin d BLUE [ L ( w + d ) − L ( w ) | ∇L ( w ) ] ˆ η = argmax η ≥ 0 (cid:34) exp (cid:16) − √ 2 p + 1 s η (cid:17) p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) (cid:16) √ 2 p + 1 s (cid:17) k η k + 1 (cid:35) So we have for • p = 1 ˆ η = s √ 3 • p = 2 ˆ η = s √ 5 1 + √ 5 2 Proof . Using Remark C . 3 and Theorem C . 5 we only need the maximum of η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) = η exp ( − f ν ( η ) ) (cid:80) p − 1 k = 0 ˜ a p ( k ) f ν ( η ) k exp ( 0 ) ˜ a p ( 0 ) 0 0 ( 16 ) = η exp (cid:16) − √ 2 νs | η | (cid:17) p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) (cid:16) √ 2 νs | η | (cid:17) k . 20 As the equation above is negative for η ≤ 0 but positive for η ≥ 0 as ˜ a p ( k ) ≥ 0 , we can assume without loss of generality η ≥ 0 when maximizing . Additionally the term is zero for η = 0 positive for η ≥ 0 and tends towards zero for η → ∞ . Therefore there exists at least one maximum . Taking the derivative for η ≥ 0 results in exp (cid:16) − √ 2 νs η (cid:17) (cid:34) − p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) (cid:16) √ 2 νs η (cid:17) k + 1 + p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) ( k + 1 ) (cid:16) √ 2 νs η (cid:17) k (cid:35) ( 19 ) So for the ﬁrst order condition we require 0 = 1 + p − 1 (cid:88) k = 1 (cid:104) ( k + 1 ) ˜ a p ( k ) ˜ a p ( 0 ) − ˜ a p ( k − 1 ) ˜ a p ( 0 ) (cid:105) (cid:16) √ 2 νs η (cid:17) k − ˜ a p ( p − 1 ) ˜ a p ( 0 ) (cid:16) √ 2 νs η (cid:17) p . Let us have a look at the coefﬁcients ˜ a p ( 0 ) = a p − 1 ( 0 ) 2 p − 1 ( 16 ) = 1 2 p − 1 ˜ a p ( p − 1 ) = a p ( p ) ( 18 ) = 1 ( 2 p − 1 ) ! ! ˜ a p ( k ) ˜ a p ( 0 ) = a p − 1 ( k ) 0 ≤ k ≤ p − 2 Now let us consider p = 1 . Here we have 0 = 1 − √ 3 s η and therefore ˆ η = s √ 3 . For p = 2 we have 0 = 1 − (cid:16) √ 5 s η (cid:17) 2 + (cid:20) 2˜ a 2 ( p − 1 ) ˜ a 2 ( 0 ) − a 1 ( 0 ) (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) = 2 2 · 2 − 1 ( 2 · 2 − 1 ) ! ! − 1 = 1 ( 16 ) , ( 17 ) √ 5 s η So we want to ﬁnd √ 5 s η which solves 0 = x 2 − x − 1 . η ≥ 0 implies only one solution remains √ 5 s ˆ η = (cid:18) 1 2 + √ 1 + 4 2 (cid:19) = 1 + √ 5 2 . Theorem D . 6 . Assuming L is a random function with Matérn covariance C p + 12 for p ∈ N , we have for ξ ≤ 0 ( i . e . (cid:96) ( w , Z ) ≤ µ ) , d ( ˆ η ) = argmin d BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] with • p = 1 ˆ η = s √ 3 1 (cid:16) 1 − √ 3 s ξ (cid:17) • p = 2 ˆ η = s 2 √ 5 (cid:32) 1 + (cid:115) 1 + 4 1 − √ 53 s ξ (cid:33) 21 Proof . By Theorem C . 5 we have ˆ η = argmin η C ( η 2 ) C ( 0 ) ξ − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) with C ( η 2 ) C ( 0 ) = C p + 12 ( η ) C p + 12 ( 0 ) = σ 2 exp ( − f ν ( η ) ) (cid:80) pk = 0 a p ( k ) f ν ( η ) k σ 2 exp ( − 0 ) (cid:80) pk = 0 a p ( k ) 0 k = exp ( − f ν ( η ) ) p (cid:88) k = 0 a p ( k ) f ν ( η ) k . Taking the derivative of this ﬁrst term , we get d dη C ( η 2 ) C ( 0 ) = exp ( − f ν ( η ) ) f ν (cid:48) ( η ) (cid:34) − p (cid:88) k = 0 a p ( k ) f ν ( η ) k + p (cid:88) k = 1 a p ( k ) kf ν ( η ) k − 1 (cid:35) = − exp ( − f ν ( η ) ) f ν (cid:48) ( η ) (cid:20) a p ( p ) f ν ( η ) p + p − 1 (cid:88) k = 0 [ a p ( k ) − ( k + 1 ) a p ( k + 1 ) ] f ν ( η ) k (cid:124) (cid:123)(cid:122) (cid:125) ( 16 ) , ( 17 ) = (cid:80) p − 1 k = 1 [ a p ( k ) − ( k + 1 ) a p ( k + 1 ) ] f ν ( η ) k (cid:21) . Recall that in ( 19 ) we have already calculated the derivative of the second therm ( the gradient covariance ) , i . e . d dη η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) = exp ( − f ν ( η ) ) (cid:34) − p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) f ν ( η ) k + 1 + p − 1 (cid:88) k = 0 ˜ a p ( k ) ˜ a p ( 0 ) ( k + 1 ) f ν ( η ) k (cid:35) = exp ( − f ν ( η ) ) (cid:20) 1 + p − 1 (cid:88) k = 1 (cid:104) ( k + 1 ) ˜ a p ( k ) ˜ a p ( 0 ) − ˜ a p ( k − 1 ) ˜ a p ( 0 ) (cid:105) f ν ( η ) k − ˜ a p ( p − 1 ) ˜ a p ( 0 ) (cid:124) (cid:123)(cid:122) (cid:125) = ( 2 p − 1 ) a p ( p ) f ν ( η ) p (cid:21) . Deﬁning ∆ L = − f ν (cid:48) ( η ) ξ and multiplying the ﬁrst order condition by exp ( f ν ( η ) ) we get 0 ! = exp ( f ν ( η ) ) d dη (cid:20) C ( η 2 ) C ( 0 ) ξ − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:21) = − 1 + ( ∆ L + ( 2 p − 1 ) ) a p ( p ) f ν ( η ) p + p − 1 (cid:88) k = 1 (cid:18) ∆ L [ a p ( k ) − ( k + 1 ) a p ( k + 1 ) ] − (cid:104) ( k + 1 ) ˜ a p ( k ) ˜ a p ( 0 ) − ˜ a p ( k − 1 ) ˜ a p ( 0 ) (cid:105)(cid:19) f ν ( η ) k So for p = 1 we have 0 ! = − 1 + ( ∆ L + ( 2 · 1 − 1 ) ) a p ( p ) f ν ( η ) p = − 1 + (cid:32) 1 − √ 3 s ξ (cid:33) √ 3 s η , which implies ˆ η = 1 √ 3 s (cid:16) 1 − √ 3 s ξ (cid:17) . 22 For p = 2 we have 0 ! = − 1 + (cid:32) 3 − √ 5 s ξ (cid:33) 1 3 ! ! 5 s 2 η 2 + (cid:32) − √ 5 s ξ [ a 2 ( 1 ) − 2 a 2 ( 2 ) ] − (cid:2) 2 a 2 ( 2 ) ˜ a 2 ( 0 ) − a 1 ( 0 ) (cid:3)(cid:33) √ 5 s η = − 1 + (cid:32) 1 − √ 5 3 s ξ (cid:33) 5 s 2 η 2 + (cid:32) − √ 5 s ξ (cid:2) 1 − 2 ( 4 − 1 ) ! ! (cid:3) − (cid:2) 2 ( 4 − 1 ) ( 4 − 1 ) ! ! − 1 (cid:3)(cid:33) (cid:124) (cid:123)(cid:122) (cid:125) = − √ 53 s ξ − 1 √ 5 s η = − 1 − (cid:32) 1 − √ 5 3 s ξ (cid:33) √ 5 s η + (cid:32) 1 − √ 5 3 s ξ (cid:33) (cid:32) √ 5 s η (cid:33) 2 √ 5 s ˆ η is therefore the positive solution of 0 = ax 2 − ax − 1 with a = (cid:16) 1 − √ 53 s ξ (cid:17) . In other words we have √ 5 s ˆ η = 1 + (cid:112) 1 + 4 / a 2 . Plugging a back in results in the claim . D . 3 ( Generalized ) rational quadratic The rational quadratic covariance function is given by C ( d ) = σ 2 (cid:18) 1 + (cid:107) d (cid:107) 2 βs 2 (cid:19) − β / 2 β > 0 . It can be viewed as a scale mixture of the squared exponential and converges to the squared exponential in the limit β → ∞ [ 33 , p . 87 ] . Theorem D . 7 ( Rational Quadratic ) . Assuming L is a stationary random function with rational quadratic covariance C , then we have for ξ ≤ 0 d ( ˆ η ) = argmin d BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] ˆ η = s (cid:112) β Root η (cid:18) 1 + √ βξ s η − ( 1 + β ) η 2 + √ βξ s η 3 (cid:19) . The unique root of the polynomial in η can be found either directly with a formula for polynomials of third degree ( e . g . using Cardano’s method ) or by bisection as the root is in [ 0 , 1 / √ 1 + β ] and the function is monotonously decreasing as all coefﬁcients except for the shift are negative ( L ( w ) ≤ 0 ! ) . Proof . By Theorem C . 5 we have ˆ η = argmin η C ( η 2 ) C ( 0 ) ξ − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) for C ( x ) = σ 2 ( 1 + xβs 2 ) − β / 2 . We therefore need to minimize (cid:18) 1 + η 2 βs 2 (cid:19) − β / 2 ξ − η (cid:18) 1 + η 2 βs 2 (cid:19) − β / 2 − 1 . 23 Substitute in ˜ η : = η √ βs , then the ﬁrst order condition is 0 ! = d d ˜ η ( 1 + ˜ η ) − β / 2 ξ − (cid:112) βs ˜ η (cid:0) 1 + ˜ η 2 (cid:1) − β / 2 − 1 Dividing both sides by √ βs we get 0 = − β 2 ( 1 + ˜ η 2 ) − β 2 − 1 2˜ η ξ √ βs − ( 1 + ˜ η 2 ) − β 2 − 2 (cid:104) 1 + ˜ η 2 − ( β 2 + 1 ) 2˜ η 2 (cid:105) = − ( 1 + ˜ η 2 ) − β 2 − 2 (cid:104) β ˜ η ξ √ βs ( 1 + ˜ η 2 ) + [ 1 − ˜ η 2 ( 1 + β ) ] (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) = 1 + √ βξs ˜ η − ( 1 + β ) ˜ η 2 + √ βξs ˜ η 3 Since ξ ≤ 0 and β > 0 all coefﬁcients of the polynomial are negative except for the shift . With the minus from the front , the polynomial starts out at − 1 in zero and only increases from there . Therefore there exists a unique positive critical point which is a minimum . D . 3 . 1 Intrinsic stationarity While this is still the covariance function of a stationary random function , its variogram is of the form γ ( d ) = C ( 0 ) − C ( d ) = σ 2 (cid:20) 1 − (cid:16) 1 + (cid:107) d (cid:107) 2 βs 2 (cid:17) − β / 2 (cid:21) = σ 2 ( 1 − 2 − β / 2 ) γ 2 , β (cid:16) d √ βs (cid:17) , where γ α , β ( d ) is the model by Schlather and Moreva [ 28 ] deﬁned as γ α , β ( d ) = (cid:40) ( 1 + (cid:107) d (cid:107) α ) − β / α − 1 2 − β / α − 1 β ∈ [ − 2 , ∞ ) \ { 0 } log 2 ( 1 + (cid:107) d (cid:107) α ) β = 0 α ∈ ( 0 , 2 ] . This generalized rational quadratic can induce a stationary random function for β > 0 but is never stationary for β ≤ 0 . The rescaling is necessary because the rational quadratic is scaled in such a way , that C ( 0 ) = σ 2 . But this condition is meaningless for intrinsically stationary random functions . So instead we scale such that γ α , β ( 1 ) = 1 . This causes the covariance with itself C α , β ( 0 ) = 1 − 2 − β / 2 to explode for β → 0 . This provides some intuition why it stops being stationary for β ≤ 0 . This model includes • the nugget effect for β > 0 and α → 0 • the fractional Brownian motion for α = − β • the rational quadratic for α = 2 and β > 0 • the squared exponential for α = 2 and β → ∞ , if we remember to rescale back to the rational quadratic as we have used β in the scaling . Since the model only results in a differentiable random function for α = 2 , we have to throw out this part of the generalization again . But we still obtain the intrinsically stationary case with β ∈ [ − 2 , 0 ] . So we will only consider γ β ( d ) : = γ 2 , β ( d ) = φ ( (cid:107) d (cid:107) 2 ) , with φ ( x ) = (cid:40) ( 1 + x ) − β / 2 − 1 2 − β / 2 − 1 β ∈ [ − 2 , ∞ ) \ { 0 } ln ( 1 + x ) / ln ( 2 ) β = 0 Taking its derivative results in φ (cid:48) ( x ) =   − β 2 ( 1 + x ) − β / 2 − 1 2 − β / 2 − 1 β ∈ [ − 2 , ∞ ) \ { 0 } 1 ln ( 2 ) ( 1 + x ) β = 0 . This implies with Theorem C . 5 that ˆ η = argmax η η φ (cid:48) ( η 2 ) φ (cid:48) ( 0 ) = argmax η η ( 1 + η 2 ) − β 2 − 1 . 24 For β ≤ − 1 this function is monotonically increasing in η and we would therefore have ˆ η = ∞ , so only the case β > − 1 is interesting . Taking the derivative with regard to η results in 0 ! = d dη = ( 1 + η 2 ) − β 2 − 2 (cid:20) ( 1 + η 2 ) − 2 η 2 (cid:16) 1 + β 2 (cid:17)(cid:21) = ( 1 + η 2 ) − β 2 − 2 (cid:2) 1 − η 2 ( 1 + β ) (cid:3) As we can see that the derivative is larger than zero before the root ˆ η = 1 / √ 1 + β and smaller afterwards it is indeed a maximum . So we get Theorem D . 8 ( Generalized Rational Quadratic ) . Assuming L is centered and intrinsically stationary with variogram γ 2 , β with β ∈ ( − 1 , ∞ ) , we have d ( ˆ η ) = argmin d BLUE [ L ( w + d ) − L ( w ) | ∇L ( w ) ] ˆ η = 1 √ 1 + β . Reversing the scaling we did , when transitioning from the rational quadratic to the generalized rational quadratic , results in ˆ η = s (cid:115) β 1 + β Notice that for β → ∞ we get the same scaling as in the squared exponential covariance case . E Toy models This section investigates the plausibility of the constant expectation assumption . E . 1 A linear model Let us assume that the real data labels are generated by Y = θ T 1 X + (cid:15) ∈ R with inputs X = θ 2 ξ ∈ R d So the randomness is driven by output randomness (cid:15) with E [ (cid:15) ] = 0 and variance σ 2 and input randomness ξ ∈ R m with iid entries of unit variance . We also use a linear model for regression f ( w , x ) = w T x , ( 20 ) and use the mean squared loss function L ( w ) = E [ ( Y − w T X ) 2 | θ ] . We conditioned on the “real parameters” θ here , so we should probably talk about its prior distribution now . So ﬁrst , we would like θ 1 ∈ R d and θ 2 ∈ R d × m to be independent ( or at least E [ θ 2 θ T 2 | θ 1 ] to be constant ) , second they should also be independent of (cid:15) and ξ . Finally let us assume E [ θ 1 ] = 0 . Proposition E . 1 ( Explicit loss in the linear model ) . For the linear probability model above , we get L ( w ) = ( θ 1 − w ) T θ 2 θ T 2 ( θ 1 − w ) + σ 2 . And with E [ θ 2 θ T 2 ] = : Σ , we can obtain E [ L ( w ) ] = w T Σ w + E [ θ T 1 Σ θ 1 ] + σ 2 (cid:124) (cid:123)(cid:122) (cid:125) = : µ = (cid:107) w (cid:107) 2Σ + µ 25 Proof . First we calculate L L ( w ) = E [ ( Y − w T X ) 2 | θ ] = E [ ( ( θ 1 − w ) T X + (cid:15) ) 2 | θ ] = E [ ( ( θ 1 − w ) T X ) 2 | θ ] + 2 E [ ( ( θ 1 − w ) T X ) E [ (cid:15) | θ , X ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 | θ ] + E [ (cid:15) 2 ] (cid:124)(cid:123)(cid:122)(cid:125) = σ 2 = E [ ( θ 1 − w ) T θ 2 ξξ T θ T 2 ( θ 1 − w ) | θ ] + σ 2 E [ ξξ T ] = I = ( θ 1 − w ) T θ 2 θ T 2 ( θ 1 − w ) + σ 2 . Then we calculate its expectation E [ L ( w ) ] = E [ ( θ 1 − w ) T E [ θ 2 θ T 2 | θ 1 ] (cid:124) (cid:123)(cid:122) (cid:125) = Σ ( θ 1 − w ) ] + σ 2 = E [ θ T 1 Σ θ 1 ] + 2 w T Σ E [ θ 1 ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 + w T Σ w + σ 2 . Notice how θ 2 ∈ R d × m allows for a complex input data distribution X by cranking up m . If we draw all entries of θ 2 iid Gaussian and assume m ≥ d , θ 2 θ T 2 is Wishart distributed . In particular we have E [ θ 2 θ T 2 ] = m I . In that case the mean is rotation invariant , i . e . E [ L ( w ) ] = m (cid:107) w (cid:107) 2 + µ . E . 2 Labels as a random function We could also have deﬁned the labels Y = f ( θ 1 , X ) + (cid:15) using the linear model f , we used for regression in ( 20 ) . We now want to consider a more general model g ( θ 1 , x ) and drop the measurement error (cid:15) , since it should be clear , that it will only materialize in a constant increase of the expected loss . Notice how we can always view Z ( x ) : = g ( θ 1 , x ) as a random function into which we plug the data X . We continue to assume X = θ 2 ξ with independency between θ 1 , θ 2 and ξ , And assume that Z centered . Then we have L ( w ) = E [ (cid:107) f ( w , X ) − Y (cid:107) 2 | θ ] = E [ (cid:107) f ( w , X ) − Z ( X ) (cid:107) 2 | θ ] . It now makes sense to calculate a conditional mean ﬁrst m X ( w ) : = E [ (cid:107) f ( w , X ) − Z ( X ) (cid:107) 2 | X ] = (cid:107) f ( w , X ) (cid:107) 2 − 2 (cid:104) f ( w , X ) , E [ Z ( X ) | X ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 (cid:105) + E [ (cid:107) Z ( X ) (cid:107) 2 | X ] (cid:124) (cid:123)(cid:122) (cid:125) = : σ 2 Z ( X ) = (cid:107) f ( w , X ) (cid:107) 2 + σ 2 Z ( X ) . Where we have used that E [ Z ( x ) ] = 0 and the independence of Z and X for E [ Z ( X ) | X ] = 0 . By the tower property of conditional expectation we now get E [ L ( w ) ] = E [ m X ( w ) ] = E (cid:107) f ( w , X ) (cid:107) 2 + E [ σ 2 Z ( X ) ] (cid:124) (cid:123)(cid:122) (cid:125) = : µ ∈ R . So in general up to a constant , we could try to calculate E (cid:107) f ( w , X ) (cid:107) 2 to get the expectation of the loss . A linear model f will lead to a quadratic mean . A bounded model f has a much better chance to have constant expectation on the other hand . But before we consider bounded models , let us use this insight to calculate the expectation of a linear model with bias . 26 E . 3 Linear regression with bias We consider w = ( w , w 0 ) and X = θ 2 ξ with θ 2 ∈ R d × m for m ≥ d and iid N ( 0 , 1 ) entries for simplicity and deﬁne the regression to be f ( w , X ) = X T w + √ mw 0 ∈ R , The normalization of the bias immediately makes sense once we calculate the resulting mean E (cid:107) f ( w , X ) (cid:107) 2 = w T E [ XX T ] (cid:124) (cid:123)(cid:122) (cid:125) = E [ θ 2 E [ ξξ T | θ ] θ T 2 ] w + 2 √ mw 0 E [ X T ] (cid:124) (cid:123)(cid:122) (cid:125) = 0 w + mw 20 = ( w T E [ θ 2 θ T 2 ] w + mw 20 ) = m (cid:107) w (cid:107) 2 , where we used the property E [ θ 2 θ T 2 ] = m I of the Wishart matrix θ 2 θ T 2 . F Regularized RFD The previous section motivated m L ( w ) : = E [ L ( w ) ] = σ 2 (cid:107) w (cid:107) 2 + µ . Turning this into an assumption without regard for its origin results in a form of L 2 - regularization of the RFD optimization problem . We assume that ¯ L = L − m L is isotropic and get due to linearity of the BLUE BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] = m L ( w + d ) + C ( (cid:107) d (cid:107) 2 ) C ( 0 ) ¯ L ( w ) + C (cid:48) ( (cid:107) d (cid:107) 2 ) C (cid:48) ( 0 ) (cid:104) d , ∇ ¯ L ( w ) (cid:105) with ¯ L ( w ) = L ( w ) − ( σ 2 (cid:107) w (cid:107) 2 + µ ) ∇ ¯ L ( w ) = ∇L ( w ) − 2 σ 2 w Since the direction of d matters in more than one place now , the minimization problem becomes more complicated . But we still want to do the minimization over d in two steps . First we ﬁx some η = (cid:107) d (cid:107) and then we optimize over η . To solve the restricted optimization problem we deﬁne the Lagrangian function L ( d , λ ) : = BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] + λ ( (cid:107) d (cid:107) 2 − η 2 ) Taking the derivative with respect to d results in 0 ! = ∇ d L ( d , λ ) = ∇ m L ( w + d ) + C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 λ d = 2 σ 2 ( w + d ) + C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 λ d This implies that d = − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w 2 σ 2 + 2 λ . Taking the derivative of the Lagrangian with respect to λ would provide us with the constraint (cid:107) d (cid:107) = η again . And since 2 σ 2 + 2 λ is a real number we can move out of the norm , we get d η = d (cid:107) d (cid:107) = − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:13)(cid:13)(cid:13) C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:13)(cid:13)(cid:13) . 27 Note that this is where a more complicated mean like m L ( w ) = w T Σ X w would fail , as we can not move Σ X out of the norm . Which is why it was important to rescale the bias in the linear model . In conclusion we have d ( η ) = − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:13)(cid:13)(cid:13) C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:13)(cid:13)(cid:13) . I . e . we have d ( η ) ∼ C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:124) (cid:123)(cid:122) (cid:125) trust decay ∇ ¯ L ( w ) (cid:122) (cid:125)(cid:124) (cid:123) ( ∇L ( w ) − ∇ m L ( w ) ) + 1 (cid:124)(cid:123)(cid:122)(cid:125) no decay ∇ m L ( w ) = C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇L ( w ) + (cid:18) 1 − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:19) ∇ m L ( w ) . In other words : We start out moving in the direction of the gradient , d ( 0 ) ∼ ∇L ( w ) , but with increasing distance η = (cid:107) d (cid:107) from w , we trust the gradient less and less and increasingly move in the direction of the gradient of the mean ∇ m L . Recall that L 2 - regularization of the loss implies f ( w ) : = L ( w ) + λ (cid:107) w (cid:107) 2 and the gradient would then be ∇ f ( w ) = ∇L ( w ) + 2 λw . So in some sense we are simply selecting a dynamic regularization λ with λ 1 + λ = 1 − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) , where the regularization term increases with η . Resubstituting the direction d ( η ) back into the BLUE we get BLUE [ L ( w + d ( η ) ) | L ( w ) , ∇L ( w ) ] = σ 2 (cid:107) w + d ( η ) (cid:107) 2 + µ + C ( η 2 ) C ( 0 ) ¯ L ( w ) + C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:104) d ( η ) , ∇ ¯ L ( w ) (cid:105) = µ + σ 2 ( (cid:107) w (cid:107) 2 + 2 (cid:104) w , d ( η ) (cid:105) + η 2 ) + C ( η 2 ) C ( 0 ) ¯ L ( w ) + (cid:28) d ( η ) , C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) (cid:29) = C + σ 2 η 2 + C ( η 2 ) C ( 0 ) ¯ L ( w ) + (cid:28) d ( η ) , C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:29) (cid:124) (cid:123)(cid:122) (cid:125) = − η (cid:13)(cid:13)(cid:13) C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇ ¯ L ( w ) + 2 σ 2 w (cid:13)(cid:13)(cid:13) , where C is a constant independent of η . Theorem F . 1 ( Regularized RFI Descent ) . For an isotropic random function L with mean E [ L ( w ) ] = σ 2 (cid:107) w (cid:107) 2 + µ , the regularized gradient ( a convex combination of ∇L ( w ) and the expected gradient ) g ( η ) : = C (cid:48) ( η 2 ) C (cid:48) ( 0 ) ∇L ( w ) + (cid:18) 1 − C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:19) ∇ E [ L ( w ) ] (cid:124) (cid:123)(cid:122) (cid:125) = 2 σ 2 w is the maximum descent direction for the 1 - step 1 - order BLUE . I . e . d ( ˆ η ) = − ˆ η g ( ˆ η ) (cid:107) g ( ˆ η ) (cid:107) = argmin d BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] with ˆ η = argmin η σ 2 X η 2 + C ( η 2 ) C ( 0 ) ( L ( w ) − m L ( w ) ) − η (cid:107) g ( η ) (cid:107) . 28 G Conservative RFD An upper bound on L ˆ L ( w + d ) = ˜ L ( w + d ) + σ (cid:15) ( d ) with ( 1 − (cid:15) ) probability requires P (cid:16) L ( w + d ) ≥ BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] (cid:124) (cid:123)(cid:122) (cid:125) = : ˜ L ( w + d ) + σ (cid:15) ( d ) (cid:17) ! ≤ (cid:15) . To ﬁnd an appropriate σ (cid:15) , one can apply the Markov inequality to immediately ﬁnd a sufﬁcient condition P ( L ( w + d ) ≥ ˆ L ( w + d ) ) = P ( L ( w + d ) − ˜ L ( w + d ) ≥ σ (cid:15) ( d ) ) ≤ P ( | L ( w + d ) − ˜ L ( w + d ) | ≥ σ (cid:15) ( d ) ) ≤ E [ ( L ( w + d ) − ˜ L ( w + d ) ) 2 ] σ (cid:15) ( d ) ! = (cid:15) . A sensible selection of σ (cid:15) is therefore σ (cid:15) ( d ) = (cid:113) 1 (cid:15) E (cid:2)(cid:0) L ( w + d ) − BLUE [ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] (cid:1) 2 (cid:3) L Gauss . = (cid:113) 1 (cid:15) Var (cid:2) L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) (cid:3) . Since the BLUE is a minimizer of the enumerator , we can actually state an explicit formula . In fact this generalizes to stochastic gradients too . Lemma G . 1 . For a stationary , centered loss L the variance inside of σ (cid:15) is given by E [ ( L ( w + d ) − BLUE [ L ( w + d ) | (cid:96) ( w , Z ) , ∇ (cid:96) ( w , ˜ Z ) ] ) 2 ] = C ( 0 ) − C ( d ) 2 C ( 0 ) + E [ (cid:15) 20 ] − (cid:104)∇C ( d ) , ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ∇C ( d ) (cid:105) . in the isotropic case this simpliﬁes to C ( 0 ) − C ( (cid:107) d (cid:107) 2 ) 2 C ( 0 ) + E [ (cid:15) 20 ] − 2 C (cid:48) ( (cid:107) d (cid:107) 2 ) 2 − C (cid:48) ( 0 ) + σ 2 (cid:107) d (cid:107) 2 . It is in particular independent of the direction of d (cid:107) d (cid:107) , so the gradient direction is still optimal . Proof . Note that BLUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] = argmin ˆ L∈ LUE [ L ( w + d ) | L ( w ) , ∇L ( w ) ] E [ (cid:107)L ( w + d ) − ˆ L(cid:107) 2 ] . We calculated this argmin in Lemma C . 1 . Here we need the minimum , which works analogously . Simply plug in a = C ( d ) C ( 0 ) + E [ (cid:15) 20 ] and b = − ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ∇C ( d ) . into the g ( a , b ) derived in that proof , i . e . g ( a , b ) = C ( 0 ) − 2 a C ( d ) + a 2 ( C ( 0 ) + E [ (cid:15) 20 ] ) + 2 (cid:104) b , ∇C ( d ) (cid:105) + (cid:104) b , ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) b (cid:105) = C ( 0 ) − C ( d ) 2 C ( 0 ) + E [ (cid:15) 2 0 ] − (cid:104)∇C ( d ) , ( −∇ 2 C ( 0 ) + E [ (cid:15) 1 (cid:15) T 1 ] ) − 1 ∇C ( d ) (cid:105) . Now simply reuse the facts ∇ 2 C ( 0 ) = 2 C (cid:48) ( 0 ) I , E [ (cid:15) 1 (cid:15) T 1 ] = σ 2 I and ∇C ( d ) = 2 C (cid:48) ( (cid:107) d (cid:107) 2 ) d for the isotropic case to get C ( 0 ) − C ( (cid:107) d (cid:107) 2 ) 2 C ( 0 ) + E [ (cid:15) 20 ] − 2 C (cid:48) ( (cid:107) d (cid:107) 2 ) 2 − C (cid:48) ( 0 ) + σ 2 (cid:107) d (cid:107) 2 . 29 Theorem G . 2 ( Conservative RFD ) . Minimizing the upper bound ˆ L ( w + d ) = ˜ L ( w + d ) + σ (cid:15) ( d ) over d in the isotropic case results in d ( ˆ η ) = − η (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107)∇ (cid:96) ( w , ˜ Z ) with ˆ η = argmin η C ( η 2 ) C ( 0 ) ( (cid:96) ( w , Z ) − µ ) − η C (cid:48) ( η 2 ) C (cid:48) ( 0 ) (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) + σ (cid:15) ( η ) Proof . As σ (cid:15) is rotation invariant , we can simply apply Lemma C . 4 . Although this time it is not possible to divide by (cid:107)∇ (cid:96) ( w , ˜ Z ) (cid:107) . H Second order methods This section is a ﬁnal addendum which was not discussed in the main work . Since we were able to derive gradient descent from conditional expectation , it is natural to ask , whether it might be possible to derive higher order methods , like the Newton method from a conditional expectation too . While we were able to recognize a Taylor - like component in the second order conditional expectation , this approach has overall felt like a dead end , especially because second order methods are not common in machine learning in the ﬁrst place . But perhaps this will be a useful starting point for future research . Lemma H . 1 . If L is a centered , intrinsically stationary , twice differentiable random function , with rotation invariant variogram γ of the form γ ( d ) = φ ( (cid:107) d (cid:107) 2 ) , then BLUE [ L ( w + d ) − L ( w ) (cid:124) (cid:123)(cid:122) (cid:125) = : L w ( d ) | ∇L ( w ) , ∇ 2 L ( w ) ] = φ (cid:48) ( (cid:107) d (cid:107) 2 ) φ (cid:48) ( 0 ) (cid:104) d , ∇L ( w ) (cid:105) − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) φ (cid:48)(cid:48) ( 0 ) 1 2 (cid:104)∇ 2 L ( w ) d , d (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) Taylor - like + tr ( ∇ 2 L ( w ) ) d + 2 (cid:20) φ (cid:48) ( 0 ) − φ (cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) + φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:107) diag ( d ) (cid:107) 2 (cid:21) . Proof . We have ˆ L w ( d ) = (cid:104) b , ∇L ( w ) (cid:105) + (cid:88) ij ∂ ij L ( w ) c ij (cid:124) (cid:123)(cid:122) (cid:125) = : (cid:104)∇ 2 L ( w ) , c (cid:105) where due to ∂ ij L = ∂ ji L only the sum c ij + c ji matters and we can assume without loss of generality c ij = c ji . We want to minimize g ( b , c ) = E [ (cid:107)L w ( d ) − ˆ L w ( d ) (cid:107) 2 ] = E [ L w ( d ) 2 ] − 2 (cid:104) b , E [ L w ( d ) ∇L ( d ) ] (cid:105) + (cid:104) b , E [ ∇L ( w ) ∇L ( w ) T ] b (cid:105) − (cid:104) E [ L w ( d ) ∇ 2 L ( w ) ] , c (cid:105) + E  (cid:18) (cid:88) i , j ∂ ij L ( w ) c ij (cid:19) 2   Lem A . 14 = 2 γ ( d ) − 2 (cid:104) b , ∇ γ ( d ) (cid:105) + (cid:104) b , ∇ 2 γ ( 0 ) b (cid:105) − 2 (cid:104)∇ 2 γ ( 0 ) − ∇ 2 γ ( d ) , c (cid:105) + (cid:88) l , k , i , j ∂ lkij γ ( 0 ) c lk c ij 30 where we have used the fact that the ﬁrst and second derivative are uncorrelated . Since b and c are in this sense independent form each other , optimizing g over b results in the same factor as in Lemma C . 2 . So we just need to consider the optimization over c . The ﬁrst order condition is 0 ! = 1 2 dg dc ij = − ∂ ij [ γ ( 0 ) − γ ( d ) ] + (cid:88) l , k ∂ lkij γ ( 0 ) c lk As we are in the rotational invariant case γ ( d ) = φ ( (cid:107) d (cid:107) 2 ) we have ( Lemma A . 15 ) ∂ ij γ ( d ) = 4 φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d i d j + 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) δ ij ∂ lkij γ ( 0 ) = 4 φ (cid:48)(cid:48) ( 0 ) [ δ ki δ jl + δ kj δ il + δ ij δ kl ] . Which simpliﬁes the ﬁrst order condition to 0 ! = 1 2 dg dc ij = 4 φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d i d j + 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) δ ij − 2 φ (cid:48) ( 0 ) δ ij + 4 φ (cid:48)(cid:48) ( 0 ) (cid:88) l , k [ δ ki δ jl + δ kj δ il + δ ij δ kl ] c lk = 4 φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d i d j + 2 [ φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) ] δ ij + 4 φ (cid:48)(cid:48) ( 0 ) [ c ji + c ij + δ ij diag ( c ) ] Taking another derivative in c results in a multiple of 4 φ (cid:48)(cid:48) ( 0 ) which is greater zero . Therefore the ﬁrst order condition describes a minimum . Recall that we have have assumed c ij = c ji without loss of generality . So for i (cid:54) = j we have 0 ! = φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d i d j + 2 φ (cid:48)(cid:48) ( 0 ) c ij which implies c ij = − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) d i d j . On the diagonal on the other hand 0 ! = φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) d i d i + 12 [ φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) ] + φ (cid:48)(cid:48) ( 0 ) [ 2 c ii + diag ( c ) ] which implies c ii = − (cid:20) φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) d i d i + 12 [ φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) ] 2 φ (cid:48)(cid:48) ( 0 ) + diag ( c ) 2 (cid:21) . Of course this is still recursive due to the diagonal . But diag ( c ) = − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:88) i d 2 i − d 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) 2 φ (cid:48)(cid:48) ( 0 ) − d 2 diag ( c ) implies diag ( c ) = − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:80) i d 2 i + d 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) 2 φ (cid:48)(cid:48) ( 0 ) 1 + d 2 . So we ﬁnally end up with c ij = − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) d i d j − δ ij   12 [ φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) ] 2 φ (cid:48)(cid:48) ( 0 ) − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:80) i d 2 i + d 2 φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) 2 φ (cid:48)(cid:48) ( 0 ) 2 ( 1 + d 2 )   (cid:124) (cid:123)(cid:122) (cid:125) = : I . where we can further simplify I to I = (cid:18) 1 − d 2 + d (cid:19) 12 [ φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) ] 2 φ (cid:48)(cid:48) ( 0 ) − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:107) diag ( d ) (cid:107) 2 2 + d = 1 2 + d (cid:20) φ (cid:48) ( (cid:107) d (cid:107) 2 ) − φ (cid:48) ( 0 ) 2 φ (cid:48)(cid:48) ( 0 ) − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:107) diag ( d ) (cid:107) 2 (cid:21) . 31 So the BLUE is of the form BLUE [ L w ( d ) | ∇L ( w ) , ∇ 2 L ( w ) ] = φ (cid:48) ( (cid:107) d (cid:107) 2 ) φ (cid:48) ( 0 ) (cid:104) d , ∇L ( w ) (cid:105) + (cid:88) ij ∂ ij L ( w ) c ij with c ij = − φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) d i d j + δ ij d + 2 (cid:20) φ (cid:48) ( 0 ) − φ (cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) + φ (cid:48)(cid:48) ( (cid:107) d (cid:107) 2 ) 2 φ (cid:48)(cid:48) ( 0 ) (cid:107) diag ( d ) (cid:107) 2 (cid:21) . Plugging these c ij into the BLUE yields the claim . 32