A COMPREHENSIVE STUDY OF RESIDUAL CNNS FOR ACOUSTIC MODELING IN ASR Vitalii Bozheniuk 1 , Albert Zeyer 1 , 2 , Ralf Schl¨uter 1 , 2 , Hermann Ney 1 , 2 1 Human Language Technology and Pattern Recognition Group , Computer Science Department RWTH Aachen University , 52074 Aachen , Germany 2 AppTek GmbH , 52062 Aachen , Germany vitalii . bozheniuk @ rwth - aachen . de , { zeyer , schlueter , ney } @ cs . rwth - aachen . de ABSTRACT Long short - term memory ( LSTM ) networks are the dominant architecture for large vocabulary continuous speech recogni - tion ( LVCSR ) acoustic modeling due to their good perfor - mance . However , LSTMs are hard to tune and computation - ally expensive . To build a system with lower computational costs and which allows online streaming applications , we ex - plore convolutional neural networks ( CNN ) . To the best of our knowledge there is no overview on CNN hyper - parameter tuning for LVCSR in the literature , so we present our results explicitly . Apart from recognition performance , we focus on the training and evaluation speed and provide a time - efﬁcient setup for CNNs . We faced an overﬁtting problem in training and solved it with data augmentation , namely SpecAugment . The system achieves results competitive with the top LSTM results . We signiﬁcantly increased the speed of CNN in train - ing and decoding approaching the speed of the ofﬂine LSTM . Index Terms — acoustic modeling , CNN , ResNet , LACE , dense prediction 1 . INTRODUCTION AND RELATED WORK Recurrent neural networks ( RNN ) and long short - term mem - ory ( LSTM ) networks [ 1 – 3 ] in particular , deal very well with sequence data . LSTM acoustic models are applied in the LVCSR tasks and achieve competitive results when properly tuned [ 4 ] . Moreover , bidirectional LSTM achieves 2 . 3 % / 5 . 0 % ( test - clean / test - other ) on the LibriSpeech dataset [ 5 ] , which is state - of - the - art performance . However , LSTMs are difﬁcult to train due to their recurrent nature . Hence , there are attempts to research alternative models , like time delay neural networks ( TDNNs ) and convolutional neural networks [ 6 , 7 ] . TDNNs and CNNs perform multiple con - volutions , which is a simple operation and can be well par - allelized . This is good for lots of practical applications like online recognition , where speed has as much importance as performance . CNN models take into account the spacial dependency of the spectrogram and exhibit a strong comple - mentarity to the RNNs , which focus on the time dependency . A combination of CNN and RNN models yields state - of - the - art performance on the Switchboard dataset [ 8 ] ( 5 . 6 % / 9 . 1 % on the Switchboard / Callhome ) . CNNs are frequently used as pure models [ 6 , 7 , 9 , 10 ] in the LVCSR tasks , where they show competitive to RNNs performance . There are many aspects , which have to be taken into ac - count to train CNN on sequence data . Unfortunately , CNNs tend to overﬁt on the 300h Switchboard dataset . We have found that the discussion of the CNN generalization in the LVCSR tasks is missing . In this work , we address this is - sue by applying SpecAugment data augmentation [ 11 ] , which was shown to be effective for other architectures [ 12 ] . An - other aim of this work is to study the inﬂuence of hyper - parameters on CNN performance in acoustic modeling . We examine ResNet [ 10 ] – a CNN model with residual shortcuts , which was ﬁrst proposed for image labeling [ 13 ] . LACE [ 14 ] is another CNN model , which we study in this work . Sim - ilarly to the ResNet , LACE processes the acoustic signal by 2 - D convolutions with residual connections . In the end , we compare LSTM and CNN performance . In addition to our ﬁndings and results , we provide the conﬁguration ﬁles to en - sure the reproducibility of the experiments . Training and inference speed of CNN are of paramount importance for us . To achieve the best speed , we investigate the peculiarities of CUDA / cuDNN [ 15 ] as implemented by [ 16 ] . Apart from that , we investigate a dense recipe [ 17 ] and other implementation tricks to achieve fast training and infer - ence . The experiments were performed with RETURNN [ 18 ] . All the code and the conﬁgs for all our experiments is publicly available 1 . 2 . RESIDUAL CNNS The input data in the ASR task is presented in the form of a spectrogram , where time and feature axes encode the tem - poral and spectral properties of the original acoustic signal . LSTMs exploit the temporal context through its recurrency . CNN acoustic models exploit the temporal as well as the spec - tral contexts through the convolutions with small ﬁlters . In this work , we use CNNs with 2 - D convolutional operations to operate locally in the time and feature axes of the spectrogram at the same time . 1 https : / / github . com / rwth - i6 / returnn - experiments / tree / master / 2019 - asr - resnet - lace - cnn ( a ) ResNet block ( b ) LACE JumpBlock Fig . 1 . The structure of ResNet building block ( a ) is similar to the LACE JumpBlock ( b ) . The main difference is in the order of BatchNorm and ReLU layers . Also , representation shape is changed in the very ﬁrst convlutional layer ( blue ) of the ﬁrst ResNet subblock . However , in LACE , an extra convolutional layer is dedicated for the reshape . The ASR task requires the acoustic model to produce a se - quence of predictions . Thus a context window is constructed at each time step of the spectrogram . This allows the CNN to interpret the windows as separate images . The CNN produces a single prediction for each window . This makes training and evaluation of the CNN as the acoustic model similar to the image recognition model . Convolutional layers transform the representation of the spectrogram and pass it to the next layer . The layer kernels can be strided to reduce the number of dimensions in the rep - resentation , thus simplifying further computations . 2 . 1 . Residual connections The number of CNN layers plays an important role in the per - formance . However , when going deeper , the model suffers from bad convergence due to the vanishing / exploding gradi - ent problem . Residual connections [ 13 ] address this issue . 2 . 1 . 1 . ResNet block The original ResNet architecture implements the residual connections by means of building blocks . Each building block is a set of residual subblocks ( Figure 1 ( a ) ) . The resid - ual subblock consists of convolutional layers and a shortcut connection , which adds the subblock input to the output of the convolutional layers . In our experiments with ResNet , we use a full pre - activation variant of the subblock [ 19 ] . 2 . 1 . 2 . LACE JumpNets The LACE architecture [ 9 ] implements residual connections with JumpNets . These small structural elements have a struc - ture , which is similar to the ResNet subblock . JumpNets are combined into JumpBlocks ( Figure 1 ( b ) ) , which are stacked and form the complete LACE architecture . The original JumpBlock exhibits the attention layer before producing the output , which is an element - wise product of the windowed representation and the trainable weight matrix . We discard this layer due to the non - windowed approach , which is dis - cussed further . 3 . EXPERIMENTS 3 . 1 . Experimentation setup For training and using our neural networks , we use RE - TURNN [ 18 ] , which is based on TensorFlow [ 16 ] . For feature extraction and decoding , we use RASR [ 20 ] . We use Switchboard - 1 Release 2 ( LDC97S62 ) [ 21 ] in all ex - periments for training . This dataset consists of 300 hours of English telephone conversations between different people on various casual topics . We use HUB5’00 ( LDC2002S09 ) dataset to evaluate the model , which consists of Switch - board ( SWB ) and Callhome ( CH ) parts . We distinguish the datasets and give the results for SWB and CH parts sepa - rately . Also , we give a total result as a combination of two parts . We use 64 mel bin LogMel features with ∆ and ∆∆ . Apart from that , we report the results for the 40 - dimensional VTLN - normalized Gammatone ( GT ) [ 22 ] . We ﬁx the to - tal number of time frames in the mini - batch to 4800 . More details on the mini - batch construction can be found in the BLSTM paper [ 4 ] . We utilize the hybrid NN - HMM ap - proach to solve the ASR task . The language model ( LM ) is a 4 - gram , trained on transcripts of the acoustic training data ( 3M running words ) and the transcripts of the Fisher English corpora ( LDC2004T19 & LDC2005T19 ) with 22M running words [ 23 ] . We use a Classiﬁcation And Regression Tree ( CART ) with 9001 labels . We keep the LM ﬁxed during training the acoustic model . We train the ANNs using a cross entropy criterion on a ﬁxed Viterbi alignment . The evaluation does not use any alignment and computes the word error rate ( WER ) between the true transcription and the hypothesized one . 3 . 2 . The architectures in detail In the experiments , we use two CNN architectures : ResNet and LACE . We have chosen the architecture parameters ac - cording to the original papers [ 10 , 14 ] and keep constant in the experiments . In ResNet , we have 4 blocks with 3 residual sub - blocks in each . The blocks contain convolutional kernels with 64 , 128 , 256 , and 512 ﬁlters receptively . The number of ﬁlters is constant within each block . In LACE , we have 4 JumpBlocks with 2 JumpNets in each . The number of ﬁlters is distributed in the JumpBlocks as 128 , 256 , 512 , and 1024 . The convolu - tional kernels in both architectures have size 3 × 3 . For comparison , we use the BLSTM [ 3 , 4 ] architecture , which yields the very best result for the SWB 300h task within our framework . The architecture consists of 6 forward and 6 backward LSTM layers . Each layer is built of 500 cells . Table 1 . ResNet Speed - up . Last column shows the training time of one subepoch . Densepredict . Fused BatchNorm Dataformat Finalpooling Time [ H : MM ] no no NHWC pool 8 : 25 yes 2 : 14 NCHW 0 : 54 yes 0 : 39 reduce mean 0 : 37 3 . 3 . Training speed 3 . 3 . 1 . Backend speed - ups We use GeForce GTX 1080 Ti for training the acoustic mod - els . Also , we use TensorFlow version 1 . 11 as a backend of our framework . To take a full advantage of the GPU paral - lelism , we have to consider the following recommendations from the TensorFlow documentation : • convolutions are computed faster when the data has the NCHW 1 format instead of the default NHWC 2 ( Perfor - mance guide [ 24 ] ) ; • the batch normalization algorithm can be fused into a single operation , which speeds up the computation ( API documentation [ 25 ] ) ; • tf . reduce mean performs better than tf . nn . pool according to the source code comments ( TensorFlow ofﬁcial ResNet implementation [ 26 ] ) when calculating the mean over the whole dimension . This is the case of the ﬁnal pooling layer for CNNs . 3 . 3 . 2 . Dense prediction The window construction at each time frame is very inefﬁ - cient and leads to a number of drawbacks [ 17 , 27 ] . Applying a few tricks [ 17 , 27 ] to the model allows the CNNs to per - form convolutions on the non - windowed sequence directly ( dense prediction ) . The idea is to remove the striding pool - ing / convolution operation . The effect of striding is critical – it grants access to a wider context for kernels in the next layers . The key idea is to implement the effect by means of sparse kernels . Hence the procedure to adopt an arbitrary CNN for the dense prediction looks as follows : • For pooling layers with stride s , change striding from s to 1 ; • Multiply the dilation rate by s in all consecutive convo - lutional and pooling layers ; • Fully connected layers can be viewed as a 1 × 1 con - volutional layer , where the dilation rate is redundant . Thus the fully connected layers remain unchanged . 1 NCHW – the data is stored as [ batch , channels , height , width ] . 2 NHWC – the data is stored as [ batch , height , width , channels ] . Table 2 . Effect of the training method and the learning rate in ResNet and LACE . Adam method uses Nesterov Momentum ( Nadam ) . Epsilon (cid:15) is used for numerical stability in Adam . Method Mom . value (cid:15) Learningrate WER [ % ] ResNet Nadam 10 − 3 17 . 5 0 . 9 1 . 0 0 . 5 · 10 − 3 17 . 0 10 − 4 18 . 7 SGD 10 − 5 16 . 8 0 . 99 — 0 . 5 · 10 − 5 16 . 6 10 − 6 17 . 5 LACE Nadam 0 . 9 10 − 3 16 . 1 1 . 0 0 . 5 · 10 − 3 15 . 8 10 − 4 16 . 5 0 . 1 0 . 5 · 10 − 3 16 . 5 SGD 10 − 5 15 . 7 0 . 99 — 0 . 5 · 10 − 5 15 . 5 10 − 6 16 . 0 0 . 95 — 0 . 5 · 10 − 5 16 . 1 0 . 9 — 0 . 5 · 10 − 5 16 . 8 3 . 3 . 3 . Effect of modiﬁcations Table 1 shows the inﬂuence of the proposed modiﬁcations on the training time of ResNet . The model was trained on the LogMel features of the SWB train corpus . According to the results , the training speed of ResNet became 13 times faster after applying all speed - ups . The above modiﬁcations signiﬁcantly increase the train - ing time and allow fast hyper - parameter tuning . We exploit all the modiﬁcations in the experiments with CNNs . 3 . 4 . Hyper - parameters 3 . 4 . 1 . Training algorithm and learning rate We experimented with training methods and their parameters . We tried an Adam with Nesterov acceleration [ 28 ] since it gives the best result for RNN models in our framework . An - other method was SGD with momentum [ 29 ] . Table 2 gives the performance of ResNet and LACE . We found that ResNet and LACE achieve the best result for the same set of hyper - parameters . In particular , SGD + momentum with learning rate 0 . 5 · 10 − 5 and momentum 0 . 99 yields the best result for both architectures . 3 . 4 . 2 . Dropout and L2 Dropout and L2 regularization prevent the ANNs from over - ﬁtting . We applied L2 to the update rule of every weight . The dropout was applied after every convolutional layer . Accord - ing to the comparison in Table 3 , we achieve the best perfor - mance with a small dropout with probability 0 . 05 and L2 with coefﬁcient 0 . 1 . 3 . 5 . Data augmentation As it turned out , the CNN models do not generalize well de - spite the regularization techniques . Table 4 shows the LACE frame error rate ( FER ) of the best subepoch during the train - ing . We conclude that CNN tends to overestimate the predic - Table 3 . Effect of dropout and L2 in ResNet . Dropout L2 WER [ % ] 0 10 − 1 16 . 6 0 . 05 16 . 3 0 . 1 16 . 5 0 . 2 16 . 8 0 . 5 error 0 10 − 3 17 . 5 10 − 2 16 . 9 10 − 1 16 . 6 0 . 5 · 10 0 17 . 2 0 . 1 10 − 2 16 . 6 Table 4 . LACE overﬁtting . Without data augmentation , train FER is lower than cross - validation ( cv ) FER , model overesti - mates . With data augmentation model underestimates . SpecAugment FER [ % ] train cv no 16 . 2 22 . 6 yes 23 . 6 22 . 1 tions , which leads to worse generalization . We address this problem by the data augmentation policy , introduced in the SpecAugment paper [ 11 ] . This paper regards two kinds of augmentation : warping and masking in the time and feature axes . We empirically found that masking 10 % of the spec - trogram without warping gives the best result . In general , data masking improved the performance of the CNNs . The WER of ResNet dropped from 16 . 3 % to 15 . 8 % . The WER of LACE dropped from 16 . 1 % to 14 . 8 % . That tells that mask - ing helps CNNs generalize better . Moreover , it conﬁrms that the original models suffer from the overﬁtting problem . All in all , SpecAugment closes the LACE result ( 14 . 8 % ) and the RNN baseline ( 14 . 6 % ) . On the contrary , the BLSTM model does not gain any beneﬁts from the data augmentation . It is worth mentioning that LogMel features suit better for CNNs , whereas LSTMs perform better on the Gammatones . We also explored MFCC but did not see improvements . 3 . 6 . Final results Table 6 contains the ﬁnal results of our experiments . Accord - ing to the word error rate , the CNN approaches the BLSTM performance ( 14 . 8 % vs . 14 . 3 % ) . We notice that the main improvement is in the CH part . Since CH is a harder task , we conclude that data augmentation indeed helped for better generalization . However , our BLSTM does not receive any beneﬁts from augmenting the data . The last lines in Table 6 contain the results from the original papers , where the mod - els were trained within the same setup . The setup serves as a baseline in the papers . 4 . CONCLUSION We investigated the dense prediction recipe and framework peculiarities to improve both the training and evaluation speed of the CNN acoustic models . The speed increased 13 times Table 5 . Investing SpecAugment data augmentation , and comparing different feature types . Model SpecAugment WER [ % ] LogMel GT ∆ + ∆∆ ResNet no 16 . 3 16 . 8 yes 15 . 8 16 . 0 LACE no 16 . 1 16 . 3 yes 14 . 8 15 . 7 BLSTM no 15 . 0 14 . 6 yes 15 . 2 14 . 3 Table 6 . Final results . Last lines show the results from the literature . The forth column contains the training time of one subepoch . Model Features Param . num . Time [ H : MM ] SpecAugm . WER [ % ] SWB CH Total ResNet LogMel ∆ + ∆∆ 36 . 7 M 0 : 37 no 10 . 6 22 . 0 16 . 3 yes 10 . 5 21 . 1 15 . 8 LACE 65 . 5 M 1 : 06 no 10 . 5 21 . 6 16 . 1 yes 9 . 8 19 . 8 14 . 8 BLSTM 41 . 1 M 1 : 03 no 10 . 0 19 . 2 15 . 0 BLSTM GT 41 . 1 M 0 : 42 no 10 . 0 19 . 2 14 . 6 yes 9 . 6 19 . 0 14 . 3 ResNet [ 10 ] LogMel ∆ + ∆∆ no 11 . 2 — — LACE [ 9 ] LogMel no 11 . 0 — — and allowed us to perform much more hyper - parameter tun - ing . Unfortunately , ofﬂine LSTM remains faster than the best CNN model . We plan to compare the CNN and LSTM models in the online recognition task , where CNN might look better due to higher parallelism . We held a comprehensive study of the optimization meth - ods and regularization parameters . The optimal set of hyper - parameters yields a stronger baseline than the one which is used in the literature . We plan to use this knowledge for fur - ther experiments with CNNs . During experiments , we found that CNNs tend to overﬁt on the 300h Switchboard dataset . We examined the SpecAug - ment data augmentation technique , which turned out to be beneﬁcial for CNN generalization and addresses the overﬁt - ting problem . As a result , CNN achieved 14 . 8 % WER on the 300h Switchboard dataset , which is close to the top LSTM performance ( 14 . 3 % ) . 5 . ACKNOWLEDGEMENTS This work has received funding from the European Re - search Council ( ERC ) under the European Union’s Hori - zon 2020 research and innovation programme ( grant agreement No 694537 , project ”SEQCLAS” ) and from a Google Focused Award . The work reﬂects only the authors’ views and none of the funding parties is responsible for any use that may be made of the information it con - tains . 6 . REFERENCES [ 1 ] Sepp Hochreiter and J¨urgen Schmidhuber , “Long short - term memory , ” Neural computation , vol . 9 , no . 8 , pp . 1735 – 1780 , 1997 . [ 2 ] Anthony J . Robinson , “An application of recurrent nets to phone probability estimation , ” IEEE transactions on neural networks , vol . 5 2 , pp . 298 – 305 , 1994 . [ 3 ] Hasim Sak , Andrew W . Senior , and Franc¸oise Beaufays , “Long short - term memory based recurrent neural net - work architectures for large vocabulary speech recogni - tion , ” CoRR , vol . abs / 1402 . 1128 , 2014 . [ 4 ] Albert Zeyer , Patrick Doetsch , Paul Voigtlaender , Ralf Schl¨uter , and Hermann Ney , “A comprehensive study of deep bidirectional LSTM RNNs for acoustic model - ing in speech recognition , ” CoRR , vol . abs / 1606 . 06871 , 2016 . [ 5 ] Christoph L¨uscher , Eugen Beck , Kazuki Irie , Markus Kitza , Wilfried Michel , Albert Zeyer , Ralf Schl¨uter , and Hermann Ney , “RWTH ASR systems for librispeech : Hybrid vs attention - w / o data augmentation , ” CoRR , vol . abs / 1905 . 03072 , 2019 . [ 6 ] O . Abdel - Hamid , A . Mohamed , H . Jiang , and G . Penn , “Applying convolutional neural networks concepts to hybrid NN - HMM model for speech recognition , ” in 2012 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , March 2012 , pp . 4277 – 4280 . [ 7 ] Ossama Abdel - Hamid , Li Deng , and Dong Yu , “Ex - ploring convolutional neural network structures and op - timization techniques for speech recognition , ” in Inter - speech 2013 . August 2013 , ISCA . [ 8 ] Kyu J . Han , Akshay Chandrashekaran , Jungsuk Kim , and Ian R . Lane , “The CAPIO 2017 conver - sational speech recognition system , ” CoRR , vol . abs / 1801 . 00059 , 2018 . [ 9 ] Dong Yu , Wayne Xiong , Jasha Droppo , Andreas Stol - cke , Guoli Ye , Jinyu Li , and Geoffrey Zweig , “Deep convolutional neural networks with layer - wise context expansion and attention , ” in Proc . Interspeech . Septem - ber 2016 , pp . 17 – 21 , ISCA - International Speech Com - munication Association . [ 10 ] George Saon , Gakuto Kurata , Tom Sercu , Kartik Au - dhkhasi , Samuel Thomas , Dimitrios Dimitriadis , Xi - aodong Cui , Bhuvana Ramabhadran , Michael Picheny , Lynn - Li Lim , Bergul Roomi , and Phil Hall , “English conversational telephone speech recognition by humans and machines , ” CoRR , vol . abs / 1703 . 02136 , 2017 . [ 11 ] Daniel S . Park , William Chan , Yu Zhang , Chung - Cheng Chiu , Barret Zoph , Ekin D . Cubuk , and Quoc V . Le , “SpecAugment : A simple data augmentation method for automatic speech recognition , ” 2019 . [ 12 ] Albert Zeyer , Parnia Bahar , Kazuki Irie , Ralf Schl¨uter , and Hermann Ney , “A comparison of transformer and LSTM encoder decoder models for ASR , ” in IEEE Au - tomatic Speech Recognition and Understanding Work - shop , Sentosa , Singapore , Dec . 2019 . [ 13 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun , “Deep residual learning for image recognition , ” CoRR , vol . abs / 1512 . 03385 , 2015 . [ 14 ] W . Xiong , L . Wu , Fil Alleva , Jasha Droppo , Xuedong Huang , and Andreas Stolcke , “The Microsoft 2017 conversational speech recognition system , ” CoRR , vol . abs / 1708 . 06073 , 2017 . [ 15 ] Sharan Chetlur , Cliff Woolley , Philippe Vandermersch , Jonathan Cohen , John Tran , Bryan Catanzaro , and Evan Shelhamer , “cuDNN : Efﬁcient primitives for deep learning , ” arXiv preprint arXiv : 1410 . 0759 , 2014 . [ 16 ] TensorFlow Development Team , “TensorFlow : Large - scale machine learning on heterogeneous systems , ” 2015 , Software available from tensorﬂow . org . [ 17 ] Tom Sercu and Vaibhava Goel , “Dense prediction on sequences with time - dilated convolutions for speech recognition , ” CoRR , vol . abs / 1611 . 09288 , 2016 . [ 18 ] Albert Zeyer , Tamer Alkhouli , and Hermann Ney , “RE - TURNN as a generic ﬂexible neural toolkit with appli - cation to translation and speech recognition , ” in An - nual Meeting of the Assoc . for Computational Linguis - tics , Melbourne , Australia , July 2018 . [ 19 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun , “Identity mappings in deep residual networks , ” CoRR , vol . abs / 1603 . 05027 , 2016 . [ 20 ] David Rybach , Stefan Hahn , Patrick Lehnen , David Nolden , Martin Sundermeyer , Zolt ´ an T ¨ uske , Simon Wiesler , Ralf Schl ¨ uter , and Hermann Ney , “RASR - the RWTH Aachen university open source speech recogni - tion toolkit , ” 12 2011 . [ 21 ] John J . Godfrey , Edward C . Holliman , and Jane Mc - Daniel , “SWITCHBOARD : Telephone speech cor - pus for research and development , ” in Proceedings of the 1992 IEEE International Conference on Acoustics , Speech and Signal Processing - Volume 1 , Washington , DC , USA , 1992 , ICASSP’92 , pp . 517 – 520 , IEEE Com - puter Society . [ 22 ] R . Schl ¨ uter , L . Bezrukov , H . Wagner , and Hermann Ney , “Gammatone features and feature combination for large vocabulary speech recognition , ” 05 2007 , vol . 4 , pp . IV – 649 . [ 23 ] Z . T ¨ uske , P . Golik , R . Schl ¨ uter , and H . Ney , “Speaker adaptive joint training of Gaussian mixture models and bottleneck features , ” in 2015 IEEE Workshop on Auto - matic Speech Recognition and Understanding ( ASRU ) , Dec 2015 , pp . 596 – 603 . [ 24 ] TensorFlow Development Team , “TensorFlow guide : Performance , ” https : / / www . tensorflow . org / guide / performance , 2019 , Accessed : 2019 - 10 - 03 . [ 25 ] TensorFlow Development Team , “TensorFlow : API , ” https : / / www . tensorflow . org / versions / r1 . 14 / api _ docs / python / tf / layers / BatchNormalization , 2019 , Accessed : 2019 - 10 - 03 . [ 26 ] TensorFlow Development Team , “Ofﬁcial ResNet source code , ” https : / / github . com / tensorflow / models / blob / master / official / r1 / resnet / resnet _ model . py , 2019 , Accessed : 2019 - 10 - 03 . [ 27 ] Tom Sercu and Vaibhava Goel , “Advances in very deep convolutional neural networks for LVCSR , ” CoRR , vol . abs / 1604 . 01792 , 2016 . [ 28 ] Timothy Dozat , “Incorporating Nesterov momentum into Adam , ” 2016 . [ 29 ] Gabriel Goh , “Why momentum really works , ” Distill , 2017 .