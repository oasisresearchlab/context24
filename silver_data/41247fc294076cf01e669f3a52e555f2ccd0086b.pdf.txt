" [ CHI2023 ] Decisions on paper 42 for CHI2023” : A Playful Twist on the Peer Review Process and Methodological Gaslighting Tessa Eagle University of California , Santa Cruz Santa Cruz , USA teagle @ ucsc . edu Lee Taber University of California , Santa Cruz Santa Cruz , USA ABSTRACT Peer review in research is like jury duty – no one wants to do it but everyone needs to in order for the community to continue to produce quality peer - reviewed work . However , there is no explicit incentive and little - to - no training for writing reviews . This is espe - cially difcult in interdisciplinary communities such as CHI , where quantitative and qualitative research are both common and reviews could be coming from someone with any area of expertise . In this piece , we give parody reviews for a quantitative paper as qualitative researcher reviewers . Many of the requests and commentary found within this piece are paraphrased from reviews we have received of our own qualitative research . Using this playful reversal , we pro - mote discussion around responsible review practices , the inclusion of more transparent research where bias , ethics , potential harm , and positionality are explicitly discussed across methodologies and epistemologies . CCS CONCEPTS • Human - centered computing → Human computer interac - tion ( HCI ) . KEYWORDS peer review , qualitative research ACM Reference Format : Tessa Eagle , Leya Breanna Baltaxe - Admony , Lee Taber , and Kathryn E . Ring - land . 2023 . " [ CHI2023 ] Decisions on paper 42 for CHI2023” : A Playful Twist on the Peer Review Process and Methodological Gaslighting . In Proceedings of Conference on Human Factors in Computing Systems ( CHI ’23 ) . ACM , New York , NY , USA , 8 pages . https : / / doi . org / 10 . 1145 / 3544549 . 3582745 1 INTRODUCTION As early career researchers , much of our time is spent thinking about publications , and , inevitably , peer review . Publishing at high - impact venues is a goal pushed upon us from the beginning of our Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specifc permission and / or a fee . Request permissions from permissions @ acm . org . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany © 2023 Association for Computing Machinery . ACM ISBN 978 - x - xxxx - xxxx - x / YY / MM . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3544549 . 3582745 Leya Breanna Baltaxe - Admony University of California , Santa Cruz Santa Cruz , USA Kathryn E . Ringland University of California , Santa Cruz Santa Cruz , USA research careers [ 18 ] . Like many others , we have come to dread receiving reviews back for our work , especially given that we are a group conducting primarily qualitative work that is often method - ologically misunderstood within HCI venues . This paper is a rever - sal of reviews we have received over the years , where a fctional quantitative submission is peer reviewed by qualitative researchers who fail to see the point of the work . Following our parody re - views , we include a refection on the peer review process and the content within each of the four reviews . This includes a discussion of sections typically found within qualitative work , but not yet as often within quantitative work ( e . g . , ethics , positionality , cita - tional justice ) . Our goal is not to widen the existing divide between methodological camps ( we all have our strengths and mix meth - ods ! ) , but to promote discussion around feedback that is being given and how we can be more intentional with our reviewing practices . These issues are not limited to qualitative researchers , but we write from our recent experiences with qualitative submissions . 2 THE REVIEWS Re : CHI 2023 submission 42 - A Machine Learning Approach to Analyzing Social Media Big Data Dear author , We are notifying you that your submission was recommended for Major Revision . The reviews can be found via the Precision Conference System . A Playful Twist on the Peer Review Process CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Submissions Reviews Chairing Account sign out Reviews of 42 - " A Machine Learning Approach to Analyzing Big Data " Reviewer 4 ( 1AC ) Expertise : Expert Originality : Medium Originality Signifcance : Medium Signifcance Rigor : Low Rigor 1AC Recommendation : We recommend Revise and Resubmit 3 CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Eagle et al . 1AC The Meta Review : This paper does machine learning . Thank you to the authors for leading us through a passionate discussion . This paper was discussed heavily at the PC meeting and we decided to bring in R3 to add to the discussion . The major concerns are summarized below : ( 1 ) None of us could agree on what this paper was about or exactly what method was used ( 2 ) As many reviewers noted , this method ( whatever it is ) may be unfamiliar to the larger CHI community , thus we encourage you to add a detailed explanation of the method preferably including the basic fundamentals , keeping in mind our length requirements of under 10 pages . ( 3 ) Missing quite a bit of related work ( R2 ) and discussion of ethical data use ( R2 ) ( 4 ) Fix grammar and formatting throughout ( R3 ) [ Editor’s Note : thanks R3 for the thorough review . . . ] Please let us know if you have any questions . In your response , we expect you to ingratiate us as much as possible , even though it will do little to change our minds : ) Also , please note that despite the popularity of ChatGPT , these reviews are , in fact , written by humans . . . I think . I have no way of verifying that . Reviewer 1 ( reviewer ) Expertise : Knowledgeable Originality : High Originality Signifcance : Medium Signifcance Rigor : Medium Rigor 1AC Recommendation : Revise and Resubmit or Reject Review : 4 A Playful Twist on the Peer Review Process CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hello fellow researchers ! From my reading of this paper , it looks like it is about using machine learning to look at social media data . This work is very exciting and I think the quantitative results are clearly presented despite being difcult to understand for a less machine learning oriented researcher . I’m not exactly sure about this , but I had a hard time understanding what your results mean in the broader context of social media . If I understand the method correctly , you gathered 10 million posts from The Social Media Platform™ , classifed half of the posts as Arbitrarily Important Research Category or Another Arbitrarily Important Research Category , and then used that to train a machine learning algorithm to detect Arbitrarily Important Research Category or . . . Another Arbitrarily Important Research Category ? While I think this is technically skillful and uses a lot of computation , I don’t understand the beneft of this research . Also , it seems like this research has drawbacks for the communities that it is being trained on . I would have liked to see you work directly with community members and survey them on whether they consider themselves Arbitrarily Important Research Category or Another Arbitrarily Important Research Category . Please also include the Arbitrarily Important Research Category and Another Arbitrarily Important Research Category levels of each author in the positionality statement as not including this may infuence the reception of the work . I’m not terribly familiar with the body of machine learning research , but isn’t there some way to express how your study or system could impact the community it is drawn from ? If it’s ok , I’d like the authors to include a large section that addresses how this could potentially impact the people the data is drawn from and how this system could be misused . I do know that no work can be conducted in this space without citing Highly Esteemed Author’s Work 1 and this primer on HCI ( Haag , M . " Hyper - Converged Infrastructures for DUMMIES . [ Sl ] . " ( 2016 ) . ) . I think this should be a major revision based on how I understand this process . * Note that while REDACTED marked himself as “Expert” due to his tenure and extensive CHI publication history , this review has been pawned of on his frst - year grad student with no context or training . Reviewer 2 ( reviewer ) Expertise : Expert Originality : Medium Originality Signifcance : Low Signifcance 1 Editor’s Note : This was edited to remove identifying information by the reviewer citing his own work 5 CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Eagle et al . Rigor : Low Rigor 1AC Recommendation : We recommend Revise and Resubmit Review : This is a paper about the application of machine learning to social media data . This has a lot of potential , but I feel there are a few key ways to strengthen the work . I’d like to remind the authors that CHI does stand for Computer HUMAN Interaction , thus we are left wondering where the humans are in this work beyond being unsolicited data generators . Additionally , how did you address the bias present in computers ? As computer users we are biased when studying computers , how does being a member of the computer - using community afect your role in this work ? Wouldn’t your bias towards using computers keep you from making objective decisions about your code ? We see you’ve used “supervised machine learning” here , did you consider instead doing thematic analysis or qualitative coding ( See - Cole , Tom , and Marco Gillies . “More than a bit of coding : ( un - ) Grounded ( non - ) Theory in HCI . ” In CHI Conference on Human Factors in Computing Systems Extended Abstracts , pp . 1 - 11 . 2022 . for more on qual work at CHI 2 [ 3 ] ) ? Citing other work , which includes a wide span of what might fall under the umbrella of “machine learning” doesn’t help me understand what this paper in particular did , the details of the methodology used , or enable me to ascertain whether the methods were pursued rigorously . I think part of this is that machine learning as a method is newer to me and likely to the other reviewers for CHI . Please elaborate on what makes for rigorous use of this method and prior instances at CHI . Given that you collected data from The Social Media Community™ , there was no discussion of how you will present this work back to the community , and how this work benefts them and their goals . In addition , you have used a number of hashtags for fnding The Social Media Community in your study . Can you clarify in your methods section how you knew that The Social Media Community is found via hashtag ? We know from other community - based work that there are a number of diferent ways community members identify themselves and each other in online platforms . Given that hashtags are just one method of identifcation , we know this is not always the most accurate . Can you please elaborate in your methods section how you knew who were members of The Social Media Community ? Refer to the following for equitable community - based work : • Harrington , Christina , Sheena Erete , and Anne Marie Piper . “Deconstructing community - based collaborative design : Towards more equitable participatory design engagements . ” Proceedings of the ACM on Human - Computer Interaction 3 , no . CSCW ( 2019 ) : 1 - 25 . [ 7 ] • Minkler , Meredith . “Ethical challenges for the “outside” researcher in community - based participatory research . ” Health Education Behavior 31 , no . 6 ( 2004 ) : 684 - 697 . [ 13 ] 2 I’ve cited one arbitrary and difcult thing instead of acknowledging there are thousands of ways to do this 6 A Playful Twist on the Peer Review Process CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany • Bondi , Elizabeth , Lily Xu , Diana Acosta - Navas , and Jackson A . Killian . “Envisioning communities : a participatory approach towards AI for social good . ” In Proceedings of the 2021 AAAI / ACM Conference on AI , Ethics , and Society , pp . 425 - 436 . 2021 . [ 1 ] There is a critical lack of engagement with responsible ML and AI literature in the Related Work and Discussion sections . Perhaps instead of suggesting how this work could be applied , consider adding a discussion of what should not come out of this work and into the world . • Denton , Emily , Alex Hanna , Razvan Amironesei , Andrew Smart , and Hilary Nicole . “On the genealogy of machine learning datasets : A critical history of ImageNet . ” Big Data Society 8 , no . 2 ( 2021 ) : 20539517211035955 . [ 5 ] • Zwitter , Andrej . “Big data ethics . ” Big Data Society 1 , no . 2 ( 2014 ) : 2053951714559253 . [ 20 ] Positionality While positionality statements have become increasingly popular in qualitative research , this has not been extended as often within quantitative studies . Positioning ourselves in relation to our work is important not only for readers but also as a practice in evaluating our identities and ties to our topics that infuence the ways we work and relate to our domains . Refer to the following for a great discussion of positionality and refexivity in HCI work : • Liang , Calvin A . , Sean A . Munson , and Julie A . Kientz . “Embracing four tensions in human - computer interaction research with marginalized people . ” ACM Transactions on Computer - Human Interaction ( TOCHI ) 28 . 2 ( 2021 ) : 1 - 47 . [ 12 ] We request that the authors include in their positionality statement a discussion of how they are taking into account their own bias . In what ways are the authors accounting for their own bias towards The Social Media Community™ ? It is well known that all researchers have bias . Machine learning has a history of ignoring the bias present in data , algorithms , and coders ( e . g . , Leavy , Susan . “Gender bias in artifcial intelligence : The need for diversity and gender theory in machine learning . ” In Proceedings of the 1st international workshop on gender equality in software engineering , pp . 14 - 16 . 2018 . [ 11 ] ) . There are some descriptive parts of the paper that seem overly deferential towards machine learning as a methodology . Please provide a clear rationale of why machine learning was necessary for this work , as well as evidence that no other method would have been better . Additionally , your results on gender diferences are not further discussed or situated within / against other HCI work on gender ( See : Stumpf , Simone , Anicia Peters , Shaowen Bardzell , Margaret Burnett , Daniela Busse , Jessica Cauchard , and Elizabeth Churchill . “Gender - inclusive HCI research and design : A conceptual review . ” Foundations and Trends® in Human – Computer Interaction 13 , no . 1 ( 2020 ) : 1 - 69 . [ 16 ] ) Ethical data use When working with social media data , it is crucial to consider the ethics of data use and potential negative consequences that could result from using such data . Researchers in this area have outlined the importance of careful data handling . Despite these posts being public and often not requiring IRB approval for collection and analysis , there are real people behind the data that need to be accounted for . We recommend the following articles as primers on this topic : • Fiesler , Casey , and Nicholas Proferes . ““Participant” perceptions of Twitter research ethics . ” Social Media + Society 4 . 1 ( 2018 ) : 2056305118763366 . [ 6 ] 7 CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Eagle et al . • Chancellor , Stevie , Michael L . Birnbaum , Eric D . Caine , Vincent MB Silenzio , and Munmun De Choudhury . “A taxonomy of ethical tensions in inferring mental health states from social media . ” In Proceedings of the conference on fairness , accountability , and transparency , pp . 79 - 88 . 2019 . [ 2 ] • Sbragaglia , Valerio , Ricardo A . Correia , and Enrico Di Minin . “Responsible use of social media data is needed : A reply to Maya - Jariego et al . “Plenty of black money : Netnography of illegal recreational underwater fshing in southern Spain” . ” Marine Policy 134 ( 2021 ) : 104780 . [ 14 ] • Jena , Millena Debaprada , Sunil Samanta Singhar , Bhabendu Kumar Mohanta , and Somula Ramasubbareddy . “Ensuring data privacy using machine learning for responsible data science . ” In Intelligent Data Engineering and Analytics , pp . 507 - 514 . Springer , Singapore , 2021 . [ 8 ] • Sloane , Mona , Emanuel Moss , Olaitan Awomolo , and Laura Forlano . “Participation is not a design fx for machine learning . ” arXiv preprint arXiv : 2007 . 02423 ( 2020 ) . [ 15 ] Citational Diversity While CHI revolves around human computer interaction , there are vastly diferent research agendas across the community . This , in turn , can lead to research being siloed , and despite potential crossover , some relevant work may be missed . Work on ethical AI / ML may be conducted by researchers who don’t actively code and develop algorithms and models . This work is still important to read and cite . It is also important to broaden our view of what work is citable—so much knowledge is disseminated through non - academic avenues despite what academia values most [ 18 ] and this is especially true for content coming directly from community members via videos , blogs , zines , etc . Further reading on this topic : • Kumar , Neha , and Naveena Karusala . “Braving citational justice in human - computer interaction . ” In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems , pp . 1 - 9 . 2021 . [ 9 ] • Vossen , Emma . Twitter Post . December 3 , 2019 . https : / / twitter . com / emmahvossen / status / 1201976313583734785 . [ 19 ] We urge the authors to push back upon capitalist - driven research methods ! CHI is a social - justice driven conference ( we discussed a manifesto ! 3 ) . What are the potential negative consequences of putting this work out into the world ? Other factors not afecting my score : • I noticed the alt text on your fgures is the computer - generated generic text ( see Fig 1 : “Bar chart” ) • Several typos and grammatical issues , I would recommend investing in a professional copy - editor to look over the entire work before resubmission ( for example , The Social Media Community is spelled incorrectly on pages 3 , 5 , and 15 ) Overall , I think this paper is leaning towards artifcial intelligence or machine learning sciences too much and the methodology of the work is not sound enough for a full CHI paper . For that and the above reasons , I recommend to Reject the paper . Reviewer 3 ( reviewer ) Expertise : 3 https : / / programs . sigchi . org / chi / 2022 / index / session / 81226 8 A Playful Twist on the Peer Review Process CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Knowledgeable Originality : Medium Originality Signifcance : High Signifcance Rigor : Medium Rigor 1AC Recommendation : Minor Revisions Review : Thanks to the authors for their submission . I was added to this submission at the last minute . Please use complete sentences throughout and correct all tpos [ sic ] . I’ve marked myself as a very confdent reviewer for these even though I took one stats class and one comp sci course in undergrad that I haven’t made use of since . I haven’t taken statistics since college but something about your choice of tests seems of here . Please spell out “machine learning” instead of using “ML” . It’s not long and is easier for people unfamiliar with the concept . I would be interested to hear the authors take on what makes good machine learning work and how this work achieves that . Otherwise , I am excited to cite this paper one day soon . For this , I recommend the paper be accepted with minor revisions . 9 CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Eagle et al . 3 REFLECTION 3 . 1 Positionality and Methodology Clashes This paper is the result of many discussions and frustrations with the peer review process within the feld of HCI . As primarily qualita - tive researchers , we have found our work to be taken less seriously and often misunderstood by reviewers who have little experience with our chosen methods . We are deeply invested in the communi - ties we work with , and take careful eforts to protect the safety of the community and accurately represent them . Intentional work , especially qualitative , is slow work that takes longer than some people may like 4 . To this end , it can be extremely demoralizing to receive reviews from researchers that have a fundamental mis - understanding of our methodological approaches and to have to extensively justify why our work is “CHI” enough . For example , authors of this piece have previously received feedback from a re - viewer confused why we did not use statistics or machine learning on a qualitative , community - based paper . For those who are early in their careers or are new to the feld , this sort of inefective feedback can be detrimental to the longevity of people in our community . Our lab is made up of and designed as an inclusive space for disabled researchers and participants . We are a team of interdisci - plinary researchers and designers focused on care and access . Many of us are working to unravel preconceived notions about academia , the feld of Human Computer Interaction ( HCI ) , internal and exter - nal ableism , and research in general , so we incorporate discussions of positionality and understandings of our own privileged identities often in our work . The qualitative and community - based work we engage in requires thoughtful consideration of positionality and addressing the biases we bring into our work 5 . We engage in slow work that centers the experiences of the communities we work with—ones we are deeply concerned with the safety and correct characterization of . Ongoing marginalization in academia leads to more work for the marginalized 6 . Who are the ones leading Diver - sity , Equity , and Inclusion eforts ? It often falls on people of color 7 . Access work is undertaken by disabled individuals that have to fght to be considered . The people fghting to change problematic systems are the ones being hurt by those systems , whereas others who beneft have the privilege of not thinking about these processes and remaining complacent . We understand the necessity of peer review and the difculty in overhauling an ingrained process , we are merely hoping to push for kinder—while still critical and mean - ingful to create better research—reviews conducted by appropriate reviewers . As people that have been on both sides of the review process , we appreciate the difculty faced by ACs in recruiting enough qualifed , motivated reviewers . 4 For further reading , see : Lau , Travis Chi Wing . “Slowness , disability , and academic productivity : The need to rethink academic culture . ” Disability at the University : A Disabled Students’ Manifesto . New York , NY : Peter Lang 209 ( 2019 ) . [ 10 ] 5 https : / / medium . com / misftlabs / creating - a - lab - with - a - culture - of - care - 2b19bb0b2a22 6 See : https : / / disabilityvisibilityproject . com / 2019 / 06 / 03 / cripping - emotional - labor - a - feld - guide / 7 https : / / www . bbc . com / worklife / article / 20200826 - how - corporate - diversity - initiatives - trap - workers - of - colour 3 . 2 Early - Career Research and Community Values Early - stage academics , in particular , can be most harmed by not having access to appropriate reviewers . Many academics , particu - larly those early in their careers where every publication “counts , ” seem to dread getting paper reviews back , as they are often written in unhelpful or unkind ways . Multiple labs we have worked with have developed group rituals for dealing with the emotional work of parsing overly critical reviews , which are difcult for graduate students both emotionally and pragmatically for their careers . This sometimes harsh process can dissuade people from pursuing or continuing work in higher education . One reason people might be giving bad reviews is that there is poor documentation on what good reviewing actually is . Although there are some ofcial resources that people can use to build their reviews , identifying useful information can be difcult . For example , while writing this piece , we came across an excellent resource 8 that we were sad not to have seen before . The ACM has a brief overview of review guidelines 9 , but this does not fully go into the process of how to review a paper ( which , of course , difers from person to person ) . Community members are doing work to improve this process [ 17 ] , but the resources must be sought out by those wanting to provide better reviews . This theme of optional extra labor is prevalent in academia , where service work that benefts others and entities beyond the self ( e . g . , mentorship , eforts to promote diversity , equity , and inclusion ) is not considered as worthwhile as research work . There is little to no incentive to review papers , other than your papers being reviewed in return , thus there is no tangible reason to provide thor - ough , kind , and intentional reviews beyond being a good person who wants to improve the work in our feld . Reviewers are under - standably burnt out and inundated with review requests—despite agreeing to review a certain number of papers upon submission , not everyone does so—we must consider how to better distribute the load of peer - review . Perhaps one way to approach this is to recenter these eforts as community eforts that help everyone . As academics , we each bring to the community our own expe - riences and expertise in certain topics and methods . We feel it is important to self - refect on our own knowledge with an assigned pa - per topic . While preparing to write this piece , we found ourselves asking some questions about reviewers and the review process . What makes one reviewer an “expert” versus “very knowledge - able” ? And what are they an “expert” in ? Is it the research area , the method , or the theory ? Or just their position at their institution ? Being an expert in a research area does not automatically make one an expert in a given method used in said topic . This is espe - cially true given the huge breadth of topics that live within the HCI community . Knowing how to navigate the review process as an author comes with a largely hidden curriculum . Authors are required to select the correct tracks at larger venues such as CHI , for example . Tracks often end up siloing diferent types of research , which is potentially harmful for certain types of research and researchers . For example , 8 https : / / docs . google . com / document / d / 1MpH3zV8WFU5avQHqrVTtxu - wNWzeTstxWy7K _ _ HK5Nc / edit ? usp = sharing 9 https : / / dl . acm . org / journal / dgov / reviewer - guidelines A Playful Twist on the Peer Review Process CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany all disability work tends to be relegated to “accessibility” tracks regardless of methods used , leading to sessions that are made up of topically and methodologically unrelated papers aside from their relation to the populations they work with . Further , authors have to learn how to “game” the review system in order to get the “right” kind of reviewers—that is , reviewers who will understand the topic area , methods used , and theories engaged with . Authors without this “inside” knowledge are seriously disadvantaged when it comes to getting efective , informative , refective reviews about their work . 3 . 3 Systemic Issues Our reviewing process is broken at a systemic level . Everyone , at every level of the review process , is overtaxed and exhausted . We recognize that ACs are overburdened . However , it is not ideal to let through reviews that contain demeaning phrasing or completely miss the point of a submission . It is common to see critical reviews without substantive feedback . The results are similar to that of “weed out” classes in engineering . Without guidance on overcoming overly critical reviews that focus on the wrong parts of the work , early career researchers are pushed out of the feld . This is especially true for groups that have been systematically excluded and are already underrepresented in the feld . This efect is compounded when a reviewer who is not invested in the review process fails to engage with a paper enough to understand what the work is about ( have you ever felt like the reviewer did not read the same paper that you wrote ? ) . Reviews can be critical and hold work to a high standard , but this only works if the reviewer understands the work being presented and has a clear fundamental knowledge of the methods . Reviewing often falls onto a graduate advisor to mentor and teach students . However , as with much of grad school , reviewing is a skill that is learned through getting thrown into the fre and frantically researching how to review on one’s own . While there do exist CHI review “how - to” guides 10 , they are neither manda - tory nor ofcially sanctioned by CHI . Researchers may receive no training whatsoever before being expected to carry out a review . Additionally , the process can difer vastly across ACM venues and within a venue based on submission type , which may not be clear to frst - time reviewers . For example , reviewing a workshop pro - posal is not anonymous , you can see the other reviewers’ responses , and there is a discussion board . Whereas for a full paper , some go through a rebuttal process ( e . g . , CHI , HRI , ASSETS ) , while others go through revisions ( e . g . , CSCW , journals ) . Because reviewing is a community endeavor , there is a certain amount of social capital that is involved in the reviewing process . Younger academics may feel pressure or guilt to accept a review request despite knowing little about the topic . Especially if an AC or editor is in dire need of a reviewer . These scenarios are common— while papers are used as a measure of success in our careers , there are no institutional incentives around reviewing . Finally , as part of the systemic issues with the peer - review pro - cess , the number of reviewers available for any given paper is im - pacted by conficts of interest . In smaller sub - communities , where only a handful of people may have domain expertise , researchers 10 e . g . , https : / / docs . google . com / document / d / 1MpH3zV8WFU5avQHqrVTtxu - wNWzeTstxWy7K _ _ HK5Nc / edit ? usp = sharing are often left with limited options . For example , researchers have to make a decision not to work with a potential collaborator or mentor in order to have more people to review future publications . This disproportionately afects budding or marginalized communities within CHI , where the pool of potential mentors and collaborators is often more tenuous and limited . Overall , the current review process has the potential to limit the scope of work within CHI as a whole—leaving researchers with the decision of doing better , more collaborative work or tailoring their work towards what is more likely to be published . This piece was intended to be educational—giving a brief introduction to discus - sion of ethics , positionality , and community - based work as they have appeared in our own reviews , as well as broader academic discussions , ( sections that belong in all work , not solely qualitative research ) —and refective in that we hope to stimulate discussion around the peer review process and double standards for qualita - tive work . We all exist within the structure of academic or industry research communities and must deal with systemic issues that are unlikely to be resolved without a large - scale reframing or restruc - turing of our power structures . However , we remain optimistic that the more we discuss and refect on the ways to make the peer review process better for everyone , that there will be a future where the process is less painful for all of us . 3 . 4 Calls to Action ( from our reviewers ) 11 Our goal is to foster conversation and bring some issues to light . We cannot solve this alone , and hope to come together with the broader CHI community to collaboratively dream a revised review process . We hope the video review during our Alt . CHI time can start in - person discussion on next steps . Our reviews were a great start in this direction , and we share some of those here . Review 13 on this manuscript , which we hope will be appended as commentary to this paper , was written collaboratively by fve fellow CHI researchers . They ofer two thoughtful suggestions for moving forward with this topic that we suggest exploring : 1 . Mapping out common reviewing pitfalls and 2 . Taking inspiration from other domains’ venues that utilize alternative peer - reviewing processes . Some examples of other domains’ venues were put forward by diferent reviewers , including : • Nature Communication’s transparent review process , which allows paper authors to have comments they receive pub - lished with the fnal work [ 4 ] ( Reviewer 9 ) • OpenReview ( https : / / openreview . net / about ) , which hosts accepted papers ( including reviews an comments ) as well as a discussion forum for continuing conversations ( Reviewer 9 ) • eLife Science’s model which requires preprints frst , followed by reviews , then published reviewed preprints , and fnally , authors can choose to have their reviewed preprints pub - lished as regular journal articles ( Reviewers 9 and 13 ) While we speak from a qualitative and mixed - method lends , Re - viewer 8 mentioned that quantitative researchers are also struggling with quality reviewing for verifying statistical rigor . However , the collaborative review rightfully calls for a rise above such division 11 This section has been added following our reading of each review we received CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany while preventing the “ misapplication of criteria from one method - ological approach to another” . Finally , Reviewer 17 notes how , as a junior researcher , being able to read examples of real paper reviews ( as is the case with some alt . chi work where reviewer can opt to have their commentary shared within the fnal publication ) has been benefcial to them . ACKNOWLEDGMENTS To all the reviewers that have missed the point and those that have been assigned work outside of their scope . And to all early researchers , don’t let the reviews get you down ! We would also like to thank all of the alt . chi reviewers that took time to engage with our work and thoughtfully considered the current and future status of peer review . REFERENCES [ 1 ] Elizabeth Bondi , Lily Xu , Diana Acosta - Navas , and Jackson A Killian . Envisioning communities : a participatory approach towards ai for social good . In Proceedings of the 2021 AAAI / ACM Conference on AI , Ethics , and Society , pages 425 – 436 , 2021 . [ 2 ] Stevie Chancellor , Michael L Birnbaum , Eric D Caine , Vincent MB Silenzio , and Munmun De Choudhury . A taxonomy of ethical tensions in inferring mental health states from social media . In Proceedings of the conference on fairness , accountability , and transparency , pages 79 – 88 , 2019 . [ 3 ] Tom Cole and Marco Gillies . More than a bit of coding : ( un - ) grounded ( non - ) theory in hci . In CHI Conference on Human Factors in Computing Systems Extended Abstracts , pages 1 – 11 , 2022 . [ 4 ] Nature Communications . Transparent peer review for all . Nature Communica - tions , 13 ( 11 ) : 6173 , Oct 2022 . [ 5 ] Emily Denton , Alex Hanna , Razvan Amironesei , Andrew Smart , and Hilary Nicole . On the genealogy of machine learning datasets : A critical history of imagenet . Big Data & Society , 8 ( 2 ) : 20539517211035955 , 2021 . [ 6 ] Casey Fiesler and Nicholas Proferes . “participant” perceptions of twitter research ethics . Social Media + Society , 4 ( 1 ) : 2056305118763366 , 2018 . Eagle et al . [ 7 ] Christina Harrington , Sheena Erete , and Anne Marie Piper . Deconstructing community - based collaborative design : Towards more equitable participatory design engagements . Proceedings of the ACM on Human - Computer Interaction , 3 ( CSCW ) : 1 – 25 , 2019 . [ 8 ] Millena Debaprada Jena , Sunil Samanta Singhar , Bhabendu Kumar Mohanta , and Somula Ramasubbareddy . Ensuring data privacy using machine learning for responsible data science . In Intelligent Data Engineering and Analytics , pages 507 – 514 . Springer , 2021 . [ 9 ] Neha Kumar and Naveena Karusala . Braving citational justice in human - computer interaction . In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems , pages 1 – 9 , 2021 . [ 10 ] Travis Chi Wing Lau . Slowness , disability , and academic productivity : The need to rethink academic culture . Disability at the University : A Disabled Students’ Manifesto . New York , NY : Peter Lang , 209 , 2019 . [ 11 ] Susan Leavy . Gender bias in artifcial intelligence : The need for diversity and gender theory in machine learning . In Proceedings of the 1st international workshop on gender equality in software engineering , pages 14 – 16 , 2018 . [ 12 ] Calvin A Liang , Sean A Munson , and Julie A Kientz . Embracing four tensions in human - computer interaction research with marginalized people . ACM Transac - tions on Computer - Human Interaction ( TOCHI ) , 28 ( 2 ) : 1 – 47 , 2021 . [ 13 ] Meredith Minkler . Ethical challenges for the “outside” researcher in community - based participatory research . Health Education & Behavior , 31 ( 6 ) : 684 – 697 , 2004 . [ 14 ] Valerio Sbragaglia , Ricardo A Correia , and Enrico Di Minin . Responsible use of social media data is needed : A reply to maya - jariego et al . “plenty of black money : Netnography of illegal recreational underwater fshing in southern spain” . Marine Policy , 134 : 104780 , 2021 . [ 15 ] Mona Sloane , Emanuel Moss , Olaitan Awomolo , and Laura Forlano . Participation is not a design fx for machine learning . arXiv preprint arXiv : 2007 . 02423 , 2020 . [ 16 ] Simone Stumpf , Anicia Peters , Shaowen Bardzell , Margaret Burnett , Daniela Busse , Jessica Cauchard , Elizabeth Churchill , et al . Gender - inclusive hci research and design : A conceptual review . Foundations and Trends® in Human – Computer Interaction , 13 ( 1 ) : 1 – 69 , 2020 . [ 17 ] Miriam Sturdee , Joseph Lindley , Conor Linehan , Chris Elsden , Neha Kumar , Tawanna Dillahunt , Regan Mandryk , and John Vines . Consequences , schmon - sequences ! considering the future as part of publication and peer review in computing research . In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems , pages 1 – 4 , 2021 . [ 18 ] Emma Vossen . Publish and perish : on publishing , precarity and poverty in academia . Journal of Working - Class Studies , 2 ( 2 ) : 121 – 135 , 2017 . [ 19 ] Emma Vossen . Tweet , December 2019 . [ 20 ] Andrej Zwitter . Big data ethics . Big Data & Society , 1 ( 2 ) : 2053951714559253 , 2014 .