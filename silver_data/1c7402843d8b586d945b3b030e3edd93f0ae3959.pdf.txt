On the Creativity of Large Language Models Giorgio Franceschelli 1 and Mirco Musolesi 2 , 1 1 University of Bologna , Italy 2 University College London , United Kingdom giorgio . franceschelli @ unibo . it , m . musolesi @ ucl . ac . uk Abstract Large Language Models ( LLMs ) are revolutionizing several areas of Artiﬁcial In - telligence . One of the most remarkable applications is creative writing , e . g . , poetry or storytelling : the generated outputs are often of astonishing quality . However , a natural question arises : can LLMs be really considered creative ? In this article we ﬁrstly analyze the development of LLMs under the lens of creativity theories , investi - gating the key open questions and challenges . In particular , we focus our discussion around the dimensions of value , novelty and surprise as proposed by Margaret Boden in her work . Then , we consider different classic perspectives , namely product , pro - cess , press and person . We discuss a set of “easy” and “hard” problems in machine creativity , presenting them in relation to LLMs . Finally , we examine the societal im - pact of these technologies with a particular focus on the creative industries , analyzing the opportunities offered by them , the challenges arising by them and the potential associated risks , from both legal and ethical points of view . Keywords : Large Language Models ; Machine Creativity ; Generative Artiﬁcial Intelligence ; Foundation Models 1 Introduction Language plays a vital role in how we think , communicate , and interact with others 1 . It is therefore of no surprise that natural language generation has always been one of the prominent branches of artiﬁcial intelligence ( Jurafsky and Martin , 2023 ) . We have witnessed a very fast acceleration of the pace of development in the past decade culminated with the invention of transformers ( Vaswani et al . , 2017 ) . The possibility of exploiting large - scale data sets and the availability of increasing computing capacity has led to the deﬁnition of the so - called foundation models , which are able to achieve state - of - the - art performance in a variety of tasks ( Bommasani et al . , 2021 ) . 1 As remarked by ChatGPT itself when asked about the importance of language . 1 a r X i v : 2304 . 00008v3 [ c s . A I ] 9 J u l 2023 Among them , large language models ( LLMs ) are indeed one of the most interesting devel - opments . They have captivated the imagination of millions of people , also thanks to a series of entertaining demonstrations and open tools released to the public . The examples are many from journal articles 2 to culinary recipes ( Lee et al . , 2020 ) and university - level essays 3 . LLMs have also been used to write papers about themselves writing papers ( GPT - 3 et al . , 2022 ) . They are com - monly used for creative tasks like poetry or storytelling and the results are often remarkable 4 . Notwithstanding , it is not obvious whether these “machines” are truly creative , at least in the sense originally discussed by Ada Lovelace ( Menabrea and Lovelace , 1843 ) . LLMs have already been analyzed ( and sometimes criticized ) from different perspectives , e . g . , fairness ( Bender et al . , 2021 ) , concept understanding ( Bender and Koller , 2020 ) , societal impact ( Tamkin et al . , 2021 ) , and anthropomorphism ( Shanahan , 2022 ) just to name a few . However , a critical question has not been considered yet : can LLMs be considered creative ? By taking into account classic frameworks for analyzing creativity , such as Boden’s three crite - ria ( Boden , 2003 ) and other prominent cognitive science and philosophical theories ( e . g . , Amabile ( 1983 ) ; Csikszentmihalyi ( 1988 ) ; Gaut ( 2010 ) ) , we will try to answer this question . We will discuss the dimensions according to which we believe LLMs should be analyzed in order to be able to evaluate their level of machine creativity . To the best of our knowledge , this article represents one of the ﬁrst investigations of the problem of LLM creativity from a theoretical and philosophical perspective . The remainder of the paper is structured as follows . First , we brieﬂy review the past de - velopments in automatic text generation and artiﬁcial creativity ( Section 2 ) that led to today’s LLMs . Then , we analyze LLMs from the perspective of Boden’s three criteria ( Section 3 ) , as well as considering other relevant philosophical theories ( Section 4 ) . Finally , we discuss the practical implications of LLMs for the arts , creative industries , design and , more in general , scientiﬁc and philosophical enquiry ( Section 5 ) . Section 6 concludes the paper , outlining the open challenges and a research agenda for the future years . 2 A Creative Journey from Ada Lovelace to Foundation Models It was the year 1843 when Ada Lovelace wrote that the Analytical Engine ( Babbage , 1864 ) “has no pretensions to originate anything . It can do whatever we know how to order it to perform” ( Menabrea and Lovelace , 1843 ) . This statement has then been deﬁned as “Lovelace’s objection” by Alan Turing , who also provided an alternative formulation : a machine can never “take us by surprise ” ( Tur - ing , 1950 ) . This was just the beginning of an ongoing philosophical discussion , which has often included psychological elements , around human creativity ( Barron , 1955 ; Berlyne , 1960 ; Bruner , 1962 ; Newell et al . , 1962 ; Stein , 1974 ) , as well as computational creativity ( Macedo et al . , 2004 ; Wiggins , 2006 ; Jordanous , 2009 ; Boden , 2009 ; Maher , 2010 ; Colton and Wiggins , 2012 ) . In general , computer scientists have always been fascinated by the possibility of building ma - chines able to express themselves through writing , e . g . , by composing poems and short stories , creating paintings , and so on . In particular , the rise of automatic text generation was contextual to 2 www . theguardian . com / commentisfree / 2020 / sep / 08 / robot - wrote - this - article - gpt - 3 3 https : / / www . theguardian . com / technology / 2022 / dec / 04 / ai - bot - chatgpt - stuns - academics - with - essay - writing - skills - and - usability 4 see , for instance : https : / / www . gwern . net / GPT - 3 2 the birth of personal computers . Examples include the Computerized Haiku by Margaret Master - man 5 , the storyteller TALE - SPIN ( Meehan , 1977 ) , Racter and its poems’ book ( Racter , 1984 ) , and UNIVERSE , which was able to generate coherent and consistent characters ( Lebowitz , 1983 ) , just to name a few . Different techniques have been explored , from planning ( e . g . , Riedl and Young ( 2010 ) ) and case - based reasoning ( e . g . , Turner ( 1994 ) ) to evolutionary strategies ( e . g . , Manurung et al . ( 2012 ) ) . Some approaches combine all of them together ( Gerv´as , 2013 ) . Only with the advent of neural networks and learning systems , we observed a real step - change . In particular , deep language models , i . e . , probabilistic models of in - context token occur - rences trained on a corpus of text with deep learning , easily allow to sample new text , facilitating and automating natural language generation . For instance , recurrent neural networks with long - short term memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) or gated - recurrent units ( GRUs ) ( Cho et al . , 2014 ) can predict next character ( Karpathy , 2015 ) , word ( Potash et al . , 2015 ) , syllables ( Zugarini et al . , 2019 ) , or events ( Martin et al . , 2018 ) given previous ones , allowing to compose text that spans from short movie scripts to knock - knock jokes ( Miller , 2019 ) . Other successful genera - tive methods include generative adversarial networks ( GANs ) ( Yu et al . , 2017 ; Zhang et al . , 2017 ) and variational auto - encoders ( VAEs ) ( Bowman et al . , 2016 ; Semeniuta et al . , 2017 ) . We refer the interested reader to ( Franceschelli and Musolesi , 2021 ) for an in - depth survey of deep learning techniques applied to creative artifacts . These models tend to scale poorly to long sequences , and they are often not able to capture the entire context . For this reason , current state - of - the - art language models make use of attention ( Bahdanau et al . , 2015 ) and transformers ( Vaswani et al . , 2017 ) . In the recent years , several models based on these mechanisms have been proposed . They usually rely on a very large number of parameters and trained on corpus datasets of greater and greater size ( Devlin et al . , 2019 ; Rad - ford et al . , 2019 ; Shoeybi et al . , 2019 ; Brown et al . , 2020 ; Raffel et al . , 2020 ; Rosset , 2020 ; Rae et al . , 2021 ; Chowdhery et al . , 2022 ; Du et al . , 2022 ; Hoffmann et al . , 2022 ; Smith et al . , 2022 ; Thoppilan et al . , 2022 ) . Although such models can be used with zero - shot or few - shot learning to produce poems and stories ( Swanson et al . , 2021 ) , the problem of adaptation has been central to their devel - opment . LLMs can involve re - training by means of plug - and - play attribute classiﬁers ( Dathathri et al . , 2020 ) ; re - training to produce paragraphs coherent with a given outline ( Rashkin et al . , 2020 ) ; ﬁne - tuning with speciﬁc corpora for writing speciﬁc text ( Sawicki et al . , 2022 ; Wertz and Kuhn , 2022 ) ; or ﬁne - tuning to maximize human preferences ( Ziegler et al . , 2019 ) or to generate speciﬁc literary outputs , such as poetry ( Pardinas et al . , 2023 ) . Nonetheless , the most impressive current derived model is ChatGPT 6 , an interactive version of GPT - 3 ( Brown et al . , 2020 ) ﬁne - tuned with re - inforcement learning with human feedback ( Stiennon et al . , 2020 ) , then further improved in GPT - 4 ( OpenAI , 2023 ) . Even though the main goal of the model is to improve conversational skills while mitigating mistakes and biases , its ability in producing on - demand poems , songs , or novels has led it to remarkable global popularity 7 . Many companies are releasing their language models at the time of writing ( e . g . , Google’s Bard 8 ) ; the competition is intensifying day by day . 5 http : / / www . in - vacua . com / cgi - bin / haiku . pl 6 https : / / openai . com / blog / chatgpt / 7 https : / / www . forbes . com / sites / martineparis / 2023 / 02 / 03 / chatgpt - hits - 100 - million - microsof t - unleashes - ai - bots - and - catgpt - goes - viral / ? sh = 70994247564e 8 https : / / bard . google . com 3 3 Large Language Models and Boden’s Three Criteria Margaret Boden deﬁnes creativity as “the ability to come up with ideas or artifacts that are new , surprising and valuable ” ( Boden , 2003 ) . In other words , Boden implicitly derives criteria that can be used to identify a creative product . They suggest that creativity is about novelty , surprise and value . We will refer to them as Boden’s three criteria . In the following , we will analyze to what extent state - of - the - art LLMs satisfy them and we will question if LLMs can be really considered creative . Value refers to utility , performance , and attractiveness ( Maher , 2010 ) . It is also related to both the quality of the output , and its acceptance by the society . Due to the large impact LLMs are already having ( Bommasani et al . , 2021 ) and the quality of outputs of the systems based on them ( Stevenson et al . , 2022 ) , it is possible to argue that the artifacts produced by them are indeed valuable . Novelty refers to the dissimilarity between the produced artifact and other examples in its class ( Ritchie , 2007 ) . However , it can also be seen as the property of not being in existence before . This is considered in reference to either the person who comes up with it or the entire human history . The former is referred to as psychological creativity ( shortened as P - creativity ) , whereas the latter as historical creativity ( shortened as H - creativity ) ( Boden , 2003 ) . While the difference appears negligible , it is substantial when discussing LLMs in general . Considering these deﬁnitions , a model writing a text that is not in its training set would be considered as P - novel , but possibly also H - novel , since LLMs are commonly trained on all available data . Their stochastic nature and the variety of prompts that are usually provided commonly lead to novel outcomes ( McCoy et al . , 2021 ) ; LLMs may therefore be capable of generating artifacts that are also new . However , one should remember how such models learn and generate . Even if prompted with the sentence “I wrote a new poem this morning : ” , they would complete it with what is most likely to follow such words , e . g . , something close to what others have written in the past ( Shanahan , 2022 ) . It is a probabilistic process after all . The degree of dissimilarity would therefore be small by design . High values of novelty would be caused either by accidental , out - of - distribution productions , or by a careful prompting , i . e . , one that would place the LLM in a completely unusual or unexpected ( i . e . , novel ) situation . Surprise instead refers to how much a stimulus disagrees with expectation ( Berlyne , 1971 ) . It is possible to identify three kinds of surprise , which correspond to three different forms of creativ - ity . Combinatorial creativity involves making unfamiliar combinations of familiar ideas . Exploratory creativity requires ﬁnding new , unexplored solutions inside the current style of thinking . Transfor - mational creativity is related to changing the current style of thinking ( Boden , 2003 ) . These three different forms of creativity involve surprise at increasing levels of abstraction : combining existing elements , exploring for new elements coherent with the current state of the ﬁeld , and transforming the state of the ﬁeld so as to introduce other elements . The autoregressive nature of classic LLMs make them unlikely to generate surprising products ( Bunescu and Uduehi , 2019 ) , since they are es - sentially trained to follow the current data distribution ( Shanahan , 2022 ) . By relying only on given distributions and being trained on them , LLMs might at most express combinatorial creativity . Of course , speciﬁc different solutions may be generated by means of prompting or conditioning . For instance , recent LLMs are able to write poems about mathematical theories , a skill that requires the application of a certain existing style to a given topic , yet leading to new and unexplored solu - tions . However , the result would hardly be unexpected for whom has prompted the text . For an external reader , the surprise would probably come by the idea of mathematical theories in verses , which is due to the user ( or by the initial astonishment of a machine capable of it ( Waite , 2019 ) ) . 4 Transformational creativity is not achievable by means of the current LLM training solutions . In theory , other forms of training or ﬁne - tuning might circumvent this limitation , allowing the model to forget the learned rules in order to forge others . However , this is not the case of current models . For instance , ChatGPT is ﬁne - tuned with Reinforcement Learning from Human Feedback ( RLHF ) ( Stiennon et al . , 2020 ) . It consists of three steps : ﬁne - tuning GPT - 4 in a supervised fashion on human - produced answers to sampled questions ; training a reward model to predict which among different texts is the most appropriate one based on human - labeled rankings ; ﬁne - tuning GPT - 4 in order to maximize the learned reward . While in theory this could lead to potentially surpris - ing generation , its strict alignment to very careful and pre - designed human responses leads to the generation of text that might be considered banal ( Hoel , 2022 ) . In conclusion , while LLMs are capable of producing artifacts that are valuable , achieving nov - elty and surprise appears to be more challenging . It is possible to argue that LLMs may be deemed able to generate creative products if we assume the deﬁnition of combinatorial creativity . In order to achieve transformational creativity , alternative learning architectures are probably necessary ; in fact , current probabilistic solutions are intrinsically limiting in terms of expressivity . We believe that this is a fundamental research area for the community for the years to come . 4 Easy and Hard Problems in Machine Creativity LLMs might be able to generate creative products in the future . However , the fact they will be able to generate these outputs will not make them intrinsically creative . Indeed , as Floridi and Chiriatti ( 2020 ) puts it , it is not what is achieved but how it is achieved that matters . An interesting deﬁnition that considers both the what and how dimensions is the one from Gaut ( 2003 ) : creativity is the capacity to produce original and valuable items by ﬂair . Exhibiting ﬂair means exhibiting a relevant purpose , understanding , judgement , and evaluative abilities . Such properties are highly correlated with those linked with process , i . e . , motivation , perception , learning , thinking , and com - munication ( Rhodes , 1961 ) . Motivation is a crucial part of creativity , as it is the ﬁrst stage of the process . Usually , it comes from an intrinsic interest in the task , i . e . , the activity is interesting and enjoyable for its own sake ( Deci and Ryan , 1985 ) . However , LLMs lack the intention to write . They can only deal with “presented” problems , which are less conductive to creativity ( Amabile , 1996 ) . The process continues with the preparation step ( reactivating store of relevant information and response algorithms ) , the response generation , and its validation and communication ( Ama - bile , 1983 ) . The last two steps allow to produce different response possibilities and to internally test them in order to select the most appropriate . Again , LLMs do not contain such self - feedback loop . At the same time , they are neither trained to directly maximize value , novelty or surprise . They only output content that is likely to follow given a stimulus in input ( Shanahan , 2022 ) . In other words , they stop at the ﬁrst stage of creative learning , i . e . , imitation , not implementing the remaining ones , i . e . , exploration and intentional deviation from conventions ( Riedl , 2018 ) . However , paraphrasing Chalmers ( Chalmers , 1996 ) , these appear as easy problems to solve in order to achieve creativity , since solutions to them can be identiﬁed by taking into consideration the underlying training and optimization processes . The hard problem in machine creativity is about the self - awareness of the creative process in itself . Indeed , a crucial aspect of the creative process is the perception and the ability of self - evaluating the generated outputs ( Amabile , 1983 ) . This can be seen as a form of creative self - awareness . While not strictly necessary to generate a response , this ability is essential in order to self - assess its quality , so as to correct it or to learn 5 from it . However , currently no LLM is able to self - evaluate its own responses . LLMs can in theory recognize certain limitations of their own texts after generating them . Then , they can try to correct , modify or rephrase the outputs if asked to do so ( i . e . , through an external intervention ) . However , they would do it only by guessing what is the most likely re - casting of such responses or through the application of a set of given rules . It is worth noting that this is something distinct from the problem of the potential emergence of theory of mind in these systems ( Bubeck et al . , 2023 ) . Indeed , product and process are not sufﬁcient to explain creativity . Rhodes ( 1961 ) theorizes that four perspectives have to be considered : product ( see Section 3 ) and process ( discussed above ) , but also the so - called press and person . Press refers to the relationship between the product and the inﬂuence its environment has upon it ( Rhodes , 1961 ) . Individuals and their works cannot be isolated from the social and historical milieu in which their actions are carried out . Products have to be accepted as creative by the society , and producers are inﬂuenced by the previously ac - cepted works , i . e . , the domain ( Csikszentmihalyi , 1988 ) . The resulting system model of creativity is a never - ending cycle where individuals always base their works on knowledge from a domain , which constantly changes thanks to new and valuable artifacts ( from different individuals ) . For example , individuals generate new works based on the current domain ; the ﬁeld ( i . e . , critics , other artists , the public , etc . ) decides which of those works are worth promoting and preserving ; the do - main is expanded and , possibly , transformed by these selected works ; individuals generate new works based on the updated current domain ; and then this cycle repeats . However , LLMs currently lack the ability of adapting through multiple iterations in the way described above ; they just rely on one , ﬁxed version of the domain and generate works based on it . The current generation of LLMs are immutable entities , i . e . , once the training is ﬁnished , they remain frozen reﬂecting a speciﬁc state of the domain . In other words , they are not able to adapt to new changes . Few - shot , one - shot and zero - shot learning ( Brown et al . , 2020 ) do not update weights and are not useful for this task , though highly effective for a variety of others . Fine tuning actually updates network weights , but it requires a potentially large training dataset . Indeed , several current research efforts are in the direction of introducing adaptation for speciﬁc domains , tasks , cultural frameworks and so on . In order to be able to be part of the never - ending creative cycle mentioned above , LLMs should constantly adapt . Continual learning ( Kirkpatrick et al . , 2017 ; Shin et al . , 2017 ) for LLMs ( Sun et al . , 2020 ; Wu et al . , 2022 ) represents a promising direction , yet unexplored for creative applications . Finally , person covers information about personality , intellect , temperament , habits , attitude , value systems , and defense mechanisms ( Rhodes , 1961 ) . While several of the properties of press and process might be achieved - or at least simulated - by generative learning solutions , those related with the creative person appear out of discussion . All the properties listed above re - quire some forms of consciousness and self - awareness , which are difﬁcult to deﬁne in themselves and are related to the hard problem introduced before . Creative - person qualities in Generative AI might eventually be the ultimate step in achieving human - like intelligence . 5 Practical Implications The application of large language models to ﬁelds like literature or journalism opens up a series of practical questions . Since LLMs can be used to produce artifacts that would be protected if made by humans , a ﬁrst concern is the deﬁnition of legal frameworks in which they will be used . Copyright for Generative AI is currently a hotly debated topic ( Guadamuz , 2017 ; Franceschelli 6 and Musolesi , 2022 ) , due to the fact that current laws do not contemplate works produced by non - human beings ( with few notable exceptions ( Bond and Blair , 2019 ) ) . Copyright applies to creative works of authorship ( as referred to in the US Copyright Code ) , i . e . , works showing a minimum de - gree of originality ( Gervais , 2002 ) and reﬂecting author’s personality ( Deltorn , 2017 ) . As discussed earlier , current LLMs might satisfy the ﬁrst condition , but they cannot be considered as creative persons , therefore missing the latter requirement . For this reason , works produced by LLMs can be protected if and only if the original contribution is provided by a human , e . g . , the user who writes the prompt that is used as input of the model , who in turn will be the rights holder . The deﬁnition of the criteria for classifying a source of originality is a fundamental problem , since there is a clear need to discriminate between protected and publicly available works . While a higher degree of novelty is not necessary for claiming protection , it might be crucial for other legal aspects . In particular , LLMs are trained in a supervised fashion on real data , which also include protected works . Apart from questions upon the legitimacy of such training ( Franceschelli and Musolesi , 2022 ) , LLMs may learn to reproduce portions of them ( Liang et al . , 2022 ) . This would violate their reproduction or adaptation right ( Bonadio and McDonagh , 2020 ) . A different , creative - oriented training approach should mitigate such risk , also facilitating fair - use doctrine application ( Asay et al . , 2020 ) . Whether or not LLM works obtain protection , we believe their societal impact will be tremen - dous ( see also Newton and Dhole ( 2023 ) ) . We have a positive view in terms of the applications of LLMs , but there are intrinsic risks related to their adoption . It is apparent that since LLMs are able to write articles or short stories , as the quality of their inputs gets better and better , there is the risk that certain jobs in the professional writing industry will essentially disappear ( Tamkin et al . , 2021 ) . However , we must remind that current LLMs are not as reliable as humans , e . g . , they cannot verify their information and they can propagate biases from training data . In addition , the quality of the output strictly depends on the prompt , which might in turn demand human skills and more time . Writers can be threaten as well . Though not in violation of copyright , LLMs may exploit certain ideas from human authors , capitalizing on their efforts in ways that are less expensive or time - consuming ( Weidinger et al . , 2022 ) . The questionable creative nature of LLMs discussed so far might suggest artiﬁcial works to be of less quality than humans , therefore not providing a real threat . Nonetheless , more creative LLMs would diverge more consistently from existing works , reducing the risk of capitalizing on others’ ideas . The lack of current copyright protection to gen - erated works can also foster such replacements for tasks where a free - of - charge text would be preferable to a high - quality ( but still costly ) one . Finally , one last threat may be posed by human and artiﬁcial works being indistinguishable ( Dehouche , 2021 ) . The users obtaining such outputs might therefore claim them as the authors , e . g . , for deceiving readers ( Grinbaum and Adomaitis , 2022 ) , for cheating during exams , or for improving bibliometric indicators ( Crothers et al . , 2022 ) . Mitigation of such threats through dedicated policies 9 or designed mechanisms of watermarks ( Kirchenbauer et al . , 2023 ) are already being developed . However , as we said , we believe that , overall , the impact of these technologies will be positive . LLMs also provide several opportunities for creative activities . Given their characteristics , humans are still required , especially for prompting , curation , and pre - / post - production . This means that the role of writers and journalists may be transformed , but not replaced . On the contrary , LLMs provide new opportunities for humans , who will be able to spend more time validating news or thinking up and testing ideas . LLMs can also adapt the same text to different styles ( see combi - 9 https : / / bigscience . huggingface . co / blog / the - bigscience - rail - license 7 natorial creativity in Section 3 ) : by doing so , an artifact can be adapted in order to reach wider audiences . In the same way , LLMs also represent a valuable tool in scientiﬁc research , especially for hypothesis generation ( Gero et al . , 2022 ) . Indeed , we believe that LLMs can also foster human - AI co - creativity ( Lee et al . , 2022 ) , since they can be used to write portions of stories in order to serve speciﬁc purposes , e . g . , they can typify all the dialogues from a character , or they can provide more detailed descriptions of scenes ( Calderwood et al . , 2020 ) . Dialogue systems based on LLMs can be used for brainstorming . In the same way , the generated responses may augment writers’ inherently multiversal imagination ( Reynolds and McDonell , 2021 ) . LLMs can also represent a source of inspiration for plot twists , metaphors ( Chakrabarty et al . , 2023 ) , or even entire story plans ( Mirowski et al . , 2022 ) , even though they sometimes appear to fail in accomplishing these tasks at human - like level ( Ippolito et al . , 2022 ) . Being intrinsically powerful tools , through human - AI co - creation , LLMs may eventually allow to develop entire new arts , as it has been the case for any impactful technology in the past centuries ( Eisenstein , 1979 ; Silva , 2022 ) . 6 Conclusion The latest generation of LLMs is attracting increasing interest from both AI researchers and the general public due to the astonishing quality of their productions . Questions naturally arise around the actual creativity of these technologies . In this paper , we have discussed whether or not LLMs can actually be deemed as creative ; we started from considering Boden’s three criteria , i . e . , value , novelty , and surprise . While LLMs are capable of value and of a weak version of novelty and surprise , their inner autoregressive nature seems to prevent them from reaching transforma - tional creativity . Then , we have examined perspectives beyond the creativity of their products . A creative process would require motivation , thinking , and perception , properties that current LLMs do not possess . The social dimension of creativity ( usually refer to as the press ) would demand to be placed in and inﬂuenced by a society of creative agents , requiring LLMs adaptive abilities that are only at a very initial stage . We have also framed the problem of creativity in LLMs , and , more in general , machine creativity , in terms of easy problems , i . e . , the technical advancements that will be needed to support algorithmic generation of outputs , and the intrinsic hard problem of introducing forms of self - awareness in the creation process itself . In addition , we have also investigated practical implications of LLMs and their creative role , considering both legal and societal impacts . In fact , the current legal framework does not appear to be completely suited to the fast moving ﬁeld of Generative AI . Moreover , the impact of these technologies for creative professions and the arts is difﬁcult to forecast at this stage , but will deﬁ - nitely be considerable . However , LLMs also provide opportunities for writers , especially in terms of human - AI co - operation . Speciﬁc ﬁne - tuning techniques might help LLMs diversify productions and explore the conceptual space they learn from data . Continual learning can enable long - term deployments of LLMs in a variety of contexts . While of course all these techniques would only simulate certain aspects of creativity , whether this would be sufﬁcient to achieve artiﬁcial , i . e . , non - human , creativity , is up to the humans themselves . 8 References T . M . Amabile . The social psychology of creativity : A componential conceptualization . Journal of Personality and Social Psychology , 45 ( 2 ) : 357 – 376 , 1983 . T . M . Amabile . Creativity In Context . Routledge , 1996 . C . D . Asay , A . Sloan , and D . Sobczak . Is transformative use eating the world ? Boston College Law Review , 61 ( 3 ) : 905 – 970 , 2020 . C . Babbage . Of the analytical engine . In Passages from the Life of a Philosopher , volume 3 , pages 112 – 141 . Longman , Green , Longman , Roberts , & Green , 1864 . D . Bahdanau , K . Cho , and Y . Bengio . Neural machine translation by jointly learning to align and translate . In Proceedings of the 3rd International Conference on Learning Representations ( ICLR’15 ) , 2015 . F . Barron . The disposition toward originality . Journal of Abnormal Psychology , 51 ( 3 ) : 478 – 485 , 1955 . E . M . Bender and A . Koller . Climbing towards NLU : On meaning , form , and understanding in the age of data . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ( ACL’20 ) , pages 5185 – 5198 , 2020 . E . M . Bender , T . Gebru , A . McMillan - Major , and S . Shmitchell . On the dangers of stochastic par - rots : Can language models be too big ? In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency ( FAccT’21 ) , page 610 – 623 , 2021 . D . E . Berlyne . Conﬂict , Arousal , and Curiosity . McGraw - Hill Book Company , 1960 . D . E . Berlyne . Aesthetics and Psychobiology . Appleton - Century - Crofts , New York , NY , 1971 . M . A . Boden . The Creative Mind : Myths and Mechanisms . Routledge , 2003 . M . A . Boden . Computer models of creativity . AI Magazine , 30 ( 3 ) : 23 – 34 , 2009 . R . Bommasani , D . A . Hudson , E . Adeli , R . Altman , S . Arora , S . von Arx , M . S . Bernstein , J . Bohg , A . Bosselut , E . Brunskill , E . Brynjolfsson , S . Buch , D . Card , R . Castellon , N . Chatterji , A . Chen , K . Creel , J . Q . Davis , D . Demszky , C . Donahue , M . Doumbouya , E . Durmus , S . Er - mon , J . Etchemendy , K . Ethayarajh , L . Fei - Fei , C . Finn , T . Gale , L . Gillespie , K . Goel , N . Good - man , S . Grossman , N . Guha , T . Hashimoto , P . Henderson , J . Hewitt , D . E . Ho , J . Hong , K . Hsu , J . Huang , T . Icard , S . Jain , D . Jurafsky , P . Kalluri , S . Karamcheti , G . Keeling , F . Khani , O . Khattab , P . W . Koh , M . Krass , R . Krishna , R . Kuditipudi , A . Kumar , F . Ladhak , M . Lee , T . Lee , J . Leskovec , I . Levent , X . L . Li , X . Li , T . Ma , A . Malik , C . D . Manning , S . Mirchandani , E . Mitchell , Z . Mun - yikwa , S . Nair , A . Narayan , D . Narayanan , B . Newman , A . Nie , J . C . Niebles , H . Nilforoshan , J . Nyarko , G . Ogut , L . Orr , I . Papadimitriou , J . S . Park , C . Piech , E . Portelance , C . Potts , A . Raghu - nathan , R . Reich , H . Ren , F . Rong , Y . Roohani , C . Ruiz , J . Ryan , C . R´e , D . Sadigh , S . Sagawa , K . Santhanam , A . Shih , K . Srinivasan , A . Tamkin , R . Taori , A . W . Thomas , F . Tram ` er , R . E . Wang , W . Wang , B . Wu , J . Wu , Y . Wu , S . M . Xie , M . Yasunaga , J . You , M . Zaharia , M . Zhang , T . Zhang , X . Zhang , Y . Zhang , L . Zheng , K . Zhou , and P . Liang . On the opportunities and risks of founda - tion models , 2021 . arXiv : 2108 . 07258 [ cs . LG ] . 9 E . Bonadio and L . McDonagh . Artiﬁcial intelligence as producer and consumer of copyright works : Evaluating the consequences of algorithmic creativity . Intellectual Property Quarterly 2020 , 2 : 112 – 137 , 2020 . T . Bond and S . Blair . Artiﬁcial intelligence & copyright : Section 9 ( 3 ) or authorship without an author . Journal of Intellectual Property Law & Practice , 14 ( 6 ) : 423 – 423 , 2019 . S . R . Bowman , L . Vilnis , O . Vinyals , A . M . Dai , R . Jozefowicz , and S . Bengio . Generating sentences from a continuous space . In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning ( CoNNL’16 ) , pages 10 – 21 , 2016 . T . Brown , B . Mann , N . Ryder , M . Subbiah , J . D . Kaplan , P . Dhariwal , A . Neelakantan , P . Shyam , G . Sastry , A . Askell , S . Agarwal , A . Herbert - Voss , G . Krueger , T . Henighan , R . Child , A . Ramesh , D . Ziegler , J . Wu , C . Winter , C . Hesse , M . Chen , E . Sigler , M . Litwin , S . Gray , B . Chess , J . Clark , C . Berner , S . McCandlish , A . Radford , I . Sutskever , and D . Amodei . Language models are few - shot learners . In Advances in Neural Information Processing Systems ( NIPS’20 ) , pages 1877 – 1901 , 2020 . J . S . Bruner . The conditions of creativity . In Contemporary approaches to creative thinking : A symposium held at the University of Colorado , pages 1 – 30 . Atherton Press , 1962 . S . Bubeck , V . Chandrasekaran , R . Eldan , J . Gehrke , E . Horvitz , E . Kamar , P . Lee , Y . T . Lee , Y . Li , S . Lundberg , et al . Sparks of Artiﬁcial General Intelligence : Early experiments with GPT - 4 , 2023 . arXiv : 2303 . 12712 [ cs . CL ] . R . C . Bunescu and O . O . Uduehi . Learning to surprise : A composer - audience architecture . In Proceedings of the 10th International Conference on Computational Creativity ( ICCC’19 ) , pages 41 – 48 , 2019 . A . Calderwood , V . Qiu , K . I . Gero , and L . B . Chilton . How novelists use generative language mod - els : An exploratory user study . In Proceedings of the IUI’20 Workshop on Human - AI Co - Creation with Generative Models , 2020 . T . Chakrabarty , V . Padmakumar , and H . He . Help me write a poem : Instruction tuning as a vehicle for collaborative poetry writing . In Proceedings of the AAAI - 23 Workshop on Creative AI Across Modalities , 2023 . D . J . Chalmers . The Conscious Mind : In Search of a Fundamental Theory . Oxford University Press , 1996 . K . Cho , B . van Merrienboer , D . Bahdanau , and Y . Bengio . On the properties of neural machine translation : Encoder - decoder approaches . In Proceedings of SSST - 8 , 8th Workshop on Syntax , Se - mantics and Structure in Statistical Translation , pages 103 – 111 , 2014 . A . Chowdhery , S . Narang , J . Devlin , M . Bosma , G . Mishra , A . Roberts , P . Barham , H . W . Chung , C . Sutton , S . Gehrmann , P . Schuh , K . Shi , S . Tsvyashchenko , J . Maynez , A . Rao , P . Barnes , Y . Tay , N . Shazeer , V . Prabhakaran , E . Reif , N . Du , B . Hutchinson , R . Pope , J . Bradbury , J . Austin , M . Is - ard , G . Gur - Ari , P . Yin , T . Duke , A . Levskaya , S . Ghemawat , S . Dev , H . Michalewski , X . Garcia , V . Misra , K . Robinson , L . Fedus , D . Zhou , D . Ippolito , D . Luan , H . Lim , B . Zoph , A . Spiridonov , 10 R . Sepassi , D . Dohan , S . Agrawal , M . Omernick , A . M . Dai , T . S . Pillai , M . Pellat , A . Lewkowycz , E . Moreira , R . Child , O . Polozov , K . Lee , Z . Zhou , X . Wang , B . Saeta , M . Diaz , O . Firat , M . Catasta , J . Wei , K . Meier - Hellstern , D . Eck , J . Dean , S . Petrov , and N . Fiedel . PaLM : Scaling language modeling with pathways , 2022 . arXiv : 2204 . 02311 [ cs . CL ] . S . Colton and G . A . Wiggins . Computational creativity : The ﬁnal frontier ? In Proceedings of the 20th European Conference on Artiﬁcial Intelligence ( ECAI’12 ) , volume 12 , pages 21 – 26 , 2012 . E . Crothers , N . Japkowicz , and H . Viktor . Machine generated text : A comprehensive survey of threat models and detection methods , 2022 . arXiv : 2210 . 07321 [ cs . CL ] . M . Csikszentmihalyi . Society , culture , and person : A systems view of creativity . In The nature of creativity : Contemporary psychological perspectives , pages 325 – 339 . Cambridge University Press , 1988 . S . Dathathri , A . Madotto , J . Lan , J . Hung , E . Frank , P . Molino , J . Yosinski , and R . Liu . Plug and play language models : A simple approach to controlled text generation . In Proceedings of the 8th International Conference on Learning Representations ( ICLR’20 ) , 2020 . E . L . Deci and R . M . Ryan . Intrinsic Motivation and Self - Determination in Human Behavior . Springer , 1985 . N . Dehouche . Plagiarism in the age of massive generative pre - trained transformers ( GPT - 3 ) . Ethics in Science and Environmental Politics , 21 : 17 – 23 , 2021 . J . - M . Deltorn . Deep creations : Intellectual property and the automata . Frontiers in Digital Humani - ties , 4 ( 3 ) : 1 – 13 , 2017 . J . Devlin , M . - W . Chang , K . Lee , and K . Toutanova . BERT : Pre - training of deep bidirectional trans - formers for language understanding . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) ( NAACL’19 ) , pages 4171 – 4186 , 2019 . N . Du , Y . Huang , A . M . Dai , S . Tong , D . Lepikhin , Y . Xu , M . Krikun , Y . Zhou , A . W . Yu , O . Firat , B . Zoph , L . Fedus , M . Bosma , Z . Zhou , T . Wang , Y . E . Wang , K . Webster , M . Pellat , K . Robin - son , K . Meier - Hellstern , T . Duke , L . Dixon , K . Zhang , Q . V . Le , Y . Wu , Z . Chen , and C . Cui . GLaM : Efﬁcient scaling of language models with mixture - of - experts . In Proceedings of the 39th International Conference on Machine Learning ( ICML’22 ) , pages 5547 – 5569 , 2022 . E . Eisenstein . The Printing Press as an Agent of Change : Communications and Cultural Transformations in Early - Modern Europe . Cambridge University Press , Cambridge , United Kingdom , 1979 . L . Floridi and M . Chiriatti . GPT - 3 : Its nature , scope , limits , and consequences . Minds and Machines , 30 ( 4 ) : 681 – 694 , 2020 . G . Franceschelli and M . Musolesi . Creativity and machine learning : A survey , 2021 . arXiv : 2104 . 02726 [ cs . LG ] . G . Franceschelli and M . Musolesi . Copyright in generative deep learning . Data & Policy , 4 : e17 , 2022 . 11 B . Gaut . Creativity and imagination . In The Creation of Art : New Essays in Philosophical Aesthetics , pages 148 – 173 . Cambridge University Press , 2003 . B . Gaut . The philosophy of creativity . Philosophy Compass , 5 ( 12 ) : 1034 – 1046 , 2010 . K . I . Gero , V . Liu , and L . Chilton . Sparks : Inspiration for science writing using language models . In Proceedings of the 2022 Designing Interactive Systems Conference ( DIS’22 ) , page 1002 – 1019 , 2022 . D . J . Gervais . Feist goes global : A comparative analysis of the notion of originality in copyright law . Journal of the Copyright Society of the U . S . A . , 49 : 949 – 981 , 2002 . P . Gerv´as . Computational modelling of poetry generation . In Symposium on Artiﬁcial Intelligence and Poetry ( AISB’13 ) , pages 11 – 16 , 2013 . GPT - 3 , A . O . Thunstr¨om , and S . Steingrimsson . Can GPT - 3 write an academic paper on itself , with minimal human input ? , 2022 . https : / / hal . archives - ouvertes . fr / hal - 03701250v1 . A . Grinbaum and L . Adomaitis . The ethical need for watermarks in machine - generated language , 2022 . arXiv : 2209 . 03118 [ cs . CL ] . A . Guadamuz . Do androids dream of electric copyright ? comparative analysis of originality in artiﬁcial intelligence generated works . Intellectual Property Quarterly , 2017 . S . Hochreiter and J . Schmidhuber . Long short - term memory . Neural Computation , 9 ( 8 ) : 1735 – 178 , 1997 . E . Hoel . The banality of ChatGPT , 2022 . https : / / erikhoel . substack . com / p / the - banality - of - chatgpt [ Retrieved on January 26 , 2023 ] . J . Hoffmann , S . Borgeaud , A . Mensch , E . Buchatskaya , T . Cai , E . Rutherford , D . de Las Casas , L . A . Hendricks , J . Welbl , A . Clark , T . Hennigan , E . Noland , K . Millican , G . van den Driess - che , B . Damoc , A . Guy , S . Osindero , K . Simonyan , E . Elsen , J . W . Rae , O . Vinyals , and L . Sifre . Training compute - optimal large language models . In Advances in Neural Information Processing Systems ( NIPS’22 ) , 2022 . D . Ippolito , A . Yuan , A . Coenen , and S . Burnam . Creative writing with an ai - powered writing assistant : Perspectives from professional writers , 2022 . arXiv : 2211 . 05030 [ cs . HC ] . A . K . Jordanous . Evaluating machine creativity . In Proceedings of the Seventh ACM Conference on Creativity and Cognition ( C & C’09 ) , page 331 – 332 , 2009 . D . Jurafsky and J . H . Martin . Speech and Language Processing . Third ( draft ) edition , 2023 . A . Karpathy . The unreasonable effectiveness of recurrent neural networks , 2015 . http : / / karpat hy . github . io / 2015 / 05 / 21 / rnn - effectiveness / [ Retrieved on August 5 , 2020 ] . J . Kirchenbauer , J . Geiping , Y . Wen , J . Katz , I . Miers , and T . Goldstein . A watermark for large language models , 2023 . arXiv : 2301 . 10226 [ cs . LG ] . 12 J . Kirkpatrick , R . Pascanu , N . Rabinowitz , J . Veness , G . Desjardins , A . A . Rusu , K . Milan , J . Quan , T . Ramalho , A . Grabska - Barwinska , D . Hassabis , C . Clopath , D . Kumaran , and R . Hadsell . Over - coming catastrophic forgetting in neural networks . Proceedings of the National Academy of Sciences , 114 ( 13 ) : 3521 – 3526 , 2017 . M . Lebowitz . Creating a story - telling universe . In Proceedings of the 8th International Joint Conference on Artiﬁcial Intelligence ( IJCAI’83 ) , page 63 – 65 , 1983 . H . H . Lee , K . Shu , P . Achananuparp , P . K . Prasetyo , Y . Liu , E . - P . Lim , and L . R . Varshney . RecipeGPT : Generative pre - training based cooking recipe generation and evaluation system . In Companion Proceedings of the Web Conference 2020 ( WWW’20 ) , page 181 – 184 , 2020 . M . Lee , P . Liang , and Q . Yang . CoAuthor : Designing a human - ai collaborative writing dataset for exploring language model capabilities . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( CHI’22 ) , pages 1 – 19 , 2022 . P . Liang , R . Bommasani , T . Lee , D . Tsipras , D . Soylu , M . Yasunaga , Y . Zhang , D . Narayanan , Y . Wu , A . Kumar , B . Newman , B . Yuan , B . Yan , C . Zhang , C . Cosgrove , C . D . Manning , C . R´e , D . Acosta - Navas , D . A . Hudson , E . Zelikman , E . Durmus , F . Ladhak , F . Rong , H . Ren , H . Yao , J . Wang , K . Santhanam , L . Orr , L . Zheng , M . Yuksekgonul , M . Suzgun , N . Kim , N . Guha , N . Chatterji , O . Khattab , P . Henderson , Q . Huang , R . Chi , S . M . Xie , S . Santurkar , S . Ganguli , T . Hashimoto , T . Icard , T . Zhang , V . Chaudhary , W . Wang , X . Li , Y . Mai , Y . Zhang , and Y . Koreeda . Holistic evaluation of language models , 2022 . arXiv : 2211 . 09110 [ cs . CL ] . L . Macedo , R . Reisenzein , and A . Cardoso . Modeling forms of surprise in artiﬁcial agents : em - pirical and theoretical study of surprise functions . In Proceedings of the Annual Meeting of the Cognitive Science Society ( CogSci’04 ) , pages 873 – 878 , 2004 . M . L . Maher . Evaluating creativity in humans , computers , and collectively intelligent systems . In Proceedings of the 1st DESIRE Network Conference on Creativity and Innovation in Design ( DE - SIRE’10 ) , page 22 – 28 , 2010 . R . Manurung , G . Ritchie , and H . Thompson . Using genetic algorithms to create meaningful poetic text . Journal of Experimental & Theoretical Artiﬁcial Intelligence , 24 ( 1 ) : 43 – 64 , 2012 . L . J . Martin , P . Ammanabrolu , W . Hancock , S . Singh , B . Harrison , and M . O . Riedl . Event representations for automated story generation with deep neural nets . In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence and 30th Innovative Applications of Artiﬁcial In - telligence Conference and 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence ( AAAI’18 / IAAI’18 / EAAI’18 ) , 2018 . R . T . McCoy , P . Smolensky , T . Linzen , J . Gao , and A . Celikyilmaz . How much do language models copy from their training data ? evaluating linguistic novelty in text generation using RAVEN , 2021 . arXiv : 2111 . 09509 [ cs . CL ] . J . R . Meehan . TALE - SPIN , an interactive program that writes stories . In Proceedings of the 5th International Joint Conference on Artiﬁcial Intelligence ( IJCAI’77 ) , pages 91 – 98 , 1977 . L . F . Menabrea and A . Lovelace . Sketch of the analytical engine invented by charles babbage . In Scientiﬁc Memoirs , volume 3 , pages 666 – 731 . Richard and John E . Taylor , 1843 . 13 A . I . Miller . The Artist in the Machine . The MIT Press , 2019 . P . Mirowski , K . W . Mathewson , J . Pittman , and R . Evans . Co - writing screenplays and theatre scripts alongside language models using Dramatron . In Proceedings of the NIPS’22 Workshop on ML for Creativity & Design , 2022 . A . Newell , J . C . Shaw , and H . A . Simon . The processes of creative thinking . In Contemporary ap - proaches to creative thinking : A symposium held at the University of Colorado , pages 63 – 119 . Atherton Press , 1962 . A . Newton and K . Dhole . Is AI art another industrial revolution in the making ? In Proceedings of the AAAI - 23 Workshop on Creative AI Across Modalities , 2023 . OpenAI . GPT - 4 Technical Report , 2023 . https : / / cdn . openai . com / papers / gpt - 4 . pdf . R . Pardinas , G . Huang , D . Vazquez , and A . Pich´e . Leveraging human preferences to master poetry . In Proceedings of the AAAI - 23 Workshop on Creative AI Across Modalities , 2023 . P . Potash , A . Romanov , and A . Rumshisky . GhostWriter : Using an LSTM for automatic rap lyric generation . In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process - ing ( EMNLP’15 ) , pages 1919 – 1924 , 2015 . Racter . The Policeman’s Beard Is Half Constructed . Warner Books , Inc . , 1984 . A . Radford , J . Wu , R . Child , D . Luan , D . Amodei , and I . Sutskever . Language models are unsuper - vised multitask learners , 2019 . J . W . Rae , S . Borgeaud , T . Cai , K . Millican , J . Hoffmann , F . Song , J . Aslanides , S . Henderson , R . Ring , S . Young , E . Rutherford , T . Hennigan , J . Menick , A . Cassirer , R . Powell , G . van den Driessche , L . A . Hendricks , M . Rauh , P . - S . Huang , A . Glaese , J . Welbl , S . Dathathri , S . Huang , J . Uesato , J . Mellor , I . Higgins , A . Creswell , N . McAleese , A . Wu , E . Elsen , S . Jayakumar , E . Buchatskaya , D . Budden , E . Sutherland , K . Simonyan , M . Paganini , L . Sifre , L . Martens , X . L . Li , A . Kun - coro , A . Nematzadeh , E . Gribovskaya , D . Donato , A . Lazaridou , A . Mensch , J . - B . Lespiau , M . Tsimpoukelli , N . Grigorev , D . Fritz , T . Sottiaux , M . Pajarskas , T . Pohlen , Z . Gong , D . Toyama , C . de Masson d’Autume , Y . Li , T . Terzi , V . Mikulik , I . Babuschkin , A . Clark , D . de Las Casas , A . Guy , C . Jones , J . Bradbury , M . Johnson , B . Hechtman , L . Weidinger , I . Gabriel , W . Isaac , E . Lockhart , S . Osindero , L . Rimell , C . Dyer , O . Vinyals , K . Ayoub , J . Stanway , L . Bennett , D . Has - sabis , K . Kavukcuoglu , and G . Irving . Scaling language models : Methods , analysis & insights from training gopher , 2021 . arXiv : 2112 . 11446 [ cs . CL ] . C . Raffel , N . Shazeer , A . Roberts , K . Lee , S . Narang , M . Matena , Y . Zhou , W . Li , and P . J . Liu . Ex - ploring the limits of transfer learning with a uniﬁed text - to - text transformer . Journal of Machine Learning Research , 21 ( 140 ) : 1 – 67 , 2020 . H . Rashkin , A . Celikyilmaz , Y . Choi , and J . Gao . PlotMachines : Outline - conditioned generation with dynamic plot state tracking . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP’20 ) , pages 4274 – 4295 , 2020 . L . Reynolds and K . McDonell . Multiversal views on language models , 2021 . arXiv : 2102 . 06391 [ cs . HC ] . 14 M . Rhodes . An analysis of creativity . The Phi Delta Kappan , 42 ( 7 ) : 305 – 310 , 1961 . M . O . Riedl . Computational creativity as meta search , 2018 . https : / / mark - riedl . medium . com / c omputational - creativity - as - meta - search - 6cad95da923b [ Retrieved on January 27 , 2023 ] . M . O . Riedl and R . M . Young . Narrative planning : Balancing plot and character . Journal of Artiﬁcial Intelligence Research , 39 ( 1 ) : 217 – 268 , 2010 . G . Ritchie . Some empirical criteria for attributing creativity to a computer program . Minds and Machines , 17 : 67 – 99 , 2007 . C . Rosset . Turing - NLG : A 17 - billion - parameter language model by microsoft , 2020 . https : / / ww w . microsoft . com / en - us / research / blog / turing - nlg - a - 17 - billion - parameter - language - model - by - microsoft / [ Retrieved on May 3 , 2022 ] . P . Sawicki , M . Grz´es , A . Jordanous , D . Brown , and M . Peeperkorn . Training GPT - 2 to represent two romantic - era authors : challenges , evaluations and pitfalls . In Proceedings of the 3th International Conference on Computational Creativity ( ICCC’22 ) , 2022 . S . Semeniuta , A . Severyn , and E . Barth . A hybrid convolutional variational autoencoder for text generation . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process - ing ( EMNLP’17 ) , pages 627 – 637 , 2017 . M . Shanahan . Talking about large language models , 2022 . arXiv : 2212 . 03551 [ cs . CL ] . H . Shin , J . K . Lee , J . Kim , and J . Kim . Continual learning with deep generative replay . In Advances in Neural Information Processing Systems ( NIPS’17 ) , 2017 . M . Shoeybi , M . Patwary , R . Puri , P . LeGresley , J . Casper , and B . Catanzaro . Megatron - LM : Train - ing multi - billion parameter language models using model parallelism , 2019 . arXiv : 1909 . 08053 [ cs . CL ] . E . Silva . How photography pioneered a new understanding of art , 2022 . https : / / www . thecolle ctor . com / how - photography - transformed - art / [ Retrieved on February 10 , 2023 ] . S . Smith , M . Patwary , B . Norick , P . LeGresley , S . Rajbhandari , J . Casper , Z . Liu , S . Prabhumoye , G . Zerveas , V . Korthikanti , E . Zhang , R . Child , R . Y . Aminabadi , J . Bernauer , X . Song , M . Shoeybi , Y . He , M . Houston , S . Tiwary , and B . Catanzaro . Using DeepSpeed and megatron to train megatron - turing NLG 530b , a large - scale generative language model , 2022 . arXiv : 2201 . 11990 [ cs . CL ] . M . I . Stein . Stimulating Creativity . Volume 1 . Academic Press , 1974 . C . Stevenson , I . Smal , M . Baas , R . Grasman , and H . van der Maas . Putting GPT - 3’s creativity to the ( alternative uses ) test . In Proceedings of the 13th International Conference on Computational Creativity ( ICCC’22 ) , 2022 . N . Stiennon , L . Ouyang , J . Wu , D . Ziegler , R . Lowe , C . Voss , A . Radford , D . Amodei , and P . F . Christiano . Learning to summarize with human feedback . In Advances in Neural Information Processing Systems ( NIPS’20 ) , pages 3008 – 3021 , 2020 . 15 F . - K . Sun , C . - H . Ho , and H . - Y . Lee . LAMOL : LAnguage MOdeling for Lifelong Language Learn - ing . In Proceedings of the 2020 International Conference on Learning Representations ( ICLR’20 ) , 2020 . B . Swanson , K . Mathewson , B . Pietrzak , S . Chen , and M . Dinalescu . Story centaur : Large language model few shot learning as a creative writing tool . In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : System Demonstrations ( EACL’21 ) , pages 244 – 256 , 2021 . A . Tamkin , M . Brundage , J . Clark , and D . Ganguli . Understanding the capabilities , limitations , and societal impact of large language models , 2021 . arXiv : 2102 . 02503 [ cs . CL ] . R . Thoppilan , D . De Freitas , J . Hall , N . Shazeer , A . Kulshreshtha , H . - T . Cheng , A . Jin , T . Bos , L . Baker , Y . Du , Y . Li , H . Lee , H . S . Zheng , A . Ghafouri , M . Menegali , Y . Huang , M . Krikun , D . Lepikhin , J . Qin , D . Chen , Y . Xu , Z . Chen , A . Roberts , M . Bosma , V . Zhao , Y . Zhou , C . - C . Chang , I . Krivokon , W . Rusch , M . Pickett , P . Srinivasan , L . Man , K . Meier - Hellstern , M . R . Morris , T . Doshi , R . D . Santos , T . Duke , J . Soraker , B . Zevenbergen , V . Prabhakaran , M . Diaz , B . Hutchinson , K . Olson , A . Molina , E . Hoffman - John , J . Lee , L . Aroyo , R . Rajakumar , A . Butryna , M . Lamm , V . Kuzmina , J . Fenton , A . Cohen , R . Bernstein , R . Kurzweil , B . Aguera - Arcas , C . Cui , M . Croak , E . Chi , and Q . Le . LaMDA : Language models for dialog applications , 2022 . arXiv : 2201 . 08239 [ cs . CL ] . A . M . Turing . Computing machinery and intelligence . Mind , LIX ( 236 ) : 433 – 460 , 1950 . S . R . Turner . The Creative Process : A Computer Model of Creativity and Storytelling . Lawrence Erlbaum Associates , Inc , Hillsdale , NJ , 1994 . A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . N . Gomez , L . Kaiser , and I . Polo - sukhin . Attention is all you need . In Advances in Neural Information Processing Systems ( NIPS’17 ) , 2017 . T . Waite . Ai - generated artworks are disappointing at auction , 2019 . https : / / www . dazeddigital . com / art - photography / article / 46839 / 1 / ai - generated - artworks - disappointing - at - auction - obvious - artificial - intelligence [ Retrieved on January 26 , 2023 ] . L . Weidinger , J . Uesato , M . Rauh , C . Grifﬁn , P . - S . Huang , J . Mellor , A . Glaese , M . Cheng , B . Balle , A . Kasirzadeh , C . Biles , S . Brown , Z . Kenton , W . Hawkins , T . Stepleton , A . Birhane , L . A . Hen - dricks , L . Rimell , W . Isaac , J . Haas , S . Legassick , G . Irving , and I . Gabriel . Taxonomy of risks posed by language models . In Proceedings of the 2022 ACM Conference on Fairness , Accountability , and Transparency ( FAccT’22 ) , page 214 – 229 , 2022 . L . Wertz and J . Kuhn . Adapting transformer language models for application in computational creativity : Generating german theater plays with varied topics . In Proceedings of the 13th Inter - national Conference on Computational Creativity ( ICCC’22 ) , 2022 . G . A . Wiggins . A preliminary framework for description , analysis and comparison of creative systems . Knowledge - Based Systems , 19 ( 7 ) : 449 – 458 , 2006 . T . Wu , M . Caccia , Z . Li , Y . - F . Li , G . Qi , and G . Haffari . Pretrained language model in continual learning : A comparative study . In Proceedings of the 2022 International Conference on Learning Representations ( ICLR’22 ) , 2022 . 16 L . Yu , W . Zhang , J . Wang , and Y . Yu . SeqGAN : Sequence generative adversarial nets with pol - icy gradient . In Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence ( AAAI’17 ) , page 2852 – 2858 , 2017 . Y . Zhang , Z . Gan , K . Fan , Z . Chen , R . Henao , D . Shen , and L . Carin . Adversarial feature match - ing for text generation . In Proceedings of the 34th International Conference on Machine Learning ( ICML’17 ) , page 4006 – 4015 , 2017 . D . M . Ziegler , N . Stiennon , J . Wu , T . B . Brown , A . Radford , D . Amodei , P . Christiano , and G . Irving . Fine - tuning language models from human preferences , 2019 . arXiv : 1909 . 08593 [ cs . CL ] . A . Zugarini , S . Melacci , and M . Maggini . Neural Poetry : Learning to Generate Poems Using Sylla - bles . In Proceedings of the 2019 International Conference on Artiﬁcial Neural Networks ( ICANN’19 ) , pages 313 – 325 , 2019 . 17