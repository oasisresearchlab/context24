Improving Recommendation Lists Through Topic Diversiﬁcation Cai - Nicolas Ziegler 1 ∗ Sean M . McNee 2 1 Institut f¨ur Informatik , Universit¨at Freiburg Georges - K¨ohler - Allee , Geb¨aude Nr . 51 79110 Freiburg i . Br . , Germany { cziegler , lausen } @ informatik . uni - freiburg . de Joseph A . Konstan 2 Georg Lausen 1 2 GroupLens Research , Univ . of Minnesota 4 - 192 EE / CS Building , 200 Union St . SE Minneapolis , MN 55455 , USA { mcnee , konstan } @ cs . umn . edu ABSTRACT In this work we present topic diversiﬁcation , a novel method designed to balance and diversify personalized recommenda - tion lists in order to reﬂect the user’s complete spectrum of interests . Though being detrimental to average accuracy , we show that our method improves user satisfaction with rec - ommendation lists , in particular for lists generated using the common item - based collaborative ﬁltering algorithm . Our work builds upon prior research on recommender sys - tems , looking at properties of recommendation lists as en - tities in their own right rather than speciﬁcally focusing on the accuracy of individual recommendations . We introduce the intra - list similarity metric to assess the topical diver - sity of recommendation lists and the topic diversiﬁcation approach for decreasing the intra - list similarity . We evalu - ate our method using book recommendation data , including oﬄine analysis on 361 , 349 ratings and an online study in - volving more than 2 , 100 subjects . Categories and Subject Descriptors H . 3 . 3 [ Information Storage and Retrieval ] : Information Retrieval and Search— Information Filtering ; I . 2 . 6 [ Artiﬁ - cial Intelligence ] : Learning— Knowledge Acquisition General Terms Algorithms , Experimentation , Human Factors , Measurement Keywords Collaborative ﬁltering , diversiﬁcation , accuracy , recommend - er systems , metrics 1 . INTRODUCTION Recommender systems [ 23 ] intend to provide people with recommendations of products they will appreciate , based on their past preferences , history of purchase , and demographic information . Many of the most successful systems make use of collaborative ﬁltering [ 27 , 8 , 11 ] , and numerous commer - cial systems , e . g . , Amazon . com’s recommender [ 16 ] , exploit ∗ Researched while at GroupLens Research in Minneapolis . Copyright is held by the International World Wide Web Conference Com - mittee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2005 , May 10 - 14 , 2005 , Chiba , Japan . ACM 1 - 59593 - 046 - 9 / 05 / 0005 . these techniques to oﬀer personalized recommendation lists to their customers . Though the accuracy of state - of - the - art collaborative ﬁl - tering systems , i . e . , the probability that the active user 1 will appreciate the products recommended , is excellent , some im - plications aﬀecting user satisfaction have been observed in practice . Thus , on Amazon . com ( http : / / www . amazon . com ) , many recommendations seem to be “similar” with respect to content . For instance , customers that have purchased many of Hermann Hesse’s prose may happen to obtain recom - mendation lists where all top - 5 entries contain books by that respective author only . When considering pure accu - racy , all these recommendations appear excellent since the active user clearly appreciates books written by Hermann Hesse . On the other hand , assuming that the active user has several interests other than Hermann Hesse , e . g . , his - torical novels in general and books about world travel , the recommended set of items appears poor , owing to its lack of diversity . Traditionally , recommender system projects have focused on optimizing accuracy using metrics such as precision / recall or mean absolute error . Now research has reached the point where going beyond pure accuracy and toward real user ex - perience becomes indispensable for further advances [ 10 ] . This work looks speciﬁcally at impacts of recommendation lists , regarding them as entities in their own right rather than mere aggregations of single and independent sugges - tions . 1 . 1 Contributions We address the afore - mentioned deﬁciencies by focusing on techniques that are centered on real user satisfaction rather than pure accuracy . The contributions we make in this paper are the following : • Topic diversiﬁcation . We propose an approach to - wards balancing top - N recommendation lists accord - ing to the active user’s full range of interests . Our novel method takes into consideration both the accuracy of suggestions made , and the user’s extent of interest in speciﬁc topics . Analyses of topic diversiﬁcation’s im - plications on user - based [ 11 , 22 ] and item - based [ 26 , 5 ] collaborative ﬁltering are provided . 1 The term “active user” refers to the person for whom rec - ommendations are made . • Intra - list similarity metric . Regarding diversity as an important ingredient to user satisfaction , metrics able to measure that characteristic feature are required . We propose the intra - list similarity metric as an eﬃ - cient means for measurement , complementing existing accuracy metrics in their eﬀorts to capture user satis - faction . • Accuracy versus satisfaction . There have been sev - eral eﬀorts in the past arguing that “accuracy does not tell the whole story” [ 4 , 12 ] . Nevertheless , no evidence has been given to show that some aspects of actual user satisfaction reach beyond accuracy . We close this gap and provide analysis from large - scale online and oﬄine evaluations , matching results obtained from ac - curacy metrics against actual user satisfaction and in - vestigating interactions and deviations between both concepts . 1 . 2 Organization Our paper is organized as follows . We discuss collabora - tive ﬁltering and its two most prominent implementations in Section 2 . The subsequent section then brieﬂy reports on common evaluation metrics and the new intra - list similarity metric . In Section 4 , we present our method for diversify - ing lists , describing its primary motivation and algorithmic clockwork . Section 5 reports on our oﬄine and online exper - iments with topic diversiﬁcation and provides ample discus - sion of results obtained . 2 . ON COLLABORATIVE FILTERING Collaborative ﬁltering ( CF ) still represents the most com - monly adopted technique in crafting academic and commer - cial [ 16 ] recommender systems . Its basic idea refers to mak - ing recommendations based upon ratings that users have assigned to products . Ratings can either be explicit , i . e . , by having the user state his opinion about a given product , or implicit , when the mere act of purchasing or mentioning of an item counts as an expression of appreciation . While im - plicit ratings are generally more facile to collect , their usage implies adding noise to the collected information [ 20 ] . 2 . 1 User - based Collaborative Filtering User - based CF has been explored in - depth during the last ten years [ 29 , 24 , 14 ] and represents the most popular recom - mendation algorithm [ 11 ] , owing to its compelling simplicity and excellent quality of recommendations . CF operates on a set of users A = { a 1 , a 2 , . . . , a n } , a set of products B = { b 1 , b 2 , . . . , b m } , and partial rating functions r i : B → [ − 1 , + 1 ] ⊥ for each user a i . Negative values r i ( b k ) denote utter dislike , while positive values express a i ’s liking of product b k . If ratings are implicit only , we represent them by set R i ⊆ B , equivalent to { b k ∈ B | r i ( b k ) 6 = ⊥ } . The user - based CF’s working process can be broken down into two major steps : • Neighborhood formation . Assuming a i as the ac - tive user , similarity values c ( a i , a j ) ∈ [ − 1 , + 1 ] for all a j ∈ A \ { a i } are computed , based upon the similarity of their respective rating functions r i , r j . In general , Pearson correlation [ 29 , 8 ] or cosine distance [ 11 ] are used for computing c ( a i , a j ) . The top - M most sim - ilar users a j become members of a i ’s neighborhood , clique ( a i ) ⊆ A . • Rating prediction . Taking all the products b k that a i ’s neighbors a j ∈ clique ( a i ) have rated and which are new to a i , i . e . , r i ( b k ) = ⊥ , a prediction of liking w i ( b k ) is produced . Value w i ( b k ) hereby depends on both the similarity c ( a i , a j ) of voters a j with r j ( b k ) 6 = ⊥ , as well as the ratings r j ( b k ) these neighbors a j assigned to b k . Eventually , a list P w i : { 1 , 2 , . . . , N } → B of top - N recom - mendations is computed , based upon predictions w i . Note that function P w i is injective and reﬂects recommendation ranking in descending order , giving highest predictions ﬁrst . 2 . 2 Item - based Collaborative Filtering Item - based CF [ 13 , 26 , 5 ] has been gaining momentum over the last ﬁve years by virtue of favorable computational complexity characteristics and the ability to decouple the model computation process from actual prediction making . Speciﬁcally for cases where | A | (cid:29) | B | , item - based CF’s com - putational performance has been shown superior to user - based CF [ 26 ] . Its success also extends to many commercial recommender systems , such as Amazon . com’s [ 16 ] . As with user - based CF , recommendation making is based upon ratings r i ( b k ) that users a i ∈ A provided for products b k ∈ B . However , unlike user - based CF , similarity values c are computed for items rather than users , hence c : B × B → [ − 1 , + 1 ] . Roughly speaking , two items b k , b e are similar , i . e . , have large c ( b k , b e ) , if users who rate one of them tend to rate the other , and if users tend to assign them identical or similar ratings . Moreover , for each b k , its neighborhood clique ( b k ) ⊆ B of top - M most similar items is deﬁned . Predictions w i ( b k ) are computed as follows : w i ( b k ) = P b e ∈ B 0 k ( c ( b k , b e ) · r i ( b e ) ) P b e ∈ B 0 k | c ( b k , b e ) | , ( 1 ) where B 0 k : = { b e | b e ∈ clique ( b k ) ∧ r i ( b k ) 6 = ⊥ } Intuitively , the approach tries to mimic real user behav - ior , having user a i judge the value of an unknown product b k by comparing the latter to known , similar items b e and considering how much a i appreciated these b e . The eventual computation of a top - N recommendation list P w i follows the user - based CF’s process , arranging rec - ommendations according to w i in descending order . 3 . EVALUATION METRICS Evaluation metrics are essential in order to judge the qual - ity and performance of recommender systems , even though they are still in their infancies . Most evaluations concentrate on accuracy measurements only and neglect other factors , e . g . , novelty and serendipity of recommendations , and the diversity of the recommended list’s items . The following sections give an outline of popular metrics . An extensive survey of accuracy metrics is provided in [ 12 ] . 3 . 1 Accuracy Metrics Accuracy metrics have been deﬁned ﬁrst and foremost for two major tasks : First , to judge the accuracy of single predictions , i . e . , how much predictions w i ( b k ) for products b k deviate from a i ’s ac - tual ratings r i ( b k ) . These metrics are particularly suited for tasks where predictions are displayed along with the prod - uct , e . g . , annotation in context [ 12 ] . Second , decision - support metrics evaluate the eﬀective - ness of helping users to select high - quality items from the set of all products , generally supposing binary preferences . 3 . 1 . 1 Predictive Accuracy Metrics Predictive accuracy metrics measure how close predicted ratings come to true user ratings . Most prominent and widely used [ 29 , 11 , 3 , 9 ] , mean absolute error ( MAE ) represents an eﬃcient means to measure the statistical accuracy of predic - tions w i ( b k ) for sets B i of products : | E | = P b k ∈ B i | r i ( b k ) − w i ( b k ) | | B i | ( 2 ) Related to MAE , mean squared error ( MSE ) squares the error before summing . Hence , large errors become much more pronounced than small ones . Very easy to implement , predictive accuracy metrics are inapt for evaluating the quality of top - N recommendation lists . Users only care about errors for high - rank products . On the other hand , prediction errors for low - rank products are unimportant , knowing that the user has no interest in them anyway . However , MAE and MSE account for both types of errors in exactly the same fashion . 3 . 1 . 2 Decision - Support Metrics Precision and recall , both well - known from information retrieval , do not consider predictions and their deviations from actual ratings . They rather judge how relevant a set of ranked recommendations is for the active user . Before using these metrics for cross - validation , K - folding is applied , dividing every user a i ’s rated products b k ∈ R i into K disjoint slices of preferably equal size . Hereby , K − 1 randomly chosen slices form a i ’s training set R xi . These rat - ings then deﬁne a i ’s proﬁle from which ﬁnal recommenda - tions are computed . For recommendation generation , a i ’s residual slice ( R i \ R xi ) is retained and not used for predic - tion . This slice , denoted T xi , constitutes the test set , i . e . , those products the recommenders intend to predict . Sarwar [ 25 ] presents an adapted variant of recall , record - ing the percentage of test set products b ∈ T xi occurring in recommendation list P xi with respect to the overall number of test set products | T xi | : Recall = 100 · | T xi ∩ = P xi | | T xi | ( 3 ) Symbol = P xi denotes the image of map P xi , i . e . , all items part of the recommendation list . Accordingly , precision represents the percentage of test set products b ∈ T xi occurring in P xi with respect to the size of the recommendation list : Precision = 100 · | T xi ∩ = P xi | | = P xi | ( 4 ) Breese et al . [ 3 ] introduce an interesting extension to re - call , known as weighted recall or Breese score . The approach takes into account the order of the top - N list , penalizing in - correct recommendations less severely the further down the list they occur . Penalty decreases with exponential decay . Other popular decision - support metrics include ROC [ 28 , 18 , 9 ] , the “receiver operating characteristic” . ROC mea - sures the extent to which an information ﬁltering system is able to successfully distinguish between signal and noise . Less frequently used , NDPM [ 2 ] compares two diﬀerent , weakly ordered rankings . 3 . 2 Beyond Accuracy Though accuracy metrics are an important facet of useful - ness , there are traits of user satisfaction they are unable to capture . However , non - accuracy metrics have largely been denied major research interest so far . 3 . 2 . 1 Coverage Among all non - accuracy evaluation metrics , coverage has been the most frequently used [ 11 , 19 , 9 ] . Coverage measures the percentage of elements part of the problem domain for which predictions can be made . 3 . 2 . 2 Novelty and Serendipity Some recommenders produce highly accurate results that are still useless in practice , e . g . , suggesting bananas to cus - tomers in grocery stores . Though being highly accurate , note that almost everybody likes and buys bananas . Hence , their recommending appears far too obvious and of little help to the shopper . Novelty and serendipity metrics thus measure the “non - obviousness” of recommendations made , avoiding “cherry - picking” [ 12 ] . For some simple measure of serendipity , take the average popularity of recommended items . Lower scores obtained denote higher serendipity . 3 . 3 Intra - List Similarity We present a new metric that intends to capture the diver - sity of a list . Hereby , diversity may refer to all kinds of fea - tures , e . g . , genre , author , and other discerning characteris - tics . Based upon an arbitrary function c ◦ : B × B → [ − 1 , + 1 ] measuring the similarity c ◦ ( b k , b e ) between products b k , b e according to some custom - deﬁned criterion , we deﬁne intra - list similarity for a i ’s list P w i as follows : ILS ( P w i ) = X b k ∈ = P wi X b e ∈ = P wi , b k 6 = b e c ◦ ( b k , b e ) 2 ( 5 ) Higher scores denote lower diversity . An interesting math - ematical feature of ILS ( P w i ) we are referring to in later sec - tions is permutation - insensitivity , i . e . , let S N be the sym - metric group of all permutations on N = | P w i | symbols : ∀ σ i , σ j ∈ S N : ILS ( P w i ◦ σ i ) = ILS ( P w i ◦ σ j ) ( 6 ) Hence , simply rearranging positions of recommendations in a top - N list P w i does not aﬀect P w i ’s intra - list similarity . 4 . TOPIC DIVERSIFICATION One major issue with accuracy metrics is their inability to capture the broader aspects of user satisfaction , hiding several blatant ﬂaws in existing systems [ 17 ] . For instance , suggesting a list of very similar items , e . g . , with respect to the author , genre , or topic , may be of little use for the user , even though this list’s average accuracy might be high . The issue has been perceived by other researchers before , coined “portfolio eﬀect” by Ali and van Stam [ 1 ] . We believe that item - based CF systems in particular are susceptible to that eﬀect . Reports from the item - based TV recommender TiVo [ 1 ] , as well as personal experiences with Amazon . com’s recommender , also item - based [ 16 ] , back our conjecture . For instance , one of this paper’s authors only gets recommenda - tions for Heinlein’s books , another complained about all his suggested books being Tolkien’s writings . Reasons for negative ramiﬁcations on user satisfaction im - plied by portfolio eﬀects are well - understood and have been studied extensively in economics , termed “law of diminishing marginal returns” [ 30 ] . The law describes eﬀects of satura - tion that steadily decrease the incremental utility of prod - ucts p when acquired or consumed over and over again . For example , suppose you are oﬀered your favorite drink . Let p 1 denote the price you are willing to pay for that product . Assuming your are oﬀered a second glass of that particular drink , the amount p 2 of money you are inclined to spend will be lower , i . e . , p 1 > p 2 . Same for p 3 , p 4 , and so forth . We propose an approach we call topic diversiﬁcation to deal with the problem at hand and make recommended lists more diverse and thus more useful . Our method represents an extension to existing recommender algorithms and is ap - plied on top of recommendation lists . 4 . 1 Taxonomy - based Similarity Metric Function c ∗ : 2 B × 2 B → [ − 1 , + 1 ] , quantifying the simi - larity between two product sets , forms an essential part of topic diversiﬁcation . We instantiate c ∗ with our metric for taxonomy - driven ﬁltering [ 33 ] , though other content - based similarity measures may appear likewise suitable . Our met - ric computes the similarity between product sets based upon their classiﬁcation . Each product belongs to one or more classes that are hierarchically arranged in classiﬁcation tax - onomies , describing the products in machine - readable ways . Classiﬁcation taxonomies exist for various domains . Ama - zon . com crafts very large taxonomies for books , DVDs , CDs , electronic goods , and apparel . See Figure 1 for one sam - ple taxonomy . Moreover , all products on Amazon . com bear content descriptions relating to these domain taxonomies . Featured topics could include author , genre , and audience . 4 . 2 Topic Diversiﬁcation Algorithm Algorithm 1 shows the complete topic diversiﬁcation algo - rithm , a brief textual sketch is given in the next paragraphs . Function P w i ∗ denotes the new recommendation list , re - sulting from applying topic diversiﬁcation . For every list en - try z ∈ [ 2 , N ] , we collect those products b from the candidate products set B i that do not occur in positions o < z in P w i ∗ and compute their similarity with set { P w i ∗ ( k ) | k ∈ [ 1 , z [ } , which contains all new recommendations preceding rank z . Sorting all products b according to c ∗ ( b ) in reverse order , we obtain the dissimilarity rank P rev c ∗ . This rank is then merged with the original recommendation rank P w i accord - ing to diversiﬁcation factor Θ F , yielding ﬁnal rank P w i ∗ . Factor Θ F deﬁnes the impact that dissimilarity rank P rev c ∗ exerts on the eventual overall output . Large Θ F ∈ [ 0 . 5 , 1 ] fa - vors diversiﬁcation over a i ’s original relevance order , while low Θ F ∈ [ 0 , 0 . 5 [ produces recommendation lists closer to the original rank P w i . For experimental analysis , we used diversiﬁcation factors Θ F ∈ [ 0 , 0 . 9 ] . Note that ordered input lists P w i must be considerably larger than the ﬁnal top - N list . For our later experiments , we used top - 50 input lists for eventual top - 10 recommendations . procedure diversify ( P w i , Θ F ) { B i ← = P w i ; P w i ∗ ( 1 ) ← P w i ( 1 ) ; for z ← 2 to N do set B 0 i ← B i \ { P w i ∗ ( k ) | k ∈ [ 1 , z [ } ; ∀ b ∈ B 0 : compute c ∗ ( { b } , { P w i ∗ ( k ) | k ∈ [ 1 , z [ } ) ; compute P c ∗ : { 1 , 2 , . . . , | B 0 i | } → B 0 i using c ∗ ; for all b ∈ B 0 i do P rev − 1 c ∗ ( b ) ← | B 0 i | − P − 1 c ∗ ( b ) ; w ∗ i ( b ) ← P − 1 w i ( b ) · ( 1 − Θ F ) + P rev − 1 c ∗ ( b ) · Θ F ; end do P w i ∗ ( z ) ← min { w ∗ i ( b ) | b ∈ B 0 i } ; end do return P w i ∗ ; } Algorithm 1 : Sequential topic diversiﬁcation 4 . 3 Recommendation Dependency In order to implement topic diversiﬁcation , we assume that recommended products P w i ( o ) and P w i ( p ) , o , p ∈ N , along with their content descriptions , eﬀectively do exert an impact on each other , which is commonly ignored by ex - isting approaches : usually , only relevance weight ordering o < p ⇒ w i ( P w i ( o ) ) ≥ w i ( P w i ( p ) ) must hold for recommen - dation list items , no other dependencies are assumed . In case of topic diversiﬁcation , recommendation interde - pendence means that an item b ’s current dissimilarity rank with respect to preceding recommendations plays an impor - tant role and may inﬂuence the new ranking . 4 . 4 Osmotic Pressure Analogy The eﬀect of dissimilarity bears traits similar to that of os - motic pressure and selective permeability known from molec - ular biology [ 31 ] . Steady insertion of products b o , taken from one speciﬁc area of interest d o , into the recommendation list equates to the passing of molecules from one speciﬁc substance through the cell membrane into cytoplasm . With increasing concentration of d o , owing to the membrane’s se - lective permeability , the pressure for molecules b from other substances d rises . When pressure gets suﬃciently high for one given topic d p , its best products b p may “diﬀuse” into the recommendation list , even though their original rank P − 1 w i ( b ) might be inferior to candidates from the prevailing domain d o . Consequently , pressure for d p decreases , paving the way for another domain for which pressure peaks . Topic diversiﬁcation hence resembles the membrane’s se - lective permeability , which allows cells to maintain their in - ternal composition of substances at required levels . 5 . EMPIRICAL ANALYSIS We conducted oﬄine evaluations to understand the ram - iﬁcations of topic diversiﬁcation on accuracy metrics , and online analysis to investigate how our method aﬀects ac - tual user satisfaction . We applied topic diversiﬁcation with Θ F ∈ { 0 , 0 . 1 , 0 . 2 , . . . 0 . 9 } to lists generated by both user - based CF and item - based CF , observing eﬀects that occur Books Science Mathematics Reference Sports Archaelogy Medicine Applied Pure Nonfiction Discrete Algebra History Astronomy Figure 1 : Fragment from the Amazon . com book taxonomy when steadily increasing Θ F and analyzing how both ap - proaches respond to diversiﬁcation . 5 . 1 Dataset Design We based online and oﬄine analyses on data we gathered from BookCrossing ( http : / / www . bookcrossing . com ) . The lat - ter community caters for book lovers exchanging books all around the world and sharing their experiences with others . 5 . 1 . 1 Data Collection In a 4 - week crawl , we collected data on 278 , 858 members of BookCrossing and 1 , 157 , 112 ratings , both implicit and explicit , referring to 271 , 379 distinct ISBNs . Invalid ISBNs were excluded from the outset . The complete BookCrossing dataset , featuring fully anon - ymized information , is available via the ﬁrst author’s home - page ( http : / / www . informatik . uni - freiburg . de / ∼ cziegler ) . Next , we mined Amazon . com’s book taxonomy , compris - ing 13 , 525 distinct topics . In order to be able to apply topic diversiﬁcation , we mined content information , focusing on taxonomic descriptions that relate books to taxonomy nodes from Amazon . com . Since many books on BookCrossing refer to rare , non - English books , or outdated titles not in print anymore , we were able to garner background knowledge for only 175 , 721 books . In total , 466 , 573 topic descriptors were found , giving an average of 2 . 66 topics per book . 5 . 1 . 2 Condensation Steps Owing to the BookCrossing dataset’s extreme sparsity , we decided to further condense the set in order to obtain more meaningful results from CF algorithms when computing rec - ommendations . Hence , we discarded all books missing taxo - nomic descriptions , along with all ratings referring to them . Next , we also removed book titles with fewer than 20 overall mentions . Only community members with at least 5 ratings each were kept . The resulting dataset’s dimensions were considerably more moderate , featuring 10 , 339 users , 6 , 708 books , and 361 , 349 book ratings . 5 . 2 Ofﬂine Experiments We performed oﬄine experiments comparing precision , re - call , and intra - list similarity scores for 20 diﬀerent recom - mendation list setups . Half these recommendation lists were based upon user - based CF with diﬀerent degrees of diver - siﬁcation , the others on item - based CF . Note that we did not compute MAE metric values since we are dealing with implicit rather than explicit ratings . 5 . 2 . 1 Evaluation Framework Setup For cross - validation of precision and recall metrics of all 10 , 339 users , we adopted K - folding with parameter K = 4 . Hence , rating proﬁles R i were eﬀectively split into training sets R xi and test sets T xi , x ∈ { 1 , . . . , 4 } , at a ratio of 3 : 1 . For each of the 41 , 356 diﬀerent training sets , we computed 20 top - 10 recommendation lists . To generate the diversiﬁed lists , we computed top - 50 lists based upon pure , i . e . , non - diversiﬁed , item - based CF and pure user - based CF . The high - performance Suggest recom - mender engine 2 was used to compute these base case lists . Next , we applied the diversiﬁcation algorithm to both base cases , applying Θ F factors ranging from 10 % up to 90 % . For evaluation , all lists were truncated to contain 10 books only . 5 . 2 . 2 Result Analysis We were interested in seeing how accuracy , captured by precision and recall , behaves when increasing Θ F from 0 . 1 up to 0 . 9 . Since topic diversiﬁcation may make books with high predicted accuracy trickle down the list , we hypothesized that accuracy will deteriorate for Θ F → 0 . 9 . Moreover , in order to ﬁnd out if our novel algorithm has any signiﬁcant , positive eﬀects on the diversity of items featured , we also applied our intra - list similarity metric . An overlap analysis for diversiﬁed lists , Θ F ≥ 0 . 1 , versus their respective non - diversiﬁed pendants indicates how many items stayed the same for increasing diversiﬁcation factors . 5 . 2 . 2 . 1 Precision and Recall . First , we analyzed precision and recall scores for both non - diversiﬁed base cases , i . e . , when Θ F = 0 . Table 1 states that user - based and item - based CF exhibit almost identical accu - racy , indicated by precision values . Their recall values diﬀer 2 Visit http : / / www - users . cs . umn . edu / ∼ karypis / suggest / . 1 2 3 4 5 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) P r e c i s i on Item - based CF User - based CF ( a ) 2 3 4 5 6 7 8 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) R e c a ll Item - based CF User - based CF ( b ) Figure 2 : Precision ( a ) and recall ( b ) for increasing Θ F Item - based CF User - based CF Precision 3 . 64 3 . 69 Recall 7 . 32 5 . 76 Table 1 : Precision / recall for non - diversiﬁed CF considerably , hinting at deviating behavior with respect to the types of users they are scoring for . Next , we analyzed the behavior of user - based and item - based CF when steadily increasing Θ F by increments of 10 % , depicted in Figure 2 . The two charts reveal that diversiﬁca - tion has detrimental eﬀects on both metrics and on both CF algorithms . Interestingly , corresponding precision and recall curves have almost identical shape . The loss in accuracy is more pronounced for item - based than for user - based CF . Furthermore , for either metric and either CF algorithm , the drop is most distinctive for Θ F ∈ [ 0 . 2 , 0 . 4 ] . For lower Θ F , negative impacts on accuracy are marginal . We believe this last observation due to the fact that precision and recall are permutation - insensitive , i . e . , the mere order of recommendations within a top - N list does not inﬂuence the metric value , as opposed to Breese score [ 3 , 12 ] . However , for low Θ F , the pressure that the dissimilarity rank exerts on the top - N list’s makeup is still too weak to make many new items diﬀuse into the top - N list . Hence , we conjecture that rather the positions of current top - N items change , which does not aﬀect either precision or recall . 5 . 2 . 2 . 2 Intra - List Similarity . Knowing that our diversiﬁcation method exerts a signif - icant , negative impact on accuracy metrics , we wanted to know how our approach aﬀected the intra - list similarity mea - sure . Similar to the precision and recall experiments , we computed metric values for user - based and item - based CF with Θ F ∈ [ 0 , 0 . 9 ] each . Hereby , we instantiated the intra - list similarity metric function c ◦ with our taxonomy - driven metric c ∗ . Results obtained from intra - list similarity analysis are given in Figure 3 ( a ) . The topic diversiﬁcation method considerably lowers the pairwise similarity between list items , thus making top - N recommendation lists more diverse . Diversiﬁcation appears to aﬀect item - based CF stronger than its user - based coun - terpart , in line with our ﬁndings about precision and recall . For lower Θ F , curves are less steep than for Θ F ∈ [ 0 . 2 , 0 . 4 ] , which also well aligns with precision and recall analysis . Again , the latter phenomenon can be explained by one of the metric’s inherent features , i . e . , like precision and recall , intra - list similarity is permutation - insensitive . 5 . 2 . 2 . 3 Original List Overlap . Figure 3 ( b ) shows the number of recommended items stay - ing the same when increasing Θ F with respect to the original list’s content . Both curves exhibit roughly linear shapes , be - ing less steep for low Θ F , though . Interestingly , for factors Θ F ≤ 0 . 4 , at most 3 recommendations change on average . 5 . 2 . 2 . 4 Conclusion . We found that diversiﬁcation appears detrimental to both user - based and item - based CF along precision and recall metrics . In fact , this outcome aligns with our expectations , considering the nature of those two accuracy metrics and the way that the topic diversiﬁcation method works . More - over , we found that item - based CF seems more susceptible to topic diversiﬁcation than user - based CF , backed by re - sults from precision , recall and intra - list similarity metric analysis . 5 . 3 Online Experiments Oﬄine experiments helped us in understanding the impli - cations of topic diversiﬁcation on both CF algorithms . We could also observe that the eﬀects of our approach are dif - ferent on diﬀerent algorithms . However , knowing about the deﬁciencies of accuracy metrics , we wanted to assess actual user satisfaction for various degrees of diversiﬁcation , thus necessitating an online survey . For the online study , we computed each recommendation list type anew for users in the denser BookCrossing dataset , 0 2 4 6 8 10 12 14 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) I n t r a - L i s t S i m il a r i t y Item - based CF User - based CF ( a ) 2 4 6 8 10 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) Item - based CF User - based CF O v e r l ap w i t h Θ F = 0 ( b ) Figure 3 : Intra - list similarity behavior ( a ) and overlap with original list ( b ) for increasing Θ F though without K - folding . In cooperation with BookCross - ing , we mailed all eligible users via the community mailing system , asking them to participate in our online study . Each mail contained a personal link that would direct the user to our online survey pages . In order to make sure that only the users themselves would complete their survey , links con - tained unique , encrypted access codes . During the 3 - week survey phase , 2 , 125 users participated and completed the study . 5 . 3 . 1 Survey Outline and Setup The survey consisted of several screens that would tell the prospective participant about this study’s nature and his task , show all his ratings used for making recommen - dations , and ﬁnally present a top - 10 recommendation list , asking several questions thereafter . For each book , users could state their interest on a 5 - point rating scale . Scales ranged from “not much” to “very much” , mapped to values 1 to 4 , and oﬀered the user to indicate that he had already read the book , mapped to value 5 . In order to successfully complete the study , users were not required to rate all their top - 10 recommendations . Neutral values were assumed for non - votes instead . However , we required users to answer all further questions , concerning the list as a whole rather than its single recommendations , before submitting their results . We embedded those questions we were actually keen about knowing into ones of lesser importance , in order to conceal our intentions and not bias users . The one top - 10 recommendation list for each user was cho - sen among 12 candidate lists , either user - based CF or item - based with Θ F ∈ { 0 , 0 . 3 , 0 . 4 , 0 . 5 , 0 . 7 , 0 . 9 } each . We opted for those 12 instead of all 20 list types in order to acquire enough users completing the survey for each slot . The assignment of a speciﬁc list to the current user was done dynamically , at the time of the participant entering the survey , and in a round - robin fashion . Thus , we could guarantee that the number of users per list type was roughly identical . 5 . 3 . 2 Result Analysis For the analysis of our inter - subject survey , we were mostly interested in the following three aspects . First , the average rating users gave to their 10 single recommendations . We expected results to roughly align with scores obtained from precision and recall , owing to the very nature of these met - rics . Second , we wanted to know if users perceived their list as well - diversiﬁed , asking them to tell whether the lists re - ﬂected rather a broad or narrow range of their reading inter - ests . Referring to the intra - list similarity metric , we expected users’ perceived range of topics , i . e . , the list’s diversity , to increase with increasing Θ F . Third , we were curious about the overall satisfaction of users with their recommendation lists in their entirety , the measure to compare performance . Both latter - mentioned questions were answered by each user on a 5 - point likert scale , higher scores denoting better performance , and we averaged the eventual results by the number of users . Statistical signiﬁcance of all mean values was measured by parametric one - factor ANOVA , where p < 0 . 05 if not indicated otherwise . 5 . 3 . 2 . 1 Single - Vote Averages . Users perceived recommendations made by user - based CF systems on average as more accurate than those made by item - based CF systems , as depicted in Figure 4 ( a ) . At each featured diversiﬁcation level Θ F , diﬀerences between the two CF types are statistically signiﬁcant , p (cid:28) 0 . 01 . Moreover , for each algorithm , higher diversiﬁcation fac - tors obviously entail lower single - vote average scores , which conﬁrms our hypothesis stated before . The item - based CF’s cusp at Θ F ∈ [ 0 . 3 , 0 . 5 ] appears as a notable outlier , op - posed to the trend , but diﬀerences between the 3 means at Θ F ∈ [ 0 . 3 , 0 . 5 ] are not statistically signiﬁcant , p > 0 . 15 . Contrarily , diﬀerences between all factors Θ F are signiﬁcant for item - based CF , p (cid:28) 0 . 01 , and for user - based CF , p < 0 . 1 . Hence , topic diversiﬁcation negatively correlates with pure accuracy . Besides , users perceived the performance of user - based CF as signiﬁcantly better than item - based CF for all corresponding levels Θ F . 5 . 3 . 2 . 2 Covered Range . Next , we analyzed whether users actually perceived the variety - augmenting eﬀects caused by topic diversiﬁcation , illustrated before through the measurement of intra - list sim - 2 . 4 2 . 6 2 . 8 3 3 . 2 3 . 4 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) S i ng l e - V o t e A v e r age s Item - based CF User - based CF ( a ) 2 . 6 2 . 8 3 3 . 2 3 . 4 3 . 6 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) C o v e r ed R ange Item - based CF User - based CF ( b ) 3 3 . 1 3 . 2 3 . 3 3 . 4 3 . 5 3 . 6 0 10 20 30 40 50 60 70 80 90 Diversification Factor ( in % ) O v e r a l l L i s t V a l ue Item - based CF User - based CF ( c ) Figure 4 : Results for single - vote averages ( a ) , covered range of interests ( b ) , and overall satisfaction ( c ) ilarity . Users’ reactions to steadily incrementing Θ F are il - lustrated in Figure 4 ( b ) . First , between both algorithms on corresponding Θ F levels , only the diﬀerence of means at Θ F = 0 . 3 shows statistical signiﬁcance . Studying the trend of user - based CF for increasing Θ F , we notice that the perceived range of reading interests covered by users’ recommendation lists also increases . Hereby , the curve’s ﬁrst derivative maintains an approximately constant level , exhibiting slight peaks between Θ F ∈ [ 0 . 4 , 0 . 5 ] . Statis - tical signiﬁcance holds for user - based CF between means at Θ F = 0 and Θ F > 0 . 5 , and between Θ F = 0 . 3 and Θ F = 0 . 9 . On the contrary , the item - based curve exhibits a drasti - cally diﬀerent behavior . While soaring at Θ F = 0 . 3 to 3 . 186 , reaching a score almost identical to the user - based CF’s peak at Θ F = 0 . 9 , the curve barely rises for Θ F ∈ [ 0 . 4 , 0 . 9 ] , remaining rather stable and showing a slight , though in - signiﬁcant , upward trend . Statistical signiﬁcance was shown for Θ F = 0 with respect to all other samples taken from Θ F ∈ [ 0 . 3 , 0 . 9 ] . Hence , our online results do not perfectly align with ﬁndings obtained from oﬄine analysis . While the intra - list similarity chart in Figure 3 indicates that diversity increases when increasing Θ F , the item - based CF chart de - ﬁes this trend , ﬁrst soaring then ﬂattening . We conjecture that the following three factors account for these peculiari - ties : • Diversiﬁcation factor impact . Our oﬄine analysis of the intra - list similarity already suggested that the eﬀect of topic diversiﬁcation on item - based CF is much stronger than on user - based CF . Thus , the item - based CF’s user - perceived interest coverage is signiﬁcantly higher at Θ F = 0 . 3 than the user - based CF’s . • Human perception . We believe that human percep - tion can capture the level of diversiﬁcation inherent to a list only to some extent . Beyond that point , in - creasing diversity remains unnoticed . For the appli - cation scenario at hand , Figure 4 suggests this point around score value 3 . 2 , reached by user - based CF only at Θ F = 0 . 9 , and approximated by item - based CF al - ready at Θ F = 0 . 3 . • Interaction with accuracy . Analyzing results ob - tained , bear in mind that covered range scores are not fully independent from single - vote averages . When ac - curacy is poor , i . e . , the user feels unable to identify recommendations that are interesting to him , chances are high his discontentment will also negatively aﬀect his diversity rating . For Θ F ∈ [ 0 . 5 , 0 . 9 ] , single - vote av - erages are remarkably low , which might explain why perceived coverage scores do not improve for increasing Θ F . However , we may conclude that users do perceive the ap - plication of topic diversiﬁcation as an overly positive eﬀect on reading interest coverage . 5 . 3 . 2 . 3 Overall List Value . The third feature variable we were evaluating , the overall value users assigned to their personal recommendation list , eﬀectively represents the target value of our studies , mea - suring actual user satisfaction . Owing to our conjecture that user satisfaction is a mere composite of accuracy and other inﬂuential factors , such as the list’s diversity , we hypothe - sized that the application of topic diversiﬁcation would in - crease satisfaction . At the same time , considering the down - ward trend of precision and recall for increasing Θ F , in ac - cordance with declining single - vote averages , we expected user satisfaction to drop oﬀ for large Θ F . Hence , we sup - posed an arc - shaped curve for both algorithms . Results for overall list value are given in Figure 4 ( c ) . Ana - lyzing user - based CF , we observe that the curve does not fol - low our hypothesis . Slightly improving at Θ F = 0 . 3 over the non - diversiﬁed case , scores drop for Θ F ∈ [ 0 . 4 , 0 . 7 ] , eventu - ally culminating in a slight but visible upturn at Θ F = 0 . 9 . While lacking reasonable explanations and being opposed to our hypothesis , the curve’s data - points de facto bear no statistical signiﬁcance for p < 0 . 1 . Hence , we conclude that topic diversiﬁcation has a marginal , largely negligible impact on overall user satisfaction , initial positive eﬀects eventually being oﬀset by declining accuracy . On the contrary , for item - based CF , results obtained look diﬀerent . In compliance with our previous hypothesis , the curve’s shape roughly follows an arc , peaking at Θ F = 0 . 4 . Taking the three data - points deﬁning the arc , we obtain sta - tistical signiﬁcance for p < 0 . 1 . Since the endpoint’s score at Θ F = 0 . 9 is inferior to the non - diversiﬁed case’s , we observe that too much diversiﬁcation appears detrimental , perhaps owing to substantial interactions with accuracy . Eventually , for overall list value analysis , we come to con - clude that topic diversiﬁcation has no measurable eﬀects on user - based CF , but signiﬁcantly improves item - based CF performance for diversiﬁcation factors Θ F around 40 % . 5 . 4 Multiple Linear Regression Results obtained from analyzing user feedback along var - ious feature axes already indicated that users’ overall satis - faction with recommendation lists not only depends on ac - curacy , but also on the range of reading interests covered . In order to more rigidly assess that indication by means of statistical methods , we applied multiple linear regression to our survey results , choosing the overall list value as depen - dent variable . As independent input variables , we provided single - vote averages and covered range , both appearing as ﬁrst - order and second - order polynomials , i . e . , SVA and CR , and SVA 2 and CR 2 , respectively . We also tried several other , more complex models , without achieving signiﬁcantly better model ﬁtting . Estimate Error t - Value Pr ( > | t | ) ( const ) 3 . 27 0 . 023 139 . 56 < 2 e − 16 SVA 12 . 42 0 . 973 12 . 78 < 2 e − 16 SVA 2 - 6 . 11 0 . 976 - 6 . 26 4 . 76 e − 10 CR 19 . 19 0 . 982 19 . 54 < 2 e − 16 CR 2 - 3 . 27 0 . 966 - 3 . 39 0 . 000727 Multiple R 2 : 0 . 305 , adjusted R 2 : 0 . 303 Table 2 : Multiple linear regression results Analyzing multiple linear regression results , shown in Ta - ble 2 , conﬁdence values Pr ( > | t | ) clearly indicate that sta - tistically signiﬁcant correlations for accuracy and covered range with user satisfaction exist . Since statistical signiﬁ - cance also holds for their respective second - order polynomi - als , i . e . , CR 2 and SVA 2 , we conclude that these relationships are non - linear and more complex , though . As a matter of fact , linear regression delivers a strong in - dication that the intrinsic utility of a list of recommended items is more than just the average value of accuracy votes for all single items , but also depends on the perceived diver - sity . 5 . 5 Limitations There are some limitations to the study , notably referring to the way topic diversiﬁcation was implemented . Though the Amazon . com taxonomies were human - created , there may still be some mismatch between what the topic diversiﬁca - tion algorithm perceives as “diversiﬁed” and what humans do . The issue is eﬀectively inherent to the taxonomy’s struc - ture , which has been designed with browsing tasks and ease of searching rather than with interest proﬁle generation in mind . For instance , the taxonomy features topic nodes la - belled with letters for alphabetical ordering of authors from the same genre , e . g . , Books → Fiction → . . . → Authors , A - Z → G . Hence , two Sci - Fi books from two diﬀerent au - thors with the same initial of their last name would be classi - ﬁed under the same node , while another Sci - Fi book from an author with a diﬀerent last - name initial would not . Though the problem’s impact is largely marginal , owing to the rel - atively deep level of nesting where such branchings occur , the procedure appears far from intuitive . An alternative approach to further investigate the accu - racy of taxonomy - driven similarity measurement , and its limitations , would be to have humans do the clustering , e . g . , by doing card sorts or by estimating the similarity of any two books contained in the book database . The results could then be matched against the topic diversiﬁcation method’s output . 6 . RELATED WORK Few eﬀorts have addressed the problem of making top - N lists more diverse . Only considering literature on collabo - rative ﬁltering and recommender systems in general , none have been presented before , to the best of our knowledge . However , some work related to our topic diversiﬁcation approach can be found in information retrieval , speciﬁcally meta - search engines . A critical aspect of meta - search engine design is the merging of several top - N lists into one single top - N list . Intuitively , this merged top - N list should reﬂect the highest quality ranking possible , also known as the “rank aggregation problem” [ 6 ] . Most approaches use variations of the “linear combination of score” model ( LC ) , described by Vogt and Cottrell [ 32 ] . The LC model eﬀectively resembles our scheme for merging the original , accuracy - based rank - ing with the current dissimilarity ranking , but is more gen - eral and does not address the diversity issue . Fagin et al . [ 7 ] propose metrics for measuring the distance between top - N lists , i . e . , inter - list similarity metrics , in order to evaluate the quality of merged ranks . Oztekin et al . [ 21 ] extend the linear combination approach by proposing rank combination models that also incorporate content - based features in order to identify the most relevant topics . More related to our idea of creating lists that represent the whole plethora of the user’s topic interests , Kummamuru et al . [ 15 ] present their clustering scheme that groups search results into clusters of related topics . The user can then conveniently browse topic folders relevant to his search in - terest . The commercially available search engine Northern Light ( http : / / www . northernlight . com ) incorporates similar functionalities . Google ( http : / / www . google . com ) uses several mechanisms to suppress top - N items too similar in content , showing them only upon the user’s explicit request . Unfor - tunately , no publications on that matter are available . 7 . CONCLUSION We presented topic diversiﬁcation , an algorithmic frame - work to increase the diversity of a top - N list of recommended products . In order to show its eﬃciency in diversifying , we also introduced our new intra - list similarity metric . Contrasting precision and recall metrics , computed both for user - based and item - based CF and featuring diﬀerent levels of diversiﬁcation , with results obtained from a large - scale user survey , we showed that the user’s overall liking of recommendation lists goes beyond accuracy and involves other factors , e . g . , the users’ perceived list diversity . We were thus able to provide empirical evidence that lists are more than mere aggregations of single recommendations , but bear an intrinsic , added value . Though eﬀects of diversiﬁcation were largely marginal on user - based CF , item - based CF performance improved signif - icantly , an indication that there are some behavioral diﬀer - ences between both CF classes . Moreover , while pure item - based CF appeared slightly inferior to pure user - based CF in overall satisfaction , diversifying item - based CF with factors Θ F ∈ [ 0 . 3 , 0 . 4 ] made item - based CF outperform user - based CF . Interestingly for Θ F ≤ 0 . 4 , no more than three items tend to change with respect to the original list , shown in Figure 3 . Small changes thus have high impact . We believe our ﬁndings especially valuable for practical application scenarios , since many commercial recommender systems , e . g . , Amazon . com [ 16 ] and TiVo [ 1 ] , are item - based , owing to the algorithm’s computational eﬃciency . 8 . FUTURE WORK Possible future directions branching out from our current state of research on topic diversiﬁcation are rife . First , we would like to study the impact of topic diversi - ﬁcation when dealing with application domains other than books , e . g . , movies , CDs , and so forth . Results obtained may diﬀer , owing to distinct characteristics concerning the struc - ture of genre classiﬁcation inherent to these domains . For instance , Amazon . com’s classiﬁcation taxonomy for books is more deeply nested , though smaller , than its movie coun - terpart [ 34 ] . Bear in mind that the structure of these tax - onomies severely aﬀects the taxonomy - based similarity mea - sure c ∗ , which lies at the very heart of the topic diversiﬁca - tion method . Another interesting path to follow would be to param - eterize the diversiﬁcation framework with several diﬀerent similarity metrics , either content - based or CF - based , hence superseding the taxonomy - based c ∗ . We strongly believe that our topic diversiﬁcation approach bears particularly high relevance for recommender systems involving sequential consumption of list items . For instance , think of personalized Internet radio stations , e . g . , Yahoo’s Launch ( http : / / launch . yahoo . com ) : community members are provided with playlists , computed according to their own taste , which are sequentially processed and consumed . Con - trolling the right mix of items within these lists becomes vi - tal and even more important than for mere “random - access” recommendation lists , e . g . , book or movie lists . Suppose such an Internet radio station playing ﬁve Sisters of Mercy songs in a row . Though the active user may actually like the re - spective band , he may not want all ﬁve songs played in se - quence . Lack of diversion might thus result in the user leav - ing the system . The problem of ﬁnding the right mix for sequential con - sumption - based recommenders takes us to another future di - rection worth exploring , namely individually adjusting the right level of diversiﬁcation versus accuracy tradeoﬀ . One approach could be to have the user himself deﬁne the de - gree of diversiﬁcation he likes . Another approach might in - volve learning the right parameter from the user’s behavior , e . g . , by observing which recommended items he inspects and devotes more time to , etc . Finally , we are also thinking about diversity metrics other than intra - list similarity . For instance , we envision a metric that measures the extent to which the top - N list actually reﬂects the user’s proﬁle . 9 . ACKNOWLEDGEMENTS The authors would like to express their gratitude towards Ron Hornbaker , CTO of Humankind Systems and chief ar - chitect of BookCrossing , for his invaluable support . Further - more , we would like to thank all BookCrossing members par - ticipating in our online survey for devoting their time and giving us many invaluable comments . Moreover , we would like to thank John Riedl , Dan Cosley , Yongli Zhang , Paolo Massa , Zvi Topol , and Lars Schmidt - Thieme for fruitful comments and discussions . 10 . REFERENCES [ 1 ] Ali , K . , and van Stam , W . TiVo : Making show recommen - dations using a distributed collaborative ﬁltering architec - ture . In Proceedings of the 2004 ACM SIGKDD Interna - tional Conference on Knowledge Discovery and Data Mining ( Seattle , WA , USA , 2004 ) , ACM Press , pp . 394 – 401 . [ 2 ] Balabanovi´c , M . , and Shoham , Y . Fab - content - based , collaborative recommendation . Communications of the ACM 40 , 3 ( March 1997 ) , 66 – 72 . [ 3 ] Breese , J . , Heckerman , D . , and Kadie , C . Empirical anal - ysis of predictive algorithms for collaborative ﬁltering . In Proceedings of the Fourteenth Annual Conference on Un - certainty in Artiﬁcial Intelligence ( Madison , WI , USA , July 1998 ) , Morgan Kaufmann , pp . 43 – 52 . [ 4 ] Cosley , D . , Lawrence , S . , and Pennock , D . REFEREE : An open framework for practical testing of recommender sys - tems using ResearchIndex . In 28th International Conference on Very Large Databases ( Hong Kong , China , August 2002 ) , Morgan Kaufmann , pp . 35 – 46 . [ 5 ] Deshpande , M . , and Karypis , G . Item - based top - n recom - mendation algorithms . ACM Transactions on Information Systems 22 , 1 ( 2004 ) , 143 – 177 . [ 6 ] Dwork , C . , Kumar , R . , Naor , M . , and Sivakumar , D . Rank aggregation methods for the Web . In Proceedings of the Tenth International Conference on World Wide Web ( Hong Kong , China , 2001 ) , ACM Press , pp . 613 – 622 . [ 7 ] Fagin , R . , Kumar , R . , and Sivakumar , D . Comparing top - k lists . In Proceedings of the Fourteenth Annual ACM - SIAM Symposium on Discrete Algorithms ( Baltimore , MD , USA , 2003 ) , SIAM , pp . 28 – 36 . [ 8 ] Goldberg , D . , Nichols , D . , Oki , B . , and Terry , D . Us - ing collaborative ﬁltering to weave an information tapestry . Communications of the ACM 35 , 12 ( 1992 ) , 61 – 70 . [ 9 ] Good , N . , Schafer , B . , Konstan , J . , Borchers , A . , Sar - war , B . , Herlocker , J . , and Riedl , J . Combining collab - orative ﬁltering with personal agents for better recommen - dations . In Proceedings of the 16th National Conference on Artiﬁcial Intelligence and Innovative Applications of Artiﬁ - cial Intelligence ( Orlando , FL , USA , 1999 ) , American Asso - ciation for Artiﬁcial Intelligence , pp . 439 – 446 . [ 10 ] Hayes , C . , Massa , P . , Avesani , P . , and Cunningham , P . An online evaluation framework for recommender systems . In Workshop on Personalization and Recommendation in E - Commerce ( Malaga , Spain , May 2002 ) , Springer - Verlag . [ 11 ] Herlocker , J . , Konstan , J . , Borchers , A . , and Riedl , J . An algorithmic framework for performing collaborative ﬁlter - ing . In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Infor - mation Retrieval ( Berkeley , CA , USA , 1999 ) , ACM Press , pp . 230 – 237 . [ 12 ] Herlocker , J . , Konstan , J . , Terveen , L . , and Riedl , J . Evaluating collaborative ﬁltering recommender systems . ACM Transactions on Information Systems 22 , 1 ( 2004 ) , 5 – 53 . [ 13 ] Karypis , G . Evaluation of item - based top - N recommenda - tion algorithms . In Proceedings of the Tenth ACM CIKM International Conference on Information and Knowledge Management ( Atlanta , GA , USA , 2001 ) , ACM Press , pp . 247 – 254 . [ 14 ] Konstan , J . , Miller , B . , Maltz , D . , Herlocker , J . , Gor - don , L . , and Riedl , J . GroupLens : Applying collaborative ﬁltering to usenet news . Communications of the ACM 40 , 3 ( 1997 ) , 77 – 87 . [ 15 ] Kummamuru , K . , Lotlikar , R . , Roy , S . , Singal , K . , and Krishnapuram , R . A hierarchical monothetic document clustering algorithm for summarization and browsing search results . In Proceedings of the Thirteenth International Con - ference on World Wide Web ( New York , NY , USA , 2004 ) , ACM Press , pp . 658 – 665 . [ 16 ] Linden , G . , Smith , B . , and York , J . Amazon . com recom - mendations : Item - to - item collaborative ﬁltering . IEEE In - ternet Computing 4 , 1 ( January 2003 ) . [ 17 ] McLaughlin , M . , and Herlocker , J . A collaborative ﬁlter - ing algorithm and evaluation metric that accurately model the user experience . In Proceedings of the 27th Annual In - ternational ACM SIGIR Conference on Research and De - velopment in Information Retrieval ( Sheﬃeld , UK , 2004 ) , ACM Press , pp . 329 – 336 . [ 18 ] Melville , P . , Mooney , R . , and Nagarajan , R . Content - boosted collaborative ﬁltering for improved recommenda - tions . In Eighteenth National Conference on Artiﬁcial In - telligence ( Edmonton , Canada , 2002 ) , American Association for Artiﬁcial Intelligence , pp . 187 – 192 . [ 19 ] Middleton , S . , Shadbolt , N . , and De Roure , D . Onto - logical user proﬁling in recommender systems . ACM Trans - actions on Information Systems 22 , 1 ( 2004 ) , 54 – 88 . [ 20 ] Nichols , D . Implicit rating and ﬁltering . In Proceedings of the Fifth DELOS Workshop on Filtering and Collaborative Filtering ( Budapest , Hungary , 1998 ) , ERCIM , pp . 31 – 36 . [ 21 ] Oztekin , U . , Karypis , G . , and Kumar , V . Expert agree - ment and content - based reranking in a meta search envi - ronment using Mearf . In Proceedings of the Eleventh Inter - national Conference on World Wide Web ( Honolulu , HW , USA , 2002 ) , ACM Press , pp . 333 – 344 . [ 22 ] Resnick , P . , Iacovou , N . , Suchak , M . , Bergstorm , P . , and Riedl , J . GroupLens : An open architecture for col - laborative ﬁltering of netnews . In Proceedings of the ACM 1994 Conference on Computer Supported Cooperative Work ( Chapel Hill , NC , USA , 1994 ) , ACM , pp . 175 – 186 . [ 23 ] Resnick , P . , and Varian , H . Recommender systems . Com - munications of the ACM 40 , 3 ( 1997 ) , 56 – 58 . [ 24 ] Sarwar , B . , Karypis , G . , Konstan , J . , and Riedl , J . Anal - ysis of recommendation algorithms for e - commerce . In Pro - ceedings of the 2nd ACM Conference on Electronic Com - merce ( Minneapolis , MN , USA , 2000 ) , ACM Press , pp . 158 – 167 . [ 25 ] Sarwar , B . , Karypis , G . , Konstan , J . , and Riedl , J . Ap - plication of dimensionality reduction in recommender sys - tems . In ACM WebKDD Workshop ( Boston , MA , USA , Au - gust 2000 ) . [ 26 ] Sarwar , B . , Karypis , G . , Konstan , J . , and Riedl , J . Item - based collaborative ﬁltering recommendation algorithms . In Proceedings of the Tenth International World Wide Web Conference ( Hong Kong , China , May 2001 ) . [ 27 ] Schafer , B . , Konstan , J . , and Riedl , J . Meta - recommendation systems : User - controlled integration of di - verse recommendations . In Proceedings of the 2002 Interna - tional ACM CIKM Conference on Information and Knowl - edge Management ( 2002 ) , ACM Press , pp . 43 – 51 . [ 28 ] Schein , A . , Popescul , A . , Ungar , L . , and Pennock , D . Methods and metrics for cold - start recommendations . In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( Tampere , Finland , 2002 ) , ACM Press , pp . 253 – 260 . [ 29 ] Shardanand , U . , and Maes , P . Social information ﬁltering : Algorithms for automating “word of mouth” . In Proceedings of the ACM CHI Conference on Human Factors in Com - puting Systems ( Denver , CO , USA , May 1995 ) , ACM Press , pp . 210 – 217 . [ 30 ] Spillman , W . , and Lang , E . The Law of Diminishing Re - turns . World Book Company , Yonkers - on - Hudson , NY , USA , 1924 . [ 31 ] Tombs , M . Osmotic Pressure of Biological Macromolecules . Oxford University Press , New York , NY , USA , 1997 . [ 32 ] Vogt , C . , and Cottrell , G . Fusion via a linear combina - tion of scores . Information Retrieval 1 , 3 ( 1999 ) , 151 – 173 . [ 33 ] Ziegler , C . - N . , Lausen , G . , and Schmidt - Thieme , L . Taxonomy - driven computation of product recommendations . In Proceedings of the 2004 ACM CIKM Conference on In - formation and Knowledge Management ( Washington , D . C . , USA , November 2004 ) , ACM Press , pp . 406 – 415 . [ 34 ] Ziegler , C . - N . , Schmidt - Thieme , L . , and Lausen , G . Ex - ploiting semantic product descriptions for recommender sys - tems . In Proceedings of the 2nd ACM SIGIR Semantic Web and Information Retrieval Workshop 2004 ( Sheﬃeld , UK , July 2004 ) .