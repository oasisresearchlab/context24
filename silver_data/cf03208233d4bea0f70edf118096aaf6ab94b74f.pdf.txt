The Civic Data Deluge : Understanding the Challenges of Analyzing Large - Scale Community Input Narges Mahyar 1 , Diana V . Nguyen 2 , Maggie Chan 2 , Jiayi Zheng 2 , Steven P . Dow 2 1 University of Massachusetts Amherst nmahyar @ cs . umass . edu 2 University of California San Diego { dvn023 , mac011 , jiz324 , spdow } @ ucsd . edu ABSTRACT Advancements in digital civics have enabled leaders to en - gage and gather input from a broader spectrum of the public . However , less is known about the analysis process around com - munity input and the challenges faced by civic leaders as en - gagement practices scale up . To understand these challenges , we conducted 21 interviews with leaders on civic - oriented projects . We found that at a small - scale , civic leaders manage to facilitate sensemaking through collaborative or individual approaches . However , as civic leaders scale engagement prac - tices to account for more diverse perspectives , making sense of the large quantity of qualitative data becomes a challenge . Civic leaders could beneﬁt from training in qualitative data analysis and simple , scalable collaborative analysis tools that would help the community form a shared understanding . Draw - ing from these insights , we discuss opportunities for designing tools that could improve civic leaders’ ability to utilize and reﬂect public input in decisions . CCS Concepts • Human - centered computing → Human computer inter - action ( HCI ) ; User interface design ; Author Keywords Digital Civics ; Community Engagement ; Qualitative Data Analysis ; Public Input INTRODUCTION Engaging people on important issues of public concern is a cornerstone of democracy . Digital civics is an emerging cross - disciplinary area that explores new ways to utilize technology for promoting democratic participation in the design and deliv - ery of civic services . The goal is to enable wider participation by utilizing technology to support participatory democracy and greater transparency [ 29 , 32 , 48 , 49 , 62 ] . Advances in digital civics have made it easier than ever for civic decision makers to collect public input through online surveys , mobile applications , online platforms , kiosks , etc . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspeciﬁcpermission and / or a fee . Request permissions from permissions @ acm . org . DIS ’19 , June 23 – 28 , 2019 , San Diego , CA , USA © 2019 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ISBN 978 - 1 - 4503 - 5850 - 7 / 19 / 06 . . . $ 15 . 00 DOI : https : / / doi . org / 10 . 1145 / 3322276 . 3322354 Arnstein theory of engagement describes public involvement as a spectrum from simply being consulted to feeling empow - ered to enact change [ 9 ] . In response , many researchers have sought to empower different voices beyond simply provid - ing data to identify issues ( e . g . [ 10 , 28 ] ) . In recent years , the HCI community has shown increasing interest in understand - ing civic decision - making processes and designing possible interventions that can support new forms of civic participa - tion [ 10 , 13 , 17 , 18 , 46 ] . Previous interview studies in civics have uncovered current practices including efforts to lower the barrier to participation , create relationships and to build trust [ 10 , 13 , 17 ] . Other researchers focus on working closely with both community and city ofﬁcials to propose principles and best practices for community engagement [ 10 ] . Prior work indicates the importance of face - to - face engagement methods [ 17 , 18 ] but also recognizes the limitations of this approach in terms of creating an inclusive , representative , and transparent process [ 14 , 20 , 42 , 53 , 57 ] . However , these studies mostly focus on how people engage with the early stages of design . As of yet , less attention has been paid to how civic leaders analyze and utilize community input , and what chal - lenges ensue with increasing this data . To address this gap , we conducted interview studies with 21 civic leaders across a broad range of organizations and roles . To set the context , we ﬁrst deﬁne what we mean by civic leaders and community input . We use civic leaders as an umbrella term to include the wide range of leaders involved with civic decision - making . In our interviewee pool we have 5 decision makers who are in charge of key decisions and are often afﬁliated with the government , 11 community leaders who work to foster change on behalf of a community , and 5 researchers who play a dual role in leading engagement efforts and analyzing and interpreting the results . We use public input and community input interchangeably to refer to qualitative input including ideas , comments , and opinions from members of the public . Similar to previous research , our study found that civic leaders have a strong preference towards hosting face - to - face meet - ings and gathering qualitative input , rather than quantitative data [ 10 , 17 , 18 ] . Furthermore , we found that , in small - scale face - to - face public methods , civic leaders have established col - laborative or individual approaches for quickly summarizing and analyzing public input . However , as civic leaders try to increase the scale in order to gather more representative in - put , they face challenges in analyzing and utilizing qualitative data . Issues of data sanctity , representativeness , and reliability Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1171 complicate the process because the data is unstructured , mul - tifaceted and needs more advanced tools for analysis . Civic leaders suggested that collaborative analysis approaches could enable decision makers to work collaboratively with other civic leaders , data analysts , as well as the public , and could help to reduce biases and increase transparency . They also pointed out that simple collaborative tools for thematic analy - sis would enable them to quickly formulate actionable insights based on a shared understanding of community desires and needs . This paper contributes insights about the iterative process of analyzing , and interpreting community input . Our ﬁndings expand on prior work by outlining speciﬁc challenges of ana - lyzing large - scale , qualitative public input and by pointing to the limitations of established face - to - face analysis approaches . These ﬁndings suggest design directions for large - scale quali - tative analysis tools that can augment current civic practices , reduce time and resources needed for analysis , and increase transparency in the data analysis process . BACKGROUND In this section , we discuss the limitations of traditional civic en - gagement practices and technological interventions that have been developed to address these constraints and to engage the public in more democratic ways . Finally , we review in - terview studies that uncovered current engagement practices and challenges , and discuss how our work adds to this body of research . Limitations of Traditional Civic Engagement Strategies Traditional civic engagement activities—like public work - shops and design charrettes—can help communities express their needs and work through civic issues with public ofﬁ - cials . Such activities are typically led by people or organiza - tions that we refer to as civic leaders . However , traditional civic engagement activities have many limitations . Civic lead - ers who organize and often facilitate these activities need to work around key constraints such as physical space and time scarcity [ 42 , 45 ] . Scheduling these spaces inherently means limiting attendance to those participants who are able to attend at a speciﬁed time and place . While decision makers might rely on civic engagement activities for actionable information , the public may also take these opportunities to release tension , frustration and grief [ 14 ] . To address conﬂicting perspectives and reach consensus among stakeholders , some civic engagement activities involve debate and deliberation . However , such debates often advan - tages people with training in analytical and rhetorical rea - soning by providing them more time to construct reasoned arguments regarding an issue [ 55 ] . Researchers found that some people simply do not like being exposed to perspectives that challenge their own [ 25 ] . Exposure to such perspectives can cause people to retreat and avoid [ 25 ] , and become po - larized toward their existing views [ 57 ] . These behaviors can lead a decision - making group to ignore their better judgment or suppress individual perspectives in favor of a false con - sensus or group harmony [ 36 , 45 ] . In many cases , members can suppress individual opinion due to reluctancy of publicly expressing concerns if they believe that others are likely to disagree with them [ 50 ] . In sum , traditional methods unevenly represent all individuals affected by a decision and often fail to capture , record and incorporate a broad spectrum of the public input [ 34 ] . While the goal of public engagement is to form a shared narrative through gathering broad community perspectives [ 37 , 52 , 55 ] , traditional civic engagement activities often do not produce a shared narrative [ 53 ] . Digital civics offer strategies towards improving the practice of civic engagement especially by in - creasing access to information and broadening engagement , enabling wider participation in the design and delivery of civic services [ 49 , 62 ] . Technology to Support the Data Pipeline in Civics Numerous technological solutions have been developed to address the limitations of traditional methods and to engage the public in more democratic ways . We deﬁne the “data pipeline” in civics as the process of gathering , analyzing and utilizing public input . We discuss methods and ways that online technology can be used to gather input from a broader range of the public , and we describe online solutions that facilitate the process of analyzing and utilizing qualitative input and public opinion . Tools for gathering community input The rise of the Internet and ubiquitous access to mobile de - vices has enabled government and city ofﬁcials to collect information about people’s needs and issues more efﬁciently than traditional methods . While public opinion polls and surveys are still commonly used to collect community input , researchers have explored ways to make surveys quicker and more accessible to the broader population [ 19 ] . WikiSurvey is one example that has been used by cities to quickly gather public input by offering a pairwise voting approach [ 54 ] . Simi - larly , mobile applications have provided new opportunities for community members to quickly report issues ( e . g . [ 2 , 26 ] ) , or share their ideas ( e . g . [ 59 ] ) with government ofﬁcials . Other methods for engaging the public make use of online forums ( e . g . NextDoor . com [ 4 ] ) . These tools generally attempt to lower barriers for the public to report issues and share ideas , while enabling city ofﬁcials to hear from a broader , more representative range of community members . Tools for analyzing and interpreting community input A key goal of online engagement is to orient community dis - cussions towards surfacing various perspectives , building con - sensus , and utilizing public opinion . Several research tools have been designed to address these issues [ 39 , 41 , 68 , 69 ] . For instance , Consider . it is a deliberation technology that explicitly structures community discussion using a visualization around the pros and cons [ 41 ] . Procid provides interaction and visual - ization features that brings consensus strategies to distributed design discussions [ 69 ] . Political Grid analyzes political opin - ion by placing tweets on an “agreeability vs . importance” grid , which are then visualized to evaluate group - based voting pat - terns [ 39 ] . These tools facilitate consensus building process using online interactive tools for distributed design decisions . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1172 A number of online deliberation platforms seek to provide support for surfacing people’ perspectives . For instance , De - liberatorium is a platform that fosters group deliberation on complex issues [ 40 ] . It combines argumentation mapping and social computing to allow people to merge insights and ﬁnd solutions to shared problems . Users rate ideas and argu - ments while moderators check pending posts . Pol . is is an open source democracy platform that allows large - scale communi - cation by incorporating machine learning , data visualization , and artiﬁcial intelligence to facilitate conversation [ 5 ] . Pol . is creates and analyzes a matrix comprised of what each person thinks about every comment through a voting system and then visually clusters people according to common beliefs . Other tools support the exploration and analysis of online dis - cussions and public input ( e . g . [ 21 , 47 , 64 ] ) . Many of these analytic tools use visualization to surface sentiment of public opinions [ 21 , 47 ] . In contrast to simply showing sentiment , there have been some attempts to visualize the topics discussed within conversations [ 23 , 61 , 63 ] . For example , MultiConVis is devised of a hierarchical topic model from a collection of online conversations by aggregating the topics of each conver - sation in the collection [ 33 ] . Other approaches make sense of civic discussions by building user interfaces to summarize discussions ( e . g . Wikum [ 67 ] ) . What most of these tools share in common is facilitating anal - ysis of qualitative comments through visualizing the data to surface sentiment , anomalies , likes / dislikes , and pros / cons . Most of these tools were built to support communication be - tween the public and decision makers . Some have been de - veloped to empower the public to play a more active role in the decision - making process , whereas as others are concerned with the analysis process . However , most of these efforts are research tools that have been built to test a concept . In practice , there is a lack of adoptable and scalable tools for analyzing and utilizing community comments . Harding et al . argue that technology builders must understand the needs of civic leaders to build systems that can be adopted in practice [ 31 ] . Our study responds to this call to uncover civic leaders’ current practices , shortcomings , and their desires for tools that can help them understand , analyze and utilize public input more effectively . Prior Interview Studies in the Civic Space Prior interview studies have provided rich insights about civic engagement in practice , especially regarding the early phases of the process . For instance , Manuel et al . revealed the im - portance of qualitative public opinions and moving past sim - ple measures of public preference ( e . g . , voting , polling ) to make better use of complex forms of public input like story - telling and debate [ 46 ] . These studies produced guidebooks with best practices for the design of civic engagement ac - tivities [ 10 , 28 ] . For instance , Asad et al . ’s interviews with participants of a community design intervention led to the de - velopment of a “playbook” for how to approach city - wide civic engagement [ 10 ] . Gordon and Mihailidis’s interviews with city employees revealed communication practices of speciﬁc neighborhoods [ 28 ] . Based on their results , they proposed es - tablishing a ﬂuid and open - ended dialogue across the boundary between a government and its communities to solve problems . More recently , Corbett and Le Dantec [ 18 ] conducted an in - terview study across different city departments to understand their practices in engaging community . They characterized community engagement practices across the following themes : raising awareness , building relationships , setting the table , ﬁnding opportunities , and a cross - cutting theme focused on technology use . These ethnographic and interview - based studies contributed to a better understanding of what happens in practice . These stud - ies uncovered a preference towards face - to - face public engage - ment in order to create relationships and build trust [ 10 , 13 , 17 ] . However , most studies in civics focus on characterizing the early phases of gathering public input , rather than understand - ing how civic leaders analyze these qualitative comments . While these studies suggest investing in digital engagement strategies for broadening participation , the extent of technol - ogy use reported in these papers is mostly limited to tools for supporting communication or enabling information and service exchange [ 17 , 18 ] , for instance , using social media platforms and online websites to engage the community or to share information with them . Closer to our focus on understanding the practices and chal - lenges of civic leaders in regards to civic data , Boehner and DiSalvo interviewed civic leaders to learn about their strategies of using civic technology and data [ 13 ] . They identiﬁed key problems , such as data access , data literacy and fragmentation that cause duplication of efforts [ 13 ] . Our study extends prior work by focusing on the practice of data analysis . In particular , what happens when civic leaders move away from relying solely on in - person analysis methods and what challenges come with analyzing and utilizing large amounts of qualitative data . We recruited a diverse interviewee pool , with some focusing on small - scale and some adopting technology to scale up public engagement . This allowed us to better understand what issues arise when civic leaders try to scale up public engagement , what technology they currently use , and what shortcomings emerge with existing technology . METHOD We conducted 21 interviews with civic leaders from a range of different roles in charge of communicating with people , gathering their issues and ideas , and analyzing and making decisions / recommendations based on community input . We focused our interviews with people in San Diego , a large metropolitan city with fast grown and numerous ongoing ur - ban planning efforts . In order to understand practices and chal - lenges of community engagement at scale , we also included participants from organizations outside San Diego who grap - ple with challenges of analyzing and utilizing large amounts of public - generated data . Participants Through professional connections with the city and design community , we collected a large list of potential interviewees ( around 100 people ) . We recruited interviewees by emailing contacts and using the snowball method [ 27 ] to ﬁnd more participants . To ensure we engaged multiple perspectives and Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1173 ID Role Role Description Engagement Practice ( s ) Decision Maker ( DM ) P1 Vice President of Engagement of Civic Initiative Facilitates engagement initiatives within community F2F , Online Surveys P2 Senior Director of Nonproﬁt Foundation Partners with groups within city in development F2F , Mobile App , Online Engagement Platform P3 Director of Performance and Analytics Leads city wide initiatives in community outreach F2F , Online Engagement Platform P4 Director of Engagement of City - Led Initiative Leads department to improve city’s efﬁciency F2F , Mobile App , Online Surveys P5 Director of Center for Civic Engagement Designs and implements programs locally F2F Community Leader ( CL ) P6 Community Activist Oversees group to promote civic engagement F2F P7 Research Collaborator in Online Platform Coordinates city - wide initiative F2F , Social Media P8 Founder and President of a civic innovation lab Founded a nonproﬁt social enterprise F2F P9 Member of local innovation lab Researches impact of technology on society F2F , Social Media , Online Engagement Platform P10 Senior Director at local innovation center Directs organization promoting engagement F2F , Online Surveys , Online Websites P11 Community Consultant Makes decisions in community engagement efforts F2F P12 Manager and Co - founder of Civic Organization Directs non - proﬁt organization focused on engagement Online Engagement Platform P13 Founder and CEO of Online Platform Created online platform to collect public opinion Online Engagement Platform P14 Civic Landscape Architect Builds plans / blueprints for local projects F2F P15 Senior Planner Directs a community non - proﬁt group F2F P16 Founder and President of a civic innovation lab Leads lab working with civic technologies and data F2F Researcher ( R ) P17 Professor and Civic Initiative Organizer Advises government on environmental issues F2F P18 Research Fellow at Participatory Project Researches urban transformation in communities Online Discussion Forum P19 Professor and Researcher Coordinates distributive network for civic tech F2F , Online Discussion Forum P20 University Researcher and Urban Planner Creates strategic plans for varying scales F2F P21 Research Coordinator Brings assessment methods to government F2F , Online Discussion Forum Table 1 : We conducted 21 interviews with civic leaders with a wide range of roles in leading civic initiatives . This table shows participants , their roles , and their engagement practices . Practices include face - to - face ( F2F ) meetings , mobile applications / online surveys to gather input , and social media or online websites to post events or share reports . While most of the above online methods create a one - way communication channel , online discussion forums and online engagement platforms engage the public online to deliberate on issues and collaborate with decision makers to better communicate their preferences and aspirations . captured practices within community engagement at various scales , we reached out to several organizations who struggle with challenges of analyzing and utilizing large amounts of public - generated data . Through these efforts , we received responses from 21 civic leaders including leaders of large scale public engagement initiatives ( e . g . Imagine Boston [ 3 ] , Every Voice Engaged [ 1 ] , Climate CoLab [ 35 ] , and Participatory Budgeting Project [ 30 ] ) . All participation was voluntary . We did not offer reimbursement as they were informed via the consent forms that we sent out before the study . Table 1 shows the participants and their roles , their engage - ment practices , and the scale of their outreach . It is notable that , while we categorized our interviewees based on their main roles , researchers often played a dual role . Several worked both as community leaders as well as analysts who were charged with analyzing and reporting results to decision makers . Procedure Interviews were semi - structured , 45 - 60 minutes long and con - ducted over the phone—with the exception of two in - person interviews . We audio recorded all the interviews and took extensive notes . We asked open - ended questions and encour - aged interviewees to describe their related experiences , such as “think about the last time you worked on a project” or “share how you’ve been involved with civic engagement” . Each in - terview was conducted with ( at least ) two members of our research team to split the roles of interviewer and note taker . We asked questions speciﬁc to civic leaders’ practices and challenges , methods used for gathering and analyzing the data , methods used for communicating gathered data back to the public , and use of technology to support the process . Finally we asked what they hope to understand from public input , as well as what their ideal civic data practices and outcomes would look like . Data Gathering and Analysis In total , we gathered ~ 1000 minutes of audio recordings , tran - scribed them , and then placed each unique statement into our analysis software ( shared spreadsheet ) . We labeled the data with summary notes and then printed out the data to perform a cluster analysis . Through an iterative coding method , we grouped common practices , tools and challenges into high - level categories . Through this process , we surfaced a set of Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1174 topics and extracted representative quotes from participants to support our analytical claims . FINDINGS The diversity of interviewee pool , with some focusing on small - scale and some adopting technology to scale up pub - lic engagement , allowed us to better understand what issues arise when civic leaders try to scale up public engagement . Table 1 summarizes different engagement practices that we found in our study . Practices include face - to - face ( F2F ) meet - ings ( 7 relied only on F2F , and 11 utilized a mix of F2F with other methods ) , use of mobile apps to gather issues ( 2 inter - viewees ) , online surveys to gather input ( 3 interviewees ) and use of social media or online websites to post the events or to share reports ( 3 interviewees ) . A few leaders adopted online discussion forums to engage the public online to deliberate on issues ( 3 interviewees ) , online engagement platforms to enable collaboration between the public and decision mak - ers , and to communicate public preferences and aspirations ( 5 interviewees ) . In the following , we discuss what civic leaders hope to capture from community input , then we reveal current practices for data analysis in small and large - scale settings , and also report on shortcomings as perceived by civic leaders . Civic Leaders Seek Rich & Representative Public Input In terms of what civic leaders hope to understand from the data , many community leaders mentioned “the community’s priorities or main supporting themes” as a guide for decisions that will be supported by the community ( P10 and P11 ) . A city ofﬁcial wanted to ideally see “expectations of the com - munity” ( P14 ) . She adds “from citizens , I hope to understand both memories and expectations for a place” . Another said they “are much more interested in people’s reactions [ . . . ] to the projects or activities or prospects for the future” ( P11 ) . Furthermore , they showed interest in understanding whether “there are particular groups that are being marginalized or shut - out by a particular proposal” or when strong opinions play into “swaying a decision one way or the other” ( P9 ) . Civic leaders talked about the need to gather metadata ( such as demographic , socio - economic , and cultural background ) to understand the source of comments ( P6 , P9 ) . One participant said : Civic data [ should ] incorporate demographic data , show where the source is coming from ( e . g . strong or passive opponent ) , [ represent ] some combination of sentiment and criteria with summary and key points , and ﬁnally , show [ what ] aspects of a project is positive or negative ( rather than stating positive or negative reaction to a project [ as a whole ] ) . ( P9 - CL ) However , even if they gather some of these metadata , in prac - tice , they cannot trace the data back to a person due to privacy and legal issues . P19 says , “we don’t track every answer from a person . [ Because ] we ran into some legal issues and cultural differences . ” Another said , ideal civic data should show “how many people share a feeling or feedback , [ and allow people to ] see each other’s comments so they can tell where the decision was derived from” ( P13 ) . In - person Engagement Affords Collaborative Analysis In small - scale face - to - face engagement meetings , manually coding community input is the most popular analysis method found among our participants . The best case scenario is a collaborative process where they engage community members in summarizing all comments during the meetings . This allows civic leaders to interact with community members through a group activity ( P18 ) to build a shared narrative of what has been discussed . We would go over it [ all comments ] together and we would talk about it and discuss it . To try to help people make more equitable decisions . . . to encourage people to think through the project ideas that they were proposing , about what the impact on the community with respect to pre - existing inequality and pre - existing racial and ethnic disparities . ( P18 - R ) Some community leaders would hand out sticky notes to com - munity members to narrow down options and prioritize based on certain criteria . These kinds of in - person analyses enable “further interaction and negotiation with community members” ( P10 ) . Furthermore , in - person analysis provides a co - learning opportunity among people with different needs ( P19 ) . It also allows a deeper dive into community needs : You get the beneﬁt of having people interacting with each other and that interplay inﬂuences the way people think about something . So you can really see how people’s values play into the way they think and you can see more of a diversity in the ways people think about a particular issue and understand why that diversity exists and why people have the positions that they have . ( P21 - R ) However analyzing the comments from dozens of people can be challenging during time - limited , face - to - face workshops , as one participant described : There were 100 + people and a bunch of comments . The leader and her team manually went through the comments and coalesced all the similar comments and how many people made that type of comment . ( P16 - CL ) The sheer amount can force civic leaders to sort data through manual coding outside of meetings ( P16 , P19 , P13 ) , mostly by an individual who may be the main decision maker ( P14 ) , facilitator ( P10 and P11 ) , or a designated note taker ( P11 ) . Thematic Analysis is Difﬁcult to Scale When dealing with large - scale community input , civic lead - ers reported a costly , tedious , and time - consuming analysis process . Due to a lack of time , decision makers often need to outsource the data analysis as the volume increases . An interesting case study that we came across in our interviews , was a large - scale initiative that required collaboration between decision makers and researchers . However , researchers who were in charge of facilitating engagement efforts and analyzing the results reported several issues in this process : Anytime you start talking about analyzing qualitative data from hun - dreds of people , this takes a lot of time and energy to boil down into something that is useful for decision makers because they certainly don’t have time to go through all of that data . ( P21 - R ) When the data is outsourced , it adds several rounds of itera - tive analyses . This iterative back and forth process between researchers and decision makers requires extensive time and resources : Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1175 We’re ﬁnding that [ qualitative data ] is having an impact that we didn’t even realize that it would have , but . . . we only have so much manpower to analyze the data and put it out there . [ . . . ] We gave them [ decision makers ] different rounds of results and then we get questions and then we would provide them with answers to those questions . So , it was an iterative process that ﬁnally generated a report that got passed around the community . ( P21 - R ) As we heard from many participants , qualitative community input is “multifaceted and challenging to analyze” ( P20 - R ) . Qualitative [ data ] is tough because it gets down to opinions and you have neighbors arguing with each other . But nobody can argue with quantitative . ( P15 - DM ) The conﬂicting nature of public input also adds challenges for data analysis and motivates decision makers to adopt less intensive practices such as relying on quantitative data . Since 2012 , [ decision makers ] only asked Likert - scale questions and they don’t ask anymore qualitative [ questions ] , because it is so hard to analyze [ data ] at global scale . ( P19 - R ) This is despite decision makers’ preferences to gather rich qualitative input . Quantitative data is easier than qualitative data to analyze but reduces decision maskers’ ability to under - stand people’s desires , issues and rationale for their input . Lack of Easy to Use Collaborative Analysis Tools Due to lack of time and difﬁculties of analyzing large volumes of qualitative data , civic leaders outsource the data analysis ( P17 - R , P2 - DM ) . One interviewee said : “we have a group of consultants who helped us do a lot of the analysis” ( P2 - DM ) . Another stated that decision makers do not have time to ana - lyze and summarize the qualitative data and therefore they out - source their data to analysts ( P21 - R ) . This shifts the analyies of community input from a collaborative , open and transparent process to a black box . When the data is outsourced , there is a disconnect between analysts , community and decision makers . The current set of tools are very time - intensive and expert - orientated . Several community leaders , particularly researchers in charge of analyzing and relaying the data to decision makers , men - tioned using social science coding tools such as ATLAS . ti [ 6 ] and Dedoose [ 7 ] for thematic coding ( P17 - R , P9 - CL , P21 - R , P19 - R ) . Researchers expressed a desire for analysis tools that help them analyze data more efﬁciently : We don’t have the software [ for extensive analysis ] . There is a wishlist for better tools ; there is a big gap between what we want to analyze and what we are able to analyze . If we had tools that can help us analyze [ the qualitative data ] in a reliable way , we could do it [ analysis ] in a more efﬁcient and optimal way . ( P19 - R ) One participant tried to use Google Docs because of its col - laborative nature to make the analysis more inclusive , but they pointed out the cloud - based word processing tool do not necessarily help get people engaged and focused on the task : Getting [ staff ] to go on Google Docs and put their meeting notes [ . . . ] , it’s hard to get them to engage . So the tools need to be quite simple , easy to use , and I think they need to be focused . ( P7 - CL ) Researchers in charge of analyzing the data reported chal - lenges of communicating with decision makers . They pointed out to two major obstacles in this process 1 ) boiling down the data to something useful for decision makers and 2 ) educating decision makers to understand qualitative data : Capturing what the decision maker actually wants can be kind of difﬁ - cult . So those are the two major obstacles that we deal with : lots of data : it is hard to boil down , and also educating decision makers on what it is that they can actually use and having them understand what they can use . So in most of the projects we’ve done so far there’s been a little bit of a “question - mark” as to , from the [ decision makers’ ] standpoint , how can I actually use this data ? It’s kind of an experimental risky thing for them . They think they’re going to get something out of it but they’re not really sure what it is because they’ve never really received data in that way . ( P21 - R ) P21 also added a point about lack of a systematic analysis process and a need to customize analysis based on different projects : There’s a lot of co - learning that needs to happen between us and deci - sion makers . Besides , every decision maker is going to have different type of needs , so in some ways we’re reinventing the way we analyze the data every single time . ( P21 - R ) These results suggest that the lack of literacy in analyzing large quantities of qualitative data makes this process more complicated . Summarization Often Leads to Marginalization Depending on the practice and the scope of the project , civic leaders adopt different strategies for utilizing public input . Civic leaders often make decisions “based on summarized data” ( P13 - CL ) . One interviewee said , “I have staff who put together the summary , and then I review it to see if I agree with it . I provide the strategy on how I want them to summarize the data based upon the activity” ( P3 - DM ) . Others attempt to use the summaries to create a consensus model . One participant talked about organizing input “based on value , need , and beneﬁt on a matrix to rank projects , and then using a consensus model where people vote together . However , consensus models can suppress minority viewpoints” ( P18 - R ) . Biases in aggregated data seems to be a common problem for both data gathered in small - scale as well as large - scale . In the case of big data , one interviewee says that “the analysis becomes a project by itself and the big challenge is to account for biases” ( P19 - R ) . Others raised similar concerns about how summarizing the input leads to missing some interesting points of view : “not all info is captured in writing down the discussion due to imperfect summaries . The beneﬁt might also be that it’s shorter , but it might be imperfect and not capture the right thing” ( P13 - CL ) . Another said : [ When you’re ] treating everybody as staying in aggregate , you lose the visions that are actually embedded into those [ individual perspectives ] . ( P19 - R ) One decision maker thought a collaborative summarization approach can beneﬁt analysis : We could reduce biases , if we could couple people into teams to look at data for interpreting [ . . . ] there is a lot of subjective information that you are collecting . So having a few people on our team to look at stuff allows for a consideration of interpreting results [ in ] different ways and trying to ﬁgure what do we think people meant when they said something . ( P3 - DM ) This echos the need for collaborative tools that can offset some of the biases and allow for more effective collaboration Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1176 to surface main themes . To communicate the results , there are several practices including putting the PowerPoint slides online ( P14 - CL ) , reporting the results online in interactive ( P4 - DM ) or static visualization form ( P2 - DM ) . While visualization is one of the most commonly used method to summarize and communicate an overview , we found some civic leaders to be reluctant to represent visualized data . One decision maker speciﬁcally made a comment about how transforming data into visualizations can create a misconception by implying that it is “the data , ” whereas in reality it is non - representative data gathered from a small group of self - selected individuals ( P3 - DM ) . Even in more participatory methods such as “col - laboratively prioritizing and voting methods , there are dangers of overlooking affected populations” ( P1 - DM ) . Interviewees who were involved with in - person engagement , similarly ex - pressed limitations surrounding how face - to - face methods are non - representative . Furthermore , one researcher mentioned “ [ decision makers ] are only interested in the answer , not so much to understand and do a deep dive analysis and interpre - tation” ( P19 - R ) . This indicates that turning data to summaries not only could lead to missing marginalized perspectives , but also could oversimplify the story . DISCUSSION AND FUTURE DIRECTIONS The ﬁndings uncovered a number of challenges as civic lead - ers seek to scale up community engagement practices . Similar to previous studies , we found that civic leaders strongly prefer hosting regular face - to - face public meetings [ 10 , 13 , 17 ] with a tendency towards gathering qualitative input—rather than purely quantitative data—to gain a richer understanding of the community’s desires and needs [ 58 ] . We found that the scale of engagement directly affects the approach to data anal - ysis . In small - scale face - to - face settings , civic leaders have adopted a variety of approaches for collaborative analysis of community data . However , as civic leaders seek to scale up community engagement and manage a deluge of data , they have to outsource the analysis . When the data is outsourced , analysis moves from an open process that used to be handled during face - to - face meetings into more of a black - box process . This not only demands more resources and labor but also jeop - ardizes the transparency of the process and raises the potential of bias and uncertainty . Most civic leaders use traditional interpretive models to code the data and surface themes [ 56 ] . A few researchers mentioned use of social science coding tools such as ATLAS . ti [ 6 ] and De - doose [ 7 ] . Although these tools are collaborative , they require expertise that can typically be used by researchers / analysts with both adequate training and a rich understanding of con - text . Even with the use of such tools , researchers described the analysis process as a very time - consuming and poten - tially fraught process . Outsourcing data analysis also requires back - and - forth communication between researchers and civic leaders to generate civic data that is ideally parsed with the community’s sentiments , key values , and themes . Researchers in charge of analyzing large - scale data reported this process as extremely tedious and time consuming . Furthermore , they wanted better collaboration tools in order to work more closely with decision makers to summarize the data . To communicate the results with decision makers and the public , most leaders created some form of summary but all acknowledged how summaries can be imperfect and biased . Some leaders hesitated to even communicate the summaries back to the public due to uncertainties around the represen - tativeness of the data . Reﬂecting on our ﬁndings , we draw out future directions for technology to combine in - person and online engagement strategies and for technology to be based on civic leaders’ speciﬁc needs to empower them in utilizing public input . We explore how in - person meetings could facilitate large - scale , collaborative analysis , and how computational methods could supplement traditional analy - sis methods . Finally , inspired by visualization research in communicating uncertainty in data , we discuss methods for representing data quality issues , such as incompleteness and non - representativeness . Creating a Tighter Coupling Between In - Person and Online Engagement While in - person meetings are still the modus operandi for civic leaders , they often limit the inclusiveness of voices and lead to non - representative summaries of the community’s perspec - tive . Many have called for a push in technology to broaden the access to civic data and allow more democratic participa - tion [ 29 , 32 , 48 ] . However , there has been some skepticism about moving to online participation . For instance , researchers have talked about the digital divide as a barrier for inclusive - ness [ 16 ] . Other issues in online engagement methods include keeping people motivated , sharing information effectively , and overcoming learning curves to work in online environ - ments [ 24 ] . Our results suggest that civic leaders may want to develop hybrid approaches that take advantage of both in - person meetings and online technologies . Some of the most successful practices we found , used hybrid models to broaden participation and increase the inclusivity of the data . Such hybrid combinations could combine tools in a novel way to communicate information and to obtain community feed - back in the broadest sense . For example , posting the results of in - person meetings online can provide information on the process as a whole , which can keep the public up - to - date with decisions and invite a broader set of the public to improve and evaluate the ideas online [ 44 ] . Coupling face - to - face methods with online gathering methods allows for a more inclusive approach by allowing people who typically do not have the time or resources to attend in - person meetings to provide input online ( e . g . families with kids ) . Such hybrid methods widen the net of collaboration and democracy in diverse communi - ties . Furthermore , previous research suggest that large - scale , open discussion can lead decision makers to make better deci - sions [ 22 ] . Our research results support this as civic leaders mentioned the value of understanding how decisions were made or consensus was built through community members’ in - teractions with one another . Incorporating the history of ideas and actions coupled with a concise description of the project and goals can help with engaging newcomers . To see more widespread adoption , tools should address the needs of both parties by being informative to decision makers and simple , quick and engaging for community members [ 44 ] . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1177 Designing Collaborative Sensemaking Approaches According to our results , one consistent problem is the lack of time and accessible tools for decision makers to analyze the data . Civic leaders consistently referred to the analysis process as being costly , time consuming , complicated , and tedious . This complexity is inherent to civic decision - making which involves many inﬂuential factors such as policy , bud - get , historical context , environmental constraints , and vari - ous stakeholders’ needs and expectations . Civic leaders have sought to address this complexity by bringing people into the analysis process during in - person meetings . However , as they try to increase the scale , the data analyses are often outsourced to an outside group or individual which essentially reduces transparency of the analysis process and creates a black box . Collaborative approaches can make the process more trans - parent by opening the analysis process up to a larger group of people . For instance , Frenzy enables a collaborative analysis approach for organizing some types of qualitative data [ 15 ] . Wikum [ 67 ] allows large numbers of readers to collectively summarize the main points in a discussion forum . However , Wikum does not support collaborative summarization , as in - dividuals summarize different parts of a discussion . More research is needed to understand how to design effective dis - cussion summaries [ 66 ] . Such approaches can be used in civics to enable analysts , researchers , and community mem - bers to collaborate with decision makers in analyzing data . As one of the decision makers in our study suggested , collabo - rative analysis can reduce biases in aggregated data . Better collaborative tools can also improve the iterative process of analysis and help build common ground between analysts and decision makers . To address the learning curve of current analysis tools , new tools should be designed for non - expert use to make sure everyone can take part in analyzing data . Methods for crowd synthesis [ 8 ] could be used to address the labor - intensive analysis by dividing up analysis work be - tween members of community , similar to the collaborative approaches in small - scale settings . Employing Computational Approaches for Qualitative Data Analysis at Scale To further address challenges of qualitative data analysis at scale , some researches have explored computational ap - proaches . For instance , Baumer et al . have proposed new ways of combining interpretive grounded theory [ 56 ] and compu - tational topic modeling approaches [ 12 ] for uncovering main themes in qualitative data [ 11 ] . In addition to improving qual - itative data analysis by employing computation , researchers in humanities have argued for more inclusive approaches for text analysis . For instance , Rhody argues for rethinking tex - tual analysis beyond current computational and social science approaches to enable transdisciplinary collaboration among people from different ﬁelds and backgrounds [ 51 ] . We might need to further investigate what matters most in civic discus - sions , and how we can go beyond surfacing sentiments and topics by capturing issues and conﬂicting opinions . Surfacing the topics alone might not be enough context for civics . It would be fruitful to develop approaches for modeling and rep - resenting discourse [ 51 ] , and for highlighting disagreements within a community [ 43 ] . Future work in civics might focus on such mixed approaches that support iterative and complex analysis without requiring a signiﬁcant time commitment . Communicating Uncertainty and Incompleteness Even in the most participatory process , biases can come into play during the collection and analysis of community input . Uncertainty and data incompleteness can hamper civic deci - sion making [ 60 ] . Furthermore , presenting the data in a formal way without caveats and explanations might imply that the data it fully represents everyone’s perspective . This is espe - cially problematic when it is just sample data collected from a self - selected group of individuals . In addition , data may be lost in the process of aggregation and summarization . Re - cently , some researchers investigated the use of visualization for representing data uncertainty in other contexts [ 38 , 70 ] . In the context of civics , it could be useful to visualize data incompleteness , which might imply obtaining metadata such as people’s location , socio - economic background and other demographic data . Several civic leaders in our study expressed interest in gathering demographic data to double check the source of the data , and to better understand the perspectives not currently represented in the data . One instance of such a system , although in a different domain , is OpinionSeer which tracks demographic data , location and uncertainty [ 65 ] . How - ever , as one of the decision makers in our study mentioned , tracing data back to people raises privacy and legal issues . Other researchers reported a preference for making anonymous contributions in online civic discussions and discussed trade - offs between understanding the source of community input and privacy [ 44 ] . It seems timely to think beyond current online data collection methods and provide mechanisms to ensure the privacy of individual information . This can increase the chances of gathering demographic data from the public , which could signiﬁcantly affect decisions ( e . g . by understanding if marginalized perspectives have been gathered and addressed ) . CONCLUSION This paper reports insights from 21 interviews with civic lead - ers on their attempts to scale up community engagement prac - tices . Our ﬁndings corroborate previous ﬁndings that civic leaders strongly prefer gathering input in small - scale face - to - face meetings , even though they are aware of the limitations this places on the inclusivity of participation . As civic leaders try to increase the scale of participation , they face challenges in analyzing and utilizing qualitative data . Due to the sheer volume and complexity of analyzing qualitative data , they of - ten outsource the data . Traditionally social science coding tools have been used to analyze large set of qualitative data , however our interviewees reported that these tools require ex - pertise or signiﬁcant training . While interactions with analysts and decision makers is vital for extracting and compiling data into useful summaries , we found inadequacies in current anal - ysis tools and a lack of collaborative approaches to facilitate interaction between decision makers and analysts . Drawing from these insights , we offer strategies and future directions for designers and HCI researchers to build technologies that could empower civic leaders in analyzing and utilizing public input at scale . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1178 ACKNOWLEDGMENTS We thank all of our interviewees for their generosity with their time and valuable insights . We also thank Andres Baez , Brian McInnis , Sanika Moharana , and Alejandro Panduro for their support and contribution in conducting , transcribing and analyzing interviews . Thank you to Madalina Fiterau , Brendan O’Connor , Ali Sarvghad and our Design Lab colleagues for providing valuable feedback . Funding for this research was provided by NSF grants 1122206 and 1122320 . REFERENCES [ 1 ] 2017 . Every Voice Engaged . ( 2017 ) . https : / / everyvoiceengaged . org [ 2 ] 2017 . Get it Done . ( 2017 ) . https : / / www . sandiego . gov / get - it - done [ 3 ] 2017 . Imagine Boston . ( 2017 ) . https : / / imagine . boston . gov [ 4 ] 2017 . Nextdoor . ( 2017 ) . https : / / nextdoor . com / [ 5 ] 2017 . Pol . is . ( 2017 ) . https : / / pol . is / gov [ 6 ] 2019 . ATLAS . ti . ( 2019 ) . https : / / atlasti . com / [ 7 ] 2019 . Dedoose . ( 2019 ) . https : / / www . dedoose . com / [ 8 ] Paul André , Aniket Kittur , and Steven P Dow . 2014 . Crowd synthesis : Extracting categories and clusters from complex data . In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing . 989 – 998 . [ 9 ] Sherry R Arnstein . 1969 . A ladder of citizen participation . Journal of the American Institute of planners 35 , 4 ( 1969 ) , 216 – 224 . [ 10 ] Mariam Asad , Christopher A Le Dantec , Becky Nielsen , and Kate Diedrick . 2017 . Creating a Sociotechnical API : Designing City - Scale Community Engagement . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM , 2295 – 2306 . [ 11 ] Eric PS Baumer , David Mimno , Shion Guha , Emily Quan , and Geri K Gay . 2017 . Comparing grounded theory and topic modeling : Extreme divergence or unlikely convergence ? Journal of the Association for Information Science and Technology 68 , 6 ( 2017 ) , 1397 – 1410 . [ 12 ] David M Blei , Andrew Y Ng , and Michael I Jordan . 2003 . Latent dirichlet allocation . Journal of machine Learning research 3 , Jan ( 2003 ) , 993 – 1022 . [ 13 ] Kirsten Boehner and Carl DiSalvo . 2016 . Data , Design and civics : an exploratory study of civic tech . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , 2970 – 2981 . [ 14 ] Caron Chess and Kristen Purcell . 1999 . Public participation and the environment : Do we know what works ? ( 1999 ) . [ 15 ] Lydia B Chilton , Juho Kim , Paul André , Felicia Cordeiro , James A Landay , Daniel S Weld , Steven P Dow , Robert C Miller , and Haoqi Zhang . 2014 . Frenzy : collaborative data organization for creating conference sessions . In Proceedings of the 32nd annual ACM conference on Human factors in computing systems . ACM , 1255 – 1264 . [ 16 ] Stephen Coleman and John Gotze . 2001 . Bowling together : Online public engagement in policy deliberation . Hansard Society London . [ 17 ] Eric Corbett and Christopher A Le Dantec . 2018a . Going the Distance : Trust Work for Citizen Participation . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 312 . [ 18 ] Eric Corbett and Christopher A Le Dantec . 2018b . The Problem of Community Engagement : Disentangling the Practices of Municipal Government . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 574 . [ 19 ] Kevin C Desouza and Akshay Bhagwatwar . 2014 . Technology - enabled participatory platforms for civic engagement : the case of US cities . Journal of Urban Technology 21 , 4 ( 2014 ) , 25 – 50 . [ 20 ] John Dewey and Melvin L Rogers . 2012 . The public and its problems : An essay in political inquiry . Penn State Press . [ 21 ] Nicholas Diakopoulos , Mor Naaman , and Funda Kivran - Swaine . 2010 . Diamonds in the rough : Social media visual analytics for journalistic inquiry . In IEEE Symposium on Visual Analytics Science and Technology ( VAST ) . IEEE , 115 – 122 . [ 22 ] Paul DiMaggio , Clark Bernier , Charles Heckscher , and David Mimno . 2018 . Interaction Ritual Threads : Does IRC Theory Apply Online ? In Ritual , Emotion , Violence . Routledge , 99 – 142 . [ 23 ] Marian Dork , Daniel Gruen , Carey Williamson , and Sheelagh Carpendale . 2010 . A visual backchannel for large - scale events . IEEE Trans . Visualization and Computer Graphics 16 , 6 ( 2010 ) . [ 24 ] Dmitry Epstein , Mary Newhart , and Rebecca Vernon . 2014 . Not by technology alone : The “analog” aspects of online public engagement in policymaking . Government Information Quarterly 31 , 2 ( 2014 ) , 337 – 344 . [ 25 ] Leon Festinger . 1957 . Cognitive dissonance theory . 1989 ) Primary Prevention of HIV / AIDS : Psychological Approaches . ( 1957 ) . [ 26 ] Marcus Foth , Ronald Schroeter , and Irina Anastasiu . 2011 . Fixing the city one photo at a time : mobile logging of maintenance requests . In Proceedings of the 23rd Australian Computer - Human Interaction Conference . ACM , 126 – 129 . [ 27 ] Omar Gelo , Diana Braakmann , and Gerhard Benetka . 2008 . Quantitative and qualitative research : Beyond the debate . Integrative psychological and behavioral science 42 , 3 ( 2008 ) , 266 – 290 . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1179 [ 28 ] Eric Gordon and Paul Mihailidis . 2016 . Civic Media : Technology , Design , Practice . MIT Press . [ 29 ] Kenneth L Hacker and Jan van Dijk . 2000 . Digital democracy : Issues of theory and practice . Sage . [ 30 ] Carolin Hagelskamp , Chloe Rinehart , Rebecca Silliman , and David Schleifer . 2016 . Public Spending , by the People . Participatory Budgeting in the United States and Canada in 2014 - 15 . Public Agenda , Tech . Rep . ( 2016 ) . [ 31 ] Mike Harding , Bran Knowles , Nigel Davies , and Mark Rounceﬁeld . 2015 . HCI , civic engagement & trust . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM , 2833 – 2842 . [ 32 ] David Held . 2006 . Models of democracy . Polity . [ 33 ] E . Hoque and G . Carenini . 2014 . ConVis : A Visual Text Analytic System for Exploring Blog Conversations . Computer Graphics Forum ( Proc . EuroVis ) 33 , 3 ( 2014 ) , 221 – 230 . [ 34 ] Judith E Innes and David E Booher . 2004 . Reframing public participation : strategies for the 21st century . Planning theory & practice 5 , 4 ( 2004 ) , 419 – 436 . [ 35 ] Joshua Introne , Robert Laubacher , Gary Olson , and Thomas Malone . 2011 . The Climate CoLab : Large scale model - based collaborative planning . In Collaboration Technologies and Systems ( CTS ) , 2011 International Conference on . IEEE , 40 – 47 . [ 36 ] Irving L Janis . 1972 . Victims of groupthink : a psychological study of foreign - policy decisions and ﬁascoes . ( 1972 ) . [ 37 ] Christopher F Karpowitz , Chad Raphael , and Allen S Hammond IV . 2009 . Deliberative democracy and inequality : Two cheers for enclave deliberation among the disempowered . Politics & Society 37 , 4 ( 2009 ) , 576 – 615 . [ 38 ] Matthew Kay , Tara Kola , Jessica R Hullman , and Sean A Munson . 2016 . When ( ish ) is my bus ? : User - centered visualizations of uncertainty in everyday , mobile predictive systems . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , 5092 – 5103 . [ 39 ] Tanyoung Kim and Carl DiSalvo . 2012 . Visualization of the public’s opinion on politically inﬂuential tweets . [ 40 ] Mark Klein . 2011 . How to harvest collective wisdom for complex problems : An introduction to the MIT deliberatorium . Center for Collective Intelligence working paper ( 2011 ) . [ 41 ] Travis Kriplean , Jonathan Morgan , Deen Freelon , Alan Borning , and Lance Bennett . 2012 . Supporting reﬂective public thought with considerit . In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work . ACM , 265 – 274 . [ 42 ] Peter Levine , Archon Fung , and John Gastil . 2005 . Future directions for public deliberation . Journal of Public Deliberation 1 , 1 ( 2005 ) . [ 43 ] Weichen Liu , Sijia Xiao , Browne Jacob T . , Ming Yang , and Steven P . Dow . 2018 . ConsensUs : Supporting Multi - Criteria Group Decisions by Visualizing Points of Disagreement . In ACM Transactions on Social Computing . [ 44 ] Narges Mahyar , Michael R James , Michelle M Ng , Reginald A Wu , and Steven P Dow . 2018 . CommunityCrit : Inviting the Public to Improve and Evaluate Urban Design Ideas through Micro - Activities . ( 2018 ) . [ 45 ] Jane Mansbridge , Janette Hartz - Karp , Matthew Amengual , and John Gastil . 2006 . Norms of deliberation : An inductive study . ( 2006 ) . [ 46 ] Jennifer Manuel , Geoff Vigar , Tom Bartindale , and Rob Comber . 2017 . Participatory Media : Creating Spaces for Storytelling in Neighbourhood Planning . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM , 1688 – 1701 . [ 47 ] Adam Marcus , Michael S Bernstein , Osama Badar , David R Karger , Samuel Madden , and Robert C Miller . 2011 . Twitinfo : aggregating and visualizing microblogs for event exploration . In Proc . CHI . ACM , 227 – 236 . [ 48 ] Pippa Norris and others . 2001 . Digital divide : Civic engagement , information poverty , and the Internet worldwide . Cambridge University Press . [ 49 ] Patrick Olivier and Peter Wright . 2015 . Digital civics : Taking a local turn . interactions 22 , 4 ( 2015 ) , 61 – 63 . [ 50 ] Dominic J Packer . 2009 . Avoiding groupthink : Whereas weakly identiﬁed members remain silent , strongly identiﬁed members dissent about collective problems . Psychological Science 20 , 5 ( 2009 ) , 546 – 548 . [ 51 ] Lisa Marie Rhody . 2016 . Why I dig : Feminist approaches to text analysis . Debates in the digital humanities ( 2016 ) . [ 52 ] David M Ryfe . 2006 . Narrative and deliberation in small group forums . Journal of Applied Communication Research 34 , 1 ( 2006 ) , 72 – 93 . [ 53 ] Mariana Salgado and Michail Galanakis . 2014 . . . . so what ? : limitations of participatory design on decision - making in urban planning . In Proceedings of the 13th Participatory Design Conference : Short Papers , Industry Cases , Workshop Descriptions , Doctoral Consortium papers , and Keynote abstracts - Volume 2 . ACM , 5 – 8 . [ 54 ] Matthew J Salganik and Karen EC Levy . 2015 . Wiki surveys : Open and quantiﬁable social data collection . PloS one 10 , 5 ( 2015 ) . [ 55 ] Lynn M Sanders . 1997 . Against deliberation . Political theory 25 , 3 ( 1997 ) , 347 – 376 . [ 56 ] Anselm Strauss and Juliet Corbin . 1994 . Grounded theory methodology . Handbook of qualitative research 17 ( 1994 ) , 273 – 85 . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1180 [ 57 ] Cass R Sunstein . 2002 . The law of group polarization . Journal of political philosophy 10 , 2 ( 2002 ) , 175 – 195 . [ 58 ] Agustina Eskenazi Alena Stern Rebecca Latourell Takaaki Masaki , Samantha Custer . 2017 . Decoding Data Use : How do leaders source data and use it to accelerate development ? Educational Leadership ( 2017 ) . [ 59 ] Linda Tischler . 2011 . Looking for Bold Ideas to Fix the City , New York Turns to Crowd Sourcing . ( Jan 2011 ) . [ 60 ] Amos Tversky and Daniel Kahneman . 1974 . Judgment under uncertainty : Heuristics and biases . science 185 , 4157 ( 1974 ) , 1124 – 1131 . [ 61 ] Fernanda B Viégas , Scott Golder , and Judith Donath . 2006 . Visualizing email content : portraying relationships from conversational histories . In Proceedings of the SIGCHI conference on Human Factors in computing systems . ACM , 979 – 988 . [ 62 ] Vasillis Vlachokyriakos , Clara Crivellaro , Christopher A Le Dantec , Eric Gordon , Pete Wright , and Patrick Olivier . 2016 . Digital civics : Citizen empowerment with and through technology . In Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems . ACM , 1096 – 1099 . [ 63 ] Furu Wei , Shixia Liu , Yangqiu Song , Shimei Pan , Michelle X Zhou , Weihong Qian , Lei Shi , Li Tan , and Qiang Zhang . 2010 . Tiara : a visual exploratory text analytic system . In Proc . ACM Conf . on Knowledge Discovery and Data Mining . 153 – 162 . [ 64 ] Y . Wu , S . Liu , K . Yan , M . Liu , and F . Wu . 2014 . OpinionFlow : Visual Analysis of Opinion Diffusion on Social Media . IEEE Trans . Visualization and Computer Graphics 20 , 12 ( Dec 2014 ) , 1763 – 1772 . [ 65 ] Yingcai Wu , Furu Wei , Shixia Liu , Norman Au , Weiwei Cui , Hong Zhou , and Huamin Qu . 2010 . OpinionSeer : interactive visualization of hotel customer feedback . IEEE Trans . Visualization and Computer Graphics 16 , 6 ( 2010 ) , 1109 – 1118 . [ 66 ] Amy X Zhang , Bryan Culbertson , and Praveen Paritosh . 2017a . Characterizing online discussion using coarse discourse sequences . In Proceedings of the Eleventh International Conference on Web and Social Media . AAAI Press . [ 67 ] Amy X Zhang , Lea Verou , and David Karger . 2017b . Wikum : Bridging discussion forums and wikis using recursive summarization . ( 2017 ) . [ 68 ] Jian Zhao , Nan Cao , Zhen Wen , Yale Song , Yu - Ru Lin , and Christopher Collins . 2014 . # FluxFlow : Visual analysis of anomalous information spreading on social media . IEEE Transactions on Visualization and Computer Graphics 20 , 12 ( 2014 ) , 1773 – 1782 . [ 69 ] Roshanak Zilouchian Moghaddam , Zane Nicholson , and Brian P Bailey . 2015 . Procid : Bridging consensus building theory with the practice of distributed design discussions . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing . ACM , 686 – 699 . [ 70 ] Torre Zuk and Sheelagh Carpendale . 2007 . Visualization of uncertainty and reasoning . In Smart graphics . Springer , 164 – 177 . Civic and Urban Spaces DIS ' 19 , June 23 – 28 , 2019 , San Diego , CA , USA 1181