ORIGINAL PAPER The Consistencies and Vagaries of the Washington State Inventory of Evidence - Based Practice : The Deﬁnition of ‘‘Evidence - Based’’ in a Policy Context Sarah Cusworth Walker 1 • Aaron R . Lyon 1 • Steve Aos 2 • Eric W . Trupin 1 Published online : 18 April 2015 (cid:2) Springer Science + Business Media New York 2015 Abstract As states increasingly establish the importance of evidence - based practice through policy and funding man - dates , the deﬁnition of evidence - based practice can have a signiﬁcant impact on investment decisions . Not meeting established criteria can mean a loss of funding for established programs and the implementation disruption of programs without a strong research base . Whether the deﬁnition of ‘‘evidence - based’’ is inﬂuenced by these high stakes contexts is an interesting question that can inform the larger ﬁeld about the value and utility of evidence - based practice lists / inven - tories for disseminating knowledge . In this paper we review the development of the Washington State Inventory of Evi - dence - Based , Research - Based and Promising Practices as a case study for the process of deﬁning evidence - based practice in a policy context . As part of this study we also present a comparison of other well - known evidence - based practice inventories and examine consistencies and differences in the process of identifying and developing program ratings . Keywords Evidence - based practice (cid:2) Inventories (cid:2) Policy Introduction The growing discipline of implementation science includes extensive discussion of the systemic elements needed for successful uptake of evidence - based practice , including readiness assessments , training and quality assurance strate - gies , sustainment planning and various modes of outcome measurement ( Aarons et al . 2011 ; Hoagwood et al . 2014 ; Proctor et al . 2009 ; Schoenwald et al . 2012 ; Fixsen et al . 2005 ) . Less emphasis is given to discussing the relative merits of different rating criteria for deﬁning which programs are well - supported . The general literature , in fact , often treats the term ‘‘evidence - based’’ as a static designation with a clear deﬁnition . However , even a cursory review of the more well - known inventories reveal variation in the process and criteria for determining what works . Further , as states move towards needing to deﬁne ‘‘evidence’’ for the purpose of policymak - ing , additional variation can creep into inventories . Policy concerns that may become reﬂected in the criteria include the cost - beneﬁt of programs , existing ﬁdelity criteria , and appli - cabilitytoculturalgroups , tonameafew . Inthispaper , wewill describe the development of the Washington State Evidence - Based Practice Inventory in light of these complexities , place the inventory in the context of other high - proﬁle inventory efforts , and discuss the beneﬁts and limitations of existing inventory ratings for policymaking and practice . The Evidence - Based Practice Paradigm The term ‘‘evidence - based practice’’ is used in various ways . In the research literature , the term is often used to denote a program with rigorous evidence of efﬁcacy or effectiveness through randomized controlled studies or quasi - experimental designs ( e . g . , Aarons et al . 2011 ) . & Sarah Cusworth Walker secwalkr @ uw . edu Aaron R . Lyon lyona @ uw . edu Steve Aos steve . aos @ wsipp . wa . gov Eric W . Trupin trupin @ uw . edu 1 Department of Psychiatry and Behavioral Sciences , University of Washington School of Medicine , 2815 Eastlake Ave E Ste 200 , Seattle , WA 98102 , USA 2 Washington State Institute for Public Policy , 110 Fifth Ave SE , Ste 214 , P . O . Box 40999 , Olympia , WA 98504 , USA 123 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 DOI 10 . 1007 / s10488 - 015 - 0652 - y However , the term originates with Sackett et al . ’s ( 2000 ) deﬁnition applied to practice settings as ‘‘the integration of the best research evidence with clinical expertise and pa - tient values’’ ( p . 1 ) which , in turn , is derived from the evidence - based medicine model ( Guyatt et al . 2008 ) . This deﬁnition is echoed in the American Psychological Asso - ciation’s deﬁnition of evidence - based practice as ‘‘the in - tegration of the best available research with clinical experience in the context of patient characteristics , culture and preferences’’ ( APA 2006 ) . As a practice model , sub - stantial weight is given to clinician expertise and patient preferences ; however , lack of deﬁnitional speciﬁcity for both of these constructs has contributed to an emphasis on tested programs as the central feature of evidence - based practice ( Drisko 2013 ) . This is true for the research lit - erature as well as for professionals’ perceptions of EBPs , in which there is considerable confusion around the deﬁnition of evidence - based practice ( Rubin 2008 ) . Taking the ap - proach adopted by Division 12 of the APA , ‘‘empirically supported treatment’’ ( EST ) or ‘‘empirically validated treatment’’ is the correct terms for describing tested pro - grams with rigorous evidence of effectiveness whereas evidence - based practice identiﬁes a clinical decision making model integrating EST’s within the practice con - text ( Chambless and Ollendick 2001 ) . While theoretically of equal value , the greater emphasis given to ESTs in the literature , over clinical and patient expertise ( given the lack of clarity in these domains ) , is affecting the approaches taken by state and local governments to mandate effective services . The evidence - based practice ‘‘movement’’ for preven - tion and intervention in child and youth services has gained signiﬁcant policy momentum in the last two decades . Evidence - based practice is endorsed by federal policy , foundation and philanthropic initiatives and is being man - dated , incentivized or otherwise encouraged by an in - creasing number of states ( Hoagwood et al . 2014 ) . Division 12 ( Clinical Psychology ) of the American Psychological Association ﬁrst published reviews of empirically - sup - ported treatments ( ESTs ) for psychiatric disorders in the mid - 1990’s ( Chambless et al . 1996 ) . This interest in ESTs was encouraged by an international evidence - based medi - cine movement exempliﬁed by the Cochrane Collabora - tion , an organization promoting systematic reviews of research as a strategy for identifying the most effective practices ( Sackett 2000 ) . Following this approach , the Division 12 Task Force on Promotion and Dissemination of Psychological Procedures embarked on an exhaustive re - view of the existing literature for rigorously studied pro - grams demonstrating successful outcomes . Other efforts to make evidence accessible to practi - tioners about what works have largely adopted a similar strategy in developing and maintaining lists of programs ranked according to levels of empirical rigor . As the challenge of program implementation has become more apparent , some have argued for other methods of distilling information about what works to practitioners through common principles or strategies based on meta - analyses ( Chorpita et al . 2005 ; Lipsey et al . 2010 ) . Despite these efforts , current policies are still largely committed to evi - dence - based program implementation as the primary mode of research translation and treatment quality . Consequent - ly , the criteria against which programs are rated as effec - tive are highly relevant and carry real world implications for funding and dissemination . This is precisely the case in Washington State where the effects of House Bill ( HB ) 2536 are currently being felt across public , child - serving systems . As described in Kerns and Trupin ( this issue ) , the strategy for treatment quality improvement in Washington State—codiﬁed in HB 2536— requires the state’s children’s mental health , juvenile jus - tice and child welfare departments to substantially increase funding allocations to evidence or research - based programs ( for this paper , abbreviated EBP ) by 2019 . To accomplish this , the departments must rely on a list of approved pro - grams in order to demonstrate baseline levels of use and improvements over time . As a legislatively - created and funded entity with a his - tory of innovative work on the cost - beneﬁt of programs for child outcomes , the Washington State Institute of Public Policy ( WSIPP ) was an obvious choice to help develop the list along with the Evidence - Based Practice Institute ( EBPI ) , also legislatively funded , located at the University of Washington . EBPI was created in 2008 through House Bill 1088 ( HB 1088 ) for the purpose of supporting the integration of evidence - based practice within children’s mental health . WSIPP was developed by the legislature in 1983 to carry out practical , non - partisan research at the direction of the legislature . WSIPP has subsequently be - come well - known for developing cost - beneﬁt estimates that calculate , in dollar amounts , potential money saved resulting from program implementation ( Aos et al . 2006 ) . The legislature directed these entities to develop the HB 2536 inventory in time for the state departments to submit a baseline report of currently funded programs , as will be described in more detail below . ( In ) Consistency Among Inventories In developing the Washington State Inventory , WSIPP and EBPI looked to other inventories as a guide for how to identify and rate programs . We discovered that inventories were largely consistent in requiring programs to meet some threshold for evaluative rigor ; however , differences emerged in the process of identifying and rating programs as well as Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 43 123 the level of rigor required for an evidence - based practice designation ( or comparable highest level ) . To understand how different inventories develop and maintain lists of programs , the process of review and the review criteria of these inventories were examined ; includ - ing how programs were initially chosen , how the inventory is updated , how information about programs are obtained , and the standard of evidence used . The most well - known in - ventories for child and adolescent well - being were reviewed including the California Evidence - Based Clearinghouse ( CEBC ; www . cebc4cw . org ) , Blueprints for Violence Prevention ( Blueprints ; www . blueprintsprograms . com ) , Ofﬁce of Juvenile Justice and Delinquency Prevention ( OJJDP ; www . crimesolutions . gov ) , Substance Abuse and Mental Health Administration National Registry of Evi - dence - Based Programs ( NREPP ; www . nrepp . samhsa . gov ) , the Institute of Education Sciences What Works Clearing - house ( IES ; www . whatworks . ed . gov ) , the American Psy - chological Association Division 53 list of evidence - based treatments ( APA ; www . effectivechildtherapy . ﬁu . edu ) and , for a similar policy - focused comparison , the state of Ore - gon’s Department of Addictions and Mental Health ap - proved programs list ( Oregon ; www . oregon . gov / oha / amh / pages / ebp / main . aspx ) . The process of review and standards of evidence were obtained from accessible descriptions of the lists from or - ganization websites , journal articles and some personal communications with the organizations to obtain sufﬁcient information about the review process to allow for a com - parison of the inventories across all the identiﬁed domains . Most information was easily accessible through informa - tion provided online . Consequently , judgments about how other lists were constructed and updated relied on the ac - curacy of these sources of information . Table 1 includes a description of the process of review broken into four components : ( 1 ) How programs are initially chosen , ( 2 ) how programs are updated , ( 3 ) how information for review is obtained , and ( 4 ) decision - making and the use of ex - pertise . Coding within each of these areas developed it - eratively through an initial review of each inventory to develop tentative codes and then a second review of in - ventories to reﬁne and ﬁnalize codes . Coding was inde - pendently conﬁrmed , through consensus , by the ﬁrst two authors of the paper . Program Selection and Review Process How Programs are Chosen In this category , inventories were coded according to whether literature searches were conducted on published research and / or unpublished research , whether program selection was guided by a topic expert , and whether ex - ternal nominations of programs were solicited . For our coding , ‘‘external’’ refers to any nominations of programs submitted by individuals not pre - identiﬁed by the inventory as staff or expert reviewers . All the inventories , with the exception of NREPP , appear to employ some review of the literature in order to identify program candidates for review . While eight of the nine inventories also have a process for accepting nominations from external sources ( external to the staff conducting the literature reviews ) , only two inventories undertake a literature search of studies published outside of the peer - reviewed literature ( IES , OJJDP ) . Further , CEBC appears to be somewhat unique in having a topic area expert assigned to identify the scope of relevant programs contained within that area for the initial review . How Programs are Updated In this category , inventories were coded according to how programs were added and updated after the initial review . This could include reviews of published and / or unpub - lished literature , the use of topic experts , and nominations from external sources . After initial program selection , most of the inventories rely solely on external nominations for adding new programs . Blueprints and APA53 are the only inventories that conduct routine literature reviews ( pub - lished only ) in the same topic area for updates ; Blueprints appears to conduct reviews more frequently ( monthly ) as compared to APA53 ( no deﬁnite schedule ) . Blueprints also solicits external nominations while Div53 relies on lit - erature reviews exclusively . Because program developers are often aware of the various inventories , relying on ex - ternal nominations for updates is likely to capture a fair amount of new evidence because developers are motivated to submit their programs for review ; however , a process that includes both internally - driven literature reviews and nominations from external sources seems likely to identify the widest array of possible programs ( only Blueprints appears to employ both currently ) . The value of which method to use for inventory updates , however , also de - pends on the intended purpose of the inventory . For in - ventories designed to summarize all the research on interventions in a given area ( e . g . , APA53 ) , restriction to published literature may be warranted . However , for in - ventories designed to support large - scale implementation— such as Washington , Oregon and , arguably , NREPP , OJJDP and CEBC—there is additional value in soliciting programs from external sources in order to capture pro - grams that administrators / clinicians may already have in - terest in pursuing and which may be overlooked in a literature review of published research . 44 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123 Decision - Making and the Use of Experts All of the inventories , with the exception of Div53 , conduct some kind of initial screen for eligibility before programs are sent to an expert for additional review . However , it should be noted that Oregon only employed an ‘‘expert’’ when internal staff disagreed about the program designa - tion . An expert was not always clearly deﬁned across in - ventories - the most general description being someone with a high level of content knowledge . NREPP sends applications to a methods expert as well as program / dis - semination expert which rate different aspects of the pro - gram . Reviewer checklists are used by Blueprints and OJJDP for rating the quality of evaluations ( discussed further below ) while other inventories rely more heavily on the expertise of the reviewer to make judgments about quality within speciﬁc parameters . IES , CEBC and NREPP all described strategies for reaching out to the program developer or nominator as a routine aspect of the review process . Lastly , ﬁnal decisions for most of the inventories were consensus - based . For CEBC , OJJDP and Oregon the inventory director or topic expert takes the role of tie - breaker when / if needed ( in Oregon , for example , this was only needed once ) . Overall , the various inventories share many common processes for identifying , updating , and reviewing programs with modest variation . The largest differences involved whether unpublished studies are routinely re - viewed when updating the inventories ( 3 / 7 ) , the use of checklists to guide reviewer decision - making ( 2 / 7 ) , and whether more than one expert reviews each program ( 4 / 7 ) . Criteria Used for Program Review Inventories also vary in the degree to which they explicitly consider different elements of evaluation quality when conducting program reviews . Table 2 outlines the criteria used by the inventories to determine the level of evaluative rigor for reviewed programs . The process of cross - refer - encing these criteria across inventories was challenging given differences in the degree of speciﬁcity provided by each source and how information about evaluation quality was used . For example , while some elements ( e . g . , ran - domized controlled trials ) were simply considered by some inventories in their decision making process , other inven - tories explicitly required included studies to meet certain criteria to be included in their review . To simplify the in - dexing of these criteria , we only noted which criteria were explicitly considered in a review , without reference to whether meeting a speciﬁc criterion was necessary . Similarly , we did not seek to quantify the weight each Table 1 Program Selection and Review Process IES CEBC Blueprints OJJDP NREPP Oregon Div53 How programs are initially chosen Literature search published x x x x x Literature search unpublished x x Topic expert x External nomination x x x x x How programs are updated Literature search published x x Literature search unpublished Topic expert External nomination x x x x x x How information for review is obtained Literature search published x x x x x Literature search unpublished x x Developer x External source x x Decision making and the use of experts Non expert initial screen x x x x x x Multiple expert review x x x x x Communication with developer x x x Consensus based x x x x Tie breaker x x Oregon does not currently have funding for additional program review and is not accepting nominations Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 45 123 criterion lent to the ﬁnal rating algorithm or category , only whether the criteria were included in that rating . There is , however , one exception to this : CEBC is the only inventory which requires a program to have been tested via ran - domized controlled trials ( RCT ) to receive its highest ranking , which we have noted in Table 2 . In all other cases , the criteria should not be interpreted as necessary condi - tions for inclusion in each inventory , but as guidelines for reviewers as they make their recommendations . As expected , the criteria for review across all invento - ries included design and analytic strategies intended to reduce internal threats to validity . Interestingly , the need for a comparison group of some kind was not always ex - plicitly noted , likely because it is a fundamental assump - tion of all guidelines . For example , NREPP deﬁnes Appropriateness of Analysis as ‘‘analyses were appropriate for inferring relationships between intervention and out - come , ’’ but does not make direct mention of comparison groups . Beyond this fundamental assumption , the speciﬁcs of the criteria noted by different inventories varied a moderate amount . Criterion Agreement Across Inventories The criteria included in Table 2 are taken , mostly verbatim , from the inventory descriptions of review processes . Some inventories provide more detailed operationalizations of terms than others . Consequently , the ﬁrst two authors de - termined whether a criterion applied across inventories through consensus ( as described above ) . For the most part , the criteria in Table 2 are expected to be self - explanatory ; however , some criteria warrant further explanation . For instance , the developmental approach to program eval - uation refers to implementation of continuous quality im - provement activities when real world contingencies make counterfactual comparisons ( e . g . , control group ) untenable . Table 2 Criteria used for program review SAMHSA Oregon Blueprints CEBC OJJDP IES Div53 Agreement Reliability of measures x x x x x 5 Replication x x x x x 5 Validity of measures x x x x 4 Confounding variables x x x x 4 Appropriateness of analysis x x x x 4 Intervention ﬁdelity measured in studies x x x 3 Missing data and attrition x x x 3 Availability of implementation materials x x x 3 Peer review journal x x x 3 Sample size x x x 3 Sustained effects x x x 3 Weight of evidence x x x 3 Availability of quality assurance x x 2 Blind design x x 2 Outcome measures are not unique to program x x 2 Theory of change x x 2 Representativeness of sample x x 2 Effect sizes x x 2 No iatrogenic effects x x 2 Year of publication x x 2 Clearly identiﬁed / deﬁned sample x x 2 RCT only x 1 Availability of training and support x 1 Real World settings x 1 Continuous Quality Improvement x 1 Intent to treat x 1 Sig results compared to sig tests x 1 Independence of evaluator x 1 46 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123 Real world settings refers to whether programs had been evaluated in the community ( e . g . , effectiveness trials ) . Signiﬁcant results compared to signiﬁcance tests evaluates the number of outcomes tested in one study in order to determine whether some outcomes could be the result of chance due to multiple comparisons . Representativeness of the sample referred to the appropriateness of the sample for the outcomes being measured and generalizability to a treatment / intervention population . In order to determine the degree of agreement among the inventories regarding the criteria used for determining levels of evidence , we calculated an ‘‘agreement’’ score for each criterion by counting the number of inventories en - dorsing that element and dividing by 7 ( total number of inventories ) . The average agreement score across all cri - terions was . 36 , or about 36 % agreement across all crite - ria . Agreement ranged from 1 to 5 ( i . e . , the criterion with the lowest endorsements included only one inventory and the highest had ﬁve inventories specifying the same crite - rion ) . The criteria with the highest number of endorsements included the reliability of measures ( 5 / 7 ) , replication ( 5 / 7 ) , validity of measures ( 4 / 7 ) , control for confounding vari - ables ( 4 / 7 ) and appropriateness of analysis ( 4 / 7 ) . The lowest included the need for an RCT for the highest level of evidence ( 1 / 7 ) , availability of training and support ( 1 / 7 ) , the current implementation of the program in real world settings ( 1 / 7 ) , and , a continuous quality improvement ap - proach for programs difﬁcult to manipulate for evaluation ( 1 / 7 ) . Within the low - frequency criteria , the latter two come from Oregon’s inventory and the second ( training and support ) came from NREPP , both which are focused on providing information for supporting implementation . These results clearly indicate the shared emphasis on in - ternal validity over external validity among inventories in the designation of exemplary programs . Only Oregon , which by necessity needed to consider the feasibility of implementation , explicitly included criteria around whether a program was studied in a real world setting and , uniquely , an alternative method to support best practice through a continuous quality improvement approach when outcomes evaluation , per se , could not be supported . Both NREPP and CEBC also included criteria around implementation materials , training and support ( NREPP ) and availability of quality assurance ( NREPP ) . This suggests that inventories may increase their focus on external validity as they be - come more directly tied to implementation and policy initiatives . Of all the inventories , Blueprints has the largest number of criteria to which reviewers are expected to attend . A checklist and guidelines for reviewers speciﬁes 18 areas for review . These areas cover most of the domains mentioned by other inventories with the exception of domains relevant to implementation ( availability of materials , training and QA ) . We should be clear , however , that all of these do - mains could potentially be—and likely are—considered by a reviewer for any of the inventories . The difference in - volves the degree to which each is explicitly identiﬁed as an area of review , rather than an assumption that reviewer expertise is sufﬁcient to ensure that adequate consideration is given to the appropriate domains . It is therefore difﬁcult to know , based only on these criteria , whether any given program would be rated differently across inventories . It could be that the differences in the explicit guidance given for review masks general similarity in approach and results . To examine this more closely , we looked at two programs to view how they fared across inventories : Multisystemic Therapy ( MST ) and Aggression Replacement Training ( ART ) , Table 3 . Cross Inventory Comparison of MST and ART MST was chosen because it was hypothesized that it would be rated at the highest level across all inventories for which it was a review candidate , given its reputation in the ﬁeld and a substantial body of evidence supporting its efﬁcacy , including multiple randomized trials ( Henggeler et al . 1997 ; Sawyer and Borduin 2011 ) . ART was chosen be - cause its evidence - base is less deﬁnite , which was hy - pothesized to provide an opportunity to detect variation among inventories ( Glick 1987 ; Porter 2013 ) . Results of inventory reviews were largely consistent with our hy - pothesis , but also yielded important differences . MST was given the highest rating by OJJDP , CEBC , Blueprints and Oregon ( which simply listed whether programs were ap - proved or not ) but only received a 3 . 0 ( out of 5 . 0 ) from NREPP for evidence tying MST directly to reductions in long term arrest rates . Further , MST was rated as ‘‘Prob - ably Efﬁcacious’’ by APA53 for disruptive behavior and ‘‘Well - supported’’ for substance abuse . In contrast , CEBC rated MST as well - supported for disruptive behavior and substance abuse . As anticipated , ART was rated differently across groups , most notably in that it was only reviewed by 3 of the 7 inventories . Of the three inventories providing ratings , OJJDP rated ART as Effective ( highest ) , NREPP rated it as 3 . 0 out of 5 for violent recidivism ( the same rating given to MST for long term arrest ) , and CEBC rated it as promising ( as compared to MST’s rating of well - supported ) . MST and ART were not rated by IES because neither has been extensively applied within the educational context . Together , the ratings for both MST and ART demon - strate variability across inventories . This variability ap - pears to be due to a combination of both the process of review ( e . g . , studies relevant to outcomes reviewed by the inventory may have been unknown or were excluded be - cause they were not published ) and the criteria Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 47 123 ( requirements for randomized design , replication and overall rigor ) . While ratings for substance abuse outcomes consistently place MST at the highest level of evidence ( CEBC , APA53 , Blueprints ) , reviews in other domains were mixed ( arrest , disruptive behavior , internalizing symptoms ) . Our review of MST and ART placement within inventories highlights some difﬁculties with using these sources to make decisions regarding program selec - tion and implementation due to ( 1 ) differences in the out - comes being assessed ; ( 2 ) variation in the studies included for reviews ; and ( 3 ) variation in the criteria used for re - view . These variations are more likely to affect programs that have an emerging evidence base than those that are better established . At the same time , the process of ad - vancing a program to a place where it can be considered well - established is costly and time - consuming , thus limit - ing the ﬁeld of contenders . For Washington State , this was a key consideration in determining what would be counted as research - and evidence - based in service of the larger goal of reconciling evaluative rigor with community en - gagement and implementation feasibility . Washington State Inventory Washington’s approach to program identiﬁcation includes both systematic literature review and program nomination processes , led by EBPI and WSIPP . As stated earlier , EBPI and WSIPP were directed by HB2536 to work together on developing the state inventory . This was directed to happen in two phases . First , evidence - based ( EB ) , research - based ( RB ) and promising practices ( PP ) needed to be deﬁned by September 30 , 2012 ( Table 4 ) . Second , an inventory of EB , RB and PP practices was to be completed in time for the child welfare , juvenile justice and children’s mental health divisions of the Department of Social and Health Services ( DSHS ) to complete a baseline assessment of current programming by June 30th , 2013 . Because the DSHS divisions needed to know what programs qualiﬁed prior to developing a count of programs , EBPI and WSIPP set an internal deadline of January 2013 to complete the ﬁrst draft of the inventory . From the time of the bill’s enactment in March 2012 to January 2013 , EBPI and WSIPP had approximately nine months to develop deﬁnitions , conduct a review of pro - grams and rate these programs according to these deﬁnitions . Further , while EBPI and WSIPP were identi - ﬁed as the leads on developing the inventory , represen - tatives from the DSHS divisions and the legislature were also included in planning meetings . Also , as noted by other papers in the issue , concurrent engagement and outreach activities were occurring with community pro - viders , with a particular focus on cultural responsiveness . Together , the perspectives of this diverse group of stakeholders ( i . e . , providers , DSHS , EBPI , WSIPP ) yielded at least four main , overlapping priorities for the inventory : ( 1 ) Interest in providing services that would be cost - beneﬁcial ; ( 2 ) Interest in having currently - funded programs represented on the list ; ( 3 ) Scientiﬁc interest in including programs with a high degree of evaluative rigor ; and , ( 4 ) Community interest in ensuring programs would be culturally responsive . These interests were addressed in both the category deﬁnitions and the review process , as outlined below . Evaluative Rigor The deﬁnition of ‘‘evidence - based’’ in the Washington State inventory requires multiple or multiple - site random - ized and / or statistically - controlled evaluations . This fol - lows WSIPP’s existing standards for research quality which include the following : ( 1 ) Use of a control / com - parison group or sophisticated statistical methods ( e . g . , propensity score matching , regression discontinuity , or instrumental variables ) ; ( 2 ) Intent - to - treat methodology , in which results for completers and non completers are Table 3 Comparison of ratings for MST and ART OJJDP NREPP * CEBC IES APA53 Blueprints Oregon MST ‘‘Effective’’ ( highest ) 3 . 0 ( out of 5 ) for long term arrestrates ‘‘Well - supported’’ ( highest ) for disruptive behavior , substance abuse , behavioralmanagement . Notreviewed ‘‘ProbablyEfﬁcacious’’ ( for disruptivebehavior ) , ‘‘Well established’’ for substance abuse ‘‘Model’’ ( highest ) for relationships with parents , delinquency , externalizing , drug use , internalizing , sexual violence , violence ‘‘Approved’’ ART ‘‘Effective’’ ( highest ) 3 . 0 ( out of 5 ) for violentrecidivism ‘‘Promising’’ ( 3 ) for disruptive behavior Notreviewed Not reviewed Not included Not listed * ART was reviewed as part of a residential model ( Mendota Juvenile Treatment Center ) 48 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123 reported ; ( 3 ) Random assignment or quasi - experimental design when sufﬁcient information is provided about pre - existing conditions ; and ( 4 ) well - validated and / or stan - dardized outcome measures . The process of identifying programs includes literature searches by WSIPP as well as an external nomination process managed by EBPI ( Lyon et al . 2014 ) . WSIPP conducts independent searches of the literature , including contacting the evaluators and devel - opers of programs to ﬁnd both published and unpublished work that document positive and unfavorable outcomes . This is important because reviewing only published papers is likely to bias outcomes towards positive results ( How - land 2011 ) . In addition , single studies that include many evaluation sites ( e . g . , national studies ) may be sufﬁcient to designate a program as evidence - based as long as the other requirements for design and statistical rigor are met . However , in most cases , multiple studies are needed to establish a program as evidence - based . In contrast , only one study is required , with the same standards for statistical rigor , for a designation of ‘‘research - based . ’’ Due to the short timeframe , the ﬁrst published inventory was largely populated with programs already reviewed by WSIPP for other purposes , but which aligned well with the goals of HB 2536 . These reviews were conducted at the request of the legislature to determine the cost - beneﬁt of programs in juvenile justice , children’s mental health and child welfare . To locate studies for cost - beneﬁt reports , WSIPP identiﬁed existing systematic reviews , followed up on studies cited in bibliographies of other studies , con - ducted literature searches through research databases ( Google Scholar , Proquest , EBSCO , ERIC , PubMed and SAGE ) and contacted authors of primary research to learn about ongoing and unpublished work as noted above . When new programs for the inventory are proposed through external nomination and appear to meet criteria for being research or evidence - based , the EBPI sends the program to WSIPP , which then conducts a review of the published and unpublished literature to identify any addi - tional research on the program before determining whether it meets criteria . If the program does not meet criteria for research or evidence - based , WSIPP sends the program back to EBPI , which reviews the program for a promising practice designation ( see Lyon et al . this issue , for a de - tailed description of the promising practices review ) . Cost - Beneﬁt WSIPP’s beneﬁt - cost model is currently being imple - mented in 13 states as part of a collaboration between Pew Charitable Trusts and the MacArthur Foundation . The model was reviewed by an independent team assembled by Pew which included faculty and research directors at aca - demic and nonproﬁt economic research centers . The ben - eﬁt - cost calculations begin with a meta - analysis of programs using literature identiﬁed by the processes out - lined above using the standardized mean difference effect size . For dichotomous outcomes , the Cox transformation process is used as an unbiased approximation of the stan - dardized mean effect size ( Sa´nchez - Meca et al . 2003 ) . Additional adjustments to effect sizes are made to address pre / post measures , small sample sizes , and multi - level data . In addition to adjustments made for statistical control , WSIPP also makes adjustments to effect sizes to better estimate results that are more likely to be achieved in real world settings ( in contrast to unadjusted effect sizes re - ported by studies in more controlled conditions ) . WSIPP makes four kinds of adjustments for real world imple - mentation : ( 1 ) the methodological quality of each study ; ( 2 ) the relevance or quality of the outcome measure ; ( 3 ) the independence of the evaluator ; and ( 4 ) laboratory or other non - real world settings . Typically , the effect of the ad - justment factors produces a smaller , adjusted effect size . To estimate change in ﬁnancial outcomes resulting from a program or policy , WSIPP examines direct program Table 4 Washington state inventory levels of evidence Evidence - based A program or practice that has been tested in heterogeneous or intended populations with multiple randomized and / or statistically - controlled evaluations , or one large multiple - site randomized and / or statistically - controlled evaluation , where the weight of the evidence from a systematic review demonstrates sustained improvements in at least one of the following outcomes : child abuse , neglect , or the need for out of home placement ; crime ; children’s mental health ; education ; or employment . Further , ‘‘evidence - based’’ means a program or practice that can be implemented with a set of procedures to allow successful replication in Washington and , when possible , has been determined to be cost - beneﬁcial Research - based A program or practice that has been tested with a single randomized and / or statistically - controlled evaluation demonstrating sustained desirable outcomes ; or where the weight of the evidence from a systematic review supports sustained outcomes as identiﬁed in the term ‘‘evidence - based’’ in RCW ( the above deﬁnition ) but does not meet the full criteria for ‘‘evidence - based . ’’ Promisingpractice A program or practice that , based on statistical analyses or a well - established theory of change , shows potential for meeting the ‘‘evidence - based’’ or ‘‘research - based’’ criteria , which could include the use of a program that is evidence - based for outcomes other than the alternative use Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 49 123 effect sizes as well as ‘‘linked’’ effect sizes . Some direct program effects can be monetized ; for example , a program that reduces child abuse will be expected to reduce child welfare system costs and victimization costs to the child . The ﬁnancial impact of linked outcomes can also be esti - mated . For example , if the reduction of child abuse is known to improve rates of high school graduation , a pro - gram’s effect on reducing child abuse can be linked to the ﬁnancial beneﬁts of high school graduation . The basic beneﬁt - cost model implements a standard economic cal - culation of the expected worth of program implementation ( investment ) by computing the net present value of a stream of estimated beneﬁts and costs that occur over time . The beneﬁts and costs are impacted by the number of outcome ‘‘units’’ produced by the program , which are derived by the effect size as outlined above . Price terms are estimated through various methods in - cluding the following : ( 1 ) labor market earnings associated with outcomes such as high school graduation , standard - ized test scores , number of years of education , mortality associated with alcohol , smoking and mental health dis - orders ; ( 2 ) value to taxpayers through avoiding crime and costs that can be avoided by people who would otherwise have been a victim of crime . The avoidance of crime in - cludes costs of police and sheriffs , superior courts , local juvenile detention , state juvenile corrections and so on . Both short and long term costs are estimated . Avoidance of victimization includes costs associated with tangible victim costs ( medical and mental health care , property damage and loss , etc . ) and intangible victim costs , which places a dollar amount on pain and suffering . Child abuse and ne - glect outcomes are also monetized by estimating the value of avoiding out of home placements as well as estimates for victim costs . ( 3 ) Alcohol , illicit drug , regular tobacco use and mental health beneﬁts are estimated using an ‘‘inci - dence - based’’ approach to estimating how much beneﬁt could be obtained if alcohol , tobacco and other drug use was reduced or mental health needs were reduced . Avoided costs are estimated based on labor market earnings , med - ical costs , crime costs , and trafﬁc collision costs . Further information on the model can be obtained from WSIPP’s technical manual which explains additional modeling with discount terms , diminishing returns , and studies used to establish the strength of association between direct pro - gram outcomes and linked outcomes ( WSIPP 2014 ) . Evaluating Funded Programs As the primary purpose of the inventory was to provide a list of eligible programs for DSHS , there was signiﬁcant interest in determining whether existing , funded programs would meet eligibility criteria . Consequently , a method for soliciting information about existing programs and evaluating them against the inventory criteria was devel - oped . The EBPI manages these submissions through the Promising Practice application ( Lyon , this issue ) , which is a slight misnomer as any program submissions go through this application process regardless of the level of evaluative rigor . EBPI conducts an initial screen of submissions to determine whether any programs have submitted eval - uations that appear to include a control group or sufﬁcient statistical control . If this is the case , the program is sent to WSIPP for a more intensive review . The Promising Prac - tice application is open to the public and accepts any program nominations . Programs that do not meet standards for research - or evidence - based are evaluated against the criteria for promising practices which require at least one of the following : ( 1 ) a pre / post test demonstrating im - provement on direct outcomes of interest to DSHS ; ( 2 ) a pre / post test demonstrating improvement on variables known in the literature to be linked to outcomes of interest ; or ( 3 ) a well - speciﬁed theory of change that is empirically - based . Further , programs cannot include elements that ap - pear to be iatrogenic based on known literature . Because promising practices are not ‘‘countable’’ under HB2536 , inclusion on the inventory as promising serves the purpose of highlighting programs that may ﬁll treatment gaps so that the Department of Social and Health Services in Washington State can make decisions about whether to support additional evaluation or program development for these practices . In addition , EBPI provides tiered technical assistance to these programs and to programs which have not made it to promising practice status focusing on logic models , internal data infrastructure , research design and data analysis . Cultural Responsiveness Concerns about cultural responsiveness played a signiﬁcant role in the development of the category deﬁnitions as well as the development of a submission process for promising programs . Representatives from EBPI met with a number of community and minority - focused coalitions and agen - cies to discuss the impact of HB2536 on practice . A con - sistent theme was concern about the appropriateness of evidence - based practice with diverse communities . Speciﬁcally , individuals were concerned that programs validated with largely white samples would now be re - quired for all populations . To address this concern , an additional requirement that programs be studied with heterogeneous populations was added to the deﬁnition of evidence - based . Heterogeneous was operationalized as in - cluding at least 30 % minority youth across all available evaluation studies because approximately 30 % of chil - dren / youth in Washington State identify as an ethnic mi - nority . Though imperfect , this was agreed upon as a 50 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123 workable approach for trying to indicate whether programs had some validity with diverse populations , as many pro - grams do not report subgroup analyses or interactions . While adjusting the deﬁnition increased the likelihood that well - established programs would be evaluated according to their appropriateness for diverse communities , a parallel effort was made to recruit submissions of community programs serving ethnic minorities through the Promising Practice application . There was enthusiasm on the part of EBPI , WSIPP and planning partners to get a better sense of the programming already occurring in communities of color in order to increase the visibility of these efforts . In sum , the Washington State Inventory was developed through careful consideration of the internal and external validity of programs , including the evaluative rigor of available evidence , cost - beneﬁt , consideration of already funded programs , and cultural responsiveness . It could be argued that the evidence - based practice movement has evolved through a ﬁrst stage of widespread acceptability and is now in a second stage of wrestling with the chal - lenges of implementation in real world settings ( McHugh and Barlow 2010 ) . As such , the Washington State inven - tory provides a glimpse into the products likely to result from the blend of research and policy contingencies in this new era . Discussion The use of program inventories to disseminate information about what works for improving outcomes in children’s mental and behavioral health is a particular strategy that has largely dominated translational research efforts . While not speciﬁcally studied , these inventories have likely con - tributed signiﬁcantly to widespread knowledge of ‘‘what works’’ by offering accessible resources to support research and practice . These inventories serve many different pur - poses . For administrators , inventories can be searched to identify programs that may ﬁt a local need or can be used to validate whether implemented programs have any evi - dence connecting them to desired outcomes . For re - searchers , inventories can point to areas where additional research would be beneﬁcial ( e . g . , to determine the appli - cability of an established program with a novel population ) or even gaps in services where new program development may be needed . However , our analysis of inventories points to considerable variation in how inventories are con - structed , and that this variation is often a function of an inventory’s intended purpose . While the internal validity of programs appears to be the dominant focus across current inventories , issues of external validity are becoming in - creasingly salient as the evidence - based ‘‘movement’’ wrestles with barriers to implementation and begins to address directly the contingencies of practice and policy - driven concerns ( Glasgow et al . 2006 ) . Given these chal - lenges , the continued use of inventories should be analyzed as one option among a growing array of strategies to en - courage the integration of best practices into real world service settings ( Powell et al . 2012 ) . A potential problem with the use of inventories in policymaking is variation in treatment rankings and the likelihood that states will develop their own criteria to make sense of this variation and align approved programs with local goals . For example , Oregon initially adopted all of the NREPP programs ( at the time , NREPP only listed programs that met a certain evidence level , cite ) but also conducted a separate literature review and solicited exter - nal nominations . Washington State began with meta - analytic reviews previously conducted by WSIPP and then added external nominations rather than automatically ac - cepting programs from other lists given the nuances of HB2536’s speciﬁc , legislative criteria . When implementing new evidence - based practice poli - cies , a state or administration could conceivably specify that any programs meeting a certain evidence level on an existing inventory would be fundable or otherwise ‘‘ap - proved’’ for implementation . Nevertheless , the examples from Washington and Oregon suggest that states are more likely to constrain or , possibly expand , criteria given the realities of local implementation . A similar example comes from ( Bumbarger and Campbell 2012 ) , who describe the process of EBP implementation in Pennsylvania . In a large - scale implementation effort , communities applying for state dollars to expand capacity for evidence - based practice were initially able to choose any program identiﬁed on a pre - approved list developed by the Communities that Care initiative ( Hawkins and Catalano 1992 ) . However , the translational agency supporting this effort soon realized the challenge of supporting such a diverse portfolio of pro - grams ; consequently , the program list was drastically re - duced . Decisions about program eligibility were made with attention to both research evidence and practical concerns . Inventories appear to be useful in their general capacity for increasing knowledge about what works ( i . e . , dissemina - tion ) , but their impact on direct practice in the absence of grant funding tied to a speciﬁc inventory ( Mihalic 2008 ) is largely unknown . Perhaps their most useful function is as a resource for the development of more localized criteria . Another challenge of the inventory approach is that it currently emphasizes internal validity and rigorous eval - uation to the exclusion or relative devaluation of commu - nity needs and input , despite the importance of community credibility for successful uptake ( Flaspohler et al . 2008 ; Wandersman et al . 2008 ) . Implementation in diverse communities is hindered by the perception that evidence - based programs are incompatible with local values ( Bernal Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 51 123 et al . 2009 ; BigFoot and Funderburk 2011 ; Lau 2006 ) not responsive or too rigid for the community context and pos - sibly present a form of modern day colonization ( Larios et al . 2011 ) . While multiple examples of successful implementa - tion of EBPs in diverse communities exist and the avail - ability of program funding is a powerful incentive for implementation even when concerns about cultural ﬁt are present ( Echo - Hawk 2011 ) , the ﬁeld does not yet have a good rubric for understanding the speciﬁc elements of a program or the contextual variables that may be more conducive to cross - community implementation ( e . g . , program length , meeting program requirements , ﬂexibility , cost , funding strategies and incentives for participating ) . Increased atten - tion and emphasis on these factors in program inventories would be useful . Further , the use of inventories and an overall framework that emphasizes top down dissemination hinders the investigation of alternative strategies that in - corporate principle ( Chorpita et al . 2005 ; Lipsey et al . 2007 ) or practice - based evidence into dissemination strategies ( Wandersman et al . 2008 ) or uses co - design as a method of increasing uptake ( Weisz et al . 2012 ) . These alternatives may have useful , important lessons for achieving the overall public health goal of widespread improvement in practice that can be masked by an exclusive focus on program - based strategies for implementation . A relevant example of an alternative to the use of speciﬁc programs on the Washington State inventory is the incorporation of broad program categories that are not tied to any speciﬁc program model , but are supported by meta - analyses . Mentoring and drug courts are both included on the Washington State inventory as research - based ap - proaches to reducing youth offending because compre - hensive literature reviews indicate that these approaches , in the aggregate , result in positive outcomes ( EBPI and WSIPP 2014 ) . Consequently , it was anticipated that the state could expect generally positive results by imple - menting various kinds of mentoring and drug court models . This suggests that the locally - developed , not rigorously evaluated models of mentoring programs and drug courts operating out of various counties in Washington State that have high community credibility and promising outcomes should be eligible for funding through House Bill 2536 . However , the literature also suggests that ﬁdelity to par - ticular versions of these programs ( e . g . , Big Brothers Big Sisters ) is likely to yield more consistent , reliable out - comes , the promotion of which is the primary driver behind HB2536 . This situation is a good example of the ways in which research knowledge must contend with the reality of contextual factors in the promotion of real world outcomes . WSIPP and EBPI are considering multiple options for addressing this issue . One option is to allow both locally - developed programs and name - brand programs to ‘‘count’’ as eligible because of the likelihood that the state will realize general , albeit modest , beneﬁts from the aggregate impact of these diverse programs . The challenge of this approach is that any individual community may implement a program that has no evidence of effectiveness , or worse , may have iatrogenic effects . Another possible option includes a hybrid model of eligibility that would allow locally developed programs to qualify as eligible as long as they include principle - based criteria for implementation . This would require the devel - opment of program standards with ﬂexibility around how the standards may be implemented in local contexts as well as an accrediting body that would review these programs for adherence to these standards . For example , a credible organization ( e . g . Mentoring Works Washington ) could be given a contract to monitor the quality of all approved mentoring programs in the state using standards developed by MENTOR – National Mentoring Partnership ( www . men toring . org ) . These standards outline benchmarks for re - cruiting , training , and monitoring mentors as well as data collection and quality assurance . Programs would then need to be accredited to be fundable and the accrediting body would provide some control around program quality while allowing individual programs to retain a unique identity and community credibility . This approach shares some similarities with other principle - based or common elements approaches to evidence synthesis ( Chorpita et al . 2005 ; Embry and Biglan 2008 ; Lipsey et al . 2010 ) and community - based approaches to program quality . The challenges of implementation include the same ﬁnancial challenges of funding quality assurance for evidence - based practice , although there would be potential savings around training and boosters because there would be less emphasis on centralized control of programming . Trade - offs in the effect sizes of outcomes would be expected as compared to an evidence - based practice approach , but there would be balancing beneﬁts for community credibility and possible sustainment that might offset this loss ( Chorpita et al . 2005 ) . The most conservative application of this approach might only allow this type of principle - based program monitoring to occur when no programs in the class of programming have found to have harmful effects or when meta - analyses of the class of programming ﬁnd the general approach to be effective . Another option is to remove the broad categories from eligibility on the inventory and only fund speciﬁc evidence - based programs because of our conﬁdence that these pro - grams will yield the best results . In this approach , there is an assumption ( or hope ) that local communities will be sufﬁ - ciently motivated by the contingencies of funding to either embark on long term , large scale research efforts to validate their programs or convert their programs into EBPs through an investment in training and abdication of some local own - ership . A substantial area of the literature suggests that 52 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123 attention to the local context may be an efﬁcient way of en - couraging widespread best practice ( Aarons et al . 2014 ; Beidas et al . 2013 ) however , as these two competing frame - works ( program vs . principle - based ) have not been evaluated side by side , it is unclear the extent to which an alternative approach would be more successful in achieving outcomes . Limitations In this paper we present an analysis of the value of in - ventory - based approaches to evidence - based program dis - semination and implementation through the lens of Washington State’s experience with developing an Evi - dence - Based Practice inventory . As such , we do not ana - lyze , in depth , other approaches to developing policy - oriented inventories and our analyses are limited to the experiences in one jurisdiction . We also limited our ana - lysis to some of the most commonly used inventories of evidence - based practices relevant for children’s mental and behavioral health . Consequently , our analysis is not com - prehensive and the inclusion of more inventories may have yielded even more insight into possible discrepancies tied to the intended uses of the tools . Conclusion The process of developing the Washington State Inventory of Research and Evidence - Based Practice brought to bear multiple considerations regarding empirical rigor , cost beneﬁt and the realities of implementation . Our review of other inventories revealed some variability in the ways programs were rated , raising questions regarding the con - sistency with which research knowledge is being dis - seminated to the public and the role of inventories in promoting best practice . Furthermore , inventory charac - teristics and review criteria were largely a function of the initiatives for which they were created . We conclude that , despite inconsistencies , inventories can play a useful role , primarily as a resource for the development of local criteria for implementation . Further , the continued integration of external validity considerations into these inventories ( e . g . , ﬂexibility and applicability across diverse contexts ) will increase their usefulness . The criteria used in the Wash - ington State Inventory highlight the contingencies faced by real world implementation , particularly in regards to the balance of local credibility and best practice . Multiple strategies for this integration ( principle and practice - based approaches ) should be explored along with traditional program - based implementation to achieve the ultimate goal of broad , public health impact . References Aarons , G . A . , Fettes , D . L . , Hurlburt , M . S . , Palinkas , L . A . , Gunderson , L . , Willging , C . E . , & Chafﬁn , M . J . ( 2014 ) . Collaboration , negotiation , and coalescence for interagency - collaborative teams to scale - up evidence - based practice . Journal of Clinical Child & Adolescent Psychology , 5 , 1 – 14 . Aarons , G . A . , Hurlburt , M . , & Horwitz , S . M . ( 2011 ) . Advancing a conceptual model of evidence - based practice implementation in public service sectors . Administration and Policy in Mental Health and Mental Health Services , 38 ( 1 ) , 4 – 23 . doi : http : / / dx . doi . org / 10 . 1007 / s10488 - 010 - 0327 - 7 . Aos , S . , Miller , M . , & Drake , E . ( 2006 ) . Evidence - based public policy options to reduce future prison construction , criminal justice costs , and crime rates . Olympia : Washington State Institute of Public Policy . Beidas , R . S . , Edmunds , J . , Ditty , M . , Watkins , J . , Walsh , L . , Marcus , S . , & Kendall , P . ( 2013 ) . Are inner context factors related to implementation outcomes in cognitive - behavioral therapy for youth anxiety ? Administration and Policy in Mental Health and Mental Health Services Research , 2 . Bernal , G . , Jimenez - Chafey , M . I . , & Domenech Rodriguez , M . M . ( 2009 ) . Cultural adaptation of treatments : A resource for considering culture in evidence - based practice . Professional Psychology : Research and Practice , 40 ( 4 ) , 361 – 368 . BigFoot , D . S . , & Funderburk , B . W . ( 2011 ) . Honoring children , making relatives : The cultural translation of parent - child inter - action therapy for American Indian and Alaska Native families . Journal of Psychoactive Drugs , 43 ( 4 ) , 309 – 318 . doi : 10 . 1080 / 02791072 . 2011 . 628924 . Bumbarger , B . , & Campbell , E . ( 2012 ) . A state agency - university partnership for translational research and the dissemination of evidence - based prevention and intervention . Administration and Policy in Mental Health and Mental Health Services Research , 39 ( 4 ) , 268 – 277 . doi : 10 . 1007 / s10488 - 011 - 0372 - x . Chambless , D . , & Ollendick , T . ( 2001 ) . Empirically supported psychological interventions : Controversies and evidence . Annual Review of Psychology , 52 , 685 – 716 . Chambless , D . L . , Sanderson , W . C . , Shoham , V . , Bennett Johnson , S . , & Pope , K . S . ( 1996 ) . An update on empirically validated therapies . Clinical Psychology , 49 ( 2 ) , 5 – 18 . Chorpita , B . , Daleiden , E . , & Weisz , J . ( 2005 ) . Identifying and selecting the common elements of evidence based interventions : A distillation and matching model . Mental Health Services Research , 7 ( 1 ) , 5 – 10 . Echo - Hawk , H . ( 2011 ) . Indigenous communities and evidence building . Journal of Psychoactive Drugs , 43 ( 4 ) , 269 – 275 . Embry , D . , & Biglan , A . ( 2008 ) . Evidence - based kernels : Funda - mental units of behavioral inﬂuence . Clinical Child and Family Psychology Review , 11 ( 3 ) , 75 – 113 . doi : 10 . 1007 / s10567 - 008 - 0036 - x . Fixsen , D . L . , Naoom , S . F . , Blase , K . A . , Friedman , R . M . , & Wallace , F . ( 2005 ) . Implementation research : A synthesis of the literature . Tampa , FL : University of South Florida , Louis de la Parte Florida Mental Health Institute , The National Implemen - tation Research Network . Flaspohler , P . , Duffy , J . , Wandersman , A . , Stillman , L . , & Maras , M . A . ( 2008 ) . Unpacking prevention capacity : An intersection of research - to - practice models and community - centered models . American Journal of Community Psychology , 41 ( 3 - 4 ) , 182 – 196 . Glasgow , R . E . , Green , L . W . , Klesges , L . M . , Abrams , D . B . , Fisher , E . B . , Goldstein , M . G . , & Orleans , C . T . ( 2006 ) . External validity : We need to do more . Annals of Behavioral Medicine , 31 ( 2 ) , 105 – 108 . doi : 10 . 1207 / s15324796abm3102 _ 1 . Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 53 123 Glick , B . G . A . P . ( 1987 ) . Aggression replacement training . JCAD Journal of Counseling & Development , 65 ( 7 ) , 356 – 362 . Guyatt , G . , Rennie , D . , Meade , M . , & Cook , D . ( 2008 ) . Preface to users’ guides to the medical literature . In Essentials of evidence - based clinical practice ( 2nd ed . ) . New York : McGraw - Hill . http : / / jamaevidence . com / resource / preface / 520 . Hawkins , J . , & Catalano , F . ( 1992 ) . Communities that care : Action for drug abuse prevention . San Francisco : Jossey - Bass Publishers . Henggeler , S . W . , Melton , G . B . , Brondino , M . J . , Scherer , D . G . , & Hanley , J . H . ( 1997 ) . Multisystemic therapy with violent and chronic juvenile offenders and their families : The role of treatment ﬁdelity in successful dissemination . Journal of Con - sulting and Clinical Psychology , 65 ( 5 ) , 821 – 833 . Hoagwood , K . E . , Olin , S . S . , Horwitz , S . , Cleek , A . , Gleacher , A . , Lewandowski , E . , & Hogan , M . ( 2014 ) . Scaling up evidence - based practices for children and families in New York State : Toward evidence - based policies on implementation for state mental health systems . Journal of Child Clinical and Adolescent Psychology , 43 ( 2 ) , 145 – 157 . doi : 10 . 1080 / 15374416 . 2013 . 8697 49 . Howland , R . H . ( 2011 ) . What you see depends on where you’re looking and how you look at it : Publication bias and outcome reporting bias . Journal of Psychosocial Nursing and Mental Health Services , 49 ( 8 ) , 13 – 15 . Larios , S . E . , Wright , S . , Jernstrom , A . , Lebron , D . , & Sorensen , J . L . ( 2011 ) . Evidence - based practices , attitudes , and beliefs in substance abuse treatment programs serving American Indians and Alaska Natives : A qualitative study . Journal of Psychoactive Drugs , 43 ( 4 ) , 355 – 359 . doi : 10 . 1080 / 02791072 . 2011 . 629159 . Lau , A . S . ( 2006 ) . Making the case for selective and directed cultural adaptations of evidence - based treatments : Examples from parent training . Clinical Psychology : Science & Practice , 13 ( 4 ) , 295 – 310 . doi : 10 . 1111 / j . 1468 - 2850 . 2006 . 00042 . x . Lipsey , M . W . , Howell , J . C . , Kelly , M . R . , Chapman , G . , & Carver , D . ( 2010 ) . Improving the effectiveness of juvenile justice programs : A new perspective on evidence - based practice . Washington , DC : Center for Juvenile Justice Reform , George - town University . http : / / cjjr . georgetown . edu / pdfs / ebp / ebppaper . pdf . Lipsey , M . W . , Howell , J . C . , & Tidd , S . T . ( 2007 ) . The standardized program evaluation protocol ( SPEP ) : A practical approach to evaluating and improving juvenile justice programs in North Carolina . Final evaluation report . Nashville : Vanderbilt Univer - sity , Center for Evaluation Research and Methodology . Lyon , A . R . , Lau , A . S . , McCauley , E . , Vander Stoep , A . , & Chorpita , B . F . ( 2014 ) . A case for modular design : Implications for implementing evidence - based interventions with culturally di - verse youth . Professional Psychology : Research and Practice , 45 ( 1 ) , 57 – 66 . doi : 10 . 1037 / a0035301 . McHugh , R . K . , & Barlow , D . H . ( 2010 ) . The dissemination and implementation of evidence - based psychological treatments : A review of current efforts . American Psychologist , 65 ( 2 ) , 73 – 84 . Mihalic Sf , F . A . A . A . S . ( 2008 ) . Implementing the life skills Training drug prevention program : Factors related to implementation ﬁdelity ( p . 3 ) . Implementation Science : IS . Porter , D . ( 2013 ) . Aggression replacement training : A comprehensive intervention for aggressive youth . School of Social Work Journal , 37 ( 2 ) , 113 – 114 . Powell , B . J . , McMillen , J . C . , Proctor , E . K . , Carpenter , C . R . , Griffey , R . T . , Bunger , A . C . , & York , J . L . ( 2012 ) . A compilation of strategies for implementing clinical innovations in health and mental health . Medical Care Research and Review , 69 ( 2 ) , 123 – 157 . doi : 10 . 1177 / 1077558711430690 . Proctor , E . K . , Landsverk , J . , Aarons , G . , Chambers , D . , Glisson , C . , & Mittman , B . ( 2009 ) . Implementation research in mental health services : An emerging science with conceptual , methodological , and training challenges . Administration & Policy in Mental Health & Mental Health Services Research , 36 ( 1 ) , 24 – 34 . doi : 10 . 1007 / s10488 - 008 - 0197 - 4 . Rubin , A . ( 2008 ) . Practitioner’s guide to using research for evidence - based practice . Hoboken , NJ : Wiley . Sackett , D . L . ( 2000 ) . Evidence - based medicine : How to practice and teach EBM . Edinburgh ; New York : Churchill Livingstone . Sa´nchez - Meca , J . M . , Marin - Martinez , F . , & Chacon - Moscoso , S . ( 2003 ) . Effect - size indices for dichotomized outcomes in meta - analysis . Psychological Methods , 8 ( 4 ) , 448 – 467 . Sawyer , A . M . , & Borduin , C . M . ( 2011 ) . Effects of multisystemic therapy through midlife : A 21 . 9 - year follow - up to a randomized clinical trial with serious and violent juvenile offenders . Journal of Consulting and Clinical Psychology , 79 ( 5 ) , 643 – 652 . Schoenwald , S . K . , Chapman , J . E . , Henry , D . B . , & Sheidow , A . J . ( 2012 ) . Taking effective treatments to scale : Organizational effects on outcomes of multisystemic therapy for youths with co - occurring substance use . Journal of Child & Adolescent Substance Abuse , 21 ( 1 ) , 1 – 31 . doi : 10 . 1080 / 1067828X . 2012 . 636684 . Wandersman , A . , Duffy , J . , Flaspohler , P . , Noonan , R . , Lubell , K . , Stillman , L . , et al . ( 2008 ) . Bridging the gap between prevention research and practice : The interactive systems framework for dissemination and implementation . American Journal of Com - munity Psychology , 41 ( 3 – 4 ) , 3 – 4 . Washington State Institue for Public Policy ( WSIPP ) . ( 2014 ) . Beneﬁt - cost technical documentation . Olympia , WA . http : / / www . wsipp . wa . gov / TechnicalDocumentation / WsippBeneﬁtCostTechnical Documentation . pdf . Weisz , J . R . , Chorpita , B . F . , Palinkas , L . A . , Schoenwald , S . K . , Miranda , J . , Bearman , S . K . , & Gibbons , R . D . ( 2012 ) . Testing standard and modular designs for psychotherapy treating depression , anxiety , and conduct problems in youth : A random - ized effectiveness trial . Archives of General Psychiatry , 69 ( 3 ) , 274 – 282 . doi : 10 . 1001 / archgenpsychiatry . 2011 . 147 . 54 Adm Policy Ment Health ( 2017 ) 44 : 42 – 54 123