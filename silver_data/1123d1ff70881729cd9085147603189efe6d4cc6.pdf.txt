I MPROVING P ROTOTYPICAL P ART N ETWORKS WITH R EWARD R EWEIGHING , R ESELECTION , AND R ETRAIN - ING Robin Netzorg ∗ University of California , Berkeley Berkeley , CA robert _ netzorg @ berkeley . edu Jiaxun Li ∗ Harvard University Cambridge , MA jiaxun _ li @ g . harvard . edu Bin Yu University of California , Berkeley Berkeley , CA binyu @ stat . berkeley . edu A BSTRACT In recent years , work has gone into developing deep interpretable methods for image classification that clearly attributes a model’s output to specific features of the data . One such of these methods is the prototypical part network ( ProtoPNet ) , which attempts to classify images based on meaningful parts of the input . While this method results in interpretable classifications , it often learns to classify from spurious or inconsistent parts of the image . Hoping to remedy this , we take in - spiration from the recent developments in Reinforcement Learning with Human Feedback ( RLHF ) to fine - tune these prototypes . By collecting human annotations of prototypes quality via a 1 - 5 scale on the CUB - 200 - 2011 dataset , we construct a reward model that learns human preferences and identify non - spurious proto - types . In place of a full RL update , we propose the reweighed , reselected , and retrained prototypical part network ( R3 - ProtoPNet ) , which adds an additional three steps to the ProtoPNet training loop . The first two steps are reward - based reweighting and reselection , which align prototypes with human feedback . The final step is retraining to realign the model’s features with the updated prototypes . We find that R3 - ProtoPNet improves the overall meaningfulness of the proto - types , and maintains or improves individual model performance . When multiple trained R3 - ProtoPNets are incorporated into an ensemble , we find increases in both interpretability and predictive performance . 1 I NTRODUCTION With the widespread use of deep learning , having these models be interpretable is more important now than ever . As these models continue to see use in high - stakes situations , practitioners hoping to justify a decision need to understand how a deep model makes a prediction , and trust that those explanations are valuable and correct Rudin et al . ( 2021 ) . One such proposed method for image classification is the prototypical part network ( ProtoPNet ) , which classifies a given image based on its similarity to prototypical parts of training images , called prototypes Chen et al . ( 2019 ) . This model aims to combine the power of deep learning with an intuitive reasoning module similar to humans . While ProtoPNet aims to learn meaningful prototypical concepts , in practice , learned prototypes suffer from learning spurious concepts , such as the background of an image , from inconsistent concepts , such as learning both the head and the wing of a bird , and from duplicating concepts , such as having two prototypes that correspond to the same wing of the same bird Bontempelli et al . ( 2023 ) . Such problems are highly detrimental to the efficacy of these models , resulting in wasted computation at best and incorrect reasoning at worst . Various methods have been proposed to account for these ∗ Equal contribution . 1 a r X i v : 2307 . 03887v2 [ c s . L G ] 5 O c t 2023 issues Bontempelli et al . ( 2023 ) ; Nauta et al . ( 2021 ) ; Barnett et al . ( 2021 ) , but these methods involve either costly labelling procedures or fall short of providing a persistent means of measuring prototype quality . We seek to increase the performance of the learned prototypes by taking inspiration from recent advances in reinforcement learning with human feedback ( RLHF ) Ouyang et al . ( 2022 ) and reward learning Lee et al . ( 2023 ) . RLHF and reward learning have become popular approaches for aligning large language models with human preferences , partially due to the flexibility of learned rewards and feedback collection methods Askell et al . ( 2021 ) . While prior work has incorporated human feedback into ProtoPNets Barnett et al . ( 2021 ) ; Bontempelli et al . ( 2023 ) , no variation of ProtoPNet has incorporated a cheap and flexible reward learning fine - tuning framework . Towards this end , we propose the reward reweighed , reselected , and retrained prototypical part network ( R3 - ProtoPNet ) , which seeks to improve the original ProtoPNet via fine - tuning with a learned reward model . With minimal human feedback data on the Caltech - UCSD Birds - 200 - 2011 ( CUB - 200 - 211 ) dataset Welinder et al . ( 2010 ) , we are able to train a high - quality reward model that achieves 90 . 1 % test accuracy when ranking human preferences , serving as a strong measure for prototype quality . R3 - ProtoPNet is then able to improve the meaningfulness of prototypes , removing dependence on spurious features , and is able to slightly decrease inconsistency across images compared to the original ProtoPNet . When used as base learners in an ensemble , R3 - ProtoPNet is able to outperform an ensemble of ProtoPNets on a held - out test dataset . In summary , our contributions are as follows . Firstly , we demonstrate that a reward model trained on small amounts of human feedback data ( roughly 500 ratings ) can accurately rank human preference data . Secondly , due to the high performance of the reward model , we propose using the reward model as a measure of prototype . Thirdly , we introduce the R3 - ProtoPNet , which uses reward - guided fine - tuning to improve prototype meaningfulness and ensemble performance . 2 R ELATED W ORK 2 . 1 R EINFORCEMENT L EARNING WITH H UMAN F EEDBACK Since the success of InstructGPT Ouyang et al . ( 2022 ) , Reinforcement Learning with Human Feedback ( RLHF ) has received a great deal of attention in the machine learning community . Although this success is recent , incorporating human feedback into reinforcement learning methods via a learned reward model has a deep history in reward learning Christiano et al . ( 2017 ) ; Jeon et al . ( 2020 ) . While works taking inspiration from InstructGPT have used proximal policy optimization ( PPO ) to fine - tune networks with human feedback Bai et al . ( 2022 ) , it is unclear to the extent that formal reinforcement learning is necessary to improve models via learned reward functions Lee et al . ( 2023 ) , or if the human feedback needs to follow a particular form Askell et al . ( 2021 ) . Some prior work incorporates the reward function as a way to weigh the likelihood term Stiennon et al . ( 2022 ) ; Ziegler et al . ( 2019 ) . Keeping this work in mind , we incorporate the reward model into ProtoPNet as a way to reweigh prototypes post - training . 2 . 2 E XAMPLE - BASED M ODELS AND P ROTOTYPICAL P ART N ETWORKS The field of interpretable deep learning is vast , with a plethora of explainability and interpretability methods available to the user . For a more complete overview of interpretable deep learning , please refer to Rudin et al . ( 2021 ) . To ground the discussion , we focus primarily on example - based models , one such example being ProtoPNet . While ProtoPNet is our model of interest , other example - based methods exist , such as the non - parametric xDNN Angelov and Soares ( 2019 ) or SITE , which performs predictions directly from interpretable prototypes Wang and Wang ( 2021 ) . While other example - based methods exist , we focus on the ProtoPNet due to its intuitive reasoning structure . Since its introduction by Chen et al . ( 2019 ) , ProtoPNets have received a great deal of attention , and various iterations have been developed . Work has explored extending the ProtoPNet to different architectures such as transformers ( Xue et al . ( 2022 ) ) , or sharing class information between prototypes ( Rymarczyk et al . ( 2021 ) ) . Donnelly et al . ( 2022 ) increase the spatial flexibility of ProtoPNet , allowing prototypes to change spatial positions depending on the pose information available in the image . 2 ProtoPNets and variations have seen success in high - stakes applications , such as kidney stone identification ( Flores - Araiza et al . ( 2022 ) ) and mammography ( Barnett et al . ( 2021 ) ) . Many works have commented on how the original ProtoPNet tends to overemphasize spurious features , and they have taken different approaches to solving this issue . Nauta et al . ( 2021 ) introduce a explainability interface to ProtoPNet , allowing users to see the dependence of the prototype on certain image attributes like hue and shape . The authors claim that seemingly dissimilar or spurious prototypes share certain difficult - to - perceive features , like texture or contrast . Barnett et al . ( 2021 ) introduce a variation of the ProtoPNet , IAIA - BL , which biases prototypes towards expert labelled annotations of classification - relevant parts of the image . Similar to how we provide human feedback at the interpretation level , Bontempelli et al . ( 2023 ) introduce the ProtoPDebug , where a user labels a prototype and image pair as " forbidden " or " valid " , and a fine - tuning step maximizes the distance between learned prototypes and patches in the forbidden set and minimizes the distance between learned prototypes and patches in the valid set . While also incorporating human feedback , Bontempelli et al . ( 2023 ) do not ground their method in RLHF , but instead includes the binary feedback as a supervised constraint into the ProtoPNet loss function . Learning a reward function via ratings allows us to simultaneously increase the interpretability of the prototypes , and develop an evaluation metric for the quality of a particular prototype . Compared to previous approaches , reward reweighing , reselection , and retraining allows for fast collection of high - quality human feedback data and the construction of a reward model that measures prototype quality while increasing the interpretability and the performance of the model . 3 P ROTOTYPICAL P ART N ETWORK ( P ROTO PN ET ) In this section , we describe the base architecture used in our method , the Prototypical Part Network ( ProtoPNet ) introduced in Chen et al . ( 2019 ) . The ProtoPNet aims to introduce interpretability to otherwise uninterpretable image classifiers . In place of predicting from an arbitrary representation , the model makes a classification based on part attention and similar prototypical parts of an image . The general reasoning of a model is to classify an unseen image by finding training images with similar prototypical parts to those of the unseen image . This approach allows the user to interrogate the reasoning of the model , and clearly see which parts of the image led to the model’s classification . 3 . 1 D ESCRIPTION Here we briefly describe the ProtoPNet , adopting the notation used in Chen et al . ( 2019 ) . The ProtoPNet architecture builds on a base convolutional neural network f , which is then followed by a prototype layer denoted g p , and a fully connected layer h . Typically , the convolutional features are taken pretrained models like VGG - 19 , ResNet - 34 , or DenseNet - 121 . The ProtoPNet injects interpretability into these convolutional architectures with the prototype layer g p , consisting of m prototypes P = { p j } m j = 1 typically of size 1 × 1 × D , where D is the shape of the convolutional output f ( x ) . By keeping the depth the same as the output of the convolutional layer , but restricting the height and width to be smaller than that of the convolutional output , the learned prototypes select a patch of the convolutional output . Reversing the convolution leads to recovering a prototypical patch of the original input image x . Using upsampling , the method constructs a activation pattern per prototype p j . To use the prototypes to make a classification given a convolutional output z = f ( x ) , ProtoPNet’s prototype layer computes a max pooling over similarity scores : g p j ( z ) = max ˜ z ∈ patches ( z ) log ( ( ∥ ˜ z − p j ∥ 22 + 1 ) ( ∥ ˜ z − p j ∥ 22 + ϵ ) ) , for some small ϵ < 1 . This function is monotonically decreasing with respect to the distance , with small values of ∥ ˜ z − p j ∥ 22 resulting in a large similarity score g p j ( z ) . Assigning m k prototypes for all K classes , such that (cid:80) Kk = 1 m k = m , the prototype layer outputs a vector of similarity scores that matches parts of the latent representation z to prototypical patches across all classes . The final layer in the model is a linear layer connecting similarities to class predictions . In order to ensure that the prototypes match specific parts of training images , during training the prototype vectors are projected onto the closest patch in the training set . For the final trained ProtoPNet , every p j corresponds to some patch of a particular image . 3 3 . 2 L IMITATIONS While ProtoPNet is capable of providing interpretable classifications , the base training described in Chen et al . ( 2019 ) results in prototypes that are inconsistent and represent spurious features of the image ( Barnett et al . ( 2021 ) ; Bontempelli et al . ( 2023 ) ) . Additionally , same - class prototypes will often converge to the same part of the image , resulting in duplicate prototypes . Chen et al . ( 2019 ) note that a prototype whose top L ( usually L = 5 ) closest training image patches come from different classes than the target class tend to be spurious and inconsistent , focusing on features like the background . To remedy this issue , they introduce a pruning operation , removing these prototypes entirely . While pruning does remove dependency on some subpar prototypes , we find that pruning still leaves some prototypes that rely on spurious and inconsistent features and does not improve accuracy . We also find that duplicate prototypes still occur after the pruning operation as well . We visualize subpar prototypes in Figure 1 . For more examples of low - quality prototypes , please see the Appendix . 4 H UMAN F EEDBACK AND THE R EWARD R EWEIGHED , R ESELECTED , AND R ETRAINED P ROTOTYPICAL P ART N ETWORK ( R3 - P ROTO PN ET ) Inspired by the recent advances in reinforcement learning with human feedback ( RLHF ) Ouyang et al . ( 2022 ) , the reward reweighed , reselected , and retrained prototypical part network ( R3 - ProtoPNet ) utilizes a learned reward model to fine - tune prototypes . In place of pruning prototypes and sacrificing potential information , we demonstrate that incorporating human feedback into the training of the Pro - toPNet improves prototype quality while increasing ensemble accuracy . In this section , we describe the collection of high - quality human feedback data , our reward model , and how we incorporate the reward model into the training loop via a three - stage training procedure . 4 . 1 H UMAN F EEDBACK C OLLECTION A crucial aspect behind the success of RLHF methods is the collection of high quality human feedback data . Unclear or homogeneous feedback may result in a poor performing reward model Christiano et al . ( 2017 ) . The design of human feedback collection is vitally important to the training of a useful reward model . The inherent interpretability of ProtoPNet leads to a useful benefit for RLHF . Given a trained ProtoPNet , it is possible for a knowledgeable user to directly critique the learned prototypes . Given a particular classification task , a human with enough expertise should be able to recognize if a particular prototype is " good " or " bad " Bontempelli et al . ( 2023 ) . In the case of classifying birds in the CUB - 200 - 2011 dataset , one of the original classification tasks used in Chen et al . ( 2019 ) , it is clear that if a prototype gives too much weight to the background of the image ( spurious ) , or if the prototype corresponds to different parts of the bird when looking at different images ( inconsistency ) , the learned prototype is not meaningfully or interpretably contributing to prediction . Given these prototypes that fail to contribute to prediction , a lay person trying to classify birds would rate these prototypes as " bad , " with the proper rating rubric . There are many different ways to elicit this notion of " goodness " from a user Askell et al . ( 2021 ) . Although it is possible to incorporate many different forms of feedback into the R3 - ProtoPNet , such as asking a user to compare prototypes to elicit preferences or ask for a binary value of whether a prototype is " good " or " bad " , we found most success with asking the users to rate a prototype on a scale from 1 to 5 . While scalar ratings can be unstable across different raters , with a clear , rule - based rating method , rating variance is reduced and it is possible to generate high - quality labels . An example rating scale on the CUB - 200 - 2011 dataset is provided in Figure 1 . 4 . 2 R EWARD L EARNING We note that , when a user provides feedback on a prototype , it is not the training image or the model prediction that the user is providing feedback on , but the prototype’s resulting interpretation : the activation patterns . Our task is therefore different from RLHF applied to language modeling or RL tasks ( Ouyang et al . ( 2022 ) , Christiano et al . ( 2017 ) ) , where human feedback is provided on the 4 3 - 10 % - 50 % overlap with bird 4 - 50 % - 80 % overlap with bird 5 - 80 % - 100 % overlap with bird 2 - 0 % - 10 % overlap with bird 1 - No overlap with bird Figure 1 : Rubric used for human feedback on the activation patterns of predictions for birds from the CUB - 200 - 2011 dataset . Ratings of 4 - 5 are correspond to high - quality prototypes , 1 - 2 to low - quality prototypes , and 3 to unclear quality prototypes . model output or resulting state . We therefore collect a rating dataset D = { ( x i , y i , h i , j , r i , j ) } n , m i = 1 , j = 1 , where x i , y i are the training image and label , and h i , j , r i , j are prototype p j ’s activation patterns and user - provided ratings for image x i . We note that collecting preferences for this entire dataset is prohibitive and unnecessary , so we only collect a subset . Given the dataset D , we generate the induced comparison dataset , whereby each entry in D is paired with one another . Given i ̸ = i ′ and / or j ̸ = j ′ , we populate a new paired dataset , D paired , which consists of the entries of D indexed by i , j , i ′ , j ′ , and a comparison c , which takes values − 1 , 1 . If the left - hand sample is greater , and therefore considered higher - quality , r i , j > r i ′ , j ′ , then c = − 1 . If the right - hand sample is greater r i , j < r i ′ , j ′ , then c = 1 . We note that , during learning , we exclude entries with | r i , j − r i ′ , j ′ | < 0 . 5 to increase the contrast between pairs . This synthetic construction allows us to model the reward function , r ( x i , h i , j ) , via the Bradley - Terry Model for pairwise preferences Bradley and Terry ( 1952 ) . We train this model with the same loss function as in Christiano et al . ( 2017 ) , a cross - entropy loss over the probabilities of ranking one pair over the other ( See the Appendix for formulation ) . This synthetic construction combinatorially increases the amount of preference data , allowing us to train a high - quality reward model on relatively small amounts of quality human feedback data . 4 . 3 R EWARD R EWEIGHED , R ESELECTED , AND R ETRAINED P ROTOTYPICAL P ART N ETWORK ( R3 - P ROTO PN ET ) After having collected high - quality human feedback data and trained a reward model , we can now incorporate it into a fine - tuning framework to improve the interpretability of ProtoPNet . We incorporate the reward model via a three step process consisting of reward weighting , reselection , and retraining . Each step is described in more detail below . 4 . 3 . 1 R EWARD R EWEIGHING Although PPO is a popular option for RLHF ( Ouyang et al . ( 2022 ) ) , there is evidence that simpler fine - tuning algorithms can lead to similar performance increases ( Askell et al . ( 2021 ) ) . Inspired by the success and the ease of implementation of reward - weighted learning Lee et al . ( 2023 ) ; Stiennon et al . ( 2022 ) ; Ziegler et al . ( 2019 ) , we develop a reward - weighted update for the ProtoPNet : max p j L reweigh ( z ∗ i , p j ) = max p j n (cid:88) i ∈ I ( p j ) r ( x i , p j ) 1 λ dist ∥ z ∗ i − p j ∥ 22 + 1 ( 1 ) where z ∗ i = argmin z ∈ patches ( f ( x i ) ) ∥ z − p j ∥ 22 , I ( p j ) = { i | y i ∈ class ( p j ) } , and λ dist is a fixed hyperparameter . We note that the loss function L reweigh is a sum of the inverse distances weighted by the reward of the prototype on that image . Since we only update the prototype p j , the only way to maximize the loss is to minimize the distance between prototype and image patches with high 5 reward r ( x i , p j ) . This causes the prototype to resemble high reward image patches , improving the overall quality of the prototypes . Wanting to preserve prototypes that already have high reward , we only update those prototypes that have relatively low mean reward ( γ ∈ { 0 . 4 , 0 . 35 } ) , and choose this reweighing threshold per base architecture and reward model ( see the Appendix for threshold choices ) . λ dist is included in the loss function to rescale distances , since the closest distances are near zero . We find best performance with λ dist = 100 . Practically , we find that optimizing this loss function leads to locally maximal solutions , resulting in local updates that do not modify prototypes with low quality values of 1 , but it’s more likely to improve prototypes with quality values of 2 or higher . If the prototype p j has high activation over the background of an image x i , for example , the closest patches z ∗ i in the training data will also be background patches , and the reward of the prototype will be low , leaving minimal room for change . It is not possible for this update to dramatically change the location of the patch in the image via this loss function . 4 . 3 . 2 P ROTOTYPE R ESELECTION In order to improve low quality prototypes that require significant manipulation , we introduce a reselection procedure based on a reward threshold . Given a prototype p j and image x i , if 1 n k (cid:80) i ∈ I ( p j ) r ( x i , p j ) < α , where α is a pre - determined threshold and n k is the number of training images in class k , or if the patch of the given prototype p j matches the patch of another prototype of the same class , we reselect the prototype . The reselection process involves iterating over patch candidates z ′ i and temporarily setting the prototype p ′ j = z ′ i , where z ′ i is chosen randomly from the patches of a randomly selected image x ′ i in the class of p j . If 1 n k (cid:80) i ∈ I ( p j ) r ( x ′ i , p ′ j ) > β , where β is an acceptance threshold , and if none of the prototypes match patch p ′ j = z ′ j , then we accept the patch candidate as the new prototype . We found that varying the α and β values per base architecture led to the best performance ( See the Appendix for threshold choices ) . We refer to the combination of reweighting and reselection as the R2 update step , and the corresponding trained model the R2 - ProtoPNet . The reasoning process behind our prototype reselection method takes inspiration from the original push operation in Chen et al . ( 2019 ) . Similar to how ProtoPNet projects prototypes onto a specific training image patch , here we reselect prototypes to be a particular reward - filtered training image patch . With a high enough acceptance threshold β , this forces the elimination of low reward prototypes while preserving the information gain of having an additional prototype . One possible alternative approach is to instead search over the training patches , and select those patches with the highest reward . We found that randomly selecting patches , in place of searching for patches with the highest reward , led to higher prototype diversity and less computation time . As discussed in Section 6 , it is possible that a reward model that more explicitly accounts for prototype diversity could alleviate the duplicate issue , but we leave this to future work . While we do not use a traditional reinforcement learning algorithm to fine - tune our model as is typically done in RLHF Askell et al . ( 2021 ) , pairing the reselection and fine - tuning steps together resembles the typical explore - exploit trade - off in RL problems . We see that fine - tuning with our reward model leads to exploit behavior , improving upon already high - quality prototypes . At the same time , the reselection step serves as a form of exploration , drastically increasing the quality of uninformative prototypes . We find that these similarities are enough to improve the quality of ProtoPNet , as discussed in the next section . 4 . 3 . 3 R ETRAINING A critical step missing in the R2 update is a connection to prediction accuracy . As discussed in Section 5 , without incorporating predictive information , performing the reward update alone results in lowered test accuracy . Since the above updates only act on the prototypes themselves , not the rest of the network , the result is a misalignment between the prototypes and the model’s base features and final predictive layer . The reward update guides the model towards more interpretable prototypes , but the reward update alone fails to use the higher quality prototypes for better prediction . To account for the lack of predictive performance , the final step of R3 - ProtoPNet is retraining . Simply retraining with the same loss function used in the original ProtoPNet update results in the realignment 6 of the prototypes and the rest of the model . Although one could worry that predictive accuracy would reduce the interpretability of the model Rudin et al . ( 2021 ) , we find that retraining increases predictive accuracy while maintaining the quality increases of the R2 update . The result is a high accuracy model with higher - quality prototypes . We explore evidence of this phenomenon and why this is the case in the following section . 5 E XPERIMENTS Here we discuss the results of training the R3 - ProtoPNet on the CUB - 200 - 2011 dataset , the same dataset as used in Chen et al . ( 2019 ) . We demonstrate that the R3 - ProtoPNet leads for higher quality prototypes across base model architectures and prototype configurations while not sacrificing predictive performance . 5 . 1 D ATASETS R3 - ProtoPNet requires two datasets : the original dataset for initial training , and the scalar ratings of activation pattern dataset . Combined , this results in the dataset described in Section 4 . To offer better comparison against the original ProtoPNet , we use the same dataset for initial training that was used in Chen et al . ( 2019 ) , the CUB - 200 - 2011 dataset Wah et al . ( 2011 ) . The CUB - 200 - 2011 dataset consists of roughly 30 images of 200 different bird species . We employ the same data augmentation scheme used in Chen et al . ( 2019 ) , which adds additional training data by applying a collection of rotation , sheer , and skew perturbations to the images , resulting in a larger augmented dataset . For the collection of the activation pattern ratings , we only provide activation patterns overlaid on the original images to the raters . Using Amazon Mechanical Turk to recruit six workers per prototype - image , and we take the average rating on an image as the user - provided rating . In total , we rate 700 prototype - image pairs according to the scale approach described in Figure 1 . 5 . 2 A RCHITECTURES AND T RAINING Similar to Chen et al . ( 2019 ) , we study the performance of R3 - ProtoPNet across five different base architectures : VGG - 19 , ResNet - 34 , ResNet - 50 , DenseNet - 121 , and DenseNet - 161 . While the original ProtoPNet sets the number of prototypes per class at m k = 10 , we additionally run the VGG - 19 architecture with m k = 5 prototypes to explore model performance when the number of prototypes is limited . No other modifications were made to the original ProtoPNet architecture . We train for 50 epochs and report results for the best performing model . The reward model r ( x i , h i ) is similar to the base architecture of the ProtoPNet . Two ResNet - 50 base architectures take in the input image x i and the associated acticvation pattern h i separately , and both have two additional convolutional layers . The outputs of the convolutional layers are concatenated and fed into a final linear layer with sigmoid activation to predict the Bradley - Terry ranking . Predicted rewards are therefore bound in the range ( 0 , 1 ) . We train the reward model for 5 epochs on a synthetic comparison dataset of 49 , 125 paired images and preference labels derived from 500 human ratings , and evaluate on a 13 , 831 testing pairs . The reward model achieves 90 . 09 % test accuracy when trained on the whole dataset , and we additionally find that the reward model converges to roughly 83 . 27 % test accuracy on a comparison dataset generated from at least 300 rated activation patterns . 5 . 3 E VALUATION M ETRICS To evaluate the performance of R3 - ProtoPNet , we compare it to ProtoPNet using three metrics : test accuracy , reward , and activation precision ( AP ) . We use test accuracy to measure the predictive per - formance of the models . As the above section demonstrates , the learned reward model achieves high accuracy in predicting which prototype ranks above another in accordance with human preferences , so we therefore use it as a measure of prototype quality . The final metric , activation precision , is a common metric that has been used in prior work to evaluate the overlap between a prototype’s activations and the pixels associated with a given bird Barnett et al . ( 2021 ) ; Bontempelli et al . ( 2023 ) , which provides a metric of interpretability independent of our method . In our work , we report a modified version of AP introduced in Bontempelli et al . ( 2023 ) to consider the specific value of the activation at each single pixel , not just the overlap alone . 7 Base ( m k ) ProtoPNet R2 - ProtoPNet R3 - ProtoPNet VGG - 19 ( 5 ) 73 . 35 ± 0 . 16 62 . 54 ± 1 . 38 75 . 54 ± 0 . 19 VGG - 19 ( 10 ) 73 . 78 ± 0 . 23 47 . 61 ± 1 . 65 75 . 34 ± 0 . 27 ResNet - 34 ( 10 ) 77 . 82 ± 0 . 10 53 . 44 ± 2 . 74 78 . 80 ± 0 . 16 ResNet - 50 ( 10 ) 76 . 53 ± 0 . 16 59 . 14 ± 2 . 36 76 . 75 ± 0 . 19 DenseNet - 121 ( 10 ) 75 . 64 ± 0 . 22 52 . 31 ± 2 . 30 77 . 10 ± 0 . 15 DenseNet161 ( 10 ) 78 . 21 ± 0 . 25 68 . 75 ± 2 . 41 78 . 04 ± 0 . 30 Ensemble ( VGG - 19 ( 10 ) + ResNet - 34 + DenseNet - 121 + DenseNet - 161 ) 80 . 95 ± 0 . 11 76 . 26 ± 0 . 84 82 . 86 ± 0 . 16 Table 1 : Average Test accuracies and standard deviations across five runs of each base architecture during the stages of R3 - ProtoPNet training , where m k is the number of prototypes per class . Ensem - bles consist of the corresponding individually trained networks . Base ( m k ) ProtoPNet R2 - ProtoPNet R3 - ProtoPNet VGG19 ( 5 ) 0 . 58 0 . 67 0 . 71 VGG19 ( 10 ) 0 . 41 0 . 64 0 . 65 ResNet - 34 ( 10 ) 0 . 39 0 . 50 0 . 48 ResNet - 50 ( 10 ) 0 . 32 0 . 47 0 . 45 DenseNet - 121 ( 10 ) 0 . 48 0 . 54 0 . 58 DenseNet - 161 ( 10 ) 0 . 43 0 . 57 0 . 54 Average 0 . 436 0 . 564 0 . 574 Table 2 : Average rewards over the test dataset of different base architectures during the stages of R3 - ProtoPNet training . By construction , values lie on the interval [ 0 , 1 ] 5 . 4 R ESULTS After training ProtoPNet , running the R2 update step , and then performing retraining , we see several trends across multiple base architectures . In Table 1 , we report the test accuracy of the different base architectures across stages of R3 - ProtoPNet training . Generally , the test accuracy from ProtoPNet substantially decreases after applying the R2 update , but retraining tends to recover most of the predictive loss , either maintaining or improving test accuracy . This accuracy maintenance and improvement demonstrates that it is possible to align prototypes with human preferences without sacrificing predictive power . In Table 2 , we report the average reward of all prototypes on all test images for a given base architecture . We see that ProtoPNet achieves an average reward between 0 . 32 and 0 . 58 across architectures , with R3 - ProtoPNet increasing average reward across all base architectures , as well . In total , R3 - ProtoPNet achieves a 31 . 65 % increase in average reward across all base architectures over ProtoPNet . Finally , we report activation precision in Table 3 . Across all base architectures , ProtoPNet has an average activation precision of 43 . 18 . As performing the R2 step increases the average reward for all base architectures , we observe a similar , if not more substantial , increase in activation precision , Base ( m k ) ProtoPNet R2 - ProtoPNet R3 - ProtoPNet VGG19 ( 5 ) 56 . 25 ± 0 . 67 66 . 31 ± 1 . 92 70 . 25 ± 1 . 21 VGG19 ( 10 ) 50 . 08 ± 0 . 64 70 . 27 ± 2 . 42 68 . 37 ± 1 . 45 ResNet - 34 ( 10 ) 32 . 04 ± 0 . 28 52 . 93 ± 2 . 15 59 . 21 ± 2 . 40 ResNet - 50 ( 10 ) 38 . 69 ± 0 . 49 46 . 26 ± 3 . 66 55 . 52 ± 1 . 94 DenseNet - 121 ( 10 ) 36 . 49 ± 0 . 37 62 . 94 ± 3 . 23 66 . 74 ± 0 . 86 DenseNet - 161 ( 10 ) 45 . 52 ± 0 . 78 60 . 87 ± 2 . 24 60 . 36 ± 1 . 28 Average 43 . 18 59 . 93 63 . 41 Table 3 : Average Activation Precision ( AP ) over the test dataset of different base architectures during the stages of R3 - ProtoPNet training . Standard deviation across the five runs is reported . 8 with all models , save for ResNet - 50 . Additionally , the final R3 step actually continues to improve activation precision , suggesting that further training increases attention on the bird even further . In all , R3 - ProtoPNet results in an 46 . 85 % increase in average activation precision over the base ProtoPNet . 5 . 5 D ISCUSSION Given the results , we see that R3 - ProtoPNet manages to increase the quality of learned proto - types without sacrificing predictive performance . Moreover , producing an ensemble of trained R3 - ProtoPNets results in an accuracy increase over an ensemble of the original trained ProtoPNets . We see that R3 - ProtoPNet results in a substantial increase of the average test reward and activation precision , verifying that prototype quality is increasing . Overall , these results demonstrate that incorporating reward information into the ProtoPNet via reweighing , reselection , and retraining does increase interpretability of ProtoPNets while maintaining or improving predictive performance , and , when incorporated into an ensemble , increases predictive performance . 6 L IMITATIONS AND F UTURE W ORK While R3 - ProtoPNet improves interpretability and predictiveness in an ensemble , there is plenty of room for improvement . We note that the reward model is trained on ratings of a single image and heatmap , highly constrained to measuring overlap between prototype and the object of interest , but it is quite possible to extend ratings to multiple images and heatmaps . This would allow for the reward model to better learn cross - image preferences , such as consistency . We hope that this could alleviate the duplicate issue as well . We note that R3 - ProtoPNet fails to entirely eliminate duplicates , with several high - reward prototypes converge to the same part of the image . While this work investigated increasing the performance of ProtoPNet , it is possible to extend the R3 update to other extensions of the ProtoPNet , such as ProtoPFormer Xue et al . ( 2022 ) . A major benefit of reward fine - tuning is its flexibility in application , and we expect that combining the R3 update with other variations of the ProtoPNet would result in further increased performance gains . Combining multiple feedback modalities , such as the binary feedback used in ProtoPDebug Bontempelli et al . ( 2023 ) , could further increase model performance . A final limitation with R3 - ProtoPNet and other methods that rely on human feedback is that the model itself might be learning features that , while seemingly confusing to a human , are helpful and meaningful for prediction . Barnett et al . ( 2021 ) argue that the ProtoPNet can predict with non - obvious textures like texture and contrast , which might be penalized via a learned reward function . Future work is necessary to investigate how ProtoPNet variants could critique human feedback , and argue against a learned reward function . 7 C ONCLUSION In this work , we propose the R3 - ProtoPNet , a method that uses a learned reward model of human feedback to improve the meaningfulness of learned prototypical parts . We find that ensembling multiple R3 - ProtoPNets results in increased performance over original ProtoPNet ensembles . Consid - ering the high performance of the reward model , we use the reward model as a measure of prototype quality alongside the established measure of activation precision , allowing us to critique the inter - pretability of ProtoPNet along a human lens . As demonstrated here , the ability of reward learning to quantize qualitative human preferences make reward - based fine - tuning a promising direction for the improvement of interpretable deep models . R EFERENCES P . Angelov and E . Soares . Towards explainable deep neural networks ( xdnn ) , 2019 . A . Askell , Y . Bai , A . Chen , D . Drain , D . Ganguli , T . Henighan , A . Jones , N . Joseph , B . Mann , N . DasSarma , et al . A general language assistant as a laboratory for alignment . arXiv preprint arXiv : 2112 . 00861 , 2021 . 9 Y . Bai , A . Jones , K . Ndousse , A . Askell , A . Chen , N . DasSarma , D . Drain , S . Fort , D . Ganguli , T . Henighan , N . Joseph , S . Kadavath , J . Kernion , T . Conerly , S . El - Showk , N . Elhage , Z . Hatfield - Dodds , D . Hernandez , T . Hume , S . Johnston , S . Kravec , L . Lovitt , N . Nanda , C . Olsson , D . Amodei , T . Brown , J . Clark , S . McCandlish , C . Olah , B . Mann , and J . Kaplan . Training a helpful and harmless assistant with reinforcement learning from human feedback , 2022 . A . J . Barnett , F . R . Schwartz , C . Tao , C . Chen , Y . Ren , J . Y . Lo , and C . Rudin . Iaia - bl : A case - based interpretable deep learning model for classification of mass lesions in digital mammography , 2021 . A . Bontempelli , S . Teso , K . Tentori , F . Giunchiglia , and A . Passerini . Concept - level debugging of part - prototype networks , 2023 . R . A . Bradley and M . E . Terry . Rank analysis of incomplete block designs : I . the method of paired comparisons . Biometrika , 39 ( 3 / 4 ) : 324 – 345 , 1952 . C . Chen , O . Li , D . Tao , A . Barnett , C . Rudin , and J . K . Su . This looks like that : deep learning for interpretable image recognition . Advances in neural information processing systems , 32 , 2019 . P . F . Christiano , J . Leike , T . Brown , M . Martic , S . Legg , and D . Amodei . Deep reinforcement learning from human preferences . Advances in neural information processing systems , 30 , 2017 . J . Donnelly , A . J . Barnett , and C . Chen . Deformable protopnet : An interpretable image classifier using deformable prototypes . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 10265 – 10275 , June 2022 . D . Flores - Araiza , F . Lopez - Tiro , E . Villalvazo - Avila , J . El - Beze , J . Hubert , G . Ochoa - Ruiz , and C . Daul . Interpretable deep learning classifier by detection of prototypical parts on kidney stones images , 2022 . H . J . Jeon , S . Milli , and A . Dragan . Reward - rational ( implicit ) choice : A unifying formalism for reward learning . In H . Larochelle , M . Ranzato , R . Hadsell , M . Balcan , and H . Lin , editors , Advances in Neural Information Processing Systems , volume 33 , pages 4415 – 4426 . Curran Asso - ciates , Inc . , 2020 . URL https : / / proceedings . neurips . cc / paper _ files / paper / 2020 / file / 2f10c1578a0706e06b6d7db6f0b4a6af - Paper . pdf . K . Lee , H . Liu , M . Ryu , O . Watkins , Y . Du , C . Boutilier , P . Abbeel , M . Ghavamzadeh , and S . S . Gu . Aligning text - to - image models using human feedback . arXiv preprint arXiv : 2302 . 12192 , 2023 . M . Nauta , A . Jutte , J . Provoost , and C . Seifert . This looks like that , because . . . explaining prototypes for interpretable image recognition . In Communications in Computer and Information Science , pages 441 – 456 . Springer International Publishing , 2021 . doi : 10 . 1007 / 978 - 3 - 030 - 93736 - 2 _ 34 . URL https : / / doi . org / 10 . 1007 % 2F978 - 3 - 030 - 93736 - 2 _ 34 . L . Ouyang , J . Wu , X . Jiang , D . Almeida , C . L . Wainwright , P . Mishkin , C . Zhang , S . Agarwal , K . Slama , A . Ray , J . Schulman , J . Hilton , F . Kelton , L . Miller , M . Simens , A . Askell , P . Welinder , P . Christiano , J . Leike , and R . Lowe . Training language models to follow instructions with human feedback , 2022 . C . Rudin , C . Chen , Z . Chen , H . Huang , L . Semenova , and C . Zhong . Interpretable machine learning : Fundamental principles and 10 grand challenges , 2021 . D . Rymarczyk , L . Struski , J . Tabor , and B . Zieli´nski . Protopshare : Prototypical parts sharing for similarity discovery in interpretable image classification . In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & amp ; Data Mining , KDD ’21 , page 1420 – 1430 , New York , NY , USA , 2021 . Association for Computing Machinery . ISBN 9781450383325 . doi : 10 . 1145 / 3447548 . 3467245 . URL https : / / doi . org / 10 . 1145 / 3447548 . 3467245 . N . Stiennon , L . Ouyang , J . Wu , D . M . Ziegler , R . Lowe , C . Voss , A . Radford , D . Amodei , and P . Christiano . Learning to summarize from human feedback , 2022 . C . Wah , S . Branson , P . Welinder , P . Perona , and S . J . Belongie . The caltech - ucsd birds - 200 - 2011 dataset . 2011 . Y . Wang and X . Wang . Self - interpretable model with transformationequivariant interpretation , 2021 . 10 P . Welinder , S . Branson , T . Mita , C . Wah , F . Schroff , S . Belongie , and P . Perona . Caltech - ucsd birds 200 . Technical Report CNS - TR - 201 , Caltech , 2010 . URL / se3 / wp - content / uploads / 2014 / 09 / WelinderEtal10 _ CUB - 200 . pdf , http : / / www . vision . caltech . edu / visipedia / CUB - 200 . html . M . Xue , Q . Huang , H . Zhang , L . Cheng , J . Song , M . Wu , and M . Song . Protopformer : Concentrating on prototypical parts in vision transformers for interpretable image recognition , 2022 . D . M . Ziegler , N . Stiennon , J . Wu , T . B . Brown , A . Radford , D . Amodei , P . Christiano , and G . Irving . Fine - tuning language models from human preferences . arXiv preprint arXiv : 1909 . 08593 , 2019 . A A LGORITHM D ESCRIPTION Algorithm 1 Reward Reweighed , Reselected , and Retrained Prototypical Part Network ( R3 - ProtoPNet ) 1 : Initialize : Collect high - quality human feedback data and train a reward model . 2 : Reward Reweighing : Perform the reward - reweighted update for the ProtoPNet , defined as follows : max p j L reweigh ( z ∗ i , p j ) = max p j n (cid:88) i ∈ I ( p j ) r ( x i , p j ) 1 λ dist ∥ z ∗ i − p j ∥ 22 + 1 Optimize this loss function , which leads to locally maximal solutions , improving the prototypes . 3 : Prototype Reselection : Run the reselection procedure based on a reward threshold . If 1 n k (cid:80) i ∈ I ( p j ) r ( x i , p j ) < α , reselect the prototype by sampling from patch candidates and temporarily setting the prototype to a new candidate that passes the acceptance threshold and is unique from other current prototypes . 4 : Retraining : Retrain the model with the same loss function used in the original ProtoPNet update , to realign the prototypes and the rest of the model . We provide a high - level description of R3 - ProtoPNet in Figure 2 . Reading from top to bottom , it illustrates the initial ProtoPNet training step , followed by reward modelling , and then the R3 updates . We also provide a high - level summary that describes the complete R3 training in Algorithm 1 . We note that z ∗ i = argmin z ∈ patches ( f ( x i ) ) ∥ z − p j ∥ 22 , I ( p j ) = { i | y i ∈ class ( p j ) } , λ dist , α are predetermined hyperparameters , and n k = | I ( p j ) | , the number of training images in class k . We trained ProtoPNet and R3 - ProtoPNet on a single A100 GPU . The ProtoPNet training took about 10 hours and required at most 100 epochs . The R2 update for each prototype needed at most 100 iterations and it took roughly 90 minutes to complete an R2 update for a ProtoPNet with 10 prototypes per class . The final retraining step for R3 - ProtoPNet required another 20 epochs and lasted roughly 3 hours . B P AIRWISE L OSS F UNCTION FOR THE R EWARD M ODEL For completeness , here is the explicit formulation of the loss function described in Section 4 . 2 : L reward = − (cid:88) i ̸ = i ′ orj ̸ = j ′ (cid:2) 1 c i , j , i ′ , j ′ = − 1 log ( exp ( r ( x i , h i , j ) ) exp ( r ( x i , h i , j ) ) + exp ( r ( x i ′ , h i ′ , j ′ ) ) ) + 1 c i , j , i ′ , j ′ = 1 log ( exp ( r ( x i ′ , h i ′ , j ′ ) ) exp ( r ( x i , h i , j ) ) + exp ( r ( x i ′ , h i ′ , j ′ ) ) ) (cid:3) where c i , j , i ′ , j ′ refers to the comparison value associated with the column indexed by i , j , i ′ , j ′ in the synthetic dataset D paired , which is explained in section 4 . 2 . The architecture of the reward model is detailed in section 5 . 2 . 11 STEP 4 : Retraining STEP 3 : Reward Reweighing and Reselection STEP 2 : Reward Modeling STEP 1 : ProtoPNet Training  Original Image Activation Pattern ProtoPNet Base Features ProtoPNet Base Features Conv . Layers For Images Conv . Layers For Patterns Concatenated Outputs Scalar Reward ( 0 to 1 ) Fully Connected ProtoPNet Conv . Outputs m - th Prototype 1st Class n - th Class 1st Prototype Similarity Score Similarity Score Need to be updated No need for update ProtoPNet Conv . Outputs m - th Prototype 1st Prototype ProtoPNet Conv . Outputs m - th Prototype 1st Class n - th Class 1st Prototype Similarity Score Similarity Score R2 - Updated R3 - Updated Determine if a prototype needs to be updated Figure 2 : A step - by - step illustration of R3 - ProtoPNet . 12 Base ( m k ) Reselection Threshold Reweigh Threshold Acceptance Threshold VGG - 19 ( 5 ) 0 . 35 0 . 40 0 . 50 VGG - 19 ( 10 ) 0 . 25 0 . 40 0 . 43 ResNet - 34 ( 10 ) 0 . 22 0 . 35 0 . 40 ResNet - 50 ( 10 ) 0 . 18 0 . 35 0 . 40 DenseNet - 121 ( 10 ) 0 . 25 0 . 35 0 . 45 DenseNet - 161 ( 10 ) 0 . 25 0 . 35 0 . 43 Table 4 : Thresholds used across base architectures during R2 step . Base ( m k ) Reselected Prototypes Reward - Reweighted Prototypes VGG - 19 ( 5 ) 44 / 1000 210 / 1000 VGG - 19 ( 10 ) 102 / 2000 385 / 2000 ResNet - 34 ( 10 ) 89 / 2000 403 / 2000 ResNet - 50 ( 10 ) 76 / 2000 441 / 2000 DenseNet - 121 ( 10 ) 231 / 2000 329 / 2000 DenseNet - 161 ( 10 ) 141 / 2000 373 / 2000 Table 5 : Total number of prototypes updated per base architecture across the two R2 steps , divided by the total number of prototypes for that network . C T HRESHOLDS AND U PDATED P ROTOTYPES As described in Section 4 . 3 , for each base architecture , the various thresholds for reweighing , reselection , and acceptance . These thresholds were chosen by examining the reward distribution of the base architectures to see if prototypes with low reward cluster around any particular values . Across models , a reweighing threshold of 0 . 4 or 0 . 35 sufficed , but further tuning was needed for the reselection and acceptance thresholds . We present the final thresholds used for each R2 step in Table 4 . Using the reselection thresholds above , we report the total number of updated prototypes for each base architecture in Table 5 . We see that , across all architectures , less than 28 % of prototypes are updated . D P ROTOTYPE E XAMPLES Here we provide some examples of prototypes from ProtoPNet , R2 - ProtoPNet , and R3 - ProtoPNet . In Figure 3 , we plot the closest image patch of the 5 prototypes trained on the VGG - 19 base architecture for four randomly selected classes from the CUB - 200 - 2011 dataset . The first row in each of blocks of images consists of the prototypes from ProtoPNet , the second row consists of prototypes from R2 - ProtoPNet , and the third row consists of prototypes from R3 - ProtoPNet . We see that the R2 update indeed centers otherwise off prototypes on the bird in question , and the complete R3 update makes those prototypes helpful for prediction . In Figure 4 , we visualize one prototype ( column ) across the same image . This Figure illustrates how the prototype changes during the R2 and R3 update when the image is held fixed . We see that prototypes are indeed localized on the birds , without having dependence on spurious features like the background . 13 Figure 3 : Closest training patches of the five prototypes of ProtoPNet ( top row ) , R2 - ProtoPNet ( middle row ) , and R3 - ProtoPNet ( bottom row ) within the same class ( 5 prototypes per class ) . Each cluster of 3 rows of images is a seperate class . Figure 4 : Prototype projections on the same image ( each column ) from ProtoPNet ( top row ) , R2 - ProtoPNet ( middle row ) , and R3 - ProtoPNet ( bottom row ) . 14