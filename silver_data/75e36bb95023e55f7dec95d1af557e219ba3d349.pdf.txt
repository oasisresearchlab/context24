CORAL : COde RepresentAtion Learning with Weakly - Supervised Transformers for Analyzing Data Analysis Ge Zhang ∗ § , Mike A . Merrill † § , Yang Liu † , Jeffrey Heer † , Tim Althoff † ∗ Department of Computer Science Peking University , Beijing , China zhangge9194 @ pku . edu . cn † Paul G . Allen School of Computer Science and Engineering University of Washington , Seattle , Washington , { mikeam , yliu0 , jheer , althoff } @ cs . washington . edu Abstract —Large scale analysis of source code , and in particular scientiﬁc source code , holds the promise of better understanding the data science process , identifying analytical best practices , and providing insights to the builders of scientiﬁc toolkits . However , large corpora have remained unanalyzed in depth , as descriptive labels are absent and require expert domain knowledge to gen - erate . We propose a novel weakly supervised transformer - based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments . We then evaluate the model on a new classiﬁcation task for labeling computational notebook cells as stages in the data analysis process from data import to wrangling , exploration , modeling , and evaluation . We show that our model , leveraging only easily - available weak supervision , achieves a 38 % increase in accuracy over expert - supplied heuristics and outperforms a suite of baselines . Our model enables us to examine a set of 118 , 000 Jupyter Notebooks to uncover common data analysis patterns . Focusing on notebooks with relationships to academic articles , we conduct the largest ever study of scientiﬁc code and ﬁnd that notebook composition correlates with the citation count of corresponding papers . I . I NTRODUCTION Data analysis is central to the scientiﬁc process . Increas - ingly , analytical results are derived from code , often in the form of computational notebooks , such as Jupyter note - books [ 1 ] . Analytical code is becoming more frequently pub - lished in order to improve replication and transparency [ 2 ] , [ 3 ] , [ 4 ] . However , as of yet no tools exist to study unlabeled source code both at scale and in depth . Previous in - depth analyses of scientiﬁc code heavily rely on expert annotations , limiting the scale of these studies to the order of a hundred examples [ 5 ] , [ 6 ] . Large - scale studies across thousands of examples have been limited to simple summaries such as the number or nature of imported libraries , total line counts , or the fraction of lines that are used for comments [ 6 ] , [ 7 ] , [ 8 ] . The software engineering community has emphasized the inade - quacy of these analyses , noting that “there is a strong need to programmatically analyze Jupyter notebooks” [ 9 ] , while HCI researchers have observed that studying the data science § These authors contributed equally to this work . process through notebooks may play a role in addressing the scientiﬁc reproducability crisis [ 5 ] , [ 10 ] . Automated annotation tools could enable researchers to answer important questions about the scientiﬁc process across millions of code artifacts . Do analysts share common sequen - tial patterns or processes in their code ? Do different scientiﬁc domains have different standards or best practices for data analysis ? How does the content of scientiﬁc code relate to the impact of corresponding publications ? To draw insights on the data science process , previous work has conceptualized the analysis pipeline as a sequence of discrete stages starting from importing libraries and wrangling data to evaluation [ 11 ] , [ 12 ] , [ 13 ] . Building on this conceptual model , our goal is to develop a tool that can automatically annotate code blocks with the analysis stage they support , enabling large - scale studies of scientiﬁc data analysis to answer the questions above . Analyzing scientiﬁc code is particularly difﬁcult because as a “means to an end” [ 14 ] , scientiﬁc code is often messy and poorly documented . Researchers engage in an iterative process as they transition between tasks and update their code to reﬂect new insights [ 15 ] , [ 16 ] . As such , a computational notebook may interleave snippets for importing libraries , wrangling data , exploring patterns , building statistical models , and evaluating analytical results , thereby building a complex and frequently non - linear sequence of tasks [ 5 ] , [ 11 ] . While some analysts use markdown annotations , README’s , or code comments to express the intended purpose of their code , these pieces of documentation are often sparse and rarely document the full analysis pipeline [ 6 ] . Domain - speciﬁc best practices , tech - niques , and libraries may additionally obfuscate the intent of any particular code snippet . As a result , interpreting scientiﬁc code typically requires signiﬁcant expertise and effort , making it prohibitively expensive to obtain ground truth labels on a large corpus , and therefore infeasible to build annotation tools which require anything more than minimal supervision . In this paper , we present COde RepresentAtion Learning with weakly - supervised transformers ( CORAL ) to classify scientiﬁc code into stages in the data analysis process . Im - a r X i v : 2008 . 12828v1 [ c s . L G ] 28 A ug 2020 portantly , the model requires only easily available weak su - pervision in the form of ﬁve simple heuristics , and does not rely on any manual annotations . We show that CORAL learns new relationships beyond the information provided by these heuristics , indicating that currently popular transformer archi - tectures [ 17 ] can be extended to weakly supervised tasks with the addition of a small amount of expert guidance . Our model achieves high agreement with human expert annotators and can be scaled to analyze millions of code artifacts , uniquely enabling large - scale studies of scientiﬁc data analysis . We describe a new task for classifying code snippets as stages in the data analysis process ( § III - A ) . We provide an extension to a corpus of 1 . 23M Jupyter Notebooks ( § III - B ) : a new dataset of expert annotations of stages in the data analysis process for 1 , 840 code cells in 100 notebooks , which we use exclusively for evaluation and not for training ( § III - C ) . Next we describe CORAL ( § IV ) : a novel graph neural network model for embedding data science code snippets and classifying them as stages in the data science process . To cap - ture semantic clues about the analyst’s intention , CORAL uses a novel masked attention mechanism to jointly model natural language context ( such as markdown comments ) with struc - tured source code ( § IV - B ) . We implement a weakly supervised architecture with ﬁve simple heuristics to compensate for the absence of labels , as labeling code requires domain expertise and is therefore expensive and infeasible at massive scale ( § IV - C ) . To further compensate for limited labels , CORAL combines this weak supervision with unsupervised topic mod - eling into a multi - task optimization objective ( § IV - E ) . We evaluate our model ( § V ) by comparing it to baselines including expert heuristics , weakly supervised LDA , and state - of - the - art neural representation techniques ( § V - A ) . We demon - strate that CORAL , using both code and surrounding natural language annotations , outperforms expert heuristics by 36 % and signiﬁcantly outperforms all other baselines . Through an ablation study we demonstrate that increased maximum sequence length M , weak supervision and unsupervied topic modeling all strictly improve performance , and that including markdown improves performance on cells without associated markdown by 13 % ( § V - B ) . Further , we explore the impact of maximum input size and dataset size on our model’s perfor - mance ( § V - C ) , showing that CORAL signiﬁcantly outperforms all baselines even when trained on only 1k examples . In a comprehensive error analysis , we demonstrate that previously unseen data science functions are correctly labeled with ap - propriate analysis stages ( § V - D ) . We then deploy our model to resolve previously unanswered questions about data analysis by linking academic notebooks and associated publications to conduct the largest ever study of scientiﬁc code ( § VI ) . We ﬁnd that ( 1 ) there are signiﬁcant differences between academic and non - academic papers , ( 2 ) that papers which include references to notebooks receive on average 22 times the number of citations as papers that do not , and ( 3 ) that papers linked to notebooks that more evenly capture the full data science process in expectation receive twice the number of citations for every one standard deviation increase in entropy between stages . In summary , the contributions of this paper are : • A new task and public dataset for classifying Jupyter cells as stages in the data science process ( § III ) . • A multi - task , weakly supervised transformer architecture for classifying code snippets which jointly models natural language and code ( § IV ) . • A comprehensive evaluation of code representation learn - ing methods ( § V ) . • The largest ever study of scientiﬁc code ( § VI ) . We make all code and data used in this work publicly available at http : / / bdata . cs . washington . edu / coral / . II . R ELATED W ORK A . Representation Learning for Source Code Early methods for code representational learning treated source code as sequence of tokens and built language models on top [ 18 ] , [ 19 ] , [ 20 ] , [ 21 ] , [ 22 ] . Later work incorporated additional information speciﬁc to source code , such as object - access patterns [ 23 ] , code comments [ 24 ] , parse trees [ 25 ] , serialized Abstract Syntax Trees ( ASTs ) [ 26 ] , [ 27 ] , ASTs as graph structures [ 28 ] , and associated repository metadata [ 29 ] . As documentation ( in markdown format ) is prevalent in Jupyter notebooks [ 6 ] , our model incorporates both markdown text and graph - structured ASTs , taking advantage of both semantic and structural information . Due to the scarcity of labeled examples , most previous work learned code representations without supervision [ 30 ] , [ 31 ] , [ 32 ] , [ 33 ] , [ 22 ] . The learned representations were mostly used for hole completion tasks , including the prediction of self - deﬁned function names [ 30 ] , API calls [ 32 ] , [ 33 ] , and variable names [ 31 ] , [ 22 ] . In contrast , our task – classifying code cells as analysis stages – arguably requires a higher level understanding of the intention of code . To overcome the bottleneck of manual labeling , we turn to weak supervision . Snorkel [ 34 ] combined labels from multiple weak supervision sources , denoised them , and used the resulting probabilistic labels to train discriminative models . Building on this idea , we introduce weak supervision to code representation learning by leveraging a small number of expert - supplied heuristics . B . Graph Neural Networks GNNs are powerful tools for a variety of tasks , including node classiﬁcation [ 35 ] , [ 36 ] , text classiﬁcation [ 37 ] , link prediction [ 38 ] , [ 39 ] , graph clustering [ 40 ] , [ 41 ] and graph classiﬁcation [ 41 ] , [ 42 ] , [ 43 ] , [ 44 ] . Additional work suggests that feeding underlying graphical syntax to a natural language model can improve generalization [ 45 ] . Tree structures have been show to help summarize source code [ 46 ] , and complete code snippets [ 28 ] , [ 47 ] in code representation learning . We build on prior work in attention - based graph neural net - works [ 48 ] and adopt a self - attention mechanism in our model that jointly learns from ASTs and markdown text . [ MODEL ] import pandas as pd Import libraries [ IMPORT ] df = pd . read _ csv ( “test . csv” ) . query ( “age > 60” ) Load data df [ “bmi " ] . mean ( ) What is the mean BMI ? [ WRANGLE ] reg = LinReg ( ) . fit ( df [ “age” ] , df [ “bmi” ] ) Fit a linear regression [ EXPLORE ] reg . residual _ plot ( ) Plot residuals [ EVALUATE ] [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] Example Notebook Cells Stages to Predict Fig . 1 . Examples of our proposed task of automatically labeling code snippets and accompanying natural language annotations as stages in the data science process , with code in blue and markdown in yellow . C . Studies of Data Analysis Practices There is signiﬁcant existing research on understanding data analysis practices ( e . g . , [ 11 ] , [ 13 ] , [ 10 ] , [ 15 ] , [ 5 ] ) , mostly using qualitative methods to elicit experiences from analysts . Some interviews focused speciﬁcally on Jupyter notebook users [ 10 ] , [ 6 ] . Despite synthesizing rich observations , interview studies were limited to dozens of participants . A few studies con - ducted large - scale analysis of Jupyter notebooks , but were limited to simple summary statistics [ 6 ] , a single library [ 7 ] , or code quality [ 8 ] . Our model enables the analysis of data science both at scale and in depth , which may validate and complement ﬁndings from previous qualitative studies . D . The Data Science Process A related branch of work [ 11 ] , [ 13 ] , [ 12 ] modeled the data analysis process as a sequence of iteratively visited stages . Other authors have noted that a better understanding of this process could improve scientiﬁc reproducability [ 5 ] , aid in the development of new analysis tools [ 15 ] , [ 10 ] , and identify common points of failure [ 49 ] . III . P REDICTION T ASK & D ATASETS We present a new task for labeling code snippets as stages in the data science process ( Figure 1 ) , identify a corpus of computational notebooks for large - scale training , and provide a new dataset of expert annotations that are used exclusively in the ﬁnal evaluation . A . Prediction Task In order to automatically learn useful data science con - structs from code , we propose a new task and accompanying dataset for classifying code snippets as stages in the data science process . Figure 1 shows ﬁve mock examples from this task . We task models with associating a snippet with one of ﬁve labels , which are drawn from and motivated by previous work : IMPORT , WRANGLE , EXPLORE , MODEL , and EVALUATE ( § II - D ) . IMPORT cells primarily load external libraries and set environment variables , while WRANGLE cells load data and perform simple transformations . EXPLORE cells are used to visualize data , or calculate simple statistics . MODEL cells deﬁne and ﬁt statistical models to the data , and ﬁnally EVALUATE cells measure the explanatory power and / or signiﬁcance of models . Additional details on these stages is available in Appendix Table VI . B . Jupyter Notebook Corpus for Training We curate a training set for this task by building upon the UCSD Jupyter notebook corpus , which contains all 1 . 23M publicly available Jupyter notebooks on Github [ 6 ] . Jupyter is the most popular IDE among data scientists , with more than 8M users [ 50 ] , [ 51 ] , at least in part because it enables users to combine code with informative natural language markdown documentation . As noted by the corpus’ authors , the dataset contains many examples of the myriad uses for notebooks , including completing homework assignments , demonstrating concepts , training lab members , and more [ 6 ] . For the purposes of this paper we ﬁltered the corpus to those notebooks that transform , model , or otherwise manipulate data by limiting our analysis to notebooks that import pandas , statsmodels , gensim , keras , scikit - klearn , xgboost or scipy . This leaves us with a total of 118k Jupyter notebooks , which we randomly split into training ( 90 % ) and validation sets ( 10 % ) . These notebooks are not annotated with any ground truth labels of data science stages . Thus , we propose a combination of unsupervised representation learning and weak supervision to study them at scale ( § IV ) . C . Expert Annotated Notebooks ( Only Used for Evaluation ) Annotation . We randomly sampled 100 notebooks containing 1840 individual cells from the ﬁltered dataset for hand - labeling . The ﬁrst two authors , who have signiﬁcant famil - iarity with the Python data science ecosystem , independently annotated the cells with one of the ﬁve data science stages . The annotators performed a preliminary round of coding , discussed their results , and produced a standardized rubric for qualitative coding , which is available in the Appendix D ( Table VI ) . The rubric clearly deﬁnes each data analysis stage and provides guidelines for when a label should and should not be used . Using this rubric , the annotators each made a second independent coding pass . We evaluate inter - rater reliability with Cohen’s kappa statistic , which corrects for agreement by chance , and ﬁnd the highest level of correspondence ( “substantial agreement” , κ = 0 . 803 ) [ 52 ] . Finally , the annotators resolved the remaining differences in their labels by discussing each disagreement , producing a ﬁnal dataset of 1840 cells for model evaluation ( § V ) . Our annotation rubric along with all data and code are available at http : / / bdata . cs . washington . edu / coral / . Importantly , these expert annotations are never used in training or validation including model selection , but only for the ﬁnal evaluation ( § V ) . Multi - Class v . s . Multi - Label . Both annotators paid close attention to potentially ambiguous cells while labeling , ob - serving that it was quite rare for a single cell to be used for multiple stages of the data science process ( less than 5 % of the time ) . Furthermore , the median cell in the dataset had two lines of code , making it difﬁcult for a cell to sufﬁciently express more than one stage . Low label ambiguity at the cell level and high inter - rater reliability support the formulation of this task as multi - class ( i . e . , ﬁve mutually exclusive labels ) rather than Multi - head Attention Add & Norm Feed Forward Add & Norm Dimension Reduction 𝑝 ! " # $ % Unsupervised Topic Model Sec IV . B Sec IV . C Sec IV . D regr = linear _ model . LinearRegression ( ) Regression Modeling code markdown Regression Modeling regr Assign linear _ model LinearRegression Sec IV . A [ CLS ] × 4 stacked Module [ MODEL ] [ IMPORT ] [ WRANGLE ] [ EXPLORE ] [ EVALUATE ] markdown code Weak Supervision Simple Heuristics 1 . 2 . 3 . 4 . 5 . Fig . 2 . An overview of the architecture of our CORAL model , which com - bines weak supervision and unsupervised topic modeling into a multitask objective . For visual clarity , we only show edges from the AST here . In practice , we also use connections between [ CLS ] and all the others nodes , and between each AST node and markdown node ( see Section IV - A ) . multi - label ( i . e . , a cell may have one or more labels ) , and the selection of cells as the unit of analysis . IV . T HE CORAL M ODEL COde RepresentAtion Learning with weakly - supervised transformers ( CORAL ) is a model for learning neural rep - resentations of data science code snippets and classifying them as stages in the data analysis process . CORAL leverages both source code abstract syntax trees ( ASTs ) and associated natural language annotations in markdown text ( see Fig . 2 ) . Model Contributions . CORAL contributes the following : • CORAL jointly learns from code and surrounding natu - ral language ( § IV - A ) , while preserving meaningful code structure through a graph - based masked attention mech - anism ( § IV - B ) . We show that adding natural language improves performance by 13 % on snippets that do not have associated markdown comments ( § V - B ) . • We address the lack of high - quality training data through an easily extensible weakly supervised objective based on ﬁve simple heuristics ( § IV - C ) . • CORAL combines this weak supervision with an ad - ditional unsupervised training objective ( again to avoid costly ground truth labels ) based on topic modeling , which we combine with other objectives in a multi - task learning framework ( § IV - E ) . A . Input Representations CORAL builds on graph neural networks [ 53 ] and masked - attention approaches [ 48 ] to encode the AST’s graph structure by ﬁrst serializing the tree and then using its adjacency matrix as an attention mask ( § IV - B ) . We add additional nodes to the AST to capture surrounding natural language . For each code cell , we concatenate its most recent prior markdown as a token sequence to the AST graph sequence ( yellow in Figure 2 ) , so long as the markdown is no more than three cells away . Concretely , we create a node for each markdown token and then connect each markdown node with each AST node . Finally , we add a virtual node [ CLS ] ( for classiﬁcation ) at the head of every input sequence and connect all the other nodes to it . Similar to BERT , we take this node’s embedding as the representation of the cell [ 54 ] . Notation . Formally , let V = { u , v , . . . } be the set of nodes in the input , where each node v is either an AST node or markdown token . For any input sequence that has more than M nodes , we truncate it and keep only the ﬁrst M nodes ( a modeling choice which we evaluate in § V - C ) . We use A to represent the graph adjacency matrix that encodes the relationship between nodes as described above . All input nodes are converted to embedding vectors of dimension d model . We assemble these embeddings into a matrix X . B . Encoding Code Cells with Attention We extend the popular BERT model [ 54 ] by adding masked multi - head attention to capture the graphical structure of ASTs . We evaluate the impact of this addition in § V - A . CORAL feeds the input code and natural language repre - sentations to an encoder , which is composed of a stack of N = 4 identical layers ( Fig . 2 ) . Similar to Transformers [ 17 ] , we equip each layer with a multi - head self - attention sublayer and a feed - forward sublayer . The graph structure is captured through masked attention ( Eq . 2 below ) . Masked Multi - Head Attention . We use Aggregate ik to repre - sent the self - attention function of head i in layer k . Let ( q , k , v ) be the query , key , and value decomposition of the input to Aggregate ik . Queries and keys are vectors of dimension d k , and values are vectors of dimension d v . For a given node u , let ( q u , k u , v u ) be the triple of query , key and value , and let N ( u ) be the set of all its neighbours . Formally , the parameters q u , k u , v u vary across each head i and layer k , but we drop additional notation for simplicity here . Then we compute aggregate results as : Aggregate ik ( u ) = Σ v ∈ N ( u ) Softmax ( q u · k v √ d k ) · v u ( 1 ) We adopt the scaling factor 1 √ d k from Vaswani et al . [ 17 ] to mitigate the the dot product’s growth in magnitude with d k . In practice , the queries , keys , and values are assembled into matrices Q , K , V . We compute the output in matrix form as : Aggregate ik ( Q , K , V ) = Softmax ( ˜ A (cid:12) QK T √ d k ) V ( 2 ) where ˜ A = A + I is the adjacency matrix with self - loops added to implement the masked attention approach , where each node only attends to its neighbours ( described in § IV - A ) and itself . Since we adopt multi - head attention , we concatenate h heads within the same layer : MultiHead ( Q , K , V ) = Concat ( head 1 , . . . , head h ) W O ( 3 ) head i = Aggregate ik ( XW iQ , XW iK , XW iV ) ( 4 ) where head i ∈ R d v and W iQ ∈ R d model × d k , W iK ∈ R d model × d k , W iV ∈ R d model × d v , and W O ∈ R h ∗ d v × d model are projection matrices that map the node embeddings X to queries , keys , values , and multi - head output , respectively . Feed Forward . In each layer , we additionally apply a fully connected feed - forward sublayer . This is composed of two linear transformations with ReLU activation in between : FFN ( x ) = W FF 2 · max ( 0 , W FF 1 · x + b FF 1 ) + b FF 2 ( 5 ) where W FF 1 ∈ R h ∗ d model × d model , W FF 2 ∈ R d model × h ∗ d model , b FF 1 and b FF 2 are parameters learned in model . Add & Norm . Each sublayer is followed by layer normaliza - tion [ 55 ] . The output of each sublayer is : LayerNorm ( x + Sublayer ( x ) ) ( 6 ) where Sublayer ( x ) is multi - head attention or feed forward . Output . The multi - head attention sublayer and feed - forward sublayer are stacked and make up one “layer” . After stacking this layer four times , the encoder’s output contains represen - tations of all the nodes in the input sequence . We take the embedding of the [ CLS ] node as the representation of the each notebook cell’s graph ( Section IV - A ) , denoted as z ∈ R d model . We compress this cell representation z into a lower - dimens - ional distribution over K “topics” to capture information about the data analysis stages . Concretely : p topic = Softmax ( W topic · z + b ) ( 7 ) where W topic ∈ R K × d model is the weighted matrix parameter and b is the bias vector . C . Weak Supervision It is prohibitively expensive to obtain manual annotations of data analysis stages at scale , as doing so would require thousands of person - hours of work by domain experts . There - fore , we use ﬁve simple heuristics to tailor CORAL to the prediction task described in § III - A : 1 ) We collect a set of seed functions and assign each to a corresponding stage based on its usage . Any cell that uses a seed is weakly labeled as the cor - responding stage . For example , any cell that calls ”sklearn . linear model . LinearRegression” is weakly labeled MODEL . The full set of 39 seed functions is available in Appendix A . We demonstrate CORAL’s ability to correctly classify unseen code outside these functions in § V - D . 2 ) A cell with one line of code that does not create a new variable is weakly labeled EXPLORE . This rule leverages a common pattern in Jupyter notebooks where users often use single line expressions to examine a variable , such as a DataFrame . 3 ) A cell with more than 30 % import statements is labeled IMPORT . 4 ) A cell whose corresponding markdown is less than four words and contains { logistic regression , machine learning , random forest } is weakly labeled MODEL . 5 ) A cell whose corresponding markdown is less than four words and contains cross validation is weakly labeled EVALUATE . Note that there may be conﬂicts between these rules . We observe that less than one percent of cells in our corpus comply with more than one of these heuristics , further supporting our decision to formulate labels as mutually exclusive . We resolve any such conﬂicts by assigning priority in the following order : IMPORT , MODEL , EVALUATE , EXPLORE , WRANGLE . 1 In this layer , we aim to compute p stage - a probability distribution over these six stages - from the topic distribution computed in Eq . 7 . We implement this by mapping the topic distribution p topic to a probability distribution p stage over the n stages = 6 stages . We compute the stage distribution p stage as follows , where W stage ∈ R K × n stages : p stage = softmax ( W stage · p topic + b stage ) ( 8 ) We adopt cross entropy loss to minimize classiﬁcation error on weak labels . For each p topic , loss is computed as : L weakly supervised = − Σ s y o , s log ( p s ) ( 9 ) where y o , s is a binary indicator ( 0 or 1 ) if stage label s is the correct classiﬁcation for observation o and p s is the predicted probability p stage is of stage s . The ﬁve weak supervision heuristics cover about 20 % of notebook cells in the training data . To minimize the model’s ambiguity on the remaining 80 % of unlabeled data , and encourage it to choose a stage for each topic , we add an additional loss function . Concretely , we add an entropy term to p stage to encourage uniqueness by forcing the topic distri - bution to map to as few stages as possible : L unique stage = − Σ s p s log ( p s ) ( 10 ) where p s is the predicted probability p stage [ s ] for stage s . This entropy objective is minimized when p s = 1 for some s and p s (cid:48) = 0 all other s (cid:48) . D . Unsupervised Learning Through Reconstruction As the weak supervision heuristics only cover about 20 % of the cells , we enrich the model with additional training through an unsupervised topic model . Here , the goal is to optimize the topic representation p topic such that we can reconstruct the intermediate cell representation z . We reconstruct z from a linear combination of its topic embeddings p topic : r = R · p topic ( 11 ) 1 We also include a dummy sixth stage to represent cells that are empty or not covered by one of these heuristics . To reﬂect the uncertainty of these stages they are not included in the model’s loss function . where R ∈ R d model × K is the learned cell embedding recon - struction matrix . This unsupervised topic model is trained to minimize the reconstruction error . We adopt the contrastive max - margin objective function using a Hinge loss formula - tion [ 56 ] , [ 57 ] , [ 58 ] . Thus , in the training process , for each cell , we randomly sample m = 5 cells from our dataset as negative samples : L unsupervised = Σ c ∈ D Σ m = 5 i = 1 max ( 0 , 1 − r c z c + r c n i ) ( 12 ) where D is the training data set , r c is reconstructed vector of cell c , z c is intermediate representation of cell c , and n i is the reconstructed vector of each negative sample . This objective function seeks to minimize the inner product between r c and n i , while simultaneously maximizing the inner product between r c and z c . We also employ a regularization term from He et al . [ 59 ] to promote the uniqueness of each topic embedding in T : L unique topic = (cid:13)(cid:13) R norm · R Tnorm − I (cid:13)(cid:13) ( 13 ) where I is the identity matrix and R norm is the result of L2 - row - normalization of R . This objective function reaches its minimum when the inner product of two topic embeddings is 0 . We demonstrate in § V - B that this additional unsupervised training improves overall classiﬁcation performance . E . Final Optimization Objective We combine the loss functions of Equations ( 9 ) , ( 10 ) , ( 12 ) , and ( 13 ) into the ﬁnal optimization objective : L = λ 1 L weakly supervised + λ 2 L unique stage + λ 3 L unsupervised + λ 4 L unique topic ( 14 ) where λ 1 , λ 2 , λ 3 and λ 4 are hyperparameters that control the weights of optimization objectives . We experiment with various training curricula and ﬁnd that CORAL with the hyperparameters in described in Appendix B achieves the best loss ( Eq . 14 ) on the validation set . Impor - tantly , this optimization and model training is based on solely on the labels from weak supervision heuristics . We do not use expert annotations ( § III - C ) , which we exclusively reserve for the ﬁnal evaluation . V . E VALUATION CORAL achieves accuracy of over 72 % on the stage classiﬁcation task using an unseen test set ( Section III - C ) , outperforming a range of baseline models and demonstrat - ing that weak supervision , unsupervised topic modeling , and adding markdown information all strictly improve overall classiﬁcation performance . A . Baseline Comparison In Figure 3 ( a ) we compare CORAL’s performance to eight baselines , which we describe below . Importantly , the lack of ground truth labels in our training set makes it impossible to evaluate a model that does not use some amount of weak supervision , as without these heuristics we cannot map between learned topics and data science stages . CORAL BERT ( No AST , No Finetuning ) BERT ( AST , No Finetuning ) BERT ( AST , Finetuning ) BERT ( No AST , Finetuning ) LDA + Weak Supervision Word2Vec ( With AST ) Expert Heuristics Word2Vec ( No AST ) 72 . 2 % 67 . 9 % 67 . 9 % 67 . 4 % 64 . 9 % 42 . 8 % 42 . 6 % 34 . 1 % 29 . 7 % ( a ) Baselines 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % Accuracy CORAL CORAL ( No Masked Attention ) CORAL ( No Unsupervised Topic Model ) CORAL ( No Markdown ) 72 . 2 % 70 . 1 % 62 . 4 % 60 . 1 % ( b ) Ablation Studies Fig . 3 . Accuracy on expert - annotated test set for all baselines ( a ) and ablation studies ( b ) . Performance improves with neural topic models and weak supervision . CORAL signiﬁcantly outperforms all baselines ( Wilcoxon signed rank , p < 0 . 001 ) and ablation studies ( p < 0 . 05 ) Expert Heuristics ( Weak Supervision Only ) . How well does a simple baseline perform that considers only library infor - mation ? For example , pandas is commonly used to wrangle data , and scikit - learn is common in modeling . We compare against an improved version of this baseline , where we include all expert heuristics described in § IV - C . This set of heuristics consider function - level and markdown information in addition to library information . This is a natural comparison since this is the exact weak supervision used in CORAL . These heuristics cover only 20 . 38 % of the test examples , so we choose one stage uniformly at random otherwise . LDA Representation + Weak Supervision . How important is it to use a deep neural encoder for our task ? To address this question , we replace CORAL’s encoder with a Latent Dirichlet Allocation ( LDA ) [ 60 ] topic model , but use the same input data ( § IV - A ) , and the same weak supervision ( Section IV - C ) . Speciﬁcally , we optimize this model with L weak supervision ( Eq . ( 9 ) ) and L unique stage ( Eq . ( 10 ) ) on top of the unsuper - vised LDA representation . We ﬁrst used the same number of LDA topics ( 50 ) as we use in CORAL ( i . e . the size of the cell representation p topic ) . However , this baseline only performed at the level of the Expert Heuristics Only baseline . In order to make this baseline stronger we doubled the number of LDA topics to 100 , which did improve performance . Neural Baselines . How well does a noncontextual neural model perform on our task ? What are the beneﬁts of using the graphical structure of ASTs instead of treating code cells as sequences of tokens in deep neural networks ? How important is the multitask objective that combines weak supervision heuristics and an unsupervised topic model ? To address these questions , we compare CORAL against the noncontextual Word2Vec [ 61 ] model and the state - of - the - art language model , BERT [ 54 ] , which have both been previously applied to source code representation learning [ 62 ] , [ 63 ] . We trained all neural baselines with both markdown and code using the same pre - training corpus as CORAL . To explore the sensitivity of these models to their input representations , we tried both treating the code as sequences of tokens and as serialized ASTs . For TABLE I I MPACT OF M AX S EQUENCE L ENGTH ON CORAL Model Max Sequence Length 80 120 160 CORAL 59 . 9 64 . 2 72 . 2 CORAL ( No Markdown ) 54 . 4 57 . 0 60 . 2 Training on markdown data in addition to code signiﬁcantly increases performance independent of maximum sequence length . TABLE II CORAL ACCURACY ACROSS WEAK SUPERVISION COVERAGE Model Weak Supervision Coverage 25 % 50 % 100 % CORAL 41 . 6 46 . 4 72 . 2 CORAL ( No Masked Attention ) 31 . 7 46 . 1 70 . 4 BERT ( AST , No Finetuning ) 26 . 1 47 . 2 67 . 9 Training with more weak supervision signiﬁcantly improves performances . the BERT baselines we used the standard architecture with the same embedding size as CORAL and masked language model pretraining . Predictions are made with a single layer using the same weak supervision heuristics as CORAL . We evaluated BERT baselines both with and without ASTs and ﬁnetuning . When ﬁnetuning , we backpropogated the single layer’s loss through the encoder . After pre - training , we optimized the model with L weak supervision ( Eq . ( 9 ) ) and L unique stage ( Eq . ( 10 ) ) on top of the learned representations of code cells . Results . Results from these experiments are available in Figure 3 ( a ) . The Expert Heuristics baseline achieves 34 . 1 % accuracy on the unseen expert annotations described in § III - C . Even though it uses the same amount of supervision , CORAL is 38 % more accurate than this baseline , demonstrating that CORAL learns signiﬁcantly more than simply memorizing the heuristic rules . CORAL also favorably compares to state - of - the - art neural language models , beating the highest performing BERT baseline by 4 . 3 % . We observe that while popular deep learning techniques like ﬁnetuning produce only a marginal difference in model performance , CORAL signiﬁcantly outper - forms all other baselines ( Wilcoxon signed rank , p < 0 . 001 ) . B . Ablation Study We just demonstrated in § V - A that CORAL improves signiﬁcantly over expert heuristics , representations that do not leverage graphical structure , and state - of - the - art neural models . Here we show that ( 1 ) adding markdown information , ( 2 ) weak supervision , and ( 3 ) additional unsupervised training all independently improve the performance of CORAL , as shown in Figure 3 ( b ) . Across all experiments we use maximum sequence length of M = 160 and train on the maximum 1M code cells , based on the best performing model overall . TABLE III CORAL ACCURACY ACROSS VARIOUS TRAINING DATASET SIZES Model Number of Cells 1k 10k 100k 1M CORAL 61 . 9 62 . 7 63 . 6 72 . 2 CORAL ( No Masked Attention ) 53 . 6 57 . 0 59 . 7 70 . 4 BERT ( AST , No Finetuning ) 41 . 0 52 . 4 63 . 2 67 . 9 Performance consistently increases with more training data but remains promising even with three orders of magnitude less training data . def Compute _ TPFN ( y _ pred , y _ true ) : ' Custom method to compute the confusion matrix ' df = pd . DataFrame ( columns = ( ' y _ pred ' , ' y _ true ' , ' count’ ) ) df [ ' y _ pred ' ] = pd . Series ( y _ pred ) df [ ' y _ true ' ] = pd . Series ( np . array ( y _ true ) ) df [ ' count ' ] = 1 return df . groupby ( [ ' y _ pred ' , ' y _ true ' ] ) . count ( ) train = pd . read _ csv ( ' D : / pyplace / hisRawData . csv ' ) import seaborn as sns train = train [ ( train . status = = 11 ) ] train = train [ ( train . power > 0 ) ] train = train [ ( train . speed > 0 ) ] sns . regplot ( x = ' speed ' , y = ' power ' , data = train ) newdf = plotTimeComp ( YOURNUMBERDATA [ 2 ] , TOTALNUMBERDATA [ 2 ] , ' Text Message Breakdown : Turing vs . Lovelace ' , ‘Lovelace ' ) Import 0 Wrangle 0 Explore 0 . 98 Model 0 . 02 Evaluate 0 Import 0 Wrangle 0 . 29 Explore 0 . 70 Model 0 . 01 Evaluate 0 Import 0 Wrangle 0 Explore 0 . 03 Model 0 . 21 Evaluate 0 . 78 ( a ) ( b ) ( c ) Fig . 4 . Example predictions . Probability distributions over stages from CORAL’s SoftMax output ( Eq . ( 7 ) ) are listed on the right side . In ( a ) , CORAL correctly identiﬁes the cell as EVALUATE rather than WRANGLE , likely by interpreting ”confusion matrix” , perhaps based on previously seen markdown . In ( b ) , the model identiﬁes the use of sns . regplot , an unseen statistical visualization function , as an example of EXPLORE . In ( c ) , CORAL correctly interprets a user - deﬁned function . CORAL without Markdown . For this ablation , we remove any markdown information from the input sequence , while keeping all other aspects of CORAL the same . We compare maximum sequence length of 80 , 120 and 160 since the maximum sequence length M may interact with markdown information due to truncation ( § IV - A ) . We ﬁnd that including markdown information consistently and signiﬁcantly improves performance 12 % at M = 160 , even though less that 9 % of cells are directly preceded by markdown ( Table I ) . Fur - thermore , these comparatively rare comments signiﬁcantly improve performance even on cells that do not have cor - responding markdown information from 59 . 6 % to 72 . 6 % , suggesting that markdown cells help CORAL better represent source code independent of these comments . CORAL with Less Weak Supervision . The weak supervi - sion heuristics described in § IV - C cover about 20 % of the training examples . We simulate lower coverage by randomly subsampling 50 % and 25 % of these weakly labeled examples ( i . e . , 10 % and 5 % of all examples ) . Higher weak supervision coverage dramatically increases performance , but even at 25 % of examples CORAL still outperforms CORAL ( No Masked Attention ) by 10 % and BERT by 15 % ( Table II ) . CORAL without Unsupervised Topic Model . This baseline evaluates the marginal beneﬁt of CORAL’s unsupervised topic model . Speciﬁcally , we remove L unsupervised ( Eq . 12 ) , and L unique topic ( Eq . 13 ) from CORAL but keep everything else the same . We show that the unsupervised training objective improves overall accuracy by 10 % ( Figure 3 ( b ) ) . This demon - strates the signiﬁcant potential of combining limited weak supervision with additional unsupervised training in a multi - task framework . C . Impact of Input Length & Training Set Size Maximum Sequence Length . We investigate how model performance changes with the maximum input sequence length M ( see Table I ) . For CORAL models with and without markdown , a larger maximum sequence length consistently TABLE IV F RACTION OF PREDICTED STAGES FOR CELLS THAT CONTAIN PREVIOUSLY UNSEEN FUNCTIONS Function Expectation IMPORT WRANGLE EXPLORE MODEL EVALUATE pandas . DataFrame . dropna Wrangle 0 0 . 93 0 . 07 0 0 pandas . DataFrame . groupby Wrangle 0 0 . 52 0 . 12 0 . 02 0 . 34 seaborn . jointplot Explore 0 0 . 00 0 . 98 0 . 00 0 . 02 seaborn . countplot Explore 0 0 . 01 0 . 98 0 . 00 0 . 01 sklearn . linear model . SGDClassiﬁer Model 0 0 0 0 . 67 0 . 32 sklearn . linear model . PassiveAggressiveClassiﬁer Model 0 0 . 06 0 0 . 61 0 . 39 sklearn . metrics . f1 score Evaluate 0 0 0 . 01 0 . 05 0 . 94 sklearn . metrics . log loss Evaluate 0 . 02 0 . 01 0 . 02 0 . 26 0 . 70 CORAL accurately categorizes common data analysis functions as frequently belonging to their expected stage . improves accuracy . Longer sequence lengths may include more markdown information and limit truncation of larger cells . Only 6 % of the training examples have more than 160 nodes , and increases in M also increase training time and memory requirements . Therefore , we did not consider models beyond M = 160 and use this setting for all other experiments . Training Dataset Size . We evaluate the accuracy of CORAL and two other high - performing models with different training dataset sizes to gauge how sensitive our model is to training data size . We ﬁx M to 160 and train with a maximum of 1M notebook cells . In all other experiments , we use the maximum 1M notebook cells for training . While performance consis - tently decreases with smaller training data ( Table III ) , CORAL achieves an accuracy of 61 . 85 % with only 1k examples and outperforms baselines by a large margin . This demonstrates that the CORAL architecture is effective at learning useful code representations even in smaller - data scenarios , such as on the order of magnitude of a typical GitHub repository . D . Error Analysis Confusion Matrix . We include a confusion matrix of CORAL’s predictions from the best performing model ( M = 160 trained on 1M examples ) in Appendix E ( Figure 8 ) . The most frequent confusion is misclassifying EXPLORE as WRANGLE . This is in part because WRANGLE and EXPLORE are the two most common stages in the hand labeled corpus , but also possibly because analysts may apply simple transfor - mations while primarily using a cell to visualize or otherwise explore data . Unseen Functions . To evaluate how well CORAL can learn beyond memorizing examples from weak supervision , we select eight common data analysis function and compare the labels of cells that contain them ( Table V - B ) . Importantly , these functions were not used in weak supervision and thus were never directly associated with any label in the model . Many functions demonstrate clear stage membership in line with our expectations ( e . g . , pandas . DataFrame . groupby , seaborn . countplot ) , demonstrating that CORAL can assign cells including these functions to likely correct stages . Other functions exhibit a more even distribution across stages . For example , sklearn . linear model . PassiveAggressiveClassifie r , a simple linear classiﬁer , appears in both MODEL and EVALUATE cells . While ambiguity between stages is rare overall ( § III - C ) we hypothesize that this confusion may be the result of the scikit - learn use pattern where users specify and evaluate their models in the same cell . Example Predictions . We highlight three predictions in Figure 4 to demonstrate CORAL’s ability to capture data analysis semantics and inherent ambiguity . In Figure 4 ( a ) , the user transforms a pandas DataFrame and calls pan - das . DataFrame . groupby , a function typically used to aggregate data . While a naive method ( e . g . , the expert heuristic baseline in § V - A ) might label the cell as WRANGLE , CORAL infers that the analyst’s intention is to use this user - deﬁned function to evaluate a classiﬁer with a confusion matrix , likely making use of the information in the comment and function parameters , and appropriately labels the cell as EVALUATE . In Figure 4 ( b ) , the analyst loads data , selects a subset , creates a plot , and ﬁts a linear regression . CORAL cor - rectly identiﬁes this example as serving to both modify data and look for patterns , but assigns a higher probability to EXPLORE , demonstrating its ability to capture the signiﬁcance of previously unseen statistical visualization methods like seaborn . regplot . In Figure 4 ( c ) , the analyst calls a user - deﬁned function . While CORAL has never seen this function or notebook , it still correctly identiﬁes the intent of the cell as EXPLORE likely by attending to tokens like “plot” and “breakdown” . VI . L ARGE S CALE S TUDIES OF S CIENTIFIC D ATA A NALYSIS Our model and datasets provide an opportunity to pose and answer previously unaddressable questions about the data analysis process , the role of scientiﬁc analysis in academic publishing , and differences between scientiﬁc domains . We note that our corpus ( § III - B ) is limited to the most recent ( potentially partial ) snapshot of the user’s analysis and that the observational nature of this data prohibits any causal claims . A . Are There Differences Between Academic Notebooks and Non - Academic Notebooks ? Differences between academic and non - academic notebooks could identify how practices vary across these communities . Method . The Semantic Scholar Open Research Corpus ( S2ORC ) is a publicly available dataset containing 8 . 1M full - text academic articles [ 64 ] . In order to relate these papers to 0 % 20 % 40 % ( a ) Fraction of Lines of Code IMPORT WRANGLE EXPLORE MODEL EVALUATE S t a g e 0 % 25 % 50 % 75 % ( b ) Fraction of Cells That Transition to Different Stage Non - Academic Academic Fig . 5 . Differences between academic and non - academic notebooks . relevant source code , we performed a regular expression search across the corpus for any reference to a GitHub repository , returning associations between 2 . 0k papers and 7 . 1k notebooks from the UCSD corpus . We use this dataset to resolve previ - ously unanswerable questions about the role of analysis code in the scientiﬁc process . Although there is no strict guarantee that a linked notebook contains the data analysis that was used to create the paper , the median notebook is linked to exactly one paper , indicating some degree of injectivity from notebooks to papers . Furthermore , manual inspection of our dataset and prior work indicate that researchers often break their analysis up across many notebooks , which may explain why papers link to multiple notebooks . So as not to bias our analysis against how a scientist decides to structure their code , we compute statistics for each paper by concatenating all associated notebooks . We compute the fraction of code devoted to each data analysis stage and the fraction of cells that are followed by a cell of a different stage and examine differences between academic and non - academic notebooks . Results . Academic notebooks devote 56 % more code to ex - ploring data and 26 % less code to developing models than non - academic notebooks ( Figure 5 ( a ) ) . Furthermore , we note that analysts on average use only 23 % of their code for the traditionally boring and laborious process of wrangling data . While the relative size of the stage likely does not accurately reﬂect the relative effort of data wrangling , it is perhaps surprising that such a maligned stage of the process [ 11 ] is represented by a comparatively low fraction of all code . We also ﬁnd signiﬁcant differences in the fraction of cells that are followed by a cell of a different stage ( Figure 5 ( b ) ) . Most interestingly , cells are in general more likely than not to transition to a different stage . This result supports the hypotheses that notebooks follow a transitory process through the data science process to complete an analysis rather than dwelling on any particular stage . B . Is the Content of Notebooks Related to the Impact of Associated Publications ? Evidence of a relationship between scientiﬁc notebooks and publication impact may encourage researchers to publish their code , and could reveal differences between the priorities placed on scientiﬁc data analysis by different domains . Method . We employ a negative binomial regression to esti - mate the impact of notebook stage distribution on the number I M P O R T W R A N G L E E X P L O R E M O D E L E V A L U A T E 5 0 5 i Domain Biology Computer Science Mathematics Fig . 6 . Results from ( R2 ) , indicating differences in how paper impact in different domains is related to the content of associated notebooks . of citations their associated papers receive . We hypothesize that notebooks which evenly and comprehensively document their analysis ( rather than focusing on just one part ) may receive more citations . In our ﬁrst regression R1 , we therefore regress citation count on the Stage Entropy = − (cid:80) k p k log p k , where p k is the fraction of the notebook that is devoted to stage k . This captures the uniformity of the distribution of stages across a paper’s associated notebooks . Here , we normalized this quantity across all publications by taking the Z - score . We controlled for a paper’s year of publication and domain . To reveal differences between disciplines , we build upon this experiment with a second regression R2 , which includes all terms from R1 except for the entropy term , but adds interaction variables between the Z - scores of the fraction of each paper’s notebook devoted to each data analysis stage and paper domains to capture differences between disciplines . additional details for these regression models are available in Appendix F . Results . We ﬁnd that papers that link to notebooks have 10 β hasNotebook = 10 1 . 34 ≈ 21 . 88 times more citations than papers that do not reference a notebook ( 95 % CI : [ 1 . 29 , 1 . 41 ] , p < 0 . 001 ) . From R1 we note that Stage Entropy is strongly related to the number of citations a publication receives , as those publications can expect a 10 β stageEntropyZ = 10 0 . 33 ≈ 2 . 11 times increase in citations with an entropy level for each standard deviation above the mean ( 95 % CI : [ 0 . 26 , 0 . 39 ] , p < 0 . 001 ) This result suggests that researchers may value notebooks which evenly document the whole data science process , rather than highlighting just one part of analysis . These results also indicate that a notebook with one standard deviation more than the average EXPLORE code would expect 10 β EXPLORE = 10 − 0 . 4325 ≈ 0 . 35 times the citations in its associated paper than a notebook with an average quantity of all stages ( 95 % CI : [ - 0 . 64 , - 0 . 22 ] , p < 0 . 001 ) . One possible explanation for this effect is that notebooks which feature a high volume of code for exploring data are associated with generating hypotheses , and may therefore be associated with incomplete or exploratory publications that are less likely to attract references . The results from R2 ( Figure 6 ) indicate signiﬁcant dif - ferences between domains . Most notably , we ﬁnd that in computer science and mathematics an increase in the portion of code devoted to wrangling data decreases the citation count in expectation , while no such interaction is present for papers from biological sciences . We hypothesize that the most popular cited notebooks in computer science and mathematics may cleanly demonstrate new techniques and models , rather than documenting an extensive data wrangling pipeline . We note that although these effect sizes may seem large , we need to consider that the median citation count for papers is only two . This implies that even with a high citation multiplier , papers with just a few citations would expect a rather moderate increase in citations . VII . C ONCLUSION We presented CORAL , a novel weakly supervised neural architecture for generating representations of code snippets and classifying them as stages in the analysis pipeline . We showed that this model outperforms a suite of baselines on this new classiﬁcation task . Further , we introduced and made public the largest dataset of code with associated publications for scientiﬁc data analysis , and employed CORAL to answer open questions about the data analysis process . R EFERENCES [ 1 ] T . Kluyver , B . Ragan - Kelley , F . P´erez , B . Granger , M . Bussonnier , J . Frederic , K . Kelley , J . Hamrick , J . Grout , S . Corlay et al . , “Jupyter notebooks - a publishing format for reproducible computational work - ﬂows . ” in Positioning and Power in Academic Publishing : Players , Agents and Agendas , 2016 . [ 2 ] E . Foster and A . Deardorff , “Open Science Framework ( OSF ) , ” Journal of the Medical Library Association : JMLA , 2017 . [ 3 ] P . Ayers , “LibGuides : Citing & publishing software : Publishing research software . ” [ 4 ] C . Pradal , G . Varoquaux , and H . Langtangen , “Publishing scientiﬁc software matters , ” Journal of Computational Science , pp . 311 – 312 , 2013 . [ 5 ] Y . Liu , T . Althoff , and J . Heer , “Paths explored , paths omitted , paths obscured : Decision points & selective reporting in end - to - end data analysis , ” in CHI , 2020 . [ 6 ] A . Rule , A . Tabard , and J . Hollan , “Exploration and explanation in computational notebooks , ” in CHI , 2018 . [ 7 ] M . Rehman , “Towards understanding data analysis workﬂows using a large notebook corpus , ” in SIGMOD , 2019 . [ 8 ] J . Wang , L . Li , and A . Zeller , “Better code , better sharing : On the need of analyzing Jupyter notebooks , ” ICSE , 2020 . [ 9 ] J . Wang , L . Li , and A . Zeller , “Better code , better sharing : on the need of analyzing jupyter notebooks , ” in ICSE , 2020 . [ 10 ] M . Kery , M . Radensky , M . Arya , B . John , and B . Myers , “The story in the notebook : Exploratory data science using a literate programming tool , ” in CHI , 2018 . [ 11 ] S . Kandel , A . Paepcke , J . Hellerstein , and J . Heer , “Enterprise data analysis and visualization : An interview study , ” TVCG , 2012 . [ 12 ] K . Wongsuphasawat , Y . Liu , and J . Heer , “Goals , process , and challenges of exploratory data analysis : An interview study , ” arXiv : 1911 . 00568 , 2019 . [ 13 ] S . Alspaugh , N . Zokaei , A . Liu , C . Jin , and M . Hearst , “Futzing and moseying : Interviews with professional data analysts on exploration practices , ” TVCG , 2018 . [ 14 ] A . Johanson and W . Hasselbring , “Software engineering for computa - tional science : Past , present , future , ” Computing in Science Engineering , 2018 . [ 15 ] M . Kery , A . Horvath , and B . Myers , “Variolite : Supporting exploratory programming by data scientists , ” in CHI , 2017 . [ 16 ] C . Hill , R . Bellamy , T . Erickson , and M . Burnett , “Trials and tribulations of developers of intelligent systems : A ﬁeld study , ” in ( VL / HCC ) , 2016 . [ 17 ] A . Vaswani , N . Shazeer , N . Parmar , J . Uszkoreit , L . Jones , A . Gomez , Ł . Kaiser , and I . Polosukhin , “Attention is all you need , ” in NeurIPS , 2017 . [ 18 ] A . Hindle , E . T . B . , Z . Su , M . Gabel , and P . Devanbu , “On the naturalness of software , ” in ICSE , 2012 . [ 19 ] Z . Tu , Z . Su , and P . Devanbu , “On the localness of software , ” in FSE , 2014 . [ 20 ] T . Nguyen , A . Nguyen , H . Nguyen , and T . Nguyen , “A statistical semantic language model for source code , ” in ESEC / FSE , 2013 . [ 21 ] M . Allamanis and C . Sutton , “Mining source code repositories at mas - sive scale using language modeling , ” in 2013 10th Working Conference on Mining Software Repositories ( MSR ) , 2013 . [ 22 ] V . Raychev , M . Vechev , and E . Yahav , “Code completion with statistical language models , ” in SIGPLAN , 2014 . [ 23 ] T . Kwon and Z . Su , “Modeling high - level behavior patterns for precise similarity analysis of software , ” in ICDM , 2011 . [ 24 ] D . Movshovitz - Attias and W . Cohen , “Natural language models for predicting programming comments , ” in ACL , 2013 . [ 25 ] P . Bielik , V . Raychev , and M . Vechev , “Phog : probabilistic model for code , ” in ICML , 2016 . [ 26 ] U . Alon , M . Zilberstein , O . Levy , and E . Yahav , “A general path - based representation for predicting program properties , ” SIGPLAN , 2018 . [ 27 ] J . Li , Y . Wang , M . Lyu , and I . King , “Code completion with neural attention and pointer networks , ” arXiv : 1711 . 09573 , 2017 . [ 28 ] M . Allamanis , M . Brockschmidt , and M . Khademi , “Learning to repre - sent programs with graphs , ” arXiv : 1711 . 00740 , 2017 . [ 29 ] Y . Zhang , F . F . Xu , S . Li , Y . Meng , X . Wang , Q . Li , and J . Han , “Higit - class : Keyword - driven hierarchical classiﬁcation of github repositories , ” in ICDM , 2019 , pp . 876 – 885 . [ 30 ] U . Alon , M . Zilberstein , O . Levy , and E . Yahav , “code2vec : Learning distributed representations of code , ” POPL , 2019 . [ 31 ] M . Allamanis , E . Barr , C . Bird , and C . Sutton , “Learning natural coding conventions , ” in FSE , 2014 . [ 32 ] M . Acharya , T . Xie , J . Pei , and J . Xu , “Mining api patterns as partial orders from source code : from usage scenarios to speciﬁcations , ” in ESEC / FSE , 2007 . [ 33 ] T . Nguyen , A . Nguyen , H . Phan , and T . Nguyen , “Exploring api embedding for api usages and applications , ” in ICSE , 2017 . [ 34 ] A . Ratner , S . Bach , H . Ehrenberg , J . Fries , S . Wu , and C . R´e , “Snorkel : Rapid training data creation with weak supervision , ” VLDB , 2019 . [ 35 ] W . Hamilton , Z . Ying , and J . Leskovec , “Inductive representation learning on large graphs , ” in NeurIPS , 2017 . [ 36 ] T . Kipf and M . Welling , “Semi - supervised classiﬁcation with graph convolutional networks , ” arXiv : 1609 . 02907 , 2016 . [ 37 ] M . Wu , S . Pan , X . Zhu , C . Zhou , and L . Pan , “Domain - adversarial graph neural networks for text classiﬁcation , ” in ICDM , 2019 . [ 38 ] M . Schlichtkrull , T . Kipf , P . Bloem , R . Van Den Berg , I . Titov , and M . Welling , “Modeling relational data with graph convolutional networks , ” in European Semantic Web Conference , 2018 . [ 39 ] M . Zhang and Y . Chen , “Link prediction based on graph neural net - works , ” in NeurIPS , 2018 . [ 40 ] M . Defferrard , X . Bresson , and P . Vandergheynst , “Convolutional neural networks on graphs with fast localized spectral ﬁltering , ” in NeurIPS , 2016 . [ 41 ] Z . Ying , J . You , C . Morris , X . Ren , W . Hamilton , and J . Leskovec , “Hierarchical graph representation learning with differentiable pooling , ” in NeurIPS , 2018 . [ 42 ] H . Dai , B . Dai , and L . Song , “Discriminative embeddings of latent variable models for structured data , ” in ICML , 2016 . [ 43 ] D . Duvenaud , D . Maclaurin , J . Iparraguirre , R . Bombarell , T . Hirzel , A . Aspuru - Guzik , and R . Adams , “Convolutional networks on graphs for learning molecular ﬁngerprints , ” in NeurIPS , 2015 . [ 44 ] F . Scarselli , M . Gori , A . Tsoi , M . Hagenbuchner , and G . Monfardini , “The graph neural network model , ” IEEE Trans . Neural Netw . , 2008 . [ 45 ] P . Battaglia , J . Hamrick , V . Bapst , A . Sanchez - Gonzalez , V . Zambaldi , M . Malinowski , A . Tacchetti , D . Raposo , A . Santoro , R . Faulkner et al . , “Relational inductive biases , deep learning , and graph networks , ” arXiv : 1806 . 01261 , 2018 . [ 46 ] P . Fernandes , M . Allamanis , and M . Brockschmidt , “Structured neural summarization , ” arXiv : 1811 . 01824 , 2018 . [ 47 ] M . Brockschmidt , M . Allamanis , A . L . Gaunt , and O . Polozov , “Gen - erative code modeling with graphs , ” arXiv : 1805 . 08490 , 2018 . [ 48 ] P . Veli ˇ ckovi ´ c , G . Cucurull , A . Casanova , A . Romero , P . Lio , and Y . Bengio , “Graph attention networks , ” arXiv : 1710 . 10903 , 2017 . [ 49 ] L . Chen , M . Ali Babar , and B . Nuseibeh , “Characterizing architecturally signiﬁcant requirements , ” IEEE Software , 2013 . [ 50 ] “Jetbrains data science in 2018 . ” [ Online ] . Available : https : / / www . jetbrains . com / research / data - science - 2018 / [ 51 ] K . Kelley and B . Granger , “Jupyter frontends : From the classic jupyter notebook to jupyterlab , nteract , and beyond . ” JupyterCon , 2017 . [ 52 ] J . Landis and G . Koch , “The measurement of observer agreement for categorical data , ” Biometrics , 1977 . [ 53 ] J . Gilmer , S . Schoenholz , P . Riley , O . Vinyals , and G . Dahl , “Neural message passing for quantum chemistry , ” in ICML , 2017 . [ 54 ] J . Devlin , M . Chang , K . Lee , and K . Toutanova , “BERT : Pre - training of deep bidirectional transformers for language understanding , ” arXiv : 1810 . 04805 , 2018 . [ 55 ] J . Ba , J . Kiros , and G . Hinton , “Layer normalization , ” arXiv : 1607 . 06450 , 2016 . [ 56 ] J . Weston , S . Bengio , and N . Usunier , “Wsabie : Scaling up to large vocabulary image annotation , ” in IJCAI , 2011 . [ 57 ] R . Socher , A . Karpathy , Q . Le , C . Manning , and A . Ng , “Grounded com - positional semantics for ﬁnding and describing images with sentences , ” TACL , 2014 . [ 58 ] M . Iyyer , A . Guha , S . Chaturvedi , J . Boyd - Graber , and H . Daum´e III , “Feuding families and former friends : Unsupervised learning for dy - namic ﬁctional relationships , ” in NAACL - HLT , 2016 . [ 59 ] R . He , W . S . Lee , H . T . Ng , and D . Dahlmeier , “An unsupervised neural attention model for aspect extraction , ” in ACL , 2017 . [ 60 ] D . Blei , A . Ng , and M . Jordan , “Latent dirichlet allocation , ” JMLR , 2003 . [ 61 ] T . Mikolov , I . Sutskever , K . Chen , G . S . Corrado , and J . Dean , “Distributed representations of words and phrases and their composi - tionality , ” in NeurIPS , 2013 . [ 62 ] Z . Feng , D . Guo , D . Tang , N . Duan , X . Feng , M . Gong , L . Shou , B . Qin , T . Liu , D . Jiang et al . , “Codebert : A pre - trained model for programming and natural languages , ” arXiv preprint arXiv : 2002 . 08155 , 2020 . [ 63 ] A . Kanade , P . Maniatis , G . Balakrishnan , and K . Shi , “Pre - trained contextual embedding of source code , ” 2019 . [ 64 ] K . Lo , L . L . Wang , M . Neumann , R . Kinney , and D . S . Weld , “S2ORC : The Semantic Scholar Open Research Corpus , ” in ACL , 2020 . A PPENDIX R EPRODUCABILITY A . Weak Supervision Seed Functions The seed functions with associated data analysis stages used in weak supervision heuristics are listed in Table V . TABLE V S EED FUNCTIONS WITH ASSOCIATED DATA ANALYSIS STAGES USED IN WEAK SUPERVISION HEURISTICS ( S ECTION IV - C ) . Stage Seed Functions Wrangle pandas . read csv pandas . read csv . dropna pandas . read csv . ﬁllna pandas . DataFrame . ﬁllna sklearn . datasets . load iris scipy . misc . imread scipy . io . loadmat sklearn . preprocessing . LabelEncoder scipy . interpolate . interp1d Explore matplotlib . pyplot . show matplotlib . pyplot . plot matplotlib . pyplot . ﬁgure seaborn . pairplot seaborn . heatmap seaborn . lmplot pandas . read csv . describe pandas . DataFrame . describe Model sklearn . decomposition . PCA sklearn . naive bayes . GaussianNB sklearn . ensemble . RandomForestClassiﬁer sklearn . linear model . LinearRegression sklearn . linear model . LogisticRegression sklearn . tree . DecisionTreeRegressor sklearn . ensemble . BaggingRegressor sklearn . neighbors . KNeighborsClassiﬁer sklearn . naive bayes . MultinomialNB sklearn . svm . SVC sklearn . tree . DecisionTreeClassiﬁer tensorﬂow . Session sklearn . linear model . Ridge sklearn . linear model . Lasso Evaluate sklearn . cross validation . cross val score sklearn . metrics . mean squared error sklearn . model selection . cross val score scipy . stats . ttest ind sklearn . metrics . accuracy score B . Experiment Setting We train CORAL with 1M cells on a single GeForce RTX 2080 Ti GPU . The model has four attention heads and four layers of dimension d model = 256 . We set the number of topics ( § IV - B ) to 50 and maximum sequence length ( M ) to 160 . We set λ 1 = 0 . 1 , λ 2 = 0 . 3 , λ 3 = 1 and λ 4 = 1 . We train the model by minimizing L in Equation ( 14 ) , using the SGD optimizer with a learning rate α = 1 × 10 − 5 , β = 0 . 9 . Training is done on mini - batches of size 16 , for up to 8 epochs with an early stopping criteria if validation error had not improved for 3 epochs . Each epoch takes about 2 . 5 hours to train . C . Algorithm The CORAL Algorithm is shown in Figure 7 . Algorithm 1 CORAL Input : Set of nodes V ; adjacency matrix A Output : Cell embedding z ; reconstruted embedding r ; probability distribution over stages p stage 1 : X = Embedding ( V ) 2 : for i = 1 to 4 do 3 : M = MultiHeadAttention ( X , A ) 4 : X = LayerNorm ( X + M ) 5 : F = FeedForward ( X ) 6 : X = LayerNorm 0 ( X + F ) 7 : z = X [ 0 [ CLS ] 0 ] 8 : p topic = Softmax ( W topic · z + b ) 9 : r = R · p topic 10 : p stage = Softmax ( W stage · p topic + b stage ) 1 Fig . 7 . CORAL Algorithm . D . Qualitative Rubric The qualitative rubric used for labeling the Expert Annotated Dataset ( Section III - C ) used for ﬁnal model evaluation is listed in Table VI . TABLE VI Q UALITATIVE RUBRIC USED FOR LABELING THE E XPERT A NNOTATED D ATASET ( S ECTION III - C ) USED FOR FINAL MODEL EVALUATION . Stage Deﬁnition When to Use When Not to Use Example Import These cells are used primar - ily to import libraries into the Python environment . Although they may serve other functions , like deﬁning constants or ini - tializing helper objects , the ma - jority of the code in these cells sets up analytical tools for use later in the notebook . Loading libraries , deﬁning constants , initializing en - vironments , connecting to databases A cell has one or more im - port statements , but most of the cell serves another pur - pose % load _ ext autoreload % autoreload 2 import pandas as pd import numpy as np from matplotlib import rcParams rcParams [ ' figure . figsize ' ] = 20 , 10 Wrangle Wrangle cells clean , ﬁlter , sum - marize , and / or integrate data . These cells often permute data for use in later cells . Cleaning data , feature pro - cessing , data transforma - tions , augmenting an exist - ing dataset , loading and / or saving data , splitting data into train and test sets Transformations are applied , but the result is simply examined ( See : Explore ) from sklearn . datasets import load _ iris data = load _ iris ( ) df = pd . DataFrame ( data . data ) IN _ PER _ CM = 0 . 393701 df = df / IN _ PER _ CM Explore Interactive explorations of data . These cells tend to yield a result that informs later decisions , or enable the user to draw new conclusions . Explore cells may also transform data , but only for the purpose of exploring rela - tionships and not for further in - depth analysis Rendering DataFrames , visualizing relationships , printing summaries of data , calculating simple statistics , examining the output of functions Visualizations are used to evaluate the performance of a model ( See : Evaluate ) df . explore ( ) Model Deﬁne and ﬁt models of rela - tionships to data . These cells may include some data trans - formations , but the primary pur - pose is to create a model to describe or predict some facet of the dataset Statistical modeling , ﬁtting and / or specifying machine learning models , simula - tion , deﬁning loss functions Signiﬁcance testing and cal - culating feature importance ( See : Evaluate ) from sklearn . neighbors import KNNeighbors knn = KNNeighbors . Classifier ( ) knn . fit ( iris _ x _ train , iris _ y _ train ) knn . predict ( iris _ x _ test ) Evaluate Measure the explanatory power or predictive accuracy of model using appropriate statistical techniques . These cells sometimes employ visualizations to explore analytical results ( e . g . plotting regression residuals ) Cross validation , signiﬁ - cance testing , inspecting model output , plotting fea - ture signiﬁcance . If a cell both evaluates and deﬁnes a machine learning model ( a common pattern ) , default to ”Model” plot _ confusion _ matrix ( knn , iris _ x _ test , iris _ y _ test ) plt . set _ title ( " Confusion Matrix” ) E . Confusion Matrix The confusion matrix for CORAL’s predictions on the data analysis stage prediction task is shown in Figure 8 . I M P O R T W R A N G L E E X P L O R E M O D E L E V A L U A T E Predicted Label IMPORT WRANGLE EXPLORE MODEL EVALUATE H a n d L a b e l 97 20 8 2 0 8 517 17 15 4 39 178 566 1 1 5 80 14 119 7 2 42 61 7 30 CORAL Predictions 72 . 2 % Accuracy 0 100 200 300 400 500 Fig . 8 . Confusion matrix for CORAL’s predictions on the data analysis stage prediction task . F . Regression Details The following details apply to both regression ( R1 ) and regression ( R2 ) . We chose to use a negative binomial for zero - inﬂated counts regression because we observed that the mean number of citations ( 8 . 52 ) was substantially less than the variance ( 1 , 308 ) . We expect that a paper’s year of publication will inﬂuence its citation count , and therefore we control for this variable . We also expect each paper’s domain to be related to notebook characteristics , so we limit our analysis to the three most common domains in GORC and control for this factor using indicator variables . We note that our analysis does not substantially change with the inclusion of the top ﬁve , 10 , or 20 domains . If a paper is linked to more than one notebook , for the purpose of these regressions , we concatenate the notebooks and calculate statistics across this concatenation .