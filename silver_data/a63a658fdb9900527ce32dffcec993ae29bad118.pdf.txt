TECHNOLOGIES DRUG DISCOVERY TODAY Analysis of complex , multidimensional datasets Mark Girolami 1 , * , Harald Mischak 2 , Ronald Krebs 2 1 Department of Computing Science , University of Glasgow , Glasgow , Scotland , UK G12 8QQ 2 Mosaiques Diagnostics & Therapeutics AG , Mellendorfer Str . 7 - 9 , D - 30625 Hannover , Germany Several exciting new technologies have been developed in the Biomedical and Life Sciences producing genome and proteome wide multidimensional datasets ( see Glossary ) . However , discovery of the relevant informa - tion from this multidimensional data is still a very challenging task . This review examines the main ana - lytic methodologies available to the practitioner . It considers their strengths and weaknesses from a prac - tical standpoint and gives an application example for the classiﬁcation of human health condition . Section Editors : Harald John – Bundeswehr , Institute of Pharmacology and Toxicology , Munich , Germany Rainer Bischoff – Centre of Pharmacy , University of Groningen , The Netherlands Introduction Genome and proteome analysis have opened new roads towards the examination of living organisms and disease . The goal of these and other approaches is to utilise an array of parameters to display different health conditions and speciﬁc diseases . Huge amounts of data are produced with these techniques which can no longer be analysed manu - ally . In general , such datasets consist of thousands of different measured variables ( dimensions ) which can con - tribute to a single observation or phenomena . As described below and in detail elsewhere [ 1 ] , the proteins measured in human urine samples are an example of a multidimensional dataset . Each of the measured proteins represents one dimension of the data ( which can be up to 3500 in a single urine sample ) . The aim of such analysis is to ﬁnd those protein correlations which best describe and predict the patients health condition . The evaluation of such multidimensional datasets requires new approaches for the retrieval of the relevant information . As a consequence , several new statistical meth - ods have been developed over the past decades to help scientists analyse their data . However , the initial enthusiasm about the wealth of data and promise of consequent infor - mation that could be obtained frequently gave way to frus - trating experiences when it came to validation of the ﬁndings in , for example , BLINDED DATASETS ( see glossary ) . Evi - dently , the ‘ CURSE OF DIMENSIONALITY ’ ( see glossary ) was re - dis - covered with great success , much to the chagrin of many scientists who frequently tended to over - interpret the data and who also had only limited tools available to deal with such high - dimensional data . In this short manuscript we attempt to summarise and review the most prominent problems and potential solutions for the examination of MULTIDIMENSIONAL DATA ( see Glossary ) . For a condensed representation , see Table 1 . The opinion given is based to a large extent on our experience using proteome data extracted from human body ﬂuids via capil - lary electrophoresis coupled with mass spectrometry for the purpose of clinical diagnosis [ 1 , 2 ] . As a brief overview , aspects of CLUSTERING ( see Glossary ) , VISUALISATION ( see Glos - sary ) , FEATURESELECTION ( see Glossary ) and extraction of multi - dimensional data will be given . The last section will exp - lain some classiﬁcation methods focussing on a practical Drug Discovery Today : Technologies Vol . 3 , No . 1 2006 Editors - in - Chief Kelvin Lam – Pﬁzer , Inc . , USA Henk Timmerman – Vrije Universiteit , The Netherlands Analytical technologies * Corresponding author : M . Girolami ( girolami @ dcs . gla . ac . uk ) 1740 - 6749 / $ (cid:1) 2006 Published by Elsevier Ltd . DOI : 10 . 1016 / j . ddtec . 2006 . 03 . 010 13 classiﬁcation example with support vector machines ( SVM ) . Figure 1 will give a graphical overview of the different steps . Clustering One of the main motivations in data clustering is to discover hitherto unknown classes or consistent groupings of objects within the data . Examples include the identiﬁcation of pos - sible disease subtypes which cannot be differentiated by conventional clinical assays alone [ 3 ] . In [ 3 ] , differential gene expression is the main feature upon which the clusters are identiﬁed thus indicating the possible molecular basis of the disease . Another motivation for clustering data is to attempt the identiﬁcation of developmental states or study certain biological processes , a notable example being the cell - cycle [ 4 ] . When data are generated from gene - expression microar - rays , both ‘Condition’ and ‘Genes’ are meaningful objects in their own right , and as such microarray data can be clustered either by Conditions , thus identifying samples in conditions ( e . g . diseased ) or at time - points which share strong similarities in their genomic proﬁle , or by Genes , implicating groups of genes that might be participating in the same pathway . The dominant clustering method of choice for most prac - titioners is HIERARCHICAL CLUSTERING ( see Glossary ) . This is prob - ably owed primarily to the availability of widely distributed and easy - to - use integrated software suites which have one of several hierarchical clustering methods at their core . How - ever , hierarchical clustering methods are not without their shortcomings . The main shortcoming is that it is difﬁcult to objectively assess the validity of identiﬁed clustering group - ings or indeed the number of groupings that might really be inherent in the data . To overcome these two main difﬁculties , clustering meth - ods based on MIXTUREMODELLING ( see Glossary ) [ 5 ] can be found to be particularly useful where LIKELIHOOD ( see Glossary ) - based measures of model ﬁt are used to select the most appropriate model of the data based on the number of components which correspond to clusters in the data . Although mixture models can improve upon hierarchical clustering methods there is an explicit assumption that each object will belong to one and only one cluster . This might be a sensible assumption when considering the clustering of data from tissue samples obtained from patients with a distinct disease . However , if gene clustering is being performed to identify functional Drug Discovery Today : Technologies | Analytical technologies Vol . 3 , No . 1 2006 Glossary Artiﬁcial neural networks : ﬂexible data models inspired by neuronal information processing . Much hyped in the mid - 1990s these methods of classiﬁcation have largely been superseded by more statistically sound methods . Bayesian classiﬁcation : predicting the class or type of an arbitrary object employing probabilities of class membership , derived from Bayesian statistics , as the main decision making mechanism . Bayesian methods : inferential methods which provide a consistent framework for reasoning with uncertainty . Bayesian statistics employ the notion of numerically encoding strength of belief in propositions by probabilities and provide consistent mechanisms to update the strength of these beliefs upon observing the outcome of the occurrence of an event . Blinded dataset : a dataset which will have key descriptors removed for testing of prediction or classiﬁcation methods . Such a dataset will provide a realistic test of a prediction method where the descriptors to be predicted are completely unavailable thus removing any possible bias in the predictions made . Clustering : clustering algorithms ﬁnd groups of items that are similar . For example , clustering is used to group tissue sample together which share similar levels of gene expression over a time course . CurseofDimensionality : referstotheexponentialgrowthofvolume asafunctionofdimensionality . Asthenumberoffeaturesdescribingdata grows the number of data points required to ﬁll the corresponding volume grows at an exponential rate . So data deﬁned in high dimensions will tend to sparsely populate the corresponding space and as such provide poor coverage of this space . In practical terms this will tend to adversely impact the performance of most methods for classiﬁcation . Decision trees : a tree - like way of representing a collection of hierarchical rules that lead to a class or value . Feature selection : the selection of a subset of features or attributes which are highly predictive of a target value , for example , a class label . Gaussian process ( GP ) classiﬁcation : the use of Gaussian process priors over functions in classifying objects . Generative topographic mapping ( GTM ) : originally proposed as a probabilistic alternative to the self - organising map ( SOM ) . The GTM provides a means of visualising high - dimensional data which might have strong nonlinear dependencies between features . Hierarchical clustering : a speciﬁc type of clustering method which produces a hierarchy of groupings from the very general – all points are assigned to one cluster – to the very speciﬁc where each data point deﬁnes a cluster . Independent component analysis ( ICA ) : a generalisation of PCA where the data directions found are as independent as possible , which is a stronger criterion than decorrelation . Kernel matrix : a matrix whose elements deﬁne the pair - wise levels of similarities between data points where the similarities are deﬁned by a speciﬁc kernel function . K - nearest neighbours : a classiﬁcation rule which makes a decision based on the majority class assigned to the K - nearest neighbours of a query point . Kohonen self - organising map ( SOM ) : a model of neural self organisation which exhibits clustering capabilities and has been hugely popular as a means of organising and visualising high - dimensional data . Likelihood : a term employed by statisticians to refer to the joint probability of a sample of data points . Mixture modelling : the modelling of a probability density function based on a mixture of simpler density functions . Multidimensional data : data for which an example is described by multiple features where each feature describes a dimension in a space deﬁned by the multiplicity of features . Partial least squares ( PLS ) : a dimension reduction method which is useful when the number of features exceeds the number of examples . Principal component analysis ( PCA ) : the aim of PCA is to derive a small number of linear combinations of a set of features that retain as much of the information in the original features as possible . Probabilistic methods : computational methods for data analysis and modelling which are based on devising a probabilistic model of the data . Support vector machine ( SVM ) : the SVM is a classiﬁcation method which is based on decision planes that deﬁne decision boundaries . The decision plane is deﬁned by the points from both classes which are closest to each other in the feature space – these points are called the Support Points as they support the decision plane . Visualisation : ameansofvisuallybeingabletoexplorethestructures – such as clusters – inherent in multidimensional data . 14 www . drugdiscoverytoday . com pathway - based groupings then it makes no biological sense that genes will only be active in a single pathway . PROBABILISTIC and BAYESIAN METHODS ( see Glossary ) of data clustering have been developed recently . These allow multi - ple cluster memberships [ 6 – 8 ] and are very promising as tools for biological data exploration and hypothesis generation for two main reasons . First , uncertainty in predictions , for exam - ple , assigning a gene to a speciﬁc cluster , is explicitly provided in the form of probabilities thus allowing the scientist to interpret and consistently reason about the validity of the eventual data partitioning based on these uncertainty mea - sures . The second reason is that background or prior knowl - edge which the scientist has about the system under study can be explicitly employed in the form of prior beliefs which can be updated and revised in the light of new experimental evidence . Visualisation When performing an exploratory analysis of multidimen - sional data , it is often useful to visualise the data in some way which provides insight into the structures underlying these data in multidimensional space . Hierarchical clustering provides a means of visualising the cluster structure , but a hierarchy is imposed . However , it is often desirable to view the natural structure of the data . For example , assessing whether the clusters are homogenous and self - consistent or if a large amount of internal variability is present . This information is lost in a hierarchical structured representation of the data . To visualise multidimensional data , the most straightfor - ward strategy is to provide a mapping from the original multidimensional space onto a two - dimensional plane in such a way that the similarity and distributional character - istics in the original space are preserved as much as possible . However , this strategy always results in loss of information . A very popular method for multidimensional data visualisation in general is the KOHONEN SELF - ORGANISING MAP ( SOM ) ( see Glos - sary ) [ 9 ] , which is widely used as a means of visualising genomic and proteomic multidimensional data [ 10 ] . The SOM algorithm has its origins in computational neurobiol - ogy as a model to study the principles of self - organisation , Vol . 3 , No . 1 2006 Drug Discovery Today : Technologies | Analytical technologies Table 1 . Overview of prominent algorithms in multidimensional data analysis Visualisation Feature selection Feature extraction Classiﬁcation Clustering Useful in creating hierarchies . X The clusters centre might be used as meta - features in some applications . X SOM Can allow visualisation of cluster and nonlinear structures in data . Drawback is that method is not probabilistic . X As above X GTM Can allow visualisation of cluster and nonlinear structures in data in a probabilistic manner . X As above X PCA and ICA Projection onto 2D subspace might highlight simple clustered structure . Drawback is that both methods are linear and might miss nonlinear relationships within data . SOM and GTM can overcome these problems . Examining the eigenvalues of the PCA can indicate if a dominant lower - dimensional subspace captures the majority of information ( variability ) in the data and so indicate the number of effective features . Using the lower dimensional projections can be seen as performing feature extraction . Can be shown theoretically that a reduction in variance of predictions can be obtained by removing dimensions associated with small eigenvalues . Often does not translate into practical beneﬁts . X Statistics X Many classical statistical tests can be employed in assessing how relevant individual features are . X X SVM X X Has proved to be a very popular and successful method of classiﬁcation with high - dimensional data problems . GP X X Has all the beneﬁts of SVM and in addition provides a fully probabilistic method of classiﬁcation . www . drugdiscoverytoday . com 15 and as such , the theoretical basis of the method has been the subject of many papers over the past 15 years . A more theoretically principled alternative to the SOM for visualisa - tion of multidimensional data is the generative topographic mapping ( GTM ) ( see Glossary ) [ 11 ] . The GTM is a fully prob - abilistic model having all the consistent semantics of prob - abilistic inference , and an interactive software tool for the topographic visualisation of multidimensional data is avail - able speciﬁcally for bioinformatics applications [ 12 ] . Feature selection Although the last two sections have focused on methods available for the exploratory analysis of multidimensional data , the remaining sections will look at analysis methods which are focused on enabling classiﬁcation decisions . Health informatics is one of the main drivers in the devel - opment of reliable methods of disease prediction . In [ 3 ] , for example , patients gene - expression proﬁles are used to distin - guish between different forms of leukaemia , whereas in [ 13 ] , various disease types were identiﬁed using capillary electro - phoresis mass spectrometry . Methods for classiﬁcation based on classical multivariate statistical theory should not be applied onto data where the number of dimensions exceeds the number of examples . For instance , data derived from a microarray study aimed towards the establishment of a classiﬁer which will discriminate between diseased and healthy tissue might well describe the expression levels of over 20 , 000 genes for each tissue sample , with generally far less than 1000 examples of each type of tissue . The problem that is encountered is the ‘Curse of Dimensionality’ [ 14 ] , which basically tells us that when the number of samples is small compared to the dimensionality of the samples , a perfect classiﬁcation can be achieved in the dataset by chance . The classiﬁer will make decisions which have little to do with the information content of the data . This will produce a classiﬁer which will subsequently make poor predictions on new datasets . To solve this problem , several strategies have been developed . One such strategy is to select the individual single features which show high discriminatory power between the classes . This can be achieved by performing several possible statistical tests to assess , for example , signiﬁcant differences of mean or median values across classes . This form of feature ranking and selection will reduce the number of features which subsequently can be used to build the classiﬁer and help to reduce the variability in the eventual classiﬁer performance . Feature extraction An alternative strategy for reducing the dimensionality of the feature space is to extract composite features using methods such as principal component analysis ( PCA ) ( see Glossary ) . PCA is a standard multivariate statistical technique which will provide the optimal reduction in dimensionality in the mean - squared sense , and so the reduced dimensional feature space will retain the maximal amount of variance in the original data . Several recent papers have employed PCA in combination with various forms of classiﬁcation methods using data from mass spectrometry [ 15 ] and gene expression [ 16 ] . Robust and probabilistic variants of PCA have been developed and applied to expression classiﬁcation and accounting for probe level noise [ 17 , 18 ] . The PCA method implicitly assumes Gaussian distributions for the subspace variables . However , various attempts have been described to relax this assumption and account for non - Gaussian effects in the data [ 19 , 20 ] . This is often referred to as independent component analysis ( ICA ) ( see Glossary ) . ICA has been applied to metabolite ﬁngerprinting [ 21 ] and identify HIV dynamics [ 22 ] . However , it currently appears doubtful if ICA can bring any further beneﬁts as a feature extraction method for improving classiﬁcation performance while using multidimensional data . Both PCA and ICA identify low - dimensional subspaces without taking into account the labellings associated with Drug Discovery Today : Technologies | Analytical technologies Vol . 3 , No . 1 2006 Figure 1 . General overview of a classiﬁcation process . Visualisation might help in a better understanding of the raw data and so contribute to feature selection . Clustering can help both in feature selection and especially in feature extraction but is not essential for classiﬁcation . Feature selection / extraction is necessaryto providethe classiﬁer with the relevant input data . 16 www . drugdiscoverytoday . com the data samples in a classiﬁcation setting , which is subopti - mal . When seeking to reduce the dimensionality of the data for the purposes of deﬁning a classiﬁcation system , the classes of the data should be taken into account . This is precisely what partial least squares ( PLS ) ( see Glossary ) [ 23 ] achieves . PLS seeks to identify a subspace which is maximally correlated with the class labels , and it would seem that this currently is the most sensible feature extraction method for classiﬁcation purposes . PLS has been widely applied in chemometrics where datasets are such that the number of attributes ( fea - tures ) is far in excess of the actual number of samples avail - able . Recently PLS has been used in leukaemia prediction from gene expression [ 24 ] , whereas in [ 25 ] , PLS is used to relate gene - expression proﬁles to various clinical phenotypes of patients . Classiﬁcation This review so far has discussed the main methods of analysis of multidimensional datasets . Now we will discuss classiﬁca - tion , which is of great importance in health informatics . In general , the diagnosis made by a physician can be viewed as a classiﬁcation . But in addition to evidence - based knowledge of a disease , which is needed for the classiﬁcation , the experi - ence of the physician has a big impact on the decision . Whereas the ﬁrst could be mathematically trained by statis - tical approaches , the second would mean to reproduce asso - ciative thinking and empirical knowledge which is still an unsolved task . Often the solution of a task can be solved by understanding and asking the right question . The correlation between the proteins synthesised by the human body and its physiological state seems to be better suited for machine learning and classiﬁcation . Because the proteins and peptides establish the complex communication , interaction and sti - mulations of cells , they more or less directly display the human health condition . So the proteins should be observed and evaluated rather than the symptoms . However , there are many peptides which are not linked to the human health condition . Owing to biological complexity it is not sufﬁcient to track and monitor single peptides for a better understanding and detection of diseases . In the case of disease detection up to thousands of peptides have to be taken into consideration to track down those peptides which are directly linked to a disease . As a new approach , a diag - nostic system was developed [ 1 ] based on the detection of changes of protein concentrations in human urine . Up to 3500 different polypeptides are detected in a single urine sample with this technique ; Fig . 2 illustrates a typical proﬁle as a point in a 6500 dimensional ‘polypeptide’ space . In a pool of 2000 human urine samples a total of 7000 polypeptides has been identiﬁed , which are present in > 40 % of at least one subset ( speciﬁc disease ) . Clearly the number of features ( pro - teins ) exceeds the number of available samples which leads to the above described ‘Curse of Dimensionality’ . These ﬁgures already show the new challenging problems which occur when predicting health states from urine samples with this huge amount of polypeptides . Other approaches in medicine only monitor several features and the number of samples is far greater than dimension of the feature space . For these approaches various classiﬁcation algorithms are used , such as neural networks ( see Glossary ) , K - NEAREST NEIGHBOURS ( see Glos - sary ) , or DECISION TREES ( see Glossary ) . However , well - known drawbacks of these algorithms are a slow convergence , inac - curacy or unsuitability for high - dimensional input . Support vector machines [ 26 ] appear to overcome many of the described problems . SVMs often are reported as achieving superior classiﬁcation performance compared to other meth - ods when compared across most applications and tasks . What is of importance is that they are fairly insensitive to the ‘Curse of Dimensionality’ , primarily because of the use of the ‘ker - nel’ matrix [ 26 ] . This matrix is derived from the similarities between samples and thus operate on dimensions which are equal to the number of data points available . ( Figure 3 gives an example derived from 100 samples , each of dimension 6500 . ) Therefore , SVMs are computationally reasonably efﬁ - cient to cope with large - scale classiﬁcation in both sample and variables . In clinical bioinformatics , they have the poten - tial to provide powerful experimental disease diagnostic models based on gene or protein expression data with thou - sands of features and a small number , as little as a few dozen of samples [ 27 , 28 ] . Vol . 3 , No . 1 2006 Drug Discovery Today : Technologies | Analytical technologies Figure 2 . An example of a multidimensional data point deﬁning a peptide proﬁle for one individual sample in 6500 dimensional space . The horizontal axis represents each of the 6500 peptides identiﬁed using capillary electrophoresis mass spectrometry and the vertical axis deﬁnes the relative abundance of the speciﬁc peptides measured . So this single sample can be considered as specifying a point in a space with 6500 coordinates , each coordinate corresponding to a speciﬁc peptide . www . drugdiscoverytoday . com 17 Furthermore , the approach described above applied towards the analysis of proteins from urine samples has performed well in the classiﬁcation of several diseases by utilising SVMs . This approach might lead to a new diagnostic tool because the classiﬁcation results can even compete with current ‘gold standards’ [ 29 , 30 , 31 ] . As shown in [ 32 ] , other applications like evaluation of therapy and therapeutic drugs are possible with this technique . However , SVMs are not without their drawbacks and the main problem is their lack of all probabilistic semantics which in clinical informatics applications is a serious problem because no sense or measure of conﬁdence in predictions can be obtained without post hoc processing . An alternative classiﬁcation method to SVM which has all the beneﬁts of the SVM in terms of its nonparametric char - acteristics but also has full probabilistic semantics is BAYESIAN CLASSIFICATION employing Gaussian process ( GP ) priors ( see Glossary ) [ 33 , 34 ] . GP classiﬁcation does not have the wide - spread appeal within the Bioinformatics community as have the SVM but some applications are beginning to appear that employ GP [ 35 ] , and we anticipate that this will increase owing to the fully probabilistic Bayesian inference which can be achieved with their use . Additional advantages of employ - ing GP for classiﬁcation within a clinical informatics setting are as follows : multiple datasets which might be heteroge - neous in their calibration and structure ( e . g . microarray gene expression and polypeptide concentrations ) can be inte - grated naturally , to provide synergistic improvements in classiﬁcation performance accuracy and / or certainty of pre - dictions in a consistent manner by employing the GP Baye - sian inferential framework . Concluding remarks The fast development of technologies that enable the gen - eration of multidimensional datasets has resulted in addi - tional demands for mathematical and statistical solutions , which have yet to be optimised for these applications . Cur - rently , SVM and GP appear to be the most promising approaches , however , certainly not without their own draw - backs and weaknesses . A key element in all these approaches is extraction and loading of the single features that are utilised , which can frequently be accomplished with good success using common statistical methods and parameters . In addition , it should be clearly emphasised that it cur - rently is imperative to consider reports on multidimensional datasets ( be it genomics , proteomics , metabolomics or any other - omics ) only as valid and publishable , if they are validated in a blinded dataset . Strict adherence to these simple rules will greatly foster the responsible utilisation of multidimensional data , and also help to expedite this emer - ging and exciting ﬁeld . References 1 Fliser , D . et al . ( 2005 ) Capillary electrophoresis coupled to mass spectrometry for clinical diagnostic purposes . Electrophoresis 26 , 2708 – 2716 2 Kolch , W . et al . ( 2005 ) Capillary electrophoresis - mass spectrometry as a powerfultoolinclinicaldiagnosisandbiomarkerdiscovery . MassSpectrom . Rev . 24 , 959 – 977 3 Golub , T . R . et al . ( 1999 ) Molecular classiﬁcation of cancer : class discoveryandclasspredictionbygene - expressionmonitoring . Science 286 , 531 – 537 4 Eisen , M . B . et al . ( 1998 ) Cluster analysis and display of genome - wide expression patterns . Proc . Natl . Acad . Sci . U S A 95 , 14863 – 14868 5 McLachlan , G . J . et al . ( 2002 ) A mixture model - based approach to the clustering of microarray expression data . Bioinformatics 18 , 413 – 422 6 Battle , A . et al . ( 2005 ) Probabilistic discovery of overlapping cellular processes and their regulation using gene expression data . J . Comput . Biol . 12 , 909 – 927 7 Rogers , S . et al . ( 2005 ) The latent process decomposition of cDNA microarray datasets . IEEE / ACM Trans . Comput . Biol . Bioinform . 2 , 143 – 156 8 Carrivick , L . et al . Identiﬁcation of prognostic signatures in breast cancer microarray data using Bayesian techniques . J . R . Soc . Interface . ISSN 1742 - 5662 ( in press ) 9 Kohonen , T . ( 2001 ) Self - Organizing Maps ( 3rd edn ) , Springer Drug Discovery Today : Technologies | Analytical technologies Vol . 3 , No . 1 2006 Figure 3 . Instead of working with the original sample representation in the original 6500 dimensional peptide space , SVM and GP classiﬁcation methods operate directly on what is called the KERNEL MATRIX ( see Glossary ) . This matrix has elements which correspond to the similarities between data points in the original space ; in other words , each element of the matrix gives a measure of how similar the measured peptide proﬁles are between a pair of samples ( red indicates highly similar and blue indicates dissimilarity ) . So the above ﬁgure shows how similar the peptide proﬁles are for every pair of samples available , where each axis corresponds to the total available samples ( 100 in total ) . The ﬁrst thing to note is that we have now drastically reduced the dimensionality of the representation of each sample , from a point in a 6500 dimensional peptide proﬁle space to a point in a 100 dimensional ‘peptide proﬁle similarity space’ . The second thing to note is that the matrix identiﬁes that there are two groups of samples which all share highly similar peptide proﬁles ( samples 1 – 45 and samples 46 – 86 ) with some evidence of a smaller group ( samples 86 – 100 ) . This data representation is now employed in SVM and GP classiﬁers rather than the original multidimensional space . 18 www . drugdiscoverytoday . com 10 Hsu , A . L . et al . ( 2003 ) An unsupervised hierarchical dynamic self - organizing approach to cancer class discovery and marker gene identiﬁcation in microarray data . Bioinformatics 19 , 2131 – 2140 11 Bishop , C . M . et al . ( 1998 ) GTM : the generative topographic mapping . Neural Comput . 10 , 215 – 234 12 D’Alimonte , D . et al . ( 2005 ) MILVA : an interactive tool for the exploration of multidimensional microarray data . Bioinformatics 21 , 4192 – 4193 13 Rogers , S . et al . ( 2005 ) Disease classiﬁcation from capillary electrophoresis : mass spectrometry . In Pattern Recognition and Data Mining : Third International Conference on Advances in Pattern Recognition , pp . 183 – 192 , Springer - Verlag 14 Somorjai , R . L . et al . ( 2003 ) Class prediction and discovery using gene microarray and proteomics mass spectroscopy data : curses , caveats , cautions . Bioinformatics 19 , 1484 – 1491 15 Yu , J . S . et al . ( 2005 ) Ovariancanceridentiﬁcation basedondimensionality reduction for high - throughput mass spectrometry data . Bioinformatics 21 , 2200 – 2209 16 Bicciato , S . et al . ( 2003 ) PCA disjoint models for multiclass cancer analysis using gene expression data . Bioinformatics 19 , 571 – 578 17 Hubert , M . and Engelen , S . ( 2004 ) Robust PCA and classiﬁcation in biosciences . Bioinformatics 20 , 1728 – 1736 18 Sanguinetti , G . et al . ( 2005 ) Accounting for probe - level noise in principal component analysis of microarray data . Bioinformatics 21 , 3748 – 3754 19 Girolami , M . and Breitling , R . ( 2004 ) Biologically valid linear factor models of gene expression . Bioinformatics 20 , 3021 – 3033 20 Martoglio , A - M . et al . ( 2002 ) A decomposition model to track gene expression signatures : preview on observer - independent classiﬁcation of ovarian cancer . Bioinformatics 18 , 1617 – 1624 21 Scholz , M . et al . ( 2004 ) Metabolite ﬁngerprinting : detecting biological features by independent component analysis . Bioinformatics 20 , 2447 – 2454 22 Draghici , S . et al . ( 2003 ) Mining HIV dynamics using independent component analysis . Bioinformatics 19 , 981 – 986 23 Wold , H . ( 1985 ) Partial least squares . In Encyclopedia of Statistical Sciences , ( Vol . 6 ) ( Kotz , S . and Johnson , N . L . , eds ) pp . 581 – 591 , Wiley 24 Fort , G . and Lambert - Lacroix , S . ( 2005 ) Classiﬁcation using partial least squares with penalized logistic regression . Bioinformatics 21 , 1104 – 1111 25 Gui , J . and Li , H . ( 2005 ) Penalized Cox Regression analysis in the high - dimensionalandlow - samplesizesettings , with applicationstomicroarray gene expression data . Bioinformatics 21 , 3001 – 3008 26 Burges , C . J . C . ( 1998 ) A tutorial on support vector machines for pattern recognition . Data Mining Knowledge Discov . 2 , 121 – 167 27 Furey , T . S . etal . ( 2000 ) Supportvectormachine ( seeglossary ) classiﬁcation and validation of cancer tissue samples using microarray expression data . Bioinformatics 16 , 906 – 914 28 Mukherjee , S . ( 2003 ) Classifying Microarray Data Using Support Vector Machines , Understanding And Using Microarray Analysis Techniques : A Practical Guide . Kluwer Academic Publishers 29 Wittke , S . et al . ( 2005 ) Detection of acute tubulointerstitial rejection by proteomic analysis of urinary samples in renal transplant recipients . Am . J . Transplant . 5 , 2479 – 2488 30 Weissinger , E - M . etal . ( 2004 ) Proteomicpatternsestablishedwithcapillary electrophoresis and mass spectrometry for diagnostic purposes . Kidney Int . 65 , 2426 – 2434 31 Theodorescu , D . et al . ( 2006 ) Discovery and validation of novel urothelial cancer protein biomarkers . Lancet Oncol . 7 , 230 – 240 32 Rossing , K . et al . ( 2005 ) Impact of diabetic nephropathy and angiotensin II receptor blockade on urinary polypeptide patterns . Kidney Int . 68 , 193 – 205 33 Williams , C . K . I . and Barber , D . ( 1998 ) Bayesian classiﬁcation with Gaussian processes . IEEE Trans . Pattern Analysis Machine Intell . 20 , 1342 – 1351 34 Girolami , M . and Rogers , S . Variational Bayesian multinomial proit regression with Gaussian process priors . In Neural Computation , MIT Press ( in press ) 35 Chu , W . et al . ( 2005 ) Biomarker discovery with Gaussian processes in microarray gene expression data . Bioinformatics 21 , 3385 – 3393 Vol . 3 , No . 1 2006 Drug Discovery Today : Technologies | Analytical technologies www . drugdiscoverytoday . com 19