a r X i v : 0708 . 3601v1 [ s t a t . A P ] 27 A ug 2007 The Annals of Applied Statistics 2007 , Vol . 1 , No . 1 , 17 – 35 DOI : 10 . 1214 / 07 - AOAS114 c (cid:13) Institute of Mathematical Statistics , 2007 A CORRELATED TOPIC MODEL OF SCIENCE 1 By David M . Blei and John D . Lafferty Princeton University and Carnegie Mellon University Topic models , such as latent Dirichlet allocation ( LDA ) , can be useful tools for the statistical analysis of document collections and other discrete data . The LDA model assumes that the words of each document arise from a mixture of topics , each of which is a distri - bution over the vocabulary . A limitation of LDA is the inability to model topic correlation even though , for example , a document about genetics is more likely to also be about disease than X - ray astron - omy . This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions . In this paper we develop the correlated topic model ( CTM ) , where the topic pro - portions exhibit correlation via the logistic normal distribution [ J . Roy . Statist . Soc . Ser . B 44 ( 1982 ) 139 – 177 ] . We derive a fast varia - tional inference algorithm for approximate posterior inference in this model , which is complicated by the fact that the logistic normal is not conjugate to the multinomial . We apply the CTM to the articles from Science published from 1990 – 1999 , a data set that comprises 57M words . The CTM gives a better ﬁt of the data than LDA , and we demonstrate its use as an exploratory tool of large document col - lections . 1 . Introduction . Large collections of documents are readily available on - line and widely accessed by diverse communities . As a notable example , scholarly articles are increasingly published in electronic form , and histor - ical archives are being scanned and made accessible . The not - for - proﬁt or - ganization JSTOR ( www . jstor . org ) is currently one of the leading providers of journals to the scholarly community . These archives are created by scan - ning old journals and running an optical character recognizer over the pages . JSTOR provides the original scans on - line , and uses their noisy version of Received March 2007 ; revised April 2007 . 1 Supported in part by NSF Grants IIS - 0312814 and IIS - 0427206 , the DARPA CALO project and a grant from Google . Supplementary material and code are available at http : / / imstat . org / aoas / supplements Key words and phrases . Hierarchical models , approximate posterior inference , varia - tional methods , text analysis . This is an electronic reprint of the original article published by the Institute of Mathematical Statistics in The Annals of Applied Statistics , 2007 , Vol . 1 , No . 1 , 17 – 35 . This reprint diﬀers from the original in pagination and typographic detail . 1 2 D . M . BLEI AND J . D . LAFFERTY the text to support keyword search . Since the data are largely unstructured and comprise millions of articles spanning centuries of scholarly work , au - tomated analysis is essential . The development of new tools for browsing , searching and allowing the productive use of such archives is thus an impor - tant technological challenge , and provides new opportunities for statistical modeling . The statistical analysis of documents has a tradition that goes back at least to the analysis of the Federalist papers by Mosteller and Wallace [ 21 ] . But document modeling takes on new dimensions with massive multi - author collections such as the large archives that now are being made accessible by JSTOR , Google and other enterprises . In this paper we consider topic models of such collections , by which we mean latent variable models of doc - uments that exploit the correlations among the words and latent semantic themes . Topic models can extract surprisingly interpretable and useful struc - ture without any explicit “understanding” of the language by computer . In this paper we present the correlated topic model ( CTM ) , which explicitly models the correlation between the latent topics in the collection , and en - ables the construction of topic graphs and document browsers that allow a user to navigate the collection in a topic - guided manner . The main application of this model that we present is an analysis of the JSTOR archive for the journal Science . This journal was founded in 1880 by Thomas Edison and continues as one of the most inﬂuential scientiﬁc journals today . The variety of material in the journal , as well as the large number of articles ranging over more than 100 years , demonstrates the need for automated methods , and the potential for statistical topic models to provide an aid for navigating the collection . The correlated topic model builds on the earlier latent Dirichlet allocation ( LDA ) model of Blei , Ng and Jordan [ 8 ] , which is an instance of a general family of mixed membership models for decomposing data into multiple la - tent components . LDA assumes that the words of each document arise from a mixture of topics , where each topic is a multinomial over a ﬁxed word vocabulary . The topics are shared by all documents in the collection , but the topic proportions vary stochastically across documents , as they are ran - domly drawn from a Dirichlet distribution . Recent work has used LDA as a building block in more sophisticated topic models , such as author - document models [ 19 , 24 ] , abstract - reference models [ 12 ] syntax - semantics models [ 16 ] and image - caption models [ 6 ] . The same kind of modeling tools have also been used in a variety of nontext settings , such as image processing [ 13 , 26 ] , recommendation systems [ 18 ] , the modeling of user proﬁles [ 14 ] and the modeling of network data [ 1 ] . Similar models were independently developed for disability survey data [ 10 , 11 ] and population genetics [ 22 ] . In the parlance of the information retrieval literature , LDA is a “bag of words” model . This means that the words of the documents are assumed to A CORRELATED TOPIC MODEL OF SCIENCE 3 be exchangeable within them , and Blei , Ng and Jordan [ 8 ] motivate LDA from this assumption with de Finetti’s exchangeability theorem . As a con - sequence , models like LDA represent documents as vectors of word counts in a very high dimensional space , ignoring the order in which the words ap - pear . While it is important to retain the exact sequence of words for reading comprehension , the linguistically simplistic exchangeability assumption is essential to eﬃcient algorithms for automatically eliciting the broad seman - tic themes in a collection . The starting point for our analysis here is a perceived limitation of topic models such as LDA : they fail to directly model correlation between topics . In most text corpora , it is natural to expect that subsets of the underlying latent topics will be highly correlated . In Science , for instance , an article about genetics may be likely to also be about health and disease , but un - likely to also be about X - ray astronomy . For the LDA model , this limitation stems from the independence assumptions implicit in the Dirichlet distri - bution on the topic proportions . Under a Dirichlet , the components of the proportions vector are nearly independent , which leads to the strong and unrealistic modeling assumption that the presence of one topic is not corre - lated with the presence of another . The CTM replaces the Dirichlet by the more ﬂexible logistic normal distribution , which incorporates a covariance structure among the components [ 4 ] . This gives a more realistic model of the latent topic structure where the presence of one latent topic may be correlated with the presence of another . However , the ability to model correlation between topics sacriﬁces some of the computational conveniences that LDA aﬀords . Speciﬁcally , the con - jugacy between the multinomial and Dirichlet facilitates straightforward approximate posterior inference in LDA . That conjugacy is lost when the Dirichlet is replaced with a logistic normal . Standard simulation techniques such as Gibbs sampling are no longer possible , and Metropolis – Hastings based MCMC sampling is prohibitive due to the scale and high dimension of the data . Thus , we develop a fast variational inference procedure for carrying out approximate inference with the CTM model . Variational inference [ 17 , 29 ] trades the unbiased estimates of MCMC procedures for potentially biased but computationally eﬃcient algorithms whose numerical convergence is easy to assess . Variational inference algorithms have been eﬀective in LDA for analyzing large document collections [ 8 ] . We extend their use to the CTM . The rest of this paper is organized as follows . We ﬁrst present the corre - lated topic model and discuss its underlying modeling assumptions . Then , we present an outline of the variational approach to inference ( the techni - cal details are collected in the Appendix ) and the variational expectation – maximization procedure for parameter estimation . Finally , we analyze the 4 D . M . BLEI AND J . D . LAFFERTY performance of the model on the JSTOR Science data . Quantitatively , we show that it gives a better ﬁt than LDA , as measured by the accuracy of the predictive distributions over held out documents . Qualitatively , we present an analysis of all of Science from 1990 – 1999 , including examples of topically related articles found using the inferred latent structure , and topic graphs that are constructed from a sparse estimate of the covariance structure of the model . The paper concludes with a brief discussion of the results and future work that it suggests . 2 . The correlated topic model . The correlated topic model ( CTM ) is a hierarchical model of document collections . The CTM models the words of each document from a mixture model . The mixture components are shared by all documents in the collection ; the mixture proportions are document - speciﬁc random variables . The CTM allows each document to exhibit mul - tiple topics with diﬀerent proportions . It can thus capture the heterogeneity in grouped data that exhibit multiple latent patterns . We use the following terminology and notation to describe the data , latent variables and parameters in the CTM . • Words and documents . The only observable random variables that we consider are words that are organized into documents . Let w d , n denote the n th word in the d th document , which is an element in a V - term vo - cabulary . Let w d denote the vector of N d words associated with document d . • Topics . A topic β is a distribution over the vocabulary , a point on the V − 1 simplex . The model will contain K topics β 1 : K . • Topic assignments . Each word is each assumed drawn from one of the K topics . The topic assignment z d , n is associated with the n th word and d th document . • Topic proportions . Finally , each document is associated with a set of topic proportions θ d , which is a point on the K − 1 simplex . Thus , θ d is a distri - bution over topic indices , and reﬂects the probabilities with which words are drawn from each topic in the collection . We will typically consider a natural parameterization of this multinomial η = log ( θ i / θ K ) . Speciﬁcally , the correlated topic model assumes that an N - word document arises from the following generative process . Given topics β 1 : K , a K - vector µ and a K × K covariance matrix Σ : 1 . Draw η d | { µ , Σ } ∼ N ( µ , Σ ) . 2 . For n ∈ { 1 , . . . , N d } : ( a ) Draw topic assignment Z d , n | η d from Mult ( f ( η d ) ) . ( b ) Draw word W d , n | { z d , n , β 1 : K } from Mult ( β z d , n ) , A CORRELATED TOPIC MODEL OF SCIENCE 5 Fig . 1 . Top : Probabilistic graphical model representation of the correlated topic model . The logistic normal distribution , used to model the latent topic proportions of a document , can represent correlations between topics that are impossible to capture using a Dirichlet . Bottom : Example densities of the logistic normal on the 2 - simplex . From left : diagonal covariance and nonzero - mean , negative correlation between topics 1 and 2 , positive corre - lation between topics 1 and 2 . where f ( η ) maps a natural parameterization of the topic proportions to the mean parameterization , θ = f ( η ) = exp { η } P i exp { η i } . ( 1 ) ( Note that η does not index a minimal exponential family . Adding any constant to η will result in an identical mean parameter . ) This process is illustrated as a probabilistic graphical model in Figure 1 . ( A probabilistic graphical model is a graph representation of a family of joint distributions with a graph . Nodes denote random variables ; edges denote possible depen - dencies between them . ) The CTM builds on the latent Dirichlet allocation ( LDA ) model [ 8 ] . La - tent Dirichlet allocation assumes a nearly identical generative process , but one where the topic proportions are drawn from a Dirichlet . In LDA and its variants , the Dirichlet is a computationally convenient distribution over topic proportions because it is conjugate to the topic assignments . But , the Dirichlet assumes near independence of the components of the proportions . In fact , one can simulate a draw from a Dirichlet by drawing from K inde - pendent Gamma distributions and normalizing the resulting vector . ( Note 6 D . M . BLEI AND J . D . LAFFERTY that there is slight negative correlation due to the constraint that the com - ponents sum to one . ) Rather than use a Dirichlet , the CTM draws a real valued random vector from a multivariate Gaussian and then maps it to the simplex to obtain a multinomial parameter . This is the deﬁning characteristic of the logistic Normal distribution [ 2 , 3 , 4 ] . The covariance of the Gaussian induces de - pendencies between the components of the transformed random simplicial vector , allowing for a general pattern of variability between its components . The logistic normal was originally studied in the context of analyzing ob - served compositional data , such as the proportions of minerals in geological samples . In the CTM , we use it to model the latent composition of topics associated with each document . The drawback of using the logistic normal is that it is not conjugate to the multinomial , which complicates the corresponding approximate posterior inference procedure . The advantage , however , is that it provides a more expressive document model . The strong independence assumption imposed by the Dirichlet is not realistic when analyzing real document collections , where one ﬁnds strong correlations between the latent topics . For example , a document about geology is more likely to also be about archeology than genetics . We aim to use the covariance matrix of the logistic normal to capture such relationships . In Section 4 we illustrate how the higher order structure given by the covariance can be used as an exploratory tool for better understanding and navigating a large corpus of documents . Moreover , modeling correlation can lead to better predictive distributions . In some applications , such as auto - matic recommendation systems , the goal is to predict unseen items condi - tioned on a set of observations . A Dirichlet - based model will predict items based on the latent topics that the observations suggest , but the CTM will predict items associated with additional topics that are correlated with the conditionally probable topics . 3 . Computation with the correlated topic model . We address two com - putational problems that arise when using the correlated topic model to analyze data . First , given a collection of topics and distribution over topic proportions { β 1 : K , µ , Σ } , we estimate the posterior distribution of the latent variables conditioned on the words of a document p ( η , z | w , β 1 : K , µ , Σ ) . This lets us embed newly observed documents into the low dimensional latent the - matic space that the model represents . We use a fast variational inference algorithm to approximate this posterior , which lets us quickly analyze large document collections under these complicated modeling assumptions . Second , given a collection of documents { w 1 , . . . , w D } , we ﬁnd maximum likelihood estimates of the topics and the underlying logistic normal distri - bution under the modeling assumptions of the CTM . We use a variant of the A CORRELATED TOPIC MODEL OF SCIENCE 7 expectation - maximization algorithm , where the E - step is the per - document posterior inference problem described above . Furthermore , we seek sparse solutions of the inverse covariance matrix between topics , and we adapt ℓ 1 - regularized covariance estimation [ 20 ] for this purpose . 3 . 1 . Posterior inference with variational methods . Given a document w and a model { β 1 : K , µ , Σ } , the posterior distribution of the per - document latent variables is p ( η , z | w , β 1 : K , µ , Σ ) ( 2 ) = p ( η | µ , Σ ) Q N n = 1 p ( z n | η ) p ( w n | z n , β 1 : K ) R p ( η | µ , Σ ) Q Nn = 1 P Kz n = 1 p ( z n | η ) p ( w n | z n , β 1 : K ) d η , which is intractable to compute due to the integral in the denominator , that is , the marginal probability of the document that we are conditioning on . There are two reasons for this intractability . First , the sum over the K val - ues of z n occurs inside the product over words , inducing a combinatorial number of terms . Second , even if K N stays within the realm of compu - tational tractability , the distribution of topic proportions p ( η | µ , Σ ) is not conjugate to the distribution of topic assignments p ( z n | η ) . Thus , we cannot analytically compute the integrals of each term . The nonconjugacy further precludes using many of the Monte Carlo Markov chain ( MCMC ) sampling techniques that have been developed for comput - ing with Dirichlet - based mixed membership models [ 10 , 15 ] . These MCMC methods are all based on Gibbs sampling , where the conjugacy between the latent variables lets us compute coordinate - wise posteriors analytically . To employ MCMC in the logistic normal setting considered here , we have to appeal to a tailored Metropolis – Hastings solution . Such a technique will not enjoy the same convergence properties and speed of the Gibbs samplers , which is particularly hindering for the goal of analyzing collections that comprise millions of words . Thus , to approximate this posterior , we appeal to variational methods as a deterministic alternative to MCMC . The idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distribution is close in Kullback – Leibler divergence to the true posterior [ 17 , 29 ] . The ﬁtted variational distribution is then used as a substi - tute for the posterior , just as the empirical distribution of samples is used in MCMC . Variational methods have had widespread application in machine learning ; their potential in applied Bayesian statistics is beginning to be realized . In models composed of conjugate - exponential family pairs and mixtures , the variational inference algorithm can be automatically derived by comput - ing expectations of natural parameters in the variational distribution [ 5 , 7 , 8 D . M . BLEI AND J . D . LAFFERTY 31 ] . However , the nonconjugate pair of variables in the CTM requires that we derive the variational inference algorithm from ﬁrst principles . We begin by using Jensen’s inequality to bound the log probability of a document , log p ( w 1 : N | µ , Σ , β ) ≥ E q [ log p ( η | µ , Σ ) ] + N X n = 1 E q [ log p ( z n | η ) ] ( 3 ) + N X n = 1 E q [ log p ( w n | z n , β ) ] + H ( q ) , where the expectation is taken with respect to q , a variational distribution of the latent variables , and H ( q ) denotes the entropy of that distribution . As a variational distribution , we use a fully factorized model , where all the variables are independently governed by a diﬀerent distribution , q ( η 1 : K , z 1 : N | λ 1 : K , ν 21 : K , φ 1 : N ) = K Y i = 1 q ( η i | λ i , ν 2 i ) N Y n = 1 q ( z n | φ n ) . ( 4 ) The variational distributions of the discrete topic assignments z 1 : N are spec - iﬁed by the K - dimensional multinomial parameters φ 1 : N ( these are mean parameters of the multinomial ) . The variational distribution of the contin - uous variables η 1 : K are K independent univariate Gaussians { λ i , ν i } . Since the variational parameters are ﬁt using a single observed document w 1 : N , there is no advantage in introducing a nondiagonal variational covariance matrix . The variational inference algorithm optimizes equation ( 3 ) with respect to the variational parameters , thereby tightening the bound on the marginal probability of the observations as much as the structure of variational distri - bution allows . This is equivalent to ﬁnding the variational distribution that minimizes KL ( q | | p ) , where p is the true posterior [ 17 , 29 ] . Details of this optimization for the CTM are given in the Appendix . Note that variational methods do not come with the same theoretical guarantees as MCMC , where the limiting distribution of the chain is ex - actly the posterior of interest . However , variational methods provide fast algorithms and a clear convergence criterion , whereas MCMC methods can be computationally ineﬃcient and determining when a Markov chain has converged is diﬃcult [ 23 ] . 3 . 2 . Parameter estimation . Given a collection of documents , we carry out parameter estimation for the correlated topic model by attempting to maximize the likelihood of a corpus of documents as a function of the topics β 1 : K and the multivariate Gaussian ( µ , Σ ) . A CORRELATED TOPIC MODEL OF SCIENCE 9 As in many latent variable models , we cannot compute the marginal like - lihood of the data because of the latent structure that needs to be marginal - ized out . To address this issue , we use variational expectation – maximization ( EM ) . In the E - step of traditional EM , one computes the posterior distri - bution of the latent variables given the data and current model parameters . In variational EM , we use the variational approximation to the posterior described in the previous section . Note that this is akin to Monte Carlo EM , where the E - step is approximated by a Monte Carlo approximation to the posterior [ 30 ] . Speciﬁcally , the objective function of variational EM is the likelihood bound given by summing equation ( 3 ) over the document collection { w 1 , . . . , w D } , L ( µ , Σ , β 1 : K ; w 1 : D ) ≥ D X d = 1 E q d [ log p ( η d , z d , w d | µ , Σ , β 1 : K ) ] . The variational EM algorithm is coordinate ascent in this objective func - tion . In the E - step , we maximize the bound with respect to the variational parameters by performing variational inference for each document . In the M - step , we maximize the bound with respect to the model parameters . This amounts to maximum likelihood estimation of the topics and multivariate Gaussian using expected suﬃcient statistics , where the expectation is taken with respect to the variational distributions computed in the E - step , b β i ∝ X d φ d , i n d , b µ = 1 D X d λ d , b Σ = 1 D X d Iν 2 d + ( λ d − b µ ) ( λ d − b µ ) T , where n d is the vector of word counts for document d . The E - step and M - step are repeated until the bound on the likelihood converges . In the analysis reported below , we run variational inference until the relative change in the probability bound of equation ( 3 ) is less than 0 . 0001 % , and run variational EM until the relative change in the likelihood bound is less than 0 . 001 % . 3 . 3 . Topic graphs . As seen below , the ability of the CTM to model the correlation between topics yields a better ﬁt of a document collection than LDA . But the covariance of the logistic normal model for topic proportions can also be used to visualize the relationships among the topics . In partic - ular , the covariance matrix can be used to form a topic graph , where the nodes represent individual topics , and neighboring nodes represent highly 10 D . M . BLEI AND J . D . LAFFERTY related topics . In such settings , it is useful to have a mechanism to control the sparsity of the graph . Recall that the graph encoding the independence relations in a Gaus - sian graphical model is speciﬁed by the zero pattern in the inverse covari - ance matrix . More precisely , if X ∼ N ( µ , Σ ) is a K - dimensional multivariate Gaussian , and S = Σ − 1 denotes the inverse covariance matrix , then we form a graph G ( Σ ) = ( V , E ) with vertices V corresponding to the random vari - ables X 1 , . . . , X K and edges E satisfying ( s , t ) ∈ E if and only if S st 6 = 0 . If N ( s ) = { t : ( s , t ) ∈ E } denotes the set of neighbors of s in the graph , then the independence relation X s ⊥ X u | X N ( s ) holds for any node u / ∈ N ( s ) that is not a neighbor of s . Recent work of Meinshausen and B¨uhlmann [ 20 ] shows how the lasso [ 28 ] can be adapted to give an asymptotically consistent estimator of the graph G ( Σ ) . The strategy is to regress each variable X s onto all of the other variables , imposing an ℓ 1 penalty on the parameters to encourage sparsity . The nonzero components then serve as an estimate of the neighbors of s in the graph . In more detail , let κ s = ( κ s 1 , . . . , κ sK ) ∈ R K be the parameters of the lasso ﬁt obtained by regressing X s onto ( X t ) t 6 = s , with the parameter κ ss serving as the unregularized intercept . The optimization problem is b κ s = arg min κ 12 k X s − X \ s κ s k 22 + ρ n k κ \ s k 1 , ( 5 ) where X \ s denotes the set of variables with X s replaced by the vector of all 1’s , and κ \ s denotes the vector κ s with component κ ss removed . The estimated set of neighbors is then c N ( s ) = { t : b κ st 6 = 0 } . ( 6 ) Meinshausen and B¨uhlmann [ 20 ] show that P ( c N ( s ) = N ( s ) ) → 1 as the sample size n increases , for a suitable choice of the regularization parameter ρ n satisfying nρ 2 n − log ( K ) → ∞ . Moreover , the convergence is exponentially fast , and as a consequence , if K = O ( n d ) grows only polynomially with sam - ple size , the estimated graph is the true graph with probability approaching one . To adapt the Meinshausen – B¨uhlmann technique to the CTM , recall that we estimate the covariance matrix Σ using variational EM , where in the M - step we maximize the variational lower bound with respect to approx - imation computed in the E - step . For a given document d , the variational approximation to the posterior of η is a normal with mean λ d ∈ R K . We treat the standardized mean vectors { λ d } as data , and regress each compo - nent onto the others with an ℓ 1 penalty . Two simple procedures can be used to then form the graph edge set , by taking the conjunction or disjunction of A CORRELATED TOPIC MODEL OF SCIENCE 11 the local neighborhood estimates : ( s , t ) ∈ E AND in case t ∈ c N ( s ) and s ∈ c N ( t ) , ( 7 ) ( s , t ) ∈ E OR in case t ∈ c N ( s ) or s ∈ c N ( t ) . ( 8 ) Figure 2 shows an example of a topic graph constructed using this method , with edges E AND formed by intersecting the neighborhood estimates . Vary - ing the regularization parameter ρ n allows control over the sparsity of the graph ; the graph becomes increasingly sparse as ρ n increases . 4 . Analyzing Science . JSTOR is an on - line archive of scholarly journals that scans bound volumes dating back to the 1600s and runs optical char - acter recognition algorithms on the scans . Thus , JSTOR stores and indexes hundreds of millions of pages of noisy text , all searchable through the Inter - net . This is an invaluable resource to scholars . The JSTOR collection provides an opportunity for developing exploratory analysis and useful descriptive statistics of large volumes of text data . As they are , the articles are organized by journal , volume and number . But the users of JSTOR would beneﬁt from a topical organization of articles from diﬀerent journals and automatic recommendations of similar articles to those known to be of interest . In some modern electronic scholarly archives , such as the ArXiv ( http : / / www . arxiv . org / ) , contributors provide meta - data with their manu - scripts that describe and categorize their work to aid in such a topical ex - ploration of the collection . In many text data sets , however , meta - data is unavailable . Moreover , there may be underlying topics and connections be - tween articles that the authors or curators have not determined . To these ends , we analyzed a large portion of JSTOR’s corpus of articles from Science with the CTM . 4 . 1 . Qualitative analysis of Science . In this section we illustrate the pos - sible applications of the CTM to automatic corpus analysis and browsing . We estimated a 100 - topic model on the Science articles from 1990 to 1999 using the variational EM algorithm of Section 3 . 2 . ( C code that implements this algorithm can be found at the ﬁrst author’s web - site and STATLIB . ) The total vocabulary size in this collection is 375 , 144 terms . We trim the 356 , 195 terms that occurred fewer than 70 times as well as 296 stop words , that is , words like “the , ” “but” or “with , ” which do not convey meaning . This yields a corpus of 16 , 351 documents , 19 , 088 unique terms and a total of 5 . 7M words . Using the technique described in Section 3 . 3 , we constructed a sparse graph ( ρ = 0 . 1 ) of the connections between the estimated latent topics . Part of this graph is illustrated in Figure 2 . ( For space , we manually removed 12 D . M . B L E I AN D J . D . L A FF E R T Y Fig . 2 . A portion of the topic graph learned from 16 , 351 OCR articles from Science ( 1990 – 1999 ) . Each topic node is labeled with its ﬁve most probable phrases and has font proportional to its popularity in the corpus . ( Phrases are found by permutation test . ) The full model can be found in http : / / www . cs . cmu . edu / ˜lemur / science / and on STATLIB . A CORRELATED TOPIC MODEL OF SCIENCE 13 topics that occurred very rarely and those that captured nontopical content such as front matter . ) This graph provides a snapshot of ten years of Sci - ence , and reveals diﬀerent substructures of themes in the collection . A user interested in the brain can restrict attention to articles that use the neuro - science topics ; a user interested in genetics can restrict attention to those articles in the cluster of genetics topics . Further structure is revealed at the document level , where each document is associated with a latent vector of topic proportions . The posterior distri - bution of the proportions can be used to associate documents with latent topics . For example , the following are the top ﬁve articles associated with the topic whose most probable vocabulary items are “laser , optical , light , electron , quantum” : 1 . “Vacuum Squeezing of Solids : Macroscopic Quantum States Driven by Light Pulses” ( 1997 ) . 2 . “Superradiant Rayleigh Scattering from a Bose – Einstein Condensate” ( 1999 ) . 3 . “Physics and Device Applications of Optical Microcavities” ( 1992 ) . 4 . “Photon Number Squeezed States in Semiconductor Lasers” ( 1992 ) . 5 . “A Well - Collimated Quasi - Continuous Atom Laser” ( 1999 ) . Moreover , we can use the expected distance between per - document topic proportions to identify other documents that have similar topical content . We use the expected Hellinger distance , which is a symmetric distance be - tween distributions . Consider two documents i and j , E [ d ( θ i , θ j ) ] = E q " X k ( p θ ik − q θ jk ) 2 # = 2 − 2 X k E q [ p θ ik ] E q (cid:20) θ jk p θ jk (cid:21) , where all expectations are taken with respect to the variational posterior distributions ( see Section 3 . 1 ) . One example of this application of the latent variable analysis is illustrated in Figure 3 . The interested reader is invited to visit http : / / www . cs . cmu . edu / ∼ lemur / science / to interactively explore this model , including the topics , their connections , the articles that exhibit them and the expected Hellinger similarity between articles . 4 . 2 . Quantitative comparison to latent Dirichlet allocation . We compared the logistic normal to the Dirichlet by ﬁtting a smaller collection of articles to CTM and LDA models of varying numbers of topics . This collection con - tains the 1 , 452 documents from 1960 ; we used a vocabulary of 5 , 612 words 14 D . M . BLEI AND J . D . LAFFERTY Fig . 3 . Using the Hellinger distance to ﬁnd similar articles to the query article “ Earth ’ s Solid Iron Core May Skew Its Magnetic Field . ” Illustrated are the top three articles by Hellinger distance to the query article and the expected posterior topic proportions for each article . Notice that each document somehow combines geology and physics . after pruning common function words and terms that occur once in the col - lection . Using ten - fold cross validation , we computed the log probability of the held - out data given a model estimated from the remaining data . A bet - ter model of the document collection will assign higher probability to the held out data . To avoid comparing bounds , we used importance sampling to compute the log probability of a document where the ﬁtted variational distribution is the proposal . Figure 4 illustrates the average held out log probability for each model and the average diﬀerence between them . The CTM provides a better ﬁt A CORRELATED TOPIC MODEL OF SCIENCE 15 Fig . 4 . ( Left ) The 10 - fold cross - validated held - out log probability of the 1960 Science corpus , computed by importance sampling . The CTM supports more topics than LDA . See ﬁgure at right for the standard error of the diﬀerence . ( Right ) The mean diﬀerence in held - out log probability . Numbers greater than zero indicate a better ﬁt by the CTM . than LDA and supports more topics ; the likelihood for LDA peaks near 30 topics , while the likelihood for the CTM peaks close to 90 topics . The means and standard errors of the diﬀerence in log - likelihood of the models is shown at right ; this indicates that the CTM always gives a better ﬁt . Another quantitative evaluation of the relative strengths of LDA and the CTM is how well the models predict the remaining words of a document after observing a portion of it . Speciﬁcally , we observe P words from a document and are interested in which model provides a better predictive distribution of the remaining words p ( w | w 1 : P ) . To compare these distributions , we use perplexity , which can be thought of as the eﬀective number of equally likely words according to the model . Mathematically , the perplexity of a word distribution is deﬁned as the inverse of the per - word geometric average of the probability of the observations , Perp ( Φ ) = D Y d = 1 N d Y i = P + 1 p ( w i | Φ , w 1 : P ) ! − 1 / ( P Dd = 1 ( N d − P ) ) , where Φ denotes the model parameters of an LDA or CTM model . Note that lower numbers denote more predictive power . The plot in Figure 5 compares the predictive perplexity under LDA and the CTM for diﬀerent numbers of words randomly observed from the doc - uments . When a small number of words have been observed , there is less uncertainty about the remaining words under the CTM than under LDA— the perplexity is reduced by nearly 200 words , or roughly 10 % . The reason 16 D . M . BLEI AND J . D . LAFFERTY Fig . 5 . ( Left ) The 10 - fold cross - validated predictive perplexity for partially observed held - out documents from the 1960 Science corpus ( K = 50 ) . Lower numbers indicate more predictive power from the CTM . ( Right ) The mean diﬀerence in predictive perplexity . Numbers less than zero indicate better prediction from the CTM . is that after seeing a few words in one topic , the CTM uses topic correlation to infer that words in a related topic may also be probable . In contrast , LDA cannot predict the remaining words as well until a large portion of the document has been observed so that all of its topics are represented . 5 . Summary . We have developed a hierarchical topic model of docu - ments that replaces the Dirichlet distribution of per - document topic pro - portions with a logistic normal . This allows the model to capture correla - tions between the occurrence of latent topics . The resulting correlated topic model gives better predictive performance and uncovers interesting descrip - tive statistics for facilitating browsing and search . Use of the logistic normal , while more complex , may have beneﬁt in the many applications of Dirichlet - based mixed membership models . One issue that we did not thoroughly explore is model selection , that is , choosing the number of topics for a collection . In other topic models , non - parametric Bayesian methods based on the Dirichlet process are a natural suite of tools because they can accommodate new topics as more documents are observed . ( The nonparametric Bayesian version of LDA is exactly the hierarchical Dirichlet process [ 27 ] . ) The logistic normal , however , does not immediately give way to such extensions . Tackling the model selection issue in this setting is an important area of future research . APPENDIX : DETAILS OF VARIATIONAL INFERENCE Variational objective . Before deriving the optimization procedure , we A CORRELATED TOPIC MODEL OF SCIENCE 17 put the objective function equation ( 3 ) in terms of the variational parame - ters . The ﬁrst term is E q [ log p ( η | µ , Σ ) ] ( 9 ) = 1 2 log | Σ − 1 | − K 2 log 2 π − 1 2E q [ ( η − µ ) T Σ − 1 ( η − µ ) ] , where E q [ ( η − µ ) T Σ − 1 ( η − µ ) ] ( 10 ) = Tr ( diag ( ν 2 ) Σ − 1 ) + ( λ − µ ) T Σ − 1 ( λ − µ ) . The nonconjugacy of the logistic normal to multinomial leads to diﬃculty in computing the second term of equation ( 3 ) , the expected log probability of a topic assignment E q [ log p ( z n | η ) ] = E q [ η T z n ] − E q " log K X i = 1 exp { η i } ! # . ( 11 ) To preserve the lower bound on the log probability , we upper bound the negative log normalizer with a Taylor expansion : E q " log K X i = 1 exp { η i } ! # ≤ ζ − 1 K X i = 1 E q [ exp { η i } ] ! − 1 + log ( ζ ) , ( 12 ) where we have introduced a new variational parameter ζ . The expectation E q [ exp { η i } ] is the mean of a log normal distribution with mean and variance obtained from the variational parameters { λ i , ν 2 i } : E q [ exp { η i } ] = exp { λ i + ν 2 i / 2 } for i ∈ { 1 , . . . , K } . This is a simpler approach than the more ﬂexible , but more computationally intensive , method taken in [ 25 ] . Using this additional bound , the second term of equation ( 3 ) is E q [ log p ( z n | η ) ] = K X i = 1 λ i φ n , i − ζ − 1 K X i = 1 exp { λ i + ν 2 i / 2 } ! + 1 − log ζ . ( 13 ) The third term of equation ( 3 ) is E q [ log p ( w n | z n , β ) ] = K X i = 1 φ n , i log β i , w n . ( 14 ) The fourth term is the entropy of the variational distribution : K X i = 1 12 ( log ν 2 i + log 2 π + 1 ) − N X n = 1 k X i = 1 φ n , i log φ n , i . ( 15 ) Note that the additional variational parameter ζ is not needed to compute this entropy . 18 D . M . BLEI AND J . D . LAFFERTY Coordinate ascent optimization . Finally , we maximize the bound in equa - tion ( 3 ) with respect to the variational parameters λ 1 : K , ν 1 : K , φ 1 : N and ζ . We use a coordinate ascent algorithm , iteratively maximizing the bound with respect to each parameter . First , we maximize equation ( 3 ) with respect to ζ , using the second bound in equation ( 12 ) . The derivative with respect to ζ is f ′ ( ζ ) = N ζ − 2 K X i = 1 exp { λ i + ν 2 i / 2 } ! − ζ − 1 ! , ( 16 ) which has a maximum at b ζ = K X i = 1 exp { λ i + ν 2 i / 2 } . ( 17 ) Second , we maximize with respect to φ n . This yields a maximum at b φ n , i ∝ exp { λ i } β i , w n , i ∈ { 1 , . . . , K } , ( 18 ) which is an application of variational inference updates within the exponen - tial family [ 5 , 7 , 31 ] . Third , we maximize with respect to λ i . Equation ( 3 ) is not amenable to analytic maximization . We use the conjugate gradient algorithm with derivative dL / d λ = − Σ − 1 ( λ − µ ) + N X n = 1 φ n , 1 : K − ( N / ζ ) exp { λ + ν 2 / 2 } . ( 19 ) Finally , we maximize with respect to ν 2 i . Again , there is no analytic solution . We use Newton’s method for each coordinate with the constraint that ν i > 0 , dL / dν 2 i = − Σ − 1 ii / 2 − N / 2 ζ exp { λ i + ν 2 i / 2 } + 1 / ( 2 ν 2 i ) . ( 20 ) Iterating between the optimizations of ν , λ , φ and ζ deﬁnes a coordinate ascent algorithm on equation ( 3 ) . ( In practice , we optimize with respect to ζ in between optimizations for ν , λ and φ . ) Though each coordinate’s optimization is convex , the variational objective is not convex with respect to the ensemble of variational parameters . We are only guaranteed to ﬁnd a local maximum , but note that this is still a bound on the log probability of a document . Acknowledgments . We thank two anonymous reviewers for their excel - lent suggestions for improving the paper . We thank JSTOR for providing access to journal material in the JSTOR archive . We thank Jon McAuliﬀe and Nathan Srebro for useful discussions and comments . A preliminary ver - sion of this work appears in [ 9 ] . A CORRELATED TOPIC MODEL OF SCIENCE 19 REFERENCES [ 1 ] Airoldi , E . , Blei , D . , Fienberg , S . and Xing , E . ( 2007 ) . Combining stochastic block models and mixed membership for statistical network analysis . Statistical Network Analysis : Models , Issues and New Directions . Lecture Notes in Comput . Sci . 4503 57 – 74 . Springer , Berlin . [ 2 ] Aitchison , J . ( 1982 ) . The statistical analysis of compositional data ( with discus - sion ) . J . Roy . Statist . Soc . Ser . B 44 139 – 177 . MR0676206 [ 3 ] Aitchison , J . ( 1985 ) . A general class of distributions on the simplex . J . Roy . Statist . Soc . Ser . B 47 136 – 146 . MR0805071 [ 4 ] Aitchison , J . and Shen , S . ( 1980 ) . Logistic normal distributions : Some properties and uses . Biometrika 67 261 – 272 . MR0581723 [ 5 ] Bishop , C . , Spiegelhalter , D . and Winn , J . ( 2003 ) . VIBES : A variational infer - ence engine for Bayesian networks . In Advances in Neural Information Process - ing Systems 15 ( S . Becker , S . Thrun and K . Obermayer , eds . ) 777 – 784 . MIT Press , Cambridge , MA . MR2003168 [ 6 ] Blei , D . and Jordan , M . ( 2003 ) . Modeling annotated data . In Proceedings of the 26th annual International ACM SIGIR Conference on Research and Develop - ment in Information Retrieval 127 – 134 . ACM Press , New York , NY . [ 7 ] Blei , D . and Jordan , M . ( 2005 ) . Variational inference for Dirichlet process mixtures . Journal of Bayesian Analysis 1 121 – 144 . MR2227367 [ 8 ] Blei , D . , Ng , A . and Jordan , M . ( 2003 ) . Latent Dirichlet allocation . Journal of Machine Learning Research 3 993 – 1022 . [ 9 ] Blei , D . M . and Lafferty , J . D . ( 2006 ) . Correlated topic models . In Advances in Neural Information Processing Systems 18 ( Y . Weiss , B . Sch¨olkopf and J . Platt , eds . ) . MIT Press , Cambridge , MA . [ 10 ] Erosheva , E . ( 2002 ) . Grade of membership and latent structure models with appli - cation to disability survey data . Ph . D . thesis , Dept . Statistics , Carnegie Mellon Univ . [ 11 ] Erosheva , E . , Fienberg , S . and Joutard , C . ( 2007 ) . Describing disability through individual - level mixture models for multivariate binary data . Ann . Appl . Statist . To appear . [ 12 ] Erosheva , E . , Fienberg , S . and Lafferty , J . ( 2004 ) . Mixed - membership models of scientiﬁc publications . Proc . Natl . Acad . Sci . USA 97 11885 – 11892 . [ 13 ] Fei - Fei , L . and Perona , P . ( 2005 ) . A Bayesian hierarchical model for learning natural scene categories . IEEE Computer Vision and Pattern Recognition 2 524 – 531 . [ 14 ] Girolami , M . and Kaban , A . ( 2004 ) . Simplicial mixtures of Markov chains : Dis - tributed modelling of dynamic user proﬁles . In Advances in Neural Information Procesing Systems 16 9 – 16 . MIT Press , Cambridge , MA . [ 15 ] Griffiths , T . and Steyvers , M . ( 2004 ) . Finding scientiﬁc topics . Proc . Natl . Acad . Sci . USA 101 5228 – 5235 . [ 16 ] Griffiths , T . , Steyvers , M . , Blei , D . and Tenenbaum , J . ( 2005 ) . Integrating topics and syntax . In Advances in Neural Information Processing Systems 17 537 – 544 . MIT Press , Cambridge , MA . [ 17 ] Jordan , M . , Ghahramani , Z . , Jaakkola , T . and Saul , L . ( 1999 ) . Introduction to variational methods for graphical models . Machine Learning 37 183 – 233 . [ 18 ] Marlin , B . ( 2004 ) . Collaborative ﬁltering : A machine learning perspective . Master’s thesis , Univ . Toronto . [ 19 ] McCallum , A . , Corrada - Emmanuel , A . and Wang , X . ( 2004 ) . The author – recipient – topic model for topic and role discovery in social networks : Experi - 20 D . M . BLEI AND J . D . LAFFERTY ments with Enron and academic email . Technical report , Univ . Massachusetts , Amherst . [ 20 ] Meinshausen , N . and B¨uhlmann , P . ( 2006 ) . High dimensional graphs and variable selection with the lasso . Ann . Statist . 34 1436 – 1462 . MR2278363 [ 21 ] Mosteller , F . and Wallace , D . L . ( 1964 ) . Inference and Disputed Authorship : The Federalist . Addison - Wesley , Reading , MA . MR0175668 [ 22 ] Pritchard , J . , Stephens , M . and Donnelly , P . ( 2000 ) . Inference of population structure using multilocus genotype data . Genetics 155 945 – 959 . [ 23 ] Robert , C . and Casella , G . ( 2004 ) . Monte Carlo Statistical Methods , 2nd ed . Springer , New York . MR2080278 [ 24 ] Rosen - Zvi , M . , Griffiths , T . , Steyvers , M . and Smith , P . ( 2004 ) . The author - topic model for authors and documents . In AUAI ’ 04 : Proceedings of the 20th Conference on Uncertainty in Artiﬁcial Intelligence 487 – 494 . AUAI Press , Ar - lington , VA . [ 25 ] Saul , L . , Jaakkola , T . and Jordan , M . ( 1996 ) . Mean ﬁeld theory for sigmoid belief networks . Journal of Artiﬁcial Intelligence Research 4 61 – 76 . [ 26 ] Sivic , J . , Rusell , B . , Efros , A . , Zisserman , A . and Freeman , W . ( 2005 ) . Dis - covering object categories in image collections . Technical report , CSAIL , Mas - sachusetts Institute of Technology . [ 27 ] Teh , Y . , Jordan , M . , Beal , M . and Blei , D . ( 2007 ) . Hierarchical Dirichlet pro - cesses . J . Amer . Statist . Assoc . 101 1566 – 1581 . [ 28 ] Tibshirani , R . ( 1996 ) . Regression shrinkage and selection via the lasso . J . R . Statist . Soc . Ser . B Stat . Methodol . 58 267 – 288 . MR1379242 [ 29 ] Wainwright , M . and Jordan , M . ( 2003 ) . Graphical models , exponential families , and variational inference . Technical Report 649 , Dept . Statistics , U . C . Berkeley . [ 30 ] Wei , G . and Tanner , M . ( 1990 ) . A Monte Carlo implementation of the EM al - gorithm and the poor man’s data augmentation algorithms . J . Amer . Statist . Assoc . 85 699 – 704 . [ 31 ] Xing , E . , Jordan , M . and Russell , S . ( 2003 ) . A generalized mean ﬁeld algorithm for variational inference in exponential families . In Proceedings of UAI 583 – 591 . Morgan Kaufmann , San Francisco , CA . Computer Science Department Princeton University Princeton , New Jersey 08540 USA E - mail : blei @ cs . princeton . edu Computer Science Department Machine Learning Department Carnegie Mellon University Pittsburgh , Pennsylvania 15213 USAE - mail : laﬀerty @ cs . cmu . edu