Follow the ( Slash ) dot : Effects of Feedback on New Members in an Online Community Cliff Lampe , Erik Johnston University of Michigan 1075 Beal Ave . Ann Arbor , MI 48109 - 2112 cacl @ umich . edu , erikwj @ umich . edu ABSTRACT Many virtual communities involve ongoing discussions , with large numbers of users and established , if implicit rules for participation . As new users enter communities like this , both they and existing members benefit when new users learn the standards for participation . Slashdot is a news and discussion site that has developed a system of distributed moderation to provide feedback about the value of posts on their site . This study examines three explanations for how new users learn to participate in a digital community : learning transfer from previous experiences , observation of other members , and feedback from other members . We find that new user behavior is affected by a combination of their viewing behavior , the moderation feedback they receive , and replies to their comments . Categories and Subject Descriptors H . 5 . 3 Information Interfaces and Presentation , Group and Organization interfaces , Asynchronous Communication General Terms Human Factors , Design Keywords Online community , learning , rating systems , feedback 1 . INTRODUCTION According to a recent report by the Pew Internet and American Life Project [ 15 ] , 25 % of Internet users in the United States participate in online chat rooms or discussions , a number which has grown over recent years . A person entering an online discussion is often joining a mature social system with existing members and a developed sense of how members behave . The entrance of new members into established discussions may be both potentially beneficial and potentially harmful to the operation of the online forum . New members may provide additional energy and ideas to persistent digital communities . However , the textual nature of most online discussions , and heterogeneity of online forums across the web , may make it difficult for new members to detect rules for how to behave . When new users have trouble conforming to discussion standards they may increase information overload for the entire community and become more vulnerable to a wide variety of deception from misbehaving users . Many methods have been used to socialize new users in an online space . Frequently Asked Question ( FAQ ) documents often explicitly state a community’s values and procedures . In some spaces new users must wait before being able to contribute content to ensure that they have time to observe the normal methods and types of participation . Some digital communities have sections allocated specifically for new users , often referred to as “newbie gardens” . Online role - playing games often provide spaces like this for new users to learn system commands and interaction standards free of harassment from more experienced players . Another common method of socializing new users is to provide direct mentorship from more experienced community members . A further method of shaping new user behavior is the use of feedback provided by the larger community , often in the form of rating systems that provide evaluations of new contributions . Slashdot is an online news and discussion site with a large , persistent membership . Founded in 1997 , Slashdot has developed a complex system of rules and cultural values , which may be difficult for the new member to perceive and conform to as they enter the site . Around 250 new users per day create accounts on the site . Slashdot has developed a system of distributed moderation by which experienced members of the site provide feedback in the form of ratings about the quality of comments posted to its discussion forums . Besides providing information about the quality of posts , this rating system may act as a shaping mechanism by which experienced members transmit the standards of posting behavior to new members . This research examines the role different mechanisms for learning might have in shaping new user behavior . Are users coming to an online community with all the skills they need to post highly rated comments ? Do new users observe others to determine how to write comments ? What role does feedback from other members play in shaping new user posting behavior ? 1 . 1 Effects as new users join online groups Online groups that are successful attract new members on an ongoing basis . Internet users interested in online discussion seek out groups that provide the maximum benefit for their investment of time and effort . Joining persistent , large groups makes sense to the new member as they are able to see a wider array of viewpoints , and have their own messages viewed by more people [ 6 , 17 ] . A new member might observe the site as a passive participant before deciding to submit comments . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . GROUP’05 , November 6 – 9 , 2005 , Sanibel Island , Florida , USA . Copyright 2005 ACM 1 - 59593 - 223 - 2 / 05 / 0011 . . . $ 5 . 00 . Established discussion spaces also benefit from having new members . New participants refresh interest and activity on a site [ 19 ] . New members can also replace users who have left the site for various reasons , keeping critical mass [ 12 ] . The existence of low barriers to entry and exit in most online discussion sites means that membership remains in a constant state of flux , which has the benefits of eliciting new viewpoints , renewing commitment , and maintaining a healthy size population . However , there are some problems that can occur when new members enter established communities . 1 . 1 . 1 New members may increase information overload People have limited ability to perceive and process information [ 16 ] , as well as limited attention spans . The propensity of the online environment to create information overload was discussed by Hiltz and Turoff [ 7 ] , who recommended designing computer mediated communication ( CMC ) systems specifically to reduce overload , including such elements as voting , moderation and sanctioning of anonymous members . Jones et al . [ 11 ] examined Usenet newsgroups and found that users are more likely to respond to simpler messages in situations of overload ; that users will end participation as overload increases and that users generate simpler responses as overload increases . Previously , Jones and Rafaeli [ 10 ] proposed that communication online takes an S - shaped pattern of frequency of occurrence . Early in the existence of a conversation space , or “virtual public” to use their term , there is a struggle to achieve critical mass of people contributing to the conversation . A sharp increase after that critical mass is achieved results in information overload , and communication levels off as participants are discouraged by the rate of messages . Butler [ 2 ] similarly found that more active listserv’s not only had more users entering the discussion , but that they lost users at a greater rate than smaller structures . 1 . 1 . 2 New members my violate norms In offline communities , new members often learn how to behave by following the nonverbal cues of fellow participants , an ability which is often lost in the CMC context [ 24 ] . Online discussion spaces often have well - developed standards of behavior for how to proceed with conversation , including what constitutes a good post , how often one should post , and how to interact with other members . Violations of these norms can lead to misunderstanding , flame wars and other types of social breakdowns that occur in online communication . Many online spaces have a vocabulary that is specific to that venue [ 3 ] . Ignorance of special terms and requests for clarification can derail conversation and irritate experienced members . It is often difficult to tell when a breach of etiquette is the result of innocent ignorance from a new user , or willful misbehavior . The digital community benefits when new members learn rules quickly . Less time needs to be spent by experienced members in explaining terms and expectations [ 26 ] , which leads to more attention placed on discussion , the central activity of the community . 1 . 1 . 3 New members may be vulnerable to deception Many types of misbehavior that take place in virtual public spheres specifically target new users . “Trolling” is posting a comment designed to trick people into aggravated responses . “The well - constructed troll is a post that induces lots of newbies and flamers to make themselves look even more clueless than they already do , while subtly conveying to the more savvy and experienced that it is in fact a deliberate troll . If you don ' t fall for the joke , you get to be in on it . [ 21 ] ” . Trolling often takes advantage of new user naïveté to elicit angry responses . In Multi - User Dungeons ( MUDs ) and Massively Multi - Player Online Roleplaying Games ( MMORPGs ) a category of player specifically targets new players , killing their carefully crafted characters [ 22 ] . In response , these spaces have protected new members by leaving all players immune to harassment or by creating “newbie gardens” , areas where new members can operate safely . This form of protection is not commonly available in other types of digital communities , where new user contributions are often immediately compared with experienced members of the site . 1 . 1 . 4 New members may be ignored If the new member receives no attention from the community , they are likely to abandon the space for not appreciating them appropriately . New members who submit contributions to an online discussion have a hope that other members of the community will value their contribution . New members might expect to feel that their contribution is worthwhile and the discussion is worth their effort if their comment receives attention in the form of replies . Appropriate attention from other community members is likely to lead to future participation . 1 . 2 Variables affecting participation outcomes To understand how new users learn how to participate appropriately for their roles in online communities we examine several methods for learning how to properly contribute in a new community . • Previous Experience o The new member has skills developed prior to joining the site either through formal education , or participation in similar forums with complementary standards . • Observation o The new member observes successful , experienced members and emulates them . • Feedback o The new member participates by posting a comment and their future contributions are shaped through direct feedback from other members through moderation and discussion . The importance of previous experience in determining new user participation is based on the theory of learning transfer [ 18 ] , which argues that learning in one context enhances related performance in another context . Observation is one component of the theory of situated learning [ 14 ] , where new members observe experienced members before starting their own participation . Learning through participation and feedback is grounded in behaviorist theories [ 8 ] , which claims that people can be shaped by feedback to learn new tasks . We believe that each of these play a unique role in the learning process and by identifying the contribution of each , we will be able to make design recommendations that take advantage of the unique qualities of each of them . 1 . 2 . 1 Transfer of learning from other digital communities It is possible that there is a universal standard for posting in online discussions , and that learning to post valued comments in , say , Usenet groups transfers to Web discussion boards . Several guides of “netiquette” are available [ 3 ] , and users participate in multiple discussion spaces at the same time [ 15 ] . It is unlikely that participants enter each virtual community tabula rasa , but rather transfer skills learned in other fora . This type of near transfer suggests that learning from a separate context enhances the ability to perform in a new context [ 18 ] . 1 . 2 . 2 Situated learning through observation of successful participants Many theories of learning point to the importance of observing others engaged in similar behaviors . In a classic study , Bandura [ 1 ] found that children who observed adults attacking a doll were more likely to engage in that behavior than children who did not observe the behavior . In small groups , members often decide how to behave based on the actions of authority members of that group [ 4 ] . Lave and Wenger [ 14 ] use the idea of apprenticeship to explain the different structure of learning in communities of practice . Being an apprentice is not the same as being a pupil . Apprenticeship for Lave here is learning as a peripheral participant that is learning to become a member of a community of practice through increasingly involved participation . At the outset , new members participate simply by observing , learning the values and practices of the community before attempting to use them . One starts out at the periphery by observing , but as one becomes a more central participant , feedback from other participants becomes increasingly important . In a study of lurkers , Preece et al [ 20 ] found that one of the main reasons given for remaining inactive was that the lurkers were observing the group to learn more about it . Many digital communities also include pages of “Frequently Asked Questions” ( FAQs ) that are intended to provide explicit guidelines on community expectations . Often , standards for behavior are more implicit , and must be discovered by the new participant . 1 . 2 . 3 Feedback from experienced users Finally , there are many ways in which new members of a discussion space may receive direct feedback from experienced participants . Feedback has been shown to affect behavior depending on a variety of factors , including the perceived legitimacy of the feedback presenter , the ability of the recipient to understand the feedback and immediacy of the feedback given [ 8 ] . Occasionally , an online discussion space will indicate how many times a message has been read . New users may also receive feedback from existing members that indicates their contributions were valued . Two ways of providing feedback are examined in this study . First , in some online communities users provide direct feedback in the form of ratings for contributions , assigning a numerical value to comments or rating a comment up or down from its current ranting . Secondly , replies to a comment indicate to the author of the parent comment that they’ve not only been read , but that someone was affected enough by their comment to post a reply [ 23 , 26 ] . 1 . 3 Research questions To tie together participation outcomes for new users with alternate explanations for how they learn to participate , we generated several research questions , which we then attempt to answer using data from an active online community . Q1 : How do new users behave when they first enter an established online community ? Q1a : How do users react differently to different types of attention from other users ? Q2 : Is there a gap between how well new users think they understand valued participation , and how they are actually rated by other community members ? Q2a : Is this potential gap associated with their previous experience ? Q3 : How are measures of learning transfer , observation and feedback related to participation outcomes for new users ? Q3a : How is previous experience related to the first contribution a new user makes ? Q3b : How are learning measures related to how comments are valued by other users ? Q3c : How are learning measures related to whether new users post comments , and the rate at which they post ? 2 . Methods We studied users of a popular discussion site that uses a comment rating system to determine the relationship between these three methods of new user learning and initial participation outcomes . This section describes the digital community we studied , data collected from that site , and a description of how the data is being used to describe the associations articulated above . 2 . 1 Slashdot : News for nerds . Stuff that Matters Slashdot 1 is a news and commentary site dedicated to technology issues , especially open source software . It attracts about a third of a million unique users each day . Figure 1 : The index page of Slashdot . As stories are posted , users of the site may comment on those stories . Each story typically engenders several hundred comments , with some stories resulting in over a thousand comments . After a comment has been posted , it can then be rated by another user with moderator eligibility . Slashdot users achieve moderator eligibility by having a positive reputation , which 1 http : / / slashdot . org results from their own positive participation in the site . In the span of one week in September 2004 , 67 , 000 registered users were eligible to rate comments and about 5 , 000 per day were selected . When selected , a user is given five moderation points , to be used within three days . Each posted comment message has a current score , from – 1 to + 5 . Upon reading a comment , a moderator can expend a point in order to raise or lower the comment’s score by 1 . Users choose from a list of descriptors for the comments , such as “Off - topic” , “Troll” , “Insightful” , “Funny” , or “Overrated” , with each comment type carrying with it an inherent - 1 or + 1 moderation . Slashdot is a large , active digital community with a strongly developed culture . This culture is expressed in special terms used by Slashdot , like calling anonymous posters “anonymous cowards” ; in - jokes that Slashdot participants share , and elements of the Slashdot interface being used in the comments themselves , for example a signature file including admonitions that anyone disagrees with the poster is “ - 5 : Wrong” , which references the Slashdot moderation system . The structure of the site has accreted over time to respond to changing user needs . New members entering the site receive feedback from moderators , but it is unclear how that feedback encourages or discourages participation . 2 . 2 Data collection Data collection included an analysis of server logs of 11 , 079 new users who made accounts on Slashdot between November 1 , 2004 and December 6 , 2004 . Whenever a Slashdot user loads a page , posts a comment , or rates another’s comment , a record of the interaction is kept on the server log . These records are associated with specific users , stories , and times , allowing us to compare interactions between users . Also , logs of user characteristics , including account creation date and reputation score as of December 6 , 2004 were gathered . Slashdot users may participate anonymously on the site without registering . Consequently , some portion of new users identified with this study had experience with the site prior to creating an account . Our assumption is that this is an independent error term , not correlated with any of our independent measures , and consequently omitting it has no effect on our analysis . Other server information related to new users included logs of all moderations that took place during the study period , including ratings of new user comments . Additionally , we collected data on all comments made during the same time frame to compare new user activity with other users during the same time period . Besides the log analysis , we conducted surveys with 233 users who had created their accounts since November 1 , 2004 . Respondents were recruited by an invitation to participate that appeared on Slashdot’s homepage in late November . Survey respondents were tracked with a unique identifier that allows responses to be matched with log data . Survey invitations appeared for only a two day period , during which time only 3 , 341 users identified in the dataset visited the site . The overall response rate for the study was 8 % . We used the account creation date to identify new users . While it is possible to have multiple accounts , or post without creating an account , the culture of Slashdot discourages both of these behaviors . Since individuals can create multiple accounts on Slashdot , we also matched user IP addresses to see if new users were experienced users with new accounts . We asked in the survey if the user has more than one account on Slashdot . In the server logs we found several instances of multiple accounts from the same IP address which we excluded from the study . No survey respondents reported other accounts or had IP addresses that matched those of other user accounts . Before examining individual user outcomes , we examined overall participation rates on Slashdot . Out of 11 , 079 new users selected for study , 1763 users ( 16 % ) made 6467 comments . Of new users who commented , 55 . 1 % made only one comment . The maximum was 248 comments . Of those who made any comments , the mean was 3 . 7 , and the median was 1 comment . New users who commented had a median of 28 minutes elapse between the creation of their account and the posting of their first comment . As has been found in other online settings [ 9 ] , drop out rates of new users are high . To get a sense of how many people abandon their accounts on Slashdot , we studied the 1000 users of the site who had created their accounts between November 1 , 2004 and November 10 , 2004 . Of these users , 5 % had visited only one Slashdot page by December 6 , 2004 . 25 % of this sample only looked at 10 or fewer pages . The median number of site pages loaded by a new user was 39 , and the mean number was 101 . The maximum number of pages loaded by one of these users was 4035 , with 27 . 5 % of users viewing more than 100 pages over the study period . 2 . 3 Measuring alternate explanations of learning As mentioned above , we have focused on three explanations for how new users learn to participate in an environment like Slashdot : learning transfer from previous experience , observation of other members , and feedback from other members . To approximate previous experiences with commenting , we asked news users questions in the online survey about their experience in online forums , self - rated computer expertise and education level . We asked survey respondents to rate their own experience in discussion sites other than Slashdot on a scale from 1 to 7 . On average , respondents scored a 4 . 3 out of 7 for experience in other sites , with a high percentage in the highest category . We also asked respondents to rate their own expertise with computers . Respondents rated themselves as very expert with computers , an average of 5 . 97 on a scale of 1 to 7 . Survey respondents were also asked about their level of education . Over 50 % of respondents reported having a college degree or graduate degree , with an additional 34 % claiming some level of college experience . Observation behavior is captured in two ways . First , the amount of time between when a user creates their account and when they post their first comment is collected . While some exceptions will exist , it could be that the longer the time spent between these two events indicates more time viewing the site . Since the exceptions to this might be significant , we also collected page request information for users . An error in logging prevented timestamp information from being collected , so we only know overall page requests during the study period . Using this , we divide users into low frequency and high frequency site viewers . User feedback is captured in two ways . Each comment from our new users begins with the default score of + 1 , which can change based on moderations from other users . We collected the final scores . Additionally , we mark the number of replies a comment receives as a form of feedback , as it indicates to the new user that their comment was not only read , but regarded enough by other users to engender a reply . Table 1 summarizes the measures of learning described above . Measures in dark gray are associated with learning transfer from previous experiences , those in light gray with observing other users , and those in white with feedback from other users . 2 . 4 New user participation outcomes There are several participation outcomes for new users that we use as estimates of their integration into the Slashdot forum . We developed three indicators of desirable new user participation on the site : scores of comments written by the new user , the rate at which comments are posted , and the overall number of comments made . The scores of comments accumulated through the distributed moderation system act as one measure that the contributor is valued by the community . The higher the average score of comments posted by a user , the more likely that user is valued . For this work , we look at the score of the first comments users make , as well as the average scores of their comment . First comment scores are important because they show whether the new user starts as a highly valued participant , or if there is an opportunity to shape their participation . The amount of time that a user waits between posts may also indicate desirable participation . In particular , delays between the first comment a user makes and the second may indicate they were turned off by the response to the first comment , while less delay may indicate that the user was drawn into the system . Another measure of successful participation is the number of comments they post during the study period . It is beneficial to have members post comments , although too many posts may indicate problematic participation . Table 2 summarizes measures of participation outcomes for new Slashdot users included in this study . 3 . Results The results of our analysis of new user behavior are structured to attempt to answer the research questions raised earlier . The first section describes initial user contributions , and how participation differs among users who receive different scores on their first comments . The second section addresses user beliefs about their ability to create valued messages and how their actual scores do or do not match that perception . The third section analyzes participation outcomes in terms of the measures of alternative learning methods described above . 3 . 1 How do new users behave when they first come to Slashdot ? To describe more fully how initial moderation affects participation , we examined posting patterns for the first three comments made by users . How many users start with negative ratings , yet go on to future success ? Which types of moderation are associated with users ceasing to post comments ? Figure 2 shows the moderation outcome of the first three comments new users made in the dataset being studied . In the chart , the possible outcomes are “Up” for when a user receives positive feedback through the rating system , “None” for when the moderation receives no ratings , “Down” for when the comment receives negative feedback through moderation and “Out” for when the user does not make additional comments . New users who received no moderation were less likely to make a second comment than users who received either positive or negative initial feedback through moderation . Even when a user receives feedback on their first comment , lack of feedback on the second is associated with approximately 30 % of users to ceasing commenting . There is some indication that receiving two negative moderations in a row make it unlikely that a user will receive a positive moderation . Though the numbers in this analysis are too low to be certain of the pattern , each path followed to the 5 th comment show no occasions where two down ratings were followed by a future up rating . Variable Explanation Online experience Survey measure of how much experience users felt they had in online discussion forums . Computer expertise Survey measure of how expert the respondent felt they were with computers . Education level Survey measure of the last educational degree the respondent received . Observation time Time between when a new user creates an account and time they post their first comment . Hit frequency How frequently the new user posts page views from the site . Score The score a user’s comments receive through moderation . Replies The number of replies a user’s comments receive from other users . Table 1 : Summary of measures used to detect different types of learning . Measure Explanation First score Score that a new user’s first comment receives through moderation . Probability of second comment The probability that a user will post a second comment after having posted a first . Time to post second comment Time it takes a user to post a second comment after posting a first Score change Difference between the scores a new user’s first and second comments receive . Number of comments Total number of comments a user made over the study period . Table 2 : Summary of measures of participation outcomes Another interesting pattern is the recovery of users whose first comment was rated negatively . In those cases where the second comment received positive rating , 4 out of the 5 cases where there were third comments were also rated positively , and none of them received a negative rating on their third comment . However , there is also a finding that some people that receive initial negative feedback continue to make comments that are commented negatively . This propensity increases at each level suggesting that negative feedback is the goal of some users , or alternatively that the user pool at this level only contains those contributors who are unable to write valued comments . These descriptions of moderation outcomes indicate that some change is happening to users between the times they post comments . The direction of moderation does seem to have some relationship with how future comments will be rated , but with this data it is unwise to make a strong causal claim . It does seem possible given these patterns that Slashdot participants are using the moderation system as a sounding board to craft their future comments . This is consistent with Goffman’s argument [ 5 ] that individuals acting in a public place are “performing” for some perceived audience . In the Slashdot case , users might be using the feedback provided through moderation to adjust their performance . 3 . 1 . 1 New users believe they can detect a good comment . One sign that new members of Slashdot are detecting the values of a discussion space is that they agree about what makes a comment highly valued . In Lampe and Resnick [ 13 ] we found that moderators on the site widely agree on what constitutes a good comment , as measured by their agreement on the score of a comment . To analyze whether new users similarly agree that they can detect a good comment , we asked several survey questions about their confidence in their ability to detect a highly rated comment . 109 of the 233 survey respondents had not made any comments since creating their user accounts . We checked responses between commenters and non - commenters and found no significant differences between their responses . Most new Slashdot members felt that they could readily detect expectations for posting a good comment . Means reported below are on a scale of 1 - 5 , where 1 indicates low agreement and 5 indicates high agreement . In four questions related to confidence in their ability to detect valued comments on the site , users strongly agreed that they knew what a good comment was ( x = 3 . 95 , C . I . 3 . 83 ≤ x ≤ 4 . 07 ) , could tell why a comment had received the score it did ( x = 3 . 72 , C . I . 3 . 59 ≤ x ≤ 3 . 85 ) , felt the expectations for writing a good comment were clear ( x = 3 . 43 , C . I . 3 . 29 ≤ x ≤ 3 . 58 ) and could write a comment that would be highly scored ( x = 3 . 66 , C . I . 3 . 51 ≤ x ≤ 3 . 81 ) . In addition , the values for the purpose of the moderation system were strongly shared amongst new users , who agreed strongly that moderation should be used to promote well written contributions ( x = 4 . 24 , C . I . 4 . 12 ≤ x ≤ 4 . 36 ) as opposed to supporting particular viewpoints ( x = 1 . 96 , C . I . 1 . 80 ≤ x ≤ 2 . 12 ) . New members also strongly agreed when asked if discussion on Slashdot was worthwhile compared to other sites ( x = 4 . 10 , C . I . 3 . 97 ≤ x ≤ 4 . 23 ) , and that the moderation system is important in fostering discussion on Slashdot ( x = 4 . 19 , C . I . 4 . 05 ≤ x ≤ 4 . 32 ) . The strong agreement on the questions reported above seems to indicate that new members of Slashdot believe they know what constitutes a good comment . To determine whether a user’s impressions of how well they understand how to write a valued comment is related to their eventual ability to write a valued comment , we used the Spearman’s rho statistic to correlate survey responses with the average score of comments a user received . Spearman’s rho is a measure of relationship between ordinal variables , and consequently a better choice here than the more common Pearson’s correlation statistic . The average score users’ Figure 2 : A diagram of posting outcomes based on moderation . Comments are rated up , down , or receive no moderation . The top row represents first comments , second row second comments , and so on . Numbers in boxes represent percentage of the previous row with that particular outcome . comments received was poorly correlated with whether the user reported they were confident they could write a comment that would be rated highly ( r = 0 . 14 , p < 0 . 15 , n = 113 ) , whether they felt the expectations for highly rated comments was clear ( r = 0 . 09 , p < 0 . 32 , n = 114 ) , whether they felt they understood why a comment received the score it did ( r = 0 . 10 , p < 0 . 29 , n = 120 ) or how concerned the user was with the scores their comments receive ( r = 0 . 03 , p < 0 . 72 , n = 120 ) . Even though Slashdot users widely agreed that they knew what constituted a highly rated comment on the site , that belief does not seem to be associated with actual production of highly rated comments . This could be because Slashdot users actually have very different opinions about what would constitute a highly rated comment , and only believe that other agree with them . It could also be that some users are creating comments they know are not going to be highly rated . 3 . 2 Mechanisms that affect user contributions In this section we examine first comment scores , whether and how quickly a second comment is posted , number 3 . 2 . 1 First comment scores When a new user creates their first comment , they have not had the opportunity to benefit from direct community feedback , but they may be affected by learning they bring from other experiences , or by observing the site prior to posting a first comment Table 3 reports an ordinary least squares regression predicting the initial score a comment will receive based on measures of previous experience and observation . This model shows that the measures of previous experience and observation of other users poorly predict the how the first comment made by a new user on the site will be rated by others . It could be that the measures of experience and observation do not adequately represent their real values . Another explanation is that users entering Slashdot for the first time share many characteristics , including ability to write comments with little difference between users . To assess possible explanations for these findings , we also analyzed the different independent variables separately with the score of the first comment . We did not find any significant relationships between any of the independent variables individually and the score of a user’s first comment . 3 . 2 . 2 Likelihood of posting a second comment Many things may happen after a user has posted their first comment . The comment may be rated positively negatively by other members of the discussion board . Some users may decide to reply to the comment . For some users , their first comment may be completely ignored . As mentioned above , 55 . 1 % of new users on Slashdot made only one comment to the site during the study period . As shown in Figure 2 , what happens to the first comment a user makes seems to have some relationship with whether they will post a second comment or not . What predicts whether the new user will post a second comment ? Of the different outcomes for a comment , do any of them predict that a second comment will follow ? Table 4 reports a logistic regression predicting the binary outcome of whether a new user will post a second comment : positive coefficients indicate higher probabilities . Whether the first comment a new user makes , and whether that comment was rated through the moderation system appear to be poor predictors of whether a user will post a second comment . Time between creating an account and posting the first comment , as well as how heavily the user requested page views were better predictors of the likelihood of posting a second comment . However , the R - squared value of this model indicates that many important factors that predict posting a second comment are not being accounted for . 3 . 2 . 3 Time to post second comment The gap in time between when a user posts their first comment and when they post their second may be an important indicator of socialization . If the user has a negative first experience , it may take them longer to post again . Consequently , even if feedback does not affect whether a user will post a second comment , it might affect how they do so . The average time between first and second post for those users who made a second comment was 2 . 6 days , and the median time was 5 . 7 hours . This disparity is caused by some outlying users who had large amounts of time between their first and second posts . Time to post a second comment was not strongly correlated with online forum experience ( r = - 0 . 20 , n = 88 ) , computer experience ( r = - 0 . 18 , n = 87 ) or education level ( r = - 0 . 13 , n = 87 ) . For measures of observation , how frequently the user requests page views was not related to time lag between first and second comments ( r = 0 . 04 , n = 392 ) . The amount of time users spent observing the site before posting their first comment also had only a weak R - square 0 . 06 df 5 , 61 Coef . t P > | t | Constant . 826 0 . 753 . 454 Forum experience - 0 . 055 - 0 . 560 . 578 Computer expertise 0 . 066 0 . 364 . 717 Education level 0 . 173 1 . 207 . 232 Page views 0 . 000 0 . 996 . 323 Observation time 0 . 000 - 0 . 866 . 390 Table 3 : Ordinary least squares regression predicting score of first user comment Pseudo R - squared 0 . 14 n 1704 Coef . Z P > | z | 1 st com replied to - 0 . 563 1 . 32 . 251 1 st comment was rated - 0 . 198 0 . 97 . 325 Time between acct creation and first post 0 . 001 9 . 21 . 002 Frequency of page views 0 . 005 37 . 29 . 001 Constant . 720 2 . 08 . 001 Table 4 : Logistic regression predicting if a new user will post a second question relationship between the time difference in posting first and second comments ( r = 0 . 09 , n = 780 ) . Table 5 shows measures of user feedback in terms of whether the first comment a new user posted was rated up , down or ignored , as well as whether that comment was replied to . Using these different first comment outcomes as separate groups , we measure the median time difference between posting the first and second comments in terms of different first comment outcomes . Users who received no rating on their first comments also took much longer to post a second comment . Users who received a negative rating on their first comment were the quickest to post a second comment . This could result from several factors . One might be that negative attention in the form of negative ratings causes users to want to prove themselves as positive contributors . It could also be that some of these users are writing inflammatory content , which they post more often and is rated negatively . Table 6 shows the multivariate explanations that predict how many minutes pass between posting a first and second comment . Survey measures are excluded from the model as they were found to be unassociated with time in the univariate analysis . The amount of time between account creation and posting a first comment minorly reduces the time to post a second comment . The major factor seems to be whether the first comment a new user writes is rated up , which reduces the overall time to post a second comment . Users who received replies to their first comment took less than a third of the time than those who did not to post a second comment , though this difference did not appear to be a significant factor in the regression model . The strong difference in users who are initially rated down was also revised by the multivariate analysis . It does seem that positive feedback from other users through ratings does reduce the overall time to post a second comment . However , this model also has a low r - squared value , indicating that many variables important to this measure are not included . It is likely that one of these missing variables would help explain the difference in findings about the importance of replies in the model versus the univariate analysis . 3 . 2 . 4 Score of second comment Another measure that a difference nonrandomly occurs between a user’s first and second comments to the site is the difference in scores between the first comment posted by the user and the second . Table 7 shows an ordinary least squares regression predicting the change in score from the first comment to the second . This model shows that if the first comment receives a positive result , it has a negative effect on the score of the second comment , and vice versa if the first comment received a negative score . Whether the first comment was replied to does not seem to be a factor , though number of page views has a small , but significant effect on the score of the second comment . The findings for the reverse roles of initial up and down may be a result of regression towards the mean , i . e . that initial scores have an element of randomness , and second scores correct the arbitrary high and low scores of the first comment . This is supported by the relatively weak correlation between the scores of the first and second comments a user makes ( r = 0 . 174 , p < . 001 , n = 792 ) . 3 . 2 . 5 Number of comments posted Although it can be dangerous to have too many comments posted , having new users create comments is a measure that they are involved in the digital community . Consequently , we looked at the overall number of comments new users made during the study period , and which factors were associated with frequency of posting . New users made an average of 26 comments over the study period , with a median of 9 comments . 15 % of new users only made one comment , and one user was responsible for 248 comments . The overall number of comments posted was not strongly correlated with online forum experience ( r = 0 . 17 , n = 124 ) , computer experience ( r = 0 . 10 , n = 122 ) or education level ( r = - 0 . 06 , n = 121 ) . However , the overall number of comments was relatively strongly correlated to how frequently the user requested page views ( r = 0 . 52 , n = 69 ) and the time lapse between their account First comment outcome Median time to post 2 nd comment First comment rated down 50 minutes First comment not rated 9 . 8 hours First comment rated up 4 . 2 hours First comment replied to 1 . 4 hours First comment not replied to 7 . 1 hours Table 5 : Time to post second comment based on first comment outcomes . R - square 0 . 06 df 5 , 386 Coef . t P > | t | Constant 7787 . 90 12 . 064 . 001 Observation time - 0 . 002 - 2 . 580 . 010 Page views - 1 . 977 - 1 . 327 . 185 First comment up - 3939 . 80 - 3 . 639 . 001 First com . down - 1983 . 14 - 0 . 974 . 331 First com . replied - 1394 . 07 - 0 . 719 . 472 Table 6 : Ordinary least squares regression predicting time in minutes to post a second comment . R - square 0 . 34 df 5 , 386 Coef . t P > | t | Constant 0 . 262 3 . 009 . 003 Observation time 0 . 000 0 . 412 . 681 Page views 0 . 000 2 . 347 . 019 First comment up - 1 . 790 - 12 . 155 . 001 First com . down 1 . 286 4 . 670 . 001 First com . replied - 0 . 062 - 0 / 238 . 812 Table 7 : Ordinary least squares regression predicting score of the second comment . creation and when they posted their first comment ( r = 0 . 38 , n = 124 ) . This could mean that users who read the site more often are more willing to post comments . One of the implications of this might be that site administrators could detect frequent viewers early on , and mark them as potentially valuable participants early in their tenure on the site . Table 8 presents an ordinary least squares regression model predicting the overall number of comments a user will make . Since measures of experience are unrelated to total comments in univariate analysis , they are excluded from the model . In the model , observation time and frequency of page views are both related to the overall number of comments a user makes , whereas the direction of initial moderation and the first reply a comment receives are not . One could imagine an active user who both comments and reads frequently , but is no better at writing comments than any other user . Other work has shown that often a few authors are responsible for the majority of messages in an online conversation forum [ 25 ] . This seems to present a consistent view of the role between reading activity and posting . 4 . Discussion New members entering an existing online community face a complicated , often overwhelming environment where it is hard to know how to act . Slashdot is an unusual digital community , with a distinct , techno - centric culture and design that has accreted over a long time . We feel this makes Slashdot an especially interesting case study in that new members of this space are likely to have an especially hard time learning the standards of practice for posting comments . Previous experience did not seem to have a relationship with how highly rated a user’s first comment becomes . Although Slashdot users report they have high levels of experience in other online discussion forums , that experience does not seem to translate into automatic success on the site . New users felt they could write a comment that would be highly valued , but when their comments were rated there was no relationship between that belief and the actual score of the comment . This could mean that there is high variability in what users see as “valued” comments . The patterns of moderation outcomes for the first three comments new users contribute suggest intriguing implications . It is clear that many users choose not to continue contributing comments after their first , though the reason for this is less clear . Being rated up on a first comment does not seem to affect whether a user will post again or not , but it does seem to affect how quickly a second post will happen . Active users as measured by time spent observing the site before commenting and frequency of page views are more likely to post a second comments and more comments overall . The high rates of drop out among new members points to an alternate use of feedback through the rating system , namely encouraging users to select themselves out of the population . In formulating this research , we focused on learning how to participate in a way that will be valued . It could be that ratings provide a way to determine whether to continue participation on the site . There is some indication from the change is scores from first comments to second that initial scores might be random , and future comments regress towards a mean score . If this is true , then there are serious implications for how rating systems might affect new users . New users who receive undeserved , somewhat random negative attention might prematurely drop out of a discussion forum . 4 . 1 Limitations and future work This study is an initial examination of new user behavior in a persistent digital community , and is largely intended only to describe different participation outcomes . We look at the relationship between different user variables and how they participate on the site . By describing this case we are hoping to motivate continued examination of how new users become socialized when entering persistent digital communities . This work depends on findings from one case : Slashdot . This site is an exceptional case in many ways , being one of the few online discussion forums to use distributed moderation , and having developed their structure over many years . We feel , for example , that the general finding that users who receive attention from experienced members will participate differently can be generalized to a wider variety of digital communities . In the Slashdot case , attention came in the form of ratings and replies from other members , while in the section above we mention other possible forms of attention site designers could use . We hope that rather than being an exception to other online interactions , the Slashdot case provides a leading example of how interactions might be shaped with different elements of observation and user feedback . In the next stage of this work it is essential to more directly address causality . One experiment in consideration is to randomly assign new users on Slashdot into groups where they receive controlled amounts of feedback and measuring how their future participation differs . Another important data collection effort for future work will be more open - ended interactions with Slashdot users , either through more sophisticated survey work , or through interviews . For example , many Slashdot users never create an account , and for those who do it is unclear why they choose to do so . If a Slashdot user participates anonymously for a year and then creates an account , it is wrong to say they are a “new” user . More qualitative work might be able to address this issue where server log analysis has not . Although the survey provided interesting insight into the beliefs of the new Slashdot members , future work with this population R - square 0 . 23 df 5 , 386 Coef . t P > | t | Constant 4 . 551 4 . 176 . 001 Observation time - 0 . 003 - 2 . 741 . 006 Page views 0 . 026 10 . 252 . 001 First comment up 1 . 760 0 . 955 . 340 First com . down - 1 . 585 - 0 . 460 . 646 First com . replied - 5 . 019 - 1 . 537 . 125 Table 8 : Ordinary least squares regression predicting number of comment . should include additional measures of digital community experiences . Additional questions related to frequency and depth of participation in other forums would be valuable new information . Future work should also include more specific information about the specific pages being accessed by new users . 5 . Conclusion The findings from this study indicate that participation outcomes for new users are affected by a mix of previous experiences , observation of other members , and feedback received through ratings and replies . Each play distinct roles in whether and how a new user will participate on the site , and how that participation will be viewed by the larger community . These findings are potentially important for designers of digital communities who need to plan for incorporating new members into their ongoing social structures . 6 . Acknowledgements We would like to thank Slashdot for their assistance with this research . In particular , thanks to Jonathon “CowboyNeal” for providing log information , and Chris “pudge” Nandor for hard work in making the survey instrument operate on the Slashdot site . Rob “CmdrTaco” Malda receives special thanks for coordinating Slashdot activities and helping at all stages of data collection . Thanks also to Shane for coding a survey module to work with the Slashdot API . Thanks to Paul Resnick for comments and guidance on this work . This material is based upon work supported by the National Science Foundation under Grant No . 0114368 and Grant No . IIS 0308006 7 . References 1 . Bandura , A . Aggression : a social learning analysis . Prentice Hall , Englewood Cliffs , NJ , 1973 . 2 . Butler , B . S . Membership Size , Communication Activity , and Sustainability : A Resource - Based Model of Online Social Structures . Information Systems Research , 12 ( 4 ) . 346 - 362 . 3 . Crystal , D . Language and the Internet . Cambridge University Press , Cambridge , UK , 2001 . 4 . Forsyth , D . R . Group Dynamics . Wadsworth Publishing Company , Belmont , CA , 1999 . 5 . Goffman , E . Behavior in Public Places : Notes on the Social Organization of Gatherings . The Free Press , New York , NY , 1963 . 6 . Hiltz , S . R . and Turoff , M . The Network Nation : Human Communication via Computer . Addison - Wesley , London , UK , 1978 . 7 . Hiltz , S . R . and Turoff , M . Structuring Computer - Mediated Communication System to Avoid Information Overload . Communications of the ACM , 28 ( 7 ) . 8 . Ilgen , D . R . , Fisher , C . D . and Taylor , M . S . Consequences of individual feedback on behavior in organizations . Journal of Applied Psychology , 64 ( 4 ) . 349 - 371 . 9 . Jones , Q . Virtual - Communities , Virtual Settlements & Cyber - Archaeology : A Theoretical Outline . Journal of Computer - Mediated Communication , 3 ( 3 ) . 10 . Jones , Q . and Rafaeli , S . , User Population and User Contributions to Virtual Publics : A Systems Model . in GROUP ' 99 , ( Phoenix , AZ , 1999 ) , ACM Press . 11 . Jones , Q . , Ravid , G . and Rafaeli , S . , An empirical exploration of mass interaction system dynamics : Individual information overload and Usenet discourse . in 35th Hawaii International Conference on System Sciences , ( 2002 ) . 12 . Kim , A . J . Community Building on the Web : Secret Strategies for Successful Online Communities . Peachpit Press , Berkeley , CA , 2000 . 13 . Lampe , C . and Resnick , P . , Slash ( dot ) and burn : distributed moderation in a large online conversation space . in Conference on Human Factors in Computing Systems ( CHI ) , ( Vienna , Austria , 2004 ) , ACM Press , 543 - 550 . 14 . Lave , J . and Wenger , E . Situated Learning : Legitimate Peripheral Participation . Cambridge University Press , New York , 1993 . 15 . Madden , M . and Rainie , L . America ' s Online Pursuits : The changing picture of who ' s online and what they do . , Pew Internet & American Life Project , Washington D . C . , 2003 . 16 . Miller , G . A . , Information Input Overload . in Conference on Self - Organizing Sytems , ( Washington , 1962 ) , Spartan Books . 17 . Palme , J . The optimal group size in computer mediated communication . in Electronic Mail , Artech House Publishers , London , 1995 . 18 . Perkins , D . N . and Salomon , G . ( eds . ) . Transfer of Learning . Pergamon Press , Oxford , UK , 1992 . 19 . Powazek , D . M . Design for Community : The Art of Connecting Real People in Virtual Places . New Riders , Indianapolis , 2002 . 20 . Preece , J . , Nonnecke , B . and Andrews , D . The top five reasons for lurking : improving community experiences for everyone . Computers in Human Behavior , 20 . 201 - 223 . 21 . Raymond , E . S . The Jargon File , 2004 . 22 . Reid , E . Hierarchy and power : Social control in cyberspace . in Smith , M . A . and Kollock , P . eds . Communities in Cyberspace , Routledge , London , 1999 , 107 - 133 . 23 . Smith , M . Measures and Maps of Usenet . in Lueg , C . and Fisher , D . eds . From Usenet to CoWebs : Interacting with Social Information Spaces , Springer Verlag , New York , NY , 2002 . 24 . Sproull , L . and Kiesler , S . Connections : New ways of working in the networked organization . MIT Press , Cambridge , MA , 1991 . 25 . Viegas , F . B . and Smith , M . , Newsgroup Crowds and Authorlines : Visualizing the Activity of Individuals in Conversational Cyberspaces . in 37th Hawaii International Conference on System Sciences , ( Honolulu , HI , 2004 ) , IEEE . 26 . Whittaker , S . , Terveen , L . , Hill , W . and Cherny , L . , The Dynamics of Mass Interaction . in Computer - Supported Cooperative Work , ( Seattle Washington , 1998 ) , ACM .