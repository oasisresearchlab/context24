MLXP : A framework for conducting replicable Machine Learning eXperiments in Python Michael Arbel Univ . Grenoble Alpes , Inria , CNRS , Grenoble INP , LJK 38000 Grenoble , France Alexandre Zouaoui Univ . Grenoble Alpes , Inria , CNRS , Grenoble INP , LJK 38000 Grenoble , France Launcher ConÔ¨Åguration options Code - base and Backups Version A Version B sub Job version manager + Logger Reader ‚Ä¢ Filter ‚Ä¢ GroupBy ‚Ä¢ Aggregate ‚Ä¢ Log _ metadata ‚Ä¢ Log _ metrics ‚Ä¢ Log _ artifacts Computing resources Storage Results Figure 1 : Overview of MLXP and its components ABSTRACT Replicability in machine learning ( ML ) research is increasingly con - cerning due to the utilization of complex non - deterministic algo - rithms and the dependence on numerous hyper - parameter choices , such as model architecture and training datasets . Ensuring repro - ducible and replicable results is crucial for advancing the field , yet often requires significant technical effort to conduct systematic and well - organized experiments that yield robust conclusions . Several tools have been developed to facilitate experiment management and enhance reproducibility ; however , they often introduce complex - ity that hinders adoption within the research community , despite being well - handled in industrial settings . To address the challenge of low adoption , we propose MLXP , an open - source , simple , and lightweight experiment management tool based on Python , avail - able at https : / / github . com / inria - thoth / mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility . CCS CONCEPTS ‚Ä¢ Software and its engineering ‚Üí Software libraries and repos - itories ; ‚Ä¢ Computing methodologies ‚Üí Machine learning . KEYWORDS Machine learning , Replicability , Reproducibility 1 INTRODUCTION In recent years , the field of Machine Learning ( ML ) has experienced rapid advancements in both research and practical applications , ranging from improved algorithms to novel use cases in various industries . Recent breakthroughs , such as Large Language Models , have been projected to yield a potential annual market value of trillions of dollars , according to a report by McKinsey [ 1 ] , under - lining their extensive applicability and the significant economic opportunities they present . Many machine learning models , such as Large Language Models for text and speech generation [ 33 ] and autonomous driving sys - tems [ 14 ] , are extremely challenging to train as they rely on sophis - ticated algorithms and demand extensive computational resources and access to vast databases . As such , these models substantially differ from traditional software systems governed by deterministic rules . ML models are constructed using stochastic methods , intro - ducing randomness through factors such as the choice of initial weights or the order of data presentation to the model . This random - ness makes it challenging to reliably assess the performance of a given method from a single execution of the algorithm . Additionally , training often involves complex optimization procedures with nu - merous hyperparameters like scheduling , momentum , and weight decay , necessitating a systematic search for optimal configurations . These hyperparameters , coupled with architectural choices and data selection , compound the complexity , demanding meticulous organization and execution of potentially numerous experiments to ensure reproducibility and replicability which are essential for making progress in research . A significant portion of data scientists and ML practitioners tend to rely on customary solutions , often characterized as ad hoc and lacking a robust methodology for experiment management [ 15 , 35 ] . As a result , these customary choices may introduce friction in the experimentation process , potentially undermining reproducibility a r X i v : 2402 . 13831v1 [ c s . L G ] 21 F e b 2024 Arbel and Zouaoui and replicability [ 16 ] . For instance , the ability to systematically test ideas and conduct extensive grid searches over hyperparameters is crucial for impartially comparing different methods . Yet , this capability can be compromised when using customary solutions , which often result in cumbersome code . Consequently , the result - ing complexity imposes a psychological burden and an increased workload on practitioners when conducting multiple experiments , thereby introducing frictions to reproducibility and replicability . Careful handling of experiments , coupled with a robust method - ology and efficient tools for experiment management , is necessary to ensure the reproducibility and replicability of results [ 18 ] . Recent years have seen a rise in experiment management tools , such as MLflow , Wandb , Neptune . ai , Comet . ml , and Sacred , which provide extensive functionalities aimed at ensuring the reproducibility and traceability of ML workflows . These tools primarily focus on sys - tematic tracking and logging of various ML assets ( datasets , models , hyperparameters , etc . ) , versioning them , and visualizing and com - paring the results of several experiments . Leveraging these tools can significantly enhance reproducibility and traceability . However , this advantage comes with a considerably steep learning curve and the need to ( re ) organize ML project code - bases to effectively uti - lize them , potentially limiting their adoption within the broader scientific community [ 21 , 22 ] . Specifically , these tools often intro - duce their own set of APIs , utilities , and concepts that researchers must familiarize themselves with and learn to use . Consequently , the learning and integration burden associated with these tools can be prohibitive in research contexts , where projects prioritize agility and flexibility over deployment considerations that enforce strict workflows and conventions well suited to industrial settings . Simple experiment management tools with a low entry barrier , tai - lored to meet the scientific demands of flexibility and frictionless reproducibility , are lacking in the current landscape . There is a need for tools that can offer such a frictionless experience starting from configuration , launching , logging to versioning and exploitation of the results while requiring minimal intrusiveness in practitioners code - base to increase adoption by the research community . We introduce MLXP ( Figure 1 ) , an open - source Python frame - work designed to streamline experiment management , ensuring minimal friction during experimentation and result exploitation while upholding essential reproducibility standards with minimal intrusion . Python is widely recognized as the language of choice for machine learning , given its extensive array of machine - learning - related packages like Jax [ 7 ] or Pytorch [ 24 ] , and its widespread adoption in the research ecosystem . Therefore , we have chosen Python as the native language for MLXP , enabling seamless in - tegration with the code - bases of the majority of machine learning projects . MLXP builds upon Hydra [ 37 ] , a popular ML framework that has garnered increasing attention from the ML research com - munity thanks to its intuitive and concise syntax for configuring and locally launching multiple experiments . MLXP extends Hydra ‚Äôs capabilities by offering a comprehensive solution for seamless multiple job submission to job schedulers , logging , version man - agement , searching , and post - processing experiments , ensuring effortless and transparent experiment management . The result is a user - friendly system that prioritizes simplicity and empowers users to conduct rigorous experiments with confidence , fostering a culture of replicability in data science research ( see Table 1 ) . 1 . 0 . 1 Paper Organization . Section 2 discusses existing tools for ex - periment management , highlighting their strengths and limitations in the scientific research context . Section 3 addresses challenges encountered when conducting ML experiments for research and proposes a set of requirements that an experiment management tool needs to satisfy to mitigate these challenges . Section 4 pro - vides background information on Hydra , while Section 5 introduces MLXP and its core components . In Section 6 , we provide examples of research projects that leverage MLXP to easily perform replicable experiments . Finally , we conclude this paper with Section 7 . 2 RELATED WORK 2 . 1 Best practices for replicability in ML Rising concerns over replicability of experiments in ML has spurred the development of new best practices aimed at ensuring a high level of reproducibility and replicability in research findings . Re - cent initiatives include various documentation frameworks for ML models [ 11 , 20 ] and checklists [ 2 ] , which outline the essential assets ( e . g . , datasets , code , experimental results ) necessary to reproduce ML models . Recent work emphasized the importance of utilizing version management tools tailored for ML code , data , and environ - ments to ensure a high level of reproducibility in machine learning [ 3 , 5 , 8 , 17 ] . Furthermore , major AI conferences such as NeurIPS , ICML , and AAAI regularly host reproducibility workshops and encourage researchers to independently verify published research results in an effort to incentivize the adoption of best practices for reproducibility in scientific work [ 28 ] . Despite these efforts , best practices for reproducibility and replicability remain less preva - lent in the research community compared to other organizational settings , as highlighted in a study by [ 27 ] , which surveyed over 300 practitioners from various organizations . The study revealed low adoption rates for crucial practices like automated / systematic hyper - parameter optimization and model selection , which are vital for replicating ML research findings and ensuring their validity . We attribute the limited adoption of such practices in research institu - tions to a mismatch between the complexity required to enforce them and the need for flexibility and simplicity in research envi - ronments . Our proposed framework aims to address this mismatch by providing a lightweight tool that integrates key best practices , assisting practitioners in conducting replicable experiments . 2 . 2 ML experiment management tools An emerging category of tools aims to tackle the complexities associated with managing ML experiments . Practical examples of such tools include MLFlow [ 38 ] , WandB [ 6 ] and NeptuneML , while instances from the literature encompass Deep - water [ 10 ] , Runway [ 34 ] , and ModelKB [ 12 , 13 ] . Most of these tools primarily focus on furnishing functionalities for storing , tracking , and versioning assets across different experiment runs to address concerns like reproducibility [ 19 , 32 ] and traceability [ 21 ] . With the emergence of experiment management tools , conduct - ing systematic comparisons between them has become imperative to identify to what extend they can ensure reproducibility . Several studies aimed to compare these tools based on criteria deemed critical for reproducibility [ 17 ‚Äì 19 , 36 ] , such as the granularity of tracked / logged information and the level of intrusiveness required MLXP : A framework for conducting replicable Machine Learning eXperiments in Python to access this information . Even though they offer very extensive functionalities such as tracking versioning different types of assets , studies such as Idowu et al . [ 18 ] , concluded that these management tools still lack maturity compared to their traditional software en - gineering counterparts which may limit their adoption by a wider community . Factors contributing to their lack of maturity include friction and overhead incurred during usage due to the necessity of code instrumentation for tracking assets [ 21 , 22 ] . Specifically , many of the most popular experiment management tools like MLFlow , typically lack solutions for handling interactions with computing resources , such as easily launching multiple jobs to a job scheduler or managing delayed job executions , both crucial for frictionless replicability . Even when these solutions are provided , ( ex . Wandb ) they often require mastery of the proposed submission tools and introduce an overhead for practitioners . Additionally , while these frameworks enable the exploitation of results from multiple ex - periments , users may witness performance degradation or scal - ability issues when dealing with a large number of experiments or extensive datasets without careful adjustments . In this work , we introduce a simple and lightweight experiment management tool that addresses some of these limiting factors by significantly simplifying the experimental process with minimal user overhead while ensuring a fundamental level of reproducibility . 3 REPLICABILITY CHALLENGES We identify a set of challenges that can increase frictions in ob - taining replicable experiments in ML research and propose several requirements that an experiment management tool can satisfy to address them . These challenges are as follows : ( C . 1 ) Boiler - plate code related to experiments management . ( C . 2 ) Command line arguments clutter when configuring and launching multiple jobs . ( C . 3 ) Increased cognitive load when keeping track of asynchro - nous and nondeterministic job execution in High Perfor - mance Computing ( HPC ) clusters . ( C . 4 ) Complexity of processing end - results when dealing with several experiments . Challenge C . 1 . Boiler - plate code , i . e . , code that is necessary to perform the task at hand but is not part of the machine learning model design , can significantly degrade code readability , particu - larly when practitioners prioritize rapid iteration over code quality , as is often the case in research . Thus , any attempt to remove boiler - plate code is valuable . For instance , Pytorch Lightning [ 9 ] , a deep learning framework built on top of Pytorch [ 24 ] , has seen a surge in popularity as it has been designed to streamline the often redundant code pieces inherent to Pytorch . While Pytorch Lightning is not designed for managing experi - ments , as it mainly focuses on reducing the complexity of the code needed for learning a model based on Pytorch , its simplicity of use can be an inspiration when designing experiment management tools such as MLXP . An important caveat , however , that is associ - ated with the apparent simplicity of frameworks such as Pytorch Lightning , is the reduced flexibility when it comes to using it for developing methods that go out of the beaten track for which it was not originally designed . Experiment management tools should not reduce flexibility at the cost of simplicity and must instead be easy to integrate to the code - base and grant end - users enough flexibility to accommodate their specific needs while reducing the complexity arising from code management . Challenge C . 2 . Command line arguments clutter is pervasive across data science sub - fields as researchers often need to carry out multiple experiments simultaneously to validate some hypoth - esis or fine - tune hyperparameters . Manually creating the required scripts to be fed to the job scheduler is cumbersome . An ideal candi - date framework for replicable machine learning experiments must include the option to seamlessly submit multiple jobs to a HPC job scheduler . Challenge C . 3 . Practitioners can usually be under heavy cog - nitive load to keep track of changes made to the code - base that led to significant performance improvement , especially when mul - tiple experiments are submitted continuously , while the code is still under development . Any solution that can ease the cognitive burden of machine learning developers is a net positive . In addition , C . 3 refers to the unavoidable interactions with HPC , which can be tedious and error - prone as experiments might be launched after several changes have been introduced into the code - base due to busy computing resources at the time of job submission . This leads to practitioners having to be extremely careful not to contaminate their code with new changes until jobs meant to use the former version have been launched . Any automated code and job version management could be helpful to circumvent this issue . Challenge C . 4 . The ability to process and analyze end - results is as important as the code required to run the method that produced them . As a consequence , exploiting results must be straightfor - ward . This entails the ability to easily query results , as well as perform grouping and aggregation operations . Finally , we observe that logging intermediate results and final outcomes can yield large files that may need to be parsed efficiently . A convenient way to overcome this challenge would be lazily loading , which provides practitioners with a quick overview of all experiments without loading heavy data in memory and allows easy access to specific results when needed . Addressing these challenges is crucial to empower practitioners to perform systematic and rigorous experiments . Therefore , we formulate a list of requirements an experiment management tool should fulfill in order to address the challenges discussed above : ( R . 1 ) Seamless integration to the code - base . ( R . 2 ) Automated code and job version management . ( R . 3 ) Easy configuration and submission of multiple jobs using hierarchical configuration files . ( R . 4 ) Intuitive and efficient result exploitation capabilities . The requirement list above is non - exhaustive as many additional features implemented in existing tools are essential in an industrial context , such as collaborative features , interactive visualizations of results , native support for Cloud computing . However , the four re - quirements we identified above are calibrated to academic research and aim to address practical challenges that arise frequently in that context . Our proposed solution , MLXP aims at addressing the challenges above by implementing the four general requirements to allow friction - less replicability in ML . Arbel and Zouaoui Table 1 : Comparing MLXP with two other popular frameworks for managing experiments Features MLXP MLFlow tracking Weights & Biases Configuration management ‚úì ‚úó ‚úó Hyper - parameter Sweeps ‚úì ‚úó ‚úì Job submission ‚úì ‚úó ‚úì Logging / Tracking ‚úì ‚úì ‚úì Experiment Search / Filtering ‚úì ‚úì ‚úì Experiment Comparison ‚úì ‚úì ‚úì Lazy loading results ‚úì ‚úó ‚úó Code versioning ‚úì ‚úì ‚úì Code version checking ‚úì ‚úó ‚úó Model Versioning ‚úó ‚úì ‚úì Interactive Visualizations ‚úó ‚úì ‚úì Collaboration and Sharing ‚úó ‚úì ‚úì 4 BACKGROUND ON HYDRA Hydra [ 37 ] is a popular open - source framework that simplifies the development of research code . Its main feature is the ability to dynamically create a hierarchical configuration by composition , and override it through configuration files and the command line interface ( CLI ) . This feature results in drastic simplification to the code as passing complex options becomes modular . The simplifi - cations to the code induced by Hydra has made it a popular tool in the ML community with several successful research projects relying on it , such as : ( 1 ) A computer vision research project on developing a large mask inpainting method using Fourier convolutions [ 30 ] . ( 2 ) PDEBench [ 31 ] , which is essentially an extensive bench - mark for scientific machine learning . ( 3 ) A multi - task reinforcement learning research project using context - based representations [ 29 ] . Hydra is itself powered by a hierarchical configuration system based on Python and named OmegaConf that allows for merging configurations from multiple sources ( e . g . , YAML configuration files , python dataclasses or objects , and CLI arguments ) . An example of a YAML configuration file can be found in Listing 1 . It allows users to specify arguments related to various parts of the code - base . Notably , Python objects can be seamlessly instantiated alongside their dedicated arguments . For instance , the model has a config - urable number of units , num _ units . Once processed by Hydra , the configuration file is automatically translated into the relevant Python objects with their corresponding attributes as expected . Furthermore , Hydra allows users to dynamically alter the YAML configuration file , either by modifying existing arguments , or by creating new options on the fly through the command line . 1 seed : 0 2 lr : 10 . 3 num _ epoch : 10 4 model : 5 num _ units : 100 6 data : 7 d _ int : 10 8 device : ' cpu ' Listing 1 : Example of a yaml configuration file ‚Äôconfig . yaml‚Äô Another key aspect of Hydra is the ability to run multiple jobs locally with different arguments using a single command . Hydra is therefore well suited for conducting machine learning research experiments at scale as one can easily sweep through different ex - perimental configurations . All in all , Hydra significantly reduces the need for boiler - plate code and allows practitioners to focus on their core research contributions . Our proposed solution builds on Hydra ‚Äôs capabilities to provide a complete experiment manage - ment tool for ML practitioners . 5 MLXP MLXP ( Machine Learning eXperimentalist for Python ) is an open - source Python package whose source code and documentation are available in https : / / github . com / inria - thoth / mlxp and https : / / inria - thoth . github . io / mlxp / and that can be easily installed using PyPi ( see https : / / pypi . org / project / MLXP / ) . MLXP builds upon the pow - erful Hydra framework to offer a comprehensive solution for easily managing multiple experiments in Python to ensure their repro - ducibility and replicability . As an open - source package , MLXP facilitates seamless experiment launching , logging , and result ex - ploitation with unparalleled efficiency . Key components include automated job launching using Hydra and hierarchical configura - tion files , meticulous logging of outputs along with metadata for enhanced replicability , automated code and job version manage - ment , seamless multi - job submission to a HPC job scheduler , and intuitive result exploitation capabilities including querying results , grouping and aggregation operations . Using these components we address the challenges described in Section 3 . The end result is an easy - to - use system that prioritize transparency and empowers users to conduct rigorous experiments with confidence , fostering a culture of replicability and reproducibility in data science research . We provide an overview of MLXP in Section 5 . 1 to illustrate its simplicity , before discussing its major components . 5 . 1 Overview and usage MLXP provides a drop - in decorator mlxp . launch that extends the Hydra decorator hydra . main and allows to dynamically pass a context object ctx to a main task function task _ function defined inside a python script . This decorator is designed to seamlessly MLXP : A framework for conducting replicable Machine Learning eXperiments in Python my _ project / configs / config . yaml mlxp . yaml logs / 1 / 2 / main . py Figure 2 : Project directory structure integrate with existing code - bases , requiring minimal adjustments to the user‚Äôs code while offering maximum compatibility and ease of integration ( ex : see the main . py script in Listing 2 ) . 1 import mlxp 2 3 @ mlxp . launch ( config _ path = ' . / configs ' ) 4 def task _ function ( ctx : mlxp . Context ) - > None : 5 config = ctx . config 6 logger = ctx . logger 7 8 print ( " Hello World ! " ) 9 10 if _ _ name _ _ = = " _ _ main _ _ " : 11 task _ function ( ) Listing 2 : Example of a Python script ( ‚Äômain . py‚Äô ) using MLXP The context object ctx provides two essential elements for launch - ing the experiment : ( 1 ) a hierarchical experiment configuration object ctx . config similar to Hydra ‚Äôs DictConfig structure , that provides information from a default configuration files and can be overridden in the command line , and ( 2 ) a logger object to store the outputs / results of the experiment in a directory automatically created by MLXP and that is unique to a particular run . Using MLXP requires only defining two reserved directories for the project : a configuration directory and a log directory . For instance , Figure 2 shows an example of a project directory structure which contains these two directories in addition to the main python script main . py : . The configuration directory configs contains two yaml files config . yaml and mlxp . yaml from which MLXP ex - tracts the default configuration for the project . The logs / direc - tory , whose location can be customized by the user , is created by default by MLXP under working directory and contains separate sub - directories , each storing information provided to the logger object during a specific run . These sub - directories are named after their order of execution ( 1 , 2 , . . . . ) so as to ensure a unique identifier . Finally , executing Python scripts using MLXP follows the same procedures as Hydra , meaning that regular Python command - lines as simple as python main . py can be used . 5 . 2 Launching Launching experiments effortlessly is paramount in research and development environments . Hydra provides an excellent mech - anism for executing multi - runs with ease , allowing users to pass multiple options through the command line effortlessly . MLXP takes this a step further by seamlessly integrating with job sched - ulers using mlxpsub command . This integration enables users to not only orchestrate and configure experiments effortlessly but also to submit them directly to a job scheduler without manual intervention . The resulting automation enhances productivity and ensures consistent and reliable execution of experiments across different computing environments . 5 . 2 . 1 The mlxpsub command . MLXP provides a shell com - mand ( mlxpsub ) which leverages the multi - run functionalities of Hydra for effortless submission of multiple runs using a single bash script . It is compatible with most of the available job schedulers , such as SLURM , TORQUE , SGE , OAR , MWM and LSF , and simply requires to specify the jobs‚Äô options in the main script using the syntax de - fined by the scheduler . Listing 3 illustrates the simplicity by which several jobs can be specified transparently in a few lines of code using a single bash script . There , it is assumed the user has access to a SLURM job scheduler . Therefore , the job specifications , such as the duration of each job , the number of tasks and the number of cpus per task , follows the scheduler‚Äôs syntax as indicated by lines starting with the string # SLURM . Finally , the last line executes the python script main . py with 4 different configuration settings provided using Hydra ‚Äôs syntax . There , 4 runs are meant to be exe - cuted , each one corresponding to a tuple of values for the options lr ( 10 . or 1 . ) and seed ( 1 or 2 ) . 1 # ! / bin / bash 2 3 # SLURM - - time = 1 - 00 : 10 : 00 4 # SLURM - - ntasks = 1 5 # SLURM - - cpus - per - task = 1 6 7 python main . py lr = 10 . , 1 . seed = 1 , 2 Listing 3 : Example of a Bash script ( ‚Äôscript . sh‚Äô ) using MLXP Simply launching the script in Listing 3 using mlxpsub script . sh command will automatically create 4 different jobs , each corre - spondingtoatuple ofoptionsprovidedtothepythonscript main . py , which are then submitted to a scheduler‚Äôs queue . By contrast , launching the script in Listing 3 directly using a scheduler‚Äôs sub - mission command ( here sbatch script . sh ) will create a single job which successively executes 4 different runs with their corre - sponding tuple of options . 5 . 2 . 2 Alternatives . Without the mlxpsub command , one would have to create a meta script that generates a script similar to script . sh for each option choices and submits it using sbatch command . Such approach does not exploit the multi - run function - ality provided by Hydra and introduces clutter in the job submis - sion code . Other popular alternatives , such as submitit package , allow a simplified job submission to a SLURM scheduler , but re - quires adapting the Python code for using it and does not support command - line submission natively . Note , however , that a plugin version of submitit for Hydra allows command - line submission , although it is specific to SLURM and requires passing the sched - uler‚Äôs options in a format that is different from the scheduler‚Äôs native one . On the other hand , mlxpsub is versatile as it is compati - ble with several schedulers and transparent because it simply relies Arbel and Zouaoui on the scheduler‚Äôs native syntax , with which the user is supposedly already familiar . 5 . 3 Logging Keeping track of information relative to an experiment / run is cru - cial for replicability . The MLXP ‚Äôs logger component provides comprehensive logging capabilities that ( 1 ) automatically handles the creation of log directories unique to each run , ( 2 ) systematically stores metadata relative to a run , such as the configurations used during execution , information about the resources used , code ver - sion , etc and ( 3 ) simply allows the user to log additional information about the run such as various metrics , checkpoints or artifacts . All log directories created using MLXP have a similar structure as the one shown in Figure 3 . Below , we discuss key requirements that MLXP satisfies to allow maximal replicability and ease of use . 5 . 3 . 1 Automatic creation of unique log directories . When running multiple experiments in parallel , storing the result of each one sep - arately guarantees that no interference can occur . MLXP ensures the each new experiment has a unique location that is resolved using a simple internal mechanism for keeping track of previous runs in a given log directory . Such a mechanism increments the total number of existing log directories by one to assign a log _ id to each new run . The new run directory is then named after the new log _ id . When submitting multiple jobs to a job scheduler using mlxpsub command discussed in Section 5 . 2 , these jobs are executed asynchronously . A naive approach would be prone to a large I / O concurrency when resolving the log _ id for each job . MLXP avoids these issues altogether by sequentially assigning a log _ id to each job and creating its corresponding log directory before submitting these jobs to a scheduler . Then upon execution , each job is forced to store its outputs in its assigned log directory . As a result , MLXP allows to easily submit a large number of jobs without worrying about clashes between different jobs . 5 . 3 . 2 Comprehensive logging . Storing information such as all the configuration options used to run experiments are basic require - ments for replicability that MLXP handles automatically . There , the object ctx . config is stored in a file config . yaml under the metadata directory of a given run ( see Figure 3 ) . Moreover , in - formation about the execution of the run , such as its ‚Äôstatus‚Äô ( ex : COMPLETED , FAILED , RUNNING ) or MLXP ‚Äôs configuration op - tions are stored in separate files ( info . yaml and mlxp . yaml ) to allow a precise monitoring of experiments . For instance , a user might want to handle results coming from incomplete experiments differently to draw correct conclusions . These additional informa - tion are especially useful when several experiments are executed and need to be filtered according to their status as discussed in Sec - tion 5 . 5 . These metadata information complement the metrics and artifacts that are stored by the user using logger . log _ metrics and logger . log _ artifacts . The former method allows storing a list of dictionaries of scalar values in a simple JSON format whereas the latter one allows storing more structured objects such as im - ages , model parameters , etc . Both modalities can be accessed easily thanks to the universal formats used and the simple file struc - ture of the log directories Figure 3 . Finally , due to re - allocation of computing resources , some experiment needs to be momentarily logs / 1 / metadata / config . yaml info . yaml mlxp . yaml metrics / train . json / keys / metrics . yaml artifacts / Checkpoint / lastckpt . pkl 2 / Figure 3 : Log directory structure stopped and then automatically relaunched , as is the case when submitting a job to a besteffort queue of a scheduler . The abil - ity to store generic checkpoints ( in a pickled format ) allows to safely resume the experiment in the exact state in which it was paused . This could be achieved , for instance , by calling the method logger . log _ checkpoint ( ckpt ) where ckpt refers to any user - defined object that contains all the environment and state variables used by a given experiment . 5 . 3 . 3 Alternatives . The logger provided by MLXP induces a mod - erate level of intrusiveness in the code as it requires the user to use call methods such as log _ metrics inside the code - base . This approach is quite common to most experiment management tools and does not constitute a strong constraint in the context of scien - tific / academic projects . Some tools such as ModelKB , MLFlow , and Wandb support automatic asset collection in non - intrusive ways . However , these approaches require explicit support for machine learning frameworks , such as Pytorch [ 24 ] and SciKit Learn [ 25 ] , and appear to be constraining in some research contexts where new insights are obtained from tracking non - standard quantities of interest . 5 . 4 Job versioning In typical machine learning research projects , there is a constant iteration between code development and experimental validation . This iterative process provides flexibility to test new ideas and refine them based on partial empirical results . While this agile development approach can accelerate scientific research , it also increases the likelihood of erroneous conclusion about a given method due to misreporting experimental results . Misreporting is likely to occur when data from different code versions are combined potentially altering the interpretation of the experiments . Although unintended , this constitutes a breach of the scientific method and can result in significant wasted time and resources . MLXP proposes a systematic way to version the experiments and to guarantee they were obtained using a specific version of the code - base . Before describing MLXP ‚Äôs versioning mechanism , we first illustrate how MLXP : A framework for conducting replicable Machine Learning eXperiments in Python Scheduler HPC Code base Job 1 Version A Version B Launcher Submission 1 T i m e Job 2 Job 3 Job 3 Job 4 Job 1 Job 2 Job 3 Job 4 Submission 2 T1 T2 Figure 4 : Execution of non - versioned jobs a lack of experiment versioning can have unintended consequences when using HPC clusters and job schedulers . 5 . 4 . 1 Parallel job submissions without versioning . A typical situ - ation the users can be confronted to in practice arises when sub - mitting multiple jobs to a busy HPC cluster . In such cases , jobs are placed in a scheduler‚Äôs queue until the required resources become available . Consequently , jobs in the queue may run asynchronously , with an uncertain start time , while still relying on a shared code - base . This code - base is likely to undergo changes even before the jobs are executed , as illustrated in Figure 5 . There , three jobs are first submitted to a scheduler at time ùëá 1 and intended to use the cur - rent code version ( version ùê¥ ) . However , due to resource constraints , only the first two jobs are executed with the correct version at ùëá 1 . The third job ( job 3 ) is deferred to time ùëá 2 , at which point a new code version ( version ùêµ ) becomes available , and a new job ( job 4 ) , intended to use the latest code version , is submitted to the queue . Consequently , both jobs 3 and 4 are executed using the current code version ( version ùêµ ) . Had sufficient resources been available at ùëá 1 , job 3 would have used version ùê¥ of the code . Thus , depending on resource availability , the same job submitted to an HPC cluster may be executed using different code versions . Next , we discuss how MLXP addresses this replicability issue . 5 . 4 . 2 MLXP ‚Äôs version manager . We introduce a version manager that builds on Git to address the replicability issue that might arise due to a delayed execution of jobs . MLXP ‚Äôs version manager operates by versioning the code with Git and associating all jobs submitted to a scheduler at a specific time with a backup copy of the code based on the latest commit available . When the requested resources become available for a job , it is executed from the linked backup copy rather than the current code - base , which may have changed meanwhile . To prevent redundant copies of the code , each backup is linked to a unique commit through the Git commit hash . Thus , only a single backup copy of the code is created as long as the code didn‚Äôt change , even if multiple jobs are submitted . Figure 5 illustrates how linking jobs to code backups ensures that each job is executed using the intended code version , irrespective of changes in the current version of the main code - base . In this scenario , even though job 3 was initially linked to version A , it will still be executed using that same version from the backup copy , despite its execution being delayed until time ùëá 2 , when the current code - base has evolved to version B . This enhanced traceability of experiments increases their replicability in the context of large HPC Scheduler HPC Code base Job 1 / v A Version A Launcher Submission 1 T i m e Job 2 / v A Job 3 / v A Job 3 / v A Job 4 / v B Job 1 Job 2 Job 3 Job 4 Submission 2 Version A Version B Version B Version A Backup T1 T2 Figure 5 : Execution of versioned jobs clusters . Furthermore , as a byproduct of this traceability , it becomes easier to pinpoint potential bugs that could have been introduced at subsequent versions of the code . This is the case , if for instance , in Figure 5 , the new job ( job 4 ) is expected to give the same results as ( job 3 ) even though it is using a new version of the code ( version B ) . Any discrepancy in the results , can alert the user about a potential bug that could either be introduced in version B or that was present in version A and was inadvertently fixed later . 5 . 4 . 3 Code version checking . The effectiveness of the proposed strategy relies upon a rigorous versioning of the code - base using Git , which might not be always guaranteed , especially in a context of fast - paced code development and limited exposure to version control practices . To overcome these limitations , MLXP offers op - tional version control checks that can be used in an interactive manner . When these are enabled and upon launching a script that uses MLXP , an interactive session is created where MLXP checks if the current Git repository containing the code - base has untracked files or uncommitted changes . It then asks the user if they want to add untracked files and create an automatic commit . Only after that , the jobs are created dynamically based on the user‚Äôs choices and are submitted to a job scheduler when available . While this procedure is optional , it can guarantee optimal versioning and traceability of the experiments , which can save a lot of time and resources especially when each experiment is computationally demanding . 5 . 5 Reading The MLXP components discussed so far focused on facilitating the submission of multiple jobs , ensuring their traceability and correct execution through rigorous versioning , and maintaining compre - hensive records of all pertinent information about each experiment , including configuration details , results , and run status . With these components , users can effortlessly submit a large number of experi - ments , resulting in a substantial amount of data . Properly designed tools are essential for efficiently managing and exploiting this data . In an effort to provide a comprehensive framework for manag - ing experiment , we propose a reader component that allows easy filtering of experimental results as well as simple grouping and aggregation operations . The reader component also offers a lazy evaluation mechanism that avoids storing in memory potentially large amount of data produced by the experiments and only loading them when necessary . Below we describe the main functionalities of the reader component . Arbel and Zouaoui 5 . 5 . 1 Filtering results . Filtering results of multiple experiments using MLXP is as easy as creating a reader object linked to the logs directory of all runs and providing a query string that allows to filter results based on information stored in the metadata directory created by MLXP logger for each run ( see Listing 4 ) . The syntax for querying is close to Python‚Äôs syntax for boolean operators and allows comparison operations ( ‚Äô = = ‚Äô , ‚Äô ! = ‚Äô , ‚Äô < ‚Äô , ‚Äô > ‚Äô , ‚Äô < = ‚Äô , ‚Äô > = ‚Äô ) , logical operations ( and ( ‚Äô & ‚Äô ) , or ( ‚Äô | ‚Äô ) , not ( ‚Äô ‚àº ‚Äô ) ) , and membership ( ‚Äôin‚Äô ) . It also supports operation precedence using parenthesis and follows similar precedence rules as in Python which allows to easily combine all these comparison operations to create sophisticated queries . For ease of use , the list of keys that can be used in a query are provided by the attribute reader . searchable of the reader . 1 In [ 1 ] : import mlxp 2 In [ 2 ] : # Creating a reader object . 3 reader = mlxp . Reader ( ' . / logs / ' ) 4 In [ 3 ] : # Searching using a query string 5 . . . query = " info . status = = ' COMPLETE ' & config . optimizer . lr < = 1 . " 6 . . . results = reader . filter ( query _ string = query ) Listing 4 : Example of an interactive IPython shell to read and filter experiment results using MLXP 5 . 5 . 2 Loading results . Once filtered , results are represented as a dataframe - like object , as shown in Listing 5 , where each row repre - sents a different run in the logs directory , whereas each column rep - resents either a metadata field appearing in at least one of the runs or a metric that was stored by calling the method log _ metrics of MLXP ‚Äôs logger object . While metadata are typically scalar quanti - ties that can readily be stored in the dataframe - like object results , metrics are typically arrays of arbitrary size that could be cum - bersome to load in the dataframe for each run . Instead , metrics are ‚Äúlazily evaluated " , meaning that they are simply linked to the location where they are stored and are only loaded when the user explicitly tries to access them . Hence the values of these fields are marked as ‚ÄúLAZYDATA " and produce an output array whenever direct access for a particular run and metric is performed . Such a lazy evaluation allows a lightweight handling of large amounts of information while still retaining simplicity and flexibility . Finally , it is always possible to easily convert the MLXP dataframe - like object into a Pandas dataframe [ 23 ] for convenience . In that case , however , ‚Äúlazy evaluation " is no longer possible . Instead , all data are loaded in memory . 1 In [ 4 ] : # Display results 2 . . . : results 3 Out [ 4 ] : 4 config . seed config . lr . . . train . iter train . loss 5 0 0 1 . . . . LAZYDATA LAZYDATA 6 0 1 1 . . . . LAZYDATA LAZYDATA 7 In [ 5 ] : # Direct access 8 . . . : results [ 0 ] [ ' train . iter ' ] 9 Out [ 5 ] : 10 [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Listing 5 : Displaying results and directly accessing ‚Äúlazily " evaluated fields using MLXP 5 . 5 . 3 Grouping and aggregation . It is often useful to group / aggre - gate results by values taken by some fields of the dataframe . While all these operations are possible using frameworks such as Pan - das , we provide some basic support for those that is compatible with the lazy evaluation functionality of MLXP reader . Listing 6 illustrate how to perform a grouping operation on the dataframe given a list of group keys and using the method groupBy which returns a dictionary of dataframes whose keys correspond to the different group values . The group keys can contain several fields of the dataframe in which case the groups are indexed by a tuple of values corresponding to these fields . It is worth noting that the metrics are still lazily evaluated , which allows to perform complex grouping operations without unnecessary memory overload . 1 In [ 6 ] : # List of group keys . 2 . . . group _ keys = [ ' config . lr ' ] 3 In [ 7 ] : # Grouping the results 4 . . . : grouped _ res = results . groupBy ( group _ keys ) 5 . . . : grouped _ res 6 Out [ 7 ] : 7 config . seed . . . train . iter train . loss 8 config . lr 9 1 . 0 . . . LAZYDATA LAZYDATA 10 1 . . . LAZYDATA LAZYDATA Listing 6 : Example of an interactive IPython shell showcasing the grouping capabilities of MLXP Finally , once groups are formed , it is possible to perform simple aggregation operations . Listing 7 illustrate how to average the field train . loss over members of each group using the method aggregate and the map AvgStd provided by MLXP . The opera - tion returns a dataframe containing the initial group keys and the results of aggregation . Here , lazy evaluation is no longer performed since computing the aggregated result requires directly accessing the metric train . loss . 1 In [ 8 ] : # Creating the aggregation maps 2 . . . from mlxp . data _ structures . contrib . aggregation _ maps import AvgStd 3 . . . agg _ maps = [ AvgStd ( ' train . loss ' ) ] 4 In [ 9 ] : # Aggregating the results 5 . . . : agg _ res = grouped _ res . aggregate ( agg _ maps ) 6 . . . : agg _ results 7 Out [ 9 ] : 8 config . lr . . . train . loss _ avg 9 0 1 . . . . [ 0 . 03 , . . . , 0 . 001 ] Listing 7 : Example of an interactive IPython shell to aggregate results using aggregation maps from MLXP 5 . 5 . 4 Alternatives . MLFlow [ 38 ] and Weights & Biases ( W & B ) [ 6 ] provide filtering capabilities that are similar to MLXP but no direct support for grouping or aggregation operations which are usually performed using frameworks such as Pandas . When dealing with large amount of output data , conversion of raw experimental re - sults into Pandas format to perform these operations can result in performance degradation and scalability issues since all results are MLXP : A framework for conducting replicable Machine Learning eXperiments in Python loaded in memory . The fully integrated support for these opera - tions in MLXP allows users to easily exploit and compare multiple results with a minimal code overhead and memory usage . 6 APPLICATIONS In this section , we provide two examples of research projects that build upon MLXP to perform replicable experiments . 6 . 1 HySUPP [ 26 ] HySUPP is an open - source python toolbox for hyperspectral unmix - ing practitioners . In essence , HySUPP enables seasoned researchers , curious students , and teachers to easily experiment with various hyperspectral unmixing methods with replicability in mind thanks to MLXP , as indicated in the Github repository of the code asso - ciated with the paper https : / / github . com / BehnoodRasti / HySUPP . HySUPP leverages basic functionalities including effortless launch - ing , comprehensive logging capabilities such as evaluation met - rics specific to unmixing and the custom - made Estimate artifact , alongside the reader component to easily filter experimental re - sults before grouping and aggregating them . Notably , the reader component enables end - users to create informative plots such as Figure 6 using minimal boiler - plate code in order to visualize the performance of several unmixing techniques , grouped by their su - pervision setup ( i . e . , supervised , semi - supervised , or blind ) where 5 runs have been aggregated so as to create error bars corresponding to the standard deviation from the mean value when accounting for randomness in data generation and model initialization . An example of a command - line instruction can be found in Listing 8 , which highlights its intuitive usage , since users may simply change the unmixing mode , data , or model depending on their needs . C N N A E U E D AA M S N e t P G M S U A D M M N e t B a y e s i a n S M A N M F Q M V M i S i C N e t S U n S A L C L S U n S A L M U A _ S L I C S 2 W S U S U n C NN S U n AA S I S A L + F C L S S I S A L + U n D I P S i V M + F C L S V C A + F C L S V C A + U n D I P S i V M + U n D I P Unmixing models 0 5 10 15 20 A b un d a n c e s S R E ( d B ) 6 . 32 6 . 7 8 . 79 13 . 12 13 . 49 18 . 04 20 . 23 21 . 04 10 . 3 11 . 06 16 . 09 16 . 4 19 . 13 21 . 26 18 . 77 18 . 94 19 . 84 20 . 43 20 . 89 21 . 18 Abundances SRE on DC1 ( SNR = 30dB ) blindsemi - supervised supervised Figure 6 : Sub - figure originating from [ 26 ] 1 $ python unmixing . py mode = semi data = DC1 model = EDAA Listing 8 : HySUPP command - line arguments 6 . 2 Benchmarking optimization algorithms MLXP has recently been used to conduct experiments for com - paring a number of optimization algorithms for deep learning . In particular , Arbel et al . [ 4 ] compared two classes of optimization methods : Gradient descent and Gauss - Newton , to provide empirical insights on the effect of these algorithms on the generalization ca - pabilities of over - parameterized networks . There , 720 independent runs totaling in 3600 GPU hours were submitted to an HPC cluster using MLXP as indicated in the Github repository of the code as - sociated with the paper https : / / github . com / MichaelArbel / Implicit - Bias - Gauss - Newton . Figure 7 shows the results of a comparison between different learning algorithms depending on the standard variation of the initial parameters . Each point in the figure was obtained using 5 independent runs each of which trains the model using a different method , totalling in 105 independent runs . Listing 9 shows a simple bash script that can be launched using mlxpsub and that submits all runs to an HPC cluster using OAR . 10 2 10 1 10 0 10 1 10 2 Initial std of the weights 10 4 10 3 10 2 L o s s Test loss at convergence Random Features 1 - HL : Gradient Descent 1 - HL : Gauss - Newton Figure 7 : Sub - figure originating from [ 4 ] 1 # ! / bin / bash 2 3 # OAR - l core = 1 , walltime = 00 : 30 : 00 4 # OAR - t besteffort 5 # OAR - p gpumem > ' 16000 ' 6 7 python main . py std = 0 . 01 , 0 . 1 , 0 . 5 , 1 . , 5 , 10 , 100 \ 8 seed = 0 , 1 , 2 , 3 , 4 \ 9 method = RF , GD , GN \ Listing 9 : Bash script for job submission Once all the runs are complete , it is possible to extract the desired results using MLXP ‚Äôs reader module as shown in Listing 10 . This example illustrates the ease by which experiments can be systemati - cally performed , starting from job submissions to results extraction . 1 import mlxp 2 from mlxp . data _ structures . contrib . aggregation _ maps import AvgStd 3 4 reader = mlxp . Reader ( " . / logs / " ) 5 results = reader . filter ( query _ string = " " ) 6 7 # Averaging over seeds 8 agg _ maps = [ AvgStd ( ' test . loss ' ) ] 9 group _ keys = [ ' config . method ' , ' config . std ' ] 10 agg _ res = results . groupBy ( group _ keys ) . aggregate ( agg _ maps ) Listing 10 : Example of code for extracting results Arbel and Zouaoui 7 CONCLUSION In conclusion , MLXP presents a comprehensive solution to the complexities of managing machine learning experiments within a research environment . Its feature - rich framework streamlines experiment launching , logging , and tracking , while facilitating ef - ficient job submission to cluster environments and offering job versioning capabilities . With a strong emphasis on reproducibility and user - friendliness , MLXP simplifies experiment management tasks , allowing researchers to focus on their primary objectives . While MLXP currently lacks explicit support for data or model versioning , as well as interactive visualization and collaborative features , its core focus on simplicity lays a foundation for future development . Potential enhancements could include increased sup - port for versioning and a reduction in logging intrusiveness to offer more flexibility to users . As MLXP continues to gain adoption within the machine learning community , it holds the promise of improving research productivity . By addressing the challenges asso - ciated with experiment management , MLXP empowers researchers to conduct rigorous and reproducible studies , ultimately advancing the state of the art in machine learning research . 8 ACKNOWLEDGMENTS This work was supported in part by the ANR BONSAI project ( grant ANR - 23 - CE23 - 0012 - 01 ) . REFERENCES [ 1 ] 2021 ( accessedAugust , 2021 ) . NotesfromtheAIFrontierInsightsfromHundredsof Use Cases . https : / / www . mckinsey . com / featured - insights / artificial - intelligence / notes - from - the - ai - frontier - applications - and - value - of - deep - learning [ 2 ] 2021 ( accessed August , 2021 ) . The Machine Learning Reproducibility Checklist . https : / / www . cs . mcgill . ca / ~ jpineau / ReproducibilityChecklist . pdf [ 3 ] Saleema Amershi , Andrew Begel , Christian Bird , Robert DeLine , Harald C . Gall , Ece Kamar , Nachiappan Nagappan , Besmira Nushi , and Thomas Zimmermann . 2019 . Software engineering for machine learning : a case study . In Proceedings of the 41st International Conference on Software Engineering : Software Engineering in Practice , ICSE ( SEIP ) 2019 , Montreal , QC , Canada , May 25 - 31 , 2019 , Helen Sharp and Mike Whalen ( Eds . ) . IEEE / ACM , 291 ‚Äì 300 . [ 4 ] Michael Arbel , Romain Menegaux , and Pierre Wolinski . 2023 . Rethinking Gauss - Newton for learning over - parameterized models . Advances in neural information processing systems ( 2023 ) . [ 5 ] Amine Barrak , Ellis E . Eghan , and Bram Adams . 2021 . On the Co - evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects . In 28th IEEE International Conference on Software Analysis , Evolution and Reengineering , SANER 2021 , Honolulu , HI , USA , March 9 - 12 , 2021 . IEEE , 422 ‚Äì 433 . [ 6 ] Lukas Biewald . 2020 . Experiment Tracking with Weights and Biases . https : / / www . wandb . com / Software available from wandb . com . [ 7 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman - Milne , and Qiao Zhang . 2018 . JAX : composable transformations of Python + NumPy programs . http : / / github . com / google / jax [ 8 ] Boyuan Chen , Mingzhi Wen , Yong Shi , Dayi Lin , Gopi Krishnan Rajbahadur , and Zhen Ming Jiang . 2022 . Towards training reproducible deep learning models . In Proceedings of the 44th International Conference on Software Engineering . 2202 ‚Äì 2214 . [ 9 ] William Falcon and The PyTorch Lightning team . 2019 . PyTorch Lightning . https : / / doi . org / 10 . 5281 / zenodo . 3828935 [ 10 ] Rudolf Ferenc , Tam√°s Viszkok , Tam√°s Aladics , Judit J√°sz , and P√©ter Heged≈±s . 2020 . Deep - water framework : The Swiss army knife of humans working with machine learning models . SoftwareX 12 ( 2020 ) , 100551 . [ 11 ] TimnitGebru , JamieMorgenstern , BrianaVecchione , JenniferWortmanVaughan , Hanna M . Wallach , Hal Daum√© III , and Kate Crawford . 2018 . Datasheets for Datasets . CoRR abs / 1803 . 09010 ( 2018 ) . arXiv : 1803 . 09010 http : / / arxiv . org / abs / 1803 . 09010 [ 12 ] Gharib Gharibi , Vijay Walunj , Rakan Alanazi , Sirisha Rella , and Yugyung Lee . 2019 . Automated management of deep learning experiments . In Proceedings of the 3rd International Workshop on Data Management for End - to - End Machine Learning . 1 ‚Äì 4 . [ 13 ] Gharib Gharibi , Vijay Walunj , Sirisha Rella , and Yugyung Lee . 2019 . Modelkb : towards automated management of the modeling lifecycle in deep learning . In 2019 IEEE / ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering ( RAISE ) . IEEE , 28 ‚Äì 34 . [ 14 ] Sorin Mihai Grigorescu , Bogdan Trasnea , Tiberiu T . Cocias , and Gigel Macesanu . 2020 . A survey of deep learning techniques for autonomous driving . J . Field Robotics 37 , 3 ( 2020 ) , 362 ‚Äì 386 . [ 15 ] Charles Hill , Rachel Bellamy , Thomas Erickson , and Margaret Burnett . 2016 . Trials and tribulations of developers of intelligent systems : A field study . In 2016 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 162 ‚Äì 170 . [ 16 ] Matthew Hutson . 2018 . Artificial intelligence faces reproducibility crisis . Science ( New York , N . Y . ) 359 ( 02 2018 ) , 725 ‚Äì 726 . https : / / doi . org / 10 . 1126 / science . 359 . 6377 . 725 [ 17 ] Samuel Idowu , Daniel Str√ºber , and Thorsten Berger . 2021 . Asset Management in Machine Learning : A Survey . In 43rd IEEE / ACM International Conference on Software Engineering : Software Engineering in Practice , ICSE ( SEIP ) 2021 , Madrid , Spain , May 25 - 28 , 2021 . IEEE , 51 ‚Äì 60 . [ 18 ] Samuel Idowu , Daniel Str√ºber , and Thorsten Berger . 2022 . Asset management in machine learning : State - of - research and state - of - practice . Comput . Surveys 55 , 7 ( 2022 ) , 1 ‚Äì 35 . [ 19 ] Richard Isdahl and Odd Erik Gundersen . 2019 . Out - of - the - box reproducibility : A survey of machine learning platforms . In 2019 15th international conference on eScience ( eScience ) . IEEE , 86 ‚Äì 95 . [ 20 ] MargaretMitchell , SimoneWu , AndrewZaldivar , ParkerBarnes , LucyVasserman , Ben Hutchinson , Elena Spitzer , Inioluwa Deborah Raji , and Timnit Gebru . 2019 . Model Cards for Model Reporting . In Proceedings of the Conference on Fairness , Accountability , and Transparency , FAT * 2019 , Atlanta , GA , USA , January 29 - 31 , 2019 , danah boyd and Jamie H . Morgenstern ( Eds . ) . ACM , 220 ‚Äì 229 . [ 21 ] Mar√ßal Mora - Cantallops , Salvador S√°nchez - Alonso , Elena Garc√≠a - Barriocanal , and Miguel - Angel Sicilia . 2021 . Traceability for trustworthy ai : A review of models and tools . Big Data and Cognitive Computing 5 , 2 ( 2021 ) , 20 . [ 22 ] Alexandru A Ormenisan , Mahmoud Ismail , Seif Haridi , and Jim Dowling . 2020 . Implicit provenance for machine learning artifacts . Proceedings of MLSys 20 ( 2020 ) . [ 23 ] The pandas development team . 2020 . pandas - dev / pandas : Pandas . https : / / doi . org / 10 . 5281 / zenodo . 3509134 [ 24 ] AdamPaszke , SamGross , FranciscoMassa , AdamLerer , JamesBradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas K√∂pf , Edward Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . PyTorch : An Imperative Style , High - Performance Deep Learning Library . _ eprint : 1912 . 01703 . [ 25 ] Fabian Pedregosa , Ga√´l Varoquaux , Alexandre Gramfort , Vincent Michel , Bertrand Thirion , Olivier Grisel , Mathieu Blondel , Peter Prettenhofer , Ron Weiss , Vincent Dubourg , et al . 2011 . Scikit - learn : Machine learning in Python . Journal of machine learning research 12 , Oct ( 2011 ) , 2825 ‚Äì 2830 . [ 26 ] Behnood Rasti , Alexandre Zouaoui , Julien Mairal , and Jocelyn Chanussot . 2023 . Image Processing and Machine Learning for Hyperspectral Unmixing : An Overview and the HySUPP Python Package . arXiv preprint arXiv : 2308 . 09375 ( 2023 ) . [ 27 ] Alex Serban , Koen van der Blom , Holger Hoos , and Joost Visser . 2020 . Adoption and effects of software engineering best practices in machine learning . In Pro - ceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement ( ESEM ) . 1 ‚Äì 12 . [ 28 ] Koustuv Sinha , Maurits Bleeker , Samarth Bhargav , Jessica Zosa Forde , Sharath Chandra Raparthy , Jesse Dodge , Joelle Pineau , and Robert Stojnic . 2023 . ML Reproducibility Challenge 2022 . ReScience C 9 , 2 ( July 2023 ) . [ 29 ] Shagun Sodhani , Amy Zhang , and Joelle Pineau . 2021 . Multi - task reinforce - ment learning with context - based representations . In International Conference on Machine Learning . PMLR , 9767 ‚Äì 9779 . [ 30 ] Roman Suvorov , Elizaveta Logacheva , Anton Mashikhin , Anastasia Remizova , ArseniiAshukha , AlekseiSilvestrov , NaejinKong , HarshithGoka , KiwoongPark , andVictorLempitsky . 2022 . Resolution - robustlargemaskinpaintingwithfourier convolutions . In Proceedings of the IEEE / CVF winter conference on applications of computer vision . 2149 ‚Äì 2159 . [ 31 ] Makoto Takamoto , Timothy Praditia , Raphael Leiteritz , Daniel MacKinlay , Francesco Alesiani , Dirk Pfl√ºger , and Mathias Niepert . 2022 . PDEBench : An extensive benchmark for scientific machine learning . Advances in Neural Infor - mation Processing Systems 35 ( 2022 ) , 1596 ‚Äì 1611 . [ 32 ] Rachael Tatman , Jake VanderPlas , and Sohier Dane . 2018 . A practical taxonomy of reproducibility for machine learning research . [ 33 ] Gemini Team , Rohan Anil , Sebastian Borgeaud , Yonghui Wu , Jean - Baptiste Alayrac , Jiahui Yu , Radu Soricut , Johan Schalkwyk , Andrew M Dai , Anja Hauth , et al . 2023 . Gemini : a family of highly capable multimodal models . arXiv preprint arXiv : 2312 . 11805 ( 2023 ) . [ 34 ] Jason Tsay , Todd Mummert , Norman Bobroff , Alan Braz , Peter Westerink , and Martin Hirzel . 2018 . Runway : machine learning model experiment management MLXP : A framework for conducting replicable Machine Learning eXperiments in Python tool . In Conference on systems and machine learning ( sysML ) . [ 35 ] Manasi Vartak , Harihar Subramanyam , Wei - En Lee , Srinidhi Viswanathan , Saadiyah Husnoo , Samuel Madden , and Matei Zaharia . 2016 . ModelDB : a system for machine learning model management . In Proceedings of the Workshop on Human - In - the - Loop Data Analytics . 1 ‚Äì 3 . [ 36 ] Thomas Wei√ügerber and Michael Granitzer . 2019 . Mapping platforms into a new open science model for machine learning . it - Information Technology 61 , 4 ( 2019 ) , 197 ‚Äì 208 . [ 37 ] Omry Yadan . 2019 . Hydra - A framework for elegantly configuring complex applications . https : / / github . com / facebookresearch / hydra [ 38 ] MateiZaharia , AndrewChen , AaronDavidson , AliGhodsi , SueAnnHong , Andy Konwinski , Siddharth Murching , Tomas Nykodym , Paul Ogilvie , Mani Parkhe , et al . 2018 . Accelerating the machine learning lifecycle with MLflow . IEEE Data Eng . Bull . 41 , 4 ( 2018 ) , 39 ‚Äì 45 .