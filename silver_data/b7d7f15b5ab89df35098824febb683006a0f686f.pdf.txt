MLXP : A framework for conducting replicable Machine Learning eXperiments in Python Michael Arbel Univ . Grenoble Alpes , Inria , CNRS , Grenoble INP , LJK 38000 Grenoble , France Alexandre Zouaoui Univ . Grenoble Alpes , Inria , CNRS , Grenoble INP , LJK 38000 Grenoble , France Launcher Conﬁguration options Code - base and Backups Version A Version B sub Job version manager + Logger Reader • Filter • GroupBy • Aggregate • Log _ metadata • Log _ metrics • Log _ artifacts Computing resources Storage Results Figure 1 : Overview of MLXP and its components ABSTRACT Replicability in machine learning ( ML ) research is increasingly con - cerning due to the utilization of complex non - deterministic algo - rithms and the dependence on numerous hyper - parameter choices , such as model architecture and training datasets . Ensuring repro - ducible and replicable results is crucial for advancing the field , yet often requires significant technical effort to conduct systematic and well - organized experiments that yield robust conclusions . Several tools have been developed to facilitate experiment management and enhance reproducibility ; however , they often introduce complex - ity that hinders adoption within the research community , despite being well - handled in industrial settings . To address the challenge of low adoption , we propose MLXP , an open - source , simple , and lightweight experiment management tool based on Python , avail - able at https : / / github . com / inria - thoth / mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility . CCS CONCEPTS • Software and its engineering → Software libraries and repos - itories ; • Computing methodologies → Machine learning . KEYWORDS Machine learning , Replicability , Reproducibility 1 INTRODUCTION In recent years , the field of Machine Learning ( ML ) has experienced rapid advancements in both research and practical applications , ranging from improved algorithms to novel use cases in various industries . Recent breakthroughs , such as Large Language Models , have been projected to yield a potential annual market value of trillions of dollars , according to a report by McKinsey [ 1 ] , under - lining their extensive applicability and the significant economic opportunities they present . Many machine learning models , such as Large Language Models for text and speech generation [ 33 ] and autonomous driving sys - tems [ 14 ] , are extremely challenging to train as they rely on sophis - ticated algorithms and demand extensive computational resources and access to vast databases . As such , these models substantially differ from traditional software systems governed by deterministic rules . ML models are constructed using stochastic methods , intro - ducing randomness through factors such as the choice of initial weights or the order of data presentation to the model . This random - ness makes it challenging to reliably assess the performance of a given method from a single execution of the algorithm . Additionally , training often involves complex optimization procedures with nu - merous hyperparameters like scheduling , momentum , and weight decay , necessitating a systematic search for optimal configurations . These hyperparameters , coupled with architectural choices and data selection , compound the complexity , demanding meticulous organization and execution of potentially numerous experiments to ensure reproducibility and replicability which are essential for making progress in research . A significant portion of data scientists and ML practitioners tend to rely on customary solutions , often characterized as ad hoc and lacking a robust methodology for experiment management [ 15 , 35 ] . As a result , these customary choices may introduce friction in the experimentation process , potentially undermining reproducibility a r X i v : 2402 . 13831v1 [ c s . L G ] 21 F e b 2024 Arbel and Zouaoui and replicability [ 16 ] . For instance , the ability to systematically test ideas and conduct extensive grid searches over hyperparameters is crucial for impartially comparing different methods . Yet , this capability can be compromised when using customary solutions , which often result in cumbersome code . Consequently , the result - ing complexity imposes a psychological burden and an increased workload on practitioners when conducting multiple experiments , thereby introducing frictions to reproducibility and replicability . Careful handling of experiments , coupled with a robust method - ology and efficient tools for experiment management , is necessary to ensure the reproducibility and replicability of results [ 18 ] . Recent years have seen a rise in experiment management tools , such as MLflow , Wandb , Neptune . ai , Comet . ml , and Sacred , which provide extensive functionalities aimed at ensuring the reproducibility and traceability of ML workflows . These tools primarily focus on sys - tematic tracking and logging of various ML assets ( datasets , models , hyperparameters , etc . ) , versioning them , and visualizing and com - paring the results of several experiments . Leveraging these tools can significantly enhance reproducibility and traceability . However , this advantage comes with a considerably steep learning curve and the need to ( re ) organize ML project code - bases to effectively uti - lize them , potentially limiting their adoption within the broader scientific community [ 21 , 22 ] . Specifically , these tools often intro - duce their own set of APIs , utilities , and concepts that researchers must familiarize themselves with and learn to use . Consequently , the learning and integration burden associated with these tools can be prohibitive in research contexts , where projects prioritize agility and flexibility over deployment considerations that enforce strict workflows and conventions well suited to industrial settings . Simple experiment management tools with a low entry barrier , tai - lored to meet the scientific demands of flexibility and frictionless reproducibility , are lacking in the current landscape . There is a need for tools that can offer such a frictionless experience starting from configuration , launching , logging to versioning and exploitation of the results while requiring minimal intrusiveness in practitioners code - base to increase adoption by the research community . We introduce MLXP ( Figure 1 ) , an open - source Python frame - work designed to streamline experiment management , ensuring minimal friction during experimentation and result exploitation while upholding essential reproducibility standards with minimal intrusion . Python is widely recognized as the language of choice for machine learning , given its extensive array of machine - learning - related packages like Jax [ 7 ] or Pytorch [ 24 ] , and its widespread adoption in the research ecosystem . Therefore , we have chosen Python as the native language for MLXP , enabling seamless in - tegration with the code - bases of the majority of machine learning projects . MLXP builds upon Hydra [ 37 ] , a popular ML framework that has garnered increasing attention from the ML research com - munity thanks to its intuitive and concise syntax for configuring and locally launching multiple experiments . MLXP extends Hydra ’s capabilities by offering a comprehensive solution for seamless multiple job submission to job schedulers , logging , version man - agement , searching , and post - processing experiments , ensuring effortless and transparent experiment management . The result is a user - friendly system that prioritizes simplicity and empowers users to conduct rigorous experiments with confidence , fostering a culture of replicability in data science research ( see Table 1 ) . 1 . 0 . 1 Paper Organization . Section 2 discusses existing tools for ex - periment management , highlighting their strengths and limitations in the scientific research context . Section 3 addresses challenges encountered when conducting ML experiments for research and proposes a set of requirements that an experiment management tool needs to satisfy to mitigate these challenges . Section 4 pro - vides background information on Hydra , while Section 5 introduces MLXP and its core components . In Section 6 , we provide examples of research projects that leverage MLXP to easily perform replicable experiments . Finally , we conclude this paper with Section 7 . 2 RELATED WORK 2 . 1 Best practices for replicability in ML Rising concerns over replicability of experiments in ML has spurred the development of new best practices aimed at ensuring a high level of reproducibility and replicability in research findings . Re - cent initiatives include various documentation frameworks for ML models [ 11 , 20 ] and checklists [ 2 ] , which outline the essential assets ( e . g . , datasets , code , experimental results ) necessary to reproduce ML models . Recent work emphasized the importance of utilizing version management tools tailored for ML code , data , and environ - ments to ensure a high level of reproducibility in machine learning [ 3 , 5 , 8 , 17 ] . Furthermore , major AI conferences such as NeurIPS , ICML , and AAAI regularly host reproducibility workshops and encourage researchers to independently verify published research results in an effort to incentivize the adoption of best practices for reproducibility in scientific work [ 28 ] . Despite these efforts , best practices for reproducibility and replicability remain less preva - lent in the research community compared to other organizational settings , as highlighted in a study by [ 27 ] , which surveyed over 300 practitioners from various organizations . The study revealed low adoption rates for crucial practices like automated / systematic hyper - parameter optimization and model selection , which are vital for replicating ML research findings and ensuring their validity . We attribute the limited adoption of such practices in research institu - tions to a mismatch between the complexity required to enforce them and the need for flexibility and simplicity in research envi - ronments . Our proposed framework aims to address this mismatch by providing a lightweight tool that integrates key best practices , assisting practitioners in conducting replicable experiments . 2 . 2 ML experiment management tools An emerging category of tools aims to tackle the complexities associated with managing ML experiments . Practical examples of such tools include MLFlow [ 38 ] , WandB [ 6 ] and NeptuneML , while instances from the literature encompass Deep - water [ 10 ] , Runway [ 34 ] , and ModelKB [ 12 , 13 ] . Most of these tools primarily focus on furnishing functionalities for storing , tracking , and versioning assets across different experiment runs to address concerns like reproducibility [ 19 , 32 ] and traceability [ 21 ] . With the emergence of experiment management tools , conduct - ing systematic comparisons between them has become imperative to identify to what extend they can ensure reproducibility . Several studies aimed to compare these tools based on criteria deemed critical for reproducibility [ 17 – 19 , 36 ] , such as the granularity of tracked / logged information and the level of intrusiveness required MLXP : A framework for conducting replicable Machine Learning eXperiments in Python to access this information . Even though they offer very extensive functionalities such as tracking versioning different types of assets , studies such as Idowu et al . [ 18 ] , concluded that these management tools still lack maturity compared to their traditional software en - gineering counterparts which may limit their adoption by a wider community . Factors contributing to their lack of maturity include friction and overhead incurred during usage due to the necessity of code instrumentation for tracking assets [ 21 , 22 ] . Specifically , many of the most popular experiment management tools like MLFlow , typically lack solutions for handling interactions with computing resources , such as easily launching multiple jobs to a job scheduler or managing delayed job executions , both crucial for frictionless replicability . Even when these solutions are provided , ( ex . Wandb ) they often require mastery of the proposed submission tools and introduce an overhead for practitioners . Additionally , while these frameworks enable the exploitation of results from multiple ex - periments , users may witness performance degradation or scal - ability issues when dealing with a large number of experiments or extensive datasets without careful adjustments . In this work , we introduce a simple and lightweight experiment management tool that addresses some of these limiting factors by significantly simplifying the experimental process with minimal user overhead while ensuring a fundamental level of reproducibility . 3 REPLICABILITY CHALLENGES We identify a set of challenges that can increase frictions in ob - taining replicable experiments in ML research and propose several requirements that an experiment management tool can satisfy to address them . These challenges are as follows : ( C . 1 ) Boiler - plate code related to experiments management . ( C . 2 ) Command line arguments clutter when configuring and launching multiple jobs . ( C . 3 ) Increased cognitive load when keeping track of asynchro - nous and nondeterministic job execution in High Perfor - mance Computing ( HPC ) clusters . ( C . 4 ) Complexity of processing end - results when dealing with several experiments . Challenge C . 1 . Boiler - plate code , i . e . , code that is necessary to perform the task at hand but is not part of the machine learning model design , can significantly degrade code readability , particu - larly when practitioners prioritize rapid iteration over code quality , as is often the case in research . Thus , any attempt to remove boiler - plate code is valuable . For instance , Pytorch Lightning [ 9 ] , a deep learning framework built on top of Pytorch [ 24 ] , has seen a surge in popularity as it has been designed to streamline the often redundant code pieces inherent to Pytorch . While Pytorch Lightning is not designed for managing experi - ments , as it mainly focuses on reducing the complexity of the code needed for learning a model based on Pytorch , its simplicity of use can be an inspiration when designing experiment management tools such as MLXP . An important caveat , however , that is associ - ated with the apparent simplicity of frameworks such as Pytorch Lightning , is the reduced flexibility when it comes to using it for developing methods that go out of the beaten track for which it was not originally designed . Experiment management tools should not reduce flexibility at the cost of simplicity and must instead be easy to integrate to the code - base and grant end - users enough flexibility to accommodate their specific needs while reducing the complexity arising from code management . Challenge C . 2 . Command line arguments clutter is pervasive across data science sub - fields as researchers often need to carry out multiple experiments simultaneously to validate some hypoth - esis or fine - tune hyperparameters . Manually creating the required scripts to be fed to the job scheduler is cumbersome . An ideal candi - date framework for replicable machine learning experiments must include the option to seamlessly submit multiple jobs to a HPC job scheduler . Challenge C . 3 . Practitioners can usually be under heavy cog - nitive load to keep track of changes made to the code - base that led to significant performance improvement , especially when mul - tiple experiments are submitted continuously , while the code is still under development . Any solution that can ease the cognitive burden of machine learning developers is a net positive . In addition , C . 3 refers to the unavoidable interactions with HPC , which can be tedious and error - prone as experiments might be launched after several changes have been introduced into the code - base due to busy computing resources at the time of job submission . This leads to practitioners having to be extremely careful not to contaminate their code with new changes until jobs meant to use the former version have been launched . Any automated code and job version management could be helpful to circumvent this issue . Challenge C . 4 . The ability to process and analyze end - results is as important as the code required to run the method that produced them . As a consequence , exploiting results must be straightfor - ward . This entails the ability to easily query results , as well as perform grouping and aggregation operations . Finally , we observe that logging intermediate results and final outcomes can yield large files that may need to be parsed efficiently . A convenient way to overcome this challenge would be lazily loading , which provides practitioners with a quick overview of all experiments without loading heavy data in memory and allows easy access to specific results when needed . Addressing these challenges is crucial to empower practitioners to perform systematic and rigorous experiments . Therefore , we formulate a list of requirements an experiment management tool should fulfill in order to address the challenges discussed above : ( R . 1 ) Seamless integration to the code - base . ( R . 2 ) Automated code and job version management . ( R . 3 ) Easy configuration and submission of multiple jobs using hierarchical configuration files . ( R . 4 ) Intuitive and efficient result exploitation capabilities . The requirement list above is non - exhaustive as many additional features implemented in existing tools are essential in an industrial context , such as collaborative features , interactive visualizations of results , native support for Cloud computing . However , the four re - quirements we identified above are calibrated to academic research and aim to address practical challenges that arise frequently in that context . Our proposed solution , MLXP aims at addressing the challenges above by implementing the four general requirements to allow friction - less replicability in ML . Arbel and Zouaoui Table 1 : Comparing MLXP with two other popular frameworks for managing experiments Features MLXP MLFlow tracking Weights & Biases Configuration management ✓ ✗ ✗ Hyper - parameter Sweeps ✓ ✗ ✓ Job submission ✓ ✗ ✓ Logging / Tracking ✓ ✓ ✓ Experiment Search / Filtering ✓ ✓ ✓ Experiment Comparison ✓ ✓ ✓ Lazy loading results ✓ ✗ ✗ Code versioning ✓ ✓ ✓ Code version checking ✓ ✗ ✗ Model Versioning ✗ ✓ ✓ Interactive Visualizations ✗ ✓ ✓ Collaboration and Sharing ✗ ✓ ✓ 4 BACKGROUND ON HYDRA Hydra [ 37 ] is a popular open - source framework that simplifies the development of research code . Its main feature is the ability to dynamically create a hierarchical configuration by composition , and override it through configuration files and the command line interface ( CLI ) . This feature results in drastic simplification to the code as passing complex options becomes modular . The simplifi - cations to the code induced by Hydra has made it a popular tool in the ML community with several successful research projects relying on it , such as : ( 1 ) A computer vision research project on developing a large mask inpainting method using Fourier convolutions [ 30 ] . ( 2 ) PDEBench [ 31 ] , which is essentially an extensive bench - mark for scientific machine learning . ( 3 ) A multi - task reinforcement learning research project using context - based representations [ 29 ] . Hydra is itself powered by a hierarchical configuration system based on Python and named OmegaConf that allows for merging configurations from multiple sources ( e . g . , YAML configuration files , python dataclasses or objects , and CLI arguments ) . An example of a YAML configuration file can be found in Listing 1 . It allows users to specify arguments related to various parts of the code - base . Notably , Python objects can be seamlessly instantiated alongside their dedicated arguments . For instance , the model has a config - urable number of units , num _ units . Once processed by Hydra , the configuration file is automatically translated into the relevant Python objects with their corresponding attributes as expected . Furthermore , Hydra allows users to dynamically alter the YAML configuration file , either by modifying existing arguments , or by creating new options on the fly through the command line . 1 seed : 0 2 lr : 10 . 3 num _ epoch : 10 4 model : 5 num _ units : 100 6 data : 7 d _ int : 10 8 device : ' cpu ' Listing 1 : Example of a yaml configuration file ’config . yaml’ Another key aspect of Hydra is the ability to run multiple jobs locally with different arguments using a single command . Hydra is therefore well suited for conducting machine learning research experiments at scale as one can easily sweep through different ex - perimental configurations . All in all , Hydra significantly reduces the need for boiler - plate code and allows practitioners to focus on their core research contributions . Our proposed solution builds on Hydra ’s capabilities to provide a complete experiment manage - ment tool for ML practitioners . 5 MLXP MLXP ( Machine Learning eXperimentalist for Python ) is an open - source Python package whose source code and documentation are available in https : / / github . com / inria - thoth / mlxp and https : / / inria - thoth . github . io / mlxp / and that can be easily installed using PyPi ( see https : / / pypi . org / project / MLXP / ) . MLXP builds upon the pow - erful Hydra framework to offer a comprehensive solution for easily managing multiple experiments in Python to ensure their repro - ducibility and replicability . As an open - source package , MLXP facilitates seamless experiment launching , logging , and result ex - ploitation with unparalleled efficiency . Key components include automated job launching using Hydra and hierarchical configura - tion files , meticulous logging of outputs along with metadata for enhanced replicability , automated code and job version manage - ment , seamless multi - job submission to a HPC job scheduler , and intuitive result exploitation capabilities including querying results , grouping and aggregation operations . Using these components we address the challenges described in Section 3 . The end result is an easy - to - use system that prioritize transparency and empowers users to conduct rigorous experiments with confidence , fostering a culture of replicability and reproducibility in data science research . We provide an overview of MLXP in Section 5 . 1 to illustrate its simplicity , before discussing its major components . 5 . 1 Overview and usage MLXP provides a drop - in decorator mlxp . launch that extends the Hydra decorator hydra . main and allows to dynamically pass a context object ctx to a main task function task _ function defined inside a python script . This decorator is designed to seamlessly MLXP : A framework for conducting replicable Machine Learning eXperiments in Python my _ project / configs / config . yaml mlxp . yaml logs / 1 / 2 / main . py Figure 2 : Project directory structure integrate with existing code - bases , requiring minimal adjustments to the user’s code while offering maximum compatibility and ease of integration ( ex : see the main . py script in Listing 2 ) . 1 import mlxp 2 3 @ mlxp . launch ( config _ path = ' . / configs ' ) 4 def task _ function ( ctx : mlxp . Context ) - > None : 5 config = ctx . config 6 logger = ctx . logger 7 8 print ( " Hello World ! " ) 9 10 if _ _ name _ _ = = " _ _ main _ _ " : 11 task _ function ( ) Listing 2 : Example of a Python script ( ’main . py’ ) using MLXP The context object ctx provides two essential elements for launch - ing the experiment : ( 1 ) a hierarchical experiment configuration object ctx . config similar to Hydra ’s DictConfig structure , that provides information from a default configuration files and can be overridden in the command line , and ( 2 ) a logger object to store the outputs / results of the experiment in a directory automatically created by MLXP and that is unique to a particular run . Using MLXP requires only defining two reserved directories for the project : a configuration directory and a log directory . For instance , Figure 2 shows an example of a project directory structure which contains these two directories in addition to the main python script main . py : . The configuration directory configs contains two yaml files config . yaml and mlxp . yaml from which MLXP ex - tracts the default configuration for the project . The logs / direc - tory , whose location can be customized by the user , is created by default by MLXP under working directory and contains separate sub - directories , each storing information provided to the logger object during a specific run . These sub - directories are named after their order of execution ( 1 , 2 , . . . . ) so as to ensure a unique identifier . Finally , executing Python scripts using MLXP follows the same procedures as Hydra , meaning that regular Python command - lines as simple as python main . py can be used . 5 . 2 Launching Launching experiments effortlessly is paramount in research and development environments . Hydra provides an excellent mech - anism for executing multi - runs with ease , allowing users to pass multiple options through the command line effortlessly . MLXP takes this a step further by seamlessly integrating with job sched - ulers using mlxpsub command . This integration enables users to not only orchestrate and configure experiments effortlessly but also to submit them directly to a job scheduler without manual intervention . The resulting automation enhances productivity and ensures consistent and reliable execution of experiments across different computing environments . 5 . 2 . 1 The mlxpsub command . MLXP provides a shell com - mand ( mlxpsub ) which leverages the multi - run functionalities of Hydra for effortless submission of multiple runs using a single bash script . It is compatible with most of the available job schedulers , such as SLURM , TORQUE , SGE , OAR , MWM and LSF , and simply requires to specify the jobs’ options in the main script using the syntax de - fined by the scheduler . Listing 3 illustrates the simplicity by which several jobs can be specified transparently in a few lines of code using a single bash script . There , it is assumed the user has access to a SLURM job scheduler . Therefore , the job specifications , such as the duration of each job , the number of tasks and the number of cpus per task , follows the scheduler’s syntax as indicated by lines starting with the string # SLURM . Finally , the last line executes the python script main . py with 4 different configuration settings provided using Hydra ’s syntax . There , 4 runs are meant to be exe - cuted , each one corresponding to a tuple of values for the options lr ( 10 . or 1 . ) and seed ( 1 or 2 ) . 1 # ! / bin / bash 2 3 # SLURM - - time = 1 - 00 : 10 : 00 4 # SLURM - - ntasks = 1 5 # SLURM - - cpus - per - task = 1 6 7 python main . py lr = 10 . , 1 . seed = 1 , 2 Listing 3 : Example of a Bash script ( ’script . sh’ ) using MLXP Simply launching the script in Listing 3 using mlxpsub script . sh command will automatically create 4 different jobs , each corre - spondingtoatuple ofoptionsprovidedtothepythonscript main . py , which are then submitted to a scheduler’s queue . By contrast , launching the script in Listing 3 directly using a scheduler’s sub - mission command ( here sbatch script . sh ) will create a single job which successively executes 4 different runs with their corre - sponding tuple of options . 5 . 2 . 2 Alternatives . Without the mlxpsub command , one would have to create a meta script that generates a script similar to script . sh for each option choices and submits it using sbatch command . Such approach does not exploit the multi - run function - ality provided by Hydra and introduces clutter in the job submis - sion code . Other popular alternatives , such as submitit package , allow a simplified job submission to a SLURM scheduler , but re - quires adapting the Python code for using it and does not support command - line submission natively . Note , however , that a plugin version of submitit for Hydra allows command - line submission , although it is specific to SLURM and requires passing the sched - uler’s options in a format that is different from the scheduler’s native one . On the other hand , mlxpsub is versatile as it is compati - ble with several schedulers and transparent because it simply relies Arbel and Zouaoui on the scheduler’s native syntax , with which the user is supposedly already familiar . 5 . 3 Logging Keeping track of information relative to an experiment / run is cru - cial for replicability . The MLXP ’s logger component provides comprehensive logging capabilities that ( 1 ) automatically handles the creation of log directories unique to each run , ( 2 ) systematically stores metadata relative to a run , such as the configurations used during execution , information about the resources used , code ver - sion , etc and ( 3 ) simply allows the user to log additional information about the run such as various metrics , checkpoints or artifacts . All log directories created using MLXP have a similar structure as the one shown in Figure 3 . Below , we discuss key requirements that MLXP satisfies to allow maximal replicability and ease of use . 5 . 3 . 1 Automatic creation of unique log directories . When running multiple experiments in parallel , storing the result of each one sep - arately guarantees that no interference can occur . MLXP ensures the each new experiment has a unique location that is resolved using a simple internal mechanism for keeping track of previous runs in a given log directory . Such a mechanism increments the total number of existing log directories by one to assign a log _ id to each new run . The new run directory is then named after the new log _ id . When submitting multiple jobs to a job scheduler using mlxpsub command discussed in Section 5 . 2 , these jobs are executed asynchronously . A naive approach would be prone to a large I / O concurrency when resolving the log _ id for each job . MLXP avoids these issues altogether by sequentially assigning a log _ id to each job and creating its corresponding log directory before submitting these jobs to a scheduler . Then upon execution , each job is forced to store its outputs in its assigned log directory . As a result , MLXP allows to easily submit a large number of jobs without worrying about clashes between different jobs . 5 . 3 . 2 Comprehensive logging . Storing information such as all the configuration options used to run experiments are basic require - ments for replicability that MLXP handles automatically . There , the object ctx . config is stored in a file config . yaml under the metadata directory of a given run ( see Figure 3 ) . Moreover , in - formation about the execution of the run , such as its ’status’ ( ex : COMPLETED , FAILED , RUNNING ) or MLXP ’s configuration op - tions are stored in separate files ( info . yaml and mlxp . yaml ) to allow a precise monitoring of experiments . For instance , a user might want to handle results coming from incomplete experiments differently to draw correct conclusions . These additional informa - tion are especially useful when several experiments are executed and need to be filtered according to their status as discussed in Sec - tion 5 . 5 . These metadata information complement the metrics and artifacts that are stored by the user using logger . log _ metrics and logger . log _ artifacts . The former method allows storing a list of dictionaries of scalar values in a simple JSON format whereas the latter one allows storing more structured objects such as im - ages , model parameters , etc . Both modalities can be accessed easily thanks to the universal formats used and the simple file struc - ture of the log directories Figure 3 . Finally , due to re - allocation of computing resources , some experiment needs to be momentarily logs / 1 / metadata / config . yaml info . yaml mlxp . yaml metrics / train . json / keys / metrics . yaml artifacts / Checkpoint / lastckpt . pkl 2 / Figure 3 : Log directory structure stopped and then automatically relaunched , as is the case when submitting a job to a besteffort queue of a scheduler . The abil - ity to store generic checkpoints ( in a pickled format ) allows to safely resume the experiment in the exact state in which it was paused . This could be achieved , for instance , by calling the method logger . log _ checkpoint ( ckpt ) where ckpt refers to any user - defined object that contains all the environment and state variables used by a given experiment . 5 . 3 . 3 Alternatives . The logger provided by MLXP induces a mod - erate level of intrusiveness in the code as it requires the user to use call methods such as log _ metrics inside the code - base . This approach is quite common to most experiment management tools and does not constitute a strong constraint in the context of scien - tific / academic projects . Some tools such as ModelKB , MLFlow , and Wandb support automatic asset collection in non - intrusive ways . However , these approaches require explicit support for machine learning frameworks , such as Pytorch [ 24 ] and SciKit Learn [ 25 ] , and appear to be constraining in some research contexts where new insights are obtained from tracking non - standard quantities of interest . 5 . 4 Job versioning In typical machine learning research projects , there is a constant iteration between code development and experimental validation . This iterative process provides flexibility to test new ideas and refine them based on partial empirical results . While this agile development approach can accelerate scientific research , it also increases the likelihood of erroneous conclusion about a given method due to misreporting experimental results . Misreporting is likely to occur when data from different code versions are combined potentially altering the interpretation of the experiments . Although unintended , this constitutes a breach of the scientific method and can result in significant wasted time and resources . MLXP proposes a systematic way to version the experiments and to guarantee they were obtained using a specific version of the code - base . Before describing MLXP ’s versioning mechanism , we first illustrate how MLXP : A framework for conducting replicable Machine Learning eXperiments in Python Scheduler HPC Code base Job 1 Version A Version B Launcher Submission 1 T i m e Job 2 Job 3 Job 3 Job 4 Job 1 Job 2 Job 3 Job 4 Submission 2 T1 T2 Figure 4 : Execution of non - versioned jobs a lack of experiment versioning can have unintended consequences when using HPC clusters and job schedulers . 5 . 4 . 1 Parallel job submissions without versioning . A typical situ - ation the users can be confronted to in practice arises when sub - mitting multiple jobs to a busy HPC cluster . In such cases , jobs are placed in a scheduler’s queue until the required resources become available . Consequently , jobs in the queue may run asynchronously , with an uncertain start time , while still relying on a shared code - base . This code - base is likely to undergo changes even before the jobs are executed , as illustrated in Figure 5 . There , three jobs are first submitted to a scheduler at time 𝑇 1 and intended to use the cur - rent code version ( version 𝐴 ) . However , due to resource constraints , only the first two jobs are executed with the correct version at 𝑇 1 . The third job ( job 3 ) is deferred to time 𝑇 2 , at which point a new code version ( version 𝐵 ) becomes available , and a new job ( job 4 ) , intended to use the latest code version , is submitted to the queue . Consequently , both jobs 3 and 4 are executed using the current code version ( version 𝐵 ) . Had sufficient resources been available at 𝑇 1 , job 3 would have used version 𝐴 of the code . Thus , depending on resource availability , the same job submitted to an HPC cluster may be executed using different code versions . Next , we discuss how MLXP addresses this replicability issue . 5 . 4 . 2 MLXP ’s version manager . We introduce a version manager that builds on Git to address the replicability issue that might arise due to a delayed execution of jobs . MLXP ’s version manager operates by versioning the code with Git and associating all jobs submitted to a scheduler at a specific time with a backup copy of the code based on the latest commit available . When the requested resources become available for a job , it is executed from the linked backup copy rather than the current code - base , which may have changed meanwhile . To prevent redundant copies of the code , each backup is linked to a unique commit through the Git commit hash . Thus , only a single backup copy of the code is created as long as the code didn’t change , even if multiple jobs are submitted . Figure 5 illustrates how linking jobs to code backups ensures that each job is executed using the intended code version , irrespective of changes in the current version of the main code - base . In this scenario , even though job 3 was initially linked to version A , it will still be executed using that same version from the backup copy , despite its execution being delayed until time 𝑇 2 , when the current code - base has evolved to version B . This enhanced traceability of experiments increases their replicability in the context of large HPC Scheduler HPC Code base Job 1 / v A Version A Launcher Submission 1 T i m e Job 2 / v A Job 3 / v A Job 3 / v A Job 4 / v B Job 1 Job 2 Job 3 Job 4 Submission 2 Version A Version B Version B Version A Backup T1 T2 Figure 5 : Execution of versioned jobs clusters . Furthermore , as a byproduct of this traceability , it becomes easier to pinpoint potential bugs that could have been introduced at subsequent versions of the code . This is the case , if for instance , in Figure 5 , the new job ( job 4 ) is expected to give the same results as ( job 3 ) even though it is using a new version of the code ( version B ) . Any discrepancy in the results , can alert the user about a potential bug that could either be introduced in version B or that was present in version A and was inadvertently fixed later . 5 . 4 . 3 Code version checking . The effectiveness of the proposed strategy relies upon a rigorous versioning of the code - base using Git , which might not be always guaranteed , especially in a context of fast - paced code development and limited exposure to version control practices . To overcome these limitations , MLXP offers op - tional version control checks that can be used in an interactive manner . When these are enabled and upon launching a script that uses MLXP , an interactive session is created where MLXP checks if the current Git repository containing the code - base has untracked files or uncommitted changes . It then asks the user if they want to add untracked files and create an automatic commit . Only after that , the jobs are created dynamically based on the user’s choices and are submitted to a job scheduler when available . While this procedure is optional , it can guarantee optimal versioning and traceability of the experiments , which can save a lot of time and resources especially when each experiment is computationally demanding . 5 . 5 Reading The MLXP components discussed so far focused on facilitating the submission of multiple jobs , ensuring their traceability and correct execution through rigorous versioning , and maintaining compre - hensive records of all pertinent information about each experiment , including configuration details , results , and run status . With these components , users can effortlessly submit a large number of experi - ments , resulting in a substantial amount of data . Properly designed tools are essential for efficiently managing and exploiting this data . In an effort to provide a comprehensive framework for manag - ing experiment , we propose a reader component that allows easy filtering of experimental results as well as simple grouping and aggregation operations . The reader component also offers a lazy evaluation mechanism that avoids storing in memory potentially large amount of data produced by the experiments and only loading them when necessary . Below we describe the main functionalities of the reader component . Arbel and Zouaoui 5 . 5 . 1 Filtering results . Filtering results of multiple experiments using MLXP is as easy as creating a reader object linked to the logs directory of all runs and providing a query string that allows to filter results based on information stored in the metadata directory created by MLXP logger for each run ( see Listing 4 ) . The syntax for querying is close to Python’s syntax for boolean operators and allows comparison operations ( ’ = = ’ , ’ ! = ’ , ’ < ’ , ’ > ’ , ’ < = ’ , ’ > = ’ ) , logical operations ( and ( ’ & ’ ) , or ( ’ | ’ ) , not ( ’ ∼ ’ ) ) , and membership ( ’in’ ) . It also supports operation precedence using parenthesis and follows similar precedence rules as in Python which allows to easily combine all these comparison operations to create sophisticated queries . For ease of use , the list of keys that can be used in a query are provided by the attribute reader . searchable of the reader . 1 In [ 1 ] : import mlxp 2 In [ 2 ] : # Creating a reader object . 3 reader = mlxp . Reader ( ' . / logs / ' ) 4 In [ 3 ] : # Searching using a query string 5 . . . query = " info . status = = ' COMPLETE ' & config . optimizer . lr < = 1 . " 6 . . . results = reader . filter ( query _ string = query ) Listing 4 : Example of an interactive IPython shell to read and filter experiment results using MLXP 5 . 5 . 2 Loading results . Once filtered , results are represented as a dataframe - like object , as shown in Listing 5 , where each row repre - sents a different run in the logs directory , whereas each column rep - resents either a metadata field appearing in at least one of the runs or a metric that was stored by calling the method log _ metrics of MLXP ’s logger object . While metadata are typically scalar quanti - ties that can readily be stored in the dataframe - like object results , metrics are typically arrays of arbitrary size that could be cum - bersome to load in the dataframe for each run . Instead , metrics are “lazily evaluated " , meaning that they are simply linked to the location where they are stored and are only loaded when the user explicitly tries to access them . Hence the values of these fields are marked as “LAZYDATA " and produce an output array whenever direct access for a particular run and metric is performed . Such a lazy evaluation allows a lightweight handling of large amounts of information while still retaining simplicity and flexibility . Finally , it is always possible to easily convert the MLXP dataframe - like object into a Pandas dataframe [ 23 ] for convenience . In that case , however , “lazy evaluation " is no longer possible . Instead , all data are loaded in memory . 1 In [ 4 ] : # Display results 2 . . . : results 3 Out [ 4 ] : 4 config . seed config . lr . . . train . iter train . loss 5 0 0 1 . . . . LAZYDATA LAZYDATA 6 0 1 1 . . . . LAZYDATA LAZYDATA 7 In [ 5 ] : # Direct access 8 . . . : results [ 0 ] [ ' train . iter ' ] 9 Out [ 5 ] : 10 [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Listing 5 : Displaying results and directly accessing “lazily " evaluated fields using MLXP 5 . 5 . 3 Grouping and aggregation . It is often useful to group / aggre - gate results by values taken by some fields of the dataframe . While all these operations are possible using frameworks such as Pan - das , we provide some basic support for those that is compatible with the lazy evaluation functionality of MLXP reader . Listing 6 illustrate how to perform a grouping operation on the dataframe given a list of group keys and using the method groupBy which returns a dictionary of dataframes whose keys correspond to the different group values . The group keys can contain several fields of the dataframe in which case the groups are indexed by a tuple of values corresponding to these fields . It is worth noting that the metrics are still lazily evaluated , which allows to perform complex grouping operations without unnecessary memory overload . 1 In [ 6 ] : # List of group keys . 2 . . . group _ keys = [ ' config . lr ' ] 3 In [ 7 ] : # Grouping the results 4 . . . : grouped _ res = results . groupBy ( group _ keys ) 5 . . . : grouped _ res 6 Out [ 7 ] : 7 config . seed . . . train . iter train . loss 8 config . lr 9 1 . 0 . . . LAZYDATA LAZYDATA 10 1 . . . LAZYDATA LAZYDATA Listing 6 : Example of an interactive IPython shell showcasing the grouping capabilities of MLXP Finally , once groups are formed , it is possible to perform simple aggregation operations . Listing 7 illustrate how to average the field train . loss over members of each group using the method aggregate and the map AvgStd provided by MLXP . The opera - tion returns a dataframe containing the initial group keys and the results of aggregation . Here , lazy evaluation is no longer performed since computing the aggregated result requires directly accessing the metric train . loss . 1 In [ 8 ] : # Creating the aggregation maps 2 . . . from mlxp . data _ structures . contrib . aggregation _ maps import AvgStd 3 . . . agg _ maps = [ AvgStd ( ' train . loss ' ) ] 4 In [ 9 ] : # Aggregating the results 5 . . . : agg _ res = grouped _ res . aggregate ( agg _ maps ) 6 . . . : agg _ results 7 Out [ 9 ] : 8 config . lr . . . train . loss _ avg 9 0 1 . . . . [ 0 . 03 , . . . , 0 . 001 ] Listing 7 : Example of an interactive IPython shell to aggregate results using aggregation maps from MLXP 5 . 5 . 4 Alternatives . MLFlow [ 38 ] and Weights & Biases ( W & B ) [ 6 ] provide filtering capabilities that are similar to MLXP but no direct support for grouping or aggregation operations which are usually performed using frameworks such as Pandas . When dealing with large amount of output data , conversion of raw experimental re - sults into Pandas format to perform these operations can result in performance degradation and scalability issues since all results are MLXP : A framework for conducting replicable Machine Learning eXperiments in Python loaded in memory . The fully integrated support for these opera - tions in MLXP allows users to easily exploit and compare multiple results with a minimal code overhead and memory usage . 6 APPLICATIONS In this section , we provide two examples of research projects that build upon MLXP to perform replicable experiments . 6 . 1 HySUPP [ 26 ] HySUPP is an open - source python toolbox for hyperspectral unmix - ing practitioners . In essence , HySUPP enables seasoned researchers , curious students , and teachers to easily experiment with various hyperspectral unmixing methods with replicability in mind thanks to MLXP , as indicated in the Github repository of the code asso - ciated with the paper https : / / github . com / BehnoodRasti / HySUPP . HySUPP leverages basic functionalities including effortless launch - ing , comprehensive logging capabilities such as evaluation met - rics specific to unmixing and the custom - made Estimate artifact , alongside the reader component to easily filter experimental re - sults before grouping and aggregating them . Notably , the reader component enables end - users to create informative plots such as Figure 6 using minimal boiler - plate code in order to visualize the performance of several unmixing techniques , grouped by their su - pervision setup ( i . e . , supervised , semi - supervised , or blind ) where 5 runs have been aggregated so as to create error bars corresponding to the standard deviation from the mean value when accounting for randomness in data generation and model initialization . An example of a command - line instruction can be found in Listing 8 , which highlights its intuitive usage , since users may simply change the unmixing mode , data , or model depending on their needs . C N N A E U E D AA M S N e t P G M S U A D M M N e t B a y e s i a n S M A N M F Q M V M i S i C N e t S U n S A L C L S U n S A L M U A _ S L I C S 2 W S U S U n C NN S U n AA S I S A L + F C L S S I S A L + U n D I P S i V M + F C L S V C A + F C L S V C A + U n D I P S i V M + U n D I P Unmixing models 0 5 10 15 20 A b un d a n c e s S R E ( d B ) 6 . 32 6 . 7 8 . 79 13 . 12 13 . 49 18 . 04 20 . 23 21 . 04 10 . 3 11 . 06 16 . 09 16 . 4 19 . 13 21 . 26 18 . 77 18 . 94 19 . 84 20 . 43 20 . 89 21 . 18 Abundances SRE on DC1 ( SNR = 30dB ) blindsemi - supervised supervised Figure 6 : Sub - figure originating from [ 26 ] 1 $ python unmixing . py mode = semi data = DC1 model = EDAA Listing 8 : HySUPP command - line arguments 6 . 2 Benchmarking optimization algorithms MLXP has recently been used to conduct experiments for com - paring a number of optimization algorithms for deep learning . In particular , Arbel et al . [ 4 ] compared two classes of optimization methods : Gradient descent and Gauss - Newton , to provide empirical insights on the effect of these algorithms on the generalization ca - pabilities of over - parameterized networks . There , 720 independent runs totaling in 3600 GPU hours were submitted to an HPC cluster using MLXP as indicated in the Github repository of the code as - sociated with the paper https : / / github . com / MichaelArbel / Implicit - Bias - Gauss - Newton . Figure 7 shows the results of a comparison between different learning algorithms depending on the standard variation of the initial parameters . Each point in the figure was obtained using 5 independent runs each of which trains the model using a different method , totalling in 105 independent runs . Listing 9 shows a simple bash script that can be launched using mlxpsub and that submits all runs to an HPC cluster using OAR . 10 2 10 1 10 0 10 1 10 2 Initial std of the weights 10 4 10 3 10 2 L o s s Test loss at convergence Random Features 1 - HL : Gradient Descent 1 - HL : Gauss - Newton Figure 7 : Sub - figure originating from [ 4 ] 1 # ! / bin / bash 2 3 # OAR - l core = 1 , walltime = 00 : 30 : 00 4 # OAR - t besteffort 5 # OAR - p gpumem > ' 16000 ' 6 7 python main . py std = 0 . 01 , 0 . 1 , 0 . 5 , 1 . , 5 , 10 , 100 \ 8 seed = 0 , 1 , 2 , 3 , 4 \ 9 method = RF , GD , GN \ Listing 9 : Bash script for job submission Once all the runs are complete , it is possible to extract the desired results using MLXP ’s reader module as shown in Listing 10 . This example illustrates the ease by which experiments can be systemati - cally performed , starting from job submissions to results extraction . 1 import mlxp 2 from mlxp . data _ structures . contrib . aggregation _ maps import AvgStd 3 4 reader = mlxp . Reader ( " . / logs / " ) 5 results = reader . filter ( query _ string = " " ) 6 7 # Averaging over seeds 8 agg _ maps = [ AvgStd ( ' test . loss ' ) ] 9 group _ keys = [ ' config . method ' , ' config . std ' ] 10 agg _ res = results . groupBy ( group _ keys ) . aggregate ( agg _ maps ) Listing 10 : Example of code for extracting results Arbel and Zouaoui 7 CONCLUSION In conclusion , MLXP presents a comprehensive solution to the complexities of managing machine learning experiments within a research environment . Its feature - rich framework streamlines experiment launching , logging , and tracking , while facilitating ef - ficient job submission to cluster environments and offering job versioning capabilities . With a strong emphasis on reproducibility and user - friendliness , MLXP simplifies experiment management tasks , allowing researchers to focus on their primary objectives . While MLXP currently lacks explicit support for data or model versioning , as well as interactive visualization and collaborative features , its core focus on simplicity lays a foundation for future development . Potential enhancements could include increased sup - port for versioning and a reduction in logging intrusiveness to offer more flexibility to users . As MLXP continues to gain adoption within the machine learning community , it holds the promise of improving research productivity . By addressing the challenges asso - ciated with experiment management , MLXP empowers researchers to conduct rigorous and reproducible studies , ultimately advancing the state of the art in machine learning research . 8 ACKNOWLEDGMENTS This work was supported in part by the ANR BONSAI project ( grant ANR - 23 - CE23 - 0012 - 01 ) . REFERENCES [ 1 ] 2021 ( accessedAugust , 2021 ) . NotesfromtheAIFrontierInsightsfromHundredsof Use Cases . https : / / www . mckinsey . com / featured - insights / artificial - intelligence / notes - from - the - ai - frontier - applications - and - value - of - deep - learning [ 2 ] 2021 ( accessed August , 2021 ) . The Machine Learning Reproducibility Checklist . https : / / www . cs . mcgill . ca / ~ jpineau / ReproducibilityChecklist . pdf [ 3 ] Saleema Amershi , Andrew Begel , Christian Bird , Robert DeLine , Harald C . Gall , Ece Kamar , Nachiappan Nagappan , Besmira Nushi , and Thomas Zimmermann . 2019 . Software engineering for machine learning : a case study . In Proceedings of the 41st International Conference on Software Engineering : Software Engineering in Practice , ICSE ( SEIP ) 2019 , Montreal , QC , Canada , May 25 - 31 , 2019 , Helen Sharp and Mike Whalen ( Eds . ) . IEEE / ACM , 291 – 300 . [ 4 ] Michael Arbel , Romain Menegaux , and Pierre Wolinski . 2023 . Rethinking Gauss - Newton for learning over - parameterized models . Advances in neural information processing systems ( 2023 ) . [ 5 ] Amine Barrak , Ellis E . Eghan , and Bram Adams . 2021 . On the Co - evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects . In 28th IEEE International Conference on Software Analysis , Evolution and Reengineering , SANER 2021 , Honolulu , HI , USA , March 9 - 12 , 2021 . IEEE , 422 – 433 . [ 6 ] Lukas Biewald . 2020 . Experiment Tracking with Weights and Biases . https : / / www . wandb . com / Software available from wandb . com . [ 7 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman - Milne , and Qiao Zhang . 2018 . JAX : composable transformations of Python + NumPy programs . http : / / github . com / google / jax [ 8 ] Boyuan Chen , Mingzhi Wen , Yong Shi , Dayi Lin , Gopi Krishnan Rajbahadur , and Zhen Ming Jiang . 2022 . Towards training reproducible deep learning models . In Proceedings of the 44th International Conference on Software Engineering . 2202 – 2214 . [ 9 ] William Falcon and The PyTorch Lightning team . 2019 . PyTorch Lightning . https : / / doi . org / 10 . 5281 / zenodo . 3828935 [ 10 ] Rudolf Ferenc , Tamás Viszkok , Tamás Aladics , Judit Jász , and Péter Hegedűs . 2020 . Deep - water framework : The Swiss army knife of humans working with machine learning models . SoftwareX 12 ( 2020 ) , 100551 . [ 11 ] TimnitGebru , JamieMorgenstern , BrianaVecchione , JenniferWortmanVaughan , Hanna M . Wallach , Hal Daumé III , and Kate Crawford . 2018 . Datasheets for Datasets . CoRR abs / 1803 . 09010 ( 2018 ) . arXiv : 1803 . 09010 http : / / arxiv . org / abs / 1803 . 09010 [ 12 ] Gharib Gharibi , Vijay Walunj , Rakan Alanazi , Sirisha Rella , and Yugyung Lee . 2019 . Automated management of deep learning experiments . In Proceedings of the 3rd International Workshop on Data Management for End - to - End Machine Learning . 1 – 4 . [ 13 ] Gharib Gharibi , Vijay Walunj , Sirisha Rella , and Yugyung Lee . 2019 . Modelkb : towards automated management of the modeling lifecycle in deep learning . In 2019 IEEE / ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering ( RAISE ) . IEEE , 28 – 34 . [ 14 ] Sorin Mihai Grigorescu , Bogdan Trasnea , Tiberiu T . Cocias , and Gigel Macesanu . 2020 . A survey of deep learning techniques for autonomous driving . J . Field Robotics 37 , 3 ( 2020 ) , 362 – 386 . [ 15 ] Charles Hill , Rachel Bellamy , Thomas Erickson , and Margaret Burnett . 2016 . Trials and tribulations of developers of intelligent systems : A field study . In 2016 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 162 – 170 . [ 16 ] Matthew Hutson . 2018 . Artificial intelligence faces reproducibility crisis . Science ( New York , N . Y . ) 359 ( 02 2018 ) , 725 – 726 . https : / / doi . org / 10 . 1126 / science . 359 . 6377 . 725 [ 17 ] Samuel Idowu , Daniel Strüber , and Thorsten Berger . 2021 . Asset Management in Machine Learning : A Survey . In 43rd IEEE / ACM International Conference on Software Engineering : Software Engineering in Practice , ICSE ( SEIP ) 2021 , Madrid , Spain , May 25 - 28 , 2021 . IEEE , 51 – 60 . [ 18 ] Samuel Idowu , Daniel Strüber , and Thorsten Berger . 2022 . Asset management in machine learning : State - of - research and state - of - practice . Comput . Surveys 55 , 7 ( 2022 ) , 1 – 35 . [ 19 ] Richard Isdahl and Odd Erik Gundersen . 2019 . Out - of - the - box reproducibility : A survey of machine learning platforms . In 2019 15th international conference on eScience ( eScience ) . IEEE , 86 – 95 . [ 20 ] MargaretMitchell , SimoneWu , AndrewZaldivar , ParkerBarnes , LucyVasserman , Ben Hutchinson , Elena Spitzer , Inioluwa Deborah Raji , and Timnit Gebru . 2019 . Model Cards for Model Reporting . In Proceedings of the Conference on Fairness , Accountability , and Transparency , FAT * 2019 , Atlanta , GA , USA , January 29 - 31 , 2019 , danah boyd and Jamie H . Morgenstern ( Eds . ) . ACM , 220 – 229 . [ 21 ] Marçal Mora - Cantallops , Salvador Sánchez - Alonso , Elena García - Barriocanal , and Miguel - Angel Sicilia . 2021 . Traceability for trustworthy ai : A review of models and tools . Big Data and Cognitive Computing 5 , 2 ( 2021 ) , 20 . [ 22 ] Alexandru A Ormenisan , Mahmoud Ismail , Seif Haridi , and Jim Dowling . 2020 . Implicit provenance for machine learning artifacts . Proceedings of MLSys 20 ( 2020 ) . [ 23 ] The pandas development team . 2020 . pandas - dev / pandas : Pandas . https : / / doi . org / 10 . 5281 / zenodo . 3509134 [ 24 ] AdamPaszke , SamGross , FranciscoMassa , AdamLerer , JamesBradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Köpf , Edward Yang , Zach DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 . PyTorch : An Imperative Style , High - Performance Deep Learning Library . _ eprint : 1912 . 01703 . [ 25 ] Fabian Pedregosa , Gaël Varoquaux , Alexandre Gramfort , Vincent Michel , Bertrand Thirion , Olivier Grisel , Mathieu Blondel , Peter Prettenhofer , Ron Weiss , Vincent Dubourg , et al . 2011 . Scikit - learn : Machine learning in Python . Journal of machine learning research 12 , Oct ( 2011 ) , 2825 – 2830 . [ 26 ] Behnood Rasti , Alexandre Zouaoui , Julien Mairal , and Jocelyn Chanussot . 2023 . Image Processing and Machine Learning for Hyperspectral Unmixing : An Overview and the HySUPP Python Package . arXiv preprint arXiv : 2308 . 09375 ( 2023 ) . [ 27 ] Alex Serban , Koen van der Blom , Holger Hoos , and Joost Visser . 2020 . Adoption and effects of software engineering best practices in machine learning . In Pro - ceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement ( ESEM ) . 1 – 12 . [ 28 ] Koustuv Sinha , Maurits Bleeker , Samarth Bhargav , Jessica Zosa Forde , Sharath Chandra Raparthy , Jesse Dodge , Joelle Pineau , and Robert Stojnic . 2023 . ML Reproducibility Challenge 2022 . ReScience C 9 , 2 ( July 2023 ) . [ 29 ] Shagun Sodhani , Amy Zhang , and Joelle Pineau . 2021 . Multi - task reinforce - ment learning with context - based representations . In International Conference on Machine Learning . PMLR , 9767 – 9779 . [ 30 ] Roman Suvorov , Elizaveta Logacheva , Anton Mashikhin , Anastasia Remizova , ArseniiAshukha , AlekseiSilvestrov , NaejinKong , HarshithGoka , KiwoongPark , andVictorLempitsky . 2022 . Resolution - robustlargemaskinpaintingwithfourier convolutions . In Proceedings of the IEEE / CVF winter conference on applications of computer vision . 2149 – 2159 . [ 31 ] Makoto Takamoto , Timothy Praditia , Raphael Leiteritz , Daniel MacKinlay , Francesco Alesiani , Dirk Pflüger , and Mathias Niepert . 2022 . PDEBench : An extensive benchmark for scientific machine learning . Advances in Neural Infor - mation Processing Systems 35 ( 2022 ) , 1596 – 1611 . [ 32 ] Rachael Tatman , Jake VanderPlas , and Sohier Dane . 2018 . A practical taxonomy of reproducibility for machine learning research . [ 33 ] Gemini Team , Rohan Anil , Sebastian Borgeaud , Yonghui Wu , Jean - Baptiste Alayrac , Jiahui Yu , Radu Soricut , Johan Schalkwyk , Andrew M Dai , Anja Hauth , et al . 2023 . Gemini : a family of highly capable multimodal models . arXiv preprint arXiv : 2312 . 11805 ( 2023 ) . [ 34 ] Jason Tsay , Todd Mummert , Norman Bobroff , Alan Braz , Peter Westerink , and Martin Hirzel . 2018 . Runway : machine learning model experiment management MLXP : A framework for conducting replicable Machine Learning eXperiments in Python tool . In Conference on systems and machine learning ( sysML ) . [ 35 ] Manasi Vartak , Harihar Subramanyam , Wei - En Lee , Srinidhi Viswanathan , Saadiyah Husnoo , Samuel Madden , and Matei Zaharia . 2016 . ModelDB : a system for machine learning model management . In Proceedings of the Workshop on Human - In - the - Loop Data Analytics . 1 – 3 . [ 36 ] Thomas Weißgerber and Michael Granitzer . 2019 . Mapping platforms into a new open science model for machine learning . it - Information Technology 61 , 4 ( 2019 ) , 197 – 208 . [ 37 ] Omry Yadan . 2019 . Hydra - A framework for elegantly configuring complex applications . https : / / github . com / facebookresearch / hydra [ 38 ] MateiZaharia , AndrewChen , AaronDavidson , AliGhodsi , SueAnnHong , Andy Konwinski , Siddharth Murching , Tomas Nykodym , Paul Ogilvie , Mani Parkhe , et al . 2018 . Accelerating the machine learning lifecycle with MLflow . IEEE Data Eng . Bull . 41 , 4 ( 2018 ) , 39 – 45 .