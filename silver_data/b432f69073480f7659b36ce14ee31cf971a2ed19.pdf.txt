Building a Conversational Agent Overnight with Dialogue Self - Play Pararth Shah 1 , Dilek Hakkani - T¨ur , Gokhan T¨ur , Abhinav Rastogi , Ankur Bapna , Neha Nayak , Larry Heck Google AI Mountain View , CA , USA Abstract We propose Machines Talking To Machines ( M2M ) , a framework combining automation and crowdsourcing to rapidly bootstrap end - to - end dialogue agents for goal - oriented dia - logues in arbitrary domains . M2M scales to new tasks with just a task schema and an API client from the dialogue system developer , but it is also customizable to cater to task - speciﬁc interactions . Compared to the Wizard - of - Oz approach for data collection , M2M achieves greater diversity and coverage of salient dia - logue ﬂows while maintaining the naturalness of individual utterances . In the ﬁrst phase , a simulated user bot and a domain - agnostic sys - tem bot converse to exhaustively generate di - alogue “outlines” , i . e . sequences of template utterances and their semantic parses . In the second phase , crowd workers provide contex - tual rewrites of the dialogues to make the ut - terances more natural while preserving their meaning . The entire process can ﬁnish within a few hours . We propose a new corpus of 3 , 000 dialogues spanning 2 domains collected with M2M , and present comparisons with pop - ular dialogue datasets on the quality and diver - sity of the surface forms and dialogue ﬂows . 1 Introduction Goal - oriented dialogue agents trained using super - vised learning methods work best when trained on dialogues of the same task . However , when de - veloping a dialogue agent to assist a user for com - pleting a new task , for example scheduling a doc - tors appointment via an online portal , a dataset of human - agent dialogues for that task may not be available since no dialogue agent exists for inter - acting with that particular API . One popular ap - proach is to collect and annotate free - form dia - logues via crowdsourcing using a Wizard - of - Oz 1 Correspondence to pararth @ google . com Figure 1 : Our proposed M2M framework : ( 1 ) the di - alogue developer provides a task schema and an API client , ( 2 ) automated bots generate dialogue outlines , ( 3 ) crowd workers rewrite the utterances and validate slot spans , ( 4 ) a dialogue model is trained with super - vised learning on the dataset . The whole process can complete in under 8 hours . setup ( Wen et al . ( 2016 ) ; Asri et al . ( 2017 ) ) . How - ever , this is an expensive and lossy process as the free - form dialogues collected from crowdworkers ( i ) might not cover all the interactions that the agent is expected to handle , ( ii ) might contain dia - logues unﬁt for use as training data ( for instance if the crowd workers use language that is either too simplistic or too convoluted ) , and ( iii ) may have a r X i v : 1801 . 04871v1 [ c s . A I ] 15 J a n 2018 errors in dialogue act annotations , requiring an ex - pensive manual ﬁltering and cleaning step by the dialogue developer . Another approach , popular among consumer - facing voice assistants , is to enable third - party developers to build dialogue “experiences” or “skills” focusing on individual tasks ( e . g . Di - alogFlow 1 , Alexa Skills 2 , wit . ai 3 ) . This provides the dialogue developer with full control over how a particular task is handled , allowing her to in - crementally add new features to that experience . However , this approach relies heavily on the de - veloper to engineer every aspect of the conversa - tional interaction and anticipate all ways in which users might interact with the agent for completing that task . It is desirable to expand this approach to make it more data - driven , bringing it closer to the Wizard - of - Oz approach popular in the dialogue re - search community . We present Machines Talking To Machines ( M2M ) , a functionality - driven process for training dialogue agents . The primary goal is to reduce the cost and effort required to build dialogue datasets by automating the task - independent steps so that a dialogue developer is required to provide only the task - speciﬁc aspects of the dialogues . Another goal is to obtain a higher quality of dialogues in terms of : ( i ) diversity of language as well as dia - logue ﬂows , ( ii ) coverage of all expected user be - haviors , and ( iii ) correctness of supervision labels . Finally , this framework is aimed towards boot - strapping dialogue agents up to the point where they can be deployed to serve real users with an acceptable task completion rate , after which they should be improved directly from user feedback using reinforcement learning . Previous work for building semantic parsers ( Wang et al . ( 2015 ) ) and parsers for mapping natural language questions to structured queries ( Zhong et al . ( 2017 ) ) rely on crowd sourcing to map automatically generated structured repre - sentations to single - shot natural language utter - ances . However , generating multi - turn dialogues in this manner requires co - ordination among mul - tiple participating agents . Inspired by recent AI game - playing literature ( Silver et al . ( 2016 , 2017 ) ) , we introduce a notion of “dialogue self - play” where two or more conversational agents in - 1 https : / / dialogﬂow . com 2 https : / / developer . amazon . com / alexa - skills - kit 3 https : / / wit . ai teract by choosing discrete conversational actions to exhaustively generate dialogue histories . In this work , we employ an agenda - based user simulator agent ( Schatzmann et al . ( 2007 ) ) and a ﬁnite state machine based system agent for the self - play step . In Section 2 we describe the mechanics of M2M and in Sections 3 and 4 we discuss the user simula - tion and crowdsourcing aspects of our method . In Section 5 we present datasets collected with this framework that we are releasing with this paper and in Section 6 we evaluate our approach by com - paring our datasets with popular dialogue datasets . We conclude with a discussion of related work in Section 7 . 2 M2M At a high level ( Figure 1 ) , M2M connects a de - veloper , who provides the task - speciﬁc informa - tion , and a framework , which provides the task - independent information , for generating dialogues centered around completing the task . Formally , the framework F maps a task speciﬁcation T to a set of dialogues D : F ( T ) → D = { d i , i ∈ 1 . . . N } ( 1 ) d i = [ ( u i 1 , . . . , u in i ) , ( a i 1 , . . . , a in i ) ] ( 2 ) Each dialogue d i is a sequence of natural lan - guage utterances ( or dialogue turns ) u ij and their corresponding annotations a ij . A dialogue turn an - notation a ij includes the semantic parse of that turn as well as additional information tied to that turn , for example who spoke at that turn and the dia - logue state at that point in the dialogue . 2 . 1 Dialogue task speciﬁcation The input to the framework is a task speciﬁca - tion obtained from the dialogue developer , which deﬁnes the scope of the dialogue interactions for the task in question . Dialogue agents can be em - ployed to complete a wide variety of tasks . In this work we focus on database querying applications , which involve a relational database which contains entities that the user would like to browse and se - lect through a natural language dialogue . This for - mulation covers a large variety of tasks that are expected of automated dialogue agents , including all tasks that map to ﬁlling a form and executing a transaction on some website . The attributes of the entities ( i . e . columns of the database ) induce Figure 2 : Example of generating an outline and its paraphrase . See text for details . a schema S of “slots” . Each slot could be a con - straint that the user cares about when selecting an entity . The developer must provide an API client C which can be queried with a SQL - like syntax to return a list of matching candidate entities for any valid combination of slot values . The task schema and API client together form the task speciﬁcation , T = ( S , C ) . ( Figure 2a . ) Dialogues that involve procedural turn - taking ( for example reading out a recipe or playing text - based games ) , or deal with unstructured knowl - edge ( for example question answering over a text document ) , are among tasks that are not covered by this formulation . These classes of dialogues can be handled by modifying the self - play phase of the framework to generate outlines for these di - alogue types . 2 . 2 Outline generation via self - play With the task speciﬁcation , the framework must generate a set of dialogues centered around that task . We divide this into two separate steps , F = F 2 ◦ F 1 , where F 1 maps the task speciﬁcation to a set of outlines O , and F 2 maps each outline to a natural language dialogue : F 1 ( T ) → O = { o i , i ∈ 1 . . . N } ( 3 ) o i = [ ( t i 1 , . . . , t in i ) , ( a i 1 , . . . , a in i ) ] ( 4 ) F 2 ( { o i } ) → { d i } ( 5 ) We deﬁne an outline o i as a sequence of tem - plate utterances t ij and their corresponding annota - tions a ij . Template utterances are simplistic state - ments with language that is easy to generate with a few rules , as we will describe below . An out - line encapsulates the ﬂow of the dialogue while abstracting out the variation in natural language of the surface forms of each dialogue turn . Outlines are easier to generate using self - play between a user bot and a system bot as the bots do not need to generate complex and diverse language that mim - ics real users and assistants . To generate an outline , the framework ﬁrst sam - ples a scenario from the task speciﬁcation . We deﬁne a scenario as a user proﬁle and user goals , s i = ( p i , g i ) ( Figure 2b ) . In a goal - oriented di - alogue , the user wants to accomplish goals with the assistance of the dialogue agent , for example booking movie tickets or reserving restaurant ta - bles . Each goal is associated with constraints that map to slots of the schema , for example the movie name , genre , number of tickets , and price range . The slots in a user goal can have ﬁxed values ( e . g . genre should be “comedy” or the user will deny the offer ) , a list of possible values ( e . g . genre should “comedy” or “action” ) , ﬂexible values ( e . g . “comedy” is preferred , but the user is open to other options ) , or open values ( e . g . the user is open to seeing movies of any genre ) . In the multi - domain setting , a goal’s slot values can refer to previous goals , for example the user may want to buy a movie ticket and then get dinner after the movie at a restaurant near the theatre chosen in the preced - ing sub - dialogue . A scenario generator samples goals g i from the task speciﬁcation by randomly choosing the constraint type and values for every slot in the schema . The values are chosen from a set that includes all available values in the database as well as some non - existent values to create un - satisﬁable user goals . In addition to the user goal , the ﬂow of the di - alogue is also dependent on the personality of the user . A user could be verbose in specifying more constraints in a single turn , or could prefer to give each constraint separately . Similarly a user could be more or less amenable to changing their goal if their original constraints are not satisﬁable . We deﬁne a user proﬁle vector p i to encapsulate all the task - independent characteristics of the user’s behavior . In its simplest version , p i could be mod - eled as a vector of probabilities concerning in - dependent aspects of the user’s behavior , which could be passed into a programmed user simula - tor . Alternatively , p i could be an embedding of a user proﬁle in a latent space , which could condi - tion a learned user simulator model . In our setup , the scenario generator samples p i from a manually speciﬁed distribution of typical user proﬁles . With the dialogue scenario s i , the framework performs dialogue self - play between a user bot B U and system bot B S to generate a sequence of turn annotations a i 1 . . . a in i as follows : B U = P ( a ij | a i 1 , . . . , a ij − 1 , p i , g i ) ( 6 ) B S = P ( a ij + 1 | a i 1 , . . . , a ij , S , C ) ( 7 ) Each turn annotation a ij consists of a dialogue frame that encodes the semantics of the turn as a dialogue act and a slot - value map , for exam - ple “inform ( date = tomorrow , time = evening ) ” is a dialogue frame that informs the system of the user’s constraints for the date and time slots . Ta - ble 4 in Appendix A has a full list of dialogue acts . B U maps a ( possibly empty ) dialogue history a i 1 . . . a ij − 1 and a scenario p i , g i to a distribution over turn annotations for the next user turn . Sim - ilarly , B S maps a dialogue history , task schema S and API client C to a distribution over system turn annotations . In dialogue self - play ( Figure 2c ) , a new turn annotation a ij is iteratively sampled from each bot until either the user’s goals are achieved and the user exits the dialogue with a “bye ( ) ” act , or a maximum number of turns are reached . In our setup , B U is an agenda - based user sim - ulator ( Schatzmann et al . ( 2007 ) ) with a modiﬁ - cation that the action selection model is condi - tioned on the user proﬁle in addition to the user goal and dialogue history . B S is modeled as a ﬁ - nite state machine ( Hopcroft et al . ( 2006 ) ) which encodes a set of task - independent rules for con - structing system turns , with each turn consisting of a response frame which responds to the user’s previous turn , and an initiate frame which drives the dialogue forward through a predetermined se - quence of sub - dialogues . For database querying applications , these sub - dialogues are : gather user preferences , query a database via an API , offer matching entities to the user , allow user to modify preferences or request more information about an entity , and ﬁnally complete the transaction ( buy - ing or reserving the entity ) . By exploring a range of parameter values for B U and B S and sampling a large number of outlines , dialogue self - play can generate a diverse set of dialogue outlines for the task . Finally , a template utterance generator maps each turn annotation to a template utterance , a ij → t ij , using a domain - general grammar similar to the one described in Wang et al . ( 2015 ) . Alternatively , the developer can provide a list of templates to use for some or all of the dialogue frames , for example if they want more control over the language used in the system turns . The template utterances t ij are an important bridge between the turn annotation a ij and the corresponding natural language utter - ance u ij , since crowd workers may not understand the annotations if presented in symbolic form . 2 . 3 Crowdsourced paraphrases To obtain a natural language dialogue from its out - line , F 2 ( o i ) → d i , the framework employs crowd sourcing to paraphrase template utterances t ij into more natural sounding utterances u ij . The para - phrase task is designed as a “contextual rewrite” task where a crowd worker sees the full dialogue template t i 1 . . . t in i , and provides the natural lan - guage utterances u i 1 . . . u in i for each turn of the di - alogue . A screenshot of the contextual rewrite task interface is provided in Figure 3 . This encourages the crowd worker to inject linguistic phenomena like coreference ( “Reserve that restaurant” ) and lexical entrainment into the utterances . We show the same outline to K > 1 crowdworkers to get more diversity of natural language utterances for the same dialogue , { u ij } k . Since t ij and u ij are paraphrases of each other , the annotations a ij automatically apply to u ij , elim - inating the need for an expensive annotation step . In practice , for a fraction of the utterances , the automatic annotation does not succeed either due to crowd workers not following instructions prop - erly or if the utterance contains a paraphrase of a slot value , for example when the crowd worker rephrases “between 5pm and 8pm” as “some time in the evening” . We employ a second round of crowdsourcing for validating the utterances . For each u ij , we ask two crowd workers if it has the same meaning as the corresponding template t ij , and we drop the utterance if either of the crowd workers say no . Dialogues which end up having no natural language utterance for at least one of the turns are dropped from the dataset . For the remaining utterances , slot values from the annota - tion a ij are tagged in the utterance with substring match . If a slot value cannot be found automat - ically , we show it to two crowd workers and ask them to annotate the slot span . Alternatively , such annotation errors be detected and corrected by ac - tive learning ( Hakkani - T¨ur et al . ( 2002 ) ; Tur et al . ( 2003 ) ) . 2 . 4 Dataset expansion The rewrites t → { u } k collected via the crowd - sourcing step F 2 can be compiled into a map L ( a ) → { u } k . As an optional step , this map could be leveraged to synthetically expand the dataset beyond what is economically feasible to collect via crowdsourcing . The self - play step F 1 can be executed to generate a larger set of out - lines O S > > O . For each turn annotation a ij of o i ∈ O S , a natural language utterance is sam - pled 4 from L ( a ij ) to create the corresponding dia - logue d i ∈ D S > > D . Dialogues in the synthetic set D S could have utterances that were written by crowdworkers under a different context , so these dialogues are of a slightly lower quality . 2 . 5 Model training The dialogues d i ∈ D ( or D S ) have natural lan - guage turns along with annotations of dialogue acts , slot spans , dialogue state and API state for each turn . These labels are sufﬁcient for train - ing dialogue models from recent literature : either component - wise models for language understand - ing ( Bapna et al . ( 2017 ) ) , state tracking ( Rastogi et al . ( 2017 ) ) , dialogue policy ( Shah et al . ( 2016 ) ) and language generation ( Nayak et al . ( 2017 ) ) , or end - to - end models ( Wen et al . ( 2016 ) ) . Further , we can construct a natural language user simu - lator by combining U B with L ( a ) , and use it to train end - to - end dialogue models with reinforce - ment learning ( Liu et al . ( 2017 ) ) . 3 User simulation and dialogue self - play Our framework hinges on having a generative model of a user that is reasonably close to actual users of the system . The motivation is that while 4 If a ij / ∈ L , then o i is dropped from O S . it is hard to develop precise models of user be - havior customized for every type of dialogue in - teraction , it is possible to create a domain - general user simulator that operates at a higher level of abstraction ( dialogue acts ) and encapsulates com - mon patterns of user behavior for a broad class of dialogue tasks . Seeding the user simulator with a task - speciﬁc schema of intents , slot names and slot values allows the framework to generate a variety of dialogue ﬂows tailored to that speciﬁc task . De - veloping a general user simulator targeting a broad class of tasks , for example database querying ap - plications , has signiﬁcant leverage as adding a new conversational pattern to the simulator beneﬁts the outlines generated for dialogue interfaces to any database or third - party API . Another concern with the use of a user sim - ulator is that it restricts the generated dialogue ﬂows to only those that are engineered into the user model . In comparison , asking crowd work - ers to converse without any restrictions could gen - erate interesting dialogues that are not anticipated by the dialogue developer . Covering complex in - teractions is important when developing datasets to benchmark research aimed towards building human - level dialogue systems . However , we ar - gue that for consumer - facing chatbots , the primary aim is reliable coverage of critical user interac - tions . Existing methods for developing chatbots with engineered ﬁnite state machines implicitly deﬁne a model of expected user behavior in the states and transitions of the system agent . A user simulator makes this user model explicit and is a more systematic approach for a dialogue devel - oper to reason about the user behaviors handled by the agent . Similarly , having more control over the dialogue ﬂows present in the dataset ensures that all and only expected user and system agent behaviors are present in the dataset . Our crowd sourcing setup obtains diverse natural language re - alizations of the abstract dialogue ﬂows generated via self - play . A dialogue agent bootstrapped with such a dataset can be deployed in front of users with a guaranteed minimum task completion rate . Subsequently , the dialogue agent can be directly improved from real user interactions , for which crowdsourcing is anyways an imperfect substitute . The self - play step also uses a system bot B S that generates valid system turns for a given task . Since our framework uses a rule - based bot which takes user dialogue acts as inputs and emits a neu - ral network based dialogue agent that works with natural language utterances , the framework effec - tively distills expert knowledge into a learned neu - ral network . The developer can customize the be - havior of the neural agent by modifying the com - ponent rules of B S . Further , the cost of developing B S can be amortized over a large class of dialogue tasks by building a domain - agnostic bot for han - dling a broad task like database querying applica - tions , similar to U S . Finally , in contrast to a rule - based bot , a neural dialogue agent is amenable to further improvement from direct user interactions via reinforcement learning ( Su et al . ( 2016 ) ; Liu et al . ( 2017 ) ) , opening up the possibility of life - long improvement in the quality of the dialogue agent . 4 Crowdsourcing In the Wizard - of - Oz setting , the dialogue task speciﬁcation is used to construct tasks by sam - pling slot values from the API client . A task is then shown to a pair of crowd workers who are asked to converse in natural language to complete the task . Subsequently , the collected dialogues are manually annotated with dialogue act and slot span labels for training dialogue models . This process is expensive as the two annotation tasks given to crowd workers in the WOz setting are difﬁcult and therefore time consuming : identify - ing the dialogue acts of an utterance requires un - derstanding the precise meaning of each dialogue act , and identifying all slot spans in an utterance requires checking the utterance against all slots in the schema . As a result , the crowdsourced anno - tations may need to be cleaned by an expert . In contrast , M2M signiﬁcantly reduces the crowd - sourcing expense by automatically annotating a majority of the dialogue turns and annotating the remaining turns with two simpler crowdsourcing tasks , “Does this utterance contain this particular slot value ? ” and “Do these two utterances have the same meaning ? ” , which are more efﬁciently done by an average crowd worker . Further , the lack of control over crowd work - ers’ behavior in the Wizard - of - Oz setting can lead to dialogues that may not reﬂect the behavior of real users , for example if the crowd worker pro - vides all constraints in a single turn . Such low - quality dialogues either need to be manually re - moved from the dataset or the crowd participants need to be given additional instructions or train - Dataset Slots Train Dev Test Restaurant price range , location , restaurant name , category , num people , date , time 1116 349 775 Movie theatre name , movie , date , time , num people 384 120 264 Table 1 : Dialogues collected with M2M . ing to encourage better interactions ( Asri et al . ( 2017 ) ) . M2M avoids this issue by using dialogue self - play to systematically generate all usable dia - logue outlines , and simplifying the crowdsourcing step to a dialogue paraphrase task . 5 Datasets We are releasing 5 two datasets totaling 3000 dia - logues collected using M2M for the tasks of buy - ing a movie ticket and reserving a restaurant ta - ble . ( Table 1 ) . The datasets were collected by ﬁrst generating outlines using dialogue self - play and then rewriting the template utterances using crowd sourcing . 6 Evaluations We present some experiments with the M2M datasets to evaluate the M2M approach for collect - ing dialogue datasets and training conversational agents with that data . 6 . 1 Dialogue diversity First we will investigate the claim that M2M leads to higher coverage of dialogue features in the dataset . We compare the M2M Restaurants train - ing dialogues with the DSTC2 ( Henderson et al . ( 2013 ) ) training set which also deals with restau - rant reservations ( Table 2 ) . M2M compares fa - vorably to DSTC2 on the ratio of unique uni - grams and bigrams to total number of tokens in the dataset , which signiﬁes a greater variety of surface forms as opposed to repeating the same words and phrases . Similarly , we count the num - ber of unique “transitions” at the semantic frame level , deﬁned as a pair of annotations a i , a i + 1 of contiguous turns . This gives a measure of diver - sity of dialogue ﬂows in the dataset . M2M has 3x the number of unique transitions per turn of the dataset . We also count unique “subdialogues” , i . e . sequences of transitions a i , a i + 1 , . . . , a i + k for k = { 3 , 5 } , and observe that M2M has fewer rep - etitions of subdialogues compared to DSTC2 . 5 https : / / github . com / google - research - datasets / simulated - dialogue Metric DSTC2 ( Train ) M2M Rest . ( Train ) Dialogues 1611 1116 Total turns 11670 6188 Total tokens 199295 99932 Avg . turns per dialogue 14 . 49 11 . 09 Avg . tokens per turn 8 . 54 8 . 07 Unique tokens / Total tokens 0 . 0049 0 . 0092 Unique bigrams / Total tokens 0 . 0177 0 . 0670 Unique transitions / Total turns 0 . 0982 0 . 2646 Unique subdialogues ( k = 3 ) / Total subdialogues ( n = 3 ) 0 . 1831 0 . 3145 Unique subdialogues ( k = 5 ) / Total subdialogues ( n = 5 ) 0 . 5621 0 . 7061 Unique full outlines / Total dialogues 0 . 9243 0 . 9292 Table 2 : Comparing DSTC2 and M2M Restaurants datasets on diversity of language and dialogue ﬂows . 6 . 2 Human evaluation of dataset quality For a subjective evaluation of the quality of the M2M datasets , we ran an experiment showing the ﬁnal dialogues to crowd workers and asking them to rate each user and system turn between 1 to 5 on multiple dimensions . Figure 4 in the Appendix presents the interface shown to crowd workers for collecting the ratings . Each turn was shown to 3 crowd workers . Table 3 presents the mean and standard deviation of ratings aggregated over all turns of the datasets . 7 Related work and discussion We presented M2M , an extensible framework for rapidly bootstrapping goal - oriented conversa - tional agents . Comparisons with the popular Dia - log State Tracking Challenge 2 dataset ( Henderson et al . ( 2013 ) ) show that M2M can be leveraged for rapidly creating high - quality datasets for training conversational agents in arbitrary domains . A key beneﬁt of our framework is that it is fully control - lable via multiple knobs : the task schema , the sce - nario generator , the user proﬁle and behavior , the system policy and the template generator . PyDial ( Ultes et al . ( 2017 ) ) , an extensible open - source toolkit which provides domain - independent im - plementations of dialogue system modules , could be extended to support M2M by adding dialogue self - play functionality . The user and system bots in this work are imple - M2M Restaurants M2M Movies User : Natural 4 . 66 ( 0 . 54 ) 4 . 70 ( 0 . 49 ) System : Polite 4 . 23 ( 0 . 62 ) 4 . 27 ( 0 . 62 ) Clear 4 . 72 ( 0 . 52 ) 4 . 75 ( 0 . 48 ) Optimal 4 . 26 ( 0 . 76 ) 4 . 32 ( 0 . 75 ) Table 3 : Human evaluation of dialogues collected with M2M . Average of crowd worker scores ( from 1 to 5 ) for user and system turns ( standard deviation in brackets ) . mented using task - general rules so that any trans - actional or form - ﬁlling task could be handled with only the task schema . For more complex tasks , the developer can extend the user and system bots or the canonical utterance generator by adding their own rules . These components could also be replaced by machine learned generative mod - els if available . Task Completion Platform ( TCP ) ( Crook et al . ( 2016 ) ) introduced a task conﬁgura - tion language for building goal - oriented dialogue interactions . The state update and policy modules of TCP could be used to implement bots that gen - erate outlines for complex tasks . ParlAI ( Miller et al . ( 2017 ) ) , a dialogue research software platform , provides easy integration with crowd sourcing for data collection and evaluation . However , the crowd sourcing tasks are open - ended and may result in lower quality dialogues as de - scribed in Section 4 . The crowd sourcing tasks in M2M are conﬁgured to be at a suitable difﬁculty level for crowd workers as they are neither open - ended nor too restrictive . The crowd workers are asked to paraphrase utterances instead of coming up with completely new ones . Acknowledgements We thank Georgi Nikolov , Amir Fayazi , Anna Khasin and Grady Simon for valuable support in design , implementation and evaluation of M2M . References Layla El Asri , Hannes Schulz , Shikhar Sharma , Jeremie Zumer , Justin Harris , Emery Fine , Rahul Mehrotra , and Kaheer Suleman . 2017 . Frames : A corpus for adding memory to goal - oriented dialogue systems . arXiv preprint arXiv : 1704 . 00057 . Ankur Bapna , Gokhan Tur , Dilek Hakkani - Tur , and Larry Heck . 2017 . Sequential dialogue context modeling for spoken language understanding . In Proc . of SIGDIAL . PA Crook , A Marin , V Agarwal , K Aggarwal , T Anas - tasakos , R Bikkula , D Boies , A Celikyilmaz , S Chandramohan , Z Feizollahi , et al . 2016 . Task completion platform : A self - serve multi - domain goal oriented dialogue platform . NAACL HLT 2016 page 47 . Dilek Hakkani - T¨ur , Giuseppe Riccardi , and Allen Gorin . 2002 . Active learning for automatic speech recognition . In Acoustics , Speech , and Signal Pro - cessing ( ICASSP ) , 2002 IEEE International Confer - ence on . IEEE , volume 4 , pages IV – 3904 . Matthew Henderson , Blaise Thomson , and Jason Williams . 2013 . Dialog state tracking challenge 2 & 3 . http : / / camdial . org / ˜mh521 / dstc / . John E . Hopcroft , Rajeev Motwani , and Jeffrey D . Ull - man . 2006 . Introduction to Automata Theory , Lan - guages , and Computation ( 3rd Edition ) . Addison - Wesley Longman Publishing Co . , Inc . , Boston , MA , USA . Bing Liu , Gokhan Tur , Dilek Hakkani - Tur , Pararth Shah , and Larry Heck . 2017 . End - to - end optimiza - tion of task - oriented dialogue model with deep re - inforcement learning . In NIPS Conversational AI Workshop . Alexander H Miller , Will Feng , Adam Fisch , Jiasen Lu , Dhruv Batra , Antoine Bordes , Devi Parikh , and Ja - son Weston . 2017 . Parlai : A dialog research soft - ware platform . arXiv preprint arXiv : 1705 . 06476 . Neha Nayak , Dilek Hakkani - Tur , Marilyn Walker , and Larry Heck . 2017 . To plan or not to plan ? dis - course planning in slot - value informed sequence to sequence models for language generation . In Proc . of Interspeech . Abhinav Rastogi , Dilek Hakkani - Tur , and Larry Heck . 2017 . Scalable multi - domain dialogue state track - ing . In Proc . of IEEE ASRU . Jost Schatzmann , Blaise Thomson , Karl Weilhammer , Hui Ye , and Steve Young . 2007 . Agenda - based user simulation for bootstrapping a pomdp dialogue sys - tem . In Human Language Technologies 2007 : The Conference of the North American Chapter of the Association for Computational Linguistics ; Com - panion Volume , Short Papers . Association for Com - putational Linguistics , pages 149 – 152 . Pararth Shah , Dilek Hakkani - T¨ur , and Larry Heck . 2016 . Interactive reinforcement learning for task - oriented dialogue management . In NIPS Deep Learning for Action and Interaction Workshop . David Silver , Aja Huang , Chris J Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Ju - lian Schrittwieser , Ioannis Antonoglou , Veda Pan - neershelvam , Marc Lanctot , et al . 2016 . Mastering the game of go with deep neural networks and tree search . Nature 529 ( 7587 ) : 484 – 489 . David Silver , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot , Laurent Sifre , Dharshan Kumaran , Thore Graepel , et al . 2017 . Mastering chess and shogi by self - play with a general rein - forcement learning algorithm . arXiv preprint arXiv : 1712 . 01815 . PH Su , M Gaˇsi´c , N Mrkˇsi´c , L Rojas - Barahona , S Ultes , D Vandyke , TH Wen , and S Young . 2016 . On - line active reward learning for policy optimisation in spoken dialogue systems . In 54th Annual Meeting of the Association for Computational Linguistics , ACL 2016 - Long Papers . volume 4 , pages 2431 – 2441 . Gokhan Tur , Robert E Schapire , and Dilek Hakkani - Tur . 2003 . Active learning for spoken language understanding . In Acoustics , Speech , and Signal Processing , 2003 . Proceedings . ( ICASSP’03 ) . 2003 IEEE International Conference on . IEEE , volume 1 , pages I – I . Stefan Ultes , Lina M Rojas Barahona , Pei - Hao Su , David Vandyke , Dongho Kim , Inigo Casanueva , Paweł Budzianowski , Nikola Mrk ˇ si ´ c , Tsung - Hsien Wen , Milica Gasic , et al . 2017 . Pydial : A multi - domain statistical dialogue system toolkit . Proceed - ings of ACL 2017 , System Demonstrations pages 73 – 78 . Yushi Wang , Jonathan Berant , Percy Liang , et al . 2015 . Building a semantic parser overnight . ACL . Tsung - Hsien Wen , David Vandyke , Nikola Mrksic , Milica Gasic , Lina M Rojas - Barahona , Pei - Hao Su , Stefan Ultes , and Steve Young . 2016 . A network - based end - to - end trainable task - oriented dialogue system . ACL . Victor Zhong , Caiming Xiong , and Richard Socher . 2017 . Seq2sql : Generating structured queries from natural language using reinforcement learning . arXiv preprint arXiv : 1709 . 00103 . A Supplemental Material Table 4 lists the dialogue acts used in our setup . The dialogue acts are based on the Cambridge dia - logue act set . Table 5 presents a full dialogue out - line and corresponding paraphrase for a dialogue spanning two interdependent tasks , where the user wants to ﬁrst buy movie tickets and then reserve a restaurant table for dinner after the movie . Figure 3 presents the interface shown to crowd workers for the dialogue rewrite task , and includes a sample dialogue outline ( consisting of template utterances ) and its paraphrase into natural lan - guage . Figure 4 presents the interface shown to crowd workers for evaluating the quality of dia - logues collected with M2M . Table 4 : List of dialogue acts . Dialogue Act Speaker Description GREETING User / System Greet the other speaker INFORM User / System Inform a slot value CONFIRM User / System Ask the other speaker to conﬁrm a given slot value REQUEST User / System Ask for the value of a slot REQUEST ALTS User Ask for more alternatives OFFER System Offer a database entity to the user SELECT System Offer more than one database entity to the user AFFIRM User / System Agree to something said by the other speaker NEGATE User / System Disagree to something said by the other speaker NOTIFY SUCCESS System Notify the user of a successful event , e . g . a booking is complete NOTIFY FAILURE System Notify the user of a failure event , e . g . a booking isn’t available THANK YOU User / System Thank the other speaker GOOD BYE User / System Say goodbye to the other speaker CANT UNDERSTAND User / System Tell the other speaker that their utterance was not understood OTHER User Unknown utterance type Table 5 : Sample multi - domain dialogue outline and paraphrase . Outline Paraphrase Annotation ( a i ) Template utterances ( t i ) NL utterances ( u i ) S : greeting ( ) Greeting . Hi , how can I help you ? U : inform ( intent = book movie , name = Inside Out , date = tomorrow , num tickets = 2 ) Book movie with name is Inside Out and date is tomorrow and num tickets is 2 . I want to buy 2 tickets for Inside Out for tomorrow . S : ack ( ) request ( time ) OK . Provide time . Alright . What time would you like to see the movie ? U : inform ( time = evening ) Time is evening . Anytime during the evening works for me . S : offer ( theatre = Cinemark 16 , time = 6pm ) Offer theatre is Cinemark 16 and time is 6pm . How about the 6pm show at Cinemark 16 ? U : afﬁrm ( ) Agree . That sounds good . S : notify success ( ) Reservation conﬁrmed . Your tickets have been booked ! U : inform ( intent = ﬁnd restaurant , meal = dinner , location = near the theatre ) Find restaurant with meal is dinner and location is near the theatre . I want to get dinner at a restaurant near the theatre . S : request ( cuisine , price range ) Provide cuisine and price range . Do you have any preference for cuisine or price range ? U : inform ( cuisine = DontCare , price range = moderate , rating = high ) Cuisine is I don’t care and price range is moderate and rating is high . I’m ﬁne with any cuisine . Look for something moderately priced , but make sure it has a high rating . S : select ( restaurant = { First Wok , Lucy’s Grill } , location = near the theatre ) Select restaurant from First Wok , Lucy’s Grill with location is near the theatre . First Wok and Lucy’s Grill are some good options near the theatre . U : inform ( intent = reserve restaurant , restaurant = First Wok , time = after the movie ) Reserve restaurant with restaurant is First Wok and time is after the movie . First Wok sounds perfect . Can you reserve a table there for dinner after the movie ? S : ack ( ) conﬁrm ( restaurant = First Wok , time = 8pm , num people = 2 ) OK . Conﬁrm restaurant is First Wok and time is 8pm and num people is 2 . Sure . Please conﬁrm that it is a table for 2 at First Wok for 8pm . U : afﬁrm ( ) Agree . That is correct . S : notify success ( ) Reservation conﬁrmed . Your table has been reserved . U : thank you ( ) good bye ( ) Thank you and good bye . Thanks ! That’s all for now . Figure 3 : Contextual rewrite task interface for paraphrasing a dialogue outline with natural language . Figure 4 : Dialogue quality evaluation task interface for rating the user and system turns of completed dialogues .