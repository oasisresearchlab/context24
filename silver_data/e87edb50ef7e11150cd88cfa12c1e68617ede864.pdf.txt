CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Children of Color’s Perceptions of Fairness in AI : An Exploration of Equitable and Inclusive Co - Design Zoe Skinner Stacey Brown Greg Walsh Digital Whimsy Lab University of Baltimore Baltimore , MD 21209 USA zoe . skinner @ ubalt . edu gwalsh @ ubalt . edu stacey . brown2 @ ubalt . edu Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . CHI ’20 Extended Abstracts , April 25 – 30 , 2020 , Honolulu , HI , USA . © 2020 Copyright is held by the author / owner ( s ) . ACM ISBN 978 - 1 - 4503 - 6819 - 3 / 20 / 04 . DOI : https : / / doi . org / 10 . 1145 / 3334480 . 3382901 Abstract When it comes to algorithmic rights and protections for chil - dren , designers will need to face new paradigms to solve problems they are targeting . The ﬁeld of Design typically deals with form and function and is executed in molecules or pixels . But algorithms have neither . More importantly , algorithms may be biased in their execution against those without privileged status such as people of color , children , and the non - afﬂuent . In this paper , we review our work on exploring perceptions of fairness in AI through co - design sessions with children of color in non - afﬂuent neighbor - hoods of Baltimore City . The design sessions aimed at designing an artiﬁcially intelligent librarian for their local branch . Our preliminary ﬁndings showcase three key themes of this group’s perceptions of fairness in the context of an artiﬁcially intelligent authority ﬁgure . Author Keywords Children ; Participatory Design ; Co - Design ; AI ; Perceptions of Fairness ; Libraries CCS Concepts • Human - centered computing → Participatory design ; • Computing methodologies → Artiﬁcial intelligence ; LBW096 , Page 1 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Figure 1 : Adults and children co - designing in the ﬁrst design session . Figure 2 : One of the group’s role - playing the scenario : “Can you help me with my resume ? ” during Week 2’s session . Introduction When it comes to algorithmic justice , who thinks of those members of our society who are most vulnerable ? While moves have been made to create inclusive and equitable co - design in the past , it is important to maintain and en - hance those activities to amplify the voices that are , un - consciously or purposefully , frequently ignored . While aca - demics have researched socio - technical systems , few pro - fessional Designers have and even fewer have considered the effects of bias in machine learning on traditionally op - pressed users . The only way to prevent this kind of injustice is for the De - sign community to develop ways to work with this target audience within the context of algorithms . One of the best ways to do this is through co - design , in which designers work with their target audience in the design of technology for them [ 5 ] . One of the best examples of this comes from Druin’s work with children as design partners [ 4 ] . This current work focuses on an outreach program loosely based on Druin’s work in which a co - design team of De - signers and children creates a “fair” artiﬁcially intelligent Librarian ( AIL ) to work in the branch of their non - afﬂuent neighborhood in Baltimore City . Through the lens of “fair - ness” and the creation of an artiﬁcial authority ﬁgure , the researchers explored children of color’s perceptions of fair - ness and how they can help inform innovative designs for algorithmic justice and learning . Background As we create technologies that mimic human behavior , we must be considerate of the structure of the social system as well as the AI artifact [ 3 ] , meaning , understanding “fair” AI requires introspection into not only the technology , but the larger social fabric in which it will be placed . In this paper , we use “AI” as a colloquial term that encom - passes systems that can respond to situations with humans in a life - like manner . This term is being used regardless of the underlying technologies such as machine learning , ex - pert systems , or neural networks . This follows the terms [ 11 ] and by extension [ 12 ] uses in discussing AI and ethics . With the increasing use of technologies to support deci - sion making and automated responses , the ethics of those systems is often discussed . We are aware of problems that exist : biases can be taught to systems through cultural in - sensitivity [ 10 ] , systems can infer information that is legally protected with high accuracy [ 6 ] , and systems can be racist and sexist [ 17 ] due to their training . While technological solutions to fairness in AI may be pos - sible , the most feasible solution is the inclusion of others in the design process . The authors of [ 10 ] discussed the use of cultural inclusion as a way to mitigate bias . In another project [ 7 ] , people with previous gang experience were in - cluded in unstructured data classiﬁcation as domain experts with the hope of reducing violence . Including those that technology affects in the design process is referred to as co - design . A Call for a More Equitable Co - Design Process We recognize that utilizing the traditional co - design process has limitations when working with marginalized commu - nities . While the method and techniques of collaborative design aim to bridge the gap between the Design commu - nity and those they are designing for , recent research in the ﬁeld reveals how the co - design process itself can in fact be exclusionary to marginalized groups [ 8 , 9 ] . The authors call for an “equity driven approach” within collaborative design methods recognizing that by nature , contemporary partici - patory design methodology is “ [ positioned ] as a privileged activity , which inherently creates an imbalance in power & LBW096 , Page 2 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Figure 3 : Children considering their votes during Week 3’s table - top poll activity . equality” [ 8 ] . Similarly recognizing that a child’s participa - tion in the design process requires a certain privileged sta - tus , co - author Walsh reﬁned Druin’s KidsTeam method by collaborating with community members in Baltimore City’s library system in order to bring design sessions directly to children living in non - afﬂuent communities [ 15 ] . Regarding the topic of perceptions of algorithmic fairness speciﬁcally , recent research also supports the need for collaborative de - sign methods that actively include traditionally marginalized groups in the design process [ 16 ] . Others have taken sim - ilar approaches in the context of public services , citing the importance of understanding the perceptions of fairness in algorithmic decision - making systems from the vantage point of those communities that are disproportionately affected by them [ 2 ] . Methods As the Design community looks to better address and incor - porate the element of fairness in artiﬁcial intelligent algorith - mic designs for children of non - afﬂuent communities , there is a need for a clearer deﬁnition of “fairness . ” Acknowledg - ing the subjective nature of the term , our research focused on gaining a better understanding of children of color’s per - ceptions of what is “fair . ” Using an age - appropriate men - tal model of algorithmic decision - making as our guide , our central research question was : How might we design an Artiﬁcially Intelligent Librarian ( AIL ) that is “fair ? ” Co - Design via KidsTeam Rooted in the philosophy that children and adults should be equal partners in the design process when designing for children [ 14 ] [ 15 ] , we combined various design methodolo - gies including participatory design , contextual inquiry and low - tech prototyping [ 1 , 13 ] . Known as KidsTeam , children and adults co - design low - ﬁdelity technology together over the course of multiple in - person design sessions that repre - sent a distinct phase of the design process ( generative , iter - ative and evaluative ) [ 14 ] . We followed a similar co - design approach as documented in [ 15 ] in which the traditional KidsTeam process is reduced to four sessions , instead of designing over the course of months or a year which can create barriers for participation [ 15 ] . We conducted each of the four 1 - hour sessions at one of Baltimore City’s public library branches . We were a team of four adults : the authors ( one professor and two gradu - ate students ) and one active Librarian at the branch . Each session was facilitated by one researcher , while the others were active participants with the children , co - designing that week’s design challenge together . We want to acknowledge that with such a short time frame it is nearly impossible to completely eliminate inherent bi - ases and the pre - established power dynamics between adults and children . The modiﬁcations made to the Kid - sTeam method were intentional choices to help alleviate such biases and come closer to an equitable design method that attempts to breakdown the traditional power dynam - ics in hopes of including traditionally marginalized groups , speciﬁcally children of color , in the design process . Participants This research focused on a target population of children of color roughly between the ages of 9 - 14 years old from non - afﬂuent communities in Baltimore City . There were no requirements for joining the co - design sessions . Any child was welcome to join in for as much time as they’d like . We also want to acknowledge that while our participant recruit - ment was inclusive given we hosted it during after - school hours at a free public library , the group was not diverse nor a representative sample because we targeted just one of the many libraries in the area , with the scope of this project focusing speciﬁcally on children of color in non - afﬂuent LBW096 , Page 3 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA communities of Baltimore City . Our research received Institutional Review Board approval and each participant signed an assent form and was given a parent permission slip to bring home explaining the na - ture of the research . We had a variety of children partici - pate throughout the four weeks . We had at least four partic - ipants during each session , several of them participated in multiple sessions . None participated in all four . Data collection and analysis For this exploratory research project , we collected qualita - tive data in the form of observational notes , photographs , artifacts and video - recordings during the design sessions . After each session , the researchers debriefed the big ideas from that day and built off of those themes to develop the next week’s design activity . At the conclusion of the four sessions , researchers went through the data to identify common themes and then collectively synthesized the in - formation for preliminary results . Results Below is a breakdown of those results by session . Week 1 : Visualizing the “Fair” AI Librarian For our ﬁrst meeting , we challenged the group participants to visually design a fair AI Librarian ( AIL ) and consider how the AIL functionalities would be “fair . ” We ﬁrst asked the group what they thought both " artiﬁcial intelligence " and " fair " meant and then the rest of the session was focused on them designing both the physical and emotional char - acteristics of the AIL based on their perceptions of what they felt it means to be “fair . ” We broke the group up into two teams of adults and children and used markers and big sheets of paper to create low - ﬁdelity sketches of their ideas . We had 4 - 7 participants come in and out of this session . See Figure 1 . Both groups vocalized that the AIL should always help peo - ple on a ﬁrst come , ﬁrst serve basis , regardless . There was a debate around whether facial recognition for personal - ization was fair or safe and how much personal identifying information ( PII ) the AIL should have access to . Week 2 : Role Playing Customer - AIL Scenarios Taking the themes from the ﬁrst session , we created four role - play scenarios to help visualize a variety of situations around privacy as it applies to customer - AIL interactions . With nine children in attendance , we broke up into groups and assigned them each a customer - AIL scenario based on real experiences encountered by the library staff . They then chose whether they wanted to act out fair and / or unfair responses . See Figure 2 . The ﬁrst group acted out : “Can you help me ﬁnd some - thing ? ” For this scenario , the children’s interpretation of fairness was focused on the AIL’s need to be kind and how they should avoid any rudeness - whether it’s intentional or not . The second group acted out : “Can you help me with my resume ? ” They acted out an unfair response through a sexist lens with an AIL refusing to help a boy with his resume and then offering to help a girl . The third group’s question was : “Can I have something to eat ? ” They were clear that the AIL’s actions should be kind and follow the general rule that the Library does not provide food to any - one . Finally , the fourth group acted out : “Can I play video games ? ” This group felt that the AIL should be able to de - termine if and for how long children should have access to playing video games based on their grades as accessed from the school district by the AIL . They also came up with a tiered system of grade - point average to computer time . Week 3 : Table - Top Poll Scenarios We framed our third design session around a new tech - nique we called Table - Top Polling , which utilized a binary LBW096 , Page 4 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA voting system for participants to judge the fairness of a par - ticular scenario . We wanted to try and identify if there was a tipping point for when the children perceived a scenario to change from fair to unfair and vice versa . Thus , some of the scenarios built off of one another , adding in stipula - tions that integrated multiple perceptions of fairness that had been brought up in the prior sessions ( i . e . Scenario 3 : ”The AIL can recognize me , ” followed by Scenario 4 : ”The AIL can recognize me . . . by connecting to the school’s ID system” ) . We presented the ﬁve participants with a series of 11 scenarios and asked everyone ( children and adults ) to vote whether such behavior was fair ( yes ) or not ( no ) . See Figure 3 for an example of this technique . Majority of our discussion focused on the AIL having access to their PII and what the AIL was going to do with that in - formation . There was concern that it could be shared with other authority ﬁgures and possibly used against them . Most felt that the Library should not know what a customer asks the AIL ( 86 % ) nor should the police be able to ac - cesses information from the AIL ( 71 % felt that was unfair ) . Week 4 : Design Recommendations with Low - Fidelity Storyboards We met with the group again for the last time and had eight children in attendance . We presented and evaluated the ﬁnal design decisions in the form of low - ﬁdelity storyboards created by researchers which visualized four major themes uncovered from earlier design sessions : ( 1 ) First come , ﬁrst service is fair regardless of personal characteristics , ( 2 ) Kindness equals fairness , ( 3 ) Usage of PII is of concern , and ( 4 ) Disciplinary actions by the AIL are up for debate . Discussion These overarching themes illuminated that while there were some general qualities of fairness that the children felt were necessary when designing an authority ﬁgure ( i . e . the AIL should be kind ) , when we dug deeper into other elements of fairness ( i . e . how should the AIL fairly recommend materi - als ) , the children were less certain about how such matters should be handled fairly . Based on our participant’s de - sign decisions and explanations , we uncovered three main themes around children of color’s perceptions of fairness . Kindness equals fairness From the very beginning it was clear that the children felt a key element of determining whether an AIL was fair is if they were kind or not . From the ﬁrst co - design session the group set the tone that under no circumstances should the AIL be the following : a bigot , gendered , sexist , racist , uncultured , homophobic or a bully . They felt that these qual - ities were essential in not only ensuring fairness but , more importantly , preventing the AIL from being rude , whether that be intentional or not . We also saw this larger point re - inforced throughout all design sessions , with participants justifying fair actions as being communicated with kindness to customers . Access to and use of a child’s PII The perception that “kindness equals fairness” highlighted some uncertainty around the second theme : access to and use of a child’s PII . Our discussions focused on how the AIL would acquire information about customers in order to make judgements about who they are and what they need . Early on , we learned that the children were well aware of contemporary examples of unfairness in their current so - ciopolitical environment . We heard discussion around the role authority ﬁgures play in their communities and the con - cern about their personal information being shared with and easily accessed by them . Through their questions and de - bates , we got the sense that they recognized there is a limit AI’s ability to mimic human - human interactions . LBW096 , Page 5 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA This points to the children’s recognition that an AIL’s func - tionality hinges upon someone inputting certain information ahead of time in order to make decisions . Even something as simple as calling a customer by name brought up con - cerns around how the AIL is able to recognize someone . We heard solutions to this issue oscillate between scanning one’s library card to facial recognition technology . However , we saw that during the table - top poll , most chil - dren were hesitant in granting the AIL access to any amount of their PII . When presented with the option of the AIL being able to recognize them , a majority of participants found that to be unfair . We heard echoes of this concern as the chil - dren were adamant that the AIL not share any of their PII with police ofﬁcers in most all circumstances . From this in - formation , it appears that the children were willing to make sacriﬁces of kindness on the front end of interactions in or - der to protect overall privacy and safety of the customer , eliminating the need for the AIL to use any type of PII . Always have the child’s best interest in mind The ﬁnal theme was that an AIL should always have a child’s best interest in mind . They vocalized that the AIL should want to help them succeed in life , speciﬁcally , in school . This was the reasoning behind why some children felt that it was okay for the AIL to use their school grades when determining the amount of computer play time . They seemed okay with this breach of privacy recognizing that there was the potential for a greater beneﬁt . But , as we got deeper into the design sessions , the majority of the children felt that an AIL accessing their grades or connecting to the school system was unfair . From this , we gathered that while the speciﬁcs of how and by what means an AIL should help children succeed in school is murky , there is a general per - ception that helping a child is a key quality of fairness . While the above is not a deﬁnitive list of fairness qualities for an AI authority ﬁgure , nor can it be generalized to en - capsulate perceptions of fairness for all children of color , we hope these themes will spark future research regarding algorithmic justice for this population . Conclusion We conducted exploratory co - design sessions to gain new perspectives on children of color’s perception of fairness in the context of an AI authority ﬁgure . Our intentional choices in techniques and physical space were in attempts to am - plify the voices of those who have traditionally been left out of the co - design process . The use of role - play and table - top polling facilitated open and collaborative discussions around difﬁcult topics in algorithmic justice for children of color such as the use of PII , socioeconomic factors , and the role of AI in disciplinary actions . Our goal is that this will act as a springboard for further work on algorithmic fairness with children of color , specif - ically as it applies to the context of decision - making and au - thority ﬁgures , in hopes of continuing to actively include and design with those who will be impacted by such algorithmic - driven interactions . It is imperative to intentionally include and amplify the voices of those traditionally marginalized groups with the intent of preventing further discrimination within our automated decision - making systems . Acknowledgements This study would not have been possible without the Enoch Pratt Free Library of Baltimore City . We especially want to thank Librarian Elizabeth Thurston for her commitment to KidsTeam and helping us throughout the study . Finally , a huge thank you to all the children of Baltimore who partici - pated during the four sessions . LBW096 , Page 6 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA REFERENCES [ 1 ] n . d . KidsTeam : Children & Adults Working as Design Partners . HCIL ( n . d . ) . https : / / hcil . umd . edu / children - as - design - partners / [ 2 ] Anna Brown , Alexandra Chouldechova , Emily Putnam - Hornstein , Andrew Tobin , and Rhema Vaithianathan . 2019 . Toward algorithmic accountability in public services : A qualitative study of affected community perspectives on algorithmic decision - making in child welfare services . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 3 ] Joanna J Bryson . 2016 . Patiency is not a virtue : AI and the design of ethical systems . In 2016 AAAI Spring Symposium Series . [ 4 ] Allison Druin . 2002 . The role of children in the design of new technology . Behaviour and information technology 21 , 1 ( 2002 ) , 1 – 25 . [ 5 ] Allison Druin , Jamie Montemayor , Jim Hendler , Britt McAlister , Angela Boltman , Eric Fiterman , Aurelie Plaisant , Alex Kruskal , Hanne Olsen , Isabella Revett , and others . 1999 . Designing PETS : A personal electronic teller of stories . In Proceedings of the SIGCHI conference on Human Factors in Computing Systems . ACM , 326 – 329 . [ 6 ] Carmen Fernández and Alberto Fernández . 2019 . Ethical and Legal Implications of AI Recruiting Software . ERCIM NEWS 116 ( 2019 ) , 22 – 23 . [ 7 ] William R Frey , Desmond U Patton , Michael B Gaskell , and Kyle A McGregor . 2018 . Artiﬁcial intelligence and inclusion : Formerly gang - involved youth as domain experts for analyzing unstructured twitter data . Social Science Computer Review ( 2018 ) , 0894439318788314 . [ 8 ] Christina Harrington , Sheena Erete , and Anne Marie Piper . 2019b . Deconstructing Community - Based Collaborative Design : Towards More Equitable Participatory Design Engagements . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 216 . [ 9 ] Christina N Harrington , Katya Borgos - Rodriguez , and Anne Marie Piper . 2019a . Engaging Low - Income African American Older Adults in Health Discussions through Community - based Design Workshops . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , 593 . [ 10 ] Maarten Sap , Dallas Card , Saadia Gabriel , Yejin Choi , and Noah A Smith . 2019 . The Risk of Racial Bias in Hate Speech Detection . In Proceedings of the 57th Conference of the Association for Computational Linguistics . 1668 – 1678 . [ 11 ] Luc Steels . 2018 . What Needs to Be Done to Ensure the Ethical Use of AI ? . In CCIA . 10 – 13 . [ 12 ] Luc Steels and Ramon Lopez de Mantaras . 2018 . The Barcelona declaration for the proper development and usage of artiﬁcial intelligence in Europe . AI Communications Preprint ( 2018 ) , 1 – 10 . [ 13 ] Kathryn Summers , Kendra Knudtzon , Holly Weeks , Nancy Kaplan , Yoram Chisik , Rahul Kulkarni , and Stuart Moulthrop . 2003 . Contextual inquiry into children’s reading : Working with children as research partners . In Proceedings of the UPA Conference . Citeseer . [ 14 ] Greg Walsh . 2013 . Anatomy of a design session . interactions 20 , 6 ( 2013 ) , 68 – 71 . LBW096 , Page 7 CHI 2020 Late - Breaking Work CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA [ 15 ] Greg Walsh . 2018 . Towards equity and equality in American co - design : a case study . In Proceedings of the 17th ACM Conference on Interaction Design and Children . ACM , 434 – 440 . [ 16 ] Allison Woodruff , Sarah E Fox , Steven Rousso - Schindler , and Jeffrey Warshaw . 2018 . A qualitative exploration of perceptions of algorithmic fairness . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 17 ] James Zou and Londa Schiebinger . 2018 . AI can be sexist and racist—it’s time to make it fair . ( 2018 ) . https : / / www . nature . com / articles / d41586 - 018 - 05707 - 8 LBW096 , Page 8