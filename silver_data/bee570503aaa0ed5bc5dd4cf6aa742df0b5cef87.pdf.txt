Kitov Victor - Model evaluation Model evaluation c (cid:13) Victor Kitov v . v . kitov @ yandex . ru Summer school on Machine Learning in High Energy Physics in partnership with August 2015 Kitov Victor - Model evaluation Introduction Advanced aspects of machine learning 8 lectures : Reminder about major algorithms . Model evaluation . Feature selection . Ensemble learning N1 . Ensemble learning N2 . Linear dimensionality reduction . Non - linear dimensionality reduction . Kernel trick . Kernelized algorithms . Deep learning . 1 / 51 Kitov Victor - Model evaluation Recommended materials Statistical Pattern Recognition . 3rd Edition , Andrew R . Webb , Keith D . Copsey , John Wiley & Sons Ltd . , 2011 . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Trevor Hastie , Robert Tibshirani , Jerome Friedman , 2nd Edition , Springer , 2009 . http : / / statweb . stanford . edu / ~ tibs / ElemStatLearn / Machine Learning : A Probabilistic Perspective . Kevin P . Murphy . Massachusetts Institute of Technology . 2012 . Lectures of Machine Learning Course ( in Russian ) . Konstantin Vorontsov . machinelearning . ru . Additional sources - wikipedia , articles , tutorials . 2 / 51 Kitov Victor - Model evaluation Reminder Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 3 / 51 Kitov Victor - Model evaluation Reminder Formal de(cid:28)nitions of machine learning Machine learning is a (cid:28)eld of study that gives computers the ability to learn without being explicitly programmed . A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance P at tasks in T improves with experience E . 4 / 51 Kitov Victor - Model evaluation Reminder Supervised machine learning Find functional relationship between input variables x and output variables y based on expert knowledge and their common observations : ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . ( x N , y N ) x as a vector is called object , pattern . individual components of x are called features , regressors , inputs . y is called output , target if y ∈ R = > regression if y ∈ { ω 1 , ω 2 , . . . ω C } = > classi(cid:28)cation / pattern recognition 5 / 51 Kitov Victor - Model evaluation Reminder Demonstration Supervised learning : x = ( x 1 , x 2 ) , y speci(cid:28)ed by color 6 / 51 Kitov Victor - Model evaluation Reminder Unsupervised learning Find functional relationship between input variables x and output variables y based on expert knowledge and only x observations : x 1 , x 2 , . . . x N Unsupervised learning is also known as clustering ( for discrete output ) Unsupervised output recovery 7 / 51 Kitov Victor - Model evaluation Reminder Semi - supervised learning A small number of joint observations is available : ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . ( x N , y N ) A bigger number of only input observations is also available : x 1 , x 2 , . . . x M Recover x → y relationship Semi - supervised output recovery 8 / 51 Kitov Victor - Model evaluation Reminder Notation ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . ( x N , y N ) - training sample , N is number of observations x i ∈ R D , D is dimensionality of data x i or ( x i , y i ) - individual sample , pattern , object . In case of feature selection or dimensionality reduction : d is output dimensionality ω 1 , ω 2 , . . . ω C - labels of classes , C - total number of classes . 9 / 51 Kitov Victor - Model evaluation Reminder Typical work(cid:29)ow ( CrispDM methodology ) 10 / 51 Kitov Victor - Model evaluation Main ML methods Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 11 / 51 Kitov Victor - Model evaluation Main ML methods Comments on some major ML methods K - NN ( metric selection , search optimization using KD - trees and ball - trees ) Random Forest Extra random trees Neural network ( later ) Boosting 12 / 51 Kitov Victor - Model evaluation Main ML methods Forward stagewise additive modeling Input : training dataset ( x i , y i ) , i = 1 , 2 , . . . n ; loss function L ( f , y ) , general form of additive classi(cid:28)er h ( x , γ ) ( dependent from parameter γ ) and the number M of successive additive approximations . 1 Fit initial approximation f 0 ( x ) ( might be taken f 0 ( x ) ≡ 0 ) 2 For m = 1 , 2 , . . . M : 1 (cid:28)nd next best classi(cid:28)er ( c m , γ m ) = arg min n (cid:88) i = 1 L ( f m − 1 ( x i ) + c m h ( x , γ m ) , y i ) 2 set f m ( x ) = f m − 1 ( x ) + c m h ( x , γ m ) Output : approximation function f M ( x ) = f 0 ( x ) + (cid:80) M j = 1 c j h ( x , γ m ) Adaboost algorithm is obtained for L ( y , f ( x ) ) = e − yf ( x ) 13 / 51 Kitov Victor - Model evaluation Main ML methods Adaboost ( discrete version ) Assumptions : loss function L ( y , f ( x ) ) = e − yf ( x ) , classi(cid:28)cation task : y ∈ } Input : training dataset ( x i , y i ) , i = 1 , 2 , . . . n ; number of additive weak classi(cid:28)ers M , a family of weak classi(cid:28)ers h ( x ) , outputting only + 1 or - 1 ( binary classi(cid:28)cation ) and trainable on weighted datasets . 1 Initialize observation weights w i = 1 / n , i = 1 , 2 , . . . n . 2 for m = 1 , 2 , . . . M : 1 (cid:28)t h m ( x ) to training data using weights w i 2 compute weighted misclassi(cid:28)cation rate : E m = (cid:80) ni = 1 w i I [ h m ( x ) (cid:54) = y i ] (cid:80) ni = 1 w i 3 compute α m = ln ( ( 1 − E m ) / E m ) 4 increase all weights , where misclassi(cid:28)cation with h m ( x ) was made : w i ← w i e α m , i ∈ { i : h m ( x i ) (cid:54) = y i } Output : composite classi(cid:28)er f ( x ) = sign (cid:16)(cid:80) Mm = 1 α m h m ( x ) (cid:17) 14 / 51 Kitov Victor - Model evaluation Main ML methods Gradient boosting - regression Input : training dataset ( x i , y i ) , i = 1 , 2 , . . . n ; loss function L ( f , y ) and the number M of successive additive approximations . 1 Fit initial approximation f 0 ( x ) ( might be taken f 0 ( x ) ≡ 0 ) 2 For each step m = 1 , 2 , . . . M : 1 calculate derivatives z i = − ∂ L ( r , y ) ∂ r | r = f m − 1 ( x ) 2 train additive approximation with classi(cid:28)er h m on ( x i , z i ) , i = 1 , 2 , . . . n with simple loss function , e . g . squared di(cid:27)erence (cid:80) ni = 1 ( h m ( x i ) − z i ) 2 3 solve univariate optimization problem : n (cid:88) i = 1 L (cid:0) f m − 1 ( x i ) + c m h m ( x i ) , y i (cid:1) → min c m ∈ R + 4 set f m ( x ) = f m − 1 ( x ) + c m h m ( x ) Output : approximation function f M ( x ) = f 0 ( x ) + (cid:80) M m = 1 c m h m ( x ) 15 / 51 Kitov Victor - Model evaluation Main ML methods Gradient boosting of trees - regression Input : training dataset ( x i , y i ) , i = 1 , 2 , . . . n ; loss function L ( f , y ) and the number M of successive additive approximations . 1 Fit constant initial approximation f 0 ( x ) : f 0 ( x ) = arg min γ (cid:80) ni = 1 L ( γ , y i ) 2 For each step m = 1 , 2 , . . . M : 1 calculate derivatives z i = − ∂ L ( r , y ) ∂ r | r = f m − 1 ( x ) 2 train regression tree h m on ( x i , z i ) , i = 1 , 2 , . . . n with squared loss function (cid:80) ni = 1 ( h m ( x i ) − z i ) 2 and extract terminal regions R jm , j = 1 , 2 , . . . J m . 3 for each terminal region R jm , j = 1 , 2 , . . . J m solve univariate optimization problem : γ jm = arg min γ (cid:88) x i ∈ R jm L ( f m − 1 ( x i ) + γ , y i ) 4 update f m ( x ) = f m − 1 ( x ) + (cid:80) J m j = 1 γ jm I [ x ∈ R jm ] Output : approximation function f M ( x ) 16 / 51 Kitov Victor - Model evaluation Main ML methods Gradient boosting for classi(cid:28)cation Suppose we have C classes . Then each class probability may be represented using C − 1 functions f i ( x ) : p i ( x ) =   e fi ( x ) 1 + (cid:80) C − 1 i = 1 e fi ( x ) , i = 1 , 2 , . . . C − 1 1 1 + (cid:80) C − 1 i = 1 e fi ( x ) i = C In classi(cid:28)cation boosting functions f i ( x ) , i = 1 , 2 , . . . C − 1 are estimated the same way as single regression function f m ( x ) in regression boosting - the loop [ for c = 1 , 2 , . . . C − 1 ] is inserted inside step 2 loop [ for m = 1 , 2 , . . . M ] . More information on boosting can be found in chapter 10 of the book (cid:16)The Elements of Statistical Learning(cid:17) ( http : / / statweb . stanford . edu / ~ tibs / ElemStatLearn / ) 17 / 51 Kitov Victor - Model evaluation Main ML methods Neural networks Structure of neural network Activation function 18 / 51 Kitov Victor - Model evaluation Margin Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 19 / 51 Kitov Victor - Model evaluation Margin Discriminative functions Classi(cid:28)cation of two classes ω 1 and ω 2 Discriminant function : g w ( x ) is de(cid:28)ned (cid:98) ω = (cid:40) ω 1 , g ( x ) ≥ 0 ω 2 , g ( x ) < 0 Linear discriminant function : g ( x ) = w T x + w 0 = < W , X > , where W = [ w 0 , w ] and X = [ 1 , x ] . If we denote classes ω 1 and ω 2 with y = + 1 and y = − 1 respectively , we get the decision rule y = sign g ( x ) . 20 / 51 Kitov Victor - Model evaluation Margin Margin De(cid:28)ne margin M ( x , y ) = g ( x ) y M ( x , y ) > 0 < = > object x is correctly classi(cid:28)ed | M ( x , y ) | = M (cid:48) ( x ) ≥ 0 measures con(cid:28)dence of decision Upper boundary on misclassi(cid:28)cation : Q accurate ( w | X ) = (cid:88) i I [ M ( x i | w ) < 0 ] ≤ (cid:88) i L ( M ( x i | w ) ) = Q approx ( w | X ) Optimization task to get weights : Q approx ( w | X ) = n (cid:88) i = 1 L ( M ( x i | w ) ) = n (cid:88) i = 1 L ( (cid:104) w , x i (cid:105) y i ) → min w 21 / 51 Kitov Victor - Model evaluation Margin Approximating loss functions SVM : ( 1 − M ) + , logistic regression : ln ( 1 + e − M ) Sigmoid : more tight approximation , but non - convex . Exponential : strongly a(cid:27)ected by outliers . 22 / 51 Kitov Victor - Model evaluation Margin Optimization Optimization task to get weights : Q approx ( w | X ) = n (cid:88) i = 1 L ( M ( x i | w ) ) = n (cid:88) i = 1 L ( (cid:104) w , x i (cid:105) y i ) → min w Gradient descent algorithm : Iteratively until convergence w ← w − η ∂ Q approx ( w | X ) ∂ w = w − η n (cid:88) i = 1 L (cid:48) ( (cid:104) w , x i (cid:105) y i ) x i y i η - parameter , controlling the speed of convergence . Faster convergence when updates are more often - e . g . at each observation . Observations may be taken randomly . 23 / 51 Kitov Victor - Model evaluation Margin Improved optimization Stochastic gradient descent algorithm Calculate (cid:98) Q approx ( w , X ) = (cid:80) ni = 1 L ( M ( x i | w ) ) Iteratively , until convergence of (cid:98) Q approx or convergence of w : 1 select random observation ( x i , y i ) 2 adapt weights : w ← w − η L (cid:48) ( (cid:104) w , x i (cid:105) y i ) x i y i 3 Estimate error : ε i = L ( (cid:104) w , x i (cid:105) y i ) 4 Recalculate (cid:98) Q approx = ( 1 − α ) (cid:98) Q approx + αε i Initial weights selection : all zeros random at [ − 12 D , 12 D ] ( for logistic approximation ) or arbitrary random w i = (cid:104) x i , y (cid:105) (cid:104) x i , x i (cid:105) 24 / 51 Kitov Victor - Model evaluation Margin Selection of η Larger η = > algorithm more prone to diverge . Plot Q approx ( w ) ( or (cid:98) Q approx ( w ) ) versus iteration number t to control convergence . Deterministic scheme : Stochastic gradient descent converges to local optima if η t → 0 (cid:80) ∞ t = 1 η t = ∞ (cid:80) ∞ t = 1 η 2 t < ∞ Example : η t = 1 t Data dependent scheme : At each step (cid:28)nd η t = arg min η Q approx ( w − η ∂ Q approx ∂ w ) Often analytical solution for such η exists 25 / 51 Kitov Victor - Model evaluation Margin Comments Margins increase robustness , by pushing decision boundary away from the samples . Non - symmetrical margin : ( g ( x ) = ˜ g , y = ˜ y ) is equivalent to ( g ( x ) = − ˜ g , y = − ˜ y ) not relevant for non - symmetric losses ( example : predicting illness ) by introducing g y ( x ) = (cid:40) g 1 ( x ) y = + 1 g 2 ( x ) y = − 1 we can treat non - symmetrical case . 26 / 51 Kitov Victor - Model evaluation Regularization Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 27 / 51 Kitov Victor - Model evaluation Regularization Regularization Useful technique to control the trade - o(cid:27) between bias and variance , can be applied to any algorithm . Q regularized ( w ) = Q ( w ) + τ | | w | | 2 Q regularized ( w ) = Q ( w ) + τ | | w | | 1 | | w | | 1 = D (cid:88) d = 1 | w d | , | | w | | 2 = D (cid:88) d = 1 ( w d ) 2 28 / 51 Kitov Victor - Model evaluation Regularization Maximum probability estimation X = { x 1 , x 2 , . . . x n } , Y = { y 1 , y 2 , . . . y n } - training sample of i . i . d . observations , ( x i , y i ) ∼ p ( y | x , w ) ML estimation (cid:98) w = arg max w p ( Y | X , w ) Using independence assumption : n (cid:89) i = 1 p ( y i | x i , w ) = n (cid:88) i = 1 ln p ( y i | x i , w ) → max w Approximated misclassi(cid:28)cation : n (cid:88) i = 1 L ( g ( x i ) y i | w ) → min w Interrelation : L ( g ( x i ) y i | w ) = − ln p ( y i | x i , w ) 29 / 51 Kitov Victor - Model evaluation Regularization Maximum a prosteriori estimation X = { x 1 , x 2 , . . . x n } , Y = { y 1 , y 2 , . . . y n } - training sample of i . i . d . observations , ( x i , y i ) ∼ p ( x , y | w ) x i ∼ p ( x | w ) MAP estimation : w is random with prior probability p ( w ) p ( w | X , Y ) = p ( X , Y , w ) p ( X , Y ) = p ( X , Y | w ) p ( w ) p ( X , Y ) ∝ p ( X , Y | w ) p ( w ) w = arg max w p ( w | X , Y ) = arg max w p ( X , Y | w ) p ( w ) n (cid:88) i = 1 ln p ( x i , y i | θ ) + ln p ( w ) → max w 30 / 51 Kitov Victor - Model evaluation Regularization Gaussian prior Gaussian prior ln p ( w , σ 2 ) = ln (cid:18) 1 ( 2 πσ 2 ) n / 2 e − | | w | | 22 2 σ 2 (cid:19) = − 1 2 σ 2 | | w | | 22 + const ( w ) Laplace prior ln p ( w , C ) = ln (cid:18) 1 ( 2 C ) n e − | | w | | 1 C (cid:19) = − 1 C | | w | | 1 + const ( w ) 31 / 51 Kitov Victor - Model evaluation Regularization L 1 norm | | w | | 1 regularizer will do feature selection . Consider Q ( w ) = n (cid:88) i = 1 L i ( w ) + 1 C D (cid:88) d = 1 | w d | if 1 C > sup w (cid:12)(cid:12)(cid:12) ∂ L ( w ) ∂ w i (cid:12)(cid:12)(cid:12) , then it becomes optimal to set w i = 0 For smaller C more inequalities will become active . 32 / 51 Kitov Victor - Model evaluation Regularization Regression Example : least squares regression (cid:80) Nn = 1 ( w T x n + w 0 − y n ) 2 + R ( w ) → min w , w 0 LASSO : least - squares regression , using | | w | | 1 Ridge : least - squares regression , using | | w | | 2 Elastic Net : : least - squares regression , using both 33 / 51 Kitov Victor - Model evaluation Regularization Multi - task lasso K outputs are solved with K regressions : in the same feature space with constraint that features become included / excluded simulataneously across all tasks . Optimization problem : min W (cid:107) XW − Y (cid:107) 2 2 + α (cid:107) W (cid:107) 21 where X ∈ R NxD , W ∈ R DxK , Y ∈ R NxK and (cid:107) W (cid:107) 21 = N (cid:88) n = 1 (cid:118) (cid:117)(cid:117) (cid:116) K (cid:88) k = 1 w 2 nk 34 / 51 Kitov Victor - Model evaluation Model output Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 35 / 51 Kitov Victor - Model evaluation Model output Model output Regression output : y ∈ R Classi(cid:28)cation output : exact class ( e . g : majority voting ) score ( SVM , nearest centroid ) class probability ( logistic regression , all tree based methods , K - NN ) Techniques for transforming score f ( x ) to probability p ( y = 1 ) : Platt scaling isotonic regression 36 / 51 Kitov Victor - Model evaluation Model output Platt scaling Platt scaling assumes logistic relationship p ( y = 1 | x ) = 1 1 + e Af ( x ) + B and (cid:28)ts parameters A , B using maximum likelihood . Logistic function For (cid:28)tting A , B training set should be di(cid:27)erent from training set where f ( x ) was (cid:28)tted ( otherwise over(cid:28)tting ) . Platt scaling is good for small datasets , otherwise use more general isotonic regression . 37 / 51 Kitov Victor - Model evaluation Model output Isotonic regression The following functional relationship is assumed : y i = m ( s i ) + ε i where ε i ∼ i . i . d . N ( 0 , σ 2 ) , s i is score of classi(cid:28)er for x i and m ( · ) is arbitrary monotone function . Using ( s i , y i ) , i = 1 , 2 , . . . N as training set , (cid:28)nd (cid:98) m = arg min m (cid:88) i ( y i − m ( f i ) ) 2 Should be (cid:28)tted on separate validation set . Piecewise constant solution is found in linear time . 38 / 51 Kitov Victor - Model evaluation Model output Sample (cid:28)t of isotonic regression 39 / 51 Kitov Victor - Model evaluation Model evaluation Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation Evaluation of class assignements 7 ROC curves 40 / 51 Kitov Victor - Model evaluation Model evaluation Evaluation of class assignements 6 Model evaluation Evaluation of class assignements 41 / 51 Kitov Victor - Model evaluation Model evaluation Evaluation of class assignements Confusion matrix Confusion matrix : Estimated classes T r u e c l a ss e s 1 2 · · · C 1 2 . . . C   n 11 n 12 n 21 n 22 . . . n CC   n ij - number of objects , belonging to ω i but classi(cid:28)ed as ω j . Visualized confusion matrix 42 / 51 Kitov Victor - Model evaluation Model evaluation Evaluation of class assignements 2 - class case Confusion matrix : Estimated class + - True class + True positives False negatives - False positives True negatives Derived performance measures : Accuracy : TP + TN P + N Error rate : FP + FN P + N FPR : FPN TPR : TPP Precision : TP TP + FP Recall : TPP F - measure : 2 1 Precision + 1 Recall F β - measure : 1 β 2 1 + β 2 1 Precision + 1 1 + β 2 1 Recall Accuracy - most intuitive but irrelevant for skewed classes All measures require speci(cid:28)cation of probability / score . 43 / 51 Kitov Victor - Model evaluation Model evaluation Evaluation of class assignements 2 - class case Confusion matrix : Estimated class + - True class + True positives False negatives - False positives True negatives Derived performance measures : Accuracy : TP + TN P + N Error rate : FP + FN P + N FPR : FPN TPR : TPP Precision : TP TP + FP Recall : TPP F - measure : 2 1 Precision + 1 Recall F β - measure : 1 β 2 1 + β 2 1 Precision + 1 1 + β 2 1 Recall Accuracy - most intuitive but irrelevant for skewed classes All measures require speci(cid:28)cation of probability / score . 43 / 51 Kitov Victor - Model evaluation Model evaluation Evaluation of class assignements Discriminability vs . reliability Discriminability measures how well classes are classi(cid:28)ed Error rate is discriminability measure Reliability how well class probabilities are estimated Likelihood ( y i is the class of x i ) : n (cid:89) i = 1 (cid:98) p ( y i | x i ) Brier score : 1 n n (cid:88) i = 1 C (cid:88) c = 1 ( I [ x i ∈ ω c ] − (cid:98) p ( ω c | x i ) ) 2 Example of good discriminability and poor reliability 44 / 51 Kitov Victor - Model evaluation ROC curves Table of Contents 1 Reminder 2 Main ML methods 3 Margin 4 Regularization 5 Model output 6 Model evaluation 7 ROC curves 45 / 51 Kitov Victor - Model evaluation ROC curves Parametrization of predicted class proportions Bayes minimum risk solution : assign x to ω 1 if λ 1 p ( ω 1 ) p ( x | ω 1 ) > λ 2 p ( ω 2 ) p ( x | ω 2 ) This condition is equivalent to p ( x | ω 1 ) p ( x | ω 2 ) > λ 2 p ( ω 2 ) λ 1 p ( ω 1 ) = µ Discriminant functions : assign x to ω 1 if g 1 ( x ) − g 2 ( x ) > µ (cid:48) . 46 / 51 Kitov Victor - Model evaluation ROC curves ROC curve ROC curve characterizes classi(cid:28)er performance for all values of parameter cut - o(cid:27) . As µ decreases , the algorithm becomes more inclined to select class ω 1 ( positive class ) TPR = 1 − ε 1 increases FPR = ε 2 also increases How to compare di(cid:27)erent classi(cid:28)ers ? 47 / 51 Kitov Victor - Model evaluation ROC curves ROC curve ROC curve characterizes classi(cid:28)er performance for all values of parameter cut - o(cid:27) . As µ decreases , the algorithm becomes more inclined to select class ω 1 ( positive class ) TPR = 1 − ε 1 increases FPR = ε 2 also increases How to compare di(cid:27)erent classi(cid:28)ers ? 47 / 51 Kitov Victor - Model evaluation ROC curves ROC properties Better ROC curves are more concave Diagonal represents random guessing Expected loss is equal to L = λ 2 p ( ω 2 ) ε 2 + λ 1 p ( ω 1 ) ε 1 = λ 2 p ( ω 2 ) ε 2 − λ 1 p ( ω 1 ) ( 1 − ε 1 ) + λ 1 p ( ω 1 ) At optimality point iso - loss surface is tangent to ROC curve with slope tangent equal to λ 2 p ( ω 2 ) λ 1 p ( ω 1 ) 48 / 51 Kitov Victor - Model evaluation ROC curves ROC quality criteria AUC : global performance characteristic equals the probability that for random x 1 ∈ ω 1 and x 2 ∈ ω 2 it would be true that : (cid:98) p ( ω 1 | x 1 ) > (cid:98) p ( ω 2 | x ) LC index : rescale λ 1 and λ 2 so that λ 1 + λ 2 = 1 de(cid:28)ne λ 1 = λ , λ 2 = 1 − λ for each λ ∈ [ 0 , 1 ] calculate L ( λ ) = (cid:40) + 1 if 1st classi(cid:28)er is better − 1 if 2nd classi(cid:28)er is better de(cid:28)ne probability for p ( λ ) ( example : triangular ) choose 1 - st classi(cid:28)er i(cid:27) ´ 1 0 L ( λ ) p ( λ ) d λ > 0 . 49 / 51 Kitov Victor - Model evaluation ROC curves Comments on model evaluation Bayes minimum error rate - theoretical lower bound for classi(cid:28)cation need to know P ( x , y ) . Training error rate - optimistically biased Test error rate - pessimistically biased ( since part of data used for error estimation ) 50 / 51 Kitov Victor - Model evaluation ROC curves Holdout estimate of error rate distribution Let e be the probability of making error on previously unseen object . Probability of observing k errors on test sample of size n : p ( k | e , n ) = (cid:18) n k (cid:19) e k ( 1 − e ) n − k Then p ( e | k , n ) = p ( e , k | n ) p ( k | n ) = p ( k | e , n ) p ( e | n ) ´ p ( k | n ) p ( e | n ) de Assuming that p ( e | n ) ≡ const , we obtain p ( e | k , n ) = p ( k | e , n ) ´ p ( k | n ) de ∝ e k ( 1 − e ) n − k Since beta - distribution Be ( x | α , β ) = [ Γ ( α + β ) / ( Γ ( α ) Γ ( β ) ) ] x α − 1 ( 1 − x ) β − 1 it follows that p ( e | k , n ) ∼ Be ( k + 1 , n − k + 1 ) 51 / 51