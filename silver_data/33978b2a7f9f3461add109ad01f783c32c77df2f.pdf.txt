Data Makes Better Data Scientists Jinjin Zhao j2zhao @ uchicago . edu University of Chicago Chicago , IL , USA Avigdor Gal avigal @ technion . ac . il Technion – Israel Institute of TechnologyHaifa , Isreal Sanjay Krishnan skr @ uchicago . edu University of Chicago Chicago , IL , USA Abstract With the goal of identifying common practices in data sci - ence projects , this paper proposes a framework for logging and understanding incremental code executions in Jupyter notebooks . This framework aims to allow reasoning about how insights are generated in data science and extract key observations into best data science practices in the wild . In this paper , we show an early prototype of this framework and ran an experiment to log a machine learning project for 25 undergraduate students . ACM Reference Format : Jinjin Zhao , Avigdor Gal , and Sanjay Krishnan . 2023 . Data Makes Better Data Scientists . In Workshop on Human - In - the - Loop Data Analytics ( HILDA ’23 ) , June 18 , 2023 , Seattle , WA , USA . ACM , New York , NY , USA , 3 pages . https : / / doi . org / 10 . 1145 / 3597465 . 3605228 1 Introduction A good data scientist must be able to reason about uncer - tain data , how this data interacts with code , and how to incorporate extensive domain knowledge with this data to draw her insights [ 7 ] . The confluence of skills needed for even simple data science tasks makes it hard to run disci - plined evaluations in data science [ 3 ] . For example , consider a new software tool that claims to improve data scientist productivity . Should productivity be measured in terms of a reduction of lines of code or the time spent working ? Further - more , how should these metrics be reconciled with questions about accuracy and robustness ? While many other industries have been revolutionized by “Metric - based Management” [ 5 ] , ironically , the key perfor - mance indicators ( KPIs ) of data science productivity have not yet been established at the individual or group level . Admit - tedly , there are numerous studies on human decision - making Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . HILDA ’23 , June 18 , 2023 , Seattle , WA , USA © 2023 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 979 - 8 - 4007 - 0216 - 7 / 23 / 06 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3597465 . 3605228 with data in psychology [ 8 ] , data visualization [ 4 ] , data sci - ence education [ 15 ] , and business decision - making [ 9 ] . How - ever , there is limited information on the processes and tasks that take place in real - world data science workflows that might cut across multiple toolkits and involve multiple data scientists . Notable seminal projects in this space include SQLShare dataset [ 11 ] and the KGTorrent dataset [ 21 ] . We argue that these studies are limited since they miss crucial steps in the data exploration and code design process – and only log the final state of a query or a code cell respec - tively . Code that fails to execute or is deleted can provide crucial clues about the data , data model , and / or data scien - tist . For example , a confusing , ill - documented schema may result in a large number of erroneous queries . With the goal of improving the community’s quantitative understanding of data science processes , we propose a system that tracks incremental code executions in Jupyter notebooks . Jupyter notebooks are computational notebooks , which are collec - tions of cells that intersperse code , results , and narrative . The framework tracks all modifications and executions to data science code written in a notebook . These logs are passed through an analysis module that : ( 1 ) recognizes the current task focus of the data scientist based on code patterns , ( 2 ) identifies what data assets are being manipulated based on data provenance , and ( 3 ) builds a temporal model of the data scientist’s overall process . In this paper , we show an initial experiment that logs the workload of 25 undergraduate students during a machine learning task . As our logging framework matures , we envi - sion several core applications , as follows . More Informative User Studies . We are interested in quan - titatively capturing the process by which data scientists gen - erate insights from data . We want to validate common as - sumptions ( e . g . , data cleaning is the most challenging part of data science ) and identify impactful problems to improve this process . More Complex User Studies . Data science is implemented across practically all domains by diverse groups of data sci - entists with varying expertise . We want our system to enable workload capture across these different groups and observe differences of approach across domains . Data Governance Insights . For some scenarios , restricting data access to particular use cases and users is extremely important . In these cases , by tracking the workflow process , a record of what data is used for insights and how that data HILDA ’23 , June 18 , 2023 , Seattle , WA , USA Jinjin Zhao , Avigdor Gal , and Sanjay Krishnan Figure 1 . Architecture of proposed Jupyter notebook log framework is accessed can be generated . This information can shape policies for data sharing and governance . Data Science Education . By understanding the differences in workflows between data science novices and experts , we can quantify the knowledge gained from experience and ed - ucation . This knowledge can be incorporated to improve our current data science curriculum . We can also generate spe - cific feedback based on a student’s workflow on assignments , allowing for individualized instruction without increasing the instructor’s burden . 2 Related Work 2 . 1 Jupyter Notebooks Jupyter notebooks are known to be messy , with poor preser - vation of history and out - of - order workflows [ 6 , 13 ] . Accord - ingly , tools have been developed to generate cleaner code slice views on notebooks for human use [ 2 , 10 , 17 , 22 , 25 ] . All these tools provide the groundwork for fine - grained quanti - tative analysis of notebook code but do not directly analyze computational behavior . 2 . 2 Data Science Studies Numerous interview studies have been made to profile data science workflows [ 12 , 14 , 24 , 28 ] . These studies have limi - tations due to human bias when self - reporting their work [ 18 ] . Quantitative studies on data science and notebook development have primarily focused on the final pipeline [ 16 , 19 , 20 , 23 , 26 , 27 ] . They give overviews on the distribu - tion of code and supporting text and are valuable studies in best practices for reproducibility and final code design . However , they do not provide insight into the process of generating those workflows . 3 Architecture and Student Logs 3 . 1 Architecture In Figure 1 , we outline the envisioned framework for data science logging in Jupyter notebooks . This framework has three steps : ( 1 ) logging all code edits to create a timeline of Figure 2 . Example of execution log generated from a com - putational notebook a data science project , ( 2 ) a breakdown of this log into self - contained mini - processes that describes the capture of data insights , ( 3 ) the extraction of KPIs from the mini - processes that allow for comparisons across processes and projects . 3 . 2 Experiment Setup An experiment was run to capture data science workflow in a classical machine learning problem . The concrete goal of this experiment is to record detailed logs of a typical amateur data science workflow using Jupyter notebooks . We recruited 25 undergraduate students from an introductory data science course at the University of Chicago and gave them a standard machine learning task . Given the full 2018 Air Flight dataset [ 1 ] , they had the task of predicting flight departure delays of over 15 minutes on the 2019 version of the same dataset ( with any column containing information at or after departure withheld ) . The 2018 and 2019 datasets had 5 , 602 , 937 and 8 , 091 , 684 samples , respectively . In order to track this process , we create an execution log with a record for each cell run in a Jupyter notebook . This log is similar to the log in nbgather [ 10 ] and the IPython’s default history database . However , there are subtle critical differences in the information tracked . We consider one exe - cution log for each notebook , logging data over the entire Jupyter notebook history . We record cell content , execution time , and error statements for each cell run . The output of the cells is not logged , a design decision due to anecdotal observation of the volume of outputs generated compared to the code written . Figure 2 shows an example of the execution log for a simple Jupyter notebook . In addition to the execution log , we have the final note - book submission and a prediction file for flight departure delays of over 15 minutes on the 2019 dataset from each student . To summarize , the sources of information we cap - tured in this experiment are : ( 1 ) code for each cell executed throughout the lifetime of the notebook , ( 2 ) wall - clock time of execution , ( 3 ) any execution results / exceptions generated , ( 4 ) final machine learning predictions , and ( 5 ) the final sub - mitted Jupyter notebook . References [ 1 ] Flight status prediction dataset . https : / / www . kaggle . com / datasets / robikscube / flight - delay - dataset - 20182022 , 2022 . Data Makes Better Data Scientists HILDA ’23 , June 18 , 2023 , Seattle , WA , USA [ 2 ] Lineapy , Jan 2023 . [ 3 ] L . Battle . Behavior - driven testing of big data exploration tools . Inter - actions , 29 ( 5 ) : 9 – 10 , 2022 . [ 4 ] L . Battle , M . Angelini , C . Binnig , T . Catarci , P . Eichmann , J . - D . Fekete , G . Santucci , M . Sedlmair , and W . Willett . Evaluating visual data anal - ysis systems : A discussion report . In Proceedings of the Workshop on Human - In - the - Loop Data Analytics , pages 1 – 6 , 2018 . [ 5 ] K . Bauer . Kpis - the metrics that drive performance management . In - formation Management , 14 ( 9 ) : 63 , 2004 . [ 6 ] S . Chattopadhyay , I . Prasad , A . Z . Henley , A . Sarma , and T . Barik . What’s wrong with computational notebooks ? pain points , needs , and design opportunities . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , CHI ’20 , page 1 – 12 , New York , NY , USA , 2020 . Association for Computing Machinery . [ 7 ] V . Dhar . Data science and prediction . Communications of the ACM , 56 ( 12 ) : 64 – 73 , 2013 . [ 8 ] W . Edwards . The theory of decision making . Psychological bulletin , 51 ( 4 ) : 380 , 1954 . [ 9 ] T . H . Davenport . How strategists use “big data” to support internal business decisions , discovery and production . Strategy & leadership , 42 ( 4 ) : 45 – 50 , 2014 . [ 10 ] A . Head , F . Hohman , T . Barik , S . M . Drucker , and R . DeLine . Managing messes in computational notebooks . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , page 1 – 12 , New York , NY , USA , 2019 . Association for Computing Machinery . [ 11 ] S . Jain , D . Moritz , D . Halperin , B . Howe , and E . Lazowska . Sqlshare : Results from a multi - year sql - as - a - service experiment . In Proceedings of the 2016 International Conference on Management of Data , pages 281 – 293 , 2016 . [ 12 ] S . Kandel , A . Paepcke , J . M . Hellerstein , and J . Heer . Enterprise data analysis and visualization : An interview study . IEEE Transactions on Visualization and Computer Graphics , 18 ( 12 ) : 2917 – 2926 , 2012 . [ 13 ] M . B . Kery , M . Radensky , M . Arya , B . E . John , andB . A . Myers . Thestory inthenotebook : Exploratorydatascienceusingaliterateprogramming tool . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , page 1 – 11 , New York , NY , USA , 2018 . Association for Computing Machinery . [ 14 ] M . B . Kery , M . Radensky , M . Arya , B . E . John , andB . A . Myers . Thestory inthenotebook : Exploratorydatascienceusingaliterateprogramming tool . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , page 1 – 11 , New York , NY , USA , 2018 . Association for Computing Machinery . [ 15 ] F . Kreuter , R . Ghani , and J . Lane . Data science education . 2019 . [ 16 ] A . Lee , D . Xin , D . Lee , and A . Parameswaran . Demystifying a dark art : Understanding real - world machine learning model development , 2020 . [ 17 ] S . Macke , H . Gong , D . J . L . Lee , A . Head , D . Xin , and A . G . Parameswaran . Fine - grained lineage for safer notebook interactions . CoRR , abs / 2012 . 06981 , 2020 . [ 18 ] A . J . Nederhof . Methods of coping with social desirability bias : A review . European Journal of Social Psychology , 15 : 263 – 280 , 1985 . [ 19 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire . A large - scale study about quality and reproducibility of jupyter notebooks . In 2019 IEEE / ACM 16th International Conference on Mining Software Reposito - ries ( MSR ) , pages 507 – 517 , 2019 . [ 20 ] F . Psallidas , Y . Zhu , B . Karlas , M . Interlandi , A . Floratou , K . Karana - sos , W . Wu , C . Zhang , S . Krishnan , C . Curino , and M . Weimer . Data science through the looking glass and what we found there . ArXiv , abs / 1912 . 09536 , 2019 . [ 21 ] L . Quaranta , F . Calefato , andF . Lanubile . Kgtorrent : Adatasetofpython jupyter notebooks from kaggle . In 2021 IEEE / ACM 18th International Conference on Mining Software Repositories ( MSR ) , pages 550 – 554 . IEEE , 2021 . [ 22 ] A . Rule . Design and Use of Computational Notebooks . PhD thesis , University of California , San Diego , USA , 2018 . [ 23 ] A . Rule , A . Tabard , and J . D . Hollan . Exploration and explanation in computational notebooks . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , page 1 – 12 , New York , NY , USA , 2018 . Association for Computing Machinery . [ 24 ] S . Shankar , R . Garcia , J . M . Hellerstein , and A . G . Parameswaran . Op - erationalizing machine learning : An interview study , 2022 . [ 25 ] S . Shankar , S . Macke , S . E . Chasins , A . Head , and A . G . Parameswaran . Bolt - on , compact , and rapid program slicing for notebooks [ scalable data science ] . Proc . VLDB Endow . , 15 ( 13 ) : 4038 – 4047 , 2022 . [ 26 ] D . Xin , L . Ma , S . Song , andA . G . Parameswaran . Howdevelopersiterate on machine learning workflows - A survey of the applied machine learning literature . CoRR , abs / 1803 . 10311 , 2018 . [ 27 ] D . Xin , H . Miao , A . G . Parameswaran , and N . Polyzotis . Production machine learning pipelines : Empirical analysis and optimization op - portunities . CoRR , abs / 2103 . 16007 , 2021 . [ 28 ] D . Xin , E . Y . Wu , D . J . - L . Lee , N . Salehi , and A . Parameswaran . Whither automl ? understanding the role of automation in machine learning workflows . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems , CHI ’21 , New York , NY , USA , 2021 . Association for Computing Machinery .