How to Support Users in Understanding Intelligent Systems ? Structuring the Discussion Malin Eiband 1 , Daniel Buschek 2 , Heinrich Hussmann 1 1 LMU Munich 2 University of Bayreuth Abstract The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human - computer interaction . Research in the field therefore highlights the need for transparency , scrutability , intelligibility , interpretability and explainability , among others . While all of these terms carry a vision of supporting users in understanding intelligent systems , the underlying notions and assumptions about users and their interaction with the system often remain unclear . We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets , user involvement , and knowledge outcomes to reveal , differentiate and classify current notions in prior work . This framework aims to resolve conceptual ambiguity in the field and provides researchers with a thinking tool to clarify their assumptions and become aware of those made in prior work . We thus hope to advance and structure the dialogue in the HCI research community on supporting users in understanding intelligent systems . Revised unpublished manuscript , draft version 2 . 0 , 21 January 2020 . 1 Introduction Interactive intelligent systems violate core interface design principles such as predictable output and easy error correction [ 5 , 29 ] . This makes them hard to design , understand , and use – an observation that has already been made decades earlier [ 43 ] , but it is only in the last years that machine learning has increasingly penetrated everyday applications and thus refuelled the discussion on how we want interaction with such systems to be shaped . One particularly challenging property of intelligent systems is their opaqueness . As a result , re - searchers [ 3 , 9 , 38 ] , practitioners [ 20 ] , policy - makers [ 75 ] and the general public [ 56 ] increasingly call for intelligent systems to be transparent [ 32 ] , scrutable [ 52 ] , explainable [ 77 ] , intelligibile [ 64 ] and interac - tive [ 29 ] , among others , which we will henceforth refer to as system qualities . Work on the system qualities follows a joint and urgent maxim : Designing interaction in a way that supports users in understanding and dealing with intelligent systems despite their often complex and black - box nature . Linked by this shared goal , the diverse terms are often employed interchangeably – and yet , prior work implies divergent assumptions about how users may best be supported . For instance , work on interpretability has recently been criticised for unclear use of the term [ 28 , 68 ] , a survey on explainability in recommenders found incompatible existing taxonomies [ 73 ] , and discussions about system transparency and accountability revealed diverging assumptions ( i . e . disclosing source code vs system auditing through experts ) [ 30 ] . A recent HCI survey shows the fractured terminological landscape in the field [ 1 ] . In particular , for supporting user understanding of intelligent systems , clarifying concepts and connect - ing diverse approaches is crucial to advance scholarship , as pointed out in a recent “roadmap” towards a 1 a r X i v : 2001 . 08301v2 [ c s . H C ] 4 F e b 2020 Usermindsets Userinvolvement Knowledgeoutcomes utilitarian interpretive critical active passive outcome process interaction meta " Why did the system do X ? What does the system think I know ? How fair and controllable is it ? " . . . Figure 1 . Our framework for structuring the discussion of how to support users in understanding intelligent systems : We examine user questions in the literature ( left , examples ) to synthesise three categories overarching prior work ( centre ) , namely assumed user mindsets , user roles and knowledge outcomes . We discuss divergent instances of each category ( right ) to differentiate approaches and solution principles in the literature . rigorous science of interpretability [ 28 ] . More generally speaking , a lack of conceptual clarity impedes scientific thinking [ 44 ] and presents challenging problems for researchers in the respective field : First , a lack of overarching conceptual frameworks renders new ideas difficult to develop and discuss in a structured way . Second , blurred terminological boundaries impede awareness of existing work , for example through varying use of keywords . Third , new prototypes often remain disconnected from the existing body of design solutions . To address this , we need a clearer conceptual understanding of the assumptions that underlie how prior work envisions to foster user understanding of intelligent systems . In this paper , we thus aim to answer the following research questions : RQ1 : Which assumptions about users and interaction with intelligent systems do researchers make when referring to the system qualities ? RQ2 : How can we structure and differentiate these assumptions ? 2 Contribution and Summary We analyse both theoretical concepts and prototype solutions through the lens of implied user questions and synthesise a conceptual framework integrating user mindsets , user involvement and knowledge outcomes to reveal , differentiate and classify notions on supporting user understanding of intelligent systems in prior work . Our analysis revealed three categories that capture and differentiate current assumptions about users and interaction with intelligent systems from an HCI perspective ( also see Figure 1 ) : 1 . User mindsets – what users seek to know ( e . g . do they want to know why the system did X vs what it was developed for ) , 2 . User involvement – how users gain knowledge ( e . g . do they actively inquire into the system or do they get presented information by it ) , and 3 . Knowledge outcomes – which knowledge users gain ( e . g . about a specific system output or how to correct the system ) . In particular , as we will describe later in more detail , we argue that these three categories are linked to users’ intentions when using a system , influence the direction of information transfer between user and system , and reflect the envisioned outcome of the system qualities , respectively . 2 Our view helps to resolve conceptual ambiguity in the field and provides researchers with a thinking tool to clarify their assumptions and become aware of those made in prior work . We thus hope to advance and structure the dialogue in the HCI research community on supporting users in understanding intelligent systems . 3 Scope and Foundations Before we present the results of our analysis in detail , we first discuss fundamental conceptual prerequisites for our work and locate our own perspective . 3 . 1 Intelligent Systems Our work focuses on interaction with intelligent systems . Many definitions for such systems exist : Following Singh [ 85 ] , a system is intelligent if we need to “attribute cognitive concepts such as intentions and beliefs to it in order to characterize , understand , analyze , or predict its behavior” . This definition hints at the potential complexity of intelligent systems and the resulting challenges for users to understand them , and thus motivates work on various system qualities in the field . 3 . 2 A Pragmatic View on Supporting User Understanding One can identify two opposing perspectives in the larger discussion of supporting users in understanding intelligent systems : A normative and a pragmatic one [ 31 ] . The normative perspective is visible in the ethical discourse about intelligent systems or reflected in legislation . It provides users with what has been called a “right to explanation” [ 36 ] , such as recently articulated in the GDPR [ 75 ] , and ties lawful use of intelligent systems to the ability to make users understand their decision - making process [ 41 ] . However , this perspective often does not take into account a user - centered point of view that considers users’ needs when building systems . In this paper , we therefore adopt the pragmatic perspective , which strives to best support users during interaction with intelligent systems . We define the purpose of this support by transferring a statement by Lynham from philosophy of science [ 69 ] to the context of our work : Supporting users in understanding intelligent systems means helping people to use a system better and in more informed ways , and to better ends and outcomes . We argue that this perspective captures well and articulates a core assumption of work on the system qualities : We as HCI researchers in the field of intelligent systems strive to create interfaces and interactions that are explainable , understandable , scrutable , transparent , accountable , intelligible , and so on , precisely because we envision users to then interact with these systems in more informed , effective and efficient ways . 3 . 3 User Knowledge and Understanding In general , HCI has widely adopted mental models [ 50 ] as representations of the knowledge users possess about a system [ 72 ] , and this is no different in work on the system qualities ( e . g . [ 31 , 58 , 90 ] ) . Mental models originate from a constructivist perspective on knowledge , where knowledge is seen as individually constructed , subjective interpretations of the world , based on previous experiences and assumptions [ 92 ] . In this paper , we adopt this perspective when we talk about knowledge , and use it interchangeably with understanding . Moreover , we assume that knowledge is gained through the transmission of information between user and system . 3 Systemquality User questions ( examples ) User mindsets User involvement Knowledge outcomes and qualities Accountability How fair and controllable is the system ? [ 77 ] How fair are system decisions ? [ 12 ] How will algorithmic - decision making impact my ( social ) work practices ? [ 16 ] How can I as a designer / developer of decision systems support human values ? [ 42 , 91 ] Users critique the system , often in a wider context beyond use ( e . g . legal , ethicalconcerns ) . Users get informed by the system , or by a third party reporting on the system ( e . g . journalists ) [ 25 , 27 ] . People discuss the system and / or challenge its implications beyond use [ 82 ] , e . g . in its organisationalcontext [ 16 , 42 , 91 ] . Users seek reflection on outputs , processes , as well as on reasons behind and implications of the system ( meta ) . What is relevant – and to what extent – may depend on the context of the system’s deployment . Debuggability ( end - user debugging ) Why did the system do X ? How / under what conditions does it do Y ? Why did it not do Y ? What ( else ) is it doing ? What if there is a change in conditions , what would happen ? [ 65 ] Users gain insight into the system to fix its errors . Users fix the system’s errors . Users need to understand outputs , processes , and interactions to give good feedback and correct the system . Users make the system more relevant to them by correcting system errors . Explainability Can I trust this model ? [ 79 ] Should I trust this prediction ? [ 39 , 79 ] What are the strengths and limitations of the system ? How can I add my knowledge and skills to the decision process ? [ 39 ] How are input and output related ? [ 49 ] Why does the system think that I want / need X ? [ 11 ] Why is this recommendation ranked at the top ? [ 89 ] Users gain insight into the system to better use it . Users get informed by the system . Users get information about outputs and processes . Explanations should be relevant to the user . They should be “sound and complete” , but not overwhelming [ 57 , 60 ] . Intelligibility Why did the system do X ? How / under what conditions does it do Y ? Why did not do Y ? What ( else ) is it doing ? What if there is a change in conditions , what would happen ? [ 22 , 65 ] Users want to use the system in better ways or to gain trust [ 67 ] . Users actively inquire into the system’s inner workings . Users seek information about outputs and processes . Users’s demand informs what is relevant . Factors related to system and context influence this [ 65 ] . Interactivity ( interactivemachinelearning ) How I can assess the state of the learned concept ? Where does the model fail ? Why did the system fail in this specific instance ? [ 29 ] How well does the system know the domain ? How sure is the system that a given output is correct ? Did the system do a simple or complex thing to arrive at the output ? [ 80 ] How to combine models ? [ 86 ] Which model works best ? [ 6 ] Users inspect the system state to refine it or guide its training [ 29 ] . Users iteratively refine the system and guide its training by giving feedback [ 29 ] . Users need to understand outputs , processes , and interactions to guide the system . What is relevant to know is defined by the machine learning task that users and system solve together . Interpretability How sensible – and not arbitrary or random – is the system ? [ 77 ] Why ? questions [ 34 ] Can you trust your model ? What else can it tell you about the world ? [ 68 ] Users gain utilitarian and interpretative insight into the system to bridge the gap between the system’s criteria and full real - world context [ 28 , 53 ] . Users get information about the system’s inner workings [ 28 , 68 ] . Users can access information about outputs and processes , which may include low - level ( expert ) information ( e . g . on inner states [ 53 ] ) . What is relevant to know depends on the user’s task with the system . Explicit call for rigorous evaluation [ 28 ] . Scrutability Why / How did the system do X ? What else does the system think I ( don’t ) know ? What would the system do if I did Y ? What does the system do for other people ? How can I tell the system what I ( don’t ) want ? [ 52 ] How can I efficiently improve recommendations ? [ 10 ] Users want to be able to interpret the system’s decisions . They may analyse and control it for more efficient use . System decision and behaviour is based on a user model , which users can adequately access and control . Users make “real effort” [ 52 ] . Users gain understanding of outputs and processes . They may also learn about interactions to influence how the system uses the user model . Information should be relevant to users , yet they may also learn about what the system considers relevant . Transparency How does the system produce an output ( i . e . data sources , reasoning steps ) ? Why did the system do sth . ( i . e . justification , motivation behind the system ) ? What is informed by the intelligent system ( i . e . reveal existence of intelligent processing ) ? How was the system developed and how is it continually being improved ? [ 77 ] Users interpret the system’s output and question the underlyingmechanisms . Users get informed by the system . Users seek understanding of outputs and processes , also beyond use ( meta ) . What is relevant – and to what extent – may depend on the context of the user’s inquiry . Table 1 . The system qualities in focus of our work , along with exemplary user questions and implied user mindsets , user involvement , and knowledge outcomes . 4 4 Analysis Here , we shortly describe our analysis process , namely how our paper set was collected and coded . 4 . 1 Paper Search To find the most widely - adopted system qualities for our survey , we started with search terms for system qualities based on our experiences as researchers working at the intersection of HCI and AI for several years . We expanded our view with the topic networks presented by Abdul et al . [ 1 ] in their 2018 survey of over 12 . 000 papers at this intersection . Their search terms cover many system qualities that we seeked to address a priori ( e . g . interpretability , intelligibility , scrutability , explainability ) . They discovered related topics using LDA topic models . We reviewed these topic networks to expand our set of system qualities ( e . g . the topics point to accountability , different types of transparency , and end - user debugging ) . The final set included scrutability , interpretability , transparency , explainability , intelligibility , interactivity ( interactive Machine Learning ) , debuggability ( end - user debugging ) , and accountability . With keyword searches on the ACM Digital Library we collected about 6000 papers on the terms . We filtered them by checking title , abstract and venue and used snowball search around most cited and recent papers to create a representative set of about 250 papers for manual coding . To be included in the final set , papers had to fulfil the below criteria : 1 . The presented contribution focuses on the system qualities and is linked to intelligent systems , 2 . the contribution involves an HCI perspective ( e . g . via a prototype , user study , design guidelines , etc . ) . Note that the final paper set is not a comprehensive list , but was selected to represent the current diversity of approaches . It is available on the project website : link - removed - for - anonymous - review 4 . 2 Paper Coding As a manifestation of users’ information demand , Lim and Dey [ 65 , 67 ] established questions users have about the workings of intelligent systems ( e . g . Why did the system do X ? ) . Since then , such user questions have gained popularity in related work on the system qualities as a way of anchoring design suggestions and solutions . For example , user questions are articulated in work by Kay and Kummerfeld [ 52 ] ( scrutability ) , Kulesza et al . [ 61 ] ( end - user debugging ) , or Rader et al . [ 77 ] ( transparency / accountability ) . The widespread use of this approach indicates the value and utility of user questions to frame research , even if these questions are not always elicited from actual users . Since so many researchers postulate or imply user questions , they help us to extract and contrast underlying perspectives : On the one hand , they connect prior work on the system qualities , on the other they reveal conceptual differences . In this paper , we thus use these questions as a coding scheme for making explicit the assumptions underlying related work , also across research on individual system qualities . We do so either by relating to such questions directly posed in related work , or by formulating questions that capture the stated assumptions , motivations , and envisioned benefits of the research . In this way , the paper set was coded by the first two authors . Coding disagreement was resolved following an open discussion strategy [ 21 ] . To provide an overview , Table 1 lists the system qualities in focus of our work , along with exemplary corresponding user questions collected from related work . 5 5 A Conceptual Framework for Supporting Users in Understand - ing Intelligent Systems Our analysis revealed three categories that capture and differentiate current assumptions about users and interaction with intelligent systems . They thus serve as a conceptual framework for structuring the discussion about supporting users in understanding intelligent systems from an HCI perspective . In the next sections , we will present each category in detail . 5 . 1 What do Users Seek to Know ? User Mindsets The user questions addressed in work on different system qualities imply different assumptions about what users seek to know : For example , searching for information about why a certain system output came into being [ 67 ] implies a very different kind of interest than wanting to know how well a program knows a given domain [ 80 ] or how a system was developed and continually improved [ 77 ] . Likewise , this is true for users asking how to correct system errors [ 61 ] compared to users that want to be merely informed about the presence of algorithmic decision - making [ 77 ] . To capture these differences , we introduce the category user mindsets . In psychological research , mindsets describe the “cognitive orientation” of people that precede the formation of intentions and planning of successive actions towards reaching a goal [ 35 ] . In the same manner , for this work we define user mindsets as users’ cognitive orientation that guides concrete intentions to interact with an intelligent system . From our analysis emerged three such mindsets that we find in prior work on the system qualities : utilitarian , interpretive , and critical , described in detail next . 5 . 1 . 1 Utilitarian Mindset A utilitarian mindset aims to predict and control system behaviour to reach a practical goal . This mindset carries a strong notion of utility and / or usability . Consequently , a utilitarian mindset is reflected by many examples in work on system qualities that imply a very practical view on user inquiry , such as found in work on explainability and intelligibility . For example , users might want to understand system recommendations to better compare products they are interested in : Pu and Lim [ 76 ] use explanations to the question Why was this recommended to me ? as a means for users to compare different recommendations to find a product they are looking for . Moreover , users might want to train more effectively when using an intelligent fitness coach [ 31 ] , or understand a system when they financially depend on it , such as observed in AirBnB [ 48 ] or social media [ 17 ] . Another example has been presented by Coppers et al . [ 22 ] , where users worked more efficiently with a system due to feedforward based on What if ? questions . Also , user questions found in work on scrutability such as How can I efficiently improve recommendations ? [ 10 ] imply a utilitarian mindset . Furthermore , this utilitarian mindset can be found on a meta - level in work on interactive machine learning and end - user debugging . Research in these areas addresses user questions such as How can I assess the state of the learned concept ? Where does the model fail ? [ 29 ] , How sure is the system that a given output is correct ? [ 80 ] , How to combine models ? [ 86 ] , Which model works best ? [ 6 ] , or How do changes affect the rest of the system ? [ 62 ] . These and similar questions imply a focus on recognising and handling system error , giving feedback , or analysing the system to better work with it in the future . 5 . 1 . 2 Interpretive Mindset An interpretive mindset strives to interpret system actions based on one’s perception of and experience with the system and its output . This mindset embraces the notion of user experience . 6 When users adopt this mindset , they do not necessarily want to reach a particular practical goal , but rather to understand the system based on a certain experience they have made . For example , a social media user might want to understand why posts of particular friends are not shown [ 17 ] . Moreover , an interpretive mindset might be adopted by users who do not understand how they are being profiled , when they believe their feedback is not being considered or feel they lack control over system output [ 17 ] . Examples for an ( implied ) interpretive mindset can be found in work on transparency ( e . g . How sensible – and not arbitrary or random – is the system ? [ 77 ] ) and interpretability ( e . g . What else can the model tell me about the world ? [ 68 ] ) . Moreover , it is reflected in many user questions articulated in work on scrutability , for example What else does the system think I ( don’t ) know ? , What would the system do if I did Y ? , or What does the system do for other people ? [ 52 ] . Although control also plays an important role in research in this field , the underlying perspective is framed in terms of experience with and perception of the system and its output rather than a practical goal . 5 . 1 . 3 Critical mindset A critical mindset stresses normative , ethical and legal reflection about intelligent systems . This echoes the ongoing wider discussion about the system qualities , such as transparency , explainabil - ity and accountability ( e . g . [ 32 , 41 , 70 , 93 , 94 ] ) . For example , a user might critique systems for their missing social intelligence [ 16 , 17 ] or might want to know why a system was developed in a certain way [ 77 ] . A critical mindset may thus be decoupled from system use . Calls for support of critical inquiry are mainly found in work on system accountability . User questions include How was the system developed and how is it continually being improved ? , What is informed by the intelligent system ( i . e . reveal existence of intelligent decision - making and processing ) ? , How fair and controllable is the system ? [ 77 ] , How fair is a system decision ? [ 12 ] or Can I trust this model ? [ 79 ] and Should I trust this prediction ? [ 39 , 79 ] . User mindsets help to understand what users seek to know when interacting with an intelligent system . We should thus make explicit which mindset ( s ) we assume as a basis for our work ( e . g . utilitarian , interpretive or critical ) . 5 . 2 How do Users Gain Knowledge ? User Involvement As introduced in the Scope and Foundations section of this paper , we assume that user understanding is built through the transmission of information between user and system . Our analysis revealed that the great majority of work on the system qualities envisions this transmis - sion of information in form of a dialogue , that is as a “cycle of communication acts channelled through input / output from the machine perspective , or perception / action from the human perspective” [ 44 ] . Dia - logue as an interaction concept inherently stresses the need for users to understand the system ( and vice versa ) [ 44 ] . It has even been argued that the characteristics of intelligent systems necessarily involve some sort of dialogue in order for users to understand them [ 80 ] . Elements of dialogue , such as structuring interaction as stages [ 44 ] , are commonly found in work on the system qualities . Most notably , end - user debugging [ 57 ] and interactive machine learning [ 51 ] make use of mixed - initiative interfaces [ 45 ] . It thus seems that the system qualities almost imply this concept of interaction , so closely are they interwoven with a dialogue structure . To support users in understanding intelligent systems , information may thus be transferred in two directions , either from user to system , or from system to user . From a user perspective , this determines how users gain knowledge – either through action ( active ) or perception ( passive ) . We introduce our second framework category , user involvement , to capture these two ways of gaining knowledge . User involvement thus describes interaction possibilities to transfer information to or receive 7 information from the system as a basis for user understanding . In the following two sections , we distinguish work on the system qualities according to these two directions . 5 . 2 . 1 Active User Involvement ( User - to - System ) User questions such as How can I tell the system what I want ? [ 52 ] , How can I detect system errors ? [ 61 ] , or What do I have to change to correct the system ? [ 61 ] point to active users whose corrections and feedback are utilised by the system . The dialogue between system and user may be initiated by both sides and is then indeed based on turn - taking , as described earlier . For example , Alkan et al . [ 2 ] presented a career goal recommender that literally employs a dialogue structure in order to suggest items and incorporate user feedback . Work on scrutability by Kay and Kummerfeld [ 52 ] emphasises the “real effort” users must make when inquiring into a system . Other work on the system qualities , in particular in end - user - debugging and interactive machine learning , sees users in active roles as debuggers [ 59 ] or teachers [ 5 ] . Systems support an active user by offering interface controls tied to aspects of their “intelligence” ( e . g . data processing , user model ) . These controls may enable users to experiment with the intelligent system ( e . g . user model controls [ 52 ] ) . For example , if a user interface offers switches for certain data sources ( e . g . in a settings view ) , users can actively experiment with the way that these data sources influence system output ( e . g . switch off GPS to see how recommendations change in a city guide app ; also see [ 52 ] ) . Moreover , Coppers et al . [ 22 ] introduced widgets for system feedforward that allow for active inquiry to answer What if ? user questions such as What will happen if I click this checkbox ? . Another example is a separate , dedicated GUI for such experimentation , which allows users to directly set the values of certain system inputs and check the resulting output ( e . g . “intelligibility testing” [ 67 ] ) . Moreover , many visual explanations offer direct manipulation that also puts users into an active role : For instance , work in interactive machine learning proposed interactions with classifier confusion matrices to express desired changes in resulting decisions [ 51 , 86 ] . Similarly , work on explanations for spam filtering enabled users to influence the classifier via interactive bar charts of word importance [ 57 ] . Furthermore , explainability 5 . 2 . 2 Passive User Involvement ( System - to - User ) User questions such as Why does the system think that I want / need X ? [ 11 ] , Why did the system do X ? [ 65 , 77 ] , Why did it not do Y ? [ 67 ] or How does the system produce an output ? [ 77 ] suggest that users want to get informed about the systems inner workings , but do not actively provide the system with feedback and corrections . Users may still initiate the dialogue with the system , but are then restricted to be recipients of information . This way of user involvement is typically assumed by work on transparency and explainability , where displaying information about a system’s inner workings is a common tool for user support . For example , related work proposed visual and textual explanations that show how recommendations are influenced by data from customers with similar preferences [ 33 ] . Further examples of supporting user understanding in a passive way include icons that indicate “intelligent” data processing [ 31 ] , interaction history [ 46 ] , annotations for specific recommendations [ 13 ] , and plots and image highlighting for classification decisions [ 79 ] or recommendations [ 88 ] . Users may also transition between passive and active involvement , which is supported , for example , by interactive visual explanations [ 57 ] . User involvement describes how user knowledge is built during interaction with a system . This depends on the direction of information transmission between user to system ( e . g . users are involved in an active or passive way ) . We should explicitly state the nature of user involvement and how it is manifested in and supported through design . 8 5 . 3 Which Knowledge Do Users Gain ? Knowledge Outcomes The envisioned result of the different system qualities is knowledge that users gain about an intelligent system . However , this knowledge may refer to different aspects of the system and interaction , such as a specific recommendation [ 76 ] or the “reasoning” of a system [ 79 ] . To account for this variety of which knowledge users gain , we introduce our third framework category , knowledge outcomes . These characterise the nature of user understanding developed about an intelligent system . Overall , our analysis surfaced four different knowledge outcomes currently addressed in the literature ( output , process , interaction , and meta ) . These knowledge outcomes are not unique to HCI or intelligent systems . For example , output and process knowledge can be found in work on theory on gaining knowledge in practice [ 69 ] . Moreover , work on complex problem solving articulates output , process and structural knowledge [ 83 ] , the latter being similar to our interaction knowledge . We also introduce two qualities of knowledge as have emerged from the reviewed literature . Borrowing established terms for knowledge qualities in applied research theory [ 40 , 69 ] , we summarise them as rigour and relevance of knowledge . 5 . 3 . 1 Output and Process Knowledge Output knowledge targets individual instances of an intelligent system ( e . g . understanding a specific movie recommendation ) . In contrast , process knowledge targets the system’s underlying model and reasoning steps ( e . g . the workings of a neural network that processes movie watching behaviour ) . Explainability research in particular distinguishes between explanations for instances and models . For example , Ribeiro et al . [ 79 ] explain classifiers with regard to two questions , Should I trust this prediction ? and Can I trust this model ? . Therefore , they design for both output and process knowledge . These two knowledge types also motivate the what and how questions posed by Lim and Dey in their work on intelligibility [ 65 ] ( e . g . What did the system do ? ) . Also , Rana and Bridge [ 78 ] introduced chained explanations ( called “Recommendation - by - Explanation” ) to explain a specific output to users . Moreover , work on accountability makes system reasoning accessible to users to support the development of process knowledge [ 12 ] . 5 . 3 . 2 Interaction and Meta Knowledge Beyond these two knowledge types , we further propose the two terms interaction knowledge and meta knowledge in this paper : First , interaction knowledge describes knowing how to do something in an interactive intelligent system . For example , supporting users in gaining this type of knowledge motivates questions in work on scrutability ( e . g . How can I tell the system what I want to know ( or not ) ? [ 52 ] ) , interactive machine learning ( e . g . How to experiment with model inputs ? [ 5 ] ) , and end - user debugging ( e . g . How can I tell the system why it was wrong ? [ 61 ] , How can I correct system errors ? [ 59 ] ) . Second , meta knowledge captures system - related knowledge beyond interaction situations , such as information from a developer blog . For example , meta knowledge motivates some questions in work on transparency , such as How is the system developed and how is it continually improved ? by Rader et al . [ 77 ] . They also explicitly add Objective explanations that inform users about how “a system comes into being” that result in meta knowledge ( e . g . development practices and contexts ) . Moreover , this knowledge type is a main driver of work on accountability , in which computer science overlaps with journalism : For instance , Diakopoulus “seeks to articulate the power structures , biases , and influences” of intelligent systems [ 25 ] . 9 5 . 3 . 3 Rigour and Relevance of Knowledge Rigour : Kulesza et al . propose the concepts of soundness and completeness in their work on explanations in intelligent systems [ 57 , 60 ] : soundness is truthful explanation , and completeness means explaining the whole system . Gilpin et al . [ 34 ] also refer to completeness , yet understand it as supporting anticipation of system behaviour in more situations . For an overarching view , we generalise this to a broader level : We regard soundness and completeness as facets of rigour . Linked back to the work by Kulesza et al . [ 57 , 60 ] , this means that a rigorous explanation , and the resulting understanding of a system , should be sound and complete . Relevance : A rigorous understanding does not need to be useful . We argue that this aspect should be of explicit interest for a pragmatic HCI perspective . We thus consider relevance as another general quality of knowledge [ 69 ] that is crucial to make explicit in the specific context of user understanding of intelligent systems . This quality highlights our pragmatic view : Elements like explanations are valuable if they add utility , that is , if they help users to gain knowledge that is relevant for using the system in better ways and towards better outcomes . In this pragmatic sense , this quality echoes Kulesza et al . ’s suggestion to “not overwhelm” users with ( irrelevant ) information [ 57 , 60 ] . What is relevant to know , and to which extent , may also depend on factors such as task and complexity of the system [ 18 ] . Knowledge outcomes and qualities characterise and make explicit what kind of user understand - ing a system seeks to facilitate . We should thus articulate the goals of our work ( e . g . output , process , interaction , and meta knowledge ) and reflect on rigour and relevance of the respective kind of knowledge . 6 Thinking Tool : Structuring Past & Future Work We have presented three categories for supporting user understanding of intelligent systems as emerged from our analysis of the literature – user mindsets , user involvement , and knowledge outcomes . These categories highlight differences and commonalities between work on the system qualities and serve as a conceptual framework of supporting users in understanding intelligent systems . Our framework introduces an overarching user - centric structure to the field that abstracts from the fractured terminological landscape . We now propose to use our framework categories as a thinking tool for researchers and practitioners to clarify and make explicit the assumptions of their work , and to structure past and future work and discussion about how to support users in understanding intelligent systems . The boxes presented throughout this article provide inspiration on what to consider . The following sections highlight further applications of our thinking tool . 6 . 1 Structuring Past Approaches and Solution Principles As guidance , Table 1 presents a summary of our view on the system qualities in focus of this paper , described through the lens of our work . Note that we do not attempt to ( re ) define these terms , but rather to provide a guiding map of the terminological landscape for further reference and discussion . Moreover , Table 2 presents exemplary solution principles from the literature to illustrate how user mindsets , user involvement and knowledge outcomes may be used to structure past work . We do not claim to provide a comprehensive survey in our work . Nevertheless , these examples were selected from a set of about 250 papers collected through search on the ACM digital library , using keywords from the topic networks presented by Abdul et al . [ 1 ] , including the listed system qualities . Studying the approaches as arranged in Table 2 reveals interesting structures : For example , explanations appear across the charted space and thus could be seen as the go - to building block for many solution 10 Knowledge Outcomes Output Process Interaction Meta U s er I n vo l v e m e n t A c ti v e Visualisations explain classification via features , user “explains back” corrections by manipulating these plots [ 57 , 61 ] . “Reasons” tabs show sensor - specific visualisations that explain current prediction [ 66 ] . Users build if - rules in an editor , system detects problems via simulation ( e . g . loops ) , user corrects them [ 24 ] . “Experimentation view” allows users to try out inputs and see system output [ 67 ] . Confusion matrices show current state of classifier [ 51 , 86 ] . Natural language dialogue enables users to ask system what they could do next [ 87 ] . Users manipulate confusion matrix to change model [ 51 ] , including re - combining multiple models [ 86 ] . Beyond runtime : Open source enables code audits and facilitates understanding of system [ 19 ] . U tilit a r i a n M i nd s e t P a ss i v e Bar charts and image regions show importance of predictors to explain specific classification [ 79 ] . Visualisations show input of similar users to explain specificrecommendation [ 33 ] . Explaining a classifier by explaining multiple specificclassifications [ 79 ] . Animation shows learning algorithm at work [ 47 ] . Text highlighting shows which words contributed to meeting detection in emails . [ 55 ] List shows user’s past interactions to explain specificrecommendation [ 46 ] . Step - by - step explanations of trigger - action rules by simulating user and system actions [ 23 ] . Beyond single systems : Educate public in computational skills to facilitate system understanding overall [ 19 ] . A c ti v e Rule - based reasoning system verbalises system decisions in natural language dialogue with user [ 87 ] . Separate profile page displays current user model [ 52 ] . Natural language dialogue enables users to ask system why it has not decided differently . [ 87 ] Constructivist learning : user manipulates system and updates mental model of it based on resulting changes in output [ 81 ] . Separate profile page enables users to edit what the system knows about them [ 52 ] “Algorithmic profiling management” : Profile page reveals what system knows and how this influences content , including past interactions ; controls enable modifications [ 4 ] . “Algorithmic UX” beyond interaction : Users engage in communication and relationship building with intelligent agents [ 74 ] . I n t e r p r e ti v e P a ss i v e Icon indicates which system output is influenced by AI [ 31 ] . Output shown with textual explanation of the decision process [ 54 ] . Animation indicates how system output is generated ( e . g . dice roll for randomness ) [ 31 ] . Explain Recommendations with usage statistics ( e . g . global popularity , repeated interest ) [ 13 ] . Beyond code : Educate / sensitise developers and decision makers to consequences of systems [ 19 ] . Icons indicate when and for which high - level goal ( e . g . ads ) user data is processed by the system [ 84 ] . A c ti v e “Algorithmicaccountability reporting” : Journalists report on black box systems , e . g . by trying out inputs systematically [ 25 , 27 ] . Beyond system understanding : Society must look not into systems but across them , that is , see their role within a larger network of actors ( incl . humans and institutions ) [ 8 ] . Challenging the system : People learn from past interactions and output how to challenge system intelligence and its normative implications through unexpected or malicious input ( e . g . manipulating public chatbot via twitter ) [ 71 ] . Beyond system use : People discuss and reflect on social implications and context of the system’s output [ 16 , 82 , 91 ] . C r i c ti ca l P a ss i v e Record models , algorithms , data , decisions for later audit [ 9 ] . Annotate recommended content pieces with indicators for quality / reliability of their source ( e . g . for news ) [ 63 ] . Textual explanations of system intelligence on a high level , not integrated into the system ( e . g . articles about system ) [ 77 ] . Explaining the logic behind an algorithm with another algorithm [ 15 ] . “Algorithmic Imaginary” : People develop understanding of intelligent systems and how to influence them based on how “they are being articulated , experienced and contested in the public domain” [ 17 ] . Textual explanations of developers’ intentions on a high level , not integrated into the system ( e . g . articles about system ) [ 77 ] . Table 2 . Examples of approaches and solution principles for supporting users’ understanding of intelligent systems , structured through our framework . This is not a comprehensive survey ; examples were selected to illustrate the diversity of approaches in the literature . 11 principles . They commonly provide output and process knowledge via text and / or plots [ 33 , 46 , 54 , 57 , 61 , 66 , 79 ] . Sometimes these representations also allow for interactivity and user corrections [ 57 , 61 , 67 ] , in particular when explanations are referred to in work on scrutability , end - user debugging , and interactive machine learning [ 24 , 51 , 52 , 86 ] . Explanations commonly arise from utilitarian mindsets , yet they also appear in work with interpretive and critical questions [ 13 , 26 , 52 , 77 ] . 6 . 2 Reflecting on Your Own Approach 6 . 2 . 1 Reframing User Questions User questions are a helpful way to uncover users’ information needs . Our thinking tool can be used to re - frame such questions in related work , in particular by reconsidering the underlying mindsets to view questions from a novel angle : For example , a question such as Why did the system do X ? is currently mostly tied to a context implying a utilitarian mindset [ 65 ] . However , this question could also reflect other mindsets and thus different underlying user motives for inquiry , depending on the envisioned context . Design solutions to this question could then foster utilitarian ( e . g . explain feature influences ) , interpretive ( e . g . explain in terms of a user’s daily life context ) , critical ( e . g . explain system decision given a community’s norms ) , or all three mindsets . 6 . 2 . 2 Explicating Perspectives with Mixed Mindsets Related , reflecting on the three mindsets presented here can help to discover structure in perspectives that mix multiple mindsets : For example , a recent set of 18 guidelines for interaction with AI [ 7 ] contained mostly utilitarian guidelines ( e . g . “Support efficient dismissial” , “Provide global controls” ) , yet two stand out as following a critical mindset ( “Match relevant social norms” , “Mitigate social biases” ) . Our lens allows to clarify and explicate this mix , revealing , in this example , that the guidelines already follow a broader perspective than the related work itself alluded to with its stated focus “on AI design guidelines that [ . . . ] could be easily evaluated by inspection of a system’s interface” . In this case , revealing this mix could help to structure discussions about such guidelines . 6 . 2 . 3 Explicitly Determining Relevance What is relevant to know for users is considered differently across the system qualities . Highlighting relevance and rigour ( see section on knowledge outcomes and Table 1 last column ) thus helps to reflect on what we consider important , for whom , and to what extent – and how we choose to determine it . For example , explanation design often involves non - expert users , possibly via a user - centred design process [ 31 ] or scenario - based elicitation [ 67 ] . This informs what is relevant for these users , what should be explained , and to what extent . In contrast , interactive machine learning focuses on information that is relevant to the task of the system ( e . g . to improve a classifier ) , which is often operated by experts [ 5 ] . Therefore , what is relevant here ( and to what extent ) is foremost informed by the machine learning task , and less so by studying or asking end - users . This can be seen , for example , in the UI elements derived in a recent survey on interactive machine learning [ 29 ] , which are closely coupled to the machine learning task ( e . g . they serve “sample review” , “feedback assignment” , etc . ) . As another example , work on end - user debugging presents an action - focused middle - ground , between user - focused ( as explainability ) and system - focused ( as interactive machine learning ) : Here , resulting knowledge should help users to make the system more relevant to them , for example , by correcting system errors from the users’ point of view ; users may be both experts [ 6 ] or non - experts [ 61 ] . 12 6 . 2 . 4 Informing Methodology Our thinking tool may be used to motivate methodological choices , for example when informing or evaluating the design of a new approach for supporting user understanding : For instance , work catering to a utilitarian mindset might benefit from a controlled environment and precise measurements in a lab study . Even simulation of system decisions might be a ( first ) option [ 28 ] . A lab study might also be a suitable choice to evaluate support for developing interaction knowledge , since users can be directly observed during interaction ( e . g . see [ 52 , 86 ] ) . In contrast , if a design or research question targets an interpretive mindset and / or meta knowledge it might be worthwhile or required to study user and system in their daily contexts of use ( e . g . see lab vs field study in education context in [ 52 ] ) . The same holds for work motivated by a critical mindset , yet other methods exist here as well , such as online surveys or data analyses of views expressed through mass media and social network discussions [ 63 ] , or policy and legal texts [ 9 , 36 , 75 ] . 6 . 3 Going Beyond 6 . 3 . 1 Inspiring New Approaches Our thinking tool can provide inspiration for new approaches and solution principles . We illustrate this using the proposed mindsets : UIs could support users in examining the system with different mindsets via “modes” for explanation views . Users could then switch between utilitarian explanations ( e . g . explain a recommendation with product features ) and interpretive or critical ones ( e . g . explain system beliefs about a user , such as that user is part of a certain target group ; reveal that recommendations are assembled by an AI and not by a human , cf . [ 31 ] ) . A more radical solution could offer three different views of the system as a whole that display or hide UI elements depending on the respective “mode” . Or the mindsets might simply help to decide which user approach to support in a system , and to identify those that remain unaddressed so far . 6 . 3 . 2 Understanding Systems Beyond Interaction Situations The critical mindset and meta knowledge capture a crucial difference between traditional ( non - intelligent ) systems and what we see today and what is yet to come : Systems are increasingly interwoven with our lives , be it in everyday applications or in areas of consequential decision - making ( e . g . financial , medical or legal ) . Their effects thus do not remain limited to a particular interaction situation . It is important that we , as researchers and practitioners , reflect on the impact of the systems we design beyond the duration of direct use . This also includes reflections on when and how intelligent systems can learn compared to humans in the same roles [ 3 ] . Examples for work in such a larger context are presented in Table 2 , in particular in the critical and meta areas ( e . g . [ 8 , 19 , 25 , 27 ] ) . Connections with HCI in such work commonly refer to accountability and transparency of intelligent systems . 6 . 3 . 3 Motivating Connections Beyond HCI & Machine Learning / AI We see recent calls for more joint research at the intersection of HCI and AI to improve system understand - ing [ 1 ] . However , this is mostly motivated by utilitarian or interpretive mindsets . Thus , another related key takeaway is to draw attention to interdisciplinary connections via the critical of the three mindsets proposed in this article : As is evident from recent “AI and data scandals” ( e . g . [ 37 , 63 , 71 ] ) , developing more understandable ( and accountable ) intelligent systems also needs to be addressed in a wider view ( cf . third wave HCI [ 14 ] ) , for example across networks of human and AI actors [ 8 ] . More generally , fruitful connections could span considerations from fields like journalism [ 25 , 27 ] and communication [ 63 , 71 ] , policy [ 3 ] , sociology [ 8 ] and education [ 19 ] , and ethical and legal concerns [ 15 , 32 ] . 13 7 Conclusion Intelligent systems tend to violate UI principles , such as predictable output [ 5 , 29 ] , which makes them diffi - cult to understand and use . To address this , researchers , practitioners , policy - makers and the general public call for system qualities such as transparency , scrutability , explainability , interpretability , interactivity , and so on . However , these terms are often blurred and employed with varying interpretations . This impedes conceptual clarity of the very properties that are envisioned to foster users’ understanding of intelligent systems . This review responds to this lack of conceptual clarity with an analysis and discussion of theoretical concepts and prototype solutions from the literature : We make explicit the diversity of different implied views on user mindsets , user roles , and knowledge outcomes . In conclusion , we provide researchers and practitioners with a framework and thinking tool to 1 ) clearly motivate and frame their work , 2 ) draw connections across work on different system qualities and related design solutions , and 3 ) articulate explicitly their underlying assumptions and goals . With our work , we thus hope to facilitate , structure and advance further discussions and research on supporting users’ understanding of intelligent systems . References 1 . A . Abdul , J . Vermeulen , D . Wang , B . Y . Lim , and M . Kankanhalli . Trends and trajectories for explainable , accountable and intelligible systems : An hci research agenda . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 582 : 1 – 582 : 18 , New York , NY , USA , 2018 . ACM . 2 . O . Alkan , E . M . Daly , A . Botea , A . N . Valente , and P . Pedemonte . Where can my career take me ? : Harnessing dialogue for interactive career goal recommendations . In Proceedings of the 24th International Conference on Intelligent User Interfaces , IUI ’19 , pages 603 – 613 , New York , NY , USA , 2019 . ACM . 3 . A . Alkhatib and M . Bernstein . Street - level algorithms : A theory at the gaps between policy and decisions . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 530 : 1 – 530 : 13 , New York , NY , USA , 2019 . ACM . 4 . O . Alvarado and A . Waern . Towards algorithmic experience : Initial efforts for social media contexts . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 286 : 1 – 286 : 12 , New York , NY , USA , 2018 . ACM . 5 . S . Amershi , M . Cakmak , W . B . Knox , and T . Kulesza . Power to the People : The Role of Humans in Interactive Machine Learning . AI Magazine , 35 ( 4 ) : 105 , December 2014 . 6 . S . Amershi , M . Chickering , S . M . Drucker , B . Lee , P . Simard , and J . Suh . Modeltracker : Redesign - ing performance analysis tools for machine learning . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems , CHI ’15 , pages 337 – 346 , New York , NY , USA , 2015 . ACM . 7 . S . Amershi , D . Weld , M . Vorvoreanu , A . Fourney , B . Nushi , P . Collisson , J . Suh , S . Iqbal , P . N . Bennett , K . Inkpen , J . Teevan , R . Kikin - Gil , and E . Horvitz . Guidelines for human - ai interaction . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 3 : 1 – 3 : 13 , New York , NY , USA , 2019 . ACM . 8 . M . Ananny and K . Crawford . Seeing Without Knowing : Limitations of the Transparency Ideal and its Application to Algorithmic Accountability . New Media & Society , 20 ( 3 ) : 973 – 989 , March 2018 . 14 9 . Association for Computing Machinery US Public Policy Council ( USACM ) . Statement on Algo - rithmic Transparency and Accountability , 2017 . 10 . K . Balog , F . Radlinski , and S . Arakelyan . Transparent , scrutable and explainable user models for personalized recommendation . In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’19 , pages 265 – 274 , New York , NY , USA , 2019 . Association for Computing Machinery . 11 . D . Billsus and M . J . Pazzani . A personal news agent that talks , learns and explains . In Proceedings of the Third Annual Conference on Autonomous Agents , AGENTS ’99 , pages 268 – 275 , New York , NY , USA , 1999 . ACM . 12 . R . Binns , M . Van Kleek , M . Veale , U . Lyngs , J . Zhao , and N . Shadbolt . “it’s reducing a human being to a percentage” : Perceptions of justice in algorithmic decisions . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 377 : 1 – 377 : 14 , New York , NY , USA , 2018 . ACM . 13 . R . Blanco , D . Ceccarelli , C . Lucchese , R . Perego , and F . Silvestri . You should read this ! let me explain you why : Explaining news recommendations to users . In Proceedings of the 21st ACM International Conference on Information and Knowledge Management , CIKM ’12 , pages 1995 – 1999 , New York , NY , USA , 2012 . ACM . 14 . S . Bødker . Third - wave hci , 10 years later – participation and sharing . interactions , 22 ( 5 ) : 24 – 31 , Aug . 2015 . 15 . M . Brkan . Ai - supported decision - making under the general data protection regulation . In Proceed - ings of the 16th Edition of the International Conference on Articial Intelligence and Law , ICAIL ’17 , pages 3 – 8 , New York , NY , USA , 2017 . ACM . 16 . A . Brown , A . Chouldechova , E . Putnam - Hornstein , A . Tobin , and R . Vaithianathan . Toward algo - rithmic accountability in public services : A qualitative study of affected community perspectives on algorithmic decision - making in child welfare services . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 41 : 1 – 41 : 12 , New York , NY , USA , 2019 . ACM . 17 . T . Bucher . The Algorithmic Imaginary : Exploring the Ordinary Affects of Facebook Algorithms . Information Communication and Society , 20 ( 1 ) : 30 – 44 , 2017 . 18 . A . Bunt , M . Lount , and C . Lauzon . Are explanations always important ? : A study of deployed , low - cost intelligent interactive systems . In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces , IUI ’12 , pages 169 – 178 , New York , NY , USA , 2012 . ACM . 19 . J . Burrell . How the machine “thinks” : Understanding opacity in machine learning algorithms . Big Data & Society , 3 ( 1 ) : 2053951715622512 , 2016 . 20 . A . Chander , R . Srinivasan , S . Chelian , J . Wang , and K . Uchino . Working with Beliefs : AI Transparency in the Enterprise . In Explainable Smart Systems Workshop at IUI 2018 , 2018 . 21 . B . Chinh , H . Zade , A . Ganji , and C . Aragon . Ways of qualitative coding : A case study of four strategies for resolving disagreements . In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems , CHI EA ’19 , pages LBW0241 : 1 – LBW0241 : 6 , New York , NY , USA , 2019 . ACM . 15 22 . S . Coppers , K . Luyten , D . Vanacken , D . Navarre , P . Palanque , and C . Gris . Fortunettes : Feedforward about the future state of gui widgets . Proceedings of the ACM on Human - Computer Interaction , 3 ( EICS ) : 20 : 1 – 20 : 20 , June 2019 . 23 . F . Corno , L . De Russis , and A . Monge Roffarello . Empowering end users in debugging trigger - action rules . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 388 : 1 – 388 : 13 , New York , NY , USA , 2019 . ACM . 24 . L . De Russis and A . Monge Roffarello . A debugging approach for trigger - action programming . In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems , CHI EA ’18 , pages LBW105 : 1 – LBW105 : 6 , New York , NY , USA , 2018 . ACM . 25 . N . Diakopoulos . Algorithmic accountability . Digital Journalism , 3 ( 3 ) : 398 – 415 , 2015 . 26 . N . Diakopoulos . Accountability in algorithmic decision making . Communications of the ACM , 59 ( 2 ) : 56 – 62 , January 2016 . 27 . N . Diakopoulos . Enabling Accountability of Algorithmic Media : Transparency as a Constructive and Critical Lens , pages 25 – 43 . Springer International Publishing , Cham , 2017 . 28 . F . Doshi - Velez and B . Kim . Towards A Rigorous Science of Interpretable Machine Learning . February 2017 . 29 . J . J . Dudley and P . O . Kristensson . A review of user interface design for interactive machine learning . ACM Transactions on Interactive Intelligent Systems , 8 ( 2 ) : 8 : 1 – 8 : 37 , June 2018 . 30 . L . Edwards and M . Veale . Slave to the Algorithm ? Why a “Right to an Explanation” Is Probably Not the Remedy You Are Looking For . 2017 . 31 . M . Eiband , H . Schneider , M . Bilandzic , J . Fazekas - Con , M . Haug , and H . Hussmann . Bringing Transparency Design into Practice . Proceedings of the 2018 Conference on Intelligent User Interfaces IUI ’18 , pages 211 – 223 , 2018 . 32 . M . Eiband , H . Schneider , and D . Buschek . Normative vs Pragmatic : Two Perspectives on the Design of Explanations in Intelligent Systems . In Explainable Smart Systems Workshop at IUI 2018 , 2018 . 33 . F . Gedikli , D . Jannach , and M . Ge . How Should I Explain ? A Comparison of Different Explanation Types for Recommender Systems . International Journal of Human Computer Studies , 2014 . 34 . L . H . Gilpin , D . Bau , B . Z . Yuan , A . Bajwa , M . Specter , and L . Kagal . Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning . 2018 . 35 . P . M . Gollwitzer . Goal Achievement : The Role of Intentions . 4 ( 1 ) : 141 – 185 , 1993 . 36 . B . Goodman and S . Flaxman . Eu regulations on algorithmic decision - making and a “right to explanation” . 38 , June 2016 . 37 . H . Grassegger and M . Krogerus . The data that turned the world upside down . https : / / motherboard . vice . com / en _ us / article / mg9vvn / how - our - likes - helped - trump - win , January 2017 . Accessed : 11 . 09 . 2018 . 38 . G . D . Hager , R . Bryant , E . Horvitz , M . J . Mataric , and V . G . Honavar . Advances in artificial intelligence require progress across all of computer science . CoRR , abs / 1707 . 04352 , 2017 . 16 39 . J . L . Herlocker , J . A . Konstan , and J . Riedl . Explaining collaborative filtering recommendations . In Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work , CSCW ’00 , pages 241 – 250 , New York , NY , USA , 2000 . ACM . 40 . A . R . Hevner , S . T . March , J . Park , and S . Ram . Design science in information systems research . MIS Q . , 28 ( 1 ) : 75 – 105 , Mar . 2004 . 41 . M . Hildebrandt . The New Imbroglio : Living with Machine Algorithms . The Art of Ethics in the Information Society . Mind you , pages 55 – 60 , 2016 . 42 . K . Holstein , J . Wortman Vaughan , H . Daumé , III , M . Dudik , and H . Wallach . Improving fairness in machine learning systems : What do industry practitioners need ? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 600 : 1 – 600 : 16 , New York , NY , USA , 2019 . ACM . 43 . K . Höök . Steps to Take Before Intelligent User Interfaces Become Real . Interacting with Computers , 12 ( 4 ) : 409 – 426 , 2000 . 44 . K . Hornbæk and A . Oulasvirta . What is interaction ? In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , CHI ’17 , pages 5040 – 5052 , New York , NY , USA , 2017 . ACM . 45 . E . Horvitz . Principles of Mixed - initiative User Interfaces . In Proceedings of the 1999 CHI Conference on Human Factors in Computing Systems , CHI ’99 , pages 159 – 166 , New York , New York , USA , 1999 . ACM Press . 46 . T . Hussein and S . Neuhaus . Explanation of spreading activation based recommendations . In Proceedings of the 1st International Workshop on Semantic Models for Adaptive Interactive Systems , SEMAIS ’10 , pages 24 – 28 , New York , NY , USA , 2010 . ACM . 47 . D . Jackson and A . Fovargue . The use of animation to explain genetic algorithms . In Proceedings of the Twenty - eighth SIGCSE Technical Symposium on Computer Science Education , SIGCSE ’97 , pages 243 – 247 , New York , NY , USA , 1997 . ACM . 48 . S . Jhaver , Y . Karpfen , and J . Antin . Algorithmic Anxiety and Coping Strategies of Airbnb Hosts . Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , pages 1 – 12 , 2018 . 49 . H . Johnson and P . Johnson . Explanation facilities and interactive systems . In Proceedings of the 1st International Conference on Intelligent User Interfaces , IUI ’93 , pages 159 – 166 , New York , NY , USA , 1993 . ACM . 50 . P . N . Johnson - Laird . Mental Models . In Foundations of Cognitive Science . The MIT Press , Cambridge , MA , US , 1989 . 51 . A . Kapoor , B . Lee , D . Tan , and E . Horvitz . Interactive optimization for steering machine classifica - tion . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’10 , pages 1343 – 1352 , New York , NY , USA , 2010 . ACM . 52 . J . Kay and B . Kummerfeld . Creating personalized systems that people can scrutinize and control : Drivers , principles and experience . ACM Transactions on Interactive Intelligent Systems , 2 ( 4 ) : 24 : 1 – 24 : 42 , January 2013 . 53 . B . Kim . Interactive and Interpretable Machine Learning Models for Human Machine Collaboration . PhD thesis , Massachusetts Institute of Technology ( MIT ) , 2015 . 17 54 . R . F . Kizilcec . How much information ? : Effects of transparency on trust in an algorithmic interface . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems , CHI ’16 , pages 2390 – 2395 , New York , NY , USA , 2016 . ACM . 55 . R . Kocielnik , S . Amershi , and P . N . Bennett . Will you accept an imperfect ai ? : Exploring designs for adjusting end - user expectations of ai systems . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , CHI ’19 , pages 411 : 1 – 411 : 14 , New York , NY , USA , 2019 . ACM . 56 . C . Kuang . Can a . i . be taught to explain itself ? https : / / www . nytimes . com / 2017 / 11 / 21 / magazine / can - ai - be - taught - to - explain - itself . html / , 2017 . Accessed : 20 . 09 . 2018 . 57 . T . Kulesza , M . Burnett , W . - K . Wong , and S . Stumpf . Principles of explanatory debugging to personalize interactive machine learning . In Proceedings of the 20th International Conference on Intelligent User Interfaces , IUI ’15 , pages 126 – 137 , New York , NY , USA , 2015 . ACM . 58 . T . Kulesza , S . Stumpf , M . Burnett , and I . Kwan . Tell me more ? The Effects of Mental Model Soundness on Personalizing an Intelligent Agent . page 1 , 2012 . 59 . T . Kulesza , S . Stumpf , M . Burnett , W . - K . Wong , Y . Riche , T . Moore , I . Oberst , A . Shinsel , and K . McIntosh . Explanatory debugging : Supporting end - user debugging of machine - learned programs . In 2010 IEEE Symposium on Visual Languages and Human - Centric Computing , pages 41 – 48 , Sept 2010 . 60 . T . Kulesza , S . Stumpf , M . Burnett , S . Yang , I . Kwan , and W . - K . Wong . Too much , too little , or just right ? ways explanations impact end users’ mental models . In 2013 IEEE Symposium on Visual Languages and Human Centric Computing , pages 3 – 10 , September 2013 . 61 . T . Kulesza , S . Stumpf , W . - K . Wong , M . M . Burnett , S . Perona , A . Ko , and I . Oberst . Why - oriented end - user debugging of naive bayes text classification . ACM Transactions on Interactive Intelligent Systems , 1 ( 1 ) : 2 : 1 – 2 : 31 , October 2011 . 62 . T . Kulesza , W . - K . Wong , S . Stumpf , S . Perona , R . White , M . M . Burnett , I . Oberst , and A . J . Ko . Fixing the Program my Computer Learned . In Proceedings of the 13th International Conference on Intelligent User Interfaces , IUI ’09 , pages 187 – 196 , 2008 . 63 . D . M . J . Lazer , M . A . Baum , Y . Benkler , A . J . Berinsky , K . M . Greenhill , F . Menczer , M . J . Metzger , B . Nyhan , G . Pennycook , D . Rothschild , M . Schudson , S . A . Sloman , C . R . Sunstein , E . A . Thorson , D . J . Watts , and J . L . Zittrain . The science of fake news . Science , 359 ( 6380 ) : 1094 – 1096 , 2018 . 64 . B . Y . Lim . Improving trust in context - aware applications with intelligibility . In Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing , UbiComp ’10 Adjunct , pages 477 – 480 , New York , NY , USA , 2010 . ACM . 65 . B . Y . Lim and A . K . Dey . Assessing Demand for Intelligibility in Context - aware Applications . In Proceedings of the 11th International Conference on Ubiquitous Computing , UbiComp ’09 , pages 195 – 204 , 2009 . 66 . B . Y . Lim and A . K . Dey . Design of an Intelligible Mobile Context - aware Application . In Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services , MobileHCI ’11 , pages 157 – 166 , 2011 . 67 . B . Y . Lim , A . K . Dey , and D . Avrahami . Why and Why Not Explanations Improve the Intelligibility of Context - aware Intelligent Systems . CHI ’09 , pages 2119 – 2128 , 2009 . 18 68 . Z . C . Lipton . The mythos of model interpretability . Queue , 16 ( 3 ) : 30 : 31 – 30 : 57 , June 2018 . 69 . S . A . Lynham . The general method of theory - building research in applied disciplines . Advances in Developing Human Resources , 4 ( 3 ) : 221 – 241 , 2002 . 70 . B . D . Mittelstadt , P . Allo , M . Taddeo , S . Wachter , and L . Floridi . The Ethics of Algorithms : Mapping the Debate . Big Data & Society , 3 ( 2 ) : 205395171667967 , 2016 . 71 . G . Neff and P . Nagy . Automation , algorithms , and politics | talking to bots : Symbiotic agency and the case of tay . International Journal of Communication , 10 , 2016 . 72 . D . A . Norman . The Design of Everyday Things , 2013 . 73 . I . Nunes and D . Jannach . A systematic review and taxonomy of explanations in decision support and recommender systems . User Modeling and User - Adapted Interaction , 27 ( 3 ) : 393 – 444 , December 2017 . 74 . C . Oh , T . Lee , Y . Kim , S . Park , S . b . Kwon , and B . Suh . Us vs . them : Understanding artificial intelligence technophobia over the google deepmind challenge match . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , CHI ’17 , pages 2523 – 2534 , New York , NY , USA , 2017 . ACM . 75 . T . E . Parliament and the Council of the European Union . General data protection regula - tion . https : / / eur - lex . europa . eu / legal - content / EN / TXT / ? qid = 1537217711675 & uri = CELEX : 32016R0679 / , Apr . 2016 . Accessed : 17 . 09 . 2018 . 76 . P . Pu and L . Chen . Trust Building with Explanation Interfaces . In Proceedings of the 11th International Conference on Intelligent User Interfaces , IUI ’06 , pages 93 – 100 , 2006 . 77 . E . Rader , K . Cotter , and J . Cho . Explanations as mechanisms for supporting algorithmic trans - parency . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 103 : 1 – 103 : 13 , New York , NY , USA , 2018 . ACM . 78 . A . Rana and D . Bridge . Explanations that are intrinsic to recommendations . In Proceedings of the 26th Conference on User Modeling , Adaptation and Personalization , UMAP ’18 , pages 187 – 195 , New York , NY , USA , 2018 . ACM . 79 . M . T . Ribeiro , S . Singh , and C . Guestrin . “why should i trust you ? ” : Explaining the predictions of any classifier . In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’16 , pages 1135 – 1144 , New York , NY , USA , 2016 . ACM . 80 . A . Sarkar . Confidence , Command , Complexity : Metamodels for Structured Interaction with Ma - chine Intelligence . In Proceedings of the 26th Annual Conference of the Psychology of Programming Interest Group , pages 23 – 36 , 2015 . 81 . A . Sarkar . Constructivist design for interactive machine learning . In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems , CHI EA ’16 , pages 1467 – 1475 , New York , NY , USA , 2016 . ACM . 82 . A . Schlesinger , K . P . O’Hara , and A . S . Taylor . Let’s talk about race : Identity , chatbots , and ai . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 315 : 1 – 315 : 14 , New York , NY , USA , 2018 . ACM . 83 . W . Schoppek . Examples , Rules , and Strategies in the Control of Dynamic Systems . Cognitive Science Quarterly , 2 ( 1 ) : 63 – 92 , 2002 . 19 84 . J . Siljee . Privacy transparency patterns . In Proceedings of the 20th European Conference on Pattern Languages of Programs , EuroPLoP ’15 , pages 52 : 1 – 52 : 11 , New York , NY , USA , 2015 . ACM . 85 . M . P . Singh . Multiagent Systems , pages 1 – 14 . Springer Berlin Heidelberg , Berlin , Heidelberg , 1994 . 86 . J . Talbot , B . Lee , A . Kapoor , and D . S . Tan . Ensemblematrix : Interactive visualization to support machine learning with multiple classifiers . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’09 , pages 1283 – 1292 , New York , NY , USA , 2009 . ACM . 87 . N . Tintarev and R . Kutlak . Demo : Making plans scrutable with argumentation and natural language generation . In Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces , IUI Companion ’14 , pages 29 – 32 , New York , NY , USA , 2014 . ACM . 88 . C . - H . Tsai and P . Brusilovsky . Evaluating visual explanations for similarity - based recommendations : User perception and performance . In Proceedings of the 27th ACM Conference on User Modeling , Adaptation and Personalization , UMAP ’19 , pages 22 – 30 , New York , NY , USA , 2019 . ACM . 89 . C . - H . Tsai and P . Brusilovsky . Explaining recommendations in an interactive hybrid social recom - mender . In Proceedings of the 24th International Conference on Intelligent User Interfaces , IUI ’19 , pages 391 – 396 , New York , NY , USA , 2019 . ACM . 90 . J . Tullio , A . K . Dey , J . Chalecki , and J . Fogarty . How it Works : A Field Study of Non - technical Users Interacting with an Intelligent System . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’07 , pages 31 – 40 , 2007 . 91 . M . Veale , M . Van Kleek , and R . Binns . Fairness and accountability design needs for algorithmic support in high - stakes public sector decision - making . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 440 : 1 – 440 : 14 , New York , NY , USA , 2018 . ACM . 92 . E . von Glasersfeld . Cognition , Construction of Knowledge , and Teaching . Synthese , 1989 . 93 . S . Wachter , B . Mittelstadt , and L . Floridi . Transparent , Explainable , and Accountable AI for Robotics . Science Robotics , 2 ( 6 ) , 2017 . 94 . S . Wachter , B . Mittelstadt , and L . Floridi . Why a Right to Explanation of Automated Decision - Making Does Not Exist in the General Data Protection Regulation . Ssrn , pages 1 – 47 , 2017 . 20