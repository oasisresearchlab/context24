Super - Acceleration with Cyclical Step - sizes Baptiste Goujaud CMAP , École Polytechnique Institut Polytechnique de Paris Damien Scieur Samsung SAIL Montreal Aymeric Dieuleveut CMAP , École Polytechnique Institut Polytechnique de Paris Adrien Taylor INRIA , École Normale Supérieure CNRS , PSL Research University , Paris Fabian Pedregosa Google Research Abstract We develop a convergence - rate analysis of mo - mentum with cyclical step - sizes . We show that under some assumption on the spectral gap of Hessians in machine learning , cyclical step - sizes are provably faster than constant step - sizes . More precisely , we develop a con - vergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds . We fur - ther propose a systematic approach to de - sign optimal ﬁrst order methods for quadratic minimization with a given spectral structure . Finally , we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our ﬁnd - ings through benchmarks on least squares and logistic regression problems . 1 Introduction One of the most iconic methods in ﬁrst order optimiza - tion is gradient descent with momentum , also known as the heavy ball method ( Polyak , 1964 ) . This method enjoys widespread popularity both in its original for - mulation and in a stochastic variant that replaces the gradient by a stochastic estimate , a method that is behind many of the recent breakthroughs in deep learn - ing ( Sutskever et al . , 2013 ) . A variant of the stochastic heavy ball where the step - sizes are chosen in cyclical order has recently come Proceedings of the 25 th International Conference on Artiﬁ - cial Intelligence and Statistics ( AISTATS ) 2022 , Valencia , Spain . PMLR : Volume 151 . Copyright 2022 by the au - thor ( s ) . Algorithm 1 : Cyclical heavy ball HB K ( h 0 , . . . , h K − 1 ; m ) Input : Initialization x 0 , momentum m ∈ ( 0 , 1 ) , step - sizes { h 0 , . . . , h K − 1 } x 1 = x 0 − h 0 1 + m ∇ f ( x 0 ) for t = 1 , 2 , . . . do x t + 1 = x t − h mod ( t , K ) ∇ f ( x t ) + m ( x t − x t − 1 ) end to the forefront of machine learning research , showing state - of - the art results on diﬀerent deep learning bench - marks ( Loshchilov and Hutter , 2017 ; Smith , 2017 ) . In - spired by this empirical success , we aim to study the convergence of the heavy ball algorithm where step - sizes h 0 , h 1 , . . . are not ﬁxed or decreasing but instead chosen in cyclical order , as in Algorithm 1 . The heavy ball method with constant step - sizes enjoys a mature theory , where it is known for example to achieve optimal black - box worst - case complexity of quadratic convex optimization ( Nemirovsky , 1992 ) . In stark contrast , little is known about the the convergence of the above variant with cyclical step - sizes . Our main motivating question is Do cyclical step - sizes improve convergence of heavy ball ? Our main contribution provides a positive answer to this question and , more importantly , quantiﬁes the speedup under diﬀerent assumptions . In particular , we show that for quadratic problems , whenever Hessian’s spectrum belongs to two or more disjoint intervals , the heavy ball method with cyclical step - sizes achieves a faster worst - case convergence rate . Recent works have shown that this assumption on the spectrum is quite natural and occurs in many machine learning a r X i v : 2106 . 09687v3 [ m a t h . O C ] 9 M a y 2022 Super - Acceleration with Cyclical Step - sizes problems , including deep neural networks ( Sagun et al . , 2017 ; Papyan , 2018 ; Ghorbani et al . , 2019 ; Papyan , 2019 ) . The concurrent work of Oymak ( 2021 ) analyzes gradient descent ( without momentum , see extended comparison in Appendix G ) under this assumption . More precisely , we list our main contributions below . • In sections 3 and 4 , we provide a tight convergence rate analysis of the cyclical heavy ball method ( Theorems 3 . 1 and 3 . 2 for two step - sizes , and Theo - rem 4 . 8 for the general case ) . This analysis highlights a regime under which this method achieves a faster worst - case rate than the accelerated rate of heavy ball , a phenomenon we refer to as super - acceleration . Theorem 5 . 1 extends the ( local ) convergence rate analysis results to non - quadratic objectives . • As a byproduct of the convergence - rate analysis , we obtain an explicit expression for the optimal parameters in in the case of cycles of length two ( Algorithm 2 ) and an implicit expression in terms of a system of K equations in the general case . • Section 6 presents numerical benchmarks illus - trating the improved convergence of the cyclical ap - proach on 4 problems involving quadratic and logistic losses on both synthetic and a handwritten digits recognition dataset . • Finally , we conclude in Section 7 with a discussion of this work’s limitations . 2 Notation and Problem Setting Throughout the paper ( except in Section 5 ) , we consider the problem of minimizing a quadratic function : min x ∈ R d f ( x ) , with f ∈ C Λ , ( OPT ) where C Λ is the class of quadratic functions with Hes - sian matrix H and whose Hessian spectrum Sp ( H ) is localized in Λ ⊆ [ µ , L ] ⊆ R > 0 : C Λ (cid:44) (cid:8) f ( x ) = ( x − x ∗ ) (cid:62) H 2 ( x − x ∗ ) + f ∗ , Sp ( H ) ⊆ Λ (cid:9) The condition Λ ⊆ [ µ , L ] implies all quadratic functions under consideration are L - smooth and µ - strongly con - vex . For this function class , we deﬁne κ , the ( inverse ) condition number , and ρ , the ratio between the center of Λ and its radius , as κ (cid:44) µ L , ρ (cid:44) L + µ L − µ = (cid:18) 1 + κ 1 − κ (cid:19) . ( 1 ) Finally , for a method solving ( OPT ) that generates a sequence of iterates { x t } , we deﬁne its worst - case rate r t and its asymptotic rate factor τ as r t (cid:44) sup x 0 ∈ R d , f ∈C Λ (cid:107) x t − x ∗ (cid:107) (cid:107) x 0 − x ∗ (cid:107) , 1 − τ (cid:44) lim sup t →∞ t √ r t . ( 2 ) 3 Super - acceleration with Cyclical Step - sizes In this section we develop one of our main contributions , a convergence rate analysis of the cyclical heavy ball method with cycles of length 2 . This analysis crucially depends on the location of the Hessian’s eigenvalues ; we assume that these are contained in a set Λ that is the union of 2 intervals of the same size Λ = [ µ 1 , L 1 ] ∪ [ µ 2 , L 2 ] , L 1 − µ 1 = L 2 − µ 2 . ( 3 ) By symmetry , this set is alternatively described by µ (cid:44) µ 1 , L (cid:44) L 2 and R (cid:44) µ 2 − L 1 L 2 − µ 1 , ( 4 ) where R is the relative length of the gap µ 2 − L 1 with respect to the diameter L 2 − µ 1 ( see Figure 1 ) . This parametrization is convenient since the relative gap plays a crucial role in our convergence analysis . Our results allow R = 0 , therefore recovering the classical setting of Hessian eigenvalues contained in an interval . Figure 1 : Hessian eigenvalue histogram for a quadratic objective on MNIST . The outlier eigenvalue at L 2 generates a non - zero relative gap R = 0 . 77 . In this case , the 2 - cycle heavy ball method has a faster asymptotic rate than the single - cycle one ( see Section 3 . 2 ) . Through a correspondence between optimization meth - ods and polynomials ( see Section 4 ) , we can derive a worst - case analysis for the cyclical heavy ball method . The outcome of this analysis is in the following the - orem , that provides the asymptotic convergence rate of Algorithm 1 for cycles of length two . All proofs of results in this section can be found in Appendix D . 3 . Theorem 3 . 1 ( Rate factor of HB 2 ( h 0 , h 1 ; m ) ) . Let f ∈ C Λ and consider the cyclical heavy ball method with 2 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Figure 2 : Asymptotic rate of cyclical ( K = 2 ) heavy ball in terms of its step - sizes h 0 , h 1 across 3 diﬀerent values of the relative gap R . In the left plot , the relative gap is zero , and so the step - sizes with smallest rate coincide ( h 0 = h 1 ) . For non - zero values of R ( center and right ) , the optimal method instead alternates between two diﬀerent step - sizes . In all plots the momentum parameter m is set according to Algorithm 2 . step - sizes h 0 , h 1 and momentum parameter m . The asymptotic rate factor of Algorithm 1 with cycles of length two is 1 − τ =   √ m if σ ∗ ≤ 1 , √ m (cid:16) σ ∗ + (cid:112) σ 2 ∗ − 1 (cid:17) 12 if σ ∗ ∈ (cid:16) 1 , 1 + m 2 2 m (cid:17) , ≥ 1 ( no convergence ) if σ ∗ ≥ 1 + m 2 2 m , with σ ∗ = max λ ∈ (cid:110) µ 1 , L 1 , µ 2 , L 2 , ( 1 + m ) h 0 + h 1 2 h 0 h 1 (cid:111) ∩ Λ | σ 2 ( λ ) | and σ 2 ( λ ) = 2 (cid:16) 1 + m − λh 0 2 √ m (cid:17) (cid:16) 1 + m − λh 1 2 √ m (cid:17) − 1 . 3 . 1 Optimal algorithm The previous theorem gives the convergence rate for all triplets ( h 0 , h 1 , m ) . This allows us for instance to map out the associated convergence rate for every pair of step - sizes . As we illustrate in Figure 2 , as we increase the relative gap ( R ) , the optimal step - sizes become further apart . Another application of the previous theorem is to ﬁnd the parameters that minimize the asymptotic conver - gence rate . Although the process rather tedious and relegated to Appendix D . 3 , the resulting momentum ( m ) and step - size parameters ( h 0 , h 1 ) are remarkably simple , and given by the expressions m = (cid:32)(cid:112) ρ 2 − R 2 − (cid:112) ρ 2 − 1 √ 1 − R 2 (cid:33) 2 ( 5 ) h 0 = 1 + m L 1 h 1 = 1 + m µ 2 . ( 6 ) Being one of our main contributions , this algorithm is also described in pseudocode in Algorithm 2 . By con - struction , this method has an asymptotically optimal convergence rate which we detail in the next Corollary : Algorithm 2 : Cyclical ( K = 2 ) heavy ball with optimal parameters Input : Initial iterate x 0 , µ 1 < L 1 ≤ µ 2 < L 2 ( where L 1 − µ 1 = L 2 − µ 2 ) Set : ρ = L 2 + µ 1 L 2 − µ 1 , R = µ 2 − L 1 L 2 − µ 1 , m = (cid:18) √ ρ 2 − R 2 − √ ρ 2 − 1 √ 1 − R 2 (cid:19) 2 x 1 = x 0 − 1 L 1 ∇ f ( x 0 ) for t = 1 , 2 , . . . do h t = 1 + m L 1 ( if t is even ) , h t = 1 + m µ 2 ( if t is odd ) x t + 1 = x t − h t ∇ f ( x t ) + m ( x t − x t − 1 ) end Corollary 3 . 2 . The non - asymptotic and asymptotic worst - case rates r Alg . 2 t and 1 − τ Alg . 2 of Algorithm 2 over C Λ for even iteration number t are r Alg . 2 t = (cid:16) √ ρ 2 − R 2 − √ ρ 2 − 1 √ 1 − R 2 (cid:17) t (cid:16) 1 + t (cid:113) ρ 2 − 1 ρ 2 − R 2 (cid:17) , 1 − τ Alg . 2 = (cid:112) ρ 2 − R 2 − (cid:112) ρ 2 − 1 √ 1 − R 2 . Note that this result also holds if we swap the 2 step - sizes in Algorithm 2 . Eigengap and accelerated cyclical step - sizes While Corollary 3 . 2 focuses on the optimal tuning of Al - gorithm 2 , Theorem D . 1 provides general convergence analysis for non - optimal parameters . In the case of existence of an eigengap , a range of cyclical step - sizes leads to an accelerated rate of convergence ( compared to the optimal constant step - size strategy ) and there - fore , an inexact parameters search can lead to such an acceleration . 3 Super - Acceleration with Cyclical Step - sizes 3 . 2 Comparison with Polyak Heavy Ball In the absence of eigenvalue gap ( R = 0 and Λ = [ µ , L ] ) , Algorithm 2 reduces to Polyak heavy ball ( PHB ) ( Polyak , 1964 ) , whose worst - case rate is detailed in Appendix B . Since the asymptotic rate of Algorithm 2 is monotonically decreasing in R , the convergence rate of the cyclical variant is always better than PHB . Furthermore , in the ill - conditioned regime ( small κ ) , the comparison is particularly simple : the optimal 2 - cycle algorithm has a √ 1 − R 2 relative improvement over PHB , as provided by the next proposition . A more thorough comparison for diﬀerent support sets Λ is discussed in Table 1 . Proposition 3 . 3 . Let R ∈ [ 0 , 1 ) . The rate factors of respectively Algorithm 2 and PHB verify 1 − τ Alg . 2 = κ → 0 1 − 2 √ κ √ 1 − R 2 + o ( √ κ ) , ( 7 ) 1 − τ PHB = κ → 0 1 − 2 √ κ + o ( √ κ ) . 4 A constructive Approach : Minimax Polynomials This section presents a generic framework that allows designing optimal momentum and step - size cycles for given sets Λ and cycle length K . We ﬁrst recall classical results that link optimal ﬁrst order methods on quadratics and Chebyshev polyno - mials . Then , we generalize the approach by showing that optimal methods can be viewed as combinations of Chebyshev polynomials , and minimax polynomials σ Λ K of degree K over the set Λ . Finally , we show how to recover the step - size schedule from σ Λ K and present the general algorithm ( Algorithm 3 ) . 4 . 1 First Order Methods on Quadratics and Polynomials A key property that we will use extensively in the anal - ysis is the following link between ﬁrst order methods and polynomials . Proposition 4 . 1 . Let f ∈ C Λ . The iterates x t satisfy x t + 1 ∈ x 0 + span { ∇ f ( x 0 ) , . . . , ∇ f ( x t ) } , ( 8 ) where x 0 is the initial approximation of x ∗ , if and only if there exists a sequence of polynomials ( P t ) t ∈ N , each of degree at most 1 more than the highest degree of all previous polynomials and P 0 of degree 0 ( hence the degree of P t is at most t ) , such that ∀ t x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) , P t ( 0 ) = 1 . ( 9 ) Example 4 . 2 ( Gradient descent ) . Consider the gra - dient descent algorithm with ﬁxed step - size h , applied to problem ( OPT ) . Then , after unrolling the update , we have x t + 1 − x ∗ = x t − x ∗ − h ∇ f ( x t ) = x t − x ∗ − hH ( x t − x ∗ ) = ( I − hH ) t + 1 ( x 0 − x ∗ ) . ( 10 ) In this case , the polynomial associated to gradient descent is P t ( λ ) = ( 1 − hλ ) t . The above proposition can be used to obtain worst - case rates for ﬁrst order methods by bounding their associated polynomials . Indeed , using the Cauchy - Schwartz inequality in ( 9 ) leads to (cid:107) x t − x ∗ (cid:107) ≤ sup λ ∈ Λ | P t ( λ ) | (cid:107) x 0 − x ∗ (cid:107) = ⇒ r t = sup λ ∈ Λ | P t ( λ ) | , where P t ( 0 ) = 1 . ( 11 ) Therefore , ﬁnding the algorithm with the fastest worst - case rate can be equivalently framed as the problem of ﬁnding the polynomial with smallest value on the eigenvalue support Λ , subject to the normalization condition P t ( 0 ) = 1 . Such polynomials are referred to as minimax . Throughout the paper , we use this polynomial - based approach to ﬁnd methods with opti - mal rates . An important property of minimax polynomials is their equioscillation on Λ ( see Theorem C . 1 and its proof for a formal statement ) . Deﬁnition 4 . 3 . ( Equioscillation ) A polynomial P t of degree t equioscillates on Λ if it veriﬁes P t ( 0 ) = 1 and there exist λ 0 < λ 1 < . . . < λ t ∈ Λ such that P t ( λ i ) = ( − 1 ) i max λ ∈ Λ | P t ( Λ ) | . ( 12 ) Example 4 . 4 ( Λ is an interval ) . The t - th order Chebyshev polynomials of the ﬁrst kind T t satisfy the equioscillation property on [ − 1 , 1 ] . It follows that min - imax polynomials on Λ = [ µ , L ] can be obtained by composing the Chebyshev polynomial T t with the linear transformation σ Λ1 : T t (cid:0) σ Λ1 ( λ ) (cid:1) T t (cid:0) σ Λ1 ( 0 ) (cid:1) = arg min P ∈ R t [ X ] , P ( 0 ) = 1 sup λ ∈ Λ | P ( λ ) | , ( 13 ) with σ Λ1 ( λ ) = L + µ L − µ − 2 L − µλ , where σ Λ1 maps the interval [ µ , L ] to [ − 1 , 1 ] . The opti - mization method associated with this minimax polyno - mial is the Chebyshev semi - terative method ( Flanders and Shortley , 1950 ; Golub and Varga , 1961 ) , described 4 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Relative gap R Set Λ Rate factor τ Speedup τ / τ PHB R ∈ [ 0 , 1 ) [ µ , µ + 1 − R 2 ( L − µ ) ] ∪ [ L − 1 − R 2 ( L − µ ) , L ] 2 √ κ √ 1 − R 2 ( 1 − R 2 ) − 12 R = 1 − √ κ / 2 [ µ , µ + √ µL 4 ] ∪ [ L − √ µL 4 , L ] 2 4 √ κ κ − 14 R = 1 − 2 γκ [ µ , ( 1 + γ ) µ ] ∪ [ L − γµ , L ] indep . of κ O ( κ − 12 ) Table 1 : Case study of the convergence of Algorithm 2 as a function of R , in the regime κ → 0 . The ﬁrst line corresponds to the regime where R is independent of κ , and we observe a constant gain w . r . t . PHB . The second line considers a setting in which R depends on √ κ , that is , the two intervals in Λ are relatively small . The asymptotic rate reads ( 1 − 2 4 √ κ ) t , improving over the ( 1 − 2 √ κ ) t rate of Polyak Heavy ball , unimprovable when R = 0 . Finally , in the third line , R depends on κ , the two intervals in Λ are so small that the convergence becomes O ( 1 ) , i . e . , is independent of κ . also in Appendix B . 1 . This method achieves the lower complexity bound for smooth strongly convex quadratic minimization ( Nemirovsky , 1995 , Chapter 12 ) or ( Ne - mirovsky , 1992 ; Nesterov , 2003 ) . The next proposition provides the main results in this subsection , which is key for obtaining Algorithm 2 . It characterizes the even degree minimax polynomial in the setting of Section 3 , that is , when Λ is the union of 2 intervals of same size . In this case , the minimax solution is also based on Chebyshev polynomials , but composed with a degree - two polynomial σ Λ2 . Proposition 4 . 5 . Let Λ = [ µ 1 , L 1 ] ∪ [ µ 2 , L 2 ] be an union of two intervals of the same size ( L 1 − µ 1 = L 2 − µ 2 ) and let m , h 0 , h 1 be as deﬁned in Algorithm 2 . Then the minimax polynomial ( solution to ( 12 ) ) is , for all t = 2 n , n ∈ N + 0 , T n (cid:0) σ Λ2 ( λ ) (cid:1) T n (cid:0) σ Λ2 ( 0 ) (cid:1) = arg min P ∈ R t [ X ] , P ( 0 ) = 1 sup λ ∈ Λ | P ( λ ) | , with σ Λ2 ( λ ) = 1 2 m ( 1 + m − λh 0 ) ( 1 + m − λh 1 ) − 1 . 4 . 2 Generalization to Longer Cycles The polynomial in Example 4 . 4 uses a linear link func - tion σ Λ1 to map Λ to [ − 1 , 1 ] . In Proposition 4 . 5 , we see that a degree two link function σ Λ2 can be used to ﬁnd the minimax polynomial when Λ is the union of two intervals . This section generalizes this approach and considers higher - order polynomials for σ K . We start with the following parametrization , with an arbitrary polynomial σ K of degree K , P t ( λ ; σ K ) (cid:44) T n ( σ K ( λ ) ) T n ( σ K ( 0 ) ) , ∀ t = Kn , n ∈ N + 0 . ( 14 ) As we will see in the next subsection , this parametriza - tion allows considering cycles of step - sizes . Our goal now is to ﬁnd the σ K that obtains the fastest conver - gence rate possible . The next proposition quantiﬁes its impact on the asymptotic rate and its proof can be found in Appendix D . 1 . Proposition 4 . 6 . For a given σ K such that sup λ ∈ Λ | σ K ( λ ) | = 1 , the asymptotic rate factor τ σ K of the method associated to the polynomial ( 14 ) is 1 − τ σ K = lim t →∞ t (cid:114) sup λ ∈ Λ | P t ( λ ; σ K ) | = (cid:16) σ 0 − (cid:112) σ 20 − 1 (cid:17) 1 K , with σ 0 (cid:44) σ K ( 0 ) . ( 15 ) For a ﬁxed K , the asymptotic rate ( 15 ) is a decreasing function of σ 0 . This motivates the introduction of the “optimal” degree K polynomial σ Λ K as the one that solves σ Λ K (cid:44) arg max σ ∈ R K [ X ] σ ( 0 ) s . t . sup λ ∈ Λ | σ ( λ ) | ≤ 1 . ( 16 ) Using the above deﬁnition , we recover the σ Λ1 and σ Λ2 from Example 4 . 4 and Proposition 4 . 5 . Finding the polynomial . Finding an exact and ex - plicit solution for the general K and Λ case is unfortu - nately out of reach , as it involves solving a system of K non - linear equations . Here we describe an approximate approach . Let σ Λ K ( x ) = (cid:80) Ki = 0 σ i x i . We propose to discretize Λ into N diﬀerent points { λ j } , then solve the linear problem max σ i σ 0 s . t . − 1 ≤ (cid:80) Ki = 0 σ i λ ij ≤ 1 , ∀ j = 1 , . . . , N . ( 17 ) To check the optimality , it suﬃces to verify that the polynomial σ Λ K satisﬁes the equioscillation property ( Deﬁnition 4 . 3 ) , as depicted in Figure 3 . 5 Super - Acceleration with Cyclical Step - sizes Remark 4 . 7 ( Relationship between optimal and mini - max polynomials ) . For later reference , we note that the optimal polynomial σ Λ K is equivalent to ﬁnding a mini - max polynomial on Λ and to rescale it . More precisely , σ Λ K is optimal if and only if σ Λ K / σ Λ K ( 0 ) is minimax . 4 . 3 Cyclical Heavy Ball and ( Non - ) asymptotic Rates of Convergence We now describe the link between σ Λ K and Algorithm 3 . Using the recurrence for Chebyshev polynomials of the ﬁrst kind in ( 14 ) , we have ∀ t = Kn , n ∈ N + 0 , T n + 1 ( σ Λ K ( λ ) ) T n + 1 ( σ Λ K ( 0 ) ) = 2 σ Λ K ( λ ) (cid:20) T n ( σ Λ K ( λ ) ) T n ( σ Λ K ( 0 ) ) (cid:21) (cid:20) T n ( σ Λ K ( 0 ) ) T n + 1 ( σ Λ K ( 0 ) ) (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) = a n − (cid:20) T n − 1 ( σ Λ K ( λ ) ) T n − 1 ( σ Λ K ( 0 ) ) (cid:21) (cid:20) T n − 1 ( σ Λ K ( 0 ) ) T n + 1 ( σ Λ K ( 0 ) ) (cid:21) (cid:124) (cid:123)(cid:122) (cid:125) = b n . It still remains to ﬁnd an algorithm associated with this polynomial . To obtain one in the form of Algorithm 1 , one can use the stationary behavior of the recurrence . From ( Scieur and Pedregosa , 2020 ) , the coeﬃcients a n and b n converge as n → ∞ to their ﬁxed - points a ∞ and b ∞ . We therefore consider here an asymptotic polynomial ¯ P t ( λ ; σ Λ K ) , whose recurrence satisﬁes ¯ P t ( λ ; σ Λ K ) = 2 a ∞ σ Λ K ( λ ) ¯ P t − K ( λ ; σ Λ K ) − b ∞ ¯ P t − 2 K ( λ ; σ Λ K ) . ( 18 ) Similarly to K = 1 , where this limit recursion cor - responds to PHB , this recursion corresponds to an instance of Algorithm 3 ( see Proposition 4 . 9 below ) , further motivating the cyclical heavy ball algorithm . The following theorem is the main result of this sec - tion and characterizes the convergence rate of Algo - rithm 1 for arbitrary momentum and step - size sequence { h i } i ∈ (cid:74) 1 , K (cid:75) . Theorem 4 . 8 . With an arbitrary momentum m and an arbitrary sequence of step - sizes { h i } , the worst - case rate of convergence 1 − τ of Algorithm 1 on C Λ is    √ m if σ ∗ ≤ 1 √ m (cid:16) σ ∗ + (cid:112) σ 2 ∗ − 1 (cid:17) K − 1 if σ ∗ ∈ (cid:32) 1 , 1 + m K 2 ( √ m ) K (cid:33) ≥ 1 ( no convergence ) if σ ∗ ≥ 1 + m K 2 ( √ m ) K , ( 19 ) where σ ∗ (cid:44) sup λ ∈ Λ | σ ( λ ; { h i } , m ) | , σ ( λ ; { h i } , m ) is the K - degree polynomial σ ( λ ; { h i } , m ) (cid:44) 1 2Tr ( M 1 M 2 . . . M K ) , ( 20 ) and M i = (cid:34) 1 + m − h K − i λ √ m − 1 1 0 (cid:35) . By optimizing over these parameters , we obtain the Algorithm 3 , a method associated to ( 18 ) , whose rate is described in Proposition 4 . 9 . All proofs can be found in Appendix D . 2 . Algorithm 3 : Cyclical ( arbitrary K ) heavy ball with optimal parameters Input : Eigenvalue localization Λ , cycle length K , initialization x 0 . Preprocessing : 1 . Find the polynomial σ Λ K such that it satisﬁes ( 16 ) . 2 . Set step - sizes { h i } i = 0 , . . . , K − 1 and momentum m that satisfy resp . equations ( 21 ) and ( 22 ) . Set x 1 = x 0 − h 0 1 + m ∇ f ( x 0 ) for t = 1 , 2 , . . . do x t + 1 = x t − h mod ( t , K ) ∇ f ( x t ) + m ( x t − x t − 1 ) end Proposition 4 . 9 . Let σ ( λ ; { h i } , m ) be the polynomial deﬁned by ( 20 ) , and σ Λ K be the optimal link function of degree K deﬁned by ( 16 ) . If the momentum m and the sequence of step - sizes { h i } satisfy σ ( λ ; { h i } , m ) = σ Λ K ( λ ) , ( 21 ) then 1 ) the parameters are optimal , in the sense that they minimize the asymptotic rate factor from Theorem 4 . 8 , 2 ) the optimal momentum parameter is m = (cid:0) σ 0 − (cid:112) σ 20 − 1 (cid:1) 2 / K , where σ 0 = σ Λ K ( 0 ) , ( 22 ) 3 ) the iterates from Algo . 3 with parameters { h i } and m form a polynomial with recurrence ( 18 ) , and 4 ) Algorithm 3 achieves the worst - case rate r Alg . 3 t and the asymptotic rate factor 1 − τ Alg . 3 r Alg . 3 t = O (cid:18) t (cid:16) σ 0 − (cid:112) σ 20 − 1 (cid:17) t / K (cid:19) , ( 23 ) 1 − τ Alg . 3 = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 / K . Solving the system ( 21 ) The system is constructed by identiﬁcation of the coeﬃcients in both polynomials σ Λ K and σ ( λ ; { h i } , m ) , which can be solved using a naive grid - search for instance . We are not aware of any eﬃcient algorithm to solve this system exactly , although it is possible to use iterative methods such as steepest descent or Newton’s method . 6 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Figure 3 : Examples of optimal polynomials σ Λ K from ( 16 ) , all of them verifying the equioscillation property ( Deﬁnition 4 . 3 ) . The “ (cid:63) ” symbol highlights the degree of σ Λ K that achieves the best asymptotic rate τ σ Λ K in ( 15 ) amongst all K ( see Section 4 . 4 ) . ( Left ) When Λ is an unique interval , all 3 polynomials are equivalently optimal τ σ Λ1 = τ σ Λ2 = τ σ Λ3 . ( Center ) When Λ is the union of two intervals of the same size , the degree 2 polynomial is optimal τ σ Λ2 > τ σ Λ3 > τ σ Λ1 . This is expected given the result in Proposition 4 . 5 . ( Right ) When Λ is the union of two unbalanced intervals , the degree 3 polynomial instead achieves the best asymptotic rate τ σ Λ3 > τ σ Λ2 > τ σ Λ1 ( see Section 4 . 4 ) . 4 . 4 Best Achievables Worst - case Guarantees on C Λ This section discusses the ( asymptotic ) optimality of Algorithm 3 . In Section 4 . 2 , the polynomial P t ( · ; σ Λ K ) was written as a composition of Chebyshev polynomi - als with σ Λ K , deﬁned in ( 16 ) . The best K is chosen as follows : we solve ( 16 ) for several values of K , then pick the smallest K among the minimizers of ( 15 ) . How - ever , following such steps does not guarantee that the polynomial P Λ t , K is minimax , as it is not guaranteed to minimize the worst - case rate sup λ ∈ Λ | P t ( λ ) | ( see ( 11 ) ) . We give here an optimality certiﬁcate , linked to a gener - alized version of equioscillation . In short , if we can ﬁnd K non overlapping intervals ( more formally , whose in - teriors are disjoint ) Λ i in Λ such that σ Λ K ( Λ i ) = [ − 1 , 1 ] then P Λ t , K is minimax for t = nK , n ∈ N + 0 . The precise result is in Theorem C . 2 . A direct consequence is the asymptotic optimality of Algorithm 3 . We note that σ Λ K might not exist for a given Λ . A complete characterization of the set Λ for which σ Λ K exists is out of the scope of this paper . A partial answer is given in ( Fischer , 2011 ) when Λ is the union of two intervals but the general case remains open . 5 Local Convergence for Non - Quadratic Functions When f is twice - diﬀerentiable , we show local conver - gence rates of Algorithm 1 ( see proof in Appendix E ) . As with Polyak heavy ball acceleration , these results are local , as the only known convergence results for Polyak heavy ball beyond quadratic objectives do not lead to an acceleration with respect to Gradient descent without momentum ( See Ghadimi et al . , 2015 , Theo - rem 4 ) . Moreover , it is possible to ﬁnd pathological counter - examples and a speciﬁc initialization for which the method does not converge globally . Lessard et al . ( 2016 , Figure 7 ) provides such a counter - example . Note the latest is not twice diﬀerentiable , but that a twice diﬀerentiable counter - example can be derived from the latest , using for instance convolutions . Theorem 5 . 1 ( Local convergence ) . Let f : R d (cid:55)→ R be a twice continuously diﬀerentiable function , x ∗ a local minimizer , and H be the Hessian of f at x ∗ with Sp ( H ) ⊆ Λ . Let x t denote the result of running Algorithm 1 with parameters h 1 , h 2 , · · · , h K , m , and let 1 − τ be the linear convergence rate on the quadratic objective ( OPT ) . Then we have ∀ ε > 0 , ∃ open set V ε : x 0 , x ∗ ∈ V ε = ⇒ (cid:107) x t − x ∗ (cid:107) = O ( ( 1 − τ + ε ) t ) (cid:107) x 0 − x ∗ (cid:107) . ( 24 ) where (cid:107) · (cid:107) denotes the Euclidean norm . In short , when Algorithm 1 is guaranteed to converge at rate 1 − τ on ( OPT ) , then the convergence rate on a nonlinear functions can be arbitrary close to 1 − τ when x 0 is suﬃciently close to x ∗ . 7 Super - Acceleration with Cyclical Step - sizes Figure 4 : Hessian Eigenvalue histogram ( top row ) and Benchmarks ( bottom row ) . The top row shows the Hessian eigenvalue histogram at optimum for the 4 considered problems , together with the interval boundaries µ 1 < L 1 < µ 2 < L 2 for the two - interval split of the eigenvalue support described in Section 3 . In all cases , there’s a non - zero gap radius R . This is shown in the bottom row , where we compare the suboptimality in terms of gradient norm as a function of the number of iterations . As predicted by the theory , the non - zero gap radius translates into a faster convergence of the cyclical approach , compared to PHB in all cases . The improvement is observed on both quadratic and logistic regression problems , even through the theory for the latter is limited to local convergence . 6 Experiments In this section we present an empirical comparison of the cyclical heavy ball method for diﬀerent length cycles across 4 diﬀerent problems . We consider two diﬀerent problems , quadratic and logistic regression , each applied on two datasets , the MNIST handwritten digits ( Le Cun et al . , 2010 ) and a synthetic dataset . The results of these experiments , together with a histogram of the Hessian’s eigenvalues are presented in Figure 4 ( see caption for a discussion ) . Dataset description . The MNIST dataset consists of a data matrix A with 60000 images of handwritten digits each one with 28 × 28 = 784 pixels . The synthetic dataset is generated according to a spiked covariance model ( Johnstone , 2001 ) , which has been shown to be an accurate model of covariance matrices arising for instance in spectral clustering ( Couillet and Benaych - Georges , 2016 ) and deep networks ( Pennington and Worah , 2017 ; Granziol et al . , 2020 ) . In this model , the data matrix A = XZ is generated from a m × n random Gaussian matrix X and an m × m deterministic matrix Z . In our case , we take n = 1000 , m = 1200 and Z is the identity where the ﬁrst three entries are multiplied by 100 ( this will lead to three outlier eigenvalues ) . We also generate an n - dimensional target vector b as b = Ax or b = sign ( Ax ) for the quadratic and logistic problem respectively . Objective function For each dataset , we consider a quadratic and a logistic regression problem , lead - ing to 4 diﬀerent problems . All problems are of the form min x ∈ R p 1 n (cid:80) ni = 1 (cid:96) ( A (cid:62) i x , b i ) + λ (cid:107) x (cid:107) 2 , where (cid:96) is a quadratic or logistic loss , A is the data matrix and b are the target values . We set the regularization parameter to λ = 10 − 3 (cid:107) A (cid:107) 2 . For logistic regression , since guaran - tees only hold at a neighborhood of the solution ( even for the 1 - cycle algorithm ) , we initialize the ﬁrst iterate as the result of 100 iteration of gradient descent . In the case of logistic regression , the Hessian eigenvalues are computed at the optimum . 7 Conclusion This work is motivated by two recent observations from the optimization practice of machine learning . First , cyclical step - sizes have been shown to enjoy excellent empirical convergence ( Loshchilov and Hutter , 2017 ; Smith , 2017 ) . Second , spectral gaps are pervasive in the Hessian spectrum of deep learning models ( Sagun et al . , 2017 ; Papyan , 2018 ; Ghorbani et al . , 2019 ; Papyan , 2019 ) . Based on the simpler context of quadratic convex minimization , we develop a convergence - rate analysis 8 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa and optimal parameters for the heavy ball method with cyclical step - sizes . This analysis highlights the regimes under which cyclical step - sizes have faster rates than classical accelerated methods . Finally , we illustrate these ﬁndings through numerical benchmarks . Main Limitations . In Section 3 we gave explicit formulas for the optimal parameters in the case of the 2 - cycle heavy ball algorithm . These formulas depend not only on extremal eigenvalues—as is usual for ac - celerated methods—but also on the spectral gap R . The gap can sometimes be estimated after computing the top eigenvalues ( e . g . top - 2 eigenvalue for MNIST ) . However , in general , there is no guarantee on how many eigenvalues are needed to estimate it and it must some - times be seen as hyperparameter . Note Theorem 3 . 1 provides a convergence analysis also for non - optimal parameters , which would give accelerated convergence rates when doing a coarse grid - search over parameters as it is often done in empirical works . Another limitation is the fact global convergence re - sults rely heavily on the quadratic assumption which is quite diﬀerent from our motivation , namely optimizing neural networks . Even if we provide local convergence guarantee in Section 5 , we are not able to estimate the size of the optimum neighborhood for which Theo - rem 5 . 1 holds . Another limitation regards long cycles . For cycles longer than 2 , we gave an implicit formula to set the optimal parameters ( Proposition 4 . 9 ) . This involves solving a set of non - linear equations whose complexity increases with the cycle length . That being said , cycli - cal step - sizes might signiﬁcantly enhance convergence speeds both in terms of worst - case rates and empiri - cally , and this work advocates that new tuning practices involving diﬀerent cycle lengths might be relevant . Acknowledgements We thank Courtney Paquette for very useful proofread - ing and feedback . The work of B . Goujaud and A . Dieuleveut is partially supported by ANR - 19 - CHIA - 0002 - 01 / chaire SCAI , and Hi ! Paris . A . Taylor ac - knowledges support from the European Research Coun - cil ( grant SEQUOIA 724063 ) . This work was partly funded by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program , reference ANR - 19 - P3IA - 0001 ( PRAIRIE 3IA Institute ) . References Naman Agarwal , Surbhi Goel , and Cyril Zhang . Accel - eration via Fractal Learning Rate Schedules . arXiv preprint arXiv : 2103 . 01338 , 2021 . Dimitri P . Bertsekas . Nonlinear programming . Journal of the Operational Research Society , 1997 . Pafnuty Lvovich Chebyshev . Théorie des mécanismes connus sous le nom de parallélogrammes . Imprimerie de l’Académie impériale des sciences , 1853 . Romain Couillet and Florent Benaych - Georges . Kernel spectral clustering of large dimensional data . Elec - tronic Journal of Statistics , 2016 . Bernd Fischer . Polynomial based iteration methods for symmetric linear systems . SIAM , 2011 . Donald A . Flanders and George Shortley . Numerical determination of fundamental modes . Journal of Applied Physics , 1950 . Euhanna Ghadimi , Hamid Reza Feyzmahdavian , and Mikael Johansson . Global convergence of the heavy - ball method for convex optimization . In 2015 Euro - pean control conference ( ECC ) , pages 310 – 315 . IEEE , 2015 . Behrooz Ghorbani , Shankar Krishnan , and Ying Xiao . An investigation into neural net optimization via hes - sian eigenvalue density . In International Conference on Machine Learning ( ICML ) , 2019 . Gabriel Goh . Why Momentum Really Works , 2017 . URL http : / / distill . pub / 2017 / momentum / . Gene H . Golub and Richard S . Varga . Chebyshev semi - iterative methods , successive overrelaxation itera - tive methods , and second order Richardson iterative methods . Numerische Mathematik , 1961 . Diego Granziol , Xingchen Wan , Samuel Albanie , and Stephen Roberts . Explaining the Adaptive Generali - sation Gap . arXiv preprint arXiv : 2011 . 08181 , 2020 . Iain M . Johnstone . On the distribution of the largest eigenvalue in principal components analysis . Annals of statistics , 2001 . Yann Le Cun , Corinna Cortes , and Chris Burges . MNIST handwritten digit database . ATT Labs [ On - line ] , 2010 . Laurent Lessard , Benjamin Recht , and Andrew Packard . Analysis and design of optimization al - gorithms via integral quadratic constraints . SIAM Journal on Optimization , 2016 . Ilya Loshchilov and Frank Hutter . SGDR : stochas - tic gradient descent with warm restarts . In Inter - national Conference on Learning Representations ( ICLR ) , 2017 . Arkadi S . Nemirovsky . Information - based complexity of linear operator equations . Journal of Complexity , 1992 . Arkadi S . Nemirovsky . Information - based complexity of convex programming . Lecture Notes , 1995 . 9 Super - Acceleration with Cyclical Step - sizes Yurii Nesterov . Introductory Lectures on Convex Opti - mization . Springer , 2003 . Samet Oymak . Super - convergence with an unstable learning rate . arXiv preprint arXiv : 2102 . 10734 , 2021 . Vardan Papyan . The full spectrum of deepnet hessians at scale : Dynamics with SGD training and sample size . arXiv preprint arXiv : 1811 . 07062 , 2018 . Vardan Papyan . Measurements of Three - Level Hierar - chical Structure in the Outliers in the Spectrum of Deepnet Hessians . In International Conference on Machine Learning ( ICML ) , 2019 . Fabian Pedregosa . On the Link Between Optimization and Polynomials , Part 1 , 2020 . URL http : / / fa . bianp . net / blog / 2020 / polyopt / . Fabian Pedregosa . On the Link Between Optimization and Polynomials , Part 3 , 2021 . URL http : / / fa . bianp . net / blog / 2021 / hitchhiker / . Jeﬀrey Pennington and Pratik Worah . Nonlinear ran - dom matrix theory for deep learning . In Advances on Neural Information Processing Systems ( NIPS ) , 2017 . Boris T . Polyak . Some methods of speeding up the con - vergence of iteration methods . USSR computational mathematics and mathematical physics , 1964 . Heinz Rutishauser . Theory of gradient methods . In Reﬁned iterative methods for computation of the so - lution and the eigenvalues of self - adjoint boundary value problems . Springer , 1959 . Levent Sagun , Utku Evci , V . Ugur Guney , Yann Dauphin , and Leon Bottou . Empirical analysis of the Hessian of over - parametrized neural networks . arXiv preprint arXiv : 1706 . 04454 , 2017 . Damien Scieur and Fabian Pedregosa . Univer - sal Asymptotic Optimality of Polyak Momentum . In International Conference on Machine Learning ( ICML ) , 2020 . Leslie N . Smith . Cyclical learning rates for training neural networks . In 2017 IEEE Winter Conference on Applications of Computer Vision ( WACV ) . IEEE , 2017 . Ilya Sutskever , James Martens , George Dahl , and Ge - oﬀrey Hinton . On the importance of initialization and momentum in deep learning . In International Conference on Machine Learning ( ICML ) , 2013 . David Young . On richardson’s method for solving linear systems with positive deﬁnite matrices . Journal of Mathematics and Physics , 1953 . URL https : / / doi . org / 10 . 1002 / sapm1953321243 . 10 Super - Acceleration with Cyclical Step - sizes : Supplementary Materials Organization of the appendix The appendix contains all proofs that were not presented in the main core of the paper . We also detail all examples , and provide some complementary elements . Appendix A details the existing link between ﬁrst order methods and family of “residual polynomials” . This term refers in all the appendix to the polynomials which value in 0 is 1 . In Appendix B , we recall some well known optimal methods for L - smooth µ - strongly convex quadratic minimization ( i . e . , when the spectrum is contained in a single interval Λ = [ µ , L ] ) . Its purpose is exclusively to recall well - known foundation of optimization that are those algorithms and their construction . In Appendix C , we recall the polynomial formulation of the optimal method design problem , as well as a fundamental property , called “equioscillation” , to characterize the solution of this problem . In Appendix D , we provide all proofs related to cyclic step - sizes . In particular , • In Appendix D . 1 , we derive the optimal algorithm in a case where Λ is the union of 2 intervals of the same size ( See ( 3 ) ) . This leads to the use of alternating step - sizes . The resulting algorithm has a stationary form which is Algorithm 1 . • Therefore , in Appendix D . 2 , we study the heavy ball with cycling step - sizes ( Algorithm 1 ) . • In Appendix D . 3 and Appendix D . 4 , we use our results to design methods with cycles of lengths K = 2 and K = 3 . For those cases , we provide a more elegant formulation of the results . In Appendix E , we provide a proof of Theorem 5 . 1 ( local behavior beyond quadratics ) and in Appendix F , we provide some information about the code we used for the experiments in quadratic and non quadratic settings . Finally , in Appendix G we discuss similarities and diﬀerences with Oymak ( 2021 ) . Contents A Relationship between ﬁrst order methods and polynomials 12 B Optimal methods for strongly convex and smooth quadratic objective 13 B . 1 Chebyshev semi - iterative method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B . 2 Polyak heavy ball method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 C Minimax Polynomials and Equioscillation Property 17 D Cyclical step - sizes 20 D . 1 Derivation of optimal algorithm with K = 2 alternating step - sizes . . . . . . . . . . . . . . . . . . 20 D . 2 Derivation of heavy ball with K step - sizes cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D . 3 Example : alternating step - sizes ( K = 2 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D . 4 Example : 3 cycling step - sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E Beyond quadratic objective : local convergence of cycling methods 36 F Experimental setup 37 G Comparison with Oymak ( 2021 ) 37 11 Super - Acceleration with Cyclical Step - sizes A Relationship between ﬁrst order methods and polynomials In this section we prove some results on the relationship between polynomials and ﬁrst order methods for quadratic minimization , which is the starting point for our theoretical framework . This relationship is classical and was exploited by Rutishauser ( 1959 ) ; Nemirovsky ( 1992 , 1995 ) ) , to name a few . The following proposition makes this relationship precise : Proposition 4 . 1 . Let f ∈ C Λ . The iterates x t satisfy x t + 1 ∈ x 0 + span { ∇ f ( x 0 ) , . . . , ∇ f ( x t ) } , ( 8 ) where x 0 is the initial approximation of x ∗ , if and only if there exists a sequence of polynomials ( P t ) t ∈ N , each of degree at most 1 more than the highest degree of all previous polynomials and P 0 of degree 0 ( hence the degree of P t is at most t ) , such that ∀ t x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) , P t ( 0 ) = 1 . ( 9 ) Proof . We successively prove both directions of the equivalence . ( = ⇒ ) Given a ﬁrst order method , we can ﬁnd a sequence of polynomials ( P t ) t ∈ N such that , for a given quadratic function f of Hessian H and a given starting point x 0 , the iterates x t verify x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) . Moreover , The polynomials sequence ( P t ) t ∈ N veriﬁes the relations deg ( P t + 1 ) ≤ max k ≤ t deg ( P k ) + 1 and P t ( 0 ) = 1 . We proceed by induction : Initial case . Let t = 0 . Then for any ﬁrst order method we have the trivial relationship x 0 − x ∗ = P 0 ( H ) ( x 0 − x ∗ ) with P 0 = 1 . This proves the implication for t = 0 , as P 0 is a degree 0 polynomial satisfying P 0 ( 0 ) = 1 . Recursion . Let t ∈ N . We assume the following statement true , ∀ k ≤ t , x k − x ∗ = P k ( H ) ( x 0 − x ∗ ) with P k ( 0 ) = 1 . We now prove this statement is also true for t + 1 . Since x t + 1 ∈ x 0 + span { ∇ f ( x 0 ) , . . . , ∇ f ( x t ) } , there exists a family ( γ t + 1 , k ) k ∈ (cid:74) 0 , t (cid:75) such that x t + 1 = x 0 − γ t + 1 , 0 ∇ f ( x 0 ) − · · · − γ t + 1 , t ∇ f ( x t ) . ( 25 ) Then , by the induction hypothesis we have : x t + 1 − x ∗ = x 0 − x ∗ − γ t + 1 , 0 H ( x 0 − x ∗ ) − · · · − γ t + 1 , t H ( x t − x ∗ ) = x 0 − x ∗ − γ t + 1 , 0 HP 0 ( H ) ( x 0 − x ∗ ) − · · · − γ t + 1 , t HP t ( H ) ( x 0 − x ∗ ) (cid:44) P t + 1 ( H ) ( x 0 − x ∗ ) . We observe that the latest polynomial has a degree at most 1 plus the highest degree of ( P k ) k ≤ t and that P t + 1 ( 0 ) = 1 ( since P t + 1 is deﬁned as 1 plus some polynomial multiple of the polynomial X ) , which concludes the proof . ( ⇐ = ) : From a family of polynomials ( P t ) t ∈ N , with deg ( P t + 1 ) ≤ max k ≤ t deg ( P k ) + 1 and P t ( 0 ) = 1 , ( 26 ) 12 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa we can obtain a ﬁrst order method such that , for any quadratic f ( and its Hessian H ) and any starting point x 0 , we verify ∀ t ∈ N , x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) . Let the sequence ( P t ) t ∈ N veriﬁes ( 26 ) for all t ∈ N . Let d = max t (cid:48) ≤ t deg ( P t (cid:48) ) . A gap in the sequence of degrees would stand in contradiction with our assumptions . Since , there is no gap in degree , for any d (cid:48) ≤ d there exists t (cid:48) ≤ t such that deg ( P t (cid:48) ) = d (cid:48) , and therefore Span ( ( P k ) k ≤ t ) = R d [ X ] . Moreover , we know P t + 1 has a degree at most d + 1 and P t + 1 ( 0 ) = 1 , so 1 − P t + 1 ( X ) X ∈ R d [ X ] . This proves the existence of ( γ t + 1 , k ) k ∈ (cid:74) 0 , t (cid:75) such that 1 − P t + 1 ( X ) X = γ t + 1 , 0 P 0 ( X ) + · · · + γ t + 1 , t P t ( X ) . ( 27 ) Then , deﬁning x t + 1 = x 0 − γ t + 1 , 0 ∇ f ( x 0 ) − · · · − γ t + 1 , t ∇ f ( x t ) , ( 28 ) we have x t + 1 − x ∗ = x 0 − x ∗ − H ( γ t + 1 , 0 ( x 0 − x ∗ ) + · · · + γ t + 1 , t ( x t − x ∗ ) ) ( 29 ) = ( 1 − X ( γ t + 1 , 0 P 0 ( X ) + · · · + γ t + 1 , t P t ( X ) ) ) ( H ) ( x 0 − x ∗ ) ( 30 ) = P t + 1 ( H ) ( x 0 − x ∗ ) . ( 31 ) Deﬁning x t for all t according to ( 28 ) gives an algorithm that has as associated residual polynomials ( P t ) t ∈ N . The above proposition can be used to obtain worst - case rates for ﬁrst order methods by bounding their associated polynomials . Indeed , using the Cauchy - Schwartz inequality in ( 9 ) leads to (cid:107) x t − x ∗ (cid:107) ≤ sup λ ∈ Λ | P t ( λ ) | (cid:107) x 0 − x ∗ (cid:107) = ⇒ r t = sup λ ∈ Λ | P t ( λ ) | , where P ( 0 ) = 1 . ( 32 ) Therefore , ﬁnding the algorithm with the fastest worst - case rate can be equivalently framed as the problem of ﬁnding the residual polynomial with smallest value on the eigenvalue support Λ . Then , ﬁnding the fastest algorithm is equivalent of ﬁnding , for each t ≥ 0 , the polynomial of degree t that reaches the smallest inﬁnite norm on the set Λ . Therefore we introduce the notion of minimax polynomial ( Deﬁnition A . 1 ) over a set Λ as the one that reaches the smallest maximal value over Λ among a set of polynomial of ﬁxed degree and P ( 0 ) = 1 . Deﬁnition A . 1 ( Minimax polynomial of degree t over Λ ) . For any , t ≥ 0 , and any relatively compact ( i . e . bounded ) set Λ ⊂ R , the minimax polynomial of degree t over Λ , written Z Λ t , is deﬁned as Z Λ t (cid:44) argmin P ∈ R t [ X ] sup λ ∈ Λ | P ( λ ) | , subject to P ( 0 ) = 1 . ( 33 ) B Optimal methods for strongly convex and smooth quadratic objective In this section , for sake of completness , we revisit some classical methods , described in e . g . ( Polyak , 1964 ; Goh , 2017 ; Pedregosa , 2020 , 2021 ) , that are optimal when the Hessian eigenvalues are contained in a single interval of the form Λ = [ µ , L ] . To make this setup explicit , we will denote the optimal polynomials σ Λ1 and Z Λ t ( respectively deﬁned in Equation ( 16 ) and Equation ( 33 ) ) by σ [ µ , L ] 1 , and Z [ µ , L ] t . 13 Super - Acceleration with Cyclical Step - sizes As mentioned in Example 4 . 4 , the minimax polynomial Z [ µ , L ] t is Z [ µ , L ] t ( λ ) = T t ( σ [ µ , L ] 1 ( λ ) ) T t ( σ [ µ , L ] 1 ( 0 ) ) , where T t denotes the t th Chebyshev polynomial ( See e . g . Chebyshev ( 1853 ) ) and σ [ µ , L ] 1 the aﬃne function σ ( λ ) (cid:44) L + µ L − µ − 2 L − µ λ that maps [ µ , L ] onto [ − 1 , 1 ] . This can be seen a consequence of the more general equioscillation discussed in Appendix C . The next section presents one method which has Z [ µ , L ] t as associated residual polynomial . This method is known as the Chebyshev semi - iterative method . B . 1 Chebyshev semi - iterative method The algorithm follows the three terms pattern from Equation ( 13 ) to iteratively form Z Λ1 , . . . , Z Λ t . Algorithm 4 : Chebyshev semi - iterative method ( Golub and Varga , 1961 ) Input : x 0 Initialize : ω 0 = 2 x 1 = x 0 − 2 L + µ ∇ f ( x 0 ) ; for t = 1 , . . . do ω t + 1 = (cid:16) 1 − 14 (cid:0) 1 − κ 1 + κ (cid:1) 2 ω t (cid:17) − 1 ; x t + 1 = x t − 2 L + µ ω t ∇ f ( x t ) + ( ω t − 1 ) ( x t − x t − 1 ) ; end Theorem B . 1 . The iterates produced by the Chebyshev semi - iterative method verify x t − x ∗ = T t ( σ [ µ , L ] 1 ( H ) ) T t ( σ [ µ , L ] 1 ( 0 ) ) ( x 0 − x ∗ ) for all t ∈ N . ( 34 ) Furthermore , this method enjoys a worst - case rate of the form (cid:107) x t − x ∗ (cid:107) ≤ 1 T t ( σ [ µ , L ] 1 ( 0 ) ) (cid:107) x 0 − x ∗ (cid:107) = O (cid:32)(cid:18) 1 − √ κ 1 + √ κ (cid:19) t (cid:33) . ( 35 ) Proof . Consider ﬁrst an algorithm whose iterates verify ( 34 ) . Then using the Cauchy - Schwartz inequality and known bounds of Chebyshev polynomials , we can show the following rate (cid:107) x t − x ∗ (cid:107) ≤ sup λ ∈ [ µ , L ] | T t ( σ [ µ , L ] 1 ( λ ) ) | T t ( σ [ µ , L ] 1 ( 0 ) ) (cid:107) x 0 − x ∗ (cid:107) = 1 T t (cid:16) 1 + κ 1 − κ (cid:17) (cid:107) x 0 − x ∗ (cid:107) since sup x ∈ [ − 1 , 1 ] | T t ( x ) | = 1 ≤ 2 (cid:18) 1 − √ κ 1 + √ κ (cid:19) t (cid:107) x 0 − x ∗ (cid:107) since T t ( x ) ≥ (cid:0) x + √ x 2 − 1 (cid:1) t 2 , ∀ x / ∈ ( − 1 , 1 ) . It remains to prove that Algorithm 4 is the one that achieves the property ( 34 ) . Using the recursion veriﬁed by Chebyshev polynomials T t + 1 ( x ) = 2 xT t ( x ) − T t − 1 ( x ) , ( 36 ) 14 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa we have x t + 1 − x ∗ = T t + 1 ( σ [ µ , L ] 1 ( H ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) ( x 0 − x ∗ ) = 2 σ [ µ , L ] 1 ( H ) T t ( σ [ µ , L ] 1 ( H ) ) ( x 0 − x ∗ ) − T t − 1 ( σ [ µ , L ] 1 ( H ) ) ( x 0 − x ∗ ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) = 2 σ [ µ , L ] 1 ( H ) T t ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) ( x t − x ∗ ) − T t − 1 ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) ( x t − 1 − x ∗ ) = 2 σ [ µ , L ] 1 ( 0 ) T t ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) (cid:18) I − 2 L + µH (cid:19) ( x t − x ∗ ) − T t − 1 ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) ( x t − 1 − x ∗ ) . Let’s introduce ω t (cid:44) 2 σ [ µ , L ] 1 ( 0 ) T t ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) . Then ω 0 = 2 and by Chebyshev recursion ( Equation ( 36 ) ) , ω t − 1 = T t − 1 ( σ [ µ , L ] 1 ( 0 ) ) T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) . With this notation we can write the above identity more compactly as x t + 1 − x ∗ = ω t (cid:18) I − 2 L + µH (cid:19) ( x t − x ∗ ) − ( ω t − 1 ) ( x t − 1 − x ∗ ) = x t − 2 L + µω t ∇ f ( x t ) + ( ω t − 1 ) ( x t − x t − 1 ) . It remains to ﬁnd a recursion on ω t to make its use tractable . Using one more time the Chebyshev recursion Equation ( 36 ) , ω − 1 t = T t + 1 ( σ [ µ , L ] 1 ( 0 ) ) 2 σ [ µ , L ] 1 ( 0 ) T t ( σ [ µ , L ] 1 ( 0 ) ) = 2 σ [ µ , L ] 1 ( 0 ) T t ( σ [ µ , L ] 1 ( 0 ) ) − T t − 1 ( σ [ µ , L ] 1 ( 0 ) ) 2 σ [ µ , L ] 1 ( 0 ) T t ( σ [ µ , L ] 1 ( 0 ) ) = 1 − 1 4 σ [ µ , L ] 1 ( 0 ) 2 2 σ [ µ , L ] 1 ( 0 ) T t − 1 ( σ [ µ , L ] 1 ( 0 ) ) T t ( σ [ µ , L ] 1 ( 0 ) ) = 1 − 1 4 σ [ µ , L ] 1 ( 0 ) 2 ω t − 1 , which can ﬁnally be written as ω t + 1 = 1 1 − 14 (cid:16) 1 − κ 1 + κ (cid:17) 2 ω t , and we recognize the Chebyshev semi - iterative method described in Algorithm 4 . This method , unlike the Polyak heavy ball ( PHB ) method , uses a diﬀerent step - size and momentum at each iteration . However , both are related , as taking the limit of ω t as t → ∞ in Algorithm 4 we obtain ω ∞ = 1 + m with m = (cid:16) 1 −√ κ 1 + √ κ (cid:17) 2 . This correspond to the parameters of PHB . We note that this is only one way to construct a method that has the Chebsyshev polynomial as residual polynomial at every iteration . However , it is possible to construct a diﬀerent update that have the Chebyshev polynomial at ﬁxed iteration , see for instance ( Young , 1953 ; Agarwal et al . , 2021 ) for one such alterative that does not require momentum . 15 Super - Acceleration with Cyclical Step - sizes B . 2 Polyak heavy ball method Algorithm 5 : Polyak Heavy ball Input : x 0 Set : m = (cid:16) 1 −√ κ 1 + √ κ (cid:17) 2 and h = 2 ( 1 + m ) L + µ . x 1 = x 0 − h 1 + m ∇ f ( x 0 ) for t = 1 , . . . do x t + 1 = x t − h ∇ f ( x t ) + m ( x t − x t − 1 ) end Theorem B . 2 . The iterates of the heavy ball algorithm verify x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) for all t ∈ N , with P t deﬁned as P t ( λ ) (cid:44) (cid:0) √ m (cid:1) t (cid:20) 2 m 1 + mT t ( σ [ µ , L ] 1 ( λ ) ) + 1 − m 1 + mU t ( σ [ µ , L ] 1 ( λ ) ) (cid:21) . ( 37 ) Furthermore , this method enjoys a worst - case rate of the form (cid:107) x t − x ∗ (cid:107) = O (cid:32) t (cid:18) √ κ − 1 √ κ + 1 (cid:19) t (cid:33) . ( 38 ) Proof . From the update deﬁned in Algorithm 5 , we identify P 0 ( λ ) = 1 P 1 ( λ ) = 1 − h 1 + mλ P t + 1 ( λ ) = ( 1 + m − hλ ) P t ( λ ) − mP t − 1 ( λ ) . Introducing ˜ P t (cid:44) P t ( √ m ) t , we have ˜ P 0 ( λ ) = 1 ˜ P 1 ( λ ) = 1 + m − hλ ( 1 + m ) √ m = 2 1 + mσ [ µ , L ] 1 ( λ ) ˜ P t + 1 ( λ ) = ( 1 + m − hλ ) √ m ˜ P t ( λ ) − ˜ P t − 1 ( λ ) = 2 σ [ µ , L ] 1 ( λ ) ˜ P t ( λ ) − ˜ P t − 1 ( λ ) . This is a second order recurrence , with 2 initializations . It allows us to identify uniquely the family ˜ P t ( λ ) = 2 m 1 + mT t ( σ [ µ , L ] 1 ( λ ) ) + 1 − m 1 + mU t ( σ [ µ , L ] 1 ( λ ) ) . ( 39 ) where U t denotes the Chebyshev polynomial of the second kind of degree t . While both T t and U t verify the same recursion as ˜ P t and T 0 = U 0 = ˜ P 0 = 1 , the diﬀerence between T and U comes when T 1 ( X ) = X and U 1 ( X ) = 2 X . This is how ˜ P t ends being a linear combination of the T t and U t . Finally , P t ( λ ) = (cid:0) √ m (cid:1) t (cid:20) 2 m 1 + mT t ( σ [ µ , L ] 1 ( λ ) ) + 1 − m 1 + mU t ( σ [ µ , L ] 1 ( λ ) ) (cid:21) . ( 40 ) Since by deﬁnition σ [ µ , L ] 1 ( [ µ , L ] ) = [ − 1 , 1 ] , T t ( σ [ µ , L ] 1 ( λ ) ) ≤ 1 and U t ( σ [ µ , L ] 1 ( λ ) ) ≤ t + 1 , ∀ t ∈ N . Hence , ∀ λ ∈ [ µ , L ] , P t ( λ ) ≤ (cid:0) √ m (cid:1) t (cid:20) 1 + 1 − m 1 + mt (cid:21) ≤ ( 2 √ κt + 1 ) (cid:18) 1 − √ κ 1 + √ κ (cid:19) t ( 41 ) and (cid:107) x t − x ∗ (cid:107) = O (cid:32) t (cid:18) √ κ − 1 √ κ + 1 (cid:19) t (cid:33) . ( 42 ) 16 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa C Minimax Polynomials and Equioscillation Property Appendix B dealt with optimal methods when Λ = [ µ , L ] . Those methods could be derived since the minimax polynomial ( Deﬁnition A . 1 ) Z [ µ , L ] t is known . In this section we consider the problem of ﬁnding minimax polynomials in a more general setting . We provide a characterization of the minimax polynomial deﬁned in deﬁnition A . 1 . For the sake of simplicity , we actually focus on the polynomial σ Λ t solution of ( 16 ) . We can easily adapt the result to Z Λ t leveraging Remark 4 . 7 . We prove the following theorem . Theorem C . 1 . Let P K be a degree K polynomial verifying P K ( Λ ) ⊂ [ − 1 , 1 ] . Then P K is the unique solution σ Λ K of eq . ( 16 ) if and only if there exists a sorted family ( λ i ) i ∈ (cid:74) 0 , K (cid:75) ∈ (cid:0) Λ (cid:1) K + 1 ( where Λ is the closure of Λ ) such that ∀ i ∈ (cid:74) 0 , K (cid:75) , P K ( λ i ) = ( − 1 ) i . The following proof is technical and requires to introduce several new notations . Hence we ﬁrst brieﬂy describe the intuition before giving the actual complete proof . ( ⇐ = ) : Assume P K “oscillates” K + 1 times between 1 and − 1 . Since P K has a degree K , it is completely determined by its values on those K + 1 points , using the Lagrange interpolation representation . We prove that P K is optimal because any other polynomial Q K , having diﬀerent values on those K + 1 points would achieve a smaller value Q K ( 0 ) at 0 . ( = ⇒ ) : We prove this by contradiction . We assume that P K doesn’t oscillate K + 1 times between 1 and − 1 , and prove that P K ( 0 ) is not optimal . To do so , we build a small perturbation εQ K such that P K + εQ K is a polynomial of degree K , which values on Λ are all in [ − 1 ; 1 ] , and with an higher value at 0 . ( Uniqueness ) We reuse the Lagrange interpolation representation to justify that 2 optimal polynomials must “oscillate” on the same points , therefore are equal . Proof . We prove successively both directions : ( ⇐ = ) : Assume ∃ λ 0 < λ 1 < · · · < λ K such that ∀ i ∈ (cid:74) 0 , K (cid:75) , P K ( λ i ) = ( − 1 ) i and P K ( Λ ) ⊂ [ − 1 , 1 ] . ( 43 ) We aim to prove that P K is the unique solution σ Λ K of eq . ( 16 ) , that is for any other polynomial Q K of degree K verifying Q K ( Λ ) ⊂ [ − 1 , 1 ] , P K ( 0 ) ≥ Q K ( 0 ) . We introduce such a polynomial Q K of degree K and bounded in absolute value by 1 on Λ . Let’s deﬁne , for all i ∈ (cid:74) 0 , K (cid:75) , v i (cid:44) Q K ( λ i ) ∈ [ − 1 , 1 ] . ( 44 ) These K + 1 values characterize Q K ( of degree K ) , and we can decompose it over Lagrange interpolation polynomials . We have Q K = K (cid:88) i = 0 v i L λ i where L λ i ( X ) (cid:44) (cid:89) j (cid:54) = i X − λ j λ i − λ j . ( 45 ) The value at 0 can be computed as Q K ( 0 ) = K (cid:88) i = 0 v i L λ i ( 0 ) = K (cid:88) i = 0 v i (cid:89) j (cid:54) = i λ j λ j − λ i . ( 46 ) Maximizing this linear function of ( v i ) i ∈ (cid:74) 0 , K (cid:75) over the (cid:96) ∞ ball B ∞ ( 1 ) (cid:44) { ( v i ) i ∈ (cid:74) 0 , K (cid:75) , ∀ i , − 1 ≤ v i ≤ 1 } leads to , for v ∗ (cid:44) arg min v ∈ B ∞ ( 1 ) (cid:80) Ki = 0 v i (cid:81) j (cid:54) = i λ j λ j − λ i , v ∗ i = sgn  (cid:89) j (cid:54) = i λ j λ j − λ i   = ( − 1 ) i . ( 47 ) 17 Super - Acceleration with Cyclical Step - sizes where sgn is the sign function ( which maps 0 to 0 , R < 0 to − 1 , and R > 0 to 1 ) . Finally , P K ( 0 ) ≥ Q K ( 0 ) ( 48 ) which concludes the proof . ( = ⇒ ) : Assume P K alternates s < K + 1 times between − 1 and 1 on Λ . We want to show that P K is not optimal in the sense described above . To do so , we construct a perturbation of P K that increases its value in 0 while still satisfying the constraint P K ( Λ ) ⊂ [ − 1 , 1 ] . Let’s deﬁne λ ( 1 ) 0 < · · · < λ ( ν 0 ) 0 < λ ( 1 ) 1 < · · · < λ ( ν 1 ) 1 < · · · < λ ( 1 ) s − 1 < · · · < λ ( ν s − 1 ) s − 1 ( 49 ) such that P K ( λ ( j ) i ) = ( − 1 ) i and ∀ λ ∈ Λ , (cid:16) ∃ ( i , j ) | λ = λ ( j ) i or | P K ( λ ) | < 1 (cid:17) . ( 50 ) In short , (cid:16) λ ( j ) i (cid:17) ( i , j ) describes all the extremal points of P K in Λ . The indices change when the sign changes , while the exponents are used to express the possible consecutive repetitions of the same value ( − 1 or 1 ) . Set ( r i ) i ∈ (cid:74) 0 , s (cid:75) as any set of positive numbers satisfying : 0 < r 0 < inf ( Λ ) < λ ( 1 ) 0 < λ ( ν 0 ) 0 < r 1 < · · · < r s < λ ( 1 ) s − 1 < λ ( ν s − 1 ) s − 1 < sup ( Λ ) < r s . ( 51 ) By deﬁnition , each interval [ r i , r i + 1 ] , i ∈ (cid:74) 0 , s − 1 (cid:75) , contains λ ( j ) i for all j , but no other extremal points of P K in Λ . Hence , P K ( [ r i , r i + 1 ] ∩ Λ ) doesn’t contain ( − 1 ) i + 1 . Since , (cid:83) i < s , i even [ r i , r i + 1 ] ∩ Λ is compact , and by continuity of P K , P K (cid:16)(cid:83) i < s , i even [ r i , r i + 1 ] ∩ Λ (cid:17) is compact . Therefore , ∃ ε − 1 > 0 | P K   (cid:91) i < s , i even [ r i , r i + 1 ] ∩ Λ   ⊂ [ − 1 + ε − 1 , 1 ] . ( 52 ) Similarly , we obtain ∃ ε 1 > 0 | P K   (cid:91) i < s , i odd [ r i , r i + 1 ] ∩ Λ   ⊂ [ − 1 , 1 − ε 1 ] . ( 53 ) We are now equipped to build the aforementioned perturbation . Let Q K ( X ) (cid:44) (cid:89) i ∈ (cid:74) 0 , s − 1 (cid:75) ( r i − X ) . ( 54 ) Note that Q K has a degree s ≤ K and satisﬁes Q K   (cid:91) i < s , i even [ r i , r i + 1 ] ∩ Λ   ⊂ R − and Q K   (cid:91) i < s , i odd [ r i , r i + 1 ] ∩ Λ   ⊂ R + . ( 55 ) Moreover , those sets are compact , by continuity of Q K , and consequently bounded . We can therefore choose a small enough ε > 0 such that ε min Q K   (cid:91) i < s , i even [ r i , r i + 1 ] ∩ Λ   > − ε − 1 and ε max Q K   (cid:91) i < s , i odd [ r i , r i + 1 ] ∩ Λ   < ε 1 . This leads to ( P K + εQ K ) ( Λ ) ⊂ [ − 1 , 1 ] . ( 56 ) And as by deﬁnition , Q K ( 0 ) > 0 , ( P K + εQ K ) ( 0 ) > P K ( 0 ) . ( 57 ) 18 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Finally ( P K + εQ K ) ∈ R K [ X ] . This proves that P K is not optimal . ( Uniqueness ) Here , we prove that the optimal polynomial is necessarily unique . To do so , we introduce 2 optimal polynomials and show there must actually be identical . Let P K an optimal polynomial and ( λ i ) i ∈ (cid:74) 0 , K (cid:75) ∈ Λ K + 1 a family on which P K interpolates alternatively 1 and − 1 . Let any other feasible polynomial Q K and ( v i ) i ∈ (cid:74) 0 , K (cid:75) its values on ( λ i ) i ∈ (cid:74) 0 , K (cid:75) : Q K = K (cid:88) i = 0 v i L λ i . ( 58 ) We have showed in the ﬁrst point of this proof that the optimal values of v i are alternatively 1 and − 1 . Consequently , if Q K is also optimal , Q K ( λ i ) = P K ( λ i ) ( 59 ) for all i ∈ (cid:74) 0 , K (cid:75) , which characterizes polynomials of degree K . Then Q K = P K ( 60 ) which shows that the optimal polynomial is unique . We now give the formal statement and the proof of the second result , used in Subsection 4 . 4 . Theorem C . 2 . T n ( σ K ) is optimal for all n if and only if σ K veriﬁes the equioscillation property ( Deﬁnition 4 . 3 , hence σ K = σ Λ K by Theorem C . 1 ) and Λ = σ − 1 K ( [ − 1 , 1 ] ) , i . e . the inverse mapping σ − 1 K transforms the interval [ − 1 , 1 ] into exactly Λ . Before providing the proof , we ﬁrst highlight that the property ∀ λ ∈ Λ , σ K ( λ ) ∈ [ − 1 , 1 ] ( 61 ) can equivalently be written Λ ⊂ σ − 1 K ( [ − 1 , 1 ] ) . ( 62 ) In other words , we are interested in the case where the reverse inclusion holds as well . This means that σ K ( λ ) ∈ [ − 1 , 1 ] ⇒ λ ∈ Λ . ( 63 ) This corresponds to a stronger form of optimality of σ K : it “fully” uses the available assumption related to Λ , in the sense that no point can be added to Λ without breaking the condition σ K ( Λ ) ⊂ [ − 1 ; 1 ] . For example , on Figure 3 , σ Λ3 does not satisfy the later property on the center graph , but satisﬁes it on the right graph . Here , we show that under this condition , T n ( σ K ) = T n ( σ Λ K ) is optimal ( in the sense of ( 16 ) ) for all n ∈ N . In Section 4 . 4 , we give another view of this condition for T n ( σ K ) to be optimal for all n . We can decompose Λ as the union of K intervals Λ i such that they have disjoint interiors and they are all mapped to [ − 1 , 1 ] by σ K . Hence , σ K maps Λ to [ − 1 , 1 ] exactly K times . Proof . From Theorem C . 1 , T n ( σ K ) is optimal for all n if and only if , for all n , there exist a sorted family of ( λ i ) i ∈ (cid:74) 0 , nK (cid:75) such that , T n ( σ K ( λ i ) ) = ( − 1 ) i . Let n ∈ N . We observe that by deﬁnition of T n , T n ( σ K ( λ ) ) = ± 1 if and only if ∃ j ∈ (cid:74) 0 , n (cid:75) | σ K ( λ ) = cos jπ n . ( 64 ) We successively treat both directions : ( ⇐ = ) we assume σ K oscillates and Λ = σ − 1 K ( [ − 1 , 1 ] ) . We aim to prove that T n ( σ K ) is optimal for all n ∈ N . By equioscillation property , we know that there exists λ (cid:48) i such that σ K ( λ (cid:48) i ) = ( − 1 ) i . ( 65 ) 19 Super - Acceleration with Cyclical Step - sizes By the intermediate value theorem , we know that for any i ∈ (cid:74) 0 ; K (cid:75) , between the pair λ (cid:48) i , λ (cid:48) i + 1 , there exist sorted ( µ ji ) ni < j < ( n + 1 ) i such that for all j ∈ (cid:74) ni + 1 ; ( n + 1 ) i − 1 (cid:75) , σ K ( µ ji ) = cos jπ n . ( 66 ) We identify λ ni = λ (cid:48) i and λ j = µ j (cid:98) j / n (cid:99) for all j not multiple of n . Then , for all (cid:96) ∈ (cid:74) 0 , nK (cid:75) : T n ( σ K ( λ (cid:96) ) ) = ( − 1 ) (cid:96) . ( 67 ) By Theorem C . 1 , we conclude that T n ( σ K ) is optimal for all n ∈ N . ( = ⇒ ) We assume T n ( σ K ) is optimal for all n ∈ N . Clearly , σ K is optimal ( n = 1 ) , and then equioscillates . We prove that moreover Λ = σ − 1 K ( [ − 1 , 1 ] ) . ( 68 ) On the one hand , for any j ∈ (cid:74) 0 , n (cid:75) , there exist at most K diﬀerent λ that veriﬁes σ K ( λ ) = cos jπn since σ K has a degree K and is not constant . Therefore , there exist at most ( n + 1 ) K diﬀerent λ such that ∃ j ∈ (cid:74) 0 , n (cid:75) | σ K ( λ ) = cos jπn , and by Eq . ( 64 ) , there thus exist at most ( n + 1 ) K diﬀerent λ such that T n ( σ K ( λ ) = ± 1 . On the other hand , the optimality of T n ( σ K ) implies the existence of at least nK + 1 such λ in Λ . Hence all but at most K − 1 values λ such that σ K ( λ ) ∈ { cos jπn , j ∈ (cid:74) 0 , n (cid:75) } belong to Λ . This holds for all n . Therefore for n large enough , all x such that σ ( x ) ∈ [ − 1 , 1 ] are as close as we want to some λ ∈ Λ . Since Λ is a closed set , then all x such that σ ( x ) ∈ [ − 1 , 1 ] are actually in Λ . We conclude Λ ⊃ σ − 1 K ( [ − 1 , 1 ] ) . ( 69 ) D Cyclical step - sizes In this appendix , we provide an analysis of momentum methods with cyclical step - sizes and derive some non - asymptotically optimal variants . D . 1 Derivation of optimal algorithm with K = 2 alternating step - sizes In this section , we consider the case where Λ is the union of 2 intervals of same size , as described in Section 3 . We start by introducing the following algorithm , and we will prove later that this algorithm is optimal ( Theorem D . 1 ) Theorem D . 1 . Let f ∈ C Λ and x 0 ∈ R d . Assume Λ deﬁned as in ( 3 ) . The iterates of Algorithm 6 veriﬁes the condition x 2 n − x ∗ = T n ( σ Λ2 ( H ) ) T n ( σ Λ2 ( 0 ) ) ( x 0 − x ∗ ) ( 70 ) and this is the optimal convergence rate over C Λ . Proof . We begin by showing the optimality of the algorithm . Using Proposition D . 2 , the polynomial in ( 70 ) equioscillates on Λ , which makes it optimal by Theorem C . 1 . By optimal , this means this is the optimal convergence rate any ﬁrst order algorithm can reach ( See ( 11 ) ) . We invite the reader to read Appendix D . 3 , where we study in details the properties of the alternating steps sizes strategy ( i . e . , K = 2 ) . As in Appendix B . 1 , we derive here the constructive approach that leads us to this algorithm . We now start showing that the iterates of Algorithm 6 follow ( 70 ) . From eq . ( 70 ) , projecting onto the eigenspace of eigenvalue λ , x 2 n − x ∗ = T n ( σ Λ 2 ( λ ) ) T n ( σ Λ2 ( 0 ) ) ( x 0 − x ∗ ) . ( 71 ) 20 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Algorithm 6 : Optimal momentum method with alternating step - sizes ( K = 2 ) Input : Initialization x 0 , µ 1 < L 1 < µ 2 < L 2 ( where L 1 − µ 1 = L 2 − µ 2 ) Set : ρ = L 2 + µ 1 L 2 − µ 1 , R = µ 2 − L 1 L 2 − µ 1 , c = (cid:113) ρ 2 − R 2 1 − R 2 ω 0 = 2 x 1 = x 0 − 1 L 1 ∇ f ( x 0 ) for t = 1 , 2 , . . . do ω t = (cid:18) 1 − 1 4 c 2 ω t − 1 (cid:19) − 1 h t = ω t L 1 ( if t is even ) , h t = ω t µ 2 ( if t is odd ) x t + 1 = x t − h t ∇ f ( x t ) + ( ω t − 1 ) ( x t − x t − 1 ) end Then , we ﬁnd a recursion deﬁnition for the subsequence ( x 2 n ) n ∈ N . Let n ≥ 1 . x 2 ( n + 1 ) − x ∗ = T n + 1 ( σ Λ2 ( λ ) ) T n + 1 ( σ Λ2 ( 0 ) ) ( x 0 − x ∗ ) , ( 72 ) = 2 σ Λ2 ( λ ) T n ( σ Λ2 ( λ ) ) − T n − 1 ( σ Λ2 ( λ ) ) T n + 1 ( σ Λ2 ( 0 ) ) ( x 0 − x ∗ ) , ( 73 ) = 2 σ Λ2 ( λ ) T n ( σ Λ2 ( 0 ) ) T n + 1 ( σ Λ2 ( 0 ) ) ( x 2 n − x ∗ ) − T n − 1 ( σ Λ2 ( 0 ) ) T n + 1 ( σ Λ2 ( 0 ) ) ( x 2 ( n − 1 ) − x ∗ ) . ( 74 ) Note that if σ Λ2 ( λ ) were a degree 1 polynomial in λ , then we would recognize a momentum update . Here , σ Λ2 ( λ ) is actually a degree 2 polynomial in λ . We will then try to identify 2 steps of momentum . From here , let c (cid:44) 1 2 (cid:18)(cid:16) σ K ( 0 ) + (cid:112) σ K ( 0 ) 2 − 1 (cid:17) 1 / 2 + (cid:16) σ K ( 0 ) − (cid:112) σ K ( 0 ) 2 − 1 (cid:17) 1 / 2 (cid:19) = (cid:114) σ K ( 0 ) + 1 2 ( 75 ) be the unique positive real number c verifying T 2 ( c ) = 2 c 2 − 1 = σ K ( 0 ) . We end up with x 2 ( n + 1 ) − x ∗ = 2 σ Λ2 ( λ ) T 2 n ( c ) T 2 ( n + 1 ) ( c ) ( x 2 n − x ∗ ) − T 2 ( n − 1 ) ( c ) T 2 ( n + 1 ) ( c ) ( x 2 ( n − 1 ) − x ∗ ) . ( 76 ) Note , the above equation suggests to introduce the sequence z l (cid:44) T l ( c ) ( x l − x ∗ ) . Indeed , the above equality simpliﬁes z 2 ( n + 1 ) = 2 σ Λ2 ( λ ) z 2 n − z 2 ( n − 1 ) . ( 77 ) Let’s look for two steps of the cyclical heavy ball method that are together equivalent to ( 76 ) . We look for an algorithm of the form ∀ n ≥ 0 , x n + 1 = x n − h n ∇ f ( x n ) + T n − 1 ( c ) T n + 1 ( c ) ( x n − x n − 1 ) , ( 78 ) i . e , projecting again onto the eigenspace of eigenvalue λ , we obtain ∀ n ≥ 0 , x n + 1 − x ∗ = (cid:18) 1 + T n − 1 ( c ) T n + 1 ( c ) − h n λ (cid:19) ( x n − x ∗ ) − T n − 1 ( c ) T n + 1 ( c ) ( x n − 1 − x ∗ ) . ( 79 ) Here we introduce the notation ω l (cid:44) (cid:18) 1 + T l − 1 ( c ) T l + 1 ( c ) (cid:19) = 2 c T l ( c ) T l + 1 ( c ) , ( 80 ) and the change of variable ˜ h l (cid:44) h l ω l . ( 81 ) 21 Super - Acceleration with Cyclical Step - sizes We rewrite ( 79 ) in terms of the sequence z and using the sequence ˜ h , ∀ n ≥ 0 , z n + 1 = T n + 1 ( c ) (cid:18) 1 + T n − 1 ( c ) T n + 1 ( c ) − h n λ (cid:19) ( x n − x ∗ ) − z n − 1 ( 82 ) = (cid:16) 2 cT n ( c ) ( 1 − ˜ h n λ ) (cid:17) ( x n − x ∗ ) − z n − 1 ( 83 ) = (cid:16) 2 c ( 1 − ˜ h n λ ) (cid:17) z n − z n − 1 . ( 84 ) We now need to ﬁnd the right sequence ˜ h n such that we recover eq . ( 77 ) . Combining the 2 following z 2 n + 1 = (cid:16) 2 c ( 1 − ˜ h 2 n λ ) (cid:17) z 2 n − z 2 n − 1 ( 85 ) z 2 n + 2 = (cid:16) 2 c ( 1 − ˜ h 2 n + 1 λ ) (cid:17) z 2 n + 1 − z 2 n ( 86 ) by isolating the odd index in the second equation and plugging it in the ﬁrst one , we get z 2 n + 2 = (cid:32) 4 c 2 ( 1 − ˜ h 2 n λ ) ( 1 − ˜ h 2 n + 1 λ ) − 1 − 2 c ( 1 − ˜ h 2 n + 1 λ ) 2 c ( 1 − ˜ h 2 n − 1 λ ) (cid:33) z 2 n − 2 c ( 1 − ˜ h 2 n + 1 λ ) 2 c ( 1 − ˜ h 2 n − 1 λ ) z 2 n − 2 . ( 87 ) We need to identify 2 σ Λ2 ( λ ) = 4 c 2 ( 1 − ˜ h 2 n λ ) ( 1 − ˜ h 2 n + 1 λ ) − 1 − 2 c ( 1 − ˜ h 2 n + 1 λ ) 2 c ( 1 − ˜ h 2 n − 1 λ ) , ( 88 ) 1 = 2 c ( 1 − ˜ h 2 n + 1 λ ) 2 c ( 1 − ˜ h 2 n − 1 λ ) . ( 89 ) Hence , we conclude from the second equation that ˜ h 2 n + 1 = ˜ h 2 n − 1 = ˜ h 1 is independent of n . And the ﬁrst equation then becomes 2 σ Λ2 ( λ ) = 4 c 2 ( 1 − ˜ h 2 n λ ) ( 1 − ˜ h 1 λ ) − 2 ( 90 ) leading also to ˜ h 2 n independent of n . We observe an alternating strategy of the “pseudo - step - sizes” ˜ h 0 and ˜ h 1 . Finally , we must ﬁx them to σ Λ2 ( λ ) = 2 c 2 ( 1 − ˜ h 0 λ ) ( 1 − ˜ h 1 λ ) − 1 . ( 91 ) Note this is possible because the equation above is valid for λ = 0 for any choice of ˜ h 0 and ˜ h 1 and the polynomial σ Λ2 + 1 can be deﬁned by its value in 0 and its roots that are exactly 1˜ h 0 and 1˜ h 1 . And from ( 155 ) , those values are µ 2 and L 1 , which gives the values ˜ h 0 = 1 L 1 and ˜ h 1 = 1 µ 2 . We now sum up what we have so far . Setting c , ˜ h 0 and ˜ h 1 as described above , the iterations ∀ n ≥ 1 , x n + 1 = x n − (cid:18) 1 + T n − 1 ( c ) T n + 1 ( c ) (cid:19) ˜ h mod ( n , 2 ) ∇ f ( x n ) + T n − 1 ( c ) T n + 1 ( c ) ( x n − x n − 1 ) ( 92 ) lead to the recursion ( 77 ) . Let deﬁne x 1 = x 0 − ˜ h 0 ∇ f ( x 0 ) , and from the above x 2 = x 1 − (cid:18) 1 + 1 2 c 2 − 1 (cid:19) ˜ h 1 λ ( x 1 − x ∗ ) + 1 2 c 2 − 1 ( x 1 − x 0 ) ( 93 ) x 2 − x ∗ = 2 c 2 σ Λ 2 ( 0 ) (cid:16) 1 − ˜ h 1 λ (cid:17) ( x 1 − x ∗ ) − 1 σ Λ 2 ( 0 ) ( x 0 − x ∗ ) ( 94 ) = 2 c 2 σ Λ2 ( 0 ) (cid:16) 1 − ˜ h 1 λ (cid:17) (cid:16) 1 − ˜ h 0 λ (cid:17) ( x 0 − x ∗ ) − 1 σ Λ2 ( 0 ) ( x 0 − x ∗ ) ( 95 ) = σ Λ2 ( λ ) σ Λ2 ( 0 ) ( x 0 − x ∗ ) ( 96 ) z 2 = σ Λ 2 ( λ ) . ( 97 ) 22 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Finally , the sequence z 2 n is deﬁned by z 0 = 1 , ( 98 ) z 1 = σ Λ2 ( λ ) , ( 99 ) z 2 ( n + 1 ) = 2 σ Λ2 ( λ ) z 2 n − z 2 ( n − 1 ) . ( 100 ) which deﬁnes exactly T n ( σ Λ2 ( λ ) ) . We conclude x 2 n − x ∗ = T n ( σ Λ2 ( λ ) ) T n ( σ Λ2 ( 0 ) ) ( x 0 − x ∗ ) . We sum up the algorithm used to reach the above equality : x 1 = x 0 − ˜ h 0 ∇ f ( x 0 ) , ( 101 ) ∀ n ≥ 0 , x n + 1 = x n − ω n ˜ h n ∇ f ( x n ) + ( ω n − 1 ) ( x n − x n − 1 ) . ( 102 ) with ω n = (cid:16) 1 + T n − 1 ( c ) T n + 1 ( c ) (cid:17) = 2 cT n ( c ) T n + 1 ( c ) . Note the recursion ω − 1 n = T n + 1 ( c ) 2 cT n ( c ) ( 103 ) = 2 cT n ( c ) − T n − 1 ( c ) 2 cT n ( c ) ( 104 ) = 1 − T n − 1 ( c ) 2 cT n ( c ) ( 105 ) = 1 − 1 4 c 2 2 cT n − 1 ( c ) T n ( c ) ( 106 ) = 1 − 1 4 c 2 ω n − 1 . ( 107 ) Finally , the sequence ω can be computed online using the recursion ω n = 1 1 − 14 c 2 ω n − 1 ( 108 ) with ω 0 = 2 . In this appendix , as well as in Appendix B , we end up with some equality of the form (cid:107) x t − x ∗ (cid:107) = T n ( σ K ( H ) ) T n ( σ K ( 0 ) ) (cid:107) x 0 − x ∗ (cid:107) . ( 109 ) The next theorem explains how to derive the rate factor from it . Proposition 4 . 6 . For a given σ K such that sup λ ∈ Λ | σ K ( λ ) | = 1 , the asymptotic rate factor τ σ K of the method associated to the polynomial ( 14 ) is 1 − τ σ K = lim t →∞ t (cid:114) sup λ ∈ Λ | P t ( λ ; σ K ) | = (cid:16) σ 0 − (cid:112) σ 20 − 1 (cid:17) 1 K , with σ 0 (cid:44) σ K ( 0 ) . ( 15 ) Proof . We observe that the rate factor of the method is upper bounded by t (cid:114) sup λ ∈ Λ | Z Λ t ( λ ) | = t (cid:118)(cid:117)(cid:117) (cid:116) sup λ ∈ Λ (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) T t / K (cid:0) σ Λ K ( λ ) (cid:1) T t / K ( σ 0 ) (cid:12)(cid:12) (cid:12)(cid:12)(cid:12) = t (cid:115) 1 | T t / K ( σ 0 ) | if sup λ ∈ Λ | σ K ( λ ) | = 1 . ( 110 ) Since σ 0 > 1 , and by using the explicit formula of Chebyshev polynomials , we have that T t / K ( σ 0 ) = (cid:16) σ 0 + (cid:112) σ 20 − 1 (cid:17) t / K + (cid:16) σ 0 − (cid:112) σ 20 − 1 (cid:17) t / K 2 ∼ t →∞ (cid:16) σ 0 + (cid:112) σ 20 − 1 (cid:17) t / K 2 . ( 111 ) 23 Super - Acceleration with Cyclical Step - sizes Algorithm 7 : Cyclical heavy ball HB K ( h 0 , . . . , h K − 1 ; m ) Input : Initialization x 0 , momentum m ∈ ( 0 , 1 ) , step - sizes { h 0 , . . . , h K − 1 } x 1 = x 0 − h 0 1 + m ∇ f ( x 0 ) for t = 1 , 2 , . . . do x t + 1 = x t − h mod ( t , K ) ∇ f ( x t ) + m ( x t − x t − 1 ) end Taking the limit gives lim t →∞ t (cid:115) 1 | T t / K ( σ 0 ) | = (cid:32) 1 σ 0 + (cid:112) σ 20 − 1 (cid:33) 1 K = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K . ( 112 ) D . 2 Derivation of heavy ball with K step - sizes cycle In this section , we consider heavy ball algorithm with a cycle of K diﬀerent step - sizes . For convenience , we restate Algorithm 1 below . We ﬁrst recall the convergence theorem 4 . 8 stated in Section 4 . 3 . Theorem 4 . 8 . With an arbitrary momentum m and an arbitrary sequence of step - sizes { h i } , the worst - case rate of convergence 1 − τ of Algorithm 1 on C Λ is   √ m if σ ∗ ≤ 1 √ m (cid:16) σ ∗ + (cid:112) σ 2 ∗ − 1 (cid:17) K − 1 if σ ∗ ∈ (cid:32) 1 , 1 + m K 2 ( √ m ) K (cid:33) ≥ 1 ( no convergence ) if σ ∗ ≥ 1 + m K 2 ( √ m ) K , ( 19 ) where σ ∗ (cid:44) sup λ ∈ Λ | σ ( λ ; { h i } , m ) | , σ ( λ ; { h i } , m ) is the K - degree polynomial σ ( λ ; { h i } , m ) (cid:44) 1 2Tr ( M 1 M 2 . . . M K ) , ( 20 ) and M i = (cid:34) 1 + m − h K − i λ √ m − 1 1 0 (cid:35) . Proof . Note a ﬁrst trick . Let’s deﬁne x − 1 (cid:44) x 0 − h 0 1 + m ∇ f ( x 0 ) . This way , x t + 1 = x t − h mod ( t , K ) ∇ f ( x t ) + m ( x t − x t − 1 ) holds for any t ≥ 0 ( including t = 0 ) . Now , let’s introduce the polynomials P t deﬁned by Proposition 4 . 1 as x t − x ∗ = P t ( H ) ( x 0 − x ∗ ) . From now , in order to highlight the K - cyclic behavior , we introduce the indexation t = nK + r , with r ∈ (cid:74) 0 , K − 1 (cid:75) . We verify the following : P − 1 ( λ ) = 1 − h 0 λ 1 + m , ( 113 ) P 0 ( λ ) = 1 , ( 114 ) ∀ n ≥ 0 , r ∈ (cid:74) 0 , K − 1 (cid:75) , P nK + r + 1 ( λ ) = ( 1 + m − h r λ ) P nK + r ( λ ) − mP nK + r − 1 ( λ ) . ( 115 ) In order to get rid of the last occurrence of m in equation above , we introduce ˜ P t ( λ ) (cid:44) 1 ( √ m ) t P t ( λ ) . 24 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa This way , the above can be written ˜ P − 1 ( λ ) = √ m (cid:18) 1 − h 0 λ 1 + m (cid:19) = 2 m 1 + mσ 0 ( λ ) , ( 116 ) ˜ P 0 ( λ ) = 1 , ( 117 ) ∀ n ≥ 0 , r ∈ (cid:74) 0 , K − 1 (cid:75) , ˜ P nK + r + 1 ( λ ) = 1 + m − h r λ √ m ˜ P nK + r ( λ ) − ˜ P nK + r − 1 ( λ ) . ( 118 ) In the following , we want to determine a formulation for the polynomials ˜ P nK . In order to do so , we introduce the following operator : A ( λ ) (cid:44)   1 + m − h K − 1 λ √ m − 1 1 0   · · ·   1 + m − h 0 λ √ m − 1 1 0   (cid:44) (cid:18) a ( λ ) b ( λ ) c ( λ ) d ( λ ) (cid:19) ( 119 ) as well as the scalar valued function σ ( λ ; { h i } , m ) (cid:44) 1 2Tr ( A ( λ ) ) . ( 120 ) This operator comes naturally in (cid:18) ˜ P ( n + 1 ) K ( λ ) ˜ P ( n + 1 ) K − 1 ( λ ) (cid:19) =   1 + m − h K − 1 λ √ m − 1 1 0   (cid:18) ˜ P ( n + 1 ) K − 1 ( λ ) ˜ P ( n + 1 ) K − 2 ( λ ) (cid:19) ( 121 ) =   1 + m − h K − 1 λ √ m − 1 1 0   · · ·   1 + m − h 0 λ √ m − 1 1 0   (cid:18) ˜ P nK ( λ ) ˜ P nK − 1 ( λ ) (cid:19) ( 122 ) = A ( λ ) (cid:18) ˜ P nK ( λ ) ˜ P nK − 1 ( λ ) (cid:19) . ( 123 ) Looking K steps at a time makes the analysis much easier as the process applying K steps is then homogeneous ( we apply A and A doesn’t depend on the index of the iterate ) . ˜ P ( n + 1 ) K ( λ ) = a ( λ ) ˜ P nK ( λ ) + b ( λ ) ˜ P nK − 1 ( λ ) , ( 124 ) ˜ P ( n + 1 ) K − 1 ( λ ) = c ( λ ) ˜ P nK ( λ ) + d ( λ ) ˜ P nK − 1 ( λ ) . ( 125 ) Combining the two above equations ( First one with incremented n + b ( λ ) times the second one - d ( λ ) times the ﬁrst one ) leads to ˜ P ( n + 2 ) K ( λ ) = ( a ( λ ) + d ( λ ) ) ˜ P ( n + 1 ) K ( λ ) − ( a ( λ ) d ( λ ) − b ( λ ) c ( λ ) ) ˜ P nK ( λ ) ( 126 ) = 2 σ ( λ ; { h i } , m ) ˜ P ( n + 1 ) K ( λ ) − ˜ P nK ( λ ) ( 127 ) where the second inequality is deduced after we recognize a ( λ ) + d ( λ ) = Tr ( A ( λ ) ) = 2 σ ( λ ; { h i } , m ) ( 128 ) and a ( λ ) d ( λ ) − b ( λ ) c ( λ ) = Det ( A ( λ ) ) = 1 ( 129 ) ( A ( λ ) is the product of matrices of determinant 1 ) . In equation ( 127 ) we recognize the recursion veriﬁed by e . g . ( T n ( σ ( λ ; { h i } , m ) ) ) n ∈ N , or ( U n ( σ ( λ ; { h i } , m ) ) ) n ∈ N , where T n ( resp . U n ) denotes the ﬁrst ( resp . second ) type Chebyshev polynomial of degree n . 25 Super - Acceleration with Cyclical Step - sizes Moreover we verify the initialization ˜ P 0 ( λ ) = 1 , ( 130 ) ˜ P K ( λ ) = a ( λ ) ˜ P 0 ( λ ) + b ( λ ) ˜ P − 1 ( λ ) ( 131 ) = a ( λ ) + b ( λ ) m 1 + m 1 + m − h 0 λ √ m . ( 132 ) We also notice that U n ( σ ( λ ; { h i } , m ) ) + (cid:18) b ( λ ) m 1 + m 1 + m − h 0 λ √ m − d ( λ ) (cid:19) U n − 1 ( σ ( λ ; { h i } , m ) ) ( 133 ) veriﬁes the same recursion of order 2 than ˜ P Kn as well as the same 2 initial terms . Finally , we conclude ˜ P nK ( λ ) = U n ( σ ( λ ; { h i } , m ) ) + (cid:18) b ( λ ) m 1 + m 1 + m − h 0 λ √ m − d ( λ ) (cid:19) U n − 1 ( σ ( λ ; { h i } , m ) ) ( 134 ) and P nK ( λ ) = (cid:0) √ m (cid:1) nK ˜ P nK ( λ ) . ( 135 ) Now we have the full expression of the polynomials associated to algorithm 1 . Then we can study it’s rate of convergence . Note for any r ∈ (cid:74) 0 , K − 1 (cid:75) , we can have a similar expression of the form P nK + r ( λ ) = (cid:0) √ m (cid:1) nK (cid:0) Q 1 r ( λ ) U n ( σ ( λ ; { h i } , m ) ) + Q 2 r ( λ ) U n − 1 ( σ ( λ ; { h i } , m ) ) (cid:1) ( 136 ) with Q 1 r and Q 2 r some ﬁxed polynomials . This is the consequence of the fact that all sequences ˜ P nK + r ( λ ) verify the same recursion formula . Only initialization are diﬀerent . In order to study the factor rate of this algorithm , let’s ﬁrst introduce M an upper bound of all the | Q ir | . For instance , let M deﬁned as follow . M = max r ∈ (cid:74) 0 , K − 1 (cid:75) , i ∈ { 1 , 2 } sup λ ∈ Λ | Q ir ( λ ) | . ( 137 ) Then , (cid:107) x t − x ∗ (cid:107) ≤ sup λ ∈ Λ | P t ( λ ) | (cid:107) x 0 − x ∗ (cid:107) ( 138 ) ≤ M (cid:0) √ m (cid:1) t (cid:18) sup λ ∈ Λ | U n ( σ ( λ ; { h i } , m ) ) | + sup λ ∈ Λ | U n − 1 ( σ ( λ ; { h i } , m ) ) | (cid:19) (cid:107) x 0 − x ∗ (cid:107) , ( 139 ) with n = (cid:98) tK (cid:99) . Set σ sup (cid:44) sup λ ∈ Λ | σ ( λ ; { h i } , m ) | . The worst - case rate veriﬁes If σ sup ≤ 1 , then r t ≤ M (cid:0) √ m (cid:1) t ( n + 1 + n ) = O (cid:16) t (cid:0) √ m (cid:1) t (cid:17) . ( 140 ) If σ sup > 1 , then r t = O (cid:18)(cid:0) √ m (cid:1) t (cid:16) σ sup + (cid:113) σ 2sup − 1 (cid:17) n (cid:19) . ( 141 ) The ﬁrst case analysis comes from the fact that U n is bounded by n + 1 on [ − 1 , 1 ] , while the second cases analysis comes from the fact that U n ( x ) grows exponentially fast outside of [ − 1 , 1 ] at a rate x + √ x 2 − 1 . 26 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Then the factor rate veriﬁes If σ sup ≤ 1 , 1 − τ = √ m . ( 142 ) If σ sup > 1 , 1 − τ = √ m (cid:16) σ sup + (cid:113) σ 2sup − 1 (cid:17) 1 / K . ( 143 ) It remains to notice that √ m (cid:16) σ sup + (cid:113) σ 2sup − 1 (cid:17) 1 / K < 1 is equivalent to σ sup < 1 + m k 2 ( √ m ) k . From this factor rate analysis , we can state Proposition 4 . 9 of Section 4 . 3 . Proposition 4 . 9 . Let σ ( λ ; { h i } , m ) be the polynomial deﬁned by ( 20 ) , and σ Λ K be the optimal link function of degree K deﬁned by ( 16 ) . If the momentum m and the sequence of step - sizes { h i } satisfy σ ( λ ; { h i } , m ) = σ Λ K ( λ ) , ( 21 ) then 1 ) the parameters are optimal , in the sense that they minimize the asymptotic rate factor from Theorem 4 . 8 , 2 ) the optimal momentum parameter is m = (cid:0) σ 0 − (cid:112) σ 20 − 1 (cid:1) 2 / K , where σ 0 = σ Λ K ( 0 ) , ( 22 ) 3 ) the iterates from Algo . 3 with parameters { h i } and m form a polynomial with recurrence ( 18 ) , and 4 ) Algo - rithm 3 achieves the worst - case rate r Alg . 3 t and the asymptotic rate factor 1 − τ Alg . 3 r Alg . 3 t = O (cid:18) t (cid:16) σ 0 − (cid:112) σ 20 − 1 (cid:17) t / K (cid:19) , ( 23 ) 1 − τ Alg . 3 = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 / K . Proof . For now we don’t assume assumption 21 yet . Set σ 0 (cid:44) σ ( 0 ; { h i } , m ) . Then , by deﬁnition ( 20 ) of σ ( λ ; { h i } , m ) , σ 0 = 1 2Tr (cid:32)(cid:20) 1 + m √ m − 1 1 0 (cid:21) K (cid:33) = T K (cid:18) 1 + m 2 √ m (cid:19) = 1 + m K 2 ( √ m ) K . ( 144 ) Hence , reversing this equality , √ m = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K . ( 145 ) From Theorem 4 . 8 , we therefore know If σ sup ≤ 1 , 1 − τ = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K . ( 146 ) If σ sup > 1 , 1 − τ = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K (cid:16) σ sup + (cid:113) σ 2sup − 1 (cid:17) 1 / K . ( 147 ) But , one can check that (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K (cid:16) σ sup + (cid:113) σ 2sup − 1 (cid:17) 1 / K ≥   σ 0 σ sup − (cid:115)(cid:18) σ 0 σ sup (cid:19) 2 − 1   1 K ( 148 ) which shows that a tuning generating the polynomial σ ( λ ; { h i } , m ) σ sup would lead to a better convergence rate . Hence , we should look for polynomials σ ( λ ; { h i } , m ) verifying σ sup ≤ 1 . And then , 1 − τ = √ m = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 K . ( 149 ) 27 Super - Acceleration with Cyclical Step - sizes which explain we aim at maximizing σ 0 subject to σ sup ≤ 1 ( ( 16 ) ) . Finally , we proved 1 ) : if σ ( λ ; { h i } , m ) = σ Λ K ( λ ) , then the tuning is optimal in the sense that this is the one that minimizes the asymptotic rate factor among all K steps - sizes based tuning . From now , we assume σ ( λ ; { h i } , m ) = σ Λ K ( λ ) . ( 150 ) Therefore , σ 0 = σ Λ K ( 0 ) ( 151 ) and 2 ) is already proven above . 3 ) follows directly from the deﬁnition of σ Λ K ( λ ) . Finally , since σ sup ≤ 1 , we know 1 − τ = √ m = (cid:18) σ 0 − (cid:113) σ 20 − 1 (cid:19) 1 / K ( 152 ) which proves part of 4 ) . To prove the expression of the worst - case rate r t , we need to apply the intermediate result ( 140 ) instead of Theorem 4 . 8 . D . 3 Example : alternating step - sizes ( K = 2 ) Proposition D . 2 . The strategy with 2 step - sizes is optimal on the union of two intervals if and only if they have the same length . Proof . This is a direct consequence of Theorem C . 2 , which implies σ Λ2 ( µ 1 ) = σ Λ2 ( L 2 ) = 1 and σ Λ2 ( µ 2 ) = σ Λ2 ( L 1 ) = − 1 . This is feasible if and only if L 2 − µ 2 = L 1 − µ 1 since σ Λ2 is a degree 2 polynomial . Indeed , set σ Λ2 ( x ) = a ( x − b ) 2 + c . Then , σ Λ2 ( µ 1 ) = σ Λ2 ( L 2 ) implies a ( µ 1 − b ) 2 + c = a ( L 2 − b ) 2 + c , then | µ 1 − b | = | L 2 − b | and ﬁnally b = µ 1 + L 2 2 . Similarly , σ Λ2 ( µ 2 ) = σ Λ2 ( L 1 ) implies b = µ 2 + L 1 2 . We conclude µ 1 + L 2 2 = µ 2 + L 1 2 , and L 2 − µ 2 = L 1 − µ 1 . Proposition 4 . 5 . Let Λ = [ µ 1 , L 1 ] ∪ [ µ 2 , L 2 ] be an union of two intervals of the same size ( L 1 − µ 1 = L 2 − µ 2 ) and let m , h 0 , h 1 be as deﬁned in Algorithm 2 . Then the minimax polynomial ( solution to ( 12 ) ) is , for all t = 2 n , n ∈ N + 0 , T n (cid:0) σ Λ2 ( λ ) (cid:1) T n (cid:0) σ Λ2 ( 0 ) (cid:1) = arg min P ∈ R t [ X ] , P ( 0 ) = 1 sup λ ∈ Λ | P ( λ ) | , with σ Λ2 ( λ ) = 1 2 m ( 1 + m − λh 0 ) ( 1 + m − λh 1 ) − 1 . Proof . From Theorem C . 2 , σ Λ2 ( µ 1 ) = 1 , ( 153 ) σ Λ2 ( L 1 ) = − 1 , ( 154 ) σ Λ2 ( µ 2 ) = − 1 , ( 155 ) σ Λ2 ( L 2 ) = 1 , ( 156 ) and this implies that T n ( σ Λ 2 ( λ ) ) T n ( σ Λ2 ( 0 ) ) is optimal . 28 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa In particular , L 1 and µ 2 are roots of σ Λ2 + 1 . Therefore , we know there exists a constant c such that σ Λ2 ( λ ) = c ( 1 − λ L 1 ) ( 1 − λ µ 2 ) − 1 . Moreover , evaluating this in µ 1 gives σ Λ2 ( µ 1 ) = c ( 1 − µ 1 L 1 ) ( 1 − µ 1 µ 2 ) − 1 = 1 , so c = 2 ( 1 − µ 1 L 1 ) ( 1 − µ 1 µ 2 ) ( 157 ) = 2 L 1 µ 2 ( L 1 − µ 1 ) ( µ 2 − µ 1 ) ( 158 ) = 2 (cid:16) µ 1 + L 2 2 (cid:17) 2 − R 2 (cid:16) L 2 − µ 1 2 (cid:17) 2 1 − R 2 4 ( L 2 − µ 1 ) 2 ( 159 ) = 2 ρ 2 − R 2 1 − R 2 . ( 160 ) Then , σ Λ2 ( λ ) = 2 ρ 2 − R 2 1 − R 2 ( 1 − λ L 1 ) ( 1 − λ µ 2 ) − 1 ( 161 ) which can be written σ Λ2 ( λ ) = 2 (cid:18) 1 + m 2 √ m (cid:19) 2 (cid:18) 1 − λ L 1 (cid:19) (cid:18) 1 − λ µ 2 (cid:19) − 1 ( 162 ) with (cid:16) 1 + m 2 √ m (cid:17) 2 = ρ 2 − R 2 1 − R 2 . Finally , m = (cid:18) √ ρ 2 − R 2 − √ ρ 2 − 1 √ 1 − R 2 (cid:19) 2 . Theorem 3 . 1 ( Rate factor of HB 2 ( h 0 , h 1 ; m ) ) . Let f ∈ C Λ and consider the cyclical heavy ball method with step - sizes h 0 , h 1 and momentum parameter m . The asymptotic rate factor of Algorithm 1 with cycles of length two is 1 − τ =   √ m if σ ∗ ≤ 1 , √ m (cid:16) σ ∗ + (cid:112) σ 2 ∗ − 1 (cid:17) 12 if σ ∗ ∈ (cid:16) 1 , 1 + m 2 2 m (cid:17) , ≥ 1 ( no convergence ) if σ ∗ ≥ 1 + m 2 2 m , with σ ∗ = max λ ∈ (cid:110) µ 1 , L 1 , µ 2 , L 2 , ( 1 + m ) h 0 + h 1 2 h 0 h 1 (cid:111) ∩ Λ | σ 2 ( λ ) | and σ 2 ( λ ) = 2 (cid:16) 1 + m − λh 0 2 √ m (cid:17) (cid:16) 1 + m − λh 1 2 √ m (cid:17) − 1 . Proof . From Theorem 4 . 8 applied to K = 2 , we immediately have the above result with σ sup = sup λ ∈ Λ (cid:12)(cid:12) (cid:12)(cid:12) 2 (cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 1 2 √ m (cid:19) − 1 (cid:12)(cid:12) (cid:12)(cid:12) . To conclude the proof , we need to prove that the optimal value of | σ Λ2 | can only be reached on (cid:110) µ 1 , L 1 , µ 2 , L 2 , ( 1 + m ) h 0 + h 1 2 h 0 h 1 (cid:111) . Indeed , σ Λ2 being convex , its maximal value can only be reached on { µ 1 , L 2 } . Its minimal value is reached on ( 1 + m ) h 0 + h 1 2 h 0 h 1 . Therefore , over Λ , the minimal value of σ Λ2 is reached on ( 1 + m ) h 0 + h 1 2 h 0 h 1 if the latest belongs to Λ . Otherwise , its minimal value is reached to the closest point in Λ to ( 1 + m ) h 0 + h 1 2 h 0 h 1 , namely , it can be any point of { µ 1 , L 1 , µ 2 , L 2 } . Proposition D . 3 ( Residual polynomial in the robust region ) . Assuming σ Λ2 ( λ ) ≥ − 1 , ∀ λ ∈ Λ , the residual polynomial associated with the cyclical heavy ball algorithm is P 2 n ( λ ) = m n (cid:34) 2 m 1 + m T 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 1 2 √ m (cid:19)(cid:33) + 1 − m 1 + m U 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 1 2 √ m (cid:19)(cid:33)(cid:35) . ( 163 ) 29 Super - Acceleration with Cyclical Step - sizes Remark D . 4 . The assumption σ 2 ( λ ) ≥ − 1 , ∀ λ ∈ Λ is veriﬁed in the robust region , and is useful here because the term (cid:16) 1 + m − λh 0 2 √ m (cid:17) (cid:16) 1 + m − λh 1 2 √ m (cid:17) is equal to 1 + σ 2 ( λ ) 2 and must be positive to make the above expression well deﬁned . Otherwise the result can hold replacing the square root with some complex number , but it brings no value when we derive the convergence rate from it . Proof . This proof reuses elements of the proof of Theorem ( 4 . 8 ) , especially Equation ( 127 ) . For sake of completeness and simplicity , we prove this result again directly in the special case K = 2 . We ﬁrst recall the recursion of Algorithm 1 for K = 2 . For sake of simplicity , we directly projet it onto the eigenspace associated to the eigenvalue λ of the Hessian of the objective function . x 2 n + 1 − x ∗ = ( 1 + m − h 0 λ ) ( x 2 n − x ∗ ) − m ( x 2 n − 1 − x ∗ ) . x 2 n + 2 − x ∗ = ( 1 + m − h 1 λ ) ( x 2 n + 1 − x ∗ ) − m ( x 2 n − x ∗ ) . ( 164 ) Identifying x t − x ∗ = P t ( λ ) ( x 0 − x ∗ ) and P t ( λ ) = ( √ m ) t ˜ P t ( λ ) , ˜ P 2 n + 1 ( λ ) = 1 + m − h 0 λ √ m ˜ P 2 n ( λ ) − ˜ P 2 n − 1 ( λ ) , ˜ P 2 n + 2 ( λ ) = 1 + m − h 1 λ √ m ˜ P 2 n + 1 ( λ ) − ˜ P 2 n ( λ ) . ( 165 ) Multiplying the ﬁrst equation by 1 + m − h 1 λ √ m and replacing 1 + m − h 1 λ √ m ˜ P 2 n + 1 ( λ ) and 1 + m − h 1 λ √ m ˜ P 2 n − 1 ( λ ) accordingly to the second equation leads to ˜ P 2 n + 2 ( λ ) + ˜ P 2 n ( λ ) = 1 + m − h 0 λ √ m 1 + m − h 1 λ √ m ˜ P 2 n ( λ ) − (cid:16) ˜ P 2 n ( λ ) + ˜ P 2 n − 2 ( λ ) (cid:17) ( 166 ) which can be written as in equation ( 127 ) ˜ P 2 n + 2 ( λ ) = (cid:18) 1 + m − h 0 λ √ m 1 + m − h 1 λ √ m − 2 (cid:19) ˜ P 2 n ( λ ) − ˜ P 2 n − 2 ( λ ) . ( 167 ) Moreover , x 1 − x ∗ = ( 1 − h 0 1 + m λ ) ( x 0 − x ∗ ) , x 2 − x ∗ = ( 1 + m − h 1 λ ) ( x 1 − x ∗ ) − m ( x 0 − x ∗ ) , ( 168 ) leading to the initialization ˜ P 1 ( λ ) = 1 √ m ( 1 − h 0 1 + m λ ) ˜ P 0 ( λ ) , ˜ P 2 ( λ ) = 1 + m − h 1 λ √ m ˜ P 1 ( λ ) − ˜ P 0 ( λ ) . ( 169 ) hence , ˜ P 2 ( λ ) = (cid:18) 1 1 + m 1 + m − h 0 λ √ m 1 + m − h 1 λ √ m − 1 (cid:19) ( 170 ) and recall ˜ P 0 ( λ ) = 1 . ( 171 ) It remains to notice that 2 m 1 + mT 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 0 2 √ m (cid:19)(cid:33) + 1 − m 1 + mU 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 0 2 √ m (cid:19)(cid:33) ( 172 ) veriﬁes the same recursion as well as the same initialization for n = 0 and n = 1 . This allows us to identify the 2 sequences of polynomials ˜ P 2 n ( λ ) = 2 m 1 + mT 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 0 2 √ m (cid:19)(cid:33) + 1 − m 1 + m U 2 n (cid:32)(cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 0 2 √ m (cid:19)(cid:33) ( 173 ) 30 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa which concludes the proof . Corollary 3 . 2 . The non - asymptotic and asymptotic worst - case rates r Alg . 2 t and 1 − τ Alg . 2 of Algorithm 2 over C Λ for even iteration number t are r Alg . 2 t = (cid:16) √ ρ 2 − R 2 − √ ρ 2 − 1 √ 1 − R 2 (cid:17) t (cid:16) 1 + t (cid:113) ρ 2 − 1 ρ 2 − R 2 (cid:17) , 1 − τ Alg . 2 = (cid:112) ρ 2 − R 2 − (cid:112) ρ 2 − 1 √ 1 − R 2 . Proof . From Proposition 4 . 5 , Algorithm 2’s parameter make σ ( λ ; { h i } , m ) = σ Λ2 . In particular , by deﬁnition , − 1 ≤ 2 (cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 1 2 √ m (cid:19) − 1 ≤ 1 . ( 174 ) and then 0 ≤ (cid:115)(cid:18) 1 + m − λh 0 2 √ m (cid:19) (cid:18) 1 + m − λh 0 2 √ m (cid:19) ≤ 1 . ( 175 ) And we know that ∀ x ≤ 1 , T n ( x ) ≤ 1 and U n ( x ) ≤ n + 1 . Therefore , using optimal parameters , and from Proposition D . 3 ˜ P 2 n ( λ ) ≤ 2 m 1 + m + ( 2 n + 1 ) 1 − m 1 + m = 1 + 2 n 1 − m 1 + m . ( 176 ) And the worst - case rate is then upper bounded r t = (cid:18) 1 + t 1 − m 1 + m (cid:19) (cid:0) √ m (cid:1) t ( 177 ) for all t even . It remains to plug m expression into the above to conclude . Note that in the proof above , all the expressions are symmetric in ( h 0 , h 1 ) , which implies that swapping those 2 step - sizes doesn’t impact this statement . Remark D . 5 . The previous statement provides the convergence rate of Algorithm 2 . It does not state that this is the optimal way to tune Algorithm 1 , but comparing the obtained rate to the one of Line 6 does . Another way to derive the optimal parameters , is to start from the result of Theorem 3 . 1 applied on a 2 step - sizes strategy , or from the result of Proposition D . 3 . This leads to minimizing m under the constraints that ζ ( λ ) (cid:44) (cid:16) 1 + m − λh 0 2 √ m (cid:17) (cid:16) 1 + m − λh 1 2 √ m (cid:17) has values between 0 and 1 on Λ = [ µ 1 , L 1 ] ∪ [ µ 2 , L 2 ] . By symmetry of Λ and the convex parabola ζ , we know that optimal parameters verify ζ ( L 1 ) = ζ ( µ 2 ) = 0 . And therefore , ζ ( µ 1 ) = ζ ( L 2 ) = 1 maximizes the range of allowed m . This way we recover the tuning of Algorithm 2 . Note that ζ is related to σ ( Λ ) 2 through the relation σ ( Λ ) 2 = 2 ζ − 1 , and therefore the 4 mentioned equalities are equivalent to the equioscillation property . The next theorem sums up the results of Proposition 3 . 3 and Table 1 . Theorem D . 6 ( Asymptotic speedup of HB with alternating step - sizes ) . 1 . Let R ∈ [ 0 , 1 ) be a ﬁxed number , then √ m = κ → 0 1 − 2 √ κ √ 1 − R 2 + o ( √ κ ) . 2 . Let R ( κ ) = κ → 0 1 − √ κ 2 + o ( √ κ ) , i . e . , Λ ≈ [ µ , µ + √ µL 4 ] ∪ [ L − √ µL 4 , L ] , then √ m = κ → 0 1 − 2 4 √ κ + o ( 4 √ κ ) , therefore leasing to a new square root acceleration . 31 Super - Acceleration with Cyclical Step - sizes 3 . Let R ( κ ) = κ → 0 1 − 2 γκ + o ( κ ) , i . e . , Λ ≈ [ µ , ( 1 + γ ) µ ] ∪ [ L − γµ , L ] , then √ m = κ → 0 (cid:114) 1 + 1 γ − (cid:114) 1 γ + o ( κ ) , therefore leading to a constant complexity . This is summed up in the Table 2 . Relative gap R Set Λ Rate factor τ Speedup τ / τ PHB R ∈ [ 0 , 1 ) [ µ , µ + 1 − R 2 ( L − µ ) ] ∪ [ L − 1 − R 2 ( L − µ ) , L ] 2 √ κ √ 1 − R 2 ( 1 − R 2 ) − 12 R = 1 − √ κ / 2 [ µ , µ + √ µL 4 ] ∪ [ L − √ µL 4 , L ] 2 4 √ κ κ − 14 R = 1 − 2 γκ [ µ , ( 1 + γ ) µ ] ∪ [ L − γµ , L ] indep . of κ O ( κ − 12 ) Table 2 : Case study of the convergence of Algorithm 2 as a function of R , in the regime where κ → 0 . The ﬁrst line corresponds to a situation where R is independent of κ , and we observe a constant gain w . r . t . heavy ball . The second line study a setting in which R depends on √ κ , meaning the two intervals in Λ are relatively small . The asymptotic rate reads ( 1 − 2 4 √ κ ) t , beating the ( 1 − 2 √ κ ) t lower bound . Finally , in the third line , R depends on κ , the two intervals in Λ are so small that the convergence becomes O ( 1 ) , i . e . , is independent of κ . Proof . 1 . Let R ∈ [ 0 , 1 ) . The momentum m satisﬁes √ m = κ → 0 (cid:112) 1 + O ( κ ) − R 2 − (cid:112) 4 κ + O ( κ 2 ) √ 1 − R 2 = κ → 0 √ 1 − R 2 + O ( κ ) − 2 √ κ + O ( κ ) √ 1 − R 2 = κ → 0 1 − 2 √ κ √ 1 − R 2 + O ( κ ) . 2 . Let R ( κ ) = κ → 0 1 − √ κ 2 + o ( √ κ ) . The momentum m veriﬁes √ m = (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − R 2 1 − R 2 − (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 + 1 − (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 . We ﬁrst focus on (cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = κ → 0 4 κ + O ( κ 2 ) √ κ + o ( √ κ ) = κ → 0 4 √ κ + o ( √ κ ) . 32 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Then , √ m = (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 + 1 − (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = κ → 0 (cid:113) 1 + 4 √ κ + o ( √ κ ) − (cid:113) 4 √ κ + o ( √ κ ) = κ → 0 1 + 2 √ κ + o ( √ κ ) − 2 4 √ κ + o ( 4 √ κ ) = κ → 0 1 − 2 4 √ κ + o ( 4 √ κ ) . 3 . Let R ( κ ) = κ → 0 1 − 2 γκ + o ( κ ) . The momentum m veriﬁes √ m = (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − R 2 1 − R 2 − (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = (cid:118)(cid:117) (cid:117)(cid:116) (cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 + 1 − (cid:118)(cid:117) (cid:117)(cid:116) (cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 . We ﬁrst focus on (cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = κ → 0 4 κ + O ( κ 2 ) 4 γκ + o ( κ ) = κ → 0 1 γ + o ( κ ) . Then , √ m = (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 + 1 − (cid:118)(cid:117)(cid:117)(cid:116)(cid:16) 1 + κ 1 − κ (cid:17) 2 − 1 1 − R 2 = κ → 0 (cid:114) 1 + 1 γ + o ( κ ) − (cid:114) 1 γ + o ( κ ) = κ → 0 (cid:114) 1 + 1 γ − (cid:114) 1 γ + o ( κ ) . D . 4 Example : 3 cycling step - sizes Proposition D . 7 . The strategy with 3 step - sizes is optimal on the union of two intervals if and only if they are of the form (cid:20) µ , µ + ( L − µ ) (cid:18) 1 2 − R 2 + 1 − R 2 4 (cid:19)(cid:21) ∪ (cid:20) L − ( L − µ ) (cid:18) 1 2 − R 2 − 1 − R 2 4 (cid:19) , L (cid:21) , for some R ∈ [ 0 , 1 ] . Proof . From Theorem C . 2 , we know that T n ( σ 3 ) is optimal for all n if and only if , Λ is the union of 3 diﬀerent intervals that are mapped on [ − 1 , 1 ] . Since , we are looking for Λ being the union of 2 intervals , we know 2 of the 3 intervals Λ is composed of share an extremity . Recall Λ = [ µ 1 , L 1 ] ∪ [ µ 2 , L 2 ] . By symmetry , we can assume without loss of generality that [ µ 1 , L 1 ] is mapped to [ − 1 , 1 ] twice , and [ µ 2 , L 2 ] once . Let’s then introduce x ∈ ( µ 1 , L 1 ) and say : σ 3 ( µ 1 ) = 1 , ( 178 ) σ 3 ( x ) = − 1 , ( 179 ) σ 3 ( L 1 ) = 1 , ( 180 ) σ 3 ( µ 2 ) = 1 , ( 181 ) σ 3 ( L 2 ) = − 1 . ( 182 ) 33 Super - Acceleration with Cyclical Step - sizes Note we also know that x is a local minima of σ 3 , leading to σ (cid:48) 3 ( x ) = 0 . We now know 3 roots of σ 3 + 1 and 3 roots of σ 3 − 1 , leading to : σ 3 ( λ ) − 1 = c ( λ − µ 1 ) ( λ − L 1 ) ( λ − µ 2 ) , ( 183 ) σ 3 ( λ ) + 1 = c ( λ − x ) 2 ( λ − L 2 ) , ( 184 ) for some non - zero constant c . Here , we want to remove the dependency in x or c . Using the two equalities above , ( λ − x ) 2 ( λ − L 2 ) − ( λ − µ 1 ) ( λ − L 1 ) ( λ − µ 2 ) = 2 c . ( 185 ) Matching the coeﬃcients of the above polynomial leads to 2 x + L 2 = µ 1 + L 1 + µ 2 ( 186 ) and ( 187 ) 2 xL 2 + x 2 = µ 1 L 1 + µ 1 µ 2 + L 1 µ 2 . ( 188 ) We plug the expression of x we get from the ﬁrst equality into the second one , L 2 ( µ 1 + L 1 + µ 2 − L 2 ) + (cid:18) µ 1 + L 1 + µ 2 − L 2 2 (cid:19) 2 = µ 1 L 1 + µ 1 µ 2 + L 1 µ 2 . ( 189 ) From here , for simplicity , we deﬁne r i (cid:44) L i − µ i L 2 − µ 1 , for i ∈ { 1 , 2 } . ( 190 ) Replacing L 1 and µ 2 by their expression using µ 1 , L 2 , r 1 and r 2 leads to r 1 = 2 √ r 2 − r 2 . ( 191 ) The reciprocal holds and we can ﬁnd x using Equation ( 186 ) or ( 188 ) . Note if Equation ( 191 ) holds , we can directly express σ 3 as the unique polynomial verifying σ 3 ( µ 1 ) = 1 , ( 192 ) σ 3 ( L 1 ) = 1 , ( 193 ) σ 3 ( µ 2 ) = 1 , ( 194 ) σ 3 ( L 2 ) = − 1 . ( 195 ) We can therefore conclude σ 3 ( λ ) = 1 − 2 ( λ − µ 1 ) ( λ − L 1 ) ( λ − µ 2 ) ( L 2 − µ 1 ) ( L 2 − L 1 ) ( L 2 − µ 2 ) . ( 196 ) From the new notations r 1 , r 2 , µ = µ 1 , L = L 2 , we know T n ( σ Λ3 ) is optimal for all n if and only if Λ = [ µ , µ + r 1 ( L − µ ) ] ∪ [ L − r 2 ( L − µ ) , L ] . ( 197 ) Let R be R = µ 2 − L 1 L 2 − µ 1 ( 198 ) as in the 2 step - sizes setting . Here , we have R = 1 − r 1 − r 2 and we assume r 1 = 2 √ r 2 − r 2 . Combining those 2 equalities gives : r 1 = 1 2 − R 2 + 1 − R 2 4 , ( 199 ) r 2 = 1 2 − R 2 − 1 − R 2 4 , ( 200 ) 34 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa leading to the desired result , i . e . , Λ = [ µ , µ + ( L − µ ) ( 1 2 − R 2 + 1 − R 2 4 ) ] ∪ [ L − ( L − µ ) ( 1 2 − R 2 − 1 − R 2 4 ) , L ] . Theorem D . 8 ( Asymptotic speedup of heavy ball when cycling over 3 step - sizes ) . Let R ∈ [ 0 , 1 ) be a ﬁxed number , then √ m = κ → 0 1 − 2 √ κ (cid:114) 1 − R 2 / 9 1 − R 2 + o ( √ κ ) . ( 201 ) Proof . From Equation ( 145 ) , √ m = (cid:18) σ ( Λ ) 3 ( 0 ) − (cid:113) σ ( Λ ) 3 ( 0 ) 2 − 1 (cid:19) 13 with σ ( Λ ) 3 ( 0 ) = 1 + 2 µ 1 L 1 µ 2 ( L 2 − µ 1 ) ( L 2 − L 1 ) ( L 2 − µ 2 ) . Using the previous notations , µ = µ 1 , ( 202 ) L = L 2 , ( 203 ) κ = µ L , ( 204 ) r i (cid:44) L i − µ i L 2 − µ 1 , for i ∈ { 1 , 2 } , ( 205 ) we can write σ ( Λ ) 3 as σ ( Λ ) 3 ( 0 ) = 1 + 2 µ 1 L 1 µ 2 ( L 2 − µ 1 ) ( L 2 − L 1 ) ( L 2 − µ 2 ) , ( 206 ) = 1 + 2 κ ( κ + r 1 ( 1 − κ ) ) ( 1 − r 2 ( 1 − κ ) ) ( 1 − κ ) 3 ( 1 − r 1 ) r 2 , ( 207 ) = κ → 0 1 + 2 κ r 1 ( 1 − r 2 ) ( 1 − r 1 ) r 2 , ( 208 ) = 1 + 2 κ (cid:16) 12 − R 2 + 1 − R 2 4 (cid:17) (cid:16) 12 + R 2 − 1 − R 2 4 (cid:17) (cid:0) 12 + R 2 + 1 − R 2 4 (cid:1) (cid:0) 12 − R 2 − 1 − R 2 4 (cid:1) , ( 209 ) = 1 + 2 κ 9 − 10 R 2 + R 4 1 − 2 R 2 + R 4 , ( 210 ) = 1 + 2 κ (cid:0) 1 − R 2 (cid:1) (cid:0) 9 − R 2 (cid:1) ( 1 − R 2 ) 2 , ( 211 ) = 1 + 2 κ 9 − R 2 1 − R 2 . ( 212 ) Then introducing brieﬂy ε (cid:44) κ 9 − R 2 1 − R 2 → κ → 0 0 , √ m = (cid:18) σ ( Λ ) 3 ( 0 ) − (cid:113) σ ( Λ ) 3 ( 0 ) 2 − 1 (cid:19) 13 , ( 213 ) = (cid:16) 1 + 2 ε − (cid:112) 1 + 4 ε + 4 ε 2 − 1 (cid:17) 13 , ( 214 ) = κ → 0 1 − 2 3 √ ε + o ( √ ε ) . ( 215 ) 35 Super - Acceleration with Cyclical Step - sizes Plugging ε expression into the latest gives √ m = κ → 0 1 − 2 √ κ (cid:114) 1 − R 2 / 9 1 − R 2 + o ( √ κ ) . ( 216 ) E Beyond quadratic objective : local convergence of cycling methods In this section , we prove a result of local convergence of the cyclical heavy ball method out of quadratic setting . We ﬁrst recall the Theorem 5 . 1 stated in Section 5 : Theorem 5 . 1 ( Local convergence ) . Let f : R d (cid:55)→ R be a twice continuously diﬀerentiable function , x ∗ a local minimizer , and H be the Hessian of f at x ∗ with Sp ( H ) ⊆ Λ . Let x t denote the result of running Algorithm 1 with parameters h 1 , h 2 , · · · , h K , m , and let 1 − τ be the linear convergence rate on the quadratic objective ( OPT ) . Then we have ∀ ε > 0 , ∃ open set V ε : x 0 , x ∗ ∈ V ε = ⇒ (cid:107) x t − x ∗ (cid:107) = O ( ( 1 − τ + ε ) t ) (cid:107) x 0 − x ∗ (cid:107) . ( 24 ) Proof . For any k multiple of K , consider S k the operator applying k steps of cycling Heavy Ball on the iterates x t and x t − 1 ( note since k is a multiple of K , Algorithm 1 consists in repeating the operator S k ) . Namely S k is an operator on R 2 d verifying S k ( ( x t , x t − 1 ) ) = ( x t + k , x t + k − 1 ) . This operator is a composition of gradients of f and aﬃne functions , and so it is continuously diﬀerentiable . Applying the mean value theorem along each coordinate of S k , we have that there exists a matrix - valued function M ( v 1 , v 2 ) for all v 1 , v 2 in the domain of S k such that S k ( v 1 ) − S k ( v 2 ) = M ( v 1 , v 2 ) ( v 1 − v 2 ) , ( 217 ) where the i th rows of M ( v 1 , v 2 ) is the gradient of the i th output of S k evaluated at a vector on the segment between v 1 and v 2 . M ( v 1 , v 2 ) =   ∇ ( S k ) 1 ( w 1 ) T . . . ∇ ( S k ) i ( w i ) T . . . ∇ ( S k ) 2 d ( w 2 d ) T   where ∀ i ∈ (cid:74) 1 , 2 d (cid:75) ,   ( S k ) i denotes the i th coordinate of S k . w i is a point on the segment [ v 1 , v 2 ] . ( 218 ) By continuity of those gradients , taking v 1 and v 2 suﬃciently close to ( x ∗ , x ∗ ) , M ( v 1 , v 2 ) can be chosen arbitrarily close to the Jacobian of S k in ( x ∗ , x ∗ ) denoted by JS ∗ k . Since by assumption the algorithm converges on the quadratic form induced by H at the rate 1 − τ , we conclude that the spectral radius of JS ∗ k is upper bounded by 1 − τ . From the previous point , we can ﬁnd a small enough neighborhood of ( x ∗ , x ∗ ) such that M ( v 1 , v 2 ) has a spectral radius arbitrarily close to 1 − τ , in particular smaller than 1 . Furthermore , it’s known for any ε > 0 , there exists an operator norm (cid:107) . (cid:107) such that (cid:107) M ( v 1 , v 2 ) (cid:107) < 1 − τ + ε . ( see e . g . ( Bertsekas , 1997 , Proposition A . 15 ) ) . Hence , for any ε > 0 , there exists a neighborhood V of ( x ∗ , x ∗ ) and an operator norm (cid:107) . (cid:107) as described above such that S k is a ( 1 − τ + ε ) - contraction on V for the norm (cid:107) . (cid:107) . This leads to convergence to the only ﬁxed point ( x ∗ , x ∗ ) with a convergence rate smaller than any 1 − τ + ε . Moreover , the ﬁrst step of the Algorithm 1 is continuous with respect to x 0 . Hence , for any V ∈ R 2 d neighborhood of ( x ∗ , x ∗ ) , there exists W ∈ R d a neighborhood of x ∗ , such that x 0 ∈ W = ⇒ ( x 1 , x 0 ) ∈ V . ( 219 ) 36 Goujaud , Scieur , Dieuleveut , Taylor , Pedregosa Finally , for any ε > 0 , there exists W a neighborhood of x ∗ such that the Algorithm 1 converges to x ∗ with a rate smaller than 1 − τ + ε . F Experimental setup Benchmarks we run using a Google colab public instance with a single CPU . Producing the results of Figure 4 took 50 minutes with this setup . The code to reproduce this ﬁgure is attached with the supplementary material in the jupyter notebook benchmarks . ipynb . G Comparison with Oymak ( 2021 ) The work of Oymak ( 2021 ) also exploits cyclical step - sizes for when the spectral structure of the Hessian contains a gap . This work appeared concurrently to the ﬁrst version of this manuscript and takes a somewhat diﬀerent stand for exploiting this particular spectral structure . We summarize the main diﬀerences in the table below . Oymak ( 2021 ) This work Algorithm Gradient descent Heavy ball - type ( optimal algorithm ) Cycle length K K is a function of the spectral assumptions K is a choice Structure of the cycle η + , . . . , η + (cid:124) (cid:123)(cid:122) (cid:125) K − 1 times , η − ( Oymak , 2021 , Deﬁnition 1 ) ( h 0 , . . . , h K − 1 ) ( See Algorithm 1 ) Optimal among chosen scheme Not proven / discussed Yes Spectral assumption Bimodal ( 2 intervals ) Any number of intervals Spectral assumption in bimodal case Rate depends on L 2 µ 2 and L 1 µ 1 Rate depends on L 2 − µ 2 and L 1 − µ 1 Typical application case in bimodal Very strong assumption on L 1 − µ 1 and very weak assumption on L 2 − µ 2 . Weak assumption on both L 2 − µ 2 and L 1 − µ 1 , more in line with em - pirical observations ( Papyan , 2018 ; Ghorbani et al . , 2019 ) . Spectrum is a single in - terval Does not recover original rate Recovers rate and optimal method Convergence beyond quadratic objectives Yes ( with extra assumptions ) Local convergence We emphasize the following points . • While we provide Theorem 4 . 8 which described the convergence rate of any cycling heavy ball ( for any cycle ) , Oymak ( 2021 ) only studied gradient descent method ( without momentum ) for a particular cycle ( for which cycle length is not a parameter , but ﬁxed by the eigenvalues ) . • Moreover , our Theorem 4 . 9 provides the optimal cycle to use for any choice of a cycle length , while optimality is not discussed in Oymak ( 2021 ) and the cycle uses only two diﬀerent step - sizes , which is somewhat arbitrary . • Furthermore , our work highlights acceleration under assumptions that seem more aligned with empirical 37 Super - Acceleration with Cyclical Step - sizes observation : Oymak ( 2021 ) shows that L 1 − µ 1 needs to be very small for his strategy to be worthwhile , while this is not really the case in our experiments ( see Figure 1 ) • Finally , in the general case in which we cannot assume any gap in the spectrum , we naturally recover the classical optimal method and rate . This is not the case in Oymak ( 2021 ) which is suboptimal in this setup . In this work , we focus on quadratic minimization and give some local convergence guarantee beyond quadratics . On the other hand , Oymak ( 2021 ) provides guarantee beyond this setup , at the cost of very restrictive assumptions . 38