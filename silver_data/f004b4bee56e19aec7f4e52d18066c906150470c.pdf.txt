Peer and Self Assessment in Massive Online Classes Chinmay Kulkarni , Koh Pang Wei , Huy Le , Daniel Chia , Kathryn Papadopoulos , Justin Cheng , Daphne Koller , Scott R . Klemmer Stanford University , Coursera Inc . , and UC San Diego Peer assessment Classmates assess each other Provide summative evaluation , and constructive criticism Peer assessment used in 100 + classes Teaching character Management Constitutional law Arguments Introduction to Philosophy Essays Social Psychology Essays Programming in Python Code Human - computer Interaction Design Child Nutrition Recipes World Music Music Our peer assessment process 1 ) Practice 2 ) Assess 5 Peers 3 ) Self - Assess ✓ sta ﬀ - graded Provides students grades and improvement - oriented feedback 59 % of student submissions get grades within 10 % of sta ﬀ grades In this talk • Why peer assessment is necessary • How peers assess open - ended work Peer assessment compares well with sta ﬀ grade ( r = 0 . 73 ) • How peers improve open - ended work Q ualitative feedback + opportunities to reﬂect • How data improves peer assessment Open - ended assignments are pedagogically valuable • Closed ended questions • constrain choices • Recognizing a correct solution does not mean you can generate it • Open ended assignments • Require students to generate solutions , not only recognize them • Can assess on more realistic tasks • Embrace multiple solutions Veloski et al 1999 Thompson et al 2000 Challenges of Open - ended assessment • Realistic , open - ended assignments require lots of grading time • sta ﬀ grading takes prohibitive labor ( 400 + hours / week ) • Machine grading reliant on lexical and syntactical features not robust enough • Though sharing work provides inspiration and encourages discussion , students don’t see others’ work The paradox of peer processes Non - experts performing expert work In - person classes • Peer grades correlate well with sta ﬀ • Peers can provide constructive criticism Does this scale to global online classes ? Falchikov and Goldﬁnch ( 2000 ) Sluijsmans et al . ( 2002 ) Kulkarni & Klemmer ( 2012 ) Tinapple et al ( 2013 ) Our peer assessment process 1 ) Practice 2 ) Assess 5 Peers 3 ) Self - Assess ✓ sta ﬀ - graded Similar to CPR but calibrate students , not the algorithm . Final peer grade is a simple median , not a calibrated , weighted mean . Carlson & Berry ( 2003 ) HCI Online Subtitle Text Free ; anyone can enroll . Open - ended assignments central to class , 6 - 7 hours / week In all , 65711 students watched videos , 5 , 876 students submitted open - ended assignments Assignments step through a human - centered design process Needﬁnding and low - ﬁ prototypes Implementation plan Functional prototypes User testing and iteration 1 ) Practice 2 ) Assess 5 Peers 3 ) Self - Assess ✓ sta ﬀ - graded Median peer grades correlate with sta ﬀ 0 5 10 15 20 − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 Median grade minus staff grade ( % of total ) S i m u l a t ed pe r c en t age Mean Sta ﬀ variation 6 . 7 % Median peer grades correlate with sta ﬀ 0 5 10 15 20 − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 Median grade minus staff grade ( % of total ) S i m u l a t ed pe r c en t age First two runs of HCI class 36 % within 5 % Mean Sta ﬀ variation 6 . 7 % Median peer grades correlate with sta ﬀ 59 % within 10 % 0 5 10 15 20 − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 Median grade minus staff grade ( % of total ) S i m u l a t ed pe r c en t age First two runs of HCI class 36 % within 5 % Mean Sta ﬀ variation 6 . 7 % Grading adequate for pass - fail class • Errors are nearly symmetric around sta ﬀ grades Summing grades on all assignments in a simulation 1096 66 24 Correctly cert No cert by grading error Cert by grading error Assessing others yields perspective and inspiration Chinn ( 2005 ) ; Tinapple et al . ( 2013 ) “… seeing how others tackled the assignments sometimes helped me point out things I missed and o ﬀ ered some perspective…” 114 similar responses “… Giving me a wider point of view based on the others work…” 36 similar responses Consistent with in - person e ﬀ ects Assessing yourself after peers yields reﬂection and comparison Boud ( 1994 ) “…Was nice to evaluate my own work AFTER evaluat [ ing ] others because I could compare my work and e ﬀ ort…” 175 similar responses “… helped me see problems that i didn ' t see before the peer evaluation…” 50 similar responses Consistent with in - person e ﬀ ects More peers yield quickly diminishing returns Heimerl et al . 2012 Nielsen 1994 S i m u l a t e d p e r c en t ag e o f s u b m i ss i o n s 0 18 35 53 70 Number of raters 1 3 5 7 9 11 26 . 6 34 . 4 37 . 0 38 . 6 39 . 8 40 . 9 41 . 4 57 . 2 60 . 4 62 . 1 63 . 1 64 . 1 % within 10 % of sta ﬀ % within 5 % of sta ﬀ Feedback on grading improves accuracy • n = 756 • Between subjects feedback v . no feedback Grading Feedback reduced error • 7 . 74 % for control to 6 . 77 % with feedback [ t ( 4998 ) = 3 . 38 , p < 0 . 01 ] • Students appreciated receiving feedback • Learning beneﬁt : knowing how to appraise work right builds conﬁdence • Helps build shared norms Qualitative feedback Improvement - oriented feedback beyond the rubric Some feedback minimal / superﬁcial Minimal “Great idea ! ” Superﬁcial “I can ' t read the words in the pics clearly” Better “Solution requirement is vague here but I ' m excited to see where you take this in the storyboards ! ” The return of the novices - as - experts paradox “fully interactive , page ﬂow is complete… make it clearer what people should do next” Experts : capture the structure of rubric Peers : Focus on superﬁcial features , even when asked not to “unpolished…Try to make UI less coloured . ” Fortune cookies for qualitative , personalized feedback • Peers can recognize errors from a list of patterns , even if they can’t articulate them • Most errors are variations on a theme + “ . . . because _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ” Cue Variation Encouraging Richer Feedback Copy , then paste } Make the prototype more interactive so the user test represents a more real - life interaction : The prototype does everything you ' re testing , but it couldn ' t hurt to make it more interactive . If the user can ' t possibly stray from the things you want to test , how do you know that the user can actually use the full application without making mistakes ? Fortune cookies feedback actionable and detailed • “Clarify the concerns , goals , and expectations of the user tests : try to expand the information in the implementation plan” • “Prototype should relate to the user needs in the storyboard more . I dont see the proposed functionallity from the storyboards here in the prototypes . ” Improving assessment Using data on peer - sta ﬀ disagreement • Which rubric items have the most disagreement ? • Rubric analysis code available on : hci . st / assess Assignment 5 : Test results Assignment 5 : Test process Assignment 5 : Test photos Assignment 5 : Test Changes Assignment 5 : Implement Redesign Assignment 5 : Alternative Redesign Assignment 4 : UserTest Complete Assignment 4 : UserTest Appropriate Assignment 4 : Goals Assignment 4 : Functionality Assignment 3 : Navigation Skeleton Assignment 3 : Implementation Plan Assignment 3 : Heuristic Evaluation Assignment 3 : Deadlines Assignment 2 : Storyboard2 Assignment 2 : Storyboard1 Assignment 2 : Prototype2 Assignment 2 : Prototype1 Assignment 2 : Point of View − 3 − 2 − 1 0 1 2 3 Median peer grade minus Staff grade Q ue s t i on Low - ﬁ prototyping Implementation Plan Functionality Test Plan & Test results Iteration & Point of view Prototype 1 Prototype 2 Storyboard 1 Deadlines Heuristic evaluationWeekly plan Navigation skeleton Functionality Test appropriateness Test completeness Alternate redesign Redesign complete Test changes Test photos Test process Storyboard 2 Goals Test results • Which rubric items have the most disagreement ? • Rubric analysis code available on : hci . st / assess Assignment 5 : Test results Assignment 5 : Test process Assignment 5 : Test photos Assignment 5 : Test Changes Assignment 5 : Implement Redesign Assignment 5 : Alternative Redesign Assignment 4 : UserTest Complete Assignment 4 : UserTest Appropriate Assignment 4 : Goals Assignment 4 : Functionality Assignment 3 : Navigation Skeleton Assignment 3 : Implementation Plan Assignment 3 : Heuristic Evaluation Assignment 3 : Deadlines Assignment 2 : Storyboard2 Assignment 2 : Storyboard1 Assignment 2 : Prototype2 Assignment 2 : Prototype1 Assignment 2 : Point of View − 3 − 2 − 1 0 1 2 3 Median peer grade minus Staff grade Q ue s t i on Low - ﬁ prototyping Implementation Plan Functionality Test Plan & Test results Iteration & Point of view Prototype 1 Prototype 2 Storyboard 1 Deadlines Heuristic evaluationWeekly plan Navigation skeleton Functionality Test appropriateness Test completeness Alternate redesign Redesign complete Test changes Test photos Test process Storyboard 2 Goals Test results Separating orthogonal questions improves agreement Orthogonal attributes combined Did the student upload interesting photos ? Orthogonal attributes separated 1 . Did the student upload photos ? 2 . Were photos interesting ? 4 % better agreement Parallellizing rubric cells improves agreement I point 3 points Non - parallel structure The storyboards are hard to follow or do not address the point of view . The storyboards reasonably address the point of view , and are reasonably easy to understand Parallellizing rubric cells improves agreement 8 % better agreement I point 3 points Parallel structure The storyboards are hard to follow or do not address the point of view . The storyboards are easy to follow and reasonably address the point of view Revising rubrics improves agreement 42 % within 5 % 0 5 10 15 20 − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 Median grade minus staff grade ( % of total ) S i m u l a t ed pe r c en t a ge Peer - sta ﬀ agreement increases for the median submission within 5 % improves 34 % - > 42 % Peer - sta ﬀ agreement outliers reduced 80 % of submissions within 20 % - > 15 % of sta ﬀ Revising rubrics improves agreement 42 % within 5 % 0 5 10 15 20 − 50 − 40 − 30 − 20 − 10 0 10 20 30 40 50 Median grade minus staff grade ( % of total ) S i m u l a t ed pe r c en t age Peer - sta ﬀ agreement increases for the median submission within 5 % improves 34 % - > 42 % Peer - sta ﬀ agreement outliers reduced 80 % of submissions within 20 % - > 15 % of sta ﬀ Peer processes can provide students deeper feedback , improve motivation and learning ( And do so at scale ) Our vision The 7 habits of highly successful peer assessment • Assignment - speciﬁc rubrics • Iterate before release ( pre and during ) • Assignment - speciﬁc training • Self assessment at the end • Sta ﬀ grades as ground truth • Adaptive grade aggregation • Provide feedback to graders http : / / hci . st / assess Machine grading ? Can algorithms reduce busywork and amplify peer processes ? Piech et al ( 2013 ) Kulkarni et al ( 2014 ) http : / / hci . st / assess Coming up L @ S : Scaling Shor t - answer Grading by Combining Peer Assessment with Algorithmic Scoring CHI wor kshop Peer and Self Assessment in Massive Online Classes Chinmay Kulkarni , Koh Pang Wei , Huy Le , Daniel Chia , Kathryn Papadopoulos , Justin Cheng , Daphne Koller , Scott R . Klemmer Back up , back up ! References • Guzdial , M . , and Turns , J . Effective discussion through a computer - mediated anchored forum . The Journal of the Learning Sciences 9 , 4 ( 2000 ) , 437 – 469 . • Blitzer , D . The wide world of computer - based education . Advances in computers 15 ( 1976 ) , 239 – 283 . • CommentSpace : Structured Support for Collaborative Visual Analysis Wesley Willett , Jeffrey Heer , Joseph Hellerstein , Maneesh Agrawala , ACM Human Factors in Computing Systems ( CHI ) , 2011 • Motivating Participation by Displaying the Value of Contribution . Al Mamunur Rashid , Kimberly Ling , Regina D Tassone , Paul Resnick , Robert Kraut , John Riedl , CHI 2006 • Group Processes in the Classroom , Second Edition . Schmuck , Richard A . ; Schmuck , Patricia A . • Apprenticeship in thinking : Cognitive development in social context . Rogoff , Barbara , Oxford University Press , 1990 • Cooperation in the Classroom . , David W . Johnson , Roger T . Johnson , Edythe Johnson Holubec , Interaction Book Company , 1991 • Managing Cultural Diversity : Implications for Organizational Competitiveness , Taylor H . Cox and Stacy Blake , The Executive , Vol . 5 , No . 3 ( Aug . , 1991 ) , • Faris , O . ( 2009 ) The Impact of Homogeneous vs . Heterogeneous Collaborative Learning Groups in Multicultural Classes on the Achievement and Attitudes of Nine Graders towardsLearning Science . Online Submission , Feb . 2009 . • Ost , Ben ( 2010 ) The role of peers and grades in determining major persistence in the sciences , Economics of Education Review • Veloski et al ( 1999 ) Patients don ' t present with ﬁve choices : an alternative to multiple - choice tests in assessing physicians ' competence Peer and Self Assessment in Massive Online Classes Chinmay Kulkarni , Koh Pang Wei , Huy Le , Daniel Chia , Kathryn Papadopoulos , Justin Cheng , Daphne Koller , Scott R . Klemmer Stanford University , Coursera Inc . , and UC San Diego Coming up L @ S : Scaling Shor t - answer Grading by Combining Peer Assessment with Algorithmic Scoring CHI wor kshop