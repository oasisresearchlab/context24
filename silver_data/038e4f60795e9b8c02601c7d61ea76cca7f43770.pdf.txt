Correlation Clustering Nikhil Bansal (cid:3) Avrim Blum (cid:3) Shuchi Chawla (cid:3) Abstract We consider the following clustering problem : we have a complete graph on n vertices ( items ) , where each edge ( u ; v ) is labeled either + or (cid:0) depending on whether u and v have been deemed to be similar or different . The goal is to produce a partition of the vertices ( a clustering ) that agrees as much as possible with the edge labels . That is , we want a clustering that maximizes the number of + edges within clusters , plus the number of (cid:0) edges between clus - ters ( equivalently , minimizes the number of disagreements : the number of (cid:0) edges inside clusters plus the number of + edges between clusters ) . This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data , and the goal is to partition the current set of documents in a way that cor - relates with f as much as possible ; it can also be viewed as a kind of “agnostic learning” problem . An interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter , as in measures such as k - median or min - sum or min - max clustering . Instead , in our formu - lation , the optimal number of clusters could be any value between 1 and n , depending on the edge labels . We look at approximation algorithms for both minimizing disagree - ments and for maximizing agreements . For minimizing dis - agreements , we give a constant factor approximation . For maximizing agreements we give a PTAS . We also show how to extend some of these results to graphs with edge labels in [ (cid:0) 1 ; + 1 ] , and give some results for the case of random noise . 1 Introduction Suppose that you are given a set of n documents to clus - ter into topics . Unfortunately , you have no idea of what a “topic” is . However , you have at your disposal a classi - ﬁer f ( A ; B ) that given two documents A and B , outputs (cid:3) Department of Computer Science , Carnegie Mellon University . f nikhil , avrim , shuchi g @ cs . cmu . edu . This research was sup - ported in part by NSF grants CCR - 0085982 , CCR - 0122581 , CCR - 0105488 , and an IBM Graduate Fellowship . whether or not it believes A and B are similar to each other . For example , perhaps f was learned from some past train - ing data . In this case , a natural approach to clustering is to apply f to every pair of documents in your set , and then to ﬁnd the clustering that agrees as much as possible with the results . Speciﬁcally , we consider the following problem . Given a fully - connected graph G with edges labeled “ + ” ( similar ) or “ (cid:0) ” ( different ) , ﬁnd a partition of the vertices into clus - ters that agrees as much as possible with the edge labels . In particular , we can look at this in terms of maximizing agreements ( the number of + edges inside clusters plus the number of (cid:0) edges between clusters ) or in terms of mini - mizing disagreements ( the number of (cid:0) edges inside clus - ters plus the number of + edges between clusters ) . These two are equivalent at optimality but , as usual , differ from the point of view of approximation . In this paper we give a constant factor approximation to the problem of minimiz - ing disagreements , and a PTAS for maximizing agreements . We also extend some of our results to the case of real - valued edge weights . This problem formulation is motivated in part by some clustering problems at Whizbang Labs in which learning algorithms have been trained to help with various clustering tasks [ 8 , 9 , 10 ] . 1 What is interesting about the clustering problem deﬁned here is that unlike most clustering formulations , we do not need to specify the number of clusters k as a separate pa - rameter . For example , in k - median [ 7 , 15 ] or min - sum clus - tering [ 20 ] or min - max clustering [ 14 ] , one can always get a perfect score by putting each node into its own cluster — the question is how well one can do with only k clusters . In our clustering formulation , there is just a single objective , 1 An example of one such problem is clustering entity names . In this problem , items are entries taken from multiple databases ( e . g . , think of names / afﬁliations of researchers ) , and the goal is to do a “robust uniq” — collecting together the entries that correspond to the same entity ( per - son ) . E . g . , in the case of researchers , the same person might appear multiple times with different afﬁliations , or might appear once with a middle name and once without , etc . In practice , the classiﬁer f typi - cally would output a probability , in which case the natural edge label is log ( Pr ( same ) / Pr ( different ) ) . This is 0 if the classiﬁer is unsure , positive if the classiﬁer believes the items are more likely in the same cluster , and negative if the classiﬁer believes they are more likely in different clusters . The case of f + ; (cid:0)g labels corresponds to the setting in which the classiﬁer has equal conﬁdence about each of its decisions . Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE and the optimal clustering might have few or many clusters : it all depends on the edge labels . To get a feel for this problem , notice that if there exists a perfect clustering , i . e . , one that gets all the edges correct , then the optimal clustering is easy to ﬁnd : just delete all “ (cid:0) ” edges and output the connected components of the graph remaining . ( This is called the “naive algorithm” in [ 10 ] . ) Thus , the interesting case is when no clustering is perfect . Also , notice that for any graph G , it is trivial to produce a clustering that agrees with at least half of the edge labels : if there are more + edges than (cid:0) edges , then simply put all vertices into one big cluster ; otherwise , put each vertex into its own cluster . This observation means that for maximiz - ing agreements , getting a 2 - approximation is easy ( note : we will show a PTAS ) . In general , ﬁnding the optimal cluster - ing is NP - hard , which can be seen via a tedious reduction from X3C ( details can be found in [ 5 ] ) . Another simple fact to notice is that if the graph contains a triangle in which two edges are labeled + and one is la - beled (cid:0) , then no clustering can be perfect . More generally , the number of edge - disjoint triangles of this form gives a lower bound on the number of disagreements of the optimal clustering . This fact is used in our constant - factor approxi - mation algorithm . For maximizing agreements , our PTAS is quite similar to the PTAS developed by [ 12 ] for MAX - CUT on dense graphs , and related to PTASs of [ 4 , 3 ] . Notice that since there must exist a clustering with at least n ( n (cid:0) 1 ) = 4 agreements , this means it sufﬁces to approximate agree - ments to within an additive factor of (cid:15)n 2 . This problem is also closely related to work on testing graph properties of [ 13 , 19 , 1 ] . In fact , we show how we can use the Gen - eral Partition Property Tester of [ 13 ] as a subroutine to get a PTAS with running time O ( ne O ( ( 1 (cid:15) ) 1 (cid:15) ) ) . Unfortunately , this is doubly exponential in 1 (cid:15) , so we also present an alterna - tive direct algorithm ( based more closely on the approach of [ 12 ] ) that takes only O ( n 2 e O ( 1 (cid:15) ) ) time . Relation to agnostic learning : One way to view this clustering problem is that edges are “examples” ( labeled as positive or negative ) and we are trying to represent the target function f using a hypothesis class of vertex clusters . This hypothesis class has limited representational power : if we want to say ( u ; v ) and ( v ; w ) are positive in this language , then we have to say ( u ; w ) is positive too . So , we might not be able to represent f perfectly . This sort of problem — trying to ﬁnd the ( nearly ) best representation of some arbi - trary target f in a given limited hypothesis language — is sometimes called agnostic learning [ 17 , 6 ] . The observation that one can trivially agree with at least half the edge labels is equivalent to the standard machine learning fact that one can always achieve error at most 1 = 2 using either the all positive or all negative hypothesis . Our PTAS for approximating the number of agreements means that if the optimal clustering has error rate (cid:23) , then we can ﬁnd one of error rate at most (cid:23) + (cid:15) . Our running time is exponential in 1 = (cid:15) , but this means that we can achieve any constant error gap in polynomial time . What makes this in - teresting from the point of view of agnostic learning is that there are very few nontrivial problems where agnostic learn - ing can be done in polynomial time . Even for simple classes such as conjunctions and disjunctions , no polynomial - time algorithms are known that give even an error gap of 1 = 2 (cid:0) (cid:15) . 2 Notation and Deﬁnitions Let G = ( V ; E ) be a complete graph on n vertices , and let e ( u ; v ) denote the label ( + or (cid:0) ) of the edge ( u ; v ) . Let N + ( u ) = f u g [ f v : e ( u ; v ) = + g and N (cid:0) ( u ) = f v : e ( u ; v ) = (cid:0)g denote the positive and negative neighbors of u respectively . We let OPT denote the optimal clustering on this graph . In general , for a clustering C , let C ( v ) be the set of vertices in the same cluster as v . We will use A to denote the clus - tering produced by our algorithms . In a clustering C , we call an edge ( u ; v ) a mistake if ei - ther e ( u ; v ) = + and yet u 62 C ( v ) , or e ( u ; v ) = (cid:0) and u 2 C ( v ) . When e ( u ; v ) = + , we call the mistake a pos - itive mistake , otherwise it is called a negative mistake . We denote the total number of mistakes made by a clustering C by mC , and use mOPT to denote the number of mistakes made by OPT . For positive real numbers x , y and z , we use x 2 y (cid:6) z to denote x 2 [ y (cid:0) z ; y + z ] . Finally , let X for X (cid:18) V denote the complement ( V n X ) . 3 A Constant Factor Approximation for Min - imizing Disagreements We now describe our main algorithm : a constant - factor approximation for minimizing the number of disagree - ments . The high - level idea of the algorithm is as follows . First , we show ( Lemma 1 ) that if we can cluster a portion of the graph using clusters that each look sufﬁciently “clean” ( Deﬁnition 1 ) , then we can charge off the mistakes made within that portion to “erroneous triangles” : triangles with two + edges and one (cid:0) edge . Furthermore , we can do this in such a way that the triangles we charge are nearly edge - disjoint , allowing us to bound the number of these mistakes by a constant factor of OPT . Second , we show ( Lemma 2 ) that there must exist a nearly optimal cluster - ing OPT 0 in which all non - singleton clusters are “clean” . Finally , we show ( Theorem 3 and Lemma 7 ) that we can al - gorithmically produce a clustering of the entire graph con - taining only clean clusters and singleton clusters , such that 2 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE mistakes that have an endpoint in singleton clusters are bounded by OPT 0 , and mistakes with both endpoints in clean clusters are bounded using Lemma 1 . We begin with a deﬁnition of a “clean” cluster and a “good” vertex . Deﬁnition 1 A vertex v is called Æ - good with respect to C , where C (cid:18) V , if it satisﬁes the following : (cid:15) j N + ( v ) \ Cj (cid:21) ( 1 (cid:0) Æ ) jCj (cid:15) j N + ( v ) \ ( V n C ) j (cid:20) Æ jCj If a vertex v is not Æ - good with respect to ( wrt ) C , then it is called Æ - bad wrt C . Finally , a set C is Æ - clean if all v 2 C are Æ - good wrt C . We now present two key lemmas . Lemma 1 Given a clustering of V in which all clusters are Æ - clean for some Æ (cid:20) 1 = 4 , then the number of mistakes made by this clustering is at most 8mOPT . Proof : Let the clustering on V be ( C 1 ; (cid:1)(cid:1)(cid:1) ; C k ) . We will bound the number of mistakes made by this clustering by 8 times the number of edge - disjoint “erroneous triangles” in the graph , where an erroneous triangle is a triangle having two + edges and one (cid:0) edge . We then use the fact that OPT must make at least one mistake for each such triangle . First consider the negative mistakes . Pick a negative edge ( u ; v ) 2 C i (cid:2) C i that has not been considered so far . We will pick a w 2 C i such that both ( u ; w ) and ( v ; w ) are positive and associate ( u ; v ) with the erroneous triangle ( u ; v ; w ) . We now show that for all ( u ; v ) , such a w can always be picked such that no other negative edges ( u 0 ; v ) or ( u ; v 0 ) ( i . e . the ones sharing u or v ) also pick w . Since C i is Æ - clean , neither u nor v has more than Æ jC i j negative neighbors inside C i . Thus ( u ; v ) has at least ( 1 (cid:0) 2Æ ) jC i j vertices w such that both ( u ; w ) and ( v ; w ) are positive . Moreover , at most 2Æ jC i j (cid:0) 2 of these could have already been chosen by other negative edges ( u ; v 0 ) or ( u 0 ; v ) . Thus ( u ; v ) has at least ( 1 (cid:0) 4Æ ) s + 2 choices of w that satisfy the required condition . Since Æ (cid:20) 1 = 4 , ( u ; v ) will always be able to pick such a w . Note that any positive edge ( v ; w ) can be chosen at most 2 times by the above scheme , once for negative mistakes on v and possibly again for negative mistakes on w . Thus we can account for at least a fourth ( because only positive edges are double counted ) of the negative mistakes using edge disjoint erroneous triangles . Now , we consider the positive mistakes . Just as above , we will associate mistakes with erroneous triangles . We will start afresh , without taking into account the labelings from the previous part . Consider a positive edge between u 2 C i and v 2 C j . Let jC i j (cid:21) jC j j . Pick a w 2 C i such that ( u ; w ) is positive and ( v ; w ) is negative . There will be at least jC i j(cid:0) Æ ( jC i j + jC j j ) such vertices as before and at most Æ ( jC i j + jC j j ) of them will be already taken . Moreover only the positive edge ( u ; w ) can be chosen twice ( once as ( u ; w ) and once as ( w ; u ) ) . Repeating the above argument , we again see that we ac - count for at least half ( hence at least a quarter ) of the posi - tive mistakes using edge disjoint triangles . Now depending on whether there are more negative mis - takes or more positive mistakes , we can choose the triangles appropriately , and hence account for at least 1 / 8 of the total mistakes in the clustering . Lemma 2 There exists a clustering OPT 0 in which each non - singleton cluster is Æ - clean , and mOPT 0 (cid:20) ( 9 Æ 2 + 1 ) mOPT . Proof : Consider the following procedure applied to the clustering of OPT and call the resulting clustering OPT 0 . Procedure Æ - Clean - Up : Let C OPT 1 ; C OPT 2 ; : : : ; C OPT k be the clusters in OPT . 1 . Let S = ; . 2 . For i = 1 ; (cid:1)(cid:1)(cid:1) ; k do : ( a ) If the number of Æ 3 - bad vertices in C OPT i is more than Æ 3 jC OPT i j , then , S = S [ C OPT i , C 0 i = ; . We call this “dissolving” the cluster . ( b ) Else , let Bi denote the Æ 3 - bad vertices in C OPT i . Then S = S [ Bi and C 0 i = C OPT i n Bi . 3 . Output the clustering OPT 0 : C 0 1 ; C 0 2 ; : : : ; C 0 k ; f x g x2S . We will prove that mOPT and mOPT 0 are closely related . We ﬁrst show that each C 0 i is Æ clean . Clearly , this holds if C 0 i = ; . Now if C 0 i is non - empty , we know that jC OPT i j (cid:21) jC 0 i j (cid:21) jC OPT i j ( 1 (cid:0) Æ = 3 ) . For each point v 2 C 0 i , we have : j N + ( v ) \ C 0 i j (cid:21) ( 1 (cid:0) Æ = 3 ) jC OPT i j (cid:0) Æ = 3 jC OPT i j = ( 1 (cid:0) 2Æ = 3 ) jC OPT i j > ( 1 (cid:0) Æ ) jC 0 i j Similarly , counting positive neighbors of v in C OPT i \ C 0 i and outside C OPT i , we get , j N + ( v ) \ C 0 i j (cid:20) ( Æ = 3 ) jC OPT i j + ( Æ = 3 ) jC OPT i j (cid:20) 2Æ 3 jC 0 i j ( 1 (cid:0) Æ = 3 ) < Æ jC 0 i j ( as Æ < 1 ) Thus each C 0 i is Æ - clean . We now account for the number of mistakes . If we dissolve some C OPT i , then clearly the mistakes associated 3 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE with vertices in original C OPT i is at least ( Æ = 3 ) 2 jC OPT i j 2 = 2 . The mistakes added due to dissolving clusters is at most jC OPT i j 2 = 2 . If C OPT i was not dissolved , then , the original mistakes in C OPT i were at least Æ = 3 jC OPT i jj Bi j = 2 . The mistakes added by the procedure is at most j Bi jjC OPT i j . Noting that 6 = Æ < 9 = Æ 2 , the lemma follows . For the clustering OPT 0 given by the above lemma , we use C 0 i to denote the non - singleton clusters and S to denote the set of singleton clusters . We will now describe Algo - rithm Cautious that tries to ﬁnd clusters similar to OPT 0 . Throughout the rest of this section , we assume that Æ = 1 44 . Algorithm Cautious : 1 . Pick an arbitrary vertex v and do the following : ( a ) Let A ( v ) = N + ( v ) . ( b ) ( Vertex Removal Step ) : While 9 x 2 A ( v ) such that x is 3Æ - bad wrt A ( v ) , A ( v ) = A ( v ) n f x g . ( c ) ( Vertex Addition Step ) : Let Y = f y j y 2 V ; y is 7Æ - good wrt A ( v ) g . Let A ( v ) = A ( v ) [ Y . 2 2 . Delete A ( v ) from the set of vertices and repeat until no vertices are left or until all the produced sets A ( v ) are empty . In the latter case , output the remaining vertices as singleton nodes . Call the clusters output by algorithm Cautious A1 ; A2 ; (cid:1)(cid:1)(cid:1) . Let Z be the set of singleton vertices created in the ﬁnal step . Our main goal will be to show that the clusters output by our algorithm satisfy the property stated below . Theorem 3 8 j , 9 i such that C 0 j (cid:18) Ai . Moreover , each Ai is 11Æ - clean . In order to prove this theorem , we need the following two lemmas . Lemma 4 If v 2 C 0 i , where C 0 i is a Æ - clean cluster in OPT 0 , then , any vertex w 2 C 0 i is 3Æ - good wrt N + ( v ) . Proof : As v ; w 2 C i , j N + ( v ) \ C 0 i j (cid:21) ( 1 (cid:0) Æ ) jC 0 i j , j N + ( w ) \ C 0 i j (cid:21) ( 1 (cid:0) Æ ) jC 0 i j and j N + ( w ) \ C 0 i j (cid:20) Æ jC 0 i j . Also , ( 1 (cid:0) Æ ) jC 0 i j (cid:20) j N + ( v ) j (cid:20) ( 1 + Æ ) jC 0 i j . Thus , we get the following two conditions . j N + ( w ) \ N + ( v ) j (cid:21) ( 1 (cid:0) 2Æ ) jC 0 i j (cid:21) ( 1 (cid:0) 3Æ ) j N + ( v ) j j N + ( w ) \ N + ( v ) j (cid:20) 2Æ jC 0 i j (cid:20) 2Æ 1 (cid:0) Æj N + ( v ) j (cid:20) 3Æ j N + ( v ) j Thus , w is 3Æ - good wrt N + ( v ) . 2 Observe that in the vertex addition step , all vertices are added in one step as opposed to in the vertex removal step Lemma 5 Given an arbitrary set X , if v1 2 C 0 i and v2 2 C 0 j , then v1 and v2 cannot both be 3Æ - good wrt X . Proof : Firstly if v is 3Æ - good wrt some arbitrary set X , then ( 1 (cid:0) 3Æ ) j X j < N + ( v ) < ( 1 + 3Æ ) j X j . Suppose that v1 and v2 are both 3Æ - good with respect to X . Then , j N + ( v1 ) \ X j (cid:21) ( 1 (cid:0) 3Æ ) j X j and j N + ( v2 ) \ X j (cid:21) ( 1 (cid:0) 3Æ ) j X j , hence j N + ( v1 ) \ N + ( v2 ) \ X j (cid:21) ( 1 (cid:0) 6Æ ) j X j , which implies that j N + ( v1 ) \ N + ( v2 ) j (cid:21) ( 1 (cid:0) 6Æ ) j X j . Also , since v1 lies in a Æ - clean cluster C 0 i in OPT 0 , j N + ( v1 ) nC 0 i j (cid:20) Æ jC 0 i j , j N + ( v2 ) nC 0 j j (cid:20) Æ jC 0 j j and C 0 i \ C 0 j = ; . It follows that j N + ( v1 ) \ N + ( v2 ) j (cid:20) Æ ( jC 0 i j + jC 0 j j ) . Now notice that jC 0 i j (cid:20) j N + ( v1 ) \ C 0 i j + Æ jC 0 i j (cid:20) j N + ( v1 ) \ X \ C 0 i j + j N + ( v1 ) \ X \ C 0 i j + Æ jC 0 i j (cid:20) j N + ( v1 ) \ X \ C 0 i j + 3Æ j X j + Æ jC 0 i j (cid:20) ( 1 + 3Æ ) j X j + Æ jC 0 i j . So , jC 0 i j (cid:20) 1 + 3Æ 1(cid:0)Æ j X j . The same holds for C 0 j . So , j N + ( v1 ) \ N + ( v2 ) j (cid:20) 2Æ 1 + 3Æ 1(cid:0)Æ j X j . However , since Æ < 1 = 9 , 2Æ ( 1 + 3Æ ) < ( 1 (cid:0) 6Æ ) ( 1 (cid:0) Æ ) and we have a contradiction . Thus the result follows . This gives us the following important corollary . Corollary 6 After the remove phase of the algorithm , no two vertices from distinct C 0 i and C 0 j can be present in A ( v ) . Now we go on to prove Theorem 3 . Proof of Theorem 3 : We will ﬁrst show that each Ai is either a subset of S or contains exactly one of the clusters C 0 j . The ﬁrst part of the theorem will follow . For a cluster Ai , let A 0 i be the set produced after the ver - tex removal phase such the cluster Ai is obtained by apply - ing the vertex addition phase to A 0 i . We have two cases . First , we consider the case when A 0 i (cid:18) S . Now during the vertex addition step , no vertex u 2 C 0 j can enter A 0 i for any j . This follows because , since C 0 j is Æ - clean and disjoint from A 0 i , for u to enter we need that Æ jC 0 j j (cid:21) ( 1 (cid:0) 7Æ ) j A 0 i j and ( 1 (cid:0) Æ ) jC 0 j j (cid:20) 7Æ j A 0 i j , and these two conditions cannot be satisﬁed simultaneously . Thus Ai (cid:18) S . In the second case , some u 2 C 0 j is present in A 0 i . How - ever , in this case observe that from Corollary 6 , no vertices from C 0 k can be present in A 0 i for any k 6 = j . Also , by the same reasoning as for the case A 0 i (cid:18) S , no vertex from C 0 k will enter A 0 i in the vertex addition phase . Now it only re - mains to show that C 0 j (cid:18) Ai . Since u was not removed from A 0 i it follows that many vertices from C 0 j are present in A 0 i . In particular , j N + ( u ) \ A 0 i j (cid:21) ( 1 (cid:0) 3Æ ) j A 0 i j and j N + ( u ) \ A 0 i j (cid:20) 3Æ j A 0 i j . Now ( 1 (cid:0) Æ ) jC 0 j j (cid:20) j N + ( u ) j implies that jC 0 j j (cid:20) 1 + 3Æ 1(cid:0)Æ j A 0 i j < 2 j A 0 i j . Also , j A 0 i \ C 0 j j (cid:21) j A 0 i \ N + ( u ) j (cid:0) j N + ( u ) \ C 0 j j (cid:21) j A 0 i \ N + ( u ) j(cid:0) Æ jC 0 j j . So we have j A 0 i \ C 0 j j (cid:21) ( 1 (cid:0) 5Æ ) j A 0 i j . We now show that all remaining vertices from C 0 j will enter Ai during the vertex addition phase . For w 2 C 0 j such that w = 2 A 0 i , j A 0 i \ C 0 j j (cid:20) 5Æ j A 0 i j and j N + ( w ) \ C 0 j j (cid:20) 4 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE Æ jC 0 j j together imply that j A 0 i \ N + ( w ) j (cid:20) 5Æ j A 0 i j + Æ jC 0 j j (cid:20) 7Æ j A 0 i j . The same holds for j A 0 i \ N + ( w ) j . So w is 7Æ - good wrt A 0 i and will be added in the Vertex Addition step . Thus we have shown that A ( v ) can contain C 0 j for at most one j and in fact will contain this set entirely . Next , we will show that for every j , 9 i s . t . C 0 j (cid:18) Ai . Let v chosen in Step 1 of the algorithm be such that v 2 C 0 j . We show that during the vertex removal step , no vertex from N + ( v ) \ C 0 j is removed . The proof follows by an easy induction on the number of vertices removed so far ( r ) in the vertex removal step . The base case ( r = 0 ) follows from Lemma 4 since every vertex in C 0 j is 3Æ - good with respect to N + ( v ) . For the induction step observe that since no vertex from N + ( v ) \ C 0 j is removed thus far , every vertex in C 0 j is still 3Æ - good wrt to the intermediate A ( v ) ( by mimicking the proof of lemma 4 with N + ( v ) replaced by A ( v ) ) . Thus A 0 i contains at least ( 1 (cid:0) Æ ) jC 0 j j vertices of C 0 j at the end of the vertex removal phase , and hence by the second case above , C 0 j (cid:18) Ai after the vertex addition phase . Finally we show that every non - singleton cluster Ai is 11Æ - clean . We know that at the end of vertex removal phase , 8 x 2 A 0 i , x is 3Æ - good wrt A 0 i . Thus , j N + ( x ) \ A 0 i j (cid:20) 3Æ j A 0 i j . So the total number of positive edges leaving A 0 i is at most 3Æ j A 0 i j 2 . Since , in the vertex addition step , we add vertices that are 7Æ - good wrt A 0 i , these can be at most 3Æ j A 0 i j 2 = ( 1 (cid:0) 7Æ ) j A 0 i j < 4Æ j A 0 i j . Thus j Ai j < ( 1 + 4Æ ) j A 0 i j . Since all vertices v in Ai are at least 7Æ - good wrt A 0 i , N + ( v ) \ Ai (cid:21) ( 1 (cid:0) 7Æ ) j A 0 i j (cid:21) 1(cid:0)7Æ 1 + 4Æ j Ai j (cid:21) ( 1 (cid:0) 11Æ ) j Ai j . Similarly , N + ( v ) \ Ai (cid:20) 7Æ j A 0 i j (cid:20) 11Æ j Ai j . This gives us the result . Now we are ready to bound the mistakes of A in terms of OPT and OPT 0 . Call mistakes that have both end points in some clusters Ai and Aj as internal mistakes and those that have an end point in Z as external mistakes . Similarly in OPT 0 , we call mistakes among the sets C 0 i as internal mistakes and mistakes having one end point in S as external mistakes . We bound mistakes of Cautious in two steps : the following lemma bounds external mistakes . Lemma 7 The total number of external mistakes made by Cautious are less than the external mistakes made by OPT 0 . Proof : From theorem 3 , it follows that Z cannot contain any vertex v in some C 0 i . Thus , Z (cid:18) S . Now , any exter - nal mistakes made by Cautious are positive edges adjacent to vertices in Z . These edges are also mistakes in OPT 0 since they are incident on singleton vertices in S . Hence the lemma follows . Now consider the internal mistakes of A . Notice that these could be many more than the internal mistakes of OPT 0 . However , we can at this point apply Lemma 1 on the graph induced by V 0 = [ iAi . In particular , the bound on internal mistakes follows easily by observing that 11Æ (cid:20) 1 = 4 , and that the mistakes of the optimal clustering on the graph induced by V 0 is no more than mOPT . Thus , Lemma 8 The total number of internal mistakes of Cau - tious is (cid:20) 8mOPT . Summing up results from the lemmas 7 and 8 , and using lemma 2 , we get the following theorem : Theorem 9 m Cautious (cid:20) 9 ( 1 Æ 2 + 1 ) mOPT . 4 A PTAS for maximizing agreements In this section , we give a PTAS for maximizing agree - ments : the total number of positive edges inside clusters and negative edges between clusters . Let OPT denote the optimal clustering and A denote our clustering . We will abuse notation and also use OPT to de - note the number of agreements in the optimal solution . As noticed in the introduction , OPT (cid:21) n ( n (cid:0) 1 ) = 4 . So it suf - ﬁces to produce a clustering that has at least OPT (cid:0) (cid:15)n 2 agreements , which will be the goal of our algorithm . Let Æ + ( V1 ; V2 ) denote the number of positive edges between sets V1 ; V2 (cid:18) V . Similarly , let Æ (cid:0) ( V1 ; V2 ) denote the num - ber of negative edges between the two . Let OPT ( (cid:15) ) denote the optimal clustering that has all non - singleton clusters of size greater than (cid:15)n . Lemma 10 OPT ( (cid:15) ) (cid:21) OPT (cid:0) (cid:15)n 2 = 2 . Proof : Consider the clusters of OPT of size less than or equal to (cid:15)n and break them apart into clusters of size 1 . Breaking up a cluster of size s reduces our objective func - tion by at most (cid:0) s 2 (cid:1) , which can be viewed as s = 2 per node in the cluster . Since there are at most n nodes in these clusters , and these clusters have size at most (cid:15)n , the total loss is at most (cid:15) n 2 2 . The above lemma means that it sufﬁces to produce a good approximation to OPT ( (cid:15) ) . Note that the num - ber of non - singleton clusters in OPT ( (cid:15) ) is less than 1 (cid:15) . Let C OPT 1 ; : : : ; C OPT k denote the non - singleton clusters of OPT ( (cid:15) ) and let C OPT k + 1 denote the set of points which corre - spond to singleton clusters . 4 . 1 A PTAS doubly - exponential in 1 = (cid:15) If we are willing to have a run time that is doubly - exponential in 1 = (cid:15) , we can do this by reducing our problem to the General Partitioning problem of [ 13 ] . The idea is as follows . Let G + denote the graph of only the + edges in G . Then , notice that we can express the quality of OPT ( (cid:15) ) in terms of just the sizes of the clusters , and the number of edges in 5 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE G + between and inside each of C OPT 1 ; : : : ; C OPT k + 1 . In par - ticular , if si = jC OPT i j and ei ; j = Æ + ( C OPT i ; C OPT j ) , then the number of agreements in OPT ( (cid:15) ) is : " k X i = 1 ei ; i # + (cid:20)(cid:18) sk + 1 2 (cid:19) (cid:0) ek + 1 ; k + 1 (cid:21) + 2 4 X i6 = j ( sisj (cid:0) ei ; j ) 3 5 : The General Partitioning property tester of [ 13 ] allows us to specify values for the si and eij , and if a partition of G + exists satisfying these constraints , will produce a partition that satisﬁes these approximately . We obtain a partition that has at least OPT ( (cid:15) ) (cid:0) (cid:15)n 2 agreements . The property tester runs in time exponential in ( 1 (cid:15) ) k + 1 and polynomial in n . Thus if we can guess the values of these sizes and num - ber of edges accurately , we would be done . It sufﬁces , in fact , to only guess the values up to an additive (cid:6) (cid:15) 2 n for the si , and up to an additive (cid:6) (cid:15) 3 n 2 for the ei ; j , because this introduces an additional error of at most O ( (cid:15) ) . So , at most O ( ( 1 = (cid:15) 3 ) 1 = (cid:15) 2 ) calls to the property tester need to be made . Our algorithm proceeds by ﬁnding a partition for each pos - sible value of si and ei ; j and returns the partition with the maximum number of agreements . We get the following re - sult : Theorem 11 The General Partitioning algorithm returns a clustering of graph G which has more than OPT (cid:0) (cid:15)n 2 agreements with probability at least 1 (cid:0) Æ . It runs in time exponential in ( 1 (cid:15) ) 1 = (cid:15) and polynomial in n and 1 Æ . 4 . 2 A singly - exponential PTAS We will now describe an algorithm that is based on the same basic idea of random sampling used by the General Partitioning algorithm . The idea behind our algorithm is as follows : Notice that if we knew the density of positive edges between a vertex and all the clusters , we could put v in the cluster that has the most positive edges to it . How - ever , trying all possible values of the densities requires too much time . Instead we adopt the following approach : We select a small random subset W of vertices and cluster them correctly into f Wi g with Wi (cid:26) Oi 8 i , by enumerating all possible clusterings of W . Since this subset is picked ran - domly , with a high probability , for all vertices v , the density of positive edges between v and Wi will be approximately equal to the density of positive edges between v and Oi . So we can decide which cluster to put v into , based on this information . However this is not sufﬁcient to account for edges between two vertices v1 and v2 , both of which do not belong to W . So , we consider subsets Ui of size m at a time and try out all possible clusterings f Uij g of them , picking the one that maximizes agreements with respect to f Wi g . This gives us the PTAS . Firstly note that if jC OPT k + 1 j < (cid:15)n , then if we only consider the agreements in the graph G nC OPT k + 1 , it affects the solution by at most (cid:15)n 2 . For now , we will assume that jC OPT k + 1 j < (cid:15)n and will present the algorithm and analysis based on this assumption . Later we will discuss the changes required to deal with the other case . In the following algorithm (cid:15) is a performance param - eter to be speciﬁed later . Let m = 88 3 (cid:2)40 (cid:15) 10 ( log 1 (cid:15) + 2 ) , k = 1 (cid:15) and (cid:15) 0 = (cid:15) 3 88 . Let pi denote the density of positive edges inside the cluster C OPT i and nij the den - sity of negative edges between clusters C OPT i and C OPT j . That is , pi = Æ + ( C OPT i ; C OPT i ) = (cid:0) jC OPT i j 2 (cid:1) and nij = Æ (cid:0) ( C OPT i ; C OPT j ) = ( jC OPT i jjC OPT j j ) . We begin by deﬁning a measure of goodness of a clus - tering f Uij g of some set Ui with respect to f Wi g , that will enable us to pick the right clustering of the set Ui . Deﬁnition 2 Ui1 ; : : : ; Ui ( k + 1 ) is (cid:15) 0 - good wrt W1 ; : : : ; Wk + 1 if it satisﬁes the following for all 1 (cid:20) j ; l (cid:20) k ( 1 ) Æ + ( Uij ; Wj ) (cid:21) ^ pj (cid:0) Wj 2 (cid:1) (cid:0) 18(cid:15) 0 m 2 ( 2 ) Æ (cid:0) ( Uij ; Wl ) (cid:21) ^ njl j Wj jj Wl j (cid:0) 6(cid:15) 0 m 2 and , for at least ( 1 (cid:0) (cid:15) 0 ) n of the vertices x and 8 j , ( 3 ) Æ + ( Uij ; x ) 2 Æ + ( Wj ; x ) (cid:6) 2(cid:15) 0 m . Our algorithm is as follows : Algorithm Divide & Choose : 1 . Pick a random subset W (cid:26) V of size m . 2 . For all partitions W1 ; : : : ; Wk + 1 of W do ( a ) Let ^ pi = Æ + ( Wi ; Wi ) = (cid:0) jWij 2 (cid:1) , and ^ nij = Æ (cid:0) ( Wi ; Wj ) = j Wi jj Wj j . ( b ) Let q = n m (cid:0) 1 . Consider a random partition of V n W into U1 ; : : : ; Uq , such that 8 i , j Ui j = m . ( c ) For all i do : Consider all ( k + 1 ) - partitions of Ui and let Ui1 ; : : : ; Ui ( k + 1 ) be the partition that is (cid:15) 0 - good wrt W1 ; : : : ; Wk + 1 ( by deﬁnition 2 above ) . If there is no such partition , choose Ui1 ; : : : ; Ui ( k + 1 ) arbitrarily . ( d ) Let Aj = [ iUij for all i . Let a ( f Wi g ) be the number of agreements of this clustering . 3 . Let f Wi g be the partition of W that maximizes a ( f Wi g ) . Return the clusters f Ai g ; f x g x2Ak + 1 cor - responding to this partition of W . 6 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE We will concentrate on the ”right” partition of of W given by Wi = W \ C OPT i , 8 i . We will show that the number of agreements of the clustering A1 ; : : : ; Ak + 1 corresponding to this partition f Wi g is at least OPT ( (cid:15) ) (cid:0) 2(cid:15)n 2 . Since we pick the best clustering , this gives us a PTAS . We will begin by showing that with a high probability , for most values of i , the partition of Ui s corresponding to the optimal partition is good with respect to f Wi g . Thus the algorithm will ﬁnd at least one such partition . Next we will show that if the algorithm ﬁnds good partitions for most Ui , then it achieves at least OPT (cid:0) O ( (cid:15) ) n 2 agreements . We will need the following results from probability the - ory . Please refer to [ 2 ] for a proof . Fact 1 : Let H ( n ; m ; l ) be the hypergeometric distribution with parameters n ; m and l ( choosing l samples from n points without replacement with the random variable tak - ing a value of 1 on exactly m out of the n points ) . Let 0 (cid:20) (cid:15) (cid:20) 1 . Then Pr [ j H ( n ; m ; l ) (cid:0) lm n j (cid:21) (cid:15)lm n ] (cid:20) 2e (cid:0) (cid:15) 2 lm 2n Fact 2 : Let X1 ; X2 ; : : : ; Xn be mutually independent r . v . s such that j Xi (cid:0) E [ Xi ] j < m for all i . Let S = P n i = 1 Xi , then Pr [ j S (cid:0) E [ S ] j (cid:21) a ] (cid:20) 2e (cid:0) a 2 2nm2 We will also need the following lemma : Lemma 12 Let Y and S be arbitrary disjoint sets and Z be a set picked from S at random . Then we have the following : Pr [ j Æ + ( Y ; Z ) (cid:0) jZj jSj Æ + ( Y ; S ) j > (cid:15) 0 j Y jj Z j ] (cid:20) 2e (cid:0)(cid:15) 02 jZj 2 . Proof : Æ + ( Y ; Z ) is a sum of j Z j random variables Æ + ( Y ; v ) ( v 2 Z ) , each bounded above by j Y j and having expected value Æ + ( Y ; S ) jSj . Thus applying Fact 2 , we get Pr [ j Æ + ( Y ; Z ) (cid:0) j Z j Æ + ( Y ; S ) = j S jj > (cid:15) 0 j Z jj Y j ] (cid:20) 2e (cid:0)(cid:15) 02 jZj 2 jYj 2 = 2jZjjYj 2 (cid:20) 2e (cid:0)(cid:15) 02 jZj = 2 Now notice that since we picked W uniformly at ran - dom from V , with a high probability the sizes of Wi s are in proportion to jC OPT i j . The following lemma formalizes this . Lemma 13 With probability at least 1 (cid:0) 2ke (cid:0)(cid:15) 02 (cid:15)m = 2 , 8 i , j Wi j 2 ( 1 (cid:6) (cid:15) 0 ) m n jC OPT i j Proof : For a given i , using Fact 1 and since jC OPT i j (cid:20) (cid:15)n , Pr [ jj Wi j (cid:0) m n jC OPT i jj > (cid:15) 0m n jC OPT i j ] (cid:20) 2e (cid:0)(cid:15) 02 mjC OPT i j = 2n (cid:20) 2e (cid:0)(cid:15) 02 (cid:15)m = 2 . Taking union bound over the k values of i we get the result . Using Lemma 13 , we show that the computed values of ^ pi and ^ nij are close to the true values pi and nij respectively . This gives us the following two lemmas 3 . Lemma 14 If Wi (cid:26) C OPT i and Wj (cid:26) C OPT j , then with probability at least 1 (cid:0) 4e (cid:0)(cid:15) 02 (cid:15)m = 4 , Æ + ( Wi ; Wj ) 2 m 2 n 2 Æ + ( C OPT i ; C OPT j ) (cid:6) 3(cid:15) 0 m 2 . Proof Sketch : We can apply lemma 12 in two steps - ﬁrst to bound Æ + ( Wi ; C OPT j ) in terms of Æ + ( C OPT i ; C OPT j ) by considering the process of picking Wi from C OPT i , and second to bound Æ + ( Wi ; Wj ) in terms of Æ + ( Wi ; C OPT j ) by ﬁxing Wi and considering the process of picking Wj from C OPT j . Then using lemma 13 , we combine the two and get the lemma . Lemma 15 With probability at least 1 (cid:0) 8 (cid:15) 02e (cid:0)(cid:15) 03 (cid:15)m = 4 , ^ pi (cid:21) pi (cid:0) 9(cid:15) 0 Proof Sketch : Note that we cannot use an argument simi - lar to the previous lemma directly here since we are dealing with edges inside the same set . We use the following trick : consider the partition of C OPT i into 1 (cid:15) 0 subsets of size (cid:15) 0 n 0 each , where n 0 = jC OPT i j . The idea is to bound the number of positive edges between every pair of subsets of C OPT i us - ing argument in the previous lemma and adding these up to get the result . Now let Uij = Ui \ C OPT j . The following lemma shows that for all i , with a high probability all Uij s are (cid:15) 0 - good wrt f Wi g . So we will be able to ﬁnd (cid:15) 0 - good partitions for most Ui s . Lemma 16 For a given i , let Uij = Ui \ C OPT j , then with probability at least 1 (cid:0) 32k 1 (cid:15) 02e (cid:0)(cid:15) 03 (cid:15)m = 4 , 8 j (cid:20) k , f Uij g are (cid:15) 0 - good wrt f Wj g . Proof Sketch : The ﬁrst and second conditions of Deﬁni - tion 2 can be obtained by applying an argument similar to lemmas 15 and 14 respectively . In order to obtain the third condition , we consider Æ + ( x ; Uij ) as a sum of m f 0 ; 1 g random variables ( corre - sponding to picking Ui from V ) , each of which is 1 iff the picked vertex lies in C OPT j and is adjacent to x . Then an ap - plication of Chernoff bound followed by union bound gives us the condition . Now we can bound the total number of agreements of A1 ; : : : ; Ak ; f x g x2Ak + 1 in terms of OPT : 3 Please refer to [ 5 ] for full proofs of the lemmas . 7 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE Theorem 17 If jC OPT k + 1 j < (cid:15)n , then A (cid:21) OPT (cid:0) 3(cid:15)n 2 with probability at least 1 (cid:0) (cid:15) . Proof : From lemma 16 , the probability that we were not able to ﬁnd a (cid:15) 0 - good partition of Ui wrt W1 ; (cid:1)(cid:1)(cid:1) ; Wk is at most 32 1 (cid:15) 02e (cid:0)(cid:15) 03 (cid:15)m = 4 . By our choice of m , this is at most (cid:15) 2 = 4 . So , with probability at least 1 (cid:0) (cid:15) = 2 , at most (cid:15) = 2 of the Ui s do not have an (cid:15) 0 - good partition . In the following calculation of the number of agree - ments , we assume that we are able to ﬁnd good partitions of all Ui s . We will only need to subtract at most (cid:15)n 2 = 2 from this value to obtain the actual number of agreements , since each Ui can effect the number of agreements by at most mn . We start by calculating the number of positive edges inside a cluster Aj . These are given by P a P x2Aj Æ + ( Uaj ; x ) . Using the fact that Uaj is good wrt f Wi g ( condition ( 3 ) ) , P x2Aj Æ + ( Uaj ; x ) (cid:21) P x2Aj ( Æ + ( Wj ; x ) (cid:0) 2(cid:15) 0 m ) (cid:0) (cid:15) 0 n j Uaj j = P b Æ + ( Wj ; Ubj ) (cid:0) 2(cid:15) 0 m j Aj j (cid:0) (cid:15) 0 n j Uaj j (cid:21) P b f ^ pj jWjj 2 2 (cid:0) 18(cid:15) 0 m 2 g (cid:0) 2(cid:15) 0 m j Aj j (cid:0) (cid:15) 0 n j Uaj j The last follows from the fact that Ubj is good wrt f Wi g ( condition ( 1 ) ) . From Lemma 13 , P x2Aj Æ + ( Uaj ; x ) (cid:21) P b f m 2 n 2 ^ pj ( 1 (cid:0) (cid:15) 0 ) 2 jC OPT j j 2 2 (cid:0) 18(cid:15) 0 m 2 g (cid:0) 2(cid:15) 0 m j Aj j (cid:0) (cid:15) 0 n j Uaj j (cid:21) m n ^ pj ( 1 (cid:0) (cid:15) 0 ) 2 jC OPT j j 2 2 (cid:0) 18(cid:15) 0 mn (cid:0) 2(cid:15) 0 m j Aj j (cid:0) (cid:15) 0 n j Uaj j Thus we bound P a Æ + ( Aj ; Uaj ) as P a Æ + ( Aj ; Uaj ) (cid:21) ^ pj ( 1 (cid:0) (cid:15) 0 ) 2 jC OPT j j 2 2 (cid:0) 18(cid:15) 0 n 2 (cid:0) 3(cid:15) 0 n j Aj j . Now using Lemma 15 , the total number of agreements is at least P j f ^ pj ( 1 (cid:0) (cid:15) 0 ) 2 jC OPT j j 2 2 g (cid:0) 18(cid:15) 0 n 2 k (cid:0) 3(cid:15) 0 n 2 (cid:21) P j f ( pj (cid:0) 9(cid:15) 0 ) ( 1 (cid:0) (cid:15) 0 ) 2 jC OPT j j 2 2 g (cid:0) 18(cid:15) 0 n 2 k (cid:0) 3(cid:15) 0 n 2 Hence , A + (cid:21) OPT + (cid:0) 11(cid:15) 0 kn 2 (cid:0) 21(cid:15) 0 n 2 k (cid:21) OPT + (cid:0) 32(cid:15) 0 n 2 k . Similarly , consider the negative edges in A . Using lemma 14 to estimate Æ (cid:0) ( Uai ; Ubj ) , we get , P ab Æ (cid:0) ( Uai ; Ubj ) (cid:21) Æ (cid:0) ( C OPT i ; C OPT j ) (cid:0) 9(cid:15) 0 n 2 (cid:0) 2(cid:15) 0 n j Ai j (cid:0) (cid:15) 0 n j Aj j Summing over all i < j , we get the total number of negative agreements is at least OPT (cid:0) (cid:0) 12(cid:15) 0 k 2 n 2 . So we have , A (cid:21) OPT (cid:0) 44(cid:15) 0 k 2 n 2 = OPT (cid:0) (cid:15)n 2 = 2 . However , since we lose (cid:15)n 2 = 2 for not ﬁnding (cid:15) 0 - good parti - tions of every Ui ( as argued before ) , (cid:15)n 2 due to C OPT k + 1 , and (cid:15)n 2 = 2 for using k = 1 (cid:15) we obtain A (cid:21) OPT (cid:0) 3(cid:15)n 2 . The algorithm can fail in four situations : more than (cid:15) = 2 Ui s do not have an (cid:15) 0 - good partition with probability at most (cid:15) = 2 , lemma 13 does not hold for some Wi with prob - ability at most 2ke (cid:0)(cid:15) 02 (cid:15)m = 2 , lemma 15 does not hold for some i with probability at most 8k (cid:15) 02e (cid:0)(cid:15) 03 (cid:15)m = 4 or lemma 14 does not hold for some pair i ; j with probability at most 4k 2 e (cid:0)(cid:15) 02 (cid:15)m = 4 . The latter three quantities are at most (cid:15) = 2 by our choice of m . So , the algorithm succeeds with probabil - ity greater than 1 (cid:0) (cid:15) . Now we need to argue for the case when jC OPT k + 1 j (cid:21) (cid:15)n . Notice that in this case , using an argument similar to lemma 13 , we can show that j Wk + 1 j (cid:21) (cid:15)m 2 with a very high prob - ability . This is good because , now with a high probability , Ui ( k + 1 ) will also be (cid:15) 0 - good wrt Wk + 1 for most values of i . We can now count the number of negative edges from these vertices and incorporate them in the proof of Theo - rem 17 just as we did for the other k clusters . So in this case , we can modify algorithm Divide & Choose to consider (cid:15) 0 - goodness of the ( k + 1 ) th partitions as well . This gives us the same guarantee as in Theorem 17 . Thus our strategy will be to run Algorithm Divide & Choose once assuming that jC OPT k + 1 j (cid:21) (cid:15)n and then again assuming that jC OPT k + 1 j (cid:20) (cid:15)n , and picking the better of the two outputs . One of the two cases will correspond to reality and will give us the desired approximation to OPT . Now each Ui has O ( k m ) different partitions . Each iter - ation takes O ( nm ) time . There are n = m Ui s , so for each partition of W , the algorithm takes time O ( n 2 k m ) . Since there are k m different partitions of W , the total running time of the algorithm is O ( n 2 k 2m ) = O ( n 2 e O ( 1 (cid:15)10 log ( 1 (cid:15) ) ) ) . This gives us the following theorem : Theorem 18 For any Æ 2 [ 0 ; 1 ] , using (cid:15) = Æ 3 , Algorithm Divide & Choose runs in time O ( n 2 e O ( 1 Æ10 log ( 1 Æ ) ) ) and with probability at least 1 (cid:0) Æ 3 produces a clustering with number of agreements at least OPT (cid:0) Æn 2 . 5 Minimizing disagreements in [ (cid:0) 1 ; 1 ] - weighted graphs In section 3 , we developed an algorithm for minimizing disagreements in a graph with + 1 and (cid:0) 1 weighted edges . Now we consider the situation in which edge weights lie in the interval [ (cid:0) 1 ; 1 ] . To address this setting , we need to deﬁne a cost model – the penalty for placing an edge inside or between clusters . One natural model is a linear cost function . Speciﬁcally , given a clustering , we assign a cost of 1(cid:0)x 2 if an edge of weight x is within a cluster and a cost of 1 + x 2 if it is placed between two clusters . For example , an edge weighing 0 : 5 incurs a cost of 0 : 25 if it lies inside a cluster and 0 : 75 oth - 8 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE erwise . A 0 (cid:0) weight edge , on the other hand , incurs a cost of 1 = 2 no matter what . It turns out that any algorithm that ﬁnds a good cluster - ing in a f(cid:0) 1 ; 1 g(cid:0) graph also works well in the [ (cid:0) 1 ; 1 ] case under a linear cost function . Theorem 19 Let A be an algorithm that produces a clus - tering on a f(cid:0) 1 ; 1 g(cid:0) graph with approximation ratio (cid:26) . Then , we can construct an algorithm A 0 that achieves an approximation ratio of ( 2(cid:26) + 1 ) on a [ (cid:0) 1 ; 1 ] (cid:0) graph , with the linear cost function . Proof : Let G be a [ (cid:0) 1 ; 1 ] (cid:0) graph , and let G 0 be the f(cid:0) 1 ; 1 g(cid:0) graph obtained when we assign a weight of 1 to all positive edges in G and (cid:0) 1 to all the negative edges ( 0 cost edges are weighted arbitrarily ) . Let OPT be the opti - mal clustering on G and OPT 0 the optimal clustering on G 0 . Also , let m 0 be the measure of cost ( on G 0 ) in the f(cid:0) 1 ; 1 g penalty model and m in the new [ (cid:0) 1 ; 1 ] penalty model . Then , m 0 OPT 0 (cid:20) m 0 OPT (cid:20) 2mOPT . The latter is because OPT incurs a greater penalty of 1 in m 0 as compared to m only when a positive edge is between clusters or a negative edge inside a cluster . In both these situations , OPT incurs a cost of at least 1 = 2 in m and at most 1 in m 0 . This gives us the above equation . Our algorithm A 0 simply runs A on the graph G 0 and outputs the resulting clustering A . So , we have , m 0 A (cid:20) (cid:26)m 0 OPT 0 (cid:20) 2(cid:26)mOPT . Now we need to bound mA in terms of m 0 A . Notice that , if a positive edge lies between two clusters in A , or a neg - ative edge lies inside a cluster , then the cost incurred by A for these edges in m 0 is 1 while it is at most 1 in m . So , the total cost due to such mistakes is at most m 0 A . On the other hand , if we consider cost due to positive edges inside clus - ters , and negative edges between clusters , then OPT also incurs at least this cost on those edges ( because cost due to these edges can only increase if they are clustered differ - ently ) . So cost due to these mistakes is at most mOPT . So we have , mA (cid:20) m 0 A + mOPT (cid:20) 2(cid:26)mOPT + mOPT = ( 2(cid:26) + 1 ) mOPT Another natural cost model is one in which an edge of weight x incurs a cost of j x j when it is clustered improperly ( inside a cluster if x < 0 or between clusters of x > 0 ) and a cost of 0 when it is correct . We do not know of any good approximation in this case ( see Section 7 ) . 6 Random noise Going back to our original motivation , if we imagine there is some true correct clustering OPT of our n items , and that the only reason this clustering does not appear per - fect is that our function f ( A ; B ) used to label the edges has some error , then it is natural to consider the case that the the errors are random . That is , there is some constant noise rate (cid:23) < 1 = 2 and each edge , independently , is mislabeled with respect to OPT with probability (cid:23) . In the machine learning context , this is called the problem of learning with random noise . As can be expected , this is much easier to handle than the worst - case problem . In fact , with very simple al - gorithms one can ( whp ) produce a clustering that is quite close to OPT , much closer than the number of disagree - ments between OPT and f . The analysis is fairly standard ( much like the generic transformation of Kearns [ 16 ] in the machine learning context , and even closer to the analysis of Condon and Karp for graph partitioning [ 11 ] ) . In fact , this problem nearly matches a special case of the planted - partition problem of McSherry [ 18 ] . We present our analy - sis anyway since the algorithms are so simple . One - sided noise : As an easier special case , let us con - sider only one - sided noise in which each true “ + ” edge is ﬂipped to “ (cid:0) ” with probability (cid:23) . In that case , if u and v are in different clusters of OPT , then j N + ( u ) \ N + ( v ) j = 0 for certain . But , if u and v are in the same cluster , then every other node in the cluster independently has proba - bility ( 1 (cid:0) (cid:23) ) 2 of being a neighbor to both . So , if the cluster is large , then N + ( u ) and N + ( v ) will have a non - empty intersection with high probability . So , consider clus - tering greedily : pick an arbitrary node v , produce a cluster Cv = f u : j N + ( u ) \ N + ( v ) j > 0 g , and then repeat on V (cid:0) Cv . With high probability we will correctly cluster all nodes whose clusters in OPT are of size ! ( logn ) . The re - maining nodes might be placed in clusters that are too small , but overall the number of edge - mistakes is only ~ O ( n ) . Two - sided noise : For the two - sided case , it is technically easier to consider the symmetric difference of N + ( u ) and N + ( v ) . If u and v are in the same cluster of OPT , then every node w 62 f u ; v g has probability exactly 2(cid:23) ( 1 (cid:0) (cid:23) ) of belonging to this symmetric difference . But , if u and v are in different clusters , then all nodes w in OPT ( u ) [ OPT ( v ) have probability ( 1 (cid:0) (cid:23) ) 2 + (cid:23) 2 = 1 (cid:0) 2(cid:23) ( 1 (cid:0) (cid:23) ) of belonging to the symmetric difference . ( For w 62 OPT ( u ) [ OPT ( v ) , the probability remains 2(cid:23) ( 1 (cid:0) (cid:23) ) . ) Since 2(cid:23) ( 1 (cid:0) (cid:23) ) is a constant less than 1 = 2 , this means we can conﬁdently de - tect that u and v belong to different clusters so long as j OPT ( u ) [ OPT ( v ) j = ! ( p nlogn ) . Furthermore , us - ing just j N + ( v ) j , we can approximately sort the vertices by cluster sizes . Combining these two facts , we can whp correctly cluster all vertices in large clusters , and then just place each of the others into a cluster by itself , making a total of ~ O ( n 3 = 2 ) edge mistakes . 9 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE 7 Open Problems and Concluding Remarks In this paper , we have presented a constant - factor ap - proximation for minimizing disagreements , and a PTAS for maximizing agreements , for the problem of clustering ver - tices in a fully - connected graph G with f + ; (cid:0)g edge labels . In Section 5 we extended some of our results to the case of real - valued labels , under a linear cost metric . One interesting open question is to ﬁnd good approxi - mations for the case when edge weights are in f(cid:0) 1 ; 0 ; + 1 g ( equivalently , edges are labeled + or (cid:0) but G is not nec - essarily fully - connected ) without considering the 0 - edges as “half a mistake” . In that context it is still easy to clus - ter if a perfect clustering exists : the same simple strategy works of removing the (cid:0) edges and producing each con - nected component of the resulting graph as a cluster . The random case is also easy if deﬁned appropriately . However , our approximation techniques do not appear to go through . We do not know how to achieve a constant - factor , or even logarithmic factor , approximation for minimizing disagree - ments . Note that we can still use our Divide & Choose algorithm to achieve an additive approximation of (cid:15)n 2 to the number of agreements . However , this does not imply a PTAS for maximizing agreements because OPT might be o ( n 2 ) in this variant . A further generalization of the problem is to consider un - bounded edge weights ( lying in [ (cid:0)1 ; + 1 ] ) . For example , the edge weights might correspond to the log odds of two documents belonging to the same cluster . Here the num - ber of disagreements could be deﬁned as the total weight of positive edges between clusters and negative edges inside clusters , and agreements deﬁned analogously . Again , we do not know of any good algorithm for approximating the number of disagreements in this case . We believe the prob - lem of maximizing agreements should be APX - hard for this generalization , but have not been able to prove it . We can show , however , that a PTAS would give an n (cid:15) approxima - tion algorithm for k - coloring , for any constant k . 4 The in - complete f(cid:0) 1 ; 0 ; + 1 g graph model seems to be as hard as this problem . For the original problem on a fully connected f + ; (cid:0)g graph , another question is whether one can approximate the correlation : the number of agreements minus the number of disagreements . It is easy to show that OPT must be (cid:10) ( n ) for this measure , but we do not know of any good approx - imation . It would also be good to improve our ( currently fairly large ) constant for approximating disagreements . References [ 1 ] N . Alon , E . Fischer , M . Krivelevich , and M . Szegedy . Ef - ﬁcient testing of large graphs . In Proceedings of the 40th 4 For details refer to [ 5 ] . Annual Symposium on Foundations of Computer Science , pages 645 – 655 , 1999 . [ 2 ] N . Alon and J . H . Spencer . The Probabilistic Method . John Wiley and Sons , 1992 . [ 3 ] S . Arora , A . Frieze , and H . Kaplan . A new rounding proce - dure for the assignment problem with applications to dense graph arrangements . In Proc . IEEE FOCS , pages 21 – 30 , 1996 . [ 4 ] S . Arora , D . Karger , and M . Karpinski . Polynomial time ap - proximation schemes for dense instances of np - hard prob - lems . In ACM Symposium on Theory of Computing , 1995 . [ 5 ] N . Bansal , A . Blum , and S . Chawla . Correlation clustering ( http : / / www . cs . cmu . edu / ˜shuchi / papers / clusteringfull . ps ) . Manuscript , 2002 . [ 6 ] S . Ben - David , P . M . Long , and Y . Mansour . Agnostic boost - ing . In Proceedings of the 2001 Conference on Computa - tional Learning Theory , pages 507 – 516 , 2001 . [ 7 ] M . Charikar and S . Guha . Improved combinatorial algo - rithms for the facility location and k - median problems . In Proceedings of the 40th Annual Symposium on Foundations of Computer Science , 1999 . [ 8 ] W . Cohen and A . McCallum . Personal communication , 2001 . [ 9 ] W . Cohen and J . Richman . Learning to match and cluster entity names . In ACM SIGIR’01 workshop on Mathemati - cal / Formal Methods in IR , 2001 . [ 10 ] W . Cohen and J . Richman . Learning to match and clus - ter large high - dimensional data sets for data integration . In Eighth ACM SIGKDD International Conference on Knowl - edge Discovery and Data Mining ( KDD ) , 2002 . [ 11 ] A . Condon and R . Karp . Algorithms for graph partitioning on the planted partition model . Random Structures and Al - gorithms , 18 ( 2 ) : 116 – 140 , 1999 . [ 12 ] F . de la Vega . Max - cut has a randomized approximation scheme in dense graphs . Random Structures and Algorithms , 8 ( 3 ) : 187 – 198 , 1996 . [ 13 ] O . Goldreich , S . Goldwasser , and D . Ron . Property testing and its connection to learning and approximation . JACM , 45 ( 4 ) : 653 – 750 , 1998 . [ 14 ] D . Hochbaum and D . Shmoys . A uniﬁed approach to ap - proximation algorithms for bottleneck problems . JACM , 33 : 533 – 550 , 1986 . [ 15 ] K . Jain and V . Vazirani . Primal - dual approximation algo - rithms for metric facility location and k - median problem . In Proc . 40th IEEE FOCS , 1999 . [ 16 ] M . Kearns . Efﬁcient noise - tolerant learning from statistical queries . In Proceedings of the Twenty - Fifth Annual ACM Symposium on Theory of Computing , pages 392 – 401 , 1993 . [ 17 ] M . J . Kearns , R . E . Schapire , and L . M . Sellie . Toward ef - ﬁcient agnostic learning . Machine Learning , 17 ( 2 / 3 ) : 115 – 142 , 1994 . [ 18 ] F . McSherry . Spectral partitioning of random graphs . In FOCS , pages 529 – 537 , 2001 . [ 19 ] M . Parnas and D . Ron . Testing the diameter of graphs . In Proceedings of RANDOM , pages 85 – 96 , 1999 . [ 20 ] L . Schulman . Clustering for edge - cost minimization . In ACM STOC , pages 547 – 555 , 2000 . 10 Proceedings of the 43 rd Annual IEEE Symposium on Foundations of Computer Science ( FOCS’02 ) 0272 - 5428 / 02 $ 17 . 00 © 2002 IEEE