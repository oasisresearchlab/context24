Comparing Execution Traces of Jupyter Notebook for Checking Correctness of Refactoring Fumiya Sato ∗ , Ayano Ikegami † , Takashi Ishio ∗ , Kazumasa Shimari ∗ , Kenichi Matsumoto ∗ ∗ Graduate School of Science and Technology , Nara Institute of Science and Technology , Nara , Japan Email : { sato . fumiya . se3 , ishio , k . shimari , matumoto } @ is . naist . jp † Yahoo Japan Corporation , Tokyo , Japan Email : aikegami @ yahoo - corp . jp Abstract —Jupyter Notebook is a popular tool for writing data analysis programs . Prior work showed that Jupyter Notebook users often duplicate their python code to try their hypothesis quickly . While such code clones can be removed by Extract Function refactoring later , users have to check that the output of a notebook is unaffected by the refactoring . However , users may not be able to compare execution results of a notebook before and after refactoring because non - textual output in Jupyter Notebook are fragile ; for example , each of executions produce non - identical graphical images even though they look the same . To address this issue , we propose a method to automatically compare API calls to execute a Jupyter Notebook in addition to the textual output , while ignoring non - textual output . Our key assumption is that the same API calls with the same parameters produce the same results even if their details are non - identical . To demonstrate the effectiveness of the approach , we implemented an automatic tool for Jupyter Notebook that extracts a function from code clones and automatically checks the correctness . Using the tool , we have extracted functions from 3 , 995 cells in 520 Jupyter Notebook ﬁles . 142 out of 520 Notebook ﬁles are executable . Our tool compared API calls to check the correctness for 88 Notebook ﬁles , while a simple textual comparison could check 22 Notebook ﬁles . Index Terms —Jupyter Notebook , Extract Function Refactor - ing , Python , Execution Trace , IPython , Code Clone I . I NTRODUCTION Jupyter Notebook is a popular tool for writing data analysis programs , which is used by 84 % of data scientists [ 5 ] . It provides an input ﬁeld which is called cell for a code fragment . Developers can interactively write source code fragments into cells , run the code , and conﬁrm the execution results such as text messages and plots for each cell . Code clones are similar source code fragments in a program . They may have a negative impact on software development and maintenance [ 3 ] . Jupyter Notebook also suffers from code clone because developers tend to reuse code cells in order to expedite experimentation and test hypotheses faster . Koenzen et al . [ 6 ] reported that Jupyter Notebook ﬁles on GitHub have a mean self - duplication rate of 7 . 6 % in their cells . To reduce the number of code clones , developers perform refactoring such as extracting function . However , refactoring may have a risk to accidentally change the program behavior . While accurate static program analysis are important for automated refactoring , such techniques are unavailable for dynamic languages such as Python . Even one of state - of - the - art methods [ 14 ] cannot extract an accurate call graph Fig . 1 . Two images produced by the same cell in a Jupyter Notebook ﬁle [ 15 ] Fig . 2 . The black area indicates differences in Fig . 1 from a Python program . As we cannot prove the accuracy of automated refactoring , developers have to check the behavior before and after refactoring . A typical approach to checking the behavior of a program before and after refactoring is testing the program [ 3 ] . Unfor - tunately , most Jupyter Notebook ﬁles have no test cases [ 11 ] . Hence , a simple alternative approach is comparing cell outputs of a Notebook ﬁle before and after refactoring . But this approach is also difﬁcult because a Notebook ﬁle may produce non - identical results for each execution because of some non - deterministic behavior in numerical calculation and graphical processing . Fig . 1 shows two graphical images produced by the same cell in a Jupyter Notebook ﬁle [ 15 ] . We obtained them by 62 2022 IEEE 16th International Workshop on Software Clones ( IWSC ) 2572 - 6587 / 22 / $ 31 . 00 ©2022 IEEE DOI 10 . 1109 / IWSC55060 . 2022 . 00019 2022 I EEE 16 t h I n t e r n a ti on a l W o r k s hop on S o f t w a r e C l on e s ( I W S C ) | 978 - 1 - 6654 - 8447 - 3 / 22 / $ 31 . 00 © 2022 I EE E | DO I : 10 . 1109 / I W S C 55060 . 2022 . 00019 Fig . 3 . Explanation of code clone simply executing the same ﬁle twice . At a glance , they appear to show the same results . But an automatic image comparison service 1 tells us that there is a difference as indicated by the black area in Fig . 2 . In this research , we propose a method to check whether refactoring is correctly performed in a Jupyter Notebook ﬁle by comparing the behavior of the ﬁle before and after Extract Function refactoring . Our key assumption is trace compatibility [ 8 ] of two versions of a program . If they are equivalent , their traces share the same expressions and values . Our method compares all API calls before and after refactoring instead of the results themselves so that we can compare arbitrary type of results such as graphical images and plots . We implemented a tool to automatically extract functions from code clones using this method . To evaluate the usefulness , we perform a case study on actual Jupyter Notebook ﬁles on Kaggle platform . The contributions of this study are listed as follows . • We have deﬁned a method to check the behavior of a program before and after refactoring using API calls instead of execution results . • We have conducted a case study and conﬁrmed that non - textual output often appears in Jupyter Notebook ﬁles . Our method helps to automatically check the behavior . • We implemented a tool to automatically perform Extract Function refactoring for Jupyter Notebook . In the remainder of the paper , we describe our related work in Section II . We describe our trace comparison method in Section III . Section IV describes our refactoring tool using the method . We show a case study of our tool in actual projects in Section V and summarize the paper in Section VI . II . R ELATED W ORK Code clone is a code fragment that is a similar or exact match to another code fragment . As shown in the Fig . 3 , a pair of similar code fragments is called a clone pair . An equivalence class of clone relation is called a clone set . In this study , we implement an automated Extract Function refactoring tool for a clone set within a Jupyter Notebook ﬁle . As individual Jupyter Notebook ﬁles are separately used , code clones in different ﬁles are out of scope . 1 https : / / www . color - site . com / image compares Many studies have been conducted to detect code clones . Sajnani et al . proposed SourcererCC [ 13 ] , which can efﬁciently detect code clones from large - scale projects . SourcererCC de - tects code clones by calculating the overlap similarity between two code blocks . Nakagawa et al . proposed NIL [ 10 ] , which identiﬁed clone candidates using an N - gram representation of token sequences and an inverted index . Then , NIL detects code clone based on the longest common subsequence between their token sequences . In our method , we adopt the metric of SourcererCC and the idea of two - step detection in NIL [ 10 ] ; we detect similar cells using the overlap similarity and then compute common code blocks in the cells . To manage code clones appropriately , Extract Function refactoring is performed to extract duplicated code into a function . There have been many studies on automatic Extract Function refactoring . For example , Maruyama [ 9 ] proposed a mechanism that semi - automatically method extraction using program slicing . In another study , Tairas et al . [ 17 ] proposed CeDAR to bridge the gap between code clone detection and automatic Extract Method refactoring . CeDAR extracts a method automatically using rule - based methods . Tsantalis et al . [ 18 ] proposed a method to determine whether extract function is applicable without changing the execution result of the program using PST ( Program Structure Tree ) mapping . In this study , we have implemented a rule - based refactoring tool for Jupyter Notebook . Jupyter Notebook is the most widely - used tool but its main - tenance is a difﬁcult task . Pimental et al . [ 11 ] reported that only 24 . 11 % of the Jupyter notebook ﬁles were successfully executed and 4 . 03 % had the same results due to the issues such as unambiguous cell execution order and dependency issues . They also reported that the output of 31 . 41 % of Jupyter notebook cells and 51 . 00 % of executed Jupyter notebook contains image . This result shows that we cannot check the correctness of refactoring using a simple comparison of textual output in nearly half of the notebooks . Chattopadhyay et al . [ 1 ] also conducted a survey about Jupyter Notebook to data scientists . They reported that refactoring was one of the difﬁcult and important tasks in the Jupyter Notebook . Data scientists reported that there was almost non - existent proper coding assistance in Jupyter Notebook environment . To solve this problem , we propose a method to check the correctness of the refactoring and implement a tool for Extract Function refactoring . Comparing execution traces of two programs is an approach to check the compatibility of the programs . Mariano et al . [ 8 ] deﬁned trace compatibility : If a derived program includes the same symbols as an original program and the trace includes events corresponding to the original program , the programs are trace compatible . They used execution traces to verify whether a program is successfully translated into a different programming language . Clune et al . [ 2 ] deﬁned a behavioral equivalence on functional programs : If two functions execute the same computation in the same order , then two functions are considered as equivalent . They used the property to classify student submissions into equivalent groups in their 63 programming classes . Our trace comparison method is based on those ideas . While the behavior of a general program depends on input , a typical Jupyter Notebook ﬁle is executed without input . Hence , we compare only two execution traces corresponding two versions of a ﬁle . III . P ROPOSED M ETHOD We propose a method to compare execution results of two versions of a Jupyter Notebook ﬁle before and after refactoring . While this study focuses on Extract Function refactoring removing code clones , our method is applicable to any refactoring satisfying the following conditions : • Two versions of a Notebook ﬁle have the same number of cells . • Each cell keeps the same behavior before and after refactoring . Suppose that the i - th cells before and after refactoring are represented by c i and c (cid:2) i . We consider two versions of a Jupyter Notebook ﬁle have the same behavior if all cells have the same behavior , i . e . ∀ i . b ( c i ) = b ( c (cid:2) i ) where b ( c ) represents the behavior of a cell c . A cell may produce various types of result , e . g . graphical images , videos , interactive widgets , and plots , in addition to textual output . For those non - textual contents , we use an execution trace of the cell as b ( c ) because it represents how the cell produced the output . We do not directly compare the out - put for each data type because unchanged cells may produce non - identical output as explained in Section I . Furthermore , it is impractical to support comparison methods for various data types generated by libraries . In the following subsections , we describe our trace collec - tion and comparison steps . A . Trace Collection In this study , an execution trace for a Jupyter Notebook cell c is a sequence of function calls triggered by an execution of the cell c . For each function call , we record the function name , argument names , and their values . We record transitive function calls if the code is written in the same Notebook ﬁle . We ignore function calls inside libraries . The resultant execution trace represents how the cell manipulated libraries to produce the result . Our execution trace does not include line numbers , because line numbers are affected by refactoring . In implementation , we use the standard function sys . settrace to insert our logging code recording function calls . We translate Notebook ﬁles before and after refactoring into python ﬁles , insert trace code between cells , and execute the ﬁles to record execution traces . We used a regular Python interpreter because sys . settrace is unusable on an iPython interpreter working in the Jupyter Notebook system . In a cell producing Fig . 1 , two function calls are included as follows : plt . figure ( figsize = ( 12 , 6 ) ) sns . lineplot ( x = " Loudness . . dB . . " , y = ’Energy’ , data = songs ) Its corresponding execution trace is following . Each function call is represented by a JSON - like format that is a pair of function name and its arguments . figure ： { ’num’ : ’None’ , ’figsize’ : ’ ( 12 , 6 ) ’ , ’dpi’ : ’None’ , ’facecolor’ : ’None’ , ’edgecolor’ : ’None’ , ’frameon’ : ’True’ , ’FigureClass’ : " < class ’matplotlib . figure . Figure’ > " , ’clear’ : ’False’ , ’kwargs’ : ’ { } ’ } inner _ f ： { ’args’ : ’ ( ) ’ , ’kwargs’ : " { ’x’ : ’Loudness . . dB . . ’ , ’y’ : ’Energy’ , ’data’ : . . . } " The ﬁrst function figure is called with the listed argu - ments . The second function inner _ f is an internal name of the lineplot function . It is called with labels and data arguments . We omitted the details of the data and remaining argument due to the space limitation . B . Trace Comparison We consider two execution traces are equivalent if both traces include the same function calls ; in other words , the function names , argument names , and their values should be the same . The function names and argument names are simply com - pared by the string comparison . For the argument values , we use heuristic rules depending on their types as follows . • Numbers : We regard two numbers as the same if they are equal within an error of 1 % . This is because numbers may vary for each execution due to rounding errors . • Strings : We regard two strings are the same if they are equal when we ignore object ID numbers in the strings . We recognize an object ID using a pattern “ < class - name at ID - number > ” . • Arrays : We regard two objects are the same if their elements are the same . Our implementation supports the standard array and ndarray because it is popular in Jupyter Notebook . • Other objects : We compare other objects by comparing their string representation . In terms of Python , two objects x and y are regarded as equivalent if str ( x ) = = str ( y ) ignoring object ID numbers in the strings . It should be noted that execution traces exclude standard library functions , e . g . print calls . We also exclude special functions _ find _ and _ load and _ handle _ fromlist that are executed for import statements . Limitation . While our method has been designed to support nondeterministic behavior in third - party libraries , our method does not support nondeterministic behavior , e . g . ran - dom numbers and timestamps explicitly used in a Notebook 64 Fig . 4 . An overview of the behavior of our refactoring tool ﬁle . In addition , our implementation uses a string representa - tion for objects . It is dependent on library implementations . Our method may recognize two equivalent objects as different if their string representations are generated in a nondetermin - istic manner , e . g . due to unordered set . There is also a risk to falsely recognize different objects as equivalent if objects omit some details from their string representations . IV . A UTOMATED E XTRACT F UNCTION R EFACTORING FOR J UPYTER N OTEBOOK Using the proposed method , we have developed a tool to automatically extract functions from code clones in a Jupyter Notebook ﬁle . Fig . 4 illustrates our refactoring with an example notebook . Our tool works in three steps : ( 1 ) Detecting cloned cells within a Notebook ﬁle , ( 2 ) Identifying a common code block to be extracted , and ( 3 ) Generating a function and replacing the blocks with function calls . A generated function is inserted to the ﬁrst occurrence of the original code clone . After the refactoring , our tool tests the behavior using the method described in Section III . While we employ the deﬁnition of SourcererCC to detect cloned cells , we extract functions only from Type - II clones inside the cells . As a Notebook ﬁle may include a number of clone sets , our tool tries to remove all of them . A . Detecting cloned cells in a Notebook ﬁle We regard a cell in a ﬁle as a code block for comparison . We detect a pair of code clones using SourcererCC’s deﬁni - tion [ 13 ] as follows . sim ( c i , c j ) = | B ( c i ) ∩ B ( c j ) | max { | B ( c i ) | , | B ( c j ) | } where B ( c ) is a set of tokens in the cell c . The set keeps identiﬁers in the cell as is . We regard a pair of cells as clones if their similarity is greater than 0 . 7 . This is because Svajlenko et al . [ 16 ] argue that clone - pairs with the similarity greater than 0 . 7 have strong clonal relationships . We make a group of clone pairs as a clone set using transitive closure . For example , if we have clone pairs ( c 1 , c 2 ) and ( c 2 , c 3 ) , then we make a clone set { c 1 , c 2 , c 3 } . Fig . 5 . Code to be used as example B . Identifying a common code block For each code clone set , our tool identiﬁes a common code block to be extracted . As a Python function can return multiple values at once , we extract a simple function from Type - II clones . If each cell uses different variables , the extracted function takes as input those variables as parameters . The code block identiﬁcation comprises the following steps . 1 ) Delete magic commands from the cells . 2 ) Modify the source code layout so that each cell has one statement per line ( e . g . , splitting a line including multiple statements separated by “ ; ” ) 3 ) Convert f - string to % operator . 4 ) Normalize literals and variables in code . We converted literals and variables to tokens named VALUE and VARIABLE , respectively . 5 ) Extract the longest common substring among the cells . This is performed by applying the LCS algorithm in an iterative manner [ 7 ] . It should be noted that the longest common substring is a consecutive code fragment . The identiﬁed common code block may not be extracted to a function . We check the following conditions . • The code block has at least two lines of code . • The code block does not include a function or class decla - ration . This condition exists because functions and classes cannot be encapsulated in a function . In addition , those code fragments are likely modularized by the Notebook author . • The common code block has no syntax errors . • The code block does not end with a control statement ( if , for , with , and try ) . • It does not cause an indentation error when we replaced a code block with a function call . Our tool extracts a function if all conditions are satisﬁed . C . Replacing the common code block with a function We extract functions using a rule - based method through the following steps . To illustrate the process , we use an example clone pair shown in Fig . 5 . 1 ) Add a display function if necessary : In Jupyter Note - book , the iPython interpreter automatically displays a value if a line at the end of a cell returns a value . To preserve the behavior , if the last line of a code block to be extracted is the last line of a cell and it is neither print nor display , we add a display function call to the code block . 2 ) Introducing function parameters : We convert literals and variables in the common code block to parameters . As the orig - inal code fragments corresponding to the common code block are Type - II clones , we simply replace them with parameter 65 TABLE I A VARIABLE CONVERSION TABLE FOR F IG . 5 Variable names Variable names Variable names after conversion in Code 1 in Code 2 VARIANCE0 train members test members VARIANCE1 pd pd VARIANCE2 train test VARIANCE3 members members VARIANCE4 train merged test merged VARIANCE5 songs songs TABLE II A LITERAL CONVERSION TABLE FOR F IG . 5 Literal Name Code 1 Code 2 VALUE0 ’msno’ ’msno’ VALUE1 ’onner’ ’inner’ VALUE2 ’song id’ ’song id’ VALUE3 ’outer’ ’outer’ variables according to their order , except for local variables declared for for loops . Table I shows a list of parameter variables for the example code . Assigning appropriate variable names is one of our future work . Similarly , we also convert literals to parameters if they have different values in the code fragments . We construct a table of literals in the code fragments as shown in Table II . We keep the common literals as is , and replace different literals with parameters of a function . As a result , we obtain a parameterized code block as shown in Fig . 6 . The code fragments to be extracted to a function may have a different number of variables with one another ; for example , a code clone uses two variables but its peer may use a single variable for two locations . In such a case , we reduce the size of the code block so that the mis - match locations are excluded from a function . 3 ) Making function calls : We create a function deﬁnition that takes as input parameter variables that are not assigned in the clone and parameterized literals that are different in the original code fragments . Our tool assigns a simple name with a sequential number such as f1 . The function is inserted to the beginning of the cell including the ﬁrst occurrence of the code block . The original code blocks are replaced with function calls . Fig . 7 shows the function generated for the example code clones . 4 ) Making a return statement : We create a return statement to send all variables used within the extracted function to the caller side . We also add an assignment to store the returned values to variables . Fig . 8 shows the example code with a generated return statement . After code clone sets are extracted to functions as much as possible , our tool executes the modiﬁed Jupyter Notebook ﬁle Fig . 6 . Code whose different literals and variables are converted to parameters Fig . 7 . Code with function calls and function declarations added Fig . 8 . The generated function with a return statement and one of its call sites to record an execution trace , and then compare the trace with the previous version . V . C ASE S TUDY We have conducted a case study to evaluate the effectiveness of the proposed method . We compare the number of Notebook ﬁles that can be checked by our method with the number of ﬁles that can be checked with a simple baseline method that compares textual output of two versions of a Notebook ﬁle . A . Dataset : Jupyter Notebook ﬁles on Kaggle platform We collect Jupyter Notebook ﬁles . As typical Jupyter Note - book ﬁles are data analysis programs , we also need dataset ﬁles to execute the Notebook ﬁles . To execute Jupyter Note - book ﬁles as much as possible within our limited size of strage , we selected the top three datasets on Kaggle platform [ 4 ] by their numbers of votes , because they are likely used by many users . The datasets are : the COVID - 19 Open Research Dataset Challenge ( CORD - 19 ) , Credit Card Fraud Detection , and Novel Corona Virus 2019 Dataset . We collected Jupyter Notebook ﬁles using at least one of the three datasets from KGTorrent [ 12 ] . We ﬁltered out ﬁles whose cells include more than 200 lines of code in total , due to the cost to inspect datasets used in the ﬁles . As a result , we could identify 1 , 111 Jupyter Notebook ﬁles out of 248 , 761 ﬁles published on Kaggle . Since some Notebook ﬁles require additional dataset ﬁles for analysis , we manually collected 124 dataset ﬁles based on their ﬁle names . After the data collection , we have tried to execute the Jupyter Notebook ﬁles . 368 ﬁles could be executed within one hour without runtime errors using the obtained datasets . The remaining ﬁles are terminated by errors . This is because some dataset ﬁles have been updated after their initial release . Some Notebook ﬁles refer to data ﬁles in an old version that are no longer included in its newer version . B . Code Clones in Dataset We have detected code clones from 1 , 111 Notebook ﬁles and try to extract functions from those clones , even though some of them are unexecutable . Table III shows the results of this step . 6 , 552 out of 26 , 156 cells in the 1 , 111 ﬁles are detected as code clones according to SourcererCC’s deﬁnition . 66 TABLE III A NUMBER OF J UPYTER N OTEBOOK FILES AND CELLS THAT TARGET OF EXTRACT FUNCTION All ﬁles Executable # notebooks # cells # notebooks # cells All ﬁles 1 , 111 26 , 156 368 6 , 922 Including clones 640 6 , 552 180 1 , 700 Functions extracted 520 3 , 995 142 1 , 015 Fig . 9 . LOC of the extracted function Our tool successfully extracted functions from 3 , 995 cells in 520 Notebook ﬁles . The result conﬁrms that many code clones exist in Jupyter Notebook ﬁles . We have tried to extract functions from all code clones , though small functions may decrease the readability of code . Our tool failed to extract functions from some code clones because the code blocks do not satisfy the conditions listed in Section IV - B . We found major reasons for failed cases : 1 , 945 out of 2 , 557 cells were not extracted because their common code blocks are at most a single line of code . 182 cells were not extracted because the duplicate code fragments included a function or class . 187 cells were not extracted due to parse errors . 71 cells were not extracted because of indentation errors . 158 cells were not extracted because last statement is control statement ( if , for , with , and try ) . 14 cells were not extracted due to technical problems of the tool . Fig . 9 shows the distribution of the number of lines of code ( LOC ) for the extracted functions . This shows that most of the extracted functions are small . C . How many Jupyter Notebook ﬁles are automatically checked using our method ? Our baseline is a textual comparison method that compares textual output of two versions of a Notebook ﬁle . We compare strings produced by the Notebook ﬁles , ignoring object IDs and rounding errors in numbers in the same manner as our method . We detected numbers in the strings using regular expressions . This method works only for Notebook ﬁles whose code clones produce only plain text . While we successfully extracted functions from code clones in 520 ﬁles , we analyze 142 out of 520 ﬁles that are executable TABLE IV T HE NUMBER OF J UPYTER N OTEBOOK FILES THAT SUCCEED AT COMPARING EXECUTION RESULTS . Notebook Category # notebooks Experiment target 142 Target of comparing API calls 117 Successfully compared API calls 88 Failed to compare API calls 13 Detected different API calls 16 The target of comparing textual output 25 Successfully compared textual output 22 Checked the same behavior using either 110 API calls or textual output TABLE V F AILURE CAUSES OF OUR TRACE COMPARISON METHOD Failure cause # notebooks A translated Python script caused a runtime error 13 Files used random numbers for processing 6 The value _ counts function in pandas library 2 returned values in different order The order of the elements of dict is different 1 Machine learning models are different 1 for each execution Directory names in the output are different because 1 our implementation stores two versions of the code in different directories Unknown ( Maybe implementation issues of our tool ) 5 Total 29 on Jupyter Notebook . We classiﬁed 142 ﬁles into two groups according to the output of code clones . If code clones are included only in cells producing textual output , we use the baseline method to compare the output . Otherwise , we use our method to compare execution traces to check the behavior of cells producing non - textual output . Table IV shows the result . We found that 117 ﬁles produce non - textual output . We have compared execution traces for 117 Notebook ﬁles . 88 of Jupyter Notebook ﬁles have the same execution traces before and after refactoring . Our tool failed to compare execution traces for 13 ﬁles due to runtime errors when they are translated into Python scripts . 9 of the 13 ﬁles failed due to implementation problems of the refactoring steps of the tool , e . g . accidentally changing a global variable to a local variable . 4 ﬁles failed probably due to the differences of runtime environments . One of them was dependent on the date of execution of the program . Another ﬁle failed because of an error of pandas library . We could not ﬁgure out actual causes for the remaining two ﬁles . The remaining 16 ﬁles resulted in different execution traces before and after refactoring . We have analyzed the reasons by manually checking the trace differences . Table V shows the details ; they are nondeterministic behavior except for the technical problems of the tool . The main reason is random values . Some functions such as value _ counts in pandas also produce strings whose contents are differently ordered for each execution . A machine - learning model also provided slightly different results to other API calls for each execution . 67 TABLE VI R EASONS WHY CELLS PRODUCED DIFFERENT TEXTUAL OUTPUT Reason # notebooks Slightly different ﬂoat numbers in strings 2 Bugs of our tool 1 Total 3 The baseline method could check only 25 Notebook ﬁles . While 22 ﬁles produced the same textual output , 3 ﬁles didn’t . Table VI shows the reasons why they produced different textual output . We found that two ﬁles performed numerical calculation and then produced strings using the results . The numbers in the strings are slightly different ( less than 1 % ) but detected as not equivalent because they are embedded in strings . An advanced textual comparison is also helpful to support automatic refactoring . In summary , the case study conﬁrmed that Jupyter Notebook ﬁles may produce different results . A traditional approach could check only 22 out of 142 ( 15 . 5 % ) because code clones were included in cells producing non - textual output . Our method increased the number of automatically checked Note - book ﬁles from 22 to 110 . This would help users to quickly check the behavior of a Notebook ﬁle after refactoring . D . Threats to Validity While some cells produce non - identical images for each execution , some cells may produce identical images for every execution . As a traditional approach comparing output ﬁles would work for those cells , our result may underestimate the capability of the traditional approach . Due to the limitation of comparing execution traces through string representations of objects used for API calls , our method has a risk to miss different behavior . However , we believe this risk is sufﬁciently small in this case study because we just extracted functions from code clones . In this case study , we assume that the behavior of Jupyter Notebook ﬁles is visible in their output cells because they are executed on Kaggle platform . We did not analyze other behavior , e . g . writing data to external ﬁles and / or external servers , that may be performed by the Notebook ﬁles . VI . C ONCLUSION In this study , we have proposed a method to compare the behavior of a Jupyter Notebook ﬁle before and after refactoring . Our case study shows that many Jupyter Notebook ﬁles include some nondeterministic behavior in third - party libraries . Our method increased the number of automatically checked Notebook ﬁles from 22 to 110 . The result shows that Jupyter Notebook ﬁles heavily rely on non - textual output ; our method provides more opportunities for developers to automatically check their source code after refactoring . In future work , we would like to extend our method to support nondeterministic behavior such as random numbers in Notebook ﬁles . Developing an advanced comparison for ob - jects and strings is also important to improve the method . Ap - plying our method to other dynamic programming languages is an interesting challenge . Another interesting challenge is developing a method to automatically decide appropriate func - tion and variable names . A CKNOWLEDGMENT This work is supported by JSPS KAKENHI Grant Number JP20H05706 . R EFERENCES [ 1 ] S . Chattopadhyay , I . Prasad , A . Z . Henley , A . Sarma , and T . Barik , “What’s wrong with computational notebooks ? pain points , needs , and design opportunities , ” in Proc . of CHI Conference on Human Factors in Computing Systems , 2020 , pp . 1 – 12 . [ 2 ] J . Clune , V . Ramamurthy , R . Martins , and U . A . Acar , “Program equivalence for assisted grading of functional programs , ” Proc . ACM Program . Lang . , vol . 4 , no . OOPSLA , 2020 . [ 3 ] M . Fowler , Refactoring : improving the design of existing code . Addison - Wesley , 2018 . [ 4 ] Kaggle Inc . , “Kaggle , ” https : / / www . kaggle . com / , last accessed on June 10 ， 2022 ． [ 5 ] T . Kluyver , B . Ragan - Kelley , F . Perez , B . Granger , M . Bussonnier , J . Frederic , K . Kelley , J . Hamrick , J . Grout , S . Corlay , P . Ivanov , D . Avila , S . Abdalla , C . Willing , and Jupyter Development Team , “Jupyter notebooks – a publishing format for reproducible computa - tional workﬂows , ” in Proc . of International Conference on Electronic Publishing , 2016 , pp . 87 – 90 . [ 6 ] A . P . Koenzen , N . A . Ernst , and M . - A . D . Storey , “Code duplication and reuse in jupyter notebooks , ” in Proc . of Symposium on Visual Languages and Human - Centric Computing , 2020 , pp . 1 – 9 . [ 7 ] Y . Lin , Z . Xing , Y . Xue , Y . Liu , X . Peng , J . Sun , and W . Zhao , “Detecting differences across multiple instances of code clones , ” in Proc . of International Conference on Software Engineering , 2014 , pp . 164 – 174 . [ 8 ] B . Mariano , Y . Chen , Y . Feng , G . Durrett , and I . Dillig , “Automated transpilation of imperative to functional code using neural - guided pro - gram synthesis , ” Proc . ACM Program . Lang . , vol . 6 , no . OOPSLA1 , 2022 . [ 9 ] K . Maruyama , “Automated method - extraction refactoring by using block - based slicing , ” in Proc . of Symposium on Software Reusability : Putting Software Reuse in Context , 2001 , pp . 31 – 40 . [ 10 ] T . Nakagawa , Y . Higo , and S . Kusumoto , “NIL : Large - scale detection of large - variance clones , ” in Proc . of Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering , 2021 , pp . 830 – 841 . [ 11 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire , “A large - scale study about quality and reproducibility of jupyter notebooks , ” in Proc . of International Conference on Mining Software Repositories , 2019 , pp . 507 – 517 . [ 12 ] L . Quaranta , F . Calefato , and F . Lanubile , “Kgtorrent : A dataset of python jupyter notebooks from kaggle , ” in Proc . of International Con - ference on Mining Software Repositories , 2021 , pp . 550 – 554 . [ 13 ] H . Sajnani , V . Saini , J . Svajlenko , C . K . Roy , and C . V . Lopes , “SourcererCC : Scaling code clone detection to big - code , ” in Proc . of International Conference on Software Engineering , 2016 , pp . 1157 – 1168 . [ 14 ] V . Salis , T . Sotiropoulos , P . Louridas , D . Spinellis , and D . Mitropoulos , “Pycg : Practical call graph generation in python , ” in Proc . of Interna - tional Conference on Software Engineering , 2021 , pp . 1646 – 1657 . [ 15 ] A . Singh , “Top 50 spotify songs with eda and prediction , ” https : / / www . kaggle . com / code / abhikbr / top - 50 - spotify - songs - with - eda - and - prediction / notebook , last accessed on August 1 , 2022 . , 2019 . [ 16 ] J . Svajlenko and C . K . Roy , “BigCloneEval : A clone detection tool evaluation framework with bigclonebench , ” in Proc . of International Conference on Software Maintenance and Evolution , 2016 , pp . 596 – 600 . [ 17 ] R . Tairas and J . Gray , “Increasing clone maintenance support by unifying clone detection and refactoring activities , ” Information and Software Technology , vol . 54 , no . 12 , pp . 1297 – 1307 , 2012 . [ 18 ] N . Tsantalis , D . Mazinanian , and G . P . Krishnan , “Assessing the refactorability of software clones , ” IEEE Transactions on Software Engineering , vol . 41 , no . 11 , pp . 1055 – 1090 , 2015 . 68