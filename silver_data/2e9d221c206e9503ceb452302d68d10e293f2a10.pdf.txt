Communicated by Ronald Williams Long Short - Term Memory Sepp Hochreiter Fakult¨at f¨ur Informatik , Technische Universit¨at M¨unchen , 80290 M¨unchen , Germany J¨urgen Schmidhuber IDSIA , Corso Elvezia 36 , 6900 Lugano , Switzerland Learning to store information over extended time intervals by recurrent backpropagation takes a very long time , mostly because of insufﬁcient , decaying error backﬂow . We brieﬂy review Hochreiter’s ( 1991 ) analysis of this problem , then address it by introducing a novel , efﬁcient , gradient - based method called long short - term memory ( LSTM ) . Truncating the gradient where this does not do harm , LSTM can learn to bridge minimal time lags in excess of 1000 discrete - time steps by enforcing constant error ﬂow through constant error carousels within special units . Multiplicative gate units learn to open and close access to the constant error ﬂow . LSTM is local in space and time ; its computational complexity per time step and weight is O . 1 / . Our experiments with artiﬁcial data involve local , distributed , real - valued , and noisy pattern representations . In compar - isons with real - time recurrent learning , back propagation through time , recurrent cascade correlation , Elman nets , and neural sequence chunk - ing , LSTM leads to many more successful runs , and learns much faster . LSTM also solves complex , artiﬁcial long - time - lag tasks that have never been solved by previous recurrent network algorithms . 1 Introduction In principle , recurrent networks can use their feedback connections to store representations of recent input events in the form of activations ( short - term memory , as opposed to long - term memory embodied by slowly changing weights ) . This is potentially signiﬁcant for many applications , including speech processing , non - Markovian control , and music composition ( Mozer , 1992 ) . The most widely used algorithms for learning what to put in short - term memory , however , take too much time or do not work well at all , espe - cially when minimal time lags between inputs and corresponding teacher signals are long . Although theoretically fascinating , existing methods do not provide clear practical advantages over , say , backpropagation in feed - forward nets with limited time windows . This article reviews an analysis of the problem and suggests a remedy . Neural Computation 9 , 1735 – 1780 ( 1997 ) c (cid:176) 1997 Massachusetts Institute of Technology 1736 Sepp Hochreiter and J¨urgen Schmidhuber The problem . With conventional backpropagation through time ( BPTT ; Williams & Zipser , 1992 ; Werbos , 1988 ) orreal - timerecurrentlearning ( RTRL ; Robinson & Fallside , 1987 ) , error signals ﬂowing backward in time tend to ( 1 ) blow up or ( 2 ) vanish ; the temporal evolution of the backpropagated error exponentially depends on the size of the weights ( Hochreiter , 1991 ) . Case 1 may lead to oscillating weights ; in case 2 , learning to bridge long time lags takes a prohibitive amount of time or does not work at all ( see section 3 ) . This article presents long short - term memory ( LSTM ) , a novel recurrent network architecture in conjunction with an appropriate gradient - based learning algorithm . LSTM is designed to overcome these error backﬂow problems . It can learn to bridge time intervals in excess of 1000 steps even in case of noisy , incompressible input sequences , without loss of short - time - lagcapabilities . Thisisachievedbyanefﬁcient , gradient - basedalgorithmfor an architecture enforcing constant ( thus , neither exploding nor vanishing ) error ﬂow through internal states of special units ( provided the gradient computation is truncated at certain architecture - speciﬁc points ; this does not affect long - term error ﬂow , though ) . Section 2 brieﬂy reviews previous work . Section 3 begins with an outline of the detailed analysis of vanishing errors due to Hochreiter ( 1991 ) . It then introduces a naive approach to constant error backpropagation for didac - tic purposes and highlights its problems concerning information storage and retrieval . These problems lead to the LSTM architecture described in section 4 . Section 5 presents numerous experiments and comparisons with competing methods . LSTM outperforms them and also learns to solve com - plex , artiﬁcial tasks no other recurrent net algorithm has solved . Section 6 discusses LSTM’s limitations and advantages . The appendix contains a de - tailed description of the algorithm ( A . 1 ) and explicit error ﬂow formulas ( A . 2 ) . 2 Previous Work This section focuses on recurrent nets with time - varying inputs ( as opposed to nets with stationary inputs and ﬁxed - point - based gradient calculations ; e . g . , Almeida , 1987 ; Pineda , 1987 ) . 2 . 1 Gradient - Descent Variants . The approaches of Elman ( 1988 ) , Fahl - man ( 1991 ) , Williams ( 1989 ) , Schmidhuber ( 1992a ) , Pearlmutter ( 1989 ) , and many of the related algorithms in Pearlmutter’s comprehensive overview ( 1995 ) suffer from the same problems as BPTT and RTRL ( see sections 1 and 3 ) . 2 . 2 Time Delays . Other methods that seem practical for short time lags only are time - delay neural networks ( Lang , Waibel , & Hinton , 1990 ) and Plate’s method ( Plate , 1993 ) , which updates unit activations based on a Long Short - Term Memory 1737 weighted sum of old activations ( see also de Vries & Principe , 1991 ) . Lin et al . ( 1996 ) propose variants of time - delay networks called NARX networks . 2 . 3 Time Constants . To deal with long time lags , Mozer ( 1992 ) uses time constants inﬂuencing changes of unit activations ( deVries and Principe’s 1991 approach may in fact be viewed as a mixture of time - delay neural net - works and time constants ) . For long time lags , however , the time constants need external ﬁne tuning ( Mozer , 1992 ) . Sun , Chen , and Lee’s alternative approach ( 1993 ) updates the activation of a recurrent unit by adding the old activation and the ( scaled ) current net input . The net input , however , tends to perturb the stored information , which makes long - term storage impractical . 2 . 4 Ring’s Approach . Ring ( 1993 ) also proposed a method for bridging long time lags . Whenever a unit in his network receives conﬂicting error signals , he adds a higher - order unit inﬂuencing appropriate connections . Although his approach can sometimes be extremely fast , to bridge a time lag involving 100 steps may require the addition of 100 units . Also , Ring’s net does not generalize to unseen lag durations . 2 . 5 Bengio et al . ’s Approach . Bengio , Simard , and Frasconi ( 1994 ) in - vestigate methods such as simulated annealing , multigrid random search , time - weighted pseudo - Newton optimization , and discrete error propaga - tion . Their “latch” and “two - sequence” problems are very similar to prob - lem 3a in this article with minimal time lag 100 ( see Experiment 3 ) . Bengio and Frasconi ( 1994 ) also propose an expectation - maximazation approach for propagating targets . With n so - called state networks , at a given time , their system can be in one of only n different states . ( See also the beginning of section 5 . ) But to solve continuous problems such as the adding problem ( section 5 . 4 ) , their system would require an unacceptable number of states ( i . e . , state networks ) . 2 . 6 Kalman Filters . Puskorius and Feldkamp ( 1994 ) use Kalman ﬁlter techniques to improve recurrent net performance . Since they use “a deriva - tive discount factor imposed to decay exponentially the effects of past dy - namic derivatives , ” there is no reason to believe that their Kalman ﬁlter - trained recurrent networks will be useful for very long minimal time lags . 2 . 7 Second Order Nets . We will see that LSTM uses multiplicative units ( MUs ) to protect error ﬂow from unwanted perturbations . It is not the ﬁrst recurrent net method using MUs , though . For instance , Watrous and Kuhn ( 1992 ) useMUsinsecond - ordernets . TherearesomedifferencesfromLSTM : ( 1 ) Watrous and Kuhn’s architecture does not enforce constant error ﬂow and is not designed to solve long - time - lag problems ; ( 2 ) it has fully con - nected second - order sigma - pi units , while the LSTM architecture’s MUs 1738 Sepp Hochreiter and J¨urgen Schmidhuber are used only to gate access to constant error ﬂow ; and ( 3 ) Watrous and Kuhn’s algorithm costs O . W 2 / operations per time step , ours only O . W / , where W is the number of weights . See also Miller and Giles ( 1993 ) for additional work on MUs . 2 . 8 Simple Weight Guessing . To avoid long - time - lag problems of gradient - based approaches , we may simply randomly initialize all network weights until the resulting net happens to classify all training sequences correctly . In fact , recently we discovered ( Schmidhuber & Hochreiter , 1996 ; Hochreiter & Schmidhuber , 1996 , 1997 ) that simple weight guessing solves many of the problems in Bengio et al . ( 1994 ) , Bengio and Frasconi ( 1994 ) , Miller and Giles ( 1993 ) , and Lin et al . ( 1996 ) faster than the algorithms these authors proposed . This does not mean that weight guessing is a good algo - rithm . It just means that the problems are very simple . More realistic tasks require either many free parameters ( e . g . , input weights ) or high weight pre - cision ( e . g . , for continuous - valued parameters ) , such that guessing becomes completely infeasible . 2 . 9 Adaptive Sequence Chunkers . Schmidhuber’s hierarchical chun - ker systems ( 1992b , 1993 ) do have a capability to bridge arbitrary time lags , but only if there is local predictability across the subsequences causing the time lags ( see also Mozer , 1992 ) . For instance , in his postdoctoral thesis , Schmidhuber ( 1993 ) uses hierarchical recurrent nets to solve rapidly cer - tain grammar learning tasks involving minimal time lags in excess of 1000 steps . The performance of chunker systems , however , deteriorates as the noise level increases and the input sequences become less compressible . LSTM does not suffer from this problem . 3 Constant Error Backpropagation 3 . 1 Exponentially Decaying Error 3 . 1 . 1 Conventional BPTT ( e . g . , Williams & Zipser , 1992 ) . Output unit k ’s target at time t is denoted by d k . t / . Using mean squared error , k ’s error signal is # k . t / D f 0 k . net k . t / / . d k . t / ¡ y k . t / / ; where y i . t / D f i . net i . t / / is the activation of a noninput unit i with differentiable activation function f i , net i . t / D X j w ij y j . t ¡ 1 / Long Short - Term Memory 1739 is unit i ’s current net input , and w ij is the weight on the connection from unit j to i . Some nonoutput unit j ’s backpropagated error signal is # j . t / D f 0 j . net j . t / / X i w ij # i . t C 1 / : The corresponding contribution to w jl ’s total weight update is ﬁ # j . t / y l . t ¡ 1 / , where ﬁ is the learning rate and l stands for an arbitrary unit connected to unit j . 3 . 1 . 2 Outline of Hochreiter’s Analysis ( 1991 , pp . 19 – 21 ) . Suppose we have a fully connected net whose noninput unit indices range from 1 to n . Let us focus on local error ﬂow from unit u to unit v ( later we will see that the analysis immediately extends to global error ﬂow ) . The error occurring at an arbitrary unit u at time step t is propagated back into time for q time steps , to an arbitrary unit v . This will scale the error by the following factor : @ # v . t ¡ q / @ # u . t / D ( f 0 v . net v . t ¡ 1 / / w uv q D 1 f 0 v . net v . t ¡ q / / P nl D 1 @ # l . t ¡ q C 1 / @ # u . t / w lv q > 1 : ( 3 . 1 ) With l q D v and l 0 D u , we obtain : @ # v . t ¡ q / @ # u . t / D n X l 1 D 1 : : : n X l q ¡ 1 D 1 q Y m D 1 f 0 l m . net l m . t ¡ m / / w l m l m ¡ 1 ( 3 . 2 ) ( proof by induction ) . The sum of the n q ¡ 1 terms Q qm D 1 f 0 l m . net l m . t ¡ m / / w l m l m ¡ 1 determines the total error backﬂow ( note that since the summation terms may have different signs , increasing the number of units n does not neces - sarily increase error ﬂow ) . 3 . 1 . 3 Intuitive Explanation of Equation 3 . 2 . If j f 0 l m . net l m . t ¡ m / / w l m l m ¡ 1 j > 1 : 0 for all m ( as can happen , e . g . , with linear f l m ) , then the largest product increases exponentially with q . That is , the error blows up , and conﬂicting error signals arriving at unit v can lead to oscillating weights and unstable learning ( for error blowups or bifurcations , see also Pineda , 1988 ; Baldi & Pineda , 1991 ; Doya , 1992 ) . On the other hand , if j f 0 l m . net l m . t ¡ m / / w l m l m ¡ 1 j < 1 : 0 for all m , then the largest product decreases exponentially with q . That is , the error vanishes , and nothing can be learned in acceptable time . 1740 Sepp Hochreiter and J¨urgen Schmidhuber If f l m is the logistic sigmoid function , then the maximal value of f 0 l m is 0 . 25 . If y l m ¡ 1 is constant and not equal to zero , then j f 0 l m . net l m / w l m l m ¡ 1 j takes on maximal values where w l m l m ¡ 1 D 1 y l m ¡ 1 coth (cid:181) 1 2 net l m ¶ ; goes to zero for j w l m l m ¡ 1 j ! 1 , and is less than 1 : 0 for j w l m l m ¡ 1 j < 4 : 0 ( e . g . , if the absolute maximal weight value w max is smaller than 4 . 0 ) . Hence with conventional logistic sigmoid activation functions , the error ﬂow tends to vanish as long as the weights have absolute values below 4 . 0 , especially in the beginning of the training phase . In general , the use of larger initial weights will not help , though , as seen above , for j w l m l m ¡ 1 j ! 1 the relevant derivative goes to zero “faster” than the absolute weight can grow ( also , some weights will have to change their signs by crossing zero ) . Likewise , increasing the learning rate does not help either ; it will not change the ratio of long - range error ﬂow and short - range error ﬂow . BPTT is too sensitive to recent distractions . ( A very similar , more recent analysis was presented by Bengio et al . , 1994 . ) 3 . 1 . 4 Global Error Flow . The local error ﬂow analysis above immediately shows that global error ﬂow vanishes too . To see this , compute X u : u output unit @ # v . t ¡ q / @ # u . t / : 3 . 1 . 5 Weak Upper Bound for Scaling Factor . The following , slightly ex - tended vanishing error analysis also takes n , the number of units , into ac - count . For q > 1 , equation 3 . 2 can be rewritten as . W u T / T F 0 . t ¡ 1 / q ¡ 1 Y m D 2 . WF 0 . t ¡ m / / W v f 0 v . net v . t ¡ q / / ; where the weight matrix W is deﬁned by [ W ] ij : D w ij , v ’s outgoing weight vector W v is deﬁned by [ W v ] i : D [ W ] iv D w iv , u ’s incoming weight vector W u T is deﬁned by [ W u T ] i : D [ W ] ui D w ui , and for m D 1 ; : : : ; q , F 0 . t ¡ m / is the diagonal matrix of ﬁrst - order derivatives deﬁned as [ F 0 . t ¡ m / ] ij : D 0 if i 6D j , and [ F 0 . t ¡ m / ] ij : D f 0 i . net i . t ¡ m / / otherwise . Here T is the transposition operator , [ A ] ij is the element in the i th column and j th row of matrix A , and [ x ] i is the i th component of vector x . Using a matrix norm k ¢ k A compatible with vector norm k ¢ k x , we deﬁne f 0 max : D max m D 1 ; : : : ; q fk F 0 . t ¡ m / k A g : For max i D 1 ; : : : ; n fj x i jg • k x k x we get j x T y j • n k x k x k y k x : Since j f 0 v . net v . t ¡ q / / j • k F 0 . t ¡ q / k A • f 0 max ; Long Short - Term Memory 1741 we obtain the following inequality : ﬂﬂﬂﬂ @ # v . t ¡ q / @ # u . t / ﬂﬂﬂﬂ • n . f 0 max / q k W v k x k W u T k x k W k q ¡ 2 A • n ¡ f 0 max k W k A ¢ q : This inequality results from k W v k x D k We v k x • k W k A k e v k x • k W k A and k W u T k x D k W T e u k x • k W k A k e u k x • k W k A ; where e k is the unit vector whose components are 0 except for the k th com - ponent , which is 1 . Note that this is a weak , extreme case upper bound ; it will be reached only if all k F 0 . t ¡ m / k A take on maximal values , and if the contributions of all paths across which error ﬂows back from unit u to unit v have the same sign . Large k W k A , however , typically result in small values of k F 0 . t ¡ m / k A , as conﬁrmed by experiments ( see , e . g . , Hochreiter , 1991 ) . For example , with norms k W k A : D max r X s j w rs j and k x k x : D max r j x r j ; we have f 0 max D 0 : 25 for the logistic sigmoid . We observe that if j w ij j • w max < 4 : 0 n 8 i ; j ; then k W k A • nw max < 4 : 0 will result in exponential decay . By setting ¿ : D ¡ nw max 4 : 0 ¢ < 1 : 0 , we obtain ﬂﬂﬂﬂ @ # v . t ¡ q / @ # u . t / ﬂﬂﬂﬂ • n . ¿ / q : We refer to Hochreiter ( 1991 ) for additional results . 3 . 2 Constant Error Flow : Naive Approach . 3 . 2 . 1 A Single Unit . To avoid vanishing error signals , how can we achieve constant error ﬂow through a single unit j with a single connec - tion to itself ? According to the rules above , at time t , j ’s local error backﬂow is # j . t / D f 0 j . net j . t / / # j . t C 1 / w jj . To enforce constant error ﬂow through j , we require f 0 j . net j . t / / w jj D 1 : 0 : 1742 Sepp Hochreiter and J¨urgen Schmidhuber Note the similarity to Mozer’s ﬁxed time constant system ( 1992 ) —a time constant of 1 : 0 is appropriate for potentially inﬁnite time lags . 1 3 . 2 . 2 The Constant Error Carousel . Integrating the differential equation above , we obtain f j . net j . t / / D net j . t / w jj for arbitrary net j . t / . This means f j has to be linear , and unit j ’s activation has to remain constant : y j . t C 1 / D f j . net j . t C 1 / / D f j . w jj y j . t / / D y j . t / : In the experiments , this will be ensured by using the identity function f j : f j . x / D x ; 8 x , and by setting w jj D 1 : 0 . We refer to this as the constant error carousel ( CEC ) . CEC will be LSTM’s central feature ( see section 4 ) . Of course , unit j will not only be connected to itself but also to other units . This invokes two obvious , related problems ( also inherent in all other gradient - based approaches ) : 1 . Input weight conﬂict : For simplicity , let us focus on a single addi - tional input weight w ji . Assume that the total error can be reduced by switching on unit j in response to a certain input and keeping it active for a long time ( until it helps to compute a desired output ) . Provided i is nonzero , since the same incoming weight has to be used for both storing certain inputs and ignoring others , w ji will often receive con - ﬂicting weight update signals during this time ( recall that j is linear ) . These signals will attempt to make w ji participate in ( 1 ) storing the input ( by switching on j ) and ( 2 ) protecting the input ( by preventing j from being switched off by irrelevant later inputs ) . This conﬂict makes learning difﬁcult and calls for a more context - sensitive mechanism for controlling write operations through input weights . 2 . Output weight conﬂict : Assume j is switched on and currently stores some previous input . For simplicity , let us focus on a single additional outgoing weight w kj . The same w kj has to be used for both retriev - ing j ’s content at certain times and preventing j from disturbing k at other times . As long as unit j is nonzero , w kj will attract conﬂicting weight update signals generated during sequence processing . These signals will attempt to make w kj participate in accessing the informa - tion stored in j and—at different times—protecting unit k from being perturbed by j . For instance , with many tasks there are certain short - time - lag errors that can be reduced in early training stages . However , 1 We do not use the expression “time constant” in the differential sense , as Pearlmutter ( 1995 ) does . Long Short - Term Memory 1743 at later training stages , j may suddenly start to cause avoidable er - rors in situations that already seemed under control by attempting to participate in reducing more difﬁcult long - time - lag errors . Again , this conﬂict makes learning difﬁcult and calls for a more context - sensitive mechanism for controlling read operations through output weights . Of course , input and output weight conﬂicts are not speciﬁc for long time lags ; they occur for short time lags as well . Their effects , however , become particularly pronounced in the long - time - lag case . As the time lag increases , stored information must be protected against perturbation for longer and longer periods , and , especially in advanced stages of learning , more and more already correct outputs also require protection against perturbation . Due to the problems set out , the naive approach does not work well ex - cept in the case of certain simple problems involving local input - output rep - resentations and nonrepeating input patterns ( see Hochreiter , 1991 ; Silva , Amarel , Langlois , & Almeida , 1996 ) . The next section shows how to do it right . 4 The Concept of Long Short - Term Memory 4 . 1 Memory Cells and Gate Units . To construct an architecture that al - lows for constant error ﬂow through special , self - connected units without the disadvantages of the naive approach , we extend the CEC embodied by the self - connected , linear unit j from section 3 . 2 by introducing addi - tional features . A multiplicative input gate unit is introduced to protect the memory contents stored in j from perturbation by irrelevant inputs , and a multiplicative output gate unit is introduced to protect other units from perturbation by currently irrelevant memory contents stored in j . The resulting , more complex unit is called a memory cell ( see Figure 1 ) . The j th memory cell is denoted c j . Each memory cell is built around a central linear unit with a ﬁxed self - connection ( the CEC ) . In addition to net c j , c j gets input from a multiplicative unit out j ( the output gate ) , and from another multiplicative unit in j ( the input gate ) . in j ’s activation at time t is denoted by y inj . t / , out j ’s by y outj . t / . We have y out j . t / D f out j . net out j . t / / I y inj . t / D f inj . net inj . t / / I where net out j . t / D X u w out j u y u . t ¡ 1 / ; and net inj . t / D X u w inju y u . t ¡ 1 / : 1744 Sepp Hochreiter and J¨urgen Schmidhuber g h 1 . 0 net w y in y out net c g y in = g + s c s c y in h y out net w c in out w ic c j j j j out w j in j j j j j j y j j j j i i i Figure 1 : Architecture of memory cell c j ( the box ) and its gate units in j ; out j . The self - recurrent connection ( with weight 1 . 0 ) indicates feedback with a delay of one time step . It builds the basis of the CEC . The gate units open and close access to CEC . See text and appendix A . 1 for details . We also have net c j . t / D X u w c j u y u . t ¡ 1 / : The summation indices u may stand for input units , gate units , memory cells , or even conventional hidden units if there are any ( see section 4 . 3 ) . All these different types of units may convey useful information about the cur - rent state of the net . For instance , an input gate ( output gate ) may use inputs from other memory cells to decide whether to store ( access ) certain infor - mation in its memory cell . There even may be recurrent self - connections like w c j c j . It is up to the user to deﬁne the network topology . See Figure 2 for an example . At time t , c j ’s output y c j . t / is computed as y c j . t / D y out j . t / h . s c j . t / / ; where the internal state s c j . t / is s c j . 0 / D 0 ; s c j . t / D s c j . t ¡ 1 / C y inj . t / g ¡ net c j . t / ¢ for t > 0 : The differentiable function g squashes net c j ; the differentiable function h scales memory cell outputs computed from the internal state s c j . 4 . 2 Why Gate Units ? To avoid input weight conﬂicts , in j controls the error ﬂow to memory cell c j ’s input connections w c j i . To circumvent c j ’s output weight conﬂicts , out j controls the error ﬂow from unit j ’s output Long Short - Term Memory 1745 1 1 2 output hidden input out 1 in 1 out 2 in 2 1 cell block block 1 cell block block 2 cell 2 cell 2 Figure 2 : Example of a net with eight input units , four output units , and two memory cell blocks of size 2 . in 1 marks the input gate , out 1 marks the output gate , and cell 1 = block 1 marks the ﬁrst memory cell of block 1 . cell 1 = block 1’s archi - tecture is identical to the one in Figure 1 , with gate units in 1 and out 1 ( note that by rotating Figure 1 by 90 degrees anticlockwise , it will match with the corre - sponding parts of Figure 2 ) . The example assumes dense connectivity : each gate unit and each memory cell sees all non - output units . For simplicity , however , outgoing weights of only one type of unit are shown for each layer . With the efﬁcient , truncated update rule , error ﬂows only through connections to output unit , and through ﬁxed self - connections within cell blocks ( not shown here ; see Figure 1 ) . Error ﬂow is truncated once it “wants” to leave memory cells or gate units . Therefore , no connection shown above serves to propagate error back to the unit from which the connection originates ( except for connections to output units ) , althoughtheconnectionsthemselvesaremodiﬁable . Thatiswhythetrun - cated LSTM algorithm is so efﬁcient , despite its ability to bridge very long time lags . See the text and the appendix for details . Figure 2 shows the architecture used for experiment 6a ; only the bias of the noninput units is omitted . connections . In other words , the net can use in j to decide when to keep or override information in memory cell c j and out j to decide when to access memory cell c j and when to prevent other units from being perturbed by c j ( see Figure 1 ) . Error signals trapped within a memory cell’s CEC cannot change , but different error signals ﬂowing into the cell ( at different times ) via its out - put gate may get superimposed . The output gate will have to learn which 1746 Sepp Hochreiter and J¨urgen Schmidhuber errors to trap in its CEC by appropriately scaling them . The input gate will have to learn when to release errors , again by appropriately scaling them . Essentially the multiplicative gate units open and close access to constant error ﬂow through CEC . Distributed output representations typically do require output gates . Both gate types are not always necessary , though ; one may be sufﬁcient . For instance , in experiments 2a and 2b in section 5 , it will be possible to use input gates only . In fact , output gates are not required in case of local output encoding ; preventing memory cells from perturbing already learned outputs can be done by simply setting the corresponding weights to zero . Even in this case , however , output gates can be beneﬁcial : they prevent the net’s attempts at storing long - time - lag memories ( which are usually hard to learn ) from perturbing activations representing easily learnable short - time - lag memories . ( This will prove quite useful in experiment 1 , for instance . ) 4 . 3 Network Topology . We use networks with one input layer , one hid - den layer , and one output layer . The ( fully ) self - connected hidden layer contains memory cells and corresponding gate units ( for convenience , we refer to both memory cells and gate units as being located in the hidden layer ) . The hidden layer may also contain conventional hidden units pro - viding inputs to gate units and memory cells . All units ( except for gate units ) in all layers have directed connections ( serve as inputs ) to all units in the layer above ( or to all higher layers ; see experiments 2a and 2b ) . 4 . 4 Memory Cell Blocks . S memory cells sharing the same input gate and the same output gate form a structure called a memory cell block of size S . Memory cell blocks facilitate information storage . As with conventional neural nets , it is not so easy to code a distributed input within a single cell . Since each memory cell block has as many gate units as a single memory cell ( namely , two ) , the block architecture can be even slightly more efﬁcient . A memory cell block of size 1 is just a simple memory cell . In the experiments in section 5 , we will use memory cell blocks of various sizes . 4 . 5 Learning . We use a variant of RTRL ( e . g . , Robinson & Fallside , 1987 ) that takes into account the altered , multiplicative dynamics caused by input and output gates . To ensure nondecaying error backpropagation through internal states of memory cells , as with truncated BPTT ( e . g . , Williams & Peng , 1990 ) , errorsarrivingatmemorycellnetinputs ( forcell c j , thisincludes net c j , net inj , net out j ) do not get propagated back further in time ( although they do serve to change the incoming weights ) . Only within memory cells , are errors propagated back through previous internal states s c j . 2 To visualize 2 For intracellular backpropagation in a quite different context , see also Doya and Yoshizawa ( 1989 ) . Long Short - Term Memory 1747 this , once an error signal arrives at a memory cell output , it gets scaled by outputgateactivationand h 0 . Thenitiswithinthememorycell’sCEC , where it can ﬂow back indeﬁnitely without ever being scaled . When it leaves the memory cell through the input gate and g , it is scaled once more by input gate activation and g 0 . It then serves to change the incoming weights before it is truncated ( see the appendix for formulas ) . 4 . 6 ComputationalComplexity . AswithMozer’sfocusedrecurrentback - propagation algorithm ( Mozer , 1989 ) , only the derivatives @ s c j = @ w il need to be stored and updated . Hence the LSTM algorithm is very efﬁcient , with an excellent update complexity of O . W / , where W the number of weights ( see details in the appendix ) . Hence , LSTM and BPTT for fully recurrent nets have the same update complexity per time step ( while RTRL’s is much worse ) . Unlike full BPTT , however , LSTM is local in space and time : 3 there is no need to store activation values observed during sequence processing in a stack with potentially unlimited size . 4 . 7 Abuse Problem and Solutions . In the beginning of the learning phase , error reduction may be possible without storing information over time . The network will thus tend to abuse memory cells , for example , as bias cells ( it might make their activations constant and use the outgoing connections as adaptive thresholds for other units ) . The potential difﬁculty is that it may take a long time to release abused memory cells and make them available for further learning . A similar “abuse problem” appears if two memory cells store the same ( redundant ) information . There are at least two solutions to the abuse problem : ( 1 ) sequential network construction ( e . g . , Fahlman , 1991 ) : a memory cell and the corresponding gate units are added to the network whenever the error stops decreasing ( see experiment 2 in section 5 ) , and ( 2 ) output gate bias : each output gate gets a negative initial bias , to push initial memory cell activations toward zero . Memory cells with more negative bias automatically get “allocated” later ( see experiments 1 , 3 , 4 , 5 , and 6 in section 5 ) . 4 . 8 Internal State Drift and Remedies . If memory cell c j ’s inputs are mostly positive or mostly negative , then its internal state s j will tend to drift away over time . This is potentially dangerous , for the h 0 . s j / will then adopt very small values , and the gradient will vanish . One way to circumvent this problem is to choose an appropriate function h . But h . x / D x , for instance , has the disadvantage of unrestricted memory cell output range . Our simple 3 FollowingSchmidhuber ( 1989 ) , wesaythatarecurrentnetalgorithmis localinspace if the update complexity per time step and weight does not depend on network size . We say that a method is local in time if its storage requirements do not depend on input sequence length . For instance , RTRL is local in time but not in space . BPTT is local in space but not in time . 1748 Sepp Hochreiter and J¨urgen Schmidhuber but effective way of solving drift problems at the beginning of learning is initially to bias the input gate in j toward zero . Although there is a trade - off between the magnitudes of h 0 . s j / on the one hand and of y in j and f 0 in j on the other , the potential negative effect of input gate bias is negligible compared to the one of the drifting effect . With logistic sigmoid activation functions , there appears to be no need for ﬁne - tuning the initial bias , as conﬁrmed by experiments 4 and 5 in section 5 . 4 . 5 Experiments Which tasks are appropriate to demonstrate the quality of a novel long - time - lag algorithm ? First , minimal time lags between relevant input signals and corresponding teacher signals must be long for all training sequences . In fact , many previous recurrent net algorithms sometimes manage to gen - eralize from very short training sequences to very long test sequences ( see , e . g . , Pollack , 1991 ) . But a real long - time - lag problem does not have any short - time - lag exemplars in the training set . For instance , Elman’s training procedure , BPTT , ofﬂine RTRL , online RTRL , and others fail miserably on real long - time - lag problems . ( See , e . g . , Hochreiter , 1991 ; Mozer , 1992 . ) A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple - minded strategies such as random weight guessing . Recently we discovered ( Schmidhuber & Hochreiter , 1996 ; Hochreiter & Schmidhuber , 1996 , 1997 ) that many long - time - lag tasks used in previous workcanbesolvedmorequicklybysimplerandomweightguessingthanbytheproposedalgorithms . For instance , guessing solved a variant of Bengio and Frasconi’s parity problem ( 1994 ) much faster 4 than the seven methods tested by Bengio et al . ( 1994 ) and Bengio and Frasconi ( 1994 ) . The same is true for some of Miller and Giles’s problems ( 1993 ) . Of course , this does not mean that guessing is a good algorithm . It just means that some previously used problems are not extremely appropriate to demonstrate the quality of previously proposed algorithms . All our experiments ( except experiment 1 ) involve long minimal time lags ; there are no short - time - lag training exemplars facilitating learning . Solutions to most of our tasks are sparse in weight space . They require either many parameters and inputs or high weight precision , such that random weight guessing becomes infeasible . We always use online learning ( as opposed to batch learning ) and logistic sigmoids as activation functions . For experiments 1 and 2 , initial weights are chosen in the range [ ¡ 0 : 2 ; 0 : 2 ] , for the other experiments in [ ¡ 0 : 1 ; 0 : 1 ] . Training sequences are generated randomly according to the various task 4 Different input representations and different types of noise may lead to worse guess - ing performance ( Yoshua Bengio , personal communication , 1996 ) . Long Short - Term Memory 1749 descriptions . In slight deviation from the notation in appendix A . 1 , each discrete time step of each input sequence involves three processing steps : ( 1 ) use current input to set the input units , ( 2 ) compute activations of hidden units ( including input gates , output gates , memory cells ) , and ( 3 ) compute output unit activations . Except for experiments 1 , 2a , and 2b , sequence ele - ments are randomly generated online , and error signals are generated only at sequence ends . Net activations are reset after each processed input se - quence . For comparisons with recurrent nets taught by gradient descent , we give results only for RTRL , except for comparison 2a , which also includes BPTT . Note , however , that untruncated BPTT ( see , e . g . , Williams & Peng , 1990 ) computes exactly the same gradient as ofﬂine RTRL . With long - time - lag problems , ofﬂine RTRL ( or BPTT ) and the online version of RTRL ( no ac - tivation resets , online weight changes ) lead to almost identical , negative results ( as conﬁrmed by additional simulations in Hochreiter , 1991 ; see also Mozer , 1992 ) . This is because ofﬂine RTRL , online RTRL , and full BPTT all suffer badly from exponential error decay . Our LSTM architectures are selected quite arbitrarily . If nothing is known aboutthecomplexityofagivenproblem , amoresystematicapproachwould be to : start with a very small net consisting of one memory cell . If this does not work , try two cells , and so on . Alternatively , use sequential network construction ( e . g . , Fahlman , 1991 ) . Following is an outline of the experiments : † Experiment 1 focuses on a standard benchmark test for recurrent nets : the embedded Reber grammar . Since it allows for training sequences with short time lags , it is not a long - time - lag problem . We include it becauseitprovidesaniceexamplewhereLSTM’soutputgatesaretrulybeneﬁcial , and it is a popular benchmark for recurrent nets that has beenusedbymanyauthors . Wewanttoincludeatleastoneexperiment where conventional BPTT and RTRL do not fail completely ( LSTM , however , clearly outperforms them ) . The embedded Reber grammar’s minimal time lags represent a border case in the sense that it is still possible to learn to bridge them with conventional algorithms . Only slightly longer minimal time lags would make this almost impossible . The more interesting tasks in our article , however , are those that RTRL , BPTT , and others cannot solve at all . † Experiment 2 focuses on noise - free and noisy sequences involving nu - merous input symbols distracting from the few important ones . The most difﬁcult task ( task 2c ) involves hundreds of distractor symbols at random positions and minimal time lags of 1000 steps . LSTM solves it ; BPTT and RTRL already fail in case of 10 - step minimal time lags ( see also Hochreiter , 1991 ; Mozer , 1992 ) . For this reason RTRL and BPTT are omitted in the remaining , more complex experiments , all of which involve much longer time lags . 1750 Sepp Hochreiter and J¨urgen Schmidhuber † Experiment 3 addresses long - time - lag problems with noise and sig - nal on the same input line . Experiments 3a and 3b focus on Bengio et al . ’s 1994 two - sequence problem . Because this problem can be solved quickly by random weight guessing , we also include a far more difﬁ - cult two - sequence problem ( experiment 3c ) , which requires learning real - valued , conditional expectations of noisy targets , given the inputs . † Experiments 4 and 5 involve distributed , continuous - valued input rep - resentations and require learning to store precise , real values for very long time periods . Relevant input signals can occur at quite different positions in input sequences . Again minimal time lags involve hun - dreds of steps . Similar tasks never have been solved by other recurrent net algorithms . † Experiment 6 involves tasks of a different complex type that also has not been solved by other recurrent net algorithms . Again , relevant input signals can occur at quite different positions in input sequences . The experiment shows that LSTM can extract information conveyed by the temporal order of widely separated inputs . Section 5 . 7 provides a detailed summary of experimental conditions in two tables for reference . 5 . 1 Experiment 1 : Embedded Reber Grammar . 5 . 1 . 1 Task . Our ﬁrst task is to learn the embedded Reber grammar ( Smith & Zipser , 1989 ; Cleeremans , Servan - Schreiber , & McClelland , 1989 ; Fahlman , 1991 ) . Since it allows for training sequences with short time lags ( of as few as nine steps ) , it is not a long - time - lag problem . We include it for two reasons : ( 1 ) it is a popular recurrent net benchmark used by many authors , and we wanted to have at least one experiment where RTRL and BPTT do not fail completely , and ( 2 ) it shows nicely how output gates can be beneﬁcial . Starting at the left - most node of the directed graph in Figure 3 , sym - bol strings are generated sequentially ( beginning with the empty string ) by following edges—and appending the associated symbols to the current string—until the right - most node is reached ( the Reber grammar substrings are analogously generated from Figure 4 ) . Edges are chosen randomly if there is a choice ( probability : 0 . 5 ) . The net’s task is to read strings , one sym - bol at a time , and to predict the next symbol ( error signals occur at every time step ) . To predict the symbol before last , the net has to remember the second symbol . 5 . 1 . 2 Comparison . We compare LSTM to Elman nets trained by Elman’s training procedure ( ELM ) ( results taken from Cleeremans et al . , 1989 ) , Fahl - man’s recurrent cascade - correlation ( RCC ) ( results taken from Fahlman , Long Short - Term Memory 1751 B T S X X P V T P V S E Figure 3 : Transition diagram for the Reber grammar . B T P E T P GRAMMAR GRAMMAR REBER REBER Figure 4 : Transition diagram for the embedded Reber grammar . Each box rep - resents a copy of the Reber grammar ( see Figure 3 ) . 1752 Sepp Hochreiter and J¨urgen Schmidhuber 1991 ) , and RTRL ( results taken from Smith & Zipser , 1989 ) , where only the few successful trials are listed ) . Smith and Zipser actually make the task easier by increasing the probability of short - time - lag exemplars . We did not do this for LSTM . 5 . 1 . 3 Training / Testing . Weusealocalinput - outputrepresentation ( seven input units , seven output units ) . Following Fahlman , we use 256 training strings and 256 separate test strings . The training set is generated ran - domly ; training exemplars are picked randomly from the training set . Test sequences are generated randomly , too , but sequences already used in the training set are not used for testing . After string presentation , all activations are reinitialized with zeros . A trial is considered successful if all string sym - bolsofallsequencesinbothtestsetandtrainingsetarepredictedcorrectly—thatis , if the output unit ( s ) corresponding to the possible next symbol ( s ) is ( are ) always the most active ones . 5 . 1 . 4 Architectures . ArchitecturesforRTRL , ELM , andRCCarereported in the references listed above . For LSTM , we use three ( four ) memory cell blocks . Each block has two ( one ) memory cells . The output layer’s only incoming connections originate at memory cells . Each memory cell and each gate unit receives incoming connections from all memory cells and gate units ( the hidden layer is fully connected ; less connectivity may work as well ) . The input layer has forward connections to all units in the hidden layer . The gate units are biased . These architecture parameters make it easy to store at least three input signals ( architectures 3 - 2 and 4 - 1 are employed to obtain comparable numbers of weights for both architectures : 264 for 4 - 1 and 276 for 3 - 2 ) . Other parameters may be appropriate as well , however . All sigmoid functions are logistic with output range [ 0 ; 1 ] , except for h , whose range is [ ¡ 1 ; 1 ] , and g , whose range is [ ¡ 2 ; 2 ] . All weights are initialized in [ ¡ 0 : 2 ; 0 : 2 ] , except for the output gate biases , which are initialized to ¡ 1 , ¡ 2 , and ¡ 3 , respectively ( see abuse problem , solution 2 of section 4 ) . We tried learning rates of 0 . 1 , 0 . 2 , and 0 . 5 . 5 . 1 . 5 Results . We use three different , randomly generated pairs of train - ing and test sets . With each such pair we run 10 trials with different initial weights . See Table 1 for results ( mean of 30 trials ) . Unlike the other methods , LSTMalwayslearnstosolvethetask . Evenwhenweignoretheunsuccessful trials of the other approaches , LSTM learns much faster . 5 . 1 . 6 Importance of Output Gates . The experiment provides a nice exam - ple where the output gate is truly beneﬁcial . Learning to store the ﬁrst T or P should not perturb activations representing the more easily learnable transitions of the original Reber grammar . This is the job of the output gates . Without output gates , we did not achieve fast learning . Long Short - Term Memory 1753 Table 1 : Experiment 1 : Embedded Reber Grammar . Number of Learning Method Hidden Units Weights Rate % of Success After RTRL 3 … 170 0 . 05 Some fraction 173 , 000 RTRL 12 … 494 0 . 1 Some fraction 25 , 000 ELM 15 … 435 0 > 200 , 000 RCC 7 – 9 … 119 – 198 50 182 , 000 LSTM 4 blocks , size 1 264 0 . 1 100 39 , 740 LSTM 3 blocks , size 2 276 0 . 1 100 21 , 730 LSTM 3 blocks , size 2 276 0 . 2 97 14 , 060 LSTM 4 blocks , size 1 264 0 . 5 97 9500 LSTM 3 blocks , size 2 276 0 . 5 100 8440 Notes : Percentage of successful trials and number of sequence presentations until success for RTRL ( results taken from Smith & Zipser , 1989 ) , Elman net trained by Elman’s pro - cedure ( results taken from Cleeremans et al . , 1989 ) , recurrent cascade - correlation ( results taken from Fahlman , 1991 ) , and our new approach ( LSTM ) . Weight numbers in the ﬁrst four rows are estimates , the corresponding papers do not provide all the technical details . Only LSTM almost always learns to solve the task ( only 2 failures out of 150 trials ) . Even when we ignore the unsuccessful trials of the other approaches , LSTM learns much faster ( the number of required training examples in the bottom row varies between 3800 and 24 , 100 ) . 5 . 2 Experiment 2 : Noise - Free and Noisy Sequences . 5 . 2 . 1 Task 2a : Noise - Free Sequences with Long Time Lags . There are p C 1 possible input symbols denoted a 1 ; : : : ; a p ¡ 1 ; a p D x ; a p C 1 D y . a i is locally represented by the p C 1 - dimensional vector whose i th component is 1 ( all other components are 0 ) . A net with p C 1 input units and p C 1 output units sequentially observes input symbol sequences , one at a time , per - manently trying to predict the next symbol ; error signals occur at every time step . To emphasize the long - time - lag problem , we use a training set consisting of only two very similar sequences : . y ; a 1 ; a 2 ; : : : ; a p ¡ 1 ; y / and . x ; a 1 ; a 2 ; : : : ; a p ¡ 1 ; x / . Each is selected with probability 0 . 5 . To predict the ﬁ - nal element , the net has to learn to store a representation of the ﬁrst element for p time steps . We compare real - time recurrent learning for fully recurrent nets ( RTRL ) , back - propagation through time ( BPTT ) , the sometimes very successful two - net neural sequence chunker ( CH ; Schmidhuber , 1992b ) , and our new method ( LSTM ) . In all cases , weights are initialized in [ ¡ 0 : 2 ; 0 : 2 ] . Due to limited computation time , training is stopped after 5 million sequence pre - sentations . A successful run is one that fulﬁlls the following criterion : after training , during 10 , 000 successive , randomly chosen input sequences , the maximal absolute error of all output units is always below 0 : 25 . 1754 Sepp Hochreiter and J¨urgen Schmidhuber Table 2 : Task 2a : Percentage of Successful Trials and Number of Training Se - quences until Success . Learning Number of % Successful Success Method Delay p Rate Weights Trials After RTRL 4 1 . 0 36 78 1 , 043 , 000 RTRL 4 4 . 0 36 56 892 , 000 RTRL 4 10 . 0 36 22 254 , 000 RTRL 10 1 . 0 – 10 . 0 144 0 > 5 , 000 , 000 RTRL 100 1 . 0 – 10 . 0 10404 0 > 5 , 000 , 000 BPTT 100 1 . 0 – 10 . 0 10404 0 > 5 , 000 , 000 CH 100 1 . 0 10506 33 32 , 400 LSTM 100 1 . 0 10504 100 5 , 040 Notes : Table entries refer to means of 18 trials . With 100 time - step delays , only CH and LSTM achieve successful trials . Even when we ignore the unsuccessful trials of the other approaches , LSTM learns much faster . Architectures . RTRL : One self - recurrent hidden unit , p C 1 nonrecurrent output units . Each layer has connections from all layers below . All units use the logistic activation function sigmoid in [ 0 ; 1 ] . BPTT : Same architecture as the one trained by RTRL . CH : Both net architectures like RTRL’s , but one has an additional out - put for predicting the hidden unit of the other one ( see Schmid - huber , 1992b , for details ) . LSTM : As with RTRL , but the hidden unit is replaced by a memory cell and an input gate ( no output gate required ) . g is the logistic sig - moid , and h is the identity function h : h . x / D x ; 8 x . Memory cell and input gate are added once the error has stopped decreasing ( see abuse problem : solution 1 in section 4 ) . Results . Using RTRL and a short four - time - step delay ( p D 4 ) , 7 = 9 of all trials were successful . No trial was successful with p D 10 . With long time lags , only the neural sequence chunker and LSTM achieved successful trials ; BPTT and RTRL failed . With p D 100 , the two - net sequence chunker solved the task in only one - third of all trials . LSTM , however , always learned to solve the task . Comparing successful trials only , LSTM learned much faster . See Table 2 for details . It should be mentioned , however , that a hierarchical chunker can also always quickly solve this task ( Schmidhuber , 1992c , 1993 ) . 5 . 2 . 2 Task 2b : No Local Regularities . With task 2a , the chunker some - times learns to predict the ﬁnal element correctly , but only because of pre - Long Short - Term Memory 1755 dictable local regularities in the input stream that allow for compressing the sequence . In a more difﬁcult task , involving many more different pos - sible sequences , we remove compressibility by replacing the determin - istic subsequence . a 1 ; a 2 ; : : : ; a p ¡ 1 / by a random subsequence ( of length p ¡ 1 ) over the alphabet a 1 ; a 2 ; : : : ; a p ¡ 1 . We obtain two classes ( two sets of sequences ) f . y ; a i 1 ; a i 2 ; : : : ; a i p ¡ 1 ; y / j 1 • i 1 ; i 2 ; : : : ; i p ¡ 1 • p ¡ 1 g and f . x ; a i 1 ; a i 2 ; : : : ; a i p ¡ 1 ; x / j 1 • i 1 ; i 2 ; : : : ; i p ¡ 1 • p ¡ 1 g . Again , every next se - quence element has to be predicted . The only totally predictable targets , however , are x and y , which occur at sequence ends . Training exemplars are chosen randomly from the two classes . Architectures and parameters are the same as in experiment 2a . A successful run is one that fulﬁlls the following criterion : after training , during 10 , 000 successive , randomly cho - sen input sequences , the maximal absolute error of all output units is below 0 : 25 at sequence end . Results . As expected , the chunker failed to solve this task ( so did BPTT and RTRL , of course ) . LSTM , however , was always successful . On average ( mean of 18 trials ) , success for p D 100 was achieved after 5680 sequence presentations . This demonstrates that LSTM does not require sequence reg - ularities to work well . 5 . 2 . 3 Task 2c : Very Long Time Lags—No Local Regularities . This is the most difﬁcult task in this subsection . To our knowledge , no other recur - rent net algorithm can solve it . Now there are p C 4 possible input symbols denoted a 1 ; : : : ; a p ¡ 1 ; a p ; a p C 1 D e ; a p C 2 D b ; a p C 3 D x ; a p C 4 D y . a 1 ; : : : ; a p are also called distractor symbols . Again , a i is locally represented by the p C 4 - dimensional vector whose i th component is 1 ( all other components are 0 ) . A net with p C 4 input units and 2 output units sequentially ob - serves input symbol sequences , one at a time . Training sequences are ran - domly chosen from the union of two very similar subsets of sequences : f . b ; y ; a i 1 ; a i 2 ; : : : ; a i q C k ; e ; y / j 1 • i 1 ; i 2 ; : : : ; i q C k • q g and f . b ; x ; a i 1 ; a i 2 ; : : : ; a i q C k ; e ; x / j 1 • i 1 ; i 2 ; : : : ; i q C k • q g . To produce a training sequence , we randomly generate a sequence preﬁx of length q C 2 , randomly generate a sequence sufﬁx of additional elements ( 6D b ; e ; x ; y ) with probability 9 = 10 or , alternatively , an e with probability 1 = 10 . In the latter case , we conclude the sequence with x or y , depending on the second element . For a given k , this leads to a uniform distribution on the possible sequences with length q C k C 4 . The minimal sequence length is q C 4 ; the expected length is 4 C 1 X k D 0 1 10 (cid:181) 9 10 ¶ k . q C k / D q C 14 : The expected number of occurrences of element a i ; 1 • i • p , in a sequence is . q C 10 / = p … qp . The goal is to predict the last symbol , which always occurs after the “trigger symbol” e . Error signals are generated only at sequence 1756 Sepp Hochreiter and J¨urgen Schmidhuber Table 3 : Task 2c : LSTM with Very Long Minimal Time Lags q C 1 and a Lot of Noise . p ( Number of Number of q ( Time Lag ¡ 1 ) Random Inputs ) qp Weights Success After 50 50 1 364 30 , 000 100 100 1 664 31 , 000 200 200 1 1264 33 , 000 500 500 1 3064 38 , 000 1000 1 , 000 1 6064 49 , 000 1000 500 2 3064 49 , 000 1000 200 5 1264 75 , 000 1000 100 10 664 135 , 000 1000 50 20 364 203 , 000 Notes : p is the number of available distractor symbols ( p C 4 is the number of input units ) . q = p is the expected number of occurrences of a given distractor symbol in a sequence . The right - most column lists the number of training sequences required by LSTM ( BPTT , RTRL , and the other competitors have no chance of solving this task ) . If we let the number of distractor symbols ( and weights ) increase in proportion to the time lag , learning time increasesveryslowly . Thelowerblockillustratestheexpectedslowdownduetoincreased frequency of distractor symbols . ends . To predict the ﬁnal element , the net has to learn to store a represen - tation of the second element for at least q C 1 time steps ( until it sees the trigger symbol e ) . Success is deﬁned as prediction error ( for ﬁnal sequence element ) of both output units always below 0 : 2 , for 10 , 000 successive , ran - domly chosen input sequences . Architecture / Learning . The net has p C 4 input units and 2 output units . Weights are initialized in [ ¡ 0 : 2 ; 0 : 2 ] . To avoid too much learning time vari - ance due to different weight initializations , the hidden layer gets two mem - ory cells ( two cell blocks of size 1 , although one would be sufﬁcient ) . There are no other hidden units . The output layer receives connections only from memory cells . Memory cells and gate units receive connections from input units , memory cells , and gate units ( the hidden layer is fully connected ) . No bias weights are used . h and g are logistic sigmoids with output ranges [ ¡ 1 ; 1 ] and [ ¡ 2 ; 2 ] , respectively . The learning rate is 0 . 01 . Note that the min - imal time lag is q C 1 ; the net never sees short training sequences facilitating the classiﬁcation of long test sequences . Results . Twenty trials were made for all tested pairs . p ; q / . Table 3 lists the mean of the number of training sequences required by LSTM to achieve success ( BPTT and RTRL have no chance of solving nontrivial tasks with minimal time lags of 1000 steps ) . Long Short - Term Memory 1757 Scaling . Table 3 shows that if we let the number of input symbols ( and weights ) increase in proportion to the time lag , learning time increases very slowly . This is another remarkable property of LSTM not shared by any other method we are aware of . Indeed , RTRL and BPTT are far from scaling reasonably ; instead , they appear to scale exponentially and appear quite useless when the time lags exceed as few as 10 steps . Distractor Inﬂuence . In Table 3 , the column headed by q = p gives the ex - pected frequency of distractor symbols . Increasing this frequency decreases learning speed , an effect due to weight oscillations caused by frequently observed input symbols . 5 . 3 Experiment3 : NoiseandSignalonSameChannel . Thisexperiment serves to illustrate that LSTM does not encounter fundamental problems if noise and signal are mixed on the same input line . We initially focus on Bengio et al . ’s simple 1994 two - sequence problem . In experiment 3c we pose a more challenging two - sequence problem . 5 . 3 . 1 Task 3a ( Two - Sequence Problem ) . The task is to observe and then classify input sequences . There are two classes , each occurring with proba - bility 0 . 5 . There is only one input line . Only the ﬁrst N real - valued sequence elements convey relevant information about the class . Sequence elements at positions t > N are generated by a gaussian with mean zero and variance 0 . 2 . Case N D 1 : the ﬁrst sequence element is 1 . 0 for class 1 , and ¡ 1 : 0 for class 2 . Case N D 3 : the ﬁrst three elements are 1 . 0 for class 1 and ¡ 1 : 0 for class 2 . The target at the sequence end is 1 . 0 for class 1 and 0 . 0 for class 2 . Correct classiﬁcation is deﬁned as absolute output error at sequence end below 0 . 2 . Given a constant T , the sequence length is randomly selected between T and T C T = 10 ( a difference to Bengio et al . ’s problem is that they also permit shorter sequences of length T = 2 ) . Guessing . Bengio et al . ( 1994 ) and Bengio and Frasconi ( 1994 ) tested sevendifferentmethodsonthetwo - sequenceproblem . Wediscovered , how - ever , that random weight guessing easily outperforms them all because the problem is so simple . 5 See Schmidhuber and Hochreiter ( 1996 ) and Hochre - iter and Schmidhuber ( 1996 , 1997 ) for additional results in this vein . LSTM Architecture . We use a three - layer net with one input unit , one output unit , and three cell blocks of size 1 . The output layer receives connec - tions only from memory cells . Memory cells and gate units receive inputs from input units , memory cells , and gate units and have bias weights . Gate 5 However , different input representations and different types of noise may lead to worse guessing performance ( Yoshua Bengio , personal communication , 1996 ) . 1758 Sepp Hochreiter and J¨urgen Schmidhuber Table 4 : Task 3a : Bengio et al . ’s Two - Sequence Problem . Number ST2 : Fraction T N Stop : ST1 Stop : ST2 of Weights Misclassiﬁed 100 3 27 , 380 39 , 850 102 0 . 000195 100 1 58 , 370 64 , 330 102 0 . 000117 1000 3 446 , 850 452 , 460 102 0 . 000078 Notes : T is minimal sequence length . N is the number of information - conveying elements at sequence begin . The column headed by ST1 ( ST2 ) gives the number of sequence presentations required to achieve stopping criterion ST1 ( ST2 ) . The right - mostcolumnliststhefractionofmisclassiﬁedposttrainingsequences ( with absolute error > 0 . 2 ) from a test set consisting of 2560 sequences ( tested after ST2 was achieved ) . All values are means of 10 trials . We discovered , however , that thisproblemissosimplethatrandomweightguessingsolvesitfasterthanLSTMandanyothermethodforwhichtherearepublishedresults . units and output unit are logistic sigmoid in [ 0 ; 1 ] , h in [ ¡ 1 ; 1 ] , and g in [ ¡ 2 ; 2 ] . Training / Testing . All weights ( except the bias weights to gate units ) are randomly initialized in the range [ ¡ 0 : 1 ; 0 : 1 ] . The ﬁrst input gate bias is initialized with ¡ 1 : 0 , the second with ¡ 3 : 0 , and the third with ¡ 5 : 0 . The ﬁrst output gate bias is initialized with ¡ 2 : 0 , the second with ¡ 4 : 0 , and the third with ¡ 6 : 0 . The precise initialization values hardly matter though , as conﬁrmedbyadditionalexperiments . Thelearningrateis1 . 0 . Allactivations are reset to zero at the beginning of a new sequence . We stop training ( and judge the task as being solved ) according to the following criteria : ST1 : none of 256 sequences from a randomly chosen test set is misclassiﬁed ; ST2 : ST1 is satisﬁed , and mean absolute test set error is below 0 . 01 . In case of ST2 , an additional test set consisting of 2560 ran - domly chosen sequences is used to determine the fraction of misclassiﬁed sequences . Results . See Table 4 . The results are means of 10 trials with different weightinitializationsintherange [ ¡ 0 : 1 ; 0 : 1 ] . LSTMisabletosolvethisprob - lem , though by far not as fast as random weight guessing ( see “Guessing” above ) . Clearly this trivial problem does not provide a very good testbed to compare performance of various nontrivial algorithms . Still , it demon - strates that LSTM does not encounter fundamental problems when faced with signal and noise on the same channel . 5 . 3 . 2 Task 3b . The architecture , parameters , and other elements are as in task 3a , but now with gaussian noise ( mean 0 and variance 0 . 2 ) added to the Long Short - Term Memory 1759 Table 5 : Task 3b : Modiﬁed Two - Sequence Problem . Number ST2 : Fraction T N Stop : ST1 Stop : ST2 of Weights Misclassiﬁed 100 3 41 , 740 43 , 250 102 0 . 00828 100 1 74 , 950 78 , 430 102 0 . 01500 1000 1 481 , 060 485 , 080 102 0 . 01207 Note : Same as in Table 4 , but now the information - conveying elements are also perturbed by noise . information - conveying elements ( t < D N ) . We stop training ( and judge the task as being solved ) according to the following , slightly redeﬁned criteria : ST1 : fewer than 6 out of 256 sequences from a randomly chosen test set are misclassiﬁed ; ST2 : ST1 is satisﬁed , and mean absolute test set error is below 0 . 04 . In case of ST2 , an additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassiﬁed sequences . Results . See Table 5 . The results represent means of 10 trials with differ - ent weight initializations . LSTM easily solves the problem . 5 . 3 . 3 Task 3c . The architecture , parameters , and other elements are as in task 3a , but with a few essential changes that make the task nontrivial : the targets are 0 . 2 and 0 . 8 for class 1 and class 2 , respectively , and there is gaus - sian noise on the targets ( mean 0 and variance 0 . 1 ; S . D . 0 . 32 ) . To minimize mean squared error , the system has to learn the conditional expectations of the targets given the inputs . Misclassiﬁcation is deﬁned as absolute differ - ence between output and noise - free target ( 0 . 2 for class 1 and 0 . 8 for class 2 ) > 0 . 1 . The network output is considered acceptable if the mean absolute difference between noise - free target and output is below 0 . 015 . Since this requires high weight precision , task 3c ( unlike tasks 3a and 3b ) cannot be solved quickly by random guessing . Training / Testing . The learning rate is 0 : 1 . We stop training according to the following criterion : none of 256 sequences from a randomly chosen test set is misclassiﬁed , and mean absolute difference between the noise - free target and output is below 0 . 015 . An additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassi - ﬁed sequences . Results . See Table 6 . The results represent means of 10 trials with dif - ferent weight initializations . Despite the noisy targets , LSTM still can solve the problem by learning the expected target values . 1760 Sepp Hochreiter and J¨urgen Schmidhuber Table 6 : Task 3c : Modiﬁed , More Challenging Two - Sequence Problem . Number Fraction Average Difference T N Stop of Weights Misclassiﬁed to Mean 100 3 269 , 650 102 0 . 00558 0 . 014 100 1 565 , 640 102 0 . 00441 0 . 012 Notes : Same as in Table 4 , but with noisy real - valued targets . The system has to learn the conditional expectations of the targets given the inputs . The right - most column provides the average difference between network output and expected target . Unlike tasks 3a and 3b , this one cannot be solved quickly by random weight guessing . 5 . 4 Experiment 4 : Adding Problem . The difﬁcult task in this section is of a type that has never been solved by other recurrent net algorithms . It shows that LSTM can solve long - time - lag problems involving distributed , continuous - valued representations . 5 . 4 . 1 Task . Each element of each input sequence is a pair of compo - nents . The ﬁrst component is a real value randomly chosen from the interval [ ¡ 1 ; 1 ] ; the second is 1 . 0 , 0 . 0 , or ¡ 1 : 0 and is used as a marker . At the end of each sequence , the task is to output the sum of the ﬁrst components of those pairs that are marked by second components equal to 1 . 0 . Sequences have random lengths between the minimal sequence length T and T C T = 10 . In a given sequence , exactly two pairs are marked , as follows : we ﬁrst randomly select and mark one of the ﬁrst 10 pairs ( whose ﬁrst component we call X 1 ) . Then we randomly select and mark one of the ﬁrst T = 2 ¡ 1 still unmarked pairs ( whose ﬁrst component we call X 2 ) . The second components of all remaining pairs are zero except for the ﬁrst and ﬁnal pair , whose second components are ¡ 1 . ( In the rare case where the ﬁrst pair of the sequence gets marked , we set X 1 to zero . ) An error signal is generated only at the sequence end : the target is 0 : 5 C . X 1 C X 2 / = 4 : 0 ( the sum X 1 C X 2 scaled to the interval [ 0 ; 1 ] ) . A sequence is processed correctly if the absolute error at the sequence end is below 0 . 04 . 5 . 4 . 2 Architecture . We use a three - layer net with two input units , one output unit , and two cell blocks of size 2 . The output layer receives connec - tions only from memory cells . Memory cells and gate units receive inputs from memory cells and gate units ( the hidden layer is fully connected ; less connectivity may work as well ) . The input layer has forward connections to all units in the hidden layer . All noninput units have bias weights . These architecture parameters make it easy to store at least two input signals ( a cell block size of 1 works well , too ) . All activation functions are logistic with output range [ 0 ; 1 ] , except for h , whose range is [ ¡ 1 ; 1 ] , and g , whose range is [ ¡ 2 ; 2 ] . Long Short - Term Memory 1761 Table 7 : Experiment 4 : Results for the Adding Problem . Number of Number of T Minimal Lag Weights Wrong Predictions Success After 100 50 93 1 out of 2560 74 , 000 500 250 93 0 out of 2560 209 , 000 1000 500 93 1 out of 2560 853 , 000 Notes : T is the minimal sequence length , T = 2 the minimal time lag . “Number of Wrong Predictions” is the number of incorrectly processed sequences ( error > 0 . 04 ) from a test set containing2560sequences . Theright - mostcolumngivesthenumberoftrainingsequences required to achieve the stopping criterion . All values are means of 10 trials . For T D 1000 thenumberofrequiredtrainingexamplesvariesbetween370 , 000and2 , 020 , 000 , exceeding 700 , 000 in only three cases . 5 . 4 . 3 State Drift Versus Initial Bias . Note that the task requires storing the precise values of real numbers for long durations ; the system must learn to protect memory cell contents against even minor internal state drift ( see section 4 ) . To study the signiﬁcance of the drift problem , we make the task even more difﬁcult by biasing all noninput units , thus artiﬁcially inducing internal state drift . All weights ( including the bias weights ) are randomly initialized in the range [ ¡ 0 : 1 ; 0 : 1 ] . Following section 4’s remedy for state drifts , the ﬁrst input gate bias is initialized with ¡ 3 : 0 and the second with ¡ 6 : 0 ( though the precise values hardly matter , as conﬁrmed by additional experiments ) . 5 . 4 . 4 Training / Testing . The learning rate is 0 . 5 . Training is stopped once the average training error is below 0 . 01 , and the 2000 most recent sequences were processed correctly . 5 . 4 . 5 Results . With a test set consisting of 2560 randomly chosen se - quences , the average test set error was always below 0 . 01 , and there were never more than three incorrectly processed sequences . Table 7 shows de - tails . The experiment demonstrates that LSTM is able to work well with dis - tributed representations , LSTM is able to learn to perform calculations in - volving continuous values , and since the system manages to store continu - ous values without deterioration for minimal delays of T = 2 time steps , there is no signiﬁcant , harmful internal state drift . 5 . 5 Experiment 5 : Multiplication Problem . One may argue that LSTM is a bit biased toward tasks such as the adding problem from the previous subsection . Solutions to the adding problem may exploit the CEC’s built - in integration capabilities . Although this CEC property may be viewed as a 1762 Sepp Hochreiter and J¨urgen Schmidhuber Table 8 : Experiment 5 : Results for the Multiplication Problem . Minimal Number of Number of Success T Lag Weights n seq Wrong Predictions MSE After 100 50 93 140 139 out of 2560 0 . 0223 482 , 000 100 50 93 13 14 out of 2560 0 . 0139 1 , 273 , 000 Notes : T is the minimal sequence length and T = 2 the minimal time lag . We test on a test set containing 2560 sequences as soon as less than n seq of the 2000 most recent training sequences lead to error > 0 . 04 . “Number of Wrong Predictions” is the number of test sequences with error > 0 . 04 . MSE is the mean squared error on the test set . The right - most column lists numbers of training sequences required to achieve the stopping criterion . All values are means of 10 trials . feature rather than a disadvantage ( integration seems to be a natural subtask ofmanytasksoccurringintherealworld ) , thequestionariseswhetherLSTM can also solve tasks with inherently nonintegrative solutions . To test this , we change the problem by requiring the ﬁnal target to equal the product ( instead of the sum ) of earlier marked inputs . 5 . 5 . 1 Task . This is like the task in section 5 . 4 , except that the ﬁrst com - ponent of each pair is a real value randomly chosen from the interval [ 0 ; 1 ] . In the rare case where the ﬁrst pair of the input sequence gets marked , we set X 1 to 1 . 0 . The target at sequence end is the product X 1 £ X 2 . 5 . 5 . 2 Architecture . This is as in section 5 . 4 . All weights ( including the bias weights ) are randomly initialized in the range [ ¡ 0 : 1 ; 0 : 1 ] . 5 . 5 . 3 Training / Testing . Thelearningrateis0 . 1 . Wetestperformancetwice : as soon as less than n seq of the 2000 most recent training sequences lead to absolute errors exceeding 0 . 04 , where n seq D 140 and n seq D 13 . Why these values ? n seq D 140 is sufﬁcient to learn storage of the relevant inputs . It is not enough though to ﬁne - tune the precise ﬁnal outputs . n seq D 13 , however , leads to quite satisfactory results . 5 . 5 . 4 Results . For n seq D 140 ( n seq D 13 ) with a test set consisting of 2560 randomly chosen sequences , the average test set error was always below 0 . 026 ( 0 . 013 ) , and there were never more than 170 ( 15 ) incorrectly processed sequences . Table 8 shows details . ( A net with additional standard hidden units or with a hidden layer above the memory cells may learn the ﬁne - tuning part more quickly . ) The experiment demonstrates that LSTM can solve tasks involving both continuous - valuedrepresentationsandnonintegrativeinformationprocess - ing . Long Short - Term Memory 1763 5 . 6 Experiment 6 : Temporal Order . In this subsection , LSTM solves other difﬁcult ( but artiﬁcial ) tasks that have never been solved by previ - ous recurrent net algorithms . The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs . 5 . 6 . 1 Task 6a : Two Relevant , Widely Separated Symbols . The goal is to clas - sify sequences . Elements and targets are represented locally ( input vec - tors with only one nonzero bit ) . The sequence starts with an E , ends with a B ( the “trigger symbol” ) , and otherwise consists of randomly chosen symbols from the set f a ; b ; c ; d g except for two elements at positions t 1 and t 2 that are either X or Y . The sequence length is randomly chosen between 100 and 110 , t 1 is randomly chosen between 10 and 20 , and t 2 is randomly chosen between 50 and 60 . There are four sequence classes Q ; R ; S ; U , which depend on the temporal order of X and Y . The rules are : X ; X ! Q I X ; Y ! R I Y ; X ! S I Y ; Y ! U . 5 . 6 . 2 Task 6b : Three Relevant , Widely Separated Symbols . Again , the goal is to classify sequences . Elements and targets are represented locally . The sequence starts with an E , ends with a B ( the trigger symbol ) , and otherwise consists of randomly chosen symbols from the set f a ; b ; c ; d g except for three elements at positions t 1 ; t 2 , and t 3 that are either X or Y . The sequence length is randomly chosen between 100 and 110 , t 1 is randomly chosen between 10 and 20 , t 2 is randomly chosen between 33 and 43 , and t 3 is randomly chosen between 66 and 76 . There are eight sequence classes— Q ; R ; S ; U ; V ; A ; B ; C —which depend on the temporal order of the X s and Y s . The rules are : X ; X ; X ! Q I X ; X ; Y ! R I X ; Y ; X ! S I X ; Y ; Y ! U I Y ; X ; X ! V I Y ; X ; Y ! A I Y ; Y ; X ! B I Y ; Y ; Y ! C . There are as many output units as there are classes . Each class is locally represented by a binary target vector with one nonzero component . With both tasks , error signals occur only at the end of a sequence . The sequence is classiﬁed correctly if the ﬁnal absolute error of all output units is below 0 . 3 . Architecture . We use a three - layer net with eight input units , two ( three ) cell blocks of size 2 , and four ( eight ) output units for task 6a ( 6b ) . Again all noninput units have bias weights , and the output layer receives connections from memory cells , only . Memory cells and gate units receive inputs from input units , memory cells , and gate units ( the hidden layer is fully con - nected ; less connectivity may work as well ) . The architecture parameters for task 6a ( 6b ) make it easy to store at least two ( three ) input signals . All activation functions are logistic with output range [ 0 ; 1 ] , except for h , whose range is [ ¡ 1 ; 1 ] , and g , whose range is [ ¡ 2 ; 2 ] . 1764 Sepp Hochreiter and J¨urgen Schmidhuber Table 9 : Experiment 6 : Results for the Temporal Order Problem . Number of Number of Task Weights Wrong Predictions Success After Task 6a 156 1 out of 2560 31 , 390 Task 6b 308 2 out of 2560 571 , 100 Notes : “Number of Wrong Predictions” is the number of incorrectly classiﬁed sequences ( error > 0 . 3 for at least one output unit ) from a test set containing 2560 sequences . The right - most column gives the number of training sequences required to achieve the stopping criterion . The results for task 6a are means of 20 trials ; those for task 6b of 10 trials . Training / Testing . The learning rate is 0 . 5 ( 0 . 1 ) for experiment 6a ( 6b ) . Training is stopped once the average training error falls below 0 . 1 and the 2000 most recent sequences were classiﬁed correctly . All weights are initial - ized in the range [ ¡ 0 : 1 ; 0 : 1 ] . The ﬁrst input gate bias is initialized with ¡ 2 : 0 , the second with ¡ 4 : 0 , and ( for experiment 6b ) the third with ¡ 6 : 0 ( again , we conﬁrmed by additional experiments that the precise values hardly matter ) . Results . With a test set consisting of 2560 randomly chosen sequences , the average test set error was always below 0 . 1 , and there were never more than three incorrectly classiﬁed sequences . Table 9 shows details . The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs . In task 6a , for instance , the delays between the ﬁrst and second relevant input and between the second relevant input and sequence end are at least 30 time steps . Typical Solutions . In experiment 6a , how does LSTM distinguish be - tween temporal orders . X ; Y / and . Y ; X / ? One of many possible solutions is to store the ﬁrst X or Y in cell block 1 and the second X = Y in cell block 2 . Before the ﬁrst X = Y occurs , block 1 can see that it is still empty by means of its recurrent connections . After the ﬁrst X = Y , block 1 can close its input gate . Once block 1 is ﬁlled and closed , this fact will become visible to block 2 ( recall that all gate units and all memory cells receive connections from all nonoutput units ) . Typical solutions , however , require only one memory cell block . The block stores the ﬁrst X or Y ; once the second X = Y occurs , it changes its state depending on the ﬁrst stored symbol . Solution type 1 exploits the connection between memory cell output and input gate unit . The following events cause different input gate activations : X occurs in conjunction with a ﬁlled block ; X occurs in conjunction with an empty block . Solution type 2 is based on a strong , positive connection between memory cell output and memory cell input . The previous occurrence of X ( Y ) is represented by a Long Short - Term Memory 1765 Table 10 : Summary of Experimental Conditions for LSTM , Part I . ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) ( 10 ) ( 11 ) ( 12 ) ( 13 ) ( 14 ) ( 15 ) Task p lag b s in out w c ogb igb bias h g ﬁ 1 - 1 9 9 4 1 7 7 264 F ¡ 1 ; ¡ 2 ; ¡ 3 ; ¡ 4 r ga h1 g2 0 . 1 1 - 2 9 9 3 2 7 7 276 F ¡ 1 ; ¡ 2 ; ¡ 3 r ga h1 g2 0 . 1 1 - 3 9 9 3 2 7 7 276 F ¡ 1 ; ¡ 2 ; ¡ 3 r ga h1 g2 0 . 2 1 - 4 9 9 4 1 7 7 264 F ¡ 1 ; ¡ 2 ; ¡ 3 ; ¡ 4 r ga h1 g2 0 . 5 1 - 5 9 9 3 2 7 7 276 F ¡ 1 ; ¡ 2 ; ¡ 3 r ga h1 g2 0 . 5 2a 100 100 1 1 101 101 10 , 504 B No og None None id g1 1 . 0 2b 100 100 1 1 101 101 10 , 504 B No og None None id g1 1 . 0 2c - 1 50 50 2 1 54 2 364 F None None None h1 g2 0 . 01 2c - 2 100 100 2 1 104 2 664 F None None None h1 g2 0 . 01 2c - 3 200 200 2 1 204 2 1264 F None None None h1 g2 0 . 01 2c - 4 500 500 2 1 504 2 3064 F None None None h1 g2 0 . 01 2c - 5 1000 1000 2 1 1004 2 6064 F None None None h1 g2 0 . 01 2c - 6 1000 1000 2 1 504 2 3064 F None None None h1 g2 0 . 01 2c - 7 1000 1000 2 1 204 2 1264 F None None None h1 g2 0 . 01 2c - 8 1000 1000 2 1 104 2 664 F None None None h1 g2 0 . 01 2c - 9 1000 1000 2 1 54 2 364 F None None None h1 g2 0 . 01 3a 100 100 3 1 1 1 102 F ¡ 2 ; ¡ 4 ; ¡ 6 ¡ 1 ; ¡ 3 ; ¡ 5 b1 h1 g2 1 . 0 3b 100 100 3 1 1 1 102 F ¡ 2 ; ¡ 4 ; ¡ 6 ¡ 1 ; ¡ 3 ; ¡ 5 b1 h1 g2 1 . 0 3c 100 100 3 1 1 1 102 F ¡ 2 ; ¡ 4 ; ¡ 6 ¡ 1 ; ¡ 3 ; ¡ 5 b1 h1 g2 0 . 1 4 - 1 100 50 2 2 2 1 93 F r ¡ 3 ; ¡ 6 All h1 g2 0 . 5 4 - 2 500 250 2 2 2 1 93 F r ¡ 3 ; ¡ 6 All h1 g2 0 . 5 4 - 3 1000 500 2 2 2 1 93 F r ¡ 3 ; ¡ 6 All h1 g2 0 . 5 5 100 50 2 2 2 1 93 F r r All h1 g2 0 . 1 6a 100 40 2 2 8 4 156 F r ¡ 2 ; ¡ 4 All h1 g2 0 . 5 6b 100 24 3 2 8 8 308 F r ¡ 2 ; ¡ 4 ; ¡ 6 All h1 g2 0 . 1 Notes : Col . 1 : task number . Col . 2 : minimal sequence length p . Col . 3 : minimal number of steps between most recent relevant input information and teacher signal . Col . 4 : number of cell blocks b . Col . 5 : block size s . Col . 6 : Number of input units in . Col . 7 : Number of output units out . Col . 8 : number of weights w . Col . 9 : c describes connectivity : F means “outputlayerreceivesconnectionsfrommemorycells ; memorycellsandgateunitsreceive connections from input units , memory cells and gate units” ; B means “each layer receives connections from all layers below . ” Col . 10 : Initial output gate bias ogb , where r stands for “randomly chosen from the interval [ ¡ 0 : 1 ; 0 : 1 ] ” and no og means “no output gate used . ” Col . 11 : initial input gate bias igb ( see Col . 10 ) . Col . 12 : which units have bias weights ? b1 stands for “all hidden units” , ga for “only gate units , ” and all for “all noninput units . ” Col . 13 : thefunction h , where id isidentityfunction , h1 islogisticsigmoidin [ ¡ 2 ; 2 ] . Col . 14 : the logistic function g , where g 1 is sigmoid in [ 0 ; 1 ] , g 2 in [ ¡ 1 ; 1 ] . Col . 15 : learning rate ﬁ . positive ( negative ) internal state . Once the input gate opens for the second time , so does the output gate , and the memory cell output is fed back to its own input . This causes . X ; Y / to be represented by a positive internal state , because X contributes to the new internal state twice ( via current internal state and cell output feedback ) . Similarly , . Y ; X / gets represented by a negative internal state . 5 . 7 Summary of Experimental Conditions . Tables 10 and 11 provide an overview of the most important LSTM parameters and architectural details for experiments 1 through 6 . The conditions of the simple experiments 2a 1766 Sepp Hochreiter and J¨urgen Schmidhuber Table 11 : Summary of Experimental Conditions for LSTM , Part II . ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) Task Select Interval Test Set Size Stopping Criterion Success 1 t1 [ ¡ 0 : 2 ; 0 : 2 ] 256 Training and test correctly pred . See text 2a t1 [ ¡ 0 : 2 ; 0 : 2 ] no test set After 5 million exemplars ABS ( 0 . 25 ) 2b t2 [ ¡ 0 : 2 ; 0 : 2 ] 10 , 000 After 5 million exemplars ABS ( 0 . 25 ) 2c t2 [ ¡ 0 : 2 ; 0 : 2 ] 10 , 000 After 5 million exemplars ABS ( 0 . 2 ) 3a t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST1 and ST2 ( see text ) ABS ( 0 . 2 ) 3b t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST1 and ST2 ( see text ) ABS ( 0 . 2 ) 3c t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST1 and ST2 ( see text ) See text 4 t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST3 ( 0 . 01 ) ABS ( 0 . 04 ) 5 t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 see text ABS ( 0 . 04 ) 6a t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST3 ( 0 . 1 ) ABS ( 0 . 3 ) 6b t3 [ ¡ 0 : 1 ; 0 : 1 ] 2560 ST3 ( 0 . 1 ) ABS ( 0 . 3 ) Notes : Col . 1 : task number . Col . 2 : training exemplar selelction , where t 1 stands for “ran - domly chosen form training set , ” t 2 for “randomly chosen from two classes , ” and t 3 for “randomly generated on line . ” Col . 3 : weight initialization interval . Col . 4 : test set size . Col . 5 : Stopping criterion for training , where ST 3 . ﬂ / stands for “average training error below ﬂ and the 2000 most recent sequences were processed correctly . ” Col . 6 : success ( correct classiﬁcation ) criterion , where ABS . ﬂ / stands for “absolute error of all output units at sequence end is below ﬂ . ” and 2b differ slightly from those of the other , more systematic experiments , due to historical reasons . 6 Discussion 6 . 1 Limitations of LSTM . † The particularly efﬁcient truncated backpropagation version of the LSTM algorithm will not easily solve problems similar to strongly delayed XOR problems , where the goal is to compute the XOR of two widely separated inputs that previously occurred somewhere in a noisy sequence . The reason is that storing only one of the inputs will not help to reduce the expected error ; the task is nondecomposable in the sense that it is impossible to reduce the error incrementally by ﬁrst solving an easier subgoal . In theory , this limitation can be circumvented by using the full gra - dient ( perhaps with additional conventional hidden units receiving input from the memory cells ) . But we do not recommend computing the full gradient for the following reasons : ( 1 ) It increases computa - tional complexity , ( 2 ) constant error ﬂow through CECs can be shown only for truncated LSTM , and ( 3 ) we actually did conduct a few exper - iments with nontruncated LSTM . There was no signiﬁcant difference to truncated LSTM , exactly because outside the CECs , error ﬂow tends Long Short - Term Memory 1767 to vanish quickly . For the same reason , full BPTT does not outperform truncated BPTT . † Each memory cell block needs two additional units ( input and output gate ) . In comparison to standard recurrent nets , however , this does not increase the number of weights by more than a factor of 9 : each conventional hidden unit is replaced by at most three units in the LSTM architecture , increasing the number of weights by a factor of 3 2 in the fully connected case . Note , however , that our experiments use quite comparable weight numbers for the architectures of LSTM and competing approaches . † Due to its constant error ﬂow through CECs within memory cells , LSTM generally runs into problems similar to those of feedforward nets’ seeing the entire input string at once . For instance , there are tasks that can be quickly solved by random weight guessing but not by the truncated LSTM algorithm with small weight initializations , such as the 500 - step parity problem ( see the introduction to section 5 ) . Here , LSTM’s problems are similar to the ones of a feedforward net with 500 inputs , trying to solve 500 - bit parity . Indeed LSTM typically behaves much like a feedforward net trained by backpropagation that sees the entire input . But that is also precisely why it so clearly outperforms previous approaches on many nontrivial tasks with signiﬁcant search spaces . † LSTM does not have any problems with the notion of recency that go beyond those of other approaches . All gradient - based approaches , however , suffer from a practical inability to count discrete time steps precisely . If it makes a difference whether a certain signal occurred 99 or 100 steps ago , then an additional counting mechanism seems necessary . Easier tasks , however , such as one that requires making a difference only between , say , 3 and 11 steps , do not pose any problems to LSTM . For instance , by generating an appropriate negative con - nection between memory cell output and input , LSTM can give more weight to recent inputs and learn decays where necessary . 6 . 2 Advantages of LSTM . † The constant error backpropagation within memory cells results in LSTM’s ability to bridge very long time lags in case of problems similar to those discussed above . † For long - time - lag problems such as those discussed in this article , LSTM can handle noise , distributed representations , and continuous values . In contrast to ﬁnite state automata or hidden Markov models , LSTM does not require an a priori choice of a ﬁnite number of states . In principle , it can deal with unlimited state numbers . 1768 Sepp Hochreiter and J¨urgen Schmidhuber † For problems discussed in this article , LSTM generalizes well , even if the positions of widely separated , relevant inputs in the input se - quence do not matter . Unlike previous approaches , ours quickly learns to distinguish between two or more widely separated occurrences of a particular element in an input sequence , without depending on ap - propriate short - time - lag training exemplars . † There appears to be no need for parameter ﬁne tuning . LSTM works well over a broad range of parameters such as learning rate , input gate bias , and output gate bias . For instance , to some readers the learn - ing rates used in our experiments may seem large . However , a large learning rate pushes the output gates toward zero , thus automatically countermanding its own negative effects . † The LSTM algorithm’s update complexity per weight and time step is essentially that of BPTT , namely , O . 1 / . This is excellent in comparison to other approaches such as RTRL . Unlike full BPTT , however , LSTM is local in both space and time . 7 Conclusion Each memory cell’s internal architecture guarantees constant error ﬂow within its CEC , provided that truncated backpropagation cuts off error ﬂow trying to leak out of memory cells . This represents the basis for bridging very long time lags . Two gate units learn to open and close access to error ﬂow within each memory cell’s CEC . The multiplicative input gate affords protection of the CEC from perturbation by irrelevant inputs . Similarly , the multiplicative output gate protects other units from perturbation by currently irrelevant memory contents . To ﬁnd out about LSTM’s practical limitations we intend to apply it to real - worlddata . Applicationareaswillincludetime - seriesprediction , music composition , and speech processing . It will also be interesting to augment sequence chunkers ( Schmidhuber , 1992b , 1993 ) by LSTM to combine the advantages of both . Appendix A . 1 Algorithm Details . In what follows , the index k ranges over output units , i ranges over hidden units , c j stands for the j th memory cell block , c vj denotes the v th unit of memory cell block c j , u ; l ; m stand for arbitrary units , and t ranges over all time steps of a given input sequence . The gate unit logistic sigmoid ( with range [ 0 ; 1 ] ) used in the experiments is f . x / D 1 1 C exp . ¡ x / . ( A . 1 ) Long Short - Term Memory 1769 The function h ( with range [ ¡ 1 ; 1 ] ) used in the experiments is h . x / D 2 1 C exp . ¡ x / ¡ 1 . ( A . 2 ) The function g ( with range [ ¡ 2 ; 2 ] ) used in the experiments is g . x / D 4 1 C exp . ¡ x / ¡ 2 . ( A . 3 ) A . 1 . 1 Forward Pass . The net input and the activation of hidden unit i are net i . t / D X u w iu y u . t ¡ 1 / ( A . 4 ) y i . t / D f i . net i . t / / . The net input and the activation of in j are net in j . t / D X u w inju y u . t ¡ 1 / ( A . 5 ) y in j . t / D f in j . net in j . t / / . The net input and the activation of out j are net out j . t / D X u w out j u y u . t ¡ 1 / ( A . 6 ) y out j . t / D f out j . net out j . t / / . The net input net c vj , the internal state s c vj , and the output activation y c vj of the v th memory cell of memory cell block c j are : net c vj . t / D X u w c vj u y u . t ¡ 1 / ( A . 7 ) s c v j . t / D s c v j . t ¡ 1 / C y in j . t / g ‡ net c v j . t / · y c vj . t / D y out j . t / h . s c vj . t / / . The net input and the activation of output unit k are net k . t / D X u : u not a gate w ku y u . t ¡ 1 / y k . t / D f k . net k . t / / . The backward pass to be described later is based on the following trun - cated backpropagation formulas . 1770 Sepp Hochreiter and J¨urgen Schmidhuber A . 1 . 2 Approximate Derivatives for Truncated Backpropagation . The trun - cated version ( see section 4 ) only approximates the partial derivatives , which is reﬂected by the … tr signs in the notation below . It truncates er - ror ﬂow once it leaves memory cells or gate units . Truncation ensures that there are no loops across which an error that left some memory cell through its input or input gate can reenter the cell through its output or output gate . This in turn ensures constant error ﬂow through the memory cell’s CEC . In the truncated backpropagation version , the following derivatives are replaced by zero : @ net in j . t / @ y u . t ¡ 1 / … tr 0 8 u ; @ net out j . t / @ y u . t ¡ 1 / … tr 0 8 u ; and @ net c j . t / @ y u . t ¡ 1 / … tr 0 8 u : Therefore we get @ y in j . t / @ y u . t ¡ 1 / D f 0 in j . net in j . t / / @ net in j . t / @ y u . t ¡ 1 / … tr 0 8 u ; @ y out j . t / @ y u . t ¡ 1 / D f 0 out j . net out j . t / / @ net out j . t / @ y u . t ¡ 1 / … tr 0 8 u ; and @ y c j . t / @ y u . t ¡ 1 / D @ y c j . t / @ net out j . t / @ net out j . t / @ y u . t ¡ 1 / C @ y c j . t / @ net in j . t / @ net in j . t / @ y u . t ¡ 1 / C @ y c j . t / @ net c j . t / @ net c j . t / @ y u . t ¡ 1 / … tr 0 8 u : Thisimpliesforall w lm notonconnectionsto c vj ; in j ; out j ( thatis , l 62 f c vj ; in j ; out j g ) : @ y c vj . t / @ w lm D X u @ y c vj . t / @ y u . t ¡ 1 / @ y u . t ¡ 1 / @ w lm … tr 0 : The truncated derivatives of output unit k are : @ y k . t / @ w lm D f 0 k . net k . t / / ˆ X u : u not a gate w ku @ y u . t ¡ 1 / @ w lm C – kl y m . t ¡ 1 / ! Long Short - Term Memory 1771 … tr f 0 k . net k . t / / 0 @ X j S j X v D 1 – c vj l w kc vj @ y c vj . t ¡ 1 / @ w lm + X j ‡ – injl C – out j l · S j X v D 1 w kc vj @ y c vj . t ¡ 1 / @ w lm C X i : i hidden unit w ki @ y i . t ¡ 1 / @ w lm C – kl y m . t ¡ 1 / ! D f 0 k . net k . t / / 8 > > > > > < > > > > > : y m . t ¡ 1 / l D k w kc vj @ y cvj . t ¡ 1 / @ w lm l D c v j P S j v D 1 w kc vj @ y cvj . t ¡ 1 / @ w lm l D in j OR l D out j P i : i hidden unit w ki @ y i . t ¡ 1 / @ w lm l otherwise ( A . 8 ) where – is the Kronecker delta ( – ab D 1 if a D b and 0 otherwise ) , and S j is the size of memory cell block c j . The truncated derivatives of a hidden unit i that is not part of a memory cell are : @ y i . t / @ w lm D f 0 i . net i . t / / @ net i . t / @ w lm … tr – li f 0 i . net i . t / / y m . t ¡ 1 / . ( A . 9 ) ( Here it would be possible to use the full gradient without affecting constant error ﬂow through internal states of memory cells . ) Cell block c j ’s truncated derivatives are : @ y in j . t / @ w lm D f 0 in j . net in j . t / / @ net in j . t / @ w lm … tr – injl f 0 in j . net in j . t / / y m . t ¡ 1 / . ( A . 10 ) @ y out j . t / @ w lm D f 0 out j . net out j . t / / @ net out j . t / @ w lm … tr – out j l f 0 out j . net out j . t / / y m . t ¡ 1 / . ( A . 11 ) @ s c vj . t / @ w lm D @ s c vj . t ¡ 1 / @ w lm C @ y in j . t / @ w lm g ‡ net c vj . t / · C y in j . t / g 0 ‡ net c vj . t / · @ net c vj . t / @ w lm … tr ‡ – injl C – c vj l · @ s c vj . t ¡ 1 / @ w lm C – injl @ y in j . t / @ w lm g ‡ net c vj . t / · C – c vj l y in j . t / g 0 ‡ net c vj . t / · @ net c vj . t / @ w lm 1772 Sepp Hochreiter and J¨urgen Schmidhuber D ‡ – injl C – c vj l · @ s c vj . t ¡ 1 / @ w lm C – injl f 0 in j . net in j . t / / g ‡ net c vj . t / · y m . t ¡ 1 / C – c vj l y in j . t / g 0 ‡ net c vj . t / · y m . t ¡ 1 / . ( A . 12 ) @ y c vj . t / @ w lm D @ y out j . t / @ w lm h . s c vj . t / / C h 0 . s c vj . t / / @ s c vj . t / @ w lm y out j . t / … tr – out j l @ y out j . t / @ w lm h . s c vj . t / / C ‡ – injl C – c vj l · h 0 . s c vj . t / / @ s c vj . t / @ w lm y out j . t / . ( A . 13 ) To update the system efﬁciently at time t , the only ( truncated ) derivatives that need to be stored at time t ¡ 1 are @ s c vj . t ¡ 1 / @ w lm ; where l D c vj or l D in j . A . 1 . 3 Backward Pass . We will describe the backward pass only for the particularly efﬁcient truncated gradient version of the LSTM algorithm . For simplicity we will use equal signs even where approximations are made according to the truncated backpropagation equations above . The squared error at time t is given by E . t / D X k : k output unit ‡ t k . t / ¡ y k . t / · 2 , ( A . 14 ) where t k . t / is output unit k ’s target at time t . Time t ’s contribution to w lm ’s gradient - based update with learning rate ﬁ is 1 w lm . t / D ¡ ﬁ @ E . t / @ w lm . ( A . 15 ) We deﬁne some unit l ’s error at time step t by e l . t / : D ¡ @ E . t / @ net l . t / . ( A . 16 ) Using ( almost ) standard backpropagation , we ﬁrst compute updates for weights to output units ( l D k ) , weights to hidden units ( l D i ) and weights Long Short - Term Memory 1773 to output gates ( l D out j ) . We obtain ( compare formulas A . 8 , A . 9 , and A . 11 ) : l D k ( output ) : e k . t / D f 0 k . net k . t / / ‡ t k . t / ¡ y k . t / · , ( A . 17 ) l D i ( hidden ) : e i . t / D f 0 i . net i . t / / X k : k output unit w ki e k . t / , ( A . 18 ) l D out j ( output gates ) : e out j . t / D f 0 out j . net out j . t / / 0 @ S j X v D 1 h . s c vj . t / / X k : k output unit w kc vj e k . t / 1 A . ( A . 19 ) For all possible l time t ’s contribution to w lm ’s update is 1 w lm . t / D ﬁ e l . t / y m . t ¡ 1 / . ( A . 20 ) The remaining updates for weights to input gates ( l D in j ) and to cell units ( l D c vj ) are less conventional . We deﬁne some internal state s c vj ’s error : e s cvj : D ¡ @ E . t / @ s c vj . t / D f out j . net out j . t / / h 0 . s c vj . t / / X k : k output unit w kc vj e k . t / . ( A . 21 ) We obtain for l D in j or l D c vj ; v D 1 ; : : : ; S j ¡ @ E . t / @ w lm D S j X v D 1 e s cvj . t / @ s c vj . t / @ w lm . ( A . 22 ) The derivatives of the internal states with respect to weights and the corresponding weight updates are as follows ( compare expression A . 12 ) : l D in j ( input gates ) : @ s c vj . t / @ w injm D @ s c vj . t ¡ 1 / @ w injm C g . net c vj . t / / f 0 in j . net in j . t / / y m . t ¡ 1 / ; ( A . 23 ) therefore , time t ’scontributionto w injm ’supdateis ( compareexpressionA . 8 ) : 1 w injm . t / D ﬁ S j X v D 1 e s cvj . t / @ s c vj . t / @ w injm . ( A . 24 ) 1774 Sepp Hochreiter and J¨urgen Schmidhuber Similarly we get ( compare expression A . 12 ) : l D c vj ( memory cells ) : @ s c vj . t / @ w c vj m D @ s c vj . t ¡ 1 / @ w c vj m C g 0 . net c vj . t / / f in j . net in j . t / / y m . t ¡ 1 / ; ( A . 25 ) therefore time t ’s contribution to w c vj m ’s update is ( compare expression A . 8 ) : 1 w c vj m . t / D ﬁ e s cvj . t / @ s c vj . t / @ w c vj m . ( A . 26 ) All we need to implement for the backward pass are equations A . 17 through A . 21 and A . 23 through A . 26 . Each weight’s total update is the sum of the contributions of all time steps . A . 1 . 4 Computational Complexity . LSTM’s update complexity per time step is O . KH C KCS C HI C CSI / D O . W / ; ( A . 27 ) where K is the number of output units , C is the number of memory cell blocks , S > 0 is the size of the memory cell blocks , H is the number of hidden units , I isthe ( maximal ) numberofunitsforwardconnectedtomemorycells , gate units and hidden units , and W D KH C KCS C CSI C 2 CI C HI D O . KH C KCS C CSI C HI / is the number of weights . Expression A . 27 is obtained by considering all computationsofthebackwardpass : equationA . 17needs K steps ; A . 18needs KH steps ; A . 19 needs KSC steps ; A . 20 needs K . H C C / steps for output units , HI steps for hidden units , CI steps for output gates ; A . 21 needs KCS steps ; A . 23 needs CSI steps ; A . 24 needs CSI steps ; A . 25 needs CSI steps ; A . 26 needs CSI steps . The total is K C 2 KH C KC C 2 KSC C HI C CI C 4 CSI steps , or O . KH C KSC C HI C CSI / steps . We conclude that LSTM algorithm’s update complexity per time step is just like BPTT’s for a fully recurrent net . At a given time step , only the 2 CSI most recent @ s c vj = @ w lm values from equations A . 23 and A . 25 need to be stored . Hence LSTM’s storage complex - ity also is O . W / ; it does not depend on the input sequence length . A . 2 Error Flow . We compute how much an error signal is scaled while ﬂowing back through a memory cell for q time steps . As a by - product , this analysis reconﬁrms that the error ﬂow within a memory cell’s CEC is indeed constant , provided that truncated backpropagation cuts off error ﬂow trying to leave memory cells ( see also section 3 . 2 ) . The analysis also highlights a Long Short - Term Memory 1775 potential for undesirable long - term drifts of s c j , as well as the beneﬁcial , countermanding inﬂuence of negatively biased input gates . Using the truncated backpropagation learning rule , we obtain @ s c j . t ¡ k / @ s c j . t ¡ k ¡ 1 / D 1 C @ y in j . t ¡ k / @ s c j . t ¡ k ¡ 1 / g ¡ net c j . t ¡ k / ¢ C y in j . t ¡ k / g 0 ¡ net c j . t ¡ k / ¢ @ net c j . t ¡ k / @ s c j . t ¡ k ¡ 1 / D 1 C X u " @ y in j . t ¡ k / @ y u . t ¡ k ¡ 1 / @ y u . t ¡ k ¡ 1 / @ s c j . t ¡ k ¡ 1 / # £ g ¡ net c j . t ¡ k / ¢ C y in j . t ¡ k / g 0 ¡ net c j . t ¡ k / ¢ £ X u " @ net c j . t ¡ k / @ y u . t ¡ k ¡ 1 / @ y u . t ¡ k ¡ 1 / @ s c j . t ¡ k ¡ 1 / # … tr 1 : ( A . 28 ) The … tr sign indicates equality due to the fact that truncated backpropaga - tion replaces by zero the following derivatives : @ y in j . t ¡ k / @ y u . t ¡ k ¡ 1 / 8 u and @ net c j . t ¡ k / @ y u . t ¡ k ¡ 1 / 8 u : In what follows , an error # j . t / starts ﬂowing back at c j ’s output . We re - deﬁne # j . t / : D X i w ic j # i . t C 1 / . ( A . 29 ) Following the deﬁnitions and conventions of section 3 . 1 , we compute error ﬂow for the truncated backpropagation learning rule . The error occur - ring at the output gate is # out j . t / … tr @ y out j . t / @ net out j . t / @ y c j . t / @ y out j . t / # j . t / . ( A . 30 ) The error occurring at the internal state is # s cj . t / D @ s c j . t C 1 / @ s c j . t / # s cj . t C 1 / C @ y c j . t / @ s c j . t / # j . t / . ( A . 31 ) Since we use truncated backpropagation we have # j . t / D X i : i no gate and no memory cell w ic j # i . t C 1 / I 1776 Sepp Hochreiter and J¨urgen Schmidhuber therefore we get @ # j . t / @ # s cj . t C 1 / D X i w ic j @ # i . t C 1 / @ # s cj . t C 1 / … tr 0 . ( A . 32 ) Equations A . 31 and A . 32 imply constant error ﬂow through internal states of memory cells : @ # s cj . t / @ # s cj . t C 1 / D @ s c j . t C 1 / @ s c j . t / … tr 1 . ( A . 33 ) The error occurring at the memory cell input is # c j . t / D @ g . net c j . t / / @ net c j . t / @ s c j . t / @ g . net c j . t / / # s cj . t / . ( A . 34 ) The error occurring at the input gate is # in j . t / … tr @ y in j . t / @ net in j . t / @ s c j . t / @ y in j . t / / # s cj . t / . ( A . 35 ) A . 2 . 1 No External Error Flow . Errors are propagated back from units l to unit v along outgoing connections with weights w lv . This “external error” ( note that for conventional units there is nothing but external error ) at time t is # ev . t / D @ y v . t / @ net v . t / X l @ net l . t C 1 / @ y v . t / # l . t C 1 / . ( A . 36 ) We obtain @ # ev . t ¡ 1 / @ # j . t / D @ y v . t ¡ 1 / @ net v . t ¡ 1 / (cid:181) @ # out j . t / @ # j . t / @ net out j . t / @ y v . t ¡ 1 / C @ # in j . t / @ # j . t / @ net in j . t / @ y v . t ¡ 1 / C @ # c j . t / @ # j . t / @ net c j . t / @ y v . t ¡ 1 / ¶ … tr 0 . ( A . 37 ) We observe that the error # j arriving at the memory cell output is not back - propagated to units v by external connections to in j ; out j ; c j . A . 2 . 2 Error Flow Within Memory Cells . We now focus on the error back - ﬂow within a memory cell’s CEC . This is actually the only type of error ﬂow that can bridge several time steps . Suppose error # j . t / arrives at c j ’s output Long Short - Term Memory 1777 at time t and is propagated back for q steps until it reaches in j or the memory cell input g . net c j / . It is scaled by a factor of @ # v . t ¡ q / @ # j . t / ; where v D in j ; c j . We ﬁrst compute @ # s cj . t ¡ q / @ # j . t / … tr 8 > < > : @ y cj . t / @ s cj . t / q D 0 @ s cj . t ¡ q C 1 / @ s cj . t ¡ q / @ # scj . t ¡ q C 1 / @ # j . t / q > 0 . ( A . 38 ) Expanding equation A . 38 , we obtain @ # v . t ¡ q / @ # j . t / … tr @ # v . t ¡ q / @ # s cj . t ¡ q / @ # s cj . t ¡ q / @ # j . t / … tr @ # v . t ¡ q / @ # s cj . t ¡ q / ˆ 1 Y m D q @ s c j . t ¡ m C 1 / @ s c j . t ¡ m / ! @ y c j . t / @ s c j . t / … tr y out j . t / h 0 . s c j . t / / ( g 0 . net c j . t ¡ q / y in j . t ¡ q / v D c j g . net c j . t ¡ q / f 0 in j . net in j . t ¡ q / / v D in j . ( A . 39 ) Consider the factors in the previous equation’s last expression . Obvi - ously , error ﬂow is scaled only at times t ( when it enters the cell ) and t ¡ q ( when it leaves the cell ) , but not in between ( constant error ﬂow through the CEC ) . We observe : 1 . The output gate’s effect is y out j . t / scales down those errors that can be reduced early during training without using the memory cell . It also scalesdownthoseerrorsresultingfromusing ( activating / deactivating ) the memory cell at later training stages . Without the output gate , the memory cell might , for instance , suddenly start causing avoidable er - rors in situations that already seemed under control ( because it was easy to reduce the corresponding errors without memory cells ) . See “Output Weight Conﬂict” in section 3 and “Abuse Problem and Solu - tion” ( section 4 . 7 ) . 2 . If there are large positive or negative s c j . t / values ( because s c j has drifted since time step t ¡ q ) , then h 0 . s c j . t / / may be small ( assuming that h is a logistic sigmoid ) . See section 4 . Drifts of the memory cell’s internalstate s c j canbecountermandedbynegativelybiasingtheinput gate in j ( see section 4 and the next point ) . Recall from section 4 that the precise bias value does not matter much . 3 . y in j . t ¡ q / and f 0 in j . net in j . t ¡ q / / are small if the input gate is negatively biased ( assume f in j is a logistic sigmoid ) . However , the potential sig - 1778 Sepp Hochreiter and J¨urgen Schmidhuber niﬁcance of this is negligible compared to the potential signiﬁcance of drifts of the internal state s c j . Some of the factors above may scale down LSTM’s overall error ﬂow , but not in a manner that depends on the length of the time lag . The ﬂow will still be much more effective than an exponentially ( of order q ) decaying ﬂow without memory cells . Acknowledgments ThankstoMikeMozer , WilfriedBrauer , NicSchraudolph , andseveralanony - mous referees for valuable comments and suggestions that helped to im - prove a previous version of this article ( Hochreiter and Schmidhuber , 1995 ) . This work was supported by DFG grant SCHM 942 / 3 - 1 from Deutsche Forschungsgemeinschaft . References Almeida , L . B . ( 1987 ) . A learning rule for asynchronous perceptrons with feed - back in a combinatorial environment . In IEEE 1st International Conference on Neural Networks , San Diego ( Vol . 2 , pp . 609 – 618 ) . Baldi , P . , & Pineda , F . ( 1991 ) . Contrastive learning and neural oscillator . Neural Computation , 3 , 526 – 545 . Bengio , Y . , & Frasconi , P . ( 1994 ) . Credit assignment through time : Alternatives to backpropagation . In J . D . Cowan , G . Tesauro , & J . Alspector ( Eds . ) , Advances in neural information processing systems 6 ( pp . 75 – 82 ) . San Mateo , CA : Morgan Kaufmann . Bengio , Y . , Simard , P . , & Frasconi , P . ( 1994 ) . Learning long - term dependencies with gradient descent is difﬁcult . IEEE Transactions on Neural Networks , 5 ( 2 ) , 157 – 166 . Cleeremans , A . , Servan - Schreiber , D . , & McClelland , J . L . ( 1989 ) . Finite - state automata and simple recurrent networks . Neural Computation , 1 , 372 – 381 . de Vries , B . , & Principe , J . C . ( 1991 ) . A theory for neural networks with time delays . In R . P . Lippmann , J . E . Moody , & D . S . Touretzky ( Eds . ) , Advances in neural information processing systems 3 , ( pp . 162 – 168 ) . San Mateo , CA : Morgan Kaufmann . Doya , K . ( 1992 ) . Bifurcations in the learning of recurrent neural networks . In Proceedings of 1992 IEEE International Symposium on Circuits and Systems ( pp . 2777 – 2780 ) . Doya , K . , & Yoshizawa , S . ( 1989 ) . Adaptive neural oscillator using continuous - time backpropagation learning . Neural Networks , 2 , 375 – 385 . Elman , J . L . ( 1988 ) . Findingstructureintime ( Tech . Rep . No . CRL8801 ) . SanDiego : Center for Research in Language , University of California , San Diego . Fahlman , S . E . ( 1991 ) . The recurrent cascade - correlation learning algorithm . In R . P . Lippmann , J . E . Moody , & D . S . Touretzky ( Eds . ) , Advances in neural infor - mation processing systems 3 ( pp . 190 – 196 ) . San Mateo , CA : Morgan Kaufmann . Long Short - Term Memory 1779 Hochreiter , J . ( 1991 ) . Untersuchungen zu dynamischen neuronalen Netzen . Diploma thesis , Institut f¨ur Informatik , Lehrstuhl Prof . Brauer , Technische Universit¨at M¨unchen . See http : / / www7 . informatik . tu - muenchen . de / ˜hochreit . Hochreiter , S . , & Schmidhuber , J . ( 1995 ) . Long short - term memory ( Tech . Rep . No . FKI - 207 - 95 ) . Fakult¨at f¨ur Informatik , Technische Universit¨at M¨unchen . Hochreiter , S . , & Schmidhuber , J . ( 1996 ) . Bridging long time lags by weight guessing and “long short - term memory . ” In F . L . Silva , J . C . Principe , & L . B . Almeida ( Eds . ) , Spatiotemporal models in biological and artiﬁcial systems ( pp . 65 – 72 ) . Amsterdam : IOS Press . Hochreiter , S . , & Schmidhuber , J . ( 1997 ) . LSTM can solve hard long time lag problems . In Advances in neural information processing systems 9 . Cambridge , MA : MIT Press . Lang , K . , Waibel , A . , & Hinton , G . E . ( 1990 ) . A time - delay neural network archi - tecture for isolated word recognition . Neural Networks , 3 , 23 – 43 . Lin , T . , Horne , B . G . , Tino , P . , & Giles , C . L . ( 1996 ) . Learning long - term de - pendencies in NARX recurrent neural networks . IEEE Transactions on Neural Networks , 7 , 1329 – 1338 . Miller , C . B . , & Giles , C . L . ( 1993 ) . Experimental comparison of the effect of order in recurrent neural networks . International Journal of Pattern Recognition and Artiﬁcial Intelligence , 7 ( 4 ) , 849 – 872 . Mozer , M . C . ( 1989 ) . A focused back - propagation algorithm for temporal se - quence recognition . Complex Systems , 3 , 349 – 381 . Mozer , M . C . ( 1992 ) . Induction of multiscale temporal structure . In J . E . Moody , S . J . Hanson , & R . P . Lippman ( Eds . ) , Advances in neural information processing systems 4 ( pp . 275 – 282 ) . San Mateo , CA : Morgan Kaufmann . Pearlmutter , B . A . ( 1989 ) . Learning state space trajectories in recurrent neural networks . Neural Computation , 1 ( 2 ) , 263 – 269 . Pearlmutter , B . A . ( 1995 ) . Gradient calculations for dynamic recurrent neural networks : A survey . IEEE Transactions on Neural Networks , 6 ( 5 ) , 1212 – 1228 . Pineda , F . J . ( 1987 ) . Generalization of back - propagation to recurrent neural net - works . Physical Review Letters , 19 ( 59 ) , 2229 – 2232 . Pineda , F . J . ( 1988 ) . Dynamics and architecture for neural computation . Journal of Complexity , 4 , 216 – 245 . Plate , T . A . ( 1993 ) . Holographic recurrent networks . In S . J . Hanson , J . D . Cowan , & C . L . Giles ( Eds . ) , Advances in neural information processing systems 5 ( pp . 34 – 41 ) . San Mateo , CA : Morgan Kaufmann . Pollack , J . B . ( 1991 ) . Language induction by phase transition in dynamical rec - ognizers . In R . P . Lippmann , J . E . Moody , & D . S . Touretzky ( Eds . ) , Advances in neural information processing systems 3 ( pp . 619 – 626 ) . San Mateo , CA : Morgan Kaufmann . Puskorius , G . V . , and Feldkamp , L . A . ( 1994 ) . Neurocontrol of nonlinear dynam - ical systems with Kalman ﬁlter trained recurrent networks . IEEE Transactions on Neural Networks , 5 ( 2 ) , 279 – 297 . Ring , M . B . ( 1993 ) . Learning sequential tasks by incrementally adding higher orders . In S . J . Hanson , J . D . Cowan , & C . L . Giles ( Eds . ) , Advances in neu - ral information processing systems 5 ( pp . 115 – 122 ) . San Mateo , CA : Morgan Kaufmann . 1780 Sepp Hochreiter and J¨urgen Schmidhuber Robinson , A . J . , & Fallside , F . ( 1987 ) . The utility driven dynamic error propagation network ( Tech . Rep . No . CUED / F - INFENG / TR . 1 ) . Cambridge : Cambridge University Engineering Department . Schmidhuber , J . ( 1989 ) . A local learning algorithm for dynamic feedforward and recurrent networks . Connection Science , 1 ( 4 ) , 403 – 412 . Schmidhuber , J . ( 1992a ) . A ﬁxed size storage O . n 3 / time complexity learning algorithm for fully recurrent continually running networks . Neural Compu - tation , 4 ( 2 ) , 243 – 248 . Schmidhuber , J . ( 1992b ) . Learning complex , extended sequences using the prin - ciple of history compression . Neural Computation , 4 ( 2 ) , 234 – 242 . Schmidhuber , J . ( 1992c ) . Learningunambiguousreducedsequencedescriptions . In J . E . Moody , S . J . Hanson , & R . P . Lippman ( Eds . ) , Advances in neural infor - mation processing systems 4 ( pp . 291 – 298 ) . San Mateo , CA : Morgan Kaufmann . Schmidhuber , J . ( 1993 ) . Netzwerkarchitekturen , Zielfunktionen und Kettenregel . Ha - bilitationsschrift , Institut f¨ur Informatik , Technische Universit¨at M¨unchen . Schmidhuber , J . , & Hochreiter , S . ( 1996 ) . Guessing can outperform many long time lag algorithms ( Tech . Rep . No . IDSIA - 19 - 96 ) . Lugano , Switzerland : Instituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale . Silva , G . X . , Amaral , J . D . , Langlois , T . , & Almeida , L . B . ( 1996 ) . Faster training of recurrent networks . In F . L . Silva , J . C . Principe , & L . B . Almeida ( Eds . ) , Spa - tiotemporal models in biological and artiﬁcial systems ( pp . 168 – 175 ) . Amsterdam : IOS Press . Smith , A . W . , & Zipser , D . ( 1989 ) . Learning sequential structures with the real - time recurrent learning algorithm . International Journal of Neural Systems , 1 ( 2 ) , 125 – 131 . Sun , G . , Chen , H . , & Lee , Y . ( 1993 ) . Time warping invariant neural networks . In S . J . Hanson , J . D . Cowan , & C . L . Giles ( Eds . ) , Advances in neural information processing systems 5 ( pp . 180 – 187 ) . San Mateo , CA : Morgan Kaufmann . Watrous , R . L . , & Kuhn , G . M . ( 1992 ) . Induction of ﬁnite - state languages using second - order recurrent networks . Neural Computation , 4 , 406 – 414 . Werbos , P . J . ( 1988 ) . Generalization of backpropagation with application to a recurrent gas market model . Neural Networks , 1 , 339 – 356 . Williams , R . J . ( 1989 ) . Complexity of exact gradient computation algorithms for re - current neural networks ( Tech . Rep . No . NU - CCS - 89 - 27 ) . Boston : Northeastern University , College of Computer Science . Williams , R . J . & Peng , J . ( 1990 ) . Anefﬁcientgradient - basedalgorithmforon - line training of recurrent network trajectories . Neural Computation , 4 , 491 – 501 . Williams , R . J . , & Zipser , D . ( 1992 ) . Gradient - based learning algorithms for recurrent networks and their computational complexity . In Y . Chauvin , & D . E . Rumelhart ( Eds . ) , Back - propagation : Theory , architectures and applications . Hillsdale , NJ : Erlbaum . Received August 28 , 1995 ; accepted February 24 , 1997 .