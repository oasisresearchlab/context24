Human - AI Interaction in the Presence of Ambiguity From Deliberation - based Labeling to Ambiguity - aware AI by Mike Schaekermann A thesis presented to the University of Waterloo in fulﬁllment of the thesis requirement for the degree of Doctor of Philosophy in Computer Science Waterloo , Ontario , Canada , 2020 c (cid:13) Mike Schaekermann 2020 Examining Committee Membership The following served on the Examining Committee for this thesis . The decision of the Examining Committee is by majority vote . External Examiner : Loren Terveen Professor , Department of Computer Science and Engineering , University of Minnesota Supervisors : Edith Law Associate Professor , David R . Cheriton School of Computer Science , University of Waterloo Kate Larson Professor , David R . Cheriton School of Computer Science , University of Waterloo Internal Member : Daniel Vogel Associate Professor , David R . Cheriton School of Computer Science , University of Waterloo Internal - External James R . Wallace Member : Associate Professor , School of Public Health and Health Systems , University of Waterloo Other Member ( s ) : Oliver Schneider Assistant Professor , Department of Management Sciences , University of Waterloo ii Author’s Declaration I hereby declare that I am the sole author of this thesis . This is a true copy of the thesis , including any required ﬁnal revisions , as accepted by my examiners . I understand that my thesis may be made electronically available to the public . iii Statement of Contributions This dissertation includes ﬁrst - authored peer - reviewed material that has appeared in con - ference and journal proceedings published by the Association for Computing Machinery ( ACM ) and by the Association for Research in Vision and Ophthalmology ( ARVO ) . The ACM’s policy on reuse of published materials in a dissertation is as follows 1 : “Authors can include partial or complete papers of their own ( and no fee is expected ) in a dissertation as long as citations and DOI pointers to the Versions of Record in the ACM Digital Library are included . ” ARVO’s publication license contains the following statement 2 : “The Author ( s ) shall retain the non - exclusive right to any use of the Work , so long as the Author ( s ) provide ( s ) attribution to the place of original publication . ” The following list serves as a declaration of the Versions of Record for works included in this dissertation : Portions of Chapter 3 : Mike Schaekermann , Joslin Goh , Kate Larson , and Edith Law . 2018 . Resolvable vs . Irresolvable Disagreement : A Study on Worker Deliberation in Crowd Work . Proc . ACM Hum . - Comput . Interact . 2 , CSCW , Article 154 ( November 2018 ) , 19 pages . DOI = 10 . 1145 / 3274423 https : / / doi . org / 10 . 1145 / 3274423 Mike Schaekermann , Graeme Beaton , Minahz Habib , Andrew Lim , Kate Larson , and Edith Law . 2019 . Understanding Expert Disagreement in Medical Data Analysis through Struc - tured Adjudication . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 76 ( November 2019 ) , 23 pages . DOI = 10 . 1145 / 3359178 https : / / doi . org / 10 . 1145 / 3359178 1 https : / / authors . acm . org / author - resources / author - rights . Accessed on March 24 , 2020 . 2 http : / / arvojournals . org / DocumentLibrary / ARVOLicensetoPublish . pdf . Accessed on March 24 , 2020 . iv Mike Schaekermann , Naama Hammel , Michael Terry , Tayyeba K . Ali , Yun Liu , Brian Basham , Bilson Campana , William Chen , Ji Xiang , Jonathan Krause , Greg S . Corrado , Lily Peng , Dale R . Webster , Edith Law , and Rory Sayres . Remote Tool - Based Adjudication for Grading Diabetic Retinopathy . Trans Vis Sci Tech . 2019 ; 8 ( 6 ) : 40 . DOI = 10 . 1167 / tvst . 8 . 6 . 40 https : / / doi . org / 10 . 1167 / tvst . 8 . 6 . 40 Portions of Chapter 4 : Mike Schaekermann , Carrie J . Cai , Abigail E . Huang , and Rory Sayres . 2020 . Expert Discussions Improve Comprehension of Diﬃcult Cases in Medical Image Assessment . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . ACM , New York , NY , USA , Paper 163 , 13 pages . DOI = 10 . 1145 / 3313831 . 3376290 https : / / doi . org / 10 . 1145 / 3313831 . 3376290 Portions of Chapter 5 : Mike Schaekermann , Graeme Beaton , Elaheh Sanoubari , Andrew Lim , Kate Larson , and Edith Law . 2020 . Ambiguity - aware AI Assistants for Medical Data Analysis . In Proceed - ings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . ACM , New York , NY , USA , Paper 379 , 14 pages . DOI = 10 . 1145 / 3313831 . 3376506 https : / / doi . org / 10 . 1145 / 3313831 . 3376506 v Abstract Ambiguity , the quality of being open to more than one interpretation , permeates our lives . It comes in diﬀerent forms including linguistic and visual ambiguity , arises for var - ious reasons and gives rise to disagreements among human observers that can be hard or impossible to resolve . As artiﬁcial intelligence ( AI ) is increasingly infused into complex domains of human decision making it is crucial that the underlying AI mechanisms also support a notion of ambiguity . Yet , existing AI approaches typically assume that there is a single correct answer for any given input , lacking mechanisms to incorporate diverse human perspectives in various parts of the AI pipeline , including data labeling , model development and user interface design . This dissertation aims to shed light on the question of how humans and AI can be eﬀective partners in the presence of ambiguous problems . To address this question , we begin by studying group deliberation as a tool to detect and analyze ambiguous cases in data labeling . We present three case studies that investigate group deliberation in the context of diﬀerent labeling tasks , data modalities and types of human labeling expertise . First , we present CrowdDeliberation , an online platform for synchronous group delib - eration in novice crowd work , and show how worker deliberation aﬀects resolvability and accuracy in text classiﬁcation tasks of varying subjectivity . We then translate our ﬁndings to the expert domain of medical image classiﬁcation to demonstrate how imposing addi - tional structure on deliberation arguments can improve the eﬃciency of the deliberation process without compromising its reliability . Finally , we present CrowdEEG , an online platform for collaborative annotation and deliberation of medical time series data , imple - menting an asynchronous and highly structured deliberation process . Our ﬁndings from an observational study with 36 sleep health professionals help explain how disagreements arise and when they can be resolved through group deliberation . Beyond investigating group deliberation within data labeling , we also demonstrate how the resulting deliberation data can be used to support both human and artiﬁcial intelli - gence . To this end , we ﬁrst present results from a controlled experiment with ten medical generalists , suggesting that reading deliberation data from medical specialists signiﬁcantly improves generalists’ comprehension and diagnostic accuracy on diﬃcult patient cases . Second , we leverage deliberation data to simulate and investigate AI assistants that not only highlight ambiguous cases , but also explain the underlying sources of ambiguity to end users in human - interpretable terms . We provide evidence suggesting that this form of ambiguity - aware AI can help end users to triage and trust AI - provided data classiﬁcations . We conclude by outlining the main contributions of this dissertation and directions for future research . vi Acknowledgements This thesis is dedicated to an indispensable bunch of awesome individuals : • all the amazing folks at the Human - Computer Interaction lab who have made this PhD a fun , crazy and rewarding experience for me — I’m proud to say that some of you have become friends for life ; • my parents Kerstin and Horst , my sister Helen and my brother Raoul , who have always sent their love and support from across the Atlantic Ocean ; • Ellie who has not only been the best and most loving partner in crime I could wish for , but who has also co - authored the paper forming the basis for Chapter 5 ; • my advisors Edith Law and Kate Larson , who helped me navigate this unknown terrain with fantastic guidance and a never - ceasing smile on their faces ; • my committee members Loren Terveen , Daniel Vogel , James Wallace and Oliver Schneider , for providing valuable feedback in reﬁning this thesis ; • Rory Sayres and Michael Terry , who have been wonderful mentors to me at Google ; • Andrew Lim and Farrah Mateen , our incredible collaborators who have built the bridges needed to conduct research with medical experts ; • Rui de Sousa for his invaluable help in recruiting expert participants from all over the world ; • all of our study participants and human annotators enlisted through hospitals and crowdsourcing platforms ; • Dr . Rajiv Raman for permission to use the fundus photograph shown in Figure 3 . 7 ; • many other friends , colleagues and collaborators at the University of Waterloo and at Google without whom this research would not have been possible : William Callaghan , Alex Williams , Jessy Ceha , Jay Henderson , Kyle Robinson , Alexandra Vtyurina , Ba - hareh Sarrafzadeh , Graeme Beaton , Joslin Goh , Robin Cohen , Carrie Cai , Naama Hammel , Sonia Phene , Yun Liu , Abigail Huang , Abi Jones , Kasumi Widner , Cristhian Cruz , Quang Duong , Olga Kanzheleva , Lily Peng , Dale Webster . This research was supported by several grants including NSERC CHRP ( CHRP 478468 - 15 ) , CIHR CHRP ( CPG - 140200 ) and a Google PhD Fellowship . Thank you for making graduate school possible ! vii Table of Contents List of Tables xiii List of Figures xiv 1 Introduction 1 1 . 1 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1 . 2 Research Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1 . 3 Research Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1 . 3 . 1 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1 . 3 . 2 Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1 . 3 . 3 Deliberation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1 . 4 Thesis Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1 . 5 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2 Background Literature 9 2 . 1 Ambiguity in Human - AI Interaction . . . . . . . . . . . . . . . . . . . . . . 10 2 . 1 . 1 Ambiguity in Data Labeling . . . . . . . . . . . . . . . . . . . . . . 10 2 . 1 . 2 Ambiguity in Model Development . . . . . . . . . . . . . . . . . . . 11 2 . 1 . 3 Ambiguity in AI Interfaces . . . . . . . . . . . . . . . . . . . . . . . 12 2 . 2 Group Deliberation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 . 2 . 1 Deliberation Protocols . . . . . . . . . . . . . . . . . . . . . . . . . 13 viii 2 . 2 . 2 Deliberation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2 . 3 Medical Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2 . 3 . 1 Expert Disagreement in Medicine . . . . . . . . . . . . . . . . . . . 15 2 . 3 . 2 Group Deliberation in Medicine . . . . . . . . . . . . . . . . . . . . 16 2 . 3 . 3 Medical Diagnosis Training . . . . . . . . . . . . . . . . . . . . . . 17 2 . 3 . 4 AI - based Clinical Decision Support . . . . . . . . . . . . . . . . . . 18 3 Group Deliberation for Data Labeling 20 3 . 1 Crowd Deliberation for Text Labeling . . . . . . . . . . . . . . . . . . . . . 21 3 . 1 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3 . 1 . 2 Deliberation Workﬂow . . . . . . . . . . . . . . . . . . . . . . . . . 22 3 . 1 . 3 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3 . 1 . 4 Research Questions and Hypotheses . . . . . . . . . . . . . . . . . . 28 3 . 1 . 5 Experimental Conditions . . . . . . . . . . . . . . . . . . . . . . . . 29 3 . 1 . 6 Data and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3 . 1 . 7 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3 . 1 . 8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3 . 1 . 9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3 . 2 Expert Deliberation for Image Labeling . . . . . . . . . . . . . . . . . . . . 43 3 . 2 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3 . 2 . 2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3 . 2 . 3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3 . 2 . 4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3 . 2 . 5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3 . 3 Expert Deliberation for Time Series Labeling . . . . . . . . . . . . . . . . . 60 3 . 3 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 3 . 3 . 2 Application Domain . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3 . 3 . 3 Structured Adjudication . . . . . . . . . . . . . . . . . . . . . . . . 62 ix 3 . 3 . 4 Research Questions and Hypotheses . . . . . . . . . . . . . . . . . . 69 3 . 3 . 5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3 . 3 . 6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3 . 3 . 7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 3 . 3 . 8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 3 . 4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 4 Deliberation Data for Labeler Training 86 4 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 4 . 2 Application Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 4 . 3 Research Questions & Hypotheses . . . . . . . . . . . . . . . . . . . . . . . 89 4 . 4 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 4 . 4 . 1 Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 4 . 4 . 2 Image Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4 . 4 . 3 Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 4 . 4 . 4 Experimental Conditions . . . . . . . . . . . . . . . . . . . . . . . . 94 4 . 4 . 5 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4 . 5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4 . 5 . 1 Quantitative Insights . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4 . 5 . 2 Qualitative Insights . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 4 . 6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4 . 6 . 1 Impact on Comprehension and Accuracy . . . . . . . . . . . . . . . 103 4 . 6 . 2 Impact on Conﬁdence and Perceived Diﬃculty . . . . . . . . . . . . 104 4 . 6 . 3 Learning from Discussions . . . . . . . . . . . . . . . . . . . . . . . 104 4 . 6 . 4 Potential Clinical Impact . . . . . . . . . . . . . . . . . . . . . . . . 105 4 . 6 . 5 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 4 . 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 x 5 Deliberation Data for Ambiguity - aware AI 108 5 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 5 . 2 Ambiguity - aware AI Assistance . . . . . . . . . . . . . . . . . . . . . . . . 110 5 . 3 Research Questions and Hypotheses . . . . . . . . . . . . . . . . . . . . . . 111 5 . 4 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 5 . 4 . 1 Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 5 . 4 . 2 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 5 . 4 . 3 Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 5 . 4 . 4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 5 . 5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 5 . 5 . 1 Expert Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 5 . 5 . 2 Quantitative Insights . . . . . . . . . . . . . . . . . . . . . . . . . . 118 5 . 5 . 3 Qualitative Insights . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5 . 6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 5 . 6 . 1 Design Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 5 . 6 . 2 Generalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 5 . 6 . 3 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 5 . 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 6 Conclusion 126 6 . 1 Contributions and Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 6 . 2 Support for Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . 127 6 . 3 Design Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 6 . 4 Opportunities for Future Work . . . . . . . . . . . . . . . . . . . . . . . . . 133 6 . 5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 References 136 APPENDICES 154 xi A Group Deliberation for Data Labeling 155 A . 1 Crowd Deliberation for Text Labeling . . . . . . . . . . . . . . . . . . . . . 155 A . 1 . 1 Pre - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . 155 A . 1 . 2 Per - case Questionnaire after Independent Classiﬁcation . . . . . . . 156 A . 1 . 3 Per - case Questionnaire after Discussion Round 2 . . . . . . . . . . . 157 A . 1 . 4 Per - case Questionnaire after Viewing Final Decisions . . . . . . . . 158 A . 2 Expert Deliberation for Time Series Labeling . . . . . . . . . . . . . . . . . 158 A . 2 . 1 Pre - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . 158 A . 2 . 2 Post - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . 160 B Deliberation Data for Labeler Training 161 B . 1 Pre - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 B . 2 Per - case Questionnaire after Training Feedback . . . . . . . . . . . . . . . 163 B . 3 Post - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 C Deliberation Data for Ambiguity - aware AI 167 C . 1 Pre - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 C . 2 Post - condition Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . 169 C . 3 Post - study Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 xii List of Tables 3 . 1 Preset choices for sources of disagreement . . . . . . . . . . . . . . . . . . . 24 3 . 2 Anticipated sources of disagreement ( before discussion ) where the propor - tions of workers are signiﬁcantly diﬀerent . . . . . . . . . . . . . . . . . . . 32 3 . 3 Re - evaluated sources of disagreement ( after discussion ) where the propor - tions of workers are signiﬁcantly diﬀerent . . . . . . . . . . . . . . . . . . . 32 3 . 4 Logistic model for understanding the likelihood of resolving a case . . . . . . 34 3 . 5 Logistic model for understanding the likelihood of resolving a case correctly . 38 3 . 6 Baseline Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3 . 7 Comparison of adjudication procedures . . . . . . . . . . . . . . . . . . . . 47 3 . 8 Inter - panel agreement among all pairs of panels as Cohen’s kappa . . . . . . 52 3 . 9 Inter - panel agreement among all pairs of panels as exact agreement . . . . . 52 3 . 10 Inter - panel agreement among all pairs of panels as strikeout rate . . . . . . 52 3 . 11 Factors used as independent variables in Q1 and Q2 . . . . . . . . . . . . . 73 3 . 12 Logistic models for understanding why disagreements arise and why they persist after adjudication . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 5 . 1 Characteristics of patient records used by the AI assistants . . . . . . . . . . 114 6 . 1 Summary of hypotheses and their degree of support from Chapter 3 . . . . . 130 6 . 2 Summary of hypotheses and their degree of support from Chapters 4 and 5 . 131 xiii List of Figures 3 . 1 Input , output and stages of the Crowd Deliberation workﬂow . . . . . . . . 23 3 . 2 Screenshots of the Crowd Deliberation interface . . . . . . . . . . . . . . . 25 3 . 3 Aggregate performance of our worker population at predicting disagree - ment / agreement by task type . . . . . . . . . . . . . . . . . . . . . . . . . 33 3 . 4 Individual workers’ answer quality in the Relation task across diﬀerent re - consideration workﬂows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3 . 5 Process diagram illustrating remote , tool - based remote adjudication . . . . 48 3 . 6 Illustration of the round - robin approach for remote , tool - based adjudication 49 3 . 7 Grading interface for remote , tool - based adjudication . . . . . . . . . . . . 50 3 . 8 Number of review rounds required per case . . . . . . . . . . . . . . . . . . 53 3 . 9 Cumulative percentage of cases resolved per adjudication round . . . . . . 53 3 . 10 Mean number of review rounds required per rubric criterion in remote , tool - based adjudication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3 . 11 Interface for structured adjudication of classiﬁcation decisions in medical time series analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3 . 12 Agreement rate by adjudication round number and patient’s health condition 75 3 . 13 Number of times each feature type was mentioned in a rationale ( log scale ) 75 3 . 14 Change in diagnostic markers from before to after adjudication . . . . . . . 78 4 . 1 Task interface for medical image assessment . . . . . . . . . . . . . . . . . 93 4 . 2 Training feedback interface for medical generalists . . . . . . . . . . . . . . 94 4 . 3 Generalists’ perception of training feedback . . . . . . . . . . . . . . . . . . 97 xiv 4 . 4 Average change in generalists’ diagnostic accuracy per case - pair . . . . . . 98 4 . 5 Improvement in generalists’ self - eﬃcacy score for diagnosis of retinal artery occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4 . 6 Change in generalists’ diagnostic conﬁdence and perceived case diﬃculty per case - pair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 4 . 7 Example adjudication discussions with mixed and consistently high ratings for answer key comprehension . . . . . . . . . . . . . . . . . . . . . . . . . 103 5 . 1 Interface for conventional and ambiguity - aware AI assistants in medical data analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5 . 2 Proportion of contentious cases out of all cases reviewed . . . . . . . . . . 117 5 . 3 Experts’ correction rate for cases with ambiguity explanation . . . . . . . . 117 5 . 4 Experts’ preferences between both AI assistants . . . . . . . . . . . . . . . 119 5 . 5 Expert ratings for perceived integrity and conﬁdence . . . . . . . . . . . . 120 xv Chapter 1 Introduction Man must not attempt to dispel the ambiguity of his being but , on the contrary , accept the task of realizing it . — Simone de Beauvoir , The Ethics of Ambiguity Major advances in artiﬁcial intelligence ( AI ) have enabled a new era of decision sup - port , shifting from relatively constrained and well - deﬁned problems to more complex and nuanced domains . Many complex domains essential to our modern society , including jour - nalism , criminal justice , healthcare or science , are full of problems for which it is diﬃcult or impossible to ﬁnd a single correct answer to a given problem . For example , jurors can arrive at diﬀerent verdicts in light of the same evidence , or doctors can form conﬂicting diagnoses looking at the same medical image . At the heart of many of these disagreements lies ambiguity , the quality of being open to more than one interpretation . 1 An acceptable approach for decision making in the presence of ambiguous problems often requires elici - tation and synthesis of diverse human perspectives to inform more well - deﬁned , complete , balanced or ethical decisions . However , existing AI approaches typically assume that there is a single correct answer for any given input , lacking mechanisms to incorporate diverse human perspectives . This assumption is prevalent in various steps of the AI pipeline , including data labeling , model development and user interface ( UI ) design . In data labeling , it has motivated the notion that inter - rater disagreement is mere “noise in the signal” originating from human mistakes , and has thus given rise to post - processing techniques that eliminate disagreement . In model 1 https : / / en . oxforddictionaries . com / definition / ambiguity ( accessed on 12 May 2020 ) 1 development , it has favoured learning techniques and evaluation metrics that support no more than a single correct output for any given input . In UI design for AI applications , it has hampered the exploration of interfaces that communicate and explain ambiguous cases to end users . This thesis aims to shed light on the question of how humans and AI can be eﬀec - tive partners in the presence of ambiguous problems . We take the stance that inter - rater disagreement carries valuable information that can and should be captured to better un - derstand the structure of ambiguous problems . One primary contribution of this work is to introduce and study group deliberation in the context of data labeling , as a tool to detect and analyze ambiguous cases , and to either resolve inter - rater disagreements organically through deliberation , or otherwise mark them as irresolvable . In support of this contribution , we present insights from three case studies about collaborative online platforms that all integrate group deliberation into data labeling workﬂows , in the context of diﬀerent data modalities , labeling tasks and types of human labeler expertise . We further demonstrate how the resulting deliberation data can be used not only to enable other human labelers to calibrate their own reasoning for better comprehension and accuracy on diﬃcult cases , but also to investigate how AI can eﬀectively communicate ambiguity to end users . The remainder of this chapter introduces the central thesis of this dissertation and pro - vides an overview of our main contributions as well as the scope of the research conducted to support our thesis . 2 1 . 1 Thesis Statement This dissertation aims to defend the following thesis statement : Ambiguity , the quality of being open to more than one interpretation , permeates our lives . It can take various forms including linguistic and visual ambiguity , arise for various reasons including heterogeneous data or vague deﬁnitions , and give rise to inter - rater disagreements that can be hard or impossible to resolve . Human and artiﬁcial intelligence can beneﬁt from novel methods that aim to detect and explain instances of ambiguity . The expected advantages of such methods are a better understanding of why disagreement arises and when it can be resolved , as well as better approaches for handling ambiguity in human decision making—both unassisted and when assisted by artiﬁcial intelligence . To defend this statement , the remainder of this dissertation addresses the following research questions : 1 . How can we capture the structure of ambiguous problems in data labeling ? ( a ) How can group deliberation be used to analyze ambiguity in data labeling ? ( b ) How can group deliberation be integrated into non - expert crowdsourcing ? ( c ) How can group deliberation be integrated into expert labeling tasks ? 2 . How can we leverage deliberation data to improve human decision making ? ( a ) How can we use deliberation data as training material for human labelers ? ( b ) How do labelers perceive deliberation data as a form of training feedback ? ( c ) How does reading deliberation data aﬀect labeling decisions for future cases ? 3 . How can we leverage deliberation data to communicate ambiguity in AI output ? ( a ) How can we simulate an AI that detects and explains ambiguous data ? ( b ) How do end users perceive an ambiguity - aware AI assistant ? ( c ) How does an ambiguity - aware AI assistant aﬀect end users’ labeling decisions ? Next , we summarize the research contributions made through the work presented in this dissertation . 3 1 . 2 Research Contributions In this dissertation , we make the following research contributions : 1 . We introduce group deliberation as a tool to detect and explain ambiguity in data labeling . To this end , we implement and investigate deliberation workﬂows within three diﬀerent contexts : ( a ) First , we present Crowd Deliberation , an online platform for synchronous group deliberation in the context of non - expert crowd work . We study how worker deliberation aﬀects resolvability and accuracy using case studies with both an objective and a subjective task , involving 316 crowd workers . ( b ) Next , we translate our ﬁndings to the expert domain of medical image clas - siﬁcation to study asynchronous group deliberation among medical specialists . We present ﬁndings from an experiment with 15 retina specialists showing that structuring deliberation arguments around a set of low - level decision criteria improves the eﬃciency of the deliberation process . ( c ) Finally , we build on the ﬁndings from the ﬁrst two studies to create CrowdEEG , an online platform for collaborative annotation and deliberation of medical time series data . CrowdEEG implements an asynchronous and highly structured de - liberation process . We present ﬁndings from an observational study with 36 sleep technologists about factors contributing to disagreement and resolvability . 2 . We demonstrate how deliberation data can be used as training material to calibrate human expert labelers . To this end , we present results from a controlled ex - periment with ten medical generalists . Our results show that reading deliberation data produced by specialists substantially improves generalists’ comprehension and diagnostic accuracy on diﬃcult patient cases . 3 . We leverage deliberation data to simulate and investigate ambiguity - aware AI , i . e . , AI that not only detects , but also explains ambiguous data classiﬁcations . We present results from a controlled experiment with twelve sleep technologists suggest - ing that ambiguity - aware AI can improve the ability of end users to triage and trust AI - provided output , and that the relevance of AI - provided ambiguity explanations is crucial for expert end users to accurately disambiguate diﬃcult cases . 4 . We publish two novel datasets containing deliberation data in the context of both non - expert and expert classiﬁcation tasks . 4 1 . 3 Research Scope Here , we deﬁne the scope of research conducted as part of this dissertation . We clarify the problem we aim to solve , the tasks and domains in which we seek to provide support for the problem , and various aspects of the proposed solutions we investigated . 1 . 3 . 1 Tasks We aim to address the problem of better supporting ambiguity in human - AI collaborative tasks . Speciﬁcally , we focus on supervised learning settings for data classiﬁcation that rely on human - labeled data to train and evaluate AI algorithms . In this context , we are interested in better understanding and supporting scenarios where the same data instance is labeled by more than one human and where there is disagreement among those humans over the correct classiﬁcation label . In particular , we are interested in instances of disagreement that arise for reasons more profound than simple human mistakes and which are hard or impossible to resolve even when reviewed and discussed collectively as a group . While there exist labeling tasks that naturally allow multiple labels for a single data instance , e . g . , assigning more than one genre to a single movie title , we are interested in classiﬁcation tasks that aim for a single label per data instance , but that leave room for interpretation as to which the correct label should be . Within this scope , we study ways to support ambiguity in diﬀerent types of : • Classiﬁcation scales : We begin by studying binary classiﬁcation tasks and move on to multi - class classiﬁcation on both nominal and ordinal scales . • Data modalities : We investigate classiﬁcation ambiguity in the context of various data modalities including text documents , images and times series data . 1 . 3 . 2 Domains The issue of ambiguity is not constrained to interpretations of people that lack a speciﬁc training background for a given task . In fact , ambiguity and inter - rater disagreement are well known phenomena even in seemingly well - deﬁned expert domains . This dissertation investigates the issue of ambiguity in both of these worlds : • Novice domain : We begin by studying inter - rater disagreement and deliberation within the context of non - expert crowd work facilitated through crowdsourcing mar - ketplaces like Amazon’s Mechanical Turk . 5 • Expert domain : Later , we translate our ﬁndings to the expert domain of med - ical data interpretation . In particular , we present case studies from two separate expert tasks : ( 1 ) assessment of eye disease by eye care physicians based on retinal images , and ( 2 ) sleep stage classiﬁcations by trained sleep health professionals based on biosignal recordings from a sleep laboratory . 1 . 3 . 3 Deliberation Finally , we study group deliberation , our primary contribution for detecting and analyzing ambiguity , in diﬀerent shapes and forms . Speciﬁcally , we provide results about deliberation dynamics with varying : • Group size : While our initial study in focuses on groups of two or three members , the remainder of the dissertation relies on groups of three for the simple reason that a majority vote can be computed for disagreements among three , but not among two . • Workﬂow : In the context of non - expert crowd work , we study synchronous delib - eration conducted soon after the initial labeling task is completed because crowd workers quickly move on to other tasks and it can be hard to motivate workers to review disagreements at a later point in time . For our expert tasks , we facilitate an asynchronous deliberation workﬂow to better ﬁt review and discussion activities into the busy schedule of health professionals . • Argument structure : In the course of our three case studies , we evolve the way deliberation arguments are captured from an open - ended to a highly structured for - mat . The underlying motivation is twofold . On the one hand , our results suggest that imposing an explicit structure on the way arguments are delivered can make the deliberation process more eﬃcient . On the other hand , collecting structured data is useful for our other objective of simulating and investigating ambiguity - aware AI assistants . 6 1 . 4 Thesis Overview Here , we provide an overview of the research included in this dissertation . We begin by outlining the related literature that our work is situated within or builds on , followed by a series of chapters that contribute to our goal of better supporting ambiguity in human - AI collaborative workﬂows for data classiﬁcation , by capturing and leveraging information about instances ambiguity . Chapter 3 : We present three case studies of collaborative platforms that integrate group deliberation into data labeling as a tool to detect and analyze instances of ambiguous data . In the course of the chapter , we evolve the proposed deliberation methods in various ways : ( a ) we begin by studying deliberation within the novice domain of microtask crowd work and later translate our ﬁndings to two diﬀerent expert domains of medical data interpre - tation ; ( b ) in the context of switching from novice to expert work , we transition from a synchronous to an asynchronous deliberation workﬂow ; ( c ) in each case study , we incre - mentally impose additional structure on how our systems collect deliberation arguments to increase eﬃciency , facilitate quantitative analysis and ultimately make deliberation data accessible for use by AI assistants . We provide insights into why inter - rater disagreements arose , under what circumstances they could be resolved and on how deliberation aﬀected accuracy , eﬃciency and higher - level decision making . Chapter 4 : We present and study a novel approach based on the idea that deliberation data can be leveraged as training material for human labelers . In particular , we present qualitative and quantitative ﬁndings from a controlled experiment suggesting that medical generalists substantially beneﬁt from reading deliberation discussions from medical special - ists in the context of a complex diagnostic image assessment task . We demonstrate that reading deliberation discussions not only helps expert labelers calibrate their reasoning towards better comprehension on diﬃcult cases , but also towards improved accuracy on a held - out test dataset . We discuss the implications of our ﬁndings for increasing the pool of trained expert labelers and medical diagnostic training . Chapter 5 : We present and study an AI assistant that not only highlights , but also explains ambiguous data to end users in a human - AI collaborative data classiﬁcation task . We call this AI assistant “ambiguity - aware” and use a Wizard - of - Oz approach to study its eﬀects on the perception and behaviour of expert end users in a medical time series classiﬁcation task for sleep stage analysis . Our qualitative and quantitative results suggest that ambiguity - aware AI assistants can help end users better triage and trust AI - suggested classiﬁcation labels , and that the quality of AI - provided ambiguity explanations strongly aﬀects experts’ ability to disambiguate diﬃcult cases . 7 Chapter 6 : Finally , we synthesize our ﬁndings from the previous three chapters and contextualize the collective impact of the research presented towards the issue of ambiguity in human - AI interaction . We summarize the main contributions and takeaways of our research in this space and conclude by discussing directions of interest for future work . Appendices : • Appendix A : We provide questionnaires used to understand how deliberation was perceived by human labelers , why they disagreed and under what circumstances disagreements could be resolved in Chapter 3 . • Appendix B : We provide questionnaires used to understand how human labelers perceived deliberation data as a form of training feedback in Chapter 4 . • Appendix C : We provide questionnaires used to understand how expert end users perceived ambiguity - aware AI assistants in Chapter 5 . 1 . 5 Terminology Here , we list and deﬁne central terms used throughout the dissertation : 1 . Ambiguity — The quality of being open to more than one interpretation . 2 2 . Deliberation — The collaborative process of discussing contested issues by considering various perspectives in order to form opinions and guide judgement . 3 3 . Adjudication — Synonym of deliberation used in the context of medical data inter - pretation . 2 https : / / en . oxforddictionaries . com / definition / ambiguity ( accessed on 12 May 2020 ) 3 https : / / www . comm . pitt . edu / oral - comm - lab / argument - deliberation / argument - deliberation - introduction ( accessed on 12 May 2020 ) 8 Chapter 2 Background Literature In this chapter , we provide an overview on background literature relevant to the content of this dissertation . We begin by outlining the role of ambiguity and how it is currently handled in various steps of the AI pipeline , including data labeling , model development and user interfaces . We then introduce the reader to existing protocols and systems to facilitate group deliberation which will later serve as a tool to detect and analyze ambiguity in data classiﬁcation . Finally , we summarize related work from the particular domain of medical decision making and the role that ambiguity , deliberation and AI play in that context . Before we begin , we introduce our high - level perspective on the central terms used in this work and on their relationship to each other , including ambiguity , uncertainty , inter - rater disagreement and group deliberation . In the context of this dissertation , ambiguity is deﬁned as the quality of being open to more than one interpretation . As such it is a speciﬁc type of uncertainty arising from the circumstance that there may exist multiple conﬂicting , yet equally valid interpretations of the same phenomenon . However , there exist other forms of uncertainty , e . g . , uncertainty due to a lack of information about the current state of the world , or uncertainty around the precise consequences of a speciﬁc action for the future state of the world . Ambiguity and inter - rater disagreement are strongly linked to each other in that am - biguity typically reveals itself in the form of disagreement among multiple independent human observers . However , both concepts are not identical with each other , nor does one imply the other . For example , given an ambiguous case , a speciﬁc group of observers may happen to agree with each other while another combination of observers may have arrived at conﬂicting interpretations . Conversely , two conﬂicting interpretations are not necessarily also equally valid , e . g . , if one observer made a mistake that ought to be corrected . 9 This is where the process of group deliberation comes into play . It can help us un - derstand whether a given disagreement is either based on mistakes and can therefore be resolved or whether a disagreement is based on some form of underlying ambiguity and is therefore irresolvable . We thus consider classiﬁcation ambiguity the central problem this work aims to address and group deliberation the central method used to expose if and how an instance of inter - rater disagreement is linked to ambiguity . 2 . 1 Ambiguity in Human - AI Interaction 2 . 1 . 1 Ambiguity in Data Labeling Ambiguity and the associated issue of inter - rater disagreement in data classiﬁcation , are both topics that have received ample attention not only in the epistemological ( e . g . , [ 55 , 97 , 136 ] ) and medical ( e . g . , [ 10 , 103 , 114 ] ) literature , but also within the human - computer interaction ( HCI , e . g . , [ 25 , 29 , 64 ] ) , human computation ( e . g . , [ 9 , 42 , 93 ] ) , and computer - supported cooperative work ( CSCW , e . g . , [ 8 , 76 , 95 ] ) communities . Aroyo and Welty [ 9 ] view inter - rater disagreement as a function of three phenomena : variability among human annotators , characteristics of the data at hand , and the quality of the task instructions . We adopt a similar approach in exploring inter - rater disagreement in data classiﬁcation , and synthesize prior work within these three categories below . In addition , we take into account data presentation as another potential source of inter - rater disagreement , pertaining to diﬀerences in the way that human annotators view the data at hand . Annotator diﬀerences . Mumpower and Stewart [ 97 ] oﬀer an early theoretical ac - count in which expert disagreement is discussed in three forms : ( 1 ) personality - based disagreement , beget by expert ideology , venality , or incompetence , ( 2 ) judgement - based disagreement , due to information gaps , and ( 3 ) structural disagreement , due to experts holding diﬀerent organizing principles or problem deﬁnitions . Garbayo [ 55 ] distinguishes verbal disagreement—disagreement due to diﬀerences in terminology or semantics between experts with respect to the problem deﬁnitions mentioned previously—from legitimate disagreement , arising despite experts having access to the same evidence . Gurari and Grauman [ 64 ] found that disagreement among crowd workers in visual question answering tasks can arise from diﬀering levels of annotator expertise . Kairam and Heer [ 76 ] showed that inter - rater agreement in a crowdsourced entity annotation task is aﬀected by how conservatively or liberally workers follow task instructions . 10 Data characteristics . In addition to grader - speciﬁc factors , disagreement may be an indicator of ambiguity , vagueness , or complexity inherent in the given data [ 8 , 9 , 10 , 111 , 114 ] . Prior works have demonstrated that inter - rater disagreement can be associated with characteristics of individual data objects , including text documents for sentiment classiﬁcation [ 111 ] , photographs for visual question answering [ 64 ] , medical images for eye disease assessment [ 114 ] , and biomedical time series data for epilepsy diagnosis [ 10 ] . Task instructions . Finally , inter - rater disagreement has been attributed to ambiguous category deﬁnitions [ 9 , 25 , 64 , 93 ] relevant to a given task . Gurari and Grauman [ 64 ] identiﬁed subjective questions and vocabulary mismatch between crowd workers as sources of disagreement . Chang et al . [ 25 ] found that worker disagreement can arise due to ambiguous or incomplete category deﬁnitions , and proposed a system to analyze crowd - generated conceptual structures post - hoc . Manam and Quinn [ 93 ] developed workﬂows for identifying and reﬁning unclear instructions for crowdsourcing tasks . Our work builds on these prior contributions by studying various factors contributing to inter - rater disagreement in various settings of data classiﬁcation . Our quantitative analyses in Chapter 3 take into account various factors , including diﬀerences between graders ( in terms of professional credentials , geographic location , and work experience ) , data characteristics of individual disagreement cases ( e . g . , in terms of data complexity ) , and the role of classiﬁcation guidelines ( in terms of individual guideline rules ) to understand inter - rater disagreement . Furthermore , Section 3 . 3 contributes a novel perspective on the problem of inter - rater disagreement by incorporating data presentation as another potential factor contributing to disagreement . In particular , that study incorporates the question to what extent diﬀer - ences in how experts choose to view the data at hand—conﬁguring the viewer interface— may be associated with experts arriving at divergent interpretations of the same data . 2 . 1 . 2 Ambiguity in Model Development Prior systems have generally taken one of three approaches to the problem of ambiguity in AI - based data classiﬁcation : Eliminating Ambiguity . Traditional machine learning classiﬁcation methods elim - inate class diversity using automatic procedures like majority vote [ 75 ] , or expectation maximization [ 107 ] . These systems tend to view ambiguity as a proxy for noise to be reduced or eliminated in the data . [ 23 , 148 ] . Aggregating Multiple Outputs . Other systems retain disagreement labels for the purpose of training multiple models ( e . g . , one for each human labeler [ 62 ] ) ; these systems 11 typically produce multiple AI predictions which are aggregated into a single label before being presented to the end user . Label Distribution Learning . A more ambiguity - centric approach to data classiﬁ - cation is label distribution learning ( LDL ) [ 57 ] , where machines are trained to predict not just one label for a given case , but a distribution of possible classiﬁcation labels [ 32 , 112 ] . Standard LDL models will assign uncertainty estimates to their classiﬁcation outputs , pro - viding degrees of plausibility for each possible label . 2 . 1 . 3 Ambiguity in AI Interfaces The question of how systems should communicate or visually represent uncertainty to end users has received ample attention in the human - computer interaction ( HCI ) community [ 130 ] . Approaches include visualizing uncertainty as extrinsic annotation ( e . g . , conﬁdence intervals ) , abstract , continuous outcomes ( e . g , probability density plots ) , or hypothetical , discrete outcomes ( e . g . , natural frequencies or icon arrays ) [ 78 ] . Kay et . al [ 78 ] suggest that communicating uncertainty through discrete outcomes can improve decision making on the part of end users . Prior work has found that collecting explanations around ambiguous cases during data labeling workﬂows can be leveraged towards more ﬁne - grained and ﬂexible post - hoc data classiﬁcation [ 25 ] . Chen et al . [ 27 ] designed an AI - supported analytics tool for the pur - pose of qualitative coding in social science shifting the focus to identifying disagreement and ambiguity among groups of human coders . Galdran et al . [ 53 ] developed a system for vessel classiﬁcation from retinal images , with the ability to classify uncertain cases and pro - vide direct uncertainty estimates for its labels while achieving state - of - the - art classiﬁcation performance . In Chapter 5 , simulate and explore an ambiguity - aware AI assistant for medical data analysis , a system that provides human - interpretable rationales for all plausible classiﬁ - cation labels . While our AI assistant is simulated in the sense that it does not predict , but merely displays human - annotated data , our work contributes novel insights about how such an ambiguity - aware system aﬀects expert perception and behaviour . 2 . 2 Group Deliberation Instances of data ambiguity typically surface in the form of conﬂicting assessments among a group of independent human observers . We are interested in leveraging inter - rater disagree - 12 ment to drive collaborative analysis of the underlying sources of ambiguity . In particular , we explore the role of group deliberation as a potential tool to detect and analyze ambigu - ity in the context of data classiﬁcation . In this section , we summarize existing protocols and systems for group deliberation to contextualize our work . 2 . 2 . 1 Deliberation Protocols A seminal protocol on structured decision making is the Delphi method [ 35 ] , where experts provide , justify and reconsider their estimates through questionnaires in multiple rounds . A facilitator controls the information ﬂow by summarizing estimates and ﬁltering out ir - relevant justiﬁcation content at each round . The Delphi process ends after a ﬁxed number of rounds or when unanimous consensus is reached . The key characteristics of the Del - phi method are anonymity of the participants , avoidance of any direct interaction among group members , as well as structured and curated information ﬂow as implemented by the facilitator . The Delphi method is typically used for forecasting , policy making and other types of complex decision making processes . Later versions of the Delphi method , e . g . , by Hartman and Baldin [ 67 ] , make use of computer - supported communication to facilitate remote collaboration , larger groups and asynchronous interaction . Group deliberation is a typical method for generating high - quality answers in expert domains such as medicine [ 62 , 82 , 115 ] ; however , little work has shown under what cir - cumstances group deliberation is resolvable or produces better decisions . Several works explored factors that inﬂuence the process and outcomes of group deliberation . Nemeth [ 99 ] found that when jurors are required to reach a unanimous decision , there is more conﬂict , more changes in assessments , and higher conﬁdence in the ﬁnal verdict reported by members of the group . Solomon [ 135 ] sees conﬂict as an important feature of any eﬀective deliberation system . He argues that dissent is both necessary and useful—as “dis - senting positions are associated with particular data or insights that would be otherwise lost in consensus formation”—and criticizes procedures that push deliberators to reach consensus . Instead , he advocates for a structured deliberation procedure that avoids the undesired eﬀects of groupthink [ 74 ] —the tendency to agree with the group by suppressing dissent and appraisal of alternatives—by actively encouraging dissent , organizing indepen - dent subgroups to discuss the same problem , and ensuring diversity of group membership . Kiesler and Sproull [ 80 ] found that time limits imposed on deliberation tend to decrease the number of arguments exchanged and to polarize discussions . The authors suggest the use of voting techniques or explicit decision rules to structure the deliberation timeline . 13 2 . 2 . 2 Deliberation Systems Building on some of these early theoretical results , online deliberation systems have been developed and validated in various domains , including public deliberation [ 84 , 85 ] , on - demand fact checking [ 83 ] , political debate [ 50 ] and knowledge base generation [ 154 ] . For example , ConsiderIt [ 50 , 84 ] is a platform for supporting public deliberation on diﬃcult decisions , such as controversial policy proposals made during U . S . state elections . Kriplean et al . [ 85 ] explored ways to promote active listening in web discussions by explicitly encour - aging discussion members to summarize the points they heard . Kriplean et al . [ 83 ] studied the correctness of statements made in public deliberation and developed an on - demand fact - checking system . Zhang et al . [ 154 ] introduced recursive summarization of discussion trees to enable large scale discussions . Liu et al . [ 91 ] proposed a visualization technique to augment deliberation for multi - criteria decision making . Their results suggest that highlighting disagreement across multiple decision criteria can cause participants to align their opinions for various reasons , from genuine consensus to appeasement . This ﬁnding reinforces the importance of designing deliberation procedures to minimize groupthink . The MicroTalk workﬂow proposed by Drapeau et al . [ 42 ] focuses on argumentation within the microtask crowdsourcing setting . In MicroTalk , workers are prompted to provide justiﬁcations for their decisions , and an algorithm selects certain justiﬁcations ( based on a metric of readability ) to present them as counterarguments to other workers , triggering them to reconsider their decision . MicroTalk is based on an asynchronous model with no interactive back and forth discussion between workers . Building on this work , Chen et al . [ 29 ] showed that a synchronous workﬂow enabling crowd workers to engage in real - time , multi - turn discussions , can lead to additional improvements in answer accuracy . In this dissertation , we study both synchronous and asynchronous workﬂows for group deliberation among human annotators to analyze how disagreement arises and under what circumstances it can be resolved . Chang et al . [ 25 ] addressed the issue of ambiguous category deﬁnitions by proposing a system to enable ﬂexible , post - hoc analysis of crowd - generated , conceptual structures , as opposed to reﬁning classiﬁcation guidelines a priori . Goyal et al . [ 60 ] designed a shared sense - making interface allowing dyads of participants to synchronously share hypotheses , evidence and other insights in a simulated criminal investigation task , leading to increased decision making performance compared to a baseline interface . Chang et al . [ 26 ] proposed a multi - step crowdsourcing workﬂow for semantic frame annotation , allowing workers to express disagreement with expert - labeled golden data presented as feedback during label - ing . While we embed our work reported in Section 3 . 1 in a similar context as Drapeau et 14 al . , our focus is on understanding how unﬁltered group deliberation , task types and other characteristics impact resolvability , beyond just answer accuracy . The systems we report on in Sections 3 . 2 and 3 . 3 leverage a workﬂow in which expert - provided classiﬁcations can be contested by other experts in a round - based collaborative manner . Our work draws inspiration from the Delphi method and builds on prior work above by deploying web - based , structured deliberation systems . We extend the state of the art by translating existing workﬂows into the complex expert domain of medical data analysis , and by integrating a procedure to collect arguments from experts in the form of explicit rationales , centered around pre - existing domain - speciﬁc decision criteria . Our approach diﬀers from several other works in that the primary objective is to achieve a better understanding of the sources and dynamics of expert disagreement , as opposed to optimizing for accuracy within data labeling workﬂows . In other words , our approach uniquely combines the two notions that ( 1 ) there may exist multiple valid answers to one and the same question and that ( 2 ) the reasons why multiple answers may be equally valid are important for expert end users to understand and that these reasons can be eﬀectively analyzed using the process of group deliberation . 2 . 3 Medical Decision Making Except for Section 3 . 1 , all other sections in Chapter 3 as well as Chapters 4 and 5 are anchored within the speciﬁc expert domain of medical decision making . Given the strong focus of this dissertation on the medical domain , this section provides a more in - depth overview on the issues of expert disagreement , group deliberation and AI - based decision support in medical decision making scenarios . 2 . 3 . 1 Expert Disagreement in Medicine Like any form of human interpretation , medical data analysis by human experts is a sub - jective process and can lead to conﬂicting assessments among independent raters [ 10 , 88 , 118 , 128 ] . The issue of inter - rater disagreement is particularly critical within medicine where unreliable clinical decisions can impact patients’ lives adversely . Indeed , Raghu et al . [ 114 ] concluded that label disagreement poses a “full - ﬂedged clinical problem in the healthcare domain . ” Prior work in medical decision making describes that medical experts are susceptible to biases in their reasoning ; for instance , “conﬁrmation bias” can lead a medical expert 15 to look only for evidence that is in line with their pre - existing hypothesis [ 19 ] . As sub - optimal decision - making in medicine can have major consequences , it is crucial to combat any reasoning biases medical experts may have . Our simulated ambiguity - aware AI aims to mitigate this bias by putting forth arguments for conﬂicting medical assessments , en - couraging perspective - taking for alternate lines of reasoning . Related literature suggests that communicating uncertainty can impact cognition and trust , and potentially inﬂuence experts’ decision - making behaviours [ 145 ] . That said , there is a body of work showing that people have a general aversion towards ambiguity [ 79 , 144 ] . For example , a study by Redelmeier and Shaﬁr suggested that the uncertainty between two medical assessments led some doctors to avoid making a decision altogether [ 117 ] . Work done in psychology acknowledges ambiguity - tolerance as a personality variable [ 20 , 72 ] . Medical education research advocates that given the inevitable nature of uncertainty in contemporary medicine , medical experts must acquire a certain level of tolerance to it [ 92 ] . Several works have suggested ways to make productive use of disagreement information in medical data labels ( e . g . , [ 10 , 15 , 32 , 71 , 114 , 124 , 129 ] ) . Inel et al . [ 71 ] introduced domain - independent quality measures for labelers , task instructions and data , based on disagreement information in a medical relation extraction task . Others developed models to predict the likelihood that a given patient case will cause expert disagreement in var - ious medical subspecialties , including epilepsy diagnosis from electrophysiological signals [ 10 ] , and eye disease diagnosis from retinal fundus photographs [ 114 ] . Finally , Barnett et al . [ 15 ] evaluated diﬀerent ways of computationally aggregating discordant medical assess - ments from labelers with varying training background to harness collective intelligence for medical diagnosis . In our work , we leverage the fact that conﬂicting expert assessments can motivate detailed adjudication discussions about diﬃcult cases , and test whether such discussions can be repurposed to improve training for medical expert labelers at scale . 2 . 3 . 2 Group Deliberation in Medicine The issue of low inter - rater reliability in the clinical domain has motivated eﬀorts to ﬁnd methods of adjudicating ambiguous cases in medical data classiﬁcation tasks . Group deliberation has also garnered support in the medical domain as a method for generat - ing a trusted reference standard for the evaluation of automated classiﬁcation methods [ 62 , 82 , 115 ] . In the context of a medical imaging study , in which the aim was to diagnose eye disease based on retinal fundus images , Krause et al . [ 82 ] found that group deliberation was more eﬀective than majority vote when it came to recall among experts . This same dataset 16 was used by Guan et al . [ 62 ] to demonstrate that ensembles of multiple grader - speciﬁc machine learning models could outperform a single - prediction model trained on majority labels , when benchmarked against an adjudicated gold standard . Penzel , Zhang , and Fietze [ 106 ] argue that group deliberation , or “consensus scoring” is the optimal training technique for human scorers in the context of sleep stage classiﬁcation . Recent work by Barnett et al . [ 15 ] showed that automatic pooling of independent opinions from multiple doctors outperformed individual diagnosis across various diagnostic tasks . However , the authors did not investigate the eﬀects of permitting communication or collaboration among doctors to allow for collective adjudication of their diagnoses . While there has been long - standing debate about the relative beneﬁts of collective decision making versus the so - called wisdom of the crowd , there is evidence suggesting that group discussions can indeed improve accuracy of decisions made both in general intelligence tasks [ 98 ] and in medical diagnosis [ 51 ] . 2 . 3 . 3 Medical Diagnosis Training Given that medical generalists far outnumber specialists in various ﬁelds [ 5 ] , research into eﬀective ways to train medical generalists for diﬃcult cases holds the potential to tap into a large pool of high - quality medical labelers . A scalable approach for medical diagnosis training is through computer - based tutorials [ 68 , 86 ] . Typically , web - based tutorials for medical training present a series of patient cases to the learner , and present case - speciﬁc feedback after the learner has submitted their answer . Our approach is similar in that we follow a simple paradigm of presenting feedback after a set of training cases . However , web - based tutorials for medical training typically focus on curation of content for case - speciﬁc feedback while selecting mostly clean - cut cases for which clear explanations exist . The work we present in Section 4 emphasizes the use of diﬃcult , contentious cases to test whether pre - existing adjudication discussions not originally intended for training generalists can be re - used for educational purposes . Our work draws inspiration from the medical education literature about discussion - based learning . The idea that medical students may learn more eﬀectively when engaging in group discussions with their peers has been implemented in the concepts of problem - based learning ( PBL ) and case - based learning ( CBL ) [ 40 , 137 ] . Both approaches aim to improve upon lecture - based learning by fostering collective clinical reasoning through group discussions . CBL is a more structured and guided variant of discussion - based learning in medicine while PBL implements an open - ended approach . In Chapter 4 , we examine whether passive consumption of specialist discussion about diﬃcult cases can yield similar 17 beneﬁts for diagnostic reasoning as has previously been reported about PBL and CBL , but applied to the context of medical labeling . 2 . 3 . 4 AI - based Clinical Decision Support Clinical decision support ( CDS ) is broadly deﬁned as the provision of intelligent assistance to clinicians , medical staﬀ , and patients [ 101 ] . CDS can include low - level functions like computerized alerts and reminders for providers and patients , or high - level functions like patient diagnosis [ 33 ] . Norman et al . [ 100 ] describe a dual process of diagnostic reason - ing , where physicians engage in ( 1 ) a non - analytic or unconscious process of hypothesis generation , and / or ( 2 ) a conscious , analytic process of hypothesis testing . The latter is an extensive computational process , and has motivated eﬀorts to develop AI - based CDS systems for diagnostic support . In such support systems , a physician cross - checks the algorithmic output against their internal knowledge , but takes responsibility for the ﬁnal diagnostic decision . Our work takes a similar approach of augmenting , instead of automating , the job of physicians [ 61 ] . Barriers to the Adoption of AI - Based CDS Systems Explainability . ML - based AI systems are typically opaque with respect to their internal functions [ 96 ] . In ﬁelds where AI is tasked with important decisions , it is imperative that automated decision making be interpretable , especially if the AI is known to be imperfect [ 21 , 81 ] . The ﬁeld of XAI [ 146 ] emerged as a response to this problem of transparency beginning in the 1970s and 1980s with the deployment of expert systems with explanation capabilities—most notably for medical decisions [ 3 ] . Explanations have been found to promote transparency in machine learning algorithms and make users more aware of how a system works [ 113 ] . Recent approaches in XAI , e . g . , Ehsan et al . [ 46 ] , demonstrate that AI systems can learn to generate human - like natural language explanations for their decisions . Mittelstaedt et al . [ 96 ] argue that there is a mechanistic link between explanation and justiﬁcation in human discourse , and that machine explanations should emulate human explanations . The simulated AI assistant we present in Chapter 5 instantiates these design principles , by providing human - interpretable rationales for its outputs . Trust . A lack of trust is arguably the most signiﬁcant barrier to adoption of AI - based systems . A CDS system can bias a physician to choose the wrong course of action against their own clinical judgement [ 43 ] . Human experts may also fail to trust a reliable system . 18 It is crucial that an appropriate level of trust in automation be established to balance over - reliance and under - reliance . Cai et al . [ 21 ] demonstrated a link between explainability and trust by showing that pathologists trusted a CDS tool for cancer diagnosis more if they could tweak its internal representation of image similarity using domain - speciﬁc concepts ( e . g . , number of fused glands ) . Addressing the problem of trust becomes more complicated in the context of uncer - tainty . Psychological uncertainty is an aversive state [ 145 ] , and thus information must be communicated eﬀectively to hedge against its negative eﬀects . There appears to be a volatile relationship between uncertainty and trust . On the one hand , trust can be under - mined by failing to communicate uncertainty ; on the other hand , admitting uncertainty can also hinder trust [ 145 ] . Thus , it is crucial that machines strike the right balance between communicating and withholding uncertainty information . Studies on how communicating uncertainty aﬀects trust are limited and have produced mixed results . While there is some evidence that trust can be fostered through explained uncertainty [ 78 ] , more research is needed . In a recent review of the matter , van der Bles [ 145 ] acknowledged that uncertainty does not always produce negative emotional eﬀects . Indeed , in the healthcare domain , Schneider et al . [ 130 ] developed a system for communi - cating uncertainty in fertility prognosis that increased users’ understanding of uncertainty without causing them to have a negative view of the system . In Chapter 5 , we study how the workﬂows and perception of medical experts is aﬀected by an AI assistant capable of identifying and explaining ambiguous cases . 19 Chapter 3 Group Deliberation for Data Labeling One primary contribution of this dissertation is to introduce and study group deliberation in the context of data labeling , as a tool to detect and analyze ambiguous cases , and to either resolve inter - rater disagreements organically through deliberation , or otherwise mark them as irresolvable . In this chapter , we present insights from three case studies about collaborative online platforms that all integrate group deliberation into data labeling workﬂows , in the context of diﬀerent data modalities , labeling tasks and types of human labeler expertise . Section 3 . 1 describes case studies on synchronous group deliberation in the context of novice crowd work and text classiﬁcation tasks of variable subjectivity . In Sections 3 . 2 and 3 . 3 , our focus shifts from novice crowd work to the expert domain of medical data interpretation , exploring asynchronous and structured deliberation workﬂows for image labeling and time series labeling respectively . 20 3 . 1 Crowd Deliberation for Text Labeling Crowdsourced classiﬁcation of data typically assumes that objects can be unambiguously classiﬁed into categories . In practice , many classiﬁcation tasks are ambiguous due to various forms of disagreement . Prior work shows that exchanging verbal justiﬁcations can signiﬁcantly improve answer accuracy over aggregation techniques . In this section , we study how worker deliberation aﬀects resolvability and accuracy using case studies with both an objective and a subjective task . Results show that case resolvability depends on various factors , including the level and reasons for the initial disagreement , as well as the amount and quality of deliberation activities . Our work reinforces the ﬁnding that deliberation can increase answer accuracy and the importance of verbal discussion in this process . We contribute a new public data set on worker deliberation for text classiﬁcation tasks , and discuss considerations for the design of deliberation workﬂows for classiﬁcation . 3 . 1 . 1 Motivation Classiﬁcation is a prevalent task in crowdsourcing as well as many real - world work practices . A common assumption for many classiﬁcation tasks is that objects can be unambiguously classiﬁed into categories , and that the quality of the labeled data can be measured by the extent to which annotators agree with one another . As a result , most post - processing techniques designed to ﬁlter or aggregate labeled data interpret inter - rater disagreement as “noise in the signal” originating from human mistakes . In practice , many classiﬁcation tasks are ambiguous , and disagreement can happen for various reasons including missing context , imprecise questions , contradictory evidence , and multiple interpretations arising from diverse levels or kinds of annotator expertise [ 64 ] . The independence of individual assessments has traditionally been considered a pre - requisite for leveraging the ‘wisdom of the crowd’ , but recent ﬁndings from crowdsourcing [ 42 ] and social behavioural research [ 98 ] have found that deliberation can help improve answer quality . Drapeau et al . [ 42 ] showed that crowdsourcing workﬂows enabling work - ers to exchange justiﬁcations and to reconsider their assessments based on each other’s arguments , can improve accuracy over output aggregation techniques . These prior works , however , have focused mostly on answer correctness , with the a priori assumption that each disagreement can be resolved to one correct answer . In this work , we take the stance that inter - rater disagreement carries valuable informa - tion [ 25 , 76 , 44 ] and that deliberation should not always lead to a unanimous consensus . 21 In particular , we investigate factors that contribute to the resolvability of a case . We con - ducted a study on two text classiﬁcation tasks—a sarcasm classiﬁcation task , which has been shown to be inherently ambiguous [ 48 ] , and a semantic relation extraction task , where objective ground truth is available [ 42 ] —to investigate how deliberation aﬀects resolvabil - ity . Our key contributions are : 1 . We study how the deliberation outcomes diﬀer depending on task subjectivity . 2 . We present observations showing that the resolvability of an ambiguous case depends on the reasons for and level of initial disagreement , the amount and quality of the deliberation activities , as well as the task and case characteristics . 3 . We publish a new dataset on worker deliberation in text classiﬁcation tasks , including all original and revised classiﬁcations as well as the deliberation dialogues . The rest of this section describes the details the deliberation workﬂow designed and im - plemented for this study , outlines the experimental procedure and ﬁndings , and concludes with a discussion of applications and design considerations . 3 . 1 . 2 Deliberation Workﬂow We designed a workﬂow enabling crowdworkers to revisit and potentially resolve disagree - ments in text classiﬁcation tasks through group discussions . The input to our workﬂow is a set of cases ( e . g . , text documents ) to be classiﬁed . Each disagreement case either gets resolved through discussion or remains unresolved . The output of our workﬂow are multiple classiﬁcation labels for each input case and its deliberation data consisting of structured information ( i . e . , original and reconsidered classiﬁcation decisions , conﬁdence levels , sources of disagreement , and evidence ) and deliberation dialogues ( e . g . , arguments , explanations , and examples ) . This workﬂow transforms input to output in four time - limited stages ( Figures 3 . 1 ) . Workﬂow stages A , B , C and D started at consecutive hours ( e . g . , 3pm , 4pm , 5pm , and 6pm ) , to ensure that all workers could complete one stage before collectively starting the next . Stage A : Independent Classiﬁcation . Workers independently performed 10 clas - siﬁcation microtasks . In each task , workers were asked to read a text document , classify it into one of two categories , provide a conﬁdence level for their decision , and highlight 22 DI SCOURSE WORKFLOW WORKERS CASES I NPUT OUTPUT DISCOURSE DIALOGUES Arguments Expl anati ons Exampl es STRUCTURED OUTPUT Evi dence Regi ons Di sagreement Sources Cl assi fi cati ons Confi dence Level s CASES resol ved / unresol ved DI SCUSS ( Part 1 ) 1 DI SCUSS ( Part 2 ) 2 RE - CLASSI FY 3 RE - EVALUATE DI SAGREEMENT 1 CLASSI FY 2 MARK EVI DENCE 3 PREDI CT DI SAGREEMENT IF DISAGREEMENT ? GROUP FORMATION IF PERFECT AGREEMENT 1 VI EW FI NAL DECI SI ONS I N GROUP 2 MI NI SURVEY B A C D Figure 3 . 1 : Input , output and stages of the deliberation workﬂow implemented for this study . Each of the green blocks represents one of multiple microtasks in each of the four workﬂow stages A , B , C and D . evidence to support their choice . Workers were then asked to predict the level of disagree - ment for the classiﬁcation task by indicating whether they expect Substantial Agreement ( “I expect most people to agree with me” ) , Half Agreement ( “I expect only about half of the people to agree with me " ) , or Substantial Disagreement ( “I expect most people to disagree with me” ) . If workers chose one of the latter two options , they were also asked to choose the sources of disagreement they anticipated . They could select multiple options from a list of six preset options as described in Table 3 . 1—Fuzzy Deﬁnition , Missing Context , Contradic - tory Evidence , Important Details , Expertise Needed , Subjective Case—covering a variety of common sources of disagreement from prior work and our pilot studies . They could provide their own rationale using an optional free - form ﬁeld . Appendix A . 1 . 2 provides a complete list of questions and answer options for this stage . Between stage A and B , our system dynamically put crowdworkers into groups of three to deliberate on one or more cases , with the constraint that there was exactly one dissenter for each disagreement case per group . To ensure group heterogeneity , the group formation procedure was randomized , i . e . , all workers had the same chance of being assigned to a group , and it was equally likely for them to be grouped together with either two random workers they disagreed with , or one with whom they agreed and one with whom they disagreed . Stage B : Discussion Round 1 . Workers joined their assigned groups to discuss their disagreement cases . For each case , workers were required to leave at least one comment in the group chat explaining why they had chosen their label for the given case . In addition , they could choose to highlight more parts of the text as evidence . The comments and associated evidence were recorded and shown to other workers in real time . Stage C : Discussion Round 2 and Reconsideration . Workers collectively re - turned to review each other’s comments and participate in a second round of discussion 23 Table 3 . 1 : Preset choices for sources of disagreement . Fuzzy Deﬁnition Other people may have diﬀerent deﬁnitions of [ sarcasm / relation ] in mind . Missing Context The text is ambiguous because of missing context ( for example , [ the identity of the product / some important information about the person or the place ] is unknown ) . Contrad . Evidence The text contains some features that indicate [ sarcasm / relation ] is expressed and other features that indicate [ absence of sarcasm / relation is not expressed ] . Important Details The text contains relevant details other people could easily miss . Expertise Needed Someone with more experience or expertise may see or understand something about the text that I don’t . Subjective Case This is a case where a person’s answer would depend heavily on their personal preferences and taste . Other Requiring free - form answer if selected on the same disagreement cases . They were asked to further discuss the case by leaving at least one comment in the group chat , and optionally highlight more evidence . After providing at least one additional discussion comment , workers were individually prompted to reconsider their original classiﬁcation decision and conﬁdence level . To submit their ﬁnal classiﬁcation , workers could choose one of the two original class labels or a third option named “irresolvable” . Finally , workers were also asked to re - evaluate what they considered the source of disagreement for the given case and group in light of the previous discussion . Workers were again given the six preset options ( Table 3 . 1 ) and an optional free - form ﬁeld , as well as the free - form answer they had provided earlier as the anticipated source of disagreement , if any ( Appendix A . 1 . 3 ) . By providing the “irresolvable” option , we reduced the bias for consensus , and incentivized workers to change their answer only if they truly believed that their updated classiﬁcation label was correct . Since we were interested in case resolvability , the reconsidered classiﬁcation decisions were collected from all discussion members individually instead of enforcing the group to produce one joint decision . Importantly , workers did not see the updated answers of other group members before stage D to reduce opportunities for strategic voting . Stage D : View Final Decisions . For each disagreement case , workers were pre - sented with the ﬁnal decision ( i . e . , either one of the class labels or “irresolvable” ) from each group member , and they were given a short open - ended survey on why they thought the disagreement was resolved or not resolved , as well as why they had changed or stuck to their original classiﬁcation decision ( Appendix A . 1 . 4 ) . General Design Considerations In the workﬂow design , we made several design decisions regarding the communication medium used for deliberation , ensuring stable pay / work ratios through ﬁller tasks , and 24 Figure 3 . 2 : Screenshots showing our worker deliberation interface paired with text classiﬁ - cation tasks for sarcasm detection ( left ) and relation extraction ( right ) . The text documents under discussion are shown on the left of each screenshot . Parts of the text are highlighted by diﬀerent group members as evidence supporting their classiﬁcation decisions . Written justiﬁcations are exchanged through a real - time chat component , with an embedded voting summary , shown on the right of each screenshot . motivating workers to return for all four consecutive workﬂow stages despite the intermit - tent breaks . Communication Medium . Our decision to use text ( versus voice or video ) as the communication medium for deliberation was motivated by prior research suggesting that written communication improves outcomes of consensus formation by avoiding bias from the tone of voice or a perceived lack of anonymity [ 35 , 120 ] . Filler Tasks . If workers had less than 10 disagreement cases to revisit in stages B , C and D , they were asked to perform ﬁller classiﬁcation tasks ( identical to the microtasks in stage A ) to ﬁll up the slots . This was to ensure that workers would not be incentivized to provide answers in stage A that were likely to agree with others just to reduce the number of disagreement cases they would have to process in subsequent stages . The data from the ﬁller tasks were not used in our experiment . Worker Retention . An important design consideration was the mechanism used to encourage workers to collectively return to stages B and C for real - time discussion . Through several pilot studies , we found a combination of monetary incentive and timed notiﬁcations to be a successful approach . As a monetary incentive , we paid ﬁxed amounts for participation in each stage and an additional bonus for full participation in all stages . We used the MTurk API to send out reminder emails ﬁve minutes before the start of stages B , C and D , with a web link to join the next stage and a notice stating that the next stage could only be joined within three minutes of its start . 25 Interface Our deliberation interface is general , i . e . , it can be integrated into any task interface for text classiﬁcation and requires only minor modiﬁcations for other data modalities like im - ages . Figure 3 . 2 shows the interface in action . The interface displays the text document in question and attaches a chat box to the right , through which group members can exchange justiﬁcations in real time . Group members are randomly assigned friendly - sounding and easy - to - remember pseudonyms ( e . g . , ‘Joyful Joliot’ or ‘Enthusiastic Easley’ ) to allow users to address each other without having to reveal their identity . Finally , the interface en - ables users to highlight parts of the text document as evidence to support their argument . Highlights and pseudonyms are color - coded per deliberator . 3 . 1 . 3 Experiment We conducted an experiment to investigate how deliberation aﬀects the outcomes of crowd - sourced text classiﬁcation tasks . Here , we describe the task types , procedure , participant recruitment and payment associated with our study . Task Types We focus on two types of text classiﬁcation tasks ( Figure 3 . 2 ) with diﬀerent degrees of inherent subjectivity . During our pilot studies , we also considered other task types ( e . g . , image , audio or video classiﬁcation ) , but decided to defer those to future work due to the added complexity of synchronously highlighting evidence in such data modalities . Sarcasm Task . For the ﬁrst task type , we asked workers to label Amazon product reviews as sarcastic or not sarcastic , using the sarcasm detection data set published by Filatova [ 48 ] . We chose this task type and data set because Sarcasm detection is considered by Filatova et al . [ 48 ] as an inherently subjective task due to the “absence of a formal deﬁnition of sarcasm” , an observation further supported empirically by low rates of inter - rater agreement in Filatova’s data set . To generate a subset of cases for our experiment , we ﬁrst identiﬁed all reviews for which Filatova [ 48 ] reported highest inter - rater disagreement ( i . e . , 3 sarcastic vs . 2 not sarcastic or vice versa ) and then , from this subset , retained the 40 most compact cases based on word count . Relation Task . For the second task type , workers were asked to indicate whether a certain semantic relation between a person and a place ( e . g . , “Nicolas Sarkozy lived in France” , “Pavarotti died in Modena” ) was expressed in a given sentence . In contrast to 26 the sarcasm detection task , this task is more well - deﬁned and objective , as the ground truth data can be determined from the oﬃcial label guidelines for the TAC KBP relations LivedIn and DiedIn as published by the Linguistic Data Consortium . We presented the corresponding relation deﬁnition to workers in each individual classiﬁcation task ( see right side of Figure 3 . 2 for an example ) to explicitly make workers aware of the label guidelines . We used all 40 sentences from the data set used by Drapeau et al . [ 42 ] , of which 25 have ground truth labels . Procedure Before the experiment , workers ﬁrst ﬁlled out a pre - study questionnaire eliciting demo - graphic information , including age group , gender , native language and self - rated proﬁciency in English ( see Appendix A . 1 . 1 ) . Participants then collectively stepped through the four consecutive stages of our workﬂow . They were free to close the browser tab or do other work after completing each stage and before the start of the next one , of which they were notiﬁed via email ﬁve minutes prior to start . Participant Recruitment We recruited 316 participants on Amazon Mechanical Turk , using workers from the US who had completed at least 500 tasks with a 90 % acceptance rate . Based on the pre - study questionnaire , almost all of our workers are native English speakers ( 97 % ) and have high self - rated proﬁciency in English ( 96 % and 4 % selected the highest and second highest levels on a 5 - point Likert scale ) . The distribution over age groups is : 18 - 25 ( 14 % ) , 26 - 35 ( 44 % ) , 36 - 45 ( 23 % ) , 46 - 55 ( 13 % ) and 56 + ( 6 % ) . About half of our workers ( 53 % ) are female . Payment Workers were paid US $ 1 for each stage that they completed . In addition , we paid a one - time completion bonus of US $ 2 to workers who completed all four stages . Each stage took workers around 15 minutes to complete , resulting in an approximate payment of US $ 6 per hour of work for participants who completed all four stages . Note that workers were free to close the browser tab or do other work after completing each stage and before the start of the next one . We incentivized workers to actively engage in discussion by oﬀering an extra bonus of US $ 0 . 50 for each disagreement case in which all group members voted for the correct expert answer ( which can be one of the two class labels , or “irresolvable” ) in stage C . As only few cases had an expert answer available , we paid the extra bonus to all groups in the end that reached consensus on one of the two target categories . 27 3 . 1 . 4 Research Questions and Hypotheses Our study aims to answer three research questions . Q1 : Why do annotators disagree with one another ? We expect disagreement to arise due to diﬀerent reasons in the two task types with diﬀerent degrees of inherent subjectivity , where the target concepts are more versus less well - deﬁned . Based on this intuition , we hypothesize that : [ H1a ] Sources of disagreement diﬀer signiﬁcantly between the two task types . [ H1b ] Annotators can predict levels of disagreement for individual cases better than random . Q2 : Under what circumstances can disagreement be resolved through worker deliberation ? A variety of factors can contribute to the resolvability of a given case . First , the characteristics of a task and its associated sources of disagreement can play a role . For example , well - deﬁned target categories may provide better grounding for convincing argu - ments , enabling groups to more easily come to a consensus . We hypothesize that : [ H2a ] Sources of disagreement aﬀect whether a case will be resolved . [ H2b ] Task subjectivity aﬀects whether a case will be resolved . Second , the characteristics of the deliberation activities can inﬂuence whether a case is resolved . We hypothesize that : [ H2c ] The extent to which members contributed equally aﬀects case resolvability . Third , we expect the amount of consensus in the label and overlap in highlighted evidence during the independent classiﬁcation phase ( i . e . , stage A ) to be predictive of a case’s resolvability . We hypothesize that : [ H2d ] The extent of the disagreement amongst group members aﬀects resolvability . [ H2e ] The amount of overlapping evidence between group members aﬀects resolv - ability . 28 Q3 : What impact does the deliberation workﬂow have on crowdsourcing outcomes and processes ? The deliberation workﬂow , which encourages workers to con - sider diverse evidence and arguments , may have a positive eﬀect on crowdsourcing outcomes and processes , such as improving the overall answer correctness and discouraging group - think ( i . e . , the tendency of group members to blindly follow the majority ) . We hypothesize that : [ H3a ] Worker deliberation improves the quality of the crowdsourced annotations . [ H3b ] The probability that a case will be resolved in favour of the initial majority vote is similar to the probability that a case will not be resolved in favour of the initial majority vote . [ H3c ] Sources of disagreement and the extent to which members contributed equally aﬀects whether a case will be resolved correctly . 3 . 1 . 5 Experimental Conditions One of the goals in our study is to understand the eﬀect of worker deliberation on the quality of crowdsourced annotations ( H3a ) . To quantify the eﬀects , we tested two additional variants of our workﬂow without the discussion component . Each participant was randomly assigned to one of the following conditions : Disagree , Discuss and Reconsider ( N = 316 ) : workers reconsider their position after discussion with other group members . This is our main condition testing our full deliberation workﬂow . If not otherwise noted , all results below are based on data from this condition . Disagree and Reconsider ( N = 26 ) : workers reconsider their position after they are shown group disagreement data , but without a discussion . This condition was added to isolate potential eﬀects of showing workers information on who agreed vs . disagreed with them . Reconsider Only ( N = 24 ) : workers reconsider their position without being shown group disagreement data and without a discussion . This condition was added to identify any learning eﬀects resulting from workers revisiting a case after labeling other cases . 29 The latter two conditions are used for hypothesis H3a only . In the results section for hypothesis H3a , we use the term Baseline for labels submitted during stage A ( i . e . , before any reconsideration ) , and otherwise refer to reconsidered labels submitted during workﬂow stage C . 3 . 1 . 6 Data and Analysis Data . For the analysis , the data include : ( a ) pre - study questionnaire data about demo - graphics and language proﬁciency , ( b ) post - study questionnaire data , probing at workers’ thoughts about and experiences with the deliberation process , ( c ) all the messages ex - changed in the deliberation workﬂow stages , ( d ) the pre - and post - deliberation classiﬁca - tion label and conﬁdence for each worker , ( e ) the highlighted evidence for each case , ( f ) the sources of disagreement as anticipated by workers before discussion and re - evaluated by workers after discussion , ( g ) the anticipated resolvability of each case , and ( h ) the anticipated level of disagreement for each case . Method . For each task type , we ran a breadth analysis on 40 text documents with up to 3 independent group discussions per document in order to identify the level of ambiguity for each case . This was followed by a depth analysis of the 10 most ambiguous cases with up to 16 independent group discussions per case , used to answer Q2 and Q3 . We analyzed the data using both quantitative ( e . g . , basic descriptive statistics , re - gression models ) and qualitative analysis of worker responses . For the logistic regression models [ 94 ] , we used the step - wise regression procedure to select the best possible com - bination of variables that could explain the dependent variable , based on improvements to the Akaike information criterion ( AIC ) . We also test the goodness of ﬁt of each model using the Hosmer - Lemeshow [ 69 ] , Osius - Rojek [ 102 ] and Stukel [ 139 ] tests at signiﬁcance level 0 . 05 . For qualitative data analysis , line - by - line inductive open coding was performed by one of the study authors to identify the emerging themes reported below . Filtering . Due to worker dropout between the four workﬂow stages , there were groups that became inactive during the deliberation process or had too few members remaining at the end . Out of all 316 participants , 78 . 2 % completed all four stages , 3 . 8 % and 4 . 8 % dropped out after stages B and C respectively , and 13 . 2 % completed only stage A . Possible explanations for the moderate dropout after stage A are that some workers might not have received or seen their email notiﬁcations in time ( or not at all ) to join stage B , or may simply not have been interested in returning to the same type of task in subsequent sessions . We excluded groups from the analysis if more than one member was inactive ( i . e . , they did not complete stages B and C ) , resulting in empty or single person groups , or 30 where the minority member was inactive , leading to groups of two people sharing the same opinion . This resulted in two types of groups that were retained for the ﬁnal analysis : • 2 vs . 1 ( unbalanced ) : all group members were active . • 1 vs . 1 ( balanced ) : one member dropped out , leaving two members with divergent opinions . From a total number of 418 groups , we excluded 110 ( 26 % ) for the aforementioned reasons , resulting in 308 active groups retained for the analysis , of which 206 ( 67 % ) were unbalanced ( 2 vs . 1 ) and 102 ( 33 % ) were balanced ( 1 vs . 1 ) groups . Data Set . The full data set is published at : https : / / github . com / crowd - deliberation / data . 3 . 1 . 7 Results Analysis based on descriptive statistics shows that there was more disagreement in the Sarcasm task than in the Relation task , conﬁrming our premise that the Sarcasm task is more subjective . Speciﬁcally , we computed the level of label disagreement for each of the 40 cases per task type using the data from the breadth run . Label disagreement was represented as entropy : H ( p ) = − p log 2 p − ( 1 − p ) log 2 ( 1 − p ) where p was the proportion of workers who chose the positive category ( e . g . , Sarcastic ) . Entropy values range from 0 to 1 ; higher values mean more disagreement ( e . g . , 50 % choos - ing Sarcastic and 50 % choosing Not Sarcastic ) and lower values indicate less disagree - ment . Cases had mean entropy values of 0 . 85 ( SD = 0 . 22 ) in the Sarcasm task and 0 . 61 ( SD = 0 . 32 ) in the Relation task , indicating less disagreement overall in the Relation task . This diﬀerence is statistically signiﬁcant under a two - sided t - test t ( 70 ) = 3 . 79 , p < 0 . 001 . Q1 : Why do annotators disagree with one another ? [ H1a ] To discover the sources of disagreement in each task , we analyzed the anticipated and re - evaluated sources of disagreement that workers provided before and after discussion . As workers were allowed to select multiple options , we use the simultaneous Pearson inde - pendence test [ 18 ] , which takes into account correlation between options . Results ( in Tables 31 Table 3 . 2 : Anticipated sources of disagree - ment ( before discussion ) where the propor - tions of workers are signiﬁcantly diﬀerent . Percentage of Participants Task Relation Sarcasm Missing Context 49 % 4 % Contrad . Evidence 18 % 43 % Table 3 . 3 : Re - evaluated sources of disagree - ment ( after discussion ) where the propor - tions of workers are signiﬁcantly diﬀerent . Percentage of Participants Task Relation Sarcasm Fuzzy Deﬁnition 28 % 43 % Missing Context 24 % 4 % Contrad . Evidence 11 % 23 % 3 . 2 and 3 . 3 ) show that certain sources of disagreement reported by the workers depend sig - niﬁcantly on the task type , both , as anticipated before discussion ( χ 2 S = 61 . 96 , p < 0 . 001 ) and as re - evaluated after the discussion ( χ 2 S = 89 . 88 , p < 0 . 001 ) . This result conﬁrms our hypothesis H1a . A higher percentage of workers anticipated Missing Context to be a dominant source of disagreement in the Relation task ( 49 % ) than in the Sarcasm task ( 4 % ) . Note that we included Missing Context to capture the extent to which it leads to disagreement , even though , following the standard TAC KBP guidelines for relation extraction , workers were instructed to avoid inferences based on missing information in the Relation task . Conversely , more workers anticipated Contradictory Evidence to be a dominant source of disagreement in the Sarcasm task ( 43 % ) than in the Relation task ( 18 % ) . After discussion , while the same trends are observed , workers also identiﬁed Fuzzy Deﬁnition as an additional source of disagreement , which is more prominent in the Sarcasm task ( 43 % ) than the Relation task ( 28 % ) . [ H1b ] For Q1 , we also investigated workers’ ability to predict disagreement . Workers were asked to predict the level of disagreement before the discussion by choosing Substantial Disagreement , Half Agreement , or Substantial Agreement . We determined the ground truth “level of disagreement " for each case by running a two - sided proportion test to see if there is a 50 / 50 split in group opinions about the label . If this null hypothesis could not be rejected at signiﬁcance level 0 . 05 , we assumed the correct ground truth answer to be Half Agreement . Otherwise , the correct ground truth answer was based on the label chosen by majority vote . Based on the depth analysis data , 7 out of 10 cases in the Sarcasm task and 3 out of 10 cases in the Relation task resulted in Half Agreement . Since very few workers predicted Substantial Disagreement ( < 1 % in each task type ) , this answer option was grouped together with Half Agreement ( known as Disagreement from hereon ) to allow for further inferential statistical tests . We measured workers’ ability to predict the resulting two discrete levels Disagreement and Agreement ( previously known as Substantial Agreement ) . For both task types , we computed workers’ rate of being correct 32 Overall Disagreement Agreement 0 % 25 % 50 % 75 % 100 % OverallAccuracy ( Balanced ) Rate of Being Correct when Predicting Disagreement ( Precision ) Rate of Detecting all Cases with Disagreement ( Recall ) Rate of Being Correct when Predicting Agreement ( Precision ) Rate of Detecting all Cases with Agreement ( Recall ) Sarcasm Task Relation Task Figure 3 . 3 : Aggregate performance of our worker population at predicting disagree - ment / agreement by task type . when predicting Disagreement or Agreement ( precision ) , their rate of detecting all cases with Disagreement or Agreement ( recall ) , as well as their overall prediction performance , in terms of balanced accuracy , a robust measure which accounts for class imbalance , deﬁned as the average of the recall values for Disagreement and Agreement . Figure 3 . 3 shows that workers’ prediction performance was higher in the Relation task in terms of all the metrics , except for Disagreement precision . In other words , only the rate of being correct when predicting Disagreement was slightly higher in the Sarcasm task ; for all other metrics , workers were more successful in the Relation task . In terms of the overall prediction performance , workers’ accuracy was signiﬁcantly better than random in the Relation task at 61 % ( χ 2 ( 1 , N = 716 ) = 158 . 63 , p < 0 . 001 ) , whereas this was not the case for the Sarcasm task . This result partially conﬁrms our hypothesis H1b . More interestingly , irrespective of the task type , when workers did predict Disagreement , their rates of being correct were higher than when predicting Agreement ; but they were also generally less successful at detecting all cases with Disagreement than those with Agreement . In summary , for Q1 , we conﬁrmed that sources of disagreement diﬀer signiﬁcantly between the two task types , identiﬁed Missing Context as a characteristic source of dis - agreement for the Relation task , and Contradictory Evidence and Fuzzy Deﬁnition for the Sarcasm task ( H1a ) . In addition , we showed that workers can predict levels of disagreement 33 signiﬁcantly better than random in the Relation task ( H1b ) . Q2 : Under what circumstances can disagreement be resolved through worker deliberation ? We analyzed the factors contributing to the resolution of disagreement using both quantita - tive and qualitative analyses of the questionnaire data . Group discussions were considered resolved if and only if all group members converged to one of the two target categories in their ﬁnal classiﬁcation . For the quantitative analysis , a logistic regression model was used to discover factors that aﬀect whether a case is resolved or not . The variables considered include , for each potential source of disagreement , the proportion of workers within a group who selected this source after discussion ( H2a ) , the task type ( H2b ) , various statistics capturing the amount of words contributed by group members ( H2c ) , the level of initial consensus ( H2d ) , and the amount of overlap between the highlighted pieces of evidence among the dissenting parties within a group ( H2e ) , as measured by the Jaccard index , a statistic for the overlap between two sets . The pairwise interactions between the selected factors were excluded due to lack of statistical signiﬁcance . 1 Table 3 . 4 : Logistic model for understanding the likelihood of resolving a case . Model Parameters Variable ˆ β Std . Error t p - value Subjective Case - 2 . 16 0 . 83 - 2 . 59 * * Fuzzy Deﬁnition - 1 . 45 0 . 53 - 2 . 73 * * Contrad . Evidence - 1 . 48 0 . 65 - 2 . 27 * # Words Min / Max - 1 . 49 0 . 67 - 2 . 24 * Group 2 vs . 1 - 0 . 67 0 . 33 - 2 . 04 * Sarcasm Task 0 . 61 0 . 36 1 . 69 [ H2a ] Results ( Table 3 . 4 ) show that certain sources of disagreement—namely , Subjec - tive Case , Fuzzy Deﬁnition , and Contradictory Evidence—decrease the probability of a case being resolved , partially conﬁrming our hypothesis H2a . 1 Statistically signiﬁcant results are reported as follows : p < 0 . 001 ( * * * ) , p < 0 . 01 ( * * ) , p < 0 . 05 ( * ) . 34 In the post - study questionnaire , workers describe some cases as straightforward , requir - ing only a second glance to reach consensus ( “After rereading , I realize the whole review is a joke and the positive points are sarcastic . ” ) , while other cases are ambiguous or con - fusing due to contradictory evidence ( e . g . , the text contains features of both sarcasm and non - sarcasm ) and missing context . [ H2b ] We speculated that task subjectivity aﬀects whether a case will be resolved . Results are mixed . Our step - wise regression procedure revealed no detectable diﬀerences in case resolvability between the Sarcasm and Relation tasks . However , in the post - study questionnaire , workers in the Relation task mentioned that well - deﬁned category deﬁnitions helped them resolve disagreement ( “We were able to refer to the deﬁnition of LivedIn . That made the answer clear . ” ) , while the “lack of instruction on what constitutes a sarcastic review” was considered a barrier to resolving cases in the Sarcasm task . [ H2c ] We hypothesize that the extent to which members contributed equally predicts whether the case will be resolved . Our model selected # Words Min / Max , i . e . , the pro - portion of words contributed by the least active and the most active contributors within a group , as a signiﬁcant predictor variable , conﬁrming hypothesis H2c . An increase in this proportion decreases the likelihood of a case being resolved . In other words , if members contributed equally , cases were signiﬁcantly less likely to be resolved than if some members contributed substantially more than others . Qualitative analysis also shows that interactions between members of the discussion groups can inﬂuence the ﬁnal outcomes . Workers said that disagreement was resolved by clarifying the task ( “Members were becoming clearer on interpretation of the task . ” ) , providing examples ( “Using examples , we were able to persuade the group member to change their opinion . ” ) , using evidence ( “We were able to point out things in the sentence that were overlooked by others . ” ) , or pointing out false assumptions ( “Others pointed out things that some of us had assumed in the sentence and changed our opinions . ” ) Many workers identiﬁed the quality of deliberation activity itself as the main driver for reaching consensus in an argumentative manner , e . g . , “People that didn’t agree listened to the arguments and were willing to change their mind to sarcastic . ” ) On the other hand , workers also reported some group - related factors that hindered the resolution of disagreement , including divergent , but equally valid interpretations ( “I think it depends on how people view sarcasm . ” ) and the lack of communication ( “ [ My group was ] not continuing a conversation . Responding with one word responses does not solve anything . ” ) [ H2d ] A consensus level of 2 vs . 1 ( i . e . , two workers agree , one worker disagrees ) also decreases the likelihood of a case being resolved compared to the consensus level of 1 vs . 1 , conﬁrming our hypothesis H2d . 35 [ H2e ] Finally , the overlap in evidence among group members was not selected as a relevant factor for the optimal model ﬁt , leading us to reject hypothesis H2e . To summarize the ﬁndings for Q2 , we found various factors that aﬀect whether a case is resolved or not , including some sources of disagreement ( H2a ) , task type ( H2b ) , the degree to which deliberation activity is balanced within a group ( H2c ) and the level of initial consensus ( H2d ) . Other factors like overlap in evidence highlighted by dissenting parties ( H2e ) had no signiﬁcant eﬀect on resolvability . Finally , our analysis shows that worker characteristics ( such as age and personality ) can have some inﬂuence on the way they deliberate ( e . g . , their overall tendency to revise their position ) , which in turn can play a role in resolving a case . Q3 : What impact does the deliberation workﬂow have on crowdsourcing out - comes and processes ? [ H3a ] To evaluate whether worker deliberation improves crowdsourcing outcomes , we com - pared workers’ answer correctness between our three experimental conditions Reconsider Only , Disagree and Reconsider , and Disagree , Discuss and Reconsider and our Baseline ( i . e . , original labels submitted in stage A ) . This part of the analysis is restricted to the 25 cases from the breadth run , where we have ground truth to assess correctness . Correctness was measured by F1 - score ( i . e . , the harmonic mean of precision and recall ) of individual workers . We did not aggregrate labels across multiple workers as our workﬂow did not contain a requirement for shared unanimous group decisions , but instead incen - tivized independent reconsideration from individual workers in all conditions . Assessments that were revised to “irresolvable” were excluded because they were neither right nor wrong . Overall , correctness was signiﬁcantly higher in Disagree , Discuss and Reconsider , with no signiﬁcant diﬀerences between the other three conditions . Figure 3 . 4 provides a visual comparison . The statistical signiﬁcance of these diﬀerences was conﬁrmed by a one - way analysis of variance ( F ( 3 , 115 ) = 6 . 42 , p < 0 . 001 ) , followed by pairwise comparisons with Holm - Bonferroni correction . The pairwise comparisons conﬁrmed that the Disagree , Dis - cuss and Reconsider workﬂow resulted in signiﬁcantly higher F1 - scores than Baseline ( t ( 64 ) = − 3 . 44 , p < 0 . 01 ) , Reconsider Only ( t ( 44 ) = − 4 . 38 , p < 0 . 001 ) , and Disagree and Reconsider ( t ( 25 ) = − 3 . 02 , p < 0 . 05 ) . There were no detectable diﬀerences among the other pairings . These results conﬁrm our hypothesis H3a that worker deliberation im - proves the quality of the crowdsourced annotations . Furthermore , this result suggests that the improvement in correctness is not due to learning eﬀects or knowledge about group disagreement data , but due to the actual discussion process . 36 l 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Baseline ReconsiderOnly Disagree and Reconsider Disagree , Discuss and Reconsider F 1 − sc o r e Figure 3 . 4 : Individual workers’ answer quality in the Relation task across diﬀerent recon - sideration workﬂows . [ H3b ] For Q3 , we were also interested in whether our proposed deliberation workﬂow discourages groupthink , an undesirable eﬀect where discussion members tend to agree with the original majority answer within a group . For this question , we performed a close - up analysis on all groups with a composition of 2 vs . 1 , and analyzed whether the proportion of cases resolved in favour of the original majority vote was similar to the proportion of cases not resolved in favour of the original majority vote . We used a two - sided proportion test which conﬁrmed our null hypothesis H3b that both outcomes were not detectably diﬀerent , χ 2 ( 1 , N = 136 ) = 0 . 60 , p = 0 . 44 . In other words , unbalanced discussion groups were equally likely to converge to the original majority opinion as they were to achieve the opposite outcome , i . e . , leave the case unresolved or converge to the original minority opinion . Our quantitative results provide evidence that our proposed deliberation workﬂow eﬀectively discourages groupthink . [ H3c ] For Q2 , we identiﬁed factors that contribute to the resolvability of a case . For hypothesis H3c , we investigate whether some of the factors considered in Q2 , e . g . , sources of disagrement and the extent to which members contributed equally , also predict whether or not a case will be resolved correctly . We performed an analysis on the subset of cases for which ground truth was available and which were resolved in the deliberation process . We used a similar step - wise logistic regression procedure as for Q2 , deﬁning the correctness of 37 the ﬁnal consensus label as the dependent variable and including the same set of predictor variables . Results ( Table 3 . 5 ) show that certain sources of disagreement made the correct resolution of a case more likely ( Missing Context ) or less likely ( Expertise Needed ) . The extent to which members contributed equally ( # Words Min / Max ) was a negative predictor for correct resolution . These results partially conﬁrm our hypothesis H3c . Table 3 . 5 : Logistic model for understanding the likelihood of resolving a case correctly . Model Parameters Variable ˆ β Std . Error t p - value Expertise Needed - 3 . 89 1 . 95 - 1 . 99 * Missing Context 2 . 91 1 . 42 2 . 05 * # Words Min / Max - 3 . 43 1 . 73 - 1 . 99 * To summarize , we provide evidence showing that worker deliberation signiﬁcantly im - proves the quality of crowdsourced annotations ( H3a ) , while discouraging undesirable be - haviour such as groupthink ( H3b ) , and that certain sources of disagreement and the extent to which members contribute equally help predict the correctness of the ﬁnal resolution ( H3c ) . 3 . 1 . 8 Discussion In this section , we studied how deliberation aﬀects resolvability and answer accuracy , and how deliberation outcomes depend on task subjectivity . Our results demonstrate that legit - imate reasons for disagreement can vary by task , and worker deliberation can help resolve some of these cases . Importantly , we identiﬁed several factors ( such as the level of initial consensus , the amount and quality of deliberation activities , and sources of disagreement ) that contribute to case resolvability . We reinforced the ﬁnding that deliberation can in - crease answer accuracy and the importance of verbal discussion in this process . Finally , our deliberation workﬂow discouraged undesirable behaviour , such as groupthink . Our anonymized data set is publicly available for research reuse . Generalizability and Applications Our empirical ﬁndings are based on experiments using two speciﬁc task types , one subjec - tive sarcasm detection task , and one objective ( person - to - place ) relation extraction task . 38 Caution is warranted for translating these results to a broader set of classiﬁcation tasks and data modalities . However , the deliberation workﬂow we propoosed is general , and can be applied to classiﬁcation tasks involving other data types ( e . g . , images , videos , time series data ) , given minor modiﬁcations to the interface to facilitate annotation of evidence in other modalities . In some complex scenarios requiring hierarchical decision processes , our workﬂow will need to be modiﬁed . For example , in image classiﬁcation , individual image features might be disambiguated ﬁrst before resolving disagreement on the image level . Our results revolve around atomic classiﬁcation problems in paid crowdwork , and it is expected that deliberation processes in more complex settings would give rise to more complex dynamics ( e . g . , circular disagreement in sequential classiﬁcation ) . In general , deliberation workﬂows like ours can be particularly useful in domains where inter - rater disagreement is rather the norm than exception , e . g . , medicine [ 44 , 36 , 118 ] . Beyond enabling workers to discuss and reconsider ambiguous cases , our deliberation work - ﬂow collects rich information from human annotators ( e . g . , conﬁdence levels , arguments , assumptions , examples , inferences , relevant features from the data , and meta information about sources of disagreement ) that can be used to teach both humans and machines . For example , prior work has shown that asking annotators to highlight relevant features in input data ( i . e . , words in a text document or regions in an image ) can improve machine inference in sentiment analysis tasks [ 133 , 151 , 152 , 153 ] and visual category learning [ 39 ] . More generally , we posit that future eﬀorts towards interpretable machine learning can use data produced through group discussion to analyze , replicate and mimic human deci - sion making processes . While some of the less structured deliberation output ( e . g . , verbal arguments ) may not yet be fully parsed by automated methods , human learners could signiﬁcantly beneﬁt from edge - case examples coming with discussions and highlighted fea - tures from more experienced annotators . Existing eﬀorts to optimize or harness workers’ ability to learn complex tasks [ 41 , 104 ] could thus leverage data produced through worker deliberation . Another outcome of deliberation could be the reﬁnement and disambiguation of anno - tation guidelines or category deﬁnitions for expert tasks . For example , scoring manuals in the ﬁeld of medical imaging undergo regular revisions to increase inter - rater reliabil - ity [ 36 , 118 ] , a procedure which could beneﬁt from structured and web - based deliberation workﬂows . Design Considerations for Classiﬁcation Tasks Redundant Labeling . Our results demonstrate that workers can predict levels of inter - rater disagreement signiﬁcantly better than random for certain task types . Prior work has shown 39 that being able to predict answer diversity can reduce cost because fewer annotators are needed when answer agreement is expected [ 64 ] . Enlisting human capabilities to predict answer agreement incurs minimal extra cost because it requires one additional human response for each data object . Task Interfaces . Our results show that certain sources of disagreement are more likely to occur for some task types than for others . In classiﬁcation tasks ( like our Relation task ) that require annotators to make decisions solely based on the information provided in the data , the interface could remind annotators who indicate “Missing Context” as the reason for anticipating disagreement to minimize ungrounded assumptions about any latent contextual information . Deliberation Workﬂow . We found a variety of factors that aﬀect case resolvability . De - liberation workﬂows for crowdsourced classiﬁcation tasks should therefore be equipped with incentive mechanisms to reduce or strengthen the eﬀect of certain factors in a task - speciﬁc manner . For example , in task types where objectivity is possible and the goal is to ﬁnd one correct answer , deliberation procedures should have incentives to reduce the impact of undesirable behaviour ( e . g . , stubbornness or lack of care ) and undesirable group dynamics ( like groupthink or lack of communication ) on the ﬁnal discussion outcome . Deliberation workﬂows for more subjective classiﬁcation tasks where the goal is to uncover multiple divergent , but equally valid interpretations of the task and data ( e . g . , sentiment analysis , relevance rating , text translation ) should incentivize group members to be assertive about their interpretation , and change their assessment only under certain conditions ( e . g . , when false assumptions or illogical conclusions are pointed out ) . Various deliberation systems have been proposed in complex domains like public de - liberation [ 50 , 84 , 85 ] , on - demand fact checking [ 83 ] , and knowledge base generation [ 154 ] . While our study is primarily embedded in the domain of paid crowd work , our empirical ﬁndings and some aspects of our workﬂow design may be informative for deliberation sys - tems in general . For example , our insight that sources of disagreement , when captured in structured form , can help predict case resolvability ( H2a ) could be leveraged to categorize debates and streamline consensus building . Our ﬁnding that equal contribution among discussion members seemed to negatively aﬀect case resolvability ( H2c ) and ﬁnal answer correctness ( H3c ) was surprising as balanced contribution is often considered beneﬁcial for fruitful discussion . This ﬁnding could inspire future investigations into the balance between active and passive forms of contribution to deliberation , related to work by Kriplean et al . [ 85 ] on active listening in web discussions . Another potentially counter - intuitive ﬁnding of our work is the fact that unbalanced ( 2 vs . 1 ) groups were less likely to resolve a case than balanced ( 1 vs . 1 ) groups ( H2d ) . To our knowledge , our study is the ﬁrst to investigate the eﬀect of initial consensus level on case resolvability by having multiple independent groups 40 of size two or three discuss the same case in crowdsourced classiﬁcation tasks . While one may expect that unbalanced groups converge faster , one possible explanation for our obser - vation is the added complexity of communication and coordination among three versus two group members . We showed that a perceived lack of expert knowledge was associated with resolving disagreements incorrectly ( H3c ) . This result has interesting connections to prior work on integrating on - demand fact checking into public deliberation [ 83 ] , suggesting that crowdsourcing workﬂows could beneﬁt from similar approaches for on - demand provision of expertise . Limitations We studied the eﬀects of deliberation in the context of two binary classiﬁcation tasks and small groups consisting of only two or three members . In practice , many classiﬁcation problems have more than two classes and discussion groups can also be larger . Future work can investigate new methods for scaling to more complex classiﬁcation problems and more complex group structures . A practical limitation of our workﬂow is its reliance on multiple rounds of synchronous communication and potential attrition between stages . Dropout was highest after the ﬁrst stage in our workﬂow , suggesting promising future research on incentives for workers to return for multiple consecutive sessions , or on systems that initiate discussions immediately as disagreement arises before workers leave the platform . Our deliberation workﬂow enables workers to consider alternative views on ambiguous cases through discussion , and produces useful data such as arguments , examples , and ev - idence . However , deliberation also incurs a cost in terms of time . A promising area for future work is to develop techniques to improve the eﬃciency of the deliberation process [ 42 ] and to characterize the cost - beneﬁt of using real - time deliberation in task workﬂows . Another direction for future work is to explore other deliberation incentives beyond mon - etary compensation , including peer - based reputation systems for constructive deliberation [ 52 ] . In more complicated incentive schemes , moderators could be rewarded for establishing a balanced and constructive deliberation among group members . 3 . 1 . 9 Conclusion This section contributes novel insights into the circumstances and outcomes of worker de - liberation for handling inter - rater disagreement in crowdsourced text classiﬁcation tasks with varying degrees of inherent subjectivity . Based on a custom - designed workﬂow for 41 real - time worker deliberation , we investigated the impact of various factors on the proba - bility that a disagreement among small groups of crowdworkers will be resolved through synchronous group discussion . Our results suggest that the reasons for and level of the initial disagreement , the amount and quality of deliberation activities , as well as the task and case characteristics play a role for resolvability . To encourage future work in the ﬁeld of worker deliberation , we publish our data set including all original classiﬁcations , discussion comments , text highlights , and revised positions from crowdworkers . Future work includes developing and validating new deliberation protocols and demonstrating how the informa - tion produced by worker deliberation can be used to train both humans and machines . 42 3 . 2 Expert Deliberation for Image Labeling With this second study on group deliberation in data labeling , our focus starts to shift from the domain of novice crowd work to the expert domain of medical data interpretation . Note that in the medical domain , the term adjudication is commonly used as a synonym of group deliberation . We therefore use both terms interchangeably throughout the remainder of this dissertation . This section explores the process of group deliberation among medical doctors in the context of interpreting and classifying medical images for the presence and severity of diabetic retinopathy ( DR ) , a complication of diabetes that can lead to vision loss . In particular , we present and evaluate a remote , tool - based and asynchronous system and structured grading rubric for adjudicating image - based assessments . We compare three diﬀerent procedures for adjudicating DR severity among panels of retina specialists : ( 1 ) in - person adjudication ( Baseline ) ; ( 2 ) remote , tool - based adjudication for assessing DR severity alone ( TA ) ; ( 3 ) remote , tool - based adjudication using a feature - based rubric ( TA - F ) . Our results suggest that remote , tool - based adjudication presents a ﬂexible and reliable alternative to in - person adjudication for DR diagnosis , and that feature - based rubrics can help accelerate consensus for tool - based adjudication of DR without compromising label quality . 3 . 2 . 1 Motivation Diabetic retinopathy ( DR ) is one of the leading causes of vision loss worldwide [ 141 ] . The process of grading DR severity involves the examination of the retina and the assessment of several features , such as microaneurysms ( MAs ) , intraretinal hemorrhages , and neo - vascularization [ 6 ] . In a teleophthalmology setting for remote screening , certiﬁed graders examine retinal fundus images to determine the presence and severity of disease as it ap - pears in a two - dimensional ( 2D ) photograph [ 134 ] . Prior work has shown that this process of human interpretation is subject to individual grader bias , as demonstrated by high intergrader variability , with kappa scores ranging from 0 . 40 to 0 . 65 [ 1 , 132 , 87 , 54 , 119 , 88 ] . This moderate - to - poor agreement between graders has led to diﬃculties in reliable evaluation of both individual graders as well as assistive technologies . Yet , due to lim - ited access to skilled healthcare providers , there continues to be a surge in interest in the development of assistive technologies , such as deep - learning systems , resulting in a sharp increase in the demand for high - quality reference standards of labeled - image data [ 63 , 82 , 115 , 62 , 140 , 122 , 47 , 58 , 56 , 4 ] . Prior work has examined diﬀerent methods for 43 resolving disagreements among experienced graders when creating a reference standard [ 82 ] , including majority vote , arbitration of disagreements by a more senior grader , and in - person adjudication among expert panels . In ophthalmology , a recognized method to obtain a reliable reference standard is expert adjudication of images [ 82 , 142 , 49 ] . Multiple experienced doctors independently grade images and discuss disagreements until resolved . Such “in - person” adjudication has been shown to produce higher - quality labels [ 82 ] but can be challenging to schedule : it requires coordination of multiple , highly experienced specialists for in - person sessions , and even small image sets on the order of a few thousand cases can take months to adjudicate due to clinical scheduling conﬂicts . In this study , we presented and evaluated a tool - based system for adjudicating images that was suitable for remote grading and removes the need for in - person sessions . Our system allowed doctors to discuss and resolve disagreements on diagnoses remotely , without convening at a set time and place . The practices described in this section aimed to increase the eﬃciency and ﬂexibility of adjudication , while maintaining the quality of the labels produced . We evaluated our system in the context of DR severity grading based on retinal fundus images . In addition , we proposed an adjudication system with the ability to impose an explicit structure on the adjudication process by organizing the process of image interpretation around a set of discrete , detailed evaluation criteria . We investigated the eﬀects of such a structure on the eﬃciency and reliability of adjudication for DR grading . Speciﬁcally , we presented a feature - based rubric for adjudication of DR severity grades , in which graders assess individual features ( MAs , hemorrhages , neovascularization , etc . ) in addition to overall DR severity . Taken together , these improvements allow high - quality reference standards to be ob - tained by the community , and have the further beneﬁt of oﬀering ﬂexibility for individual graders to schedule their reviewing activity around their clinical duties . 3 . 2 . 2 Methods Experimental Design The experiment conducted for this study compared three diﬀerent adjudication procedures for assessing DR severity based on retinal fundus images as follows : in - person adjudication ( Baseline ) ; remote , tool - based adjudication ( TA ) for assessing DR severity alone ; and remote , tool - based adjudication using a feature - based rubric to assess DR severity ( TA - F ) . 44 Table 3 . 6 : Baseline Characteristics The experiment implemented a between - subjects design in which independent panels of three retina specialists each graded and adjudicated the same set of images following one of three adjudication procedures ( Baseline , TA , TA - F ) . We describe the image set , each of the three adjudication procedures , and details about the retina specialist graders below . For each design , graders were primarily assessing DR severity , but not diabetic macular edema ( DME ) . Image Set We used a subset of 499 images ( Table 3 . 6 ) from the development dataset used by Krause et al . [ 82 ] The image set consisted of central ﬁeld - of - view images obtained from patients who presented for DR screening at three eye hospitals in India ( Aravind Eye Hospital , Sankara Nethralaya , and Narayana Nethralaya ) . The image set was sampled to include approximately 50 % of cases that had some level of DR [ 82 ] . Anonymized patient codes 45 were provided from two of three hospitals , allowing us to verify no patient duplication . For the third hospital , patient codes were not provided ; this allows for the possibility that 169 images from this hospital may contain multiple images from the same patient ; given that these were sampled from a much larger set of images , duplication is unlikely . Image sizes ranged from 640 × 480 to 2588 × 3388 pixels , and were presented to adjudicators at the original resolutions . All images were de - identiﬁed according to the Health Insurance Portability and Accountability Act Safe Harbor before transfer to study investigators . Ethics review and institutional review board exemption were obtained through the Quorum Review institutional review board ( Seattle , WA ) . Adjudication Procedures Baseline Adjudication . Following the practices described in Krause et al . [ 82 ] , our Baseline adjudication procedure consisted of the following three stages : ( 1 ) an initial inde - pendent evaluation ; ( 2 ) remote review of disagreements ; and ( 3 ) in - person discussion and ﬁnal resolution of remaining cases . For the ﬁrst stage , three fellowship - trained retina specialists undertook independent grading of the image set . Images in which the independent graders agreed were considered resolved . Next , each of the three retina specialists independently reviewed one - third of the remaining images with any level of disagreement . This independent review procedure was facilitated through the use of online spreadsheets . Cases that remained unresolved after the independent review round were discussed by all three retina specialists in person . During the in - person sessions , all three retina specialists were present at a set time and place , and conﬂicting grades were reviewed and adjudicated within the panel until all specialists came to an agreement . The time from start of independent grading to full adjudication for the image set was around 3 months . While the total time each grader spent on grading and adjudication activities was not tracked precisely , a substantial portion of the 3 - month period was due to diﬃculties in scheduling the retina specialists to physically convene for in - person discussions . Tool - Based Adjudication ( TA ) . To ensure the continuity of the adjudication pro - cess and to reduce the logistic overhead associated with in - person adjudication , we designed and implemented a tool - based system for remote adjudication that removes the need for in - person sessions ( Figure 3 . 5 ) . Similar to the Baseline procedure , the TA procedure com - mences with independent grading : each panel member ﬁrst assesses each image for DR severity . Next , those images with any level of disagreement are reviewed by one panel member at a time in a round - robin fashion until agreement is reached for the given case ( Figure 3 . 6 ) . For each review round , the active grader reviews all grades and comments 46 Table 3 . 7 : Comparison of adjudication procedures provided in previous rounds , re - grades the given image for DR severity , and provides more detailed comments , or replies to other graders’ comments . To handle cases with persistent disagreement , the TA procedure imposes a limit on the number of review rounds for each case . In our studies , each case was limited to a maximum of 15 review rounds ( i . e . , 5 reviews per grader for a panel of 3 graders ) . See Table 3 . 7 for a comparison of the Baseline and TA adjudication procedures . Tool - Based Adjudication With Feature Rubric ( TA - F ) . Disagreements over DR 47 Figure 3 . 5 : Process diagram illustrating remote TA ; images are ﬁrst graded independently by each panel member ( round 0 ) ; cases with any level of disagreement after independent grading are reviewed by all graders in a round - robin fashion ( rounds 1 – N ) ; the procedure ends after N review rounds . severity can arise for various reasons ( e . g . , due to divergent assessments of the presence and extent of individual features or due to divergent interpretations of whether a reti - nal pathology is diabetic in nature or not ) . One beneﬁt of the tool - based adjudication procedure proposed in this section is the ability to impose an explicit structure to the adjudication process by introducing prompts for individual , detailed evaluation criteria . This ability can be leveraged to remind graders of the speciﬁc criteria they should apply to assess an image ( e . g . , from standardized grading guidelines ) so that discussions over po - tential disagreements are grounded in predeﬁned factors relevant to the overall diagnostic 48 Figure 3 . 6 : Illustration of the round - robin approach for remote TA in the context of DR severity grading . decision . In our experiment , we developed a feature - based rubric in which graders were ﬁrst prompted to assess each image for a set of DR - related features before assessing the image for overall DR severity . Following the International Clinical Diabetic Retinopathy ( ICDR ) disease severity scale [ 6 ] , we included the following 10 features in the TA - F procedure : MAs , cotton - wool spots , hard exudates , retinal hemorrhage ( heme ) , venous beading ( VB ) , in - traretinal microvascular abnormalities ( IRMA ) , neovascularization or ﬁbrous proliferation , preretinal or vitreous hemorrhage , laser scars from panretinal photocoagulation , and laser scars from focal photocoagulation . Graders assessed whether each feature was present , not present , or ungradable . For heme , graders also assessed whether any retinal hemorrhage was extensive in four quadrants , based on standard photo 2A from the Early Treatment Diabetic Retinopathy Study ( ETDRS ) [ 2 ] . For VB , graders also assessed whether deﬁ - nite venous beading , if present , was observed in two or more quadrants , based on ETDRS standard photo 6A [ 2 ] . Similarly , for IRMA , graders assessed whether any IRMA was prominent , based on ETDRS standard photo 8A [ 2 ] . Intergrader disagreement may not 49 Figure 3 . 7 : Grading interface for remote TA - F for DR severity assessment . Grader pseudonyms ( RX , RY , RZ ) are used to associate grading decisions and discussion com - ments from previous rounds with speciﬁc ( anonymized ) grader identities . The current grader’s pseudonym is highlighted with bold white font ( see RZ ) . The panel on the right - hand side lists all prompts included in the TA - F procedure . only arise over the presence or severity of disease , but also over the speciﬁc classiﬁcation and etiology of an observed pathology . In particular , the appearance of DR may resemble other forms of retinal disease , such as hypertensive retinopathy ( HTN ) , retinal vein occlu - sion ( RVO ) , and retinal artery occlusion ( RAO ) [ 14 , 17 ] . Graders were therefore prompted to assess for the presence of HTN , RVO , and RAO in addition to providing a DR severity assessment . In the adjudication interface ( Figure 3 . 7 ) , disagreements were visualized for 50 both feature - and diagnosis - level decisions to inform adjudicators about assessments from other panel members . Full agreement within a panel was only required regarding the over - all gradeability of an image as well as for the diagnosis decisions ( DR , HTN , RVO , RAO ) in order to resolve a case ; cases could be resolved despite disagreements on individual features . Graders We recruited 14 American Board of Ophthalmology – certiﬁed fellowship - trained retina spe - cialists to form ﬁve adjudication panels , including one panel for the Baseline procedure , two panels for the TA procedure ( Panels A and B ) , and two panels for the TA - F procedure ( Panels C and D ) . Due to the limited availability of retina specialists , one of 14 graders participated in two panels ( Baseline and TA Panel B ) ; otherwise , each grader participated in one panel only . Participating retina specialist graders completed their fellowship train - ing between the years 2009 and 2017 and the number of years in practice ( post fellowship ) at the time of participation in the study ranged from 0 . 5 to 8 . 5 years . Evaluating Tool - Based Adjudication We evaluated the tool - based adjudication procedures ( TA , TA - F ) for reliability and ef - ﬁciency . Reliability was assessed in terms of agreement with the Baseline adjudication procedure , using Cohen’s quadratically weighted kappa score [ 31 ] . A nonparametric boot - strap procedure [ 30 ] with 2000 samples was used to compute conﬁdence intervals ( CIs ) for the kappa scores . The weighting function for the calculation of kappa scores was the square of the stepwise distance between DR grades on a ﬁve - point ordinal scale ( e . g . , a disagreement between no DR and severe nonproliferative diabetic retinopathy [ NPDR ] , which are three steps apart on the ICDR scale , would receive a weight of 3 2 = 9 when calculating kappa ; larger disagreements would more strongly reduce this metric ) . Images unanimously deemed ungradable and those with persistent disagreement after 15 review rounds in any of the panels were excluded from kappa score calculations . Exact agreement rates and strikeout rates ( i . e . , the fraction of images for which grades diﬀered by > 2 steps ) were calculated as additional measures of agreement for each panel pair . The eﬃciency of TA versus TA - F was evaluated using the number of review rounds required to resolve each case in a given panel , and using the cumulative percentage of cases resolved in each round , including independent grading ( round 0 ) and the subsequent review rounds ( rounds 1 – 15 ) . We used the standard permutation test to assess the statistical 51 signiﬁcance of these diﬀerences [ 30 ] . Due to software - related irregularities , in which the full - adjudication discussions were not recorded , 11 images ( 2 % ) were excluded from the analysis . Results are based on the remaining 488 cases . For TA - F speciﬁcally , the relative eﬃciency of resolving disagreements on each of the rubric criteria was assessed as the number of review rounds required to reach agreement on a given criterion , or as the round number in which a case was closed despite disagreement on the criterion . 3 . 2 . 3 Results Reliability Table 3 . 8 : Inter - panel agreement among all pairs of panels as Cohen’s kappa . Table 3 . 9 : Inter - panel agreement among all pairs of panels as exact agreement . Table 3 . 10 : Inter - panel agreement among all pairs of panels as strikeout rate . Remote TA grades showed high agreement with the Baseline adjudication procedure ( Table 3 . 8 ) , with Cohen’s kappa scores of 0 . 943 ( 95 % CI , 0 . 919 – 0 . 962 ) and 0 . 948 ( 95 % CI , 52 0 . 931 – 0 . 964 ) for the two panels assessing DR severity alone without the use of a feature rubric ( TA ) , and 0 . 921 ( 95 % CI , 0 . 886 – 0 . 948 ) and 0 . 963 ( 95 % CI , 0 . 949 – 0 . 975 ) for the two panels using the feature - based rubric ( TA - F ) . Both TA and TA - F showed high rates of re - producibility , as measured by the Cohen’s kappa score between the two independent panels for each procedure . The kappa score for agreement was at 0 . 932 ( 95 % CI , 0 . 911 – 0 . 950 ) be - tween the two panels in the TA procedure and at 0 . 919 ( 95 % CI , 0 . 882 – 0 . 949 ) for TA - F . Exact agreement rates ( Table 3 . 9 ) and strikeout rates ( Table 3 . 10 ) are reported as addi - tional measures of agreement for each pair of panels . Eﬃciency Figure 3 . 8 : Number of review rounds re - quired per case ( i . e . , number of rounds un - til agreement or 15 in case of persistent dis - agreement ) for each of the four adjudication panels . Figure 3 . 9 : Cumulative percentage of cases resolved per adjudication round for TA procedures . Cases adjudicated using TA - F were resolved in signiﬁcantly fewer rounds compared with assessing DR severity without the rubric ( TA ; p < 0 . 001 ; permutation test , Figure 3 . 8 ) . During independent grading ( round 0 ) , graders were in agreement for 72 % of all cases using TA - F , compared with 67 % TA , and to 58 % in Baseline in - person adjudication . Using 53 Figure 3 . 10 : Mean number of review rounds required per rubric criterion in remote TA - F . The Y axis indicates the number of rounds after independent grading until either agree - ment was reached for the given criterion ; or the case was closed due to overall agreement on the diagnosis level . Note that the mean number of review rounds may be below 1 because cases not requiring adjudication due to independent agreement were considered to have 0 review rounds . Green bars correspond to feature criteria , blue bars correspond to diﬀeren - tial diagnosis criteria . Error bars indicate the 95 % conﬁdence intervals . CWS , cotton - wool spot ; HE , hard exudate ; NVFP , neovascularization or ﬁbrous proliferation ; PRHVH , Pre - retinal or vitreous hemorrhage ; PRP , pan - retinal photocoagulation scars ; FLP , focal laser photocoagulation scars . TA - F , only 3 % of the cases required more than one full “round - robin” of reviews from the panel ( round 3 ) , compared with 9 % of the cases in the absence of the feature - based rubric ( Figure 3 . 9 ) . Both diﬀerences were statistically signiﬁcant under a permutation test of two panels for TA versus two panels for TA - F ( p = 0 . 004 for round 0 , p < 0 . 001 for round 3 ) . The only two cases with persistent disagreement after 15 rounds of review were observed in the TA procedure ( Panel A ) . Overall , cases assessed as mild NPDR or severe NPDR in the Baseline adjudication procedure showed the lowest rates of independent agreement ( i . e . , before adjudication ) , with agreement rates of 31 . 7 % and 44 . 6 % , respectively . Mild NPDR and severe NPDR were also the only two categories with any persistent disagreement ( 1 case each ) , and with the highest proportion of cases requiring more than two rounds of review for at least one of three graders ( 3 . 3 % and 1 . 8 % , respectively ) in order to reach a consensus . Among the 10 feature criteria included in the TA - F rubric , assessments of the presence and extent of heme , VB , and IRMA required the greatest number of review rounds on av - erage ( Figure 3 . 10 ) . As for the diﬀerential diagnosis section of the TA - F rubric , assessment of HTN required more review rounds on average than assessments of RVO and RAO . Finally , each of the four panels conducting tool - based adjudication completed all 499 54 images within 58 days from initial grading to full adjudication , with the fastest panel completing in 19 days . Note that these durations also include intervals of idle time in which the system waited for graders to complete their review passes , and that graders performed other labeling tasks during their own idle intervals . The total amount of time spent on grading and reviewing activities is therefore substantially lower than the corresponding end - to - end durations per panel . 3 . 2 . 4 Discussion As machine - learning methods become more common in ophthalmology , the need to accu - rately assess diagnostic performance grows . Algorithms that may be used to automate or augment aspects of eye care should be subjected to rigorous evaluation of their perfor - mance , against trusted reference standards . This in turn motivates the development of high - quality reference standards , a process that has received relatively little attention in the literature . Previous studies suggest that adjudication can not only reliably be used to evaluate DR severity , but should be the reference standard used in deep - learning algorithms [ 82 ] . While several methods may be used for this process , such as in - person adjudication among expert panels and arbitration of disagreements by a senior grader ( Domalpally A , et al . IOVS 2018 ; 59 : ARVO E - Abstract 4676 ) these methods rely on the time and expertise of certain physicians . In the present study , we present a tool - based system for remote expert adjudication of image - based interpretations and evaluate the system in the context of DR severity assessment . In this tool - based method , anonymity allows for an unbiased review of the image , with further clarity added by the rubric feature . Furthermore , the ﬂexibility inherent in the design increases its appeal and ease of use . Performance of Tool - Based Adjudication Our study suggests that remote , tool - based adjudication procedures can produce DR grades that are in high agreement with the reference standard of in - person adjudication while oﬀering a range of beneﬁts : an increase in ﬂexibility to accommodate graders’ schedules , the possibility to anonymize graders throughout the adjudication process to avoid potential biases grounded in grader identity or seniority , and the option to explicitly structure the adjudication process around detailed evaluation criteria . Research into eﬃcient and reliable procedures to produce high - quality grading decisions can be applied to manual screening in teleophthalmology settings and to the validation of 55 automated methods , such as deep - learning systems . In both cases , reliable classiﬁcation decisions are required to avoid potentially devastating consequences , such as missing cases of advanced disease . Validating the classiﬁcation performance against a reliable reference standard may be of particular importance for automated methods as , once deployed into a clinical screening setting , these methods can aﬀect large patient populations in a short amount of time . Beyond the evaluation of our remote TA procedure for adjudicating fundus images for DR severity assessment , we demonstrate how our proposed tool - based procedure can provide structure to the adjudication process itself using explicit prompts for detailed evaluation criteria . The resulting TA - F procedure leads to a signiﬁcant reduction in the number of rounds needed to resolve disagreements . One possible explanation for the ob - served eﬃciency improvement may be that the feature rubric helped graders communicate their rationale and the speciﬁc source of disagreement more eﬃciently than was otherwise achieved through free - form comments ( e . g . , by focusing communication on the speciﬁc guideline criteria that graders are instructed to factor into a diagnosis ) . Besides eﬃciency in communication , the guideline - centric rubrics may serve as a lightweight checklist , lead - ing graders to be more consistent in their individual practices . This may reduce variance or allow graders to externalize the diagnostic criteria in a way that reduces their task - related mental workload . As supporting evidence ( Figure 3 . 9 ) , the ﬁrst - round agreement rates were signiﬁcantly higher with the use of the rubric , even before further adjudication . Finally , the rubrics lead to the production of structured information ( i . e . , the speciﬁc eval - uation criteria applied in each case ) , facilitating detailed quantitative analyses to examine how and why disagreements arise both across a set of images and for individual cases . Still , there were speciﬁc features of the disease that required more discussion . While the explicit reasons for why heme , VB , and IRMA required the greatest number of review rounds are not clear , it is possible that the overlap between the objective ( i . e . , simple pres - ence or absence ) and subjective ( i . e . , extent and prominence ) features of these particular anatomic abnormalities led to more disagreement . Despite standard reference photographs to help guide whether or not the heme is extensive , VB is deﬁnite , or the IRMA is promi - nent , there is an inherent subjectivity to the process . Ultimately , the physician’s gestalt leads her to deﬁne disease severity . This same overall impression or pattern recognition may explain why venous and arterial occlusions resolved in fewer rounds , as these diseases have a hallmark appearance . HTN , on the other hand , can overlap with and mimic several other eye diseases , DR being the most common , and giving a deﬁnitive diagnosis based on a fundus photograph alone can be challenging . Exploring these feature - based discrepancies may provide more insight on how the model synthesizes the information within the image and also on how to continue to improve it . 56 Utility in Clinical Practice We believe the technology we describe here may have several clinical applications . First , our approach for remote adjudication is well suited for integration into existing telemedical workﬂows , which face the same problem of high intergrader variability as is the case for on - site clinical grading [ 134 ] . Here , our proposed system can help resolve ambiguous cases through group decision - making [ 15 ] on demand to improve clinical outcomes on a patient - by - patient basis . Apart from adjudication , our tool’s functionality of integrating feature - level rubrics into the image interpretation process may facilitate grading by individual graders in diﬃcult cases , by helping list and systematize the image ﬁndings . Second , expanding TA and TA - F use for rare conditions or diﬃcult to diagnose cases , where a patient may otherwise be advised to travel to seek a second or third opinion , could potentially have an important impact on time to diagnosis and treatment , which are likely to impact quality of life and healthcare costs . Third , and perhaps most importantly , our adjudication tool lends itself naturally for generating highly reliable and trusted reference standards for the validation of automated methods , such as deep - learning models . The process of building and evaluating deep - learning models typically involves at least the three following distinct datasets : a ‘devel - opment’ dataset used to train the model , a ‘tuning’ dataset used to select high - performing model candidates during the training phase , and a ‘validation’ dataset used to benchmark the performance of the ﬁnal model . While development datasets , in many cases , consist of tens or hundreds of thousands of training examples , the datasets used for tuning and validation are typically smaller scale , on the order of several hundred up to a few thousand cases . The methods presented here can facilitate the creation of tuning and validation sets with a substantially reduced overhead , due to lower time and coordination requirements as compared to in - person adjudication . In this study , we demonstrated the feasibility of remote TA for a set of 499 images , positioning it as a useful procedure especially for generating tuning and validation datasets . The availability of a highly trusted validation dataset is of critical importance especially for so - called “black box” systems , where there is limited ability to understand how the model makes its diagnosis . As methods for re - mote adjudication in clinical decision - making scale , it may become feasible to produce adjudicated datasets large enough to be used for training , which would extend the current state - of - the - art in model development . 57 Limitations Our study is not without limitations . First , while we quantify the reliability of each adju - dication method using consensus grades from two independent expert panels , the metrics reported in this section remain relative ones given the lack of an absolute , objective gold standard for DR severity assessment in the context of our study . To alleviate this issue , further work may benchmark adjudication decisions from digital fundus images against more rigorous diagnostic procedures ( e . g . , dilated fundus exam by a retina specialist ) [ 7 ] or objective outcomes , such as any future development of blindness . Second , graders partic - ipating in this study were practicing retina specialists rather than research - grade reading center graders . While reading center gradings may be a more standardized gold standard , the incorporation of insights from clinical practice into the grading may render our results more applicable to real - life scenarios than may otherwise be the case with research - grade readings . Third , we only adjudicated DR severity , but did not adjudicate DME . Agree - ment levels may be lower overall given diﬃculties in diagnosing DME on 2D fundus photos . Finally , the grading decisions in this study were based on fundus images without accompa - nying patient information or clinical records . In practice , DR severity assessment based on digital fundus photography should consider patient history and be complemented by more rigorous diagnostic procedures including dilated fundus examination by a trained eye care professional and optical coherence tomography ( OCT ) or other imaging techniques when indicated to conﬁrm the diagnosis [ 7 ] . Our TA - F procedure included a mechanism to assign pseudonyms to graders to avoid biases grounded in grader identity . Anonymization of graders was not possible during the in - person discussions of the Baseline adjudication procedure , and could not be done for the TA procedure because the functionality for grader anonymization was added at a later stage of our tool’s development . Anonymization of members in group - based decision processes generally reduces incentives for groupthink behavior , and thus tends to slow down consensus formation rather than accelerating it [ 131 , 109 ] . Thus , we reason that our reported beneﬁt of TA - F is an underestimate of the true beneﬁt , relative to comparing TA and TA - F when neither ( or both ) is anonymized . Our results show that remote , tool - based adjudication can help organize the consensus formation process especially for those cases that can be resolved in the ﬁrst few review rounds , but falls short of fully alleviating the problem of small portions of disagreement cases persisting over several review rounds . Future work may explore methods to accelerate resolution for such hard cases , for example , by investigating if aggregation methods like majority vote after the ﬁrst two review rounds are suﬃcient proxies for ﬁnal adjudicated decisions , or by implementing automatic techniques to schedule video conference calls to 58 discuss small collections of hard cases among panelists without the need to involve a human coordinator . Other promising avenues for future research revolve around the development of feature rubrics for improved eﬃciency and reliability of adjudication procedures . Understanding which strategies and practices for rubric development generally result in the biggest im - provements across various diagnostic tasks would be helpful for the community so that other researchers can reliably produce eﬀective rubrics for diﬀerent areas of medical image interpretation . 3 . 2 . 5 Conclusion Remote , tool - based adjudication presents a reliable alternative to in - person adjudication for DR severity assessment . The system allows ﬂexibility so that graders can schedule their reviewing around their clinical duties . Additional beneﬁts include the option of blinding graders from the identity of other panel members and the ability to structure the discussion of controversial cases around a set of discrete evaluation criteria . We found that feature - based rubrics for DR can help accelerate consensus formation for tool - based adjudication without compromising label quality . 59 3 . 3 Expert Deliberation for Time Series Labeling We conclude this chapter with a ﬁnal case study on group deliberation in data labeling , again within the domain of medical decision making . In this section , we build on our insights from section 3 . 2 to design and implement CrowdEEG , an online platform enabling groups of medical experts to collaboratively label and adjudicate complex medical time series data . Prior work shows that expert disagreement can arise due to diverse factors including expert background , the quality and presentation of data , and guideline clarity . In this section , we study how these factors predict initial discrepancies in the context of medical time series analysis , examining why certain disagreements persist after adjudication , and how adjudication impacts clinical decisions . Results from a case study with 36 experts and 4 , 543 adjudicated cases in a sleep stage classiﬁcation task show that these factors con - tribute to both initial disagreement and resolvability , each in their own unique way . We provide evidence suggesting that structured adjudication can lead to signiﬁcant revisions in treatment - relevant clinical parameters . Our work demonstrates how structured adjudi - cation can support consensus and facilitate a deep understanding of expert disagreement in medical data analysis . 3 . 3 . 1 Motivation Receiving a reliable diagnosis is one of the fundamental steps in health care delivery ; it sheds light on the state of a patient’s health condition and informs subsequent treatment decisions . The diagnostic process often requires visual analysis of medical data ( e . g . , x - rays , ultrasounds , electrophysiological signals ) and a subsequent classiﬁcation thereof ( e . g . , normal vs . abnormal ) . Expert classiﬁcation tasks relying on visual analysis , however , tend to give rise to expert disagreement due to their inherently subjective nature—and the medical domain presents no exception in this regard . In certain non - expert domains ( e . g . , crowdsourcing ) , techniques like majority vote and other computational methods ( e . g . , EM algorithm ) are used to aggregate divergent human assessments into what is assumed to be a “correct” answer . By contrast , other approaches acknowledge that disagreement carries valuable information [ 44 , 129 ] , and that resolving disagreements is not always possible or desirable , even if human graders are given the opportunity to deliberate on a case [ 125 ] . In the clinical domain , collaborative , team - based decision making has long been deemed superior to individual diagnosis by the National Academy of Medicine [ 12 ] . Little is understood , however , about the factors that contribute 60 to expert disagreement and processes that facilitate resolution of disagreement in medical data analysis from a socio - technical perspective . Our work addresses this research gap by studying the sources and dynamics of expert disagreement in medical data analysis through structured , collaborative adjudication , i . e . , the process of reviewing and potentially resolving divergent assessments collectively as a group . Our ﬁndings from an observational case study with 36 experts and 4 , 543 adjudi - cated cases in a sleep stage classiﬁcation task reveal that diverse factors , including expert background , the quality and presentation of data , and classiﬁcation guidelines , contribute to both initial disagreement and resolvability . Our ﬁndings also demonstrate how adju - dication can lead to signiﬁcant revisions in experts’ quantiﬁcation of diagnostic markers , which in turn have the potential to impact patients’ lives through changes in treatment outcomes . Our main contributions are : 1 . We demonstrate how the sources and dynamics of expert disagreement in medical data analysis can be understood through collective adjudication . 2 . We conducted an observational study to analyze expert disagreement , illuminating diverse factors impacting the extent of disagreement , including expert background , the quality and presentation of data , and guideline clarity . 3 . We contribute a structured adjudication workﬂow to capture expert rationales in a guideline - centric and interoperable format . In what follows , we detail the design evolution of our structured adjudication workﬂow , outline our research questions , methods , and ﬁndings , and conclude with a discussion of use cases and design considerations for our approach . 3 . 3 . 2 Application Domain We embed our work in the ﬁeld of biomedical time - series classiﬁcation , an expert domain with typically low inter - rater agreement rate , and deploy our adjudication system in the context of sleep stage classiﬁcation , where the average rate of agreement among two in - dependent experts is approximately 82 % with Cohen’s kappa of 0 . 76 [ 36 ] . Sleep stage classiﬁcation lends itself as a task for our case study , as it not only involves lengthy and complex guidelines likely to spur inter - rater disagreement , but sleep data includes a wide range of signal modalities , many of which are integral parts of other diagnostic procedures in medicine . The task of sleep stage classiﬁcation involves mapping ﬁxed - length segments 61 of a polysomnogram , i . e . , a continuous multimodal medical time series recording , to one of ﬁve sleep stages — Wake , Rapid Eye Movement ( REM ) sleep or one of three non - REM sleep stages ( NREM 1 , NREM 2 , NREM 3 ) . The resulting sequence of sleep stages , called a hypnogram , serves as a relevant artifact in the diagnostic process for various sleep - related disorders and other neurological diseases . The classiﬁcation of time series segments into sleep stages is based on the presence of distinguishing features of the EEG waveform and other supportive signal modalities like respiratory information . 3 . 3 . 3 Structured Adjudication For the purpose of our study , we designed and implemented a workﬂow and interface for collective expert adjudication of classiﬁcation decisions in the context of medical data anal - ysis . Here , we describe our iterative design process and the resulting design considerations that informed our ﬁnal design and implementation . Design Evolution Our design process was structured into three steps : ( 1 ) formative sessions of in - person adjudication to acquire a better understanding of inter - personal dynamics and expert ar - gumentation patterns used in medical adjudication , ( 2 ) adjudication via video conference as a testbed for remote adjudication , and ( 3 ) web - based adjudication informed by insights from the ﬁrst two steps . In all three steps , the CrowdEEG signal viewer 2 was used for independent classiﬁcation , but it was only in the ﬁnal stage where adjudication of disagree - ments was conducted directly within the web interface . In - Person Adjudication . An initial formative session of in - person adjudication was conducted with three board - certiﬁed sleep technologists . After an initial round of inde - pendent classiﬁcation , researchers organized an in - person meeting in the hospital to host adjudication discussions for select disagreement cases . All members of the expert panel convened at a set time and place to collectively discuss disagreements in front of a shared screen . 106 minutes of discussion content was recorded ( using screen capture and audio recording ) , transcribed , and analyzed . Our ﬁndings led to several design considerations both general and speciﬁc to our data modality : • Discussions were primarily centered around the classiﬁcation guidelines , including the presence of individual patterns or features in the data . This observation primarily 2 http : / / crowdeeg . ca 62 informed our motivation for integrating classiﬁcation guidelines into the ﬁnal web - based approach . • Inter - personal dynamics occasionally distracted from the case at hand ( e . g . , jokes about the grading style or background of other panel members ) , or caused bias in favour of certain experts ( e . g . , the most dominant ones or the ones with highest perceived expertise ) . Based on this ﬁnding , we decided to hide information about expert identity and expert background in our web - based implementation . • For some disagreement cases , experts requested to review data windows before or after the case in question ( speciﬁc to sequential data ) . In addition , resolving cer - tain disagreements triggered consensus on short cascades of subsequent cases in the recording timeline . Based on these two insights , we decided to have experts review all cases in a given recording for our web - based procedure , one expert at a time , to account for any sequential dependencies . • The conﬁguration of the viewer ( e . g . , signal visibility and amplitude scaling ) played a role in discussing and resolving disagreements . We noticed that , for certain cases , adjusting the viewer settings triggered consensus without further argumentation . Inspired by this observation , we decided to allow experts to conﬁgure various aspects of the viewing interface , and to record viewer settings for each classiﬁcation decision to facilitate quantitative analysis . Remote Adjudication via Video Conference . In a second step , we conducted a 1 - hour experimental session for remote adjudication with the same three experts , this time using video conference as the communication medium . The cases discussed in this step were distinct from the cases previously discussed in person . All three panel members and one moderator ( whose role was to ensure adjudication discussions stayed on topic ) joined the video conference at the same time . Each expert was assigned one colour ( red , green , or blue ) that could be used to annotate the location and shape of characteristic features on a shared screen during discussion . Discussions were recorded ( via screen capture and audio recording ) and analyzed , resulting in additional ﬁndings : • Despite the fact that experts were not co - located in the same room , inter - personal dy - namics seemed to inﬂuence the discussion based on perceived grader experience and the eﬀectiveness of individual communication or argumentation skills . While part of this behavior may have been inﬂuenced by the fact that the same three experts had previously conducted in - person adjudication on separate cases , this observation 63 reinforced our design consideration to anonymize experts during adjudication and informed our choice of text as a communication medium during web - based adjudica - tion . • The logistics of scheduling multiple domain experts to collectively join a meeting at a set time even without the need for a co - located face - to - face setup proved to be prohibitive for a large - scale study . This realization motivated our decision to implement an asynchronous approach for our web - based adjudication workﬂow in which experts review disagreement cases in a round - robin fashion , one expert at a time . • The interplay of distinct features within the same classiﬁcation case , as well as dis - agreements over the exact transition boundaries between diﬀerent feature types were topics of contention and became evident through on - screen drawing . Inspired by this observation , we included measures of signal complexity in our data analysis , both with regard to the frequency domain ( i . e . , how complex is the signal overall ? ) and from a time - frequency view ( i . e . , how complex is the signal due to transitions over time ? ) . Web - based Adjudication . Our design considerations derived from the ﬁrst two steps informed an early prototype of our web - based adjudication workﬂow and interface . The primary motivations for moving the adjudication process to a web - based implementation were ( 1 ) the ability to orchestrate adjudication at a larger scale involving multiple concur - rent expert panels ( 2 ) , mitigation of certain undesirable factors observed during in - person adjudication and adjudication via video conference , and ( 3 ) the introduction of explicit structure to the process of collecting expert rationales for post - hoc quantitative analysis . Our ﬁrst iteration of the web - based adjudication workﬂow addressed the former two mo - tivations by reducing scheduling conﬂicts among experts through a round - based scheduling approach , by hiding information about grader identity and background , and by using text as a communication medium . We conducted a small - scale pilot using our initial prototype with three independent panels , each with three experts . The objective of the pilot was to validate the overall interface and workﬂow and to analyze open - ended discussion contents before attempting a more structured approach of collecting expert rationales . Open - ended discussion comments collected during the pilot were generally free of inter - personal comments , concise ( ranging from a few words to one or two sentences ) , and focused primarily on speciﬁc rules from the classiﬁcation guidelines including low - level features referenced therein . While the majority of comments matched this description , we noticed 64 ( a ) Rationale form . ( b ) Data view for disagreement case with expert discussion . Figure 3 . 11 : Interface for structured adjudication of classiﬁcation decisions in medical time series analysis . that few comments contained arguments not captured by the classiﬁcation guidelines ( e . g . , addressing implicit nuances with regard to ambiguous terminology used for individual rules in the guideline or referring to the assumed health condition of the patient ) . Based on these ﬁndings , we decided to proceed with integrating classiﬁcation guidelines into the workﬂow in an extensible and structured manner . We also decided to retain the option of providing open - ended comments throughout the process to cover the few cases in which guidelines were insuﬃcient for a comprehensive rationale . The remainder of this section outlines our ﬁnal design and implementation of web - based , structured adjudication . Rule - based Representation of Guidelines In our approach , classiﬁcation guidelines are represented as a set of inference rules matching a basic template : IF Evidence A Present AND Evidence B Present THEN Classify as X 65 Each rule deﬁnes a Boolean proposition or a conjunction ( AND connection ) of multiple propositions ( e . g . , rapid eye movements are present AND low - chin EMG tone is present AND low - amplitude , mixed - frequency EEG is present ) that need to be true in order to make a certain annotation decision ( e . g . , classify as REM sleep ) . We will later refer to the propositions on the left side as evidence criteria . More complex rules can be decomposed to match this template . For example , disjunctions ( OR connections ) can be split into multiple rules relying just on conjunctions . While our case study demonstrates the utility of our approach using a domain - speciﬁc guideline , the overall approach is domain - agnostic , borrowing basic concepts of proposi - tional logic . For our case study , we adapted a domain - speciﬁc classiﬁcation guideline [ 70 ] , translating it into a set of 36 separate inference rules . These rules referenced a set of 15 unique basic features whose presence were relevant for at least one of the rules . We also included placeholder rules , one for each of the possible classiﬁcation choices , that could be selected in case none of the other rules applied . Workﬂow Our ﬁnal workﬂow consists of two stages . First , all panel members independently perform an initial classiﬁcation pass on the entire data record . Second , each expert reviews and re - classiﬁes all disagreement cases in the record in a round - based fashion , one expert at a time . We describe both workﬂow stages below . Classiﬁcation . The entry point for participants is an automated email notiﬁcation with a link to sign into our web - based system to proceed with their classiﬁcation task . During initial classiﬁcation , each grader classiﬁes all cases in the data record ( i . e . , 30 - second windows within a sleep EEG ) into task - speciﬁc categories ( i . e . , one of ﬁve sleep stages ) . To account for sequential dependencies in the classiﬁcation process [ 128 ] as observed in the pilot study , graders can navigate back and forth through all cases and are free to adjust previous classiﬁcation decisions throughout their pass . Our pilot studies revealed that the speciﬁc way in which graders choose to view the data—i . e . , which signals they choose to be visible , how they scale individual signal amplitudes , and which frequency ﬁlters they apply—can aﬀect individual grading decisions . Our workﬂow therefore allows for graders to adjust viewer settings throughout their pass and to revise grading decisions accordingly . As graders are free to update prior classiﬁcations throughout their pass , our workﬂow requires that graders explicitly mark a pass as complete , so grading decisions can be locked in for comparison with other panel members . Adjudication . Our pilot study made clear the fact that scheduling multiple domain 66 experts to synchronously adjudicate a disagreement at a set time is logistically prohibitive for a large - scale study with geographically remote participants . This insight informed our design consideration to choose an asynchronous , round - based approach for our adjudication workﬂow . In each round , the active grader reviews all disagreement cases among the data record , remotely and on their own time . Our system notiﬁes individual panel members via email when their adjudication pass becomes available . Upon login , graders are immediately positioned on the ﬁrst disagreement case in the data record , and can jump to the next or previous disagreement case as they proceed . During each adjudication pass , the active grader reviews each disagreement case at least once before the pass can be marked as complete . The approach to navigation and re - classiﬁcation is similar to the workﬂow previously described for independent classiﬁcation . In addition , graders are required to provide an explicit , structured rationale for each re - classiﬁcation decision , and must choose at least one domain - speciﬁc guideline in support of their classiﬁcation . For each guideline rule cited , graders are asked to indicate the extent to which they believe the given rule - speciﬁc evidence criteria to be met . Finally , graders are given the option to leave an open - ended comment about each decision to account for cases where guidelines are insuﬃcient to explain a comprehensive rationale . All rationales collected from previous adjudication rounds are presented to graders automatically when they navigate to a given disagreement case , encouraging adjudicators to review any prior case - speciﬁc discussion within the panel . A disagreement case is considered resolved when all graders in a panel converge on the same classiﬁcation . The adjudication process ends when all disagreement cases are resolved , or when a speciﬁed number of adjudication rounds have been completed . In our case study , we limit adjudication to three rounds , i . e . , one review pass per panel member . User Interface We designed a user interface ( UI ) to implement our workﬂows for classiﬁcation and adju - dication within a web - based platform ( Figure 3 . 11 ) . Components of the adjudication UI were integrated into the classiﬁcation UI to ensure contextual vicinity between case - speciﬁc expert discussions and signal data . We brieﬂy describe the classiﬁcation UI below , followed by a more detailed outline of the adjudication UI . Classiﬁcation . The primary purpose of the classiﬁcation UI is to enable experts to view and classify complex data eﬃciently without violating any existing domain - speciﬁc conventions . It is therefore designed to emulate existing viewer software for the domain - speciﬁc task at hand ( i . e . , sleep staging ) . The largest portion of the screen is devoted to data presentation , with controls streamlined for eﬃcient user input . In addition to 67 on - screen controls , there is hotkey functionality for navigation , classiﬁcation , and select viewer settings ( cf . Figure 3 . 11b , components 6 to 9 ) . Adjudication . The adjudication UI ( Figure 3 . 11 ) is designed for explicit and justiﬁed collaborative decision making . Its components are general and can be instantiated in the context of other data classiﬁcation tasks ( e . g . , for text documents or images ) . Our pilot study suggested that a critically important step for experts in understanding disagreements is a compact view of any conﬂicting classiﬁcation choices within the group . Therefore , the adjudication UI visualizes group decisions by displaying the number of votes assigned to each classiﬁcation category using circular indicators attached to classiﬁcation buttons . Disagreement cases are visually contrasted from agreement cases to guide graders’ attention using multiple red - colored vote indicators ( Figure 3 . 11b , component 6 ) . As disagreement cases can be scattered across a single contiguous data record , our adju - dication UI extends the base navigation panel with two additional buttons ( and hotkeys ) to jump directly to the subsequent and previous disagreement case ( Figure 3 . 11b , component 9 ) . To facilitate structured communication between members of an expert panel , the ad - judication UI includes a discussion component to render case - speciﬁc expert rationales . Early prototype testing suggested that some graders re - classiﬁed disagreement cases with - out reviewing prior discussions on the case . We therefore chose to automatically open the discussion component as soon as graders navigate to a disagreement case to encourage ac - tive review of prior arguments . Expert rationales and open - ended comments ( if any ) from all group members are displayed in chronological order ( Figure 3 . 11b ) . Each guideline rule cited within can be expanded using mouse - over to reveal information about pertinent evidence criteria . As our pilot study showed that inter - personal dynamics can distract from deliberation , we chose to use expert pseudonyms allowing group members to distin - guish their own rationale ( Figure 3 . 11b , component 4 ) from those of other experts in the group ( Figure 3 . 11b , component 5 ) while hiding any information about expert identity or background . For the purpose of providing justiﬁcations for re - classiﬁcation decisions , the adjudica - tion UI includes a rationale form ( Figure 3 . 11a ) . The rationale form is triggered when a grader submits a classiﬁcation choice , and classiﬁcation choices are saved only after the form has been completed and submitted . The form consists of three parts : a rule selector ( Figure 3 . 11a , component 1 ) enabling experts to search a catalogue of pre - deﬁned guideline rules and to cite those that best represent their rationale ; a component asking experts to specify the extent to which they believe each of the evidence criteria for the selected rule ( s ) are met ( Figure 3 . 11a , component 2 ) ; and the option to provide an additional open - ended 68 comment ( Figure 3 . 11a , component 3 ) . Graders are required to select at least one guideline rule in support of their classiﬁcation choice , but can choose to cite additional rules if appli - cable even if those happen to contradict their classiﬁcation . The design consideration here was to allow graders to discuss potential nuances or conﬂicts between multiple guidelines rules by citing several ones and clarifying their reasoning using open - ended comments . The rationale form is domain - agnostic and can be instantiated for a speciﬁc application domain by providing a rule - based representation of the pertinent classiﬁcation guidelines , in the format described above . 3 . 3 . 4 Research Questions and Hypotheses Our study addresses three research questions . Q1 : Why do experts disagree during independent classiﬁcation ? Diverse factors including training background and preferences in data presentation may cause experts to arrive at divergent classiﬁcation decisions , beyond characteristics inherent in the data itself . For example , experts with varying credentials or varying levels of work experience may be more likely to disagree . Likewise , our formative design process suggested that experts may disagree solely based on the use of diﬀerent viewer settings . Based on these intuitions , we hypothesize that : [ H1a ] Diﬀerences in expert background ( i . e . , credentials , geographic location , and work experience ) are associated with higher disagreement . [ H1b ] Diﬀerences in viewer settings ( i . e . , signal visibility , amplitude scaling , and frequency ﬁlters ) are associated with higher disagreement . [ H1c ] Certain data characteristics ( i . e . , abnormalities in a patient’s health condition , and case - speciﬁc signal complexity ) are associated with higher disagreement . Q2 : Why do certain disagreements persist after collective adjudication ? The same factors that contribute to independent disagreements may similarly con - tribute to the dynamics of adjudication among panels of experts , and may help explain why certain disagreements get resolved through exchange of arguments while others per - sist . Beyond this intuition , we take the stance that knowing about the speciﬁc criteria over which experts disagree will best explain why certain cases get resolved and others do not . We hypothesize that : 69 [ H2a ] Diﬀerences in expert background aﬀect the likelihood of resolving a case . [ H2b ] Diﬀerences in viewer settings aﬀect the likelihood of resolving a case . [ H2c ] Data characteristics aﬀect the likelihood of resolving a case . [ H2d ] The speciﬁc structure of a disagreement ( i . e . , discrepancies over the presence of individual features in the data ) carries greater explanatory power for understanding why certain disagreements persist after adjudication , compared to the other factors ( i . e . , diﬀerences in expert background or viewer settings , and data characteristics ) . Q3 : What impact does adjudication have on clinical decision making ? Collaborative decision making has been championed by national health research insti - tutions [ 12 ] , which assume that team - based approaches lead to signiﬁcant improvements in clinical decision making . Adopting the paradigm of collective intelligence in healthcare , we hypothesize that : [ H3a ] Experts perceive collective adjudication as useful for arriving at reliable and trustworthy classiﬁcation decisions . [ H3b ] Adjudication can lead to signiﬁcant revisions in treatment - relevant diagnostic markers . 3 . 3 . 5 Methods Here we describe the details of our observational case study including participant recruit - ment , data set , procedure and statistical analysis . Participant Recruitment We recruited 36 expert participants via domain - speciﬁc online platforms . Based on the pre - study questionnaire ( Appendix A . 2 . 1 ) , our expert participants were located in the United States ( 26 ) , Canada ( 7 ) , the European Union ( 2 ) , and other unspeciﬁed geographic locations ( 1 ) . The majority of participants ( 30 ) were Registered Polysomnographic Tech - nologists ( RPSGT ) ; six held lower credentials . More than half of our expert participants ( 23 ) reported having at least ﬁve years of experience working as sleep technologists . Out of our 36 participants , 31 self - reported as female and ﬁve as male . The distribution over 70 age groups was : 18 - 25 ( 1 ) , 26 - 35 ( 8 ) , 36 - 45 ( 16 ) , 46 - 55 ( 8 ) , 56 + ( 3 ) . Participants were paid US $ 112 . 50 for two scoring passes ( independent classiﬁcation and one review pass ) via online gift cards , or the equivalent amount in the currency of their speciﬁed location , corresponding to an hourly rate of US $ 37 . 50 with three hours of estimated total work on average . Data For the purpose of our study , we sampled just over 86 hours of sleep recording data from twelve diﬀerent patients with a mean recording duration of 7 . 19 hours ( SD = 43 mins ) , reﬂecting the standard length of a night at a sleep laboratory . Our dataset included patients with four diﬀerent health conditions ( three healthy patients , three with Parkinson’s disease , three with Alzheimer’s disease and three with sleep apnea ) . The distribution over patient age groups was : 40 - 44 ( 2 ) , 60 - 64 ( 1 ) , 65 - 69 ( 2 ) , 70 - 74 ( 3 ) , 75 - 79 ( 4 ) . Six patients were female and six were male . The complete dataset included 10 , 349 individual classiﬁcation cases each corresponding to one 30 - second window of biosignal data to be classiﬁed into one of ﬁve diﬀerent sleep stages . Almost half of all cases ( 4 , 543 ; 44 % ) resulted in some level of expert disagreement over the correct classiﬁcation label . Note that agreement rates here refer to exact agreement among three experts whereas rates reported in prior work refer to agreement among just two experts and are therefore expected to be higher . Out of all disagreement cases , about one third ( 1 , 667 ; 37 % ) remained unresolved after three rounds of collective adjudication . Procedure Before the study , experts ﬁrst completed a pre - study questionnaire ( Appendix A . 2 . 1 ) so - liciting demographic information , including age group , gender , geographic location , their highest credential , as well as the number of years of work experience in the sleep health profession . The 36 expert participants were randomly grouped into groups of three and each group was assigned to one of the twelve recordings for collective adjudication . Hence , each expert grader participated in exactly one panel and each recording was scored and ad - judicated by the same set of three experts . Experts ﬁrst performed an initial independent classiﬁcation pass on their assigned recordings , followed by three rounds of adjudication , one round per grader in the panel . The order in which experts performed the review passes was scheduled based on expert availability in each panel , i . e . , for each panel , the three experts were sequenced based on their earliest possible availability for completing a full review pass . Alternative sequencing options such as randomization may be desirable 71 based on the speciﬁc study setup , e . g . , if experts are part of multiple distinct adjudication panels . In our case study , where experts are part of exactly one panel and perform one review pass each , individual availability was taken into account as a social requirement to reduce delays between review passes . In each adjudication round , the active grader stepped through each individual disagreement case , re - scored the case , and provided a rationale for their ﬁnal classiﬁcation decision . In each adjudication round and for each disagreement case , the active grader was presented with the most recent classiﬁcations from all three panel members , as well as the grades and rationales submitted during each of the preceding rounds . Note that our observational case study treats independent classiﬁcation and adju - dication as consecutive workﬂow stages , rather than distinct experimental conditions . The study concluded with a post - study questionnaire ( Appendix A . 2 . 2 ) allowing participants to provide open - ended feedback about the beneﬁts and drawbacks of the adjudication inter - face and procedure . We also included two questions to assess the degree to which experts agreed that ‘The adjudication process was useful for generating a reliable hypnogram’ , and the degree to which experts agreed that ‘The ﬁnal adjudicated hypnogram can be trusted more than the hypnogram from my ﬁrst pass’ , both on 5 - point Likert scales . Analysis For Q1 and Q2 , we analyzed how various socio - technical factors like expert background , data characteristics , and viewer settings , were associated with expert disagreement during independent classiﬁcation ( Q1 ) , and with the likelihood of leaving a disagreement unre - solved after collective adjudication ( Q2 ) . We investigated both research questions using logistic regression models . For Q1 , the logistic model was run on all classiﬁcation cases ( N = 10 , 349 ) , the dependent outcome variable indicating whether a case had any level of disagreement ( N = 4 , 543 ) versus perfect agreement among all three experts . For Q2 , we ran a sub - analysis on just those cases with any initial disagreement ( N = 4 , 543 ) to understand why some disagreements persisted after three rounds of collective adjudication ( N = 1 , 667 ) , whereas other disagreements managed to get resolved . Both analyses shared a base set of independent variables , described in Table 3 . 11 . For Q2 speciﬁcally , we derived additional independent variables from the structured rationales experts submitted during adjudication . The complete set of all 36 guideline rules mentioned 15 unique basic features . We derived one independent variable for each one of these features , which assumed a true value if some , but not all panel members had mentioned the feature in their rationale for a given disagreement case , and false if either all or none had mentioned it . This approach allowed us to condense expert rationales from a complex set of guideline rules into a compact view of basic features to gauge the explanatory 72 Table 3 . 11 : Factors used as independent variables in Q1 and Q2 . Category Variable Description Grader Diﬀerences Experience true if panel members had diﬀerent levels of work experience , i . e . , if some had 5 + years of work experience , while others did not ; false if all panel members had the same level of work experience Location true if panel members were from diﬀerent geographic locations ; false if all were from the same location Credentials true if some , but not all panel members held an RPSGT credential ; false if either all or none were RPSGTs Viewer Diﬀerences Frequency Filter true if some , but not all panel members had activated the frequency ﬁlter while making a classiﬁcation decision ; false if either all or none had activated the frequency ﬁlter Amplitude Scaling true if some , but not all panel members adjusted the sensitivity of the signals for a given case ; false if either all or none had made adjustments to amplitude scaling Signal Visibility true if there were diﬀerences among panel members in how many sig - nals were visible when making a classiﬁcation decision ; false if all looked at the same set of signals for a given case Data Characteristics Patient Condition one of three disease conditions—Alzheimer’s , Parkinson’s , or sleep apnea—compared to the healthy baseline Signal Complexity true if the EEG for a given classiﬁcation case was more complex than the median case with regard to the frequency domain ; complexity was measured as spectral entropy , which is high if the signal contains multiple dominant frequencies , and low if it only contains one main frequency [ 13 ] Signal Transitions true if the EEG for a given classiﬁcation case was more complex than the median case with regard to the time - frequency domain ; measured as entropy over the dominant frequencies for each 2 - second segment within a 30 - second window power of feature - level expert rationales for understanding why certain disagreements persist after adjudication . For Q3 , we used paired t - tests to compare the value of aggregate diagnostic mark - ers before and after adjudication . A one - sample Wilcoxon signed rank test was used to understand if experts considered the adjudication process useful for making their classiﬁ - cation decisions more reliable and trustworthy as per the two questions in the post - study questionnaire . For qualitative data analysis , line - by - line inductive open coding was performed by one of the study authors to identify the emerging themes reported below . 73 3 . 3 . 6 Results Structured adjudication resulted in a 20 - 30 % increase in inter - rater agreement over the course of three rounds ( cf . Figure 3 . 12 ) . The machine - readable outputs of our system allowed for several insights to be had regarding the dynamics of our structured adjudication process . We observed vast diﬀerences in the role that diﬀerent features ( i . e . , distinct evidence criteria mentioned in the classiﬁcation guidelines ) played for adjudication . Not only were certain features mentioned orders of magnitude more often than others ( cf . Figure 3 . 13 ) ; diﬀerent features also contributed to the resolvability of disagreements in diverse ways . Here we present the results of our data analysis with respect to each of our research questions . Q1 : Why do experts disagree ? In determining the causes of disagreement during independent classiﬁcation , various fac - tors were analyzed across diﬀerent groups of variables , including diﬀerences in grader back - ground and viewer settings , as well as characteristics inherent in the data itself ( Table 3 . 12 , left side ) : • For grader background , diﬀerences in work experience , as well as geographic lo - cation , were signiﬁcant determinants in predicting disagreement before adjudication . Diﬀerences in grader credentials were not found to be signiﬁcant in predicting initial disagreement among a panel—results providing partial support for our hypothesis H1a . • Diﬀerences in viewer settings used by graders during independent classiﬁcation— frequency ﬁlters , amplitude scaling , and signal visibility—all were signiﬁcant factors for initial agreement rates . However , while diﬀerences in frequency ﬁlter settings and amplitude scaling were associated with disagreement , diﬀerences in signal visibility ( i . e . , diﬀerences in whether graders were viewing all or only a subset of the available signals ) , was found to be a signiﬁcant predictor of initial agreement among a panel . Our results partially conﬁrm hypothesis H1b . • With respect to data characteristics , and in line with our hypothesis H1c , the overall signal complexity for a given case contributed to initial disagreement . Dis - agreement was signiﬁcantly higher for patients with Parkinson’s and Alzheimer’s disease , compared to a baseline of healthy patients . The same insight is also re - ﬂected in our observation that these two health conditions exhibited the lowest levels of inter - rater agreement before adjudication ( Figure 3 . 12 ) . 74 l l l l l l l l l l l l l l l l 50 60 70 80 90 0 1 2 3 Round Number A g r ee m en t [ % ] Normal Parkinson ' s Alzheimer ' s Sleep Apnea Figure 3 . 12 : Agreement rate by adjudi - cation round number and patient’s health condition . 25 200 1000 4000 A l pha R h y t h m K C o m p l e x L A M F S l eep S p i nd l e S l o w W a v e A c t i v i t y R ap i d E y e M o v e m en t s Lo w C h i n E M G T one A r ou s a l E E G 4 − 7 H z + S l o w i ng S l o w E y e M o v e m en t s V e r t e x W a v e E y e B li n ks M a j o r B od y M o v e m en t H i gh C h i n E M G T one R ead i ng E y e M o v e m en t s Feature Type # M en t i on s i n R a t i ona l e s Figure 3 . 13 : Number of times each fea - ture type was mentioned in a rationale ( log scale ) . Q2 : Why do disagreements persist ? In analyzing which factors were associated with persistent disagreement—i . e . , cases with initial disagreement that remained unresolved after adjudication vs . those that were re - solved through adjudication—similar patterns were observed across variable groups ( Table 3 . 12 , right side ) . Many of the same factors associated with initial disagreement were also signiﬁcant explanatory variables for the outcome of persistent disagreement after adjudi - cation , oﬀering partial support for hypotheses H2a , H2b , and H2c . There were notable shifts , however , in the way that certain variables were associated with resolving a case compared to how they contributed to initial disagreement . We focus on those variables with diﬀerential eﬀects between Q1 and Q2 . Variance in grader credentials , while not found to cause disagreements in Q1 , was as - sociated with an increased likelihood of resolving disagreement cases ( Q2 ) . Similarly , with respect to data characteristics , sleep apnea patients did not give rise to more disagreement than healthy patients did during independent classiﬁcation , but disagreement cases could be resolved more readily for sleep apnea patients than for the healthy baseline . On the other hand , Alzheimer’s disease did not signiﬁcantly contribute to the persistence of dis - agreement , despite the fact that it contributed to initial disagreement . Where the EEG signal itself was concerned , overall signal complexity correlated with greater resolvability , whereas signal complexity in terms of transitions over time was associated with higher 75 Table 3 . 12 : Logistic models for understanding why experts disagree during independent classiﬁcation ( Q1 ) , and why certain disagreements persist after adjudication ( Q2 ) . Q1 : Why Disagree ? Q2 : Why Unresolved ? Independent Variable ˆ β SE t p ˆ β SE t p Grader Diﬀerences Experience 0 . 69 0 . 06 12 . 30 * * * 0 . 58 0 . 13 4 . 45 * * * Location 0 . 36 0 . 07 5 . 06 * * * 0 . 50 0 . 20 2 . 57 * Credentials - 0 . 06 0 . 07 - 0 . 79 - 0 . 93 0 . 16 - 5 . 91 * * * Viewer Diﬀerences Frequency Filter 0 . 51 0 . 07 7 . 72 * * * 0 . 83 0 . 14 5 . 76 * * * Amplitude Scaling 0 . 19 0 . 05 3 . 91 * * * 0 . 04 0 . 11 0 . 32 Signal Visibility - 0 . 25 0 . 07 - 3 . 39 * * * - 0 . 44 0 . 17 - 2 . 62 * * Data Characteristics Patient Condition Parkinson’s 0 . 73 0 . 07 10 . 98 * * * 0 . 91 0 . 16 5 . 61 * * * Alzheimer’s 0 . 27 0 . 08 3 . 30 * * * - 0 . 18 0 . 19 - 0 . 97 Sleep Apnea 0 . 14 0 . 09 1 . 55 - 0 . 59 0 . 23 - 2 . 58 * * Signal Complexity 0 . 22 0 . 04 5 . 05 * * * - 0 . 57 0 . 11 - 4 . 98 * * * Transitions - 0 . 07 0 . 04 - 1 . 64 0 . 54 0 . 10 5 . 19 * * * Feature Disagreements ( Q2 ) Slow Wave Activity 5 . 12 0 . 21 24 . 31 * * * LAMF 2 . 16 0 . 11 19 . 38 * * * Arousal 1 . 88 0 . 19 9 . 65 * * * Alpha Rhythm 1 . 67 0 . 12 13 . 64 * * * Eye Blinks 1 . 51 0 . 39 3 . 85 * * * K Complex 1 . 33 0 . 12 10 . 85 * * * Sleep Spindle 1 . 25 0 . 13 9 . 77 * * * Reading Eye Movements 0 . 83 0 . 54 1 . 55 Low Chin EMG Tone 0 . 71 1 . 02 0 . 69 Vertex Wave 0 . 61 0 . 35 1 . 76 High Chin EMG Tone 0 . 57 1 . 07 0 . 53 Rapid Eye Movements 0 . 48 1 . 02 0 . 48 EEG 4 - 7 Hz + Slowing 0 . 05 0 . 25 0 . 20 Slow Eye Movements - 0 . 82 0 . 36 - 2 . 32 * Major Body Movement - 2 . 88 0 . 59 - 4 . 88 * * * 76 chances of leaving a case unresolved . Diﬀerences in amplitude scaling , amenable to viewer settings , were not signiﬁcant for resolving disagreements , despite causing disagreement during independent classiﬁcation . In addition to this base set of variables , Table 3 . 12 provides a list of 15 EEG features mentioned in at least one of the structured expert rationales from our study . For seven of these , we found that disagreements over feature presence were signiﬁcantly associated with leaving cases unresolved . We found the opposite to be true for two other features— slow eye movement and major body movement—where discrepancies over feature presence were correlated with consensus formation . Most importantly , however , across all variable groups , it were the feature - level variables that showed the greatest eﬀect sizes for case resolvability overall . This ﬁnding conﬁrms our hypothesis H2d , the claim that the structure of a disagreement , with respect to feature - level rationales , holds the greatest explanatory power for why disagreements remain unresolved even after adjudication . Q3 : What impact does adjudication have on clinical decision making ? We assessed this question through both qualitative and quantitative measures . Through a post - study survey , expert graders responded that the structured adjudication process was both useful for generating a reliable hypnogram ( p < 0 . 001 ) , and that the ﬁnal adjudicated hypnogram could be trusted more than the original one ( p < 0 . 001 ) . These ﬁndings support our hypothesis H3a . Changes in sleep parameters from before to after adjudication were analyzed as objective measurements for the impact of adjudication ( Figure 3 . 14 ) . We observed a signiﬁcant decrease ( p < 0 . 05 ) in the percentage of sleep time classiﬁed as REM sleep ( % REM ) —a treatment - relevant diagnostic marker—with shifts ranging between - 7 . 5 % and 1 . 4 % . This ﬁnding oﬀers support for our hypothesis H3b . Qualitative Feedback . All participants were given the opportunity to provide open - ended feedback regarding both the interface of our adjudication system , as well as the adjudication procedure deployed in our study . Where the design of our platform was concerned , graders commended the clarity of its structured , guideline - centric format , with quick access to a comprehensive list of classiﬁcation guidelines , and a view through which to appreciate other graders’ classiﬁcations and rationales . Grader feedback was unanimously in favour of the collaborative practice of adjudication , especially in a structured format that allowed for the exchange of individual justiﬁcations for one’s classiﬁcation decisions . As group discussion was achieved remotely , our graders felt that having a clear - cut time window for each individual grader to complete their pass on the record was beneﬁcial for promoting eﬃciency of adjudication . However , the sequential 77 l −10 −5 0 5 % N1 % N3 % REM Diagnostic Marker C hange a ft e r A d j ud i c a t i on Figure 3 . 14 : Change in diagnostic markers from before to after adjudication . nature of our procedure , where the ﬁrst grader must complete their pass before the second grader in the panel can begin theirs , was considered an obstacle by some participants . At the same time , our graders recognized that real - time adjudication may be challenging given the logistical burden of scheduling synchronous sessions among experts , even if facilitated through a remote , web - based system . Our procedure , requiring graders to construct structured arguments in support of their decisions in terms of rules from the classiﬁcation guideline , was said to have the potential to improve individual graders’ scoring abilities . To borrow a quote from one expert , a guideline - centric adjudication procedure can help both those who have been scoring for years and are thus “stuck in their ways” , as well those with minimal scoring experience , who may need a concrete guide . Where the eﬀects of adjudication on consensus are concerned , most participants perceived the exchange of arguments and justiﬁcations among the group as a balanced approach for reviewing cases collectively : “Sometimes I still disagreed . Other times I changed my perspective . ” One expert reinforced our stance that “truly subjective cases” ought to be recognized as such , rather than enforcing artiﬁcial consensus . 3 . 3 . 7 Discussion Our core contribution in this section is an observational study of expert disagreement in the domain of medical data analysis . With prior work establishing that group deliberation can be a useful and eﬀective method for resolving disagreement , we built a structured , guideline - centric adjudication system and workﬂow to facilitate our study . 78 In addition to oﬀering further support for adjudication as a method of supporting consensus formation , our ﬁndings help elucidate the reasons why experts disagree about medical data classiﬁcation decisions , why some of those disagreements persist , and how adjudication outcomes may translate to clinical outcomes . We discuss the generalizability of our ﬁndings and their potential applications , oﬀer design considerations for expert ad - judication workﬂows , and conclude by addressing limitations of our study and directions for future work . Generalizability and Applications Our case study was limited to the speciﬁc task of interpreting and classifying biomedical time series , so caution is warranted in generalizing the results to outside domains and to task types other than data classiﬁcation ( e . g . , policy design , content generation ) . However , sleep stage classiﬁcation is a good exemplar for the medical domain , as it shares several characteristics with other diagnostic tasks : ( 1 ) expert disagreement is prevalent within ; ( 2 ) data classiﬁcation guidelines are lengthy and complex ; ( 3 ) data analysis includes a wide range of signal modalities ( i . e . , EEG , ECG , eye movements , muscle activation , etc . ) , many of which are integral parts also of other diagnostic procedures in medicine ; and ( 4 ) various low - level features of the data ( e . g . , alpha rhythm , sleep spindles , K - complexes ) provide the basis for higher level assessments ( i . e . , sleep stage classiﬁcations , diagnosis of sleep disorders ) . We characterize the types of medical data analysis tasks to which our ﬁndings may generalize below . Grader Diﬀerences . Findings on the eﬀect of expert background on disagreement dynamics may generalize better to tasks where procedures for expert certiﬁcation vary between countries or where such procedures may undergo signiﬁcant changes over time . In those cases , diﬀerences in graders’ geographic location or professional experience may play a more signiﬁcant role in contributing to disagreement than for tasks where certiﬁcation procedures are are globally standardized and remain relatively stable over time . Viewer Settings . Findings on the eﬀect of viewer settings on disagreement dynamics may generalize better to tasks where complex patient data can be viewed from diﬀerent perspectives . Perspective adjustment can take various forms , including adjustment of the amount of data viewed ( e . g . , montage selection in multimodal time series , or region - of - interest adjustment in interpretation of pathology slides ) , or application of certain ﬁlter settings ( e . g . , frequency ﬁlters in time series or audio data , or color ﬁlters in image data ) . Such ﬁndings would not directly apply to classiﬁcation tasks with static data views ( e . g . , text - based patient records ) . 79 Data Characteristics . We included disease condition and signal complexity as vari - ables to understand the eﬀect of data characteristics on expert disagreement . The speciﬁc operationalization of these variables may need to be adjusted for other task domains . The idea , however , that certain pathologies or pattern complexity may complicate data inter - pretation is domain - agnostic and may generalize to other task types . Guidelines . The proposed guideline - centric adjudication process is general , and appli - cable to task types where pre - existing guidelines in the expert community can be mapped to a set of classiﬁcation rules . Evidence - based grading guidelines are widely available across multiple medical subspecialties [ 11 ] , and the organization of guidelines into easily identi - ﬁable grading recommendations is encouraged within the medical community [ 59 ] . There are , however , some diagnostic tasks , such as diagnosis of epilepsy or glaucoma , for which comprehensive guidelines yet have to be developed . The approach may generalize better to tasks with only a few classiﬁcation categories ( e . g . , sleep staging , diabetic retinopathy grading , prostate cancer grading ) than to classiﬁcation tasks with very large decision spaces ( e . g . , comprehensive diﬀerential diagnosis ) or multiple classiﬁcations with respect to the same patient record . In our study , we hypothesized ( H2d ) that disagreements over the presence of speciﬁc features in the data would oﬀer the strongest explanatory power for the resolvability of disagreements . Indeed , our results conﬁrm that such feature - level rationales contribute the strongest to explaining why certain disagreements persist after adjudication . This ﬁnding suggests that expert disagreement , while inﬂuenced by social factors and speciﬁcs of data presentation , can be best explained by leveraging feature - level justiﬁcations from experts in medical data analysis . Since feature - level justiﬁcations were directly derived from guideline rules cited during collaborative adjudication , disagreements on the feature level can be considered a quantitative lens on low - level ambiguities within the guidelines . Another ﬁnding from our study was that structured adjudication can lead to signiﬁcant revisions in clinical parameters relevant to real - world treatment outcomes ( H3a ) . In our sleep stage classiﬁcation task , adjudication caused a signiﬁcant decrease in % REM , com - pared to an independently annotated record . Clinical decision making in many scenarios hinges on the proportion of time spent in REM sleep recorded on an EEG . For instance , REM sleep is decreased in several neurodegenerative disorders , including Parkinson’s dis - ease and Alzheimer’s disease , and among older adults without Alzheimer’s , decreased REM sleep is associated with a higher likelihood of developing the disease in the future [ 105 ] . The detection of REM sleep behaviour disorder , which is associated with a high risk of fu - ture Parkinson’s disease , is critically dependent on the accurate classiﬁcation of REM sleep [ 110 ] . REM sleep is also decreased in sleep apnea , and the restoration of normal amounts of REM sleep can be a marker of therapeutic eﬃcacy in sleep apnea treatment . These 80 insights position the adjudication process as something more than an academic exercise in consensus formation , but an approach with the potential of altering clinical outcomes as a direct result of changes in diagnostic markers . Applications . There are several potential applications of our structured adjudication system and procedure . First , a system like our own can be easily implemented in the training of novice readers . In our study , diﬀerences in grader experience predicted both discrepancies before adjudication , and persistent disagreement afterwards . Beyond the ob - vious explanations for this , it is worth repeating that our expert participants highlighted that adjudication may be helpful both for novice graders , and more experienced graders . Our guideline - centric platform allowed for graders to go entirely by - the - book in their ap - proach to classiﬁcation , but the more seasoned scorers may well have stuck to their tried and true reasons for their classiﬁcation decisions , perhaps overlooking certain nuances in the data that those following the rules would have better attended to , leading to disagree - ment cases . Thus , our system may have equal potential for helping more experienced readers reconsider their grading habits . While structured adjudication was made possible in our study by the fact that standard - ized , agreed - upon classiﬁcation guidelines are pre - existent within the expert community , our rationale form retained the option of providing open - ended comments . For domains where standardized classiﬁcation guidelines do not yet exist ( e . g . , epilepsy diagnosis ) , our hybrid approach could oﬀer the potential of mining open - ended arguments to extract ex - plicit inference rules and thus iteratively generate a more structured representation of classiﬁcation guidelines . The interoperable output of our structured adjudication system may also lend itself naturally as input to other decision support systems . For example , machine learning models could be trained using structured , ambiguity - aware data sets to not only classify by diagnostic category ( e . g . , normal vs . abnormal ) , but also to identify ambiguous cases and to explain those cases in terms of potentially controversial classiﬁcation guidelines or evidence criteria pertinent to the data at hand [ 32 ] . Design Considerations for Expert Adjudication Davies and Chandler [ 37 ] delineate ﬁve design categories of an online deliberation system : purpose , population , spatiotemporal distance , communication medium , and deliberative process ( e . g . , identiﬁability and structure ) . In this section , we designed and implemented an adjudication interface for expert users to engage in remote , anonymous , asychronous 81 adjudication of medical time series data in a web - based environment through a structured , guideline - centric procedure . Purpose and Population . To facilitate adjudication of medical time series data in the context of sleep stage classiﬁcation , we designed a system and user interface to emulate existing sleep scoring software , and embedded functionality for adjudication within . This ensured that users could engage in eﬀective group deliberation in a familiar environment . True to the nature of our application domain , our system was aimed at expert users . By engaging a population of expert users , we discovered that viewer settings play an important role in causing and resolving disagreement . For example , diﬀerences in signal visibility increased the likelihood of resolving disagreements through adjudication . While the reason for this is debatable , we suggest that experts’ preferences and information needs in the context of making clinical decisions may vary with their level and type of professional experience . Designers of expert adjudication systems should therefore take into account the fact that both expert background and preferences for interface settings aﬀect how assessments are made and how divergent assessments are adjudicated . However , diﬀerences in certain viewer settings ( e . g . , gain adjustments ) were also associated with initial and persistent disagreement . While providing experts with suﬃcient amount of ﬂexibility for viewer conﬁgurations seems necessary in order to enable exploration of complex medical data , adjudication systems may beneﬁt from ways to share viewer settings between experts . In particular , if diﬀerences in viewer settings spur disagreement and make the resolution of discrepancies less likely , a feature allowing experts to view data “through the lens” of another grader and temporarily adopt the other experts’ viewer settings may prove helpful for more eﬀective adjudication . Spatiotemporal Distance and Medium . In order to conduct a large - scale study with numerous expert users , we chose to deploy our system within a web - based environ - ment , and had users participate remotely and asynchronously . While these decisions were largely informed by logistical reasons , we also wanted to design a system to enable ef - fective adjudication in real - world contexts were local , real - time deliberation is infeasible . That said , we acknowledge that synchronous systems may be more eﬀective at fostering agreement between users [ 37 ] . Deliberative Process . We enacted an anonymous deliberation process to eliminate user identiﬁability and reduce inter - personal bias during adjudication . Our ﬁndings on how grader diﬀerences aﬀected disagreement dynamics should therefore be interpreted in the context of how diﬀerent expert backgrounds may translate into diﬀerent approaches to reasoning and arguing about corner cases , rather than bias introduced by mere perception of authority . 82 Adding structure to the process of collecting expert rationale allowed for detailed quan - titative analyses of adjudication dynamics in our observational study . It is well documented that more structure fosters more deliberative behavior in an online deliberation setting [ 37 ] . However , in structuring a system around domain - speciﬁc annotation guidelines , structure can limit the eﬃciency of the workfklow when said guidelines are numerous and complex . Our design may have reduced input eﬃciency by forcing graders to navigate through a com - prehensive set of rules irrespective of their classiﬁcation decisions . However , unlike more conﬁrmatory UI designs , we argue that this structure encourages participants to consider alternative lines of reasoning during adjudication . We showed how complex classiﬁcation guidelines can be integrated into adjudication processes in a ﬂexible and interoperable fashion . At the same time , our analysis leveraged a more compact view of expert ra - tionale referencing basic feature types mentioned within the guideline rules . One design consideration by way of promoting input eﬃciency for structured rationales is to use the presence or absence of distinct , low - level features as an entry point for collecting expert rationales . A hybrid approach may solicit compact , feature - level assessments ﬁrst , in order to intelligently recommend pertinent classiﬁcation rules for adjudication in a second step . Sharing and leveraging the insights of other users in a collaborative workﬂow has been found to increase task performance [ 60 ] . However , in the same study , Goyal and Fussell found that users who collaborated through an interface designed for shared sense making do not report an increased sense of success during the task , and view such an interface as having lower utility than standard setups . These reports suggest that users may need to be informed in real time about the utility of deliberation systems that employ new but important design elements—and may involve extra steps in the workﬂow—if such systems are to be readily adopted by new users . Limitations Despite the demonstrated use cases of adjudication , there are limitations to the process . First and foremost , adjudication is resource - intensive , a factor potentially hindering adop - tion in real - world contexts . Our study demonstrates how elements of structure can beneﬁt adjudication procedures , and future work may explore how added structure could translate to increased eﬃciency and reductions in cost . While a quantitative cost - beneﬁt analysis is beyond the scope of our study and will be left for future work , we demonstrate ways to counter the challenges of scheduling synchronous expert meetings through a round - based approach where experts can review cases on their own time . Future work may investigate hybrid methods encouraging turn - based adjudication procedures , while providing the op - portunity for real - time communication for times when experts happen to review the same 83 case concurrently to make eﬃcient use of their resources . Our pilot study involved the same three experts conducting adjudication both in per - son and via video conference . While we ensured that experts discussed diﬀerent cases in both stages , it is possible that certain behaviors observed via video conference may have been inﬂuenced by previous face - to - face interactions ( e . g . , perceived level of experience , word choice , intonation patterns ) . Our design considerations concerning inter - personal dynamics ( e . g . , choice of text as a communication medium ) were primarily informed by in - person adjudication and subsequently reinforced by the possibility that these may also play a role in adjudication via video conference . Future work may explore the diﬀerential eﬀects of communication media in medical adjudication using controlled between - subjects experiments . Another aspect of the adjudication practice left for future work is the question of when to deploy such a system in a real - world context , clinical or otherwise . In settings where a single expert reader is the norm , what are the costs of introducing collective adjudication , given its demonstrated advantages in medical data analysis ? If adjudication is deemed too costly to be routine , what are the indications that may alert clinicians to when group deliberation is necessary ? Our ﬁndings demonstrate that adjudication outcomes can translate to changes in diagnostic measures , suggesting that the use of adjudication should be prioritized for those critical disagreement cases that have the highest potential of impacting patients through revisions in treatment outcomes . 3 . 3 . 8 Conclusion In this section , we introduced a novel perspective on the problem of expert disagreement in medical data analysis using a structured form of collaborative adjudication to study the nature and dynamics of disagreement from a socio - technical perspective . We demon - strated the applicability of our approach in the context of medical time series analysis for sleep stage classiﬁcation , and showcased how the structured data produced can facili - tate a deep understanding of the diverse factors playing a role in generating and resolving disagreements , including expert background , data complexity , viewer settings and classiﬁ - cation guidelines . Our proposed workﬂow for structured adjudication has implications for the design of decision support for clinical group decision making and for the collection of expert - labeled data in the context of other applications like computer - aided diagnosis . 84 3 . 4 Conclusion In this chapter , we have explored the question of how group deliberation can be used as a tool within data labeling workﬂows to help analyze instances of ambiguity resulting in inter - rater disagreement . We reported three case studies of using both synchronous and asynchronous deliberation workﬂows not only in novice crowd work , but also in the expert domain of medical data interpretation . Our studies have demonstrated that group deliberation is a versatile approach for addressing ambiguity in various data modalities , including text documents , images and time series data , and that imposing structure on the process does not only improve eﬃciency , but also helps us understand why disagreement arises and when it may be resolved . The remainder of this dissertation aims to demonstrate that the deliberation data collected in this process can be put into use to not only train less experienced human labelers , but also to simulate and explore AI systems that highlight and explain ambiguity in human - AI collaborative data analysis workﬂows . 85 Chapter 4 Deliberation Data for Labeler Training Workﬂows for medical data labeling critically depend on accurate assessments from human experts . Yet human assessments can vary markedly , even among medical experts . Prior re - search has demonstrated beneﬁts of labeler training on performance . Here we utilized two types of labeler training feedback : highlighting incorrect labels for diﬃcult cases ( “individ - ual performance” feedback ) , and expert discussions from adjudication of these cases . We presented ten generalist eye care professionals with either individual performance alone , or individual performance and expert discussions from specialists . Compared to performance feedback alone , seeing expert discussions signiﬁcantly improved generalists’ understanding of the rationale behind the correct diagnosis while motivating changes in their own labeling approach ; and also signiﬁcantly improved average accuracy on one of four pathologies in a held - out test set . This work suggests that image adjudication may provide beneﬁts beyond developing trusted consensus labels , and that exposure to specialist discussions can be an eﬀective training intervention for medical diagnosis . 4 . 1 Motivation In recent years , major advances in machine learning ( ML ) have enabled a new era of decision support tools ( DST ) for critical medical diagnostic tasks . With the increased capabilities of deep learning models , DSTs are being developed to support much more complex diagnostic processes with critical inﬂuence on patient outcomes . As these technologies mature , they hold the potential to increase access to healthcare—a demonstrated need for large sections of the developing world [ 16 ] . However , medical specialists suﬃciently trained to perform complex diagnoses are exceptionally rare [ 16 ] . 86 Alongside this growth in deep learning has been a parallel , increased need for large - scale , labeled medical data to power the training of such models . Because medical data is often highly regulated , and patient outcome is typically not available in the original data source , lack of access to ground truth - labeled data has become a key barrier to the development and evaluation of machine learning systems in medical domains [ 28 ] . As a result , contemporary ML - powered algorithms typically rely on medical practitioners to manually label data ground truth [ 28 , 63 , 138 ] . The collection of manually - labeled ground truth data from clinicians raises fundamental human challenges . Because many high - stakes medical decisions are highly nuanced and can be subject to personal opinion , even clinician assessments vary markedly . To combat this problem , current labeling approaches enlist a small set of specialized world experts to “adjudicate” the decision via consensus , as a way of producing a more reliable gold standard [ 82 ] . However , specialists of this caliber are exceptionally rare and expensive . To make the process more scalable , medical generalists with less training may be recruited to perform labeling at larger scale [ 140 ] . This is an enticing approach given that medical generalists far outnumber specialists ( e . g . optometrists outnumber ophthalmologists 4 . 9 - fold [ 5 ] ) . Yet , generalists are less experienced : diﬃcult patient cases lead to high inter - labeler variability and incorrect diagnoses [ 82 ] , limiting algorithmic validity and introducing the risk of adverse outcomes for patients’ lives . In this chapter , we address these challenges by introducing and studying adjudica - tion feedback for training medical image labelers . Our approach has the potential to help decrease the dependency on specialists by expanding the set of trained labelers to less - specialized workers , like generalists . The central underlying idea is to reuse existing metadata from medical specialists’ adjudication of diﬃcult cases to improve medical gen - eralists’ comprehension and labeling accuracy . Speciﬁcally , we study whether discussion dialogs , generated as a side product in the costly process of adjudication , can be repur - posed as training material in medical data labeling workﬂows . We draw inspiration from prior research in crowdsourcing , which has demonstrated beneﬁts of labeler training on the performance of non - experts on the web . Our research applies labeler training to the high - stakes , challenging domain of medicine , advancing our understanding of how to provide feedback to this emerging population of medical labelers . In our controlled experiment , we examined the impact of two diﬀerent forms of labeler training feedback : individual performance on diﬃcult cases , and specialist discussions from adjudication of these cases . We presented ten certiﬁed eye care professionals with either individual performance alone , or individual performance and specialist discussions . Our results suggest that reading specialist discussions has beneﬁts for generalists’ comprehen - sion of diﬃcult cases , on their motivation to alter their own labeling approach , and on 87 their diagnostic accuracy on a held - out test set . Our main contributions are : 1 . We conducted an empirical study to understand the beneﬁt of presenting adjudication discussions of diﬃcult cases as a form of training feedback in medical data labeling . 2 . We present results suggesting that showing adjudication discussions can improve comprehension of the rationale behind the correct diagnosis while motivating changes with respect to medical generalists’ labeling approach . 3 . We demonstrate that these beneﬁts observed during training also translated into improved diagnostic accuracy in a held - out test set . Taken together , this research advances our understanding of the emerging ﬁeld of med - ical labeling , and provides new implications for how to scale medical data collection on high - stakes tasks with diﬃcult - to - obtain ground truth . 4 . 2 Application Domain Every year , eye disease causes vision impairments or blindness for millions of people world - wide . In particular , retinal pathologies such as diabetic retinopathy ( DR ) rank among the leading causes of vision loss in many industrialized countries [ 141 ] . To combat the issue , several national governments have established population - wide screening programs for early disease detection . One of the central diagnostic artifacts in the assessment of retinal disease is fundus photography , i . e . , photographs taken of the background of a patient’s eye ( Figure 4 . 1 ) . Digital fundus photos are used both in tele - medical screening [ 134 ] and for the development of deep learning models for AI - assisted retinal assessment [ 63 , 108 ] . Regardless of the setting , expertise from certiﬁed medical professionals is required to determine the presence and severity of disease as it appears in the image . While the diagnostic criteria for retinal assessment are governed by oﬃcial medical guidelines , image interpretation by medical experts remains a subjective process [ 82 ] . The resulting inter - rater disagreement may not only arise over the presence of disease , but also over the speciﬁc classiﬁcation of an observed pathology . In particular , the appearance of diabetic retinopathy may resemble other forms of retinal disease such as hypertensive retinopathy ( HTNR ) , retinal vein occlusion ( RVO ) and retinal artery occlusion ( RAO ) . It is crucial that treatment decisions are formed based on correct diﬀerential diagnoses to avoid adverse outcomes for patients . 88 Eye care professionals with varying levels of specialization are concerned with the as - sessment of retinal disease [ 5 ] : ( 1 ) optometrists present the largest group of professionals trained for retinal assessment ; as generalists , they typically refer diﬃcult - to - assess cases to other experts , such as ( 2 ) general ophthalmologists , i . e . , medical doctors who completed a multi - year residency program in general eye and vision care ; at the highest level of special - ization , there is a small population of ( 3 ) retina specialists worldwide—ophthalmologists who completed a two - year fellowship program in retinal assessment after completing their eye care residency . Our application domain is representative of other medical subspecialties . Not only does it require the subjective process of image interpretation by human experts ; it also involves diﬀerent types of easy - to - confuse pathologies ( DR , HTNR , RVO , RAO ) , that require a deep understanding of symptomatic diﬀerences to be reliably diﬀerentiated . 4 . 3 Research Questions & Hypotheses The case discussions used in our training study are the by - product of an adjudication pro - cess designed to analyze and resolve diagnostic disagreements among highly trained medical specialists . As such , the discussion dialogs are expected to reﬂect types of vocabulary and reasoning grounded in a deep understanding of a certain medical subspecialty . Yet , the case discussions were not collected with an educational purpose in mind . As a result , they may exhibit weaknesses when used for labeler training . For example , the fact that the dialogs are rooted in disagreements and the potential use of specialist jargon may cause confusion among less specialized medical professionals . Our study addresses two primary research questions about how medical generalists perceive ( Q1 ) and act upon ( Q2 ) the presentation of case - speciﬁc adjudication discussions from specialists as a form of medical diagnosis training . Q1 : How do medical generalists perceive reading of specialist discussions as a form of labeler training feedback ? Medical assessments can be contentious and it is possible for one expert to take the perspective of another expert without necessarily agreeing with their ﬁnal conclusion . Fur - thermore , even if an expert understands and agrees with the diagnostic reasoning for one speciﬁc case , it is not guaranteed that this will also motivate a change in their own labeling approach for other cases . In this study , we examine these three aspects—comprehension , agreement , adaptation— separately , and hypothesize that reading of specialist discussions as a form of training 89 feedback for medical generalists will : [ H1a ] Improve comprehension of the rationale behind the correct diagnosis . [ H1b ] Increase agreement with the answer key . [ H1c ] Motivate adaptations in generalists’ labeling approach . Q2 : How does reading of specialist discussions aﬀect generalists’ diagnostic reasoning for future patient cases ? Beyond studying generalists’ perception of our training inverventions , it is crucial to investigate its eﬀect on future medical assessments . We project that the presentation of case - speciﬁc adjudication discussions during labeler training will have beneﬁts for general - ists’ diagnostic reasoning in a held - out test set . In particular , we hypothesize that reading of adjudication discussions during training will : [ H2a ] Improve diagnostic accuracy . [ H2b ] Increase case - speciﬁc diagnostic conﬁdence . [ H2c ] Lower perceived case diﬃculty . [ H2d ] Improve overall diagnostic self - eﬃcacy . 4 . 4 Methods 4 . 4 . 1 Experts Our study involved two distinct groups of experts with varying levels of specialization who contributed during diﬀerent stages of our data collection and experimental procedure . Specialist Adjudicators . Three retina specialists collectively generated the answer key and adjudication discussions for the medical images used in this study . The adjudica - tion process implemented the remote , round - based protocol for group discussion described in Section 3 . 2 . First , each specialist adjudicator labeled each fundus image independently . 90 Images with any level of disagreement were then reviewed in a round - robin fashion , by one specialist at a time . In each review round , the active specialist adjudicator was encouraged to explain the rationale behind their diagnostic reasoning within a text - based discussion thread , and to revise their diagnosis labels if they felt an adjustment was indicated based on insights from the adjudication discussion . The adjudication process ended for a given image when all members of the adjudication committee reached a unanimous consensus on all diagnosis labels for that image ( or after a maximum of 15 review rounds , i . e . , up to ﬁve reviews per adjudicator ) . Note that this adjudication procedure was not designed with the purpose of training medical generalists in mind , but to create trusted ground truth labels for the validation of machine learning models . This study explores whether the discussion metadata generated as a side product in the process can be recycled as an eﬀective tool for training medical generalists . Generalists . Ten certiﬁed eye care professionals with varying training backgrounds participated as generalist labelers in the training experiment of our study . These in - cluded people at a lower level of specialization and those with substantially fewer years of retina - speciﬁc training compared to members of the specialist adjudication committee . We assigned each of the ten generalist labelers to one of the two types of training feedback , ensuring that both groups were relatively balanced with respect to training background and professional experience . There was no overlap between the two groups of specialist adjudicators and generalist labelers in our study . 4 . 4 . 2 Image Sets Our study used two distinct image sets : a train set used to elicit each generalist’s baseline labeling performance before receiving training feedback , and a held - out test set used to measure their labeling performance after training . Our training feedback focused on those image cases in the train set where labels from generalists diﬀered from the answer key . Both image sets consisted of 36 images each . Images were selected from a larger set of 499 cases labeled by our committee of three retina specialists using the adjudication procedure outlined above . Specialists indepen - dently agreed on 329 out of the 499 cases , leaving 170 disagreement cases for the round - based review and discussion process . We performed a qualitative content analysis on these 170 disagreement cases based on the dialogs of their corresponding adjudication discus - 91 sions . The objective of our qualitative analysis was to group diﬃcult cases based on the speciﬁc source of disagreement as well as the ﬁnal adjudicated consensus labels . Disagreement sources were categorized in a ﬁne - grained and domain - speciﬁc manner ( e . g . , the dark - red ﬁlter needs to be activated in order to detect the development of new vessels around the optic disk , evidence suggesting diagnosis of proliferative diabetic eye disease ) . Based on this ﬁne - grained categorization , we formed pairs of cases sharing the same source of disagreement and ﬁnal consensus labels . From each pair , we assigned one case to the train set and the other to the test set . Train and test set were thus enriched for diﬃcult cases and each image in the train set matched a separate image in the test set . In summary , we used 72 distinct cases in our experiment , 36 for training and 36 for testing . These 72 cases were selected from a larger set of 170 disagreement cases following the procedure described above . The remaining 98 cases could not be paired based on their source of disagreement and consensus labels and were therefore not used . 4 . 4 . 3 Procedure Our study was designed to test two diﬀerent forms of training feedback . The experiment was structured accordingly as a three - step procedure : a training task involving assessment of all images in the train set ; a feedback phase providing information about cases from the training task where an generalist’s answer diﬀered from the adjudicated answer key ; a testing task with all images from the held - out test set . A pre - study questionnaire ( Appendix B . 1 ) and a post - study questionnaire ( Appendix B . 3 ) were administered before and after the study respectively . The pre - study qustionnaire was used to collect information about generalists’ training background and professional ex - perience . We also elicited generalists’ self - eﬃcacy at detecting each of the four pathologies both before and after the study . We determined the number of training cases and dis - cussion points to show based on early piloting of the study and taking into account the constraints of the image selection procedure described above . Training Task . Generalists assessed images for overall gradability and for the presence of four diﬀerent pathologies : diabetic retinopathy ( DR ) , hypertensive retinopathy ( HTNR ) , retinal vein occlusion ( RVO ) , and retinal artery occlusion ( RAO ) . Generalists also rated their own diagnostic conﬁdence and perceived case diﬃculty , each on 5 - point Likert scales . While there exist alternative ways of measuring conﬁdence , we used a 5 - point scale for its granularity , following practices from prior clinical research [ 122 ] . Finally , for each case , generalists provided an open - ended explanation of the reasoning behind their rationale . Figure 4 . 1 shows the task interface including all input prompts for a gradable image . 92 Figure 4 . 1 : Task interface for medical image assessment . The medical image shown is an illustrative example rather than from the real dataset . Training Feedback . After completing the training task , generalist labelers received an email notiﬁcation with a link to an automatically generated feedback document . For each case labeled during the training task , the feedback document compared the answer provided by the generalist to the adjudicated answer key . Generalists were asked to review each case where their answer diﬀered from the answer key . For each case reviewed , generalists ﬁlled out a short questionnaire ( Appendix B . 2 ) , rating their level of comprehension for the rationale behind the answer key ( 5 - point Likert 93 Figure 4 . 2 : Training feedback interface for medical generalists . The medical image shown is an illustrative example rather than from the real dataset . scale ) , specifying the extent to which they agreed with the answer key ( one of three answer options ) , and indicating whether they would change anything about their future labeling approach ( including an open - ended explanation of what they would change ) . The purpose of the case review questionnaire was twofold . First , the surveys helped ensure that gener - alists reviewed the feedback carefully . Second , the surveys were used to collect structured information about generalists’ perception of the feedback provided . Testing Task . After reviewing the feedback for each of the cases where their answer diﬀered from the answer key , generalists were assigned the testing task with images from the held - out test set they had not previously seen . The labeling procedure of the testing task was identical to that of the training task . 4 . 4 . 4 Experimental Conditions We compared two forms of training feedback for medical generalists to examine the impact of presenting generalists with specialist adjudication discussions for diﬃcult cases : • Performance Only : Our baseline condition identiﬁed all cases where any of the diagnosis labels provided by generalists during the training task diﬀered from the 94 adjudicated answer key . For each of these cases , our feedback interface presented the medical image in question along with a list comparing generalist - provided labels with the adjudicated answer key ( Figure 4 . 2 , left and middle ) . • Performance & Discussion : In addition to providing individual performance feed - back about the correctness of labels , our second type of training feedback also pre - sented generalists with case - speciﬁc discussions from our specialist adjudication pro - cedure . Specialist discussions were presented in a tabular format listing text - based comments ( Figure 4 . 2 , right ) . Specialist identities were anonymized to avoid poten - tial biases on the side of generalists . To support validation of our ﬁndings , we make our data , including adjudication discus - sions , characteristics of the generalist experts , as well as their labeling performance and survey responses , publicly available as auxiliary material . 4 . 4 . 5 Analysis For Q1 , we analyzed responses to our case review surveys to understand how medical gen - eralists perceive reading of specialist discussions as a form of labeler training feedback . The case review surveys were collected for all cases where one or more generalist - provided labels diﬀered from the answer key . The Mann - Whitney U test was employed to compare Likert type survey responses about perceived level of comprehension of the rationale behind the correct diagnosis ( H1a ) , agreement with the answer key ( H1b ) , and generalists’ intention to change their labeling approach in the future ( H1c ) . We also qualitatively analyzed the open - ended explanations for why ( or why not ) generalists agreed with the answer key and what ( if anything ) they would change about their future labeling approach and why . For Q2 , we leveraged the fact that our two image sets for training and testing were composed of paired case examples . That is , for each case in the train set , there existed a separate case in the test set which had caused disagreement among specialist adjudicators for the same reason as the training example . We refer to these as train example and test example belonging to the same case - pair . For our hypotheses about improvements in accuracy ( H2a ) , increased diagnostic conﬁ - dence ( H2b ) , and lowered perceived case diﬃculty ( H2c ) , we ﬁrst computed the respective score deltas between the test example and the train example for each case - pair and gen - eralist labeler . Score deltas for correctness were computed separately for each pathology type ( 1 indicating improvement , i . e . , wrong in train and correct in test ; 0 indicating no 95 change , i . e . , wrong or correct in both train and test ; - 1 indicating decreased performance , i . e . , correct in train , but wrong in test ) , and averaged across all generalists per group . We then compared the resulting average accuracy improvements per case - pair between both groups using a permutation test ( with 9999 bootstrap samples , stratiﬁed by case - pair ) . Score deltas for conﬁdence and diﬃculty were computed once for each case - pair and gen - eralist . We tested for diﬀerences between both groups using one - sided Mann Whitney U tests . Finally , we hypothesized that exposing generalists to adjudication discussions from spe - cialists would lead to an improvement in overall diagnostic self - eﬃcacy ( H2d ) . Improve - ment was measured as the pre - to - post - study diﬀerence in diagnostic self - eﬃcacy scores for each generalist and pathology type . Given the limited number of generalists in each group , results for this hypothesis are descriptive and should therefore only be used as an indication . Open - ended survey responses collected from generalists after reviewing feedback for each training case were analyzed qualitatively . Line - by - line inductive open coding was performed by one of the study authors to identify emerging themes and recurring themes are reported below . 4 . 5 Results 4 . 5 . 1 Quantitative Insights Q1 : How do medical generalists perceive reading of specialist discussions as a form of labeler training feedback ? Our hypothesis ( H1a ) that exposing generalist labelers to adjudication discussions would facilitate a deeper understanding of the rationale behind the correct diagnosis was conﬁrmed , indicating a very large eﬀect size ( U = 4620 . 50 , z = - 4 . 44 , p < 0 . 001 , r = 0 . 99 ) . Generalists strongly agreed that they understood the rationale behind the answer key and could explain it to one of their colleagues in about half ( 49 . 1 % ; N = 114 ) of all cases reviewed along with adjudication discussions , compared to only 17 . 5 % ( N = 120 ) of cases reviewed without adjudication discussions ( Figure 4 . 3 , top ) . For the question as to whether generalists agreed with the answer key after reviewing the training feedback , no signiﬁcant diﬀerence was detected between the two training feedback conditions , leaving our hypothesis ( H1b ) unconﬁrmed ( U = 6470 . 50 , z = - 1 . 23 , n . s . , r = 0 . 28 ; Figure 4 . 3 , mid - dle ) . Finally , generalists who were provided with adjudication discussions during training 96 Perf . & Discussion Performance Only 100 75 50 25 0 25 50 75 100 % Cases Reviewed Strongly Disagree Somewhat Disagree Neutral Somewhat Agree Strongly Agree " Do you understand the rationale behind the answer key and could explain it to one of your colleagues ? " Perf . & Discussion Performance Only 100 75 50 25 0 25 50 75 100 % Cases Reviewed No , I strongly disagree with at least some of the answer key prompts . No , but I think all the answers in the answer key are reasonable . Yes , I would change all my answers to match the answer key " Do you agree with the answer key for all prompts ? " Perf . & Discussion Performance Only 100 75 50 25 0 25 50 75 100 % Cases Reviewed No Yes " Is there anything you would change about how you grade in the future ? " Figure 4 . 3 : Generalists’ perception of training feedback . 97 feedback were signiﬁcantly more likely to express an intention of changing their labeling approach in the future than generalists who were presented with just performance feed - back alone , conﬁrming our hypothesis ( H1c ) ( U = 5853 . 00 , z = - 2 . 20 , p < 0 . 05 , r = 0 . 49 ; Figure 4 . 3 , bottom ) . Generalists indicated that they would adjust their labeling approach for more than half ( 55 . 3 % ) of the cases reviewed along with adjudication discussions , while generalists in the group with performance feedback alone denied any future adjustment to their labeling approach for more than half ( 59 . 2 % ) of the cases reviewed . Q2 : How does reading of specialist discussions aﬀect generalists’ diagnostic reasoning for future patient cases ? −20 % −10 % 0 % 10 % 20 % DiabeticRetinopathy HypertensiveRetinopathy Retinal Vein Occlusion Retinal Artery Occlusion Training Feedback Performance Only Perf . & Discussion Figure 4 . 4 : Average change in generalists’ diagnostic accuracy per case - pair in train set and held - out test set . Error bars indicate 95 % conﬁdence intervals . These beneﬁts of reading adjudication discussions for generalists’ perception during training feedback in part also translated to improvements in diagnostic accuracy on the held - out test set ( H2a ) ( Figure 4 . 4 ) . Generalists exposed to adjudication discussions during training feedback showed signiﬁcantly greater accuracy improvements for diagnosing RAO ( µ = 10 . 6 % , CI [ 2 . 9 % , 18 . 2 % ] ) than generalists exposed to performance feedback alone ( µ = 1 . 7 % , CI [ - 6 . 8 % , 10 . 1 % ] ; p < 0 . 05 ; N = 36 case - pairs ) . No diﬀerences were detected for the other pathology types . Generalists exposed to discussions achieved an 98 absolute test accuracy of 93 % for RAO detection , up from 83 % in training . Accuracies for DR , HTNR and RVO stayed constant before and after training at 61 % , 64 % and 83 % respectively . This beneﬁt of showing adjudication discussions for accuracy improvements in RAO diagnosis was accompanied by similar improvements in self - eﬃcacy ( H2d ) : while none of the generalists exposed to performance - only feedback reported any improvements in self - eﬃcacy for RAO diagnosis , the majority of generalists presented with adjudication discussions did ( one generalist with one step of improvement , a second generalist with two steps of improvement , and a third generalist with four steps of improvement on the 5 - point Likert scale for self - eﬃcacy ; Figure 4 . 5 ) . Perf . & Discussion Performance Only 5 4 3 2 1 0 1 2 3 4 5 # Experts 4 steps down 3 steps down 2 steps down 1 step down No change 1 step up 2 steps up 3 steps up 4 steps up Figure 4 . 5 : Improvement in generalists’ self - eﬃcacy score for diagnosis of retinal artery occlusion ( RAO ) after training feedback . Finally , we hypothesized that the presentation of adjudication discussions would lead to increased case - speciﬁc diagnostic conﬁdence ( H2b ) and lowered levels of perceived case diﬃculty ( H2c ) in the testing task ( Figure 4 . 6 ) . Both hypotheses were rejected . Indeed , we observed the opposite eﬀect : Reading adjudication discussions during training feedback was associated with greater reductions in diagnostic conﬁdence ( U = 18555 . 00 , z = - 2 . 74 , p < 0 . 01 , r = 0 . 61 ) and greater increases in perceived case diﬃculty ( U = 14521 . 00 , z = - 2 . 08 , p < 0 . 05 , r = 0 . 47 ) compared to training with performance feedback alone . 99 Perf . & Discussion Performance Only 120 100 80 60 40 20 0 20 40 60 80 100 120 # Case Pairs 4 steps down 3 steps down 2 steps down 1 step down No change 1 step up 2 steps up 3 steps up 4 steps up Change in Diagnostic Confidence per Case Pair & Expert Perf . & Discussion Performance Only 120 100 80 60 40 20 0 20 40 60 80 100 120 # Case Pairs 4 steps up 3 steps up 2 steps up 1 step up No change 1 step down 2 steps down 3 steps down 4 steps down Change in Perceived Difficulty per Case Pair & Expert Figure 4 . 6 : Change in generalists’ diagnostic conﬁdence and perceived case diﬃculty per case - pair in train set and held - out test set . 4 . 5 . 2 Qualitative Insights In addition to quantitative measures of diagnosis performance and attitudinal constructs ( such as self - reported comprehension ) , generalists also provided qualitative feedback about their experience reviewing training cases with and without adjudication discussions . Several themes emerged from this qualitative feedback . We describe some of the more salient themes below , with representative quotes from generalists . 100 Expressions of confusion and uncertainty when comparing their answers to the answer key : Without specialist discussions , the reasons why generalists were incorrect were often opaque : • “I think there could be subtle VB [ venous beading ] . . . is that the rationale for severe ? I think if the resolution was better , I might be able to clearly see IRMA [ intraretinal microvascular abnormalities ] temp to the fovea . . . is that the rationale for severe ? I wasn’t 100 % on these two things , thus the moderate grade . ” • “would like clariﬁcation in the image about the features that make this severe NPDR [ non - proliferative diabetic retinopathy ] and not a CRVO [ central retinal vein occlu - sion , a potential alternate diagnosis ] . ” By contrast , when specialist discussions were present , generalists often cited speciﬁc details of their discussions in explaining their understanding of why they were incorrect : • “blurry view but i can go along with the heme noted by other graders” • “I could see the PDR [ proliferative diabetic retinopathy ] that they were discussing . THere fore [ sic ] I could agree with them for DR [ diabetic retinopathy ] . ” Acknowledgement of missed features : In some cases , generalists originally failed to notice a feature ; but when directed to the relevant part of the image by specialist discussion , acknowledged their miss : • “Agree with PRP scars , should have been PDR . . . . will pay closer attention to laser scars” • “Wasn’t able to detect small / early IRMA . . . ” [ When asked what they would change about grading behavior , the response was ] “Try to detect IRMA better” Calibrating cutoﬀs to match other clinicians : In many cases , generalists recog - nized that a pathology was potentially present , but ambiguous . They explicitly called out the potential for disagreement due to subjective diﬀerences in the cutoﬀ for a ﬁnding being clinically signiﬁcant . This theme emerged particularly in feedback around hypertensive retinopathy ( referred to as HTN in reader comments ) and image gradability . In each case , these subjective diﬀerences could lead to real changes in clinical outcomes , discussed below . Much feedback was given around distinguishing hypertensive retinopathy versus dia - betic retinopathy . Sample comments : 101 • “AV nicking is present and per guidelines that would be considered mild and thus a yes grade for HTN . I do agree that it is mild and I was more generous with grading HTN as G2 mentioned in adjudication . . . [ I plan on ] being more conservative on HTN grading . ” • “Overall , tended to undercall HTN in patients with clear DM” • “I don’t think the AV nicking is as prominent as they [ adjudicators ] say , but I can see why they might have thought that . . . I will look out closer for AV nicking” In addition to hypertensive retinopathy , generalists in our study also expressed uncer - tainty around the threshold for considering a low - quality image gradable : • “Image is blurry making my grade more of a guess , so I marked ungradable . . . [ I plan to ] mark referable pathology even if image is blurry and there is some doubt” • “As adjuncter G2 noted , image resolution makes the grading of MAs [ microaneurysms ] more of a guess ; I didn’t mark it as ungradable since unlikely moderate or worse DR present . ” Again , this distinction is subjective , yet has real - world implications : Many screening programs will refer patients to specialists if their image is ungradable . In both of these cases , the subjective diﬀerences reﬂect a common pattern observed in other cases of vari - ability among eye doctors . Prior work by Kalpathy - Cramer et al . [ 77 ] demonstrated that disagreements among doctors could be explained by diﬀerences in transition points between diﬀerent severity levels . Doctors tended to order cases by severity in a consistent manner ; but varied in the point at which a case was “severe enough” . This suggests that feedback of the sort provided here can substantially improve concordance among doctors , by enabling them to calibrate to the same expert level ( i . e . , in the case of our study , to a specialist - provided answer key ) . A similar phenomenon was reported in a national screening program for breast cancer [ 90 ] . 4 . 6 Discussion In this chapter , we introduced a novel perspective on the problem of calibrating medi - cal professionals for accurate assessment of diﬃcult cases in medical image labeling . We demonstrated empirically that specialist discussions from adjudication of diﬃcult cases can be successfully used as training material for generalist labelers . 102 Case A Case B Case D Case C Case B Case A 100 75 50 25 0 25 50 75 100 % Expert Responses Strongly Disagree Somewhat Disagree Neutral Somewhat Agree Strongly Agree " Do you understand the rationale behind the answer key and could explain it to one of your colleagues ? " Case C Case D Figure 4 . 7 : Example adjudication discussions with mixed ratings ( cases A and B ) and consistently high ratings ( cases C and D ) for answer key comprehension . 4 . 6 . 1 Impact on Comprehension and Accuracy Our experimental results suggest that exposure to specialist discussions during training feedback improves generalists’ comprehension of the rationale behind the correct diagnosis ( H1a ) , and makes a future adjustment of their labeling approach more likely ( H1c ) . We also demonstrated that these beneﬁts observed during training translate into greater im - provements in diagnostic accuracy in a held - out test set ( H2a ) and diagnostic self - eﬃcacy ( H2d ) for one of the four retinal pathologies included in the study . While the overall eﬀect on answer key comprehension was strongly positive , some adjudication discussions were perceived as more helpful than others . Figure 4 . 7 shows four examples of adjudication discussions : two discussions that received mixed ratings from generalists for answer key comprehension ( cases A and B ) , and two with consistently high ratings among generalists ( cases C and D ) . Case A is an example of a discussion characterized by vague language and phrases of uncertainty on the side of specialists , whereas the discussion for case B consists of a single comment only . While a full semantic analysis of our adjudication discussions is beyond the scope of our study , both language use and overall length of a discussion may have aﬀected generalists’ perception its usefulness . Future research may explore ways to motivate specialists a priori ( i . e . , before or during adjudication ) to produce discussion points well suited for training purposes , and evaluate design parameters such as the number of cases and discussion points to show during training . 103 4 . 6 . 2 Impact on Conﬁdence and Perceived Diﬃculty We also hypothesized that presentation of specialist discussions during training feedback would increase generalists’ labeling conﬁdence ( H2b ) and decrease their perceived case diﬃculty ( H2c ) in the testing task , compared to showing performance feedback alone . Neither hypothesis could be conﬁrmed . In fact , we observed the opposite eﬀect : generalists who had seen adjudication discussions for diﬃcult cases , scored lower on labeling conﬁdence and higher on perceived diﬃculty on similar case types in the testing task . One possible explanation for this unexpected observation could be what has been coined the Dunning - Kruger eﬀect [ 45 ] or meta - ignorance : the phenomenon that performance and conﬁdence are often inversely correlated in intellectual tasks . This eﬀect has been primarily explained with so - called unknown unknowns on the side of poor performers , i . e . , their relative lack of awareness of deﬁciencies in their own expertise . Another possible explanation may be that the performance - only training condition did not reveal any information about the diﬃculty of a case . In other words , it did not transmit any information that would help generalists appreciate how hard the training cases were , whereas the training condition including adjudication discussions made the notion of diﬃcult and contentious cases immediately transparent to generalists . 4 . 6 . 3 Learning from Discussions Our work presented in this chapter contributes to the existing body of literature on discussion - based learning . The beneﬁts learners can draw from active participation in group discussions have been established in prior educational and psychological literature . These works have studied diﬀerences between learning from online versus face - to - face dis - cussions . In medical education speciﬁcally , the concept of discussion - based learning has been studied under the names of problem - based learning ( PBL ) and case - based learning ( CBL ) [ 40 , 137 ] . Both PBL and CBL diﬀer from lecture - based learning in that they engage medical students in small discussion groups for the purpose of collective clinical reasoning . PBL is a more open - ended form of discussion - based learning while CBL imposes more guidance and structure on the discussion process . To our knowledge , there has been little prior research in repurposing expert case dis - cussions for training purposes . Previous work [ 68 ] examined one potential application for screening mammography using a pre - existing public annotated image set , but cited a range 104 of challenges , including data curation and quality issues . The authors noted that in prac - tice , separating the production and use of data for diﬀerent purposes was diﬃcult to do cleanly . We believe our work has managed to avoid some of the challenges demonstrated in that work through careful matching of the adjudication and training tasks : generalists were making the same clinical judgments as specialist adjudicators , oriented around the same inputs ( only image data , no metadata ) ; both groups had previously been through a certiﬁcation process for the task ; our experimental design included some data curation ; and the adjudication format intrinsically elicited more detailed justiﬁcations among experts that would not be elicited in a screening context . Thus , our results extend the existing body of educational literature insofar as they demonstrate the beneﬁts of exposing individuals to consumption of case discussions , as opposed to engaging groups in active discussions . This approach is inherently more scalable and ﬂexible in nature than group - based learning . 4 . 6 . 4 Potential Clinical Impact The contributions described here are framed in the context of training machine learning models : Generalist graders are trained to label images used for training a model , and our interventions aim to bring their performance closer to that of specialists . The adjudication discussions used for training were also collected as part of obtaining a test data set for an ML model . These improvements should enable higher - quality ML models , by improving the quality of training data collected by generalist labelers . Our training intervention depends on the availability of specialist labels and discus - sions . Yet , as generalists’ labeling accuracy increases through training , the need for label redundancy may decrease [ 89 ] , enabling more eﬃcient labeling strategies . Our work opens up questions about how best to distribute work between specialists and generalists in the absence of data ground truth . For example , specialists could be recruited to label a small , contained subset of data for training generalists , empowering generalists to take on the rest . Our results may also translate to clinical practice without relying on ML model develop - ment . The labeling workﬂow we use here is similar to that used in telemedicine enterprises , including screening for eye disease [ 34 , 24 ] . Likewise , the adjudication discussions we col - lected may mirror arbitration discussions used in some screening programs [ 90 ] . Thus , there may be potential to use discussions generated for screening purposes in training non - experts . Future work in this direction might aim at mapping new cases , with unknown labels , to similar cases in the adjudicated set , allowing clinicians to view discussions around 105 similar cases in the context of a case being screened . In this way , training interventions like the one we demonstrate may expand the reach of screening programs without necessarily requiring ML systems . 4 . 6 . 5 Limitations Our study has several limitations . First , our experiment was conducted with ten medical generalist labelers and three specialist adjudicators . While future work may aim to repro - duce our ﬁndings with larger participant samples , samples of this size are not uncommon in studies of medical experts like ours where recruitment is a challenge . Given the sample size , we ensured to balance the level of experience of generalists between our two groups , and triangulated our ﬁndings with qualitative analysis , to enrich and provide further support for our quantitative ﬁndings . Second , our study is situated in the medical subspeciality of image - based diagnosis in ophthalmology . While the general approach of collecting and presenting adjudication discussions can be easily applied to outside domains ( medical or non - medical ) , caution is warranted in generalizing our ﬁndings to other disciplines . That said , prior work has demonstrated the prevalence of expert disagreement [ 15 ] and the eﬀectiveness of discussion - based learning [ 38 ] across various medical domains , suggesting that our results on passive consumption of specialist discussions may generalize to other subspecialties as well . We encourage future work to validate our approach in other application scenarios . Third , the remote nature of our study and the tight schedules of our expert participants did not permit precise control over the timing of the individual steps in the procedure . The overall study duration was about one week , but we did not account for potential diﬀerences in time experts spent between training and feedback , and between feedback testing phase . Our study also did not include a measure of long - term improvement . Finally , the cost eﬀectiveness of our proposed technique depends on a pre - existing elec - tronic framework for asynchronous adjudication . While most tele - medical grading centers do not yet use such kind of procedure , there is a growing body of research in HCI on designing and developing methods for online group discussion among crowd workers , that could be leveraged towards a broader applicability of our approach [ 127 ] . 106 4 . 7 Conclusion In this chapter , we provided a novel perspective on the challenge of improving compre - hension and diagnostic accuracy in medical data labeling . We demonstrated that existing specialist discussions from adjudication of diﬃcult cases can be reused as training material for generalist labelers—without introducing additional cost to the labeling process . Our results suggest that the presentation of specialist adjudication discussions can improve gen - eralists’ comprehension of the rationale behind the correct diagnosis , and make a future adjustment of their labeling approach more likely . Furthermore , we showed that these beneﬁts observed during training also translated into signiﬁcantly greater improvements in diagnostic accuracy on a held - out test set for one out of four pathologies . The ﬁndings presented in this chapter have important implications beyond medical diagnosis training alone , highlighting a practical method applicable to expert labeler training in high - stakes data labeling broadly . While this chapter has demonstrated how deliberation data can be put into use to improve human comprehension of diﬃcult cases , the next chapter will shed light on the question of how deliberation data can be leveraged to help AI assistants highlight and explain instances of ambiguity to human end users . 107 Chapter 5 Deliberation Data for Ambiguity - aware AI AI assistants for clinical decision - making show increasing promise in medicine . However , medical assessments can be contentious , leading to expert disagreement . This raises the question of how AI assistants should be designed to handle the classiﬁcation of ambiguous cases . Our study compared two AI assistants that provide classiﬁcation labels for medical time series data along with quantitative uncertainty estimates : conventional vs . ambiguity - aware . We simulated our ambiguity - aware AI based on real - world expert discussions to highlight cases likely to lead to expert disagreement , and to present arguments for con - ﬂicting classiﬁcation choices . Our results demonstrate that ambiguity - aware AI can alter expert workﬂows by signiﬁcantly increasing the proportion of contentious cases reviewed . We also found that the relevance of AI - provided arguments ( selected from guidelines either randomly or by experts ) aﬀected experts’ accuracy at revising AI - suggested labels . The work we present in this chapter contributes a novel perspective on the design of AI for contentious clinical assessments . 5 . 1 Motivation AI systems show increasing promise for numerous clinical applications . Recent advances in deep learning have spawned AI systems with expert - level performance in several domains of medical data classiﬁcation ( e . g . , [ 115 , 116 , 138 ] ) . However , contentious patient cases leading to expert disagreement are prevalent in medicine [ 82 ] . Given the gravity of correct 108 clinical assessments , an important question in the design of AI for medical data analysis is how the system should communicate uncertainty about the classiﬁcation of ambiguous cases . State - of - the - art AI systems are capable of providing quantitative uncertainty estimates ( e . g . , 70 % conﬁdent that a patient case is abnormal ) . These estimates are typically de - rived from posterior probability distributions over the possible classiﬁcation labels . How - ever , prior work has shown that these estimates do not always reliably predict expert disagreement [ 114 ] . Furthermore , numeric representations of uncertainty alone may not be suﬃcient for human experts to make sense of the underlying reasons behind the AI’s uncertainty . Prior work in explainable AI ( XAI ) has established the importance of providing reasons for AI - suggested labels to foster model transparency and user trust [ 3 , 113 , 147 ] . Building on this body of work , we argue that explanations for label ambiguity can be leveraged by AI assistants to support medical reasoning . We detail a within - subject study with twelve expert participants who interacted with both a conventional and an ambiguity - aware AI assistant , reviewing a total of 4 , 514 AI - suggested labels , out of which 22 % were contentious . Both assistants used quantitative representations to communicate uncertainty , but our ambiguity - aware AI also highlighted contentious cases and explained why they were ambiguous by providing human - interpretable arguments for the conﬂicting labels . While this feature was simulated using cases and arguments selected from real - world expert discussions , participants were unaware of its simulated nature . Our ﬁndings suggest that explaining ambiguity can beneﬁt AI - assisted medical reasoning . Our main contributions are : 1 . We present a novel approach for communicating ambiguity in AI - assisted medical reasoning , and provide evidence that ambiguity - aware AI can alter experts’ workﬂows by eﬀectively re - directing their attention and review activity to contentious cases . 2 . We demonstrate that while explaining ambiguity can contribute to experts’ labeling accuracy , its impact heavily depends on the relevance of the arguments provided ( se - lected from guidelines either randomly or by experts ) . Speciﬁcally , if the arguments are not suﬃciently relevant , experts’ accuracy can suﬀer to the point below that of random guessing ( i . e . , less than 50 % accurate ) . 3 . We provide design considerations for communicating uncertainty in AI - assisted med - ical reasoning , laying a foundation for future implementations of AI systems better capable of conveying information about contentious cases . 109 In the following sections , we introduce the design of our AI assistants , followed by our research questions , hypotheses and methods . We then detail our quantitative and qualitative ﬁndings , and conclude with a discussion of design considerations . Figure 5 . 1 : Interface for conventional and ambiguity - aware AI assistants in medical data analysis . 5 . 2 Ambiguity - aware AI Assistance In this study , we explore how human - AI collaboration is aﬀected by an AI system’s ability to not only ﬂag if speciﬁc cases are on the classiﬁcation boundary between two or more categories , but also explain why a given case may be ambiguous . Speciﬁcally , we compare a simulated AI system that provides experts with arguments for conﬂicting classiﬁcation choices for a contentious case to a conventional AI assistant that only provides numeric uncertainty estimates . Our ambiguity - aware AI system uses a Wizard of Oz approach . That is , justiﬁcations for conﬂicting classiﬁcation labels were hand - authored by human experts using the round - based discussion procedure described in Section 3 . 3 . To compare the ambiguity - aware AI assistant to a conventional AI assistant , we led participants to believe that the justiﬁcations presented to them were generated by an AI while , in fact , they were manually selected by human experts . Figure 5 . 1 illustrates how the two AI assistants—conventional AI vs ambiguity - aware AI—were integrated into an existing expert interface for classiﬁcation of medical time series 110 data . Both AIs suggested classiﬁcation labels based on a state - of - the - art deep learning algorithm for sleep stage classiﬁcation [ 138 ] , which has an average accuracy of 87 % ( when judged against consensus labels from an expert panel ) . Both AI assistants provided a sequence overview of all suggested labels ( hypnogram ) , in which each label corresponded to a 30 - second segment in the timeline of a multi - hour patient recording . Experts could open a case by selecting the corresponding time window in the overview , or by navigating through the recording chronologically . The key diﬀerence between the two AI assistants was in how they communicated uncer - tainty to expert end users . Typical output from machine learning algorithms includes not only the predicted classiﬁcation label , but also a likelihood distribution over all possible classiﬁcation choices . Both of our AI assistants were designed to communicate this type of quantitative uncertainty estimate in two ways ( Figure 5 . 1 , blue labels 3 and 4 ) : ( 1 ) in the timeline overview , quantitative uncertainty was visualized by mapping the conﬁdence level ( in percentage ) for each possible classiﬁcation label to a transparency value used to display the label option in the timeline—low conﬁdence classiﬁcation labels were more transpar - ent , and high conﬁdence classiﬁcation labels were more opaque ; ( 2 ) in the case detail view , quantitative uncertainty was displayed in a tabular format , listing all possible classiﬁcation choices ordered from most to least likely along with their percentage conﬁdence levels . While our conventional AI employed this baseline representation of uncertainty , our ambiguity - aware AI also communicated qualitative uncertainty based on arguments gath - ered from real - world expert discussions ( Figure 5 . 1 , orange labels 5 and 6 ) . Speciﬁcally , the timeline overview was augmented with an additional layer highlighting contentious cases that were likely to spur expert disagreement . Note that these suggestions did not dictate the order in which cases were presented to experts for review : experts were still free to decide how to navigate the recording timeline and what cases to review in which order . In addition , the case detail view for contentious cases was extended with an ambigu - ity explanation , listing human - interpretable arguments for conﬂicting classiﬁcation choices . These arguments corresponded to discrete scoring rules from the oﬃcial guidelines for sleep stage classiﬁcation , and were based on data from real - world expert discussions as described above . 5 . 3 Research Questions and Hypotheses Our work addresses two primary research questions about the impact of ambiguity - aware AI on the behaviour ( Q1 ) and perception ( Q2 ) of medical experts . 111 Q1 : How does ambiguity - aware AI aﬀect medical assessments ? Expert time is a limited and expensive resource in clinical settings and should therefore be allocated eﬃciently . We take the stance that while medical experts should make their clinical assessments with care , AI assistants can help prioritize which cases require their attention the most . Our ambiguity - aware AI is designed to redirect experts’ attention towards cases likely to be contentious , and to provide arguments explaining the underlying classiﬁcation ambiguity . Our projection is that ambiguity explanations can inform clinical judgement and thus increase experts’ classiﬁcation accuracy without reducing the number of cases reviewed . Speciﬁcally , we envision that the relevance of ambiguity explanations is crucial for success - fully informing expert judgement . We hypothesize that : [ H1a ] The proportion of contentious cases reviewed by experts will be higher with an ambiguity - aware AI . [ H1b ] Expert eﬃciency in terms of the overall number of cases reviewed will not suﬀer with an ambiguity - aware AI . [ H1c ] Expert accuracy in terms of the overall portion of cases reviewed and labeled correctly will be higher with an ambiguity - aware AI . [ H1d ] The accuracy of classiﬁcation labels experts assigned to contentious cases will depend on the relevance of the provided ambiguity explanations . Q2 : How is ambiguity - aware AI perceived by medical experts ? HCI research has established that poor user perception can be a barrier to adoption of technology regardless of performance . It is therefore important to investigate expert perception , beyond the primary outcome of reliability in AI - assisted clinical assessments . We hypothesize that : [ H2a ] Experts will have a preference for an ambiguity - aware AI . [ H2b ] Experts will consider an ambiguity - aware AI more trustworthy . [ H2c ] Highlighting and explaining contentious cases will not increase experts’ cog - nitive load . [ H2d ] Experts with higher ambiguity tolerance ( as a personality trait ) will have a stronger preference for the ambiguity - aware AI . 112 5 . 4 Methods Here we describe the details of our controlled experiment including the task , data set , study procedure , and statistical analysis . In our study , we simulate a scenario in which a medical AI assistant ﬁrst analyzes a patient case to suggest classiﬁcation labels of a certain kind . A trained medical expert then reviews and corrects as many AI - suggested classiﬁcations as possible within a given time window . This setting represents a future scenario where ( imperfect ) AI systems are deployed in time - sensitive clinical workﬂows while requiring oversight from human experts . 5 . 4 . 1 Task We conducted our study in the ﬁeld of biomedical time - series classiﬁcation , an expert do - main with typically high rates of inter - rater disagreement . In particular , we compared our conventional and ambiguity - aware AIs in the context of assisting trained medical profes - sionals in the task of sleep stage classiﬁcation ( see description in Section 3 . 3 . 2 ) . Figure 5 . 1 shows the expert classiﬁcation interface used in our study . 5 . 4 . 2 Data We selected two separate patient records ( i . e , polysomnographic sleep studies ) with similar characteristics ( Table 5 . 1 ) to examine the two AI assistants under comparable conditions while avoiding learning eﬀects on the side of experts . Note that patient records where selected such that the AI accuracy measured against just the contentious cases was close to 50 % for both patients , meaning that correction by human experts was only required for about half of those cases . In addition to counter - balancing the order in which conventional and ambiguity - aware AI were presented to ex - perts , the assignment of AI assistant to patient record was also fully counter - balanced . A separate third patient record was randomly selected for a practice phase preceding the main task . Adjudication data . We source the data required to simulate our ambiguity - aware AI ( i . e . , which cases have high expert disagreement , and what are the arguments for diﬀerent classiﬁcation labels ) from our previous study reported in Section 3 . 3 . This prior work intro - duced a round - based procedure to adjudicate clinical classiﬁcation disagreements among groups of experts using a highly structured argument format . In particular , arguments 113 Table 5 . 1 : Characteristics of patient records used by the AI assistants . Patient A Patient B Pathology Dementia Dementia Sex Female Male Age Group 70 - 74 years 75 - 79 years Recording Duration 6h 52 min 30 sec 6h 18 min 30 sec # Cases Total 825 757 % Contentious Cases Total 18 % 18 % % Contentious Cases out of all Correct AI Suggestions 12 % 11 % % Contentious Cases out of all Incorrect AI Suggestions 48 % 51 % AI Accuracy Overall 84 % 83 % AI Accuracy on Contentious Cases 55 % 51 % were collected in the form of discrete classiﬁcation rules taken from the oﬃcial medical guidelines ( e . g . , In patients who generate alpha rhythm , score stage N1 if the alpha rhythm is extenuated and replaced by low - amplitude , mixed frequency activity for more than half of the epoch ) . This data set was used to simulate output for our ambiguity - aware AI : cases that had caused expert disagreement and produced conﬂicting arguments in this data set were high - lighted as contentious cases by our ambiguity - aware AI . Arguments put forward during the real - world adjudication process were presented for these cases to explain the ambigu - ity around conﬂicting classiﬁcation labels . For Q1 , we sought to examine the impact of argument relevance on clinical decision making for contentious cases ( H1d ) . To this end , we added noise to ambiguity explanations by replacing a random subset ( 20 % ) of argu - ments with scoring rules randomly selected from the same medical guidelines . Otherwise , justiﬁcations were displayed as selected by experts during prior discussions , without fur - ther manipulation . Our randomization procedure was constrained to ensure that randomly selected arguments were never mentioned in the real - world expert discussion for a given case , and that all arguments presented were still pertinent to their classiﬁcation choice : for example , an argument for REM sleep could only be replaced with another argument for REM sleep . Finally , classiﬁcation accuracy ( of either AI or human experts ) was measured against the consensus decision of our round - based adjudication procedure involving a panel of three independent experts for each classiﬁcation decision . 114 5 . 4 . 3 Procedure We recruited twelve sleep technologists as expert participants for our study . Our experts were recruited with the help of an allied sleep technologist from a local research clinic who posted our recruitment letter to a domain - speciﬁc Facebook group with about 4700 sleep technologists from diﬀerent countries . Each expert was exposed to both AI assistants in a counter - balanced manner . Consent procedure and pre - study questionnaire . After providing informed con - sent for participation in the study , experts reported information about their demographics ( age , gender , geographic location ) and professional background ( professional or academic training , number of years of professional experience ) . We employed the Intolerance of Am - biguity scale , a psychometric survey instrument developed by Budner [ 20 ] , to learn about each expert’s general level of tolerance for ambiguity in decision making . We included the phenomenological denial sub - scale consisting of four statements : • An expert who doesn’t come up with a deﬁnite answer probably doesn’t know too much . • There is really no such things as a problem that can’t be solved . • People who insist upon a yes or no answer just don’t know how complicated things really are . • Many of our most important decisions are based on insuﬃcient information . Experts rated their level of agreement for each of the four statements on a 7 - point Likert scale . Appendix C . 1 provides a complete list of questions and answer options from the pre - study questionnaire . Practice phase . Next , experts familiarized themselves for about 5 minutes with our waveform classiﬁcation user interface and with the basic interface components common to both AI assistants . Tasks . Experts performed the same main task twice , once with the ambiguity - aware AI assistant and once with the conventional variant , in a counter - balanced order . In each task , experts were asked to review the waveform of a particular patient record within a limited time window of 15 minutes . The patient record was fully pre - classiﬁed by the AI assistant and experts were asked to correct as many of the AI - suggested labels as possible within the given time limit . Experts could revise AI - suggested labels by selecting a diﬀerent sleep stage label in the classiﬁcation UI ( Figure 5 . 1 , gray labels 1 and 2 ) . After each of the two tasks , experts ﬁlled out a brief feedback questionnaire ( Appendix C . 2 ) probing for their 115 perception of each AI assistant . The survey included scales to measure perceived trust towards the AI assistant [ 73 ] , cognitive load ( NASA - TLX ; [ 66 ] ) during the task , perceived diagnostic utility and mental support provided by the AI assistant , and whether experts thought they would use the AI in practice . Post - study questionnaire . After completing the tasks , experts compared both AI assistants with respect to perceived reliability , trustworthiness , capability and provided an overall preference . Experts rated each of these four items on a 7 - point Likert scale ranging from 1 ( totally version A ) , 2 ( much more version A than B ) , 3 ( slightly more version A than B ) , 4 ( neutral ) , etc . to 7 ( totally version B ) . Appendix C . 3 provides a complete list of questions and answer options from the post - study questionnaire . After completing the post - study questionnaire , participants received a debrief statement informing them about the simulated nature of the ambiguity - aware AI in this study . Experts were compensated with CA $ 50 via online gift cards ( or the equivalent amount in their preferred currency ) for participation in the study , with an average study duration of one hour . 5 . 4 . 4 Analysis For Q1 , we investigated the impact of our ambiguity - aware AI on experts’ behaviour in reviewing AI - suggested classiﬁcation labels . We used dependent t - tests to compare both AI assistants with respect to the following outcome measures per expert : the proportion of contentious cases out of all reviewed cases ( H1a ) , the number of cases reviewed given a ﬁxed time window ( H1b ) , and the accuracy rate of expert - provided labels ( H1c ) . For our secondary analysis on the relevance of arguments for contentious cases ( H1d ) , we used Pearson’s chi - squared test of independence to compare experts’ average accuracy at revising AI - suggested labels when presented with either expert - selected arguments only vs . cases with one or more randomly selected arguments . For Q2 , we compared experts’ perception of both AI assistants . A possible trend in overall preference ( H2a ) for either of the AI assistants was examined using a one - sample Wilcoxon signed rank test . Self - reported scores for perceived trust ( H2b ) and cognitive workload ( H2c ) were compared between both AI assistants using Wilcoxon signed - rank tests . Finally , we used a Pearson’s chi - squared test of independence to test whether experts’ overall tendency of ambiguity tolerance ( ambiguity - tolerant vs . intolerant ) was associated with their overall preference for either AI assistant ( preference for ambiguity - aware AI vs . conventional AI ; H2d ) . Finally , line - by - line inductive open coding was performed by one of the study authors to extract emerging themes from open - ended survey responses submitted by experts after 116 20 % 40 % 60 % Conventional AI Ambiguity−aware AI Figure 5 . 2 : Proportion of contentious cases out of all cases reviewed . Ambiguity - aware AI guided experts’ attention to contentious cases . Connecting lines correspond to indi - vidual experts . 0 % 20 % 40 % 60 % 80 % Contains Random Argument All Arguments Expert−Selected Relevance of Ambiguity Explanation Figure 5 . 3 : Experts’ correction rate for cases with ambiguity explanation . The rele - vance of ambiguity explanations aﬀects clin - ical assessments of contentious cases . Error bars present 95 % conﬁdence intervals . interacting with each AI . Experts were asked to reﬂect on how they decided which cases to review and why , what information they used to make these decisions , and how information about the AI’s uncertainty aﬀected their decision making . Recurring themes are reported below . 5 . 5 Results 5 . 5 . 1 Expert Participants Based on the pre - study questionnaire , our expert participants were located in the United States ( 6 ) , Canada ( 4 ) , the European Union ( 1 ) and one other unspeciﬁed location ( 1 ) . Eleven of our expert participants reported having at least ten years of experience working as sleep technologists , and one participant reported having ﬁve to ten years of experience . Out of the twelve experts , ﬁve self - reported as female , six as male , and one participant did not specify their gender . The distribution over age groups was : 26 - 35 ( 1 ) , 36 - 45 ( 7 ) , 46 - 55 ( 1 ) , 56 + ( 2 ) , with one participant who did not specify their age group . 117 5 . 5 . 2 Quantitative Insights Q1 : How does ambiguity - aware AI aﬀect medical assessments ? We hypothesized that the ambiguity - aware AI assistant would alter experts’ workﬂow and increase the number of contentious cases they review in the patient recording ( H1a ) . On average , the proportion of contentious cases out of all cases reviewed was signiﬁcantly greater with the ambiguity - aware AI ( M = . 38 , SE = . 05 ) than with the conventional AI ( M = . 23 , SE = . 03 ) , conﬁrming our hypothesis ( Figure 5 . 2 ) . This diﬀerence was signiﬁcant t ( 11 ) = - 2 . 82 , p < . 05 , indicating a large eﬀect size r = . 48 . We also hypothesized that using the ambiguity - aware AI would not negatively aﬀect the number of cases reviewed by experts ( H1b ) . Our results show that there was no signiﬁcant diﬀerence in the number of cases experts reviewed with the conventional AI ( M = 197 . 25 , SE = 47 . 60 ) compared with the ambiguity - aware AI ( M = 178 . 92 , SE = 57 . 02 ) , t ( 11 ) = . 50 , p = . 63 . This result provides support for our hypothesis that experts’ eﬃciency at reviewing AI - suggested labels was not negatively aﬀected by being exposed to ambiguity explanations for contentious cases . Our projection that experts would achieve a higher overall labeling accuracy when assisted by the ambiguity - aware AI compared to the conventional one ( H1c ) could not be conﬁrmed , t ( 11 ) = 1 . 00 , p = . 34 , r = . 53 . Finally , we examined the potential impact of the relevance of ambiguity explanations for contentious cases on the likelihood that an expert would revise an AI - suggested label correctly ( H1d ) . We observed a signiﬁcant association between the relevance of arguments ( whether they contain randomly selected arguments or not ) and experts’ accuracy rate at revising AI suggestions χ 2 = 16 . 83 , p < . 001 . In other words , the chance of a label getting revised correctly by an expert was signiﬁcantly higher if the arguments provided were selected from guidelines via adjudication discussions ( i . e . , were relevant ) than if they were selected from the guidelines randomly ( Figure 5 . 3 ) . The odds of a label getting revised correctly by an expert were 4 . 48 times higher ( odds ratio ) if the arguments provided were selected from guidelines by experts than if they were selected from the guidelines randomly . Q2 : How is ambiguity - aware AI perceived by medical experts ? For Q2 , we explored experts’ perception of both AI assistants . Results for our hypoth - esis that experts would have an overall preference for the ambiguity - aware AI ( H2a ) were mixed and were not statistically signiﬁcant ( p = . 88 ) . Except for two experts who did not have a preference for either AI , preferences were polarized . Out of the ten participants who expressed a preference , half preferred the ambiguity - aware AI assistant and the other half preferred the conventional AI ( Figure 5 . 4 ) . 118 Which AI did you prefer overall ? Which AI was more capable ? Which AI was more reliable ? −7 −5 −3 −1 1 3 5 7 Totally ConventionalAI Much more Slightlymore Neutral Slightlymore Much more Totally Ambiguity−aware AI Figure 5 . 4 : Experts’ preferences between both AI assistants . While no signiﬁcant diﬀerences could be detected between both AIs regarding perceived overall trust ( p = . 47 ) , the ambiguity - aware variant was considered to have signiﬁcantly greater integrity ( p < . 05 ) , and we observed a potential , yet statistically insigniﬁcant trend suggesting that experts may have had higher conﬁdence in the ambiguity - aware AI than in the conventional one ( p = . 09 ; Figure 5 . 5 ) . These results provide partial support for our hypothesis H2b . Furthermore , there were no detectably signiﬁcant diﬀerences between the cognitive load scores of the two AI assistants on the NASA - TLX scale ( p = . 77 ) , providing support for our hypothesis about their comparable mental demand ( H2c ) . Finally , while experts varied in their level of ambiguity tolerance ( M = 17 . 25 , SE = 1 . 17 ) , ranging from 10 to 26 on a scale from 4 to 28 , no signiﬁcant eﬀect of ambiguity tolerance on expert perception could be detected ( p = . 62 ) , leading us to reject hypothesis H2d . 5 . 5 . 3 Qualitative Insights Our qualitative analysis of participant responses to open - ended survey questions yielded insights on how our ambiguity - aware AI assistant can aﬀect experts’ workﬂows and their mental model of AI assistants . Altering expert workﬂows . Time constraints play an important role in real - world clinical workﬂows [ 143 ] . Case triaging—determining the priority for which cases receive an 119 Ambiguity−aware AI Conventional AI −8 −6 −4 −2 0 2 4 6 8 The AI Assistant has integrity Ambiguity−aware AI Conventional AI −8 −6 −4 −2 0 2 4 6 8 StronglyDisagree SomewhatDisagree Neutral SomewhatAgree StronglyAgree I am confident in the AI Assistant Figure 5 . 5 : Expert ratings for perceived integrity and conﬁdence from trust in automation scale . expert’s attention ﬁrst—is a common practice in medicine . Similarly , our ambiguity - aware AI assistant triages based on ambiguity by prioritizing contentious patient cases that need more attention from the expert . Our qualitative ﬁndings suggest that some experts found the ambiguity - aware AI system to be more helpful in reducing cognitive load compared to the conventional assistant : “Assistant B [ ambiguity - aware ] was more helpful in making me think as it listed the scoring rules that could apply to the epoch . " Our analysis further highlights the eﬀectiveness of the ambiguity - aware AI assistant in redirecting experts’ attention to contentious cases . That is , six out of twelve experts in our study explicitly mentioned that their workﬂow diﬀered between the two AI assistants , such that they prioritized checking contentious cases using the ambiguity - aware AI : “I ﬁrst 120 chose the areas that the AI had marked as ambiguous and then tried to check sleep onset , REM onset , and stage 3 as time allowed . " One major criticism to the traditional approach of representing AI uncertainty with numeric conﬁdence values is that it is not suﬃcient for experts to make sense of the underlying reasons behind the AI’s uncertainty . Our qualitative evidence suggests that in choosing between numeric representations of AI uncertainty and human - interpretable ambiguity arguments experts found the latter to be more eﬀective in guiding their attention : “When I saw that the [ conventional ] AI had lower than an 80 % conﬁdence in the scored stage I tried to double check that epoch . . . I mostly used the areas marked as ambiguous [ by the ambiguity - aware AI ] as opposed to the percentage of certainty . " In our study , we imposed time limits to understand how ambiguity - aware AI would help guide expert attention under the time constraints of real - world workﬂows . This temporal constraint was received diﬀerently by diﬀerent expert participants . While some experts perceived the timers to be “very frustrating " , others found them useful : “The time limit was great as my ﬁrst instinct was to review the entire study and see if I was in agreement " . Mental models of AI assistants . Experts have preconceived mental models about the level of ambiguity in diﬀerent cases . For instance , experts may draw from their prior experience of disagreements with other colleagues and have intuitions about what type of medical assessment is the most diﬃcult to agree upon in their speciﬁc domain ( e . g . certain classiﬁcations and stage transitions ) . It is therefore possible that these intuitions are projected onto the AI assistant to anticipate where the AI would likely make mistakes : “I had to think where do we , as scoring techs , usually have the strongest disagreement and check those epochs . " Beyond preconceptions , we also observed that experts developed their own mentals models about the two types of AI systems : “AI 1 [ ambiguity - aware ] was rather impressive actually . Although in study 2 , the persistent arousals may have interfered with accuracy of AI 2 [ conventional ] . " Further , their interaction experience with the same AI assistant can also shape their judgement of where they will likely disagree with the system : “On ’B’ [ ambiguity - aware ] , I tried to focus more on the ambiguous epochs indicated by the AI and then on the staging that the AI in ’A’ [ conventional ] did not perform well with . " AI assistants could leverage this insight by grouping contentious cases based on an expert’s reviewing and correction behaviour to adjust to their internal representation of speciﬁc types of ambiguity . 121 5 . 6 Discussion In this chapter , we studied how highlighting and explaining ambiguity by AI assistants can aid medical experts in their decision making for contentious clinical cases . We con - ducted a within - subjects study to investigate the use of ambiguity - aware AI assistants by medical experts . Our results show that the ambiguity - aware AI can alter experts’ work - ﬂows by increasing the proportion of contentious cases reviewed while maintaining overall productivity . While experts’ overall labeling accuracy was not aﬀected by providing ambiguity - awareness , we observed a signiﬁcant eﬀect of argument relevance on experts’ case correction rate . This promising insight motivates future research into the development and validation of ambiguity - aware AI systems capable of providing highly relevant ambiguity explanations for previously unseen cases . Experts’ overall preferences and perceived levels of trust for either AI were polarized . Results suggested higher perceived integrity , and a trend towards higher conﬁdence in the ambiguity - aware AI assistant compared to the conventional variant . These mixed results may indicate the existence of other latent variables ( e . g . , experts’ familiarity with or trust in automation technology ) which could shape experts’ perception of AI systems generally . Here , we discuss the generalizability and design implications of our ﬁndings and conclude with limitations of our study and directions for future work . 5 . 6 . 1 Design Implications Our ﬁndings have implications for diﬀerent stages in the design of AI - based CDS systems , ranging from data collection over model training to the design of user interfaces for AI systems . Data collection . In our work , we simulate an AI assistant’s capability to identify mul - tiple conﬂicting arguments for why a medical classiﬁcation decision may be contentious . To this end , we rely on discussion metadata from our previous study on collective adjudication among medical experts reported in Section 3 . 3 . Developing an AI system capable of gen - erating ambiguity explanations for previously unseen cases would require that structured information on contentious cases is given in the training data . While several approaches have been suggested to collect unstructured , open - ended arguments for contentious classi - ﬁcation cases [ 29 , 42 , 125 , 129 ] , recent work from the medical domain demonstrates that imposing structure on the discussion process can facilitate a deeper understanding of ex - pert disagreement [ 123 ] and accelerate consensus formation [ 127 ] . We recommend that data 122 collection procedures for AI - based CDS systems be equipped with structured discussion procedures to beneﬁt from these ﬁndings and facilitate the development of ambiguity - aware classiﬁcation models . Model training . Our study suggests that expert workﬂows and trust can be positively aﬀected by endowing AI - based CDS systems with the ability to not only make classiﬁcation suggestions , but also to identify which cases may be contentious and why . Implementation of such systems would require that supervised machine learning models are equipped with additional prediction targets beyond classiﬁcation labels alone . These additional prediction targets could include the likelihood and potential sources of expert disagreement . They could be integrated either into one joint training process or by developing several separate models , one for each target . Cohen et al . [ 32 ] describe some additional requirements and challenges in this context . User interfaces . In this work , we evaluate one speciﬁc way of displaying and ex - plaining ambiguity to expert end users by visually emphasizing contentious cases within a collection of cases and by providing text - based arguments for conﬂicting classiﬁcation choices . While our results suggest that this representation may be eﬀective , we recommend that future work may explore more complex design considerations such as prioritization of cases based on their disagreement likelihood , and interactive ﬁlters to group cases which may be contentious for similar reasons . 5 . 6 . 2 Generalizability Our study sheds light on the use of ambiguity - awareness in the speciﬁc domain of sleep stage classiﬁcation based on biomedical time series data . Therefore , caution is warranted in generalizing the results of this study to outside domains . However , we argue that similar displays of ambiguity explanations can be useful for various types of medical assessments because the issues motivating our study are prevalent across subspecialties . Despite the abundance of standardized medical guidelines [ 11 ] , expert disagreement is prevalent across medical disciplines [ 15 , 128 ] , making our approach useful beyond the speciﬁc domain of sleep health . For example , diﬀerential diagnosis of epilepsy requires that specialized neurologists visually inspect EEG data similar in nature to that used in our study . Ambiguity - aware AI assistants could support the small pool of specialists world - wide in detecting epileptiform abnormalities [ 10 ] and thus increase access to healthcare for patients with epilepsy in low - and middle - income countries [ 149 , 150 ] . The issue of expert disagreement in medical assessments has also been addressed using structured adjudication for other data modalities , e . g . , assessment of retinal images for 123 diabetic retinopathy grading [ 126 , 127 ] or glaucoma risk assessment [ 65 , 108 ] . These studies suggest that the recommendations we make for data collection in this work have been considered independently and may be of merit beyond the development of ambiguity - aware AI systems . 5 . 6 . 3 Limitations In this chapter , we conducted a within - subjects study to investigate the use of ambiguity - aware AI assistants by medical experts . Due to the tight working schedule of our experts and the remote nature of our study , it was challenging to control the timing of each step in the experiment precisely . For instance , participants varied in how long they waited after completing the ﬁrst main task before starting the second one . This lack in experimental control may have impacted the extent to which exposure to the ﬁrst AI assistant aﬀected how experts interacted with the latter one . In our Wizard - of - Oz study , the ambiguity - aware AI was simulated , in the sense that the assistant presented ambiguity information and arguments generated from real expert discussions . While prior work has demonstrated the potential of predicting the likeli - hood of expert disagreement directly from raw medical data [ 114 ] , future work can focus on training machine - learning algorithms based on ambiguity explanation data to provide human - interpretable arguments for previously unseen contentious cases . Finally , related work shows that medical practitioners seek to understand the speciﬁc strengths and weaknesses of an AI before interacting with it [ 22 ] . Our work oﬀers similar ﬁndings by showing that explaining AI uncertainty can be useful also during the interaction and help experts allocate cognitive resources and reassess their level of trust appropriately for each speciﬁc case . While we did not detect a signiﬁcant eﬀect of ambiguity tolerance on overall AI preference , we observed a trend that experts with higher ambiguity tolerance exhibited more polarized preferences towards either AI assistant . Future research may explore how diﬀerent variables such as personality traits [ 20 ] , domain - speciﬁc and culture - speciﬁc communication styles [ 121 ] may shape these expectations and perceptions on the side of medical experts . 5 . 7 Conclusion In this chapter , we provided a novel perspective on the problem of how AI assistants for medical reasoning can explain ambiguous cases to human experts . Our results from a user 124 study with twelve medical experts comparing a conventional AI assistant to a simulated ambiguity - aware AI assistant suggest that the system’s ability to not only ﬂag , but also explain contentious patient cases has merits for end users . In particular , we observed that in comparison to the conventional AI , the ambiguity - aware AI was more eﬀective in guiding experts’ attention to contentious medical cases . In addition , our results demonstrate that if explanations contain irrelevant arguments , experts’ accuracy at correcting AI - suggested labels can drop below 50 % . The work we presented in this chapter has implications for the design of AI - based technology not only in the ﬁeld of medicine , but more broadly in ﬁelds that face similar challenges with classiﬁcation ambiguity and expert disagreement . 125 Chapter 6 Conclusion Through the research presented in this dissertation , we introduced and studied novel ap - proaches for handling ambiguity in human - AI collaborative workﬂows for data classiﬁcation problems . We provided methods , open datasets and empirical insights to address ambigu - ity in various steps of the AI pipeline . Most importantly , we provided a thorough analysis of group deliberation as a tool to understand and capture the structure of ambiguous cases in data labeling . We also showed how the resulting deliberation data can be lever - aged to improve outcomes both in the training of human labelers and for communicating classiﬁcation ambiguity in AI - powered assistive interfaces . In this chapter , we summarize our main contributions , specify how our ﬁndings support the thesis statement , provide design recommendations for handling ambiguity in human - AI collaborative workﬂows and conclude with several directions for future research in the broader space of human - AI interaction in the context of complex , ill - deﬁned and ambiguous problems . 6 . 1 Contributions and Impact We implemented and studied group deliberation as a tool to detect and explain ambiguity in data labeling workﬂows . Our investigation focused on three diﬀerent contexts . First , we presented Crowd Deliberation , the ﬁrst platform enabling synchronous group deliberation in the context of non - expert crowdsourcing . We presented evidence suggest - ing that group deliberation can not only signiﬁcantly improve label accuracy , but that 126 it also produces data that helps to understand why disagreement arises and under what circumstances it can be resolved . Second , we applied our ﬁndings to the expert domain of medical image classiﬁcation , applying an asynchronous deliberation workﬂow to a complex diagnostic task performed by medical specialist labelers . Our insights from an experiment with 15 retina specialists showed that structuring deliberation arguments around a set of low - level diagnostic criteria signiﬁcantly improved the eﬃciency of the deliberation process without compromising its reliability . Third , we leveraged our ﬁndings to design and build CrowdEEG , the ﬁrst online plat - form enabling expert crowds to collaboratively annotate and deliberate on medical time series data . CrowdEEG implemented an asynchronous workﬂow combined with an even more structured format for capturing human arguments . Using an observational study with 36 sleep technologists , we analyzed various factors including expert background , data characteristics , labeling guidelines and viewer conﬁgurations to better understand how disagreement arises and when it can be resolved . We also demonstrated how the resulting deliberation data can be put into use to train human expert labelers . Our evidence from a controlled experiment with ten medical general - ists suggests that reading deliberation data from medical specialists substantially improved generalists’ comprehension as well as their diagnostic accuracy on diﬃcult patient cases . Finally , we leveraged deliberation data for a separate goal , to simulate and study ambiguity - aware AI , i . e . , AI that not only highlights ambiguous cases , but also explains the underlying sources of ambiguity to end users . Our results from an experiment with twelve sleep technologists demonstrated that this form of ambiguity - aware AI can signiﬁcantly improve the ability of expert end users to triage and trust AI - provided output . We also provided evidence suggesting not only that expert end users paid attention to AI - provided ambiguity explanations , but also that the relevance of these explanations to the speciﬁc case at hand was crucial for human experts to accurately classify diﬃcult ambiguous cases . To stimulate future research in this space , we made two novel datasets publicly available that contain deliberation data from both non - expert and expert classiﬁcation tasks . 6 . 2 Support for Thesis Statement We summarize our hypotheses from Chapters 3 , 4 and 5 in Tables 6 . 1 and 6 . 2 , including the degree to which each hypothesis was supported by our observations . Collectively , our insights provide support for the central claims of our thesis statement . 127 Claim : Ambiguity , the quality of being open to more than one interpretation , permeates our lives . It can take various forms including linguistic and visual ambiguity , arise for various reasons including heterogeneous data or vague deﬁ - nitions , and give rise to inter - rater disagreements that can be hard or impossible to resolve . We demonstrated that ambiguity in data classiﬁcation can take various forms . Linguistic ambiguity was prevalent not only in the data instances to be classiﬁed , e . g . , the text docu - ments involved in the sarcasm detection and semantic relation veriﬁcation tasks in Section 3 . 1 . It could also be found in the classiﬁcation guidelines used by humans to interpret the data at hand , irrespective of data modality . Visual ambiguity was showcased in the context of both image classiﬁcation ( Section 3 . 2 ) and time series classiﬁcation ( Section 3 . 3 ) tasks . We provided analyses of the various sources of classiﬁcation ambiguity : Section 3 . 1 demonstrated that fuzzy deﬁnitions in classiﬁcation guidelines , discrepancies in labeler ex - pertise , subjectivity , missing context , contradictory evidence or easy - to - miss details in the data can constitute relevant sources of inter - rater disagreements in novice crowd work . Sec - tion 3 . 3 provided similar insights in the context of expert tasks , assessing potential sources of inter - rater disagreement within four categories : grader diﬀerences , viewer diﬀerences , data characteristics , classiﬁcation guidelines . Chapter 3 described three separate case studies that provide support for the claim that certain instances of classiﬁcation ambiguity give rise to inter - rater disagreement that is hard or impossible to resolve even when addressed through group deliberation . Claim : Human and artiﬁcial intelligence can beneﬁt from novel methods that aim to detect and explain instances of ambiguity . The expected advantages of such methods are a better understanding of why disagreement arises and when it can be resolved , as well as better approaches for handling ambiguity in human decision making—both unassisted and when assisted by artiﬁcial intelligence . Chapter 3 , speciﬁcally Sections 3 . 1 and 3 . 3 demonstrate how group deliberation is an ef - fective tool to understand why disagreement arises and under which circumstances it can be resolved in the context of data classiﬁcation . Chapters 4 and 5 illustrate how the resulting deliberation data can be leveraged to support better approaches for handling ambiguity in human decision making . The case study described in Chapter 4 serves as an example of how deliberation data can be used as training material to improve unassisted human decision making on diﬃcult cases in the context of medical image classiﬁcation . Chapter 5 showcased how deliberation data 128 can be used to support human decision making assisted by an AI capable of not only highlighting ambiguous classiﬁcations , but also explaining potential sources of ambiguity in human - interpretable terms . 6 . 3 Design Recommendations This dissertation aims to provide guidance for both researchers and practitioners who seek to incorporate the notion of ambiguity into the design and implementation of human - AI collaborative systems . Based on the insights from our studies on group deliberation in data labeling and experiments leveraging deliberation data for labeler training and for ambiguity - aware AI , we make the following recommendations for handling ambiguity within various steps of the AI pipeline . Data Labeling : Procedures for data labeling should aim to identify and diﬀerentiate between instances of ambiguity that are unintended and avoidable , and instances of am - biguity that are either inevitable or an intentional part of the concept being labeled . The recommendations below should be applied with this distinction in mind . Labeling guidelines are a common source of linguistic ambiguity giving rise to inter - rater disagreement . Unintentional vagueness in the labeling guidelines should be systematically analyzed and removed . However , it is possible that the labeling guidelines are standardized and may not be changed . Inter - rater disagreement is a useful signal to identify potential instances of ambiguity . However , label disagreements may also be due to other reasons such as human input mistakes . The circumstance that human labelers can guess the likelihood of disagreement for some tasks better than chance may be used to allocate resources for label redundancy more eﬃciently . Group deliberation is a useful tool to decide which instances of inter - rater disagreement are hard or impossible to resolve and why . Deliberation therefore can be used to identify truly ambiguous cases as well as the underlying sources of persistent disagreement . Delib - eration data may in turn be used to identify and reduce unintentional vagueness in labeling guidelines . Groupthink dynamics should be mitigated by hiding the true identity and professional credentials ( e . g . , academic degree ) of discussion members , by encouraging balanced con - tribution among the group and by avoiding explicit incentives for reaching unanimous consensus . 129 Table 6 . 1 : Summary of hypotheses and their degree of support from Chapter 3 . 3 . 1 Crowd Deliberation for Text Labeling Q1 : Why disagree ? H1a Sources of disagreement diﬀer by task type . Supported ( * * * ) H1b Annotators can predict disagreement levels . Supported for Relation task ( * * * ) Q2 : Why unresolved ? H2a Sources of disagreement aﬀect resolvability . Supported for Subjective Case ( * * ) , Fuzzy Deﬁnition ( * * ) and Contradictory Evidence ( * ) H2b Task subjectivity aﬀects resolvability . Partially supported H2c Extent of equal contribution aﬀects resolvability . Supported ( * ) H2d Level of initial consensus aﬀects resolvability . Supported ( * ) H2e Amount of overlap in evidence aﬀects resolvability . Not supported Q3 : Impact ? H3a Worker deliberation improves answer correctness . Supported ( * * * ) H3b Groupthink is discouraged by our deliberation incentives . Supported H3c Sources of disagreement and the extent of equal contribu - tion aﬀect whether cases get resolved correctly . Supported for Expertise Needed ( * ) , Missing Context ( * ) and the extent of equal contribution ( * ) 3 . 2 Expert Deliberation for Image Labeling H1 Remote and in - person deliberation produce similar labels . Supported H2 Remote deliberation is a reproducible process . Supported H3 Imposing argument structure helps resolve disagreements more eﬃciently in remote deliberation . Supported ( * * * ) 3 . 3 Expert Deliberation for Time Series Labeling Q1 : Why disagree ? H1a Diﬀerences in expert background predict disagreement . Supported for Experience ( * * * ) and Location ( * * * ) H1b Diﬀerences in viewer settings predict disagreement . Supported ( * * * ) H1c Data characteristics predict disagreement . Supported for Parkinson’s Disease ( * * * ) , Alzheimer’s Disease ( * * * ) and Signal Complexity ( * * * ) Q2 : Why unresolved ? H2a Diﬀerences in expert background predict resolvability . Supported ( * ) H2b Diﬀerences in viewer settings predict resolvability . Supported for Freq . Filter ( * * * ) and Signal Visib . ( * * ) H2c Data characteristics predict resolvability . Supported for Parkinson’s Disease ( * * * ) , Sleep Apnea ( * * ) , Sig . Complexity ( * * * ) , Sig . Transitions ( * * * ) H2d Disagreement sources predict resolvability most strongly . Partially supported Q3 : Impact ? H3a Experts ﬁnd deliberation labels reliable and trustworthy . Supported ( * * * ) H3b Deliberation can signiﬁcantly change diagnostic markers . Supported for % REM ( * ) 130 Table 6 . 2 : Summary of hypotheses and their degree of support from Chapters 4 and 5 . 4 Deliberation Data for Labeler Training Q1 : Perception - Reading specialist discussions . . . H1a Improves generalists’ comprehension of the correct diagnosis . Supported ( * * * ) H1b Increases generalists’ agreement with the answer key . Not supported H1c Motivates adaptations in generalists’ labeling approach . Supported ( * ) Q2 : Behaviour - Reading specialist discussions . . . H2a Improves generalists’ diagnostic accuracy . Supported for retinal artery occlusion ( * ) H2b Increases generalists’ diagnostic conﬁdence . Opposite association found ( * * ) H2c Lowers generalists’ perceived case diﬃculty . Opposite association found ( * ) H2d Improves generalists’ diagnostic self - eﬃcacy . Partially supported 5 Deliberation Data for Ambiguity - aware AI Q1 : Behaviour ? H1a Experts review more contentious cases with an ambiguity - aware AI . Supported ( * ) H1b Ambiguity - aware AI does not reduce experts’ overall eﬃciency . Supported H1c Ambiguity - aware AI improves experts’ overall label accuracy . Not supported H1d Relevance of ambiguity explanations predicts expert accuracy . Supported ( * * * ) Q2 : Perception ? H2a Experts generally prefer an ambiguity - aware AI . Not supported H2b Experts consider an ambiguity - aware AI more trustworthy . Supported for Integrity ( * ) H2c Ambiguity - aware AI does not inrease cognitive load . Supported H2d Ambiguity tolerance predicts preference for an ambiguity - aware AI . Not supported Asynchronous workﬂows for group deliberation are suitable for expert tasks as they allow potentially busy labelers to complete their review activities on their own schedule . For fast - paced crowdsourcing marketplaces where human labelers may only be available for a very limited amount of time , synchronous ( real - time ) deliberation workﬂows may be more eﬀective . Structuring arguments during deliberation procedures does not only help resolve dis - agreements more eﬃciently , but can also produce useful metadata that can be parsed by machines to understand why disagreements arise and why disagreements persist after group deliberation . Sources of ambiguity are diverse and can diﬀer by task type . Human labelers may arrive at conﬂicting interpretations for a variety of reasons , including diﬀerences in personal background , diﬀerences in viewer settings , characteristics of the data at hand and vagueness 131 in the labeling guidelines . For factors that can be controlled ( e . g . , viewer settings ) it may be desirable to keep those consistent for all labelers or to store information about the speciﬁc conﬁgurations labelers make . Training of human labelers can be enriched with discussion data from group deliberation processes . In particular , less experienced human labelers can improve their comprehension , labeling style and accuracy for diﬃcult - to - classify cases by reading deliberation discussions from more experienced human labelers . Model Development : While this dissertation does not make explicit contributions to the space of model development , we propose high - level recommendations for future modeling approaches seeking to incorporate the notion of ambiguity . Uncertainty and ambiguity are not identical . Uncertainty is characterized by a lack of information needed to make accurate judgements about the current or future state of the world . Ambiguity on the other hand is characterized by an inherent openness to multiple interpretations about the same phenomenon . Both uncertainty and ambiguity should be incorporated into model development . Yet , we recommend they be treated as separate concepts . The likelihood of ambiguity may be treated as a prediction target in model development . However , ambiguous examples may be rare compared to non - ambiguous examples in a given training set . We therefore recommend methods to address potential class imbalance when modeling the likelihood of ambiguity . Sources of ambiguity may be extracted from deliberation data implementing a struc - tured argument format . These may be treated as auxiliary prediction targets for models with strong explainability requirements . User Interfaces : Communication between human users and AI models is both fa - cilitated and constrained by UI design considerations . Designers should carefully decide whether and how ambiguity - related information is exposed to end users to strike the right balance for an eﬃcient , eﬀective and trusted interaction . User trust can be promoted by highlighting and explaining instances of ambiguity as predicted by an AI model . However , factors like user - speciﬁc or context - speciﬁc ambiguity tolerance may aﬀect whether exposing ambiguity information contributes to the establish - ment or erosion of user trust . Triaging is the process of prioritizing and allocating human intervention in resource - limited settings . The predicted likelihood of ambiguity for a given case can be a useful 132 criterion for experts to choose which AI - suggestions to review ﬁrst and thus save time and cognitive resources on clear - cut cases . Ambiguity explanations should be human - interpretable , case - speciﬁc and accurate . An irrelevant or inaccurate ambiguity explanation may signiﬁcantly harm a user’s ability to disambiguate a case correctly . If accurate ambiguity explanations cannot be reliably pro - duced they should not be exposed to the end user . The impact of ambiguity is a factor that should determine whether users are exposed to ambiguity information . For a resourceful and eﬃcient interaction , we recommend that users should not be exposed to ambiguity information for cases where diﬀerent interpretations cannot yield diﬀerent decision outcomes . For example , if diﬀerent interpretations of a medical image will not aﬀect the ﬁnal treatment decision for a given patient , doctors should not be informed about the underlying ambiguity to be economical with their cognitive resources . 6 . 4 Opportunities for Future Work In the previous three chapters , we discussed design implications , research limitations and directions for future with a speciﬁc focus on the research scope for each particular study . In this section , we broaden the scope and address additional directions for future research in the space of human - AI collaborative decision - making for complex , ill - deﬁned and am - biguous problems . Systems to label data of variable ambiguity : So far , the research presented in Chapters 3 and 4 has produced individual building blocks within the bigger picture of how interactive labeling can be done in the presence of ambiguous data . The future holds the potential to connect these separate components into one integrated system capable of intelligent adaptation when confronted with data of variable ambiguity . I am excited to pursue this research direction centered around the following questions : How can we leverage group deliberation to fuel automatic creation and reﬁnement of labeling guidelines ? How to evaluate labeler quality in the presence of ambiguity ? How to provide personalized feedback to labelers by triangulating error types , labeling guidelines and group deliberation ? Can we synthesize ambiguous cases as training material for human labelers ? How can we close the loop by leveraging feedback from end users of an AI system to reﬁne the labeling process ? Building ambiguity - aware AI assistants : The research presented in Chapter 5 has paved the way for understanding the potential beneﬁt of communicating ambiguity in AI 133 output . Yet , these preliminary contributions relied on a Wizard - of - Oz approach leveraging a simulated version of an ambiguity - aware AI assistant . Future research holds the potential to explore the open question of how ambiguity - aware AI can and should be implemented . One possible pathway towards implementation of ambiguity - aware AI would be to combine techniques from multi - label learning , label distribution learning and model ex - plainability to develop AI assistants capable of not only recognizing , but also explaining ambiguous data . The large corpus of structured deliberation data for over 15 , 000 cases that we collected within the CrowdEEG platform may lend itself as training input for the purpose of building an initial prototype . However , the availability of ambiguity - related data alone may not be suﬃcient for building ambiguity - aware AI assistants . The research community may also need to develop novel evaluation metrics that allow us to measure the quality of labels and explanations , produced either by humans or machines , in the presence of inherently ambiguous problems . Besides the modeling aspect of this broader question , more research is needed to un - derstand when and how AI assistants should communicate ambiguity to end users : Should an AI selectively communicate ambiguous cases , e . g . , only if they have the potential to im - pact meaningful outcomes like medical treatment decisions ? Should systems adapt to user characteristics like ambiguity tolerance ? What are eﬀective formats to explain ambiguity for diﬀerent data modalities ? Supporting other forms of problem complexity : The research presented in this dissertation intentionally keeps a narrow focus on ambiguous classiﬁcation problems . How - ever , there exist many other reasons why humans may face complexity in decision making and interpreting data . Recognizing this circumstance , future research may aim to enhance the people - AI partnership for problems that are complex for reasons other than ambiguity . For example , problems may be complex because they require the ability to make associa - tions that are poorly understood by the current state of science . Arguably , one of the most complex domains in our modern society is the advancement of science . The progress of scientiﬁc discovery requires creative exploration and rigorous testing of novel hypotheses . Diversity of perspectives and scientiﬁc disagreements are integral to this process . In recent years , AI systems have acquired the somewhat surprising ability to make com - plex data associations previously unknown or thought impossible within scientiﬁc commu - nities . For example , deep neural networks can predict conditions like anemia or the risk of future heart attacks from just a single photo of a patient’s retina . Yet , the immediate ben - eﬁt of these models to the process of scientiﬁc discovery remains limited due the black - box nature of the underlying models . We posit that there is value in exploring methods from Explainable AI ( XAI ) to foster diversity in scientiﬁc reasoning , exploring the following 134 research directions : How can XAI become part of the modern scientiﬁc toolbox ? How can XAI help scientists diversify their process of hypothesis generation ? How can we support productive scientiﬁc discourse through XAI - powered collaborative thinking ? 6 . 5 Summary In this chapter , we summarized our main contributions , outlined how our ﬁndings support the central claims of our thesis statement , provided design recommendations for handling ambiguity in human - AI collaborative workﬂows and concluded with several directions for future work in the space of human - AI interaction for complex , ill - deﬁned and ambiguous problems . The research presented in this work provides a novel perspective on the question of how humans and AI can be eﬀective partners in the presence of ambiguous problems . We have provided a foundation and proposed directions we believe to be useful to enable future research in this space . 135 References [ 1 ] Grading diabetic retinopathy from stereoscopic color fundus photographs – an exten - sion of the modiﬁed Airlie House classiﬁcation . ETDRS report number 10 . Early Treatment Diabetic Retinopathy Study Research Group . Ophthalmology , 98 ( 5 Suppl ) : 786 – 806 , 5 1991 . [ 2 ] Diabetic retinopathy PPP 2014 : standard photographs 2A , 6A , 8A , 2014 . [ 3 ] Ashraf Abdul , Jo Vermeulen , Danding Wang , Brian Y . Lim , and Mohan Kankanhalli . Trends and Trajectories for Explainable , Accountable and Intelligible Systems : An HCI Research Agenda . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18 , pages 1 – 18 , New York , New York , USA , 2018 . ACM Press . [ 4 ] Michael David Abràmoﬀ , Yiyue Lou , Ali Erginay , Warren Clarida , Ryan Amelon , James C Folk , and Meindert Niemeijer . Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integration of Deep Learning . Investigative ophthalmology & visual science , 57 ( 13 ) : 5200 – 5206 , 10 2016 . [ 5 ] Alaa Al Ali , Stephen Hallingham , and Yvonne M . Buys . Workforce supply of eye care providers in Canada : optometrists , ophthalmologists , and subspecialty ophthal - mologists . Canadian Journal of Ophthalmology , 50 ( 6 ) : 422 – 428 , 12 2015 . [ 6 ] American Academy of Ophthalmology . International Clinical Diabetic Retinopathy Disease Severity Scale , Detailed Table , 2010 . [ 7 ] American Academy of Ophthalmology . Diabetic retinopathy PPP - updated 2017 , 2017 . [ 8 ] Paul André , Aniket Kittur , and Steven P Dow . Crowd synthesis : Extracting cate - gories and clusters from complex data . In Proceedings of the 17th ACM conference 136 on Computer supported cooperative work & social computing , pages 989 – 998 . ACM , 2014 . [ 9 ] Lora Aroyo and Chris Welty . The three sides of crowdtruth . Journal of Human Computation , 1 : 31 – 34 , 2014 . [ 10 ] Elham Bagheri , Justin Dauwels , Brian C . Dean , Chad G . Waters , M . Brandon West - over , and Jonathan J . Halford . Interictal epileptiform discharge characteristics un - derlying expert interrater agreement . Clinical Neurophysiology , 128 ( 10 ) : 1994 – 2005 , 10 2017 . [ 11 ] A . Baker , K . Young , J . Potter , and I . Madan . A review of grading systems for evidence - based guidelines produced by medical specialties . Clinical Medicine , 10 ( 4 ) : 358 – 363 , 8 2010 . [ 12 ] Erin P . Balogh , Bryan T . Miller , and John R . Ball , editors . Improving Diagnosis in Health Care . National Academies Press , Washington , D . C . , 12 2015 . [ 13 ] Forrest S Bao , Xin Liu , and Christina Zhang . PyEEG : An Open Source Python Module for EEG / MEG Feature Extraction . Computational Intelligence and Neuro - science , 2011 : 1 – 7 , 2011 . [ 14 ] Irene A Barbazetto . Diabetic Retinopathy : The Masqueraders . Retinal Physician , 7 ( 6 ) , 2010 . [ 15 ] Michael L . Barnett , Dhruv Boddupalli , Shantanu Nundy , and David W . Bates . Com - parative Accuracy of Diagnosis by Collective Intelligence of Multiple Physicians vs Individual Physicians . JAMA Network Open , 2 ( 3 ) : e190096 , 3 2019 . [ 16 ] Andrew Bastawrous and Benjamin D Hennig . The global inverse care law : a distorted map of blindness . British Journal of Ophthalmology , 96 ( 10 ) : 2 – 1358 , 10 2012 . [ 17 ] Abdhish R Bhavsar . Diabetic retinopathy diﬀerential diagnoses , 2019 . [ 18 ] Christopher R . Bilder and Thomas M . Loughin . Testing for Marginal Independence between Two Categorical Variables with Multiple Responses . Biometrics , 60 ( 1 ) : 241 – 248 , 3 2004 . [ 19 ] Brian H . Bornstein and A . Christine Emler . Rationality in medical decision making : a review of the literature on doctors’ decision - making biases . Journal of Evaluation in Clinical Practice , 7 ( 2 ) : 97 – 107 , 5 2001 . 137 [ 20 ] Stanley Budner . Intolerance of ambiguity as a personality variable . Journal of Personality , 30 ( 1 ) : 29 – 50 , 3 1962 . [ 21 ] Carrie J . Cai , Emily Reif , Narayan Hegde , Jason Hipp , Been Kim , Daniel Smilkov , Martin Wattenberg , Fernanda Viegas , Greg S . Corrado , Martin C . Stumpe , and Michael Terry . Human - Centered Tools for Coping with Imperfect Algorithms during Medical Decision - Making . Number 45 , 2 2019 . [ 22 ] Carrie J . Cai , Samantha Winter , David Steiner , Lauren Wilcox , and Michael Terry . " Hello AI " : Uncovering the Onboarding Needs of Medical Practitioners for Human - AI Collaborative Decision - Making . Proceedings of the ACM on Human - Computer Interaction , 3 ( CSCW ) : 1 – 24 , 11 2019 . [ 23 ] Arthur Carvalho and Kate Larson . A Consensual Linear Opinion Pool . In Proceedings of the Twenty - Third International Joint Conference on Artiﬁcial Intelligence , pages 2518 – 2524 , Beijing , China , 2013 . AAAI Press . [ 24 ] Anthony A . Cavallerano and Paul R . Conlin . Teleretinal Imaging to Screen for Diabetic Retinopathy in the Veterans Health Administration . Journal of Diabetes Science and Technology , 2 ( 1 ) : 33 – 39 , 1 2008 . [ 25 ] Joseph Chee Chang , Saleema Amershi , and Ece Kamar . Revolt : Collaborative Crowdsourcing for Labeling Machine Learning Datasets . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17 , pages 2334 – 2346 , New York , New York , USA , 2017 . ACM , ACM Press . [ 26 ] Nancy Chang , Praveen Paritosh , David Huynh , and Collin Baker . Scaling semantic frame annotation . In Proceedings of The 9th Linguistic Annotation Workshop , pages 1 – 10 , 2015 . [ 27 ] Nan - Chen Chen , Margaret Drouhard , Rafal Kocielnik , Jina Suh , and Cecilia R Aragon . Using Machine Learning to Support Qualitative Coding in Social Science : Shifting The Focus to Ambiguity . ACM Transactions on Interactive Intelligent Sys - tems , ( Human - Centered Machine Learning ) , 2018 . [ 28 ] Po - Hsuan Cameron Chen , Yun Liu , and Lily Peng . How to develop machine learning models for healthcare . Nature Materials , 18 ( 5 ) : 410 – 414 , 5 2019 . [ 29 ] Quanze Chen , Jonathan Bragg , Lydia B . Chilton , and Daniel S . Weld . Cicero : Multi - Turn , Contextual Argumentation for Accurate Crowdsourcing . In Proceedings of the 138 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 , pages 1 – 14 , New York , New York , USA , 10 2019 . ACM Press . [ 30 ] L M Chihara and T C Hesterberg . Mathematical Statistics with Resampling and R . Wiley , 2018 . [ 31 ] Jacob Cohen . A Coeﬃcient of Agreement for Nominal Scales . Educational and Psychological Measurement , 20 ( 1 ) : 37 – 46 , 4 1960 . [ 32 ] Robin Cohen , Mike Schaekermann , Sihao Liu , and Michael Cormier . Trusted AI and the Contribution of Trust Modeling in Multiagent Systems . In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems , AAMAS ’19 , pages 1644 – 1648 , Richland , SC , 2019 . International Foundation for Autonomous Agents and Multiagent Systems . [ 33 ] David A . Cook , Jonathan Sherbino , and Steven J . Durning . Management Reasoning - Beyond the Diagnosis . JAMA , 319 ( 22 ) : 2267 , 6 2018 . [ 34 ] Jorge Cuadros and George Bresnick . EyePACS : An Adaptable Telemedicine System for Diabetic Retinopathy Screening . Journal of Diabetes Science and Technology , 3 ( 3 ) : 509 – 516 , 5 2009 . [ 35 ] Norman Dalkey and Olaf Helmer . An Experimental Application of the DELPHI Method to the Use of Experts . Management Science , 9 ( 3 ) : 458 – 467 , 4 1963 . [ 36 ] Heidi Danker - Hopfe , Peter Anderer , Josef Zeitlhofer , Marion Boeck , Hans Dorn , Georg Gruber , Esther Heller , Erna Loretz , Doris Moser , Silvia Parapatics , Bernd Saletu , Andrea Schmidt , and Georg Dorﬀner . Interrater reliability for sleep scoring according to the Rechtschaﬀen & Kales and the new AASM standard . Journal of Sleep Research , 18 ( 1 ) : 74 – 84 , 3 2009 . [ 37 ] Todd Davies and Reid Chandler . Online deliberation design . Democracy in motion : Evaluation the practice and impact of deliberative civic engagement , pages 103 – 131 , 2012 . [ 38 ] Jasmin Diwan , Chinmay Shah , Saurin Sanghavi , and Amit Shah . Comparison of case - based learning and traditional lectures in physiology among ﬁrst year undergraduate medical students . National Journal of Physiology , Pharmacy and Pharmacology , page 1 , 2017 . 139 [ 39 ] Jeﬀ Donahue and Kristen Grauman . Annotator rationales for visual recognition . In 2011 International Conference on Computer Vision , pages 1395 – 1402 . IEEE , 11 2011 . [ 40 ] Tim Dornan , Albert Scherpbier , Nigel King , and Henny Boshuizen . Clinical teach - ers and problem - based learning : a phenomenological study . Medical Education , 39 ( 2 ) : 163 – 170 , 2 2005 . [ 41 ] Shayan Doroudi , Ece Kamar , Emma Brunskill , and Eric Horvitz . Toward a Learn - ing Science for Complex Crowdsourcing Tasks . In Proceedings of the 2016 SIGCHI Conference on Human Factors in Computing Systems - CHI ’16 , pages 2623 – 2634 , New York , New York , USA , 2016 . ACM Press . [ 42 ] Ryan Drapeau , Lydia B . Chilton , Jonathan Bragg , and Daniel S . Weld . MicroTalk : Using Argumentation to Improve Crowdsourcing Accuracy . In Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing ( HCOMP ) , 2016 . [ 43 ] Stephan Dreiseitl and Michael Binder . Do physicians value decision support ? A look at the eﬀect of decision support systems on physician opinion . Artiﬁcial Intelligence in Medicine , 33 ( 1 ) : 25 – 30 , 1 2005 . [ 44 ] Anca Dumitrache , Lora Aroyo , and Chris Welty . Crowdsourcing Ground Truth for Medical Relation Extraction . ACM Transactions on Interactive Intelligent Systems , 8 ( 2 ) : 1 – 20 , 7 2018 . [ 45 ] David Dunning . The Dunning – Kruger Eﬀect . pages 247 – 296 . 2011 . [ 46 ] Upol Ehsan , Pradyumna Tambwekar , Larry Chan , Brent Harrison , and Mark O . Riedl . Automated rationale generation : a technique for explainable AI and its ef - fects on human perceptions . In Proceedings of the 24th International Conference on Intelligent User Interfaces - IUI ’19 , pages 263 – 274 , New York , New York , USA , 2019 . ACM Press . [ 47 ] Andre Esteva , Brett Kuprel , Roberto A . Novoa , Justin Ko , Susan M . Swetter , He - len M . Blau , and Sebastian Thrun . Dermatologist - level classiﬁcation of skin cancer with deep neural networks . Nature , 542 ( 7639 ) : 115 – 118 , 2 2017 . [ 48 ] Elena Filatova . Irony and Sarcasm : Corpus Generation and Analysis Using Crowd - sourcing . In Nicoletta Calzolari , Khalid Choukri , Thierry Declerck , Mehmet Ugur Dogan , Bente Maegaard , Joseph Mariani , Jan Odijk , and Stelios Piperidis , edi - tors , Proceedings of the Eight International Conference on Language Resources and 140 Evaluation - LREC ’12 , pages 392 – 398 . European Language Resources Association ( ELRA ) , 2012 . [ 49 ] Alan D Fleming , Keith A Goatman , Sam Philip , Gordon J Prescott , Peter F Sharp , and John A Olson . Automated grading for diabetic retinopathy : a large - scale audit using arbitration by clinical experts . The British journal of ophthalmology , 94 ( 12 ) : 1606 – 10 , 12 2010 . [ 50 ] Deen G . Freelon , Travis Kriplean , Jonathan Morgan , W . Lance Bennett , and Alan Borning . Facilitating Diverse Political Engagement with the Living Voters Guide . Journal of Information Technology & Politics , 9 ( 3 ) : 279 – 297 , 7 2012 . [ 51 ] Matthew J . Gabel , Norman L . Foster , Judith L . Heidebrink , Roger Higdon , Howard J . Aizenstein , Steven E . Arnold , Nancy R . Barbas , Bradley F . Boeve , James R . Burke , Christopher M . Clark , Steven T . DeKosky , Martin R . Farlow , William J . Jagust , Claudia H . Kawas , Robert A . Koeppe , James B . Leverenz , Anne M . Lipton , Elaine R . Peskind , R . Scott Turner , Kyle B . Womack , and Ed - ward Y . Zamrini . Validation of Consensus Panel Diagnosis in Dementia . Archives of Neurology , 67 ( 12 ) , 12 2010 . [ 52 ] Snehalkumar ( Neil ) S . Gaikwad , Mark Whiting , Karolina Ziulkoski , Alipta Ballav , Aaron Gilbee , Senadhipathige S . Niranga , Vibhor Sehgal , Jasmine Lin , Leonardy Kristianto , Angela Richmond - Fuller , Jeﬀ Regino , Durim Morina , Nalin Chhibber , Dinesh Majeti , Sachin Sharma , Kamila Mananova , Dinesh Dhakal , William Dai , Vic - toria Purynova , Samarth Sandeep , Varshine Chandrakanthan , Tejas Sarma , Adam Ginzberg , Sekandar Matin , Ahmed Nasser , Rohit Nistala , Alexander Stolzoﬀ , Kristy Milland , Vinayak Mathur , Rajan Vaish , Michael S . Bernstein , Catherine Mullings , Shirish Goyal , Dilrukshi Gamage , Christopher Diemert , Mathias Burton , and Sharon Zhou . Boomerang : Rebounding the Consequences of Reputation Feedback on Crowd - sourcing Platforms . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology - UIST ’16 , pages 625 – 637 , New York , New York , USA , 2016 . ACM Press . [ 53 ] Adrian Galdran , M . Meyer , P . Costa , MendonCa , and A . Campilho . Uncertainty - Aware Artery / Vein Classiﬁcation on Retinal Images . In 2019 IEEE 16th Interna - tional Symposium on Biomedical Imaging ( ISBI 2019 ) , pages 556 – 560 . IEEE , 4 2019 . [ 54 ] Sapna Gangaputra , James F Lovato , Larry Hubbard , Matthew D Davis , Barbara A Esser , Walter T Ambrosius , Emily Y Chew , Craig Greven , Letitia H Perdue , Wai T Wong , Audree Condren , Charles P Wilkinson , Elvira Agrón , Sharon Adler , Ronald P 141 Danis , and ACCORD Eye Research Group . Comparison of standardized clinical clas - siﬁcation with fundus photograph grading for the assessment of diabetic retinopathy and diabetic macular edema severity . Retina ( Philadelphia , Pa . ) , 33 ( 7 ) : 1393 – 9 , 2013 . [ 55 ] Luciana Garbayo . Epistemic Considerations on Expert Disagreement , Normative Justiﬁcation , and Inconsistency Regarding Multi - criteria Decision Making . Con - straint Programming and Decision Making , 539 : 35 – 45 , 2014 . [ 56 ] Rishab Gargeya and Theodore Leng . Automated Identiﬁcation of Diabetic Retinopa - thy Using Deep Learning . Ophthalmology , 124 ( 7 ) : 962 – 969 , 2017 . [ 57 ] Xin Geng . Label distribution learning . IEEE Transactions on Knowledge and Data Engineering , 28 ( 7 ) : 1734 – 1748 , 2016 . [ 58 ] Jeﬀrey Alan Golden . Deep Learning Algorithms for Detection of Lymph Node Metastases From Breast Cancer : Helping Artiﬁcial Intelligence Be Seen . JAMA , 318 ( 22 ) : 2184 – 2186 , 2017 . [ 59 ] Gowri Gopalakrishna , Miranda W Langendam , Rob JPM Scholten , Patrick MM Bossuyt , and Mariska MG Leeﬂang . Guidelines for guideline developers : a systematic review of grading systems for medical tests . Implementation Science , 8 ( 1 ) : 78 , 12 2013 . [ 60 ] Nitesh Goyal and Susan R Fussell . Eﬀects of sensemaking translucence on distributed collaborative analysis . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing , pages 288 – 302 . ACM , 2016 . [ 61 ] Cosima Gretton . Trust and Transparency in Machine Learning - Based Clinical Deci - sion Support . pages 279 – 292 . 2018 . [ 62 ] Melody Guan , Varun Gulshan , Andrew Dai , and Geoﬀrey Hinton . Who said what : Modeling individual labelers improves classiﬁcation . In AAAI Conference on Artiﬁ - cial Intelligence , 2018 . [ 63 ] Varun Gulshan , Lily Peng , Marc Coram , Martin C . Stumpe , Derek Wu , Arunacha - lam Narayanaswamy , Subhashini Venugopalan , Kasumi Widner , Tom Madams , Jorge Cuadros , Ramasamy Kim , Rajiv Raman , Philip C . Nelson , Jessica L . Mega , and Dale R . Webster . Development and Validation of a Deep Learning Algorithm for De - tection of Diabetic Retinopathy in Retinal Fundus Photographs . Jama , 304 ( 6 ) : 649 – 656 , 2016 . 142 [ 64 ] Danna Gurari and Kristen Grauman . CrowdVerge : Predicting If People Will Agree on the Answer to a Visual Question . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17 , pages 3511 – 3522 , New York , New York , USA , 2017 . ACM , ACM Press . [ 65 ] Naama Hammel , Mike Schaekermann , Sonia Phene , Carter Dunn , Lily Peng , Dale R Webster , and Rory Sayres . A Study of Feature - based Consensus Formation for Glau - coma Risk Assessment . Investigative Ophthalmology & Visual Science , 60 ( 9 ) : 164 , 2019 . [ 66 ] Sandra G . Hart and Lowell E . Staveland . Development of NASA - TLX ( Task Load Index ) : Results of Empirical and Theoretical Research . pages 139 – 183 . 1988 . [ 67 ] Francis T . Hartman and Andrew Baldwin . Using Technology to Improve Delphi Method . Journal of Computing in Civil Engineering , 9 ( 4 ) : 244 – 249 , 10 1995 . [ 68 ] Mark Hartswood , Rob Procter , Paul Taylor , Lilian Blot , Stuart Anderson , Mark Rounceﬁeld , and Roger Slack . Problems of data mobility and reuse in the provision of computer - based training for screening mammography . In Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI ’12 , page 909 , New York , New York , USA , 2012 . ACM Press . [ 69 ] David W . Hosmer and Stanley Lemesbow . Goodness of ﬁt tests for the multi - ple logistic regression model . Communications in Statistics - Theory and Methods , 9 ( 10 ) : 1043 – 1069 , 1980 . [ 70 ] Conrad Iber , Sonia Ancoli - Israel , Andrew L Cheeson Jr . , and Stuart F Quan . The AASM Manual for the Scoring of Sleep and Associated Events : Rules , Terminology and Technical Speciﬁcations . American Academy of Sleep Medicine , 2007 . [ 71 ] Oana Inel , Lora Aroyo , Chris Welty , and Robert - Jan Sips . Domain - Independent Quality Measures for Crowd Truth Disagreement . In The 12th International Semantic Web Conference ( ISWC2013 ) , 2013 . [ 72 ] Hayley K . Jach and Luke D . Smillie . To fear or ﬂy to the unknown : Tolerance for ambiguity and Big Five personality traits . Journal of Research in Personality , 79 : 67 – 78 , 4 2019 . [ 73 ] Jiun - Yin Jian , Ann M . Bisantz , and Colin G . Drury . Foundations for an Empirically Determined Scale of Trust in Automated Systems . International Journal of Cognitive Ergonomics , 4 ( 1 ) : 53 – 71 , 3 2000 . 143 [ 74 ] Alan M . Jones . Victims of Groupthink : A Psychological Study of Foreign Policy Decisions and Fiascoes . The ANNALS of the American Academy of Political and Social Science , 407 ( 1 ) : 179 – 180 , 5 1973 . [ 75 ] Samed Jukić and Jasmin Kevrić . Majority vote of ensemble machine learning meth - ods for real - time epilepsy prediction applied on eeg pediatric data . TEM Journal , 7 ( 2 ) : 313 , 2018 . [ 76 ] Sanjay Kairam and Jeﬀrey Heer . Parting Crowds : Characterizing Divergent In - terpretations in Crowdsourced Annotation Tasks . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing - CSCW ’16 , pages 1635 – 1646 , New York , New York , USA , 2016 . ACM Press . [ 77 ] Jayashree Kalpathy - Cramer , J . Peter Campbell , Deniz Erdogmus , Peng Tian , Dha - ranish Kedarisetti , Chace Moleta , James D . Reynolds , Kelly Hutcheson , Michael J . Shapiro , Michael X . Repka , Philip Ferrone , Kimberly Drenser , Jason Horowitz , Kemal Sonmez , Ryan Swan , Susan Ostmo , Karyn E . Jonas , R . V . Paul Chan , Michael F . Chiang , Michael F . Chiang , Susan Ostmo , Kemal Sonmez , J . Peter Campbell , R . V . Paul Chan , Karyn Jonas , Jason Horowitz , Osode Coki , Cheryl - Ann Eccles , Leora Sarna , Audina Berrocal , Catherin Negron , Kimberly Denser , Kristi Cumming , Tammy Osentoski , Tammy Check , Mary Zajechowski , Thomas Lee , Evan Kruger , Kathryn McGovern , Charles Simmons , Raghu Murthy , Sharon Galvis , Jerome Rotter , Ida Chen , Xiaohui Li , Kent Taylor , Kaye Roll , Jayashree Kalpathy - Cramer , Deniz Erdogmus , Maria Ana Martinez - Castellanos , Samantha Salinas - Longoria , Rafael Romero , Andrea Arriola , Francisco Olguin - Manriquez , Miroslava Meraz - Gutierrez , Carlos M . Dulanto - Reinoso , and Cristina Montero - Mendoza . Plus disease in retinopathy of prematurity : improving diagnosis by ranking disease sever - ity and using quantitative image analysis . Ophthalmology , 123 ( 11 ) : 2345 – 2351 , 11 2016 . [ 78 ] Matthew Kay , Tara Kola , Jessica R . Hullman , and Sean A . Munson . When ( ish ) is My Bus ? : User - centered Visualizations of Uncertainty in Everyday , Mobile Predictive Systems . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI ’16 , pages 5092 – 5103 , New York , New York , USA , 2016 . ACM Press . [ 79 ] Gideon Keren and Léonie E . M . Gerritsen . On the robustness and possible accounts of ambiguity aversion . Acta Psychologica , 103 ( 1 - 2 ) : 149 – 172 , 11 1999 . [ 80 ] Sara Kiesler and Lee Sproull . Group decision making and communication technology . Organizational Behavior and Human Decision Processes , 52 ( 1 ) : 96 – 123 , 6 1992 . 144 [ 81 ] Rafal Kocielnik , Saleema Amershi , and Paul N . Bennett . Will You Accept an Imper - fect AI ? : Exploring Designs for Adjusting End - user Expectations of AI Systems . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 , pages 1 – 14 , New York , New York , USA , 2019 . ACM Press . [ 82 ] Jonathan Krause , Varun Gulshan , Ehsan Rahimy , Peter Karth , Kasumi Widner , Greg S . Corrado , Lily Peng , and Dale R . Webster . Grader Variability and the Impor - tance of Reference Standards for Evaluating Machine Learning Models for Diabetic Retinopathy . Ophthalmology , 3 2018 . [ 83 ] Travis Kriplean , Caitlin Bonnar , Alan Borning , Bo Kinney , and Brian Gill . Integrat - ing on - demand fact - checking with public dialogue . In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing - CSCW ’14 , pages 1188 – 1199 , New York , New York , USA , 2014 . ACM Press . [ 84 ] Travis Kriplean , Jonathan Morgan , Deen Freelon , Alan Borning , and Lance Bennett . Supporting reﬂective public thought with considerit . In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work - CSCW ’12 , page 265 , New York , New York , USA , 2012 . ACM Press . [ 85 ] Travis Kriplean , Michael Toomim , Jonathan Morgan , Alan Borning , and Andrew Ko . Is this what you meant ? : promoting listening on the web with reﬂect . In Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI ’12 , page 1559 , New York , New York , USA , 2012 . ACM Press . [ 86 ] Joseph D Kronz , Mark A Silberman , William C Allsbrook , and Jonathan I Epstein . A web - based tutorial improves practicing pathologists’ Gleason grading of images of prostate carcinoma specimens obtained by needle biopsy . Cancer , 89 ( 8 ) : 1818 – 1823 , 10 2000 . [ 87 ] Helen K Li , Larry D Hubbard , Ronald P Danis , Adol Esquivel , Jose F Florez - Arango , Nicola J Ferrier , and Elizabeth A Krupinski . Digital versus ﬁlm Fundus photography for research grading of diabetic retinopathy severity . Investigative ophthalmology & visual science , 51 ( 11 ) : 5846 – 52 , 11 2010 . [ 88 ] P R Lichter . Variability of expert observers in evaluating the optic disc . Transactions of the American Ophthalmological Society , 74 : 532 – 72 , 1976 . [ 89 ] Christopher H Lin , Daniel S Weld , and others . To re ( label ) , or not to re ( label ) . In Second AAAI Conference on Human Computation and Crowdsourcing , 2014 . 145 [ 90 ] J . C Liston and B . J . G Dall . Can the NHS Breast Screening Programme Aﬀord not to Double Read Screening Mammograms ? Clinical Radiology , 58 ( 6 ) : 474 – 477 , 6 2003 . [ 91 ] Weichen Liu , Sijia Xiao , Jacob T Browne , Ming Yang , and Steven P Dow . ConsensUs : Supporting Multi - Criteria Group Decisions by Visualizing Points of Disagreement . ACM Transactions on Social Computing , 1 ( 1 ) : 4 : 1 – 4 : 26 , 1 2018 . [ 92 ] Vera P . Luther and Sonia J . Crandall . Commentary : Ambiguity and Uncertainty : Neglected Elements of Medical Education Curricula ? Academic Medicine , 86 ( 7 ) : 799 – 800 , 7 2011 . [ 93 ] V K Chaithanya Manam and Alexander J Quinn . WingIt : Eﬃcient Reﬁnement of Unclear Task Instructions . In The Sixth AAAI Conference on Human Computation and Crowdsourcing , number HCOMP , pages 108 – 116 , 2018 . [ 94 ] Peter McCullagh and John Nelder . Generalized Linear Models . Chapman & Hall / CRC , 2 edition , 1989 . [ 95 ] Nora McDonald , Sarita Schoenebeck , and Andrea Forte . Reliability and Inter - rater Reliability in Qualitative Research : Norms and Guidelines for CSCW and HCI Prac - tice . Proceedings of the ACM on Human - Computer Interaction , 3 ( CSCW ) : 1 – 23 , 11 2019 . [ 96 ] Brent Mittelstadt , Chris Russell , and Sandra Wachter . Explaining Explanations in AI . In Proceedings of the Conference on Fairness , Accountability , and Transparency - FAT * ’19 , pages 279 – 288 , New York , New York , USA , 11 2019 . ACM Press . [ 97 ] Jeryl L . Mumpower and Thomas R . Stewart . Expert Judgement and Expert Dis - agreement . Thinking & Reasoning , 2 ( 2 - 3 ) : 191 – 212 , 7 1996 . [ 98 ] Joaquin Navajas , Tamara Niella , Gerry Garbulsky , Bahador Bahrami , and Mariano Sigman . Aggregated knowledge from a small number of debates outperforms the wisdom of large crowds . Nature Human Behaviour , 1 2018 . [ 99 ] Charlan Nemeth . Interactions Between Jurors as a Function of Majority vs . Una - nimity Decision Rules . Journal of Applied Social Psychology , 7 ( 1 ) : 38 – 56 , 3 1977 . [ 100 ] Geoﬀrey R . Norman , Lawrence E . M . Grierson , Jonathan Sherbino , Stanley J . Ham - stra , Henk G . Schmidt , and Silvia Mamede . Expertise in Medicine and Surgery . In The Cambridge Handbook of Expertise and Expert Performance , pages 331 – 355 . Cambridge University Press , 2018 . 146 [ 101 ] J . A . Osheroﬀ , J . M . Teich , B . Middleton , E . B . Steen , A . Wright , and D . E . Det - mer . A Roadmap for National Action on Clinical Decision Support . Journal of the American Medical Informatics Association , 14 ( 2 ) : 141 – 145 , 3 2007 . [ 102 ] Gerhard Osius and Dieter Rojek . Normal Goodness - of - Fit Tests for Multinomial Models with Large Degrees of Freedom . Journal of the American Statistical Associ - ation , 87 ( 420 ) : 1145 – 1152 , 12 1992 . [ 103 ] Susannah BF Paletz , Joel Chan , and Christian D Schunn . Uncovering uncertainty through disagreement . Applied Cognitive Psychology , 30 ( 3 ) : 387 – 400 , 2016 . [ 104 ] Shengying Pan , Kate Larson , Joshua Bradshaw , and Edith Law . Dynamic Task Allocation Algorithm for Hiring Workers that Learn . In Proceedings of the 25th International Joint Conference on Artiﬁcial Intelligence ( IJCAI 2016 ) , pages 3825 – 3831 , New York , 2016 . [ 105 ] Matthew P Pase , Jayandra J Himali , Natalie A Grima , Alexa S Beiser , Claudia L Satizabal , Hugo J Aparicio , Robert J Thomas , Daniel J Gottlieb , Sandford H Auer - bach , and Sudha Seshadri . Sleep architecture and the risk of incident dementia in the community . Neurology , 89 ( 12 ) : 1244 – 1250 , 2017 . [ 106 ] Thomas Penzel , Xiaozhe Zhang , and Ingo Fietze . Inter - scorer reliability between sleep centers can teach us what to improve in the scoring rules . Journal of Clinical Sleep Medicine , 9 ( 1 ) : 81 – 87 , 2013 . [ 107 ] Anh T Pham , Raviv Raich , and Xiaoli Z Fern . Dynamic programming for instance annotation in multi - instance multi - label learning . IEEE transactions on pattern anal - ysis and machine intelligence , 39 ( 12 ) : 2381 – 2394 , 2017 . [ 108 ] Sonia Phene , R . Carter Dunn , Naama Hammel , Yun Liu , Jonathan Krause , Naho Kitade , Mike Schaekermann , Rory Sayres , Derek J . Wu , Ashish Bora , Christopher Semturs , Anita Misra , Abigail E . Huang , Arielle Spitze , Felipe A . Medeiros , April Y . Maa , Monica Gandhi , Greg S . Corrado , Lily Peng , and Dale R . Webster . Deep Learning and Glaucoma Specialists : The Relative Importance of Optic Disc Features to Predict Glaucoma Referral in Fundus Photographs . Ophthalmology , 9 2019 . [ 109 ] T Postmes and M Lea . Social processes and group decision making : anonymity in group decision support systems . Ergonomics , 43 ( 8 ) : 1252 – 74 , 8 2000 . [ 110 ] Ronald B Postuma , Alex Iranzo , Michele Hu , Birgit Högl , Bradley F Boeve , Raﬀaele Manni , Wolfgang H Oertel , Isabelle Arnulf , Luigi Ferini - Strambi , Monica Puligheddu , 147 and others . Risk and predictors of dementia and parkinsonism in idiopathic REM sleep behaviour disorder : a multicentre study . Brain , 142 ( 3 ) : 744 – 759 , 2019 . [ 111 ] Stefan Räbiger , Gizem Gezici , Yücel Saygın , and Myra Spiliopoulou . Predicting worker disagreement for more eﬀective crowd labeling . In 2018 IEEE 5th Interna - tional Conference on Data Science and Advanced Analytics ( DSAA ) , pages 179 – 188 . IEEE , 2018 . [ 112 ] Stefan Räbiger , Gizem Gezici , Myra Spliliopoulou , and Yücel Sayg \ in . Predicting worker disagreement for more eﬀective crowd labeling . In 2018 IEEE 5th Interna - tional Conference on Data Science and Advanced Analytics ( DSAA ) . IEEE , 2018 . [ 113 ] Emilee Rader , Kelley Cotter , and Janghee Cho . Explanations as Mechanisms for Supporting Algorithmic Transparency . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18 , pages 1 – 13 , New York , New York , USA , 2018 . ACM Press . [ 114 ] Maithra Raghu , Katy Blumer , Rory Sayres , Ziad Obermeyer , Robert Kleinberg , Sendhil Mullainathan , and Jon Kleinberg . Direct Uncertainty Prediction for Medical Second Opinions . 7 2018 . [ 115 ] Pranav Rajpurkar , Awni Y . Hannun , Masoumeh Haghpanahi , Codie Bourn , and Andrew Y . Ng . Cardiologist - Level Arrhythmia Detection with Convolutional Neural Networks . 7 2017 . [ 116 ] Paisan Raumviboonsuk , Jonathan Krause , Peranut Chotcomwongse , Rory Sayres , Rajiv Raman , Kasumi Widner , Bilson J . L . Campana , Sonia Phene , Korn - wipa Hemarat , Mongkol Tadarati , Sukhum Silpa - Archa , Jirawut Limwattanay - ingyong , Chetan Rao , Oscar Kuruvilla , Jesse Jung , Jeﬀrey Tan , Surapong Or - prayoon , Chawawat Kangwanwongpaisan , Ramase Sukumalpaiboon , Chainarong Luengchaichawang , Jitumporn Fuangkaew , Pipat Kongsap , Lamyong Chualinpha , Sarawuth Saree , Srirut Kawinpanitan , Korntip Mitvongsa , Siriporn Lawanasakol , Chaiyasit Thepchatri , Lalita Wongpichedchai , Greg S . Corrado , Lily Peng , and Dale R . Webster . Deep learning versus human graders for classifying diabetic retinopathy severity in a nationwide screening program . npj Digital Medicine , 2 ( 1 ) : 25 , 12 2019 . [ 117 ] D A Redelmeier and E Shaﬁr . Medical decision making in situations that oﬀer multiple alternatives . JAMA , 273 ( 4 ) : 302 – 5 , 1 1995 . 148 [ 118 ] Richard S . Rosenberg and Steven van Hout . The American Academy of Sleep Medicine Inter - scorer Reliability Program : Sleep Stage Scoring . Journal of Clini - cal Sleep Medicine , 1 2013 . [ 119 ] Paisan Ruamviboonsuk , Khemawan Teerasuwanajak , Montip Tiensuwan , Kanok - wan Yuttitham , and Thai Screening for Diabetic Retinopathy Study Group . In - terobserver agreement in the interpretation of single - ﬁeld digital fundus images for diabetic retinopathy screening . Ophthalmology , 113 ( 5 ) : 826 – 32 , 5 2006 . [ 120 ] Harold Sackman . Delphi assessment : Expert opinion , forecasting , and group process . Technical report , RAND CORP SANTA MONICA CA , 1974 . [ 121 ] Elaheh Sanoubari , Stela H . Seo , Diljot Garcha , James E . Young , and Veronica Loureiro - Rodriguez . Good Robot Design or Machiavellian ? An In - the - Wild Robot Leveraging Minimal Knowledge of Passersby’s Culture . In 2019 14th ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) , pages 382 – 391 . IEEE , 3 2019 . [ 122 ] Rory Sayres , Ankur Taly , Ehsan Rahimy , Katy Blumer , David Coz , Naama Hammel , Jonathan Krause , Arunachalam Narayanaswamy , Zahra Rastegar , Derek Wu , Shawn Xu , Scott Barb , Anthony Joseph , Michael Shumski , Jesse Smith , Arjun B . Sood , Greg S . Corrado , Lily Peng , and Dale R . Webster . Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy . Ophthalmology , 126 ( 4 ) : 552 – 564 , 4 2019 . [ 123 ] Mike Schaekermann , Graeme Beaton , Minahz Habib , Andrew Lim , Kate Larson , and Edith Law . Capturing Expert Arguments from Medical Adjudication Discussions in a Machine - readable Format . In Companion Proceedings of The 2019 World Wide Web Conference - WWW ’19 , volume 2 , pages 1131 – 1137 , New York , New York , USA , 2019 . ACM Press . [ 124 ] Mike Schaekermann , Graeme Beaton , Elaheh Sanoubari , Andrew Lim , Kate Larson , and Edith Law . Ambiguity - aware AI Assistants for Medical Data Analysis . In Pro - ceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20 , Honolulu , HI , USA , 2020 . ACM Press . [ 125 ] Mike Schaekermann , Joslin Goh , Kate Larson , and Edith Law . Resolvable vs . Irre - solvable Disagreement : A Study on Worker Deliberation in Crowd Work . In Pro - ceedings of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW 2018 ) , volume 2 , pages 1 – 19 , New York City , NY , 11 2018 . 149 [ 126 ] Mike Schaekermann , Naama Hammel , Brian Basham , Bilson Campana , Edith Law , Lily Peng , Dale R Webster , and Rory Sayres . Asynchronous Remote Adjudication for Grading Diabetic Retinopathy . Investigative Ophthalmology & Visual Science , 60 ( 9 ) : 158 , 2019 . [ 127 ] Mike Schaekermann , Naama Hammel , Michael Terry , Tayyeba K . Ali , Yun Liu , Brian Basham , Bilson Campana , William Chen , Xiang Ji , Jonathan Krause , Greg S . Cor - rado , Lily Peng , Dale R . Webster , Edith Law , and Rory Sayres . Remote Tool - Based Adjudication for Grading Diabetic Retinopathy . Translational Vision Science & Technology , 8 ( 6 ) : 40 , 12 2019 . [ 128 ] Mike Schaekermann , Edith Law , Kate Larson , and Andrew Lim . Expert Disagree - ment in Sequential Labeling : A Case Study on Adjudication in Medical Time Series Analysis . In 1st Workshop on Subjectivity , Ambiguity and Disagreement in Crowd - sourcing at HCOMP 2018 , Zurich , Switzerland , 2018 . [ 129 ] Mike Schaekermann , Edith Law , Alex C Williams , and William Callaghan . Re - solvable vs . Irresolvable Ambiguity : A New Hybrid Framework for Dealing with Uncertain Ground Truth . In 1st Workshop on Human - Centered Machine Learning at SIGCHI 2016 , San Jose , CA , 2016 . [ 130 ] Hanna Schneider , Julia Wayrauther , Mariam Hassib , and Andreas Butz . Communi - cating Uncertainty in Fertility Prognosis . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 , pages 1 – 11 , New York , New York , USA , 2019 . ACM Press . [ 131 ] Craig R . Scott . The impact of physical and discursive anonymity on group mem - bers’ multiple identiﬁcations during computer - supported decision making . Western Journal of Communication , 63 ( 4 ) : 456 – 487 , 12 1999 . [ 132 ] Ingrid U Scott , Neil M Bressler , Susan B Bressler , David J Browning , Clement K Chan , Ronald P Danis , Matthew D Davis , Craig Kollman , Haijing Qin , and Di - abetic Retinopathy Clinical Research Network Study Group . Agreement between clinician and reading center gradings of diabetic retinopathy severity level at base - line in a phase 2 study of intravitreal bevacizumab for diabetic macular edema . Retina ( Philadelphia , Pa . ) , 28 ( 1 ) : 36 – 40 , 1 2008 . [ 133 ] Manali Sharma , Di Zhuang , and Mustafa Bilgic . Active Learning with Rationales for Text Classiﬁcation . In Proceedings of the Annual Conference of the North Amer - ican Chapter of the Association for Computational Linguistics : Human Language Technologies ( NAACL HLT ) , 2015 . 150 [ 134 ] Lili Shi , Huiqun Wu , Jiancheng Dong , Kui Jiang , Xiting Lu , and Jian Shi . Telemedicine for detecting diabetic retinopathy : a systematic review and meta - analysis . British Journal of Ophthalmology , 99 ( 6 ) : 823 – 831 , 6 2015 . [ 135 ] Miriam Solomon . Groupthink versus The Wisdom of Crowds : The Social Epistemol - ogy of Deliberation and Dissent . The Southern Journal of Philosophy , 44 ( S1 ) : 28 – 42 , 3 2006 . [ 136 ] Miriam Solomon . The social epistemology of NIH consensus conferences . In Estab - lishing medical reality , pages 167 – 177 . Springer , 2007 . [ 137 ] Malathi Srinivasan , Michael Wilkes , Frazier Stevenson , Thuan Nguyen , and Stuart Slavin . Comparing Problem - Based Learning with Case - Based Learning : Eﬀects of a Major Curricular Shift at Two Institutions . Academic Medicine , 82 ( 1 ) : 74 – 82 , 1 2007 . [ 138 ] Jens B . Stephansen , Alexander N . Olesen , Mads Olsen , Aditya Ambati , Eileen B . Leary , Hyatt E . Moore , Oscar Carrillo , Ling Lin , Fang Han , Han Yan , Yun L . Sun , Yves Dauvilliers , Sabine Scholz , Lucie Barateau , Birgit Hogl , Ambra Stefani , Se - ung Chul Hong , Tae Won Kim , Fabio Pizza , Giuseppe Plazzi , Stefano Vandi , Elena Antelmi , Dimitri Perrin , Samuel T . Kuna , Paula K . Schweitzer , Clete Kushida , Paul E . Peppard , Helge B . D . Sorensen , Poul Jennum , and Emmanuel Mignot . Neu - ral network analysis of sleep stages enables eﬃcient diagnosis of narcolepsy . Nature Communications , 9 ( 1 ) : 5229 , 12 2018 . [ 139 ] Thérèse A . Stukel . Generalized Logistic Models . Journal of the American Statistical Association , 83 ( 402 ) : 426 – 431 , 6 1988 . [ 140 ] Daniel Shu Wei Ting , Carol Yim - Lui Cheung , Gilbert Lim , Gavin Siew Wei Tan , Nguyen D . Quang , Alfred Gan , Haslina Hamzah , Renata Garcia - Franco , Ian Yew San Yeo , Shu Yen Lee , Edmund Yick Mun Wong , Charumathi Sabanayagam , Mani Baskaran , Farah Ibrahim , Ngiap Chuan Tan , Eric A . Finkelstein , Ecosse L . Lam - oureux , Ian Y . Wong , Neil M . Bressler , Sobha Sivaprasad , Rohit Varma , Jost B . Jonas , Ming Guang He , Ching - Yu Cheng , Gemmy Chui Ming Cheung , Tin Aung , Wynne Hsu , Mong Li Lee , and Tien Yin Wong . Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes . JAMA , 318 ( 22 ) : 2211 , 12 2017 . [ 141 ] Daniel Shu Wei Ting , Gemmy Chui Ming Cheung , and Tien Yin Wong . Diabetic retinopathy : global prevalence , major risk factors , screening practices and public 151 health challenges : a review . Clinical & experimental ophthalmology , 44 ( 4 ) : 260 – 77 , 5 2016 . [ 142 ] Emanuele Trucco , Alfredo Ruggeri , Thomas Karnowski , Luca Giancardo , Edward Chaum , Jean Pierre Hubschman , Bashir Al - Diri , Carol Y Cheung , Damon Wong , Michael Abràmoﬀ , Gilbert Lim , Dinesh Kumar , Philippe Burlina , Neil M Bressler , Herbert F Jelinek , Fabrice Meriaudeau , Gwénolé Quellec , Tom Macgillivray , and Bal Dhillon . Validating retinal fundus image analysis algorithms : issues and a proposal . Investigative ophthalmology & visual science , 54 ( 5 ) : 3546 – 59 , 5 2013 . [ 143 ] Evangelia Tsiga , Efharis Panagopoulou , Nick Sevdalis , Anthony Montgomery , and Alexios Benos . The inﬂuence of time pressure on adherence to guidelines in primary care : an experimental study . BMJ Open , 3 ( 4 ) : e002700 , 4 2013 . [ 144 ] A . Tversky and D . Kahneman . Judgment under Uncertainty : Heuristics and Biases . Science , 185 ( 4157 ) : 1124 – 1131 , 9 1974 . [ 145 ] Anne Marthe van der Bles , Sander van der Linden , Alexandra L . J . Freeman , James Mitchell , Ana B . Galvao , Lisa Zaval , and David J . Spiegelhalter . Communicating un - certainty about facts , numbers and science . Royal Society Open Science , 6 ( 5 ) : 181870 , 5 2019 . [ 146 ] Michael Van Lent , William Fisher , and Michael Mancuso . An explainable artiﬁcial intelligence system for small - unit tactical behavior . In Proceedings of the national conference on artiﬁcial intelligence , pages 900 – 907 . Menlo Park , CA ; Cambridge , MA ; London ; AAAI Press ; MIT Press ; 1999 , 2004 . [ 147 ] Danding Wang , Qian Yang , Ashraf Abdul , and Brian Y . Lim . Designing Theory - Driven User - Centric Explainable AI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 , pages 1 – 15 , New York , New York , USA , 2019 . ACM Press . [ 148 ] Simon C Warby , Sabrina L Wendt , Peter Welinder , Emil G S Munk , Oscar Carrillo , Helge B D Sorensen , Poul Jennum , Paul E Peppard , Pietro Perona , and Emmanuel Mignot . Sleep - spindle detection : crowdsourcing and evaluating performance of ex - perts , non - experts and automated methods . Nature Methods , 11 ( 4 ) : 385 – 392 , 2 2014 . [ 149 ] Jennifer Williams , Fodé Abass Cisse , Mike Schaekermann , Foksuna Sakadi , Nana Ra - hamatou Tassiou , Aissatou Kenda BAH , Abdoul Bachir Djibo Hamani , Andrew Lim , Edward C W Leung , Tadeu A Fantaneau , Tracey Milligan , Vidita Khatri , Daniel 152 Hoch , Manav Vyas , Alice Lam , Gladia Hotan , Joseph Cohen , Edith Law , and Farrah Mateen . Utilizing a wearable smartphone - based EEG for pediatric epilepsy patients in the resource poor environment of Guinea : A prospective study . Neurology , 92 ( 15 Supplement ) , 2019 . [ 150 ] Jennifer A Williams , Fodé Abass Cisse , Mike Schaekermann , Foksouna Sakadi , Nana Rahamatou Tassiou , Gladia C . Hotan , Aissatou Kenda Bah , Abdoul Bachir Djibo Hamani , Andrew Lim , Edward C . W . Leung , Tadeu A . Fantaneanu , Tracey A . Milligan , Vidita Khatri , Daniel B . Hoch , Manav V . Vyas , Alice D . Lam , Joseph M . Cohen , Andre C . Vogel , Edith Law , and Farrah J . Mateen . Smartphone EEG and remote online interpretation for children with epilepsy in the Republic of Guinea : Quality , characteristics , and practice implications . Seizure , 71 : 93 – 99 , 10 2019 . [ 151 ] Ainur Yessenalina , Yejin Choi , and Claire Cardie . Automatically Generating An - notator Rationales to Improve Sentiment Classiﬁcation . In Proceedings of the ACL 2010 Conference Short Papers , ACLShort ’10 , pages 336 – 341 , Stroudsburg , PA , USA , 2010 . Association for Computational Linguistics . [ 152 ] Omar F . Zaidan , Jason Eisner , and Christine D . Piatko . Using " Annotator Ratio - nales " to Improve Machine Learning for Text Categorization . In Proceedings of the Annual Conference of the North American Chapter of the Association for Compu - tational Linguistics : Human Language Technologies ( NAACL HLT ) , pages 260 – 267 , 2007 . [ 153 ] Omar F . Zaidan , Jason Eisner , and Christine D . Piatko . Machine learning with annotator rationales to reduce annotation cost . In Proceedings of the NIPS 2008 Workshop on Cost Sensitive Learning , 2008 . [ 154 ] Amy X . Zhang , Lea Verou , and David Karger . Wikum : Bridging Discussion Forums and Wikis Using Recursive Summarization . In Proceedings of the 2017 ACM Confer - ence on Computer Supported Cooperative Work and Social Computing - CSCW ’17 , pages 2082 – 2096 , New York , New York , USA , 2017 . ACM Press . 153 APPENDICES 154 Appendix A Group Deliberation for Data Labeling This appendix includes the complete set of questionnaires used in Chapter 3 . A . 1 Crowd Deliberation for Text Labeling This appendix includes the complete set of questionnaires used in Section 3 . 1 . A . 1 . 1 Pre - study Questionnaire 1 . How old are you ? ( multiple choice ) ◦ 18 - 25 ◦ 26 - 35 ◦ 36 - 45 ◦ 46 - 55 ◦ 56 + 2 . What is your gender ? ( multiple choice ) ◦ Female ◦ Male ◦ Other : 155 ◦ Prefer not to say 3 . Is English your ﬁrst language ? ( multiple choice ) ◦ Yes ◦ No 4 . Is English your ﬁrst language ? ( 5 - point Likert scale ) ◦ 1 - Very Poor ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Very Good A . 1 . 2 Per - case Questionnaire after Independent Classiﬁcation 1 . Do you think other people might choose a diﬀerent answer than you did ? ( multiple choice ) ◦ I expect most people to agree with me . [ end of survey ] ◦ I expect only about half of the people to agree with me . ◦ I expect most people to disagree with me . 2 . Why do you think other people might choose a diﬀerent answer ? ( checkboxes ) ◦ Other people may have diﬀerent deﬁnitions of [ sarcasm / relation ] in mind . ◦ The text is ambiguous because of missing context ( for example , [ the identity of the product / some important information about the person or the place ] is unknown ) . ◦ The text contains some features that indicate [ sarcasm / relation ] is expressed and other features that indicate [ absence of sarcasm / relation is not expressed ] . ◦ The text contains relevant details other people could easily miss . ◦ Someone with more experience or expertise may see or understand something about the text that I don’t . 156 ◦ This is a case where a person’s answer would depend heavily on their personal preferences and taste . ◦ Other : 3 . Please elaborate on your answer to the previous question , explaining why you think other people might choose a diﬀerent answer : 4 . If there were other people who chose a diﬀerent answer than you did , do you think a group discussion would help to resolve the case ? ( multiple choice ) ◦ Yes , a group discussion would help to resolve the case . ◦ No , a group discussion would not help to resolve the case . A . 1 . 3 Per - case Questionnaire after Discussion Round 2 1 . Based on your deliberation , why do you think the other people in the group chose a diﬀerent answer ? ( checkboxes ) ◦ [ Free - form reason from previous survey if the participant submitted any ] ◦ Other people may have diﬀerent deﬁnitions of [ sarcasm / relation ] in mind . ◦ The text is ambiguous because of missing context ( for example , [ the identity of the product / some important information about the person or the place ] is unknown ) . ◦ The text contains some features that indicate [ sarcasm / relation ] is expressed and other features that indicate [ absence of sarcasm / relation is not expressed ] . ◦ The text contains relevant details other people could easily miss . ◦ Someone with more experience or expertise may see or understand something about the text that I don’t . ◦ This is a case where a person’s answer would depend heavily on their personal preferences and taste . ◦ Other : 2 . Please elaborate on your answer to the previous question ( for example , if you changed your mind about the source of disagreement , please explain why ) : 157 A . 1 . 4 Per - case Questionnaire after Viewing Final Decisions 1 . Why do you think this case [ could be / could not be fully ] resolved ? 2 . Did somebody make you doubt your original answer ? Why or why not ? ( multiple choice ) ◦ Yes : ◦ No : 3 . Did somebody make you change your original answer ? Why or why not ? ( multiple choice ) ◦ Yes : ◦ No : 4 . Did you manage to convince someone to change their answer or conﬁdence level ? Why do you think you were able / unable to convince them ? ( multiple choice ) ◦ Yes : ◦ No : 5 . Describe how you feel about the deliberation process : 6 . Describe how you feel about the deliberation outcome : A . 2 Expert Deliberation for Time Series Labeling This appendix includes the complete set of questionnaires used in Section 3 . 3 . A . 2 . 1 Pre - study Questionnaire 1 . How old are you ? ( multiple choice ) ◦ 18 - 25 ◦ 26 - 35 ◦ 36 - 45 158 ◦ 46 - 55 ◦ 56 + 2 . What is your gender ? ( multiple choice ) ◦ Female ◦ Male ◦ Other : ◦ Prefer not to say 3 . Where are you located ? ( multiple choice ) ◦ Canada ◦ United States ◦ European Union ◦ Other : 4 . Do you have any professional or academic training in sleep staging ? ( multiple choice ) ◦ Yes ◦ No 5 . If yes , please specify which kind of training , degree or certiﬁcate you hold : 6 . How long have you worked in sleep ? ( multiple choice ) ◦ Not at all ◦ < 3 months ◦ 3 - 6 months ◦ 6 - 12 months ◦ 1 - 2 years ◦ 2 - 3 years ◦ 3 - 5 years ◦ 5 - 10 years ◦ 10 + years 159 A . 2 . 2 Post - study Questionnaire 1 . What computer did you use to complete our study ? Please list the brand / model , and year , if you know this information ( e . g . , laptop “Macbook” from 2013 ; or desktop “Dell” from 2015 ) : 2 . What web browser did you use when running our study ? ( e . g . , Chrome , Firefox , Edge , Opera , etc . ) : 3 . How much would you agree with the following statements ? ( each item answered on a 5 - point Likert scale ranging from 1 “Strongly Disagree” to 5 “Strongly Agree” ) ◦ “The adjudication process was useful for generating a reliable hypnogram . ” ◦ “The ﬁnal adjudicated hypnogram can be trusted more than the hypnogram from my ﬁrst pass . ” 4 . What could be improved about the adjudication interface ? 5 . What were the good parts about the adjudication interface ? 6 . What could be improved about the adjudication procedure ? 7 . What were the good parts about the adjudication procedure ? 160 Appendix B Deliberation Data for Labeler Training This appendix includes the complete set of questionnaires used in Chapter 4 . B . 1 Pre - study Questionnaire 1 . How good do you think you are at grading diabetic retinopathy ( DR ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 2 . How good do you think you are at detecting the presence or absence of hypertensive retinopathy ( HTNR ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 161 3 . How good do you think you are at detecting the presence or absence of retinal vein occlusion ( RVO ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 4 . How good do you think you are at detecting the presence or absence of retinal artery occlusion ( RAO ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 5 . What is your training background ? ( multiple choice ) ◦ Optometrist ◦ General Ophthalmologist ◦ Retina Specialist ◦ Other : 6 . How many years has it been since you ﬁnished residency or since you graduated from optometry school ? 7 . Approximately how many DR images have you graded ? ( multiple choice ) ◦ less than 100 ◦ 100 to 500 ◦ more than 500 8 . How many years of experience do you have grading diabetic retinopathy ( DR ) images ? 162 B . 2 Per - case Questionnaire after Training Feedback 1 . Which of your responses diﬀered from the answer key ( if any ) ? ( checkboxes ) ◦ Diabetic Retinopathy ( DR ) ◦ Hypertensive Retinopathy ( HTNR ) ◦ Retinal Vein Occlusion ( RVO ) ◦ Retinal Artery Occlusion ( RAO ) 2 . After reviewing the answer sheet for this case , do you understand the rationale behind the answer key and could explain it to one of your colleagues ? ( 5 - point Likert scale ) ◦ 1 - Strongly Disagree ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Strongly Agree 3 . After reviewing the answer sheet for this case , do you agree with the answer key for all prompts ? ( multiple choice ) ◦ No , I strongly disagree with at least some of the answer key prompts . ◦ No , but I think all the answers in the answer key are reasonable . ◦ Yes , I would change all my answers to match the answer key . 4 . Please explain in your own words why ( or not ) you agree with the answer key for all prompts : 5 . After reviewing the answer sheet for this case , is there anything you would change about how you grade in the future ? ( multiple choice ) ◦ No ◦ Yes 6 . If yes , please explain what you would change and why . Otherwise , please explain why you would not change anything : 7 . Is there anything else you are still confused about after reviewing the case details ? 163 B . 3 Post - study Questionnaire 1 . How good do you think you are at grading diabetic retinopathy ( DR ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 2 . How good do you think you are at detecting the presence or absence of hypertensive retinopathy ( HTNR ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 3 . How good do you think you are at detecting the presence or absence of retinal vein occlusion ( RVO ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 4 . How good do you think you are at detecting the presence or absence of retinal artery occlusion ( RAO ) ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 164 ◦ 4 ◦ 5 - Extremely good 5 . How mentally demanding was the overall task ? ( 5 - point Likert scale ) ◦ 1 - Not at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely 6 . How physically demanding was the overall task ? ( 5 - point Likert scale ) ◦ 1 - Not at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely 7 . How hurried or rushed was the pace of the overall task ? ( 5 - point Likert scale ) ◦ 1 - Not at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely 8 . How successful were you in accomplishing what you were asked to do ? ( 5 - point Likert scale ) ◦ 1 - Perfect ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Failure 165 9 . How hard did you have to work to accomplish your level of performance ? ( 5 - point Likert scale ) ◦ 1 - Not at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely 10 . How insecure , discouraged , irritated , stressed , and annoyed were you ? ( 5 - point Likert scale ) ◦ 1 - Not at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely 11 . The personalized feedback provided in this study was useful overall . ( 5 - point Likert scale ) ◦ 1 - Strongly Disagree ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Strongly Agree 166 Appendix C Deliberation Data for Ambiguity - aware AI This appendix includes the complete set of questionnaires used in Chapter 5 . C . 1 Pre - study Questionnaire 1 . How old are you ? ( multiple choice ) ◦ 18 - 25 ◦ 26 - 35 ◦ 36 - 45 ◦ 46 - 55 ◦ 56 + 2 . What is your gender ? ( multiple choice ) ◦ Female ◦ Male ◦ Other : ◦ Prefer not to say 3 . Where are you located ? ( multiple choice ) 167 ◦ Canada ◦ United States ◦ European Union ◦ Other : 4 . Do you have any professional or academic training in sleep staging ? ( multiple choice ) ◦ Yes ◦ No 5 . If yes , please specify which kind of training , degree or certiﬁcate you hold : 6 . How long have you worked in sleep ? ( multiple choice ) ◦ Not at all ◦ < 3 months ◦ 3 - 6 months ◦ 6 - 12 months ◦ 1 - 2 years ◦ 2 - 3 years ◦ 3 - 5 years ◦ 5 - 10 years ◦ 10 + years 7 . How often do you think two independent sleep experts tend to disagree on the correct sleep stage for a given polysomnography ( PSG ) epoch ? On average , I think , two independent experts will : ( multiple choice ) ◦ disagree on less than 1 % of the cases ◦ disagree on 2 % - 10 % of the cases ◦ disagree on 11 % - 20 % of the cases ◦ disagree on 21 % - 30 % of the cases ◦ disagree on 31 % - 40 % of the cases ◦ disagree on 41 % - 50 % of the cases 168 ◦ disagree on more than half of the cases 8 . How good do you think you are at sleep stage classiﬁcation ? ( 5 - point Likert scale ) ◦ 1 - Not good at all ◦ 2 ◦ 3 ◦ 4 ◦ 5 - Extremely good 9 . How strongly do you agree with the following statements ? ( each item answered on a 7 - point Likert scale ranging from 1 “Strongly Disagree” to 5 “Strongly Agree” ) ◦ “An expert who doesn’t come up with a deﬁnite answer probably doesn’t know too much . ” ◦ “There is really no such things as a problem that can’t be solved . ” ◦ “People who insist upon a yes or no answer just don’t know how complicated things really are . ” ◦ “Many of our most important decisions are based on insuﬃcient information . ” C . 2 Post - condition Questionnaire 1 . How strongly do you agree with the following statements ? ( each item answered on a 5 - point Likert scale ranging from 1 “Strongly Disagree” to 5 “Strongly Agree” ) ◦ “The AI Assistant supported my interpretation of the sleep recording . ” ◦ “The AI Assistant helped me think through diﬀerent options to interpret the sleep recording and organize my thoughts . ” ◦ “I would continue using the AI assistant in practice . ” 2 . How strongly do you agree with the following statements ? ( each item answered on a 5 - point Likert scale ranging from 1 “Strongly Disagree” to 5 “Strongly Agree” ) ◦ “The AI Assistant was deceptive . ” ◦ “The AI Assistant behaved in an underhanded manner . ” 169 ◦ “I was suspicious of the AI Assistant’s intent , action , or outputs . ” ◦ “I was wary of the AI Assistant . ” ◦ “The AI Assistant’s actions will have a harmful or injurious outcome . ” ◦ “I was conﬁdent in the AI Assistant . ” ◦ “The AI Assistant provided security . ” ◦ “The AI Assistant had integrity . ” ◦ “The AI Assistant was dependable . ” ◦ “The AI Assistant was reliable . ” ◦ “I can trust the AI Assistant . ” ◦ “I am familiar with the AI Assistant . ” 3 . Please tell us more about how your experience with this task : ( each item answered on a 5 - point Likert scale ranging from 1 “Not at all” to 5 “Extremely” ) ◦ How mentally demanding was the task ? ◦ How physically demanding was the task ? ◦ How hurried or rushed was the pace of the task ? ◦ How successful were you at accomplishing what you were asked to do ? ◦ How hard did you have to work to accomplish your level of performance ? ◦ How insecure , discouraged , irritated , stressed and annoyed were you ? 4 . How did you decide which epochs to review and why ? 5 . Which information provided by the AI Assistant did you use to decide which epochs to review and why ? 6 . How did information about the AI Assistant’s uncertainty aﬀect your decision mak - ing ? C . 3 Post - study Questionnaire 1 . Please compare AI Assistant A ( the one you interacted with ﬁrst , right after the playground task ) and AI Assistant B ( the one you interacted with last ) : ( each item 170 answered on a 7 - point Likert scale ranging from 1 “Totally Assistant A” , 2 “Much more Assistant A than B” , 3 “Slightly more Assistant A than B” , 4 “Neutral” , etc . to 7 “Totally Assistant B” ) ◦ Which of the two AI Assistants was more reliable ? ◦ Which of the two AI Assistants was more trustworthy ? ◦ Which of the two AI Assistants was more capable ? ◦ Which of the two AI Assistants did you prefer overall ? 2 . Is there anything else you would like to tell us before completing the study ? 171