To Tune or Not to Tune ? Adapting Pretrained Representations to Diverse Tasks Matthew E . Peters 1 ∗ , Sebastian Ruder 2 , 3 †∗ , and Noah A . Smith 1 , 4 1 Allen Institute for Artiﬁcial Intelligence , Seattle , USA 2 Insight Research Centre , National University of Ireland , Galway , Ireland 3 Aylien Ltd . , Dublin , Ireland 4 Paul G . Allen School of CSE , University of Washington , Seattle , USA { matthewp , noah } @ allenai . org , sebastian @ ruder . io Abstract While most previous work has focused on dif - ferent pretraining objectives and architectures for transfer learning , we ask how to best adapt the pretrained model to a given target task . We focus on the two most common forms of adaptation , feature extraction ( where the pre - trained weights are frozen ) , and directly ﬁne - tuning the pretrained model . Our empirical re - sults across diverse NLP tasks with two state - of - the - art models show that the relative perfor - mance of ﬁne - tuning vs . feature extraction de - pends on the similarity of the pretraining and target tasks . We explore possible explanations for this ﬁnding and provide a set of adaptation guidelines for the NLP practitioner . 1 Introduction Sequential inductive transfer learning ( Pan and Yang , 2010 ; Ruder , 2019 ) consists of two stages : pretraining , in which the model learns a general - purpose representation of inputs , and adaptation , in which the representation is transferred to a new task . Most previous work in NLP has focused on pretraining objectives for learning word or sen - tence representations ( Mikolov et al . , 2013 ; Kiros et al . , 2015 ) . Few works , however , have focused on the adap - tation phase . There are two main paradigms for adaptation : feature extraction and ﬁne - tuning . In feature extraction ( ) the model’s weights are ‘frozen’ and the pretrained representations are used in a downstream model similar to classic feature - based approaches ( Koehn et al . , 2003 ) . Al - ternatively , a pretrained model’s parameters can be unfrozen and ﬁne - tuned ( ) on a new task ( Dai and Le , 2015 ) . Both have beneﬁts : enables use of task - speciﬁc model architectures and may be (cid:63) The ﬁrst two authors contributed equally . † Sebastian is now afﬁliated with DeepMind . Conditions Guidelines Pretrain Adapt . Task Any Any Add many task parameters Any Any Add minimal task parameters Hyper - parameters Any Any Seq . / clas . and have similar performance ELMo Any Sent . pair use BERT Any Sent . pair use Table 1 : This paper’s guidelines for using feature extraction ( ) and ﬁne - tuning ( ) with ELMo and BERT . Seq . : sequence labeling . Clas . : classiﬁcation . Sent . pair : sentence pair tasks . computationally cheaper as features only need to be computed once . On the other hand , is conve - nient as it may allow us to adapt a general - purpose representation to many different tasks . Gaining a better understanding of the adapta - tion phase is key in making the most use out of pretrained representations . To this end , we com - pare two state - of - the - art pretrained models , ELMo ( Peters et al . , 2018 ) and BERT ( Devlin et al . , 2018 ) using both and across seven diverse tasks including named entity recognition , natural language inference ( NLI ) , and paraphrase detec - tion . We seek to characterize the conditions under which one approach substantially outperforms the other , and whether it is dependent on the pretrain - ing objective or target task . We ﬁnd that and have comparable performance in most cases , except when the source and target tasks are ei - ther highly similar or highly dissimilar . We fur - thermore shed light on the practical challenges of adaptation and provide a set of guidelines to the NLP practitioner , as summarized in Table 1 . 2 Pretraining and Adaptation In this work , we focus on pretraining tasks that seek to induce universal representations suitable for any downstream task . Word representations Pretrained word vectors ( Turian et al . , 2010 ; Pennington et al . , 2014 ) have been an essential component in state - of - the - art NLP systems . Word representations are often ﬁxed and fed into a task speciﬁc model ( ) , al - though can provide improvements ( Kim , 2014 ) . Recently , contextual word representations learned supervisedly ( e . g . , through MT ; McCann et al . , 2017 ) or unsupervisedly ( typically through lan - guage modeling ; Peters et al . , 2018 ) have signif - icantly improved over noncontextual vectors . Sentence embedding methods Such methods learn sentence representations via different pre - training objectives such as previous / next sentence prediction ( Kiros et al . , 2015 ; Logeswaran and Lee , 2018 ) , NLI ( Conneau et al . , 2017 ) , or a com - bination of objectives ( Subramanian et al . , 2018 ) . During the adaptation phase , the sentence repre - sentation is typically provided as input to a linear classiﬁer ( ) . LM pretraining with has also been successfully applied to sentence - level tasks . Howard and Ruder ( 2018 , ULMFiT ) propose tech - niques for ﬁne - tuning a LM , including triangu - lar learning rate schedules and discriminative ﬁne - tuning , which uses lower learning rates for lower layers . Radford et al . ( 2018 ) extend LM - to ad - ditional sentence and sentence - pair tasks . Masked LM and next - sentence prediction BERT ( Devlin et al . , 2018 ) combines both word and sentence representations ( via masked LM and next sentence prediction objectives ) in a single very large pretrained transformer ( Vaswani et al . , 2017 ) . It is adapted to both word and sentence level tasks by with task - speciﬁc layers . 3 Experimental Setup We compare ELMo and BERT as representatives of the two best - performing pretraining settings . This section provides an overview of our methods ; see the supplement for full details . 3 . 1 Target Tasks and Datasets We evaluate on a diverse set of target tasks : named entity recognition ( NER ) , sentiment analysis ( SA ) , and three sentence pair tasks , natural language in - ference ( NLI ) , paraphrase detection ( PD ) , and se - mantic textual similarity ( STS ) . NER We use the CoNLL 2003 dataset ( Sang and Meulder , 2003 ) , which provides token level an - notations of newswire across four different entity types ( PER , LOC , ORG , MISC ) . SA We use the binary version of the Stan - ford Sentiment Treebank ( SST - 2 ; Socher et al . , 2013 ) , providing sentiment labels ( negative or positive ) for sentences of movie reviews . NLI We use both the broad - domain MultiNLI dataset ( Williams et al . , 2018 ) and Sentences Involving Compositional Knowledge ( SICK - E ; Marelli et al . , 2014 ) . PD For paraphrase detection ( i . e . , decide whether two sentences are semantically equiva - lent ) , we use the Microsoft Research Paraphrase Corpus ( MRPC ; Dolan and Brockett , 2005 ) . STS We employ the Semantic Textual Similarity Benchmark ( STS - B ; Cer et al . , 2017 ) and SICK - R ( Marelli et al . , 2014 ) . Both datasets provide a similarity value from 1 to 5 for each sentence pair . 3 . 2 Adaptation We now describe how we adapt ELMo and BERT to these tasks . For we require a task - speciﬁc architecture , while for we need a task - speciﬁc output layer . For fair comparison , we conduct an extensive hyper - parameter search for each task . Feature extraction ( ) For both ELMo and BERT , we extract contextual representations of the words from all layers . During adaptation , we learn a linear weighted combination of the layers ( Pe - ters et al . , 2018 ) which is used as input to a task - speciﬁc model . When extracting features , it is im - portant to expose the internal layers as they typi - cally encode the most transferable representations . For SA , we employ a bi - attentive classiﬁcation network ( McCann et al . , 2017 ) . For the sentence pair tasks , we use the ESIM model ( Chen et al . , 2017 ) . For NER , we use a BiLSTM with a CRF layer ( Lafferty et al . , 2001 ; Lample et al . , 2016 ) . Fine - tuning ( ) : ELMo We max - pool over the LM states and add a softmax layer for text classi - ﬁcation . For the sentence pair tasks , we compute cross - sentence bi - attention between the LM states ( Chen et al . , 2017 ) , apply a pooling operation , then add a softmax layer . For NER , we add a CRF layer on top of the LSTM states . Fine - tuning ( ) : BERT We feed the sentence representation into a softmax layer for text classi - ﬁcation and sentence pair tasks following Devlin Pretraining Adaptation NER SA Nat . lang . inference Semantic textual similarity CoNLL 2003 SST - 2 MNLI SICK - E SICK - R MRPC STS - B Skip - thoughts - 81 . 8 62 . 9 - 86 . 6 75 . 8 71 . 8 ELMo 91 . 7 91 . 8 79 . 6 86 . 3 86 . 1 76 . 0 75 . 9 91 . 9 91 . 2 76 . 4 83 . 3 83 . 3 74 . 7 75 . 5 ∆ = - 0 . 2 - 0 . 6 - 3 . 2 - 3 . 3 - 2 . 8 - 1 . 3 - 0 . 4 BERT - base 92 . 2 93 . 0 84 . 6 84 . 8 86 . 4 78 . 1 82 . 9 92 . 4 93 . 5 84 . 6 85 . 8 88 . 7 84 . 8 87 . 1 ∆ = - 0 . 2 0 . 5 0 . 0 1 . 0 2 . 3 6 . 7 4 . 2 Table 2 : Test set performance of feature extraction ( ) and ﬁne - tuning ( ) approaches for ELMo and BERT - base compared to one sentence embedding method . Settings that are good for are colored in red ( ∆ = - > 1 . 0 ) ; settings good for are colored in blue ( ∆ = - < - 1 . 0 ) . Numbers for baseline methods are from respective papers , except for SST - 2 , MNLI , and STS - B results , which are from Wang et al . ( 2018 ) . BERT ﬁne - tuning results ( except on SICK ) are from Devlin et al . ( 2018 ) . The metric varies across tasks ( higher is always better ) : accuracy for SST - 2 , SICK - E , and MRPC ; matched accuracy for MultiNLI ; Pearson correlation for STS - B and SICK - R ; and span F 1 for CoNLL 2003 . For CoNLL 2003 , we report the mean with ﬁve seeds ; standard deviation is about 0 . 2 % . et al . ( 2018 ) . For NER , we extract the representa - tion of the ﬁrst word piece for each token and add a softmax layer . 4 Results We show results in Table 2 comparing ELMo and BERT for both and approaches across the seven tasks against with Skip - thoughts ( Kiros et al . , 2015 ) , which employs a next - sentence pre - diction objective similar to BERT . Both ELMo and BERT outperform the sentence embedding method signiﬁcantly , except on the se - mantic textual similarity tasks ( STS ) where Skip - thoughts is similar to ELMo . The overall perfor - mance of and shows small differences except for a few notable cases . For ELMo , we ﬁnd the largest differences for sentence pair tasks where consistently outperforms . For BERT , we ob - tain nearly the opposite result : signiﬁcantly out - performs on all STS tasks , with much smaller differences for the others . Discussion Past work in NLP ( Mou et al . , 2016 ) showed that similar pretraining tasks transfer bet - ter . 1 In computer vision ( CV ) , Yosinski et al . ( 2014 ) similarly found that the transferability of features decreases as the distance between the pre - training and target task increases . In this vein , Skip - thoughts—and Quick - thoughts ( Logeswaran and Lee , 2018 ) , which has similar performance— which use a next - sentence prediction objective 1 Mou et al . ( 2016 ) , however , only investigate transfer be - tween classiﬁcation tasks ( NLI → SICK - E / MRPC ) . similar to BERT , perform particularly well on STS tasks , indicating a close alignment between the pretraining and target task . This strong alignment also seems to be the reason for BERT’s strong rel - ative performance on these tasks . In CV , generally outperforms when trans - ferring from ImageNet supervised classiﬁcation pretraining to other classiﬁcation tasks ( Kornblith et al . , 2018 ) . Recent results suggest is less use - ful for more distant target tasks such as semantic segmentation ( He et al . , 2018 ) . This is in line with our results , which show strong performance with between closely aligned tasks ( next - sentence prediction in BERT and STS tasks ) and poor per - formance for more distant tasks ( LM in ELMo and sentence pair tasks ) . Confounding factors may be the suitability of the inductive bias of the model architecture for sentence pair tasks and ’s poten - tially increased ﬂexibility due to a larger number of parameters , which we will both analyze next . 5 Analyses Modelling pairwise interactions LSTMs con - sider each token sequentially , while Transform - ers can relate each token to every other in each layer ( Vaswani et al . , 2017 ) . This might facilitate with Transformers on sentence pair tasks , on which ELMo - performs comparatively poorly . We additionally compare different ways of en - coding the sentence pair with ELMo and BERT . For ELMo , we compare encoding with and with - out cross - sentence bi - attention in Table 3 . When SICK - E SICK - R STS - B MRPC ELMo - + bi - attn . 83 . 8 84 . 0 80 . 2 77 . 0 w / o bi - attn . 70 . 9 51 . 8 38 . 5 72 . 3 Table 3 : Comparison of ELMO - cross - sentence em - bedding methods on dev . sets of sentence pair tasks . SICK - E SICK - R STS - B MRPC BERT - , joint enc . 85 . 5 86 . 4 88 . 1 83 . 3 separate encoding 81 . 2 86 . 8 86 . 8 81 . 4 Table 4 : Comparison of BERT - cross - sentence em - bedding methods on dev . sets of sentence pair tasks . adapting the ELMo LSTM to a sentence pair task , modeling the sentence interactions by ﬁne - tuning through the bi - attention mechanism provides the best performance . 2 This provides further evidence that the LSTM has difﬁculty modeling the pair - wise interactions during sequential processing— in contrast to a Transformer LM that can be ﬁne - tuned in this manner ( Radford et al . , 2018 ) . For BERT - , we compare joint encoding of the sentence pair with encoding the sentences sepa - rately in Table 4 . The latter reduces performance , which shows that BERT representations encode cross - sentence relationships and are therefore par - ticularly well - suited for sentence pair tasks . Impact of additional parameters We evaluate whether adding parameters is useful for both adap - tation settings on NER . We add a CRF layer ( as used in ) and a BiLSTM with a CRF layer ( as used in ) to both and show results in Table 5 . We ﬁnd that additional parameters are key for , but hurt performance with . 3 In addition , requires gradual unfreezing ( Howard and Ruder , 2018 ) to match performance of feature extraction . ELMo ﬁne - tuning We found ﬁne - tuning the ELMo LSTM to be initially difﬁcult and re - quired careful hyper - parameter tuning . Once tuned for one task , other tasks have similar hyper - parameters . Our best models used slanted trian - gular learning rates and discriminative ﬁne - tuning ( Howard and Ruder , 2018 ) and in some cases gradual unfreezing . 2 This is similar to text classiﬁcation tasks , where we ﬁnd max - pooling to outperform using the ﬁnal hidden state , simi - lar to ( Howard and Ruder , 2018 ) . 3 in fact optimizes a larger number of parameters than , so a reduced expressiveness does not explain why it un - derperforms on dissimilar settings . Model conﬁguration F 1 + BiLSTM + CRF 95 . 5 + CRF 91 . 9 + CRF + gradual unfreeze 95 . 5 + BiLSTM + CRF + gradual unfreeze 95 . 2 + CRF 95 . 1 Table 5 : Comparison of CoNLL 2003 NER develop - ment set performance ( F 1 ) for ELMo for both feature extraction and ﬁne - tuning . All results averaged over ﬁve random seeds . TE GO TR FI SL BERT - 84 . 4 86 . 7 86 . 1 84 . 5 80 . 9 ∆ = - - 1 . 1 - 0 . 2 - 0 . 6 0 . 4 - 0 . 6 JS div 0 . 21 0 . 18 0 . 14 0 . 09 0 . 09 Table 6 : Accuracy of feature extraction ( ) and dif - ference compared to ﬁne - tuning ( ) with BERT - base trained on training data of different MNLI domains and evaluated on corresponding dev sets . TE : telephone . FI : ﬁction . TR : travel . GO : government . SL : slate . Impact of Target Domain Pretrained language model representations are intended to be univer - sal . However , the target domain might still im - pact the adaptation performance . We calculate the Jensen - Shannon divergence based on term distri - butions ( Ruder and Plank , 2017 ) between the do - mains used to train BERT ( books and Wikipedia ) and each MNLI domain . We show results in Ta - ble 6 . We ﬁnd no signiﬁcant correlation . At least for this task , the distance of the source and target domains does not seem to have a major impact on the adaptation performance . Representations at different layers In addi - tion , we are interested how the information in the different layers of the models develops over the course of ﬁne - tuning . We measure this informa - tion in two ways : a ) with diagnostic classiﬁers ( Adi et al . , 2017 ) ; and b ) with mutual information ( MI ; Noshad et al . , 2018 ) . Both methods allow us to associate the hidden activations of our model with a linguistic property . In both cases , we use the mean of the hidden activations of BERT - base 4 of each token / word piece of the sequence ( s ) as 4 We show results for BERT as they are more inspectable due to the model having more layers . Trends for ELMo are similar . the representation . 5 With diagnostic classiﬁers , for each example , we extract the pretrained and ﬁne - tuned represen - tation at each layer as features . We use these features as input to train a logistic regression model ( linear regression for STS - B , which has real - valued outputs ) on the training data of two single sentence ( CoLA 6 and SST - 2 ) and two pair sentence tasks ( MRPC and STS - B ) . We show its performance on the corresponding dev sets in Fig - ure 1 . Figure 1 : Performance of diagnostic classiﬁers trained on pretrained and ﬁne - tuned BERT representations at different layers on the dev sets of the corresponding tasks . For all tasks , diagnostic classiﬁer performance generally is higher in higher layers of the model . Fine - tuning improves the performance of the diag - nostic classiﬁer at every layer . For the single sen - tence classiﬁcation tasks CoLA and SST - 2 , pre - trained performance increases gradually until the last layers . In contrast , for the sentence pair tasks MRPC and STS - B performance is mostly ﬂat after the fourth layer . Relevant information for sentence pair tasks thus does not seem to be concentrated primarily in the upper layers of pretrained repre - sentations , which could explain why ﬁne - tuning is particularly useful in these scenarios . Computing the mutual information with regard to representations of deep neural networks has 5 We observed similar results when using max - pooling or the representation of the ﬁrst token . 6 The Corpus of Linguistic Acceptability ( CoLA ; Warstadt et al . , 2018 ) consists of examples of expert English sentence acceptability judgments drawn from 22 books and journal ar - ticles on linguistic theory . It uses the Matthews correlation coefﬁcient ( Matthews , 1975 ) for evaluation and is available at : nyu - mll . github . io / CoLA only become feasible recently with the develop - ment of more sophisticated MI estimators . In our experiments , we use the state - of - the - art ensem - ble dependency graph estimator ( EDGE ; Noshad et al . , 2018 ) with default hyper - parameter values . As a sanity check , we compute the MI between hidden activations and random labels and random representations and random labels , which yields 0 in every case as we would expect . 7 We show the mutual information I ( H ; Y ) be - tween the pretrained and ﬁne - tuned mean hidden activations H at each layer of BERT and the out - put labels Y on the dev sets of CoLA , SST - 2 , and MRPC in Figure 2 . Figure 2 : The mutual information between ﬁne - tuned and pretrained mean BERT representations at different layers and the labels on the dev set of the corresponding tasks . The MI between pretrained representations and labels is close to 0 across all tasks and layers , ex - cept for SST . In contrast , ﬁne - tuned representa - tions display much higher MI values . The MI for ﬁne - tuned representations rises gradually through the intermediate and last layers for the sentence pair task MRPC , while for the single sentence classiﬁcation tasks , the MI rises sharply in the last layers . Similar to our ﬁndings with diagnostic classiﬁers , knowledge for single sentence classiﬁ - cation tasks thus seems mostly concentrated in the last layers , while pair sentence classiﬁcation tasks gradually build up information in the intermediate and last layers of the model . 6 Conclusion We have empirically analyzed ﬁne - tuning and fea - ture extraction approaches across diverse datasets , ﬁnding that the relative performance depends on the similarity of the pretraining and target tasks . We have explored possible explanations and pro - vided practical recommendations for adapting pre - trained representations to NLP practicioners . 7 For the same settings , we obtain non - zero values with earlier estimators ( Saxe et al . , 2018 ) , which seem to be less reliable for higher numbers of dimensions . References Yossi Adi , Einat Kermany , Yonatan Belinkov , Ofer Lavi , and Yoav Goldberg . 2017 . Fine - grained anal - ysis of sentence embeddings using auxiliary predic - tion tasks . In Proceedings of ICLR . Daniel M . Cer , Mona T . Diab , Eneko Agirre , I˜nigo Lopez - Gazpio , and Lucia Specia . 2017 . Semeval - 2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation . In Proceedings of SemEval . Qian Chen , Xiaodan Zhu , Zhenhua Ling , Si Wei , Hui Jiang , and Diana Inkpen . 2017 . Enhanced LSTM for natural language inference . In Proceedings of ACL . Alexis Conneau , Douwe Kiela , Holger Schwenk , Lo¨ıc Barrault , and Antoine Bordes . 2017 . Supervised learning of universal sentence representations from natural language inference data . In Proceedings of EMNLP . Andrew M . Dai and Quoc V . Le . 2015 . Semi - supervised sequence learning . In NIPS . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In Proceedings of NAACL . William B . Dolan and Chris Brockett . 2005 . Automati - cally constructing a corpus of sentential paraphrases . In Proceedings of the Third International Workshop on Paraphrasing . Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F . Liu , Matthew Peters , Michael Schmitz , and Luke S . Zettlemoyer . 2017 . AllenNLP : A deep semantic natural language processing platform . Kaiming He , Ross Girshick , and Piotr Doll´ar . 2018 . Rethinking ImageNet pre - training . arXiv preprint arXiv : 1811 . 08883 . Jeremy Howard and Sebastian Ruder . 2018 . Universal language model ﬁne - tuning for text classiﬁcation . In Proceedings of ACL . Yoon Kim . 2014 . Convolutional neural networks for sentence classiﬁcation . Proceedings of EMNLP , pages 1746 – 1751 . Diederik P . Kingma and Jimmy Ba . 2015 . Adam : A method for stochastic optimization . In Proceedings of ICLR . Ryan Kiros , Yukun Zhu , Ruslan Salakhutdinov , Richard S . Zemel , Antonio Torralba , Raquel Urta - sun , and Sanja Fidler . 2015 . Skip - thought vectors . In NIPS . Philipp Koehn , Franz Josef Och , and Daniel Marcu . 2003 . Statistical phrase - based translation . In Pro - ceedings of NAACL . Simon Kornblith , Jonathon Shlens , Quoc V Le , and Google Brain . 2018 . Do better ImageNet models transfer better ? arXiv preprint arXiv : 1805 . 08974 . John D . Lafferty , Andrew McCallum , and Fernando Pereira . 2001 . Conditional random ﬁelds : Prob - abilistic models for segmenting and labeling se - quence data . In Proceedings of ICML . Guillaume Lample , Miguel Ballesteros , Sandeep Sub - ramanian , Kazuya Kawakami , and Chris Dyer . 2016 . Neural architectures for named entity recognition . In Proceedings of NAACL - HLT . Lajanugen Logeswaran and Honglak Lee . 2018 . An efﬁcient framework for learning sentence represen - tations . In Proceedings of ICLR . Ilya Loshchilov and Frank Hutter . 2017 . Fixing weight decay regularization in Adam . CoRR , abs / 1711 . 05101 . Marco Marelli , Stefano Menini , Marco Baroni , Luisa Bentivogli , Raffaella Bernardi , Roberto Zamparelli , et al . 2014 . A SICK cure for the evaluation of com - positional distributional semantic models . In Pro - ceedings of LREC . Brian W . Matthews . 1975 . Comparison of the pre - dicted and observed secondary structure of t4 phage lysozyme . Biochimica et Biophysica Acta ( BBA ) - Protein Structure , 405 ( 2 ) : 442 – 451 . Bryan McCann , James Bradbury , Caiming Xiong , and Richard Socher . 2017 . Learned in translation : Con - textualized word vectors . In NIPS . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Cor - rado , and Jeff Dean . 2013 . Distributed representa - tions of words and phrases and their compositional - ity . In NIPS . Lili Mou , Zhao Meng , Rui Yan , Ge Li , Yan Xu , Lu Zhang , and Zhi Jin . 2016 . How transferable are neural networks in NLP applications ? Proceedings EMNLP . Morteza Noshad , Yu Zeng , and Alfred O . Hero III . 2018 . Scalable mutual information estima - tion using dependence graphs . arXiv preprint arXiv : 1801 . 09125 . Sinno Jialin Pan and Qiang Yang . 2010 . A survey on transfer learning . IEEE Transactions on Knowledge and Data Engineering , 22 ( 10 ) : 1345 – 1359 . Jeffrey Pennington , Richard Socher , and Christo - pher D . Manning . 2014 . GloVe : Global vectors for word representation . In Proceedings of EMNLP . Matthew E . Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 2018 . Deep contextualized word rep - resentations . In Proceedings of NAACL - HLT . Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 . Improving language under - standing by generative pre - training . Sebastian Ruder . 2019 . Neural Transfer Learning for Natural Language Processing . Ph . D . thesis , Na - tional University of Ireland , Galway . Sebastian Ruder and Barbara Plank . 2017 . Learning to select data for transfer learning with Bayesian opti - mization . In Proceedings EMNLP . Erik F . Tjong Kim Sang and Fien De Meulder . 2003 . Introduction to the CoNLL - 2003 shared task : Language - independent named entity recognition . In Proceedings of CoNLL . Andrew M Saxe , Yamini Bansal , Joel Dapello , Madhu Advani , Artemy Kolchinsky , Brendan D Tracey , and David D Cox . 2018 . On the Information Bottleneck Theory of Deep Learning . In Proceedings of ICLR 2018 . Richard Socher , Alex Perelygin , Jean Y . Wu , Jason Chuang , Christopher D . Manning , Andrew Y . Ng , and Christopher Potts . 2013 . Recursive deep mod - els for semantic compositionality over a sentiment treebank . In Proceedings of EMNLP . Sandeep Subramanian , Adam Trischler , Yoshua Ben - gio , and Christopher J Pal . 2018 . Learning gen - eral purpose distributed sentence representations via large scale multi - task learning . In Proceedings of ICLR . Joseph P . Turian , Lev - Arie Ratinov , and Yoshua Ben - gio . 2010 . Word representations : A simple and gen - eral method for semi - supervised learning . In Pro - ceedings of ACL . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention is all you need . In NIPS . Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R . Bowman . 2018 . GLUE : A multi - task benchmark and analysis plat - form for natural language understanding . Alex Warstadt , Amanpreet Singh , and Samuel R Bow - man . 2018 . Neural network acceptability judg - ments . arXiv preprint arXiv : 1805 . 12471 . Adina Williams , Nikita Nangia , and Samuel Bowman . 2018 . A broad - coverage challenge corpus for sen - tence understanding through inference . In Proceed - ings of NAACL . Jason Yosinski , Jeff Clune , Yoshua Bengio , and Hod Lipson . 2014 . How transferable are features in deep neural networks ? In NIPS . A Experimental Details For fair comparison , all experiments include ex - tensive hyper - parameter tuning . We tuned the learning rate , dropout ratio , weight decay and number of training epochs . In addition , the ﬁne - tuning experiments also examined the impact of triangular learning rate schedules , gradual un - freezing , and discriminative learning rates . Hyper - parameters were tuned on the development sets and the best setting evaluated on the test sets . All models were optimized with the Adam op - timizer ( Kingma and Ba , 2015 ) with weight decay ﬁx ( Loshchilov and Hutter , 2017 ) . We used the publicly available pretrained ELMo 8 and BERT 9 models in all experiments . For ELMo , we used the original two layer bidi - rectional LM . In the case of BERT , we used the BERT - base model , a 12 layer bidirectional trans - former . We used the English uncased model for all tasks except for NER which used the English cased model . A . 1 Feature Extraction To isolate the effects of ﬁne - tuning contextual word representations , all feature based models only include one type of word representation ( ELMo or BERT ) and do not include any other pretrained word representations . For all tasks , all layers of pretrained represen - tations were weighted together with learned scalar parameters following Peters et al . ( 2018 ) . NER For the NER task , we use a two layer bidi - rectional LSTM in all experiments . For ELMo , the output layer is a CRF , similar to a state - of - the - art NER system ( Lample et al . , 2016 ) . Feature ex - traction for ELMo treated each sentence indepen - dently . In the case of BERT , the output layer is a soft - max to be consistent with the ﬁne - tuned experi - ments presented in Devlin et al . ( 2018 ) . In ad - dition , as in Devlin et al . ( 2018 ) , we used doc - ument context to extract word piece representa - tions . When composing multiple word pieces into a single word representation , we found it beneﬁ - cial to run the biLSTM layers over all word pieces before taking the LSTM states of the ﬁrst word piece in each word . We experimented with other pooling operations to combine word pieces into a 8 https : / / allennlp . org / elmo 9 https : / / github . com / google - research / bert single word representation but they did not provide additional gains . SA We used the implementation of the bi - attentive classiﬁcation network in AllenNLP ( Gardner et al . , 2017 ) with default hyper - parameters , except for tuning those noted above . As in the ﬁne - tuning experiments for SST - 2 , we used all available annotations during training , in - cluding those of sub - trees . Evaluation on the de - velopment and test sets used full sentences . Sentence pair tasks When extracting features from ELMo , each sentence was handled sepa - rately . For BERT , we extracted features for both sentences jointly to be consistent with the pretrain - ing procedure . As reported in Section 5 this im - proved performance over extracting features for each sentence separately . Our model is the ESIM model ( Chen et al . , 2017 ) , modiﬁed as needed to support regression tasks in addition to classiﬁcation . We used de - fault hyper - parameters except for those described above . A . 2 Fine - tuning When ﬁne - tuning ELMo , we found it beneﬁcial to use discriminative learning rates ( Howard and Ruder , 2018 ) where the learning rate decreased by 0 . 4 × in each layer ( so that the learning rate for the second to last layer is 0 . 4 × the learning rate in the top layer ) . In addition , for SST - 2 and NER , we also found it beneﬁcial to gradually unfreeze the weights starting with the top layer . In this setting , in each epoch one additional layer of weights is unfrozen until all weights are training . These set - tings were chosen by tuning development set per - formance . For ﬁne - tuning BERT , we used the default learning rate schedule ( Devlin et al . , 2018 ) that is similar to the schedule used by Howard and Ruder ( 2018 ) . SA We considered several pooling operations for composing the ELMo LSTM states into a vec - tor for prediction including max pooling , average pooling and taking the ﬁrst / last states . Max pool - ing performed slightly better than average pooling on the development set . Sentence pair tasks Our bi - attentive ﬁne - tuning mechanism is similar to the the attention mech - anism in the feature based ESIM model . To ap - ply it , we ﬁrst computed the bi - attention between all words in both sentences , then applied the same “enhanced” pooling operation as in ( Chen et al . , 2017 ) before predicting with a softmax . Note that this attention mechanism and pooling operation does not add any additional parameters to the net - work .