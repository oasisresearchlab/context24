Few - shot Instruction Prompts for Pretrained Language Models to Detect Social Biases Shrimai Prabhumoye 1 , Rafal Kocielnik 2 , Mohammad Shoeybi 1 , Anima Anandkumar 1 , 2 , Bryan Catanzaro 1 1 NVIDIA , 2 California Institute of Technology { sprabhumoye @ nvidia . com , rafalko @ caltech . edu } Abstract Warning : this paper contains content that may be offensive or upsetting . Detecting social bias in text is challenging due to nuance , subjectivity , and difﬁculty in ob - taining good quality labeled datasets at scale , especially given the evolving nature of so - cial biases and society . To address these challenges , we propose a few - shot instruction - based method for prompting pre - trained lan - guage models ( LMs ) . We select a few class - balanced exemplars from a small support repository that are closest to the query to be labeled in the embedding space . We then pro - vide the LM with instruction that consists of this subset of labeled exemplars , the query text to be classiﬁed , a deﬁnition of bias , and prompt it to make a decision . We demon - strate that large LMs used in a few - shot con - text can detect different types of ﬁne - grained biases with similar and sometimes superior ac - curacy to ﬁne - tuned models . We observe that the largest 530B parameter model is signiﬁ - cantly more effective in detecting social bias compared to smaller models ( achieving at least 13 % improvement in AUC metric compared to other models ) . It also maintains a high AUC ( dropping less than 2 % ) when the labeled repository is reduced to as few as 100 samples . Large pretrained language models thus make it easier and quicker to build new bias detectors . 1 Introduction Detecting social bias in text is of utmost importance as stereotypes and biases can be projected through language ( Fiske , 1993 ) . Detecting bias is challeng - ing because it can be expressed through seemingly innocuous statements which are implied and rarely explicit , and the interpretation of bias can be sub - jective leading to noise in labels . In this work , we focus on detecting social bias in text as deﬁned in Sap et al . ( 2020 ) using few - shot instruction - based prompting of pre - trained language models ( LMs ) . Current approaches that detect bias require large labeled datasets to train the models ( Chung et al . , 2019 ; Waseem and Hovy , 2016 ; Zampieri et al . , 2019 ; Davidson et al . , 2017a ) . Collecting such labeled sets is an expensive process and hence they are not easily available . Furthermore , most of the prior work relies on ﬁnetuning ( Sap et al . , 2020 ; Mandl et al . , 2019 ; Zampieri et al . , 2019 ) neural architectures which is costly in case of large LMs ( Strubell et al . , 2019 ) and access to ﬁnetune large LMs may be limited ( Brown et al . , 2020 ) . Prior work on bias detection has not fo - cused on modeling multiple types of biases across datasets as it requires careful optimization to suc - ceed ( Hashimoto et al . , 2017 ; Søgaard and Gold - berg , 2016 ; Ruder , 2017 ) . Finetuning a model can also lead to over - ﬁtting especially in case of smaller train sets and to catastrophic forgetting of knowledge present in the pre - trained model ( Fatemi et al . , 2021 ) . Moreover , ﬁnetuning approaches are prone to be affected by noisy labels ( Song et al . , 2022 ) which is especially an issue with datasets for bias detection . The human labeling used to an - notate these datasets can introduce bias and noisy labels ( Hovy and Prabhumoye , 2021 ) . We harness the knowledge present in large scale pre - trained language models ( Davison et al . , 2019 ; Zhou et al . , 2020 ; Petroni et al . , 2019 ; Zhong et al . , 2021 ; Shin et al . , 2020 ) to detect a rich set of bi - ases . Our method prompts the LM with a textual post and labeled exemplars along with instructions to detect bias in the given post . We explore the capabilities of LMs to ﬂexibly accommodate differ - ent dimensions of bias without any ﬁnetuning and with limited access to labeled samples ( few - shot classiﬁcation ) . Prompt - engineering plays a central role in ﬁnetuning - free approaches ( Liu et al . , 2021b ) . It is the process of creating a prompting function that results in the best performance on the desired down - stream task . Prompt - engineering can be performed a r X i v : 2112 . 07868v2 [ c s . C L ] 15 A p r 2022 Figure 1 : Overview of our approach : We use a sentence encoder to project the query Q and the textual posts x 1 , . . . x N from labeled repository D to the same embedding space . We use cosine similarity metric to select equal number of posts with highest similarity to Q from each class c 1 . . . c n as shots . The tokens of the selected shots and their labels are concatenated along with the deﬁnition of bias and query to ﬁll - in instruction template p , and ﬁnally passed to the pre - trained LM to make a prediction based on conditional token probability for each class . by a human engineer who manually creates the desired prompts using domain expertise and intu - ition . It can also be performed by sophisticated algorithms that search for the best template for the downstream task but this too requires learning of a small number of weights ( ﬁnetuning ) . Our approach : We provide the LM with ex - emplars where semantically similar text is used in both biased and unbiased contexts . This speciﬁ - cally can be useful in identifying implicit biases . To achieve that , we use prompt - engineering ap - proach in combination with a novel method to sam - ple class - balanced exemplars in case of few - shot bias classiﬁcation . We propose the use of sentence similarity metric to sample exemplars instead of uniform ( Gao et al . , 2020 ; Logan IV et al . , 2021 ) or random ( Brown et al . , 2020 ) sampling explored before . As shown in Figure 1 , we ﬁrst utilize sen - tence encoder to project the labeled posts and the query post to the same embedding space . We use a similarity metric to identify posts from labeled repository that are closest in meaning to the query post . We then select an equal number of exemplars from each class label to be provided as context for few - shot classiﬁcation . To summarize , our contributions are as follows : • To our best knowledge , we are the ﬁrst to adopt few - shot instruction based techniques to detect social bias without ﬁnetuning . • We propose a novel approach to select class - balanced exemplars for few - shot classiﬁcation ( § 2 ) • We establish few - shot based benchmarks on eight binary and multi - class classiﬁcation tasks across two datasets , even beating the ﬁne - tuning tech - niques on three tasks ( § 3 . 6 ) . • We demonstrate that our technique maintains per - formance with smaller repository sizes ( less than 2 % AUC point drop in downsizing the labeled repository from 35k to 100 samples ) ( § 4 ) . • Finally , we scale our technique to a large LM with 530B parameters and illustrate that it can achieve at least 13 % AUC improvement com - pared to other models on majority of tasks . Our proposed technique does not require any ad - ditional complex tuning to perform multiple tasks and is ﬂexible to identify a diverse set of biases focusing on coarse - grained ( binary classiﬁcation ) as well as ﬁne - grained ( if the text was targeted or untargeted insult , who is targeted in the text , etc ) tasks . Our experiments show that pretrained lan - guage models are robust against noisy labels . We demonstrate that the models are able to predict the correct label more than a third of the time even when provided with 100 % ﬂipped labels . Addi - tionally , we present ablations to understand the contribution of different semantic components of our method , and an exhaustive qualitative analysis of the LM predictions . 2 Methodology We study the ability of pre - trained LMs to detect implicit bias in text . We also investigate if large LMs are capable of doing so with limited access to labeled samples ( few - shot classiﬁcation ) and with - out any ﬁnetuning . We propose to sample class - balanced exemplars from a labeled repository based on their semantic similarity with the query post . We only provide the LM with a few examples of bias , the textual post to be classiﬁed and a deﬁnition of bias and prompt it to make a decision . We present all our exemplars as Question - Answer pairs in con - text . Our approach consists of syntactic structured component and a semantic content component . The syntactic structure is provided by special tags such as Post , Question and Answer . The semantic con - tent consists of the textual exemplars chosen from the labeled repository and their labels . Formally , we have a textual post Q and a deﬁni - tion of bias d . We want to categorize text Q in C classes . We provide a language model M with Q and d , and we check the probability p M ( c i | Q ; d ) of each class c i ∈ C , where ; denotes concatena - tion of the two strings Q and d . We consider the class with the highest probability to be the predic - tion of M for the given Q and d . For example as shown in Figure 1 , consider the binary classiﬁcation task of predicting if a piece of text is offensive or not . Here , the text Q to be labeled is Newbie here , saw this on twitter , I am trying as I am so tired of con - servatives being blocked and banned . and the deﬁnition d of offensiveness used to annotate the text is Could this post be considered offensive , disrespectful , or toxic to anyone / someone ? . The ﬁnal input to model M would be p = [ “ Post : ” ; Q ; “ Question : ” ; d ; “ Answer : ” ] , where we concatenate the tags Post , Question and Answer . Since it is a binary classiﬁcation task , we calculate the probabilities of the tokens Yes and No in the following manner : p M ( “ Yes ” | p ) and p M ( “ No ” | p ) . The token that has the higher probability is considered the prediction of the task . Similarly , in case of a multi - class classiﬁcation task of predicting if the text contains hate , offense or profanity , the d is What does this post contain - hate , offense , profanity ? We calculate the probabil - ity of the tokens hate , offense and profanity given the input p . In the zero - shot case , only the input p as de - scribed above is provided as input to M . In the few - shot case , we need a labeled repository D where each sample ( x i , c i ) is a tuple of the tex - tual post and the class of the post respectively . We also know the deﬁnition d to be used for classi - ﬁcation . In the k - shot case , k samples are cho - sen from D . The input p to the model M in this case is the concatenation of the following strings p = [ x 1 ; d ; c 1 ; . . . ; x k ; d ; c k ; Q ; d ] . We add the tags Post , Question and Answer for structure . Selection of k - shots In case of few - shot classiﬁ - cation , we select k exemplars from D repository to be provided as context to M . Instead of randomly selecting the k exemplars , we select the samples that are closest in meaning to the text Q which we want to classify . We project Q and all the text samples from D in the same embedding space . We use cosine similarity and select the k exemplars that have the highest cosine similarity scores . We also have an additional constraint of selecting equal number of exemplars from each class C to ensure balanced representation of labels . For example , in case of 32 - shot binary classiﬁcation , we select 16 positive exemplars and 16 negative exemplars that are closest in meaning to Q . 3 Experiments and Results 3 . 1 Datasets We consider two separate datasets and a total of eight bias classiﬁcation tasks . Note that models ﬁnetuned on one dataset would need to be further optimized or ﬁnetuned on the other dataset but this is not the case for our approach . Social Bias Frames ( SBIC ) This dataset ( Sap et al . , 2020 ) contains ﬁne - grained categorization of textual comments to better model the pragmatic frames in which people project social biases and stereotypes onto others . It contains four binary clas - siﬁcation tasks and one multi - class classiﬁcation task ( % age positive samples in test set are shown in brackets ) : ( 1 ) offensive task ( 57 . 8 % pos ) : predict if the text is offensive or not , ( 2 ) intent task ( 53 . 1 % pos ) : predict if the text is an intentional insult or not , ( 3 ) lewd task ( 9 . 6 % pos ) : predict if the text contains lewd language or not , ( 4 ) group task ( 41 . 1 % pos ) : predict if the text is offensive to a group or an individual , and ( 5 ) target group ( WHO ) task : if the text is offensive to a group then identify the group targeted in the text . We design the target group identiﬁcation as a seven - way classiﬁcation task where the target group categories are - body , culture , disabled , gender , race , social , victim . Sap et al . ( 2020 ) treat these ﬁve tasks as a single generative task where the entire frame is generated token by token . We treat them as ﬁve separate classiﬁcation tasks . HASOC This dataset ( Mandl et al . , 2019 ) is re - leased in three Indo - European Languages . We only focus on English tasks . It consists of two binary classiﬁcation tasks and one multi - class classiﬁca - tion task : ( 1 ) HOF task ( 25 . 0 % pos ) : is a coarse - grained task of determining whether a post contains hate , offensive , and profane content ( as one label ) or not , ( 2 ) HOP task : is a ﬁne - grained task that Model Sampling Offensive Intent Lewd Group AUC F1 AUC F1 AUC F1 AUC F1 SC - 78 . 80 - 78 . 60 - 80 . 70 - 69 . 90 KWD - 58 . 31 70 . 94 56 . 94 67 . 17 59 . 83 35 . 39 55 . 50 57 . 16 TF - IDF TF - IDF 63 . 64 72 . 76 65 . 00 69 . 71 56 . 77 21 . 67 65 . 57 59 . 74 Meg - 1 . 3 rnd 55 . 02 64 . 45 55 . 25 51 . 57 49 . 80 0 . 00 52 . 33 28 . 73 Meg - 1 . 3 rnd - 50 ↑ 4 . 69 % 57 . 60 64 . 13 ↑ 5 . 05 % 58 . 04 57 . 79 ↑ 10 . 20 % 54 . 88 19 . 02 ↑ 6 . 34 % 55 . 65 52 . 20 Meg - 1 . 3 TF - IDF ↑ 12 . 92 % 62 . 13 73 . 49 ↑ 14 . 39 % 63 . 20 69 . 04 ↑ 23 . 21 % 61 . 36 22 . 11 ↑ 16 . 87 % 61 . 16 60 . 61 Meg - 8 . 3 rnd 61 . 49 74 . 11 61 . 96 68 . 17 50 . 10 0 . 86 60 . 68 48 . 40 Meg - 8 . 3 rnd - 50 ↑ 5 . 94 % 65 . 14 76 . 20 ↑ 4 . 71 % 64 . 88 73 . 05 ↑ 27 . 09 % 63 . 67 24 . 10 ↑ 4 . 55 % 63 . 44 63 . 01 Meg - 8 . 3 TF - IDF ↑ 7 . 38 % 66 . 03 77 . 53 ↑ 7 . 31 % 66 . 49 74 . 39 ↑ 38 . 26 % 69 . 27 28 . 00 ↑ 7 . 79 % 65 . 41 65 . 31 MT - NLG rnd 75 . 77 79 . 76 72 . 97 75 . 24 51 . 80 7 . 47 71 . 56 64 . 92 MT - NLG rnd - 50 ↑ 0 . 98 % 76 . 51 79 . 49 ↑ 3 . 65 % 75 . 63 78 . 38 ↑ 41 . 58 % 73 . 34 32 . 68 ↑ 4 . 47 % 74 . 76 71 . 69 MT - NLG TF - IDF ↑ 3 . 73 % 78 . 60 82 . 19 ↑ 4 . 99 % 76 . 61 79 . 77 ↑ 51 . 62 % 78 . 54 41 . 06 ↑ 7 . 22 % 76 . 73 73 . 74 Table 1 : Results for the 32 - shot prompting on four binary classiﬁcation tasks offensive , intent , lewd and group from SBIC dataset ( § 3 . 1 ) . The best performance in each task is presented in bold . We show the relative percentage improvement ( ↑ ) in AUC score compared to the rnd sampling . The improvement gained by MT - NLG by using TF - IDF vs . rnd - 50 is less compared to smaller models . considers the type of offense . This is a three - way classiﬁcation task to predict if a post contains hate speech , offensive language or profane content . ( 3 ) Target task ( 85 . 1 % pos ) : entails further categoriz - ing the text as targeted or untargeted insult . Only posts that have a positive label in HOF task are considered for HOP and Target tasks . 3 . 2 Baselines We consider two simple heuristics which can prove to be strong baselines for these tasks due to high correlation of certain keywords with labels . Keyword - based ( KWD ) We use 3 common keyword - based baselines . The LDNOOBW dataset ( Shutterstock , 2013 ) contains 403 banned English words and has been used in prior research ( Salmi - nen et al . , 2019 ; Simonite , 2021 ) . The dataset of bad , offensive and profane words ( von Ahn , 2021 ) contains more than 1300 English terms that could be found offensive or profane . Finally , obscenity and profanity dataset contains more than 1600 pop - ular English keywords for profanities and their vari - ations grouped into 10 categories including sexual acts , sexual orientation , racial / ethnic slurs , reli - gious offense ( SurgeAI , 2021 ) . We assign a positive label to a post if it contains at least one keyword from the list in a given dataset . As these datasets don’t perfectly align with the categories from ( Sap et al . , 2020 ) and ( Mandl et al . , 2019 ) , in Table 1 and 2 we report the metrics for the best performing dataset ( KWD ) . TF - IDF We use Term Frequency - Inverse Doc - ument Frequency ( TF - IDF ) to project the text Q and the posts from D in the common embedding space ( Scikit - learn , 2022b ) . For k - shot classiﬁca - tion , we select k / | C | posts with the highest cosine similarity score from each class i . e we select equal number of posts from each class . We then average the similarity scores for the selected samples from each class . The class that has the highest average score is considered the prediction of the TF - IDF baseline . 3 . 3 Sampling Techniques Random ( rnd ) k exemplars are randomly se - lected from repository D are provided as context to the language models . This sampling is agnostic to the labels of the exemplars selected . Class Balanced Random ( rnd - 50 ) In this tech - nique , we randomly select k class balanced exem - plars from D i . e . in case of binary classiﬁcation , we ensure that k / 2 exemplars are randomly selected from the positive class and k / 2 exemplars are ran - domly selected from the negative class . Similarity Based This technique selects k exem - plars based on their semantic similarity to the query to be classiﬁed Q . As described in Section § 3 . 2 , we use TF - IDF representation to encode Q and ex - emplar text in D . We select k / | C | exemplars with the highest cosine similarity score from each class . 3 . 4 Evaluation Metric Following Sap et al . ( 2020 ) , we use binary F1 score of the positive class for measuring the performance of our models on offensive , intent , lewd , and group tasks . Additionally , we also report area under the Model Sam HOF Target F1m F1w F1m F1w HS 78 . 82 83 . 95 51 . 11 75 . 63 KWD 56 . 88 71 . 68 41 . 31 53 . 17 TF - IDF TF - IDF 51 . 71 60 . 45 45 . 49 63 . 02 Meg - 1 . 3 rnd - 50 52 . 06 61 . 51 40 . 63 50 . 17 Meg - 1 . 3 TF - IDF 52 . 96 60 . 88 45 . 36 57 . 33 Meg - 8 . 3 rnd - 50 58 . 64 65 . 27 45 . 62 59 . 79 Meg - 8 . 3 TF - IDF 58 . 52 63 . 99 51 . 25 67 . 16 MT - NLG rnd - 50 63 . 02 74 . 19 30 . 02 32 . 57 MT - NLG TF - IDF 65 . 81 75 . 22 36 . 27 42 . 48 Table 2 : Results from the 32 - shot prompting on two HASOC binary classiﬁcation tasks ( § 3 . 1 ) . The best per - formance in each task is presented in bold . curve ( AUC ) which measures the ability of a clas - siﬁer to distinguish between classes ( Scikit - learn , 2022a ) . For the WHO task we report the weighted F1 scores and AUC . Similar to Mandl et al . ( 2019 ) , we report F1 - macro ( F1m ) and F1 weighted ( F1w ) for HOF , HOP and Target tasks . 3 . 5 Modeling Details For language model M , we use off - the - shelf pre - trained models . We use Megatron 1 . 3B parame - ter model ( Meg - 1 . 3 ) and Megatron 8 . 3B parame - ter model ( Meg - 8 . 3 ) models pre - trained using the toolkit in Shoeybi et al . ( 2019 ) . To understand the scaling of our technique to larger LMs , we perform experiments with MT - NLG which is a GPT - style 530B parameter model ( Smith et al . , 2022 ) . We use the train sets of SBIC and HASOC as labeled repository D for sampling exemplars for k - shot classiﬁcation . We pre - process the SBIC train set to ensure that the test set does not overlap with the train set . We compute levenshtein distance 1 l between each sentence Q in the test set and labeled repository x i . If the ratio r = 2 ∗ l | Q | ∗ | x i | , is less than 0 . 1 , then we discard the train sentence x i . Here | . | indicates the length of the sentence in terms of number of characters . The ratio r tells us if the train sentence x i can be transformed to test sentence Q by changing less than 10 % of the characters . We use TF - IDF as a baseline . The k shots picked by TF - IDF are also used in experiments with LMs in k - shot classiﬁcation . All the binary classiﬁcation tasks are performed with k = 32 for uniformity . The bias deﬁnitions used for all the tasks are pro - vided in Appendix A . 1 . Model Sam WHO ( 7 - way ) HOP ( 3 - way ) AUC F1w F1m F1w HS - - 54 . 46 72 . 77 TF - IDF TF - IDF 72 . 26 42 . 94 32 . 90 32 . 46 Meg - 1 . 3 rnd - 50 63 . 40 28 . 87 34 . 41 38 . 68 Meg - 1 . 3 TF - IDF 67 . 29 32 . 84 35 . 11 38 . 29 Meg - 8 . 3 rnd - 50 76 . 08 43 . 25 28 . 64 29 . 23 Meg - 8 . 3 TF - IDF 82 . 27 51 . 21 25 . 24 25 . 05 MT - NLG rnd - 50 86 . 67 64 . 82 48 . 02 51 . 54 MT - NLG TF - IDF 88 . 60 67 . 91 46 . 28 48 . 10 Table 3 : Results for the multi - class classiﬁcation tasks from HASOC and SBIC ( § 3 . 1 ) . Due to the number of classes , the WHO classiﬁcation is performed with k = 28 and HOP is performed with k = 3 . The best performance in each task is presented in bold . 3 . 6 Results The main results for the six binary classiﬁcation tasks in the 32 - shot case are shown in Tables 1 and 2 . 2 We show the results from Sap et al . ( 2020 ) ( SC ) and Mandl et al . ( 2019 ) ( HS ) to understand how close our models perform in comparison to ﬁnetuned state - of - the - art models . 3 From these Tables we see that in general as the size of the LM increases , the AUC and F1 perfor - mance for detecting bias improves . We also see that MT - NLG performs the best on both AUC and F1 metrics for all the SBIC tasks and it performs better than ﬁnetuned SC model on three tasks - of - fensive , intent and group . In Table 1 , we see that using class balanced random sampling ( rnd - 50 ) per - forms much better than random sampling . We also observe that similarity based TF - IDF sampling per - forms better than random ( rnd - 50 ) sampling . We note that as the model size increases , the improve - ment gained by using better sampling technique reduces . Concretely , across the four SBIC tasks , the average AUC gain between rnd - 50 and TF - IDF sampling is 9 . 6 % for Meg - 1 . 3 , 3 . 9 % for Meg - 8 . 3 and 3 . 4 % for MT - NLG . This shows that the larger models are robust towards the sampling technique . For HOF task , MT - NLG performs better than baselines on both F1 metrics . For Target task , this is not the case because of the skewed distribution of indicative keywords in this task . We perform an analysis of the percentage of posts that contain key - 1 https : / / pypi . org / project / python - Levenshtein / 2 In § B we show additional analysis with more sampling techniques which can perform better for these bias tasks . But we show in Table 10 and Table 11 that these sampling tech - niques take advantage of the keywords that are used only in one speciﬁc contexts . Hence , they cannot not generalize to other tasks . 3 More details of SC and HS models can be found in § A . 4 Model Sampling AUC F1 Sampling AUC F1 D Size TF - IDF TF - IDF 62 . 74 ± 0 . 00 55 . 97 ± 0 . 00 - - - 35k TF - IDF TF - IDF ↓ 1 . 56 % 61 . 76 ± 0 . 21 55 . 43 ± 0 . 23 - - - 10k TF - IDF TF - IDF ↓ 9 . 26 % 56 . 93 ± 0 . 72 51 . 39 ± 0 . 37 - - - 1k TF - IDF TF - IDF ↓ 10 . 60 % 56 . 09 ± 1 . 19 50 . 38 ± 0 . 53 - - - 100 Meg - 1 . 3 TF - IDF 61 . 96 ± 0 . 00 56 . 31 ± 0 . 00 rnd - 50 56 . 54 ± 0 . 00 48 . 29 ± 0 . 00 35k Meg - 1 . 3 TF - IDF ↓ 0 . 06 % 61 . 92 ± 0 . 30 56 . 34 ± 0 . 17 rnd - 50 56 . 55 ± 0 . 54 48 . 07 ± 0 . 53 10k Meg - 1 . 3 TF - IDF ↓ 3 . 20 % 59 . 98 ± 0 . 13 53 . 61 ± 0 . 10 rnd - 50 57 . 00 ± 0 . 07 48 . 40 ± 0 . 05 1k Meg - 1 . 3 TF - IDF ↓ 5 . 47 % 58 . 57 ± 0 . 38 50 . 30 ± 0 . 20 rnd - 50 56 . 99 ± 0 . 51 47 . 93 ± 1 . 05 100 Meg - 8 . 3 TF - IDF 66 . 80 ± 0 . 00 61 . 31 ± 0 . 00 rnd - 50 64 . 28 ± 0 . 00 59 . 09 ± 0 . 00 35k Meg - 8 . 3 TF - IDF ↑ 1 . 03 % 67 . 49 ± 0 . 01 63 . 20 ± 0 . 04 rnd - 50 64 . 02 ± 0 . 04 58 . 88 ± 0 . 12 10k Meg - 8 . 3 TF - IDF ↓ 1 . 60 % 65 . 73 ± 0 . 22 60 . 61 ± 0 . 19 rnd - 50 64 . 93 ± 0 . 45 59 . 50 ± 0 . 36 1k Meg - 8 . 3 TF - IDF ↓ 2 . 93 % 64 . 84 ± 0 . 41 59 . 92 ± 0 . 24 rnd - 50 64 . 73 ± 0 . 51 59 . 50 ± 0 . 29 100 MT - NLG TF - IDF 77 . 62 ± 0 . 00 69 . 19 ± 0 . 00 rnd - 50 75 . 06 ± 0 . 00 65 . 56 ± 0 . 00 35k MT - NLG TF - IDF ↑ 0 . 0 % 77 . 62 ± 0 . 10 69 . 24 ± 0 . 20 rnd - 50 75 . 31 ± 0 . 28 65 . 82 ± 0 . 20 10k MT - NLG TF - IDF ↓ 0 . 95 % 76 . 88 ± 0 . 31 67 . 47 ± 0 . 28 rnd - 50 75 . 30 ± 0 . 46 65 . 86 ± 0 . 32 1k MT - NLG TF - IDF ↓ 1 . 84 % 76 . 19 ± 0 . 28 66 . 71 ± 0 . 36 rnd - 50 75 . 62 ± 0 . 71 66 . 11 ± 0 . 47 100 Table 4 : Results of the 32 - shot prompting on the four SBIC classiﬁcation tasks with decreasing sizes of labeled repository D . We show the std for AUC and F1 metric on 3 versions of the dataset downsized with different seeds . We show the relative percentage improvement ( ↑ ) or decrements ( ↓ ) in AUC score compared to the 35 k support repository size . The largest model MT - NLG experiences the smallest decrease in performance ( only 1 . 84 % AUC ) . words and their correlation with labels ( details in Appendix A . 2 ) . We note that keywords are present at ﬁve times higher rate in positive posts of the Target task compared to the other tasks . The results for the two multi - class classiﬁcation tasks are shown in Table 3 . For both the tasks we experiment with different values of k ( k = { 7 , 28 } for WHO task and k = { 3 , 12 } for HOP task ) and we present the best performing results . The results for WHO task are shown for k = 28 shots , to ensure equal samples from each of the seven classes . Similarly , the HOP results are shown for k = 3 i . e one exemplar is picked from each of the three classes that is the closest to the textual post Q . For both the multi - class classiﬁcations tasks , MT - NLG performs better than baselines on all metrics illustrating the effectiveness of our approach . We are unable to show the keyword baselines for multi - class tasks because of lack of keyword repositories that clearly align with all the classes . 4 Analysis and Discussion Smaller Size of D We experiment with smaller sizes of repository D to understand how the size of D affects model performance . The goal is to understand the amount of annotated data required by our technique to detect social bias . The original SBIC train set contains ∼ 35 k instances ( used as D in § 3 . 6 ) . We down - sample examples from this dataset to create smaller sets of sizes 10 k , 1 k and 100 . A labeled set D of size 100 means that we can only select the k - shots from 100 samples . We ensure that we have equal label distribution in the down - sampled sets . For each size , we use three different random seeds to generate down - sampled data . We then average the results of the four 32 - shot SBIC classiﬁcation tasks for the sets produced by three random seeds and present the results in Table 4 for both rnd - 50 and TF - IDF sampling . We observe that in case of rnd - 50 sampling , there is practically no change in performance of the lan - guage models with the reduction in support reposi - tory size . The standard deviation for the AUC score across the four D sizes is 0 . 43 for Meg - 1 . 3 , 0 . 54 for Meg - 8 . 3 and 0 . 47 for MT - NLG , which is extremely low . We see that in case of TF - IDF sampling , the performance of the larger models does not degrade substantially when the size of the labeled repository is reduced to as low as 100 samples . For example , the relative percentage AUC drop is only 2 . 93 % for Meg - 8 . 3 and 1 . 84 % for MT - NLG as opposed to 5 . 47 % for the Meg - 1 . 3 model . The performance for the TF - IDF baseline however drops substan - tially by AUC 10 . 6 % . For all the cases , MT - NLG performs better than the baselines on both AUC and F1 metric . For each support repository size , the LMs using TF - IDF based sampling perform better than the corresponding LMs using rnd - 50 sampling . Contribution of Components We have four components in our input to model M : the textual exemplars x i , the deﬁnition d , the class label c i of exemplar x i , and the textual query post Q which we want to classify . We perform ablation studies Model Post Def . Q AUC F1 Meg - 1 . 3 (cid:88) (cid:88) (cid:88) 63 . 20 69 . 04 Meg - 1 . 3 (cid:88) (cid:55) (cid:88) ↓ 3 . 70 % 60 . 96 70 . 99 Meg - 1 . 3 (cid:55) (cid:55) (cid:55) ↓ 19 . 00 % 51 . 27 65 . 03 Meg - 1 . 3 (cid:55) (cid:88) (cid:88) ↓ 20 . 03 % 50 . 62 61 . 27 Meg - 1 . 3 (cid:55) (cid:88) (cid:55) ↓ 22 . 09 % 49 . 32 52 . 32 Meg - 8 . 3 (cid:88) (cid:88) (cid:88) 66 . 49 74 . 39 Meg - 8 . 3 (cid:88) (cid:55) (cid:88) ↑ 1 . 76 % 67 . 66 72 . 81 Meg - 8 . 3 (cid:55) (cid:88) (cid:88) ↓ 20 . 80 % 52 . 66 69 . 27 Meg - 8 . 3 (cid:55) (cid:88) (cid:55) ↓ 22 . 94 % 51 . 24 67 . 05 Meg - 8 . 3 (cid:55) (cid:55) (cid:55) ↓ 23 . 28 % 51 . 01 66 . 69 MT - NLG (cid:88) (cid:88) (cid:88) 76 . 61 79 . 77 MT - NLG (cid:88) (cid:55) (cid:88) ↓ 2 . 43 % 74 . 75 73 . 37 MT - NLG (cid:55) (cid:88) (cid:88) ↓ 18 . 17 % 52 . 69 70 . 44 MT - NLG (cid:55) (cid:88) (cid:55) ↓ 34 . 64 % 50 . 07 65 . 30 MT - NLG (cid:55) (cid:55) (cid:55) ↓ 34 . 51 % 50 . 17 61 . 14 Table 5 : Ablation studies on SBIC Intent task to un - derstand the contribution of each component of the in - struction . Ablations performed with 32 - shots prompt - ing . The highest capacity model MT - NLG achieves best performance with all the instruction components . with x i , d and Q to understand the contribution of each of them in making the ﬁnal prediction . Ex - periments with perturbation of label c i are shown separately in the later section . The results of 32 - shot intent classiﬁcation ablation studies are shown in Table 5 . The AUC performance drops consistently as we remove the deﬁnition d , the text of exemplars x i , and the query Q . For Meg - 8 . 3 and MT - NLG , we see that removing all three has the most impact on the performance . The AUC drops on average by 19 . 67 % across models by only removing the textual exemplars x i ( The model gets class labels c i of exemplars , d and Q as input ) . This suggests that MT - NLG pays attention to all the components and the textual examples are as important as knowing their labels . Absence of deﬁnition has the smallest impact causing only a minor drop in AUC of 3 . 7 % for Meg - 1 . 3 and 2 . 43 % for MT - NLG , but no drop for Meg - 8 . 3 . Robustness to Labels To understand the contri - bution of labels in the k - shot binary classiﬁcation performance , we perform two ablation studies . We ﬂip the labels ( Flip ) of the exemplars i . e . if the ground truth label of exemplar x i is c i = “Yes " , then we supply the label “No " and vice versa . This study is done to understand the robustness of the model to noisy or wrong labels . In the second ex - periment , we ﬂip the labels only 50 % ( Random ) of the times . Hence , 50 % of the time model gets correct label to an exemplar and gets a wrong label Model Sam AUC F1 Experiment Meg - 1 . 3 TF - IDF 42 . 75 49 . 63 Flip Meg - 1 . 3 TF - IDF 52 . 61 59 . 23 Random Meg - 1 . 3 TF - IDF 63 . 20 69 . 04 Correct Meg - 8 . 3 TF - IDF 39 . 26 51 . 37 Flip Meg - 8 . 3 TF - IDF 54 . 02 63 . 88 Random Meg - 8 . 3 TF - IDF 66 . 49 74 . 39 Correct MT - NLG TF - IDF 38 . 10 41 . 80 Flip MT - NLG TF - IDF 61 . 08 65 . 18 Random MT - NLG TF - IDF 76 . 61 79 . 77 Correct Meg - 1 . 3 rnd - 50 44 . 46 36 . 07 Flip Meg - 1 . 3 rnd - 50 50 . 81 47 . 97 Random Meg - 1 . 3 rnd - 50 58 . 04 57 . 79 Correct Meg - 8 . 3 rnd - 50 48 . 45 57 . 18 Flip Meg - 8 . 3 rnd - 50 56 . 57 64 . 31 Random Meg - 8 . 3 rnd - 50 64 . 88 73 . 05 Correct MT - NLG rnd - 50 44 . 68 43 . 45 Flip MT - NLG rnd - 50 63 . 79 66 . 20 Random MT - NLG rnd - 50 75 . 63 78 . 38 Correct Table 6 : Experiment to understand the role of labels in prediction with 32 - shot prompts on SBIC Intent task . The Flip and Random denote reversing the labels or re - placing half of the shots with random label respectively . the other 50 % of time . We show the results on 32 - shot intent classiﬁcation task for both TF - IDF and rnd - 50 sampling in Table 6 . We observe that the AUC and F1 accuracy drops the most when we supply ﬂipped labels . Inter - estingly , even in this case , the language models identify intentional insult more than a third of the time . In case of supplying Random labels , the rel - ative AUC performance of LMs drops on average by 16 % ( similar results shown for general NLP tasks ( Song et al . , 2022 ) ) further showcasing the ro - bustness of the models towards wrong labels . The rnd - 50 sampling is in general more robust to label ﬂips compared to the TF - IDF sampling . For ex - ample , on average for the Random experiment , the AUC performance across LMs drops by 13 . 6 % for rnd - 50 sampling and 18 . 6 % for TF - IDF sampling . TF - IDF picks the shots carefully to augment the LMs ability to classify and hence when wrong la - bels are provided , this sampling technique suffers more loss compared to rnd - 50 . k - shots vs Metric To understand how the perfor - mance of the models scale with number of shots , we present Figure 2 . It shows the graph of AUC metric for the group classiﬁcation task with number of shots ranging from 8 to 96 with a step size of 8 . We observe that for the group task , the perfor - mance improves up to k in range of 32 − 48 and Figure 2 : Experiment with varying number of instruc - tion shots for the Group binary classiﬁcation task from SBIC . Change in AUC scores is plotted against the number of shots k provided as context to the LMs . then plateaus . The best performance of MT - NLG is at k = 48 with AUC score of 76 . 96 which is a 0 . 23 AUC improvement over k = 32 . We would like to note that the k at which the model achieves optimal performance is task dependent . For unifor - mity we choose to report performance at k = 32 for all tasks but we believe that each category can be further improved . Qualitative Analysis We randomly sample 50 cases where MT - NLG makes a wrong prediction and TF - IDF is correct . Similarly , we also sample 50 cases where MT - NLG predicts the correct label and TF - IDF makes correct predictions . All the samples are picked for the offensive classiﬁcation task . Based on manual qualitative coding among two of the authors , we further divide these cases into ﬁve categories . 4 Overall , we observe that for both TF - IDF and MT - NLG , high number of errors occur when there are no keywords 5 present in the query text . In case of offensive posts without keywords , MT - NLG makes 12 % less errors compared to TF - IDF . This shows that TF - IDF ﬁnds it challenging to identify implicit bias in the text . We observe that some posts may contain trigger words such as white nationalist , Black , Jews , Muslims , 9 / 11 , etc . which causes TF - IDF to retrieve shots with offensive keywords from D . We also observe that TF - IDF picks irrelevant offensive shots when enough content on the same topic is not available in D . When such irrelevant posts are used , TF - IDF makes a wrong prediction . When keywords are present in the posts , there are lesser number of errors . Some of the posts are not 4 Example posts for each category are shown in Table 9 in Appendix ( Warning : the examples shown contain content that maybe offensive or upsetting ) . 5 based on LDNOOBW dataset ( Shutterstock , 2013 ) Figure 3 : Error classes from qualitative analysis of dis - agreements between TF - IDF and MT - NLG on Offen - sive task from SBIC . The higher percentage of cor - rect classiﬁcations even in cases of missing explicit keywords by MT - NLG compared to TF - IDF ( red bars ) demonstrates that MT - NLG better captures implicit bias . offensive but contain slurs such as f * * k , b * * * h , etc . Finally , 5 % instances are mislabeled according to the human annotators . 6 Some posts were clearly offensive and were annotated as non - offensive whereas other posts that seem to be non - offensive were marked as offensive suggesting potential am - biguity of interpretation . In these cases , there was not enough context to make a decision , a limitation particularly relevant to social media datasets and identiﬁed in prior work ( Chen et al . , 2018 ) . 5 Related Work Bias Detection Approaches for detecting bias in text can broadly be put into 3 categories : 1 ) keyword - based , 2 ) feature - representation - based , 3 ) supervised - training - based . Keywords - based methods rely on curated lexi - cons of words ( Hatebase . org , 2021 ; Shutterstock , 2013 ; von Ahn , 2021 ) . Despite wide use ( Sood et al . , 2012 ; Mondal et al . , 2017 ; Simonite , 2021 ) they can be at a signiﬁcant mismatch with human ratings ( Davidson et al . , 2017b ) and can’t easily capture phenomenons such as sarcasm , humor ( Ra - jadesingan et al . , 2015 ) or polysemy ( Sahlgren et al . , 2018 ) . Such lexicons require constant up - dates as new slang develops ( Nobata et al . , 2016 ) . Prior work has also explored sophisticated fea - ture representations including n - grams , linguistic and syntactic features ( Nobata et al . , 2016 ) , TF - IDF ( Salminen et al . , 2018 ) , Bag of Words and word embeddings ( Djuric et al . , 2015 ) , as well as content - speciﬁc features such as mentions , proper nouns , named entities , and target group speciﬁc vocabularies ( Waseem et al . , 2017 ) . Topic model - ing approaches such as Labeled Latent Dirichlet Allocation have also been proposed ( Saleem et al . , 6 Note that we do not claim that 5 % of the entire corpus is mislabeled . 2017 ) . Overall , these methods try to provide better feature representations than keywords ( Sahlgren et al . , 2018 ) , but rely on careful feature engineering which might be speciﬁc to particular context and hence makes them inﬂexible across settings . Supervised training based methods rely on large labeled datasets to train models ( Badjatiya et al . , 2017 ; Pavlopoulos et al . , 2017 ; Zhang et al . , 2018 ; D’sa et al . , 2019 ; Caselli et al . , 2020 ; Silva et al . , 2020 ) . Recently , transformers have been ﬁne - tuned for hate - speech detection ( Caselli et al . , 2020 ) , de - tection of targeted offensive language ( Rosenthal et al . , 2021 ) and bias ( Sap et al . , 2020 ) . Simi - lar techniques with focus on toxicity have been adopted in the commercial Perspective API ( Per - spectiveAPI , 2021 ) . These methods are more ef - fective than keywords or custom features ( Bad - jatiya et al . , 2017 ) , but they rely on large labeled datasets and are expensive , or even impossible ( e . g . , Perspective API ) to retrain . Furthermore , most approaches focus on binary coarse - grained hate - speech or toxicity classiﬁcation , rather than on nu - anced and target group speciﬁc issues of bias . Prompting Recent success of large pre - trained language models ( Devlin et al . , 2019 ; Brown et al . , 2020 ; Smith et al . , 2022 ) has opened the ﬁeld to the new direction of prompting ( Liu et al . , 2021b ) them for various NLP tasks . We demonstrate the success of this technique on social bias detection tasks which makes it possible to detect different social biases without a huge labeled set and training separate models for each task . Schick et al . ( 2021 ) self - diagnose toxicity in machine generated text . This is closest to our work . We focus on bias and a more ﬁne - grained understanding of bias such as the target group , intentional or unintentional offense etc . in human written text . Bias in human written text can be more challenging to detect as it can be riddled with sarcasm and humor . Schick et al . ( 2021 ) evaluate the success of large LMs to detect toxicity using the scores provided by automated classiﬁer Perspective API ( PerspectiveAPI , 2021 ) as the ground truth . We evaluate the success of large LMs at detecting bias using human annotated labels as ground truth . Most importantly , Schick et al . ( 2021 ) sample the most toxic and most non - toxic examples from RealToxicity ( Gehman et al . , 2020 ) dataset i . e . they sample the extreme cases of toxicity , and report the performance of large LMs on zero - shot classiﬁcation . We report few - shot classiﬁcation performance on the entire test set of two independent datasets with human - provided labeling . Additionally , we also showcase multi - class classiﬁcation capability of our approach . Liu et al . ( 2021a ) investigate the retrieval of ex - emplars that are semantically - similar to a test sam - ple . The two key differences of our work are : ( 1 ) we focus on few - shot instruction based bias detec - tion , and ( 2 ) we select equal number of exemplars from each class . 6 Conclusion The paper proposes a novel technique to select exemplars for few - shot instruction - based method to detect bias using pretrained model’s internal knowledge and no ﬁne - tuning . On two separate datasets involving binary and multi - class classiﬁca - tion ( ranging from coarse - grained to ﬁne - grained tasks ) , we demonstrate that sufﬁciently large pre - trained LMs can detect bias beating keyword and semantic - based heuristics as well as ﬁne - tuned models on some of the tasks . In subsequent ex - periments we show that our method is : 1 ) ﬂexible in incorporating various bias deﬁnitions , 2 ) robust against small pool of labeled documents to select shots from , 3 ) relies on deeper semantic interpre - tation , rather than surface level heuristics , which we show through extensive ablation studies and manual qualitative inspection . The ethical consid - erations are discussed in Appendix C . References Pinkesh Badjatiya , Shashank Gupta , Manish Gupta , and Vasudeva Varma . 2017 . Deep learning for hate speech detection in tweets . In Proceedings of the 26th international conference on World Wide Web companion , pages 759 – 760 . Tom B . Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M . Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam Mc - Candlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learn - ers . CoRR , abs / 2005 . 14165 . Tommaso Caselli , Valerio Basile , Jelena Mitrovi´c , and Michael Granitzer . 2020 . Hatebert : Retraining bert for abusive language detection in english . arXiv preprint arXiv : 2010 . 12472 . Nan - Chen Chen , Margaret Drouhard , Rafal Kociel - nik , Jina Suh , and Cecilia R Aragon . 2018 . Us - ing machine learning to support qualitative coding in social science : Shifting the focus to ambiguity . ACM Transactions on Interactive Intelligent Systems ( TiiS ) , 8 ( 2 ) : 1 – 20 . Yi - Ling Chung , Elizaveta Kuzmenko , Serra Sinem Tekiroglu , and Marco Guerini . 2019 . CONAN - COunter NArratives through nichesourcing : a mul - tilingual dataset of responses to ﬁght online hate speech . In Proceedings of the 57th Annual Meet - ing of the Association for Computational Linguis - tics , pages 2819 – 2829 , Florence , Italy . Association for Computational Linguistics . Thomas Davidson , Debasmita Bhattacharya , and Ing - mar Weber . 2019 . Racial bias in hate speech and abusive language detection datasets . arXiv preprint arXiv : 1905 . 12516 . Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017a . Automated hate speech detection and the problem of offensive language . In Proceedings of the International AAAI Conference on Web and Social Media , volume 11 . Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017b . Automated hate speech detection and the problem of offensive language . In Proceedings of the 11th International AAAI Confer - ence on Web and Social Media , ICWSM ’17 , pages 512 – 515 . Joe Davison , Joshua Feldman , and Alexander M Rush . 2019 . Commonsense knowledge mining from pre - trained models . In Proceedings of the 2019 Con - ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer - ence on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1173 – 1178 . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171 – 4186 , Minneapolis , Minnesota . Associ - ation for Computational Linguistics . Nemanja Djuric , Jing Zhou , Robin Morris , Mihajlo Gr - bovic , Vladan Radosavljevic , and Narayan Bhamidi - pati . 2015 . Hate speech detection with comment em - beddings . In Proceedings of the 24th international conference on world wide web , pages 29 – 30 . Serena Does , Belle Derks , and Naomi Ellemers . 2011 . Thou shalt not discriminate : How emphasizing moral ideals rather than obligations increases whites’ support for social equality . Journal of Experimental Social Psychology , 47 ( 3 ) : 562 – 571 . Ashwin Geet D’sa , Irina Illina , and Dominique Fohr . 2019 . Towards non - toxic landscapes : Automatic toxic comment detection using dnn . arXiv preprint arXiv : 1911 . 08395 . Zahra Fatemi , Chen Xing , Wenhao Liu , and Caim - ing Xiong . 2021 . Improving gender fairness of pre - trained language models without catastrophic forget - ting . arXiv preprint arXiv : 2110 . 05367 . Susan T Fiske . 1993 . Controlling other people : The impact of power on stereotyping . American psychol - ogist , 48 ( 6 ) : 621 . Lucie Flekova , Jordan Carpenter , Salvatore Giorgi , Lyle Ungar , and Daniel Preo¸tiuc - Pietro . 2016 . An - alyzing biases in human perception of user age and gender from text . In Proceedings of the 54th Annual Meeting of the Association for Computational Lin - guistics ( Volume 1 : Long Papers ) , pages 843 – 854 . Tianyu Gao , Adam Fisch , and Danqi Chen . 2020 . Making pre - trained language models better few - shot learners . arXiv preprint arXiv : 2012 . 15723 . Samuel Gehman , Suchin Gururangan , Maarten Sap , Yejin Choi , and Noah A Smith . 2020 . Realtoxici - typrompts : Evaluating neural toxic degeneration in language models . In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3356 – 3369 . Kazuma Hashimoto , Caiming Xiong , Yoshimasa Tsu - ruoka , and Richard Socher . 2017 . A joint many - task model : Growing a neural network for multiple nlp tasks . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1923 – 1933 . Hatebase . org . 2021 . Hatebase . https : / / hatebase . org / . ( Accessed on 12 / 08 / 2021 ) . Dirk Hovy and Shrimai Prabhumoye . 2021 . Five sources of bias in natural language processing . Lan - guage and Linguistics Compass , 15 ( 8 ) : e12432 . Jiachang Liu , Dinghan Shen , Yizhe Zhang , Bill Dolan , Lawrence Carin , and Weizhu Chen . 2021a . What makes good in - context examples for gpt - 3 ? arXiv preprint arXiv : 2101 . 06804 . Pengfei Liu , Weizhe Yuan , Jinlan Fu , Zhengbao Jiang , Hiroaki Hayashi , and Graham Neubig . 2021b . Pre - train , prompt , and predict : A systematic survey of prompting methods in natural language processing . arXiv preprint arXiv : 2107 . 13586 . Robert L Logan IV , Ivana Balaževi´c , Eric Wallace , Fabio Petroni , Sameer Singh , and Sebastian Riedel . 2021 . Cutting down on prompts and parameters : Simple few - shot learning with language models . arXiv preprint arXiv : 2106 . 13353 . Sharon L Lohr . 2021 . Sampling : design and analysis . Chapman and Hall / CRC . Thomas Mandl , Sandip Modha , Prasenjit Majumder , Daksh Patel , Mohana Dave , Chintak Mandlia , and Aditya Patel . 2019 . Overview of the hasoc track at ﬁre 2019 : Hate speech and offensive content identiﬁ - cation in indo - european languages . In Proceedings of the 11th Forum for Information Retrieval Evalu - ation , FIRE ’19 , page 14 – 17 , New York , NY , USA . Association for Computing Machinery . Mainack Mondal , Leandro Araújo Silva , and Fabrí - cio Benevenuto . 2017 . A measurement study of hate speech in social media . In Proceedings of the 28th ACM conference on hypertext and social media , pages 85 – 94 . Yixin Nie , Adina Williams , Emily Dinan , Mohit Bansal , Jason Weston , and Douwe Kiela . 2020 . Ad - versarial nli : A new benchmark for natural language understanding . In Proceedings of the 58th Annual Meeting of the Association for Computational Lin - guistics , pages 4885 – 4901 . Chikashi Nobata , Joel Tetreault , Achint Thomas , Yashar Mehdad , and Yi Chang . 2016 . Abusive lan - guage detection in online user content . In Proceed - ings of the 25th international conference on world wide web , pages 145 – 153 . John Pavlopoulos , Prodromos Malakasiotis , and Ion Androutsopoulos . 2017 . Deeper attention to abu - sive user content moderation . In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 1125 – 1135 . PerspectiveAPI . 2021 . Perspec - tive | developers . https : / / developers . perspectiveapi . com / s / . ( Accessed on 12 / 08 / 2021 ) . Fabio Petroni , Tim Rocktäschel , Sebastian Riedel , Patrick Lewis , Anton Bakhtin , Yuxiang Wu , and Alexander Miller . 2019 . Language models as knowl - edge bases ? In Proceedings of the 2019 Confer - ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer - ence on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2463 – 2473 , Hong Kong , China . As - sociation for Computational Linguistics . Ashwin Rajadesingan , Reza Zafarani , and Huan Liu . 2015 . Sarcasm detection on twitter : A behavioral modeling approach . In Proceedings of the eighth ACM international conference on web search and data mining , pages 97 – 106 . Nils Reimers and Iryna Gurevych . 2019 . Sentence - bert : Sentence embeddings using siamese bert - networks . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics . Sara Rosenthal , Pepa Atanasova , Georgi Karadzhov , Marcos Zampieri , and Preslav Nakov . 2021 . Solid : A large - scale semi - supervised dataset for offensive language identiﬁcation . In Findings of the Associ - ation for Computational Linguistics : ACL - IJCNLP 2021 , pages 915 – 928 . Sebastian Ruder . 2017 . An overview of multi - task learning in deep neural networks . arXiv preprint arXiv : 1706 . 05098 . Magnus Sahlgren , Tim Isbister , and Fredrik Olsson . 2018 . Learning representations for detecting abu - sive language . In Proceedings of the 2nd Workshop on Abusive Language Online ( ALW2 ) , pages 115 – 123 . Haji Mohammad Saleem , Kelly P Dillon , Susan Be - nesch , and Derek Ruths . 2017 . A web of hate : Tack - ling hateful speech in online social spaces . arXiv preprint arXiv : 1709 . 10159 . Joni Salminen , Hind Almerekhi , Ahmed Mohamed Kamel , Soon - gyo Jung , and Bernard J Jansen . 2019 . Online hate ratings vary by extremes : A statistical analysis . In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval , pages 213 – 217 . Joni Salminen , Hind Almerekhi , Milica Milenkovi´c , Soon - gyo Jung , Jisun An , Haewoon Kwak , and Bernard J Jansen . 2018 . Anatomy of online hate : de - veloping a taxonomy and machine learning models for identifying and classifying hate in online news media . In Twelfth International AAAI Conference on Web and Social Media . Maarten Sap , Dallas Card , Saadia Gabriel , Yejin Choi , and Noah A Smith . 2019 . The risk of racial bias in hate speech detection . In Proceedings of the 57th annual meeting of the association for computational linguistics , pages 1668 – 1678 . Maarten Sap , Saadia Gabriel , Lianhui Qin , Dan Ju - rafsky , Noah A . Smith , and Yejin Choi . 2020 . So - cial bias frames : Reasoning about social and power implications of language . In Proceedings of the 58th Annual Meeting of the Association for Compu - tational Linguistics , pages 5477 – 5490 , Online . As - sociation for Computational Linguistics . Timo Schick , Sahana Udupa , and Hinrich Schütze . 2021 . Self - diagnosis and self - debiasing : A pro - posal for reducing corpus - based bias in nlp . arXiv preprint arXiv : 2103 . 00453 . Scikit - learn . 2022a . Roc - auc - score . https : / / scikit - learn . org / stable / modules / generated / sklearn . metrics . roc _ auc _ score . html . ( Accessed on 04 / 13 / 2022 ) . Scikit - learn . 2022b . Tﬁdfvectorizer . https : / / tinyurl . com / scikit - tfidf . ( Accessed on 04 / 13 / 2022 ) . Taylor Shin , Yasaman Razeghi , Robert L . Logan IV , Eric Wallace , and Sameer Singh . 2020 . AutoPrompt : Eliciting Knowledge from Language Models with Automatically Generated Prompts . In Proceed - ings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4222 – 4235 , Online . Association for Computational Linguistics . Mohammad Shoeybi , Mostofa Patwary , Raul Puri , Patrick LeGresley , Jared Casper , and Bryan Catan - zaro . 2019 . Megatron - lm : Training multi - billion pa - rameter language models using model parallelism . arXiv preprint arXiv : 1909 . 08053 . Shutterstock . 2013 . List of dirty , naughty , obscene , and otherwise bad words . https : / / github . com / LDNOOBW / List - of - Dirty - Naughty - Obscene - and - Otherwise - Bad - Words . ( Accessed on 12 / 07 / 2021 ) . Samuel Caetano da Silva , Thiago Castro Ferreira , Ri - celli Moreira Silva Ramos , and Ivandré Paraboni . 2020 . Data - driven and psycholinguistics - motivated approaches to hate speech detection . Computación y Sistemas , 24 ( 3 ) : 1179 – 1188 . Tom Simonite . 2021 . Ai and the list of dirty , naughty , obscene , and otherwise bad words | wired . https : / / www . wired . com / story / ai - list - dirty - naughty - obscene - bad - words / . ( Accessed on 12 / 07 / 2021 ) . Shaden Smith , Mostofa Patwary , Brandon Norick , Patrick LeGresley , Samyam Rajbhandari , Jared Casper , Zhun Liu , Shrimai Prabhumoye , George Zerveas , Vijay Korthikanti , et al . 2022 . Using deepspeed and megatron to train megatron - turing nlg 530b , a large - scale generative language model . arXiv preprint arXiv : 2201 . 11990 . Anders Søgaard and Yoav Goldberg . 2016 . Deep multi - task learning with low level tasks supervised at lower layers . In Proceedings of the 54th Annual Meet - ing of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 231 – 235 , Berlin , Germany . Association for Computational Linguis - tics . Hwanjun Song , Minseok Kim , Dongmin Park , Yooju Shin , and Jae - Gil Lee . 2022 . Learning from noisy labels with deep neural networks : A survey . IEEE Transactions on Neural Networks and Learning Sys - tems . Sara Owsley Sood , Elizabeth F Churchill , and Judd Antin . 2012 . Automatic identiﬁcation of personal insults on social news sites . Journal of the Ameri - can Society for Information Science and Technology , 63 ( 2 ) : 270 – 285 . POS - tagger Spacy . 2022 . Part - of - speech - tagging . https : / / spacy . io / usage / linguistic - features # pos - tagging . ( Accessed on 04 / 13 / 2022 ) . Emma Strubell , Ananya Ganesh , and Andrew McCal - lum . 2019 . Energy and policy considerations for deep learning in nlp . In Proceedings of the 57th An - nual Meeting of the Association for Computational Linguistics , pages 3645 – 3650 . SurgeAI . 2021 . Github - surge - ai / profanity : The world’s largest profanity list . https : / / github . com / surge - ai / profanity . ( Accessed on 12 / 07 / 2021 ) . Stefanie Ullmann and Marcus Tomalin . 2020 . Quar - antining online hate speech : technical and ethical perspectives . Ethics and Information Technology , 22 ( 1 ) : 69 – 80 . Luis von Ahn . 2021 . Offensive / profane word list . https : / / www . cs . cmu . edu / ~ biglou / resources / . ( Accessed on 12 / 07 / 2021 ) . Zeerak Waseem , Thomas Davidson , Dana Warmsley , and Ingmar Weber . 2017 . Understanding abuse : A typology of abusive language detection subtasks . arXiv preprint arXiv : 1705 . 09899 . Zeerak Waseem and Dirk Hovy . 2016 . Hateful sym - bols or hateful people ? predictive features for hate speech detection on Twitter . In Proceedings of the NAACL Student Research Workshop , pages 88 – 93 , San Diego , California . Association for Computa - tional Linguistics . Laura Weidinger , John Mellor , Maribeth Rauh , Conor Grifﬁn , Jonathan Uesato , Po - Sen Huang , Myra Cheng , Mia Glaese , Borja Balle , Atoosa Kasirzadeh , et al . 2021 . Ethical and social risks of harm from language models . arXiv preprint arXiv : 2112 . 04359 . Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Sara Rosenthal , Noura Farra , and Ritesh Kumar . 2019 . Predicting the type and target of offensive posts in social media . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1415 – 1420 , Minneapolis , Minnesota . Association for Computational Linguistics . Ziqi Zhang , David Robinson , and Jonathan Tepper . 2018 . Detecting hate speech on twitter using a convolution - gru based deep neural network . In Eu - ropean semantic web conference , pages 745 – 760 . Springer . Zexuan Zhong , Dan Friedman , and Danqi Chen . 2021 . Factual probing is [ MASK ] : Learning vs . learning to recall . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , pages 5017 – 5033 , Online . Association for Computational Linguistics . Xuhui Zhou , Yue Zhang , Leyang Cui , and Dandan Huang . 2020 . Evaluating commonsense in pre - trained language models . In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol - ume 34 , pages 9733 – 9740 . Task Deﬁnition Tokens Offensive Does this post contain offensive language ? Yes , No Intent Does this post contain intentional insults ? Yes , No Lewd Does this post contain sexual content ? Yes , No Group Does this post contain offense to a group ? Yes , No WHO Who is referred to / targeted by this post - body , culture , disabled , gender , race , social , victim ? body , culture , disabled , gender , race , social , victim HOF Does this post contain any form of non - acceptable language such as hate speech , offensiveness , aggression , profanity ? Yes , No HOP This post contains hate , offense , profanity Target Does this post contain an insult / threat to an individual , group , or others ? Yes , No Table 7 : Deﬁnitions of bias used for each task . We formulate the deﬁnitions based on labeling instruction from the SBIC and HASOC datasets . For binary deﬁnitions we use a template in the form of : Does this post contain < attribute > ? A Appendix A . 1 Bias Deﬁnitions The bias deﬁnitions used in all experiments are mentioned in Table 7 . We formulate the deﬁnitions for binary tasks based on labeling instruction from the datasets . We use a template for the deﬁnition in the form of : Does this post contain { attribute } ? . We also mention the tokens for which we calculate classiﬁcation probability in Table 7 . A . 2 Keyword analysis We study the percentage of keywords present in the textual posts of the test set for each binary task and their correlation to labels . We use a superset of keyword ( S ) containing 3173 keywords which is a union of keywords from ( von Ahn , 2021 ; SurgeAI , 2021 ; Shutterstock , 2013 ) . We calculate the over - lap of keywords ( S ) with a textual post Q . Speciﬁ - cally , we check its correlation with the pos and neg labels for the binary tasks i . e percentage of positive textual posts p that have at least one keyword from S and percentage of negative textual posts n that have at least one keyword from S . The ratio is cal - culated as p / n and tells us the rate at which positive posts have a higher / lower overlap with keywords compared to the negative posts . This analysis along with label correlation is presented in Table 8 . We can see that the Target task from HASOC has the highest ratio indicating that it has higher overlap of keywords with the positive labeled examples as compared to the negative labeled examples leading to a hard to beat heuristic . A . 3 Qualitative Analysis Qualitative analysis as described in section § 4 is shown in Table 9 . We show the representative tex - tual posts for each category ( Warning : the exam - Task pos neg ratio Offensive ( 57 . 8 % pos ) 85 . 20 74 . 51 1 . 14 Intent ( 53 . 1 % pos ) 84 . 75 76 . 08 1 . 11 Lewd ( 9 . 6 % pos ) 93 . 35 79 . 34 1 . 18 Group ( 41 . 1 % pos ) 84 . 89 77 . 76 1 . 09 HOF ( 25 . 0 % pos ) 88 . 54 76 . 07 1 . 16 Target ( 85 . 1 % pos ) 88 . 57 15 . 51 5 . 71 Table 8 : Keyword analysis for the SBIC tasks , based on union of keywords from ( von Ahn , 2021 ; SurgeAI , 2021 ; Shutterstock , 2013 ) . The percentages show the overlap of keywords with the pos and neg labels for the binary tasks . We can see that the Target task from HASOC has the highest overlap of keywords with gold labels leading to a hard to beat heuristic . We show the percentage of positive samples in the test set in brack - ets . ples shown contain content that maybe offensive or upsetting ) . A . 4 Details on Finetuned Models We provide details of the SC and HS models de - scribed in § 3 . 6 . The SC model ( Sap et al . , 2020 ) was trained to predict the entire social bias frame given the text of the post i . e given the post the GPT - 2 model was ﬁnetuned to generate the token corre - sponding to each of the ﬁve classes in the original set ( w [ lewd ] , w [ offensive ] , w [ intent ] , w [ group ] , w [ in - group ] ) . Note that we don’t present results for the in - group classiﬁcation task . We report the results of best performing systems in English in the HASOC track at FIRE 2019 : Hate Speech and Offensive Content Identiﬁcation in Indo - European Languages ( Mandl et al . , 2019 ) . We report the result of YNU _ wb team for HOF task , and 3Idiots team for HOP and target task . Id Error Class % Textual Post O . Cases where MT - NLG makes correct prediction and TF - IDF is wrong 1 Post is offensive and does not contains offen - sive keywords 38 . 33 What’s black , white and rolls around on the beach ? A black man and a seagull ﬁghting over a chip . . Y 2 Post is not offensive and does not contain offensive keywords 38 . 33 Bush : ’it’s hard to unify the country when the news media is so split up . Remember when the news media mattered a lot more’ . N 3 Post is offensive and contains offensive key - words 10 . 00 Y’RE ALL ANNOYING B * * CH YOU’RE NOT MY CLASSMATES ANYMORE . Y 4 Post is not offensive but contains keywords 8 . 33 RT @ markiplier : In the car with my frands going F * * * ING HYPERSPEED . N 5 Mislabeled post 5 . 00 omg twitter you are killing me with dumb this morning . N Cases where MT - NLG makes wrong prediction and TF - IDF is correct 6 Post is not offensive and does not contain offensive keywords 31 . 67 The answer is yes I can ﬁt 2 more people in this shirt therefor nobody will know I’m not wearing a bra . N 7 Post is offensive and does not contain offen - sive keywords 26 . 67 I know I keep hoping I wake up one day and this multira - cial hell Canada was all just a terrible nightmare , but it never happens . Y 8 Post is offensive and contains offensive key - words 21 . 67 number of femoids who would rather die than spend more than 5 seconds with an incel . Y 9 Post is not offensive but contains keywords 15 . 00 AHHH karmas a b * * ch . N 11 Mislabeled post 5 . 00 Are you a bummer tied to a tree ? No ? ? BUMMER ON THE LOOSE ! ! N Table 9 : Analysis of the offensive task from ( Sap et al . , 2020 ) . Textual Post indicates grounded information . O . indicates the ground truth label for the category . Model Sampling Intent AUC F1 SC - - 78 . 60 SBERT SBERT 79 . 36 82 . 36 Meg - 1 . 3 SBERT 64 . 47 68 . 05 Meg - 8 . 3 SBERT 68 . 95 75 . 42 MT - NLG SBERT 77 . 59 80 . 00 Table 10 : Results for the 32 - shot prompting on intent classiﬁcation task from SBIC dataset § 3 . 1 on SBERT baseline and three language models which use shots sampled by SBERT . B Analysis on Sentence - BERT In this section , we show additional experiments using Sentence - BERT ( SBERT ) sampling tech - nique ( Reimers and Gurevych , 2019 ) to select class - balanced shots for k - shot classiﬁcation . Although , the experiments in Table 10 show that SBERT can perform better for the bias tasks explored in this pa - per , results in Table 11 and Table 12 show this tech - nique does not generalize to other tasks . We show in Section § B . 1 that these sampling techniques take advantage of the keywords that are used only in one speciﬁc contexts . We use SBERT to encode the post Q and the posts from D into a common embedding space . 7 7 https : / / huggingface . co / sentence - transformers / all - MiniLM - L12 - v2 Model Sampling Intent AUC F1 SBERT SBERT - strat 23 . 51 14 . 54 Meg - 1 . 3 SBERT - strat 58 . 16 60 . 32 Meg - 8 . 3 SBERT - strat 64 . 52 71 . 45 MT - NLG SBERT - strat 74 . 15 76 . 43 Table 11 : Results for the 32 - shot prompting on intent classiﬁcation task on data stratiﬁed SBERT baseline and three language models which use shots sampled by data stratiﬁed SBERT . We then select k / | C | posts with the highest cosine similarity score from each class . These shots are provided as context to language model M . For the SBERT baseline , we consider the class that has the highest average score as the prediction . Results The results for 32 - shot intent classiﬁca - tion task on the SBERT baseline and the LMs using SBERT sampling technique are shown in Table 10 . We observe that SBERT performs better than all the language models in intent classiﬁcation task . Data Stratiﬁcation The SBIC dataset exhibits strong correlation between semantic similarity ( cap - tured by SBERT ) and target class . This creates a very strong heuristic baseline , that is hard to beat even for ﬁne - tuned model ( see Table 10 ) . To re - move the impact of such simple heuristic and inves - tigate whether the model is capable of reasoning on the shot content , we implemented a stratiﬁed ran - domization for balanced shot selection . The goal of the stratiﬁcation is to select equal number of shots for each class , such that the mean difference in cosine similarity of the class - balanced shots is minimized . To accomplish this , we implemented a binning - based stratiﬁcation of shot - query cosine similarity scores ( Lohr , 2021 ) . After SBERT embeddings and cosine similari - ties are calculated for all the train set , the stratiﬁed shot selection follows the following steps : 1 ) shots are ordered based on cosine similarity with the query text , 2 ) shots are allocated into bins based on their cosine similarity score using bin thresh - olds calculated using numpy implementation of histogram based binning 8 , 3 ) starting from the bin with the highest similarity and moving downwards , a maximum number of class - balanced shots from each bin is selected , and 4 ) step 3 is repeated until the total desired number of shots are selected . As a result of this process , the average difference in mean cosine similarity between the shots for two classes is 0 . 0029 ( SD = 0 . 0020 ) . The summary of shot stratiﬁcation impact can be seen in Table 11 . Using stratiﬁed class - balanced shots , removes the impact of simple SBERT heuristic reducing AUC from 79 . 36 to 23 . 52 and F1 from 82 . 36 to 14 . 54 , but at the same time has limited impact on the performance of the MT - NLG model reducing AUC by 4 . 4 % ( 77 . 59 to 74 . 15 ) and F1 by 4 . 5 % ( 80 . 00 to 76 . 43 ) . The results in Table 11 illustrates the strong cor - relation between semantic similarity captured by SBERT and target class . We would like to note that this correlation is speciﬁc to bias datasets since often times they are collected using certain refer - ential terms and keywords . To further bolster our hypothesis we present additional analysis of corre - lation of keyword in the shots selected by SBERT and the label , as well as an analysis on ANLI task ( see § B . 1 ) . B . 1 Analysis Keyword overlap We performed additional in - depth analysis to better understand the strong per - formance of SBERT heuristic on the SBIC dataset . Through qualitative exploration of the “Intent” task we observed an overlap of the key referential 8 ‘auto’ binning - takes the maximum of Sturges and Freedman Diaconis estimators - https : / / numpy . org / doc / stable / reference / generated / numpy . histogram _ bin _ edges . html terms present in test set queries , such as “clin - ton” , “p * * * phile” predominantly with shots for one label , but not the other . This could suggest that in the SBIC dataset , the presence of such terms is sufﬁcient to decide on the label . To verify this observation at scale , we lemmatized the test queries extracting only NOUNS and PRONOUNS ( via part - of - speech tagging ( Spacy , 2022 ) ) . This process extracted referential terms such as : “clin - ton” , “p * * * phile” , “b * * ch” , “lord” , etc . We fur - ther counted the frequency of these terms in shots picked from the train set with the same and oppo - site label ( based on gold ) . For example , for post ‘Alex Jones & Mike Cernovich : “It’s crazy how the P * * * philes . . . all LOOK LIKE P * * * PHILES " . . . # Truth’ the extracted terms are ‘alex’ , ‘cernovich’ , ‘jones’ , ‘mike’ , ‘p * * * phile’ , ‘truth’ . The frequency of these terms in same labeled shots resolves to : ‘p * * * phile’ : 12 , ‘cernovich’ : 1 , ‘alex’ : ’ , ‘jones’ : 1 , while for the opposite label shots these terms as much less frequent : ‘p * * * phile’ : 6 , ‘cernovich’ : 1 , ‘truth’ : 2 . From this we can see that e . g . , the term “p * * * phile” is much more frequent in the shots with the same label , which suggests it is used in only one context . At the scale of the entire test set , we quantiﬁed that the same label shots have on average 11 . 36 terms overlapping with the test posts , while for the opposite label shots this overlap amounts to only 7 . 76 . Furthermore the ratio of keyword overlap between same and opposite label shots is 1 . 97 on average , meaning that there are almost 2x as many keywords overlapping with test post for the same labeled shots compared to opposite labeled shots . This analysis suggests that crucial referential terms present in test posts , which , in essence , could be used in a biased or unbiased context ( i . e . , even term “p * * * phile” could be used in challenging con - texts , such as a policy announcement or criticisms ) are used in only one context . Such strong associa - tion of terms and context permits the mere presence or absence of such referential terms to be sufﬁcient to decide on the label , which in turn drives high performance of simple SBERT heuristic . ANLI Results To further investigate our hypoth - esis that bias datasets have the property of using certain words or phrases only in biased or non - biased context , we test our approach on another task - the ANLI task . The ANLI ( Nie et al . , 2020 ) dataset is an adversarially mined natural language inference Model k ANLI - R1 ANLI - R2 ANLI - R3 SBERT 48 28 . 10 30 . 80 30 . 16 MT - NLG 48 40 . 20 44 . 40 49 . 08 SBERT 24 27 . 10 31 . 90 30 . 75 MT - NLG 24 43 . 00 45 . 60 50 . 42 SBERT 9 28 . 50 38 . 50 35 . 08 MT - NLG 9 45 . 00 44 . 90 51 . 33 Table 12 : Accuracy for the k - shot prompting on three different natural language inference datasets on the SBERT baseline and MT - NLG with k = { 48 , 24 , 9 } ( NLI ) dataset that aims to create a difﬁcult set of NLI problems . It has 3 iterative rounds of data collection marked as ANLI - R1 , ANLI - 2 and ANLI - R3 . Following ( Smith et al . , 2022 ) , we rephrase the NLI problem into a question - answering format where each example is structured as “ < premise > Question : < hypothesis > . True , False or Neither ? Answer : ” . This prompt is given to the language model and we calculate the probability of tokens “True” , “False” and “Neither” . The token with the highest likelihood assigned by the model is considered as the model prediction . In case of k - shot classiﬁcation , we use SBERT to select class - balanced shots which are most sim - ilar to the query . The results of accuracy scores for the three rounds of ANLI datasets are shown in Table 12 with k = { 48 , 24 , 9 } . We observe that for all values of k , MT - NLG performs signiﬁcantly better than the SBERT baseline . ANLI datasets were collected with the goal of having diverse set of contexts from various domains . They contain text extracted from Wikipedia , News ( extracted from Common Crawl ) , ﬁction ( extracted from Sto - ryCloze and CBT ) , formal spoken text ( excerpted from court and presidential debate transcripts in the Manually Annotated Sub - Corpus ( MASC ) of the Open American National Corpus3 ) and causal or procedural text , which describes sequences of events or actions , extracted from WikiHow ( see Section 2 . 5 in ( Nie et al . , 2020 ) ) . Hence , the dataset does not contain samples that use certain phrases only in one context or uses certain words / phrases only for one label . Hence , this further solidiﬁes our argument that our approach is generally applicable but struggles to perform better than heuristic - based SBERT baseline when the dataset is skewed . C Ethical Considerations The intended use of the proposed instruction - based detection techniques is to aid the identiﬁcation of different forms of bias either in human or AI pro - duced textual content . One potential applications can be to assist moderation of content in settings such as social media or in end - user AI applications involving language generation such as conversa - tional agents . It can also be used to add safety fea - tures to the generation outputs of large LMs as well as it can assist in building future large LMs that are not biased by identifying biases in pretraining data . Given the limitations of our proposed method , it should likely not be used as a sole measure for detecting bias , but we believe it could serve as a low - effort initial ﬁlter and feedback mechanism . Furthermore , we see its use as potential means for promoting positive online interaction and higher community standards ( Does et al . , 2011 ) . Our proposed method , can unfortunately be mis - used intentionally or unintentionally ( Weidinger et al . , 2021 ) . We speciﬁcally see the dangers of using our approach for censorship ( Ullmann and Tomalin , 2020 ) or limiting expression in speciﬁc settings where violent or lewd language might be intended ( e . g . , crime ﬁction ) . Furthermore , there is danger of misappropriating our approach to intro - duce racially targeted censorship based on dialect ( Sap et al . , 2019 ) . Despite certain degree of ro - bustness and our additional experiments ( e . g . , with ﬂipped labels ) , our method still relies on super - vised labeling of few - shots provided as context , and as such can be affected by the imperfections of the labeling , such as racial bias in existing la - beled datasets ( Davidson et al . , 2019 ) . As shown in recent work , human annotation process can be a source of bias in itself ( Hovy and Prabhumoye , 2021 ) and labelers may even be selected by mali - cious actors to introduce biased interpretation on purpose ( Flekova et al . , 2016 ) .