Previous studies have examined various aspects of user behavior on the Web , including general information - seeking patterns , search engine use , and revisitation habits . Little research has been conducted to study how users navigate and interact with their Web browser across different information - seeking tasks . We have conducted a ﬁeld study of 21 participants , in which we logged detailed Web usage and asked participants to provide task categorizations of their Web usage based on the following categories : Fact Finding , Informa - tion Gathering , Browsing , and Transactions . We used implicit measures logged during each task session to provide usage measures such as dwell time , number of pages viewed , and the use of speciﬁc browser navigation mechanisms . We also report on differences in how participants interacted with their Web browser across the range of information - seeking tasks . Within each type of task , we found several distinguishing character - istics . In particular , Information Gathering tasks were the most complex ; participants spent more time com - pleting this task , viewed more pages , and used the Web browser functions most heavily during this task . The results of this analysis have been used to provide impli - cations for future support of information seeking on the Web as well as direction for future research in this area . Introduction Previous research has contributed to a general under - standing of the types of information - seeking tasks in which users engage on the Web ( Choo , Detlor , & Turnbull , 2000 ; Morrison , Pirolli , & Card , 2001 ; Sellen , Murphy , & Shaw , 2002 ) ; however , much of this research was conducted 5 to 10 years ago . While commercial Web browser functional - ity ( e . g . , bookmarks , back button ) has changed little over this time , the Web has changed dramatically . The number of available Web pages has grown exponentially since these early studies ( from millions to several billions ) , and the uses of the Web have expanded dramatically to include wikis , blogs , and various Web - based applications ( e . g . , online photo sharing , banking , e - learning ) . Google has changed the way users search on the Web , and the popu - larity of open - source browsers such as Firefox has con - tributed to a wide variety of Web browser toolbars and plug - ins . There is a large body of research examining how users navigate the Web ( Catledge & Pitkow , 1995 ; Tauscher & Greenberg , 1997 ; Weinreich , Obendorf , Herder , & Mayer , 2006 ) as well as mechanisms for Web navigation ( MacKay , Kellar , & Watters , 2005 ; Milic - Frayling , Sommerer , & Rodden , 2003 ; Moyle & Cockburn , 2003 ) ; however , these studies are typically conducted in the ﬁeld without any un - derstanding of the types of tasks undertaken by the user or in a laboratory setting for a focused set of tasks . For instance , user - revisitation patterns on the Web have been studied extensively ( Cockburn & McKenzie , 2001 ; Herder , 2005 ; Tauscher & Greenberg , 1997 ) in a ﬁeld setting , but without any understanding of how revistation may be impacted by the underlying task type . How a user completes a revistation task may be inﬂuenced by the user’s task and intentions . For example , a user who revisits a Web page to reﬁnd a previ - ously found fact may need different navigational support than a user who is revisiting a Web page to monitor new information . One gap that we have identiﬁed in the literature is the lack of research examining how users interact with their Web browsers within the context of task . Therefore , the goals of this research are threefold . First , we aim to gain an understanding of the types of Web browser functionalities ( e . g . , number of pages viewed , windows opened , use of Web browser navigation mechanisms ) currently being used dur - ing information - seeking tasks . Second , we aim to determine whether there are differences in the use of these functionali - ties across the different information - seeking tasks . Third , we aim to provide an updated view of the types of tasks being performed on the Web . It also is important that these data are collected in as natural of a user environment as possible . An understanding of how users interact with their Web browsers across different information - seeking tasks has the potential JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY , 58 ( 7 ) : 999 – 1018 , 2007 A Field Study Characterizing Web - Based Information - Seeking Tasks Melanie Kellar , Carolyn Watters , and Michael Shepherd Faculty of Computer Science , Dalhousie University , Halifax , Nova Scotia , B3H 1W5 Canada . E - mail : { melanie , watters , shepherd @ cs . dal . ca } Received October 26 , 2005 ; revised August 16 , 2006 ; accepted August 16 , 2006 © 2007 Wiley Periodicals , Inc . • Published online 29 March 2007 in Wiley InterScience ( www . interscience . wiley . com ) . DOI : 10 . 1002 / asi . 20590 to provide valuable insight into how future tools should be designed . This article reports on a recent ﬁeld study in which we studied differences in how users interact with their Web browser during the following information - seeking tasks : Fact Finding , Information Gathering , Browsing , and Trans - actions . Participants were asked to annotate their Web usage with task information while using a custom Web browser that logged their interactions with the browser . The key contribution of this article is a characterization of the differ - ences in how users interact with their Web browsers across the range of information - seeking tasks . This understanding has been used to provide implications for future support of Web - based information seeking as well as to provide direc - tion for future research in this area . In the next section , we present an overview of the related literature followed by a description of the methodological and data - collection techniques used during the ﬁeld study . We then report general observations describing the charac - teristics of participants’ task sessions as well as differences between the tasks according to the following elements : dwell time , windows opened , pages loaded , use of Web browser navigation tools , time of day , use of Google , use of site - speciﬁc searches , and use of Web browser functions . Next , we provide a summary of the ﬁndings and discuss the implications of our results . Finally , we conclude with future directions for this research . Related Work In this section , we discuss related work exploring the relationship of task to user behavior on the Web and method - ologies for studying the information - seeking behavior of Web users . Task and User Behavior on the Web There is a large body of theoretical research examining information seeking in both electronic and nonelectronic environments ( Belkin , 1980 ; Ellis , 1989 ; Kuhlthau , 1991 ; Marchionini , 1995 ; Wilson , 1997 ) ; however , information seeking on the Web is a newer branch of research and differs from library - based information seeking in the complexity of the resources and the tools used . Cothey ( 2002 ) noted that “There is little underlying theory of Web information searching as distinct from information search theory more generally and especially information searching in electronic environments” ( p . 68 ) . Several studies have examined general user behavior on the Web . In one of the ﬁrst studies of Web usage , Catledge and Pitkow ( 1995 ) classiﬁed usage strategies into three cate - gories : serendipitous , general purpose , and searcher . Pitkow and Kehoe ( 1996 ) reported ﬁve main uses of the Web from the fourth Graphics , Visualization , and Usability Center ( GVU ) WWW survey ( http : / / www . cc . gatech . edu / gvu / user _ surveys / ) : browsing , entertainment , work , shopping , and other uses . They also noted that the activities had remained fairly con - sistent since the second study . Only a few in - depth studies , however , have examined overall information - seeking behavior on the Web in relation to the user’s intent or task . One of the most comprehensive studies was conducted by Choo et al . ( 2000 ) , who studied critical incidents of information seeking on the Web among 34 knowledge workers . Using interviews , questionnaires , and data logging over a 2 - week period , signiﬁcant episodes of information seeking were characterized as undirected viewing , conditioned viewing , informal search , and formal search . Morrison et al . ( 2001 ) studied signiﬁcant Web actions through 2 , 188 responses to the 10th GVU WWW user survey . Participants were asked to describe a recent episode in which they found information on the Web that led to a signiﬁcant decision or action . The participants reported four main goals : collect , ﬁnd , explore , and monitor . Sellen et al . ( 2002 ) studied the Web activities of 24 knowledge workers over 2 days . Participants were interviewed in front of their of Web history at the end of the second day and described the different activities in which they engaged . Activities were classiﬁed into six main categories : ﬁnding , information gathering , browsing , transacting , communicat - ing , and housekeeping . Finally , Rozanski , Bollman , and Lipman ( 2001 ) analyzed the clickstream data of 2 , 466 users and reported seven main Web usage occasions : quickies , just the facts , single mission , do it again , loitering , informa - tion please , and surﬁng . This work was conducted from a commercial standpoint since the focus of their work was for marketing purposes . Although these studies differed in methodology and re - search goals , there are strong similarities among the resul - tant categorizations , shown in Table 1 . The ﬁrst is the short answer or informal search , including fact ﬁnding and simple lookup . In this category , the goal of the user is to retrieve some short , speciﬁc information , possibly on one page . The second category , the formal search , is the more 1000 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi TABLE 1 . Common categories of user behavior found in previous research . Choo et al . ( 2000 ) Morrison et al . ( 2001 ) Sellen et al . ( 2002 ) Rozanski et al . ( 2002 ) 1 Informal search Find Finding Just the facts / Quickies 2 Formal search Collect Information gathering Information please / Single mission 3 Undirected viewing Explore Browsing Surﬁng / Loitering 4 Conditioned viewing Monitoring N / A Do it again 5 N / A N / A Transacting / Communicating / Housekeeping N / A traditional bibliographic search in which the user’s goal is to collect enough information on a topic . This may require multiple pages and overlapping data for conﬁrmation or al - ternate views on the topic . The third category is the ludic no - tion of browsing , where the user is engaged in spontaneous information seeking . The fourth category is monitoring , which includes repeated visits to one or more Web pages to monitor or check for dynamic information . As can be seen in Table 1 , monitoring is not always included as a distinct information - seeking task . The ﬁnal category consists of the remaining Web tasks studied by Sellen et al . ( 2002 ) , which include of non - information - seeking tasks such as transacting ( e . g . , online transactions ) , communicating ( e . g . , chat rooms and discussion boards ) , and housekeeping ( e . g . , maintaining Web pages ) . While our research complements the previous research presented in Table 1 , it differs several ways . First , we are providing an updated view of users’ information - seeking behavior on the Web . The previous studies were published between 2000 and 2002 , and the Web has changed substan - tially since that time . Second , we are incorporating a number of elements from each of the previous studies in this single study , including : a week - long observation of all participant behavior in a natural environment , logging of ﬁne - grain Web browser interactions , and detailed task information . Finally , we have conducted an in - depth analysis of the differences in Web browser interactions across information - seeking task sessions . Task Session In the study of user behavior on the Web , a session is gen - erally deﬁned as a period of continuous Web usage ; how - ever , the speciﬁc deﬁnition of a session tends to vary across researchers and research disciplines . For instance , based on their client - side transaction logs , Catledge and Pitkow ( 1995 ) deﬁned a session as a period of continuous Web usage with no break is usage greater than 25 . 5 min . In their studies of search engine transaction logs , Jansen and Spink ( 2003 ) measured session duration from the time the ﬁrst query was submitted to the search engine until the user quit the search engine . They reported that 52 % of all sessions lasted less than 15 min . Montgomery and Faloutsos ( 2001 ) deﬁned a session as a period of continuous Web usage , beginning when the user has not accessed the Web in the pre - vious 2 hr . Grace - Martin and Gay ( 2001 ) used a 10 min of inactivity cutoff while Hawkey and Inkpen ( 2005b ) used both a 10 - and 30 - min cutoff . In this work , we use the term task session to represent a period of continuous Web usage , annotated with the same task information , with no break in usage greater than 25 . 5 min . Information - Seeking Strategies There is a large body of research exploring more focused aspects of information seeking , such as categorizations of search engine queries and the search strategies employed by users on the Web . This area of research provides a better understanding of speciﬁc user Web search behavior and pro - vides some insight into improving support for users engaging in Web - based information - seeking tasks . Broder’s ( 2002 ) Web search taxonomy categorized search strategies into three categories : navigational , where the user’s goal is to reach a speciﬁc Web site ; informational , where the user’s goal is to ﬁnd information thought to exist on some Web page ; and transactional , where the user’s goal is to per - form a Web - based activity . Broder concluded that although each type of strategy is motivated by different goals , search engines must be able to support all strategies . Rose and Levinson ( 2004 ) extended Broder’s taxonomy to create a search - goal hierarchy , which was used to manually classify a set of AltaVista queries . They reported that only 35 % of all queries appeared to be of the type traditionally supported by search engines ( e . g . , directed and undirected search , advice seeking ) while over 40 % of the queries were noninforma - tional , such as resource - based queries looking for products and services . This suggests that the primary focus of many commercial search engines may be misguided . Lee , Liu , and Cho ( 2005 ) further extended this work to automatically classify Web search goals and were able to correctly catego - rize 90 % of the search goals evaluated . Jansen , Spink , and Pedersen ( 2005 ) categorized 2 , 600 AltaVista search queries and found that almost 50 % of the queries were related to peo - ple , places , or things . Approximately another 25 % of the queries were related to commerce , travel , employment , and technology , and the remaining 25 % were related to topics such as education , sciences , entertainment , and government . Jansen et al . ( 2005 ) also reported a high incidence of naviga - tional queries , suggesting that users are increasingly using search engines as a navigation mechanism . For instance , the three most common queries from the 2002 AltaVista dataset were “google , ” “yahoo , ” and “ebay . ” Previous research also has examined the strategies users employ to conduct Web searches . Fidel and Efthimiadis ( 1999 ) studied user information - seeking behavior through interviews and direct observations . They reported that although there were common search strategies among the participants ( e . g . , search queries , rapidly scanned results ) , they also observed that individuals had developed their own personal search strategies . Teevan , Alvarado , Ackerman , and Karger ( 2004 ) found two common search strategies among participants : ori - enteering ( i . e . , approaching the task as a sequence of small steps ) and teleporting ( i . e . , jumping directly to the desired information ) . The orienteering strategy was more common among participants , as it allowed them to iterate toward their information goal rather than explicitly state an initial , fully articulated query . Analysis of search engine logs also has yielded informa - tion on user search strategies . Spink , Wolfram , Jansen , and Saracevic ( 2001 ) analyzed over 1 million queries submitted to the Excite search engine in 1997 and found that users employed few search terms , rarely modiﬁed their queries , and rarely used advanced - search features . Between 1998 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1001 DOI : 10 . 1002 / asi and 2002 , Jansen et al . ( 2005 ) observed a decrease in the number of one - term queries and an increase in longer queries . The ﬁeld study described in this article was conducted with experienced Web users , and it is important to note that the information - seeking strategies of users may be inﬂu - enced by their level of experience . Aula , Jhaveri , and Käki ( 2005 ) and Aula and Käki ( 2003 ) studied the Web search strategies of expert users to better understand how to make the strategies of experienced users available to novice users . In a 10 - month longitudinal study , Cothey ( 2002 ) examined the change in students’information - seeking behavior as they gained more experience over time . As the students became more experienced , they began to visit a more distinct set of Web pages , accessed the Web less frequently , and exhibited a lower rate of search queries . Cooper ( 2001 ) found that session length for use of an electronic library catalogue increased over a 16 - month study period while the number of searches per session remained constant . Much of the research examining users’ search strategies on the Web has been conducted in the workplace . Rieh ( 2003 ) conducted one of the ﬁrst studies examining Web searching behavior in the home and found that users searched differently in the home than they did in previous research that examined search strategies in the workplace . In Rieh’s study , participants searched the Web more frequently , but for shorter periods of time , and the types of searches conducted were much broader . During our ﬁeld study , we recruited laptop users to capture participants’ Web usage both at home and at school / work ; however , this study did not focus on differences in use between home and school / work but instead focused on capturing a comprehensive picture of Web use . Strategies for Studying Information - Seeking Behavior on the Web A wide variety of methodologies has been employed to study information - seeking behavior on the Web , and the three main strategies include laboratory experiments , sample surveys , and ﬁeld studies . The choice of research strategy inﬂuences the generalizability of the results , the precision of the measurements and conditions being studied , and the realism of the scenario in which the data are collected ( McGrath , 1995 ) . McGrath stated that no single research strategy can maximize all three features ; choosing to maxi - mize one strategy comes at the expense of the others , and the decision of which strategy to use should be carefully consid - ered . The interested reader can refer to Martzoukou ( 2005 ) for a more detailed review of information - seeking research strategies . Within this research domain , we are beginning to see a wider variety of strategies employed , even within single studies . Laboratory experiments have been used in a number of studies ( Hölscher & Strube , 2000 ; Jhaveri & Räihä , 2005 ) and afford researchers a greater level of control . Researchers can mandate the tasks in which a user engages , and the software is standardized across all participants . Data are often collected using video / screen capture , direct observations , and logging methods such as transaction logs . Although videorecordings are easy to capture , the subsequent data coding that needs to take place can be very time consuming . One major drawback of laboratory experiments is that they do not offer much realism . Typically , participants are asked to complete tasks under time constraints and on lab computers , without their usual Web resources ( e . g . , book - marks , Web history , toolbars ) . The tasks often are contrived , and task success can be inﬂuenced by several factors includ - ing cognitive and problem - solving abilities and styles ( Kim & Allen , 2002 ) , domain knowledge ( Hölscher & Strube , 2000 ) , and Web experience ( Cothey , 2002 ; Hölscher & Strube , 2000 ) . One alternative , as used by Schiano , Stone , and Bectarte ( 2001 ) , is to invite participants to perform a task they already needed to do . Sample Surveys are often used ( Aula et al . , 2005 ; Morrison et al . , 2001 ; Pitkow & Kehoe , 1996 ) because they can be ad - ministered to large and diverse populations and can produce data with a high degree of generalizability . Unlike other re - search strategies , the method of data collection is relatively uncomplicated . The downside of sample surveys is that participants are studied outside of the context of their information seeking , which can decrease the level of realism . As noted by Sellen et al . ( 2002 ) , the way in which questions are asked can bias the results toward certain types of events . Teevan et al . ( 2004 ) noted that simple semantics such as the difference between asking participants what they were “looking for” versus “searching for” may inﬂuence what participants report . This is true , however , across all research strategies . Field studies are becoming an increasingly common re - search strategy for studying user behavior on the Web ( Choo et al . , 2000 ; Sellen et al . , 2002 ; Teevan et al . , 2004 ) . The pri - mary strength of ﬁeld studies is the increase in realism , as participants are observed working in their own environ - ment ( s ) , with their own tools ( e . g . , bookmarks , history , choice of browser ) , and completing tasks that are motivated by the participant and not the researcher . In general , ﬁeld studies are conducted with a relatively small , homogenous set of participants , which can lessen the generalizability of results . Due to the natural environment in which ﬁeld studies are conducted , data collection can be dif - ﬁcult , and researchers must often accept a loss of precision and control . Researchers have employed a variety of meth - ods to observe their participants , such as transaction logs ( client / proxy / server side ) , video / screen capture , and partici - pant diaries . In this research , we chose to conduct a ﬁeld study with the aim of maximizing the realism of the research setting . This methodology does come at the expense of both generalizabil - ity and precision , but allowed us to study the information - seeking habits of our participants in their own environments ( e . g . , home / work / school ) , engaging in their own tasks , and with access to their usual navigation mechanisms ( e . g . , book - marks , history , auto - complete ) . While participants were asked 1002 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi to use a custom - built Web browser , it mimicked Microsoft Internet Explorer ( IE ) as closely as possible and included all of their IE settings and usage history to try and minimize the effects of the study ( and the logging tool ) on their behavior . Implicit Measures Implicit measures ( Kelly & Teevan , 2003 ) consist of the collection of user - behavior traces that can be recorded without any intervention on the part of the user . Typically , this includes measures such as dwell time ; mouse , keyboard , and scrolling activity ; and interactions with a Web document , such as saving or printing . Implicit measures have been stud - ied as a nonobtrusive method for inferring user interest exten - sively ( Claypool , Le , Waseda , & Brown , 2001 ; Kelly & Belkin , 2001 ; Morita & Shinoda , 1994 ; Oard & Kim , 2001 ) . In our research , we are not using implicit measures to infer interest but rather to deﬁne task characteristics . While implicit measures may be used on a per - page basis , we have examined the implicit measures recorded over the course of a task session . The logged measures include dwell time , number of windows opened , number of pages loaded , the use of Web browser navigation mechanisms , time of day , the use of Google , the use of site - speciﬁc searches , and Web browser functions . Previous researchers ( Mat - Hassan & Levene , 2005 ; Seo & Zhang , 2000 ) have used implicit measures to explore information - seeking behavior on the Web ; however , we are studying users’ information - seeking behavior over a wide range of information needs rather than within a single portal or dataset . Research Questions This exploratory research was conducted to answer the following research questions : R1 : What Web browser functionalities are currently being used during Web - based information - seeking tasks ? In particular , we are interested in the usage ( or lack thereof ) of Web browser navigation mechanisms ( e . g . , auto - complete , bookmarks , history ) , browser functions ( e . g . , windows and pages loaded , use of copy / cut / paste ) , and search tools dur - ing Fact Finding , Information Gathering , Browsing , and Transactions . R2 : Are there differences in the patterns of use of Web browser functionality across Web - based information - seeking tasks ? We are interested in whether there are signiﬁcant differ - ences in how participants interact with their Web browser between Fact Finding , Information Gathering , Browsing , and Transactions . The primary goals of this work are to gain a better under - standing of ( a ) the types of Web browser functionalities ( e . g . , number of pages viewed , windows opened , use of Web browser navigation mechanisms ) currently being used during information - seeking tasks , ( b ) the differences in the use of these functionalities across the different information - seeking tasks , and ( c ) the types of tasks users are currently perform - ing on the Web . This new understanding may provide insight into how to better support Web - based information seeking . Therefore , we expect that the results of this research can be used to guide the design of more effective tools to better support users in their information - seeking tasks on the Web . Methodology To address our primary research questions ( R1 and R2 ) , two forms of data collection were needed . First , participants’ Web usage and Web browser interactions were logged over the course of the ﬁeld study using a custom - built Web browser . Second , participants were asked to use an electronic diary to describe and categorize their Web usage according to a deﬁned categorization . In advance of the ﬁeld study , a pilot study and a focus group were conducted to evaluate two electronic - diary techniques and reﬁne the task categoriza - tion provided to participants . This section ﬁrst describes the pilot study and the focus group , followed by a description of the methodology and data - collection strategies used in con - ducting the ﬁeld study . Pilot Study A 4 - day pilot study was conducted with 6 participants , all recruited from within our research lab at Dalhousie University . Participants were asked to use a custom Web browser for all their Web usage during the pilot study , which logged all interactions with the browser ( including URLs visited ) . Participants also were asked to categorize their Web usage according to the following ﬁve categories : Fact Finding , Information Gathering , Monitoring , Browsing , and Other . During a 15 - min training session before the pilot study , par - ticipants were introduced to the task categorization , and each category was carefully explained . Upon completion of the study , participants completed a postsession question - naire , which allowed us to explore their experiences with the study software ( i . e . , Web browser and electronic diaries ) and the task categorization . One goal of the pilot study was to determine which of two electronic - diary methods allowed participants to more easily and more accurately record task information related to their Web usage . The ﬁrst electronic - diary method required users to provide task information in real time using a toolbar available within the custom Web browser . The second method required users to record their task information at the end of each day using a task diary . Participants in the pilot study used the tool - bar for half the pilot study ( 2 days ) and the task diary for the other half . The order in which the participants used the two different methods was counterbalanced . The results of the pilot study found that the participants were equally split on overall preference and ease of use for the two input methods ; however , most participants ( n (cid:1) 5 ) reported that they felt they JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1003 DOI : 10 . 1002 / asi were more accurate in their task assignments when using the toolbar . Since the participants were equally split on the two techniques in terms of ease of use and overall preference , we decided to provide the study participants with both methods , allowing them to use either as needed . The second goal of the pilot study was to evaluate how well participants were able to categorize their Web usage according to the ﬁve task categories ( i . e . , Fact Finding , Infor - mation Gathering , Monitoring , Browsing , and Other ) . These ﬁve categories had been chosen based on previous work on information - seeking behavior on the Web ( Choo et al . , 2000 ; Morrison et al . , 2001 ; Sellen et al . , 2002 ) . Before starting the ﬁeld study , we needed to verify that the categories reﬂected most of the tasks in which users engage on the Web while remaining easy to understand and relatively distinct . Partici - pants struggled with the task of Monitoring because it often led to new tasks and was hard to distinguish from “re - Fact Finding” or “re - Browsing . ” One example given was reading online comics . A participant was unsure whether repeatedly reading the same comic strip was browsing or monitoring . Additionally , half of the participants ( n (cid:1) 3 ) reported that it was difﬁcult to distinguish between Fact Finding and Infor - mation Gathering . Participants also used the category “Other” for several types of tasks , most notably e - mail . Based on these results , a focus group ( described in the next section ) was held to reﬁne the task categorizations . The pilot study also was an opportunity to detect “bugs” within the customized Web browser and logging tools as well as reﬁne the training materials provided to participants . Focus Group for Task Reﬁnement Ten participants from the Faculty of Computer Science at Dalhousie University ( students and faculty ) took part in an informal focus group , none of whom had taken part in the pilot study . We selected 40 task descriptions from the larger set of task descriptions collected during the pilot study to use during the focus group . Each task description was printed on an index card and spread out on a large table ( as shown in Figure 1 ) . Examples of task descriptions included “Searching for papers on direct input , ” “Looking for the ﬁnal Superbowl score , ” and “Updating my blog . ” The participants were asked to work together as a group to organize the tasks and form a consensus on categories based on the goal of the task . Although some participants had backgrounds in Web behav - ior and information science research , the focus - group partic - ipants were not informed of the categories used in the pilot study or in previous literature . During the hour - long session , participants rearranged the task groupings several times . The content and number of categories ﬂuctuated continually during the course of the ses - sion . After much discussion among the participants , the cate - gories began to stabilize , and six ﬁnal categories emerged ( shown in Table 2 ) . We labeled the categorizations produced by the focus - group participants as Looking for Speciﬁc Infor - mation , Passing Time & Entertainment , Transactions & Com - munication , Information Gathering , Routine & Hobby , and Monitoring . The task categories that evolved from this focus group were in fact very similar to the tasks reported in the literature . Based on the ﬁndings of our pilot study , we hypothesized that Monitoring is an activity within information - seeking tasks rather than an independent task and therefore eliminated this category . We also merged the categories Passing Time & Entertainment and Routine & Hobby into a single category ( i . e . , Browsing ) , as it was difﬁcult to clearly articulate the distinction between these two categories because they are both serendipitous in nature and lack speciﬁc goals . The result - ing task categories , shown in Figure 2 , are Fact Finding , Browsing , Information Gathering , and Transactions . Typically , Transactions ( e . g . , e - mail or banking ) have not been classiﬁed 1004 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi FIG . 1 . Task descriptions . TABLE 2 . Initial task categories after focus group . Task Examples Looking for speciﬁc information Location of a conference workshop Finding percentage of the population who is left - handed Passing time / entertainment Random surﬁng Just browsing Ebay Transactions / communication Checking my e - mail Online banking Information gathering Trying to ﬁnd a reviewer to review a conference paper Looking for references on a topic Routine / hobby Reading my favorite comic Reading blogs Monitoring Checking to see if a project page is up to date so I can send the URL to a colleague Looking up the prices of my stocks FIG . 2 . Task categories . as information - seeking tasks ; however , given the growing proportion of browser activities that these tasks constitute , we felt it was important to study information - seeking behavior in the context of all Web usage . For the ensuing ﬁeld study , the following task descrip - tions were provided to all participants . Fact Finding . Fact Finding is deﬁned as a task in which you are looking for speciﬁc facts or pieces of information . These are usually short - lived tasks that are completed over a single session because you either ﬁnd the answer or you do not . Examples include looking for tomorrow’s weather , a pizza - dough recipe , or printer drivers for your printer . Information Gathering . Information Gathering involves the collection of information , often from multiple sources . This type of task can take place over a single day or may stretch out over several days . Unlike Fact Finding , you do not always know when you have completed the task , and there is no one speciﬁc answer . Examples include building a biblio - graphy for a research paper , researching different car models when buying a new car , or planning an upcoming vacation . Just Browsing . Browsing is deﬁned as a serendipitous task where you may be visiting Web pages with no speciﬁc goal in mind . You may allow yourself to take part for a predeter - mined period of time ( e . g . , “I have 20 min before my meet - ing” ) . This type of task is your classic “Web browsing , ” with no speciﬁc goal in mind other than entertainment or to “see what’s new . ” Sometimes this is done as part of a daily rou - tine . Examples include reading the news , your favorite comic , or a friend’s blog . Transactions . Transactions are deﬁned as tasks in which you are performing an online action . Often , a username / pass - word is associated with the transaction . Examples include Web - based e - mail , banking , or posting to a message board . Other . A ﬁnal category of Other was provided to partici - pants in the event they encountered tasks during the study in which they either were not sure how to categorize or which did not ﬁt within any of the predeﬁned categories . Partici - pants also were instructed to categorize their homepage as “Other” if they did not use it as part of task since it loads each time the Web browser loads , and these pages were not included in our analysis . Field Study Twenty - one university students from Dalhousie University took part in a 1 - week ﬁeld study in March 2005 . Although 23 participants were recruited , only data for 21 participants were analyzed . One of the original participants did not ﬁnish the study , and another participant’s data were not usable because the task descriptions were incomplete and inconsistent . E - mailed recruitment notices were circulated and stated that all university students who were laptop and IE users were eligible to participate . Laptop users were targeted be - cause we could capture most of their Web usage on a single machine and because it facilitated installation of the custom software . In addition , since the Web browser used during the study was a clone of IE , participants were required to be cur - rent users of IE . The academic background of the participants was divided among computer science ( n (cid:1) 11 ) , health informatics ( n (cid:1) 2 ) , business ( n (cid:1) 4 ) , economics ( n (cid:1) 2 ) , kinesiology ( n (cid:1) 1 ) , and arts ( n (cid:1) 1 ) . Participants also were from both the grad - uate and undergraduate communities : computer science ( 7 graduates / 4 undergraduates ) , health informatics ( 2 grad - uates ) , business ( 4 graduates ) , economics ( 2 graduates ) , kine - siology ( 1 undergraduate ) , and arts ( 1 undergraduate ) . The median age - group category of the participants was 20 to 29 years , and the gender was almost evenly split with 11 males and 10 female participants . The median category of Web usage reported by the participants was between 30 to 39 hr of Web usage per week . Although computer science ( CS ) students are typically considered to be highly technical , all participants were experienced Web users . All participants were the primary users of their laptops , and 5 participants also reported that they used a desktop ( either at home or work ) for some of their Web usage . On the ﬁrst day of the study , each participant met with the researcher administering the study for a 1 - hr session . Partic - ipants signed an informed consent which outlined the proce - dures in which they would be involved while taking part in the study . The custom Web browser and logging tools were then installed on the participant’s laptop . The custom Web browser was conﬁgured with the same settings as those the participant used in IE , such as auto - complete , the bookmarks toolbar , and the Google toolbar . Both the demographic and Web browser navigation inventory questionnaires ( descri - bed in the next section ) were administered at this time . The researcher then carefully described the different informa - tion - seeking categories and explained how to use both electronic - diary methods ( i . e . , the task toolbar and the task diary ) to record task information . Participants then took part in a short training exercise in which they were required to complete several short information - seeking tasks using both electronic - diary methods to categorize their Web usage . Finally , participants were given printouts of the task deﬁni - tions ( which also were available online ) and instructions for the study tools . After a 1 - week period , participants returned to meet with the same researcher . At this time , the software was uninstalled from the participant’s laptop , and all logging data were copied on a backup disk . Participants completed a ﬁnal poststudy questionnaire and were paid $ 25 for their participation in the study . Before we began our analysis , a single researcher manu - ally reviewed all participants’data . We encountered some sit - uations in which the task information did not appear to match the URLs recorded . In cases where the behavior was habitual and obvious , the researcher changed the task information . JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1005 DOI : 10 . 1002 / asi In all other cases , the participants were contacted to clarify the task information . Data Collection Over the course of the ﬁeld study , three types of partici - pant data were collected : qualitative task data , implicit mea - sures data , and questionnaire data . Qualitative task data . The qualitative task data consisted of a user’s task categorization ( Fact Finding , Information Gath - ering , Just Browsing , Transactions , and Other ) and a short textual description of the task ( e . g . , “Reading the news , ” “Looking for an e - mail address” ) . Participants were asked to categorize all Web activity recorded by the custom Web browser , shown in Figure 3a , and not just usage thought to be information - seeking related . Based on the results of the pilot study , participants were given the option to provide their task information in real time using the task toolbar shown in Figure 3b , at the end of the day using the task diary shown in Figure 3c , or using a combination of both techniques . Participants who used the toolbar method were instructed to ﬁll in the toolbar before beginning a new task . An auto - complete function was implemented for the textual descrip - tion based on feedback received during the pilot study . Participants quickly built a small library of tasks to choose from when assigning task information for repeated tasks . Tool tips displaying task deﬁnitions were displayed when a participant hovered over one of the task buttons . Participants who chose to use the task diary to assign task information were instructed to do so at the end of day . The task diary , similar to the approach used by Hawkey and Inkpen ( 2005a ) , allowed participants to assign task informa - tion to multiple URLs at once . Similar to the task toolbar , an auto - complete function was implemented for the task diary . The items in the auto - complete function were shared between the toolbar and the task diary . Tool tips displaying task deﬁnitions were displayed when a participant hovered over one of the task buttons . The task diary also allowed all participants to delete any Web site addresses that they were uncomfortable sharing with the researchers involved in the study . It was hoped that this would help encourage partici - pants to work on the Web as they normally would . Regardless of the method used to collect the task infor - mation , each URL visited was associated with a task catego - rization and description . This information was recorded in a log ﬁle , shown in Table 3 , in the following format : Window ID , Date & Time , Page Title , URL , Task Type , and Task Description . Implicit measures . The implicit measures data were col - lected by the custom Web browser , shown in Figure 3a , 1006 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi FIG . 3 . An example of a participant’s Web activity as recorded by the custom Web browser . TABLE 3 . Task information log ﬁle . Window _ ID Date _ Time Page _ Title URL Task _ Type Task _ Description 26229 8 / 14 / 2005 18 : 03 : 48 . 018 MSN . com http : / / www . msn . com Other Homepage 26229 8 / 14 / 2005 18 : 04 : 12 . 273 Local Weather http : / / www . weather . com Fact Finding Weather 26229 8 / 14 / 2005 18 : 04 : 40 : 965 CNN . com http : / / www . cnn . com Just Browsing News 26229 8 / 14 / 2005 18 : 05 : 41 . 812 Travelocity http : / / www . travelocity . com Info Gathering Plan trip 26229 8 / 14 / 2005 18 : 05 : 48 . 572 Guides & Advice http : / / www . dest . travelocity . com Info Gathering Plan trip which was built to mimic IE and provided the same func - tionality , menus , and interface as IE . In our research , we were primarily interested in implicit measures consisting of participants’ direct interactions with the Web browser inter - face . Table 4 displays a classiﬁcation of the implicit mea - sures logged by the custom Web browser . This classiﬁcation was partially based on Byrne , John , Wehrle , and Crow’s ( 1999 ) taskonomy of Web tasks , which was developed to better understand the range of tasks that Web browsers must support . Oard and Kim ( 2001 ) developed a classiﬁcation of observable behaviors based on two dimensions : behavior category ( i . e . , purpose of the category ) and minimum scope of the object being manipulated . While this classiﬁcation was developed in the context of implicit measures , it has a larger focus on documents and content , as opposed to Web browser interface interactions . Oard and Kim’s classiﬁcation also has been further extended by Kelly and Teevan ( 2003 ) and Jansen and McNeese ( 2005 ) . The custom Web browser generated a detailed summary of all user interactions within the browser during each Web session . Two main types of implicit measures were recorded : document complete events ( i . e . , use of Web browser naviga - tion mechanisms ) and browser function events . Document complete events were recorded as each Web page was loaded . A listing of all document complete events is shown in the leftmost column of Table 4 and included navigation mechanisms such as the back button , auto - complete , book - marks , and history . Browser function events consisted of all other menu , button , and shortcut interactions with the Web browser . This included actions such as opening and closing a window , printing or saving a document , and edit functions such as cut / copy / paste . For all logged implicit measures , the data collected consisted of the Window ID , Date & Time , Browser Event ( document _ complete or browser _ function ) , and Description ( of the event ) . Participants did not have access to this log ﬁle ; therefore , the URLs of the pages loaded were omitted in case the corresponding URL was deleted using the task diary . Before analysis , timestamps were used to merge the two log ﬁles shown in Table 3 and in Table 5 . JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1007 DOI : 10 . 1002 / asi TABLE 4 . A classiﬁcation of the implicit measures logged during the ﬁeld study . Browser function events Document complete events a File Edit View Misc . tools Auto - Complete New Select All Toggle Favorites Highlight Search Terms Back Button Window Find Toggle History Internet Options Back Menu Open Copy b Stop Favorites Save As Paste b View Source Forward Button Page Setup Cut b Privacy Report Forward Menu Print Google Toolbar Print Preview History Properties Home Button Close HyperlinksNew Window OtherReload Button Select URL Typed - in URL a Includes navigation conducted through button clicks , shortcut keys , and menu interactions . b We differentiated between cut , copy , and paste that occurred within the Web browser Web page and within the Web browser combo - boxes ( the address ﬁeld and Google toolbar ) . TABLE 5 . Event log ﬁle . Window _ ID Date _ Time Event Description 26229 8 / 14 / 05 18 : 03 : 41 . 139 Browser _ Function Open _ Session 26229 8 / 14 / 05 18 : 03 : 48 . 039 Document _ Complete First _ Page 26229 8 / 14 / 05 18 : 04 : 12 . 404 Document _ Complete Bookmarks _ Toolbar 26229 8 / 14 / 05 18 : 04 : 40 . 985 Document _ Complete Enter _ Key 26229 8 / 14 / 05 18 : 05 : 25 . 659 Browser _ Function Paste _ Combo 26229 8 / 14 / 05 18 : 05 : 41 . 832 Document _ Complete Go _ Button 26229 8 / 14 / 05 18 : 05 : 48 . 582 Document _ Complete Clicked _ Link 26229 8 / 14 / 05 18 : 05 : 52 , 337 Browser _ Function Close _ Session Each participant was asked to e - mail their data to the study researcher at the end of each day using a custom e - mail application . This application e - mailed both log ﬁles to the re - searchers . This allowed the researchers to ensure that partici - pants were correctly recording their data without problems . Researchers also could contact participants if more than 2 days passed without any data submitted to determine if there were any problems . Questionnaires . Participants completed three separate ques - tionnaires over the course of the study . During the prestudy session , a demographic questionnaire was used to collect par - ticipants’ demographic information and current Web usage . An inventory questionnaire of the Web browser navigation mechanisms used also was completed by participants during the prestudy session . Upon completion of the study , partici - pants completed a poststudy questionnaire which examined any difﬁculties that they encountered during the study . Results General Observations In this section , we report general observations describing the characteristics of participants’ task sessions . As previ - ously stated , a task session is deﬁned as a period of contin - uous Web usage , annotated with the same task information , and with no break in usage greater than 25 . 5 min . In the case of Transactions , a new task session was identiﬁed using either the 25 . 5 - min lapse in activity or an explicit ses - sion logout indicated by the existence of the “logout” string in a Transaction URL ( e . g . , www . mail . yahoo . com / logout ) . Overall , participants recorded a total of 1 , 192 task sessions involving 13 , 498 pages over the week - long study . The mean number of task sessions completed per participant was 56 . 8 ( median (cid:1) 52 , SD (cid:1) 31 . 97 ) with a range of 16 to 140 tasks . A breakdown of the number of task sessions com - pleted by each participant is shown in Table 6 . We found no signiﬁcant difference between the CS and non - CS groups in terms of the number of task sessions completed . The CS group recorded a mean of 58 . 4 task sessions while the non - CS group recorded a mean of 54 . 1 task sessions . The break - down of all task sessions across all participants is shown in Figure 4 . Fact Finding task sessions accounted for 18 . 3 % ( n (cid:1) 218 ) of all Web usage . Looking for weather information appeared to be the most common Fact Finding task , accounting for 11 . 5 % ( n (cid:1) 25 ) of task sessions in this category . Other com - mon Fact Finding tasks included looking for course - or assignment - related material , song lyrics , and speciﬁc soft - ware . Fact Finding tasks appeared to be somewhat split be - tween ludic and school - or work - related activities . Information Gathering task sessions accounted for 13 . 4 % ( n (cid:1) 160 ) of all Web usage . There was no single representa - tive task , but common tasks included job hunting , course - or project - related research , researching a new purchase ( e . g . , a computer or an iPod ) , and course / admissions information . Many of the Information Gathering tasks were related to technology concepts . Browsing task sessions accounted for 19 . 9 % ( n (cid:1) 237 ) of all Web usage . Browsing tasks appeared to be primarily ludic in nature and consisted of news reading in 40 . 5 % ( n (cid:1) 96 ) of tasks in this category . Other common tasks included reading blogs , visiting gaming - related sites , and reading music / TV / movie - related Web pages . Transactions were the most frequently recorded task sessions , accounting for 46 . 7 % ( n (cid:1) 557 ) of all Web usage . Transactions were primarily made up of Web - based e - mail , accounting for 80 . 4 % ( n (cid:1) 448 ) of all transactions and 38 % 1008 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi TABLE 6 . The breakdown of tasks by participant . Total tasks by FF IG BR TR OT participant ( % ) ( % ) ( % ) ( % ) ( % ) 61 3 . 3 9 . 8 . 0 83 . 6 3 . 3 45 44 . 4 15 . 6 . 0 40 . 0 . 0 37 . 0 13 . 5 13 . 5 67 . 6 5 . 4 68 30 . 9 2 . 9 10 . 3 55 . 9 . 0 140 11 . 4 19 . 3 21 . 4 47 . 9 . 0 32 18 . 8 28 . 1 21 . 9 31 . 3 . 0 75 4 . 0 18 . 7 26 . 7 50 . 7 . 0 39 17 . 9 43 . 6 35 . 9 2 . 6 . 0 16 31 . 3 . 0 12 . 5 50 . 0 6 . 3 52 32 . 7 9 . 6 7 . 7 25 . 0 25 . 0 55 5 . 5 7 . 3 20 . 0 67 . 3 . 0 70 20 . 0 15 . 7 58 . 6 5 . 7 . 0 52 40 . 4 15 . 4 5 . 8 36 . 5 1 . 9 41 31 . 7 4 . 9 4 . 9 58 . 5 . 0 92 32 . 6 1 . 1 46 . 7 19 . 6 . 0 122 4 . 9 9 . 8 19 . 7 64 . 8 . 8 20 25 5 . 0 . 0 70 . 0 . 0 25 8 . 0 12 . 0 36 . 0 44 . 0 . 0 42 14 . 3 14 . 3 16 . 7 54 . 8 . 0 28 60 . 7 21 . 4 3 . 6 14 . 3 . 0 80 5 . 0 17 . 5 8 . 8 68 . 8 . 0 Note . FF (cid:1) Fact Finding ; IG (cid:1) Information Gathering ; BR (cid:1) Browsing ; TR (cid:1) Transactions ; OT (cid:1) Other . FIG . 4 . The breakdown of all task sessions across all participants . of all Web usage . Other types of transactions recorded by our participants included online bill payments and blog / message board entries . Finally , only a small number of task sessions were cate - gorized as Other , and they accounted for 1 . 7 % ( n (cid:1) 20 ) of all Web usage . These were tasks such as viewing Web pages during Web development and may have been speciﬁc to our user population ( i . e . , mainly CS students ) . Repeated Tasks We examined the occurrence of repeated tasks on a per - participant basis . A task was deﬁned as repeated if , within a participant’s list of tasks , there were multiple occurrences of a task session with the same task categorization and similar task description . For instance , two task sessions categorized as Fact Finding and labeled as “checking the weather” and “weather , ” respectively , were recorded as repeated tasks . Of the 218 Fact Finding task sessions , we found that 55 . 5 % ( n (cid:1) 121 ) were repeated at least once . This category had the lowest proportion of repeated tasks . There appear to be three main reasons why Fact Finding tasks were repeated : monitoring , reﬁnding , and task variants . When monitoring , participants were looking for speciﬁc dynamic information such as the current weather forecast . When reﬁnding , partic - ipants were looking to return to a previously found piece of static information . Task variants occurred when participants were searching for related pieces of speciﬁc information , such as looking for programming resources . One example of this was a participant who labeled two tasks “looking for Java documentation , ” where in one case he was looking for information on hash tables while in another case he was looking for Java documentation on substrings . For Information Gathering task sessions , 58 . 8 % ( n (cid:1) 94 ) of tasks were repeated at least once . Information Gathering tasks appeared to be repeated because participants continued with their tasks at a later time . Since Information Gathering tasks tend to be longer in duration , they were often broken up over a day or even over several days . Among some partici - pants , we saw Information Gathering tasks that stretched over as many as 6 days , such as a participant who was researching graduate - school admission information . Browsing tasks were highly repetitive , as 84 . 4 % ( n (cid:1) 200 ) of task sessions were repeated at least once . Browsing tasks were primarily habitual or monitoring tasks such as checking the news or a friend’s blog . We observed many participants who repeated the same Browsing tasks daily over the course of the study . Transactions were the most often repeated task , with 95 . 2 % ( n (cid:1) 530 ) of task sessions repeated at least once . As previously mentioned , Transactions consisted mainly of e - mail , which often was accessed by participants several times during the day . Implicit Measures We were interested in studying elements of user behavior while users were engaging in information - seeking tasks that could be collected implicitly ; that is , without any intervention from the user . We studied the following elements : dwell time , windows opened , pages loaded , use of Web browser naviga - tion tools , time of day , use of Google , use of site - speciﬁc searches , and use of Web browser functions . We present de - scriptive statistics and the results of statistical analysis where appropriate . Raw data were analyzed using nonparametric one - way ANOVAs ( Kruskal – Wallis ) because the data did not exhibit a normal distribution . Nominal data were analyzed using chi - square tests . An alpha value of 0 . 05 was used for all omnibus tests . Pairwise post hoc tests were conducted using the Mann – Whitney and chi - square tests , and alpha values were determined using the Bonferroni correction to decrease the possibility of Type 1 errors . Tasks labeled as Other ap - peared to be speciﬁc to our population and accounted for only a small percentage of all tasks ( 1 . 7 % ) ; therefore , we only re - port descriptive statistics on this task , and it was not included in any statistical analyses . Dwell time . In a ﬁeld setting , it can be problematic to accu - rately record dwell time ( i . e . , the amount of time participants spend reading and interacting with a particular Web page ) . Although we can record the time of each page access , it often is not possible to determine where a participant’s atten - tion is directed . In this study , we were interested in the amount of time participants spent completing their information - seeking tasks . Task duration was measured from the time the ﬁrst page in a task was loaded until the time in which the last page was loaded . This means that duration was measured only for task sessions in which more than one page was loaded , excluding 192 ( 16 % ) sessions . This method resulted in a smaller , but more reliable , set of task - duration data . How - ever , note that these data are not as reliable as are laboratory - collected task - duration data . The mean time , per task session , is shown in Figure 5 . The mean duration recorded for Fact Finding task sessions JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1009 DOI : 10 . 1002 / asi FIG . 5 . The mean time ( in seconds ) that participants spent in completing each task . was 481 . 6 s ( SD (cid:1) 1169 . 9 ) . The mean duration of Informa - tion Gathering task sessions was 1087 . 6 s ( SD (cid:1) 2048 . 0 ) . The mean duration recorded for Browsing task sessions was 648 . 1 s ( SD (cid:1) 856 . 5 ) . The mean duration for Transactions was 468 . 7 s ( SD (cid:1) 1084 . 4 ) . Finally , the mean duration for Other task sessions was 437 . 9 s ( SD (cid:1) 692 . 5 ) . Signiﬁcant differences were found for dwell time be - tween task sessions , Kruskal – Wallis H (cid:1) 40 . 720 , df (cid:1) 3 , p (cid:1) . 000 . Pairwise comparisons were conducted using the Mann – Whitney test , and an adjusted alpha level of 0 . 008 was used . Information Gathering task sessions were signiﬁ - cantly longer than both Fact Finding ( p (cid:1) . 000 ) and Trans - actions ( p (cid:1) . 000 ) sessions , but not Browsing sessions . Browsing task sessions also were signiﬁcantly longer than both Fact Finding ( p (cid:1) . 000 ) and Transactions ( p (cid:1) . 000 ) sessions . The task - duration data exhibited a high degree of vari - ability for each task type . Task duration can be inﬂuenced by the task complexity , familiarity with the task ( e . g . , habitual tasks ) , and domain knowledge ( Hölscher & Strube , 2000 ) . The duration of a Transactions task session , for instance , may depend on the amount of e - mail an individual receives over the course of a day and the number of times the e - mail account is accessed . Windows opened . The number of windows opened during each task session was calculated by counting the number of unique window IDs recorded during a single task session . The custom Web browser provided a pop - up blocker , so pop - up advertisements did not have a large impact on the number of windows opened . Note that Web - based e - mail clients differ in the number of windows launched for e - mail operations . For example , one e - mail client we logged opened a new win - dow for each composed or read message while another used the existing browser window for all operations . Therefore , the number of windows opened during Transactions task ses - sions may be highly variable . In general , a low number of windows was opened across the different task sessions ; in total , 1 , 934 windows were loaded during the ﬁeld study . Figure 6 displays the mean number of windows opened across all task sessions . The mean number of windows opened during Fact Finding task sessions was 1 . 48 ( median (cid:1) 1 , SD (cid:1) 1 . 34 ) . For Information Gathering task sessions , the mean number of windows opened dur - ing a task was 2 . 28 ( median (cid:1) 1 , SD (cid:1) 3 . 21 ) . For Browsing task sessions , the mean number of windows opened was 1 . 43 ( median (cid:1) 1 , SD (cid:1) 1 . 05 ) . The mean number of windows opened during Transactions was 1 . 58 ( median (cid:1) 1 , SD (cid:1) 1 . 34 ) . Finally , the mean number of windows opened during Other task sessions was 1 . 35 ( median (cid:1) 1 , SD (cid:1) 0 . 99 ) . Signiﬁcant differences were found for the number of win - dows opened between tasks , Kruskal – Wallis H (cid:1) 15 . 650 , df (cid:1) 3 , p (cid:1) . 001 . Pairwise comparisons were conducted using the Mann – Whitney test , and an adjusted alpha level of 0 . 008 was used . More windows were opened during Information Gathering task sessions than they were in both Fact Finding ( p (cid:1) . 003 ) and Browsing ( p (cid:1) . 005 ) sessions . Signiﬁcant differences also were found between Fact Finding and Transactions ( p (cid:1) . 006 ) sessions . Due to the small number of windows opened overall , these results do not have strong practical signiﬁcance . We reﬂect on the small number of windows opened during the task sessions in the Discussion . Pages loaded . The number of pages loaded during a task session was calculated by counting the number of top - level frames loaded . This means that for pages with frames , only one page was counted . Similar to the number of windows opened , the number of pages loaded for Transactions was inﬂuenced by the Web - based e - mail services , some of which loaded a new page for each e - mail viewed or sent while others loaded a single page for the entire session . Figure 7 displays the mean number of pages loaded across all task sessions . In total , 13 , 498 pages were loaded during the field study . The mean number of pages loaded during Fact 1010 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi FIG . 6 . The mean number of windows opened during each task . FIG . 7 . The mean number of pages opened during each task . Finding task sessions was 8 . 1 ( median (cid:1) 5 , SD (cid:1) 9 . 7 ) . During Information Gathering task sessions , the mean num - ber of pages loaded was 31 . 4 ( median (cid:1) 8 , SD (cid:1) 61 . 8 ) . For Browsing task sessions , the mean number of pages loaded was 10 . 3 ( median (cid:1) 5 , SD (cid:1) 15 . 2 ) . During Transactions , the mean number of pages loaded was 7 . 3 ( median (cid:1) 4 , SD (cid:1) 10 . 0 ) . Finally , during Other task sessions , the mean number of pages loaded was 11 . 2 ( median (cid:1) 4 , SD (cid:1) 21 . 2 ) . Signiﬁcant differences were found for the number of pages loaded between task sessions , Kruskal – Wallis H (cid:1) 49 . 904 , df (cid:1) 3 , p (cid:1) . 000 . Pairwise comparisons were con - ducted using the Mann – Whitney test , and an adjusted alpha level of 0 . 008 was used . The number of pages viewed during Information Gathering task sessions was signiﬁcantly higher than that for all other tasks : Fact Finding ( p (cid:1) . 000 ) , Brows - ing ( p (cid:1) . 000 ) , and Transactions ( p (cid:1) . 000 ) . Web browser navigation mechanisms . The use of Web browser navigation mechanisms was logged during the ﬁeld study ( see Table 7 for a complete listing of navigation mech - anisms logged ) . The custom Web browser differentiated be - tween the use of the auto - complete function ( Auto - Complete ) , selecting a URL from the drop - down address menu ( Select URL ) , and typing a URL directly into the address bar ( Typed - in URL ) ; the use of these navigation mechanisms was logged as separate navigation events . Navigation classiﬁed as “Other” consisted of navigation that was not explicitly captured , such as new windows launched by a Web page and clicked links that could not be detected ( due to JavaScript , AJAX , etc . ) . In these cases , we could detect that a high - level document com - plete event ﬁred ( i . e . , a single page loaded ) , but could not identify the direct source of the navigation event . We observed that these events often occurred within Transactions ( e . g . , Web - based e - mail and online applications ) . New Window typ - ically consisted of new windows initiated either by the user or automatically from a script ; however , the custom Web browser provided a pop - up blocker , so pop - up advertisements likely did not account for much of the new - window usage . The cus - tom Web browser provided participants with access to their IE auto - complete , bookmarks , history , and select URL . This meant that participants had full access to their navigation history accrued before the study . In this analysis , we were interested in the most common methods of navigation when initiating a new task session . A more detailed exploration of the impact of task and individual differences on the use of Web browser navigation mechanisms can be found in Kellar , Watters , and Shepherd ( 2006b ) . We observed several types of navigation mechanisms used to initiate new tasks : auto - complete , back button , clicked links , bookmarks , Google toolbar , URLs selected from the drop - down address menu , and typed - in URLs . Overall , typed - in URLs were the most common navigation mechanism used to initiate a new task session . The proportions of navigation mechanisms used across all tasks are shown in the bar chart displayed in Figure 8 . For ease of readability , the navigation mechanisms with minimal use were not included in this ﬁgure . Within Fact Finding task sessions , there were signiﬁcant differences between the navigation mechanisms used , x 2 ( 9 , N (cid:1) 218 ) (cid:1) 233 . 101 , p (cid:1) . 000 . Typed - in URLs were the most common method ( n (cid:1) 73 ; 33 . 5 % ) for initiating Fact Finding task sessions , followed by the Google toolbar ( n (cid:1) 51 ; 23 . 4 % ) and bookmarks ( n (cid:1) 32 ; 14 . 7 % ) . Pairwise com - parisons ( a (cid:1) 0 . 005 ) showed that with the exception of the Google toolbar , typed - in URLs were used more often than were all other navigation mechanisms ( p s (cid:1) . 000 ) . Simi - larly , the use of the Google toolbar was signiﬁcantly higher than were all other navigation mechanisms ( p s (cid:1) . 000 ) , with the exception of bookmarks . Within Information Gathering task sessions , there were signiﬁcant differences between the navigation mechanisms used , x 2 ( 7 , N (cid:1) 160 ) (cid:1) 78 . 800 , p (cid:1) . 000 . These tasks were commonly initiated through typed - in URLs ( n (cid:1) 42 ; 26 . 3 % ) , followed by the Google toolbar ( n (cid:1) 41 ; 25 . 6 % ) and the auto - complete function ( n (cid:1) 26 ; 16 . 3 % ) . The use of naviga - tion mechanisms when initiating Information Gathering tasks appears to be more evenly distributed ; pairwise comparisons ( a (cid:1) 0 . 005 ) did not reveal a signiﬁcant difference between the use of typed - in URLs , the Google toolbar , and the auto - complete function . There were signiﬁcant differences within Browsing task sessions between the navigation mechanisms used , x 2 ( 10 , N (cid:1) 237 ) (cid:1) 216 . 878 , p (cid:1) . 000 . Browsing task sessions were most commonly initiated through typed - in URLs ( n (cid:1) 73 ; 30 . 8 % ) , followed by bookmarks ( n (cid:1) 50 ; 21 . 1 % ) , and selected URLs ( n (cid:1) 24 ; 10 . 1 % ) . Pairwise comparisons ( a (cid:1) 0 . 005 ) JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1011 DOI : 10 . 1002 / asi TABLE 7 . The navigation mechanisms logged by the custom Web browser . Auto Complete Forward Menu New Window Back Button Google Toolbar Other Back Menu History Reload Button Favorites Home Button Select URL Forward Button Hyperlinks Typed - in URL FIG . 8 . The proportion of navigation tools used to initiate a new task . did not reveal a signiﬁcant difference between the use of typed - in URLs and bookmarks ; however , typed - in URLs and bookmarks were used more often than were all other browser navigation mechanisms ( p s (cid:1) . 000 ) . Within Transactions , there were signiﬁcant differences between the navigation mechanisms used , x 2 ( 10 , N (cid:1) 557 ) (cid:1) 1099 . 853 , p (cid:1) . 000 . Transactions were primarily initiated through two mechanisms : bookmarks ( n (cid:1) 200 ; 35 . 9 % ) and typed - in URLs ( n (cid:1) 194 ; 34 . 8 % ) . Pairwise comparisons ( a (cid:1) 0 . 005 ) did not show a signiﬁcant difference between the use of these two mechanisms ; however , bookmarks and typed - in URLs were used more often than were all other mechanisms ( p s (cid:1) . 000 ) . Finally , task sessions labeled as “Other” were most commonly initiated using typed - in URLs ( n (cid:1) 9 ; 45 % ) . Time of day . The time during which a task session was initi - ated was categorized across four time - of - day categories : morning ( 6 A . M . – 11 : 59 A . M . ) , afternoon ( 12 : 00 P . M . – 5 : 59 P . M . ) , evening ( 6 : 00 P . M . – 11 : 59 P . M . ) , and overnight ( 12 : 00 A . M . – 5 : 59 A . M . ) . Previous research by Beitzel , Jensen , Chowdhury , Grossman , and Frieder ( 2004 ) reported that time of day had an impact on the popularity and uniqueness of topically categorized queries . In our research , we were interested in knowing , for each task type , the most common time of day in which that task was initiated . The bar chart presented in Figure 9 shows the proportion of task sessions by time of day . Pairwise comparisons were conducted using chi - square tests , with an adjusted alpha level of 0 . 008 . Within Fact Finding task sessions , there were signiﬁcant differences between time of day , x 2 ( 3 , N (cid:1) 218 ) (cid:1) 63 . 505 , p (cid:1) . 000 . Fact Finding task sessions most commonly occurred in the afternoons ( n (cid:1) 98 ; 45 % ) . Pairwise comparisons re - vealed signiﬁcant differences between occurrences in the afternoon and all other times of day : morning ( p (cid:1) . 000 ) , evening ( p (cid:1) . 003 ) , and overnight ( p (cid:1) . 000 ) . Within Information Gathering task sessions , there were signiﬁcant differences between time of day , x 2 ( 3 , N (cid:1) 160 ) (cid:1) 28 . 750 , p (cid:1) . 000 . Information Gathering task sessions most commonly occurred in the evenings ( n (cid:1) 65 ; 40 . 6 % ) . Pair - wise comparisons revealed a signiﬁcant difference between occurrences in the evenings and both morning ( p (cid:1) . 000 ) and overnight ( p (cid:1) . 000 ) , but not the afternoon . There were signiﬁcant differences within Browsing task sessions between time of day , x 2 ( 3 , N (cid:1) 237 ) (cid:1) 32 . 755 , p (cid:1) . 000 . Browsing tasks were most commonly recorded in the afternoon ( n (cid:1) 89 ; 37 . 6 % ) . Pairwise comparisons revealed a signiﬁcant difference between occurrences in the afternoon and in both the morning ( p (cid:1) . 001 ) and overnight ( p (cid:1) . 000 ) , but not in the evening . Within Transactions , there were signiﬁcant differences between time of day , x 2 ( 3 , N (cid:1) 557 ) (cid:1) 87 . 919 , p (cid:1) . 000 . Transactions were very closely split between afternoons ( n (cid:1) 180 ; 32 . 3 % ) and evenings ( n (cid:1) 200 ; 35 . 9 % ) . Pairwise com - parisons did not ﬁnd a signiﬁcant difference between these two time periods ; however , signiﬁcant differences were found between these two time periods and both mornings ( p (cid:1) . 000 ) and overnight ( p (cid:1) . 000 ) . Finally , task sessions classiﬁed as “Other” were almost evenly distributed over mornings ( n (cid:1) 6 ; 30 % ) , afternoons ( n (cid:1) 6 ; 30 % ) , and evenings ( n (cid:1) 7 ; 35 % ) . Use of Google . The use of Google has become ubiquitous in today’s Web environment . Aula et al . ( 2005 ) found that 95 . 3 % of 236 survey respondents reported using Google as their primary search engine . We examined the use of Google by participants across the different task sessions . All URLs were mined for the string “Google . ” After eliminating Google e - mail ( GMail ) and within - site searches ( provided by Google and addressed in the next section , site - speciﬁc searches ) , we recorded the number of queries submitted to Google per task . We saw very little evidence of the use of al - ternate search engines ( (cid:2) 1 % ) , with the exception of those used for site - speciﬁc searches . Google was accessed in 78 of 218 ( 35 . 8 % ) of the Fact Find - ing task sessions . When Google was used within Fact Finding task sessions , the mean number of queries submit - ted was 2 . 18 ( SD (cid:1) 3 . 90 ) . Within Information Gathering task sessions , Google was used in 66 of 160 ( 41 . 25 % ) of all task ses - sions . The mean number of queries submitted per Informa - tion Gathering task sessions was 2 . 72 ( SD (cid:1) 3 . 08 ) . The use of Google dramatically declined for the remaining task ses - sions , occurring only in 8 . 43 % of Browsing task sessions , 0 . 005 % of Transactions , and 0 . 05 % of Other task sessions . We found that in addition to the main Google search engine , par - ticipants also used the Google Image , Scholar , and Map searches . There were no signiﬁcant difference in the use of Google between Fact Finding and Information Gathering task sessions nor was there any difference in the number of queries between the two tasks . We also examined the difference in the query length submitted to Google between Fact Finding and Information 1012 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi FIG . 9 . The proportion of tasks across time of day . Gathering task sessions . The mean query length for Fact Finding task sessions was 4 . 72 words ( SD (cid:1) 2 . 57 ) , com - pared with 3 . 32 words ( SD (cid:1) 2 . 26 ) for Information Gather - ing task sessions . On average , Fact Finding queries were longer than were Information Gathering queries , t ( 337 ) (cid:1) 5 . 360 , p (cid:1) . 000 . Due to the nature of the task , participants often submitted very speciﬁc query strings when completing Fact Finding tasks , such as excerpts of song lyrics , partial or full publication titles , and speciﬁc questions ( e . g . , “how do I oil the heat sink fan” ) . The queries submitted during Infor - mation Gathering task sessions were more vague and tended to represent general topics rather than speciﬁc questions . Site - speciﬁc searches . The use of site - speciﬁc searches also was examined . These were deﬁned as searches that were conducted within a speciﬁc Web site or domain . To retrieve these instances , we collected all URLs which con - tained the term “q (cid:1) ” , which is a typical string used to repre - sent queries within a URL . We then removed all Google searches processed in the previous section , but included searches of individual domains powered by Google . The most common site - speciﬁc searches were product searches within commercial Web sites and searches within online databases or libraries . Overall , we saw a small num - ber of site - speciﬁc searches ( n (cid:1) 27 in total ) , most of which occurred within Information Gathering tasks ( n (cid:1) 19 ; 70 . 3 % ) . Six instances ( n (cid:1) 6 ; 22 . 2 % ) were found within Fact Find - ing tasks and two ( n (cid:1) 2 ; 7 . 4 % ) were found within Browsing tasks . The small amount of data collected did not warrant any statistical analysis . Use of browser functions . Browser functions were logged as they were used , and their use was associated with the task ses - sion being performed in the corresponding window . We were interested in how the use of these functions differed across task sessions . The following browser functions were logged dur - ing the ﬁeld study : copy , paste , cut , ﬁnd on page , print , save , and the creation of new bookmarks . A total of 178 browser functions were recorded across all participants , and the break - down within task sessions is shown in Table 8 . Information Gathering task sessions recorded the highest number of browser functions ( n (cid:1) 97 ; 54 . 5 % ) , followed by Fact Finding ( n (cid:1) 33 ; 18 . 5 % ) , Transactions ( n (cid:1) 25 ; 14 . 0 % ) , and Browsing ( n (cid:1) 23 ; 12 . 9 % ) . Signiﬁcant differences were found between task and the following tools : creating book - marks , x 2 ( 3 , N (cid:1) 45 ) (cid:1) 34 . 022 , p (cid:1) . 000 , using the ﬁnd function , x 2 ( 2 , N (cid:1) 17 ) (cid:1) 8 . 941 , p (cid:1) . 001 , copying , x 2 ( 3 , N (cid:1) 39 ) (cid:1) 17 . 308 , p (cid:1) . 001 , and pasting , x 2 ( 3 , N (cid:1) 67 ) (cid:1) 24 . 164 , p (cid:1) . 000 . Pairwise comparisons ( a (cid:1) 0 . 008 ) using chi - square analysis found signiﬁcant differences between Information Gathering and all other tasks for creating bookmarks , copy , and paste ( p s (cid:2) . 004 ) . Within Information Gathering tasks , the most common functions included pasting text ( n (cid:1) 34 ; 35 . 1 % ) , copying text ( n (cid:1) 21 ; 21 . 6 % ) , and creating new bookmarks ( n (cid:1) 28 ; 28 . 9 % ) . Copied text typically consisted of HTML content ( Web page ) , and pasted text typically con - sisted of URLs and search strings pasted to the address and Google toolbar ( combo boxes ) . Discussion Summary of Task Characteristics Based on the results presented in the previous section , we now provide a general characterization of each type of task for our sample population , recognizing that the task types are complex . Table 9 provides a summary of characteristics for each type of task . We have omitted time of day since it is likely to be speciﬁc to the population sampled in this study . Fact Finding task sessions were relatively short - lived , lasted 8 min on average , and recorded an average of 1 . 5 win - dows opened during a task session . Just over half of all Fact Finding tasks were repeated at least once , and this was attrib - uted to reﬁnding information , monitoring information , and conducting sets of related tasks . Tasks appeared to be evenly split between work - or school - related and personal tasks . We observed a relatively small number of pages viewed during Fact Finding task sessions . Typed - in URLs and the Google toolbar were the most common navigation mechanism used to initiate Fact Finding task sessions . The search nature of this task was reﬂected in the use of Google during 35 % of Fact Finding task sessions , and participants tended to submit longer , more speciﬁc queries . The use of browser functions was minimal during task sessions of this type . Participants exhibited a rich set of behavior during Infor - mation Gathering task sessions . This task was the longest in duration , averaging 18 min per task session . On average , 2 . 3 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1013 DOI : 10 . 1002 / asi TABLE 8 . The use of browser functions within tasks . FF IG BR TR Function ( n (cid:2) 33 ) ( n (cid:2) 97 ) ( n (cid:2) 23 ) ( n (cid:2) 25 ) Copy Web page 1 / 218 17 / 160 0 0 (cid:2) 1 % 10 . 6 % Combo box 5 / 218 4 / 160 6 / 237 6 / 557 2 . 3 % (cid:2) 1 % (cid:2) 1 % (cid:2) 1 % Paste Web page 0 2 / 160 0 1 / 557 (cid:2) 1 % (cid:2) 1 % Combo box 13 / 218 32 / 160 9 / 237 10 / 557 6 % 20 % (cid:2) 1 % (cid:2) 1 % Cut Web page 0 1 / 160 0 0 (cid:2) 1 % Combo box 0 0 0 0 Find on Page 5 / 218 11 / 160 0 1 / 557 2 . 3 % 6 . 9 % (cid:2) 1 % Print 3 / 218 2 / 160 0 2 / 557 (cid:2) 1 % (cid:2) 1 % (cid:2) 1 % Save 2 / 218 0 0 0 (cid:2) 1 % Add Bookmark 4 / 218 28 / 160 8 / 237 5 / 557 1 . 8 % 17 . 5 % (cid:2) 1 % (cid:2) 1 % Note . FF (cid:1) Fact Finding ; IG (cid:1) Information Gathering ; BR (cid:1) Browsing ; TR (cid:1) Transactions . windows were opened per task session . We observed a rela - tively high number of pages loaded during Information Gathering task sessions . Many Information Gathering tasks were related to participants’ course or research work . Over half of all Information Gathering tasks were repeated at least once . Typed - in URLs , the Google toolbar , and auto - complete were the most common methods of initiating a new task ses - sion . We observed the largest number of Google searches and within - site searches during this task , and the queries submit - ted to Google appeared to be shorter , and more general , than were Fact Finding queries . We also observed the highest usage of browser functions within Information Gathering tasks . Participants were observed creating new bookmarks , using the copy and paste functions , and using the ﬁnd on this page function . The average length of a Browsing task session was 10 min , and an average of 1 . 4 windows were opened during the ses - sion . We observed a relatively small number of pages viewed during Browsing task sessions . The most dominant charac - teristic of Browsing was the habitual nature of this task . On average , almost 85 % of Browsing task sessions were repeated at least once , and we observed a high degree of monitoring within this task . The most common methods of navigation when initiating a new task session were typed - in URLs and bookmarks , which support the repetitive / monitoring nature of Browsing tasks . Unlike Fact Finding and Information Gathering , participants seldom used Google or site - speciﬁc searches when BR . The use of browser functions was mini - mal within Browsing tasks . Transactions differ from traditional information - seeking tasks in that the user’s goal is not to change his or her state of knowledge but instead to exchange or communicate information . The average length of a Transactions task session was close to 8 min . Transactions were the most often repeated tasks , with 95 % of all Transactions tasks repeated at least once , and consisted primarily of e - mail . It is difﬁcult to characterize the number of pages loaded and windows opened because these functions were inﬂuenced by the type of Web - based e - mail ; however , within a single individual , we would expect that the number of pages and windows opened during Transactions would be more consistent . Transactions were commonly accessed using typed - in URLs and bookmarks . The use of Google , site - speciﬁc searches , and browser functions were minimal within Transactions . Kellar , Watters , and Shepherd ( 2006a ) presented a classiﬁca - tion of Web information tasks , consisting of three main information goals : Information Seeking , Information Ex - change , and Information Maintenance . We found that while all tasks categorized as Transactions by our participants shared the same goal ( information exchange ) , there was a clear distinction of two types of tasks . Tasks with a commu - nication component were more strictly deﬁned as Communi - cations ( e . g . , e - mail , Web publishing ) while those tasks that were based on the exchange on information through online actions were categorized as Transactions ( e . g . , banking , on - line shopping ) . Further research is needed to more carefully characterize these two task types . There was a clear division separating the four task types into two groups : search - based and revisitation - based . While Fact Finding and Information Gathering were characterized as search - based tasks , with a heavy use of Google and site - speciﬁc searches , Browsing and Transactions were character - ized by a heavy level of monitoring and revisitation . Between Fact Finding and Information Gathering , Information Gath - ering was a more complex task , where participants inter - acted much more with the Web browser , loaded signiﬁcantly more Web pages , and used more browser functions , for a longer period of time . When we compared our results with previous research ( Choo et al . , 2000 ; Morrison et al . , 2001 ; Sellen et al . , 2002 ) , we did not observe any consistent trends across the data . The most common information - seeking task ( excluding Transac - tions ) within our study was Browsing whereas Fact Finding was the most common task reported by Choo et al . ( 2000 ) , and Information Gathering was the most common task re - ported by both Morrison et al . ( 2001 ) and Sellen et al . ( 2002 ) . It is difﬁcult to compare previous research due to the difference in task categories , populations , and methods of data collection . For instance , Morrison et al . may have found a higher incidence of Information Gathering because partic - ipants were asked to report an incident where they found information on the Web that led to a signiﬁcant decision or action , which is a characteristic of Information Gathering in itself . The use of knowledge workers in previous research , compared with our use of university students , also may play a role in the differences in the distributions . Therefore , it is difﬁcult to comment on whether the differences in usage across the studies are indicative of the evolution of information - seeking behavior on the Web or whether they are a result of methodological differences . 1014 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi TABLE 9 . General task characterization . Fact ﬁnding Information gathering • Shorter duration • Longer duration • Small number of pages • Larger number of pages viewed viewed • Large search component • Large search component • Relatively longer queries • Relatively shorter queries • Little use of browser • Greatest use of functions browser functions • Typed - in URLs , Google • Typed - in URLs , Google Toolbar , Bookmarks Toolbar , Auto - Complete Browsing Transactions • Shorter duration • Shorter duration • Small number of pages • Number of pages and viewed windows inﬂuenced by type e - mail • Often repeated • Most often repeated • Little use of browser • Little use of browser functions functions • Typed - in URLs , • Bookmarks , Bookmarks , Typed - in URLs Select - URL Implications and Directions for Future Research We have examined how participants used the features of their Web browsers to engage in information - seeking tasks on the Web , and the results of our analysis suggest that par - ticipants interacted differently with their Web browsers across the different information - seeking tasks . These results provide insight into those tasks that need stronger support in Web browsers as well as directions for future research . These ﬁndings are not strictly tied to Web browsers but also provide insight into how future information systems may better support users during their information - seeking tasks . Dominant task attributes . We have examined information - seeking tasks in the context of two of the most dominant task attributes : use of browser functions and search versus revis - tation . In Table 10 , we plotted the tasks on these two dimen - sions . One can see that few browser functions are used during the non - search - based tasks , leaving one quadrant open . This raises the question of whether this is due to an absence of browser functions that support Browsing and Transactions or because the functionality is simply not required during these tasks . Given the dynamic nature of Web pages often viewed during Transactions ( e . g . , banking , travel bookings ) , it was surprising that we did not observe more instances of the print - ing , saving , and copying of information ; however , it could be the case that participants did use these functions , but that they were provided by the online applications themselves ( using Java Script or AJAX ) and therefore not detected by our log - ging tool . This is a potential topic for future research and could be investigated through the use of new logging tools that allow researchers to log AJAX - based interactions ( Atterer , Wnuk , & Schmidt , 2006 ) . Repeated tasks . While we observed the highest number of repeated tasks and revisitation across Browsing and Trans - actions , revisitation occurred across all tasks . The nature of the revisitation differed according to the underlying task type . During Fact Finding task sessions , we observed participants engaging in repeated tasks to monitor new information , re - ﬁnd previously found information , and engage in variants of a previous task . During Information Gathering task sessions , tasks were typically repeated when participants were contin - uing an unﬁnished task . Repeated tasks that occurred during Browsing and Transactions task sessions appeared to be primarily due to monitoring of particular Web sites . Each of these different types of tasks requires different support . While Web browsers and information systems may not be able to reliably anticipate what type of static information users may want to reﬁnd during subsequent Fact Finding task sessions , improved history functions ( as discussed later ) may better support this behavior . Support for repeated Infor - mation Gathering sessions conducted to continue a task could be provided through saved sessions and representa - tions of previous Web browser interactions . Repeated task sessions occurring as the result of Browsing and Transac - tions could be better supported by recognizing the repetitive and habitual nature of these tasks . Complex information - seeking tasks . Information Gathering was the most complex task type in terms of Web browser in - teractions that we considered . We observed the highest num - ber of Web browser interactions during Information Gathering tasks , including functions such as copy / cut / paste , ﬁnd , and the creation of bookmarks . In addition to the use of these browser functions , Information Gathering tasks had the largest number of pages viewed , were the longest in duration , and were often search - based . While these browser interactions themselves are not particularly complex , the combination of these inter - actions contributes to the overall complexity of the task . Pro - viding a visual representation or traces of all Web browser interactions may help users to work more efﬁciently during a task session as well as during future task sessions . The history function . Over the course of the study , we did not observe any use of the history function even though par - ticipants had access to their usual IE history during the study ( i . e . , history collected before and during the study ) . This conﬁrms previous research that reported little use of the his - tory function ( Aula et al . , 2005 ; Tauscher & Greenberg , 1997 ) . While researchers have investigated how to better represent the large number of previously viewed pages in a way that is meaningful to users ( Ayers & Stasko , 1995 ; Kaasten , Greenberg , & Edwards , 2002 ) , commercial Web browsers have done little to ameliorate this problem . In the postsession questionnaires , many of our participants report - ed that they found it difﬁcult to ﬁnd previously visited URLs through the history function and only used it as a last resort . During Information Gathering task sessions , we observed a large number ( 34 . 5 on average ) of pages viewed during a single task session . Users can quickly accumulate a large number of history entries during a single task session ; Web browsers and information systems should provide mecha - nisms that allow users to easily revisit any of the pages viewed during the session . It is apparent from the current implemen - tations of the history function that a simple listing of page titles and URLs is not sufﬁcient to allow users to reﬁnd previ - ously viewed pages , and more research is needed into how to provide more effective representation of previous visited pages . As reported in a more detailed analysis of the use of the Web browser mechanism in this study ( Kellar et al . , 2006b ) , a history function that better supports individual differences among users , such as ﬁling , searching , and typed commands , JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1015 DOI : 10 . 1002 / asi TABLE 10 . Tasks were plotted on two dimensions : use of browser func - tions and search versus revisitation . Search Revisitation Low use of browser Fact Finding Transactions Browsing functions High use of browser Information functions Gathering may be more effective and appeal to a wider variety of Web users . Alternatively , it also may be worthwhile to explore his - tory entries tagged with an automatically generated task de - scription based on the content of the visited pages . Windows management . There has been little research exam - ining the number of Web browser windows viewed during a Web session . In this research , we have examined differences in the number of windows that were opened across task ses - sions . We observed only a small number of windows opened across all information - seeking tasks , ranging from an average of 2 . 28 windows during Information Gathering task sessions to 1 . 43 during Browsing task sessions . This result was sur - prising in that we expected to observe a much larger number of windows opened during more complex tasks , particularly during Information Gathering task sessions . It could be the case that users typically employ a single browser window per task , opening a window for each concurrent task session . Qualitative user reports from previous research have alluded to a task - dependent windows - management strategy ( Hawkey & Inkpen , 2005b ; Weinreich et al . , 2006 ) . Alternatively , the number of windows opened could be inﬂuenced by laptop users , who traditionally have a smaller amount of screen real estate and may be reluctant to open a large number of browser windows . A wider survey of users is needed to better under - stand how browsers and information systems can better sup - port windows management during information - seeking tasks . In particular , the role of tabbed browsing on windows man - agement during different information - seeking task sessions must be explored . Methodological Decisions and Study Limitations We also acknowledge the limitations of this study . We used a convenience sample population consisting of univer - sity students , meaning that we cannot expect that our results will generalize to a more general population . Instead , the re - sults of this study provide insight into how skilled Web users conduct their information - seeking tasks on the Web . When designing this study , we accepted several tradeoffs , one of which was a short duration ( 1 week ) of observation . From a pragmatic viewpoint , it would not have been feasible to expect users to provide detailed descriptions of the Web usage for extended periods of time . Although this means we likely captured more habitual tasks and a smaller number of “new” or “one - off” tasks , in choosing this strategy we gained the ability to gather very detailed task information . The primary beneﬁt of this methodology design was that we were able to obtain a relatively realistic view of the partici - pants’ everyday Web use annotated with task information . We observed participants working with their own navigation mechanisms ( e . g . , bookmarks , history , toolbars , etc . ) and undertaking tasks that were not motivated by a researcher . Requiring users to annotate their daily Web usage and use a custom Web browser had the potential to reduce the naturalness for which we were striving . The postsession questionnaires asked participants if having to record task infor - mation changed the way they usually work on the Web ; the median participant response was “a little . ” When asked if the custom Web browser used in the study changed the way they usually work on the Web , the median participant re - sponse was again “a little . ” Figure 10 displays the distribution for the responses ; however , these data are subjective and do not provide insight into how the study may have impacted participants’ behavior on the Web . A more objective measure at our disposal is the number of pages viewed during the study in comparison with previous week - long ﬁeld studies examining user behavior on the Web ( Hawkey & Inkpen , 2005a , 2006 ) . Through this comparison , we learn that our participants viewed approximately 30 % less Web pages . This may indicate that we received only snapshots of participants’ usage on the Web and that they may have used an alternate browser in instances where they perhaps became tired of an - notating their data or were viewing sensitive information . One aspect that we could not explore was task switching . Some participants reported that using the task toolbar to anno - tate their Web information inﬂuenced their usual task - switching habits . For instance , 1 participant reported that instead of switching between multiple tasks , she would sometimes fully complete one task before beginning a new task because this would then minimize the amount of task - information updates required . Conclusions and Future Work We have presented the results of a study in which we ex - amined how participants interacted differently with their Web browsers across information - seeking tasks . Within each type of task ( i . e . , Fact Finding , Information Gathering , Browsing , Transactions ) , we found several distinguishing characteristics . In particular , Information Gathering was the 1016 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi FIG . 10 . Participant responses to questions asking if having to record their task information and use a custom browser impacted how they normally work on the Web . most complex task . On average , participants spent more time completing this task type , viewed more pages , and used the Web browser functions most heavily , indicating more research is needed to support users in their Information Gathering tasks . We also gained a better understanding of the role of Transactions within our participants’ Web usage and observed that Transactions accounted for a large portion of their Web use , primarily due to Web - based e - mail . Over - all , we observed that participants used their Web browsers to engage in a mix of task types and on a regular basis . Based on these ﬁndings , we have provided implications for the fu - ture support of information seeking on the Web as well as direction for future research in this area . We have two main areas to explore as part of our future work . First , we will explore whether the implicit measures collected could be used to predict the type of task in which a user is engaging . A preliminary exploration of the usefulness of implicit measures to predict users’ tasks using individual decision - tree models has shown promise , with prediction ac - curacy ranging from 44 to 94 % ( Kellar & Watters , 2006 ) . We would like to further explore other machine learning techniques to improve the accuracy of the task predictions based on implicit measures . If successful , predictive models would be useful in providing proactive support for information - seeking tasks . As this study was not designed to formally study moni - toring behavior , the second piece of future work will focus on Web - based monitoring . While we were able to gather in - formal observations about monitoring , we were not able to study it in a precise manner ; however , it became clear through this study that monitoring is a frequent Web activity . We would like to further study whether monitoring is in fact an independent information - seeking task or simply an activ - ity across all Web information tasks . Therefore , a more fo - cused study will be conducted to better understand the types of information that users monitor as well as how monitoring tasks are carried out within the Web browser . Acknowledgments This research was funded by the Natural Sciences and Engineering Research Council of Canada . We thank the anonymous reviewers for their insightful and helpful com - ments , members of the EDGE and WIFL research labs , and all study participants for their time and effort . References Atterer , R . , Wnuk , M . , & Schmidt , A . ( 2006 ) . Knowing the user’s every move : User activity tracking for Web site usability evaluation and im - plicit interaction . Proceedings of the 15th International Conference on the World Wide Web ( WWW 2006 ) ( pp . 203 – 212 ) , Edinburgh , Scotland . Aula , A . , Jhaveri , N . , & Käki , M . ( 2005 ) . Information search and re - access strategies of experienced Web users . Proceedings of the 14th Interna - tional Conference on the World Wide Web ( WWW 2005 ) ( pp . 583 – 592 ) , Chiba , Japan . Aula , A . , & Käki , M . ( 2003 ) . Understanding expert search strategies for designing user - friendly search interfaces . Proceedings of the IADIS International Conference WWW / Internet ( ICWI 2003 ) ( pp . 759 – 762 ) , Algarve , Portugal . Ayers , E . Z . , & Stasko , J . T . ( 1995 ) . Using graphic history in browsing the World Wide Web . Proceedings of the 4th International Conference on the World Wide Web , Boston . Retrieved from http : / / www . w3 . org / pub / Conferences / WWW4 / Papers2 / 270 Beitzel , S . M . , Jensen , E . C . , Chowdhury , A . , Grossman , D . , & Frieder , O . ( 2004 ) . Hourly analysis of a very large topically categorized Web query log . Proceedings of the 27th International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR 2004 ) ( pp . 321 – 328 ) , Shefﬁeld , United Kingdom . Belkin , N . J . ( 1980 ) . Anomalous states of knowledge as a basis for informa - tion retrieval . Canadian Journal of Information Science , 5 , 133 – 143 . Broder , A . ( 2002 ) . A taxonomy of Web search . SIGIR Forum , 36 ( 2 ) , 3 – 10 . Byrne , M . D . , John , B . E . , Wehrle , N . S . , & Crow , D . ( 1999 ) . The tangled Web we wove : A taskonomy of WWW use . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’99 ) ( pp . 544 – 551 ) , Pittsburgh , PA . Catledge , L . D . , & Pitkow , J . E . ( 1995 ) . Characterizing browsing strategies in the World Wide Web . Proceedings of the 3rd International World Wide Web Conference ( pp . 1065 – 1073 ) , Darmstadt , Germany . Choo , C . W . , Detlor , B . , & Turnbull , D . ( 2000 ) . Information seeking on the Web : An integrated model of browsing and searching [ Online Serial ] . First Monday , 5 ( 2 ) , http : / / ﬁrstmonday . org / issues / issues5 _ 2 / choo / index . html Claypool , M . , Le , P . , Waseda , M . , & Brown , D . ( 2001 ) . Implicit interest in - dicators . Proceedings of the 6th International Conference on Intelligent User Interfaces ( IUI 2001 ) ( pp . 33 – 40 ) , Santa Fe , NM . Cockburn , A . , & McKenzie , B . ( 2001 ) . What do Web users do ? An empiri - cal analysis of Web use . International Journal of Human – Computer Stud - ies , 54 ( 6 ) , 903 – 922 . Cooper , M . D . ( 2001 ) . Usage patterns of a Web - based library catalog . Journal of the American Society for Information Science and Technology , 52 ( 2 ) , 137 – 148 . Cothey , V . ( 2002 ) . Alongitudinal study of World Wide Web users’informa - tion - searching behavior . Journal of the American Society for Information Science and Technology , 53 ( 2 ) , 67 – 78 . Ellis , D . ( 1989 ) . A behavioural approach to information retrieval system design . Journal of Documentation , 45 ( 3 ) , 171 – 212 . Fidel , R . , & Efthimiadis , E . ( 1999 ) . Web searching behavior of aerospace engineers . Proceedings of the 22nd International ACM SIGIR Confer - ence on Research and Development in Information Retrieval ( SIGIR ’99 ) ( pp . 319 – 320 ) , Berkeley , CA . Grace - Martin , M . , & Gay , G . ( 2001 ) . Web browsing , mobile computing and academic performance . Educational Technology & Society , 4 ( 3 ) , 95 – 107 . Hawkey , K . , & Inkpen , K . ( 2005a ) . Privacy gradients : Exploring ways to manage incidental information during co - located collaboration . Proceed - ings of the CHI ‘05 Extended Abstracts on Human Factors in Computing Systems ( pp . 1431 – 1434 ) , Portland , OR . Hawkey , K . , & Inkpen , K . ( 2005b ) . Web browsing today : The impact of chang - ing contexts on user activity . Proceedings of the CHI ‘05 Extended Abstracts on Human Factors in Computing Systems ( pp . 1443 – 1446 ) , Portland , OR . Hawkey , K . , & Inkpen , K . ( 2006 ) . Examining the content and privacy of Web browsing incidental information . Proceedings of the 15th Interna - tional Conference on the World Wide Web ( WWW 2006 ) ( pp . 123 – 132 ) , Edinburgh , Scotland . Herder , E . ( 2005 ) . Characterizations of user Web revisit behavior . Proceed - ings of the Workshop on Adaptivity and User Modeling in Interactive Systems ( ABIS 2005 ) , Saarbrücken , Germany ( pp . 32 – 37 ) . Hölscher , C . , & Strube , G . ( 2000 ) . Web search behavior of Internet experts and newbies . Proceedings of the 10th International Conference on the World Wide Web ( WWW 2000 ) ( pp . 337 – 346 ) , Amsterdam . Jansen , B . J . , & McNeese , M . D . ( 2005 ) . Evaluating the effectiveness of and patterns of interactions with automated searching assistance . Journal of the American Society for Information Science and Technology , 56 ( 14 ) , 1480 – 1503 . Jansen , B . J . , & Spink , A . ( 2003 ) . An analysis of Web documents retrieved and viewed . Proceedings of the 4th International Conference on Internet Computing ( pp . 65 – 69 ) , Las Vegas , NV . JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 1017 DOI : 10 . 1002 / asi Jansen , B . J . , Spink , A . , & Pedersen , J . ( 2005 ) . A temporal comparison of Altavista Web searching . Journal of the American Society for Informa - tion Science and Technology , 56 ( 6 ) , 559 – 570 . Jhaveri , N . , & Räihä , K . - J . ( 2005 ) . The advantages of a cross - session Web workspace . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI 2005 ) ( pp . 1949 – 1952 ) , Portland , OR . Kaasten , S . , Greenberg , S . , & Edwards , C . ( 2002 ) . How people recognize previously seen Web pages from titles , URLs and thumbnails . Proceed - ings of the Human – Computer Interaction 2002 ( pp . 247 – 265 ) , London . Kellar , M . , & Watters , C . ( 2006 ) . Using Web browser interactions to predict task . Proceedings of the 15th International Conference on World Wide Web ( WWW 2006 ) ( pp . 843 – 844 ) , Edinburgh , Scotland . Kellar , M . , Watters , C . , & Shepherd , M . ( 2006a ) . Agoal - based classiﬁcation of Web information tasks . Proceedings of the annual meeting of the American Society for Information Science and Technology ( ASIS & T 2006 ) , Austin , TX . Kellar , M . , Watters , C . , & Shepherd , M . ( 2006b ) . The impact of task on the usage of Web browser navigation mechanisms . Proceedings of Graphics Interface ( GI 2006 ) ( pp . 235 – 242 ) , Quebec City . Kelly , D . , & Belkin , N . J . ( 2001 ) . Reading time , scrolling and interaction : ex - ploring implicit sources of user preference for relevance feedback . Proceed - ings of the 24th International Conference on Research and Development in Information Retrieval ( SIGIR 2001 ) ( pp . 408 – 409 ) , New Orleans . Kelly , D . , & Teevan , J . ( 2003 ) . Implicit feedback for inferring user prefer - ence : A bibliography . SIGIR Forum , 37 ( 2 ) , 18 – 28 . Kim , K . - S . , & Allen , B . ( 2002 ) . Cognitive and task inﬂuences on Web searching behavior . Journal of the American Society for Information Science and Technology , 53 ( 2 ) , 109 – 119 . Kuhlthau , C . ( 1991 ) . Inside the search process : Information seeking from the user’s perspective . Journal of the American Society for Information Science and Technology , 42 ( 5 ) , 361 – 371 . Lee , U . , Liu , Z . , & Cho , J . ( 2005 ) . Automatic identiﬁcation of user goals in Web search . Proceedings of the 14th International Conference on World Wide Web ( WWW 2005 ) ( pp . 391 – 400 ) , Chiba , Japan . MacKay , B . , Kellar , M . , & Watters , C . ( 2005 ) . An evaluation of landmarks for re - ﬁnding information on the Web . Proceedings of the CHI ’05 Extended Abstracts on Human Factors in Computing Systems ( pp . 1609 – 1612 ) , Portland , OR . Marchionini , G . ( 1995 ) . Information seeking in electronic environments . New York : Cambridge University Press . Martzoukou , K . ( 2005 ) . A review of Web information seeking research : Considerations of method and foci of interest . Information Research , 10 ( 2 ) . Retrieved from http : / / informationr . net / ir / 10 - 2 / paper215 . html Mat - Hassan , M . , & Levene , M . ( 2005 ) . Associating search and navigation behavior through log analysis . Journal of the American Society for Infor - mation Science and Technology , 56 ( 9 ) , 913 – 934 . McGrath , J . E . ( 1995 ) . Methodology matters : Doing research in the behavioral and social sciences . In R . Baeker , J . Grudin , W . Buxton , & S . Greenberg ( Eds . ) , Human – computer interaction : Toward the Year 2000 ( pp . 152 – 169 ) . Milic - Frayling , N . , Sommerer , R . , & Rodden , K . ( 2003 ) . Webscout : Support for revisitation of Web pages within a navigation session . Proceedings of the 2003 IEEE / WIC International Conference on Web Intelligence ( WI ’03 ) ( pp . 689 – 693 ) , Halifax , Nova Scotia , Canada . San Francisco : Morgan - Kaufman . Montgomery , A . L . , & Faloutsos , C . ( 2001 ) . Identifying Web browsing trends and patterns . IEEE Computer , 34 ( 7 ) , 94 – 95 . Morita , M . , & Shinoda , Y . ( 1994 ) . Information ﬁltering based on user behavior analysis and best match text retrieval ( pp . 272 – 281 ) . Proceed - ings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’94 ) , Dublin , Ireland . Morrison , J . B . , Pirolli , P . , & Card , S . K . ( 2001 ) . A taxonomic analysis of what World Wide Web activities signiﬁcantly impact people’s decisions and actions . Proceedings of the CHI ‘01 Extended Abstracts on Human Factors in Computing Systems ( pp . 163 – 164 ) , Seattle , WA . Moyle , M . , & Cockburn , A . ( 2003 ) . The design and evaluation of a ﬂick gesture for “back” and “forward” in Web browsers . Proceedings of the 4th Australasian Conference on User Interface ( AUIC 2003 ) ( pp . 39 – 46 ) , Adelaide , Australia . Oard , D . W . , & Kim , J . ( 2001 ) . Modeling information content using observ - able behavior . Proceedings of the annual meeting of the American Society for Information Science and Technology ( ASIS & T 2001 ) ( pp . 38 – 45 ) , Washington , DC . Pitkow , J . E . , & Kehoe , C . M . ( 1996 ) . Emerging trends in the WWW user population . Communications of the ACM , 39 ( 6 ) , 106 – 108 . Rieh , S . Y . ( 2003 ) . Investigating Web searching behaviour in home environ - ments . Proceedings of the annual meeting of the American Society for In - formation Science and Technology ( ASIS & T 2003 ) ( pp . 255 – 264 ) , Long Beach , CA . Rose , D . E . , & Levinson , D . ( 2004 ) . Understanding user goals in Web search . Proceedings of the 13th International Conference on the World Wide Web ( WWW 2004 ) ( pp . 13 – 19 ) , New York . Rozanski , H . D . , Bollman , G . , & Lipman , M . ( 2001 ) . Seize the occasion ! The seven - segment system for online marketing [ Online Serial ] . Strategy & Competition . Retrieved from http : / / www . strategy - business . com / press / article / 19940 ? pg (cid:1) 19940 Schiano , D . J . , Stone , M . , & Bectarte , R . ( 2001 ) . Search and the subjective Web . Proceedings of the CHI ’01 Extended Abstracts on Human Factors in Computing Systems ( pp . 165 – 166 ) , Seattle , WA . Sellen , A . J . , Murphy , R . , & Shaw , K . L . ( 2002 ) . How knowledge work - ers use the Web . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI 2002 ) ( pp . 227 – 234 ) , Minneapo - lis , MN . Seo , Y . - W . , & Zhang , B . - T . ( 2000 ) . Learning user’s preferences by analyz - ing Web - browsing behaviors . Proceedings of the International Confer - ence on Autonomous Agents 2000 ( pp . 381 – 387 ) , Barcelona , Spain . Spink , A . , Wolfram , D . , Jansen , M . B . , & Saracevic , T . ( 2001 ) . Searching the Web : The public and their queries . Journal of the American Society for Information Science and Technology , 52 ( 3 ) , 226 – 234 . Tauscher , L . , & Greenberg , S . ( 1997 ) . How people revisit Web pages : Empirical findings and implications for the design of history systems . International Journal of Human – Computer Studies , 47 , 97 – 137 . Teevan , J . , Alvarado , C . , Ackerman , M . S . , & Karger , D . R . ( 2004 ) . The perfect search engine is not enough : Astudy of orienteering behavior in directed search . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI 2004 ) ( pp . 415 – 422 ) , Vienna . Weinreich , H . , Obendorf , H . , Herder , E . , & Mayer , M . ( 2006 ) . Off the beaten tracks : Exploring three aspects of Web navigation . Proceedings of the 15th International Conference on the World Wide Web ( WWW 2006 ) ( pp . 133 – 142 ) , Edinburgh , Scotland . Wilson , T . D . ( 1997 ) . Information behaviour : An interdisciplinary perspec - tive . Information Processing and Management , 33 ( 4 ) , 551 – 572 . 1018 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY—May 2007 DOI : 10 . 1002 / asi