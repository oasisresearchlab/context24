Journal of Visual Languages & Computing Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 Interactive , visual fault localization support for end - user programmers $ Joseph R . Ruthruff ! , Shrinu Prabhakararao , James Reichwein , Curtis Cook , Eugene Creswick , Margaret Burnett School of Electrical Engineering and Computer Science , Oregon State University , Corvallis , OR 97331 , USA Received 1 January 2004 ; received in revised form 1 June 2004 ; accepted 1 July 2004 Abstract End - user programmers are writing an unprecedented number of programs , primarily using languages and environments that incorporate a number of interactive and visual programming techniques . To help these users debug these programs , we have developed an entirely visual , interactive approach to fault localization . This paper presents the approach . We also present the results of a think - aloud study that examined interactive , human - centric issues that arise in end - user debugging using a fault localization approach . Our results provide insights into the contributions such approaches can make to the end - user debugging process . r 2004 Elsevier Ltd . All rights reserved . Keywords : End - user programming ; Visual fault localization ; Debugging ; End - user software engineering ; Testing ; Slicing ; Form - based visual programs ARTICLE IN PRESS www . elsevier . com / locate / jvlc 1045 - 926X / $ - see front matter r 2004 Elsevier Ltd . All rights reserved . doi : 10 . 1016 / j . jvlc . 2004 . 07 . 001 $ This paper updates and extends earlier work that appeared in S . Prabhakararao , C . Cook , J . Ruthruff , E . Creswick , M . Main , M . Durham , M . Burnett , Strategies and behaviors of end - user programmers with interactive fault localization , in : Proceedings of the IEEE Symposium on Human - Centric Computing Languages and Environments , Auckland , New Zealand , October 28 – 31 , 2003 , pp . 15 – 22 and Ref . [ 2 ] . ! Corresponding author . Department of Computer Science and Engineering , University of Nebraska - Lincoln , 256 Avery Hall , Lincoln , NE 68588 0115 , USA . Tel . : + 14024722401 ; fax : + 14024727767 . E - mail addresses : ruthruff @ cse . unl . edu ( J . R . Ruthruff ) , prabhash @ cs . orst . edu ( S . Prabhakararao ) , jreichwe @ san . rr . com ( J . Reichwein ) , cook @ cs . orst . edu ( C . Cook ) , creswick @ cs . orst . edu ( E . Creswick ) , burnett @ cs . orst . edu ( M . Burnett ) . 1 . Introduction Recent years have seen an explosive growth of end - user programming . In fact , by the year 2005 , it is estimated that there will be approximately 55 million end - user programmers in the US alone , as compared to an estimated 2 . 75 million professional programmers [ 3 ] . Real - world examples of end - user programming environments include educational simulation builders , web authoring systems , multimedia authoring systems , e - mail ﬁltering rule systems , CAD systems , scientiﬁc visualization systems , and spreadsheets . These systems all make use of at least some visual programming techniques in order to support their users , such as graphical syntaxes , drag - and - drop to specify desired outcomes or appearances , programming by demonstration , and / or immediate visual feedback about a program’s semantics . But , how reliable are the programs end users write using such systems ? One of the most widely used real - world end - user programming paradigms is the spreadsheet . Despite its perceived simplicity , evidence from this paradigm reveals that end - user programs often contain faults [ 4 – 6 ] . ( Following standard terminology [ 7 ] , in this paper , a failure is an incorrect computational result , and a fault is the incorrect part of the program that caused the failure . ) Perhaps even more disturbing , users regularly express unwarranted conﬁdence in the quality of these programs [ 6 , 8 ] . To help solve this reliability problem , we have been working on a vision we call ‘‘end - user software engineering’’ [ 9 ] . The concept of end - user software engineering is a holistic approach to the facets of software development in which end users engage . Its goal is to bring some of the gains from the software engineering community to end - user programming environments , without requiring end users to have training , or even interest , in traditional software engineering concepts or techniques . Our end - user software engineering devices communicate with their users entirely through interactive , visual mechanisms . We are prototyping our end - user software engineering methodologies in the form - based paradigm because it is so widespread in practice . Form - based languages provide a declarative approach to programming , characterized by a dependence - driven , direct - manipulation working model [ 10 ] . The form - based paradigm is inherently visual because of its reliance on multiple dimensions . In form - based languages , the programmer designs a form in two or more dimensions , including formulas which will ultimately compute values . Each formula corresponds to the right - hand side of an equation , i . e . , f ( Y 1 , Y 2 , y , Y n ) , and is also associated with a mechanism to visually display the formula’s value . For example , in spreadsheet languages , a cell is the mechanism that associates a formula with a display mechanism . When a cell’s formula is deﬁned , the underlying evaluation engine calculates the cell’s value and those of other affected cells ( at least those that are visible to the user ) , and displays new results . Examples of this paradigm include not only commercial spreadsheet systems , but also research languages used for purposes such as producing high - quality visualizations of complex data [ 11 ] , for specifying full - featured GUIs [ 12 , 13 ] , for matrix manipulation [ 14 – 16 ] , for providing steerable simulation environments for scientists [ 17 ] , for web programming [ 18 ] , and for working visually with user - deﬁned objects [ 19 , 20 ] . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 4 In this paper , we introduce interactive fault localization , an entirely interactive , visual approach to fault localization that we have integrated into our end - user software engineering concept . Our approach has been carefully designed around ﬁve properties , chosen after consideration of both software engineering and human - computer interaction principles . Although we have prototyped our ideas in the form - based paradigm , our design allows our approach to be incorporated into other types of visual programming environments . We also describe a think - aloud study that we conducted to evaluate how our approach impacts the debugging efforts of end users , and to examine interactive , human - centric issues that arise in end - user debugging using visual fault localization . The results of this study indicate that our visual fault localization approach can favorably affect the debugging efforts of end users working in form - based visual programming environments . In addition , our study yields several insights into the interactive , end - user debugging process that future work on end - user debugging may need to consider . The remainder of this paper is organized as follows : Section 2 discusses previous research that is related to fault localization ; Section 3 describes the end - user software engineering devices with which our approach to fault localization is integrated ; Section 4 introduces our interactive , visual approach to fault localization ; Section 5 describes the procedures of the think - aloud study ; Section 6 outlines the results of the study ; Section 7 discusses the implications of our study’s results in further detail ; Section 8 discusses threats to the validity of our study’s results ; Section 9 discusses integrating our approach into other programming paradigms ; and Section 10 presents our conclusions . 2 . Related work Our approach to interactive fault localization is intertwined with an environment that emphasizes software engineering practices . Because of this , we look ﬁrst at the area of software engineering devices that make use of visualizations for debugging . The FIELD environment was aimed primarily at program comprehension as a vehicle for both debugging and instruction [ 21 ] . This work draws from and builds upon earlier work featuring visualizations of code , data structures , and execution sequences , such as PECAN [ 22 ] , the Cornell Program Synthesizer [ 23 ] , and Gandalf [ 24 ] . ZStep [ 25 ] , on the other hand , aims squarely at debugging and providing visualizations of the correspondences between static program code and dynamic program execution . Its navigable visualizations of execution history are representa - tive of similar features found in some visual programming languages such as KidSim / Cocoa / Stagecast [ 26 ] and Forms / 3 [ 19 , 27 ] . An example of visualization work that is especially strong in low - level debugging such as memory leaks and performance tuning is PV [ 28 ] . Low - level visualization work speciﬁcally aimed at performance debugging in parallel programming is surveyed by Heath [ 29 ] . Finally , Eick’s work focuses on high - level views of software , mostly with an eye to keeping the bugs under control during the maintenance phase [ 30 ] . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 5 Work aimed particularly at aiding end - user programmers with debugging and other software engineering tasks is beginning to emerge . Ko and Myers present the Whyline [ 31 ] , an ‘‘Interrogative Debugging’’ device for the 3D programming environment Alice . Users pose questions in the form of ‘‘Why did y ’’ or ‘‘Why didn’t y ’’ that the Whyline answers by displaying a visualization of a partial program slice that the user can navigate , expand , and compare with previous questions . Igarashi et al . present devices to aid spreadsheet users in dataﬂow visualization and editing tasks [ 32 ] . S2 [ 33 ] provides a visual auditing feature in Excel 7 . 0 : similar groups of cells are recognized and shaded based upon formula similarity , and are then connected with arrows to show dataﬂow . This technique builds upon the Arrow Tool , a dataﬂow visualization device proposed by Davis [ 34 ] . Carr proposes reMIND + [ 35 ] , a visual end - user programming language with support for reusable code and type checking . reMIND + also provides a hierarchical ﬂow diagram for increased program understanding . Outlier ﬁnding [ 36 ] is a method of using statistical analysis and interactive techniques to direct end - user programmers’ attention to potentially problematic areas during automation tasks . The approach uses visual cues to indicate abnormal situations while performing search and replace or simultaneous editing tasks . Because not all outliers are incorrect , the approach uses a heuristic to determine the outliers to highlight for the user . Finally , the assertions approach in Forms / 3 has been shown empirically to help end - user programmers correct errors [ 37 , 38 ] . Ayalew and Mittermeir present a method of fault tracing for spreadsheets based on ‘‘interval testing’’ and slicing [ 39 ] . In their approach , which is similar to assertions in Forms / 3 [ 37 ] , user - speciﬁed intervals are compared with cell values and system - generated intervals for each cell . When the user - speciﬁed and system - generated intervals for a cell do not agree with the actual spread - sheet computation , the cell is ﬂagged as displaying a ‘‘symptom of a fault’’ . Furthermore , upon request from the user , a ‘‘fault tracing’’ approach can be used to identify the ‘‘most inﬂuential faulty cell’’ from the cells perceived by the system to contain symptoms of faults . ( In the case of a tie , one cell is arbitrarily chosen . ) There are many differences between their approach and our fault localization approach ; we describe two such differences . First , unlike the approach of Ayalew and Mittermeir , our fault localization approach includes robustness features against user mistakes ( described in Section 4 . 3 . 1 ) . Second , our approach explicitly supports an incremental end - user debugging process by allowing users to make decisions that can have the effect of improving our continuously updated feedback . Fault localization is a very speciﬁc type of debugging support . Behind the scenes , most fault localization research has been based on slicing and dicing techniques . ( We discuss slicing and dicing in Section 4 . 2 . ) A survey of these techniques was made by Tip [ 40 ] . Agrawal et al . have built upon these techniques for traditional languages in w slice [ 41 ] . w slice is based upon displaying dices of the program relative to one failing test and a set of passing tests . Tarantula utilizes testing information from all passing and failing tests to visually highlight possible locations of faults [ 42 ] . Both these techniques are targeted at the professional ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 6 programmer , and return results after running a batch of tests . Besides being explicitly targeted at end users , our methods differ from their approaches in that our methods are interactive and incremental . Pan and Spafford developed a family of heuristics appropriate for automated fault localization [ 43 ] . They present a total of 20 different heuristics that they felt would be useful . These heuristics are based on the program statements exercised by passing and failing tests . Our approach directly relates to three of these heuristics : the set of all cells exercised by failed tests , cells that are exercised by a large number of failed tests , and cells that are exercised by failing tests and that are not exercised by passing tests . 3 . Background : end - user software engineering We believe that interactive and visual programming techniques , which provide support that has enabled end - user programming to evolve into a widespread phenomenon , can help reduce the reliability problem in programs created by end users . Towards this end , our ‘‘end - user software engineering’’ vision consists of a blend of components that come together seamlessly through interactive visual devices . Components that have been implemented in Forms / 3 [ 19 ] —a form - based visual programming language—include : ‘‘What You See Is What You Test’’ ( WYSIWYT ) [ 44 – 46 ] , a visual testing methodology for form - based visual programs ; an automated test case generation device [ 47 ] ; a test re - use approach [ 48 ] ; and an approach for supporting assertions by end users [ 37 ] . Visual fault localization has been blended into this combination by marrying it to the WYSIWYT visual testing methodology . Therefore , we brieﬂy describe WYSIWYT here . The underlying assumption behind the WYSIWYT testing methodology is that , as a user incrementally develops a form - based visual program , he or she can also be testing incrementally . The system communicates with the user about testing through visual devices . Fig . 1 presents an example of WYSIWYT in Forms / 3 . In WYSIWYT , untested cells that have non - constant formulas are given a red border ( light gray in this paper ) , indicating that the cell is untested . ( Cells whose formulas are simply constants do not participate in WYSIWYT devices , since the assumption is that they do not need to be tested . ) For example , the Total _ Score cell has never been tested ; hence , its border is red ( light gray ) . The borders of such cells remain red until they become more ‘‘tested’’ . In order for cells to become more tested , tests must occur . But tests can occur at any time—intermingled with editing formulas , adding new formulas , and so on . The process is as follows . Whenever a user notices a correct value , he or she can place a checkmark in the decision box at the corner of the cell he or she observes to be correct : this testing decision constitutes a successful ‘‘test’’ . Such checkmarks increase the ‘‘testedness’’ of a cell , which is reﬂected by adding more blue to the cell’s border ( more black in this paper ) . For example , in Fig . 1 , the Weightedavgquiz cell has been given a checkmark , which is enough to ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 7 fully test this cell , thereby changing its border from red to blue ( light gray to black ) . Further , because a correct value in a cell C depends on the correctness of the cells contributing to C , these contributing cells participate in C ’s test . Consequently , in this example the border of cell avgquiz also turns blue ( black ) . Although users may not realize it , the ‘‘testedness’’ colors that result from placing checkmarks reﬂect the use of a dataﬂow test adequacy criterion that measures the interrelationships in the source code that have been covered by the users’ tests . A cell is fully tested if all its interrelationships have been covered ; if only some have been covered then the cell is partially tested . These partially tested cells would have borders in varying shades of purple ( shades of gray ) . ( Details of the test adequacy criterion are given in [ 45 , 46 ] . ) For example , the partially tested EC _ Award cell has a purple ( gray ) border . In addition to providing feedback at the cell level , WYSIWYT gives the user feedback about testedness at two other granularities . A percent testedness indicator provides testedness feedback at the program granularity by showing a progress bar that ﬁlls and changes color from red to blue ( following the same colorization continuum as cell borders ) as the overall testedness of the program increases . The testedness bar can be seen at the top of Fig . 1 ; the tested indicator shows that the program is 18 % tested . Testedness feedback is also available at a ﬁner granularity through dataﬂow arrows . In Fig . 1 , the user has triggered the dataﬂow arrows for the ExtraCredit cell . These arrows depart from tradition in two ways . First , they can depict dataﬂow relationships between subexpressions within cell formulas , as ARTICLE IN PRESS Fig . 1 . An example of WYSIWYT in the Forms / 3 language . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 8 displayed with the EC _ Award formula in Fig . 1 . Second , when the user activates the dataﬂow arrows for a cell , the arrows connecting the formulas’ subexpressions follow the same red - to - blue color continuum of cell borders at the granularity of subexpressions . This has the effect of showing users the untested cell reference combinations that still need testing . In addition to color - coding the arrows , the system also provides testedness feedback through an intelligent explanation system [ 38 ] . In Fig . 1 , the user has chosen to examine one of the purple ( gray ) subexpression arrows leading into EC _ Award , which shows that the relationship between ExtraCredit and the indicated subexpression of EC _ Award is 50 % tested . WYSIWYT has been empirically shown to be useful to both programmers and end users [ 49 , 50 ] ; however , it does not by itself explicitly support a debugging effort to localize the fault ( s ) causing an observed failure . Providing this debugging support is the aim of our interactive , visual fault localization approach , which we describe next . 4 . Visual fault localization Testing and debugging are closely related : testing is detecting the presence of failures , and debugging is tracking the failure down to its fault ( fault locali - zation ) and then ﬁxing the fault . Taking advantage of this relationship , in our approach , the WYSIWYT testing methodology is used as a springboard to fault localization . Here is the way this springboard works . In addition to the possibility of noticing that a cell’s value is correct , there is also the possibility of noticing that a cell’s value is incorrect . In the ﬁrst case , the user checks off the cell’s value via WYSIWYT’s decision box , as we explained in Section 3 . The second case is where fault localization comes in . In that case , which happens when the user has detected a failure , the user can ‘‘X out’’ the value by placing an X - mark in the decision box instead . These X - marks trigger a fault likelihood calculation for each cell with a non - constant formula that might have contributed to the failure . This likelihood , updated for each appropriate cell after any testing decision or formula edit , is represented by visually highlighting the interior of suspect cells in different shades of red . As the fault likelihood of a cell grows , the suspect cell is highlighted in increasingly darker shades of red . The darkest cells are thus estimated to be the most likely to contain the fault , and are therefore the best candidates for the user to consider in trying to debug . Of course , given the immediacy of visual feedback that is customary in form - based visual programming environments , the visual highlighting is added or adjusted immediately whenever the user adds another checkmark or X - mark . For example , suppose that after working awhile , the user has gotten the Grade program to the stage shown at the top of Fig . 2 . At that point , the user notices that the output value of the Total _ Score cell is incorrect : the total score is obviously too high . Upon spotting this failure , the user places an X - mark ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 9 in the Total _ Score cell , triggering fault likelihood calculations for all cells whose values dynamically contributed to the value in Total _ Score , and causing the interiors of cells suspected of containing faults to be highlighted in red ( gray in this paper ) , as shown at the bottom of Fig . 2 . The cells deemed ( by the system ) most likely to contain the fault are highlighted the darkest . ARTICLE IN PRESS Fig . 2 . ( Top ) A Grade program at an early stage of testing . ( Bottom ) The user notices an incorrect output value in Total _ Score and places an X - mark in the cell . All cells that could have dynamically contributed to this incorrect value have been highlighted in shades of red ( gray in this paper ) , with darker shades corresponding to increased fault likelihood . In this bottom example , avgquiz and Weight - edavgquiz have an estimated fault likelihood of ‘‘Very Low’’ ( the fault likelihood of avgquiz is indicated by the explanation tooltip ) while EC _ Award and the four rightmost cells have a fault likelihood of ‘‘Low’’ ( not indicated by tooltips ) . There is no estimated fault likelihood higher than ‘‘Low’’ in this particular example because the technique is hesitant to assign higher estimations when only one failure ( X - mark ) has been noted by the user . Finally , the testedness borders of cells with non - zero fault likelihood have faded to draw visual focus to the fault likelihood component of the cell . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 10 4 . 1 . Design constraints The above example rests upon a number of constraints that we believe must be met for viability with end - user programmers working in a form - based language . The constraints we imposed upon our design are : 1 . Most modern form - based visual programs are incremental and modeless : Most form - based visual programming does not require the separate code , compile , link , and execute modes typically required by traditional programming languages . Developers simply draw and manipulate live calculation objects on forms and continually observe how the results seem to be working out . Thus , in order for testing and debugging techniques to be consistent with this way of working , users must be allowed to debug and test incrementally and modelessly , in parallel with program development . 2 . Form - based visual program developers may not understand testing and debugging theory : For an end - user audience , testing and debugging techniques cannot depend on the user understanding testing or debugging theory , nor should such techniques rely on specialized vocabularies based on such theory . 3 . Form - based visual programming environments must maintain trust with their end - user developers : End - user programmers may not understand the reasons if debugging feedback leads them astray . Instead , they are likely to lose trust in our approach and ignore the feedback . ( As [ 51 ] explains , trust is critical to users actually believing a system’s output . ) Therefore , our methodology must maintain some consistent level of guarantees about its correctness , so that users can understand to what extent they can believe its feedback . 4 . Form - based visual programs offer immediate feedback : When an end - user programmer changes a formula , the environment displays the results quickly . Users have come to expect this responsiveness from form - based visual programs and may not accept functionality that signiﬁcantly inhibits responsiveness . Therefore , the integration of testing and debugging into these programming environments must minimize the overhead it imposes , and must be interactive and incremental . One direct result of these constraints is that many preexisting fault localization techniques that have been designed for professional programming situations are not suited towards the audience we wish to target . For example , batch - oriented algorithms for fault localization that expect all testing to be completed before any fault localization processing can be performed inherently cannot meet the incremental , immediate feedback constraint . An entirely new approach to fault localization is therefore necessary to meet these constraints . 4 . 2 . Slicing and dicing Our approach performs its calculations leading to the visual representation of fault likelihood by drawing from information gleaned via slicing , and by making use of that information in a manner inspired by dicing . This information is used to ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 11 incrementally calculate the colorizations of our fault localization approach so that they can be interactively updated after each user action . Weiser introduced program slicing [ 52 ] as a technique for analyzing program dependencies . A slicing criterion / p , V S , where p is a program point and V is a subset of program variables , is used to deﬁne a program slice . Program slices can then be classiﬁed into two categories : backward slicing ﬁnds all the program points P that affect a given variable v at a given program point p , whereas forward slicing ﬁnds all the program points P that are affected by a given variable v at a given program point p . ( In Forms / 3 , cells are considered variables . Consequently , a cell A is in the backward slice of a cell B if B is dependent on A —meaning the value of A affects the output of B . Conversely , in this example , B is said to be in the forward slice of A . ) Weiser’s slicing algorithm calculates static slices , based solely on information contained in source code , by iteratively solving dataﬂow equations . Dynamic slicing [ 53 ] uses information gathered during program execution in addition to information contained in source code to compute slices . Using this information , dynamic slices ﬁnd program points that may affect ( or may be affected by ) a given variable at a given point under a given execution . Consequently , dynamic slicing usually produces smaller slices than static slicing . Because our purpose is to localize the possible location of faults to the smallest possible section of the program , our approach is based on dynamic slicing . In the WYSIWYT framework , checkmarks on a cell C cause testedness to automatically propagate up C ’s backward dynamic slice to all contributing cells . Our approach leverages this support , as we will describe in Section 4 . 3 . 2 . Considering slicing as a fault localization technique , a slice can make use of information only about variables that have had incorrect values . But program dicing [ 54 ] , in addition to making use of information about variables that have had incorrect values , can also make use of information about where correct values have occurred . Using dicing , faults can be further localized by ‘‘subtracting’’ the slices on correct variables away from the slices on the incorrect variable . Our approach is similar in concept to dicing , but is not exactly the same . Lyle and Weiser describe the cases in which a dice on an incorrect variable not caused by an omitted statement is guaranteed to contain the fault responsible for the incorrect value in their Dicing Theorem [ 54 ] . In this theorem , the ﬁrst assumption eliminates the case where an incorrect variable is misidentiﬁed as a correct variable . The second assumption removes the case where a variable is correct despite depending on an incorrect variable ( e . g . , when a subsequent computation happens to compensate for an earlier incorrect computation for certain inputs ) . The third assumption removes the case where two faults counteract each other and result in an accidently correct value . The theorem is outlined below : Dicing theorem . A dice on an incorrect variable contains a fault ( except for cases where the incorrect value is caused by omission of a statement ) if all of the following assumptions hold : 1 . Testing has been reliable and all incorrectly computed variables have been identiﬁed . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 12 2 . If the computation of a variable , v , depends on the computation of another variable , w , then whenever w has an incorrect value then v does also . 3 . There is exactly one fault in the program . The Dicing Theorem [ 54 ] states that a dice is only guaranteed to contain a fault if the above three assumptions can be met . Meeting these assumptions is clearly not possible in end - user programming . As just one example , end users , like any human performing interactive testing , seem likely to make incorrect testing decisions . ( We will return to this point in Sections 6 and 7 . ) Because of this mismatch with the audiences and environments we choose to support , our approach is different from dicing in both deﬁnition and implementation , as we describe in the remainder of this section . 4 . 3 . A heuristic for estimating fault likelihood Since computing an exact fault likelihood value for a program point in any program is not possible , a heuristic must be employed to estimate such a value based on acquired information . We have chosen to use a dicing - like procedure to formulate our approach to fault localization . Traditionally , a dice on an incorrect cell would be formed after making a binary decision about cells : either a cell is indicated as incorrect or it is not ; but this does not allow for the possibility of user testing mistakes , multiple faults in a program , etc . To account for these realities , our methodology instead estimates the likelihood that a cell contains one or more faults that contribute to a value marked incorrect . We call this likelihood the fault likelihood of a cell . Let I be the set of cell values marked incorrect by the program developer . The fault likelihood of a cell C is an estimate of the likelihood that C contains one or more faults that contribute to an incorrect value in I . Estimates are made according to ﬁve properties . 4 . 3 . 1 . Five properties for fault localization By piggy - backing off of the testing information base maintained by the WYSIWYT methodology , our approach maintains ﬁve properties , 1 summarized below . These properties have been devised with both software engineering and human – computer interaction principles in mind and , as alluded to in Section 1 , form the essence of our approach . We will use producer – consumer terminology to keep dataﬂow relationships clear ; that is , a producer of a cell C contributes to C ’s value , and a consumer of C uses C ’s value . In slicing terms , producers are all the cells in C ’s backward slice , and consumers are all the cells in C ’s forward slice . C is said to participate in a test ( or to have a test ) if the user has marked ( with a checkmark or an X - mark ) C or any of C ’s consumers . Of course , these markings become obsolete if an input value is edited . Our fault localization approach removes all of the X - marks and checkmarks on cells affected ARTICLE IN PRESS 1 In [ 2 ] , we presented six properties for estimating fault likelihood , but later removed the fourth property . Properties 4 and 5 in this paper correspond to Properties 5 and 6 in [ 2 ] . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 13 by such an edit , although their effects on fault likelihood information are preserved . 2 If a non - constant cell formula is edited ( which constitutes a change in the program’s source code ) , our approach internally removes the tests and fault likelihood values of the cells affected by the edit ( including the edited cell ) , and updates the user interface accordingly . Property 1 . If cell C or any of its consumers have a failed test , then C will have non - zero fault likelihood . This ﬁrst property ensures that every cell that might have contributed to the computation of an incorrect value will be assigned some positive fault likelihood . This reduces the chance that the user will become frustrated searching for a fault that is not in any of the highlighted cells , which could ultimately lead to a loss of a user’s trust in the system , and therefore violate the third design constraint that we placed on any fault localization approach for form - based visual programs in Section 4 . 1 . The property also acts as a robustness feature by ensuring that ( possibly incorrect ) checkmarks do not bring the fault likelihood of a faulty cell to zero . Property 2 . The fault likelihood of a cell C is proportional to the number of C ’s failed tests . This property is based on the assumption that the more incorrect calculations a cell contributes to , the more likely it is that the cell contains a fault . Property 3 . The fault likelihood of C is inversely proportional to the number of C ’s successful tests . The third property , in contrast to Property 2 , assumes that the more correct calculations a cell contributes to , the less likely it is that the cell contains a fault . Property 4 . An X - mark on C blocks the effects of any checkmarks of C ’s consumers ( forward slice ) from propagating to C ’s producers ( backward slice ) . This property is speciﬁcally to help narrow down where the fault is by preventing ‘‘dilution’’ of important clues . More speciﬁcally , producers that contribute only to incorrect values are darker , even if those incorrect values contribute to correct values further downstream . This prevents dilution of the cells’ colors that lead only to X - marks . ( In short , X - marks block the propagation of checkmarks ) . One example of this behavior is given in Fig . 3 . Property 5 . A checkmark on C blocks the effects of any X - marks on C ’s consumers ( forward slice ) from propagating to C ’s producers ( backward slice ) , with the exception of the minimal fault likelihood property required by Property 1 . ARTICLE IN PRESS 2 Fault localization approaches can also internally maintain the input values corresponding to X - marks and checkmarks so as to retrieve these tests if the program reverts to these input values and the program has not been edited in a manner that affects the test . ( In fact , this tactic is utilized by some versions of our prototype . ) J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 14 Similar to Property 4 , this property uses checkmarks to prune off C ’s producers from the highlighted area if they contribute to only correct values , even if those values eventually contribute to incorrect values . ( Put another way , checkmarks block most of the propagation of X - marks . ) Fig . 4 depicts this behavior , and it is largely due to Property 5 that the fault’s location has been narrowed down to being most ARTICLE IN PRESS Fig . 4 . An illustration of Property 5’s effect . In this example , the strategically placed checkmarks in the Weightedavgquiz , EC _ Award , WeightedFinal , and ErrorsExist ? cells block all but the minimal ( ‘‘Very Low’’ ) fault likelihood from the X - mark in Total _ Score . Fig . 3 . An illustration of Property 4’s effect . WeightedMidterm and Total _ Score are the darkest cells . In this example , the X - mark in Weightedavgquiz blocks any effects of the checkmark in Total _ Score from traveling upstream past the X - mark . This has the desirable effect of coloring Weightedavgquiz , which is where the fault resides , fairly dark . ( It is possible for an incorrect value to occasionally or coincidentally produce a correct value downstream , and Property 4 helps to allow for this possibility . In this example , Total _ Score has a correct value because the incorrect value of Weightedavgquiz —which the user has noted with an X - mark—has been ‘‘counteracted’’ by a second fault in the WeightedFinal cell , which has an incorrect value not yet noted by the user . ) J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 15 likely in either WeightedMidterm or Total _ Score , which are thus colored the darkest . ( In Fig . 4 , the fault is an incorrect mathematical operation in the WeightedMidterm cell ; instead of multiplying the value of the Midterm cell by 0 . 3 , Midterm is multiplied by 0 . 4 . ) 4 . 3 . 2 . Implementing the properties To implement these properties , let NumBlockedFailedTests ( C ) ( NBFT ) be the number of values belonging to C ’s consumers that are marked incorrect , but are blocked by a value marked correct along the dataﬂow path from C to the value marked incorrect . Furthermore , let NumReachableFailedTests ( C ) ( NRFT ) be the result of subtracting NBFT from the number of C ’s consumers ( i . e . , the failed tests that are not blocked ) . Finally , let there be NumBlockedSuccessfulTests ( C ) ( NBST ) and NumReachableSuccessfulTests ( C ) ( NRST ) , with deﬁnitions similar to those above . If a cell C has no failed tests , the fault likelihood of C is ‘‘None’’ . If NumReachableFailedTests ( C ) = 0 , but NumBlockedFailedTests ( C ) 4 0 , the fault like - lihood of C is ‘‘Very Low’’ . Otherwise , there are X - marks in C ’s consumers that reach C ( are not blocked by checkmarks ) , and C ’s fault likelihood is estimated using the equation below , and then mapped to a colorization using the scheme in Table 1 . 3 Fault likelihood ð C Þ ¼ max ð 1 ; 2 $ NRFT % NRST Þ : As alluded to in Section 4 . 2 , our approach leverages WYSIWYT’s current dynamic slicing support for X - marks . Just as checkmarks’ effects are propagated up the marked cell’s backward dynamic slice , so too are the effects of X - marks . Consequently , for each cell , our approach is able to maintain a list of all checkmarks and X - marks affecting the cell . These lists are how we implement our ﬁve properties to calculate the fault likelihood for a cell . The two interactions that trigger action from our fault localization approach are ( 1 ) placing a checkmark or X - mark , and ( 2 ) changing a non - constant formula . Our approach keeps track of which cells are reached by various marks so that the blocking characteristics of Properties 4 and 5 can be computed given complex , intertwined dataﬂow relationships . Let p be the number of cell C ’s producers , i . e . , ARTICLE IN PRESS Table 1 Mapping fault likelihood calculations to color intensities Intensity of color Fault likelihood ( C ) Low 1 – 2 Medium 3 – 4 High 5 – 9 Very high 10 + 3 This equation , as well as the moment when fault likelihood is mapped to a colorization scheme , has changed slightly since [ 2 ] after analyzing the results of a recent formative study of the effectiveness of our approach [ 55 ] . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 16 cells in C ’s backward dynamic slice . Furthermore , let e be the number of edges in the graph consisting of C ’s backward dynamic slice , and let m be the total number of marks ( tests ) in the program . The time complexity of placing or undoing a checkmark or X - mark is O ðð p þ e Þ m 2 Þ : Adding or modifying C ’s non - constant formula has time complexity O ð p þ em 2 Þ ; but in this case the edges e are those of a ( dataﬂow ) multigraph because a cell can be referenced multiple times in a formula . ( Details of the complexity analysis of our approach are provided in [ 56 ] . ) 5 . Study Will end users be able to beneﬁt from the fault localization approach we have just described , and if so , how will they integrate it with their own problem - solving procedures and strategies ? To explore this issue , we conducted a study investigating the following research questions : RQ1 : Do end users perceive value in the fault localization feedback over time ? RQ2 : How thoroughly do end users understand the interactive , visual fault localization feedback ? RQ3 : What types of faults do the debugging strategies of end users reveal ? RQ4 : How does fault localization feedback inﬂuence an end user’s interactive debugging strategy ? RQ5 : How do wrong testing decisions affect fault localization feedback ? To obtain the necessary qualitative information , we conducted a think - aloud study using ten end users as participants . A think - aloud study allows participants to verbalize the reasoning for their actions . Traditional experiments based on statistical methods provide quantitative information but do not provide the qualitative information we sought for this work . For example , a traditional experiment cannot explain user behaviors or reactions to fault localization feedback , or provide insights into the cognitive thought process of a user ; rather , such experiments provide only indirect clues about the human - centric issues we sought to investigate . 5 . 1 . Procedure Our participants were divided into two groups : a control group having only the features of WYSIWYT and a treatment group having both WYSIWYT and our integrated , visual fault localization approach . The control group was able to place X - marks but , unlike the treatment group , these X - marks did not trigger fault localization feedback . ( A control group was needed for the focus of RQ3 . ) Each session was conducted one - on - one between an examiner and the participant . The participant was given training on thinking aloud , and a brief tutorial ( described in Section 5 . 3 ) on the environment he or she would be using , followed by a practice task . Also , the environment had an on - demand explanation system via tooltips , available for all objects , reducing or eliminating the need for memorization . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 17 The same ( male ) examiner was used for all ten participants . The examiner was positioned behind the participant in order to be out of the participant’s ﬁeld of vision . This prevented facial expressions and body language by the examiner from inﬂuencing the participant . Furthermore , the think - aloud training and tutorial given to each participant was recited from a script . These measures were taken to ensure that the same treatment was given to each participant , and that the examiner did not inﬂuence the study’s results . After familiarizing themselves with their environment , each participant worked on the tasks detailed in Section 5 . 4 . The data collected for each session included audio transcripts , electronic transcripts capturing all user interactions with the system , post - session written questionnaires , and the examiner’s observations . Audio transcripts captured the participants’ verbalizations as they performed the given tasks . Electronic transcripts captured user actions such as editing the values in a cell , placing a checkmark or X - mark in a decision box , and turning on / off arrows indicating dataﬂow relationships between cells . Post - session questionnaires asked about the usefulness of the WYSIWYT features in ﬁnding and ﬁxing errors , and tested the participants’ understanding of our fault localization approach . ( Readers are referred to [ 57 ] for the contents of the post - session questionnaires . ) In addition , the examiner took notes of his observations during the session . 5 . 2 . Participants Our participants were 10 business majors with spreadsheet experience . We chose business majors because spreadsheets are a kind of form - based program commonly used for business applications . We required participants to be experienced with spreadsheets because we did not want learning of basic spreadsheet functionality to be a variable in our study . The participants were equally divided into two groups . We distributed participants based on their experience with spreadsheets and their grade point average so that there was an even distribution of experienced and less experienced participants in both groups . The information about their spreadsheet experience and whether they had taken a programming class was gathered via a background questionnaire that the participants ﬁlled out prior to the study—this information is summarized in Table 2 . It is fairly common these days for business students to take a high school or college programming class , but none of the participants had any professional programming experience . ARTICLE IN PRESS Table 2 The number of participants of each gender , the number who had spreadsheet experience in the speciﬁed settings , and the number who had taken programming classes Gender Spreadsheet experience ? Programming classes ? M F High school College Professional Personal High school College Control 1 4 2 4 2 4 2 1 Treatment 4 1 2 5 2 4 0 2 J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 18 5 . 3 . Tutorial Before the study’s tasks , participants were given training on thinking aloud and a brief tutorial on the environment they would use . ( Readers are referred to [ 57 ] for the entire text of the tutorials . ) Since it was essential for the participants to think aloud , the participants were asked to do two ‘‘thinking aloud’’ practice problems : adding the numbers ‘‘678’’ and ‘‘789’’ , and counting the number of windows in their parents’ house . The tutorial about the Forms / 3 environment and WYSIWYT was designed such that the participants received practice editing , testing , and debugging cells . As part of the tutorial , all participants were led through a debugging exercise . For the treatment participants , this exercise was done using the fault localization feedback in order to promote its use later in the experiment , which was necessary to obtain the data needed for our research questions . To keep the debugging exercise as similar as possible for the control participants’ version , the debugging exercise was done using dataﬂow arrows . In the tutorial , the participants performed tasks on their own machines according to verbal instructions . Participants were free to ask questions or seek clariﬁcations during the tutorial . The tutorial ended when each feature in the environment had been the subject of at least one testing , debugging , or exploratory hands - on task , and when the participant had no further questions . At the end of the tutorial , the participants were given two minutes to explore the program they were working with during the tutorial to allow them to work further with the features taught in the tutorial . As a ﬁnal tutorial task , to prepare the participants for the study’s tasks , participants were given ﬁve minutes to practice debugging a simple program . 5 . 4 . Tasks Allwood classiﬁed faults in spreadsheets as mechanical , logical , and omission faults [ 58 ] ; this scheme is also used in Panko’s work [ 6 ] . Under Allwood’s categorization , mechanical faults include simple typographical errors or wrong cell references in the cell formulas . Mistakes in reasoning were classiﬁed as logical faults . Logical faults are more difﬁcult than mechanical faults to detect and correct , and omission faults are the most difﬁcult [ 58 ] . An omission fault is information that has never been entered into the formula . Drawing from this research , we seeded ﬁve faults of varying difﬁculty into each of two Forms / 3 programs . When classifying these faults , we realized that Allwood’s scheme does not always clearly differentiate mechanical faults from logical faults . For example , the scheme does not specify how to distinguish typographical mistakes ( mechanical faults ) from mistakes in reasoning ( logical faults ) . In our study , if a seeded fault was a single incorrect character adjacent on the keyboard to the correct character ( e . g . , a 4 that should have been a 3 ) , the fault was classiﬁed as a ‘‘mechanical’’ fault—the result of a typographical error . If the fault was missing information , such as a missing cell reference or subexpression , it was classiﬁed as an ‘‘omission’’ fault . Otherwise , the fault was classiﬁed as a ‘‘logical’’ fault . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 19 The ﬁrst of our two experimental tasks was the Grade program from Fig . 1 , which computes the total score for a course given input for three quizzes , extra credit , a midterm , and a ﬁnal exam . There is also a cell that indicates when an input cell is outside the valid range . Grade had three mechanical faults , one logical fault , and one omission fault—these faults are detailed in Table 3 . This program was designed to be the ‘‘easier’’ of our two tasks based on its size , the complexity of its formulas , and our choice of seeded faults . The other program , Payroll , is presented in Fig . 5 . Into Payroll , we seeded one mechanical fault , three logical faults , and one omission fault—these faults are detailed in Table 4 . This program was much larger , had more levels of data ﬂow , and had a greater number of cells with non - constant formulas in relation to input cells when compared to the Grade program . Participants were given these two programs ( tasks ) in varying order , with instructions to test and correct any errors found in the programs . For each task , the participants were provided the unveriﬁed program and a description of the program’s expected functionality . Furthermore , the participants were provided a single example of the expected output values given speciﬁed inputs for the Grade task , and two such examples for the Payroll task . Participants had a time limit of 15 min to complete the Grade task and 30 min to complete the Payroll task . 6 . Results The primary technique we used for analyzing the data was in essence to re - create each participant’s session . To do this , we replayed the participant’s audio transcript while at the same time replaying the system - captured electronic transcripts that had captured each participant’s actions and the system responses . This procedure allowed us to view the system’s behavior for each user action while at the same time listening to what the participant said , allowing discovery of strategy choices or failure discoveries that would not have been evident from either the audio or the electronic transcript alone . The examiner’s notes were also used to be sure we considered points the examiner had noticed during the experiment . Session data were also subdivided into ‘‘episodes’’ , where each episode was considered to start when a participant showed evidence of discovering a failure ( evidenced through what they said or through the placement of an X - mark ) until that participant either ﬁxed the fault causing the failure or turned their attention to a different failure or fault . The subdivision into episodes was useful in determining the sequence in which cells were being considered to track down the faults causing an observed failure . ( Sequence information was needed for research questions RQ3 , RQ4 , and RQ5 . ) 6 . 1 . RQ1 : do end users perceive value in the fault localization feedback over time ? Blackwell’s model of attention investment [ 59 ] is one model of user problem - solving behavior predicting that users will not want to enter an X - mark unless the ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 20 AR T I C L E I N P R E SS Table 3 The faults seeded in the Grade task Cell name Faulty formula Correct formula Fault type avgquiz ( quizl + quiz2 + quiz3 ) / 4 ( quizl + quiz2 + quiz3 ) / 3 Mechanical EC _ Award if ExtraCredit 4 25 then 5 else ( if ExtraCredit X 20 then 2 else 0 ) if ExtraCredit 4 25 then 5 else ( if ExtraCredit X 20 then 3 else 0 ) Mechanical WeightedMidterm Midterm * 0 . 4 Midterm * 0 . 3 Mechanical ErrorsExist ? if quizl 4 100 or quiz2 4 100 or quiz3 4 100 or Midterm 4 100 or Final 4 10 or ExtraCredit 4 100 then ‘‘Error Exists’’ else ‘‘No Error’’ if quizl 4 100 or quiz2 4 100 or quiz3 4 100 or Midterm 4 100 or Final 4 100 or ExtraCredit 4 100 then ‘‘Error Exists’’ else ‘‘No Error’’ Logical Total _ Score if ErrorsExist ? = ‘‘No Error’’ then Weightedavgquiz + WeightedMidterm + WeightedFinal else ‘‘Cannot be computed’’ if ErrorsExist ? = ‘‘No Error’’ then Weightedavgquiz + WeightedMidterm + WeightedFinal + EC _ Award else ‘‘Cannot be computed’’ Omission J . R . R u t h r u ff e t a l . / Jou r na l o f V i s ua l L anguag e s and C o m pu t i ng 16 ( 2005 ) 3 – 40 21 beneﬁts of doing so are clear to them . The model considers the costs , beneﬁts , and risks users weigh in deciding how to complete a task . For example , if the ultimate goal is to forecast a budget using a spreadsheet , then using a relatively unknown feature such as an X - mark has cost , beneﬁt , and risk . The costs are ﬁguring out when and where to place the X - mark and thinking about the resulting feedback . The beneﬁt of ﬁnding faults may not be clear after only one X - mark ; in fact , the user may have to expend even more costs ( place more X - marks ) for beneﬁts to become clear . The risks are that going down this path will be a waste of time , or worse , will mislead the user into looking for faults in the correct formulas instead of in the incorrect ones . First , we consider whether participants , having brieﬂy seen X - marks in the tutorial , were willing to place even one X - mark to help their debugging efforts . The answer was that they were : four of the ﬁve treatment participants placed at least one X - mark , especially when they needed assistance debugging a failure ( discussed further in Section 6 . 4 ) . The participant who did not place an X - mark ( treatment participant TP3 ) explained during a post - session interview that she had forgotten about them , and wished she had used them : TP3 : I wish I could redo the problem with the X - mark . If I would have done that , it would have been a lot more easier . Given a ﬁrst use , one indicator of whether users perceive value is whether they continue to use the device . That is , in our interactive fault localization system , the ﬁrst interaction about a failure ( X - mark ) leads to feedback , and his feedback may or may not be perceived by the user as having enough value to lead him or her to place a second X - mark . In general , a difference in any interactive fault localization approach from traditional approaches is that the accuracy of feedback about fault locations must be considered at every step of the way—especially in early steps , not just at the end of some long batch of tests . As the attention investment model explains , if the early feedback is not seen as providing information that is truly of practical help , there may never be any more interactions with the system ! This was exactly the case for participant ARTICLE IN PRESS Fig . 5 . The Payroll program . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 22 AR T I C L E I N P R E SS Table 4 The faults seeded in the Payroll task Cell name Faulty formula Correct formula Fault type HealthInsur if MStatus = ‘‘Married’’ then 480 else 300 if MStatus = ‘‘Married’’ then 480 else 390 Mechanical EmployeeTaxes SocSec + Medicare + FedWithhold + InsurCost SocSec + Medicare + FedWithhold Logical SingleWithHold if AdjustMinusWithhold o 119 then 0 else ( AdjustMinusWithhold ) * 0 . 10 if AdjustMinusWithhold o 119 then 0 else ( AdjustMinusWithhold % 119 ) * 0 . 10 Logical SocSec if GrossOver87K = 0 then TotalGrossPay * 0 . 062 else GrossOver87K * 0 . 062 if GrossOver87K = 0 then TotalGrossPay * 0 . 062 else 87000 * 0 . 062 Logical InsurCost HealthInsur + LifeInsurPremium HealthInsur + LifeInsurPremium + Dental Omission J . R . R u t h r u ff e t a l . / Jou r na l o f V i s ua l L anguag e s and C o m pu t i ng 16 ( 2005 ) 3 – 40 23 TP5 , who placed only one X - mark in the Grade task , and explained after the session : TP5 : To me , putting an X - mark just means I have to go back to do more work . In his case , the X - mark he placed was in a cell whose only contributors were input cells ; consequently , because our fault localization approach does not tint input cells ( which have constant formulas rather than non - constant formulas ) , the only cell tinted was the cell in which he just placed the X - mark . Since this feedback did not add to the information he already had , it is not surprising that he found no beneﬁt from placing an X - mark . This indicates the importance of the feedback ( reward ) , even in the early stages of use ; if the reward is not deemed sufﬁcient for further attention , a user may not pursue further use . However , the other three treatment participants who placed an X - mark went on to place a second X - mark later in the session . Further , after placing this second X - mark , all three participants then went on to place a third X - mark . The successes that rewarded these participants ( detailed in Section 6 . 3 ) appear to have outweighed their perceived costs of testing and marking failures with X - marks , suggesting that the rewards of placing X - marks were sufﬁciently conveyed to these users . To help discern the perceived value of using fault localization in the participants’ view , we included an evaluation in the post - session questionnaire regarding how helpful placing X - marks was ‘‘in ﬁnding and ﬁxing errors’’ . Treatment participants TP1 and TP2 used X - marks on both problems and rated the feedback as ‘‘Helpful’’ . The one participant , TP3 , who forgot about using X - marks and wished that she had , rated the feedback as ‘‘Quite Helpful’’ . Participant TP5 , who placed only one X - mark , offered ‘‘No Opinion’’ about the value of the feedback on the post - session questionnaire . Surprisingly , participant TP4 , who used X - marks ﬁve times on the Payroll problem and whose verbal comments ( see Section 6 . 3 ) suggested that he used the feedback during his debugging , rated the feedback as ‘‘Not Helpful’’ . 6 . 2 . RQ2 : how thoroughly do end users understand the interactive feedback ? To what extent did the participants understand the message the interactive feedback was trying to communicate ? We investigated two levels of understanding : the deeper level being the ability to predict feedback under various circumstances , and the more shallow level of being able to interpret feedback received . Prediction was deemed to be a quantitatively greater level of understanding than interpretation because predicting fault localization feedback can only be done if one understands the semantics of the feedback ( i . e . , is able to accurately interpret the feedback ) . To investigate these two levels of understanding , the post - session questionnaire for our treatment participants had 11 questions , approximately evenly divided between the two levels , regarding the effects of X - marks on the interior colorings of a cell . The participants’ ability to predict behavior , as measured by six questions , was mixed . Again using producer – consumer terminology , all participants were able to correctly predict the impacts on producer cells of placing a single X - mark ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 24 ( two questions ) . About half the participants were able to predict the impacts on consumer cells of placing a single X - mark ( one question ) and to predict the impacts when multiple X - and check - marks were involved ( three questions ) . However , their ability to interpret behaviors ( the interpretation level of understanding ) was uniformly good : all four of the participants who actually used X - marks during the session were able to explain the meanings of the colorings and marks , and to say what those meanings implied about faults ( ﬁve questions ) . For example , some responses to questions about what it means when the interior of cells get darker or get lighter were : TP1 : If the color becomes lighter , the cells have less of a chance to be wrong . TP2 : The chance of an error in the darker cells is greater than in the lighter cells . TP4 ( referring to a darker cell ) : Higher chance of error in that cell . Note that these responses are not a product of the participants reciting the contents of the tutorial . The tutorial introduced participants to the mechanics of placing X - marks and recited the contents of any explanation tooltips that were on the screen , but did not contain any content regarding how to interpret the semantics of the feedback , or how to compare feedback between cells . Finally , these post - session questionnaire results are corroborated by the actions of the users themselves , as detailed in the next two sections . 6 . 3 . RQ3 : what types of faults do the debugging strategies of end users reveal ? Because this work is about fault localization , we focus on users’ abilities to identify the location of faults , as deﬁned by either an explicit verbal statement or by the fact that they edited the cell’s formula . ( Once identiﬁed , corrections usually followed ; 60 of the 69 faults were corrected once identiﬁed . ) Once a failure was spotted , users exhibited two kinds of debugging strategies to ﬁnd the fault causing the failure : an ad hoc strategy , in which they examined cell formulas in no particular order that we could discern ; and a dataﬂow strategy , which we deﬁned as following the failure’s dependencies back through cell references until they found the fault . Note that the use of a dataﬂow strategy by participants is not in itself surprising , given the contents of the tutorial . A bit more surprising is the extensive use of ad hoc debugging instead of dataﬂow , despite the participants’ practice during the tutorial with dataﬂow . A dataﬂow strategy can be accomplished through mental effort alone , but participants rarely did this : mostly they used either arrows , the fault localization feedback , or a combination of both . Note that we have categorized as dataﬂow only sink - to - source ( logically top - down ) sequences . This is not a limitation because no instances of source - to - sink sequences ( logically bottom - up ) were observed . Also note that strategy categorizations used here reﬂect what participants actually did , not what they might have chosen to do in the absence of the system’s guidance . How successful were these strategy choices ? Table 5 enumerates the participants’ strategy choices failure - by - failure and the corresponding success rates . Comparing ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 25 the ﬁrst two columns’ percentages column - wise shows that , for both participant groups , dataﬂow debugging tended to be more successful than ad hoc . Within dataﬂow , the treatment participants’ success rates with X - marks tended to exceed the dataﬂow total success rates . A row - wise comparison of the denominators in the dataﬂow column also shows that treatment participants tended to use dataﬂow strategies nearly twice as frequently as the control participants ( The table’s numerators cannot be compared column - wise or row - wise as raw values , because participants usually started with ad hoc and then sometimes switched to dataﬂow for hard - to - locate faults . ) Participants had a slightly greater tendency to identify faults with ad hoc strategies than with dataﬂow . Table 6 shows the number of times that faults were identiﬁed in ARTICLE IN PRESS Table 6 The number of faults that were identiﬁed using ad hoc and dataﬂow debugging strategies across all participants Grade Payroll Ad hoc Dataﬂow Ad hoc Dataﬂow Control 13 6 6 3 Treatment 13 9 9 6 Table 5 The success rates of identifying a fault contributing to an observed failure ( faults identiﬁed / failures observed ) , for each debugging strategy Ad hoc Dataﬂow Total Dataﬂow total Using X - mark Grade Control 13 / 20 6 / 6 N / A 19 / 26 ( 65 % ) ( 100 % ) ( 73 % ) Treatment 13 / 16 9 / 10 5 / 5 22 / 26 ( 81 % ) ( 90 % ) ( 100 % ) ( 85 % ) Total 26 / 36 15 / 16 41 / 52 ( 72 % ) ( 94 % ) ( 79 % ) Payro11 Control 6 / 17 3 / 6 N / A 9 / 23 ( 35 % ) ( 50 % ) ( 39 % ) Treatment 9 / 21 6 / 12 3 / 5 15 / 33 ( 43 % ) ( 50 % ) ( 60 % ) ( 45 % ) Total 15 / 38 9 / 18 24 / 56 ( 39 % ) ( 50 % ) ( 43 % ) For each row and column except ‘‘Total’’ , a numerator of 25 is the maximum possible . ( The maximum possible is 25 because each task had ﬁve faults , and the control and treatment groups who worked on these tasks were each composed of ﬁve participants . ) J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 26 the Grade and Payroll tasks using each strategy . It is not all that surprising that more faults were identiﬁed with ad hoc strategies because , as Table 5 indicates , ad hoc strategies were used more often than dataﬂow strategies . However , in Table 5 , note that the percentage of faults identiﬁed per failure was consistently higher using dataﬂow . With what types of faults did the strategies matter ? To investigate this , we ﬁrst looked at the faults that were the easiest to correct : mechanical faults , according to Allwood’s classiﬁcation scheme [ 58 ] . Easy faults : The participants’ strategy choices did not matter with the easiest faults . The easiest are mechanical faults , according to Allwood , and were usually found regardless of the strategy used . Over all tasks and all participants , 33 of the 40 mechanical faults were identiﬁed . Because strategy did not seem to matter in identifying easy faults , we next looked at the faults that were the harder to correct : logical and omission faults , according to Allwood . Hard faults : Participants had a much harder time identifying the harder faults . Of the 60 logical and omission faults among the treatment participants , only 32 were identiﬁed . However , the ad hoc and dataﬂow strategies were almost identically split in terms of identifying these harder faults—17 of the faults were identiﬁed using ad hoc strategies , and 15 were identiﬁed using dataﬂow strategies . Since these data were not very revealing about when different strategies paid off , we considered the locality of these types of faults relative to where in the program’s output the participant ﬁrst noticed a failure that was caused by one of these faults . Locality , of course , is a characteristic of faults that we could not use to select the faults to seed into our programs because the characteristic is based on each participant’s individual actions . Local faults : Strategy did not matter much with the ‘‘local’’ faults ( those in which the failed value spotted by the participant was in the same cell as the faulty formula ) . This is often the case in smaller programs , where there are fewer cells to reference and the likelihood of a local fault is greater , and probably contributed to both groups’ greater success in the Grade task . Non - local faults : Strategy mattered a great deal for the non - local faults . Over all of the participants and tasks , 16 non - local faults were identiﬁed—all using dataﬂow . Not a single non - local fault was identiﬁed using the ad hoc strategy . In fact , for seven of these non - local fault identiﬁcations ( by six different participants ) , the participants began their search for the fault using an ad hoc strategy and , when unable to succeed , switched to a dataﬂow strategy , with which they succeeded in ﬁnding the fault . Our fault localization approach augments dataﬂow strategies , which is especially illustrated by treatment participants TP4 and TP5 . Both participants found all faults in the smaller Grade task . Both participants also found the mechanical fault and one of the logical faults in the larger Payroll task in short order . But then , they both got stuck on where to go next . At this critical juncture , TP4 decided to place an X - mark ( carried out with a right - click ) on a failure . Once he saw the feedback , he rapidly progressed through the rest of the task , placing ﬁve X - marks and correcting the ﬁnal three faults in only seven minutes . The transcripts show that the initial ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 27 X - mark , which initiated the ( dataﬂow - oriented ) fault localization feedback , was a key turning point for him : TP4 ( thinking aloud ) : Right click that ‘ cause I know it ’ s incorrect , highlights everything that ’ s possible error y EmployeeTaxes is also incorrect . My NetPay is incorrect . Adjusted gross pay ( AdjustGrossPay ) is incorrect , so click those wrong . Whereas TP4 made the deﬁning decision to use the X - mark , TP5 did not . TP5’s pattern of looking at cells gradually became ad hoc . He started randomly looking at formulas . He made decisions about the values in various cells and eventually happened upon a local fault , bringing his total to three . He never searched the dataﬂow path any further than one cell . He said ‘‘ I ’ m getting confused here ’’ numerous times , but did not change his approach . 6 . 4 . RQ4 : how does this feedback inﬂuence an interactive debugging strategy ? We had initially expected that treatment participants would always place X - marks whenever they observed a failure and would then use the subsequent visual feedback to guide their interactive debugging strategy , but this was not the case . Instead , they were very conservative , placing only 15 X - marks in total . They seemed to view the X - marks as a device to be called upon only when they were in need of assistance . ( Recent ﬁndings on ‘‘politeness’’ in human – computer communications may be relevant here [ 60 ] . If participants viewed X - marks as criticisms , then Nass et al . ’s ﬁndings would suggest that , just as people are reluctant to criticize a human face - to - face , the participants may have felt reluctant to place an X - mark unless they were extremely sure doing so was warranted . ) For example , only late in the session , when treatment participant TP1 got stuck debugging the failures , did he turn to the fault localization device : TP1 ( thinking aloud ) : I don ’ t know how to check the kind of error it is . I ’ ll mark it wrong and see what happens . When participants did place an X - mark , the visual feedback often had an immediate impact on their interactive debugging strategy : regardless of what their previous strategy had been , as soon as the feedback appeared , the participants switched to a dataﬂow strategy by limiting their search to those cells with estimated fault likelihood and ignoring cells with no assigned fault likelihood . TP1 ( thinking aloud ) : I ’ m going to right - click on the Total _ Score . See that the weighted average , the weighted quiz ( Weightedavgquiz ) , the WeightedMidterm , and the WeightedFinal , and the error box ( Error - sExist ? ) all turn pink . The fault localization feedback beckons the user toward a dataﬂow strategy , but it has attributes dataﬂow arrows do not have . First , it produces a smaller search space ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 28 than the dataﬂow arrows , because it highlights only the producers that actually did contribute to a failure ( a combination of dynamic slices ) , rather than including the producers that could contribute to failures in other circumstances ( a combination of static slices ) . Second , it prioritizes the order in which the users should consider the cells , so that the faulty ones are likely to be considered earliest . The above shows that TP1 took advantage of the reduction in search space brought about by the tinting of the producers of a cell with a failure . But did the participants also take advantage of this prioritization , indicated by some cells being darker than others ? Our electronic transcripts indicate that the answer to this question is yes . When the participants searched cell formulas for a fault after placing an X - mark , 77 % of these searches initially began at the cell with the darkest interior shading . As an example , here is a continuation of the above quote from TP1 after placing an X - mark : TP1 ( thinking aloud ) : See that the weighted average , the weighted quiz ( Weighted - avgquiz ) , the WeightedMidterm , and the WeightedFinal , and the error box ( ErrorsExist ? ) all turn pink . The Total _ Score box is darker though . Another example can be found in a post - session questionnaire response from TP5 : TP5 : The problem has been narrowed to the darker cells . When the fault was not in the darkest cell , participants’ searches would gradually progress to the next darkest cell and so on . Some participants realized that the colorings’ differentiations could be enhanced if they made further testing decisions by placing checkmarks and X - marks , carried out by left - or right - clicking a cell’s decision box . TP4 ( thinking aloud ) : Right click that ‘ cause I know it ’ s incorrect , highlights everything that ’ s possible errors . Now , I know my TotalGrossPay is correct . I ’ ll left click that one and simplify it . From the results of this and the previous sections , it is clear that fault localization succeeded in drawing participants into a suitable strategy ( dataﬂow ) , even when participants had not realized that a dataﬂow strategy would help them succeed better than ad hoc approaches . Further , it is clear that participants were inﬂuenced by the feedback’s prioritization information when more than one color was present—in that they looked ﬁrst to the darkest cells , and then to the next darkest , and so on— and that their doing so was tied to the dataﬂow strategy , which located a class of faults that was not found without it ( as detailed in Section 6 . 3 ) . 6 . 5 . RQ5 : how do wrong testing decisions affect fault localization feedback ? Being human , the end - user participants in our study made some mistakes in their testing decisions . Here we consider the types of mistakes they made , and the impact of these mistakes on the users’ successful use of the fault localization feedback . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 29 ( Because the control participants did not have fault localization feedback , we consider only the treatment participants . ) In total , the ﬁve treatment participants placed 241 checkmarks , of which 11 ( 4 . 56 % ) were wrong—that is , the user pronounced a value correct when in fact it was incorrect . Surprisingly , however , none of the 15 X - marks placed by participants were incorrect . A possible reason for this difference may be a perceived seriousness of contradicting a computer’s calculations , meaning participants were only willing to place X - marks when they were really sure their decision was correct . ( This is consistent with observations in an earlier small study [ 61 ] of a participant exhibiting great faith in the computer’s output , and with Nass et al . ’s politeness ﬁndings [ 60 ] . ) For example , at one point , participant TP1 placed an X - mark in a cell , then reconsidered the mark because he was unsure the X - mark was really warranted . TP1 ( thinking aloud ) : So , I ’ ll right click on that one . I ’ m not sure if this is right . Eh , I ’ ll leave it as a question mark . In contrast , checkmarks were often placed even if the user was unsure whether the marks were warranted . Our verbal transcripts include ten different statements by treatment participants with this sentiment . For example , consider the following quotes from two treatment participants : TP1 ( thinking aloud ) : I ’ ll go ahead and left click the LifeInsurPremium box because I think that one ’ s right for now . TP3 ( thinking aloud ) : I think these are right , ( so ) check that . What impact did the wrong checkmarks have on fault localization ? Four of the 11 wrong checkmarks were placed with a combination of X - marks , resulting in incorrect fault localization feedback . All four of these particular checkmarks , placed by three different participants , adversely affected the participants’ debugging efforts . For example , during the Grade task , TP1 placed an incorrect checkmark in the ( faulty ) WeightedMidterm cell . He later noticed that the Total _ Score cell , although its formula was correct , had an incorrect value ( due to the fault in WeightedMidterm ) . Unable to detect the source of this failure , he turned to the fault localization approach and placed an X - mark in the Total _ Score cell : TP1 ( thinking aloud ) : The Total _ Score box is darker though . And it says the error likelihood is low , while these other boxes that are a little lighter say the error likelihood is very low . Ok , so , I ’ m not sure if that tells me anything . The participant knew that Total _ Score was correct . Fig . 6 illustrates that had it not been for the wrong checkmark , the faulty WeightedMidterm cell would have been one of the two darkest cells in the program . Instead , the wrongly placed checkmark caused WeightedMidterm to be colored the same as its correct siblings , thus providing the participant with no insightful fault localization feedback . ( The participant eventually corrected the fault after a search of over six minutes . ) ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 30 Participant TP2 , faced with a similar scenario as in Fig . 6 , was overcome with frustration and confusion : TP2 ( thinking aloud ) : All right y so , I ’ m doing something wrong here ( long pause ) I can ’ t ﬁgure out what I ’ m doing wrong . TP2’s confusion resulted in nearly seven minutes of inactivity . He eventually located and corrected the fault , but remained ﬂustered for the duration of the session . As this example points out , it may not be realistic to ignore the fact that end users will provide some wrong information . In our study , even though less than 5 % of the checkmarks placed by the participants were incorrectly placed , these incorrect marks adversely affected the debugging efforts of 60 % ( three out of ﬁve ) of the treatment ARTICLE IN PRESS Fig . 6 . ( Top ) The Grade task , with an incorrect checkmark in WeightedMidterm , as seen by participant TP1 . Total _ Score is the darkest , and the other six all are the same shade . ( Bottom ) What TP1 would have seen without the wrong checkmark : WeightedMidterm would be as dark as Total _ Score . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 31 participants ! Given the presence of mistakes , robustness features may be necessary to allow success even in the presence of mistakes . Toward this end , recall that Property 1 from Section 4 . 3 . 1 ensures that our fault localization approach colors every cell in the dataﬂow chain contributing to a failure—even the cells the user may have previously checked off . Clearly , however , this attempt at robustness was not enough to completely counteract the impacts of mistakes . Alternative approaches whose build - up of historical information can outweigh some small number of errors [ 55 ] are another possibility . 7 . Discussion Previous fault localization research has focused primarily on techniques to aid professional programmers performing batch testing . In contrast , our study focuses on supporting end - user programmers with an interactive fault localization approach . In generalizing our experiences in end - user debugging , there are many issues that future work in this area may need to consider . 7 . 1 . When was fault localization used ? We expected that participants would place X - marks on every cell with an incorrect output value ( failure ) . We further expected that participant verbalizations and electronic transcripts would show that our interactive , visual feedback effectively guided the debugging efforts of participants . While our feedback proved helpful to participants when they actually triggered it , in most cases , participants did not initially invoke our fault localization approach for assistance . Instead , they treated fault localization as a resource to be called upon only when they had exhausted their own debugging abilities . The reason for this behavior might be tied to attention investment [ 59 ] . According to this model , users manage their attention ( time ) by attempting to maximize their beneﬁt - cost return in terms of the time spent to get their work done . Applied to debugging , this suggests that as long as users are progressing at a good rate at ﬁnding the faults ( the beneﬁt ) , users may see no reason to expend time ( cost ) on ﬁguring out which cells to mark correct or incorrect . Of course , the amount of cost and beneﬁts users perceive would vary from person to person , since these perceptions could be inﬂuenced by many factors such as their past experience , their aversion to risk , their perception of the slope of the learning curve , and their conﬁdence level . However , once the easiest faults have been found and the time ( cost ) to ﬁnd more faults starts to increase , the extra time to mark cells may be perceived as being worthwhile , potentially saving time on the ‘‘hard - to - ﬁnd’’ faults . 7 . 2 . When was fault localization helpful ? Many of the faults that contributed to failures observed by participants were either ‘‘local’’ to the failure or would be considered by Allwood as ‘‘easy’’ [ 58 ] . In ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 32 both these cases , participants were usually able to locate the fault relatively easily , and without the assistance of our fault localization approach ( or even a dataﬂow - based debugging strategy ) . Hence , our fault localization support did not appear to be especially useful to participants when searching for easy and / or local faults , a result we did not anticipate . However , fault localization proved especially useful for participants searching for non - local faults ( Recall from Section 6 . 3 that not a single non - local fault was identiﬁed using an ad hoc strategy ; all 16 identiﬁed non - local faults were localized using a dataﬂow - based debugging strategy . ) Some participants realized without help that a dataﬂow strategy was needed , but some did not . While dataﬂow - based debugging strategies may seem intuitively superior in the eyes of trained computer scientists , such strategies may not come naturally to end - user programmers . One key way our fault localization approach helped was to lead participants toward a suitable strategy . Once participants were engaged in a suitable strategy , fault localization helped further by prioritizing the order they should follow the strategy . 7 . 3 . Mistakes in end - user debugging Our study brings a unique approach to fault localization by considering interactive debugging with respect to end - user programmers . Our results suggest that end users may make mistakes during interactive debugging . Moreover , in our study , these mistakes had a big impact on the success of the majority of our treatment participants . This brings up a variety of issues that future work on end - user debugging may need to consider . Recall from Section 4 that we employed a dicing - like heuristic as the cornerstone of our fault localization approach . We hypothesized that end users might make mistakes during interactive debugging , and such mistakes would violate the ﬁrst assumption of the Dicing Theorem [ 54 ] that testing has been reliable . ( In fact , the correctness of testing decisions is a common assumption in software engineering research . ) This hypothesis was supported through our study , as 4 . 56 % of the WYSIWYT checkmarks placed by our participants were incorrectly assigned . This result indicates that fault localization approaches explored in previous research , such as program dicing , may be inherently unsuitable for end - user programming environments . We also found that while our study’s participants made incorrect testing decisions regarding perceived correct values ( i . e . , erroneously placed checkmarks in WYSIWYT ) , they never pronounced values incorrect when those values were in fact correct ( i . e . , erroneously placed X - marks ) . This trend was later corroborated by a recent empirical study [ 62 ] that we conducted . The ﬁnding could be tied to the pervasive phenomenon of positive test bias—the tendency to choose tests that conﬁrm one’s hypothesis rather than tests that refute it [ 63 ] . One lesson that developers of end - user programming environments might take away from this result is to place greater trust in the ‘‘X - mark - equivalent’’ decisions made by users , and be more wary when considering the impact of a checkmark on fault likelihood ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 33 calculations . Fault localization approaches may also beneﬁt from including features to enhance robustness in the face of a few mistakes . 7 . 4 . The importance of early feedback In our research into fault localization for end users , we strove for an approach that would provide useful feedback at all stages of the debugging process , from beginning to end . However , our study helped us to realize that the importance of any approach’s early feedback when targeting end users . In our study , if an approach’s early feedback did not seem to be useful , users did not give the fault localization device a chance to produce better feedback later . This may be the most challenging result of our study from the perspective of future fault localization approaches . It is not surprising that previous fault localization research has not focused on approaches providing useful early feedback—after all , early feedback is of little consequence to professional programmers performing batch testing of test suites . Yet this early feedback may be paramount to the success of an interactive fault localization approach in an end - user programming environment . The lesson for developers of end - user programming environments is that the fault localization approaches best suited for end users may be those that provide quality early feedback , even if sacriﬁcing the quality of later feedback is necessary . 4 8 . Threats to validity Every study has threats to the validity of its results , and these must be considered in order to assess the meaning and impact of results . This section discusses potential threats to the validity of our study and , where possible , how we attempted to mitigate the impact of these threats on our results . ( See [ 64 ] for a general discussion of validity evaluation and a threats classiﬁcation . ) Threats to internal validity are other factors that may be responsible for our results . It is possible that the speciﬁc faults seeded in our two programs are responsible for our results . To mitigate this factor , as described in Section 5 . 4 , we seeded faults according to Allwood’s classiﬁcation scheme [ 58 ] to ensure that different types of faults were seeded in our tasks . Another threat to the internal validity of our study is due to the possibility that participants may not have understood programs’ functionality sufﬁciently to correct the faults . Also , the study’s time limits could have interacted with the learning styles of some participants . It is also possible that the one - on - one nature of the study between the participant and the examiner intimidated the participants , or caused them to try to give answers that they thought the examiner wanted to hear , but this ﬁnal limitation is inherent in any think - aloud study . ARTICLE IN PRESS 4 A recent formative experiment [ 55 ] examines the quality of the early and later feedback provided by the approach presented in this paper , as well as the feedback of two separate fault localization approaches . J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 34 Threats to construct validity question whether the results of a study are based on appropriate information . ( Stated another way , threats to construct validity question whether the dependent variables of the study are appropriate . ) Because of the qualitative information necessary to answer our research questions , many of our results come from the verbalizations of our participants . However , one limitation of using participant verbalizations is the possibility that our participants did not vocalize some of their thought processes during the completion of the study’s tasks . If this was the case , it is possible that we were deprived of information that could have changed the conclusions of our study . ( Verbalizations may then be considered a dependent variable of our experiment with inherent limitations . ) We attempted to mitigate this possibility by politely requesting that participants ‘‘think aloud’’ when they displayed periods of silence . But again , this threat is characteristic of all think - aloud studies . Threats to external validity limit the extent to which results can be generalized . In considering this matter , program representativeness is an issue facing our study . If the programs used in our study did not mimic those that real end users create , our results may not generalize . Also , to better control study conditions and ensure that participants could complete the tasks in the alloted time , our programs were not large , and therefore may not necessarily be representative of real end - user programs . In future work , although it may sacriﬁce some internal validity , a solution to this issue may be to replicate this study using programs obtained from real end users with real faults . An alternative solution may be to gather additional empirical evidence through future studies using a greater range and number of programs . Finally , the fault seeding process was chosen to help control the threats to internal validity , but this came at a cost of some external validity , because the faults and fault patterns of real end users may differ from those introduced into our study tasks . 9 . Fault localization in other paradigms The design of a fault localization approach can be broken down into three components : the ‘‘information base’’ maintained by the approach , how that information base is leveraged by a heuristic to estimate the fault likelihood of a given program point , and how that fault likelihood is communicated to a user . Given these components , we believe that it is feasible to extend our approach to other programming paradigms . Because the visualization mechanisms of communicating fault likelihood calculations are highly dependent on the targeted paradigm , we discuss some issues that may arise in extending our approach’s information base and heuristic to other paradigms , beginning with the implications stemming from our heuristic . 9 . 1 . Heuristic implications As mentioned in Sections 1 and 4 . 3 . 1 , we based our heuristic around ﬁve properties , which were chosen with human - centric principles in mind as much as ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 35 software engineering considerations . These properties reﬂect how the information base is manipulated ( e . g . , tracking the number of blocked failed tests ) just as much as the mathematical calculations that map testing information to fault likelihood values . A valuable characteristic of these properties is their generality . Although our properties were stated using cells and their values as examples of variables and cell formulas as examples of program points , the properties could be applied to other paradigms’ equivalent variable and program point structures . For example , in Prograph [ 65 ] , each node with its incoming wires is a program point corresponding to a cell formula , and each incoming or outgoing port on a dataﬂow node is a variable corresponding to a cell value . It follows that the mathematical calculations that map testing information to fault likelihood values can also be adjusted as needed in other programming environments . This generality means that the only assumption made by our properties is that the system has knowledge of which tests were dynamically exercised by each program point . Thus , we turn to the availability of systems and methodologies to track this information next . 9 . 2 . Information base The information base of our approach is a record of tests affecting each variable . To obtain this testing information , we coupled our fault localization approach to the WYSIWYT testing methodology ; therefore , because of the generality of the properties , any paradigm capable of supporting WYSIWYT is capable of supporting our fault localization approach . Although WYSIWYT was designed for form - based visual programs , it is generalizable to other programming paradigms as well . For example , there has been Prograph - based work to adapt the methodology to the dataﬂow paradigm [ 66 ] . WYSIWYT has also been successfully extended to the screen transition paradigm [ 67 ] , which has a strong relationship with rule - based programming and also to more traditional state transition diagrams . However , even without WYSIWYT , any other form of maintaining the same testing information would also provide the necessary information base . 10 . Conclusions This paper presents an entirely visual , interactive approach to fault localization for end - user programmers . Our approach estimates the likelihood that each program point contains a fault , and interactively updates this fault likelihood information as future testing decisions are made by a user . In a think - aloud study that we conducted using this approach , visual fault localization feedback guided participants into effective dataﬂow - based debugging strategies , which were tied with the identiﬁcation of difﬁcult - to - detect faults . We also found that our participants made mistakes during interactive testing and debugging that adversely affected their debugging ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 36 efforts . We hope that the results of our study will help developers of future fault localization approaches and future programming environments better address the unique needs of end - user programmers . Acknowledgements We thank the members of the Visual Programming Research Group at Oregon State University for their feedback and help . We also thank the anonymous reviewers for their comments and suggestions , which improved the quality and presentation of this paper . This work was supported in part by the National Science Foundation under ITR - 0082265 and in part by the EUSES Consortium via NSF grant ITR - 0325273 . The opinions and conclusions in this paper are those of the authors and do not necessarily represent those of the NSF . References [ 2 ] J . Reichwein , G . Rothermel , M . Burnett , Slicing spreadsheets : an integrated methodology for spreadsheet testing and debugging , in : Proceedings of the Second Conference on Domain Speciﬁc Languages , October 1999 , pp . 25 – 38 . [ 3 ] B . W . Boehm , C . Abts , A . W . Brown , S . Chulani , Software Cost Estimation with COCOMO II , Prentice - Hall , PTR , Upper Sadle River , NJ , 2000 . [ 4 ] D . Cullen , Excel snafu costs ﬁrm $ 24m , The Register , June 19 , 2003 , http : / / www . theregister . co . uk / content / 67 / 31298 . html , Last Accessed : July 19th , 2004 . [ 5 ] R . Panko , Finding spreadsheet errors : most spreadsheet errors have design ﬂaws that may lead to long - term miscalculation , Information Week , May 1995 , p . 100 . [ 6 ] R . Panko , What we know about spreadsheet errors , Journal on End User Computing ( 1998 ) 15 – 21 . [ 7 ] ANSI / IEEE , IEEE Standard Glossary of Software Engineering Terminology , IEEE , New York , 1983 . [ 8 ] C . Cook , M . Burnett , D . Boom , A bug’s eye view of immediate visual feedback in direct - manipulation programming systems , in : Proceedings of Empirical Studies of Programmers : Seventh Workshop , Alexandria , VA , October 1997 , pp . 20 – 41 . [ 9 ] M . Burnett , G . Rothermel , C . Cook , End – user software engineering , Communications of the ACM 47 ( 9 ) ( 2004 ) 53 – 58 . [ 10 ] A . Ambler , M . Burnett , B . Zimmerman , Operational versus deﬁnitional : a perspective on programming paradigms , Computer 25 ( 9 ) ( 1992 ) 28 – 43 . [ 11 ] E . Chi , J . Riedl , P . Barry , J . Konstan , Principles for information visualization spreadsheets , IEEE Computer Graphics and Applications , July / August 1998 . [ 12 ] B . Myers , Graphical techniques in a spreadsheet for specifying user interfaces , in : Proceedings of the ACM Conference on Human Factors in Computing Systems , May 1991 , pp . 243 – 249 . [ 13 ] T . Smedley , P . Cox , S . Byme , Expanding the utility of spreadsheets through the integration of visual programming and user interface objects , in : ACM Workshop on Advanced Visual Interfaces , May 1996 , pp . 148 – 155 . [ 14 ] A . Ambler , The Formulate visual programming language , Dr . Dobb’s Journal ( 1999 ) 21 – 28 . [ 15 ] S . Peyton Jones , A . Blackwell , M . Burnett , A user - centered approach to functions in Excel , in : Proceedings of the ACM International Conference on Functional Programming , Uppsala , Sweden , August 25 – 29 , 2003 , pp . 165 – 176 . [ 16 ] G . Viehstaedt , A . Ambler , Visual representation and manipulation of matrices , Journal of Visual Languages and Computing 3 ( 3 ) ( 1992 ) 273 – 298 . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 37 [ 17 ] M . Burnett , R . Hossli , T . Pulliam , B . VanVoorst , X . Yang , Toward visual programming languages for steering in scientiﬁc visualization : a taxonomy , IEEE Computer Science and Engineering 1 ( 4 ) ( 1994 ) . [ 18 ] M . Burnett , S . Chekka , R . Pandey , FAR : an end - user language to support cottage e - services , in : Proceedings of the IEEE Symposium on Human - Centric Languages , Stresa , Italy , September 2001 , pp . 195 – 202 . [ 19 ] M . Burnett , J . Atwood , R . Djang , H . Gottfried , J . Reichwein , S . Yang , Forms / 3 : a ﬁrst - order visual language to explore the boundaries of the spreadsheet paradigm , Journal of Functional Programming 11 ( 2 ) ( 2001 ) 155 – 206 . [ 20 ] M . Burnett , H . Gottfried , Graphical deﬁnitions : expanding spreadsheet languages through direct manipulation and gestures , ACM Transactions on Computer – Human Interaction 5 ( l ) ( 1998 ) l – 33 . [ 21 ] S . Reiss , Visualization for software engineering—programming environments , in : J . Stasko , J . Domingue , M . Brown , B . Price ( Eds . ) , Software Visualization : Programming as a Multimedia Experience , MIT Press , Cambridge , MA , 1998 , pp . 259 – 276 . [ 22 ] S . Reiss , Graphical program development with PECAN program development systems , in : Proceedings of the Symposium on Practical Software Development Environments , April 1984 . [ 23 ] T . Teitelbaum , T . Reps , The Cornell program synthesizer : a syntax directed programming environment , Communications of the ACM 24 ( 9 ) ( 1981 ) 563 – 573 . [ 24 ] D . Notkin , R . Ellison , G . Kaiser , E . Kant , A . Habermann , V . Ambriola , C . Montanegero , Special issue on the Gandalf project , Journal on Systems and Software 5 ( 2 ) ( 1985 ) . [ 25 ] H . Lieberman , C . Fry , ZStep 95 : a reversible , animated source code stepper , in : J . Stasko , J . Domingue , M . Brown , B . Price ( Eds . ) , Software Visualization : Programming as a Multimedia Experience , MIT Press , Cambridge , MA , 1998 , pp . 277 – 292 . [ 26 ] N . Heger , A . Cypher , D . Smith , Cocoa at the Visual Programming Challenge 1997 , Journal of Visual Languages and Computing 9 ( 2 ) ( 1998 ) 151 – 168 . [ 27 ] J . Atwood , M . Burnett , R . Walpole , E . Wilcox , S . Yang , Steering programs via time travel , in : Proceedings of the IEEE Symposium on Visual Languages , Boulder , CO , September 3 – 6 , 1996 , pp . 4 – 11 . [ 28 ] D . Kimelman , B . Rosenburg , T . Roth , Visualization of dynamics in real world software systems , in : J . Stasko , J . Domingue , M . Brown , B . Price ( Eds . ) , Software Visualization : Programming as a Multimedia Experience , MIT Press , Cambridge , MA , 1998 , pp . 293 – 314 . [ 29 ] M . Heath , A . Malony , D . Rover , Visualization for parallel performance evaluation and optimization , in : J . Stasko , J . Domingue , M . Brown , B . Price ( Eds . ) , Software Visualization : Programming as a Multimedia Experience , MIT Press , Cambridge , MA , 1998 , pp . 347 – 365 . [ 30 ] S . Eick , Maintenance of large systems , in : J . Stasko , J . Domingue , M . Brown , B . Price ( Eds . ) , Software Visualization : Programming as a Multimedia Experience , MIT Press , Cambridge , MA , 1998 , pp . 315 – 328 . [ 31 ] A . J . Ko , B . A . Myers , Designing the Whyline : a debugging interface for asking questions about program failures , in : Proceedings of the ACM Conference on Human Factors in Computing Systems , Vienna , Austria , April 24 – 29 , 2004 , pp . 151 – 158 . [ 32 ] T . Igarashi , J . D . Mackinlay , B . - W . Chang , P . T . Zellweger , Fluid visualization of spreadsheet structures , in : Proceedings of the IEEE Symposium on Visual Languages , 1998 , pp . 118 – 125 . [ 33 ] J . Sajaniemi , Modeling spreadsheet audit : a rigorous approach to automatic visualization , Journal on Visual Languages and Computing 11 ( l ) ( 2000 ) 49 – 82 . [ 34 ] J . S . Davis , Tools for spreadsheet auditing , International Journal on Human – Computer Studies 45 ( 1996 ) 429 – 442 . [ 35 ] D . A . Carr , End - user programmers need improved development support , in : CHI 2003 Workshop on Perspectives in End - User Development , April 2003 , pp . 16 – 18 . [ 36 ] R . C . Miller , B . A . Myers , Outlier ﬁnding : focusing user attention on possible errors , in : Proceedings of the ACM Symposium on User Interface Software and Technology , November 2001 , pp . 81 – 90 . [ 37 ] M . Burnett , C . Cook , O . Pendse , G . Rothermel , J . Summet , C . Wallace , End - user software engineering with assertions in the spreadsheet paradigm , in : Proceedings of the 25th International Conference on Software Engineering , Portland , OR , May 3 – 10 , 2003 , pp . 93 – 103 . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 38 [ 38 ] A . Wilson , M . Burnett , L . Beckwith , O . Granatir , L . Casburn , C . Cook , M . Durham , G . Rothermel , Harnessing curiosity to increase correctness in end - user programming , in : Proceedings of the ACM Conference on Human Factors in Computing Systems , Fort Lauderdale , FL , April 5 – 10 , 2003 , pp . 305 – 312 . [ 39 ] Y . Ayalew , R . Mittermeir , Spreadsheet debugging , in : Proceedings of the European Spreadsheet Risks Interest Group , Dublin , Ireland , July 24 – 25 , 2003 . [ 40 ] F . Tip , A survey of program slicing techniques , Journal on Programming Languages 3 ( 3 ) ( 1995 ) 121 – 189 . [ 41 ] H . Agrawal , J . R . Horgan , S . London , W . E . Wong , Fault localization using execution slices and dataﬂow tests , in : Proceedings of the International Symposium Software Reliability Engineering , Toulouse , France , October 1995 , pp . 143 – 151 . [ 42 ] J . A . Jones , M . J . Harrold , J . Stasko , Visualization of test information to assist fault localization , in : Proceedings of the 24th International Conference on Software Engineering , Orlando , FL , May 19 – 25 , 2002 , pp . 467 – 477 . [ 43 ] H . Pan , E . Spafford , Toward automatic localization of software faults , in : Proceedings of the 10th Paciﬁc Northwest Software Quality Conference , October 1992 . [ 44 ] M . Burnett , A . Sheretov , G . Rothermel , Scaling up a ‘What You See Is What You Test’ methodology to spreadsheet grids , in : Proceedings of the IEEE Symposium on Visual Languages , Tokyo , Japan , September 13 – 16 , 1999 , pp . 30 – 37 . [ 45 ] G . Rothermel , M . Burnett , L . Li , C . Dupuis , A . Sheretov , A methodology for testing spreadsheets , ACM Transactions on Software Engineering and Methodology 10 ( l ) ( 2001 ) 110 – 147 . [ 46 ] G . Rothermel , L . Li , C . Dupuis , M . Burnett , What you see is what you test : a methodology for testing form - based visual programs , in : Proceedings of the 20th International Conference on Software Engineering , June 1998 , pp . 198 – 207 . [ 47 ] M . Fisher , M . Cao , G . Rothermel , C . R . Cook , M . M . Burnett , Automated test case generation for spreadsheets , in : Proceedings of the 24th International Conference on Software Engineering , Orlando , FL , May 19 – 25 , 2002 , pp . 141 – 151 . [ 48 ] M . Fisher II , D . Jin , G . Rothermel , M . Burnett , Test reuse in the spreadsheet paradigm , in : Proceedings of the IEEE International Symposium on Software Reliability Engineering , November 2002 . [ 49 ] V . Krishna , C . Cook , D . Keller , J . Cantrell , C . Wallace , M . Burnett , G . Rothermel , Incorporating incremental validation and impact analysis into spreadsheet maintenance : an empirical study , in : Proceedings of the International Conference on Software Maintenance , Florence , Italy , November 2001 , pp . 72 – 81 . [ 50 ] K . J . Rothermel , C . R . Cook , M . M . Burnett , J . Schonfeld , T . R . G . Green , G . Rothermel , WYSIWYT testing in the spreadsheet paradigm : an empirical evaluation , in : Proceedings of the 22nd International Conference on Software Engineering , Limerick , Ireland , June 4 – 11 , 2000 , pp . 230 – 239 . [ 51 ] C . Corritore , B . Kracher , S . Wiedenbeck , Trust in the online environment , in : HCI International , vol . 1 , New Orleans , LA , August 2001 , pp . 1548 – 1552 . [ 52 ] M . Weiser , Program slicing , IEEE Transactions on Software Engineering 10 ( 4 ) ( 1984 ) 352 – 357 . [ 53 ] B . Korel , J . Laski , Dynamic slicing of computer programs , Journal of Systems and Software 13 ( 3 ) ( 1990 ) 187 – 195 . [ 54 ] J . R . Lyle , M . Weiser , Automatic program bug location by program slicing , in : Proceedings of the Second International Conference on Computers and Applications , 1987 , pp . 877 – 883 . [ 55 ] J . Ruthruff , E . Creswick , M . Burnett , C . Cook , S . Prabhakararao , M . Fisher II , M . Main , End - user software visualizations for fault localization , in : Proceedings of the ACM Symposium on Software Visualization , San Diego , CA , June 11 – 13 , 2003 , pp . 123 – 132 . [ 56 ] J . Reichwein , M . M . Burnett , An integrated methodology for spreadsheet testing and debugging , Technical Report 04 - 60 - 02 , Oregon State University , Corvallis , OR , January 2004 , http : / / eecs . oregonstate . edu / library / ? call = 2004 - 6 , Last Accessed : July 19th , 2004 . [ 57 ] S . Prabhakararao , C . R . Cook , Interactive fault localization for end - user programmers : a think aloud study , Technical Report 04 - 60 - 03 , Oregon State University , Corvallis , OR , January 2004 , http : / / eecs . oregonstate . edu / library / call = 2004 - 39 , Last Accessed : July 19th , 2004 . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 39 [ 58 ] C . Allwood , Error detection processes in statistical problem solving , Cognitive Science 8 ( 4 ) ( 1984 ) 413 – 437 . [ 59 ] A . Blackwell , First steps in programming : a rationale for attention investment models , in : Proceedings of the IEEE Symposium on Human - Centric Computing Languages and Environments , Arlington , VA , September 3 – 6 , 2002 , pp . 2 – 10 . [ 60 ] C . Nass , Y . Moon , P . Carney , Are people polite to computers ? Responses to computer - based interviewing systems , Journal of Applied Social Psychology 29 ( 5 ) ( 1999 ) 1093 – 1110 . [ 61 ] L . Beckwith , M . Burnett , C . Cook , Reasoning about many - to - many requirement relationships in spreadsheets , in : Proceedings of the IEEE Symposium on Human - Centric Computing Languages and Environments , Arlington , VA , September 3 – 6 , 2002 , pp . 149 – 157 . [ 62 ] J . R . Ruthruff , M . Burnett , G . Rothermel , The impact of two orthogonal factors in interactive fault localization , Technical Report 04 - 60 - 08 , Oregon State University , Corvallis , OR , June 2004 , http : / / eecs . oregonstate . edu / library / call = 2004 - 37 , Last Accessed : July 19th , 2004 . [ 63 ] L . M . Leventhal , B . E . Teasley , D . S . Rohlman , Analyses of factors related to positive test bias in software testing , International Journal of Human – Computer Studies 41 ( 2004 ) 717 – 749 . [ 64 ] C . Wohlin , P . Runeson , M . Host , B . Regnell , A . Wesslen , Experimentation in Software Engineering , Kluwer Academic Publishers , Boston , MA , 2000 . [ 65 ] P . T . Cox , F . R . Giles , T . Pietrzykowski , Prograph : a step towards liberating programming from textual conditioning , in : IEEE Workshop on Visual Languages , Rome , Italy , October 4 – 6 , 1989 , pp . 150 – 156 . [ 66 ] M . Karam , T . Smedley , A testing methodology for a dataﬂow based visual programming language , in : Proceedings of the IEEE Symposium on Human - Centric Computing Languages and Environ - ments , Stresa , Italy , September 5 – 7 , 2001 , pp . 280 – 287 . [ 67 ] D . Brown , M . Burnett , G . Rothermel , H . Fujita , F . Negoro , Generalizing WYSIWYT visual testing to screen transition languages , in : Proceedings of the IEEE Symposium on Human - Centric Computing Languages and Environments , Auckland , New Zealand , October 28 – 31 , 2003 , pp . 203 – 210 . ARTICLE IN PRESS J . R . Ruthruff et al . / Journal of Visual Languages and Computing 16 ( 2005 ) 3 – 40 40