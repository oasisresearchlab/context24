Volume xx ( 200y ) , Number z , pp . 1 – 14 Learning Human Viewpoint Preferences from Sparsely Annotated Models S . Hartwig 1 , M . Schelling 1 , C . v . Onzenoodt 1 , P . - P . Vázquez 2 , P . Hermosilla 1 , T . Ropinski 1 1 Ulm University , Germany 2 Universitat Politècnica de Catalunya , Spain Siamese CNN PointCNN CNN 1 . Learn human preference 2 . Fast inference 3 . Best view prediction 1 . 0 0 . 0 0 . 6 Figure 1 : Within this paper we present a neural view quality measure learned directly from data . ( 1 ) We collected a sparsely annotated data set of human viewpoint preferences for 3 , 220 different models which enabled us to learn our view quality measure using a Siamese architecture . This large - scale data set allowed our measure to generalize to unseen model categories . Moreover , we demonstrate two methods for fast inference of such measure : ( 2 ) we use convolutional neural networks to predict the view quality measure of a single image , and ( 3 ) we use point convolutional neural networks to predict the best view of a model from 3D data directly . Abstract View quality measures compute scores for given views and are used to determine an optimal view in viewpoint selection tasks . Unfortunately , despite the wide adoption of these measures , they are rather based on computational quantities , such as entropy , than human preferences . To instead tailor viewpoint measures towards humans , view quality measures need to be able to capture human viewpoint preferences . Therefore , we introduce a large - scale crowdsourced data set , which contains 58 k annotated viewpoints for 3220 ModelNet40 models . Based on this data , we derive a neural view quality measure abiding to human preferences . We further demonstrate that this view quality measure not only generalizes to models unseen during training , but also to unseen model categories . We are thus able to predict view qualities for single images , and directly predict human preferred viewpoints for 3D models by exploiting point - based learning technology , without requiring to generate intermediate images or sampling the view sphere . We will detail our data collection procedure , describe the data analysis and model training , and will evaluate the predictive quality of our trained viewpoint measure on unseen models and categories . To our knowledge , this is the first deep learning approach to predict a view quality measure solely based on human preferences . CCS Concepts • Computing methodologies → Neural networks ; Rasterization ; • Human - centered computing → Empirical studies in visu - alization ; 1 . Introduction Viewpoint selection is the task to automatically determine an op - timal viewpoint for a given 3D model . To support this task , sev - eral view quality measures have been proposed [ PB96 ; VBP * 09 ; SS02 ; VFSH01 ] . While these measures are based on engineered features , such as entropy and occupancy , they do not consider hu - man preferences . As these preferences are not solely based on a model’s geometry , but also its category and probably commonly used depictions , they are hard to capture with conventional view quality measures . Secord et al . [ SLF * 11 ] proposed to learn human preferences from data instead . However , they still rely on different submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 2 / Learning Human Viewpoint Preferences from Sparsely Annotated Models handcrafted quality measures , which are then used in a linear com - bination to compute a goodness score for each image . Therefore , within this paper , we propose an alternative to feature engineered view quality measures , by introducing a fully - learned view quality measure based on human viewpoint preferences . To consider human viewpoint preferences during viewpoint se - lection , we introduce a crowdsourced human viewpoint prefer - ence data set , that annotates 58 k views of ModelNet40 mod - els [ WSK * 15 ] chosen from 28 categories . To make such a large scale data collection feasible , we are naturally only able to collect annotations for a subset of all possible viewpoints . Thus , in order to leverage this sparsely annotated data , we exploit standard CNNs to reconstruct dense view spheres which encode human viewpoint preferences . The thus reconstructed human viewpoint preferences are then used to train two different models for fast inference of this measure . The first model uses CNNs to predict the view quality measure of a single image directly , while the second uses a point cloud - based architecture [ SHVR21 ] to directly predict human pre - ferred viewpoints based on a 3D model . The process of data col - lection and learning is illustrated in Figure 1 . By using a cross val - idation study , we demonstrate that our thus learned human view quality measure does not only generalize to 3D models not seen during training , but also to unseen model categories . Thus , the con - tributions made in this paper are fourfold : • We provide a large scale human viewpoint preference data set , which contains annotations for 3 , 220 3D models , available at [ project - link ] • We propose a neural view quality measure learned from user an - notated data , that mimics human preferences . • We demonstrate that the proposed view quality measure gener - alizes to model categories unseen during training . • We enable fast prediction of such view quality measures from a single image , as well as of human preferred viewpoints from 3D models directly by using neural networks . Within the remainder of this paper , we will first discuss the work related to our approach in Section 2 , before providing details on crowdsourcing human viewpoint preferences in Section 3 . We then describe our view quality measure in Section 4 , followed by Sec - tions 5 , 6 , and 7 where we describe how we learn such measure using neural networks . Finally , we address limitations of our ap - proach in Section 8 and conclude in Section 9 . 2 . Related Work Many aspects can be of importance , when it comes to the defini - tion of the best view for a 3D model . One could argue for aes - thetics , information content , recognizability , or culture - bound rea - soning . Thus , view quality measures ideally capture these aspects , to measure the quality of a selected view of an object . Early re - search focused on analyzing primitives , e . g . , number of visible tri - angles , projected area , or number of degenerated faces in orthog - onal projections [ PB96 ; BDP00 ] . Later , more sophisticated mea - sures were introduced , such as viewpoint entropy [ VFSH01 ] , view - point Kullback - Leibler distance [ NSGP * 05 ] , viewpoint mutual in - formation [ FSG09 ] , or mesh saliency [ LVJ05 ] , to name only a few . In the following , we briefly outline this field , by first discussing conventional view quality measures , before reviewing those which are learned , and those which incorporate human preferences . Viewpoint selection . Many conventional view quality measures try to capture the properties , which a view should have , in order to make it relevant . Researchers have referred to the optimal proper - ties of a view as its quality , goodness , or noteworthiness . The fea - tures that were evaluated have increased in complexity over time . Among the dozens of papers one can find , some of them deal with geometric features , such as counts of visible polygons or area from a certain view [ PB96 ; BDP00 ] . Other papers use information - based measures based on geometric features , such as entropy , mutual in - formation , or Kullback - Leibler divergence , etc . [ VFSH01 ; FSG09 ; VFSG06 ; NSGP * 05 ; TMWS12 ] . Most of the techniques use brute - force approaches , which require evaluating several hundreds of views . This is costly , since it takes several seconds to some minutes , depending on the complexity of the object , even for modern com - puters . The interested reader can refer to the survey by Bonaventura et al . [ BFS * 18 ] where they describe and analyze the most common measures applied to polygonal models . Learning - based methods . Recently , some techniques that make advantage of learning algorithms have been developed . Some techniques use such algorithms to improve over previous mea - sures [ SLF * 11 ] , or to assign scores to candidate views or pho - tographs [ KTL * 17 ; YLLY19 ; ZFY20 ] . In the context of point cloud recognition tasks , viewpoint feature histograms have been pro - posed which are used as feature descriptors [ RBTH10 ; RWL * 21 ] , which can be used for point cloud registration [ AMB17 ] . In the work of Fang et al . [ FZS * 20 ] , a canonical viewpoint is predicted by a network for point cloud classification . The ambiguity of best viewpoint selection was addressed through a dynamic label genera - tion strategy by Schelling et al . [ SHVR21 ] , to directly predict high quality viewpoints , which the authors demonstrated for four view quality measures . Learning on Image Pairs . Ranking stimuli using paired compar - isons is a long - standing technique [ Thu27 ; KS40 ; Gut46 ; Ken48 ; Mos51 ; BT52 ] . In 1927 , Thurstone [ Thu27 ] introduced the law of comparative judgment . This seminal work introduced a statisti - cal model to determine the user preference with respect to a given set of stimuli . Another commonly used statistical model for rank - ing from paired comparisons is the one proposed by Bradley and Terry [ BT52 ] . In the last decade , several works have used these ideas in combination with recent advances in machine learning . Liang et al . [ LG14 ] used an active learning approach to learn the ranking of a set of images with respect to a human - annotated at - tribute . However , the images were described by handcrafted fea - tures and , as a ranking function , a simple linear model was used . Recently , Zhang et al . [ ZIE * 18 ] use a neural network to assess the perceptual similarity between images . Their method first computed the difference between features of a pre - trained network for a given image patch as input and a corrupted version of the same patch . These differences were used to learn to rank two corrupted patches , with respect to the original image , based on human annotations . However , none of these works used a deep neural network to learn to rank the preferred point of view from human - annotated images . Evaluating human preferences . Although , the selection of view - points has been studied for some time , in areas such as object recog - submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 3 nition , there are not so many studies that have evaluated the prefer - ences of subjects with regard to a certain viewpoint of a synthetic object . Blanz et al . [ BTB99 ] evaluated the views users preferred to represent an object ( the best possible impression of the objects shown in a screen ) in two experiments . They gathered the opinions of 36 and 18 users for sets of 14 and 18 models , respectively . Later , Tarr and Kriegman evaluated the perceived similarity by presenting subjects different views for a torus and a bell [ TK01 ] . Jagadeesan et al . [ JLC * 09 ] analyzed the view preferences of 80 users using Ama - zon Mechanical Turk . In this case , they used only CAD models , and the experiment was run using a small set of prefixed views of 5 models . Another small set of users’ preferences was gathered by Dutagaci et al . [ DCG10 ] , where they analyzed 68 models with 26 subjects . Our goal is to analyze a much wider range of models with a larger number of users , in order to learn a human viewpoint pref - erence predictor , and to investigate how this generalizes beyond model categories . To the authors’ knowledge , the most extensive user study to date is the one by Secord et al . [ SLF * 11 ] . They used a high number of users ( 524 ) that choose among pairs of views of 16 different , rather complex models . Their goal was to learn a set of weights , that can be applied to previously developed measures in order to compute the view goodness function of the input images and use the Bradley - Terry model [ BT52 ] to simulate human pref - erences on viewpoint selection . On the contrary , in this work , we aim to learn human view preferences without relying on predefined metrics or any statistical model . 3 . Sparsely Annotated Data Set Collecting human view preferences poses several challenges . Sim - ply asking for the subjective ‘best view‘ of a model might be suffi - cient to indicate appealing view directions , but it does not provide insights on how a user would compare two views to each other . Furthermore , selecting the ‘best view‘ is a challenging task for an average user as it involves rotating a 3D model , a task with which some participants might not be familiar with . Consequently , these annotations should be handled with care , and thorough quality as - sessments are required . In order to capture the relations between different viewpoints , we are interested instead in how humans per - ceive a view direction in relation to another one . Asking a user to quantify the quality of a view direction with respect to another is subject to high levels of ambiguity . Differ - ent users might perceive values differently , i . e . , the same numerical value might have different meanings for different users . With these considerations in mind and inspired by Secord et al . [ SLF * 11 ] , we design our view quality measure from data collected in a discrete forced choice experiment . Thus , we only ask users to decide be - tween ‘better‘ or ‘worse‘ , effectively only assessing the order be - tween the presented views , rather than their absolute quality . Using such setup , we collect a large human - annotated data set that contains sparse annotations for 3 , 220 3D models . We favor a large number of annotated models instead of highly dense annota - tions for each of them , since neural networks have been shown to generalize well to unseen data , and can thus fill in the missing an - notations . In the rest of this section , we describe how we generated this data set . Please select the best and the worst view by drag and dropping an image into the corresponding box . Study : Human Viewpoint Preferences Figure 2 : In our online study , the participant has to select the best and worst view amongst three given views . The web interface pro - vides a drag and drop method to make the selection . A selection can be changed by drag and dropping another view on top of an al - ready selected choice . Each participant has to annotate 50 triplets . 3 . 1 . Online Study In order to collect annotations for our viewpoint data set , we im - plemented a web application , enabling us to crowdsource the view - point preferences via Amazon Mechanical Turk . As mentioned be - fore , to keep the workload low , maintain participants motivation , and therefore achieve higher quality , we formulate our study as a discrete forced choice experiment . This design choice is also in line with a similar study by Secord et al . [ SLF * 11 ] , which conducted a two - alternative forced choice experiment . Presenting triplets to participants , instead of tuples , increases the number of annotated tuples per interaction of the participant from 1 to 3 . Also , expos - ing participants to only three stimuli does not compromise the ob - server’s channel capacity [ Mil56 ; RA11 ] enabling stable measure - ments . We formulate the annotation task as follows : Given a triplet of views , the participants have to select the views they consider to be the best and worst , respectively , amongst the presented three views . In order to make a selection for a view , the participants have to drag and drop the view into a corresponding box labeled ‘best‘ and ‘worst‘ . The web interface can be seen in Figure 2 . As view preference is dependent on the context in which the im - ages are presented , the instructions given to the participants can have an influence on their choices . To ensure that different partic - ipants have the same understanding of the task , we formulate the instructions stated below , following the examples given by previ - ous user studies [ BTB99 ; SLF * 11 ] . You will be presented three images . Your task is to se - lect the views which are in your opinion the best and the worst views of the presented set . There is no right or wrong . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 4 / Learning Human Viewpoint Preferences from Sparsely Annotated Models Figure 3 : This attention check is used to detect if a participant did not understand the task or is clicking through the study . The first view is what we consider the best view of a baseball bat . In some cases , multiple views might be equally good or bad . In this case , try to enforce a decision . In this experiment , “best view” corresponds to the most familiar view of an object . Consider that you will show only one view of the objects to another person , and the person should be able to recognize the object as quickly as possible by looking at that view . Each participant had to label 50 triplets , which took 8 minutes on average . We also included attention checks to detect partici - pants that did not understand the task or intentionally select bad views , by adding view pair examples showing a definite best and worst view enabling us to filter out bad responses , see Figure 3 for such a check . Data from participants who did not pass this test was not considered in our data set . Additionally , we ensured that each triplet of views is labeled twice by two different participants , in or - der to get robust annotations . In total , 950 participants finished the described annotation task . 3 . 2 . 3D Models For our data set , we used ModelNet40 [ WSK * 15 ] , which contains a large set of models for man - made objects spanning several cat - egories . In total , ModelNet40 features 40 model categories , from which we remove 12 categories , which leaves us with 28 cate - gories . We kept categories with more than 115 models ( 93 train - ing , 5 validation , 17 testing ) , and removed categories ( bowl , cup , curtain , person , stool , tent ) with an insufficient number of models . We further excluded the categories ( xbox , cone , glass box , mantel , range hood , stairs ) which exhibit similar shapes in all models . For instance , the shapes in the categories xbox and cone are virtually identical . For each category , we use 115 models summing up to 3 , 220 models . Lastly , we generate 18 views ( 6 triplets ) for each of these 3 , 220 models , which yields 57 , 960 images in total . 3 . 3 . View Generation To be able to collect viewpoint annotations for the given models , we sample random positions on a unit sphere around each model . Since we are using triplets of viewpoints for the annotation task , and to maximize the difference of information content between views , we sample three camera positions at once using corners of an equi - lateral triangle ( see Figure 4 ) . We obtain these camera positions by first sampling a random point p ∈ S 2 on a unit sphere . Sec - ond , we randomize the angle θ between p and the triangle points t i , i ∈ [ 0 , 1 , 2 ] . Third , we randomly rotate t i around p by φ . Finally , φ θ p t0 t1 t2 Figure 4 : Our employed view sampling strategy , where red dots represent sampled views from a unit sphere around a model . We first sample a random sphere point p displayed in green . Then we generate an equilateral triangle centered around p and use triangle points as viewpoints . B e s t A i r p l ane B a t h t ub B ed B en c h B oo ks he l f B o tt l e C a r C ha i r D e sk D oo r D r e ss e r F l o w e r po t G u i t a r K e y boa r dLa m p Lap t op M on i t o r N i gh t s t and P i ano P l an t R ad i o S i n k S o f a T ab l e T en t T o il e t T vs t and V a s e A i r p l ane B a t h t ub B ed B en c h B oo ks he l f B o tt l e C a r C ha i r D e sk D oo r D r e ss e r F l o w e r po t G u i t a r K e y boa r dLa m p Lap t op M on i t o r N i gh t s t and P i ano P l an t R ad i o S i n k S o f a T ab l e T en t T o il e t T vs t and V a s e 1 . 0 0 . 0 W o r s t 1 . 0 0 . 0 Figure 5 : Agreement of human labels for our selected categories of ModelNet40 for best viewpoints ( top ) and worst viewpoints ( bottom ) . As can be seen , participants agreed on both best view and worst view in most of the cases for all categories . we select each triangle point t i as eye vector for our cameras . For each camera , we set the target vector to the center of the model , and the up vector to the positive y - axis . We repeat this process six times for each model , to obtain 6 triplets , resulting in 18 views per model . Note , that our sparse view sampling generates thirteen times fewer views per model compared to Secord et al . [ SLF * 11 ] . For render - ing , we use perspective projection . We place a light located at the camera’s position facing the object , and we used ambient occlusion in order to increase visible details . 3 . 4 . Validation To validate the crowdsourced annotations , we examined the best and worst view agreement between participants . To do so , we count how often a view was labeled as best or worst , and normalize by di - viding by the number of participants per view . In Figure 5 we dis - play the thus obtained averaged agreement rate per model category submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 5 for best view selection ( top ) and worst view selection ( bottom ) . As can be seen , participants show high agreement on both best view and worst view in most of the cases for all categories . While cate - gories with model shapes that are rotational invariant , e . g . , bottle , flower pot , vase , have a best view agreement rate below 80 % , cat - egories of models with clear front and back sides show agreement rates for best views close to 90 % , e . g . , bookshelf , dresser , piano , sofa . Looking at the bottom row in Figure 5 , the agreement rate for all categories is above 80 % , except for airplane , indicating a slight dispute regarding airplanes seen from the bottom . Moreover , we evaluated if the collected data were consistent among models of the same category . To evaluate this , we selected all view directions that were selected as the best in a triplet for all models in a given category , and use a kernel density estimation ( KDE ) to approximate this distribution . In order to compute KDE in a spherical domain , we replace the traditional Gaussian function used in KDE with the Von Mises - Fisher distribution [ Fis53 ] . Fig . 6 illustrates the resulting density maps on the sphere for each cate - gory . We can see that users were consistent with the annotations among different models of the same category , since each category has distributions with localized areas with high density . For exam - ple , users prefer to view objects from the category bookshelf from the front while cars are preferred from a left or right viewpoint . 4 . Modeling Human Viewpoint Preferences The collected data set is composed of pairs of images annotated by humans , which indicates their preference of one image over the other . With these binary classifications for all views of a given model , it is possible to reconstruct a quantitative measure of the view preference distribution and , therefore , possible to capture the global ranking of all views . In the following , we will give a brief mathematical description of the reconstruction from binary classi - fications . Let f : D → R be a function , defined on a finite set D . In the context of this work D ⊂ S 2 are the sampled viewpoints on the view sphere , and f is the ( unknown ) distribution of human view preference for a given model . For each point v ∈ D , we define the count of v as count ( v ) : = | { x ∈ D : f ( x ) < f ( v ) } | , ( 1 ) where f ( x ) < f ( v ) indicates that the user prefers view v over x . We define the reconstruction f ∗ of f as : f ∗ : D → R f ∗ ( v ) : = count ( v ) | D \ { v } | . ( 2 ) This reconstruction can be understood as a normalized counter of how many times a view direction was selected over other view di - rections in the set . Thus , this reconstruction serves as a quantitative view quality measure , that can be used to compare different views of a given 3D model : Proposition 1 . The reconstruction f ∗ preserves the order induced by f on the points in D , i . e . , f ( v 1 ) < f ( v 2 ) ⇔ f ∗ ( v 1 ) < f ∗ ( v 2 ) , ∀ v 1 , v 2 ∈ D . ( 3 ) Proof to this proposition and further insights are provided in Ap - pendix A . 5 . View Quality Measure Learning Computing our measure will require evaluating Eq . ( 1 ) for all ele - ments in the set D . However , asking a user to annotate all possible pairs of view directions for a given model is not practical . In this paper , we propose to learn Eq . ( 1 ) from sparse data instead . 5 . 1 . Architecture Our model follows a Siamese architecture similar to the one proposed in the one - shot classification framework of Koch et al . [ KZS15 ] . We consider two images as input , which are gener - ated from the two view directions we aim to compare , v 1 and v 2 . These images are then processed by two image encoders which have shared parameters , resulting in two latent vectors describing a compressed representation of the two input images . The image encoder is a CNN composed of four feature transformation blocks with increasing feature size : 16 , 32 , 64 , 128 and a down sampling factor of 2 . Each block is a stack of three layers : convolution , batch normalization and ReLU activation . After the last layer , we ap - pend a global average pooling layer , which outputs a latent vector for each image . For our experiments , we use an image resolution of 256x256 , and a latent vector with size 128 ( see Table 6 in the appendix for a comparison of different sizes ) . The two latent rep - resentations are then concatenated and processed by a multi - layer perceptron ( MLP ) with three hidden layers with 256 , 128 , 32 fea - tures each . The last layer applies a sigmoid activation function to output the probability of f ( v 1 ) < f ( v 2 ) . Based on these probabil - ities , we can then estimate f ∗ . Figure 7 provides an illustration of the used architecture . Note that if our model is trained with random view directions , we can estimate f ∗ for a 3D shape by sampling directions in the sphere , D , at any resolution . Previous work has suggested statistical models to predict the probability of f ( v 1 ) < f ( v 2 ) from the goodness of each in - put [ BT52 ] . This model was used by Secord et al . [ SLF * 11 ] to learn the goodness function with a linear model . The Bradley - Terry model [ BT52 ] models the variance in the annotations as variance in the goodness score , and solve it with Maximum Likelihood Esti - mation ( MLE ) . We favor instead a model that directly predicts this probability with a deep neural network and models the variance di - rectly from data using Binary Cross Entropy , an optimization com - parable to MLE . Further , neural networks are able to generalize by exploiting information present in the input images , which proba - bilistic models do not account for . 5 . 2 . Training We train our viewpoint selector using the Adam optimizer [ KB14 ] for 200 epochs and a batch size of 16 , using a learning rate of 0 . 001 , reducing it by a factor of 0 . 1 if the validation loss did not submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 6 / Learning Human Viewpoint Preferences from Sparsely Annotated Models Airplane + Z + Y + X Bathtub Bed Bench Bookshelf Bottle Car Chair Desk Door Dresser Flower pot Guitar Keyboard Lamp Laptop Monitor Night stand Piano Plant Radio Sink Sofa Table Tent Toilet Tv stand Vase Figure 6 : Collected per - category view preference . For each category , we select all view directions which were labeled best in a triplet and use a kernel density estimation on the sphere to approximate this distribution . Yellow regions indicate view directions highly favored by humans . The resulting distributions indicate that , on the data collected , there is consistency among objects from the same category . Latent Vector concat . MLP shared weights < CNN CNN Figure 7 : The architecture of our viewpoint selector consists of a feature encoder and decoder for classification . The input to the net - work are two images which are fed successively into the encoder , resulting in two latent vectors of size 128 . The decoder concate - nates both latent vectors before further processing and outputting a binary prediction . improve for 5 epochs . We use Binary Cross - Entropy as our loss function . We regularize our model by using a dropout probability of 0 . 5 [ HSK * 12 ] in the last MLP , and further add Gaussian noise to the ground truth labels with a standard deviation of 0 . 2 , preventing label from flipping and clamping the label to be in range [ 0 , 1 ] . 5 . 3 . Evaluation In this section , we describe the experiments conducted to evaluate our Siamese network . 5 . 3 . 1 . Learning the View Quality Measure In this experiment , we evaluated the performance of our network on predicting the human view preference for a given pair of images . Data set . We use the data described in Section 3 to train the model that mimics human view preferences . First , we select only those triplets in which two users agreed on their annotation . Then , we generate three pairs of annotated images from each triplet for train - ing . Lastly , we divide the resulting images in three different sets : training , validation , and testing . We make sure that in each split we have models from all categories , but images from one model are present in only one of the three sets . The training set is com - posed of 60 , 462 image pairs , the validation set is composed of 3 , 576 pairs , and the test set contains 9 , 978 image pairs . We call this data set FILTERED . To further augment the data during train - ing , we apply random rotations in the range of [ − 90 , 90 ] degrees . The network learns to be invariant to rotations around the camera’s eye vector , which is only possible since we fixed the camera’s up vector . In a scenario with arbitrary camera up vector , this data aug - mentation strategy would be problematic to rank viewpoints . How - ever , disabling random rotations results in worse performance , see Section 5 . 3 . 1 . Since our model is not equivariant to the order of the input images in each pair , we also randomly invert the order of the images in a pair and its corresponding label during training . Results . To measure the performance of the model , we compute three measures : Binary accuracy ( Acc . ) , area under the receiver operating characteristic curve ( AUC ) , and area under the precision - recall curve ( AUPR ) . We evaluated the metrics on each pair of im - ages in the test set in the two possible permutations . In Fig . 8 ( red ) , we report the resulting values for all metrics and categories . Re - sults show that our model can mimic human preference with high accuracy . For categories such as bottle , dresser , or table , the model is able to achieve high values in all metrics . On the other hand , other categories are more difficult to predict , such as airplane or monitor . Table 1 presents the averaged performance over all predic - tions , where we compare our method against existing visual met - rics and the data - driven approach of Secord et al . [ SLF * 11 ] , which combines existing view quality measures in order to approximate human preferences . We use the original weights presented in the paper , denoted as S ECORD , in Table 1 , and the optimized weights in our training data set , denoted as S ECORD * . Fig . 9 illustrates the reconstructed view quality measure for one model of each category from our test set . Models are rendered from the viewpoint with the highest value based on our measure , which is also indicated by a submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 7 Accuracy AUC AUPR O URS U NFILTERED 79 . 9 % 0 . 796 0 . 847 S ECORD * [ SLF * 11 ] 76 . 7 % 0 . 767 0 . 825 S ECORD [ SLF * 11 ] 73 . 4 % 0 . 734 0 . 800 A BOVE [ GRMS01 ] 69 . 7 % 0 . 697 0 . 773 V IEWPOINT E NTROPY [ VFSH01 ] 68 . 4 % 0 . 684 0 . 763 S URFACE V ISIBILITY [ PB96 ] 63 . 1 % 0 . 631 0 . 723 K ULLBACK - L EIBLER [ NSGP * 05 ] 62 . 1 % 0 . 621 0 . 716 S ILHOUETTE L ENGTH [ HS97 ] 59 . 2 % 0 . 592 0 . 694 P ROJECTED A REA [ PB96 ] 52 . 1 % 0 . 521 0 . 641 M AX D EPTH [ SS02 ] 50 . 7 % 0 . 507 0 . 630 M UTUAL I NFORMATION [ FSG09 ] 43 . 8 % 0 . 438 0 . 578 * using optimized weights on UNFILTERED . Table 1 : Performance results of our best network , Secord et al . [ SLF * 11 ] and several common view quality measures which are not based on human preferences . Performance is averaged over all predictions . red dot in the view quality distribution . In the supplementary ma - terial , we also provide the predicted view quality distribution for all the models in our data set . Looking at symmetric shapes like car or table , one can see equal predictions for both , left and right sides . Rotation symmetric shapes like lamp , bottle and vase show arbitrary good viewpoints around the equator . 5 . 3 . 2 . Ambiguous Annotations As human annotations are subject to ambiguity , we also investi - gated how robust our model is to ambiguous image pairs . Data set . In our previous experiment , we filtered the data for those triplets in which two users agreed with their annotations . In this experiment instead , we generate a training / validation set which is composed of all collected data without filtering . Note that even if the users did not agree in the annotation of the complete triplet , they can still agree on some resulting pairs generated from the triplet . The resulting data set , U NFILTERED , is composed of 92 , 364 pairs of images for training , 5 , 700 image pairs for validation , and , as before , 9 , 978 image pairs for testing . Results . We train the same model as in the previous experiment on this new data set . Fig . 8 ( green ) illustrates the results for all metrics and each category . We can see that there is only a small change in performance for most of the categories . Moreover , Table 2 also presents the averaged performance over all predictions , where the model achieves an accuracy of 79 . 9 % , an AUC of 0 . 796 , and a AUPR of 0 . 847 . When we disable random rotations during training , the accuracy drops to 78 . 9 % . These results indicate that our model is robust to ambiguities on the human annotations and also benefits from the additional training data . 5 . 3 . 3 . Generalization to Unseen Categories Fig . 6 indicates that there is consistency on the view preference for models of the same category . Therefore , measuring the ability of our model to generalize to unseen categories is of key importance to measure the validity of our measure . Accuracy AUC AUPR O URS U NFILTERED 79 . 9 % 0 . 796 0 . 847 O URS F ILTERED 79 . 7 % 0 . 800 0 . 850 O URS 6 - FOLD 76 . 2 % 0 . 765 0 . 824 Table 2 : Performance comparison of our Siamese model for the three data set . Subset 1 Subset 2 Subset 3 Subset 4 Subset 5 Subset 6 chair bathtub airplane bottle bed dresser monitor bench bookshelf keyboard car guitar sink dresser door laptop desk plant tent night stand flower pot sofa lamp sink tv stand radio piano vase toilet table Table 3 : For our 6 - fold cross validation experiment , we randomly split 28 categories into subsets of 5 categories . Note that for Subset 6 , we randomly chose two additional categories to add up to five categories . For each subset , we train a network with the absence of images of the respective categories . During test time for each classifier , we measure performance for each individual category which has been left out during training . Data set . For this experiment , we use a 6 - fold cross validation on the 28 categories to evaluate the model . Table 3 illustrates the cate - gory types that are left out during training in each fold . We refer to this data set as 6 - F OLD . Results . We train the same model as in the previous experiments on the new data set . Fig . 8 ( blue ) presents the resulting metrics for each of the categories . For most of the categories , training with this new data set translates only into a slight drop in performance . For other categories , such as bottle or vase , the drop in performance is large but still in a good range . When we look at the averaged performance over all predictions in Table 2 , we can also see a small drop in performance for all metrics . Further , we compute view quality distributions on the sphere for two common models in computer graphics , the Stanford dragon and the heptoroid , using Eq . ( 2 ) and our Siamese model . In Fig - ure 10 , we compare our learned view quality measure against the best model of Secord et al . [ SLF * 11 ] , which approximates hu - man preferences specifically collected for these models . Note that these models are out of the domain for our training data , as they are animals or abstract models , in contrast to the man - made ob - jects of ModelNet40 used for training . The method of Secord et al . [ SLF * 11 ] selects the right side of the models as best view , whereas our method favors the left side . Despite these differences , our measure generates similar distribution as the ones published by Secord et al . [ SLF * 11 ] . 6 . View Quality Measure Inference Evaluating our quality measure can be computationally expensive since it requires evaluating Eq . ( 1 ) for all views in a finite set , gener - ating a quadratic cost with respect to the number of sampled views . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 8 / Learning Human Viewpoint Preferences from Sparsely Annotated Models 0 , 5 1 , 0 0 , 5 1 , 0 0 , 5 1 , 0 6 - FOLD UNFILTERED FILTERED V a s e T v s t a n d T o i l e t T e n t T a b l e S o f a S i n k R a d i o P l a n t P i a n o N i g h t s t a n d M o n i t o r L a p t o p L a m p K e y b o a r d G u i t a r F l o w e r p o t D r e ss e r D oo r D e s k C h a i r C a r B o tt l e B oo k s h e l f B e n c h B e d B a t h t u b A i r p l a n e A U P R A UC A cc . Figure 8 : Performance results of our Siamese model to predict the binary classification for the image pairs in our test set . We evaluate three different training methodologies : F ILTERED , where we train with image pairs in which users agreed in the annotation ; U NFILTERED , where we train with all training data from our data set ; and 6 - FOLD , where the model did not see the respective categories during training . Airplane Bathtub Bed Bench Bookshelf Bottle Car Chair Desk Door Dresser Flower pot Guitar Keyboard Lamp Laptop Monitor Night stand Piano Plant Radio Sink Sofa Table Tent Toilet Tv stand Vase Figure 9 : Visualization of the learned human view quality measure for one model per category . In the upper row , we show the best view for each model sampled from our view quality measure . In the row below we show the corresponding learned viewpoint distribution which was reconstructed from our viewpoint selector . The red dot indicates the best viewpoint . Secord et al . 2011 Ours Figure 10 : Generalization of our viewpoint selection network , which has been trained on ModelNet40 . On the left , we show the selected best view and corresponding view quality from Secord et al . [ SLF * 11 ] , respectively . On the right , we demonstrate our learned view quality measure . Red dots represent the best viewpoint which is displayed above . This demonstrates the generalization of our viewpoint selector to novel categories : dragon and heptoroid . In order to reduce this cost , we propose to train a convolutional neural network to predict the value of the reconstructed f ∗ , Eq . ( 2 ) , directly from a single image . With this setup , our measure can be approximated in milliseconds for a single image without comparing it to any other view direction . 6 . 1 . Architecture For our CNN encoder , we used the same architecture as in our Siamese model , followed by a two - layer MLP with 64 and 32 hid - den neurons each . The final prediction is processed by a sigmoid function to transform the prediction into the range [ 0 , 1 ] . 6 . 2 . Training We train our model using the Adam optimizer [ KB14 ] for 15 epochs , and a batch size of 32 , using a learning rate of 0 . 005 , reduc - ing it by factor 0 . 1 after 10 and 15 epochs . We regularize our model submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 9 1 . 0 1 . 0 Predictions V i e w po i n t Q ua li t y M ea s u r e 0 . 0 0 . 0 Figure 11 : Viewpoint quality values vs predictions of our regres - sion model . We can see that most of our predictions are close to the viewpoint quality values , close to the diagonal line , indicating a perfect regression model . by using weight decay with a factor of 0 . 001 . As a loss function , we use Binary Cross - Entropy loss for this regression problem , since it avoids gradients becoming zero due to the last sigmoid layer . 6 . 3 . Evaluation In this section , we measure the accuracy of our regression model . Data set . For this data set , we use the same models from Model - Net40 as used in our user study . We reconstruct the view quality measure for each model as follows . For a given model , we sam - ple 1 , 000 view directions on a Fibonacci sphere around the model , which yields uniformly distributed viewpoints . Next , we generate image pairs for all combinations C = (cid:0) 10002 (cid:1) to select two images from the sampled views . Finally , we reconstruct the view quality measure using Eq . ( 2 ) and our Siamese model . We create three different splits of the data : training , validation , and test , containing 2 . 5M , 168K , and 476K images each . Note that images from the same model are all contained in the same set . To further augment the training data , we use random rotations of 90 degrees and add Gaussian noise to the images with a standard de - viation of 0 . 002 . Results . We measure the performance of the model with Mean Squared Error ( MSE ) and Coefficient of Determination ( R 2 ) , for which our regression model achieved a performance of 0 . 02 and 0 . 71 , respectively . We further analyzed the model by plotting the predicted values vs . viewpoint quality values in Fig . 11 . We see that most of the points are close to a perfect regression model , indicated by the diagonal line . 7 . Best View Prediction In many applications , the user is only interested in obtaining the best view from which to inspect a 3D model . This setup will re - quire to evaluate our view quality measure for all views in a fi - nite set , which can be computationally demanding . Recent research has proposed to learn the best view of a 3D model directly from 3D data [ SHVR21 ] according to four established handcrafted view quality measures . This method uses point convolutional neural net - works [ HRV * 18 ] to analyze the 3D structure of a model , and to pre - dict the best view for the different view quality measures . In this pa - per , we employ the same architecture as Schelling et al . [ SHVR21 ] to predict the best view direction for a 3D model based on our pro - posed view quality measure instead of the handcrafted quality mea - sures used in the paper . This enables us to perform a prediction within milliseconds . 7 . 1 . Architecture We use the same architecture as the one proposed by Schelling et al . [ SHVR21 ] . The shape encoder uses four point convolution lay - ers [ HRV * 18 ] with increasing receptive fields [ 0 . 05 , 0 . 2 , 0 . 3 , √ 3 ] and number of features [ 128 , 256 , 1024 , 2048 ] . The resulting latent vector is then processed by an MLP with two hidden layers and 1 , 024 and 256 features , in order to predict the best view . 7 . 2 . Training We train our model to predict the best view for models of all 28 categories at the same time . For the loss , we use the one proposed by Schelling et al . [ SHVR21 ] , where the ground truth label used in each step is computed based on the prediction of the network . This loss uses two methods to generate the ground truth label , multi la - bels and Gaussian labels , that are applied at different stages of the training . This procedure avoids inconsistencies during training , due to symmetries in the view quality distribution for similar models . We use an Adam optimizer [ KB14 ] with batch size of 8 and learn - ing rate of 0 . 001 , which is multiplied by 0 . 75 every 200 epoch . We train for a total of 3 , 000 epochs and switch from multi labels to Gaussian labels after 1 , 500 . 7 . 3 . Evaluation In this section , we evaluate if our measure can be learned by the best view predictor of Schelling et al . [ SHVR21 ] directly from 3D models . Data set . To train this model , we used the 3D models and view quality measures described in Section 6 . 3 . Moreover , we use 4 , 096 points sampled on the surface of the objects as also done in the PointNet paper [ QYSG17 ] , to represent the 3D shape of each ob - ject . We divide the data in three data splits : training , validation , and testing , where each set contains 2 , 576 , 322 , and 322 3D models respectively . Results . We measure performance with the normalized view qual - ity measure , which ranges from 0 to 1 for each model , as it is normalized based on the minimum and maximum value among all views for a single model . The trained model is able to predict viewpoints with an average normalized viewpoint quality of 0 . 88 . We report per - category results in Fig . 12 . Looking at individual category accuracy , only four categories ( flower pot , lamp , plant , vase ) have a value lower than 0 . 8 , indicating that the method can learn the best view direction in our view quality measure directly from the 3D shape of a model . Note , that for rotational symmetric models , the best views are in a narrow region close to the equator . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 10 / Learning Human Viewpoint Preferences from Sparsely Annotated Models 0 . 5 0 . 8 1 . 0 A i r p l ane B a t h t ub B ed B en c h B oo ks he l f B o tt l e C a r C ha i r D e sk D oo r D r e ss e r F l o w e r po t G u i t a r K e y boa r dLa m p Lap t op M on i t o r N i gh t s t and P i ano P l an t R ad i o S i n k S o f a T ab l e T en t T o il e t T vs t and V a s e Figure 12 : Normalized view quality of the predicted viewpoints per category of the viewpoint prediction network of Schelling et al . [ SHVR21 ] trained on our learned human view quality measure . That means that a small variation of the latitude in the view predic - tion translates in a bigger error in our metric than for other shapes , where the best view region has a more uniform shape . Finally , we provide a qualitative comparison between the best view predicted by Schelling et al . [ SHVR21 ] trained on our hu - man viewpoint measure and , four handcrafted measures used by the original paper . In Figure 13 we display viewpoints which were selected by Schelling et al . [ SHVR21 ] conditioned to our human viewpoint measure in the first column , Visibility Ra - tio [ PB96 ] , Viewpoint Entropy [ VFSH01 ] , Viewpoint Kullback - Leibler [ NSGP * 05 ] and Viewpoint Mutual Information [ FSG09 ] in the other columns . We can see that the best view predictor trained on our human viewpoint measure provides robust viewpoint selec - tions for different types of models , while handcrafted viewpoint measures select viewpoints with high levels of occlusions , making it difficult to identify the individual objects . 8 . Limitations Our method is not free of limitations . The generalization ability of CNNs enabled us to learn the human view preference from data directly . However , this technology is susceptible to variations of the input data . This makes our models dependent on the render - ing algorithm used during training . However , generalization to out - of - distribution rendering algorithms could be introduced with style transfer techniques . Moreover , our method does not consider the context of such vi - sualizations . Different tasks will select different views as good , e . g . , a 3D modeling software will require a good overview of the model while a volume rendered image in the context of medical visualiza - tion will favor views where relevant information is shown . There - fore , context - aware view quality measures should be further inves - tigated by collecting adequate data sets . Lastly , in our experiments , we consider the up vector of the camera fixed . Although , recent work have suggested learning the up vector of a model [ KTL * 17 ] , further experiments are needed to investigate the effect of arbitrary camera rotations on the human preference . 9 . Conclusion Within this paper , we have shown how viewpoint preference of hu - mans can be simulated with modern deep learning techniques . We Ours VR VE VKL VMI Figure 13 : Visualization of the best viewpoint selected by the net - work from Schelling et al . [ SHVR21 ] for different view quality mea - sures . For columns from left to right , we show the best viewpoint for our human - based measure , Visibility Ratio , Viewpoint Entropy , Viewpoint Kullback - Leibler and Viewpoint Mutual Information . collected a large - scale data set using Amazon Mechanical Turk to crowdsource viewpoint annotations . This data set enabled us to de - sign a neural view quality measure based on human preferences that is able to simulate human preference with high accuracy . Moreover , we evaluated our learned measure with respect to ambiguous anno - tations in the human - annotated data , which showed that our tech - nique benefits from additional training data while being robust to the ambiguities at the same time . Lastly , we evaluate the generaliza - tion ability of our model to unseen model categories , which resulted only in a small drop in performance . Furthermore , we provide two methods for fast inference of our measure . The first method es - timates the view quality measure of an image directly by using convolutional neural networks . The second method uses point con - volutional neural networks , to predict the view direction with the best view quality value from a 3D model directly . In the future , we would like to investigate the correlation of human quality measures , as the one proposed in this paper , with the performance of neural networks on different downstream tasks . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 11 References [ AMB17 ] A VIDAR , D AVID , M ALAH , D AVID , and B ARZOHAR , M EIR . “Local - to - global point cloud registration using a dictionary of viewpoint descriptors” . 2017 IEEE International Conference on Computer Vision ( ICCV ) . IEEE . 2017 , 891 – 899 2 . [ BDP00 ] B ARRAL , P . , D ORME , G . , and P LEMENOS , D . “Scene Under - standing techniques using a virtual camera” . Proc . Eurographics’00 , short presentations . Ed . by de S OUSA , A . and T ORRES , J . C . 2000 2 . [ BFS * 18 ] B ONAVENTURA , X AVIER , F EIXAS , M IQUEL , S BERT , M ATEU , et al . “A survey of viewpoint selection methods for polygonal models” . Entropy 20 . 5 ( 2018 ) , 370 2 . [ BT52 ] B RADLEY , R ALPH A LLAN and T ERRY , M ILTON E . “Rank anal - ysis of incomplete block designs : I . The method of paired comparisons” . Biometrika 39 . 3 / 4 ( 1952 ) , 324 – 345 2 , 3 , 5 , 13 , 14 . [ BTB99 ] B LANZ , V OLKER , T ARR , M ICHAEL J , and B ÜLTHOFF , H EIN - RICH H . “What object attributes determine canonical views ? ” : Percep - tion 28 . 5 ( 1999 ) , 575 – 599 3 . [ DCG10 ] D UTAGACI , H ELIN , C HEUNG , C HUN P AN , and G ODIL , A FZAL . “A benchmark for best view selection of 3D objects” . Proceedings of the ACM workshop on 3D object retrieval . 2010 , 45 – 50 3 . [ Fis53 ] F ISHER , R ONALD A YLMER . “Dispersion on a sphere” . Proceed - ings of the Royal Society of London . Series A . Mathematical and Physi - cal Sciences 217 . 1130 ( 1953 ) , 295 – 305 5 . [ FSG09 ] F EIXAS , M IQUEL , S BERT , M ATEU , and G ONZÁLEZ , F RAN - CISCO . “A unified information - theoretic framework for viewpoint selec - tion and mesh saliency” . ACM Transactions on Applied Perception ( TAP ) 6 . 1 ( 2009 ) , 1 – 23 2 , 7 , 10 . [ FZS * 20 ] F ANG , J IN , Z HOU , D INGFU , S ONG , X IBIN , et al . “RotPredic - tor : Unsupervised Canonical Viewpoint Learning for Point Cloud Clas - sification” . 2020 International Conference on 3D Vision ( 3DV ) . IEEE . 2020 , 987 – 996 2 . [ GRMS01 ] G OOCH , B RUCE , R EINHARD , E RIK , M OULDING , C HRIS , and S HIRLEY , P ETER . “Artistic composition for image creation” . Euro - graphics Workshop on Rendering Techniques . Springer . 2001 , 83 – 88 7 . [ Gut46 ] G UTTMAN , L OUIS . “An approach for quantifying paired com - parisons and rank order” . The Annals of Mathematical Statistics 17 . 2 ( 1946 ) , 144 – 163 2 . [ HRV * 18 ] H ERMOSILLA , P . , R ITSCHEL , T . , V AZQUEZ , P - P , et al . “Monte Carlo Convolution for Learning on Non - Uniformly Sampled Point Clouds” . ACM Transactions on Graphics ( Proceedings of SIG - GRAPH Asia 2018 ) 37 . 6 ( 2018 ) 9 . [ HS97 ] H OFFMAN , D ONALD D and S INGH , M ANISH . “Salience of visual parts” . Cognition 63 . 1 ( 1997 ) , 29 – 78 7 . [ HSK * 12 ] H INTON , G EOFFREY E , S RIVASTAVA , N ITISH , K RIZHEVSKY , A LEX , et al . “Improving neural networks by preventing co - adaptation of feature detectors” . arXiv preprint arXiv : 1207 . 0580 ( 2012 ) 6 . [ JLC * 09 ] J AGADEESAN , A P RASANNA , L YNN , A NDREW , C ORNEY , J ONATHAN R , et al . “Geometric reasoning via internet crowdsourcing” . 2009 SIAM / ACM Joint Conference on Geometric and Physical Model - ing . 2009 , 313 – 318 3 . [ KB14 ] K INGMA , D IEDERIK P and B A , J IMMY . “Adam : A method for stochastic optimization” . arXiv preprint arXiv : 1412 . 6980 ( 2014 ) 5 , 8 , 9 . [ Ken48 ] K ENDALL , M AURICE G EORGE . “Rank correlation methods . ” ( 1948 ) 2 . [ KS40 ] K ENDALL , M AURICE G and S MITH , B B ABINGTON . “On the method of paired comparisons” . Biometrika 31 . 3 / 4 ( 1940 ) , 324 – 345 2 . [ KTL * 17 ] K IM , S EONG - HEUM , T AI , Y U - W ING , L EE , J OON - Y OUNG , et al . “Category - Specific Salient View Selection via Deep Convolutional Neural Networks” . Computer Graphics Forum . Vol . 36 . 8 . Wiley Online Library . 2017 , 313 – 328 2 , 10 . [ KZS15 ] K OCH , G REGORY , Z EMEL , R ICHARD , and S ALAKHUTDINOV , R USLAN . “Siamese neural networks for one - shot image recognition” . ICML Deep Learning Workshop . Vol . 2 . 2015 5 . [ LG14 ] L IANG , L UCY and G RAUMAN , K RISTEN . “Beyond comparing image pairs : Setwise active learning for relative attributes” . Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2014 , 208 – 215 2 . [ LVJ05 ] L EE , C HANG H A , V ARSHNEY , A MITABH , and J ACOBS , D AVID W . “Mesh saliency” . ACM Trans . Graph . 24 . 3 ( 2005 ) , 659 – 666 . ISSN : 0730 - 0301 . DOI : http : / / doi . acm . org / 10 . 1145 / 1073204 . 1073244 2 . [ Mil56 ] M ILLER , G EORGE A . “The magical number seven , plus or minus two : Some limits on our capacity for processing information . ” Psycho - logical review 63 . 2 ( 1956 ) , 81 3 . [ Mos51 ] M OSTELLER , F REDERICK . “Remarks on the method of paired comparisons : II . The effect of an aberrant standard deviation when equal standard deviations and equal correlations are assumed” . Psychometrika 16 . 2 ( 1951 ) , 203 – 206 2 . [ NSGP * 05 ] N EUMANN , L , S BERT , M , G OOCH , B , P URGATHOFER , W , et al . “Viewpoint quality : Measures and applications” . Proceedings of the 1st Eurographics Workshop on Computational Aesthetics in Graphics , Visualization and Imaging . Aire - la - Vile : The Eurographics Association Press . 2005 , 185 – 192 2 , 7 , 10 . [ PB96 ] P LEMENOS , D . and B ENAYADA , M . “Intelligent Display in Scene Modeling . New Techniques to Automatically Compute Good Views” . Proc . International Conference GRAPHICON’96 . St Petersbourg , Rus - sia , July 1996 1 , 2 , 7 , 10 . [ QYSG17 ] Q I , C HARLES R UIZHONGTAI , Y I , L I , S U , H AO , and G UIBAS , L EONIDAS J . “Pointnet + + : Deep hierarchical feature learning on point sets in a metric space” . Advances in Neural Information Processing Sys - tems 30 ( 2017 ) 9 . [ RA11 ] R ADINSKY , K IRA and A ILON , N IR . “Ranking from pairs and triplets : Information quality , evaluation methods and query complexity” . Proceedings of the fourth ACM international conference on Web search and data mining . 2011 , 105 – 114 3 . [ RBTH10 ] R USU , R ADU B OGDAN , B RADSKI , G ARY , T HIBAUX , R O - MAIN , and H SU , J OHN . “Fast 3d recognition and pose using the view - point feature histogram” . 2010 IEEE / RSJ International Conference on Intelligent Robots and Systems . IEEE . 2010 , 2155 – 2162 2 . [ RWL * 21 ] R U , C HANGLEI , W ANG , F EI , L I , T ONG , et al . “Outline view - point feature histogram : An improved point cloud descriptor for recogni - tion and grasping of workpieces” . Review of Scientific Instruments 92 . 2 ( 2021 ) , 025010 2 . [ SHVR21 ] S CHELLING , M ICHAEL , H ERMOSILLA , P EDRO , V ÁZQUEZ , P ERE - P AU , and R OPINSKI , T IMO . “Enabling Viewpoint Learning through Dynamic Label Generation” . Computer Graphics Forum ( 2021 ) . ISSN : 1467 - 8659 . DOI : 10 . 1111 / cgf . 142643 2 , 9 , 10 , 14 . [ SLF * 11 ] S ECORD , A DRIAN , L U , J INGWAN , F INKELSTEIN , A DAM , et al . “Perceptual models of viewpoint preference” . ACM Transactions on Graphics ( TOG ) 30 . 5 ( 2011 ) , 1 – 12 1 – 8 , 13 . [ SS02 ] S TOEV , S TANISLAV L and S TRASSER , W OLFGANG . “A case study on automatic camera placement and motion for visualizing histor - ical data” . IEEE Visualization , 2002 . VIS 2002 . IEEE . 2002 , 545 – 548 1 , 7 . [ Thu27 ] T HURSTONE , L OUIS L . “A law of comparative judgment . ” Psy - chological review 34 . 4 ( 1927 ) , 273 2 . [ TK01 ] T ARR , M ICHAEL J and K RIEGMAN , D AVID J . “What defines a view ? ” : Vision research 41 . 15 ( 2001 ) , 1981 – 2004 3 . [ TMWS12 ] T AO , J UN , M A , J UN , W ANG , C HAOLI , and S HENE , C HING - K UANG . “A unified approach to streamline selection and viewpoint se - lection for 3D flow visualization” . IEEE Transactions on Visualization and Computer Graphics 19 . 3 ( 2012 ) , 393 – 406 2 . [ VBP * 09 ] V IEIRA , T HALES , B ORDIGNON , A LEX , P EIXOTO , A DELAIL - SON , et al . “Learning good views through intelligent galleries” . Com - puter Graphics Forum . Vol . 28 . 2 . Wiley Online Library . 2009 , 717 – 726 1 . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 12 / Learning Human Viewpoint Preferences from Sparsely Annotated Models [ VFSG06 ] V IOLA , I VAN , F EIXAS , M IQUEL , S BERT , M ATEU , and G ROLLER , M EISTER E DUARD . “Importance - driven focus of atten - tion” . IEEE Transactions on Visualization and Computer Graphics 12 . 5 ( 2006 ) , 933 – 940 2 . [ VFSH01 ] V ÁZQUEZ , P ERE - P AU , F EIXAS , M IQUEL , S BERT , M ATEU , and H EIDRICH , W OLFGANG . “Viewpoint selection using viewpoint en - tropy . ” VMV . Vol . 1 . Citeseer . 2001 , 273 – 280 1 , 2 , 7 , 10 . [ WSK * 15 ] W U , Z HIRONG , S ONG , S HURAN , K HOSLA , A DITYA , et al . “3D ShapeNets : A deep representation for volumetric shapes” . Proceed - ings of the IEEE Conference on Computer Vision and Pattern Recogni - tion . 2015 , 1912 – 1920 2 , 4 . [ YLLY19 ] Y ANG , C HANGHE , L I , Y ANDA , L IU , C AN , and Y UAN , X I - AORU . “Deep learning - based viewpoint recommendation in volume vi - sualization” . Journal of Visualization 22 . 5 ( 2019 ) , 991 – 1003 2 . [ ZFY20 ] Z HANG , Y AN , F EI , G UANGZHENG , and Y ANG , G ANG . “3D viewpoint estimation based on aesthetics” . IEEE Access 8 ( 2020 ) , 108602 – 108621 2 . [ ZIE * 18 ] Z HANG , R ICHARD , I SOLA , P HILLIP , E FROS , A LEXEI A , et al . “The unreasonable effectiveness of deep features as a perceptual metric” . Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2018 , 586 – 595 2 . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . / Learning Human Viewpoint Preferences from Sparsely Annotated Models 13 Appendix A : Proofs In this section , we show that it is possible to reconstruct a function f on a finite set D solely from information about the binary classifi - cations f ( v 1 ) < f ( v 2 ) , v 1 , v 2 ∈ D , up to composition with a strictly monotonically increasing function . First , let us recall Prop . 1 from Section 4 , Proposition 2 . This reconstruction f ∗ preserves the order induced by f on the points in D , i . e . f ( v 1 ) < f ( v 2 ) ⇔ f ∗ ( v 1 ) < f ∗ ( v 2 ) , ∀ v 1 , v 2 ∈ D . ( 3 ) Proof . We show both directions subsequently . ” ⇒ ” : Let v 1 , v 2 ∈ D with f ( v 1 ) < f ( v 2 ) . Then { x ∈ D : f ( x ) < f ( v 1 ) } ⊂ { x ∈ D : f ( x ) < f ( v 2 ) } as f ( x ) < f ( v 1 ) ⇒ f ( x ) < f ( v 2 ) . By the definitions ( 2 ) and ( 1 ) it follows that count ( v 1 ) < count ( v 2 ) ⇒ f ∗ ( v 1 ) < f ∗ ( v 2 ) . ” ⇐ ” : Let v 1 , v 2 ∈ D with f ∗ ( v 1 ) < f ∗ ( v 2 ) . Assume that f ( v 1 ) > f ( v 2 ) , then the first part of the proof implies f ∗ ( v 1 ) > f ∗ ( v 2 ) , which is a contradiction . Further , assume f ( v 1 ) = f ( v 2 ) , then apparently count ( v 1 ) = count ( v 2 ) and thus f ∗ ( v 1 ) = f ∗ ( v 2 ) by definition ( 2 ) , which is also a contradiction . Thus f ( v 1 ) < f ( v 2 ) . Corollary 1 . The reconstruction f ∗ of f satisfies f ∗ = g ◦ f , ( 4 ) for a strictly monotonically increasing function g : R → R . Proof . Let I be the image of f , i . e . I : = { f ( v ) : v ∈ D } . Further , let D + ⊂ D , s . t . f (cid:12)(cid:12) D + : D + → I is bijective . Then f (cid:12)(cid:12) D + is invertible on D + , i . e . ∃ f − 1 : I → D + . Then the function g , defined as g : I → R g ( x ) : = f ∗ ◦ f − 1 ( x ) , satisfies f ∗ = g ◦ f . It is left to show that g is strictly monotonically increasing on I . Let x 1 , x 2 ∈ I with x 1 < x 2 . As x 1 , x 2 ∈ I there exist v 1 , v 2 ∈ D + s . t . x 1 = f ( v 1 ) , x 2 = f ( v 2 ) . From Prop . 2 , Eq . ( 3 ) we can infer f ∗ ( v 1 ) < f ∗ ( v 2 ) . Then g ( x 1 ) = f ∗ ( f − 1 ( x 1 ) ) = f ∗ ( v 1 ) < f ∗ ( v 2 ) = f ∗ ( f − 1 ( x 2 ) ) = g ( x 2 ) . This is also the best possible reconstruction from binary classifi - cations , as is clarified below . Proposition 3 . Let f 1 , f 2 : D → R be two functions , s . t . f 2 = g ◦ f 1 for a strictly monotonically increasing function g : R → R . Then the two functions f 1 and f 2 : = g ◦ f 1 yield the same binary classifi - cations , i . e . , f 1 ( v 1 ) < f 1 ( v 2 ) ⇔ f 1 ( v 1 ) < f 2 ( v 2 ) . Proof . The strict monotonicity of g directly implies that ∀ v 1 , v 2 ∈ D : f 1 ( v 1 ) < f 1 ( v 2 ) ⇔ g ( f 1 ( v 1 ) < g ( f 1 ( v 2 ) ) ⇔ f 2 ( v 1 ) < f 2 ( v 2 ) . Appendix B : Ablation Studies In this section , we describe the ablation studies we carry out to validate our design decisions : Handcrafted vs neural features . To evaluate the improvements introduced by the neural feature extraction module , i . e . , the convo - lutional neural network , we substituted the features computed by this module with a list of different handcrafted view quality mea - sures as in Secord et al . [ SLF * 11 ] . The rest of the Siamese network remains the same , the features of both images are concatenated and processed by the MLP which predicts the final probability . The re - sults of the experiments are presented in Table 4 , where we can see that , as expected , neural features always achieve better performance than handcrafted view quality measures . Data set sparsity . In our data set , we annotated each model with 18 views . However , previous work have favored the number of annotated views per model instead of a large variety of different annotated models , using 240 images per model instead [ SLF * 11 ] . Therefore , in this experiment , we evaluate which setup allows for a better generalization of our neural network , large number of mod - els annotated with a few images or few models more densely an - notated . We selected five categories from ModelNet40 , airplane , chair , flower pot , sofa , toilet , and selected seven models for each . For each model , we rendered 240 images , resulting in a total of 8400 images , and collected human preferences as described in Sec - tion 3 . We call this new data set D ENSE . Moreover , we created an - other data set with comparable number of images , by selecting 93 models for each of the five categories from our original data set , re - sulting in a total of 8370 images . We call this new data set S PARSE . We train our model with both data sets and compare the perfor - mance of our network . We can see in Table 5 that the model trained with the S PARSE data set achieves higher accuracy on all metrics , confirming that the neural network generalizes better with numer - ous different models even if the annotations are highly sparse . Probability computation . Previous work [ SLF * 11 ] has suggested computing a goodness of score from the image features that can be used in the Bradley and Terry model [ BT52 ] to compute the final probability of f ( x ) < f ( v ) . We use instead a neural network that takes both feature vectors and predicts the final probability . In this ablation study , we compare both methods with neural and handcrafted image features . Results are presented in Table 4 , where we can see that our neural probability predictor always achieves higher accuracy than the goodness score approach used by Secord et al . [ SLF * 11 ] . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) . 14 / Learning Human Viewpoint Preferences from Sparsely Annotated Models Features Ranking Acc . AUC AUPR N EURAL N EURAL 79 . 9 % 0 . 796 0 . 847 N EURAL G OODNESS S CORE 78 . 7 % 0 . 787 0 . 840 H ANDCRAFTED N EURAL 77 . 8 % 0 . 778 0 . 834 H ANDCRAFTED G OODNESS S CORE 76 . 7 % 0 . 767 0 . 825 Table 4 : Performance comparison between handcrafted vs . neu - ral viewpoint features . We also compare how the final probabil - ity is computed , with a goodness score in the Bradley and Terry model [ BT52 ] vs . using a neural network . The results indicate a better performance for neural extracted features in combination with a neural network to rank viewpoints . Data set # models # images Acc . AUC AUPR cat . total model total S PARSE 93 465 18 8370 75 . 6 % 0 . 760 0 . 820 D ENSE 7 35 240 8400 74 . 4 % 0 . 750 0 . 812 Table 5 : Effect of the sparsity of annotations on the prediction abil - ity of our model . A large number of models annotated with only a few images yields a higher generalization ability of our model than fewer models more densely annotated . Features size . In this experiment , we evaluate the effect of the fea - ture vector size in the final accuracy of the model . We train different models with feature vector sizes of 64 , 128 , and 256 . We can see in Table 6 that a feature vector size of 128 results in the higher accuracy . Performance statistics . In a last experiment , we measure the vari - ance in performance of our model over several training runs . There - fore , we average the scores of 10 models optimized using identical hyperparameters . In Table 7 , we show the averaged results for mod - els trained on the UNFILTERED and FILTERED data set , reporting the mean and standard deviation of the corresponding metric . Size Acc . 64 78 . 5 % 128 79 . 9 % 256 78 . 8 % Table 6 : Performance comparison using different sizes for the la - tent code in our viewpoint selection network . Appendix C : Quantitative Results on the Best View Prediction Task . In Section 7 . 3 , we showed qualitative comparison results between our human - based and four handcrafted view quality measures . In this section , we quantitatively evaluate how handcrafted view qual - ity measures perform using our view quality metric as ground truth . First , as an upper bound , we measure how the network of Schelling et al . [ SHVR21 ] performs when it is trained with our view qual - ity measure . For each predicted viewpoint , we compute the near - est neighbor to one of the 1000 sampled points in the sphere and Data set Acc . AUC AUPR U NFILTERED 79 . 9 % ± 0 . 004 0 . 796 ± 0 . 004 0 . 847 ± 0 . 003 F ILTERED 79 . 7 % ± 0 . 010 0 . 800 ± 0 . 010 0 . 850 ± 0 . 008 Table 7 : We averaged the performance of our models over 10 train - ing runs and report the mean and standard deviation of all metrics . Airplane Bench Bottle Car Chair Sofa Table Toilet 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 S i m il a r i t y t o H u m a n P r e f e r e n c e VEVKLVMIVR Figure 14 : Comparison between handcrafted quality measures and their similarity to our human based quality measure on the best view prediction task . compute the mean accuracy per category : AIRPLANE 92 % , BENCH 87 % , BOTTLE 86 % , CAR 94 % , CHAIR 89 % , SOFA 95 % , TABLE 92 % and TOILET 84 % . To evaluate the handcrafted view quality measures , we use the same procedure , but we train the network of Schelling et al . [ SHVR21 ] using the different handcrafted measures instead . In Figure 14 , the results show strong differences between view quality measure and category . submittedtoCOMPUTERGRAPHICS Forum ( 6 / 2022 ) .