MERMAID : Metaphor Generation with Symbolism and Discriminative Decoding Tuhin Chakrabarty 1 Xurui Zhang 3 , Smaranda Muresan 1 , 4 and Nanyun Peng 2 1 Department of Computer Science , Columbia University 2 Computer Science Department , University of California , Los Angeles , 3 Tsinghua University , 4 Data Science Institute , Columbia University { tuhin . chakr , smara } @ cs . columbia . edu thuzxr @ gmail . com , violetpeng @ cs . ucla . edu Abstract Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts , making connections between unre - lated concepts , and deviating from the literal meaning . Based on a theoretically - grounded connection between metaphors and symbols , we propose a method to automatically con - struct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus ( Jacobs , 2018 ) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference . For the generation task , we incorporate a metaphor discrimina - tor to guide the decoding of a sequence to se - quence model ﬁne - tuned on our parallel data to generate high quality metaphors . Human evaluation on an independent test set of literal statements shows that our best model gener - ates metaphors better than three well - crafted baselines 66 % of the time on average . A task - based evaluation shows that human - written po - ems enhanced with metaphors proposed by our model are preferred 68 % of the time compared to poems without metaphors . 1 Introduction Czech novelist Milan Kundera in his book “The unbearable lightness of being " said “Metaphors are not to be triﬂed with . A single metaphor can give birth to love . " . Metaphors allow us to communicate not just in - formation , but also real feelings and complex at - titudes ( Veale et al . , 2016 ) . While most compu - tational work has focused on metaphor detection ( Gao et al . , 2018 ; Stowe et al . , 2019 ; Shutova et al . , 2010 ; Tsvetkov et al . , 2014 ; Veale et al . , 2016 ; Stowe and Palmer , 2018 ) , research on metaphor generation is under - explored ( Yu and Wan , 2019 ; Stowe et al . , 2020 ) . Generating metaphors could impact many downstream applications such as cre - Literal Input1 The wildﬁre spread through the forest at an amazing speed . GenMetaphor1 The wildﬁre danced through the forest at an amazing speed . Literal Input2 The window panes were rattling as the wind blew through them GenMetaphor2 The window panes were trembling as the wind blew through them Table 1 : Examples of two generated metaphors Gen - Metaphor1 and GenMetaphor2 by our best model MER - MAID from their literal inputs . ative writing assistance , literary or poetic content creation . Relevant statistics demonstrate that the most frequent type of metaphor is expressed by verbs ( Steen , 2010 ; Martin , 2006 ) . We therefore focus on the task of generating a metaphor starting from a literal utterance ( Stowe et al . , 2020 ) . where we transform a literal verb to a metaphorical verb . ( See Table 1 for examples of literal sentences and the generated metaphors ) . To tackle the metaphor generation problem we need to address three challenges : 1 ) the lack of training data that consists of pairs of literal utter - ances and their equivalent metaphorical version in order to train a supervised model ; 2 ) ensur - ing that amongst the seemingly endless variety of metaphoric expressions the generated metaphor can fairly consistently capture the same general mean - ing as the literal one , with a wide variety of lexical variation ; and 3 ) computationally overcome the in - nate tendency of generative language models to produce literal text over metaphorical one . In an attempt to address all these challenges , we introduce our approach for metaphor generation called MERMAID ( MEtaphor geneRation with syM - bolism And dIscriminative Decoding ) , making the following contributions : • A method to automatically construct a corpus that contains 93 , 498 parallel [ literal sentence , metaphorical sentence ] pairs by leveraging a r X i v : 2103 . 06779v1 [ c s . C L ] 11 M a r 2021 the theoretically - grounded relation between metaphor and symbols . Barsalou et al . ( 1999 ) showed how perceptual symbols arising from perception are used in conceptual tasks such as representing propositions and abstract con - cepts . Philosopher Susanne Langer in her es - say “Expressiveness and Symbolism” stated “A metaphor is not language , it is an idea ex - pressed by language , an idea that in its turn functions as a symbol to express something” . Our approach has two steps : 1 ) identify a set of sentences that contains metaphorical verbs from an online poetry corpus ; 2 ) convert these metaphorical sentences to their literal versions using Masked Language Models and structured common sense knowledge achieved from COMET ( Bosselut et al . , 2019 ) , a lan - guage model ﬁne - tuned on ConceptNet ( Speer et al . , 2017 ) . For the later , we exploit the SymbolOf relation to make sure the generated sentence that contains the literal sense of the verb has the same symbol as the metaphorical sentence . For example , for the metaphorical sentence “The turbulent feelings that surged through his soul " our method will generate “The turbulent feelings that continued through his soul " maintaining the common symbolic meaning of ( love , loss , despair , sorrow , loneli - ness ) between the two ( Sec . 2 ) . • Using a metaphor discriminator to guide the decoding of a sequence - to - sequence model ﬁne - tuned on our parallel data to generate high quality metaphors . Our system MER - MAID , ﬁne - tunes BART ( Lewis et al . , 2019 ) – a state of the art pre - trained denoising au - toencoder built with a sequence to sequence model , on our automatically collected parallel corpus of [ literal sentence , metaphorical sen - tence ] pairs ( Sec . 3 . 1 ) to generate metaphors . A discriminative model trained in identifying metaphors is further used to complement our generator and guide the decoding process to improve the generation ( Sec . 3 . 2 ) . Human evaluations show that this approach generates metaphors that are better than two literary ex - perts 21 % of the time on average , better 81 % of the time than two well - crafted baselines , and better 36 % of the time than ﬁne - tuned BART ( Lewis et al . , 2019 ) ( Sec . 5 ) . • A task - based evaluation : improving the quality of human written poems . Evaluation via Amazon Mechanical Turk shows that poems enhanced with metaphors generated by MERMAID are preferred by Turkers 68 % of the times compared to poems without metaphors , which are preferred 32 % of the times ( Sec . 6 ) . Our code , data and models can be found in the link https : / / github . com / tuhinjubcse / MetaphorGenNAACL2021 2 Dataset Creation with MLM and Symbolism Datasets for metaphors are scarce . To our knowl - edge , there is no large scale parallel corpora con - taining literal and metaphoric paraphrases . The closest and most useful work is that of Mohammad et al . ( 2016 ) . However the size of this data - set is small : 171 instances , which is not sufﬁcient to train deep learning models . Recently , Stowe et al . ( 2020 ) rely on available metaphor detection datasets to generate metaphors by a metaphor - masking frame - work , where they replace metaphoric words in the input texts with metaphor masks ( a unique “metaphor” token ) , hiding the lexical item . This creates artiﬁcial parallel training data : the input is the masked text , with the hidden metaphorical word , and the output is the original text ( e . g . , The war [ MASK ] many people → The war uprooted many people ) . The major issue with such mask - ing strategy is that it ignores the semantic mapping between the literal verb and the metaphorical verb . Moreover , there are only 11 , 593 such parallel in - stances , still too small to train a neural model . The lack of semantic mapping between the artiﬁcial par - allel training data samples , coupled with limited size thus affects the lexical diversity and meaning preservation of generated metaphors at test time . In light of these challenges , we propose to compose a large - scale parallel corpora with literal to metaphor - ical sentence pairs to learn the semantic mappings . We start with collecting a large - scale corpora of metaphorical sentences ( Section 2 . 1 ) and leverage masked language model and symbolism - relevant common sense knowledge to create literal version for each metaphorical sentence ( Section 2 . 2 ) . 2 . 1 Metaphor dataset collection Metaphors are frequently used in Poetry to explain and elucidate emotions , feelings , relationships and That wounded forehead dashed with blood and wine To heal and raise from death my heart That wounded forehead covered with blood and wine To heal and help from death my heart BART DECODERTARGET ENCODERTARGET The tax cut will help the economy Black desert covered in iron silences The tax cut will stimulate the economy Black desert gripped in iron silences BART MLM COMET DISCRIMANTOR SOURCE Figure 1 : A schematic illustration of our system , which shows the data creation and training process where we use MLM along with COMET to transform an original metaphorical input to a literal output evoking similar symbolic meaning and use them to ﬁne - tune BART . other elements that could not be described in or - dinary language . We use this intuition to identify a naturally occurring poetry corpus that contains metaphors called Gutenberg Poetry Corpus ( Jacobs , 2018 ) . 1 The corpus contains 3 , 085 , 117 lines of po - etry extracted from hundreds of books . Not every sentence in the corpus contains a metaphorical verb . So as a ﬁrst step , we identify and ﬁlter sentences containing a metaphorical verb . We build a classiﬁer by ﬁne - tuning BERT ( De - vlin et al . , 2018 ) on a metaphor detection corpus VU A MSTERDAM ( Steen , 2010 ) . Since our work is focused on verbs , we only do token classiﬁcation and calculate loss for verbs . Figure 2 illustrates the BERT - based token - level classiﬁer . The classiﬁca - tion accuracy on test set is 74 . 7 % , which is on par with most state of art methods . Using the metaphor detection model , we identify 622 , 248 ( 20 . 2 % ) sentences predicted by our model as containing a metaphoric verb . Considering the classiﬁer can introduce noise as the accuracy of the metaphor detection model is far from oracle 100 % , we only retain sentences which are predicted by our model with a conﬁdence score of 95 % ( i . e . , predic - tion probability 0 . 95 ) . This results in a total number of 518 , 865 ( 16 . 8 % ) metaphorical sentences . 2 . 2 Metaphoric to Literal Transformation with Symbolism After identifying high quality metaphorical sen - tences , we want to obtain their literal counterparts to create a parallel training data . Masked lan - guage models like BERT ( Devlin et al . , 2018 ) , or roBERTa ( Liu et al . , 2019 ) can be used for ﬁll - in - the - blank tasks , where the model uses the context words surrounding a masked token to predict the masked word . We borrow this framework to mask 1 https : / / github . com / aparrish / gutenberg - poetry - corpus BERT [ CLS ] Linear + Softmax w 1 v 1 w 2 w 3 w 4 v 2 [ SEP ] M L Figure 2 : BERT - base - cased model to identify metaphor . v 1 and v 2 represent the verbs of a sentence . ( M ) de - notes softmax probabality of verb being metaphorical while ( L ) denotes it literal softmax probability the metaphorical verb ( Table 2 Row1 vs Row2 ) from a sentence and use BERT - base - cased model to obtain the top 200 candidate verbs to replace the metaphorical one to generate literal sentences ( Ta - ble 2 Row3 ) . There are two main issues in solely relying on MLM predicted verbs : 1 ) they are not necessarily literal in nature ; 2 ) after replacing the default MLM predicted verb , the metaphorical sen - tence and the new sentence with the replaced verb might be semantically dissimilar . 2 . 2 . 1 Ensuring Literal Sense Even though our inductive biases tell us that the chance of a predicted token having a literal sense is higher than having metaphorical one , this cannot be assumed . To ﬁlter only literal candidate verbs we re - rank the MLM predicted mask tokens based on literal scores obtained from 2 . 1 since the model can predict the softmax probability of a verb in a sentence being both literal or metaphorical ( Table 2 Row4 ) . 2 . 2 . 2 Ensuring Meaning Preservation It can be said that we can potentially pair the sen - tence with the top most literal ranked verb with the input sentence containing the metaphorical verb but Input The turbulent feelings that surged through his soul . Masked The turbulent feelings that [ MASK ] through his soul . RankedbyMLMProb ( ‘tore’ , 0 . 11 ) , ( ‘ran’ , 0 . 10 ) , ( ‘ripped’ , 0 . 09 ) , ( ‘ﬂowed’ , 0 . 03 ) , ( ‘rushed’ , 0 . 01 ) , . . . . . , ( ‘eased’ , 0 . 01 ) , . . . . , ( ‘continued’ , 0 . 0005 ) , . . . RankedbyLiteralProb ( ‘eased’ , 0 . 12 ) , ( ‘continued’ , 0 . 0008 ) , ( ‘spread’ , 0 . 0004 ) , ( ‘kicked’ , 0 . 99 ) , ( ‘punched’ , 0 . 99 ) , . . . . . , ( ‘screamed’ , 0 . 99 ) , . . . . . Table 2 : Table showing a metaphorical sentence ( Row1 ) where the metaphorical verb surge is masked ( Row2 ) . Row3 shows predicted tokens ranked by de - fault LM probability . Row4 shows predicted tokens ranked by metaphoricity scores obtain from model de - scribed in 2 . 1 . Lower scores means more literal . Meta Input The turbulent feelings that surged through his soul . Inp Symbol love , loss , despair , sorrow , loneliness Lit Output1 The turbulent feelings that eased through his soul . (cid:55) Symbol peace , love , happiness , joy , hope Lit Output2 The turbulent feelings that continued through his soul . (cid:51) Symbol love , loss , despair , sorrow , loneliness Table 3 : Table showing input metaphorical sentence and literal outputs along with the associated symbolic meaning obtained from COMET ( Bosselut et al . , 2019 ) . Lit Output1 is an incorrect candidate since the symbolic meanings are divergent . they might symbolically or semantically represent different abstract concepts . For example , in Table 3 , after replacing the metaphorical verb “surge " with the top most literal verb “eased " , the sentence “The turbulent feelings that eased through his soul " evoke a different and polar symbolic meaning of peace , love , happiness , joy & hope in comparison to the input with metaphorical verb , which evokes a symbolic meaning of love , loss , despair , sorrow & loneliness . To tackle this problem we ensure that the transformed literal output represents the same symbolic meaning as the metaphorical input . To generate the common sense SYMBOL that is implied by the literal or metaphorical sentences , we feed the sentences as input to COMET ( Bosselut et al . , 2019 ) and restrict it to return top - 5 beams . COMET is an adaptation framework for construct - ing commonsense knowledge based on pre - trained language models . Our work only leverages the SymbolOf relation from COMET 2 . We now need a method to combine information 2 https : / / mosaickg . apps . allenai . org / comet _ conceptnet from MLM and symbolic knowledge obtained from COMET described above . To do this , we ﬁlter can - didates from MLM token predictions based on the symbolic meaning overlap between the metaphor - ical input and literal output ﬁrst . To ensure that the quality is high , we put a strict requirement that all the 5 symbolic beams ( typically words or short phrases ) for the input metaphorical sentence should match all the 5 symbolic beams for the output literal sentence . Between multiple literal candidates all having beam overlap of 5 , they are further ranked by reverse metaphoricity ( i . e . , literal ) scores . The top most candidate is returned thereafter . We ﬁ - nally end up with 90 , 000 pairs for training and 3 , 498 pairs for validation . 3 Metaphor Generation Our goal of generating metaphors can be broken down into two primary tasks : 1 ) generating the appropriate substitutions for the literal verb while being pertinent to the context ; 2 ) ensuring that the generated utterances are actually metaphorical . 3 . 1 Transfer Learning from BART To achieve the ﬁrst goal , we ﬁne - tune BART ( Lewis et al . , 2019 ) , a pre - trained conditional lan - guage model that combines bidirectional and auto - regressive transformers , on the collected parallel corpora . Speciﬁcally , we ﬁne - tune BART by treat - ing the literal input as encoder source and the metaphorical output as the the decoder target ( Fig - ure 1 ) . One issue of the pre - trained language mod - els is that they have a tendency to generate literal to - kens over metaphorical ones . To overcome this , we introduce a rescoring model during the decoding process to guide to model to favor more metaphori - cal verbs . The rescoring model is inspired by Holtz - man et al . ( 2018 ) ; Goldfarb - Tarrant et al . ( 2020 ) and detailed as follows . 3 . 2 Discriminative Decoding We have a base metaphor generation model p ( z | x ) which is learned by ﬁne - tuning BART ( Lewis et al . , 2019 ) on pairs of literal ( x ) and metaphorical ( z ) sentences . We propose to modify the decoding ob - jective to incorporate a Metaphor detection rescor - ing model a and re - rank the base , or “naive " BART generated hypotheses , bringing the metaphoric rep - resentation closer to the rescoring model’s specialty and desirable attribute . The modiﬁed decoding ob - jective becomes : That wounded forehead dashed with blood and wine To heal and raise from death my heart That wounded forehead covered with blood and wine To heal and help from death my heart BART DECODERTARGET ENCODERTARGET The tax cut will help the economy Black desert covered in iron silences The tax cut will stimulate the economy Black desert gripped in iron silences BART MLM COMET DISCRIMANTOR SOURCE Figure 3 : Schematic showing the decoding step where we use ﬁne - tuned BART along with a metaphor detecting discriminator to generate a metaphorical sentence conditioned on a literal input f λ ( x , z ) = m (cid:88) i − log p ( z | z < i , x ) + λa ( x , z i . . . m ) ( 1 ) where λ is a weight of the score given by a . Implementation Details We use top - k sampling strategy ( Fan et al . , 2018 ) ( k = 5 ) to generate metaphors conditioned on a literal input . Our rescoring model a is a RoBERTa model ﬁne - tuned on a combined dataset of ( Steen , 2010 ; Beigman Klebanov et al . , 2018 ) to classify sen - tences as literal or metaphorical based on whether there exists a metaphorical verb . It is a sentence level task where the model predicts a sentence as literal or metaphorical . We down - sample the data to maintain a ratio of ( 1 : 1 ) between two classes and use 90 % of the data to train and 10 % for valida - tion . We achieve a considerably decent validation accuracy of 83 % . We manually tune λ using grid search on a small subset of 3 , 498 validation sam - ples of our parallel automatic data and choose the best value . Figure 3 shows the process of re - ranking BART hypothesis using the discriminator described above to generate novel metaphorical replacements for literal verbs . All the hyper - parameters for data creation , ﬁne - tuning and discriminative decoding are exactly the same as mentioned in Appendix ( Supplementary material ) . 4 Experimental Setup To compare the quality of the generated metaphors , we benchmark our MERMAID model against human performance ( i . e . , the two creative writing experts HUMAN1 ( a novelist ) & HUMAN2 ( a poet ) who aren’t the authors of the paper ) [ Section 4 . 2 ] and three baseline systems described below . 4 . 1 Baseline Systems Lexical Replacement ( LEXREP ) : We use the same idea as our data creation process ( Section 2 . 2 ) . We use our model described in Section 2 . 1 to re - rank the predicted tokens from a mask lan - guage model based on metaphoricity scores . We ﬁlter the top 25 ranked metaphorical candidates and further rerank them based on symbolic mean - ing overlap with the literal using COMET ( Bosselut et al . , 2019 ) and replace the literal verb with the top most candidate . Metaphor Masking ( META _ M ) : We use the metaphor masking model proposed by Stowe et al . ( 2020 ) where the language model learns to replace a masked verb with a metaphor . They train a seq2seq model with the encoder input of the for - mat ( The tax cut [ MASK ] the economy ) and the decoder output being the actual metaphorical sen - tence ( The tax cut lifted the economy ) . During inference they mask the literal verb and expect the language model to inﬁll a metaphorical verb . BART : We use generations from BART model ﬁne - tuned on our automatically created data with - out the discriminative decoding . This helps us gauge the effect of transfer learning from a large generative pre - trained model which also accounts for context unlike retrieval based methods . 4 . 2 Test Data To measure the effectiveness of our approach , we need to evaluate our model on a dataset that is inde - pendent of our automatically created parallel data and that is diverse across various domains , gen - res and types . Hence we rely on test data from multiple sources . As our ﬁrst source , we randomly sample literal and metaphorical sentences with high conﬁdence ( > 0 . 7 ) and unique verbs from the ex - isting dataset of Mohammad et al . ( 2016 ) . For the metaphorical sentences from Mohammad et al . ( 2016 ) we convert them to their literal equivalent the same way as discussed in Section 2 . 2 without the use of COMET as we do not need it . To ensure diversity in genre , as our second source we scrape W RITING P ROMPT and O CPOETRY subreddits for sentences with length up to 12 words , which are lit - eral in nature based on prediction from our model described in Section 2 . 1 . We collate 500 such sen - tences combined from all sources and randomly sample 150 literal utterance for evaluation . We use two literary experts ( not authors of this paper ) — a student in computer science who is also a poet , and a student in comparative literature who is the author of a novel — to write corresponding metaphors for each of these 150 inputs for evalua - tion and comparison . 4 . 3 Evaluation Criteria Automatic evaluation One important aspect to evaluate the quality of the generated metaphors is whether they are faithful to the input : while we change literal sentences to metaphorical ones , it should still maintain the same denotation as the input . To this end , we calculate the Semantic Simi - larity between the metaphorical output and the in - put using sentence - BERT ( SBERT ) ( Reimers and Gurevych , 2019 ) . We also calculate corpus - level BLEU - 2 ( Papineni et al . , 2002 ) and BERTScore ( Zhang et al . , 2019 ) with human written references . Human evaluation Since automatic evaluation is known to have signiﬁcant limitations for cre - ative generation ( Novikova et al . , 2017 ) , we further conduct human evaluation on a total of 900 ut - terances , 600 generated from 4 systems and 300 generated by two human experts . We proposed a set of four criteria to evaluate the generated output : ( 1 ) Fluency ( Flu ) ( “How ﬂuent , grammatical , well formed and easy to understand are the generated ut - terances ? ” ) , ( 2 ) Meaning ( Mea ) ( “Are the input and the output referring or meaning the same thing ? " ) ( 3 ) Creativity ( Crea ) ( “How creative are the gen - erated utterances ? ” ) , and ( 4 ) Metaphoricity ( Meta ) ( “How metaphoric are the generated utterances” ) . The human evaluation is done on the Amazon Me - chanic Turk platform . Each Turker was given a literal input and 6 metaphorical outputs ( 4 from sys - tem outputs – 3 baselines and our proposed system MERMAID , and 2 from humans ) at a time , with the metaphorical outputs randomly shufﬂed to avoid potential biases . Turkers were instructed to evalu - ate the quality of the metaphorical sentences with respect to the input and not in isolation . As we evaluate on four dimensions for 900 utterances , we have a total of 3600 evaluations . Each criteria was rated on a likert scale from 1 ( not at all ) to 5 ( very ) . Each group of utterances was rated by three sepa - rate Turkers , resulted in 42 , 48 , 44 and 53 Turkers for the four evaluation tasks respectively . We pay them at a rate of $ 15 per hour . 5 Results Based on the semantic similarity metric shown in column 1 of Table 4 , our system MERMAID is better in preserving the meaning of the input than System Similarity ↑ BLEU - 2 ↑ BertScore ↑ LEXREP 79 . 6 68 . 7 0 . 56 META _ M 73 . 2 61 . 0 0 . 62 BART 83 . 6 65 . 0 0 . 65 MERMAID 85 . 0 66 . 7 0 . 71 HUMAN1 86 . 6 - - HUMAN2 84 . 2 - - Table 4 : Automatic evaluation results on test set where MERMAID signiﬁcantly outperforms other automatic methods for 2 out of 3 metrics ( p < . 001 ) accord - ing to approximate randomization test ) . BLEU - 2 and BertScore is calculated w . r . t to Human references ( HU - MAN1 & HUMAN2 ) . Corpus level BLEU - 2 and Semantic Similarity are in range of ( 0 - 100 ) while BertScore is in range ( 0 - 1 ) System Flu Mea Crea Meta HUMAN1 3 . 83 3 . 77 4 . 02 3 . 52 HUMAN2 3 . 29 3 . 43 3 . 58 3 . 16 LEXREP 2 . 21 2 . 59 2 . 16 1 . 98 META _ M 2 . 10 1 . 91 2 . 00 1 . 89 BART 3 . 33 3 . 08 3 . 16 2 . 85 MERMAID 3 . 46 3 . 35 3 . 50 3 . 07 Table 5 : Human evaluation on four criteria of metaphors quality for systems and humans generated metaphors . We show average scores on a likert scale of 1 - 5 where 1 denotes the worst and 5 be the best . Bold - face denotes the best results overall and underscore de - notes the best among computational models . the other baselines . As mentioned , we calculate BLEU - 2 and BERTScore between system outputs and human references . MERMAID is better than the other baselines according to BERTScore . In terms of BLEU - 2 , MERMAID is second best . Table 5 shows the average scores for the hu - man evaluation on four metaphor quality criteria for MERMAID , the baselines , and human written metaphors on the test set . The inter - annotator agree - ments computed using Krippendorff’s alpha for Creativity , Meaning , Fluency and Metaphoricity are 0 . 44 , 0 . 42 , 0 . 68 , 0 . 52 respectively . The results demonstrate that MERMAID is signiﬁcantly better than the baselines on all four criteria ( p < . 001 according to approximate randomization test ) . Ta - ble 6 presents several generation outputs from dif - ferent systems along with human judgements on individual criteria . We observe that incorporating a discriminator often guides our model to gener - ate better metaphors than the already strong base - line using BART . Finally , incorporating symbolic meaning in data creation step helps our model to maintain the same meaning as the input . Literal System Metaphor Flu Mea Crea Meta The scream ﬁlled the night HUMAN1 The scream pierced the night 4 . 3 5 . 0 3 . 7 4 . 0 HUMAN2 The scream covered the night 2 . 7 4 . 0 3 . 0 3 . 0 LEXREP The scream held the night 1 . 7 3 . 7 2 . 0 1 . 7 META _ M The scream opened the night 1 . 0 1 . 0 1 . 0 1 . 0 BART The scream ﬁlled the night 2 . 3 1 . 0 2 . 3 1 . 0 MERMAID The scream pierced the night 4 . 3 5 . 0 3 . 7 4 . 0 The wildﬁre spread through the forest at an amazing speed HUMAN1 The wildﬁre ravaged through the forest at an amazing speed 4 . 7 4 . 3 4 . 0 3 . 0 HUMAN2 The wildﬁre leapt through the forest at an amazing speed 3 . 7 3 . 0 5 . 0 3 . 7 LEXREP The wildﬁre saw through the forest at an amazing speed 1 . 3 1 . 0 2 . 7 3 . 3 META _ M The wildﬁre grows through the forest at an amazing speed 3 . 7 2 . 7 2 . 7 4 . 0 BART The wildﬁre swept through the forest at an amazing speed 4 . 0 3 . 7 4 . 7 4 . 0 MERMAID The wildﬁre danced through the forest at an amazing speed 3 . 0 4 . 0 4 . 0 3 . 7 My heart beats when he walks in the room HUMAN1 My heart skips when he walks in the room 4 . 7 5 . 0 4 . 0 4 . 3 HUMAN2 My heart sings when he walks in the room 5 . 0 4 . 3 3 . 7 3 . 3 LEXREP My heart made when he walks in the room 1 . 0 1 . 0 1 . 0 1 . 0 META _ M My heart came when he walks in the room 1 . 7 1 . 0 1 . 3 1 . 3 BART My heart sings when he walks in the room 5 . 0 4 . 3 3 . 7 3 . 7 MERMAID My heart jumps when he walks in the room 4 . 7 4 . 7 4 . 3 4 . 0 After a glass of wine , he relaxed up a bit HUMAN1 After a glass of wine , he loosened up a bit 4 . 7 5 . 0 5 . 0 4 . 0 HUMAN2 After a glass of wine , he unfurled up a bit 2 . 0 5 . 0 2 . 0 3 . 7 LEXREP After a glass of wine , he followed up a bit 3 . 7 1 . 0 2 . 7 1 . 7 META _ M After a glass of he touched up a bit 1 . 3 1 . 0 1 . 7 2 . 0 BART After a glass of wine , he dried up a bit 2 . 7 1 . 0 2 . 3 2 . 0 MERMAID After a glass of wine , he loosened up a bit 4 . 3 5 . 0 5 . 0 3 . 7 The tax cut will help the economy HUMAN1 The tax cut will uplift the economy 4 . 7 5 . 0 4 . 7 4 . 0 HUMAN2 The tax cut will fertilize the economy 4 . 0 4 . 3 4 . 3 3 . 7 LEXREP The tax cut will bring the economy 1 . 7 3 . 0 2 . 7 1 . 7 META _ M The tax cut will prevent the economy 1 . 7 1 . 0 2 . 0 1 . 0 BART The tax cut will strengthen the economy 5 . 0 5 . 0 4 . 3 3 . 7 MERMAID The tax cut will stimulate the economy 5 . 0 4 . 7 3 . 7 4 . 0 I tried to resolve things over be - tween them HUMAN1 I tried to tide things over between them 4 . 3 3 . 0 3 . 7 4 . 3 HUMAN2 I tried to patch things over between them 4 . 7 4 . 7 5 . 0 2 . 0 LEXREP I tried to push things over between them 3 . 3 1 . 0 2 . 3 2 . 0 META _ M I tried to make things over between them 4 . 0 1 . 0 2 . 7 2 . 7 BART I tried to put things over between them 4 . 7 2 . 0 3 . 0 2 . 7 MERMAID I tried to smooth things over between them 4 . 7 4 . 7 5 . 0 4 . 0 Table 6 : Examples of generated outputs from different systems ( with human written metaphors as references ) . We show average scores ( over three annotators ) on a 1 - 5 scale with 1 denotes the worst and 5 be the best . The italics texts in the literal column represent the verb while those in Metaphor column represents the generated metaphorical verb . Boldface indicates the best results . 6 Task Based Evaluation Metaphors are frequently used by creative writing practitioners , in particular poets , to embellish their work . We posit that MERMAID can be used to edit literal sentences in poems to add further creativ - ity . To test this hypothesis , we ﬁrst crawl original poems submitted by authors from the sub - reddit OCP OETRY . The poems are of variable lengths , so to ensure parity we break them into Quatrains ( four sentence stanza ) . We randomly sample 50 such Quatrains containing at least one sentence with a literal verb in it . We use our metaphor detection model ( Section 2 . 1 ) to detect literal verbs . We then select a sentence containing a lit - eral verb from each Quatrain and use MER - MAID to re - write it so that the resulting output is metaphorical . We ignore common verbs like is , was , are , were , have , had . If there are more than one sentence in Quatrain with literal verbs , we choose the sentence with a literal verb that has the highest probability for being literal . For sentences with multiple literal verbs , we choose the verb with highest literal probability . Our goal is to see if re - written poems are qualita - tively better than the original forms . To do this , we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re - written version . 15 Turkers were re - cruited for the task . Each Quatrain was evaluated by 3 distinct Turkers . Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers . Figure 4 : Percentage of Preference of Original Qua - trains vs Quatrains rewritten by MERMAID And the hills have a shimmer of light between , And the valleys are covered with misty veils , And . . . . . . . . . , . . . . . And the hills have a shimmer of light between , And the valleys are wrapped with misty veils , And . . . . . . . . . , Leaves on a maple , burst red with the shorter days ; Falling to the ground . . . . . Leaves on a maple , burgeoned red with the shorter days ; Falling to the ground . . . . . Table 7 : Example Quatrains from reddit where MER - MAID rewrites a sentence containing a literal verb to make it metaphorical . 7 Related Work Most researchers focused on identiﬁcation and in - terpretation of metaphor , our work on metaphor generation is relatively new . 7 . 1 Metaphor Detection For metaphor detection , researchers focused on variety of features , including unigrams , imageabil - ity , sensory features , WordNet , bag - of - words fea - tures ( Klebanov et al . , 2014 ; Tsvetkov et al . , 2014 ; Shutova et al . , 2016 ; Tekiro˘glu et al . , 2015 ; Hovy et al . , 2013 ; Köper and im Walde , 2016 ) . With advent of deep learning approaches , Gao et al . ( 2018 ) used BiLSTM models based on GloVe ( Pennington et al . , 2014 ) and ELMo word vectors ( Peters et al . , 2018 ) to detect metaphoric verbs . In - spired by the linguistic theories , MIP ( Semino et al . , 2007 ; Steen , 2010 ) and SPV ( Wilks , 1975 , 1978 ) , Mao et al . ( 2019 ) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings . Re - cent work on metaphor detection have also used pretrained language models ( Su et al . , 2020 ; Gong et al . , 2020 ) . While we focus on metaphor gen - eration , we use ( Devlin et al . , 2018 ) to detect metaphoric verbs to create parallel data and ( Liu et al . , 2019 ) to rescore our generated hypothesis during decoding . 7 . 2 Metaphor Generation Some early works made contributions to use tem - plate and heuristic - based methods ( Abe et al . , 2006 ; Terai and Nakagawa , 2010 ) to generate “A is like B” sentences , more popularly referred to as similes . Chakrabarty et al . ( 2020 ) concentrated on simile generation , applying seq2seq model to paraphrase a literal sentence into a simile . Other attempts learned from the mappings of different domains and generated conceptual metaphors of pattern “A is B” ( Hervás et al . , 2007 ; Mason , 2004 ; Gero and Chilton , 2019 ) . These works paid attention to the relationship between nouns and concepts to create elementary ﬁgurative expressions . Recent metaphor generation works mainly aimed at verbs . Yu and Wan ( 2019 ) proposed an unsupervised metaphor extraction method , and developed a neural generation model to generate metaphorical sentences from literal - metaphorical verb pairs . They however do not focus on literal to metaphorical sentence transfer , but generate a sentence given a metaphorical ﬁt word . The clos - est to our work is that of Stowe et al . ( 2020 ) , who focus on building a seq2seq model , using a spe - cial mask token to mask the metaphorical verbs as input , and the original metaphorical sentences as output . However , this model face challenges in transferring the literal sentences to metaphorical ones , while maintaining the same meaning . We , on the contrary , focus on maintaining the same mean - ing through parallel data creation focusing on sym - bolism . Additionally , we incorporate a metaphor detection model as a discriminator to improve de - coding during generation . 8 Conclusion We show how to transform literal sentences to metaphorical ones . We propose a novel way of creating parallel corpora and an approach for gener - ating metaphors that beneﬁts from transfer learning and discriminative decoding . Human and auto - matic evaluations show that our best model is suc - cessful at generating metaphors . We further show that leveraging symbolic meanings helps us learn better abstract representations and better preserva - tion of the denotative meaning of the input . Fu - ture directions include learning diverse conceptual metaphoric mapping using our parallel data and constraining our metaphoric generations based on particular mapping . 9 Ethics Our data is collected from Reddit and we under - stand and respect user privacy . Our models are ﬁne - tuned on sentence level data obtained from user posts . These do not contain any explicit de - tail which leaks information about a users name , health , negative ﬁnancial status , racial or ethnic origin , religious or philosophical afﬁliation or be - liefs , sexual orientation , trade union membership , alleged or actual commission of crime . Second , although we use language models trained on data collected from the Web , which have been shown to have issues with bias and abusive language ( Sheng et al . , 2019 ; Wallace et al . , 2019 ) , the inductive bias of our models should limit in - advertent negative impacts . Unlike model vari - ants such as GPT , BART is a conditional language model , which provides more control of the gener - ated output . Furthermore , we speciﬁcally encode writing style from a poetic corpus in our models and train on parallel data in the direction of literal to metaphorical style . Open - sourcing this technology will help to generate metaphoric text assisting cre - ative writing practitioners or non native language speakers to improve their writing . We do not envi - sion any dual - use that can cause harm for the use of our the metaphor generation system . References Keiga Abe , Kayo Sakamoto , and Masanori Nakagawa . 2006 . A computational model of the metaphor gen - eration process . In Proceedings of the Annual Meet - ing of the Cognitive Science Society , volume 28 . Lawrence W Barsalou et al . 1999 . Perceptual symbol systems . Behavioral and brain sciences , 22 ( 4 ) : 577 – 660 . Beata Beigman Klebanov , Chee Wee ( Ben ) Leong , and Michael Flor . 2018 . A corpus of non - native written English annotated for metaphor . In Proceedings of the 2018 Conference of the North American Chap - ter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Pa - pers ) , pages 86 – 91 , New Orleans , Louisiana . Asso - ciation for Computational Linguistics . Antoine Bosselut , Hannah Rashkin , Maarten Sap , Chai - tanya Malaviya , Asli Celikyilmaz , and Yejin Choi . 2019 . COMET : Commonsense transformers for au - tomatic knowledge graph construction . In Proceed - ings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4762 – 4779 , Florence , Italy . Association for Computational Lin - guistics . Tuhin Chakrabarty , Smaranda Muresan , and Nanyun Peng . 2020 . Generating similes effortlessly like a pro : A style transfer approach for simile generation . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6455 – 6469 , Online . Association for Computa - tional Linguistics . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . Bert : Pre - training of deep bidirectional transformers for language understand - ing . arXiv preprint arXiv : 1810 . 04805 . Angela Fan , Mike Lewis , and Yann Dauphin . 2018 . Hi - erarchical neural story generation . arXiv preprint arXiv : 1805 . 04833 . Ge Gao , Eunsol Choi , Yejin Choi , and Luke Zettle - moyer . 2018 . Neural metaphor detection in context . arXiv preprint arXiv : 1808 . 09653 . Katy Ilonka Gero and Lydia B Chilton . 2019 . Metapho - ria : An algorithmic companion for metaphor cre - ation . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , pages 1 – 12 . Seraphina Goldfarb - Tarrant , Tuhin Chakrabarty , Ralph Weischedel , and Nanyun Peng . 2020 . Content plan - ning for neural story generation with aristotelian rescoring . arXiv preprint arXiv : 2009 . 09870 . Hongyu Gong , Kshitij Gupta , Akriti Jain , and Suma Bhat . 2020 . Illinimet : Illinois system for metaphor detection with contextual and linguistic information . In Proceedings of the Second Workshop on Figura - tive Language Processing , pages 146 – 153 . Raquel Hervás , Rui P Costa , Hugo Costa , Pablo Gervás , and Francisco C Pereira . 2007 . Enrichment of automatically generated texts using metaphor . In Mexican International Conference on Artiﬁcial Intel - ligence , pages 944 – 954 . Springer . Ari Holtzman , Jan Buys , Maxwell Forbes , Antoine Bosselut , David Golub , and Yejin Choi . 2018 . Learning to write with cooperative discriminators . arXiv preprint arXiv : 1805 . 06087 . Dirk Hovy , Shashank Srivastava , Sujay Kumar Jauhar , Mrinmaya Sachan , Kartik Goyal , Huying Li , Whit - ney Sanders , and Eduard Hovy . 2013 . Identifying metaphorical word use with tree kernels . In Pro - ceedings of the First Workshop on Metaphor in NLP , pages 52 – 57 . Arthur M Jacobs . 2018 . The gutenberg english poetry corpus : exemplary quantitative narrative analyses . Frontiers in Digital Humanities , 5 : 5 . Beata Beigman Klebanov , Ben Leong , Michael Heil - man , and Michael Flor . 2014 . Different texts , same metaphors : Unigrams and beyond . In Proceedings of the Second Workshop on Metaphor in NLP , pages 11 – 17 . Maximilian Köper and Sabine Schulte im Walde . 2016 . Distinguishing literal and non - literal usage of ger - man particle verbs . In Proceedings of the 2016 con - ference of the north American chapter of the associa - tion for computational linguistics : Human language technologies , pages 353 – 362 . Mike Lewis , Yinhan Liu , Naman Goyal , Mar - jan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer . 2019 . Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension . arXiv preprint arXiv : 1910 . 13461 . Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Man - dar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019 . Roberta : A robustly optimized bert pretraining ap - proach . arXiv preprint arXiv : 1907 . 11692 . Rui Mao , Chenghua Lin , and Frank Guerin . 2019 . End - to - end sequential metaphor identiﬁcation inspired by linguistic theories . In Proceedings of the 57th An - nual Meeting of the Association for Computational Linguistics , pages 3888 – 3898 . James H Martin . 2006 . A corpus - based analysis of con - text effects on metaphor comprehension . Zachary J Mason . 2004 . Cormet : a computational , corpus - based conventional metaphor extraction sys - tem . Computational linguistics , 30 ( 1 ) : 23 – 44 . Saif Mohammad , Ekaterina Shutova , and Peter Tur - ney . 2016 . Metaphor as a medium for emotion : An empirical study . In Proceedings of the Fifth Joint Conference on Lexical and Computational Seman - tics , pages 23 – 33 , Berlin , Germany . Association for Computational Linguistics . Jekaterina Novikova , Ondˇrej Dušek , Amanda Cer - cas Curry , and Verena Rieser . 2017 . Why we need new evaluation metrics for NLG . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2241 – 2252 , Copenhagen , Denmark . Association for Computa - tional Linguistics . Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli . 2019 . fairseq : A fast , extensi - ble toolkit for sequence modeling . arXiv preprint arXiv : 1904 . 01038 . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic eval - uation of machine translation . In Proceedings of the 40th annual meeting on association for compu - tational linguistics , pages 311 – 318 . Association for Computational Linguistics . Jeffrey Pennington , Richard Socher , and Christopher D Manning . 2014 . Glove : Global vectors for word rep - resentation . In Proceedings of the 2014 conference on empirical methods in natural language process - ing ( EMNLP ) , pages 1532 – 1543 . Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 2018 . Deep contextualized word repre - sentations . arXiv preprint arXiv : 1802 . 05365 . Nils Reimers and Iryna Gurevych . 2019 . Sentence - bert : Sentence embeddings using siamese bert - networks . arXiv preprint arXiv : 1908 . 10084 . Elena Semino et al . 2007 . Mip : a method for identifying metaphorically used words in discourse . Metaphor and symbol , 22 ( 1 ) : 1 – 39 . Emily Sheng , Kai - Wei Chang , Prem Natarajan , and Nanyun Peng . 2019 . The woman worked as a babysitter : On biases in language generation . In Pro - ceedings of the 2019 Conference on Empirical Meth - ods in Natural Language Processing and the 9th In - ternational Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3398 – 3403 . Ekaterina Shutova , Douwe Kiela , and Jean Maillard . 2016 . Black holes and white rabbits : Metaphor iden - tiﬁcation with visual features . In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Hu - man Language Technologies , pages 160 – 170 . Ekaterina Shutova , Lin Sun , and Anna Korhonen . 2010 . Metaphor identiﬁcation using verb and noun cluster - ing . In Proceedings of the 23rd International Con - ference on Computational Linguistics ( Coling 2010 ) , pages 1002 – 1010 . Robyn Speer , Joshua Chin , and Catherine Havasi . 2017 . Conceptnet 5 . 5 : An open multilingual graph of gen - eral knowledge . In Thirty - First AAAI Conference on Artiﬁcial Intelligence . Gerard Steen . 2010 . A method for linguistic metaphor identiﬁcation : From MIP to MIPVU , volume 14 . John Benjamins Publishing . Kevin Stowe , Sarah Moeller , Laura Michaelis , and Martha Palmer . 2019 . Linguistic analysis improves neural metaphor detection . In Proceedings of the 23rd Conference on Computational Natural Lan - guage Learning ( CoNLL ) , pages 362 – 371 . Kevin Stowe and Martha Palmer . 2018 . Leveraging syntactic constructions for metaphor identiﬁcation . In Proceedings of the Workshop on Figurative Lan - guage Processing , pages 17 – 26 . Kevin Stowe , Leonardo Ribeiro , and Iryna Gurevych . 2020 . Metaphoric paraphrase generation . arXiv preprint arXiv : 2002 . 12854 . Chuandong Su , Fumiyo Fukumoto , Xiaoxi Huang , Jiyi Li , Rongbo Wang , and Zhiqun Chen . 2020 . Deep - met : A reading comprehension paradigm for token - level metaphor detection . In Proceedings of the Sec - ond Workshop on Figurative Language Processing , pages 30 – 39 . Serra Sinem Tekiro˘glu , Gözde Özbal , and Carlo Strap - parava . 2015 . Exploring sensorial features for metaphor identiﬁcation . In Proceedings of the Third Workshop on Metaphor in NLP , pages 31 – 39 . Asuka Terai and Masanori Nakagawa . 2010 . A compu - tational system of metaphor generation with evalua - tion mechanism . In International Conference on Ar - tiﬁcial Neural Networks , pages 142 – 147 . Springer . Yulia Tsvetkov , Leonid Boytsov , Anatole Gershman , Eric Nyberg , and Chris Dyer . 2014 . Metaphor detec - tion with cross - lingual model transfer . In Proceed - ings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Pa - pers ) , pages 248 – 258 . Tony Veale , Ekaterina Shutova , and Beata Beigman Klebanov . 2016 . Metaphor : A computational per - spective . Synthesis Lectures on Human Language Technologies , 9 ( 1 ) : 1 – 160 . Eric Wallace , Shi Feng , Nikhil Kandpal , Matt Gardner , and Sameer Singh . 2019 . Universal adversarial trig - gers for attacking and analyzing nlp . arXiv preprint arXiv : 1908 . 07125 . Yorick Wilks . 1975 . A preferential , pattern - seeking , se - mantics for natural language inference . Artiﬁcial in - telligence , 6 ( 1 ) : 53 – 74 . Yorick Wilks . 1978 . Making preferences more active . Artiﬁcial intelligence , 11 ( 3 ) : 197 – 223 . Zhiwei Yu and Xiaojun Wan . 2019 . How to avoid sen - tences spelling boring ? towards a neural approach to unsupervised metaphor generation . In Proceed - ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin - guistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 861 – 871 , Minneapo - lis , Minnesota . Association for Computational Lin - guistics . Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi . 2019 . Bertscore : Eval - uating text generation with bert . arXiv preprint arXiv : 1904 . 09675 . A Appendix For retrieving commonsense symbolism of the sen - tences , we use the pre - trained COMET model 3 and retrieve top 5 candidates for each input . 3 https : / / github . com / atcbosselut / comet - commonsense 1 . No of Parameters : For metaphor detection at token level we use BERT - base - cased model ( 110M ) . For generation we use the BART large checkpoint ( 400M parameters ) and use the implementation by FAIRSEQ ( Ott et al . , 2019 ) 4 . For discriminative decoding we use RoBERTa large model ( 355M ) 2 . No of Epochs : For metaphor detection at to - ken level for parallel data creation we ﬁne - tune it for 3 epochs . We ﬁne - tune pre - trained BART for 70 epochs for MERMAID model and save best model based on validation perplexity . For discriminator we ﬁne - tune RoBERTa - large model for 10 epoch and save the checkpoint for best validation accuracy 3 . Training Time : For metaphor detection train - ing time is 40 minutes . Our training time is 280 minutes for BART . For discriminator we train it for 60 minutes 4 . Hardware Conﬁguration : We use 4 RTX 2080 GPU 5 . Training Hyper parameters : We use the same parameters mentioned in the github repo where BART was ﬁne - tuned for CNN - DM summarization task with the exception of MAX - TOKENS ( size of each mini - batch , in terms of the number of tokens . ) being 1024 for us . For discrminator ﬁnetuning of roberta we use same parameters as RTE task 5 6 . Decoding Strategy & Hyper Parame - ters : For decoding we generate metaphors from our models using a top - k random sam - pling scheme ( Fan et al . , 2018 ) . At each timestep , the model generates the probabil - ity of each word in the vocabulary being the likely next word . We randomly sample from the k = 5 most likely candidates from this dis - tribution . 4 https : / / github . com / pytorch / fairseq / tree / master / examples / bart 5 https : / / github . com / pytorch / fairseq / blob / master / examples / roberta / README . glue . md