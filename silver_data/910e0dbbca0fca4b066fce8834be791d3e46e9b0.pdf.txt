Fairness Certification for Natural Language Processing and Large Language Models Vincent Freiberger 1 and Erik Buchmann 2 Dept . of Computer Science , Leipzig University , Germany 1 , 2 Center for Scalable Data Analytics and Artificial Intelligence ( ScaDS . AI ) Dresden / Leipzig , Germany 1 , 2 freiberger @ cs . uni - leipzig . de 1 buchmann @ informatik . uni - leipzig . de 2 Abstract . Natural Language Processing ( NLP ) plays an important role in our daily lives , particularly due to the enormous progress of Large Lan - guage Models ( LLM ) . However , NLP has many fairness - critical use cases , e . g . , as an expert system in recruitment or as an LLM - based tutor in ed - ucation . Since NLP is based on human language , potentially harmful biases can diffuse into NLP systems and produce unfair results , discrim - inate against minorities or generate legal issues . Hence , it is important to develop a fairness certification for NLP approaches . We follow a qualitative research approach towards a fairness certifica - tion for NLP . In particular , we have reviewed a large body of literature on algorithmic fairness , and we have conducted semi - structured expert interviews with a wide range of experts from that area . We have system - atically devised six fairness criteria for NLP , which can be further refined into 18 sub - categories . Our criteria offer a foundation for operationaliz - ing and testing processes to certify fairness , both from the perspective of the auditor and the audited organization . Keywords : Fairness , Certification , NLP 1 Introduction Fairness is important for Natural Language Processing ( NLP ) approaches : NLP is used in high - stakes contexts such as healthcare [ 1 ] . It is also integrated into daily - use technologies , e . g . , voice assistants like Amazon Alexa [ 2 ] or AI - based chatbots like ChatGPT [ 3 ] . A lack of fairness often materializes as allocative or representational harm [ 4 ] for marginalized groups [ 5 ] . An example of allocative harm is a resume filtering system , that prefers male applicants [ 6 ] . Representa - tional harm would be a translation app that translates to gender stereotypes [ 7 ] , cultural stereotypes or demeaning language [ 6 , 8 , 9 ] . To avoid harm , a fair NLP application must not only resist gender bias [ 10 – 13 ] , but also ableist [ 14 ] , eth - nical [ 11 , 15 , 16 ] , age - related [ 17 ] , religion - related [ 18 ] or sexuality bias [ 11 , 19 ] . Performant NLP models tend to be opaque and complex [ 20 , 21 ] . Interactions with such sociotechnical systems are typically also complex [ 22 ] . Hence , it is not practically achievable for users or affected individuals to verify fairness . Fairness certification could be embraced to reduce information asymmetries [ 23 ] . a r X i v : 2401 . 01262v1 [ c s . C L ] 2 J a n 2024 2 Freiberger et al . The concern of this paper is to develop a broad set of criteria , that can be used by an auditor to assess and certify the fairness of an approach that makes use of NLP , be it a large language model , a recruiting - tool or an AI - chatbot for teaching . This is challenging : First , many different definitions of fairness exist [ 24 , 25 ] , and some of them are contradictory [ 26 , 27 ] . Second , it is still unclear yet , which fairness criteria are important for NLP approaches and how they impact each other . For instance , residual unfairness may remain after bias mitigation has been resolved [ 28 ] . Third , efforts towards fair NLP and certifying fairness of related AI approaches exist [ 29 – 34 ] . But there is neither an established nor a holistic framework on how fairness could be certified and what could be audited [ 35 , 36 ] . To approach certification criteria that can be applied in practice , we need to consider the challenges of professionals tasked with auditing the fairness of a system , as well as the challenges of developers that encounter fairness issues when creating NLP systems . Thus , our research question is as follows : What criteria are relevant to consider for fairness certification for NLP ap - proaches from a practitioner’s point of view ? To approach our research question , we strive for broad , qualitative research . We have decided to derive a basic set of auditable fairness criteria from litera - ture on NLP , AI fairness and AI certification . Based on this set of criteria , we have developed a concept for a semi - structured interview with stakeholders from business and research . We have analyzed the interview transcripts in order to find out ( a ) which measures for ensuring fairness need to be considered for NLP approaches , and ( b ) how they influence each other . We make four contributions : – We outline and structure an extensive , up - to - date body of research literature on NLP fairness , AI fairness and fairness auditing from the last years . – We describe our qualitative research approach , which is based on a series of semi - structured interviews with 14 experts from various areas related to NLP and algorithmic fairness . – We devise a hierarchical coding scheme for the certification of fairness for NLP approaches . – We provide an overview of the six main criteria , with 18 sub - criteria on the second hierarchy level for the auditing of NLP approaches , which we have devised from the interview transcripts . To the best of our knowledge , we are the first to devise a holistic , hierar - chical coding scheme for the fairness certification of NLP approaches , which is backed up by literature and expert interviews . Our findings allow to develop a fairness certification for a wide range of NLP applications , including large lan - guage models and other text - generating AI approaches . Fairness is related to trust [ 37 ] and reduces information asymmetries [ 23 ] . Thus , our results help to establish a selling point for consumers , particularly marginalized groups . From a legal perspective , a certification can be a precautionary measure [ 34 ] . Paper structure : Section 2 reviews related work and gives a theoretical background . Section 3 outlines our research method . The Sections 4 and 5 eval - uate and discuss our findings . Finally , Section 6 concludes . Fairness Certification 3 2 Related Work In this section , we review existing literature on the Natural Language Processing ( NLP ) workflow and its modes of use . We provide an overview of fairness in Artificial Intelligence ( AI ) approaches on which NLP builds . Finally , we describe existing certification approaches for such AI approaches . 2 . 1 Natural Language Processing NLP makes human language computable [ 38 ] , and can be distinguished into Nat - ural Language Understanding ( NLU ) and Natural Language Generation ( NLG ) [ 38 ] . NLU refers to a machine’s comprehension of human language and “extracting valuable information for downstream tasks” [ 38 ] . Text summarization [ 39 , 40 ] , intend recognition [ 40 ] , machine translation [ 40 ] , named entity recognition [ 41 ] , sentiment analysis [ 42 ] or text classification [ 40 ] are examples for NLU . NLG refers to producing human - understandable text or speech in natural languages [ 43 ] . This is done by predicting the next token in a sequence of words , based on data sources like graphics , video , text , or audio . Prominent examples are AI - based Large Language Models ( LLMs ) like ChatGPT [ 3 ] . Modern NLP is based on language models that allow the creation of word embeddings [ 40 , 44 ] in the semantic space of a language . Building an NLP model starts with selecting the corpus , a “collection of linguistic data , either compiled from written texts or transcribed from recorded speech” [ 41 ] . Models are typi - cally large - scale Deep Learning models , which are pretrained on very large sets of text data [ 40 ] . Word embeddings resulting from language models can also be utilized as features for downstream NLP models [ 10 ] . Evaluation is typically performed on specific benchmark data sets for the field [ 40 ] . 2 . 2 Artificial Intelligence Fairness Fairness can be defined as the equally performant treatment of all humans by AI , without discriminating against any individuals or communities [ 45 ] . The whole life cycle of AI needs to be considered , as fairness problems can occur at all stages [ 46 ] . We employ the Cross Industry Standard Process for Data Mining ( CRISP - DM ) [ 47 ] when referring to the life cycle of AI . We chose a broad and generalist definition of fairness to encapsulate different conceptualizations of fairness brought up by interview partners ( cf . Sec . 4 ) . Fairness and its perception are context - dependent and communally derived [ 48 , 49 ] . For measuring fairness , numerous metrics have been discussed in literature [ 24 , 27 , 50 ] , some of which even ( partially ) contradict each other . Assessing fairness is pursued on different levels . This necessitates differenti - ating between individual fairness and ( sub ) group fairness , which in themselves have substantial normative differences [ 25 , 51 ] . Bias [ 25 ] is often brought up when fairness is discussed , even if it is not necessarily correlated to fairness [ 52 ] . Bias is a “dynamic and social and not [ just ] a statistical issue” [ 46 ] . Mitigating biases can be handled during preprocessing , inprocessing or postprocessing [ 53 , 54 ] in 4 Freiberger et al . a software life cycle . Preprocessing captures the data processing before infer - ence . Inprocessing targets the inference process itself . Postprocessing addresses operations performed after inference . To understand how biases impact fairness , understanding in what ways they are harmful to which specific groups is required [ 4 ] . Biases can result in discrim - ination , i . e . , in differences in the predictive power of models for an individual based on membership in different protected groups [ 55 , 56 ] . It may occur directly by utilizing protected attributes such as gender , race , disability , or sexuality . It can also be indirectly , via correlations to excluded protected attributes [ 25 ] . These attributes may overlap for certain social groups , causing multi - dimensional discrimination like intersectionality [ 57 ] . Different social groups use language differently [ 58 ] . NLP approaches tend to perform worse for marginalized groups because their usage of language is underrepresented [ 5 ] . Language also transmits beliefs about social groups and imposes labels on them , representing their societal position [ 4 ] . Language use reflects power relationships and social discrimination [ 11 ] . Biases in human lan - guage are surfaced in word embeddings [ 10 , 11 ] . This is problematic , because word embeddings are used for downstream NLP models , and biases diffuse into these models [ 11 ] . There they can cause allocative or representational harm , and must be targeted by debiasing [ 10 , 59 ] . 2 . 3 Artificial Intelligence Certification According to ISO / IEC 17000 : 2020 [ 60 ] , certification is a “third party attestation ( · · · ) related to an object of conformity assessment” [ 60 ] . The object of confor - mity assessment refers to an entity to which specific needs and expectations are put into place [ 60 ] . A certification authority performs a provider - independent audit with comprehensive checks to assess conformity with certification criteria , standards , or performance claims [ 35 , 61 , 62 ] . Third - party auditors are “indepen - dent organizations or individuals with no obligation or contractual relationship to the audit target” [ 35 ] . A certification communicates information , signals qual - ity to the system user , and assurance raises trust [ 62 , 63 ] . This is important in high - stakes domains [ 64 ] . It helps providers to show their legitimacy and to improve their services and products [ 34 ] . Certifications are defined by content , source , and process elements [ 63 , 64 ] . Content captures the specific subject of assessments in an audit . The source describes the institution providing accredi - tation . Process describes how an assessment for certification is conducted . For AI auditing , use - case - specific approaches have been suggested [ 35 ] . Com - mon auditing standards do not exist [ 29 , 35 ] yet . This is problematic , as such systems are increasingly deployed in high - stakes domains [ 29 ] . Current fairness audits [ 30 , 32 , 33 ] focus on quantitative aspects . Such audits evaluate system outputs according to mathematical fairness definitions without procedural or qualitative assessment , and without considering intersectionality [ 35 ] . This does not necessarily result in a fair system . Fairness Certification 5 3 Our Research Method Because fairness certification of NLP approaches [ 35 , 65 ] is a novel , barely de - fined research topic ( cf . Sec . 2 ) , we approach our research question with semi - structured expert interviews . Such interviews bring consistency , focus , and struc - ture to the interview while offering room for improvisation [ 65 , 66 ] . In particular , we follow Myers’ qualitative research methodology [ 65 ] and Corbin & Strauss’ approach to data analysis [ 66 , 67 ] . Our research method consist of the steps literature review , interview guide , interviewee selection , conducting and transcribing the interviews , coding the in - terview transcripts , annotation and reflection and refinement of the interviews . In the following , we explain these 8 steps . Literature Review : This step provides us with all the information nec - essary to develop an interview guide and to interpret the interview results . In particular , we need an overview of NLP , AI fairness , and certification ( cf . Sec . 2 ) . Interview Guide : On the basis of the literature review , we developed an interview guide with open questions ( see Appendix 3 ) . Our guide structures the interview into 8 parts . In the first part , ( 1 ) we welcome our interviewees and ( 2 ) explain the organization of the interview . Next , ( 3 ) we capture the professional background of the interviewee , and ( 4 ) clarify the terminology for the interview . Then ( 5 ) we ask for criteria for NLP development and ( 6 ) for sustaining NLP fairness over time . After that , ( 7 ) we leave room for open topics to talk about . Finally , ( 8 ) we thank our interviewee and conclude the interview . Interviewee Selection : Our research question calls for practically appli - cable research results drawn from the entire life cycle of NLP approaches . To investigate relevant criteria for fairness certification , we decided to select inter - viewees as follows : The interviewees should be working in the private sector on NLP in a relevant role like data scientist , consultant , or manager . They should have a minimum of two years of industry experience in NLP and , ideally , expo - sure to fairness in AI . To broaden our findings [ 65 , 68 ] , we wanted to gain diverse experts in the dimensions of age , gender , and cultural background . To identify matching candidates , we browsed LinkedIn and Xing as well as the authors’ professional networks . Matching candidates received a flyer with basic information on the research project and the interview guide in advance , to let them judge whether they could provide value and reflect on the topic . On this basis , we selected 12 interviewees . In addition , we made an exception to include two more interviewees with a background in academia . These inter - viewees were involved in algorithmic audits . Both had more than three years of experience in algorithmic fairness . Table 1 gives an overview of the interview partners in the order in which they were interviewed . The first column of the table contains the ID of the interview partner . In the following , we will use the ID to relate a statement to a person . The second and third columns describe the role of the interviewee and the business area of the interviewee’s company . The last column contains the interviewee’s professional expertise . Conducting the Interviews : We conducted one interview in English and 13 in German , all of them via video conferences . The interviews followed the 6 Freiberger et al . Table 1 . Overview of interview partners ID Profession Affiliation Expertise I 1 CTO Regional , AI Solutions NLU , whole life cycle I 2 Data Scientist International , IT Consulting Wide variety in NLP , whole life cycle I 3 Data Scientist National , IT Services Wide variety in NLP , whole life cycle I 4 Team Lead NLP National , AI and Research Wide variety in NLP I 5 CTO National , Conversational AI Solutions NLG I 6 Head of NLP National , IT Services NLG I 7 Data Scientist International , IT NLU , search query handling I 8 Associate Vice President AI International , IT Consulting Wide variety in NLP I 9 Senior Program Manager International , IT Conversational AI , ASR I 10 Consultant Multinational , IT Consulting NLU I 11 Tech Lead NLP International , IT Consulting Wide variety in NLP I 12 Research Scientist National , Research Algorithmic fairness I 13 Co - Founder National , IT Services NLU , focus : preprocessing I 14 Senior Applied Scientist Multinational , E - Commerce Algorithmic fairness , certification interview guide , as explained in the second step . During the main section of the interview , we generally encouraged further input on topics verbally and non - verbally by active listening , open questions , and reflecting back to the intervie - wee [ 65 ] . The average duration of all 14 interviews was 51 minutes . Transcribing the Interviews : We transcribed all interviews manually , fol - lowing the guidelines of Dresing and Pehl [ 69 ] . We adapted the guidelines by leaving out timestamps , which were not needed . To ensure correctness , we asked the interviewees to check our transcripts . Coding the Transcriptions : To gain insight from the transcripts , we de - veloped a coding scheme in an iterative approach based on open and axial cod - ing [ 67 ] . Open coding allows us to freely name concepts represented in interview data . We use axial coding to understand concepts’ context , cause , and con - sequence . We identify concepts for the coding scheme by interviewees naming them explicitly , by abstracting from specific ideas to concepts , or by identifying examples for a concept . Fairness Certification 7 Fig . 1 . Top - level codes for the fairness certification of NLP approaches In advance of our results in Sec . 4 , Figure 1 shows the main criteria identified in our coding scheme . On this level , the coding scheme consists of six criteria , that are relevant for NLP certification . The scheme is divided into the auditing organization and the audited one . Governance Criteria are relevant for the certification process for both of them , from opposite points of view . The general characteristics and aims of the certification are explained by Process Criteria . Finally , Data - Related Criteria , Project Planning Criteria , Modeling & Evaluation Criteria and Operations Criteria specify processes and contents of the audited organization . Note that coding is an iterative process , i . e . , Figure 1 illustrates the last version of dynamic refinement . Annotation and Reflection : We annotated 1162 text passages , utilizing memos to capture the central ideas and concepts represented in the passage . After multiple iterations over the scheme , 1095 coded passages contained 587 overall codes , with 124 belonging to open coding and 473 to axial coding . Refinement : Conducting interviews is a dynamic process . After conduct - ing and analyzing our first four interviews , we adapted our interview guide . In particular , we worked on our questions to simplify the interview process and to avoid unnecessary follow - up questions . Furthermore , we learned that our broad approach puts a lot of time pressure on the interviewer , which does not allow us to ask in - depth questions . Thus , we decided to ask our interviewees about prior experiences and thoughts on fairness certification first . This allows us to skip topics in the subsequent interview , that the interviewees did not feel qualified enough to talk about . 8 Freiberger et al . 4 Interview Findings This section gives an overview of all criteria for NLP certification that we have identified . Figure 2 illustrates our hierarchy of codes and sub - codes . Note that we further subdivided the sub - codes , which we omit here for lack of space . Ap - pendix B contains a more detailed description of our results . Fig . 2 . Mind map of the coding scheme for the fairness certification of NLP approaches In the following , we explain the coding of our six criteria together with their sub - criteria captured by sub - codes . The intervieweees who addressed them are denoted in brackets . Criteria that were mentioned by many interviewees tend to be more important for a fairness certification of NLP approaches . Process Criteria refer to the implementation of a fairness certification pro - cess . Our interviewees identified three distinguished aspects of process criteria , which we modeled as sub - codes . The sub - code Fairness Understanding captures properties of fairness concep - tion , that should be considered for fairness certification according to our inter - views . Examples include cultural dependence ( I 9 , I 13 , I 14 ) , use case dependence ( I 1 , I 3 , I 5 , I 6 , I 7 , I 8 , I 12 , I 13 , I 14 ) , dynamic ( I 13 , I 14 ) and non - binary ( I 12 ) prop - erties . It also covers the interviewees’ conceptions of what is to be considered fair , which is nuanced , and varies between conceptions like conditional statistical parity ( I 1 , I 4 , I 6 , I 7 ) , counterfactual fairness ( I 3 , I 4 , I 5 , I 10 , I 13 ) and notions of inclusiveness ( I 2 , I 4 , I 7 , I 8 , I 9 , I 11 ) . The sub - code Certification Market Factors covers how a certification and regulation may affect each other ( I 1 , I 3 , I 4 , I 6 , I 14 ) , the market which may build around certification ( I 1 ) and the adoption of certification dependent on alignment with corporate goals ( I 1 , I 10 , I 11 , I 13 , I 14 ) . In Design of Assessment , we capture properties for an assessment and its scope . A requirement is finding a way to create an audit process that is as holistic as possible ( I 1 , I 6 , I 8 , I 9 , I 11 , I 12 , I 13 , I 14 ) . A certification shouldn’t put small companies at a disadvantage ( I 3 , I 5 , I 6 , I 11 ) . Interviewees discuss the approach to certification . They propose , for instance , to certify individuals Fairness Certification 9 instead of processes ( I 5 , I 6 , I 8 ) . Building a certification on existing standards and best practices is also agreed upon ( I 1 , I 5 , I 8 , I 10 , I 11 , I 13 , I 14 ) . Another major point of discussion was to what extent a certification process should be risk - dependent in its scope ( I 1 , I 2 , I 9 , I 14 ) or in its obligation to be performed ( I 2 , I 10 , I 11 ) . Governance Criteria refer to all measures the certified organization should undertake to ensure the integrity of its processes and management . Recall that governance is important both from the perspective of the auditing and the au - dited organization . We have identified two sub - codes for governance criteria , which encompass several aspects . The sub - code Model Reporting & Transparency follows the idea of disclosing biases , basic information , data understanding , model explanations , and evalua - tion . For basic information , our interviewees want fields of application ( I 4 , I 13 ) , the underlying fairness definition , and the frameworks the system was built on ( I 4 ) to be disclosed . Data understanding covers all relevant information about the data the system utilizes and how it is processed ( I 4 , I 12 , I 13 ) . Model explanations cover disclosing the model with its architecture ( I 4 , I 8 , I 12 ) and explainability of the model ( I 2 , I 3 , I 4 , I 6 , I 8 , I 12 ) . For evaluation , interviewees bring up that reporting should contain what has been tested regarding the robustness of the model against biases ( I 2 , I 4 ) . Evaluation metrics and their resulting data from testing should be publicly documented ( I 4 , I 8 ) . Reporting data bias ( I 4 ) , model bias ( I 3 ) , and countermeasures against biases that are implemented ( I 3 , I 13 ) should also be done . As disclosure mechanisms , our interviewees suggest utiliz - ing Model Cards ( I 2 , I 3 , I 4 , I 8 , I 12 , I 13 , I 14 ) , open sourcing the model entirely ( I 8 ) or giving the users an option to access relevant information to the prediction the model made for them on demand ( I 8 ) . The sub - code Organizational Criteria targets both the audited and the au - diting organization . Interviewees highlight that the context of the auditing or - ganization should be reflected because it can impede its ability to make neutral and holistic assessments . This involves the region it is located in ( I 13 ) , the po - litical system it is integrated into ( I 13 ) , its initiator ( I 13 ) , its integration in the economic system ( I 2 , I 10 ) , or the lack in diversity of professional backgrounds of its auditors ( I 6 , I 9 ) . The auditing organization consolidates best practices ( I 2 , I 6 , I 9 , I 12 ) . For the audited organization the diversity of the development team ( I 2 , I 4 , I 10 , I 11 , I 13 ) , internal accountability mechanisms and employee qualifi - cation and training should be checked . Internal accountability involves checking for internal assessments regarding fairness that are put in place by the audited organization ( I 7 , I 14 ) . Roles should be assigned clear responsibilities regarding fairness ( I 1 , I 2 , I 3 , I 6 , I 7 , I 9 , I 10 , I 11 , I 14 ) . Interviewees mention that account - ability also needs someone who is responsible for fairness in the organization and supervision targeted at fairness ( I 3 , I 5 , I 6 , I 7 , I 11 ) . Employee qualification & training are seen by interviewees to provide a reasonable job fit for employees ( I 3 , I 6 ) . A holistic overview of the process of training and operating an NLP sys - tem and what could go wrong regarding fairness ( I 6 , I 8 ) should also be provided . 10 Freiberger et al . Moreover , the interviews reveal the importance of raising general awareness of bias issues and ensuring general knowledge about biases ( I 1 , I 3 , I 4 , I 6 , I 7 , I 11 ) . Project Planning Criteria address processes in the planning stage of a product and its underlying business problem that the certification institution should assess . Their scope ends right before data understanding in the CRISP - DM framework . This involves two sub - codes . For sub - code Define & Assess the Planned Application , the interviewees agree about checking definitions of use case ( I 4 , I 8 , I 9 , I 12 , I 13 , I 14 ) and stakeholders ( I 6 , I 7 , I 8 , I 11 ) provided by the audited organization . This entails an assessment for the latter regarding fairness issues or vulnerabilities to fairness risks . This involves checking that mechanisms are in place to properly understand users and stakeholders , as well as the fairness properties of the use case itself . Moreover , the planned solution should fit the use case , also from a fairness perspective ( I 3 , I 4 , I 5 , I 12 ) . The sub - code Fairness Targets in Requirements implies checking if a standardized set of requirements has been established to ensure fairness regarding the project over its life cycle ( I 2 , I 4 , I 9 ) . With Data - related Criteria , we refer to all assessment processes regarding data used for training and retraining as well as regarding procedures for handling and transforming data . Those procedures are described by six sub - codes . The sub - code Data Assessment comprises an assessment of data quality re - garding fairness by checking representativeness via distribution checks ( I 1 , I 4 , I 8 , I 9 , I 13 ) and by assessing regarding harmful , toxic or incorrect data ( I 2 , I 4 , I 10 , I 13 , I 14 ) . Interviewees suggest introducing processes for bias checks ( I 1 , I 2 , I 4 , I 5 , I 7 , I 9 , I 11 ) and assessments of the sourcing ( I 1 , I 2 , I 4 , I 5 , I 6 , I 8 , I 12 , I 13 ) and collection ( I 3 , I 12 , I 14 ) of data . Sub - code Annotation captures characteris - tics of the annotation process to support fairness . Annotating fairness relevant attributes ( I 1 , I 2 , I 11 ) is suggested by interviewees . Moreover , comprehensive an - notation guidelines ( I 4 , I 6 , I 7 ) and inter - annotator agreements ( I 4 , I 7 , I 10 , I 13 ) are important considerations . Sub - code Preprocessing focuses on requirements for filtering ( I 3 , I 4 , I 13 ) , selecting ( I 1 , I 4 , I 7 , I 8 , I 10 , I 12 , I 13 ) , anonymizing ( I 3 , I 5 , I 7 , I 14 ) or mapping the data robustly ( I 2 ) to ensure fairness . The sub - code Data for Evaluation entails that suitable , representative data is used ( I 2 , I 9 , I 11 ) and discusses where to take it from . Interviewees highlight the importance of fairness invariance testing ( I 2 , I 3 ) . Sub - code Data for Continuous Improvement comprises a monitoring process during the operation of the model and a feed - back loop targeting improvements and counteracting fairness issues . As criteria for monitoring , interviewees mention fairness test sets ( I 2 , I 3 , I 6 , I 8 , I 9 ) , drift monitoring ( I 2 , I 3 , I 4 , I 5 , I 6 , I 7 , I 13 ) , as well as a request assessment for under - represented groups ( I 2 ) as practices to be implemented by the audited company . Feedback loop practices regarding data need to consider how in - use data like user behavior , usage data or prediction confidence ( I 5 , I 9 , I 10 ) and user - made correc - tions ( I 4 ) are handled to improve or maintain fairness . Finally , the sub - code Data Storage covers how data should be stored in development and operations ( I 10 ) and cached in operations ( I 1 ) . Fairness Certification 11 The Modeling & Evaluation Criteria include the two sub - codes Modeling and Evaluation , which can be explained as follows : Modeling is about embedding fairness into the model architecture itself . In - terviewees mention integrating hard - coded elements into the system to ensure fairness ( I 4 , I 5 , I 6 , I 14 ) , training a separate fairness classifier for the model out - puts ( I 5 ) , working with constraints ( I 2 ) or embedding fairness into the model’s optimization itself ( I 4 , I 5 , I 7 , I 10 ) . Evaluation investigates what tests should be conducted to evaluate a model and how to assess the fairness of a trained model . At the center of this intervie - wees discuss functional testing criteria which take an outside perspective on the system’s outputs given some specific inputs . Interview partners came up with six concepts relevant to this which are validation of predictions ( I 1 , I 4 , I 6 ) , involving affected stakeholders in evaluation ( I 5 ) , adversarial testing ( I 1 , I 4 , I 7 ) , data sets or benchmarks for testing ( I 2 , I 3 , I 6 , I 8 , I 9 , I 11 , I 13 ) , criteria focused on human - computer interaction ( I 2 , I 7 , I 11 ) and ethics criteria ( I 4 , I 11 ) . Another important consideration for evaluation is what metrics should be utilized to test a system’s fairness . Interviewees name metrics focused on robustness or generalization as an essential element to measure the model’s likelihood of generating unexpected , unfair results ( I 2 , I 4 , I 8 ) . Impact - based metrics depend on the chosen fairness paradigm ( I 3 , I 7 , I 8 , I 12 , I 14 ) . Another approach proposed by I 10 introduces a metric that penalizes the model’s use of sensitive attributes or unethical content when calculating loss . Operations Criteria center around what should be considered regarding fairness when deploying a model and what mechanisms must be implemented to maintain fairness over time . That subsumes three sub - codes . Sub - code Deployment centers around assessments suited to shipping differ - ent model sizes and pruning models , which affect their fairness behavior ( I 2 , I 3 , I 6 ) . Interviewees define the sub - code Monitoring as a continuous assessment procedure of the audited company . This is to ensure its system’s fairness over time , involving fairness tests with the roll - out of updates ( I 1 , I 2 , I 3 , I 7 ) and intervention strategies to counteract adversarial or unintentional , but harmful , misuse ( I 9 , I 10 , I 13 ) . Finally , with the sub - code Feedback Loop , interviewees aim to get the flow of information back from the operational model assessed . This involves checking for the option of flagging system outputs that are perceived as unfair by users and for an option for users to make corrections ( I 1 , I 2 , I 3 , I 7 ) . Interviewees consider leveraging the data which is acquired by that to under - stand fairness issues ( I 1 , I 2 , I 3 , I 7 ) and , in some cases , trigger retraining ( I 1 , I 3 , I 4 ) . An assessment includes checking if data utilized for retraining the system in continuous improvement is subjected to the same assessments , filtering and preprocessing steps as the initial data ( I 10 ) . Table 2 provides an overview of which of our interviewees addressed which ( sub - ) code . A comparison of this table with Table 1 indicates that we indeed obtained a broad range of experts , which are likely to cover the entire range of the fairness certification process for NLP approaches . 12 Freiberger et al . Table 2 . Second - level coding scheme for fairness certification of NLP approaches I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 ProcessCriteria FairnessUnderstanding ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Design of Assessment ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Certification Market Factors ✓ ✓ ProjectPlanningCriteria Define & Assess the Planned Application ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Fairness Targets in Requirements ✓ ✓ ✓ Data - related Criteria Data Assessment ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Annotation ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Preprocessing ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Data for Evaluation ✓ ✓ ✓ ✓ ✓ ✓ ✓ Data for Continuous Improvement ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Data Storage ✓ ✓ Modeling & EvaluationCriteria Modeling ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Evaluation ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ OperationsCriteria Deployment ✓ ✓ ✓ Monitoring ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Feedback Loop ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GovernanceCriteria Model Reporting & Transparency ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ OrganizationalCriteria ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 5 Discussion In this section , we briefly discuss our results . We were interested to learn the en - tire range of criteria that must be considered to establish fairness certification for an NLP approach . To this end , we have conducted and coded 14 semi - structured interviews with a wide span of diverse experts from business and research . Thus , we think that our findings are well applicable to certifying fairness in a corpo - rate environment . However , we did not cover other sectors , such as public or military . Furthermore , we might not have reached theoretical saturation with 14 interviewees . Moreover , designing and testing a fairness certification process itself was beyond our scope . One might wonder if this work on fairness and bias was influenced by bias itself . We explicitly tried to exclude the following biases : Selection bias ( selecting interview partners on personal preferences ) , bias in materials ( providing docu - Fairness Certification 13 ments before the interviews could have influenced the interviewees ) , verbal / non - verbal bias ( due to misunderstandings between interviewer and interviewee ) and bias in data analysis ( subjective coding ) . We observed , that our interviewees focused particularly on criteria for data and functional testing of solutions . However , there is a bias in the relevance perception of modeling between NLP fairness research and interviewees’ per - spectives . Existing research already considers model architectures inhibiting so - cial biases , for instance , via regularization [ 70 – 72 ] , adversarial training [ 6 , 72 ] or adapting the loss function to support fairness [ 73 , 74 ] . Even though approaches mentioned in interviews are consistent with the literature , the topic was barely mentioned or deemed relatively unimportant in interviews ( I 1 , I 5 ) . Future re - search may investigate this mismatch . 6 Conclusion Fairness certification for natural language processing approaches such as large language models , AI - based chatbots or healthcare applications is an important issue , that is still unresolved . In this paper , we have conducted and analyzed 14 semi - structured expert interviews with mostly NLP experts in the industry and two algorithmic fairness experts in academia . Our interviewees helped us identify six main criteria and 18 criteria on the second hierarchy level of an open coding scheme for certifying the fairness of an NLP approach . Those criteria are an important building block towards operationalizing and testing NLP processes to certify fairness , from the perspective of the auditor as well as from the perspective of the audited organization . Our interviewees have raised plenty of open questions for future research . For instance : How should a certification process handle the use case dependence of fairness or its non - binary nature and subjectiveness while in a dynamic en - vironment ? To what extent would it make sense to make such a certification mandatory ? On which best practices and standards should a certification be built ? How extensive should it be ? Finally , how must a certification process be structured to specifically address large language models ? References 1 . A . Wong , J . M . Plasek , S . P . Montecalvo , and L . Zhou , “Natural language process - ing and its implications for the future of medication safety : A narrative review of recent advances and challenges , ” Pharmacotherapy : The Journal of Human Phar - macology and Drug Therapy , vol . 38 , no . 8 , pp . 822 – 841 , 2018 . 2 . I . Lopatovska , K . Rink , I . Knight , K . Raines , K . Cosenza , H . Williams , P . Sorsche , D . Hirsch , Q . Li , and A . Martinez , “Talk to me : Exploring user interactions with the amazon alexa , ” Journal of Librarianship and Information Science , vol . 51 , no . 4 , pp . 984 – 997 , 2019 . 3 . OpenAI , “ ChatGPT [ large language model ] , ” https : / / chat . openai . com , 2023 . 14 Freiberger et al . 4 . S . L . Blodgett , S . Barocas , H . Daum´e III , and H . Wallach , “Language ( technology ) is power : A critical survey of “bias” in nlp , ” in Proceedings of the 58th annual meeting of the association for computational linguistics , 2020 , pp . 5454 – 5476 . 5 . N . Markl , “Language variation and algorithmic bias : understanding algorithmic bias in British English automatic speech recognition , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 521 – 534 . 6 . T . Sun , A . Gaut , S . Tang , Y . Huang , M . ElSherief , J . Zhao et al . , “Mitigating gender bias in natural language processing : Literature review , ” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019 , pp . 1630 – 1640 . 7 . G . Stanovsky , N . A . Smith , and L . Zettlemoyer , “Evaluating gender bias in machine translation , ” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019 , pp . 1679 – 1684 . 8 . A . Caliskan , J . J . Bryson , and A . Narayanan , “Semantics derived automatically from language corpora contain human - like biases , ” Science , vol . 356 , no . 6334 , pp . 183 – 186 , 2017 . 9 . L . Weidinger , J . Uesato , M . Rauh , C . Griffin , P . - S . Huang , J . Mellor et al . , “Tax - onomy of risks posed by language models , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 214 – 229 . 10 . T . Bolukbasi , K . - W . Chang , J . Y . Zou , V . Saligrama , and A . T . Kalai , “Man is to computer programmer as woman is to homemaker ? debiasing word embeddings , ” in Advances in neural information processing systems , 2016 , pp . 4349 – 4357 . 11 . O . Papakyriakopoulos , S . Hegelich , J . C . M . Serrano , and F . Marco , “Bias in word embeddings , ” in Proceedings of the 2020 conference on fairness , accountability , and transparency , 2020 , pp . 446 – 457 . 12 . R . Tatman , “Gender and dialect bias in YouTube’s automatic captions , ” in Pro - ceedings of the first ACL workshop on ethics in natural language processing , 2017 , pp . 53 – 59 . 13 . A . Ovalle , P . Goyal , J . Dhamala , Z . Jaggers , K . - W . Chang , A . Galstyan , R . Zemel , and R . Gupta , ““i’m fully who i am” : Towards centering transgender and non - binary voices to measure biases in open language generation , ” in Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency , 2023 , p . 1246 – 1266 . 14 . S . Hassan Awadallah , M . Huenerfauth , and C . O . Alm , “Unpacking the interde - pendent systems of discrimination : Ableist bias in nlp systems through an intersec - tional lens , ” in Findings of the Association for Computational Linguistics : EMNLP 2021 , 2021 , pp . 3116 – 3123 . 15 . B . Bridgeman , C . Trapani , and Y . Attali , “Comparison of human and machine scoring of essays : Differences by gender , ethnicity , and country , ” Applied Measure - ment in Education , vol . 25 , no . 1 , pp . 27 – 40 , 2012 . 16 . S . L . Blodgett and B . O’Connor , “Racial disparity in natural language pro - cessing : A case study of social media african - american english , ” arXiv preprint arXiv : 1707 . 00061 , 2017 . 17 . M . Diaz , I . Johnson , A . Lazar , A . M . Piper , and D . Gergle , “Addressing age - related bias in sentiment analysis , ” in Proceedings of the 2018 chi conference on human factors in computing systems , 2018 , pp . 1 – 14 . 18 . T . Brown , B . Mann , N . Ryder , M . Subbiah , J . Kaplan , P . Dhariwal , A . Neelakan - tan , P . Shyam , G . Sastry , A . Askell et al . , “Language models are few - shot learners , ” Advances in neural information processing systems , vol . 33 , pp . 1877 – 1901 , 2020 . Fairness Certification 15 19 . S . Garg , V . Perot , N . Limtiaco , A . Taly , E . H . Chi , and A . Beutel , “Counterfac - tual fairness in text classification through robustness , ” in Proceedings of the 2019 AAAI / ACM Conference on AI , Ethics , and Society , 2019 , pp . 219 – 226 . 20 . M . Danilevsky , K . Qian , R . Aharonov , Y . Katsis , B . Kawas , and P . Sen , “A survey on bias and fairness in machine learning , ” arXiv preprint arXiv : 2010 . 00711 , 2020 . 21 . B . Lepri , N . Oliver , E . Letouz´e , A . Pentland , and P . Vinck , “Fair , transparent , and accountable algorithmic decision - making processes , ” Philosophy & Technology , vol . 31 , no . 4 , pp . 611 – 627 , 2018 . 22 . A . Chouldechova and A . Roth , “A snapshot of the frontiers of fairness in machine learning , ” Communications of the ACM , vol . 63 , no . 5 , pp . 82 – 89 , 2020 . 23 . P . Cihon , M . J . Kleinaltenkamp , J . Schuett , and S . D . Baum , “Ai certification : Ad - vancing ethical practice by reducing information asymmetries , ” IEEE Transactions on Technology and Society , vol . 2 , no . 4 , pp . 200 – 209 , 2021 . 24 . S . Verma and J . Rubin , “Fairness definitions explained , ” in 2018 ACM / IEEE In - ternational Workshop on Software Fairness , 2018 , pp . 1 – 7 . 25 . N . Mehrabi , F . Morstatter , N . Saxena , K . Lerman , and A . Galstyan , “A survey on bias and fairness in machine learning , ” ACM Computing Surveys , vol . 54 , no . 6 , pp . 1 – 35 , 2021 . 26 . A . Chouldechova , “Fair prediction with disparate impact : A study of bias in re - cidivism prediction instruments , ” Big Data , vol . 5 , no . 2 , pp . 153 – 163 , 2017 . 27 . M . Defrance and T . De Bie , “Maximal fairness , ” in Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency , 2023 , p . 851 – 880 . 28 . N . Kallus and A . Zhou , “Residual unfairness in fair machine learning from preju - diced data , ” in International Conference on Machine Learning . PMLR , 2018 , pp . 2439 – 2448 . 29 . I . D . Raji , A . Smart , R . N . White , M . Mitchell , T . Gebru , B . Hutchinson , J . Smith - Loud , D . Theron , and P . Barnes , “Closing the ai accountability gap , ” in Proceedings of the 2020 conference on fairness , accountability , and transparency , 2020 , pp . 33 – 44 . 30 . P . Adler , C . Falk , S . Friedler , T . Nix , G . Rybeck , C . Scheidegger , B . Smith , and S . Venkatasubramanian , “Auditing black - box models for indirect influence , ” Knowledge and Information Systems , vol . 54 , no . 1 , pp . 95 – 122 , 2018 . 31 . C . Dwork , M . Hardt , T . Pitassi , O . Reingold , and R . Zemel , “Fairness through awareness , ” in Proceedings of the 3rd innovations in theoretical computer science conference , 2012 , pp . 214 – 226 . 32 . S . Park , S . Kim , and Y . Lim , “Fairness audit of machine learning models with confidential computing , ” in Proceedings of the ACM Web Conference 2022 , 2022 , pp . 3488 – 3499 . 33 . S . Segal , Y . Adi , B . Pinkas , C . Baum , C . Ganesh , and J . Keshet , “Fairness in the eyes of the data : Certifying machine - learning models , ” in Proceedings of the 2021 AAAI / ACM Conference on AI , Ethics , and Society , 2021 , pp . 926 – 935 . 34 . R . N . Landers and T . S . Behrend , “Auditing the ai auditors : A framework for evalu - ating fairness and bias in high stakes ai predictive models , ” American Psychologist , 2022 . 35 . S . Costanza - Chock , I . D . Raji , and J . Buolamwini , “Who audits the auditors ? recommendations from a field scan of the algorithmic auditing ecosystem , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 1571 – 1583 . 36 . F . Petersen , D . Mukherjee , Y . Sun , and M . Yurochkin , “Post - processing for indi - vidual fairness , ” in Advances in Neural Information Processing Systems , vol . 34 , 2021 , pp . 25 944 – 25 955 . 16 Freiberger et al . 37 . C . Starke , J . Baleis , B . Keller , and F . Marcinkowski , “Fairness perceptions of algorithmic decision - making : A systematic review of the empirical literature , ” Big Data & Society , vol . 9 , no . 2 , 2022 . 38 . Y . Kang , Z . Cai , C . - W . Tan , Q . Huang , and H . Liu , “Natural language process - ing ( nlp ) in management research : A literature review , ” Journal of Management Analytics , vol . 7 , no . 2 , pp . 139 – 172 , 2020 . 39 . E . D . Liddy , “Natural language processing , ” Encyclopedia of Library and Informa - tion Science , vol . 2126 , p . 2140 , 2001 . 40 . D . D . Otter , J . Medina , and J . Kalita , “A survey of the usages of deep learn - ing for natural language processing , ” IEEE Transactions on Neural Networks and Learning Systems , vol . 32 , no . 2 , pp . 604 – 624 , 2021 . 41 . D . Khurana , A . Koli , K . Khatter , and S . Singh , “Natural language processing : State of the art , current trends and challenges , ” Multimedia Tools and Applications , pp . 1 – 32 , 2022 . 42 . J . Yi , T . Nasukawa , R . Bunescu , and W . Niblack , “Sentiment analyzer : Extracting sentiments about a given topic using natural language processing techniques , ” in Third IEEE international conference on data mining , 2003 , pp . 427 – 434 . 43 . D . McDonald , “Natural language generation , ” Handbook of Natural Language Pro - cessing , vol . 2 , pp . 121 – 144 , 2010 . 44 . A . Matthews , I . Grasso , C . Mahoney , Y . Chen , E . Wali , T . Middleton et al . , “Gen - der bias in natural language processing across human languages , ” in Proceedings of the First Workshop on Trustworthy Natural Language Processing , 2021 , pp . 45 – 54 . 45 . M . Ashok , R . Madan , A . Joha , and U . Sivarajah , “Ethical framework for arti - ficial intelligence and digital technologies , ” International Journal of Information Management , vol . 62 , no . 2 , p . 102433 , 2022 . 46 . E . Ntoutsi , P . Fafalios , U . Gadiraju , V . Iosifidis , W . Nejdl , M . - E . Vidal et al . , “Bias in data - driven artificial intelligence systems—an introductory survey , ” Wiley Interdisciplinary Reviews : Data Mining and Knowledge Discovery , vol . 10 , no . 3 , pp . 1 – 14 , 2020 . 47 . R . Wirth and J . Hipp , “CRISP - DM : Towards a standard process model for data mining , ” in Proceedings of the 4th international conference on the practical appli - cations of knowledge discovery and data mining , vol . 1 , 2000 , pp . 29 – 39 . 48 . M . Skirpan and M . Gorelick , “The authority of “fair” in machine learning , ” arXiv preprint arXiv : 1706 . 09976 , 2017 . 49 . A . Schmidt and M . Wiegand , “A survey on hate speech detection using natural language processing , ” in Proceedings of the fifth international workshop on natural language processing for social media , 2017 , pp . 1 – 10 . 50 . A . Z . Jacobs , S . L . Blodgett , S . Barocas , H . Daum´e III , and H . Wallach , “The meaning and measurement of bias : Lessons from natural language processing , ” in Proceedings of the 2020 conference on fairness , accountability , and transparency , 2020 , p . 706 . 51 . R . Binns , “On the apparent conflict between individual and group fairness , ” in Proceedings of the 2020 conference on fairness , accountability , and transparency , 2020 , pp . 514 – 524 . 52 . L . Cabello , A . K . Jørgensen , and A . Søgaard , “On the independence of association bias and empirical fairness in language models , ” in Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency , 2023 , p . 370 – 378 . 53 . R . Bellamy , K . Dey , M . Hind , S . Hoffman , S . Houde , K . Kannan , P . Lohia , J . Mar - tino , S . Mehta , A . Mojsilovic et al . , “Ai fairness 360 : An extensible toolkit for detecting and mitigating algorithmic bias , ” IBM Journal of Research and Devel - opment , vol . 63 , no . 4 / 5 , pp . 4 – 1 , 2019 . Fairness Certification 17 54 . S . A . Friedler , C . Scheidegger , S . Venkatasubramanian , S . Choudhary , E . P . Hamil - ton , and D . Roth , “A comparative study of fairness - enhancing interventions in ma - chine learning , ” in Proceedings of the conference on fairness , accountability , and transparency , 2019 , pp . 329 – 338 . 55 . I . Chen , F . D . Johansson , and D . Sontag , “Why is my classifier discriminatory ? ” in Advances in neural information processing systems , 2018 , pp . 3543 – 3554 . 56 . F . Calmon , D . Wei , B . Vinzamuri , K . N . Ramamurthy , and K . R . Varshney , “Op - timized pre - processing for discrimination prevention , ” in Proceedings of the 31st international conference on neural information processing systems , 2017 , pp . 3995 – 4004 . 57 . A . Roy , J . Horstmann , and E . Ntoutsi , “Multi - dimensional discrimination in law and machine learning - a comparative overview , ” in Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency , 2023 , p . 89 – 100 . 58 . C . Harris , M . Halevy , A . Howard , A . Bruckman , and D . Yang , “Exploring the role of grammar and word choice in bias toward african american english ( aae ) in hate speech classification , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 789 – 798 . 59 . J . Chen , I . Berlot - Attwell , X . Wang , S . T . Hossain , and F . Rudzicz , “Exploring text specific and blackbox fairness algorithms in multimodal clinical nlp , ” in Proceedings of the 3rd Clinical Natural Language Processing Workshop , 2020 , pp . 301 – 312 . 60 . I . 17000 : 2020 , Conformity assessment – Vocabulary and general principles . ISO , 2020 . 61 . IEEE , “Ieee standard for software reviews and audits , ” IEEE Std , vol . 1028 , pp . 1 – 53 , 2008 . 62 . S . Lins , T . Kromat , J . L¨obbers , A . Benlian , and A . Sunyaev , “Why don’t you join in ? a typology of information system certification adopters , ” Decision Sciences , vol . 53 , no . 3 , pp . 452 – 485 , 2022 . 63 . J . Lansing , A . Benlian , and A . Sunyaev , ““unblackboxing” decision makers’ inter - pretations of is certifications in the context of cloud service certifications , ” Journal of the Association for Information Systems , vol . 19 , no . 11 , pp . 1064 – 1096 , 2018 . 64 . N . Scharowski , M . Benk , S . J . K¨uhne , L . Wettstein , and F . Br¨uhlmann , “Certifica - tion labels for trustworthy ai : Insights from an empirical mixed - method study , ” in FAccT ’23 : Proceedings of the 2023 ACM Conference on Fairness , Accountability , and Transparency , 2023 , pp . 248 – 260 . 65 . M . D . Myers , Qualitative research in business & management . Sage Publications Limited , 2020 . 66 . J . Corbin and A . Strauss , Basics of qualitative research techniques and procedures for developing grounded theory . Sage publications , 2015 . 67 . —— , “Grounded theory research : Procedures , canons , and evaluative criteria , ” Qualitative sociology , vol . 13 , no . 1 , pp . 3 – 21 , 1990 . 68 . M . Jakesch , Z . Bu¸cinca , S . Amershi , and A . Olteanu , “How different groups pri - oritize ethical values for responsible ai , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 310 – 323 . 69 . T . Dresing and T . Pehl , Praxisbuch interview , transkription & analyse : Anleitungen und regelsysteme f¨ur qualitativ forschende . Dr . Dresing & Pehl GmbH , 2018 . 70 . T . Kamishima , S . Akaho , H . Asoh , and J . Sakuma , “Fairness - aware classifier with prejudice remover regularizer , ” in Machine Learning and Knowledge Discovery in Databases . Springer , 2012 , pp . 35 – 50 . 71 . M . Veale and R . Binns , “Fairer machine learning in the real world : Mitigating discrimination without collecting sensitive data , ” Big Data & Society , vol . 4 , no . 2 , p . 2053951717743530 , 2017 . 18 Freiberger et al . 72 . M . Yurochkin and Y . Sun , “SenSeI : Sensitive set invariance for enforcing individual fairness , ” arXiv preprint arXiv : 2006 . 14168 , 2020 . 73 . C . Dwork , N . Immorlica , A . T . Kalai , and M . Leiserson , “Decoupled classifiers for group - fair and efficient machine learning , ” in Conference on fairness , accountability and transparency , 2018 , pp . 119 – 133 . 74 . F . Kamiran , A . Karim , and X . Zhang , “Decision theory for discrimination - aware classification , ” in 2012 IEEE 12th International Conference on Data Mining , 2012 , pp . 924 – 929 . 75 . S . Corbett - Davies , E . Pierson , A . Feller , S . Goel , and A . Huq , “Algorithmic de - cision making and the cost of fairness , ” in Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining , 2017 , pp . 797 – 806 . 76 . M . J . Kusner , J . Loftus , C . Russell , and R . Silva , “Counterfactual fairness , ” in Proceedings of the 31st international conference on neural information processing systems , 2017 , pp . 4066 – 4076 . 77 . M . Mitchell , S . Wu , A . Zaldivar , P . Barnes , L . Vasserman , B . Hutchinson , E . Spitzer , I . D . Raji , and T . Gebru , “Model cards for model reporting , ” in Pro - ceedings of the conference on fairness , accountability , and transparency , 2019 , pp . 220 – 229 . 78 . T . Wolf , L . Debut , V . Sanh , J . Chaumond , C . Delangue , A . Moi et al . , “Hugging - face’s transformers : State - of - the - art natural language processing , ” arXiv preprint arXiv : 1910 . 03771 , 2019 . 79 . M . Pushkarna , A . Zaldivar , and O . Kjartansson , “Data cards : Purposeful and trans - parent dataset documentation for responsible AI , ” in 2022 ACM Conference on Fairness , Accountability , and Transparency , 2022 , pp . 1776 – 1826 . A Interview Guide Table 3 : Interview guide on criteria for fairness certification of NLP approaches Min Minacc . Goal Contents and exemplary questions Expectedresults 7 7 Buffer , wel - come Recent projects , travel , . . . relaxed start 3 10 Sort out or - ganizationalmatters Obtain consent for recording , outline objective of work , outline process of interview , outline anonymization and tools used GDPR , research goal and proce - dure clear 5 15 Understandbackgroundofintervie - wee In which step of the AI lifecycle are you primarily in contact with NLP systems ? What points of contact have you had with fairness so far ? What incidents or issues have there been with it ? What contact points might there have been about certification ? Contact points to topic , profes - sional practice of the intervie - wee 3 18 Gettingstartedwiththetopic What does fairness of AI or NLP mean to you ? How would you define fairness of NLP ? What are your initial thoughts on the topic of fair - ness certification of AI ? And NLP specifically ? Terminologyclarified , entry point Fairness Certification 19 10 28 Criteria for NLP system develop - ment Which requirements should be made to processes in development to ensure fairness ? Why ? ( Business understanding , data understanding , preprocessing , annotation , modeling , explainability , evaluation ) What are the organizational requirements that should be considered for fairness certification of NLP ? Why ? ( Team , structure ) What must suppliers of software components adhere to in order to maintain fairness of the end product ? Do you see any other issues in the NLP systems development phase that have not yet been raised ? What are the biggest challenges for NLP providers ? Where are the opportunities ? Answers to questions for de - velopment crite - ria ; contextual understanding 10 38 Criteria for NLP system operation Are there any aspects regarding deployment that should be considered ? How does an AI system need to be monitored af - ter it goes live to detect fairness issues that arise ? Who needs to be involved and how is the monitoring organized ? How often and how extensively do you think fairness problems should be checked ? How might such an audit be conducted ? What are the key issues that need special attention ? What processes should be initiated in case of fair - ness violations ? How can improvements be made ? To what extent is the interaction of users with the system to be controlled ? Why ? What interactions of the user with the system have to be considered ? What are opportunities and challenges for NLP providers ? Understandinghowtomaintainfairnessinanoperationalsystemandwhythisisso , and how this can be captured in criteria 5 43 Clear out open topics Are there any other criteria apart from the ones dis - cussed so far that you would use for fairness certifi - cation in NLP ? What else is important to you to say about fairness certification in NLP ? Additionaldimensions 2 45 Pointingoutnextsteps Describe follow - up . Do you know anyone else who could help me with an interview ? Further points of contact B Detailed Interview Findings B . 1 Certification Criteria Certification criteria target everything generally relevant to establishing a relevant fairness certification process . Interviewees describe concepts that can be clustered in the categories of fairness understanding , design of assessment , and certification market factors . Fairness Understanding Having a good understanding of the fairness properties for the given application can be seen as an essential foundation for building an effective certification process ( I 12 , I 14 ) . Findings follow the structure of codes identified in open coding , as introduced in the previous chapter . It makes sense to keep perspective on the findings by first introducing subjective fairness understandings of interviewees . Some tended to focus on an equal opportunity aspect , others on equal performance or other conceptualization . The different perspectives of interviewees on fairness also may mani - fest in their suggestions for certification measures . Interviewees one , four , six , and seven share the fairness perspective of Fairness through Awareness ( I 1 , I 4 , I 6 , I 7 ) which aims 20 Freiberger et al . at providing similar outcomes to similar individuals [ 31 ] . This similarity establishes fair treatment on a meritocratic baseline ( I 14 ) . Hence , factors that are objectively relevant to a decision , like skills , should be the baseline for a decision . Conditional statistical parity [ 75 ] could also be utilized as a suitable fairness conception from the literature . A form of enablement that may account for socioeconomic backgrounds , like migration ( I 14 ) , could also be considered ( I 14 ) . That would allow for the reduction of structural inequalities like gender discrimination in our society ( I 6 ) . The target could be changing these structural inequalities by actively intervening ( I 1 , I 4 , I 7 ) . This concept could be captured by statistical parity , where the demographics of groups that receive a positive ( or negative ) classification meet the demographics of the overall population [ 31 ] . This conception , however , comes with the limitation that a group’s representation in the overall population needs to be large enough to be considered . Treatment based on a meritocratic baseline is closely related to the notion of a non - discriminatory system . It avoids an unjustifiable negative impact on affected individuals ( I 3 , I 5 ) and is how interviewees three , four , five , ten , and 13 view fairness ( I 3 , I 4 , I 5 , I 10 , I 13 ) . It closely resembles the literature’s counterfactual fairness definition , which emphasizes decision invariance to a protected attribute [ 76 ] . It may depend on the use case in its context when utilizing a protected attribute to justify negative impact may be acceptable ( I 3 , I 5 ) . As mentioned by interviewees two , four , seven , eight , nine , and eleven , inclusive - ness ties closely into this notion ( I 2 , I 4 , I 7 , I 8 , I 9 , I 11 ) . It means a system should not be restricted in accessibility and usability , and there should not be a barrier to usage by a group ( I 2 , I 11 ) . Ideally , the system also represents a diverse spectrum and avoids echo chambers ( I 4 , I 7 , I 8 ) . There can also be trade - offs between fairness dimensions . For instance , striving for inclusiveness can impair fairness regarding counterfactual fair - ness in a recruitment context ( I 1 ) . Interviewees two , four , nine , and eleven emphasize equal performance as their subjective fairness understanding ( I 2 , I 4 , I 9 , I 11 ) . It focuses on delivering the same performance to each user independent of group membership . The system can cope , for instance , with the variance of expression without sacrific - ing performance for certain groups ( I 9 ) . It may even be interpreted as an equal user experience ( I 9 ) . Literature also employs the mathematical fairness definition of coun - terfactual fairness to capture this conception of fairness [ 76 ] . For some use cases like voice assistants , this fairness conception can be even business - critical due to user di - versity ( I 2 , I 9 ) . This subjective nature makes it more difficult to clearly define fairness in a certification process ( I 1 , I 3 , I 5 , I 12 ) . Defining fairness before an assessment is an important factor for operationalizing a fairness certification ( I 5 , I 8 , I 9 , I 12 , I 14 ) . An - other challenge in defining fairness for an assessment is rooted in the strong dependence on context imposed by the use case ( I 1 , I 3 , I 5 , I 6 , I 7 , I 8 , I 12 , I 13 , I 14 ) . This use case dependence is particularly highlighted by interviewees three , twelve , and 14 . The use case may , for instance , determine whether procedural or distributional fairness should be the focus ( I 14 ) . Required criteria for designing content for an assessment may vastly differ between fairness conceptions ( I 6 , I 14 ) . Some interviewees see the large variance in use cases as a reason for calibrating an assessment to the specific use case by adapting its contents to the fairness situation the application imposes ( I 6 , I 7 , I 11 , I 14 ) . It may make sense to employ different fairness definitions depending on the affected stake - holders and context for the certification process , as discussed in “Project Planning Criteria” for the audited company , and adapt the certification process accordingly ( I 5 , I 8 ) . Moreover , a dynamic environment implies changing fairness perceptions over time , so fairness definitions for stakeholders may need to be reassessed ( I 13 , I 14 ) . Ethics and moral considerations are closely related to fairness ( I 8 , I 9 , I 10 , I 14 ) . A fairness defi - nition can be grounded in the underlying definition of ethics ( I 8 ) . Interviewee eight Fairness Certification 21 quotes Joscha Bach to define ethics as a “ [ r ] ational process negotiation with yourself and others to decide how to resolve a conflict of interest under the condition of shared value” ( I 8 ) . Hence , grounding a fairness definition in ethics could cope with the varying fairness conceptions ( I 8 ) . An assessment should ideally not result in a binary outcome in the form of fair vs . not fair , but it should be more nuanced , describing to what degree which fairness concepts are met ( I 12 ) . Otherwise , it is difficult to determine the point when something starts to become unfair ( I 8 ) . It is crucial to view fairness as a societal issue as our fairness perception is culturally rooted in the same way some biases are ( I 6 , I 12 ) . Different values in another cultural context may cause a regional dependence of fairness ( I 9 , I 13 , I 14 ) . The fairness perception in China may not align with the typical European fairness understanding , for instance ( I 13 ) . This societal bias poses a challenge for assessing and operating models on a global scale ( I 13 ) . In practice , the topic of fairness in NLP systems is barely considered by most development teams ( I 1 ) . That partially leads to a lack of understanding and awareness of the problems that must be addressed in a certification process ( I 11 , I 14 ) . Besides a lack of practical understanding of creating a good fairness certification process , some fundamental in - sights from research to ground such a process are still missing ( I 14 ) . Fairness should not be viewed as an isolated concept as it is entangled with other variables of Respon - sible Artificial Intelligence ( RAI ) ( I 1 , I 7 , I 9 ) . Transparency , for instance , has already been addressed as a substantial part of a fairness certification . Data security , privacy , and reliability are also named as influencing factors on fairness ( I 1 , I 7 , I 9 ) . Improving such should not affect fairness negatively but rather benefit it , making an integrated assessment of fairness with other RAI variables interesting ( I 9 ) . Design of Assessment Interviewees also give insight into what they consider a suitable design of an assessment for certifying fairness . That includes general prop - erties of the certification process that the certifying institution should consider and information about the scope of an assessment . The hierarchical open coding scheme for “Design of Assessment” is displayed in Figure 3 . Fig . 3 . Criteria relevant to “Design of Assessment” hierarchically mapped Assessment Properties : Assessment properties describe how an assessment should be approached and what may be considered when creating such an assessment proce - dure . However , potential roadblocks need to be considered for an assessment to gain traction . First , the feasibility of such a holistic certification process that does not in - duce unfairness among industry players is questioned to a certain extent ( I 3 , I 5 , I 6 , 22 Freiberger et al . I 11 ) . Apart from the resource intensity of undergoing such a certification process which makes it inaccessible for small players ( I 1 , I 13 ) , there is the potential issue of only lim - ited access which can be granted to the auditor by the company ( I 12 , I 13 ) and the issue of complexity of the topic as well as the systems ( I 3 , I 5 ) which may make it unattain - able in the near future ( I 14 ) . A certification process cannot be seen as a guarantee that no bias issues will occur ( I 5 ) . A challenge to master for the auditor is to provide an assessment that is as objective as possible , which is difficult with humans involved ( I 13 ) . The approach may define how effective a certification process turns out to be . Currently , approaches to auditing the fairness of a system tend to be too selective and hence do not cover the entire range of problems that need to be addressed ( I 14 ) . Having an end - to - end approach covering the system holistically may be essential ( I 1 , I 6 , I 8 , I 9 , I 11 , I 12 , I 13 , I 14 ) . Also , hard - coded parts of the system should be investigated in an assessment as they sometimes can be decisive for the system’s outputs ( I 6 , I 9 ) . It may make sense to focus on an assessment regarding the most pressing fairness issues so that a system can operate without causing anyone considerable harm ( I 1 , I 2 , I 3 , I 10 ) . An approach that is partially practiced in the industry employs a human comparison to define the threshold for unfairness ( I 4 ) . It may be criticized as it does not reflect the target of systems’ being as fair as possible ( I 2 , I 4 ) . Another approach focuses more on certifying individuals involved in the systems development and operations than on processes established in an organization or data and reports generated by it ( I 5 , I 6 , I 8 ) . Interviewee eight describes the concept of quick checks , which is discussed further in the chapter “Criteria for Governance” , as such an employee - focused certification process may be more effective ( I 8 ) and may have a longer - lasting lifespan in ensuring fairness ( I 6 ) . Another perspective envisions a modular certification process where different en - deavors , like anonymizing data , are all certified individually ( I 5 ) . The process factors define specific nuances a certification process should consider and what is required for implementation . The assessment process should first differentiate between assessment for Business - to - Business ( B2B ) and assessment for Business - to - Consumer ( B2C ) use cases ( I 8 ) . In B2B use cases , different aspects like competitive fairness are relevant , which are irrelevant for B2C and the other way around ( I 4 , I 7 ) . An assessment should be built on standards , KPIs , and best practices ( I 1 , I 5 , I 8 , I 10 , I 11 , I 13 , I 14 ) . They help companies align their processes and offer clarity . Existing frameworks should be utilized to build a process on ( I 1 , I 8 , I 12 , I 13 ) . Currently , only internal certification processes are established in some companies , which generate Lessons Learned ( I 14 ) . The latter eventually translates over time into the development of an independent third - party assessment facilitated by an exchange of discovered working mechanisms between practitioners and in exchange with policymakers ( I 14 ) . Currently , the develop - ment of processes for auditing fairness is driven by assessing the practical applicability of research ( I 14 ) . When an independent third - party certification is first introduced , it will also manifest and improve with learnings out of experimentation and practical ap - plication ( I 1 , I 13 , I 14 ) . When standards are set and a consistent process is established , the reasoning for reaching the result of an assessment is to be addressed , as it should be grounded in a factual foundation ( I 13 ) . Transfer learning needs to be considered separately here as the provider of the unspecified pretrained baseline model and the provider of the custom model , which is then trained for the specific task , are involved ( I 2 , I 9 , I 11 ) . Interviewee nine argues that the certifications of both models should be kept distinct so that the custom model is certified independently of the ideally already certified baseline model ( I 9 ) . That may entail that only already certified baseline mod - els are safe and viable options when pursuing certification for one’s custom model ( I 11 ) . Reviewing the baseline model by oneself as a provider of a custom model is barely fea - Fairness Certification 23 sible ( I 11 ) . Some interviewees argue that with training the custom model ( I 2 , I 9 ) or due to the use case dependence of fairness ( I 1 ) , there is a responsibility shift regarding fairness from the provider of the baseline model to the provider of the custom model . That is because the data for training on the baseline model defines the final behavior of the model regarding fairness ( I 1 , I 2 , I 9 ) . Scope : Interviewee nine poses the critical question of where the scope of a certifica - tion process should end ( I 9 ) . Nearly all interviewees discuss the scope of a certification process to a certain extent ( I 1 , I 2 , I 3 , I 4 , I 5 , I 6 , I 7 , I 8 , I 9 , I 11 , I 12 , I 13 , I 14 ) . It may make sense to start basic first when introducing a certification process for fairness and grow it more nuanced and complex over time ( I 9 , I 14 ) . The generalization of the certification process is a major concern regarding its scope ( I 3 , I 5 , I 7 , I 8 , I 9 , I 14 ) . A certification should be as general as possible without sacrificing the ability to properly judge a use case ( I 3 , I 14 ) . There is a need to specify criteria depending on which use case is present ( I 3 , I 5 , I 8 , I 11 , I 12 , I 13 , I 14 ) or the geographical location the model is used ( I 9 ) . As an example , a hiring context and a context in the medical domain may be compared ( I 14 ) . In the hiring context , it makes sense to introduce criteria assessing the similarity of embeddings between genders in the semantic vector space . In the medical domain , the differences between genders may be valuable information to improve treatment specif - ically for each gender . Criteria should not favor semantic similarity there , but rather focus on the result , which is the success of the treatment for each gender . Clustering into similar use cases may be problematic as human perception of similarity may not align with what models represent ( I 3 ) , and there needs to be more clarity for assign - ing clusters ( I 14 ) . Another challenge when specifying criteria is determining what is to be considered relevant ( I 3 ) . The scope that should be certified may also vary in its thoroughness . That may depend on either the risk invoked by the system regarding unfairness ( I 1 , I 2 , I 6 , I 9 , I 14 ) or on the number of users affected by it ( I 3 ) . The risk of a system is typically rooted in its application and not just in the underlying technical system and hence may shift with the application context changing ( I 8 ) . For a risk - based differentiation between use cases , it may make sense to hierarchically increase the thoroughness and frequency of an assessment with the societal impact a system has ( I 6 , I 14 ) . For low - risk applications , interviewee 14 suggests that no external assessment may be required , and an internal assessment should be pursued by the company and documented ( I 14 ) . That way , internal product , and process expertise can be leveraged in an internal audit to make it more streamlined ( I 14 ) . However , an external assessment should be regularly required for high - risk applications ( I 14 ) . It may also make sense to require more transparency with increasing risk ( I 14 ) . To assess the risk of an applica - tion , systemic issues like gender discrimination should be considered . This may be done by looking at the potential severity of impact in isolation and the cumulated effect of combined negative impacts in the context of our societal system ( I 14 ) . Ultimately , it is a societal consideration how much emphasis is put on certification ( I 14 ) . To push forward fairness in the industry , some interviewees suggest making a certification at least in some cases obligatory as they fear lacking utilization of certification without an obligation ( I 2 , I 10 , I 11 ) . Particularly , large language models are mentioned to require a certification process ( I 4 ) . Others argue that certification should much rather provide guidelines based on which the application is certified without introducing an obligation ( I 3 , I 5 ) . Some interviewees suggest that it may make sense to certify a more extensive scope than NLP fairness ( I 8 , I 9 , I 14 ) . A certification for ML or even algorithmic fair - ness , in general , is suggested ( I 8 , I 9 ) , and an extension of the certification process to assess other RAI principles is proposed ( I 9 ) . 24 Freiberger et al . Certification Market Factors Market factors describe how a certification process would influence its major stakeholders to act economically and how the market envi - ronment is shaped . Currently , uncertainty is in the market as there are no established standards or best practices and regulatory uncertainty ( I 1 , I 14 ) . There is a reciprocal influence between a certification process and potential future regulation regarding sys - tems’ fairness . On one side , the prospect of coming regulation in the field incentivizes companies to prepare for future compliance by already undergoing certification ( I 1 , I 3 , I 14 ) . On the other hand , certification may guide regulation by introducing standards and giving direction to industry players ( I 4 , I 6 ) . An important factor for adopting a certification process lies in its alignment with the company’s business goals and viabil - ity ( I 3 , I 11 ) . The company first needs to accept the fairness criteria , which are assessed by the certification process ( I 9 ) . It also needs to see the potential benefits of a certifi - cation process . A fairness assessment process may make the company less vulnerable to potential lawsuits ( I 1 , I 7 ) , signal trustworthiness and reliability to the user ( I 8 , I 9 ) , and even drive innovations ( I 3 , I 9 ) . Fairness issues become increasingly critical ( I 9 ) , and certification may be an important selling point for a system ( I 1 , I 10 , I 11 , I 14 ) . The more business - critical fairness becomes , the higher the certification adoption rate ( I 13 ) . A whole market may develop around such a certification of fairness , as most companies will require external consulting to ensure compliance with established standards and certification requirements ( I 1 ) . B . 2 Governance Criteria Governance Criteria focus on governance aspects that impact a system’s fairness and should be assessed in a fairness certification process . A certification process may be beneficial as it can augment the company’s internal assessment procedures and provide feedback that is , in the long term , beneficial for the company ( I 7 ) . In the context of fairness , governance should establish a moral standard and a reciprocal fairness awareness , as users and providers should be able to judge fairness appropriately ( I 3 , I 4 , I 8 ) . This may be established by introducing a certification process , as it is currently barely considered in the industry ( I 10 ) . However , there may be one constraint regarding governance if a company is developing systems for customers who will use them for their products . The solution provider then needs to satisfy the customer’s wishes and hence is restricted regarding governance ( I 6 , I 10 ) . The interviews brought to attention two major concepts that need to be considered for governance : “Model Reporting & Transparency” and “Organizational Criteria” . Model Reporting & Transparency Providing model reporting and transparency focuses on reducing information asymmetries between the system’s provider and stake - holders by providing information about the application that may be relevant to stake - holders , particularly for identifying biases ( I 1 , I 3 , I 6 , I 11 , I 13 ) or avoiding the occur - rence of fairness issues . It can be seen as an accountability mechanism for the system’s provider ( I 1 , I 7 , I 13 ) . Model reporting may also provide essential information for an au - dit regarding fairness ( I 4 , I 13 ) . Criteria for model reporting are hierarchically mapped in Figure 4 . Transparency may be essential in maintaining a good reputation as a company ( I 7 ) . It may be limited by confidentiality requirements within the company ( I 13 ) . The relevance of good model reporting and transparency may be particularly high for API services ( I 2 ) . That is because the customer using the service or build - ing on it faces information asymmetry and only accesses the system via an interface Fairness Certification 25 Fig . 4 . Criteria relevant to “Model Reporting & Transparency” hierarchically mapped ( I 2 ) . The form in which transparency is provided may also vary . The ideal form of transparency in operations would be given with open - sourcing key components of the system ( I 8 ) . That enables stakeholders to improve components of the system as they can not just understand the system but also participate ( I 8 ) , which may be bene - ficial regarding fairness as affected groups actively can contribute . That way , their perspective might be better represented in the system’s continuous improvement . A lower degree of stakeholder access may be chosen with only information about the sys - tem and its inferences provided as another form of transparency in operations . Then there need to be considerations regarding the design of the interface providing such information as it should not negatively impact the system’s usability ( I 8 ) . Users may not all be interested in transparency about the system ( I 8 ) . Interviewee eight suggests making such information immediately available with the click of a button ( I 8 ) . An often - mentioned implementation of model reporting can be found in Model Cards or closely related conceptions ( I 2 , I 3 , I 4 , I 8 , I 12 , I 13 , I 14 ) . The paper introducing model cards [ 77 ] is mentioned and referred to by interviewees ( I 12 , I 13 , I 14 ) . A model card reports on model details , intended use , factors that may influence the model regarding fairness , metrics evaluation data , training data , quantitative analyses , ethical consid - erations , and caveats and recommendations [ 77 ] . The version of model cards provided by Hugging Face [ 78 ] is also mentioned in the context of providing transparency ( I 2 , I 3 ) . Interviewee eight suggests an extended and adapted concept ( I 8 ) which inspires an adapted version extended with some contents mentioned in this chapter and can be found in Table 5 in the Appendix . Introducing a similar concept to model cards leads to better - documented models and improved transparency , which are beneficial , as previously mentioned , not just for fairness ( I 3 , I 13 ) . Model cards can help by giv - ing direction to model users or even providers by raising awareness on fairness issues and potential risks of an application and starting a reflection process ( I 8 ) . The con - tent of disclosures in operations or model reporting can be clustered in biases , basic information , data understanding , model characteristics , and evaluation . Basic informa - tion should include information about the intended fields of application ( I 4 , I 13 ) , the underlying fairness definition , and the frameworks the system was built on ( I 4 ) . The latter may be important information for an audit , making the process reproducible ( I 4 ) . Moreover , information about the model’s generalizability in the domain should be 26 Freiberger et al . provided , giving an indication of model quality ( I 4 ) . Furthermore , it should be commu - nicated by the company based on what fairness definition it built the system and what the latter excludes ( I 12 ) . Data understanding covers all relevant information about the data the system utilizes and how it is processed . This is essential as NLP systems tend to be trained on vast corpora of sometimes even confidential data , which makes it hard to judge for users what fairness issues may potentially arise ( I 13 ) . Data understand - ing involves providing the context of data collection , as there might be incentives for unrepresentative data collection ( I 12 ) . Moreover , the data source ( I 12 ) , the volume of data ( I 4 ) as well as distributions ( I 12 ) should be made transparent . It should become clear what has been used as training data and what as evaluation data ( I 4 ) . Data split - ting and sampling should be described , and their representativeness stated ( I 4 ) . All data preprocessing steps should be described ( I 12 ) and a sample of the data may be provided ( I 13 ) . Transparency may also be important regarding model explanations ( I 1 , I 2 , I 3 , I 4 , I 6 , I 7 , I 8 , I 11 , I 12 ) . It should first be reported which models are utilized for the system ( I 4 , I 12 ) . It may also make sense to go deeper and document how the model trains itself on all layers ( I 8 ) . This ties into how a model learns certain things , which is addressed by explainability ( I 11 ) . Explainability should allow the user to understand the basis of the system’s decision and contextualize it ( I 1 , I 2 , I 3 , I 7 , I 8 ) . It may be essential to make explainability information transparent ( I 2 , I 3 , I 4 , I 6 , I 8 , I 12 ) . The users’ domain expertise in the specific field may help uncover particular fairness issues ( I 6 ) . Explainability enables a plausibility check by a human to see whether unwanted decision drivers may be involved that could indicate fairness issues ( I 2 , I 3 , I 4 ) . In some cases , explainability may also be a business requirement and needed anyway ( I 4 ) . How - ever , there may be a tradeoff in the quality of explainability feasible for a model and its performance . That is because large and complex models that make operationalizing explainability difficult tend to perform better ( I 2 , I 3 , I 4 ) . Smaller , interpretable mod - els may be a solution for certain tasks ( I 4 , I 6 ) , as discussed in the chapter “Project Planning Criteria” . Multiple explainability tools are utilized for black - box models for which a minimum standard that needs to be met may be specified in a certification ( I 2 ) . But it should consider the different suitability of certain tools depending on the use case ( I 2 ) . Mentioned tools that may be utilized include attention scores over all layers ( I 2 ) , integrated gradient score and token influence prediction ( I 2 , I 4 ) , and visu - alization ( I 3 , I 8 ) . The latter may be hard to quantify ( I 3 ) . Non - quantifiable measures involve a process of human sensemaking , which induces a bias ( I 3 ) . Because of that , quantifiable explainability measures should be preferred ( I 3 ) . The last consideration for reporting explainability is the comprehensiveness of the communicated information to non - expert users ( I 8 , I 12 ) . Just reporting very technical details on the model and its inference is probably useless for most users , and the addition of comprehensive expla - nations is required ( I 8 , I 12 ) . The steps taken for evaluation should also be reported . It should be reported what has been tested regarding the robustness of the model against biases ( I 2 , I 4 ) . There may be some form of scoring on standardized test sets targeting various biases and allowing for a benchmarking of the model , which can be reported ( I 2 , I 4 , I 13 ) . Evaluation metrics should be publicly documented and reported ( I 4 , I 8 ) . The reporting of biases covers data bias ( I 4 ) , model bias ( I 3 ) , and countermeasures against biases that are implemented ( I 3 , I 13 ) . If biases can’t be mitigated , there should be at least a hint to users of the system priming them for the potential occurrence of such ( I 4 , I 7 ) . Interviewee two also suggests testing on tasks not intended for the system but may give hints regarding bias issues that may also occur for the original task and hence should be reported ( I 2 ) . Fairness Certification 27 Organizational Criteria Organizational criteria revolve around general processes established in an organization , accountability - related aspects , and team - and qualification - related aspects . The findings of this work include organizational criteria for the audited and considerations for the auditing organization . The hierarchical structure of all men - tioned concepts is visualized in Figure 5 . Fig . 5 . “Organizational Criteria” hierarchically mapped Considerations for the auditing organization mainly focus on who is performing the assessment . When first establishing a certification process , it may make sense to collab - orate with independent auditing firms with expertise ( I 12 ) . It may also be important to consider the background of the certifying third party as the region it is located in ( I 13 ) , the political system it is integrated into ( I 13 ) , its initiator ( I 13 ) , or its integration in the economic system ( I 2 , I 10 ) influence it . Interviewee 13 suggests an international union like the OECD may be a good platform to build such a certification process ( I 13 ) . Ex - perts should be responsible for the certification assessment process and continuously consolidate and update best practices ( I 2 , I 6 , I 9 , I 12 ) . Regarding the background of those experts , interviewees suggest involving linguists ( I 9 ) , domain experts ( I 6 ) , data scientists and data engineers ( I 6 ) , and ethics or sociology experts ( I 6 , I 9 ) . Criteria for the Audited Organization : Organizational criteria for the audited or - ganization include employee qualification and training , internal accountability , and diversity of the development team . For employee qualification and training , the job fit of the employee is an important concept ( I 3 , I 6 ) . In a step like annotation , there can be the issue that due to cost reasons , typically , employees assigned to annotate are not on long - term contracts and hence may also lack the motivation to make a conscious effort to follow annotation guidelines ( I 6 ) . The latter , however , is crucial for fairness as it provides the foundation to train the model ( I 6 ) . Who is assigned what task may indicate how successfully fairness may be implemented ( I 6 ) . Hence , it would make sense to assess who is assigned what job and whether qualifications are sufficient to avoid 28 Freiberger et al . imposing fairness risks ( I 6 ) . The awareness for fairness needs to be raised , and an un - derstanding of bias issues needs to be present in the organization as there tends to be lacking expertise regarding fairness in development teams ( I 1 , I 3 , I 4 , I 6 , I 7 , I 11 ) . That needs to be done for the development team of an application by sensitizing to potential biases and training ( I 1 , I 3 , I 6 , I 7 ) . Again , annotators are highlighted as particularly rel - evant for such training ( I 3 , I 6 ) . Moreover , customer - centric teams should be trained in detecting and identifying biases ( I 11 ) . That may be achieved by establishing practices that make customer - centric teams build empathy with the affected individuals and change their perspective to understand the affected groups’ perceptions ( I 11 ) . Further - more , such practices may be essential when first introducing a certification process as they enhance the overall problem understanding , which in turn helps the certification process to evolve ( I 11 ) . The installment and contents of such training might be subject to an audit . Moreover , members of the development team should all have a rough holis - tic overview ( I 6 , I 8 ) . A possible measure supportive of this would be offering workshops that foster a holistic overview ( I 6 ) . It may be checked that every development team member is aware of their fairness impact on the product ( I 8 ) . Contextualizing such quick checks for different roles that each have a different perspective on the product may be a valuable approach to fostering fairness ( I 8 ) . Tech - focused team members may benefit from a quick check on data understanding and mitigation layers ( I 6 , I 8 ) . Anno - tators and non - tech - focused team members can be equipped with a system and process understanding ( I 6 , I 8 ) . That would shift the focus of the certification audit further onto the employees as their workflow is certified , similar to a checklist ( I 8 ) . For internal ac - countability , roles need to be assigned clear responsibilities , internal assessment or audit procedures should be established , and supervision and responsibility for fairness must be addressed . Starting with roles , establishing a fairness officer or team dedicated to fairness makes sense ( I 1 , I 2 , I 6 , I 7 , I 9 , I 10 , I 14 ) . That person or team should be respon - sible for the fairness definition in the specific use case ( I 1 , I 9 ) , conducting an internal fairness assessment ( I 1 , I 2 , I 6 ) and providing advice on fairness to the development team and supporting it , for instance in its decision making ( I 1 , I 7 , I 9 ) . The main issue that arises is that such an individual or team requires financial resources ( I 2 , I 11 , I 14 ) . There may also be an external assessor for fairness who reports to the company ( I 1 ) . The development team functions as a problem solver for fairness issues reported by one of the above assessors ( I 1 , I 3 , I 11 ) without being the responsible instance for detecting fairness issues ( I 1 , I 6 , I 11 ) . It may also make sense for certain use cases to implement different levels of clearance , for instance , when confronted with sensitive contents in a feedback loop to ensure appropriate and fairness - compliant handling ( I 7 ) . For the spe - cific project , interviewees view some form of supervision or fairness responsibility as an essential concept . Either the data curator ( I 3 ) , a separately consulting organizational instance ( I 7 ) , or the project or department manager ( I 6 , I 11 ) should be responsible for fairness in the specific project . The initiative for fairness , however , needs to come from the company’s particular commitments regarding fairness ( I 11 ) . Some processes , like annotation , also need careful supervision and verification by someone accountable ( I 4 , I 6 ) . An internal audit or assessment can also be a vital tool for a company to ensure accountability regarding fairness ( I 7 , I 14 ) . An assessment may target the impact an application has on its stakeholders as a foundation for fairness considerations ( I 7 ) . It may be updated and tracked continuously , and with changing stakeholders , it needs to be reiterated ( I 7 ) . Red teaming is suggested as another form of assessment ( I 7 ) . It pro - vides a holistic system testing for potential issues in a set time interval . The so - called “red team” is to find fairness vulnerabilities in the system . That could be seen as an exemplary holistic and focused company internal fairness assessment . Such assessments Fairness Certification 29 are typically motivated by ensuring to be regulatory compliant with future legislation besides ensuring fairness ( I 14 ) . These assessments or internal audits should take place continuously ( I 7 ) and should not be conducted by the development team but rather by a separate organizational instance ( I 14 ) . An aspect that may be assessed which may vary in its importance for fairness between use cases , is the amount of human oversight in operations ( I 4 ) . One more dynamic should be considered regarding accountability which addresses binding claims made by a supplier about his product that shift re - sponsibility back to the supplier ( I 1 ) . An assessment should also check the diversity of the development team ( I 2 , I 11 ) . Due to different perceptions and realities of life , a diverse development team fosters problem - understanding and thus helps identify and solve fairness issues ( I 2 ) . The major challenge lies in defining what a diverse team is ( I 2 ) . It is debatable which dimensions like gender , race , or sexual orientation diversity are relevant to consider when building a diverse team ( I 2 ) . It should ideally represent minorities ( I 2 ) . Currently , development teams lack the diversity of their users in di - mensions like age and gender ( I 2 , I 11 ) . To be , to a certain degree , representative of the user may be a requirement to check ( I 2 , I 11 ) . Diversity may also be supported via ( sometimes lacking ) interdisciplinary collaboration within the development team ( I 6 , I 10 ) . Interviewees particularly highlight diversity in the development team for anno - tators as they directly impact and potentially bias data ( I 4 , I 10 , I 13 ) . That , however , comes with the limitation that someone needs to be qualified to label , which may be more or less restrictive , dependent on the use case ( I 10 ) . B . 3 Project Planning Criteria Project planning criteria are set up to assess the considerations regarding fairness in the project’s planning phase , where business understanding and process planning are done , and requirements for the project are set . Interviewees are divided on whether project planning criteria are useful to implement and what should be assessed with what kind of purpose in mind . In such early stages of the lifecycle , one interviewee argues it might be challenging to set criteria for certifying fairness ( I 13 ) . Another interviewee argues that a consequence of criteria for business understanding may be neglecting some products’ specialized nature , leaving little room for steering decisions ( I 6 ) . Define & Assess the Planned Application Interviewee 14 argues that in a certification process , the purpose of use and not the technology itself should be the focus of an assessment for certification ( I 14 ) . Hence , the problem definition that the system builds on should be investigated ( I 8 , I 12 , I 13 ) . That motivates an assessment that can already occur before the actual implementation starts , even though it may be associated with many manual processes ( I 11 ) . According to many interviewees , solid definitions of the use case and the application planned are essential factors that should be checked in a fairness assessment ( I 4 , I 8 , I 9 , I 12 , I 13 , I 14 ) . Moreover , users or stakeholders , in general , need to be identified ( I 8 ) . In pursuit of being able to assess the impact on stakeholders and the planned application , three major concepts need to be considered : First , a good user and stakeholder understanding needs to be established . Second , the fairness properties of the use case need to be identified and assessed . Third , the planned solution must be checked to determine whether its fit for the use case favors fairness . The hierarchical mapping of all relevant concepts can be found in Figure 6 . User & Stakeholder Understanding : User and stakeholder understanding capture assessing processes to gain knowledge about general attributes and characteristics of 30 Freiberger et al . Fig . 6 . Criteria relevant to “Define & Assess Application” hierarchically mapped stakeholders like demographics ( I 6 ) , a target group definition ( I 8 ) , the understanding of possibly multiple levels of distinct users involved ( I 8 ) , and the context users find them - selves in ( I 11 ) . The assessment of the context of the users is highlighted by interviewee eleven as some stakeholders may be dependent on the system without having alter - natives ( I 11 ) , making a fairness issue more impactful . Interviewee eight highlights the importance of an assessment for direct and indirect users as well as other stakeholders separately , instead of a singular focus on the end user , to avoid vague results ( I 8 ) . This conception is supported by the fact that some systems operate in multi - sided markets , and fairness impacts multiple stakeholders ( I 7 ) . The composition of the target group should be explained for an assessment to check ( I 7 , I 8 ) . Some use cases may natu - rally exclude certain groups without being unfair ( I 7 ) . Moreover , the audited company should have mechanisms to assess the likelihood of marginalized groups within the in - dividuals affected by the application ( I 8 ) . It should be assessed whether the planned system considers the particular interests of all its user groups ( I 11 ) . However , it needs to be checked whether such processes may lead the company to develop misconceptions of their stakeholders as there is a risk of misinterpretations about their true nature ( I 9 ) . Use Case : Fairness Properties : Assessing the fairness properties of the use case involves checking the definition of fairness for the use case and the implications of the latter on the defined stakeholders ( I 12 , I 14 ) . A definition can be seen as essential as fairness aspects can differ drastically between use cases and conceptualizations of fairness ( as discussed in the previous chapter ) . It sets the focus of what needs to be considered for the technical system ( I 14 ) . As different stakeholders are involved , there may be multiple fairness definitions to consider ( I 12 ) . What can also be assessed in an audit is whether the fairness approach that the audited company chooses is suitable for the use case ( I 14 ) . The underlying business case should also be assessed regarding fairness ( I 9 , I 10 , I 14 ) . An unfair underlying business case invalidates all attempts to create a fair system ( I 9 , I 10 ) . Interviewee 10 suggests utilizing a question catalog with yes / no questions on the use case . The business responsible for the project may fill it out and provide it for an audit to identify the fairness of a use case ( I 10 ) . Another property of a use case that needs to be considered is its vulnerability to fairness risks which may be introduced via sensitive criteria ( I 7 ) . Name ( I 5 ) and region ( I 2 , I 5 ) are just two examples of such attributes . Credit scoring is mentioned as an exemplary Fairness Certification 31 application strongly affected by fairness risks ( I 13 ) . An assessment should not just check whether the fairness risks of a use case are being identified but also if there are mitigation strategies for those risks in place ( I 7 ) . The last property to consider is how much fairness is a viable business factor for the use case ( I 13 ) . It may indicate to the auditor how much the company is by itself incentivized to take action . Planned Solution : Use Case Fit : The planned solution is investigated for its fit to the use case . That involves the ML model itself but also its application . As a first step , it should be assessed if there is a viable alternative to the ML model in the form of a deterministic solution or a rule - based set that should be used instead ( I 5 , I 12 ) . Such solutions tend to be less affected by biases ( I 4 , I 5 ) . It can also be the case that a manual process should be preferred based , for instance , on ethical considerations ( I 12 ) . If an ML - based solution is chosen , model size and the amount of data flowing into a model should also be assessed ( I 5 ) . The bigger a model gets , the larger is the potential and complexity regarding biases ( I 3 ) . Smaller models usually also offer the advantage of better explainability ( I 4 ) , which is a topic discussed with the concept “Model Reporting and Transparency” in the chapter “Criteria for Governance” . However , there is typically a trade - off in other KPIs like performance ( I 4 , I 5 ) . Moreover , the chosen objective should be assessed for certification . It should not represent a loosely connected and just correlated proxy for the business problem at hand but rather represent it most accurately ( I 12 ) . Often , correlated information on ethnicity is used for predicting an objectively unrelated variable leading to harmful results , for instance , for the black community ( I 12 ) . To check for that issue , the company should state its business goal so the certifying institution can assess the chosen objective’s suitability ( I 12 ) . A mismatch should be easily detectable by an auditor ( I 12 ) . Fairness Targets in Requirements Another finding is that fairness should be embedded in the requirements set for the NLP project ( I 2 , I 4 , I 9 ) . Fairness - related criteria should be kept in an abstract form to be applicable to all projects ( I 2 ) . When defining tasks , these requirements should help to give direction in the form of a checklist of where to focus and where problems may occur , hence introducing a reflection process ( I 2 ) . A requirements catalog should direct the development and improvement of the system towards systematically including fairness considerations by enforcing specific steps for fairness to be considered ( I 9 ) . It embodies the company’s vision regarding fairness and its commitment to it ( I 2 , I 9 ) . Such a requirements catalog or checklist may be assessed in an audit ( I 9 ) . B . 4 Data - related Criteria Most interviewees agree on the relevance of data - related criteria for certification of fairness ( I 1 , I 4 , I 5 , I 6 , I 8 , I 9 , I 10 , I 13 , I 14 ) . One general aspect that should be considered regarding data is , that all processes leading to the data that the model eventually uses , should be reproducible for an audit ( I 4 ) . Apart from the data assessment itself , it is also discussed how data should be managed and how it should be handled and transformed in processes over the lifecycle like annotation , preprocessing , evaluation , and continuous improvement . Data Assessment Data assessment focuses on judging the suitability of data for training and retraining models that operate fairly . The data set on which it is trained 32 Freiberger et al . Fig . 7 . Criteria relevant to “Data Assessment” hierarchically mapped can be seen as the major fairness issue ( I 1 , I 2 , I 5 , I 6 , I 7 , I 10 ) , which gives reason for a thorough investigation . It needs to be assessed regarding its fairness , specifically regarding the use case it is intended to be used for ( I 1 ) . The data’s specific impact on the overall system needs to be considered ( I 5 , I 7 ) . Subject matter experts may be required to assess data regarding fairness in an audit ( I 12 ) . However , it needs to be considered that data may be scarce for specific use cases , leaving little room for improvements for the audited company ( I 6 ) . As another limitation , interviewee five mentions that some information may also not be available for assessment in certain cases ( I 5 ) . Data assessment entails an assessment of the data quality by the auditor but also an assessment of company processes that aim at avoiding data affecting fairness negatively . All relevant concepts for data assessment are portrayed in Figure 7 . Data Quality Assessment : Data quality is a concept that covers to what extent data captures reality completely and in an undistorted way while keeping suitability for the use case in mind [ 79 ] . Other facets of data quality are not discussed here . Rep - resentativeness ( I 1 , I 2 , I 4 , I 6 , I 7 , I 8 , I 9 , I 10 , I 11 , I 12 , I 13 ) and identifying harmful and incorrect data ( I 2 , I 4 , I 8 , I 9 , I 10 , I 13 , I 14 ) are mentioned for the data quality assess - ment . Representativeness captures data , including all the variables needed , to represent the diversity of users of the system ( I 12 ) . Interviewee twelve mentions the prominence of lacking representativeness of certain ethnic groups , causing fairness issues for ML models in the past ( I 12 ) . To assess the representativeness of data , it makes sense to introduce distribution checks ( I 1 , I 4 , I 8 , I 9 , I 13 ) . Distributions can be aimed at being representative of overall society ( I 10 , I 12 ) or the system’s users ( I 2 , I 6 , I 7 , I 11 ) . Lacking representation in distributions can either happen by the exclusion of certain groups ( I 6 , I 7 ) or by skewed representation ( I 1 , I 4 , I 6 , I 13 ) and should be explained ( I 6 ) . Repre - sentativeness is assessed for different attributes or target variables represented in data distributions ( I 4 , I 13 ) . For assessment , statistic screening ( I 12 ) and frequency - based screening ( I 4 , I 10 ) may be utilized . The latter could also be described as a NER - based representations screening ( I 10 ) . Words indicative of a class for a sensitive attribute and related words are automatically tracked via their semantic vector representation , which mirrors what NER does ( I 10 ) . Their frequencies are compared to the repre - sentative real - world ratio . A comparison to a reference dataset regarding frequencies Fairness Certification 33 and semantic representation may also be utilized ( I 10 ) . Checking label distributions in the annotation process may uncover some underlying data issues ( I 6 ) . One particular factor that may impact representativeness after data acquisition is preprocessing . Rep - resentativeness may be negatively affected by data cleaning in the form of the removal of outliers ( I 10 , I 12 ) or by sampling ( I 4 ) . That makes assessing distributions before and after preprocessing necessary ( I 12 ) . The sourcing and collection of data need some assessment by the certifying institution regarding representation issues . It should be checked whether the data distribution represents the use case ( I 2 , I 6 , I 8 ) . As data is sometimes procured from abroad and collected there in an entirely different societal background , there may be issues with its representativeness for the use case ( I 6 , I 8 ) . It could be checked in an assessment whether the application context of data meets its distributions . Similar issues occur when poorly fitting open datasets are used for first model iterations ( I 2 ) . Induced selection biases can be found by inspecting the underlying data collection process ( I 6 , I 12 ) . The context of time ( I 12 ) , process char - acteristics , education of subjects , and location of the collection could be considered and questioned regarding possible adverse downstream effects ( I 6 ) . Data augmentation also needs to be mentioned , considering the representativeness of data ( I 11 , I 13 ) . It can be assessed whether the baseline set for augmentation captures diverse enough data to represent the system’s users ( I 11 ) . Because then there likely will not be a negative impact on representativeness after augmentation ( I 11 ) . A company could even utilize data augmentation to improve the representativeness of data ( I 13 ) . That may be done by augmenting larger quantities for the underrepresented group . However , this comes with the limitation of a loss in label accuracy ( I 13 ) . The approach to data augmenta - tion and its downstream impacts may be relevant to check in an audit . Online learning utilization may enable a system to adjust automatically toward better representation ( I 9 ) . It captures systems that automatically go through continuous improvement cycles by automated capturing of data for retraining ( I 9 ) . When representation issues can be identified , the system could be set up to counter - steer automatically ( I 9 ) . Harmful or incorrect data negatively impacts data quality , and systems should be checked for it ( I 2 , I 4 , I 10 , I 13 , I 14 ) . Interviewee 13 mentions harmful contents in baseline data as an issue that should be checked , particularly for generative models ( I 13 ) . Misinformation in data is also an issue that should be investigated ( I 8 , I 13 ) . However , in some cases , there is a grey area where it is difficult to judge to which extent data accurately rep - resents the true nature of facts ( I 8 ) . The last factor that should be assessed regarding harmful data is word embeddings which become relevant in transfer learning ( I 2 , I 4 , I 9 , I 14 ) . The semantic representation of words in vector space needs to be checked for unwanted associations like gender with occupation ( I 2 , I 4 , I 14 ) . Therefore , distances between embeddings in vector space should be checked before training a specific model ( I 4 , I 14 ) . A reference data set may again be established to set an expectation for vec - tor space representation ( I 10 ) . Embeddings do have a downstream impact . Companies can handle the impacts of vector space representation by utilizing readjustable models ( I 4 ) or in their decision - making by compensating for their impact ( I 14 ) . Such coping mechanisms may also be an aspect to assess for certification . Internal Processes Assessment : Apart from investigating the data itself , a com - pany’s processes for sourcing , collecting , and checking it for biases should be assessed . Such mechanisms may be essential to consider as they can indicate how much stability of fairness in data over time can be expected . The data sourcing assessment investigates how the company procures its data and what considerations are made to ensure sourc - ing quality data for building a fair system ( I 13 ) . The selection process for sources is a central consideration here ( I 1 , I 2 , I 4 , I 8 , I 12 , I 13 ) . There may , for instance , be quality 34 Freiberger et al . indicators like reviews or reputation of a data set of interest ( I 1 ) , ratings for articles or scientific journals utilized ( I 13 ) , or , in the case of social media data , for instance , an up - vote to down - vote ratio ( I 4 ) . Social media , however , needs careful consideration regarding the intended use case as it tends to be very opinionated and often biased ( I 4 ) . Sources renowned for extreme perspectives , which tend to be biased , should be avoided ( I 4 , I 8 ) . When sourcing externally , it should be checked for transparency about the data collection process ( I 1 , I 12 ) . Sources used should be documented ( I 4 ) , and the sourced data should be reviewed regarding use case fit ( I 2 , I 4 , I 5 , I 6 , I 13 ) . The data collection assessment checks for suitable processes to collect data ( I 12 , I 14 ) . The col - lection of data by the company itself , in contrast to data sourcing , gives the company complete control over its data acquisition ( I 12 ) . Guidelines for the use case should be set for the collection process to avoid bias and may be reviewed for certification ( I 3 ) . Data is sometimes synthetic and , in the case of chatbots , sometimes a result of brain - storming different ways of expression for responses in the development team ( I 2 ) . That is why special guidelines may be required to avoid inducing bias ( I 2 ) . Assessing forms of bias checks a company performs may help assess processes enforcing fairness ( I 1 , I 6 ) at the cost of being resource intensive ( I 2 ) . Human involvement should be checked , as it makes sense to pursue a semi - automated approach in this case ( I 2 ) . That is because a human has a more refined sense of detecting relevant biases or fairness issues than a machine ( I 2 ) . Hence , it should be checked whether a human assesses a data sample for potential fairness issues ( I 2 , I 10 ) . The company should establish processes for an explo - rative data understanding of the dataset , helping to assess its quality and contents ( I 1 , I 4 , I 5 , I 10 , I 11 ) . The contents , like ways of expression ( I 5 , I 9 ) , contained entities ( I 5 ) , and personalized data ( I 5 ) , already indicate potential biases in data towards which a system needs to be robust . Even just the language that is used may indicate whether a system is susceptible to gender bias ( I 13 ) . Moreover , data contents should also be internally checked for diversity and inclusiveness ( I 4 , I 7 , I 9 , I 11 ) , potential stereotypes represented , and harmful content ( I 1 , I 8 ) . A company should investigate all of this , considering metadata describing data sources to get the context that may be relevant for identifying issues ( I 11 ) . Data bias checks should also help the company uncover un - fairly annotated data ( I 6 , I 12 ) . That can be done by comparing incorrectly labeled data between groups upon review ( I 12 ) . If such unfairness is discovered , a company should investigate why it happens ( I 6 ) . In some cases , a company may uncover bias in data , but there may be no suitable mitigation strategies . That should lead to documentation of the issue for mitigation in downstream processes ( I 3 ) . Annotation The annotation process of data can be considered an important factor for the fairness of a system based on supervised learning ( I 4 , I 6 ) . When investigating the annotation process of a company in an audit , it may be challenging to assess the quality of annotations in some cases as there may not be a ground truth that can be utilized for evaluation ( I 12 ) . Moreover , there is a high degree of subjectivity to annota - tion ( I 5 ) . If there is an objective ground truth , however , one may assume that a correct label is a fair label and evaluate fairness by the correctness of labels ( I 12 ) . To certify annotation , one may consider processes established in a company for annotating data . Three major concepts are identified for company processes . First , fairness - relevant as - pects in the data should be annotated in the data ( I 1 , I 2 , I 11 ) . That is a crucial step for being able to evaluate the fairness of the system for an audit or continuous im - provement ( I 1 , I 2 ) . Another way this benefits the development of fair systems is that it facilitates purposeful sampling allowing for fair representation of certain groups in Fairness Certification 35 data ( I 2 ) . The annotation process of fairness - relevant aspects may either be manually labeled or automatically tagged ( I 2 ) . However , one drawback to such annotations is also mentioned with its possible negative performance impact ( I 1 ) . Second , there is a tradeoff between labor cost and time and the quality of annotations to handle ( I 2 , I 11 ) . To a certain extent , saving cost and time will sacrifice quality and hence fairness of an - notations . Interviewee four proposes an approach that may be requested of companies as an option to pursue a more reflected annotation strategy regarding label quality . This tradeoff can be managed appropriately by including filters defining what needs to be annotated manually ( I 4 ) . Third , there should be a set of measures implemented by the company that causes a minimization of fairness risks in the annotation process ( I 4 , I 5 , I 7 , I 10 , I 12 , I 13 ) . Like every human , human annotators have biases that motivate annotation processes so that their biases are at least less reflected in their annotations of data ( I 7 , I 11 ) . While annotating data , one may require the annotator to see similar annotated examples to the respective one that should be annotated ( I 7 ) , facilitating a more objective decision . One may also require the process to be designed so that sensi - tive information stays hidden and cannot bias the annotators’ decisions ( I 7 ) . An aspect frequently mentioned is involving multiple annotators in labeling a data point ( I 4 , I 5 , I 7 , I 10 , I 12 , I 13 ) . Interviewee four suggests that it should be case - dependent to either re - quire multiple annotators or not ( I 4 ) . Particularly for topics that are very opinionated and perceived differently by every individual , like politics , multiple annotators should be involved to reduce biases ( I 4 ) . The aggregation of multiple annotators’ opinions on the correct label can either be achieved by a majority vote between annotators ( I 5 , I 12 ) or by an Inter - Annotator Agreement ( I 4 , I 7 , I 10 , I 13 ) . An Inter - Annotator Agreement score quantifies how similarly multiple annotators annotate the same pieces of text ( I 4 , I 10 ) . Particularly referring to topics that tend to be opinionated , a very high score rep - resents a high agreement which may indicate that annotators are in a bubble ( I 10 ) . It should raise organizational questions which are discussed in “Criteria for Governance” . However , in the case of developing generative models , finding a suitable metric as a baseline for such an Inter - Annotator Agreement may be difficult ( I 5 ) . Another integral approach the audited company should pursue is utilizing annotation guidelines ( I 4 , I 6 , I 7 ) . These guidelines should be documented so they can be audited and as objective as possible ( I 4 ) . A beneficial consequence of introducing annotation guidelines besides fairness is more consistency and hence higher quality of annotation ( I 6 ) . Preprocessing Preprocessing includes all steps for transforming , cleaning , selecting , and expanding data after annotation for training and evaluation . When auditing this step in the lifecycle , it must be considered that its scope may vary strongly between use cases ( I 3 , I 11 ) . That makes it problematic to require certain steps for all use cases . Instead , the steps taken should be documented and assessed regarding fairness impact ( I 4 ) . One concept mentioned several times is that the steps in preprocessing should be checked to determine whether they induce bias or counteract measures to improve fairness from previous lifecycle steps ( I 10 , I 11 , I 12 ) . The same criteria established to be met by the data before preprocessing should also be met after preprocessing ( I 12 ) . Robustness - ensuring mappings are a way to transform data by mapping it to a stan - dard form ( I 2 ) . They may be helpful to pursue in some use cases as they can remove usability hurdles ( I 2 ) . They can be , for instance , utilized to enable people with migra - tion backgrounds to achieve a similar performance compared to native speakers of a language . For instance , articles can be mapped to a generic domain , making typical mistakes irrelevant ( I 2 ) . Anonymization may be a very related approach because it 36 Freiberger et al . aims to drop the sensitive attribute in the data ( I 3 , I 5 , I 7 , I 14 ) . The issue with such an anonymization approach is that dropping the sensitive attribute makes it challenging to assess the fairness of a model at a later stage , as the information from the sensitive attribute is lost ( I 7 ) . Because of that , anonymization counteracts the annotation of fairness - relevant attributes as an earlier step in the lifecycle . Another problem with anonymization is that it does not necessarily solve the problem of biased data , as there usually are indirect encodings through correlations that can materialize as second - order effects ( I 8 ) . In some use cases , anonymization may not be suited as it may undermine the solution’s functionality because of a negative performance impact ( I 5 , I 10 ) . The same applies to differential privacy , which is an approach for comparing the effect of including the sensitive attribute and dropping it ( I 5 , I 7 ) . Nevertheless , in some cases , it may be viable and should be considered by the company ( I 5 , I 7 ) . An important consideration for preprocessing that may be assessed is the filtering and selection of data ( I 3 , I 4 , I 13 ) . Filtering and selection aim to improve the data quality for use via filtering mechanisms and ensure appropriate data splitting . Filtering may target re - moving harmful content like obscene language ( I 13 ) or low - quality or biased data ( I 3 , I 4 ) . However , it may be challenging for some topics to identify biased data clearly ( I 13 ) . Moreover , defining fair filters objectively and calibrating how restrictive they should be poses a challenge ( I 4 ) . Particularly the use of social media data raises the question of controllability for such a process ( I 13 ) . When selecting data , unbiased and balanced data sampling is essential ( I 1 , I 4 , I 7 , I 8 , I 10 , I 12 , I 13 ) . Depending on how data is selected , a biased selection could be made , which is to be avoided ( I 1 , I 7 , I 10 , I 12 ) . When split - ting the data into training data and data for evaluation , it should be assessed whether the distributions of training , evaluation , and overall data roughly match ( I 10 , I 13 ) . As mentioned , that helps avoid bias and ensures the integrity of evaluation results ( I 10 ) . It was also mentioned that the data splitting might consider underrepresented minorities by representing them in the data samples equally compared to other instances that are better represented ( I 10 ) . Data for Evaluation In evaluation , some aspects and concepts regarding data must be considered . Functional testing of a system is input - based testing assessing the outputs and hence views the system from an external perspective suited for an audit ( I 10 ) . The procedure is discussed in the chapter “Modeling & Evaluation Criteria” . Test data is targeted at unveiling biases present in the model ( I 3 ) . It should represent the diversity of actual usage of the system and the challenges that come with it ( I 2 , I 11 ) . Moreover , it should suit the assessment regarding the specific fairness definition imposed by the audit ( I 9 ) . A relevant consideration is the sourcing of data sets utilized for evaluation . It could be acquired by accumulating in - use data of the system over time ( I 12 ) . Another way of acquiring test data is going by previous precedents of fairness issues and utilizing relevant data regarding these ( I 13 ) . The last approach to getting data is artificially generating it , making a targeted evaluation of fairness easier as it can be designed to reveal biases ( I 2 , I 6 ) . Targeted segmentation for sampling data of inputs can help assess specific biases ( I 7 ) . However , these data sets may also be biased and only provide partial insight into fairness issues ( I 8 ) . Another vital consideration for functional testing is utilizing fairness invariance tests ( I 2 , I 3 ) . Such tests utilize targeted replacements in inputs to assess the model’s invariance to variation in sensitive attributes’ values ( I 2 ) . These replacements , as a form of augmentation , are usually generated by multiple language models ( I 2 ) and involve approaches like forward and backward translation ( I 2 , I 3 ) . Drawbacks to this approach are mentioned with the Fairness Certification 37 complexity of such augmentations in NLP ( I 3 ) and potential model biases present in the models utilized for generating replacements ( I 3 ) . Nonetheless , interviewee two sees robustness invariance tests as an integral part of certifying the fairness of a system ( I 2 ) . It is suggested to propose a range of measures and a minimum standard for creating such augmentations for fairness invariance tests ( I 2 ) . Data for Continuous Improvement Data - related criteria in continuous im - provement target data considerations for maintaining a fair system over the time of its operation . That involves considering monitoring practices as well as feedback loop practices . Continuous improvement is affected regarding data by two challenges : main - taining data diversity ( I 9 ) and extending continuously relevant testing data ( I 9 ) . As criteria for monitoring , test sets ( I 2 , I 3 , I 6 , I 8 , I 9 ) , drift monitoring ( I 2 , I 3 , I 4 , I 5 , I 6 , I 7 , I 13 ) , as well as a request assessment for underrepresented groups ( I 2 ) are mentioned as practices to be implemented by the audited company . Test sets in continuous improve - ment are the same data - wise compared to test sets in evaluation , but they are used continuously to assess whether fairness can be maintained or whether certain issues have occurred ( I 2 , I 3 , I 6 , I 8 , I 9 ) . Drift monitoring is described either as data - centric ( I 2 , I 4 , I 5 , I 7 , I 13 ) or as model - centric ( I 2 , I 4 , I 5 ) in its approach . It may reveal the necessity to retrain a system to stay fair and performant over time ( I 2 , I 3 , I 4 , I 5 , I 6 , I 13 ) . A model - centric approach considers model behavior like changing performance or confidence in predictions ( I 2 , I 5 ) and explainability ( I 2 , I 4 ) of the latter , in the form of explanation scores or some other form of quality assessment for the explanation , in detecting drifts . In a data - centric approach , the variation of incoming data for the model to handle compared to the original training data is assessed ( I 2 , I 4 , I 5 , I 7 , I 13 ) . Continuously appearing outliers may be an indication ( I 2 ) . It may be helpful to trace back to the kind of requests causing the drift to understand the underlying issue ( I 2 ) . The reaction strategies to an occurring drift may be interesting for an audit ( I 3 ) . When users change , assessing them for semantic overrepresentation , which may cause an un - favorable data drift , may also be relevant ( I 2 ) . The frequency at which such drifts may occur is dependent on the dynamics of the environment in which the application is operational ( I 2 , I 5 ) . That should impact the assessment of the calibration of such a drift monitoring system . An assessment should also consider that drift detection may lack a measurable foundation for some use cases ( I 3 ) . Hence , it should only be required for changing environments and if it is measurable . The consequences of drift monitor - ing may materialize in improved fairness and performance , making it compatible with business goals ( I 2 , I 3 ) . An assessment of requests for underrepresented groups focuses on better understanding marginalized groups’ requests ( I 2 ) . That helps uncover po - tential issues to counteract the negative impact of underrepresentation in data they face ( I 2 ) . That may be performed by checking semantic differences in the expression of marginalized groups compared to better - represented groups ( I 2 ) . Substantial devi - ations may indicate potential issues in system performance later ( I 2 ) . Feedback loop practices regarding data need to consider how in - use data and user - made corrections are handled . One condition that needs to be met by both is user privacy ( I 7 , I 9 , I 10 ) . That may target a confidential treatment of user feedback ( I 7 ) as well as removing sensitive attributes from that feedback ( I 10 ) and ensuring data protection and data security when saving data ( I 9 ) . In - use data is gathered by regular operation of the sys - tem without any user interaction targeted directly at generating feedback and should also be subject to assessment ( I 5 , I 9 ) . One form of in - use data that can be utilized is prediction queries dependent on confidence in the prediction , either taken for retrain - 38 Freiberger et al . ing future models in case of low confidence or for usage as test data otherwise ( I 9 ) . That works in favor of underrepresented groups as their queries are more likely to be predicted with low confidence and hence taken for model improvement . Metadata may be essential to understand fairness - relevant aspects in such a case ( I 6 , I 9 ) . However , if metadata is unavailable , the context of the query stays hidden , and it may be challeng - ing to target model improvement toward fairness ( I 9 ) . Another form of in - use data can be gathered by indirectly measuring user discontent by analyzing behavior when the user interacts with the application ( I 5 ) . That may indicate a system’s impact on its users , and after passing a certain threshold , it can be estimated to be unfair , requiring intervention ( I 5 ) . The indirect form of measurement offers the advantage of gather - ing data without influencing or biasing the user in their feedback statement , which may happen when directly asking for it ( I 5 ) . A dedicated team or a quality assessor should check usage data regularly for such aspects ( I 5 ) . If users can make immediate corrections or give feedback which may be taken to improve the system , particularly the quality of feedback data needs attention as feedback may be biased or incorrect ( I 4 ) . If the user is facilitated to correct an annotation , clear guidelines should be estab - lished by the company and communicated to the user ( I 4 ) . Hence , data is consistent with the originally annotated data , and incorrect annotations may be prevented . The latter should also be enforced by a controlling instance that has to review and accept user annotations ( I 4 ) . Moreover , there should be some form of filter rejecting harmful , like for instance stereotyped , feedback data generated by the user from being utilized for model retraining ( I 10 ) . There are industry examples where such data has caused a system to become heavily biased ( I 9 , I 10 ) . Data Storage Regarding data storage which covers how data is stored in develop - ment and operations and cached in operations , two concepts are introduced . First , the storage of sensitive attributes should be avoided in a raw form ( I 10 ) . That has gained relevance due to the trend towards the ELT ( Extract Load Transform ) paradigm , as there is an interest in utilizing the raw and not the transformed data ( I 10 ) . Avoid - ing saving sensitive attributes raw can also be partially motivated by compliance with GDPR , as personally identifiable information should not be saved ( I 10 ) . This focus on privacy highlights again that fairness is interrelated with other RAI topics . Second , interviewee one mentions caching of previous predictions of the system as a potential issue ( I 1 ) . In case of unfair previous predictions , this necessitates clearing the cash to restore fairness of the system ( I 1 ) ) . B . 5 Modeling & Evaluation Criteria In this chapter , fairness criteria for building NLP models and evaluating them and the system they are embedded in are explained . Modeling Modeling , particularly model selection , is perceived as less critical for certifying fairness compared to criteria targeting other lifecycle steps ( I 1 , I 5 , I 10 ) . A potential issue mentioned regarding the utilization of pretrained models is a lacking holistic understanding of it ( I 3 ) , making it hard to assess and enforce fairness for train - ing the custom model . That leads interviewee three to suggest only permitting the usage of , to some degree , trustworthy and documented baseline models for certification ( I 3 ) . That could solve the existing business process and documentation conflict in case Fairness Certification 39 a poorly documented model outperforms a well - documented one ( I 3 ) . This would also incentivize providers of performant models to document them appropriately . When de - veloping and training models , fairness can be rooted in the model architecture ( I 2 , I 4 , I 5 , I 6 , I 7 , I 10 , I 14 ) . For doing so , interviews reveal four potential strategies . The first introduces a fairness classifier built onto the model used ( I 5 ) . For implementing some - thing like this , however , a labeling of fairness for all annotations used in the original model would be required , which would be very resource - consuming and impractical ( I 5 ) . The second approach embeds fairness into the objectives optimized by the model ( I 4 , I 5 , I 7 ) . That may be done by performing a hyper - parameter optimization of subjec - tive fairness , even though it would be time and resource - consuming ( I 5 ) . Another way would be to calculate the loss function , which is to be optimized only on non - sensitive attributes or to penalize using the latter ( I 10 ) . Either way , the question needs to be asked whether the intended model is suitable for such approaches in practice ( I 7 ) . The third approach aims at explicitly calibrating a model to avoid certain biases or , in a way , hard - code fairness into the system ( I 4 , I 5 , I 6 , I 14 ) . That may go beyond the NLP systems prediction and involve rules for decision - making based on the prediction ( I 6 , I 14 ) . As a fourth approach , interviewee two suggests introducing constraints for model controllability ( I 2 ) . Language generation via a prompt - based model is mentioned as an example where there should be the possibility to avoid biases by setting certain parameters influencing constraints for the model ( I 2 ) . Otherwise , the model would mirror potential biases in baseline data ( I 2 ) . All of the mentioned approaches should be assessed regarding their impact on the performance and fairness of the system ( I 5 ) . Evaluation Evaluation is essential for assessing a model’s fairness ( I 3 , I 7 , I 9 ) . It involves testing the model or the system it is embedded in regarding various targets or metrics . Reproducibility must be given for company internal evaluation processes so they can be assessed in an audit ( I 8 ) . A challenging factor may be the definition of fairness which is the foundation for evaluating fairness ( I 8 , I 12 ) . As the definition of fairness differs between the auditor and all affected stakeholders ( I 12 ) , a hierarchical approach to evaluation focused on the system’s stakeholders may make sense as a base - line for evaluation ( I 8 ) . Another challenging factor is to be found in the context of data as a prediction foundation . Sometimes truthfulness or accuracy may be questionable but hard to verify , posing a major issue for evaluation ( I 8 ) . In open - domain systems like question - answering systems or recommender systems delivering multiple suggestions , the complexity of human language becomes a challenge for fairness evaluation ( I 2 , I 8 ) . The auditor may assess all criteria in this chapter directly as system tests . However , they should also be embedded in the company’s internal evaluation processes , which can be audited . Interviewees name two major concepts that drive evaluation . Func - tional testing criteria ( I 1 , I 2 , I 3 , I 4 , I 5 , I 6 , I 7 , I 8 , I 9 , I 11 , I 12 , I 13 , I 14 ) and metrics ( I 2 , I 4 , I 7 , I 8 , I 9 , I 10 , I 14 ) are defining factors identified for assessing fairness in evaluation . Functional Testing Criteria : Functional testing criteria take an outside perspective on the system’s outputs given some specific inputs . An outside perspective on the appli - cation is taken , and no deep insights into the model architecture are utilized here ( I 9 ) . The outputs for a given set of inputs indicate model behavior and resulting fairness issues ( I 1 , I 5 , I 6 , I 7 , I 9 , I 11 , I 12 , I 14 ) . Functional testing of the model can be considered an essential part of a certification process for fairness as it captures what the model predicts and what impacts its users ( I 9 ) . In comparison , the already mentioned data is the model’s baseline , but with nonlinear relationships of a model , even seemingly fair data can result in biased models ( I 8 , I 9 ) . Hence , functional testing criteria capture the 40 Freiberger et al . actual fairness of a system better . For such an approach to be meaningful , a relevant scope of inputs is required regarding quantity ( I 2 ) and diversity of input data , repre - senting a wide variety of user groups ( I 9 , I 11 ) . That may be facilitated via a mainly automated approach to testing ( I 2 , I 3 , I 4 , I 11 ) . Adversarial testing is mentioned for prompt - based generative language models ( I 1 , I 4 , I 7 ) . In that case , it involves utilizing specific queries that suggest or invoke stereotypes is a practical approach for fairness testing ( I 1 , I 4 , I 7 ) . If the assumption of the stereotype is met by the most probable words to be inferred by the generative model , a fairness issue is detected ( I 1 , I 7 ) . These associations made by the model can be assessed in an audit ( I 4 ) . Interviewee seven also points out the necessity of finding a metric for evaluating the fairness of a model going off a defined set of prompts ( I 7 ) . Another approach is utilizing specific test data sets ( I 2 , I 3 , I 6 , I 8 , I 9 , I 11 , I 13 ) . The data utilized here has been discussed previously in the chapter “Data - related Criteria” . The mentioned data is used as model input with a specific expectation for the predictions’ quality ( I 2 ) . It may make sense to analyze the distribution of predictions to assess their quality , which may reveal systemic issues ( I 1 , I 11 , I 12 ) . Interviewee one highlights that biased distributions in data should be accepted as a reality ( I 1 ) . However , a change in decision - relevant variables like qualifi - cations in a recruitment context should also change the distribution of predictions for a specific group ( I 1 ) . Hence , relevant variables to the topic should be checked to be the decision drivers . The latter may be achieved utilizing explainability tools discussed in “Model reporting & Transparency” in the “Criteria for Governance” chapter . Decision - irrelevant variables should , however , not influence model inference ( I 2 ) . This can be checked by introducing robustness invariance testing ( I 1 , I 2 , I 4 ) , which should be an essential part of a fairness audit ( I 2 ) . It aims to assess how capable a system is of gen - eralizing regarding a sensitive attribute . The latter may be , for instance , represented in expression difficulties by non - native speakers of a language ( I 2 ) . Protected groups may be introduced , representing sensitive attributes that could become an issue in the use case ( I 2 ) . By augmenting initial test data via replacements for protected groups or the addition of nonsense , the robustness of the model can be checked in the form of adversarial testing ( I 2 , I 4 ) . For most use cases , a larger invariance between a protected group and the rest of the users results in improved fairness ( I 2 , I 4 ) . However , there are exceptions , like the medical domain , where separation is essential and should be the focus ( I 3 , I 13 , I 14 ) . Assessment should be performed automatically with the additional requirement of involving a human assessment of a subset of data as a plausibility check for the automated assessment ( I 2 ) . This invariance testing is enabled by annotating fairness - relevant aspects like gender in data ( I 2 ) . Predictions of the model may also be required to be validated ( I 1 , I 4 , I 6 ) . Simpler models and particularly rule - based sets have the advantage of being typically less affected by biases , and decisions based on them tend to be based on relevant factors ( I 4 ) . Previously set expectations may also be used to validate a model’s prediction ( I 1 , I 6 ) . A discrepancy between prediction and what is used for validation should be investigated ( I 6 ) . Interviewee five suggests af - fected stakeholders as a source of functional evaluation regarding the system’s fairness ( I 5 ) . As a deciding factor to overcome individual differences in opinions on the fairness of the system , a majority vote or some other kind of threshold may be introduced ( I 5 ) . The challenge of possible discrimination of minorities which such an approach could induce , can be addressed by sampling minorities representatively when selecting the composition of affected individuals responsible for evaluation ( I 5 ) . An audit assessment should also consider ethics and moral considerations relevant to the system ( I 4 , I 11 ) . As fairness and ethics are closely related , there needs to be a discussion as to what extent fairness should represent normative values in a society ( I 9 ) . A central question Fairness Certification 41 here is : Is it fair if a system results in morally questionable actions that affect all groups in the same way ( I 9 ) ? A good fairness understanding of the use case , which is discussed in “Certification Criteria” is required to resolve ambiguity . The last criteria to consider for functional testing focus on the interaction of humans with computers and the involvement of humans to improve fairness . Humans sometimes tend to ac - cept predictions of machines as accurate and tend to become complacent in reviewing system outputs before utilizing them for decision - making , which is called automation bias . This automation bias is to be avoided as it does not counteract biases in predic - tions from impacting decision - making based on them ( I 11 ) . Hence , there should also be criteria for human decision - making that counteract automation bias ( I 11 ) . Humans should generally be involved and in the loop when assessing system fairness ( I 2 , I 7 , I 11 ) . Regarding the application of the model , there should be an assessment regarding the necessity of human involvement in decision - making . In specific , particularly crit - ical applications , human involvement in decision - making may be crucial ( I 12 ) . Many fairness issues are only apparent to humans and may currently not be automatically detectable ( I 2 ) . Moreover , humans may give the automatic system a plausibility check ( I 2 ) . Metrics : Metrics aim to make the fairness of NLP models measurable . Metrics are of essential importance for a targeted , quantifiable and reproducible assessment of the fairness of an application ( I 7 , I 8 , I 12 ) . Metrics based on a generalization of the model are named as well as metrics that follow an impact - based approach regarding fairness . Robustness and generalization may be seen as essential elements in assuring confidence in the model’s property of not generating unexpected , unfair results ( I 2 , I 4 , I 8 ) . For the first , the expected calibration error is named as a metric keeping track of the robustness of a model ( I 8 ) . Also , standard metrics commonly utilized in data science may already indicate the generalization of a model and hence may be assessed ( I 4 ) . The impact - based approach targets potential fairness issues for particular groups more directly ( I 8 ) , which manifests in the composition of fairness metrics . A major point of discussion in such metrics is their level of specificity ( I 3 , I 7 , I 14 ) . A specific metric for fairness will not easily transfer between use cases as there is yet to be a dynamic fairness metric capable of handling that variety ( I 7 ) . Less specificity in the metric will lead to less relevant results in the specific use case ( I 3 , I 14 ) . Interviewee eight hints at a trade - off between performance and fairness metrics that may need to be made by sacrificing some performance for the merit of the impact - based metric ( I 8 ) . Also , some metrics should consider and capture diversity information as a context when assessing performance metrics ( I 8 ) . Another impact - based metric would be a performance disparity threshold that needs to be met ( I 2 , I 7 , I 9 ) . The quality of service provided by the system should be evaluated regarding differences between groups , and a threshold should set a maximum acceptable deviation ( I 7 , I 9 ) . That may be expanded beyond the mere performance of the system and might include factors like accessibility and usability of the system as well ( I 2 , I 9 ) . The latter aims at removing barriers to system use ( I 2 ) and may be less quantifiable than just performance disparity . Another approach would be introducing a metric that penalizes the model’s use of sensitive attributes or unethical content . It would be used by the optimizer of the model when calculating loss ( I 10 ) . By this kind of reinforcement , some major fairness issues that could occur may be ruled out ( I 10 ) . For instance , in question answering , responses that contain insensitive or stereotyped content could be avoided ( I 10 ) . A wide variety of fairness metrics are also represented in different definitions of fairness available to choose from ( I 8 , I 12 , I 14 ) . No matter what metric is used in the end , its impact on the technical system must be assessed , and its suitability and implications for the use case must be assessed ( I 1 , I 8 ) . 42 Freiberger et al . B . 6 Operations Criteria Bringing a system into operation and going through continuous improvement cycles to maintain its quality should be investigated in a fairness certification process . These steps in the lifecycle determine whether a system will maintain fairness over time ( I 9 ) . Fairness can deteriorate in operations , for instance , at the cost of performance ( I 2 ) . Interviewee six mentions that the certificate must maintain integrity over time ( I 6 ) , which supports the relevance of system operations criteria for fairness certification . Operations need to build quick reaction strategies to counteract fairness issues that occur swiftly ( I 1 , I 7 , I 11 ) . Standard pipelines for retraining and publishing models are not viable as they take too long ( I 7 ) . Another consideration in operations is the resource distribution for models , which should not put fair models at a disadvantage regarding resource access compared to other models ( I 10 ) . When drastic changes are made or occur in a systems environment , there may be a requirement to renew the overall certification assessment or at least those assessments affected by the change ( I 5 , I 7 ) . Such changes may come in various forms . A new version roll - out may be a reason ( I 1 , I 7 , I 8 ) . There should also be detailed documentation throughout model versions to ensure traceability when a fairness issue occurs ( I 1 ) . Adaptions in data ( I 1 , I 7 , I 9 ) , changing stakeholders ( I 7 ) , or a change in the system’s application ( I 8 ) may be further reasons for renewing affected assessment steps . Deployment ( I 2 , I 3 , I 6 ) , monitoring ( I 1 , I 2 , I 3 , I 5 , I 7 , I 8 , I 9 , I 13 ) , and the feedback loop for continuous improvement ( I 1 , I 2 , I 3 , I 4 , I 6 , I 7 , I 8 , I 9 , I 10 ) are relevant to interviewees as the main criteria for operations , Deployment Criteria for deployment are mainly brought up in the context of pro - viding different model sizes via pruning or knowledge distillation approaches ( I 2 ) . First , it needs to be checked whether pruned or quantized models still fulfill fairness - related criteria established before to the same extent as their larger counterparts do ( I 2 ) . It makes little sense to perform an intense assessment for the large model that the user does not utilize in practice and , for instance , miss out on functional assessment of the smaller model that a user may use more frequently ( I 2 ) . Smaller models tend to be more prone to underfitting issues and should be checked accordingly ( I 3 ) . On the other hand , larger models may need more testing as they are opaque and tend to contain more complex and challenging to identify biases with more data being used for training them ( I 3 , I 6 ) . When choosing a suitable model size , performance and fairness objectives should play a role ( I 2 ) . Fig . 8 . Criteria relevant to “Monitoring” hierarchically mapped Monitoring Monitoring can be seen as a set of mechanisms that ensure fairness by only giving clearance for a new model version to be introduced or current versions to Fairness Certification 43 stay operational if fairness criteria can be assured ( I 1 , I 5 , I 7 , I 9 , I 13 ) . It typically involves fairness testing and intervention mechanisms , as shown in Figure 8 . Fairness tests have already been introduced under “Functional Testing Criteria” in the “Modeling & Eval - uation Criteria” chapter . Such an evaluation process based on fairness test sets should be extended to assess the model over time as it goes through continuous improvement cycles ( I 1 , I 2 , I 3 , I 7 ) . The goal is to quantify the occurrence of fairness issues ( I 3 ) which may be valuable information for an audit . With every new model or change in data or parameters , the fairness dimension of the system may be affected ( I 2 ) . Interviewee one mentions the importance of standards that can be systematically checked with every system retraining ( I 1 ) . Such testing could be run at a very granular level assessing , for instance , confidence and explanation scores on a sentence level as a foundation for later judging fairness ( I 2 ) . Passing scores or red flags should be established for fairness test sets to make the fairness testing impactful ( I 2 , I 3 ) . Interviewees are divided on how automated such testing can be . While interviewees one and nine see potential in automated fairness testing , interviewee seven argues the process is not automatable as there is no dynamic fairness metric ( I 1 , I 7 , I 9 ) . Depending on the automation of the testing , frequent testing is favorable ( I 1 , I 3 ) . In large - scale organizations or in a highly dynamic context , where retraining models takes place multiple times a day , a fairness assessment with every retraining could cause a considerable overhead ( I 3 ) . The com - pany may execute this fairness testing process right after quality assessment to assess the fairness impact of changes in quality ( I 9 ) . In some cases , a system may be oper - ated or operating in a potentially harmful way . To prevent the system from causing damage , intervention mechanisms are established . Particularly for transfer learning , there is the issue of a user or third party potentially misusing a system for unintended , unethical purposes like spreading misinformation , which needs to be avoided if possible ( I 13 ) . Interviewee 13 mentions the curse of globalization in this context , as traceability of what is built with baseline models stays unknown to the provider ( I 13 ) . Moreover , misuse has previously also caused issues with systems with automatic retraining on usage data ( I 9 ) . Targeted , hostile user behavior in the form of interactions with the system can make it develop strikingly unfair tendencies ( I 9 , I 10 ) . Manual controls and automated corrective mechanisms on a quantifiable foundation may be implemented to ensure system fairness over time ( I 9 ) . Generally , a certification process could check whether the issue of misuse is addressed and whether potential countermeasures are in place . Moderating users , on the other hand , can be an option to cope with user behavior that negatively affects a system’s fairness ( I 3 ) . Interviewee three mentions four shortcomings of moderating users of a system . Its practicality is questionable as a large volume of data needs to be handled , making it impossible to have human over - sight ( I 3 ) . That leads to utilizing models for moderation which may also be biased ( I 3 ) . Moreover , moderation may just cause a problem shift by users utilizing competitors’ services , making moderation , not incentive compatible as it counteracts business goals ( I 3 ) . In case a system develops extreme , unacceptable behavior , there also needs to be some form of kill switch , as running the system until the next update is rolled out may sometimes be unacceptable ( I 7 ) . Feedback Loop Typically , a system is not designed to be static but rather dynamic , utilizing feedback data from its use for improvements ( I 9 ) . That offers the advantages of outgrowing systemic biases ( I 6 ) and raising general model quality ( I 2 ) , which aligns with business goals . A similar approach to active learning may be beneficial for creating an effective feedback loop for counteracting biases in the system ( I 4 ) . By deducing 44 Freiberger et al . Fig . 9 . Criteria relevant to the “Feedback Loop” hierarchically mapped systematic characteristics of biased cases and giving them as input back to users , their feedback can be more targeted ( I 4 ) . To operationalize a system’s feedback loop , an interface for the user to provide feedback or potentially even corrections for fairness issues is required . The acquired feedback data needs to be used appropriately to improve the system . The latter is displayed in Figure 9 . Feedback Interface : The interface should allow users to interact with the system ( I 1 , I 2 , I 3 , I 7 ) . A challenge such interfaces must overcome is engaging the user to utilize them ( I 2 , I 4 ) . There may be a reluctance due to convenience ( I 4 ) or disengagement and disinterest in continuing to use the system if someone is offended by an occurring fairness issue ( I 2 ) . An audit might consider the extent to which such interaction mech - anisms are utilized to evaluate their effectiveness in exposing fairness issues so they can be addressed . In some cases , it may also be challenging to establish an interface in the first place as the user only indirectly encounters the model a few stages downstream in the process of an application ( I 3 ) . So , an audit should consider how the model is embedded in a system . An organization may also have user - centric teams that extend feedback information acquired in a visual interface integrated into the application ( I 7 ) . Those user - centric teams must closely collaborate with the organization’s development team as their input may be required to identify previously unknown fairness issues , which can then be replicated and mitigated by the development team ( I 7 ) . It may make sense to check for an interface for users to correct inference of a system ( I 4 , I 8 ) . The number of corrections made by users may also be relevant information for a cer - tification ( I 4 ) . Interviewee 4 also mentions corrections that can be made for a model explanation as feedback on what users would base the prediction on ( I 4 ) . The second form of interface targets reporting or flagging fairness issues encountered in use ( I 1 , I 2 , I 3 , I 7 ) . The users should be able to express their discontent with the fairness issue that they should be able to describe ( I 1 , I 2 , I 3 , I 7 ) . It should be checked whether such an interface is immediately accessible by a user ( I 2 ) . Use of Feedback Data : The issue with such interfaces is a tendency to shift ac - countability for fairness onto the system user , who may also be biased ( I 6 ) . Hence , for certification , the use of feedback data and accountability that comes along with it should be checked . That covers correctly understanding the fairness issue at hand , evaluation and preprocessing of data that should flow into retraining a system , and mechanisms for deciding when to retrain the system . Understanding the fairness issue Fairness Certification 45 at hand involves the challenge of tracing the fairness issue back to the part of the system where it originates ( I 1 , I 8 ) . Mechanisms for understanding the cause of the fair - ness issue should be implemented ( I 1 , I 2 , I 3 , I 7 ) . Assessing the expectation of the user and his or her explanation of unfairness for the prediction may be a first step in un - derstanding the issue ( I 1 ) . Tracking who specifically reports fairness issues may hint , for instance , at a marginalized group that may have been neglected in development ( I 2 ) . Depending on the system , such an investigation may be only done if a minimum number of reports mention the same fairness issue ( I 3 ) . After identifying the primary cause , the issue may be tested and reproduced as a baseline for mitigation ( I 7 ) . The incoming feedback data from model use that should go into retraining should not be used without evaluating incoming data and preprocessing ( I 10 ) . This step should be automated ( I 10 ) as human involvement would be impractical with the amount of data . The evaluation of incoming data and preprocessing measures may be guided by what has been previously performed to assess data quality and to preprocess data ( I 10 ) . For instance , sensitive attributes may be removed as it may be done in the initial pre - processing ( I 10 ) . Retraining should be initiated in case a fairness issue is repeatedly reported ( I 3 ) or regularly based on feedback data from model use ( I 4 ) . It should also be systematically checked whether the issue is removed with retraining ( I 1 ) . For utilizing feedback data in the most effective way possible , an active learning approach may give directions on what is needed for improvements regarding biases ( I 4 ) .