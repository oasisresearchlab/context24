Learning Dynamic Coherence with Graph Attention Network for Biomedical Entity Linking Mumeng Bo Department Computer Science of Technology Beijing Institute of Technology Beijing , China mumeng bo @ bit . edu . cn Meihui Zhang Department Computer Science of Technology Beijing Institute of Technology Beijing , China meihui zhang @ bit . edu . cn Abstract —Biomedical entity linking , which aligns various dis - ease mentions in unstructured documents to their corresponding standardized entities in a knowledge base ( KB ) , is an essential task in biomedical natural language processing . Unlike in general domain , the speciﬁc challenge is that biomedical entities often have many variations in their surface forms , and there are limited biomedical corpora for learning the correspondence . Recently , biomedical entity linking has been shown to signiﬁcantly beneﬁt from neural - based deep learning approaches . However , existing works mostly have not exploited the topical coherence in their models . Moreover , most of the collective models use a sequence - based approach , which may generate an accumulation of errors and perform unnecessary computation over irrelevant entities . Most importantly , these models ignore the relationships among mentions within a single document , which are very useful for linking the entities . In this paper , we propose an effective graph attention neural network , which can dynamically capture the relationships between entity mentions and learn the coherence representation . Besides , unlike graph - based models in general domain , our model does not require large extra resources to learn representations . We conduct extensive experiments on two biomedical datasets . The results show that our model achieves promising results . I . I NTRODUCTION Biomedical Entity Linking ( Disease Normalization ) refers to the task of identifying mentions with different surface forms in biomedical texts to corresponding concepts in a knowledge base ( KB ) . It is an important task that is widely used in ﬁelds such as biomedical diagnosis [ 9 ] , biomedical data mining [ 2 ] and biomedical KB construction [ 11 ] . Entity linking in biomedical domain is more challenging than that in general domain . First , there are rich corpora resources for model training in general domain . However , in biomedical domain , the datasets are usually small and contain limited annotations , which makes learning less ef - fective . Moreover , unlike in general domain , there are fewer external resources such as KB , alias tables , and entity co - occurrence probabilities that can be utilized to help make linking decisions . Second , the mentions in general domain tend to have many types , so that the type information can be injected in the model and acts as a discriminative feature in the linking process . However , mentions in biomedical texts usually only refer to diseases or drugs , making it insigniﬁcant to use type for entity disambiguation . Further , biomedical KB ( such as UMLS 1 ) are less informative comparing to general KB . For example , some entities have no deﬁnitions or long descriptions in biomedical KB , which makes it challenging to learn entity representation . In addition , biomedical terms are usually long ( i . e . , consisting of multiple words ) and complex . Thus , ab - breviations are often used and misspellings occur easily . As such , biomedical terms often have many surface forms due to abbreviations , morphological variations , synonyms , different word orderings , etc . As a result , it is more difﬁcult to represent biomedical entities . Due to the difﬁculties mentioned above , existing entity linking models that are successful in general domain are not always effective when applying in biomedical domain in practice . In recent years , owing to the remarkable progress of deep learning , various learning - based methods have been proposed for entity linking . Existing methods [ 3 ] , [ 13 ] , [ 15 ] , [ 17 ] , [ 35 ] mainly combine morphological features and semantic features to represent biomedical entity , and utilize various neural structures , such as CNN [ 31 ] to learn local represen - tation . Most of these methods rely on the mention text or its surrounding context to make linking predictions and neglect the impact of other entities in the same document . NormCo [ 1 ] and NormCG [ 10 ] model biomedical entity linking as a sequential decision problem that disambiguates mentions by LSTM [ 19 ] or GRU [ 20 ] . They introduce a global coherence model , which can capture topical coherence of the mentions within a single document . However , such sequence - based models may suffer from the accumulation of errors and the information loss in the process of information transmission , leading to inaccurate linking decisions . Furthermore , these models sequentially compute the inﬂuence of the contextual entity mentions based on the positions of the mentions in the document without understanding the association between the mentions . For instance , even when a disease entity mention is irrelevant with its subsequent mention , it is part of the input sequence in the model since they are close in position in the text . A more recent work [ 21 ] introduces a GCN ( Graph Con - volutional Network ) - based method , which leverages MeSH’s graphical structure [ 23 ] to learn entity representation and the relationships between entities ( represented by the weights on 1 https : / / www . nlm . nih . gov / research / umls / index . html 978 - 0 - 7381 - 3366 - 9 / 21 / $ 31 . 00 ©2021 IEEE 2021 I n t e r n a t i o n a l J o i n t C o n f e r e n c e o n N e u r a l N e t w o r k s ( I J C NN ) | 978 - 1 - 6654 - 3900 - 8 / 21 / $ 31 . 00 © 2021 I EEE | D O I : 10 . 1109 / I J C NN 52387 . 2021 . 9533687 edges ) . However , the same entity pair in different documents may have different inﬂuence on each other in learning the coherence . As such , it is insufﬁcient to build a static graph model with ﬁxed weights associated to each entity mention pair . In this paper , we propose an effective biomedical entity linking model based on graph neural network ( GNN ) , in which linking decisions are made not merely by mention’s local information but also by the dynamically learnt intra - document relationships among entity mentions . Speciﬁcally , we model mentions as nodes in the graph and represent each node with its mention feature and context feature . In this way , we can learn better representations that capture the semantics in the context . In addition , we design a graph attention network ( GAT ) to dynamically capture the intra - document relationships ( represented as edges between the entity mention nodes ) , which can effectively indicate the inﬂuence of different contextual mentions on the target mention . It is worth noting that our model not only addresses the problems of sequence - based models mentioned above since entity coherence are well captured by the graph attention network , but also overcomes the shortcomings of static graph model with ﬁxed weights . In addition , unlike the graph - based entity linking model in general domain , we do not rely on external corpus resources , ensuring our model has good applicability in applications with limited access to external resources , for example , in biomedi - cal domain . Furthermore , through the multi - hop mechanism of graph neural network , we can learn the relationships of entity mentions more deeply and capture the topical coherence more accurately . The main contributions of our work are summarized as follows : • We identify the challenges in biomedical entity linking and discuss existing solutions with their limitations . • We propose a novel graph attention network that can dynamically learn the intra - document relationships of contextual mentions and effectively capture the topical coherence for biomedical entity linking . • We examine various optimizations ( e . g . , incorporating context feature in mention representation , injecting se - mantic relationship in graph attention network ) to further boost the model performance . • We conduct extensive experimental study on two biomed - ical benchmark datasets and the results validate the ef - fectiveness of our model . The rest of this paper is organized as follows : In Section II , we review related works . In Section III , we present the details of our proposed model . We analyse the experimental results in Section IV . Finally , we conclude and discuss future work in Section V . II . R ELATED WORK Biomedical entity linking is mainly divided into two steps . The ﬁrst step is to generate candidate entity sets from the KB , and we need to ensure that the recall of candidate entity sets is as high as possible . We often use rule - based methods to generate candidate entity sets . The next step is to rank candidate entity sets and select the most suitable biomedical standardized terminology for each entity mention . In the early stage , some methods [ 12 ] , [ 14 ] , [ 18 ] use rule - based approaches to do biomedical entity linking . However , these methods use manual designed approaches that can only capture the morphological features and can not generalize to new unseen datasets . What’s more , the rule - based approach is broad and non - speciﬁc ( e . g . cancer , without indicating exactly what kind of cancer ) . Then with the development of deep learning in the ﬁeld of biomedical nature language process , Leaman et al . [ 15 ] deal with problem by joint named entity recognition and entity linking , and also solve the problem of tokens not present in the mention by using TF - IDF - based representation [ 32 ] of word vector , which greatly improves the effect of entity linking compared with the traditional rule - based method . But this method is limited by the accuracy of the named entity recognition step . The subsequent method [ 33 ] uses Word2vec [ 34 ] and TreeLSTM [ 16 ] , which enables us to obtain richer semantic features and syntactic properties . Tested on the new dataset BC5CDR [ 28 ] , this method greatly improves the robustness of the biomedical entity linking algorithm . With the neural network structure is widely used in other ﬁelds of natural language processing , the use of convolutional neural networks to obtain text features has become widespread . Li et al . [ 35 ] capture the semantics and morphology of mention by using convolutional neural networks and rule - based methods . After that , Mondal et al . [ 17 ] use Triplet Network to learn the similarity of mention - candidate pair and optimize Triplet Network by distinguishing the distance of positive candidate entity and negative candidate entity for mention . In addition , similar to methods in general domain , we can also use the mention’s contextual coherence semantic information to further improve the effectiveness of biomedical entity linking . Wright et al . [ 1 ] use a bidirectional GRU [ 20 ] to capture the topical coherence form contextual mentions for making linking decisions . Tang et al . [ 10 ] combine the CNN - based ranking algorithm [ 35 ] and the NormCo algorithm [ 1 ] , which further expresses the semantics of mention , and also capture coherence semantics among contextual mentions . However , traditional learning - based methods only use context - independent word vector representation and do not use context - based word vector representation . With the application and development of context - based language representation models such as BERT [ 37 ] and ELMo [ 38 ] in the ﬁeld of na - ture language process , Ji et al . [ 3 ] propose an architecture that greatly enhances the effectiveness of the model by ﬁne - tuning the BERT [ 37 ] , BioBERT [ 6 ] and ClinicalBERT [ 39 ] . In addition , Schumacher et al . [ 40 ] use ELMo to represent entity mention and entity concept in KB . However , the shortcoming of these methods is that their models require longer training time , more parameters , and more hardware for training . In summary , all these methods aim at fully exploit the information of the entity mention , i . e . , to represent the mor - phological features and semantic features of the entity mention from different perspectives , such as the entity mention itself , the contextual sentences , the contextual entities , and the re - lationships between entities , so as to correct match with the standardized terminology in KB . III . M ETHOD In this section , we present the details of our proposed model . We ﬁrst formally deﬁne the biomedical entity linking problem . Given a set of biomedical documents D , each document d ∈ D contains a list of entity mentions m 1 , · · · , m i , · · · , m N . Biomedical entity linking is to align each m i in document d with their corresponding canonical entity e i in KB . The notations are summarized in Table I . Next we introduce the prepossessing and candidate genera - tion steps . Then , we describe our proposed biomedical entity linking model , including ( a ) mention representation and ( b ) multi - layer graph attention network . TABLE I T ABLE D EFINITION OF NOTATIONS . Notation Description D A set of biomedical documents d A document in biomedical datasets D m A entity which is to be disambiguated C A set of candidate entities e A candidate entity in C c Context feature for each mention m h A node in the graph g The coherence representation from multi - layer GAT A . Preprocessing Similar to previous works [ 1 ] , [ 3 ] , [ 10 ] , we ﬁrst preprocess the entity mentions in the unstructured document and the entity concepts in KB . Speciﬁcally , for each mention in the text , we ﬁrstly apply spelling correction to the text and then use Ab3P [ 4 ] toolkit to check the abbreviations and replace with the most likely long form . In addition , we lowercase all characters and remove punctuation and stopwords . Finally , we replace and normalize the representation of numbers in datasets to English forms . B . Candidate Generation The essence of entity linking is to calculate the similarity score for each entity mention in corpus with entity concepts in KB . However , it is infeasible to perform pair - wise similarity computation for all concepts in KB , especially when the KB is large . Therefore , for each entity mention m i in corpus , we ﬁrst generate a set of candidate entities C i = ( e i 1 , · · · , e i k ) from KB and only employ ranking model on candidate entities to reduce the computation cost . We consider both morphology feature and semantics sim - ilarity to construct the candidate entity sets . We ﬁrst apply multi - pass sieve system [ 5 ] to generate top k 1 entities as the candidates , which are similar in terms of morphology feature . We then generate candidates based on semantic similarity . Here we use BioWordVec [ 41 ] to encode entity mention and entity concept to capture the semantics , and calculate the cosine similarity between the mention and entity concepts in KB . The top k 2 concepts with the highest similarity score are chosen as candidates . We then integrate the k 1 candidate entity concepts based on morphology feature and the top k 2 candidate entity concepts based on semantic similarity . Finally , k = k 1 + k 2 entity concepts are selected as candidate sets . If there are overlapping entities in the two sets , we generate more candidate entity concepts based on the semantic similarity until we hit the target k . In this way , we get each mention m i in the corpus and its corresponding k candidate entity concepts C i = ( e i 1 , · · · , e i k ) from KB . C . Ranking Model After obtaining a list of candidate entities , we put them into the ranking model . In this step , we select the most suitable entity from a list of candidate entities by enhancing the topical coherence among contextual mentions within a single document . Fig . 1 illustrates the overall framework of our proposed graph - based ranking model . As shown , the model consists of three components : Mention Representation layer , Multi - layer GAT and Output layer . 1 ) Basic Graph Structure : Given a document d ∈ D , we construct a basic graph based on all entity mentions in d , where the nodes correspond to the mentions in the document , and the edges represent the relationships between contextual entity mentions . For each node in graph , we represent it by combining mention feature and context feature . Mention feature can be represented by the semantics of the current mention through BioWordVec [ 41 ] and context feature can be represented by the surrounding words . Here we denote the mention feature and context feature of m i as ˜ m i and c i , respectively . Thus , each node in the graph can be represented as : h i = ˜ m i + c i ( 1 ) After establishing the graph nodes , we build the adjacency matrix and the edges ( i . e . relationships ) between these nodes . We consider all mentions in current l - th layer of graph that expressed as nodes in each document as a sequence Z l = (cid:8) h l 1 , h l 2 , · · · , h l i , · · · , h l N (cid:9) , h l i ∈ R F , where N is the number of nodes and F is the node’s input feature dimension . For each current mention m i to be disambiguated , we establish adjacent edges between node h i and its neighborhood node h j , j ∈ N i , where N i = { i − q , · · · , i − 1 , i + 1 , · · · , i + q } . Here we simply use cosine similarity to establish a semantic relationship between the current node h i and the surrounding q nodes as r ij : r ij = cos ( h i , h j ) ( 2 ) Through this basic graph structure , we can capture the global topical coherence among various entity mentions by exploiting the inﬂuence of the adjacent entity mentions in a single document . During the training process , for every entity mention in every graph layer , we update the current node h i by Fig . 1 . The overall framework of the proposed model . capturing the state of the q surrounding nodes for coherence representation . 2 ) Graph Attention Network : In order to better learn the dynamic relationships between mentions , we design a graph attention mechanism to differentiate the more coherent men - tions . Here we introduce graph attention network [ 7 ] , a novel neural network architecture that operates on graph - structured data . We represent these relationships between contextual entity mentions and the current disambiguation mention m i by ( implicitly ) associating different weights to different neigh - boring nodes . Specially , in the l - th layer , we use a set of node features , Z l = (cid:8) h l 1 , h l 2 , · · · , h li , · · · , h lN (cid:9) , as input for graph attention network structure . For each node h li ∈ Z l , the input of our graph attention network layer is h li with its q neighboring node features from Z l . In the graph attention network structure , we assign W i ∈ R F (cid:48) × F and W j ∈ R F (cid:48) × F , where F (cid:48) is the node’s output feature dimension , to be the weight matrix applied on current node and its neighboring nodes . We also deﬁne graph attention mechanism → a ∈ R 2 F (cid:48) + 1 , which is the attention weight vector of a single - layer feed forward neural network between node i and one of its neighboring node j ( j ∈ N i ) . Then we calculate the correlation between two nodes as follows : e ( i , j ) = σ ( → a ( W i h i (cid:107) W j h j (cid:107) r ij ) ) ( 3 ) where Wh i , Wh j ∈ R F (cid:48) , and r ij represents semantic rela - tionship between two nodes and (cid:107) denotes the concatenation operation of two input vectors . σ ( · ) is a nonlinear activation function ELU . To make coefﬁcients easily comparable among different nodes , we apply a softmax function on the attention coefﬁcient e ( i , j ) to do a normalization , and we get α ( i , j ) : α ( i , j ) = softmax ( e ( i , j ) ) = exp ( e ( i , j ) ) (cid:80) k ∈ N i exp ( e ( i , k ) ) ( 4 ) which ensures the attention coefﬁcient of node i sums up to 1 . As shown in Fig . 2 , we clearly illustrate how to represent the graph attention structure α ( i , j ) between node i and node j . Here we further illustrate the attention propagation between mention i and its neighboring mentions N i . For example , Fig . 3 illustrates that h l 3 will capture its surrounding mentions h l 1 to h l 5 . And we will get a next ( l + 1 ) - th layer’s new node h l + 1 3 corresponding to mention m 3 . In order to show the relationships between two nodes from multiple perspectives , we use multi - head attention ( with M = Fig . 2 . An illustration of attention between node i and node j . 8 heads ) mechanism for each link from a neighboring mention node j ∈ N i to node i . h ( l + 1 ) i = M (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m = 1 σ   (cid:88) j ∈ N i α ( m ) ( i , j ) W ( m ) h j   ( 5 ) Then the features from multi - head attention are concatenated to obtain the output to ( l + 1 ) - th layer , denoted as h l + 1 i . After updating the features of each node in Z l , the next ( l + 1 ) - th layer produce a new set of node features , Z l + 1 = (cid:8) h l + 1 1 , h l + 1 2 , · · · , h l + 1 i , · · · , h l + 1 N (cid:9) , h i ∈ R F (cid:48) , which will be returned for the input of the next layer . After passing through the n - th layer graph attention neural network , we fully capture coherence representation g 1 , g 2 , · · · , g i , · · · , g N corresponding to every entity mention with a single document . 3 ) Output Layer : After obtaining the coherence represen - tation , we combine the local feature to further improve the effect of entity ranking . Here we get score for each candidate entity e ij by a distance function as follows : Ψ ( m i , e ij ) = λ · d ( g i , e ij ) + ( 1 − λ ) · d ( m i , e ij ) ( 6 ) where e ij ∈ C i is candidate entity for m i . The ﬁrst part coherence model is used to capture global document - level coherence of the context entities , and the latter local model is used to calculate the semantic similarity between mention and entity in a given KB . In addition , we use λ to balance coherence model and local model . Moreover , we use d ( · ) to represent Euclidean distance . Finally , we get the ranking score of each candidate entity in C i corresponding to mention m i . D . Training For training , we adopt a triplet [ 42 ] ranking loss function to make the score of positive examples higher than the score of negative examples . The objective function is as follows : θ ∗ = arg θ (cid:88) d ∈ D (cid:88) m ∈ d (cid:88) e ∈ C min ( 0 , γ + Ψ ( m , e + ) − Ψ ( m , e − ) ) ( 7 ) where γ stands for the margin hyper - parameter . D repre - sents the training set , which contains thousands of documents . e + and e − represent mention’s positive example and negative Fig . 3 . An illustration of attention propagation by node 3 on its neighborhood . example respectively . Speciﬁcally , we choose the ground truth as the positive example e + , and the candidate entities set ( excluding the correct entity ) as the negative examples e − . Our goal is to ﬁnd the optimal parameters θ that minimize our objective function eqn . ( 7 ) . IV . E XPERIMENTAL ANALYSIS In this section , We ﬁrst introduce the datasets and evaluation criteria used in experiments . After that , we describe the param - eters and settings of our experiments in detail . Experimental results show the effectiveness of our proposed model . Finally , we make an experimental analysis about our proposed method . A . Datasets and Evalution In this section , we present the dataset and evalution metrics used in our experiments . 1 ) Datasets : We evaluate our model on two benchmarks NCBI [ 8 ] and BC5CDR [ 28 ] for biomedical entity link - ing . Here we use the following benchmarks with ofﬁcial train / dev / test splits . We can see the statistics of each dataset in Table II . TABLE II D ATASET S TATISTICS . Corpus NCBI BC5CDR ( Disease ) train dev test train dev test Documents 592 100 100 950 50 500 Mentions 5134 787 964 8006 420 4424 Concepts 11915 171203 • NCBI Disease corpus : NCBI [ 8 ] is one of the most popular datasets to evaluate the biomedical entity linking model . The NCBI Disease Corpus contains 793 PubMed abstracts in total . This corpus was manually annotated by 14 experts and contains a total of 6892 disease entities as well as 790 unique disease concepts . Each disease entity mention in NCBI was assigned a unique disease identiﬁer in MEDIC [ 25 ] , which is a disease dictionary that contains human - related genes and genetic disorders in the OMIM [ 27 ] database and disease category in the MeSH [ 26 ] . MEDIC disease dictionary contains a total of 11 , 915 concept unique ID and 71 , 923 synonyms . • BioCreative V CDR : BioCreative V CDR ( BC5CDR ) [ 28 ] , a dataset originally designed to extract chemical - disease relation ( CDR ) , can also be used to evaluate the biomedical entity linking task . In our paper , we use disease mentions for evaluation . The dataset contains 1500 PubMed abstracts , of which 1000 are used for training and 500 for testing . We further divide up the training dataset of BC5CDR into 950 for training , 50 for validation . Similar to the NCBI dataset , disease mentions in BC5CDR are labeled by the corresponding diseases in the MEDIC disease dictionary . 2 ) Competitive Models and Evaluation Metrics : To analyze and show the performance of our model , we compare our model with the following state - of - the - art methods . • Sieve - based Model [ 5 ] : The best rule - based system use multi - pass sieve based on manual rules to measure the morphological similarities between mention and entity in KB . • DNorm [ 15 ] : A deep learning - based method uses vec - tor space to represent mention and similarity matrix to measure mention and standard entity in KB . • TaggerOne [ 36 ] : A machine learning - based method uses semi - Markov approach to solve biomedical entity linking problem by joint named entity recognition and disam - biguation . • CNN - based Ranking [ 35 ] : A deep learning - based method uses CNN architecture to represent the semantics of biomedical entities and then rank candidate entities . • BERT - based Ranking [ 3 ] : Fine - tuning on the BERT - based pre - trained language model and then rank candidate entities . • NormCo [ 1 ] : A sequence - based global model uses Bi - LSTM for coherence representation . • Local Model : A baseline model only uses BioWordVec [ 41 ] to represent entity mention text and then rank candidate entities . Similar to previous works , we use accuracy ( computed as the fraction of correctly linked entity mentions in the corpus ) as the evaluation metric to measure the performance of different models . B . Experimental settings For each mention m in documents , we use the top k = 30 entity candidates generated by candidate generation steps fol - lowing section III - B . Moreover , for a mention corresponding to multiple different concepts in KB , we handle it as a mention corresponding to a unique concept in KB by splitting the original mention . We set h = 200 as representation dimen - sion and used BioWordVec [ 41 ] word embeddings trained on large unlabeled biomedical text . To learn the context feature representation of mention , we choose 10 words in the context surrounding the mention . What’s more , we use q = 3 contextual mentions as current mention’s neighboring nodes . In addition , We use the values of following parameters : ( learning rate : 1e - 5 , batch size : 41 , dropout rate for our model : 0 . 1 , γ : 14 . 1 ) . We implemented our model in Tensorﬂow and run experiments on a Titan X GPU . The source code and trained models will be released . C . Overall Performance To demonstrate the effectiveness of enriching entity rela - tionships with graph attention network for biomedical entity linking we designed , we evaluate the performance of our model with previous methods on two benchmarks ( test set ) . The accuracy for other methods are directly obtained from the original papers . TABLE III P ERFORMANCE OF DIFFERENT MODELS . Model name NCBI BC5CDR ( Disease ) DNorm 82 . 20 87 . 9 Sieve - based Model 84 . 65 84 . 1 TaggerOne 87 . 70 88 . 9 CNN - based Ranking 86 . 10 − BERT - based Ranking 89 . 06 − NormCo 87 . 80 88 . 0 Local Model 86 . 44 87 . 1 Our model 89 . 69 89 . 3 As shown in Table III , our model produces superior results on both NCBI and BC5CDR ( Disease ) datasets . Especially , we outperform the state - of - the - art BERT - based method [ 3 ] on accuracy by 0 . 6 % on NCBI dataset , which indicates that we successfully capture topical coherence by capturing entity rela - tionships , thus improving the disambiguation performance . As for the local baseline model , we can see that our model greatly outperforms it . Thus , we can conclude that the contextual mentions can actually help to make correct linking decision . In addition , our model is better than sequence - based model NormCo [ 1 ] by 1 . 3 % on BC5CDR ( Disease ) dataset , which indicates the relationships between contextual mentions are necessary for coherence model . D . Ablation Study In order to analyze the effect of each component of our model , we conduct an ablation study for our proposed model by removing individual component of our model as shown in Table IV . TABLE IV A BLATION STUDY . Model NCBI BC5CDR ( Disease ) - Context Feature 88 . 17 88 . 9 - Semantic Relationship 89 . 21 89 . 1 - Multi - layer GAT 86 . 58 87 . 2 Our model 89 . 69 89 . 3 Firstly , context feature can make the same mention have different representations in different contexts , which makes the Fig . 4 . Performance on datasets with different layers . representation of the mention more accurate . Context feature can serve as a plug - and - enhance component of existing men - tion representation , and removing it can decrease by 1 . 54 % and 0 . 4 % respectively on NCBI and BC5CDR ( Disease ) datasets . This illustrates that the mention’s context in NCBI dataset has more disambiguation information and stronger relevance to mention . Secondly , equipped with the semantic relationship by eqn . ( 2 ) , graph attention network will capture more relevant mention in a single document . Without this com - ponent , the accuracy will drop by 0 . 34 % on average . It also demonstrates the effectiveness of our semantic relationship injection in biomedical entity linking system clearly . Thirdly , the novel designed coherence representation can effectively capture topical coherence among various entity mentions in the entire document . If we remove the Multi - layer GAT com - ponent , the accuracy of our model will drop markedly , 3 . 11 % o and 2 . 1 % on the NCBI and BC5CDR ( Disease ) datasets , respectively . This indicates that the coherence representation is an indispensable and critical step in biomedical entity linking . In conclusion , these components are necessary and have positive contributions to the effectiveness of our proposed model . E . Performance on datasets with different layers To understand how the structure of our model captures the correlation between entities , we analyse our model perfor - mance on datasets with different layers . As shown in Fig . 4 , our model achieves the best performance in the 2 - th layer of the network in NCBI dataset and the 3 - th layer in BC5CDR dataset . This reﬂects the superiority of a multi - layer GAT structure for making linking decisions . With deeper layers of GAT , we can not only capture more information of the contextual mentions , but also we can better update the information that determines the current node . We change the relationship weights between mentions by updating them on different layers , so that we can fully capture the complex relationships between richer contextual mentions . However , as the number of layers increases , the accuracy rises initially and then degrades . There are several reasons for this phenomenon : Firstly , in biomedical domain , there is an over - ﬁtting [ 43 ] problem mainly because the biomedical dataset is too small , which is our limitation in the biomedical domain . What’s more , for the NCBI dataset , due to the small number of mentions contained in each document in the datasets , it is difﬁcult for us to capture the contextual rela - tionships by using more mention information in the context . In order to further improve the effectiveness of our model , we will utilize more typical biomedical datasets for training . Secondly , as the number of network layers increases , the deep Multi - layer GAT will have the over - smooth [ 29 ] problem . We capture large range contextual mention’s information in the neighborhood , so that the coherence representation of all mentions in our document tends to be the same . Thus the hidden layer representations of each node will tend to the same representation , which will cause bad performance . This be - cause Multi - layer GAT should capture collective information from a suitable range and appropriate degree among neighbor entity nodes . For future work , we will introduce the idea of residual [ 30 ] to solve the problem of the number of layers of graph neural networks . F . Parameters and Testing Time In this section , we compare our model’s parameters and testing time with BERT - based ranking model and sequence - based NormCo model . As shown in Table V , the third column to the sixth column show the the testing time of these models on the CPU and GPU . The CPU we used is single Intel ( R ) Core ( TM ) i7 - 5930K CPU @ 3 . 50GHz , and the GPU is described in section IV - B . TABLE V N UMBER OF PARAMETERS AND OBSERVED TESTING TIME . Model Parameters NCBI BC5CDR ( Disease ) GPU CPU GPU CPU BERT 340M 292s 343s 1121s 1409s NormCo 43 . 5M 31s 42s 95s 113s Our model 28 . 9M 25s 36s 73s 97s As we can see , although BERT - based model [ 3 ] has made remarkable achievements in the past , the parameters of BERT - based model is much greater than that other models , which also reﬂects that the more parameters , the better the perfor - mance in deep learning . From Table V , our model uses the least parameters while has the best efﬁciency on both CPU and GPU . What’s more , compared with sequence - based NormCo model [ 1 ] , our model uses fewer parameters , and outperforms it by 1 . 89 % which means that we save unnecessary parameters , making contextual entity mentions more conducive to linking decisions . V . C ONCLUSION AND FUTURE WORK In this paper , we present a novel graph attention neural network to model topical coherence for biomedical entity linking . Particularly , we use graph attention network to enrich the relationships between any two entity mentions and ex - tract the most useful coherence representation through multi - hop mechanism . Experiments conducted on two benchmarks validate the effectiveness of our model both in accuracy and efﬁciency . For future works , extensions can be made to introduce synonym corpus and incorporate the entity relationships from knowledge bases to get more effective model training for biomedical entity linking . R EFERENCES [ 1 ] Wright D . NormCo : Deep disease normalization for biomedical knowl - edge base construction [ D ] . UC San Diego , 2019 . [ 2 ] Rjeily C B , Badr G , El Hassani A H , et al . Medical data mining for heart diseases and the future of sequential mining in medical ﬁeld [ M ] / / Machine Learning Paradigms . Springer , Cham , 2019 : 71 - 99 . [ 3 ] Ji Z , Wei Q , Xu H . Bert - based ranking for biomedical entity normal - ization [ J ] . AMIA Summits on Translational Science Proceedings , 2020 , 2020 : 269 . [ 4 ] Sohn S , Comeau D C , Kim W , et al . Abbreviation deﬁnition identiﬁ - cation based on automatic precision estimates [ J ] . BMC bioinformatics , 2008 , 9 ( 1 ) : 402 . [ 5 ] D’Souza J , Ng V . Sieve - based entity linking for the biomedical do - main [ C ] / / Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) . 2015 : 297 - 302 . [ 6 ] Lee J , Yoon W , Kim S , et al . BioBERT : a pre - trained biomedical language representation model for biomedical text mining [ J ] . Bioinfor - matics , 2020 , 36 ( 4 ) : 1234 - 1240 . [ 7 ] Veliˇckovi´c P , Cucurull G , Casanova A , et al . Graph attention networks [ J ] . arXiv preprint arXiv : 1710 . 10903 , 2017 . [ 8 ] Do˘gan R I , Leaman R , Lu Z . NCBI disease corpus : a resource for disease name recognition and concept normalization [ J ] . Journal of biomedical informatics , 2014 , 47 : 1 - 10 . [ 9 ] Kose U , Deperlioglu O , Alzubi J , et al . Deep Learning Architectures for Medical Diagnosis [ M ] / / Deep Learning for Medical Decision Support Systems . Springer , Singapore , 2021 : 15 - 28 . [ 10 ] Tang C , Chen W , Wang T , et al . NormCG : A Novel Deep Learning Model for Medical Entity Linking [ M ] / / Intelligent Data Engineering and Analytics . Springer , Singapore , 565 - 573 . [ 11 ] Callahan T J , Tripodi I J , Pielke - Lombardo H , et al . Knowledge - Based Biomedical Data Science [ J ] . Annual Review of Biomedical Data Science , 2020 , 3 . [ 12 ] Ning K , Bharat S , Zubair A , et al . Using rule - based natural language processing to improve disease normalization in biomedical text [ J ] . Journal of American Medical Informatics Association Jamia , 2013 , 20 ( 5 ) : 876 - 881 . [ 13 ] Zhu M , Celikkaya B , Bhatia P , et al . Latte : Latent type modeling for biomedical entity linking [ C ] / / Proceedings of the AAAI Conference on Artiﬁcial Intelligence . 2020 , 34 ( 05 ) : 9757 - 9764 . [ 14 ] D’Souza J , Ng V . Sieve - based entity linking for the biomedical do - main [ C ] / / Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) . 2015 : 297 - 302 . [ 15 ] Leaman R , Islamaj Do˘gan R , Lu Z . DNorm : disease name normalization with pairwise learning to rank [ J ] . Bioinformatics , 2013 , 29 ( 22 ) : 2909 - 2917 . [ 16 ] Liu W , Liu P , Yang Y , et al . An Attention - Based Syntax - Tree and Tree - LSTM Model for Sentence Summarization [ J ] . International Journal of Performability Engineering , 2017 , 13 ( 5 ) . [ 17 ] Mondal I , Purkayastha S , Sarkar S , et al . Medical entity linking using triplet network [ J ] . arXiv preprint arXiv : 2012 . 11164 , 2020 . [ 18 ] Ghiasvand O , Kate R . UWM : Disorder Mention Extraction from Clinical Text Using CRFs and Normalization Using Learned Edit Distance Patterns [ C ] / / SemEval 2014 . 2014 . [ 19 ] Hochreiter S , Schmidhuber J . Long short - term memory [ J ] . Neural com - putation , 1997 , 9 ( 8 ) : 1735 - 1780 . [ 20 ] Dey R , Salem F M . Gate - variants of gated recurrent unit ( GRU ) neural networks [ C ] / / 2017 IEEE 60th international midwest symposium on circuits and systems ( MWSCAS ) . IEEE , 2017 : 1597 - 1600 . [ 21 ] Pujary D , Thorne C , Aziz W . Disease Normalization with Graph Embeddings [ C ] / / Proceedings of SAI Intelligent Systems Conference . Springer , Cham , 2020 : 209 - 217 . [ 22 ] Grover A , Leskovec J . node2vec : Scalable feature learning for net - works [ C ] / / Proceedings of the 22nd ACM SIGKDD international con - ference on Knowledge discovery and data mining . 2016 : 855 - 864 . [ 23 ] Coletti M H , Bleich H L . Medical subject headings used to search the biomedical literature [ J ] . Journal of the American Medical Informatics Association , 2001 , 8 ( 4 ) : 317 - 323 . [ 24 ] Scarselli F , Gori M , Tsoi A C , et al . The graph neural network model [ J ] . IEEE Transactions on Neural Networks , 2008 , 20 ( 1 ) : 61 - 80 . [ 25 ] Davis A P , Wiegers T C , Rosenstein M C , et al . MEDIC : a practical dis - ease vocabulary used at the Comparative Toxicogenomics Database [ J ] . Database , 2012 , 2012 . [ 26 ] Lipscomb C E . Medical subject headings ( MeSH ) [ J ] . Bulletin of the Medical Library Association , 2000 , 88 ( 3 ) : 265 . [ 27 ] Hamosh A , Scott A F , Amberger J S , et al . Online Mendelian Inheri - tance in Man ( OMIM ) , a knowledgebase of human genes and genetic disorders [ J ] . Nucleic acids research , 2005 , 33 ( suppl 1 ) : D514 - D517 . [ 28 ] Li J , Sun Y , Johnson R J , et al . BioCreative V CDR task corpus : a resource for chemical disease relation extraction [ J ] . Database , 2016 , 2016 . [ 29 ] Chen D , Lin Y , Li W , et al . Measuring and relieving the over - smoothing problem for graph neural networks from the topological view [ C ] / / Proceedings of the AAAI Conference on Artiﬁcial Intelligence . 2020 , 34 ( 04 ) : 3438 - 3445 . [ 30 ] Li G , Muller M , Thabet A , et al . Deepgcns : Can gcns go as deep as cnns ? [ C ] / / Proceedings of the IEEE International Conference on Computer Vision . 2019 : 9267 - 9276 . [ 31 ] Albawi S , Mohammed T A , Al - Zawi S . Understanding of a convolu - tional neural network [ C ] / / 2017 International Conference on Engineering and Technology ( ICET ) . IEEE , 2017 : 1 - 6 . [ 32 ] Aizawa A . An information - theoretic perspective of tf – idf measures [ J ] . Information Processing and Management , 2003 , 39 ( 1 ) : 45 - 65 . [ 33 ] Liu H , Xu Y . A deep learning way for disease name representation and normalization [ C ] / / National CCF conference on natural language processing and Chinese computing . Springer , Cham , 2017 : 151 - 157 . [ 34 ] Goldberg Y , Levy O . word2vec Explained : deriving Mikolov et al . ’s negative - sampling word - embedding method [ J ] . arXiv preprint arXiv : 1402 . 3722 , 2014 . [ 35 ] Li H , Chen Q , Tang B , et al . CNN - based ranking for biomedical entity normalization [ J ] . Bmc Bioinformatics , 2017 , 18 ( S11 ) : 385 . [ 36 ] Leaman R , Lu Z . TaggerOne : joint named entity recognition and nor - malization with semi - Markov Models [ J ] . Bioinformatics , 2016 , 32 ( 18 ) : 2839 - 2846 . [ 37 ] Kenton J D M W C , Toutanova L K . BERT : Pre - training of Deep Bidi - rectional Transformers for Language Understanding [ C ] / / Proceedings of NAACL - HLT . 2019 : 4171 - 4186 . [ 38 ] Peters M , Neumann M , Iyyer M , et al . Deep Contextualized Word Representations [ C ] / / Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) . 2018 : 2227 - 2237 . [ 39 ] Alsentzer E , Murphy J , Boag W , et al . Publicly Available Clinical BERT Embeddings [ C ] / / Proceedings of the 2nd Clinical Natural Language Processing Workshop . 2019 : 72 - 78 . [ 40 ] Schumacher E , Mulyar A , Dredze M . Clinical Concept Linking with Contextualized Neural Representations [ C ] / / Proceedings of the 58th An - nual Meeting of the Association for Computational Linguistics . 2020 : 8585 - 8592 . [ 41 ] Zhang Y , Chen Q , Yang Z , et al . BioWordVec , improving biomedical word embeddings with subword information and MeSH [ J ] . Scientiﬁc data , 2019 , 6 ( 1 ) : 1 - 9 . [ 42 ] Hoffer E , Ailon N . Deep metric learning using triplet net - work [ C ] / / International Workshop on Similarity - Based Pattern Recogni - tion . Springer , Cham , 2015 : 84 - 92 . [ 43 ] Panchal G , Ganatra A , Shah P , et al . Determination of over - learning and over - ﬁtting problem in back propagation neural network [ J ] . International Journal on Soft Computing , 2011 , 2 ( 2 ) : 40 - 51 .