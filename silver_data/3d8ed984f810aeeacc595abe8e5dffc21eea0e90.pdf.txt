a r X i v : 1204 . 3982v1 [ m a t h . O C ] 1 8 A p r 2012 Adaptive Restart for Accelerated Gradient Schemes Brendan O’Donoghue Emmanuel Cand ` es May 10 , 2014 Abstract In this paper we demonstrate a simple heuristic adaptive restart technique that can dra - matically improve the convergence rate of accelerated gradient schemes . The analysis of the technique relies on the observation that these schemes exhibit two modes of behavior depending on how much momentum is applied . In what we refer to as the ‘high momentum’ regime the iterates generated by an accelerated gradient scheme exhibit a periodic behavior , where the period is proportional to the square root of the local condition number of the objective function . This suggests a restart technique whereby we reset the momentum whenever we observe periodic behavior . We provide analysis to show that in many cases adaptively restarting allows us to recover the optimal rate of convergence with no prior knowledge of function parameters . 1 Introduction Accelerated gradient schemes were ﬁrst proposed by Yurii Nesterov in 1983 , [ 14 ] . He demonstrated a simple modiﬁcation to gradient descent that could obtain provably optimal performance for the complexity class of ﬁrst - order algorithms applied to minimize smooth convex functions . The method , and its successors , are often referred to as ‘accelerated methods’ . In recent years there has been a resurgence of interest in ﬁrst - order optimization methods [ 19 , 16 , 21 , 1 , 12 ] , driven primarily by the need to solve very large problem instances unsuited to second - order methods . Accelerated gradient schemes can be thought of as momentum methods , in that the step taken at the current iteration depends on the previous iterations , and where the momentum grows from one iteration to the next . When we refer to restarting the algorithm we mean starting the algorithm again , taking the current iteration as the new starting point . This erases the memory of previous iterations and resets the momentum back to zero . Unlike gradient descent , accelerated methods are not guaranteed to be monotone in the objective value . A common observation when running an accelerated method is the appearance of ripples or bumps in the trace of the objective value ; these are seemingly regular increases in the objective , see Figure ( 1 ) for an example . In this paper we demonstrate that this behavior occurs when the momentum has exceeded a critical value ( the optimal momentum value derived by Nesterov in [ 15 ] ) and that the period of these ripples is proportional to the square - root of the ( local ) condition number of the function . Separately , we show that the optimal restart interval is also proportional to the square root of the condition number . Combining these results we show that restarting when we observe an increase in the function value allows us to recover the optimal linear convergence rate in many cases . Indeed if the function is locally well - conditioned we can use restarting to obtain a linear convergence rate inside the well - conditioned region . 1 Smooth unconstrained optimization . We wish to minimize a smooth convex function of a variable x ∈ R n [ 3 ] , minimize f ( x ) ( 1 ) where f : R n → R has a Lipschitz continuous gradient with constant L , i . e . , k∇ f ( x ) − ∇ f ( y ) k 2 ≤ L k x − y k 2 , ∀ x , y ∈ R n . We shall denote by f ⋆ the optimal value of the above optimization problem , if the minimizer exists and is unique then we shall write it as x ⋆ . Further , a function is said to be strongly convex if there exists a µ > 0 such that f ( x ) ≥ f ⋆ + ( µ / 2 ) k x − x ⋆ k 22 , ∀ x ∈ R n , where µ is referred to as the strong convexity parameter . The condition number of a smooth , strongly convex function is L / µ . 2 Accelerated methods Accelerated ﬁrst - order methods to solve ( 1 ) were ﬁrst developed by Nesterov [ 14 ] , this scheme is from [ 15 ] : Algorithm 1 Accelerated scheme I Require : x 0 ∈ R n , y 0 = x 0 , θ 0 = 1 and q ∈ [ 0 , 1 ] 1 : for k = 0 , 1 , . . . do 2 : x k + 1 = y k − t k ∇ f ( y k ) 3 : θ k + 1 solves θ 2 k + 1 = ( 1 − θ k + 1 ) θ 2 k + qθ k + 1 4 : β k + 1 = θ k ( 1 − θ k ) / ( θ 2 k + θ k + 1 ) 5 : y k + 1 = x k + 1 + β k + 1 ( x k + 1 − x k ) 6 : end for There are many variants of the above scheme , see , e . g . , [ 21 , 16 , 12 , 1 , 2 ] . Note that by setting q = 1 in the above scheme we recover gradient descent . For a smooth convex function the above scheme converges for any t k ≤ 1 / L ; setting t k = 1 / L and q = 0 obtains a guaranteed convergence rate of f ( x k ) − f ⋆ ≤ 4 L k x 0 − x ⋆ k 2 ( k + 2 ) 2 . ( 2 ) If the function is also strongly convex with parameter µ , then a choice of q = µ / L ( the reciprocal of the condition number ) will achieve f ( x k ) − f ⋆ ≤ L (cid:18) 1 − r µ L (cid:19) k k x 0 − x ⋆ k 2 . ( 3 ) This is often referred to as linear convergence . With this convergence rate we can achieve an accuracy of ǫ in O s L µ log 1 ǫ ! ( 4 ) iterations . 2 In the case of a strongly convex function the following simpler scheme obtains the same guar - anteed rate of convergence [ 15 ] : Algorithm 2 Accelerated scheme II Require : x 0 ∈ R n , y 0 = x 0 1 : for k = 0 , 1 , . . . do 2 : x k + 1 = y k − ( 1 / L ) ∇ f ( y k ) 3 : y k + 1 = x k + 1 + β ⋆ ( x k + 1 − x k ) 4 : end for Where we set β ⋆ = 1 − p µ / L 1 + p µ / L . ( 5 ) Note that in Algorithm 1 , using the optimal choice q = µ / L , we have that β k ↑ β ⋆ . Taking β k to be a momentum parameter , then for a strongly convex function β ⋆ is the maximum amount of momentum we should apply ; when we have a value of β higher than β ⋆ we refer to it as ‘high momentum’ . We shall return to this point later . The convergence of these schemes is optimal in the sense of the lower complexity bounds derived by Nemirovski and Yudin in [ 13 ] . However , this convergence is only guaranteed when the function parameters µ and L are known in advance . 2 . 1 Robustness A natural question to ask is how robust are accelerated methods to errors in the estimates of the Lipschitz constant L and strong convexity parameter µ ? For the case of an unknown Lipschitz constant we can estimate the optimal step - size by the use of backtracking ; see , e . g . , [ 21 , 19 ] . Estimating the strong convexity parameter is much more challenging . Estimating the strong convexity parameter . In [ 16 ] Nesterov demonstrated a method to bound µ , similar to the backtracking scheme for L described above . His scheme achieves a conver - gence rate quite a bit slower than Algorithm 1 with a known value of µ . In practice , we often assume or guess that µ is zero , which corresponds to setting q = 0 in Algorithm 1 . Indeed many discussions of accelerated algorithms do not even include a q term ; the original algorithm in [ 14 ] did not use a q . However , this can dramatically slow down the convergence of the algorithm . Figure 1 shows Algorithm 1 applied to minimize a positive deﬁnite quadratic function in n = 200 dimensions , with optimal choice of q being q ⋆ = µ / L = 4 . 1 × 10 − 5 ( a condition number of about 2 . 4 × 10 4 ) , and step size t = 1 / L . Each trace is the progress of the algorithm with a diﬀerent choice of q ( hence a diﬀerent estimate of µ ) . We observe that slightly over or underestimating the optimal value of q for the function can have a severe detrimental eﬀect on the rate of convergence of the algorithm . We also note the clear diﬀerence in behavior between the cases where we underestimate and where we overestimate q ⋆ ; in the latter we observe monotonic convergence but in the former we notice the appearance of regular ripples or bumps in the traces . 3 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 k f ( x k ) − f ⋆ q = 0 q = q ⋆ / 10 q = q ⋆ / 3 q = q ⋆ q = 3 q ⋆ q = 10 q ⋆ q = 1 Figure 1 : Convergence of Algorithm 1 with diﬀerent estimates of q . Interpretation . The optimal momentum depends on the condition number of the function ; speciﬁcally , higher momentum is required when the function has a higher condition number . Under - estimating the amount of momentum required leads to slower convergence . However we are more often in the other regime , that of overestimated momentum , because generally q = 0 , in which case β k ↑ 1 ; this corresponds to high momentum and rippling behavior , as we see in Figure 1 . This can be visually understood in Figure ( 2 ) , which shows the trajectories of sequences generated by Algorithm 1 minimizing a positive deﬁnite quadratic in two dimensions , under q = q ⋆ , the optimal choice of q , and q = 0 . The high momentum causes the trajectory to overshoot the minimum and oscillate around it . This causes a rippling in the function values along the trajectory . Later we shall demonstrate that the period of these ripples is proportional to the square root of the ( local ) condition number of the function . Lastly we mention that the condition number is a global parameter ; the sequence generated by an accelerated scheme may enter regions that are locally better conditioned , say , near the optimum . In these cases the choice of q = q ⋆ is appropriate outside of this region , but once we enter it we expect the rippling behavior associated with high momentum to emerge , despite the optimal choice of q . 4 −0 . 1 −0 . 05 0 0 . 05 0 . 1 −0 . 1 −0 . 05 0 0 . 05 0 . 1 x 1 x 2 q = q ⋆ q = 0 Figure 2 : Sequence trajectories under Algorithm 1 . 3 Restarting 3 . 1 Fixed restart For strongly convex functions an alternative to choosing the optimal value of q is to use restarting , [ 16 , 10 ] . One example of a ﬁxed restart scheme is as follows : Algorithm 3 Fixed restarting Require : x 0 ∈ R n , y 0 = x 0 , θ 0 = 1 1 : for j = 0 , 1 , . . . do 2 : carry out Algorithm 1 with q = 0 for k steps 3 : set x 0 = x k , y 0 = x k and θ 0 = 1 . 4 : end for We restart the algorithm every k iterations , taking as our starting point the last point produced by the algorithm , where k is a ﬁxed restart interval . In other words we ‘forget’ all previous iterations and reset the momentum back to zero . Optimal ﬁxed restart interval . We can obtain an upper bound on the optimal restart interval . If we restart every k iterations we have , at outer iteration j , inner loop iteration k ( just before a restart ) , f ( x ( j + 1 , 0 ) ) − f ⋆ = f ( x ( j , k ) ) − f ⋆ ≤ 4 L k x ( j , 0 ) − x ⋆ k / k 2 ≤ (cid:16) 8 L / µk 2 (cid:17) ( f ( x ( j , 0 ) ) − f ⋆ ) , where the ﬁrst inequality is the convergence guarantee of Algorithm 1 , and the second comes from the strong convexity of f . So after jk steps we have f ( x ( j , 0 ) ) − f ⋆ ≤ (cid:16) 8 L / µk 2 (cid:17) j ( f ( x ( 0 , 0 ) ) − f ⋆ ) . 5 If we assume we have jk = c total iterations and we wish to minimize ( 8 L / µk 2 ) j over j and k jointly , we obtain k ⋆ = e q 8 L / µ . ( 6 ) Using this as our restart interval we obtain an accuracy of ǫ in less than O ( p L / µ log ( 1 / ǫ ) ) iterations , i . e . , the optimal linear convergence rate as in equation ( 4 ) . The drawbacks in using ﬁxed restarts are that ﬁrstly it depends on unknown parameters L and , more importantly , µ , and secondly it is a global parameter that may be inappropriate in better conditioned regions . 3 . 2 Adaptive restart The above analysis suggests that an adaptive restart technique may be useful . In particular we want a scheme that makes some computationally cheap observation and decides whether or not to restart based on that observation . In this paper we suggest two schemes that perform well in practice and provide some analysis to show accelerated convergence when these schemes are used . • Function scheme : we restart whenever f ( x k ) > f ( x k − 1 ) . • Gradient scheme : we restart whenever ∇ f ( y k − 1 ) T ( x k − x k − 1 ) > 0 . Empirically we observe that these two schemes perform similarly well . The gradient scheme has two advantages over the function scheme . Firstly near to the optimum the gradient scheme may be more numerically stable . Secondly all quantities involved in the gradient scheme are already calculated in accelerated schemes , so no extra computation is required . We can give rough justiﬁcations for each scheme . The function scheme restarts at the bottom of the troughs as in Figure 1 , thereby avoiding the wasted iterations where we are moving away from the optimum . The gradient scheme restarts whenever the momentum term and the negative gradient are making an obtuse angle . In other words we restart when the momentum seems to be taking us in a bad direction , as measured by the negative gradient at that point . Figure 3 shows the eﬀect of diﬀerent restart intervals on minimizing a positive deﬁnite quadratic function in n = 500 dimensions . In this particular case the upper bound on the optimal restart interval is every 700 iterations . We note that when this interval is used the convergence is better than when no restart is used , however not as good as using the optimal choice of q . We also note that restarting every 400 iterations performs about as well as restarting every 700 iterations , suggesting that the optimal restart interval is somewhat lower than 700 . We have also plotted the performance of the two adaptive restart schemes . The performance is on the same order as the algorithm with the optimal q and much better than using the ﬁxed restart interval . ( Conjugate gradient methods , [ 11 ] , will generally outperform an accelerated gradient scheme when minimizing a quadratic ; we use quadratics here simply for illustrative purposes . ) Figure 4 demonstrates the function restart scheme trajectories in the two dimensional example , restarting resets the momentum and prevents the characteristic spiralling behavior . 6 200 400 600 800 1000 1200 1400 1600 1800 2000 10 −18 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 Gradient descent No Restart Restart every 100 Restart every 400 Restart every 1000 Restart optimal , 700 With q = µ / L Adaptive Restart − Function scheme Adaptive Restart − Gradient scheme k f ( x k ) − f ⋆ Figure 3 : Comparison of ﬁxed and adaptive restart intervals . −0 . 1 −0 . 05 0 0 . 05 0 . 1 −0 . 1 −0 . 05 0 0 . 05 0 . 1 x 1 x 2 q = q ⋆ q = 0 adaptive restart Figure 4 : Sequence trajectories under scheme I and with adaptive restart . 7 4 Analysis In this section we consider applying an accelerated scheme to minimizing a positive deﬁnite quadratic . We shall see that once the momentum is larger than a critical value we observe periodicity in the iterates . We use this to prove linear convergence when using adaptive restarting . The analysis presented in this section is similar in spirit to the analysis of the heavy ball method in [ 18 , § 3 . 2 ] . 4 . 1 Minimizing a quadratic Consider minimizing a convex quadratic . Without loss of generality we can assume that f has the following form : f ( x ) = ( 1 / 2 ) x T Ax where A ∈ R n × n is positive deﬁnite and symmetric . In this case x ⋆ = 0 and f ⋆ = 0 . We have strong convexity parameter µ = λ min > 0 and L = λ max , where λ min and λ max are the minimum and maximum eigenvalues of A , respectively . 4 . 2 The algorithm as a linear dynamical system We shall assume a ﬁxed step - size t = 1 / L for simplicity . Given quantities x 0 and y 0 = x 0 , Algorithm 1 is carried out as follows , x k + 1 = y k − ( 1 / L ) Ay k y k + 1 = x k + 1 + β k ( x k + 1 − x k ) . For the rest of the analysis we shall take β k to be constant and equal to some β for all k . This is a somewhat crude approximation , but by making it we can show that there are two regimes of behavior of the system , depending on the value of β . Consider the eigenvector decomposition of A = V Λ V T . Denote by w k = V T x k , v k = V T y k . In this basis the update equations can be written w k + 1 = v k − ( 1 / L ) Λ v k v k + 1 = w k + 1 + β ( w k + 1 − w k ) . These are n independently evolving dynamical systems . The i th system evolves according to w k + 1 i = v ki − ( λ i / L ) v ki v k + 1 i = w k + 1 i + β ( w k + 1 i − w ki ) , where λ i is the i th eigenvalue of A . Eliminating the sequence v ( k ) i from the above we obtain the following recurrence relation for the evolution of w i : w k + 2 i = ( 1 + β ) ( 1 − λ i / L ) w k + 1 i − β ( 1 − λ i / L ) w ki , k = 0 , 1 , . . . , where w 0 i is known and w 1 i = w 0 i ( 1 − λ i / L ) , i . e . , a gradient step from w 0 i . The update equation for v i is identical , diﬀering only in the initial conditions , v k + 2 i = ( 1 + β ) ( 1 − λ i / L ) v k + 1 i − β ( 1 − λ i / L ) v ki , k = 0 , 1 , . . . , where v 0 i = w 0 i and v 1 i = ( ( 1 + β ) ( 1 − λ i / L ) − β ) v 0 i . 8 4 . 3 Convergence properties The behavior of this system is determined by the characteristic polynomial of the recurrence rela - tion , r 2 − ( 1 + β ) ( 1 − λ i / L ) r + β ( 1 − λ i / L ) . ( 7 ) Let β ⋆i be the critical value of β for which this polynomial has repeated roots , i . e . , β ⋆i : = 1 − p λ i / L 1 + p λ i / L . If β ≤ β ⋆i then the polynomial ( 7 ) has two real roots , r 1 and r 2 , and the system evolves according to [ 7 ] w ki = c 1 r k 1 + c 2 r k 2 . ( 8 ) When β = β ⋆i the roots coincide at the point r ⋆ = ( 1 + β ) ( 1 − λ i / L ) / 2 = ( 1 − p λ i / L ) ; this corresponds to critical damping . We have the fastest monotone convergence at rate ∝ ( 1 − p λ i / L ) k . Note that if λ i = µ then β ⋆ i is the optimal choice of β as given by equation ( 5 ) and the convergence rate is the optimal rate , as given by equation ( 3 ) . This is the case because , as we shall see , the smallest eigenvalue will come to dominate the convergence of the entire system . If β < β ⋆i we are in the low momentum regime , and we say the system is over - damped . The convergence rate is dominated by the larger root , which is greater than r ⋆ , i . e . , the system exhibits slow monotone convergence . If β > β ⋆i then the roots of the polynomial ( 7 ) are complex ; we are in the high momentum regime and the system is under - damped and exhibits periodicity . In that case the characteristic solution is given by [ 7 ] w ki = c i ( β ( 1 − λ i / L ) ) k / 2 ( cos ( kψ i − δ i ) ) where ψ i = cos − 1 ( ( 1 − λ i / L ) ( 1 + β ) / 2 q β ( 1 − λ i / L ) ) . and δ i and c i are constants that depend on the initial conditions ; in particular for β ≈ 1 we have δ i ≈ 0 and we will ignore it . Similarly , v ki = ˆ c i ( β ( 1 − λ i / L ) ) k / 2 (cid:16) cos ( kψ i − ˆ δ i ) (cid:17) where ˆ δ i and ˆ c i are constants , and again ˆ δ i ≈ 0 . For small θ we know that cos − 1 ( √ 1 − θ ) ≈ √ θ , and therefore if λ i ≪ L , then ψ i ≈ q λ i / L . In particular the frequency of oscillation for the mode corresponding to the smallest eigenvalue µ is approximately given by ψ µ ≈ p µ / L . To summarize , based on the value of β we observe the following behaviors : • β > β ⋆i : high momentum , under - damped • β < β ⋆i : low momentum , over - damped • β = β ⋆ i : optimal momentum , critically damped . 9 4 . 4 Observable quantities We don’t observe the evolution of the modes , but we can observe the evolution of the function value ; which is given by f ( x k ) = n X i = 1 ( w ki ) 2 λ i . and if β > β ⋆ = ( 1 − p µ / L ) / ( 1 + p µ / L ) we are in the high momentum regime for all modes and thus f ( x k ) = n X i = 1 ( w ki ) 2 λ i ≈ n X i = 1 ( w 0 i ) 2 λ i β k ( 1 − λ i / L ) k cos 2 ( kψ i ) . The function value will quickly be dominated by the smallest eigenvalue and we have that f ( w k ) ≈ ( w 0 µ ) 2 µβ k ( 1 − µ / L ) k cos 2 (cid:18) k q µ / L (cid:19) , ( 9 ) where we have replaced ψ µ with p µ / L , and we are using the subscript µ to denote those quantities corresponding to that mode . A similar analysis for the gradient restart scheme yields ∇ f ( y k ) T ( x k + 1 − x k ) ≈ µv kµ ( w k + 1 µ − w kµ ) ∝ β k ( 1 − µ / L ) k sin ( 2 k q µ / L ) . ( 10 ) In other words observing the quantities in ( 9 ) or ( 10 ) we expect to see oscillations at a frequency proportional to p µ / L , i . e . , the frequency of oscillation is telling us something about the condition number of the function . 4 . 5 Convergence with adaptive restart If we apply Algorithm 1 with q = 0 to minimize a quadratic we start with β 0 = 0 , i . e . , the system is in the low momentum , monotonic regime . Eventually β k becomes larger than β ⋆ and we enter the high momentum , oscillatory regime . It takes about ( 3 / 2 ) p L / µ iterations for β k to exceed β ⋆ . After that the system is under - damped and the iterates obey equations ( 9 ) and ( 10 ) . Under either adaptive restart scheme , equations ( 9 ) and ( 10 ) indicate that we shall observe the restart condition after a further ( π / 2 ) p L / µ iterations . We restart and the process begins again , with β k set back to zero . Thus under either scheme we restart approximately every k ⋆ = π + 3 2 s L µ iterations ( cf . , the upper bound on optimal ﬁxed restart interval ( 6 ) ) . Following a similar derivation to § 3 . 1 , this restart interval guarantees us an accuracy of ǫ within O ( p L / µ log ( 1 / ǫ ) ) iterations , i . e . , we have recovered the optimal linear convergence rate of equation ( 4 ) via adaptive restarting , with no prior knowledge of µ . 4 . 6 Extension to smooth convex minimization In many cases the function we are minimizing is well approximated by a quadratic near the optimum , i . e . , there is a region inside of which f ( x ) ≈ f ( x ⋆ ) + ( x − x ⋆ ) T ∇ 2 f ( x ⋆ ) ( x − x ⋆ ) , 10 and loosely speaking we are minimizing a quadratic . Once we are inside this region we will observe behavior consistent with the analysis above , and we can exploit this behavior to achieve fast con - vergence by using restarts . Note that the Hessian at the optimum may have smallest eigenvalue λ min > µ , the global strong convexity parameter , in other words we can achieve a faster local convergence than even if we had exact knowledge of the global parameter . This result is similar in spirit to the restart method applied to the non - linear conjugate gradient method , where it is desir - able to restart the algorithm once it reaches a region in which the function is well approximated by a quadratic [ 17 , § 5 . 2 ] . The eﬀect of these restart schemes outside of the quadratic region is unclear . In practice we observe that restarting based on one of the criteria described above is almost always helpful , even far away from the optimum . However , we have observed cases where restarting far from the optimum can slow down the early convergence slightly , until the quadratic region is reached and the algorithm enters the rapid linear convergence phase . 5 Numerical examples In this section we describe three further numerical examples that demonstrate the improvement of accelerated algorithms under an adaptive restarting technique . 5 . 1 Log - sum - exp Here we minimize a smooth convex function that is not strongly convex . Consider the following optimization problem minimize ρ log (cid:16)P mi = 1 exp (cid:16) ( a Ti x − b i ) / ρ (cid:17)(cid:17) where x ∈ R n . The objective function is smooth , but not strongly convex , it grows linearly asymptotically . Thus , the optimal value of q in Algorithm 1 is zero . The quantity ρ controls the smoothness of the function , as ρ → 0 , f ( x ) → max i = 1 , . . . , m ( a Ti x − b i ) . As it is smooth , we expect the region around the optimum to be well approximated by a quadratic ( we consider only examples where the optimal value is ﬁnite ) , and thus we expect to eventually enter a region where our restart method will obtain linear convergence without any knowledge of where this region is , the size of the region or the local function parameters within this region . For smaller values of ρ the smoothness of the objective function decreases and thus we expect to take more iterations before we enter the region of linear convergence . As a particular example we took n = 20 and m = 100 ; we generated the a i and b i randomly . Figure 5 demonstrates the performance of four diﬀerent schemes for four diﬀerent values of ρ . We selected the step size for each case using backtracking . We note that both restart schemes perform well , eventually beating both gradient descent and the accelerated scheme . Both the function and gradient schemes eventually enter a region of fast linear convergence . For large ρ we see that even gradient descent performs well , as , similar to the restarted method , it is able to automatically exploit the local strong convexity of the quadratic region around the optimum . Notice also the appearance of the periodic behavior . 11 0 200 400 600 800 1000 10 −15 10 −10 10 −5 10 0 0 200 400 600 800 1000 10 −15 10 −10 10 −5 10 0 0 200 400 600 800 1000 10 −15 10 −10 10 −5 10 0 0 200 400 600 800 1000 10 −15 10 −10 10 −5 10 0 k k k k ( f k − f ⋆ ) / f ⋆ ( f k − f ⋆ ) / f ⋆ ( f k − f ⋆ ) / f ⋆ ( f k − f ⋆ ) / f ⋆ ρ = 0 . 05 ρ = 0 . 1 ρ = 0 . 5 ρ = 1 Figure 5 : Minimizing a smooth but not strongly convex function ; the black line is gradient descent , the blue line is Algorithm 1 , the red line is the function adaptive restart scheme , the green line is the gradient adaptive restart scheme . 5 . 2 Sparse linear regression Consider the following optimization problem : minimize ( 1 / 2 ) k Ax − b k 22 + ρ k x k 1 , ( 11 ) over x ∈ R n , where A ∈ R m × n and in general n ≫ m . This is a widely studied problem in the ﬁeld of compressed sensing , see e . g . , [ 4 , 9 , 5 , 20 ] . Loosely speaking problem ( 11 ) seeks a sparse vector with a small measurement error . The quantity ρ trades oﬀ these two competing objectives . The iterative soft - threshold algorithm ( ISTA ) can be used to solve ( 11 ) [ 6 , 8 ] . ISTA relies on the soft - thresholding operator : T α ( x ) = sign ( x ) max ( | x | − α , 0 ) , where all the operations are applied elementwise . The ISTA algorithm , with constant step - size t , is given by 12 Algorithm 4 ISTA Require : x ( 0 ) ∈ R n 1 : for k = 0 , 1 , . . . do 2 : x k + 1 = T ρt ( x k − tA T ( Ax k − b ) ) . 3 : end for The convergence rate of ISTA is guaranteed to be at least O ( 1 / k ) , making it analogous to gradient descent . The fast iterative soft thresholding algorithm ( FISTA ) was developed in [ 2 ] ; a similar algo - rithm was also developed by Nesterov in [ 16 ] . FISTA essentially applies acceleration to the ISTA algorithm ; it is carried out as follows , Algorithm 5 FISTA Require : x ( 0 ) ∈ R n , y 0 = x 0 and θ 0 = 1 1 : for k = 0 , 1 , . . . do 2 : x k + 1 = T ρt ( y k − tA T ( Ay k − b ) ) 3 : θ k + 1 = ( 1 + q 1 + 4 θ 2 k ) / 2 4 : β k + 1 = ( θ k − 1 ) / θ k + 1 5 : y k + 1 = x k + 1 + β k + 1 ( x k + 1 − x k ) . 6 : end for For any choice of t ≤ 1 / λ max ( A T A ) FISTA obtains a convergence rate of at least O ( 1 / k 2 ) . The objective in problem ( 11 ) is non - smooth , so it does not ﬁt the class of problems we are considering in this paper . However we are seeking a sparse solution vector x , and we note that once the non - zero basis of the solution has been identiﬁed we are essentially minimizing a quadratic . Thus we expect that after a certain number of iterations adaptive restarting can provide linear convergence . It is easy to show that the function adaptive restart scheme can be performed without an extra application of the matrix A , which is the costly operation in the algorithm . In performing FISTA we do not evaluate a gradient , however FISTA can be thought of as a generalized gradient scheme , in which we take x k + 1 = T λt ( y k − tA T ( Ay k − b ) ) : = y k − tG ( y k ) to be a generalized gradient step , where G ( y k ) is the generalized gradient at y k . In this case the gradient restart scheme amounts to restarting whenever G ( y k ) T ( x k + 1 − x k ) > 0 , or equivalently ( y k − x k + 1 ) T ( x k + 1 − x k ) > 0 . ( 12 ) We generated data for the numerical instances as follows . Firstly the entries of A were sampled from a standard normal distribution . We then randomly generated a sparse vector y with n entries , only s of which were non - zero . We then set b = Ay + w , where the entries in w were IID sampled from N ( 0 , 0 . 1 ) . This ensured that the solution vector x ⋆ is approximately s - sparse . We chose ρ = 1 and the step size t = 1 / λ max ( A T A ) . Figure 6 shows the dramatic speedup that adaptive restarting can provide , for two diﬀerent examples . 13 0 500 1000 1500 2000 2500 3000 3500 4000 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 Adaptive restart − Function scheme Adaptive restart − Gradient scheme FISTAISTA 0 500 1000 1500 2000 2500 3000 3500 4000 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 Adaptive restart − Function scheme Adaptive restart − Gradient scheme FISTAISTA k k ( f ( x k ) − f ⋆ ) / f ⋆ ( f ( x k ) − f ⋆ ) / f ⋆ n = 2000 , m = 100 , s = 20 n = 2000 , m = 500 , s = 100 Figure 6 : Adaptive restarting applied to the FISTA algorithm . 5 . 3 Quadratic programming Consider the following quadratic program , minimize ( 1 / 2 ) x T Qx + q T x subject to a ≤ x ≤ b , ( 13 ) over x ∈ R n , where Q ∈ R n × n is positive deﬁnite and a , b ∈ R n are ﬁxed vectors . The constraint inequalities are to be interpreted element - wise , and we assume that a < b . We denote by Π C ( z ) the projection of a point z onto the constraint set , which amounts to thresholding the entries in z . Projected gradient descent can solve ( 13 ) ; it is carried out as follows , x k + 1 = Π C (cid:16) x k − t ( Qx k + q ) (cid:17) . Projected gradient descent obtains a guaranteed convergence rate of O ( 1 / k ) . Acceleration has been successfully applied to the projected gradient method , [ 16 , 2 ] . Algorithm 6 Accelerated projected gradient Require : x 0 ∈ R n , y 0 = x 0 and θ 0 = 1 1 : for k = 0 , 1 , . . . do 2 : x k + 1 = Π C (cid:16) y k − t ( Qy k + q ) (cid:17) 3 : θ k + 1 solves θ 2 k + 1 = ( 1 − θ k + 1 ) θ 2 k 4 : β k + 1 = θ k ( 1 − θ k ) / ( θ 2 k + θ k + 1 ) 5 : y k + 1 = x k + 1 + β k + 1 ( x k + 1 − x k ) 6 : end for For any choice of t ≤ 1 / λ max ( Q ) accelerated projected gradient schemes obtain a convergence rate of at least O ( 1 / k 2 ) . 14 The presence of constraints make this a non - smooth optimization problem , however once the constraints that are active have been identiﬁed the problem reduces to minimizing a quadratic on a subset of the variables , and we expect adaptive restarting to increase the rate of convergence . As in the sparse regression example we can use the generalized gradient in our gradient based restart scheme , i . e . , we restart based on condition ( 12 ) . As a ﬁnal example , we set n = 500 and generate Q and q randomly ; Q has a condition number of 10 7 . We take b to be the vector of all ones , and a to be that of all negative ones . For information , the solution to this problem has 70 active constraints . The step - size is set to t = 1 / λ max ( Q ) for all algorithms . Figure 7 shows the performance of projected gradient descent , accelerated projected gradient descent , and the two restart techniques . 200 400 600 800 1000 1200 1400 1600 1800 2000 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 Adaptive restart − Function scheme Adaptive restart − Gradient scheme Accelerated projected gradient descent Projected gradient descent k ( f ( x k ) − f ⋆ ) / f ⋆ Figure 7 : Adaptive restarting applied to the accelerated projected gradient algorithm . 6 Summary In this paper we have demonstrated a simple heuristic adaptive restart technique that can improve the convergence performance of accelerated gradient schemes for smooth convex optimization . We restart the algorithm whenever we observe a certain condition on the objective function value or gradient value . We provided some qualitative analysis to show that we can recover the optimal linear rate of convergence in many cases ; in particular near the optimum of a smooth function we can potentially dramatically accelerate the rate of convergence , even if the function is not glob - ally strongly convex . We demonstrated the performance of the scheme on some simple numerical examples . 15 Acknowledgments We are very grateful to Stephen Boyd for his help and encouragement . We would also like to thank Stephen Wright for his advice and feedback , and Stephen Becker and Michael Grant for useful discussions . E . C . would like to thank the ONR ( grant N00014 - 09 - 1 - 0258 ) and the Broadcom Foundation for their support . References [ 1 ] A . Auslender and M . Teboulle . Interior gradient and proximal methods for convex and conic optimization . SIAM J . Opt . , 16 ( 3 ) : 697 – 725 , 2006 . [ 2 ] A . Beck and M . Teboulle . A fast iterative shrinkage - thresholding algorithm for linear inverse problems . SIAM J . Img . Sci . , 2 : 183 – 202 , March 2009 . [ 3 ] S . Boyd and L . Vandenberghe . Convex optimization . Cambridge University Press , 2004 . [ 4 ] E . Cand ` es , J . Romberg , and T . Tao . Stable signal recovery from incomplete and inaccurate measurements . Communications on Pure and Applied Mathematics , 59 ( 8 ) : 1207 – 1223 , August 2006 . [ 5 ] E . Cand ` es and M . Wakin . An introduction to compressive sampling . Signal Processing Mag - azine , IEEE , 25 ( 2 ) : 21 – 30 , March 2008 . [ 6 ] A . Chambolle , R . De Vore , N . Lee , and B . Lucier . Nonlinear wavelet image processing : Vari - ational problems , compression , and noise removal through wavelet shrinkage . IEEE Transac - tions on Image Processing , 7 ( 3 ) : 319 – 335 , March 1998 . [ 7 ] A . Chiang . Fundamental Methods of Mathematical Economics . McGraw - Hill , 1984 . [ 8 ] I . Daubechies , M . Defrise , and C . De Mol . An iterative thresholding algorithm for linear inverse problems with a sparsity constraint . Communications on Pure and Applied Mathematics , 57 ( 11 ) : 1413 – 1457 , November 2004 . [ 9 ] D . Donoho . Compressed sensing . IEEE Transactions on Information Theory , 52 ( 4 ) : 1289 – 1306 , April 2006 . [ 10 ] M . Gu , L . Lim , and C . Wu . PARNES : A rapidly convergent algorithm for accurate recovery of sparse and approximately sparse signals . http : / / arxiv . org / abs / 0911 . 0492 , 2009 . Technical report . [ 11 ] M . Hestenes and E . Stiefel . Methods of conjugate gradients for solving linear systems . Journal of Research of the National Bureau of Standards , 49 ( 6 ) : 409 – 436 , December 1952 . [ 12 ] G . Lan , Z . Lu , and R . Monteiro . Primal - dual ﬁrst - order methods with o ( 1 / ǫ ) iteration - complexity for cone programming . Mathematical Programming , 2009 . [ 13 ] A . Nemirovski and D . Yudin . Problem complexity and method eﬃciency in optimization . Wiley - Interscience series in discrete mathematics . John Wiley & Sons , 1983 . 16 [ 14 ] Y . Nesterov . A method of solving a convex programming problem with convergence rate O ( 1 / k 2 ) . Soviet Mathematics Doklady , 27 ( 2 ) : 372 – 376 , 1983 . [ 15 ] Y . Nesterov . Introductory lectures on convex optimization : A basic course . Kluwer Academic Publishers , 2004 . [ 16 ] Y . Nesterov . Gradient methods for minimizing composite objective function . http : / / www . ecore . be / DPs / dp _ 1191313936 . pdf , 2007 . CORE discussion paper . [ 17 ] J . Nocedal and S . Wright . Numerical optimization . Springer series in operations research . Springer , 2000 . [ 18 ] B . Polyak . Introduction to optimization . Translations series in mathematics and engineering . Optimization Software , Publications Division , 1987 . [ 19 ] M . Grant S . Becker , E . Cand ` es . Templates for convex cone problems with applications to sparse signal recovery . Mathematical Programming Computation , 3 ( 3 ) : 165 – 218 , August 2011 . [ 20 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58 ( 1 ) : 267 – 288 , February 1994 . [ 21 ] P . Tseng . On accelerated proximal gradient methods for convex - concave optimization . https : / / www . math . washington . edu / ~ tseng / papers / apgm . pdf , 2008 . 17