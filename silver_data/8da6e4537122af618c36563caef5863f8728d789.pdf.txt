a r X i v : 2308 . 13479v1 [ c s . C L ] 25 A ug 2023 Prompting a Large Language Model to Generate Diverse Motivational Messages A Comparison with Human - Written Messages Samuel Rhys Cox samcox @ comp . nus . edu . sg National University of Singapore Ashraf Abdul ashrafabdul @ pm . me Unaﬃliated Wei Tsang Ooi ooiwt @ comp . nus . edu . sg National University of Singapore ABSTRACT Large language models ( LLMs ) are increasingly capable and preva - lent , and can be used to produce creative content . The quality of content is inﬂuenced bythe promptused , withmore speciﬁcprompts that incorporate examples generally producing better results . On from this , it couldbe seen thatusing instructions writtenforcrowd - sourcing tasks ( that are speciﬁc and include examples to guide workers ) could prove eﬀective LLM prompts . To explore this , we used a previous crowdsourcing pipeline that gave examples to peo - ple to help them generate a collectively diverse corpus of motiva - tional messages . We then used this same pipeline to generate mes - sages using GPT - 4 , and compared the collective diversity of mes - sages from : ( 1 ) crowd - writers , ( 2 ) GPT - 4 using the pipeline , and ( 3 & 4 ) two baseline GPT - 4 prompts . We found that the LLM prompts using the crowdsourcing pipeline caused GPT - 4 to produce more diverse messages than the two baseline prompts . We also discuss implications from messages generated by both human writers and LLMs . CCS CONCEPTS • Human - centered computing → Empirical studies in HCI . KEYWORDS Large Language Models , Crowdsourcing , PromptEngineering , Cre - ativity 1 INTRODUCTION Large language models ( LLMs ) can be usedto producecreative con - tent [ 3 , 4 , 17 , 20 ] . For example , Karinshak et al . [ 17 ] used GPT - 3 to generate messages to persuade people to receive the Covid - 19 vaccine . Furthermore , the quality of LLM output is aﬀected by the prompt used [ 2 , 28 ] . Speciﬁcally , it has been found that LLMs func - tion better with few - shot prompts ( i . e . , instructions alongside ex - ample output ) [ 2 ] rather than using zero - shot prompts ( with no examples ) . This indicates that crowdsourcing instructions ( that of - ten include speciﬁc language , examples and atomised tasks ) could prove eﬀective as LLM prompts , and some prior work has used in - structions from crowd - sourcing tasks to prompt LLMs [ 4 , 29 ] . For example , Cegin et al . [ 4 ] found thatChatGPT couldparaphrase text in more diverse ways than the crowd , and Wu et al . investigated the ease in adopting multiple crowdsourcing pipelines into LLM prompts [ 29 ] . Similarly , recent studies have compared behaviour and task proﬁciency between humans and LLMs [ 18 , 27 ] , with some ﬁnding LLMs exceed crowd - workers in a number of tasks [ 12 , 13 , 26 ] ( such as data labelling [ 12 , 26 ] ) , while others have found humans more proﬁcient in nuanced creative tasks [ 15 , 24 ] . Simple - GPT prompt : You will now write 250 messages to motivate someone to exercise . Please use the following guidelines : - Write concise messages ( imagine the message could be sent to people on a smartphone ) . Messages should generally be 1 to 3 sentences long . - Output your response with one message per line . Diverse - Naïve - GPTprompt : You will now write 250 messages to motivate someone to exercise . Please use the following guidelines : - Write concise messages ( imagine the message could be sent to people on a smartphone ) . Messages should generally be 1 to 3 sentences long . - Ensure the messages are diverse from one another , by limiting message repetition and similarity ( suchasbylimiting repetition of themesor phras - ing ) . - Output your response with one message per line . Phrase - GPT prompt : Youwill now write250 messagesto motivatesomeone to exercise , andyou will beshown short phrasesto inspireyou . Please use the following guide - lines : - Use the phrases for inspiration to write a message that would motivate someone to exercise . - Write concise messages ( imagine the message could be sent to people on a smartphone ) . Messages should generally be 1 to 3 sentences long . - You can write using the entire phrases , or fragments of phrases . - Ignore phrases that you don’t ﬁnd relevant to writing motivational mes - sages for exercise ( but feel free to be creative if you can use any of the phrases to write your message ) . - Outputyourresponseincommaseparatedformatof : Inspirationalphrase , motivational message generated from phrase . Below are the 250 phrases to use : elite athletes often train pink name brand workout outﬁt care to use proper form · · · Figure 1 : Prompt instructions given to GPT - 4 . Instructions for crowd - workers ( similar to Phrase - GPT ) found in [ 7 ] . Building on this prior work , we investigate the use of a crowd - sourcing pipeline to generate a collectively diverse corpus of moti - vational messages to encourage physical activity . Speciﬁcally , we used the Directed Diversity [ 7 ] pipeline to prompt GPT - 4 [ 3 ] to generate motivational messages ( a complex creativity task ) by in - corporating short example phrases into each message . We found that GPT - 4 messages produced using [ 7 ] were more diverse than two baseline GPT - 4 prompts . Finally , we discuss insights from mes - sages generated by humans and GPT - 4 . 2 METHOD We comparedthe collectivediversity ofmotivationalmessages that were generated by human writers and by three diﬀerent GPT - 4 prompts via the ChatGPT web - UI ( see Figure 1 for GPT - 4 prompts ) . For the Human - Written condition , we used 250 messages written Samuel Rhys Cox , Ashraf Abdul , and Wei Tsang Ooi by crowd - workers ( taken from Cox et al . [ 7 ] ) . Here workers were shown one 3 to 5 word phrase to inspire them when writing each message . These phrases were chosen to be semantically diverse in order to create a collectively diverse corpus of messages . We used the same phrases and instructions to prompt GPT - 4 to write 250 messages ( in the Phrase - GPT condition ) . For comparison , we also prompted with a Simple - GPT condition : simply asking GPT - 4 to write 250 messages ; and a Diverse - Naïve - GPT condition : request - ing GPT - 4 to write 250 messages that are diverse from one another . Inspirational phrase Human - Written message Phrase - GPT message Ex . 1 lots of fear and intimidation If you can’t exercise outside , try listening to music while exercis - ing inside . It can help relax and motivate you at the same time . Overcome fear and intimidation inyourexercisejourneybyseek - ing support , educating yourself , and taking small steps towards your goals . Ex . 2 homeo - pathic doses of exercise Even small amounts of exercise can help you become healthy . You don’t need to dedicate 2 hoursadayto becomehealthier ! Start with small , manageable doses of exerciseto build a foun - dation for a healthier , more ac - tive lifestyle . Ex . 3 military press one rep Don’t focus on working out for 30 minutes , an hour , etc . Just focus on your exercises one repata time ! Challenge yourself with exercises like the military press , focusingonformandgrad - ually increasing weight for im - proved strength . Ex . 4 rats doing regular aerobic exercise Regular running and aerobic ex - ercise is great ! Never stop work - ing out . Take inspiration from studies , even those on rats , that show the beneﬁts of regular aerobic exercise for overall health and longevity . Ex . 5 vice president of program development Just 30 minutes of exercise a day can change your life ! Seek guidance from experienced professionalslikevicepresidents of program development to opti - mizeyourﬁtness routine and ap - proach . Table 1 : Example messages written by the crowd and GPT - 4 using the same phrases for inspiration . 3 RESULTS To calculate the diversity of each set of messages , we calculated the mean pairwise Euclidean distance [ 7 , 16 ] between all messages within each condition ( where a higher distance reﬂects more di - versity ) . From lowest to highest diversity , this gave us : 4 . 13 for Simple - GPT , 4 . 29for Naïve - Diverse - GPT , 5 . 66 for Phrase - GPT , and 6 . 90 for Human - Written . This indicates that such a crowd - sourcing pipeline could be used to increase the diversity of content generated by LLMs . While Phrase - GPT did not produce a corpus of messages as diverse as those from human - writers , this may be due to diﬀerences in message length ( with Human - Written averag - ing 24 . 0 words , and Phrase - GPT 18 . 7 words ) . In addition , Simple - GPT averaged 9 . 2 and Naïve - Diverse - GPT 9 . 8 words per message ( emphasising the impact of including examples when prompting LLMs ) . The work involved in producing messages should also be noted . The 250 human - written messages took on average 73 seconds each to be written , while GPT - 4 took roughly 6 seconds per message . Additionally , while some human - written messages in [ 7 ] were ex - cluded ( such as those using poor levels of English or apparent gibberish ) , the LLM - written messages seemingly suﬀered from no such issues . Example Human - Written and Phrase - GPT messages can be found in Table 1 alongside their respective inspirational phrases . 4 DISCUSSION AND CONCLUSION This study has demonstrated the eﬀectiveness of using a crowd - sourcing pipeline ( [ 7 ] ) to generate more diverse messages com - pared to two baseline prompts . However , similar to some previ - ous creativity tasks that require more advanced reasoning abilities [ 15 , 24 ] , human - writers were more successful than GPT - 4 . Further investigation could alter LLM parameters such as temperature ( de - fault 1 . 0 on ChatGPT [ 22 ] ) . Several additional insights are demonstrated by examples in Ta - ble 1 . Both human and LLM messages demonstrated the ability to draw metaphors from phrases ( Ex . 2 ) . GPT - 4 may have had diﬃ - culty deciding on the relevance of phrases and would generally incorporate phrases , while human writers would act more discern - ingly ( see Ex . 4 and Ex . 5 ) . This emphasises that LLMs follow the form rather than meaning of language [ 1 ] , and implies that crowd - sourcing pipelines could be atomised further for LLMs ( e . g . , includ - ing an initial step asking the LLM tojudge the relevance of a phrase to physical activity ) . At times , human - writers would not incorporate phrases ( per - hapsif they donothave domain knowledge ofmore esoteric phrases ) while the LLM could ( Ex . 3 ) . However , risk of AI hallucination ( e . g . , within healthcare [ 3 , 5 , 14 , 19 , 25 ] such as LLMs misunderstanding medical vocabulary or providing advice that does not follow med - ical guidelines [ 5 , 14 ] ) should be noted , and additional measures would be needed to ensure the veracity of output . Additionally , ( while one could prompt a LLM to incorporate dif - ferent conversational styles or sociocultural perspectives ) , atten - tion is needed to ensure that it does not produce harmful cultural stereotypes [ 10 , 11 , 21 ] . Similarly , human writers incorporate per - sonal experiences into messages [ 6 ] ( Ex . 1 ) that may not necessar - ily be available to a LLM ( if such experiences are poorly repre - sented ) . Finally , while we used automated measures to indicate diversity of each experiment condition , further human evaluations for diver - sity could be conducted [ 7 , 9 , 23 ] , in addition to human evaluations of message eﬃcacy ( e . g . , motivation [ 7 , 8 ] ) . ACKNOWLEDGMENTS This research is part of the programme DesCartes and is supported by the National Research Foundation , Prime Minister’s Oﬃce , Sin - gapore under its Campus for Research Excellence and Technologi - cal Enterprise ( CREATE ) programme . Prompting a Large Language Model to Generate Diverse Motivational Messages REFERENCES [ 1 ] Emily M Bender and Alexander Koller . 2020 . Climbing towards NLU : On meaning , form , and understanding in the age of data . In Proceedings of the 58th annual meeting of the association for computational linguistics . 5185 – 5198 . http : / / dx . doi . org / 10 . 18653 / v1 / 2020 . acl - main . 463 [ 2 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 . LanguageModels areFew - Shot Learners . Advances in Neural Information Processing Systems 33 ( 2020 ) , 1877 – 1901 . NeurIPS pdf link . [ 3 ] Sébastien Bubeck , Varun Chandrasekaran , Ronen Eldan , Johannes Gehrke , Eric Horvitz , Ece Kamar , Peter Lee , Yin Tat Lee , Yuanzhi Li , Scott Lundberg , et al . 2023 . Sparks of artiﬁcial general intelligence : Early experiments with gpt - 4 . arXiv preprint arXiv : 2303 . 12712 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 12712 [ 4 ] Jan Cegin , Jakub Simko , and Peter Brusilovsky . 2023 . ChatGPT to Re - place Crowdsourcing of Paraphrases for Intent Classiﬁcation : Higher Diver - sity and Comparable Model Robustness . arXiv preprint arXiv : 2305 . 12947 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2305 . 12947 [ 5 ] Shan Chen , Benjamin H Kann , Michael B Foote , Hugo JWL Aerts , Guer - gana K Savova , Raymond H Mak , and Danielle S Bitterman . 2023 . The util - ity of ChatGPT for cancer treatment information . medRxiv ( 2023 ) , 2023 – 03 . https : / / www . medrxiv . org / content / 10 . 1101 / 2023 . 03 . 16 . 23287316v1 [ 6 ] Heather L Coley , Rajani S Sadasivam , Jessica H Williams , Julie E Volkman , Yu - Mei Schoenberger , Connie L Kohler , Heather Sobko , Midge N Ray , Jeroan J Al - lison , Daniel E Ford , et al . 2013 . Crowdsourced Peer - Versus Expert - Written Smoking - Cessation Messages . American journal of preventive medicine 45 , 5 ( 2013 ) , 543 – 550 . https : / / doi . org / 10 . 1016 / j . amepre . 2013 . 07 . 004 [ 7 ] Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Christian von der Weth , and Brian Y . Lim . 2021 . Directed Diversity : Leveraging Language Em - bedding Distances for Collective Creativity in Crowd Ideation . In Proceed - ings of the 2021 CHI Conference on Human Factors in Computing Systems . https : / / doi . org / 10 . 1145 / 3411764 . 3445782 [ 8 ] Roelof AJ de Vries , Khiet P Truong , Sigrid Kwint , Constance HC Drossaert , and Vanessa Evers . 2016 . Crowd - Designed Motivation : Motivational Messages for Exercise Adherence Based on Behavior Change Theory . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , 297 – 308 . https : / / doi . org / 10 . 1145 / 2858036 . 2858229 [ 9 ] Steven P Dow , Alana Glassco , Jonathan Kass , Melissa Schwarz , Daniel L Schwartz , and Scott R Klemmer . 2010 . Parallel prototyping leads to bet - ter design results , more divergence , and increased self - eﬃcacy . ACM Transactions on Computer - Human Interaction ( TOCHI ) 17 , 4 ( 2010 ) , 18 . https : / / doi . org / 10 . 1145 / 1879831 . 1879836 [ 10 ] EsinDurmus , KarinaNyugen , ThomasILiao , NicholasSchiefer , AmandaAskell , Anton Bakhtin , Carol Chen , Zac Hatﬁeld - Dodds , Danny Hernandez , Nicholas Joseph , et al . 2023 . Towards Measuring the Representation of Subjective Global Opinions in Language Models . arXiv preprint arXiv : 2306 . 16388 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2306 . 16388 [ 11 ] Shangbin Feng , Chan Young Park , Yuhan Liu , and Yulia Tsvetkov . 2023 . From PretrainingDatatoLanguageModelstoDownstreamTasks : TrackingtheTrails of Political Biases Leading to Unfair NLP Models . In Proceedings of the 61st An - nual MeetingoftheAssociationforComputationalLinguistics ( Volume1 : LongPa - pers ) . AssociationforComputationalLinguistics , Toronto , Canada , 11737 – 11762 . https : / / doi . org / 10 . 18653 / v1 / 2023 . acl - long . 656 [ 12 ] FabrizioGilardi , MeysamAlizadeh , andMaëlKubli . 2023 . ChatGPToutperforms crowdworkersfortext - annotation tasks . ProceedingsoftheNationalAcademyof Sciences 120 , 30 ( 2023 ) , e2305016120 . https : / / doi . org / 10 . 1073 / pnas . 2305016120 [ 13 ] Perttu Hämäläinen , Mikke Tavast , and Anton Kunnari . 2023 . Evaluating Large Language Models in Generating Synthetic HCI Research Data : a Case Study . In Proceedings of the 2023 CHI Conference on Human Factorsin Computing Systems . 1 – 19 . https : / / doi . org / 10 . 1145 / 3544548 . 3580688 [ 14 ] Katharina Jeblick , Balthasar Schachtner , Jakob Dexl , Andreas Mittermeier , Anna Theresa Stüber , Johanna Topalis , Tobias Weber , Philipp Wesp , Bastian Sabel , Jens Ricke , et al . 2022 . ChatGPT Makes Medicine Easy to Swallow : An Exploratory Case Study on Simpliﬁed Radiology Reports . arXiv preprint arXiv : 2212 . 14882 ( 2022 ) . https : / / doi . org / 10 . 48550 / arXiv . 2212 . 14882 [ 15 ] Sophie Jentzsch and Kristian Kersting . 2023 . ChatGPT is fun , but it is not funny ! Humor is still challenging Large Language Models . arXiv preprint arXiv : 2306 . 04563 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2306 . 04563 [ 16 ] MariusKaminskasand DerekBridge . 2016 . Diversity , Serendipity , Novelty , and Coverage : A Survey and Empirical Analysis of Beyond - Accuracy Objectives in Recommender Systems . ACM Transactions on Interactive Intelligent Systems ( TiiS ) 7 , 1 ( 2016 ) , 1 – 42 . https : / / doi . org / 10 . 1145 / 2926720 [ 17 ] Elise Karinshak , Sunny Xun Liu , Joon Sung Park , and Jeﬀrey T Hancock . 2023 . Working With AI to Persuade : Examining a Large Language Model’s Ability to Generate Pro - Vaccination Messages . Proceedings of the ACM on Human - Computer Interaction 7 , CSCW1 ( 2023 ) , 1 – 29 . https : / / doi . org / 10 . 1145 / 3579592 [ 18 ] Andrew Kyle Lampinen . 2022 . Can language models handle recursivelynested grammaticalstructures ? A casestudy on comparing models and humans . arXiv preprint arXiv : 2210 . 15303 ( 2022 ) . https : / / doi . org / 10 . 48550 / arXiv . 2210 . 15303 [ 19 ] Peter Lee , Sebastien Bubeck , and Joseph Petro . 2023 . Beneﬁts , Limits , and Risks of GPT - 4 as an AI Chatbot for Medicine . New England Journal of Medicine 388 , 13 ( 2023 ) , 1233 – 1239 . https : / / doi . org / 10 . 1056 / NEJMsr2214184 [ 20 ] Sue Lim and Ralf Schmälzle . 2023 . Artiﬁcial intelligence for health mes - sage generation : an empirical study using a large language model ( LLM ) and prompt engineering . Frontiers in Communication 8 ( 2023 ) , 1129082 . https : / / doi . org / 10 . 3389 / fcomm . 2023 . 1129082 [ 21 ] Piotr Mirowski , Kory W Mathewson , Jaylen Pittman , and Richard Evans . 2023 . Co - WritingScreenplaysand TheatreScriptswith LanguageModels : Evaluation by Industry Professionals . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1 – 34 . https : / / doi . org / 10 . 1145 / 3544548 . 3581225 [ 22 ] OpenAI . 2023 . API Reference - OpenAI API . ( 2023 ) . https : / / platform . openai . com / docs / api - reference / chat / create # chat / create - temperature [ 23 ] Pao Siangliulue , Joel Chan , Steven P Dow , and Krzysztof Z Gajos . 2016 . IdeaHound : Improving Large - scale Collaborative Ideation with Crowd - Powered Real - time Semantic Modeling . In Proceedings of the 29th An - nual Symposium on User Interface Software and Technology . 609 – 624 . https : / / doi . org / 10 . 1145 / 2984511 . 2984578 [ 24 ] Claire Stevenson , Iris Smal , Matthijs Baas , Raoul Grasman , and Han van der Maas . 2022 . Putting GPT - 3’s Creativity to the ( Alternative Uses ) Test . arXiv preprint arXiv : 2206 . 08932 ( 2022 ) . https : / / doi . org / 10 . 48550 / arXiv . 2206 . 08932 [ 25 ] Ross Taylor , Marcin Kardas , Guillem Cucurull , Thomas Scialom , Anthony Hartshorn , Elvis Saravia , Andrew Poulton , Viktor Kerkez , and Robert Sto - jnic . 2022 . Galactica : A Large Language Model for Science . arXiv preprint arXiv : 2211 . 09085 ( 2022 ) . https : / / doi . org / 10 . 48550 / arXiv . 2211 . 09085 [ 26 ] Petter Törnberg . 2023 . ChatGPT - 4Outperforms Experts and CrowdWorkersin Annotating Political Twitter Messageswith Zero - Shot Learning . arXiv preprint arXiv : 2304 . 06588 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2304 . 06588 [ 27 ] Albert Webson , Alyssa Marie Loo , Qinan Yu , and Ellie Pavlick . 2023 . Are Language Models Worse than Humans at Following Prompts ? It’s Complicated . arXiv preprint arXiv : 2301 . 07085 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2301 . 07085 [ 28 ] Tongshuang Wu , Michael Terry , and CarrieJun Cai . 2022 . AI Chains : Transpar - entandControllableHuman - AIInteraction byChainingLargeLanguageModel Prompts . In Proceedingsofthe2022CHIconferenceonhumanfactorsincomputing systems . 1 – 22 . https : / / doi . org / 10 . 1145 / 3491102 . 3517582 [ 29 ] Tongshuang Wu , Haiyi Zhu , Maya Albayrak , Alexis Axon , Amanda Bertsch , Wenxing Deng , Ziqi Ding , Bill Guo , Sireesh Gururaja , Tzu - Sheng Kuo , et al . 2023 . LLMs as Workers in Human - Computational Algorithms ? Replicating Crowdsourcing Pipelines with LLMs . arXiv preprint arXiv : 2307 . 10168 ( 2023 ) . https : / / doi . org / 10 . 48550 / arXiv . 2307 . 10168