Operationalizing content moderation “accuracy” in the Digital Services Act Johnny Tian - Zheng Wei jtwei @ usc . edu University of Southern California Los Angeles , California , USA Frederike Zufall zufall @ kit . edu Karlsruhe Institute of Technology Karlsruhe , Germany Robin Jia robinjia @ usc . edu University of Southern California Los Angeles , California , USA ABSTRACT The Digital Services Act 1 , recently adopted by the EU , requires social media platforms to report the “accuracy” of their automated content moderation systems . The colloquial term is vague , or open - textured—the literal accuracy ( number of correct predictions di - vided by the total ) is not suitable for problems with large class imbalance , and the ground truth and dataset to measure accuracy against is unspecified . Without further specification , the regulatory requirement allows for deficient reporting . In this interdisciplinary work , we operationalize “accuracy” reporting by refining legal con - cepts and relating them to technical implementation . We start by elucidating the legislative purpose of the Act to legally justify an interpretation of “accuracy” as precision and recall . These metrics remain informative in class imbalanced settings while also reflect - ing the proportional balancing of Fundamental Rights of the EU Charter . We then focus on the estimation of recall , as its naive estimation can incur extremely high annotation costs and dispro - portionately interfere with the platform’s right to conduct business . Through a simulation study , we show that recall can be efficiently estimated using stratified sampling with trained classifiers , and provide concrete recommendations for its application . Finally , we present a case study of recall reporting for a subset of Reddit under the Act . Based on the language in the Act , we identify a number of ways recall could be reported due to underspecification . We report on one possibility using our improved estimator , and discuss the implications and need for legal clarification . CCS CONCEPTS • Applied computing → Law ; • Mathematics of computing → Hypothesis testing and confidence interval computation . KEYWORDS Content moderation , EU law , Digital Services Act , inferential statis - tics , variance reduction . 1 Regulation ( EU ) 2022 / 2065 of the European Parliament and of the Council of 19 Octo - ber 2022 on a Single Market For Digital Services and amending Directive 2000 / 31 / EC ( DSA ) Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . Under review , Oct . , 2023 © 2024 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - XXXX - X / 18 / 06 . . . $ 15 . 00 https : / / doi . org / XXXXXXX . XXXXXXX ACM Reference Format : JohnnyTian - ZhengWei , FrederikeZufall , andRobinJia . 2024 . Operationaliz - ing content moderation “accuracy” in the Digital Services Act . In Woodstock ’18 : ACM Symposium on Neural Gaze Detection , June 03 – 05 , 2018 , Wood - stock , NY . ACM , New York , NY , USA , 12 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION Many concerns have been raised about the role of social media platforms in spreading harmful content such as online hate [ 31 ] , and these concerns persist despite the fact that social media com - panies voluntarily moderate their content [ 9 ] . On the other hand , content moderation raises the question of whether companies are interfering with user’s freedom of speech . Research continues to demonstrate that voluntary moderation is inadequate , and these issues will remain unresolved so long as social media platforms have conflicting interests in prioritizing platform growth and profit over providing safe online spaces [ 27 ] . In light of these conflicting interests , legislators have increas - ingly sought to enact legal requirements for content moderation [ 17 ] . Transparency requirements , which require companies to re - lease reports about their content moderation systems , are the first and necessary step in the regulatory landscape . However , these requirements need to be defined with care to ensure their relevance . Like any legal instrument , transparency requirements must respect the rights of all entities involved . Governing content moderation is a balancing act between conflicting rights : on one hand there is a need to protect the right to the freedom of speech , and on the other to curtail societal harms [ 8 ] . This balance must be made all while considering the technical feasibility of such legal requirements , so as to respect companies’ right to conduct business . The Digital Services Act , recently adopted by the EU , is a mile - stone legal instrument addressing content moderation . The Act promises to have a far - reaching “Brussels Effect” beyond the EU ( see § 3 ) and set the standard for other legislation worldwide . It includes a transparency requirement for social media platforms to report the “accuracy” of their content moderation systems . How - ever , the colloquial term “accuracy” is underspecified , and the regulatory requirement , as it stands , allows for deficient reporting . This is the departure point of our work : we operational - ize “accuracy” reporting , and in operationalizing we both refine legal concepts and relate them to technical implementation . In its entirety , our interdisciplinary work is a legally and tech - nically sound interpretation of content moderation “accuracy” as precision and recall in the Digital Services Act . Our argumenta - tion can be useful for future regulatory guidelines or legislative amendment , which , if adopted , would be enforced by the European Commission . Our contribution can be viewed from two perspectives . a r X i v : 2305 . 09601v2 [ c s . S I ] 20 O c t 2023 Under review , Oct . , 2023 Wei , Zufall , and Jia From a legal perspective , we identify the sources of underspecifi - cation in implementing the “accuracy” reporting requirement and their appropriate instantiations . From a technical perspective , we formulate the reporting of recall as unbiased statistical estimation , and show how such estimation can be made efficient with stratified sampling . Our paper contains three parts : ( 1 ) § 3 starts with elucidating the legislative purpose of the Act to legally justify an interpretation of “accuracy” as precision and recall . These metrics remain informative in class imbal - anced settings and reflect balancing considerations of the Fundamental Rights of the EU Charter . ( 2 ) § 4 formulates the reporting of recall as an unbiased statisti - cal estimation problem . Since naive estimation can pose an disproportionate burden for companies , we then propose the use of stratified sampling . In § 5 we derive recommendations for its use through a simulation study . ( 3 ) § 6 presents a case study of a recall reporting requirement for Reddit under the Act . Based on the language in the Act , we identify a number of ways recall could be reported due to underspecification . We choose one possibility to report , and discuss the implications and need for legal clarification . 2 BACKGROUND AND RELATED WORK Count - based metrics are common . At the time of writing , many social media companies voluntarily report basic content moderation statistics . Even without direct regulation , companies often provide transparency reports to build public trust in their platform . 2 For instance , Twitter reports a few count - based metrics such as the number of content that Twitter took moderation action against in a given reporting period . 3 However , these basic statistics can conflate multiple phenomena . When the number of takedowns of hate speech increases , it is not immediately clear whether it is due to ( 1 ) an increase in hate speech , ( 2 ) an increase in moderation , or ( 3 ) a change in their content moderation policy . Prevalence is industry best practice . To the best of our knowledge , only Facebook , Youtube , and Snap publicly provide exposure - based metrics . As an example , Facebook reports the prevalence of differ - ent categories of content violations over time . Such a prevalence metric would address concerns of ( 1 ) , reflecting increases in hate speech . While the exact methods to compute such a prevalence are unclear , they note that “we calculate this metric by selecting a sample of content seen on Facebook and then labeling how much of it shouldn’t be there . ” 4 Estimating prevalence is an industry best practice , with Facebook’s internal studies deeming it a major statistic [ 29 ] . With the direct connection between prevalence and moderation thoroughness , we speculate that these large companies may use prevalence along with other statistics , such as number of takedowns , to compute a moderation recall , which addresses ( 2 ) , for monitoring moderation performance internally [ 18 ] . Prevalence is used in academic study as well . Social media re - search uses prevalence in online discussions to study the impact of 2 https : / / integrityinstitute . org / news / institute - news / integrity - institute - releases - overview - of - online - social - platform - transparency 3 https : / / transparency . twitter . com / 4 https : / / about . fb . com / news / 2019 / 05 / measuring - prevalence / real world events , assuming that prevalence online is a proxy for public opinion [ 24 ] . There are many examples of social media re - search using prevalence estimated from classifier outputs , without human annotations . However , completely automated prevalence classification introduces bias , and for the purposes of legal reporting unbiased estimation should be provided . Park et al . [ 26 ] provides unbiased estimates for the prevalence of macro - norm violating com - ments ( violations of implicit Reddit site - wide rules derived from Chandrasekharan et al . [ 6 ] ) . They use stratified sampling with two strata , using a classifier for stratification . Our results here comple - ment theirs by providing insights into the application of stratified sampling , and how to improve its efficiency . Metrics required by regulation . Almost all major social media plat - forms provide transparency reports for German users as specified by Germany’s Network Enforcement Act ( NetzDG ) . 5 NetzDG requires companies to periodically provide information and statistics related to the content moderation conducted at these companies ( require - ments which are succeeded by those in the Digital Services Act ) . Amongst the required information is also the community guidelines by which companies moderate content , showing that additional regulation can address concern ( 3 ) . We highlight these reports to demonstrate the potential of regulation to standardize the set of transparency metrics reported on . Our work takes the view that regulation should require the reporting of recall , a prevalence - based metric . Requiring recall can reflect general moderation effective - ness , and encourages companies to periodically conduct studies of their platform design [ 15 ] . 3 THE DIGITAL SERVICES ACT The Digital Services Act ( DSA ) , recently adopted by the EU , will be fully applicable from February 17th , 2024 onwards , and introduces a comprehensive new framework for content moderation . Social media providers with more than 45 million average monthly active recipients in the Union ( Art . 33 ( 1 ) DSA ) , which currently includes U . S . - established platforms Twitter , Instagram , Youtube , and Face - book , are classified as “very large online platforms” ( VLOPs ) and will have to comply with the full set of requirements set forth in the Act . 6 The Act introduces concrete requirements for content moderation in two categories : 1 ) requirements to act against illegal content and 2 ) transparency requirements . In this paper , we focus on the DSA’s transparency requirements and on moderation in the sense of content deletion . The “Brussels Effect” . The DSA is a milestone legal instrument targeting content moderation . Hence , our study of the Act may have an impact beyond the EU—a phenomenon termed the “the Brussels effect” [ 5 ] . For one , governing bodies outside of the EU will likely reference the DSA in considering legislation for their own jurisdictions . As companies may already have the infrastructure to satisfy EU requirements , for other governments to adopt all or some of the DSA poses a low regulatory burden . Moreover , the DSA ( like the GDPR ) has a broad territorial scope , and requires companies operating in the EU to apply EU standards to all users , regardless of their nationality ( Art . 2 ( 1 ) DSA ) . Since the EU is the 5 Netzwerkdurchsetzungsgesetz vom 1 . September 2017 ( BGBl . I S . 3352 ) . 6 https : / / ec . europa . eu / commission / presscorner / detail / en / ip _ 23 _ 2413 . Operationalizing content moderation “accuracy” in the Digital Services Act Under review , Oct . , 2023 world’s largest single market , companies often choose to abide by EU regulations instead of forfeiting such business opportunity . Considerations for future implementation . The Commission has begun to adopt implement and designating decisions for the DSA . 7 In addition , a new European Centre for Algorithmic Transparency has been established to support the Commission in analyzing trans - parency reports . 8 The effects of the Regulation will have to be reviewed by 2025 and 2027 ( Art . 91 DSA ) —allowing for an ongoing debate on its implementation and potential future amendments . 3 . 1 Content moderation The DSA addresses two classes of content in its definition of “con - tent moderation” ( Art . 3 ( t ) DSA ) : • Illegal content . This is defined as “any information that [ . . . ] is not in compliance with Union law or the law of any Member State [ . . . ] , irrespective of the precise subject matter or nature of that law” by Art . 3 ( h ) DSA . For instance , national law of the EU Member States must contain a minimum standard to incriminate hate speech as specified by an EU Framework Decision . 9 Furthermore , the EU Commission has also started an initiative to add “all forms of hate crime and hate speech , whether because of race , religion , gender or sexuality” 10 to the list of EU crimes in Art . 83 ( 1 ) TFEU . 11 • Content that violates terms and conditions . Another content moderation scenario that the DSA specifically addresses are any activities that are aimed “at detecting , identifying and ad - dressing [ . . . ] information incompatible with their terms and conditions” ( Art . 3 ( t ) DSA ) . This includes the “community guidelines” as a part of the terms and conditions . Legal background . The DSA , like any other regulatory instru - ment in EU law , is bound and to be interpreted and applied in accordance with the EU Charter of Fundamental Rights ( EUCh ) 12 . While they only directly bind EU institutions ( Art . 51 EUCh ) , they also affect providers as private entities as they guide the interpreta - tion of provisions regulating their relation to citizens . 13 Namely , Recital ( 3 ) of the DSA explicitly refers to three protected rights that are all relevant here : • Freedom of expression and of information ( Art . 11 EuCH ) pro - tects online content from censorship and moderating mea - sures . Moderating online posts interferes with this right and requires justification . • The right to non - discrimination ( Art . 21 EuCH ) forbids any discrimination based on grounds like sex , race , colour , ethnic 7 https : / / digital - strategy . ec . europa . eu / en / policies / digital - services - act - package . 8 https : / / algorithmic - transparency . ec . europa . eu / . 9 Framework Decision 2008 / 913 / JHA of 28 November 2008 on combating certain forms and expressions of racism and xenophobia by means of criminal law and national laws transposing it . 10 Communicationof9 . 12 . 2021 , COM ( 2021 ) 777final . ThiswouldallowtheCommission to replace the existing Framework Decision by a new Directive further elaborating on a more extensive notion of hate speech incrimination . 11 Treaty on the Functioning of the European Union , [ 2016 ] OJ C202 / 1 . 12 Charter of Fundamental Rights of the European Union , OJ C 326 , 26 . 10 . 2012 , p . 391 . 13 ECJ , C - 360 / 10 – Sabam , 26 . 2 . 2012 , para . 52 . or social origin and is especially reflected in the EU frame - work against the expression of hatred [ 32 ] . 14 To prevent discrimination , this right may provide the justification for the interference with freedom of expression . • Freedom to conduct a business ( Art . 16 EuCH ) is equally men - tioned by the DSA and relevant for our context as any obliga - tion imposed by the DSA to providers is interfering with this right . In addition to balancing out freedom of expression and the purpose of non - discrimination , the freedom to conduct a business also requires proportionality . Balancing out these rights is a task not only performed by the legislator upon drafting regulation like the DSA , but also at the time of its subsequent executive implementation and judicial in - terpretation ( Art . 52 ( 5 ) EUCh ) . And it continues to be relevant not only for regulatory implementing decisions , but also at the level of technical implementation . 3 . 2 “Accuracy” as a transparency requirement Along with balancing out the aforementioned rights and freedoms , any automated content moderation system as a step of subsequent implementation of the Act also needs to be justified from a demo - cratic point of view : The EU Charter ( Art . 52 ( 1 ) ) demands a legal base ( here : the DSA ) as any person that is subject to an interference must have the possibility to influence the origin of that interference ( here : the obligation to moderate content ) through democratic elec - tions . In this way , transparency does not only enable democratic control and judicial review , but also offers insight to metrics that measure to which extent democratic legitimacy is carried through to the last step of implementing content moderation . Transparency requirements in the Act . The DSA requires that Terms and Conditions must contain information on any policies , procedures , measures and tools used for the purpose of content moderation , including algorithmic decision - making and human review ( Art . 14 ( 1 ) DSA ) . Even further , providers are obliged to publish comprehensive reports at least once a year—in the case of VLOPs , every six months—on any content moderation they engaged in during the relevant period ( Art . 15 ( 1 ) , Art . 42 ( 1 ) , ( 2 ) DSA ) . In all these cases , Art . 15 ( 1 ) ( e ) DSA requires providers to report : “anyusemadeofautomatedmeansforthepurposeofcontentmoderation , including a qualitative description , a specifica - tion of the precise purposes , indicators of the accuracy and the possible rate of error of the automated means usedinfulfillingthosepurposes , andanysafeguardsapplied . ” In addition , VLOPs must also conduct mandatory risk assessments that include their content moderation systems ( Art . 34 DSA ) . Troublingly , the DSA leaves the term “accuracy” underspecified . Art . 15 ( 1 ) ( e ) DSA only asks for “indicators of the accuracy and the possible rate of error " . From a technical perspective , the term “accuracy” raises a few questions : if we are estimating accuracy in its literal sense ( correct number of predictions divided by the total ) , to which ground truth are we measuring against ? In addition , accuracy is not suitable for problems with large class imbalance , so 14 Usually , national constitutions contain a similar guarantee which is again reflected in national laws , e . g . , Criminal Law , that once more constitute " illegal content " in the sense of the DSA . Under review , Oct . , 2023 Wei , Zufall , and Jia what metrics would be more appropriate ? We identify “accuracy” as an open - textured term , which needs interpretation in novel contexts . Without further interpretation , “accuracy” reporting required by the DSA will exhibit inconsistency , and social media providers may each make their own decisions regarding test data and evaluation metrics . They could choose to report the metric most beneficial for themselves , which would undercut the purpose of the regulation . What does “accuracy” refer to ? Interpreting the notion of “ac - curacy” beyond its literal wording needs to be in line with the legislative purpose of the Act and requires justification . Future le - gal adaptations of our analysis depends on this legal justification . We attempt to elucidate and define the notion of “accuracy” in Art . 15 ( 1 ) ( e ) of the DSA using a method of systematic interpretation in EU law : inter - instrumental interpretation [ 13 ] . By referencing the definitions and systematic understanding from another instrument of EU law , we can make an attempt to apply it to “accuracy . ” We refer to the recent proposal for an Artificial Intelligence Act ( AI Act ) , 15 a parallel development in EU law related to specific high - risk automated systems . Similar to the DSA , the AI Act would apply to U . S . - based providers if they are “placing on the market or putting into service AI systems in the Union” ( Art . 2 ( 1 ) ( a ) AI Act ) . While the AI Act is specifically targeted towards high - risk systems , it outlines a general regulatory approach to automated classification systems . With respect to “accuracy” , Art . 15 ( 1 ) of the AI Act proposal requires “ ( high - risk ) AI systems to be designed and developed in such a way that they achieve , in the light of their intended purpose , an appropriate level of accuracy [ . . . ] and perform consistently in those respects throughout their lifecycle . ” Similarly to the DSA , the draft only mentions “accuracy” and not other metrics such as precision or recall . Upon further inspection , however , the legislative process reveals that the draft was inspired by the policy guidelines of the High Level Expert Group on AI [ 16 ] . In these guidelines , the use of accuracy is intended to reflect the “trustworthiness” of an AI system , but a complementary footnote stated that “accuracy is only one performance metric and it might not be the most appropriate depending on the application , ” and that the F1 score , false positives , and false negatives may also be used . Hence , any interpretation of “accuracy” beyond its literal wording in the DSA , should use an “appropriate metric depending on the application " . 3 . 3 Metrics and their legal implications Having arrived at opening up the term to a potential broader inter - pretation , leads us to the question of potential legal implication of choosing between basic accuracy metrics from computer science . Accuracy . This metric is by far mentioned the most in EU law , and technically defined as : Accuracy = 𝑇𝑃 + 𝑇𝑁 𝑇𝑃 + 𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁 where TP , FP , TN , FN are true positives , false positives , true negatives , false negatives , respectively . Using accuracy as a metric for content moderation is problematic because of label - imbalance in the content moderation setting : in a huge set of content like social media posts , the illegal posts are usually far outnumbered by the posts that do not violate specific laws . Accuracy can be highly inflated if the test data is label - imbalanced , as a classifier that only outputs the majority label will achieve high accuracy . In § 5 . 1 , we show that 15 COM ( 2021 ) 206 final . even a simple classifier can achieve an accuracy of 95 % due to the high imbalance in the data set . Precision . Precision measures the fraction of examples positively predicted ( e . g . , removed by a content moderation system ) that are truly positive ( e . g . , illegal content , depending on the ground truth ) : Precision = 𝑇𝑃 𝑇𝑃 + 𝐹𝑃 . ( 1 ) Hence , it would indicate how much content that is actually not violating laws , is being removed regardless . There is a direct relation to the right to free speech : In an attempt to “over - fulfill” content moderation , many more posts would be subject to moderation . And as any moderating measure is an interference with freedom of speech , that is only justified if it does not outweigh the conflicting interest in moderation , overmoderation would be unlawful , too , and could be measured by precision . Recall . Finally , recall , would measure the fraction of truly pos - itive content ( e . g . , illegal posts ) that was predicted positive ( e . g . removed by the moderation system ) . Recall = 𝑇𝑃 𝑇𝑃 + 𝐹𝑁 . ( 2 ) As low recall would indicate the amount of illegal content still visible on the platform , it can be seen as an indicator for the right to non - discrimination . In this sense , it appears most appropriate with the purpose for which the DSA imposes a reporting obligation : to ensure compliance with EU or national laws against illegal content . Trade - offs and F1 - score . However , as pointed out above ( Sect . 3 . 1 ) , the DSA has to achieve an overall balance between freedom of expression , the right to non - discrimination and the freedom to conduct a business as guaranteed by the EuCH . To strike a bal - ance between freedom of expression and the regulatory purpose to guarantee non - discrimination , the F1 score as the harmonic mean between precision and recall appears suitable . However , we encour - age legislators and regulators to reason about content moderation “accuracy” in terms of precision and recall separately , as otherwise the score may obscure whether errors come from overmoderation or undermoderation . Furthermore , we address the question to which extent technical decisions on the implementation of reporting these metrics , affect the right to conduct business of platform providers . 4 STATISTICALLY ESTIMATING LEGAL REPORTING REQUIREMENTS Given that interpreting content moderation “accuracy” as precision and recall is legally sensible , we now discuss the technical imple - mentation of precision and recall . Computing such metrics can be non - trivial , as we cannot exhaustively annotate the subsets ( i . e . all visible content ) needed to compute the desired quantities . The only practical alternative is statistical estimation of these metrics via sampling and annotation . We highlight two statistical concepts necessary for the legal purposes of transparency reporting : Unbiasedness . Forlegalpurposes , any estimatewe provideshould be “true” and reflect the true value of the metric . In statistical terms , we can think of the true value of the metric as one computed over a set of exhaustive human annotations ( the “expectation” , see eq . Operationalizing content moderation “accuracy” in the Digital Services Act Under review , Oct . , 2023 3 ) . Since exhaustive annotation may not be possible , we sample and annotate to compute an estimate . A statistical estimate comes in the form of a point estimate ( best guess of true value ) and a confidence interval ( i . e . we are 95 % confident that the true value is between this range ) and the estimator is unbiased if the point esti - mate , averaged over many experiments , is equal to the true value . To achieve this property requires human ground truth annotations , as solely relying on classifiers introduces bias because they make errors relative to the ground truth [ 12 ] . Number of samples ( cost ) . An unduly burdensome reporting re - quirement would interfere disproportionately with the right to conduct business ( Art . 16 EUCh ) . Thus , our statistical analyses focuses on cost : how many annotations are needed to provide a rea - sonable width confidence interval ? In particular , recall estimation is costly because it requires estimating the number of remaining positives among all visible content ( false negatives , 𝐹𝑁 ) . The rarer they are , the more samples are needed to construct a reasonable estimate for the false negatives . This same challenge is well known for tasks like information retrieval , where recall is defined over all webpages on the web [ 1 ] . With variance reduction techniques , we can provide a narrower interval for a fixed cost [ 25 ] . 4 . 1 Estimating precision Auditing content moderation precision is qualitatively difficult but statistically simple , and so is not the focus of our work . Appendix B contains a discussion on estimating precision . 4 . 2 Estimating recall Measuring recall poses a challenging statistical problem , which is the focus of our work . Unlike precision , recall requires looking through the negatives ( i . e . visible content on the platform during the last reporting period ) to determine the number of false nega - tives 𝐹𝑁 . Typically , the prevalence of false negatives among the negatives is rare e . g . 0 < 𝑝 < < 0 . 1 . Appendix A . 1 contains a statis - tical formulation estimating the prevalence 𝑝 . The simplest way to estimate this probability is by applying the random sampling esti - mator ˆ 𝑝 . When estimating the probability of a rare event , we would like to control the coefficient of variance CV ˆ 𝑝 = SE ˆ 𝑝 / ˆ 𝑝 which is the relative estimation error to the prevalence . In other words , we are estimating the rare probability within some percentage accuracy [ 7 ] . If we require estimates to have a CV ˆ 𝑝 ≤ 20 % , the rarer the true prevalence , the smaller the standard error will need to be . Propagating confidence intervals . Once we have a statistical esti - mate for the false negative rate 𝑝 , the confidence intervals on 𝑝 can propagate to a confidence interval on recall . In the case where we know the number of true positives 𝑇𝑃 , the confidence intervals on 𝑝 can directly be translated to confidence intervals on 𝐹𝑁 then to recall , by plugging in the upper and lower estimates of 𝑝 into the recall equation . If 𝑇𝑃 is not a known quantity and also an estimate , confidence intervals can be obtained through bootstrap simulation . 4 . 3 Stratified sampling To reduce the burden of a recall reporting requirement , we propose applying stratified sampling , which is a variance reduction tech - nique . Stratified sampling can reduce the cost of recall estimation , by combining it with a machine learning classifier . Crucially , it is unbiased and more efficient than a random sampling estimator . Stratified sampling is unbiased even if the leveraged classifier has undesirable properties such as low accuracy or low fairness [ 4 ] , as a human is the final decision maker . A suboptimal classifier may cause the method to require more annotations , but it cannot intro - duce bias . Appendix A . 2 contains notation for stratified sampling . Intuition . Given any partition of the data into strata ( i . e . , bins ) , we can apply the stratified sampling estimator . This estimator is computed by annotating a random sample within each stratum , then combining the mean for each stratum in an unbiased manner . A good stratification can significantly improve the variance of the stratified sampling estimator over random sampling . In particular , if some strata have very low or very high prevalance , those strata will have lower variance . Then , we can allocate our samples to focus on estimating prevalences of the other high variance strata . 4 . 4 Binning and allocation The variance of stratified sampling depends on two choices : binning ( i . e . , how to partition the examples ) and allocation ( i . e . , how many examples to annotate in each stratum ) . We now describe methods to choose each . Binning . An effective way to create strata is with a classifier trained to identify violating content . Given a piece of content , the classifier predicts the probability that it violates community guide - lines . With the predictions scores from the classifier , content can be binned in a few ways by their score . The number of bins 𝐿 is a hyperparameter , which we investigate in § 5 . ( 1 ) Equal width bin - ning . The classifiers we consider score each comment from [ 0 , 1 ] . Equal - width binning splits this interval into 𝐿 equal width intervals each corresponding to a bin . Sampling across bins thus samples across the entire scoring range of the classifier . ( 2 ) Quantile binning . Once the comments are scored , quantile binning splits the scoring range into equal sized quantiles , where each bin will contain the same number of points . The range of scores within each quantile will depend on the scoring distribution of the classifier . ( 3 ) Oracle binning . This binning is not possible in practice as it requires the labels of all the data in the pool , but we include it as an oracle method . Using a recursive search procedure and the labels of all the training data , we can search for a good binning . At each recursive step , a bin is split into two at a point where the resulting variance is minimized . We recursively break down the classifier’s scoring interval into a number of bins that is a power of two . Allocation . Once the bins are partitioned , an allocation of how to sample from each strata ( i . e . , a choice of the 𝑛 ℎ ’s ) can be made . ( 1 ) Equal allocation . This baseline allocation samples from each bin equally . ( 2 ) Optimal allocation . For a given stratification , an optimal sample allocation minimizes the variance of the estimator . The so - lution is given in Appendix A . 3 . This allocation is an oracle method : it cannot be run in practice , as it requires the standard deviations within each stratum , which are not known beforehand . We include this allocation method for analysis purposes . ( 3 ) Pilot allocation . To approximate the optimal allocation , we adopt a simplified ver - sion of Bennett and Carvalho [ 3 ] . We annotate a fixed number of pilot samples within each stratum , which we use to estimate the Under review , Oct . , 2023 Wei , Zufall , and Jia 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 600 700 800 900 1000 1100 1200 Binning ( w / optimal alloc . ) oracle eqwidth quantile 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 500 750 1000 1250 1500 1750 2000 Allocation ( w / quantile bins ) optimal pilot : 50 equal 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 600 800 1000 1200 1400 1600 1800 2000 Pilot samples ( w / quantile bins ) optimal pilot : 10 pilot : 50 pilot : 100 Number of strata # o f s a m p l e s r e q u i r e d Figure 1 : Number of samples required to estimate the prevalence of toxicity ( 4 . 1 % ) within 20 % in the unfiltered CivilComments , with different number of strata . Binning is based on predicted scores from a finetuned Roberta model . All pilot results are averaged over 30 trials . As the number of strata grows , estimators using optimal allocation require fewer samples . Pilot methods outperform the equal allocation baseline , but when there are too many strata , annotation effort is wasted on the pilot samples . standard deviation per stratum . We then approximate the optimal allocation using these standard deviations . If the size of the pilot sample within the stratum exceeds the optimal allocation for that stratum , no additional samples are collected for that stratum . If the optimal allocation is more than the pilot sample , the additional samples are collected . We use psuedocounts ( adding 1 positive and 1 negative example ) to ensure each stratum is allocated samples . 4 . 5 Analyzing the burden of estimation The variance of the random and stratified sampling estimators can be calculated analytically , so their power ( i . e . number of samples needed to achieve some estimation accuracy ) can also be calculated if we assume the central limit theorem . This assumption is justified because the sample sizes we consider are large , so the estimators are normally distributed . We consider the goal of reporting ˆ 𝑝 ± ( 0 . 1 ) ˆ 𝑝 with 95 % prob - ability . The coefficient of variance is effectively required to be CV 𝑟𝑒𝑞 = 0 . 1 / 𝑧 . 95 , and in turn SE 𝑟𝑒𝑞 = ( 0 . 1 ) ˆ 𝑝 / 𝑧 . 95 , where 𝑧 ( 1 − 𝛼 ) is test statistic of the normal variable . Since SE 𝑟𝑒𝑞 is a function of 𝑛 , the total number of annotations collected , we can solve for 𝑛 for each estimator . Power calculations for random sampling are given in Appendix A . 4 . Appendix A . 4 and A . 5 contain closed form solu - tions for the cost of random and stratified sampling , respectively . 5 EFFICIENTLY ESTIMATING RECALL FOR A KEYWORD FILTER In this section , we determine which choices for binning , allocation ( described in § 4 . 4 ) , and classifier are best for estimating recall with stratified sampling . We use CivilComments [ 4 ] as our testbed and emulate a content moderation system by constructing a keyword - based filter . While Civilcomments is annotated for toxicity and it is not an operationalized legal definition or community guideline , this setting provides a large dataset to study recall estimation in general . Similar to pool - based active learning [ 30 ] , we assume in our simulated experiments that the dataset is initially unlabeled , and we “annotate” an example by revealing the label from the dataset . 5 . 1 A simple keyword filter To emulate an automated content moderation system , we construct a keyword - based filter to “moderate” comments in the Civilcom - ments dataset ( see § 5 . 2 ) . We choose the list of keywords using the top 10 positive features from an unigram logistic regression trained to predict comment toxicity . In total , the dataset contains about 100K toxic comments out of 1 . 7M comment , and the overall prevalence of toxicity is 5 . 9 % . The filter achieves a precision of 58 % , removing 57K comments with 32K of these actually being toxic . Amongst the unfiltered examples , the prevalence is 4 . 1 % . The filter would only achieve a recall of 33 % , removing about 33K of the total 100K toxic comments . However , the accuracy would be nearly 95 % . Note that the high accuracy obscures the fact that only 33 % of the toxic comments were found and removed . In our simulated experiments , the goal is to statistically estimate the recall using as few labeled examples as possible . By estimating the prevalence of toxicity in the unfiltered comments , we can calculate recall . In our experiments , we compare costs when estimating the prevalence of 4 . 1 % to within 20 % . If prevalence is estimated as such , the confidence intervals on the recall would translate to [ 28 . 9 % , 37 . 9 % ] . 5 . 2 Experimental setup Dataset . The CivilComments dataset [ 4 ] contains comments la - beled by their toxicity , from the archive of the Civil Comments platform , a commenting plugin for independent news sites . Each comment is annotated for toxicity by at least 10 annotators , and if half of the annotators label it toxic , the toxicity label is positive . For our study , we use 100K random examples of the training set for training , and the rest of the 1 . 7M training examples for simulation . This creates a large testing setting which better represents content moderation , where U may be large and the number of examples to train the model is small . This dataset naturally presents a rare prevalence estimation problem as 5 . 9 % of comments are toxic . Models . Roberta [ 19 ] is a transformer - based language model which is pretrained on a large corpus of English data , which we find Operationalizing content moderation “accuracy” in the Digital Services Act Under review , Oct . , 2023 𝑛 for CV ˆ 𝑝 𝑠𝑡 ≤ 20 % Reduction Comments Mod . ˆ 𝑝 𝑠𝑡 95 % CI Stratified Random w / strat . removed recall 95 % CI r / politics 7 . 00 % [ 6 . 42 % , 7 . 58 % ] 1033 1401 26 % 0 . 77 % 9 . 91 % [ 9 . 22 % , 10 . 71 % ] r / AskReddit 4 . 75 % [ 4 . 29 % , 5 . 21 % ] 1425 2142 33 % 0 . 70 % 12 . 92 % [ 11 . 91 % , 14 . 11 % ] r / sex 3 . 00 % [ 2 . 63 % , 3 . 37 % ] 2477 3556 30 % 2 . 06 % 40 . 72 % [ 37 . 94 % , 43 . 93 % ] r / pcmasterrace 4 . 00 % [ 3 . 61 % , 4 . 39 % ] 1488 2567 42 % 0 . 89 % 18 . 20 % [ 16 . 86 % , 19 . 78 % ] r / wow 4 . 25 % [ 3 . 76 % , 4 . 74 % ] 2078 2455 15 % 0 . 75 % 14 . 97 % [ 13 . 63 % , 16 . 60 % ] r / legaladvice 1 . 50 % [ 1 . 25 % , 1 . 75 % ] 5027 7591 33 % 7 . 65 % 83 . 62 % [ 81 . 38 % , 85 . 97 % ] Table 1 : Statistics derived from pilot annotations . Stratified sampling was applied with 8 quantile bins , 50 pilot annotations per bin , and a Roberta finetuned on personal attack data . Estimating the prevalence here is equivalent to using equal proportion allocation . The number of samples required and efficiency for estimation with random and pilot stratified sampling is provided assuming the lower bound of prevalence . to have strong performance for CivilComments after finetuning . We use typical hyperparameters for small datasets [ 22 ] , and make two critical adjustments : increasing the batch size to 32 ( we hypothesize that since the labels can be noisy , the gradients are unstable ) , and subsampling the negative examples so that the training data is balanced ( the resulting training data has 11k examples ) . 5 . 3 Discussion Results . As shown in Figure 1 , using the right techniques and pa - rameters for stratified sampling makes a large difference : the worst choices perform as poorly as the random sampling estimator , while the best choice can save up to half of the annotation effort . With optimal allocation , increasing the number of strata decreases vari - ance . Intuitively , optimal allocation is more effective with more bins because a more precise allocation is given to each strata , according to the strata prevalence . The pilot allocation , which approximates the optimal allocation , outperforms equal allocation . However , as the number of bins increases , the total number of pilot samples also increases , resulting in wasted samples . Appendix C . 1 contains addi - tional results for stratification with different classifiers . In general , while simple classifiers are effective , Roberta dramatically reduces the number of samples needed to estimate the prevalence . Recommendations . Based on our simulation in CivilComments , our recommendations for conducting stratified sampling in practice are : ( 1 ) Choose quantile binning . Among the two practical binning methods , quantile binning outperforms equal width binning . It performs at near oracle binning performance if the number of bins is sufficiently large and the optimal allocation is used . ( 2 ) Choose pilot allocation . Attempting to estimate a good allocation with pilot samples will greatly reduce the variance of the stratified sampling estimator . ( 3 ) Make a reasonable choice for the number of pilot samples . If the prevalence is suspected to be low , or the estimation requirement is precise , inefficiency in pilot samples is less of a concern ( because the pilot samples are needed anyways ) . When the prevalence is high and the number of samples required may be less than the total pilot samples , a reasonable choice needs to be made about the number of pilot samples collected . The experimenter must consider how much data annotation is acceptable at worst . In Appendix C . 2 we show that these recommendations are robust for lower prevalences as well . 6 RECALL REPORTING IN PRACTICE In this section , we present a case study of recall reporting for Reddit . As the requirement currently underspecifies the technical imple - mentation , we identify a number of ways recall could be reported . We then choose one to report and discuss the implications and need for legal clarification . We choose to focus on Reddit for two reasons : first , Reddit qualifies as an “online platform” as defined in Art . 3 ( i ) DSA and therefore is obliged to provide transparency reports annually ( Art . 15 ( 1 ) DSA ) . 16 Second , Reddit data is most readily available for academic study . 6 . 1 Addressing underspecification In implementing a technical evaluation to satisfy “accuracy” report - ing for the DSA , three basic specifications are needed : Evaluation metric . As discussed in § 3 , we establish recall as a technically and legally sensible metric , which we showed in § 5 can be efficiently estimated with stratified sampling . Ground truth . The DSA addresses two classes of content ( see § 3 . 1 ) . To report recall , we must consider how the true positives and true negatives can be annotated : • Illegal content . To identify illegal content , legal definitions ( e . g . , punishable hate speech ) would need to be operational - ized as clear annotation guidelines [ 32 ] . To be feasible at the scale social media requires , such identification likely needs to be possible for a skilled person with no legal training . • Content that violates terms and conditions . A convenience in identifying whether speech violates community guidelines as a part of terms and conditions is that it is already oper - ationalized by the platform . The measurement of accuracy on the guidelines is a measurement of whether the modera - tion system is achieving its intended purpose . This would also complement other transparency measures about the guidelines themselves ( see Art . 14 DSA ) . As the scope of the DSA applies to all content , and all platforms have community guidelines [ 9 ] , this specification may be general enough to allow for explicit legal clarification . However , regula - tors should take caution when specifying recall on illegal content . Such reporting is only feasible if the respective legal bases defining content illegality are amenable to operationalization . 16 https : / / ec . europa . eu / commission / presscorner / detail / en / ip _ 23 _ 2413 . Under review , Oct . , 2023 Wei , Zufall , and Jia Test set . Finally , the set of content ( or test set ) to report the accuracy metric over needs to be decided . Since the DSA speci - fies bi - annual or annual reporting periods , it would be natural to evaluate on content accumulated within the last reporting period . The DSA does not provide further guidance , and we speculate two appropriate test sets : • All visible content . An accuracy metric could be evaluated over all visible content on the platform , which would follow industry best practices for prevalence metrics ( see § 2 ) . • All notified content . Art . 16 DSA requires providers to put in place mechanisms to allow users to notify ( i . e . , flag ) con - tent they consider to be illegal . When a notice is submitted , providers are liable if they do not act expeditiously to pro - vide a legal examination and remove or disable access to this content if it is illegal ( Art . 16 ( 3 ) , Art . 6 ( 1 ) ( b ) DSA ) . As plat - forms are expected to review this content , the moderation recall of this subset may be especially important . However , anyone can flag content without concrete reason , possibly even with malicious intent [ 21 ] . The specification of a test set will be heavily platform dependent . For Reddit , content may mean posts or comments , but for Youtube it may mean videos . In Snapchat , most content is not publicly visible , and it may be more appropriate to calculate recall over views . Regulators should be wary of overgeneralizing , as social media platforms have diverse functionality [ 11 ] . Underspecifying the test set allows for custom implementation and may encourage meaningful reporting . Our study . For the purposes of our study , we choose to report the recall of personal attacks on all visible comments . We find this to reasonably satisfy the requirement as Reddit has community guidelines against such speech and most of the discussion happen in the comments . Practically , the other options require data or annotations that are not easily obtained . In our preliminary analysis of comments , we found that personal attacks can be identified consistently . This is corroborated by Habernal et al . [ 14 ] , which reports that personal attacks in r / changemyview have high inter - annotator agreement . 6 . 2 Experimental setup Study of interest . We restrict our study to a recent dump of Reddit ( December 2022 ) on Pushshift . io [ 2 ] . We select 6 subreddits with a range of macro - norm violation prevalence , as estimated in Park et al . [ 26 ] , and that have more than 1000 comments per day 17 : ( most macro - norm violations ) r / politics , r / AskReddit , r / sex , r / pcmasterrace , r / wow , r / legaladvice ( least ) At the time of writing , 4 out of the 6 subreddits we study explicitly have rules against “personal attacks " ( politics : rule 4 , AskReddit : rule 8 , wow : rule 1 , legaladvice : rule 5 ) . The other two subreddits : r / sex , r / pcmasterrace , also have rules regarding constructive en - gagement and respectful conversation , respectively . Data collection . To download the comments , we first randomly sample a post submission ( akin to a thread ) from the subreddit in the Pushshift data dump . We then use Praw 18 to collect all the 17 Accessed January 21st , 2023 , as per https : / / subredditstats . com / . 18 https : / / praw . readthedocs . io / en / stable / non - top level comments in the submission . We repeat this process until we have either 100K comments or have exhausted all possible comments for that subreddit . Removed comments . While publicly viewable posts from Red - dit are accessible through the Pushshift data stores , records for comments removed are incomplete . Therefore , we can only derive an upper bound of content moderation recall by assuming every removal decision is a true positive . Personal attacks . Using the data from Habernal et al . [ 14 ] , we fine - tuned a Roberta classifier on a balanced training set with an equal number of personal attacks and negatives from r / changemyview . We label comments as personal attacks according to Habernal et al . [ 14 ] and additional annotation details are provided in Appendix D . 6 . 3 Discussion Results . Refer to Table 1 . As a sanity check , we see that ranking subreddits by personal attack prevalence aligns closely with their ranking by macro - norm violation as estimated in Park et al . [ 26 ] . By the prevalences , we can conclusively determine that r / politics has the biggest problem with personal attacks out of all the subreddits we examine . Moderation recall is also low for r / politics . While they have a rule against personal attacks , we speculate that moderators avoid removing too many comments to appear politically neutral . For r / legaladvice , personal attack prevalence is already low , and moderators also remove many comments that are off topic , which may inflate the true positives and consequently the recall . In terms of methodology , applying our version of stratified sampling often yields substantial sample reductions . Confidence intervals . Even though we collected a small number of samples , the confidence intervals on the recall are narrow because the relative difference between the true positives and false negatives was often large . This means that as the number of personal attacks removed surpasses the prevalence , the recall becomes easier to estimate . Since we were able to provide tight bounds around the recall with a small number of annotator hours , we expect it feasible for Reddit to report more comprehensive recall measures annually . Discrepancies with the reporting requirement . There are two dis - crepancies between our experiment and the actual reporting re - quirement of the DSA . First , our recall is reported over a month while Reddit only needs to provide transparency reports every year . Our method can easily be adopted to a larger pool and the estima - tion difficulty only increases in the larger pool if the prevalence decreases . Second , the removed comments we consider could have been removed through either automated or manual means , while only recall reporting of automated tools are required by the DSA . Since we do not have records of how the comments were removed , we cannot easily provide an isolated accuracy of the automated systems , but our method is general . However , we point out that the recall presented is in line with current industry prevalence metrics , and we suggest legislators expand the scope of the requirement to an aggregate moderation accuracy . Operationalizing content moderation “accuracy” in the Digital Services Act Under review , Oct . , 2023 7 CONCLUSION In this work , we identified an oversight in the Digital Services Act regarding the reporting of “accuracy” for content moderation . We took an interdisciplinary approach to operationalize this “accuracy” and provide a legally and technically sound interpretation of con - tent moderation “accuracy” as precision and recall . Precision and recall directly reflect the Fundamental Rights forming the basis of content moderation regulation , and this work addresses the feasi - bility and necessary specifications when reporting “accuracy” with recall . By providing legal justification , our work can be directly used for future regulatory guidelines or legislative amendment . REFERENCES [ 1 ] Javed A . Aslam , Virgil Pavlu , and Emine Yilmaz . 2006 . A Statistical Method for System Evaluation Using Incomplete Judgments . In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( Seattle , Washington , USA ) ( SIGIR ’06 ) . Association for Computing Machinery , New York , NY , USA , 541 – 548 . https : / / doi . org / 10 . 1145 / 1148170 . 1148263 [ 2 ] Jason Baumgartner , Savvas Zannettou , Brian Keegan , Megan Squire , and Jeremy Blackburn . 2020 . ThePushshiftRedditDataset . In ProceedingsoftheFourteenthIn - ternationalAAAIConferenceonWebandSocialMedia , ICWSM2020 , HeldVirtually , Original Venue : Atlanta , Georgia , USA , June 8 - 11 , 2020 , Munmun De Choudhury , Rumi Chunara , Aron Culotta , and Brooke Foucault Welles ( Eds . ) . AAAI Press , 830 – 839 . https : / / ojs . aaai . org / index . php / ICWSM / article / view / 7347 [ 3 ] Paul N . Bennett and Vitor R . Carvalho . 2010 . Online stratified sampling : eval - uating classifiers at web - scale . In Proceedings of the 19th ACM Conference on Information and Knowledge Management , CIKM 2010 , Toronto , Ontario , Canada , October 26 - 30 , 2010 , Jimmy X . Huang , Nick Koudas , Gareth J . F . Jones , Xin - dong Wu , Kevyn Collins - Thompson , and Aijun An ( Eds . ) . ACM , 1581 – 1584 . https : / / doi . org / 10 . 1145 / 1871437 . 1871677 [ 4 ] Daniel Borkan , Lucas Dixon , Jeffrey Sorensen , Nithum Thain , and Lucy Vasser - man . 2019 . Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification . In Companion of The 2019 World Wide Web Conference , WWW 2019 , San Francisco , CA , USA , May 13 - 17 , 2019 , Sihem Amer - Yahia , Mo - hammad Mahdian , Ashish Goel , Geert - Jan Houben , Kristina Lerman , Julian J . McAuley , Ricardo Baeza - Yates , and Leila Zia ( Eds . ) . ACM , 491 – 500 . https : / / doi . org / 10 . 1145 / 3308560 . 3317593 [ 5 ] Anu Bradford . 2012 . The Brussels Effect . 107 , 1 ( 2012 ) , 1 – 68 . https : / / heinonline . org / HOL / P ? h = hein . journals / illlr107 & i = 1 [ 6 ] EshwarChandrasekharan , MattiaSamory , ShagunJhaver , HunterCharvat , AmyS . Bruckman , Cliff Lampe , Jacob Eisenstein , and Eric Gilbert . 2018 . The Internet’s Hidden Rules : An Empirical Study of Reddit Norm Violations at Micro , Meso , and Macro Scales . Proc . ACM Hum . Comput . Interact . 2 , CSCW ( 2018 ) , 32 : 1 – 32 : 25 . https : / / doi . org / 10 . 1145 / 3274301 [ 7 ] Philip M . Dixon , Aaron M . Ellison , and Nicholas J . Gotelli . 2005 . Im - proving the precision of estimates of the frequency of rare events . Ecology 86 , 5 ( 2005 ) , 1114 – 1123 . https : / / doi . org / 10 . 1890 / 04 - 0601 arXiv : https : / / esajournals . onlinelibrary . wiley . com / doi / pdf / 10 . 1890 / 04 - 0601 [ 8 ] Evelyn Douek . 2021 . Governing Online Speech : From " Posts - as - Trumps " to Proportionality and Probability . Columbia Law Review 121 , 3 ( 2021 ) , 759 – 834 . [ 9 ] T . Gillespie . 2018 . Custodians of the Internet : Platforms , Content Moderation , and the Hidden Decisions that Shape Social Media . Yale University Press . https : / / books . google . com . co / books ? id = - RteDwAAQBAJ [ 10 ] Tarleton Gillespie . 2020 . Content moderation , AI , and the question of scale . Big Data & Society 7 , 2 ( 2020 ) , 2053951720943234 . https : / / doi . org / 10 . 1177 / 2053951720943234 arXiv : https : / / doi . org / 10 . 1177 / 2053951720943234 [ 11 ] Tarleton Gillespie , Patricia Aufderheide , Elinor Carmi , Ysabel Gerrard , Robert Gorwa , Ariadna Matamoros - Fernández , Sarah T . Roberts , Aram Sinnreich , and Sarah Myers West . 2020 . Expanding the debate about content moderation : Schol - arly research agendas for the coming policy debates . Internet Policy Review 9 , 4 ( Oct . 2020 ) . https : / / doi . org / 10 . 14763 / 2020 . 4 . 1512 [ 12 ] Robert Gorwa , Reuben Binns , and Christian Katzenbach . 2020 . Algorithmic content moderation : Technical and political challenges in the automation of platform governance . Big Data & Society 7 , 1 ( Jan . 2020 ) , 2053951719897945 . https : / / doi . org / 10 . 1177 / 2053951719897945 Publisher : SAGE Publications Ltd . [ 13 ] Stefan Grundmann . 2011 . Inter - Instrumental - Interpretation : Systembildung durch Auslegung im Europäischen Unionsrecht . Rabels Zeitschrift für ausländis - ches und internationales Privatrecht / The Rabel Journal of Comparative and Inter - nationalPrivateLaw 75 , 4 ( 2011 ) , 882 – 932 . https : / / www . jstor . org / stable / 41304262 Publisher : Mohr Siebeck GmbH & Co . KG . [ 14 ] Ivan Habernal , Henning Wachsmuth , Iryna Gurevych , and Benno Stein . 2018 . Before Name - Calling : Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation . In Proceedingsofthe2018ConferenceoftheNorthAmericanChap - teroftheAssociationforComputationalLinguistics : HumanLanguageTechnologies , Volume 1 ( Long Papers ) . Association for Computational Linguistics , New Orleans , Louisiana , 386 – 396 . https : / / doi . org / 10 . 18653 / v1 / N18 - 1036 [ 15 ] Anna - Sophie Harling , Declan Henesy , and Eleanor Simmance . 2023 . Trans - parency Reporting : The UK Regulatory Perspective . Journal of Online Trust and Safety 1 , 5 ( Jan . 2023 ) . https : / / doi . org / 10 . 54501 / jots . v1i5 . 108 [ 16 ] High - Level Expert Group on AI . HLEG . 2020 . The Assessment List for Trustworthy Artificial Intelligence ( ALTAI ) for self assessment . Publications Office . https : / / doi . org / 10 . 2759 / 791819 [ 17 ] Kyle Langvardt . 2017 . Regulating online content moderation . Georgetown Law Journal 106 ( 2017 ) , 1353 . [ 18 ] Yi Liu . 2019 . Estimating the prevalence of rare events — theory and practice . https : / / www . unofficialgoogledatascience . com / 2019 / 08 / [ 19 ] YinhanLiu , MyleOtt , NamanGoyal , JingfeiDu , MandarJoshi , DanqiChen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019 . RoBERTa : A Robustly Optimized BERT Pretraining Approach . CoRR abs / 1907 . 11692 ( 2019 ) . arXiv : 1907 . 11692 http : / / arxiv . org / abs / 1907 . 11692 [ 20 ] Amanda M Macias . 2020 . Facebook CEO Mark Zuckerberg calls for more reg - ulation of online content . https : / / www . cnbc . com / 2020 / 02 / 15 / facebook - ceo - zuckerberg - calls - for - more - government - regulation - online - content . html [ 21 ] Robinson Meyer . 2014 . The primary way to report harassment on the Social Web is broken . https : / / www . theatlantic . com / technology / archive / 2014 / 08 / the - way - we - report - harassment - on - the - social - web - is - broken / 378730 / [ 22 ] Marius Mosbach , Maksym Andriushchenko , and Dietrich Klakow . 2021 . On the Stability of Fine - tuning BERT : Misconceptions , Explanations , and Strong Baselines . In 9th International Conference on Learning Representations , ICLR 2021 , Virtual Event , Austria , May 3 - 7 , 2021 . OpenReview . net . https : / / openreview . net / forum ? id = nzpLWnVAyah [ 23 ] Casey Newton . 2019 . The trauma floor . https : / / www . theverge . com / 2019 / 2 / 25 / 18229714 / cognizant - facebook - content - moderator - interviews - trauma - working - conditions - arizona [ 24 ] BrendanO’Connor , RamnathBalasubramanyan , BryanR . Routledge , andNoahA . Smith . 2010 . From Tweets to Polls : Linking Text Sentiment to Public Opinion Time Series . In Proceedings of the Fourth International Conference on Weblogs and Social Media , ICWSM 2010 , Washington , DC , USA , May 23 - 26 , 2010 , William W . Cohen and Samuel Gosling ( Eds . ) . The AAAI Press . http : / / www . aaai . org / ocs / index . php / ICWSM / ICWSM10 / paper / view / 1536 [ 25 ] Art B . Owen . 2013 . Monte Carlo theory , methods and examples . [ 26 ] Joon Sung Park , Joseph Seering , and Michael S . Bernstein . 2022 . Measuring the Prevalence of Anti - Social Behavior in Online Communities . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 451 ( nov 2022 ) , 29 pages . https : / / doi . org / 10 . 1145 / 3555552 [ 27 ] Scott Pelley . 2021 . Whistleblower : Facebook is misleading the public on progress against hate speech , violence , misinformation . https : / / www . cbsnews . com / news / facebook - whistleblower - frances - haugen - misinformation - public - 60 - minutes - 2021 - 10 - 03 / [ 28 ] Penn State Statistics . 2022 . Lesson 6 : Stratified sampling : Stat 506 . https : / / online . stat . psu . edu / stat506 / lesson / 6 [ 29 ] Radha Iyengar Plumb . 2019 . Exploring feedback from data and governance experts : A research - based response to the Data Transparency Advisory Group report . https : / / research . facebook . com / blog / 2019 / 05 / exploring - feedback - from - data - and - governance - experts - a - research - based - response - to - the - data - transparency - advisory - group - report / [ 30 ] Burr Settles . 2012 . Active Learning . Morgan & Claypool Publishers . https : / / doi . org / 10 . 2200 / S00429ED1V01Y201207AIM018 [ 31 ] Alexandra A Siegel . 2020 . Online hate speech . Social media and democracy : The state of the field , prospects for reform ( 2020 ) , 56 – 88 . [ 32 ] Frederike Zufall , Marius Hamacher , Katharina Kloppenborg , and Torsten Zesch . 2022 . A Legal Approach to Hate Speech – Operationalizing the EU’s Le - gal Framework against the Expression of Hatred as an NLP Task . In Proceed - ings of the Natural Legal Language Processing Workshop 2022 . Association for Computational Linguistics , Abu Dhabi , United Arab Emirates ( Hybrid ) , 53 – 64 . https : / / aclanthology . org / 2022 . nllp - 1 . 5 A ADDITIONAL STATISTICAL BACKGROUND A . 1 Formulating prevalence estimation To estimate 𝐹𝑁 , we refactor this estimation as a problem of estimat - ing the prevalence 𝑝 , or rate of false negatives among the predicted negatives . With the probability 𝑝 , we could use 𝐹𝑁 = 𝑝 | N | to obtain the number of false negatives , where N are the predicted negatives . Under review , Oct . , 2023 Wei , Zufall , and Jia 𝑝 Prec . 20 % 10 % 5 % 0 . 1 865 3458 13830 0 . 059 1532 6127 24508 0 . 01 9508 38031 152122 0 . 001 95941 383762 1535047 Table 2 : Number of samples needed for the random sampling estimator to estimate different prevalences , to different lev - els of precision . A normal approximation is assumed . The smaller the prevalence the more difficult it is to estimate precisely . Highlighted in bold is the prevalence of positives in the CivilComments dataset . Statistically , we estimate the false negative rate , or prevalence as : 𝑝 = E 𝑥 ∼N [ 𝑠 ( 𝑥 ) ] ( 3 ) where 𝑠 ( 𝑥 ) = I ( 𝑥 ∈ 𝑆 ) is an binary function indicating whether the content 𝑥 is a true positive ( i . e . violating or illegal content ) . The simplest way to estimate this probability is by applying the random sampling estimator ˆ 𝑝 where ˆ 𝑝 = 1 𝑛 𝑛 ∑︁ 𝑖 = 1 𝑠 ( 𝑥 𝑖 ) ( 4 ) and { 𝑥 𝑖 , 𝑖 = 1 . . . 𝑛 } is a random sample from the remaining content U . This estimator has standard error SE ˆ 𝑝 = √︁ ˆ 𝑝 ( 1 − ˆ 𝑝 ) / 𝑛 , which is directly proportional to width of the confidence interval on 𝑝 if a normal approximation is applied . This quantifies how precisely we have estimated 𝑝 . A . 2 Stratified sampling notation We adopt notation from Penn State Statistics [ 28 ] . Let 𝐿 be the number of strata , 𝑁 be the total size of the dataset , 𝑁 ℎ be the number of total items in stratum ℎ , and 𝑛 ℎ be the number of samples drawn and annotated from stratum ℎ for estimation . The stratified sampling estimator ˆ 𝑝 𝑠𝑡 is defined by the equations : ˆ 𝑝 𝑠𝑡 = 𝐿 ∑︁ ℎ = 1 𝑁 ℎ 𝑁 · ˆ 𝑝 ℎ ( 5 ) where ˆ 𝑝 ℎ = 1 𝑛 ℎ 𝑖 = 1 ∑︁ 𝑛 ℎ 𝑠 ( 𝑥 ℎ 𝑖 ) ∀ ℎ ∈ { 1 , . . . , 𝐿 } ; ( 6 ) and { 𝑥 ℎ𝑖 } 𝑛 ℎ 𝑖 = 1 is a random sample of 𝑛 ℎ examples from stratum ℎ . In other words , ˆ 𝑝 𝑠𝑡 is a weighted average of the ˆ 𝑝 ℎ ’s , which are our prevalence estimates within each stratum . This estimator is unbiased , i . e . , the expected value E [ ˆ 𝑝 𝑠𝑡 ] of the estimator equals the true probability 𝑝 . Given a stratification , we can analytically calculate the variance of the stratified estimator as (cid:99) Var ( ˆ 𝑝 𝑠𝑡 ) = 𝐿 ∑︁ ℎ = 1 (cid:18) 𝑁 ℎ 𝑁 (cid:19) 2 (cid:18) 1 − 𝑛 ℎ 𝑁 ℎ (cid:19) 2 · ˆ 𝑝 ℎ ( 1 − ˆ 𝑝 ℎ ) 𝑛 ℎ . ( 7 ) Note that the second term ( 1 − 𝑛 ℎ / 𝑁 ℎ ) is a finite population cor - rection and ≈ 1 when the strata are large relative to the number of annotations . The standard error is then SE ˆ 𝑝 𝑠𝑡 = √︃ (cid:99) Var ( ˆ 𝑝 𝑠𝑡 ) and the confidence intervals of the estimator can be calculated by assuming the estimator follows a normal distribution ( by the central limit theorem ) . A . 3 Optimal allocation for stratified sampling For a given stratification , there is a closed form solution of an optimal allocation which minimizes the variance of the stratified sampling estimator . For each stratum , the number of samples to allocate is given by 𝑛 𝑜𝑝𝑡ℎ = 𝑛 · 𝑁 ℎ 𝜎 ℎ (cid:205) 𝑘 𝑁 𝑘 𝜎 𝑘 ( 8 ) where 𝜎 ℎ is the standard deviation of samples within stratum ℎ , and 𝑛 is the total number of planned samples to annotate . While these standard deviations cannot be known beforehand , an estimate can be made for 𝑛 𝑜𝑝𝑡ℎ using a small pilot sample from each stratum . A . 4 Power calculations for random sampling For random sampling we have SE 2 𝑟𝑒𝑞 = ˆ 𝑝 ( 1 − ˆ 𝑝 ) 𝑛 ∴ 𝑛 = ˆ 𝑝 ( 1 − ˆ 𝑝 ) SE 2 𝑟𝑒𝑞 ( 9 ) to achieve the desired accuracy . For reference , the number of samples to estimate different preva - lences , to different levels of precision , with random sampling is listed in Table 2 . A . 5 Power calculations for stratified sampling Given a stratification , we can analytically calculate costs as well . For equal allocation stratified sampling , we have SE 2 𝑟𝑒𝑞 = 𝐿 ∑︁ ℎ = 1 (cid:18) 𝑁 ℎ 𝑁 (cid:19) 2 · ˆ 𝑝 ℎ ( 1 − ˆ 𝑝 ℎ ) ( 1 / 𝐿 ) 𝑛 ( 10 ) ∴ 𝑛 = (cid:32) 𝐿 SE 2 𝑟𝑒𝑞 (cid:33) 𝐿 ∑︁ ℎ = 1 (cid:18) 𝑁 ℎ 𝑁 (cid:19) 2 · ˆ 𝑝 ℎ ( 1 − ˆ 𝑝 ℎ ) ( 11 ) where we simplified the variance term of the stratified estimator by dropping the population correction ( if 𝑁 ℎ is large relative to 𝑛 , the correction is approximately 1 ) . For optimal allocation stratified sampling , we have SE 2 𝑟𝑒𝑞 = 𝐿 ∑︁ ℎ = 1 (cid:18) 𝑁 ℎ 𝑁 (cid:19) 2 · ˆ 𝑝 ℎ ( 1 − ˆ 𝑝 ℎ ) 𝑛 𝑜𝑝𝑡ℎ ( 12 ) ∴ 𝑛 = (cid:32) 1 SE 2 𝑟𝑒𝑞 (cid:33) 𝐿 ∑︁ ℎ = 1 (cid:18) 𝑁 ℎ 𝑁 (cid:19) 2 · ˆ 𝑝 ℎ ( 1 − ˆ 𝑝 ℎ ) 𝑐 𝑜𝑝𝑡ℎ ( 13 ) where 𝑐 𝑜𝑝𝑡ℎ is the proportion of 𝑛 allocated to stratum ℎ . B ESTIMATING PRECISION To estimate precision , the true positives ( i . e . , violating or illegal content ) need to be determined within the positives ( i . e . , removed content ) . Statistically , we are estimating the false positive rate 𝑞 = E 𝑥 ∼P [ 𝑠 ( 𝑥 ) ] where P denotes the set of positives and 𝑠 ( 𝑥 ) = I ( 𝑥 ∈ 𝑆 ) is an binary function indicating whether the content 𝑥 is a true positive . The value of 𝑞 is then exactly the precision . 𝑞 is only difficult to estimate when it is almost 0 or 1 . The former is Operationalizing content moderation “accuracy” in the Digital Services Act Under review , Oct . , 2023 20 % 10 % 5 % Binning Allocation [ 28 . 9 % , 37 . 9 % ] [ 30 . 8 % , 35 . 2 % ] [ 31 . 2 % , 34 . 0 % ] Random sampling n / a n / a 2247 8986 35942 Unigram oracle : 8 optimal 991 3961 15844 quantiles : 8 pilot : 50 1242 ± 45 4965 ± 176 19936 ± 784 Roberta ( balanced ) oracle : 8 optimal 578 2312 9247 quantile : 8 pilot : 50 940 ± 39 3758 ± 153 15025 ± 627 Distilbert ( balanced ) oracle : 8 optimal 628 2511 10044 quantile : 8 pilot : 50 961 ± 38 3846 ± 164 15362 ± 642 Table 3 : Number of samples needed for the stratified sampling estimator to estimate prevalence of toxicity ( 4 . 1 % ) in the unfiltered CivilComments within 5 , 10 , and 20 % . The intervals denote the confidence intervals on the recall . Oracle : 8 means 8 strata were created and pilot : 50 means 50 pilot samples are taken within each strata for variance estimation . Oracle binning and optimal allocation are oracle techniques which rely on labelled data that is not available beforehand in practice . We chose 8 bins and 50 pilot samples because these settings were effective in Figure 1 . unlikely , as companies won’t deploy useless systems , and the latter would mean the system works well and exact precision estimates are unnecessary . We can construct a sample mean estimator ˆ 𝑞 , by sampling 𝑁 items from P and computing the percentage of correct removals . The distribution of this sample mean ˆ 𝑞 is approximately normal , and reasonable confidence intervals are easy to obtain . The qualitative difficulty is in implementing the scoring function 𝑠 ( 𝑥 ) . Since the ground truth will be applied to content detected by an automated system , many false positives may be borderline . In the case where the ground truth is illegal content , a trained legal expert may be needed , and some borderline content may even need to wait for judicial decisions . In the case where the ground truth are community guidelines , the decision boundaries of these guidelines are continually refined , and the moderators judging content violation need substantial training [ 23 ] . C ADDITIONAL STRATIFIED SAMPLING EXPERIMENTS C . 1 Stratification with different classifiers We experiment using stratified sampling with a few supervised classification techniques . These classifiers are used to score the unlabeled examples , from which the strata are created . Models . We study three models : a unigram logistic regression model , Roberta [ 19 ] , and Distilbert . Roberta [ 19 ] is a transformer - based language model which is pretrained on a large corpus of English data . Distilbert is smaller version of Roberta with the same model architecture but also 40 % less parameters . We find both pre - trained models to have strong performance for civilcomments after finetuning . We use typical hyperparameters for small datasets [ 22 ] , and make two adjustments that we found to be critical : increasing the batch size to 32 ( we hypothesize that since the labels can be noisy , the gradients are unstable ) , and subsampling the negative examples so that the training data is balanced ( the resulting training data has 11k examples ) . Results . In Table 3 , we show that using a classifier can greatly reduce annotation requirements over a random sampling estima - tor . Estimating prevalence within 20 % of 4 . 1 % requires only a few hundred samples . While the unigram model is effective , Roberta dramatically reduces the number of samples needed to estimate the prevalence . The gap between quantile and oracle binning meth - ods is larger for Roberta than for unigram models , suggesting that Roberta would benefit from a better binning method . C . 2 Stratification for rarer prevalences For each example , civilcomments has a real valued label of toxicity corresponding to the proportion of annotators that labelled it as toxic . When converting the labels to binary , a threshold of 0 . 5 yields a prevalence of 5 . 9 % , which is what we report on in § 5 . Here we present results for different prevalences by varying the threshold among { 0 . 48 , 0 . 83 , 0 . 95 } . We vary the number of total pilot samples and bins , and by choosing appropriate powers of two , each bin will always have an integer number of pilot samples . Refer to Figure 2 . In general , stratified sampling provides signifi - cant efficiency over random sampling ( comparing to the figures in Table 2 ) . The number of pilot samples collected matters for both very small and very large prevalences . In the case of high preva - lence , pilot samples outnumber the optimal allocation so there will be wasted samples . In the case of low prevalence , additional pilot samples help estimate the standard deviation of the strata correctly for optimal allocation . Finally , increasing the number of strata does not help with lower prevalences , as less samples are available for each bin to estimate the standard deviation with . D ADDITIONAL ANNOTATION DETAILS We label a comment as a personal attack if it fits any of criteria defined in [ 14 , Table 2 ] including : vulgar insult , illiteracy insult , ridicule or sarcasm , condescension , accusation of stupidity , accu - sation of ignorance , and accusation of argumentative fallacies . We found most personal attacks to be immediately recognizable be - cause it included insults or accusations . For borderline cases , such as rude comments , we labelled it a personal attack if it didn’t engage with the parent comment . In some other cases , viewing the original comment thread of Reddit was helpful . When the comment was part of a “flame war” , we would annotate it as a personal attack . All annotation was conducted by trusted and compensated annotator , and the rate of annotation was about 600 examples per hour . In a small interannotator agreement study with the first author , the Under review , Oct . , 2023 Wei , Zufall , and Jia 5 10 15 20 25 30 500 600 700 800 900 1000 8 . 07 % pilot : 256 pilot : 512 pilot : 1024 5 10 15 20 25 30 4000 4500 5000 5500 6000 1 . 11 % pilot : 256 pilot : 512 pilot : 1024 5 10 15 20 25 30 24000 26000 28000 30000 32000 0 . 28 % pilot : 256 pilot : 512 pilot : 1024 Number of strata # o f s a m p l e s r e q u i r e d Figure 2 : Number of samples required to estimate the prevalence of toxicity within 20 % in civilcomments , with different prevalences , with different number of strata . Binning is based on predicted scores from a finetuned Roberta classifier . All pilot results are averaged over 100 trials . The numbers associated with the pilot is the total number of pilot samples across all bins . For small prevalences , more pilot samples are better , because more samples estimate the standard deviations within each bin better . In contrast , more bins fares worse because each bin has less pilot samples . agreement was substantial , with a Cohen’s 𝜅 of 0 . 69 ( out of 100 examples , both annotators agreed on 96 ) . E ETHICAL CONSIDERATIONS Risks of content moderation . Content moderation can violate the right to the freedom of expression . Automated content moderation further raises ethical concerns about problematic training data or application of such systems perpetuating societal power dynamics [ 10 ] . We do not approve the use of these systems for censorship and have especially highlighted that automated content modera - tion needs to respect freedom of speech . We promote the use of stratified sampling : it involves humans and is unbiased with respect to a human defined ground truth . Finally , our work focuses on the evaluation of content moderation rather than directly making re - moval decisions , and we hope it can help further the study of fair interventions to reduce social media harms . Risks of regulatory capture . Regulation can be co - opted to serve minority interests . Facebook , for instance , has openly requested for additional social media regulation , which is speculated to serve their interest in preventing competition [ 20 ] . Our position on reg - ulation is not neutral either , as this work proposes new forms of regulation . However , the entirety of this work is dedicated to con - tent moderation’s recall legal and technical sensibility . A large focus of our work is on technical feasibility , which is especially important in considering the compliance of new companies and preventing regulatory capture . Data statement . Our experiments are based on data sets that include postings qualifying as personal data in the sense of the EU General Data Protection Regulation . Even though the data sets have been made publicly available before , our experiments qualify as “processing " and thus needs to be justified . We base this justification on Art . 6 ( 1 ) ( f ) , Art . 89 GDPR for purposes of scientific research .