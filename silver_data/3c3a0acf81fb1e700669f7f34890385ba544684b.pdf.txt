Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models Michael Xieyang Liu Carnegie Mellon University Pittsburgh , PA , USA xieyangl @ cs . cmu . edu Tongshuang Wu Carnegie Mellon University Pittsburgh , PA , USA sherryw @ cs . cmu . edu Tianying Chen Carnegie Mellon University Pittsburgh , PA , USA tianyinc @ andrew . cmu . edu Franklin Mingzhe Li Carnegie Mellon University Pittsburgh , PA , USA mingzhe2 @ cs . cmu . edu Aniket Kittur Carnegie Mellon University Pittsburgh , PA , USA nkittur @ cs . cmu . edu Brad A . Myers Carnegie Mellon University Pittsburgh , PA , USA bam @ cs . cmu . edu ABSTRACT Decision - making in unfamiliar domains can be challenging , de - manding considerable user effort to compare different options with respect to various criteria . Prior research and our formative study found that people would benefit from seeing an overview of the information space upfront , such as the criteria that others have pre - viously found useful . However , existing sensemaking tools struggle with the “cold - start” problem — it not only requires significant in - put from previous users to generate and share these overviews , but such overviews may also be biased and incomplete . In this work , we introduce a novel system , Selenite , which leverages LLMs as reasoning machines and knowledge retrievers to automatically pro - duce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes . Subsequently , Selenite also adapts as people use it , helping users find , read , and navigate unfamiliar in - formation in a systematic yet personalized manner . Through three studies , we found that Selenite produced accurate and high - quality overviews reliably , significantly accelerated users’ information pro - cessing , and effectively improved their overall comprehension and sensemaking experience . KEYWORDS Sensemaking , Decision making , Large Language Models , Natural Language Processing , Human - AI Collaboration 1 INTRODUCTION Whether it is parents delving into the vast sea of baby stroller choices or developers choosing a JavaScript frontend framework , people frequently find themselves having to make decisions in un - familiar topics and domains . In these situations , individuals often have to iteratively find , read , collect , and organize numerous in - formation about different options with respect to various criteria [ 17 , 53 , 54 , 64 ] , which can become a rather messy and overwhelm - ing experience . One challenge lies within the initial reading process — due to unfamiliarity with the topic , people may struggle to fully understand certain content as they read , or fail to recognize impor - tant aspects that should otherwise warrant their attention [ 57 , 87 ] , leading to a limited viewpoint and misguided decisions [ 39 , 117 ] . For example , novice developers might not be aware of crucial cri - teria of a software library like its stability , community size and support , or ease of integration with existing codebases , resulting in the choice of a sub - optimal library . Another challenge arises when people have to sift through nu - merous online reviews and comparison articles but with limited time and cognitive bandwidth — a more complete understanding of the decision - making space ideally requires reading “everything , ” but this is usually impractical or impossible . Instead , people often adopt the “selective ( or non - linear ) reading” strategy [ 113 ] , where they only read the paragraphs that discuss information that they consider relevant or valuable and bypass the rest [ 24 , 112 ] , e . g . , in a focused session where they would like to compare different options with respect to the same criterion . However , it is challenging for people to gauge the potential value of articles or paragraphs , es - pecially long - winded ones , just by skimming and without delving into a more thorough read [ 15 , 32 , 35 ] . For instance , information snippets about the maneuverability of different baby strollers may be dispersed throughout a review and appear in diverse variations ( e . g . , “agile enough to go through tight spots” and “easy to steer and navigate small corners” both arguably discuss “maneuverability” ) , making it difficult for people to spot these variations effectively and navigate efficiently among such scattered details . In response to these difficulties , prior work on sensemaking and knowledge reuse has shown promising evidence that people would benefit from seeing an overview of the information space before they dive into the sensemaking process [ 12 , 18 , 33 , 36 , 66 , 72 , 76 , 83 , 88 , 98 , 99 , 102 ] — for example , Kittur et al . reported that having read an overview of the criteria that earlier users found useful can help peo - ple build intuition and understanding of the decision space upfront , leading to significantly improved digestion of the source material , better - structured mental models , and ultimately well - informed de - cisions [ 53 , 54 ] . Similarly , our formative study found that people expressed a desire for such comprehensive overviews to help them more systematically read , understand , and strategically navigate unfamiliar information . However , existing sensemaking systems have struggled with the “cold start” issue — they often require sub - stantial effort from the current user to personally go through the unfamiliar content and gather and structure information to obtain such an overview [ 6 , 52 , 54 , 65 , 97 ] , which defeats the premise of re - ceiving one upfront . And even if a previous user has generated such an overview , it could often be incomplete [ 29 , 66 , 83 ] , with a biased perspective [ 34 , 48 ] , or in formats that make it hard for the current user to readily understand and take advantage of [ 49 , 50 , 65 ] . 1 a r X i v : 2310 . 02161v1 [ c s . H C ] 3 O c t 2023 Liu et al . https : / / www . babylist . com / hello - baby / best - strollers Best Strollers of 2023 e Criteria that you looked at : Safety Comfort Price Weight and size Foldability Ease of cleaning Durability Criteria that you ignored : Maneuverability Storage space Design You might be interested in further searching for “Best strollers of 2023” based on : Ease of assembly Brake & locking system f a b c c 1 c 2 c 3 d c 4 Best baby strollers Figure 1 : The main user interface of Selenite , which provides users with a comprehensive overview of the information space in the sidebar ( a ) . When users encounter an unfamiliar topic ( b ) , Selenite offers them a global grounding based on commonly considered criteria ( c ) as well as the options encountered so far ( d ) , helping them develop quick intuitions of the topic . As users read new articles , Selenite provides local grounding through page - level and paragraph - level summaries and annotations ( e ) , enabling effective comprehension and efficient navigation between the content of their interests . Upon leaving a page , Selenite dynamically summarizes users’ progress and suggests avenues for finding additional new information ( f ) in subsequent searches . To overcome these challenges and go beyond prior systems , we explore the idea of providing users with a comprehensive overview of the information space upfront to jumpstart as well as guide their subsequent sensemaking and decision - making processes in a novel system named Selenite . 1 On a high - level , when users encounter an unfamiliar topic , Selenite leverages GPT - 4 , a large language model developed by OpenAI , as a knowledge retriever to offer them a global grounding based on commonly considered criteria , helping users develop quick intuitions of the topic ( Figure 1c ) . As users read new articles , Selenite contextualizes that overview and pro - vides local grounding via page - level and paragraph - level summaries and annotations ( Figure 1e ) , enabling effective comprehension and efficient navigation of the content of their interests . Upon leav - ing a page , Selenite dynamically summarizes users’ progress and suggests avenues for finding additional information to help users expand their perspectives rather than duplicating existing knowl - edge ( Figure 1f ) . Through an accuracy and coverage evaluation of Selenite , we verified its feasibility to provide a sufficiently accu - rate and high - quality global overview to the users . Furthermore , 1 Selenite is named after a soft and transparent gemstone , and stands for “ S mart E nvironment for L ogical E xtraction and N avigation of I nformation using T echnological E xpertise . ” additional usability and case studies revealed that Selenite acceler - ated users’ information processing , facilitated their comprehension , and effectively improved their overall reading and sensemaking experience . The contributions described in this work include : • a formative study showcasing people’s need for support when finding and reading information for decision - making , • Selenite , a novel system providing users with a comprehensive overview of the information space upfront to jumpstart as well as guide their subsequent reading and sensemaking processes , • as part of Selenite , a novel user interface to interact with LLM - generated content and a novel user experience to contextualize that content into people’s existing sensemaking workflows , go - ing beyond the widely - used conversational interfaces in gener - ative AI research and applications [ 3 , 11 , 16 , 80 , 114 ] . • an accuracy and coverage evaluation of Selenite that demon - strates the feasibility of our approach , • usability and case studies of Selenite that offer insights into its usability , usefulness , and effectiveness , • a discussion of design implications for future LLM - powered sensemaking and decision - making tools . 2 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models 2 RELATED WORK 2 . 1 Reading and Making Sense of Online Information Building on theories of sensemaking as defined as developing a mental model of an information space in service of a user’s goals [ 25 , 56 , 94 ] , at a high level , a typical online sensemaking process involves first reading and understanding information from various data sources ( e . g . , online comparison articles , blog posts , video re - views , etc . ) , and then collecting and organizing such information to form a schema or representation ( e . g . , a comparison table , a decision tree , etc . ) to interpret the space [ 65 , 85 ] . Notably , there have been many research as well as commercial tools and systems developed to support this latter stage of sensemaking , i . e . , collecting and orga - nizing information . For example , tools like SearchPad [ 9 ] , Hunter Gather [ 97 ] , Unakite [ 65 ] , as well as commercial systems like the Ev - ernote clipper [ 30 ] , allow a user to capture entire pages or portions of web content , categorize them , and later assemble them into a co - herent document or structure for their own sensemaking , decision making , or sharing and collaboration . Furthermore , to reduce the disruption to users’ overall workflow , prior work has also explored using lightweight interactions [ 68 ] to streamline the process of collection and triaging information , or even automatically keeping track of content of interest on behalf of the user [ 28 , 47 , 67 ] . However , an inherent assumption that these prior systems make is that the user has fully read and grasped the content that they are about to collect using the system . But , in many circumstances , the initial stage of sensemaking , i . e . , reading and comprehension , can be just as , if not more , challenging — for example , users may run into unfamiliar terms [ 8 ] , domain - specific jargon and formulae [ 42 ] , and nonce words [ 41 ] that require expertise for comprehension [ 45 , 79 ] . Within - documentation navigation while reading can also be challenging , often requiring users to repeatedly and manually scroll back and forth to locate and compare conceptually similar or related content [ 5 , 35 , 42 , 91 ] . Discovering new and unseen content after reading can also be surprisingly difficult , as prior work revealed that a generic search query could return many redundant and near - duplicate documents [ 27 , 84 , 89 ] despite the extensive use of duplicate detection algorithms in modern search engines [ 86 ] . Therefore , in this work , we focus on supporting users’ reading and understanding of unfamiliar content in the first place . 2 . 2 Reading Support Tools One of the most popular strategies that people employ when con - suming information is “active reading , ” which involves perform - ing a wide range of cognitive activities ( e . g . , skimming , learning , critical thinking , annotation , note - taking , reflecting , and critical thinking ) to achieve a deep level of understanding of the target content [ 1 , 10 , 95 ] . Prior work has introduced various tools that support many of these activities during an active reading process . For example , XLibris is a digital platform that enables highlight - ing , annotating , and collecting content of interest , and later helps users find similar content within a document [ 95 ] ; LiquidText [ 104 ] and GatherReader [ 46 ] further took advantage of the touch - based gestures on large - screen mobile tablets to enable features such as parallel reading of different parts of the same document as well as flexible capturing and structuring knowledge via drag and drop . However , the fundamental challenge remains — it is both time and effort - intensive for individual users to iteratively read through and understand unfamiliar content by themselves without expert guidance or systematic strategy [ 78 , 101 ] , which we also confirmed in our formative study . Despite being feature - rich , aforementioned reading support tools generally fail to provide clear guidance to help users decide “what to read” and “how to most effectively read it” . Instead , Selenite addresses the lack of guidance issue by provid - ing users with an upfront comprehensive overview to help users formulate an initial mental model of “what might be worth reading . ” In addition , while reading , Selenite provides in - context glanceable annotations for each content paragraph , helping users more effec - tively understand the content and optionally perform “selective reading . ” 2 . 3 Eliciting Knowledge from Large Language Models Recent advances in large language models ( LLMs ) like GPT - 4 [ 81 ] , PaLM [ 22 ] , and LlaMa [ 106 , 107 ] showcase impressive capabilities in answering user questions . These models are trained on large volumes of data , and as a result , their parameters might contain a significant body of factual as well as synthesized knowledge across a wide range of domains [ 23 , 110 ] , In this work , we leverage GPT - 4 as a knowledge retriever to retrieve a list of commonly considered aspects given an arbitrary topic , which is used to directly help users understand that topic at the beginning and systematically read about and explore that topic afterwards . However , LLMs face well - known challenges like hallucination and falsehood [ 7 , 105 ] , which could make their outputs uncertain and less trustworthy , often requiring manual inspection and verification before use [ 69 ] . In this work , we address these issues with a two - prong approach : 1 ) reducing hallucination through techniques such as Self - Refine [ 71 ] ; 2 ) grounding LLM generations with the content that users would actually read , enabling natural verification . 3 FORMATIVE STUDY & DESIGN GOALS To better understand the obstacles people encounter in their reading and sensemaking strategies during decision - making , we conducted a formative study . Building upon the insights from this study and existing research , we established a clear set of design goals . 3 . 1 Formative Study 3 . 1 . 1 Methodology . Participants were a convenience sample of 8 information workers ( 5 male , 3 female ) recruited through social media listings and mailing lists . To capture a variety of processes , we recruited 3 doctoral students , 2 professional software develop - ers , 2 researchers , and 1 administrative staff member . While we do not claim that this sample is representative of all information workers , the interviews were very informative and helped motivate the design of Selenite . We began by asking participants to recall experiences of con - ducting decision - making tasks on topics that they were not familiar 3 Liu et al . with . 2 We then explored how they manage those situations . We asked the participants to provide context by reviewing their browser histories to cue their recollections while retrospectively describing those tasks . We solicited their workflows , strategies , frustrations , and needs . Finally , we had participants use the open - source Unakite system [ 65 ] 3 to make sense of a topic that they are not familiar with ( e . g . , for people who have not yet had children to figure out the best baby strollers to purchase for their future child ) and gathered their workflows , strategies , mental models . 3 . 1 . 2 Findings . Participants reported a total of 28 distinct topics that they encountered and explored , such as “ choosing a hybrid app framework , ” “ choosing the best time tracking tool , ” “ Selecting a VR headset , ” “ picking the best running shoes , ” and “ selecting the perfect engagement ring . ” For a complete catalog of these topics , please refer to Table 4 in the supplemental materials . Below , we report major findings from the study : People often find themselves feeling lost or unsure of where to begin and desire a big - picture understanding of important criteria ( or aspects ) of an information space be - fore diving deeper . When approaching an unfamiliar topic , one common strategy that participants reported employing is to find some sort of “ overview of different aspects ” ( P3 ) that would give them “ an intuition of what to care about and some guidance on what to look out for regarding each option ” ( P5 ) in their subsequent exploration . For example , when investigating which time - tracking app to use , P5 was able to find a few articles that provided such overviews at the beginning , e . g . , under the section “ What makes the best time tracking software ? ” 4 , However , participants complained that such lists of criteria are often “ subjective , incomplete ” ( P1 ) , can contain aspects that they “ most likely don’t care about ” ( P2 ) , and worse yet , “ do not represent how the rest of an article would be structured ” ( P6 ) . In addition , for certain topics such as “ best birthday gift ideas ” , such overviews of criteria are hard to find upfront , in which case they would have to employ a bottom - up approach by reading through a series of articles back - and - forth , which is often considered “ time - consuming ” ( P1 ) and “ hard to actually follow through ” ( P2 ) . Without these “ important criteria to keep in mind ” ( P4 ) upfront , participants reported feeling “ overwhelmed by large amounts of unfamiliar infor - mation ” ( P6 ) , lacking “ a sense of clarity and structure ” ( P7 ) , and can easily lose focus during sensemaking . These findings prompted us to generate an initial overview of the commonly considered crite - ria given an information space to provide users with some global grounding and an anchor point for their subsequent reading and sensemaking . Identifying and consistently keeping track of criteria is challenging , even with lightweight collection and organiza - tion mechanisms introduced in Unakite . Participants reported struggling with identifying and unifying criteria while reading content , which is a “ significant cognitive load ” ( P4 ) when they are unfamiliar with a topic . One of the challenges is that the same crite - rion can be discussed in various ways across different articles ( and 2 We subsequently kept track of these topics and used them in our system evaluations . 3 Unakite is a Chrome extension that helps users collect and organize information into comparison tables in a lightweight fashion while searching and browsing web articles . We used Unakite since it was shown to be easy to learn and use , and can support virtually all web page styles and structures [ 65 ] . 4 https : / / zapier . com / blog / best - time - tracking - apps / even within the same article ) , and it is hard for participants to rec - ognize those variations without expert advice and time - consuming to flip back and forth to make sure they are eventually unified and consistently represented in their Unakite tables . For example , when investigating the topic of “baby strollers , ” P6 first saw a stroller should be “agile and nimble to be able to go through tight spots and sidewalks , ” and put it down as “nimbleness” in Unakite ; later when she saw another segment that stated that a particular stroller is “easy to steer and handle and can smoothly navigate tight corners , ” she created another criterion called “steering . ” It wasn’t until when she saw a segment in a third article that described a stroller having great “maneuverability and control” did she realize that all of these were practically describing the same aspect , “maneuverability , ” and she had to go back and readjust and combine those criteria and their associated evidence in Unakite . Additionally , P7 recounted a similar experience when searching for washers and dryers for his first house and admitted that “ oftentimes , coming up with the right keyword or jargon to summarize what I saw can be surprisingly hard , and I really wish someone would just do that for me . ” Selenite tackles this issue by providing a comprehensive list of frequently considered criteria upfront , eliminating the need for individuals to find and characterize criteria themselves . People need reading and navigation guidance both at para - graph level as well as article level . Participants reported often having “ limited attention span ” ( P8 ) when reading online articles and can only focus on a certain amount of information , usually the first few paragraphs or the first few sentences within a paragraph , before getting distracted or lost . For example , P5 in her quest to find a suitable time - tracking app pointed to a typical situation where “ sometimes a paragraph , even a short one , could be quite convoluted and have a lot of intertwined information , for example , and at first I thought this paragraph was just about money , but the rest of the paragraph was actually about lots of other things like platform com - patibility . ” But , since participants tend to skim through content quickly , it often leads to potential misunderstandings or missing important details . In such situations , participants desired “ some simple metadata of what’s covered in a paragraph ” ( P4 ) to give them an intuition of what the paragraph is about and whether it is worth reading . These findings prompted us to provide clear in - situ per - paragraph summaries and the option for users to clarify convoluted paragraphs by “zooming in” on them in Selenite . The same applies to the page level , where participants wanted to be able to “ preview a page before investing time reading it ” ( P3 ) to understand whether it discusses detailed aspects that they care about . Additionally , such preview can also help them “ maximize the information gained from each page ” ( P5 ) , i . e . , help them avoid read - ing duplicate information and aspects without learning anything new . As P4 put it , “ if I’ve already learned about all the aspects from the other pages , I don’t have to read this one . ” However , as discussed previously , such previews ( even if they are in the form of an abstract or table of contents ) are not always available for each article . And even if they do exist , they are almost always not grounded by a person’s past reading and information collection activity . Selenite fulfills the page - preview need by offering users a concise overview of what’s covered ( and not ) in a page to help users gauge its value . 4 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models “Best baby stroller” Encountered options Common criteria B Collected info Unseen criteria Ignored criteria New search : best baby stroller on ease of assembly Find webpage - of - interest With keywords search Global grounding With criteria and options Local grounding With in - context mapping Suggest next steps Through status tracking best baby stroller Figure 2 : Main stages and features of Selenite : After the user searches and finds an initial webpage - of - interest to read , Selenite provides : 1 ) global grounding with a set of common criteria as well as options encountered so far , 2 ) local grounding with in - context mapping of criteria per paragraph , and 3 ) suggestions for next steps in sensemaking . Additionally , Selenite tackles the issue of personalization by pre - senting users with a progress summary based on their previous sensemaking activities at the end of a page . Furthermore , as participants became more familiar with a topic , their reading patterns started to get increasingly selective and non - linear . For example , we have observed that participants use a combi - nation of keyword searches and flipping back and forth in an effort to find relevant information about a particular criterion that they cared about ( with respect to different options ) , which they thought was “ haphazard ” ( P1 ) and “ inefficient ” ( P7 ) . This led us to suggest potentially fruitful search keywords to users for discovering more unseen information in the end - of - page progress summary . 3 . 2 Summary of Design Goals We postulate that an effective user interface / interaction paradigm for helping users find and read about key information during sense - making should support : • [ D1 ] As the user starts investigating a topic , provide a global grounding using common criteria as well as the options encountered to help users build intuitions of the information space and promote structured thinking ; • [ D2 ] During their reading , provide a local grounding using page - level as well as paragraph - level summary and an - notation to enable an accurate understanding of and effective navigation within and across articles ; • [ D3 ] Upon finishing , dynamically suggest next steps in sensemaking based on users’ existing reading and informa - tion collection activities to avoid missing important aspects after reading as well as maximize the information gain in future readings . 4 THE SELENITE SYSTEM Based on the design goals , we designed and implemented the Se - lenite Chrome extension prototype to help people read about and make sense of unfamiliar topics with the help of global grounding ( summarized in Figure 2 ) . We will first illustrate how an end - user , Adam , would interact with Selenite . 4 . 1 Example Usage Scenario Adam , an expectant father , is seeking guidance in selecting a baby stroller for his upcoming child . As someone without prior experi - ence in child - rearing , he decided to rely on Selenite to help him while going through review articles and product pages of baby strollers . Adam did a quick Google search and clicked on the first result page , which appeared to be a review article titled “The 10 Best Baby Strollers Put To The Test” . Upon opening the page , Selenite auto - matically recognized the topic of the page as “best baby strollers” ( Figure 1b ) , and then automatically presented a global overview in the sidebar that is injected directly into every web page ( Figure 1a ) . The overview contained a list of criteria ( Figure 1c ) that are commonly considered by people when discussing the topic . In ad - dition , Selenite also automatically parsed the web page content and extracted the different baby stroller options and presented them under the “Options encountered so far” section ( Figure 1d ) in the sidebar . Notice that , without any additional effort from Adam , he received the automatic recognition of topics , retrieval of commonly considered criteria , and extraction of options from the page “for free , ” all of which occurred as soon as the page finished loading . This overview provides Adam with a comprehensive grounding of the information space regarding the topic of “best baby strollers . ” After quickly skimming them , Adam now felt that he already has an intuition built up about what criteria he should look out for when picking baby strollers , all without delving into the article itself . Based on the global options and criteria , Selenite contextualizes the ones that are covered on the current page by highlighting them in the sidebar ( also with the ones that are not present on the page low - lighted , for example , see Figure 1c & d ) , helping users better understand and find specific information of interest while browsing . In addition , as Adam read the article , he noticed that Selenite pro - vides in - context annotations of mentioned criteria above each paragraph ( which research has shown to be the unit of information that people usually think in and work with during sensemaking [ 73 , 96 ] ) that is on the page ( Figure 1e ) . He quickly learned that he could just skim those mentioned criteria to get a rough idea of what a particular paragraph is about and decide if that paragraph is worth reading . When he came across information about the maneuverability of a specific stroller while reading the article , Adam became interested 5 Liu et al . While it is harder to push and a wee bit heavier than the smallest product , it is easier to use and earned a higher score for quality than most of the best umbrella strollers we tested… The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces… … a Maneuverability Wheel type Price Maneuverability This full - size stroller is relatively easy to push , but the plastic wheels make uneven terrain , gravel , and grass more challenging . While not the best in a group that includes rubber tires and joggers , the plastic wheels … … Wheel type Durability Price Customer review Maneuverability Weight and size Figure 3 : Selenite enables structured and efficient navigation by selected criterion through clicking the “previous / next” buttons ( a ) , after which Selenite will automatically scroll the page to reveal the previous / next mentioning of the target criterion . in finding out if there were any details about the maneuverability of other stroller options as well . To facilitate this , he decided to use the “previous / next” buttons ( Figure 3a ) to quickly navigate among the paragraphs that discussed maneuverability . Here , the afore - mentioned annotations not only offer paragraph overviews during linear skimming but also act as bookmarks for non - linear navigation between distinct parts of the page that pertain to similar criteria . Later , when Adam encountered a particularly convoluted para - graph with multiple criteria and options that he couldn’t quite absorb after a first pass , he decided to leverage the “zoom in” fea - ture that Selenite offers — he can query for more comprehensive descriptions that clarify which sentences or phrases within the paragraph pertain to specific criteria and sentiments ( positive , neu - tral , or negative ) ( Figure 4 ) by clicking the “Analyze” button ( Figure 4a ) that appears when hovering the cursor over a paragraph . As Adam navigated the page and read the content , Selenite keeps track of the criteria that he paid attention to on the page based on dwell time [ 21 , 67 ] . When Adam reached the end of the current article , Selenite presented a summary block ( Figure 1f ) , automat - ically describing his research status and recommending possible next steps . This summary contains three sections : ( 1 ) criteria that users have seen evidence for based on implicit behavior tracking ( emphasizing their focus and priorities ) , ( 2 ) the remaining ones that occurred on the page that the user did not pay attention to ( to help users confirm that they skipped certain information intentionally , not by oversight ) , and ( 3 ) a set of suggested search queries that can potentially help users find subsequent web pages for broadening their perspectives and maximizing their information gain . As sug - gested by Selenite , Adam then searched for “baby strollers that are easy to assemble” and “brake & locking system of baby strollers” on Google , finding additional articles that contain information about these previously not encountered criteria . 4 . 2 Detailed Designs We now discuss how the various Selenite features are designed and implemented to support the design goals . The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces making it highly suitable for running errands and public venues . While the Cruz v2 is more expensive than most of the best full - size strollers and jogging competitors , many users feel its higher quality features more than justify the higher cost . The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces making it highly suitable for running errands and public venues . While the Cruz v2 is more expensive than most of the best full - size strollers and jogging competitors , many users feel its higher quality and easy - to - use features more than justify the higher cost . Durability Maneuverability Wheel type Maneuverability Price Zoom in 💡 Analyze a b c Price Maneuverability Wheel type Durability Figure 4 : When encountering a particularly convoluted paragraph ( e . g . , the paragraph on the left ) with multiple criteria and options that users can’t quite absorb in the first pass , they can click the “Analyze” button ( a ) and leverage the “zoom in” feature that Selenite offers to query for more comprehensive descriptions that clarify which sentences or phrases pertain to which specific criteria and sentiments . Selenite wraps phrases and sentences in colored boxes , with green denoting “positive” ( b ) , red denoting “negative” , and grey denoting “neutral” ( not shown ) . 4 . 2 . 1 [ D1 ] Providing Global Grounding using Common Criteria and Options Encountered . In Selenite , we explore the idea of having the system provide users with an initial overview of criteria that are typically significant and frequently considered by people when exploring a particular topic . Selenite also performs information extraction on each page to identify the options that a user has encountered during their sensemaking process . Naturally , users have the flexibility to reorder , pin , edit , add , or delete any options and criteria to tailor them precisely to their specific preferences . We discuss the relevant designs and the rationale behind those designs below : Automatically recognizing topics . Selenite goes beyond previous sensemaking systems [ 19 , 53 , 55 , 60 , 65 – 68 ] by autonomously iden - tifying and classifying web pages into broad topics based on their titles and content . For example , “React vs . Svelte : Performance , DX , and more” 5 , “Angular vs React vs Vue : Which Framework to Choose” 6 , and “What are the key differences between Meteor , Em - ber . js and Backbone . js ? ” 7 would all be recognized as “Comparison of JavaScript frameworks” ; while “iRobot vs Shark Vacuums Bought , Tested , and Compared” 8 and “The best robot vacuum you can buy right now” 9 would go under the topic of “best robot vacuums . ” Un - like previous systems that require users to manually create projects or folders [ 17 , 38 , 65 ] , Selenite further lowers the barrier for entry , enabling users to quickly begin utilizing and reaping the benefits of the system , especially from the list of commonly considered criteria based on that topic . To achieve this , we frame the topic recognition as a summariza - tion task for a large language model , i . e . , GPT - 4 , where the input context is the title and initial five paragraphs of a given web page . Specifically , we first asked GPT - 4 ( taking advantage of its gener - alizability to various domains [ 81 ] ) 10 to summarize the web page given its title and initial paragraphs ( with the temperature set to 0 to minimize LLM hallucination ) 11 and then offer a search phrase 5 https : / / blog . logrocket . com / react - vs - svelte / 6 https : / / www . codeinwp . com / blog / angular - vs - vue - vs - react / 7 https : / / backbone447 . rssing . com / chan - 17153281 / article10 . html 8 https : / / www . rtings . com / vacuum / learn / irobot - vs - shark 9 https : / / www . theverge . com / 22997597 / best - robot - vacuum - cleaner 10 Please refer to section D . 1 in the supplemental materials for the detailed prompt design . 11 Empirically , we found this step helped GPT - 4 to better engage with the context provided . It also aligns with the idea of Chain - of - thought prompting proposed by [ 111 ] . 6 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models that would enable one to find similar web pages using a modern search engine , which we use as the topic . Selenite then clusters the semantically similar topics ( note that each web page has an associated topic generated by GPT - 4 ) based on the cosine distances on topic semantic embeddings computed using SentenceBERT [ 93 ] , as shown in the previous paragraph . Naturally , users have the flexi - bility to manually create , edit , and remove topics , as well as reassign pages to different topics based on their personal opinions . Automaticallyretrievingcommonlyconsideredcriteria . Ifweadopt the “bottom - up” approach discussed in prior work [ 38 , 60 , 65 ] , an intuitive method for obtaining criteria would involve extracting them from individual paragraphs on a page . However , in our initial attempts , we found that this method faced significant challenges that limited its effectiveness . One of the main issues was the lack of uniformity among the criteria extracted from different paragraphs — each paragraph presented its own variations and nuances , making it difficult to establish a cohesive and standardized set of criteria ( similar to what was reported by the formative study participants ) . Additionally , the approach lacked a comprehensive global perspec - tive , failing to consider the broader context and overarching themes of the topic . As a result , manual review , correction , and unification of the extraction results were frequently necessary , making the process impractical and inefficient . To address these challenges , we instead explored an alternative “top - down” approach , where we directly query an “oracle” for a globally applicable and comprehensive set of criteria . We are par - ticularly inspired by recent research indicating that the majority of people’s information - seeking needs are not novel [ 70 ] — “previous people” have experimented with most search needs and synthesized information into summarized knowledge such as review articles . While it is impractical for individuals to process and synthesize vast amounts of information online , LLMs excel at this . Recent studies suggest that LLMs can be highly effective in processing and integrating information , making them potentially valuable for tasks like knowledge graph querying and retrieving common sense knowledge [ 2 , 110 , 116 ] , and , in our particular case , a suitable “or - acle” for providing a set of commonly considered criteria given a particular topic . In Selenite , we take advantage of the fact the LLMs have aggre - gated various review articles during its pre - training , and use GPT - 4 as a knowledge retriever — for any given topic , we prompt it to pro - duce a list of around 20 commonly considered criteria ( Figure 1c ) , complete with their respective names ( Figure 1c1 ) and descriptions ( Figure 1c2 ) for each topic . We strive to minimize potential anchor - ing bias by achieving a balance between relevance and diversity in our prompting strategy . On the one hand , we specifically requested criteria that are deemed as “most relevant to the topic , ” “frequently considered , ” and can “cover a broad range of perspectives . ” On the other hand , we adopted the Self - Refine technique [ 71 ] , employ - ing an iterative query approach with GPT - 4 . In each iteration , we requested the generation of five additional criteria that were “differ - ent , more diverse , and more important” than the previous ones . 12 We also relied on GPT - 4 for ranking the criteria based on their 12 Please refer to section D . 3 for the detailed prompt design . importance . 13 The whole generation process remains within a rea - sonable time frame ( approximately 5 to 10 seconds , depending on the topic ) , which we deemed sufficient to test the idea of supply - ing commonly considered criteria as a form of global grounding to users’ reading and sensemaking processes . Still , users have the freedom to request additional criteria ( without repetition ) if they believe the existing list is not comprehensive enough ( Figure 1c3 ) , or manually add criteria ( Figure 1c4 ) . We present an accuracy and coverage evaluation in section 5 that provides initial evidence that this approach is sufficient for our prototyping purpose . We leave for future work to experiment with advanced approaches , such as retrieval - augmented LLMs [ 63 ] , that would potentially provide increased perceived external validity . Automatically recognizing encountered options . Instead of relying on GPT - 4 to access its internal knowledge and retrieve a set of com - monly considered options , we instead leverage its zero - shot infor - mation extraction capability and expansive context window size [ 81 ] to directly extract options from the entire text content of a web page . This approach ensures that the options presented in the sidebar align with a user’s sensemaking process , i . e . , they are indeed what users have encountered as opposed to something that users would potentially never run into . It also circumvents the potential concern where the world knowledge of an LLM is out - of - date , for example , GPT - 4 only “knows” information up to September 2021 [ 81 ] . 14 In addition , it surpasses the limited heuristics employed in previous approaches such as Crystalline [ 67 ] , which rely on page titles and HTML < h > - tags as sources for options . This is crucial because stud - ies have consistently demonstrated that web developers frequently disregard semantic web standards and best practices [ 44 , 74 ] . For instance , it is common to find pages where every piece of content is enclosed in < div > tags regardless of their semantic roles . To enhance the reliability of the GPT - 4 - based option extraction pipeline , we have found it useful to follow these steps 15 : firstly , we prompt GPT - 4 to determine if a page ( based on its title and content ) is likely discussing multiple options , such as product review and comparison pages or blog posts , or focusing on a single option , like item detail pages on platforms like Amazon or Airbnb . Once deter - mined , we instruct GPT - 4 to extract the relevant options accord - ingly . Interestingly , it appears that GPT - 3 . 5 [ 82 ] lacks the necessary reasoning capabilities for this particular option extraction task . 4 . 2 . 2 [ D2 ] Providing Local Grounding using Page & Paragraph - level Summary and Annotation . To address the issues we uncovered in the formative study where convoluted and intertwined paragraph and page structures often result in overlooked information or an inability for users to comprehend information , Selenite offers the following features : In - context summaries and annotations of paragraphs . With ac - cess to the initial set of common criteria as well as the options 13 Inthesupplementalmaterials , wedocumentalistof24criteriathatSeleniteretrieves for the topic of “best baby strollers” in Table 5 to offer an intuition of the quality and coverage of our current GPT - 4 - based approach . 14 While the direct retrieval of criteria from LLMs may also face this potential issue , in practice , we operate under the assumption that criteria are unlikely to suddenly emerge or become outdated . 15 Empirically , we found these steps to be useful for our specific information extraction task using a large language model . 7 Liu et al . extracted from each page , Selenite performs content analysis on each paragraph within a given page to identify the specific criteria being discussed and presents them as in - context annotations above the respective paragraph ( Figure 1e ) . This feature enables users to swiftly scan through a page , understand the key points of each paragraph , and selectively concentrate on the paragraphs that are valuable and engaging for gathering information . Such content analysis is enabled by recent advances in large pre - trained transformer models [ 26 , 62 , 109 ] fine - tuned to perform zero - shot text classification tasks following a natural language in - ference ( NLI ) paradigm [ 118 ] . Specifically , for example , to assess if a given text ( e . g . , “ Angular is very hard to pick up ” ) covers the criterion of “ learning curve , ” we can input the text as the premise and a hypothesis of “ This content discusses { learning curve } . ” into the NLI model . The entailment and contradiction prob - abilities are then converted into label probabilities , indicating the likelihood that the content pertains to the specified criterion . We used the bart - large - mnli model 16 for this purpose and consid - ered options and criteria with a score above 0 . 96 as true positives , displaying them in descending order of scores . We determined this threshold empirically , prioritizing recall over precision , as discussed further in section 5 . In scenarios where users still struggle to comprehend content despite the presence of in - context annotations , Selenite can perform a deeper analysis on - demand by leveraging the advanced reasoning capabilities of GPT - 4 . Specifically , through parallel and carefully orchestrated prompts , Selenite produces a more comprehensive description that clarifies which sentences or phrases pertain to specific criteria and sentiments ( positive , neutral , or negative ) ( Fig - ure 4 ) . Although a formal evaluation of this method is beyond the scope of this work , recent research suggests that this analysis achieves state - of - the - art performance in terms of quality , accuracy , and granularity [ 14 , 81 ] , making it suitable for this work . Page - level overview of options and criteria . As evidenced by our formative study , providing a page - level overview of the information space to users can greatly assist them in reading and sensemaking tasks . To facilitate this , Selenite consolidates paragraph - level meta - data into the sidebar’s options and criteria entries , with the entries that are present on the page highlighted ( Figure 1c & d ) . This offers two key benefits . Firstly , it provides a comprehensive summary of all the available options and criteria specific to the current page , which allows users to quickly understand the focus of the page as well as judge its value against their personal interests . Secondly , Selenite enables structured and efficient navigation . By utilizing the “previous / next” button for a given criterion ( Figure 3a ) , users can swiftly navigate between distinct parts of the page related to identified criteria ( Figure 3 ) . This feature saves users time and effort , as it eliminates the need for manual searching and filtering , which our formative study found to be the common practice . It is worth noting that the combination of page and paragraph - level annotations effectively addresses a significant limitation ini - tially highlighted by Crystalline and further revealed in our for - mative study : the inability to manually recognize “latent / implicit criteria , ” where the same criterion can be expressed in various forms 16 The model can be accessed on - demand through a remote API service that we implemented . without being explicitly mentioned . For instance , it involves iden - tifying the criterion of “price” from a statement like “I bought this mp3 player for almost nothing” [ 90 ] . 4 . 2 . 3 [ D3 ] Dynamically Suggesting Next Steps in Sensemaking . Tra - ditional sensemaking systems have aimed to assist users in man - aging and arranging their past experiences , such as collecting and organizing information that they have encountered [ 38 , 52 , 60 , 65 , 67 , 68 ] . However , we propose that there is an untapped opportunity to utilize these activities to guide users’ attention towards their next steps in sensemaking . Selenite offers two such “forward - looking” benefits . Firstly , it reminds users of potentially overlooked criteria once they reach the bottom of a page . Secondly , it suggests explor - ing content related to criteria that users have not encountered or have limited evidence about , aiming to maximize information gain . To achieve this second objective , we leverage users’ reading activ - ities in terms of the subset of criteria they cared about ( determined by if a user spent time dwelling on a particular paragraph ) and the subset they have intentionally ignored ( those that exist on the page but users chose to skip reading about , i . e . , did not spend time on ) 17 to recommend additional relevant and diverse criteria to search for and read about from the remaining global list . This requirement for the suggested criteria to be both relevant and diverse is similar to the exploration - exploitation trade - off in information retrieval [ 4 ] , and has also been extensively documented in recommender system literature — they help maintain user engagement and interest while avoiding over - fitting and filter bubbles [ 59 , 103 ] . To operationalize this idea , we can consider it as a graph prob - lem : By constructing a fully connected graph using the global list of criteria as vertices , we assign edge weights as distances between re - spective criteria in a semantic embedding space and vertex weights as the criterion’s relevance to the subset of criteria that the user cared about . Our objective then is to recommend a diverse subset of criteria ( vertices ) that have large distances between each other while still being relevant to what users cared about . That is , we need to find a sub - graph 𝐺 ′ of size 𝑛 , which maximizes a weighted ( 𝛽 > 0 ) sum of vertex weights 𝑤 𝑉 ( relevance ) and edge weights 𝑤 𝐸 ( diversity ) : arg max 𝐺 ′ ⊂ 𝐺 , | 𝐺 ′ | = 𝑛 𝛽 · 𝑤 𝑉 ( 𝐺 ′ ) + 𝑤 𝐸 ( 𝐺 ′ ) To build the graph , we measure relevance with the perplex - ity score of the sentence “ { global _ criterion } tend to be considered together ( or is a trade - off ) with { cared _ about _ criterion } ” using GPT - 2 [ 92 ] 18 , and characterize diversity with the cosine distance between the SentenceBERT [ 93 ] embeddings of 17 The threshold of determining if the user indeed paid attention to a paragraph is set to2secondsbasedonourempiricaltesting . Futureworkcaninvestigatemoreadaptive methods , such as taking into account the length of a paragraph , the amount of new information contained in a paragraph compared to users’ existing knowledge , or if users appear to be idling and performing irrelevant activities . 18 Perplexity is a measurement of how well a probability model predicts a sample [ 13 ] . In the context of natural language processing , it measures how well a language model predicts a given sequence of words . Here , we use perplexity to characterize how much a language model ( e . g . , GPT - 2 ) is “surprised” by seeing a given sentence . If the perplexity score for the sentence is low , it means that the model can predict the sequence of words well based on its learned language patterns . In other words , the sentence is statistically likely and coherent according to the language model . Unlike GPT - 3 . 5 or GPT - 4 , which are only available in the form of an auto - completion API at the time of writing of this thesis , GPT - 2 is open - source , and we can therefore obtain the perplexity of a sentence directly as the exponent of its inference loss [ 51 ] . 8 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models the two vertices ( criteria ) . Here , we follow the classic greedy peel - ing algorithm [ 115 ] by dropping vertices with the lowest weights ( the sum of vertex and every edge weight ) one at a time in a greedy fashion until the graph size reaches 𝑛 = 2 . We provide suggested search keywords based on specific criteria ( Figure 1f ) , allowing users to refine their queries ( e . g . , “comparison of JavaScript frameworks , on community support ” , or “best baby strollers , on ease of assembly ” ) . This aims to help users find and review targeted information more effectively , ensuring a consistent and continuous acquisition of knowledge . 4 . 3 Implementation Notes The Selenite browser extension is implemented in HTML , Type - Script , and CSS and uses the React JavaScript library [ 31 ] for build - ing UI components . It uses Google Firebase for backend functions , database , and user authentication . We leverage GPT - 4 for several use cases in Selenite , and encoun - tered several challenges 19 : First , due to the limited context window size of GPT - 4 ( 8192 tokens or approximately 6100 English words ) , we occasionally need to divide the entire text content of a web page into smaller chunks and run parallel queries to extract options . As of July 2023 , we don’t have access to the version of GPT - 4 with a 32k context window , which would significantly reduce the need for chunking and parallel queries . Second , unfortunately , there are oc - casions when the GPT - 4 model becomes overloaded with requests or takes an exceptionally long time to respond . To mitigate these problems and provide uninterrupted user experience to Selenite users , we have employed the following two approaches : 1 ) Dual API requests : We send two identical requests using separate API keys simultaneously . We prioritize the response that returns first with valid information , indicating that it is not an error and con - tains the requested information from the prompt ; 2 ) Graceful error handling & retry : In the event of an error , we introduce a random delay ( ranging from 1 to 5 seconds ) before retrying the request . We repeat this retry process for up to 5 attempts , allowing sufficient opportunity for a successful response . Note that these issues are attributable , in part , to the current limited beta status of GPT - 4 . Consequently , it is uncertain whether these issues will persist in the future . Nevertheless , we delve into them here to provide a com - prehensive and accurate accounting of our experience interacting with the API . To efficiently perform natural language inference ( NLI ) during the analysis of article content to produce per - paragraph summaries and annotations of options and criteria , we experimented with both the roberta - large - mnli and bart - large - mnli models that are fine - tuned for multi - genre natural language inference ( MNLI ) tasks 20 , and ended up using the latter for its better performance in our informal testing . In addition , we implemented a REST API service that the Chrome extension can query on demand . To de - crease model inference time and ensure a smooth user experience , we ran the service on multiple Google Cloud virtual machines with NVIDIA L4 GPUs . This arrangement proved to be at least 120 times 19 Due to length limitations , we document the specific prompt designs for those tasks in section D in the supplemental materials , and only discuss a few challenges we experienced while interacting with the GPT - 4 API here in this section . 20 These two models are considered to be able to achieve state - of - the - art performance as of June 2023 . faster compared to using only a CPU for inference . Typically , it can process all the content paragraphs from a single webpage ( against up to 10 different options and 20 different criteria ) within 5 seconds . As explained previously , we implemented Selenite using state - of - the - art NLP models : off - the - shelf GPT - 4 [ 81 ] and NLI models finetuned on BART [ 62 ] . These models were chosen for their strong performance that would satisfy our prototyping needs as well as their generalizability across different application domains ( c . f . Sec - tion 5 ) . However , it is important to note that our contributions lie more in the concept of grounded reading , interface design , and un - derlying NLP task abstractions , which are independent of specific model usage . We anticipate that these designs will remain valid as AI techniques continue to advance [ 69 ] . 5 STUDY 1 : ACCURACY AND COVERAGE EVALUATION While Selenite can help ground users in what to read , its impact may backfire if the list of options and criteria is not accurate or comprehensive — Anchoring bias [ 108 ] 21 may cause readers to more easily miss information that is indeed included in the page but not reflected in the Selenite - generated options and criteria list . Here we evaluate whether Selenite can : ( 1 ) accurately report options that are present on a web page , ( 2 ) comprehensively report critical criteria people commonly consider , by testing it on a set of diverse topics . 5 . 1 Methodology 5 . 1 . 1 Topic Sampling . We collected ten topics that exhibit a mix - ture of practicality and diversity ( Table 1 ) : ( 1 ) we randomly sampled 5 topics ( out of the 28 topics reported ) reported by participants in the formative study , and ( 2 ) we collected 5 more from Wirecutter , a popular review site — the three most popular product guides listed in their 2021 year - in - review ( at the time of writing , the 2022 year - in - review has not been published ) as well as their two most recently updated guides for June 2023 . 5 . 1 . 2 Groundtruth Dataset Creation for Options and Criteria . To collect groundtruth criteria that the general audience would care about for each topic , we mimic a typical information collection workflow , where people rely on top sources from popular search engines for their authenticity and credibility . Specifically , we first gathered the top five Google search results using the query template “ best [ product or category ] ” ( excluding promotions or ads ) . Then , for each web page , two authors independently first read through and annotated the options and criteria mentioned in every paragraph , and then merged all the annotations , excluding duplicate ones . Note that since many criteria are mentioned in a descriptive manner ( e . g . , the phrase “It is available in a black finish” implicitly refers to “aesthetics” ) , the two authors had some variance in how they named essentially the same criteria . Therefore , the two authors iteratively discussed and resolved their conflicts , merging criteria that they believed were semantically equivalent , producing the 21 Anchoring bias refers to people’s inclination towards relying too excessively on the initial set of information they were exposed to on a topic . Regardless of the accuracy or quality of that information , people use it as a reference point , or an “anchor , ” to make subsequent judgments or decisions . 9 Liu et al . Topic Count Topic - Level Paragraph - Level # Groundtruth # Selenite Precision Recall F1 Precision Recall F1 Best washing machines 19 24 0 . 88 1 . 0 0 . 93 0 . 91 1 . 0 0 . 95 Birthday gift ideas 11 21 0 . 57 0 . 91 0 . 70 0 . 57 0 . 96 0 . 72 Best hybrid app frameworks 15 21 0 . 86 0 . 93 0 . 89 0 . 83 1 . 0 0 . 91 Best time tracking tools 21 21 0 . 81 0 . 95 0 . 88 0 . 88 0 . 98 0 . 93 Deep learning frameworks 25 20 0 . 80 0 . 84 0 . 82 0 . 87 0 . 95 0 . 91 Best sleeping bags 19 21 0 . 81 0 . 89 0 . 85 0 . 95 1 . 0 0 . 97 Best air purifiers 20 24 0 . 83 1 . 0 0 . 91 0 . 83 0 . 98 0 . 90 Best robot vacuums 23 28 0 . 82 1 . 0 0 . 90 0 . 95 1 . 0 0 . 97 Best baby strollers 22 24 0 . 92 1 . 0 0 . 96 0 . 81 1 . 0 0 . 90 Best tropical vacation spots 15 19 0 . 74 0 . 93 0 . 82 0 . 92 1 . 0 0 . 96 Mean 19 . 0 22 . 3 0 . 80 0 . 95 0 . 87 0 . 85 0 . 98 0 . 91 Table 1 : Statistics of the accuracy and coverage evaluation on Selenite’s capability to retrieve a high - quality set of commonly considered criteria by topic . groundtruth criteria list . Finally , for the topics that we sampled from the formative studies where participants explicitly collected options and criteria using Unakite , we double - checked and were able to verify that all the criteria that they identified were indeed included in our groundtruth dataset , providing preliminary evidence to the soundness of our groundtruth dataset . 5 . 1 . 3 Evaluation Metrics . Option Extraction . Since in Selenite , we directly extract options from web pages , we evaluated this capability using the accuracy , that is , the percentage of options extracted by Selenite out of all the options available on a page . Criteria Retrieval . We also evaluated Selenite’s ability to retrieve the right set of criteria on two levels . First , to answer whether Selenite helps find useful criteria for each topic , we compute topic - level precision ( “the fraction of criteria retrieved by Selenite that were in the groundtruth” ) and recall ( “the fraction of groundtruth criteria that are were retrieved by Selenite” ) . Second , tomeasurewhetherSelenite provideshigh - qualityground - ings per paragraph , we additionally randomly sampled 20 para - graphs per topic , and computed paragraph - level precision ( “the frac - tion of criteria recognized by Selenite that were indeed mentioned in the paragraph” ) and recall ( “the fraction of criteria mentioned in the paragraph that were recognized by Selenite” ) . 5 . 2 Results 5 . 2 . 1 Option Extraction . Selenite achieved 100 % accuracy on ex - tracting options from web pages , i . e . , as long as there was an option explicitly mentioned on a web page , Selenite was able to correctly extract it . This directly speaks to the strong reasoning and infor - mation extraction capabilities of GPT - 4 as described in OpenAI’s technical report [ 81 ] . 5 . 2 . 2 Criteria Retrieval . We present the result of criteria retrieval evaluation metrics in Table 1 , which provides initial evidence to Selenite’s strong capability in presenting to the user a comprehen - sive set of criteria that people commonly consider . Notice that for most of the topics , Selenite retrieved more criteria compared to the groundtruth set . This is not surprising , partly due to the fact that GPT - 4 has likely synthesized information from significantly more sources than what was considered during the construction of the groundtruth dataset ( five web pages for each topic ) . Theoretically , there is also a possibility that GPT - 4 hallucinated some criteria that are largely irrelevant to a given topic , however , upon further man - ual inspection , we did not see evidence of hallucination , at least for the 10 topics considered in this evaluation ( for example , Table 5 shows a list of commonly considered criteria that Selenite retrieves for the topic of “best baby strollers” ) . Topic - level recommendations . Selenite achieved both high recall and high precision on multiple topics ( e . g . , best washing machines , best baby strollers , and best robot vacuums ) , and usually achieves higher recall than precision , suggesting that Selenite has the ten - dency of finding supersets of what users would generally be able to identify from reading , i . e . , criteria in the groundtruth set . We qualitatively analyzed the topics with a lower - than - average topic - level criteria recall , and found two contributing reasons : ( 1 ) Some web pages cover factual information that is not necessarily relevant . Multiple pages describing Best Hybrid App Frameworks mentioned First Release Date , which arguably is not a criterion neces - sary for selection . ( 2 ) Some criteria are inter - correlated . For example , in the case of deep learning framework , whereas it did not explic - itly mention “ growth speed , ” Selenite did suggest “ innovation , ” whose description is “ the ability of the framework to stay up - to - date with the latest research and developments in deep learning , and to incorporate new techniques and architectures as they emerge . ” While we did not count these two as equivalent in the evaluation , in practice , these two have a high correlation , and we believe having one included might be sufficient . Still , this potential mismatch reflects the necessity of allowing users to edit the criteria and descriptions . Meanwhile , upon initial observation , Selenite’s lower precision on certain topics may suggest its inclination towards retrieving unnecessary criteria . However , a closer examination revealed an interesting insight : For instance , when it comes to topics like birth - day gift ideas , popular web pages often present a list of 10 + diverse options that lack strict comparability and are all described using generic terms such as “fun” or “sweet . ” This lack of specificity makes it challenging to determine a comprehensive set of groundtruth criteria . In contrast , Selenite offers comprehensive overviews that 10 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models encompass factors like personalization , uniqueness , practicality , sen - timentality , and presentation ( wrapping ) , among others . Paragraph - levelGrounding . Selenitealsoachievedhighper - parag - raph performances , again with a bias towards higher recalls . This is intentional — we tuned the parameters of the NLI - based method such that it is more likely for Selenite to claim non - existing criteria than overlooking actual existing ones . This approach prioritizes avoiding information loss , which , suggested by prior work [ 69 ] , is a more expensive mistake compared to user verification . We order the criteria based on their probability score from the NLI model and will , in future iterations , fade the ones with a lower score . We did notice that in some rare circumstances , the NLI perfor - mance can be influenced by a criterion’s description , e . g . , changing “appropriate for age” to “appropriate for kids , adults , or elderly” can reduce Selenite’s error on recognizing arbitrary numbers as ages . Therefore , in future iterations of Selenite , we will provide a hint to users , prompting them to try tweaking the description when they attempt to delete a criterion due to its seemingly low grounding efficacy . 6 STUDY 2 : USABILITY EVALUATION In addition , we conducted an initial usability study to verify if the features provided by Selenite are usable and if the approach of providing global as well as contextual grounding can allow users to read , navigate , and comprehend information more efficiently . Specifically , we were interested in the following quantitative re - search questions : • [ RQ1 ] Does using Selenite speed up people’s process of reading and understanding information ? • [ RQ2 ] Does using Selenite help people achieve a more compre - hensive understanding of an information space ? • [ RQ3 ] Can Selenite help people obtain new information in ad - dition to their existing knowledge ? 6 . 1 Participants We recruited 12 participants ( 5 female , 7 male ) aged 21 - 40 ( mean age = 28 . 9 , SD = 5 . 2 ) through emails and social media . Participants were required to be 18 or older and fluent in English . All participants reported that they regularly engage in the process of seeking and sifting through large volumes of online information , whether for professional or personal purposes , on a weekly basis . 6 . 2 Procedure The study was a within - subjects design , where participants were presented with two tasks and were asked to complete each one under a different condition , counterbalanced for order . For each task , participants were given a topic that they needed to investigate and two web pages relevant to the topic that they were required to read and process . The two topics were “best baby strollers” 22 and “best robot vacuums” 23 . The provided two web pages for each respective topic were all product comparison pages used in the 22 The two web pages that participants were required to read for “best baby strollers” are : 10 Best Strollers of 2023 | Tested by GearLab and 11 Best Baby Strollers of 2023 , Tested by Parents & Experts 23 The two web pages that participants were required to read for “best robot vacuums” are : The best robot vacuum cleaners to get in 2023 - The Verge and Best robot vacuums in 2023 tested and rated | Tom’s Guide previous accuracy and coverage evaluation ( see section 5 . 1 ) . For each task , participants were asked to read through the two required pages , either by themselves without any aid ( a control condition simulating how people normally read ) or with Selenite ( experimen - tal condition ) . While reading , they were instructed to write down as many criteria as they learned and thought were important for the topic as well as the reason why they were important as if they needed to thoroughly explain the topic to a friend later . After fin - ishing the two pages , participants were instructed to optionally search ( using Google ) and gather additional information that they still wanted to learn about but weren’t able to from reading the required pages . We imposed a 25 - minute limit per task to keep participants from getting caught up in one of the tasks . However , they were instructed to inform the researcher that they felt like they could make no further progress , i . e . , having learned as much as they could about the given topic . To ensure realism and participant engagement , the tasks were selected based on actual topics that the formative study participants reported investigating . Rather than letting participants search for their own pages to read from the get - go , we provided them with a predefined set of pages to enable a fair comparison of the results ( e . g . , speed , etc . ) . Requiring participants to use predefined pages ( each contains , on average , 15 screenfuls of content ) for the first portion of the study also helps ensure that the two tasks are of roughly equal difficulty in terms of reading and cognitive processing effort . As described in the results , there was no significant difference by task . Each study session started by obtaining consent and having par - ticipants fill out a demographic survey . Participants were then given a 5 - minute tutorial showcasing the various features of Selenite and a 5 - minute practice session before starting . At the end of the study , the researcher conducted a NASA TLX survey and a questionnaire , eliciting feedback on their experience in both conditions . Each study was conducted via Zoom for about 60 minutes . Each participant was compensated with $ 15 USD . The study was approved by our institution’s IRB . 6 . 3 Results All participants were able to complete all tasks in both condi - tions , and nobody went over the pre - imposed time limit . Below , we present primarily quantitative evidence to evaluate the usability of Selenite with respect to our research questions . First , we were interested in understanding if Selenite can help participants read and process information faster compared to the baseline condition ( RQ1 ) . To examine this , we measured the time it took for them to finish reading all the materials in each task . A two - way repeated measures ANOVA was conducted to examine the within - subject effects of the condition ( baseline vs . Selenite ) and task on completion time . There was a statistically significant effect of condition ( F ( 1 , 20 ) = 102 . 5 , p < 0 . 01 ) such that participants completed tasks significantly faster ( 36 . 3 % ) with Selenite ( Mean = 840 . 3 seconds , SD = 102 . 7 seconds ) than in the baseline condition ( Mean = 1319 . 3 seconds , SD = 120 . 0 seconds ) . There was no sig - nificant effect of task ( F ( 1 , 20 ) = 0 . 40 , p = 0 . 53 ) , indicating the two tasks were indeed of roughly equal difficulty . These results suggest that Selenite helped participants read and comprehend information 11 Liu et al . Mental demand Physical demand Temporal demand Performance Effort Frustration Selenite 3 . 0 ( 3 . 03 ± 1 . 76 ) * 1 . 0 ( 0 . 51 ± 1 . 74 ) 2 . 5 ( 2 . 26 ± 1 . 68 ) * 8 . 5 ( 8 . 47 ± 1 . 32 ) * 3 . 5 ( 4 . 08 ± 1 . 88 ) * 0 . 5 ( 0 . 33 ± 1 . 51 ) Baseline 6 . 5 ( 6 . 43 ± 2 . 07 ) * 1 . 0 ( 0 . 79 ± 1 . 98 ) 4 . 0 ( 4 . 29 ± 2 . 08 ) * 6 . 5 ( 6 . 54 ± 1 . 73 ) * 6 . 0 ( 5 . 98 ± 2 . 23 ) * 1 . 0 ( 0 . 89 ± 1 . 91 ) Table 2 : Study 2 participants’ responses to NASA TLX questions ( on a scale from 0 to 10 ) in study 2 . Format : median ( mean ± standard deviation ) . Statistically significant differences ( p < 0 . 05 ) through t - tests are marked with an * . Question category Statement Response Comprehensibility I would consider my interactions with the tool to be understandable and clear . 6 ( 6 . 33 ± 1 . 10 ) Learnability I would consider it easy for me to learn how to use this tool . 7 ( 6 . 71 ± 1 . 04 ) Enjoyability I enjoyed the features provided by the tool . 6 ( 6 . 13 ± 1 . 72 ) Applicability Using this tool would make solving sensemaking problems more efficient and effective . 6 ( 6 . 28 ± 1 . 39 ) Recommendability If possible , I would recommend the tool to my friends and colleagues . 6 ( 6 . 23 ± 0 . 94 ) Table 3 : Study 2 participants’ responses to System Usability Scale questions ( on a scale of 1 to 7 , where 1 represents “strongly disagree” and 7 represents “strongly agree” ) in study 2 regarding their Selenite experience . Format : median ( mean ± standard deviation ) more efficiently . We discuss additional qualitative insights into why Selenite was more efficient in the following open - ended case study ( section 7 . 3 ) . In addition , we were interested in understanding if Selenite can help participants achieve a more comprehensive understanding of a topic ( RQ2 ) . To measure this , we first compared the quantity of criteria that participants externalized under each condition . As a pre - filtering step , two researchers rated all the criteria that participants externalized as either valid or invalid blind to the conditions . Valid criteria are considered as ones that are relevant to the topic and backed by specific evidence that can be traced back to the content , consistent with those standards used by prior work in judging the quality of subjective evidence [ 17 ] . After resolving conflicts ( which were minimal ) between the two researchers and filtering out the criteria that were invalid , we found that the average total number of valid criteria increased by 90 . 4 % when using Selenite ( Mean = 12 . 93 , SD = 3 . 90 ) compared to the baseline condition ( Mean = 6 . 79 , SD = 4 . 07 ) , which is statistically significant ( p < 0 . 01 ) under a t - test . Thus , using Selenite appeared to enable participants to identify and learn significantly more criteria about a topic compared to people’s current way of reading information . In addition to quantity , we also examined the quality of the cri - teria by comparing the ones that participants externalized with the groundtruth criteria curated in the previous accuracy and cov - erage evaluation — we can calculate the precision ( calculated as 𝑛 Hit / 𝑛 Total ) and recall ( calculated as 𝑛 Hit / 𝑛 Groundtruth ) of partic - ipants’ criteria that hit the groundtruth ( where 𝑛 Total is the total number of valid criteria participants externalized , and 𝑛 Groundtruth is the number of groundtruth criteria for each task ) . On average , participants in the Selenite condition achieved significantly ( p < 0 . 05 ) higher precision ( 98 . 8 % vs . 78 . 4 % ) as well as significantly ( p < 0 . 05 ) higher recall ( 73 . 0 % vs . 30 . 4 % ) in both tasks . Thus , using Selen - ite appeared to have enabled participants to improve the quality of their understanding of an information space in terms of its criteria . Furthermore , to understand if Selenite can help participants ob - tain new information in addition to what they have already learned from reading the two required pages ( RQ3 ) , we examined : 1 ) the number of additional searches that they performed in the Selen - ite condition ( Mean = 2 . 01 , SD = 1 . 39 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 33 , SD = 0 . 62 ) ; 2 ) the number of additional pages visited in the Selenite condition ( Mean = 2 . 76 , SD = 2 . 32 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 42 , SD = 0 . 74 ) ; and 3 ) the number of additional criteria that participants externalized in the Selenite condition ( Mean = 1 . 58 , SD = 0 . 91 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 33 , SD = 0 . 22 ) . These results suggest that Selenite did encourage and help participants to seek additional information beyond their existing perspective . Last but not least , participants filled out a NASA TLX [ 40 ] cog - nitive load scale and a System Usability Scale ( SUS ) [ 61 ] question - naire for each condition . SUS Likert items were integer - coded on a scale from 1 ( strongly disagree ) to 7 ( strongly agree ) . The median response values are presented in Tables 2 and 3 . Notably , partic - ipants perceived Selenite to have significantly lowered workload across mental , temporal , and effort demands as well as significantly increased perceived performance based on paired t - tests ) . This suggests that using Selenite can reduce the cognitive load and in - teraction costs when reading and understanding information , even when users had to learn and get used to a new user interface . 7 STUDY 3 : OPEN - ENDED CASE STUDY Encouraged by the promising performance outcome of the previous two studies , we conducted a third open - ended case study to un - derstand the usefulness and effectiveness of the Selenite prototype from a qualitative perspective . 7 . 1 Participants We recruited 8 participants ( 3 male , 5 female ; 3 students , 2 soft - ware engineers , 1 dermatologist , 1 accountant , and 1 researcher ) aged 24 - 55 years old ( Mean = 33 . 6 , SD = 8 . 1 ) through emails and social media . The same recruitment requirements were applied , but individuals who participated in the previous usability study were excluded from this study . 7 . 2 Methodology Each participant first completed two pre - defined tasks , where they were instructed to use Selenite to help them read information about 12 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models an unfamiliar topic . From the total 28 topics that participants re - ported having explored in the formative study , we randomly se - lected two that the participant was unfamiliar with ( indicated in their screening survey ) . For each task , participants were presented with a set of three web pages that covered the topic that the forma - tive study participants had gone through . The provided web pages were primarily review articles comparing several options together or product detail pages . We imposed a 20 - minute limit per task to keep participants from getting caught up in one of the tasks . To further explore Selenite’s potential , all participants were then instructed to use Selenite to help them make sense of a third topic that they intend to explore in real - life . In order to encourage the exploration of potential new insights , we purposefully did not limit the topic to be unfamiliar to the participants . This allows partici - pants to optionally revisit previous topics of interest and potentially uncover fresh perspectives . Each study session began by obtaining consent and having par - ticipants fill out a demographic survey . Participants were then given a 5 - minute tutorial showcasing the various features of Selen - ite and a 5 - minute practice session before starting . At the end of the study , the researcher conducted a semi - structured inter - view , eliciting feedback on using Selenite . The interviews were recorded and transcribed , after which qualitative coding and the - matic analysis [ 20 ] were performed by the first author . Each study was conducted via Zoom for up to 60 minutes . Each participant was compensated with $ 15 USD . The study was approved by our institution’s IRB . 7 . 3 Results Below , we present the major qualitative findings from the obser - vation of participants’ behaviors using Selenite as well as their feedback from the post - study interviews . 24 Time and effort savings . All of the participants mentioned that using Selenite would save them a lot of time and effort compared to using their typical reading and information collection workflow , echoing the quantitative results reported in the usability evaluation ( see section 6 ) . First of all , having access to the global overview felt like “ a game - changer ” ( P8 ) that offers a “ bird’s - eye view ” ( P4 ) or access to “ on - demand expert opinion ” ( P1 ) that “ took away the anxiety and guesswork of wondering what other folks would actually care about ” ( P5 ) . P7 suggested that “ this is something that I always wished for when reading about stuff that I’m not an expert in . It se - riously saves me a ton of time that I’d otherwise spend trying to wrap my head around it little by little , ” while P6 , who couldn’t “ stand the huge deal of work of figuring out stuff that I’m not used to ” said “ now I really feel like I’m chilling in the passenger seat and not having to do all the heavy - lifting personally . ” Second , participants seemed to appreciate the in - context anno - tations and summaries of each paragraph provided by Selenite . They thought that this feature “ made things incredibly easy ” ( P3 ) by “ helping me grasp the key points without wasting time reading a paragraph through ” ( P1 ) , and “ felt like back in the day when my classmate would mark all the important stuff in the textbook after a class when I couldn’t make it . ” However , some did report that 24 To see all the topics that participants explored in this study , please refer to Table 6 in the supplemental material . the in - context annotations can occasionally be “ a little bit distract - ing ” , especially for paragraphs that are “ apparently unrelated to the main content ” ( P2 ) , such as those that talk about related articles or terms of services , suggesting that future versions of Selenite should consider more robust content filtering techniques . Last but not least , participants also appreciated that Selenite can help them brainstorm search queries that would enable them to find new information more efficiently that was “ almost always one step ahead ” ( P4 ) , especially in the third task . For example , after reading two review articles about e - readers , Selenite suggested that P5 could do some additional investigations about “supported file formats” and “syncing across devices . ” P5 admitted that “ I’d totally miss those if I’m by myself , and even if I’m trying to be super careful , it would take me forever to figure out that I need to check out those as - pects . ” Additionally , we observed that when integrating the Selenite suggested criteria into subsequent search queries , the search engine did return result pages that turned out to be noticeably different yet sufficiently high - quality for users to explore . Impact on reading patterns and habits . Participants all mentioned that they immediately checked out the commonly considered crite - ria from the sidebar before diving into reading the first web page . They claimed that compared to what they normally do , which is “ just have to hunker down and read ” , reading the overview first helped them “ cut to the chase and get a feel of what’s out there ” ( P7 ) and remind them of criteria that would otherwise “ slip my [ their ] mind ” ( P3 ) . On a per - paragraph level , we noticed an initial hesitation among some participants ( 3 out of 8 ) towards relying solely on the pro - vided criteria labels . As a safety precaution , they personally read through a handful of paragraphs to confirm the labels’ accuracy and reliability . We further corroborated this observation with their reflections , such as “ I’ve never seen anything like this before , so hon - estly , I was a bit skeptical at first . But hey , everything looked legit ! ” ( P5 ) After this initial hurdle , participants tend to “ rely on the labels to tell me the gist of a paragraph ” ( P4 ) and only read paragraphs that discuss criteria that they truly cared about . For the content that participants did end up reading , they think the corresponding criteria “ definitely helped me [ them ] process and digest it better ” ( P6 ) , and even “ saved me [ them ] from otherwise misunderstanding things ” ( P7 ) . For example , while exploring healthy diet plans , P7 reflected that he would have initially thought a paragraph detailing the caloric allocation for each meal was seemingly discussing “calorie intake , ” however , Selenite preemptively clarified that the focus was on “portion control , ” i . e . , “providing guidelines on portion sizes . ” Participants also enjoyed the easy navigation feature that Selen - ite offers , and used it to frequently jump between different criteria mentionings for easy comparison and digestion ( 7 / 8 ) . They claimed that finding specific criteria about different options in a long article used to be “ link finding a needle in a haystack ” ( P8 ) that they were hesitant to do , but with Selenite , “ it’s more like following a well - lit path ” ( P5 ) . For example , P2 reflected on her experience exploring VPN solutions , and claimed that “ now I get it , McAfee Safe Connect seems to be keeping track of all sorts of my information while Surf - Shark doesn’t do any of that . If I can’t quickly switch between these two points on the page , by the time I reach SurfShark’s no - logging policy , I would have totally forgotten about what McAfee does , or 13 Liu et al . that I should even be concerned about logging at all . ” In addition , participants liked the fact that they can more effectively break out from the original structure and narrative of an article ; for instance , P1 recounted that “ you don’t gotta stick to what the authors say anymore , ya know ? Because , let’s face it , their storylines can get all tangled and complicated sometimes . ” Last but not least , we did not observe much usage of the “zoom in” feature , where Selenite can leverage GPT - 4 to provide a thor - ough analysis of a piece of content — only 3 participants tried it for a total of 8 times . We hypothesize that 1 ) the web pages utilized in the study were all professionally crafted , resulting in content that was relatively easy to comprehend ; 2 ) the criteria labels gen - erated by our NLI pipeline proved to be adequate in addressing the participants’ information needs ; 3 ) the time required for the “zoom in” feature to provide a useful analysis , typically ranging from 5 to 10 seconds , still exceeded the participants’ patience and attention span . Future work could explore solutions to address this limited adoption from these perspectives , for example , with models that boast significantly increased inference speeds . Additional findings . One interesting theme that emerged was that participants ( 4 / 8 ) opted to use Selenite in the third task to revisit topics that they had previously explored and wanted to be able to “ double - check ” ( P3 ) whether their prior understanding of the topic was truly comprehensive . Consistently , each participant uncovered something new that they hadn’t considered before . For example , P3 , who had recently been making plans to move in with his partner , revisited the topic of “choosing the right mattress , ” and realized that he had never taken into consideration criteria such as “motion transfer” ( i . e . , the extent to which movement on one side of the mattress affects the other side ) or “noise reduction” ( i . e . , the ability of the mattress to minimize noise from springs , coils , or other components ) , which prompted him to reassess his original mattress purchase . As another example , P4 , a professional software engineer , revisited the topic of “choosing a hybrid app framework” and discovered that he had neglected to consider the “Licensing and legal considerations” ( i . e . , compliance with licensing requirements and legal considerations ) as suggested by Selenite . Consequently , P4 was able to find additional evidence to confirm the validity of their original framework choice made back in 2017 . In the post - study interview , many participants ( 6 / 8 ) felt that now they “ can’t imagine reading without a tool like this ( Selenite ) ” ( P3 ) . Half even inquired about the possibility of installing Selenite on their personal computers for post - study usage , and we gladly fulfilled their requests . Despite encountering a few bugs in our research prototype during the study and having no obligation or incentives for continued usage after the study , the fact that they were willing to do so suggests that our grounded reading approach indeed holds value for our participants . 8 DISCUSSION Some of the participants ( 3 / 8 ) from the case study expressed con - cern about the coverage of Selenite’s overview criteria and the criteria labels for each paragraph at the beginning . They wondered if Selenite might overlook important criteria that they should also consider . This concern was valid , given that we presented the tool as an AI - powered oracle that could potentially be fallible or over - look certain factors and encouraged users to conduct their own explorations in addition to relying on Selenite’s insights . However , our accuracy and coverage evaluation described in section 5 pro - vides an initial validation that the criteria and options provided by Selenite are indeed comprehensive , relevant , and accurate . In addition , after the study , participants also acknowledged that the current set of criteria offered by Selenite already “ far exceeds what I [ they ] could identify and keep track of on my [ their ] own ” ( P4 ) ; therefore , they “ wouldn’t mind at all if the algorithm misses any minor ones ” ( P1 ) . Indeed , despite the participants’ awareness of the opportunity to request additional criteria from Selenite in the case of insufficient coverage ( as confirmed in the post - study interviews ) , we did not observe any instances of such usage . Nevertheless , fur - ther research is necessary to unlock : 1 ) methods that would further enhance the coverage and accuracy of Selenite , such as leveraging retrieval - augmented models ; 2 ) mechanisms and interventions de - signed to reduce over - dependence on Selenite as well as encourage user - led explorations with critical thinking . Though primarily designed as a tool for grounded reading , Se - lenite might also have the potential to address some of the issues identified by prior work regarding structuring information during sensemaking — prior research suggested that asking users to struc - ture information too early might lead to a more poorly structured information space [ 54 ] . In addition , the knowledge structures that people created often become obsolete , and new structures often emerge as their mental representations evolve over the course of their investigation [ 33 , 43 , 54 ] , resulting in having to spend sig - nificant effort in refactoring the structures every once in a while . Here , Selenite provides users with a well - structured framework from the outset , including a set of commonly acknowledged crite - ria . This readily usable scaffold serves as a starting point , aiming to encompass the majority’s perspective and thereby minimizing the necessity of refactoring or restructuring . Hopefully , it simplifies the iterative and cognitive - demanding process of building a mental model , transforming it into a possibly more manageable task of refining and pruning [ 67 ] . There wasalso a concern that GPT - 4 could potentially hallucinate or generate irrelevant or even false criteria and thus mislead users in their subsequent exploration . However , it is important to note that in our case study , as well as in study 2 , we did not observe any such episodes or evidence of this occurring . This could be attributed to the fact that the topics explored in the study were all common subjects with abundant source materials available online , which were likely encountered by GPT - 4 during its training process . We would like to further conjecture that even if hallucination occurs , users can readily identify irrelevant or false criteria by carefully reading their descriptions and comparing them with common sense or their intuitive knowledge about the topic , mitigating the actual impact of hallucination . 9 LIMITATIONS & FUTURE WORK Connections among criteria . In Selenite , we made the implicit as - sumption that criteria are completely independent . However , in real - ity , there could be connections among criteria — for instance , when 14 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models evaluating the “Best Baby Stroller , ” the specific criterion of “suspen - sion system” falls under the broader category of “safety” ( hierarchy ) , while on the other hand , aspects like “price” and “versatility” are typ - ically trade - offs that are impractical to optimize for simultaneously ( correlation ) . Currently , Selenite takes into account one form of cri - teria connections , i . e . , relevance between criteria , when suggesting the next steps . This proved promising in the study , which gave us reasons to believe that further exploiting these connections between criteria can better support users’ reading . For example , instead of presenting the criteria in a simple list , one can imagine creating a behind - the - scenes knowledge graph where criteria are connected using edges of relations ( TypeOf , CompetesWith , etc . ) . By initially displaying a portion of this graph and allowing users to “zoom in” on the specific criteria they are interested in ( e . g . , a subset of “safety” - related features ) , we can help users intuitively reason through an initially overwhelming list . In addition , one can again imagine “overview first , details later” - style UIs [ 100 ] that accommodate cri - teria hierarchies , e . g . , multi - level tables or lists , granting users the flexibility to combine or decompose criteria at decision time . Availability of domain knowledge in LLMs . LLMs , such as GPT - 4 , possess an extensive range of encoded knowledge , yet they might lack domain - specific information for specialized or emerging topics , as well as for topics involving confidential or sensitive information . Our technical implementation in Selenite is primarily based on extracting knowledge ( e . g . , commonly considered criteria ) from commercially available LLMs , and its effectiveness is highly depen - dent on the LLM’s capability to capture and synthesize relevant domain knowledge from its training data . Without such knowledge , the guidance provided may be subpar . In addition , LLMs themselves can sometimes be biased , and the response they generate might be incorrect or harmful [ 58 , 77 ] . However , our approach to ground the reading process with domain knowledge would also work with other sources of knowledge bases as well , for example , Unakite + Strata tables [ 65 , 66 ] , or crowdsourced [ 37 , 75 ] , or a combination of them . Furthermore , we should also urge users to thoroughly ex - amine the Selenite overview when dealing with critical situations . Generalizing beyond decision - making . In this work , we focus on helping people with decision - making tasks , where they often need to systematically compare different options with respect to various criteria . As evidenced by our formative as well as case studies , it is beneficial for people to be aware of the criteria that other people commonly consider upfront to help with their subsequent sense - making journey . However , there are other types of sensemaking tasks , such as those that are purely exploratory or investigatory ( e . g . , debugging , learning a new skill ) , that do not entail well - established options and criteria , and as such , they are not ideally compatible for Selenite to assist with . Nevertheless , we postulate that the notion of procuring comprehensive expert perspectives upfront may still apply in these non - decision - making tasks — for example , one can imagine asking an LLM for advice on a range of typical strategies to try when debugging or a list of common steps to take to master a new skill . Future research can work on enabling users to obtain these categories of overviews by adapting and customizing the LLM prompts used in Selenite ( that were originally used to obtain criteria ) and continue to receive similar in - context annotations and reading guidance grounded on those overviews . Impact on learning . The current design of Selenite functions as an “index” to direct users to relevant parts of a web page for reading and processing . However , we need to be cautious about a potential risk associated with this approach — some users might believe they have gained sufficient knowledge about a topic by merely reading the overview and may , therefore , skip engaging with the actual web content . This behavior could lead to incomplete , biased , or even inaccurate understandings of the subject . It is akin to only reading the table of contents or indices of a book without delving into the actual passages . Nevertheless , our studies conducted under controlled settings have shown that participants did , in fact , engage with the actual web content after going through the overviews . To build on this promising evidence , future research should addi - tionally investigate interface and interaction designs that motivate users to explore and read the actual web content with the assistance of Selenite - style guidance . One potential approach could be progres - sively revealing criteria information to users based on their reading behavior , encouraging deeper exploration and understanding . Field Study . In the future , once all the bugs and usability issues have been thoroughly addressed , we aim to conduct an extensive , long - term field study on a larger scale , where people will have both sufficient motivations to investigate topics relevant to their own personal context and familiarity with Selenite through repeated us - age . This will potentially help us gain insights into situations where Selenite performs reasonably well , as well as situations where it may fall short . Additionally , we are also interested in Selenite’s long - term impact on people’s analytical skills and problem - solving abilities . 10 CONCLUSION Making decisions in unfamiliar domains can often be both cogni - tively and physically demanding , with users having to sift through large volumes of information and compare different options with respect to various criteria . Previous sensemaking research as well as our new formative study has shown that people would benefit a lot from seeing an overview of the information space , such as the criteria that others have previously found useful . However , existing systems have been limited by the “cold start” issue — they require substantial effort from previous users to gather and structure infor - mation to produce such an overview , and , even if it’s been produced , they lack straightforward methods of sharing that overview with future users and guarantee that future users would find it to be comprehensive , unbiased , and useful . In this work , by leveraging recent advances in large language models and natural language processing , we introduce a novel sys - tem named Selenite that automates finding this initial set of options , criteria , and evidence matching them to provide a comprehensive overview to users at the start of their sensemaking process . In ad - dition , it also adapts as people use it , helping users find , read , and navigate unfamiliar information in a systematic yet personalized manner . As such , it provides a valuable proof of concept of how a future LLM - powered sensemaking tool that provides users with comprehensive overviews and in - context reading guidance can scaffold their decision - making and learning in an unfamiliar space . 15 Liu et al . REFERENCES [ 1 ] Mortimer J Adler and Charles Van Doren . 2014 . How to read a book : The classic guide to intelligent reading . Simon and Schuster . [ 2 ] Badr AlKhamissi , Millicent Li , Asli Celikyilmaz , Mona Diab , and Marjan Ghazvininejad . 2022 . A Review on Language Models as Knowledge Bases . https : / / doi . org / 10 . 48550 / arXiv . 2204 . 06031 arXiv : 2204 . 06031 [ cs ] . [ 3 ] Anthropic . 2023 . Claude . https : / / claude . ai / chats [ 4 ] Kumaripaba Athukorala , Alan Medlar , Antti Oulasvirta , Giulio Jacucci , and Dorota Glowacka . 2016 . Beyond Relevance : Adapting Exploration / Exploitation in Information Retrieval . In Proceedings of the 21st International Conference on Intelligent User Interfaces ( IUI ’16 ) . Association for Computing Machinery , New York , NY , USA , 359 – 369 . https : / / doi . org / 10 . 1145 / 2856767 . 2856786 [ 5 ] TalAugust , LucyLuWang , JonathanBragg , MartiA . Hearst , AndrewHead , and Kyle Lo . 2023 . Paper Plain : Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing . ACM Transactions on Computer - Human Interaction ( April 2023 ) . https : / / doi . org / 10 . 1145 / 3589955 Just Accepted . [ 6 ] Michelle Q . Wang Baldonado and Terry Winograd . 1997 . SenseMaker : An Information - exploration Interface Supporting the Contextual Evolution of a User’s Interests . In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems ( CHI ’97 ) . ACM , New York , NY , USA , 11 – 18 . https : / / doi . org / 10 . 1145 / 258549 . 258563 [ 7 ] Yejin Bang , Samuel Cahyawijaya , Nayeon Lee , Wenliang Dai , Dan Su , Bryan Wilie , Holy Lovenia , Ziwei Ji , Tiezheng Yu , Willy Chung , Quyet V . Do , Yan Xu , and Pascale Fung . 2023 . A Multitask , Multilingual , Multimodal Evaluation of ChatGPT on Reasoning , Hallucination , and Interactivity . https : / / doi . org / 10 . 48550 / arXiv . 2302 . 04023 arXiv : 2302 . 04023 [ cs ] . [ 8 ] CHARLES BAZERMAN . 1985 . Physicists Reading Physics : Schema - Laden PurposesandPurpose - LadenSchema . WrittenCommunication 2 , 1 ( Jan . 1985 ) , 3 – 23 . https : / / doi . org / 10 . 1177 / 0741088385002001001 Publisher : SAGEPublications Inc . [ 9 ] Krishna Bharat . 2000 . SearchPad : explicit capture of search context to support Web search . Computer Networks 33 , 1 ( June 2000 ) , 493 – 501 . https : / / doi . org / 10 . 1016 / S1389 - 1286 ( 00 ) 00047 - 5 [ 10 ] Andrea Bianchi , So - Ryang Ban , and Ian Oakley . 2015 . Designing a Physical Aid to Support Active Reading on Tablets . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems ( CHI ’15 ) . Association for Computing Machinery , New York , NY , USA , 699 – 708 . https : / / doi . org / 10 . 1145 / 2702123 . 2702303 [ 11 ] BlockTechnology . 2023 . AskYourPDF : The Best PDF AI Chat App . https : / / askyourpdf . com [ 12 ] Joel Brandt , Mira Dontcheva , Marcos Weskamp , and Scott R . Klemmer . 2010 . Example - centric Programming : Integrating Web Search into the Development Environment . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’10 ) . ACM , New York , NY , USA , 513 – 522 . https : / / doi . org / 10 . 1145 / 1753326 . 1753402 [ 13 ] Peter F Brown , Stephen A Della Pietra , Vincent J Della Pietra , Jennifer C Lai , and Robert L Mercer . 1992 . An estimate of an upper bound for the entropy of English . Computational Linguistics 18 , 1 ( 1992 ) , 31 – 40 . [ 14 ] Sébastien Bubeck , Varun Chandrasekaran , Ronen Eldan , Johannes Gehrke , Eric Horvitz , Ece Kamar , Peter Lee , Yin Tat Lee , Yuanzhi Li , Scott Lundberg , Harsha Nori , Hamid Palangi , Marco Tulio Ribeiro , and Yi Zhang . 2023 . Sparks of Artificial General Intelligence : Early experiments with GPT - 4 . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 12712 arXiv : 2303 . 12712 [ cs ] . [ 15 ] Jürgen Buder , Christina Schwind , Anja Rudat , and Daniel Bodemer . 2015 . Se - lective reading of large online forum discussions : The impact of rating visual - izations on navigation and learning . Computers in Human Behavior 44 ( March 2015 ) , 191 – 201 . https : / / doi . org / 10 . 1016 / j . chb . 2014 . 11 . 043 [ 16 ] Robert Capra and Jaime Arguello . 2023 . How does AI chat change search behaviors ? https : / / doi . org / 10 . 48550 / arXiv . 2307 . 03826 arXiv : 2307 . 03826 [ cs ] . [ 17 ] Joseph Chee Chang , Nathan Hahn , and Aniket Kittur . 2020 . Mesh : Scaffolding Comparison Tables for Online Decision Making . In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology ( UIST ’20 ) . Association for Computing Machinery , New York , NY , USA , 391 – 405 . https : / / doi . org / 10 . 1145 / 3379337 . 3415865 [ 18 ] JosephCheeChang , NathanHahn , AdamPerer , andAniketKittur . 2019 . Search - Lens : composing and capturing complex user interests for exploratory search . In Proceedings of the 24th International Conference on Intelligent User Interfaces ( IUI ’19 ) . Association for Computing Machinery , Marina del Ray , California , 498 – 509 . https : / / doi . org / 10 . 1145 / 3301275 . 3302321 [ 19 ] Joseph Chee Chang , Yongsung Kim , Victor Miller , Michael Xieyang Liu , Brad A Myers , andAniketKittur . 2021 . Tabs . do : Task - CentricBrowserTabManagement . In The 34th Annual ACM Symposium on User Interface Software and Technology . Association for Computing Machinery , New York , NY , USA , 663 – 676 . https : / / doi . org / 10 . 1145 / 3472749 . 3474777 [ 20 ] Kathy Charmaz . 2006 . Constructing Grounded Theory : A Practical Guide through Qualitative Analysis . SAGE . Google - Books - ID : 2ThdBAAAQBAJ . [ 21 ] Xiang’Anthony’ Chen , Chien - Sheng Wu , Tong Niu , Wenhao Liu , and Caiming Xiong . 2022 . Marvista : A Human - AI Collaborative Reading Tool . arXiv preprint arXiv : 2207 . 08401 ( 2022 ) . [ 22 ] Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Se - bastian Gehrmann , Parker Schuh , Kensen Shi , Sasha Tsvyashchenko , Joshua Maynez , Abhishek Rao , Parker Barnes , Yi Tay , Noam Shazeer , Vinodkumar Prabhakaran , Emily Reif , Nan Du , Ben Hutchinson , Reiner Pope , James Brad - bury , Jacob Austin , Michael Isard , Guy Gur - Ari , Pengcheng Yin , Toju Duke , Anselm Levskaya , Sanjay Ghemawat , Sunipa Dev , Henryk Michalewski , Xavier Garcia , Vedant Misra , Kevin Robinson , Liam Fedus , Denny Zhou , Daphne Ip - polito , David Luan , Hyeontaek Lim , Barret Zoph , Alexander Spiridonov , Ryan Sepassi , David Dohan , Shivani Agrawal , Mark Omernick , Andrew M . Dai , Thanumalayan Sankaranarayana Pillai , Marie Pellat , Aitor Lewkowycz , Er - ica Moreira , Rewon Child , Oleksandr Polozov , Katherine Lee , Zongwei Zhou , Xuezhi Wang , Brennan Saeta , Mark Diaz , Orhan Firat , Michele Catasta , Jason Wei , Kathy Meier - Hellstern , Douglas Eck , Jeff Dean , Slav Petrov , and Noah Fiedel . 2022 . PaLM : Scaling Language Modeling with Pathways . https : / / doi . org / 10 . 48550 / arXiv . 2204 . 02311 arXiv : 2204 . 02311 [ cs ] . [ 23 ] Roi Cohen , Mor Geva , Jonathan Berant , and Amir Globerson . 2023 . Crawling the Internal Knowledge - Base of Language Models . https : / / doi . org / 10 . 48550 / arXiv . 2301 . 12810 arXiv : 2301 . 12810 [ cs ] . [ 24 ] Dick Cunninghamand Scott L . Shablak . 1975 . Selective ReadingGuide - O - Rama : The Content Teacher’s Best Friend . The Journal of Reading ( 1975 ) . [ 25 ] BrendaDervin . 1983 . Anoverviewofsense - makingresearchconcepts , methods , and results to date . ( 1983 ) . http : / / www . worldcat . org / title / overview - of - sense - making - research - concepts - methods - and - results - to - date / oclc / 733067203 [ 26 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of Deep Bidirectional Transformers for Language Under - standing . arXiv : 1810 . 04805 [ cs ] ( May 2019 ) . http : / / arxiv . org / abs / 1810 . 04805 arXiv : 1810 . 04805 . [ 27 ] G . A . Di Lucca , M . Di Penta , and A . R . Fasolino . 2002 . An approach to identify duplicated web pages . In Proceedings 26th Annual International Computer Soft - ware and Applications . 481 – 486 . https : / / doi . org / 10 . 1109 / CMPSAC . 2002 . 1045051 ISSN : 0730 - 3157 . [ 28 ] Mira Dontcheva , Steven M . Drucker , Geraldine Wade , David Salesin , and Michael F . Cohen . 2006 . Summarizing Personal Web Browsing Sessions . In Proceedings of the 19th Annual ACM Symposium on User Interface Soft - ware and Technology ( UIST ’06 ) . ACM , New York , NY , USA , 115 – 124 . https : / / doi . org / 10 . 1145 / 1166253 . 1166273 [ 29 ] Paul Dourish and Victoria Bellotti . 1992 . Awareness and coordination in shared workspaces . In Proceedings of the 1992 ACM conference on Computer - supported cooperative work ( CSCW ’92 ) . Association for Computing Machinery , Toronto , Ontario , Canada , 107 – 114 . https : / / doi . org / 10 . 1145 / 143457 . 143468 [ 30 ] Evernote . [ n . d . ] . Best Note Taking App - Organize Your Notes with Evernote . https : / / evernote . com [ 31 ] Facebook . 2018 . React - A JavaScript library for building user interfaces . https : / / reactjs . org / [ 32 ] David K . Farkas and Christopher Raleigh . 2013 . Designing Documents for Selective Reading . Information Design Journal 20 , 1 ( Jan . 2013 ) , 2 – 15 . https : / / doi . org / 10 . 1075 / idj . 20 . 1 . 01far Publisher : John Benjamins . [ 33 ] Kristie Fisher , Scott Counts , and Aniket Kittur . 2012 . Distributed Sensemaking : ImprovingSensemakingbyLeveragingtheEffortsofPreviousUsers . In Proceed - ings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , New York , NY , USA , 247 – 256 . https : / / doi . org / 10 . 1145 / 2207676 . 2207711 [ 34 ] Andrew J . Flanagin and Miriam J . Metzger . 2000 . Perceptions of Internet Infor - mation Credibility . Journalism & Mass Communication Quarterly 77 , 3 ( Sept . 2000 ) , 515 – 540 . https : / / doi . org / 10 . 1177 / 107769900007700304 Publisher : SAGE Publications Inc . [ 35 ] RaymondFok , HitaKambhamettu , LucaSoldaini , JonathanBragg , KyleLo , Marti Hearst , Andrew Head , and Daniel S Weld . 2023 . Scim : Intelligent Skimming Support for Scientific Papers . In Proceedings of the 28th International Conference on Intelligent User Interfaces ( IUI ’23 ) . Association for Computing Machinery , New York , NY , USA , 476 – 490 . https : / / doi . org / 10 . 1145 / 3581641 . 3584034 [ 36 ] Stefan Haefliger , Georg von Krogh , and Sebastian Spaeth . 2007 . Code Reuse in Open Source Software . Management Science 54 , 1 ( Nov . 2007 ) , 180 – 193 . https : / / doi . org / 10 . 1287 / mnsc . 1070 . 0748 Publisher : INFORMS . [ 37 ] Nathan Hahn , Joseph Chang , Ji Eun Kim , and Aniket Kittur . 2016 . The Knowl - edge Accelerator : Big Picture Thinking in Small Pieces . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , USA , 2258 – 2270 . https : / / doi . org / 10 . 1145 / 2858036 . 2858364 [ 38 ] Nathan Hahn , Joseph Chee Chang , and Aniket Kittur . 2018 . Bento Browser : ComplexMobileSearchWithoutTabs . In Proceedingsofthe2018CHIConference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , Montreal QC , Canada , 251 : 1 – 251 : 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3173825 [ 39 ] Dianne J . Hall and Robert A . Davis . 2007 . Engaging multiple perspectives : A value - based decision - making model . Decision Support Systems 43 , 4 ( Aug . 2007 ) , 1588 – 1604 . https : / / doi . org / 10 . 1016 / j . dss . 2006 . 03 . 004 16 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models [ 40 ] Sandra G . Hart and Lowell E . Staveland . 1988 . Development of NASA - TLX ( Task Load Index ) : Results of Empirical and Theoretical Research . In Advances in Psychology , Peter A . Hancock and Najmedin Meshkati ( Eds . ) . Human Mental Workload , Vol . 52 . North - Holland , 139 – 183 . https : / / doi . org / 10 . 1016 / S0166 - 4115 ( 08 ) 62386 - 9 [ 41 ] Andrew Head , Kyle Lo , Dongyeop Kang , Raymond Fok , Sam Skjonsberg , Daniel S . Weld , and Marti A . Hearst . 2021 . Augmenting Scientific Pa - pers with Just - in - Time , Position - Sensitive Definitions of Terms and Symbols . arXiv : 2009 . 14237 [ cs ] ( April 2021 ) . http : / / arxiv . org / abs / 2009 . 14237 arXiv : 2009 . 14237 . [ 42 ] Andrew Head , Amber Xie , and Marti A . Hearst . 2022 . Math Augmentation : How Authors Enhance the Readability of Formulas using Novel Visual Design Practices . In Proceedings of the 2022 CHI Conference on Human Factors in Com - puting Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , 1 – 18 . https : / / doi . org / 10 . 1145 / 3491102 . 3501932 [ 43 ] Marti A . Hearst . 2014 . What’s Missing from Collaborative Search ? Computer 47 , 3 ( March 2014 ) , 58 – 61 . https : / / doi . org / 10 . 1109 / MC . 2014 . 77 [ 44 ] Lawrence J . Henschen and Julia C . Lee . 2009 . Using Semantic - Level Tags in HTML / XML Documents . In Universal Access in Human - Computer Interac - tion . Applications and Services ( Lecture Notes in Computer Science ) , Constantine Stephanidis ( Ed . ) . Springer , Berlin , Heidelberg , 683 – 692 . https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 02713 - 0 _ 72 [ 45 ] Terje Hillesund . 2010 . Digital reading spaces : How expert readers handle books , the Web and electronic paper . First Monday ( April 2010 ) . https : / / doi . org / 10 . 5210 / fm . v15i4 . 2762 [ 46 ] Ken Hinckley , Xiaojun Bi , Michel Pahud , and Bill Buxton . 2012 . Informal Information Gathering Techniques for Active Reading . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , NewYork , NY , USA , 1893 – 1896 . https : / / doi . org / 10 . 1145 / 2207676 . 2208327 event - place : Austin , Texas , USA . [ 47 ] Andrew Hogue and David Karger . 2005 . Thresher : automating the unwrapping ofsemanticcontentfromtheWorldWideWeb . In Proceedingsofthe14thinterna - tionalconferenceonWorldWideWeb ( WWW’05 ) . AssociationforComputingMa - chinery , New York , NY , USA , 86 – 95 . https : / / doi . org / 10 . 1145 / 1060745 . 1060762 [ 48 ] Johan F . Hoorn and Teunis D . van Wijngaarden . 2010 . Web Intelligence for the Assessment of Information Quality : Credibility , Correctness , and Readability . Web Intelligence and Intelligent Agents ( March 2010 ) . https : / / doi . org / 10 . 5772 / 8372 Publisher : IntechOpen . [ 49 ] Amber Horvath , Michael Xieyang Liu , River Hendriksen , Connor Shannon , Emma Paterson , Kazi Jawad , Andrew Macvean , and Brad A Myers . 2022 . Un - derstanding How Programmers Can Use Annotations on Documentation . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , 1 – 16 . https : / / doi . org / 10 . 1145 / 3491102 . 3502095 [ 50 ] Jane Hsieh , Michael Xieyang Liu , Brad A . Myers , and Aniket Kittur . 2018 . An Exploratory Study of Web Foraging to Understand and Support Programming Decisions . In 2018 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . 305 – 306 . https : / / doi . org / 10 . 1109 / VLHCC . 2018 . 8506517 ISSN : 1943 - 6092 . [ 51 ] HuggingFace . 2023 . Perplexity of fixed - length models . https : / / huggingface . co / docs / transformers / perplexity [ 52 ] Hyeonsu B . Kang , Joseph Chee Chang , Yongsung Kim , and Aniket Kittur . 2022 . Threddy : An Interactive System for Personalized Thread - based Exploration and Organization of Scientific Literature . https : / / doi . org / 10 . 1145 / 3526113 . 3545660 arXiv : 2208 . 03455 [ cs ] . [ 53 ] Aniket Kittur , Andrew M . Peters , Abdigani Diriye , and Michael Bove . 2014 . Standing on the Schemas of Giants : Socially Augmented Information Foraging . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing ( CSCW ’14 ) . ACM , New York , NY , USA , 999 – 1010 . https : / / doi . org / 10 . 1145 / 2531602 . 2531644 [ 54 ] AniketKittur , AndrewM . Peters , AbdiganiDiriye , TruptiTelang , andMichaelR . Bove . 2013 . Costs and Benefits of Structured Information Foraging . In Proceed - ings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’13 ) . ACM , NewYork , NY , USA , 2989 – 2998 . https : / / doi . org / 10 . 1145 / 2470654 . 2481415 [ 55 ] Aniket Kittur , Bongwon Suh , and Ed H . Chi . 2008 . Can you ever trust a wiki ? impacting perceived trustworthiness in wikipedia . In Proceedings of the 2008 ACM conference on Computer supported cooperative work ( CSCW ’08 ) . Association for Computing Machinery , San Diego , CA , USA , 477 – 480 . https : / / doi . org / 10 . 1145 / 1460563 . 1460639 [ 56 ] G . Klein , B . Moon , and R . R . Hoffman . 2006 . Making Sense of Sensemaking 1 : Alternative Perspectives . IEEE Intelligent Systems 21 , 4 ( July 2006 ) , 70 – 73 . https : / / doi . org / 10 . 1109 / MIS . 2006 . 75 [ 57 ] Torkel Klingberg . 2009 . The Overflowing Brain : Information Overload and the Limits of Working Memory . Oxford University Press , USA . Google - Books - ID : IxMSDAAAQBAJ . [ 58 ] Sachin Kumar , Vidhisha Balachandran , Lucille Njoo , Antonios Anastasopoulos , and Yulia Tsvetkov . 2023 . Language Generation Models Can Cause Harm : So What Can We Do About It ? An Actionable Survey . In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics , Dubrovnik , Croatia , 3299 – 3321 . https : / / aclanthology . org / 2023 . eacl - main . 241 [ 59 ] Matevž Kunaver and Tomaž Požrl . 2017 . Diversity in recommender systems – A survey . Knowledge - Based Systems 123 ( May 2017 ) , 154 – 162 . https : / / doi . org / 10 . 1016 / j . knosys . 2017 . 02 . 009 [ 60 ] Andrew Kuznetsov , Joseph Chee Chang , Nathan Hahn , Napol Rachatasumrit , Bradley Breneisen , Julina Coupland , and Aniket Kittur . 2022 . Fuse : In - Situ Sensemaking Support in the Browser . https : / / doi . org / 10 . 1145 / 3526113 . 3545693 arXiv : 2208 . 14861 [ cs ] . [ 61 ] James R . Lewis . 2018 . The System Usability Scale : Past , Present , and Future . International Journal of Human – Computer Interaction 34 , 7 ( July 2018 ) , 577 – 590 . https : / / doi . org / 10 . 1080 / 10447318 . 2018 . 1455307 Publisher : Taylor & Francis _ eprint : https : / / doi . org / 10 . 1080 / 10447318 . 2018 . 1455307 . [ 62 ] Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2019 . BART : DenoisingSequence - to - SequencePre - trainingforNaturalLanguageGeneration , Translation , and Comprehension . CoRR abs / 1910 . 13461 ( 2019 ) . http : / / arxiv . org / abs / 1910 . 13461 arXiv : 1910 . 13461 . [ 63 ] Patrick Lewis , Ethan Perez , Aleksandra Piktus , Fabio Petroni , Vladimir Karpukhin , Naman Goyal , Heinrich Küttler , Mike Lewis , Wen - tau Yih , Tim Rocktäschel , Sebastian Riedel , and Douwe Kiela . 2020 . Retrieval - Augmented Generation for Knowledge - Intensive NLP Tasks . In Ad - vances in Neural Information Processing Systems , Vol . 33 . Curran Asso - ciates , Inc . , 9459 – 9474 . https : / / proceedings . neurips . cc / paper / 2020 / hash / 6b493230205f780e1bc26945df7481e5 - Abstract . html [ 64 ] Michael Xieyang Liu , Nathan Hahn , Angelina Zhou , Shaun Burley , Emily Deng , Aniket Kittur , and Brad A . Myers . 2018 . UNAKITE : Support Developers for Capturing and Persisting Design Rationales When Solving Problems Using Web Resources . Workshop on Designing Technologies to Support Human Problem Solving at the IEEE Sympo - sium on Visual Languages and Human - Centric Computing ( Oct . 2018 ) . https : / / par . nsf . gov / biblio / 10152060 - unakite - support - developers - capturing - persisting - design - rationales - when - solving - problems - using - web - resources [ 65 ] Michael Xieyang Liu , Jane Hsieh , Nathan Hahn , Angelina Zhou , Emily Deng , Shaun Burley , Cynthia Taylor , Aniket Kittur , and Brad A . Myers . 2019 . Unakite : Scaffolding Developers’ Decision - Making Using the Web . In Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology ( UIST ’19 ) . ACM , New Orleans , LA , USA , 67 – 80 . https : / / doi . org / 10 . 1145 / 3332165 . 3347908 event - place : New Orleans , LA , USA . [ 66 ] Michael Xieyang Liu , Aniket Kittur , and Brad A . Myers . 2021 . To Reuse or Not To Reuse ? A Framework and System for Evaluating Summarized Knowledge . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( April 2021 ) , 166 : 1 – 166 : 35 . https : / / doi . org / 10 . 1145 / 3449240 [ 67 ] Michael Xieyang Liu , Aniket Kittur , and Brad A . Myers . 2022 . Crystalline : Lowering the Cost for Developers to Collect and Organize Information for Decision Making . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3491102 . 3501968 event - place : New Orleans , LA , USA . [ 68 ] Michael Xieyang Liu , Andrew Kuznetsov , Yongsung Kim , Joseph Chee Chang , Aniket Kittur , and Brad A . Myers . 2022 . Wigglite : Low - cost Information Collec - tion and Triage . In The 35th Annual ACM Symposium on User Interface Software and Technology ( UIST ’22 ) . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3526113 . 3545661 [ 69 ] Michael Xieyang Liu , Advait Sarkar , Carina Negreanu , Benjamin Zorn , Jack Williams , Neil Toronto , and Andrew D . Gordon . 2023 . “What It Wants Me To Say” : Bridging the Abstraction Gap Between End - User Programmers and Code - Generating Large Language Models . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI ’23 ) . Association for Computing Machinery , New York , NY , USA , 1 – 31 . https : / / doi . org / 10 . 1145 / 3544548 . 3580817 [ 70 ] Nelson F . Liu , Tianyi Zhang , and Percy Liang . 2023 . Evaluating Verifiabil - ity in Generative Search Engines . https : / / doi . org / 10 . 48550 / arXiv . 2304 . 09848 arXiv : 2304 . 09848 [ cs ] . [ 71 ] AmanMadaan , NiketTandon , PrakharGupta , SkylerHallinan , LuyuGao , Sarah Wiegreffe , UriAlon , NouhaDziri , ShrimaiPrabhumoye , YimingYang , Shashank Gupta , BodhisattwaPrasadMajumder , KatherineHermann , SeanWelleck , Amir Yazdanbakhsh , and Peter Clark . 2023 . Self - Refine : Iterative Refinement with Self - Feedback . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 17651 arXiv : 2303 . 17651 [ cs ] . [ 72 ] Lynne M . Markus . 2001 . Toward a Theory of Knowledge Reuse : Types of Knowledge Reuse Situations and Factors in Reuse Success . Journal of Management Information Systems 18 , 1 ( May 2001 ) , 57 – 93 . https : / / doi . org / 10 . 1080 / 07421222 . 2001 . 11045671 Publisher : Routledge _ eprint : https : / / doi . org / 10 . 1080 / 07421222 . 2001 . 11045671 . [ 73 ] Catherine C . Marshall and Sara Bly . 2005 . Saving and Using Encountered In - formation : Implications for Electronic Periodicals . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’05 ) . ACM , New York , 17 Liu et al . NY , USA , 111 – 120 . https : / / doi . org / 10 . 1145 / 1054972 . 1054989 event - place : Port - land , Oregon , USA . [ 74 ] Joaquim Mendes , Nuno Laranjeiro , and Marco Vieira . 2018 . Toward characterizing HTML defects on the Web . Software : Practice and Expe - rience 48 , 3 ( 2018 ) , 750 – 757 . https : / / doi . org / 10 . 1002 / spe . 2545 _ eprint : https : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / spe . 2545 . [ 75 ] Danaë Metaxa , Joon Sung Park , Ronald E . Robertson , Karrie Karahalios , Christo Wilson , Jeff Hancock , and Christian Sandvig . 2021 . Auditing Algorithms : Understanding Algorithmic Systems from the Outside In . Foundations and Trends® in Human – Computer Interaction 14 , 4 ( Nov . 2021 ) , 272 – 344 . https : / / doi . org / 10 . 1561 / 1100000083 Publisher : Now Publishers , Inc . . [ 76 ] MeredithRingelMorrisandEricHorvitz . 2007 . SearchTogether : AnInterfacefor Collaborative Web Search . In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology ( UIST ’07 ) . ACM , New York , NY , USA , 3 – 12 . https : / / doi . org / 10 . 1145 / 1294211 . 1294215 [ 77 ] Moin Nadeem , Anna Bethke , and Siva Reddy . 2021 . StereoSet : Measuring stereotypical bias in pretrained language models . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) . Association for Computational Linguistics , Online , 5356 – 5371 . https : / / doi . org / 10 . 18653 / v1 / 2021 . acl - long . 416 [ 78 ] An T . Nguyen , Aditya Kharosekar , Saumyaa Krishnan , Siddhesh Krishnan , Elizabeth Tate , Byron C . Wallace , and Matthew Lease . 2018 . Believe it or not : Designing a Human - AI Partnership for Mixed - Initiative Fact - Checking . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology ( UIST ’18 ) . Association for Computing Machinery , New York , NY , USA , 189 – 199 . https : / / doi . org / 10 . 1145 / 3242587 . 3242666 [ 79 ] David Nicholas , Peter Williams , Ian Rowlands , and Hamid R . Jamali . 2010 . Researchers’ e - journal use and information seeking behaviour . Journal of Information Science 36 , 4 ( Aug . 2010 ) , 494 – 516 . https : / / doi . org / 10 . 1177 / 0165551510371883 Publisher : SAGE Publications Ltd . [ 80 ] OpenAI . 2023 . ChatGPT . https : / / chat . openai . com [ 81 ] OpenAI . 2023 . GPT - 4 Technical Report . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 08774 arXiv : 2303 . 08774 [ cs ] . [ 82 ] Long Ouyang , Jeff Wu , Xu Jiang , Diogo Almeida , Carroll L . Wainwright , Pamela Mishkin , Chong Zhang , Sandhini Agarwal , Katarina Slama , Alex Ray , John Schulman , Jacob Hilton , Fraser Kelton , Luke Miller , Maddie Simens , Amanda Askell , Peter Welinder , Paul Christiano , Jan Leike , and Ryan Lowe . 2022 . Training language models to follow instructions with human feedback . https : / / doi . org / 10 . 48550 / arXiv . 2203 . 02155 arXiv : 2203 . 02155 [ cs ] . [ 83 ] Sharoda A . Paul and Meredith Ringel Morris . 2009 . CoSense : Enhancing Sense - making for Collaborative Web Search . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’09 ) . ACM , New York , NY , USA , 1771 – 1780 . https : / / doi . org / 10 . 1145 / 1518701 . 1518974 [ 84 ] Alvaro Pereira , Ricardo Baeza - Yates , and Nivio Ziviani . 2006 . Where and How Duplicates Occur in the Web . In 2006 Fourth Latin American Web Congress . 127 – 134 . https : / / doi . org / 10 . 1109 / LA - WEB . 2006 . 39 [ 85 ] Peter Pirolli and Stuart Card . 2005 . The Sensemaking Process and Lever - age Points for Analyst Technology as Identified Through Cognitive Task Analysis . In Proceedings of International Conference on Intelligence Analy - sis . http : / / www . phibetaiota . net / wp - content / uploads / 2014 / 12 / Sensemaking - Process - Pirolli - and - Card . pdf [ 86 ] Yannis Plegas and Sofia Stamou . 2013 . Reducing information redundancy in search results . In Proceedings of the 28th Annual ACM Symposium on Applied Computing ( SAC ’13 ) . Association for Computing Machinery , New York , NY , USA , 886 – 893 . https : / / doi . org / 10 . 1145 / 2480362 . 2480533 [ 87 ] Marlene A . Plumlee . 2003 . The Effect of Information Complexity on Analysts’ Use of That Information . The Accounting Review 78 , 1 ( Jan . 2003 ) , 275 – 296 . https : / / doi . org / 10 . 2308 / accr . 2003 . 78 . 1 . 275 [ 88 ] Luca Ponzanelli , Alberto Bacchelli , and Michele Lanza . 2013 . Seahawk : Stack Overflow in the IDE . In 2013 35th International Conference on Software Engineer - ing ( ICSE ) . IEEE , San Francisco , CA , USA , 1295 – 1298 . https : / / doi . org / 10 . 1109 / ICSE . 2013 . 6606701 [ 89 ] G . Poonkuzhali , R . Kishore Kumar , R . Kripa Keshav , P . Sudhakar , and K . Sarukesi . 2011 . Correlation Based Method to Detect and Remove Redun - dant Web Document . Advanced Materials Research 171 - 172 ( 2011 ) , 543 – 546 . https : / / doi . org / 10 . 4028 / www . scientific . net / AMR . 171 - 172 . 543 Publisher : Trans Tech Publications Ltd . [ 90 ] Soujanya Poria , Erik Cambria , Lun - Wei Ku , Chen Gui , and Alexander Gelbukh . 2014 . A rule - based approach to aspect extraction from product reviews . In Proceedings of the second workshop on natural language processing for social media ( SocialNLP ) . 28 – 37 . [ 91 ] Napol Rachatasumrit , Jonathan Bragg , Amy X . Zhang , and Daniel S Weld . 2022 . CiteRead : Integrating Localized Citation Contexts into Scientific Paper Reading . In 27th International Conference on Intelligent User Interfaces ( IUI ’22 ) . Association for Computing Machinery , New York , NY , USA , 707 – 719 . https : / / doi . org / 10 . 1145 / 3490099 . 3511162 [ 92 ] Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , and others . 2019 . Language models are unsupervised multitask learners . OpenAI blog 1 , 8 ( 2019 ) , 9 . [ 93 ] Nils Reimers and Iryna Gurevych . 2019 . Sentence - BERT : Sentence Embeddings using Siamese BERT - Networks . https : / / doi . org / 10 . 48550 / arXiv . 1908 . 10084 arXiv : 1908 . 10084 [ cs ] . [ 94 ] Daniel M . Russell , Mark J . Stefik , Peter Pirolli , and Stuart K . Card . 1993 . The Cost Structure of Sensemaking . In Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems ( CHI ’93 ) . ACM , New York , NY , USA , 269 – 276 . https : / / doi . org / 10 . 1145 / 169059 . 169209 [ 95 ] Bill N . Schilit , Gene Golovchinsky , and Morgan N . Price . 1998 . Beyond paper : supporting active reading with free form digital ink annotations . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’98 ) . ACM Press / Addison - Wesley Publishing Co . , USA , 249 – 256 . https : / / doi . org / 10 . 1145 / 274644 . 274680 [ 96 ] M . C . Schraefel and Yuxiang Zhu . 2001 . Interaction design for Web - based , within - page collection making and management . In Proceedings of the 12th ACM conference on Hypertext and Hypermedia ( HYPERTEXT ’01 ) . Association for Computing Machinery , New York , NY , USA , 125 . https : / / doi . org / 10 . 1145 / 504216 . 504247 [ 97 ] M . C . schraefel , Yuxiang Zhu , David Modjeska , Daniel Wigdor , and Shengdong Zhao . 2002 . Hunter Gatherer : Interaction Support for the Creation and Manage - ment of Within - web - page Collections . In Proceedings of the 11th International Conference on World Wide Web ( WWW ’02 ) . ACM , New York , NY , USA , 172 – 181 . https : / / doi . org / 10 . 1145 / 511446 . 511469 [ 98 ] Nikhil Sharma . 2008 . Sensemaking handoff : When and how ? Proceedings of the American Society for Information Science and Technology 45 , 1 ( Jan . 2008 ) , 1 – 12 . https : / / doi . org / 10 . 1002 / meet . 2008 . 1450450234 [ 99 ] Nikhil Sharma and George Furnas . 2009 . Artifact usefulness and usage in sensemaking handoffs . Proceedings of the American Society for Information ScienceandTechnology 46 ( 2009 ) . https : / / doi . org / 10 . 1002 / meet . 2009 . 1450460219 [ 100 ] Ben Shneiderman . 2000 . Designing trust into online experiences . Commun . ACM 43 , 12 ( Dec . 2000 ) , 57 – 59 . https : / / doi . org / 10 . 1145 / 355112 . 355124 [ 101 ] Alexandra N Spichtig , Elfrieda H Hiebert , Christian Vorstius , Jeffrey P Pascoe , P David Pearson , and Ralph Radach . 2016 . The decline of comprehension - based silent reading efficiency in the United States : A comparison of current data with performance in 1960 . Reading Research Quarterly 51 , 2 ( 2016 ) , 239 – 259 . Publisher : Wiley Online Library . [ 102 ] Bongwon Suh , Ed H . Chi , Aniket Kittur , and Bryan A . Pendleton . 2008 . Lifting the veil : improving accountability and social transparency in Wikipedia with wikidashboard . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’08 ) . Association for Computing Machinery , Florence , Italy , 1037 – 1040 . https : / / doi . org / 10 . 1145 / 1357054 . 1357214 [ 103 ] João Sá , Vanessa Queiroz Marinho , Ana Rita Magalhães , Tiago Lacerda , and Diogo Goncalves . 2022 . Diversity Vs Relevance : A Practical Multi - objective Study in Luxury Fashion Recommendations . In Proceedings of the 45th Inter - national ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’22 ) . Association for Computing Machinery , New York , NY , USA , 2405 – 2409 . https : / / doi . org / 10 . 1145 / 3477495 . 3531866 [ 104 ] Craig S . Tashman and W . Keith Edwards . 2011 . LiquidText : A Flexible , Multi - touch Environment to Support Active Reading . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’11 ) . ACM , New York , NY , USA , 3285 – 3294 . https : / / doi . org / 10 . 1145 / 1978942 . 1979430 event - place : Vancouver , BC , Canada . [ 105 ] H . HoldenThorp . 2023 . ChatGPTisfun , butnotanauthor . Science 379 , 6630 ( Jan . 2023 ) , 313 – 313 . https : / / doi . org / 10 . 1126 / science . adg7879 Publisher : American Association for the Advancement of Science . [ 106 ] Hugo Touvron , Thibaut Lavril , Gautier Izacard , Xavier Martinet , Marie - Anne Lachaux , TimothéeLacroix , BaptisteRozière , NamanGoyal , EricHambro , Faisal Azhar , Aurelien Rodriguez , Armand Joulin , Edouard Grave , and Guillaume Lample . 2023 . LLaMA : Open and Efficient Foundation Language Models . https : / / doi . org / 10 . 48550 / arXiv . 2302 . 13971 arXiv : 2302 . 13971 [ cs ] . [ 107 ] Hugo Touvron , Louis Martin , Kevin Stone , Peter Albert , Amjad Almahairi , Yasmine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhosale , andothers . 2023 . Llama2 : Openfoundationandfine - tunedchatmodels . arXiv preprint arXiv : 2307 . 09288 ( 2023 ) . [ 108 ] Amos Tversky and Daniel Kahneman . 1974 . Judgment under Uncertainty : Heuristics and Biases : Biases in judgments reveal some heuristics of thinking under uncertainty . science 185 , 4157 ( 1974 ) , 1124 – 1131 . Publisher : American association for the advancement of science . [ 109 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention Is All You Need . https : / / doi . org / 10 . 48550 / arXiv . 1706 . 03762 arXiv : 1706 . 03762 [ cs ] . [ 110 ] Chenguang Wang , Xiao Liu , and Dawn Song . 2020 . Language Models are Open KnowledgeGraphs . https : / / doi . org / 10 . 48550 / arXiv . 2010 . 11967 arXiv : 2010 . 11967 [ cs ] . [ 111 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Brian Ichter , Fei Xia , Ed Chi , Quoc Le , and Denny Zhou . 2023 . Chain - of - Thought Prompting 18 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models Elicits Reasoning in Large Language Models . https : / / doi . org / 10 . 48550 / arXiv . 2201 . 11903 arXiv : 2201 . 11903 [ cs ] . [ 112 ] Dale M . Willows . 1974 . Reading between the Lines : Selective Attention in Good and Poor Readers . Child Development 45 , 2 ( 1974 ) , 408 – 415 . https : / / doi . org / 10 . 2307 / 1127962 Publisher : [ Wiley , Society for Research in Child Development ] . [ 113 ] Dale M . Willows and G . E . MacKinnon . 1973 . Selective reading : Attention to the " unattended " lines . Canadian Journal of Psychology / Revue canadienne de psychologie 27 , 3 ( 1973 ) , 292 – 304 . https : / / doi . org / 10 . 1037 / h0082480 Place : Canada Publisher : University of Toronto Press . [ 114 ] Wondershare . 2023 . Chat with PDF * Powered by ChatGPT . https : / / www . hipdf . com / chat - with - pdf [ 115 ] Nicholas C . Wormald . 1995 . Differential Equations for Random Processes and Random Graphs . The Annals of Applied Probability 5 , 4 ( 1995 ) , 1217 – 1235 . https : / / doi . org / 10 . 1214 / aoap / 1177004612 Publisher : Institute of Mathematical Statistics . [ 116 ] ZhengyuanYang , ZheGan , JianfengWang , XiaoweiHu , YumaoLu , ZichengLiu , and Lijuan Wang . 2022 . An Empirical Study of GPT - 3 for Few - Shot Knowledge - Based VQA . Proceedings of the AAAI Conference on Artificial Intelligence 36 , 3 ( June 2022 ) , 3081 – 3089 . https : / / doi . org / 10 . 1609 / aaai . v36i3 . 20215 Number : 3 . [ 117 ] Ilan Yaniv and Shoham Choshen - Hillel . 2012 . When guessing what another person would say is better than giving your own opinion : Using perspective - taking to improve advice - taking . Journal of Experimental Social Psychology 48 , 5 ( Sept . 2012 ) , 1022 – 1028 . https : / / doi . org / 10 . 1016 / j . jesp . 2012 . 03 . 016 [ 118 ] Wenpeng Yin , Jamaal Hay , and Dan Roth . 2019 . Benchmarking Zero - shot Text Classification : Datasets , Evaluation and Entailment Approach . https : / / doi . org / 10 . 48550 / arXiv . 1909 . 00161 arXiv : 1909 . 00161 [ cs ] . 19 Liu et al . A UNFAMILIAR TOPICS PARTICIPANTS EXPLORED IN THE FORMATIVE STUDY Index Theme Topic Participants 1 Software & online services Choosing a hybrid app framework P2 , P5 2 Selecting a secure password manager P3 , P7 3 Choosing a suitable ERP ( Enterprise Resource Planning ) solution P1 4 Choosing a reliable VPN ( Virtual Private Network ) provider P1 , P5 5 Picking a deep learning framework P8 6 Deciding on the best data visualization tool P6 7 Choosing the best time tracking tool P4 8 Consumer Electronics & Technology Choosing a high - quality digital camera P2 , P5 9 Choosing the best action camera P8 10 Selecting a VR headset P7 11 Picking a drone P3 , P5 12 Picking a smart home ecosystem P1 , P6 , P8 13 Home Appliances & Furniture Picking the best robot vacuum P2 , P5 14 Choosing the best air purifier P4 15 Selecting the best washing machine P3 16 Picking the right refrigerator P3 17 Selecting the best mattress P3 , P6 18 Outdoor & Adventure Choosing the best city bike P7 19 Choosing the best barbecue grill P3 20 Choosing the best tropical vacation location P4 21 Health & fitness Choosing an effective diet plan P1 , P7 22 Picking a reliable treadmill P3 23 Picking the best running shoes P8 24 Gifts & special events Choosing a birthday gift P5 , P6 25 Picking the right wedding venue P2 26 Picking the perfect engagement ring P2 27 Parenting Choosing the best baby stroller P8 28 Pets Choosing a breed of dog to adopt P4 Table 4 : Unfamiliar topics ( organized by themes ) that participants in the formative study reported encountering and exploring . Some topics were explored by multiple participants , such as “Picking a smart home ecosystem” and “Choosing a reliable VPN provider . ” 20 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models B COMMONLY CONSIDERED CRITERIA ELECITED FROM SELENITE Below is a list of commonly considered criteria that Selenite retrieves for the topic of “best baby strollers” by leveraging GPT - 4 as a knowledge retriever : Criteria Name Criteria Description 1 Safety Ensuring the stroller has proper safety features such as a secure harness , sturdy construction , and reliable brakes . 2 Comfort Providing a comfortable seat with adequate padding and support for the baby , as well as adjustable recline positions . 3 Maneuverability Having smooth and easy maneuverability , with features like swivel wheels , suspension systems , and the ability to navigate tight spaces . 4 Durability Ensuring the stroller is built to last , with high - quality materials and strong construction . 5 Storage Offering ample storage space for carrying essentials such as diaper bags , snacks , and personal items . 6 Folding and Portability Allowing for easy folding and compact storage , as well as being lightweight for convenient transportation . 7 Versatility Providing features that allow the stroller to adapt to different terrains , weather conditions , and age ranges . 8 Ease of Use Having user - friendly features like adjustable handles , intuitive controls , and easy - to - clean fabrics . 9 Price Considering the affordability and value for money in relation to the features and quality of the stroller . 10 Customer Reviews Taking into account feedback and recommendations from other parents who have used the stroller . 11 Weight and size Considering the weight and size of the stroller to ensure it is manageable and fits well in different environments . 12 Ease of cleaning Ensuring the stroller is easy to clean and maintain , with removable and washable fabric components . 13 Adjustability The stroller should have adjustable handlebars and footrests to accommodate different caregivers and growing babies . 14 Canopy A large and adjustable canopy to protect the baby from the sun and other elements . 15 Reversible seat Having the option to face the baby towards the parent or away from the parent . 16 Brake system Having a reliable brake system that is easy to engage and disengage . 17 Compatibility with car seats Offering the ability to attach a car seat to the stroller for convenient travel . 18 Adjustable height Allowing for adjustable handlebars to accommodate different heights of caregivers . 19 Easy assembly Providing clear instructions and easy assembly process for the stroller . 20 Design and aesthetics Considering the overall design and aesthetics of the stroller to match personal preferences . 21 Weight capacity Specifying the maximum weight limit the stroller can safely carry . 22 Warranty Checking for a warranty or guarantee that covers any potential defects or issues with the stroller . 23 Brand reputation Considering the reputation and reliability of the brand manufacturing the stroller . 24 Accessories Offering additional accessories such as rain covers , mosquito nets , or parent organizers for added convenience . Table 5 : Commonly considered criteria that Selenite retrieves for the topic of “best baby strollers . ” 21 Liu et al . C TOPICS EXPLORED IN STUDY 3 Participant Topics Participant Topics P1 Choosing a high - quality digital camera P5 Picking the best robot vacuum Choosing the best air purifier Picking a drone Picking a suitable hand truck Choosing the best e - reader P2 Selecting the best washing machine P6 Picking the right refrigerator Choosing a reliable VPN provider Choosing the best barbecue grill Deciding on unique thank - you gifts Choosing the best tropical vacation location P3 Selecting the best mattress P7 Choosing an effective diet plan Choosing the best city bike Choosing a breed of dog to adopt Choosing a birthday gift Selecting a suitable SUV P4 Selecting a secure password manager P8 Picking a reliable treadmill Choosing the best tropical vacation location Picking the right wedding venue Choosing a hybrid app framework Choosing the best skiing venue Table 6 : Topics that participants in study 3 explored . D GPT - 4 PROMPTS USED IN SELENITE Here , we outline the techniques we employed to guide the GPT - 4 model , developed by OpenAI [ 81 ] , within the context of Selenite . If not explicitly stated , the temperature of the model is set to 0 . 3 for a balance between consistency and creativity . If not explicitly stated , the initial [ System Message ] 25 was set as the following : [ System Message ] You are a helpful assistant that performs content analysis according to user requests . Follow the user’s requirements carefully and to the letter . D . 1 Obtaining topic from a web page The prompt that we used to obtain a concise topic given a web page ( with [ title ] and [ content of the first few paragraphs ] ) is a two - step prompt : [ User Message ] Step 1 Given the following information of an article : Title : [ title ] First few paragraphs : [ content of the first few paragraphs ] What is this article about ? [ Assistant Message ] This article is about . . . [ User Message ] Step 2 I want to find articles similar to this one in terms of the general topic . What should I search for ? Output one search phrase ( in double quotes ) . Additionally , we set the n parameter to 10 , thereby instructing GPT - 4 to produce 10 simultaneous responses . Subsequently , we determined the most commonly occurring one among these 10 as the topic for the article . While it could be assumed that a higher value for n would result in a lengthier response time from the model , our observations indicate that such delay is practically insignificant . D . 2 Obtaining options from a web page The prompt that we used to obtain extract options from a given web page ( with [ title ] and [ content of the web page ] ) is a two - step prompt : 25 Thesystemmessagehelpssetthebehaviorofthemodelresponse , i . e . , the [ Assistant Message ] . However , asstatedbyOpenAI : “ . . . notethatthesystemmessageisoptionalandthe model’sbehaviorwithoutasystemmessageislikelytobesimilartousingagenericmessagesuchas‘Youareahelpfulassistant’ . . . ” ( https : / / platform . openai . com / docs / guides / gpt / chat - completions - api ) 22 Selenite : Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models [ User Message ] Step 1 Given the following information of an article : Title : [ title ] Is the article likely to be discussing one or more aspects of " one specific option " ( e . g . , a single javascript framework , for example , React , or a single baby stroller option , or a specific Airbnb listing ) or " multiple options / topics " ? Output in the following format : Reasoning : your reasoning process . Verdict : " one specific option / multiple options " [ Assistant Message ] Reasoning : model’s reasoning process . . . Verdict : " one specific option / multiple options " [ User Message ] Step 2 Now , given the content of the article below , what is / are the options ? Content : [ content of the web page ] Output should be in the following format : [ " option _ 1 " , " option _ 2 " , . . . ] D . 3 Obtaining commonly - considered criteria from a web page The prompt that we used to obtain a set of commonly considered criteria resembles something like the following ( given a [ topic ] ) : [ User Message ] Step 1 : Ask for an initial set of criteria What are some common aspects , criteria , or dimensions that people consider on the topic of [ topic ] ? Note that the criteria should be * * most relevant to the topic * * , * * frequently considered * * , and can * * cover a broad range of perspectives * * . Output should be a single bulleted list in the format of : - Criterion : short description . Do not output anything else . [ Assistant Message ] - [ Criterion 1 ] : [ Short description ] - [ Criterion 2 ] : [ Short description ] - [ Criterion 3 ] : [ Short description ] . . . [ User Message ] Step 2 + : Ask for additional criteria until we get around 20 . Give me five more that are different from , more diverse than , and possibly as important as the ones listed above . Output in the same format . D . 4 Obtaining detailed analysis of text content The prompts that we used to obtain a detailed analysis of text content given the [ text content ] , the list of NLI criteria , and the list of [ options ] on the corresponding web page is two - fold : First , we ask GPT - 4 to extract phrases from the content that describes a given criterion as well as determine each extracted phrase’s sentiment with respect to the criterion : [ User Message ] 23 Liu et al . Given the following * * content * * and list of * * criteria * * : * * Content * * : [ content ] * * Criteria ( with definitions ) * * : - [ NLI Criterion 1 ] : [ description ] - [ NLI Criterion 2 ] : [ description ] . . . For each criterion : 1 ) extract * * every possible * * utterance that * * mentions * * or * * explicitly describes * * that criterion from the content 2 ) perform sentiment analysis to determine if the utterance is " positive " , " neutral " , or " negative " with respect to that criterion . Remember to use the * * exact same words * * from the content . Do not paraphrase ! Output must follow the format below : # # criterion _ 1 _ name - " extracted _ sentence _ or _ phrase _ 1 " - > positive , - " extracted _ sentence _ or _ phrase _ 2 " - > neutral , # # criterion _ 2 _ name NONE FOUND # # criterion _ 3 _ name - " extracted _ sentence _ or _ phrase _ 1 " - > neutral , - " extracted _ sentence _ or _ phrase _ 2 " - > negative , - " extracted _ sentence _ or _ phrase _ 3 " - > positive , Second , we ask GPT - 4 to label each extracted phrase with a possible [ option ] on the web page ( we framed options as “subjects” of a phrase to achieve a better empirical performance ) : [ User Message ] Given the following * * content * * and the * * phrases * * extracted from the content below : * * Content * * : [ content ] * * Extracted phrases * * : - " extracted _ phrase _ 1 " - " extracted _ phrase _ 2 " - " extracted _ phrase _ 3 " . . . For each phrase , determine the * * subject * * of the phrase based on the * * content * * . Possible subjects are : [ option _ 1 , option _ 2 , option _ 3 , . . . ] Say " N / A " if you cannot determine the subject . Output should be in the following format : " extracted phrase 1 " - > " subject " or " N / A " " extracted phrase 2 " - > " subject " or " N / A " . . . 24