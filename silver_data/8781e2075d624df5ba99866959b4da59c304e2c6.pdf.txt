GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support Qian Wan Zhicong Lu City University of Hong Kong City University of Hong Kong Hong Kong , China Hong Kong , China qianwan3 - c @ my . cityu . edu . hk zhiconlu @ cityu . edu . hk Figure 1 : An overview of the GANCollage interface : ( a ) The tool panel consists of a “style mix” section , a “new images” section with a “request” icon , and a tile of unused sticky notes . ( b ) The main mood board canvas comprises sticky notes and generated images . Designers can scale , move , or highlight them . ( c ) The drawing canvas can be dragged anywhere at any time by the designer , and becomes transparent if not hovered on . ( d ) The Style Mix function will mix styles between two groups and display results in the “style mix” section on the tool panel . ABSTRACT During past decades , Artifcial Intelligence ( AI ) has been consis - tently used in Creativity Support Tools ( CSTs ) . Recently , with the development of generative AI models , particularly Generative Ad - versarial Nets ( GAN ) in Computer Vision , it became possible that AI directly generates visual ideas . However , there were rarely any Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specifc permission and / or a fee . Request permissions from permissions @ acm . org . DIS ’23 , July 10 – 14 , 2023 , Pittsburgh , PA , USA © 2023 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 9893 - 0 / 23 / 07 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3563657 . 3596072 work in creativity research that harnessed the design ideas gener - ated by such models directly for design space exploration . In this paper , we propose a StyleGAN - driven digital mood board , GAN - Collage , that integrates AI generated visual ideas into the ideation phase for creativity support . GANCollage supports semantic explo - rations of StyleGAN generations in an iterative human - in - the - loop manner , using an AI - driven interactive tagging system . Our evalua - tion involving 10 participants manifests that GANCollage provides more creativity support without compromising the fnal results . It also ofers a more enjoyable , explicit and efective way of exploring AI generated visual ideas for ideation . CCS CONCEPTS • Human - centered computing → Interactive systems and tools . 136 DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Wan and Lu KEYWORDS creativity support , ideation , human - AI interaction , digital mood board ACM Reference Format : Qian Wan and Zhicong Lu . 2023 . GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support . In Designing Interactive Systems Conference ( DIS ’23 ) , July 10 – 14 , 2023 , Pittsburgh , PA , USA . ACM , New York , NY , USA , 11 pages . https : / / doi . org / 10 . 1145 / 3563657 . 3596072 1 INTRODUCTION A Creativity Support Tool ( CST ) is defned as a tool that “ runs on one or more digital systems , encompasses one or more creativity - focused features , and is employed to positively infuence users of varying expertise in one or more distinct phases of the creative process ” by Frich et al . [ 11 ] . In the HCI community , the design of creativity support tools plays a fundamental role in the study of creativity . It usually adopts various computational support and interaction techniques to facilitate creative activities , and recently Artifcial Intelligence ( AI ) and Machine Learning ( ML ) have come into notice . For years , Artifcial Intelligence has been involved in the design of CSTs in a variety of ways . Previously , AI was used to detect de - sign attributes [ 19 , 24 , 25 ] , summarise or retrieve existing designs or materials [ 3 , 19 , 24 , 25 , 32 ] , generate semantic stimuli [ 19 , 25 ] , mixing diferent design styles [ 23 ] , etc . With the development of Deep Learning , particularly generative models such as Generative Adversarial Network ( GAN ) [ 13 ] , the potential of AI for design ideation has been completely opened up by its capability of generat - ing realistic data of high quality and diversity . In character design , for example , AI models like StyleGAN are now able to directly gen - erate diverse and realistic images or visual ideas . Previous works have already used GANs to generate character faces [ 20 ] or sup - port the colourisation of a character design sketch [ 7 ] . However , how such model could be leveraged during the ideation phase to support design space exploration remains an unanswered question , particularly for novice users who lack knowledge of the design space , which easily leads to fxation [ 18 , 26 ] that hinders novelty [ 15 , 27 , 30 ] . We are particularly motivated to know how genera - tions of AI models like GAN could be integrated into CSTs , and how we could support novice users in exploring and making sense of potential AI generations . In this paper , we present our creativity support tool , GANCollage , a web - based digital mood board that seamlessly integrates AI gener - ations into the ideation phase . In order for designers to draw inspi - rations from a generative AI , GANCollage supports semantically - enhanced methods of exploring a GAN latent space in a human - in - the - loop manner . It introduces an interactive tagging system , powered by a pre - trained anime image classifer , in the form of “sticky notes” on the mood board canvas to help designers organise and make sense of these generated ideas . Our user study manifests that , compared to traditional image galleries , GANCollage provides more meaningful creativity support , and allows designers to be more expressive and creative , while maintaining overall qualities of delivered fnal results . Through interviews with our participants , we also found that GANCollage was perceived to be more enjoyable , explicit , and customisable in terms of design idea exploration , while potentially compromising some levels of diversity . Our contributions to the HCI community are three - fold . First , we tentatively propose a digital mood board to integrate AI generated images into creativity support for design space exploration . Second , our user study provides insights into human - AI interaction and digital mood board design for CST researchers . Finally , our system can be used to facilitate character design , which might hopefully beneft researchers in various felds using virtual characters . 2 RELATED WORK We frst review the background of Generative Adversarial Networks to motivate our research questions and the design , then review prior work in creativity support tools and character creation interfaces to anchor our contributions to HCI . 2 . 1 Background : Generative Adversarial Network A Generative Adversarial Network ( GAN ) [ 13 ] is a type of gener - ative deep learning model that is able to generate realistic data . Particularly in the feld of Computer Vision , with the invention of StyleGAN [ 21 , 22 ] , such models are known for its capability of producing high - resolution , visually - appealing , and photo - realistic images of high diversity . The original StyleGAN model takes as input a high - dimensional vector of Gaussian noise , maps it into a style vector , and then feeds the style vector into a neural network ( i . e . , the generator of a GAN ) that outputs a high - resolution image . Following up StyleGAN , a large amount of work in both HCI and Computer Vision have extensively studied the rigging , editing , and explorations of the StyleGAN latent space , in order to converge to user preferred generations [ 10 , 41 ] . Nevertheless , to our surprise , little research in HCI has harnessed the power of StyleGAN to facili - tate ideation , and particularly design space exploration in creativity support tools ( CSTs ) . Previously , CST researchers have explored supporting ideation via text prompts [ 19 ] , visual stimuli [ 38 ] , group collaborations [ 37 , 39 ] , etc . In these systems or workfows , new ideas or designs suggested by a computer system were usually retrievals or re - combinations of existing materials ( e . g . , online search ) [ 19 , 25 ] , sometimes with simple rule - based computer generations ( e . g . , using genetic algorithm ) [ 23 ] , or targeted generations based on specifc user needs ( e . g . , style transfer ) [ 17 ] . For example , in a visual de - sign setting such as fashion [ 19 ] , character [ 1 ] , or interior design [ 25 ] , instead of providing fully computer - generated visual ideas to support novice users in exploring the entire design space , existing technological assistance in generating new design ideas is limited to text attributes analysis , online image search , automatic preview of prototype sketches , or convergent image generation etc . For that purpose , we are motivated to study how we could use StyleGAN to provide complete computer - generated visual ideas for inspirations in the ideation phase of a visual design to support novice users . Specifcally , we ask the following research questions : • RQ1 : How can we seamlessly integrate StyleGAN genera - tions into a Creativity Support Tool ? • RQ2 : How can we support users in understanding and re - fecting on computer - generated images during ideation ? 137 GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA • RQ3 : How can we support explorations of a StyleGAN latent space during the ideation phase in a diversifed and less - demanding way ? In the following subsections , we briefy review related work in the feld of CST and character design , and explain how our work contributes to these lines of research . 2 . 2 Creativity Support Tools Previous creativity support tools primarily facilitate ideation by retrieving , analysing , suggesting , and blending existing related ma - terials . For example , Koch et al . [ 25 ] proposed a digital mood board that attaches semantic labels to images via AI labeling algorithms , in order to help designers articulate , communicate , and refect on vague , visual ideas . The tool then translates ideas from designers into search terms , and support serendipitous discovery via online semantic search . In their later work [ 24 ] , AI text suggestions were also incorporated into the workfow , though novel images were still curated by image - based search engine . Jeon et al . [ 19 ] , in their work intended for fashion designers , introduced a data - driven cre - ativity support tool . Based on fashion image data labeled with text attributes , they developed AI models capable of fashion attribute detection , style clustering , style forecasting , and style merging , to help expert designers analyse previous fashion trends and set future design direction . Another line of research in CST also focused on group collab - oration in order to boost creativity during ideation . These works often provide cultural and conceptual diversity , usually in a group brainstorming setting . For example , Wang et al . [ 38 ] proposed a model that intelligently suggests and selects pictorial stimuli based on group conversation during a brainstorming session . In a follow - up study [ 39 ] , they also experimented the system with participants of diferent cultural backgrounds , and proved the signifcance of cultural diversity within a group in boosting creativity . To facilitate group brainstorming , Shih et al . [ 37 ] , in an earlier work , proposed a collaborative mind - mapping tool , GroupMind , which was proved in their evaluation to outperform traditional whiteboards . In order to address the problem of “design fxation” [ 18 ] during the ideation phase , computer supported generations were also used in CST research to provide novel designs or ideas . For example , in industrial design , previous CSTs tend to integrate Genetic Al - gorithms ( GE ) into their workfow . Kim et al . [ 23 ] in their system intended for garden design , allowed users to manipulate the ge - netic operators between their design and other collaborators’ , and substituted the ftness function in GE with user selection . Recently , the development of generative AI models have aroused research interest in the feld of HCI , and also in the study of CSTs . Mozafari et al . [ 32 ] tentatively proposed an image - based query approach for User Interface Design . Their system could frst fnd a nearest latent code ( the high - dimensional input vector ) of an input image in the StyleGAN latent space via gradient descent , and then supports synthesis from a source image to a desired style by mixing style vectors fed into the StyleGAN generator . Our work also integrates StyleGAN generations into a creativity support tool for design space exploration , adopting a human - in - the - loop manner . Unlike image queries , we use a digital mood board with semantic labels as it helps translate the often vague visual ideas of AI for novice users . We also support both serendipitous and convergent explorations of the StyleGAN latent space based on user feedback , either visual or semantic . In this way we not only provide more diverse visual ideas during ideation to address “design fxation” , but also help users understand and refect on those computer generations . 2 . 3 Character creation in avatar - mediated systems Character design plays an important role in various felds of HCI such as story - telling [ 36 ] , virtual worlds [ 9 , 31 ] , and recently even live streaming [ 29 ] . Previously in virtual worlds such as video games , a Character Creation Interface ( CCI ) was usually ofered to help users customise their character . However , such interface has constantly been questioned around its limited fexibility and freedom in customisation [ 9 ] . Besides games , EmoG [ 36 ] is a char - acter design tool facilitating story - boarding . It is able to generate sketches of characters with emotional expressions based on in - put strokes . Other work in character design provides support in various design phases such as colourisation [ 1 , 7 ] . Our work also contributes to this feld by introducing AI generations to facilitate ideation of potential character design . 3 DESIGN GOALS Based on the literature review and the aforementioned research questions , we derived the following three design goals . Goal 1 : Use a digital mood board with semantic support that integrates StyleGAN generations for ideation . In order to address RQ1 and also RQ2 , we need to fnd a way of explicitly displaying and “articulating” AI generated visual ideas . For that purpose , a physical mood board has already been solidly proved to be a perfect ft [ 4 , 12 ] , since it both boosts creativity , and facilitates communication of ideas that are hard to express . Koch et al . [ 24 , 25 ] also implemented a digitalised mood board powered by semantic labels , which proved to be efective in the articulation and sense - making of vague visual ideas . Therefore , we would like to adopt a digital mood board with semantic support to display AI generated visual ideas , and what AI “might think” when producing these generations . Goal 2 : Provide customisable data - driven semantic atributes over visual ideas to help users interact with the AI . . It is worth noticing that the SemanticCollage [ 25 ] and ImageSense [ 24 ] designed by Koch et al . were all deployed in a group ideation setting , which implied human - human interaction . The semantic labels in their systems were outputs of a classifcation algorithm and could be translated into online search terms for ideation . We believe that such level of customisation is not enough if the designer wants to interact with and “collaborate” with an AI . Focusing on our RQ2 , we also would like to use a pre - trained classifer to generate semantic labels , similar to FashionQ [ 19 ] and SemanticCollage . However , we aim to improve on such methods by allowing users to customise all the semantic information on the mood board . It includes altering the text , adding their own texts , and manipulating many - to - many relationships between semantic attributes and generated images . 138 DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Wan and Lu Most importantly , we believe that all semantic labels on the mood board should be efectively used to explore AI generations . In this way , designers are able to articulate their needs , and ex - plicitly communicate with the AI beyond based on their existing understanding of the design space . Goal 3 : Support both serendipitous and convergent explo - rations of StyleGAN generations using both existing images and semantic atributes . To address RQ3 , we are deeply inspired by Mozafari et al . [ 32 ] . We believe the exploration of a StyleGAN latent space should be both serendipitous , which guarantees diver - sity , and convergent , which allows designers to fne - tune images of interest . We also aim to combine both semantic and visual informa - tion on the mood board to support the latent space exploration . To that end , we borrow an idea from genetic exploration of a design space [ 23 ] , since it seems intuitive , self - explanatory , and demands few computational overheads or user knowledge of AI . 4 GANCOLLAGE In this section we frst provide an overview of our system , and illustrate its workfow via a use case scenario . We then elaborate on specifc features on our user interface and briefy explain how they were implemented . 4 . 1 System Overview We present our system , GANCollage , a digital mood board that leverages StyleGAN generations to support ideation for designers . The front - end user interface of GANCollage comprises a tool panel ( Figure 1a ) , a mood board canvas ( Figure 1b ) , and a drawing canvas ( Figure 1c ) . The mood board canvas consists of images generated by the StyleGAN , and sticky notes with semantic predictions from the classifer . The tool panel is seamlessly integrated into the main canvas where designers can directly drag new tags and images onto the mood board . It holds a tile of new sticky notes , a “style mix” section , and a “new image” section , where user - requested images are spawned . The drawing canvas ( see Figure 2d ) stores all images archived by the designer alongside their sticky notes . It becomes transparent when not focused on , and can also be dragged to the screen centre for drawing or writing at any time . Below we illustrate our workfow and main components via a use case scenario of character design . Suppose Lesley , an illustrator , would like to design an anime character as her virtual avatar on social media . She hopes to browse through AI generated images to draw inspirations . 4 . 1 . 1 The Ideation Phase . Lesley connects to GANCollage on the web via a browser , and initially some random images and sticky notes are displayed . Lesley would like to explore as many design ideas as possible in the frst place . Therefore , she clicks on the “request” icon to request more randomly generated images from the server . Meanwhile , sticky notes are also updated while new semantic attributes are detected on incoming generations , and new images also assigned to some existing tags . Lesley browses through these generated images and sticky notes by moving and scaling them on the mood board canvas . She also discards some unwanted images or tags by clicking on the “delete” icon . Lesley fnds that some semantic tags predicted might not be accurate . She therefore clicks on a sticky note to highlight corre - sponding images currently assigned to it , and deselects images she believes to be without such property described by the tag . Lesley also identifes some common properties of a group of generated images and hopes to mark it down . She then drags a new sticky note from the tool panel onto the canvas , double clicks to alter the text , and then selects all images corresponding to it . After a while , Lesley happens to fnd some images or tags of interest . In order to explore more similar ideas , she clicks on the “more similar” icon on some images or tags to request more similar generations . Lesley also wants to try mixing a style specifed by some sticky notes with another particular style in some images of her interest . She therefore selects the frst group of tags , clicks on the pop - up “mix” icon , and selects another group of images for “style mixing” . She then sorts resulting images by re - organising and adjusting semantic tags , and tries “style mixing” again . After several times , she fnally fnds some styles she feels intrigued or inspired . 4 . 1 . 2 The Convergent Thinking Phase . After some time of explo - ration , Lesley focuses on some specifc images she would like to save to her drawing canvas as references . She then clicks on these images to highlight them one by one . The system scales these im - ages to their original size , and attaches to them all related sticky notes . Lesley reads through these semantic tags to understand and digest what AI wants to convey in these generated images . She also tries clicking on the “more similar” icon to request more similar images when an image is highlighted . The above described process is illustrated in Figure 2a . After several trials , the system fnally converges to the image with satisfying quality . Lesley then clicks on the “archive” icon to archive images to her canvas . In the end , she drags the canvas into the screen centre , crops out intriguing parts of these images , and starts drawing her own anime character based on these references . 4 . 2 GANCollage User Interfaces In this subsection , we summarise the main components and features of GANCollage and briefy discuss algorithms involved and user actions required . 4 . 2 . 1 Sticky Notes . In order to achieve our Goal 1 and Goal 2 , we implement a semantic tagging system via virtual sticky notes . A sticky note in GANCollage contains a semantic tag that corre - sponds to multiple images , while one image can also have multiple corresponding sticky notes . A sticky note is used to help designers make sense of AI generations , brainstorm text ideas , and explore more potential generations . In GANCollage , whenever an image is obtained from the backend , and spawned on the front - end inter - face , its corresponding tags predicted by the classifer will be made into sticky notes and placed on the mood board canvas . If any tag predicted already exists in the front - end , the system will assign the image to the already - existing sticky note . For each sticky note on the mood board canvas , designers can click on it to highlight the tag , and the system will highlight all images currently assigned to it . Designers can then select or deselect images they believe to be relevant or irrelevant . Designers can also 139 GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA ( a ) An image highlighted : GANCollage attaches all sticky notes related to this image , scales it to its original resolution . The designer can click : the “delete” icon to discard it , the “more similar” icon to request more similar images , the “archive” icon on the upper right corner to save it to the drawing canvas . ( b ) A sticky note highlighted : GANCollage highlights all corresponding images related to the semantic tag . The designer can click : the “delete” icon to discard it , the “more similar” icon to request more images of this property , the images to select or deselect related images . The text can also be altered by double clicking . ( c ) Style Mix : the designer selects the frst group ( purple ) , clicks on the pop - up “mix” icon , selects another group ( blue ) and clicks on the icon again to perform a style mix . The “Style Mix” panel on the left shows a preview during the whole process . ( d ) The drawing canvas : the designer can drag it anywhere to write , draw , crop saved images , and organise archived results . Figure 2 : an overview of GANCollage features alter the tag of each sticky note by double clicking on the text . Figure 2b provides an illustration of how designers could interact with a highlighted sticky note . If a user likes to defne a customised tag , he or she can drag a sticky note from the tile of unused new tags on the tool panel , and input their own text . The user then needs to highlight the new sticky note , and select images he or she believes to have such property . 4 . 2 . 2 Exploring New Generations . To achieve our Goal 3 , GANCol - lage supports exploring StyleGAN generations via three methods : “random sampling” , “more similar generations” , and “style mix” . Random Sampling . In GANCollage , designers can request more random images by clicking on the “request” icon . Upon clicking , the front - end will randomly sample input vectors from a Gaussian distribution , and send them to the back end . Upon receiving these vectors , the backend server will feed them through the StyleGAN generator , and obtain generated images from its output . These gen - erations will fnally be sent to the front - end interface and displayed in the “new image” section . Style Mix . Inspired by Genetic Algorithm ( GA ) like methods of design space exploration [ 23 ] , GANCollage also allows users to mix styles between any two groups , either images or semantic tags . To do that a user can draw a selection triangle to select the frst group , 140 DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Wan and Lu click on the pop - up “mix” icon , draw another selection triangle to select the second , and then click the “mix” icon again to mix the styles between the two groups ( see Figure 2c as an illustration ) . Each group could include both images and sticky notes , and resulting images will be displayed in the “style mix” section . When mixing styles of two given groups , GANCollage frst re - trieves all corresponding images and input vectors of two groups . We refer to the two groups of vectors as  1 and  2 . It then ran - domly samples a subset of each group ,  1 ′ and  2 ′ , to serve as “parents” to breed a new generation  =  1 ′ ⊗  2 ′ . In our implemen - tation , we use each dimension of the 512 - dimensional input vector of the StyleGAN as the “genes” , and GANCollage performs a uni - form crossover to combine genes of the “parents” , and a mutation operation to alter some gene values to produce more diversity . More Similar Generations . In order to support convergent ex - ploration of StyleGAN latent space as specifed in Goal 3 , we use a method of requesting generations similar to existing images or semantic labels . Whenever a user fnds an image or a sticky note of interest , GANCollage allows him or her to explore more similar generations by adding some variations . For an image , if the user clicks the “more similar” icon , the front - end will add some random Gaussian noises to the input vector , and display resulting images from the backend around the image of interest at a smaller scale . For a sticky note , GANCollage retrieves all images corresponding to the tag , randomly samples two subsets , and performs a genetic operator same as “style mix” between those subsets , and displays resulting images close the the sticky note . 4 . 2 . 3 Sort and Archive . Driven by our frst two design goals , we would like to support designers in organising results on the mood board , and provide a way of recording inspirations . To that end , GANCollage supports sorting the mood board canvas , and archiving generated results of interest . Once the mood board canvas becomes cluttered , the designer could click on the “sort” icon to arrange all the sticky notes and images in several piles . If the user happens to fnd an image of interest , he or she could click on it , and the system will highlight the image by scaling it to the original resolution ( 512x512 ) , and attaching all corresponding sticky notes to it . The user could archive the image along with all semantic tags onto the canvas by clicking on the “archive” icon under the “highlight” mode . If the user is only interested in a small part of the generated image , he or she could also crop the image on the drawing canvas . 4 . 3 Implementation The front - end user interfaces of GANCollage were programmed using React as the web framework . The back - end server of GAN - Collage was programmed in Python using Django framework , and deployed on a computer equipped with an Nvidia GeForce RTX 3080 . The backend server of GANCollage holds a StyleGAN trained at a resolution of 512x512 , and a classifer pre - trained on anime images that extracts semantic labels . It returns a StyleGAN gen - erated image upon an incoming HTTP request attached with an input vector , along with semantic tags predicted by the classifer over the generated image . The classifer of GANCollage was a pre - trained illustration2vec [ 35 ] model , and the StyleGAN was trained over a dataset of 4584 anime images crawled from safebooru . org using keywords such as “simple background” , “full body” , “1girl” , “1boy” , etc . During training , we augmented our data by small translations , and random cropping from the centre . The whole training process took over 200 epochs on the same computer as the back - end server . We stopped training when the quality of generations stopped improving for several consecutive epochs . We monitored the quality of the generated images using the Fréchet Inception Distance ( FID ) [ 16 ] . 5 EVALUATION Our motivation was to assess the efectiveness of GANCollage in assisting users in exploring and digesting AI - generated visual ideas . To this end we conducted a user study to evaluate GANCollage in terms of creativity support , practical usability , and the quality of fnal results produced . We intended to compare GANCollage with the most common method of exploring AI generated content . Therefore we created a simple image gallery displaying StyleGAN generations as the baseline , as inspired by both previous work [ 32 ] , and proliferating design gallery platforms , such as Pinterest [ 34 ] and Behance [ 2 ] . 5 . 1 Participants In order to conduct our user study with novice users , we recruited 10 participants with no expertise of character design ( 5 male , 5 female , aged between 18 and 25 ) . For convenience we refer to them as P1 to 10 . All participants provided informed consent and agreed to the recording of the session and anonymised publication of the results . For each participant , we ofered a coupon equivalent to 50 HKD after the study . 5 . 2 Study Procedure At the beginning of our study , all participant were required to complete a short survey regarding their demographic data , and a self - identifcation of their knowledge of AI , especially generative AI models such as GAN . We then asked our participants to imagine a scenario where they would like to design an anime character for themselves , either as a social media profle picture , or a virtual avatar in the meta - verse , and that they frst would like to browse through some AI generations to draw inspirations . We conducted a within - subjects study . Each participants were required to use two systems : GANCollage , and an simple image gallery , and the order was randomly assigned . The system of im - age gallery also includes a dummy canvas , and users can click on images they like to bring them to the dummy canvas . When using both systems , participants were required to complete same tasks : 1 ) archive intriguing and inspirational images to their canvas 2 ) articulate design ideas they obtain while using the system . Each par - ticipant was also required to think aloud and was asked questions regarding their thought process while using these systems . The study proceeded as follows : • Step 1 : The participant provided demographic data ( age , gen - der ) , and self - identifcation of his or her AI knowledge . • Step 2 : The participant was randomly assigned to either GANCollage , or the image gallery . We frst spent 10 minutes on average training the participant to use our system , and ofered another 10 minutes for him or her to try all features . 141 GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Once the participant felt familiar enough with the system , we refreshed the web page and asked him or her to complete the task in 10 minutes . • Step 3 : The participant was required to complete a survey regarding using the frst system . • Step 4 : The participant switched groups ( from the image gallery to GANCollage or vice versa ) , and , after training and trial , was required to complete the same task in 10 minutes . • Step 5 : The participant was required to complete a survey regarding using the second system . • Step 6 : The participant was interviewed about preferences between two systems , evaluation of fnal results , refection of usability , understanding of AI generations , etc . The whole process was screen - recorded and audio - taped for further analysis , and for each system used by each participant , we kept a screenshot of the fnal results on the web page . Figure 3 : NASA - TLX of GANCollage and the image gallery 5 . 3 Survey Questions Our survey questions consisted of three main themes : creativity support , usability and usefulness , and fnal results evaluation . We used 7 - point Likert scales for all questions ( 1 : Strongly Disagree ; 7 : Strongly Agree ) . In the frst theme , we used four dimensions in Creativity Support Index ( CSI ) [ 5 ] : Enjoyment , Exploration , Ex - pressiveness , and Immersion . In the second theme , we used four dimensions of the NASA Task Load Index ( NASA - TLX ) [ 14 ] to evaluate the ease of use of two systems : Mental Demand , Phys - ical Demand , Efort , and Frustration Level . For GANCollage , we evaluate the usefulness of 7 main features : More Similar ( image ) , More Similar ( tag ) , Style Mix , Tagging , Highlight , Archive , and Ran - dom Sampling . In the third theme , we extended the “Results Worth Efort” dimension of CSI into 8 questions , in order reduce efects of randomness of AI in fnal results , and focus our survey results mainly on ideation . These 8 questions include self - rated satisfac - tion , quality and diversity of images , design ideas or inspirations obtained from the system , etc . 5 . 4 Results 5 . 4 . 1 Usability . The NASA - TLX score of two systems are sum - marised as in Figure 3 . After some time of training , participants ( e . g . , P2 , and P4 ) usually said GANCollage was overall “simple” and “explicit” . However , participants reported relatively higher level of mental demand (       = 3 . 5 ,    = 2 . 75 ) and eforts (       = 3 ,    = 1 ) , as compared to frustration (       = 2 ,    = 2 ) , and physical demand (       = 2 . 5 ,    = 3 . 25 ) . The reason was mainly because GANCollage required users to read through seman - tic labels and sort out the mood board when it became cluttered ( e . g . , “ I felt it was mentally demanding because I had to focus on each sticky notes to make sense of it ” - P10 , “ I was too focused on organis - ing things on the mood board while using GANCollage - P7” ) . These results manifested that participants felt GANCollage was overall easy to use , though as expected , an image gallery was perceived to be easier since it was much less complicated . In terms of usefulness , GANCollage was deemed by many users as “impressive” , and “a preferred method” of exploring AI gener - ations ( e . g , P4 , P5 ) . A summary of ablative usefulness ratings is shown in Figure 4 . All features surveyed achieved       > 5 and    ≤ 2 . 5 . 4 . 2 Creativity Support & Final Results . We frst calculate the mean of each dimension of CSI , and then compute the median and IQR for each dimension and the overall score . As summarised in Ta - ble 1 , our GANCollage outperformed the traditional image gallery in Enjoyment , Expressiveness , and Immersion . While using GANCol - lage , most users ( e . g . , P4 - 5 , P10 ) said it was “more interactive” and therefore “more enjoyable” , and P4 particularly mentioned that “ I prefer using GANCollage for exploration since I could kind of tell the AI what to generate . ” The reason that the score of Exploration was not signifcantly higher was partly because image galleries were generally perceived as easier to use ( e . g . , P7 ) . Survey questions of fnal results , however , showed no signif - cant diferences between the two systems . The image gallery re - ported       = 5 . 75 ,    = 0 . 9375 , while GANCollage slightly better with       = 5 . 8125 ,    = 1 . 5 . 4 . 3 Reflection on Semantic Support . The semantic support is one major diference between GANCollage and traditional image galleries . During our study , we found that all participants spent at least some time browsing through AI suggested tags , highlighting them to see corresponding images , and highlighting images to read through all tags attached . In particular , P2 was found to spend most time exploring semantic features . He used to highlight specifc sticky notes to browse and organise images on the mood board , 142 DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Wan and Lu Figure 4 : User rated usefulness of GANCollage features Image Gallery GANCollage ( ours ) Median IQR MEDIAN IQR Enjoyment Exploration Expressiveness Immersion 5 . 5 6 5 . 5 5 . 5 1 0 . 5 0 . 5 1 . 375 6 6 6 . 25 6 1 0 . 875 1 0 . 75 Overall 5 . 75 1 . 75 6 0 . 75 Table 1 : Creativity Support Index of GANCollage and the image gallery mix styles between sticky notes , and click on “more similar” icon of individual sticky notes . He explained that he preferred using tags to sort out the mood board and explore new generations , because it was more “explicit” compared to an image gallery . P7 added that , “ If there would be a stage where I were to draw something ( on the canvas ) , I would pretty much use these ( archived sticky notes ) as a reference ” . P4 complimented that , “ The most intriguing thing ( of GANCollage ) is the ability of the AI to illustrate properties from ( generated ) images . I don’t know what these properties are but the AI already knew ( and explained to me ) , and that impressed me the most . ” However , P7 noted that it would be more mentally demanding than a simple gallery without semantic support to organise and make sense of the sticky notes on a mood board , especially when it sometimes became cluttered . During our study , there was also a case where P8 came across a semantic tag “monochrome” that intrigued and inspired her , but when she tried telling the AI to gen - erate images of such property , results from the system constantly frustrated her . Furthermore , P1 was the only user that showed obvious inclination towards a system without any semantic tags . She explained that , “ I was more interested in images that are more abstract and sort of inexplicable by semantic tags , images that might not look like a character at all . The sticky notes of GANCollage for me seemed distracting ” 5 . 4 . 4 Gallery display vs . Interactive mood board . Towards the end of each study , we asked our participants to compare the two systems in terms of exploring new generations . Many of our users ( P2 , P5 - 6 , P8 , P10 ) showed clear preference for a mood board such as GANCollage , describing it as a more enjoyable and customisable form . P5 , P6 and P8 all stressed that they would like to interact with the AI via a mood board instead of browsing through static generated images in a gallery , because it would be more enjoyable . P6 particularly noted that it would be quite tedious if she were to explore an image gallery without any interaction for a long time . Besides , P5 and P8 also emphasised that an interactive mood board allows for more room of customisation . P8 mentioned she could actually “fne - tune” an image using GANCollage , while on a gallery you could only click to save it . P5 also said that , “ In an image gallery , if you fnd an image you like or you dislike , there’s nothing really you can do , you can only scroll down for more . While in another prototype ( GANCollage ) , you can flter out very similar images . You can also use various generation ( exploration ) methods , and that was very efective and helpful . ” Some other users ( P4 and P7 ) also noted the advantage of a image gallery as being more straightforward , though not necessarily negative towards a mood board . P7 said it to be easier to use , and “clear at a glance” . P4 also felt that such a gallery brought about more diversity during his study . However , survey results showed that GANCollage was at least comparable to a image gallery , in terms of fnal results delivered . 5 . 4 . 5 Genetic Exploration of a GAN Latent Space . Among our three exploration methods , “style mix” was found to be the most com - monly used one . Most users said it was because it introduced more diversity ( e . g . , P5 ) , and allowed for more information specifed ( both visual and semantic ) , which implied more room of customisation ( e . g , P8 ) . Of all participants , P5 was observed to use style mix al - most as the sole method of requesting new generations . He was also found to only mix groups consisting of sticky notes and he explained that , “ When using the style mix function , I felt tags were better for that purpose because if you use images , too many attributes 143 GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA would be entangled . ” . When asked about his criteria of selecting tags , he said he basically chose tags of diferent types ( clothes , hair , accessories , etc . ) as one group to mix with another . After one round of style mix , he usually re - organised the mood board with newly generated images , and then selected another two group for another round “style mix” . The user actions of P5 almost perfectly refected our initial design idea of a Genetic Algorithm like exploration method replacing the ftness function with user selecting and fltering . When asked to what degree results of “style mix” refected what he really wanted from the AI , he said , “ Although some of them don’t really ft , but most of them that come out do ft the tags I selected ” . 5 . 4 . 6 User Perception of Generative AI . By observing how partic - ipants use our system , especially those with fewer knowledge of AI , we identifed an obvious discrepancy between their percep - tion of AI and how AI really works . Such discrepancy often led to improper use of some AI - related functions . For example , almost all participants , when frstly clicking on the “more similar” icon of a sticky note , expected the AI to generate completely new im - ages that ft the semantic description . However , such feature was actually implemented via a genetic operator over its own group of images , and if there were only a handful of images related to that semantic attribute , the resulting images would be very similar . Some of our users reported some level of frustration ( e . g . , P1 , P3 , P10 ) upon seeing these low - diversity results , and said it was some - what useless . Our survey also showed the “more similar” feature of sticky notes was perceived to be the most useless among all features (       = 2 . 5 ,    = 1 . 75 ) . During the study , once a participant looked confused , researchers usually explained that , “ By clicking on the “more similar” icon on a sticky note , AI will only generate images from what you tell him or her to have such property . If there were no or very few images on the mood board related to such semantic information , the AI would have no idea what to generate and end up replicating these few existing images ” Such explanation clearly reduced some confusion among participants . Some participants like P3 who has absolutely zero technical knowledge were also observed to have gained a sense of how AI works upon that feature , and obtained decent results after using it properly . 6 DISCUSSION Our user study showed that GANCollage provided more creativity support , especially in terms of Expressiveness , while producing a fnal result at least comparable to traditional image galleries . In - terviews with participants also revealed GANCollage served as a more enjoyable , explicit , and efective form of exploring AI genera - tions . Our observations during the study also provide insights into how users make sense of AI - related features , and the AI behind the scene . In next two subsections we briefy discuss design implica - tions of our fndings during user study for CST design , human - AI interaction and digital mood board . 6 . 1 Promoting Explainability in CST Design According to our observations of how participants used GANCol - lage , the discrepancy between user perception of the AI and how AI actually works signifcantly compromises the usability of the system . It might well lead to frustration , and distrust in the AI . Communicating to the user how AI works in a way he or she could understand is key to the proper use of AI - related features . Although in the feld of computer - supported creativity [ 8 , 19 , 28 ] , AI and other computing technologies have been deployed in many meaningful ways , little attention yet has been paid to how explainability of these AI models should be communicated . We argue that such explainability , or literacy of computing tech - nologies in general , plays a key role in successful deployment of these computer support , in order to promote creativity . For CST design , we believe that explaining to users of diferent backgrounds how AI works to boost creativity is equally important to deploy - ing it , as it builds trust and facilitates interaction . In GANCollage , the technical details of the StyleGAN and the AI classifer were largely obscured by the user interface using metaphors such as sticky notes . However , discrepancy did occur during the user study as no design eforts were made to address the invisible gap between the interface and the AI beneath . In our case , researchers had to explain the detailed workfow in a straightforward language . We hope that afordances of a future CST design could imply such an explanation . It could , for example , map the technical process of a working AI to the user interface using metaphors , in order to make the entire workfow self - explanatory . 6 . 2 Genetic Exploration in Human - AI Interaction Our user study revealed that , participants usually preferred “style mix” among all three exploration methods of StyleGAN , reasons being that it was more enjoyable and customisable . We believe such fndings open up potential of introducing genetic exploration in human - AI interaction . Such genetic approach has already been extensively used in design space exploration , particularly in 3D shape modeling [ 6 , 33 , 40 ] . Kim et al . [ 23 ] also introduced such approach into a group collaboration setting . We argue that it also deserves attention in supporting human interaction with AI . We envision that for future AI - based CSTs , we can model the entire human - in - the - loop interactions with AI as a genetic algo - rithm , and the fnal results that are satisfying for users from the AI as the output . Starting from some randomly initialised results from the AI , user browsing , selecting , and fltering could be seen as the ftness function , and each request from users to AI could be processed as a genetic operator . In this way , through evolution and “user selection” , the system could fnally converge to satisfactory results . We believe that such process helps better articulate vague user needs when interacting with AI - based creativity support tools , especially a generative AI that sometimes delivers inexplicable or somewhat random results . It also demands little user knowledge of how AI or genetic algorithm works . 6 . 3 Re - examining Digital Mood Board GANCollage takes the form of a digital mood board , which proved to be “enjoyable” both quantitatively and qualitatively . Such out - come agrees with previous studies of physical mood boards , that claimed they were fun to create [ 12 ] . It is also more interactive and explicit . However , our study also revealed that a digitalised mood board is often more demanding and lacks the “clarity at one 144 DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA Wan and Lu glance” , compared to an image gallery on a web page . An intuitive method of addressing such problem could be directly deploying a smaller image gallery on the mood board canvas . However , the low resolution of images displayed , and how the gallery interacts with the main canvas still remain an issue . We believe that future CSTs should consider blending the two forms in order to take advantages of both for design ideation . 6 . 4 Limitations The StyleGAN involved in our study was trained over merely 4584 images , which limits the power of the AI itself . It is a bit uncertain how our design features or study results generalises to a larger design space . The classifer was also pre - trained years ago over semantic tags crawled online , some of which seemed unft for design ideation . A larger AI generation space and more accurate , diversifed classifcation would yield richer and more accurate results from the user study . In the future , we would like to fne - tune our StyleGAN on a larger dataset , deploy the system publicly , and conduct our survey over a larger group of people in order to report signifcant statistical results . Besides , the major drawback of GANCollage was reported by our users to be its lack of “diversity” at initial state or “clarity at glance” , as compared to the image gallery . We believe we should consider how we could take advantage of both systems to improve on GANCollage , or digital mood boards in general . For example , we could aim to integrate a cascading image gallery into a mood board that could to some degree preserve high - resolution of images displayed . Furthermore , in this paper we only selected an image gallery as the baseline . The efectiveness of GANCollage compared to other interactive design , sampling algorithms , or AI techniques for design ideation remains unclear . 6 . 5 Future Work We believe that our design and study could inspire future work in various felds . Building upon GANCollage in creativity research , we should consider deploying generative AI models to support a range of creative processes beyond ideation . For example , after using GAN for ideation , we can also introduce generative AI for co - creation , correction , or suggestion during the design phase . The potential that generative AI be introduced into a group brainstorming session or a group co - creation setting should also be considered . Furthermore , our methods of exploring StyleGAN generations should also inspire future work in human - AI interaction . We be - lieve the potential of genetic algorithm approach should be frstly seriously considered in the user exploration of AI latent space . Moreover , to efectively incorporate AI in HCI design , greater at - tention and design eforts should be directed towards promoting ’AI literacy’ across various AI - supported systems . 7 CONCLUSION In this paper we propose a creativity support tool that harnesses the power of a StyleGAN to facilitate ideation . It takes the form of a digital mood board with data - driven and customisable seman - tic support , which allows for both serendipitous and convergent explorations of the StyleGAN latent space using the semantic sys - tem . Our evaluation manifests it is more enjoyable , more explicit , and more efective . Our study also provides design implications for CST design and human - AI interaction , which hopefully will inspire future creativity research involving generative AI models . ACKNOWLEDGMENTS This research was partially supported by the 2021 CCF - Tencent Rhino - Bird Research Fund and the Research Matching Grant Scheme ( RMGS , Project No . 9229095 ) . We thank Tencent , China Computer Federation ( CCF ) , and Research Grants Council of Hong Kong for their support and guidance . REFERENCES [ 1 ] Rawan Alghofaili , Matthew Fisher , Richard Zhang , Michal Lukáč , and Lap - Fai Yu . 2021 . Exploring Sketch - based Character Design Guided by Automatic Col - orization . In Graphics Interface 2021 . [ 2 ] Behance . 2022 . Search the creative world at work . Retrieved September 16 , 2022 from https : / / www . behance . net / [ 3 ] Carole Bouchard , Jean - francois Omhover , Celine Mougenot , Ameziane Aoussat , and Stephen J Westerman . 2008 . TRENDS : a content - based information retrieval system for designers . In Design Computing and Cognition’08 . Springer , 593 – 611 . [ 4 ] Tracy Diane Cassidy . 2008 . Mood boards : Current practice in learning and teaching strategies and students’ understanding of the process . International journal of fashion design 1 , 1 ( 2008 ) , 43 – 54 . [ 5 ] Erin Cherry and Celine Latulipe . 2014 . Quantifying the creativity support of digital tools through the creativity support index . ACM Transactions on Computer - Human Interaction ( TOCHI ) 21 , 4 ( 2014 ) , 1 – 25 . [ 6 ] Orestes Chouchoulas and Alan Day . 2007 . Design exploration using a shape grammar with a genetic algorithm . open house international ( 2007 ) . [ 7 ] Yuanzheng Ci , Xinzhu Ma , Zhihui Wang , Haojie Li , and Zhongxuan Luo . 2018 . User - guided deep anime line art colorization with conditional adversarial net - works . In Proceedings of the 26th ACM international conference on Multimedia . 1536 – 1544 . [ 8 ] Nicholas Davis , Chih - PIn Hsiao , Kunwar Yashraj Singh , Lisa Li , and Brian Magerko . 2016 . Empirically studying participatory sense - making in abstract drawing with a co - creative cognitive agent . In Proceedings of the 21st International Conference on Intelligent User Interfaces . 196 – 207 . [ 9 ] Nicolas Ducheneaut , Ming - Hui Wen , Nicholas Yee , and Greg Wadley . 2009 . Body and mind : a study of avatar personalization in three virtual worlds . In Proceedings of the SIGCHI conference on human factors in computing systems . 1151 – 1160 . [ 10 ] Noyan Evirgen and Xiang’Anthony’ Chen . 2022 . GANzilla : User - Driven Direction Discovery in Generative Adversarial Networks . arXiv preprint arXiv : 2207 . 08320 ( 2022 ) . [ 11 ] Jonas Frich , Lindsay MacDonald Vermeulen , Christian Remy , Michael Mose Biskjaer , and Peter Dalsgaard . 2019 . Mapping the landscape of creativity support tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 – 18 . [ 12 ] Steve Garner and Deana McDonagh - Philp . 2001 . Problem interpretation and resolution via visual stimuli : the use of ‘mood boards’ in design education . Journal of Art & Design Education 20 , 1 ( 2001 ) , 57 – 64 . [ 13 ] Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . 2020 . Generative adversarial networks . Commun . ACM 63 , 11 ( 2020 ) , 139 – 144 . [ 14 ] Sandra G Hart . 1986 . NASA task load index ( TLX ) . ( 1986 ) . [ 15 ] Scarlett R Herring , Chia - Chen Chang , Jesse Krantzler , and Brian P Bailey . 2009 . Getting inspired ! Understanding how and why examples are used in creative design practice . In Proceedings of the SIGCHI conference on human factors in computing systems . 87 – 96 . [ 16 ] Martin Heusel , Hubert Ramsauer , Thomas Unterthiner , Bernhard Nessler , and Sepp Hochreiter . 2017 . Gans trained by a two time - scale update rule converge to a local nash equilibrium . Advances in neural information processing systems 30 ( 2017 ) . [ 17 ] Angel Hsing - Chi Hwang . 2022 . Too late to be creative ? AI - empowered tools in creative processes . In CHI Conference on Human Factors in Computing Systems Extended Abstracts . 1 – 9 . [ 18 ] David G Jansson and Steven M Smith . 1991 . Design fxation . Design studies 12 , 1 ( 1991 ) , 3 – 11 . [ 19 ] Youngseung Jeon , Seungwan Jin , Patrick C Shih , and Kyungsik Han . 2021 . Fash - ionQ : an ai - driven creativity support tool for facilitating ideation in fashion design . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 18 . [ 20 ] Yanghua Jin , Jiakai Zhang , Minjun Li , Yingtao Tian , Huachun Zhu , and Zhihao Fang . 2017 . Towards the automatic anime characters creation with generative adversarial networks . arXiv preprint arXiv : 1708 . 05509 ( 2017 ) . 145 GANCollage : A GAN - Driven Digital Mood Board to Facilitate Ideation in Creativity Support DIS ’23 , July 10 – 14 , 2023 , Pitsburgh , PA , USA [ 21 ] Tero Karras , Samuli Laine , and Timo Aila . 2019 . A style - based generator ar - chitecture for generative adversarial networks . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 4401 – 4410 . [ 22 ] Tero Karras , Samuli Laine , Miika Aittala , Janne Hellsten , Jaakko Lehtinen , and Timo Aila . 2020 . Analyzing and improving the image quality of stylegan . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 8110 – 8119 . [ 23 ] Kevin Gonyop Kim , Richard Lee Davis , Alessia Eletta Coppi , Alberto Cattaneo , and Pierre Dillenbourg . 2022 . Mixplorer : Scafolding Design Space Exploration through Genetic Recombination of Multiple Peoples’ Designs to Support Novices’ Creativity . In CHI Conference on Human Factors in Computing Systems . 1 – 13 . [ 24 ] Janin Koch , Nicolas Tafn , Michel Beaudouin - Lafon , Markku Laine , Andrés Lucero , and Wendy E Mackay . 2020 . Imagesense : An intelligent collaborative ideation tool to support diverse human - computer partnerships . Proceedings of the ACM on human - computer interaction 4 , CSCW1 ( 2020 ) , 1 – 27 . [ 25 ] Janin Koch , Nicolas Tafn , Andrés Lucero , and Wendy E Mackay . 2020 . Semantic - Collage : enriching digital mood board design with semantic labels . In Proceedings of the 2020 ACM Designing Interactive Systems Conference . 407 – 418 . [ 26 ] Bryan Lawson and Kees Dorst . 2013 . Design expertise . Routledge . [ 27 ] Brian Lee , Savil Srivastava , Ranjitha Kumar , Ronen Brafman , and Scott R Klemmer . 2010 . Designing with interactive example galleries . In Proceedings of the SIGCHI conference on human factors in computing systems . 2257 – 2266 . [ 28 ] Ryan Louie , Andy Coenen , Cheng Zhi Huang , Michael Terry , and Carrie J Cai . 2020 . Novice - AI music co - creation via AI - steering tools for deep generative models . In Proceedings of the 2020 CHI conference on human factors in computing systems . 1 – 13 . [ 29 ] Zhicong Lu , Chenxinran Shen , Jiannan Li , Hong Shen , and Daniel Wigdor . 2021 . More kawaii than a real - person live streamer : understanding how the otaku community engages with and perceives virtual YouTubers . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 14 . [ 30 ] Richard L Marsh , Joshua D Landau , and Jason L Hicks . 1996 . How examples may ( and may not ) constrain creativity . Memory & cognition 24 , 5 ( 1996 ) , 669 – 680 . [ 31 ] Victoria McArthur , Robert John Teather , and Jennifer Jenson . 2015 . The avatar afordances framework : mapping afordances and design trends in character creation interfaces . In Proceedings of the 2015 annual symposium on Computer - Human Interaction in Play . 231 – 240 . [ 32 ] Mohammad Amin Mozafari , Xinyuan Zhang , Jinghui Cheng , and Jin LC Guo . 2022 . GANSpiration : Balancing Targeted and Serendipitous Inspiration in User Interface Design with Style - Based Generative Adversarial Network . In CHI Con - ference on Human Factors in Computing Systems . 1 – 15 . [ 33 ] Marcin L Pilat and Christian Jacob . 2008 . Creature academy : A system for virtual creature evolution . In 2008 IEEE Congress on Evolutionary Computation ( IEEE World Congress on Computational Intelligence ) . IEEE , 3289 – 3297 . [ 34 ] Pinterest . 2022 . Discover recipes , home ideas , style inspiration and other ideas to try . Retrieved September 16 , 2022 from https : / / www . pinterest . com / [ 35 ] Masaki Saito and Yusuke Matsui . 2015 . Illustration2vec : a semantic vector repre - sentation of illustrations . In SIGGRAPH Asia 2015 Technical Briefs . 1 – 4 . [ 36 ] Yang Shi , Nan Cao , Xiaojuan Ma , Siji Chen , and Pei Liu . 2020 . EmoG : supporting the sketching of emotional expressions for storyboarding . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 37 ] Patrick C Shih , David H Nguyen , Sen H Hirano , David F Redmiles , and Gillian R Hayes . 2009 . GroupMind : supporting idea generation through a collaborative mind - mapping tool . In Proceedings of the ACM 2009 international conference on Supporting group work . 139 – 148 . [ 38 ] Hao - Chuan Wang , Dan Cosley , and Susan R Fussell . 2010 . Idea expander : support - ing group brainstorming with conversationally triggered visual thinking stimuli . In Proceedings of the 2010 ACM conference on Computer supported cooperative work . 103 – 106 . [ 39 ] Hao - Chuan Wang , Susan R Fussell , and Dan Cosley . 2011 . From diversity to creativity : Stimulating group brainstorming with cultural diferences and conversationally - retrieved pictures . In Proceedings of the ACM 2011 conference on Computer supported cooperative work . 265 – 274 . [ 40 ] Kai Xu , Hao Zhang , Daniel Cohen - Or , and Baoquan Chen . 2012 . Fit and diverse : Set evolution for inspiring 3d shape galleries . ACM Transactions on Graphics ( TOG ) 31 , 4 ( 2012 ) , 1 – 10 . [ 41 ] Enhao Zhang and Nikola Banovic . 2021 . Method for Exploring Generative Adversarial Networks ( GANs ) via Automatically Generated Image Galleries . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 15 . 146