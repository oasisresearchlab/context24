AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms ZANA BUÇINCA ∗ , Harvard University , USA CHAU MINH PHAM ∗ , University of Massachusetts Amherst , USA MAURICE JAKESCH , Cornell University , USA MARCO TULIO RIBEIRO , Microsoft Research , USA ALEXANDRA OLTEANU , Microsoft Research , Canada SALEEMA AMERSHI , Microsoft Research , USA While demands for change and accountability for harmful AI consequences mount , foreseeing the downstream effects of deploying AI systems remains a challenging task . We developed AHA ! ( Anticipating Harms of AI ) , a generative framework to assist AI practitioners and decision - makers in anticipating potential harms and unintended consequences of AI systems prior to development or deployment . Given an AI deployment scenario , AHA ! generates descriptions of possible harms for different stakeholders . To do so , AHA ! systematically considers the interplay between common problematic AI behaviors as well as their potential impacts on different stakeholders , and narrates these conditions through vignettes . These vignettes are then filled in with descriptions of possible harms by prompting crowd workers and large language models . By examining 4113 harms surfaced by AHA ! for five different AI deployment scenarios , we found that AHA ! generates meaningful examples of harms , with different problematic AI behaviors ( e . g . , false positives vs . false negatives ) resulting in different types of harms . Prompting both crowds ( N = 98 ) and a large language model with the vignettes resulted in more diverse examples of harms than those generated by either the crowd or the model alone . To gauge AHA ! ’s potential practical utility , we also conducted semi - structured interviews with responsible AI professionals ( N = 9 ) . Participants found AHA ! ’s systematic approach to surfacing harms important for ethical reflection and discovered meaningful stakeholders and harms they believed they would not have thought of otherwise . Participants , however , differed in their opinions about whether AHA ! should be used upfront or as a secondary - check and noted that AHA ! may shift harm anticipation from an ideation problem to a potentially demanding review problem . Drawing on our results , we discuss design implications of building tools to help practitioners envision possible harms . ACM Reference Format : Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi . 2023 . AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms . 1 , 1 ( June 2023 ) , 23 pages . 1 INTRODUCTION In August 2020 , thousands of UK students protested " F * ck the Algorithm " in response to the UK government employing an algorithm to predict their scores on A - level exams canceled due to the Covid - 19 pandemic [ 35 ] . The algorithm disproportionately affected poorer students and led to 40 % of students receiving lower grades than expected on the exams that largely determine their university placement . While the backlash forced the government to retract the grades , the incident had already left students , families , and teachers scrambling to appeal the results and secure placement at universities [ 1 , 36 , 65 ] , universities struggling to adjust resources due to unexpected admission numbers [ 16 ] , and * Equal contribution Authors’ addresses : Zana Buçinca , zbucinca @ seas . harvard . edu , Harvard University , Boston , MA , USA ; Chau Minh Pham , ctpham @ umass . edu , University of Massachusetts Amherst , Amherst , MA , USA ; Maurice Jakesch , mpj32 @ cornell . edu , Cornell University , Ithaca , NY , USA ; Marco Tulio Ribeiro , marco . correia @ microsoft . com , Microsoft Research , Redmond , WA , USA ; Alexandra Olteanu , alexandra . olteanu @ microsoft . com , Microsoft Research , Montréal , Quebec , Canada ; Saleema Amershi , samershi @ microsoft . com , Microsoft Research , Redmond , WA , USA . 2023 . Manuscript submitted to ACM Manuscript submitted to ACM 1 a r X i v : 2306 . 03280v1 [ c s . H C ] 5 J un 2023 2 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi Fig . 1 . Overview of AHA ! for the hiring scenario . 1 ) Given a description of an AI deployment scenario and 2 ) a list of problematic AI behaviors of interests provided by a user , 3 ) AHA ! first generates relevant stakeholders . Crossed with possible AI behaviors , these stakeholders form the Ethical Matrix , 4 ) for which AHA ! then generates a separate vignette for each cell . 5 ) These vignettes along with the scenario description are then used to prompt crowd judges and an LLM to elicit examples of possible harms . 6 ) The examples can then be coded and clustered under high - level categories of harms to characterize them ( optional ) . government officials to contend with legal and reputational damage [ 12 ] . The UK’s automated grading fiasco is just one example of the growing number of incidents caused by AI systems deployed in applications and services sectors [ 42 ] . While demands for change and accountability for harmful AI consequences mount [ 70 ] , foreseeing the downstream effects of deploying AI systems remains a challenging task [ 8 ] . Anticipating harms requires grappling with the complexity and scope of contexts that even seemingly simple AI technologies may impact . The UK’s automated grading system , for example , had a wide range of consequences that would have required envisioning the interplay between various stakeholders ( from students and their families to universities and government agencies ) , problematic AI behaviors ( e . g . , inaccurate predictions , systematic biases ) , and how the AI might be used ( e . g . , admittance decisions , mitigation options , misuses ) . Even when guidance is given about how to proactively reflect on possible harms , the responsibility of carrying out this task most often falls on the shoulders of untrained and time - constrained practitioners such as system engineers or project managers [ 60 ] . Moreover , even motivated practitioners can fail to envision downstream consequences to diverse stakeholders due to demographically skewed backgrounds and homogeneous experiences [ 11 , 13 , 31 ] . In this paper , we present AHA ! ( Anticipating Harms of AI ) , a framework to support AI system creators , auditors and other decision - makers in foreseeing potential harms an AI system may cause before it is deployed or even implemented ( see Figure 1 and Section 3 ) . Given a short description of a deployment scenario , AHA ! surfaces a set of potential harms by systematically considering how various stakeholders ( e . g . , individuals , groups , entities ) experience problematic AI behaviors ( e . g . , false positives / negatives ) , narrating those experiences through vignettes ( fictional scenarios used in the social sciences to elicit people’s judgments when direct real - world investigations are impractical , unethical , or too complex to control [ 5 , 28 ] ) , and generating harms to complete the vignettes by prompting crowdworkers or large language models . The possible harms surfaced by AHA ! are then reviewed by AI practitioners to inform development and deployment decisions . To evaluate AHA ! ’s capacity to surface possible harms , we first ran a series of experiments where we applied AHA ! to five different AI deployment scenarios : hiring , loan application , content moderation , communication compliance , and disease diagnosis . Our analyses in Section 4 show that AHA ! generates meaningful examples of possible harms ( i . e . , sensible and relevant to the given scenario ) . We also find that varying certain dimensions of problematic AI behaviors ( e . g . , false positives / negatives ) significantly impacts the types of harms surfaced and that crowds and a large language Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 3 model ( GPT - 3 ) surface comparable numbers of unique harms but significantly different types of harms . We further show that the combination of crowds + GPT3 produced significantly larger and more diverse examples of harms than either the crowd or the model alone . We then conducted a semi - structured interview study with ( N = 9 ) Responsible AI professionals from industry and academia , where they reviewed the harms surfaced by our five experiments and gave us feedback on AHA ! ’s potential practical utility ( see Section 5 ) . Our participants indicated that they found AHA ! ’s systematic approach to ethical reflection important for considering a broad range of harmful outcomes and discovered meaningful examples of possible harms they believed they would not have thought of on their own . However , they differed in opinion on whether practitioners should use AHA ! upfront or as a secondary - check after engaging with the task of anticipating harms on their own . They also suggested that AHA ! shifts harm anticipation from an ideation problem to a demanding review problem . Given these findings , we discuss implications and trade - offs in building tools for ethical reflection about AI systems ( see Section 6 ) . 2 BACKGROUND AND RELATED WORK Responsible AI Practices . Realizing the promise of responsible AI [ 14 , 24 , 32 , 46 , 49 , 70 ] remains challenging in practice due to cultural , socio - technical , and organizational factors . Recommendations for addressing these challenges include ethical education [ 54 ] , establishing ethical norms in AI communities [ 39 , 66 ] , aligning responsible AI goals with other organizational incentives [ 51 , 60 ] , introducing governance structures and review boards [ 7 , 38 , 45 , 59 ] , and developing tools and processes to operationalize responsible AI goals [ 51 , 59 , 60 ] . These are complementary approaches , with AHA ! falling within the realm of tools to help development teams , auditors , and others anticipate AI risks and harms . Within the scope of responsible AI tools and processes , several target specific stages in the AI development life cycle [ 69 ] including before [ 3 , 4 ] , during [ 19 , 47 ] , and after development [ 57 , 62 ] . Datasheets for datasets [ 19 ] and model cards [ 47 ] aim to support documentation and reflection on decisions made during data collection and model training / testing . Others have proposed tools to support responsible AI decision making and documentation throughout product development [ 41 , 59 ] . AHA ! is designed to help product teams proactively anticipate harms early and before resources have been spent making it increasingly difficult to change course [ 21 , 60 ] . Moreover , anticipating or identifying harms is often recommended as the first step in harm prevention or remediation [ 55 , 69 ] , followed by activities such as cost - benefit analyses , root - cause analyses , and mitigation decision - making . AHA ! may thus serve to support or connect responsible AI documentation and decision making at other stages of product development . Anticipating Downstream Harms of yet unbuilt or deployed technologies is fundamentally an exercise in creativity and imagination [ 8 , 21 ] . Responsible AI guidance for anticipating harms often advises reflecting on the interplay between stated ethical principles or values , AI system behaviors , and intended contexts of use ( e . g . , who will be using the system and where and when ) , among other considerations [ 8 , 55 ] . Yet , each of these remain challenging to consider individually , let alone in combination . Many have criticized the vagueness and ambiguity of high - level AI ethical values and the subsequent difficulty practitioners face translating them into meaningful actions [ 8 , 33 , 39 , 48 , 51 ] , or have shown the difficulties practitioners have when foreseeing problematic system behaviors ( e . g . , failures or biases ) or imagining how those behaviors might manifest in specific contexts [ 8 , 29 ] . We ground harm anticipation in vignettes depicting problematic system behaviors manifesting in specific contexts of use and involving a range of stakeholders ( see Section 3 ) . Closest to our framework are approaches that propose systematic exploration of harmful outcomes by structuring the reflection process [ 44 , 55 , 66 ] . The Failure Modes and Effects Analysis ( FMEA ) approach [ 67 ] prompts developers to enumerate potential system failures and then think through effects , possible causes , and mitigation actions . We Manuscript submitted to ACM 4 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi structure the AHA ! framework like an ethical matrix [ 44 ] , a conceptual tool originally developed to facilitate systematic consideration of biotechnology implications in food production [ 43 ] . We chose the ethical matrix , as it structures reflection around impacted stakeholders and ethical goals , concepts , or system behaviors , providing a more comprehensive view of AI impact than solely system - centered frameworks such as FMEA . O’Neil and Gunn [ 55 ] proposed the ethical matrix as a guide to systematically consider benefits and harms in the context of AI technologies . They structure the matrix by impacted stakeholders as rows and AI principles ( e . g . , justice ) , technical concepts ( e . g . , accuracy ) , and / or problematic outcomes ( e . g . , false positives / negatives ) as columns . Ethical matrices are typically used to guide discussions amongst developers and decision - makers or amongst impacted stakeholders in participatory settings [ 18 , 37 , 59 ] . Discussants are often invited to openly deliberate on issues concerning individual cells or participate in more guided workshops probing specific cell topics . Concerns , discussions , and evidence are then captured per cell for later reflection by decision - makers . Our work builds on these previous approaches for systematic exploration of harms in several important ways . First , we take a semi - automated approach to generate ethical matrices for given application scenarios , including automatically suggesting relevant stakeholders ( rows ) based on guidance about common stakeholder groups impacted by AI technolo - gies [ 45 , 55 ] and automatically populating matrix cells with descriptive vignettes to help elicit empathy and ground reflection , a challenge emphasized by prior work [ 44 , 55 ] . Second , we investigate the feasibility and effectiveness of using crowdsourcing or pre - trained language models to solicit or generate meaningful examples of harms . Such solutions can encourage impact assessments of AI technologies beyond high - risk applications—as is typically recommended by most policies and frameworks due to their high - cost [ 38 , 45 , 59 , 59 ] —by making conducting such assessments easier . Envisioning Methods . Many methods have been proposed in the fields of cognitive psychology and human - computer interaction for spurring creative thinking and ideation about possible futures ( for overviews see [ 17 , 40 , 53 ] ) . In the context of responsible AI , questions and prompts have been used to help AI developers envision potential harms [ 3 , 41 , 45 ] . Ballard et al [ 3 ] designed a game to facilitate imagining harms through playing cards prompting players to craft hypothetical AI product reviews from various stakeholder perspectives and considering various ethical principles . Another approach to eliciting attitudes about hypothetical AI deployment futures is the factorial survey [ 26 , 34 ] . Factorial surveys typically present participants with fictional narratives and solicit responses about those narratives , often manipulating dimensions of interest ( e . g . , contexts of use or system behaviors ) . These envisioning tools , particularly those leveraging groups or crowds [ 53 , 61 ] , can help broaden perspectives of often homogenous research and development teams within the tech industry [ 3 , 8 , 55 ] . We draw on the factorial survey method and automatically generate partial vignettes for every cell in AHA ! ’s ethical matrix , and then use those vignettes as prompts to be completed with potential harms by crowds . Simulations are also widely used in computer science and other fields to help predict possible futures [ 2 , 50 ] . Recent work has used large - language models ( LLMs ) to simulate and study human behavior . Horton [ 30 ] refers to LLMs as imperfect computational models of humans , or homo silicus , and used them to replicate past studies in behavioral economics . In another example , Park et al . [ 56 ] used LLMs to simulate social interactions between distinct LLM - powered personas in social computing systems to help system designers refine community rules of engagement . AHA ! also uses LLMs to complete vignettes with possible examples of harms , complementing examples elicited from crowds . 3 THE AHA ! FRAMEWORK DESIGN AHA ! is a generative framework designed to help AI developers , auditors , and other decision - makers anticipate harms prior to the development or deployment of AI systems . Figure 1 overviews the AHA ! framework . Given a description of an AI deployment scenario , AHA ! first generates potential stakeholders for that system via a large language model . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 5 Next , AHA ! populates the cells of an ethical matrix , where the rows correspond to the generated stakeholders and the columns correspond to a pre - defined set of AI behaviors . Each cell is populated with vignettes depicting scenarios where the corresponding stakeholder ( row ) experiences a behavior ( column ) of the AI system in question . Figure 1 shows a sample vignette for the hiring scenario . In this example , the stakeholder is an applicant experiencing a false negative behavior—the AI system determined they were not qualified for a position ( incorrectly ) . Finally , AHA ! prompts crowdworkers and a large language model to complete the vignettes with descriptions of harmful consequences . Stakeholders ( Rows ) . Reasoning about the ethical implications of an AI system requires careful consideration of relevant stakeholders . Given the far - reaching consequences of many AI - based incidents [ 42 ] , responsible AI guidance has started to emphasize consideration of a broad range of both direct and indirect stakeholders when envisioning possible harms [ 45 ] . Direct stakeholders are considered those who directly interact with or are immediately affected by an AI system , while indirect stakeholders may be people associated with direct stakeholders or larger community groups . For example , in a hiring scenario direct stakeholders may include the applicant and the hiring manager while indirect stakeholders may be the applicant’s family , society at large , or future applicants . AI Behaviors ( Columns ) . Given that we are interested in harms arising in AI deployment scenarios , we chose to distinguish between various problematic AI behaviors that stakeholders may experience . We make this distinction to set up fictitious scenarios—that stakeholders may find themselves in—in the vignettes we use as prompts to crowds and large language models . Problematic AI behaviors can correspond directly to model errors such as false positives and false negatives . However , not all errors are equal and factors such as how often they occur and how egregious they are can also lead to different outcomes . We discuss ways in which the vignettes can operationalize these differences in Section 4 . 1 . Vignette Design ( Matrix Cells ) . Given an AI use scenario , AHA ! generates vignettes for each stakeholder ( row ) and problematic behavior ( column ) combination . We designed the vignettes to provide context and priming for e . g . , crowdworkers and large language models to complete them with realistic descriptions of harms ( see Figure 1 for an example vignette ) . 1 Drawing on both psychology research showing that people view empathy as cognitively effortful and thus tend to avoid it when possible [ 10 ] and our early experimentation with different vignette designs , 2 we formulated the vignettes from a second - person perspective ( e . g . , imagine you are a [ stakeholder ] ) . Furthermore , for scenarios where the use of the AI systems was ambiguous , we added more explicit language about how the systems’ predictions will be leveraged . For example , in a communication compliance setting where “ the AI system detected toxic language in an email of an employee ” , we specified that the system “ will notify the employee’s manager ” . 3 4 CHARACTERIZING THE HARMS SURFACED BY AHA ! To assess AHA ! ’s capacity to surface meaningful harms , we ran a series of experiments generating and then characterizing harms for five different hypothetical binary classification scenarios inspired by real - world AI incidents [ 42 ] : hiring and loan applications , content moderation and communication compliance , and disease diagnosis . Here , we describe our experiments and present findings from the exploratory data analysis . 1 Since part of our goal was to understand how crowd and GPT - 3 generations differ , to prevent any confounding , we prompted both with the same vignette design . Vignettes can be further tailored for the intended generation source . 2 We experimented with priming for both third - person ( e . g . , imagine Joey is a [ stakeholder ] ) and second - person ( e . g . , imagine you are a [ stakeholder ] ) perspectives , observing that crowdworkers seemed to engage more deeply when the vignettes were written in the second - person perspective . 3 In our pilot experiments , leaving this unspecified resulted in ( both crowdworkers and GPT - 3 ) completing the vignettes with descriptions of possible problematic uses of the AI system rather than descriptions of possible harmful consequences . Manuscript submitted to ACM 6 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi 4 . 1 Experiments We applied AHA ! as we envisioned practitioners would in practice . Specifically , after describing each AI scenario , we used AHA ! to generate a list of relevant stakeholders and populate an ethical matrix with vignettes describing those stakeholders experiencing various problematic AI behaviors we were interested in . We then passed the vignettes to crowdworkers and a language model to complete . Finally , we manually reviewed and coded the harms for further analysis . Generating stakeholders . To generate a range of both direct and indirect stakeholders for each scenario , we constructed a one - shot learning prompt for GPT - 3 . The prompt consisted of a context - specific list of stakeholders we manually defined for the hiring scenario based on consideration of relevant real - world incidents [ 42 ] . It included : the applicant ( for whom the AI system makes its hiring recommendation ) , other applicants and future applicants ( whose job prospects may be impacted by the hiring recommendations for the target applicant or by use of the system ) , the hiring manager ( who would rely on the AI system’s recommendation ) , the tech company deploying the AI system and their HR team , the AI system developers ( who may be accountable for the system’s behavior ) , the applicant’s family / friends , and applicants identifying with various demographic groups ( e . g . , racial , ethnic , or gender minorities and intersections ) . Given this prompt , AHA ! then generated stakeholders for the other four scenarios , which we then refined before proceeding to the next stage . Selecting problematic AI behaviors . Because we were interested in studying the impact of various problematic AI behaviors on what harms AHA ! surfaces , we considered behaviors spanning several dimensions : – False - positive / False - negative : whether the system predicts an outcome when it should not or does not predict an outcome when it should . We distinguished between these errors because they often result in different real - world costs . In the hiring scenario , for example , if a system determines an applicant is qualified for a position when they are not , the company may be harmed by the applicant not being able to perform their duties . If the system determines an applicant is not qualified when they are , the tech company may miss an opportunity to hire an appropriate candidate . – One - time / Accumulated : whether the system makes a one - time error of that type or that error is made repeatedly or systematically over time . For example , a one - time false positive in the hiring scenario may go unnoticed but repeated hiring of unqualified candidates may reflect poorly on a hiring manager . – Egregious / Unspecified : whether the system makes a severe error or an error of unspecified severity . Again in the hiring scenario , if the top candidate in a field is deemed unqualified for a tech company working in that field , the company may lose out on an opportunity to lead the field in that area . – Specified - / Unspecified - harm : whether the vignette is conditioned on a specific harm ( e . g . , financial strain or emotional distress ) or the harm is not - specified . For example , in the hiring scenario , the generated vignette might probe on financial strain by modifying the " you may be harmed because . . . " clause to " you may experience financial strain because . . . " For these columns , practitioners using AHA ! can specify harms they deem important for their scenario . AHA ! ’s ethical matrix thus included 2 4 ( sixteen ) types of problematic behaviors ( columns ) per scenario . Completing vignettes with crowdsourcing . We recruited crowd judges from the Clickworker crowdsourcing platform ( www . clickworker . com ) in June - August 2022 . Judges were first presented with a consent form before proceeding with the task . Each judge was then presented with four distinct vignettes from a single scenario , but for different stakeholders and problematic system behaviors . 4 Vignettes were selected randomly from that scenario’s corresponding ethical matrix until each vignette was filled out by three judges . After completing the task , judges were also prompted with demographic and 4 We did so as in our pilot experiments , when prompted with multiple vignettes , the crowdworkers provided more diverse responses . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 7 background questions . Figure 2 contains the example of a crowdsourcing task . Across all scenarios , we employed 105 English speaking judges from North America . For each scenario , we capped the number of HITs per judge to 5 . Fig . 2 . A sample crowdsourcing task . To remove spammers and under - performing judges ( and re - judge corresponding HITs ) , we used a mix of manual checks , speed checks , and attention checks . For manual checks , one of the authors read through the responses and flagged any gibberish ( e . g . " abfbaufue " ) or absolutely irrelevant text ( e . g . random concept definition copied directly from the Wikipedia ) . For speed checks , we determined those who submitted the task under five seconds to be spammers because this amount of time was too short to produce any meaningful open - ended answers . For attention checks , we incorporated a brief questions at the end of the task ( " What is the color of the sky ? " ) . We hypothesized that people who gave random answer like " green " were not paying full attention . On average , we flagged and requested re - judgments of ≈ 40 % of responses to each task . Judges were paid for all HITs they performed , even when identified as spammers , paying on average about $ 15 ( USD ) per hour . In total , 98 trusted judges participated in our tasks . 5 This crowdsourcing study was approved by our institution’s IRB . Completing vignettes with a large language model . We also experimented with generating examples of harm for each of our five scenarios by prompting GPT - 3 ( davinci model with temperature set to 0 . 95 ) . For each scenario , we 5 The judges participating in our experiments span a range of self - reported ages ( 18 - 25 : 17 % , 25 - 39 : 49 % , 40 - 64 : 31 % , 65 + : 2 % ) , familiarity with similar AI systems ( with over 50 % of those reporting this saying to be slightly familiar or not familiar at all ) , and whether they believe to have experienced adverse impacts due to AI systems ( with only 15 % of those reporting this saying to have experienced adverse impacts from AI systems ) Manuscript submitted to ACM 8 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi other well−being social−societalrepresentational quality−of−serviceloss−of−rightslegal−reputationalallocational Comm . compl . Content moder . Disease diag . Hiring Loan appl . 0 100 200 300 400 500 Freq Comm . compl . Contentmoder . Diseasediag . Hiring Contentmoder . . 71 ( n . s . ) - - - Diseasediag . . 71 ( n . s . ) . 31 ( n . s . ) - - Hiring < . 0001 < . 0001 < . 0001 - Loanapplic . < . 0001 < . 0001 < . 0001 . 02 Fig . 3 . Distribution of harm categories across scenarios . p - values for χ 2 pairwise comparisons with Holm - Bonferroni corrections . populated AHA ! ’s ethical matrix cells by prompting the model with the scenario description , the list of stakeholders , a few scenario - specific example harms for few - shot learning which we wrote by hand for random stakeholder - problematic AI behavior combinations ( as we envision practitioners could do , e . g . , for more obvious stakeholders ) , and the vignette to be completed . For each vignette ( cell ) we generated three different completions . 6 Coding the examples of harm . We obtained a total of 4113 generations across all five scenarios . To characterize the examples of harm generated by AHA ! , we followed a thematic analysis approach [ 9 , 22 , 52 ] . First , one of the authors read the generations and open coded them to capture the types of harms produced . Another author then reviewed the preliminary codes and iteratively aggregated them into broader harm taxonomy . Throughout the process , both annotators regularly compared annotations between different scenarios until categories stabilized . This process resulted in over 50 unique codes , grouped into 8 high - level categories : 7 1 ) disparities in quality - of - service , 2 ) representational harms , 3 ) harms affecting people’s well - being , 4 ) legal and reputation harms , 5 ) allocational harms , 6 ) loss of rights or agency , 7 ) other social and societal harms , as well as 8 ) a catch - all category for harms not belonging in any other category . See Appendix A . 3 for a detailed overview of the resulting harm taxonomy . We refer to the higher - level harms ( e . g . , allocational ) as harm categories and lower - level codes ( e . g . , opportunity loss ) as harm subcategories . 4 . 2 RQ1 : Does AHA ! generate meaningful examples of possible harms ? We consider a harm meaningful if it is not nonsense , is an actual harmful outcome , and is relevant to the scenario in question . To understand whether AHA ! surfaces meaningful harms , we first look at the prevalence of generations coded as not meaningful . Only 7 % ( 288 ) of the examples elicited from crowds and GPT - 3 were coded as not meaningful ( 63 . 2 % from crowds ) . Of these , 86 . 4 % were nonsensical ( not clear or does not make sense in context ) while the remaining were not a harm ( sensible but does not represent an actual harm to a stakeholder ) . To understand whether AHA ! ’s surfaces harms relevant to the given deployment scenarios , we look at whether the harms differ across scenarios in a meaningful way . That is , we would expect different scenarios to generate different harm categories ( e . g . , the hiring scenario could be considered quite different than the content moderation scenario in terms of stakes and context ) and more similar scenarios to generate similar harm categories ( e . g . , the hiring and loan application scenarios are similar in that they are both instances of a resource allocation problem , while the content moderation and communication compliance scenarios are similar in that they are both instances of online content moderation policy enforcement ) . To test this hypothesis , we a ran Chi - square ( χ 2 ) analysis on the distribution of harm generated and grouped into harm categories , followed by post - hoc χ 2 pairwise comparisons of scenarios with Holm - Bonferroni correction to account for multiple comparisons . We found significant differences in the distribution of harm categories across scenarios ( χ 2 6 Through the iterative design of the prompts , we observed that more context about the task ( i . e . , providing the list of stakeholders coupled with the scenario description and the set of examples ) led to more sensible completions from GPT - 3 . 7 A single generation may have multiple codes and / or categories . In our analyses of distributions , we count a generation only once if it has multiple codes from the same high - level category . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 9 Falsepositive / negative Harmspecified / notspecified Accumulated / one - time Egregious / notspecified GPT - 3 / Crowd Scenario N df χ 2 p - value χ 2 p - value χ 2 p - value χ 2 p - value χ 2 p - value Comm . compl . 1000 7 30 . 25 < . 0001 37 . 03 < . 0001 10 . 65 . 15 ( n . s . ) 10 . 57 . 16 ( n . s . ) 32 . 16 < . 0001 Contentmoder . 878 7 28 . 61 < . 0001 35 . 70 < . 0001 7 . 39 . 39 ( n . s . ) 4 . 95 . 67 ( n . s . ) 49 . 42 < . 0001 Diseasediag . 920 7 12 . 36 . 09 112 . 37 < . 0001 8 . 75 . 27 ( n . s . ) 10 . 47 . 16 ( n . s . ) 30 . 46 < . 0001 Hiring 833 6 14 . 33 . 03 58 . 69 < . 0001 5 . 05 . 54 ( n . s . ) 5 . 83 . 44 ( n . s . ) 10 . 66 . 09 Loanapplic . 751 5 6 . 04 . 30 ( n . s . ) 24 . 64 < . 0001 6 . 39 . 27 ( n . s . ) 2 . 67 . 75 ( n . s . ) 30 . 04 < . 0001 Table 1 . χ 2 statistics when comparing the distributions of harm categories across different experimental conditions . 8 −4 1 −1 2 −5 −4 4 −7 −7 1 2 2 −2 1 10 −2 −2 0 0 −4 0 4 4 −3 4 1 1 4 −1 −3 −1 4 −1 2 0 5 −9 −2 1 −9 −7 0 −3 9 4 −1 6 4 −4 −1 −2 5 0 −1 −2 −1 0 2 1 −3 1 −1 1 −1 6 0 −1 2 −3 −2 −12 −22 0 0 22 −1 12 5 1 −1 −1 −3 0 0 −4 −2 0 5 0 2 5 3 −1 −6 1 −2 23 −2 0 −7 −2 −12 2 3 1 −3 0 −2 0 0 0 −1 −2 2 6 0 0 −1 −1 −4 17 −4 −3 −4 −2 −4 −3 1 2 3 −2 −2 5 −2 −1 −2 1 −1 Comm . compl . Content moder . Disease diag . Hiring Loan appl . F P vs . F N H a r m − s pe c . vs . no t A cc u m u l . vs . one − t i m e E g r eg . vs . no t −20 −10 0 10 20 −20 −10 0 10 20 −20 −10 0 10 20 −20 −10 0 10 20 −20 −10 0 10 20 other well−being social−societal representational quality−of−service loss−of−rights / agency legal−reputationalallocational other well−being social−societal representational quality−of−service loss−of−rights / agency legal−reputationalallocational other well−being social−societal representational quality−of−service loss−of−rights / agency legal−reputationalallocational other well−being social−societal representational quality−of−service loss−of−rights / agency legal−reputationalallocational percentage point difference Fig . 4 . Percentage point differences in the fraction of different harm categories when contrasting AI behaviors across scenarios . When specifying the harm , we experimented with different harms across scenarios : emotional distress for communication compliance and content moderation , financial concerns for hiring and loan application , health concerns for disease diagnosis . 1 . 4 −5 −5 . 87 5 . 64 −3 . 83 −1 . 89 6 . 25 5 . 74 4 . 13 −2 . 49 1 3 . 32 0 . 55 2 . 6 0 . 29 −2 . 92 −11 . 21 −5 . 65 −0 . 27 1 . 3 −3 . 85 5 . 44 4 . 75 2 . 3 −1 . 81 −3 . 11 0 . 43 1 . 22 −2 . 42 −1 . 1 −9 . 05 −0 . 15 6 . 09 −4 . 84 −3 −2 . 14 0 . 14 6 . 38 2 . 39 0 . 49 6 . 94 3 . 06 allocational legal / reputational loss of rights / agency quality of service representat . social / societal well−being other −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 −12 −8 −4 0 4 All scenariosLoan appl . Hiring Disease diag . Content moder . Comm . compl . percentage point difference ( Crowd vs . GPT−3 ) Fig . 5 . Distribution of harms surfaced by Crowd vs . GPT - 3 . Positive values indicate that Crowd , and negative values indicate that GPT - 3 generated more examples of harms . ( 28 , N = 4382 ) = 1577 , p < . 0001 ) . Our results also show that the distribution of harm categories was more similar for similar scenarios ( Figure 3 ) . For example , both the hiring and loan application scenarios , where the AI system’s role is to distribute benefits to decision - subjects , surfaced mainly allocational harms . 8 Whereas , the communication compliance and content moderation scenarios , where the AI system’s decision can result in punitive outcomes for decision - subjects , surfaced more legal and reputational harms . Finally , for the disease diagnosis scenario , where the AI’s decision can impact one’s well - being , the harms concentrated more around quality of service . 9 8 In our taxonomy , allocational harms are defined as : when the AI system’s output affects the allocation of resources or opportunities relating to finance , education , employment , healthcare , housing , insurance , or social welfare . 9 In our taxonomy , quality of service harms are defined as : when either the AI system or a service the AI system is used for does not work equally well for different individuals or groups or doesn’t work as intended . Manuscript submitted to ACM 10 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi 19 . 2 13 . 7 14 . 1 18 . 5 13 . 9 12 . 9 11 8 . 2 7 . 6 14 . 2 11 . 1 10 . 9 13 . 7 9 10 . 3 Comm . compl . Content moder . Disease diag . Hiring Loan appl . 0 5 10 15 20 U n i que s ub c a t egego r i e s pe r s t a k eho l de r ( m ean ) source GPT−3 Crowd Crowd + GPT−3 Crowd & GPT - 3 Comm . compl . backlash , badactors , bannedfromsite , creatingbusywork , distrustandreputationaldamage , economicstrain , erodingrelationships , feelingbadforothers , job security , legal repercussions , liability , loss ofagency , lossof motivation , lossof privacy , lossof rights , lost content , mental health , opportunity loss , physical health , productivityloss , reinforcedstereotype , safety , scapegoat , self - doubt , snowballeffect , targetoftoxiclanguage , toxicenvironment , unaware , underperformingAI , underspecifiedorrepeatedharm , unfairtreatment , waste , worksatisfaction / fit Contentmod - erat . backlash , badactors , bannedfromsite , creatingbusywork , distrustandreputationaldamage , economicstrain , erodingrelationships , feelingbadforothers , inadequate service , jobsecurity , lackofaccesstoinformation , legalrepercussions , liability , lossofagency , lossofmotivation , lossofprivacy , lossofrights , lostcontent , mental health , productivity loss , publichealth , opportunityloss , reinforced stereotype , safety , scapegoat , self - doubt , social issues , snowball effect , target oftoxic language , toxicenvironment , unaware , underperformingAI , underspecifiedorrepeatedharm , unfairtreatment , waste , worksatisfaction / fit Diseasediag . . badactors , creatingbusywork , commiseration , distrustandreputationaldamage , economicstrain , erodingrelationships , inadequateservice , jobsecurity , legal repercussions , liability , lossofagency , lossofprivacy , mentalhealth , physicalhealth , publichealth , qualityoflife , reinforcedstereotype , scapegoat , snowballeffect , underperformingAI , underspecifiedorrepeatedharm , unfairtreatment , waste Hiring . bad actors , creating busywork , commiseration , distrust and reputational damage , economic strain , eroding relationships , job security , legal repercussions , liability , loss of motivation , mental health , opportunity loss , productivity loss , reinforced stereotype , scapegoat , self - doubt , snowball effect , toxic environment , underperformingAI , underspecifiedorrepeatedharm , unfairtreatment , waste , worksatisfaction / fit Loan appl . . badactors , creatingbusywork , commiseration , distrustandreputationaldamage , economicstrain , erodingrelationships , incapability , jobsecurity , legalrepercussions , liability , mentalhealth , opportunityloss , productivityloss , qualityoflife , scapegoat , self - doubt , snowballeffect , socialissues , underperformingAI , underspecified orrepeatedharm , unfairtreatment , waste Fig . 6 . Unique harm subcategories covered by the examples obtained for each scenario with Crowd and GPT - 3 . The top plot shows the means of unique number of subcategories per stakeholder generated by Crowd only , GPT - 3 only , and Combined ( Crowd + GPT - 3 ) . Error bars indicate one standard error . ( See Table 2 for significance tests . ) 4 . 3 RQ2 : Does varying the dimensions of problematic AI behaviors impact the harms AHA ! surfaces ? We manipulated various dimensions of problematic AI behaviors— false positive / negative , accumulated / one - time , egregious / not - specified , or harm specified / not - specified —in our experiments to understand their impact on harms gener - ated . If priming on a behavioral dimension impacted harms surfaced by AHA ! , we would expect to see differences in the types of harms generated across that dimension ( e . g . , between false positive vs false negative harms ) . To test this , we again ran Chi - square ( χ 2 ) analyses on the distribution of harm categories conditioned on each behavioral dimension . Our results show that priming with false positives vs . false negatives behaviors resulted in significantly or marginally significantly different distributions of harm categories in each scenario , except for the loan application scenario ( see Table 1 ) . In the communication compliance and content moderation scenarios , for instance , false negatives ( failure to detect toxic content ) resulted in more examples of representational harms , while false positives ( erroneously detecting that someone used toxic language ) resulted in more examples of allocational harms ( see Figure 4 ) . We also saw that priming about specific harms that practitioners may deem important for their scenario ( e . g . , you may experience emotional distress because . . . ) indeed helped surface more examples of those ( or related ) harms ( see Figure 4 ) . For instance , we observed that priming about financial concerns resulted in a significantly higher prevalence of examples about allocational harms in both the hiring and loan application scenarios . We did not see any significant differences in distribution of harms when conditioning problematic behaviors on one - time vs . accumulated or egregious vs . unspecified . This could either be because our manipulations were not salient enough to elicit different harms for those dimensions or because those dimensions might not result in meaningfully different types of harm . For example , when varying the one - time vs accumulated dimension for users of a social media platform in the content moderation scenario , AHA ! wrote " If the system determines a post contains toxic language when it does not , you may be harmed because . . " in the one - time variation and " If the system determines posts contain toxic language when they do not , you may be harmed because . . " in the accumulated variation . Future work might experiment with increasing the saliency of this dimension ( e . g . , " If the system often determines posts contain toxic language when they do not . . " ) . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 11 4 . 4 RQ3 : Does eliciting harms from crowds vs . GPT - 3 impact the harms generated ? AHA ! generates vignettes that set up fictional scenarios to be completed with possible harms . In our experiments , we tried prompting crowdworkers and GPT - 3 to complete these vignettes to understand the trade - offs between these approaches . To examine the trade - offs , we first compared the distribution of harm categories surfaced by crowds versus GPT - 3 as before with Chi - square ( χ 2 ) analyses ( see Table 1 ) . We found significant differences in the distribution of harms across harm categories between crowds vs . GPT - 3 in all scenarios , with the exception of the hiring scenario where we found marginally significant differences . When examining these differences ( see Figure 5 ) , we see that overall GPT - 3 generated more examples of e . g . , quality of service , allocational , well - being harms , while the crowds provided more examples of e . g . , legal & reputational and representational harms . Next , we wanted to compare crowds vs . GPT - 3 in terms of the diversity of harms surfaced by each approach . As a proxy for diversity , we look at the total number of unique harm subcategories each approach surfaced across their stakeholder groups . That is , because the same harm subcategory can have widely different consequences depending on which stakeholder they affect ( e . g . , reputational harms towards the whole medical community vs . a single doctor have vastly different impacts ) we count these separately . 10 We conducted paired t - tests comparing the mean number of unique subcategories of harms generated by Crowds vs GPT - 3 and between Crowds - only , GPT - 3 - only , and their Combination ( Crowd + GPT - 3 ) ( see Table 2 ) . We found that while using either only crowds or only GPT - 3 resulted in a comparable number of unique harm subcategories , together they produce a more comprehensive set of subcategories in all scenarios which are both significantly larger and significantly more diverse than either GPT - 3 or crowd alone ( see Figure 6 ) . Scenario Crowdmean ( standarderror ) GPT - 3mean ( standarderror ) Crowd + GPT - 3 ( standarderror ) Crowdvs . GPT - 3 Crowdvs . Combined GPT - 3vs . Combined Comm . compl . 13 . 67 ( 0 . 83 ) 14 . 08 ( 0 . 68 ) 19 . 17 ( 0 . 93 ) t ( 11 ) = - 0 . 5p = . 63 ( n . s . ) t ( 11 ) = - 11p < . 0001 t ( 11 ) = - 8 . 53p < . 0001 Contentmoder . 13 . 92 ( 0 . 72 ) 12 . 92 ( 0 . 92 ) 18 . 46 ( 0 . 97 ) t ( 12 ) = 1 . 85p = . 13 ( n . s . ) t ( 12 ) = - 8 . 42p < . 0001 t ( 12 ) = - 10 . 29p < . 0001 Diseasediag . 8 . 23 ( 0 . 40 ) 7 . 65 ( 0 . 42 ) 11 . 00 ( 0 . 45 ) t ( 16 ) = 1 . 61p = . 13 ( n . s . ) t ( 16 ) = - 8 . 76p < . 0001 t ( 16 ) = - 10 . 87p < . 0001 Hiring 11 . 09 ( 0 . 46 ) 10 . 91 ( 0 . 64 ) 13 . 58 ( 0 . 40 ) t ( 10 ) = 0 . 27p = . 79 ( n . s . ) t ( 10 ) = - 6 . 49p < . 0001 t ( 10 ) = - 9 . 11p < . 0001 Loanapplic . 9 ( 0 . 38 ) 10 . 27 ( 0 . 56 ) 13 . 72 ( 0 . 51 ) t ( 10 ) = - 2 . 05p = . 07 ( n . s . ) t ( 10 ) = - 11 . 63p < . 0001 t ( 10 ) = - 8 . 37p < . 0001 Table 2 . Paired t - test comparing the mean number of unique subcategories of harms generated by Crowd only , GPT - 3 only , and Combined ( Crowd + GPT - 3 ) . For all the scenarios , Crowd and GPT - 3 combined generate significantly more unique subcategories together than either alone . 5 PRACTITIONER PERSPECTIVES ON AHA ! To assess the potential practical utility of AHA ! , we conducted semi - structured interviews with industry practitioners and academics with responsible AI expertise . This section describes our IRB - approved study and findings . 5 . 1 Interview Study Participants . We reached out to 20 practitioners and academics with responsible AI experience ( e . g . , experience conducting or reviewing AI product impact assessments ) or expertise from our professional networks and through snowball sampling . Nine people accepted our invitation , including seven from a large technology company and two academics from different institutions . Participants covered four professional roles : Project Managers [ P1 , P7 ] —project managers with specific training in responsible AI concepts and best practices and with experience helping development 10 Note that this may still be considered a lower bound measure of diversity because two harms with the same subcategory for the same stakeholder could also represent different consequences ( e . g . , “Then despite my best efforts , I might then be relieved soon of duty , and would then have a difficult practical time of it to gain / regain unemployment benefits from my state , when it might have been better that no hiring of me happened in the first place due to the faulty AI” and “your salary might be lower than it should be” ) . Manuscript submitted to ACM 12 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi teams conduct impact assessments , Responsible AI Practitioners [ P4 , P5 , P6 ] —practitioners whose work involves researching , guiding , or developing responsible AI tools and practices , Responsible AI Policy [ P8 , P9 ] —practitioners responsible for development of and compliance with responsible AI policies , and Responsible AI Academics [ P2 , P3 ] —university professors who study responsible AI challenges and solutions . Protocol . Before each interview , we obtained informed consent from participants and asked for their permission to record the session . Interviews were conducted via a video call platform . We started each interview with an overview of the project and study protocol . Each interview then consisted of two tasks featuring two distinct AI deployment scenarios randomly chosen for each participant from our five original scenarios . For each task , participants were first presented with an AI deployment scenario and then asked to freely brainstorm possible harms . They were then shown the harms surfaced by AHA ! for that scenario via a simple interactive tool we designed to help them navigate the results ( see Appendix A . 5 ) . The tool grouped harms by stakeholder and harm categories and subcategories . Participants could explore harms by filtering them by stakeholder or expanding harm categories to reveal individual harms . We asked participants to think aloud throughout the interview and reflect on anything notable about specific harms AHA ! surfaced . We also asked for participants’ reflections on AHA ! ’s overall approach and their thoughts on its potential practical utility . Each interview lasted about 50 minutes with task 1 taking most of the time ( typically 30 - 35 minutes ) while task 2 was used solely as an overview of a different scenario for the remainder of the time ( about 5 - 7 minutes ) . Qualitative analysis . To analyze the interview data , we used a bottom - up thematic analysis approach [ 68 ] . First , we reviewed transcripts and videos highlighting quotes relevant to our study goals of understanding participant reactions to AHA ! ’s generated harms , overall approach , and practical utility . We then thematically sorted excerpts to identify themes . 5 . 2 R4 : Do experienced professionals see value in AHA ! for helping teams anticipate harms in practice ? AHA ! surfaced sensible harms participants believed they otherwise would not have thought of . All our participants pointed out harms surfaced by AHA ! that they found sensible but unlikely to have thought of on their own . When reviewing harms towards direct stakeholders ( e . g . , decision subjects ) in the communication compliance ( in the workplace ) scenario , P8 said “ [ What ] was surprising to me was loss of motivation . [ There was ] an article from the New York Times [ about workplace monitoring ] . [ T ] his is just a form of workplace monitoring , right ? [ T ] he article is more about like productivity [ and ] tracking how much you’re like online . [ This is ] kind of a different version of that too . ” In another example , P2 commented about mental health related harms towards applicants in the loan application scenario , saying “Mental health , that’s surprising to me because that’s one I think of as a risk for other applications , particularly in the online setting and I guess here [ in ] a loan application scenario [ . . ] it’s more of a second order effect from having a loan denied [ . . ] that’s one I wouldn’t have immediately thought of in this context . ” Similarly , when reviewing possible harms towards indirect stakeholders in the content moderation scenario , P6 remarked “I would not have thought about content moderators . And I should have thought about them because I know that there’s a lot of stress , it’s a very hard job . ” In another example from this scenario , P3 said “ [ I ] t’s interesting how people think that AI system developers could be fired for the performance of the systems” when reviewing possible harms surfaced towards system developers . Although all participants pointed out surprising harms they considered sensible , some also noted harms they believed to be out of scope . When examining emotional harms that applicants could face in the hiring scenario , P3 said “emotional kind of harm , that I wouldn’t necessarily associate with the system , but just the nature of hiring processes . ” Another participant argued that harms caused by misusing AI systems are different than downstream harms from intended uses . Specifically , in the context of discussing harms of misuse surfaced by AHA ! in the communication compliance scenario Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 13 ( e . g . , “ [ s ] omebody has used someone else’s laptop or their e - mail account to send [ toxic ] emails in order to try to get them fired” [ P4 ] ) , P4 said “ [ T ] here’s appropriate use of the system [ . . ] There is unsupported use where the system wasn’t designed to work this way [ . . ] And then there is misuse , [ . . ] somebody is actively going into this with like adversarial malicious intent [ . . ] I think that is distinct from sort of the downstream harms that might occur . ” Participants noted that AHA ! ’s systematic approach was important for considering a broad range of harms . Several participants commented that AHA ! ’s systematic approach was important for increasing the coverage of harms considered when developing AI systems . P7 , a PM with experience helping product teams consider the potential impact of their AI systems , said “One of the challenges we have is that it is not clear how well we have covered all of the potential harms . [ S ] o it is regularly the case that we’ll go through impact review , maybe some members of the V team aren’t there because they had conflicts . Maybe people are just having an off day and they’re not being very creative or thoughtful [ . . ] there are all these human opportunities for error , so having a tool in place that helps us to be more systematic is really essential . ” Three participants highlighted specific components of AHA ! as impactful . P2 called out the value of thinking broadly about stakeholders when examining harms : “Other applicants is one that’s very easily forgotten about [ in the hiring scenario ] . And so that’s nice that that’s there as a reminder . And family and friends of the applicant , that’s probably one that’s overlooked . ” P5 stressed the benefits of drawing attention to different problematic behaviors : “in [ the communication compliance ] scenario , I think it’s easy to think about harms that can arise with false positives , because there are all these reputational damages and potential damages to job security . But thinking about false negative harms may be a little bit more challenging . ” Others noted the value of connecting stakeholders to system behaviors , with P1 saying “ [ it ] has a really good structure and having just read a very structured approach to generating harms made it much easier to repeat that [ like ] starting with stakeholders , listing types of harms and then connecting types of harms to stakeholders . ” Interestingly , before participants first saw AHA ! ’s results , most appeared to take some kind of structured approach when asked to brainstorm harms on their own . P5 , P6 , and P7 began listing harms that arise due to problematic AI behaviors such as false positives and false negatives , while P8 and P9 spoke to problematic AI behaviors in connection with stakeholders . While this might be an artifact of their prior training or experience , it also provides some face validity to the benefits of AHA ! ’s approach given that all of our participants had some level of Responsible AI expertise . Of the remaining participants , P1 , P2 , and P4 , approached harm anticipation by considering violations to common responsible AI desiderata ( e . g . , privacy , fairness ) while P3 started with a high - level taxonomy of harms they tried to map to the deployment scenario they were presented with . This could be considered a top - down approach to anticipating harms , whereas AHA ! generates descriptions of harms which can then be grouped into harm categories bottom - up . However , the AHA ! ’s ethical matrix could be extended to condition on responsible AI desiderata or harm taxonomies , similar to how we incorporated harm specifications in our experiments . Participants differed in their opinions about when and how AHA ! should or could be used in practice . Participants who had experienced teams struggling with conducting impact assessments commented on the benefits of using a tool like AHA ! as a starting point . P5 noted that “ [ product ] teams are struggling with starting from basically zero and do not have any training in responsible AI to anticipate these types of harms , ” adding that a tool like this “would give them a place to start . ” Similarly , P8 commented that “this would be better for like the teams who are actually filling out the impact assessment [ like ] kind of [ a ] starting place for brainstorming . ” Other participants recommended that a tool like AHA ! should be used during brainstorming to broaden perspectives and coverage of harms . P1 commented that “I think it’s useful both as like a cross check . Is there anything that I missed [ sic ] general brainstorm that can then be taken to a deeper consideration of different kinds of harms . Also definitely useful Manuscript submitted to ACM 14 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi to get additional perspectives here , thoughts about harms , especially for people who are maybe approaching this kind of thought process for the first time . ” P6 similarly remarked that teams could use AHA ! “when they have that brainstorming block [ and ] can’t think of any other things [ . . ] helping with broadening that brainstorming field . ” Still others advocated for using tools like AHA ! only after teams deeply engage with the task themselves . P4 said “I would want to continue using this as sort of that secondary check because I do like forcing teams into some open world envisioning and before they have a framework that they can just lean on and check boxes and look at specific things and call it a day . ” Similarly , P8 pondered about whether “it is better to have a completed impact assessment where somebody has copy - pasted [ from the generations ] or have practitioners think through these things and learn to flex this muscle . ” Participants suggested various ways that AHA ! could or should inform development decisions . Particpants recognized ways in which AHA ! could already inform development decision - making . P5 noted that looking at the harms caused by different problematic AI behaviors might directly imply some mitigations : “maybe this tool can help people think about [ . . ] what kind of mistakes lead to more harms ? Is it false positives or is it false negatives ? So that [ it ] can help developers [ . . ] focus their attention on making specific improvements to the model . ” Other participants saw the comprehensive set of harms produced by AHA ! as providing convincing evidence that could encourage leaders and decision - makers to exercise caution or better resource mitigation work . P4 noted that “ [ W ] here I see this being useful is if I need to convince someone . Basically , you need to be really worried about this because look at all of the things that might go wrong that we feel really uncomfortable with having a long laundry list of things can somewhat help with landing that message . ” Similarly , P3 said “OK well I’m creating this system . How big of a trouble am I in ? Let me quantify it [ and then ] talk to an executive and say Hi , look at this like we’ve got 700 issues on the legal consequences of this [ . . ] maybe the executives will say yes , I will give you extra budget for the legal team for this project . ” Other participants wanted more explicit support for mitigations . P9 remarked “it’s a really hard thing to assess severity [ so ] some indication of the severity of harm [ . . ] would be really useful [ I’d like to ] click on a harm and have a suggested mitigation . ” They further elaborated that “we’re asking our product teams to make choices about which harms they mitigate based on limited resources and [ . . ] need more help on like is this above the line or below the line ? It’s great to know this stuff will happen , but what are we willing to sign up to mitigate ? ” Similarly , P5 said “ [ W ] hen people look at harms , they also want to think about whether there is an opportunity for mitigation . ” 6 DISCUSSION & FUTURE WORK Trade - offs between a harm ideation problem versus a potentially demanding review problem . AHA ! was designed to support practitioners tasked with anticipating a wide range of downstream harms , a task responsible AI proponents advocate for to mitigate the negative impact of AI - based systems to people and society [ 8 , 52 , 66 ] . Our experiments and interview study show that AHA ! can help surface a wide range of meaningful harms that even experienced responsible AI practitioners believed they would have likely overlooked . However , several practitioners were also concerned that AHA ! may be shifting the problem from thinking of harms to reviewing a large number of potentially noisy examples . P8 remarked “If I put on my value sensitive design kind of hat , yeah , absolutely [ . . ] let’s think about and legitimate all these stakeholders . When I think about like doing this as a practitioner [ . . ] the flip side is with a tool like this you might generate so many different harms that the work is actually going through and filtering out what are the actionable harms ? ” While we believe AHA ! can be extended to help prioritize the harms it helps surface ( e . g . , by recruiting crowd judges to rate the potential severity of the harm examples as a subsequent stage ) , we recommend caution when automating value - laden decisions such as what harms to prioritize mitigating . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 15 Trade - offs between offloading harm anticipation to a tool like AHA ! versus encouraging practitioners to deeply engage in the task . Several of our study participants expressed concerns that a tool like AHA ! could in fact encourage practitioners to sidestep the task of examining the impact of their technologies or could disincentivize them from learning to consider the consequences of their technologies . We acknowledge this trade - off , and recommend AHA ! be primarily used as a secondary check to first allow on - the - ground practitioners the opportunity to “learn to flex this muscle” [ P8 ] , as some of our participants suggested . Future work may also consider using AHA ! as an educational tool or to grow a repository of harms that arise across various AI deployment scenarios for knowledge sharing and further study . P9 emphasized “nobody should be starting from scratch with an impact assessment [ . . ] we should be building on each other’s learning and understanding what’s different about the nuances of my scenario or my system , not the things that we already know . You know , what’s the delta from what we do know ? ” Tradeoffs between decoupling versus tying harm anticipation to mitigations . Participants , particularly those tasked with implementing responsible AI mitigations , wanted more guidance on actionability , with P4 asking “if I’m approaching this from a product perspective and I’m figuring out how do I actually build this system responsibly , what are the things I need to mitigate ? What do I do with this information ? ” While we believe AHA ! can be extended to provide more guidance on actionability , we argue that decoupling harm anticipation from harm mitigation is critical for several reasons . First , research on effective brainstorming strategies emphasize the benefits of divergence before convergence [ 15 ] . We took this approach with AHA ! to encourage open and unrestricted consideration of a wide range of harms before fixating on any particular problem or solution . Some participants indicated they appreciated this separation . P8—a responsible AI policy practitioner with extensive experience reviewing and guiding teams on impact assessments—noted “ [ teams ] tend to be really focused on problem - solving . So a lot of times , the harm comes with the solution [ . . ] Our process is like trying to decouple those things , right ? Like first you think about all the things that can go wrong and then eventually you move toward , how do you fix this ? ” Second , while automating part of the generation of possible harms can increase coverage of harm considerations , we believe the community should exercise caution when attempting to automate value - laden decision - making such as harm prioritization . P6 , for example , pushed back on the idea of assigning severity scores to harms asking “who assigns the severity scores” and “whose values and whose perspective are reflected [ by the severity scores ] . ” Finally , knowledge for how to mitigate the harms surfaced by AHA ! may require deep organizational , technical , and regulatory expertise , which remains difficult to reliably provide especially in an semi - automated fashion or at scale . Tradeoffs when eliciting harms from crowd judges and GPT - 3 . In its’ current implementation , AHA ! elicits examples of harm from both crowd judges and GPT - 3 to uncover a wider range of issues by providing perspectives beyond those held by often homogeneous AI teams [ 3 , 8 , 55 ] and make it easier to conduct impact assessments . While in our experiments , using both crowd judges and GPT - 3 overall resulted in more diverse and comprehensive examples of harm ( Section 4 . 4 ) , and while some participants recognized that trying to provide more diverse perspectives is “the right way of thinking about how to develop [ tools like AHA ! ] ” [ P2 ] , the examples from both the crowd judges and GPT - 3 may still reflect the same ‘status quo’ and may surface harmful biases [ 71 ] . For instance , crowd judges heavily skew towards certain demographics ( young and white ) [ 23 ] , while LLMs were found to produce language that stereotypes , demeans , and erases groups , individuals , or their experiences [ 6 , 20 , 27 , 63 , 72 ] . To mitigate some of these , future work could draw on creativity research [ 61 , 64 ] and , for instance , explore different approaches to eliciting descriptions of possible harms by priming crowd judges ( and LLMs ) with different types of harm examples to boost or guide their imagination . Furthermore , AHA ! can also be extended to include other ways to complete the vignettes e . g . , by having the responsible AI practitioners Manuscript submitted to ACM 16 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi complete the vignettes by themselves , or by recruiting diverse participants representing relevant stakeholders in a given scenario . 6 . 1 Limitations & Ethical Considerations While our goal is to facilitate ethical reflection , as we discussed in the previous sections , tools like AHA ! also come with risks , especially due to how they might be used ( e . g . , people might over rely on AHA ! to superficially comply with responsible AI requirements ) , or due to how the examples of harm were obtained ( e . g . , they might surface harmful biases encoded by LLMs or held by crowd judges ) . Furthermore , in its’ current instantiation , AHA ! is designed to surface harms dues to problematic AI behaviors . However , harms may arise not only due to unintended , problematic AI behaviours ; but also due to intended uses particularly when AI systems are employed for tasks that are conceptually or practically impossible ( e . g . , predicting criminality from physical appearance ) [ 58 ] , or due to malicious uses such as employing AI systems for undemocratic or fraudulent purposes [ 72 ] . By attempting to formalize and automate parts of the process of ideating about possible harms ( particularly with the use of LLMs ) , our approach might also be critiqued as “algorithmic formalism” [ 25 ] . We , however , envision AHA ! being used more as a secondary check or to support practitioners when they do not know where to start , and we do not recommend using it as standalone to fill out impact assessment—which we believe accounts for its’ limitations and the realities practitioners need to navigate . The design of AHA ! also recognizes that the process of envisioning harms is inherently contextual . REFERENCES [ 1 ] Richard Adams , Sally Weale , and Caelainn Barr . 2020 . A - level results : almost 40 % of teacher assessments in England downgraded . The Guardian ( 2020 ) . https : / / www . theguardian . com / education / 2020 / aug / 13 / almost - 40 - of - english - students - have - a - level - results - downgraded [ 2 ] Anish Agarwal , Abdullah Alomar , and Devavrat Shah . 2022 . On multivariate singular spectrum analysis and its variants . ACM SIGMETRICS Performance Evaluation Review 50 , 1 ( 2022 ) , 79 – 80 . [ 3 ] Stephanie Ballard , Karen M . Chappell , and Kristen Kennedy . 2019 . Judgment Call the Game : Using Value Sensitive Design and Design Fiction to Surface Ethical Concerns Related to Technology . Proceedings of the 2019 on Designing Interactive Systems Conference ( 2019 ) . [ 4 ] Julia Barnett and Nicholas Diakopoulos . 2022 . Crowdsourcing Impacts : Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making . In Proceedings of the 2022 AAAI / ACM Conference on AI , Ethics , and Society . 56 – 67 . [ 5 ] Christine Barter and Emma Renold . 1999 . The use of vignettes in qualitative research . Social research update 25 , 9 ( 1999 ) , 1 – 6 . [ 6 ] Emily M Bender , Timnit Gebru , Angelina McMillan - Major , and Shmargaret Shmitchell . 2021 . On the Dangers of Stochastic Parrots : Can Language Models Be Too Big ? . In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency . 610 – 623 . [ 7 ] Michael S Bernstein , Margaret Levi , David Magnus , Betsy Rajala , Debra Satz , and Charla Waeiss . 2021 . ESR : Ethics and Society Review of Artificial Intelligence Research . arXiv preprint arXiv : 2106 . 11521 ( 2021 ) . [ 8 ] Margarita Boyarskaya , Alexandra Olteanu , and Kate Crawford . 2020 . Overcoming Failures of Imagination in AI Infused System Development and Deployment . arXiv preprint arXiv : 2011 . 13416 ( 2020 ) . [ 9 ] Virginia Braun and Victoria Clarke . 2006 . Using thematic analysis in psychology . Qualitative Research in Psychology 3 ( 2006 ) , 101 – 77 . [ 10 ] C Daryl Cameron , Cendri A Hutcherson , Amanda M Ferguson , Julian A Scheffer , Eliana Hadjiandreou , and Michael Inzlicht . 2019 . Empathy is hard work : People choose to avoid empathy because of its cognitive costs . Journal of Experimental Psychology : General 148 , 6 ( 2019 ) , 962 . [ 11 ] Jack Clark . 2016 . Artificial intelligence has a ‘sea of dudes’ problem . Bloomberg Technology 23 ( 2016 ) . [ 12 ] Sean Coughlan . 2020 . A - levels and GCSEs : Boris Johnson blames ’mutant algorithm’ for exam fiasco . BBC News ( 2020 ) . https : / / www . bbc . com / news / education - 53923279 [ 13 ] Kate Crawford . 2016 . Artificial intelligence’s white guy problem . The New York Times 25 , 06 ( 2016 ) . [ 14 ] Deloitte . 2020 . Bringing Transparency and Ethics into AI ? ( 2020 ) . https : / / perma . cc / 8LPD - JN74 [ 15 ] Ronald A Finke , Thomas B Ward , and Steven M Smith . 1996 . Creative cognition : Theory , research , and applications . MIT press . [ 16 ] Catherine Fletcher . 2020 . This half - baked fix will leave most UK universities struggling . The Guardian ( 2020 ) . https : / / www . theguardian . com / commentisfree / 2020 / aug / 18 / results - universities - students [ 17 ] Jonas Frich , Lindsay MacDonald Vermeulen , Christian Remy , Michael Mose Biskjaer , and Peter Dalsgaard . 2019 . Mapping the Landscape of Creativity Support Tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 18 . DOI : http : / / dx . doi . org / 10 . 1145 / 3290605 . 3300619 Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 17 [ 18 ] Ian G Johnson and Clara Crivellaro . 2021 . Opening Research Commissioning To Civic Participation : Creating A Community Panel To Review The Social Impact of HCI Research Proposals . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 17 . [ 19 ] Timnit Gebru , Jamie Morgenstern , Briana Vecchione , Jennifer Wortman Vaughan , Hanna M . Wallach , Hal Daumé III , and Kate Crawford . 2018 . Datasheets for Datasets . CoRR abs / 1803 . 09010 ( 2018 ) . http : / / arxiv . org / abs / 1803 . 09010 [ 20 ] SamuelGehman , SuchinGururangan , MaartenSap , YejinChoi , andNoahA . Smith . 2020 . RealToxicityPrompts : EvaluatingNeuralToxicDegeneration in Language Models . In Findings of the Association for Computational Linguistics : EMNLP 2020 . Association for Computational Linguistics , Online , 3356 – 3369 . DOI : http : / / dx . doi . org / 10 . 18653 / v1 / 2020 . findings - emnlp . 301 [ 21 ] Audley Genus and Andy Stirling . 2018 . Collingridge and the dilemma of control : Towards responsible and accountable innovation . Research policy 47 , 1 ( 2018 ) , 61 – 69 . [ 22 ] Graham R . Gibbs . 2007 . Analyzing Qualitative Data . [ 23 ] Joseph K Goodman and Gabriele Paolacci . 2017 . Crowdsourcing consumer research . Journal of Consumer Research 44 , 1 ( 2017 ) , 196 – 210 . [ 24 ] Google . 2020 . Our Principles . ( 2020 ) . https : / / perma . cc / VHZ5 - DJJJ [ 25 ] Ben Green and Salomé Viljoen . 2020 . Algorithmic realism : expanding the boundaries of algorithmic thought . In Proceedings of the 2020 conference on fairness , accountability , and transparency . 19 – 31 . [ 26 ] Nina Grgi´c - Hlaˇca , Christoph Engel , and Krishna P . Gummadi . 2019 . Human Decision Making with Machine Assistance : An Experiment on Bailing and Jailing . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 178 ( nov 2019 ) , 25 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3359280 [ 27 ] Thomas Hartvigsen , Saadia Gabriel , Hamid Palangi , Maarten Sap , Dipankar Ray , and Ece Kamar . 2022 . ToxiGen : A Large - Scale Machine - Generated DatasetforAdversarialandImplicitHateSpeechDetection . In Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics ( Volume 1 : Long Papers ) . Association for Computational Linguistics , Dublin , Ireland , 3309 – 3326 . DOI : http : / / dx . doi . org / 10 . 18653 / v1 / 2022 . acl - long . 234 [ 28 ] César A Hidalgo , Diana Orghian , Jordi Albo Canals , Filipa De Almeida , and Natalia Martín . 2021 . How humans judge machines . [ 29 ] Matthew K . Hong , Adam Fourney , Derek DeBellis , and Saleema Amershi . 2021 . Planning for Natural Language Failures with the AI Playbook . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems ( CHI ’21 ) . Association for Computing Machinery , New York , NY , USA , Article 386 , 11 pages . DOI : http : / / dx . doi . org / 10 . 1145 / 3411764 . 3445735 [ 30 ] John J . Horton . 2022 . Large Language Models as Simulated Economic Agents : What Can We Learn from Homo Silicus ? ( 2022 ) . https : / / john - joseph - horton . com / papers / llm _ ask . pdf [ 31 ] The White House . 2016 . Preparing for the future of artificial intelligence . Executive Office of the President National Science and Technology Council ( 2016 ) . [ 32 ] IBM . 2020 . IBM’s Principles for Trust and Transparency . ( 2020 ) . https : / / perma . cc / 8LPD - JN74 [ 33 ] Maurice Jakesch , Zana Buçinca , Saleema Amershi , and Alexandra Olteanu . 2022 . How Different Groups Prioritize Ethical Values for Responsible AI . arXiv preprint arXiv : 2205 . 07722 ( 2022 ) . [ 34 ] Sarah Janboecke , Diana Löffler , and Marc Hassenzahl . 2020 . Using Experimental Vignettes to Study Early - Stage Automation Adoption . CoRR abs / 2004 . 07032 ( 2020 ) . https : / / arxiv . org / abs / 2004 . 07032 [ 35 ] Daan Kolkman . 2020 . F * * k the algorithm ? : what the world can learn from the UK’s A - level grading fiasco . Impact of Social Sciences Blog ( 2020 ) . [ 36 ] Jill Lawless and Taryn Siegel . 2020 . UK scraps exam grading system that enraged students , parents . AP News ( 2020 ) . https : / / apnews . com / article / virus - outbreak - england - international - news - roger - taylor - london - 7b27d2cfc54c05c9589091fcb91d2771 [ 37 ] Min Kyung Lee , Daniel Kusbit , Anson Kahng , Ji Tae Kim , Xinran Yuan , Allissa Chan , Daniel See , Ritesh Noothigattu , Siheon Lee , Alexandros Psomas , and others . 2019 . WeBuildAI : Participatory framework for algorithmic governance . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) , 1 – 35 . [ 38 ] David Leslie . 2019 . Understanding artificial intelligence ethics and safety . arXiv preprint arXiv : 1906 . 05684 ( 2019 ) . [ 39 ] David Liu , Priyanka Nanayakkara , Sarah Ariyan Sakha , Grace Abuhamad , Su Lin Blodgett , Nicholas Diakopoulos , Jessica R . Hullman , and Tina Eliassi - Rad . 2022 . Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews . In Proceedings of the 2022 AAAI / ACM Conference on AI , Ethics , and Society ( AIES ’22 ) . 424 – 435 . [ 40 ] Todd Lubart . 2001 . Models of the Creative Process : Past , Present and Future . Creativity Research Journal 13 ( 2001 ) , 295 – 308 . [ 41 ] Michael A . Madaio , Luke Stark , Jennifer Wortman Vaughan , and Hanna Wallach . 2020 . Co - Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 14 . DOI : http : / / dx . doi . org / 10 . 1145 / 3313831 . 3376445 [ 42 ] Sean McGregor . 2021 . Preventing repeated real world AI failures by cataloging incidents : The AI incident database . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol . 35 . 15458 – 15463 . [ 43 ] Ben Mepham . 2000 . A Framework for the Ethical Analysis of Novel Foods : The Ethical Matrix . Journal of Agricultural and Environmental Ethics 12 ( 2000 ) , 165 – 176 . [ 44 ] B . Mepham , Matthias Kaiser , Erik Thorstensen , S . Tomkins , and K . Millar . 2006 . Ethical Matrix Manual . Agricultural and Forest Meteorology - AGR FOREST METEOROL ( 01 2006 ) . [ 45 ] Microsoft . Microsoft Responsible AI Standard , v2 . ( ? ? ? ? ) . https : / / blogs . microsoft . com / wp - content / uploads / prod / sites / 5 / 2022 / 06 / Microsoft - Responsible - AI - Standard - v2 - General - Requirements - 3 . pdf [ 46 ] Microsoft . 2020 . Responsible AI . ( 2020 ) . https : / / perma . cc / 7AKE - 3GH3 Manuscript submitted to ACM 18 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi [ 47 ] Margaret Mitchell , Simone Wu , Andrew Zaldivar , Parker Barnes , Lucy Vasserman , Ben Hutchinson , Elena Spitzer , Inioluwa Deborah Raji , and Timnit Gebru . 2018 . Model Cards for Model Reporting . CoRR abs / 1810 . 03993 ( 2018 ) . http : / / arxiv . org / abs / 1810 . 03993 [ 48 ] Brent D . Mittelstadt . 2019 . AI Ethics - Too Principled to Fail ? CoRR abs / 1906 . 06668 ( 2019 ) . http : / / arxiv . org / abs / 1906 . 06668 [ 49 ] University Montreal . 2017 . The Montreal Declaration for a Responsible Development of Artificial Intelligence . ( 2017 ) . https : / / perma . cc / 8LPD - JN74 [ 50 ] Frances C Moore , Katherine Lacasse , Katharine J Mach , Yoon Ah Shin , Louis J Gross , and Brian Beckage . 2022 . Determinants of emissions pathways in the coupled climate – social system . Nature 603 , 7899 ( 2022 ) , 103 – 111 . [ 51 ] Luke Munn . 2022 . The uselessness of AI ethics . AI and Ethics ( 08 2022 ) . DOI : http : / / dx . doi . org / 10 . 1007 / s43681 - 022 - 00209 - w [ 52 ] Priyanka Nanayakkara , Jessica Hullman , and Nicholas Diakopoulos . 2021 . Unpacking the Expressed Consequences of AI Research in Broader Impact Statements . arXiv preprint arXiv : 2105 . 04760 ( 2021 ) . [ 53 ] Bernard A . Nijstad and Wolfgang Stroebe . 2006 . How the Group Affects the Mind : A Cognitive Model of Idea Generation in Groups . Personality and Social Psychology Review 10 ( 2006 ) , 186 – 213 . [ 54 ] Jeffrey C . Oliver and Torbet McNeil . 2021 . Undergraduate data science degrees emphasize computer science and statistics but fall short in ethics training and domain - specific context . PeerJ Computer Science 7 ( 2021 ) . [ 55 ] Cathy O’Neil and Hanna Kiri Gunn . 2020 . Near - Term Artificial Intelligence and the Ethical Matrix . Ethics of Artificial Intelligence ( 2020 ) . [ 56 ] Joon Sung Park , Lindsay Popowski , Carrie Cai , Meredith Ringel Morris , Percy Liang , and Michael S . Bernstein . 2022 . Social Simulacra : Creating Populated Prototypes for Social Computing Systems . In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology ( UIST ’22 ) . Article 74 , 18 pages . [ 57 ] Ethan Perez , Saffron Huang , Francis Song , Trevor Cai , Roman Ring , John Aslanides , Amelia Glaese , Nathan McAleese , and Geoffrey Irving . 2022 . Red Teaming Language Models with Language Models . ArXiv abs / 2202 . 03286 ( 2022 ) . [ 58 ] Inioluwa Deborah Raji , I Elizabeth Kumar , Aaron Horowitz , and Andrew Selbst . 2022 . The fallacy of AI functionality . In 2022 ACM Conference on Fairness , Accountability , and Transparency . 959 – 972 . [ 59 ] Inioluwa Deborah Raji , Andrew Smart , Rebecca N White , Margaret Mitchell , Timnit Gebru , Ben Hutchinson , Jamila Smith - Loud , Daniel Theron , and Parker Barnes . 2020 . Closing the AI accountability gap : defining an end - to - end framework for internal algorithmic auditing . In Proceedings of the 2020 Conference on Fairness , Accountability , and Transparency . 33 – 44 . [ 60 ] Bogdana Rakova , Jingying Yang , Henriette Cramer , and Rumman Chowdhury . 2021 . Where responsible AI meets reality : Practitioner perspectives on enablers for shifting organizational practices . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) , 1 – 23 . [ 61 ] Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Christian Von Der Weth , and Brian Y . Lim . 2021 . Directed diversity : Leveraging language embedding distances for collective creativity in crowd ideation . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 – 35 . [ 62 ] Marco Tulio Ribeiro and Scott M . Lundberg . 2022 . Adaptive Testing and Debugging of NLP Models . In Annual Meeting of the Association for Computational Linguistics . [ 63 ] Emily Sheng , Kai - Wei Chang , Prem Natarajan , and Nanyun Peng . 2019 . The Woman Worked as a Babysitter : On Biases in Language Generation . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) . 3407 – 3412 . [ 64 ] Pao Siangliulue , Joel Chan , Steven P Dow , and Krzysztof Z Gajos . 2016 . IdeaHound : improving large - scale collaborative ideation with crowd - powered real - time semantic modeling . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology . 609 – 624 . [ 65 ] Tom Simonite . 2020 . Skewed Grading Algorithms Fuel Backlash Beyond the Classroom . WIRED ( 2020 ) . https : / / www . wired . com / story / skewed - grading - algorithms - fuel - backlash - beyond - classroom / [ 66 ] Jessie J Smith , Saleema Amershi , Solon Barocas , Hanna Wallach , and Jennifer Wortman Vaughan . 2022 . Real ml : Recognizing , exploring , and articulating limitations of machine learning research . In 2022 ACM Conference on Fairness , Accountability , and Transparency . 587 – 597 . [ 67 ] Jacob J . Stadler and Neal J . Seidl . 2013 . Software failure modes and effects analysis . In 2013 Proceedings Annual Reliability and Maintainability Symposium ( RAMS ) . 1 – 5 . DOI : http : / / dx . doi . org / 10 . 1109 / RAMS . 2013 . 6517710 [ 68 ] Anselm Strauss and Juliet Corbin . 1994 . Grounded theory methodology : An overview . ( 1994 ) . [ 69 ] Harini Suresh and John Guttag . 2021 . A framework for understanding sources of harm throughout the machine learning life cycle . In Equity and access in algorithms , mechanisms , and optimization . 1 – 9 . [ 70 ] European Union . 2020 . Ethics guidelines for trustworthy AI . ( 2020 ) . https : / / perma . cc / 8LPD - JN74 [ 71 ] Claudia Wagner , Markus Strohmaier , Alexandra Olteanu , Emre Kıcıman , Noshir Contractor , and Tina Eliassi - Rad . 2021 . Measuring algorithmically infused societies . Nature 595 , 7866 ( 2021 ) , 197 – 204 . [ 72 ] Laura Weidinger , Jonathan Uesato , Maribeth Rauh , Conor Griffin , Po - Sen Huang , John Mellor , Amelia Glaese , Myra Cheng , Borja Balle , Atoosa Kasirzadeh , and others . 2022 . Taxonomy of risks posed by language models . In 2022 ACM Conference on Fairness , Accountability , and Transparency . 214 – 229 . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 19 A APPENDIX A . 1 Scenario Descriptions In this Appendix , we provide additional details about the scenario descriptions we used in our experiments ( Table 3 ) , including the stakeholders ( Table 4 ) . Scenariodescription Communicationcompliance A company wants to deploy an AI system to scan its employees’ emails for toxic language . If the system detects that an employee is using toxic languageintheiremails , itwillautomaticallynotifytheemployee’smanager . Contentmoderation AsocialmediacompanywantstodeployanAIsystemtodetectandremovepoststhatmaycontaintoxiclanguage . Diseasediagnosis AmedicalclinicwantstodeployanAIsystemtoscanthemedicalrecordsofpatientsanddeterminewhethertheyhaveacertaindisease . Hiring AtechcompanywantstodeployanAIhiringsystemtoscantheresumesofapplicantsandpredictwhethertheyareagoodfitforagivenjobopening . Loanapplication AbankwantstodeployanAIsystemtoscantheinformationofloanapplicantsandpredictwhethertheyarelikelytorepayaloan . Table 3 . Scenario descriptions for all five AI deployment scenarios we considered in our experiments . Stakeholders Communicationcompliance theemployee ( sender ) , theemployee ( receiver ) , theemployees ( asagroup ) , thefamily / friendsoftheemployee , themanager , thecompany , theHR team , thelegalteam , theAIsystemdevelopers , theemployeeswhoidentifyasracialorethnicminorities , theemployeeswhoidentifyaswomen , the employeeswhoidentifybothaswomenandasracialorethnicminorities Contentmoderation theuserwritingthesocialmediapost , theuserswhowerementionedinthepost , thecontentmoderators , thesocialmediacompany , othersocialmedia companies , theemployeesofthesocialmediacompany , othersocialmediaplatformusers , thefamily / friendsoftheuserwritingthesocialmediapost , theAIsystemdevelopers , theonlinecommunity , theuserswhoidentifyasracialorethnicminorities , theuserswhoidentifyaswomen , theuserswho identifybothaswomenandasracialorethnicminorities Diseasediagnosis thepatient , otherpatients , futurepatients , thedoctor , nurses , otherdoctors , themedicalcommunity , otherpatientssufferingfromthesamedisease , the clinc , otherclinics , thehealthinsurancecompanies , thefamily / friendsofthepatient , theAIsystemdevelopers , thepatientswhoidentifyasracialor ethnicminorities , thepatientswhoidentifyaswomen , thepatientswhoidentifybothaswomenandasracialorethnicminorities Hiring theapplicant , otherapplicants , futureapplicants , thehiringmanager , theHRteam , the company , theAIsystemdevelopers , thefamily / friendsofthe applicant , theapplicantswhoidentifyasracialorethnicminorities , theapplicantswhoidentifyaswomen , theapplicantswhoidentifybothaswomen andasracialorethnicminorities Loanapplication theapplicant , otherapplicants , theemployeesof thebank , thebank , otherbanks , theAI systemdevelopers , the family / friends oftheapplicant , society , theapplicantswhoidentifyasracialorethnicminorities , theapplicantswhoidentifyaswomen , theapplicantswhoidentifybothaswomenandas racialorethnicminorities Table 4 . The sets of stakeholders we used in our experiments for each of the five AI deployment scenarios . Manuscript submitted to ACM 20 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi A . 2 Crowdsourcing task Demographic / Background questions that accompanied our crowdsourcing task include : 1 ) Have you experienced discrimination on the basis of your race , ethnicity , gender , nationality , sexual orientation , ability or religious beliefs ? 2 ) Have you experienced any adverse impacts from any AI or computational systems you have had to use in the past ? 3 ) Do you have any experience with AI systems like the one in scenario above ? 4 ) How familiar are you with how AI systems like the one in the scenario above work ? 5 ) What is your age range ? 6 ) What is the color of the sky ? ( Attention check question ) 7 ) Please let us know if you have any comments / feedback for improving this task . Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 21 A . 3 The taxonomy of harms Code Description : Ifthegeneratedexample Qualityofserviceharms : wheneithertheAIsystemoraservicetheAIsystemisusedfordoesn’tworkequallywellfordifferentindividualsorgroupsordoesn’tworkasintended . inadequateservice describesstakeholdersreceivinginaccurateorinvalidassessmentsthatleadtolowordegradingqualityservicesduetotheoutputofanAIsystem unfairtreatment describesastakeholderisfeelingorisbeingtreatedunfairly , includingduetorace , ethnicity , age , gender , orotherdemographicorgroupattribute lostcontent describesastakeholderlosingcontentorworkduetotheoutputofanAIsystem unawareofharmfulbehaviour describeshowtheoutputofanAIsystemcanleadtoastakeholderbecomingorremainingunawareoftheirownharmfulbehavior underperformingAI describestheAIsystemnotperformingaccordingtoitsintendedusageoraccordingtoexpectations Representationalharms : whentheAIsystem’soutputcontainsstereotypical / demeaning / erasingdescriptions , depictions , orrepresentationsofpeople , cultures , orsociety reinforcingstereotypes describeshowtheAIsystemoutputperpetuates , amplifies , orreinforcesexistingstereotypes targetoftoxiclanguage describesastakeholderbeingexplicitlyorimplicitlytargetedbytoxiclanguage , orbeingexposedtotoxiclanguageorexposingotherstotoxiclanguage Well - beingharms : Whenastakeholder’sphysical , mentalorgeneralwell - beingisormightbeaffectedbytheoutputofanAIsystem physicalhealth describesastakeholder’shealthconditiondeterioratingorbeingaffectedasaresultoftheoutputoftheAIsystem mentalhealth describesastakeholderexperiencingsomeformofemotionalorpsychologicaldistressdistressoranincreaseintheiremotionalorpsychologicaldistress , includingworries , feelingshame , anxietyorstress qualityoflife describesastakeholder’s ( typicallyanindividual , grouporcommunity ) needsnotbeingfulfilledasaresultoftheoutputoftheAIsystem lossofmotivation describesastakeholderfeelingdiscouragedorlosingmotivationbecauseoftheoutputoftheAIsystem self - doubt describesstakeholdersreevaluatingtheirabilitiesorworth , orbecomingreluctantormorecarefulwiththeirbehaviorduetotheAIsystem’soutput safety describesastakeholderfeelingunsafeorhavingtheirsafetybeingthreatenedasaresultoftheAIsystem’soutput commiseration describeshowtheAIsystem’soutputcanleadastakeholderfeelingsorrow , regretorempathyforotherstakeholders Legalandreputationalharms : WhenthelegalpositionorreputationofastakeholderisormightbeaffectedbytheoutputofanAIsystem distrustandreputationaldam - age describesastakeholderreputationbeingdamaged , ortoastakeholderdistrustingexperts , institutionsorotherstakeholders backlash describesastakeholdersexperiencingrepercussionorbacklashfromotherpeople legalrepercussions describesastakeholderexperiencingorbeinglikelytoexperiencelegalrepercussions scapegoat describesstakeholdersbeingblamedorheldresponsiblefortheAIsystemfailureorforthemistakesofothers liability describesstakeholdersbeingheldresponsiblefortheirownactions Socialandsocietalharms : WhentheAIsystem’soutputaffectsormightaffectrelationships , socialinstitutions , communities , orcanerodesocialanddemocraticstructures publichealth describesthepossibilityofadiseasespreadingonalargerscaleasaresultoftheAIsystems’failure erodingrelationships describes how the output of the AI system leads to miscommunication between stakeholders , a stakeholder feeling misunderstood or misunderstanding others , stakeholders’relationshipbeingaffected , orstakeholderslosingcontactwithafriendoracquaintance badactors notesordescribesasituationwhereonestakeholdertakesadvantageofotherstakeholdersoroftheirsituation socialissues describeshowtheoutputoftheAIsystemcanleadtocrimeorothersocialproblems toxicenvironment describeshowtheAIsystem’soutputleadsorcanleadtoatoxicenvironmentbeingcreatedintheworkplaceorinacommunity Lossofrightsoragency : WhentheAIsystem’soutputleadsormightleadtoalossofagencyorcontroloveraspectsofsomeone’slife , lossofprivacy , orlossofotherhumanrights lossofagency describeshowanAIsystemcanleadtoastakeholderautonomyandagencybeingatrisk , ortothestakeholdernotfeelingincontrolofmakingtheirown decisions lossofprivacy describeshowtheAIsystem’soutputleadstoastakeholderriskingtoorlosingtheirprivacy lossofrights describeshowtheAIsystem’soutputleadstoastakeholderriskingtoorlosingvariousrights Allocationalharms : WhentheAIsystem’soutputaffectstheallocationofresourcesoropportunitiesrelatingtofinance , education , employment , healthcare , housing , insurance , orsocial welfare creatingbusywork describesorcanleadtothestakeholderhavingtoworkforlonger ( i . e . , spendingmoretime ) oronmoretasksthannecessary economicstrain describesorcanleadtostakeholders ( e . g . , company , applicant , family ) experiencingeconomicstrain , losingmoney / customers / business , ordealingwith raisingcosts jobsecurity describesthepossibilitythatastakeholdermightlosetheirjobasaresultoftheAIsystem’soutput waste describeshowtheAIsystemleadsorcanleadtoastakeholderfeelingthattheyhavewastedtimeorotherresources worksatisfaction / fit describeshowtheAIsystem’soutputcanleadtostakeholdersnotgettingasuitablepositionornotbeingsatisfiedwiththeirjob productivityloss describeshowtheAIsystem’soutputcanleadtoproductivitylossintheworkplace , includingduetounqualifiedemployees , lackofdiversity , astakeholder underperforming , orlowerworkquality opportunityloss describeshowtheAIsystem’soutputcanleadtostakeholdersmissingoutonanopportunity , includingfailingtohirethemostqualifiedindividuals lackofaccesstoinformation describesastakeholdernothaving , beingdeniedorlosingaccesstoinformation bannedfromsite describesstakeholdersbeingbannedfromusingaforumorplatform Otherharms : Whenitisunclearwhatthespecificharmmightbeduetounderspecification , orwhenitgenerallytalksabouttheamplificationorexacerbationofharms underspecified or repeated harm doesnotelaborateuponwhatthespecificharmmightbeortheexampleismerelyrepeatingthepremise snowballeffect describeshowtheAIsystem’soutputcanleadtosomestatedorunstatedharmsbeingamplifiedorperpetuated Notmeaningful : Whenwhatisdescribedisnotaharm notaharm ismeaningfulincontextbutitdoesnotrepresentanactualharmtoanyofstakeholders nonsensical isnotclearandthereasoningdoesn’tmakesenseincontext Table 5 . Harms taxonomy , including top and second level harm categories as well as corresponding working definitions Manuscript submitted to ACM 22 Zana Buçinca , Chau Minh Pham , Maurice Jakesch , Marco Tulio Ribeiro , Alexandra Olteanu , and Saleema Amershi A . 4 Examples of harm for each category in our taxonomy Code Exampleharm : ( Scenario – ProblematicAIBehavior – Stakeholder ) Qualityofserviceharms : wheneithertheAIsystemoraservicetheAIsystemisusedfordoesn’tworkequallywellfordifferentindividualsorgroupsordoesn’tworkasintended . inadequateservice ( DiseaseDiag . – FP – thepatient ) “theclinicwillaccidentallydelayyourtreatment , resultinginprolongationofpatient’ssufferingorevendeath” unfairtreatment ( LoanAppl . – FN – womenapplicants ) “societymightviewyouaslesscapablethanmenandaslessworthyoffinancialincentives” lostcontent ( Comm . compl . – FP – theemployee ( sender ) ) “youremailmightbesuppressedandpeopleyouemailwillthenneverrespondback” unawareofharmfulbehaviour ( Contentmod . – FN – womenusers ) “youmayusetoxiclanguageunintentionallyandthesystemwillnotcensorit , potentiallyleadingtosocialostracization . ” underperformingAI ( LoanAppl . – FN – women & racialminorityappl . ) “youmighthavebeengiventheloanifthesystemhadaccuratelyassessedyourqualifications . ” Representationalharms : whentheAIsystem’soutputcontainsstereotypical / demeaning / erasingdescriptions , depictions , orrepresentationsofpeople , cultures , orsociety reinforcingstereotypes ( Diseasediag . – FN – racial & ethnicminoritypatients ) “mygroupmaybeprofiledasnotlikelytohaveitsotestsarenotordered” targetoftoxiclanguage ( Contentmod . – FN – racial & ethnicminorityusers ) “Icouldreadoffensivelanguageaboutthingsthatoffendme” Well - beingharms : Whenastakeholder’sphysical , mentalorgeneralwell - beingisormightbeaffectedbytheoutputofanAIsystem physicalhealth ( Diseasediag . – FN – women & racialminoritypatients ) “thenImightperishwithoutanyrecoursetoamedicalsolution . ” mentalhealth ( Hiring – FN – family / friendsoftheapplicant ) “youmightfeelfrustratedknowingthatyourlovedoneisnotqualifiedorthattheyarebeingtreatedunjustly” qualityoflife ( Loanappl . – FN – theapplicant ) “IneedthemoneyforsomethingandthenIwouldn’tbeabletopaymybillsorImaymissoutonadealforacar . ” lossofmotivation ( Contentmoder . – FP – theonlinecommunity ) “usersmightmissadiscussion . Theoriginalpostermaybediscouragedfromadditionalposts , otherusers mayfeellikethecommunityistoxic . ” self - doubt ( Contentmoder . – FN – thecontentmoderators ) “youmightfeelyouaren’tdoingyourjobcorrectly” safety ( Comm . compliance – FN – theemployee ( receiver ) ) “youmightfeelharassedbythepersonsendingtheemails” commiseration ( Diseasediag . – FP – family / friendsofthepatient ) “Icouldhaveafriendorfamilymembertakingtreatmentthatcouldhurtthembecausetheydon’tneed thattreatment . Idon’twanttowatchmyfamily / friendsuffer . ” Legalandreputationalharms : WhenthelegalpositionorreputationofastakeholderisormightbeaffectedbytheoutputofanAIsystem distrustandreputationaldamage ( Diseasediag . – FN – themedicalcommunity ) “professionalsfromothercommunitieswillbemotivatedtocastdoubtontheeffectivenessofyoursystem” backlash ( Contentmoder . – FP – theemployeesofthesocialmediacompany ) “Iwillbeperceivedaspartofthemachineryofatoxicandpossiblyracist / sexist / politically biased / etc . corporationandwillbethetargetofcriticismbothonandoffline . ” legalrepercussions ( Comm . compl . – FN – thelegalteam ) “Iwillenduphavingtodefendthecompanyfromlawsuitsthatwillbedifficultifnotimpossibletowin . ” scapegoat ( Loanappl . – FP – family / friendsoftheapplicant ) “youmighthavetosupportyourfamilymember / friendwhomaynotbeabletorepaytheloan” liability ( Hiring – FP – thehiringmanager ) “youcouldenduphavingtotakeresponsibilityofmistakesmadebyunqualifiedworkers . ” Socialandsocietalharms : WhentheAIsystem’soutputaffectsormightaffectrelationships , socialinstitutions , communities , orcanerodesocialanddemocraticstructures publichealth ( Diseasediag . – FN – themedicalcommunity ) “thepatientmayspreadthedisease” erodingrelationships ( Contentmoder . – FN – family / friendsoftheuserwritingthesocialmediapost ) “eitheryoumaystopprovidingsupporttoyourfamilymember / friendor yourfamily / friendmaybehurtwhenotherspointoutthetoxiclanguagepost” badactors ( Diseasediag . – FP – otheremployees ) “theclinicmayover - diagnosethediseasejusttoobtainmoretreatmentfees” socialissues ( Contentmoder . – FN – theonlinecommunity ) “Thetoxiclanguagemayhavelongtermimpactsonthebehavioral / mentalhealthoftheonlinecommunity . ” toxicenvironment ( Comm . compl . – FP – womenemployees ) “yourmanagermightfeeloffendedbywhatyousendwhichwillcreateundesirabletensionintheworkplace” Lossofrightsoragency : WhentheAIsystem’soutputleadsormightleadtoalossofagencyorcontroloveraspectsofsomeone’slife , lossofprivacy , orlossofotherhumanrights lossofagency ( Diseasediag . – FN – thepatient ) “youwillbeprescribedthewrongtreatmentandwillfeellikeyouarebeingtreatedasaguinneapigforAIapplications” lossofprivacy ( Comm compl . – FP – the employee ( receiver ) ) “I would be concerned abouta lack of personal review that could potentiallylead to consequences . Also , I wouldfeelundersurveillanceandmisunderstoodbyanunfeelingmachine . ” lossofrights ( Contentmoder . – FP – othersocialmediaplatformusers ) “freedomofspeechwasviolated” Allocationalharms : WhentheAIsystem’soutputaffectstheallocationofresourcesoropportunitiesrelatingtofinance , education , employment , healthcare , housing , insurance , orsocial welfare creatingbusywork ( Hiring – FP – themanager ) “youmayfeel thatyouremployeeis writingtoxicemailsto allhis / hercolleaguesand you‘llfeelhelpless inresolvingtheissue andangrythatyouneedtospendmoretimeinhis / hercaseinsteadofhandlingotheremployees” economicstrain ( Contentmoder . – FP – thesocialmediacompany ) “peoplewillcomplainaboutyourcompany , youmaylosecustomers” jobsecurity ( Loanappl . – FN – theemployeesofthebank ) “youremployermightthinkthatyouwereatfaultinthefaultydecision , andyoumightbepunishedorfired” waste ( Hiring – FP – women & racialminorityapplicants ) “youcouldhaveyourtimewastedbycomingtoaninterviewyouarenotevenqualifiedorevenhired andthenoverwhelmedbyajobyoucannotdo . ” worksatisfaction / fit ( Hiring – FP – thehiringmanager ) “Icouldbehiringsomeonewhoisn’tproperlyadequatewithknowledgeandthatpersoncouldruinaprogramorcreatea securityrisk . ” productivityloss ( Comm . compl . – FP – themanager ) “timemaybewastedbythefalsetoxicityalarms , whichcoulddelayimportantwork” opportunityloss ( Comm . compl . – FN – theHRteam ) “itwillbecomenearlyimpossibletohirequalifiedcandidatesduetothenegativeperceptionoftheworkplace . ” lackofaccesstoinformation ( Contentmoder . – FP – theonlinecommunity ) “importantmessagesmightnevergetposted . Opencommunicationintheonlineforumwillbehinderedand somemembersmaybeunfairlybannedfromthecommunity . ” bannedfromsite ( Contentmoder . – FP – theuserwritingthesocialmediapost ) “yourpostmaybedeemedasspam , youmightbesuspendedfromusingtheplatformand otherusersmightstoptrustingyou” Otherharms : Whenitisunclearwhatthespecificharmmightbeduetounderspecification , orwhenitgenerallytalksabouttheamplificationorexacerbationofharms underspecifiedorrepeatedharm ( Loanappl . – FP – society ) “itisbadthingstotheloanapplicants” snowballeffect ( Comm . compl . – FN – racial & ethnicminorityemployees ) “youremailsmightbeperceivedbythemanagertocontaintoxiclanguage ; thiscouldtaintyour reputationandtriggeraseriesofeventsthatcouldresultinpunishment” Notmeaningful : Whenwhatisdescribedisnotaharm notaharm ( Hiring – FN – family / friendsoftheapplicant ) “yourfriend / familymembermighttakeloansfromothersourcesotherthanbanks” nonsensical ( Loanappl . – FP – society ) “manyloanseekerswouldtakeoutloansfromyourbank , makingthegovernmentthinkthatlendingthemoneytoyourbankis notfinanciallyviable” Table 6 . Examples of harms generated with AHA ! Manuscript submitted to ACM AHA ! : Facilitating AI Impact Assessment by Generating Examples of Harms 23 A . 5 Interview materials Fig . 7 . The interactive interface used during interviews to allow participants navigate through the harm examples surfaced by AHA ! . Participants could select stakeholders and filter the examples of harms accordingly . Manuscript submitted to ACM