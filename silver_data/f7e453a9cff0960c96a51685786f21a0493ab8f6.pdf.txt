PARTIAL DIFFERENTIAL EQUATION REGULARIZATION FOR SUPERVISED MACHINE LEARNING ADAM M . OBERMAN Abstract . This article is an overview of supervised machine learning problems for regression and classiﬁcation . Topics include : kernel methods , training by stochastic gradient descent , deep learning architecture , losses for classiﬁcation , statistical learning theory , and dimension inde - pendent generalization bounds . Implicit regularization in deep learning examples are presented , including data augmentation , adversarial training , and additive noise . These methods are re - framed as explicit gradient regularization . 1 . Introduction In this work we present a mathematically oriented introduction to the gradient regularization approach to deep learning , with a focus on convolutional neural networks ( CNNs ) for image clas - siﬁcation . Image classiﬁcation by deep neural networks is now well established , as exempliﬁed by the impressive performance of models on data sets such as those presented in Example 1 . 1 . The goal is to build a classiﬁcation map ( model ) from images to labels , as illustrated in Figure 1 . Example 1 . 1 . The MNIST dataset consists of m = 70 , 000 , d = 28 × 28 greyscale images of handwritten digits . CNNs achieve an error of less than 1 % on MNIST . On this simple data set , support vector machines ( SVM ) achieve accuracy almost as high . The CIFAR - 10 dataset consists of m = 60 , 000 d = 32 × 32 × 3 = 3072 colour images in K = 10 classes , with 6000 images per class . CNNs can achieve accuracy better that 96 % on this dataset . ImageNet has K = 21 , 841 Date : October 4 , 2019 . 2010 Mathematics Subject Classiﬁcation . Primary 65N99 , Secondary 35A15 , 49M99 , 65C50 . This material is based on work supported by the Air Force Oﬃce of Scientiﬁc Research under award number FA9550 - 18 - 1 - 0167 . Figure 1 . Illustration of the classiﬁcation map f ( x ) : X → Y on the ImageNet dataset . 1 a r X i v : 1910 . 01612v1 [ c s . L G ] 3 O c t 2019 2 ADAM M . OBERMAN class labels with m = 14 , 197 , 122 total images , at a resolution of 256 × 256 pixels with 3 color channels , so d = 256 × 256 × 3 = 196608 . See Figure 1 for an illustration of the classiﬁcation map on ImageNet . Current models achieve accuracy greater than 84 % and Top 5 accuracy ( meaning the correct label is in the ﬁve highest ranked predictions ) better than 97 % . Machine learning can be divided into supervised , unsupervised , and reinforcement learning . Supervised learning can be further divided into regression , where the target function value is a number , and classiﬁcation , where the target is labels . Deep learning , which refers to machine learning using the deep neural network hypothesis class , is having an impact in many areas , far too many to cover eﬀectively here . By focussing on CNNs for classiﬁcation and by writing for a mathematical audience , we hope to make a contribution to this literature . Another topic which may be of interest to readers is generative models using convolutional neural networks [ GPAM + 14 ] . These models sample a distribution with the goal of generating new images from the distribution . Recent work has exploited connections with Optimal Transportation [ ACB17 ] . 2 . Machine Learning problem definition Let x ∈ X be an input , where X ⊂ [ 0 , 1 ] d and the dimension is large , d (cid:29) 1 . We consider a target set Y , which will be either Y = R , for regression problems , or Y = { 1 , . . . , K } , for ( mono - label ) classiﬁcation problems . The dataset ( 2 . 1 ) S m = { ( x 1 , y 1 ) , . . . , ( x m , y m ) } consists of m samples , x i , drawn i . i . d . from a data distribution , ρ ( x ) , with support X . The labels y i ∈ Y are also given . The non - negative function L : Y × Y → R is a loss function if it is zero only when y 1 = y 2 . We say the loss is convex if it is convex as a function of y 1 for every ﬁxed y 2 . For regression problems , the quadratic loss L ( y 1 , y 2 ) = (cid:107) y 1 − y 2 (cid:107) 2 is often used . Our objective is to ﬁnd a function f : X → Y which minimizes the expected loss ( 2 . 2 ) L D ( f ) = E x ∼ ρ [ L ( f ( x ) , y ( x ) ) ] = (cid:90) X L ( f ( x ) , y ( x ) ) dρ ( x ) The expected loss depends on unavailable data and labels , so it needs to be approximated . One common practice is to divide the data set into a training set and a holdout test set which is not used for training . Then the expected loss is estimated on the test set . This procedure allows for training by minimizing the empirical loss ( EL ) L S m [ f ] = 1 m m (cid:88) i = 1 L ( f ( x i ) , y i ) . Typically , the functions considered are restricted to a parametric class ( 2 . 3 ) H = { f ( x , w ) | w ∈ R D } so that minimization of ( EL ) can be rewritten as the ﬁnite dimensional optimization problem ( EL - W ) min w 1 m m (cid:88) i = 1 L ( f ( x i , w ) , y i ) . The problem with using ( EL - W ) as a surrogate for ( 2 . 2 ) , is that a minimizer of ( EL - W ) could overﬁt , meaning that the expected loss is much larger than the empirical loss . Classical machine learning methods avoid overﬁtting by restricting H to be a class of simple ( e . g . linear ) functions . Remark 2 . 1 . Some machine learning textbooks focus on a statistical point of view , which is impor - tant for problems where is measurement or label noise , or when there is prior statistical information . REGULARIZATION FOR MACHINE LEARNING 3 For example in linear regression , we assume a parametric form for f ( x ) , and the goal is to over - come the noise in the data . In contrast , for image classiﬁcation on benchmark data sets , such as ImageNet , the images are very clear , and the number of incorrect or ambiguous labels is less than a fraction of a percent , which is still small compared to the 4 % error . Thus to ﬁrst approximation , we can assume that the images are free of noise , that all labels are correct , and that there are no ambiguous images . In other words , y i = y ( x i ) for a label function y ( x ) . It is our opinion that the challenge in learning y ( x ) comes not from uncertainty , noise , or ambiguity , but rather from the complexity of the functions involved . The point of view of this work , expanded upon in [ FCAO18 ] , is that while deep learning models are parametric , the high degree of expressibility of deep neural networks renders the models eﬀec - tively nonparametric . For such problems , regularization of the loss is necessary for generalization . Regularization is a topic in machine learning as well , but it is interpreted in a wider sense . We will show that some implicit regularization methods in learning can be reinterpreted as explicit regularized models . 2 . 1 . Classiﬁcation losses . A method for classiﬁcation with K classes , Y = { 1 , . . . , K } , is to output a a scoring function for each class , f = ( f 1 , . . . , f K ) and choose the index of the maximum component as the classiﬁcation ( 2 . 4 ) C ( f ( x ) ) = arg max j f j ( x ) The relevant loss is the 0 - 1 classiﬁcation loss , L 0 , 1 ( f , k ) = (cid:40) 0 if C ( f ) = k 1 otherwise However , this loss is both discontinuous and nonconvex , so a surrogate convex loss is used in practice . The margin of the function f ( x ) is given by ( 2 . 5 ) L max ( f , k ) = max i f i − f k which is convex upper bound to the classiﬁcation loss , making it a convex surrogate loss [ MRT18 , Ch 4 . 7 ] . This loss was proposed in [ CS01 ] and studied in [ Z + 04 ] . The non - convex margin loss [ MRT18 , Ch 9 ] is used to obtain margin bounds for multi - class classiﬁcation . 3 . Function approximation and regularization 3 . 1 . Regularization . The problem of ﬁtting a function can be cast as a multi objective problem : ( I ) ﬁt the given data points , ( II ) reduce overﬁtting . Minimizing the empirical loss corresponds to a relaxation of ( I ) . Choosing a a class of functions ( hypothesis space ) H which does not overﬁt corresponds to a hard constraint on ( II ) and leads to ( EL ) . Deﬁning a regularization functional R ( f ) which is some measure of overﬁt , leads to a soft constraint on ( II ) . Hypothesis classes are often parametric , but they can also be deﬁned using the regularization functional , as H = { f | R ( f ) ≤ C } . See Table 1 . The regularized empirical loss minimization problem takes the form ( EL - R ) min f 1 m m (cid:88) i = 1 L ( f ( x i ) , y i ) + λ R ( f ) , where λ is a parameter which measures the strength of the regularization term . Example 3 . 1 ( Cubic splines ) . One dimensional cubic splines are piecewise cubic , twice - diﬀerentiable interpolating functions [ Wah90 ] . We can write cubic spline interpolation in the form min f ∈H 1 m m (cid:88) i = 1 L ( f ( x i ) , y i ) , H = { f ( x ) and f (cid:48) ( x ) continuous } . 4 ADAM M . OBERMAN Constraint Fit data No overﬁt Hard f ( x i ) = y i for all i f ∈ H Soft min f 1 m (cid:80) mi = 1 L ( f ( x i ) , y i ) λ R ( f ) Table 1 . Balancing the objectives of ﬁtting the data and not overﬁtting on unseen data , using hard or soft constraints . Equivalently , the solution is characterized by min f R curv ( f ) = (cid:90) ( f (cid:48)(cid:48) ( x ) ) 2 dx , subject to { f ( x i ) = y i , i = 1 , . . . m } Regularized loss functionals such as ( EL - R ) arise in mathematical approaches to image process - ing [ AK06 , Sap06 ] as well as inverse problems in general . The approach has a mature mathematical theory which includes stability , error analysis , numerical convergence , etc . Mathematical Image processing tools have also been used for other important deep learning tasks , such as Image Seg - mentation . In signal processing , the loss and the regularizer are designed to adapt to the signal and noise model . So , for example , quadratic losses arise from Gaussian noise models . Classical Tychonov regularization [ TA77 ] , R Tych ( f ) = (cid:82) D | ∇ f ( x ) | 2 is compatible with smooth signals . Total Variation regularization [ ROF92 ] , R TV ( f ) = (cid:82) D | ∇ f ( x ) | is compatible with images . 3 . 2 . Curse of dimensionality . Mathematical approximation theory [ Che66 ] allows us to prove convergence of approximations f m → f with rates which depend on the error of approximation and on the typical distance between a sampled point and a given data point , h . For uniform sampling of the box [ 0 , 1 ] d with m points , we have h = m 1 / d . When the convergence rate is a power of h , this is an example of the curse of dimensionality : the number of points required to achieve a given error grows exponentially in the dimension . Since the dimension is large , and the number of points is in the millions , the bounds obtained are vacuous . There are situations in function approximation where convergence is exponentially fast . For example , Fourier approximation methods can converge exponentially fast in h when the function are smooth enough . Next we will see the how kernel methods can overcome the curse of dimensionality . Later we will present a connection between kernel methods and Fourier regularization . 4 . Kernel methods The state of the art methods in machine learning until the mid 2010s were kernel methods [ MRT18 , Ch 6 ] , which are based on mapping the data x ∈ X into a high dimensional feature space , Φ : X → H , where Φ ( x ) = ( φ 1 ( x ) , φ 2 ( x ) , . . . ) . The hypothesis space consists of linear combinations of feature vectors , H ker = (cid:40) f ( x , w ) | f ( x , w ) = (cid:88) i w i φ i ( x ) (cid:41) The feature space is a reproducing kernel Hilbert space , H , which inherits an inner product from the mapping Φ . This allows costly inner products in H to be replaced with a function evaluation K ( x , y ) = Φ ( x ) · Φ ( y ) = (cid:88) i φ i ( x ) . · φ i ( y ) The regularized empirical loss functional is given by ( EL - K ) min f ∈H ker 1 m m (cid:88) i = 1 L ( f ( x i , w ) , y i ) + λ 2 (cid:107) w (cid:107) 2 H REGULARIZATION FOR MACHINE LEARNING 5 For convex losses , ( EL - K ) is a convex optimization in w . For classiﬁcation , the margin loss is used , and the optimization problem corresponds to quadratic programming . In the case of quadratic losses , the optimization problem is quadratic , and the minimizer of ( EL - K ) has the explicit form f ( x ) = m (cid:88) i = 1 w i K ( x , x i ) , ( M + λI ) w = y where the coeﬃcients c are given by the solution of the system of linear equations with M ij = K ( x i , x j ) , and I is the identity matrix . Note that the regularization term has a stabilizing eﬀect : the condition number of the system with λI improves with λ > 0 . Better conditioning of the linear system means that the optimal weights are less sensitive to changes in the data w ∗ = ( M + λI ) − 1 y Algorithm to minimize ( EL - K ) are designed to be written entirely in terms of inner products , allowing for high dimensional feature spaces . 5 . Training and SGD 5 . 1 . Optimization and variational problems . Once we consider a ﬁxed hypothesis class , ( EL - R ) becomes ( EL - W ) , which is a ﬁnite dimensional optimization problem for the parameters w . Optimization problems are easier to study and faster to solve when they are convex [ BV04 ] . Support vector machines are aﬃne functions of w and x . Kernel methods are aﬃne functions of w . This makes the optimization for kernels methods a convex problem . We consider problems where the number of data points , m , the dimension of the data , n , and the number of parameters , D , are all large . In this case the majority of optimization algorithms developed for smaller scale problems are impractical , for the simple reason that it may not eﬃcient to work with m × m matrices or take gradients of losses involving m copies of n dimensional data . Stochastic gradient descent ( SGD ) has emerged as the most eﬀective algorithm [ BCN16 ] : where previous algorithms tried to overcome large data by visiting each date point once , SGD uses many more iterations , visiting data multiple times , making incremental progress towards the optimum at each iteration . For this reason , the number of iterations of SGD is measured in epochs , which corresponds to a unit of m evaluations of ∇L ( f ( x i ) , y i ) . 5 . 2 . Stochastic gradient descent . Evaluating the loss ( EL ) on all m data points can be costly . Deﬁne random minibatch I ⊂ { 1 , . . . , m } , and deﬁne the corresponding minibatch loss by L I ( w ) = 1 | I | (cid:88) i ∈ I L ( f ( x i , w ) , y i ) Stochastic gradient descent corresponds to w k + 1 = w k − h k ∇ w L I k ( w k ) , I k random , h k learning rate Example 5 . 1 ( simple SDG example ) . Let x i be i . i . d . samples from the uniform probability ρ 1 ( x ) for x ∈ [ 0 , 1 ] 2 , the two dimensional unit square . Consider estimating the mean using the following quadratic loss ( EL - Q ) min f ∈H 1 m m (cid:88) i = 1 ( x i − f ( x i , w ) ) 2 where H = { f ( x , w ) = w | w ∈ R 2 } is simply the set of two dimensional vectors representing the mean . Empirical loss minimization of ( EL - Q ) corresponds to simply computing the sample mean , w ∗ = w ∗ ( S m ) = (cid:80) x i / m . The full dataset and a random minibatch are illustrated in Figure 2 . As the value w k gets closer to the minimum , the error in the gradient coming from the minibatch increases , as illustrated in the Figure . As a result a decreasing time step ( learning rate ) h k is used . 6 ADAM M . OBERMAN - 1 - 0 . 5 0 0 . 5 1 - 1 - 0 . 5 0 0 . 5 1 Figure 2 . Full data set and a minibatch for data sampled uniformly in the square . Component gradients ( black ) and the minibatch gradient ( green ) . Closer to the minimum , the relative error in the minibatch gradient is larger . The schedule for h k is of order 1 / k , and the convergence rate for SGD , even in the strongly convex case , is also of order 1 / k . 5 . 3 . The convergence rate of SGD . See [ BCN16 ] for a survey on results on SGD . The following simple result was proved in [ OP19 ] . Suppose f is µ - strongly convex and L - smooth , with minimum at w ∗ . Let q ( w ) = (cid:107) w − w ∗ (cid:107) 2 . Write ∇ mb f ( w ) = ∇ f ( w ) + e , with e is a mean zero random error term , with variance σ 2 . Consider the stochastic gradient descent iteration w k + 1 = w k − h k ∇ mb f ( w k ) , ( SGD ) with learning rate ( SLR ) h k = 1 µ ( k + q − 1 0 α − 1 S ) , Theorem 5 . 2 . Let w k , h k be the sequence given by ( SGD ) ( SLR ) . Then , E [ q k | w k − 1 ] ≤ 1 α S k + q − 1 0 , for all k ≥ 0 . The convergence rate of SGD is slow : the error decreases on the order of 1 / k for strongly convex problems . This means that if it takes 1000 iterations to reach an error of (cid:15) , it may take ten times as many iterations to further decrease the error to (cid:15) / 10 . However , the the tradeoﬀ of speed for memory is worth it : while the number of iterations to achieve a small error is large , the algorithm overcomes the memory bottleneck , which would make computing the full gradient of the loss function impractical . 5 . 4 . Accelerated SGD . In practice , better empirical results are achieved using the accelerated version of SGD . This algorithm is the stochastic version of Nesterov’s accelerated gradient descent [ Nes13 ] . See [ Goh17 ] for an exposition on Nesterov’s method . Nesterov’s method can be interpreted as the discretization of a second order ODE [ SBC14 ] . In [ LO19 ] we show how the continuous time interpretation of Nesterov’s method with stochastic gradients leads to accelerated convergence rates for Nesterov’s SGD , using a Liapunov function analysis similar to the one described above . 6 . Statistical learning theory 6 . 1 . Concentration of measure . Consider the experiment of ﬂipping a possibly biased coin . Let X k ∈ { − 1 , 1 } represent the outcomes of the coin toss . After m coin tosses , let S m = 1 m (cid:80) mk = 1 X k be the sample mean . The expected value of X , µ = E [ X ] is the diﬀerence of the probabilities , REGULARIZATION FOR MACHINE LEARNING 7 p H − p T , which is zero when the coin is fair . In practice , due the randomness , the sample mean will deviate from the mean , and we expect the deviation to decrease as m increases . The Central Limit Theorem quantiﬁes the deviation √ m ( S m − µ ) converges to the normal distribution as m → ∞ , so that , | S m − µ | ≈ 1 √ m for m large . Concentration of measure inequalities provide non - asymptotic bounds . For example , Hoeﬀdings inequality [ MRT18 , Appendix D ] applied to random variables taking values in [ − 1 , 1 ] gives P ( | S m − µ | > (cid:15) ) ≤ 2 exp (cid:18) − m(cid:15) 2 2 (cid:19) for any (cid:15) > 0 . Setting δ = 2 exp (cid:0) − m(cid:15) 2 / 2 (cid:1) and solving for (cid:15) allows us to restate the result as ( 6 . 1 ) | S m − µ | ≤ (cid:114) 2 log ( 2 / δ ) m with probability 1 − δ , for any δ > 0 . 6 . 2 . Statistical learning theory and generalization . Statistical learning theory ( see [ MRT18 , Chapter 3 ] and [ SSBD14 , Chapter 4 ] ) can be used to obtain dimension independent sample com - plexity bounds for the expected loss ( generalization ) of a learning algorithm . These are bounds which depend on the number of samples , m but not on the dimension of the underlying data , n . The hypothesis space complexity approach restricts the hypothesis space ( 2 . 3 ) to limit the ability of functions to overﬁt by bounding the the generalization gap L gap [ f ] : = L D [ f ] − L S [ f ] . The dependence of the generalization gap on the learning algorithm is removed by considering the worst - case gap for functions in the hypothesis space L gap [ f A ( S ) ] ≤ sup f ∈H L gap [ f ] For example , [ MRT18 , Theorem 3 . 3 ] ( which applies to the case of bounded loss function 0 ≤ L ≤ 1 ) , states that for any δ > 0 ( 6 . 2 ) L gap [ f A ( S ) ] ≤ R m ( H ) + (cid:115) ln 1 δ 2 m , with probability ≥ 1 − δ where R ( H ) is the Rademacher complexity of the hypothesis space H . Observe that ( 6 . 2 ) has a similar form to ( 6 . 1 ) , with the additional term coming from the hypothesis space complexity . Thus , restricting to a low - complexity hypothesis space reduces learning bounds to sampling bounds . The Rademacher complexity of H measures the probability that some function f ∈ H is able to ﬁt samples of size m with random labels [ MRT18 , Defn 3 . 1 and 3 . 2 ] . Example 6 . 1 . Consider the hypothesis space consisting of the set of axis aligned rectangles in the plane . Set x = ( x 1 , x 2 ) , w = ( w 1 , w 2 , w 3 , w 4 ) and f ( x , w ) = (cid:40) 1 if w 1 ≤ x 1 ≤ w 2 , and w 3 ≤ x 2 ≤ w 4 0 otherwise and consider the hypothesis space given by H = { f ( x , w ) | w 1 ≤ w 2 , w 3 ≤ w 4 } Then R ( H ) → 0 as m → ∞ . 8 ADAM M . OBERMAN Stability approach to generalization . The stability approach to generalization [ SSBD14 , Chapter 13 ] [ BE02 ] considers perturbations of the data set S . It measures how much changing a data point in S leads to a change in f A ( S ) . It also leads to estimates of the form ( 6 . 2 ) , with the Rademacher complexity of the hypothesis space replaced by the uniform stability of the algorithm . For example , see [ SSBD14 , Ch 13 and Ch 15 ] for ridge - regression and support vector machines . Robustness approach to generalization . The robustness approach [ XM12 ] considers how much the loss value can vary with respect to the input space of ( x , y ) . They deﬁne an algorithm A to be ( K , (cid:15) ( S ) ) - robust if the dataset can be partitioned into K disjoint sets { C i } Ki = 1 and there is a function (cid:15) ( S ) ≥ 0 such that for all s ∈ S s , z ∈ C i = ⇒ | L ( f A ( S ) , s ) − L ( f A ( S ) , z ) | ≤ (cid:15) ( S ) For ( K , (cid:15) ( S ) ) - robust algorithms , the result is ( 6 . 3 ) L gap [ f A ( S ) ] ≤ (cid:15) ( S ) + 2 M (cid:115) K ln 2 + ln 1 δ 2 m , with probability ≥ 1 − δ So the result ( 6 . 3 ) trades robustness for Rademacher complexity , with the addition of a term measuring the number of sets in the partition . However , if we consider an L - Lipschitz function f , then the function is (cid:15) robust for a partition of the set by balls of radius (cid:15) / L . However the number of such balls depends on the dimension , K = ( 1 / (cid:15) ) d , so , in this case , the curse of dimensionality is still there , but absorbed into the constant K . 7 . Deep Neural Networks The deep learning hypothesis class is nonlinear and nonconvex in both w and x . This is diﬀerent from support vector machines , which are aﬃne in both variables , and kernel methods , which are aﬃne in w . The nonconvexity makes the analysis of the parametric problem ( EL - W ) much more complicated . On the other hand , studying ( EL - R ) in the nonparametric setting allows us to analyze the existence , uniqueness and stability of solutions , without the additional complicating details of the parameterization . 7 . 1 . Network architecture . The architecture of a deep neural network refers to the deﬁnition of the hypothesis class . The function f ( x ; w ) is given by a composition of linear ( aﬃne ) layers with a nonlinearity , σ ( t ) , which is deﬁned to act componentwise on vectors σ ( y 1 , . . . , y n ) ≡ ( σ ( y 1 ) , . . . , σ ( y n ) ) . In modern architectures , the nonlinearity is the rectiﬁed linear unit ( ReLU ) , σ ( t ) = max ( t , 0 ) . Deﬁne the i th layer of the network using ( consistently sized ) rectangular matrix , W i , and bias vector , b i composed with the nonlinearity f ( i ) = σ ( W i x + b i ) . The neural network with J layers is given by the composition of layers with consistent dimensions , f ( x , w ) = f J ◦ · · · ◦ f 1 , w = ( W 1 , . . . , W J ) where the parameter w is the concatenation of the matrices in each layer w = ( W 1 , . . . , W J ) . The network is deep when J is large . Convolutional neural networks [ GBC16 , Chapter 9 ] constrain the matrices to be sparse , leading to signiﬁcant reduction in the total number of parameters . The basic convolutional network has a sparsity structure of 9 points , corresponding to nearest neighbors on a two by two grid . The weights are repeated at diﬀerent grid points in such a way that the matrix vector product , Wx , is equivalent to a convolution . REGULARIZATION FOR MACHINE LEARNING 9 7 . 2 . Backpropagation . Backpropagation is the terminology for symbolic diﬀerentiation of a neu - ral network . Using the parametric representation of the mode , we can compute the full gradient vector ∇ f = ( ∇ w f ( x , w ) , ∇ x f ( x , w ) ) using the chain rule . The ﬁrst component , ∇ w f ( x , w ) is used to optimize the weights for training . 7 . 3 . DNN Classiﬁcation . For classiﬁcation problems with K classes , the ﬁnal layer of the neural network is K dimensional and the classiﬁcation is given by ( 2 . 4 ) . Here , we interpret the DNN classiﬁcation loss as a smoothed version of the max loss ( 2 . 5 ) . The standard loss function for DNN classiﬁcation is the composition of the Kullback - Liebler divergence with the softmax function softmax ( f ) = 1 (cid:80) Ki = 1 exp ( f i ) ( exp ( f 1 ) , . . . , exp ( f K ) ) . The Kullback - Leibler divergence from information theory [ Bis06 , Section 1 . 6 ] [ GBC16 , Section 3 . 13 ] is deﬁned on probability vectors , by L KL ( q , p ) = − K (cid:88) i = 1 p i log ( q i / p i ) , When y i is the one hot vector ( the standard basis vector e i ) , ( 0 , . . . , 1 , . . . , 0 ) the composition results in ( KL - SM ) L KL − SM ( f , k ) = − log ( softmax ( f ) k ) = log (cid:16)(cid:88) exp ( f i ) (cid:17) − f k The usual explanation for the loss is the probabilistic interpretation 1 which applies when clas - siﬁers output , p ( x ) = ( p 1 ( x ) , . . . , p K ( x ) ) , an estimate of the probabilities of that the image x is in the class i . The probabilistic explanation is not valid , since ( i ) the KL - divergence is composed with a ( hard coded , not learned ) softmax , and ( ii ) there is no probabilistic interpretation of softmax ( f ) . Instead , we return to the maximum loss , ( 2 . 5 ) , and consider a smooth approximation of max ( v ) given by g (cid:15) ( v ) ( 7 . 1 ) L max (cid:15) ( f , k ) = g (cid:15) ( f ) − f k For example , if we take ( 7 . 2 ) g (cid:15) ( v ) = (cid:15) log (cid:32) K (cid:88) i = 1 exp ( f i / (cid:15) ) (cid:33) Then g (cid:15) is smooth for (cid:15) > 0 and g (cid:15) → max as (cid:15) → 0 , since max i v i ≤ g (cid:15) ( v ) ≤ (cid:15) log K + g (cid:15) ( v ) which follows from max i v i = log ( exp ( max i ( v i ) ) ) ≤ log (cid:32) K (cid:88) i = 1 exp ( f i ) (cid:33) ≤ log ( K exp ( max i v ) i ) = max ( v ) + log K . Using ( 7 . 1 ) with (cid:15) = 1 in ( 7 . 2 ) we recover ( KL - SM ) . We thus interpret the ( KL - SM ) loss as a smooth version of the classiﬁcation loss ( 2 . 5 ) . 1 see “An introduction to entropy , cross entropy and KL divergence in machine learning , ”“KL Divergence for Machine Learning , ” and “Light on Math Machine Learning : Intuitive Guide to Understanding KL Divergence” 10 ADAM M . OBERMAN - 4 - 2 0 2 4 - 3 - 2 - 1 0 1 2 3 Max Loss 0 . 1 0 . 1 0 . 5 0 . 5 1 1 1 1 . 5 1 . 5 1 . 5 2 2 2 . 5 2 . 5 - 4 - 2 0 2 4 - 3 - 2 - 1 0 1 2 3 LSE . 2 Loss 0 . 1 0 . 1 0 . 5 0 . 5 1 1 1 . 5 1 . 5 1 . 5 2 2 2 . 5 2 . 5 - 4 - 2 0 2 4 - 3 - 2 - 1 0 1 2 3 LSE - 1 Loss 0 . 5 1 1 1 . 5 1 . 5 2 2 2 2 . 5 2 . 5 Figure 3 . Classiﬁcation Losses : level sets of diﬀerent classiﬁcation losses , along with the classiﬁcation boundary . From left to right : max loss , LSE loss with (cid:15) = . 2 , LSE loss with (cid:15) = 1 . Original LogBarrier IFGSM Original LogBarrier IFGSM Figure 4 . Adversarial images for (cid:96) ∞ perturbations , generated by classiﬁcation ( LogBarrier ) and loss ( IFGSM ) adversarial attacks , compared to the original clean image , for MNIST ( left ) and CIFAR10 ( right ) . Where IFGSM has diﬃculties ﬁnding low distortion adversarial images , the classiﬁcation attack succeeds . 7 . 4 . Batch normalization . Deep models require batch normalization [ IS15 ] , which was intro - duced to solve the vanishing gradient problem , which arises when training using the ReLU activa - tion function . In the case that W j x i < 0 , for all i = 1 , . . . , m where x < 0 means that each component of the vector is negative , then σ ( W j x ) will always be zero , and ∇ w f ( x ) = 0 . This is a problem even if the inequality above holds only for a particular mini - batch . Then gradients are zero and the network cannot update the weights . Proper initialization can correct this problem , but for deep networks , batch normalization is still required for accuracy . Batch normalization consists of adding a layer which has a shift and scaling to make the outputs of a layer mean zero and variance one . Thus f ( x , w ) now depends on the statistics of S m , which is no longer consistent with the hypothesis class deﬁnition . It is possible to enlarges the hypothesis class to include statistics of the data , but this makes the problem ( EL ) more complicated . 8 . Adversarial attacks Robustness of f ( x , w ) refers to the lack of sensitivity of the model to small changes in the data . The existence of adversarial examples [ GSS14 ] indicates that the models are not robust . The objective of adversarial attacks is to ﬁnd the minimum norm vector which leads to misclassiﬁcation , ( 8 . 1 ) min { (cid:107) v (cid:107) | C ( f ( x + v ) ) (cid:54) = C ( ( f ( x ) ) } . The classiﬁcation attack ( 8 . 1 ) corresponds to a global , non - diﬀerentiable optimization problem . A more tractable problem is given by attacking a loss function . The choice of loss used for adversarial REGULARIZATION FOR MACHINE LEARNING 11 Figure 5 . Illustration of an adversarial attack on the classiﬁcation boundary . attacks can be a diﬀerent one from ( KL - SM ) loss used for training . The max - loss ( 2 . 5 ) , as well as a smoothed version , was used for adversarial attacks on deep neural networks in [ CW17 ] . Adversarial attacks are illustrated in Figure 4 . Write (cid:96) ( x ) = L ( f ( x ) , y ( x ) ) for the loss of the model . For a given adversarial distance , λ , the optimal loss attack on the image vector , x , is deﬁned as ( 8 . 2 ) max (cid:107) v (cid:107)≤ λ (cid:96) ( x + v ) . The solution , x + v , is the perturbed image vector within a given distance of x which maximally increases the loss . The Fast Gradient Sign Method ( FGSM ) [ GSS14 ] arises when attacks are measured in the ∞ - norm . It corresponds to a one step attack in the direction v given by the signed gradient vector v i = ∇ (cid:96) ( x ) i | ∇ (cid:96) ( x ) i | . ( 8 . 3 ) The attack direction ( 8 . 3 ) arises from linearization of the objective in ( 8 . 2 ) , which leads to ( 8 . 4 ) max (cid:107) v (cid:107) ∞ ≤ 1 v · ∇ (cid:96) ( x ) By inspection , the minimizer is the signed gradient vector , ( 8 . 3 ) , and the optimal value is (cid:107)∇ (cid:96) ( x ) (cid:107) 1 . When the 2 - norm is used in ( 8 . 4 ) , the optimal value is (cid:107)∇ (cid:96) ( x ) (cid:107) 2 and the optimal direction is the normalized gradient v = ∇ (cid:96) ( x ) (cid:107)∇ (cid:96) ( x ) (cid:107) 2 . ( 8 . 5 ) More generally , when a generic norm is used in ( 8 . 4 ) , the maximum of the linearized objective deﬁnes the dual norm [ BV04 , A . 1 . 6 ] . 8 . 1 . Classiﬁcation attacks . In [ FPO19 ] we implemented to the barrier method from constrained optimization [ NW06 ] to perform the classiﬁcation attack ( 8 . 1 ) . While attacks vectors are normally small enough to be invisible , for some images , gradient based attacks are visible . The barrier attack method generally performs as well as the best gradient based attacks , and on the most challenging examples results in smaller attack vectors , see Figure 5 . 9 . Regularization in DNN 9 . 1 . Statistical Learning Theory approach to generalization in Machine Learning . In practice , neural networks can perfectly ﬁt random labelings of the data [ ZBH + 16 ] . Thus , the Rademacher complexity of the neural network hypothesis class is one which means that general - ization of neural networks can not be established using the hypothesis space complexity approach . 12 ADAM M . OBERMAN The work of [ ZBH + 16 ] did not use data augmentation , which is typically used when training net - works to achieve better generalization . In the sequel we will study when data augmentation can be interpreted as a form of regularization , with the hope that it can be used to better understand generalization of deep neural networks . 9 . 2 . Regularization in DNN practice . Various forms of algorithmic regularization are im - plemented to improve generalization [ GBC16 , Chapter 7 ] . Examples follow , which will then be interpreted as variational regularization . Early work on shallow neural networks implemented Tychonoﬀ gradient regularization of the form ( EL - R ) directly : [ DLC92 ] showed that it improved generalization error . In [ Bis95 ] , it was shown that regularization can be achieved by adding random noise to the data x , which avoids the additional computational cost of explicit gradient regularization . Data augmentation [ LBB + 98 ] improves generalization using simple image transformations such as small rotations , cropping , or intensity or contrast adjustment . Dropout [ SHK + 14 ] consists of randomly , with small probability , changing some model weights to zero . Dropout is another form of regularization , but this time the transformation is a perturbation of the model weights . A Bayesian interpretation of dropout [ GG16 ] was later retracted [ HMG17 ] . Cutout [ DT17 ] consists of randomly sending a smaller square of pixels in the image to zero ( or one ) , and recently outperformed dropout on benchmarks . Mixup [ ZCDL17 ] consists of taking convex combinations of data points x i , x j and the corre - sponding labels y i , y j . Gaussian noise averaging has recently reappeared in deep neural networks [ LAG + 18 , LCZH18 , LCWC18 , CRK19 ] as a method to certify a network to be robust to adversarial perturbations . The averaged network adds random noise with variance σ 2 and chooses the most likely classiﬁer , C smooth ( x ) = arg max C ( x + η ) , η = N ( 0 , σ 2 ) , which requires many evaluations of the network . 10 . PDE regularization interpretation In this section we demonstrate how algorithmic or implicit regularization approaches to deep learning can be made explicit . The ﬁrst point we make is that we can rewrite the kernel loss as in the form of ( EL - R ) . This suggests that generalization of kernel methods could also be approached from the point of view of regularization . The regularization functional ( EL - R ) is non - parametric , which brings it closer to the approach we take for highly expressive deep neural networks . Next we show how various forms of implicit regularization used in DNNs can be made explicit . 10 . 1 . Kernels and regularization . Observe that ( EL - K ) corresponds to ( EL - W ) with a regular - ization term involving the Hilbert space norm of the weights . Mathematically , ( EL - K ) is a strange object , because it involves functions , weights and the Hilbert space norm . We will rewrite in the form of a regularized functional ( EL - R ) . Regularization interpretation of kernels is discussed in [ GJP95 , SS98 , Wah90 ] . Consider the case where K ( x 1 , x 2 ) = G ( x 1 − x 2 ) where G is real and symmetric , and the Fourier transform ˆ G ( y ) is a symmetric , positive function that goes to zero as y → ∞ . Then it can be shown that ( EL - K ) corresponds to ( EL - R ) with ( 10 . 1 ) R Ker ( f ) = (cid:90) R n (cid:107) ˆ f ( y ) (cid:107) 2 ˆ G ( y ) dy , where ˆ f is the Fourier transform of f . Refer to [ GJP95 ] . Example 10 . 1 . See [ SS98 ] for details . The Gaussian kernel corresponds to G ( x ) = exp ( −(cid:107) x (cid:107) 2 / 2 ) , ˆ G ( y ) = C exp ( −(cid:107) y (cid:107) 2 / 2 ) . REGULARIZATION FOR MACHINE LEARNING 13 In this case , the regularization is given by R Ker ( f ) = ∞ (cid:88) n = 0 1 2 n n ! (cid:107) ( ∇ ) n f (cid:107) 2 L 2 . Thus we see that kernel methods can be interpreted as regularized functional ( EL - R ) with Fourier regularization . 10 . 2 . SGD and regularization . Some authors seek to explain generalization using the prop - erties of the SGD training algorithm , for example [ HRS16 ] . We consider two examples in the overparameterized setting , where the number of parameters can be greater than the number of data points . The ﬁrst example shows that without regularization , SDG can train to zero loss and fail to generalize . The second example shows that implicit regularization by smoothing the function class can lead to generalization . The point of these examples to show that SGD does not regularize without additional implicit regularization . Example 10 . 2 . Consider the dataset S m , deﬁned in ( 2 . 1 ) , with x i i . i . d . samples from the uniformly probability ρ 1 ( x ) on [ 0 , 1 ] 2 , the two dimensional unit square . Set y ( x ) = ρ 1 ( x ) = (cid:40) 1 , x ∈ [ 0 , 1 ] 2 0 , otherwise Consider the quadratic loss ( EL - Q ) with the overparameterized hypothesis space H δ = (cid:40) f ( x , w ) = m (cid:88) i = 1 w i δ x i ( x ) (cid:41) where δ x i ( x ) = 1 if x = x i , and zero otherwise . In this case , it is clear that ( i ) setting w ∗ i = 1 makes the empirical loss zero , and ( ii ) the generalization error is large , since on a new data point , x , f ( x , w ∗ ) = 0 (cid:54) = y ( x ) . However , if we replace H δ with a smooth hypothesis class H G = (cid:40) f ( x , w ) = m (cid:88) i = 1 w i G ( x i − x ) (cid:41) where G is a smooth approximation of the delta function ( for example a Gaussian kernel as in Example 10 . 1 ) , then it is possible to generalize better . Adding a parameter representing the width of the kernel allows us to tune for the optimal width , leading to better regularization for a given sample size . Learning guarantees for classiﬁcation using kernels are usually interpreted using the notion of margin [ MRT18 , Ch 6 ] . However , this simple example illustrated that the interpretation ( EL - R ) can also be used , with the kernel width corresponding to the regularization parameter λ . 10 . 3 . Image transformation and implicit regularization . Consider an abstract data trans - formation , x (cid:55)→ T ( x ) which transforms the image x . This could be data augmentation , random cutout , adding random gaussian noise to an image , or an adversarial perturbation . The data transformation replaces ( EL ) with the data augmented version ( EL - A ) min f ∈H 1 m m (cid:88) i = 1 L ( f ( T ( x i ) ) , y i ) 14 ADAM M . OBERMAN which we can rewrite as min f ∈H L S m ( f ) + R T ( f ( x i ) ) where ( RT ) R T ( f ( x i ) ) = L ( f ( T ( x i ) ) , y i ) − L ( f ( x i ) , y i ) Here the regularization is implicit , and the strength of the regularization is controlled by making the transformation T closer to the identity . 10 . 4 . Data augmentation . Here show how adding noise leads to regularization , following [ Bis95 ] . Lemma 10 . 3 . The augmented loss problem ( EL - A ) with quadratic loss and additive noise ( 10 . 2 ) T ( x ) = x + v , v random , E ( v ) = 0 , E ( v i v j ) = σ 2 is equivalent to the regularized loss problem ( EL - R ) with ( 10 . 3 ) R noise ( f ) = σ 2 m m (cid:88) i = 1 (cid:18) f 2 x + ( f − y ) f xx + σ 2 4 f 2 xx (cid:19) Proof . For clarity we treat the function as one dimensional . A similar calculation can be done in the higher dimensional case . Apply the Taylor expansion f ( x + v ) = f ( x ) + vf x + 1 2 v 2 f xx + O ( v 3 ) to the quadratic loss L ( f , y ) = ( f ( x + v ) − y ) 2 . Keeping only the lowest order terms , we have ( f ( x + v ) − y ) 2 = ( f ( x ) − y ) 2 + 2 ( f x v + 1 2 v 2 f xx ) ( f ( x ) − y ) + ( f x v + 1 2 v 2 f xx ) 2 Taking expectations and applying ( 10 . 2 ) to drop the terms with odd powers of v gives ( 10 . 3 ) . (cid:3) Remark 10 . 4 . Note that this regularization is empirical , unlike the previous ones . So the regulariza - tion may not lead to a well posed problem . A more technical argument could lead to a well - deﬁned functional , where the density depends on the sampled density convolved with a Gaussian . 10 . 5 . Adversarial training . In [ FCAO18 ] it was shown that adversarial training , T ( x ) = x + λv with attack vector v given by ( 8 . 3 ) or by ( 8 . 5 ) can be interpreted as Total Variation regularization of the loss , R AT ( f ) = λ (cid:107)L (cid:48) ( f ) ∇ f ( x ) (cid:107) ∗ . A diﬀerent scaling was considered in [ FO19 ] , which corresponds to adversarial training with T ( x ) = x + λ ∇L ( x ) . The corresponding regularization is Tychonoﬀ regularization of the loss , R Tyc ( f ) = λ (cid:107)L (cid:48) ( f ) ∇ f ( x ) (cid:107) 22 which was implemented using ﬁnite diﬀerences rather than backpropagation . Double backprop - agation is computationally expensive because the loss function depends on ∇ x f ( x , w ) , training requires a mixed derivative ∇ x ( (cid:107)∇ x f (cid:107) 2 ) . For deep networks , the mixed derivatives leads to very large matrix multiplication . Scaling the regularization term by λ and allowed for robustness com - parable to many steps of adversarial training , at the cost of a single step . REGULARIZATION FOR MACHINE LEARNING 15 11 . Conclusions We studied the regularization approach to generalization and robustness in deep learning for image classiﬁcation . Deep learning models lack the theoretical foundations of traditional machine learning methods , in particular dimension independent sample complexity bounds of the form ( 6 . 2 ) . We showed that kernel methods have a regularization interpretation , which suggests that the same bounds can be obtained by considering the regularized functional ( EL - R ) . The deep learning hypothesis space is too expressive for hypothesis space complexity bounds such as ( 6 . 2 ) to be obtained . However , that argument ignores the data augmentation , which is known to be a form of regularization . We showed that many other modern data augmentation methods , including adversarial training , can also be interpreted as regularization . The regularized loss ( RT ) may be more amenable to analysis than ( EL - A ) , provided the regularizer is enough to make the problem mathematically well - posed . Examples coming from data augmentation and adversarial training to show that ( RT ) can be represented as an explicit PDE regularized model . The regularization interpretation makes a link between traditional machine learning methods and deep learning models , but requires nontraditional interpretations in both settings . Prov - ing generalization for nonparametric models with Fourier regularization ( 10 . 1 ) could be ﬁrst step towards generalization bounds for regularized neural networks . While the regularization interpre - tation ( RT ) is empirical , data augmentation rich enough ( such as adding Gaussian noise ) to make the empirical measure supported on the full data distribution could lead to a well - posed global regularization . References [ ACB17 ] Martin Arjovsky , Soumith Chintala , and L´eon Bottou . Wasserstein gan , 2017 . [ AK06 ] Gilles Aubert and Pierre Kornprobst . Mathematical problems in image processing : Partial Diﬀerential Equations and the Calculus of Variations , volume 147 . Springer Science & Business Media , 2006 . [ BCN16 ] L´eon Bottou , Frank E Curtis , and Jorge Nocedal . Optimization methods for large - scale machine learn - ing . arXiv preprint arXiv : 1606 . 04838 , 2016 . [ BE02 ] Olivier Bousquet and Andr´e Elisseeﬀ . Stability and generalization . Journal of Machine Learning Re - search , 2 : 499 – 526 , 2002 . [ Bis95 ] Chris M Bishop . Training with noise is equivalent to tikhonov regularization . Neural computation , 7 ( 1 ) : 108 – 116 , 1995 . [ Bis06 ] Christopher M Bishop . Pattern recognition and machine learning . springer , 2006 . [ BV04 ] Stephen Boyd and Lieven Vandenberghe . Convex Optimization . Cambridge University press , 2004 . [ Che66 ] Elliott Ward Cheney . Introduction to approximation theory . McGraw - Hill , 1966 . [ CRK19 ] Jeremy M Cohen , Elan Rosenfeld , and J Zico Kolter . Certiﬁed adversarial robustness via randomized smoothing . arXiv preprint arXiv : 1902 . 02918 , 2019 . [ CS01 ] Koby Crammer and Yoram Singer . On the algorithmic implementation of multiclass kernel - based vector machines . Journal of machine learning research , 2 ( Dec ) : 265 – 292 , 2001 . [ CW17 ] Nicholas Carlini and David A . Wagner . Towards evaluating the robustness of neural networks . In 2017 IEEE Symposium on Security and Privacy , SP 2017 , San Jose , CA , USA , May 22 - 26 , 2017 , pages 39 – 57 , 2017 . [ DLC92 ] Harris Drucker and Yann Le Cun . Improving generalization performance using double backpropagation . IEEE Transactions on Neural Networks , 3 ( 6 ) : 991 – 997 , 1992 . [ DT17 ] Terrance Devries and Graham W . Taylor . Improved regularization of convolutional neural networks with cutout . CoRR , abs / 1708 . 04552 , 2017 . [ FCAO18 ] Chris Finlay , Jeﬀ Calder , Bilal Abbasi , and Adam Oberman . Lipschitz regularized deep neural networks generalize and are adversarially robust , 2018 . [ FO19 ] Chris Finlay and Adam M Oberman . Scaleable input gradient regularization for adversarial robustness , 2019 . [ FPO19 ] Chris Finlay , Aram - Alexandre Pooladian , and Adam M . Oberman . The logbarrier adversarial attack : making eﬀective use of decision boundary information , 2019 . [ GBC16 ] Ian Goodfellow , Yoshua Bengio , and Aaron Courville . Deep learning . MIT press , 2016 . [ GG16 ] Yarin Gal and Zoubin Ghahramani . Dropout as a bayesian approximation : Representing model un - certainty in deep learning . In international conference on machine learning , pages 1050 – 1059 , 2016 . 16 ADAM M . OBERMAN [ GJP95 ] Federico Girosi , Michael Jones , and Tomaso Poggio . Regularization theory and neural networks archi - tectures . Neural computation , 7 ( 2 ) : 219 – 269 , 1995 . [ Goh17 ] Gabriel Goh . Why momentum really works . Distill , 2017 . [ GPAM + 14 ] Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . Generative adversarial nets . In Advances in neural information processing systems , pages 2672 – 2680 , 2014 . [ GSS14 ] Ian J . Goodfellow , Jonathon Shlens , and Christian Szegedy . Explaining and harnessing adversarial examples . CoRR , abs / 1412 . 6572 , 2014 . [ HMG17 ] Jiri Hron , Alexander G de G Matthews , and Zoubin Ghahramani . Variational gaussian dropout is not bayesian . arXiv preprint arXiv : 1711 . 02989 , 2017 . [ HRS16 ] Moritz Hardt , Benjamin Recht , and Yoram Singer . Train faster , generalize better : Stability of stochas - tic gradient descent . In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48 , ICML’16 , pages 1225 – 1234 . JMLR . org , 2016 . [ IS15 ] Sergey Ioﬀe and Christian Szegedy . Batch normalization : Accelerating deep network training by re - ducing internal covariate shift . arXiv preprint arXiv : 1502 . 03167 , 2015 . [ LAG + 18 ] Mathias Lecuyer , Vaggelis Atlidakis , Roxana Geambasu , Daniel Hsu , and Suman Jana . Certiﬁed ro - bustness to adversarial examples with diﬀerential privacy . arXiv preprint arXiv : 1802 . 03471 , 2018 . [ LBB + 98 ] Yann LeCun , L´eon Bottou , Yoshua Bengio , Patrick Haﬀner , et al . Gradient - based learning applied to document recognition . Proceedings of the IEEE , 86 ( 11 ) : 2278 – 2324 , 1998 . [ LCWC18 ] Bai Li , Changyou Chen , Wenlin Wang , and Lawrence Carin . Certiﬁed adversarial robustness with additive gaussian noise , 2018 . [ LCZH18 ] Xuanqing Liu , Minhao Cheng , Huan Zhang , and Cho - Jui Hsieh . Towards robust neural networks via random self - ensemble . In Proceedings of the European Conference on Computer Vision ( ECCV ) , pages 369 – 385 , 2018 . [ LO19 ] Maxime Laborde and Adam M . Oberman . A lyapunov analysis for accelerated gradient methods : From deterministic to stochastic case , 2019 . [ MRT18 ] Mehryar Mohri , Afshin Rostamizadeh , and Ameet Talwalkar . Foundations of machine learning . MIT press , 2018 . [ Nes13 ] Yurii Nesterov . Introductory lectures on convex optimization : A basic course , volume 87 . Springer Science & Business Media , 2013 . [ NW06 ] Jorge Nocedal and Stephen Wright . Numerical optimization . Springer Science & Business Media , 2006 . [ OP19 ] Adam M . Oberman and Mariana Prazeres . Stochastic gradient descent with polyak’s learning rate , 2019 . [ ROF92 ] Leonid I Rudin , Stanley Osher , and Emad Fatemi . Nonlinear Total Variation based noise removal algorithms . Physica D : nonlinear phenomena , 60 ( 1 - 4 ) : 259 – 268 , 1992 . [ Sap06 ] Guillermo Sapiro . Geometric partial diﬀerential equations and image analysis . Cambridge university press , 2006 . [ SBC14 ] Weijie Su , Stephen Boyd , and Emmanuel Candes . A diﬀerential equation for modeling nesterov’s accelerated gradient method : Theory and insights . In Advances in Neural Information Processing Systems , pages 2510 – 2518 , 2014 . [ SHK + 14 ] Nitish Srivastava , Geoﬀrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov . Dropout : A simple way to prevent neural networks from overﬁtting . The Journal of Machine Learning Research , 15 ( 1 ) : 1929 – 1958 , 2014 . [ SS98 ] Alex J Smola and Bernhard Sch¨olkopf . From regularization operators to support vector kernels . In Advances in Neural information processing systems , pages 343 – 349 , 1998 . [ SSBD14 ] Shai Shalev - Shwartz and Shai Ben - David . Understanding Machine Learning : From Theory to Algo - rithms . Cambridge University Press , 2014 . [ TA77 ] AN Tikhonov and V Ya Arsenin . Solutions of Ill - Posed Problems . Winston and Sons , New York , 1977 . [ Wah90 ] Grace Wahba . Spline models for observational data , volume 59 . Siam , 1990 . [ XM12 ] Huan Xu and Shie Mannor . Robustness and generalization . Machine Learning , 86 ( 3 ) : 391 – 423 , 2012 . [ Z + 04 ] Tong Zhang et al . Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization . The Annals of Statistics , 32 ( 1 ) : 56 – 85 , 2004 . [ ZBH + 16 ] Chiyuan Zhang , Samy Bengio , Moritz Hardt , Benjamin Recht , and Oriol Vinyals . Understanding deep learning requires rethinking generalization . CoRR , abs / 1611 . 03530 , 2016 . [ ZCDL17 ] Hongyi Zhang , Moustapha Ciss´e , Yann N . Dauphin , and David Lopez - Paz . mixup : Beyond empirical risk minimization . CoRR , abs / 1710 . 09412 , 2017 . E - mail address : adam . oberman @ mcgill . ca