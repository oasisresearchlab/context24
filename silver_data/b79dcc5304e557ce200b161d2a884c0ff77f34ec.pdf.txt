NeuSpell : A Neural Spelling Correction Toolkit Sai Muralidhar Jayanthi , Danish Pruthi , Graham Neubig Language Technologies Institute Carnegie Mellon University { sjayanth , ddanish , gneubig } @ cs . cmu . edu Abstract We introduce NeuSpell , an open - source toolkit for spelling correction in English . Our toolkit comprises ten different models , and benchmarks them on naturally occurring mis - spellings from multiple sources . We ﬁnd that many systems do not adequately leverage the context around the misspelt token . To remedy this , ( i ) we train neural models using spelling errors in context , synthetically constructed by reverse engineering isolated misspellings ; and ( ii ) use contextual representations . By training on our synthetic examples , correction rates im - prove by 9 % ( absolute ) compared to the case when models are trained on randomly sampled character perturbations . Using richer contex - tual representations boosts the correction rate by another 3 % . Our toolkit enables practition - ers to use our proposed and existing spelling correction systems , both via a uniﬁed com - mand line , as well as a web interface . Among many potential applications , we demonstrate the utility of our spell - checkers in combating adversarial misspellings . The toolkit can be ac - cessed at neuspell . github . io . 1 1 Introduction Spelling mistakes constitute the largest share of errors in written text ( Wilbur et al . , 2006 ; Flor and Futagi , 2012 ) . Therefore , spell checkers are ubiquitous , forming an integral part of many ap - plications including search engines , productivity and collaboration tools , messaging platforms , etc . However , many well performing spelling correc - tion systems are developed by corporations , trained on massive proprietary user data . In contrast , many freely available off - the - shelf correctors such as En - chant ( Thomas , 2010 ) , GNU Aspell ( Atkinson , 2019 ) , and JamSpell ( Ozinov , 2019 ) , do not ef - fectively use the context of the misspelled word . 1 Code and pretrained models are available at : https : / / github . com / neuspell / neuspell Figure 1 : Our toolkit’s web and command line inter - face for spelling correction . For instance , they fail to disambiguate (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) thaught to taught or thought based on the context : “Who (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) thaught you calculus ? ” versus “I never (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) thaught I would be awarded the fellowship . ” In this paper , we describe our spelling correction toolkit , which comprises of several neural mod - els that accurately capture context around the mis - spellings . To train our neural spell correctors , we ﬁrst curate synthetic training data for spelling cor - rection in context , using several text noising strate - gies . These strategies use a lookup table for word - level noising , and a context - based character - level confusion dictionary for character - level noising . To populate this lookup table and confusion matrix , we harvest isolated misspelling - correction pairs from various publicly available sources . a r X i v : 2010 . 11085v1 [ c s . C L ] 21 O c t 2020 Further , we investigate effective ways to incor - porate contextual information : we experiment with contextual representations from pretrained models such as ELMo ( Peters et al . , 2018 ) and BERT ( De - vlin et al . , 2018 ) and compare their efﬁcacies with existing neural architectural choices ( § 5 . 1 ) . Lastly , several recent studies have shown that many state - of - the - art neural models developed for a variety of Natural Language Processing ( NLP ) tasks easily break in the presence of natural or syn - thetic spelling errors ( Belinkov and Bisk , 2017 ; Ebrahimi et al . , 2017 ; Pruthi et al . , 2019 ) . We determine the usefulness of our toolkit as a counter - measure against character - level adversarial attacks ( § 5 . 2 ) . We ﬁnd that our models are better defenses to adversarial attacks than previously proposed spell checkers . We believe that our toolkit would encourage practitioners to incorporate spelling cor - rection systems in other NLP applications . Model CorrectionRates Time per sentence ( milliseconds ) A SPELL ( Atkinson , 2019 ) 48 . 7 7 . 3 ∗ J AM S PELL ( Ozinov , 2019 ) 68 . 9 2 . 6 ∗ CHAR - CNN - LSTM ( Kim et al . , 2015 ) 75 . 8 4 . 2 SC - LSTM ( Sakaguchi et al . , 2016 ) 76 . 7 2 . 8 CHAR - LSTM - LSTM ( Li et al . , 2018 ) 77 . 3 6 . 4 B ERT ( Devlin et al . , 2018 ) 79 . 1 7 . 1 SC - LSTM + E LMO ( input ) 79 . 8 15 . 8 + E LMO ( output ) 78 . 5 16 . 3 + B ERT ( input ) 77 . 0 6 . 7 + B ERT ( output ) 76 . 0 7 . 2 Table 1 : Performance of different correctors in the NeuSpell toolkit on the BEA - 60K dataset with real - world spelling mistakes . ∗ indicates evaluation on a CPU ( for others we use a GeForce RTX 2080 Ti GPU ) . 2 Models in NeuSpell Our toolkit offers ten different spelling correction models , which include : ( i ) two off - the - shelf non - neural models , ( ii ) four published neural models for spelling correction , ( iii ) four of our extensions . The details of ﬁrst six systems are following : • GNU Aspell ( Atkinson , 2019 ) : It uses a com - bination of metaphone phonetic algorithm , 2 Ispell’s near miss strategy , 3 and a weighted edit distance metric to score candidate words . • JamSpell ( Ozinov , 2019 ) : It uses a variant of the SymSpell algorithm , 4 and a 3 - gram lan - guage model to prune word - level corrections . 2 http : / / aspell . net / metaphone / 3 https : / / en . wikipedia . org / wiki / Ispell 4 https : / / github . com / wolfgarbe / SymSpell • SC - LSTM ( Sakaguchi et al . , 2016 ) : It corrects misspelt words using semi - character represen - tations , fed through a bi - LSTM network . The semi - character representations are a concate - nation of one - hot embeddings for the ( i ) ﬁrst , ( ii ) last , and ( iii ) bag of internal characters . • CHAR - LSTM - LSTM ( Li et al . , 2018 ) : The model builds word representations by passing its individual characters to a bi - LSTM . These representations are further fed to another bi - LSTM trained to predict the correction . • CHAR - CNN - LSTM ( Kim et al . , 2015 ) : Similar to the previous model , this model builds word - level representations from individual charac - ters using a convolutional network . • B ERT ( Devlin et al . , 2018 ) : The model uses a pre - trained transformer network . We aver - age the sub - word representations to obtain the word representations , which are further fed to a classiﬁer to predict its correction . To better capture the context around a misspelt token , we extend the SC - LSTM model by aug - menting it with deep contextual representations from pre - trained ELMo and BERT . Since the best point to integrate such embeddings might vary by task ( Peters et al . , 2018 ) , we append them either to semi - character embeddings before feeding them to the biLSTM or to the biLSTM’s output . Cur - rently , our toolkit provides four such trained mod - els : ELMo / BERT tied at input / output with a semi - character based bi - LSTM model . Implementation Details Neural models in NeuSpell are trained by posing spelling correction as a sequence labeling task , where a correct word is marked as itself and a misspelt token is labeled as its correction . Out - of - vocabulary labels are marked as UNK . For each word in the input text sequence , models are trained to output a probability distribution over a ﬁnite vocabulary using a softmax layer . We set the hidden size of the bi - LSTM network in all models to 512 and use { 50 , 100 , 100 , 100 } sized convolution ﬁlters with lengths { 2 , 3 , 4 , 5 } re - spectively in CNNs . We use a dropout of 0 . 4 on the bi - LSTM’s outputs and train the models using cross - entropy loss . We use the BertAdam 5 opti - mizer for models with a BERT component and the 5 github . com / cedrickchee / pytorch - pretrained - BERT Adam ( Kingma and Ba , 2014 ) optimizer for the remainder . These optimizers are used with default parameter settings . We use a batch size of 32 ex - amples , and train with a patience of 3 epochs . During inference , we ﬁrst replace UNK predic - tions with their corresponding input words and then evaluate the results . We evaluate models for accu - racy ( percentage of correct words among all words ) and word correction rate ( percentage of misspelt to - kens corrected ) . We use AllenNLP 6 and Hugging - face 7 libraries to use ELMo and BERT respectively . All neural models in our toolkit are implemented using the Pytorch library ( Paszke et al . , 2017 ) , and are compatible to run on both CPU and GPU en - vironments . Performance of different models are presented in Table 1 . 3 Synthetic Training Datasets Due to scarcity of available parallel data for spelling correction , we noise sentences to gener - ate misspelt - correct sentence pairs . We use 1 . 6 M sentences from the one billion word benchmark ( Chelba et al . , 2013 ) dataset as our clean corpus . Using different noising strategies from existing lit - erature , we noise ∼ 20 % of the tokens in the clean corpus by injecting spelling mistakes in each sen - tence . Below , we brieﬂy describe these strategies . R ANDOM : Following Sakaguchi et al . ( 2016 ) , this noising strategy involves four character - level operations : permute , delete , insert and replace . We manipulate only the internal characters of a word . The permute operation jumbles a pair of consecu - tive characters , delete operation randomly deletes one of the characters , insert operation randomly inserts an alphabet and replace operation swaps a character with a randomly selected alphabet . For every word in the clean corpus , we select one of the four operations with 0 . 1 probability each . We do not modify words of length three or smaller . W ORD : Inspired from Belinkov and Bisk ( 2017 ) , we swap a word with its noised counterpart from a pre - built lookup table . We collect 109 K misspelt - correct word pairs for 17 K popular English words from a variety of public sources . 8 For every word in the clean corpus , we replace it by a random misspelling ( with a probability of 0 . 3 ) 6 allennlp . org / elmo 7 huggingface . co / transformers / model doc / bert . html 8 https : / / en . wikipedia . org / , dcs . bbk . ac . uk , norvig . com , cor - pus . mml . cam . ac . uk / efcamdat sampled from all the misspellings associated with that word in the lookup table . Words not present in the lookup table are left as is . P ROB : Recently , Piktus et al . ( 2019 ) released a corpus of 20 M correct - misspelt word pairs , gener - ated from logs of a search engine . 9 We use this cor - pus to construct a character - level confusion dictio - nary where the keys are (cid:104) character , context (cid:105) pairs and the values are a list of potential character re - placements with their frequencies . This dictionary is subsequently used to sample character - level er - rors in a given context . We use a context of 3 characters , and backoff to 2 , 1 , and 0 characters . Notably , due to the large number of unedited char - acters in the corpus , the most probable replacement will often be the same as the source character . P ROB + W ORD : For this strategy , we simply con - catenate the training data obtained from both W ORD and P ROB strategies . 4 Evaluation Benchmarks Natural misspellings in context Many publicly available spell - checkers correctors evaluate on iso - lated misspellings ( Atkinson , 2019 ; Mitton , na ; Norvig , 2016 ) . Whereas , we evaluate our systems using misspellings in context , by using publicly available datasets for the task of Grammatical Er - ror Correction ( GEC ) . Since the GEC datasets are annotated for various types of grammatical mis - takes , we only sample errors of SPELL type . Among the GEC datasets in BEA - 2019 shared task 10 , the Write & Improve ( W & I ) dataset along with the LOCNESS dataset are a collection of texts in English ( mainly essays ) written by language learners with varying proﬁciency levels ( Bryant et al . , 2019 ; Granger , 1998 ) . The First Certiﬁcate in English ( FCE ) dataset is another collection of essays in English written by non - native learners tak - ing a language assessment exam ( Yannakoudakis et al . , 2011 ) and the Lang - 8 dataset is a collection of English texts from Lang - 8 online language learn - ing website ( Mizumoto et al . , 2011 ; Tajiri et al . , 2012 ) . We combine data from these four sources to create the BEA - 60 K test set with nearly 70K spelling mistakes ( 6 . 8 % of all tokens ) in 63044 sentences . The JHU FLuency - Extended GUG Corpus ( JFLEG ) dataset ( Napoles et al . , 2017 ) is another 9 https : / / github . com / facebookresearch / moe 10 www . cl . cam . ac . uk / research / nl / bea2019st / Spelling correction systems in NeuSpell ( Word - Level Accuracy / Correction Rate ) Synthetic Natural Ambiguous WORD - TEST PROB - TEST BEA - 60K JFLEG BEA - 4660 BEA - 322 A SPELL ( Atkinson , 2019 ) 43 . 6 / 16 . 9 47 . 4 / 27 . 5 68 . 0 / 48 . 7 73 . 1 / 55 . 6 68 . 5 / 10 . 1 61 . 1 / 18 . 9 J AM S PELL ( Ozinov , 2019 ) 90 . 6 / 55 . 6 93 . 5 / 68 . 5 97 . 2 / 68 . 9 98 . 3 / 74 . 5 98 . 5 / 72 . 9 96 . 7 / 52 . 3 CHAR - CNN - LSTM ( Kim et al . , 2015 ) 97 . 0 / 88 . 0 96 . 5 / 84 . 1 96 . 2 / 75 . 8 97 . 6 / 80 . 1 97 . 5 / 82 . 7 94 . 5 / 57 . 3 SC - LSTM ( Sakaguchi et al . , 2016 ) 97 . 6 / 90 . 5 96 . 6 / 84 . 8 96 . 0 / 76 . 7 97 . 6 / 81 . 1 97 . 3 / 86 . 6 94 . 9 / 65 . 9 CHAR - LSTM - LSTM ( Li et al . , 2018 ) 98 . 0 / 91 . 1 97 . 1 / 86 . 6 96 . 5 / 77 . 3 97 . 6 / 81 . 6 97 . 8 / 84 . 0 95 . 4 / 63 . 2 B ERT ( Devlin et al . , 2018 ) 98 . 9 / 95 . 3 98 . 2 / 91 . 5 93 . 4 / 79 . 1 97 . 9 / 85 . 0 98 . 4 / 92 . 5 96 . 0 / 72 . 1 SC - LSTM + E LMO ( input ) 98 . 5 / 94 . 0 97 . 6 / 89 . 1 96 . 5 / 79 . 8 97 . 8 / 85 . 0 98 . 2 / 91 . 9 96 . 1 / 69 . 7 + E LMO ( output ) 97 . 9 / 91 . 4 97 . 0 / 86 . 1 98 . 0 / 78 . 5 96 . 4 / 76 . 7 97 . 9 / 88 . 1 95 . 2 / 63 . 2 + B ERT ( input ) 98 . 7 / 94 . 3 97 . 9 / 89 . 5 96 . 2 / 77 . 0 97 . 8 / 83 . 9 98 . 4 / 90 . 2 96 . 0 / 67 . 8 + B ERT ( output ) 98 . 1 / 92 . 3 97 . 2 / 86 . 9 95 . 9 / 76 . 0 97 . 6 / 81 . 0 97 . 8 / 88 . 1 95 . 1 / 67 . 2 Table 2 : Performance of different models in NeuSpell on natural , synthetic , and ambiguous test sets . All models are trained using P ROB + W ORD noising strategy . collection of essays written by English learners with different ﬁrst languages . This dataset con - tains 2 K spelling mistakes ( 6 . 1 % of all tokens ) in 1601 sentences . We use the BEA - 60 K and JFLEG datasets only for the purposes of evaluation , and do not use them in training process . Synthetic misspellings in context From the two noising strategies described in § 3 , we additionally create two test sets : WORD - TEST and PROB - TEST . Each of these test sets contain around 1 . 2 M spelling mistakes ( 19 . 5 % of all tokens ) in 273 K sentences . Ambiguous misspellings in context Besides the natural and synthetic test sets , we create a chal - lenge set of ambiguous spelling mistakes , which require additional context to unambiguously cor - rect them . For instance , the word (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) whitch can be corrected to “witch” or “which” depending upon the context . Simliarly , for the word (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) begger , both “bigger” or “beggar” can be appropriate corrections . To create this challenge set , we select all such mis - spellings which are either 1 - edit distance away from two ( or more ) legitimate dictionary words , or have the same phonetic encoding as two ( or more ) dictionary words . Using these two criteria , we sometimes end up with inﬂections of the same word , hence we use a stemmer and lemmatizer from the NLTK library to weed those out . Finally , we manually prune down the list to 322 sentences , with one ambiguous mistake per sentence . We refer to this set as BEA - 322 . We also create another larger test set where we ar - tiﬁcially misspell two different words in sentences to their common ambiguous misspelling . This pro - cess results in a set with 4660 misspellings in 4660 sentences , and is thus referred as BEA - 4660 . No - tably , for both these ambiguous test sets , a spelling correction system that doesn’t use any context in - formation can at best correct 50 % of the mistakes . 5 Results and Discussion 5 . 1 Spelling Correction We evaluate the 10 spelling correction systems in NeuSpell across 6 different datasets ( see Table 2 ) . Among the spelling correction systems , all the neu - ral models in the toolkit are trained using synthetic training dataset , using the P ROB + W ORD synthetic data . We use the recommended conﬁgurations for Aspell and Jamspell , but do not ﬁne - tune them on our synthetic dataset . In all our experiments , vo - cabulary of neural models is restricted to the top 100K frequent words of the clean corpus . We observe that although off - the - shelf checker Jamspell leverages context , it is often inadequate . We see that models comprising of deep contextual representations consistently outperform other exist - ing neural models for the spelling correction task . We also note that the BERT model performs con - sistently well across all our benchmarks . For the ambiguous BEA - 322 test set , we manually evalu - ated corrections from Grammarly—a professional paid service for assistive writing . 11 We found that our best model for this set , i . e . BERT , outperforms corrections from Grammarly ( 72 . 1 % vs 71 . 4 % ) We attribute the success of our toolkit’s well per - forming models to ( i ) better representations of the context , from large pre - trained models ; ( ii ) swap invariant semi - character representations ; and ( iii ) training models with synthetic data consisting of noise patterns from real - world misspellings . We follow up these results with an ablation study to understand the role of each noising strategy ( Ta - 11 Retrieved on July 13 , 2020 . Sentiment Analysis ( 1 - char attack / 2 - char attack ) Defenses No Attack Swap Drop Add Key All Word - Level Models SC - LSTM ( Pruthi et al . , 2019 ) 79 . 3 78 . 6 / 78 . 5 69 . 1 / 65 . 3 65 . 0 / 59 . 2 69 . 6 / 65 . 6 63 . 2 / 52 . 4 SC - LSTM + E LMO ( input ) ( F ) 79 . 6 77 . 9 / 77 . 2 72 . 2 / 69 . 2 65 . 5 / 62 . 0 71 . 1 / 68 . 3 64 . 0 / 58 . 0 Char - Level Models SC - LSTM ( Pruthi et al . , 2019 ) 70 . 3 65 . 8 / 62 . 9 58 . 3 / 54 . 2 54 . 0 / 44 . 2 58 . 8 / 52 . 4 51 . 6 / 39 . 8 SC - LSTM + E LMO ( input ) ( F ) 70 . 9 67 . 0 / 64 . 6 61 . 2 / 58 . 4 53 . 0 / 43 . 0 58 . 1 / 53 . 3 51 . 5 / 41 . 0 Word + Char Models SC - LSTM ( Pruthi et al . , 2019 ) 80 . 1 79 . 0 / 78 . 7 69 . 5 / 65 . 7 64 . 0 / 59 . 0 66 . 0 / 62 . 0 61 . 5 / 56 . 5 SC - LSTM + E LMO ( input ) ( F ) 80 . 6 79 . 4 / 78 . 8 73 . 1 / 69 . 8 66 . 0 / 58 . 0 72 . 2 / 68 . 7 64 . 0 / 54 . 5 Table 3 : We evaluate spelling correction systems in NeuSpell against adversarial misspellings . ble 4 ) . 12 For each of the 5 models evaluated , we observe that models trained with P ROB noise out - perform those trained with W ORD or R ANDOM noises . Across all the models , we further observe that using P ROB + W ORD strategy improves correc - tion rates by at least 10 % in comparison to R AN - DOM noising . Spelling Correction ( Word - Level Accuracy / Correction Rate ) Model Train Noise Natural test sets BEA - 60 K JFLEG CHAR - CNN - LSTM ( Kim et al . , 2015 ) R ANDOM 95 . 9 / 66 . 6 97 . 4 / 69 . 3 W ORD 95 . 9 / 70 . 2 97 . 4 / 74 . 5 P ROB 96 . 1 / 71 . 4 97 . 4 / 77 . 3 P ROB + W ORD 96 . 2 / 75 . 5 97 . 4 / 79 . 2 SC - LSTM ( Sakaguchi et al . , 2016 ) R ANDOM 96 . 1 / 64 . 2 97 . 4 / 66 . 2 W ORD 95 . 4 / 68 . 3 97 . 4 / 73 . 7 P ROB 95 . 7 / 71 . 9 97 . 2 / 75 . 9 P ROB + W ORD 95 . 9 / 76 . 0 97 . 6 / 80 . 3 CHAR - LSTM - LSTM ( Li et al . , 2018 ) R ANDOM 96 . 2 / 67 . 1 97 . 6 / 70 . 2 W ORD 96 . 0 / 69 . 8 97 . 5 / 74 . 6 P ROB 96 . 3 / 73 . 5 97 . 4 / 78 . 2 P ROB + W ORD 96 . 3 / 76 . 4 97 . 5 / 80 . 2 BERT ( Devlin et al . , 2018 ) R ANDOM 96 . 9 / 66 . 3 98 . 2 / 74 . 4 W ORD 95 . 3 / 61 . 1 97 . 3 / 70 . 4 P ROB 96 . 2 / 73 . 8 97 . 8 / 80 . 5 P ROB + W ORD 96 . 1 / 77 . 1 97 . 8 / 82 . 4 SC - LSTM R ANDOM 96 . 9 / 69 . 1 97 . 8 / 73 . 3 + E LMO ( input ) W ORD 96 . 0 / 70 . 5 97 . 5 / 75 . 6 P ROB 96 . 8 / 77 . 0 97 . 7 / 80 . 9 P ROB + W ORD 96 . 5 / 79 . 2 97 . 8 / 83 . 2 Table 4 : Evaluation of models on the natural test sets when trained using synthetic datasets curated using dif - ferent noising strategies . 5 . 2 Defense against Adversarial Mispellings Many recent studies have demonstrated the suscep - tibility of neural models under word - and character - level attacks ( Alzantot et al . , 2018 ; Belinkov and Bisk , 2017 ; Piktus et al . , 2019 ; Pruthi et al . , 2019 ) . To combat adversarial misspellings , Pruthi et al . ( 2019 ) ﬁnd spell checkers to be a viable defense . 12 To fairly compare across different noise types , in this experiment we include only 50 % of samples from each of P ROB and W ORD noises to construct the P ROB + W ORD noise set . Therefore , we also evaluate spell checkers in our toolkit against adversarial misspellings . We follow the same experimental setup as Pruthi et al . ( 2019 ) for the sentiment classiﬁcation task under different adversarial attacks . We ﬁnetune SC - LSTM + E LMO ( input ) model on movie reviews data from the Stanford Sentiment Treebank ( SST ) ( Socher et al . , 2013 ) , using the same noising strat - egy as in ( Pruthi et al . , 2019 ) . As we observe from Table 3 , our corrector from NeuSpell toolkit ( SC - LSTM + E LMO ( input ) ( F ) ) outperforms the spelling corrections models proposed in ( Pruthi et al . , 2019 ) in most cases . 6 Conclusion In this paper , we describe NeuSpell , a spelling correction toolkit , comprising ten different mod - els . Unlike popular open - source spell checkers , our models accurately capture the context around the misspelt words . We also supplement mod - els in our toolkit with a uniﬁed command line , and a web interface . The toolkit is open - sourced , free for public use , and available at https : / / github . com / neuspell / neuspell . A demo of the trained spelling correction models can be accessed at https : / / neuspell . github . io / . Acknowledgements The authors thank Punit Singh Koura for insight - ful discussions and participation during the initial phase of the project . References Moustafa Alzantot , Yash Sharma , Ahmed Elgohary , Bo - Jhang Ho , Mani Srivastava , and Kai - Wei Chang . 2018 . Generating natural language adversarial ex - amples . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2890 – 2896 , Brussels , Belgium . Association for Computational Linguistics . Kevin Atkinson . 2019 . Gnu aspell . Yonatan Belinkov and Yonatan Bisk . 2017 . Synthetic and natural noise both break neural machine transla - tion . Christopher Bryant , Mariano Felice , Øistein E . An - dersen , and Ted Briscoe . 2019 . The BEA - 2019 shared task on grammatical error correction . In Pro - ceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 52 – 75 , Florence , Italy . Association for Com - putational Linguistics . Ciprian Chelba , Tomas Mikolov , Mike Schuster , Qi Ge , Thorsten Brants , Phillipp Koehn , and Tony Robin - son . 2013 . One billion word benchmark for measur - ing progress in statistical language modeling . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . Bert : Pre - training of deep bidirectional transformers for language understand - ing . Javid Ebrahimi , Anyi Rao , Daniel Lowd , and Dejing Dou . 2017 . Hotﬂip : White - box adversarial exam - ples for text classiﬁcation . Michael Flor and Yoko Futagi . 2012 . On using context for automatic correction of non - word misspellings in student essays . In Proceedings of the Seventh Workshop on Building Educational Applications Us - ing NLP , pages 105 – 115 , Montr ´ eal , Canada . Associ - ation for Computational Linguistics . Sylviane Granger . 1998 . The computer learner corpus : a versatile new source of data for SLA research . na . Yoon Kim , Yacine Jernite , David Sontag , and Alexan - der M . Rush . 2015 . Character - aware neural lan - guage models . Diederik P . Kingma and Jimmy Ba . 2014 . Adam : A method for stochastic optimization . Hao Li , Yang Wang , Xinyu Liu , Zhichao Sheng , and Si Wei . 2018 . Spelling error correction using a nested rnn model and pseudo training data . Roger Mitton . na . Corpora of misspellings . Tomoya Mizumoto , Mamoru Komachi , Masaaki Na - gata , and Yuji Matsumoto . 2011 . Mining revi - sion log of language learning SNS for automated Japanese error correction of second language learn - ers . In Proceedings of 5th International Joint Con - ference on Natural Language Processing , pages 147 – 155 , Chiang Mai , Thailand . Asian Federation of Natural Language Processing . Courtney Napoles , Keisuke Sakaguchi , and Joel Tetreault . 2017 . JFLEG : A ﬂuency corpus and benchmark for grammatical error correction . In Pro - ceedings of the 15th Conference of the European Chapter of the Association for Computational Lin - guistics : Volume 2 , Short Papers , pages 229 – 234 , Valencia , Spain . Association for Computational Lin - guistics . Peter Norvig . 2016 . Spelling correction system . Filipp Ozinov . 2019 . Jamspell . Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , and Adam Lerer . 2017 . Automatic differentiation in pytorch . In NIPS - W . Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 2018 . Deep contextualized word repre - sentations . Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Tech - nologies , Volume 1 ( Long Papers ) . Aleksandra Piktus , Necati Bora Edizel , Piotr Bo - janowski , Edouard Grave , Rui Ferreira , and Fabrizio Silvestri . 2019 . Misspelling oblivious word embed - dings . Proceedings of the 2019 Conference of the North . Danish Pruthi , Bhuwan Dhingra , and Zachary C . Lip - ton . 2019 . Combating adversarial misspellings with robust word recognition . Proceedings of the 57th Annual Meeting of the Association for Computa - tional Linguistics . Keisuke Sakaguchi , Kevin Duh , Matt Post , and Ben - jamin Van Durme . 2016 . Robsut wrod reocginiton via semi - character recurrent neural network . Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D . Manning , Andrew Ng , and Christopher Potts . 2013 . Recursive deep models for semantic compositionality over a sentiment tree - bank . In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1631 – 1642 , Seattle , Washington , USA . Asso - ciation for Computational Linguistics . Toshikazu Tajiri , Mamoru Komachi , and Yuji Mat - sumoto . 2012 . Tense and aspect error correction for ESL learners using global context . In Proceed - ings of the 50th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Pa - pers ) , pages 198 – 202 , Jeju Island , Korea . Associa - tion for Computational Linguistics . Reuben Thomas . 2010 . Enchant . W . John Wilbur , Won Kim , and Natalie Xie . 2006 . Spelling correction in the pubmed search engine . Inf . Retr . , 9 ( 5 ) : 543 – 564 . Helen Yannakoudakis , Ted Briscoe , and Ben Medlock . 2011 . A new dataset and method for automatically grading ESOL texts . In Proceedings of the 49th An - nual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 180 – 189 , Portland , Oregon , USA . Association for Computational Linguistics .