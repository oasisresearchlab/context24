6 AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions SUNGJAE CHO , POSTECH , South Korea YOONSU KIM âˆ— , KAIST , South Korea JAEWOONG JANG , POSTECH , South Korea INSEOK HWANG , POSTECH , South Korea Imagine a near - future smart home . Home - embedded visual AI sensors continuously monitor the resident , inferring her activities and internal states that enable higher - level services . Here , as home - embedded sensors passively monitor a free person , good inferences happen randomly . The inferencesâ€™ confidence highly depends on how congruent her momentary conditions are to the conditions favored by the AI models , e . g . , front - facing or unobstructed . We envision new strategies of AI - to - Human Actuation ( AHA ) that empower the sensory AIs with proactive actuation so that they induce the personâ€™s conditions to be more favorable to the AIs . In this light , we explore the initial feasibility and efficacy of AHA in the context of home - embedded visual AIs . We build a taxonomy of actuations that could be issued to home residents to benefit visual AIs . We deploy AHA in an actual home rich in sensors and interactive devices . With 20 participants , we comprehensively study their experiences with proactive actuation blended with their usual home routines . We also demonstrate the substantially improved inferences of the actuation - empowered AIs over the passive sensing baseline . This paper sets forth an initial step towards interweaving human - targeted AIs and proactive actuation to yield more chances for high - confidence inferences without sophisticating the model , in order to improve robustness against unfavorable conditions . CCS Concepts : â€¢ Human - centered computing â†’ Empirical studies in interaction design ; Ambient intelligence ; â€¢ Computing methodologies â†’ Intelligent agents ; Active vision . Additional Key Words and Phrases : Human - AI Interaction , IoT , Actuation ACM Reference Format : Sungjae Cho , Yoonsu Kim , Jaewoong Jang , and Inseok Hwang . 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . 7 , 1 , Article 6 ( March 2023 ) , 32 pages . https : / / doi . org / 10 . 1145 / 3580812 1 INTRODUCTION Imagine your toddler son is playing on the floor . You just see that he is taking something into his mouth but you cannot confirm what that is as he is sitting sideways to you . You hail him ; he turns around . Now you see a clear view of a leftover cookie in his hand , feeling relaxed . Similarly , a kindergarten kid has just rushed to her âˆ— Yoonsu Kim was an undergraduate student at POSTECH during a major period of this research . Authorsâ€™ addresses : Sungjae Cho , sungjaecho @ postech . ac . kr , POSTECH , Cheongam - ro 77 , Pohang , Gyeongbuk , South Korea , 37673 ; Yoonsu Kim , yoonsu16 @ kaist . ac . kr , KAIST , Daehak - ro 291 , Daejeon , South Korea , 34141 ; Jaewoong Jang , jaewoong . jang @ postech . ac . kr , POSTECH , Cheongam - ro 77 , Pohang , Gyeongbuk , South Korea , 37673 ; Inseok Hwang , i . hwang @ postech . ac . kr , POSTECH , Cheongam - ro 77 , Pohang , Gyeongbuk , South Korea , 37673 . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . Â© 2023 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . 2474 - 9567 / 2023 / 3 - ART6 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3580812 Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 2 â€¢ Cho et al . Fig . 1 . An example of how AI - to - human actuation complements an AI sensor under perception difficulty grandfather , showing off her masterpiece drawing right in front of his eyes , as usual . The grandfather , farsighted , cannot see the details . â€œWow , too close , sweetie . â€ He says . â€œOops , sorry grandpa . â€ She moves it a little back . The key takeaways from these episodes are : ( 1 ) our human - to - human perception abilities are often challenged by unfavorable conditions , and ( 2 ) we , humans , often overcome such challenges by actively actuating the target towards a more favorable condition to help our perception , rather than passively waiting until the conditions become favorable . Now , think about a near - future smart home , which embeds a number of AI - powered sensors surrounding the living area , continuously monitoring the residentâ€™s physiology [ 3 , 56 ] , home activities [ 66 , 78 , 79 , 98 ] , emotions [ 99 , 100 , 115 ] , exercises [ 109 , 111 ] , verbal expressions [ 30 , 38 , 39 , 138 ] , social interactions [ 62 , 75 , 158 ] , etc . , producing contextual ingredients that enable higher - level services . Naturally , the sensors often suffer from unfavorable human - to - AI conditions , e . g . , views being obstructed , the target person being too far or too close , and points of interest such as the face being sideways or opposite orientation . Here , the sensors remain passive and have no choice but to wait for favorable human - to - AI conditions to happen . In this paper , we propose a novel strategy of AI - to - Human Actuation ( AHA ) . We begin by posing a question : â€œWhat if an AI sensor is complemented by a proactive actuation on the target human to induce her conditions more favorable to the AIâ€™s perception ? â€ This is analogous to how humans improvise for easier perceptions as illustrated in the first paragraph . Figure 1 depicts an example embodiment of AHA out of many that we developed . The vision AI sensor continuously infers the userâ€™s context by observing her activity and objects being used . Currently the user is standing with her back to the camera , making its inference difficult . At this point , a proactive actuation is given , e . g . , the smart speaker initiates a conversation in this example . It naturally induces her to turn around , yielding a favorable view to the vision AI sensor . We present an initial exploration of AHA , in terms of its real feasibility , efficacy to AI , and user acceptance , with focus on smart home - embedded visual AI services . Specifically , we explore the following three aspects . Firstly , the method of AI - to - human actuation . We anticipate that modern smart homes would be rich in not only sensors but also actuators , e . g . , pervasive displays , smart speakers , autonomous lighting , and home robotics . Originally , they are born to serve the residentsâ€™ comfort . But we envision that they can be appropriated to benefit the inferences of other sensors . In this light , to help characterize the actuation , we devise a taxonomy of actuations across human - perceptive properties and actuation - induced reaction based on existing actuatorsâ€™ actions . Secondly , the benefit of AI - to - human actuation . To motivate , we experiment the dependency between the AI sensorsâ€™ inferences and the human - to - AI conditions , demonstrating diminishing confidence as the condition deviates from the ideal . Then , using the taxonomy we devised , we deploy various types of AI - to - human actuation in a near - future smart home environment with multiple visual AIs running . Extensive experiment results demonstrate how such actuations improve various human - sensing visual AI modelsâ€™ inference quality . Thirdly , the acceptance of AI - to - human actuation . Actuationâ€™s noticeability or acceptability to the target human may vary , depending on the type of actuation , the obtrusive levels of the actuation , and the context of the Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 3 human . We derive quantitative and qualitative findings on the user experiences from the experiments in our actuated smart home . To explore the benefit and acceptance aspects , we instrumented an actual home with a number of cameras , speakers , and displays deployed â€“ some representative samples of pervasive home sensors and actuators . Therein , we experimented with a total of 20 participants , for a total of 42 hours , under the actuated setting as well as baselines . We believe that our AHA approach brings a new dimension to improving the effective performances of AI models . Previous research [ 19 , 32 , 50 , 163 ] on tackling unfavorable human - to - AI conditions have focused mainly on model - or dataset - centric fortification , while leaving the AI sensors passive . These attempts usually resulted in increased system sophistication or computation complexity , making it less appropriate to run on existing commodity smart devices . Instead , our AHA newly investigates the AI - human interaction dimension , integrating emerging proactive interactions into a crisp showcase of eliciting favorable sensing conditions and following AI performance improvement , without robustness - aware specialization on models or datasets . Our key contributions are as follows . First , we propose a novel concept of AI - to - Human Actuation ( AHA ) , a strategy to strengthen human - targeted AI modelâ€™s effective performances under sub - optimal sensing conditions ; our strategy is orthogonal to existing ML / CV approaches which have not considered the actively interact - able nature of humans . Second , we comprehensively explore the characterization and initial embodiment of AHA , with crisp experimental demonstration in an apartment home that AHA substantially improves the AI performance in various sensing conditions and yields positive and acceptable user experiences . Third , from experiments and observations , we derive early guidelines to facilitate exercising AHA in further deployment . The rest of this paper is organized as follows . Â§ 2 outlines the background and related works . In Â§ 3 , we motivate AHA through micro - experiments that articulate the AIâ€™s sensitivity upon sub - optimal conditions . Â§ 4 suggests an initial taxonomy of actuation , and Â§ 5 defines the scope of this initial study on AHA . Â§ 6 describes our testbed in a real apartment home . Â§ 7 and Â§ 8 describe our experiments and organize our findings in terms of AI performance metrics and user experiences . Â§ 9 discusses implications , limitations , and preliminary feasibility . Â§ 10 concludes the paper . 2 BACKGROUND Broadly , AHA is positioned in the problem space of facilitating human - targeted AI be more effective in unfavorable real - life sensing conditions . We firstly review the approaches for robust AI in ML , CV , and HCI literature ( Â§ 2 . 1 ) , and highlight that AHA opens a novel , orthogonal solution to this problem . Methodologically , AHA adopts proactive interactions . Therefore , we examine existing proactive interactions from which we cultivate our actuations ( Â§ 2 . 2 ) . For extended positioning , we reflect AHA upon the domain of Human - AI Interaction and clarify the unique goal that differentiates AHA as well as those shared in common ( Â§ 2 . 3 ) . Finally , we review prior studies that have conceptually inspired and are partly analogous to our approach and clarify the main differences . ( Â§ 2 . 4 , Â§ 2 . 5 ) . 2 . 1 AI Models with Robustness to Sub - optimal AI - to - Human Conditions Advanced architectures of visual AI models have been developed to strengthen in - the - wild robustness , e . g . , diverse facial orientations [ 32 , 50 ] and occluded human bodies [ 19 , 105 , 135 , 163 , 165 ] . Due to their increased complexity , these models become harder to run on resource - tight platforms . Model downsizing [ 47 , 53 , 89 ] has been proposed to mitigate this issue , but it suffers from a trade - off between computation and accuracy . Cloud offloading [ 48 , 121 , 154 ] has also been suggested , but it accompanies privacy concerns . In HCI and UbiComp , achieving in - the - wild robustness with resource - tight platforms has been a serious mission to ensure practical usability [ 63 , 120 ] . GymCam [ 71 ] , a single - point vision that oversees concurrent exercises , tackles the problem of heavy occlusion by understanding that an exercise may consist of repetitive motions . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 4 â€¢ Cho et al . FitByte [ 12 ] and NeckSense [ 164 ] present robust wearable solutions to monitor eating that are usable during an entire unconstrained day , by developing a multi - modal sensing covering end - to - end intake activities and sensor - fusion for accurate chewing detection . Still , each solution is engineered with activity - specific specializations ; applicability to other activities would require extensive re - engineering . The multi - modal sensing architectures essentially share the philosophy of strengthening AI . After all , it is a tug - of - war of real - life robustness vs . complexity of the model or system . AHA opens a new dimension to fight this war . AHA leverages the abundant smart devices around us ( Â§ 2 . 2 ) to exert a cue to the human so that her condition is steered from AI - unfavorable to AI - favorable . AHAâ€™s unique advantage is to benefit even basic models without specialization to boost their effective robustness in real - life . Furthermore , AHA may be flexible upon emerging new types of unfavorable real - life conditions , in constrat to model specialization , which would require re - training , modifications , or both . 2 . 2 Proactive Interaction from Everyday Devices Proactive interactions have been studied extensively in push notifications [ 69 , 95 , 96 , 108 , 113 , 116 ] , proactive speakers [ 13 , 17 , 25 , 72 , 82 , 83 , 117 , 144 , 145 ] , and interactive displays [ 5 , 137 , 142 , 157 , 159 ] . Advanced un - derstanding of the userâ€™s demand and interruptibility show the possibility of tailoring when , where , and how proactive interactions are delivered and naturally blended with the userâ€™s current context [ 8 , 17 , 61 , 74 , 116 ] . For example , Voit et al . [ 140 ] explored proactive notifications from a smart plant system in opportune locations at opportune moments to reduce the negative effects and induce the user to respond immediately . Also , Cha et al . [ 17 ] explored various factors that are related to opportune moments for proactive smart speakers to initiate a conversation , such as personal or social contexts and movements of the residents . We believe that we can reasonably anticipate near - future readiness to easily accommodate AHA in our everyday environments . Desired actuation could be exerted by modestly appropriating proactive interactions which are readily abundant and familiar to the users . Furthermore , proliferation of proactive interactions from diverse devices of various modality would allow multiple options to convey a desired actuation at a given moment such that the actuation could be exerted optimally , e . g . , in a least obtrusive , most timely , or context - relevant manner . 2 . 3 Human - AI Interaction Models The spectrum of human - AI interaction models has been largely discussed in terms of the balance between human - control and computer - automation [ 130 ] . Examples of interaction models in this spectrum include : ( 1 ) AI guides and human performs , such as in guided creation [ 80 ] and design ideation [ 77 ] ; ( 2 ) AI suggests and human decides , such as in recommender systems [ 27 , 67 , 68 , 84 ] and auto - completion [ 40 ] ; ( 3 ) AI decides and human fixes , such as in code translation [ 147 ] and auto - correction [ 11 , 91 ] ; ( 4 ) human suggests and AI decides , such as in generative models [ 103 , 167 ] . Most human - AI interaction models eventually converge to assisting human tasks . Although our work shares the key stakeholders ( i . e . , humans , AIs , and signals in between ) , AHA pursues a different goal â€“ assisting the AIs to perform better by deploying actuation that remains as transparent as possible to the user . Fundamentally though , helping the AI perform better would be said to prevent an ill - performing AI from harming the human experiences [ 8 ] . 2 . 4 Steering User Behaviors and Contexts In mobile crowd sensing [ 16 , 37 , 46 , 58 â€“ 60 , 73 , 119 , 131 , 152 , 156 ] , incentivization counters the non - uniform coverage and information quality [ 122 , 153 ] , steering the crowdâ€™s location , sensing density , and quality of Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 5 sensing [ 70 , 114 ] . Proper sensory stimuli can also guide the userâ€™s internal context changes , such as inducing emo - tion [ 136 ] , regulating breathing rates [ 107 ] , improving cognitive performance [ 24 ] , and manipulating subjective time [ 129 ] . Our approach shares high - level philosophy , in that a cue is exerted to steer a condition associated with the user in a way to improve task - specific metrics . However , our notion of steering features a unique objective , i . e . , improving the effective performance of perceptive AI models that are not necessarily sophisticated for real - life robustness . To this end , our approach separates the userâ€™s condition to be steered from the userâ€™s context to be inferred . Thus , our research questions include preserving the userâ€™s target context despite actuation . Also , we question noticeability , awkwardness , and contextual fit of a certain actuation type with respect to the AIâ€™s sensing task , instead of monetary incentives . 2 . 5 Initiation and Calibration Gestures to Help Segmentation and Recognition Gesture - based systems often employ initiation gestures or calibration gestures that precede the main gesture . Initiation gestures are pre - designed for easy spotting out of continuous noisy sensor streams , so that the system activates the main recognition on - demand and reduces false - positives [ 110 ] . Examples include rotating the palm [ 10 ] , a tea pot gesture [ 141 ] , raising the forearm [ 76 ] , opening up the arm [ 155 ] , or pressing a button [ 90 , 126 ] . Users are sometimes requested to perform a calibration gesture [ 1 ] to help the system reset the accumulated error offsets . Similarly , voice - recognition systems often require a hot word [ 18 , 124 ] before the main verbal query being issued . These methods may look partly analogous to our strategy , in that the user performs a preliminary step to help the main sensing that shortly follows , but AHA differs from them in several ways . Firstly , a specific initiation / calibration gesture is tightly coupled with a certain gestural input system ; the user is asked to perform this initiation gesture whenever using the system . In AHA , an actuation is not affixed to an AI sensor ; each time , an appropriate actuation is chosen dynamically in a way that resolves the currently unfavorable human - to - AI conditions and naturally blends with the userâ€™s context . Secondly , the user pre - learns initiation / calibration gestures and willfully performs them when instructed . In contrast , actuations expect usersâ€™ natural reactions , not necessarily pre - learned nor instructed ; ideally , the user may remain unaware that her action is actuation - induced . Thirdly , initiation / calibration gestures are extra efforts in addition to the main gestures . In AHA , actuations are conveyed by piggybacking on existing proactive interactions , minimizing the userâ€™s extra interaction burdens . Finally , although an initiation / calibration gesture signals an upcoming main gesture and helps the sensor re - calibrate itself , the extrinsic conditions around the main gesture remain unchanged . However , the actuations of AHA aim to actively change the extrinsic conditions of the target sensing task itself . We note that actuations and initiation / calibration actions are not mutually exclusive . For example , the user says the hot word â€œHey Googleâ€ and wakes up the speaker . However , she is speaking in a direction away from the speaker , getting the recognition hard . An appropriate actuation may induce her to naturally speak toward the speaker , helping to increase the reliability of speech recognition . 3 MOTIVATING EXPERIMENTS To motivate , we experimented our hypothesis that typical human - targeted visual AI models would favor an ideal condition of the target person where the model performs well , and its performance diminishes as the condition deviates from ideal . To obtain empirical results , we sampled three human - targeted visual AI models that infer the gender and age [ 146 ] and the body weight [ 6 ] of the target person . To prevent us from arbitrarily intervening in the models , we used pre - trained models as presented by their original authors . The details of each experiment are as follows . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 6 â€¢ Cho et al . The gender model , pre - trained in UTKface dataset [ 166 ] , takes inputs of face images cropped by MTCNN [ 162 ] and infers the personâ€™s gender . To test whether this gender model favors a certain facial orientation , we tested it on the K - Face dataset [ 22 ] , which consists of professional - grade facial photos of 1 , 000 individuals , taken systematically in a 360Â° photogrammetry studio [ 151 ] to capture facial photos simultaneously from numerous angles . ( a ) Gender confidence along viewing angle ( b ) Age error along viewing angle ( c ) Body weight error along distance Fig . 2 . Error and confidence scores variation on each model by different human - to - AI conditions Figure 2a summarizes the confidence trend of gender inferences varied with facial orientations . The confidence reaches its peak around the frontal view ( i . e . , 0Â° - 15Â° ) and diminishes as the viewing angle deviates to either sideways . We observe similar trends from the age model in Figure 2b . Note that this figure shows the error rates , not confidences . The trend reveals the error being at a minimum at - 30Â° , and growing as the viewing angle deviates . Although the minimum occurred slightly away from face - on ( 0Â° ) , it confirms our hypothesis that the model favors a certain ideal angle . The body weight model , pretrained in the original authorsâ€™ proprietary dataset , takes 2D images and predicts the weight of the person shown . To observe whether the model favors a certain distance between the camera and the person , we test the pre - trained model with our in - house dataset that consists of a total of 125 images of 8 individuals ( 2F 6M ; age min / mean / max : 21 . 0 / 25 . 5 / 43 . 0 ; height min / mean / max : 1 . 62 / 1 . 76 / 1 . 85 m ; weight min / mean / max : 53 . 0 / 76 . 1 / 100 . 0 kg ) standing 0 . 5 to 6 . 5 m from the camera . The test was conducted using our in - house dataset because we could not find a public dataset that had labels of both the personâ€™s weight and distance from the camera . Still , our dataset covers a large range of body weight . Figure 2c reveals that the model achieves the minimum error of 7 % at 1 . 1 m distance ; the errors grow as the distance deviates â€“ rapidly growing at closer distances and stay doubled or higher at farther distances . So far , all three modelsâ€™ behaviors are consistent with our hypothesis , despite a myriad of other models having yet to be tested . Our experiments indicate that a visual AI model likely has a favored spatial condition of the target where the model performs best , and its performance diminishes as the condition deviates from the favored one . Conversely , our observations above support AHA â€“ actuating the target person to change her condition to one that is favorable to the AI model would yield the inference results of better confidence or accuracy . 4 TAXONOMY OF ACTUATIONS By proposing a new approach that proactive actuations help AI sensors overcome unfavorable conditions , this paper will be exploring the guidelines of which actuation is effective for which AI sensors and for which context the person is currently in , to effectively elicit favorable conditions for that sensor given the situation and the target context . Such guidelines may be of limited applicability if they directly refer to the specific device , implementation , or contents of an actuation being experimented here . To facilitate flexible applications , we suggest a taxonomy to position each actuation type therein . Let us take an example of background music played from a surround speaker Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 7 array . Rather than viewing it directly as a specific speaker , we have it sit in the taxonomy as a representative example of auditory actuations , ambiently embodied in the home space , that may elicit certain physical motions of the user relevant to her attention or perception . To build the taxonomy , we thoroughly reviewed literature and commercial products that present new devices , interactions , or designs targeted for interactive spaces . We performed multiple iterations to find appropriate dimensions that could effectively categorize the reviewed work , in the perspective of actuation and AI , actuation and human , and the characteristics of actuations . We describe each dimension of the taxonomy below . 4 . 1 Dimension A . Induced Human Reaction We anticipate that an actuation will induce a reaction from the target person . Many types of reaction can occur , and at levels from motor to cognitive [ 15 , 102 ] , but we first set a scope of reactions to be those that can be sensed by typical sensor types pervasive in smart environments : cameras and microphones . Within this scope , we characterize an actuation as one that may induce ( 1 ) physical movement , ( 2 ) speech , or both , from the target person . Many devices are known to induce physical movements by a person . Public interactive displays [ 2 , 14 , 44 , 97 , 142 , 159 ] induce physical movements from the pedestrians , known as honey - pot effects . Limited abilities of a robot vacuum cleaner can induce the resident to resolve obstacles or dead ends [ 33 , 35 , 127 ] . Ambient displays [ 54 , 93 ] attract attention , often changing the personâ€™s facial orientation . Ambient lights remind the person to make physical movements for well - being [ 36 , 92 ] . Auditory nudges also result in a personâ€™s movement [ 94 , 143 ] . These induced movements may discontinue a situation that is unfavorable to a visual AI model , e . g . , the target person getting clear of an obstructed view . Proactiveness of smart speakers has been actively studied [ 17 , 72 , 145 , 149 ] and even commercialized [ 7 , 43 ] . Smart speakers that initiate a conversation may induce both reactions â€“ turning towards the speaker [ 85 ] and speaking , favoring both visual and audio - based AI models [ 55 , 101 ] . For example , inducing a person to speak would help AI models that opportunistically infer her non - verbal contexts , especially when it is in need of additional confirmative speech samples . Although we initially consider two categories in this dimension , considering additional types of sensor could generatively extend it . 4 . 2 Dimension B . Ambience We classify an actuation in terms of the ambience of its presence and transition . Ambient actuations leverage existing background signals and produce a slowly - progressing change , so that they are less obtrusive to the user and less interfering with her on - going context . However , its low noticeability may delay the userâ€™s reaction . Examples of ambient actuations include subtle updates on pervasive displays [ 54 , 93 ] and a silently - changing background [ 125 ] . These changes are less salient , having a low possibility of intervening the user context . In contrast , obtrusive visual effects , e . g . , blinks indicating urgent notifications [ 93 ] , are non - ambient actuations , likely inducing an instant reaction from the user . Similarly , prominent sounds that yield the userâ€™s instant movements [ 94 , 143 ] are non - ambient actuations . Non - ambient actuations may be invasive of the userâ€™s on - going context . They include public displays [ 2 , 14 , 44 , 97 , 142 , 159 ] conveying an explicit cue to the user to move around or take a break [ 36 , 92 ] . Robotic appliances getting stuck in a troubling situation [ 33 , 35 , 127 ] can also be seen as non - ambient actuations , which divert the userâ€™s attention to start remedying the robot . Proactively initiated conversations [ 7 , 17 , 43 , 72 , 145 , 149 ] both intervene the userâ€™s context and call for an instant reaction , e . g . , responding to the speaker , thus they are seen as non - ambient actuation . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 8 â€¢ Cho et al . 4 . 3 Dimension C . Sensory Dependence Actuations are conveyed to the user through certain sensory channels , whose momentary availability may vary and thereby affect the userâ€™s noticeability of that actuation [ 20 , 21 , 81 ] . A well - known relevant concept is change blindness [ 64 , 132 ] , which refers to the human inability to detect environmental visual changes due to oneâ€™s attention [ 133 ] or other factors [ 41 ] . The userâ€™s line of sight being off the display or her mind being occupied elsewhere would make it difficult for her to timely see the actuation [ 14 , 36 , 54 , 65 , 92 , 93 , 97 , 106 , 128 , 134 , 142 ] . Auditory actuations [ 7 , 17 , 43 , 94 , 143 , 149 ] are largely insensitive to the line - of - sight constraint , but their noticeability can decrease in a noisy environment . Further , a concept of change deafness [ 31 , 139 ] , similar to change blindness , has been developed . Thus , the sensory channels that an actuation utilizes influence the chances of the userâ€™s reaction . We consider this sensory dependence as one of our taxonomy dimensions . Besides visual and auditory , other senses such as olfactory [ 106 ] or tactile feedback [ 92 , 137 ] could be used for actuations , but are beyond the scope of our study . 5 STUDY GOALS AND SCOPE In this paper , we premiere the novel concept of AHA , showcase its early embodiment , and study how it benefits the AI and influences human experiences , in a realistic living environment . As set forth in Â§ 1 , our experimental goals are : ( 1 ) methods â€“ prototype various methods of AHA that are feasible in upcoming smart homes ; ( 2 ) benefits â€“ observe how such actuation bring changes to the userâ€™s conditions and benefits the AI sensorsâ€™ performance ; ( 3 ) acceptability â€“ understand the user perception and acceptance of the actuation being applied . Study scope . As our everyday life is increasingly surrounded by AI - backed sensors and smart devices , the applicable scope of the AHA can be vast , e . g . , home and work , mobile and public spaces . Given this vast scope and the initial nature of our concept , we focus on an exemplary , manageable - sized real - life setup and assess key factors on AI performance and human experiences . In this light , we define this initial studyâ€™s scope as follows : â€¢ Target environment is set to be a near - future smart living space ( e . g . , a home ) instrumented with pervasive displays , speakers , and vision sensors backed by human - targeted AI models that infer the residentâ€™s various contexts . â€¢ Impact of actuation on AI output . The exerted actuation may ( or may not ) change the personâ€™s conditions observable by the AI sensor and eventually influence the AI modelâ€™s output behaviors . We investigate the impact on the AI modelâ€™s output behaviors in quantitative terms , such as accuracy , confidence , response time , and so on . â€¢ Impact of actuation on user experience . Upon an actuation applied , a number of user experience factors would be in question . She may or may not notice the presence of actuation . If she does notice , she may or may not feel that it is obtrusive , or even annoying . If she knows the underlying intent of the actuation , she might respond differently to that actuation . We investigate the userâ€™s noticeability , acceptability , and knowledge factors . Beyond scope . Two major agenda would lie in our AHA research : given an AI model , ( 1 ) How can we automatically determine the right timing and context to trigger an actuation , as well as the appropriate type of that actuation ? ( 2 ) Once triggered , what is the expected benefit to the AIâ€™s performance and what might be possible side - effects on the user ? In this study , we seek answers to the 2nd - group questions . The 1st - group questions are beyond this paperâ€™s scope . The rationales are as follows . Automating the triggering decision of an actuation requires prerequisites : the system should be able to predict the level of benefits and side - effects if an actuation is applied , taking account of the dependency on the actuation type and the current context . Now it is evident that the scope of this initial study , Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 9 i . e . , answering the 2nd - group questions , is essentially to develop these prerequisites . We believe that this initial study will nourish the exploration of the 1st - group problems , which will be the next agenda of AHA research . We narrow the type of sensors / actuators to commercially mature audio - visual devices â€“ home IoT cameras , smart speakers , and pervasive displays . Although the concept of AHA applies to experimental devices ( wireless , tactile , robotic , etc . ) , we leave it to future work to keep this initial study manageable - sized . For the userâ€™s knowledge factor and possible novelty effects , this study previews these matters over a short term . A longitudinal study is reserved as a follow - up . 6 TESTBED To empirically test the AHA approach , we constructed a testbed that emulates a near - future smart living space . The goals of our testbed are to facilitate the participantsâ€™ natural home activities , to apply various actuations through the smart devices and services deployed , and to study the influences on the AI modelsâ€™ output and the participantsâ€™ experiences . Fig . 3 . Testbed floorplan and device placements . Name of each camera shown in the near yellow box Figure 3 illustrates our testbed setup established in a real home , i . e . , a campus apartment , instrumented with multiple smart speakers , pervasive displays , and vision sensors . For ease of programmability , we employed smartphones as equivalent replacements of smart speakers and vision sensors ; we installed 10 smartphones , having their cameras , speakers , and microphones cover the testbed space . We also installed 3 displays â€“ one 80 - inch projector screen and two 32 - inch LCD monitors , to emulate pervasive displays . In total , these devices function as 10 vision sensors , 2 smart speakers , 4 surround speakers , 1 ambient display ( 80 inches ) , and 2 interactive displays ( 32 inches each ) in the testbed . On top of these devices , we implemented the following services that are likely in near - future smart homes : a fake window on the ambient display , a background music player on the surround speakers , content - push services on the interactive displays , and smart - speaker services . Details of these services and the actuation deployed on them are covered in Â§ 7 . 4 . In an isolated bedroom , we set up an administratorâ€™s console ( Figure 4 ) providing live video feeds from all the vision sensors as well as input interfaces to trigger a smart speaker to initiate a conversation , to control the surround speakersâ€™ volume , to change the scene on the fake window , and to switch the content of an interactive display . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 10 â€¢ Cho et al . Fig . 4 . Administratorâ€™s console interfaces 7 EXPERIMENT DESIGN For experiments , we deployed multiple visual AI models to run behind the vision sensors in the testbed . We also deployed multiple types of actuations to be triggered from smart devices and services . We recruited 20 participants ( 10F & 10M ; age min / mean / max : 20 / 22 . 6 / 28 y ) through our university bulletins . The basic unit of our experiment is a session ( 60 min long ) . Each participant is invited to the testbed , asked to make herself at home for a single session . During her stay , all the vision sensors record videos from the respective angles ( Figure 3 ) and various types of actuations are given occasionally . After the session , participants are given a 30 - min semi - structured interview . To study the impact of their knowledge of actuations ( Â§ 5 ) , 14 ( 7F , 7M ) of the 20 participants were invited to another round of experiment of the same format , conducted after a week . Each participant was compensated an amount worth USD 15 per session . They were notified of video recordings in advance and voluntarily signed a consent form . The experiment was approved by our university IRB . Details of the experiment design are as follows . 7 . 1 Participant Instructions At the 1st round , we briefed them only that the intention of our study is to explore their experiences with proactive smart devices in a near - future home . We did not mention actuation , its intent , nor mechanism . Only at the end of the interview , they were told about the real purpose ; this was to avoid potential biases that may affect the 1st - round results . To establish common experimental conditions across participants while eliciting natural home activities , we requested the participants to perform a set of 11 home activities ( listed below ) but also allowed some degree of individual flexibility . â€¢ Read books at the table â€¢ Use a laptop at the ta - ble â€¢ Sit at 3 different chairs â€¢ Write on whiteboards â€¢ Use a smartphone â€¢ Wander around â€¢ Brush the dust off â€¢ Eat fruits â€¢ Eat snacks â€¢ Make tea or coffee â€¢ Drink water The participants were asked to perform every activity at least once during the session but in any order and timing they were pleased . Personal activities are also allowed , with a list of examples given : such as using a personal device like a tablet , doing their own study , or reading papers , etc , in the intervals between the requested activities . These policies were to semi - control the activities across participants but also to let them feel relaxed and behave naturally . The requested activities are listed on the whiteboard in the testbed space . We shuffled their order every session . Before each session , we replenished supply items such as fruits , snacks , bottled water , and tea bags . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 11 7 . 2 Experiment Sessions Each session runs for 60 min , in which it switches settings â€“ baseline and actuated . The baseline setting reflects the conventional one - way sensing . The actuated setting reflects our approach in which proactive actuations are occasionally exerted to hopefully elicit favorable human - to - AI conditions . In both settings , all vision sensors record videos and make continuous inferences . We collected a total of 420 h of video from all cameras Ã— sessions Ã— participants . A session starts in the actuated setting , then quietly switches to the baseline , then quietly back to the actuated setting . Each setting runs for 20 min . All switches are performed quietly without notifying the participant . During the total 40 min of actuated setting , more than 10 actuation instances ( 2 + per actuation type , with a minimum 3 + min of gap between consecutive actuations ) are triggered in a Wizard - of - Oz manner ( Â§ 7 . 5 ) . As discussed in Â§ 5 , automated AHA is beyond the scope of this paper ; this paper is to fulfill the prerequisite to move on to that stage . 7 . 3 AI Models Deployed We consider human - targeted visual AI models that could be pervasively deployed on commodity home IoT products such as networked RGB cameras [ 9 , 29 , 42 , 123 ] . Although such AI models are diverse , we curate a few exemplary models to showcase the efficacy of our approach . To help our model choices be representative of a large spectrum of human - targeted vision applications , we systematically sampled the following three families . â€¢ Human face detection provides a versatile primitive for many vision applications , e . g . , inferring identity , emotion , age , gender , etc . In our experiment , we select a face detector , MTCNN [ 162 ] , to serve as such a common primitive . â€¢ Human whole - body sensing enables useful applications . Health applications would be of special interest in smart homes , such as inferring oneâ€™s weight , activity , motor diseases , etc . We select Detectron2 human pose estimator [ 150 ] as a basic model , and also a weight estimator [ 6 ] as an applied model . â€¢ Human - object interaction ( HOI ) often serves as a lens to understand the humanâ€™s rich context . We employ an HOI detector , CDN [ 161 ] , that jointly detects a human , the object under interaction , and the verb label that indicates her activity with that object . HOI is a useful primitive for home automation , such as illuminating or ventilating . 7 . 4 Actuations Deployed We deployed 5 types of actuation that could benefit visual AI models suffering from unfavorable orientation , distance , or occlusion ( Figure 5 ) . They operate on the pervasive displays and smart speakers in the testbed , and proactively generate changes or engage the user through audio - visual channels . Note that our actuations were largely adopted from proactive interaction literature ( Â§ 2 . 2 and Â§ 4 ) , rather than designed from scratch . It was our purposeful decision to have our experiments focus on our key questions â€“ exploring the AI benefits and user experiences with proactive actuations that complement AI sensors . Creating a brand new proactive interaction may have extra merits , but will multiply the number of independent variables to validate , and dilute our focus . Table 1 lists our classifications of the five actuations according to the taxonomy in Â§ 4 . Below , we describe the details of how each actuation operates and what factors govern the operation . â€¢ Ambient visual + auditory effects from a fake window ( ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ) : Pervasive displays have been studied for ambient home services , e . g . , a fake window or a digital photo frame [ 26 , 148 ] with auto - changing content . Adopting such , we implemented ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ - type actuations â€“ a fake window service in the ambient display and speakers . It shows occasional smooth transitions so that the user may naturally turn toward them . We implemented visual transitions of snowing , turning on / off the lamp , and a time - lapse of an outdoor landscape . We also implemented slow audio transitions of bird - chirping and bonfires . Given their mild changes , we Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 12 â€¢ Cho et al . Table 1 . Classification of deployed actuations Induced Human Reaction Ambience Sensory Dependence Face Body Speech Ambient Non - ambient Visual Auditory Actuations ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ O O O O ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ O O O ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ O O O O ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ O O O O ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ O O O O O O expected that the actuation of ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ type would exhibit relatively low obtrusiveness , drawing the personâ€™s reaction in an un - or semi - conscious manner . â€¢ Subtle changes in spatial volume of surround speakers ( ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ) : An intelligent music player would proactively change the spatial volume distribution , adaptive to the personâ€™s context [ 45 , 143 , 168 ] . Adopting such , we deployed ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ - type actuations that slowly change the volume of either side of 4 surround speaker units . It slightly and slowly increases the volume on one side ; then after a while , slowly decreases it back to the original volume . Due to its gradual change , we expected that the actuation of ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ type would yield a mild , unawarely reaction . â€¢ Proactive visual information delivery from interactive displays ( ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : Inspired by content - adapting displays that respond to the userâ€™s distance and orientation [ 5 , 137 , 142 , 159 ] , we expect that the converse would function as actuation â€“ inducing the user to change her distance or orientation in response to proactively - changing information . ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ - type actuation works as follows . By default , the interactive display shows a natural background . When triggered , it silently switches to an indication of a news update available . As the actuation is audio - free , she may not notice it immediately . When she moves closer , more details of the news gradually appear until she reaches the distance favored by the whole - body AI models ( human pose estimator [ 150 ] and weight estimator [ 6 ] , Â§ 7 . 3 ) . Once she stops interacting , or if she does not notice the actuation for 3 minutes , the display reverts to the default state . We expected that this type of actuation would require conscious attention from the user , as she explicitly interacts with the display . â€¢ Proactive auditory information delivery from a smart speaker ( ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ) : When addressed , people usually turn towards the caller ; this property is often utilized in smart speakers [ 85 , 118 ] . Leveraging such human responses and the proactive voice - assistant literature [ 17 , 25 , 72 , 144 , 145 ] , we deployed ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ - type actuation that the smart speaker proactively initiates a conversation . Compared to ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , we expected that ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ would exhibit a higher level of attention and interruption , due to the verbal cues being instantly perceivable regardless of the userâ€™s line - of - sight . â€¢ Proactive audiovisual information delivery from both devices ( ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : This type of actuation extends ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ by adding the smart speaker , for audiovisual information update to induce a more prompt change of the userâ€™s distance or orientation . We expected it to bring combined effects of ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ . 7 . 5 Actuation Triggering Criteria To regularize how to trigger actuations in a Wizard - of - Oz setting , we defined a set of protocols that were agreed upon by the researchers who administered the sessions . Each protocol is a chain of determining : ( 1 ) an unfavorable condition , ( 2 ) cameras that suffer from that condition , ( 3 ) a candidate actuation ( s ) to trigger , and ( 4 ) the induced human reaction ( Table 2 ) . Eligible unfavorable conditions were defined based on literature ( Â§ 2 . 1 ) and found in our motivation ( Â§ 3 ) â€“ a participant whose face is invisible or seen sideways , staying against a camera , whose body is partly occluded or totally out of a cameraâ€™s field of view ( FoV ) . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 13 Fig . 5 . Actuations deployed on the testbed Table 2 . Criteria for triggering actuation Unfavorable Condition In Trigger Candidataes Induced Human Reaction When the face is a side - view or heading down CC ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( CC ) Face Turn CL ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( CC ) , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( CL ) Face Turn CR ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( CC ) , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( CR ) Face Turn When the face is a side - view or hidden PL ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( PL ) Face Turn PR ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( PR ) Face Turn When the person is turn against the camera or the face is heading down WB ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ( WB ) Face Turn WC ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ( WC ) Face Turn When the part of body is occluded or the entire body is not shown PC ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( PC ) , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( PC ) Face Turn & Body Movement During sessions , three researchers remotely oversee the experiment progress at the administratorâ€™s console ( Figure 4 ) . According to the protocols above , two researchers first observe all cameras , then shortly agree to an eligible unfavorable condition and its associated actuation candidate ( s ) . If multiple types of candidates exist , they prioritize the least - triggered type . At least 3 minutes of separation is ensured between consecutive actuations . Once the researchers agree , they manually trigger the agreed actuation ( Figure 4 ) . One researcher makes annotations along the experiment timeline . 7 . 6 Post - session Interviews We interviewed participants about general experiences in the actuated environment , e . g . , level of comfort , any unusual or memorable action of a device , etc . Then we asked how they felt upon an actuation triggered , what information was delivered , whether they intentionally did not respond to it , and to guess the number of actuations that had been triggered . For each actuation type the participant noticed and remembered , we asked them to evaluate their experience quantitatively ( in 7 - point Likert scale ) and qualitatively in four user experience attributes : { Awkward , Disturbing , Interesting , Useful } . The first two attributes are to explore the negative impact with that type of actuation , Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 14 â€¢ Cho et al . which may trade - off with the improvement of AI performance . The later two are to investigate the positive impacts with that type of actuation . For missing actuation types that they did not mention , we gave a hint , e . g . , â€œ Have you noticed the volume change of the background music ? â€ Our interview protocol was that , only if they recall that type of actuation , we repeat the aforementioned quantitative and qualitative questions . No participant recalled missing actuation types despite our hints . 8 RESULTS We organize our results as follows . To outline , Â§ 8 . 1 summarizes the basic statistics . Then , we present the results from two perspectives â€“ AIs and humans . Firstly , Â§ 8 . 2 investigates the AI - side benefits and impacts . Specifically : â€¢ Actuation has been applied to on - going AI inference . Does it bring improvement ? ( Â§ 8 . 2 . 1 ) â€¢ AIs may be doing good or bad before the actuation . Does the actuation bring improvement anyway ? ( Â§ 8 . 2 . 1 ) â€¢ The temporal responses . How does AI performance change along time upon an actuation applied ? ( Â§ 8 . 2 . 2 ) â€¢ Do we observe overall improvement in the actuation settings over the baseline settings ? ( Â§ 8 . 2 . 3 ) â€¢ Between 1st and 2nd rounds , do we observe any meaningful change of AIsâ€™ performance ? ( Â§ 8 . 2 . 4 ) Secondly , Â§ 8 . 3 reports the human factors with AI - to - human actuations . Specifically , we find : â€¢ The participantsâ€™ experiences in actuated environments , in terms of quantitative ratings . ( Â§ 8 . 3 . 1 ) â€¢ The participantsâ€™ experiences in actuated environments , in terms of qualitative responses . ( Â§ 8 . 3 . 2 ) â€¢ The participantsâ€™ behaviors in actuated environments : rate of reactions to the actuations applied . ( Â§ 8 . 3 . 3 ) â€¢ Between 1st and 2nd rounds , did their behaviors and experiences change ? ( Â§ 8 . 3 . 4 ) 8 . 1 Basic Statistics of the Experiments Table 3 provides a comprehensive summary of our experiment results organized in two major dimensions , as follows : â€¢ Performance Improvement dimension tabulates the improvement in each AI modelâ€™s inference quality upon an actuation applied , shown in either increasing confidence score ( MTCNN , CDN , Detectron2 ) âˆ— or decreasing MAE ( Weight ) . Each cell shows the quality value observed in the after - actuation period , followed by the difference amount from the before - actuation period , and a bar chart for visibility , juxtaposing after - and before - actuation quality bars . â€¢ User Acceptance dimension represents the participantsâ€™ subjective ratings of the actuated environment in four UX attributes . A high rating in Interesting and Useful , and a low rating in Awkward and Disturbing would be indicative of positive acceptance . Ratings are color - coded for visibility , such that ( blue , yellow , red ) = ( 7 . 0 , 4 . 0 , 1 . 0 ) for Interesting and Useful , and ( 1 . 0 , 4 . 0 , 7 . 0 ) for Awkward and Disturbing . Table 3 separately lists the all - round results , as well as the 1st - and 2nd - round results for between - round analysis ( Â§ 8 . 2 . 4 , Â§ 8 . 3 . 4 ) . Note that the all - round results include all participants ( ğ‘ = 20 ) , whereas the round - specific results include only the participants who joined both rounds ( ğ‘ = 14 ) . Therefore , the all - round results are not necessarily the average of both round - specific results . This is to perform a fair comparison between rounds by only considering comparable pairs of sessions , as we try to investigate possible knowledge factors ( Â§ 5 ) . This convention consistently applies to the round - specific results presented afterward . Table 4 summarizes the number of actuations triggered in all 34 sessions , their breakdown into each actuation type , and per - session average . The distribution of actuation types across sessions did not differ significantly . Once an actuation is triggered , it is activated for the transition effect duration ( e . g . , ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ) or until the partic - ipant responds and completes two - way interactions ( e . g . , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) . Figure 6 shows the durations . ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ âˆ— Confidence scores âˆˆ [ 0 , 1 ] by default . For Detectron2 , we show the sum of scores in 17 joints â€“ each score is known to be unbounded [ 4 ] . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 15 Table 3 . Overall results of each actuation organized in two dimensions : Performance Improvement & User Acceptance . Each cell in performance improvement indicates ( from top to bottom ) : peak confidence ( or minimum MAE for Weight â€  ) at after - actuation period ; its difference from before - actuation period ; a blue bar indicating the peak confidence ( or minimum MAE for Weight â€  ) at after - actuation period ; a red bar indicating the peak confidence ( or minimum MAE for Weight â€  ) at before - actuation period . Each cell in user acceptance is an average 1 - 7 Likert scale from the user interviews . â€  : the performance of Weight model is shown in MAE ( mean absolute error ) ; a negative difference indicates improvement âˆ— : a statistically significant ( ğ‘ < 0 . 05 ) improvement in AI performance induced by a triggered actuation . âˆ—âˆ— : a 2nd - round user rating with statistically significant ( ğ‘ < 0 . 05 ) difference from the 1st round . Taxonomy Performance Improvement User Acceptance ( Likert scale : 1 - 7 ) Body Induced reaction Ambience Sensory Actuation Face ( Confidence ) Action ( Confidence ) Pose ( Confidence ) Weight â€  ( MAE ) Awkward Disturbing Interesting Useful ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ 0 . 80 ( + 0 . 17 âˆ— ) 2 . 82 2 . 27 4 . 52 3 . 55 1R 0 . 74 ( + 0 . 14 âˆ— ) 2 . 58 2 . 42 4 . 17 3 . 42 Face O Visual & Auditory R o u n d s 2R 0 . 85 ( + 0 . 20 âˆ— ) 3 . 50 âˆ—âˆ— 2 . 58 4 . 33 3 . 67 ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ 0 . 87 ( + 0 . 28 âˆ— ) 3 . 22 2 . 78 4 . 59 3 . 85 1R 0 . 87 ( + 0 . 23 âˆ— ) 3 . 56 3 . 22 3 . 89 3 . 11 Face O Auditory R o u n d s 2R 0 . 92 ( + 0 . 33 âˆ— ) 3 . 56 3 . 33 5 . 56 âˆ—âˆ— 3 . 67 ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ 0 . 98 ( + 0 . 29 âˆ— ) 0 . 76 ( + 0 . 33 âˆ— ) 10 . 01 ( + 5 . 25 âˆ— ) 15 . 57 â€  ( - 11 . 64 âˆ— ) 2 . 45 1 . 42 4 . 76 5 . 33 1R 1 . 00 ( + 0 . 23 âˆ— ) 0 . 74 ( + 0 . 32 âˆ— ) 10 . 12 ( + 5 . 02 âˆ— ) 13 . 54 â€  ( - 15 . 30 âˆ— ) 2 . 58 1 . 83 4 . 75 5 . 42 Face & Body X Visual R o u n d s 2R 0 . 95 ( + 0 . 36 ) 0 . 77 ( + 0 . 34 ) 9 . 95 ( + 5 . 60 âˆ— ) 17 . 60 â€  ( - 10 . 97 âˆ— ) 2 . 42 1 . 08 5 . 17 5 . 58 ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ 0 . 78 ( + 0 . 25 âˆ— ) 2 . 30 3 . 18 5 . 52 5 . 39 1R 0 . 81 ( + 0 . 32 âˆ— ) 2 . 67 3 . 75 5 . 17 5 . 58 Face X Auditory R o u n d s 2R 0 . 80 ( + 0 . 20 âˆ— ) 2 . 17 2 . 67 5 . 33 5 . 25 ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ 0 . 90 ( + 0 . 31 âˆ— ) 0 . 76 ( + 0 . 36 âˆ— ) 10 . 05 ( + 5 . 84 âˆ— ) 15 . 38 â€  ( - 16 . 09 âˆ— ) 2 . 24 3 . 52 5 . 61 5 . 79 1R 0 . 93 ( + 0 . 29 âˆ— ) 0 . 74 ( + 0 . 29 âˆ— ) 10 . 23 ( + 6 . 01 âˆ— ) 13 . 66 â€  ( - 18 . 50 âˆ— ) 2 . 77 3 . 92 5 . 23 5 . 54 Face & Body X Visual & Auditory R o u n d s 2R 0 . 91 ( + 0 . 36 âˆ— ) 0 . 80 ( + 0 . 41 âˆ— ) 10 . 21 ( + 6 . 01 âˆ— ) 16 . 07 â€  ( - 17 . 36 âˆ— ) 2 . 08 3 . 23 5 . 77 6 . 08 Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 16 â€¢ Cho et al . Table 4 . Total number of actuations triggered ( average number of actuations per experiment session ) ğ‘« ğ’‚ğ’ğ’ƒğ’Šğ’†ğ’ğ’• ğ‘º ğ’”ğ’–ğ’“ğ’“ğ’ğ’–ğ’ğ’… ğ‘« ğ’Šğ’ğ’•ğ’†ğ’“ğ’‚ğ’„ğ’•ğ’Šğ’—ğ’† ğ‘º ğ’”ğ’ğ’‚ğ’“ğ’• ğ‘«ğ‘º ğ’Šğ’ğ’•ğ’†ğ’“ğ’‚ğ’„ğ’•ğ’Šğ’—ğ’† All All rounds 110 ( 3 . 24 ) 112 ( 3 . 29 ) 92 ( 2 . 71 ) 81 ( 2 . 38 ) 73 ( 2 . 15 ) 468 1st rounds 42 ( 3 . 00 ) 44 ( 3 . 14 ) 39 ( 2 . 79 ) 30 ( 2 . 14 ) 30 ( 2 . 14 ) 185 2nd rounds 47 ( 3 . 36 ) 46 ( 3 . 29 ) 35 ( 2 . 50 ) 33 ( 2 . 36 ) 28 ( 2 . 00 ) 189 Fig . 6 . Duration for an actuation staying activated , once triggered . Fig . 7 . Distribution of participantsâ€™ activities : before - actuation period vs . base - line ( a ) Faces ( MTCNN ) ( b ) Human - Object Interaction ( CDN ) ( c ) Body pose ( detectron2 ) Fig . 8 . Visualizing all actuation instances : Before - actuation confidence ( X - axis ) vs . After - actuation confidence ( Y - axis ) had the longest duration with the largest min - max range due to its silent , visual - only change , resulting in long latencies until the participants notice . ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ had a shorter duration , as the actuation includes both auditory and visual effects . Other actuation types lasted shorter , because they are one - way actuations not waiting for the user reaction . Figure 7 shows the annotated activities that the participants were doing , at the baseline setting and at the moment of an actuation triggered . Overall , the activity distribution was a good mixture of the requested pre - defined activities and the suggested free activity examples . Note that the seemingly uneven distribution is due to different amounts of time spent per activity . The distributions do not differ significantly across participants , indicating that each experiment session was under modestly similar conditions in terms of the participantâ€™s activities . 8 . 2 AI Perspective : Benefits and Impacts from Actuations Does applying an actuation to on - going AI inference bring improvement ? We begin with this central question in exploring the efficacy of AHA , in quantitative quality metrics of confidence levels or error amounts in AI inference . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 17 8 . 2 . 1 Comprehensive Results : How Many Actuations Bring How Much Improvement on AI Inferences ? Figure 8 reports comprehensive results that actuations do benefit on - going AI inference quality . Each dot represents a single actuation instance . The X - axis means the peak confidence in the 20 - second period immediately before an actuation triggered ( hereinafter before - actuation period ) . The Y - axis means the peak confidence in a variable - length period immediately after the actuation triggered ( hereinafter after - actuation period ) . The cut - off time for the latter is variable as we take into account only the period the participant actively reacts to the actuation . Â§ 8 . 2 . 2 details the cut - off time distribution . Once a participant ends her actuation - relevant reaction , the AIâ€™s performance is no longer tracked . Given the definition of X - and Y - axes , the dots inside the upper - left triangle represent the number of actuations which brought positive changes in the AI performance . Overall , 92 . 1 % of actuations brought improvement . A key finding is that an improvement could occur over almost the entire range of before - actuation conditions , not just before - actuation condition was poor . We investigated the other 7 . 9 % of cases where actuations brought negative changes . Many of those occurred when the participant was standing or too close to the camera ; her induced reaction , slightly approaching to the camera , resulted in her face outside from the cameraâ€™s FoV . A preventive measure may be to set peripheral margins of the FoV and to set the interior as actuation - eligible , so that she likely remains inside the FoV after reacting . Another prevention would be to apply actuations only if a before - actuation confidence threshold is not met , i . e . , â€˜do not actuate when already good - enoughâ€™ . Our results indicate that a threshold of 0 . 95 will prevent 46 % of these negative changes , and a threshold of 0 . 85 would prevent 54 % of them . 8 . 2 . 2 Temporal Responses : How AI Performance Changes over Time upon an Actuation ? Â§ 8 . 2 . 1 confirmed that actuations do change the on - going AI performance , mostly positively . Now we investigate how such changes gradually appear ( and disappear ) along time â€“ for a time range spanning the before - and after - actuation periods ( defined in Â§ 8 . 2 ) . For space - saving , we visualize the continuous traces from select actuation types and beneficiary sensors . Other actuation - sensor combinations exhibit similar temporal trends . Figure 9 and 10 plot the trends of confidence scores for the face ( MTCNN ) and body pose ( Detectron2 ) models , respectively . Figure 11 plots the trend of body weight estimation errors ( in kg ) . The figures represent the momentary distribution of confidence scores ( or errors ) every 10 - second interval . Vertical orange lines indicate the moment when actuation is triggered . Each figure accompanies a blue area plot at the top , representing the number of unfinished actuation instances being counted in each violin plot along time . Recall that , for each actuation issued , we stop tracking the performance once the participant finishes actuation - induced reactions . The above results disclose that it takes a while until the AI inference quality reaches meaningful improvement since an actuation was triggered . To our empirical observation , this â€˜response timeâ€™ seemed to be influenced ( a ) ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ , Ceiling Center ( b ) ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Partition Center Fig . 9 . Increase of confidence scores at face detection Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 18 â€¢ Cho et al . ( a ) ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Partition Center ( b ) ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Partition Center Fig . 10 . Increase of confidence scores at 2D pose estimation Fig . 11 . Decrease of MAE at Weight model â€“ ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Partition Center ( a ) CDF for ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( b ) CDF for ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ Fig . 12 . CDF of response times at face detection ( a ) CDF for ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( b ) CDF for ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ Fig . 13 . CDF of response times at 2D pose estimation ( 2nd round ) by diverse human factors , e . g . , individualsâ€™ different sensitivity to gradual sound changes or change blindness . Still , a bounded response time is preferable for practical applications . To this end , we present Figure 12 and 13 that shows the mainstream CDFs representing the time for the growing confidence to exceed a certain level of threshold . At face detection , it takes â‰¤ 23 s until 80 % of the actuation instances to exceed a confidence of 0 . 7 , and â‰¤ 24 s until 70 % to exceed 0 . 9 . For pose estimation , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ took 28 s and 98 s , respectively , for 70 % of the actuation instances to exceed a confidence of 8 . 0 . The results show that most response times are within 20 to 30 s in our actuation embodiment , except ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ which show longer response times in inducing body movements . 8 . 2 . 3 Between Settings : AI Performance between Baseline vs . Actuated Settings . Table 5 compares the average performance in the entire baseline settings vs . in the after - actuation periods of actuated settings , per model per vision sensor . Overall , for most AI models and sensors , actuations yield improvement not only over the before - actuation periods but also over the average baseline settings ( which is todayâ€™s conventional setting ) . The only exception is for the whiteboard - calendar ( WC ) camera with the face model ( MTCNN ) , where the actuated setting exhibits slightly lower confidence . After a thorough investigation of the raw videos , we verified that this sole exception is due to the WC cameraâ€™s low - height installation ( 1 m above the floor ) . Coincidentally , if an actuation is applied when the participant is at a specific location , this low - height view makes a tricky condition that may cause obstruction of the participantâ€™s face . We believe that the low - height installation of WC may prefer a different actuation to induce a favorable reaction , such as one that induces the person to step back from the current location . 8 . 2 . 4 Between Rounds : AI Performance between 1st vs . 2nd Rounds . Now we investigate possible performance differences between rounds ( i . e . , 1st â†’ 2nd ) attributable to novelty or knowledge effects . Referring back to Table 3 , Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 19 Table 5 . Performance comparison of baseline ( denoted bs . ) vs . actuated ( denoted act . ) settings in a face model [ 146 ] and in body models [ 6 , 150 , 161 ] . Values indicate confidence , whereas â€  indicates errors â€“ a lower value means improvement . Models Camera CC CL CR PC PL PR WB WC bs . act . bs . act . bs . act . bs . act . bs . act . bs . act . bs . act . bs . act . MTCNN 0 . 50 0 . 65 * 0 . 67 0 . 76 0 . 57 0 . 68 * 0 . 48 0 . 65 * 0 . 69 0 . 77 * 0 . 56 0 . 73 * 0 . 64 0 . 77 0 . 53 0 . 50 Detectron2 2 . 95 7 . 39 * 4 . 26 5 . 63 * 4 . 53 6 . 54 * CDN 0 . 24 0 . 50 * 0 . 46 0 . 49 0 . 45 0 . 50 Weight 30 . 24 â€  26 . 77 â€  ( a ) Awkward ( b ) Disturbing ( c ) Interesting ( d ) Useful Fig . 14 . Likert scale distributions of each user experience attribute the Performance Improvement dimension separately compares 1st - and 2nd - round results . Although the before - actuation confidences ( or errors ) sometimes vary between rounds , we observe that after - actuation performances are rather similar between the 1st and 2nd rounds . This result implies that the observable effectiveness of our actuation strategies may not be heavily affected by the participantsâ€™ knowledge or novelty factor that would likely change between rounds . 8 . 3 Human Perspective : Experiences and Behaviors in Actuated Environment How did the users perceive the actuated environment ? How did they behave ? Here we report the participantsâ€™ post - session interview results . For the 7 - pt Likert - scale questions , we report their ratings per actuation type , per user experience attribute . For the qualitative questions , three researchers coded each response , grouping them by actuation type and user experience attribute ( Â§ 7 . 6 ) . Then , they iteratively clustered the codes into high - level themes . The example quotes presented in this paper are representative of the broad themes grounded in our data [ 23 ] . We annotate each statement in ( Participant , Actuation type , User context ) format . Participant label is in either F ğ‘› or S ğ‘› format , indicating the f irst round and the s econd round , respectively . ğ‘› is the participant ID . Actuation type and User context labels are optionally given when a statement is related to a particular type and context . When we asked the participants how they felt about the environment , most of them responded they were comfortable . ( F06 ) : â€œI felt if I were in a cozy study cafe . â€ ( F01 ) : â€œIt felt like my dormitory room . â€ We asked if they were familiar with smart devices , what type of devices they have used . A smart speaker was the dominant response . ( F12 ) : â€œIâ€™m currently using a smart speaker called Kakao Mini , to schedule an alarm , to play music , and to ask about the weather . â€ We asked if they recall memorable actions . Every participant included 1 + non - ambient actuations . Few mentioned the surround speaker ( ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ) although it was an ambient type , because the default volume of background music was set somewhat high . ( F09 , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ , Reading a book ) : â€œI wished the background music was quieter when reading a book . But it became louder . â€ Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 20 â€¢ Cho et al . 8 . 3 . 1 Participantsâ€™ Experiences : Quantitative Ratings . We asked participants to rate their experiences over four user experience attributes : â€œHow { awkward , disturbing , interesting , useful } did you feel the actuated environment ? Please rate your experience ( 1 : not at all , 7 : definitely ) . â€ Figure 14 depicts their rating distributions . Overall , their negative feelings seemed marginal , supported by low ratings on the negative attributes , i . e . , Awkward ( ğœ‡ = 2 . 62 , ğœ = 1 . 58 ) and Disturbing ( ğœ‡ = 2 . 56 , ğœ = 1 . 78 ) on a 7 - pt scale . For Disturbing , they rated ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ least disturbing ( ğœ‡ = 1 . 42 , ğœ = 0 . 92 ) , followed by ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( ğœ‡ = 2 . 27 , ğœ = 1 . 46 ) . They felt neutral with ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( ğœ‡ = 3 . 52 , ğœ = 1 . 81 ) and ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ( ğœ‡ = 3 . 18 , ğœ = 1 . 78 ) . For Awkward , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ was rated neutral , i . e . , ( ğœ‡ = 3 . 22 , ğœ = 1 . 91 ) . Ratings of the awkwardness of the other four actuation types were similarly low , i . e . , 2 . 24 - 2 . 82 . We will reason this observation in Â§ 8 . 3 . 2 . The participants highly rated the positive attributes , i . e . , Interesting ( ğœ‡ = 4 . 95 , ğœ = 1 . 55 ) and Useful : ( ğœ‡ = 4 . 75 , ğœ = 1 . 85 ) . For Interesting , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ( ğœ‡ = 5 . 52 , ğœ = 1 . 10 ) and ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( ğœ‡ = 5 . 61 , ğœ = 1 . 07 ) were the top two . Given their familiarity with smart speakers , they found the â€˜proactiveâ€™ property interesting ; ( S12 , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ) : â€œ It was interesting that the speaker starts a conversation on its own . â€ For Useful , the non - ambient types were rated highly , i . e . , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( ğœ‡ = 5 . 79 , ğœ = 1 . 04 ) , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( ğœ‡ = 5 . 33 , ğœ = 1 . 29 ) , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ( ğœ‡ = 5 . 39 , ğœ = 1 . 04 ) . In contrast , the ambient types were rated neutral , i . e . , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( ğœ‡ = 3 . 85 , ğœ = 2 . 32 ) , ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( ğœ‡ = 3 . 55 , ğœ = 1 . 83 ) . The high usefulness of non - ambient actuations seemed mainly attributed by the rich and explicit information delivery , e . g . , ( S10 , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : â€œ It told me about a news headline and also I could see the details . I think it is quite useful . â€ Overall , we observed a high average rating ( 4 . 85 / 7 . 00 ) from the two positive UX attributes and a low average rating ( 2 . 61 / 7 . 00 ) from the two negative UX attributes . We reason such results by analyzing their responses below . 8 . 3 . 2 Participantsâ€™ Experiences : Qualitative Responses . Although the results in Â§ 8 . 3 . 1 indicate the overall levels of the participantsâ€™ awkward or disturbing experiences are low , we investigate the minority samples that they felt otherwise . Figure 14a shows ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ and ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ rise at higher Awkward ratings . They came from those who wondered about the cause of changes ; ( F07 , ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ) : â€œSuddenly I heard some bird - chirping and bonfire ; I wondered why that happened . â€ ( F12 , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ , Using a phone ) : â€œI couldnâ€™t figure out why the volume went up ; I wondered if I did something wrong . â€ Actuation types of auditory elements seem likely correlated with the degree of Awkward or Disturbing . Some participants compared ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ in pair , assessing that visual - only actuations were far less disturbing than the combination of visual and auditory . ( F05 , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Using a phone ) : â€œ [ ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ] is like air . It feels like the information was just floating thereâ€ We found that human context may vary the degree of awkward or disturbed . ( S04 , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ , Reading a book ) : â€œI felt [ smart speaker initiating a conversation ] disturbing as I was reading a book . Itâ€™d have been no problem if I was cleaning the room . â€ ( F11 , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , Drinking tea ) : â€œI was just drinking tea ; no serious thoughts . It was good to get a news pushed . â€ In our interviews , the participants expressed various levels of affinity with proactive speakers . Some felt proactive smart speakers natural ; their high affinity seemed correlated with low Awkward ratings . ( S05 , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ) : â€œTechnologies develop rapidly . I expected smart home devices would do such [ proactive operations ] soon . â€ Still , many took a proactive smart speaker as an exotic experience , rating highly on Interesting but also on Awkward . ( F18 , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ) : â€œIt was refreshing . â€ ( F07 , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ , Reading a book ) : â€œI was alone , and the speaker suddenly talked to me . It was awkward . â€ 8 . 3 . 3 Participantsâ€™ Behaviors : Reaction Rates . We now analyze reaction rates , i . e . , the ratio of actuation instances which participants turned to or moved toward the source ( Table 6 ) . ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ type marked the highest reaction rate ( 95 . 89 % ) in inducing the participant to turn her face . ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ was the second highest ( 87 . 65 % ) . Not surprisingly , Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 21 Table 6 . The percentage ( % ) of reaction rates ( the number of reacted actuations ) Induced Reaction Face Turn Face Turn & Body Movement Actuation All ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ All rounds 65 . 81 % ( 308 ) 51 . 82 % ( 57 ) 41 . 96 % ( 47 ) 68 . 48 % ( 63 ) 87 . 65 % ( 71 ) 95 . 89 % ( 70 ) 51 . 09 % ( 47 ) 47 . 95 % ( 35 ) 1st rounds 64 . 32 % ( 119 ) 42 . 86 % ( 18 ) 43 . 18 % ( 19 ) 71 . 79 % ( 28 ) 86 . 67 % ( 26 ) 93 . 33 % ( 28 ) 48 . 72 % ( 19 ) 33 . 33 % ( 10 ) 2nd rounds 68 . 25 % ( 129 ) 59 . 57 % ( 28 ) 36 . 96 % ( 17 ) 71 . 43 % ( 25 ) 93 . 94 % ( 31 ) 100 . 00 % ( 28 ) 54 . 29 % ( 19 ) 64 . 29 % ( 18 ) the ambient - type actuations , i . e . , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( 42 . 0 % ) and ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( 51 . 8 % ) , exhibited lower reaction rates than the non - ambient - types . Cross - referencing with Figure 14b , the results imply a trade - off between reaction rates and potential disturbance . 8 . 3 . 4 Participantsâ€™ Experiences : Comparison between 1st and 2nd Rounds . For the 14 participants who joined both rounds , we performed paired t - tests between their 7 - pt Likert scale ratings in the 1st and 2nd rounds . In Table 3 , the User Acceptance dimension shows marginal mean score differences between rounds or insufficient statistical significance . Only two pairs were of statistically significant differences : from Awkward attribute for ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ( 1 . 58 â†’ 2 . 50 , ğ‘ = 0 . 04 ) and from the Interesting attribute for ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( 2 . 89 â†’ 4 . 56 , ğ‘ = 0 . 04 ) . Both are the ambient actuation type . Qualitative responses support that the differences came from the participantsâ€™ knowledge about the actuatorsâ€™ operations . A few 1st - round responses mentioned ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ , specifically wondering why the sound effect transitioned ( e . g . , crackling bonfire , bird chirping ) . After they had learned from the 1st - round interviews , they tended to perceive the transitions more often in the 2nd round , possibly leading to an elevated Awkward level . Nonetheless , the absolute scores of Awkward remained at the lower end of the 7 - pt scale , implying that the overall user - perceived awkwardness was not high . Similarly , the 2nd - round resulted in more participants newly recognizing the change in ambient sound volume . ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , and ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ exhibited interesting trends in the 2nd round , despite insufficient statistical significance . The scores on Awkward and Disturbing tended to decrease where Interesting and Useful pre - dominantly increased . We speculate that the participants , who were little familiar with the proactive interaction in the 1st round , were getting used to them and appreciating them . ( S11 , ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : â€œAt the 1st round , it was kinda weird asking me if I want to know the news . But now I know I can decide which to hear or not . â€ ( S03 , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : â€œNow I know the display varies the details based on my distance . â€ We also analyzed potential differences in the participantsâ€™ reaction rates between rounds , for each pair of same - type actuations . None of the pairs exhibited statistically significant changes . However , it is noteworthy a slight decrease in ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ( 43 . 18 % â†’ 36 . 96 % ) , due to a few participants in the 2nd - round feeling less intention to react . ( S07 , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ) : â€œI feel no need to turn as I already know the changesâ€ Overall , the results so far exhibit positive implications towards lasting effects of AHA . Still , we stress that deriving a more conclusive assessment of potential novelty or knowledge effects would require extended deploy - ments . 8 . 4 Context Preserving Actuations We used a Human - Object Interaction ( CDN ) model to check whether the participantsâ€™ context changed across the moment of an actuation . For every second , oneâ€™s most plausible behaviors ( e . g . working on the computer , sitting , standing , etc . ) were inferred independently from all cameras . Then , the top - scored label was selected as the behavior at that second . We separately aggregated these labels for the before - actuation period and for the after - actuation period , then determined the representative behavior in each period . Table 7 lists the rate of behavior label changes across periods . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 22 â€¢ Cho et al . Table 7 . The percentage ( % ) of changing behavior per actuations between before the actuation and after the actuation Actuations ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ 12 . 73 % 6 . 82 % 45 . 16 % 16 . 18 % 35 . 29 % The result indicates that an actuation of ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( 35 . 29 % ) or ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ( 45 . 16 % ) type has a higher chance of changing the participantâ€™s behavior context , compared to other types of actuations ( 6 . 82 % - 16 . 18 % ) . This result is not surprising , because ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ induce the person to eventually walk towards the interactive display . Inspecting every single behavior - change with either ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ or ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , more than 70 % were the changes from â€˜sitâ€™ â†’ â€˜standâ€™ . For other actuation types , most were the same , i . e . , â€˜work on the computerâ€™ â†’ â€˜sitâ€™ . Note that these two behavior labels are not mutually exclusive . These results suggest two implications . Firstly , an actuation of ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ , or ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ is highly likely ( i . e . , 83 . 82 % - 93 . 18 % ) to preserve the userâ€™s behavioral context . Secondly , even in the few cases that do change the behavior , the actual change may not be necessarily a big jump in semantics . 9 DISCUSSION 9 . 1 AI - to - Human Actuation Guidelines We experimented various practical aspects of our actuation approach , including performance improvement , user experience and acceptance . We derived the following guidelines to facilitate informed applications of AHA in real - life . â€¢ Use non - ambient auditory actuations to expect the highest reaction rate of face turns . Our analysis reveals over 80 % of reaction rates upon ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ and ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ actuations ( Table 6 ) . Given their obtrusiveness , however , one may exercise this guideline as a last resort when other actuation types are inapplicable or unsuccessful . â€¢ Avoid non - ambient actuations when the person is engaged in a non - trivial activity . Our study reveals non - ambient actuations introduce varying levels of obtrusiveness depending on the userâ€™s on - going activity ( Â§ 8 . 3 . 2 ) . Expect she may accept non - ambient actuations with less disturbance when her engagement is low . â€¢ Incorporate causality implications in ambient - type actuation designs . Our unexpected finding was that a few participants rated ğ· ğ‘ğ‘šğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ - and ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ - type actuations awkward despite their ambient property ; a major reason was the unclear cause of changes ( Â§ 8 . 3 . 2 ) . Notably , some participants attempted to correlate their contextual behaviors with such changes , e . g . , ( F06 , ğ‘† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ ) : â€œThe volume has increased because I read a book . â€ We recommend designing ambient - type actuations that imply causality , even though it is illusive . â€¢ Choose an actuation type that is less influential to the human context that the AI captures . We found that actuations inducing physical relocation ( ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) likely change the HOI context ( Â§ 8 . 4 ) . Given the human context of the AIâ€™s interest , choose an actuation type whose expected human reaction is orthogonal to the target context or of low probability to influence it . Refer to empirically measured probabilities such as Table 7 . â€¢ Prioritize visual - only actuations in non - urgent situations . Visual - only actuation , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ , has been found to exhibit low disturbance but take varying delays until being noticed . If the application can tolerate a delayed improvement of the inference quality , consider visual - only actuation as a primary choice ( Â§ 3 ) . â€¢ Consider an actuation with interactive displays for long retention of favorable facial orientation . The two - way nature of ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ and ğ·ğ‘† ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ yields an actuation that lasts longer than the others ( Figure 6 ) , indicating extended retention of favored facial orientation . If the beneficiary AI model prefers a continuous view of the targetâ€™s face [ 56 ] , an actuation type involving interactive displays would be a priority consideration . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 23 â€¢ Consider individual differences in affinity with specific actuators : Our experiments discovered individ - ualsâ€™ varying affinity with the proactiveness of smart speakers ( Â§ 8 . 3 . 2 ) . If available , take into account the prior of the userâ€™s personal affinity with an actuation - specific property in the process to choose an appropriate actuation type . 9 . 2 Acceptance of AI - to - Human Actuation In interviews , we discussed the general acceptability of the actuated environment and the number of actuations . For the general acceptability , most participants responded positively in three perspectives : ( 1 ) actuations are perceived as natural near - future home devicesâ€™ actions . ( ( F01 ) : â€œ I think I would live with these devices in near future . â€ ) ; ( 2 ) actuations are perceived as a kind of care . ( ( F05 ) : â€œ The environment took care of me at the right level of not interrupting my concentration . â€ ) ; ( 3 ) actuations become more familiar . ( ( S11 ) : â€œ I felt the environment comfortable , and didnâ€™t mind actuations being triggered . â€ , ( S03 ) : â€œ I think I can stay here longer â€ ) . Overall positive user experiences ( Â§ 8 . 3 . 1 ) and a trend of decreasing awkward and disturbing for non - ambient actuations ( Â§ 8 . 3 . 4 ) support these responses . Also , two participants who initially reported ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ negatively did not report so in the 2nd round . These trends imply that people may accept actuated settings as a future smart home and become familiar as they continue to co - exist with them . Most participants replied that the number of actuations was moderate and comfortable . Interestingly , three in the 2nd round noticed fewer actuations than in the 1st round , though the numbers were the same . A few wanted more actuations , e . g . , ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ , when they are bored , doing nothing , or wandering around . In contrast , one in the 2nd round , who had negatively responded about the acceptability , suggested that lowering the frequency of ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ would increase the acceptability of the environment . These responses are analogous with the varying affinity of ğ‘† ğ‘ ğ‘šğ‘ğ‘Ÿğ‘¡ and the human context factors ( Â§ 8 . 3 . 2 ) . Although the above results provide overall positive implications , it is important to note the occasional negative user experiences derived from actuations . As reported in Â§ 8 . 3 . 2 , we found few participants feeling ambient actuations awkward , looking for the cause of the changes . Non - ambient auditory actuations were sometimes reported to be more disturbing , due to their obtrusive nature , than other types . Participants also reported varying levels of disturbance depending on their contexts . Still , they opened a room for making actuations positive to them , such as correlating their context with ambient actuations and being willing to have non - ambient auditory actuations when they are not busy . Additionally , almost all participants called for customizations in non - ambient actuations , relating the content to their real - life ( F08 , ğ· ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ) : â€œ It would be much more useful if the information is about my schedules or memos . â€ Putting together our findings , our suggested setup could be a good starting point for most people , but the type and the number of actuations could be tailored to the person , and also to her growing familiarity with the actuated setting . Furthermore , we hope our guidelines ( Â§ 9 . 1 ) provide insights when designing and triggering actuations to diminish the reported negative experiences and increase the acceptability of the actuated setup . 9 . 3 Limitations 9 . 3 . 1 User - control and Customization of Actuator Devices . Our experiment did not allow the participants to control or reconfigure the actuator devices . In a real home , however , the resident has a full ownership and thus control of the devices , e . g . , stopping the music when watching a movie . It would be a next - step milestone to devise dynamic actuation mechanisms that adapt to the user - controlled device status and auto - extract a viable actuation therein . 9 . 3 . 2 Manual Administration of Actuation - triggering . Automatically determining an actuation - demanding sit - uation and the right type of actuation is beyond the scope of this paper ( Â§ 5 ) . Our findings will shed light on Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 24 â€¢ Cho et al . developing the logic , cost metrics , and input variables to automate determination of the timing and type of an actuation to trigger . 9 . 3 . 3 Experiments Focused on Single - person Smart Home . We acknowledge that the eventual scope of AHA will extend to multi - personal spaces . In near future , public spaces would be rich in AI sensors and actuators that respond to various contexts of passers - by [ 2 , 5 , 28 , 51 , 52 , 57 , 71 , 86 , 87 , 112 ] . We envision that the basic principles of AHA would be applicable to such spaces and benefit the AI sensors therein , although the embodiment of individual actuations may differ . Furthermore , multi - personal social context needs to be considered when estimating the awkwardness or the context changes due to actuation [ 88 ] . The literature on the social dimension of human interruptibility may provide guidance [ 34 , 49 ] . 9 . 3 . 4 Actuation May Influence Target Context . As discussed in Â§ 9 . 1 , there would be varying degrees of influence relationship between the type of target context and the type of actuation being exerted . Static contexts such as body weight and identity , are certainly invariant . Dynamic contexts such as mood or activities may or may not change upon actuation . For dynamic , the chance of a change would be actuation type - dependent ( Â§ 8 . 4 ) . A similar trend could be expected with mood ; an actuation type with high surprise may result in a momentary facial change . The response time may be another variable ; the userâ€™s context would likely change as the response time increases . We call for a further study on deriving an informed model specializing in the actuation - to - context influenceability . 9 . 3 . 5 Long - term Deployment . It is an open question how the effect of actuations would last over time . As the users learn about the mechanism , they may become sensitized to subtle actuations . Or , as they get used to the actuated environment , novelty effects may diminish . For initial explorations , we performed two rounds of experiments ; most metrics remained similar between rounds ( Â§ 8 . 2 . 4 , Â§ 8 . 3 . 4 ) . Still , extended deployments would be needed to derive concrete answers . 9 . 4 Mini - experiment on Resource - efficient Practical Perception of Unfavorable Conditions Before concluding , we ask ourselves about a premise to make AHA work automatically â€“ existing AI models not only sense the original human context ( e . g . , gender , facial expressions ) but also perceive an additional context indicating the current unfavorable condition , if any ( e . g . , sideways facial view to the left ) , at minimal complexity growth . This ability would advise the automated system of the directionality of a promising actuation . Say that a facial emotion network additionally infers oneâ€™s facial orientation being off - ideal by 45Â° to the left . This may be a hint to the system to choose an actuation that originates from her right side . We admit it is not comprehensive enough to cover diverse choices of available actuations , but it may halve or narrow down the set of actuations to choose from . To show its feasibility , we added a lightweight branch - network to our gender classification model so that it additionally estimates an actuation - demanding condition ( i . e . facial angles ) . We explored a total of 9 branch - networks , trained with K - Face dataset [ 22 ] . Table 8 specifies the two top - performing branch - networks . Both had very small MAEs ( < 4Â° ) , but their sizes differ greatly â€“ 2 . 3M vs . 66k parameters , given the baseline model having 6 . 0M . Obviously , resource - hungry home AI products would strongly favor the second architecture as its complexity growth is only 1 % of the baseline . Although our branch - network resulted in acceptable error rates with marginal complexity growth , we do not generalize this single case ; thorough verification is beyond the scope of this mini - experiment . However , our approach is consistent with the practice of transfer learning [ 104 , 160 ] ; our observation may not necessarily be a lucky outlier . We deployed the baseline and its branched - out model on a Raspberry Pi 4 . Table 9 compares the latencies . The branched - out model demonstrates the feasibility of extra estimation of an unfavorable condition at negligible latency increase ( 1 . 09 - 2 . 36 % ) . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 25 Table 8 . Two of the branch network candidates with the best performance Layer to Branch Network to Attach MAE ( Â° ) # of Training Parameters 5th Conv2D layer Conv2D & MaxPool2D layer , Number of filters = 512 2 . 48 2 , 363 , 906 1st FC layer Dense layer , Number of units = 128 3 . 82 66 , 178 Table 9 . Latency Comparison in Raspberry Pi 4 & Intel Neural Computes Stick2 Environment Input Batch Size ( # of images ) Baseline Latency ( ms ) Latency with Branch Network ( ms ) Increase Ratio ( % ) 4 25 . 43 26 . 03 2 . 36 20 123 . 80 125 . 15 1 . 09 80 488 . 59 494 . 39 1 . 19 10 CONCLUSION In this paper , we proposed a novel concept of AI - to - human actuation ( AHA ) , which is a strategy to strengthen a human - targeted AI modelâ€™s effective performances without further sophisticating the model , by leveraging AI - favorably steered subtle human reactions . We presented a systematic exploration in terms of methods , benefits , and acceptance of AHA . We instrumented a testbed in a real apartment with rich sensors , actuators , human - targeted visual AIs , and proactive smart home agents , where various types of actuation were embodied . Our experiments with 20 participants resulted in 34 1 - hour - long sessions , 468 instances of actuation applied , and 420 - hour - long videos . We assessed AHA in terms of AI accuracy , confidence , response time , as well as human noticeability , acceptability , and knowledge factors . We anticipate that our initial findings call for the communityâ€™s interest in further deepening proactive actuation strategies for synergistic AI - empowered human living space and provide a new perspective on human - AI interaction . ACKNOWLEDGMENTS This research was supported by the National Research Foundation of Korea ( NRF ) grant funded by the Korea government ( MSIT ) ( No . 2021R1A2C200386612 ) . This research was also partly supported by the IITP grants by the MSIT , Korea ( IITP - 2020 - 0 - 01778 , IITP - 2022 - 0 - 00240 ) , and by the KOCCA grant funded by the Ministry of Culture , Sports and Tourism in 2022 ( Project Number : R2021040136 , Contribution Rate : 10 % ) . REFERENCES [ 1 ] [ n . d . ] . Find & improve your locationâ€™s accuracy . Retrieved Aug . 11 , 2022 from https : / / support . google . com / maps / answer / 2839911 ? hl = en [ 2 ] Christopher Ackad , Martin Tomitsch , and Judy Kay . 2016 . Skeletons and silhouettes : Comparing user representations at a gesture - based large display . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 2343 â€“ 2347 . [ 3 ] Fadel Adib , Hongzi Mao , Zachary Kabelac , Dina Katabi , and Robert C Miller . 2015 . Smart homes that monitor breathing and heart rate . In Proceedings of the 33rd annual ACM conference on human factors in computing systems . 837 â€“ 846 . [ 4 ] alessiosarullo . 2019 . KeypointRCNNpredictedprobabilitiesgreaterthan1 ? https : / / github . com / facebookresearch / detectron2 / issues / 319 . [ 5 ] Florian Alt , Daniel Buschek , David Heuss , and JÃ¶rg MÃ¼ller . 2021 . Orbuculum - Predicting When Users Intend to Leave Large Public Displays . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 1 ( 2021 ) , 1 â€“ 16 . [ 6 ] Can Yilmaz Altinigne , Dorina Thanou , and Radhakrishna Achanta . 2020 . Height and weight estimation from unconstrained images . In ICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 2298 â€“ 2302 . [ 7 ] Amazon . 2022 . Proactive Events API Reference . Retrieved Aug 10 , 2022 from https : / / developer . amazon . com / en - US / docs / alexa / smapi / proactive - events - api . html [ 8 ] Saleema Amershi , Dan Weld , Mihaela Vorvoreanu , Adam Fourney , Besmira Nushi , Penny Collisson , Jina Suh , Shamsi Iqbal , Paul N . Bennett , Kori Inkpen , Jaime Teevan , Ruth Kikin - Gil , and Eric Horvitz . 2019 . Guidelines for Human - AI Interaction . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI â€™19 ) . Association for Computing Machinery , New York , NY , USA , 1 â€“ 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300233 [ 9 ] Arlo . 2022 . Arlo Indoor Security Camera . Retrieved May 15 , 2022 from https : / / www . arlo . com / en - us / cameras / essential / arlo - essential - indoor . html Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 26 â€¢ Cho et al . [ 10 ] Ilhan Aslan , Andreas Uhl , Alexander Meschtscherjakov , and Manfred Tscheligi . 2016 . Design and exploration of mid - air authentication gestures . ACM Transactions on Interactive Intelligent Systems ( TiiS ) 6 , 3 ( 2016 ) , 1 â€“ 22 . [ 11 ] Nikola Banovic , Ticha Sethapakdi , Yasasvi Hari , Anind K Dey , and Jennifer Mankoff . 2019 . The limits of expert text entry speed on mobile keyboards with autocorrect . In Proceedings of the 21st International Conference on Human - Computer Interaction with Mobile Devices and Services . 1 â€“ 12 . [ 12 ] Abdelkareem Bedri , Diana Li , Rushil Khurana , Kunal Bhuwalka , and Mayank Goel . 2020 . Fitbyte : Automatic diet monitoring in unconstrained situations using multimodal sensing on eyeglasses . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 â€“ 12 . [ 13 ] Frank Bentley , Chris Luvogt , Max Silverman , Rushani Wirasinghe , Brooke White , and Danielle Lottridge . 2018 . Understanding the long - term use of smart speaker assistants . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 3 ( 2018 ) , 1 â€“ 24 . [ 14 ] Gilbert Beyer , Vincent Binder , Nina JÃ¤ger , and Andreas Butz . 2014 . The puppeteer display : attracting and actively shaping the audience with an interactive public banner display . In Proceedings of the 2014 conference on Designing interactive systems . 935 â€“ 944 . [ 15 ] Meera M Blattner and Ephraim P Glinert . 1996 . Multimodal integration . IEEE multimedia 3 , 4 ( 1996 ) , 14 â€“ 24 . [ 16 ] Djallel Eddine Boubiche , Muhammad Imran , Aneela Maqsood , and Muhammad Shoaib . 2019 . Mobile crowd sensing â€“ taxonomy , applications , challenges , and solutions . Computers in Human Behavior 101 ( 2019 ) , 352 â€“ 370 . [ 17 ] Narae Cha , Auk Kim , Cheul Young Park , Soowon Kang , Mingyu Park , Jae - Gil Lee , Sangsu Lee , and Uichin Lee . 2020 . Hello there ! is now a good time to talk ? Opportune moments for proactive interactions with smart speakers . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 3 ( 2020 ) , 1 â€“ 28 . [ 18 ] Guoguo Chen , Carolina Parada , and Georg Heigold . 2014 . Small - footprint keyword spotting using deep neural networks . In 2014 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 4087 â€“ 4091 . [ 19 ] Yu Cheng , Bo Yang , Bo Wang , Wending Yan , and Robby T Tan . 2019 . Occlusion - aware networks for 3d human pose estimation in video . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 723 â€“ 732 . [ 20 ] Woohyeok Choi , Jeungmin Oh , Taiwoo Park , Seongjun Kang , Miri Moon , Uichin Lee , Inseok Hwang , Darren Edge , and Junehwa Song . 2016 . Designing interactive multiswimmer exergames : a case study . ACM Transactions on Sensor Networks ( TOSN ) 12 , 3 ( 2016 ) , 1 â€“ 40 . [ 21 ] Woohyeok Choi , Jeungmin Oh , Taiwoo Park , Seongjun Kang , Miri Moon , Uichin Lee , Inseok Hwang , and Junehwa Song . 2014 . MobyDick : an interactive multi - swimmer exergame . In Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems . 76 â€“ 90 . [ 22 ] Yeji Choi , Hyunjung Park , Gi Pyo Nam , Haksub Kim , Heeseung Choi , Junghyun Cho , and Ig - Jae Kim . 2021 . K - FACE : A Large - Scale KIST Face Database in Consideration with Unconstrained Environments . https : / / doi . org / 10 . 48550 / ARXIV . 2103 . 02211 [ 23 ] Juliet Corbin and Anselm Strauss . 2014 . Basics of qualitative research : Techniques and procedures for developing grounded theory . Sage publications . [ 24 ] Jean Costa , FranÃ§ois GuimbretiÃ¨re , Malte F Jung , and Tanzeem Choudhury . 2019 . Boostmeup : Improving cognitive performance in the moment by unobtrusively regulating emotions with a smartwatch . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 3 , 2 ( 2019 ) , 1 â€“ 23 . [ 25 ] Andrea Cuadra , Shuran Li , Hansol Lee , Jason Cho , and Wendy Ju . 2021 . My Bad ! Repairing Intelligent Voice Assistant Errors Improves Interaction . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) , 1 â€“ 24 . [ 26 ] Kadian Davis , Evans Owusu , Jun Hu , Lucio Marcenaro , Carlo Regazzoni , and Loe Feijs . 2016 . Promoting social connectedness through human activity - based ambient displays . In Proceedings of the international symposium on interactive technology and ageing populations . 64 â€“ 76 . [ 27 ] Yashar Deldjoo , Markus Schedl , Paolo Cremonesi , and Gabriella Pasi . 2020 . Recommender systems leveraging multimedia content . ACM Computing Surveys ( CSUR ) 53 , 5 ( 2020 ) , 1 â€“ 38 . [ 28 ] Zhiwei Deng , Mengyao Zhai , Lei Chen , Yuhao Liu , Srikanth Muralidharan , Mehrsan Javan Roshtkhari , and Greg Mori . 2015 . Deep structured models for group activity recognition . arXiv preprint arXiv : 1506 . 04191 ( 2015 ) . [ 29 ] Ecobee . 2022 . Ecobee Smart Camera . Retrieved May 15 , 2022 from https : / / www . ecobee . com / en - us / cameras / smart - camera - with - voice - control / [ 30 ] Ariel Ephrat , Inbar Mosseri , Oran Lang , Tali Dekel , Kevin Wilson , Avinatan Hassidim , William T Freeman , and Michael Rubinstein . 2018 . Looking to listen at the cocktail party : A speaker - independent audio - visual model for speech separation . arXiv preprint arXiv : 1804 . 03619 ( 2018 ) . [ 31 ] Ranmalee Eramudugolla , Dexter RF Irvine , Ken I McAnally , Russell L Martin , and Jason B Mattingley . 2005 . Directed attention eliminates â€˜change deafnessâ€™ in complex auditory scenes . Current Biology 15 , 12 ( 2005 ) , 1108 â€“ 1113 . [ 32 ] Sachin Sudhakar Farfade , Mohammad J Saberian , and Li - Jia Li . 2015 . Multi - view face detection using deep convolutional neural networks . In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval . 643 â€“ 650 . [ 33 ] Julia Fink , ValÃ©rie Bauwens , FrÃ©dÃ©ric Kaplan , and Pierre Dillenbourg . 2013 . Living with a vacuum cleaning robot . International Journal of Social Robotics 5 , 3 ( 2013 ) , 389 â€“ 408 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 27 [ 34 ] James Fogarty , Scott E Hudson , Christopher G Atkeson , Daniel Avrahami , Jodi Forlizzi , Sara Kiesler , Johnny C Lee , and Jie Yang . 2005 . Predicting human interruptibility with sensors . ACM Transactions on Computer - Human Interaction ( TOCHI ) 12 , 1 ( 2005 ) , 119 â€“ 146 . [ 35 ] Jodi Forlizzi and Carl DiSalvo . 2006 . Service robots in the domestic environment : a study of the roomba vacuum in the home . In Proceedings of the 1st ACM SIGCHI / SIGART conference on Human - robot interaction . 258 â€“ 265 . [ 36 ] Jutta Fortmann , Tim Claudius Stratmann , Susanne Boll , Benjamin Poppinga , and Wilko Heuten . 2013 . Make me move at work ! An ambient light display to increase physical activity . In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops . IEEE , 274 â€“ 277 . [ 37 ] Raghu K Ganti , Fan Ye , and Hui Lei . 2011 . Mobile crowdsensing : current state and future challenges . IEEE communications Magazine 49 , 11 ( 2011 ) , 32 â€“ 39 . [ 38 ] Ruohan Gao and Kristen Grauman . 2021 . Visualvoice : Audio - visual speech separation with cross - modal consistency . In 2021 IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) . IEEE , 15490 â€“ 15500 . [ 39 ] Yang Gao , Yincheng Jin , Jiyang Li , Seokmin Choi , and Zhanpeng Jin . 2020 . EchoWhisper : Exploring an Acoustic - based Silent Speech Interface for Smartphone Users . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 3 ( 2020 ) , 1 â€“ 27 . [ 40 ] Shaona Ghosh and Per Ola Kristensson . 2017 . Neural networks for text correction and completion in keyboard decoding . arXiv preprint arXiv : 1709 . 06429 ( 2017 ) . [ 41 ] E Bruce Goldstein and Laura Cacciamani . 2021 . Sensation and perception . Cengage Learning . [ 42 ] Google . 2022 . Nest Indoor & Outdoor Smart Security Cameras . Retrieved May 15 , 2022 from https : / / store . google . com / category / nest _ cams [ 43 ] Google . 2022 . Notifications for smart home Actions . Retrieved Aug 10 , 2022 from https : / / developers . google . com / assistant / smarthome / develop / notifications [ 44 ] Saul Greenberg , Nicolai Marquardt , Till Ballendat , Rob Diaz - Marino , and Miaosen Wang . 2011 . Proxemic interactions : the new ubicomp ? interactions 18 , 1 ( 2011 ) , 42 â€“ 50 . [ 45 ] Chris Greenhalgh , Adrian Hazzard , Sean McGrath , and Steve Benford . 2016 . GeoTracks : Adaptive music for everyday journeys . In Proceedings of the 24th ACM international conference on Multimedia . 42 â€“ 46 . [ 46 ] Bin Guo , Zhu Wang , Zhiwen Yu , Yu Wang , Neil Y Yen , Runhe Huang , and Xingshe Zhou . 2015 . Mobile crowd sensing and computing : The review of an emerging human - powered sensing paradigm . ACM computing surveys ( CSUR ) 48 , 1 ( 2015 ) , 1 â€“ 31 . [ 47 ] Song Han , Huizi Mao , and William J Dally . 2015 . Deep compression : Compressing deep neural networks with pruning , trained quantization and huffman coding . arXiv preprint arXiv : 1510 . 00149 ( 2015 ) . [ 48 ] Seungyeop Han , Haichen Shen , Matthai Philipose , Sharad Agarwal , Alec Wolman , and Arvind Krishnamurthy . 2016 . Mcdnn : An approximation - based execution framework for deep stream processing under resource constraints . In Proceedings of the 14th Annual International Conference on Mobile Systems , Applications , and Services . 123 â€“ 136 . [ 49 ] RikardHarrandVictorKaptelinin . 2007 . Unpackingthesocialdimensionofexternalinterruptions . In Proceedingsofthe2007international ACM conference on Supporting group work . 399 â€“ 408 . [ 50 ] Jun He , Dongliang Li , Bin Yang , Siming Cao , Bo Sun , and Lejun Yu . 2017 . Multi view facial action unit detection based on CNN and BLSTM - RNN . In 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition ( FG 2017 ) . IEEE , 848 â€“ 853 . [ 51 ] Javier Hernandez , Mohammed Hoque , Will Drevo , and Rosalind W Picard . 2012 . Mood meter : counting smiles in the wild . In Proceedings of the 2012 ACM Conference on Ubiquitous Computing . 301 â€“ 310 . [ 52 ] Luke Hespanhol and Martin Tomitsch . 2015 . Strategies for intuitive interaction in public urban spaces . Interacting with Computers 27 , 3 ( 2015 ) , 311 â€“ 326 . [ 53 ] Geoffrey Hinton , Oriol Vinyals , Jeff Dean , et al . 2015 . Distilling the knowledge in a neural network . arXiv preprint arXiv : 1503 . 02531 2 , 7 ( 2015 ) . [ 54 ] Marius Hoggenmueller , Alexander Wiethoff , and Martin Tomitsch . 2018 . Designing low - res lighting displays as ambient gateways to smart devices . In Proceedings of the 7th ACM International Symposium on Pervasive Displays . 1 â€“ 8 . [ 55 ] Zhengwei Huang , Ming Dong , Qirong Mao , and Yongzhao Zhan . 2014 . Speech emotion recognition using CNN . In Proceedings of the 22nd ACM international conference on Multimedia . 801 â€“ 804 . [ 56 ] Sinh Huynh , Rajesh Krishna Balan , JeongGil Ko , and Youngki Lee . 2019 . VitaMon : measuring heart rate variability using smartphone front camera . In Proceedings of the 17th Conference on Embedded Networked Sensor Systems . 1 â€“ 14 . [ 57 ] Inseok Hwang , Qi Han , and Archan Misra . 2005 . MASTAQ : a middleware architecture for sensor applications with statistical quality constraints . In Third IEEE International Conference on Pervasive Computing and Communications Workshops . IEEE , 390 â€“ 395 . [ 58 ] Inseok Hwang , Hyukjae Jang , Lama Nachman , and Junehwa Song . 2010 . Exploring inter - child behavioral relativity in a shared social environment : a field study in a kindergarten . In Proceedings of the 12th ACM international conference on Ubiquitous computing . 271 â€“ 280 . [ 59 ] Inseok Hwang , Hyukjae Jang , Taiwoo Park , Aram Choi , Youngki Lee , Chanyou Hwang , Yanggui Choi , Lama Nachman , and Junehwa Song . 2012 . Leveraging childrenâ€™s behavioral distribution and singularities in new interactive environments : Study in kindergarten field trips . In International Conference on Pervasive Computing . Springer , 39 â€“ 56 . [ 60 ] Inseok Hwang , Youngki Lee , Taiwoo Park , and Junehwa Song . 2012 . Toward a mobile platform for pervasive games . In Proceedings of the first ACM international workshop on Mobile gaming . 19 â€“ 24 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 28 â€¢ Cho et al . [ 61 ] Inseok Hwang , Youngki Lee , Chungkuk Yoo , Chulhong Min , Dongsun Yim , and John Kim . 2019 . Towards interpersonal assistants : next - generation conversational agents . IEEE Pervasive Computing 18 , 2 ( 2019 ) , 21 â€“ 31 . [ 62 ] Inseok Hwang , Chungkuk Yoo , Chanyou Hwang , Dongsun Yim , Youngki Lee , Chulhong Min , John Kim , and Junehwa Song . 2014 . TalkBetter : family - driven mobile intervention care for children with language delay . In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing . 1283 â€“ 1296 . [ 63 ] Hyukjae Jang , Sungwon Peter Choe , Inseok Hwang , Chanyou Hwang , Lama Nachman , and Junehwa Song . 2012 . RubberBand : augmenting teacherâ€™s awareness of spatially isolated children on kindergarten field trips . In Proceedings of the 2012 ACM conference on ubiquitous computing . 236 â€“ 239 . [ 64 ] Melinda S Jensen , Richard Yao , Whitney N Street , and Daniel J Simons . 2011 . Change blindness and inattentional blindness . Wiley Interdisciplinary Reviews : Cognitive Science 2 , 5 ( 2011 ) , 529 â€“ 546 . [ 65 ] GÃ¼l Kaner , HÃ¼seyin UÄŸur GenÃ§ , Salih Berk DinÃ§er , Deniz ErdoÄŸan , and Aykut CoÅŸkun . 2018 . GROW : a smart bottle that uses its surface as an ambient display to motivate daily water intake . In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems . 1 â€“ 6 . [ 66 ] Bumsoo Kang , Inseok Hwang , Jinho Lee , Seungchul Lee , Taegyeong Lee , Youngjae Chang , and Min Kyung Lee . 2018 . My being to your place , your being to my place : Co - present robotic avatars create illusion of living together . In Proceedings of the 16th Annual International Conference on Mobile Systems , Applications , and Services . 54 â€“ 67 . [ 67 ] Bumsoo Kang , Seungwoo Kang , and Inseok Hwang . 2021 . MomentMeld : AI - augmented Mobile Photographic Memento towards Mutually Stimulatory Inter - generational Interaction . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 â€“ 16 . [ 68 ] Bumsoo Kang , Sujin Lee , Alice Oh , Seungwoo Kang , Inseok Hwang , and Junehwa Song . 2015 . Towards Understanding Relational Orientation : Attachment Theory and Facebook Activities . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing . 1404 â€“ 1415 . [ 69 ] Bumsoo Kang , Chulhong Min , Wonjung Kim , Inseok Hwang , Chunjong Park , Seungchul Lee , Sung - Ju Lee , and Junehwa Song . 2017 . Zaturi : We put together the 25th hour for you . create a book for your baby . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . 1850 â€“ 1863 . [ 70 ] Ryoma Kawajiri , Masamichi Shimosaka , and Hisashi Kashima . 2014 . Steered crowdsensing : Incentive design towards quality - oriented place - centric crowdsensing . In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing . 691 â€“ 701 . [ 71 ] Rushil Khurana , Karan Ahuja , Zac Yu , Jennifer Mankoff , Chris Harrison , and Mayank Goel . 2018 . GymCam : Detecting , recognizing and tracking simultaneous exercises in unconstrained scenes . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 4 ( 2018 ) , 1 â€“ 17 . [ 72 ] Auk Kim , Woohyeok Choi , Jungmi Park , Kyeyoon Kim , and Uichin Lee . 2018 . Interrupting drivers for interactions : Predicting opportune moments for in - vehicle proactive auditory - verbal tasks . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 4 ( 2018 ) , 1 â€“ 28 . [ 73 ] Byoungjip Kim , SangJeong Lee , Youngki Lee , Inseok Hwang , Yunseok Rhee , and Junehwa Song . 2011 . Mobiiscape : Middleware support for scalable mobility pattern monitoring of moving objects in a large - scale city . Journal of Systems and Software 84 , 11 ( 2011 ) , 1852 â€“ 1870 . [ 74 ] Wonjung Kim , Seungchul Lee , Youngjae Chang , Taegyeong Lee , Inseok Hwang , and Junehwa Song . 2021 . Hivemind : social control - and - use of IoT towards democratization of public spaces . In Proceedings of the 19th Annual International Conference on Mobile Systems , Applications , and Services . 467 â€“ 482 . [ 75 ] Wonjung Kim , Seungchul Lee , Seonghoon Kim , Sungbin Jo , Chungkuk Yoo , Inseok Hwang , Seungwoo Kang , and Junehwa Song . 2020 . Dyadic Mirror : Everyday Second - person Live - view for Empathetic Reflection upon Parent - child Interaction . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 3 ( 2020 ) , 1 â€“ 29 . [ 76 ] Yuhwan Kim , Seungchul Lee , Inseok Hwang , Hyunho Ro , Youngki Lee , Miri Moon , and Junehwa Song . 2014 . High5 : promoting interpersonal hand - to - hand touch for vibrant workplace with electrodermal sensor watches . In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing . 15 â€“ 19 . [ 77 ] Janin Koch , AndrÃ©s Lucero , Lena Hegemann , and Antti Oulasvirta . 2019 . May AI ? Design ideation with cooperative contextual bandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 â€“ 12 . [ 78 ] Thomas Kosch , Kevin Wennrich , Daniel Topp , Marcel Muntzinger , and Albrecht Schmidt . 2019 . The digital cooking coach : using visual and auditory in - situ instructions to assist cognitively impaired during cooking . In Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments . 156 â€“ 163 . [ 79 ] Sanna KuoppamÃ¤ki , Sylvaine Tuncer , Sara Eriksson , and Donald McMillan . 2021 . Designing Kitchen Technologies for Ageing in Place : A Video Study of Older Adultsâ€™ Cooking at Home . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 2 ( 2021 ) , 1 â€“ 19 . [ 80 ] Chunggi Lee , Sanghoon Kim , Dongyun Han , Hongjun Yang , Young - Woo Park , Bum Chul Kwon , and Sungahn Ko . 2020 . GUIComp : A GUI Design Assistant with Real - Time , Multi - Faceted Feedback . Association for Computing Machinery , New York , NY , USA , 1 â€“ 13 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 29 https : / / doi . org / 10 . 1145 / 3313831 . 3376327 [ 81 ] Haechan Lee , Miri Moon , Taiwoo Park , Inseok Hwang , Uichin Lee , and Junehwa Song . 2013 . Dungeons & swimmers : designing an interactive exergame for swimming . In Proceedings of the 2013 ACM conference on Pervasive and Ubiquitous Computing adjunct publication . 287 â€“ 290 . [ 82 ] Jinho Lee , Inseok Hwang , Thomas Hubregtsen , Anne E Gattiker , and Christopher M Durham . 2017 . Sci - Fii : Speculative conversational interface framework for incremental inference on modularized services . In 2017 18th IEEE International Conference on Mobile Data Management ( MDM ) . IEEE , 278 â€“ 285 . [ 83 ] Jinho Lee , Inseok Hwang , Thomas S Hubregtsen , Anne E Gattiker , and Christopher M Durham . 2019 . Accelerating conversational agents built with off - the - shelf modularized services . IEEE Pervasive Computing 18 , 2 ( 2019 ) , 47 â€“ 57 . [ 84 ] Jungeun Lee , Sungnam Kim , Minki Cheon , Hyojin Ju , JaeEun Lee , and Inseok Hwang . 2022 . SleepGuru : Personalized Sleep Planning System for Real - life Actionability and Negotiability . In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology . 1 â€“ 16 . [ 85 ] Sunok Lee , Minji Cho , and Sangsu Lee . 2020 . What If Conversational Agents Became Invisible ? Comparing Usersâ€™ Mental Models According to Physical Entity of AI Speaker . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 3 ( 2020 ) , 1 â€“ 24 . [ 86 ] Youngki Lee , Younghyun Ju , Chulhong Min , Seungwoo Kang , Inseok Hwang , and Junehwa Song . 2012 . Comon : Cooperative ambience monitoring platform with continuity and benefit awareness . In Proceedings of the 10th international conference on Mobile systems , applications , and services . 43 â€“ 56 . [ 87 ] Youngki Lee , Seungwoo Kang , Chulhong Min , Younghyun Ju , Inseok Hwang , and Junehwa Song . 2015 . CoMon + : A cooperative context monitoring system for multi - device personal sensing environments . IEEE Transactions on Mobile Computing 15 , 8 ( 2015 ) , 1908 â€“ 1924 . [ 88 ] Youngki Lee , Chulhong Min , Chanyou Hwang , Jaeung Lee , Inseok Hwang , Younghyun Ju , Chungkuk Yoo , Miri Moon , Uichin Lee , and Junehwa Song . 2013 . Sociophone : Everyday face - to - face interaction monitoring platform using multi - phone sensor fusion . In Proceeding of the 11th annual international conference on Mobile systems , applications , and services . 375 â€“ 388 . [ 89 ] Hao Li , Asim Kadav , Igor Durdanovic , Hanan Samet , and Hans Peter Graf . 2016 . Pruning filters for efficient convnets . arXiv preprint arXiv : 1608 . 08710 ( 2016 ) . [ 90 ] Jiayang Liu , Lin Zhong , Jehan Wickramasuriya , and Venu Vasudevan . 2009 . uWave : Accelerometer - based personalized gesture recognition and its applications . Pervasive and Mobile Computing 5 , 6 ( 2009 ) , 657 â€“ 675 . [ 91 ] Jillian Madison . 2012 . Damn You , Autocorrect ! Random House . [ 92 ] Victor Mateevitsi , Khairi Reda , Jason Leigh , and Andrew Johnson . 2014 . The health bar : a persuasive ambient display to improve the office workerâ€™s well being . In Proceedings of the 5th augmented human international conference . 1 â€“ 2 . [ 93 ] Andrii Matviienko , Maria Rauschenberger , Vanessa Cobus , Janko Timmermann , Heiko MÃ¼ller , Jutta Fortmann , Andreas LÃ¶cken , Christoph Trappe , Wilko Heuten , and Susanne Boll . 2015 . Deriving design guidelines for ambient light systems . In Proceedings of the 14th international conference on mobile and ubiquitous multimedia . 267 â€“ 277 . [ 94 ] Marilyn McGee - Lennon , Maria Wolters , Ross McLachlan , Stephen Brewster , and Cordelia Hall . 2011 . Name that tune : musicons as reminders in the home . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 2803 â€“ 2806 . [ 95 ] Chulhong Min , Saumay Pushp , Seungchul Lee , Inseok Hwang , Youngki Lee , Seungwoo Kang , and Junehwa Song . 2014 . Uncovering embarrassing moments in in - situ exposure of incoming mobile messages . In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing : Adjunct Publication . 1045 â€“ 1054 . [ 96 ] Varun Mishra , Florian KÃ¼nzler , Jan - Niklas Kramer , Elgar Fleisch , Tobias Kowatsch , and David Kotz . 2021 . Detecting receptivity for mHealth interventions in the natural environment . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 2 ( 2021 ) , 1 â€“ 24 . [ 97 ] JÃ¶rg MÃ¼ller , Robert Walter , Gilles Bailly , Michael Nischt , and Florian Alt . 2012 . Looking glass : a field study on noticing interactivity of a shop window . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 297 â€“ 306 . [ 98 ] Rajalakshmi Nandakumar , Alex Takakuwa , Tadayoshi Kohno , and Shyamnath Gollakota . 2017 . Covertband : Activity information leakage using music . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 1 , 3 ( 2017 ) , 1 â€“ 24 . [ 99 ] Hong - Wei Ng , Viet Dung Nguyen , Vassilios Vonikakis , and Stefan Winkler . 2015 . Deep learning for emotion recognition on small datasets using transfer learning . In Proceedings of the 2015 ACM on international conference on multimodal interaction . 443 â€“ 449 . [ 100 ] Fatemeh Noroozi , Ciprian Adrian Corneanu , Dorota KamiÅ„ska , Tomasz SapiÅ„ski , Sergio Escalera , and Gholamreza Anbarjafari . 2018 . Survey on emotional body gesture recognition . IEEE transactions on affective computing 12 , 2 ( 2018 ) , 505 â€“ 523 . [ 101 ] Tin Lay Nwe , Say Wei Foo , and Liyanage C De Silva . 2003 . Speech emotion recognition using hidden Markov models . Speech communication 41 , 4 ( 2003 ) , 603 â€“ 623 . [ 102 ] Zeljko Obrenovic and Dusan Starcevic . 2004 . Modeling multimodal human - computer interaction . Computer 37 , 9 ( 2004 ) , 65 â€“ 72 . [ 103 ] Aaron van den Oord , Sander Dieleman , Heiga Zen , Karen Simonyan , Oriol Vinyals , Alex Graves , Nal Kalchbrenner , Andrew Senior , and Koray Kavukcuoglu . 2016 . Wavenet : A generative model for raw audio . arXiv preprint arXiv : 1609 . 03499 ( 2016 ) . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 30 â€¢ Cho et al . [ 104 ] Maxime Oquab , Leon Bottou , Ivan Laptev , and Josef Sivic . 2014 . Learning and transferring mid - level image representations using convolutional neural networks . In Proceedings of the IEEE conference on computer vision and pattern recognition . 1717 â€“ 1724 . [ 105 ] Yanwei Pang , Jin Xie , Muhammad Haris Khan , Rao Muhammad Anwer , Fahad Shahbaz Khan , and Ling Shao . 2019 . Mask - guided attention network for occluded pedestrian detection . In Proceedings of the IEEE / CVF International Conference on Computer Vision . 4967 â€“ 4975 . [ 106 ] Eleftherios Papachristos , Timothy Robert Merritt , Tobias Jacobsen , and Jimmi Bagger . 2020 . Designing Ambient Multisensory Notification Devices : Managing Disruptions in the Home . In 19th International Conference on Mobile and Ubiquitous Multimedia . 59 â€“ 70 . [ 107 ] Pablo E Paredes , Yijun Zhou , Nur Al - Huda Hamdan , Stephanie Balters , Elizabeth Murnane , Wendy Ju , and James A Landay . 2018 . Just breathe : In - car interventions for guided slow breathing . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 1 ( 2018 ) , 1 â€“ 23 . [ 108 ] Chunjong Park , Junsung Lim , Juho Kim , Sung - Ju Lee , and Dongman Lee . 2017 . Donâ€™t Bother Me . Iâ€™m Socializing ! A Breakpoint - Based Smartphone Notification System . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . 541 â€“ 554 . [ 109 ] Taiwoo Park , Inseok Hwang , Uichin Lee , Sunghoon Ivan Lee , Chungkuk Yoo , Youngki Lee , Hyukjae Jang , Sungwon Peter Choe , Souneil Park , and Junehwa Song . 2012 . ExerLink : enabling pervasive social exergames with heterogeneous exercise devices . In Proceedings of the 10th international conference on Mobile systems , applications , and services . 15 â€“ 28 . [ 110 ] Taiwoo Park , Jinwon Lee , Inseok Hwang , Chungkuk Yoo , Lama Nachman , and Junehwa Song . 2011 . E - gesture : a collaborative architecture for energy - efficient gesture recognition with hand - worn sensor and mobile devices . In Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems . 260 â€“ 273 . [ 111 ] Taiwoo Park , Uichin Lee , Scott MacKenzie , Miri Moon , Inseok Hwang , and Junehwa Song . 2014 . Human factors of speed - based exergame controllers . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1865 â€“ 1874 . [ 112 ] Callum Parker , Martin Tomitsch , and Judy Kay . 2018 . Does the public still look at public displays ? A field observation of public displays in the wild . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 2 ( 2018 ) , 1 â€“ 24 . [ 113 ] Veljko Pejovic and Mirco Musolesi . 2014 . InterruptMe : designing intelligent prompting mechanisms for pervasive applications . In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing . 897 â€“ 908 . [ 114 ] Dan Peng , Fan Wu , and Guihai Chen . 2015 . Pay as how well you do : A quality based incentive mechanism for crowdsensing . In Proceedings of the 16th ACM International Symposium on Mobile Ad Hoc Networking and Computing . 177 â€“ 186 . [ 115 ] Stefano Piana , Alessandra StaglianÃ² , Francesca Odone , and Antonio Camurri . 2016 . Adaptive body gesture representation for automatic emotion recognition . ACM Transactions on Interactive Intelligent Systems ( TiiS ) 6 , 1 ( 2016 ) , 1 â€“ 31 . [ 116 ] Martin Pielot , Bruno Cardoso , Kleomenis Katevas , Joan SerrÃ  , Aleksandar Matic , and Nuria Oliver . 2017 . Beyond interruptibility : Predicting opportune moments to engage mobile phone users . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 1 , 3 ( 2017 ) , 1 â€“ 25 . [ 117 ] Martin Porcheron , Joel E . Fischer , Stuart Reeves , and Sarah Sharples . 2018 . Voice Interfaces in Everyday Life . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI â€™18 ) . Association for Computing Machinery , New York , NY , USA , 1 â€“ 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3174214 [ 118 ] Swadhin Pradhan , Wei Sun , Ghufran Baig , and Lili Qiu . 2019 . Combating replay attacks against voice assistants . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 3 , 3 ( 2019 ) , 1 â€“ 26 . [ 119 ] Moo - Ryong Ra , Bin Liu , Tom F La Porta , and Ramesh Govindan . 2012 . Medusa : A programming framework for crowd - sensing applications . In Proceedings of the 10th international conference on Mobile systems , applications , and services . 337 â€“ 350 . [ 120 ] Meera Radhakrishnan , Darshana Rathnayake , Ong Koon Han , Inseok Hwang , and Archan Misra . 2020 . ERICA : enabling real - time mistake detection & corrective feedback for free - weights exercises . In Proceedings of the 18th Conference on Embedded Networked Sensor Systems . 558 â€“ 571 . [ 121 ] Xukan Ran , Haolianz Chen , Xiaodan Zhu , Zhenming Liu , and Jiasi Chen . 2018 . Deepdecision : A mobile deep learning framework for edge video analytics . In IEEE INFOCOM 2018 - IEEE Conference on Computer Communications . IEEE , 1421 â€“ 1429 . [ 122 ] Francesco Restuccia , Sajal K Das , and Jamie Payton . 2016 . Incentive mechanisms for participatory sensing : Survey and research challenges . ACM Transactions on Sensor Networks ( TOSN ) 12 , 2 ( 2016 ) , 1 â€“ 40 . [ 123 ] Ring . 2022 . Ring Indoor Cam . Retrieved May 15 , 2022 from https : / / ring . com / products / mini - indoor - security - camera [ 124 ] Tara Sainath and Carolina Parada . 2015 . Convolutional neural networks for small - footprint keyword spotting . ( 2015 ) . [ 125 ] Samsung . 2022 . Samsung The Frame TV . Retrieved May 15 , 2022 from https : / / www . samsung . com / us / tvs / the - frame / highlights / [ 126 ] Thomas SchlÃ¶mer , Benjamin Poppinga , Niels Henze , and Susanne Boll . 2008 . Gesture recognition with a Wii controller . In Proceedings of the 2nd international conference on Tangible and embedded interaction . 11 â€“ 14 . [ 127 ] Eike Schneiders , Anne Marie Kanstrup , Jesper Kjeldskov , and Mikael B Skov . 2021 . Domestic robots and the dream of automation : Understanding human interaction and intervention . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1 â€“ 13 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . AI - to - Human Actuation : Boosting Unmodified AIâ€™s Robustness by Proactively Inducing Favorable Human Sensing Conditions â€¢ 6 : 31 [ 128 ] Andreas Seiderer , Ilhan Aslan , Chi Tai Dang , and Elisabeth AndrÃ© . 2019 . Indoor air quality and wellbeing - enabling awareness and sensitivity with ambient IoT displays . In European Conference on Ambient Intelligence . Springer , 266 â€“ 282 . [ 129 ] Kiichi Shirai , Kyosuke Futami , and Kazuya Murao . 2021 . A Method to Manipulate Subjective Time by Using Tactile Stimuli of Wearable Device . In 2021 International Symposium on Wearable Computers . 63 â€“ 67 . [ 130 ] Ben Shneiderman . 2020 . Human - centered artificial intelligence : Reliable , safe & trustworthy . International Journal of Human â€“ Computer Interaction 36 , 6 ( 2020 ) , 495 â€“ 504 . [ 131 ] Pieter Simoens , Yu Xiao , Padmanabhan Pillai , Zhuo Chen , Kiryong Ha , and Mahadev Satyanarayanan . 2013 . Scalable crowd - sourcing of video from mobile devices . In Proceeding of the 11th annual international conference on Mobile systems , applications , and services . 139 â€“ 152 . [ 132 ] Daniel J Simons and Daniel T Levin . 1997 . Change blindness . Trends in cognitive sciences 1 , 7 ( 1997 ) , 261 â€“ 267 . [ 133 ] Daniel J Simons and Daniel T Levin . 1998 . Failure to detect changes to people during a real - world interaction . Psychonomic Bulletin & Review 5 , 4 ( 1998 ) , 644 â€“ 649 . [ 134 ] Sichao Song and Seiji Yamada . 2018 . Bioluminescence - inspired human - robot interaction : designing expressive lights that affect humanâ€™s willingness to interact with a robot . In 2018 13th ACM / IEEE International Conference on Human - Robot Interaction ( HRI ) . IEEE , 224 â€“ 232 . [ 135 ] Yi - FanSong , ZhangZhang , CaifengShan , andLiangWang . 2020 . Richlyactivatedgraphconvolutionalnetworkforrobustskeleton - based action recognition . IEEE Transactions on Circuits and Systems for Video Technology 31 , 5 ( 2020 ) , 1915 â€“ 1925 . [ 136 ] Katrin Starcke , Johanna Mayr , and Richard von Georgi . 2021 . Emotion Modulation through Music after Sadness Inductionâ€”The Iso Principle in a Controlled Experimental Study . International Journal of Environmental Research and Public Health 18 , 23 ( 2021 ) , 12486 . [ 137 ] Tim Claudius Stratmann , Andreas LÃ¶cken , Uwe Gruenefeld , Wilko Heuten , and Susanne Boll . 2018 . Exploring vibrotactile and peripheral cues for spatial attention guidance . In Proceedings of the 7th ACM International Symposium on Pervasive Displays . 1 â€“ 8 . [ 138 ] Jiayao Tan , Xiaoliang Wang , Cam - Tu Nguyen , and Yu Shi . 2018 . SilentKey : A new authentication framework through ultrasonic - based lip reading . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 1 ( 2018 ) , 1 â€“ 18 . [ 139 ] Michael S Vitevitch . 2003 . Change deafness : the inability to detect changes between two voices . Journal of Experimental Psychology : Human Perception and Performance 29 , 2 ( 2003 ) , 333 . [ 140 ] Alexandra Voit , Dominik Weber , Yomna Abdelrahman , Marie Salm , PaweÅ‚ W WoÅºniak , Katrin Wolf , Stefan Schneegass , and Niels Henze . 2020 . Exploring Non - Urgent Smart Home Notifications using a Smart Plant System . In 19th International Conference on Mobile and Ubiquitous Multimedia . 47 â€“ 58 . [ 141 ] Robert Walter , Gilles Bailly , and JÃ¶rg MÃ¼ller . 2013 . StrikeAPose : revealing mid - air gestures on public displays . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 841 â€“ 850 . [ 142 ] Miaosen Wang , Sebastian Boring , and Saul Greenberg . 2012 . Proxemic peddler : a public advertising display that captures and preserves the attention of a passerby . In Proceedings of the 2012 international symposium on pervasive displays . 1 â€“ 6 . [ 143 ] Nigel Warren , Matt Jones , Steve Jones , and David Bainbridge . 2005 . Navigation via continuously adapted music . In CHIâ€™05 extended abstracts on Human factors in computing systems . 1849 â€“ 1852 . [ 144 ] Jing Wei , Tilman Dingler , and Vassilis Kostakos . 2021 . Understanding User Perceptions of Proactive Smart Speakers . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 4 ( 2021 ) , 1 â€“ 28 . [ 145 ] Jing Wei , Benjamin Tag , Johanne R Trippas , Tilman Dingler , and Vassilis Kostakos . 2022 . What Could Possibly Go Wrong When Interacting with Proactive Smart Speakers ? A Case Study Using an ESM Application . In CHI Conference on Human Factors in Computing Systems . 1 â€“ 15 . [ 146 ] Sun Weiran . 2020 . Real - time age , gender and emotion prediction based on livestream webcam data . Retrieved May 12 , 2022 from https : / / github . com / ianforme / face - prediction [ 147 ] Justin D Weisz , Michael Muller , Stephanie Houde , John Richards , Steven I Ross , Fernando Martinez , Mayank Agarwal , and Kartik Talamadupula . 2021 . Perfection not required ? Human - AI partnerships in code translation . In 26th International Conference on Intelligent User Interfaces . 402 â€“ 412 . [ 148 ] Donghee Yvette Wohn , Hyejin Hannah Kum - Biocca , Astha Sharma , and Anisah Khandakar . 2020 . A Room With aâ€ Fakeâ€ View : Installing Digital Windows in Windowless Offices . In ACM International Conference on Interactive Media Experiences . 180 â€“ 184 . [ 149 ] Tong Wu , Nikolas Martelaro , Simon Stent , Jorge Ortiz , and Wendy Ju . 2021 . Learning When Agents Can Talk to Drivers Using the INAGT Dataset and Multisensor Fusion . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 5 , 3 ( 2021 ) , 1 â€“ 28 . [ 150 ] Yuxin Wu , Alexander Kirillov , Francisco Massa , Wan - Yen Lo , and Ross Girshick . 2019 . Detectron2 . https : / / github . com / facebookresearch / detectron2 . [ 151 ] Xangle3D . 2020 . 3D scan . Fast and Efficient . Retrieved May 13 , 2022 from https : / / xangle3d . com / [ 152 ] Haoyi Xiong , Yu Huang , Laura E Barnes , and Matthew S Gerber . 2016 . Sensus : a cross - platform , general - purpose system for mobile crowdsensing in human - subject studies . In Proceedings of the 2016 ACM international joint conference on pervasive and ubiquitous computing . 415 â€“ 426 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 . 6 : 32 â€¢ Cho et al . [ 153 ] Dejun Yang , Guoliang Xue , Xi Fang , and Jian Tang . 2015 . Incentive mechanisms for crowdsensing : Crowdsourcing with smartphones . IEEE / ACM transactions on networking 24 , 3 ( 2015 ) , 1732 â€“ 1744 . [ 154 ] Juheon Yi , Sunghyun Choi , and Youngki Lee . 2020 . EagleEye : Wearable camera - based person identification in crowded urban spaces . In Proceedings of the 26th Annual International Conference on Mobile Computing and Networking . 1 â€“ 14 . [ 155 ] Yafeng Yin , Lei Xie , Tao Gu , Yijia Lu , and Sanglu Lu . 2019 . AirContour : Building contour - based model for in - air writing gesture recognition . ACM Transactions on Sensor Networks ( TOSN ) 15 , 4 ( 2019 ) , 1 â€“ 25 . [ 156 ] Chungkuk Yoo , Inseok Hwang , Seungwoo Kang , Myung - Chul Kim , Seonghoon Kim , Daeyoung Won , Yu Gu , and Junehwa Song . 2017 . Card - stunt as a service : Empowering a massively packed crowd for instant collective expressiveness . In Proceedings of the 15th Annual International Conference on Mobile Systems , Applications , and Services . 121 â€“ 135 . [ 157 ] Chungkuk Yoo , Inseok Hwang , Eric Rozner , Yu Gu , and Robert F Dickerson . 2016 . Symmetrisense : Enabling near - surface interactivity on glossy surfaces using a single commodity smartphone . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 5126 â€“ 5137 . [ 158 ] Chungkuk Yoo , Seungwoo Kang , Inseok Hwang , Chulhong Min , Seonghoon Kim , Wonjung Kim , and Junehwa Song . 2019 . Mom , I see You Angry at Me ! Designing a Mobile Service for Parent - child Conflicts by In - situ Emotional Empathy . In Proceedings of the 5th ACM Workshop on Mobile Systems for Computational Social Science . 21 â€“ 26 . [ 159 ] Naoto Yoshida , Sho Hanasaki , and Tomoko Yonezawa . 2018 . Attracting attention and changing behavior toward wall advertisements with a walking virtual agent . In Proceedings of the 6th International Conference on Human - Agent Interaction . 61 â€“ 66 . [ 160 ] Jason Yosinski , Jeff Clune , Yoshua Bengio , and Hod Lipson . 2014 . How transferable are features in deep neural networks ? Advances in neural information processing systems 27 ( 2014 ) . [ 161 ] Aixi Zhang , Yue Liao , Si Liu , Miao Lu , Yongliang Wang , Chen Gao , and Xiaobo Li . 2021 . Mining the Benefits of Two - stage and One - stage HOI Detection . Advances in Neural Information Processing Systems 34 ( 2021 ) . [ 162 ] Kaipeng Zhang , Zhanpeng Zhang , Zhifeng Li , and Yu Qiao . 2016 . Joint face detection and alignment using multitask cascaded convolutional networks . IEEE signal processing letters 23 , 10 ( 2016 ) , 1499 â€“ 1503 . [ 163 ] Shanshan Zhang , Jian Yang , and Bernt Schiele . 2018 . Occluded pedestrian detection through guided attention in cnns . In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition . 6995 â€“ 7003 . [ 164 ] Shibo Zhang , Yuqi Zhao , Dzung Tri Nguyen , Runsheng Xu , Sougata Sen , Josiah Hester , and Nabil Alshurafa . 2020 . Necksense : A multi - sensor necklace for detecting eating activities in free - living conditions . Proceedings of the ACM on interactive , mobile , wearable and ubiquitous technologies 4 , 2 ( 2020 ) , 1 â€“ 26 . [ 165 ] Tianshu Zhang , Buzhen Huang , and Yangang Wang . 2020 . Object - occluded human shape and pose estimation from a single color image . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition . 7376 â€“ 7385 . [ 166 ] Zhifei Zhang , Yang Song , and Hairong Qi . 2017 . Age progression / regression by conditional adversarial autoencoder . In Proceedings of the IEEE conference on computer vision and pattern recognition . 5810 â€“ 5818 . [ 167 ] Jun - Yan Zhu , Taesung Park , Phillip Isola , and Alexei A Efros . 2017 . Unpaired image - to - image translation using cycle - consistent adversarial networks . In Proceedings of the IEEE international conference on computer vision . 2223 â€“ 2232 . [ 168 ] Dmitry N Zotkin , Ramani Duraiswami , and Larry S Davis . 2004 . Rendering localized spatial audio in a virtual auditory space . IEEE Transactions on multimedia 6 , 4 ( 2004 ) , 553 â€“ 564 . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . , Vol . 7 , No . 1 , Article 6 . Publication date : March 2023 .