Spatial Scan Statistics for Graph Clustering Bei Wang ∗ Jeﬀ M . Phillips † Robert Schreiber ‡ Dennis Wilkinson § Nina Mishra ¶ Robert Tarjan k Abstract In this paper , we present a measure associated with detection and inference of statistically anomalous clusters of a graph based on the likelihood test of observed and expected edges in a subgraph . This measure is adapted from spatial scan statistics for point sets and provides quantitative assessment for clusters . We discuss some important properties of this statistic and its relation to modularity and Bregman divergences . We apply a simple clustering algorithm to ﬁnd clusters with large values of this measure in a variety of real - world data sets , and we illustrate its ability to identify statistically signiﬁcant clusters of selected granularity . 1 Introduction . Numerous techniques have been proposed for identifying clusters in large networks , but it has proven diﬃcult to meaningfully and quantitatively assess them , especially from real - world data whose clustering structure is a pri - ori unknown . One of the key challenges encountered by previous clustering methods is rating or evaluating the results . In large networks , manual evaluation of the results is not feasible , and previous studies have thus turned to artiﬁcially created graphs with known struc - ture as a test set . However , many methods , especially those in which the number of clusters must be speci - ﬁed as an algorithm parameter , give very poor results when applied to real - world graphs , which often have a highly skewed degree distribution and overlapping , com - plex clustering structure [ 18 , 33 ] . 1 . 1 Prior Work on Measures . Modularity . The problem of assessment was par - tially solved by the introduction of modularity [ 12 ] , a global objective function used to evaluate clusters that rewards existing internal edges and penalizes missing internal edges . Non - overlapping clusters , or partitions of a graph , are obtained by maximizing the distance from what a random graph would predict , either by extremal optimization [ 15 ] , fast greedy hierarchical al - ∗ Duke University : beiwang @ cs . duke . edu † Duke University : jeffp @ cs . duke . edu ‡ HP Labs : rob . schreiber @ hp . com § HP Labs : dennis . wilkinson @ hp . com ¶ Visiting Search Labs , Microsoft Research ; University of Vir - ginia : nmishra @ cs . virginia . edu k Princeton University ; HP Labs : robert . tarjan @ hp . com gorithms [ 31 , 37 ] , simulated annealing [ 38 ] or spectral clustering [ 32 ] . However , modularity cannot directly assess how unexpected and thus signiﬁcant individual clusters are . Additionally it cannot distinguish between clusterings of diﬀerent granularity on the same network . For example , comparable overall modularities were reported for hard clusterings of the same scientiﬁc citation graph into 44 , 324 , and 647 clusters [ 37 ] , results which are clearly of varying usefulness depending on the application . Spatial Scan Statistics . Scan statistics [ 19 ] mea - sure densities of data points for a sliding window on or - dered data . The densest regions under a ﬁxed size win - dow are considered the most anomalous . This notion of a sliding window has been generalized to neighbor - hoods on directed graphs [ 36 ] where the neighborhood of a vertex is restricted to vertices within some constant number of edges in the graph . The number of neighbors is then compared to an expected number of neighbors based on previous data in a time - marked series . Spatial scan statistics were introduced by Kull - dorﬀ [ 23 ] to ﬁnd anomalous clusters of points in 2 or greater dimensions without ﬁxing a window size . These statistics measure the surprise of observing a particu - lar region by computing the log - likelihood of the prob - ability of the most likely model for a cluster versus the probability of the most likely model for no cluster . Kull - dorﬀ argues that the region with the largest spatial scan statistic is the most likely to be generated by a diﬀer - ent distribution , and thus is most anomalous . This test was shown to be the most powerful test [ 27 ] for ﬁnding a region which demonstrates that the data set is not generated from a single distribution . Kulldorﬀ [ 23 ] de - rived expressions for the spatial scan statistic under a Poisson and Bernoulli model . Agarwal et . al . [ 2 ] general - ized this derivation to 1 - parameter exponential families , and Kulldorﬀ has studied various ( see [ 26 ] ) other forms of this statistic . Many techniques exist for computing the statistic quickly for anomaly detection for point sets [ 30 , 25 , 1 ] . 1 . 2 Previous Algorithmic Work . Clustering is well - established as an important method of information 727 extraction from large data sets . Hard clustering divides data into disjoint clusters while soft clustering allows data elements to belong to more than one cluster . Exist - ing techniques include MCL [ 41 ] , Ncut [ 40 ] , graclus [ 13 ] , MCODE [ 5 ] , iterative scan [ 9 ] , k - clique - community [ 37 ] , spectral clustering [ 35 , 32 ] , simulated annealing [ 28 ] , or partitioning using network ﬂow [ 34 ] , edge centrality [ 18 ] , and many other techniques e . g . [ 42 ] . Several statistically motivated graph clustering techniques exist [ 20 , 39 , 22 ] . Itzkovitz et . al . discussed distributions of subgraphs in random networks with ar - bitrary degree sequence , which have implications for de - tecting network motifs [ 20 ] . Sharan et . al . introduced a probabilistic model for protein complexes taking con - servation into consideration [ 39 ] . Koyuturk et . al . iden - tiﬁed clusters by employing a min - cut algorithm where a subgraph was considered to be statistically signiﬁcant if its size exceeded a probabilistic estimation based on a piecewise degree distribution model [ 22 ] . These tech - niques are all diﬀerent from our approach as our model based on spatial scan statistics has properties essential for detecting statistically signiﬁcant clusters . A general clustering framework using Bregman di - vergences as optimization functions has been proposed by Banerjee et . al . [ 14 , 6 , 7 ] . This approach is of note because the optimization function we use can be inter - preted as a Bregman divergence , although our theoreti - cal and algorithmic approaches are completely diﬀerent ( as explained in Section 3 ) . 1 . 3 Our Contribution . Our main contribution is the generalization of spatial scan statistics from point sets to graphs . Our statistic , Poisson discrepancy , oﬀers a metric that determines how signiﬁcant the clusters are using a normalized measure of likelihood . By comparison to random graphs with the same expected degree sequence as the network under consideration , a p - value based on the Poisson discrepancy is associated to each cluster found , providing a problem independent measure of its statistical signiﬁcance . The statistic is correctly normalized to provide an absolute measure of cluster signiﬁcance on an individual level , allowing comparison between clusters from diﬀerent networks or from diﬀerent clusterings of the same network . We also implement two simple greedy algorithms which seek to maximize the Poisson discrepancy . Our method includes a tunable parameter which we show is strongly correlated to the average size of the found clusters . This parameter allows the user to vary the expected size of the clusters found by the algorithms , which may be useful for various applications . In addi - tion , because clusters of diﬀerent size may be meaning - fully compared with each other , our method provides a way to identify important levels of granularity within a network . In fact , our results using real - world data show that statistically signiﬁcant clusters of varying sizes can exist in a given neighborhood of a network , implying that real - world networks display granularity on a num - ber of diﬀerent levels . Finally , our method allows data points to ﬁt in any number of clusters , including zero . This makes it applicable to real - world networks in which some vertices may not have any kind of strong associa - tion . In Sections 2 and 3 of this paper , we present the the - oretical foundation of our clustering method . The algo - rithm and variations are described in Section 4 . Bipar - tite extensions of our algorithm are explained in Section 5 . In Section 6 , we empirically evaluate our objective function and algorithms on real and synthetic datasets . We calculate the algorithms’ runtime and power ; we evaluate other clustering algorithms using Poisson dis - crepancy ; and we demonstrate that the clusters found using Poisson discrepancy with our algorithm are mean - ingful , are scaled reliably with a tuning parameter γ , and can overlap with each other . 2 Deﬁnitions . Graphs . Let G = ( V , E ) be an undirected graph with vertex set V and edge set E , such that E is a multiset of elements in [ V ] 2 , where [ V ] 2 denotes a set of 2 - element multisets of V . That is [ V ] 2 = { { u , v } | u , v ∈ V } , where u and v are not necessarily distinct . G allow self - loops and multiple edges between a pair of vertices . A cluster Z = ( V Z , E Z ) is a subgraph of G induced by subset V Z ⊆ V . The collection of clusters Z ⊆ G is denoted as Z . The subgraph in G not in Z is ¯ Z = ( V , E \ E Z ) . Let c ( Z ) be the number of observed edges in cluster Z ⊆ G . c ( G ) is the total number of observed edges in G . c ∗ ( x ) is the observed number of edges between a pair of vertices x = { v i , v j } ∈ [ V ] 2 . The degree k i of a vertex v i is the number of edges that contain v i . A self - loop at v i is counted twice in the degree . The neighborhood of a vertex set U ⊆ V is the set of vertices { v | { v , u } ∈ E for u ∈ U and v ∈ V \ U } , denoted N ( U ) . The size of a set V is denoted | V | . We deﬁne the distance between two vertex sets U and V as d ( U , V ) = | U | + | V | − 2 | U ∩ V | . Poisson Random Graph Model . Many large real - world graphs have diverse , non - uniform , degree distributions [ 8 , 4 , 3 ] that are not accurately described by the classic Erd¨os and R´enyi random graph models [ 16 ] . We consider a Poisson random graph model here that captures some main characteristics of real - world graphs , speciﬁcally , allowing vertices to have diﬀerent expected degrees . Notice that this model is diﬀerent 728 from models used in [ 10 , 12 ] . The model generates random graphs with a given expected degree sequence k = h k 1 , k 2 , . . . , k | V | i for the vertex set V = h v 1 , v 2 , . . . , v | V | i , where vertex v i has expected degree k i . Let k V = P | V | i = 1 k i and m = k V / 2 . The model chooses a total of m pairs of vertices as edges through m steps , with replacement . At each step , each of the two end - points of an edge is chosen among all vertices through a Poisson proces , proportional to their degrees . The probability that each end point of a chosen edge contains vertex v i is k i / k V . The expected degree of vertex v i after this process is m ( k i / k V + k i / k V ) = m ( k i / 2 m + k i / 2 m ) = k i . We refer to this model as the Poisson random graph model . It ﬁxes the total number of edges and allows multiple edges between any pair of vertices . A bipartite multigraph example captured by a Poisson random graph model is an internet bulletin board . There is a set of users who post under diﬀerent threads . A post in a thread would constitute an edge . Of course a single user can post multiple times to a single thread , and some users post more often and some threads generate more posts . 3 Spatial Scan Statistics for Graphs . In this section we generalize the notion of a spatial scan statistic [ 23 ] to graphs . We also highlight some important properties of this statistic , as well as its relation to local modularity and Bregman divergences . It is this spatial scan statistic for graphs that we use to provide quantitative assessment of signiﬁcant clusters . 3 . 1 Spatial Scan Statistics Poisson Model . Given G = ( V , E ) , its edge set describes a degree se - quence k = h k 1 , k 2 , . . . , k | V | i for the vertex set V = h v 1 , v 2 , . . . , v | V | i , where k i is the degree of vertex v i . Let k V = P | V | i = 1 k i and c ( G ) = k V / 2 . According to the Poisson random graph model with k as the given expected degree sequence and c ( G ) as the total number of expected edges , deﬁne µ ∗ ( x ) as the expected number of edges connecting the pair x = { v i , v j } ∈ [ V ] 2 . For i 6 = j , µ ∗ ( x ) = k i k j / 2 c ( G ) . For i = j , µ ∗ ( x ) = k 2 i / 4 c ( G ) . For a subgraph A = ( V A , E A ) ⊆ G , deﬁne µ ( A ) as the expected number of edges in A . Let K V A = P v i ∈ V A k i , µ ( A ) = P x ∈ [ V A ] 2 µ ∗ ( x ) = k 2 V A / 4 c ( G ) . Notice that µ ( G ) = P x ∈ [ V ] 2 µ ∗ ( x ) = c ( G ) . A simple example is shown in Figure 1 where cluster Z is induced by dark vertices in the graph , c ( G ) = µ ( G ) = 10 , c ( Z ) = 6 and µ ( Z ) = 13 2 / ( 4 × 10 ) = 169 / 40 . In the spatial scan statistics Poisson model , we assume edges in G are generated by a Poisson process N with intensity λ . The probability that there are exactly Figure 1 : Example graph . n occurrences in the process is denoted as Poi ( n , λ ) = e − λ λ n / n ! . N assumes the same vertex set and expected degree sequence as G and generates edges according to the Poisson random graph model . Let N ( A ) be the random variable describing the number of edges in the subgraph A ⊆ G under such a process . N ( A ) follows a Poisson distribution , denoted by N ( A ) ∼ Poi ( n , λ ) . Under such a model , consider a single cluster Z = ( V Z , E Z ) ⊂ G , such that edges in Z are generated with rate p , while edges in ¯ Z are generated with rate q . The null hypothesis H 0 : p = q assumes complete spatial randomness [ 21 ] , that is , edges inside and outside the cluster are generated under the same rate . The alternative hypothesis H 1 : p > q assumes that edges inside the cluster are generated at a higher rate than those outside the cluster . µ therefore represents a known underlying intensity that generates edges under H 0 [ 24 ] . Therefore , under H 0 , the number of edges in any given cluster A ⊆ G is Poisson distributed , N ( A ) ∼ Poi ( n , pµ ( A ) ) for some value p . Under H 1 , ∀ A ⊆ G , N ( A ) ∼ Poi ( n , pµ ( A ∩ Z ) + qµ ( A ∩ ¯ Z ) ) [ 23 ] . Likelihood Ratio Test . The spatial scan statis - tics for graphs are based on the likelihood ratio test derived from [ 23 ] . The spatial scan statistics Poisson Model assumes under the null hypothesis H 0 that all edges in G are generated at the same rate . That is , under H 0 , the probability of c ( G ) edges being observed in G is Poi ( c ( G ) , pµ ( G ) ) . The density function f ( x ) of a speciﬁc pair of vertices x ∈ [ V ] 2 being an actual edge is f ( x ) = pµ ∗ ( x ) pµ ( G ) = µ ∗ ( x ) µ ( G ) . Incorporating the density of each edge , the likeli - hood function under H 0 is L 0 = max p = q Poi ( c ( G ) , pµ ( G ) ) Y x ∈ E f ( x ) = max p = q e − pµ ( G ) ( pµ ( G ) ) c ( G ) c ( G ) ! Y x ∈ E µ ∗ ( x ) µ ( G ) = max p = q e − pµ ( G ) p c ( G ) c ( G ) ! Y x ∈ E µ ∗ ( x ) For a ﬁxed Z , L 0 takes its maximum when p = q = c ( G ) µ ( G ) . 729 That is L 0 = e − c ( G ) c ( G ) ! µ c ( G ) µ ( G ) ¶ c ( G ) Y x ∈ E µ ∗ ( x ) . The alternative hypothesis H 1 assumes that for a particular cluster Z = ( V Z , E Z ) ⊂ Z , the edges in Z are generated at a diﬀerent rate than those in ¯ Z . Under H 1 , the probability of c ( G ) edges being observed in G is Poi ( c ( G ) , pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) ) . For x ∈ E Z , the density function f ( x ) is f ( x ) = pµ ∗ ( x ) pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) . For x ∈ E \ E Z , f ( x ) = qµ ∗ ( x ) pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) . The likelihood function under H 1 is L ( Z ) = max p > q Poi ( c ( G ) , pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) ) · Y x ∈ E Z f ( x ) Y x ∈ E \ E Z f ( x ) = max p > q e − pµ ( Z ) − q ( µ ( G ) − µ ( Z ) ) c ( G ) ! · ( pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) ) c ( G ) · Y x ∈ E Z pµ ∗ ( x ) pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) · Y x ∈ E \ E Z qµ ∗ ( x ) pµ ( Z ) + q ( µ ( G ) − µ ( Z ) ) = max p > q e − pµ ( Z ) − q ( µ ( G ) − µ ( Z ) ) c ( G ) ! p c ( Z ) q c ( G ) − c ( Z ) · Y x ∈ E µ ∗ ( x ) For a ﬁxed Z , L ( Z ) takes its maximum when p = c ( Z ) µ ( Z ) , q = c ( G ) − c ( Z ) µ ( G ) − µ ( Z ) . If c ( Z ) µ ( Z ) > c ( G ) − c ( Z ) µ ( G ) − µ ( Z ) , L ( Z ) = e − c ( G ) c ( G ) ! µ c ( Z ) µ ( Z ) ¶ c ( Z ) µ c ( G ) − c ( Z ) µ ( G ) − µ ( Z ) ¶ c ( G ) − c ( Z ) · Y x ∈ E µ ∗ ( x ) otherwise L ( Z ) = L 0 . Given a cluster Z ∈ Z , we deﬁne the likelihood ratio , LR , as the ratio of L ( Z ) and L 0 . For the Poisson model c ( G ) = µ ( G ) , and if c ( Z ) µ ( Z ) > c ( G ) − c ( Z ) µ ( G ) − µ ( Z ) , we have LR ( Z ) = L ( Z ) L 0 = µ c ( Z ) µ ( Z ) ¶ c ( Z ) µ c ( G ) − c ( Z ) µ ( G ) − µ ( Z ) ¶ c ( G ) − c ( Z ) . Otherwise LR = 1 . We then deﬁne the likelihood ratio test statistic , Λ , as the maximum likelihood ratio over all clusters Z ∈ Z , Λ = max Z ∈Z LR ( Z ) . Poisson Discrepancy . Let r ( Z ) = c ( Z ) c ( G ) and b ( Z ) = µ ( Z ) µ ( G ) , we deﬁne the Poisson discrepancy , d P , as d P ( Z ) = r ( Z ) log r ( Z ) b ( Z ) + ( 1 − r ( Z ) ) log 1 − r ( Z ) 1 − b ( Z ) . Intuitively , r ( Z ) is the observed edge ratio and b ( Z ) is the baseline edge ratio in Z and G . Since log Λ = c ( G ) max Z ∈Z d P ( Z ) , for the cluster Z that maximizes d P , d P ( Z ) constitutes the test statistic Λ . This means that the likelihood test based on max Z ∈Z d P ( Z ) is identical to one based on Λ . Since 0 < r ( Z ) , b ( Z ) ≤ 1 , from this point on , we evaluate clusters based on the Poisson discrepancy . d P determines how surprising r ( Z ) is compared to the rest of the distribution . Thus clusters with larger values of d P are more likely to be inherently diﬀerent from the rest of the data . Point Set Model . It should be noted that these deﬁnitions were originally formulated where the set of all possible edges , [ V ] 2 , was instead a point set , the function µ measured a baseline estimate of the popula - tion , and the function c measured reported data , such as instances of a disease . In this setting , the set of pos - sible clusters is usually restricted to those contained in some geometrically described family of ranges . This set can often be searched exhaustively in time polynomial in the size of the point set . In our setting we have 2 | V | possible ranges ( vertex sets which induce Z ) — an in - tractably large number . Hence , Section 4 explains how to explore these ranges eﬀectively . Signiﬁcance of a Cluster . Although we can con - sider many ( or all ) subsets and determine the one that is most anomalous by calculating the Poisson discrep - ancy , this does not determine whether this value is sig - niﬁcant . Even a graph generated according to a Pois - son random graph model will have some cluster which is most anomalous . For a graph G = ( V , E ) with de - gree sequence k and for a particular cluster Z ⊆ G we can compare d P ( Z ) to the distribution of the values of the most anomalous clusters found in a large set of ( say 1000 ) random data graphs . To create a random data graph , we ﬁx V ; then we randomly select c ( G ) edges according to a Poisson random graph model with ex - pected degree sequence k . If the Poisson discrepancy for the original cluster is greater than all but a . 05 frac - tion of the most anomalous clusters from the random 730 data sets , then we say it has a p - value of . 05 . The lower the p - value , the more signiﬁcantly anomalous the range is . These high discrepancy clusters are most signiﬁcant because they are the most unlikely compared to what is expected from our random graph model . 3 . 2 Properties of Spatial Scan Statistics . Kull - dorﬀ has proved some optimal properties for the likeli - hood ratio test statistic for point sets [ 23 ] . In the con - text of graphs , we describe those properties essential for detecting statistically anomalous clusters in terms of d P . For details and proofs , see [ 23 , 27 ] . As a direct consequence of Theorem 1 in [ 23 ] , we have Theorem 3 . 1 . Let X = { x i | x i ∈ E } c ( G ) i = 1 be the set of edges in G = ( V , E ) where Z = ( V Z , E Z ) is the most likely cluster . Let X ′ = { x ′ i | x ′ i ∈ [ V ] 2 } c ( G ) i = 1 be an alternative conﬁguration of a graph G ′ = ( V , E ′ ) where ∀ x i ∈ E Z , x ′ i = x i . If the null hypothesis is rejected under X , then it is also rejected under X ′ . Intuitively , as long as the edges within the subgraph constituting the most likely cluster are ﬁxed , the null hypothesis is rejected no matter how the rest of the edges are shuﬄed around [ 23 ] . This theorem implies that : 1 . d P ( Z ) does not change as long as its internal struc - ture and the total number of reported edges outside Z remains the same . Intuitively , clusters deﬁned by other subgraphs do not aﬀect the discrepancy on Z . Formally , d P ( Z ) is independent of the value of c ∗ ( x ) for any potential edge x ∈ E \ E Z , as long as c ( ¯ Z ) remains unchanged . 2 . If the null hypothesis is rejected by d P , then we can identify a speciﬁc cluster that is signiﬁcant and implies this rejection . This distinguishes between saying “there exist signiﬁcant clusters” and “the cluster Z is a signiﬁcant cluster , ” where d P can do the latter . Theorem 3 . 2 . d P is individually most powerful for ﬁnding a single signiﬁcant cluster : for a ﬁxed false positive rate and for a given set of subgraphs tested , it is more likely to detect over - density than any other test statistic [ 30 ] . This is a direct consequence of Theorem 2 in [ 23 ] and is paramount for eﬀective cluster detection . ( Its power is also explored in Section 6 ) . It implies that : 3 . We can determine the single potential edge x ∈ [ V ] 2 ( or set of edges , such as those described by adding a vertex to the cluster ) that will most increase the Poisson discrepancy of a cluster , and thus most decrease its p - value . 3 . 3 Spatial Scan Statistics and Local Modular - ity . Several local versions of modularity have been used to discover local community structure [ 11 , 29 ] . Speciﬁ - cally , local modularity introduced in [ 38 ] is used to ﬁnd the community structure around a given node . The local modularity of Z ⊆ G measures the diﬀerence between the number of observed edges c ( Z ) and the number ex - pected , µ ( Z ) , M γ ( Z ) = c ( Z ) − γµ ( Z ) . One approach to clustering is to ﬁnd the cluster Z that locally maximizes M γ . The γ parameter with default value 1 , is a user speciﬁed knob [ 38 ] that scales the expected number of edges within Z under a Poisson random graph model . We observe that it eﬀectively tunes the size of the clusters which optimize M γ ( Z ) . For a ﬁxed cluster Z , M γ can be treated as a linear function of γ , where its intersection with the Y - axis is c ( Z ) , and its slope is − µ ( Z ) . M γ for all Z ∈ Z forms a family of linear functions whose upper envelope corresponds to clusters that maximize M as γ varies . It can be observed that as γ increases , c ( Z ) is non - increasing and µ ( Z ) is non - decreasing for the cluster Z that maximizes M γ . It is important to distinguish M from d P . While M measures the edge distance from the expected random graph , d P measures the diﬀerence in how likely the total number of edges are to occur in a general random graph and how likely they are to occur in cluster Z and its complement as separate random graph models . To summarize , M calculates the distance , and spatial scan statistics measure how unexpected this distance is , given Z . Several properties are also shared between d P and M . The tuning knob γ can be used in Poisson discrep - ancy to scale the expected number of edges in a cluster Z . d P , γ ( Z ) = r ( Z ) log r ( Z ) γb ( Z ) + ( 1 − r ( Z ) ) log µ 1 − r ( Z ) 1 − b ( Z ) ¶ Technically , the function d P , γ describes the eﬀect of scaling by γ the expected number of edges in a cluster Z ( but not outside the cluster ) , while not allowing q , the parameter to model the random graph outside this cluster , to reﬂect γ . Thus in the same way as with M γ for large γ , clusters need to have signiﬁcantly more edges than expected to have a positive d P value . The following lemma highlights this relationship . Lemma 3 . 1 . Consider two clusters Z 1 and Z 2 such that d P , γ ( Z 1 ) = d P , γ ( Z 2 ) and that c ( Z 1 ) > c ( Z 2 ) . Then for any δ > 0 we know d P , γ + δ ( Z 1 ) < d P , γ + δ ( Z 2 ) . The same property holds for M γ and µ in place of d P , γ and c , respectively . That is , consider two 731 clusters Z 1 and Z 2 such that M γ ( Z 1 ) = M γ ( Z 2 ) and that µ ( Z 1 ) > µ ( Z 2 ) . Then for any δ > 0 we know M γ + δ ( Z 1 ) < M γ + δ ( Z 2 ) . Proof . We can write d P , γ ( Z ) = d P ( Z ) − r ( Z ) log γ , thus as γ increases d P , γ ( Z 1 ) will decrease faster than d P , γ ( Z 2 ) . We can also write M γ ( Z ) = c ( G ) ( r ( Z ) − γb ( Z ) ) = M 1 ( Z ) − c ( G ) ( γ − 1 ) b ( Z ) . Thus the same argument applies . This implies that for the discrepancy measure , we should expect the size of the optimal clusters to be smaller as we increase γ , as is empirically demonstrated in Section 6 . 3 . Using some machinery developed by Agarwal et . al . [ 2 ] we can create an ε - approximation of d P with O ( 1 ε log 2 | V | ) linear functions with parameters r ( Z ) and b ( Z ) , in the sense that the upper envelope of this set of linear functions will be within ε of d P . We can inter - pret M γ as a linear function whose slope is controlled by the value of γ . Figure 2 shows how M 1 and M 2 , respectively , approximate d P . Thus we can ﬁnd the op - timal cluster for O ( 1 ε log 2 | V | ) values of γ and let Z be the corresponding cluster from this set which has the largest value of d P ( Z ) . Let Z ∗ be the cluster that has the largest value of d P ( Z ∗ ) among all possible clusters . Then d P ( Z ) + ε ≥ d P ( Z ∗ ) . However , a further study of Agarwal et . al . [ 1 ] showed that a single linear function ( which would be equivalent to γ = 2 for M γ ) approximated d P on average to within about 95 % for a problem using point sets . Note in Figure 2 how M 2 seems to approximate d P better than M 1 , at least for a large portion of the domain containing smaller clusters . 3 . 4 Spatial Scan Statistics and Bregman Diver - gences . Many Bregman divergences can be interpreted as spatial scan statistics . The KL - divergence , a Breg - man divergence , when between two 2 - point distribu - tions , is equivalent to d P up to a constant factor . Banerjee et . al . [ 7 ] use Bregman divergences in a diﬀerent way than does this paper . In the context of graph clustering , Bregman hard clustering ﬁnds a bi - partitioning and a representative for each of the partitions such that the expected Bregman divergence of the data points ( edges ) from their representatives is minimized [ 7 ] . For details and derivations , see [ 7 ] . Given a graph G = ( V , E ) , let x i be a potential edge x i ∈ [ V ] 2 . Set X = { x i } c ( G ) 2 i = 1 ⊆ R has probability bS rS 0 1 1 . 0 0 . 5 1 . 5 bS rS 0 1 1 . 0 0 . 5 1 . 5 Figure 2 : Comparison of d P ( gridded ) to 1 m M 1 ( trans - parent , upper panel ) and 1 m M 2 ( transparent , lower panel ) over ( r ( Z ) , b ( Z ) ) ∈ [ 0 , 1 ] 2 such that r ( Z ) > b ( Z ) . Recall that r ( Z ) and b ( Z ) are the actual and expected fraction of a graph’s edges which lie in a particular clus - ter ; for applications to large networks , a range of say ( 0 , 0 . 2 ) 2 is most pertinent to clustering . For this range , M 2 is shown to approximate d P more closely than M 1 . measure µ ∗ . Cluster Z induces a bi - partition of G , Z and ¯ Z . Let µ ( Z ) and µ ( ¯ Z ) be the induced measures on the partitions , where µ ( Z ) = P x i ∈ [ V Z ] 2 µ ∗ ( x i ) and µ ( ¯ Z ) = P x i ∈X \ [ V Z ] 2 µ ∗ ( x i ) . Let η Z and η ¯ Z denote the partition representative values . η Z = X x i ∈ [ V Z ] 2 µ ∗ ( x i ) µ ( Z ) c ∗ ( x i ) η ¯ Z = X x i ∈X \ [ V Z ] 2 µ ∗ ( x i ) µ ( ¯ Z ) c ∗ ( x i ) Deﬁne η i = η Z for x i ∈ [ V Z ] 2 , otherwise η i = η ¯ Z . The Bregman clustering seeks to minimize the divergence between the two c ( G ) 2 - point distributions { h c ∗ ( x 1 ) , c ∗ ( x 2 ) , . . . , c ∗ ( x c ( G ) 2 ) i , h η 1 , η 2 , . . . η c ( G ) 2 i } 732 We , on the other hand , maximize the KL - divergence between the two 2 - point distributions { h r ( Z ) , 1 − r ( Z ) i , h b ( Z ) , 1 − b ( Z ) i } . But the methods do not conﬂict with each other . Their η Z and η ¯ Z variables are akin to p and q in the derivation of the scan statistic . By minimizing their Bregman divergence , they are trying to allow η Z and η ¯ Z to be as close to variables they represent as possible ( i . e . η Z should be close to each c ∗ ( x i ) for x i deﬁned on Z ) ; and by maximizing our discrepancy we are separating p and q as much as possible , thus probabilistically representing the cluster edges and non - cluster edges more accurately with these ratios , respectively . However , the Bregman divergence used by Banerjee et . al . [ 6 , 7 ] typically assumes a less informative , uniform random graph model where µ ∗ ( x i ) = µ ∗ ( x j ) for all i and j . Also when minimizing the KL - divergence , no edge at x i would imply c ∗ ( x i ) = 0 , thus implying that the cor - responding term of the KL - divergence , c ∗ ( x i ) log c ∗ ( x i ) η i , is undeﬁned . In their Bregman divergence model most similar to ours , this poses a problem as c ∗ ( x i ) can be 0 in our model ; thus we do not compare the performance of these algorithms . 4 Algorithms . In this section , we describe two bottom - up , greedy clustering algorithms . For a graph G = ( V , E ) there are 2 | V | possible clusters ( subgraphs ) induced by subsets of V . That is , | Z | ≤ 2 | V | , where each cluster Z = ( V Z , E Z ) ∈ Z is induced by a vertex set V Z ⊆ V . Clearly it is intractable to calculate discrepancy for every possible cluster through exhaustive search , as is often done with spatial scan statistics . We can , however , hope to ﬁnd a locally optimal cluster . For an objective function Ψ : 2 | V | → R , deﬁne a local maximum as a subset U ⊆ V such that adding or removing any vertex will decrease Ψ ( U ) . For some objective function Ψ and two vertex sets U and W , deﬁne ∂ Ψ ( U , W ) = ½ Ψ ( U ∪ W ) − Ψ ( U ) W ⊂ V \ U Ψ ( U \ W ) − Ψ ( U ) W ⊂ U where Ψ ( V Z ) = d P ( Z ) for Z = ( V Z , E Z ) ⊆ Z induced by V Z . Let U + ( resp . U − ) be the set of vertices in N ( U ) ( resp . U ) such that ∂ Ψ ( U , v ) > 0 for each vertex v . U + F ( resp . U − F ) denotes the subset of U + ( resp . U − ) that contains the fraction F of vertices with the largest ∂ Ψ ( U , v ) values . We now are set to describe two algorithms for reﬁning a given subset U to ﬁnd a local maximum in Ψ . Notice that both algorithms can be used to locally optimize any objective function , not limited to the Poisson discrepancy used here . Greedy Nibble . The Greedy Nibble algorithm ( Algorithm 1 ) alternates between an expansion phase and a contraction phase until the objective function can - not be improved . During expansion ( resp . contraction ) we iteratively add ( resp . remove ) the vertex that most improves the objective function until this phase can no longer improve the objective function . Algorithm 1 Greedy - Nibble ( U ) repeat expand = false ; contract = false v + = arg max v ∈ N ( U ) ∂ Ψ ( U , v ) . while ∂ Ψ ( U , v + ) > 0 do expand = true U = U ∪ v + . v + = arg max v ∈ N ( U ) ∂ Ψ ( U , v ) . v − = arg max v ∈ U ∂ Ψ ( U , v ) . while ∂ Ψ ( U , v − ) > 0 do contract = true U = U \ v − . v − = arg max v ∈ U ∂ Ψ ( U , v ) . until expand = false and contract = false Greedy Chomp . The Greedy Chomp algorithm ( Algorithm 2 ) is a more aggressive and faster version of the Greedy Nibble algorithm . Each phase adds a fraction F of the vertices which individually increase the Ψ value . If adding these F | U + | vertices simultaneously does not increase the overall Ψ value , then the fraction F is halved , unless F | U + | ≤ 1 . Similar to simulated annealing , this algorithm makes very large changes to the subset at the beginning but becomes more gradual as it approaches a local optimum . Theorem 4 . 1 . Both the Greedy Nibble algorithm and the Greedy Chomp algorithm converge to a local maxi - mum for Ψ . Proof . The algorithms increase the value of Ψ at each step , and there is a ﬁnite number of subsets , so they must terminate . By deﬁnition the result of termination is a local maximum . 4 . 1 Variations . There are many possible heuristic variations on the above algorithms . We use Greedy Nibble because of its simplicity and Greedy Chomp because it performs similarly but faster . In terms of initial seed selection , when time is not an issue , we recommend using each vertex as a seed . This ensures that every interesting cluster contains at least one seed . For larger graphs , randomly sampling some vertices as seeds should work comparably [ 38 ] . Clusters tend to be larger in this case , so most of them will still 733 Algorithm2 Greedy - Chomp ( U ) repeat expand = false ; F = 1 Calculate U + F while ¡ ∂ Ψ ( U , U + F ) < 0and F | U + | ≥ 1 ¢ do F = F / 2 ; Update U + F while ¡ ∂ Ψ ( U , U + F ) > 0 ¢ do expand = true U = U ∪ U + F . Calculate U + F ; F = 1 while ¡ ∂ Ψ ( U , U + F ) < 0and F | U + | ≥ 1 ¢ do F = F / 2 ; Update U + F contract = false ; F = 1 Calculate U − F while ¡ ∂ Ψ ( U , U − F ) < 0and F | U − | ≥ 1 ¢ do F = F / 2 ; Update U − F while ¡ ∂ Ψ ( U , U − F ) > 0 ¢ do contract = true U = U \ U − F . Calculate U − F ; F = 1 while ¡ ∂ Ψ ( U , U − F ) < 0and F | U − | ≥ 1 ¢ do F = F / 2 ; Update U − F until ( expand = false and contract = false ) containsomeseed . Alternatively , wecouldrunanother clusteringalgorithmtogenerateaninitialseedandjustuseourgreedyalgorithmsasareﬁnement . Ingeneral , weuse d P astheobjectivefunction , but itismorepronetogettingstuckinlocalmaximathanis M . Thusweenhanceeachinitialseedbyrunning theexpansionphaseofthealgorithmwith M 2 sinceit closelyapproximates d P asshowninFigure2 . Sinceouremphasisisonthediscrepancymeasure - mentratherthanclusteringtechnique , wefocusonil - lustratingthatthesesimpleclusteringtechniquesbasedonPoissondiscrepancyﬁndlocallysigniﬁcantclusters . 4 . 2Complexity . Itisdiﬃculttoanalyzeouralgo - rithmspreciselybecausetheymayalternatebetweentheexpansionandcontractionphasesmanytimes . But Theorem4 . 1showsthatthisprocessisﬁnite , andwe noticethatrelativelyfewcontractionstepsareeverper - formed . Hencewefocusonanalyzingtheworstcaseof theexpansionphaseinbothalgorithms . Bothalgorithmsareoutputdependent , wherethe runtimedependsonthesizeoftheﬁnalsubset | V Z | and thesizeofitsneighborhood | N ( V Z ) | . ForGreedyNibblewecanmaintain N ( V Z ) and calculate v + in O ( | N ( V Z ) | ) timesince d P onlydepends onthenumberofedgesand K V Z . Thusthealgorithm takes O ( | V Z | · | N ( V Z ) | ) timeforeachseedsince v + needs tobecalculatedeachiteration . TheGreedyChompalgorithmcouldreverttotheGreedyNibblealgorithmif F isimmediatelyreduced to1 / | U + | ateveryiteration . Soworstcaseitisno fasterthanGreedyNibble . Infact , eachiterationtakes O ( | N ( V Z ) | log | N ( V Z ) | ) timebecausethe ∂ Ψ ( U , v ) val - uesaresortedforall v ∈ U + . However , inpractice , amuchsmallernumberofiterationsarerequiredbe - causealargefractionofverticesareaddedateachiteration . If F wereﬁxedthroughoutthealgorithm , thenwecanlooselyboundtheruntimeas O ( log | V Z | · | N ( V Z ) | log | N ( V Z ) | ) . Since F isgenerallylargewhen mostoftheverticesareadded , thisisafairestimateof theasymptotics . ThisanalysisisfurtherevaluatedempiricallyinSection6 . 5BipartiteExtensions . Manydatasetswhichareapplicabletothismodel ( see Section6 ) arebipartitegraphs . Thuswederiveherethe keybipartiteextensionsofsection2andsection3 . Anundirectedgraph G = ( V , E ) is bipartite ifthere isapartition V = X ∪ Y with X and Y disjointand E ⊆ { { u , v } | u ∈ X , v ∈ Y } . Aclusterisdeﬁnedasa subgraph Z = ( V Z , E Z ) = ( X Z ∪ Y Z , E Z ) . Thebipartite versionofthePoissonrandomgraphmodeldoesnotallowself - loopsandthetotalnumberofobservededges in G is c ( G ) = P v i ∈ X k i = P v j ∈ Y k j . Thebipartite versionofthelocalmodularityandPoissondiscrepancyfollownaturally . 6Analysis . Thissectionfocusesonempiricallyexploringfouras - pectsofthiswork . First , weinvestigatethepowerand runtimeofouralgorithms . Second , weusePoissondis - crepancyasatooltoevaluateandcomparediﬀerentclusteringalgorithms . Third , weinvestigateproper - tiesoftheclustersfoundbyouralgorithmsmaximizingPoissondiscrepancy . Speciﬁcally , weshowthatvary - ingthe γ parametercangivereliableestimatesofthe sizeofclustersandweexaminecaseswhendistinctrel - evantclustersoverlap . Finally , throughoutthisanalysis wedemonstratethatmaximizingPoissondiscrepancyrevealsinterestingandrelevantclustersinreal - world graphs . 6 . 1PerformanceofOurAlgorithms . RuntimeonReal - worldDatasets . Wedemon - stratetheeﬀectivenessofouralgorithmonavarietyofrealworlddatasets , thesizesofwhicharesummarized inTable1 . The DBR datasetdescribesconnectionsbetween threads ( theset X ) andusers ( theset Y ) ofthe 734 Duke Basketball Report message board from 2 . 18 . 07 to 2 . 21 . 07 . Other datasets include Web 1 which links websites and users , Firm which links AP articles and business ﬁrms , Movie which links reviewers and movies through positive reviews , and Gene which links genes and PubMed articles . In each case , high discrepancy clusters can be used to either provide advertising focus onto a social group or insight into the structure of the dataset . Dataset | X | | Y | | E | Nibble Chomp DBR 68 97 410 0 . 025 0 . 018 Web 1023 1008 4230 0 . 179 0 . 049 Firm 4950 7355 30168 9 . 377 0 . 251 Movie 1556 57153 1270473 - 32 . 91 Gene 6806 595036 1578537 - 242 . 7 Table 1 : Sizes of real - world datasets and the average runtime in seconds for the Greedy Nibble and Greedy Chomp algorithms starting with singleton seeds . Run - times for Web and Firm were generated with 100 random samples . Runtimes for Movie and Gene were generated with 50 random samples . Power tests . The power of the test is the prob - ability that the test statistic exceeds a critical value under some alternative hypothesis compared to some null hypothesis [ 19 ] . To calculate the power of our algo - rithm , we synthetically insert signiﬁcant clusters into 100 random graphs and report the fraction of these graphs where our algorithm found the injected cluster . In particular , we generate bipartite graphs using the Poisson random graph model such that | X | = | Y | = 100 and | E | = 500 where the expected degrees of vertices vary between 3 and 7 . To inject a signiﬁcant cluster , we choose a random set of vertices V Z = X Z ∪ Y Z , where X Z ⊂ X , Y Z ⊂ Y , and | X Z | = | Y Z | = 15 . We increase the probability that an edge is chosen between two vertices in X Z and Y Z by a factor of ρ . We scale the probabilities on the other pairs of vertices in the graph so that each vertex retains its original expected degree . By choosing an appropriate value of ρ , we can generate graphs with the same expected degree sequence and whose injected cluster is expected to be signiﬁcant . We repeat this process until we generate 100 graphs whose injected clusters have a p - value less than 0 . 005 . We run Greedy Nibble and Greedy Chomp using each vertex as a seed . We say that we successfully found the injected cluster Z induced by V Z = X Z ∪ Y Z if the algorithm returns a cluster ˆ Z = X ˆ Z ∪ Y ˆ Z such that d ( X ˆ Z , X Z ) ≤ | X Z | and d ( Y ˆ Z , Y Z ) ≤ | Y Z | and it either 1 We thank Neilsen / / Netratings , Inc . , who provided the WEB dataset to us , for permission to use its data in this investigation . has p - value less than 0 . 05 or is among the top 5 clusters found . We report the power of the algorithms in Table 2 . It shows that 85 % of the time Greedy Chomp locates the injected clusters . Note that we have used a relaxed criteria to determine when an injected cluster is found by our algorithm ; a tighter qualiﬁcation would reduce this power measurement . Algorithm Nibble Chomp Power 0 . 83 0 . 85 Table 2 : Power for Greedy Nibble and Greedy Chomp tested on graphs of size 100 × 100 with an injected cluster of size 15 × 15 with p - value at most 0 . 005 . 6 . 2 Algorithm Comparison . Poisson discrepancy provides an absolute measure of cluster signiﬁcance . This allows comparison between diﬀerent clusterings of the same graph . We can evaluate the eﬀectiveness of existing clustering algorithms by calculating the discrepancy of the clusters they ﬁnd . Furthermore , we can enhance these clusters using Greedy Nibble or Greedy Chomp to maximize their discrepancy and evaluate how far from the local optimum these clusters used to be . We illustrate this by running the MCL algorithm [ 41 ] and the Ncut algorithm [ 40 ] on DBR . MCL generated 35 clusters and we ﬁxed the number of clusters in Ncut to be 10 . We report the top 4 clusters with the highest d P value in Table 3 . Ncut seems to ﬁnd clusters with higher discrepancy . We then use clusters found by MCL and DBR as seed sets in the Greedy Chomp , further reﬁning them in terms of their discrepancy . Ncut tends to do better than MCL in ﬁnding clusters within closer proximity of discrepancy local maxima . MCL 0 . 0376 0 . 0248 0 . 0223 0 . 0211 MCL + Chomp 0 . 0667 0 . 0790 0 . 0620 0 . 0698 Ncut 0 . 0692 0 . 0529 0 . 0527 0 . 0473 Ncut + Chomp 0 . 0757 0 . 0688 0 . 0635 0 . 0713 Table 3 : d P values of top 4 clusters found with MCL and Ncut on DBR and the d P values after their reﬁnement with Greedy Chomp . 6 . 3 Cluster Properties . Cluster Overlap Analysis . Many graph cluster - ing methods partition the data into disjoint subsets , es - sentially making each a cluster . Our approach ﬁnds clusters which may overlap , and it considers the rest of the graph uninteresting instead of forcing it to be a clus - 735 ter . We examine the top 6 clusters found from Greedy Nibble on DBR in an overlap matrix ( Table 4 ) . We use each vertex in X as a singleton seed set . The 1st , 2nd , 3rd and 5th clusters are very similar , representing a consistent set of about 13 threads on topics discussing the performance of players and strategy . The 4th clus - ter contains 14 threads which were posted by users who seem more interested in the site as a community and are more gossiping than analyzing . The 6th cluster contains an overlap of the above two types of topics and users : users who are interested in the community , but also take part in the analysis . The rest of the threads ( about 60 ) deal with a wider and less focused array of topics . C 1 2 3 4 5 6 d P p - value 1 13 12 12 1 12 8 0 . 0783 0 . 009 2 12 12 11 1 12 8 0 . 0764 0 . 019 3 12 11 14 0 11 7 0 . 0754 0 . 020 4 1 1 0 14 1 6 0 . 0749 0 . 020 5 12 12 11 1 13 8 0 . 0718 0 . 022 6 8 8 7 6 8 16 0 . 0703 0 . 077 Table 4 : Overlap of threads among the top 6 clusters for DBR with their d P and p - values found with the Greedy Nibble algorithm . γ as a Resolution Scale . For our algorithm , as γ varies , we observe an inverse linear correlation between γ and the average cluster size ( Figure 3 ) . We also show that as γ varies , our algorithm locates clusters that are statistically signiﬁcant on diﬀerent scales , and that their contents remain meaningful . This near - linear correlation makes γ a reliable res - olution scale for our clustering algorithm . As γ goes to 0 , the algorithm produces the whole graph as a cluster . As γ goes to inﬁnity , the algorithm produces trivial sin - gletons . The ﬂexibility to modify γ allows a user to bal - ance the importance of the statistical signiﬁcance of the clusters found , maximized by d P , γ , and their preferred size weighted by γ . This helps resolve issues ( previ - ously noted about modularity by [ 17 ] ) about the pre - ferred size of clusters which optimize d P . For instance when searching for more focused clusters of smaller size , a reasonable γ weight can be easily inferred . Cluster content and γ . Manual evaluation of the results show that the contents of the clusters remain meaningful and useful as γ is varied . For example , the top clusters found on the Movie dataset with γ = 200 are shown to be popular box oﬃce movies in the 90’s as they are consistently reviewed favorably by various reviewers . The top two clusters found on the Gene dataset with γ = 200 are genes in the UDP glucuronosyltransferase 1 and 2 family . The 4th ranked cluster consists of genes such as MLH and PMS , both are related to DNA mismatch 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 50 100 150inverse gamma vs . average cluster size X a v e r age c l u s t e r s i z e X inverse gamma 0 0 . 02 0 . 04 0 . 06 0 . 08 0 . 1 0 20 40 60 80 100inverse gamma vs . average cluster size X a v e r age c l u s t e r s i z e X inverse gamma 0 0 . 005 0 . 01 0 . 015 0 5 10 15 20 25 30inverse gamma vs . average cluster size X a v e r age c l u s t e r s i z e X inverse gamma 0 1 2 3 4 5 x 10 −3 5 10 15 20 25 30inverse gamma vs . average cluster size X a v e r age c l u s t e r s i z e X inverse gamma Figure 3 : Plot of 1 / γ vs . average cluster size on Web ( top left ) , Firm ( top right ) , Movie ( bottom left ) , and Gene ( bottom right ) . repair . The 8th ranked cluster for γ = 200 persists as the top ranked cluster when γ = 600 ; it consists of several genes for the zona pellucida glycoprotein , i . e ZP1 and ZP3A . As γ increases , the nontrivial clusters with a high d P , γ - discrepancy should generally be much denser inter - nally , since the ratio between the actual internal edges and expected edges should be greater than a given γ . On the other hand , clusters which persist as the top ranked clusters as γ increases are those that are most statisti - cal signiﬁcant in a dynamic setting . As γ increases , we would expect the number of such extremely anomalous clusters to decrease . For example , as shown in Figure 4 for the Web data set , as γ increases , the number of out - lier clusters with comparatively very large discrepancy decreases . For γ = 4 , many clusters seem to be sig - niﬁcantly larger than the large component , while with γ = 6 and γ = 8 there are very few . Finally , with γ = 10 all clusters are basically in the same component . The identiﬁcation of clusters of varying size but consistently high statistical signiﬁcance suggests that real - world networks are characterized by many diﬀerent levels of granularity . This result is consistent with , e . g , the contrasting ﬁndings of [ 37 ] and [ 31 ] , where clusters of vastly diﬀerent sizes but comparable modularities are detected in the same data set . This ﬁnding calls into question the wide variety of clustering methods which are only designed to detect one cluster for a given region or group of nodes , and a further study would be of interest . 736 0 0 . 005 0 . 01 0 . 015 0 . 02 0 . 025 0 50 100 150 200 250 cluster rank vs . cluster discrepancy c l u s t e r r an k cluster discrepancy 0 0 . 005 0 . 01 0 . 015 0 50 100 150 200 250 cluster rank vs . cluster discrepancy c l u s t e r r an k cluster discrepancy 0 0 . 002 0 . 004 0 . 006 0 . 008 0 . 01 0 . 012 0 50 100 150 200 250 cluster rank vs . cluster discrepancy c l u s t e r r an k cluster discrepancy 0 2 4 6 8 x 10 −3 0 50 100 150 200 250 cluster rank vs . cluster discrepancy c l u s t e r r an k cluster discrepancy Figure 4 : Web : cluster rank vs . cluster discrepancy with each X vertex used as a singleton seed set , γ = 4 ( top left ) , γ = 6 ( top right ) , γ = 8 ( bottom left ) , γ = 10 ( bottom right ) . Top ranked clusters appear at the bottom right of each ﬁgure . 7 Conclusions . The main contribution of this paper is the introduction of a quantitative and meaningful measure , Poisson dis - crepancy , for clusters in graphs , derived from spatial scan statistics on point sets . According to our deﬁni - tion , the higher the discrepancy , the better the cluster . We identify interesting relations between Poisson dis - crepancy , local modularity , and Bregman divergences . To illustrate the usefulness of this statistic , we de - scribe and demonstrate two simple algorithms which ﬁnd local optima with respect to the spatial scan statis - tic . In the context of real - world and synthetic datasets that are naturally represented as bipartite graphs , this method has identiﬁed individual clusters of vertices that are statistically the most signiﬁcant . These clusters are the least likely to occur under a random graph model and thus best identify closely - related groups within the network . Our model places no restrictions on overlap - ping of clusters , thus allowing a data point to be clas - siﬁed into two or more groups to which it belongs . As our greedy algorithms are the simplest and most intu - itive approach , it remains an open problem to ﬁnd more eﬀective algorithms to explore the space of potential subgraphs to maximize the Poisson discrepancy . Notice that Poisson discrepancy can also detect regions that are signiﬁcantly under - populated by requiring p < q in the alternative hypothesis . Similarly the spatial scan statistic Bernoulli model for graph clustering can be derived from the correspond - ing model for point sets . However , this model requires that each potential edge be chosen with equal probabil - ities , regardless of the degree of a vertex . Also , under this model each pair of vertices can have at most one edge . In summary , we argue that a graph cluster should be statistically justiﬁable , and a quantitative justiﬁca - tion comes from a generalization of spatial scan statis - tics on graphs , such as the Poisson discrepancy . References [ 1 ] D . Agarwal , A . McGregor , J . M . Phillips , S . Venkata - subramanian , and Z . Zhu . Spatial scan statistics : Ap - proximations and performance study . In 12 Annual ACM SIGKDD International Conference on Knowl - edge Discovery and Data Mining , 2006 . [ 2 ] D . Agarwal , J . M . Phillips , and S . Venkatasubrama - nian . The hunting of the bump : On maximizing sta - tistical discrepancy . In Proceedings 17th Annual ACM - SIAM Symposium on Discrete Algorithms , pages 1137 – 1146 , 2006 . [ 3 ] W . Aiello , F . Chung , and L . Lu . Random evolution in massive graphs . Handbook of massive data sets , pages 97 – 122 , 2002 . [ 4 ] R . Albert , H . Jeong , and A . Barabasi . The diameter of the world wide web . Nature , 401 : 130 , 1999 . [ 5 ] G . D . Bader and C . W . V . Hogue . An automated method for ﬁnding molecular complexes in large pro - tein interaction networks . BMC Bioinformatics , 4 : 2 , 2003 . [ 6 ] A . Banerjee , I . S . Dhillon , J . Ghosh , S . Merugu , and D . S . Modha . A generalized maximum entropy ap - proach to bregman co - clustering and matrix approxi - mation . In Proceedings 10th Annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2004 . [ 7 ] A . Banerjee , S . Merugu , I . S . Dhillon , and J . Ghosh . Clustering with bregman divergences . Journal of Ma - chine Learning , 6 : 1705 – 1749 , 2005 . [ 8 ] A . Barabasi and R . Albert . Emergence of scaling in random networks . Science , 286 : 509 , 1999 . [ 9 ] J . Baumes , M . Goldberg , M . Krishnamoorthy , M . Magdon - Ismail , and N . Preston . Finding com - munities by clustering a graph into overlapping sub - graphs . International Conference on Applied Comput - ing ( IADIS 2005 ) , Feb 2005 . [ 10 ] F . Chung and L . Lu . Connected components in random graphs with given expected degree sequences . Annals of Combinatorics , 6 ( 2 ) : 125 – 145 , 2002 . [ 11 ] A . Clauset . Finding local community structure in networks . Physical Review E , 72 , 2005 . [ 12 ] A . Clauset , M . E . J . Newman , and C . Moore . Finding community structure in very large networks . Phys . Rev . E , 70 : 066111 , 2004 . 737 [ 13 ] I . S . Dhillon , Y . Guan , and B . Kulis . Weighted graph cuts without eigenvectors : A multilevel approach . IEEE Transactions on Pattern Analysis and Machine Intelligence , 2007 ( to appear ) . [ 14 ] I . S . Dhillon , S . Mallela , and D . S . Modha . Information - theoretic co - clustering . In Proceedings 9th Annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2003 . [ 15 ] J . Duch and A . Arenas . Community detection in com - plex networks using extremal optimization . Physical Review E , 72 : 027104 , 2005 . [ 16 ] P . Erd¨os and A . R´enyi . On random graphs . Publ . Math . Debrecen , 1959 . [ 17 ] S . Fortunato and M . Barthelemy . Resolution limit in community detection . Proc . Natl . Acad . Sci . USA , 104 : 36 , 2007 . [ 18 ] M . Girvan and M . E . J . Newman . Community struc - ture in social and biological networks . Proc . Natl . Acad . Sci . USA , 99 ( 12 ) : 7821 – 7826 , 2002 . [ 19 ] J . Glaz , J . Naus , and S . Wallenstein . Scan Statistics . Springer - Verlag , New York , 2001 . [ 20 ] S . Itzkovitz , N . Kashtan , G . Ziv , and U . Alon . Sub - graphs in random networks . Physical Review E , 68 , 2003 . [ 21 ] J . M . Jennings , F . C . Curriero , D . Celentano , and J . M . Ellen . Geographic identiﬁcation of high gonorrhea transmission areas in baltimore , maryland . American Journal of Epidemiology , 161 ( 1 ) : 73 – 80 , 2005 . [ 22 ] M . Koyuturk , W . Szpankowski , and A . Grama . Assess - ing signiﬁcance of connectivity and conservation in pro - tein interaction networks . Journal of Computational Biology , 14 ( 6 ) : 747 – 764 , 2007 . [ 23 ] M . Kulldorﬀ . A spatial scan statistic . Communications in Statistics : Theory and Methods , 26 : 1481 – 1496 , 1997 . [ 24 ] M . Kulldorﬀ . Spatial scan statistics : models , calcula - tions , and applications . Scan Statistics and Applica - tions , pages 303 – 322 , 1999 . [ 25 ] M . Kulldorﬀ . Prospective time - periodic geographical disease surveillance using a scan statistic . Journal of the Royal Statistical Society , Series A , 164 : 61 – 72 , 2001 . [ 26 ] M . Kulldorﬀ . SaTScan User Guide , 7 . 0 edition , August 2006 . [ 27 ] M . Kulldorﬀ , T . Tango , and P . J . Park . Power compar - isons for disease clustering tests . Computational Statis - tics & Data Analysis , 42 ( 4 ) : 665 – 684 , April 2003 . [ 28 ] F . J . McErlean , D . A . Bell , and S . I . McClean . The use of simmulated annealing for clustering data in databases . Information Systems , 15 ( 2 ) : 233 – 245 , 1990 . [ 29 ] S . Muﬀ , F . Rao , and A . Caﬂisch . Local modularity measure for network clusterizations . Phys Rev E Stat Nonlin Soft Matter Phys , 72 ( 5 Pt 2 ) , November 2005 . [ 30 ] D . B . Neill and A . W . Moore . Rapid detection of signiﬁcant spatial clusters . In Proceedings 10th Annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2004 . [ 31 ] M . E . J . Newman . Fast algorithm for detecting community structure in networks . Physical Review E , 69 : 066133 , 2004 . [ 32 ] M . E . J . Newman . Finding community structure in networks using the eigenvectors of matrices . Phys . Rev . E , 74 : 036104 , 2006 . [ 33 ] G . Palla , I . Derenyi , I . Farkas , and T . Viscek . Uncov - ering the overlapping community structure of complex networks in nature and society . Nature , 435 : 814 – 818 , 2005 . [ 34 ] D . A . Plaisted . A heuristic algorithm for small separa - tors in arbitrary graphs . SIAM J . Comput . , 19 ( 2 ) : 267 – 280 , 1990 . [ 35 ] A . Pothen , H . Simon , and K . Liou . Partitioning sparse matrices with eigenvalues of graphs . SIAM J . Matrix Anal . Appl . , 11 ( 3 ) : 430 – 452 , 1990 . [ 36 ] C . E . Priebe , J . M . Conroy , D . J . Marchette , and Y . Park . Scan statistics on enron graphs . Computa - tional & Mathimatical Organization Theory , 11 ( 3 ) : 229 – 247 , October 2005 . [ 37 ] J . M . Pujol , J . Bejar , and J . Delgado . Clustering al - gorithm for determining community structure in large networks . to appear Physical Review E , 2006 . [ 38 ] J . Reichardt and S . Bornholdt . Statistical mechanics of community detection . Phys . Rev . E , 74 : 016110 , 2006 . [ 39 ] R . Sharan , T . Ideker , B . Kelley , R . Shamir , and R . Karp . Identiﬁcation of protein complexes by com - parative analysis of yeast and bacterial proten in - teraction data . Journal of Computational Biology , 12 ( 6 ) : 835 – 846 , 2005 . [ 40 ] J . Shi and J . Malik . Normalized cutes and image segmentation . IEEE Transactions on Pattern Analysis and Machine Intelligence , 22 ( 8 ) : 888 – 905 , August 2000 . [ 41 ] S . von Dongen . Graph Clustering by Flow Simulation . PhD thesis , University of Utrecht , May 2000 . [ 42 ] D . M . Wilkinson and B . A . Huberman . A method for identifying communities of related genes . Proc . Natl . Acad . Sci . USA , 101 ( 14 ) : 1073 , 2004 . 738