Evaluating HCI Research beyond Usability Abstract Evaluating research artefacts is an important step to showcase the validity of a chosen approach . The CHI community has developed and agreed upon a large variety of evaluation methods for HCI research ; however , sometimes those methods are not applicable or not sufficient . This is especially the case when the contribution lies within the context of the application area , such as for research in sustainable HCI , HCI for development , or design fiction and futures studies . In this SIG , we invite the CHI community to share their insights from projects that encountered problems in evaluating research and aim to discuss solutions for this difficult topic . We invite researchers from all areas of HCI research who are interested to engage in a debate of issues in the process of validating research artefacts . Author Keywords Evaluation ; Research Methods ; Validation ; Sustainable HCI ; HCI4D ; Design Fiction ; Futures Studies . ACM Classification Keywords H . 5 . 2 Information interfaces and presentation ( e . g . , HCI ) : User interfaces — Evaluation / methodology . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the Owner / Author . CHI ' 18 Extended Abstracts , April 21 – 26 , 2018 , Montreal , QC , Canada © 2018 Copyright is held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 5621 - 3 / 18 / 04 . https : / / doi . org / 10 . 1145 / 3170427 . 3185371 Christian Remy Aarhus University Aarhus , Denmark remy @ cc . au . dk Oliver Bates Lancaster University Lancaster , United Kingdom o . bates @ lancaster . ac . uk Jennifer Mankoff University of Washington Seattle , WA , USA jmankoff @ cs . washington . edu Adrian Friday Lancaster University Lancaster , United Kingdom a . friday @ lancaster . ac . uk Introduction Due to the interdisciplinary nature of the field , research in HCI often engages with problems of and develops solutions for various application domains . Some of those domains have spawned vibrant subdomains of research within the CHI community , such as sustainable HCI ( SHCI ) or HCI for development ( HCI4D ) [ 9 ] . Research in those domains is subject to the same review process that asks to validate proposed solutions to identified problems , but this often proves to be a difficult task for a variety of reasons : the established evaluation methods that focus on evaluating the usability of an HCI research artefact do not apply ; engaging with the target audience and environment which the artefact was designed for might be challenging ; the artefact might not be mature enough for a formal evaluation or is subject to a future context that does not exist yet . In this SIG , we aim to start an active and open - ended debate within the HCI community how to solve those evaluation challenges . We find motivation in recent efforts to identify solutions addressing this issue , such as in sustainable HCI [ 10 , 12 , 13 , 20 ] or information visualization [ 3 ] . However , this SIG is not limited to one particular application area and the evaluation thereof ; we welcome any CHI attendee who has encountered such evaluation challenges in their own research or who is interested in discussing the issue on a broader scope . Our goal is to establish a common ground within the community , formulate a concrete problem statement , and identify avenues for future research how to solve issues in evaluating HCI research . To this end , we envision an engaged and active discussion with all participants and aim to follow up on this SIG’s topic after its conclusion . Background In the traditional usability process , evaluation is an important step to check whether or not an implemented solution addresses the requirements and needs of users [ e . g . , 6 , 11 ] . While there is a large collection of methodologies and established processes within the realm of HCI [ e . g . , 2 , 7 , 14 , 17 ] those methods focus on evaluating the usability aspect of the solution . Those evaluation methods evolved over time and have been revisited or debated in the past [ e . g . , 1 , 4 ] . For solutions addressing HCI research problems that require a validation beyond traditional usability metrics , such a repository of knowledge does not ( yet ) exist . In addition to the goals being different to usability , other circumstances such as unknown future context of use [ e . g . , 15 ] or ambiguity of evaluating certain metrics , such as sustainability [ 19 ] further complicate the evaluation process . In the field of sustainable HCI the community decided that a one - size - fits - all approach is not possible [ 18 ] and the evaluation process should be developed on a per - project basis ; however , this puts the burden entirely on the researcher and severely hampers acceptance of novel research in the peer review process as the evaluation method itself is not an established one . This issue has also been subject of debate in other disciplines such as design fiction [ 8 ] , information visualization [ 3 ] , or action research [ 5 ] . Agenda We invite researchers from all areas of HCI to join us in this SIG to debate about evaluation issues for HCI research projects that go beyond traditional usability contributions . Following a brief introduction into the problem space and examples by the organizers to provoke thoughts of the attendees , we want everyone to engage in an open - ended discussion , reporting on similar stories of problems in past research projects or ideas for solutions . While the organizers’ common background is in sustainability and we lean on ideas from recent efforts to solve this issue [ 10 , 12 , 15 ] we will not limit our discussion to this application area , as the evaluation issue permeates other research domains at CHI as well . We aim to conclude the SIG with avenues to go forward and identify potential solutions that can be applied to practice in future projects or be subject of more nuanced debate in future workshops . References 1 . Louise Barkhuus and Jennifer A . Rode . 2007 . From Mice to Men - 24 Years of Evaluation in CHI . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , ACM . http : / / doi . org / 10 . 1145 / 1240624 . 2180963 2 . Alan J . Dix , Janet E . Finlay , Gregory D . Abowd , Russell Beale , and Janet E . Finley . 1998 . Human - Computer Interaction . Prentice Hall , London ; New York . 3 . Geoffrey Ellis and Alan Dix . 2006 . An Explorative Analysis of User Evaluation Studies in Information Visualisation . Proceedings of the 2006 AVI Workshop on BEyond Time and Errors : Novel Evaluation Methods for Information Visualization , ACM , 1 – 7 . http : / / doi . org / 10 . 1145 / 1168149 . 1168152 4 . Saul Greenberg and Bill Buxton . 2008 . Usability Evaluation Considered Harmful ( Some of the Time ) . Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , ACM , 111 – 120 . http : / / doi . org / 10 . 1145 / 1357054 . 1357074 5 . Gillian R . Hayes . 2011 . The Relationship of Action Research to Human - computer Interaction . ACM Trans . Comput . - Hum . Interact . 18 , 3 : 15 : 1 – 15 : 20 . http : / / doi . org / 10 . 1145 / 1993060 . 1993065 6 . International Organization for Standardization . 2010 . ISO 9241 - 210 : 2010 - Ergonomics of human - system interaction - - Part 210 : Human - centred design for interactive systems . 7 . Jonathan Lazar , Jinjuan Heidi Feng , and Harry Hochheiser . 2017 . Research Methods in Human - Computer Interaction , Second Edition . Morgan Kaufmann , Cambridge , MA . 8 . Joseph Lindley , Paul Coulton , and Emmett L . Brown . 2016 . Peer Review and Design Fiction : “Honestly , they’re not just made up . ” Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems , ACM , 583 – 595 . http : / / doi . org / 10 . 1145 / 2851581 . 2892568 9 . Yong Liu , Jorge Goncalves , Denzil Ferreira , Bei Xiao , Simo Hosio , and Vassilis Kostakos . 2014 . CHI 1994 - 2013 : Mapping Two Decades of Intellectual Progress Through Co - word Analysis . Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems , ACM , 3553 – 3562 . http : / / doi . org / 10 . 1145 / 2556288 . 2556969 10 . Anton Lundström and Daniel Pargman . 2017 . Developing a Framework for Evaluating the Sustainability of Computing Projects . Proceedings of the 2017 Workshop on Computing Within Limits , ACM , 111 – 117 . http : / / doi . org / 10 . 1145 / 3080556 . 3080562 11 . Jakob Nielsen . 1994 . Usability engineering . Morgan Kaufmann Publishers , San Francisco , Calif . 12 . Christian Remy , Oliver Bates , Alan Dix , et al . 2018 . Evaluation beyond Usability : Validating Sustainable HCI Research . Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 13 . Christian Remy , Oliver Bates , Thomas Vanessa , and Elaine May Huang . 2017 . The Limits of Evaluating Sustainability . Proceedings of the Third Workshop on Computing within Limits , ACM Press . 14 . Yvonne Rogers , Jenny Preece , and Helen Sharp . 2011 . Interaction design . Wiley , Hoboken , N . J . ; Chichester . 15 . Antti Salovaara , Antti Oulasvirta , and Giulio Jacucci . 2017 . Evaluation of Prototypes and the Problem of Possible Futures . Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , ACM , 2064 – 2077 . http : / / doi . org / 10 . 1145 / 3025453 . 3025658 16 . Michael Sedlmair , Petra Isenberg , Miriah Meyer , and Tobias Isenberg . 2018 . BELIV2018 . 17 . Ben Shneiderman , Catherine Plaisant , Maxine Cohen , and Steven Jacobs . 2009 . Designing the User Interface : Strategies for Effective Human - Computer Interaction . Pearson , Boston . 18 . M . Six Silberman , Eli Blevis , Elaine Huang , et al . 2014 . What Have We Learned ? : A SIGCHI HCI & Sustainability Community Workshop . CHI ’14 Extended Abstracts on Human Factors in Computing Systems , ACM , 143 – 146 . http : / / doi . org / 10 . 1145 / 2559206 . 2559238 19 . M . Six Silberman and Bill Tomlinson . 2010 . Toward an ecological sensibility : tools for evaluating sustainable HCI . CHI ’10 Extended Abstracts on Human Factors in Computing Systems , ACM , 3469 – 3474 . http : / / doi . org / 10 . 1145 / 1753846 . 1754003 20 . Kentaro Toyama . 2015 . Preliminary thoughts on a taxonomy of value for sustainable computing . First Monday 20 , 8 .