Noname manuscript No . ( will be inserted by the editor ) Towards a Change Taxonomy for Machine Learning Systems Empirical Study of Contributions to Published ML Pipeline Repositories Aaditya Bhatia ( ) · Ellis E . Eghan · Manel Grichi · William G . Cavanagh · Zhen Ming ( Jack ) Jiang · Bram Adams Received : date / Accepted : date Abstract Machine Learning ( ML ) research publications commonly provide open - source implementations on GitHub , allowing their audience to replicate , validate , or even extend machine learning algorithms , data sets and metadata . However , thus far little is known about the degree of collaboration activity happening on such ML research repositories , in particular regarding ( 1 ) the degree to which such repositories receive contributions from forks , ( 2 ) the na - ture of such contributions ( i . e . , the types of changes ) , and ( 3 ) the nature of Aaditya Bhatia ( Ph . D . at SAIL ) the corresponding author and · Bram Adams ( Director of MCIS Lab ) Queen’s University Kingston , ON , Canada E - mail : { aaditya . bhatia , bram . adams } @ queensu . ca Ellis E . Eghan Assistant Professor , University of Cape Coast , Ghana E - mail : elliseghan @ gmail . com Manel Grichi Lead Data Scientist , VibroSystM Inc . , Montreal , Canada E - mail : grichimanel @ gmail . com William G . Cavanagh Student , Polytechnique Montreal , Canada E - mail : william . glazer - cavanagh @ polymtl . ca Zhen Ming ( Jack ) Jiang Associate Professor and director of SCALE Lab , York University , Canada E - mail : zmjiang @ cse . yorku . ca Aaditya Bhatia is the corresponding author a r X i v : 2203 . 11365v1 [ c s . S E ] 21 M a r 2022 2 Bhatia et . al . changes that are not contributed back to forks , which might represent missed opportunities . In this paper , we empirically study contributions to 1 , 346 ML research repositories and their 67 , 369 forks , both quantitatively and quali - tatively ( by building on Hindle et al . ’s seminal taxonomy of code changes ) . We found that while ML research repositories are heavily forked , only 9 % of the forks made modiﬁcations to the forked repository . 42 % of the latter sent changes to the parent repositories , half of which ( 52 % ) were accepted by the parent repositories . Our qualitative analysis on 539 contributed and 378 local ( fork - only ) changes extends Hindle et al . ’s taxonomy with one new top - level change category related to ML ( Data ) , and 15 new sub - categories , including nine ML - speciﬁc ones ( input data , output data , program data , sharing , change evaluation , parameter tuning , performance , pre - processing , model training ) . While the changes that are not contributed back by the forks mostly con - cern domain - speciﬁc customizations and local experimentation ( e . g . , parame - ter tuning ) , the origin ML repositories do miss out on a non - negligible 15 . 4 % of Documentation changes , 13 . 6 % of Feature changes and 11 . 4 % of Bug ﬁx changes . The ﬁndings in this paper will be useful for practitioners , researchers , toolsmiths , and educators . Keywords Machine Learning · Change Taxonomy · GitHub Collaborations · Contribution Management 1 Introduction The notion of “Software Engineering for Machine Learning / Artiﬁcial Intelli - gence” ( SE4ML / SE4AI ) is becoming widespread in the software engineering community , with software engineering conferences featuring dedicated tracks and with dedicated venues appearing ( RAISE , SEMLA , CAIN ) . As shown in Figure 1 , the term SE4ML actually covers at least three diﬀerent types of software projects : ML frameworks ( Type I ) , ML pipelines ( Type II ) and ML applications ( Type III ) . The lowest tier of SE4ML systems ( Type I ) consists of machine learning ( ML ) frameworks like Tensorﬂow 1 and PyTorch 2 . These are generic frame - works providing the building blocks for implementing ML classiﬁcation , regres - sion , recommendation , and clustering algorithms in any possible domain . The middle tier ( Type II ) consists of ML pipelines that build on the ML frame - works to provide infrastructure [ 1 , 2 ] to ingest data , perform pre - processing , build , tune and evaluate ML models speciﬁc to a given domain ( e . g . , image recognition or fraud detection ) . These pipelines , by themselves , only produce datasets , models and evaluation / execution metadata [ 3 ] , they are not geared towards end users , but instead cater to inter - disciplinary teams of data sci - entists , data engineers and developers [ 4 ] . Finally , the third tier ( Type III ) is responsible for integrating said artifacts ( notably the models ) into robust 1 https : / / www . tensorflow . org 2 https : / / pytorch . org Towards a Change Taxonomy for Machine Learning Systems 3 Fig . 1 : Hierarchy of software projects related to machine learning . This paper focuses on Type - II ML pipeline projects linked to arXiv papers , which we refer to as “ML research repositories” . ML applications for end users , leveraging ML models trained by a Type II pipeline on an application - speciﬁc data set . While projects could be both Type II and III at the same time , the contrasting goals of experimentation ( type II ; to get the best - performing models ) and robustness ( type III ; of end - user applications ) mean that it is recommended to keep the corresponding artifacts separated , but with clear traceability to each other [ 4 ] . While Type - I and Type - III projects are developed , respectively , by a small number of large open - source projects and many individual software organi - zations , a substantial proportion of Type - II projects , i . e . , ML pipelines , are produced by researchers as a by - product of a published paper [ 5 ] . This is be - cause most of the major research venues , both in the AI ( e . g . , AAAI [ 6 ] , NeurIPS [ 7 ] , ICML [ 8 ] ) and software engineering domains ( e . g . , ICSE [ 9 ] , EMSE [ 10 ] , TSE [ 11 ] ) , have strict submission requirements to facilitate open science and reproducibility of research results , which requires submission of datasets and code along with their papers . Typically , researchers would up - load a preprint of their work on a repository like ArXiv , including a GitHub repository with the pipeline code and ( potentially ) a labeled data set to train the pipeline on . Popular online indexes like PapersWithCode 3 or ModelDepot 4 provide searchable lists of papers , and their associated artifacts like implemen - tations and / or datasets . Since Type - II ML pipelines typically are shared in the form of GitHub repositories 5 , an important question is to what extent such projects adhere to and beneﬁt from the open - source collaboration models leveraged by tradi - tional ( non - AI ) GitHub projects . For example , certain assumptions of estab - 3 https : / / paperswithcode . com 4 https : / / modeldepot . io / 5 In the remainder of this paper , we use the term “ML research repositories” to identify such Type - II ML pipelines . 4 Bhatia et . al . lished software engineering activities like requirements engineering , software design and quality assurance are no longer valid [ 12 , 13 ] , while a typical ML development team does not only feature traditional developer and tester roles , but also data scientists and data engineers [ 1 ] . Such multi - disciplinary col - laboration leads to a wide range of diﬀerent artifacts other than source code that require proper versioning and traceability with the code [ 3 ] . In short , new software engineering practices are emerging , replacing existing models of ( multi - disciplinary ) collaboration . While OSS collaborations in non - ML software are extensively studied by researchers [ 14 , 15 , 16 , 17 , 18 ] , this is not the case in the context of ML . The typical GitHub collaborative coding model would see the OSS community fork an ML research project [ 17 ] , make changes to the source code , and push those changes back to the original project using Pull Requests ( PRs ) . The devel - opers of the original project would then check such PRs and accept or reject the proposed changes . Accepted PRs would then lead to code changes being merged in the ML research repository , a so - called upstream change . Often - times , community developers could also make changes for their own use that they would not contribute back , i . e . , so - called downstream changes . GitHub’s collaborative forking model has been known to improve the productivity of multifaceted software development and management tasks like making new features , handling issue , sharing knowledge , adding documentation , and man - aging upstream and downstream code [ 17 ] . In the context of ML research repositories , major collaboration - related questions remain answered . For instance , are ML research repositories just on - line storage sites of reproducibility packages , or do real collaborations emerge that help maintain and evolve the ML repositories ? What kind of community - based changes do these repositories receive , and which ones do they miss ( not contributed back ) ? Do existing code change taxonomies capture the nature of such changes , or do the many kinds of artifacts other than source code [ 3 ] require a substantial revision of those taxonomies ? Answering these questions is essential to help software practitioners in having a better understanding of building and maintaining ML software . At the same time , ML software ed - ucators need an understanding of ML changes to equip students and novice developers in the development of ML applications . Finally , toolsmiths can use these insights to shape future tools and clarify use cases for their tools in the ML era . Hence , this paper empirically studies changes in ML research reposito - ries and their forks , using a mixed - methods approach . First , we quantitatively mine the community collaborations to 1 , 346 ML research repositories ( contain - ing implementations of 1 , 144 arXiv publications ) obtained via ModelDepot’s “Deep Search” engine to analyze the behavior involving their forks , i . e . , how active is online collaboration on ML research repositories ? We then perform a large - scale qualitative analysis of 539 upstream and 378 downstream changes , adapting Hindle et al . [ 19 ] ’s taxonomy of code changes . Notably , we address the following research questions : Towards a Change Taxonomy for Machine Learning Systems 5 – RQ1 : To what extent do ML research repositories form the basis of other contributors’ work ? ML research repositories are heavily and transitively forked , yet overall only 9 % of forks made modiﬁcations . 41 . 6 % of the latter forks sent changes back to the parent ML research repositories ( i . e . , upstream changes ) , half of which ( 52 % ) were accepted by the parent repositories . The time taken to merge those pull requests is about four times faster than for NPM packages on GitHub [ 20 ] and 24 times faster than for Github projects in general [ 21 ] . – RQ2 : What are the types of changes in ML research repositories ? Using the seminal code change taxonomy of Hindle et al . [ 19 ] as a starting point , we identiﬁed two new categories of changes in ML research repos - itories , namely Data , and Dependency Management . Furthermore , we re - ﬁned the taxonomy with 15 new sub - categories of changes . Nine of the sub - categories ( i . e . , input data , output data , program data , sharing , change evaluation , parameter tuning , performance , pre - processing , model training ) are ML - speciﬁc , while six ( i . e . , add package , remove package , update pack - age , ﬁle permissions , comprehension , add auto - generated code ) are more general sub - categories . – RQ3 : How do downstream changes diﬀer from upstream changes in ML research repositories ? Manual comparison of changes contributed back by the forks to the ori - gin ML repository ( upstream changes ) with changes not contributed back ( downstream changes ) shows that downstream changes typically are domain - oriented and add input / output data , perform parameter tuning , add new functional features , and perform other non - functional changes like inden - tation , refactoring or cleaning up the source code . In contrast to this , upstream changes beneﬁt the parent repository by updating packages or ﬁxing bugs for the parent repository . Both downstream and upstream con - tributions add documentation , and ﬁx bugs . The remainder of the paper is organized as follows : Section 3 presents the data collection and design of our study . Section 2 summarises the related literature . Section 4 discusses the motivation , approach , and results for each of our research questions . Section 6 discusses threats to the validity of our study while Section 5 explains the implications of our ﬁndings . Finally , Section 7 concludes the paper . 2 Related Work 2 . 1 Code Change Classiﬁcation Prior work in code change taxonomies initiated in 1976 with Swanson et al . ’s work on identifying changes during software maintenance in terms of correc - tive , adaptive and perfective [ 22 ] . Understanding such categories of changes enhances software decision - making . These changes were adopted as extended - Swanson categories by Hindle et al . [ 19 ] in their seminal taxonomy of software 6 Bhatia et . al . changes in 2008 . Our qualitative study builds on Hindle’s change taxonomy , ex - tending it with two new high - level change categories and 15 new sub - categories of changes . Later work shifted direction from establishing taxonomies to automated classiﬁcation of code changes in terms of activities deﬁned by such change taxonomies . In Hindle et al . ’s [ 23 ] later publication , the authors automatically classify maintenance changes into corrective , adaptive , perfective , feature ad - dition , and non - functional improvement categories using ML techniques . Yan et al . [ 24 ] improved this approach of classifying code changes using a Dis - criminative Probability Latent Semantic Analysis ( DPLSA ) approach , which showed its beneﬁts in multi - category classiﬁcations of code change activities during the evolution of software . Recently , in 2021 , Ghadab et al . [ 25 ] further improvised classiﬁcation of code changes using BERT ( Bidirectional Encoder Representations from Transformers ) approach . Code change taxonomies are used for a variety of purposes . In 2009 , Ben - estad et al . [ 26 ] performed a literature survey on publications that assessed the impact of individual code changes on the maintenance and evolution of software systems . Wu et al . [ 27 ] extracted missing links between bugs and committed changes by creating an automated tool , Relink . Furthermore , Bis - syand´e et al . [ 28 ] evaluated the eﬃcacy of linking bug reports to code changes by benchmarking Relink against alternative bug - linking solutions . Cort´es - Coy et al . [ 29 ] used code changes to automatically generate commit messages . Farago et al . [ 30 ] studied code changes to understand the impact of change operations like add / update / delete on ISO / IEC - 9126 quality attributes of the software . Software developers and researchers developing such tools may wish to incul - cate our extended taxonomy of code changes to better support the development of ML systems . Our work on code changes on ML research repositories can help researchers and toolsmiths shape future tools and more clearly understand use cases for their tools in the ML era . 2 . 2 Multi - repository software development via Forking Many researchers study collaborative development . For instance , Zhou et al . [ 18 ] identiﬁed eﬃcient practices for developers collaborating using forks . The au - thors build regression models to correlate eﬃcient practices with respect to the behavior around forking . They found how the modularity of a code base and its contributions , as well as upfront management of which bugs require ﬁxing by contributors , correlate with higher contribution volume and pull request acceptance . Later , in 2020 , in a follow - up work [ 17 ] , Zhou et al . elucidated the percep - tions around “hard forks” ( forks that split development into a competing line of a new repository ) , against those of “social forks” ( forks that create a public copy of the repository on a social website like BitBucket or GitHub ) . While hard forking traditionally has been considered a bad practice for developers Towards a Change Taxonomy for Machine Learning Systems 7 and users [ 31 ] , the authors found that the perceptions around hard - forking have changed in modern times . Nowadays , hard forks emerge out of social forks , and are seen as a positive non - competitive alternative to the original repository . Constantino et al . [ 32 ] identiﬁed the rationales , processes , and chal - lenges behind collaborative activities on GitHub by conducting surveys . The authors found that GitHub collaborations contribute to software development , issue management , repository management , and documentation tasks . Brisson et al . [ 33 ] studied collaborations on GitHub projects by analyzing transitive forks , user statistics , pull requests , and issues . Furthermore , Biazzini et al . [ 15 ] identiﬁed dispersion metrics for fork - induced code changes . Ren et al . [ 34 ] developed a web UI for the management of forking - based collaborations with features like fork searching and tagging . Other research [ 35 , 36 ] studies the nature of upstream contributions in the form of Pull Requests and identiﬁes the nature of competing contributions . However , none of this prior research studies the collaborative development of ML software . We build on the results from prior studies to compare the forking dynamics of ML research repositories . 2 . 3 Software engineering for machine learning In 2020 , Ozkaya [ 13 ] identiﬁed that ML systems are substantially diﬀerent from traditional software systems , and hence need dedicated research . For example , Washizaki et al . found that ML software engineering design patterns diﬀer from those of non - ML software [ 12 ] . Further , in 2020 , Ozkaya and Ipek [ 37 ] illustrated how the stochastic nature of ML changes the software development practices in ML . Overall , Mart´ınez et al . [ 38 ] performed a literature review on Software Engineering for AI - based systems . Recent widespread advances in ML have instigated researchers to study the maintenance activities and challenges in ML code . In 2019 , Amershi et al . [ 1 ] uncovered the challenges in managing ML software at Microsoft , in particu - lar identifying the typical ML pipeline and corresponding software activities . Furthermore , Zhang et al . [ 39 ] and Arpteg et al . [ 40 ] studied the software challenges faced in deep learning applications . Sambasivan et al . [ 41 ] iden - tiﬁed data engineering challenges for which multiple roles of data engineers ( like data collectors , annotators , ML developers , data licensing teams ) require powerful data infrastructure in order to support machine learning processes . In the context of the data lifecycle used for ML , Polyzotis et al . [ 42 ] illustrated the challenges faced at Google . O’Leary and Uchida [ 43 ] also studied problems with creating ML pipelines from existing code at Google . The work of Fan et al . [ 5 ] is the closest to our paper . While the authors study a similar dataset as ours ( i . e . , 1 , 149 academic ML ( AI ) repositories refer - encing ArXiv publications ) , the authors focus on characterizing popular versus unpopular academic repositories in terms of the number of stars on GitHub , and analyzing factors correlations between the number of paper citations and GitHub repository metrics . However , our paper is the ﬁrst to study the extent 8 Bhatia et . al . of actual OSS collaborations happening on ML research repositories ( instead of paper activity based on those repositories ) , and to manually identify the nature of code changes performed on such ML pipeline projects . 3 Data Collection and Experiment Setup ModelDepot . io " ArXiv " reference filter 1 , 346 ML repositories Data Collection Quantitative Analysis of ML Pipelines Repository Stats Analyzer 6 , 069 Non - Trivial Forks 67 , 369 Repository Forks Qualitative Analysis of ML Pipelines 95 % ConfidenceInterval 5 % Confidence Level 378 DownstreamCommits Fork Stats Analyzer 207 PRs having 539 UpstreamCommits 2 , 509 Pull Requests Top 5 % Repositories With Maximum Non - Trivial Forks 445 Pull Requests 10817 DownstreamCommits Fig . 2 : Data collection and processing steps . Figure 2 presents an overview of our data collection procedure along with the design of our empirical study to address the research questions of the introduction . RQ1 quantitatively studies the OSS collaboration characteristics on ML research repositories , while RQ2 and RQ3 perform a qualitative analysis on the nature of changes performed during this collaboration . 3 . 1 Data Collection ModelDepot was a popular online model store containing 1 ) a catalogue of pre - trained ML models , and 2 ) a GitHub search engine for ML model pipeline implementations called “Deep Search” . The latter engine eﬀectively was an index of GitHub projects related to ML , allowing to search the projects based on name , ML framework ( e . g . , Tensorﬂow ) , programming language , and model category ( i . e . , “computer vision” , “natural language” , “reinforcement learn - ing” , “generative” and “audio” ) . At the time this study was conducted in Summer 2019 , ModelDepot indexed over 50 , 000 ML implementations . Towards a Change Taxonomy for Machine Learning Systems 9 While both ModelDepot and its major competitor at that time , Paper - sWithCode , indexed GitHub repositories , we selected ModelDepot for our study because it was the most popular and diverse at the time of crawling the data ( i . e . , 50 , 000 model implementations compared to 8 , 500 on paperswith - code . com 6 ) . The fact that ModelDepot did not require manual contributions to register new models , but leveraged its automated “Deep Search” engine to track new ML model repositories , was another reason why we opted for ModelDepot . To collect the ModelDepot data , we built a scraper ( crawler ) to mine ML projects in the “Computer Vision” and “Machine Learning” cate - gories , which were the two most common categories of models 7 . After sorting by the search engine’s “best match” feature , we then focused on the top 5 , 000 non - fork projects . Since this paper focuses on the evolution of ML model pipelines produced by researchers or inspired by the work of researchers , we ﬁltered the 5 , 000 crawled projects using string matching to check the readme ﬁles for the pres - ence of the term “arxiv” ( referring to an ArXiv URL of an academic paper ) . This yielded 1 , 346 ML research repositories as our dataset for this study . Within this dataset , 1 , 144 unique academic papers were referenced and 23 % of these publications were referenced more than once . One of these publica - tions , “Deep Residual for Image Recognition” 8 was referenced the most ( 46 ) by the repositories in our dataset . 3 . 2 Quantitative Analysis of Forking in ML Research Repositories ( RQ1 ) To analyze the dynamics of OSS collaboration through forks of ML research repositories ( RQ1 ) , we use the GitHub Search API 9 to analyze each of the 1 , 346 repositories in our dataset . We perform our quantitative analysis via eight metrics related to forking as described in Table 1 . Five out of these metrics ( the bolded ones in Table 1 ) are adapted from Brisson et al . [ 33 ] . Given a large number of forks , Brisson et al . [ 33 ] suggested that fork data is noisy . For this reason , we introduce the concept of Non - trivial forks to identify forks with modiﬁcations . Such Non - trivial forks contain at least one commit that does not occur in the parent repository ( downstream changes ) , or contributed back at least one commit via a pull request ( upstream changes ) . To identify forks with downstream changes , we ﬁrst calculate for each fork F the set of commits S F whose commit id does not occur in the par - ent repository . Since the resulting set S F of fork commits could still contain commits that have been merged upstream through rebasing ( changing their commit id ) , we then check for each commit in S F to see if any commit in the parent repository has the same commit message subject , author name , and 6 https : / / twitter . com / paperswithcode / status / 1091315540092768257 7 https : / / web . archive . org / web / 20190404211946 / https : / / modeldepot . io / search / results ? q = 8 https : / / arxiv . org / abs / 1512 . 03385 9 https : / / docs . github . com / en / rest / reference / search 10 Bhatia et . al . Table 1 : Metrics used in RQ1 . The bolded metrics are adapted from Brisson et al . [ 33 ] . The abbreviation “repo” is used for “repository” . Metric Deﬁnition Star # # users who starred the repo Fork # # forks created from the repo Forking Depth # times the forks themselves are forked Non - trivial forks # forks where the forked code base is modiﬁed PR # # PRs sent to the parent repo by its forks PR Accept % ( # PRs merged into parent repo ) / ( PR # ) First Fork Time # Days between creation of a repo and its ﬁrst fork Final Fork Time # Days between creation of a repo and its ﬁnal fork author date , since those metadata ﬁelds have been found to be stable during rebase [ 44 ] . If so , we remove those commits from S F , since they also exist in the parent repository . Since we may be missing cases where a fork had all of its commits merged as PRs , we then check the list of forks that submitted PRs using the GitHub Search API , and add such forks into S F . If the result - ing S F is not empty , we consider each F in S F to be a Non - trivial fork . We used the Star # and Fork # metrics to measure the popularity of reposi - tories , as indicated by Borges et al . [ 45 ] . We computed the First Fork time and Final fork time metrics to indicate the temporal aspects of ML research repositories . The First Fork time indicates the speed of the OSS community in adapting an ML implementation , whereas the ﬁnal fork time indicates the longevity of collaboration activities on ML pipeline code . In addition , we calculate the metric , forking depth , which is the number of transitive forks ( i . e . , forks of forks ) of the repositories in our data set , as a measure of the value of the latter repositories for the OSS community in terms of collaborative purposes . 3 . 3 Qualitative Analysis of Change Types in Forks ( RQ2 / RQ3 ) In 2008 , Hindle et al . [ 19 ] presented a taxonomy of code changes for traditional ( non - ML ) software , which is still the authoritative taxonomy in use today , de - spite being 13 years old at the time of writing this paper . As shown in Table 2 , the authors identiﬁed a taxonomy of seven code change dimensions , along with 24 sub - categories of changes . Since software development has become more col - laborative , for example , due to platforms like GitHub , additional change types might need to be added to the taxonomy in Table 2 . Furthermore , since the three types of ML software ( see Fig . 1 ) have ob - tained a prominent place in software engineering , and the nature of the ma - chine learning software lifecycle is substantially diﬀerent from that of tradi - tional software [ 1 ] , one would expect further change types to be added . ML practitioners use data pre - processing techniques , iteratively tune model hyper - parameters for obtaining the most optimal ML model under the data science life cycle . Reusing ML software requires users to understand the rationale Towards a Change Taxonomy for Machine Learning Systems 11 Table 2 : Hindle et al . ’s taxonomy of change types for traditional SE [ 19 ] . Category Sub - Category Deﬁnition Maintenance Bug ﬁx Fixing bugs ( e . g . , adding exception control , conditional state - ments ) Cross Cross - cutting changes ( e . g . , logging ) Maintenance Performing activities during maintenance cycle other than ﬁx - ing bugs Parameter list change Updating in the parameters list Debug Setting up debug , tracking process ( e . g . , printing variable val - ues , execution times ) Meta - Program Documentation Changing the software documentation ( e . g . , read - me ﬁle , code comments ) Build / Conﬁg Changing build or work - space conﬁguration ﬁles ( e . g . , setup . txt or . yml ) Testing Adding unit tests , bench - marking , changing test environment Internationalization Adding language support other than English Non - Functional Source Code Change Refactor Structural changes without changing the behavior ( e . g . , re - naming variables , optimizing code ) Clean up Deleting code not used by the program ( e . g . , print statements , comments , unused imports ) Indent Adding proper indentation or formatting the code Token replace Renaming tokens like variable or method names Source Management Merge Merging commits or pull requests Source control Managing repository ﬁles ( e . g , adding ﬁles to git ignore ) Versioning Changing the software release version Branching Creating a side development branch from the main branch External Code submitted by developers who are not a part of the core team Implementation Feature Adding new functional features Platform Changing hardware or platform speciﬁc code ( e . g . , changing GPU hardware acceleration , changing ﬁle access for new plat - form ) Module Management Add module Adding modules / directories / ﬁles Move module Moving modules / directories / ﬁles Remove module Removing modules / directories / ﬁles Legal Licence Changing copyright or authorship behind the ML implementations , instigating users to change their code for comprehending the ML code . All of this has led to a variety of types of arti - facts other than source code that require change [ 3 ] . In this study , we build on Hindle et al . ’s taxonomy to identify the types of changes in ML research repositories by studying a statistically signiﬁcant random sample of 1 ) fork PRs merged by the parent ML research repository ( i . e . , upstream changes ) and 2 ) commits within the forked repositories that were not submitted as PRs ( i . e . , downstream changes ) . We describe our qualitative analysis process below : 1 . Selection of repositories for sampling . Since not all forked repositories made changes , let alone sent them as PRs upstream , we ﬁrst select the repositories having the most active forks . To do so , we ordered the repositories by our metric non - trivial forks and 12 Bhatia et . al . obtained the top 5 percentile , i . e . , 23 repositories . Overall , these 23 repos - itories had 10 , 817 downstream commits and 445 PRs . From these , using a 95 % conﬁdence level and 5 % conﬁdence interval , we obtain a statistically representative sample of 1 ) 207 PRs containing 539 upstream commits ( upstream sample ) 10 , and 2 ) 378 downstream commits ( Downstream Sample ) . Both samples were stratiﬁed , such that repositories with a higher propor - tion of Non - trivial forks had more data points in the samples . Since a PR can comprise multiple commits , we selected all the commits for the sample of 207 PRs and obtained 539 upstream commits . Similar to obtain - ing the stratiﬁed upstream sample , we used the proportion of # Commits with respect to the Non - trivial forks of each of the 23 parent reposito - ries for creating the stratiﬁed downstream sample . 2 . Study Participants . We used teams from two universities to manu - ally classify the types of changes in upstream and downstream commits . University - A classiﬁed downstream changes while University - B classi - ﬁed upstream changes . Both teams included two or more grad students , and one faculty member , all having knowledge of ML and non - ML soft - ware design and development . Due to the large - scale nature of our study , we employed four coders in team A and three coders in Team B . 3 . Extending taxonomy of changes . From the sample of 378 downstream commits , Team - A ﬁrst performed a pilot study on an initial sample of 78 commits to validate the extent to which Hindle’s taxonomy [ 19 ] was able to classify code changes or required reﬁnements . The 78 commits were distributed across the three coders of Team - A such that each commit had two coders and each coder had 26 commits in common with each of the other coders . The assignment of commits to each coder was anonymous . Each coder then individually coded their 52 ( 2 x 26 ) assigned commits , identifying all types of changes within the commits under study ( more than one type of change could apply ) . In cases where the change ( sub - ) type could not be found using Hindle’s change categories , the coders individually could create a new category . Once ﬁnished , the coders met online discussing only the new types of changes that they had identiﬁed , without considering the speciﬁc commits tagged with these new types . The proposed new types could be merged , renamed , or removed until a consensus was reached . Team - A then re - labeled their samples using the enhanced Hindle’s taxon - omy . Once done , the coding results were combined into a spreadsheet . In the ﬁrst phase , each coder had to check the commits they were as - signed that had conﬂicting coding results . This was done asynchronously by adding comments on the spreadsheet . If a coder was in accord with the other coder’s interpretation , a disagreement was resolved , otherwise , it was left open . In a second phase , the remaining disagreement cases were then discussed in person by Team - A , possibly reﬁning the taxonomy . 10 PRs on GitHub are not limited to just one commit . Towards a Change Taxonomy for Machine Learning Systems 13 With this ﬁnal version of the taxonomy , Team - A started labeling the re - mainder of its samples , using the same style of assignment as for the initial 78 ( i . e . , anonymously sharing the same number of commits with each other coder ) . In parallel , Team - B was assigned the 207 PRs in a similar manner . While both teams could still make changes to the taxonomy during this coding , we observed saturation in the labels after tagging the initial set of 78 downstream commits , i . e . , no new ( sub - ) categories were identiﬁed in the later part of labeling the 300 downstream samples and 539 upstream samples . 4 . Calculation of inter - rater agreement . Once coding was ﬁnished , both teams individually used the spreadsheet - based and in - person resolution of disagreement used initially for the ﬁrst 78 cases . To calculate inter - rater agreement , since each sample was rated by two participants , we use Krip - pendoﬀ’s Alpha [ 46 ] as our metric for the inter - rater agreement . This metric supports multi - label classiﬁcation by multiple participants . A similar ap - proach of obtaining inter - rater agreement in a multi - rater setting was used by Heng et al . [ 47 ] to manually tag logging data , and Salza et al . [ 48 ] to classify mobile app updates . Across the three coding activities ( 78 and 300 commits for Team - A , and 207 PRs for Team - B ) , the teams reported high agreements with a Krippendoﬀ’s α = 98 % on the sample of 378 downstream changes and a Krippendoﬀ’s α = 92 % for the sample of 539 upstream changes . These values of inter - rater agreements are high and reﬂect the statistical robustness of our data labeling results . Given that the ﬁnal change taxonomy spans 39 change sub - categories , and that we coded 378 downstream commits and 207 upstream PRs ( containing 539 commits ) across two teams of seven coders ( and two universities ) , the resulting empirical study was non - trivial . For instance , in the downstream change 11 performed by fork “BoseAslCohort” for the project “Youtube - 8m” , the authors manually inspected changes for 27 changed ﬁles , which included 11 , 755 code additions . Overall , it took an estimated six man - months to ﬁnish the qualitative study . 3 . 4 Replication Package All the scripts along with the mined data are provided in the replication pack - age 12 11 https : / / github . com / BoseAslCohort / youtube - 8m / commit / c1b01315bafc24e83248cd862a9324bb21d4d52d 12 Tentative g - drive ( https : / / drive . google . com / drive / folders / 1LfQhc5pQVj58wFlXUCzgjxuK _ uVRb1W3 ? usp = sharing ) will be replaced by a GitHub repository after publication . 14 Bhatia et . al . 50 % 60 % 70 % 80 % 90 % 100 % Percentage of Repositories 0 % 20 % 40 % 60 % 80 % 100 % P e r c e n t a g e o f N o n - T r i v i a l F o r k s Fig . 3 : Percentage of Non - Trivial Forks across studied repositories . 52 % repositories do not have any fork modiﬁcation to the forked source code . 4 Case Studies 4 . 1 RQ1 : To what extent do ML research repositories form the basis of other contributors’ work ? Motivation . Currently , there is no empirical evidence regarding the extent to which 1 ) open - sourcing ML research code helps the OSS community in building new applications and 2 ) the OSS community contributes and helps maintain the original ML research implementations . In contrast , for non - ML software , prior research [ 15 , 18 , 33 , 35 , 36 ] has studied the nature of multi - repository de - velopment and maintenance of OSS projects . Hence , in this RQ , we analyze the OSS development activities around research - based ML pipeline repositories . Approach . As discussed in Section 3 , we extract 1 , 346 GitHub repositories having references to machine learning ArXiv publications , then use the GitHub Search API 13 to obtain the metrics identiﬁed in Table 1 . Results . 4 . 1 . 1 Non - Trivial Forks Only 9 % of forks of the ML research repositories have modiﬁcations to the forked source code ( i . e . , have Non - Trivial Forks ) Figure 3 shows 13 https : / / docs . github . com / en / rest / reference / repos Towards a Change Taxonomy for Machine Learning Systems 15 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 # Forks ( Log Base 10 ) 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 # S t a r s ( L o g B a s e 10 ) 0 10 20 30 40 50 R e p o s i t o r y C o un t Fig . 4 : Hexbin for Star # ( logged ) and Fork # ( logged ) indicates high correla - tion . Popularity can be indicated by either of the metrics . the cumulative percentage of repositories having Non - Trivial Forks . Since 51 . 6 % of the repositories do not have any fork modiﬁcation , and the slope of the curve is gentle and linear until 90 % , only a low number of ML research repositories have Non - Trivial Forks . 4 . 1 . 2 Popularity of ML research repositories ML research repositories have a high median star # of 22 and fork # of 8 . These numbers stand in stark comparison to the datasets used by prior research for non - ML repositories . We observed a median of zero stars and forks for the replication dataset provided by Brisson et al . [ 33 ] , consisting of 13 , 431 projects . The comparisons of star # and forks # for Brisson’s dataset with our study are statistically signiﬁcant with Wilcoxon Rank sum p − value < 0 . 01 Star # and fork # are highly correlated ( Spearman ρ = 0 . 94 ) as shown by the data distribution in Figure 4 . 20 % of the repositories have less than ﬁve stars and ﬁve forks as indicated by the darker color at the low end of star # and fork # in Figure 4 . These results again contrast to the low correlation of 0 . 45 found by Brisson et al . on their non - ML dataset of 13 , 431 repositories , suggesting a much weaker connection between stars and forks . Several hypotheses might explain this contradiction , and require future work to be validated . For example , due to the current hype of AI technologies , ML repositories might be substantially 16 Bhatia et . al . First Fork Final Fork 0 200 400 600 800 1000 1200 T i m e i n D a y s Fig . 5 : The ﬁrst forking time represents the speed of the open source com - munity in adapting ML research repositories , whereas the ﬁnal forking time represents the longevity of adaptations . more popular than non - ML repositories . It could also be that , due to the quick succession of new AI algorithms , the OSS community uses forks for the purpose of “bookmarking” or keeping copies of interesting ML research implementations [ 49 ] . One indication of the latter hypothesis could be the high percentage ( 91 % ) of forks without any code change ( i . e . , trivial forks ) that we found earlier . 4 . 1 . 3 Speed and Longevity of Forking Forks on ML repositories appear as fast as the 11 th day ( median 11 . 5 ) , while fork - based collaboration sustains a median of 2 . 6 years . Figure 5 shows the distribution of the time of ﬁrst and ﬁnal ( at the time of analysis ) fork for each repository in our dataset . A median ML research repository receives its ﬁrst forks on the 11 th day after creation date , while an ML research repository is forked till a median of 2 . 6 years of the creation of the repository . Although the final fork time may be impacted by the time of our analysis , nevertheless , a value of 2 . 6 years deﬁnitely shows that the ML repositories are not just data dumps but can foster online collaboration . Towards a Change Taxonomy for Machine Learning Systems 17 4 . 1 . 4 Transitive Forking 24 % of ML research repositories have transitive forks ( i . e . , fork repositories with their own forks ) . We call the direct forks of a repos - itory level - one transitivity , while a transitivity of level - two indicates the forks of the level - one transitive forks , and so on . We observed 1 , 581 , 44 , and 7 cases of level - one , level - two , and level - three forking transitivity for 226 , 28 , and 3 ML research repositories respectively . The ML research repository with the highest forking transitivity in our dataset includes the Autopilot - TensorFlow project 14 , which is an implementation of self - driving car research 15 . This project has 281 forks , 10 level - one , 3 level - two , and 5 level - three transitive forks . We compare our ﬁndings to the transitivity results reported by Brisson et al . [ 33 ] , who conducted an analysis of the March 2019 GHTorrent dataset and reported 385 level - one , 12171 level - two , 778 level - three , 84 level - four , 11 level - ﬁve , and 2 level - six forks . A χ 2 test of independence between these ﬁndings and ours , yielded a p − value < 0 . 01 , representing a statistically signiﬁcant diﬀerence in data distribution between ML and non - ML repositories . 4 . 1 . 5 Upstream Contribution 41 . 6 % of Non - Trivial Forks send changes back to the original repos - itories in the form of upstream contributions to the ML research repository . A total of 607 pull requests were submitted upstream by Non - Trivial Forks , out of which 316 were merged into the original repositories . This re - sulted in 52 . 1 % acceptance . This value is slightly lower than that of a recent study on NPM packages by Dey and Mockus [ 20 ] , who reported a PR accep - tance rate of 60 % . 27 . 5 % of the upstream PRs were submitted on the same day as that of the creation of the fork . In particular , a fork takes a median of 22 hours to submit a PR . After receiving a PR , the parent ML research repository takes a median of seven hours to review the upstream changes before deciding on them , as shown by the violin plots in Figure 6 . This is approximately four times faster than the median PR acceptance times ( 27 . 7 hours ) for NPM packages on GitHub [ 20 ] . Another study performed on 1 . 9 million PRs on GitHub in 2013 by Gousios et al . [ 21 ] reported a median of seven days to merge a PR . 14 https : / / github . com / SullyChen / Autopilot - TensorFlow 15 https : / / arxiv . org / pdf / 1604 . 07316 . pdf 18 Bhatia et . al . PR CreationTime PR MergingTime 0 1000 2000 3000 4000 5000 6000 7000 8000 H o u r s Fig . 6 : Time spent in sending PRs ( left ) and merging the PRs into the upstream parent repository ( right ) . : Summary of RQ1 The OSS community forks the ML research as fast as a median of 11 . 5 days after the creation of the repository , and forking continues until a median of 2 . 6 years . ML research repositories are heavily and transi - tively forked , yet only 9 % of the forks are non - trivial . Of those , 43 % sent upstream changes back to the parent ML research repositories , with a 52 % acceptance rate . PR merge times of ML research reposito - ries are faster than the values reported by prior research for non - ML repositories . 4 . 2 RQ2 : What are the types of changes in ML research repositories ? Motivation . Since Hindle et al . ’s taxonomy of changes focused on traditional software systems known in 2008 [ 19 ] , this research question performs a qualitative anal - ysis to identify the types of changes made in ML research repositories , possibly extending Hindle’s change taxonomy . Through this , we wish to help software practitioners in building and maintaining ML software , which not only involves code changes but also changes to many other kinds of artifacts ( e . g . , dataset and models ) [ 3 ] . Furthermore , training / education teams need an understand - Towards a Change Taxonomy for Machine Learning Systems 19 Implementation Platform Feature Maintenance Parameter Tuning Performance Pre - processing Model Training Cross cutting concern Debug Maintenance Bug Fix DependencyManagement Remove package Update Package Add Package Data Input Data Output Data Program Data Meta Program Sharing Change Evaluation Comprehension Internationalization Testing Build / Config Documentation Legal License License Non Functional Code Changes Token replace Indent Refactor Cleanup Type of Changes in ML and Traditional Software Module Management Add autogenerated Code Split Remove Move Add Source Management File Permission Branch Versioning External Source Control Merge Fig . 7 : Enhanced version of Hindle’s change taxonomy . The bolded change ( sub - ) categories were identiﬁed by this study . ing of the types of code changes to better equip students and novice developers in supporting ML applications . Approach . In RQ1 , we observed that 52 % of the PRs sent by Non - trivial forks are merged into the parent ML research repository . In this research question , we qualitatively analyze 1 ) the types of changes that were merged with the parent repositories , which we call upstream changes ; and 2 ) the types of changes that were performed within the forked repositories , but not pushed to the parent repository , which we call downstream changes . Using the sampling and coding approach discussed in Section 3 . 3 , the coders of team A and Team B validated and enhanced Hindle et al . ’s taxonomy [ 19 ] . This sec - tion reports on the new change ( sub - ) categories identiﬁed in the analyzed code changes of ML repositories . Results . Hindle et al . ’s change taxonomy [ 19 ] was extended with two new categories and 15 new sub - categories of changes . Only one of the two new high - level change categories was ML - speciﬁc , i . e . , Data , while the other one , i . e . , Dependency Management , represents an update to the orig - inal taxonomy related to modern library dependency management activities ( which were less relevant 13 years ago ) . A graphical summary of the extended taxonomy of change ( sub - ) categories is provided in Figure 7 . In the subsections below , we brieﬂy describe and illustrate each new ( sub - ) category and how it complements the existing taxonomy : 4 . 2 . 1 Maintenance Category Code changes performing software maintenance activities . We identiﬁed four new maintenance change sub - categories in the context of machine learning . – Parameter Tuning . [ 50 ] Changes made to parameter values for tweaking the ML pipeline . 20 Bhatia et . al . – Model Training : [ 51 ] Structural change to the source code responsible for training the machine learning model . For instance , adding dropout layers , or changing epochs . Such changes diﬀer from Parameter Tuning as they are performed at a structural level , rather than a simple change of ( hyper - ) parameter variables . – Performance : [ 52 ] Any change pertaining to the eﬃciency of the ML model training process . For instance , re - writing ML operations in Tensorﬂow for better computation eﬃciency , reducing execution times . – Pre - processing : [ 53 ] Source code changes related to manipulation , cleaning , and ﬁltering of data before feeding it to the model . 4 . 2 . 2 Meta Program As identiﬁed by Hindle et al . , Meta program changes update the metadata of the program ( i . e . , data required by the project , but not the source code ) . For instance , makeﬁles , readme ( documentation ) ﬁles . We identiﬁed three new change sub - categories . – Sharing . [ 54 ] Sharing the source code for better presentation and man - agement . For instance , converting python scripts into Jupyter notebooks , which are better suited for sharing data science processing and results , or sharing the dependency environment via Docker containers . – Change evaluation . [ 55 ] This type of change adds code to run and test changes made to the machine learning model , typically in the form of a driver script that trains a model on a given data set , then outputs the resulting model’s predictions on a test set . – Comprehension [ 56 ] Code changes clarifying the role and / or implementa - tion of parts of the code , for example using code comments or by adding a succession of print statements . Such code changes diﬀer from documen - tation since the former changes are meant for self - understanding , whereas the latter explicitly document a project for end - users , for example using readme ﬁles . 4 . 2 . 3 Module management As identiﬁed by prior research , module management changes the way ﬁles are named and organized into source code modules . In addition to Hindle’s sub - categories ( i . e . , add , rename and delete module ) , we identiﬁed a new change sub - category , Adding auto - generated code [ 57 ] for adding new ﬁles that are generated automatically by external tools , alternative IDEs or varying environment conﬁgurations . 4 . 2 . 4 Data Category [ NEW ] Any change to the source code that handles entering / exiting / manipulating external data of the project . This category is clearly ML - speciﬁc . Towards a Change Taxonomy for Machine Learning Systems 21 – Output Data . [ 58 ] Changing the way the output data of the ML program is saved . For example , changing the dimensions of images to be saved as the output of an image processing application . – Input Data . [ 59 ] Code changes for data reading or ingesting external data into the project . For example , adding code for accommodating new images for testing the ML model . – Program Data . [ 60 ] Changing the metadata used by a project , not the actual external data ( or its characteristics ) used by the ML model . For example , changing base directories / absolute paths or other project conﬁg - urations in the source code . 4 . 2 . 5 Source Management Hindle et al . described Source management as changes performed due to the way a version control system is being used by a project . Along with the ﬁve sub - categories identiﬁed by Hindle et al . , we identiﬁed Changing ﬁle permis - sions [ 61 ] for cases where access permissions ( e . g . , executability of a script ) were added / updated / removed . 4 . 2 . 6 Dependency Management [ NEW ] We identiﬁed this new category related to handling packages ( dependencies ) re - quired by the project . In particular , we identiﬁed three sub - categories , namely , Add package [ 62 ] , Remove package [ 63 ] , and Update package [ 64 ] . : Summary of RQ2 Hindle et al . ’s taxonomy of software code changes [ 19 ] had to be ex - tended with two high - level change categories ( ML - speciﬁc Data , and generic Dependency management ) . We also extended the taxonomy by identifying 15 new sub - categories of changes , nine of which ( i . e . , input data , output data , program data , sharing , change evaluation , parameter tuning , performance , pre - processing , model training ) are ML - speciﬁc . 4 . 3 RQ3 : How do downstream changes diﬀer from upstream changes in ML research repositories ? Motivation . In RQ1 , we observed that 41 . 6 % of Non - trivial forks sub - mit upstream contributions back to the ML research repositories . Since this means that contributions by more than half of the forks were never sent up - stream , it is interesting to understand the nature of such contributions , i . e . , what did the ML community need in addition to the original development in the parent repository ( merged PRs ) , and what did the authors of the ML repository miss out on ( code changes not contributed back ) ? In particular , the upstream changes studied in this paper help to identify the missing aspects 22 Bhatia et . al . L i c e n s e R e m o v e P a c k a g e U pd a t e P a c k a g e A dd P a c k a g e V e r s i o n i n g F il e p e r m i ss i o n S o u r c e C o n t r o l M e r g e P r o g r a m D a t a I n p u t D a t a O u t p u t D a t a A dd R e g e n e r a t e d M o d u l e R e m o v e M o d u l e M o v e M o d u l e A dd M o d u l e P l a t f o r m F e a t u r e T o k e n R e p l a c e I n d e n t R e f a c t o r C l e a nu p I n t e r n a t i o n a li z a t i o n S h a r i n g C h a n g e E v a l u a t i o n T e s t i n g C o m p r e h e n s i o n B u il d / C o n f i g D o c u m e n t a t i o n C r o ss C u tt i n g C o n c e r n M o d e l p e r f o r m a n c e D e b u g P r e - p r o c e ss i n g M a i n t e n a n c e P a r a m T un i n g M o d e l T r a i n i n g B u g F i x 15 . 0 % 10 . 0 % 5 . 0 % 0 . 0 % 5 . 0 % 10 . 0 % 15 . 0 % 20 . 0 % 25 . 0 % P e r c e n t o f s a m p l e s Implementation Maintenance Module Mgmnt . Dep . Mgmnt . Non Fsc . Scs . Mgmnt . Meta Data Misc . Legal . Fig . 8 : Percentage of change sub - categories present in the 378 samples of Downstream commits and 539 samples of Upstream commits . Values above y = 0 . 0 % ( plotted using lighter color palette ) indicate the percentage of up - stream changes containing a speciﬁc change sub - category , whereas values below y = 0 . 0 % ( darker color palette ) indicate the same for downstream changes . of the original ML parent repository contributed back by the OSS commu - nity . Conversely , understanding the downstream changes studied in this paper helps us determine to what extent essential features or contributions have been missed . Approach . This RQ uses the sample of 378 downstream changes and 539 upstream changes labeled with high inter - rater agreements in RQ2 , but this time to analyze the prevalence of each change sub - category of the taxonomy in Figure 7 . In particular , we compute the percentage of upstream and down - stream changes for each ( sub - ) category , then compare our ﬁndings between downstream and upstream changes . These results are summarized in Figure 8 . Results . For both upstream and downstream commits , heavy changes occur in Documentation ( Meta program ) , bug ﬁxes and Model train - ing ( Maintenance ) ; and adding new features ( implementation ) . Fig - ure 8 shows substantial peaks in the Maintenance and Meta program cat - egories . We attribute such results to the nature of data science life cycle , where ML pipelines require substantial maintenance activities during experi - mentation with and tweaking of data and models . ML tasks include multiple iterations of updating data pre - processing , tuning parameters , updating model building code , and evaluating model performance . Maintenance changes are much more prevalent in downstream com - Towards a Change Taxonomy for Machine Learning Systems 23 mits than in upstream commits . This is visible through the higher percent - ages of non functional and data changes in downstream commits . We attribute this imbalance to downstream users adapting ML research for their domain - speciﬁc tasks , rather than improving the upstream repository for generic us - age . In order to improve personal understanding of the ML code , downstream users added changes from the new change sub - category , comprehension , such as added print statements . In contrast , we found no cases of comprehension in upstream commits . We also observed nine cases of our new sub - category change evaluation for downstream changes , while none for upstream commits . Intuitively , down - stream developers needed scripts to train and test models ( potentially after making some other changes ) against their domain - speciﬁc data . Conversely , there were 45 cases of our new change sub - category , Dependency Manage - ment , in upstream commits . Such changes add , update or remove ML library dependencies . Finally , accepted PRs were merged into either a project’s main branch or alternative branches , which led to more instances of merging changes ( Source Management ) in upstream commits than in downstream commits . : Summary of RQ3 Both upstream and downstream contributions to ML research repos - itory add documentation , ﬁx bugs and add features . Downstream de - velopers change input / output data , perform parameter tuning , add new functional features , and perform other non - functional changes like in - dentation , refactoring , or cleaning up the source code . Such changes are domain oriented . On the other hand , upstream changes beneﬁt the parent repository by updating packages or ﬁxing bugs for the parent repository . 5 Implications In this section , we discuss the implications of our ﬁndings for ML practitioners , the OSS ML community , software researchers , ML educators , and toolsmiths . Organizations and / or individuals wishing to open - source their ML repositories should have realistic assumptions . While our ﬁndings show that researchers do not necessarily “dump” their ML research code on GitHub , but receive and merge open - source contributions to maintain their repository , this is not guaranteed . For one , only 9 % of forks are Non - trivial forks , of which 41 . 6 % send contributions upstream via a pull request , about half of which ( 52 . 1 % ) are accepted into the parent repository ( see RQ1 ) . Two lessons can be learned from this . On the one hand , the ML research repositories are missing out on almost 60 % of forks having contributions that are never sent back upstream . Even the 41 . 6 % of forks that do contribute might not contribute all contributions they have made . While it is OK for changes like parameter tuning not to be contributed back , Table 3 shows that 24 Bhatia et . al . Table 3 : Top - 10 most common change sub - categories within our sample of 387 downstream commits , extracted from Fig . 8 . Sub - category Distribution ( % ) Documentation 15 . 4 Parameter Tuning 14 . 4 Feature 13 . 6 Bug ﬁx 11 . 4 Cleaning Up 11 . 2 Model Training 10 . 9 build / conﬁg 9 . 8 Add Module 9 . 8 Maintenance 9 . 3 Refactoring 8 . 8 Versioning 0 . 3 the “lost” contributions of forks also include ( amongst others ) 15 . 4 % of Docu - mentation changes , 13 . 6 % new Features for the pipeline , and 11 . 4 % new Bug ﬁxes . Future work should look into why those were never sent back . One pos - sible hypothesis to validate is that ML research repositories might be forked by AI experts with less of a traditional software engineering background . On the other hand , of those contributions that were propagated back , only half were merged . Future work should consider the reasons for rejection of this work , i . e . , to what extent was rejection based on the quality of the contribu - tion versus the contribution being too tied to the contributor’s own use case , or even versus the responsiveness of the ML repository owners . Whichever the outcome , and similar to traditional open - source development , receiving many high - volume contributions requires eﬀort [ 35 ] . Researchers can use our ex - tended taxonomy to obtain a holistic picture of software changes in ML systems , while software educators may wish to update their curricula to revise future training of ( ML ) software engineers . In particular , the three new sub - categories related to Data changes , the two re - lated to Meta Program as well as the four related to Maintenance of ML code indicate a need for revising and adapting existing best practices towards the needs of software engineering for ML systems . This is only exacerbated by the prevalence of the Comprehension change sub - category , indicating diﬃculties of developers to understand ML code . As a side - eﬀect , we also identiﬁed six additional change sub - categories that are not ML - speciﬁc . These complement Hindle et al . ’s original change tax - onomy with change types related to new development activities that have appeared in the past 13 years , such as more elaborate library dependency management ( Remove / Update / Add Package ) . Future work could build on this extension of the taxonomy , even outside the scope of ML software engineer - ing . An updated taxonomy of code changes can help toolsmiths in adapting and innovating software engineering tools . As mentioned in Section 2 , code change data is used for a variety of purposes such as extrac - tion of missing traceability links [ 27 ] , auto - generation of commit messages [ 29 ] , Towards a Change Taxonomy for Machine Learning Systems 25 and analysis of quality impact [ 30 ] . At the same time , current development environments and tools used by developers need to be modernized as well . For example , given that many developers add comments to the code in order to better understand the ML logic , which might pollute the code base , perhaps less invasive annotation or other functionality is required in future IDEs . 6 Threats to Validity Threats to Construct Validity . Quantitative studies can be subject to researcher bias . To minimize this , we used multiple participants ( i . e . , two teams with four people in Team - A , and three people in Team - B ) . Both teams had in - depth knowledge of software development . Furthermore , the teams pair - wise labeled each sample and achieved high inter - rater agreements of Kripendoﬀ’s a = 98 % for Team - A and a = 92 % for Team - B . For answering our RQ2 , we analyzed both the types of changes made by PRs merged into upstream parent repository and by downstream changes performed within the forks but never sent upstream . However , we do not study changes rejected by the upstream repository . While future work should analyze such cases , we feel conﬁdent about the completeness of our taxonomy , as we reached saturation in obtaining new labels within the initial 78 samples of downstream commits . No new categories were found in the later part of 300 downstream or any of the 539 upstream commits . Threats to External Validity . In Section 3 , we mine Type II ML pipeline repositories inspired by researchers . For this , we check whether a repository cites an arXiv research paper in its README ﬁle . Irrespective of whether the repository is created by the authors of the ML research publication or by external members of the community , a repository citing an ML arXiv publication is inﬂuenced by the research and can be termed as a “ML research repository” . Moreover , we focused on the repositories implementing image processing or machine learning in ModelDepot , since they were the most popular on Mod - eldepot and cover a wide range of popular ML application domains . Future work should focus on other domains like NLP and Audio Processing . Further - more , a similar study to ours should be performed on the other two types of ML software systems in Fig . 1 , i . e . , ML frameworks and ML applications , as well as Type II projects that are not related to research papers . Threats to Reliability Validity takes into account the replication of our study . We provide all the details and necessary scripts to replicate this study . The replication data for qualitative analysis consists of the labeled sample of upstream and downstream changes . We also provide all lists of repositories , along with the mined forking data for quantitative analysis in RQ1 . 26 Bhatia et . al . 7 Conclusion Open - source community developers use and reﬁne OSS repositories . Although prior studies have investigated the nature of open source contributions in non - ML software , one can imagine the nature of such community changes , as well as the way in which developers collaborate , to be diﬀerent for Machine Learning software . Hence , this paper studies the forking dynamics and the types of changes performed in 57 , 369 forks of 1 , 346 ML pipeline projects related to research publications . We found that while most forks ( 91 % ) do not modify an ML research repository after forking it , 41 . 6 % of the forks with modiﬁcations contribute valuable changes to the parent ML research repository , with a 52 . 1 % accep - tance rate . We performed an extensive qualitative study that identiﬁed the types of changes in ML software . We identiﬁed one new top - level change cat - egory , Data , in the context of ML , and one more generic category ( depen - dency management ) . Along with this , we extend the taxonomy of changes by adding 15 new sub - categories , including nine ML - speciﬁc ones ( input data , output data , program data , sharing , change evaluation , parameter tuning , per - formance , pre - processing , model training ) and six generic ones ( i . e . adding package , removing package , updating package , ﬁle permissions , comprehension , adding auto - generated code ) . Our results aim to help software practitioners in having a better under - standing of ML changes that can be leveraged while training new developers , and to support building and maintaining ML software . Furthermore , future work should look deeper into the reasons why potentially valid Documenta - tion , Feature and Bug ﬁx changes were not contributed back upstream . Acknowledgement We thank Greg Wilson for providing insightful ideas and comments for this work . We also thank Boyuan Chen , Minke Xiu and Javier Rosales for their contributions to the analysis and feedback on this work . References 1 . S . Amershi , A . Begel , C . Bird , R . DeLine , H . Gall , E . Kamar , N . Nagappan , B . Nushi , and T . Zimmermann , “Software engineering for machine learning : A case study , ” in 2019 IEEE / ACM 41st International Conference on Software Engineering : Software Engineering in Practice ( ICSE - SEIP ) , 2019 , pp . 291 – 300 . 2 . N . Nahar , S . Zhou , G . Lewis , and C . K¨astner , “Collaboration challenges in building ml - enabled systems : Communication , documentation , engineering , and process , ” in 2022 IEEE / ACM 44th International Conference on Software Engineering ( ICSE ) , 2022 . 3 . S . Idowu , D . Str¨uber , and T . Berger , “Asset management in machine learning : A sur - vey , ” in 2021 IEEE / ACM 43rd International Conference on Software Engineering : Software Engineering in Practice ( ICSE - SEIP ) , 2021 , pp . 51 – 60 . 4 . D . Sato , A . Wider , and C . Windheuser , “Continuous delivery for machine learning , ” https : / / martinfowler . com / articles / cd4ml . html # DeploymentPipelines , 2019 . Towards a Change Taxonomy for Machine Learning Systems 27 5 . Y . Fan , X . Xia , D . Lo , A . E . Hassan , and S . Li , “What makes a popular academic AI repository ? ” Empirical Software Engineering , vol . 26 , no . 1 , pp . 1 – 35 , 2021 . 6 . “Reproducibility checklist , 36th conference on artiﬁcial intelligence ( aaai ) , ” https : / / aaai . org / Conferences / AAAI - 22 / reproducibility - checklist / , 2022 . 7 . “Neurips 2021 code and data submission guidelines , ” https : / / nips . cc / Conferences / 2021 / PaperInformation / CodeSubmissionPolicy , 2021 . 8 . J . Pineau , “The machine learning reproducibility checklist v2 . 0 , ” https : / / www . cs . mcgill . ca / ∼ jpineau / ReproducibilityChecklist . pdf , 2020 . 9 . “Icse 2022 open science policy , ” https : / / conf . researchr . org / track / icse - 2022 / icse - 2022 - papers # open - science - policy , 2022 . 10 . D . M´endez Fern´andez , M . Monperrus , R . Feldt , and T . Zimmermann , “The open science initiative of the empirical software engineering journal , ” Empirical Softw . Engg . , vol . 24 , no . 3 , p . 1057 – 1060 , jun 2019 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s10664 - 019 - 09712 - x 11 . “The ad hoc committee on open science and reproducibility , ” https : / / www . computer . org / volunteering / boards - and - committees / open - science - reproducibility , 2020 . 12 . H . Washizaki , H . Uchida , F . Khomh , and Y . - G . Gu´eh´eneuc , “Studying software engi - neering patterns for designing machine learning systems , ” in 2019 10th International Workshop on Empirical Software Engineering in Practice ( IWESEP ) . IEEE , 2019 , pp . 49 – 495 . 13 . I . Ozkaya , “What is really diﬀerent in engineering ai - enabled systems ? ” IEEE Software , vol . 37 , no . 4 , pp . 3 – 6 , 2020 . 14 . A . Lima , L . Rossi , and M . Musolesi , “Coding together at scale : Github as a collaborative social network , ” in Eighth international AAAI conference on weblogs and social media , 2014 . 15 . M . Biazzini and B . Baudry , ““may the fork be with you” : novel metrics to analyze collaboration on github , ” in Proceedings of the 5th international workshop on emerging trends in software metrics , 2014 , pp . 37 – 43 . 16 . Y . Hu , J . Zhang , X . Bai , S . Yu , and Z . Yang , “Inﬂuence analysis of github repositories , ” SpringerPlus , vol . 5 , no . 1 , pp . 1 – 19 , 2016 . 17 . S . Zhou , B . Vasilescu , and C . K¨astner , “How has forking changed in the last 20 years ? a study of hard forks on github , ” in 2020 IEEE / ACM 42nd International Conference on Software Engineering ( ICSE ) . IEEE , 2020 , pp . 445 – 456 . 18 . —— , “What the fork : a study of ineﬃcient and eﬃcient forking practices in social coding , ” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering , 2019 , pp . 350 – 361 . 19 . A . Hindle , D . M . German , and R . Holt , “What do large commits tell us ? a taxonomical study of large commits , ” in Proceedings of the 2008 International Working Conference on Mining Software Repositories , ser . MSR ’08 . New York , NY , USA : Association for Computing Machinery , 2008 , p . 99 – 108 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 1370750 . 1370773 20 . T . Dey and A . Mockus , “Which pull requests get accepted and why ? a study of popular npm packages , ” arXiv preprint arXiv : 2003 . 01153 , 2020 . 21 . G . Gousios , M . Pinzger , and A . v . Deursen , “An exploratory study of the pull - based software development model , ” in Proceedings of the 36th International Conference on Software Engineering , 2014 , pp . 345 – 355 . 22 . E . B . Swanson , “The dimensions of maintenance , ” in Proceedings of the 2nd interna - tional conference on Software engineering , 1976 , pp . 492 – 497 . 23 . A . Hindle , D . M . German , M . W . Godfrey , and R . C . Holt , “Automatic classication of large changes into maintenance categories , ” in 2009 IEEE 17th International Confer - ence on Program Comprehension . IEEE , 2009 , pp . 30 – 39 . 24 . M . Yan , Y . Fu , X . Zhang , D . Yang , L . Xu , and J . D . Kymer , “Automatically classifying software changes via discriminative topic model : Supporting multi - category and cross - project , ” Journal of Systems and Software , vol . 113 , pp . 296 – 308 , 2016 . 25 . L . Ghadhab , I . Jenhani , M . W . Mkaouer , and M . B . Messaoud , “Augmenting commit classiﬁcation by using ﬁne - grained source code changes and a pre - trained deep neural language model , ” Information and Software Technology , vol . 135 , p . 106566 , 2021 . 28 Bhatia et . al . 26 . H . C . Benestad , B . Anda , and E . Arisholm , “Understanding software maintenance and evolution by analyzing individual changes : a literature review , ” Journal of Software Maintenance and Evolution : Research and Practice , vol . 21 , no . 6 , pp . 349 – 378 , 2009 . 27 . R . Wu , H . Zhang , S . Kim , and S . - C . Cheung , “Relink : recovering links between bugs and changes , ” in Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering , 2011 , pp . 15 – 25 . 28 . T . F . Bissyand´e , F . Thung , S . Wang , D . Lo , L . Jiang , and L . R´eveill ` ere , “Empirical evaluation of bug linking , ” in 2013 17th European Conference on Software Maintenance and Reengineering , 2013 , pp . 89 – 98 . 29 . L . F . Cort´es - Coy , M . Linares - V´asquez , J . Aponte , and D . Poshyvanyk , “On automati - cally generating commit messages via summarization of source code changes , ” in 2014 IEEE 14th International Working Conference on Source Code Analysis and Manipu - lation , 2014 , pp . 275 – 284 . 30 . C . Farag´o , P . Heged˜us , and R . Ferenc , “The impact of version control operations on the quality change of the source code , ” in International Conference on Computational Science and Its Applications . Springer , 2014 , pp . 353 – 369 . 31 . K . Fogel , Producing open source software : How to run a successful free software project . ” O’Reilly Media , Inc . ” , 2005 . 32 . K . Constantino , S . Zhou , M . Souza , E . Figueiredo , and C . K¨astner , “Understanding collaborative software development : An interview study , ” in Proceedings of the 15th International Conference on Global Software Engineering , 2020 , pp . 55 – 65 . 33 . S . Brisson , E . Noei , and K . Lyons , “We are family : Analyzing communication in github software repositories and their forks , ” in 2020 IEEE 27th International Conference on Software Analysis , Evolution and Reengineering ( SANER ) . IEEE , 2020 , pp . 59 – 69 . 34 . L . Ren , S . Zhou , and C . K¨astner , “Poster : Forks insight : Providing an overview of github forks , ” in 2018 IEEE / ACM 40th International Conference on Software Engineering : Companion ( ICSE - Companion ) , 2018 , pp . 179 – 180 . 35 . M . M . Rahman and C . K . Roy , “An insight into the pull requests of github , ” in Pro - ceedings of the 11th Working Conference on Mining Software Repositories , 2014 , pp . 364 – 367 . 36 . X . Zhang , Y . Chen , Y . Gu , W . Zou , X . Xie , X . Jia , and J . Xuan , “How do multiple pull requests change the same code : A study of competing pull requests in github , ” in 2018 IEEE International Conference on Software Maintenance and Evolution ( IC - SME ) . IEEE , 2018 , pp . 228 – 239 . 37 . I . Ozkaya , “What is really diﬀerent in engineering ai - enabled systems ? ” IEEE Software , vol . 37 , no . 4 , pp . 3 – 6 , 2020 . 38 . S . Mart´ınez - Fern´andez , J . Bogner , X . Franch , M . Oriol , J . Siebert , A . Trendowicz , A . M . Vollmer , and S . Wagner , “Software engineering for ai - based systems : A survey , ” arXiv preprint arXiv : 2105 . 01984 , 2021 . 39 . T . Zhang , C . Gao , L . Ma , M . Lyu , and M . Kim , “An empirical study of common challenges in developing deep learning applications , ” in 2019 IEEE 30th International Symposium on Software Reliability Engineering ( ISSRE ) . IEEE , 2019 , pp . 104 – 115 . 40 . A . Arpteg , B . Brinne , L . Crnkovic - Friis , and J . Bosch , “Software engineering challenges of deep learning , ” in 2018 44th Euromicro Conference on Software Engineering and Advanced Applications ( SEAA ) . IEEE , 2018 , pp . 50 – 59 . 41 . N . Sambasivan , S . Kapania , H . Highﬁll , D . Akrong , P . Paritosh , and L . M . Aroyo , ““everyone wants to do the model work , not the data work” : Data cascades in high - stakes ai , ” in proceedings of the 2021 CHI Conference on Human Factors in Computing Systems , 2021 , pp . 1 – 15 . 42 . N . Polyzotis , S . Roy , S . E . Whang , and M . Zinkevich , “Data lifecycle challenges in production machine learning : a survey , ” ACM SIGMOD Record , vol . 47 , no . 2 , pp . 17 – 28 , 2018 . 43 . K . O’Leary and M . Uchida , “Common problems with creating machine learning pipelines from existing code , ” 2020 . 44 . D . M . German , B . Adams , and A . E . Hassan , “Continuously mining distributed ver - sion control systems : an empirical study of how linux uses git , ” Empirical Software Engineering , vol . 21 , no . 1 , pp . 260 – 299 , 2016 . Towards a Change Taxonomy for Machine Learning Systems 29 45 . H . Borges and M . T . Valente , “What’s in a github star ? understanding repository star - ring practices in a social coding platform , ” Journal of Systems and Software , vol . 146 , pp . 112 – 129 , 2018 . 46 . K . Krippendorﬀ , “Computing krippendorﬀ’s alpha - reliability , ” 2011 . 47 . H . Li , W . Shang , B . Adams , M . Sayagh , and A . E . Hassan , “A qualitative study of the beneﬁts and costs of logging from developers’ perspectives , ” IEEE Transactions on Software Engineering , 2020 . 48 . P . Salza , F . Palomba , D . Di Nucci , C . D’Uva , A . De Lucia , and F . Ferrucci , “Do develop - ers update third - party libraries in mobile apps ? ” in Proceedings of the 26th Conference on Program Comprehension , 2018 , pp . 255 – 265 . 49 . E . Kalliamvakou , G . Gousios , K . Blincoe , L . Singer , D . M . German , and D . Damian , “The promises and perils of mining github , ” in Proceedings of the 11th Working Conference on Mining Software Repositories , ser . MSR 2014 . New York , NY , USA : Association for Computing Machinery , 2014 , p . 92 – 101 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 2597073 . 2597074 50 . “Parameter tuning example , ” https : / / github . com / google / youtube - 8m / commit / 0e526caace96d3cf6f0686757d568f9ﬀba998b4 , 2017 . 51 . “Model tuning example , ” https : / / github . com / shikorab / tf - faster - rcnn / commit / 327778b2c4f297b307ﬀ0de552d2bfc47278e290 , 2018 . 52 . “Performance example , ” https : / / github . com / google / youtube - 8m / pull / 69 , 2018 . 53 . “Pre - processing example , ” https : / / github . com / lancele / Semantic - Segmentation - Suite / commit / d50b5c812392614fc2bdaf269921beb1f7086f63 , 2018 . 54 . “Sharing example , ” https : / / github . com / anishathalye / neural - style / pull / 40 , 2016 . 55 . “Change evaluation example , ” https : / / github . com / r - or / deep sort / blob / df7d69b2014aa3fad6cb526dca1c73831ed185db / ov test . py , 2019 . 56 . “Comprehension example , ” https : / / github . com / google / youtube - 8m / commit / 3439e33d81df8cd906987ee5889ebc937186114a , 2017 . 57 . “Adding auto - generated ﬁles example , ” https : / / github . com / ruqiang826 / TensorBox / commit / 8dee2634c679708c835d0dfcec57b7f1f0d87c8b , 2016 . 58 . “Output data example , ” https : / / github . com / Mappy / tf - faster - rcnn / commit / 51e0889fbdcd4c48f31def4c1cb05a5a4db04671 , 2018 . 59 . “Input data example , ” https : / / github . com / google / youtube - 8m / commit / 4619056162f466293d99e0c59512f8d0f3427fe2 , 2017 . 60 . “Program data example , ” https : / / github . com / Bruceeeee / facenet / commit / d9e6213cd8286334000ddf75529eba3662cef38a # diﬀ - dbc5c3b9f46e69236207956b34904d0dea62ﬀ866d442e97bb397ﬀ49a03a86b , 2017 . 61 . “Change ﬁle permission example , ” https : / / github . com / eewindﬂy / facenet / blob / 8dc38b504ee52ﬀ34fb9356dfdece85e96cb5162 / crop . sh , 2016 . 62 . “Adding package example , ” https : / / github . com / google / youtube - 8m / commit / 09774db80a515b667a91b14fe21a6134f3856c7a , 2019 . 63 . “Removing package example , ” https : / / github . com / yangcan2017 / text - detection - ctpn / commit / 30d3c59a59fb8926aea1ecb21c866240accc2a5a # diﬀ - 84fc656e2e2cfcf20b59e6e57f6bf2fdf2593ecﬀcb70d419225ec389b644857 , 2017 . 64 . “Update package example , ” https : / / github . com / google / youtube - 8m / commit / 72f42cd938d3cf4f928614a5fcdca237489e7c92 , 2018 .