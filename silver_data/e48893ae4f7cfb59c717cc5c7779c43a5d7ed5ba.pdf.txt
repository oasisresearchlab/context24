TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Abstract — An outlier is an observation that deviates so much from other observations that it seems to have been generated by a different mechanism . Outlier detection has many applications , such as data cleaning , fraud detection and network intrusion . The existence of outliers can indicate individuals or groups that exhibit a behavior that is very different from most of the individuals of the data set . Frequently , outliers are removed to improve accuracy of estimators , but sometimes , the presence of an outlier has a certain meaning , which explanation can be lost if the outlier is deleted . In this paper we study the effect of the presence of outliers on the performance of three well - known classifiers based on the results observed on four real world datasets . We use detection of outliers based on robust statistical estimators of the center and the covariance matrix for the Mahalanobis distance , detection of outliers based on clustering using the partitioning around medoids ( PAM ) algorithm , and two data mining techniques to detect outliers : Bay’s algorithm for distance - based outliers , and the LOF , a density - based local outlier algorithm . Index Terms —Anomaly detection , Distance based outliers , Outlier Detection , Supervised classification , I . INTRODUCTION ACCORDING to Hawkins [ 11 ] , “An outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism” . Almost all the studies that consider outlier identification as their primary objective are in the field of statistics . A comprehensive treatment of outliers from the statistical point of view appears in Barnet and Lewis [ 4 ] . They provide a list of about 100 discordancy tests for detecting outliers in data following well known distributions . The choice of an appropriate discordancy test depends on : a ) the distribution , b ) the knowledge of the distribution parameters , c ) the number of expected outliers , and d ) the type of expected outliers . These methods have two main drawbacks . First , almost all of them are exclusively for univariate data , making them unsuitable for multidimensional datasets . Second , all of them are distribution - based , and most of the time the data distribution is unknown . However , real - world datasets are commonly multivariate with unknown distribution . Manuscript received November 20 , 2004 . This work was supported in part by the Office of Naval Research U . S . under Grant N00014 - 00 - 1 - 0360 and by the NSF under Grant EIA 99 - 77071 . E . Acuna is with the Mathematics Department , University of Puerto Rico at Mayaguez , Mayaguez , PR 00680 Puerto Rico ( phone : 787 - 832 - 4040 ; fax : 787 - 265 - 5454 ; e - mail : edgar @ cs . uprm . edu ) . C . Rodriguez is with the Mathematics Department , University of Puerto Rico at Mayaguez , Mayaguez , PR 00680 Puerto Rico ( e - mail : caroline @ math . uprm . edu ) . An empirical study of the effect of outliers on the misclassification error rate Edgar Acuña and Caroline Rodríguez TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2 Penny and Jolliffe [ 20 ] compared six statistical techniques for outlier detection . The comparison is carried out on simulated datasets and it is based on the proportion , P n , of occasions on which exactly the correct identification of inserted outliers is made . They conclude that it is desirable to run several outlier detection methods on a given dataset to identify possible outliers . Hodge and Austin [ 12 ] , and Markou and Singh [ 16 ] [ 17 ] presented a large survey on outlier detection techniques with emphasis on neural networks based methods , where outlier detection is best known as novelty detection . They did not carry out comparative experiments . Detecting outliers , instances in a dataset with unusual properties , is an important data mining task . People in the data mining community became interested in outliers after Knorr and Ng [ 14 ] proposed a non - parametric approach to outlier detection based on the distance of an instance to its nearest neighbors . Outliers can arise due to many reasons , among them : mechanical faults , changes in system behavior , fraudulent behavior , human error , instrumentation error , or simply through natural deviation from a standard situation . Due to this , outlier detection has applications in areas such as : fraud detection , network intrusion , and data cleaning . Frequently , outliers are removed to improve accuracy of the estimators . However , this practice is not recommendable because outliers can sometimes provide very useful information . Section 2 of this paper includes a brief discussion of the treatment of outliers for univariate data . Section 3 focuses on methods for detection of multivariate outliers . Four methods of outlier detection are considered : a method based on robust estimation of the Mahalanobis distance , a method based on the PAM algorithm for clustering , a distance - based method and a density - based method . Section 4 of this paper covers the effect and treatment of outliers in supervised classification . The experimental results appear in section 5 , and the conclusions of our work are presented in section 6 . II . U NIVARIATE O UTLIERS Given a data set of n observations of a variable x , let x be the mean and let s be standard deviation of the data distribution . One observation is declared as an outlier if it lies outside of the interval ) , ( cs x cs x + − , ( 1 ) where the value of c is usually taken as 2 or 3 . The justification of these values relies on the fact that assuming normal distribution one expects to have a 95 . 45 % ( 99 . 75 % , respectively ) percent of the data on the interval centered in the mean with a semi - length equal to two ( three , respectively ) standard deviations . From ( 1 ) , the observation x is considered an outlier if c s x x > − | | . ( 2 ) The problem with the above criteria is that it assumes normal distribution of the data something that frequently does not occur . Furthermore , the mean and standard deviation are highly sensitive to outliers . John Tukey [ 29 ] introduced several methods for exploratory data analysis , one of them was the Boxplot . The Boxplot is a graphical display where the outliers appear tagged . Two types of outliers are distinguished : mild outliers and extreme outliers . An observation x is declared an extreme outlier if it lies outside of the interval ( Q 1 - 3 × IQR , Q 3 + 3 × IQR ) . Notice that the center of the interval is ( Q 1 + Q 3 ) / 2 and its radius is 3 . 5 × IQR , where IQR = Q 3 - Q 1 is called the Interquartile Range and can be considered a robust estimator of variability which can replace s in ( 2 ) . On the TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 3 other hand , ( Q 1 + Q 3 ) / 2 is a robust estimator of the center that can be used instead of x in ( 1 ) . An observation x is declared a mild outlier if it lies outside of the interval ( Q 1 - 1 . 5 × IQR , Q 3 + 1 . 5 × IQR ) . The numbers 1 . 5 and 3 are chosen by comparison with a normal distribution . All major statistical software applications include Boxplots among their graphical displays . Let us consider the very well known Iris dataset , which has 150 instances , four features and three classes . Fig . 1 shows the outliers detected in the class 1 ( setosa ) through the boxplots of their features . V1 V2 V3 V4 0 1 2 3 4 5 6 23 24 44 Fig 1 . Boxplots of the four features in class 1 of the Iris dataset showing the three outliers detected . Using the same graphical display we detect as outliers the instance number 99 in the second class , which has an abnormal value in the third feature , and the instance 107 that has an abnormal value in the first feature of class 3 . III . M ULTIVARIATE O UTLIERS Let us consider a dataset D with p features and n instances . In a supervised classification context , we must also know the classes to which each of the instances belongs . It is very common to include the classes as the last column of the data matrix . The objective is to detect all the instances that seem to be unusual , these will be the multivariate outliers . One might think that multivariate outliers can be detected based on the univariate outliers in each feature , but as shown in Fig . 2 this is not true . The instance appearing in the upper right corner is a multivariate outlier but it is not an outlier in each feature . On the other hand , an instance can have values that are outliers in several features but , when considered as a whole , the instance might not be a multivariate outlier . There are several methods for detecting multivariate outliers [ 12 ] , [ 16 ] [ 17 ] . The methods discussed in this paper are : robust statistical - based outlier detection , outlier detection by clustering , distance - based outlier detection and density - based local outlier detection . The before mentioned methods are discussed in the next sections . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 4 A . Robust Statistical based outlier detection Let x be an observation of a multivariate data set consisting of n observations and p features . Let x be the centroid of the dataset , which is a p - dimensional vector with the means of each feature as its components . Let X be the matrix of the original dataset with columns centered by their means . Then , the p × p matrix S = 1 / ( n - 1 ) X’X represents the covariance matrix of the p features . The multivariate version of ( 2 ) is c D > − − = − ) x ( x S ) x ( x x x , 1 ' ) ( 2 , ( 3 ) where D 2 is called the Mahalanobis square distance from x to the centroid of the dataset . An observation with a large Mahalanobis distance can be considered as an outlier . Assuming that the data follows a multivariate normal distribution , it can be proved that the distribution of the Mahalanobis distance behaves as a Chi - Square distribution for a large number of instances . Therefore the proposed cutoff point in ( 3 ) is given by c = 2 ) 1 , ( α χ − p , where χ 2 stands for the Chi - Square distribution and α is a signification level usually taken as . 05 . 5 10 15 20 25 5 10 15 20 x y Fig . 2 . Example of a bidimensional outlier that is not an outlier in either of its projections . A basic method for detecting multivariate outliers is to observe the outliers that appear in the boxplot of the distribution of the Mahalanobis square distance of the all instances . Looking at Fig . 3 we notice that five outliers ( instances 119 , 132 , 118 , 101 , and 135 ) are detected in class 3 , one outlier ( instance 69 ) is detected in class 2 , and two outliers ( instances 42 and 44 ) are detected in class 1 of the Iris dataset . People in the data mining community prefer to rank the instances using an outlyingness measure rather than classify the instance as an outlier or a non - outliers . Rocke and Woodruff [ 23 ] stated that the Mahalanobis distance works well identifying scattered outliers . However , in data with clustered outliers the Mahalanobis distance measure does not perform well detecting outliers . Data sets with multiple outliers or clusters of outliers are subject to the masking and swamping effects [ 27 ] . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 5 Masking effect . It is said that an outlier masks a second one that is close by if the latter can be considered an outlier by itself , but not if it is considered along with the first one . 1 2 3 0 2 4 6 8 10 12 14 4244 69 119 132 118101 135 Fig . 3 . Outliers in the three classes of the Iris dataset detected by the boxplots of the Mahalanobis square distances Equivalently after the deletion of one outlier , the other instance may emerge as an outlier . Masking occurs when a group of outlying points skews the mean and covariance estimates toward it , and the resulting distance of the outlying point from the mean is small . Swamping effect . It is said that an outlier swamps another instance if the latter can be considered outlier only under the presence of the first one . In other words after the deletion of one outlier , the other outlier may become a “good” instance . Swamping occurs when a group of outlying instances skews the mean and covariance estimates toward it and away from other “good” instances , and the resulting distance from these “good” points to the mean is large , making them look like outliers . For instance , consider the data set due to Hawkins , Bradu , and Kass that appears in [ 26 ] consisting of 75 instances and 3 features , where the first fourteen instances had been contaminated to be outliers . Using the Mahalanobis distance only , observation 14 is detected as an outlier as shown in Fig . 4 . The remaining thirteen outliers appear to be masked . The masking and swamping problem can be solved by using robust estimates of the centroid ( location ) and the covariance matrix ( shape ) , which by definition , are affected less by outliers . Outlying points are less likely to enter into the calculation of the robust statistics , so they will not be able to influence the parameter estimates used in the Mahalanobis distance . Among the robust estimators of the centroid and the covariance matrix are the minimum covariance determinant ( MCD ) and the minimum volume ellipsoid ( MVE ) both of them introduced by Rousseeuw [ 25 ] . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 6 0 20 40 60 1 2 3 4 5 6 Index M aha l anob i s D i s t an c e 14 12 13 Fig . 4 . The Masking effect of multivariate outliers in the Hawkins data set The Minimum Volume Ellipsoid ( MVE ) estimator is the center and the covariance of a subsample size h ( h ≤ n ) that minimizes the volume of the ellipsoid defined by the covariance matrix of the subsample . Formally , MVE = ( * * , J J S x ) , ( 4 ) where J = { set of h instances : * * ( ) ( ) J K Vol S Vol S ≤ for all K s . t . # ( K ) = h } . The value of h can be thought of as the minimum number of instances which must not be outlying and is usually h = [ ( n + p + 1 ) / 2 ] , where [ . ] is the greatest integer function . That is , h is the greatest integer less than or equal to ( n + p + 1 ) / 2 . On the other hand the Minimum Covariance Determinant ( MCD ) estimator is the center and the covariance of a subsample of size h ( h ≤ n ) that minimizes the determinant of the covariance matrix associate to the subsample . Formally , MCD = ( * * , J J S x ) , ( 5 ) where J = { set of h instances : * * | | | | J K S S ≤ for all K s . t . # ( K ) = h } . As before , it is common to take h = [ ( n + p + 1 ) / 2 ] . The MCD estimator underestimates the scale of the covariance matrix , so the robust distances are slightly too large , and too many instances tend to be nominated as outliers . A scale - correction has been implemented , and it seems to work well . The algorithms to compute the MVE and MCD estimators are based on combinatorial arguments ( for more details see [ 26 ] ) . Taking in account their statistical and computational efficiency , the MCD is preferred over the MVE . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 7 In this paper both estimators , MVE and MCD , have been computed using the function cov . rob available in the package lqs of R ( http : / / cran . r - project . org ) . This function uses the best algorithms available so far to compute both estimators [ 28 ] . Replacing the classical estimators of the center and the covariance in the usual Mahalanobis distance in ( 3 ) , by either the MVE or MCD estimator , outlying instances will not skew the estimates and can be identified as outliers by large values of the Mahalanobis distance . The most common cutoff point k is again the one based in a Chi - Square distribution , although Hardin and Rocke [ 10 ] propose a cutoff point based on the F distribution that they claim to be a better one . Other methods for estimating robust covariances can be found in [ 2 ] , [ 3 ] , and [ 9 ] . In this paper , two strategies to detect outliers using robust estimators of the Mahalanobis distances have been used : First , choose a given number of instances appearing at the top of a ranking based on their robust Mahalanobis measure . Second , choose as a multivariate outlier the instances that are tagged as upper outliers in the Boxplot of the distribution of these robust Mahalanobis distances . In order to identify the multivariate outliers in each of the classes of the Iris dataset through boxplots of the distribution of the robust version of the Mahalanobis distance , we consider 10 repetitions of each algorithm obtaining the results shown in Tables I and II . Notice that both methods detect two outliers in the first class , but the MVE method detects the instance 42 as a second outlier whereas the MCD method detects the instance 24 . All the remaining outliers detected by both methods are the same . Fig . 5 shows a plot of the ranking of the instances in class 3 of the Iris dataset by their robust Mahalanobis distance using the MVE estimator . Fig . 6 shows a plot of the ranking of the instances in class 3 of Iris by their robust Mahalanobis distance using the MCD estimator . According to Rocke and Woodruff [ 22 ] robust methods work well detecting scattered outliers but fail to detect clustered outliers . For this type of outlier , it is better to use a clustering algorithm as will be discussed in the next section . TABLE I TOP OUTLIERS PER CLASS IN THE IRIS DATASET BY FREQUENCY AND THE OUTLYINGNESS MEASURE USING THE MVE ESTIMATOR Instance Class Frequency Outlyingness 44 1 8 5 . 771107 42 1 8 5 . 703519 69 2 9 5 . 789996 119 3 8 5 . 246318 132 3 6 4 . 646023 TABLE II TOP OUTLIERS PER CLASS IN THE IRIS DATASET BY FREQUENCY AND THE OUTLYINGNESS MEASURE USING THE MCD ESTIMATOR Instance Class Frequency Outlyingness 44 1 10 6 . 55747 24 1 10 5 . 960466 69 2 10 6 . 224652 119 3 10 5 . 390844 132 3 7 4 . 393585 TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 8 0 10 20 30 40 50 1 2 3 4 5 Index M aha l abob i s d i s t an c e ( m v e ) 119 132 118 123 106107135136108101 Fig . 5 . Plot of the instances in class 3 of Iris ranked by their Mahalanobis distance using MVE estimator B . Detection of outliers using clustering A clustering technique can be used to detect outliers [ 23 ] . Scattered outliers will form a cluster of size 1 and clusters of small size can be considered as clustered outliers . There are a large number of clustering techniques [ 8 ] [ 13 ] . In this paper , we only considered the Partitioning Around Medoids ( PAM ) method , which is similar but more robust than k - means . PAM was introduced by Kaufman and Rousseeuw [ 13 ] , and uses h - clustering on medoids to identify clusters . PAM works efficiently on small data sets , but it is extremely costly for larger ones . This led to the development of CLARA ( Clustering Large Applications ) [ 13 ] , [ 19 ] where multiple samples of the data set are generated , and then PAM is applied to the samples . Given h , the number of partitions to construct , the PAM method creates an initial partitioning . It then uses an iterative relocation technique that attempts to improve the partitioning by moving instances from one group to another . The general criterion of good partitioning is that instances in the same cluster are “close” or related to each other , whereas instances of different clusters are “far apart” or very different . In order to find h clusters , PAM’s approach is to determine a representative instance for each cluster . This representative instance called medoid , is meant to be the most centrally located instance within the cluster . More specifically , a medoid can be defined as that instance of a cluster , whose average dissimilarity to all the objects in the cluster is minimal . After finding the set of medoids , each object of the data set is assigned to the nearest medoid . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 9 0 10 20 30 40 50 1 2 3 4 5 Index M aha l abob i s d i s t an c e ( m c d ) 119 132118123 106107135136108101 Fig . 6 . Plot of the instances in class 3 of Iris ranked by their Mahalanobis distance using MCD estimator The PAM algorithm’s complexity is O ( n ( n - h ) 2 ) . Hence it becomes too costly for a large values n and h . PAM is very robust to the presence of outliers and does not depend on the order in which instances are examined . After the allocation of the instances to the h clusters , one must determine the separation between them . The separation of the cluster C is defined as the smallest dissimilarity between two objects ; one of which belongs to Cluster C and the other which does not . If the separation of a cluster is large enough , then all the instances that belong to the cluster are considered outliers . In order to detect the clustered outliers one must vary the number , h , of clusters until clusters of small size are obtained that have a large separation from other clusters . The PAM algorithm can be evaluated using the function pam available in the library cluster in R . Examining the separation measures of ten clusters generated by the PAM algorithm in each of the classes of the Iris dataset , we detected the outliers shown in Table III . Notice that in the third class , PAM detects the instance number 107 as an outlier but does not detect the instance 119 . C . Distance based outlier detection Given a distance measure on a feature space , two different definitions of distance - based outliers are the following : 1 . An instance x in a dataset D is an outlier with parameters p and λ if at least a fraction p of the objects are a distance greater than λ from x [ 14 ] , [ 15 ] , [ 16 ] . This definition has certain difficulties such as the determination of λ and the lack of a ranking for the outliers . Thus an instance with very few neighbors within a distance λ can be regarded as strong an outlier as an instance with more neighbors within a distance λ . Furthermore , the time complexity of the algorithm is O ( kn 2 ) , where k is the number of features and n is the number of instances . Hence it is not an adequate definition to use with datasets having a large number of instances . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 10 2 . Given the integer numbers k and n ( k < n ) , outliers are the top n instances with the largest distance to their k - th nearest neighbor [ 21 ] . One shortcoming of this definition is that it only considers the distance to the k - th neighbor and ignores information about closer points . An alternative is to use the greatest average distance to the k nearest neighbors . The drawback of this alternative is that it takes longer calculate . In this paper a variant of one recently developed algorithm [ 5 ] for distance - based outlier detection has been used . TABLE III OUTLIERS IN THE IRIS DATASET ACCORDING TO THE PAM ALGORITHM Instance Class Separation 42 1 0 . 6244998 58 2 0 . 6480741 61 2 0 . 6480741 94 2 0 . 6480741 99 2 0 . 6480741 107 3 0 . 9110434 118 3 0 . 8185353 132 3 0 . 8185353 Bay’s Algorithm . Bay and Schwabacher [ 5 ] proposed a simple nested loop algorithm that tries to reconcile definitions 1 and 2 . It gives near linear time performance when the data is in random order because a simple pruning rule is used . The performance of the algorithm in the worst case is of quadratic order . The algorithm is described in Fig . 7 . The main idea in the algorithm is that for each instance in D one keeps track of the closest neighbors found so far . When an instance’s closest neighbors achieve a score lower than a cutoff then the instance is removed because it can no longer be an outlier . In this paper the score function used has been the sum of the distance to the k neighbors . The mean distance and the median distance to the k neighbors could also be considered as score function . Experimentally , we did not see much effect on the outlier detection due to the choice of the score function . As more instances are processed the algorithm finds more extreme outliers and the cutoff increases along with pruning efficiency . Bay and Schwabacher showed experimentally that Bay’s algorithm is linear with respect to the number of neighbors and that is almost linear with respect to the number of instances . Using six large datasets they found a complexity of order O ( n α ) where α varied from 1 . 13 to 1 . 32 . Working with three datasets : Ionosphere , Vehicle and Diabetes [ 6 ] we have obtained an α value near to 1 . 5 . The top 20 potential outliers in the third class of the Iris dataset according to Bay’s algorithm are shown in Fig . 8 . Clearly , the instance 107 is detected as an outlier . There is a second group that includes 132 , 118 , 110 , 119 and 120 . Input : k : number of nearest neighbors ; n : number of outliers to return ; D : dataset randomly ordered , BS : size of blocks in which D is divided . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 11 1 . Let distance ( x , y ) return the Euclidean distance between x and y . Another type of distance such as the Manhattan can be used instead of the Euclidean distance . 2 . Let maxdist ( x , Y ) return the maximum distance between the instance x and the set of instances Y . 3 . Let Closest ( x , Y , k ) return the k closest instances in Y to x . 4 . Let score ( x ) return median distance to the k neighbors 5 . Begin c ← 0 Set the cutoff for pruning to 0 . O ←φ Initializate the set of outliers as the empty set . NB ← ceiling ( # instances in D / BS ) While nb < NB { Neighbors ( b ) ←φ for all b in B ( nb ) For each d in D { For each b in B , b ≠ d { If | Neigbors ( b ) | < k or distance ( b , d ) < maxdist ( b , Neighbors ( b ) ) { Neighbors ( b ) ← Closest ( b , Neighbors ( b ) ∪ d , k ) If ( score ( Neighbors ( b ) , b ) < c { Remove b from B ( nb ) } } } } O ← Top ( B ( nb ) ∪ O , n ) ; Keep only the top n outliers c ← min ( score ( o ) ) for all in O ; The cutoff is the score of the weakest outlier } end Output : O , a set of outliers Fig . 7 . Bay’s Algorithm for finding distance - based outliers D . Density - based local outliers For this type of outlier the density of the neighbors of a given instance plays a key role . Furthermore an instance is not explicitly classified as either outlier or non - outlier ; instead for each instance a local outlier factor ( LOF ) is computed which will give an indication of how strongly an instance can be considered an outlier . Fig . 9 , taken from Breuning et al . [ 7 ] , shows the weakness of the distance - based method which identifies as outlier the instance o 1 , but does not consider o 2 as an outlier . The following two definitions are needed in order to formalize the algorithm to detect density - based local outliers : Definition 1 . k - distance of an instance x For any positive integer k , the k - distance of an instance x , denoted by k - distance ( x ) , is defined as the distance d ( x , y ) between x an instance y ε D such that : ( i ) for at least k instances y’ ∈ D - { x } it holds that d ( x , y’ ) ≤ d ( x , y ) ( ii ) for at most k - 1 instances y’ ∈ D - { x } it holds that d ( x , y’ ) < d ( x , y ) . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 12 5 10 15 20 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 Index sc o r e 1 0 7 1 3 2 1 1 8 1 1 01 1 91 2 0 1 3 51 0 91 3 6 1 1 51 0 11 3 01 0 61 2 31 0 81 3 41 3 11 0 31 2 61 4 Fig . 8 . Top 20 instances in class 3 of the Iris dataset ranked by the Bay’s algorithm score function . Definition 2 . Reachability distance of an instance x w . r . t . instance y Let k be a positive integer number . The reachability distance of the instance x with respect to the instance y is defined as reach - dist k ( x , y ) = max { k - distance ( y ) , d ( x , y ) } ( 6 ) The Local outlier factor ( LOF ) of an instance x is defined by ( ) 1 ( ) ( ) ( ) [ ] | ( ) | MinPts MinPts y N x MinPts MinPts MinPts lrd y lrd x LOF x N x ∈ − = ∑ ( 7 ) where lrd ( . ) represents the Local reachability density of an instance . Given an instance x , its lrd is defined as the inverse of the average reachability distance based on the MinPts - nearest neighbor of the instance x . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 13 Fig . 9 . Example to show the weakness of the distance - based method to detect outliers The density - based local algorithm to detect outliers requires only one parameter , MinPts , which is the number of nearest neighbors used in defining the local neighborhood of the instance . The LOF measures the degree to which an instance x can be considered as an outlier . Breuning et al . show that for instances deep inside a cluster their LOF’s are close to 1 and should not be labeled as a local outlier . Since LOF is not monotonic , they recommended finding the LOF for each instance of the datasets using MinPts - nearest neighbors , where MinPts assumes a range of values from MinPtsLB to MinPtsUB . They also suggested MINPtsLB = 10 and MinPtsUB = 20 . In this paper , LOF were found in a range of MinPtsLB = 10 and MinPtsUB = 20 or MinPtsLB = 20 and MinPtsUB = 30 , depending on which range produced a more monotonic graph . Having determined MinPtsLB and MinPtsUB , the LOF of each instance is computed within this range . Finally , all the instances are ranked with respect to the maximum LOF value within the specified range . That is , the ranking of an instance x is based on : Max { LOF MinPts ( x ) s . t . MinPtsLB ≤ MinPts ≤ MinPtsUB } ( 8 ) The algorithm to detect density - based local outliers is shown in Fig . 10 . Breuning et al . state that the time complexity of the maxLOF algorithm depends on the dimensionality of the data and it can be analyzed by studying independently the time complexity of the two main steps required to produce the LOF factor for each instance of the dataset . The first step , finding the k - distance neighborhood , has a runtime complexity of O ( n * time for a k - nn query ) . Therefore , the actual time complexity of this step is determined by the method used to perform the k - nn queries . For low dimensionality ( no more than 5 features ) , if a grid based approach is used the query can be performed in constant time leading to a complexity of O ( n ) to complete the entire step . For medium dimensionality ( between 5 and 10 features ) , an index can be used that would provide an average complexity of O ( log n ) for the k - nn queries , leading to a total complexity of O ( n log n ) . Finally , for high dimensional data , a sequential scan may be used with a complexity of O ( n ) that would lead to a total complexity of O ( n 2 ) . Finding the maximum outlier factors of all observations in the dataset can be done in linear time . Using the Ionosphere dataset , which has 32 features and 351 instances , the time complexity estimated by us was O ( n 1 . 95 ) . Fig . 11 shows the top 20 potential outliers in the third class of the Iris dataset using the LOF algorithm . Clearly the instance 107 is detected as an outlier . There is a second group that includes 119 , 118 , 132 , and 123 . Input : Dataset D , MinptsLB , MinptsUB TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 14 Let Maxlofevct = φ For each i in the interval [ MinPtsLB , MinPtsUB ] { 1 . Find the i nearest neighbors and their distance from each observation in D 2 . Calculate the local reachability density for each observation in D 3 . Compute the lof of each observation in D 4 . Maxlofvect = max ( maxlofvect , lof ) } 5 . rank ( D , Maxlofvect ) ; ordering instances of D in decreasing according to their corresponding Maxlofvect end Output : O , a set of outliers Fig . 10 . Algorithm to detect density - based local outliers 5 10 15 20 1 . 2 1 . 4 1 . 6 1 . 8 lower minpts : 10 upper minpts : 20 Observation number l o c a l ou t li e r f a c t o r 1 0 7 1 1 9 1 1 8 1 3 2 1 2 3 1 0 6 1 3 6 1 1 0 1 0 8 1 3 1 1 0 7 1 1 9 1 1 8 1 3 2 1 2 3 1 0 6 1 3 6 1 1 0 1 0 8 1 F IG . 11 . Top 20 instances in class 3 of the Iris dataset according to their LOF measure IV . EFFECT OF THE TREATMENT OF OUTLIERS IN SUPERVISED CLASSIFICATION In the literature , it is frequently mentioned that the presence of outliers affects the performance of a classifier , but there are few studies verifying such claim . This is not the case in a regression context where there are a large number of studies showing the effect of outliers in the regression model . Two main aspects to consider in supervised classification are feature selection and the estimation of the misclassification error rate . In this paper , an evaluation of the effect of outliers in those aspects is considered . We have considered three classifiers : linear discriminant analysis ( LDA ) , k - nearest neighbor classifier and a classifier based on decision trees named Rpart ( it stands for recursive partitioning ) . We have used 10 - fold cross validation as the estimation method for the misclassification error rate . There are plenty of feature selection methods and we TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 15 choose two that have given us good results in our previous work [ 1 ] . One is the sequential forward selection ( SFS ) method and the other one is a method called Relief . The first one is a wrapper method that requires a classifier and the second one is a filter method that does not require a classifier . Usually a filter selects more features than a wrapper . After the detection of outliers we want to see how they affect a ) the estimation of the misclassification error rate , b ) the feature selection process and , c ) the misclassification error rate after feature selection . V . EXPERIMENTAL RESULTS Four well - known Machine Learning datasets , Iris , Bupa , Diabetes , and Vehicle are used to show the effect of outliers on feature selection and on the estimation of the misclassification error rate . These datasets are available on the Machine Learning database repository at the University California , Irvine [ 6 ] . We use all the criteria described in section 3 and the visual help provided by the parallel coordinate plot [ 20 ] , [ 26 ] to decide about the doubtful outliers . The following outliers have been detected in the Iris dataset . Outliers in class 1 : ( 9 ) 16 , 15 , 34 , 42 , 44 , 24 , 23 , 19 , 45 Outliers in class 2 ( 9 ) 71 , 63 , 58 , 61 , 94 , 99 , 69 , 84 , 88 Outliers in class 3 ( 7 ) 107 , 119 , 132 , 118 , 120 , 123 , 110 A 16 . 67 % of the instances are considered as outliers in this dataset for at least one of criterion . In Table IV , the misclassification error of three classifiers has been computed using three types of samples : a ) the original sample , b ) a sample obtained by deleting the outliers from the original sample and , c ) a sample obtained by deleting a random subsample from the original sample . In the last case , the size of the random subsample that is excluded from the original dataset is equal to the number of outliers , but it does not contain any of them . In order to take into account the effect of variability , we have repeated the experiment ten times and the average misclassification error estimated by 10 - fold cross validation is reported on tables IV and V . TABLE IV THE MISCLASSIFICATION ERROR RATE FOR THE LDA , KNN AND RPART CLASSIFIERS IN IRIS USING THREE DIFFERENT TYPES OF SAMPLES Original Simple Deleting outliers Deleting a random subsample LDA 2 . 02 1 . 44 2 . 37 Knn ( k = 1 ) 4 . 05 1 . 60 4 . 72 Rpart 6 . 69 3 . 64 6 . 82 TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 16 We notice in table IV that the misclassification error rate for all three classifiers decreases in more than 25 % when outliers are removed , whereas there is not much change on the misclassification error when a random subsample of instances is removed . Table V shows the features selected using the three types of samples described before as well as the misclassification error rates of the three classifiers after feature selection . The feature selection methods used here are : sequential forward selection ( SFS ) along with the three classifiers used in Table IV and Relief . Notice that there are few differences between the subset of features selected by the four methods . Once again , the performance of the three classifiers is affected by the deletion of outliers , but the effect is greater than before since the misclassification error rate for the three classifiers decreases by more than 45 % . Since Relief is a feature selection method that does not require a classifier we have not included a misclassification error rate for it . Secondly , we consider the Bupa dataset , which has 345 instances , 6 features and 2 classes . Using the same methodology employed for the previous dataset we have detected the following outliers in each class of Bupa . Outliers in class 1 : ( 21 ) 190 , 317 , 316 , 182 , 205 , 335 , 345 , 343 , 189 , 312 , 344 , 175 , 168 , 183 , 25 , 172 , 311 , 167 , 326 , 148 , 261 . Outliers in class 2 ( 20 ) 85 , 36 , 134 , 233 , 331 , 300 , 179 , 323 , 342 , 111 , 115 , 77 , 186 , 252 , 294 , 139 , 307 , 224 , 286 , 157 A 11 . 88 % of all instances are declared as outliers . In Table VI , the misclassification error of three classifiers : LDA , Knn and Rpart has been computed based on the three type of samples considered before . Notice that the classifiers LDA and Knn are the most affected since their misclassification error rate decreases by more than 10 % . The least affected classifier has been Rpart , for which the misclassification error rate increases in less than 5 % . TABLE V FEATURES SELECTED IN IRIS USING SFS AND RELIEF FOR THE THREE TYPES OF SAMPLES Original Sample Deleting outliers Deleting a random subsample Features Error Features Error Features Error SFS ( lda ) 4 , 2 3 . 7 4 , 3 , 1 0 . 80 4 4 . 08 SFS ( knn ) 4 , 3 4 . 01 4 , 3 1 . 60 4 , 3 4 . 89 SFS ( rpart ) 4 5 . 29 4 2 . 52 4 4 . 85 Relief 2 , 3 , 4 - - 4 , 3 - - 4 , 3 - - TABLE VI THE MISCLASSIFICATION ERROR RATE FOR THE LDA , KNN AND RPART CLASSIFIERS IN BUPA USING THREE DIFFERENT TYPES OF SAMPLES Original Simple Deleting outliers Deleting a random subsample LDA 31 . 82 26 . 80 32 . 05 Knn ( k = 7 ) 31 . 55 28 . 09 32 . 30 Rpart 31 . 86 33 . 42 32 . 94 TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 17 Table VII shows the features selected using the three types of samples described before , as well as the misclassification error rates of the three classifiers after feature selection . Notice that the deletion of outliers shows some effect of the feature selection process , in particular when Relief is used . Also , the effect of the outliers on the estimation of the misclassification error after feature selection is more evident than before , since in this case the misclassification error rate for the three classifiers decreases in more than 10 % . Furthermore , we can see that lower misclassification errors are obtained for a sample where the feature selection is performed after deleting outliers . Next , we consider a third dataset named Diabetes , which has 768 instances , 8 features and 2 classes . In this dataset we have detected the following outliers : Outliers in class 1 : ( 38 ) 229 , 372 , 454 , 488 , 59 , 623 , 50 , 61 , 82 , 427 , 495 , 523 , 76 , 183 , 343 , 538 , 460 , 295 , 685 , 457 , 124 , 337 , 476 , 496 , 490 , 264 , 149 , 675 , 248 , 704 , 8 , 287 , 154 , 487 , 646 , 259 , 520 , 261 Outliers in class 2 : ( 26 ) 707 , 580 , 126 , 10 , 350 , 503 , 371 , 194 , 358 , 14 , 5 , 46 , 446 , 662 , 333 , 485 , 436 , 79 , 585 , 410 , 9 , 656 , 754 , 187 , 716 , 255 . An 8 . 33 % of all instances are declared as outliers . In Table VIII the misclassification error of three classifiers : LDA , Knn and Rpart has been computed based on the three type of samples considered before . Notice that , for this dataset , the deletion of outliers does not show a major effect on the misclassification error rate for the LDA and knn classifiers , since it decreases by less than 10 % for the three classifers . The greater effect is on the Rpart classifier where the misclassification error decreases on 11 . 6 % after deleting the outliers . One might think that perhaps there are more outliers , but if we look at the results of the sample where a random subsample has been deleted , the results observed are quite similar to those obtained with the original sample . TABLE VII FEATURES SELECTED IN BUPA USING SFS AND RELIEF FOR THE THREE TYPES OF SAMPLES Original Sample Deleting outliers Deleting a random subsample Features Error Features Error Features Error SFS ( lda ) 5 , 4 , 3 , 6 32 . 31 5 , 3 , 4 27 . 80 5 , 4 , 3 , 6 * 32 . 38 SFS ( knn ) 5 , 3 , 1 33 . 03 5 , 3 , 4 31 . 50 5 , 3 33 . 04 SFS ( rpart ) 5 , 3 , 6 , 2 31 . 30 5 , 3 , 2 29 . 19 5 , 3 , 6 31 . 11 Relief 6 , 3 , 4 - - 6 , 4 , 5 , 1 - - 3 , 6 - - TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 18 Table IX shows the features selected using the three types of samples described before , as well as the misclassification error rates of the three classifiers after feature selection . Notice that there are few differences between the subset of features selected with the three types of samples using the four methods . Also , we observe only an small effect of the deletion of outliers on the misclassification error after feature selection , since the misclassification error rate for the three classifiers decreases by less than 10 % . Finally , we consider a fourth dataset named Vehicle , which has 846 instances , 18 features and 4 classes . In this dataset we have detected the following outliers : Outliers in class 1 ( 19 ) : 5 , 101 , 128 , 545 , 816 , 688 , 836 , 734 , 86 , 382 , 322 , 532 , 397 , 6 , 55 , 275 , 156 , 684 , 687 Outliers in class 2 ( 18 ) : 613 , 114 , 412 , 182 , 797 , 74 , 90 , 729 , 580 , 592 , 464 , 516 , 161 , 503 , 350 , 517 , 16 , 211 Outliers in class 3 ( 27 ) : 124 , 615 , 12 , 420 , 423 , 232 , 290 , 368 , 566 , 663 , 184 , 250 , 835 , 352 , 689 , 379 , 139 , 27 , 261 , 395 , 637 , 600 , 311 , 662 , 777 , 440 , 643 Outliers in class 4 ( 9 ) : 389 , 38 , 136 , 707 , 292 , 524 , 392 , 273 , 353 An 8 . 62 % of all instances are declared as outliers . In Table X the misclassification error of three classifiers : LDA , Knn and Rpart had been computed based on the three types of samples described before . Notice that for this dataset , the deletion of outliers seems to TABLE VIII THE MISCLASSIFICATION ERROR RATE FOR THE LDA , KNN AND RPART CLASSIFIERS IN DIABETES USING THREE DIFFERENT TYPES OF SAMPLES Original Simple Deleting outliers Deleting a random subsample LDA 22 . 77 21 . 30 22 . 70 Knn ( k = 13 ) 25 . 51 24 . 71 25 . 43 Rpart 25 . 64 22 . 68 25 . 50 TABLE IX FEATURES SELECTED IN DIABETES BY SFS AND RELIEF FOR THE THREE TYPES OF SAMPLES Original Sample Deleting outliers Deleting a random subsample Features Error Features Error Features Error SFS ( lda ) 6 , 2 , 7 , 8 22 . 63 6 , 2 , 3 , 8 , 7 20 . 45 6 , 2 , 7 , 8 22 . 85 SFS ( knn ) 2 , 6 , 8 , 1 24 . 27 2 , 8 , 6 , 1 22 . 54 2 , 8 , 6 , 1 24 . 15 SFS ( rpart ) 2 , 6 25 . 50 2 , 6 23 . 31 2 , 3 25 . 60 Relief 1 , 8 , 7 - - 1 , 7 , 8 , 4 - - 1 , 8 , 7 , 4 - - TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 19 affect only the LDA classifier , since the misclassification error rate for the Knn and Rpart classifiers decreases in less than 5 % whereas for the LDA decreases in more than 10 % . Table XI shows the features selected using the three types of samples described before , as well as the misclassification error rates of the three classifiers after feature selection for the Vehicle dataset . Notice that the deletion of outliers shows some effect on the feature selection process , in particular when SFS ( Rpart ) and Relief are used . On the other hand , the misclassification error rate decreases in about 20 % for the LDA classifier whereas the Rpart classifier is barely affected since its misclassification error decreases only a 5 % . An alternative to deleting outliers is to treat them as missing values . Some people prefer the latter because it avoids the loss of sample size but others do not like this method much because it can create bias in the estimation . In this paper we have not experimented with this option . TABLE X THE MISCLASSIFICATION ERROR RATE FOR THE LDA , KNN AND RPART CLASSIFIERS IN VEHICLE USING THREE DIFFERENT TYPES OF SAMPLES Original Simple Deleting outliers Deleting a random subsample LDA 22 . 02 18 . 48 22 . 04 Knn ( k = 3 ) 34 . 71 32 . 62 34 . 49 Rpart 31 . 82 30 . 57 31 . 22 TABLE XI FEATURES SELECTED IN VEHICLE BY SFS AND RELIEF FOR THE THREE TYPES OF SAMPLES Original Sample Deleting outliers Deleting a random subsample Features Error Features Error Features Error SFS ( lda ) 11 , 6 , 3 , 1 , 10 , 8 , 5 , 4 , 17 , 18 24 . 35 11 , 10 , 8 , 6 , 1 , 3 , 18 14 , 5 , 4 , 17 19 . 53 11 , 6 , 3 , 1 , 10 , 8 , 5 , 4 , 17 , 18 23 . 70 SFS ( knn ) 5 , 2 , 6 , 8 , 9 , 10 28 . 25 6 , 8 , 11 , 5 , 1 7 , 14 30 . 95 8 , 6 , 1 , 5 , 2 , 3 29 . 35 SFS ( rpart ) 6 , 12 , 18 , 3 , 17 , 15 30 . 46 6 , 12 , 18 , 15 , 9 29 . 05 12 , 6 , 18 , 3 , 15 , 8 29 . 94 Relief 18 , 16 , 15 , 17 , 10 , 12 , 9 , 3 , 7 , 1 , 8 , 2 , 11 - - 6 , 16 , 18 , 5 , 15 , 17 , 14 , 9 , 12 , 10 , 1 , 11 , 8 - - 16 , 18 , 15 , 17 , 10 , 12 , 9 , 3 , 7 , 1 , 2 , 8 , 11 - - TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 20 VI . C ONCLUSION In this paper we have used several methods for outlier detection and applied them to four well - known machine - learning datasets . We can not recommend a unique method for detecting outliers because some methods are efficient for detecting certain types of outliers but fail to detect others . On the other hand , our experimental results evidence the effect of the deletion of outliers on the performance of classifiers . The LDA and K - nn classifiers seem to be affected more by the presence of outliers than the Rpart classifier . Furthermore , the presence of outliers shows some effect on feature selection but this effect does not seem to be as evident as on the estimation of the misclassification error rate . However it is clear that misclassification error rate decreases using features selected after removing outliers . In a future work we are planning to include more outlier detection methods and to consider larger datasets . A CKNOWLEDGMENT Edgar Acuna’s research work was supported by grant N00014 - 03 - 1 - 0359 from ONR and Caroline Rodriguez held a graduate fellowship through the grant EIA 99 - 77071 from NSF when this work was carried out . R EFERENCES [ 1 ] E Acuña , F . Coaquira , and M . Gonzalez , “A comparison of feature selection procedures for classifiers based on kernel density estimation , ” Proc . of the Int . Conf . on Computer , Communication and Control technologies , CCCT’03 . Vol I . pp . 468 - 472 . Orlando , Florida , 2003 . [ 2 ] A . C . Atkinson , “Fast very robust methods for the detection of multiple outliers , ” Journal of the American Statistical Association , 89 : pp . 1329 - 1339 , 1994 . [ 3 ] A . C . Atkinson , M . Riani , and A . Cerioli , Exploring Multivariate Data with the Forward Search . Springer - Verlag . New York , 2004 . [ 4 ] V . Barnett and T . Lewis , Outliers in Statistical Data . John Wiley . New York , 1994 . [ 5 ] S . D . Bay , and M . Schwabacher , “Mining distance - based outliers in near linear time with randomization and a simple pruning rule , ” Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2003 . [ 6 ] C . L . Blake and C . J . . Mertz , UCI Repository of machine learning databases [ http : / / www . ics . uci . edu / mlearn / MLRepository . html ] . Irvine , CA : University of California , Department of Information and Computer Science , 1998 . [ 7 ] M . Breuning , H . Kriegel , R . T . Ng , and J . Sander , LOF : Identifying density - based local outliers . In Proceedings of the ACM SIGMOD International Conference on Management of Data , 2000 . [ 8 ] C . Fraley and A . E . Raftery , “Model - based clustering , discriminant analysis , and density estimation , ” Journal of the American Statistical Association , 97 , pp . 611 - 631 , 2002 [ 9 ] A . Hadi , “Identifying multiple outliers in multivariate data , ” Journal of the Royal Statistical Society B , 54 : pp . 761 - 771 , 1992 . [ 10 ] J . Hardin , and D . M . Rocke , “Outlier Detection in the Multiple Cluster Setting using the Minimum Covariance Determinant Estimator , ” Computational Statistics and Data Analysis , 44 , pp . 625 – 638 , 2004 . [ 11 ] D . Hawkins , Identification of Outliers . Chapman and Hall . London , 1980 . [ 12 ] V . J . . Hodge , and J . Austin , “A survey of Outlier Detection Methodologies , ” Artificial Intelligence Review , 22 : pp . 85 - 126 , Kluwer ACADEMIC Publishers , 2004 . [ 13 ] L . Kaufman , and P . J . Rousseeuw , Finding Groups in Data : An Introduction to Cluster Analysis . Wiley , New York , 1990 . [ 14 ] E . Knorr , and R . Ng , “A unified approach for mining outliers , ” In Proc . KDD , pp . 219 – 222 , 1997 . [ 15 ] E . Knorr , and R . Ng , “Algorithms for mining distance - based outliers in large datasets , ” In Proc . 24th Int . Conf . Very Large Data Bases , VLDB , po . 392 – 403 , 24 – 27 , 1998 . [ 16 ] E . Knorr . , R . Ng , and V . Tucakov , “Distance - based outliers : Algorithms and applications , ” VLDB Journal : Very Large Data Bases , 8 ( 3 – 4 ) : pp . 237 – 253 , 2000 . [ 17 ] M . Markou and S . Singh , “Novelty Detection : A Review , Part I : Statistical Approaches , ” Signal Processing , 83 , pp . 2481 - 2497 , 2003 . [ 18 ] M . Markou and S . Singh , “Novelty Detection : A Review , Part II : Neural Network Based Approaches , “Signal Processing , 83 , pp . 2499 - 2521 , 2003 . [ 19 ] R . Ng , and J . Han , “Efficient and effective clustering methods for spatial data mining , ” Proc . 20 th Int . Conf . on Very Large Data bases . Morgan and Kaufmann Publishers , San Francisco , pp 144 - 155 , 1994 . [ 20 ] K . L . Penny , and I . T . Jolliffe , “A comparison of multivariate outlier detection methods for clinical laboratory safety data , ” The Statistician , 50 ( 3 ) : pp . 295 - 308 , 2001 . [ 21 ] S . Ramaswamy , R . Rastogi , and K . Shim , “Efficient algorithms for mining outliers from large data sets , ” In Proceedings of the ACM SIGMOD International Conference on Management of Data , 2000 . [ 22 ] D . Rocke and D . Woodruff , “Computational Connections Between Robust Multivariate Analysis and Clustering , ” in COMPSTAT 2002 Proc . in Computational Statistics , Wolfgang Härdle and Bernd Rönz eds . , 255 – 260 , Heidelberg : Physica - Verlag , 2002 . [ 23 ] D . Rocke and D . Woodruff , “Identification of outliers in multivariate data , ” Journal of the American Statistical Association , vol . 91 , no . 435 , pp . 1047 - 1061 , 1996 . [ 24 ] C . K . Rodriguez , “A computational environment for data preprocessing in supervised classification , ” M . S . thesis , Math . Dept . , University of Puerto Rico at Mayagüez . 2004 . [ 25 ] P . J . Rousseeuw , “Multivariate estimation with high breakdown point , ” in Mathematical statistics and applications , Vol B , eds . W Grossman , G . Pfug , L . Vincze , and W . Werz , Dordrecht , Reidel , 1985 . TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 21 [ 26 ] P . J . Rousseeuw and A . Leroy , A . Robust Regression and Outlier Detection . John Wiley , 1987 . [ 27 ] P . J . Rousseeuw and B . Van Zomeren , “Unmasking multivariate outliers and leverage points . ” Journal of the American Statistical Association , 85 : pp . 633 - 639 , 1990 . [ 28 ] P . J . Rousseeuw and K . Van Driessen , “A Fast Algorithm for the Minimum Covariance Determinant Estimator , ” Technometrics , 41 , pp . 212 - 223 , 1999 . [ 29 ] J . W . Tukey , Exploratory Data Analysis . Addison - Wesley . Boston , 1977 . . [ 30 ] E . Wegman , “Hyperdimensional Data Analysis Using Parallel Coordinate Plot , ” Journal of the American Statistical Association , 85 , pp . 664 - 675 , 1990 . E . Acuna received the BSc ( Hons ) degree in statistical engineering in 1978 from the UNALM , Peru , the PhD degree in Statistics in 1989 from the University of Rochester , USA . Currently he is Professor of Statistics and Computing and Information Science and Engineering in the Mathematics Department , University of Puerto Rico at Mayaguez ( UPRM ) . He is the leader of the research group in Computational and Statistical Learning for knowledge discovery at the mathematics department of the UPRM , which is funded by the Office of Naval Research . His research interests include : statistical and computational methods for intelligent data analysis in particular nonparametric methods related to feature selection , missing values , outlier detection and reduction of dimensionality in classification . He has published more than 10 research publications in these areas . Dr . Acuna was selected by the editors of the Hispanic Engineer and Information Technology magazine ( Baltimore , USA ) as one of Power Hitter in Business and Technology among the Hispanic scientists for the year 2003 . C . Rodriguez received the BSc ( Hons ) degree in mathematics with concentration in computer science in 1987 , and the MSc degree in scientific computing in 2004 both from the University of Puerto Rico - Mayaguez ( UPRM ) , Puerto Rico . Currently she is a doctoral student in Computing and Information Science and Engineering at the UPRM . She is a member of the research group in Computational and Statistical Learning for knowledge discovery at the Mathematics Department of the UPRM , which is funded by the Office of Naval Research . She has been performing research in data preprocessing since 2003 . Her main research interests include data mining , visualization and intelligent data analysis .